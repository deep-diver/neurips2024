[{"type": "text", "text": "Deterministic Uncertainty Propagation for Improved Model-Based Offline Reinforcement Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Abdullah Akg\u00fcl Manuel Hau\u00dfmann Melih Kandemir Department of Mathematics and Computer Science University of Southern Denmark Odense, Denmark {akgul,haussmann,kandemir}@imada.sdu.dk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Current approaches to model-based offline reinforcement learning often incorporate uncertainty-based reward penalization to address the distributional shift problem. These approaches, commonly known as pessimistic value iteration, use Monte Carlo sampling to estimate the Bellman target to perform temporal difference based policy evaluation. We find out that the randomness caused by this sampling step significantly delays convergence. We present a theoretical result demonstrating the strong dependency of suboptimality on the number of Monte Carlo samples taken per Bellman target calculation. Our main contribution is a deterministic approximation to the Bellman target that uses progressive moment matching, a method developed originally for deterministic variational inference. The resulting algorithm, which we call Moment Matching Offline Model-Based Policy Optimization (MOMBO), propagates the uncertainty of the next state through a nonlinear Q-network in a deterministic fashion by approximating the distributions of hidden layer activations by a normal distribution. We show that it is possible to provide tighter guarantees for the suboptimality of MOMBO than the existing Monte Carlo sampling approaches. We also observe MOMBO to converge faster than these approaches in a large set of benchmark tasks. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Offilne reinforcement learning (Lange et al., 2012; Levine et al., 2020) aims to solve a control task using an offline dataset without interacting with the target environment. Such an approach is essential in cases where environment interactions are expensive or risky (Shortreed et al., 2011; Singh et al., 2020; Nie et al., 2021; Micheli et al., 2022). Directly adapting traditional online off-policy reinforcement learning methods to offline settings often results in poor performance due to the adverse effects of the distributional shift caused by the policy updates (Fujimoto et al., 2019; Kumar et al., 2019). The chief reason for the incompatibility is the rapid accumulation of overestimated actionvalues during policy improvement. Pessimistic Value Iteration (PEVI) (Jin et al., 2021) offers a theoretically justified solution to this problem that penalizes the estimated rewards of synthetic state transitions proportionally to the uncertainty of the predicted next state. The framework encompasses many state-of-the-art offline reinforcement learning algorithms as special cases (Yu et al., 2020; Sun et al., 2023). ", "page_idx": 0}, {"type": "text", "text": "Model-based offline reinforcement learning approaches first fit a probabilistic model on the real state transitions and then supplement the real data with synthetic samples generated from this model throughout the course of policy search (Janner et al., 2019). One line of work addresses distributional shift by constraining policy learning (Kumar et al., 2019; Fujimoto and Gu, 2021) where the policy is trained to mimic the behavior policy and is penalized based on the discrepancy between its actions and those of the behavior policy, similarly to behavioral cloning. A second strategy introduces conservatism to training by $(i)$ perturbing the training objective with a high-entropy behavior policy (Kumar et al., 2020; Yu et al., 2021), (ii) penalizing state-action pairs proportionally to their estimated variance (Yu et al., 2020; An et al., 2021; Bai et al., 2022; Sun et al., 2023), or (iii) adversarially training the environment model to minimize the value function (Rigter et al., 2022) to prevent overestimation in policy evaluation for out-of-domain state-action pairs. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In the offline reinforcement learning literature, uncertaintydriven approaches exist for both model-free (An et al., 2021; Bai et al., 2022) and model-based settings (Yu et al., 2020; Sun et al., 2023), all aimed at learning a pessimistic value function (Jin et al., 2021) by penalizing it with an uncertainty estimator. The impact of uncertainty quantification has been investigated in both online (Abbas et al., 2020) and offline (Lu et al., 2021) scenarios, particularly in model-based approaches, which is the focus of our work. ", "page_idx": 1}, {"type": "text", "text": "Despite the strong theoretical evidence suggesting that the quality of the Bellman target uncertainties has significant influence on model quality (O\u2019Donoghue et al., 2018; Luis et al., 2023; Jin et al., 2021), the existing implementations rely on Monte Carlo samples and heuristics-driven uncertainty quantifiers being used as reward penalizers (Yu et al., 2020; Sun et al., 2023) which results in a high degree of randomness that manifests itself as significant delays in model convergence. Figure 1 demonstrates why this is the case by a toy example. The uncertainty around the estimated action-value of the next state does not shrink even after taking 10000 samples on the next state and passing them through a Q-network. Our moment matching-based approach predicts a similar mean with significantly smaller variance at the cost of only two Monte Carlo samples. ", "page_idx": 1}, {"type": "image", "img_path": "OFmclNhp0y/tmp/c56dc6427e8600710b38ed18e689ff0077324ab07df5eb758426a7ade916ad35.jpg", "img_caption": ["Figure 1: Moment Matching versus Monte Carlo Sampling. Moment matching offers sharp estimates of the action-value of the next state at the cost of only two forward passes through a critic network. A similar sharpness cannot be reached even with 10000 Monte Carlo samples, which is 5000 times more costly. See Appendix C.1.1 for details. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "We identify the randomness introduced by Bellman target approximation via Monte Carlo sampling as the primary limiting factor for the performance of the PEVI approaches used in offline reinforcement learning. We have three main contributions: ", "page_idx": 1}, {"type": "text", "text": "(i) We present a new algorithm that employs progressive moment matching, an idea that has been developed originally for deterministic variational inference of Bayesian neural networks (Wu et al., 2019a), for the first time to deterministically propagate environment model uncertainties through Q-function estimates. We refer to this algorithm as Moment Matching Offline Model-Based Policy Optimization (MOMBO).   \n(ii) We develop novel suboptimality guarantees for both MOMBO and existing sampling-based approaches highlighting that MOMBO is provably more efficient.   \n(iii) We present comprehensive experiment results showing that MOMBO significantly accelerates training convergence while maintaining asymptotic performance. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Reinforcement learning. We define a Markov Decision Process (MDP) as a tuple $\\mathcal{M}\\triangleq(\\mathcal{S},\\mathcal{A},r,\\mathrm{P},\\mathrm{P}_{1},\\gamma,H)$ where $\\boldsymbol{S}$ represents the state space and $\\boldsymbol{\\mathcal{A}}$ denotes the action space. We have $r:S\\times A\\to[0,R_{\\mathrm{max}}]$ as a bounded deterministic reward function and $\\mathrm{P}:S\\times{\\mathcal{A}}\\times{\\mathit{\\dot{\\Delta}}}\\Delta(S)\\rightarrow$ $[0,1]$ as the probabilistic state transition kernel, where $\\Delta(S)$ denotes the probability simplex defined over the state space. The MDP has an initial state distribution $\\mathrm{P_{1}}\\in\\bar{\\Delta}(S)$ , a discount factor $\\gamma\\in(0,1)$ , and an episode length $H$ . We use deterministic policies $\\pi:{\\mathcal{S}}\\rightarrow A$ and deterministic rewards in analytical demonstrations for simplicity and without loss of generality. Our results extend straightforwardly to probabilistic policies and reward functions. The randomness on the next state may result from the system stochasticity or the estimation error of a probabilistic model trained on the data collected from a deterministic environment. We denote the action-value of a policy $\\pi$ for a ", "page_idx": 1}, {"type": "text", "text": "state-action pair $(s,a)$ as: ", "page_idx": 2}, {"type": "equation", "text": "$$\nQ_{\\pi}(s,a)\\triangleq\\mathbb{E}_{\\pi}\\left[\\sum_{i=h}^{H}\\gamma^{i-h}r(s_{i},a_{i})|s_{h}=s,a_{h}=a\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This function maps to the range $[0,R_{\\mathrm{max}}/1\\!-\\!\\gamma]$ . ", "page_idx": 2}, {"type": "text", "text": "Offline reinforcement learning algorithms perform policy search using an offline collected dataset $\\bar{\\boldsymbol{D}}=\\{(s,a,r,\\bar{s^{\\prime}})\\}$ from the target environment via a behavior policy $\\pi_{\\beta}$ . The goal is to find a policy $\\pi$ that minimizes the suboptimality, defined as ${\\mathrm{Subopt}}(\\pi;s)\\triangleq Q_{\\pi^{*}}(s,\\pi^{*}(s))-Q_{\\pi}(s,\\pi(s))$ for an initial state $s$ and optimal policy $\\pi^{*}$ . For brevity, we omit the dependency on $s$ in $\\operatorname{Subopt}(\\cdot)$ . Model-based reinforcement learning approaches often train state transition probabilities and reward functions on the offline dataset by maximum likelihood estimation with an assumed density, modeling these as an ensemble of heteroscedastic neural networks (Lakshminarayanan et al., 2017). Mainstream model-based offline reinforcement learning methods adopt the Dyna-style (Sutton, 1990; Janner et al., 2019), which suggests training an off-the-shelf model-free reinforcement learning algorithm on synthetic data generated from the learned environment model $\\widehat{\\mathcal{D}}$ using initial states from $\\mathcal{D}$ . It has been observed that mixing minibatches collected from synthet i c and real datasets improves performance in both online (Janner et al., 2019) and offline (Yu et al., 2020, 2021) settings. ", "page_idx": 2}, {"type": "text", "text": "Scope and problem statement. We focus on model-based offline reinforcement learning due to its superior performance over model-free methods (Yu et al., 2020; Kidambi et al., 2020; Yu et al., 2021; Rigter et al., 2022; Sun et al., 2023). A primary challenge in offline reinforcement learning is the distributional shift caused by the limited coverage of the state-action space. When the policy search algorithm probes unobserved state-action pairs during training, significant value approximation errors arise. In standard supervised learning, such errors diminish as new observations are collected. However, in reinforcement learning, overestimation errors are exploited by the policy improvement step and accumulate, resulting in a phenomenon known as the overestimation bias (Thrun and Schwartz, 1993). Techniques developed to overcome this problem in the online setting such as min-clipping (Fujimoto et al., 2018) are insufficient for offline reinforcement learning. Algorithms that use reward penalization proportional to the estimated uncertainty of an unobserved next state are commonly referred to as Pessimistic Value Iteration (Yu et al., 2020; Sun et al., 2023). These algorithms are grounded in a theory that provides performance improvement guarantees by bounding the variance of next state predictions (Jin et al., 2021; Uehara and Sun, 2022). We demonstrate the theoretical and practical limitations of PEVI-based approaches and address them with a new solution. ", "page_idx": 2}, {"type": "text", "text": "Bellman operators. Both our analysis and the algorithmic solution build on a problem that arises from the inaccurate estimation of the Bellman targets in critic training. The error stems from the fact that during training the agent has access only to the sample Bellman operator: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\mathbb{B}}_{\\pi}Q(s,a,s^{\\prime})\\triangleq r(s,a)+\\gamma Q(s^{\\prime},\\pi(s^{\\prime}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $s^{\\prime}$ is a Monte Carlo sample of the next state. However, the actual quantity of interest is the exact Bellman operator that is equivalent to the expectation of the sample Bellman operator: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{B}_{\\pi}Q(s,a)\\triangleq\\mathbb{E}_{s^{\\prime}\\sim\\mathrm{P}(\\cdot|s,a)}\\left[\\widehat{\\mathbb{B}}_{\\pi}Q(s,a,s^{\\prime})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "3 Pessimistic value iteration algorithms ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A pessimistic value iteration algorithm (Jin et al., 2021), denoted as $\\mathbb{A}_{\\mathrm{PEVI}}(\\widehat{\\mathbb{B}}_{\\pi}^{\\Gamma}Q,\\widehat{\\mathbb{P}})$ , performs Dynastyle model-based learning using the following pessimistic sample Bellma n targ e t for temporal difference updates during critic training: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\widehat{\\mathbb{B}}_{\\pi}^{\\Gamma}Q(s,a,s^{\\prime})\\triangleq\\widehat{\\mathbb{B}}_{\\pi}Q(s,a,s^{\\prime})-\\Gamma_{\\widehat{\\mathbb{P}}}^{Q}(s,a)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\Gamma_{\\widehat{\\mathrm{P}}}^{Q}(s,a)$ admits a learned state transition kernel $\\widehat{\\mathrm{P}}$ and a value function $Q$ to map a state-action pair to a  penalty score. Notably, this penalty term d o es not depend on an observed or previously sampled $s^{\\prime}$ as it quantifies the uncertainty around $s^{\\prime}$ using $\\widehat{\\mathrm{P}}$ . The rest follows as running an offthe-shelf model-free online reinforcement learning algorith m , for instance Soft Actor-Critic (SAC) (Haarnoja et al., 2018), on a replay buffer that comprises a blend of real observations and synthetic data generated fromP using the recent policy of an intermediate training stage. We study the following two PEVI  va riants in this paper due to their representative value: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "i) Model-based Offline Policy Optimization (MOPO) ( $\\mathrm{Yu}$ et al., 2020) penalizes directly the uncertainty of a state as inferred by the learned environment model $\\Gamma_{\\widehat{\\mathrm{P}}}^{Q}(s,a)\\,\\triangleq\\,\\mathrm{var}_{\\widehat{\\mathrm{P}}}[s^{\\prime}]$ MOPO belongs to the family of methods where penalties are based on  the uncertaint y of the next state.   \nii) MOdel-Bellman Inconsistency penalized offLinE Policy Optimization (MOBILE) (Sun et al., 2023) propagates the uncertainty of the next state to the Bellman target through Monte Carlo sampling and uses it as a penalty: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Gamma_{\\widehat{\\mathrm{P}}}^{Q}(s,a)\\triangleq\\widehat{\\mathrm{var}}_{s^{\\prime}\\sim\\widehat{\\mathrm{P}}}^{N}[\\widehat{\\mathbb{B}}_{\\pi}Q(s,a,s^{\\prime})]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\widehat{\\mathrm{var}}_{s^{\\prime}\\sim\\widehat{P}}^{N}$ denotes the empirical variance of the quantity in brackets with respect to $N$ sam p les dr awn i.i.d. from $\\widehat{\\mathrm{P}}$ . MOBILE represents the family of methods that penalize rewards based on the uncertai n ty of the Bellman target. ", "page_idx": 3}, {"type": "text", "text": "Both approaches approximate the mean of the Bellman target by evaluating the sample Bellman operator $\\widehat{\\mathbb{B}}_{\\pi}Q(s,a,s^{\\prime})$ with a single $s^{\\prime}$ available either from real environment interaction within the dataset o r  a single sample taken from $\\widehat{\\mathrm{P}}$ . The following theorem establishes the critical role the penalty term plays in the model perform  ance. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1 (Suboptimality of PEVI (Jin et al., 2021)). For any $\\pi$ derived with $\\mathbb{A}_{P E V I}(\\widehat{\\mathbb{B}}_{\\pi}^{\\Gamma}Q,\\widehat{\\mathbb{P}})$ that satisfies ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathbb{B}_{\\pi}Q(s,a)-\\widehat\\mathbb{B}_{\\pi}Q(s,a,s^{\\prime})|\\leq\\Gamma_{\\widehat{\\mathbb{P}}}^{Q}(s,a),\\qquad\\forall(s,a)\\in\\mathcal{S}\\times\\mathcal{A}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with probability at least $1-\\delta$ for some error tolerance $\\delta\\in(0,1)$ where $s^{\\prime}\\sim\\operatorname{P}(\\cdot|s,a)$ , the following inequality holds for any initial state $s_{1}\\in\\mathcal{S}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nS u b O p t(\\pi)\\leq2\\sum_{i=1}^{H}\\mathbb{E}_{\\pi^{*}}\\left[\\Gamma_{\\widehat{P}}^{Q}(s_{i},a_{i})\\Big|s_{1}\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "When deep neural networks are used as value function approximators, calculating their variance becomes analytically intractable even for normally distributed inputs. Consequently, reward penalties \u0393Q are typically derived from Monte Carlo samples, which are prone to high estimator variance ( see Figure 1). Our key finding is that using high-variance estimates for reward penalization introduces three major practical issues in the training process of offline reinforcement learning algorithms: ", "page_idx": 3}, {"type": "text", "text": "$(i)$ The information content of the distorted gradient signal shrinks, causing delayed convergence to the asymptotic performance.   \n(ii) The first two moments of the Bellman target are poorly approximated for feasible sample counts.   \n(iii) Larger confidence radii need to be used to counter the instability caused by $(i)$ and the high overestimation risk caused by the inaccuracy described in $(i i)$ , which unnecessarily restricts model capacity. ", "page_idx": 3}, {"type": "text", "text": "3.1 Theoretical analysis of sampling-based PEVI ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We motivate the benefit of our deterministic uncertainty propagation scheme by demonstrating the prohibitive relationship of the sample Bellman operator when used in the PEVI context. The analysis of the distribution around the Bellman operator can be characterized as follows. We are interested in the distribution of a deterministic map $y=f(x)$ for a normally distributed input $x\\sim\\mathcal{N}(\\mu,\\sigma^{2})$ . We analyze this complex object via a surrogate of it. For a sample set ${\\cal{S}}_{N}=\\{x_{i}\\}_{i=1}^{N}$ obtained i.i.d. ", "page_idx": 3}, {"type": "text", "text": "from $\\mathcal{N}(\\mu,\\sigma^{2})$ , let $\\mu_{N}$ and $\\sigma_{N}^{2}$ be its empirical mean and variance. Now consider the following two random variables $\\begin{array}{r}{y_{N}=\\frac{1}{N}\\sum_{i=1}^{N}f(x_{i})}\\end{array}$ and $y_{N}^{\\prime}=f(x^{\\prime})$ for $x^{\\prime}\\sim\\mathcal{N}(\\mu_{N},\\sigma_{N}^{2})$ . Note that $y_{1}$ and $y_{1}^{\\prime}$ are equal in distribution. Furthermore, both $y_{N}$ and $y_{N}^{\\prime}$ converge to the true $y$ as $N$ tends to infinity. We perform our analysis on $y_{N}^{\\prime}$ where analytical guarantees are easier to build. Furthermore, $y_{N}^{\\prime}$ is a tighter approximation of both $y_{1}$ and $y_{1}^{\\prime}$ . We construct the following suboptimality proof for the PEVI algorithmic family that approximates the uncertainty around the Bellman operator in the way $y_{N}^{\\prime}$ is built. In this theorem and elsewhere, $A_{l}$ stands for the weights of the l\u2212th layer of a MultiLayer Perceptron (MLP) which has 1-Lipschitz activation functions and $\\left\\Vert\\cdot\\right\\Vert$ denotes $L1$ -norm. See Appendix B.2 for the proof. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2 (Suboptimality of sampling-based PEVI). For any policy $\\pi_{M C}$ learned by $\\mathbb{A}_{P E V I}(\\widehat{\\mathbb{B}}_{\\pi}^{\\Gamma}Q,\\widehat{\\mathbb{P}})$ using $N$ Monte Carlo samples to approximate the Bellman target with respect to an actio n-val u e network defined as an $L-$ layer MLP with 1-Lipschitz activation functions, the following inequality holds for any error tolerance $\\delta\\in(0,1)$ with probability at least $1-\\delta$ ", "page_idx": 4}, {"type": "equation", "text": "$$\nS u b O p t(\\pi_{M C})\\le2H\\prod_{l=1}^{L}\\lVert A_{l}\\rVert\\sqrt{-\\frac{8\\log(\\delta/4)R_{\\operatorname*{max}}^{2}}{\\lfloor{N/2}\\rfloor(1-\\gamma)^{2}}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The bound in Theorem 2 is prohibitively loose since $R_{\\operatorname*{max}}^{2}/(1-\\gamma)^{2}$ is a large number in practice. For instance, the maximum realizable step reward in the HalfCheetah $-\\upgamma4$ environment of the MuJoCo physics engine (Todorov et al., 2012) is at least around 11.7 according to Hui et al. (2023). Choosing the usual $\\gamma=0.99$ , we obtain $R_{\\mathrm{max}}^{2}/(1-\\gamma)^{2}=1.37\\times10^{6}$ . Furthermore, as the bound depends on the number of samples for the next state $N$ , it becomes looser as $N\\rightarrow1$ . The bound is not defined for $N=1$ , but the loosening trend is clear. ", "page_idx": 4}, {"type": "text", "text": "4 MOMBO: Moment matching offline model-based policy optimization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our key contribution is the finding that quantifying the uncertainty around the Bellman target brings significant benefits to the model-based offilne reinforcement learning setting. We obtain a deterministic approximation using a moment matching technique originally developed for Bayesian inference. This method propagates the estimated uncertainty of the next state $s^{\\prime}$ obtained from a learned environment modelP through an action-value network by alternating between an affine transformation of a normally d i stributed input to another normally distributed linear activation and projecting the output of a nonlinear activation to a normal distribution by analytically calculating its first two moments. The resulting normal distributed output is then used to build a lower confidence bound on the Bellman target, which is an equivalent interpretation of reward penalization. Such a deterministic approximation is both a more accurate uncertainty quantifier and a more robust quantity to be used during training in a deep reinforcement learning setting. Its contribution to robust and sampleefficient training has been observed in other domains (Wang and Manning, 2013; Wu et al., 2019a). Prior work attempted to propagate full covariances through deep neural nets (see, e.g. Wu et al., 2019a; Look et al., 2023; Wright et al., 2024) at a prohibitive computational cost (quadratic in the number of neurons) that does not bring a commensurate empirical benefit. Therefore, we choose to propagate only means and variances. ", "page_idx": 4}, {"type": "text", "text": "Assuming the input of a fully-connected layer to be $X\\,\\sim\\,{\\mathcal{N}}(X|\\mu,\\sigma^{2})$ , the pre-activation $Y$ for a neuron associated with weights $\\theta$ is given by $Y\\ =\\ \\theta^{\\top}X$ , i.e., $Y\\;\\sim\\;{\\mathcal{N}}(Y|\\theta^{\\top}\\mu,(\\theta^{2})^{\\top}\\sigma^{2})$ , where we absorb the bias into $\\theta$ for simplicity and the square on $\\theta$ is applied element-wise. For a ReLU activation function $\\mathrm{r}(x)\\triangleq\\operatorname*{max}(0,x)^{1}$ , mean $\\widetilde{\\mu}$ and variance $\\widetilde{\\sigma}^{2}$ of $Y=\\operatorname*{max}(0,X)$ are analytically tractable (Frey and Hinton, 1999). We ap p roximate the  o utput with a normal distribution $\\widetilde{X}\\sim\\mathcal{N}(\\widetilde{X}|\\widetilde{\\mu},\\widetilde{\\sigma}^{2})$ and summarize several properties regarding $\\widetilde{\\mu}$ and $\\widetilde{\\sigma}^{2}$ below. See Appendix  B .1 for p r oo fs   of all results in this subsection. ", "page_idx": 4}, {"type": "text", "text": "Lemma 1 (Moment matching). For $X\\sim{\\mathcal{N}}(X|\\mu,\\sigma^{2})$ and $Y=\\operatorname*{max}(0,X)$ , we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}{(i)}&{{}\\widetilde{\\mu}\\triangleq\\mathbb{E}\\left[Y\\right]=\\mu\\Phi(\\alpha)+\\sigma\\phi(\\alpha),}&{}&{{}\\widetilde{\\sigma}^{2}\\triangleq\\mathrm{var}\\left[Y\\right]=(\\mu^{2}+\\sigma^{2})\\Phi(\\alpha)+\\mu\\sigma\\phi(\\alpha)-\\widetilde{\\mu}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\alpha=\\mu/\\sigma$ , and $\\phi(\\cdot),\\,\\Phi(\\cdot)$ are the probability density function (pdf) and cumulative distribution function (cdf) of the standard normal distribution, respectively. Additionally, we have that ", "page_idx": 5}, {"type": "equation", "text": "$$\n(i i)\\quad{\\widetilde{\\mu}}\\geq\\mu\\quad a n d\\quad(i i i)\\quad{\\widetilde{\\sigma}}^{2}\\leq\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 outlines the moment matching process. This process involves two passes through the linear layer: one for the first moment and one for the second, along with a few additional operations that take negligible computation time. In contrast, sampling-based methods require $N$ forward passes, where $N$ is the number of samples. Thus, for any $N>2$ , sampling-based methods introduce computational overhead. For example, MOBILE (Sun et al., 2023) uses $N=10$ . See Figure 1 and Figure 3 for illustrations on the effect of varying $N$ . ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 Deterministic uncertainty propagation through moment matching ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "function MOMENTMATCHINGTHROUGHLINEAR $(\\theta,b,X)$ $\\theta$ er, $b$ : bias of the layer $X=\\tilde{\\mathcal{N}}(X|\\mu_{X},\\sigma_{X}^{2})$ $\\triangleright$ input a normal distribution $(\\mu_{Y},\\sigma_{Y}^{2})\\gets(\\theta^{\\top}\\mu_{X}+b,\\theta^{2\\top}\\sigma_{X}^{2})$ \u25b7transform mean and variance return $\\dot{Y}=\\mathcal{N}(Y|\\mu_{Y},\\sigma_{Y}^{2})$ \u25b7output the transformed distribution   \nend function   \nfunction MOMENTMATCHINGTHROUGHRELU $(X)$ $X=\\mathcal{N}(X|\\mu_{X},\\sigma_{X}^{2})$ \u25b7input a normal distribution \u03b1 \u00b5X/\u03c3X $\\mu_{Y}\\leftarrow\\dot{\\mu}_{X}\\Phi(\\alpha)+\\sigma_{X}\\phi(\\alpha)$ \u25b7compute the first two moments of ReLU $(X)$ $\\sigma_{Y}^{2}\\leftarrow\\left(\\mu_{X}^{2}+\\sigma_{X}^{2}\\right)\\Phi(\\alpha)+\\mu_{X}\\sigma_{X}\\phi(\\alpha)-\\mu_{Y}^{2}$ \u25b7 $\\phi/\\Phi$ are the normal pdf/cdf return $Y=\\mathcal{N}(Y|\\mu_{Y},\\sigma_{Y}^{2})$ $\\triangleright$ output a normal distribution with these moments   \nend function ", "page_idx": 5}, {"type": "text", "text": "We propagate the distribution of the next state predicted by the environment model through the action-value function network using moment matching. We define a moment matching Bellman target distribution as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widetilde{\\mathbb{B}}_{\\pi}Q(s,a,s^{\\prime})\\overset{d}{=}r(s,a)+\\gamma Q_{M M}(s^{\\prime},\\pi(s^{\\prime}))\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for $s^{\\prime}\\,\\sim\\,\\widehat{\\mathrm{P}}(\\cdot|s,a)$ and $Q_{M M}(s^{\\prime},a^{\\prime})$ a normal distribution with mean $\\mu_{M M}(s^{\\prime},a^{\\prime})$ and variance $\\sigma_{M M}^{2}(s^{\\prime},a^{\\prime})$ obtained as the outcome of a progressive application of Algorithm 1 through the layers of a critic network $Q$ . The sign $\\underline{{\\underline{{d}}}}$ denotes equality in distribution, i.e. the expressions on the two sides share the same probability law. We perform pessimistic value iteration using a lower confidence bound on $\\widetilde{\\mathbb{B}}_{\\pi}Q\\bar{(s,a,s^{\\prime})}$ as the Bellman target ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widetilde{\\mathbb{B}}_{\\pi}^{\\Gamma}Q(s,a,s^{\\prime})\\triangleq r(s,a)+\\gamma\\mu_{M M}(s^{\\prime},\\pi(s^{\\prime}))-\\beta\\gamma\\sigma_{M M}(s^{\\prime},\\pi(s^{\\prime}))\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for some radius $\\beta>0$ . ", "page_idx": 5}, {"type": "text", "text": "4.1 Theoretical analysis of moment matching-based uncertainty propagation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The following lemma provides a bound on the 1-Wasserstein distance $W_{1}$ between the true distribution $\\rho_{Y}$ of a random variable after being transformed by a ReLU activation function and its moment matched approximation $\\rho_{\\widetilde{X}}$ . See Appendix B.3 for the proofs of all results presented in this subsection. ", "page_idx": 5}, {"type": "text", "text": "Lemma 2 (Moment matching bound). For the following three random variables ", "page_idx": 5}, {"type": "equation", "text": "$$\nX\\sim\\rho_{X},\\qquad Y=\\operatorname*{max}(0,X),\\qquad\\widetilde{X}\\sim\\mathcal{N}(\\widetilde{X}|\\widetilde{\\mu},\\widetilde{\\sigma}^{2})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with ${\\widetilde{\\mu}}=\\mathbb{E}\\left[Y\\right]$ and $\\widetilde{\\sigma}^{2}=\\mathrm{var}\\left[Y\\right]$ the following inequality ho lds ", "page_idx": 5}, {"type": "equation", "text": "$$\nW_{1}(\\rho_{Y},\\rho_{\\widetilde{X}})\\leq\\int_{-\\infty}^{0}F_{\\widetilde{X}}(u)d u+W_{1}(\\rho_{X},\\rho_{\\widetilde{X}})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $F_{\\widetilde{X}}(\\cdot)$ is cdf of $\\widetilde{X}$ . If $\\cdot\\rho_{X}={\\mathcal{N}}(X|\\mu,\\sigma^{2}),$ , it can be further simplified to ", "page_idx": 5}, {"type": "equation", "text": "$$\nW_{1}(\\rho_{Y},\\rho_{\\widetilde{X}})\\leq\\widetilde{\\sigma}\\phi\\left(\\frac{\\widetilde{\\mu}}{\\widetilde{\\sigma}}\\right)-\\widetilde{\\mu}\\Phi\\left(-\\frac{\\widetilde{\\mu}}{\\widetilde{\\sigma}}\\right)+|\\mu-\\widetilde{\\mu}|+|\\sigma-\\widetilde{\\sigma}|.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Applying this to a moment matching $L-$ layer MLP yields the following deterministic bound. ", "page_idx": 6}, {"type": "text", "text": "Lemma 3 (Moment matching MLP bound). Let $f(X)$ be an $L-$ layer MLP with ReLU activation $\\mathrm{r}(x)=\\operatorname*{max}(0,x)$ . For $l=1,\\dotsc,L-1$ , the sampling-based forward-pass is ", "page_idx": 6}, {"type": "equation", "text": "$$\nY_{0}=X_{s},\\qquad Y_{l}=\\operatorname{r}(f_{l}(Y_{l-1})),\\qquad Y_{L}=f_{L}(Y_{L-1})\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $f_{l}(\\cdot)$ is the $l$ -th layer and $X_{s}$ a sample of $\\mathcal{N}(X|\\mu_{X},\\sigma_{X}^{2})$ . Its moment matching pendant is ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\widetilde{X}_{0}\\sim\\mathcal{N}(\\widetilde{X}_{0}|\\mu_{X},\\sigma_{X}^{2}),\\qquad\\widetilde{X}_{l}\\sim\\mathcal{N}\\left(\\widetilde{X}_{l}\\Big|\\mathbb{E}\\left[r(f_{l}(\\widetilde{X}_{l-1}))\\right],\\mathrm{var}\\left[r(f_{l}(\\widetilde{X}_{l-1}))\\right]\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The following inequality holds for $\\widetilde{\\rho}_{Y}=\\rho_{\\widetilde{X}_{L}}=\\mathcal{N}(\\widetilde{X}_{L}|\\mathbb{E}[f(\\widetilde{X}_{L-1})],\\mathrm{var}[f(\\widetilde{X}_{L-1})]).$ ", "page_idx": 6}, {"type": "equation", "text": "$$\nW_{1}(\\rho_{Y},\\widetilde{\\rho}_{Y})\\leq\\sum_{l=2}^{L}\\left(G(\\widetilde{X}_{l-1})+C_{l-1}\\right)\\prod_{j=l}^{L}\\lVert A_{j}\\rVert,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$$\nG(\\widetilde{X}_{l})=\\widetilde{\\sigma}_{l}\\phi\\left(\\frac{\\widetilde{\\mu}_{l}}{\\widetilde{\\sigma}_{l}}\\right)-\\widetilde{\\mu}_{l}\\Phi\\left(-\\frac{\\widetilde{\\mu}_{l}}{\\widetilde{\\sigma}_{l}}\\right)\\leq1,\\qquad C_{l}\\leq|A_{l}\\widetilde{\\mu}_{l-1}-\\widetilde{\\mu}_{l}|+\\left|\\sqrt{A_{l}^{2}\\widetilde{\\sigma}_{l-1}^{2}}-\\widetilde{\\sigma}_{l}\\right|\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\sqrt{\\cdot}\\,a n d\\left(\\cdot\\right)^{2}$ are app lied elementwis e. ", "page_idx": 6}, {"type": "text", "text": "Relying on Lemma 6 again, this result provides an upper bound on the suboptimality of our moment matching approach. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3 (Suboptimality of moment matching-based PEVI algorithms). For any policy $\\pi_{M M}$ derived with $\\mathbb{A}_{P E V I}(\\widetilde{\\mathbb{B}}_{\\pi}^{\\Gamma}Q,\\widehat{\\mathbb{P}})$ learned by a penalization algorithm that uses moment matching to approximate the Bell man t a rget with respect to an action-value network defined as an $L-$ layer MLP with 1-Lipschitz activation functions, the following inequality holds ", "page_idx": 6}, {"type": "equation", "text": "$$\nS u b O p t(\\pi_{M M})\\leq2H\\sum_{l=2}^{L}\\left(G(\\widetilde{X}_{l-1})+C_{l-1}\\right)\\prod_{j=l}^{L}\\lVert A_{j}\\rVert.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The bound in Theorem 3 is much tighter than Theorem 2 in practice as $R_{\\operatorname*{max}}^{2}/(1-\\gamma)^{2}$ is very large while the Lipschitz continuities $\\|A_{j}\\|$ of the two-layer MLPs used in the experiments are in the order of low hundreds according to empirical investigations (Khromov and Singh, 2024). Another favorable property of Theorem 3 is that its statement is exact, as opposed to the probabilistic statement made in Theorem 2. The provable efficiency of MOMBO could be of independent interest for safety-critical use cases of offline reinforcement learning where the availability of analytical performance guarantees is a fundamental requirement. ", "page_idx": 6}, {"type": "text", "text": "4.2 Implementation details of MOMBO ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We adopt the model learning scheme from Model-Based Policy Optimization (MBPO) (Janner et al., 2019) and use SAC (Haarnoja et al., 2018) as the policy search algorithm. These approaches represent the best practices in many recent model-based offline reinforcement learning methods (Yu et al., 2020, 2021; Sun et al., 2023). However, we note that most of our findings are more broadly applicable. Following MBPO, we train an independent heteroscedastic neural ensemble of transition kernels modeled as Gaussian distributions over the next state and reward. We denote each ensemble element as $\\widehat{\\mathrm{P}}_{n}(s^{\\prime},r|s,a)$ for $n\\in\\{1,\\ldots,N_{\\mathrm{ens}}\\}$ . After evaluating the performance of each model on a validatio n  set, we select the $N_{\\mathrm{elite}}$ best-performing ensemble elements for further processing. We use these elite models to generate $k$ -step rollouts with the current policy and to create the synthetic dataset $\\widehat{\\mathcal{D}}$ , which we then combine with the real observations $\\mathcal{D}$ . We retain the mean and variance of the pre di ctive distribution for the next state to propagate it through the action-value function while assigning zero variance to the real tuples. ", "page_idx": 6}, {"type": "text", "text": "The lower confidence bound given in Equation (1) builds on our MDP definition, which assumes a deterministic reward function and a deterministic policy for illustrative purposes. In our implementation, the reward model also follows a heteroscedastic ensemble of normal distributions. We also incorporate the uncertainty around the predicted reward of a synthetic sample into the Bellman target calculation. Furthermore, our policy function is a squashed Gaussian distribution trained by SAC in the maximum entropy reinforcement learning setting. For further details, refer to the implementation. ", "page_idx": 6}, {"type": "table", "img_path": "OFmclNhp0y/tmp/d7504ab8d0a2a5e1e3a2d973351f3bfee3e5e3fb21041dd8c1009d2a286adb4d.jpg", "table_caption": ["Table 1: Performance evaluation on the D4RL dataset. Normalized reward at 3M gradient steps and Area Under the Learning Curve (AULC) (mean $\\pm$ std) scores are averaged across four repetitions for the MuJoCo domain of the D4RL offline reinforcement learning dataset. The highest means are highlighted in bold and are underlined if they fall within one standard deviation of the best score. The average normalized score is the average across all tasks. The average ranking is based on the rank of the mean. "], "table_footnote": ["\u2020High standard deviation due to failure in one repetition, which can be mitigated by increasing \u03b2. Median result: 31.3 "], "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We compare MOMBO against MOPO and MOBILE, the two representative PEVI variants, across twelve tasks from the D4RL dataset (Fu et al., 2020), which consists of data collected from three MuJoCo environments (halfcheetah, hopper, walker2d) with behavior policies exhibiting four degrees of expertise (random, medium, medium-replay, and medium-expert). We use the datasets collected with \u2018v2\u2019 versions of the MuJoCo environments to be commensurate with the state of the art. We evaluate model performance using two scores: ", "page_idx": 7}, {"type": "text", "text": "(i) Normalized Reward: Total episode reward collected in evaluation mode after offline training ends, normalized by the performances of random and expert policies.   \n(ii) Area Under Learning Curve (AULC): Average normalized reward computed at the intermediate steps of training. ", "page_idx": 7}, {"type": "text", "text": "AULC indicates how fast a model converges to its final performance. A higher AULC reflects more efficient learning other things being equal. We report the main results in Table 1 and provide the learning curves of the halfcheetah environment in Figure 2 for visual investigation. We present the learning curves for the remaining tasks in Figure 4. We obtain the results for the baseline models from the log files provided by the OfflineRL library (Sun, 2023). We implement MOMBO into the MOBILE (Sun et al., 2023) code base shared by its authors and use their experiment configurations wherever applicable. See Appendix C for details. The source code of our algorithm is available at https://github.com/adinlab/MOMBO. The OfflineRL library does not contain the log files for the random datasets for MOPO at the time of writing. We replicate these experiments using the MOBILE code based on the prescribed configurations. ", "page_idx": 7}, {"type": "text", "text": "Theorem 1 indicates that the performance of a PEVI algorithm depends on how tightly its reward penalizer $\\Gamma_{\\widehat{\\mathrm{P}}}^{Q}(s,a)$ upper bounds the Bellman operator error $|\\mathbb{B}_{\\pi}\\overset{\\cdot}{Q}(s,a)\\rvert-\\overset{\\textstyle\\bigwedge}{\\mathbb{B}}_{\\pi}\\overset{\\textstyle\\bigwedge}{Q}(s,a)|$ . We use this theoreti cal result to compare the quality of the reward penalizers of the  three models based on average values of the following two performance scores calculated across data points observed during 10 evaluation episodes: ", "page_idx": 7}, {"type": "image", "img_path": "OFmclNhp0y/tmp/885c18b8d209b69ae60293cb2e88c4259810c3f8487f2dcb420341308df0eb7b.jpg", "img_caption": ["Figure 2: Evaluation results on halfcheetah for four settings. The dashed, dotted, and solid curves represent the mean of the normalized rewards across ten evaluation episodes and four random seeds. The shaded area indicates one standard deviation from the mean. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "OFmclNhp0y/tmp/404ef8ed26ef0bd41a5da4b7d5925e4a5091726da4ed60d0c600e35ac2401630.jpg", "table_caption": ["Table 2: Uncertainty quantification on the D4RL dataset. Accuracy and tightness (mean\u00b1std) scores are averaged across four repetitions. The highest means are highlighted in bold and are underlined if they fall within one standard deviation of the best score. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "(i) Accuracy: $\\begin{array}{r}{\\mathbb{1}(\\Gamma_{\\widehat{\\mathrm{P}}}^{Q}(s,a)\\ge|\\mathbb{B}_{\\pi}Q(s,a)-\\widehat{\\mathbb{B}}_{\\pi}Q(s,a)|)}\\end{array}$ for the indicator function 1, i.e., how often the reward  penalizer is a valid $\\xi$ \u2212u ncertainty quantifier as assumed by Theorem 1. (ii) Tightness: $\\Gamma_{\\widehat{\\mathrm{P}}}^{Q}(s,a)-|\\mathbb{B}_{\\pi}Q(s,a)-\\widehat{\\mathbb{B}}_{\\pi}Q(s,a)|$ , i.e., how sharp a $\\xi-$ uncertainty quantifier the reward pe nalizer is. ", "page_idx": 8}, {"type": "text", "text": "Table 2 shows that MOMBO provides more precise uncertainty estimates compared to the samplingbased approaches. It also indicates that MOMBO provides tighter estimates of the Bellman operator error, meaning that the sampling-based approaches use larger confidence radii. See Appendix C.1.2 for details. ", "page_idx": 8}, {"type": "text", "text": "The D4RL dataset is a heavily studied benchmark database where many hyperparameters, such as penalty coefficients, are tuned by trial and error. We argue that this is not feasible in a realistic offilne reinforcement learning setting where the central assumption is that policy search needs to be performed without real environment interactions. Furthermore, it is more realistic to assume that collecting data from expert policies is more expensive than random exploration. One would then need to perform offline reinforcement learning on a dataset that comprises observations obtained at different expertise levels. The Mixed offline reinforcement learning dataset (Hong et al., 2023) satisfies both desiderata, as the baseline models are not engineered for its setting and its tasks comprise data from two policies: an expert or medium policy and a random policy, presented in varying proportions. We compare MOMBO against MOBILE and MOPO on this dataset for a fixed and shared penalty coefficient for all models. We find that MOMBO outperforms both baselines in final performance and learning speed in this more realistic setup. See Appendix C.1.3 and Table 3 for further details and results. ", "page_idx": 8}, {"type": "text", "text": "Our key experimental findings can be summarized as follows: ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "(i) MOMBO converges faster and trains more stably. It learns more rapidly and reaches its final performance earlier as visible from the AULC scores. Its moment matching approach provides more informative gradient signals and better estimates of the first two moments of the Bellman target compared to sampling-based approaches, which suffer from high estimator variance caused by Monte Carlo sampling. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "(ii) MOMBO delivers a competitive final training performance. It ranks highest across all tasks and outperforms other model-based offline reinforcement learning approaches, including COMBO (Yu et al., 2021) with an average normalized reward of 66.8, TT (Janner et al., 2021) with 62.3, and RAMBO (Rigter et al., 2022) with 67.7, all evaluated on the same twelve tasks in the D4RL dataset. We took these scores from Sun et al. (2023).   \n(iii) MOMBO delivers superior final training performance when expert data is limited. MOMBO outperforms the baselines (at $1\\%$ in the Mixed dataset) in both AULC and normalized reward. For normalized reward, MOMBO achieves 37.0, compared to 32.4 for MOBILE and 27.9 for MOPO, averaged across all tasks at $1\\%$ . In terms of AULC, MOMBO attains 29.5, outperforming the 28.0 of MOBILE and the 23.5 of MOPO. See Table 3 for details.   \n(iv) MOMBO provides more precise estimates of Bellman target uncertainty. It outperforms both baselines in accuracy for 9 out of 12 tasks and in tightness all 12 tasks in the D4RL dataset. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The main objective of MOMBO is to accurately quantify the uncertainty surrounding a Bellman target estimate caused by the error in predicting the next state. We achieve this by deterministically propagating uncertainties through the value function with moment matching and introducing pessimism by constructing a lower confidence bound on the target Q-values. We analyze our model theoretically and evaluate its performance in various environments. Our findings may lay the groundwork for further theoretical analysis of model-based reinforcement learning algorithms in continuous state-action spaces. Our algorithmic contributions can also be instrumental in both model-based online and model-free offilne reinforcement learning setups (An et al., 2021; Bai et al., 2022). ", "page_idx": 9}, {"type": "text", "text": "Limitations. The accuracy of the learned environment models sets a bottleneck on the performance of MOMBO. The choice of the confidence radius $\\beta$ is another decisive factor in model performance. MOMBO shares these weaknesses with the other state-of-the-art model-based offline reinforcement learning methods (Yu et al., 2020; Lu et al., 2021; Sun et al., 2023) and does not contribute to their mitigation. Furthermore, our theoretical analyses rely on the assumption of normally distributed Q-values, which may be improved with heavy-tailed assumed densities. Considering the bounds, a limitation is that we claim a tighter bound compared to a baseline bound that we derived ourselves. However, the most important factor in that bound, $R_{\\operatorname*{max}}^{2}/(1-\\gamma)^{2}$ , arises from Hoeffding\u2019s inequality, i.e., a classical statement. As such, we assume our bound to be rigorous and not inadvertently biased. Finally, our moment matching method is limited to activation functions for which the first two moments are either analytically tractable or can be approximated with sufficient accuracy, which includes all commonly used activation functions. Extensions to transformations such as, e.g., BatchNorm (Ioffe and Szegedy, 2015), or other activation functions remove the analytical tractability for moment matching. However, these are usually not used in our specific applications. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "AA and MH thank the Carlsberg Foundation for supporting their research under the grant number CF21-0250. We are grateful to Birgit Debrabant for her valuable inputs in the development of some theoretical aspects. We also thank all reviewers and the area chair for improving the quality of our work by their thorough reviews and active discussion during the rebuttal phase. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Abbas, Z., Sokota, S., Talvitie, E., and White, M. (2020). Selective Dyna-style planning under limited model capacity. In International Conference on Machine Learning.   \nAn, G., Moon, S., Kim, J.-H., and Song, H. O. (2021). Uncertainty-based offline reinforcement learning with diversified Q-ensemble. In Advances in Neural Information Processing Systems.   \nBai, C., Wang, L., Yang, Z., Deng, Z., Garg, A., Liu, P., and Wang, Z. (2022). Pessimistic bootstrapping for uncertainty-driven offline reinforcement learning. In International Conference on Learning Representations.   \nCasella, G. and Berger, R. (2024). Statistical inference. CRC Press.   \nCetin, E., Tirinzoni, A., Pirotta, M., Lazaric, A., Ollivier, Y., and Touati, A. (2024). Simple ingredients for offline reinforcement learning. In International Conference on Machine Learning.   \nCheng, C.-A., Xie, T., Jiang, N., and Agarwal, A. (2022). Adversarially trained actor critic for offline reinforcement learning. In International Conference on Machine Learning.   \nChhachhi, S. and Teng, F. (2023). On the 1-wasserstein distance between location-scale distributions and the effect of differential privacy. arXiv preprint arXiv:2304.14869.   \nFeinberg, V., Wan, A., Stoica, I., Jordan, M. I., Gonzalez, J. E., and Levine, S. (2018). Model-based value estimation for efficient model-free reinforcement learning. arXiv preprint arXiv:1803.00101.   \nFrey, B. J. and Hinton, G. E. (1999). Variational learning in nonlinear Gaussian belief networks. Neural Computation.   \nFu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S. (2020). D4RL: Datasets for deep data-driven reinforcement learning.   \nFujimoto, S. and Gu, S. S. (2021). A minimalist approach to offline reinforcement learning. In Advances in Neural Information Processing Systems.   \nFujimoto, S., Hoof, H., and Meger, D. (2018). Addressing function approximation error in actor-critic methods. In International Conference on Machine Learning.   \nFujimoto, S., Meger, D., and Precup, D. (2019). Off-policy deep reinforcement learning without exploration. In International Conference on Machine Learning.   \nGast, J. and Roth, S. (2018). Lightweight probabilistic deep networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.   \nHaarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., Kumar, V., Zhu, H., Gupta, A., Abbeel, P., et al. (2018). Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905.   \nHau\u00dfmann, M., Hamprecht, F. A., and Kandemir, M. (2019). Deep active learning with adaptive acquisition. In International Joint Conference on Artificial Intelligence.   \nHoeffding, W. (1963). Probability inequalities for sums of bounded random variables. Journal of the American Statistical Association.   \nHong, Z.-W., Agrawal, P., Combes, R. T. d., and Laroche, R. (2023). Harnessing mixed offline reinforcement learning datasets via trajectory weighting. In International Conference on Learning Representations.   \nHui, D. Y.-T., Courville, A. C., and Bacon, P.-L. (2023). Double Gumbel Q-learning. In Advances in Neural Information Processing Systems.   \nIoffe, S. and Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning.   \nJanner, M., Fu, J., Zhang, M., and Levine, S. (2019). When to trust your model: Model-based policy optimization. In Advances in Neural Information Processing Systems.   \nJanner, M., Li, Q., and Levine, S. (2021). Offline reinforcement learning as one big sequence modeling problem. In Advances in Neural Information Processing Systems.   \nJeong, J., Wang, X., Gimelfarb, M., Kim, H., Abdulhai, B., and Sanner, S. (2023). Conservative Bayesian model-based value expansion for offline policy optimization. In International Conference on Learning Representations.   \nJin, Y., Yang, Z., and Wang, Z. (2021). Is pessimism provably efficient for offline RL? In International Conference on Machine Learning.   \nKhromov, G. and Singh, S. P. (2024). Some fundamental aspects about Lipschitz continuity of neural networks. In International Conference on Learning Representations.   \nKidambi, R., Rajeswaran, A., Netrapalli, P., and Joachims, T. (2020). MOReL $:$ Model-based offline reinforcement learning. In Advances in Neural Information Processing Systems.   \nKingma, D. P. and Ba, J. L. (2015). Adam: A method for stochastic optimization. In International Conference on Learning Representations.   \nKostrikov, I., Fergus, R., Tompson, J., and Nachum, O. (2021). Offline reinforcement learning with Fisher divergence critic regularization. In International Conference on Machine Learning.   \nKumar, A., Fu, J., Soh, M., Tucker, G., and Levine, S. (2019). Stabilizing off-policy Q-learning via bootstrapping error reduction. In Advances in Neural Information Processing Systems.   \nKumar, A., Zhou, A., Tucker, G., and Levine, S. (2020). Conservative Q-learning for offline reinforcement learning. In Advances in Neural Information Processing Systems.   \nLakshminarayanan, B., Pritzel, A., and Blundell, C. (2017). Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in Neural Information Processing Systems.   \nLange, S., Gabel, T., and Riedmiller, M. (2012). Batch reinforcement learning. In Reinforcement learning: State-of-the-art. Springer.   \nLevine, S., Kumar, A., Tucker, G., and Fu, J. (2020). Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643.   \nLook, A., Kandemir, M., Rakitsch, B., and Peters, J. (2023). A deterministic approximation to neural SDEs. IEEE Transactions on Pattern Analysis and Machine Intelligence.   \nLu, C., Ball, P. J., Parker-Holder, J., Osborne, M. A., and Roberts, S. J. (2021). Revisiting design choices in offline model-based reinforcement learning. In International Conference on Learning Representations.   \nLuis, C. E., Bottero, A. G., Vinogradska, J., Berkenkamp, F., and Peters, J. (2023). Model-based uncertainty in value functions. In International Conference on Artificial Intelligence and Statistics.   \nMicheli, V., Alonso, E., and Fleuret, F. (2022). Transformers are sample-efficient world models. In International Conference on Learning Representations.   \nNie, X., Brunskill, E., and Wager, S. (2021). Learning when-to-treat policies. Journal of the American Statistical Association.   \nO\u2019Donoghue, B., Osband, I., Munos, R., and Mnih, V. (2018). The uncertainty Bellman equation and exploration. In International Conference on Machine Learning.   \nPeng, X. B., Kumar, A., Zhang, G., and Levine, S. (2019). Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177.   \nPitcan, Y. (2017). A note on concentration inequalities for U-statistics. arXiv preprint arXiv:1712.06160.   \nRigter, M., Lacerda, B., and Hawes, N. (2022). RAMBO-RL: Robust adversarial model-based offline reinforcement learning. In Advances in Neural Information Processing Systems.   \nShortreed, S. M., Laber, E., Lizotte, D. J., Stroup, T. S., Pineau, J., and Murphy, S. A. (2011). Informing sequential clinical decision-making through reinforcement learning: an empirical study. Machine learning.   \nSiegel, N. Y., Springenberg, J. T., Berkenkamp, F., Abdolmaleki, A., Neunert, M., Lampe, T., Hafner, R., Heess, N., and Riedmiller, M. (2020). Keep doing what worked: Behavior modelling priors for offline reinforcement learning. In International Conference on Learning Representations.   \nSingh, A., Yu, A., Yang, J., Zhang, J., Kumar, A., and Levine, S. (2020). COG: Connecting new skills to past experience with offline reinforcement learning. In Conference on Robot Learning.   \nSun, Y. (2023). Offlinerl-kit: An elegant pytorch offline reinforcement learning library. https://github .com/yihaosun1124/OfflineRL-Kit.   \nSun, Y., Zhang, J., Jia, C., Lin, H., Ye, J., and Yu, Y. (2023). Model-Bellman inconsistency for model-based offline reinforcement learning. In International Conference on Machine Learning.   \nSutton, R. S. (1990). Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In Machine Learning Proceedings 1990. Morgan Kaufmann.   \nThrun, S. and Schwartz, A. (1993). Issues in using function approximation for reinforcement learning. In Proceedings of the 1993 Connectionist Models Summer School.   \nTodorov, E., Erez, T., and Tassa, Y. (2012). MuJoCo: A physics engine for model-based control. In IEEE/RSJ International Conference on Intelligent Robots and Systems.   \nUehara, M. and Sun, W. (2022). Pessimistic model-based offline reinforcement learning under partial coverage. In International Conference on Learning Representations.   \nVillani, C. et al. (2009). Optimal transport: old and new. Springer.   \nWang, S. and Manning, C. (2013). Fast dropout training. In International Conference on Machine Learning.   \nWasserman, L. (2004). All of Statistics. Springer New York.   \nWright, O., Nakahira, Y., and Moura, J. M. (2024). An analytic solution to covariance propagation in neural networks. In International Conference on Artificial Intelligence and Statistics.   \nWu, A., Nowozin, S., Meeds, E., Turner, R. E., Hernandez-Lobato, J. M., and Gaunt, A. L. (2019a). Deterministic variational inference for robust Bayesian neural networks. In International Conference on Learning Representations.   \nWu, Y., Tucker, G., and Nachum, O. (2019b). Behavior regularized offline reinforcement learning. arXiv preprint arXiv:1911.11361.   \nXie, T., Cheng, C.-A., Jiang, N., Mineiro, P., and Agarwal, A. (2021). Bellman-consistent pessimism for offline reinforcement learning. In Advances in Neural Information Processing Systems.   \nYu, T., Kumar, A., Rafailov, R., Rajeswaran, A., Levine, S., and Finn, C. (2021). COMBO: Conservative offline model-based policy optimization. In Advances in Neural Information Processing Systems.   \nYu, T., Thomas, G., Yu, L., Ermon, S., Zou, J. Y., Levine, S., Finn, C., and Ma, T. (2020). MOPO: Model-based offline policy optimization. In Advances in Neural Information Processing Systems. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "APPENDIX ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Related work ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Offline reinforcement learning. The goal of offilne reinforcement learning is to derive an optimal policy from fixed datasets collected through interactions with a behavior policy in the environment. This approach is particularly relevant in scenarios where direct interaction with the environment is costly or poses potential risks. The applications of offline reinforcement learning span various domains, including robotics (Singh et al., 2020; Micheli et al., 2022) and healthcare (Shortreed et al., 2011; Nie et al., 2021). Notably, applying off-policy reinforcement learning algorithms directly to offline reinforcement learning settings often fails due to issues such as overestimation bias and distributional shift. Research in offline reinforcement learning has evolved along two main avenues: model-free and model-based approaches. ", "page_idx": 13}, {"type": "text", "text": "Model-based offline reinforcement learning. Dyna-style model-based reinforcement learning (Sutton, 1990; Janner et al., 2019) algorithms employ a environment model to simulate the true transition model, generating a synthetic dataset that enhances sample efficiency and serves as a surrogate environment for interaction. Model-based offline reinforcement learning methods vary in their approaches to applying conservatism using the environment model. For instance, MOPO (Yu et al., 2020) and MORel (Kidambi et al., 2020) learn a pessimistic value function by penalizing the MDP generated by the environment model, with rewards being penalized based on different uncertainty measures of the environment model. COMBO (Yu et al., 2021) is a Dyna-style model-based approach derived from conservative Q-learning (Kumar et al., 2020). RAMBO (Rigter et al., 2022) introduces conservatism by adversarially training the environment model to minimize the value function, thus ensuring accurate predictions. MOBILE (Sun et al., 2023) penalizes the value function based on the uncertainty of the Bellman operator through sampling. CBOP (Jeong et al., 2023) introduces conservatism by applying a lower confidence bound to the value function with an adaptive weighting of $h$ -step returns in the model-based value expansion framework (Feinberg et al., 2018). ", "page_idx": 13}, {"type": "text", "text": "Uncertainty-driven offline reinforcement learning approaches apply penalties in various ways. As model-free methods, EDAC (An et al., 2021) applies penalties based on ensemble similarities, while PBRL (Bai et al., 2022) penalizes based on the disagreement among bootstrapped Qfunctions. Uncertainty-driven model-based offline reinforcement learning algorithms adhere to the meta-algorithm known as pessimistic value iteration (Jin et al., 2021). This meta-algorithm introduces pessimism through a $\\xi$ -uncertainty quantifier to minimize the Bellman approximation error. MOPO (Yu et al., 2020) and MOBILE (Sun et al., 2023) can be seen as practical implementations of this meta-algorithm. MOPO leverages prediction uncertainty from the environment model in various ways, as explored by Lu et al. (2021), while MOBILE uses uncertainty in the value function for the synthetic dataset through sampling. MOMBO is also based on PEVI, and our $\\xi.$ -uncertainty quantifier is derived from the uncertainty of the Bellman target value, which does not rely on sampling. Instead, it directly propagates uncertainty from the environment models to the value function, thanks to moment matching. ", "page_idx": 13}, {"type": "text", "text": "Model-free offline reinforcement learning algorithms can be broadly categorized into two groups: policy regularization and value regularization. Policy regularization methods aim to constrain the learned policy to prevent deviations from the behavior policy (Fujimoto et al., 2019; Kumar et al., 2019; Wu et al., 2019b; Peng et al., 2019; Siegel et al., 2020). For instance, during training, TD3-BC (Fujimoto and Gu, 2021) incorporates a behavior cloning term to regulate its policy. On the other hand, value regularization methods (Kostrikov et al., 2021; Xie et al., 2021; Cheng et al., 2022) introduce conservatism during the value optimization phase using various approaches. For example, CQL (Kumar et al., 2020) penalizes Q-values associated with out-of-distribution actions to avoid overestimation. Similarly, EDAC (An et al., 2021), which employs ensemble value networks, applies a similar penalization strategy based on uncertainty measures over Q-values. ", "page_idx": 13}, {"type": "text", "text": "Uncertainty propagation via various moment matching-based approaches has been wellresearched, primarily in the area of Bayesian deep learning (Frey and Hinton, 1999; Wang and Manning, 2013; Wu et al., 2019a), with applications, e.g., in active learning (Hau\u00dfmann et al., 2019) and computer vision (Gast and Roth, 2018). ", "page_idx": 13}, {"type": "text", "text": "B Theoretical analysis ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "This section contains proofs for all the theoretical statements made in the main text. ", "page_idx": 14}, {"type": "text", "text": "B.1 Prerequisites ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We first restate and extend the following result on moment matching with the ReLU activation function. Our focus on the ReLU activation throughout this work is due to the fact that it is currently the most commonly used activation function in deep reinforcement learning. One could derive similar results for any piecewise linear activation function and several others as well. ", "page_idx": 14}, {"type": "text", "text": "Lemma 1 (Moment matching). For $X\\sim{\\mathcal{N}}(X|\\mu,\\sigma^{2})$ and $Y=\\operatorname*{max}(0,X)$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}{(i)}&{{}\\widetilde{\\mu}\\triangleq\\mathbb{E}\\left[Y\\right]=\\mu\\Phi(\\alpha)+\\sigma\\phi(\\alpha),}&{}&{{}\\widetilde{\\sigma}^{2}\\triangleq\\mathrm{var}\\left[Y\\right]=(\\mu^{2}+\\sigma^{2})\\Phi(\\alpha)+\\mu\\sigma\\phi(\\alpha)-\\widetilde{\\mu}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\alpha=\\mu/\\sigma$ , and $\\phi(\\cdot),\\,\\Phi(\\cdot)$ are the probabi lity density function (pdf) and cumulative distri bution function (cdf) of the standard normal distribution, respectively. Additionally, we have that ", "page_idx": 14}, {"type": "equation", "text": "$$\n(i i)\\quad{\\widetilde{\\mu}}\\geq\\mu\\quad a n d\\quad(i i i)\\quad{\\widetilde{\\sigma}}^{2}\\leq\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof of Lemma $^{l}$ . See Frey and Hinton (1999) for the derivation of $(i)$ , i.e., $\\widetilde{\\mu}$ and $\\widetilde{\\sigma}^{2}$ . Statement $(i i)$ holds as ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\tilde{\\mu}}=\\mathbb{E}\\left[Y\\right]=\\int_{0}^{\\infty}x p(x)d y\\geq\\int_{-\\infty}^{0}x p(x)d x+\\int_{0}^{\\infty}x p(x)d x=\\mu.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[Y^{2}\\right]=\\int_{0}^{\\infty}x^{2}p(x)d x\\le\\int_{-\\infty}^{0}x^{2}p(x)d x+\\int_{0}^{\\infty}x^{2}p(x)d x=\\mathbb{E}\\left[X^{2}\\right].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, we get $(i i i)$ by ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\widetilde{\\sigma}^{2}=\\mathrm{var}\\left[Y\\right]=\\mathbb{E}\\left[Y^{2}\\right]-\\widetilde{\\mu}^{2}\\leq\\mathbb{E}\\left[X^{2}\\right]-\\mu^{2}=\\sigma^{2}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "concluding the proof. ", "page_idx": 14}, {"type": "text", "text": "Although we do not require the following lemma in our final results, a helpful result is the following inequality between the cdfs of two normally distributed random variables $X$ and its matched counterpart $\\widetilde{X}$ . ", "page_idx": 14}, {"type": "text", "text": "Lemma 4  ( $A n$ inequality between normal cdfs). For two normally distributed random variables $X\\sim{\\mathcal{N}}(X|\\mu,\\sigma^{2})$ and $\\widetilde{X}\\sim\\mathcal N(\\widetilde{X}|\\widetilde{\\mu},\\widetilde{\\sigma}^{2})$ with ${\\widetilde{\\mu}}\\sigma\\geq\\mu{\\widetilde{\\sigma}}.$ , the following inequality holds: ", "page_idx": 14}, {"type": "equation", "text": "$$\nF_{\\widetilde{X}}(u)\\leq F_{X}(u)\\qquad f o r\\,u\\leq0.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Given $\\widetilde{\\mu}$ and $\\widetilde{\\sigma}^{2}$ as in Lemma 1, the assumptions of this lemma hold and it is applicable to our mome n t matc h ing propagation. ", "page_idx": 14}, {"type": "text", "text": "Proof of Lemma 4. Assume $u\\leq0$ . We have that ", "page_idx": 14}, {"type": "equation", "text": "$$\nF_{{\\widetilde{X}}}(u)\\leq F_{X}(u)\\quad\\Leftrightarrow\\quad\\Phi\\Big(\\frac{u-\\widetilde{\\mu}}{\\widetilde{\\sigma}}\\Big)\\leq\\Phi\\Big(\\frac{u-\\mu}{\\sigma}\\Big).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "As it is a monotonically increasing function, this in turn  is equivalent to ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{u-\\widetilde{\\mu}}{\\widetilde{\\sigma}}\\leq\\frac{u-\\mu}{\\sigma}}&{\\:\\Leftrightarrow u\\sigma-\\widetilde{\\mu}\\sigma\\leq u\\widetilde{\\sigma}-\\mu\\widetilde{\\sigma}}\\\\ &{\\:\\Leftrightarrow u(\\sigma-\\widetilde{\\sigma})\\leq\\widetilde{\\mu}\\sigma-\\mu\\widetilde{\\sigma}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which holds as $u(\\sigma-\\tilde{\\sigma})<0$ via Lemma $1$ for $u<0$ , and $\\widetilde{\\mu}\\sigma\\geq\\mu\\widetilde{\\sigma}$ by  assumption. ", "page_idx": 14}, {"type": "text", "text": "We provide the following definition, as we derive our bounds using Wasserstein distances. ", "page_idx": 14}, {"type": "text", "text": "Definition 1 (Wasserstein distance). Let $(M,d)$ denote a metric space, and let $p\\,\\in\\,[1,\\infty]$ . The $p$ -Wasserstein distance between two probability measures $\\rho_{1}$ and $\\rho_{2}$ on $M$ , with finite $p$ -moments, is defined as ", "page_idx": 15}, {"type": "equation", "text": "$$\nW_{p}(\\rho_{1},\\rho_{2})\\triangleq\\operatorname*{inf}_{\\gamma\\in\\chi(\\rho_{1},\\rho_{2})}\\left(\\mathbb{E}_{(x,y)\\sim\\gamma}\\left[d(x,y)^{p}\\right]\\right)^{1/p}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\chi(\\cdot,\\cdot)$ represents the set of all couplings of two measures, $\\rho_{1}$ and $\\rho_{2}$ . ", "page_idx": 15}, {"type": "text", "text": "For $p=1$ , the 1-Wasserstein distance, $W_{1}$ , can be simplified (see, e.g., Villani et al., 2009) to ", "page_idx": 15}, {"type": "equation", "text": "$$\nW_{1}(\\rho_{1},\\rho_{2})=\\int_{\\mathbb{R}}|F_{1}(x)-F_{2}(x)|d x\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\rho_{1},\\rho_{2}$ are two probability measures on $\\mathbb{R}$ , and $F_{1}(\\cdot)$ and $F_{2}(\\cdot)$ are their respective cdfs. ", "page_idx": 15}, {"type": "text", "text": "Given this definition, we prove the following two bounds on $W_{1}$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma 5 (Wasserstein inequalities). The following inequalities hold for the 1-Wasserstein metric ", "page_idx": 15}, {"type": "equation", "text": "$$\nW_{1}(\\rho_{U}^{1},\\rho_{U}^{2})\\leq\\|A\\|W_{1}(\\rho_{V}^{1},\\rho_{V}^{2}),f o r\\,U=A V+b.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\rho_{x}$ is the density of $x$ , $U\\in\\mathbb{R}^{d_{u}}$ , $V\\in\\mathbb{R}^{d_{v}}$ , $A\\in\\mathbb{R}^{d_{u}\\times d_{v}}$ , and $\\boldsymbol{b}\\in\\mathbb{R}^{d_{u}}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma $^{5}$ . Given the Kantorovich-Rubinstein duality (Villani et al., 2009) for $W_{1}$ we have ", "page_idx": 15}, {"type": "equation", "text": "$$\nW_{1}(\\mu,\\nu)=\\frac{1}{K}\\operatorname*{sup}_{\\operatorname*{lip}(f)\\leq K}\\mathbb{E}_{x\\sim\\mu}\\left[f(x)\\right]-\\mathbb{E}_{y\\sim\\nu}\\left[f(y)\\right]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for any two probability measures $\\mu,\\,\\nu$ , and where $\\operatorname{lip}(f)<K$ specifies the family of $K$ -Lipschitz functions. ", "page_idx": 15}, {"type": "text", "text": "We use this to first prove $(i i)$ . For $Z=g(U)$ we have that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{1}(\\rho_{Z}^{1},\\rho_{Z}^{2})=\\underset{\\mathrm{lip}(f)\\leq1}{\\operatorname*{sup}}\\ \\mathbb{E}_{z\\sim\\rho_{Z}^{1}}\\left[f(z)\\right]-\\mathbb{E}_{z\\sim\\rho_{Z}^{2}}\\left[f(z)\\right]}\\\\ &{\\qquad\\qquad=\\underset{\\mathrm{lip}(f)\\leq1}{\\operatorname*{sup}}\\mathbb{E}_{u\\sim\\rho_{U}^{1}}\\left[f(g(u))\\right]-\\mathbb{E}_{u\\sim\\rho_{U}^{2}}\\left[f(g(u))\\right],}\\\\ &{\\qquad\\qquad\\leq\\underset{\\mathrm{lip}(h)\\leq1}{\\operatorname*{sup}}\\mathbb{E}_{u\\sim\\rho_{U}^{1}}\\left[h(u)\\right]-\\mathbb{E}_{u\\sim\\rho_{U}^{2}}\\left[h(u)\\right]=W_{1}(\\rho_{U}^{1},\\rho_{U}^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the inequality holds as $f\\circ g$ has a Lipschitz constant $L\\leq1$ , i.e., we end up with a supremum over a smaller class of Lipschitz functions which we revert in the inequality. This gives us the desired inequality. ", "page_idx": 15}, {"type": "text", "text": "To prove $(i)$ we use that for $\\|A\\|\\leq1$ , the transformation $g(V)\\triangleq1$ is 1-Lipschitz and the result follows via $(i i)$ . For $\\|A\\|>1$ we use that $g(\\cdot)$ is $K$ -Lipschitz and the result follows by adapting the proof accordingly. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Finally, the following lemma allows us to relate the 1-Wasserstein distance of two transformed distributions to an absolute difference between the respective transforming functions. ", "page_idx": 15}, {"type": "text", "text": "Lemma 6 (Wasserstein to suboptimality). Consider two probability distributions $P_{x},P_{u}$ defined on a measurable space $(S,\\sigma(S))$ and two transformed random variables $y\\ =\\ f(x)\\ \\sim\\ P_{y}$ (with $x\\sim P_{x}.$ ), and $z=g(u)\\sim P_{z}$ (with $u\\sim P_{u},$ ) for functions $f,g:S\\rightarrow\\mathbb{R}.$ . If ", "page_idx": 15}, {"type": "equation", "text": "$$\nW_{1}(P_{y},P_{z})\\leq C\\qquad t h e n\\quad|\\mathbb{E}_{x\\sim P_{x}}\\left[f(x)\\right]-\\mathbb{E}_{u\\sim P_{u}}\\left[g(u)\\right]|\\leq C\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for any constant $C\\geq0$ . ", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma 6. Assume that $W_{1}(P_{y},P_{z})\\,\\le\\,C$ for some $C\\geq0$ . The Kantorovich-Rubinstein duality (Villani et al., 2009), with $K=1$ gives us ", "page_idx": 15}, {"type": "equation", "text": "$$\nW_{1}(P_{y},P_{z})=\\operatorname*{sup}_{\\operatorname*{lip}(h)\\leq1}\\mathbb{E}_{y\\sim P_{y}}\\left[h(y)\\right]-\\mathbb{E}_{z\\sim P_{z}}\\left[h(z)\\right]\\leq C.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "As the identity function $\\lambda:x\\mapsto x$ is 1-Lipschitz, we can lower bound the supremum further with ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}_{y\\sim P_{y}}\\left[y\\right]-\\mathbb{E}_{z\\sim P_{z}}\\left[z\\right]\\leq\\operatorname*{sup}_{\\mathrm{lip}(h)\\leq1}\\mathbb{E}_{y\\sim P_{y}}\\left[h(y)\\right]-\\mathbb{E}_{z\\sim P_{z}}\\left[h(z)\\right]\\leq C.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Using what is colloquially known as the law of the unconscious statistician (Casella and Berger, 2024) , we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}_{y\\sim P_{y}}\\left[y\\right]=\\mathbb{E}_{x\\sim P_{x}}\\left[f(x)\\right]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and analogously for $\\mathbb{E}_{z\\sim P_{z}}\\left[z\\right]$ . Therefore ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname{\\mathbb{E}}_{y\\sim P_{y}}\\left[y\\right]-\\operatorname{\\mathbb{E}}_{z\\sim P_{z}}\\left[z\\right]=\\operatorname{\\mathbb{E}}_{x\\sim P_{x}}\\left[f(x)\\right]-\\operatorname{\\mathbb{E}}_{u\\sim P_{u}}\\left[g(u)\\right]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which gives us ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{x\\sim P_{x}}\\left[f(x)\\right]-\\mathbb{E}_{u\\sim P_{u}}\\left[g(u)\\right]\\leq C.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "As $W_{1}$ is symmetric, we can follow the same argument and additionally have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{u\\sim P_{u}}\\left[g(u)\\right]-\\mathbb{E}_{x\\sim P_{x}}\\left[f(x)\\right]\\leq C.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Using that for $x\\in\\mathbb{R}$ with $x<C$ and $-x<C$ it follows that $|x|<C$ , we can combine these two inequalities and get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{x\\sim P_{x}}\\left[f(x)\\right]-\\mathbb{E}_{u\\sim P_{u}}\\left[g(u)\\right]\\right|\\leq C\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "as desired. ", "page_idx": 16}, {"type": "text", "text": "B.2 Proof of theorem 2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proving Theorem 2 requires the following upper $W_{1}$ bound for an MLP with a 1-Lipschitz activation function. ", "page_idx": 16}, {"type": "text", "text": "Lemma 7 ( $W_{1}$ bound on an $M L P$ ). Consider an $L-$ layer MLP $f(\\cdot)$ ", "page_idx": 16}, {"type": "equation", "text": "$$\nY_{l}=A_{l}^{\\top}X_{l-1}+b_{l},\\qquad X_{l}=\\operatorname{r}(Y_{l}),\\qquad l=1,\\ldots,L\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $A_{l},\\;b_{l}$ are the weight matrix and bias vector for layer $l$ , respectively, and $\\mathrm{r}(\\cdot)$ denotes an elementwise 1-Lipschitz activation function. We write $Y\\,\\triangleq\\,Y_{L}\\,=\\,f(X_{0}),$ , where $X_{0}$ is the input. Assuming two measures $X_{1}\\sim\\rho_{X}^{1}$ and $X_{2}\\sim\\rho_{X}^{2}$ , we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\nW_{1}(\\rho_{Y}^{1},\\rho_{Y}^{2})\\leq\\prod_{l=1}^{L}\\|A_{l}\\|W_{1}(\\rho_{X}^{1},\\rho_{X}^{2}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof of Lemma 7. We have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{W_{1}(\\rho_{Y}^{1},\\rho_{Y}^{2})\\leq\\|A_{L}\\|W_{1}(\\rho_{X_{L-1}}^{1},\\rho_{X_{L-1}}^{2}),}}&{\\mathrm{via\\,Lemma\\,5\\,(i)}}\\\\ &{}&{=\\|A_{L}\\|W_{1}\\big(\\rho_{h(Y_{L-1})}^{1},\\rho_{h(Y_{L-1})}^{2}\\big)\\leq\\|A_{L}\\|W_{1}\\big(\\rho_{Y_{L-1}}^{1},\\rho_{Y_{L-1}}^{2}\\big),}&{\\mathrm{via\\,Lemma\\,5\\,(ii)}}\\\\ &{}&{\\leq\\cdots\\leq\\displaystyle\\prod_{l=1}^{L}\\|A_{l}\\|W_{1}\\big(\\rho_{X}^{1},\\rho_{X}^{2}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The $W_{1}$ distance between two measures on the output distribution is upper bounded by the $W_{1}$ distance between the corresponding input measures with a factor given by the product of the norms of the weight matrices $A_{l}$ . This result allows us to provide a probabilistic bound on the $W_{1}$ distance between a normal $\\rho_{Y}$ and its empirical, sampling-based, estimate $\\hat{\\rho}_{Y}$ . ", "page_idx": 16}, {"type": "text", "text": "The following lemma allows us to extend this result to a probabilistic sampling-based upper bound. Lemma 8 (Sampling-based MLP bound). For an $L$ \u2212layer MLP $f(\\cdot)$ with 1-Lipschitz activation function, let $Y\\triangleq Y_{L}=f(X)$ where $X\\sim{\\mathcal{N}}(X|\\mu_{X},\\sigma_{X}^{2})$ , the following bound holds: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(W_{1}(\\rho_{Y},\\rho_{\\hat{Y}}^{N})\\leq\\prod_{l=1}^{L}\\lVert A_{l}\\rVert\\sqrt{-\\frac{8\\log(\\delta/4)R_{\\operatorname*{max}}^{2}}{\\lfloor N/2\\rfloor(1-\\gamma)^{2}}}\\right)\\geq1-\\delta\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\delta\\in(0,1)$ and $\\rho_{\\hat{Y}}^{N}$ is the empirical estimate given $N$ i.i.d. samples. ", "page_idx": 16}, {"type": "text", "text": "Proof of Lemma 8. With Hoeffding\u2019s inequality (Wasserman, 2004), we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname{P}\\left(\\left|\\hat{\\mu}_{N}-\\mu\\right|\\geq\\varepsilon\\right)\\leq2\\exp\\left(-2N^{2}\\varepsilon^{2}\\bigg/\\sum_{i=1}^{N}\\left(\\frac{R_{\\operatorname*{max}}}{1-\\gamma}-0\\right)^{2}\\right)=2\\exp\\left(-\\frac{2\\varepsilon^{2}N(1-\\gamma)^{2}}{R_{\\operatorname*{max}}^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for the empirical mean $\\hat{\\mu}_{N}$ of $Y$ . As variances are U-statistics with order $m\\ =\\ 2$ and kernel $h=\\textstyle{\\frac{1}{2}}({\\dot{x}}-y)^{2}$ using Hoeffding (1963); Pitcan (2017), we have for the empirical covariance of $Y,\\hat{\\sigma}_{N}^{2}$ , that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(\\hat{\\sigma}_{N}^{2}-\\sigma^{2}\\geq\\varepsilon)\\leq\\exp\\left(-\\frac{\\varepsilon^{2}\\lfloor N/m\\rfloor}{2\\|h\\|_{\\infty}^{2}}\\right)=\\exp\\left(-\\frac{\\varepsilon^{2}\\lfloor N/2\\rfloor(1-\\gamma)^{2}}{2R_{\\operatorname*{max}}^{2}}\\right)}\\\\ {\\Leftrightarrow\\,}&{\\mathbb{P}(\\hat{\\sigma}_{N}^{2}-\\sigma^{2}\\leq\\varepsilon)\\geq1-\\exp\\left(-\\frac{\\varepsilon^{2}\\lfloor N/2\\rfloor(1-\\gamma)^{2}}{2R_{\\operatorname*{max}}^{2}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\|\\cdot\\|_{\\infty}$ denotes $L^{\\infty}$ -norm. Applying the inequality $\\hat{\\sigma}_{N}\\le\\sqrt{\\sigma^{2}+\\varepsilon}\\le\\sigma+\\sqrt{\\varepsilon}$ , we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(\\hat{\\sigma}_{N}\\leq\\sigma+\\varepsilon)\\geq1-\\exp\\left(-\\frac{\\varepsilon^{2}\\lfloor N/2\\rfloor(1-\\gamma)^{2}}{2R_{\\operatorname*{max}}^{2}}\\right)}\\\\ {\\Leftrightarrow\\,}&{\\mathbb{P}(\\hat{\\sigma}_{N}-\\sigma\\geq\\varepsilon)\\leq\\exp\\left(-\\frac{\\varepsilon^{2}\\lfloor N/2\\rfloor(1-\\gamma)^{2}}{2R_{\\operatorname*{max}}^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "As the same inequality holds for $\\sigma-\\hat{\\sigma}_{N}\\geq\\varepsilon$ , a union bound give us that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\vert\\sigma-\\hat{\\sigma}_{n}\\vert\\ge\\varepsilon)\\le2\\exp\\left(-\\frac{\\varepsilon^{2}\\lfloor N/2\\rfloor(1-\\gamma)^{2}}{2R_{\\operatorname*{max}}^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Using an analytical upper bound (Chhachhi and Teng, 2023) on the 1-Wasserstein distance between two normal distributions, ", "page_idx": 17}, {"type": "equation", "text": "$$\nW_{1}(N(\\mu_{1},\\sigma_{1}^{2}),N(\\mu_{2},\\sigma_{2}^{2}))\\leq|\\mu_{1}-\\mu_{2}|+|\\sigma_{1}-\\sigma_{2}|\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}\\big(W_{1}(\\mathcal{N}(\\mu,\\sigma^{2}),\\mathcal{N}(\\hat{\\mu}_{N},\\hat{\\sigma}_{N}))\\geq2\\varepsilon\\big)}&{}\\\\ {\\leq\\mathbb{P}\\left(|\\mu-\\hat{\\mu}_{N}|+|\\sigma-\\hat{\\sigma}_{N}|\\geq2\\varepsilon\\right)}\\\\ {\\leq\\mathbb{P}(|\\hat{\\mu}_{N}-\\mu|\\geq\\varepsilon\\mathrm{~or~}|\\hat{\\sigma}_{N}-\\sigma|\\geq\\varepsilon)}\\\\ {\\leq\\mathbb{P}(|\\hat{\\mu}_{N}-\\mu|\\geq\\varepsilon)+\\mathbb{P}(|\\hat{\\sigma}_{N}-\\sigma|\\geq\\varepsilon),\\qquad\\qquad\\mathrm{union~bou~}}\\\\ {\\leq2\\exp(-2N c\\varepsilon^{2})+2\\exp(-0.5\\lfloor N/2\\rfloor c\\varepsilon^{2})}\\\\ {\\leq4\\exp(-0.5\\lfloor N/2\\rfloor c\\varepsilon^{2})\\triangleq\\delta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we use the shorthand notation $c=(1-\\gamma)^{2}/R_{\\mathrm{max}}^{2}$ . ", "page_idx": 17}, {"type": "text", "text": "Solving for $\\bar{\\varepsilon}=2\\varepsilon$ gives us ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\bar{\\varepsilon}=\\sqrt{-\\frac{8\\log(\\delta/4)}{\\lfloor{N/2}\\rfloor c}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The bound is then given as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(W_{1}\\left(\\mathcal{N}(\\mu,\\sigma^{2}),\\mathcal{N}(\\hat{\\mu}_{N},\\hat{\\sigma}_{N}^{2})\\right)\\leq\\sqrt{-\\frac{8\\log(\\delta/4)}{\\lfloor{N}/{2}\\rfloor c}}\\right)\\geq1-\\delta\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which gives us with Lemma 7, that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(W_{1}(\\rho_{Y},\\rho_{\\hat{Y}}^{N})\\leq\\prod_{l=1}^{L}\\lVert A_{l}\\rVert\\sqrt{-\\frac{8\\log(\\delta/4)R_{\\operatorname*{max}}^{2}}{\\lfloor N/2\\rfloor(1-\\gamma)^{2}}}\\right)\\geq1-\\delta.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Finally, applying this result to the relation in Lemma 6 allows us to prove the desired non-asymptotic bound on the performance of the sampling-based model. ", "page_idx": 17}, {"type": "text", "text": "Theorem 2 (Suboptimality of sampling-based PEVI algorithms). For any policy $\\pi_{M C}$ learned by $\\mathbb{A}_{P E V I}(\\widehat{\\mathbb{B}}_{\\pi}^{\\Gamma}Q,\\widehat{\\mathbb{P}})$ using $N$ Monte Carlo samples to approximate the Bellman target with respect to an actio n-val u e network defined as an $L-$ layer MLP with 1-Lipschitz activation functions, the following inequality holds for any error tolerance $\\delta\\in(0,1)$ with probability at least $1-\\delta$ ", "page_idx": 18}, {"type": "equation", "text": "$$\nS u b O p t(\\pi_{M C})\\le2H\\prod_{l=1}^{L}\\lVert A_{l}\\rVert\\sqrt{-\\frac{8\\log(\\delta/4)R_{\\operatorname*{max}}^{2}}{\\lfloor N/2\\rfloor(1-\\gamma)^{2}}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof of Theorem 2. Define $\\begin{array}{r}{C=\\prod_{l=1}^{L}\\lVert A_{l}\\rVert\\sqrt{-\\frac{8\\log(\\delta/4)R_{\\operatorname*{max}}^{2}}{\\lfloor{N/2}\\rfloor(1-\\gamma)^{2}}}}\\end{array}$ Then, combining the bound from Lemma 8 with the result from Lem ma 6, we get that with probability $1-\\delta$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\left|\\mathbb{E}_{X\\sim\\mathcal{N}(\\mu_{X},\\sigma_{X}^{2})}\\left[\\widehat{\\mathbb{B}}_{\\pi}Q(X)\\right]-\\mathbb{E}_{X\\sim\\mathcal{N}(\\widehat{\\mu}_{X},\\widehat{\\sigma}_{X}^{2})}\\left[\\widehat{\\mathbb{B}}_{\\pi}Q(X)\\right]\\right|\\leq C}\\\\ &{}&{\\left|\\mathbb{B}_{\\pi}Q(s,a)-\\mathbb{E}_{X\\sim\\mathcal{N}(\\widehat{\\mu}_{X},\\widehat{\\sigma}_{X}^{2})}\\left[\\widehat{\\mathbb{B}}_{\\pi}Q(X)\\right]\\right|\\leq C}\\\\ &{}&{\\left|\\mathbb{B}_{\\pi}Q(s,a)-\\widehat{\\mathbb{B}}_{\\pi}Q(s,a,s^{\\prime})\\right|\\leq C.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that $X=(s,a,s^{\\prime})$ , i.e., the state, action, and next state tuple, is distributed as described in the main text. Therefore $C$ is a $\\xi$ -uncertainty quantifier, with $\\xi=\\delta$ . Applying Theorem 1 finalizes the proof. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "B.3 Proof of theorem 3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In order to prove Theorem 3, i.e., an upper bound on our proposed moment matching approach, we require the following two lemmata. ", "page_idx": 18}, {"type": "text", "text": "The first lemma allows us to upper bound the 1-Wasserstein distance between an input distribution and its output distribution after transformation by the ReLU activation function. ", "page_idx": 18}, {"type": "text", "text": "Lemma 2 (Moment matching bound). For the following three random variables ", "page_idx": 18}, {"type": "equation", "text": "$$\nX\\sim\\rho_{X},\\qquad Y=\\operatorname*{max}(0,X),\\qquad\\widetilde{X}\\sim\\mathcal{N}(\\widetilde{X}|\\widetilde{\\mu},\\widetilde{\\sigma}^{2})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "with ${\\widetilde{\\mu}}=\\mathbb{E}\\left[Y\\right]$ and $\\widetilde{\\sigma}^{2}=\\mathrm{var}\\left[Y\\right]$ the following inequality holds ", "page_idx": 18}, {"type": "equation", "text": "$$\nW_{1}(\\rho_{Y},\\rho_{\\widetilde{X}})\\leq\\int_{-\\infty}^{0}F_{\\widetilde{X}}(u)d u+W_{1}(\\rho_{X},\\rho_{\\widetilde{X}})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $F_{\\widetilde{X}}(\\cdot)$ is cdf of $\\widetilde{X}$ . If $\\dot{\\rho}_{X}=\\mathcal{N}(X|\\mu,\\sigma^{2}),$ , it can be further simplified to ", "page_idx": 18}, {"type": "equation", "text": "$$\nW_{1}(\\rho_{Y},\\rho_{\\widetilde{X}})\\leq\\widetilde{\\sigma}\\phi\\left(\\frac{\\widetilde{\\mu}}{\\widetilde{\\sigma}}\\right)-\\widetilde{\\mu}\\Phi\\left(-\\frac{\\widetilde{\\mu}}{\\widetilde{\\sigma}}\\right)+|\\mu-\\widetilde{\\mu}|+|\\sigma-\\widetilde{\\sigma}|.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof of Lemma 2. For a generic $X\\,\\sim\\,\\rho_{X}$ , $Y\\,=\\,\\operatorname*{max}(0,X)$ and $\\widetilde{X}\\,\\sim\\mathcal N(\\widetilde{\\mu},\\widetilde{\\sigma}^{2})$ , we have with $F_{Y}(\\dot{u})=\\mathbb{1}_{u\\geq0}F_{X}(u)^{2}$ and (2) that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{1}(\\rho_{Y},\\rho_{\\widetilde{X}})=\\displaystyle\\int_{\\mathbb{R}}\\left|F_{Y}(u)-F_{\\widetilde{X}}(u)\\right|d u}\\\\ &{\\qquad\\qquad=\\displaystyle\\int_{-\\infty}^{0}F_{\\widetilde{X}}(u)d u+\\int_{0}^{\\infty}\\left|F_{X}(u)-F_{\\widetilde{X}}(u)\\right|d u}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\int_{-\\infty}^{0}F_{\\widetilde{X}}(u)d u+\\int_{0}^{\\infty}\\left|F_{X}(u)-F_{\\widetilde{X}}(u)\\right|d u+\\displaystyle\\int_{-\\infty}^{0}\\left|F_{X}(u)-F_{\\widetilde{X}}(u)\\right|d u}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "2The indicator function $\\mathbb{1}$ is defined as ${\\mathbb I}_{S}=1$ if the statement $S$ is true and 0 otherwise. ", "page_idx": 18}, {"type": "equation", "text": "$$\n=\\int_{-\\infty}^{0}F_{\\widetilde{X}}(u)d u+W_{1}(\\rho_{X},\\rho_{\\widetilde{X}}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "If $\\rho_{X}={\\mathcal{N}}(X|\\mu,\\sigma)$ , we can upper bound the $W_{1}$ distance via Chhachhi and Teng (2023) ", "page_idx": 19}, {"type": "equation", "text": "$$\nW_{1}(N(\\mu_{1},\\sigma_{1}^{2}),N(\\mu_{2},\\sigma_{2}^{2}))\\leq|\\mu_{1}-\\mu_{2}|+|\\sigma_{1}-\\sigma_{2}|\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and evaluate the integral ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\int_{-\\infty}^{0}F_{\\widetilde{X}}(u)d u=\\int_{-\\infty}^{0}\\Phi\\left(\\frac{u-\\widetilde{\\mu}}{\\widetilde{\\sigma}}\\right)d u=\\widetilde{\\sigma}\\phi\\left(\\frac{\\widetilde{\\mu}}{\\widetilde{\\sigma}}\\right)-\\widetilde{\\mu}\\Phi\\left(-\\frac{\\widetilde{\\mu}}{\\widetilde{\\sigma}}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we used that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\int\\Phi(a+b x)d x\\triangleq\\frac{1}{b}\\big((a+b x)\\Phi(a+b x)+\\phi(a+b x)\\big)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\circeq$ signifies equality up to an additive constant. Together this gives us that ", "page_idx": 19}, {"type": "equation", "text": "$$\nW_{1}(\\rho_{Y},\\rho_{\\tilde{x}})\\leq\\sigma\\phi\\left(\\frac{\\widetilde{\\mu}}{\\widetilde{\\sigma}}\\right)-\\widetilde{\\mu}\\Phi\\left(-\\frac{\\widetilde{\\mu}}{\\widetilde{\\sigma}}\\right)+|\\mu-\\widetilde{\\mu}|+|\\sigma-\\widetilde{\\sigma}|.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Lemma 3 (Moment matching MLP bound). Let $f(X)$ be an $L-$ layer MLP with ReLU activation $\\mathrm{r}(x)=\\operatorname*{max}(0,x)$ . For $l=1,\\dotsc,L-1$ , the sampling-based forward-pass is ", "page_idx": 19}, {"type": "equation", "text": "$$\nY_{0}=X_{s},\\qquad Y_{l}=\\operatorname{r}(f_{l}(Y_{l-1})),\\qquad Y_{L}=f_{L}(Y_{L-1})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $f_{l}(\\cdot)$ is the $l$ -th layer and $X_{s}$ a sample of $\\mathcal{N}(X|\\mu_{X},\\sigma_{X}^{2})$ . Its moment matching pendant is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\widetilde{X}_{0}\\sim\\mathcal{N}(\\widetilde{X}_{0}|\\mu_{X},\\sigma_{X}^{2}),\\qquad\\widetilde{X}_{l}\\sim\\mathcal{N}\\left(\\widetilde{X}_{l}\\Big|\\mathbb{E}\\left[r(f_{l}(\\widetilde{X}_{l-1}))\\right],\\mathrm{var}\\left[r(f_{l}(\\widetilde{X}_{l-1}))\\right]\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The following inequality holds for $\\widetilde{\\rho}_{Y}=\\rho_{\\widetilde{X}_{L}}=\\mathcal{N}(\\widetilde{X}_{L}|\\mathbb{E}[f(\\widetilde{X}_{L-1})],\\mathrm{var}[f(\\widetilde{X}_{L-1})]).$ ", "page_idx": 19}, {"type": "equation", "text": "$$\nW_{1}(\\rho_{Y},\\widetilde{\\rho}_{Y})\\leq\\sum_{l=2}^{L}\\left(G(\\widetilde{X}_{l-1})+C_{l-1}\\right)\\prod_{j=l}^{L}\\lVert A_{j}\\rVert\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\nG(\\widetilde{X}_{l})=\\widetilde{\\sigma}_{l}\\phi\\left(\\frac{\\widetilde{\\mu}_{l}}{\\widetilde{\\sigma}_{l}}\\right)-\\widetilde{\\mu}_{l}\\Phi\\left(-\\frac{\\widetilde{\\mu}_{l}}{\\widetilde{\\sigma}_{l}}\\right)\\leq1,\\qquad C_{l}\\leq|A_{l}\\widetilde{\\mu}_{l-1}-\\widetilde{\\mu}_{l}|+\\left|\\sqrt{A_{l}^{2}\\widetilde{\\sigma}_{l-1}^{2}}-\\widetilde{\\sigma}_{l}\\right|\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\sqrt{\\cdot}\\,a n d\\left(\\cdot\\right)^{2}$ are app lied elementwis e. ", "page_idx": 19}, {"type": "text", "text": "Proof of Lemma 3. Using Lemma 2 we have that the $W_{1}$ distance between the deterministic forward pass $\\rho_{Y_{l}}$ and the matching-based forward pass $\\rho_{\\widetilde{X}_{l}}$ is given as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{W_{1}(\\rho_{Y_{l}},\\rho_{\\tilde{X}_{l}})\\leq G(\\widetilde{X}_{l})+W_{1}(\\rho_{f_{l}(Y_{l-1})},\\rho_{\\tilde{X}_{l}}),}&{\\mathrm{~where~}G(X)\\triangleq\\displaystyle\\int_{-\\infty}^{0}F_{X}(u)d u}\\\\ {\\leq G(\\widetilde{X}_{l})+W_{1}(\\rho_{f_{l}(Y_{l-1})},\\rho_{f_{l}(\\tilde{X}_{l-1})})}\\\\ {+\\left.W_{1}(\\rho_{f_{l}(\\tilde{X}_{l-1})},\\rho_{\\tilde{X}_{l}}),\\right.}&{\\mathrm{~via~the~triangle~inequality}}\\\\ {\\leq G(\\widetilde{X}_{l})+\\|A_{l}\\|W_{1}(\\rho_{Y_{l-1}},\\rho_{\\tilde{X}_{l-1}})+C_{l},}&{C_{l}\\triangleq W_{1}(\\rho_{f_{l}(\\tilde{X}_{l-1})},\\rho_{\\tilde{X}_{l}})}\\\\ {\\leq G(\\widetilde{X}_{l})+\\|A_{l}\\|\\left(G(\\widetilde{X}_{l-1})\\right.}\\\\ {\\left.+\\|A_{l-1}\\|W_{1}(\\rho_{Y_{l-2}},\\rho_{\\tilde{X}_{l-2}})+C_{l-1}\\right)+C_{l}.}&\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "That is, for the entire MLP we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{1}(\\rho_{Y},\\widetilde{\\rho}_{Y})=W_{1}(\\rho_{Y_{L}},\\rho_{\\widetilde{X}_{L}})=\\|A_{L}\\|W_{1}(\\rho_{Y_{L-1}},\\rho_{\\widetilde{X}_{L-1}})}\\\\ &{\\qquad\\qquad\\leq\\|A_{L}\\|\\left(G(\\widetilde{X}_{L-1})+\\|A_{L-1}\\|W_{1}(\\rho_{Y_{L-2}},\\rho_{\\widetilde{X}_{L-2}})+C_{L-1}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\leq\\sum_{l=3}^{L}\\Big(G(\\widetilde{X}_{l-1})+C_{l-1}\\Big)\\prod_{j=l}^{L}\\lVert A_{j}\\rVert+W_{1}(\\rho_{Y_{1}},\\rho_{\\widetilde{X}_{1}})\\prod_{l=2}^{L}\\lVert A_{l}\\rVert\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where, finally, ", "page_idx": 20}, {"type": "equation", "text": "$$\nW_{1}(\\rho_{Y_{1}},\\rho_{\\tilde{X}_{1}})\\leq\\sigma\\phi\\left(\\frac{\\widetilde{\\mu}}{\\widetilde{\\sigma}}\\right)-\\widetilde{\\mu}\\Phi\\left(-\\frac{\\widetilde{\\mu}}{\\widetilde{\\sigma}}\\right)+\\left|A_{1}\\mu_{X}-\\widetilde{\\mu}_{1}\\right|+\\left|\\sqrt{A_{1}^{2}\\sigma_{X}^{2}}-\\widetilde{\\sigma}_{1}\\right|.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "$G(\\widetilde{X}_{l})$ and $C_{l}$ are given as ", "page_idx": 20}, {"type": "equation", "text": "$$\nG(\\widetilde{X}_{l})=\\widetilde{\\sigma}_{l}\\phi\\left(\\frac{\\widetilde{\\mu}_{l}}{\\widetilde{\\sigma}_{l}}\\right)-\\widetilde{\\mu}_{l}\\Phi\\left(-\\frac{\\widetilde{\\mu}_{l}}{\\widetilde{\\sigma}_{l}}\\right),\\qquad C_{l}\\leq|A_{l}\\widetilde{\\mu}_{l-1}-\\widetilde{\\mu}_{l}|+\\left|\\sqrt{A^{2}\\widetilde{\\sigma}_{l-1}^{2}}-\\widetilde{\\sigma}_{l}\\right|\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\sqrt{\\cdot}$ and $(\\cdot)^{2}$ are applied elementwise. ", "page_idx": 20}, {"type": "text", "text": "Theorem 3 (Suboptimality of moment matching-based PEVI algorithms). For any policy $\\pi_{M M}$ derived with $\\mathbb{A}_{P E V I}(\\widetilde{\\mathbb{B}}_{\\pi}^{\\Gamma}Q,\\widehat{\\mathbb{P}})$ learned by a penalization algorithm that uses moment matching to approximate the Bell man  t arget with respect to an action-value network defined as an $L-$ layer MLP with 1-Lipschitz activation functions, the following inequality holds ", "page_idx": 20}, {"type": "equation", "text": "$$\nS u b O p t(\\pi_{M M})\\leq2H\\sum_{l=2}^{L}\\left(G(\\widetilde{X}_{l-1})+C_{l-1}\\right)\\prod_{j=l}^{L}\\lVert A_{j}\\rVert.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof of Theorem 3. The proof is analogous to the one of Theorem 2. Note that $X=(s,a,s^{\\prime})$ , i.e., the state-action pair, is distributed as described in the main text. ", "page_idx": 20}, {"type": "text", "text": "Define $\\begin{array}{r}{C\\,=\\,\\sum_{l=2}\\left(G(\\tilde{X}_{l-1})+C_{l-1}\\right)\\prod_{j=l}^{L}\\lVert A_{j}\\rVert_{2}}\\end{array}$ . By combining this theorem with the results from Theore m 1, we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\left|\\mathbb{E}_{X\\sim\\mathcal{N}(\\mu_{X},\\sigma_{X}^{2})}\\left[\\widetilde{\\mathbb{B}}_{\\pi}Q(X)\\right]-\\mathbb{E}_{X\\sim\\mathcal{N}(\\tilde{\\mu}_{X},\\tilde{\\sigma}_{X}^{2})}\\left[\\widetilde{\\mathbb{B}}_{\\pi}Q(X)\\right]\\right|\\leq C}\\\\ &{}&{\\left|\\mathbb{B}_{\\pi}Q(s,a)-\\mathbb{E}_{X\\sim\\mathcal{N}(\\tilde{\\mu}_{X},\\tilde{\\sigma}_{X}^{2})}\\left[\\widetilde{\\mathbb{B}}_{\\pi}Q(X)\\right]\\right|\\leq C}\\\\ &{}&{\\left|\\mathbb{B}_{\\pi}Q(s,a)-\\widetilde{\\mathbb{B}}_{\\pi}Q(s,a,s^{\\prime})\\right|\\leq C}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "holds deterministically. Therefore, $C$ is a $\\xi$ -uncertainty quantifier for $\\xi=\\delta=0$ . Applying Theorem 1 finalizes the proof. ", "page_idx": 20}, {"type": "text", "text": "C Further details on experiments ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "C.1 Experiment procedures ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "C.1.1 Moment matching versus Monte Carlo sampling experiment ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We sample an initial state $s_{1}$ from the environments. Next, we use the expert policies $\\pi^{*}$ , treated as optimal policies, provided3 in the D4RL datasets $\\mathrm{Fu}$ et al., 2020) to sample action $a_{1}$ from the expert policy. Using the learned environment model $\\widehat{\\mathrm{P}}$ , we predict the next state samples $s_{2}~\\sim$ $\\widehat{\\mathrm{P}}(\\cdot|s_{1},a_{1})$ . For each sample, we evaluate the next acti o n-value $Q(s_{2},\\pi^{*}(s_{2}))$ using the previously l earned critics as the value function. We then evaluate the next action-value using moment matching $Q_{M M}(s_{2},\\pi^{*}(s_{2}))$ and visualize the distributions. For this experiment, we use the learned critics from seed 0. We repeat this process for all tasks in the D4RL dataset and present the results in Figure 3. ", "page_idx": 20}, {"type": "image", "img_path": "OFmclNhp0y/tmp/74ed4d8e01c703405afc09b2f8c528262f65cbc7036a8281b7dc1890e404d35e.jpg", "img_caption": ["Figure 3: Moment Matching versus Monte Carlo Sampling. A comparison of moment matching and Monte Carlo sampling methods for estimating the next value for all tasks in the D4RL dataset. "], "img_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "OFmclNhp0y/tmp/bd5df5bd14ad97decec75038c6e4db80fcb17c8dd91698360186bda2c835248a.jpg", "table_caption": ["Table 3: Performance evaluation on the Mixed dataset. Normalized reward at 2M gradient steps and Area Under the Learning Curve (AULC) (mean\u00b1std) scores are averaged across four repetitions for the Mixed offline reinforcement learning dataset. The highest means are highlighted in bold and are underlined if they fall within one standard deviation of the best score. The average normalized score is the average across all tasks, and the average ranking is based on the rank of the mean. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "C.1.2 Uncertainty quantifier experiment ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We generate 10 episode rollouts using the real environment and previously trained policies in evaluation mode. From these rollouts, we select state, action, reward, and next state tuples at every $10^{\\mathrm{th}}$ step, including the final step. For each selected tuple, we calculate the mean accuracy and tightness scores using the learned environment models. We repeat this process for each of the 4 seeds across every task and report the mean and standard deviation of the accuracies and tightness scores. We estimate the exact Bellman target by taking samples from the policies, and the number of samples is 1000. Table 2 shows the results of this experiment. ", "page_idx": 22}, {"type": "text", "text": "C.1.3 Mixed offline reinforcement learning dataset experiment ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The behavior policy of the Mixed datasets consists of a combination of random and medium or expert policies, mixed at a specified demonstration ratio. Throughout the experiments, we use the configurations of Cetin et al. (2024). See Appendix C.2 for details regarding the hyperparameters. Results in Table 3 show that MOMBO converges faster with minimal expert input, indicating that it effectively leverages limited information while maintaining competitive performance. Compared to other PEVI approaches, MOMBO demonstrates faster learning, greater robustness, and provides strong asymptotic performance, particularly under the constraints of limited expert knowledge and fixed training budgets, where hyperparameter tuning is not feasible. See Figures 5 to 6 for visualizations of the learning curves. ", "page_idx": 22}, {"type": "table", "img_path": "OFmclNhp0y/tmp/c2e6787ae043e35fd55f4a60d62eb54d9d40446fd2513e1fa5a6f2a06604ad9c.jpg", "table_caption": ["Table 4: Common hyperparameters and sources used in the experimental pipeline. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "C.2 Hyperparameters and experimental setup ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we provide all the necessary details to reproduce MOMBO. We evaluate MOMBO with four repetitions using the following seeds: [0, 1, 2, 3]. We run the experiments using the official repository of MOBILE (Sun et al., 2023),4 as well as our own model implementation, available at $\\mathrm{h}\\,\\mathsf{t}\\,\\mathsf{p}\\,\\mathbf{s}:/\\,/\\,\\mathsf{g}\\,.$ ithub.com/adinlab/MOMBO. We list the hyperparameters related to the experimental pipeline in Table 4. ", "page_idx": 23}, {"type": "text", "text": "C.2.1 Environment model training ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Following prior works (Yu et al., 2020, 2021; Sun et al., 2023), we model the transition model P as a neural network ensemble that predicts the next state and reward as a Gaussian distribution. We formulate this as $\\widehat{\\mathrm{P}}_{\\theta}(s^{\\prime},r)\\,=\\,\\mathcal{N}(\\mu_{\\theta}(s,a),\\Sigma_{\\theta}(s,a))$ , where $\\mu_{\\theta}$ and $\\Sigma_{\\theta}$ are neural networks that model the parame t ers of the Gaussian distribution. These neural networks are parameterized by $\\theta$ and we learn a diagonal covariance $\\textstyle\\sum_{\\theta}(a,s)$ . ", "page_idx": 23}, {"type": "text", "text": "We use $N_{\\mathrm{ens}}=7$ ensemble elements and select $N_{\\mathrm{elite}}\\,=\\,5$ elite elements from the ensemble based on a validation dataset containing 1000 transitions. Each model in the ensemble consists of a 4-layer feedforward neural network with 200 hidden units, and we apply $L2$ weight decay with the following weights for each layer, starting from the first layer: $[2.5\\times10^{-5}$ , $5\\!\\times\\!10^{-5}$ , $7.5\\!\\times\\!10^{-5}$ , $7.5\\!\\times\\!10^{-5},1\\!\\times$ $10^{-\\overline{{4}}}]$ . We train the environment model using maximum likelihood estimation with a learning rate of 0.001 and a batch size of 256. We apply early stopping with 5 steps using the validation dataset. The only exception is for walker2d-medium dataset, which has a fixed number of 30 learning episodes. We use Adam optimizer (Kingma and Ba, 2015) for the environment model. ", "page_idx": 24}, {"type": "text", "text": "We use the pre-trained environment models provided in Sun (2023) to minimize differences and reduce training time. For the Mixed dataset experiments, we train environment models using four seeds $(0,1,2,3)$ and use them for each baseline. ", "page_idx": 24}, {"type": "text", "text": "C.2.2 Policy training ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Architecture and optimization details. We train MOMBO for 3000 episodes on the D4RL dataset and 2000 episodes on the Mixed dataset, performing updates to the policy and Q-function 1000 times per episode with a batch size of 256. We set the learning rate for the critics to 0.0003, while the learning rate for the actor is 0.0001. We use the Adam optimizer (Kingma and Ba, 2015) for both the actor and critics. We employ a cosine annealing learning rate scheduler for the actor. For the D4RL dataset, both the actor and critics architectures consist of 2-layer feedforward neural networks with 256 hidden units. For the Mixed dataset, the actor uses a 5-layer feedforward neural network with 1024 hidden units, and the critics consist of a 3-layer feedforward neural network with 256 hidden units. Unlike other baselines, our critic network is capable of deterministically propagating uncertainties via moment matching. The critic ensemble consists of two networks. For policy optimization, we mostly follow the SAC (Haarnoja et al., 2018), with a difference in the policy evaluation phase. Using MOBILE\u2019s configuration, we apply the deterministic Bellman backup operator instead of the soft Bellman backup operator. We set the discount factor to $\\gamma~=~0.99$ and the soft update parameter to $\\tau\\,=\\,0.005$ . The $\\alpha$ parameter is learned during training with the learning rate of 0.0001, except for hopper-medium and hopper-medium-replay, where $\\alpha\\:=\\:0.2$ . We set the target entropy to $-\\dim(A)$ , where dim is the number of dimensions. We train all networks using a random mixture of the real dataset $\\mathcal{D}$ and synthetically generated rollouts $\\widehat{\\mathcal{D}}$ with real and synthetic data ratios of $p$ and $1-p$ , respectively. We set $p\\,=\\,0.05$ , except for h alfcheetah-medium-expert, where $p=0.5$ . ", "page_idx": 24}, {"type": "text", "text": "Synthetic dataset generation for policy optimization. We follow the same procedure for synthetic dataset generation as it is common in the literature. During this phase, we sample a batch of initial states from the real dataset $\\mathcal{D}$ and sample the corresponding actions from the current policy. For each state-action pair in the batch, the environment models predict a Gaussian distribution over the next state and reward. We choose one of the elite elements from the ensemble uniformly at random and take a sample from the predicted distribution for the next state and reward. We store these rollout transitions in a synthetic datasetD, where we also keep their variances in the buffer. We generate new rollouts at the beginning of e a ch episode and append them toD. We set the batch size for rollouts to 50000, and store only the rollouts from the last 5 and 10 e pi sodes in $\\widehat{\\mathcal{D}}$ for the D4RL dataset and the Mixed dataset, respectively. For the Mixed dataset, the rollout leng th $(k)$ is 5 for all tasks. Table 5 presents the rollout length for each task for the D4RL dataset. ", "page_idx": 24}, {"type": "text", "text": "Penalty coefficients. We adopt all configurations from MOBILE with the exception of $\\beta$ due to the differences in the scale of the uncertainty estimators. For the D4RL dataset, we provide our choices for $\\beta$ in Table 5. For the Mixed dataset, we select penalty coefficient $\\beta$ as 2.0 for all tasks in order to apply a penalty corresponding to $95\\%$ confidence level. We provide all the other shared hyperparameters in Table 4. ", "page_idx": 24}, {"type": "text", "text": "Experiment compute resources. We perform our experiments on three computational resources: 1) Tesla V100 GPU, Intel(R) Xeon(R) Gold 6230 CPU at $2.10\\ \\mathrm{GHz}$ , and $46~\\mathrm{GB}$ of memory; 2) NVIDIA Tesla A100 GPU, AMD EPYC 7F72 CPU at $3.2\\ \\mathrm{GHz}$ , and 256 GB of memory; and 3) GeForce RTX 4090 GPU, Intel(R) Core(TM) i7-14700K CPU at $5.6\\:\\mathrm{GHz}$ , and 96 GB of memory. We measure the computation time for 1000 gradient steps to be approximately 8 seconds for the D4RL dataset and 11 seconds for the Mixed dataset on the third device. Assuming the environment models are provided and excluding evaluation time, the total time required to reproduce all MOMBO repetitions is approximately 330 hours for D4RL and 600 hours for the Mixed dataset. The computational cost for other experiments is negligible. ", "page_idx": 24}, {"type": "table", "img_path": "OFmclNhp0y/tmp/254d3d529e83deadb64e34ab369b06446394cd698912341385c78d392fe121f5.jpg", "table_caption": ["Table 5: Hyperparameters of MOMBO on the D4RL dataset. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "C.3 Visualizations of learning curves ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Figures 4 to 6 illustrate the learning curves of PEVI-based approaches, including ours, across gradient steps. In the figures, the thick (dashed/dotted/solid) curve represents the mean normalized rewards across ten evaluation episodes and four random seeds, with the shaded area indicating one standard deviation from the mean. The legend provides the mean and standard deviation of the normalized reward and AULC scores in this order. We show the results for halfcheetah in the left panel, hopper in the middle panel, and walker2d in the right panel. The horizontal axis represents gradient steps and the vertical axis shows normalized episode rewards. ", "page_idx": 25}, {"type": "image", "img_path": "OFmclNhp0y/tmp/5199e28c0644a7a233c341555d126792b3796ee15cdee4906f3c50781bd2f3d9.jpg", "img_caption": ["Figure 4: Learning curves for the D4RL dataset. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "OFmclNhp0y/tmp/b8e9b2d3a17bdebedfddc7b4df5ed3a8b84bff03c185b3fafd2622ea5a7dcec9.jpg", "img_caption": ["Figure 5: Learning curves for the Mixed dataset with trained policy demonstration ratios of 0.01 and 0.05. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "OFmclNhp0y/tmp/ac25c9191037fb83e7aaef9a0f2404b87a1d6f4e4b72f1cb2b98a038753b686d.jpg", "img_caption": ["Figure 6: Learning curves for the Mixed dataset with trained policy demonstration ratios of 0.1 and 0.5. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We summarize our contribution in the abstract and in greater detail in the last paragraph of the introduction. These are theoretical (discussed in Section 3 and Section 4) and empirical (discussed in Section 5). ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We discuss limitations of the approach as part of our concluding discussion in Section 6. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We provide all assumptions in the respective theoretical lemmata and theorems. Appendix B includes all statements together with their respective proofs. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We highlight all experimental details and hyperparameters to ensure reproducibility in Appendix C. The experimental benchmarks come from standard libraries and for the public repository we base part of our evaluation on we reference the specific commit we worked with. Additionally, we make an implementation of our model available at https://github.com/adinlab/MOMBO. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. ", "page_idx": 30}, {"type": "text", "text": "In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We release a version of our model. The data is generated from standard public libraries for this specific task which are referenced in Appendix C. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We provide the necessary details about the experimental pipeline in Appendix C, along with the source code needed to reproduce our results. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We report error bars for each experiment together with details on what the respective numbers mean. We report mean $\\pm$ one standard deviation. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 31}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We report the computational resources and provide an approximate estimate of the computation time for all the experiments in Appendix C, in the paragraph titled \u201cExperiment compute resources\u201d. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We read the Code of Ethics carefully and ensured that our work complies with it. As the research is foundational we do not anticipate any ethical problems. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [No] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper focuses on foundational research. As such there is no immediate short term social impact of the work to be expected, neither positive nor negative. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: As our research is foundational research there is no safeguard needed at the current level of research. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We do not rely on any fixed data sets. The public libraries and the repository that we rely on are referrenced and explicitly acknowledged. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/dataset $_{\\mathrm{{S}}}$ has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: There are no new assests released with this work. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: No crowdsourcing or research with human subjects was performed as part of this project. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: No crowdsourcing or research with human subjects was performed as part of this project. As such no IRB approval was necessary. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]