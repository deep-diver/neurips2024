<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>Masked Pre-training Enables Universal Zero-shot Denoiser &#183; NeurIPS 2024</title>
<meta name=title content="Masked Pre-training Enables Universal Zero-shot Denoiser &#183; NeurIPS 2024"><meta name=description content="Masked Pre-training empowers a universal, fast zero-shot image denoiser!"><meta name=keywords content="Computer Vision,Image Generation,üè¢ University of Science and Technology of China,"><link rel=canonical href=https://deep-diver.github.io/neurips2024/posters/ofgtscasbr/><link type=text/css rel=stylesheet href=/neurips2024/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/neurips2024/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/neurips2024/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/neurips2024/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/neurips2024/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/neurips2024/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/neurips2024/favicon-16x16.png><link rel=manifest href=/neurips2024/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/neurips2024/posters/ofgtscasbr/"><meta property="og:site_name" content="NeurIPS 2024"><meta property="og:title" content="Masked Pre-training Enables Universal Zero-shot Denoiser"><meta property="og:description" content="Masked Pre-training empowers a universal, fast zero-shot image denoiser!"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posters"><meta property="article:published_time" content="2024-09-26T00:00:00+00:00"><meta property="article:modified_time" content="2024-09-26T00:00:00+00:00"><meta property="article:tag" content="Computer Vision"><meta property="article:tag" content="Image Generation"><meta property="article:tag" content="üè¢ University of Science and Technology of China"><meta property="og:image" content="https://deep-diver.github.io/neurips2024/posters/ofgtscasbr/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/neurips2024/posters/ofgtscasbr/cover.png"><meta name=twitter:title content="Masked Pre-training Enables Universal Zero-shot Denoiser"><meta name=twitter:description content="Masked Pre-training empowers a universal, fast zero-shot image denoiser!"><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Posters","name":"Masked Pre-training Enables Universal Zero-shot Denoiser","headline":"Masked Pre-training Enables Universal Zero-shot Denoiser","abstract":"Masked Pre-training empowers a universal, fast zero-shot image denoiser!","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/neurips2024\/posters\/ofgtscasbr\/","author":{"@type":"Person","name":"AI Paper Reviewer"},"copyrightYear":"2024","dateCreated":"2024-09-26T00:00:00\u002b00:00","datePublished":"2024-09-26T00:00:00\u002b00:00","dateModified":"2024-09-26T00:00:00\u002b00:00","keywords":["Computer Vision","Image Generation","üè¢ University of Science and Technology of China"],"mainEntityOfPage":"true","wordCount":"4914"}]</script><meta name=author content="AI Paper Reviewer"><link href=https://neurips.cc/ rel=me><link href=https://x.com/NeurIPSConf rel=me><link href rel=me><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://x.com/algo_diver/ rel=me><script src=/neurips2024/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/neurips2024/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/neurips2024/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/neurips2024/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/neurips2024/ class="text-base font-medium text-gray-500 hover:text-gray-900">NeurIPS 2024</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/neurips2024/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><div><div class="cursor-pointer flex items-center nested-menu"><a class="text-base font-medium text-gray-500 hover:text-primary-600 dark:hover:text-primary-400" title>Oral
</a><span><span class="relative block icon"><svg viewBox="0 0 20 20" fill="currentcolor" aria-hidden="true"><path fill-rule="evenodd" d="M5.23 7.21a.75.75.0 011.06.02L10 11.168l3.71-3.938a.75.75.0 111.08 1.04l-4.25 4.5a.75.75.0 01-1.08.0l-4.25-4.5a.75.75.0 01.02-1.06z" clip-rule="evenodd"/></svg></span></span></div><div class="absolute menuhide"><div class="pt-2 p-5 mt-2 rounded-xl backdrop-blur shadow-2xl"><div class="flex flex-col space-y-3"><a href=/neurips2024/oral-ai-applications/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(o) AI Applications</p></a><a href=/neurips2024/oral-ai-theory/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(o) AI Theory</p></a><a href=/neurips2024/oral-image-generation/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(o) Image Generation</p></a><a href=/neurips2024/oral-large-language-models/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(o) Large Language Models</p></a><a href=/neurips2024/oral-others/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(o) Others</p></a><a href=/neurips2024/oral-reinforcement-learning/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(o) Reinforcement Learning</p></a></div></div></div></div><div><div class="cursor-pointer flex items-center nested-menu"><a class="text-base font-medium text-gray-500 hover:text-primary-600 dark:hover:text-primary-400" title>Spotlight
</a><span><span class="relative block icon"><svg viewBox="0 0 20 20" fill="currentcolor" aria-hidden="true"><path fill-rule="evenodd" d="M5.23 7.21a.75.75.0 011.06.02L10 11.168l3.71-3.938a.75.75.0 111.08 1.04l-4.25 4.5a.75.75.0 01-1.08.0l-4.25-4.5a.75.75.0 01.02-1.06z" clip-rule="evenodd"/></svg></span></span></div><div class="absolute menuhide"><div class="pt-2 p-5 mt-2 rounded-xl backdrop-blur shadow-2xl"><div class="flex flex-col space-y-3"><a href=/neurips2024/spotlight-ai-theory/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(s) AI Theory</p></a><a href=/neurips2024/spotlight-large-language-models/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(s) Large Language Models</p></a><a href=/neurips2024/spotlight-optimization/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(s) Optimization</p></a><a href=/neurips2024/spotlight-others/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(s) Others</p></a><a href=/neurips2024/spotlight-reinforcement-learning/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(s) Reinforcement Learning</p></a></div></div></div></div><a href=/neurips2024/posters/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Posters</p></a><a href=/neurips2024/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/neurips2024/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Oral</p><span><span class="relative block icon"><svg viewBox="0 0 20 20" fill="currentcolor" aria-hidden="true"><path fill-rule="evenodd" d="M5.23 7.21a.75.75.0 011.06.02L10 11.168l3.71-3.938a.75.75.0 111.08 1.04l-4.25 4.5a.75.75.0 01-1.08.0l-4.25-4.5a.75.75.0 01.02-1.06z" clip-rule="evenodd"/></svg></span></span></a></li><li class=mt-1><a href=/neurips2024/oral-ai-applications/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(o) AI Applications</p></a></li><li class=mt-1><a href=/neurips2024/oral-ai-theory/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(o) AI Theory</p></a></li><li class=mt-1><a href=/neurips2024/oral-image-generation/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(o) Image Generation</p></a></li><li class=mt-1><a href=/neurips2024/oral-large-language-models/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(o) Large Language Models</p></a></li><li class=mt-1><a href=/neurips2024/oral-others/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(o) Others</p></a></li><li class=mt-1><a href=/neurips2024/oral-reinforcement-learning/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(o) Reinforcement Learning</p></a></li><li class=mb-2></li><li class=mt-1><a class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Spotlight</p><span><span class="relative block icon"><svg viewBox="0 0 20 20" fill="currentcolor" aria-hidden="true"><path fill-rule="evenodd" d="M5.23 7.21a.75.75.0 011.06.02L10 11.168l3.71-3.938a.75.75.0 111.08 1.04l-4.25 4.5a.75.75.0 01-1.08.0l-4.25-4.5a.75.75.0 01.02-1.06z" clip-rule="evenodd"/></svg></span></span></a></li><li class=mt-1><a href=/neurips2024/spotlight-ai-theory/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(s) AI Theory</p></a></li><li class=mt-1><a href=/neurips2024/spotlight-large-language-models/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(s) Large Language Models</p></a></li><li class=mt-1><a href=/neurips2024/spotlight-optimization/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(s) Optimization</p></a></li><li class=mt-1><a href=/neurips2024/spotlight-others/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(s) Others</p></a></li><li class=mt-1><a href=/neurips2024/spotlight-reinforcement-learning/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(s) Reinforcement Learning</p></a></li><li class=mb-2></li><li class=mt-1><a href=/neurips2024/posters/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Posters</p></a></li><li class=mt-1><a href=/neurips2024/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/neurips2024/posters/ofgtscasbr/cover_hu17373104127731384042.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/neurips2024/>NeurIPS 2024</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/neurips2024/posters/>Posters</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/neurips2024/posters/ofgtscasbr/>Masked Pre-training Enables Universal Zero-shot Denoiser</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Masked Pre-training Enables Universal Zero-shot Denoiser</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2024-09-26T00:00:00+00:00>26 September 2024</time><span class="px-2 text-primary-500">&#183;</span><span>4914 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">24 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_posters/oFgTScAsBr/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_posters/oFgTScAsBr/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/neurips2024/tags/computer-vision/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Computer Vision
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/neurips2024/tags/image-generation/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Image Generation
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/neurips2024/tags/-university-of-science-and-technology-of-china/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ University of Science and Technology of China</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="AI Paper Reviewer" src=/neurips2024/img/avatar_hu1344562329374673026.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">AI Paper Reviewer</div><div class="text-sm text-neutral-700 dark:text-neutral-400">As an AI, I specialize in crafting insightful blog content about cutting-edge research in the field of artificial intelligence</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://neurips.cc/ target=_blank aria-label=Homepage rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg fill="currentcolor" height="800" width="800" id="Capa_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 491.398 491.398"><g><g id="Icons_19_"><path d="M481.765 220.422 276.474 15.123c-16.967-16.918-44.557-16.942-61.559.023L9.626 220.422c-12.835 12.833-12.835 33.65.0 46.483 12.843 12.842 33.646 12.842 46.487.0l27.828-27.832v214.872c0 19.343 15.682 35.024 35.027 35.024h74.826v-97.62c0-7.584 6.146-13.741 13.743-13.741h76.352c7.59.0 13.739 6.157 13.739 13.741v97.621h74.813c19.346.0 35.027-15.681 35.027-35.024V239.091l27.812 27.815c6.425 6.421 14.833 9.63 23.243 9.63 8.408.0 16.819-3.209 23.242-9.63 12.844-12.834 12.844-33.65.0-46.484z"/></g></g></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://x.com/NeurIPSConf target=_blank aria-label=X-Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href target=_blank aria-label=Line rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg id="Capa_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 14.707 14.707"><g><rect x="6.275" y="0" style="fill:currentColor" width="2.158" height="14.707"/></g></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://x.com/algo_diver/ target=_blank aria-label=X-Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#masked-pre-train>Masked Pre-train</a></li><li><a href=#iterative-filling>Iterative Filling</a></li><li><a href=#real-world-noise>Real-world Noise</a></li><li><a href=#generalization-limits>Generalization Limits</a></li><li><a href=#future-directions>Future Directions</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#masked-pre-train>Masked Pre-train</a></li><li><a href=#iterative-filling>Iterative Filling</a></li><li><a href=#real-world-noise>Real-world Noise</a></li><li><a href=#generalization-limits>Generalization Limits</a></li><li><a href=#future-directions>Future Directions</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>oFgTScAsBr</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Xiaoxiao Ma et el.</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href="https://openreview.net/forum?id=oFgTScAsBr" target=_blank role=button>‚Üó OpenReview
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://neurips.cc/virtual/2024/poster/93634 target=_blank role=button>‚Üó NeurIPS Homepage
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href="https://huggingface.co/spaces/huggingface/paper-central?tab=tab-chat-with-paper&amp;paper_id=oFgTScAsBr&amp;paper_from=neurips" target=_blank role=button>‚Üó Chat</a></p><audio controls><source src=https://ai-paper-reviewer.com/oFgTScAsBr/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Current zero-shot image denoising methods often produce blurry results or require significant computation time. This is because they typically train new networks from scratch for each noisy image, limiting the information available for training and affecting quality/speed. Existing methods also struggle with generalizing across different noise types.</p><p>The paper introduces Masked Pre-train then Iterative fill (MPI), a new zero-shot denoising approach. <strong>MPI first pre-trains a model on a massive dataset of natural images using a masking strategy.</strong> Then, it uses this pre-trained model to iteratively refine a single noisy image, focusing on masked regions to minimize the gap between pre-training and inference. <strong>This method significantly improves denoising quality and speed</strong>, outperforming existing zero-shot methods while demonstrating robust generalization to different noise types and even medical images.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-4885c192e7a3ab929a4127e813a43729></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-4885c192e7a3ab929a4127e813a43729",{strings:[" A novel zero-shot image denoising paradigm, Masked Pre-train then Iterative fill (MPI), is proposed, achieving high-quality results with reduced inference time. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-1bfae18d0fa6bd10a6b19e24e0519b3f></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-1bfae18d0fa6bd10a6b19e24e0519b3f",{strings:[" MPI effectively leverages pre-trained knowledge from natural images for effective zero-shot denoising on unseen noise and various image types. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-4bae3a41f845ce6d5eec4a59ab221d39></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-4bae3a41f845ce6d5eec4a59ab221d39",{strings:[" Comprehensive experiments demonstrate MPI's superiority and efficiency across diverse noisy scenarios and even generalizes to medical images. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This paper is crucial for researchers in image processing and computer vision due to its introduction of a novel zero-shot denoising paradigm. The <strong>universal applicability across diverse noise types and image modalities</strong>, along with the <strong>significant reduction in inference time</strong>, makes it highly relevant to current research trends and opens new avenues for practical applications. Its <strong>innovative use of masked pre-training</strong> offers a promising direction for future research in zero-shot learning and image restoration.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/figures_0_1.jpg alt></figure></p><blockquote><p>This figure demonstrates the superior performance of the proposed method (Ours) compared to existing state-of-the-art zero-shot and supervised/unsupervised image denoising methods. Panel (a) showcases a significant reduction in inference time while maintaining competitive performance with respect to Peak Signal-to-Noise Ratio (PSNR). Panel (b) highlights improved generalization across diverse noise types. Lastly, panel (c) visually presents the effectiveness of the method in removing real-world and medical image noise, demonstrating its superior performance compared to existing methods.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/tables_4_1.jpg alt></figure></p><blockquote><p>This table presents a quantitative comparison of different image denoising methods on three datasets (CSet, McMaster, and CBSD) for Gaussian noise removal. The comparison is done across different noise levels (œÉ = 10, 25, 50). The results are presented in terms of PSNR and SSIM values, with the best results highlighted and the second-best underlined. The table also includes the average inference time for each method. Supplementary materials provide similar results for Poisson noise removal.</p></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">Masked Pre-train<div id=masked-pre-train class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#masked-pre-train aria-label=Anchor>#</a></span></h4><p>The Masked Pre-train phase is <strong>critical</strong> to the success of the MPI (Masked Pre-train then Iterative fill) model. It leverages <strong>masked image modeling</strong> on a large dataset of natural images (like ImageNet) to learn robust and generalizable image representations. <strong>Random masking</strong> strategically obscures portions of the images, forcing the model to reconstruct the missing parts based on contextual information. This process effectively <strong>encodes the inherent structure and statistical properties</strong> of natural images into the model&rsquo;s weights. The resulting pre-trained model, therefore, isn&rsquo;t just a denoiser trained on specific noise patterns, but rather possesses a powerful capacity for <strong>zero-shot generalization</strong>. This pre-training phase is what allows the model to handle unseen noise types effectively during the subsequent iterative filling steps. The choice of masking strategy, the size of the dataset, and the training procedure all play important roles in ensuring that the model effectively learns the underlying structure of the image, which then provides a strong basis for later zero-shot inference. It&rsquo;s a key innovation, <strong>differentiating MPI from traditional zero-shot denoising methods</strong> that train entirely from scratch on a single noisy image.</p><h4 class="relative group">Iterative Filling<div id=iterative-filling class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#iterative-filling aria-label=Anchor>#</a></span></h4><p>The proposed method, Masked Pre-train then Iterative Fill (MPI), innovatively uses an iterative refinement strategy called &ldquo;Iterative Filling&rdquo; for zero-shot image denoising. Instead of training a new model for each noisy image, <strong>MPI leverages pre-trained weights from a model trained on a massive dataset of natural images using a masked image modeling approach.</strong> This pre-trained model already implicitly possesses knowledge about image structures and distributions, providing a strong foundation for effective denoising. The iterative filling process progressively refines the noisy image through iterative optimization, focusing on masked regions in each step and gradually assembling a complete denoised image. This strategy is particularly effective as it avoids overfitting to the specific noise characteristics of a single noisy image while still exploiting the power of pre-trained knowledge for high-quality results. <strong>The iterative approach makes the method more efficient than traditional zero-shot methods that train a new network from scratch for every image.</strong> The combination of pre-training and iterative refinement is key to achieving both high performance and significantly reduced inference time.</p><h4 class="relative group">Real-world Noise<div id=real-world-noise class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#real-world-noise aria-label=Anchor>#</a></span></h4><p>The section on real-world noise in this research paper is crucial because it addresses the limitations of existing denoising methods that primarily focus on synthetic noise. <strong>Real-world noise is significantly more complex</strong>, exhibiting strong spatial correlations and varying intensity levels. The authors acknowledge that <strong>simply applying masking strategies designed for synthetic noise is insufficient</strong> for effectively handling real-world scenarios. Consequently, they propose incorporating <strong>downsampling</strong> to reduce spatial correlations, along with a <strong>larger masking ratio</strong> to better capture the characteristics of real-world noise patterns. This approach showcases the <strong>generalizability</strong> and <strong>robustness</strong> of their proposed masked pre-training method, demonstrating its effectiveness in scenarios that go beyond the constraints of laboratory-generated datasets. The results on real-world datasets, such as SIDD and PolyU, further highlight this method&rsquo;s capabilities. This is a <strong>key strength</strong> of the paper as it bridges the gap between theoretical findings using synthetic datasets and practical applications using real images with varied noise types.</p><h4 class="relative group">Generalization Limits<div id=generalization-limits class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#generalization-limits aria-label=Anchor>#</a></span></h4><p>The heading &lsquo;Generalization Limits&rsquo; in a research paper would explore the boundaries of a model&rsquo;s ability to perform well on unseen data. A thoughtful analysis would delve into <strong>factors limiting generalization</strong>, such as the <strong>size and diversity of the training dataset</strong>, the <strong>model&rsquo;s architecture and capacity</strong>, and the <strong>nature of the noise or variations present in the data</strong>. The discussion might include a comparison of performance on training data versus unseen data, possibly quantified by metrics like generalization gap. It would be crucial to <strong>identify specific types of noise or data distributions</strong> where the model underperforms, along with an exploration of the reasons why. For instance, <strong>a model trained on Gaussian noise might fail to generalize to real-world noise with complex spatial correlations</strong>. A strong analysis would also discuss the extent to which the model&rsquo;s limitations are inherent to the task itself or whether they result from specific design choices. Ultimately, the section on &lsquo;Generalization Limits&rsquo; would reveal valuable insights into the robustness and applicability of the presented research.</p><h4 class="relative group">Future Directions<div id=future-directions class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-directions aria-label=Anchor>#</a></span></h4><p>Future research directions for masked pre-training in zero-shot image denoising could explore <strong>more sophisticated masking strategies</strong>, moving beyond simple random masking to incorporate spatial and frequency information. This might involve <strong>adaptive masking</strong> that adjusts the masking ratio based on image content complexity or noise characteristics. Furthermore, exploring different <strong>pre-training datasets</strong> beyond ImageNet, perhaps incorporating specialized datasets for specific noise types or image modalities, could improve performance on less-represented domains. <strong>Investigating alternative optimization methods</strong> during the iterative filling stage, potentially employing techniques from diffusion models or other advanced optimization frameworks, could yield higher quality denoised images. Finally, a thorough investigation into the <strong>generalization capabilities</strong> of the approach, evaluating its performance on a wider range of unseen noise types and image modalities, is crucial for practical application. Specifically, applying the proposed method to <strong>real-world challenging scenarios</strong>, such as medical imaging or low-light photography, will offer valuable insights and potential improvements.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/figures_2_1.jpg alt></figure></p><blockquote><p>The figure shows the results of applying a model trained on ImageNet with 70% pixel-wise masking to a noisy image. The &lsquo;Directly ensemble&rsquo; column shows the result of simply averaging predictions from the pre-trained model, demonstrating a basic level of denoising. The &lsquo;+Zero-shot Optim.&rsquo; column, on the other hand, shows the result of applying iterative filling which further improves upon the quality of the denoised image. This illustrates the effectiveness of the proposed iterative filling optimization step for zero-shot denoising.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/figures_2_2.jpg alt></figure></p><blockquote><p>The figure shows the PSNR and SSIM values for a noisy image, a denoised image obtained by directly averaging the predictions from a pre-trained model, and a denoised image obtained by further optimizing the pre-trained model using an iterative filling method. The iterative filling method significantly improves the PSNR and SSIM values, demonstrating its effectiveness in enhancing denoising performance.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/figures_3_1.jpg alt></figure></p><blockquote><p>This figure illustrates the Masked Pre-train then Iterative fill (MPI) paradigm. It shows the two main stages: 1) Masked Pre-training where a model learns to reconstruct masked natural images, extracting general knowledge about image distributions. 2) Iterative Filling, a zero-shot inference process where the pre-trained model is fine-tuned on a single noisy image to iteratively refine its denoising capabilities. The iterative process involves masked predictions, where only predictions for masked regions are retained to avoid overfitting and achieve efficient denoising in a limited number of iterations. The final denoised image is created by combining predictions across all the iterations.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/figures_4_1.jpg alt></figure></p><blockquote><p>This figure demonstrates the superiority of the proposed method (Ours) over existing zero-shot and supervised/unsupervised image denoising methods. Panel (a) shows a comparison of computational cost and performance, highlighting the faster inference time of the proposed method. Panel (b) showcases its better generalization across various noise types. Finally, panel (c) illustrates its ability to effectively denoise real-world and medical images.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/figures_5_1.jpg alt></figure></p><blockquote><p>This figure shows qualitative comparisons of denoising results on unseen noise types (salt & pepper and speckle noise). The results of the proposed method (MPI) are compared against several other methods including DIP, ZS-N2N, and Restormer. Restormer, a model trained on Gaussian noise, serves as a baseline for comparison. The figure highlights the superiority of the proposed MPI method in handling these different unseen noise types, producing images with noticeably improved visual quality and fewer artifacts.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/figures_6_1.jpg alt></figure></p><blockquote><p>This figure shows the qualitative results of the proposed method on real-world noisy images from two datasets: SIDD and PolyU. The results are compared against several other state-of-the-art denoising methods (DIP, N2V*, N2S*, ZS-N2N, FasterDIP). The figure showcases the visual improvements achieved by the proposed method, particularly in terms of detail preservation and noise reduction. The PSNR/SSIM values are provided below each image, which quantifies the performance gains.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/figures_7_1.jpg alt></figure></p><blockquote><p>This figure shows that the pre-trained model generalizes better to unseen noise types compared to a model trained from scratch. The results show that the pre-trained model is able to denoise images with unseen noise better than a model trained from scratch, even when the content of the image is different from the images used in pre-training. This suggests that the pre-trained model has learned a more general representation of images that is able to handle unseen noise better than a model trained only on seen noise. The result indicates the model trained on ImageNet with 70% pixel-wise masking, The denoised images are obtained by directly ensemble of predictions from fixed pre-trained weights. Its performance can be further improved with iterative filling.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/figures_7_2.jpg alt></figure></p><blockquote><p>This figure shows the effect of using a pre-trained model for denoising. It presents a comparison of denoising results on an image with Gaussian noise (œÉ=25) at different iterations (t=10, 100, 500, 1000, 1500) using both pre-trained weights (orange) and weights initialized from scratch (blue). The results visually demonstrate that the pre-trained model leads to better denoising performance, even at earlier iterations.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/figures_8_1.jpg alt></figure></p><blockquote><p>This figure shows the visualization of features extracted from different layers of the model during inference. The top row shows the CKA and PCA analysis of the features, which reveals that the pre-trained model and the model trained from scratch have significantly different feature distributions in the last layers. The bottom row shows the feature maps from selected layers of the pre-trained model and the model trained from scratch. This comparison shows that the pre-trained model tends to restore the complete image, while the model trained from scratch focuses primarily on restoring the masked regions.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/figures_8_2.jpg alt></figure></p><blockquote><p>This figure shows the impact of different masking ratios on the denoising performance using synthetic noise. The x-axis represents the masking ratio (percentage of pixels masked), and the y-axis shows the peak signal-to-noise ratio (PSNR) in dB. The plot shows that a masking ratio of around 30% provides the best balance between noise removal and detail preservation. Lower masking ratios result in insufficient noise reduction, while higher ratios lead to over-smoothing and loss of detail. The optimal ratio of 30% is specific to synthetic noise.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/figures_15_1.jpg alt></figure></p><blockquote><p>This figure illustrates the Exponential Moving Average (EMA) process used in the Iterative Filling step of the Masked Pre-train then Iterative fill (MPI) method. In each iteration (t), the model generates predictions (yt). Only the predictions for the masked regions (Mt‚äôyt) are considered reliable and are used to update the ensemble (·ªπt). The contribution of each step&rsquo;s reliable predictions is weighted by Œ≤, an exponential weight. Unreliable pixels are kept unchanged. This process continues across multiple iterations (T) to refine the denoised output.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/figures_16_1.jpg alt></figure></p><blockquote><p>This figure illustrates the zero-shot denoising process with adaptations for real-world noise. A downsampling step reduces spatial correlations in the noisy image before processing. The model processes the downsampled image using masked pre-training and iterative filling, with the resulting prediction combined using EMA (Exponential Moving Average). Finally, an upsampling step restores the denoised image to its original resolution. The green arrows highlight the downsampling and upsampling operations, which are only applied to the SIDD dataset.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/figures_16_2.jpg alt></figure></p><blockquote><p>This figure shows the impact of different masking ratios on the performance of the denoising model. The x-axis represents the masking ratio (percentage of pixels masked), while the y-axis represents the PSNR (Peak Signal-to-Noise Ratio), a metric that measures image quality. The plot shows an optimal masking ratio of around 30% for synthetic noise. A lower ratio doesn&rsquo;t remove enough noise, while a higher ratio leads to over-smoothing, reducing detail. This finding is crucial for balancing noise reduction and detail preservation in the denoising process.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/figures_17_1.jpg alt></figure></p><blockquote><p>This figure illustrates the two-stage process of the Masked Pre-train then Iterative fill (MPI) method. The first stage, Masked Pre-training, involves training a model (DŒ∏) to reconstruct natural images from which random patches have been masked. The learned weights (Œ∏) from this stage are then used in the second stage, Iterative Filling. In this stage, the model, initialized with the pre-trained weights, iteratively refines its prediction of a single noisy image (x) by focusing on reconstructing masked portions of the image. The process repeats for &lsquo;T&rsquo; steps, ultimately producing a final denoised prediction (y). The inclusion of the pre-trained model improves both speed and quality compared to training a model from scratch for each image.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/figures_17_2.jpg alt></figure></p><blockquote><p>The figure shows the impact of different regularization techniques on the performance of the zero-shot denoising method. Total Variation (TV) regularization, data augmentation, and early stopping are compared to the original method. The results show that early stopping provides the best balance between performance and computational cost.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/figures_18_1.jpg alt></figure></p><blockquote><p>This figure illustrates the process of adapting the zero-shot denoising method to handle real-world noise. Real-world noise often exhibits strong spatial correlations, which standard masking techniques struggle with. To address this, the model incorporates downsampling and upsampling steps. The downsampling reduces spatial correlation before denoising, and the upsampling restores the image to its original resolution after processing. The figure also highlights the use of a larger masking ratio (80%-95%) to further mitigate the impact of remaining spatial correlations. Notably, this adaptation is specifically applied to the SIDD dataset; not all images undergo downsampling and upsampling.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/figures_19_1.jpg alt></figure></p><blockquote><p>This figure shows the impact of pre-training on the performance of a denoising model at various noise levels. The left panel shows results for Gaussian noise, while the right panel presents results for Poisson noise. Both panels display the PSNR (peak signal-to-noise ratio) against different noise levels (œÉ for Gaussian and Œª for Poisson). Two lines are plotted in each panel, one for the baseline model (trained without pre-training), and one for the model trained using the paper&rsquo;s proposed masked pre-training approach. The plots illustrate that pre-training significantly improves denoising performance, particularly at higher noise levels, showcasing the effectiveness of the proposed method.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/figures_19_2.jpg alt></figure></p><blockquote><p>This figure shows the results of an experiment comparing the performance of a denoising model with and without pre-training on different noise levels. The results show that pre-training significantly improves performance on all noise levels, particularly with stronger noise. This is because pre-training provides the model with a better foundation for understanding and removing noise patterns, which helps to reduce overfitting to specific noise characteristics and improve generalization across various noise levels. The left plot shows results for Gaussian noise, while the right plot shows results for Poisson noise.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/figures_21_1.jpg alt></figure></p><blockquote><p>This figure shows a visual comparison of the denoising results from various methods on a specific noisy image patch (SIDDval_34_22) from the SIDD validation dataset. It allows for a qualitative assessment of the performance of each method in terms of noise reduction and detail preservation. The methods compared include the proposed &lsquo;Ours&rsquo; method, along with several baselines and state-of-the-art zero-shot and other denoising techniques, such as DIP-SURE, DDNM, DDPG, MM-BSN and PUCA. The Ground Truth (GT) is also provided for reference.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/figures_23_1.jpg alt></figure></p><blockquote><p>This figure demonstrates the effectiveness of the proposed MPI method for image denoising. Subfigure (a) compares the computational cost and performance of MPI against other zero-shot denoising methods, highlighting its speed advantage. Subfigure (b) showcases MPI&rsquo;s superior generalization ability across various noise types compared to both zero-shot and supervised/unsupervised approaches. Finally, subfigure (c) illustrates MPI&rsquo;s capability to effectively denoise real-world images with spatial noise correlation, using images from the SIDD and FMD benchmark datasets.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/figures_23_2.jpg alt></figure></p><blockquote><p>This figure provides a visual overview of the Masked Pre-train then Iterative fill (MPI) paradigm. It shows the two main stages: 1) Masked Pre-training, where a model learns to reconstruct masked natural images, and 2) Iterative filling, which uses the pre-trained weights to iteratively refine a noisy image. The iterative process involves masking parts of the image, making predictions, and updating the model weights to better reconstruct the masked regions. This process is repeated multiple times, leading to improved denoising. The figure highlights how the pre-trained knowledge enables faster and better zero-shot denoising compared to traditional methods.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/figures_24_1.jpg alt></figure></p><blockquote><p>This figure compares the denoising performance of various methods on a noisy patch from the CBSD dataset using Gaussian noise with a standard deviation of 10. The methods compared include DIP, N2V*, N2S*, ZS-N2N, FasterDIP, and the proposed method (Ours). The reference image is provided for comparison. The results are presented in terms of PSNR and SSIM values, which are quantitative metrics of image quality.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/figures_25_1.jpg alt></figure></p><blockquote><p>This figure demonstrates the superiority of the proposed method (Ours) compared to existing zero-shot and supervised/unsupervised image denoising methods across three key aspects: computational cost, generalization ability, and real-world noise removal. (a) shows the reduced inference time of the proposed method. (b) highlights the improved generalization across different noise types. (c) showcases the effectiveness on real-world noisy images from SIDD and FMD benchmark datasets.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/figures_26_1.jpg alt></figure></p><blockquote><p>This figure shows the qualitative and quantitative results of the proposed MPI method on Gaussian and Poisson noise. The results are compared against several state-of-the-art denoising methods. The qualitative comparison shows the visual results of denoising on noisy patches from the CBSD-44 and McMaster-14 datasets. The quantitative comparison shows the PSNR and SSIM values for each method.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/figures_27_1.jpg alt></figure></p><blockquote><p>This figure shows a qualitative comparison of different image denoising methods on a noisy patch from the CBSD dataset with Poisson noise (Œª=10). The comparison includes the original noisy image, the ground truth (reference) image, and results from DIP, N2V*, N2S*, ZS-N2N, FasterDIP, the faster version of the proposed MPI method, and the proposed MPI method. The visual results demonstrate the relative performance of each method in terms of noise reduction and detail preservation.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/figures_28_1.jpg alt></figure></p><blockquote><p>This figure compares the denoising results of different methods on a CBSD dataset patch with Poisson noise (Œª=25). The methods compared include DIP, N2V*, N2S*, ZS-N2N, FasterDIP, the proposed &lsquo;Ours (faster)&rsquo; method, and the full &lsquo;Ours&rsquo; method. The figure visually demonstrates the performance differences of each method in terms of noise reduction and detail preservation.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/figures_29_1.jpg alt></figure></p><blockquote><p>The figure shows a qualitative comparison of denoising results on a noisy patch from the CBSD dataset with Poisson noise (Œª=50). It compares the performance of different methods including DIP, N2V*, N2S*, ZS-N2N, FasterDIP, and the proposed method (MPI). The results demonstrate the superior performance of the MPI method in terms of noise removal and detail preservation compared to the other methods.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/figures_29_2.jpg alt></figure></p><blockquote><p>This figure compares the results of different denoising methods on a noisy patch from the Kodak dataset, where the noise is Salt and Pepper noise with density d=0.045. The methods compared include SwinIR, Restormer, Neighbor2Neighbor, Blind2Unblind, DIP, ZS-N2N, and the proposed MPI method (both faster and full versions). The figure shows the visual quality of the denoised images produced by each method, highlighting the performance differences.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/figures_30_1.jpg alt></figure></p><blockquote><p>This figure compares the feature distributions obtained from a model with and without pre-training using Kernel Canonical Correlation Analysis (CKA) and Principal Component Analysis (PCA). The results show that pre-trained weights significantly impact the feature distributions and improve the performance in image restoration. The pre-trained model focuses on restoring the complete image, while the untrained model focuses only on the masked areas.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/figures_31_1.jpg alt></figure></p><blockquote><p>This figure compares the denoising performance of different methods on a noisy patch from the Kodak dataset with Poisson noise (Œª=40). The comparison includes several supervised and unsupervised methods, along with the proposed MPI method. The results show that MPI achieves superior denoising quality and better detail preservation compared to the other methods. This demonstrates the generalization capability of the proposed method to various unseen noise types.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/figures_32_1.jpg alt></figure></p><blockquote><p>This figure shows a qualitative comparison of denoising results on a real-world noisy image patch from the PolyU dataset. The comparison includes the noisy image, the ground truth, and denoising results from several methods (DIP, N2V*, N2S*, ZS-N2N, FasterDIP, MPI (faster version), and MPI). The goal is to visually demonstrate the performance of MPI in removing real-world noise while preserving image details. Each method&rsquo;s PSNR and SSIM scores are also displayed.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/figures_33_1.jpg alt></figure></p><blockquote><p>This figure shows the qualitative comparison of denoising results on real noisy images from SIDD and PolyU datasets. The results demonstrate the effectiveness of the proposed method (Ours) in removing real-world noise compared to other existing methods (DIP, N2V*, N2S*, ZS-N2N, FasterDIP). It highlights the method&rsquo;s ability to preserve image details while effectively removing noise.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/figures_33_2.jpg alt></figure></p><blockquote><p>This figure shows a qualitative comparison of denoising results on a real noisy image patch from the SIDD dataset. The comparison includes the noisy image, the ground truth (reference), and the denoised images generated by various methods including DIP, N2V*, N2S*, ZS-N2N, FasterDIP, and the proposed method (MPI). The results illustrate the effectiveness of the authors&rsquo; proposed method in achieving high quality results with preserved details compared to the other methods.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/figures_34_1.jpg alt></figure></p><blockquote><p>This figure shows a qualitative comparison of different denoising methods on a specific noisy patch from the SIDD validation dataset (SIDDval_34_22). It visually compares the results of DIP, N2V*, N2S*, ZS-N2N, FasterDIP, and the proposed &lsquo;Ours (faster)&rsquo; and &lsquo;Ours&rsquo; methods against the ground truth image. The comparison allows a visual assessment of the relative performance of each method in terms of noise reduction and detail preservation.</p></blockquote></details><details><summary>More on tables</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/tables_5_1.jpg alt></figure></p><blockquote><p>This table presents a quantitative comparison of the generalization performance of various image denoising methods on the Kodak dataset. The methods were initially trained using Gaussian noise with a standard deviation of 25. The table evaluates their performance across five different unseen noise types: Gaussian noise with varying standard deviations, Poisson noise with varying lambda values, Noise Level Function (NLF) noise, speckle noise with varying variance values, and salt-and-pepper noise with varying density values. The average PSNR and SSIM values across these different noise types are reported for each method, providing an assessment of their ability to generalize to unseen noise conditions. The results indicate that the proposed method shows superior generalization capabilities compared to others.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/tables_6_1.jpg alt></figure></p><blockquote><p>This table presents a quantitative comparison of different image denoising methods on three real-world noisy image datasets: SIDD, PolyU, and FMD. The comparison is based on Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) metrics, which measure the quality of the denoised images. The table also includes the average inference time for each method, highlighting the computational efficiency of the proposed MPI method. The results demonstrate the superior performance of the MPI method, particularly its faster inference speed, compared to existing methods.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/tables_8_1.jpg alt></figure></p><blockquote><p>This table presents the ablation study results on the impact of pre-trained weights on the performance of the proposed zero-shot denoising method. It compares the performance of the model with and without pre-training on three different datasets: CSet, SIDD, and FMD. The results are presented in terms of PSNR and SSIM values. The table also shows the effect of different values of the exponential weight parameter Œ≤ on the model&rsquo;s performance.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/tables_9_1.jpg alt></figure></p><blockquote><p>This ablation study analyzes different ensemble strategies for the proposed zero-shot denoising method. The table compares the performance (PSNR/SSIM) and inference time for six different strategies: &lsquo;Avg after 500e&rsquo;, &lsquo;Average&rsquo;, &lsquo;EMA w/o mask&rsquo;, &lsquo;w/o Ensemble&rsquo;, &lsquo;Last&rsquo;, and &lsquo;EMA&rsquo;. The results highlight the impact of the ensemble method on the overall denoising quality and efficiency.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/tables_9_2.jpg alt></figure></p><blockquote><p>This table presents a quantitative comparison of different image denoising methods on three benchmark datasets (CSet, McMaster, and CBSD) using Gaussian noise with different standard deviations (œÉ). The results are shown in terms of Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM). The best results for each metric and dataset are highlighted, and the second-best results are underlined. The supplementary material includes results for Poisson noise removal.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/tables_17_1.jpg alt></figure></p><blockquote><p>This table shows the results of experiments conducted to evaluate the performance of the proposed pre-training strategy when applied to different network architectures. The table compares the PSNR values obtained using the proposed method with two different values of beta (0.9 and 0.99) across multiple iterations (800, 900, and 1200). The average inference time for each setting is also provided. The purpose is to demonstrate the generalizability and effectiveness of the proposed method across various network architectures.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/tables_18_1.jpg alt></figure></p><blockquote><p>This table compares the performance of the proposed method with and without adding Gaussian noise (œÉ=25) during pre-training. The results show that adding additional assumptions about the noise during pre-training leads to a decline in effectiveness, indicating the benefit of learning from the natural distribution of images without making additional assumptions about the noise.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/tables_19_1.jpg alt></figure></p><blockquote><p>This table shows the performance of the proposed Masked Pre-train then Iterative fill (MPI) method on different network architectures (DnCNN and ResNet). It demonstrates that the pre-training approach consistently improves performance across various network architectures, regardless of the beta value used (0.9 or 0.99). The results are presented in terms of PSNR and SSIM, and the average inference time is also shown.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/tables_20_1.jpg alt></figure></p><blockquote><p>This table presents a quantitative comparison of different image denoising methods on three benchmark datasets (CSet, McMaster, and CBSD) using Gaussian noise with different noise levels (œÉ = 10, 25, 50). The methods compared include DIP, N2V*, N2S*, ZS-N2N, FasterDIP, and the proposed method (Ours and Ours (faster)). The results are reported in terms of Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM). The best performing method for each noise level and dataset is highlighted, and the second-best performing method is underlined. Results for Poisson noise removal are available in the supplementary material.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/tables_20_2.jpg alt></figure></p><blockquote><p>This table presents a quantitative comparison of different image denoising methods on three benchmark datasets (CSet, McMaster, and CBSD) for Gaussian noise removal. The comparison is based on Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) metrics, which measure the quality of the denoised images. The table shows the performance of various methods at different levels of Gaussian noise (œÉ=10, 25, 50). The best results for each noise level and dataset are highlighted, with the second-best results underlined. Additional results for Poisson noise removal are available in the supplementary material.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/tables_21_1.jpg alt></figure></p><blockquote><p>This table presents a quantitative comparison of different image denoising methods on three datasets (CSet, McMaster, and CBSD) using Gaussian noise with different noise levels (œÉ = 10, 25, 50). The methods compared include DIP, N2V*, N2S*, ZS-N2N, FasterDIP, and the proposed method (Ours and Ours (faster)). For each method and dataset, the Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) are reported. The best results for each noise level are highlighted, and the second-best results are underlined. Additional results for Poisson noise removal are available in the supplementary material.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/tables_22_1.jpg alt></figure></p><blockquote><p>This table presents a quantitative comparison of the generalization performance of ensemble versions of N2V* and N2S* on the Kodak dataset. It evaluates the performance of these methods across five different noise types: Gaussian (œÉ=25), Gaussian (œÉ‚àà[10,50]), Poisson (Œª‚àà[10,50]), NLF, Speckle (v‚àà[10,50]), and S&amp;P (d‚àà[0.02,0.05]). The results show the average PSNR and SSIM scores achieved by each method for each noise type, providing insights into their generalization capabilities across various noise conditions.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/tables_22_2.jpg alt></figure></p><blockquote><p>This table presents a quantitative comparison of different image denoising methods on three real-world noisy image datasets: SIDD, PolyU, and FMD. The metrics used for comparison are PSNR and SSIM, common measures of image quality after denoising. The table also includes the average inference time for each method, showing the computational efficiency.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/oFgTScAsBr/tables_22_3.jpg alt></figure></p><blockquote><p>This table presents a quantitative comparison of different image denoising methods on three datasets (CSet, McMaster, and CBSD) using Gaussian noise with standard deviations of 10, 25, and 50. The methods compared include DIP, N2V*, N2S*, ZS-N2N, FasterDIP, and the proposed method (Ours and Ours (faster)). The results are reported in terms of PSNR and SSIM, showing the superior performance of the proposed method in most scenarios. The &lsquo;Supp.&rsquo; reference likely indicates additional results are available in a supplementary document. The table also includes average inference times, highlighting the speed advantage of the proposed method.</p></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-5158551105ee42f21ceab8d1f3359f33 class=gallery><img src=https://ai-paper-reviewer.com/oFgTScAsBr/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/oFgTScAsBr/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/oFgTScAsBr/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/oFgTScAsBr/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/oFgTScAsBr/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/oFgTScAsBr/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/oFgTScAsBr/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/oFgTScAsBr/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/oFgTScAsBr/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/oFgTScAsBr/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/oFgTScAsBr/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/oFgTScAsBr/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/oFgTScAsBr/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/oFgTScAsBr/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/oFgTScAsBr/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/oFgTScAsBr/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/oFgTScAsBr/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/oFgTScAsBr/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/oFgTScAsBr/19.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/oFgTScAsBr/20.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/neurips2024/posters/ofgtscasbr/&amp;title=Masked%20Pre-training%20Enables%20Universal%20Zero-shot%20Denoiser" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/neurips2024/posters/ofgtscasbr/&amp;text=Masked%20Pre-training%20Enables%20Universal%20Zero-shot%20Denoiser" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/neurips2024/posters/ofgtscasbr/&amp;subject=Masked%20Pre-training%20Enables%20Universal%20Zero-shot%20Denoiser" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_posters/oFgTScAsBr/index.md",oid_likes="likes_posters/oFgTScAsBr/index.md"</script><script type=text/javascript src=/neurips2024/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/neurips2024/posters/zaxumqoaf4/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced Extrapolation in LLMs</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-09-26T00:00:00+00:00>26 September 2024</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/neurips2024/posters/zjremskvyh/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Marginal Causal Flows for Validation and Inference</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-09-26T00:00:00+00:00>26 September 2024</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2024
AI Paper Reviewer</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/neurips2024/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/neurips2024/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>