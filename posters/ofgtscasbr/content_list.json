[{"type": "text", "text": "Masked Pre-training Enables Universal Zero-shot Denoiser ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xiaoxiao $\\mathbf{M}\\mathbf{a}^{1*}$ Zhixiang Wei1\u2217 Yi Jin1\u2217 Pengyang Ling1,2 Tianle Liu1 Ben Wang1 Junkang Dai1 Huaian Chen1\u2020 ", "page_idx": 0}, {"type": "text", "text": "1 University of Science and Technology of China 2 Shanghai AI Laboratory {xiao_xiao,zhixiangwei,lpyang27,tleliu,wblzgrsn,junkangdai,anchen}@mail.ustc.edu.cn {jinyi08}@ustc.edu.cn ", "page_idx": 0}, {"type": "image", "img_path": "oFgTScAsBr/tmp/1cd744946291643e72c77448a4e70bbb9821662489e231eb1d1d4ecfa35ab062.jpg", "img_caption": ["Figure 1: (a) Ours surpasses current zero-shot methods with reduced inference time (on CSet with Gaussian $\\scriptstyle\\sigma=25$ , see Sec. 3.2). (b) It shows better generalization across different noise types than current zero-shot & supervised/unsupervised methods (Sec. 3.3). (c) And can remove spatial correlated real-world noise, results are from SIDD benchmark [1] and FMD [2] (Sec. 3.4, Sec. 3.5). "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this work, we observe that model trained on vast general images via masking strategy, has been naturally embedded with their distribution knowledge, thus spontaneously attains the underlying potential for strong image denoising. Based on this observation, we propose a novel zero-shot denoising paradigm, i.e., Masked Pre-train then Iterative fill (MPI). MPI first trains model via masking and then employs pre-trained weight for high-quality zero-shot image denoising on a single noisy image. Concretely, MPI comprises two key procedures: 1) Masked Pre-training involves training model to reconstruct massive natural images with random masking for generalizable representations, gathering the potential for valid zero-shot denoising on images with varying noise degradation and even in distinct image types. 2) Iterative fliling exploits pre-trained knowledge for effective zeroshot denoising. It iteratively optimizes the image by leveraging pre-trained weights, focusing on alternate reconstruction of different image parts, and gradually assembles fully denoised image within limited number of iterations. Comprehensive experiments across various noisy scenarios underscore the notable advances of MPI over previous approaches with a marked reduction in inference time. Code available at https://github.com/krennic999/MPI. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Image denoising [3,4], as a branch of image restoration, has been the subject of extensive exploration. The prevalent approach to restore noise-degraded images is learning from multiple noisy instances. Nonetheless, both supervised learning from noisy-clean pairs [5\u20137] and unsupervised training $[8-$ 10] necessitate the collection of additional noisy datasets. Moreover, such methods may foster dependencies on specific patterns or intensities of training noise, hindering their performance in unfamiliar noise situations [11,12]. ", "page_idx": 1}, {"type": "text", "text": "As an alternative, zero-shot approaches [13\u201315] attempt to train network on a single noisy image for denoised output, negating the need for additional noisy data collection. Dedicated to obviating concerns about generalization issues, these techniques include blind-spot networks that reconstruct from corrupted inputs [16,17], DIPs [13,18\u201321] which exploit the characteristics of deep networks to learn the mapping from random noise to noisy images, as well as sub-sample based strategies [22,23] utilize spatial correlations to generate training pairs from sub-sampled instances. ", "page_idx": 1}, {"type": "text", "text": "However, current zero-shot methods train new networks from scratch for each noisy image, which presents two major issues: 1) Despite success in current zero-shot approaches rely on regularization or designed priors such as noise perturbations [13], under-parameterized networks [18,22], dropoutensemble [14] and blind-spot networks [16], the limited information from a single image to train network often lead to overly blurred content, noise artifacts or sub-optimal quality. Several methods tend to rely on known noise distribution [3,20,24] for more information, but their applicability is limited. 2) Training a new network from scratch for each noisy image is time-consuming. Existing zero-shot methods typically require several minutes [13] or more [14]. And attempts at faster zero-shot denoising [22,23] often compromise on performance. ", "page_idx": 1}, {"type": "text", "text": "Compared to previous zero-shot approaches, learning the feature distribution from vast natural images offers a more intuitive approach. This is grounded in two considerations: Real natural images are both abundant and readily available, and despite variations in noise patterns, many natural images share common characteristics [25]. We seek to enhance zero-shot denoising with minimal reliance on pre-defined priors or regularization, aiming for a better startpoint for various noise patterns instead of from scratch. To this end, we delve into the potential of masked image modelling [26,27] on natural images with no assumptions about noisy patterns and intensities [28]. Specifically, we make the following observation: combined with a simple ensemble operation, a masked pre-trained model can naturally denoise images with unseen noise degradation. ", "page_idx": 1}, {"type": "text", "text": "Building upon above observation, we introduce a zero-shot denoising paradigm, i.e., Masked Pre-train then Iterative flil (MPI). MPI first pre-trains a model on ImageNet with pixel-wise masking strategy, then the pre-trained model is optimized on a single image with unseen noise for denoised prediction in zero-shot inference stage. The optimization goal in inference is designed to predict masked regions, and only predictions of masked areas are preserved for denoised prediction, thereby minimizing the gap between pre-training and inference. The pre-trained weights provide more generic knowledge, preventing premature over-fitting during inference and reducing the need for strong regularization. We are able to handle a wider range of noise scenarios with less information about noise patterns or intensities. Remarkably, we find that extracted representation can even generalize to medical images that distinctly different from natural ones [2]. It also offers a better startpoint than scratch training, enabling high-quality denoising around 10 seconds, underscoring the potential of our method in practical application. The main contributions of this paper are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce a novel zero-shot denoising paradigm, i.e., Masked Pre-train then Iterative fill (MPI), which introduces masked pre-training in this context for the first time, simultaneously improving both image quality and inference speed on unseen noisy images. \u2022 We develop a pre-training scheme with pixel-wise random masks to capture distribution knowledge of natural images. Based on pre-trained knowledge, we propose iterative filling for zero-shot inference on a specific noisy image. This process is optimized using pre-trained weights, and focuses on alternatively reconstruct different parts of noisy image, predictions in iterations are sequentially assembled for high-quality denoised output with efficiency. \u2022 Extensive experiments demonstrate MPI\u2019s superiority, efficiency and robustness in diverse noisy scenarios. In nutshell, MPI achieves significant performance gains across various noise types with reduced inference time, highlighting its potential for practical applications. ", "page_idx": 1}, {"type": "text", "text": "2 Methods ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In Sec. 2.1, we first investigate the properties of models trained with masking, proving that models trained with masking strategy can learn representations beneficial for denoising. Our observation lead us to propose a zero-shot denoising paradigm that includes pre-training (Sec. 2.2) and iterative optimizing (Sec. 2.3). We further illustrate how to remove spatially correlated real noise in Sec. 2.4. ", "page_idx": 2}, {"type": "image", "img_path": "oFgTScAsBr/tmp/a8bc5dfee8533aa53adbdc81c28833902a0d3ea9fc6e07eedc6e88393f9e40c5.jpg", "img_caption": ["Figure 2: Example of model trained on ImageNet with $70\\%$ pixel-wise masking, denoised image is obtained by directly ensemble of predictions from fixed pre-trained weights (\u201cDirectly ensemble\u201d), its performance can be further improved with iterative filling (\u201c+Zero-shot Optim.\u201d). "], "img_footnote": [], "page_idx": 2}, {"type": "image", "img_path": "oFgTScAsBr/tmp/dbf675afd314e529575bf9e2e0acb95edbe5a62e5cbbe84f76f11867c135ed63.jpg", "img_caption": ["Figure 3: Evaluation on an ImageNet subset shows pre-trained model\u2019s inherent denoising ability, but performance limited without optimization. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "2.1 Motivation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Masked Image Modeling [26,27,29] has significantly advanced computer vision by training on vast natural image sets to grasp their knowledge distributions. It shows great potential applicability under diverse scenarios and have been proven beneficial for high-level downstream tasks [30,31]. ", "page_idx": 2}, {"type": "text", "text": "To further explore its capability in denoising, we train a model on natural images with pixel-wise random masks (for details, see Sec. 2.2) and assess its performance against a target image with unseen noise distribution. Surprisingly, we observe that a simple average of predictions from a fixed-state trained model can denoise on unseen noise, as shown in Fig. 3, sometimes achieve remarkably good performance, as an example is presented in Fig. 2. This observation suggests that a masked pre-trained model can serve as a natural image denoiser. However, artifacts exist in the results, which can be attributed to lack of knowledge about specific degradation patterns in the target image. ", "page_idx": 2}, {"type": "text", "text": "Drawing on prior insight, we develop an efficient zero-shot denoising pipeline, leveraging pre-trained knowledge by incorporating noise characteristics from a single noisy image (Fig. 4), i.e., Masked Pre-train then Iterative flil. The model is firstly pre-trained with random masks $M$ and corresponding element-wise negation $\\hat{M}$ to acquire natural image distributions, formulated as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\arg\\operatorname*{max}_{\\theta}p(I\\odot\\hat{M}|I\\odot M;\\theta),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "for $I$ indicates natural image without any degradation priors, typically sourced from extensive datasets (e.g.ImageNet [28]). We use element-wise multiplication $\\left(\\odot\\right)$ . For denoising on specific noisy image $x$ , pre-trained parameter $\\theta$ is loaded and further optimized with known $x$ from $\\scriptstyle t=1$ to $t{=}T$ for $T$ iterations, and predictions are aggregated for final prediction $\\overline{y}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\overline{{y}}=E n s e m b l e\\{\\mathcal{D}_{\\theta_{t}}(x)\\}_{t=1}^{T},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{D}_{\\theta_{t}}(\\cdot)$ is network parameterized by $\\theta_{t}$ , optimized from pre-trained $\\theta$ . Masked Pre-training process is detailed in Sec. 2.2 and Ensemble in Sec. 2.3. ", "page_idx": 2}, {"type": "text", "text": "2.2 Masked Pre-training ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Masking strategy. Given the distinct requirements between low-level and high-level tasks in \u201csemantics\u201d [32], we implement specialized masking strategy to achieve finer-grained image representations, i.e., a pixel-wise masking strategy. Specifically, given an input image $I\\in\\mathbb{R}^{H\\times\\breve{W}\\times C^{\\prime}}$ divided into random patches of size 1, a subset of them are randomly replaced by mask token with probability $p$ (for further discussion of $p$ , see Sec. 4). When the mask token is set to 0, the masked image $M\\odot I$ with random mask $M\\in\\dot{\\mathbb{R}}^{H\\times W\\times C}$ corresponds to a bernoulli sampling of the input image $I$ . For ", "page_idx": 2}, {"type": "image", "img_path": "oFgTScAsBr/tmp/44efb5e6bad69fcd267726ff0c3d7fd305f4218f66f3ceb7ebd9da0f5d8d1c3c.jpg", "img_caption": ["Figure 4: An overview of the proposed MPI paradigm consisting Masked Pre-training and Iterative filling. During pre-training $\\mathcal{D}_{\\theta}(\\cdot)$ learns to reconstruct masked natural images. And the pre-trained weights $\\theta$ are saved for zero-shot denoise, i.e., Iterative filling, to denoise a specific noisy image $x$ . During zero-shot inference, network is initialized with pre-trained weights $\\theta$ , then the weights are further optimized on $x$ for $T$ steps, results from $t$ -th $(t{=}1,2,\\ldots,T{-}1)$ optimizing steps are gathered to obtain final denoised prediction $\\overline{y}$ . Compared to current zero-shot methods, just adding one more step to load a pre-trained model enables faster and high-quality zero-shot denoising. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "each element $M_{[k]}$ in $M$ , we have: ", "page_idx": 3}, {"type": "equation", "text": "$$\nM_{[k]}=\\left\\{\\begin{array}{l l}{0,}&{w i t h\\;p r o b.\\;\\;p;}\\\\ {1,}&{w i t h\\;p r o b.\\;\\;1-p.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Pre-training scheme. During pre-training, the network $\\mathcal{D}_{\\theta}(\\cdot)$ is trained to learn recovering natural image $I$ itself with random mask $M$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\tilde{I}={\\mathcal{D}}_{\\theta}(M\\odot I).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We set the same optimization strategy outlined in [27], focusing loss computation on masked prediction areas $\\tilde{I}$ . This directs network efforts towards reconstructing these specific regions, with the reconstruction loss denoted as $L_{r e c}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nL_{r e c}(\\tilde{I},I)=\\left\\|\\hat{M}\\odot\\tilde{I}-\\hat{M}\\odot I\\right\\|_{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The Mean Squared Error (MSE) loss is adopted to learn relatively smoother representation. For the architecture of network $\\mathcal{D}(\\cdot)$ , we employ the same U-shaped hourglass architecture as in DIP [13], which has been proven a powerful zero-shot denoising architecture [19]. Furthermore, its relatively small parameter configuration enables accelerated training, alleviating potential inference computational costs and rendering it more appropriate for zero-shot denoising tasks. ", "page_idx": 3}, {"type": "text", "text": "Algorithm 1: Iterative filling. Pipeline designed to leverage pre-trained representation $\\theta$ for zero-shot denoising. ", "page_idx": 3}, {"type": "text", "text": "Input: Noisy image $x$ , pre-trained parameter $\\theta$ , network $\\mathcal{D}(\\cdot)$ , exponential weight $\\beta$ , masking ratio $p$ .   \nOutput: denoised ensemble $\\overline{y}$ from predictions of iteration $\\{y_{t}\\}$ .   \nload pre-trained parameter $\\theta$ for $\\mathcal{D}(\\cdot)$ as $\\theta_{1}$   \ninitialize $\\overline{{y}}$   \nfor $t$ from 1 to $T$ do generate random mask $M_{t}$ with mask ratio $p$ $\\begin{array}{r l}&{\\breve{y}_{t}=\\mathcal{D}_{\\theta_{t}}\\big(M_{t}\\odot x\\big)}\\\\ &{\\hat{M}_{t}=-M_{t}}\\\\ &{\\theta_{t+1}=\\theta_{t}-\\nabla_{\\theta}\\left\\|\\hat{M}_{t}\\odot y_{t}-\\hat{M}_{t}\\odot x\\right\\|_{2}}\\\\ &{\\overline{{y}}\\leftarrow\\hat{M}_{t}\\odot(\\beta\\cdot\\overline{{y}}+(1-\\beta)\\cdot y_{t})+M_{t}\\odot\\overline{{y}}}\\end{array}$ ", "page_idx": 3}, {"type": "image", "img_path": "oFgTScAsBr/tmp/a1bb6e2295a03e7d8baecafbbb93802a0413c5cc33a351935c14c32137e4a15b.jpg", "img_caption": ["Figure 5: Qualitative denoising results on Gaussian and Poisson noise. The quantitative PSNR/SSIM results are provided underneath. Noisy patches are from CBSD-44 and McMaster-14, respectively. Best viewed in color (zoom-in for better comparison). "], "img_footnote": [], "page_idx": 4}, {"type": "table", "img_path": "oFgTScAsBr/tmp/fae00d070ba9c2d21ebf9c45662248ad9671994f28b26359ed03eb4af363164d.jpg", "table_caption": ["Table 1: Quantitative comparison on CSet, McMaster & CBSD for Gaussian noise removal. Best results highlighted and second underlined. See Supp. for poisson noise removal. "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "2.3 Iterative Filling ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Overall design. As observed in Sec. 2.1, an iterative optimization process is designed to leverage pre-trained knowledge for zero-shot denoising. Unlike other MIM approaches [26,27] that fine-tune with entire images as input, since only one noisy image is accessible, we employ a self-supervised manner to learn the mapping from a noisy image to itself. However, this direct self-mapping approach introduces significant gap between the zero-shot inference stage and pre-training stage and lacks constraints for learning a noise identity mapping. ", "page_idx": 4}, {"type": "text", "text": "Considering above challenges, we retain the same masking strategy in Sec. 2.2 for both input and loss computation, i.e., network still learns to reconstruct masked regions, but from single noisy image rather than natural images. This leads to a pixel-based iterative refinement process, which resembles mechanism of blind-spot networks [16]. Specifically, for input noisy image $x$ , random mask $M_{t}$ and its element-wise negation $\\hat{M_{t}}$ in $t$ -th iteration, prediction $y_{t}$ and aggregated result $\\overline{{y}}$ can be derived: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle y_{t}=\\mathcal{D}_{\\theta_{t}}(M_{t}\\odot x);}}\\\\ {{\\displaystyle\\overline{{y}}=\\sum_{t}a_{t}\\cdot y_{t}\\odot\\hat{M}_{t},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\theta_{t}$ denotes network parameter at iteration $t$ , $a_{t}$ is corresponding coefficient where $\\textstyle\\sum_{t}a_{t}=1$ . The optimization objective at each iteration is as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\underset{\\theta_{t}}{\\operatorname{arg\\,min}}\\left\\|\\hat{M}_{t}\\odot y_{t}-\\hat{M}_{t}\\odot x\\right\\|_{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The optimization task, represented by $L_{r e c}(y_{t},x)$ , learns to reconstruct noisy image cropped by random masks, aligns with pre-training. The alignment minimizes the gap between pre-training and zero-shot inference to avoid over-fitting, and reduces the inference steps required, thus accelerating the denoising process. Thanks to the well-crafted mechanism, we can accomplish high-quality results with preserved details in reduced time without any other regularization. ", "page_idx": 4}, {"type": "text", "text": "Pixel-based iterative refinement. For a lower mask ratio and reconstruction of more detailed images, we abandon constraints on unmasked regions in previous optimization goals (Eq. 5 and Eq. 7), thus making information under these areas unreliable, we preserve only the results corresponding to M\u02c6 for final denoised outcome $\\overline{{y}}$ . However, as one forward pass provide partial denoising results, an ensemble process is crucial. Specially, we employ an Exponential Moving Average (EMA) strategy to optimize the use predictions during iterations with little increase in inference time (Sec. 4): ", "page_idx": 4}, {"type": "image", "img_path": "oFgTScAsBr/tmp/4773b4a18262df7b450742fdebf23c811698efb306d95fdc9881ddaf3ae62b59.jpg", "img_caption": ["Figure 6: Qualitative results on unseen noise types. Restormer is trained with Gaussian $\\scriptstyle\\sigma=25$ . Noisy patches are from kodim07 and kodim12. "], "img_footnote": [], "page_idx": 5}, {"type": "table", "img_path": "oFgTScAsBr/tmp/c82054c5011d16ed215736ec2e4ddb22e828ef977ee399dcebd03cf8232a30c9.jpg", "table_caption": ["Table 2: Quantitative generalization evaluation results on Kodak. All supervised/unsupervised methods trained on $\\scriptstyle\\sigma=25$ Gaussian, tested on 5 unseen noise types. (Average from all 6 settings.) "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\overline{{\\boldsymbol{y}}}=\\hat{M}_{t}\\odot(\\boldsymbol{\\beta}\\cdot\\overline{{\\boldsymbol{y}}}+(1-\\boldsymbol{\\beta})\\cdot\\boldsymbol{y}_{t})+M_{t}\\odot\\overline{{\\boldsymbol{y}}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "For an in-depth look at proposed ensemble algorithm, see Alg. 1. During inference, the pre-trained weights not only provide a better startup but also act as regularization for the network, preventing from over-fitting too early and leading to better performance with less inference time (Sec. 4). ", "page_idx": 5}, {"type": "text", "text": "2.4 Adaptation to Real-world Noise Removal ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Real-world noise exhibit strong spatial correlations, i.e.the noise is correlated across adjacent pixels. In such scenarios, employing a straightforward masking mechanism still allows the model to learn information related to noise patterns. To address this problem, we apply larger masking ratios than that used for synthetic noise. Additionally, we integrate a simple Pixel-shuffle Down-sampling (PD) mechanism during zero-shot inference to reduce spatial correlation in noise. ", "page_idx": 5}, {"type": "text", "text": "Specifically, instead of directly processing noisy image $\\boldsymbol{x}\\in\\mathbb{R}^{1\\times C\\times H\\times W}$ in Eq. 6a, we handle its down-sampled versions $\\begin{array}{r}{D o w\\bar{n}(\\bar{x})\\in\\mathbb{R}^{d^{2}\\times C\\times\\frac{\\bar{H}}{d}\\times\\frac{W}{d}}}\\end{array}$ using simple Pixel-shuffle with factor $d$ , and $d^{2}$ sub-samples are concatenated along batch dimension for joint denoising. Following the same iterative fliling mechanism described above, we apply pixel unshuffle to the denoised result $\\overline{{y}}$ to obtain final denoised outcome $U p({\\overline{{y}}})$ . We add minimal PD operations to address spatial correlated noise, illustrating the effect of pre-trained weights, performance on real-world noisy dataset can be improved by applying better sub-sampling approaches [35\u201337] (Sec. 4.2) as they have been intensively studied. ", "page_idx": 5}, {"type": "text", "text": "3 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We assess our method against typical methods including DIP [13], Noise2Void (N2V) [16], Noise2Self (N2S) [17], Zero-Shot Noise2Noise (ZS-N2N) [22], and FasterDIP [19]. We modify N2V and N2S to single-image version $(\\mathrm{N}2\\mathrm{V}^{*}$ and $\\mathrm{N}2\\mathrm{S}^{\\ast}$ ). EMA ensemble result of DIP and FasterDIP are reported with their official code. Refer to supplementary material (Supp.) for EMA results of $\\mathrm{N}2\\mathrm{V}^{*}$ and $\\mathrm{N}2\\mathrm{S}^{\\ast}$ , and comparison with more DIP-based [20], diffusion-based [41,42], zero-shot modifications from unsupervised methods [36,43,44]. Only non-ensemble ZS-N2N is presented due to negligible performance differences with EMA version. We compare Peak Signal-to-Noise Ratio (PSNR) and ", "page_idx": 5}, {"type": "image", "img_path": "oFgTScAsBr/tmp/eaf89d90af36124ea58e726c57adb31ab3f4069694b39e799ed3f7d56d92881b.jpg", "img_caption": ["36.61/0.938 PSNR/SSIM 36.23/0.942 35.63/0.940 30.28/0.926 36.65/0.941 36.89/0.950 37.29/0.961 38.12/0.965 Figure 7: Qualitative results on real noise removal from SIDD and PolyU. Noisy patches are from SIDDval31_1 and Canon80D_8_8_3200_ball_16. "], "img_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "oFgTScAsBr/tmp/1872fce61e1240dee33dacc759d93a35235590b923fa2a9f5e49aa4c14a5155f.jpg", "table_caption": ["Table 3: Quantitative comparison on SIDD, PolyU and FMD for real noise removal. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Structure Similarity Index Measure (SSIM) on synthetic (Sec. 3.2, Sec. 3.3) and real noise (Sec. 3.4).   \nAdditional tests on medical images (Sec. 3.5) show our method\u2019s adaptability beyond natural images. ", "page_idx": 6}, {"type": "text", "text": "3.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Pre-training. Pre-training is performed on two Nvidia RTX 3090 GPUs using Adam optimizer with $\\beta_{1}{=}0.9$ and $\\beta_{2}{=}0.9$ . Initial learning rate is $2e^{-3}$ and decays to $1e^{-5}$ with cosine annealing strategy over 80K iterations with a batch size of 64. We initiate pre-training on randomly cropped $256\\!\\times\\!256$ patches from subset of ImageNet [28] with around 48,000 images. Two sets of pre-trained weights with masking probability $p$ ${\\it p}{=}0.3$ for synthetic noise and a higher ratio of $0.8{\\sim}0.95$ for spatially correlated noise) are trained. Further discussion of $p$ is in Sec. 4. ", "page_idx": 6}, {"type": "text", "text": "Zero-shot inference. We set learning rate during inference to $2e^{-3}$ , and same masking ratio $p$ as pre-training (0.3 for synthetic, $0.8{\\sim}0.95$ for real noise) is set. EMA weight $\\beta{=}0.99$ for 1000 iterations (specially, 800 iterations for SIDD). Additionally, with $\\beta{=}0.9$ , we achieve performance surpassing most zero-shot methods within 200 iterations, denoted as \u201cfaster\u201d. See Supp. for detailed setting. ", "page_idx": 6}, {"type": "text", "text": "3.2 Gaussian & Poisson Noise ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We investigate Gaussian Noise with $\\sigma{\\in}[10,\\!25,\\!50]$ and Poisson noise with $\\lambda{\\in}[10,25,50]$ separately on three datasets: CSet [3], McMaster [33] and CBSD [34], with 9, 18 and 68 high-quality images, respectively. Results are shown in Table 1. The model is tested across various noise types with same experimental setups, without prior knowledge of noise distribution or intensity. ", "page_idx": 6}, {"type": "text", "text": "Analysis. DIP tends to produce over-blurry results and struggles especially with intense noise. While ZS-N2N manages to remove weak noise, its simple down-sample approach falters with stronger noise and cause artifacts. As Fig. 5 illustrates, under Gaussian noise $\\scriptstyle\\sigma=25$ and Poisson noise $\\lambda{=}25$ , our method excels in both noise reduction and detail preservation. In some cases, we see an improvement of over 1dB, highlighting the effectiveness of our zero-shot paradigm. ", "page_idx": 6}, {"type": "text", "text": "Average inference time is listed in Table 1. Our \u201cfaster\u201d version achieve the fastest inference speed while surpassing comparing methods in most cases. Even with $\\beta\\:=\\:0.99$ , our method exhibits competitive inference time and significantly better performance. Params and FLOPs are in Supp. ", "page_idx": 6}, {"type": "image", "img_path": "oFgTScAsBr/tmp/b5d53306082f89167887d0536c04c9ac23ff601a338219ada27684ca8cfc9da3.jpg", "img_caption": ["Figure 8: Validation of pre-trained representations on image content differs from natural images. Comparing between \u201cBaseline\u201d (w/o pre-train) and \u201cOurs\u201d (w pre-train). Noisy patch is from TwoPhoton_MICE_3. See quantitative comparison at Table 4. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "oFgTScAsBr/tmp/4c1193e80d33cb5379c11490b1899a06db4d0de4c3bfb0ee8b695605dba637dd.jpg", "img_caption": ["Figure 9: Effect of pre-trained model. Examples using Gauss $\\scriptstyle\\sigma=25$ removal on $\\mathrm{F\\_}16$ with $\\beta{=}0.99$ . Pre-trained results are labeled in orange, while default initialized results are labeled in blue. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "3.3 Generalization on Unseen Noise ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We believe zero-shot denoising with natural image knowledge offers new perspectives on improving generalizability of denoising methods. We select several recent supervised (SwinIR [7], Restormer [38]) and unsupervised (Neighbor2Neighbor [10], Blind2Unblind [39]) methods trained on Gaussian with $\\scriptstyle\\sigma=25$ for demostration. Testing them on 5 unknown noise types on Kodak [46]. ", "page_idx": 7}, {"type": "text", "text": "Analysis. As illustrated in Table 2 and Fig. 6, although methods trained on multiple noisy images achieve better results on noisy cases with the same distribution, they exhibit poor generalization performance. In contrast, zero-shot methods often perform better generalization capabilities, especially our method which achieves the best performance across all types of generalization noise. ", "page_idx": 7}, {"type": "text", "text": "3.4 Real Noisy Datasets ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We assess denoising capability of MPI on synthetic noise in previous experiments. However, realworld noise is more complicated and challenging. We test on SIDD [1] and PolyU [45] datasets, including 1280 patches from the SIDD validation and 1280 from SIDD benchmark, and all 100 official patches from PolyU to show our paradigm on real images. Due to the differences between synthetic and real noise, we report results from comparison methods from their optimal iteration. ", "page_idx": 7}, {"type": "text", "text": "Analysis. As shown in Table 3, our method excels over other zero-shot approaches on both datasets. This underlines our method\u2019s effectiveness on real-world noise removal. Fig. 7 show our method\u2019s capability to balance noise removal and detail retention. In essence, our method is adept at real-world denoising, offering a robust solution for image quality enhancement in challenging situations. ", "page_idx": 7}, {"type": "text", "text": "3.5 Generalization to Medical Images ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The pre-trained model, which has learned the feature distributions of natural images, raises a question: Can this knowledge be applied to other image types? To answer this question, we select a fluorescence microscopy dataset (FMD) [2] characterized by colors and textures distinctly different from natural images, using all released 48 images in testset for evaluation. See Supp. for more image types. ", "page_idx": 7}, {"type": "text", "text": "Analysis. Our method still excels in denoising performance, as seen in Table 3. Despite such monochromatic microscopic images are not included in pre-training dataset and exhibits large differences between natural images, pre-trained knowledge still enhances zero-shot denoising performance, as evidenced in Fig. 8 and Table 4, demonstrating the generalizability of pre-trained weights. ", "page_idx": 7}, {"type": "image", "img_path": "oFgTScAsBr/tmp/ed43b683f0831d28d8cfb562aeabd1c41bd9c5959b8a52adcb8c76666929347c.jpg", "img_caption": ["Figure 10: CKA (above) [47] and PCA (below) visualization of features extracted from the final timestep of model. Distribution of pre-trained model (\u201cPretrained\u201d) and from scratch (\u201cBaseline\u201d) during inference is significantly different in last layers. Pre-trained model tends to restore the complete image, while the baseline model primarily focusing on restoring masked regions only. ", "Table 4: Ablation of pre-training, with default settings noted in gray . \u201cSIDD\u201d denotes SIDD validation. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "oFgTScAsBr/tmp/fbe7ba51e80803556cdcd650b36a8e57beb61a61140af473eeb836b284d531e8.jpg", "img_caption": ["Figure 11: Effect of masking ratios. $30\\%$ balances noise removal and prevents oversmoothing for synthetic noise. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "oFgTScAsBr/tmp/0aaf0f3cd644d0904c191c9c9e369cb8f300ad4418c9c0df5de05fcf09981e49.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4 Ablation Study & Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "4.1 Ablation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Pre-trained weights. Building on Sec. 2.1, we question the role of pre-trained weights in zero-shot inference by comparing inference with pre-trained weights and optimizing from scratch, resembling a standard blind-spot network [16]. As depicted in Fig. 9 and Table 4, the latter quickly peaks and risks over-ftiting due to the simple task of content recovery from masked images, making it challenging to specify an optimal iteration for all images. Conversely, the pre-trained model achieves better initial performance and maintains close-to-optimal performance for more extended period of time. ", "page_idx": 8}, {"type": "text", "text": "Unlike other zero-shot techniques train models from scratch for a single image to learn noise-resistant image content, we offer a new perspective by showing that a pre-trained model can aid in zero-shot tasks. The pre-trained weights, encapsulating views from multiple natural images, making it more robust to iteration count and provides better options for faster zero-shot denoising. ", "page_idx": 8}, {"type": "text", "text": "Moreover, We investigated the impact of pre-training on inference at the hidden layer level (see Fig. 10). Features extracted with pre-trained weights exhibit significant divergence from those produced by the baseline, i.e., the usual zero-shot denoising approach. Specifically, pre-trained model restores the complete image, with more distinct features between layers, whereas the baseline model\u2019s features are less differentiated between layers, tending to only restore the masked parts, which may result in sub-optimal convergence towards local minima. ", "page_idx": 8}, {"type": "text", "text": "Masking ratios. Fig. 11 shows impact of different masking ratios on denoising with Gaussian $\\scriptstyle\\sigma=25$ . Lower masking ratios fails to completely remove noise, while higher masking ratios can cause overly smoothed results. A $30\\%$ masking ratio balances detail preservation and noise reduction for synthetic noise. However, a higher $p$ of $0.8{\\sim}0.95$ is required for real-world noise. See Supp. for more details. ", "page_idx": 8}, {"type": "text", "text": "Ensemble strategy. We explore ensemble strategies, including EMA-based (\u201cEMA\u201d), straightforward averaging during iterations (\u201cAverage\u201d) and average after specific optimization step (\u201cAvg after $500\\mathrm{e^{\\circ}}$ , where 500 is optimal). Due to the inability to obtain predictions for all pixels in a single forward pass (Sec. 2.3), the \u201cw/o Ensemble\u201d result comes from the final prediction for each pixel. And \u201cLast\u201d provided for results of final forward prediction, a significant performance drop is caused by unreliable pixels (For details, see Supp.). Additionally, to validate our mask-based ensemble strategy, we remove masks from Eq. 7 and Eq. 8, with full-pixel loss and ensemble (\u201cEMA w/o Mask\u201d). See results in Table 5. Proposed \u201cEMA\u201d achieve significant better performance, aiding denoising with efficiency. ", "page_idx": 8}, {"type": "text", "text": "4.2 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our method uses masking and minimal PD during inference to highlight pre-training\u2019s role without explicit regularization. We now explore further enhancements during inference with more strategies. ", "page_idx": 8}, {"type": "table", "img_path": "oFgTScAsBr/tmp/d016cdf329492977d67e3bb6513077955e711f63e207a365a312e9c9ef5a95bf.jpg", "table_caption": ["Table 5: Ablation of ensemble strategy. \u201cTime (s)\u201d denotes Infer. time (s) "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "oFgTScAsBr/tmp/d5ab24740035073af4ea993853bf710b8482035d11d73e07d34769a4b53a62b8.jpg", "table_caption": ["Table 6: Discussion on over-fitting. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Over-fitting. Although pre-training mitigates over-fitting for synthetic and real noise, the overparameterized network may still learn noise patterns over time due to lack of explicit mechanisms to avoid identity mapping. This is common challenge in many zero-shot models, we suggest earlystopping [21] (Table 6,\u201cES\u201d for early-stopping) to avoid over-fitting and reduce inference time. ", "page_idx": 9}, {"type": "text", "text": "Additionally, we compare with other over-fitting prevention methods, e.g., TV regularization of output and augmentation to input image. These approaches either resulted in suboptimal performance or longer inference times. More details on over-ftiting and prevention strategies can be found in Supp. ", "page_idx": 9}, {"type": "text", "text": "Sub-sampling. Shown in Sec. 2.4, minimal pixel-shuffle is used to reduce spatial correlation in real-world noise, but may cause chessboard artifact and reduce performance due to its regular downsampling strategy. Better down-sampling strategies have been widely studied, and here we choose RSG [37] to illustrate, see results at Table 7. For more comparison and visual comparison, see Supp. ", "page_idx": 9}, {"type": "text", "text": "5 Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported in part by the National Natural Science Foundation of China under Grant 62401532, in part by the Anhui Provincial Key Research and Development Plan 202304a05020072, and in part by the Anhui Provincial Natural Science Foundation 2308085QF226, in part by the Fundamental Research Funds for the Central Universities WK2090000065, and in part by the China Postdoctoral Science Foundation under Grant 2022M720137. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Abdelrahman Abdelhamed, Stephen Lin, and Michael S Brown. A high-quality denoising dataset for smartphone cameras. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1692\u20131700, 2018.   \n[2] Yide Zhang, Yinhao Zhu, Evan Nichols, Qingfei Wang, Siyuan Zhang, Cody Smith, and Scott Howard. A poisson-gaussian denoising dataset with real fluorescence microscopy images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11710\u201311718, 2019.   \n[3] Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, and Karen Egiazarian. Image denoising by sparse 3-d transform-domain collaborative filtering. IEEE Transactions on image processing, 16(8):2080\u20132095, 2007.   \n[4] Matteo Maggioni, Vladimir Katkovnik, Karen Egiazarian, and Alessandro Foi. Nonlocal transformdomain filter for volumetric data denoising and reconstruction. IEEE transactions on image processing, 22(1):119\u2013133, 2012.   \n[5] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian denoiser: Residual learning of deep CNN for image denoising. IEEE Transactions on Image Processing, 26(7):3142\u2013 3155, jul 2017.   \n[6] Kai Zhang, Wangmeng Zuo, and Lei Zhang. Ffdnet: Toward a fast and flexible solution for CNN based image denoising. IEEE Transactions on Image Processing, 2018.   \n[7] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. Swinir: Image restoration using swin transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1833\u20131844, 2021.   \n[8] Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala, and Timo Aila. Noise2noise: Learning image restoration without clean data. In International Conference on Machine Learning, pages 2965\u20132974. PMLR, 2018.   \n[9] Wenchao Du, Hu Chen, and Hongyu Yang. Learning invariant representation for unsupervised image restoration. In Proceedings of the ieee/cvf conference on computer vision and pattern recognition, pages 14483\u201314492, 2020.   \n[10] Tao Huang, Songjiang Li, Xu Jia, Huchuan Lu, and Jianzhuang Liu. Neighbor2neighbor: Self-supervised denoising from single noisy images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 14781\u201314790, 2021.   \n[11] Hao Chen, Chenyuan Qu, Yu Zhang, Chen Chen, and Jianbo Jiao. Multi-view self-supervised disentanglement for general image denoising. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12281\u201312291, 2023.   \n[12] Haoyu Chen, Jinjin Gu, Yihao Liu, Salma Abdel Magid, Chao Dong, Qiong Wang, Hanspeter Pfister, and Lei Zhu. Masked image training for generalizable deep image denoising. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1692\u20131703, 2023.   \n[13] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Deep image prior. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.   \n[14] Yuhui Quan, Mingqin Chen, Tongyao Pang, and Hui Ji. Self2self with dropout: Learning self-supervised denoising from single image. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1890\u20131898, 2020.   \n[15] Jun Cheng, Tao Liu, and Shan Tan. Score priors guided deep variational inference for unsupervised realworld single image denoising. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12937\u201312948, 2023.   \n[16] Alexander Krull, Tim-Oliver Buchholz, and Florian Jug. Noise2void-learning denoising from single noisy images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2129\u20132137, 2019.   \n[17] Joshua Batson and Loic Royer. Noise2self: Blind denoising by self-supervision. In International Conference on Machine Learning, pages 524\u2013533. PMLR, 2019.   \n[18] Reinhard Heckel and Paul Hand. Deep decoder: Concise image representations from untrained nonconvolutional networks. In International Conference on Learning Representations, 2018.   \n[19] Yilin Liu, Jiang Li, Yunkui Pang, Dong Nie, and Pew-Thian Yap. The devil is in the upsampling: Architectural decisions made simpler for denoising with deep image prior. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12408\u201312417, 2023.   \n[20] Yeonsik Jo, Se Young Chun, and Jonghyun Choi. Rethinking deep image prior for denoising. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5087\u20135096, 2021.   \n[21] Zenglin Shi, Pascal Mettes, Subhransu Maji, and Cees GM Snoek. On measuring and controlling the spectral bias of the deep image prior. International Journal of Computer Vision, 130(4):885\u2013908, 2022.   \n[22] Youssef Mansour and Reinhard Heckel. Zero-shot noise2noise: Efficient image denoising without any data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 14018\u201314027, June 2023.   \n[23] Jason Lequyer, Reuben Philip, Amit Sharma, Wen-Hsin Hsu, and Laurence Pelletier. A fast blind zero-shot denoiser. Nature Machine Intelligence, 4(11):953\u2013963, Nov 2022.   \n[24] Markku Makitalo and Alessandro Foi. Optimal inversion of the anscombe transformation in low-count poisson image denoising. IEEE Transactions on Image Processing, 20(1):99\u2013109, 2011.   \n[25] Anish Mittal, Rajiv Soundararajan, and Alan C Bovik. Making a \u201ccompletely blind\u201d image quality analyzer. IEEE Signal processing letters, 20(3):209\u2013212, 2012.   \n[26] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000\u201316009, 2022.   \n[27] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple framework for masked image modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9653\u20139663, 2022.   \n[28] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \n[29] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. In International Conference on Learning Representations, 2021.   \n[30] Yunyao Mao, Jiajun Deng, Wengang Zhou, Yao Fang, Wanli Ouyang, and Houqiang Li. Masked motion predictors are strong 3d action representation learners. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10181\u201310191, 2023.   \n[31] Jiang-Tian Zhai, Xialei Liu, Andrew D Bagdanov, Ke Li, and Ming-Ming Cheng. Masked autoencoders are efficient class incremental learners. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 19104\u201319113, 2023.   \n[32] Yihao Liu, Anran Liu, Jinjin Gu, Zhipeng Zhang, Wenhao Wu, Yu Qiao, and Chao Dong. Discovering distinctive\" semantics\" in super-resolution networks. arXiv preprint arXiv:2108.00406, 2021.   \n[33] Lei Zhang, Xiaolin Wu, Antoni Buades, and Xin Li. Color demosaicking by local directional interpolation and nonlocal adaptive thresholding. Journal of Electronic imaging, 20(2):023016\u2013023016, 2011.   \n[34] David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001, volume 2, pages 416\u2013423. IEEE, 2001.   \n[35] Yuqian Zhou, Jianbo Jiao, Haibin Huang, Yang Wang, Jue Wang, Honghui Shi, and Thomas Huang. When awgn-based denoiser meets real noises. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 13074\u201313081, 2020.   \n[36] Wooseok Lee, Sanghyun Son, and Kyoung Mu Lee. Ap-bsn: Self-supervised denoising for real-world images via asymmetric pd and blind-spot network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17725\u201317734, 2022.   \n[37] Yizhong Pan, Xiao Liu, Xiangyu Liao, Yuanzhouhan Cao, and Chao Ren. Random sub-samples generation for self-supervised real image denoising. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12150\u201312159, 2023.   \n[38] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. Restormer: Efficient transformer for high-resolution image restoration. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5728\u20135739, 2022.   \n[39] Zejin Wang, Jiazheng Liu, Guoqing Li, and Hua Han. Blind2unblind: Self-supervised image denoising with visible blind spots. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2027\u20132036, 2022.   \n[40] Tobias Plotz and Stefan Roth. Benchmarking denoising algorithms with real photographs. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1586\u20131595, 2017.   \n[41] Yinhuai Wang, Jiwen Yu, and Jian Zhang. Zero-shot image restoration using denoising diffusion null-space model. arXiv preprint arXiv:2212.00490, 2022.   \n[42] Tomer Garber and Tom Tirer. Image restoration by denoising diffusion models with iteratively preconditioned guidance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 25245\u201325254, 2024.   \n[43] Dan Zhang, Fangfang Zhou, Yuwen Jiang, and Zhengming Fu. Mm-bsn: Self-supervised image denoising for real-world with multi-mask based on blind-spot network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4189\u20134198, 2023.   \n[44] Hyemi Jang, Junsung Park, Dahuin Jung, Jaihyun Lew, Ho Bae, and Sungroh Yoon. Puca: patch-unshuffle and channel attention for enhanced self-supervised image denoising. Advances in Neural Information Processing Systems, 36, 2024.   \n[45] Jun Xu, Hui Li, Zhetong Liang, David Zhang, and Lei Zhang. Real-world noisy image denoising: A new benchmark. arXiv preprint arXiv:1804.02603, 2018.   \n[46] Rich Franzen. Kodak lossless true color image suite. source: http://r0k. us/graphics/kodak, 4(2):9, 1999.   \n[47] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural network representations revisited. In International conference on machine learning, pages 3519\u20133529. PMLR, 2019.   \n[48] Jun Xu, Yuan Huang, Ming-Ming Cheng, Li Liu, Fan Zhu, Zhou Xu, and Ling Shao. Noisy-as-clean: Learning self-supervised denoising from corrupted image. IEEE Transactions on Image Processing, 29:9316\u20139329, 2020.   \n[49] Nick Moran, Dan Schmidt, Yu Zhong, and Patrick Coady. Noisier2noise: Learning to denoise from unpaired noisy data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12064\u201312072, 2020.   \n[50] Tongyao Pang, Huan Zheng, Yuhui Quan, and Hui Ji. Recorrupted-to-recorrupted: Unsupervised deep learning for image denoising. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2043\u20132052, 2021.   \n[51] Yi Zhang, Dasong Li, Ka Lung Law, Xiaogang Wang, Hongwei Qin, and Hongsheng Li. Idr: Selfsupervised image denoising via iterative data refinement. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2098\u20132107, 2022.   \n[52] Reyhaneh Neshatavar, Mohsen Yavartanoo, Sanghyun Son, and Kyoung Mu Lee. Cvf-sid: Cyclic multivariate function for self-supervised image denoising by disentangling noise from image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17583\u201317591, 2022.   \n[53] Xin Lin, Chao Ren, Xiao Liu, Jie Huang, and Yinjie Lei. Unsupervised image denoising in real-world scenarios via self-collaboration parallel generative adversarial branches. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12642\u201312652, 2023.   \n[54] Xiaohe Wu, Ming Liu, Yue Cao, Dongwei Ren, and Wangmeng Zuo. Unpaired learning of deep image denoising. In European conference on computer vision, pages 352\u2013368. Springer, 2020.   \n[55] Samuli Laine, Tero Karras, Jaakko Lehtinen, and Timo Aila. High-quality self-supervised deep image denoising. Advances in Neural Information Processing Systems, 32, 2019.   \n[56] Zichun Wang, Ying Fu, Ji Liu, and Yulun Zhang. Lg-bpn: Local and global blind-patch network for self-supervised real-world denoising. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18156\u201318165, 2023.   \n[57] Yaochen Xie, Zhengyang Wang, and Shuiwang Ji. Noise2same: Optimizing a self-supervised bound for image denoising. Advances in neural information processing systems, 33:20320\u201320330, 2020.   \n[58] Yeong Il Jang, Keuntek Lee, Gu Yong Park, Seyun Kim, and Nam Ik Cho. Self-supervised image denoising with downsampled invariance loss and conditional blind-spot network. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 12196\u201312205, October 2023.   \n[59] Junyi Li, Zhilu Zhang, Xiaoyu Liu, Chaoyu Feng, Xiaotao Wang, Lei Lei, and Wangmeng Zuo. Spatially adaptive self-supervised learning for real-world image denoising. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9914\u20139924, 2023.   \n[60] Kwanyoung Kim and Jong Chul Ye. Noise2score: tweedie\u2019s approach to self-supervised image denoising without clean images. Advances in Neural Information Processing Systems, 34:864\u2013874, 2021.   \n[61] Kwanyoung Kim, Taesung Kwon, and Jong Chul Ye. Noise distribution adaptive self-supervised image denoising using tweedie distribution and score matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2008\u20132016, 2022.   \n[62] Zongsheng Yue, Hongwei Yong, Qian Zhao, Deyu Meng, and Lei Zhang. Variational denoising network: Toward blind noise modeling and removal. Advances in neural information processing systems, 32, 2019.   \n[63] Mohammad Zalbagi Darestani and Reinhard Heckel. Accelerated mri with un-trained neural networks. IEEE Transactions on Computational Imaging, 7:724\u2013733, 2021.   \n[64] Metin Ersin Arican, Ozgur Kara, Gustav Bredell, and Ender Konukoglu. Isnas-dip: Image-specific neural architecture search for deep image prior. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1960\u20131968, 2022.   \n[65] Chao Wang, Zhedong Zheng, Ruijie Quan, Yifan Sun, and Yi Yang. Context-aware pretraining for efficient blind image decomposition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18186\u201318195, 2023.   \n[66] Naishan Zheng, Man Zhou, Yanmeng Dong, Xiangyu Rui, Jie Huang, Chongyi Li, and Feng Zhao. Empowering low-light image enhancer through customized learnable priors. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12559\u201312569, 2023.   \n[67] Guilin Liu, Fitsum A Reda, Kevin J Shih, Ting-Chun Wang, Andrew Tao, and Bryan Catanzaro. Image inpainting for irregular holes using partial convolutions. In Proceedings of the European conference on computer vision (ECCV), pages 85\u2013100, 2018.   \n[68] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[69] Abdelrahman Abdelhamed, Marcus A Brubaker, and Michael S Brown. Noise flow: Noise modeling with conditional normalizing flows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3165\u20133173, 2019.   \n[70] Tim Brooks, Ben Mildenhall, Tianfan Xue, Jiawen Chen, Dillon Sharlet, and Jonathan T Barron. Unprocessing images for learned raw denoising. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11036\u201311045, 2019.   \n[71] Chang Qiao, Di Li, Yuting Guo, Chong Liu, Tao Jiang, Qionghai Dai, and Dong Li. Evaluation and development of deep neural networks for image super-resolution in optical microscopy. Nature Methods, 18(2):194\u2013202, 2021.   \n[72] Chen Chen, Qifeng Chen, Jia Xu, and Vladlen Koltun. Learning to see in the dark. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3291\u20133300, 2018. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Introduction ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "This document provides supplementary materials for the main paper. Specifically, a brief conclusion of works related to ours is in Sec. B Sec. C presents more details and demonstrations about proposed iterative filling (Sec. C.1, Sec. C.2) and different strategies to adapt to real-world spatial-correlated noise (Sec. C.3). Sec. D presents more discussion about masking ratio (Sec. D.1), over-fitting (Sec. D.2) and different downsampling strategies (Sec. D.3). Additionally, we conduct further analysis of pre-training (Sec. E) and expanded our framework to other network structures (Sec. F). More experimental details and qualitative comparison results can be found in Sec. G and Sec. I. ", "page_idx": 14}, {"type": "text", "text": "B Related Works ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Unsupervised Image Denoising ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Unlike supervised approaches [5\u20137,38], unsupervised denoising focuses on situations when paired data is unavailable. Methods in this category include: ", "page_idx": 14}, {"type": "text", "text": "Paired noisy-noisy images. To learn consistent representations from varied noise observations of the same scene, Lehtinen et al. [8] train on mapping from two noisy observations of the same scene. Additional approaches utilize synthetic noise to generate noisy pairs, as seen in [48\u201352], as well as [11] learns shared latent from multiple noise observations. ", "page_idx": 14}, {"type": "text", "text": "Unpaired noisy-clean images. Du et al. [9] propose to learn decoupled representations of contents and noise from images. Lin et al. [53] extend this by using separated noise representations to guide noise synthesis, thereby enhancing the denoising process. Additionally, Wu et al. [54] employ a distillation loss from both real and synthetic noisy images. ", "page_idx": 14}, {"type": "text", "text": "Noisy images only. Techniques like blind-spot [16,17,39,55\u201357], substitution followed by image reconstruction [16, 17], multiple sub-sampled images from a single noisy scene [10] or above approaches combined [36,37,58] are developed when only one observation is available from a noise scene. Li et al. [59] integrate blind spot strategies and structural insights for adaptive denoising. Score matching and posterior inference are also utilized in [60,61]. ", "page_idx": 14}, {"type": "text", "text": "In the context of zero-shot denoising tasks, only a single noisy image is visible during training, presenting greater challenges than methods described above. ", "page_idx": 14}, {"type": "text", "text": "B.2 Zero-shot Image Denoising ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Compared to unsupervised methods, zero-shot denoising is more challenging as it aims to train a network for denoising when a single noisy image available. Typical strategies involve utilizing spatial correlations [3,4], variation based priors [15,62] or low-frequency characteristics of images, corrupting and reconstructing part of images [14,16,17], or constructing paired training sets from sub-sampled noisy images [22,23]. Among which Noise2Void [16] is initially designed for learning from multiple noisy images, shows promise in its zero-shot version. Noise2Self [17]reconstructs cyclically masked regions of input noisy image. Although there is a gap compared to supervised or unsupervised methods. While dropout-ensemble [14] is adapted on a noisy image for better performance, it leads to over-smoothing and incurs large computational costs. Noise2Fast [23] and Zero-Shot Noise2Noise [22] are fast but struggle to completely remove noise from images, especially spatially-correlated real noise, resulting in suboptimal visual results. DIP [13] and its variants [18,63] exploits the features of deep networks to learn mappings from random noise to images. Early stopping [21] or other approaches [20,64] are used to prevent over-fitting, FasterDIP [19] further discusses the influence of network structure on its performance. However, current zero-shot methods often takes a long time, and parameter settings are carefully selected for various image contents and noise degradation. ", "page_idx": 14}, {"type": "text", "text": "B.3 Masked Image Modeling ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Masked Image Modeling (MIM) helps in learning pre-trained representations for downstream tasks by masking a portion of input images [26,27,29] and training models to predict the masked contents. Due to its impressive effects in high-level tasks [30,31], Masked Image Modeling has also found applications in low-level visual tasks. For instance, Wang et al. [65] applies random patch masks during the pre-training of image deraining and desnowing models in handling adverse weather conditions. Zheng et al. [66] integrates Masked Autoencoder (MAE) to learn illumination-related structural information in a supervised low-light enhancement framework. Notably, despite the successful applications of Masked Image Modeling in several low-level vision tasks, its application in the pre-training scheme for denoising models has not yet been explored. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "C More Details about MPI ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1 More Details about Iterative Filling ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In the main paper, we mention that Iterative Filling is optimization steps based on pre-trained weights for zero-shot denoising. And as depicted in Sec. 3.3 in main paper, to fully leverage the results of each optimization step and preserve more image detail, only the masked regions ${\\hat{M}}_{t}\\odot y_{t}$ are constrained by the loss at each optimization timestep and considered reliable, others are labeled as \u201cunreliable pixels\u201d and discarded (see Sec. C.2 to know why we need \u201cunreliable pixels\u201d). During the iterative optimization, we maintain an ensemble version $\\overline{y}$ , assembling reliable parts $\\hat{M_{t}}\\odot y_{t}$ of each prediction $y_{t}$ via EMA while keeping the rest unchanged, as shown in Fig. 12. ", "page_idx": 15}, {"type": "text", "text": "With sufficient iterations, ensemble from each pixel is ensemble from hundreds of predictions, ensuring a high-quality ensemble outcome. ", "page_idx": 15}, {"type": "image", "img_path": "oFgTScAsBr/tmp/2c36fe2aa9e5f3bb0e8ad71ee7dbe1d697955d07c851b3b21c52a1afcd430b79.jpg", "img_caption": ["Figure 12: Details of EMA process in Iterative filling. Masked regions of predictions from each optimization step $t$ is assembled to a ensemble $\\overline{{y}}$ . "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "C.2 Why Discard \u201cUnreliable Pixels\u201d ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "During the zero-shot denoising process, unmasked parts of each optimization result $M_{t}\\odot y_{t}$ needed be discarded, referred to as \u201cunreliable pixels\u201d, primarily due to the following reasons: ", "page_idx": 15}, {"type": "text", "text": "1) The pre-training task is set to reconstruct masked regions, that is only masked areas are constrained for reconstruction, while pixels in unmasked areas significantly differ from the actual pixels in the image. This discrepancy might result from the skip connections in the network architecture of DIP [13]. To maximize the utilize of pre-trained weight and avoid conflicting, pixels corresponding to these unmasked regions should not be considered during optimizing. ", "page_idx": 15}, {"type": "text", "text": "2) For spatially uncorrelated noise, we employ an extremely low mask ratio and distinct mask settings for each color channel to preserve as much image information as possible. In this scenario, it is impractical to expect the network to retain all pixel values in its output, as this can easily lead to identity mapping. ", "page_idx": 15}, {"type": "text", "text": "3) While partial convolution [67] or others designed for image inpainting can mitigate this problem, they often lead to sub-optimal performance with risks of over-ftiting to noise, and require specialized network architectures, limiting the adaptability of proposed framework to other network structures. ", "page_idx": 15}, {"type": "text", "text": "For our zero-shot denoising framework, which obtains denoised images via iterative optimization, using a portion of noisy image as cues and training network to complete this \"fill-in-the-blank\" task proves most effective. Moreover, the optimization generates reconstructions with various masks in iterations, assembling these results to achieve final denoised prediction requires only maintaining an ensemble result additionally, incurring negligible time and space resources. ", "page_idx": 15}, {"type": "image", "img_path": "oFgTScAsBr/tmp/56a6c6b4ea245847cadf4cf65ea3802f6463d52da918b81b4bfd0fc60e8765cb.jpg", "img_caption": ["Figure 13: An overview of the zero-shot denoising stage with adaptation to real-world noise. We adapt downsample $^{\\star}{D o w n(\\cdot)}^{\\,,\\bullet}$ and upsample \u201c $U p(\\cdot)^{,}$ to achieve noisy subsamples with less spatial correlation in noise, labeled in green arrows, and larger masking ratio is used to further deal with remaining spatial correlations. Actually, not all real noisy images needed to be sub-sampled, we only adapt $^{\\bullet}{D o w n(\\cdot)^{\\bullet}}$ and \u201c $\\,\\!U p(\\cdot)^{\\,\\bullet}$ to SIDD dataset. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "C.3 Different Strategies in Dealing with Synthetic & Real Noise ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We adapt downsample \u201c $^{\\prime}D o w n(\\cdot)^{\\bullet}$ and upsample \u201c $\\mathcal{U}p(\\cdot)^{\\bullet}$ to achieve noisy subsamples with less spatial correlation in noise, and larger masking ratio $80\\%{\\sim}95\\%$ $90\\%$ for SIDD and $85\\%$ for others) with a unified mask for all channels is used to further deal with remaining spatial correlations. Actually, not all real noisy images needed to be sub-sampled, we only adapt \u201c $\\bar{\\mathbf{\\xi}}\\bar{D o w n}(\\cdot)^{,,}$ and ${}^{\\bullet\\bullet}U p(\\cdot)$ \u201d to SIDD dataset. See Fig. 13 for a simple framework, which specifically designed for real-world noise is labeled in green arrows. ", "page_idx": 16}, {"type": "text", "text": "D Additional Discussion ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "D.1 Masking Ratio ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In the main paper, we discussed the optimal masking ratio for removing spatially uncorrelated synthetic noise and notes that significantly larger mask ratios are used for real noise. For real spatially correlated noise, the situation becomes more complex, with no single optimal mask ratio. Based on experience, the most effective mask range is between $80\\%{\\sim}95\\%$ , which is influenced by the noise\u2019s spatial correlation, noise intensity, and the information in image. ", "page_idx": 16}, {"type": "text", "text": "For the SIDD dataset, we investigated the impact of the masking ratio on SIDD validation, as shown in Fig. 14, finding that a $90\\%$ masking ratio is optimal. This is attributed to SIDD images containing limited content information and the noise exhibiting strong spatial correlation. ", "page_idx": 16}, {"type": "text", "text": "However, the optimal masking ratio for SIDD differs from that for synthetic noise, primarily due to the spatial correlation of the noise within the image. Synthetic noise is spatially uncorrelated, meaning noise signals at neighboring positions do not influence each other. In contrast, real noise, after undergoing a series of ISP processes, exhibits a more complex distribution, resembling blurred spots rather than independent points (see Fig. 15). For synthetic noise, selecting a small masking ratio allows for quicker recovery of image details. Conversely, for real noise, a small masking ratio ", "page_idx": 16}, {"type": "image", "img_path": "oFgTScAsBr/tmp/3be67f01dd4ac0528ea938ef23d88e5e6d30a5b840a20700e10a8dca83fa74f9.jpg", "img_caption": ["Figure 14: Effect of masking ratios on real-world noise (SIDD validation). $90\\%$ for removing strong spatial correlated noise. ", "Masking Ratio \ud835\udc5d(%) "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "oFgTScAsBr/tmp/86d248181df3b0331ffbb2d8d226be20229cd1de291106850100a5e1c60de03a.jpg", "img_caption": ["Figure 15: Illustration of spatial-correlated real-world noise (right) and synthetic noise (left). "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Table 8: Extension of proposed pre-training strategy into other network architectures. A performance improvement can be observed for both settings of beta $=\\!0.9$ and 0.99 in the experiment across various network architectures. ", "page_idx": 17}, {"type": "table", "img_path": "oFgTScAsBr/tmp/281bca54d761a59d2e68e43d710b574f78ba3cb3ddb654ba67b5053ad338e973.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "may lead to the model fitting the noise distribution by relying on neighboring pixel values. In such cases, a larger masking ratio helps mitigate the influence of noise. ", "page_idx": 17}, {"type": "text", "text": "D.2 Over-fitting & Regularization ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In the main text, we highlighted that our proposed zero-shot denoising framework still faces overftiting issues with increased iteration counts, with Table 8 and Fig. 16 showing this problem\u2019s impact across more datasets and more regularization. To address this issue, after validation and comparison, we recommend adopting a simple early-stopping strategy to prevent over-ftiting\u2014a straightforward, effective approach without additional computational costs. We also compared other strategies: ", "page_idx": 17}, {"type": "text", "text": "\u2022 Employing TV regularization helps against overfitting but still leads to performance drop and lower peak PSNR as iterations increase.   \n\u2022 Adding random transformations to the input image, including filps and random translations, lead to steady performance improvement and higher peak PSNR over more iterations, but increase inference time.   \n\u2022 Early stopping stops close to peak performance with minimal calculation, providing stable, high-quality results with no added time. ", "page_idx": 17}, {"type": "image", "img_path": "oFgTScAsBr/tmp/10286d77ed9f2cd99f08ce60aa45c4ee532808b17a5f682bf4fb10bcb841737e.jpg", "img_caption": ["Figure 16: Influence of different regularization strategies during iterations, including Total Variation $(^{\\leftarrow}\\!+\\!\\Gamma\\mathsf{V}^{\\bullet})$ , random image augmentation $\\mathrm{{(^{\\circ}+A u g^{\\circ})}}$ , and early stopping $(^{\\bullet\\bullet}+\\mathrm{ES}^{\\circ})$ . \"Ours\" and \"Faster\" are the methods evaluated in mainpaper. Example is tested on F16_512rgb with Gaussian $\\scriptstyle\\sigma=25$ . "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "D.3 Down-sampling in Real-world Denoising ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "As illustrated in Fig. 13, specialized downsampling is employed to reduce the spatial correlation of real noise, with different downsampling strategies yielding varying outcomes. Simple pixelshuffle $(^{\\leftarrow}\\!+\\!\\mathrm{PD}^{\\setminus})$ can easily lead to checkerboard artifacts, whereas more randomized sub-sampling strategies [37] ", "page_idx": 17}, {"type": "text", "text": "Table 9: Discussion of noise in pre-trained dataset. Additional assumptions of noise in pre-trained dataset result in lower performance ( $\"+{\\mathrm{G}},$ auss $(\\sigma{=}25$ )(N2N)\"). ", "page_idx": 18}, {"type": "table", "img_path": "oFgTScAsBr/tmp/2f640d2cdec3a82c4c92257c34a0d3bbcd6aa7079f32e18565f1e552a89316d6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "$(^{\\bullet\\bullet}{+}\\mathrm{RSG}^{\\circ})$ can more effectively disrupt noise spatial correlations and, due to their randomness, avoid checkerboard artifacts, as depicted in Fig. 17. A potential issue with this approach is the over-smoothed denoising predictions. Therefore, downsampling is only applied in strong spatiallycorrelated real noise. ", "page_idx": 18}, {"type": "image", "img_path": "oFgTScAsBr/tmp/3c372d97392b35597975609285d3189b401f54a680eb645d2566000b57c2b258.jpg", "img_caption": ["Figure 17: Validation of different down-sampling strategies in real-world denoising. Better downsampling strategies can further enhance denoising performance of our pipeline. Noisy patches are from SIDDval_12_2 and SIDDval_20_3. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "E Additional Analysis on Pre-training ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "E.1 Noise Intensity ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In the main text, we have already mentioned that pre-training aids in the removal of various types of noise. We validated the relationship between pre-trained weights and different input noise intensities on the CSet dataset, as shown in Fig. 18. Pre-training enhances denoising performance across different noise levels, particularly in the case of strong noise, where the knowledge provided by pre-training effectively avoids over-fitting to the noise. ", "page_idx": 18}, {"type": "text", "text": "E.2 Masking Ratio ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We analyze the impact of pre-training on different paradigms under various masking ratios, as shown in Fig. 19. Our study reveals that pre-training plays a significant role in enhancing denoising performance across various masking ratios (especially in cases of $20\\leq p\\leq80)$ ). ", "page_idx": 18}, {"type": "text", "text": "E.3 Discuss of Noise in Pre-training ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In the main text, we use the well-known natural image dataset ImageNet without making any assumptions about the presence or type of noise in each image, hoping to learn the statistical distribution rules from a large number of natural images. Here, we add synthetic noise of specified distribution and intensity during pre-training, and perform pre-training from noise to itself (denoted as \u201c+Gauss $(\\sigma{=}25)$ (N2N)\u201d), and adopt the same iterative denoising strategy, proving that additional assumptions about specific noise type or noise level in pre-training leads to a decline in effectiveness, as shown in Table. 9. Networks that are too small fail to learn sufficient denoising information, falling short of the corresponding zero-shot approaches. ", "page_idx": 18}, {"type": "image", "img_path": "oFgTScAsBr/tmp/95b0b9f72cfcdb6295df218f39490de30fa77cc9687b24466c7adf605a0bfb14.jpg", "img_caption": ["Figure 18: Effect of pre-training on different noise levels from Gaussian (left) and Poisson (right) on CSet. Pre-training is beneficial for all 6 noise levels, especially in cases of intense noise. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "oFgTScAsBr/tmp/d5016d94b1dc1a398fff2dca17e56f63be8f95eb9350e287527096bfbe5ad01e.jpg", "img_caption": ["Figure 19: Effect of pre-training on different masking ratios $p$ from Gaussian noise on CSet. Pretraining is beneficial for all masking ratios, especially in cases of $20\\leq p\\leq80$ . Table 10: Extension of proposed pre-training strategy into other network architectures. A performance improvement can be observed for both settings of beta $=\\!0.9$ and 0.99 in the experiment across various network architectures. ", "Masking Ratio $p$ $\\%)$ "], "img_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "oFgTScAsBr/tmp/deb1e691ad170ed76644ec96f1e2b646f5c35626cd8f2dfb6f5d94af03dd0f63.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "F Extension to other network structures ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In the main text, we discuss the effect of pre-training on the proposed model using the same network architecture as DIP [13]. We are curious whether this prior knowledge could be applied to other model architectures. Here, we compare the impact of pre-training under different model settings. Specifically, we evaluate the removal of Gaussian noise with sigma $=\\!25$ on CSet using three additional network architectures (DnCNN [5], ResNet [68]), as shown in Table 10. The pre-training approach consistently brings performance gains across various network architectures. ", "page_idx": 19}, {"type": "text", "text": "G More Experimental Settings & Results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "G.1 Quantitative analysis of Poisson noise removal ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Due to space limitations in the text, there is no quantitative comparison of Poisson noise, which is listed in this section, see Table 11. ", "page_idx": 19}, {"type": "text", "text": "G.2 Quantitative comparison with more methods ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Here we compare more recent methods including additional DIP-based zero-shot method (DIPSURE [20]), diffusion-based methods (DDNM [41], DDPG [42]), and zero-shot modifications from unsupervised methods (AP-BSN [36], MM-BSN [43], PUCA [44]), refer to Table 12 for results. ", "page_idx": 19}, {"type": "table", "img_path": "oFgTScAsBr/tmp/1c8f54f89d487a5b0530b84cc0c77be282b3b3dc9171f4fd0748037d3e4b1288.jpg", "table_caption": ["Table 11: Quantitative comparison on CSet, McMaster and CBSD dataset for Poisson noise removal $(\\lambda{\\in}[10,\\!25,\\!50])$ ). For best results highlighted and second underlined. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "oFgTScAsBr/tmp/90257423fa30c29d1874560359fd231a043b8f658674e14a78583c276c16c810.jpg", "table_caption": ["Table 12: Quantitative comparison with other DIP-based method (DIP-SURE [20]), Diffusion methods (DDNM, DDPG), zero-shot methods modified from other Self-supervised methods (APBSN, MM-BSN, PUCA). DIP-SURE, DDNM and DDPG requires additional noise variance as input, and DIP-SURE applies different iterations for each image. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Specifically, for DIP-SURE, we report both peak performance and the performance at the final iteration. Since DIP-SURE is specifically designed for Gaussian and Poisson noise, and requires the input of Gaussian noise variance, for real-world denoising tasks we provide estimated variance from paired data, to report the best performance. For diffusion-based methods, which are trained exclusively on Gaussian noise and also require variance as prior, we use the same variance estimation approach to report their best results. For self-supervised methods, which can be easily adapted to a single image with minimal changes, we follow their original settings. In each iteration, we crop eight same-size patches from the noisy image to form a batch, and perform inference on the full image every 10 iterations, and combine denoised images using the same ensemble strategy as our method for fairness. ", "page_idx": 20}, {"type": "text", "text": "We observe that DIP-SURE, due to its priors on noise type and variance, performs slightly better than our method under Gaussian noise settings. However, its performance significantly drops when dealing with real noise, especially when reporting the last performance. Since diffusion models are inherently Gaussian denoisers, they perform well on Gaussian noise when the variance is known, but also face challenges with real noise. The modified blind-spot network-based methods can handle severe real noise relatively well, but they may suffer from potential image detail loss and require long inference times. ", "page_idx": 20}, {"type": "text", "text": "G.3 Ensemble results of N2V and N2S ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In the main text, we present versions of DIP [13] and FasterDIP [19] with EMA (Exponential Moving Average) ensembles, as these processes are included in their source codes. To provide additional information for comparison, we also adapted $\\mathrm{N}2\\mathrm{V}^{*}$ [16] and $\\mathrm{N}2\\mathrm{S}^{\\ast}$ [17] to their corresponding EMA ensemble versions, as shown in Table 13, 14, 15. Generally, the ensemble versions of these methods can improve the PSNR by $1\\!\\sim\\!2$ dB. However, even though the enhanced N2V may outperform our Faster version in some cases, it does not affect the performance comparison with our $\\beta{=}0.99$ version, which remains the best. Moreover, our $\\beta{=}0.99$ version achieves this with less than half the inference time required by these methods. ", "page_idx": 20}, {"type": "image", "img_path": "oFgTScAsBr/tmp/deb7634ae3e153c925dce615fb4ab5e1a2e78f3da24ae530318c4dd8a073db18.jpg", "img_caption": ["Figure 20: Comparison of different methods under SIDD validation (SIDDval_34_22). "], "img_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "oFgTScAsBr/tmp/d1b1fd2feb867b4f4d0b3fe4105565f3678dc007d9facc40fccfea242e52f98c.jpg", "table_caption": ["Table 13: Quantitative comparison of ensemble version of $\\mathrm{N}2\\mathrm{V}^{*}$ and $\\mathrm{N}2\\mathrm{S}^{\\ast}$ on CSet, McMaster and CBSD dataset for Gaussian and Poisson noise removal. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "G.4 Details of Unseen Noise ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Gaussian Noise. Gaussian noise follows a normal distribution and is commonly encountered in digital imaging, especially during sensor data acquisition and transmission. It represents random variations in intensity and color information in images, making it a fundamental noise model in image processing. For each element in clean image $I_{[k]}$ , is represented by: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\hat{I}_{[k]}=I_{[k]}+\\sigma\\cdot N_{[k]},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $N_{[k]}$ represents random variable sampled from a standard normal distribution, which is characterized by its standard deviation $(\\sigma)$ . ", "page_idx": 21}, {"type": "text", "text": "Poisson Noise. Poisson noise is prevalent in scenarios with low-light conditions, such as astronomical imaging or medical imaging, where the photon count is inherently random and follows a Poisson distribution. Poisson noise models the variation of intensity based on a Poisson distribution, is generally expressed as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\hat{I}_{[k]}=P(I_{[k]}\\cdot\\lambda)/\\lambda,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\lambda$ indicates the event rate, and $P(\\cdot)$ denotes random variable generated from Poisson distribution. ", "page_idx": 21}, {"type": "text", "text": "Noise Level Function (NLF). Noise level function, also referred to as heteroscedastic gaussian model [69], is commonly described by a varying standard deviation across the image. This type of noise is widely used to express the read-shot noise in camera imaging pipeline, where different parts of the image exhibit different noise levels. It is typically modeled as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\hat{I}_{[k]}\\sim\\mathcal{N}(\\mu=I_{[k]},\\sigma^{2}=\\sigma_{r}+\\sigma_{s}\\cdot I_{[k]})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\sigma_{r}$ and $\\sigma_{r}$ represent different standard deviations in distinct regions of the image. Noise parameter calibrated for [40] in work [70] obeys a log-linear rule: ", "page_idx": 21}, {"type": "equation", "text": "$$\nl o g(\\sigma_{r})=2.18\\cdot l o g(\\sigma_{s})+1.2\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We choose $\\sigma_{s}\\in[0.01,0.012]$ to better illustrate the generality. ", "page_idx": 21}, {"type": "text", "text": "Speckle Noise. Speckle noise is an interference pattern produced by the coherent processing of a signal, especially in active radar and ultrasound imaging. This noise is particularly common in radar and ultrasound images, where it can significantly degrade the quality of the image. Its mathematical representation is: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\hat{I}_{[k]}=I_{[k]}+I_{[k]}\\cdot U_{[k]},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $U_{[k]}$ is sampled from uniform distribution with mean 0 and $v$ representing the standard deviation of the noise. ", "page_idx": 21}, {"type": "table", "img_path": "oFgTScAsBr/tmp/243164e2203f54cbaffe95b2975c7efbc3862daa562817ad6ad312cd1706c5e4.jpg", "table_caption": ["Table 14: Quantitative comparison of ensemble version of $\\mathrm{N}2\\mathrm{V}^{*}$ and $\\mathrm{N}2\\mathrm{S}^{\\ast}$ on Kodak with 5 noise types for generalization evaluation. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "oFgTScAsBr/tmp/637ed86fc8daa6d720447b122f0bcf4a655e58552e530573930ca2cdddc40b6c.jpg", "table_caption": ["Table 15: Quantitative comparison of ensemble version of $\\mathrm{N}2\\mathrm{V}^{*}$ and $\\mathrm{N}2\\mathrm{S}^{\\ast}$ on SIDD, PolyU and FMD dataset for real-world noise removal. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Salt-and-Pepper Noise (S&P). Salt-and-Pepper noise, also known as impulse noise, is characterized by sharp and sudden disturbances in an image signal. It\u2019s typically represented as sparse white and black pixels, hence the name. This noise can be caused by sharp and sudden disturbances in the image signal, often due to transmission errors, faulty memory locations, or timing errors in digital image sensors. Its mathematical representation is: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\hat{I}_{[k]}=I_{[k]}+S_{[k]}-S_{[k]},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $S_{[k]}$ and $P_{[k]}$ represents salt and pepper noise, respectively. For each of them are Bernoulli sample with probability $d$ of $I_{m a x}/I_{m i n}$ and probability $1-d$ of 0, which makes the probability total affected is $2\\cdot d$ . ", "page_idx": 22}, {"type": "text", "text": "G.5 Additional Computational Costs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In analyzing the performance of deep learning models, it\u2019s crucial to consider both the Floating Point Operations (FLOPs) and the model parameters. FLOPs give us an insight into the computational complexity of the model, which affects inference time and resource utilization, while the number of parameters indicates its capacity to learn and adapt to complex data patterns. A balance between these two aspects is essential for efficient and effective model performance. Our analysis, as reflected in the comparison between Table 16, demonstrates that our method successfully achieves this balance. It maintains computational efficiency without compromising the model\u2019s ability to accurately process and analyze data, an essential factor for practical application in varied computational environments. ", "page_idx": 22}, {"type": "text", "text": "G.6 Zero-shot Denoising on More Image Types ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In the main text, we demonstrate the ability of proposed MPI to generalize to other types of images through a medical imaging dataset. Further here, we explore new types of images, including a microscopy imaging dataset BioSR [71] and extremely low-light dataset SID [72]. See Fig. 21 and Fig. 22 for qualitative examples. ", "page_idx": 22}, {"type": "table", "img_path": "oFgTScAsBr/tmp/58cace31362ee66c202a333ea854599c7851051f4dc239328ca4b59a6c057804.jpg", "table_caption": ["Table 16: Efficiency comparisons of deep learning-based methods on Params and FLOPs under input size $256\\times256$ with a single forward step. Iterations used for synthetic noise is provided for reference. "], "table_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "oFgTScAsBr/tmp/0be87424f6b9b88a79b12f0c437e9c5595d8437d496a988b6eea021aee4d6639.jpg", "img_caption": ["(a) Noisy Image (b) Denoised by Ours (c) Estimated Noise Map Figure 21: On a noisy microscopy image (a) using the proposed MPI to denoise retaining the structural information in the image as much as possible (b), see the noise map (c). "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "oFgTScAsBr/tmp/ad72481acbea1c237686871efdc43835393715e53347014e7516a08718b66003.jpg", "img_caption": ["Figure 22: For extremely low-light images, there is serious color bias in the expected denoising result (a) and captured noisy image (b). This color bias is still retained in the denoising result, but the noise is basically removed. This is discussed in Sec. H.2. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "H Concluding Remarks ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this study, we introduce Masked Pre-train then Iterative flil (MPI), a zero-shot denoising paradigm utilizing pre-trained model with random masks on natural images. The pre-trained weights is optimized on a specific noisy image through Iterative fliling process, and predictions with corresponding masks during inference is combined for enhanced quality and faster inference. ", "page_idx": 23}, {"type": "text", "text": "H.1 Broader impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "From the perspective of our work, we have pioneered the use of generalizable knowledge from natural images without any assumptions about noise degradation, offering an efficient framework for handling diverse synthetic and real noises with significantly reduced inference time, which is a critical issue in zero-shot denoising and makes their practical applications feasible. Notably, our zero-shot method excels in generalization compared to current supervised and unsupervised methods, offering new insights into denoising. ", "page_idx": 23}, {"type": "text", "text": "H.2 Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Although proposed MPI has shown effects in removal of various types of noises, the mask-based noise-supervised denoising setting does not seem to allow the removal of non-zero mean noise. So when dealing with extremely low-light images with severe color bias, the color bias still remains after denoising; this is a common problem in zero-shot denoising, because there is no prior regarding noise-clean image pair in specific domain, but it may limit several practical applications, and we are currently trying to solve this problem in other ways. ", "page_idx": 23}, {"type": "text", "text": "I Additional Qualitative Results ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The following figures show the denoising comparison on both synthetic noise removal (see Fig. 23 - Fig. 31) and denoising real noise data (Fig. 32 - Fig. 35). ", "page_idx": 24}, {"type": "image", "img_path": "oFgTScAsBr/tmp/798f7c499764dbaad0d4e88f1af8aa084bf1e58a183f7f5ce64a08043cbff761.jpg", "img_caption": ["Figure 23: Qualitative comparison of results on CBSD [34] with Gaussian $\\sigma{=}10$ . Noisy patch is from CBSD-11. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "oFgTScAsBr/tmp/676030495cb5e44dfb77792691b9193881414393a8e974496e26e7887ef5cfb9.jpg", "img_caption": ["Figure 24: Qualitative comparison of results on CBSD [34] with Gaussian $\\sigma{=}25$ . Noisy patch is from CBSD-31. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "oFgTScAsBr/tmp/63b31f7f1193d47059c1f26d8858b823912a9010a1c30932ffb706e178ce80da.jpg", "img_caption": ["Figure 25: Qualitative comparison of results on Kodak [46] with Gaussian $\\scriptstyle\\sigma=50$ . Noisy patch is from kodim20. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "oFgTScAsBr/tmp/30a8fc34d0598bdd7146e68042f4644c4fc0eda48921ecde8ffb3972557969c4.jpg", "img_caption": ["Figure 26: Qualitative comparison of results on CBSD [34] with Poisson $\\lambda{=}10$ . Noisy patch is from CBSD-33. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "oFgTScAsBr/tmp/e16ffd32281218441677572f63b17c07c242e9f3641b1d3da1cb13b0130546f5.jpg", "img_caption": ["Figure 27: Qualitative comparison of results on CBSD [34] with Poisson $\\lambda{=}25$ . Noisy patch is from CBSD-56. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "oFgTScAsBr/tmp/1c2f71f3797e8e9a6eddab44664944eb7b82647d94c16e29ba8c86074a7d7d0b.jpg", "img_caption": ["Figure 28: Qualitative comparison of results on CBSD [34] with Poisson $\\lambda{=}50$ . Noisy patch is from CBSD-05. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "oFgTScAsBr/tmp/c1cb8c94be57f1e03785afd3742eca0c470ef5ca8b2e1fbc9ca5ee9c3a74fac9.jpg", "img_caption": ["Figure 29: Qualitative comparison of generalization on Kodak [46] with S&P $d{=}0.045$ . Noisy patch is from kodim11. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "oFgTScAsBr/tmp/7086581e3b516498042313e393de723c87017b78bcbe735b9c46c24821d7a7de.jpg", "img_caption": [], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Figure 30: Qualitative comparison of generalization on Kodak [46] with NLF [40]. Noisy patch is from kodim02. ", "page_idx": 30}, {"type": "image", "img_path": "oFgTScAsBr/tmp/20e4f5fb4c0d9b189671311af1f28212070ec089103cb48a058014155233b35b.jpg", "img_caption": ["Figure 31: Qualitative comparison of generalization on Kodak [46] with Poisson $\\lambda{=}40$ . Noisy patch is from kodim04. "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "oFgTScAsBr/tmp/32a9b519baa1fcb05b5b7e5e183973ff0692e5dd57a4aaa899c3fb86acfebe99.jpg", "img_caption": ["Figure 32: Qualitative comparison of realnoise on PolyU [45]. Noisy patch is from Sony_3- 5_200_1600_classroom_14. "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "oFgTScAsBr/tmp/03ae7807db3e6e1b2e72deaf8b762574428f35706daf36349845e12015afdbaf.jpg", "img_caption": ["Figure 33: Qualitative comparison of realnoise on PolyU [45]. Noisy patch is from Canon5D2_5_200_3200_toy_1. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "oFgTScAsBr/tmp/e6a96f3bc405389840ff5d888de81140984bb79386694940b754be870c329e90.jpg", "img_caption": ["Figure 34: Qualitative comparison of realnoise on SIDD [1]. Noisy patch is from SIDDval_20_8. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "oFgTScAsBr/tmp/383562f60492d0b7093666aff51dd54e3d60da7520d5a9e78e9c03206d4f15c2.jpg", "img_caption": ["Figure 35: Qualitative comparison of realnoise on SIDD [1]. Noisy patch is from SIDDval_13_19. "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: NA ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 35}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: NA ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 35}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 35}, {"type": "text", "text": "Justification: NA ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 36}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: NA ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 36}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: NA Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 37}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: NA Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 37}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [No] ", "page_idx": 37}, {"type": "text", "text": "Justification: NA Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean. ", "page_idx": 37}, {"type": "text", "text": "\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified. \u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). \u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 38}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] Justification: NA Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 38}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] Justification: NA Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 38}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: NA Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 38}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 38}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] Justification: NA Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 39}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes]   \nJustification: NA   \nGuidelines: \u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset. \u2022 The authors should state which version of the asset is used and, if possible, include a URL. \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset. \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. \u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. \u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. \u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 39}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes]   \nJustification: NA   \nGuidelines: \u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] Justification: NA Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 40}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] Justification: NA ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 40}]