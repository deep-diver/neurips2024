{"references": [{"fullname_first_author": "M. Zaheer", "paper_title": "Deep sets", "publication_date": "2017-03-06", "reason": "This paper introduces the Deep Sets architecture, a foundational model for permutation-equivariant neural networks that is frequently used as a comparison baseline in the current paper."}, {"fullname_first_author": "A. Navon", "paper_title": "Equivariant architectures for learning in deep weight spaces", "publication_date": "2023-01-12", "reason": "This paper is among the most closely related works, focusing on permutation-equivariant neural functionals for simple feedforward networks, which the current paper extends to more complex architectures."}, {"fullname_first_author": "T. Unterthiner", "paper_title": "Predicting neural network accuracy from weights", "publication_date": "2020-02-11", "reason": "This paper proposes a strong baseline method for generalization prediction using statistical features of network weights, which is then compared against the proposed approach in the current paper's RNN generalization experiment."}, {"fullname_first_author": "J. Harrison", "paper_title": "A closer look at learned optimization: Stability, robustness, and inductive biases", "publication_date": "2022-12-01", "reason": "This paper provides essential background on meta-learned optimizers, which are the focus of the learned optimizer experiments in the current paper."}, {"fullname_first_author": "A. Zhou", "paper_title": "Neural functional transformers", "publication_date": "2023-05-13", "reason": "This is another closely related work by the same authors, demonstrating the use of neural functionals for processing the weight-space features of transformer networks, providing further context and support for the current paper's claims."}]}