[{"figure_path": "W89fKKP2AO/tables/tables_3_1.jpg", "caption": "Table 1: Rank correlation between predicted and actual success rates of RNNs on an arithmetic task. Predicting with UNF significantly outperforms STATNN [Unterthiner et al., 2020].", "description": "This table presents the results of an experiment comparing the performance of three different methods in predicting the success rate of Recurrent Neural Networks (RNNs) trained for an arithmetic task.  The methods are Deep Sets, STATNN (a strong baseline method), and the proposed Universal Neural Functional (UNF) method. The evaluation metric is Kendall's tau (\u03c4), which measures rank correlation. The results demonstrate that UNF significantly outperforms the other two methods.", "section": "4.1 RNN generalization prediction"}, {"figure_path": "W89fKKP2AO/tables/tables_6_1.jpg", "caption": "Table 1: Rank correlation between predicted and actual success rates of RNNs on an arithmetic task. Predicting with UNF significantly outperforms STATNN [Unterthiner et al., 2020].", "description": "This table shows the results of an experiment comparing three different methods for predicting the success rate of recurrent neural networks (RNNs) trained on an arithmetic task.  The methods are Deep Sets, STATNN (a strong baseline), and UNF (the proposed method).  The performance is measured using Kendall's Tau (\u03c4), which is a rank correlation coefficient.  The table shows that UNF significantly outperforms both Deep Sets and STATNN, indicating that it's more effective at extracting information from the RNN weights to predict their performance.", "section": "4.1 RNN generalization prediction"}, {"figure_path": "W89fKKP2AO/tables/tables_14_1.jpg", "caption": "Table 3: Number of parameters used by f(\u00b7) in each learned optimizer, for each task. Note that NFN and UNF are identical for the MLP task. This count does not include the other meta-learned scalars in Eq. 15, which are \u03b1, \u03b3\u03bf, \u03b2.", "description": "This table presents the number of parameters used by the function f(\u00b7) in different learned optimizers (UNF, Deep Set, NFN) across various tasks (MLP on FashionMNIST, CNN on CIFAR-10, RNN on LM1B, Transformer on LM1B).  Note that the parameter count excludes meta-learned scalars \u03b1, \u03b3, and \u03b2. The UNF and NFN optimizers are equivalent in the MLP task.", "section": "3.4 Multiple feature channels"}]