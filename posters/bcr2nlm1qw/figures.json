[{"figure_path": "bCR2NLm1QW/figures/figures_1_1.jpg", "caption": "Figure 1: Video representation with diffusion prior. Given an RGB video, we can represent the video using a canonical image. However, the canonical image and reconstruction training process focuses only on reconstruction quality and could produce an unnatural canonical image. This could cause problems with downstream tasks such as prompt-based video editing. In the bottom example, if the hand is distorted in the canonical image, the image editor, such as ControlNet [75], may not recognize it and could introduce an irrelevant object instead. In this paper, we propose introducing the diffusion prior from a LoRA [18] fine-tuned diffusion model to the training pipeline and constraining the canonical image to be natural. Our method facilitates several downstream tasks, such as (a) video editing, (b) dynamic segmentation, and (c) video style transfer.", "description": "This figure illustrates the core idea of the NaRCan framework.  It shows how a video is represented by a canonical image, highlighting the problem of existing methods generating unnatural canonical images unsuitable for downstream tasks like video editing.  The solution proposed by the authors is to integrate a diffusion prior to generate high-quality, natural canonical images suitable for various applications. Three example applications (video editing, dynamic segmentation, and style transfer) are showcased.", "section": "1 Introduction"}, {"figure_path": "bCR2NLm1QW/figures/figures_3_1.jpg", "caption": "Figure 2: Our proposed framework. Given an input video sequence, our method aims to represent the video with a natural canonical image, which is a crucial representation for versatile downstream applications. (a) First, we fine-tune the LoRA weights of a pre-trained latent diffusion model on the input frames. (b) Second, we represent the video using a canonical MLP and a deformation field, which consists of homography estimation and residual deformation MLP for non-rigid residual deformations. By relying entirely on the reconstruction loss, the canonical MLP often fails to represent a natural canonical image, causing problems for downstream applications. E.g., image-to-image translation methods such as ControlNet [75] may not be able to recognize that there is a train in the canonical image. (c) Therefore, we leverage the fine-tuned latent diffusion model to regularize and correct the unnatural canonical image into a natural one. Specifically, we sophistically design a noise scheduling corresponding to the frame reconstruction process. (d) The natural and artifacts-free canonical image can then be facilitated to various downstream tasks such as video style transfer, dynamic segmentation, and editing, such as adding handwritten characters of \"NaRCan\".", "description": "This figure illustrates the NaRCan framework, which uses a hybrid deformation field and diffusion prior for video editing. It shows the steps of LoRA fine-tuning, video representation using canonical MLP and homography, diffusion prior integration for natural image generation, and the application of the natural canonical image to downstream video editing tasks such as style transfer, dynamic segmentation, and video editing.", "section": "3 Method"}, {"figure_path": "bCR2NLm1QW/figures/figures_4_1.jpg", "caption": "Figure 3: Noise and diffusion prior update scheduling. Initially, our model fits object outlines before the fields converge and without the diffusion prior, resulting in unnatural elements in the canonical image due to complex non-rigid objects. Upon introducing the diffusion prior with increased noise and update frequency, the model learns to generate natural, high-quality images, leading to convergence. Thus, the strength of noise and the update frequency will also decrease. Moreover, it's worth mentioning that update scheduling cuts training time from 4.8 hours to 20 minutes.", "description": "This figure illustrates the noise and diffusion prior update scheduling strategy used in the NaRCan model training. Initially, without the diffusion prior, the model produces unnatural canonical images due to complex object dynamics. The introduction of a diffusion prior with increased noise and frequent updates gradually improves image quality and reduces training time significantly.  The noise level and update frequency decrease as training progresses and the model converges.", "section": "3.2 Diffusion Prior"}, {"figure_path": "bCR2NLm1QW/figures/figures_5_1.jpg", "caption": "Figure 4: Linear interpolation. After using the grid trick [18] to obtain the highly consistent canonical images Ck and Ck+1, we interpolate all frames within the overlap window. As time progresses, the weight for reconstructing each frame gradually shifts from referencing Ck to solely referencing Ck+1. We achieve editing results with remarkable temporal consistency through this linear interpolation approach. Please refer to our supplementary material for more video results.", "description": "This figure illustrates the linear interpolation method used in Separated NaRCan for smooth transitions between canonical images.  It shows how the weights shift from one canonical image to the next over a series of frames, ensuring temporal consistency in the edited video. The consistent transitions between canonical images are crucial for high-quality video editing results.", "section": "3.3 Separated NaRCan"}, {"figure_path": "bCR2NLm1QW/figures/figures_6_1.jpg", "caption": "Figure 5: Qualitative comparisons on text-guided video-to-video translation. Our method achieves prompt alignment, synthesis quality, and temporal consistency best. Zoom in for the best view, and please refer to the supplementary materials for video comparisons. (a) In the camel scene, Medm [13] fails to generate clear-textured images to ensure temporal consistency, while CCEdit [15] fails to correctly identify the second camel in the background. (b) CoDeF [45] misses capturing the presence of a person in the bottom right corner, Hashing-nvd [5] exhibit noticeable contours due to masking, and both MeDM and CCEdit suffer from temporal inconsistency issues. For instance, in MeDM, the person transitions from wearing black clothes to blue clothes. (c) MeDM and CCEdit still exhibit temporal inconsistency issues, such as significant color, texture, and structure changes. Other methods almost entirely lose the original train information or appear as unnatural artifacts.", "description": "This figure compares the results of different video-to-video translation methods on three example scenes.  Our method (Ours) shows superior performance in prompt alignment, image quality, and maintaining temporal consistency compared to CoDeF, Hashing-nvd, MeDM, and CCEdit. The comparison highlights issues like blurry textures, missed objects, and temporal inconsistencies in the other methods.", "section": "4.2 Evaluation"}, {"figure_path": "bCR2NLm1QW/figures/figures_7_1.jpg", "caption": "Figure 6: User Study. Our method achieves the highest user preference ratios across all three aspects, compared with MeDM [13], CoDeF [45], Hashing-nvd [5].", "description": "This figure shows the results of a user study comparing the proposed method (NaRCan) against three baseline methods (MeDM, CoDeF, and Hashing-nvd) across three aspects: temporal consistency, text alignment, and overall quality.  The bar chart clearly indicates that NaRCan outperforms the other methods in all three aspects, demonstrating its superior performance in text-guided video editing.", "section": "4 Experiments"}, {"figure_path": "bCR2NLm1QW/figures/figures_7_2.jpg", "caption": "Figure 7: Qualitative comparisons on the canonical image. Our method generates more natural canonical images through a fine-tuned diffusion prior compared with CoDeF [45], Hashing-nvd [5]. The capability of the canonical image to represent input frames plays a crucial role in downstream applications. (Hashing-nvd consists of two canonical images. Here, we have selected the canonical image representing the foreground.)", "description": "This figure compares the quality of canonical images generated by three different methods: the proposed method (NaRCan), CoDeF, and Hashing-nvd.  NaRCan produces significantly more natural-looking canonical images, especially in scenes with complex movement, as shown in the examples for \"Train\" and \"Butterfly\".  The improved naturalness of NaRCan's canonical images is attributed to the integration of a fine-tuned diffusion prior.  The better quality of these images enhances the effectiveness of subsequent downstream video editing tasks.", "section": "4 Experiments"}, {"figure_path": "bCR2NLm1QW/figures/figures_8_1.jpg", "caption": "Figure 8: Qualitative comparisons on (a) adding handwritten characters and (b) dynamic video segmentation. Our method represents a natural image via diffusion prior, thus can achieve temporally consistent video editing and able to precisely edit desired areas.", "description": "This figure shows a comparison of the results of adding handwritten characters and dynamic video segmentation using the proposed NaRCan method and other existing methods (CoDeF and Hashing-nvd). The top row displays the results for adding handwritten characters to a video of goldfish, while the bottom row shows the results for dynamic video segmentation on a model train video.  The results demonstrate NaRCan's ability to generate more natural and temporally consistent video edits compared to other methods.", "section": "4.2 Evaluation"}, {"figure_path": "bCR2NLm1QW/figures/figures_8_2.jpg", "caption": "Figure 8: Qualitative comparisons on (a) adding handwritten characters and (b) dynamic video segmentation. Our method represents a natural image via diffusion prior, thus can achieve temporally consistent video editing and able to precisely edit desired areas.", "description": "This figure compares the results of adding handwritten characters and performing dynamic video segmentation using three different methods: the proposed method (Ours), CoDeF, and Hashing-nvd. The top row shows the results of adding handwritten characters, demonstrating the ability of each method to precisely add text to the video. The bottom row shows the results of dynamic video segmentation, illustrating the effectiveness of each method in isolating and manipulating specific objects in the video over time. The proposed method demonstrates superior performance in both tasks due to its ability to generate natural canonical images.", "section": "4.3 Ablation Study"}, {"figure_path": "bCR2NLm1QW/figures/figures_8_3.jpg", "caption": "Figure 9: Ablation studies. (a) Deformation modeling: (Top) We show that canonical images without homography modeling fail to generate a faithful image as the capacity of residual deformation MLP could dominate the training process and still achieve near-perfect frame reconstruction. (Mid) On the contrary, without residual deformation MLP, our method cannot model local non-rigid transformation, resulting in blurry foreground objects. (Bottom) Combining homography and residual deformation MLP has the best of both worlds and achieves the best canonical image representation. (b) Diffusion prior: (Top) Without diffusion prior to regularizing the canonical image, the training process relies only on the frame reconstruction and could sacrifice the faithfulness of the canonical image. (Bottom) Our fine-tuned diffusion prior effectively corrects the canonical image to faithfully represent the input frames and results in natural canonical images.", "description": "This figure presents ablation studies to demonstrate the impact of different components of the NaRCan model.  Panel (a) shows results with different combinations of homography and residual deformation MLP, highlighting the importance of both for accurate deformation modeling. Panel (b) compares results with and without the diffusion prior, showcasing how it improves the naturalness of the canonical images.", "section": "4.3 Ablation Study"}, {"figure_path": "bCR2NLm1QW/figures/figures_8_4.jpg", "caption": "Figure 9: Ablation studies. (a) Deformation modeling: (Top) We show that canonical images without homography modeling fail to generate a faithful image as the capacity of residual deformation MLP could dominate the training process and still achieve near-perfect frame reconstruction. (Mid) On the contrary, without residual deformation MLP, our method cannot model local non-rigid transformation, resulting in blurry foreground objects. (Bottom) Combining homography and residual deformation MLP has the best of both worlds and achieves the best canonical image representation. (b) Diffusion prior: (Top) Without diffusion prior to regularizing the canonical image, the training process relies only on the frame reconstruction and could sacrifice the faithfulness of the canonical image. (Bottom) Our fine-tuned diffusion prior effectively corrects the canonical image to faithfully represent the input frames and results in natural canonical images.", "description": "This figure presents ablation studies to demonstrate the effectiveness of the proposed hybrid deformation field and the diffusion prior in NaRCan. Part (a) shows the impact of using homography and residual deformation MLP for modeling deformations. It highlights that combining both methods leads to better canonical image representation. Part (b) showcases the impact of the diffusion prior in improving the quality of canonical images by ensuring naturalness and faithfulness to input frames.", "section": "4.3 Ablation Study"}, {"figure_path": "bCR2NLm1QW/figures/figures_9_1.jpg", "caption": "Figure 10: Trade-off between reconstruction quality and temporal consistency with varying separations. The visual results demonstrate that increasing the number of separations (from 1 to 5) improves the reconstruction quality of video frames. However, the table reveals a trade-off: as the number of separations increases, temporal consistency decreases. Our empirical findings suggest that using 3 separations typically achieves a balance between reconstruction quality and temporal consistency for most scenarios.", "description": "This figure shows the impact of using different numbers of separations in the Separated NaRCan model on reconstruction quality and temporal consistency.  Increasing separations improves reconstruction quality but reduces temporal consistency.  The optimal balance is found with 3 separations.", "section": "4 Experiments"}, {"figure_path": "bCR2NLm1QW/figures/figures_9_2.jpg", "caption": "Figure 11: Failure cases.", "description": "This figure shows three examples where the NaRCan model fails to generate satisfactory results, highlighting the limitations of the method in handling complex scenes with rapid movements or significant changes in lighting or object appearance.  These failure cases emphasize the challenges in processing videos with highly dynamic and non-rigid motion, where accurately representing the scene with a single canonical image is particularly difficult.", "section": "4 Experiments"}, {"figure_path": "bCR2NLm1QW/figures/figures_15_1.jpg", "caption": "Figure 12: Canonical analysis. (a) Compared to existing canonical-based methods, our approach robustly generates high-quality natural canonical images regardless of the video length. (b) Our method accurately preserves the correct foreground and background information from the original scenes, avoiding severe distortions or warping and preventing generating content inconsistent with the scene. For example, in \u201ckite-surfing\u201d, Hashing-nvd erroneously generates an additional person.", "description": "This figure compares the canonical images generated by the proposed method (NaRCan), CoDeF, and Hashing-nvd for two video clips, one with a bear and another with a kitesurfer. The left side shows the original video frames and the generated canonical images. The right side shows the foreground and background separation results for Hashing-nvd. NaRCan produces high-quality, natural-looking canonical images that faithfully represent the original video content, unlike the other methods that exhibit distortions, inaccuracies, or even generate artifacts not present in the original video. The comparison highlights NaRCan's superior ability to maintain temporal consistency and generate high-quality canonical images, crucial for accurate and coherent video editing.", "section": "A.1 Single NaRCan compared with Separated NaRCan"}, {"figure_path": "bCR2NLm1QW/figures/figures_16_1.jpg", "caption": "Figure 13: Separated NaRCan and per-frame warping. Since we have the flexibility to separate into multiple canonical images, we conduct an experiment to determine how many canonical images are optimal. We test using Separated NaRCan with segmentation equal to 3 and segmentation equal to N, where N equals the number of frames in the video. The results show that the editing information is damaged and has significant displacement due to inaccurate optical flow.", "description": "This figure compares the results of using Separated NaRCan with 3 segmentations versus using a per-frame approach for video editing. The experiment shows that using multiple segmentations(3 in this case) is significantly better in terms of preserving the integrity of editing information in the video than using a per-frame approach, where N equals to the number of frames. This is because using a per-frame approach results in significant damage to editing information due to the accumulation of inaccuracies in optical flow.", "section": "A.1 Single NaRCan compared with Separated NaRCan"}, {"figure_path": "bCR2NLm1QW/figures/figures_16_2.jpg", "caption": "Figure 3: Noise and diffusion prior update scheduling. Initially, our model fits object outlines before the fields converge and without the diffusion prior, resulting in unnatural elements in the canonical image due to complex non-rigid objects. Upon introducing the diffusion prior with increased noise and update frequency, the model learns to generate natural, high-quality images, leading to convergence. Thus, the strength of noise and the update frequency will also decrease. Moreover, it's worth mentioning that update scheduling cuts training time from 4.8 hours to 20 minutes.", "description": "This figure illustrates the training schedule for integrating a diffusion prior into the NaRCan model.  It shows how the noise level and frequency of diffusion prior updates are adjusted throughout the training process. Initially, high noise is used to quickly learn general object shapes. As the model converges, the noise is reduced, leading to more refined and natural-looking canonical images. The dynamic scheduling significantly speeds up training, reducing it from 4.8 hours to 20 minutes.", "section": "3.2 Diffusion Prior"}, {"figure_path": "bCR2NLm1QW/figures/figures_17_1.jpg", "caption": "Figure 15: Canonical images after using the grid trick. Using Separated NaRCan, we will obtain multiple canonical images, and through the grid trick, we can generate a high-quality and consistent style transfer canonical image. Therefore, Separated NaRCan still demonstrates excellent performance in this task.", "description": "This figure shows three examples of style transfer using the proposed Separated NaRCan method.  The grid trick is used to combine multiple canonical images generated for different video segments, resulting in a single, high-quality canonical image suitable for style transfer.  This demonstrates the effectiveness of the method even when handling complex video scenes that require segmentation into multiple canonical images.", "section": "A.4 Grid Trick"}]