[{"figure_path": "bGhsbfyg3b/tables/tables_20_1.jpg", "caption": "Table 1: Quantitative evaluation results of the learned dynamic P's estimation during testing. The results are calculated during testing under different [seen : unseen] ratios, where E = 20.", "description": "This table presents a quantitative evaluation of the learned dynamic transition model P's performance during the testing phase.  It shows the mean squared error (MSE) for next state and reward prediction across different ratios of seen to unseen opponent policies in the testing set.  The results are averaged over multiple experimental runs (E=20). Lower MSE values indicate better estimation accuracy.", "section": "5 Experiments"}, {"figure_path": "bGhsbfyg3b/tables/tables_32_1.jpg", "caption": "Table 1: Quantitative evaluation results of the learned dynamic P's estimation during testing. The results are calculated during testing under different [seen : unseen] ratios, where E = 20.", "description": "This table presents a quantitative evaluation of the learned dynamic transition model P's performance in estimating the next state and reward during testing.  The results are broken down by environment (Predator Prey, Level-Based Foraging, Overcooked) and different ratios of seen to unseen opponent policies in the test set (10:0, 10:5, 10:10, 5:10, 0:10). The evaluation metrics used are the average mean squared error (MSE) for the next state prediction and the average MSE for the reward prediction.  The E = 20 parameter refers to the frequency of opponent policy switches during testing.", "section": "5 Experiments"}, {"figure_path": "bGhsbfyg3b/tables/tables_33_1.jpg", "caption": "Table 1: Quantitative evaluation results of the learned dynamic P's estimation during testing. The results are calculated during testing under different [seen : unseen] ratios, where E = 20.", "description": "This table presents a quantitative analysis of the learned transition dynamics model P's performance during the testing phase.  It shows the average Mean Squared Error (MSE) for next state and reward prediction across different ratios of seen and unseen opponent policies in the testing set. The results highlight how well the model generalizes to unseen opponents.", "section": "5.2 Empirical Analysis"}]