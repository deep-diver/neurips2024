[{"type": "text", "text": "Analysis of Corrected Graph Convolutions ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Robert Wang\u2217 Aseem Baranwal \u2020 Kimon Fontoulakis\u2021 ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Machine learning for node classification on graphs is a prominent area driven by applications such as recommendation systems. State-of-the-art models often use multiple graph convolutions on the data, as empirical evidence suggests they can enhance performance. However, it has been shown empirically and theoretically, that too many graph convolutions can degrade performance significantly, a phenomenon known as oversmoothing. In this paper, we provide a rigorous theoretical analysis, based on the two-class contextual stochastic block model (CSBM), of the performance of vanilla graph convolution from which we remove the principal eigenvector to avoid oversmoothing. We perform a spectral analysis for $k$ rounds of corrected graph convolutions, and we provide results for partial and exact classification. For partial classification, we show that each round of convolution can reduce the misclassification error exponentially up to a saturation level, after which performance does not worsen. We also extend this analysis to the multi-class setting with features distributed according to a Gaussian mixture model. For exact classification, we show that the separability threshold can be improved exponentially up to $O(\\log n/\\log\\log n)$ corrected convolutions. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graphs naturally represent complex relational information found in a plethora of applications such as social analysis [Backstrom and Leskovec, 2011], recommendation systems [Ying et al., 2018, Borisyuk et al., 2024], computer vision [Monti et al., 2017], materials science and chemistry [Reiser et al., 2022], statistical physics [Battaglia et al., 2016, Bapst et al., 2020], financial forensics [Zhang et al., 2017, Weber et al., 2019] and traffic prediction in Google Maps [Derrow-Pinion et al., 2021]. ", "page_idx": 0}, {"type": "text", "text": "The abundance of relational information in combination with features of the corresponding entities has led to improved performance of machine learning models for classification and regression tasks. Central to the field of machine learning on graphs is the graph convolution operation. It has been shown empirically [Defferrard et al., 2016, Kipf and Welling, 2017, Gasteiger et al., 2019, Rossi et al., 2020] that using graph convolutions to the feature data enhances the prediction performance of a model, but too many graph convolutions can have the opposite effect [Oono and Suzuki, 2020, Chen et al., 2020, Keriven, 2022, Wu et al., 2023], an issue known as oversmoothing. Several solutions have been proposed for this problem, we refer the reader to the survey of Rusch et al. [2023]. ", "page_idx": 0}, {"type": "text", "text": "In this paper we provide a rigorous spectral analysis, based on the contextual stochastic block model [Deshpande et al., 2018], to show that the oversmoothing phenomenon can be alleviated by excluding the principal eigenvector\u2019s component from the graph convolution matrix. This is similar to the state-of-the-art normalization approach used in Zhao and Akoglu [2020], Rusch et al. [2023] (PairNorm). However, our method explicitly uses the principal eigenvector. We provide below some intuition about why excluding the principal eigenvector helps to alleviate over-smoothing. ", "page_idx": 0}, {"type": "text", "text": "Let $A$ be the adjacency matrix of the given graph, and $D$ be the degree matrix. Vanilla graph convolutions are represented using matrices such as $D^{-1}A$ or $D^{-1/2}\\bar{A}D^{-1/2}$ [Kipf and Welling, 2017]. Suppose our graph is $d-$ regular, meaning that each node has exactly $d$ neighbors. In this case, both graph convolutions reduce to $\\textstyle{\\frac{1}{d}}A$ . The top eigenvector of $A$ is $\\mathbb{1}$ with eigenvalue $d$ , where $\\mathbb{1}$ is the vector of all ones. This means that $\\begin{array}{r}{\\operatorname*{lim}_{k\\to\\infty}\\frac{1}{d^{k}}A^{k}=\\frac{1}{n}\\mathbb{1}\\mathbb{1}^{\\top}}\\end{array}$ , which implies that applying many convolutions is equivalent to projecting our data onto the all-ones vector. Thus, all feature values will converge to the same point. Therefore, we should expect, as verified by most real-world and synthetic experiments, that many rounds of the convolution $x\\mapsto{\\textstyle{\\frac{1}{d}}}A x$ will lead to a large learning error. However, if we instead perform convolution with the corrected matrix $\\begin{array}{r}{\\tilde{A}:=\\frac{1}{d}A-\\frac{1}{n}\\mathbb{1}\\mathbb{1}^{\\top}}\\end{array}$ , then the convergent behavior of $x\\mapsto\\tilde{A}^{k}x$ would be equivalent to projecting $x$ onto the second eigenvector of $A$ . This eigenvector is known to capture information about sparse bipartitions in the graph $G$ [Cheeger, 1970, Alon and Milman, 1985, Alon, 1986], and so for certain problems, like binary classification, we may expect this eigenvector to capture a larger amount of information about our signal. We note that another well-studied graph matrix is the Laplacian, D-A. In the regular case, this has the same eigenvectors as the adjacency matrix, but with reversed spectrum. The trivial eigenvector we remove is exactly the Nullspace of the Laplacian. ", "page_idx": 1}, {"type": "text", "text": "In our analysis, we study the classification problem in the contextual stochastic block model, with a focus on linear, binary classification. Our results are stated in terms of the following corrected convolution matrices: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\hat{A}=D^{-1/2}A D^{-1/2}-\\frac{1}{\\|^{\\top}D\\|}D^{1/2}\\|\\|^{\\top}D^{1/2}\\qquad\\mathrm{and}\\qquad\\tilde{A}=\\frac{1}{d}A-\\frac{1}{n}\\mathbb{1}\\mathbb{1}^{\\top},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $d:=2|E|/n$ is the empirical average degree in $A$ , where $|E|$ is the number of edges in the graph. Note that $\\hat{A}$ is derived from the normalized adjacency matrix, while $\\tilde{A}$ is (up to a scalar multiple) its unnormalized counterpart. Briefly, we demonstrate that when the graph is of reasonable quality, the corrected graph convolutions exponentially improve both partial and exact classification guarantees. Depending on the density and quality of the given graph, improvement becomes saturated after ${\\mathcal{O}}(\\log n)$ convolutions in our partial and exact classification results. However, in comparison to a similar analysis in [Wu et al., 2023] for vanilla graph convolutions (without correction), we show that classification accuracy does not become worse as the number of convolutions increases. ", "page_idx": 1}, {"type": "text", "text": "1.1 Our Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this work, we provide, to our knowledge, the first theoretical guarantees on partial and exact classification after $k$ rounds of graph convolutions in the contextual stochastic block model. Our main result is to show that each graph convolution with the corrected matrix reduces the classification error by a multiplicative factor until a certain point of \u201csaturation\u201d and the number of convolutions required until saturation depends on the amount of input feature variance. We show that the accuracy of the linear classifier at the point of saturation only depends on the strength of the signal from the graph. This is in contrast to the uncorrected convolution matrix, which will always exhibit a decrease in classification accuracy after many convolutions. Finally, we show that given slightly stronger assumptions on graph density and signal strength, the convolved data at the point of saturation will be linearly separable with high probability. To quantify our results, we let $p$ and $q$ be the intra- and inter-class edge probabilities with $\\gamma(p,q)=|p-q|/(\\dot{p}+q)$ being the \u201crelative signal strength\u201d in the graph. Let $\\bar{d}$ be the expected degree of each vertex. Our results can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 If $p+q\\geq\\Omega(\\frac{\\log^{2}n}{n})$ and $\\gamma\\geq\\Omega(\\frac{1}{\\sqrt{d}})$ , each convolution with $\\tilde{A}$ reduces classification error by a factor of about $\\textstyle{\\frac{1}{\\gamma^{2}d}}$ until the fraction of errors is $O\\big(\\frac{1}{\\gamma^{2}d}\\big)$ \u2022 If $p+q\\geq\\Omega(\\frac{\\log^{2}n}{n})$ and $\\gamma\\geq\\Omega\\big(\\sqrt{\\frac{\\log n}{d}}\\big)$ logd\u00af  n), each convolution with A\u02c6 reduces classification error by a factor of about $\\frac{\\log n}{\\gamma^{2}d}$ until the fraction of errors is $O({\\frac{\\log n}{\\gamma^{2}d}})$ \u2022 If $p+q\\geq\\Omega(\\frac{\\log^{3}n}{n})$ , $\\begin{array}{r}{\\gamma\\geq\\Omega(k\\sqrt{\\frac{\\log n}{\\bar{d}}})}\\end{array}$ and the input features has signal-to-noise ratio at least $\\Omega(\\sqrt{\\frac{\\log n}{n}})$ , the data is linearly separable after $k$ rounds of convolutions with $\\tilde{A}$ . ", "page_idx": 1}, {"type": "text", "text": "To obtain our partial classification results, we use spectral analysis to bound the mean-squared-error between the convolved features and the true signal. For exact classification, we prove a concentration inequality on the total amount of message received by a vertex through \u201cincorrect paths\u201d of length $k$ after $k$ rounds of convolution through a combinatorial moment analysis. Using this, we establish entry-wise bounds on the deviation of the convolved feature vector from the true signal. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Finally, we extend our partial-recovery result to the multi-class setting. In this setting, we assume our features are distributed according to a Gaussian mixture model with $L$ equal-sized clusters and our graph is distributed according to a $L$ -block stochastic block model. Our analysis for partial recovery generalizes easily to the multi-class setting with the use of basic non-linear classifiers. Just as before, we show that convolution with the corrected, un-normalized adjacency matrix, A\u02dc, reduces classification error by a constant fraction each round, until a point of saturation where no further improvement is made. ", "page_idx": 2}, {"type": "text", "text": "2 Literature review ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Research on graph learning has increasingly focused on methods that integrate node features with relational information, particularly within the semi-supervised node classification framework, see, for example, Scarselli et al. [2009], Cheng et al. [2011], Gilbert et al. [2012], Dang and Viennet [2012], G\u00fcnnemann et al. [2013], Yang et al. [2013], Hamilton et al. [2017], Jin et al. [2019], Mehta et al. [2019], Chien et al. [2022], Yan et al. [2021]. These studies have underscored the empirical advantages of incorporating graph structures when available. ", "page_idx": 2}, {"type": "text", "text": "The literature also addresses the expressive capacity [Lu et al., 2017, Balcilar et al., 2021] and generalization potential [Maskey et al., 2022] of Graph Neural Networks (GNNs), including challenges like oversmoothing [Keriven, 2022, Xu et al., 2021, Oono and Suzuki, 2020, Li et al., 2018, Rusch et al., 2023]. In our paper, we ground our work on the contextual stochastic block model [Deshpande et al., 2018], a widely used statistical framework for analyzing graph learning and inference problems. Recent theoretical studies have extensively used the CSBM to illustrate several statistical, information-theoretic, and combinatorial results on relational data accompanied by node features. In Deshpande et al. [2018], Lu and Sen [2020], the authors investigate the classification thresholds for accurately classifying a significant portion of nodes from this model, given linear sample complexity and large but bounded degree. Additionally, Hou et al. [2020] introduces graph smoothness metrics to quantify the utility of graphical information. Further developments in Chien et al. [2021, 2022], Baranwal et al. [2021, 2023a] extend the application of CSBM, establishing exact classification thresholds for graph convolutions in multi-layer networks, accompanied by generalization guarantees. A theoretical exploration of the graph attention mechanism (GAT) is provided by Fountoulakis et al. [2023], delineating conditions under which attention can improve node classification tasks. More recently, Baranwal et al. [2023b] provide the locally Bayes optimal message-passing architecture for node classification for the general CSBM. ", "page_idx": 2}, {"type": "text", "text": "In this paper, we provide exact and partial classification guarantees for multiple graph convolution operations. Previous investigations have often been confined to a few convolution layers, limiting the understanding of their effects on variance reduction (see, for example, Baranwal et al. [2023a]). Our findings contribute a novel spectral perspective on graph convolutions, describing how the fraction of recoverable nodes is influenced by the signal-to-noise ratio in the node features and the scaled difference between intra- and inter-class edge probabilities. We also demonstrate that the oversmoothing phenomenon can be alleviated by excluding the principal eigenvector\u2019s component from the adjacency matrix \u2013 a strategy somewhat akin to the normalization approach used in Zhao and Akoglu [2020] (PairNorm), albeit our method explicitly uses the principal eigenvector and is grounded in rigorous spectral justifications. ", "page_idx": 2}, {"type": "text", "text": "In a relatively recent work [Wu et al., 2023] the authors rigorously analyze the phenomenon of oversmoothing in GNNs for the 2-block CSBM by identifying two competing effects of graph convolutions: the mixing effect, which homogenizes node representations across different classes, and the denoising effect, which homogenizes node representations within the same class. Their analysis shows that oversmoothing occurs when the mixing effect dominates the denoising effect, and they quantify the number of layers required for this transition. In contrast, we work with the corrected graph convolution in the 2-block CSBM and show that it improves performance exponentially up to saturation, after which more convolutions do not improve nor degrade performance. On a technical level, the previous work only analyzes the distribution of a single node\u2019s feature values after convolution and does not take into account correlations between nodes. In our work, we use spectral analysis of higher powers of the convolution matrix, which takes into account correlations between nodes to obtain our partial classification results over the whole dataset. To handle the modified convolution in the exact classification setting, we analyze the error more directly through matrix perturbation analysis rather than trying to directly count the higher-order neighbors of each vertex as in previous works [Baranwal et al., 2023a, Wu et al., 2023]. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3 Preliminaries and Model Description ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Throughout this paper, we use $\\mathbb{1}$ to denote the all-ones vector and $e_{i}$ to denote the $i^{t h}$ standard basis vector in $\\mathbb{R}^{n}$ . Given a vector $x\\in\\mathbb{R}^{n}$ , we use $\\left\\|x\\right\\|$ to denote its Euclidean norm $\\sqrt{\\textstyle\\sum_{i=1}^{n}x(i)^{2}}$ . We use $\\left\\|{\\boldsymbol{x}}\\right\\|_{\\infty}$ to denote its infinity norm, $\\mathrm{max}_{i=1}^{n}\\left|x(i)\\right|$ . For a matrix , we use $\\|M\\|$ to denote its operator norm, $\\mathrm{max}_{x\\neq0,\\|x\\|=1}\\left\\|M x\\right\\|$ . We use $\\begin{array}{r}{\\|M\\|_{F}=\\sqrt{\\sum_{i,j}M_{i,j}^{2}}}\\end{array}$ to denote its Frobenius norm. We also make routine use of the spectral theorem, which says that if $M$ is a $n\\times n$ symmetric matrix, then it can be diagonalized with $n$ orthogonal eigenvectors and real eigenvalues. In particular, there exist $\\lambda_{1},\\lambda_{2},...\\lambda_{n}\\in\\mathbb{R}$ and orthonormal vectors $w_{1},w_{2},...w_{n}\\in\\mathbb{R}^{n}$ such that $\\begin{array}{r}{M=\\stackrel{\\cdot}{\\sum}_{i=1}^{n}\\lambda_{i}w_{i}w_{i}^{\\top}}\\end{array}$ . Note that when $M$ is symmetric, $\\begin{array}{r}{\\|M\\|=\\operatorname*{max}_{i}|\\lambda_{i}|=\\operatorname*{max}_{x:\\|x\\|=1}|x^{\\top}M x|}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "Finally, we use the $\\mathcal{N}(\\mu,\\Sigma)$ to a Gaussian distribution with mean $\\mu$ and covariance matrix $\\Sigma$ . For one-dimensional Gaussians, we use $\\mathcal{N}(\\mu,\\sigma^{2})$ . For $X\\,\\sim\\,{\\mathcal{N}}(\\mu,{\\dot{\\sigma}}^{2})$ , we will frequently use the Gaussian tail bound: $\\begin{array}{r}{\\mathbf{Pr}\\left[|X-\\mu|>t\\sigma\\right]\\le\\exp(-\\frac{t^{2}}{2})}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "3.1 Contextual Stochastic Block Model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we formally describe the contextual stochastic block model introduced by [Deshpande et al., 2018]. Our model is defined by parameters $n,m\\in\\mathbb{N},p,q\\in[0,1],\\mu,\\nu,\\in\\mathbb{R}^{m}$ and $\\sigma\\in\\mathbb{R}^{+}$ In the model, we are given a random undirected graph, $G=(V,E)$ , where $|V|=n$ , drawn from the 2-block stochastic block model and features drawn from the Gaussian mixture model. Our vertices are partitioned into two classes, $S$ and $T$ , of size $n/2$ , which we want to recover. For each pair of vertices $i,j\\in S$ and $i,j\\in T$ , the edge $(i,j)$ is in $E$ independently with probability $p$ while for each pair $i\\in S$ and $j\\in T$ , the edge $(i,j)$ is in $E$ with probability $q$ . In addition to the graph, we are also given a feature matrix $X\\in\\mathbb{R}^{n\\times m}$ drawn from a Gaussian mixture model with two centers $\\mu$ and $\\nu$ . For each $i\\in V$ , we let $g_{i}\\sim\\mathcal{N}(0,\\sigma^{2}I_{m})$ be an i.i.d. Gaussian noise vector. Now let $(x_{i})_{i\\in n}$ be the rows of $X$ . For $i\\in S$ , we have $x_{i}=\\mu+g_{i}$ and for each $i\\in T$ , we have $x_{i}=\\nu+g_{i}$ . ", "page_idx": 3}, {"type": "text", "text": "In the multi-class setting, our nodes are partitioned into $L$ classes, $\\mathcal{C}_{1},...\\mathcal{C}_{L}$ , of size $n/L$ . The inter-class edge probability is $p$ and intra-class edge probability is $q$ . We assume the features are generated by a Gaussian mixture with $L$ centers $c_{1},...c_{L}\\,\\in\\,\\mathbb{R}^{m}$ . If node $i$ is in class $l$ , then we observe its feature vector as $x_{j}=c_{l}+g_{i}$ . In addition, we will let $\\mu_{i}:=c_{l}$ , for $i\\in\\mathcal{C}_{l}$ , denote the center for vertex $i$ . ", "page_idx": 3}, {"type": "text", "text": "4 Results and Interpretation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In our analysis, there are two types of objectives. In the exact classification objective, the aim is to exactly recover $S$ and $T$ with probability $1-o(1)$ . In the partial classification, or \u201cdetection\u201d problem, the goal is to correctly classify $1-o(1)$ fraction of vertices correctly with probability $1-o(1)$ . We begin by stating our results for the partial classification regime. For ease of notation, we will assume $p>q$ from this point forward. We show in Appendix B.1 that this assumption is made without loss of generality. ", "page_idx": 3}, {"type": "text", "text": "Theorem 4.1. Suppose we are given a 2-block $m$ -dimensional CSBM with parameters $n,p\\ >$ $q,\\mu,\\nu,\\sigma$ satisfying $\\begin{array}{r l r}{\\gamma(p,q)}&{{}\\!:=\\!}&{\\frac{p-q}{p+q}\\;\\;\\ge\\;\\Omega\\left(\\sqrt{\\frac{1}{n p}}\\right)}\\end{array}$ and $\\begin{array}{r}{p\\ \\geq\\ \\Omega\\left(\\frac{\\log^{2}n}{n}\\right)}\\end{array}$ . There exists a linear classifier such that after $k$ rounds of convolution with $\\tilde{A}$ , will, with probability at least $\\begin{array}{r}{1-\\frac{1}{2}\\exp\\!{\\big(}{-}\\Omega\\!\\left(\\frac{n\\|\\mu-\\nu\\|^{2}}{\\sigma^{2}}\\right){\\big)}}\\end{array}$ , misclassify at most ", "page_idx": 3}, {"type": "equation", "text": "$$\nO\\left({\\frac{1}{\\gamma^{2}p}}+{\\Bigl(}{\\frac{C}{\\gamma{\\sqrt{n p}}}}{\\Bigr)}^{2k}{\\frac{\\sigma^{2}}{\\left\\|\\mu-\\nu\\right\\|^{2}}}n\\log n{\\Bigr)}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "vertices, where $C$ is an absolute constant. Furthermore, $\\textstyle i f\\gamma\\geq\\Omega({\\sqrt{\\frac{\\log n}{n p}}})$ then with probability at least $\\begin{array}{r}{1-\\frac{1}{2}\\exp(-\\Omega\\Big(\\frac{n\\|\\mu-\\nu\\|^{2}}{\\sigma^{2}}\\Big))}\\end{array}$ , the same linear classifier after $k$ rounds of convolution with $\\hat{A}\\,w i l l$ misclassify at most ", "page_idx": 4}, {"type": "equation", "text": "$$\nO\\left({\\frac{\\log n}{\\gamma^{2}p}}+{\\Bigl(}{\\frac{C\\log n}{\\gamma{\\sqrt{n p}}}}{\\Bigr)}^{2k}{\\frac{\\sigma^{2}}{\\left\\|\\mu-\\nu\\right\\|^{2}}}n\\log n{\\Bigr)}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "vertices. ", "page_idx": 4}, {"type": "text", "text": "Now we take a closer look at the error bound. For brevity, we will focus on our results regarding convolutions with $\\tilde{A}$ . First, we see that an important ratio in our bound is the term $1/(\\gamma^{2}n p)$ . This term is small if $\\gamma^{2}$ is much larger than the inverse of the expected degree of each vertex, $(p+q)n/2$ , which is at most $n p$ . Our assumption that this ratio is upper bounded by a constant means that we need the signal from the graph to be sufficiently strong. Now, if we examine our misclassification error bound, and let $\\rho=\\bar{C}/(\\bar{\\gamma}^{2}n p)$ where $C$ is a sufficiently large constant, then we see that the fraction of misclassified vertices is at most $\\rho\\!+\\!\\rho^{k}\\sigma^{2}\\log n\\big/\\left\\|\\mu-\\nu\\right\\|^{2}$ . Our assumption on the parameters ensures that $\\rho<1$ . Note that only the second term depends on $k$ , and the feature\u2019s noise-to-signal ratio. This term measures the amount of error introduced by the variance in the features and exponentially decreases with $k$ . Moreover, after about $k=\\log_{1/\\rho}\\left(\\sigma^{2}\\log n/(\\rho\\left\\|\\mu-\\nu\\right\\|^{2})\\right)$ convolutions, the $\\rho$ term, which only depends on graph parameters, will dominate over the variance term, indicating that more convolutions will not improve the quality of the convolved features beyond the quality of the signal from the graph. If $\\sigma/\\left\\|\\bar{\\mu}-\\nu\\right\\|$ is constant, we will always reach our optimal error bound of $\\bar{O(\\rho)}$ when $k=O(\\log\\log n)$ . Moreover, if $\\gamma=\\Omega(1)$ , as was assumed in Baranwal et al. [2021], then we will have $1/\\rho\\ge\\Omega(n p)$ . This means even when $\\sigma/\\left\\Vert\\mu-\\nu\\right\\Vert\\approx\\sqrt{n/\\log n}$ , we will reach optimality in constant nu\u221amber of convolutions with high probability if the graph is moderately dense. For example, if $p=1/\\sqrt{n}$ , then we only need 3 convolutions and if $p=\\bar{\\Omega(1)}$ , then we only need 2. On the other hand, if $\\gamma$ is on the order of $\\Theta(1/{\\sqrt{n p}})$ , then in the worst case, we may need $\\log n$ convolutions to reach our optimal bound. Next, we state our results for exact classification. ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.2. Suppose we are given a 2-block $m$ -dimensional CSBM with parameters $n,p\\ >$ q, \u00b5, \u03bd, \u03c3 satisfying \u03b3(p, q) \u2265\u2126 k longp n and $p\\,\\geq\\,{\\frac{\\log^{3}n}{n}}$ . Then after $k=O(\\log n)$ rounds of graph convolution with $\\tilde{A}$ , our data is linearly separable with probability $1-n^{-\\Omega(1)}\\;i\\!f$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{\\left\\|\\mu-\\nu\\right\\|}{\\sigma}\\geq\\Omega\\left(\\operatorname*{max}\\left(\\sqrt{\\frac{\\log n}{n}},\\,\\left(\\frac{C}{\\gamma\\sqrt{n p}}\\right)^{k}\\sqrt{\\log n}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $C$ is an absolute constant. ", "page_idx": 4}, {"type": "text", "text": "Here, we bound the minimum signal-to-noise ratio required for exact classification as a function of $p,q,n$ and $k$ . Just like the partial classification result, our function has a term that decreases exponentially with $k$ and a term independent of $k$ . The rate of decrease for the dependent term is proportional to $1/(\\gamma{\\sqrt{n p}})$ , or $\\sqrt{\\rho}$ . We see once again that with more convolutions, the requirement on the feature signal-to-noise ratio for exact classification becomes exponentially weaker. Moreover, since we assumed that $\\gamma\\geq\\Omega(k{\\sqrt{\\log n/(n p)}})$ , as long as $\\|\\mu-\\nu\\|\\,\\geq\\,\\Omega(\\sigma{\\sqrt{\\log n/n}})$ , the data becomes linearly separable after $k=O(\\log n/\\log\\log n)$ convolutions. Just as in the partial classification case, we observe that the larger $\\gamma$ is, the fewer convolutions we need to obtain the optimal bound. In particular,\u221a if $\\gamma=\\Omega(1)$ and $p=\\Omega(1)$ then one convolution already gives the optimal bound, and if $p=1/\\sqrt{n}$ , then two convolutions are enough. For technical reasons, we only analyze exact classification using convolution with the corrected un-normalized adjacency matrix, A\u02dc. Similar bounds should hold for $\\hat{A}$ based on our simulation results, but we leave this for future work. ", "page_idx": 4}, {"type": "text", "text": "4.1 Discussion on our Assumptions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Both Theorem 4.1 and Theorem 4.2 require a lower bound of $\\gamma\\geq\\omega(1/\\sqrt{n p})$ , and this is to ensure that the signal from the graph is strong enough so that a convolution does not destroy the signal from the data. Also, implicit in the probability bound of Theorem 4.\u221a1 and in Theorem 4.2, is the assumption that our signal-to-noise ratio, $\\|\\mu-\\nu\\|\\,/\\sigma$ , is at least $\\omega(1/\\sqrt{n})$ so the feature noise does not completely drown out the signal. Our lower-bound assumption on $p$ is to ensure concentration in the behavior of the degrees and the adjacency matrix towards their expectation. In Theorem 4.2, we also assume that $k=O(\\log n)$ . This is done mainly for technical reasons of our proof but we note that this assumption is made without loss of generality because as mentioned, the bound in Theorem 4.2 does not improve for $k\\geq\\log n$ . Finally, we note that the case $p>q$ corresponds to a homophilous graph, and the case $p<q$ corresponds to a heterophilous graph (see Luan et al. [2021], Ma et al. [2022] for more). For binary classification, it has been shown [Baranwal et al., 2023a] that one can assume $p>q$ without loss of generality and make corresponding adjustments in the classifier. As such, we assume that $p>q$ . For more detail regarding this assumption, see Appendix B.1. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "5 One-Dimensional CSBM ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In Baranwal et al. [2021], the authors showed that analyzing the linear classifier for the $m$ -dimensional CSBM reduces to analyzing the 1-dimensional model. We say that a CSBM is one-dimen\u221asional and c\u221aentered with parameters $n,p,q,\\sigma$ if it has one-dimensional features and means $1/\\sqrt{n}$ and $-1/\\sqrt{n}$ . That i\u221as, we have one feature\u221a vector $x\\in\\mathbb{R}^{n}$ given by $x=s+g$ , where $g\\sim\\mathcal{N}(0,\\sigma^{2}I_{n})$ and ${\\dot{s}}(i)=1/{\\sqrt{n}}$ for $i\\in S$ and $-1/\\sqrt{n}$ for $i\\in T$ . We will refer to $s$ as our signal vector and for ease of notation, we normalize it so that it always has unit norm. The following lemma allows us to reduce the analysis of the linear classifier for a general CSBM to the analysis of the 1-dimensional centered CSBM (proof in Appendix B.1). Thus, in the proofs of our main theorems, we will analyze the 1-dimensional case before applying Lemma 5.1. ", "page_idx": 5}, {"type": "text", "text": "Lemma 5.1. Given an $m$ -dimensional 2-block CSBM, there exist $w\\in\\mathbb{R}^{m}$ and $b\\in\\mathbb{R}^{n}$ such that Xw + b = s + g where for each vertex i, gi is i.i.d. N(0, \u03c3\u20322) with \u03c3\u2032 =\u221an\u22254\u00b5\u03c3\u2212\u03bd\u2225. ", "page_idx": 5}, {"type": "text", "text": "In the 1-dimensional model, it is clear how our signal $s$ is present in our features. Our convolution matrix also captures the signal because it can be viewed as a perturbation of the matrix $s s^{\\top}$ . This is especially evident with the un-normalized convolution matrix $\\tilde{A}$ , which satisfies the following ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tilde{A}=\\eta s s^{\\top}+\\frac{1}{d}R+d^{\\prime}\\mathbb{1}\\mathbb{1}^{\\top}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\eta:=(p-q)n/(2d)$ is the signal strength, $d^{\\prime}:=((p+q)n/2-d)/(n d)$ is the average degree deviation, and $R$ is the \u201cedge-deviation\" matrix, where $R_{i,j}\\,=\\,A_{i,j}\\,-\\,\\mathbb{E}[A_{i,j}]$ . Since $R$ has i.i.d. zero-mean entries with variance at most $p$ , we can use standard matrix concentration inequalities to show it is not too big. Likewise, $d^{\\prime}$ is small due to degree concentration, and together, these two concentration results imply that $\\tilde{A}$ is close to $\\gamma\\boldsymbol{s}\\boldsymbol{s}^{\\intercal}$ . In fact, if we show degree concentration for all vertices, then we can show that $\\hat{A}$ also behaves like $\\tilde{A}$ . We state these concentration results below. ", "page_idx": 5}, {"type": "text", "text": "Proposition 5.2. Assume that $p=\\Omega(\\frac{\\log^{2}n}{n}),$ , and let $\\begin{array}{r}{\\gamma=\\frac{p-q}{p+q}}\\end{array}$ . With probability $1-n^{-\\Omega(1)}$ , $w e$ have the following concentration results ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\tilde{A}-\\gamma s s^{\\top}\\right\\|\\leq O(\\frac{1}{\\sqrt{n p}})\\,a n d\\left\\|\\hat{A}-\\gamma s s^{\\top}\\right\\|\\leq O(\\sqrt{\\frac{\\log n}{n p}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "These concentration properties are crucial for spectral analysis. Details are given in Appendix B.1. ", "page_idx": 5}, {"type": "text", "text": "6 Partial Classification ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we give a sketch of the proof of our partial classification result, Theorem 4.1. The full proofs can be found in Appendix C. We will show that partial classification can be achieved if the convolved vector is well-correlated with our signal vector $s$ as defined in the beginning of Section 5 in the centered-1 dimensional CSBM. We will analyze our result for convolutions using the matrix $M\\in\\{\\tilde{A},\\hat{A}\\}$ . ", "page_idx": 5}, {"type": "text", "text": "Proposition 6.1. Given a centered $^{\\,l}$ -dimensional 2 block CSBM with parameters $n,p>q$ , $\\sigma$ and $\\begin{array}{r}{\\gamma(p,q)=\\frac{p-q}{p+q}}\\end{array}$ , suppose our convolution matrix $M$ satisfies $\\left\\|M-\\gamma s s^{\\top}\\right\\|\\leq\\delta$ and $\\gamma\\geq C\\delta$ for $a$ large enough constant $C$ . Let $x_{k}=M^{k}x_{i}$ , be the result of applying $k$ rounds of graph convolution to our input feature $x$ . Then with probability at least $\\begin{array}{r}{1-\\frac12\\exp\\bigl(-\\frac1{4\\sigma^{2}}\\bigr)}\\end{array}$ , there exists a scalar $C_{k}$ and an absolute constant $C^{\\prime}$ such that ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\|C_{k}x_{k}-s\\|^{2}\\leq O\\left(\\frac{\\delta^{2}}{\\gamma^{2}}+\\Big(\\frac{C^{\\prime}\\delta}{\\gamma}\\Big)^{2k}n\\sigma^{2}\\log n\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The main idea of our analysis is to use the fact that the top eigenvector of $M$ , denoted $\\hat{s}$ , is well correlated with our signal $s$ . Since $\\|M-\\gamma s s^{\\top}\\|\\;\\leq\\;\\delta$ and $\\gamma\\~>~C\\delta$ by assumption, standard Matrix perturbation arguments imply that the spectrum of $M$ will be in $(\\gamma\\pm\\delta,\\pm\\delta,\\pm\\delta,...)$ with high probability. Given our assumption of $\\gamma\\,>\\,C\\delta$ , there will be a large gap between the top eigenvalue of $M$ and the rest of its eigenvalues. A well-known result Davis and Kahan implies that $\\left\\|s-\\hat{s}\\right\\|^{2}\\,\\leq\\,O(\\delta^{2}/\\gamma^{2})$ , i.e. $s$ is close to $\\hat{s}$ . Thus, we prove Proposition 6.1 by showing that the influence of the rest of the eigenvectors on our convolved vector, $x_{k}=M^{k}x$ , decreases exponentially with $k$ , which allows us to bound the squared norm distance between $x_{k}$ and $s$ . In particular, we take our normalization constant to be $\\dot{C}_{k}\\,\\approx\\,1/\\lambda_{1}^{k}$ , where $\\lambda_{1}$ is the maximum eigenvalue of $M$ . Note that $\\mathrm{lim}_{k\\rightarrow\\infty}(1/\\lambda_{1}^{k})M^{k}\\,=\\,\\hat{s}\\hat{s}^{\\top}$ . Roughly speaking, we decompose our convolution vector as $C_{k}x_{k}\\approx(1/\\lambda_{1}^{k})M^{k}s+(1/\\lambda_{1}^{k})M^{k}{}_{\\mathcal{G}}$ . To bound the distance of this vector from $s$ , we analyze the contribution of each of the two terms to our error separately. That is, we show that with high probability $\\|(1/\\lambda_{1}^{k})M^{k}s-s\\|^{2}\\leq O(\\delta^{2}/\\gamma^{2})$ and $\\left\\|M^{k}g\\right\\|^{2}\\leq O((\\delta/\\gamma)^{2k}n\\sigma^{2}\\log n)$ . Note that the first error term is from taking the convolution of the noisy graph with the true signal, and thus does not decrease with $k$ . The second error term, on the other hand, comes from variance in the features, $g$ , and thus decreases with our noise level $\\sigma$ and drops exponentially with each convolution. ", "page_idx": 6}, {"type": "text", "text": "Finally, given Proposition 6.1, we can prove the partial classification result by noting that if we partition the convolved 1-dimensional data around 0, then each misclassified vertex contributes $1/n$ to the mean-squared error, which means the number of misclassified vertices is at most $\\left\\|C_{k}x_{k}-s\\right\\|^{2}n$ . This, combined with Lemma 5.1 to generalize to the $m$ \u2212dimensional case will prove Theorem 4.1 ", "page_idx": 6}, {"type": "text", "text": "7 Exact Classification ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we sketch the proof of Theorem 4.2 for exact classification using the un-normalized corrected convolution matrix A\u02dc. Full proofs can be found in Appendix D. To show linear separability, we would like $x_{k}\\,=\\,\\tilde{A}^{k}x$ to have positive entries for all vertices in $S$ and negative entries for all vertices in $T$ . This means that we want to show $\\left\\|C_{k}x_{k}-s\\right\\|_{\\infty}<1/\\sqrt{n}$ for some appropriate scalar $C_{k}$ . In particular, we will take $C_{k}$ to be $1/\\eta^{k}$ , where $\\eta$ is our empirical estimate of $\\gamma(p,q)$ . In partial classification, it sufficed to bound the mean squared error $\\left\\|C_{k}x_{k}-s\\right\\|_{2}^{2}$ , using spectral analysis but bounding $\\|C_{k}x_{k}-s\\|_{\\infty}$ requires more work because now we are bounding the entrywise instead of average error. In our approach, we bound the volume of messages passed through \u201cincorrect paths\u201d in our graph and show that the contribution from these messages is small. Then, we show the other source of error, the feature variance, is reduced exponentially with each round of convolution. As with the partial classification result, we first prove our result in the 1-dimensional centered model: ", "page_idx": 6}, {"type": "text", "text": "Proposition 7.1. Suppose we are given a $^{\\,l}$ -dimensional centered 2-block CSBM with parameters $n,p,q,\\sigma$ and $k=O(\\log n)$ such that $\\begin{array}{r}{\\gamma=\\frac{p-q}{p+q}\\geq\\Omega\\left(k\\sqrt{\\frac{\\log n}{n p}}\\right)}\\end{array}$ , $p\\geq{\\frac{\\log^{3}n}{n}}$ , and $\\begin{array}{r}{\\sigma\\leq O\\left(\\frac{1}{\\sqrt{\\log n}}\\right)}\\end{array}$ Then with probability at least $1-n^{-\\Omega(1)}$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\left\\|\\frac{1}{\\eta^{k}}\\tilde{A}^{k}x-s\\right\\|_{\\infty}\\leq\\frac{1}{2\\sqrt{n}}+O\\left(\\left(\\frac{C}{\\gamma\\sqrt{n p}}\\right)^{k}\\sigma\\sqrt{\\log n}\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Given, Proposition 7.1, Theorem 4.2 follows immediately by applying Lemma 5.1. We now give a sketch of our proof for Proposition 7.1. To bound each entry of $C_{k}x_{k}\\mathrm{~-~}s$ , we must bound $|e_{u}^{\\top}\\tilde{A}^{k}x/\\eta^{k}-e_{u}^{\\top}s|$ for all $u~\\in~V$ . Similar to in partial classification, we will split our error into error from $\\tilde{A}^{k}s$ and error from ${\\tilde{A}}^{k}g$ . That is, for each $u\\,\\in\\,V$ , we separately upper bound $|e_{u}^{\\top}(\\eta s s^{\\top}+R^{\\prime})^{k}s-e_{u}^{\\top}s|$ and $|e_{u}^{\\top}(\\eta s s^{\\top}+R^{\\prime})^{k}g|$ , where $R^{\\prime}=\\tilde{A}\\!-\\!\\eta s s^{\\top}$ . The matrix $(\\eta s s^{\\top}+R^{\\prime})^{k}$ , when expanded out, can be written as $\\eta^{k}s s^{\\top}$ plus a sum of $2^{k}-1$ terms, each of which is a noncommutative product of matrices of the form $\\bar{\\eta}{s}s^{\\top}$ or $R^{\\prime}$ . We group these error matrices into terms of order $\\ell$ for $\\ell\\in[k]$ , where the $\\ell^{t h}$ order terms are comprised of products that contain $\\ell$ copies of $R^{\\prime}$ and $k-\\ell$ copies of $\\bar{\\gamma}s s^{\\top}$ . ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "In our analysis, we first use degree concentration to show that instead of analyzing the error w.r.t. $R^{\\prime}$ , it suffices to analyze the error w.r.t. $\\scriptstyle{\\frac{1}{d}}R$ . To bound $\\begin{array}{r}{e_{u}^{\\top}(\\eta s s^{\\top}+\\frac{1}{d}R)^{k}\\dot{s}}\\end{array}$ , we expand it out and find that each term arising from an error matrix of order $\\ell$ can be written as a multiple of $e_{u}^{\\top}R^{a_{1}}s\\!\\cdot\\!s^{\\top}R^{a_{2}}s\\!\\cdot\\!...s^{\\top}R^{a_{L}}s$ where $a_{1},...a_{L}$ are non-negative integers satisfying $a_{1}\\!+\\!a_{2}\\!+\\!...a_{L}=\\ell$ . The symmetric terms can be bounded by showing $s^{\\top}R^{a}\\bar{s}\\lesssim(\\sqrt{n p})^{\\bar{a}}$ using simple spectral arguments. To control the asymmetric term, we show that with high probability, $\\begin{array}{r}{|e_{u}^{\\top}R^{a_{1}}s|\\leq\\frac{1}{\\sqrt{n}}(C n p\\log n)^{a_{1}/2}}\\end{array}$ for a constant $C$ . This part is the most technical part and requires the slightly stronger graph density assumption: $p\\geq\\Omega(\\log^{3}n/n)$ . Thus, we have $|\\dot{e}_{u}^{\\top}R^{a_{1}}s\\cdot s^{\\top}R^{a_{2}}s\\cdot...s^{\\top}R^{a_{L}}s|\\leq\\frac{{\\mathrm{\\bar{\\Phi}}}}{\\sqrt{n}}(\\bar{C}\\bar{n}\\log n)^{\\ell/2}$ for some constant $C$ . The analysis for bounding $e_{u}\\tilde{A}^{k}g$ is similar. Finally, by combining these bounds with our assumptions that $\\gamma$ is large enough, we obtain Proposition 7.1. ", "page_idx": 7}, {"type": "text", "text": "Now, we take a closer look at the step of bounding the asymmetric term. Recall that $R$ is a random symmetric matrix with i.i.d. zero mean entries. The term $e_{u}^{\\top}R^{\\ell}s$ can be expressed as $\\begin{array}{r}{\\sum_{w}R_{w(0),w(1)}R_{w(1),w(2)},...R_{w(\\ell-1),w(a)}s(w(a))}\\end{array}$ where the sum is over all walks, $w$ , of length $a$ in the complete graph over $n$ vertices starting at $w(0):=u$ . From a message-passing perspective, one can interpret this as bounding the deviation between the amount of signal message $u$ receives over paths of a certain length and the amount of message it expects to receive. To bound this term, we use a path counting argument to control its higher moments, and then apply Markov\u2019s inequality: $\\begin{array}{r}{\\mathbf{Pr}\\left[|e_{u}^{\\top}\\mathring{R}^{a}s|\\ge\\lambda\\right]<\\frac{1}{\\lambda^{2t}}\\mathbb{E}[|e_{u}^{\\top}R^{a}s|^{2t}]}\\end{array}$ . ", "page_idx": 7}, {"type": "text", "text": "8 Multi-class Analysis on Gaussian Mixture Model ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we will formally state and sketch our results for the multi-class analysis. Full proofs are in Appendix E. For simplicity, we will only analyze convolution with the un-normalized corrected convolution matrix, $\\tilde{A}$ . The reason that the corrected convolution still gives good performance is that when class sizes are balanced, the second eigenspace of the expected adjacency matrix has multiplicity $L-1$ and exactly captures the $L$ clusters (see Lemma E.1). Before formally stating our result, we will introduce some useful notation: ", "page_idx": 7}, {"type": "text", "text": "\u2022 Graph Signal: $\\begin{array}{r}{\\lambda:=\\frac{(p-q)n}{d L}}\\end{array}$ is the strength of the signal from the graph. ", "page_idx": 7}, {"type": "text", "text": "\u2022 Graph Noise: $\\begin{array}{r}{\\delta:=C(\\frac{1}{d}(\\sqrt{n p(1-p)/L}+\\sqrt{n q(1-q)}))}\\end{array}$ ) for some constant $C.\\ \\delta$ is an upper bound on the graph noise. ", "page_idx": 7}, {"type": "text", "text": "\u2022 Let $U:=\\mathbb{E}[X]$ be the matrix whose $i^{t h}$ column is $\\mu_{i}$ . We also assume our features are centered on expectation so that $U^{\\top}\\mathbf{1}=0$ . This is not restrictive since it can be satisfied by applying a linear shift to the features \u2022 Let $\\begin{array}{r}{\\Delta=\\operatorname*{min}_{i,j\\in[n]}\\|\\mu_{i}-\\mu_{j}\\|}\\end{array}$ be the minimum distance between the centers ", "page_idx": 7}, {"type": "text", "text": "Theorem 8.1. Given the CSBM with parameters, $p,q,L,n,m,$ , suppose $\\begin{array}{r}{\\operatorname*{min}(p,q)\\geq\\Omega(\\frac{\\log^{2}n}{n})}\\end{array}$ and $|\\lambda|>4k\\delta$ . Let $\\begin{array}{r}{X^{(k)}=\\frac{1}{\\lambda^{k}}\\tilde{A}^{k}X}\\end{array}$ be the feature matrix after $k$ rounds of convolutions with scalaing factor $1/\\lambda^{k}$ . Let $x_{i}^{(k)}$ be the $i^{t h}$ row of the matrix $X^{(k)}$ . Then with probability $1-n^{-\\Omega(1)}$ , at least $n-n_{e}$ nodes, $i$ , satisfy $\\left\\|x_{i}^{(k)}-\\mu_{i}\\right\\|<\\Delta/2$ where ", "page_idx": 7}, {"type": "equation", "text": "$$\nn_{e}=O\\Big((k\\delta/|\\lambda|)^{2}\\frac{\\|U\\|_{F}^{2}}{\\Delta^{2}}+(L+n(\\delta/|\\lambda|)^{2k})\\frac{\\sigma^{2}m\\log n}{\\Delta^{2}}\\Big).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In particular, the quadratic classifer $x\\,\\mapsto\\,s o f t m a x(\\left\\lvert\\lvert x-c_{l}\\right\\rvert\\right\\rvert^{2})_{l=1}^{L}$ will correctly classify at least $n-n_{e}$ points, and when $n_{e}=o(n)$ , then we can correctly classify ${\\mathrm{\\dot{1}}}-o(1)$ fraction of points. ", "page_idx": 7}, {"type": "text", "text": "Intuitively, our theorem states that each convolution will cause the points in each cluster to \u201ccontract\u201d towards their means up until a certain saturation point is reached. If after this contraction, many points are closer to their own centers than to any other centers, the softmax classifier will correctly classify them. Just as in Theorem 4.1, our error bound consists of one component depending on the variance, $\\sigma^{2}$ , that is large at the beginning and decreases exponentially with each convolution. The classification accuracy at the point of saturation $\\left(k\\approx\\log n\\right)$ depends on the squared product between graph\u2019s signal-to-noise ratio, $\\delta/|\\lambda|.$ , and the \u201cseparation ratio\u201d of the datasets: $\\|U\\|_{F}\\,/\\Delta$ . As in the two-class setting, our theorem captures both the homophilic and heterophilic settings. However, for large $L$ , our error parameter, $\\delta$ , can be much larger if $q>p$ . This observation of noisier graphs in the heterophilic setting, leading to less accurate performance, is consistent with observations from previous studies [Choi et al., 2023]. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "9 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we demonstrate our results empirically. For synthetic data, we show Theorems 4.1 and 4.2 for linear binary classification. For real data, we show that removing the principal component of the adjacency matrix exhibits positive effects on multi-class node classification problems as well. ", "page_idx": 8}, {"type": "text", "text": "9.1 Synthetic Data ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "For synthetic data from the CSBM, we demonstrate the beneftis of removing the principal component of the adjacency matrix before performing convolutions for both variants of convolution described in Equation (1). We choose $n=2000$ nodes with 20 features for each node, sampled from a Gaussian mixture. The intra-edge probability is fixed to $p=O(\\log^{3}n/n)$ . We perform linear classification to demonstrate the results in Theorem 4.1 and Theorem 4.2, training a one-layer GCN network both with and without the corrected convolutions and perform an empirical comparison. ", "page_idx": 8}, {"type": "text", "text": "We provide plots for two different settings: (1) Fix $\\gamma=|p-q|/(p+q)=2/3$ and vary signal-tonoise ratio of the node features, $\\|\\mu-\\nu\\|/\\sigma$ , for different number of convolutions. We observe in Figure 1 that as the number of convolutions increases, the original GCN [Kipf and Welling, 2017] (in blue) starts performing poorly, while the corrected versions (in orange and green) retain the accuracy for lower signal-to-noise ratio; (2) Fix $\\|\\mu-\\nu\\|/\\sigma=1$ and vary the graph relative signal strength, $\\gamma$ , for different number of convolutions. We observe the same trends in this setting, as depicted in Figure 2. The vertical lines represent the threshold for exact classification from Theorem 4.2. ", "page_idx": 8}, {"type": "image", "img_path": "MSsQDWUWpd/tmp/6fcc10f0ccb6865a89b3ffeb834b66586ecf75fdefa92cb83ec47aa50ef23c36.jpg", "img_caption": ["Figure 1: Accuracy plot (average over 50 trials) against the signal-to-noise ratio of the features (ratio of the distance between the means to the standard deviation) for increasing number of convolutions. Here, $v\\;=\\;D^{1/2}\\mathbb{1}$ and the \u201cGCN with $v v^{\\top}$ removed\u201d refers to convolution with the corrected, normalized adjacency matrix. \u201cGCN with $\\mathbb{1}\\mathbb{1}^{\\top}$ removed\u201d is the corrected, unnormalized matrix. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "9.2 Real Data ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Similar to synthetic data, we compare the results for corrected graph convolution to the original GCN on the following real graph benchmarks datasets: CORA, CiteSeer, and Pubmed citation networks [Sen et al., 2008] in the multi-class setting. In Figure 3, we see that overall the accuracy of every learning method decreases as the number of convolutions increases but the corrected convolutions converge to an accuracy much higher than that of the uncorrected convolution. This is attributed to the fact that for multi-class classification on general graphs, the important information about class memberships is typically captured by the top $C$ eigenvectors (except the first one) where $C$ is greater than the number of classes [Lee et al., 2014]. In general, these eigenvectors could have different eigenvalues. Since the limiting behavior of many rounds of convolutions is akin to projecting the features onto the eigenvector(s) corresponding to the second eigenvalue, we only expect this to capture partial information about the multi-class structure. By contrast, we show, in Appendix E.1, that for synthetic data with balanced classes, the classification accuracy only increases with more convolutions if they are corrected to remove the top eigenvector. ", "page_idx": 8}, {"type": "image", "img_path": "MSsQDWUWpd/tmp/60338daf69416d578beea15f267e744c24cd5f5c7d7fbc94f63eea549aa403b0.jpg", "img_caption": ["Figure 2: Accuracy plot (average over 50 trials) against graph relative signal strength $(\\gamma=\\vert p-$ $q|/(p+q))$ for various values of the number of convolutions. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "image", "img_path": "MSsQDWUWpd/tmp/b53b8f33dedc49dfe956370b61a07a8df9fb7b541b9392a2174c98c7e87f0b0e.jpg", "img_caption": ["Figure 3: Accuracy plots (average over 50 trials) against the number of layers for real datasets. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "10 Conclusion and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this study, we utilized spectral methods to obtain partial and exact classification results for the linear classifier with corrected convolution matrices in the 2-block CSBM. Our spectral approach highlights, theoretically, how removing the top eigenvector can mitigate oversmoothing and improve classification accuracy. We prove that the removal of the top eigenvector results in reducing feature variance and correcting the asymptotic behavior of many rounds of convolution towards the second, rather than the top eigenvector of the adjacency matrix. Finally, we showed that our analysis can be generalized to the multi-class setting. We hope our analysis can lead to further developments in theoretical and practical studies of GNNs. A natural extension of this work would be to generalize our analysis to broader classes of multi-class models. For example, if the size of classes are unbalanced, the second eigenspace may not capture all the information about the clusters. In addition, the distribution of features may not follow a standard Gaussian mixture model, but more complicated distributions, possibly with multiple centers [Baranwal et al., 2023a]. Another natural setting to consider is when clusters in the feature distribution do not exactly match clusters in the graph. Extending our analysis to these settings will likely require more sophisticated network architectures and activation functions. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "K. Fountoulakis would like to acknowledge the support of the Natural Sciences and Engineering Research Council of Canada (NSERC). Cette recherche a \u00e9t\u00e9 financ\u00e9e par le Conseil de recherches en sciences naturelles et en g\u00e9nie du Canada (CRSNG), [RGPIN-2019-04067, DGECR-2019-00147]. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Noga Alon. Eigenvalues and Expanders. Combinatorica, 6:83\u201396, 1986. ", "page_idx": 10}, {"type": "text", "text": "Noga Alon and Vitali Milman. $\\ell_{1}$ -Isoperimetric Inequalities for Graphs, and Superconcentrators. Journal of Combinatorial Theory, Series B, 38(1):73\u201388, 1985.   \nLars Backstrom and Jure Leskovec. Supervised random walks: predicting and recommending links in social networks. In Proceedings of the fourth ACM international conference on Web search and data mining, pages 635\u2013644, 2011.   \nMuhammet Balcilar, Guillaume Renton, Pierre H\u00e9roux, Benoit Ga\u00fcz\u00e8re, S\u00e9bastien Adam, and Paul Honeine. Analyzing the Expressive Power of Graph Neural Networks in a Spectral Perspective. In International Conference on Learning Representations, 2021.   \nAfonso S. Bandeira and Ramon van Handel. Sharp Nonasymptotic Bounds on the Norm of Random Matrices with Independent Entries. The Annals of Probability, 44(4):2479 \u2013 2506, 2016.   \nVictor Bapst, Thomas Keck, A Grabska-Barwi\u00b4nska, Craig Donner, Ekin Dogus Cubuk, Samuel S Schoenholz, Annette Obika, Alexander WR Nelson, Trevor Back, Demis Hassabis, et al. Unveiling the Predictive Power of Static Structure in Glassy Systems. Nature Physics, 16(4):448\u2013454, 2020.   \nAseem Baranwal, Kimon Fountoulakis, and Aukosh Jagannath. Graph Convolution for SemiSupervised Classification: Improved Linear Separability and Out-of-Distribution Generalization. In Proceedings of the 38th International Conference on Machine Learning, volume 139, pages 684\u2013693, 2021.   \nAseem Baranwal, Kimon Fountoulakis, and Aukosh Jagannath. Effects of Graph Convolutions in Multi-layer Networks. In The Eleventh International Conference on Learning Representations, 2023a.   \nAseem Baranwal, Kimon Fountoulakis, and Aukosh Jagannath. Optimality of message-passing architectures for sparse graphs. In Advances in Neural Information Processing Systems, volume 36, pages 40320\u201340341, 2023b.   \nP. Battaglia, R. Pascanu, M. Lai, D. J. Rezende, and K. Kavukcuoglu. Interaction Networks for Learning about Objects, Relations and Physics. In Advances in Neural Information Processing Systems (NeurIPS), 2016.   \nFedor Borisyuk, Shihai He, Yunbo Ouyang, Morteza Ramezani, Peng Du, Xiaochen Hou, Chengming Jiang, Nitin Pasumarthy, Priya Bannur, Birjodh Tiwana, Ping Liu, Siddharth Dangi, Daqi Sun, Zhoutao Pei, Xiao Shi, Sirou Zhu, Qianqi Shen, Kuang-Hsuan Lee, David Stein, Baolei Li, Haichao Wei, Amol Ghoting, and Souvik Ghosh. LiGNN: Graph Neural Networks at LinkedIn, 2024.   \nJeff Cheeger. A Lower Bound for the Smallest Eigenvalue of the Laplacian. Problems in Analysis, pages 195\u2013199, 1970.   \nDeli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and Relieving the Over-Smoothing Problem for Graph Neural Networks from the Topological View. Proceedings of the AAAI Conference on Artificial Intelligence, 34(04):3438\u20133445, Apr. 2020.   \nHong Cheng, Yang Zhou, and Jeffrey Xu Yu. Clustering Large Attributed Graphs: A Balance between Structural and Attribute Similarities. ACM Transactions on Knowledge Discovery from Data, 12, 2011.   \nEli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive Universal Generalized PageRank Graph Neural Network. In International Conference on Learning Representations, 2021.   \nEli Chien, Wei-Cheng Chang, Cho-Jui Hsieh, Hsiang-Fu Yu, Jiong Zhang, Olgica Milenkovic, and Inderjit S Dhillon. Node Feature Extraction by Self-Supervised Multi-scale Neighborhood Prediction. In International Conference on Learning Representations, 2022.   \nYoonhyuk Choi, Jiho Choi, Taewook Ko, and Chong-Kwon Kim. Improving signed propagation for graph neural networks in multi-class environments. arXiv preprint arXiv:2301.08918, 2023.   \nT. A. Dang and E. Viennet. Community Detection based on Structural and Attribute Similarities. In The Sixth International Conference on Digital Society (ICDS), 2012.   \nMicha\u00ebl Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016.   \nAustin Derrow-Pinion, Jennifer She, David Wong, Oliver Lange, Todd Hester, Luis Perez, Marc Nunkesser, Seongjae Lee, Xueying Guo, Brett Wiltshire, Peter W. Battaglia, Vishal Gupta, Ang Li, Zhongwen Xu, Alvaro Sanchez-Gonzalez, Yujia Li, and Petar Velickovic. ETA Prediction with Graph Neural Networks in Google Maps. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management, CIKM \u201921, page 3767\u20133776, 2021.   \nYash Deshpande, Subhabrata Sen, Andrea Montanari, and Elchanan Mossel. Contextual Stochastic Block Models. In Advances in Neural Information Processing Systems (NeurIPS), 2018.   \nKimon Fountoulakis, Amit Levi, Shenghao Yang, Aseem Baranwal, and Aukosh Jagannath. Graph attention retrospective. Journal of Machine Learning Research, 24(246):1\u201352, 2023.   \nJohannes Gasteiger, Aleksandar Bojchevski, and Stephan G\u00fcnnemann. Predict then Propagate: Graph Neural Networks meet Personalized PageRank. In International Conference on Learning Representations, 2019.   \nJerome Gilbert, Ernest Valveny, and Horst Bunke. Graph Embedding in Vector Spaces by Node Attribute Statistics. Pattern Recognition, 45(9):3072\u20133083, 2012.   \nStephan G\u00fcnnemann, Ines F\u00e4rber, Sebastian Raubach, and Thomas Seidl. Spectral Subspace Clustering for Graphs with Feature Vectors. In IEEE 13th International Conference on Data Mining, 2013.   \nWilliam L. Hamilton, Rex Ying, and Jure Leskovec. Inductive Representation Learning on Large Graphs. NIPS\u201917: Proceedings of the 31st International Conference on Neural Information Processing Systems, pages 1025\u20131035, 2017.   \nYifan Hou, Jian Zhang, James Cheng, Kaili Ma, Richard T. B. Ma, Hongzhi Chen, and Ming-Chang Yang. Measuring and Improving the Use of Graph Information in Graph Neural Networks. In International Conference on Learning Representations, 2020.   \nDi Jin, Ziyang Liu, Weihao Li, Dongxiao He, and Weixiong Zhang. Graph Convolutional Networks Meet Markov Random Fields: Semi-Supervised Community Detection in Attribute Networks. Proceedings of the AAAI Conference on Artificial Intelligence, 3(1):152\u2013159, 2019.   \nNicolas Keriven. Not Too Little, Not Too Much: A theoretical Analysis of Graph (Over)Smoothing. In Advances in Neural Information Processing Systems, 2022.   \nThomas N. Kipf and Max Welling. Semi-Supervised Classification with Graph Convolutional Networks. In International Conference on Learning Representations (ICLR), 2017.   \nJames R. Lee, Shayan Oveis Gharan, and Luca Trevisan. Multiway spectral partitioning and higherorder cheeger inequalities. J. ACM, 61(6), dec 2014. ISSN 0004-5411. doi: 10.1145/2665063. URL https://doi.org/10.1145/2665063.   \nQimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning. In Thirty-Second AAAI conference on artificial intelligence, 2018.   \nChen Lu and Subhabrata Sen. Contextual Stochastic Block Model: Sharp Thresholds and Contiguity. ArXiv, 2020. arXiv:2011.09841.   \nZhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The Expressive Power of Neural Networks: A View from the Width. In Advances in Neural Information Processing Systems, volume 30, 2017.   \nSitao Luan, Chenqing Hua, Qincheng Lu, Jiaqi Zhu, Mingde Zhao, Shuyuan Zhang, Xiao-Wen Chang, and Doina Precup. Is Heterophily A Real Nightmare For Graph Neural Networks To Do Node Classification? arXiv preprint arXiv:2109.05641, 2021.   \nYao Ma, Xiaorui Liu, Neil Shah, and Jiliang Tang. Is Homophily a Necessity for Graph Neural Networks? In International Conference on Learning Representations, 2022.   \nSohir Maskey, Ron Levie, Yunseok Lee, and Gitta Kutyniok. Generalization analysis of message passing neural networks on large random graphs. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.   \nNikhil Mehta, Lawrence Carin Duke, and Piyush Rai. Stochastic Blockmodels meet Graph Neural Networks. In Proceedings of the 36th International Conference on Machine Learning, volume 97, pages 4466\u20134474, 2019.   \nFederico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and Michael M. Bronstein. Geometric Deep Learning on Graphs and Manifolds Using Mixture Model CNNs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.   \nKenta Oono and Taiji Suzuki. Graph Neural Networks Exponentially Lose Expressive Power for Node Classification. In International Conference on Learning Representations, 2020.   \nPhilipp Reiser, Maximilian Neubert, Andreas Eberhard, Luca Torresi, Chen Zhou, Chen Shao, Houssam Metni, Clint van Hoesel, Henrik Schopmans, Timo Sommer, and Pascal Friederich. Graph Neural Networks for Materials Science and Chemistry. Communications Materials, 3:93, 2022.   \nEmanuele Rossi, Fabrizio Frasca, Ben Chamberlain, Davide Eynard, Michael M. Bronstein, and Federico Monti. SIGN: Scalable Inception Graph Neural Networks. CoRR, abs/2004.11198, 2020.   \nT. Konstantin Rusch, Michael M. Bronstein, and Siddhartha Mishra. A Survey on Oversmoothing in Graph Neural Networks. arXiv preprint arXiv:2303.10993, 2023.   \nFranco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The Graph Neural Network Model. IEEE Transactions on Neural Networks, 20(1), 2009.   \nPrithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective classification in network data. AI magazine, 29(3):93, 2008.   \nRoman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science, volume 47. Cambridge University Press, 2018.   \nMark Weber, Giacomo Domeniconi, Jie Chen, Daniel Karl I Weidele, Claudio Bellei, Tom Robinson, and Charles E Leiserson. Anti-Money Laundering in Bitcoin: Experimenting with Graph Convolutional Networks for Financial Forensics. arXiv preprint arXiv:1908.02591, 2019.   \nXinyi Wu, Zhengdao Chen, William Wei Wang, and Ali Jadbabaie. A Non-Asymptotic Analysis of Oversmoothing in Graph Neural Networks. In The Eleventh International Conference on Learning Representations, 2023.   \nKeyulu Xu, Mozhi Zhang, Jingling Li, Simon Shaolei Du, Ken-Ichi Kawarabayashi, and Stefanie Jegelka. How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks. In International Conference on Learning Representations, 2021.   \nYujun Yan, Milad Hashemi, Kevin Swersky, Yaoqing Yang, and Danai Koutra. Two Sides of the Same Coin: Heterophily and Oversmoothing in Graph Convolutional Neural Networks, 2021.   \nJ. Yang, J. McAuley, and J. Leskovec. Community Detection in Networks with Node Attributes. In 2013 IEEE 13th International Conference on Data Mining, pages 1151\u20131156, 2013.   \nRex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and Jure Leskovec. Graph Convolutional Neural Networks for Web-Scale Recommender Systems. KDD \u201918: Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 974\u2013983, 2018.   \nSi Zhang, Dawei Zhou, Mehmet Yigit Yildirim, Scott Alcorn, Jingrui He, Hasan Davulcu, and Hanghang Tong. Hidden: Hierarchical Dense Subgraph Detection with Application to Financial Fraud Detection. In Proceedings of the 2017 SIAM International Conference on Data Mining, pages 570\u2013578. SIAM, 2017.   \nLingxiao Zhao and Leman Akoglu. PairNorm: Tackling Oversmoothing in GNNs. In International Conference on Learning Representations, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Linear Algebra and Probability Background ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Before beginning our proofs, we will establish the following theorems from the literature and basic facts that we will use throughout the proofs. ", "page_idx": 14}, {"type": "text", "text": "A.1 Linear Algebra ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For matrix inequalities, we use the following inequalities about matrix spectral norms. ", "page_idx": 14}, {"type": "text", "text": "Theorem A.1. ([Vershynin, 2018] theorem 4.5.3.) Let $A$ be a symmetric matrix with eigenvalues $\\lambda_{1}\\geq\\lambda_{2}\\geq...\\lambda_{n}$ and $B$ be a symmetric matrix $\\mu_{1}\\geq\\mu_{2}\\geq...\\mu_{n}$ . Suppose $\\|A-B\\|\\leq\\delta$ . Then, $\\operatorname*{max}_{i}|\\lambda_{i}-\\mu_{i}|\\leq\\delta$ ", "page_idx": 14}, {"type": "text", "text": "We also use the following basic result to relate the matrix spectral norm to its maximum entry ", "page_idx": 14}, {"type": "text", "text": "Lemma A.2. Let $M$ be an $n\\times n$ symmetric matrix such that each row of $M$ has at most m non-zero entries and each entry of $M$ has an absolute value at most $\\varepsilon$ . Then $\\|M\\|\\leq\\varepsilon m$ ", "page_idx": 14}, {"type": "text", "text": "Proof. We will use the fact that for any scalars $a,b$ , we have $2a b\\leq a^{2}+b^{2}$ since $a^{2}+b^{2}-2a b=$ $(a-\\dot{b})^{2}\\geq0$ . We will also use supp $(M)$ to denote the set of non-zero entries in $M$ . Let $x$ be a unit vector. Then we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|x^{\\top}M x|=|\\displaystyle\\sum_{i}\\displaystyle\\sum_{j=1}^{n}x(i)x(j)M_{i,j}|}\\\\ &{\\qquad\\qquad\\le\\varepsilon\\displaystyle\\sum_{i,j\\in\\mathrm{supp}(M)}x(i)x(j)}\\\\ &{\\qquad\\qquad\\le\\varepsilon\\displaystyle\\sum_{i,j\\in\\mathrm{supp}(M)}\\displaystyle\\frac{1}{2}x(i)^{2}+\\frac{1}{2}x(j)^{2}}\\\\ &{\\qquad\\qquad\\le\\varepsilon m\\displaystyle\\sum_{i}x(i)^{2}}\\\\ &{\\qquad\\qquad=\\varepsilon m}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Where the last inequality follows from the fact for every $i$ , there are at most $m$ entries $j$ such that $M_{i,j}\\neq0$ . ", "page_idx": 14}, {"type": "text", "text": "A.2 Concentration Inequalities ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Throughout our analysis, we will mainly use the following two concentration inequalities. The first is a bound on the the expected deviation of the sum of Bernoulli random variables ", "page_idx": 14}, {"type": "text", "text": "Theorem A.3. Let $X_{1},...X_{n}$ be Bernoulli random variables with mean at most p and $\\textstyle S_{n}=\\sum_{i=1}^{n}X_{i}$ . Then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{Pr}\\left[\\vert S_{n}-\\mathbb{E}[S_{n}]\\vert>t\\right]<\\exp(-\\Omega(\\frac{t^{2}}{n p}))\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The second concentration inequality we will upper-bounds the spectral norm of random matrices whose entries have zero mean and bounded variance. ", "page_idx": 14}, {"type": "text", "text": "Theorem A.4. ([Bandeira and van Handel, 2016] Remark 3.13) Let $R$ be a random matrix whose entries $R_{i,j}$ are independent, zero mean, with variance $\\sigma^{2}$ . Then with probability $1-n^{-\\Omega(1)}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\|R\\right\\|\\leq O(\\sigma{\\sqrt{n}}+\\log n)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In particular, $\\begin{array}{r}{i f\\sigma^{2}=\\Omega(\\frac{\\log^{2}n}{n})}\\end{array}$ then $\\lVert R\\rVert\\leq O(\\sigma{\\sqrt{n}})$ ", "page_idx": 14}, {"type": "text", "text": "We will also use the standard tail bound on the norm of a Gaussian vector ", "page_idx": 14}, {"type": "text", "text": "Lemma A.5. Let $g\\ \\ \\sim\\ \\mathcal{N}(0,\\sigma^{2}I_{n})$ be a random Gaussian vector. Then $\\mathbf{Pr}\\left[\\left|\\left|g\\right|\\right|>t\\right]\\ \\leq$ $\\textstyle2\\exp(-\\frac{t^{2}}{2n\\sigma^{2}})$ ", "page_idx": 14}, {"type": "text", "text": "A.3 Other Facts ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We will also be using the following basic fact about approximating the exponential function Lemma A.6. For all $x$ , $1+x\\leq e^{x}$ . If $x\\leq1$ , then $e^{x}\\leq1+2x$ . ", "page_idx": 15}, {"type": "text", "text": "B Proofs in Section 5 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Proof of Lemma 5.1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In the following section, we give the missing proofs of the basic properties of the CSBM. The first is the reduction from the $m-$ dimensional model to the centered $1-$ dimensional model, which we will restate here: ", "page_idx": 15}, {"type": "text", "text": "Lemma B.1. Given an $m$ -dimensional 2-block CSBM, there exist $w\\in\\mathbb{R}^{m}$ and $b\\in\\mathbb{R}^{n}$ such that Xw + b = s + g where for each vertex i, gi is i.i.d. N(0, \u03c3\u20322) with \u03c3\u2032 =\u221an\u22254\u00b5\u03c3\u2212\u03bd\u2225. ", "page_idx": 15}, {"type": "text", "text": "Proof. Let $X_{i,}$ : be the $i^{t h}$ row of $X$ . For each $i\\in S$ , we have $X_{i,:}=\\mu+g_{i}$ and for each $j\\in T$ we have $X_{j,:}=\\nu+g_{j}$ , where $g_{i}\\sim\\mathcal{N}(0,\\sigma^{2}I_{m})$ are i.i.d. random Gaussian noise vectors. Now we pick $w=(\\mu-\\nu)$ , $\\begin{array}{r}{b=-\\frac{1}{2}\\langle\\mu+\\nu,\\mu-\\nu\\rangle\\vec{\\mathbb{1}}}\\end{array}$ . Now $\\boldsymbol{x}^{\\prime}=\\boldsymbol{X}\\boldsymbol{w}+\\boldsymbol{b}$ . For $i\\in S$ , we let ", "page_idx": 15}, {"type": "equation", "text": "$$\ns^{\\prime}(i):=\\langle\\mu-\\frac{1}{2}(\\mu+\\nu),\\mu-\\nu\\rangle=\\frac{1}{2}\\left\\Vert\\mu-\\nu\\right\\Vert^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and for $i\\in T$ , we let ", "page_idx": 15}, {"type": "equation", "text": "$$\ns^{\\prime}(i):=\\langle\\nu-\\frac12(\\mu+\\nu),\\mu-\\nu\\rangle=-\\frac12\\left\\lVert\\mu-\\nu\\right\\rVert^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The $s^{\\prime}$ part is our signal. On the other hand, the noise at each entry is given by $x^{\\prime}(i)\\mathrm{~-~}s^{\\prime}(i)\\mathrm{~=~}$ $\\left\\langle g_{i},\\mu-\\nu\\right\\rangle$ which is Gaussian with standard deviation $\\sigma\\left\\|\\mu-\\nu\\right\\|$ . Now, if we let $\\begin{array}{r}{x=\\frac{4}{\\sqrt{n}\\|\\mu-\\nu\\|^{2}}x^{\\prime}}\\end{array}$ then we have $x=s+g$ , where $\\textstyle s(i)\\in\\pm{\\frac{1}{\\sqrt{n}}}$ and each entry of $g$ is i.i.d. Gaussian with standard deviation \u03c3\u2032 :=\u221an\u22254\u03bd\u03c3\u2212\u00b5\u2225. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Proof of Proposition 5.2 and Characterizing the Convolution Matrix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we will provide some crucial properties of the graph convolution matrices in (1) and then use them to prove Proposition 5.2. The adjacency matrix $A$ is a random matrix with expectation ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}[A]={\\left[\\begin{array}{l l}{p J_{n/2}}&{q J_{n/2}}\\\\ {q J_{n/2}}&{p J_{n/2}}\\end{array}\\right]}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Where $J_{m}$ denotes the $m\\times m$ all-ones matrix. It can be seen that $\\mathbb{E}[A]$ is a rank-2 matrix, with eigenvectors $\\textstyle{\\frac{1}{\\sqrt{n}}}\\mathbb{1}$ and $s$ because $\\mathbb{E}[A]\\mathbb{1}={\\textstyle\\frac{1}{2}}(p+q)n\\mathbb{1}$ and ${\\mathbb E}[A]s={\\textstyle\\frac12}(p-q)n s$ , which means ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\mathbb E}[A]=\\frac{1}{2}(p+q){\\mathbb1}{\\mathbb1}^{\\top}+\\frac{1}{2}(p-q)n s s^{\\top}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, in the centered one-dimensional CSBM, our true signal, $s$ , is encoded in two ways: as the second eigenvector of the matrix $\\mathbb{E}[A]$ and as $\\mathbb{E}[x]$ . We can express our unnormalized corrected convolution matrix in terms of the signal, $s s^{\\top}$ , as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\tilde{A}=\\frac{1}{d}A-\\frac{1}{n}\\mathbb{1}\\mathbb{1}^{\\top}=\\frac{(p-q)n}{2d}s s^{\\top}+\\frac{\\frac{1}{2}(p+q)n-d}{n d}\\mathbb{1}\\mathbb{1}^{\\top}+\\frac{1}{d}(A-\\mathbb{E}[A])\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Recall from main text that we defined $\\begin{array}{r}{\\eta:=\\frac{(p-q)n}{2d}}\\end{array}$ be the signal strength, $\\begin{array}{r}{d^{\\prime}:=\\frac{(p+q)n/2-d}{n d}}\\end{array}$ to be the degree deviation, and $R=A-\\mathbb{E}[A]$ to be the error matrix. Thus, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\tilde{A}=\\eta s s^{\\top}+\\frac{1}{d}R+d^{\\prime}\\mathbb{1}\\mathbb{1}^{\\top}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that $R$ has independent zero mean entries with bounded variance, and such random matrices have well-studied concentration properties. The main idea of our analysis is to show that $d^{\\prime}$ and ", "page_idx": 15}, {"type": "text", "text": "$R$ are small compared to our matrix signal strength $\\eta$ with high probability so that $\\tilde{A}$ behaves like $\\eta s s^{\\top}\\approx\\gamma s s^{\\top}$ . To analyze convolution with the normalized $\\hat{A}$ , we use degree-concentration to show that $\\hat{A}$ is close to $\\tilde{A}$ , and so also behaves like $\\gamma s s^{\\top}$ . ", "page_idx": 16}, {"type": "text", "text": "Now we prove the main proposition in our section, Proposition 5.2, which we restate as follows: ", "page_idx": 16}, {"type": "text", "text": "Proposition B.2. Assume that $\\textstyle p=\\Omega({\\frac{\\log n}{n}})$ , and let $\\begin{array}{r}{\\gamma=\\frac{p-q}{p+q}}\\end{array}$ . With probability $1-n^{-\\Omega(1)}$ , we have the following concentration results ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\tilde{A}-\\gamma s s^{\\top}\\right\\|\\leq O(\\frac{1}{\\sqrt{n p}})\\,a n d\\left\\|\\hat{A}-\\gamma s s^{\\top}\\right\\|\\leq O(\\sqrt{\\frac{\\log n}{n p}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. To bound the deviation of the average degree is equivalent to bounding the total number of edges in the graph. Note that the expected number of edges in the graph is $\\textstyle{\\frac{1}{4}}{\\overline{{(p+q)n^{2}}}}$ . Thus, we apply Theorem A.3 with $t=\\Theta((p+q)n^{1.5})$ to obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{Pr}\\left[||E|-{\\frac{1}{4}}(p+q)n^{2}|>t\\right]\\leq\\exp(-\\Omega(p n))=n^{-\\Omega(1)}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since $d\\,=\\,2|E|/n$ , we have that with high probability, $|d-{\\textstyle\\frac{1}{2}}(p+q)n|\\,\\leq\\,O((p+q)\\sqrt{n})$ and $\\begin{array}{r}{|d^{\\prime}|\\,=\\,\\frac{|d-\\frac12(p+q)n|}{n d}\\,\\le\\,O(\\frac{(p+q)\\sqrt{n}}{(p+q)n^{2}(1-o(1)))}\\,\\le\\,O(1/n^{1.5})}\\end{array}$ . Thi ives us bound number 1. get bound number 2, we apply Theorem A.4 on the error matrix $R$ $R$ variance at most $p$ . To get bound number 3, let $\\begin{array}{r}{R^{\\prime}=\\frac{1}{d}R+d^{\\prime}\\mathbb{1}\\mathbb{1}^{\\top}=\\bar{\\tilde{A}}-\\eta s s^{\\top}}\\end{array}$ . Then with high probability, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|{R^{\\prime}}\\right\\|\\leq\\displaystyle\\frac{1}{d}\\left\\|{R}\\right\\|+d^{\\prime}\\left\\|{J}\\right\\|}\\\\ &{\\qquad\\leq\\displaystyle\\frac{1}{\\frac{1}{2}(p+q)n(1-o(1))}\\left\\|{R}\\right\\|+d^{\\prime}\\left\\|{\\mathfrak{I}}\\right\\|^{\\top}\\right\\|}\\\\ &{\\qquad\\leq O(\\displaystyle\\frac{1}{\\sqrt{p n}}+\\displaystyle\\frac{1}{\\sqrt{n}})\\quad\\mathrm{since}\\left\\|{\\mathfrak{I}}\\right\\|^{\\top}\\right\\|=n}\\\\ &{\\qquad=O(\\displaystyle\\frac{1}{\\sqrt{p n}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Finally, we analyze the corrected normalized adjacency matrix $\\hat{A}$ . In this case, we want to show degree concentration for every node. Let $d_{v}$ be the degree of node $v$ , and ${\\bar{d}}={\\textstyle{\\frac{1}{2}}}(p+q)n$ be the expected degree. By applying Theorem A.3 and using our assumption $p>q$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{Pr}\\left[|d_{v}-{\\bar{d}}|>\\sqrt{C n p\\log n}\\right]\\le\\exp(-\\Omega(\\log n))\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, with high probability, we have $|d_{v}-d|\\leq O(\\sqrt{n p\\log n})$ for all $v\\in V$ . Now, to analyze the normalized adjacency matrix, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\hat{A}-\\gamma s s^{\\top}\\right\\|=\\left\\|\\hat{A}-\\gamma s s^{\\top}+\\tilde{A}-\\tilde{A}\\right\\|}\\\\ &{\\qquad\\qquad\\leq\\left\\|\\hat{A}-\\tilde{A}\\right\\|+\\left\\|\\tilde{A}-\\gamma s s^{\\top}\\right\\|}\\\\ &{\\qquad\\qquad\\leq\\left\\|D^{-1/2}A D^{-1/2}-\\frac{1}{d}A\\right\\|+\\left\\|\\frac{1}{\\sum_{v}d_{v}}D^{1/2}\\mathbb{1}\\mathbb{1}^{\\top}D^{1/2}-\\frac{1}{n}\\mathbb{1}\\mathbb{1}^{\\top}\\right\\|+O(\\frac{1}{\\sqrt{n p}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "To bound  D\u22121/2AD\u22121/2 \u2212d1A  , let \u03b5 = $\\varepsilon\\,=\\,{\\sqrt{\\frac{C\\log n}{n p}}}$ for large enough $C$ so that for all $v,\\,d_{v}\\,\\in$ ${\\bar{d}}(1\\pm\\varepsilon)$ . We note that the adjacency matrix has at most $\\bar{d}(1+\\varepsilon)$ non-zero entries for each row and for each $u,v$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n|\\frac{1}{\\sqrt{d_{u}d_{v}}}-\\frac{1}{d}|\\leq\\frac{1}{\\bar{d}(1-\\varepsilon)}-\\frac{1}{\\bar{d}(1+\\varepsilon)}\\leq O(\\frac{\\varepsilon}{d})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, by applying Lemma A.2, we have $\\begin{array}{r}{\\left\\|D^{-1/2}A D^{-1/2}-\\frac{1}{d}A\\right\\|\\le O(\\varepsilon)}\\end{array}$ . Similarly, for all $u,v$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n|\\frac{\\sqrt{d_{u}d_{v}}}{\\sum_{i}d_{i}}-\\frac{1}{n}|\\leq\\frac{\\bar{d}(1+\\varepsilon)}{\\bar{d}n(1-\\varepsilon)}-\\frac{1}{n}\\leq O(\\frac{\\varepsilon}{n})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, Lemma A.2 implies that $\\begin{array}{r}{\\left\\|\\frac{1}{\\sum_{v}d_{v}}D^{1/2}\\mathbb{1}\\mathbb{1}^{\\top}D^{1/2}-\\frac{1}{n}\\mathbb{1}\\mathbb{1}^{\\top}\\right\\|\\leq O(\\varepsilon).}\\end{array}$ . Finally, this implies that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\|{\\hat{A}}-\\gamma s s^{\\top}\\right\\|\\leq O({\\sqrt{\\frac{\\log n}{n p}}})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that the case $p\\,>\\,q$ corresponds to a homophilous graph, and the case $p<q$ corresponds to a heterophilous graph (see Luan et al. [2021], Ma et al. [2022] for more), however, for binary classification, it has been shown [Baranwal et al., 2023a] that once can assume $p>q$ without loss of generality, and make corresponding adjustments in the classifier. Indeed in our case, if $p<q$ then we can take $-\\tilde{A}$ as our convolution matrix so that its maximum modulus eigenvalue is positive. Moreover, since our signal, $s$ , is centered, taking a convolution with $\\tilde{A}$ is equivalent to taking a convolution with $-\\tilde{A}$ . Thus, we can assume without loss of generality that $\\gamma$ is always non-negative, i.e. $p>q$ for the rest of our analysis. ", "page_idx": 17}, {"type": "text", "text": "C Proofs in Section 6 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we will formally prove our first main theorem on partial classification, Theorem 4.1.   \nTo start, we will show the correlation between the top eigenvector of the convolution matrix $M$ and $s$ .   \nNote that this result is well-known in the literature (for example see Vershynin [2018] chapter 4.5).   \nWe will give the proof here for completeness. Lemma C.1. Suppose $M$ satisfies $\\left\\|M-\\gamma s s^{\\top}\\right\\|\\leq\\delta$ where $\\gamma>C\\delta$ for large enough constant $C$ .   \nLet s\u02c6 be the top eigenvector of $M$ . Then $\\begin{array}{r}{\\langle s,\\hat{s}\\rangle^{2}\\ge1-4\\frac{\\delta^{2}}{\\gamma^{2}}}\\end{array}$ , and $\\left\\|s-{\\hat{s}}\\right\\|^{2}=O(\\delta/\\gamma)$ . ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Proof. By assumption, we can express $M$ as $\\gamma s s^{\\top}+R^{\\prime}$ where $\\|R^{\\prime}\\|\\leq\\delta$ . Now let $\\lambda_{k}$ be the $k^{t h}$ largest eigenvalue of $M$ . By Theorem A.1, we have $|\\lambda_{1}-\\gamma|\\leq\\delta$ and $\\forall i>1$ , $|\\lambda_{i}|\\leq\\delta$ . Now consider the matrix $I-s s^{\\top}$ , which is the projection matrix onto the orthogonal complement subspace of $s$ . Since $M=\\gamma s s^{\\top}+R^{\\prime}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\gamma(I-s s^{\\top})=\\gamma I-M+R^{\\prime}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and the matrix $\\gamma I-M$ has the same eigenvectors as $M$ . In particular, $\\hat{s}$ is an eigenvector with eigenvalues $\\gamma-\\lambda_{1}$ . Thus, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma\\sqrt{1-\\langle s,\\hat{s}\\rangle^{2}}=\\gamma\\sqrt{\\hat{s}^{\\top}(I-s s^{\\top})\\hat{s}}}\\\\ &{\\qquad\\qquad\\qquad=\\left\\lVert\\gamma(I-s s^{\\top})\\hat{s}\\right\\rVert}\\\\ &{\\qquad\\qquad=\\left\\lVert(\\gamma I-M)\\hat{s}+R^{\\prime}\\hat{s}\\right\\rVert}\\\\ &{\\qquad\\qquad\\leq\\left\\lVert(\\gamma I-M)\\hat{s}\\right\\rVert+\\left\\lVert R^{\\prime}\\hat{s}\\right\\rVert}\\\\ &{\\qquad\\qquad\\leq|\\gamma-\\lambda_{1}|+\\delta}\\\\ &{\\qquad\\qquad\\leq2\\delta}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, we have $\\langle s,\\hat{s}\\rangle^{2}\\geq1-4\\delta^{2}/\\gamma^{2}$ . Since by our assumption, $\\delta/\\gamma$ is small, $\\hat{s}$ is very close to either $s$ or $-s$ . For our analysis, it doesn\u2019t matter which of these is the case as both $s$ and $-s$ equally separate the points in our two classes. Thus, we can assume WLOG that $\\langle s,{\\hat{s}}\\rangle>0$ . As a consequence, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|s-\\hat{s}\\|^{2}=2-2\\langle s,\\hat{s}\\rangle=2-2\\sqrt{1-2\\delta/\\gamma}=O(\\delta/\\gamma)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the second inequality follows from applying the first order approximation $\\textstyle{\\sqrt{1-x}}=1-{\\frac{x}{2}}\\,-$ $O(x^{2})$ using the fact that $\\delta/\\gamma$ is small. Note that if $\\langle s,{\\hat{s}}\\rangle$ was negative, we can do the same analysis in the following sections with $-s$ instead of $s$ . \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Now we will prove the main result about partial classification in the 1-dimensional model, Proposition 6.1, which we restate as follows: ", "page_idx": 18}, {"type": "text", "text": "Proposition C.2. Given a centered $^{\\,I}$ -dimensional 2 block CSBM with parameters $n,p>q$ , $\\sigma$ and $\\begin{array}{r}{\\gamma(p,q)=\\frac{p-q}{p+q}}\\end{array}$ , suppose our convolution matrix $M$ satisfies $\\|M-\\gamma\\mathring{s}s^{\\top}\\|\\leq\\delta$ and $\\gamma\\geq C\\delta$ for $a$ large enough constant $C$ . Let $x_{k}=M^{k}x_{i}$ , be the result of applying $k$ rounds of graph convolution to our input feature $x$ . Then with probability at least $\\begin{array}{r}{1-\\frac12\\,\\mathrm{exp}(-\\frac{1}{4\\sigma^{2}}),}\\end{array}$ , there exists a scalar $C_{k}$ and an absolute constant $C^{\\prime}$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|C_{k}x_{k}-s\\|^{2}\\leq O\\left(\\frac{\\delta^{2}}{\\gamma^{2}}+\\Big(\\frac{C^{\\prime}\\delta}{\\gamma}\\Big)^{2k}n\\sigma^{2}\\log n\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Let $\\lambda_{1}~\\ge~...\\lambda_{n}$ be the eigenvalues of $M$ and $w_{1}~=:~\\hat{s},w_{2}...w_{n}$ be their corresponding eigenvectors. By Lemma C.1, the eigenvalues satisfy $\\lambda_{1}\\,\\in\\,\\gamma\\pm\\delta$ and $|\\lambda_{i}|\\leq\\delta$ for $i\\,>\\,1$ . The convolution vector after $k$ rounds can be expressed as $x_{k}=M^{k}g+M^{k}s.$ , and we will analyze each term individually. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{{\\cal M}^{k}g=\\lambda_{1}^{k}\\langle g,w_{1}\\rangle\\hat{s}+\\displaystyle\\sum_{i>1}\\lambda_{i}^{k}\\langle g,w_{i}\\rangle w_{i}}}\\\\ {{\\ \\ \\ \\ \\ =\\lambda_{1}^{k}\\langle g,w_{1}\\rangle s+\\lambda_{1}^{k}\\langle g,w_{1}\\rangle(\\hat{s}-s)+\\displaystyle\\sum_{i>1}\\lambda_{i}^{k}\\langle g,w_{i}\\rangle w_{i}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and similarly, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle M^{k}s=\\lambda_{1}^{k}\\langle s,w_{1}\\rangle\\hat{s}+\\sum_{i>1}\\lambda_{i}^{k}\\langle s,w_{i}\\rangle w_{i}}}\\\\ {{\\displaystyle\\ \\ \\ =\\lambda_{1}^{k}\\langle s,w_{1}\\rangle s+\\lambda_{1}^{k}\\langle s,w_{1}\\rangle(\\hat{s}-s)+\\sum_{i>1}\\lambda_{i}^{k}\\langle s,w_{i}\\rangle w_{i}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now we will let $E_{g}$ and $E_{s}$ be the error vectors, i.e. vectors that are not in the span of $s$ , from each of the terms respectively. That is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{E_{g}=\\lambda_{1}^{k}\\langle g,w_{1}\\rangle(\\hat{s}-s)+\\sum_{i>1}\\lambda_{i}^{k}\\langle g,w_{i}\\rangle w_{i}}}}\\\\ {{\\displaystyle{E_{s}=\\lambda_{1}^{k}\\langle s,w_{1}\\rangle(\\hat{s}-s)+\\sum_{i\\ge1}\\lambda_{i}^{k}\\langle s,w_{i}\\rangle w_{i}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\nx_{k}=\\lambda_{1}^{k}\\langle s+g,\\hat{s}\\rangle s+E_{g}+E_{s}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, we will let $C_{k}=(\\lambda_{1}^{k}\\langle s+g,\\hat{s}\\rangle)^{-1}$ and bound the squared norm of the error terms $C_{k}(E_{g}+E_{s})$ . We will also use the fact that for any vectors $y,z$ , we have $\\left\\|y+z\\right\\|^{2}\\leq2\\left\\|y\\right\\|^{2}+2\\left\\|z\\right\\|^{2}$ . Using this, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\|E_{g}\\|^{2}\\leq2\\lambda_{1}^{2k}|\\langle\\hat{s},g\\rangle|^{2}\\,\\|s-\\hat{s}\\|^{2}+2\\left\\|\\sum_{i>1}\\lambda_{i}^{k}\\langle g,w_{i}\\rangle w_{i}\\right\\|^{2}}\\\\ &{\\qquad\\leq{\\cal O}(\\lambda_{1}^{2k}\\delta^{2}/\\gamma^{2})|\\langle\\hat{s},g\\rangle|^{2}+2\\delta^{2k}\\left\\|\\displaystyle\\sum_{i>1}\\langle g,w_{i}\\rangle w_{i}\\right\\|^{2}}\\\\ &{\\quad\\leq{\\cal O}(\\lambda_{1}^{2k}\\delta^{2}/\\gamma^{2})|\\langle\\hat{s},g\\rangle|^{2}+2\\delta^{2k}\\left\\|g\\right\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where in the second inequality, we used the fact that Lemma C.1 implies $\\left\\|s-\\hat{s}\\right\\|^{2}\\leq O(\\delta^{2}/\\gamma^{2})$ . By Lemma A.5, we have $\\left\\|g\\right\\|^{2}=O(\\sigma^{2}n\\log n)$ with probability $1-n^{-\\Omega(1)}$ . Also, $\\langle\\hat{s},g\\rangle$ has distribution $\\mathcal{N}(0,1)$ because $\\hat{s}$ is a unit vector. Thus with probability $\\exp(-\\frac{1}{4\\sigma^{2}})$ , $|\\langle\\hat{s},g\\rangle|\\leq1/2$ . If both these events happen, then we have $\\left\\|E_{g}\\right\\|^{2}\\leq O(\\lambda_{1}^{2k}\\delta^{2}/\\gamma^{2}+\\delta^{2k}\\sigma^{2}n\\log n)$ . Now, to bound the other error term, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\left\\|E_{s}\\right\\|^{2}\\leq2\\lambda_{1}^{2k}\\left\\|s-\\hat{s}\\right\\|^{2}\\langle\\hat{s},s\\rangle^{2}+2\\left\\|\\sum_{i>1}\\lambda_{2}^{k}\\langle w_{i},s\\rangle w_{i}\\right\\|^{2}}}\\\\ &{}&{\\leq2\\lambda_{1}^{2k}\\left\\|s-\\hat{s}\\right\\|^{2}\\langle\\hat{s},s\\rangle^{2}+2\\delta^{2k}\\left\\|s\\right\\|^{2}}\\\\ &{}&{\\leq O(\\lambda_{1}^{2k}\\delta^{2}/\\gamma^{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the last inequality follows from the fact that $\\|s\\|=1$ and $|\\lambda_{1}\\!-\\!\\gamma|\\leq\\delta$ , which is a constant factor smaller than $\\lambda_{1}$ , which means $\\delta^{2k}=O(\\lambda_{1}^{2k}\\delta^{2}/\\gamma^{\\stackrel{..}{2}})$ . Finally, we want to lower bound $\\langle\\hat{s},s\\rangle+\\langle s,g\\rangle$ by a constant so $C_{k}\\,=\\,O(\\lambda_{1}^{k})$ . By Lemma C.1, we have $\\langle\\hat{s},s\\rangle>1-O(\\delta/\\gamma)$ . We can assume that our constant $C$ is large enough so that $\\gamma/\\delta\\,<\\,1/4$ , which means that $\\langle{\\hat{s}},s\\rangle\\,\\geq\\,3/4\\,$ . Then $\\langle\\hat{s},s\\rangle_{.}+\\langle s,g\\rangle\\geq1/4$ as long as $\\langle s,g\\rangle\\geq0$ or $|\\langle s,g\\rangle|\\leq1/2$ . This happens with probability at least $\\begin{array}{r}{\\frac{1}{2}+\\frac{1}{2}(1-\\exp(-\\frac{1}{4\\sigma^{2}}))=\\bar{1}-\\frac{1}{2}\\exp(-\\frac{1}{4\\sigma^{2}})}\\end{array}$ , and if this does occur, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\left\\|C_{k}x_{k}-s\\right\\|^{2}\\leq2C_{k}^{2}\\left\\|E_{g}\\right\\|^{2}+2C_{k}^{2}\\left\\|E_{s}\\right\\|^{2}}}\\\\ {{\\leq O(\\lambda_{1}^{2k}(\\lambda_{1}^{-2k}\\delta^{2}/\\gamma^{2}+\\delta^{2k}n\\sigma^{2}\\log n))}}\\\\ {{O(\\delta^{2}/\\gamma^{2}+(\\delta/\\lambda_{1})^{2k}n\\sigma^{2}\\log n)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Moreover, since $\\lambda_{1}\\geq\\gamma-\\delta$ , we have $\\delta/\\lambda_{1}\\,\\le\\,\\delta/(\\gamma-\\delta)=O(\\delta/\\gamma)$ since $\\gamma$ is a least a constant factor larger than $\\delta$ , and this gives us our final bound. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "With our main proposition established, we can prove our main theorem as follows: ", "page_idx": 19}, {"type": "text", "text": "Proof. (Theorem 4.1) First, we apply Lemma 5.1 to reduce to the one dimensional centered CSBM with parameters $p,q$ and $\\begin{array}{r}{\\sigma^{\\prime}\\,=\\,\\,O\\bigl(\\frac{\\sigma}{\\sqrt{n}\\|\\mu-\\nu\\|}\\bigr)}\\end{array}$ . Now let $\\ x\\;=\\;X w+b$ be our transformed onedimensional feature vector and let $x_{k}=M^{k}x$ where $M$ is either $\\tilde{A}$ or $\\hat{A}$ . To bound the mean-squared error of our convolved data, we will apply Proposition 6.1. By Proposition 5.2, we can take $\\begin{array}{r}{\\delta=O(\\frac{1}{\\sqrt{n p}})}\\end{array}$ if $M=\\tilde{A}$ and $\\delta=O(\\sqrt{\\frac{\\log n}{n p}})$ if $M={\\hat{A}}$ . Then, as long as $\\gamma\\geq C\\delta$ for large enough constant $C$ , we have a scalar $C_{k}$ and constant $C^{\\prime}$ such that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\|C_{k}x_{k}-s\\right\\|^{2}\\leq O\\left(\\frac{\\delta^{2}}{\\gamma^{2}}+\\left(\\frac{C^{\\prime}\\delta}{\\gamma}\\right)^{2k}\\frac{\\sigma^{2}\\log n}{\\left\\|\\mu-\\nu\\right\\|^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now, we take 0 to be the threshold, meaning we put vertex $i$ in the first class if $x_{k}(i)<0$ and put it in the second class otherwise. Note that this partitioning scheme is indifferent to the scaling of the vector $x_{k}$ , so we can equivalently appl\u221ay it to $C_{k}x_{k}$ (note this does not nece\u221assitate computing $C_{k}$ directly). Since each $i\\in S$ has $s(i)=\\bar{1}\\bar{/}\\sqrt{n}$ and each $j\\in T$ has $s(j)=-1/\\sqrt{n}$ , a vertex can only be misclassified if it contributes at least $1/n$ to the total squared distance $\\left\\|C_{k}x_{k}-s\\right\\|^{2}$ . Thus, the total number of misclassified vertices is at most $\\left\\|C_{k}x_{k}-s\\right\\|^{2}n$ , which gives us the main theorem after substituting the appropriate value of $\\delta$ . \u53e3 ", "page_idx": 19}, {"type": "text", "text": "D Proofs in Section 7 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we will formally prove Proposition 7.1. Before we begin, we will, as a warmup, analyze the behavior of 1 convolution for the centered 1-dimensional CSBM. The following proposition is essentially equivalent to theorem 1.2 in Baranwal et al. [2021]. The proof here is not used later on, but gives some intuition about how to analyze exact classification using a matrix-focused framework. ", "page_idx": 19}, {"type": "text", "text": "Proposition D.1. Suppose we are given the $^{\\,l}$ -dimensional centered CSBM with parameters $n,p,q,\\sigma$ such that $p\\geq\\Omega(\\frac{\\log^{2}n}{n})$ , $\\begin{array}{r}{\\gamma\\geq\\Omega\\big(\\sqrt{\\frac{\\log n}{n p}}\\big)}\\end{array}$ , and $\\sigma\\leq O(\\frac{1}{\\sqrt{\\log n}})$ . Then with probability $1-n^{-\\Omega(1)}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\|s-\\frac{1}{\\eta}x_{1}\\right\\|_{\\infty}\\leq O(\\sigma\\Big(\\frac{1}{\\gamma}\\sqrt{\\frac{\\log n}{n p}}+\\sqrt{\\frac{\\log n}{n}}\\Big))+\\frac{1}{2\\sqrt{n}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. By the definition of our convolution matrix, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{\\eta}x_{1}=\\frac{1}{\\eta}(\\eta s s^{\\top}+R^{\\prime})(s+g)=s+\\frac{1}{\\eta}R^{\\prime}s+(s s^{\\top}+\\frac{1}{\\eta}R^{\\prime})g\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, we can bound the infinity norm error as: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\|s-{\\frac{1}{\\eta}}x_{1}\\right\\|_{\\infty}=\\left\\|{\\frac{1}{\\eta}}R^{\\prime}s+(s s^{\\top}+{\\frac{1}{\\eta}}R^{\\prime})g\\right\\|_{\\infty}\\leq\\left\\|(s s^{\\top}+{\\frac{1}{\\eta}}R^{\\prime})g\\right\\|_{\\infty}+\\left\\|{\\frac{1}{\\eta}}R^{\\prime}s\\right\\|_{\\infty}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since the infinity norm is the maximum absolute value of all entries, we need to bound terms $|e_{u}^{\\top}R^{\\prime}s|/\\eta$ and $\\bar{|e_{u}^{\\top}(s s^{\\top}}+R^{\\prime}/\\eta)g|$ for all $u\\,\\in\\,V$ . We will start by bounding the Gaussian part of the error. Since $e_{u}^{\\top}(s s^{\\top}+R^{\\prime}/\\eta)g\\sim\\mathcal{N}(0,\\sigma^{2}\\left\\|R^{\\prime}e_{u}/\\eta+\\langle s,e_{u}\\rangle s\\right\\|^{2})$ , we have that with high probability $\\begin{array}{r}{\\operatorname*{max}_{i\\in n}|e_{u}^{\\top}(s s^{\\top}+R^{\\prime}/\\eta)g|\\ \\leq\\ O(\\sigma\\sqrt{\\log n}\\cdot\\|R^{\\prime}e_{u}/\\eta+\\langle s,e_{u}\\rangle s|}\\end{array}$ ). Now note that $|\\langle s,e_{u}\\rangle|=1/\\sqrt{n}$ and by Theorem A.4, $\\begin{array}{r}{\\|R^{\\prime}e_{u}\\|\\le\\|R^{\\prime}\\|\\le O(\\sigma\\sqrt{\\frac{1}{n p}})}\\end{array}$ . Finally, by Proposition 5.2, we have $\\eta=\\gamma(1\\pm o(1))$ . Thus, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n|e_{u}^{\\top}(s s^{\\top}+R^{\\prime}/\\eta)g|\\leq O(\\sigma\\Big(\\frac{1}{\\gamma}\\sqrt{\\frac{\\log n}{n p}}+\\sqrt{\\frac{\\log n}{n}}\\Big))\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now, to bound the error term $|e_{u}^{\\top}R^{\\prime}s|$ , we have $e_{u}^{\\top}R^{\\prime}s\\ =\\ e_{u}^{\\top}({\\textstyle{\\frac{1}{d}}}R\\,+\\,d^{\\prime}\\mathbb{1}\\mathbb{1}^{\\top})s\\ =\\ {\\textstyle{\\frac{1}{d}}}e_{u}^{\\top}R s$ since $s$ is orthogonal to the all-ones vector. Now, WLOG, suppose $u\\ \\in\\ S$ . Notice that \u221a1n( i\u2208S Ru,i \u2212 i\u2208T Ru,i), where both sums are sums over independent shifted Bernoulli random v\u221aariables with vari\u221aance at most $p$ . Thus, by applying Theorem A.3, we have $\\begin{array}{r}{|e_{u}^{\\top}R^{\\prime}s|\\leq\\frac{1}{\\sqrt{n}}\\cdot O(\\sqrt{n p\\log n})=O(\\sqrt{p\\log n})}\\end{array}$ . Now, using the fact that $\\begin{array}{r}{\\check{d}\\geq\\frac{1}{2}(p+q)n(1-o(1))}\\end{array}$ , e.  hTahveen $\\begin{array}{r}{|\\frac1d e_{u}^{\\top}R s|\\le O(\\frac1{\\sqrt{n}}\\cdot\\sqrt{\\frac{\\log n}{n p}})}\\end{array}$ n. oBuyg ho usro  atshsautmption, $\\gamma\\geq C\\sqrt{\\frac{\\log n}{n p}})$ longp n ) for large enough constant $C$ $C$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\bigl|\\frac{1}{\\eta}e_{u}^{\\top}R^{\\prime}s\\bigr|=O\\Bigl(\\frac{1}{\\gamma(1-o(1))}\\sqrt{\\frac{\\log n}{n p}}\\Bigr)\\leq\\frac{1}{2\\sqrt{n}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "D.1 Proof of Main Result ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we prove Proposition 7.1, which we restate as follows: ", "page_idx": 20}, {"type": "text", "text": "Proposition D.2. Suppose we are given a $^{\\,l}$ -dimensional centered 2-block CSBM with parameters $n,p,q,\\sigma$ and $k=O(\\log n)$ such that $\\begin{array}{r}{\\gamma\\,=\\,\\frac{p-q}{p+q}\\,\\geq\\,\\Omega(k\\sqrt{\\frac{\\log n}{n p}})}\\end{array}$ longp n ), p \u2265 $p\\geq\\,{\\frac{\\log^{3}n}{n}}$ , and $\\begin{array}{r}{\\sigma\\leq O(\\frac{1}{\\sqrt{\\log n}})}\\end{array}$ . Then with probability at least $1-n^{-\\Omega(1)}$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\|{\\frac{1}{\\eta^{k}}}{\\tilde{A}}^{k}x-s\\right\\|_{\\infty}\\leq{\\frac{1}{2{\\sqrt{n}}}}+O(({\\frac{C}{\\gamma{\\sqrt{n p}}}})^{k}\\sigma{\\sqrt{\\log n}})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Before proving the main proposition, we will use degree-concentration to reduce analyzing the error matrix $\\mathbf{\\dot{\\boldsymbol{R}}^{\\prime}}=\\mathbf{\\check{\\boldsymbol{\\tilde{A}}}}-\\eta s s^{\\intercal}$ to analysing $\\scriptstyle{\\frac{1}{d}}R$ instead. This is useful because our analysis crucially uses the fact that our error matrix $R$ has zero-mean Radamacher entries. ", "page_idx": 20}, {"type": "text", "text": "Proposition D.3. Let $R^{\\prime}=\\tilde{A}-\\eta s s^{\\top}$ and suppose $k\\leq O(\\log n)$ . Then with high probability, we have that $\\begin{array}{r}{\\|R^{\\prime k}-\\frac{1}{d^{k}}R^{k}\\|\\leq O(\\frac{C^{k}k\\sqrt{\\log n}}{(\\sqrt{p n})^{k}\\cdot\\sqrt{n}})}\\end{array}$ for a constant $C$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. Before beginning the proof, w\u221ae first need to give a tighter degree-concentration b\u221aound to show that with high probability, |d\u2032| \u2264O(nl2o\u221ag pn ) . By applying Theorem A.3 with $t=\\Theta(n{\\sqrt{p\\log n}})$ . ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{Pr}\\left[||E|-{\\frac{1}{4}}(p+q)n^{2}|>t\\right]\\leq\\exp(-\\Omega{\\Bigl(}{\\frac{n^{2}p\\log n}{(p+q)n^{2}}}{\\Bigr)})=n^{-\\Omega(1)}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $d=2|E|/n$ and with high probability at, $|E|\\ge\\frac{1}{4}(p+q)n^{2}-t\\ge\\frac{1}{4}(p+q)n^{2}(1-o(1))$ , we have $d\\geq\\textstyle{\\frac{1}{2}}(p+q)n(1-o(1))=\\Omega(n p)$ . Putting these together, we get the bound: ", "page_idx": 20}, {"type": "equation", "text": "$$\n|d^{\\prime}|=\\frac{|d-\\frac{1}{2}(p+q)n|}{n d}=2\\frac{||E|-\\frac{1}{4}(p+q)n^{2}|}{n^{2}d}\\leq O(\\frac{n\\sqrt{p\\log n}}{n^{3}p})\\leq O(\\frac{\\sqrt{\\log n}}{n^{2}\\sqrt{p}})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now, recall from Equation (2) that $\\begin{array}{r}{R^{\\prime}=\\frac{1}{d}R+d^{\\prime}\\mathbb{1}\\mathbb{1}^{\\top}}\\end{array}$ . By Proposition 5.2, we have that with high probability, $\\begin{array}{r}{\\frac{1}{d}\\left\\|R\\right\\|\\leq\\sqrt{\\frac{C}{n p}}}\\end{array}$ for some constant $C$ . We will let $\\begin{array}{r}{\\delta:=\\sqrt{\\frac{C}{n p}}}\\end{array}$ be this upper bound. Now consider the matrix $({\\textstyle{\\frac{1}{\\delta}}}R^{\\prime})^{k}$ . We will let $\\begin{array}{r}{M_{0}=\\frac{1}{d\\delta}R}\\end{array}$ and $\\begin{array}{r}{M_{1}=\\frac{d^{\\prime}}{\\delta}\\mathbb{1}\\mathbb{1}^{\\top}}\\end{array}$ . Then, we have $\\|M_{0}\\|\\leq1$ , and $\\begin{array}{r}{\\left\\|M_{1}\\right\\|\\leq\\frac{d^{\\prime}n}{\\delta}}\\end{array}$ . Since $\\begin{array}{r}{|d^{\\prime}|\\leq\\frac{\\sqrt{\\log n}}{n^{2}\\sqrt{p}}}\\end{array}$ with high probability, we will assume this event occurs. Thus, we have $\\begin{array}{r}{\\|M_{1}\\|\\leq O(\\frac{n\\sqrt{n p\\log n}}{n^{2}\\sqrt{p}})\\leq O(\\sqrt{\\frac{\\log n}{n}})}\\end{array}$ . Finally, we have $(\\textstyle{\\frac{1}{\\delta}}R^{\\prime})^{k}=(M_{0}+M_{1})^{k}$ . We are going to expand this out, and so let us introduce some notation. Let $\\binom{[k]}{\\ell}=\\{i:=(i_{1},i_{2},...i_{k})\\in$ $\\{0,1\\}^{k}:i_{1}+i_{2}+...i_{k}=\\ell\\}$ . That is, it\u2019s the set of length $k$ binary sequences with exactly $\\ell$ terms equaled to 1. Now, we have ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{(M_{0}+M_{1})^{k}-M_{0}^{k}=\\displaystyle\\sum_{\\ell=0}^{k}\\sum_{i\\in\\binom{[k]}{\\ell}}\\displaystyle\\prod_{j=1}^{k}M_{i j}-M_{0}^{k}}}\\\\ {{=\\displaystyle\\sum_{\\ell=1}^{k}\\sum_{i\\in\\binom{[k]}{\\ell}}\\displaystyle\\prod_{j=1}^{k}M_{i j}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For any matrices $M,N$ , their spectral norms satisfy $\\lVert M N\\rVert\\leq\\lVert M\\rVert\\lVert N\\rVert$ . Thus, for each $i\\in\\big(\\mathsf{\\Gamma}_{\\ell}^{[k]}\\big)$ , we have $\\begin{array}{r}{\\left\\|\\prod_{j=1}^{k}M_{i_{j}}\\right\\|\\leq\\|M_{1}\\|^{\\ell}}\\end{array}$ since $\\|M_{0}\\|\\leq1$ . Thus, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left\\|(M_{0}+M_{1})^{k}-M_{0}^{k}\\right\\|\\leq\\displaystyle\\sum_{\\ell=1}^{k}\\left(\\!\\!\\begin{array}{l}{k}\\\\ {\\ell}\\end{array}\\!\\!\\right)\\|M_{1}\\|^{\\ell}}\\\\ {\\displaystyle\\qquad\\qquad\\qquad=(1+\\|M_{1}\\|)^{k}-1}\\\\ {\\leq e^{k\\|M_{1}\\|}-1}\\\\ {\\leq2k\\|M_{1}\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the second last inequality follows from Lemma A.6 because $k\\,\\|M_{1}\\|\\leq k{\\sqrt{\\frac{\\log n}{n}}}\\leq1$ logn n\u22641 by our assumptions on $k$ . Finally, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left\\|{R^{\\prime k}-\\frac{1}{d^{k}}R^{k}}\\right\\|=\\delta^{k}\\left\\|{(M_{0}+M_{1})^{k}-M_{0}}\\right\\|}\\\\ {\\displaystyle\\leq2\\delta^{k}k\\left\\|{M_{1}}\\right\\|}\\\\ {\\displaystyle\\leq O(\\frac{k C^{k}\\sqrt{\\log n}}{(\\sqrt{p n})^{k}\\cdot\\sqrt{n}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For a constant $C$ . ", "page_idx": 21}, {"type": "text", "text": "As a corollary, we can show the following: ", "page_idx": 21}, {"type": "text", "text": "Corollary D.4. Let $x$ and $y$ be unit vectors. Then $\\begin{array}{r}{|x^{\\top}R^{\\prime k}y|\\leq|x^{\\top}(\\frac{1}{d^{k}}R^{k})y|+O(\\frac{C^{k}k\\sqrt{\\log n}}{(\\sqrt{p n})^{k}\\cdot\\sqrt{n}}))f o}\\end{array}$ some constant $C$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. This follows from the basic properties of the spectral norm ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{|x^{\\top}R^{\\prime}{}^{k}y|=|x^{\\top}(\\frac{1}{d^{k}}R^{k}+R^{\\prime^{k}}-\\frac{1}{d^{k}}R^{k})y|}}\\\\ {{\\qquad\\qquad\\leq|x^{\\top}(\\frac{1}{d^{k}}R^{k})y|+\\|x\\|\\,\\|y\\|\\left\\|R^{\\prime k}-\\frac{1}{d^{k}}R^{k}\\right\\|}}\\\\ {{\\qquad\\qquad\\leq|x^{\\top}(\\frac{1}{d^{k}}R^{k})y|+O(\\frac{C^{k}k\\sqrt{\\log n}}{(\\sqrt{p n})^{k}\\cdot\\sqrt{n}}))\\quad\\mathrm{by~Proposition~D.3}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Corollary D.4 is key to proving our main technical lemma, the proof of which we will defer to the next section. ", "page_idx": 21}, {"type": "text", "text": "$p\\geq{\\frac{\\log^{3}n}{n}}$ , we have that with probability $1-n^{-\\Omega(1)}$ , for all $u\\in V$ and all $k\\in\\{1,2,...O(\\log n)\\}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n|e_{u}^{\\top}R^{\\prime k}s|\\leq\\frac{1}{\\sqrt{n}}\\Big(C\\sqrt{\\frac{\\log n}{p n}}\\Big)^{k}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for some constant $C$ ", "page_idx": 22}, {"type": "text", "text": "Now, we are ready to prove our main result, Proposition 7.1. The proof is similar to that of Proposition D.3 but requires the use of Lemma D.5 to obtain a sharper bound on the error terms than by simply applying the spectral norm bound as we did in the degree-concentration proof. ", "page_idx": 22}, {"type": "text", "text": "Proof. (Proposition 7.1) Just like in the $k=1$ case, we express $\\tilde{A}$ as $\\eta s s^{\\top}+R^{\\prime}$ and decompose our convolution vector into the following terms: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{\\eta^{k}}\\tilde{A}^{k}x=\\displaystyle\\frac{1}{\\eta^{k}}\\tilde{A}^{k}(s+g)}\\\\ {\\displaystyle=s+(\\frac{1}{\\eta^{k}}\\tilde{A}^{k}-s s^{\\top})s+\\frac{1}{\\eta^{k}}\\tilde{A}^{k}g}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In order to bound $\\begin{array}{r}{\\left\\lVert s-\\frac{1}{\\eta^{k}}\\tilde{A}^{k}x\\right\\rVert_{\\infty}}\\end{array}$ , we will bound, for all $u\\in V$ , the error terms $\\begin{array}{r}{|e_{u}^{\\top}(\\frac{1}{\\eta^{k}}\\tilde{A}^{k}-s s^{\\top})s|}\\end{array}$ and $|e_{u}^{\\top}\\frac{1}{\\eta^{k}}\\tilde{A}^{k}g|$ . Similar to in the proof of Proposition D.3, we start by expanding out $\\begin{array}{r}{\\frac{1}{\\eta^{k}}\\tilde{A}^{k}}\\end{array}$ . Let $Y_{0}=s{{s}^{\\dagger}}$ and $Y_{1}=R^{\\prime}$ . Then we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{\\eta^{k}}\\tilde{A}^{k}=\\frac{1}{\\eta^{k}}(\\eta Y_{0}+Y_{1})^{k}}\\\\ &{\\qquad=\\frac{1}{\\eta^{k}}\\displaystyle\\sum_{\\ell=0}^{k}\\eta^{k-\\ell}\\sum_{i\\in\\binom{k}{\\ell}}\\displaystyle\\prod_{j=1}^{k}Y_{i,j}}\\\\ &{\\qquad=\\displaystyle\\sum_{\\ell=0}^{k}\\eta^{-\\ell}\\sum_{i\\in\\binom{k}{\\ell}}\\displaystyle\\prod_{j=1}^{k}Y_{i,j}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that $(s s^{\\top})^{2}=s s^{\\top}$ , which means that the first term in our summation (i.e. the $\\ell=0$ terms) is simply $s s^{\\top}$ . Now we start by bounding the error terms involving $s$ . ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|e_{u}^{\\top}(\\frac{1}{\\eta^{k}}\\tilde{A}^{k}-s s^{\\top})s|=|\\displaystyle\\sum_{\\ell=1}^{k}\\eta^{-\\ell}\\displaystyle\\sum_{i\\in\\binom{[k]}{\\ell}}e_{u}^{\\top}\\displaystyle\\prod_{j=1}^{k}Y_{i_{j}}s|}\\\\ {\\le\\displaystyle\\sum_{\\ell=1}^{k}\\eta^{-\\ell}\\displaystyle\\sum_{i\\in\\binom{[k]}{\\ell}}|e_{u}^{\\top}\\displaystyle\\prod_{j=1}^{k}Y_{i_{j}}s|}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For each $i\\in({k\\atop\\ell})$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n|e_{u}^{\\top}\\prod_{j=1}^{k}Y_{i_{j}}s|=|e_{u}^{\\top}R^{\\prime a_{1}}s\\cdot s^{\\top}R^{\\prime a_{2}}s\\cdot...s^{\\top}R^{\\prime a_{L}}s|\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $a_{1},...a_{L}$ are non-negative integers satisfying $a_{1}+...a_{L}=\\ell$ . This is because the product $\\textstyle\\prod_{j=1}^{k}Y_{i_{j}}$ has exactly $\\ell$ terms of the form $R^{\\prime}$ and $s s^{\\top}$ for the rest of the terms. By Proposition 5.2, the matrix $R^{\\prime}$ has spectral norm at most $\\frac{C_{1}}{\\sqrt{n p}}$ for some constant $C_{1}$ with high probability. For each tteerrmm , $|s^{\\top}R^{\\prime a_{j}}s|$ Lfeorm $j>1$ ,. 5w ae nbdo guinvde $\\begin{array}{r}{|s^{\\top}R^{\\prime a_{j}}s|\\leq\\|R^{\\prime}\\|^{a_{j}}\\leq\\left(\\frac{C_{1}}{\\sqrt{n p}}\\right)^{a_{j}}}\\end{array}$ . mFeo rc tohnes tfairnstt $|e_{u}^{\\top}R^{\\prime a_{1}}s|$ $\\begin{array}{r}{|e_{u}^{\\top}R^{\\prime a_{1}}s|\\leq\\frac{1}{\\sqrt{n}}\\Big(C_{2}\\sqrt{\\frac{\\log n}{n p}}\\Big)^{a_{1}}}\\end{array}$ $C_{2}$ . Thus, we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n|e_{u}^{\\top}\\prod_{j=1}^{k}Y_{i_{j}}s|\\leq{\\frac{1}{\\sqrt{n}}}{\\Big(}{\\frac{\\operatorname*{max}(C_{1},C_{2})}{\\sqrt{n p}}}{\\Big)}^{a_{1}+\\ldots a_{L}}{\\sqrt{\\log n}}^{a_{1}}\\leq{\\frac{1}{\\sqrt{n}}}{\\Big(}C{\\sqrt{\\frac{\\log n}{n p}}}{\\Big)}^{\\ell}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $C$ is a large enough constant such that $C\\,\\geq\\,\\operatorname*{max}(C_{1},C_{2})$ . Now, we let $\\begin{array}{r}{\\rho:=\\,\\frac{C}{\\eta}\\sqrt{\\frac{\\log n}{n p}}}\\end{array}$ Assuming that $\\begin{array}{r}{\\gamma\\geq9C k\\sqrt{\\frac{\\log n}{n p}}}\\end{array}$ and with high probability, $\\eta\\in\\gamma(1\\pm o(1))$ , we have $\\begin{array}{r}{\\rho\\le\\frac{1}{8k}}\\end{array}$ with high probability. Thus, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|e_{u}^{\\top}(\\frac{1}{\\eta^{k}}{\\tilde{A}^{k}}-s s^{\\top})s|\\leq\\displaystyle\\sum_{\\ell=1}^{k}{\\eta^{-\\ell}}\\sum_{i\\in\\{\\frac{k}{\\ell}\\}}|e_{u}^{\\top}\\prod_{j=1}^{k}Y_{i,j}s|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{1}{\\sqrt{n}}\\sum_{\\ell=1}^{k}\\bigg(\\binom{k}{\\ell}\\eta^{-\\ell}\\Big(C\\sqrt{\\frac{\\log n}{n p}}\\Big)^{\\ell}}\\\\ &{\\qquad=\\displaystyle\\frac{1}{\\sqrt{n}}\\sum_{\\ell=1}^{k}\\Big(\\frac{k}{\\ell}\\Big)\\rho^{\\ell}}\\\\ &{\\qquad=\\displaystyle\\frac{1}{\\sqrt{n}}((1+\\rho)^{k}-1)}\\\\ &{\\qquad\\leq\\frac{1}{4\\sqrt{n}}\\quad\\mathrm{by~Lemma~A\\,}\\delta\\,\\mathrm{and}\\rho\\leq\\frac{1}{8k}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now, we bound the Gaussian part of the error: ", "page_idx": 23}, {"type": "equation", "text": "$$\ne_{u}^{\\top}\\tilde{A}g=\\sum_{\\ell=0}^{k-1}\\eta^{-\\ell}\\sum_{i\\in\\left(\\stackrel{[k]}{\\ell}\\right)}e_{u}^{\\top}\\prod_{j=1}^{k}Y_{i_{j}}g+\\eta^{-k}e_{u}^{\\top}R^{\\prime k}g\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For each $i\\in\\left({k\\atop\\ell}\\right)$ where $\\ell<k$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n|e_{u}^{\\top}\\prod_{j=1}^{k}Y_{i_{j}}g|=|e_{u}^{\\top}R^{\\prime a_{1}}s\\cdot s^{\\top}R^{\\prime a_{2}}s\\cdot...s^{\\top}R^{\\prime a_{L}}g|\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where ${a_{1}}\\!+{{a_{2}}\\!+...{a_{L}}}=\\ell$ . This is because when $\\ell<k$ , there is at least one $Y_{i_{j}}$ term that is equalled to $s s^{\\top}$ , which means in the product, the leftmost term must be $e_{u}^{\\top}R^{\\prime a_{1}}s$ for some $a_{1}$ and the right most term must be $s^{\\top}R^{\\prime a_{L}}g$ for some $a_{L}$ . Once again, we use the fact that $\\begin{array}{r}{|s^{\\top}R^{\\prime a_{j}}s|\\leq\\|R^{\\prime}\\|^{a_{j}}\\leq(\\frac{C_{1}}{\\sqrt{n p}})^{a_{j}}}\\end{array}$ for all $j>1$ , and by Lemma D.5, we have $|e_{u}^{\\top}R^{\\prime a_{1}}s|\\leq(C_{2}\\sqrt{\\frac{\\log n}{n p}})^{a_{1}}$ where $C_{1},C_{2}$ are absolute constants. To bound the last term, we have that $s^{\\top}R^{\\prime a}g\\sim\\mathcal{N}(0,\\sigma^{2}\\left\\|R^{\\prime a}s\\right\\|^{2})$ for all $a>0$ . Thus, with high probability, we have that $\\begin{array}{r}{|s^{\\top}R^{\\prime a_{L}}g|\\,\\leq\\,O(\\sigma\\,\\|R^{\\prime a_{L}}s\\|\\,\\sqrt{\\log n})\\,\\leq\\,O(\\sigma(\\frac{C_{1}}{\\sqrt{n p}})^{a_{L}}\\sqrt{\\log n})}\\end{array}$ . Th\u221aus, we can apply the same bounds as we did for the error term involving $s$ but now with an extra $\\sigma{\\sqrt{\\log n}}$ factor: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\mathcal{l}_{u}^{\\tau}\\prod_{j=1}^{k}Y_{i,g}\\!\\!\\!\\!}&{=\\big|e_{u}^{\\top}R^{\\alpha_{1}}s\\cdot s^{\\top}R^{\\alpha_{2}}s\\cdot\\dots s^{\\top}R^{\\alpha_{L}}g\\big|}\\\\ {\\quad}&{\\displaystyle\\leq O\\big(\\frac{\\sigma\\sqrt{\\log n}}{\\sqrt{n}}\\cdot\\sum_{\\ell=0}^{k-1}\\binom{k}{\\ell}\\rho^{\\ell}\\big)}\\\\ {\\displaystyle}&{\\leq O(\\frac{\\sigma\\sqrt{\\log n}}{\\sqrt{n}}\\cdot(1+\\rho)^{k})}\\\\ {\\displaystyle}&{\\leq O(\\frac{\\sigma\\sqrt{\\log n}}{\\sqrt{n}})\\quad\\mathrm{by\\Lemma~A}.6}\\\\ {\\displaystyle}&{\\leq O(\\frac{1}{4\\sqrt{n}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Where the last inequality follows assuming \u03c3 \u2264C\u2032\u221a1log n for some large enough constant $C^{\\prime}$ . Finally, to bound the $k^{t h}$ order term, we have that with high probability, $\\begin{array}{r}{\\eta^{-k}|e_{u}^{\\top}R^{\\prime k}g|\\leq(\\frac{C_{1}}{\\eta\\sqrt{n p}})^{k}\\sigma\\sqrt{\\log n}}\\end{array}$ . ", "page_idx": 23}, {"type": "text", "text": "Putting everything together, and using the fact that $\\eta\\geq\\gamma(1-o(1))$ with high probability, we have that for all $u\\in V$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n|e_{u}^{\\top}s-\\frac{1}{\\eta^{k}}\\tilde{A}^{k}x|\\leq\\frac{1}{\\eta^{k}}(|e_{u}^{\\top}(\\tilde{A}^{k}-s s^{\\top})s|+|e_{u}^{\\top}\\tilde{A}^{k}g|)\\leq\\frac{1}{2\\sqrt{n}}+\\Big(\\frac{C}{\\gamma\\sqrt{n p}}\\Big)^{k}\\sigma\\sqrt{\\log n}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for some absolute constant $C$ ", "page_idx": 24}, {"type": "text", "text": "Given Proposition 7.1, we can derive Theorem 4.2 by using the standard reduction in Lemma 5.1. ", "page_idx": 24}, {"type": "text", "text": "Proof. (Theorem 4.2) By applying Lemma 5.1, we see that given $m$ -dimensional features with feature matrix $X$ , we can transform it to a centered 1-dimensional feature vector $x=X w+b=s+g$ where $g\\sim\\mathcal{N}(0,\\sigma^{\\prime2}I)$ and $\\begin{array}{r}{\\sigma^{\\prime}=\\frac{4\\sigma}{\\sqrt{n}\\|\\nu-\\mu\\|}}\\end{array}$ . Thus, we have $\\begin{array}{r}{\\|\\nu-\\mu\\|=\\frac{4\\sigma}{\\sigma^{\\prime}\\sqrt{n}}}\\end{array}$ . By Proposition 7.1, our 1-dimensional features become linearly separable as long as $\\begin{array}{r}{\\Big(\\frac{C}{\\gamma\\sqrt{n p}}\\Big)^{k}\\sigma^{\\prime}\\sqrt{\\log n}<\\frac{1}{2\\sqrt{n}}}\\end{array}$ \u221a1  for some absolute constant $C$ . Given our expression of $\\sigma^{\\prime}$ in terms of the mean distance, this is equivalent to ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|\\nu-\\mu\\|\\geq\\sigma{\\Big(}{\\frac{C}{\\gamma{\\sqrt{n p}}}}{\\Big)}^{k}{\\sqrt{\\log n}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "In Proposition 7.1, we also needed to assumed that $\\begin{array}{r}{\\sigma^{\\prime}\\leq O(\\frac{1}{\\sqrt{\\log n}})}\\end{array}$ , which implies that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|\\nu-\\mu\\|\\geq\\Omega(\\sigma\\sqrt{\\frac{\\log n}{n}})\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "D.2 Message Passing Error Bound ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we will prove our main technical lemma, Lemma D.5. By Corollary D.4, it suffices to control the term $|e_{u}^{\\top}R^{\\underline{{k}}}s|$ in order to control $|e_{u}^{\\top}R^{\\prime k}s|$ . Since this is the sum over many dependent random variables, we cannot easily compute its moment generating function. Instead, we will compute the moments directly. In particular, we will apply Markov\u2019s inequality using the $2t^{t h}$ moment of this random variable $\\mathbb{E}[(e_{u}^{\\dagger}R^{k}s)^{2t}]$ for an appropriately chosen $t$ . We now state our main result for this section as follows: ", "page_idx": 24}, {"type": "text", "text": "Proposition D.6. Suppose $R$ is an $n\\times n$ symmetric random matrix where for each $1\\,\\leq\\,i\\,\\leq\\,j$ , $R_{i,j}=1-p_{i,j}$ with probability $p_{i,j}$ and $-p_{i,j}$ with probability $1-p_{i,j}$ and are independent. Let $p=\\operatorname*{max}_{i,j}p_{i,j}$ , and suppose $p\\geq\\Omega(\\frac{\\log^{3}n}{n})$ . Then we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbf{Pr}\\left[\\left|e_{u}^{\\top}R^{k}s\\right|>\\frac{1}{\\sqrt{n}}(C\\sqrt{n p\\log n})^{k}\\right]\\leq\\exp(-\\Omega(C\\log n))\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. The term $e_{u}^{\\top}R^{k}s$ can be written as the sum over all walks of length $k$ originating from $u$ . Let $\\mathcal{W}_{u,k}$ denote the set of all walks of length $k$ originating at $u$ . For brevity, we will use $\\mathcal{W}$ to denote $\\mathcal{W}_{u,k}$ . For each $w\\in\\mathcal{W}$ , let $w(j)$ be the $j^{t h}$ vertex in the walk. Note that $w(0)=u$ always. Then we have ", "page_idx": 24}, {"type": "equation", "text": "$$\ne_{u}^{\\top}R^{k}s=\\sum_{w\\in\\mathcal{W}}\\prod_{j=1}^{k}R_{w(j-1),w(j)}s(w(k))\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[(e_{u}^{\\top}R^{k}s)^{2t}]=\\displaystyle\\sum_{w_{1},w_{2}\\dots w_{2t}\\in\\mathcal{W}}\\mathbb{E}[\\displaystyle\\prod_{i=1}^{2t}\\prod_{j=1}^{k}R_{w_{i}(j-1),w_{i}(j)}s(w_{i}(k))]}\\\\ &{\\qquad\\qquad\\le\\displaystyle\\sum_{w_{1},w_{2}\\dots w_{2t}\\in\\mathcal{W}}\\|\\mathbb{E}[\\displaystyle\\prod_{i=1}^{2t}\\prod_{j=1}^{k}R_{w_{i}(j-1),w_{i}(j)}]\\|\\cdot\\prod_{i=1}^{2t}|s(w_{i}(k))|}\\\\ &{\\qquad=\\displaystyle\\frac{1}{n^{t}}\\sum_{w_{1},w_{2}\\dots w_{2t}\\in\\mathcal{W}}|\\mathbb{E}[\\displaystyle\\prod_{i=1}^{2t}\\prod_{j=1}^{k}R_{w_{i}(j-1),w_{i}(j)}]|}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Where the last equality follows from the fact that $|s(v)|\\,=\\,1/{\\sqrt{\\,n\\,}}$ for all $v\\,\\in\\,V$ . Now, for each $\\vec{w}=(w_{1},...w_{2t})\\stackrel{.}{\\in}\\mathcal{W}^{\\bar{2}t}$ and edge $h\\in n\\times n$ , let $\\#_{\\vec{w}}(h)$ be the number of times the edge $h$ occurs in the graph $w_{1}\\cup w_{2}\\cup,...w_{2t}$ . In other words, for $h=\\{a,b\\}$ , # $\\mathit{\\Pi}_{\\vec{w}}(h)$ is the number of times the variable $R_{a,b}$ occurs in the product $\\begin{array}{r}{\\prod_{i=1}^{2t}\\prod_{j=1}^{k}R_{w_{i}(j-1),w_{i}(j)}}\\end{array}$ . We will also let $\\lvert\\vec{w}\\rvert$ denote the number of unique edges in the graph formed by the union of these $2t$ walks. For example, if $\\vec{w}$ consist of the walks $(1,2,3)$ , and $(1,2,4)$ , then we have $\\#_{\\vec{w}}(1,2)=2$ , $\\#_{\\vec{w}}(2,3)=1$ , $\\#_{\\vec{w}}(2,4)=1$ , and $|\\vec{w}|=3$ . Using this notation, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{w_{1},w_{2}...w_{2t}\\in\\mathcal{W}}|\\mathbb{E}[\\prod_{i=1}^{2t}\\prod_{j=1}^{k}R_{w_{i}(j-1),w_{i}(j)}]|=\\sum_{\\vec{w}\\in\\mathcal{W}^{2t}}\\prod_{h\\in[n]\\times[n]}|\\mathbb{E}[R_{h}^{\\#\\vec{w}(h)}]|\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now we note that each $R_{h}$ has $\\mathbb{E}[R_{h}]=0$ and for all $k\\geq2$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathbb{E}[R_{h}^{k}]|\\leq\\mathbb{E}[|R_{h}^{k}|]=p_{h}(1-p_{h})^{k}+p_{h}^{k}(1-p_{h})=p_{h}\\big((1-p_{h})^{k}+p_{h}^{k-1}(1-p_{h})\\big)\\leq p_{h}\\leq p_{h},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Thus, the term $\\begin{array}{r}{\\prod_{h\\in[n]\\times[n]}|\\mathbb{E}[R_{h}^{\\#_{\\#}(h)}]|}\\end{array}$ is only nonzero if in the union of the edges in the walks $w_{1},...w_{2t}$ , each edge is counted at least twice, and if it is non-zero, then it is the product of at most $\\lvert\\vec{w}\\rvert$ terms of value at most $p$ . We now let $\\mathcal{W}_{p a i r}^{2t}$ be the set of such walks, which we will denote as \u201cvalid walks\u201d. As an example, when $t=2$ and $k=2$ , the walks $\\{(1,2,3),(1,2,3)\\}$ would be valid but $\\{(1,2,3),(1,2,4)\\}$ would not be valid. Now we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\vec{w}\\in\\mathcal{W}^{2t}}\\prod_{h\\in[n]\\times[n]}|\\mathbb{E}[R_{h}^{\\#_{\\vec{w}}(h)}]|\\le\\sum_{w\\in\\mathcal{W}_{p a i r}^{2t}}p^{|\\vec{w}|}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\sum_{\\ell=1}^{t k}p^{\\ell}|\\{\\vec{w}\\in\\mathcal{W}_{p a i r}^{2t},\\ |\\vec{w}|=\\ell\\}|}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now, what we have left is a counting problem. We need to count the number of valid sets of walks whose union has exactly $\\ell$ distinct edges. We note that $\\ell$ can be at most $t k$ because otherwise, there must be an edge that is counted at most once, making the walks invalid. Since it is difficult to count this quantity exactly, we will just upper-bound it as follows: ", "page_idx": 25}, {"type": "text", "text": "Proposition D.7. Suppose $p\\geq{\\frac{\\log^{3}n}{n}}$ and $t\\leq{\\frac{\\log n}{2k}}$ log n . Then we have: ", "page_idx": 25}, {"type": "equation", "text": "$$\n|\\{\\vec{w}\\in\\mathcal{W}_{p a i r}^{2t},\\ |\\vec{w}|=\\ell\\}|\\leq{\\binom{2t k}{2\\ell}}(2\\ell-1)!!\\cdot\\ell^{2k t-2\\ell}(n)^{\\ell}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The notation of $n!!$ denotes $n\\cdot n-2\\cdot n-4...1$ . Given this upper bound on the number of valid sets of walks, we are ready to give our final bound. We will let C1 2lokg nfor large enough constant $C_{1}$ . Note that the only constraint on $t$ is that it is a positive integer, and this is feasible as long as $k=O(\\log n)$ . Since $p\\geq\\frac{\\log^{3}n}{n}$ logn3 n, we have np \u2265(tk/C1)3. We will make the substitution b = tk \u2212\u2113. Then we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\ell=1}^{t k}p^{\\ell}|\\{\\vec{w}\\in\\mathcal{W}_{p a i r}^{2t},\\ |\\vec{w}|=\\ell\\}|\\leq\\displaystyle\\sum_{b=0}^{t k-1}(n p)^{t k-b}(t k-b)^{2b}(2t k-2b-1)!\\binom{2t k}{2b}}\\\\ &{\\le\\displaystyle\\sum_{b=0}^{t k-1}(n p)^{t k-b}(2t k)^{t k-b}(2t k)^{2b}\\frac{(2t k)^{2b}e^{2b}}{(2b)^{2b}}}\\\\ &{\\leq(n p)^{t k}(2t k)^{t k}\\displaystyle\\sum_{b=0}^{t k-1}\\frac{(2t k)^{3b}e^{2b}}{(2b)!(n p)^{b}}}\\\\ &{\\leq(n p)^{t k}(2t k)^{t k}\\displaystyle\\sum_{b\\geq0}^{t k}\\frac{(\\mathcal{C}_{1}e^{2})^{b}}{(2b)!}}\\\\ &{\\leq O((n p)^{t k}(2t k)^{t k}\\displaystyle\\sum_{b\\geq0}^{t k}\\frac{(\\mathcal{C}_{1}e^{2})^{b}}{(2b)!}}\\\\ &{\\leq O((n p)^{t k}(2t k)^{t k}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "In the second line, we used the fact that $(2t k-2b-1)!!$ is the product of $t k-b$ terms of value at most 2tk as well as the standard upper bound of 22tbk \u2264 (2t(k2)b2)b2be2b, t he second last bound follows ", "page_idx": 25}, {"type": "text", "text": "from our assumption on $p$ , and the final bound follows from the fact that the sum converges. Putting all of this together, we have that for $\\begin{array}{r}{t=\\frac{C_{1}\\log n}{2k}}\\end{array}$ C1 log nand p \u2265 $p\\geq{\\frac{\\log^{3}n}{n}}$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}[(e_{u}^{\\top}R^{k}s)^{2t}]\\leq O(\\frac{(2t k n p)^{t k}}{n^{t}})\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Now, we can apply Markov\u2019s inequality on the $t^{t h}$ moment with $\\begin{array}{r}{t=\\frac{C_{1}\\log n}{2k}}\\end{array}$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\operatorname*{Pr}\\big[\\big[|\\,|e_{u}^{\\top}R^{k}s|>\\frac{1}{\\sqrt{n}}(C\\sqrt{n p\\log n})^{k}\\big]\\leq\\frac{n^{t}\\mathbb{E}\\big[\\big(e_{u}^{\\top}R^{k}s\\big)^{2t}\\big]}{C^{2t k}\\big(n p\\log n\\big)^{t k}}}}\\\\ &{}&{\\leq O(\\frac{\\left(n p\\log n\\right)^{t k}C_{1}^{t k}}{C^{2t k}\\big(n p\\log n\\big)^{t k}})}\\\\ &{}&{\\leq O(\\frac{\\sqrt{C_{1}}^{\\log n}}{C^{\\log n}})}\\\\ &{}&{=\\exp(-\\Omega(\\log n))}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for a sufficiently large constant $C$ ", "page_idx": 26}, {"type": "text", "text": "Now, we combine our results from degree-concentration and the message passing error bound to prove the main result of this section, Lemma D.5. ", "page_idx": 26}, {"type": "text", "text": "Proof. (Lemma D.5) First, note that for $k=0$ , the bound clearly holds, as $\\begin{array}{r}{|e_{u}^{\\top}s|=\\frac{1}{\\sqrt{n}}}\\end{array}$ . Now for general $k$ , we start by applying Corollary D.4 to obtain ", "page_idx": 26}, {"type": "equation", "text": "$$\n|e_{u}^{\\top}R^{\\prime k}s|\\leq\\frac{1}{d^{k}}|e_{u}^{\\top}R^{k}s|+O(\\frac{k C_{1}^{k}\\sqrt{\\log n}}{\\sqrt{n p}^{k}\\cdot\\sqrt{n}})\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For some constant $C_{1}$ . Now by Proposition D.6, we have that with high probability $e_{u}^{\\top}R^{k}s\\ \\leq$ $\\begin{array}{r}{\\frac{1}{\\sqrt{n}}(C_{2}\\sqrt{n p\\log n})^{k}}\\end{array}$ for some constant $C_{2}$ and $d~\\geq~{\\frac{1}{2}}(p\\,+\\,q)n(1~-~o(\\bar{1}))$ . Thus, we have d1k |eu\u22a4 Rks| \u2264 \u221a1n C2 lopgn n . Note that $\\sqrt{\\log n}^{k}$ is bigger than $k{\\sqrt{\\log n}}$ for large enough $n$ . Thus, the $\\begin{array}{r}{\\frac{1}{d^{k}}|e_{u}^{\\top}R_{-}^{k}s|}\\end{array}$ term clearly dominates so we can take $C$ to be around $\\operatorname*{max}(C_{1},C_{2})$ to obtain our upper bound. Finally, by applying union bound, we can ensure that with high probability, bound holds for all $u\\in V$ . \u53e3 ", "page_idx": 26}, {"type": "text", "text": "D.2.1 Proof of proposition Proposition D.7 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we complete our proof of the main message passing error bound by proving the upper bound on the number of valid sets of walks of length $k$ . ", "page_idx": 26}, {"type": "text", "text": "Proof. (Proposition D.7) The idea to bound the quantity $|\\{\\vec{w}\\in\\mathcal{W}_{p a i r}^{2t}$ , $|\\vec{w}|=\\ell\\}|$ is to first count all the ways to partition the edges of the walks into exactly $\\ell$ groups of size at least 2. Then, for each partition, we count the number of ways to assign the vertices of the walks in a way such that all edges of the same group are assigned to the same pair of vertices. In the grouping stage, we first pick $2\\ell$ walk edges and pair them with each other, assigning a new group to each pair. Then for the rest of the $2t k-2\\ell$ edges, we assign each of them to one of the $\\ell$ groups. This ensures that each group has at least two edges. The number of ways to pick $2\\ell$ edges is $\\binom{2\\bar{k}t}{2\\ell}$ . The number of ways to pair these edges is $(2\\ell-1)!!=(2\\ell-1)(2\\ell-3)\\ldots\\cdot1.$ . Finally, the number of way to assign the rest of the edges to one of the groups is $\\ell^{(2k t-2\\ell)}$ . Thus, the total number of valid partitions of the edges is taitg hmt oasst $\\binom{2t k}{2\\ell}(2\\ell-1)!!\\cdot\\ell^{2k t-2\\ell}$ . Note that this upper bound is tight when $\\ell=t k$ but becomes less $\\ell$ ", "page_idx": 26}, {"type": "text", "text": "Given a partition of the edges, we now count the number of ways to assign the vertex variables $w_{i}(j)$ . We will give a simple assignment algorithm such that every valid assignment is a possible output of the algorithm. Note that not every output of the algorithm is necessarily valid, so we are actually over-counting. ", "page_idx": 26}, {"type": "image", "img_path": "MSsQDWUWpd/tmp/80d2a6691b88993795d3189436abbcb4169eda0a7f1505005fe03feb50b07a58.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "To see that every possible valid set of walks can be outputted by this procedure, if we are given some $\\vec{w}=(w_{1},...w_{2t})\\stackrel{\\cdot}{\\in}\\mathcal{W}_{p a i r}^{2t}$ , we can take the partitioning of the edges in the first step to be to be the partition induced by the union of the edges of the walks. Then, we can simply follow the vertices of the walks in the same order as in our assignment procedure and see that each time we encounter a new edge corresponds to the first case of the inner for-loop and each time we traverse an already traversed edge corresponds to the second case in the inner for-loop where the procedure does not fail. Thus, the number of valid sets of walks with $\\ell$ distinct edges is indeed upper bounded by the total number of possible outputs to our vertex assignment procedure. ", "page_idx": 27}, {"type": "text", "text": "To bound the number of possible outcomes in our procedure, we see that in first case of the inner for-loop, there are at most $n$ possible assignments for the vertex $w_{i}(j)$ . In the second step, there is only one possible assignment if the procedure doesn\u2019t fail. Since we encounter the first case of the for-loop at most $\\ell$ times, the total number of vertex assignments given a fixed partitioning of edges is at most n\u2113. Finally, this gives us the desired bound: ", "page_idx": 27}, {"type": "equation", "text": "$$\n|\\{\\vec{w}\\in\\mathcal{W}_{p a i r}^{2t},\\ |\\vec{w}|=\\ell\\}|\\leq{\\binom{2t k}{2\\ell}}(2\\ell-1)!!\\cdot\\ell^{2k t-2\\ell}(n)^{\\ell}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "E Proofs for Section 8 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this section, we will prove our main results for multi-class analysis, Theorem 8.1. First, we will characterize the convolution matrix in terms of the expected adjacency matrix: ", "page_idx": 28}, {"type": "text", "text": "Lemma E.1. In the multi-class setting, the convolution matrix, $\\tilde{A}_{:}$ , can be decomposed as $\\tilde{A}=M\\!+\\!R^{\\prime}$ where: ", "page_idx": 28}, {"type": "text", "text": "\u2022 M has rank $L-1$ , with $L-1$ eigenvalues equalled to $\\begin{array}{r}{\\frac{(p-q)n}{d L}=\\lambda.}\\end{array}$ . Also, $M U=\\lambda U$ \u2022 $R^{\\prime}$ is a random matrix such that with probability at least $1\\,-\\,n^{-\\Omega(1)}$ , $\\|{\\cal R^{\\prime}}\\|\\quad\\le$ $\\begin{array}{r}{C(\\frac{1}{d}(\\sqrt{n p(1-p)/L}+\\sqrt{n q(1-q)}))=\\delta}\\end{array}$ ", "page_idx": 28}, {"type": "text", "text": "Proof. The expected adjacency matrix in the multi-class setting can be written as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}[A]={\\left[\\begin{array}{l l l}{p J_{n/L}}&{q J_{n/L}}&{\\cdot\\cdot\\cdot q J_{n/L}}\\\\ {q J_{n/L}}&{p J_{n/L}}&{\\cdot\\cdot\\cdot q J_{n/L}}\\\\ {q J_{n/L}}&{\\cdot\\cdot\\cdot}&{\\cdot\\cdot q J_{n/L}}\\\\ {q J_{n/L}}&{q J_{n/L}}&{\\cdot\\cdot\\cdot p J_{n/L}}\\end{array}\\right]}=(q J_{L}+(p-q)I_{L})\\otimes J_{n/L}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The top (normalized) eigenvector of $\\mathbb{E}[A]$ is $\\textstyle{\\frac{1}{\\sqrt{n}}}\\mathbb{1}$ . The top eigenvalue is the expected degree $d\\,=\\,p n/L\\,+\\,(L\\,-\\,1)q n/L$ . Moreover, $\\mathbb{E}[A]$ has rank $L$ and its eigenvectors are of the form ${\\sqrt{\\frac{L}{n}}}v\\otimes\\mathbb{1}_{n/L}$ where $v$ is an eigenvector of the matrix $\\left(q J_{L}+(p-q)I_{L}\\right)$ . Since the top eigenvector is $\\begin{array}{r}{\\frac{1}{\\sqrt{n}}\\mathbb{1}=\\frac{1}{\\sqrt{L}}\\mathbb{1}_{L}\\otimes\\sqrt{\\frac{L}{n}}\\mathbb{1}_{n/L}}\\end{array}$ , the rest of the eigenvectors of $\\mathbb{E}[A]$ must be of the form $\\scriptstyle{\\sqrt{\\frac{L}{n}}}v_{2}\\;\\otimes$ $\\begin{array}{r}{\\mathbb{1}_{n/L},...\\sqrt{\\frac{L}{n}}v_{L}\\otimes\\mathbb{1}_{n/L}}\\end{array}$ , where $v_{2},...v_{L}$ are an orthonormal basis of the subspace in $\\mathbb{R}^{L}$ orthogonal to $\\mathbb{1}_{L}$ . Thus, for $l=2,..L$ , we have $\\begin{array}{r}{\\mathbb{E}[A](v_{l}\\otimes\\mathbb{1})=\\frac{1}{L}(p-q)n(v_{l}\\otimes\\mathbb{1})}\\end{array}$ , which means the second to $L^{t h}$ eigenvalues of $\\mathbb{E}[A]$ are equalled to $\\lambda d$ . ", "page_idx": 28}, {"type": "text", "text": "Now let $R=A-\\mathbb{E}[A]$ and . Then we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\tilde{A}=\\frac{1}{\\bar{d}}A-\\frac{1}{n}\\mathbb{1}\\mathbb{1}^{\\top}=(\\frac{1}{\\bar{d}}-\\frac{1}{\\bar{d}})A+\\frac{1}{\\bar{d}}R+\\frac{1}{\\bar{d}}\\mathbb{E}[A]-\\frac{1}{n}\\mathbb{1}\\mathbb{1}^{\\top}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Now, let $\\begin{array}{r}{M:=\\,\\frac{1}{d}\\mathbb{E}[A]\\,-\\,\\frac{1}{n}\\mathbb{1}\\mathbb{1}^{\\top}}\\end{array}$ . Note that the non-zero eigenspace of $M$ is equivalent to 2nd to $L^{t h}$ eigenspace of $\\mathbb{E}[A]$ and the corresponding eigenvalue is $\\lambda$ . To show that $M U=U$ , let $u$ be any column of $U$ . By our assumption that each class has one center, the entries of $u$ are constant on each class. If we define $\\boldsymbol{v}\\in\\mathbb{R}^{L}$ such that $v(l)=u(i)$ for $i\\in\\mathcal{C}_{l}$ , then we can write $u=v\\otimes\\mathbb{1}_{n/L}$ . Since by our assumption, $u\\perp\\mathbb{1}_{n}$ , we must have $v\\perp\\mathbb{L}_{L}$ . Thus, we see that $u$ is exactly in the subspace spanned by the non-zero eigenvectors of $M$ , which means $M u=\\lambda u$ . ", "page_idx": 28}, {"type": "text", "text": "Now, we simply have to bound the spectral norm of the matrix $\\textstyle R^{\\prime}:={\\frac{1}{d}}R+({\\frac{1}{d}}-{\\frac{1}{d}})A$ . We start with bounding $\\|R\\|$ . To do so, we can write $R$ as $R\\,=\\,R_{p}+R_{q}$ , where $R_{p}$ and $R_{q}$ contain the entries of $R$ corresponding to intra- and inter- class edges respectively. Note that $R_{p}$ is block diagonal with $L$ blocks of size $n/L$ . Each block is size $n/L\\stackrel{}{\\times}n/L$ and have $i.i.d$ entries. By Theorem A.4, each block has spectral norm at most $O(\\sqrt{n p(1-p)/L})$ with probability at least $1-n^{-\\Omega(1)}$ . Similarly, we have $\\|R_{q}\\|\\leq O(\\sqrt{n q(1-q)})$ with probability at least $1-n^{-\\Omega(1)}$ . This means $\\|R\\|\\leq O(\\sqrt{n p(1-p)/L}+\\sqrt{n q(1-q)})$ . ", "page_idx": 28}, {"type": "text", "text": "Now, we bound the degree deviation. Recall that $\\begin{array}{r}{d=\\frac{2|E|}{n}}\\end{array}$ where $|E|$ is the number of edges in $G$ . By applying Theorem A.3 with $t=\\sqrt{n p(1-p)/L}+\\sqrt{n q(1-q)}$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbf{Pr}\\left[\\vert\\bar{d}-d\\vert>t\\right]\\le\\exp(-\\Omega(\\frac{t^{2}}{\\operatorname*{max}(p,q)}))\\le n^{-\\Omega(1)}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "since $\\operatorname*{min}(p,q)\\,\\geq\\,\\log^{2}n/n$ . Assuming that $|\\bar{d}-d|\\,\\leq\\,t$ and noting that $\\|A\\|\\,\\leq\\,{\\bar{d}}$ , we have $\\begin{array}{r}{\\left\\|(\\frac{1}{d}-\\frac{1}{d})A\\right\\|\\leq\\|R\\|\\leq t/d.}\\end{array}$ Thus, we have $\\tilde{A}=M\\!+\\!R^{\\prime}$ , where $\\|R^{\\prime}\\|\\leq\\delta$ with high probability. ", "page_idx": 28}, {"type": "text", "text": "We will also need to bound the operator norm distance between the $k^{t h}$ convolution and $M^{k}$ ", "page_idx": 28}, {"type": "text", "text": "Lemma E.2. Suppose $|\\lambda|>2k\\delta.$ . Then with high probability, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left\\|{\\frac{1}{\\lambda^{k}}}({\\tilde{A}}^{k}-M^{k})\\right\\|\\leq2k\\delta/|\\lambda|.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. By Lemma E.1, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\tilde{A}^{k}=(M+R^{\\prime})^{k}=M^{k}+\\sum_{l=1}^{k}\\sum_{b\\in\\binom{[k]}{l}}\\prod_{i=1}^{k}M^{1-b(i)}R^{\\prime b(i)},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the inner sum is over bit-strings $b$ of length $k$ with exactly $l\\textsubscript{11}$ \u2019s and $k-l\\;0^{\\circ}\\mathrm{s}$ . Note that $\\left\\|M\\right\\|=\\left|\\lambda\\right|$ and $\\|R^{\\prime}\\|\\leq\\delta$ with high probability. Using the fact that $\\left\\|A B\\right\\|\\leq\\left\\|A\\right\\|\\left\\|B\\right\\|$ and triangle inequality, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left\\|{\\frac{1}{\\lambda^{k}}}((M+R^{\\prime})^{k}-M^{k})\\right\\|\\leq{\\frac{1}{|\\lambda|^{k}}}\\sum_{l=1}^{k}{\\binom{k}{l}}\\left\\|M\\right\\|^{k-l}\\left\\|R^{\\prime}\\right\\|^{l}\\leq\\sum_{l=1}^{k}{\\binom{k}{l}}({\\frac{\\delta}{|\\lambda|}})^{l}=(1+{\\frac{\\delta}{|\\lambda|}})^{k}-1\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Our assumption that $|\\lambda|>4\\delta k$ implies the RHS is at most $2k\\delta/|\\lambda|$ by Lemma A.6. ", "page_idx": 29}, {"type": "text", "text": "Now we are ready to prove Theorem 8.1. ", "page_idx": 29}, {"type": "text", "text": "Proof. We can express the total squared error after $k$ convolutions as: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\left\\|x_{i}^{(k)}-\\mu_{i}\\right\\|^{2}=\\left\\|X^{(k)}-U\\right\\|_{F}^{2}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We decompose our data as $X^{(k)}=U+G$ , where $G$ is a Gaussian matrix with i.i.d $N(0,\\sigma^{2})$ entries. Recall that we take our scaling factor to be $1/\\lambda^{k}$ . Thus, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left\\lVert X^{(k)}-U\\right\\rVert_{F}^{2}=\\left\\lVert\\frac{1}{\\lambda^{k}}\\tilde{A}^{k}(U+G)-U\\right\\rVert_{F}^{2}\\le2\\left\\lVert\\frac{1}{\\lambda^{k}}\\tilde{A}^{k}U-U\\right\\rVert_{F}^{2}+2\\left\\lVert\\frac{1}{\\lambda^{k}}\\tilde{A}^{k}G\\right\\rVert_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "By Lemma E.1, we have $M U=\\lambda U$ . Let $u_{1},...u_{m}$ be the columns of $U$ . Then, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|\\frac{1}{\\lambda^{k}}\\tilde{A}^{k}U-U\\right\\|_{F}^{2}=\\left\\|\\frac{1}{\\lambda^{k}}(\\tilde{A}^{k}-M^{k})U\\right\\|_{F}^{2}}&{}\\\\ {=\\displaystyle\\sum_{i=1}^{m}\\left\\|\\frac{1}{\\lambda^{k}}(\\tilde{A}^{k}-M^{k})u_{i}\\right\\|^{2}}&{}\\\\ {\\leq\\displaystyle\\sum_{i=1}^{m}\\left\\|\\frac{1}{\\lambda^{k}}(\\tilde{A}^{k}-M^{k})\\right\\|^{2}\\left\\|u_{i}\\right\\|^{2}}&{}\\\\ {=\\left\\|\\frac{1}{\\lambda^{k}}(\\tilde{A}^{k}-M^{k})\\right\\|^{2}\\left\\|U\\right\\|_{F}^{2}}&{}\\\\ {\\leq O(k\\delta/\\lambda|\\lambda|)^{2}\\left\\|U\\right\\|_{F}^{2}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the last inequality follows from Lemma E.2. By Lemma A.5, we have, with high probability, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left\\|\\frac{1}{\\lambda^{k}}\\tilde{A}^{k}G\\right\\|_{F}^{2}\\leq\\frac{1}{\\lambda^{2k}}T r(\\tilde{A}^{2k})\\sigma^{2}m\\log n\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Since adding $R^{\\prime}$ to $M$ perturbs its eigenvalues by at most $\\delta$ (Theorem A.1), we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{1}{\\lambda^{2k}}T r(\\tilde{A}^{2k})\\leq(1+\\delta/|\\lambda|)^{2k}(L-1)+n(\\delta/|\\lambda|)^{2k}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "$|\\lambda|~>~4\\delta k$ implies $(1\\,+\\,\\delta/|\\lambda|)^{2k}\\ \\leq\\ O(1)$ . Now let $n_{e}$ be the number of points $i$ such that $\\left\\|{\\bar{x}}_{i}^{(k)}-\\mu_{i}\\right\\|\\geq\\Delta/2$ . Then we have ", "page_idx": 29}, {"type": "equation", "text": "$$\nn_{e}\\Delta^{2}\\leq4\\left\\|X^{(k)}-U\\right\\|_{F}^{2}\\leq O((k\\delta/|\\lambda|)^{2}\\left\\|U\\right\\|_{F}^{2}+(L+n(\\delta/|\\lambda|)^{2k})m\\sigma^{2}\\log n)\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Dividing both sides by $\\Delta^{2}$ gives us the desired bound. ", "page_idx": 30}, {"type": "text", "text": "Finally, we recall that $\\Delta$ is the minimum distance between centers. Thus if $\\left\\|x_{i}^{(k)}-\\mu_{i}\\right\\|<\\Delta/2$ , then it is closer to its own center than any other center. Thus, the softmax classifier can correctly classify all such points. ", "page_idx": 30}, {"type": "text", "text": "E.1 Additional Figures for Multi-class simulation ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Finally, we conclude our section with some additional figures to illustrate the performance of the corrected convolution on synthetic data. We compare the corrected and uncorrected convolutions via both linear and non-linear models. Our means are class means are given by the standard basis vectors. For the linear model (figs 4a \u2013 4c), we look at five 1-vs-all classifiers followed by a softmax to predict the class label, while the non-linear method (figs 4d \u2013 4f) follows a typical two-layer MLP-based architecture. In both cases, we observe that the corrected convolutions do not deteriorate in performance as the number of convolutions increases. Note that while the performances are similar between linear and non-linear classification, non-linear classification is required in general when each individual class mean cannot be separated from all others via a linear hyperplane. ", "page_idx": 30}, {"type": "image", "img_path": "MSsQDWUWpd/tmp/44577812fd43cc8d3b21ae7c7369b1dc8840284ea69a4721d9903543bb3264a4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "(a) Linear method, $q=0.02$ , $\\sigma=\\left({\\mathfrak{b}}\\right)$ Linear method, $q=0.02$ , $\\sigma=\\left(\\mathsf{c}\\right)$ Linear method, $q=0.04$ , $\\sigma=$ 1. 8. 8. ", "page_idx": 30}, {"type": "image", "img_path": "MSsQDWUWpd/tmp/32e45041dfc0764829bee300c91bbc5d79e955e74ffaf95afa84a71b36712cb2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "(d) Nonlinear method, $q\\ =\\ 0.02,$ ,(e) Nonlinear method, $q~=~0.02.$ ,(f) Nonlinear method, $q~=~0.04$ , $\\sigma=1$ . $\\sigma=8$ . $\\sigma=8$ . ", "page_idx": 30}, {"type": "text", "text": "Figure 4: Accuracy plot (averaged over 50 trials) on CSBM data with 5 balanced classes, 500 nodes per class and orthogonal means, with fixed $p=0.1$ . ", "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The claims made match our theoretical contributions. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: We mention that we work on the contextual stochastic block model. Also, throughout the paper we mention limitations of our analysis. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We do exactly what the question asks. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: We provide information about the settings in our experiments. We also provide reproducible code. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We provide code that reproduces the experiments by just executing simple Python notebooks. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We do exactly what the question asks. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We do provide errors bars in our plots. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We do exactly what the question asks. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We have read the NeurIPS Code of Ethics. Our work is theoretical and we don\u2019t believe it has a direct path to negative societal impact. On the contrary, we hope that our thorough theoretical analysis helps in interpreting performance of the models that we use, which could, potentially, have positive impact in safety, security, discrimination, surveillance, deception and harassment, environment, human rights, bias and fairness. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [No] ", "page_idx": 34}, {"type": "text", "text": "Justification: As we mention above, our work is theoretical and we don\u2019t believe it has a direct path to negative societal impact. On the contrary, we hope that our thorough theoretical analysis helps in interpreting performance of the models that we use, which could, potentially, have positive impact in safety, security, discrimination, surveillance, deception and harassment, environment, human rights, bias and fairness. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 34}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: Our paper is theoretical. It poses no such risks. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not release new assets ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}]