[{"type": "text", "text": "Learning to Edit Visual Programs with Self-Supervision ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "R. Kenny Jones Brown University russell_jones $@$ brown.edu ", "page_idx": 0}, {"type": "text", "text": "Renhao Zhang University of Massachusetts, Amherst renhaozhang@cs.umass.edu ", "page_idx": 0}, {"type": "text", "text": "Aditya Ganeshan Brown University aditya_ganeshan $@$ brown.edu ", "page_idx": 0}, {"type": "text", "text": "Daniel Ritchie Brown University daniel_ritchie@brown.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We design a system that learns how to edit visual programs. Our edit network consumes a complete input program and a visual target. From this input, we task our network with predicting a local edit operation that could be applied to the input program to improve its similarity to the target. In order to apply this scheme for domains that lack program annotations, we develop a self-supervised learning approach that integrates this edit network into a bootstrapped finetuning loop along with a network that predicts entire programs in one-shot. Our joint finetuning scheme, when coupled with an inference procedure that initializes a population from the one-shot model and evolves members of this population with the edit network, helps to infer more accurate visual programs. Over multiple domains, we experimentally compare our method against the alternative of using only the one-shot model, and find that even under equal search-time budgets, our editing-based paradigm provides significant advantages. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "People seldom write code with a linear workflow. The process of authoring code often involves substantial trial-and-error: possibly correct programs are evaluated through execution to see if they raise exceptions or break input-output assumptions. When an error is identified, an edit is made, and this process is repeated. It is difficult to imagine writing any moderately complex program in a one-shot paradigm, without being able to debug intermediate program versions. ", "page_idx": 0}, {"type": "text", "text": "The field of program synthesis studies how to automatically infer a program that meets an input specification [13]. In this work, we consider the sub-problem of visual program induction (VPI), where the input specification is a visual target (e.g. an image) and the goal is to find a program whose execution recreates the target [33]. This task has numerous applications across visual computing disciplines, including reverse-engineering, structure analysis, manipulation, and generative modeling. ", "page_idx": 0}, {"type": "text", "text": "This problem area has garnered significant interest, with many works exploring learning-based solutions. For domains with annotated data, supervised approaches perform well [41, 43]. For domains that lack program annotations, a variety of unsupervised and self-supervised learning paradigms have been proposed [19, 34, 38, 45]. Moreover, initial investigations have explored the capabilities of Large Language Models for solving simple VPI tasks [3]. ", "page_idx": 0}, {"type": "text", "text": "Though these prior approaches have made impressive progress, a common limitation is that they operate within the aforementioned one-shot paradigm. For instance, when using an autoregressive network these systems will condition on a visual target and iteratively predict program tokens until completion. While this sequential inference procedure is sometimes wrapped in a more complex outersearch (e.g. beam-search or sequential Monte Carlo [10]), is allowed to reason over partial program executions [25], or is given access to executor-gradients that guide an inner-loop search [12, 45], all of these paradigms are distinct from how people typically write programs. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this work, we present a model that learns how to edit visual programs in a goal-directed manner.   \nOur network consumes a complete input program, this program\u2019s executed state, and a visual target.   \nIt then proposes a local edit operation that modifies the input program to better match the target.   \nIn contrast with one-shot approaches, this framing allows our network to explicitly reason over a complete program and its execution, in order to decide how this program should be modified. ", "page_idx": 1}, {"type": "text", "text": "We train our network without access to any ground-truth program annotations. To accomplish this, we propose an integration of our edit network with prior self-supervised bootstrapping approaches for one-shot VPI models [19]. During iterative finetuning rounds, we source paired training data for our edit network by first constructing pairs of start and end programs, and then using a domain-aware algorithm to find a set of edit operations that would bring about this transformation. This process jointly finetunes both our edit network and a one-shot network, and we propose an integrated inference algorithm that leverages the strengths of both of these paradigms: the one-shot model produces rough estimates that are refined with the edit network. We find that this joint self-supervised learning set-up forms a virtuous cycle: the one-shot model provides a good initialization state for the edit network, and the edit network improves inner-loop inference, creating better bootstrapped training data for the one-shot model. ", "page_idx": 1}, {"type": "text", "text": "We experimentally compare the effectiveness of integrating our edit network into this joint paradigm against using one-shot models alone. Controlling for equal inference time, over multiple visual programming domains, we find that using the edit network improves reconstruction performance. Moreover, we find that the reconstruction gap between these two paradigms widens as more time is spent on test-time program search. Further, we demonstrate our method performs remarkably well even with very limited data, as learning how to edit is an inherently more local task compared with learning how to author a complete program. Finally, we run an ablation study to understand and justify our system design. ", "page_idx": 1}, {"type": "text", "text": "In summary, we make the following contributions: (1) A model that learns how to predict local edits that improve visual programs towards a target. (2) A self-supervised learning paradigm that jointly trains an edit network and a one-shot network through bootstrapped finetuning. ", "page_idx": 1}, {"type": "text", "text": "We release code for our experiments at: https://github.com/rkjones4/VPI-Edit ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Visual Program Induction There has been growing interest in works that aim to infer structured procedural representations that explain visual data [33]. While some research has investigated geometric heuristics to search for a well-reconstructing program [9, 42], most methods employ learned networks to guide this search. For visual programming domains that come with annotations, networks can be trained with ground-truth program supervision [1, 17, 41, 43]. ", "page_idx": 1}, {"type": "text", "text": "However, most visual domains of interest lack such annotated data. As a result, a host of techniques have been investigated for this unsupervised setting, including: reinforcement learning [10, 34], differentiable execution architectures [20, 31, 36, 44, 45], learned proxy executors [8, 38], or bootstrapping methods [11, 15, 19]. All of these works operate within the aforementioned one-shot paradigm. Of note, SIRI investigates how analytical code rewriting operations can improve VPI networks in a bootstrapped learning paradigm [12]. Our system shares a similar motivation in that we aim to rewrite visual programs in a goal-directed fashion. However, instead of modifying programs with domain-specific fixed operations (e.g. differentiable parameter optimization), we explore a generalized alternative by introducing a network that learns how to edit programs. ", "page_idx": 1}, {"type": "text", "text": "Our edit network reasons over the visual execution produced by an input program to decide how the program should be edited. The idea of reasoning over program executions to improve search has been successfully demonstrated for both general program synthesis problems [6, 47] and visual program induction [10, 25]. However, different from our approach, which predicts a local edit that modifies a complete program, these approaches reason over the executions of partial programs in order to better guide auto-regressive synthesis (i.e. they largely operate in a one-shot paradigm). ", "page_idx": 1}, {"type": "image", "img_path": "uzIWqRzjEP/tmp/e42462c2e7dc65581e33ae7e4e5b9ccd0d0b789dcadc394de0aeba54606d8b87.jpg", "img_caption": ["Figure 1: We design a network that learns how to locally edit an input program towards a target. It first predicts what type of edit operation should be applied, then it predicts where that edit operation should be applied, and finally it autoregressively samples any parameters the edit operation requires. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Program Repair A number of program synthesis methods have been proposed that learn how to repair or fix programs for domains where ground-truth programs are available. SED interleaves a series of synthesis, execution and debugging steps in order to improve synthesis of Karel programs from input/output examples [14]. Related approaches have explored learning how to \u2018fix\u2019 programs end-toend by manipulating latent-space encodings of programs under a fixed decoder for the RobustFill domain [2]. While our method shares a similar motivation with these works, we demonstrate the efficacy of our approach on more complex visual programming domains, we don\u2019t assume access to ground-truth program annotations, and our edit network only predicts a single local edit operation at each step, instead of rewriting an entire program. ", "page_idx": 2}, {"type": "text", "text": "With the growing attention around the abilities of Large Language Models (LLMs), a number of recent works have explored how LLMs can be used to fix programs from input/output examples [5, 23, 24, 35]. Though differing in details, the typical formulation these methods take involves presenting an LLM with a previous program version, and asking it to either (i) debug exceptions or (ii) modify program behavior in light of input/output mismatches. While these initial forays show promise, LLMs have not yet been able to write complex visual programs (in part due to poor visual reasoning capabilities), and even for more general program synthesis tasks the performance gains of code-editing LLMs are not definitive [26]. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we present our approach for learning how to edit visual programs. First we formalize our task of unsupervised visual program induction. For a particular domain, we are given a domainspecific language (DSL) $L$ and an executor $E$ that converts programs $z$ from $L$ into visual outputs $x$ . Given visual inputs from a target visual dataset that lacks program annotations, $x^{*}\\in X^{*}$ , our goal is to find find $z^{*}\\in L$ , such that $E(z^{*})\\sim x^{*}$ . This measure of similarity is usually checked under a domain specific reconstruction metric $M$ . ", "page_idx": 2}, {"type": "text", "text": "A general approach employed by prior visual program induction works is to use an autoregressive model (e.g. a Transformer) that is conditioned on a visual encoding to predict a well-reconstructing program: $p(z|x)$ . These one-shot models iteratively predict the next program token until the program is complete. We present a framework that employs a similar autoregressive model, but instead of predicting a complete program from scratch, we instead predict a local edit that modifies an input program. In the rest of this section, we first present how we design our edit network (Sec. 3.1). Then we discuss our unsupervised training procedure where we jointly finetune an edit network along with a one-shot network (Sec. 3.2. Finally, we describe how we combine these networks to search for visual programs (Sec. 3.3). ", "page_idx": 2}, {"type": "text", "text": "3.1 Edit Network Design ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our edit network $p(e|z,x)$ learns how to predict a local edit operation that improves an input program towards a visual target (see Figure 1). We provide our network with a triplet input state: the tokens of an input program $z$ , this program\u2019s executed output $E(z)$ , and a visual target $x$ . From this state, our network is tasked with predicting an edit operation $e$ that could be applied to the input program. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Edit Operations. There are many ways to parameterize the space of possible program edits. We choose to constrain the possible edit operations our network can produce by forcing it to select from a set of local editing operations designed for visual programs. For instance, for functional visual programming DSLs with transformation and combinator functions, we allow for seven different edit operations: modifying a transform\u2019s parameters $(M P)$ , modifying a transform $(M T)$ , adding a transform $(A T)$ , removing a transform $(R T)$ , modifying a combinator $(M C)$ , removing a combinator $(R C)$ , or adding a combinator $(A C)$ . We provide more details in Appendix E. Some of these edit operations do not take in parameters (removing a transform) while others require new parameters (e.g. to modify the parameters of a transform we need to know the new parameters). Each of these edit operations can be applied to a program at a specific token location, and results in a local change. Subsequently, we task our edit network with predicting three items: an edit operation type, a location for that edit operation, and any extra parameters that operation requires. ", "page_idx": 3}, {"type": "text", "text": "We design our system with this somewhat constrained edit operation set as it has a number of advantages. First, the application and effect of each edit operation is local; this simplifies the learning task and allows us flexibility at inference time. Moreover, ensuring that edit operations are tied to the semantics of the underlying DSL helps to promote program edits that result in syntactically valid modified programs. We compare our edit operation design against alternative formulations in our experimental results (Sec. 4.5). ", "page_idx": 3}, {"type": "text", "text": "Architecture. We implement our edit network as a Transformer decoder. This network has full attention over the conditioning information: each visual input (the executed output of the input program and the target) is encoded into a sequence of visual tokens (e.g. with a CNN) and each token of the input program is lifted with an embedding layer. ", "page_idx": 3}, {"type": "text", "text": "To predict the edit operation type, we take the output Transformer embedding from the first index of input program sequence. This embedding is sent through a linear layer which predicts a distribution over the possible edit operation types (yellow boxes, Fig. 1). ", "page_idx": 3}, {"type": "text", "text": "To predict the edit operation location, we consider the embeddings that the Transformer produces over the tokens of the input program. Each of these location codes is sent through a linear layer, which predicts a value for each operation type. For a chosen operation type, we then normalize these values into a probability distribution across the length of the input program sequence (dark-blue boxes, Fig. 1). This distribution models the likelihood of where a specific edit operation type should be applied. ", "page_idx": 3}, {"type": "text", "text": "Finally, we use our network to autoregressively sample any extra parameters that a chosen edit operation might require. To accomplish this, we first slightly reformat the input program by inserting a special \u2018sentinel token\u2019 [29] associated with the chosen edit operation in two places: (1) at the specified edit operation location and (2) at the end location of the current program ( $\\mathcal{S}A T,$ , Fig. 1). This \u2018sentinel\u2019 tokens allows the network to know what operation is being applied to which position. Then, starting from the location of the second sentinel token, we can use the network to iteratively generate a sequence of parameter predictions with causal attention-masking, until an \u2018END\u2019 token is chosen (green boxes, Fig. 1). ", "page_idx": 3}, {"type": "text", "text": "Training. Given an input program, how do we know which edit operations are helpful? If we have access to not only a visual target, but also its corresponding program, we can find a set of edit operations that would transform the input program into this target. We follow this logic to source training data for our edit network: given a start program and an end program, we analytically identify a set of edit operations that would bring about this transformation with a findEdits function. We can then convert this set of edit operations into a large set of (input, output) pairs that our network can train on. We provide further details on this algorithm in Appendix E. Once we have sourced paired data, through teacher-forcing we can train our network in a supervised fashion with a cross-entropy loss on the predicted operation type, location, and each parameter token. Though we lack known programs for the target domain of interest, we next discuss a bootstrapped finetuning procedure that provides a work-around for this issue. ", "page_idx": 3}, {"type": "text", "text": "Algorithm 1 Network Training ", "text_level": 1, "page_idx": 4}, {"type": "image", "img_path": "uzIWqRzjEP/tmp/f25111a82b98f286dae7ff71a043617d7df6326163ac5c457deaed565b0111f7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "image", "img_path": "uzIWqRzjEP/tmp/d4f65f41f8deef1c7967d1e9386f3dd72a7521c78f96eda92f23bf16b7076488.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 2: Left: our bootstrapping algorithm that finetunes an edit network and a one-shot model towards a target dataset. Right: our inference algorithm that initializes a population with a one-shot model and then mutates it towards a visual target through iterative rounds of edits and resampling. ", "page_idx": 4}, {"type": "text", "text": "3.2 Learning Paradigm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As we operate in a paradigm where we don\u2019t have access to ground-truth programs for our target set $X^{*}$ , we take inspiration from recent self-supervised approaches that employ bootstrapped finetuning for visual program induction [12, 19]. Specifically, we develop an algorithm (Alg. 1) that integrates edit network training into the PLAD finetuning framework. ", "page_idx": 4}, {"type": "text", "text": "PLAD Finetuning. We begin with an overview of the PLAD method, which is depicted with the black text in Alg. 1 (see [19] for details). At the start of each round, the program inference network $p(z|x)$ is run over the target dataset $X^{*}$ ; the results of this inference procedure populate the entries of a best programs data-structure $P^{\\mathrm{BEST}}$ according to $M$ . Then an unconditional generative model $p(z)$ is trained over the entries of $P^{\\mathrm{BEST}}$ , and a set of \u2018dreamed\u2019 programs, $P^{G}$ , are sampled from this network. The weights of $p(z|x)$ are then finetuned using paired data sourced from $P^{\\mathrm{BEST}}$ and $P^{G}$ . These steps are repeated for a set number of rounds, or until convergence. ", "page_idx": 4}, {"type": "text", "text": "Edit Model Finetuning. The blue-colored lines in Alg. 1 indicate the modifications we make to the PLAD algorithm to incorporate our edit network. Lines 8-10 explain the training logic. First we use $p(z|x)$ to sample a set of programs $P^{S}$ conditioned on the executed outputs of the generated programs ${\\big.}^{\\prime}P^{G}$ . Treating $P^{S}$ as the starting points and $P^{G}$ as the end points, we can then use our findEdits operation to find sets of edit operations $E S$ that would realize these transformations. This provides us with paired data that we can use to finetune the weights of the edit network through teacher forcing, as explained in the prior section. ", "page_idx": 4}, {"type": "text", "text": "Synthetic Pretraining. PLAD finetuning is typically initialized with a synthetic pretraining phase (Alg. 1, line 1). During pretraining, random programs are sampled from $L$ , and $p(z{\\bar{|}}x)$ can be trained on the paired data produced by executing these samples. Similarly, as we discuss in the results section, we find it useful to \u2018pretrain\u2019 the edit network on synthetic data (Alg. 1, line 2). While multiple formulations are possible here, we re-use the same logic shown on lines 8-10, except we replace the set of target programs $P^{G}$ with random programs sampled from $L$ . ", "page_idx": 4}, {"type": "text", "text": "3.3 Inference Algorithm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "With the above procedure we can train our edit network, but how can we use this network to find improved visual programs? This question is not only relevant at test-time, but also impacts bootstrapped training, as we run an inner-loop search to populate the entries of $P^{\\mathrm{BEST}}$ (Alg. 1, line 5). As depicted on the right side of Figure 2, we design a search procedure that combines the strengths of the one-shot and editing paradigms. This search procedure maintains a population of programs, which are evolved over a number of rounds. The initial population is produced by sampling $\\bar{p(z|x)}$ . Then for each round, we use the edit network to sample sets of edits for every program in the current population. We apply each of these sampled edits, and then re-sample the population for the next round according to a ranking based on $M$ . ", "page_idx": 4}, {"type": "text", "text": "Table 1: Across multiple visual programming domains we evaluate test-set reconstruction accuracy. In all cases, we find that our joint paradigm that integrates an edit network with one-shot models outperforms the alternative of using only one-shot models. ", "page_idx": 5}, {"type": "table", "img_path": "uzIWqRzjEP/tmp/1ba51a95cc0d05327687658a0ea988d8edc728e71feb9d7050219a76e52c7d94.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "This formulation has a number of advantages. Instead of starting from a blank canvas, or with random samples, we allow $p(z|x)$ to produce initial rough program estimates. These guesses are then refined through mutations over a series of editing rounds that are all directed at improving similarity towards the visual target. In Section 4.5 we compare this algorithm against alternative formulations. Critically, by applying this joint inference procedure during finetuning we form a virtuous cycle: improving the inference strategy leads to better $P^{\\mathrm{BEST}}$ entries, which results in better training data for $p(z|x)$ and $p(e|z,x)$ , which in turn allows us to find to better $P^{\\mathrm{BEST}}$ entries in subsequent finetuning rounds. Finally, we note that this formulation maintains a nice symmetry between $p(z|x)$ and $p(e|z,x)$ : in out joint finetuning algorithm $p(e|z,x)$ trains on sequences sourced from sampling $p(z|x)$ , and in this way its training distribution of edit operations well matches the population used to initialize the inference algorithm. ", "page_idx": 5}, {"type": "text", "text": "4 Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We evaluate our edit network with experiments over multiple domains. First we describe our experimental design (Sec. 4.1). Then we compare the ability of different methods to accurately infer visual programs in terms of reconstruction performance (Sec. 4.2). We analyze how this performance changes as a function of time spent on inference (Sec. 4.3) or the size of the training target dataset (Sec. 4.4). Finally, we discuss results of an ablation study on our method in Section 4.5. ", "page_idx": 5}, {"type": "text", "text": "4.1 Experimental Design ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We provide a high-level overview of our experimental design. See Appendix D for details. ", "page_idx": 5}, {"type": "text", "text": "Methods. We compare our approach $(O S{+}E d i t)$ against the alternative of using only a one-shot model $(O S\\,O n l y)$ . As described in Section 3, our approach jointly finetunes an edit network along with a one-shot network, and uses both of these networks to infer visual programs (Fig. 2). To control for the added time cost incurred by our inference procedure, we adapt a sampling-based inference loop for the OS Only variant, which we find results in a surprisingly strong baseline. ", "page_idx": 5}, {"type": "text", "text": "Domains. We consider three VPI domains (see App C): Layout, 2D CSG, and 3D CSG. In the Layout domain, scenes are created by placing colored 2D primitives on a canvas, and optionally modifying them by changing their size, location, or forming a symmetry group. In constructive solid geometry (CSG), complex shapes are formed by combining simple shapes with boolean set operations (union, intersection, difference). Our 2D CSG and 3D CSG domains differ in terms of their primitive types (e.g. squares vs cuboids) and the parameterizations of transformation functions: generalizing notions of scaling, translating, rotating, and symmetry grouping from $\\textstyle\\mathbb{R}^{2}$ to $\\mathbb{R}^{3}$ . ", "page_idx": 5}, {"type": "text", "text": "Network Details. For each domain, we implement $p(z|x)$ as a decoder-only Transformer [39] that conditions on a set of visual tokens and predicts up to a maximum sequence length $S L$ . Similarly, we implement $p(e|z,x)$ as a Transformer with the same architecture, except that it conditions on (i) two sets of visual tokens and (ii) an input program of length $S L$ , and it is only allowed to predict edit parameters up to a length of $E L$ . Our visual encoders are all standard CNNs. For Layout we use a 2D CNN that takes in an RGB $64\\mathrm{x}64$ image, for 2D CSG we use a 2D CNN that takes in a binary 64x64 image, and for 3D CSG we use a 3D CNN that takes in a $32^{3}$ voxel grid. ", "page_idx": 5}, {"type": "text", "text": "Reconstruction Metric. The reconstruction metric $M$ guides the inference algorithm and also performs early stopping with respect to a validation set. For Layout we use cIoU, an intersection over union metric which only counts intersections on color matches [18]. For 2D CSG we use an edge-based Chamfer distance $(C D)$ [34]. For 3D CSG we use intersection over union $(I o U)$ . ", "page_idx": 5}, {"type": "image", "img_path": "uzIWqRzjEP/tmp/d7e07bf99f21c51f0c8b3a7952c32e3ca31e7078c824caf0f0ca6c8646065839.jpg", "img_caption": ["Figure 3: Comparing reconstructions of one-shot models (top) against our joint approach (middle). "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Target Data. Like prior bootstrapping methods, our finetuning algorithm specializes our networks towards a target dataset of interest, $X^{*}$ , that lacks known programs. For 2D CSG we use shapes from the dataset introduced by CSGNet [34], originally sourced from Trimble 3D warehouse. For 3D CSG we use shapes from the dataset introduced by PLAD [19], originally sourced from ShapeNet [4]. While we use the same test-sets as prior work $3000\\,/\\,1000$ for 2D CSG / 3D CSG), we find that our method is able to offer good performance with much less training data. In our base experiments, we use 1000/100 train/val shapes for 2D CSG (from 10000 / 3000 available) and and 1000/100 train/val shapes for 3D CSG (from $10000\\,/\\,1000$ available). For the Layout domain, we use the manually designed scenes sourced from [18] (1000 train / 100 val / 144 test). ", "page_idx": 6}, {"type": "text", "text": "4.2 Reconstruction Accuracy ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We compare our $O S+E d i t$ approach against OS Only on each method\u2019s ability to infer visual programs that accurately reconstruct test-set inputs in Table 1. As demonstrated, our joint finetuning paradigm that combines an edit network with a one-shot network consistently improves reconstruction performance. In these experiments, we ensure that each method gets to spend the same amount of time on inference by setting search parameters so that the average inference time per shape was equal: $\\sim5,\\sim10,\\sim60$ seconds per shape for Layout, 2D CSG, and 3D CSG respectively. For $O S\\,O n l y$ , we use a sampling-based inference search where the model samples a population of complete programs for a set number of rounds. Though this approach provides a strong baseline, it was not as effective as combining our edit networks with one-shot initializations. In fact, for the 2D CSG domain, our formulation achieves reconstruction scores that surpass the performance of related methods that assume access to executor-gradients. On the 2D CSG test-set, we achieve a Chamfer distance (CD) of 0.111 (lower is better), whereas UCSG-Net [20] gets a CD of 0.320, SIRI [12] gets a CD of 0.260, and ROAP [36] gets a CD of 0.210 . Note that as the DSL, architecture, objective, and inference procedures differ across these various works, it\u2019s hard to make any absolute claims from this direct comparison. Nevertheless we would like to emphasize that our method\u2019s reconstruction performance on this task is very strong in the context of the related literature. We visualize reconstructions from this experiment in Figure 3, and find that qualitative evidence supports the quantitative trends. ", "page_idx": 6}, {"type": "text", "text": "4.3 Search Time ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "While one-shot models must author new programs from scratch without execution-feedback, our edit network has the capacity to reason over an input program, compare its execution versus the visual target, and decide how this program should be modified. As such, we hypothesize that integrating our edit network into our inference procedure will be increasingly advantageous over the $O S\\,O n l y$ approach as more time is spent on test-time search. To validate this hypothesis, we explore how the reconstruction gap between these paradigms changes as a function of time spent on search (Figure 4, left). For 2D CSG we take a subset of the test-set (300 shapes) and run more rounds of our inference algorithm. As demonstrated, as more time is spent on test-time search (i.e. as the number of rounds increases) the reconstruction gap between $O S\\,O n l y$ and $O S+E d i t$ grows wider. Moreover, we note that even on the first round there is a gap between the methods, as the one-shot network trained in the $O S{+}E d i t$ paradigm had access to better $P^{\\mathrm{BEST}}$ entries throughout the finetuning process (i.e. the aforementioned virtuous cycle). We present qualitative results that show how the edit network evolves the population of programs towards the visual target in Figure 5. ", "page_idx": 6}, {"type": "image", "img_path": "uzIWqRzjEP/tmp/da970aa31f37acad13263aa2987ea34f50113fb7b59bb739b127e65219b83351.jpg", "img_caption": ["Figure 4: For 2D CSG, we compare reconstruction accuracy (Chamfer distance, lower is better, Y-axis) between using an edit network and using only a one-shot network while varying time spent on inference $(l e f t)$ and training set size $(r i g h t)$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.4 Training with limited data ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "While both $O S+E d i t$ and OS Only are unsupervised in the sense that they don\u2019t have access to any ground-truth program annotations, they do require an input set of visual data to form a target training set. We hypothesize that our edit network will be especially useful for domains with limited data (even limited unannotated data) as the program editing task is inherently more local than trying to author a complete program. Consider for instance that during finetuning, in a one-shot paradigm each visual datum can only contribute a single training example, while in our paradigm an entire distribution of edit operations can be sourced by considering the many possible edit paths one could take to transform a start program into an end program. We validate this hypothesize with an experiment where we train versions of these systems while varying the size of the target training set (Fig. 4, right). Our joint paradigm offers very strong performance even while finetuning towards an input set of just 100 training shapes, matching the performance of OS Only when it has 10x more data. ", "page_idx": 7}, {"type": "text", "text": "4.5 Method Ablations ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We run an ablation experiment to evaluate the design of our system on the Layout domain. We present results of this experiment in Table 2. In the rest of this section we detail all of the alternative formulations we compare against. ", "page_idx": 7}, {"type": "text", "text": "Edit Operations. Our default edit networks learn how to predict local edit operations from a limited set of options. We compare this paradigm with two alternatives. In the next program mode, we task the edit network with predicting all of the tokens of the program that would be created by applying the target edit operation to the input program. In the final program mode, we task the edit network with predicting the tokens of the final program associated with the visual target. This formulation was inspired by the success of denoising diffusion models for visual synthesis tasks [16], though in our setting this variant is basically an alternative one-shot model with extra conditioning information but with the same target sequences. As demonstrated, neither of these approaches is as performant as our formulation where edits are predicted as local operations. Moreover, predicting an entire program is much slower compared with predicting an edit, so fewer rounds of our inference algorithm can be run with the same search time budget. ", "page_idx": 7}, {"type": "text", "text": "Program Corruption. We source paired training data for our edit network by constructing (start, end) program pairs and then analytically finding a set of edit operations that would complete this transformation. For an alternative, we can look towards discrete diffusion methods [30, 37, 40, 46]. In our corruption variant we take inspiration from these works and design a program corruption algorithm for the Layout domain. This corruption algorithm takes an end program as input, and then samples corruption operations (i.e. inverse edit operations) that can be used as paired data for our edit network (Appendix F). As seen, this alternative formulation was not as performant as our default approach. One reason for this is that it hard to design a corruption process that converts end programs (e.g. $P^{G}$ ) into the distribution of programs that we have access to at inference time (e.g. $\\bar{P}^{S}$ ). Conversely, by applying our findEdits operation on $P^{G}$ and $P^{S}$ pairs, we can source paired data for our edit network that does match this distribution. ", "page_idx": 7}, {"type": "table", "img_path": "uzIWqRzjEP/tmp/f618b11982836191e7ed60a125684f8a381200e66b5396d2ea1cbaf59e587f7a.jpg", "table_caption": ["Table 2: Ablation study comparing our method against alternative formulations. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "uzIWqRzjEP/tmp/0f1702ffe94968e684dcbf2116ab9e591326c5f6aca15bbfc6d8ba2f7c3c68d1.jpg", "img_caption": ["Figure 5: Our inference procedure edits samples from an initial population (top) towards a target (bottom). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Pretraining and Finetuning. In our default version there are three training phases. First, $p(z|x)$ undergoes pretraining on synthetic data. Second, $p(e|z,x)$ undergoes pretraining on synthetic data using samples from $p(z|\\bar{x})$ . Then both of these networks are jointly finetuned with respect to $X^{*}$ . In the No FT variant, we don\u2019t finetune either network, in no one-shot $F T$ we don\u2019t finetune $p(z|x)$ , in no edit $F T$ we don\u2019t finetune $p(e|z,x)$ , and in no edit $P T$ we don\u2019t pretrain $p(e|z,x)$ . While the performance of our system remains remarkable strong even under these ablations, we get the best results by using all three training phases. Interestingly, for settings where $p(z|x)$ is not specialized for $X^{*}$ , the reconstruction accuracy gap dramatically increases between the best sample in the starting population and the best sample in the final population of our inference procedure. For instance, for the no one-shot $F T$ variant, the first round cIoU score is 0.88 which gets increased to 0.972 (0.092 improvement) through the mutations proposed by the edit model, while in our default variant the first round cIoU is 0.925 (an improvement of .055). ", "page_idx": 8}, {"type": "text", "text": "Inference Algorithm. We compare our inference algorithm with two alternative versions. In Naive OS we initialize the first population with $p(z|x)$ , and make edits to each population member with $p(e|z,x)$ , but we skip the population resampling step according to $M$ , and instead apply the highest likelihood edit from $p(e|z,x)$ . While the edit network is still helpful in this paradigm (0.022 improvement from the first to the last round), it performs worse compared with our default implementation. In Rand $+E d i t$ , we remove $p(z|x)$ and instead flil the initial population with random program sampled from $L$ . This provides a much worse initialization (0.302 cIoU in the first round), and though our edit network successfully mutates these samples towards the target, better reconstruction performance is gained by combining our edit network with initial guesses from a one-shot model. ", "page_idx": 8}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We have presented a system that learns how to edit visual programs in a goal-directed fashion. We develop a self-supervised bootstrapping approach that allows us to train an edit network for domains that lack ground-truth program annotations. We compare our proposed paradigm, that jointly finetunes a one-shot model and an edit network, against the alternative of using only a one-shot model, and find that our approach infers more accurate program reconstructions. Further, we find this performance gap is more pronounced when more time is spent on program search or when less training data is available. Finally, we justified the design of our method with an ablation experiment. ", "page_idx": 8}, {"type": "text", "text": "While our proposed approach advances the field of visual program induction, it does come with a few limitations. Compared with prior work, we need to train another network, this impacts the time required for both pretraining and finetuning stages. Moreover, the full benefit of using an edit network is best realized with a more complex program search, and as such we use search-time budgets that are slightly more costly compared with prior work. Though our formulation would offer improved performance for work-flows that can afford to spent more time on program search, it would be useful to consider potential speed-ups of our system [7]. Finally we note that our current formulation requires access to a domain-aware findEdits operation that can analytically find a set of edits that realizes a transformation from a start program to an end program. While we find that our implementation generalizes across a range of visual programming domains, in future work, it would be interesting to consider to what degree this domain-aware procedure could be replaced by more general program difference algorithms [32]. Looking ahead, we believe our framework can serve as inspiration for how to train networks that learn how to edit programs without ground-truth annotations over an even wider array of program synthesis tasks. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We would like to thank the anonymous reviewers for their helpful suggestions. Renderings of 3D shapes were produced using the Blender Cycles renderer. This work was funded in parts by NSF award #1941808 and a Brown University Presidential Fellowship. Daniel Ritchie is an advisor to Geopipe and owns equity in the company. Geopipe is a start-up that is developing 3D technology to build immersive virtual copies of the real world with applications in various fields, including games and architecture. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Autodesk. Fusion 360. https://www.autodesk.com/products/fusion-360/. Accessed: 2022-10-16. [2] Matej Balog, Rishabh Singh, Petros Maniatis, and Charles Sutton. Neural program synthesis with a differentiable fixer, 2020. [3] S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with gpt-4, 2023.   \n[4] Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu. ShapeNet: An Information-Rich 3D Model Repository. arXiv:1512.03012, 2015.   \n[5] Xinyun Chen, Maxwell Lin, Nathanael Sch\u00e4rli, and Denny Zhou. Teaching large language models to self-debug. In The Twelfth International Conference on Learning Representations, 2024. [6] Xinyun Chen, Chang Liu, and Dawn Song. Execution-guided neural program synthesis. In International Conference on Learning Representations, 2019.   \n[7] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memory-efficient exact attention with io-awareness, 2022. [8] Boyang Deng, Sumith Kulal, Zhengyang Dong, Congyue Deng, Yonglong Tian, and Jiajun Wu. Unsupervised learning of shape programs with repeatable implicit parts. In Advances in Neural Information Processing Systems (NeurIPS), 2022.   \n[9] Tao Du, Jeevana Priya Inala, Yewen Pu, Andrew Spielberg, Adriana Schulz, Daniela Rus, Armando Solar-Lezama, and Wojciech Matusik. Inversecsg: automatic conversion of 3D models to csg trees. In Annual Conference on Computer Graphics and Interactive Techniques Asia (SIGGRAPH Asia). ACM, 2018.   \n[10] Kevin Ellis, Maxwell Nye, Yewen Pu, Felix Sosa, Josh Tenenbaum, and Armando Solar-Lezama. Write, execute, assess: Program synthesis with a repl. In Advances in Neural Information Processing Systems (NeurIPS), 2019.   \n[11] Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sabl\u00e9-Meyer, Lucas Morales, Luke Hewitt, Luc Cary, Armando Solar-Lezama, and Joshua B Tenenbaum. DreamCoder: Bootstrapping inductive program synthesis with wake-sleep library learning. In ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software (SIGPLAN), pages 835\u2013850, 2021.   \n[12] Aditya Ganeshan, R. Kenny Jones, and Daniel Ritchie. Improving unsupervised visual program inference with code rewriting families. In Proceedings of the International Conference on Computer Vision (ICCV), 2023.   \n[13] Sumit Gulwani, Oleksandr Polozov, Rishabh Singh, et al. Program synthesis. Foundations and Trends\u00ae in Programming Languages, 4(1-2):1\u2013119, 2017.   \n[14] Kavi Gupta, Peter Ebert Christensen, Xinyun Chen, and Dawn Song. Synthesize, execute and debug: Learning to repair for neural program synthesis. In Advances in Neural Information Processing Systems, 2020.   \n[15] Luke B Hewitt, Tuan Anh Le, and Joshua B Tenenbaum. Learning to learn generative programs with memoised wake-sleep. In Uncertainty in Artificial Intelligence.   \n[16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020.   \n[17] R. Kenny Jones, Theresa Barton, Xianghao Xu, Kai Wang, Ellen Jiang, Paul Guerrero, Niloy J. Mitra, and Daniel Ritchie. Shapeassembly: Learning to generate programs for 3d shape structure synthesis. ACM Transactions on Graphics (TOG), 39(6), 2020.   \n[18] R. Kenny Jones, Siddhartha Chaudhuri, and Daniel Ritchie. Learning to infer generative template programs for visual concepts. In International Conference on Machine Learning (ICML), 2024.   \n[19] R. Kenny Jones, Homer Walke, and Daniel Ritchie. Plad: Learning to infer shape programs with pseudo-labels and approximate distributions. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.   \n[20] Kacper Kania, Maciej Zieba, and Tomasz Kajdanowicz. UCSG-NET - unsupervised discovering of constructive solid geometry tree. In Advances in Neural Information Processing Systems (NeurIPS), volume 33, pages 8776\u20138786, 2020.   \n[21] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015.   \n[22] Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2):83\u201397, 1955.   \n[23] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[24] Ansong Ni, Miltiadis Allamanis, Arman Cohan, Yinlin Deng, Kensen Shi, Charles Sutton, and Pengcheng Yin. Next: Teaching large language models to reason about code execution, 2024.   \n[25] Maxwell Nye, Yewen Pu, Matthew Bowers, Jacob Andreas, Joshua B. Tenenbaum, and Armando Solar-Lezama. Representing partial programs with blended abstract semantics. In International Conference on Learning Representations, 2021.   \n[26] Theo X. Olausson, Jeevana Priya Inala, Chenglong Wang, Jianfeng Gao, and Armando SolarLezama. Is self-repair a silver bullet for code generation? In International Conference on Learning Representations (ICLR), 2024.   \n[27] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In Advances in Neural Information Processing Systems (NeurIPS), 2017.   \n[28] Zeju Qiu, Weiyang Liu, Haiwen Feng, Zhen Liu, Tim Z Xiao, Katherine M Collins, Joshua B Tenenbaum, Adrian Weller, Michael J Black, and Bernhard Sch\u00f6lkopf. Can large language models understand symbolic graphics programs? arXiv preprint arXiv:2408.08313, 2024.   \n[29] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. CoRR, abs/1910.10683, 2019.   \n[30] Machel Reid, Vincent J. Hellendoorn, and Graham Neubig. Diffuser: Discrete diffusion via edit-based reconstruction, 2022.   \n[31] Daxuan Ren, Jianmin Zheng, Jianfei Cai, Jiatong Li, Haiyong Jiang, Zhongang Cai, Junzhe Zhang, Liang Pan, Mingyuan Zhang, Haiyu Zhao, et al. CSG-Stump: A learning friendly CSG-like representation for interpretable shape parsing. In IEEE International Conference on Computer Vision (ICCV), pages 12478\u201312487, 2021.   \n[32] Eduardo Rinaldi, Davide Sforza, and Fabio Pellacini. Nodegit: Diffing and merging node graphs. ACM Trans. Graph., 42(6), dec 2023.   \n[33] Daniel Ritchie, Paul Guerrero, R. Kenny Jones, Niloy J. Mitra, Adriana Schulz, Kar l D. D. Willis, and Jiajun Wu. Neurosymbolic Models for Computer Graphics. Computer Graphics Forum, 2023.   \n[34] Gopal Sharma, Rishabh Goyal, Difan Liu, Evangelos Kalogerakis, and Subhransu Maji. CSGNet: Neural Shape Parser for Constructive Solid Geometry. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.   \n[35] Alexander G Shypula, Aman Madaan, Yimeng Zeng, Uri Alon, Jacob R. Gardner, Yiming Yang, Milad Hashemi, Graham Neubig, Parthasarathy Ranganathan, Osbert Bastani, and Amir Yazdanbakhsh. Learning performance-improving code edits. In The Twelfth International Conference on Learning Representations, 2024.   \n[36] Hao Tang and Kevin Ellis. From perception to programs: regularize, overparameterize, and amortize. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming, MAPS 2022, page 30\u201339, New York, NY, USA, 2022. Association for Computing Machinery.   \n[37] Jiapeng Tang, Yinyu Nie, Lev Markhasin, Angela Dai, Justus Thies, and Matthias Nie\u00dfner. Diffuscene: Denoising diffusion models for generative indoor scene synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024.   \n[38] Yonglong Tian, Andrew Luo, Xingyuan Sun, Kevin Ellis, William T. Freeman, Joshua B. Tenenbaum, and Jiajun Wu. Learning to Infer and Execute 3D Shape Programs. In International Conference on Learning Representations (ICLR), 2019.   \n[39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.   \n[40] Qiuhong Anna Wei, Sijie Ding, Jeong Joon Park, Rahul Sajnani, Adrien Poulenard, Srinath Sridhar, and Leonidas Guibas. Lego-net: Learning regular rearrangements of objects in rooms, 2023.   \n[41] Rundi Wu, Chang Xiao, and Changxi Zheng. DeepCAD: A deep generative network for computer-aided design models. In IEEE International Conference on Computer Vision (ICCV), pages 6772\u20136782, 2021.   \n[42] Xianghao Xu, Wenzhe Peng, Chin-Yi Cheng, Karl D. D. Willis, and Daniel Ritchie. Inferring CAD modeling sequences using zone graphs. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.   \n[43] Xiang Xu, Karl DD Willis, Joseph G Lambourne, Chin-Yi Cheng, Pradeep Kumar Jayaraman, and Yasutaka Furukawa. SkexGen: Autoregressive generation of CAD construction sequences with disentangled codebooks. In International Conference on Machine Learning (ICML), 2022.   \n[44] Fenggen Yu, Qimin Chen, Maham Tanveer, Ali Mahdavi Amiri, and Hao Zhang. $\\mathrm{D}\\mathbb{S}^{\\wedge}2\\mathbb{S}\\mathrm{CSG}$ : Unsupervised learning of compact CSG trees with dual complements and dropouts. In Thirtyseventh Conference on Neural Information Processing Systems, 2023.   \n[45] Fenggen Yu, Zhiqin Chen, Manyi Li, Aditya Sanghi, Hooman Shayani, Ali Mahdavi-Amiri, and Hao Zhang. CAPRI-Net: Learning compact CAD shapes with adaptive primitive assembly. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 11768\u201311778, 2022.   \n[46] Guangcong Zheng, Xianpan Zhou, Xuewei Li, Zhongang Qi, Ying Shan, and Xi Li. Layoutdiffusion: Controllable diffusion model for layout-to-image generation, 2024.   \n[47] Amit Zohar and Lior Wolf. Automatic program synthesis of long programs with a learned garbage collector. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Table 3: We evaluate reconstruction accuracy for \"challenge\" tasks that come from concepts or categories not present in the target training set. For both layout and 3D CSG, we observe that our joint paradigm that integrates an edit network with one-shot models outperforms the alternative of using only one-shot models. ", "page_idx": 12}, {"type": "table", "img_path": "uzIWqRzjEP/tmp/eb500ccc3d1f0e5562e74ddbfbfb908d3dac29b9417094a0636c8d2b8eb33635.jpg", "table_caption": [], "table_footnote": [], "page_idx": 12}, {"type": "image", "img_path": "uzIWqRzjEP/tmp/ed2e17c4bda37df63607fae29c3249440c2690ddf87ef7550bfce69c3ba3d2a2.jpg", "img_caption": ["Figure 6: Qualitative reconstructions of \"challenge\" tasks for 3D CSG. "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "A Appendix Overview ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We overview the contents of our appendices. In section B we include more experimental results. We then provide additional details on our visual programming domains (Section C), on our experimental design (Section D), on our editing operations (Section E), and on our program corruption experiments (Section F). ", "page_idx": 12}, {"type": "text", "text": "B Additional Results ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "B.1 Performance on more challenging tasks ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Our formulation employs a self-supervised finetuning scheme that specializes our inference networks towards a target dataset of interest. But how do our networks fare on visual inputs that are outside of these distributions? For instance, one might hypothesize that the performance gap between our joint paradigm and the one-shot paradigm might shrink when these approaches are given more challenging problems (e.g. when there is a large distribution gap between training and testing data). ", "page_idx": 12}, {"type": "text", "text": "Note though, that as we focus on local edits, our edit networks learn how to solve a local problem: given a current program and some visual target, we task our network with making any edit that would make the current program more similar to the target. Our hypothesis is that this framing should actually scale better than the one-shot networks when the target scenes become more complex or when they are further out-of-distribution from the training data. ", "page_idx": 12}, {"type": "text", "text": "Our intuition here, is that as the task complexity increases, it becomes more likely that the one-shot network will make mistakes. The edit network is able to account for the mistakes of the one-shot network and suggest local fixes that make improvements in a goal-directed fashion. When the target is out-of-distribution, even if the edit network has not seen a similar example, it can still compare the current program\u2019s execution against the target scene. Reasoning over the differences between the two states admits a more local task (as evidenced by our data efficient learning), and this property can aid in generalization. ", "page_idx": 12}, {"type": "text", "text": "To validate the above hypothesis, we set up an experiment to compare how our formulation (which uses a one-shot and edit network jointly) performs against using only the one-shot network for more challenging tasks in the Layout and 3D CSG domains. For the Layout domain, we evaluate the methods on scenes of new \u201cchallenge\u201d concepts (e.g. butterfiles / snowmen) that were not seen in the training / validation sets. For 3D CSG, we evaluate the methods on \u201cchallenge\u201d shapes from other categories of ShapeNet (airplanes, knives, lamps, laptops, motorbikes, mugs, pistols, skateboards, rifles, vessels) that were not part of the original finetuning training set (chairs, tables, benches, couches). ", "page_idx": 12}, {"type": "image", "img_path": "uzIWqRzjEP/tmp/a00dc78d2f21f3d6cffa80f1e2e96c14409f48ee098d6eb29331d46a59704246.jpg", "img_caption": ["Figure 7: Qualitative reconstructions of \"challenge\" tasks for the layout domain. We compare against GPT-4V in a zero-shot setting (column 1), when an in-content example (ICE) is provided in the prompt (column 2), and when the one-shot model\u2019s predicted program is provided as input (column 3). Our approach (column 5) finds more accurate reconstructions of these out-of-distribution targets (column 6) compared with using only the one-shot network (column 4). "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Using the same models from Section 4.2, we compare the reconstruction performance for these challenge tasks. In Table 3, we report the reconstruction performance over 192 challenge tasks for the layout domain and 100 challenge tasks for the 3D CSG domain. As seen from both the quantitative and qualitative comparisons (Figures 6 and 7), it\u2019s clear that our approach, which utilizes both the one-shot and edit networks, outperforms using only the one-shot network for these more challenging program induction tasks, even when they are further outside the training distribution. ", "page_idx": 13}, {"type": "text", "text": "B.2 Comparison to large vision-language models ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "As discussed in Section 2, there has been much recent research that has investigated how LLMs can aid in program synthesis tasks. Relatedly, some works have even begun to examine to what extent large vision-language models are able to understand programs that capture visual data [28]. Following similar ideas, we ran an experiment to explore how well large vision-language models (e.g. GPT-4v) are able to perform on our visual program induction tasks. ", "page_idx": 13}, {"type": "text", "text": "We provide some qualitative results of using GPT-4v to predict visual programs on examples from our layout domain in Figure 7. These predictions were made with a relatively straightforward prompt containing: a task-description, a description of the DSL, and the input image that should be reconstructed (zero-shot, col 1). We then tried improving this prompt by adding an in-context example of a (program, image) pair (one-shot, col 2). We also experimented with providing GPT-4v with a program predicted from the one-shot network, along with this program\u2019s execution, and asking it to edit the program to make it more similar to the target image (col 3). Please find full prompts for these experiments at https://github.com/rkjones4/VPI-Edit . ", "page_idx": 13}, {"type": "text", "text": "As can be seen, GPT-4v in this setting proved inferior to our proposed method (col 5). While we do not include these results to say that these sorts of large vision-language models will not ever be of use for this task, we do believe that these results showcase that this task is not easily solved with currently available frontier models. ", "page_idx": 13}, {"type": "table", "img_path": "uzIWqRzjEP/tmp/5931206b0b8a975e749a20936e21e3ee4c5b94770152f95602da9292e40a03ba.jpg", "table_caption": ["Table 4: Ablation study on our method for the 2D CSG domain. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "B.3 Method Ablations on 2D CSG domain ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In Section 4.5 we presented results for an ablation experiment on the layout domain. We include additional ablation results on the 2D CSG domain in Table 4. Note that while some ablation conditions do come close to our default performance (e.g. no edit $F T$ ) these ablation conditions are also made possible by our contributions, as they all use an edit network. When comparing our method against an alternative without an edit network $(O S\\,O n l y$ , Table 1) we have consistently seen that our method offers a meaningful improvement. Below we offer some additional commentary on these results. ", "page_idx": 14}, {"type": "text", "text": "No edit FT In this ablation condition the edit network is pretrained (with synthetic random data), but is then kept frozen during the joint finetuning. As the task of the edit network is mostly local, we find that the edit network is able to achieve impressive performance even when it does not get to finetune towards data in the target distribution. That said, the edit network is still very important in this ablation condition (if it\u2019s removed then this condition becomes $O S\\,O n l y$ ). Even though the edit network remains fixed during finetuning, it still helps to find better solutions during inner-loop inference (Alg 1, line 5), and this better training data leads to a better one-shot network. However, once again, the performance of the system is maximized when the edit network is also allowed to update during finetuning. ", "page_idx": 14}, {"type": "text", "text": "No one-shot FT This condition does impressively well for the layout domain. This is because even though the one-shot network is much worse in this setting, the edit network can overcome almost all of its mistakes, as layout is a relatively easier domain. Consider that for the layout domain, the default approach has a starting cIoU of 0.925 (initialized from the one-shot network, which is finetuned) which gets improved to 0.980 through improvements made by the edit network. However, the one-shot network of this ablation condition drops the starting cIoU to 0.88 (when it is kept frozen), and yet the edit network is still able to raise this performance all the way to 0.972 (explaining the strong reconstruction score of this condition). That said, when considering the 2D CSG ablation results in Table 4, we see that for more complex domains it is critical to also finetune the one-shot network, as this ablation condition achieves only a Chamfer distance of 0.230 compared with the Chamfer distance of 0.111 achieved by our default approach. ", "page_idx": 14}, {"type": "text", "text": "C Domain Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section we detail the domain-specific language used for each visual programming domain. ", "page_idx": 15}, {"type": "text", "text": "Layout DSL The layout domain creates scenes by placing colored primitives on a 2D canvas, optionally transforming them, and finally combines them together. ", "page_idx": 15}, {"type": "text", "text": "$S T A R T\\rightarrow U B l o c k;$ ;   \n$U B l o c k\\rightarrow\\operatorname{UNION}(S h B l o c k,U B l o c k)~|~S h B l o c k;$   \n$S h B l o c k\\rightarrow(S y m B l o c k\\mid C B l o c k\\mid M B l o c k\\mid S c B l o c k);(P B l o c k\\mid A u S l o c k)$ UBlock) SymBlock \u2212\u2192SymReflect(axis) | SymRotate(n) | SymTranslate(n, x, y) $C B l o c k\\rightarrow\\complement o1o r(c t y p e)$   \n$M B l o c k\\rightarrow\\tt M o v e(x,y)$   \n$S c B l o c k\\rightarrow{\\tt S c a l e}(w,h)$   \n$P B l o c k\\rightarrow\\mathrm{Prim}(p t y p e)$   \n$a x i s\\to X\\ \\mid\\ Y$   \nctype \u2212\u2192red | green | blue   \n$p t y p e\\to s q u a r e\\mid$ circle | triangle   \n$n\\in(1,6)$   \n$x,y,w,h\\in[-1,1]$ ", "page_idx": 15}, {"type": "text", "text": "In this domain, union is the only combinator operation that combines \u2018shape\u2019-typed inputs by layering them on top of one another. SymReflect, SymRotate, SymTranslate, Color, Move, Scale are all transformation operations that consume a single \u2018shape\u2019-typed input and apply some geometric logic to it. Prim is a special command that produces a \u2018shape\u2019-typed output from only a parameter-type argument. ", "page_idx": 15}, {"type": "text", "text": "2D CSG DSL Our 2D constructive solid geometry domain assembles complex shapes using boolean set operations. Following recent work [44] we find it useful to split each program into a set of positive sub-expressions (POS) and negative sub-expressions (NEG). Each sub-expression is allowed to take an arbitrary CSG expression, and then to form the final output all of the positive sub expressions are first unioned together, all of the negative sub expressions are then unioned together, and this second group is differenced out from the first group. This process well-matches typical procedural modeling workflows. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S T A R T\\rightarrow P O S,N E G}\\\\ &{P O S\\rightarrow E,P O S\\mid\\emptyset}\\\\ &{N E G\\rightarrow E,N E G\\mid\\emptyset}\\\\ &{E\\rightarrow B E E\\mid T E\\mid}\\\\ &{B\\rightarrow U n i o n\\mid D i f f e r n c e\\mid I n t e r s e c t i o n}\\\\ &{T\\rightarrow M n e e(F,F)\\mid S c a l e(F,F)\\mid R o t a t e(F)\\mid R e f l e c t(a x i s)}\\\\ &{P\\rightarrow\\mathrm{Prin}(p t y p e)}\\\\ &{p t y p e\\rightarrow s q u a r e\\mid c i r c l e\\mid t r i a n g l e}\\\\ &{a x i s\\rightarrow X\\mid Y}\\\\ &{F\\rightarrow\\left[-1,1\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In this domain, there are three combinator operations that combine multiple \u2018shape\u2019-typed inputs: union, difference and intersection. Move, scale, rotate and reflect are all transformation functions that consume a single \u2018shape\u2019-typed input and apply a geometric modification. Once again, Prim is a special command that produces a \u2018shape\u2019-typed argument from only a parameter-type argument. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S T A R T\\rightarrow P O S,N E G}\\\\ &{P O S\\rightarrow E,P O S\\mid\\theta}\\\\ &{N E G\\rightarrow E,N E G\\mid\\theta}\\\\ &{E\\rightarrow B E E\\mid T E\\mid}\\\\ &{B\\rightarrow U n i o n\\mid D i f f e r n c e\\mid I n t e r s e c t i o n}\\\\ &{T\\rightarrow M n e e(F,F,F)\\mid S c a l e(F,F,F)\\mid R o t a t e(F,F,F)\\mid R e f l e c t(a x i s)}\\\\ &{P\\rightarrow\\mathbf{Prin}(p t y p e)}\\\\ &{p t y p e\\rightarrow c u b o i d\\mid s p h e r e\\mid c y l i n d e r}\\\\ &{a x i s\\rightarrow X\\mid Y\\mid Z}\\\\ &{F\\rightarrow\\left[-1,1\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The split between combinator, transformation and primitive creating functions is the same as in 2D CSG. ", "page_idx": 16}, {"type": "text", "text": "Sampling $L$ As previously discussed, we follow prior work and use a synthetic pretraining phase [12, 19, 34, 38]. In this pretraining phase we randomly sample programs from the above grammars. We employ simple rejection criteria to ensure these random samples are useful (e.g. no execution errors, outputs remain within the canvas, etc.), and find it effective to build in some of this rejection logic during the sampling phase (to improve the speed at which we can sample programs). All of the models we evaluate in our experiments train with the same sampling logic. ", "page_idx": 16}, {"type": "text", "text": "D Experimental Design Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Network details For our 2D domains (2D CSG and Layout) we use a 2D CNN. The image size of both domains is $64\\mathrm{x}64$ , but in 2D CSG there is only one input feature (occupancy) while in Layout there are three channels (RGB). The network we utilize consists of four layers, each containing convolution, ReLU, max-pooling, and dropout operations. Each convolution layer employs a kernel size of 3, a stride of 1, and padding of 1, with channel dimensions of 32, 64, 128, and 256 respectively. The CNN\u2019s output is a $(4\\mathrm{x}4\\mathrm{x}256)$ dimensional vector, which we reshape into a (16x256) vector. This vector is then processed through a 3-layer MLP with ReLU and dropout, resulting in a final (16x256) vector that serves as a 16-token encoding of the visual input. For our 3D CNN model, we adopt a similar convolutional approach by extending all 2D convolutions to 3D. We adjust the kernel size to 4, use padding of size 2. When processing voxel grids of size $32^{3}$ , this produces outputs of size $(2\\mathrm{x}2\\mathrm{x}2\\mathrm{x}256)$ . We pass these outputs through a 3-layer MLP to generate eight 256-dimensional visual tokens. ", "page_idx": 16}, {"type": "text", "text": "Our transformer networks are standard decoder-only variants. We use learned positional encodings and a hidden-dimension size of 256 and dropout of 0.1. We use networks with 8 layers and 16 heads. We set the maximum program sequence length $S L$ to 128, 164, 256 for the Layout, 2D CSG, and 3D CSG domains respectively. We set the maximum edit sequence length $E L$ to 32, 32, 48 for the Layout, 2D CSG, and 3D CSG domains respectively. Each prediction head (edit type, location, parameters) is modeled with a three-layer MLP with a dropout of 0.1. ", "page_idx": 16}, {"type": "text", "text": "Training details We implement all of our networks in PyTorch [27]. All of our experiments are run on NVIDIA GeForce RTX 3090 graphic cards with 24GB of VRAM and consume up to 128GB of RAM (for 3D CSG experiments). We use the Adam optimizer [21] with a learning rate of 1e-4. For $p(z|x)$ pretraining we use a batch size of 128/128/64, for $p(e|z,x)$ pretraining we use a batch size of 128/128/32, for $p(z|x)$ finetuning we use a batch size of 20/20/20, and for $p(e|z,x)$ finetuning we use a batch size of 128/128/32 for Layout / 2D CSG / 3D CSG domains respectively. We pretrain on synthetic programs until convergence with respect to a validation set of synthetic program, for 34 $/\\,17\\,/\\,18$ million iterations, which takes $6\\,/\\,7\\,/\\,7$ days for $p(z|x)$ and $70\\,/\\,30\\,/\\,25$ million iterations, which takes $7\\,/\\,8\\,/\\,8$ days for $p(e|z,x)$ for the Layout, 2D CSG, and 3D CSG domains respectively. We finetune each method for a maximum of 6 days or until convergence, which took $40\\,7\\,40\\,/\\,30$ bootstrap rounds for the Layout, 2D CSG and 3D CSG domains. For each finetuning run we use a $P^{G}$ set of size 10000. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Inference Procedure For our test-time inference program search we use the following population size / number of round parameters for each domain: Layout (32, 32), 2D CSG (32, 32), 3D CSG (80, 25). When using the Os Only method, we keep the same population / mutation general logic, but each mutation is just a randomly sampled program from $\\bar{p(z|x)}$ . In both cases, the best reconstructing program ever seen in any round\u2019s population is returned as the \u2018chosen\u2019 program. The settings for this method are: Layout (32, 10), 2D CSG (32, 10), 3D CSG (25, 25). We set these parameters so that the time spent on inference per shape is even between the two modes (5, 10, 60 seconds for the three domains). For our inner-loop inference step that populates $P^{\\mathrm{BEST}}$ , we use a less expensive search time budget for both modes, approximately taking (2, 5, 10 seconds for each domain respectively). We sample programs from $p(z|x)$ with top-p (.9) nucleus sampling. We sample edits from $p(e|z,x)$ with a beam search of size 3. Interestingly, we found that this sampling strategy for $O s$ Only outperformed a beam-search with a beam size set to the maximum number of tokens in each $L$ . ", "page_idx": 17}, {"type": "text", "text": "E Visual Program Edits ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "E.1 Local Edit Operations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "As described in Section 3, our network predicts local edit operations. We find it useful to constrain the set of possible edit operations as described in Section 4.5. ", "page_idx": 17}, {"type": "text", "text": "In order to use these local edit operations, we require a few properties of the underlying DSL. We require that it is a functional language, where each valid function has a \u2018shape\u2019 return type. Through a slight abuse-of-notation, we refer to functions that implicitly consume a single \u2018shape\u2019-typed argument as transformation functions (e.g. Move), and we refer to functions that consume multiple \u2018shape\u2019- typed arguments as combinator functions (e.g. Union). Note that as described in Section C, there may also be special functions that instantiate \u2018shape\u2019-types from only non-\u2018shape\u2019-typed arguments (e.g. Prim functions). ", "page_idx": 17}, {"type": "text", "text": "Specifically, our formulation allows the network to predict one of the following edit operations: ", "page_idx": 17}, {"type": "text", "text": "\u2022 Modify parameters $(M P)$ : modifies the parameter values of a transform function. Note that this does not modify the function type (unlike MT). Requires additional parameter predictions to set the new values.   \n\u2022 Modify transform (MT): modifies a transformation function, by removing the transform and adding in a new transform with new parameters. Requires additional parameter predictions to set the new function and parameter values.   \n\u2022 Add transform $(A T)$ : adds a transform operator that is applied to the chosen location. Requires additional parameter predictions to specify the new function to be added and its parameters.   \n\u2022 Remove transform $(R T)$ : removes a transform operator and its parameter from the program. Does not require additional parameters   \n\u2022 Modify Combinator (MC): modifies a combinator function (e.g. changing difference to an intersection). Requires additional parameter predictions to set the new function.   \n\u2022 Remove Combinator $(R C)$ : removes a combinator operator (e.g. union) by specifying one input branch of the function to be completed deleted (to all of this sub-expressions leaf nodes).   \n\u2022 Add Combinator $(A C)$ : adds a combinator operator under the chosen transformation. ", "page_idx": 17}, {"type": "text", "text": "Adding a combinator (such as union) requires a sequence of additional predictions to flil in one of the \u2018shape\u2019-typed branches of this operator that was not previously in the program. ", "page_idx": 17}, {"type": "text", "text": "We once again note that each of these edit operations has a local effect. For instance, as depicted in Figure 1 adding a new transform function inserts a transform node into an already existing tree of functions. Similarly, removing a transform functions simply results in forming a skip connection from the chosen operator\u2019s parent function to the chosen operator\u2019s child function. Somewhat more arbitrary changes can be enacted by removing or adding combinators, in order to produce or remove entire expression trees, though these are inserted or removed from specific locations. While this framing does focus on local edits, and as such our edit network makes local changes in program space, some of these changes can have dramatic effects in the execution space. For instance, consider changing a boolean operation type in CSG from difference to union. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "E.2 findEdits Algorithm ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Given a starting program and an end program we develop an algorithm that analytically finds a set of edit operations that would transform the starting program into the end program. This algorithm is used to source data for the edit network, as we describe in the next section. ", "page_idx": 18}, {"type": "text", "text": "We design our findEdits algorithm to try to find the \u201cminimal cost\u201d set of edit operations that would transform a start program to an end program. Our instantiation of the algorithm works over multiple visual programming domains for the set of edit operations we consider. However, there are many alternative ways this algorithm could be instantiated, and such alterations could prove useful in helping our method adapt for very different domains. As one extreme point, consider that for general purpose programming languages, a simple \u201cgit-diff\u201d command could be used to turn a (start, end) program pair into a set of local insert/keep/delete edits. ", "page_idx": 18}, {"type": "text", "text": "Our implementation evaluates valid transformations in terms of program semantics (e.g. executions) not just syntax (e.g. token matching), as there are many distinct programs in our domains that will produce equivalent execution outputs (e.g. note that the argument ordering of union for CSG languages does not change the execution output). We hypothesize that using a findEdit algorithm alternative that does not consider such \"semantic-equivalences\" would result in a \u201cworse\u201d edit network (as the patterns in the training data would be less consistent), but it would be interesting to explore how different algorithms would effect system performance in future work. ", "page_idx": 18}, {"type": "text", "text": "There are two main steps to this algorithm. First considering two sub-expressions $a$ and $b$ , we need to find an approximately minimal set of edit operations such that applying these edit operations to $a$ would recreate the visual output of $b$ . With this logic in hand, we can consider two entire programs $A$ and $B$ , split them into a set of sub-expressions, $\\mathbf{\\bar{A}}=\\{a_{0},...,a_{k}\\}$ and $B=\\{b_{0},...,b_{m}\\}$ , and then solve a matching problem to see how we should match each $a_{i}$ to each $b_{j}$ while accounting for domain-specific ordering requirements. ", "page_idx": 18}, {"type": "text", "text": "Finding edits for sub-expressions Given two sub-expression $a$ and $b$ from one of our DSLs, we find a set of edit operations to convert $a$ to $b$ with the following recursive logic. If $a$ and $b$ have no combinator operators or order-dependant transformation functions (e.g. symmetry operations) then we can simply compare the transform functions and their arguments to see which transforms in $a$ need to be modified, added, or removed. If both $a$ and $b$ have a combinator operation, then we recurse this match on the respective sub-programs. If only $a$ has a combinator operation, we know that we need to remove one of $a$ \u2019s expression trees, so we check which of the combinator\u2019s input expression trees has the better match towards $b$ . If only $b$ has a combinator operation, we know that we need to add an expression tree into $a$ with an $A C$ edit operation. The cost of this edit operation is just the length of all of the tokens of that expression tree; we evaluate the match between $a$ and each of the sub-expression within $b$ to determine which sub-expression to add with the edit operation. Any time an order dependant transform function differs between $a$ and $b$ we will either need to add, remove, or modify this transform. Note that this type of edit operation may also introduce ordering dependencies for later edit operations (which we keep track of). ", "page_idx": 18}, {"type": "text", "text": "Finding a minimal matching From the above procedure we know the edit operations and the edit cost of transforming any sub-expression $a$ into another sub-expression $b$ . We design our DSLs so that it is possible to break each program into a series of sub-expressions. For Layout this is done by splitting the top-level UBlock into the top-level ShBlocks. For CSG this is done by splitting each $P O S$ block into $E$ blocks and each $N E G$ block into $E$ blocks. Note that there is some order dependency in this match: for CSG positive sub-expressions must be matched to other positive sub-expressions, while negative sub-expressions must be matched to other negative sub-expressions. For the Layout domain, Union is not an order invariant operator as it controls how primitives are layered on the canvas. Therefore we keep the order of Layout sub-expressions fixed, although we allow each sub-expression to optionally match to an empty sub-expression $\\varnothing$ . A match from $a_{i}$ to $\\varnothing$ implies that $a_{i}$ will be removed with a $R C$ edit operation, while a match from $\\varnothing$ to $b_{i}$ implies that $b_{i}$ will be added with a $A C$ edit operation. We consider all valid possible ways to enact this matching by calculating the cost of each sub-expression match and then extracting out a solution with the Hungarian matching algorithm [22]. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "E.3 Converting edits operations to training data ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "From the above logic we find a set of edit operations $E S$ given input programs $A$ and $B$ . As mentioned, while there may be some ordering dependencies in this set that we keep track of (e.g. adding a transform on top of newly added combinator function) this set of edit operations can be otherwise ordered arbitrarily. While many formulations are possible here we choose to convert this set into paired data for our edit network with the following procedure. ", "page_idx": 19}, {"type": "text", "text": "Say $E S$ contains $n$ independent edits. For each $i$ starting at 0 and ending at $n-1$ we first consider all possible ways that we could have chosen $i$ edits from $E S$ . To avoid exponential blow-up, we sub-sample from this set, and choose 5 previous edit sets for each $i$ . Then for each set of previous edits $p e_{i}$ , for each next edit $e\\in E S$ and $e\\not\\in p e_{i}$ , we add the following triplet to the training data for our edit network: the input program is $p e_{i}(A)$ , the target visual target is $E(B)$ , and the target edit operation is $e$ . ", "page_idx": 19}, {"type": "text", "text": "E.4 Generality of our framing ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "While we designed our edit operations with the task of visual program induction in mind, we believe that these operations are quite flexible. Many other functional DSLs for visual programs (and for other program synthesis tasks) could likely be subsumed directly under our framework, as long as these languages meet the criteria described in Section E.1. For instance, this set of edit operations should be able to handle any DSL expressible as a Context Free Grammar. ", "page_idx": 19}, {"type": "text", "text": "Under these assumptions, the edit operations we use are quite basic and make limited domain assumptions. For an input functional program, edits to transform functions allow for local edits (delete/insert/modify) that don\u2019t affect the branching factor, while edits to combinator functions allow for local edits (delete/insert) that do affect the branching factor. We employ this formulation for a few reasons: (1) it is general enough to support any program-program transformation (under our assumption set) and (2) applying any of these local edits creates a syntactically complete program that can be immediately executed. ", "page_idx": 19}, {"type": "text", "text": "That said, our framework and core contributions are not tied to this specific set of edit operations. Our edit network and proposed training scheme could be easily integrated with any set of local edit operations (assuming an analogous findEdits algorithm can be designed for this new set of edits). So while we believe that the set of edit operations we introduce is quite general (as evidenced by their usefulness across multiple difficult visual programming domains), we are also excited to see how our general learning-to-edit framework could be extended to even more complex DSLs and edit operations. ", "page_idx": 19}, {"type": "text", "text": "F Program Corruption ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "As we mention in Section 4.5 there are some high-level connections between the formulation we propose and discrete diffusion models: both do iterative error-correction and learn in a self-supervised manner to \u2018fix\u2019 incorrect targets. To this end, we explored alternative formulations that \u2018corrupted\u2019 programs. As we wanted to maintain the property that each intermediate step of the \u2018corruption\u2019 process is a valid program (e.g. it would not cause an executor error) we designed a domain-specific corruption process for our Layout domain. Unlike unconditional generative diffusion models that need to have strict requirements about the distribution they noise towards, we did not find this necessary in our setting as our iterative error-correcting framing is explicitly goal-directed in the form of a visual target. Specifically, our corruption process starts with an \u2018end\u2019 program and randomly samples \u2018inverse\u2019 edit operations for a random number of corruption steps. We then replace our findEdits step in Algorithm 1 with this corruption logic, where the start program is ignored. ", "page_idx": 19}, {"type": "text", "text": "While this variant is not as a performant as our default version, it still sources useful training data for our edit network. Our view is that, when possible, it is better to source these edit operations by considering start program and end program pairs, but for domains where such edit difference scripts are hard to analytically find, this corruption variant offers an alternative. While its possible that better corruption processes could close this gap, designing them is non-trivial. From one perspective, when we want to combine one-shot models and edit networks at inference time, the corruption behavior we want should noise \u2018end\u2019 programs towards those produced by the one-shot model \u2013 this is exactly the distribution we get access to with the findEdits approach that considers program-to-program transformations. Another benefit of this formulation, is that the distribution of edit operations we train over is naturally allowed to evolve and keeps in sync automatically with the finetuned one-shot model. Maintaining this property with a corruption-based procedure would likely be impractical. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We support out two contributions as laid out at the end of Section 1 with a range of experimental evidence. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Yes we discuss limitations of our approach in Section 5. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We provide the necessary algorithm to recreate our experiments in the main paper and provide additional information in the Appendices. Further, we have released code of the system to aid in reproducibility. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: Yes. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We provide this information in Section 4.1 and the Appendices. Further, this information is available with the released code. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [No] ", "page_idx": 21}, {"type": "text", "text": "Justification: It was not feasible for us to get access to enough compute to calculate meaningful error bars. We do analyze multiple versions of our algorithm across different visual programming domains, and different amounts of training data, and find consistency in our results. ", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We provide this information in the Appendix. ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We conform with the code of ethics. ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: We do not foresee any significant amount of direct societal impact as a result of this work. ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] Justification: We do not forsee any such risks. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have properly cited and referenced all relevant assets. ", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] Justification: We do not release new assets. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our paper does not involve human subjects. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our paper does not involve human subjects. ", "page_idx": 22}]