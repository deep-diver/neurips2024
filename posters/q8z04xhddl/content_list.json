[{"type": "text", "text": "MoE Jetpack: From Dense Checkpoints to Adaptive Mixture of Experts for Vision Tasks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xingkui Zhu\u2217 Yiran Guan\u2217 Dingkang Liang Yuchao Chen Yuliang Liu\u2020 Xiang Bai ", "page_idx": 0}, {"type": "text", "text": "Huazhong University of Science and Technology {adlith, yiranguan, dkliang, ylliu, xbai}@hust.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The sparsely activated mixture of experts (MoE) model presents an effective alternative to densely activated (dense) models, combining improved accuracy with computational efficiency. However, training MoE models from scratch requires extensive data and computational resources, a challenge that limits their widespread adoption. To address this, we introduce MoE Jetpack, a framework designed to fine-tune the abundant and easily accessible dense checkpoints into MoE models. MoE Jetpack incorporates two key techniques: (1) checkpoint recycling, which initializes MoE models with dense checkpoints to accelerate convergence and enhance accuracy, minimizing the need for extensive pre-training; (2) the hyperspherical adaptive MoE (SpheroMoE) layer, which optimizes the MoE architecture to enhance fine-tuning performance and efficiency. Experimental results indicate that MoE Jetpack doubles the convergence speed and enhances accuracy by $2.8\\%$ on ImageNet-1K. On smaller datasets, it achieves up to 8-fold faster convergence and over $30\\%$ accuracy gains, highlighting its efficiency. The code is available at https://github.com/Adlith/MoE-Jetpack. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Increasing model scale is a key factor in boosting deep learning performance [1, 2, 3]. However, as models expand in size, their computational demands surge, resulting in considerable slowdowns during both training and inference phases. A promising approach that decouples model size from computational costs is the sparsely activated mixture of experts (MoE) [4, 5, 6]. Unlike densely activated models (referred to as dense models hereafter) [7, 8] that apply all network parameters to each input, MoE dynamically activates distinct parts of the model based on the input tokens. This allows for model scaling without substantially increasing the $\\mathrm{FLOPs}^{2}$ , thereby maintaining training and inference speeds during model upscaling. Recent advancements have seen successful implementations of MoE across various domains [11, 12, 13]. ", "page_idx": 0}, {"type": "text", "text": "Despite their potential, MoE models face significant adoption challenges primarily due to the lack of pre-trained models. Unlike dense models, which benefit from a vast collection of pretrained resources available through platforms such as Hugging Face [14] and Timm [15], most MoE models must be trained from scratch with randomly initialized weights. This process demands substantial computational power and large datasets, limiting MoE research to a select few teams with the necessary resources. Consequently, our research aims to reduce the training time and data requirements for MoE models by leveraging the pre-trained knowledge from dense checkpoints. We will specifically investigate whether utilizing dense checkpoints can enhance the accuracy and convergence speed of MoE models during fine-tuning. ", "page_idx": 0}, {"type": "image", "img_path": "Q8Z04XhDdL/tmp/faac32785aa685def48f5bb8dac22e2af0dda3eab61e88f2f45fca3fc8ed2885.jpg", "img_caption": ["Figure 1: (a) MoE Jetpack converts dense checkpoints into initialization weights for MoE models, facilitating faster convergence and improved performance while maintaining equivalent FLOPs. Here, Exp. represents individual experts, $E$ denotes the number of experts, and $L$ indicates the total number of layers. (b) Performance comparison among ViT trained from scratch, pre-trained and fine-tuned ViT, Soft MoE [6] trained from scratch, and MoE Jetpack across multiple datasets. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose MoE Jetpack, a new approach for fine-tuning pre-trained dense checkpoints into MoE models. As illustrated in Fig. 1(a), MoE Jetpack leverages the sunk cost of dense pre-training to boost MoE performance and expedite convergence. It comprises two key techniques. Firstly, checkpoint recycling is used to initialize MoE models from dense checkpoints. Unlike sparse upcycling [16], which simply duplicates the Multilayer Perceptron (MLP) to create experts, checkpoint recycling utilizes diverse dense checkpoints and strategic weight selection, providing flexibility and generating higher-quality initialization weights for MoE models. The second technique is the hyperspherical adaptive MoE (SpheroMoE) layer, which presents an optimized MoE architecture that facilitates the seamless integration of dense checkpoints and enhances fine-tuning performance. Existing MoE architectures, such as Switch Transformers [4] and Soft MoE [6], are not designed to incorporate pre-trained dense checkpoints, often resulting in optimization inefficiencies and over-specialization during fine-tuning. The SpheroMoE layer mitigates these challenges by normalized token mixing, expert regularization, and adaptive dual-path mechanism, ensuring smoother integration and improved performance. ", "page_idx": 1}, {"type": "text", "text": "By equipping dense checkpoints with MoE Jetpack, as illustrated in Fig. 1(b), the fine-tuned MoE models achieve significantly higher accuracy. Comprehensive evaluations across various image classification datasets of different scales further demonstrate the effectiveness of MoE Jetpack. In summary, our contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce checkpoint recycling, which pioneers the sampling of dense checkpoints to initialize MoE experts, enhancing initialization flexibility, diversifying experts, and eliminating the computational burden of MoE pre-training. \u2022 We develop the spheroMoE layer, optimized for fine-tuning dense checkpoints into MoE architectures, alleviating optimization challenges, and preventing the over-specialization of experts. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we recap the core concepts needed to understand the MoE Jetpack, specifically focusing on the sparsely activated mixture of experts (MoE) and the routing mechanisms. Existing MoE models are typically derived from dense Vision Transformer (ViT) by replacing certain multilayer perceptron (MLP) layers with MoE layers. Each MoE layer consists of a Router function, $\\mathrm{Router}(x;\\theta_{g a t e})$ , which directs input tokens to a set of \"experts\", where each expert is parameterized as $\\mathrm{MLP}(\\cdot;\\theta_{i})$ . Although the experts in MoE models are typically similar, the routing mechanisms vary and are critical to performance. Several routing algorithms have been developed, including top- ${\\cdot k}$ [17], BASE and Sinkhorn-BASE layers [18, 19], Hash layers [20], Expert Choice routing [21], and soft routing [6]. ", "page_idx": 1}, {"type": "text", "text": "The most commonly used mechanism is top- $k$ routing, which reduces computational overhead by selectively activating only the top- ${\\cdot k}$ experts most relevant to the input tokens. The routing decision is computed as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{Router}(x;\\theta_{g a t e})=\\mathrm{top}{\\ \u2013k}\\left(\\mathrm{softmax}(\\mathrm{MLP}(x;\\theta_{g a t e}))\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "and the output is aggregated by combining the contributions of the selected experts: ", "page_idx": 2}, {"type": "equation", "text": "$$\ny=x+\\sum_{i\\in E}\\mathrm{Router}(x;\\theta_{g a t e})\\cdot\\mathrm{Expert}(x;\\theta_{i}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\theta$ denotes the weights, $E$ is the set of activated experts, and $|E|=K$ . However, top- $k$ routing presents challenges such as imbalanced expert utilization, token dropping, and scalability issues. ", "page_idx": 2}, {"type": "text", "text": "A more balanced and representative approach, Soft MoE [6], effectively addresses these challenges through implicit soft assignments and serves as our baseline. Instead of assigning tokens to specific experts, Soft MoE computes weighted combinations of all input tokens for each expert. Given the input tokens $\\mathbf{X}\\in\\mathbb{R}^{m\\times d}$ , where $m$ is the number of tokens and $d$ is their dimensionality, the learnable matrix $\\Phi\\in\\mathbb{R}^{d\\times(e\\cdot s)}$ project the tokens into $e\\times s$ slots, where $e$ is the number of experts and $s$ is the number of slots per expert. The projection is calculated as $\\tilde{\\mathbf{X}}=\\mathrm{softmax}(\\mathbf{X}\\boldsymbol{\\Phi})^{\\top}\\mathbf{X}$ . Here, $\\tilde{\\mathbf{X}}\\in\\mathbb{R}^{(e\\cdot s)\\times d}$ represents the transformed inputs that are weighted combinations of X. Each MoE layer includes $e$ expert functions $\\{f_{i}:\\mathbb{R}^{d}\\stackrel{}{\\rightarrow}\\mathbb{R}^{d}\\}_{i=1}^{e}$ , with each expert handling $s$ slots. The intermediate outputs $\\tilde{\\mathbf{Y}}\\in\\mathbb{R}^{(e\\cdot s)\\times d}$ are obtained by applying the experts to the slots: $\\tilde{\\mathbf{Y}}_{i,j}=f_{i}(\\tilde{\\mathbf{X}}_{i,j})$ for $i\\in\\{1,\\ldots,e\\}$ and $j\\in\\{1,\\ldots,s\\}$ . Finally, the output tokens $\\mathbf{Y}\\in\\mathbb{R}^{m\\times d}$ are reconstructed by combining the expert outputs: $\\mathbf{Y}=\\mathrm{softmax}(\\mathbf{X}\\Phi)\\tilde{\\mathbf{Y}}$ . ", "page_idx": 2}, {"type": "text", "text": "3 MoE Jetpack ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we present the overarching concept of the MoE Jetpack. It is divided into two phases: checkpoint recycling dense checkpoints to initialize MoE models and fine-tuning MoE models using the hyperspherical adaptive MoE (SpheroMoE) layer. ", "page_idx": 2}, {"type": "text", "text": "3.1 Checkpoint Recycling ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Checkpoint recycling is a foundational phase in the MoE Jetpack framework, transforming pretrained dense checkpoints (predecessors) into high-quality initialization weights (successors) for MoE models. This approach leverages the rich pre-trained knowledge from predecessors through weight reuse, boosting the performance and convergence speed of successors. The recycling procedure involves sampling a portion of the weights from the predecessors\u2019 multilayer perceptrons (MLPs) to construct experts, ensuring expert diversity and adaptability in expert size to meet varied needs. ", "page_idx": 2}, {"type": "text", "text": "To define the process of checkpoint recycling (as illustrated in Fig. 2(a)), consider predecessors with $N$ layers, where each layer $L_{i}$ has a feature dimension of $d$ and a hidden dimension of $4d$ . The object is to transform this predecessor into a successor MoE model $S$ , which also has $N$ layers, each denoted as $L_{i}^{\\prime}$ , but with a reduced feature dimension $d^{\\prime}\\leq d$ . Following the Soft MoE architecture [6], the successor comprises two segments: a dense part with $N_{1}$ layers and an MoE part with $N_{2}$ layers, where $\\begin{array}{r}{N_{1}=N_{2}=\\frac{N}{2}}\\end{array}$ . Formally, the successor model is represented as: ", "page_idx": 2}, {"type": "equation", "text": "$$\nS=\\left(\\{L_{i}^{\\prime}\\}_{i=0}^{\\frac{N}{2}-1},\\{L_{i}^{\\prime}\\}_{i=\\frac{N}{2}}^{N-1}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Inspired by Weight Selection [22], our recycling process ensures consistency in feature dimensions. We explore four primary strategies to guide the recycling of checkpoints: ", "page_idx": 2}, {"type": "text", "text": "Importance-Based Weight Sampling (default): We select weights across hidden units and feature dimensions to construct initialization weights for diverse experts. For feature dimension selection, maintaining consistency across all layers is essential [22]. We achieve this by calculating the mean output features across the $N$ layers and selecting the top- $d^{\\prime}$ dimensions based on these averages: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\bar{O}=\\frac{1}{N}\\sum_{i=1}^{N}O_{i},\\quad\\mathrm{top}-d^{\\prime}=\\mathrm{argsort}(\\bar{O})[:d^{\\prime}]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "image", "img_path": "Q8Z04XhDdL/tmp/1a4f97996d8ae56bb66958cf30523cc75d75e85bf5085a4931edd78312560378.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: (a) Checkpoint Recycling: Selects hidden units and dimensions from the dense checkpoint MLP, transforming pre-trained weights into diverse experts to initialize MoE models. (b) SpheroMoE Layer: Uses cross-attention to dispatch tokens to slots and combine slots back into tokens. Input tokens are projected as keys, and a random query is initialized to compute token-slot similarity in hyperspherical space. Experts then process their assigned slots. ", "page_idx": 3}, {"type": "text", "text": "where $O_{i}$ is the output features of layer $L_{i}$ , and $\\bar{O}\\in\\mathbb{R}^{n\\times d}$ is the average output features. For hidden units, sampling is performed independently for each layer based on the magnitude of activations, as the arrangement of units does not impact the output. Activation values from batches of images are used to form a probability distribution, and units are sampled accordingly: ", "page_idx": 3}, {"type": "equation", "text": "$$\nP(h|H)=\\frac{A_{h}}{\\sum_{h^{\\prime}\\in H}A_{h^{\\prime}}},\\quad h_{\\mathrm{successor}}\\sim P(h|H),\\quad|h_{\\mathrm{successor}}|=4d^{\\prime},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $A_{h}$ is the activation value of hidden units $h$ , and $H$ is the set of all hidden units. This method selects the most important weights for the successor model while promoting diversity among experts through the sampling process. ", "page_idx": 3}, {"type": "text", "text": "Co-Activation Graph Partitioning: This strategy groups frequently co-activated hidden units into one expert. We construct a co-activation graph by counting the co-activations of hidden units in the testing procedure. Each unit is a vertex in the co-activation graph, and the edges represent their co-activation frequency. Formally, let $G=(V,E)$ be the co-activation graph, where $V$ represents the hidden units and $E$ represents edges with weights indicating co-activation counts. Using the Metis graph partitioning [23], we get several subgraphs: ", "page_idx": 3}, {"type": "equation", "text": "$$\nG=\\bigcup_{i=1}^{k}G_{i},\\quad G_{i}=(V_{i},E_{i}),\\quad V_{i}\\cap V_{j}=\\emptyset\\;{\\mathrm{for}}\\;i\\neq j.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Experts are formed by the combination of sub-graphs. This method leverages the natural grouping of hidden units, ensuring each expert captures a specific functional subset of the predecessor model. ", "page_idx": 3}, {"type": "text", "text": "Uniform Weight Selection: Weights are selected uniformly across dimensions and hidden units. For a predecessor with feature dimension $d$ and a successor with dimension $d^{\\prime}$ , weights are chosen as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nW_{\\mathrm{successor}}^{(i)}=W_{\\mathrm{predecessor}}^{(k)},\\quad k=\\left\\lfloor\\frac{i\\cdot d}{d^{\\prime}}\\right\\rfloor,\\quad i\\in\\{0,\\ldots,d^{\\prime}-1\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This method ensures an even distribution of the pre-trained weights across the successor MoE. ", "page_idx": 3}, {"type": "text", "text": "Random Weight Sampling: Weights are randomly selected from the predecessor model. Let $S$ be a random subset of feature dimension indices: ", "page_idx": 3}, {"type": "equation", "text": "$$\nS\\subseteq0,\\ldots,d-1,\\quad|S|=d^{\\prime}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Then, the weights for the successor are chosen as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nW_{\\mathrm{successor}}^{(i)}=W_{\\mathrm{predecessor}}^{(j)},\\quad j\\in S,\\quad i\\in\\{0,\\ldots,d^{\\prime}-1\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Through the ablation in Sec. 4.3, Importance-Based Weight Sampling is identified as the default method for recycling dense checkpoints to initialize MoE models. ", "page_idx": 4}, {"type": "text", "text": "Notably, the computational overhead introduced by Checkpoint Recycling is virtually negligible. Methods such as Random Sampling and Uniform Selection operate with minimal additional processing, as they directly select experts from the dense checkpoint without further computations. While Graph Partitioning and Importance-Based Sampling involve a preliminary inference step to determine neuron importance or co-activation patterns, the time required is minimal. For instance, performing inference on a subset of 30, 000 images with an RTX 4090 takes less than 5 minutes. This efficient process ensures that Checkpoint Recycling significantly enhances model initialization while introducing almost no additional overhead. ", "page_idx": 4}, {"type": "text", "text": "3.2 SpheroMoE Layer ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Following the initialization of MoE weights through Checkpoint Recycling, the next step is fine-tuning on downstream datasets. To enhance performance and stability, we designed the hyperspherical adaptive MoE (SpheroMoE) layer (Fig. 2(b)), introducing three key improvements: SpheroMoE Routing to alleviate optimization challenges, Expert Regularization to prevent over-specialization, and Adaptive Dual-path MoE (Fig. 3) for better performance and efficiency. Additionally, the pseudo-code detailing these features\u2019 implementation can be found in Appendix C. ", "page_idx": 4}, {"type": "text", "text": "SpheroMoE Routing: As shown in Fig. 2(b), the proposed hyperspherical MoE (SpheroMoE) routing mechanism utilizes cross-attention [24] to distribute inputs across experts. Each expert receives an input slot, a weighted average of all input tokens. To maintain consistency between dense checkpoints and MoE layers $M\\,=\\,\\dot{\\{L_{i}^{\\prime}\\}}_{i=N/2}^{N-1}$ , input tokens $\\mathbf{X}\\,\\in\\,\\mathbb{R}^{b\\times n\\times d}$ (where $b$ represents the batch size, $n$ represents the token length, and $d$ represents the input dimension) are layer normalized inherited from dense checkpoints, resulting in $\\mathbf{X}_{\\mathrm{norm}}$ . Queries $\\mathbf{Q}\\,\\in\\,\\mathbb{R}^{b\\times(e\\times s)\\times d}$ are randomly initialized and similarly normalized to align with $\\mathbf{X}_{\\mathrm{norm}}$ , producing $\\mathbf{Q}_{\\mathrm{norm}}$ . The layer normalization process ensures the consistency of distributions between the MoE model, input queries, and the pre-trained dense model. The normalized $\\mathbf{X}_{\\mathrm{norm}}$ are projected to form keys $\\mathbf{K}\\in\\mathbb{R}^{b\\times n\\times d}$ for the cross-attention mechanism. ", "page_idx": 4}, {"type": "image", "img_path": "Q8Z04XhDdL/tmp/c4a3b2362b0165c229594e67c0af82a0d65e00b6706a040502cd6c26e0e4e216.jpg", "img_caption": ["Figure 3: The Adaptive Dual-path MoE structure enhances the SpheroMoE Router by adapting it into a dual-branch system, designed to optimize computational efficiency and model performance. This configuration directs high-impact tokens to a core path with fewer but larger experts while routing less critical tokens to a universal path equipped with a greater number of smaller experts. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "To reduce numerical instability, $\\mathbf{Q}_{\\mathrm{norm}}$ and $\\mathbf{K}$ are projected onto a hyperspherical space using L2 normalization, ensuring that the resulting dot products reflect cosine similarities rather than unbounded magnitudes. This confines values within a stable range, preventing softmax saturation and enabling more balanced attention distributions, which improves model generalization. The similarity between $\\mathbf{Q}_{\\mathrm{norm}}$ and $\\mathbf{K}$ is computed, yielding similarity logits $\\mathbf{S}\\in\\mathbb{R}^{b\\times(e\\times s)\\times n}$ : $\\mathbf{S}=\\mathbf{Q}_{\\mathrm{norm}}\\mathbf{K}^{T}$ . Input slots $\\tilde{\\mathbf{X}}\\in\\mathbb{R}^{b\\times(e\\times s)\\times d}$ for experts are formed by a softmax operation along the $n$ dimension of $\\mathbf{S}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{X}}=\\frac{\\exp(\\mathbf{S}_{i j k})}{\\sum_{k^{\\prime}=1}^{n}\\exp(\\mathbf{S}_{i j k^{\\prime}})}\\mathbf{X}_{\\mathrm{norm}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Each expert processes its corresponding input slots $\\tilde{\\mathbf{X}_{i}}$ independently, generating outputs $\\tilde{\\mathbf{Y}_{i}}$ . These outputs are then weighted by $\\mathbf{S}$ (after applying a softmax operation along the $(e\\times s)$ dimension) to aggregate the experts\u2019 contributions, producing the final output $\\mathbf{Y}\\in\\mathbb{R}^{b\\times n\\times d}$ of the MoE layer: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{Y}=\\frac{\\exp\\left(\\mathbf{S}_{i j k}\\right)}{\\sum_{j^{\\prime}=1}^{e\\times s}\\exp\\left(\\mathbf{S}_{i j^{\\prime}k}\\right)}\\tilde{\\mathbf{Y}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In summary, SpheroMoE routing leverages layer normalization, hyperspherical projection, and crossattention to effectively distribute inputs across experts, ensuring numerical stability and consistency with pre-trained dense models for improved optimization. ", "page_idx": 5}, {"type": "text", "text": "Expert Regularization: To improve generalization, SpheroMoE regularizes routing and expert behavior, preventing experts from over-specializing on specific inputs or outputs from depending excessively on certain experts. For the former, we introduce learnable softmax temperatures $T_{d i s p a t c h}$ and $T_{c o m b i n e}$ to precisely control token dispatch and slot combination, enabling smooth transitions between broad and focused attention. Since token dispatch distributes tokens across slots, while slot combination aggregates slot outputs back into tokens, these dual temperatures provide flexible control to each process. Both temperatures are initialized high to promote a broad distribution of attention, preventing early specialization. As the training process, these temperatures adaptively decrease, allowing experts to focus more precisely on relevant features and specialize where advantageous. Additionally, we added a certain level of normal noise to the similarity logits S, which improves generalization. For the latter, we utilized stochastic expert dropout, where each expert $i$ is randomly deactivated with a probability $p$ . It ensures that no single expert becomes a crutch for the entire output, promoting a more balanced utilization of all experts. These techniques form an expert regularization strategy that maintains expert versatility and mitigates over-ftiting, ensuring the MoE model performs robustly on downstream datasets. ", "page_idx": 5}, {"type": "text", "text": "Adaptive Dual-path MoE: To mitigate computational redundancy for less critical tokens and concentrate resources on essential ones, SpheroMoE Routing directs input tokens into core and universal slots based on importance. Building on this, the Adaptive Dual-Path structure is designed to assign each slot type to a distinct pathway for optimized processing, as illustrated in Fig. 3. The core pathway consists of a limited number of core experts with larger parameter counts, specifically configured for processing high-importance tokens. In contrast, the universal pathway comprises a larger set of small experts, each with one-fourth of the parameters of core experts, optimized for handling less critical slots. This dual-path configuration improves resource allocation by focusing computation on key tokens, thereby preserving model accuracy and enhancing processing efficiency. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Experimental Setups ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Models. We validate our approach using the Vision Transformer (ViT)[7] and ConvNeXt[8] models. Specifically, we initialize the weight of V-JetMoE-T and C-JetMoE-F by transforming the dense checkpoints of ViT-S and ConvNeXt-T (pre-trained on ImageNet-21K and sourced from timm) through checkpoint recycling. As detailed in Sec. 3.1, V-JetMoE-T retains the dense layer structure of ViT-T in its first half, while the latter half is equipped with SpheroMoE layers. Each SpheroMoE layer consists of $N/2$ core experts and $N$ universal experts, where $N$ is the number of input tokens. Further details are in Appendix A. ", "page_idx": 5}, {"type": "text", "text": "Datasets. We evaluate MoE Jetpack on 8 image classification datasets, including ImageNet-1K [25], CIFAR-10, CIFAR-100 [26], Flowers [27], Pets [28], STL-10 [29], Food-101 [30], and DTD [31]. These datasets encompass a diverse range of classification challenges, including object classification, fine-grained species recognition, and texture classification. ", "page_idx": 5}, {"type": "text", "text": "Baseline Implementation. We follow the implementation details outlined by Xu et al. [22] for comparisons of the dense models. For the MoE models, we employ Soft MoE [6] as the baseline and have replicated it across all datasets. Our MoE Jetpack and Soft MoE utilize the same training strategies as the dense models to ensure comparison fairness. All implementations were executed using the MMPretrain framework [32] on RTX4090. More information can be found in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "4.2 Main Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Tab. 1 compares the performance of the MoE Jetpack with Dense ViT models and Soft MoE models on various image datasets using ViT-T (a) and ConvNeXt-F (b) architectures. All models maintain approximately the same number of FLOPs. The columns in Tab. 1 are defined as follows: ", "page_idx": 5}, {"type": "text", "text": "\u2022 Dense: Refers to dense models trained from scratch on each specific dataset. ", "page_idx": 5}, {"type": "table", "img_path": "Q8Z04XhDdL/tmp/fdda74e9afab013cf3626c4ef8124a7e2c2aa1778636da60136a699119763853.jpg", "table_caption": ["Table 1: Performance comparison on visual recognition tasks with ViT-T and ConvNeXt-F. "], "table_footnote": ["(a) ViT-T (b) ConvNeXt-F "], "page_idx": 6}, {"type": "text", "text": "\u2022 Dense (21k): Represents dense models initialized with ImageNet-21K pre-trained weights, followed by fine-tuning on the target datasets.   \n\u2022 Soft MoE: Reports the results of Soft MoE models trained from scratch on each dataset.   \n\u2022 MoE Jetpack: Shows the performance of SpheroMoE models, initialized using checkpoint recycling with pre-trained dense checkpoints from ImageNet-21K, followed by fine-tuning on downstream datasets. ", "page_idx": 6}, {"type": "text", "text": "The MoE Jetpack, benefiting from the pre-trained knowledge embedded in dense checkpoints, consistently surpasses the performance of both Soft MoE models trained from scratch and dense models with ImageNet-21K initialization. These results underscore the effectiveness of MoE Jetpack. ", "page_idx": 6}, {"type": "text", "text": "4.3 Ablations ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We perform ablation studies to assess the impact of various components and hyper-parameters within the MoE Jetpack. By default, we use a ViT-T model with the SpheroMoE layer integrated from layers 7 to 12, comprising 98 core experts and 196 universal experts (detailed in Appendix A). The Checkpoint Recycling method transforms dense checkpoints of ViT-S and ViT-T, pre-trained on ImageNet-21k, into initial weights for our V-JetMoE-T model. ", "page_idx": 6}, {"type": "text", "text": "Effect of MoE Jetpack Components. We conducted the ablation of two key components of the MoE Jetpack on three datasets. As shown in Tab. 2, integrating Checkpoint Recycling with the Soft MoE baseline significantly improves performance across all datasets, with a mean accuracy increment of $10.2\\%$ . The SpheroMoE layer further enhances performance, achieving a mean accuracy of $87.9\\%$ . These results demonstrate the efficacy of both components, especially when used together, highlighting their synergistic effect in boosting performance. ", "page_idx": 6}, {"type": "text", "text": "Checkpoint Recycling vs. Sparse Upcycling. To compare the four checkpoint recycling strategies mentioned in Sec. 3.1 and the method of using duplicated MLPs to construct experts in Sparse Upcycling [16], we conducted experiments on ImageNet. For fairness, we also employed our SpheroMoE layer in the Sparse Up", "page_idx": 6}, {"type": "table", "img_path": "Q8Z04XhDdL/tmp/ef476ddca22a91684a8381093726f873604c7d5108dc9b3dfe4b9b184b9a0a2a.jpg", "table_caption": ["Table 3: Checkpoint Recycling vs. Sparse Upcycling "], "table_footnote": ["cycling. The results, summarized in Tab. 3, show that Importance-Based Sampling achieves the highest performance, demonstrating its "], "page_idx": 6}, {"type": "table", "img_path": "Q8Z04XhDdL/tmp/525c2e257096aca1293d6d023ec73c1c9174e0add098d8b5b38651ae1a051a39.jpg", "table_caption": ["Table 2: Ablation Study on MoE Jetpack Components. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "Q8Z04XhDdL/tmp/e0f84a005d9cba47e3e996a39fea142403781e690b10ef3bd96b6a5543b0c957.jpg", "table_caption": ["Table 4: Effectiveness of SpheroMoE with the Dual-Path Structure. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "Q8Z04XhDdL/tmp/1d09e61de5a96d2e5f9481bbe5d994dafd653789c5b56eb8e5587a8f4e40e41f.jpg", "table_caption": ["Table 5: Comparison of Model Variants with Different Configurations "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "effectiveness in leveraging critical weights to enhance model performance and convergence speed. Additionally, Checkpoint Recycling is highly flexible, allowing the construction of experts of varying sizes to meet different needs, a feature not provided by sparse upcycling. ", "page_idx": 7}, {"type": "text", "text": "Effect of Dual-Path Structure. The Tab. 4 presents an ablation study on the effectiveness of the dual-path structure in SpheroMoE. When configured with 197 core experts only (no universal experts), SpheroMoE achieves a higher accuracy on ImageNet-1K compared to Soft MoE with the same number of experts. Introducing the dual-path structure with 98 core experts and 196 universal experts (each with one-fourth of the parameters of core experts) further enhances accuracy to 79.9 while reducing the computational cost to 1.1G FLOPs. This result highlights the efficiency of the dual-path structure, which allows SpheroMoE to allocate resources adaptively and achieve better performance without increasing the overall FLOPs. ", "page_idx": 7}, {"type": "text", "text": "Core Experts Ratio. To evaluate the effectiveness of the Adaptive Dualpath MoE structure introduced in Sec.3.2 and to identify the optimal ratio between core and universal experts, we conducted ablation on the CIFAR-100 dataset. With a fixed total number of experts, we varied the ratio of core experts to find the ideal balance between performance and resource allocation. As shown in Fig. 4, the highest accuracy is achieved when core experts constitute approximately $1/3$ of the total experts. ", "page_idx": 7}, {"type": "text", "text": "MoE Jetpack Configurations. This part evaluates the impact of various ", "text_level": 1, "page_idx": 7}, {"type": "image", "img_path": "Q8Z04XhDdL/tmp/3fb4767af525046b6d78d0b887a4c3fea03d8277700e720f1414bd79da284381.jpg", "img_caption": ["Figure 4: CIFAR-100 accuracy across different ratios of core (dark) to universal (light) experts, highlighting optimal performance at a 1/3 core ratio. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "MoE Jetpack configurations on model performance, as summarized in Tab. 5. The experiments focus on the placement of SpheroMoE layers, the number of experts per layer, and the base size of converted dense checkpoints. Results indicate that more SpheroMoE layers generally enhance performance, though placing it before layer 7 slightly hurt the performance. Consequently, SpheroMoE layers were incorporated into layers 7\u221212. Additionally, models with more experts exhibit improved accuracy, highlighting the beneftis of increased expert specialization and diversity. Models converted from larger dense checkpoints demonstrate superior performance. These findings suggest that MoE network performance can be improved by increasing the number of MoE layers, incorporating more experts, and utilizing larger base models. ", "page_idx": 7}, {"type": "image", "img_path": "Q8Z04XhDdL/tmp/8fc91e3ce3e47b0f11771b182491aa4ca3d0b5c25ad44b2a2ea1032647450602.jpg", "img_caption": ["Figure 5: Comparison of convergence speeds using MoE Jetpack versus training from scratch on ImageNet (left) and CIFAR-100 (right). MoE Jetpack achieves target accuracies significantly faster, demonstrating a $2\\mathbf{x}$ speed increase on ImageNet and an $8\\mathrm{{x}}$ increase on CIFAR-100. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "Q8Z04XhDdL/tmp/4a76fefa82a7fce01b31f2fd25f268543543d5f35bf89f298fb6cb38b92cd4df.jpg", "img_caption": ["Figure 6: (a) The attention maps generated by five experts in response to the input image, highlighting the experts\u2019 specialization. (b) These line charts show varying contributions of core and universal experts, with core experts\u2019 influence peaking in later layers, emphasizing their detailed feature refinement, contrasted with the consistent input of universal experts. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.4 Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We analyze the impact of MoE Jetpack on convergence speed for MoE models fine-tuned on ImageNet1K and CIFAR-100 dataset. Additionally, we offer insights into expert attention patterns and the contribution of each expert to the output tokens. ", "page_idx": 8}, {"type": "text", "text": "Accelerating MoE Convergence with MoE Jetpack. The effect of MoE Jetpack on convergence speed is illustrated in Fig. 5 for ImageNet (left) and CIFAR-100 (right). In both cases, models with MoE Jetpack achieve target accuracy significantly faster. For ImageNet, MoE Jetpack enables the model to reach approximately $77\\%$ top-1 accuracy within 150 epochs\u2014twice as fast as training from scratch. This acceleration is even more pronounced on smaller datasets like CIFAR-100, where MoE Jetpack achieves $76\\%$ top-1 accuracy by around 40 epochs, an eightfold improvement over the baseline. These results underscore MoE Jetpack\u2019s efficiency in accelerating convergence, reducing fine-tuning time and computational demands. ", "page_idx": 8}, {"type": "text", "text": "Intuition of Expert Attention Patterns. We visualize the attention maps of experts in Fig. 6(a), which illustrates that different experts focus on different parts of the input image. This diversity in attention suggests that each expert specializes in capturing unique aspects of the input, enhancing the model\u2019s ability to represent features comprehensively. The specialization allows the MoE model to combine multiple perspectives, resulting in a more robust and detailed understanding of the input. ", "page_idx": 8}, {"type": "text", "text": "Contribution of Each Expert to Final Results. Fig. 6(b) demonstrates the varying contributions of core and universal experts across different layers of the MoE model. Core experts show an increasing influence in the later layers, emphasizing their role in refining specific and highly relevant features. Additionally, the contributions among core experts are markedly uneven, some experts can impact output tokens $17\\times$ more than others, reflecting greater specialization and diversity in their focus areas. ", "page_idx": 8}, {"type": "text", "text": "In contrast, universal experts maintain a relatively consistent contribution level, indicating a more uniform integration of broader contextual information throughout the network. This hierarchical structure, balancing the specialized refinement by core experts with the generalized understanding provided by universal experts, enhances the model\u2019s overall performance and robustness. ", "page_idx": 9}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Sparsely activated Mixture of Experts (MoE). Scaling Laws [33] indicate that increasing model parameters can enhance performance. However, traditional densely activated models (dense models) [7, 8] activate all parameters for every input, resulting in high computational costs as models scale. In contrast, MoE models [12, 34, 35, 36] activate only a subset of parameters for specific input tokens, enabling efficient scaling to trillions of parameters with sublinear increases in computational costs [37, 5, 4]. To optimize input token allocation among experts, various routing mechanisms have been developed. BASELayer [18] formulates token-to-expert allocation as a linear assignment problem, while EC-CF2 [21] propose expert choice routing, soft routing methods like SMEAR [38], and Soft MoE [6] implicit soft assignments involving all tokens. However, few studies explore leveraging dense model checkpoints to accelerate MoE training [16]. ", "page_idx": 9}, {"type": "text", "text": "Knowledge transfer with pre-trained models. Knowledge transfer occurs between identical or distinct models. Pre-training followed by fine-tuning is well-established for identical models, utilizing large datasets through supervised learning (e.g., ImageNet21k [25], JFT-300M [39]) or self-supervised methods (e.g., BERT [40], CLIP [41], MAE [42], DINO [43], EVA [44, 45]). These approaches produce foundation models with broad applicability, and subsequent fine-tuning consistently improves performance. For distinct models, knowledge distillation [46] trains a smaller student model to mimic the larger teacher model, enhancing efficiency. Additional strategies include weight pruning [47, 48, 48, 49, 50], which removes redundant parameters, and weight selection [22] initializes a smaller model with a subset of weights from a pre-trained larger model. ", "page_idx": 9}, {"type": "text", "text": "Research on transferring knowledge from dense checkpoints to MoE models is limited. MoEfication [51] partitions a dense model into MoE components, while Sparse Upcycling [16] replicates a dense model multiple times to form a MoE model. Our MoE Jetpack recycles important weights from larger dense checkpoints to initialize experts of various sizes, combining the flexibility of knowledge transfer across distinct model types with the efficiency of transfer between identical model types. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduced MoE Jetpack, a novel framework for fine-tuning pre-trained dense checkpoints into Mixture of Experts model. Our approach leverages checkpoint recycling, which inherits the knowledge of open-source dense checkpoints and the hyperspherical adaptive MoE (SpheroMoE) layer to enhance fine-tuning performance. These innovations contribute to improved convergence speed and model accuracy. The MoE Jetpack significantly improved various visual tasks while maintaining computational efficiency. ", "page_idx": 9}, {"type": "text", "text": "The limitation of our approach lies in its reliance on the quality of pre-trained dense checkpoints; inadequately trained or poorly generalized dense models may hinder performance improvements. Additionally, while our experiments focused on visual tasks, further research is needed to validate the generalizability of MoE Jetpack across other domains, such as natural language processing and reinforcement learning. Future work may address these limitations, further enhancing the scalability and robustness of the framework, and broadening MoE applicability across diverse tasks. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported by the National Natural Science Foundation of China (Grant Nos. U2341227 and 62226104). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \u201cImagenet classification with deep convolutional neural networks,\u201d Proc. of Advances in Neural Information Processing Systems, vol. 25, 2012. 1 [2] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image recognition,\u201d in Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2016, pp. 770\u2013778. 1 [3] X. Zhai, A. Kolesnikov, N. Houlsby, and L. Beyer, \u201cScaling vision transformers,\u201d in Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2022, pp. 12 104\u201312 113. 1 [4] W. Fedus, B. Zoph, and N. Shazeer, \u201cSwitch transformers: Scaling to trillion parameter models with simple and efficient sparsity,\u201d Journal of Machine Learning Research, vol. 23, no. 120, pp.   \n1\u201339, 2022. 1, 2, 10 [5] C. Riquelme, J. Puigcerver, B. Mustafa, M. Neumann, R. Jenatton, A. Susano Pinto, D. Keysers, and N. Houlsby, \u201cScaling vision with sparse mixture of experts,\u201d Proc. of Advances in Neural Information Processing Systems, vol. 34, pp. 8583\u20138595, 2021. 1, 10 [6] J. Puigcerver, C. R. Ruiz, B. Mustafa, and N. Houlsby, \u201cFrom sparse to soft mixtures of experts,\u201d in Proc. of Intl. Conf. on Learning Representations, 2023. 1, 2, 3, 6, 7, 8, 10 [7] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., \u201cAn image is worth 16x16 words: Transformers for image recognition at scale,\u201d in Proc. of Intl. Conf. on Learning Representations, 2020. 1, 6, 10 [8] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie, \u201cA convnet for the 2020s,\u201d in Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2022, pp. 11 976\u201311 986.   \n1, 6, 10 [9] Z. Fan, R. Sarkar, Z. Jiang, T. Chen, K. Zou, Y. Cheng, C. Hao, Z. Wang et al., \u201cM3vit: Mixtureof-experts vision transformer for efficient multi-task learning with model-accelerator co-design,\u201d Proc. of Advances in Neural Information Processing Systems, vol. 35, pp. 28 441\u201328 457, 2022.   \n1 [10] S. Rajbhandari, C. Li, Z. Yao, M. Zhang, R. Y. Aminabadi, A. A. Awan, J. Rasley, and Y. He, \u201cDeepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale,\u201d in Proc. of Intl. Conf. on Machine Learning. PMLR, 2022, pp. 18 332\u201318 346. 1 [11] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand et al., \u201cMixtral of experts,\u201d arXiv preprint arXiv:2401.04088,   \n2024. 1 [12] D. Dai, C. Deng, C. Zhao, R. Xu, H. Gao, D. Chen, J. Li, W. Zeng, X. Yu, Y. Wu et al., \u201cDeepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models,\u201d arXiv preprint arXiv:2401.06066, 2024. 1, 10 [13] B. Lin, Z. Tang, Y. Ye, J. Cui, B. Zhu, P. Jin, J. Zhang, M. Ning, and L. Yuan, \u201cMoe-llava: Mixture of experts for large vision-language models,\u201d arXiv preprint arXiv:2401.15947, 2024.   \n1 [14] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz et al., \u201cHuggingface\u2019s transformers: State-of-the-art natural language processing,\u201d arXiv preprint arXiv:1910.03771, 2019. 1 [15] R. Wightman, \u201cPytorch image models,\u201d https://github.com/rwightman/pytorch-image-models,   \n2019. 1 [16] A. Komatsuzaki, J. Puigcerver, J. Lee-Thorp, C. R. Ruiz, B. Mustafa, J. Ainslie, Y. Tay, M. Dehghani, and N. Houlsby, \u201cSparse upcycling: Training mixture-of-experts from dense checkpoints,\u201d in Proc. of Intl. Conf. on Learning Representations, 2022. 2, 7, 10 [17] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean, \u201cOutrageously large neural networks: The sparsely-gated mixture-of-experts layer,\u201d in Proc. of Intl. Conf. on Learning Representations, 2016. 2   \n[18] M. Lewis, S. Bhosale, T. Dettmers, N. Goyal, and L. Zettlemoyer, \u201cBase layers: Simplifying training of large, sparse models,\u201d in Proc. of Intl. Conf. on Machine Learning. PMLR, 2021, pp. 6265\u20136274. 2, 10   \n[19] A. Clark, D. de Las Casas, A. Guy, A. Mensch, M. Paganini, J. Hoffmann, B. Damoc, B. Hechtman, T. Cai, S. Borgeaud et al., \u201cUnified scaling laws for routed language models,\u201d in Proc. of Intl. Conf. on Machine Learning. PMLR, 2022, pp. 4057\u20134086. 2   \n[20] S. Roller, S. Sukhbaatar, J. Weston et al., \u201cHash layers for large sparse models,\u201d Proc. of Advances in Neural Information Processing Systems, vol. 34, pp. 17 555\u201317 566, 2021. 2   \n[21] Y. Zhou, T. Lei, H. Liu, N. Du, Y. Huang, V. Zhao, A. M. Dai, Q. V. Le, J. Laudon et al., \u201cMixture-of-experts with expert choice routing,\u201d Proc. of Advances in Neural Information Processing Systems, vol. 35, pp. 7103\u20137114, 2022. 2, 10   \n[22] Z. Xu, Y. Chen, K. Vishniakov, Y. Yin, Z. Shen, T. Darrell, L. Liu, and Z. Liu, \u201cInitializing models with larger ones,\u201d in Proc. of Intl. Conf. on Learning Representations, 2024. 3, 6, 10   \n[23] G. Karypis, \u201cMetis: Unstructured graph partitioning and sparse matrix ordering system,\u201d Technical report, 1997. 4   \n[24] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d Proc. of Advances in Neural Information Processing Systems, vol. 30, 2017. 5   \n[25] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \u201cImagenet: A large-scale hierarchical image database,\u201d in Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition. Ieee, 2009, pp. 248\u2013255. 6, 10   \n[26] A. Krizhevsky, G. Hinton et al., \u201cLearning multiple layers of features from tiny images,\u201d 2009. 6   \n[27] M.-E. Nilsback and A. Zisserman, \u201cAutomated flower classification over a large number of classes,\u201d in Indian conference on computer vision, graphics & image processing. IEEE, 2008, pp. 722\u2013729. 6   \n[28] O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. Jawahar, \u201cCats and dogs,\u201d in Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition. IEEE, 2012, pp. 3498\u20133505. 6   \n[29] A. Coates, A. Ng, and H. Lee, \u201cAn analysis of single-layer networks in unsupervised feature learning,\u201d in Proceedings of the international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings, 2011, pp. 215\u2013223. 6   \n[30] L. Bossard, M. Guillaumin, and L. Van Gool, \u201cFood-101\u2013mining discriminative components with random forests,\u201d in Proc. of European Conference on Computer Vision. Springer, 2014, pp. 446\u2013461. 6   \n[31] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and A. Vedaldi, \u201cDescribing textures in the wild,\u201d in Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2014, pp. 3606\u20133613. 6   \n[32] M. Contributors, \u201cOpenmmlab\u2019s pre-training toolbox and benchmark,\u201d https://github.com/ open-mmlab/mmpretrain, 2023. 6   \n[33] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, \u201cScaling laws for neural language models,\u201d arXiv preprint arXiv:2001.08361, 2020. 10   \n[34] B. Lin, Z. Tang, Y. Ye, J. Cui, B. Zhu, P. Jin, J. Zhang, M. Ning, and L. Yuan, \u201cMoe-llava: Mixture of experts for large vision-language models,\u201d arXiv preprint arXiv:2401.15947, 2024. 10   \n[35] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand et al., \u201cMixtral of experts,\u201d arXiv preprint arXiv:2401.04088, 2024. 10   \n[36] X. Wu, S. Huang, and F. Wei, \u201cMole: Mixture of lora experts,\u201d in Proc. of Intl. Conf. on Learning Representations, 2023. 10   \n[37] D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen, \u201cGshard: Scaling giant models with conditional computation and automatic sharding,\u201d in Proc. of Intl. Conf. on Learning Representations, 2020. 10   \n[38] M. Muqeeth, H. Liu, and C. Raffel, \u201cSoft merging of experts with adaptive routing,\u201d arXiv preprint arXiv:2306.03745, 2023. 10   \n[39] C. Sun, A. Shrivastava, S. Singh, and A. Gupta, \u201cRevisiting unreasonable effectiveness of data in deep learning era,\u201d in Porc. of IEEE Intl. Conf. on Computer Vision, 2017, pp. 843\u2013852. 10   \n[40] J. D. M.-W. C. Kenton and L. K. Toutanova, \u201cBert: Pre-training of deep bidirectional transformers for language understanding,\u201d in Proceedings of NAACL-HLT, 2019, pp. 4171\u20134186. 10   \n[41] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., \u201cLearning transferable visual models from natural language supervision,\u201d in Proc. of Intl. Conf. on Machine Learning. PMLR, 2021, pp. 8748\u20138763. 10   \n[42] K. He, X. Chen, S. Xie, Y. Li, P. Doll\u00e1r, and R. Girshick, \u201cMasked autoencoders are scalable vision learners,\u201d in Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2022, pp. 16 000\u201316 009. 10   \n[43] M. Oquab, T. Darcet, T. Moutakanni, H. V. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. HAZIZA, F. Massa, A. El-Nouby et al., \u201cDinov2: Learning robust visual features without supervision,\u201d Transactions on Machine Learning Research, 2023. 10   \n[44] Y. Fang, Q. Sun, X. Wang, T. Huang, X. Wang, and Y. Cao, \u201cEva-02: A visual representation for neon genesis,\u201d arXiv preprint arXiv:2303.11331, 2023. 10   \n[45] Q. Sun, Y. Fang, L. Wu, X. Wang, and Y. Cao, \u201cEva-clip: Improved training techniques for clip at scale,\u201d arXiv preprint arXiv:2303.15389, 2023. 10   \n[46] G. Hinton, O. Vinyals, and J. Dean, \u201cDistilling the knowledge in a neural network,\u201d 2014. 10   \n[47] S. Han, J. Pool, J. Tran, and W. Dally, \u201cLearning both weights and connections for efficient neural network,\u201d Proc. of Advances in Neural Information Processing Systems, vol. 28, 2015. 10   \n[48] S. Ashkboos, M. L. Croci, M. G. do Nascimento, T. Hoefler, and J. Hensman, \u201cSlicegpt: Compress large language models by deleting rows and columns,\u201d in Proc. of Intl. Conf. on Learning Representations, 2023. 10   \n[49] M. Xia, T. Gao, Z. Zeng, and D. Chen, \u201cSheared llama: Accelerating language model pretraining via structured pruning,\u201d in Proc. of Intl. Conf. on Learning Representations, 2023. 10   \n[50] F. Yu, K. Huang, M. Wang, Y. Cheng, W. Chu, and L. Cui, \u201cWidth & depth pruning for vision transformers,\u201d in Proc. of the AAAI Conf. on Artificial Intelligence, vol. 36, no. 3, 2022, pp. 3143\u20133151. 10   \n[51] Z. Zhang, Y. Lin, Z. Liu, P. Li, M. Sun, and J. Zhou, \u201cMoefication: Transformer feed-forward layers are mixtures of experts,\u201d in Findings of the Association for Computational Linguistics: ACL 2022, 2022, pp. 877\u2013890. 10   \n[52] I. Loshchilov and F. Hutter, \u201cDecoupled weight decay regularization,\u201d in Proc. of Intl. Conf. on Learning Representations, 2018. 15   \n[53] E. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le, \u201cRandaugment: Practical automated data augmentation with a reduced search space,\u201d in Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition workshops, 2020, pp. 702\u2013703. 15   \n[54] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, \u201cmixup: Beyond empirical risk minimization,\u201d in Proc. of Intl. Conf. on Learning Representations, 2018. 15   \n[55] S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, and Y. Yoo, \u201cCutmix: Regularization strategy to train strong classifiers with localizable features,\u201d in Porc. of IEEE Intl. Conf. on Computer Vision, 2019, pp. 6023\u20136032. 15   \n[56] Z. Zhong, L. Zheng, G. Kang, S. Li, and Y. Yang, \u201cRandom erasing data augmentation,\u201d in Proc. of the AAAI Conf. on Artificial Intelligence, vol. 34, no. 07, 2020, pp. 13 001\u201313 008. 15   \n[57] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, \u201cRethinking the inception architecture for computer vision,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 2818\u20132826. 15   \n[58] H. Touvron, M. Cord, A. Sablayrolles, G. Synnaeve, and H. J\u00e9gou, \u201cGoing deeper with image transformers,\u201d in Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 32\u201342. 15 ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Detailed Model Configurations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we present the detailed model configurations for the main experiments in Sec. 4 in Tab. 6. We refer to pre-trained dense checkpoints as predecessors and the derived MoE models as successors. We use ImageNet-21k pre-trained predecessor from timm with our Checkpoint Recycling algorithm to generate initialized weights for the successor. ", "page_idx": 14}, {"type": "table", "img_path": "Q8Z04XhDdL/tmp/d322cd9db258d2b5f9781f224eb860ebf0da0995765cc388d927d3b888279535.jpg", "table_caption": ["Table 6: Configurations for Models. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "B Experiment Settings and Time Costs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section of the appendix, we provide a comprehensive description of the training settings used in our experiments. Tab. 7 outlines the standard training configuration utilized across our experiments. Tab. 8 details the dataset-specific training configurations, capturing variations in batch size, warmup epochs, total training epochs, and drop path rates for each dataset employed in our experiments. ", "page_idx": 14}, {"type": "text", "text": "Our experiments were conducted on RTX 4090 GPU. Training V-JetMoE-T on the CIFAR-100 dataset (60,000 images) required 2.5 GPU hours while training on the ImageNet-1K dataset (1,281,167 images) required 120 GPU hours. Training C-JetMoE-F on CIFAR-100 also required 2.5 GPU hours and 156 GPU hours on ImageNet-1K. For V-JetMoE-S, training on CIFAR-100 required 8 GPU hours and 200 GPU hours on ImageNet-1K. Compared to the original dense models (ViT-Tiny, ConvNeXt-Femto, ViT-Small), our method achieves nearly equivalent training times. ", "page_idx": 14}, {"type": "text", "text": "For all the experiments presented in our paper, we required 3, 300 GPU hours for training. In total, we spent approximately 8, 000 GPU hours for exploration and validation of our work. ", "page_idx": 14}, {"type": "table", "img_path": "Q8Z04XhDdL/tmp/4e33a4f47e668abf54b47f517508d629f92c32ad409bd1ad2503dea4dc0c8af3.jpg", "table_caption": ["Table 7: Our basic recipe for model training. "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "Q8Z04XhDdL/tmp/2f24f3883aaa0088f48d5a5b4a0639b83c2548eef93fb670d3f196eac1b5f1fd.jpg", "table_caption": ["Table 8: Hyper-parameter setting on ViT-T. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "C Implementation of SpheroMoE Layer ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Algorithm 1: Simple implementation of SpheroMoE. ", "page_idx": 15}, {"type": "text", "text": "ef parallel_expert_forward(x, experts) \"\"\" Traditional MoE models use a for loop to process each token through the experts. By merging all expert weights into a large matrix, our implementation allows for a single matrix multiplication operation for each layer across all tokens and experts, replacing multiple individual operations. \"\"\" ${\\tt x}=$ einsum(x, experts.weight_1, \"b e s d1, e d2 d1 -> b e s d2\") $\\textbf{x}=\\textbf{x}+$ rearrange(experts.bias_1, \"e d2 -> () e () d2\") ${\\tt x}=$ experts.act $(\\mathbf{x})$ ${\\tt x}=$ einsum(x, experts.weight_2, \"b e s d2, e d1 d2 -> b e s d1\") $\\textbf{x}=\\textbf{x}+$ rearrange(experts.bias_2, \"e d1 -> () e () d1\") return x ", "page_idx": 15}, {"type": "text", "text": "def spheromoe_layer(X, Q, T, core_experts, univ_experts): 111111 ", "page_idx": 15}, {"type": "text", "text": "Performs the Spheromoe layer operation. ", "page_idx": 15}, {"type": "text", "text": "Parameters: X (tensor): tensor with shape (batch, token_num, channel).   \n$\\mathbb{Q}$ (tensor): tensor with shape (expert_num, slots_per_expert, channel).   \nT (tensor): the learnable temperature list for the softmax function.   \ncore_experts, univ_experts (expert): expert weight for MoE layer. Returns:   \ntensor: Output tensor after applying the Spheromoe layer operations. \"\"\"   \nX_norm $=$ layer_norm(X, dim $\\_{=-1}$ )   \n$\\mathtt{Q\\_n o r m}\\ =\\ 12\\_\\mathtt{n o r m}(\\mathtt{Q})$   \nK = l2_norm(K_project(X_norm)) ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "# Compute similarity logits. logits $=$ einsum(K, $\\mathsf{Q}$ _norm, \"b n d, e s d $->$ b n e s\") ", "page_idx": 15}, {"type": "text", "text": "# create normal noise noise $=$ normal_noise(logits) $^\\ast$ self.noise_mult # Apply softmax to similarity logits. Dispatch $=$ softmax(logits/T[0], dim $\\scriptstyle1=1$ ) $^+$ noise Combine $=$ softmax(logits/T[1], dim $\\overrightharpoon{.}$ [-1,-2]) $^+$ noise ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "# Token dispatch.   \nX_hat $=$ einsum(Dispatch, X_norm, \"b n d, b n e s -> b e s d\") X_core $=$ X_hat[:, :core_num, :, :]   \nX_univ $=$ X_hat[:, core_num:, :, :]   \n# Using core experts and universal experts processes each slot.   \nY_hat $=$ stack([ parallel_expert_forward(X_core, core_experts), parallel_expert_forward(X_univ, univ_experts)   \n], dim $=\\!1$ ) ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "# Expert dropout. Y_hat $=$ expert_drop(Y_hat) ", "page_idx": 15}, {"type": "text", "text": "# Token combine. ${\\textsf{Y}}=$ einsum(Combine, Y_hat, \"b n e s, b e s d -> b n d\" ) ", "page_idx": 15}, {"type": "text", "text": "return Y ", "page_idx": 15}, {"type": "text", "text": "D Dynamic Allocation and Focus Regions of Experts in MoE Jetpack ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we discuss the dynamic allocation and focus regions of core and universal experts across different layers of MoE Jetpack. We used the same test images as in the main text, visualizing the focus regions of the most important (i.e., those with the highest output contribution) core and universal experts for each MoE layer in Fig. 7. The corresponding contribution values for these experts are listed in Tab. 9. ", "page_idx": 16}, {"type": "text", "text": "Our findings are as follows: Initially, in the shallower network layers (MoE Layer 7 and 8), the core experts contribute less than the universal experts, and their focus regions are relatively dispersed. As the network deepens, in MoE Layer 9, the most important core and universal experts show similar contribution values and focus regions. With further depth (MoE Layers 10, 11, and 12), the dominance of the core experts becomes increasingly evident, with significantly higher contribution values than the universal experts. Core experts focus on prominent objects in the images and are inclined to capture global information. ", "page_idx": 16}, {"type": "text", "text": "These experts\u2019 dynamic allocation and different focus region tendencies are crucial to our method. Different experts have varying capabilities in extracting information at various granularities, and the network facilitates collaboration among these experts to produce the final output. This illustrates the effective utilization of expert diversity in the MoE model. ", "page_idx": 16}, {"type": "image", "img_path": "Q8Z04XhDdL/tmp/ec00cbb59de28f6f7efe57064c99a20f04acfc3d2765e1f781dd6ac9b2009aef.jpg", "img_caption": ["Figure 7: Visualization of the attention map identified by the most important core experts and universal experts across different layers (MoE Layer 07 to MoE Layer 12). The images show the regions deemed most relevant by each type of expert at each layer. "], "img_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "Q8Z04XhDdL/tmp/02cbd5eb0c57d36db01e476094fd9fcdedad19b7af3e5c04918b888254fb9e12.jpg", "table_caption": ["Table 9: Contribution values of core and universal experts across network layers. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "E Broader Impacts ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The proposed MoE Jetpack framework significantly enhances the accessibility and efficiency of MoE models by utilizing pre-existing dense checkpoints to substantially reduce the computational costs associated with training these models from scratch. This method not only minimizes the environmental footprint by decreasing the reliance on extensive GPU resources but also bridges the resource gap, facilitating wider adoption and fostering innovation across the AI community. Additionally, our commitment to open-sourcing all experimental code promotes greater transparency and collaboration in research. We have carefully considered the potential societal impacts of our method and believe it does not pose any significant ethical or fairness concerns, thereby ensuring its responsible application. ", "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: The claims in the abstract and introduction accurately reflect the scope and contributions of the paper, as they are directly supported by the detailed theoretical analysis and empirical results in Sec. 3 and Sec. 4. The paper also clearly differentiates between its current contributions and scope. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: We discuss the limitations of out proposed MoE Jetpack in Sec. 6. We discuss the computational efficiency in Appendix B. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: [NA] Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We disclose all necessary information to reproduce the main results in Sec. 4.1.   \nFor more detailed experiment configurations, please refer to Appendix A and B. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: Since the code has not been fully organized yet, we will make the code and model weights available as open source once the organization is complete. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: the paper provides all necessary details for understanding the training and test processes. We conducted training and testing using publicly available datasets (like ImageNet and CIFAR), the specifics of which are listed in Sec. 4.1. For more detailed experimental configurations, such as hyperparameters, type of optimizer used, etc., please refer to Appendix A and B. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [No] ", "page_idx": 20}, {"type": "text", "text": "Justification: We did not provide error bars because our experiments used a fixed training and testing dataset and a constant random seed (3407), which resulted in minimal variability (below $0.1\\%$ ) in network training outcomes across runs. Given this stability, we concluded that error bars were unnecessary. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Detailed specifications of the computing resources used for our experiments are listed in Sec. 4.1. For comprehensive information regarding the compute consumption and execution time for each experimental run, please refer to Appendix B. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The research presented in this paper adheres to all aspects of the NeurIPS Code of Ethics. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We discuss the Broader Impacts in Appendix E. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: All existing assets used in our paper, including code, datasets, and models, are properly credited to their original creators. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [No] ", "page_idx": 23}, {"type": "text", "text": "Justification: The new assets introduced in the paper, including models and code, have not yet been released. Documentation for these assets is currently in preparation and will be provided upon their official release. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]