[{"figure_path": "Q8Z04XhDdL/tables/tables_6_1.jpg", "caption": "Table 1: Performance comparison on visual recognition tasks with ViT-T and ConvNeXt-F.", "description": "This table presents a comparison of the performance of different models on various image recognition datasets.  The models compared include Dense models (trained from scratch), Dense models initialized with ImageNet-21k pre-trained weights, Soft MoE models (trained from scratch), and MoE Jetpack models. The datasets include ImageNet-1k, Food-101, CIFAR-10, CIFAR-100, STL-10, Flowers, Pets, and DTD.  The table shows the accuracy achieved by each model on each dataset, highlighting the improvement achieved by MoE Jetpack compared to other approaches.", "section": "4.2 Main Results"}, {"figure_path": "Q8Z04XhDdL/tables/tables_6_2.jpg", "caption": "Table 3: Checkpoint Recycling vs. Sparse Upcycling", "description": "This table compares the performance of four different checkpoint recycling strategies (Random Sampling, Uniform Selection, Graph Partitioning, Importance-based Sampling) against the Sparse Upcycling method [16] on the ImageNet dataset.  The results demonstrate the superior performance of Importance-based Sampling, achieving an accuracy of 79.9 compared to the other methods and Sparse Upcycling, which only achieves 79.1.", "section": "4.3 Ablations"}, {"figure_path": "Q8Z04XhDdL/tables/tables_6_3.jpg", "caption": "Table 2: Ablation Study on MoE Jetpack Components.", "description": "This table presents the ablation study results on the MoE Jetpack components. It compares the performance of different model configurations on ImageNet, CIFAR-100, and Flowers datasets. The configurations include the baseline ViT-T model, Soft MoE with Checkpoints Recycling, Soft MoE with Checkpoints Recycling and SpheroMoE, demonstrating the contribution of each component to the model's performance.  The mean accuracy across all datasets is shown for each configuration, illustrating the performance improvements from incorporating each component.", "section": "4.3 Ablations"}, {"figure_path": "Q8Z04XhDdL/tables/tables_7_1.jpg", "caption": "Table 4: Effectiveness of SpheroMoE with the Dual-Path Structure.", "description": "This table presents a comparison of the performance of different model configurations on ImageNet.  The models compared include SoftMoE with 197 experts, SpheroMoE with 197 core experts and 0 universal experts, and SpheroMoE with 98 core experts and 196 universal experts. The table shows the ImageNet accuracy and FLOPs (floating point operations) for each model configuration. The results highlight the effectiveness of the SpheroMoE architecture, particularly when using the dual-path structure with a combination of core and universal experts, in improving accuracy while maintaining computational efficiency.", "section": "4.3 Ablations"}, {"figure_path": "Q8Z04XhDdL/tables/tables_7_2.jpg", "caption": "Table 1: Performance comparison on visual recognition tasks with ViT-T and ConvNeXt-F.", "description": "This table compares the performance of different models on visual recognition tasks using two different architectures, ViT-T and ConvNeXt-F.  It shows the accuracy achieved by dense models trained from scratch, dense models initialized with ImageNet-21K pre-trained weights and fine-tuned on the target dataset, Soft MoE models trained from scratch, and MoE Jetpack models. The MoE Jetpack models are initialized using checkpoint recycling with pre-trained dense checkpoints from ImageNet-21K and then fine-tuned. The table highlights the superior performance of MoE Jetpack across various datasets compared to the baseline models.", "section": "4.2 Main Results"}, {"figure_path": "Q8Z04XhDdL/tables/tables_14_1.jpg", "caption": "Table 6: Configurations for Models.", "description": "This table presents the detailed model configurations for the main experiments. It shows the model, FLOPs (floating point operations per second), initialization method, MoE (Mixture of Experts) layers, number of core experts, and number of universal experts for both successors (V-JetMoE-T and C-JetMoE-F) and predecessors (ViT-S/16 and ConvNeXt-T).  The predecessors are pre-trained dense checkpoints used to initialize the successor MoE models via checkpoint recycling. The table clarifies the architectural differences between the dense models and the resulting MoE models. ", "section": "4.1 Experimental Setups"}, {"figure_path": "Q8Z04XhDdL/tables/tables_14_2.jpg", "caption": "Table 1: Performance comparison on visual recognition tasks with ViT-T and ConvNeXt-F.", "description": "This table presents a performance comparison of visual recognition tasks using two different model architectures, ViT-T and ConvNeXt-F.  It compares the performance of Dense models (trained from scratch and with ImageNet-21k pre-trained weights), Soft MoE models (trained from scratch), and MoE Jetpack models (initialized using checkpoint recycling with ImageNet-21k pre-trained checkpoints and then fine-tuned). The results are shown for various datasets, including ImageNet-1k, Food-101, CIFAR-10, CIFAR-100, STL-10, Flowers, Pets, and DTD.  The table highlights the improvements in accuracy achieved by MoE Jetpack compared to the other methods.", "section": "4.2 Main Results"}, {"figure_path": "Q8Z04XhDdL/tables/tables_14_3.jpg", "caption": "Table 8: Hyper-parameter setting on ViT-T.", "description": "This table lists the hyperparameter settings used for training the Vision Transformer (ViT-T) model on eight different image classification datasets.  The hyperparameters include batch size, warmup epochs, total training epochs, and drop path rate.  These settings were adjusted for each dataset to optimize performance.", "section": "4.1 Experimental Setups"}, {"figure_path": "Q8Z04XhDdL/tables/tables_16_1.jpg", "caption": "Table 9: Contribution values of core and universal experts across network layers.", "description": "This table shows the contribution values of core and universal experts across different layers (MoE Layer 7 to 12) of the MoE Jetpack model.  The contribution values indicate the relative importance of core and universal experts in producing the final output of each layer.  The values demonstrate that the importance of core experts generally increases as the network goes deeper, while the contribution of universal experts decreases. ", "section": "4.4 Analysis"}]