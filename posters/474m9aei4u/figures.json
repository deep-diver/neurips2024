[{"figure_path": "474M9aeI4U/figures/figures_0_1.jpg", "caption": "Figure 1: We propose COrrespondence-guided Video Editing (COVE), which leverages the correspondence information of the diffusion feature to achieve consistent and high-quality video editing. Our method is capable of generating high-quality edited videos with various kinds of prompts (style, category, background, etc.) while effectively preserving temporal consistency in generated videos.", "description": "This figure shows examples of video editing results using the proposed COVE method.  The top row displays the original source videos.  The bottom rows demonstrate various video editing results using different text prompts.  These prompts cover style changes, object replacement, and background changes, showcasing the method's versatility and ability to maintain temporal consistency.", "section": "Introduction"}, {"figure_path": "474M9aeI4U/figures/figures_1_1.jpg", "caption": "Figure 2: Comparison between COVE (our method) and previous methods[11, 73].", "description": "This figure compares the approach of COVE with previous methods for obtaining correspondence information among tokens across frames in video editing. Previous methods rely on pretrained optical flow models to establish a one-to-one correspondence between tokens in consecutive frames.  COVE, in contrast, leverages the inherent diffusion feature correspondence, enabling a more accurate one-to-many correspondence, which is more robust in capturing complex temporal relationships.", "section": "Introduction"}, {"figure_path": "474M9aeI4U/figures/figures_3_1.jpg", "caption": "Figure 3: The overview of COVE. (a). Given a source video, we extract the diffusion feature of each frame using the pre-trained T2I model and calculate the correspondence among tokens (detailed in Figure 4). (b). During the video editing process, we sample the tokens in noisy latent based on correspondence and apply self-attention among them. (c). The correspondence-guided attention can be seamlessly integrated into the T2I diffusion model for consistent and high-quality video editing.", "description": "This figure illustrates the overall pipeline of the COVE model for video editing. It shows three main stages: (a) correspondence calculation using a sliding-window based approach on the diffusion features, (b) utilizing this correspondence information for guided attention and token merging within the self-attention mechanism during the inversion process, and (c) the overall video editing pipeline integrating the correspondence-guided attention into a pre-trained text-to-image diffusion model.  The subfigures break down the process into more manageable steps, highlighting the key components and their interactions.", "section": "3 Method"}, {"figure_path": "474M9aeI4U/figures/figures_4_1.jpg", "caption": "Figure 4: Sliding-window-based strategy for correspondence calculation.", "description": "This figure illustrates the sliding-window-based strategy used for efficient correspondence calculation in the COVE model.  It shows how, instead of comparing a token in one frame to every token in every other frame (computationally expensive), the algorithm only compares it to tokens within a small window in the adjacent frames. This significantly reduces computational cost while still effectively capturing temporal correspondence information. The figure uses a visual example of a cat's face across multiple frames to depict the process, highlighting how the window center adjusts frame by frame, focusing on the most similar regions between frames.", "section": "3.2 Correspondence Acquisition"}, {"figure_path": "474M9aeI4U/figures/figures_6_1.jpg", "caption": "Figure 5: Qualitative results of COVE. COVE can effectively handle various types of prompts, generating high-quality videos. For both global editing (e.g., style transferring and background editing) and local editing (e.g., modifying the appearance of the subject), COVE demonstrates outstanding performance. Results are best-viewed zoomed-in.", "description": "This figure showcases various examples of video editing results achieved by the COVE model.  Each row presents a source video and its edited versions based on different prompts which include style transfer (e.g., Van Gogh style, Cartoon style), background changes (e.g., snowy winter, milky way), and object transformations (e.g., teddy bear to raccoon).  The results highlight the model's ability to maintain both high visual quality and temporal consistency across diverse editing tasks.", "section": "4 Experiment"}, {"figure_path": "474M9aeI4U/figures/figures_7_1.jpg", "caption": "Figure 6: Qualitative comparison of COVE and various state-of-the-art methods. Our method outperforms previous methods across a wide range of source videos and editing prompts, demonstrating superior visual quality and temporal consistency. Results are best-viewed zoomed-in.", "description": "This figure showcases a qualitative comparison between the proposed COVE method and five other state-of-the-art video editing methods (FateZero, RAVE, FRESCO, TokenFlow, and FLATTEN). The comparison is done across three different source videos and editing prompts, highlighting COVE's superior performance in terms of visual quality and temporal consistency.", "section": "4.2 Qualitative Results"}, {"figure_path": "474M9aeI4U/figures/figures_8_1.jpg", "caption": "Figure 7: Ablation study about the correspondence-guided attention and the value of K. w/o means do not apply correspondence-guided attention.", "description": "This figure shows the ablation study on the correspondence-guided attention and the effect of parameter K. The top row shows the source video. The second row shows the results without correspondence-guided attention, highlighting the flickering artifacts. The following rows (K=1, K=3, K=5) show the results with correspondence-guided attention and different values of K, demonstrating that increasing K improves the visual quality up to a certain point, after which further increases yield diminishing returns.", "section": "4.4 Ablation Study"}, {"figure_path": "474M9aeI4U/figures/figures_9_1.jpg", "caption": "Figure 8: Token merging would not impair the quality of edited video.", "description": "This figure shows a comparison of video editing results with and without temporal dimensional token merging.  The top row displays the source video frames. The second row shows the edited video frames without token merging, exhibiting some inconsistencies. The bottom row presents the edited video frames with token merging, demonstrating that the merging process does not negatively impact the quality of the results.", "section": "3.3 Correspondence-guided Video Editing"}, {"figure_path": "474M9aeI4U/figures/figures_14_1.jpg", "caption": "Figure 9: Ablation Study on the window size l.", "description": "This figure shows the ablation study on the window size (l) in the sliding-window-based strategy for correspondence calculation. It compares the results of using different window sizes (l=3, l=9) and without using the sliding window strategy (w/o).  The results demonstrate the impact of the window size on the quality of the edited video, showing that a window size of 9 strikes a balance between accuracy and efficiency.", "section": "4.4 Ablation Study"}, {"figure_path": "474M9aeI4U/figures/figures_15_1.jpg", "caption": "Figure 10: Visualization of the correspondence in long videos. Given a long video, we first obtain the correspondence information (K = 3) through the sliding-window strategy. Then, considering a point in the first frame (the red point in the first image of the second row), we visualize the correspondence (respectively marked in yellow, green, and blue) in each frame.", "description": "This figure visualizes how the sliding-window-based method identifies corresponding tokens across frames in a long video.  It uses a specific token in the first frame as a reference and highlights its corresponding tokens (marked in yellow, green, and blue) in subsequent frames. This demonstrates the accuracy of the method in establishing correspondence for video editing.", "section": "C Visualisation of the Correspondence"}, {"figure_path": "474M9aeI4U/figures/figures_16_1.jpg", "caption": "Figure 11: Ablation Study about correspondence in inversion. Here Without Corr means not applying the correspondence-guided attention during inversion, which suffers blurring and flickering. With Corr means the correspondence-guided attention is applied in both inversion and denoising stages, illustrating satisfying performance.", "description": "This figure shows an ablation study on the impact of correspondence-guided attention during the inversion process in video editing.  The left side demonstrates results without correspondence-guided attention, where videos show blurring and flickering effects, indicating reduced temporal consistency. In contrast, the right side presents results with correspondence-guided attention applied in both inversion and denoising stages.  These results exhibit improved visual quality and temporal consistency, demonstrating the effectiveness of the proposed method.", "section": "3.3 Correspondence-guided Video Editing"}, {"figure_path": "474M9aeI4U/figures/figures_17_1.jpg", "caption": "Figure 12: Qualitative results of our methods. Our method can effectively handle various kinds of prompts, generating high-quality videos. Results are best viewed in zoomed-in.", "description": "This figure shows several example results of applying the COVE method to various videos. Each row represents a different video and prompt, demonstrating the model's ability to generate high-quality results for a wide range of prompts. The results include style transfers (e.g., Van Gogh, comic book styles) and background changes, indicating the model's versatility.", "section": "4.2 Qualitative Results"}, {"figure_path": "474M9aeI4U/figures/figures_18_1.jpg", "caption": "Figure 13: Qualitative results of our methods. Our method can effectively handle various kinds of prompts, generating high-quality videos. Results are best viewed in zoomed-in.", "description": "This figure shows several examples of video editing results using the proposed method (COVE). Each row shows a different source video and the results of applying several different editing prompts (e.g. changing the style or appearance of the person or object in the video). The goal is to demonstrate that COVE can be used to generate high-quality videos with a variety of editing prompts while maintaining temporal consistency.", "section": "4.2 Qualitative Results"}]