[{"figure_path": "Jm2aK3sDJD/tables/tables_2_1.jpg", "caption": "Table 1: Comparative analysis of methods based on evaluation, flexibility, and interpretability. Here, \u2713 denotes the method satisfies the requirement, \u25b3 denotes the method partially satisfies the requirement, and \u00d7 denotes the method does not satisfy the requirement. We compare with SOTA methods including LF-CBM [14], Labo [27] and LM4CV [25].", "description": "This table compares different concept bottleneck model (CBM) methods based on three key aspects: evaluation metrics, flexibility (e.g., the choice of backbone network), and interpretability.  It highlights the strengths and weaknesses of each approach, indicating whether they fully satisfy (\u2713), partially satisfy (\u25b3), or do not satisfy (\u00d7) specific criteria related to controlling information leakage, handling various numbers of concepts, using different backbones, achieving accurate concept prediction, incorporating vision-guided filtering, and providing interpretable decisions.  The comparison includes three state-of-the-art (SOTA) methods as baselines for the proposed VLG-CBM approach.", "section": "2 Related work"}, {"figure_path": "Jm2aK3sDJD/tables/tables_8_1.jpg", "caption": "Table 2: Performance comparison between VLG-CBM, LF-CBM[14], LM4CV[25], LaBo[27] and a random baseline. The random baseline has 1024 neurons for CIFAR10 and CIFAR100, 512 for CUB, 2048 for Places365, and 4096 for ImageNet. The results of LM4CV and LaBo on CUB, Places365, and ImageNet are marked as \"N/A\" because they could not be applied on non-CLIP backbones.", "description": "This table presents a comparison of the performance of the proposed Vision-Language-Guided Concept Bottleneck Model (VLG-CBM) against several state-of-the-art baselines on five image classification datasets.  The metrics used are accuracy at NEC=5 (Acc@5) and average accuracy across different numbers of effective concepts (NECs).  Note that some baseline methods were not applicable to all datasets due to their reliance on specific architectures or backbones.", "section": "5.1 Performance comparison"}, {"figure_path": "Jm2aK3sDJD/tables/tables_8_2.jpg", "caption": "Table 3: Performance comparison on ImageNet and CUB datasets with CLIP-RN50 backbone.", "description": "This table presents a performance comparison between different methods (LF-CBM, LM4CV, LaBo, and VLG-CBM) on the ImageNet and CUB datasets.  The backbone used is CLIP-RN50.  The metrics used are Accuracy at NEC=5 (Acc@5) and Average Accuracy (Avg. Acc) across different NEC values.  VLG-CBM shows superior performance compared to the baseline methods on both datasets and for both metrics.", "section": "5.1 Performance comparison"}, {"figure_path": "Jm2aK3sDJD/tables/tables_9_1.jpg", "caption": "Table 1: Comparative analysis of methods based on evaluation, flexibility, and interpretability. Here, \u2713 denotes the method satisfies the requirement, \u25b3 denotes the method partially satisfies the requirement, and \u00d7 denotes the method does not satisfy the requirement. We compare with SOTA methods including LF-CBM [14], Labo [27] and LM4CV [25].", "description": "This table compares different concept bottleneck model (CBM) methods based on their ability to control information leakage, flexibility in choosing the backbone, accuracy of concept prediction, and overall interpretability.  It uses checkmarks (\u2713), partial checkmarks (\u25b3), and crosses (\u00d7) to indicate whether each method satisfies, partially satisfies, or fails to satisfy these criteria, respectively.  The table highlights the advantages of the proposed VLG-CBM method in comparison to state-of-the-art methods.", "section": "2 Related work"}, {"figure_path": "Jm2aK3sDJD/tables/tables_16_1.jpg", "caption": "Table C.1: Accuracy at NEC@5 and Average accuracy for different confidence threshold T.", "description": "This ablation study analyzes the impact of varying the confidence threshold (T) on the accuracy of the VLG-CBM model.  The threshold determines which bounding boxes from the object detector are included in the concept annotations.  The table shows that there is minimal change in accuracy at NEC=5 and average accuracy across various values of T, indicating that the model is robust to this hyperparameter.", "section": "C Ablation Studies"}, {"figure_path": "Jm2aK3sDJD/tables/tables_17_1.jpg", "caption": "Table 1: Comparative analysis of methods based on evaluation, flexibility, and interpretability. Here, \u2713 denotes the method satisfies the requirement, \u25b3 denotes the method partially satisfies the requirement, and \u00d7 denotes the method does not satisfy the requirement. We compare with SOTA methods including LF-CBM [14], Labo [27] and LM4CV [25].", "description": "This table compares different methods for training concept bottleneck models based on their ability to control information leakage, flexibility in choosing a model backbone, accuracy of concept prediction, whether concept filtering is vision-guided, and the interpretability of the final decision.  The symbols \u2713, \u25b3, and \u00d7 indicate whether a method fully satisfies, partially satisfies, or does not satisfy a given requirement, respectively. The table also includes a comparison to state-of-the-art (SOTA) methods (LF-CBM, LaBo, and LM4CV).", "section": "2 Related work"}, {"figure_path": "Jm2aK3sDJD/tables/tables_22_1.jpg", "caption": "Table 2: Performance comparison between VLG-CBM, LF-CBM[14], LM4CV[25], LaBo[27] and a random baseline. The random baseline has 1024 neurons for CIFAR10 and CIFAR100, 512 for CUB, 2048 for Places365, and 4096 for ImageNet. The results of LM4CV and LaBo on CUB, Places365, and ImageNet are marked as \\\"N/A\\\" because they could not be applied on non-CLIP backbones.", "description": "This table compares the performance of the proposed VLG-CBM model against three state-of-the-art concept bottleneck models (LF-CBM, LaBo, LM4CV) and a random baseline across five image classification datasets (CIFAR10, CIFAR100, CUB, Places365, ImageNet).  The table presents the accuracy at NEC=5 (Acc@5) and the average accuracy across different NEC values.  Note that LaBo and LM4CV are not applicable ('N/A') for all datasets, due to limitations in their architecture or availability.", "section": "5.1 Performance comparison"}, {"figure_path": "Jm2aK3sDJD/tables/tables_22_2.jpg", "caption": "Table 1: Comparative analysis of methods based on evaluation, flexibility, and interpretability. Here, \u2713 denotes the method satisfies the requirement, \u25b3 denotes the method partially satisfies the requirement, and \u00d7 denotes the method does not satisfy the requirement. We compare with SOTA methods including LF-CBM [14], Labo [27] and LM4CV [25].", "description": "This table compares several methods for training concept bottleneck models (CBMs) based on three key aspects: evaluation (control over information leakage, accurate concept prediction, vision-guided concept filtering), flexibility (flexible backbone, unlimited concept numbers), and interpretability (interpretable decision).  Each method (LF-CBM, LaBo, LM4CV, and VLG-CBM) is evaluated using checkmarks representing whether it fully satisfies (\u2713), partially satisfies (\u25b3), or does not satisfy (\u00d7) each criterion.", "section": "2 Related work"}, {"figure_path": "Jm2aK3sDJD/tables/tables_23_1.jpg", "caption": "Table 1: Comparative analysis of methods based on evaluation, flexibility, and interpretability. Here, \u2713 denotes the method satisfies the requirement, \u25b3 denotes the method partially satisfies the requirement, and \u00d7 denotes the method does not satisfy the requirement. We compare with SOTA methods including LF-CBM [14], Labo [27] and LM4CV [25].", "description": "This table compares different methods for training concept bottleneck models (CBM) based on three key aspects: evaluation (control on information leakage, accurate concept prediction, vision-guided concept filtering), flexibility (unlimited concept numbers, flexible backbone), and interpretability (interpretable decision).  It shows that VLG-CBM, the proposed method, is superior in all three aspects compared to existing state-of-the-art methods (LF-CBM, LaBo, LM4CV).", "section": "2 Related work"}]