[{"type": "text", "text": "On the Optimal Time Complexities in Decentralized Stochastic Asynchronous Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Alexander Tyurin Peter Richtarik KAUST\\* AIRI, Skoltech+ KAUST\\* ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We consider the decentralized stochastic asynchronous optimization setup, where many workers asynchronously calculate stochastic gradients and asynchronously communicate with each other using edges in a multigraph. For both homogeneous and heterogeneous setups, we prove new time complexity lower bounds under the assumption that computation and communication speeds are bounded. We develop a new nearly optimal method, Fragile SGD, and a new optimal method, Amelie SGD, that converge under arbitrary heterogeneous computation and communication speeds and match our lower bounds (up to a logarithmic factor in the homogeneous setting). Our time complexities are new, nearly optimal, and provably improve all previous asynchronous/synchronous stochastic methods in the decentralized setup. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We consider the smooth nonconvex optimization problem ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb{R}^{d}}\\Big\\{f(x):=\\mathbb{E}_{\\xi\\sim\\mathcal{D}_{\\xi}}\\left[f(x;\\xi)\\right]\\Big\\},\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $f\\,:\\,\\mathbb{R}^{d}\\times\\mathbb{S}_{\\xi}\\rightarrow\\mathbb{R}$ , and $\\mathcal{D}_{\\xi}$ is a distribution on a non-empty set $\\mathbb{S}_{\\xi}$ . For a given $\\varepsilon>0$ ,we want to find a possibly random point $\\textstyle{\\bar{x}}$ , called an $\\varepsilon.$ -stationary point, such that $\\mathbb{E}[\\|\\nabla f(\\bar{x})\\|^{2}]\\leq\\varepsilon$ We analyze the heterogeneous setup and the convex setup with smooth and non-smooth functions in Sections C and D. ", "page_idx": 0}, {"type": "text", "text": "1.1 Decentralized setup with times ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We investigate the following decentralized asynchronous setup. Assume that we have $n$ workers/nodes with the associated computation times $\\{h_{i}\\}$ , and communications times $\\{\\rho_{i\\to j}\\}$ . It takes less or equal to $h_{i}\\,\\in\\,[0,\\infty]$ seconds to compute a stochastic gradient by the $i^{\\mathrm{th}}$ node, and less or equal $\\rho_{i\\to j}\\in[0,\\infty]$ seconds to send directly a vector $\\boldsymbol{v}\\in\\mathbb{R}^{d}$ from the $i^{\\mathrm{th}}$ node to the $j^{\\mathrm{th}}$ node (it is possible that $h_{i}=\\infty$ and $\\rho_{i\\to j}=\\infty$ ). All computations and communications can be done asynchronously and in parallel. We would like to emphasize that $h_{i}\\in[0,\\infty]$ and $\\rho_{i\\rightarrow j}\\in[0,\\infty]$ are only upper bounds, and the real and effective computation and communication times can be arbitrarily heterogeneous and random. For simplicity of presentation, we assume the upper bounds are static; however, in Section 5.5, we explain that our result can be trivially extended to the case when the upper bounds are dynamic. ", "page_idx": 0}, {"type": "text", "text": "We consider any weighted directed multigraph parameterized by a vector $h\\in\\mathbb{R}^{n}$ such that $h_{i}\\in$ $[0,\\infty]$ , and amatrix of distances $\\{\\rho_{i\\to j}\\}_{i,j}\\in\\mathbb{R}^{n\\times n}$ such that $\\rho_{i\\to j}\\,\\in\\,[0,\\infty]$ for all $i,j\\in[n]$ and ", "page_idx": 0}, {"type": "image", "img_path": "IXRa8adMHX/tmp/4a17d8f45c8f6a91b3712f2b755a861da84858440d0d2cf6acfc78cfdb5f4ce4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: On the left: an example of a multigraph with $n=6$ .The edges with $\\rho_{i\\to j}=\\infty$ are omitted. The shortest distance between nodes 5 and 3 is $\\tau_{5\\to3}=\\rho_{5\\to1}+\\rho_{1\\to2}+\\rho_{2\\to3}$ Note that $\\rho_{5\\rightarrow3}=\\infty$ On the right: an example of a spanning tree that illustrates the shortest paths from every node to node 3. The shortest distance between nodes 6 and 3 is $\\tau_{6\\rightarrow3}=\\infty$ because $\\rho_{6\\rightarrow i}=\\infty$ for all $i\\neq6$ $\\rho_{i\\to i}=0$ for all $i\\in[n]$ . Every worker $i$ is connected to any other worker $j$ withtwoedges $i\\rightarrow j$ and $j\\rightarrow i$ . For this setup, it would be convenient to define the distance of the shortest path from worker $i$ to worker $j$ ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tau_{i\\to j}:=\\displaystyle\\operatorname*{min}_{\\mathrm{path}\\in P_{i\\to j}}\\sum_{(u,v)\\in\\mathrm{path}}\\rho_{u\\to v}\\in[0,\\infty],}\\\\ &{\\mathrm{where~}P_{i\\to j}:=\\left\\lbrace\\left[(k_{1},k_{2}),\\ldots,(k_{m},k_{m+1})\\right]\\big|\\,\\forall m\\in\\mathbb{N}\\forall p\\in[m+1]\\,\\forall k_{p}\\in[n],\\right.}\\\\ &{\\left.k_{1}=i,k_{m+1}=j,\\forall j\\in\\lbrace2,\\ldots,m\\rbrace\\,k_{j-1}\\neq k_{j}\\neq k_{j+1}\\right\\rbrace}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "is the set of all possible paths without loops from worker $i$ to worker $j$ for all $i,j\\in[n]$ . One can easily show that the triangle inequality $\\tau_{i\\to j}\\,\\le\\,\\tau_{i\\to k}+\\tau_{k\\to j}$ holds for all $i,j,k\\in[n]$ . Note that $\\tau_{i\\to j}\\,\\leq\\,\\rho_{i\\to j}$ for all $i,j\\in[n]$ . It is important to distinguish $\\tau_{i\\to j}$ and $\\rho_{i\\to j}$ because it is possible that $\\tau_{i\\to j}<\\rho_{i\\to j}=\\infty$ if workers $i$ and $j$ are connected by an edge $\\rho_{i\\to j}=\\infty$ , and there is a path through other workers (see Fig. 1). ", "page_idx": 1}, {"type": "text", "text": "We work with the following standard assumption from smooth nonconvex stochastic optimization literature. ", "page_idx": 1}, {"type": "text", "text": "Assumption 1. $f$ is diffrentiable and $L$ smooth, i.e., $\\|\\nabla f(x)-\\nabla f(y)\\|\\leq L\\,\\|x-y\\|\\,\\forall x,y\\in\\mathbb{R}^{d}.$ Assumption 2. There exist $f^{*}\\in\\mathbb{R}$ such that $f(x)\\geq f^{*}$ for all $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ ", "page_idx": 1}, {"type": "text", "text": "Assumption3.For all $x~\\in~\\mathbb{R}^{d}$ stochasticgradients $\\nabla f(x;\\xi)$ areunbiasedand $\\sigma^{2}$ -variancebounded, i.e.,. $\\mathbb{E}_{\\xi}[\\nabla f(x;\\xi)]\\,=\\,\\nabla f(x)$ and $\\mathbb{E}_{\\xi}[\\|\\nabla f(x;\\xi)-\\nabla\\dot{f}(x)\\|^{2}]\\,\\le\\,\\sigma^{2}$ , where $\\sigma^{2}\\,\\geq\\,0$ We also assume that computation and communication times are statistically independent of stochastic gradients. ", "page_idx": 1}, {"type": "text", "text": "2 Previous Results ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Time complexity with one worker ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "For the case when $n=1$ and $\\tau_{1\\rightarrow1}=0$ , convergence rates and time complexities of problem (1) are well-understood. It is well-known that the stochastic gradient method (SGD),i.e., $x^{{\\dot{k}}+1}=x^{k}-$ $\\gamma\\nabla f(x^{k};\\xi^{k})$ , where $\\{\\xi^{k}\\}$ are i.i.d. from $\\mathcal{D}_{\\xi}$ ,has the optimal oraclecomplexity $\\Theta(L\\Delta/\\varepsilon+\\sigma^{2}L\\Delta/\\varepsilon^{2})$ (Ghadimi and Lan, 2013; Arjevani et al., 2022). Assuming that the computation time of one stochastic gradient is bounded by $h_{1}$ , we can conclude that the optimal time complexity is ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T_{\\mathrm{single}}^{\\tau=0}:=\\Theta\\left(h_{1}\\times\\left(\\frac{L\\Delta}{\\varepsilon}+\\frac{\\sigma^{2}L\\Delta}{\\varepsilon^{2}}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "seconds in the worst case. ", "page_idx": 1}, {"type": "text", "text": "2.2  Parallel optimization without communication costs ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Assume that $n>1$ and $\\tau_{i\\to j}=0$ for all $i,j\\in[n]$ , and computation times of stochastic stochastic gradients are arbitrarily heterogeneous. The simplest baseline method in this setup is Minibatch SGD, ", "page_idx": 1}, {"type": "table", "img_path": "IXRa8adMHX/tmp/eb9a6f5acade0c12330aa762a313ce89225c75a4cf86db8c7f9c5f5f6cca714f.jpg", "table_caption": ["Table 1: Homogeneous Case (1). The time complexities to get an $\\varepsilon$ -stationary point in the nonconvex setting. We assume that $\\tau_{i\\to j}~=~\\tau_{j\\to i}$ for ali $i,j\\;\\in\\;[n]$ in this table. Abbr.: $\\sigma^{2}$ is defined as $\\mathbb{E}_{\\xi}[\\|\\nabla f(x;\\xi)-\\nabla f(x)\\|^{2}]\\le\\bar{\\sigma}^{2}$ for all $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ \uff0c $L$ is a smoothness constant of $f,\\Delta:=f(x^{0})-f^{*}$ "], "table_footnote": ["(@) The mapping $t^{*}$ isdefined in Defion . transparent that the obtained results are suboptimal. (0 Meaning tht the coresponding time complexity $\\rightarrow\\infty$ $\\operatorname*{max}_{i\\in[n]}h_{i}\\to\\infty$ "], "page_idx": 2}, {"type": "text", "text": "i.e., ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{x}^{k+1}={x}^{k}-\\frac{\\gamma}{n}\\displaystyle\\sum_{i=1}^{n}\\nabla f({x}^{k};{\\xi}_{i}^{k}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\{\\xi_{i}^{k}\\}$ are i.i.d. from $\\mathcal{D}_{\\xi}$ and the gradient $\\nabla f(x^{k};\\xi_{i}^{k})$ is calculated in worker $i$ in parallel. This method waits for stochastic gradients from all workers; thus, it is not robust to \u201cstragglers\" and in the worst case the time complexity of such an algorithm is ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T_{\\mathrm{mini}}^{\\tau=0}:=\\Theta\\left(\\operatorname*{max}_{i\\in[n]}h_{i}\\times\\left(\\frac{L\\Delta}{\\varepsilon}+\\frac{\\sigma^{2}L\\Delta}{n\\varepsilon^{2}}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "which depends on the time $\\operatorname*{max}_{i\\in[n]}h_{i}$ of the slowest worker. There are many other more advanced methods including Picky SGD (Cohen et al., 2021), Asynchronous SGD (e.g., (Recht et al., 2011; Nguyen et al., 2018; Mishchenko et al., 2022; Koloskova et al., 2022)), and Rennala SGD (Tyurin and Richtarik, 2023) that are designed to be robust to workers\u2019 chaotic computation times. Under the assumption that the computation times of the workers are heterogeneous and bounded by $\\{h_{i}\\}$ Tyurin and Richtarik (2023) showed that Rennala SGD is the first method that achieves the optimal timecomplexity ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T_{\\mathrm{Rennala}}^{\\tau=0}:=\\Theta\\left(\\underset{m\\in[n]}{\\operatorname*{min}}\\left[\\left(\\frac{1}{m}\\sum_{i=1}^{m}\\frac{1}{h_{\\pi_{i}}}\\right)^{-1}\\left(\\frac{L\\Delta}{\\varepsilon}+\\frac{\\sigma^{2}L\\Delta}{m\\varepsilon^{2}}\\right)\\right]\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "$\\pi$ $h_{i}:h_{\\pi_{1}}\\leq\\cdot\\cdot\\leq h_{\\pi_{n}}$ $T_{\\mathrm{Rennala}}^{\\tau=0}\\leq$ $T_{\\mathrm{mini}}^{\\tau=0}$ ", "page_idx": 2}, {"type": "text", "text": "2.3  Parallel optimization with communication costs $\\tau_{i\\to j}$ ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We now consider the setup where workers? communication times can not be ignored. This problem leads to a research field called decentralized optimization. This setup is the primary case for us. All $n$ workers calculate stochastic gradients in parallel and communicate with each other. Numerous works consider this setup, and we refer to Yang et al. (2019); Koloskova (2024) for detailed surveys. Typically, methods in this setting use the gossip matrix framework (Duchi et al., 2011; Shi et al., 2015; Koloskova et al., 2021) and get an iteration converge rate that depends on the spectral gap of a mixing matrix. However, such rates do not give the physical time of algorithms (see also Section B). ", "page_idx": 2}, {"type": "text", "text": "Let us consider a straightforward baseline: Minibatch SGD. We can implement (4) in a way that all workers calculate one stochastic gradient (takes at most $\\operatorname*{max}_{i\\in[n]}h_{i}$ seconds) and then aggregate them to one pivot worker $j^{*}$ (takes at most $\\operatorname*{max}_{i\\in[n]}\\tau_{i\\to j^{*}}$ seconds). Then, pivot worker $j^{*}$ calculates ", "page_idx": 2}, {"type": "text", "text": "Table 2: Heterogeneous Case (17). Time complexities to get an $\\varepsilon$ -stationary point in the nonconvex setting. Abbr.: $\\bar{\\sigma}^{2}$ is defined as $\\mathbb{E}_{\\xi}[\\|\\nabla f_{i}(x;\\bar{\\xi})-\\nabla f_{i}(x)\\bar{\\|}^{2}]\\le\\sigma^{2}$ for all $\\bar{x^{\\ast}}\\in\\mathbb{R}^{d},i\\in[n]$ \uff0c $L$ is a smoothness constant of $\\textstyle f={\\frac{1}{n}}\\sum_{i=1}^{n^{-}}f_{i}$ \uff0c $\\Delta:=f(x^{0})-f^{*}$ ", "page_idx": 3}, {"type": "table", "img_path": "IXRa8adMHX/tmp/1484c22641a6ed2a9efca82837699a90898e689c8c7d692f963d5c83b4164b51.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "a new point $x^{k+1}$ and broadcasts it to all workers (takes $\\operatorname*{max}_{i\\in[n]}\\tau_{j^{*}\\to i}$ seconds). One can easily see that the time complexity of such a procedure is4 ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T_{\\mathrm{mini}}:=\\Theta\\left(\\operatorname*{max}\\left\\{\\underset{i,j\\in[n]}{\\operatorname*{max}}\\tau_{i\\to j},\\underset{i\\in[n]}{\\operatorname*{max}}h_{i}\\right\\}\\left(\\frac{L\\Delta}{\\varepsilon}+\\frac{\\sigma^{2}L\\Delta}{n\\varepsilon^{2}}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We can analyze any other asynchronous decentralized method, which will be done with more advanced methods inSection5.4. ", "page_idx": 3}, {"type": "text", "text": "But what is the best possible (optimal) time complexity we can get in the setting from Section 1.1? ", "page_idx": 3}, {"type": "text", "text": "Unlike the setups from Sections 2.1 and 2.2 when the communication times are zero $(\\tau_{i\\to j}=0$ for all $i,j\\in[n])$ , the optimal time complexity and an optimal method for the case $\\tau_{i\\to j}\\geq0$ for all $i,j\\in[n]$ are not known. Our main goal in this paper is to solve this problem. ", "page_idx": 3}, {"type": "text", "text": "3 Contributions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We consider the class of functions that satisfy the setup and the assumptions from Section 1.1 and show that (informally) it is impossible to develop a method that will converge faster than (7) seconds. Next, we develop a new asynchronous stochastic method, Fragile SGD, that is nearly optimal (i.e., almost matches this lower bound; see Table 1 and Corollary 1). This is the first such method. It provably improves on Asynchronous SGD (Even et al., 2024) and all other synchronous and asynchronous methods (Bornstein et al., 2023). We also consider the heterogeneous setup (see Table 2 and Section C), where we discover the optimal time complexity by proving another lower bound and developing a new optimal method, Amelie SGD, with weak assumptions. The developed methods can guarantee the iteration complexity $\\mathcal{O}\\left(L\\Delta/\\varepsilon\\right)$ with arbitrarily heterogeneous random computation and communication times (Theorems 4 and 8). Our findings are extended to the convex setup in Section D, where we developed new accelerated methods, Accelerated Fragile SGD and Accelerated AmelieSGD. ", "page_idx": 3}, {"type": "text", "text": "4 Lower Bound ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In order to construct our lower bound, we consider any (zero-respecting) method that can be represented by Protocol 1. This protocol captures all virtually distributed synchronous and asynchronous methods, such as Minibatch SGD, SWIFT (Bornstein et al., 2023), Asynchronous SGD (Even et al., 2024), and Gradient Tracking (Koloskova et al., 2021). ", "page_idx": 3}, {"type": "text", "text": "For all such methods we prove the following theorem. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1 (Lower Bound; Simplified Presentation of Theorem 19). Consider Protocol $^{\\,l}$ with $\\nabla f(\\cdot;\\cdot)$ We takeany $h_{i}\\geq0$ and $\\tau_{i\\to j}\\geq0$ for all $i,j\\in[n]$ such that $\\tau_{i\\to j}\\leq\\tau_{i\\to k}+\\tau_{k\\to j}$ for all $i,k,j\\in[n]$ . We fix $L,\\Delta,\\varepsilon,\\sigma^{2}>0$ thatsatisfytheinequality $\\varepsilon<c L\\Delta$ forsomeuniversalconstant c. For any (zero-respecting) algorithm, there exists a function $f$ ,which satisfyAssumptions 1, 2 and $f(0)-f^{*}\\leq\\Delta_{*}$ and a stochastic gradient mapping $\\nabla f(\\cdot;\\cdot)$ , which satisfies Assumption 3, such that therequired timetofind $\\varepsilon,$ -solutionis ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Omega\\left(\\frac{1}{\\log n+1}\\frac{L\\Delta}{\\varepsilon}\\operatorname*{min}_{j\\in[n]}t^{*}\\big(\\sigma^{2}/\\varepsilon,[h_{i}]_{i=1}^{n},[\\tau_{i\\to j}]_{i=1}^{n}\\big)\\right)}\\\\ &{\\overset{\\mathrm{Def~2}}{=}\\Omega\\left(\\frac{1}{\\log n+1}\\frac{L\\Delta}{\\varepsilon}\\operatorname*{min}_{j\\in[n]}\\operatorname*{min}_{k\\in[n]}\\operatorname*{max}\\left\\{\\operatorname*{max}\\{\\tau_{\\pi_{j,k}\\to j},h_{\\pi_{j,k}}\\},\\frac{\\sigma^{2}}{\\varepsilon}\\left(\\sum_{i=1}^{k}\\frac{1}{h_{\\pi_{j,i}}}\\right)^{-1}\\right\\}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Where.for $j\\in[n]$ $\\pi_{j}$ isapermutationthat sorts $\\{\\operatorname*{max}\\{\\tau_{i\\to j},h_{i}\\}\\}_{i=1}^{n},i.e.,\\operatorname*{max}\\{\\tau_{\\pi_{j,1}\\to j},h_{\\pi_{j,1}}\\}\\leq$ $\\cdot\\cdot\\leq\\operatorname*{max}\\{\\tau_{\\pi_{j,n}\\to j},h_{\\pi_{j,n}}\\}$ ", "page_idx": 4}, {"type": "text", "text": "Protocol 1 Simplified Presentation of Protocol 8 ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "1: Init $S_{i}=\\emptyset$ (all available information) on worker $i$ for all $i\\in[n]$   \n2: Run the following two loops in each worker in parallel ", "page_idx": 4}, {"type": "text", "text": "3: while True do ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4: Calculate a new point $\\boldsymbol{x}_{i}^{k}$ based on $S_{i}$ (takes 0 seconds)   \n5: Calculate a stochastic gradient $\\nabla f(x_{i}^{k};\\xi)$ (or $\\nabla f_{i}(x_{i}^{k};\\xi))\\ \\ \\ \\xi\\sim{\\mathcal{D}}_{\\xi}$ (takes $h_{i}$ seconds)   \n6: Atomic add $\\nabla f(x_{i}^{k};\\xi)$ (or $\\nabla f_{i}(x_{i}^{k};\\xi))$ to $S_{i}$ (atomic operation, takes O seconds) ", "page_idx": 4}, {"type": "text", "text": "7: end while ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "8: while True do ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "9:  Send(a any vector from $\\mathbb{R}^{d}$ based on $S_{i}$ to any worker $j$ and go to the next step of this loop without waiting (takes $\\tau_{i\\to j}$ seconds to send; worker $j$ adds this vector to $S_{j}$ 10: end while ", "page_idx": 4}, {"type": "text", "text": "", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "(a): When we prove the lower bounds, we allow algorithms to send as many vectors as they want in parallel fromworker $i$ toworker $j$ for all $i\\neq j\\in[n]$ ", "page_idx": 4}, {"type": "text", "text": "The intuition and meaning of the formula (8) is discussed in Section 5.2. Note that if we take $n=1$ and $\\tau_{1\\rightarrow1}=0$ our lower bound reduces to the lower bound (3) up to a log factor. Moreover, if we take $n>1$ and $\\tau_{i\\to j}=0$ for all $i,j\\in[n]$ , then (8) reduces to (5) up to a log factor. Thus, (8) is nearly consistent with the lower bounds from (Arjevani et al., 2022; Tyurin and Richtarik, 2023). We get an extra $\\log n$ factor due to the generality of our setup. The reason is technical, and we explain it in Section E.5. In a nutshell, the lower problem reduces to the analysis of the concentration of the timeseries $y^{T}:=\\operatorname*{min}_{j\\in[n]}y_{j}^{T}$ and $\\begin{array}{r}{y_{j}^{T}:=\\operatorname*{min}_{i\\in[n]}\\left\\{y_{i}^{T-1}+h_{i}\\eta_{i}^{T}+\\tau_{i\\to j}\\right\\}}\\end{array}$ where $y_{i}^{0}=0$ for all $i\\,\\in\\,[n]$ , and $\\{\\eta_{i}^{k}\\}$ are i.i.d. geometric random variables. This analysis is not trivial due to the $\\operatorname*{min}_{i\\in[n]}$ operations. Virtually all previous works that analyzed lower bounds did not have such a problem because they analyzed time series with a sum structure (e.g.. $\\bar{y}^{T}:=\\bar{y}^{T-1}+{\\varrho}^{T}$ , where $\\{\\varrho^{k}\\}$ are some random variables, and $\\bar{y}^{0}=0$ ", "page_idx": 4}, {"type": "text", "text": "Let us define an auxiliary function to simplify readability. ", "page_idx": 4}, {"type": "text", "text": "Definition 2 (Equilibrium Time). A mapping $t^{*}\\ :\\ \\mathbb{R}_{\\geq0}\\,\\times\\,\\mathbb{R}_{\\geq0}^{n}\\,\\times\\,\\mathbb{R}_{\\geq0}^{n}\\,\\rightarrow\\,\\mathbb{R}_{\\geq0}$ with inputs $s$ (scalar), $[h_{i}]_{i=1}^{n}$ (vector), and $[\\bar{\\tau}_{i}]_{i=1}^{n}$ (vector) is called the equilbrium time if it is defined as follows. Find a permutation $\\pi$ that sorts max $\\mathfrak{c}\\{\\bar{\\tau}_{i},h_{i}\\}$ as $\\operatorname*{max}\\{\\bar{\\tau}_{\\pi_{1}},h_{\\pi_{1}}\\}\\leq\\cdots\\leq\\operatorname*{max}\\{\\bar{\\tau}_{\\pi_{n}},h_{\\pi_{n}}\\}$ Then the mapping returns the value ", "page_idx": 4}, {"type": "equation", "text": "$$\nt^{*}(s,[h_{i}]_{i=1}^{n},[\\overline{{\\tau}}_{i}]_{i=1}^{n})\\equiv\\operatorname*{min}_{k\\in[n]}\\operatorname*{max}\\left\\{\\operatorname*{max}\\{\\bar{\\tau}_{\\pi_{k}},h_{\\pi_{k}}\\},s\\left(\\sum_{i=1}^{k}\\frac{1}{h_{\\pi_{i}}}\\right)^{-1}\\right\\}\\in[0,\\infty].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "5 New Method: Fragile SGD ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We introduce a novel optimization method characterized by time complexities that closely align with the lower bounds established in Section 4. Our algorithms leverage spanning trees. A spanning tree is a tree (undirected unweighted graph) encompassing all workers. The edges of spanning trees are virtual and not related to the edges defined in Section 1.1 (see Fig. 1). ", "page_idx": 4}, {"type": "text", "text": "1: Input: starting point $x^{0}$ , stepsize $\\gamma$ batch size $S$ , pivot worker $j^{*}$ , spanning trees $\\overline{{s t}}$ and $\\overline{{s t}}_{\\mathrm{bc}}$   \n2: Start Process 0 (Algorithm 3) in worker $j^{*}$   \n3: Start Process $i$ (Algorithm 4) in all workers for all $i\\in[n]$ (including worker $j^{*}$   \nAlgorithm 3 Process 0 (running in worker $j^{*}$   \n1: for $k=0,1,\\ldots,K-1$ do   \n2: Send $x^{k}$ to Process $j^{*}$ takes $\\tau_{j^{\\ast}\\to j^{\\ast}}=0$ seconds   \n3: Init $(g^{k},s^{k})=(0,0)$   \n4: while $s<S$ do   \n5: Wait for a message $(g_{j^{*},\\mathrm{send}}^{k},s_{j^{*},\\mathrm{send}}^{k})$ from Proces \\*   \n6: $g^{k}=g^{k}+g_{j^{*},s}^{k}$ $s^{k}=s^{k}+s_{j^{*}}^{k}$   \n7: end while   \n8: $\\begin{array}{r}{x^{k+1}=x^{k}-\\frac{\\gamma}{s^{k}}g^{k}}\\end{array}$   \n9: end for   \nAlgorithm 4 Process i (running in worker i)   \n1: Init (,ex,sex) $(g_{i,\\mathrm{next}}^{k},s_{i,\\mathrm{next}}^{k})=(0,0)$ for all $k\\in\\{0,\\ldots,K-1\\}^{(\\mathrm{a})}$ $k_{\\operatorname*{max}}=0$   \n2: Run the following three functions in parallel   \n3: function BroadcastFurtherAndCalculateStochasticGradients   \n4: while True   \n5: Get a new point $x^{\\bar{k}}$ sent by Process $\\mathfrak{n e x t}_{\\overline{{s t}}_{\\mathrm{bc}},j^{*}}(i)$ $\\vartriangleright\\bar{\\v{k}}$ is not necessarily equals to the current $k$ from Alg. 3   \n6: Atomic update $k_{\\operatorname*{max}}=\\operatorname*{max}\\{\\bar{k},k_{\\operatorname*{max}}\\}$   \n7: for all $p$ such that $\\mathrm{next}_{s t_{\\mathrm{bc}},j^{*}}(p)=i$ do broadcasts $x^{\\Bar{k}}$ further   \n8: Send(b) $x^{\\bar{k}}$ to Process $p$ and go to the next step without waiting $\\vartriangleright$ $\\tau_{i\\to p}$   \n9: end for   \n10: while not received a new point > immediately stops the loop when receives a new point   \n12 Calculate $\\nabla f(x^{\\bar{k}};\\xi),\\quad\\xi\\sim\\mathcal{D}$ takes at most $h_{i}$ second   \nRun atomic add g,nex $g_{i,\\mathrm{next}}^{\\bar{k}}=g_{i,\\mathrm{next}}^{\\bar{k}}+\\nabla f(x^{\\bar{k}};\\xi),s_{i,\\mathrm{next}}^{\\bar{k}}=s_{i,\\mathrm{next}}^{\\bar{k}}+1$   \n13: end while   \n14: end while   \n15: end function   \n16: function Receive VectorsFromPrevious Workers   \n17: while True   \n18: Wait for a message (gp $(g_{p,\\mathrm{send}}^{\\hat{k}},s_{p,\\mathrm{send}}^{\\hat{k}})$ from any Process $p$ such that $\\mathfrak{n e x t}_{s t,j^{*}}(p)=i$   \n19: Run atomic add g,ext $\\begin{array}{r}{g_{i,\\mathrm{next}}^{\\hat{k}}=g_{i,\\mathrm{next}}^{\\hat{k}}+g_{p,\\mathrm{send}}^{\\hat{k}},s_{i,\\mathrm{next}}^{\\hat{k}}=s_{i,\\mathrm{next}}^{\\hat{k}}+s_{p}^{\\hat{k}}}\\end{array}$ send   \n20: Atomic update $k_{\\operatorname*{max}}=\\operatorname*{max}\\{\\hat{k},k_{\\operatorname*{max}}\\}$   \n21: end while   \n22: end function   \n23: function SendVectorsToNextWorker   \n24: while True   \n25: Atomic init gmd, s,sd $s_{i,\\mathrm{send}}^{k_{\\mathrm{max}}}=g_{i,\\mathrm{next}}^{k_{\\mathrm{max}}},$ gi,nex, Si,next and reset 9,mex $g_{i,\\mathrm{next}}^{k_{\\mathrm{max}}}=0$ $s_{i,\\mathrm{next}}^{k_{\\mathrm{max}}}=0$   \n26: Send(b) $(g_{i,\\mathrm{send}}^{k_{\\mathrm{max}}},s_{i,\\mathrm{send}}^{k_{\\mathrm{max}}})$ sa) to Process nextt, (i) and wait take at most $\\tau_{i\\to\\mathrm{next}_{\\overline{{s\\,t}},j}\\,*}\\left(i\\right)$ seconds to wait   \n27: end while   \n28: end function ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "(a): To simplify the listing of the algorithm, we assume here that a worker can store $K$ auxiliary vectors in the memory. One can see that it's not necessary, and it is sufcient to maintain only one vector gmax In particular, it is sufficient to modify the logic of Lines 12 and 19, where we run the add operations only if $\\bar{k}=k_{\\operatorname*{max}}$ and $\\hat{k}=k_{\\mathrm{max}}$ . The efficient implementation has $O(d)$ foats memory complexity per worker. (b): BroadcastFurtherAndCalculateStochasticGradients and SendVectorsToNextWorker may try to send through the same edge. In this case, we can interleave their communications and decrease the speed of each line by at most two. ", "page_idx": 5}, {"type": "text", "text": "Definition 3 (mapping $\\mathsf{n e x t}_{T,j}(i))$ . Take a spanning tree $T$ and fix any worker $j\\in[n]$ . For $i=j$ we define $\\mathtt{n e x t}_{T,j}(i)=0$ . For ali $i\\neq j\\in[n]$ , we define $\\mathrm{next}_{T,j}(i)$ as the index of the next worker on the path of the spanning tree $T$ from worker $i$ to worker $j$ ", "page_idx": 6}, {"type": "text", "text": "Our new method, Fragile SGD, is presented in Algorithms 2, 4, and 3. While Fragile SGD seems to be lengthy, the idea is pretty simple. All workers do three jobs in parallel: calculate stochastic gradients, receive vectors, and send vectors through spanning trees. A pivot worker aggregates all stochastic gradientsin $g^{k}$ and, at somemoment,does $\\overleftarrow{x^{k+1}}=x^{k}-\\gamma g^{k}$ The algorithms formalize this idea. ", "page_idx": 6}, {"type": "text", "text": "Algorithm 2 requires a starting point $x^{0}$ , a stepsize $\\gamma_{:}$ a batch size $S$ , the index $j^{*}$ of a pivot worker, and spanning trees $\\overline{{s t}}$ and $\\overline{{s t}}_{\\mathrm{bc}}$ for the input. We need two spanning $\\overline{{s t}}_{\\mathrm{bc}}$ and $\\overline{{s t}}$ trees because, in general, the fastest communication of a vector from $j^{*}$ to $i$ and from $i$ to $j^{*}$ should be arranged through two different paths. Algorithm 2 starts $n+1$ processes running in parallel. Note that the pivot worker $j^{*}$ runs two parallel processes, called Process O and Process $j^{*}$ , and any other worker $i$ runs one Process $i$ . Process O broadcasts a new point $x^{k}$ through $\\overline{{s t}}_{\\mathrm{bc}}$ to all other processes and goes to the loop where it waits for messages from Process $j^{*}$ . Process $i$ starts three functions that will be running in parallel: i) the first function's job is to receive a new point, broadcast it further, and start the calculation of stochastic gradients, ii) the second function receives stochastic gradients from all previous processes that are sending vectors to worker $j^{*}$ , ii) the third function sends vectors the next worker on the path to $j^{*}$ . By the definition of $\\mathfrak{n e x t}_{s t,j^{*}}(\\cdot)$ , all calculated stochastic vectors are sent to worker $j^{*}$ , where they are first aggregated in Process $j^{*}$ , and then, since $\\mathrm{next}_{s t,j^{*}}(j^{*})=0$ Proces $j^{*}$ $g_{j^{*},\\mathrm{send}}^{k}$ themmentwhnthnberf stochastic gradients $s^{k}$ aggregated in $g^{k}$ is greater or equal to $S$ When it happens, the loop stops, and Process O does a gradient-like step. The structure of the algorithm and the idea of spanning trees resemble the ideas from (Vogels et al., 2021; Tyurin and Richtarik, 2023). The main observation is that this algorithm is equivalent to $\\begin{array}{r}{x^{k+1}=x^{k}-\\frac{\\gamma}{s^{k}}\\sum_{i=1}^{s^{k}}\\nabla f(x^{k};\\xi_{i}^{k})}\\end{array}$ , where $s^{k}\\geq S$ and $\\{\\xi_{i}^{k}\\}$ are i.i.d. samples. Note that all stochastic gradients calculated at points $x^{0},\\ldots,x^{k-1}$ will be ignored in the $k^{\\mathrm{th}}$ iteration of Algorithm 3. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4. Let Assumptions 1, 2, and $3$ hold. We take $\\gamma=1/2L$ , batch size $S=\\operatorname*{max}\\lbrace\\left\\lceil\\sigma^{2}/\\varepsilon\\right\\rceil,1\\rbrace$ any pivot worker $j^{*}\\in[n]$ , and any spanning trees $\\overline{{s t}}$ and $\\overline{{s t}}_{\\mathrm{bc}}$ in Algorithm 2. For all $K\\geq16L\\Delta/\\varepsilon$ we get $\\begin{array}{r}{\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\|\\nabla f(x^{k})\\right\\|^{2}\\right]\\leq\\varepsilon}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "The proof is simple and uses standard techniques from (Lan, 2020; Khaled and Richtarik, 2022). The result of the theorem holds even if $h_{i}=\\infty$ and $\\tau_{i\\to j}=\\infty$ for all $i,j\\in[n]$ because $h_{i}$ and $\\tau_{i\\to j}$ are only upper bounds on the real computation and communications speeds. In Algorithm 3, each iteration $k$ can be arbitrarily slow, and still, the result of Theorem 4 holds and the method converges after $O(L\\Delta/\\varepsilon)$ iterations. The next result gives time complexity guarantees for our algorithm. ", "page_idx": 6}, {"type": "text", "text": "Theorem 5. Consider the assumptions and the parameters from Theorem 4. For any pivot worker $j^{*}\\in[n]$ andspanningtrees $\\overline{{s t}}$ and $\\overline{{s t}}_{\\mathrm{bc}}$ ,Algorithm2converges after atmost ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Theta\\left(\\frac{L\\Delta}{\\varepsilon}t^{*}(\\sigma^{2}/\\varepsilon,[h_{i}]_{i=1}^{n},[\\mu_{i\\to j^{*}}+\\mu_{j^{*}\\to i}]_{i=1}^{n})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "seconds, where $\\mu_{i\\to j^{*}}$ $(\\mu_{j^{*}\\to i})$ is an upper bound on the times required to send a vector from worker $i$ to worker $j^{*}$ (from worker $j^{*}$ to worker $i$ )along thespanning tree $\\overline{{s t}}$ (spanning tree $\\overline{{s t}}_{\\mathrm{bc}},$ ", "page_idx": 6}, {"type": "text", "text": "Note that our method does not need the knowledge of $\\{h_{i}\\}$ and $\\{\\mu_{i\\to j}\\}$ to guarantee the time complexity rate, and it automatically obtains it. ", "page_idx": 6}, {"type": "text", "text": "Corollary 1. Consider the assumptions and the parameters from Theorem 5. Let us take a pivot worker $j^{*}=\\arg\\operatorname*{min}_{j\\in[n]}t^{*}(\\sigma^{2}/\\varepsilon,[h_{i}]_{i=1}^{\\dot{n}},[\\tau_{i\\rightarrow j}+\\tau_{j\\rightarrow i}]_{i=1}^{n})$ andaspanningtree $\\overline{{s t}}$ (spanning tree $\\overline{{s t}}_{\\mathrm{bc}}$ that connectseveryworker $i$ to worker $j^{*}$ (worker $j^{*}$ to every worker $i$ )withtheshortestdistance $\\tau_{i\\to j^{*}}\\left(\\tau_{j^{*}\\to i}\\right)$ .Then Algorithm 2converges after at most ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{*}:=\\Theta\\left(\\frac{L\\Delta}{\\varepsilon}\\operatorname*{min}_{j\\in[n]}t^{*}(\\sigma^{2}/\\varepsilon,[h_{i}]_{i=1}^{n},[\\tau_{i\\to j}+\\tau_{j\\to i}]_{i=1}^{n})\\right)}\\\\ &{\\quad\\overset{\\mathrm{Def~2}}{=}\\Theta\\left(\\frac{L\\Delta}{\\varepsilon}\\operatorname*{min}_{j\\in[n]}\\operatorname*{min}_{k\\in[n]}\\operatorname*{max}\\left\\{\\operatorname*{max}\\{\\tau_{\\pi_{j,k}\\to j}+\\tau_{j\\to\\pi_{j,k}},h_{\\pi_{j,k}}\\},\\frac{\\sigma^{2}}{\\varepsilon}\\left(\\sum_{i=1}^{k}\\frac{1}{h_{\\pi_{j,i}}}\\right)^{-1}\\right\\}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "seconds, where, for all $j\\in[n],\\pi_{j},$ . is a permutation that sorts $\\{\\operatorname*{max}\\{\\tau_{i\\to j}+\\tau_{j\\to i},h_{i}\\}\\}_{i=1}^{n}$ ", "page_idx": 6}, {"type": "text", "text": "This corollary has better time complexity guarantees than Theorem 5 because, by the definition of $\\tau_{i\\to j}$ $\\tau_{i\\to j}\\leq\\mu_{i\\to j}$ for al $i,j\\in[n]$ . However, it requires the particular choice of a pivot worker and spanning trees. ", "page_idx": 7}, {"type": "text", "text": "5.1 Discussion ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Comparing the lower bound (7) with the upper bound (11), one can see that Fragile SGD has a nearly optimal time complexity. If we ignore the $\\log n$ factor in (7) and assume that $\\tau_{i\\to j}=\\tau_{j\\to i}$ for all $i,j\\in[n]$ , which is a weak assumption in many applications, then Fragile SGD is optimal. ", "page_idx": 7}, {"type": "text", "text": "Unlike most works (Even et al., 2024; Lian et al., 2018; Koloskova et al., 2021) in the decentralized setting, our time complexity guarantees do not depend on the spectral gap of the mixing matrix that defines the topology of the multigraph. The structure of the multigraph is coded in the times $\\{\\tau_{i\\to j}\\}$ We believe that this is an advantage of our guarantees since (11) defines a physical time instead of an iteration rate that depends on the spectral gap. ", "page_idx": 7}, {"type": "text", "text": "5.2  Interpretation of the upper and lower bounds (11) and (7) ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "One interesting property of our algorithm is that some workers will potentially never contribute to optimizationbecause either theircomputations aretooslow orcommunicationtimes to $j^{*}$ aretoo large. Thus, only a subset of the workers should work to get the optimal time complexity! ", "page_idx": 7}, {"type": "text", "text": "Assume in this subsection that the computation and communication are fixed to $\\{h_{i}\\}$ and $\\{\\tau_{i\\to j}\\}$ One can see that (12) is the $\\mathrm{min}_{j\\in[n]}\\,\\mathrm{min}_{k\\in[n]}$ over some formula. In view of our algorithm, an index $j^{*}$ that minimizes in (12) is the index of a pivot worker that is the most \u201ccentral\" in the multigraph.  An index $k^{*}$ that minimizes $\\operatorname*{min}_{k\\in[n]}$ defines a set of workers $\\{\\pi_{j^{*},1},\\ldots\\pi_{j^{*},k^{*}}\\}$ that can potentially contribute to optimization. The algorithm and and the time complexity will not depend on workers $\\{\\pi_{j^{\\ast},k^{\\ast}+1},\\ldots\\pi_{j^{\\ast},n}\\}$ because they are too slow or they are too far from worker $j^{*}$ . Thus, up to a constant factor, we have $T_{*}\\ =\\ L\\Delta/_{\\varepsilon}\\operatorname*{max}\\left\\{\\,\\operatorname*{max}\\{\\tau_{\\pi_{j^{*}\\,,k^{*}}\\to j^{*}}\\ +\\right.$ $\\begin{array}{r}{\\tau_{j^{*}\\to\\pi_{j^{*},k^{*}}},h_{\\pi_{j^{*},k^{*}}}\\bigr\\},\\sigma^{2}/\\varepsilon\\bigr(\\sum_{i=1}^{k^{*}}1_{\\left/h_{\\pi_{j^{*},i}}\\right)}^{-1}\\bigr\\}}\\end{array}$ where $\\tau_{\\pi_{j^{*},k^{*}}\\to j^{*}}+\\tau_{j^{*}\\to\\pi_{j^{*},k^{*}}}$ is the timeraguired to communicate with the farthest worker that can conributeto optimization, $h_{\\pi_{j^{*},k^{*}}}$ is the computation time of the slowest worker that can contribute to optimization, and $\\begin{array}{r}{\\sigma^{2}/\\varepsilon\\big(\\sum_{i=1}^{k^{*}}{1/h_{\\pi_{j}*}}_{,i}\\big)^{-1}}\\end{array}$ is the time required to \u201celiminate\u201d enough noise before the algorithm does an update of $x^{k}$ ", "page_idx": 7}, {"type": "text", "text": "5.3 Limitations ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To get the nearly optimal complexity, it is crucial to select the right pivot worker $j^{*}$ and spanning trees according to the rules of Corollary 1, which depend on the knowledge of the bounds of times. For now, we believe that is this a price for the optimality. Note that Theorem 5 does not require this knowledge and it works with any $j^{*}$ and any spanning tree; thus, we can use any heuristic to estimate an optimal $j^{*}$ and optimal spanning trees. One possible strategy is to estimate the performance of workers and the communication channels using load testings. ", "page_idx": 7}, {"type": "text", "text": "5.4  Comparison with previous methods ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Let us discuss the time complexities of previous methods. Note that none of the previous methods can converge faster than (7) due to our lower bound. First, consider (6) of Minibatch SGD. This time complexity depends on the slowest computation time $\\operatorname*{max}_{i\\in[n]}h_{i}$ and the slowest communication times $\\operatorname*{max}_{i,j\\in[n]}\\tau_{i\\to j}$ In the asynchronous setup, it is possible that one the workers is a straggler, i.e., $\\operatorname*{max}_{i\\in[n]}{\\dot{h}}_{i}\\approx\\infty$ , and Minibatch SGD can be arbitrarily slow. Our time complexities (10) and (12) are robust to stragglers, and ignore them. Assume the last worker $n$ is a straggler and $h_{n}=\\infty$ \uff0c then one can take permutations with $\\pi_{j,n}=n$ for all $j\\in[n]$ , and the minimum operator $\\operatorname*{min}_{k\\in[n]}$ in (12) will not choose $k=n$ because ma $\\mathfrak{u x}\\{\\tau_{\\pi_{j,n}\\to j}+\\tau_{j\\to\\pi_{j,n}},h_{\\pi_{j,n}}\\}=\\infty$ for all $j\\in[n]$ ", "page_idx": 7}, {"type": "text", "text": "We now consider a recent work by Even et al. (2024), where the authors analyzed Asynchronous SGD in the decentralized setting. In the homogeneous setting, their converge rate depends on the maximum compute delay and, thus, is not robust to stragglers. For the case $\\tau_{i\\to j}=0$ our time complexity (11) reduces to (5). At the same time, it was shown (Tyurin and Richtarik, 2023) that the time complexity of Asynchronous SGD for $\\tau_{i\\to j}=0$ is strongly worse than (5); thus, the result by Even et al. (2024) is suboptimal in our setting even if $\\tau_{i\\to j}=0$ for all $i,j\\in[n]$ . The papers by Bornstein et al. (2023); Lian et al. (2018) also consider the same setting, and they share a logic in that they sample a random worker and wait while it is calculating a stochastic gradient. If one of the workers is a straggler, they can wait arbitrarily long, while our method automatically ignores slow computations. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5.5   Time complexity with dynamic bounds ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We can easily generalize Theorem 5 to the case when bounds on the times are not static. ", "page_idx": 8}, {"type": "text", "text": "Theorem 6.Consider the assumptions and the parameters from Theorem 4.In each iteration $k$ of Algorithm 3, the computation times of worker $i$ are bounded by $h_{i}^{k}$ . Let us fix any pivot worker $j^{*}\\in[n]$ and any spanning trees $\\overline{{s t}}$ and $\\overline{{s t}}_{\\mathrm{bc}}$ . Then Algorithm 2 converges after at most ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Theta\\left(\\sum_{k=0}^{\\lceil16L\\Delta/\\varepsilon\\rceil}t^{*}\\big(\\sigma^{2}/\\varepsilon,\\big[h_{i}^{k}\\big]_{i=1}^{n},\\big[\\mu_{i\\to j^{*}}^{k}+\\mu_{j^{*}\\to i}^{k}\\big]_{i=1}^{n}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "seconds, where $\\mu_{i\\to j^{*}}^{k}\\left(\\mu_{j^{*}\\to i}^{k}\\right)$ is an upper bound ontimes requiredto send avectorfromworker i to worker $j^{*}$ (from worker $j^{*}$ to worker $i$ )along the spanning tree $\\overline{{s t}}$ (spanning tree $\\overline{{s t}}_{\\mathrm{bc}}$ )in iteration $k$ of Algorithm 3. ", "page_idx": 8}, {"type": "text", "text": "This result is more general than (10), and it shows that our method is robust to changing com$\\{h_{i}^{k}\\}$ $\\{\\mu_{i\\to j}^{k}\\}$ $n$ $j^{*}$   \ni.e., $\\operatorname*{max}\\{h_{n}^{1},\\mu_{n\\to j^{*}}^{1},\\mu_{j^{*}\\to n}^{1}\\}\\approx\\infty$ then u method wilignoreit,but if $\\operatorname*{max}\\{h_{n}^{2},\\mu_{n\\to j^{*}}^{2},\\mu_{j^{*}\\to n}^{2}\\}$   \nis small in the second iteration, then our method can potentially use the stochastic gradients from   \nworker $n$ ", "page_idx": 8}, {"type": "text", "text": "6 Example: Line or Circle ", "text_level": 1, "page_idx": 8}, {"type": "image", "img_path": "IXRa8adMHX/tmp/6a0fc7c2584791b20d704b57210cb5f7c7af718763e39f4a2cdea8286ac6f008.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 2: Line with $\\rho_{i+1\\to i}\\,=\\,\\rho_{i\\to i+1}\\,=\\,\\rho$ for all $i\\,\\in\\,[n\\mathrm{~-~}1]$ \uff0c $\\rho_{i\\to j}~=~\\infty$ otherwise. For all $i\\neq j\\in[n]$ , edges $i\\rightarrow j$ and $j\\rightarrow i$ are merged and visualized with one undirected edge. ", "page_idx": 8}, {"type": "text", "text": "Let us consider Line graphs where we can get more explicit and interpretable formulas for (11). We analyze ND-Mesh, ND-Torus, and Star graphs in Section A. Surprisingly, even in some simple cases like Line or Star graphs, as far as we know, we provide new time complexity results and insights. In Section J, we show that our theoretical results are supported by computational experiments. ", "page_idx": 8}, {"type": "text", "text": "We take a Line graph with the computation speeds $h_{i}=h$ for all $i\\in[n]$ , and the communication speeds of the edges $\\rho_{i\\to i+1}=\\rho_{i+1\\to i}=\\rho$ for all $i\\in[n-1]$ and $\\rho_{i\\to j}=\\infty$ for all other $i,j\\in[n]$ One can easily show the time required to send a vector between two workers. $i,j\\,\\in\\,[n]$ equals $\\tau_{i\\to j}=\\tau_{j\\to i}=\\rho|i-j|$ . See an example with $n=7$ in Fig. 2. We can substitute these values to (11) and get ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T_{\\mathrm{line}}=\\frac{L\\Delta}{{\\varepsilon}}\\underset{j\\in[n]}{\\mathrm{min}}\\,\\underset{k\\in[n]}{\\mathrm{min}}\\,\\underset{}{\\mathrm{max}}\\left\\{\\operatorname*{max}\\{\\rho|j-\\pi_{j,k}|,h\\},\\frac{\\sigma^{2}h}{{\\varepsilon}k}\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\pi_{j,1}=j,\\pi_{j,2},\\pi_{j,3}=j+1,j-1$ or $\\pi_{j,2},\\pi_{j,3}=j-1,j+1$ (only for $n-1\\geq j\\geq2)$ and $n$ is od, then, larly, $\\begin{array}{r}{j^{*}=\\frac{n-1}{2}+1}\\end{array}$ minimizes $\\operatorname*{min}_{j\\in[n]}$ and $T_{\\mathrm{line}}=$ ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{L\\Delta}{\\varepsilon}\\underset{d\\in\\{0,\\ldots,\\frac{n-1}{2}\\}}{\\operatorname*{min}}\\operatorname*{max}\\left\\{\\rho d,h,\\frac{\\sigma^{2}h}{\\varepsilon(2d+1)}\\right\\}\\simeq\\frac{L\\Delta}{\\varepsilon}\\left[h+\\left\\{\\begin{array}{l l}{\\sigma^{2}h/\\varepsilon,}&{\\mathrm{if~}\\sqrt{\\sigma^{2}h/\\varepsilon\\rho}\\leq1,}\\\\ {\\sqrt{\\rho\\sigma^{2}h/\\varepsilon},}&{\\mathrm{if~}n>\\sqrt{\\sigma^{2}h/\\varepsilon\\rho}>1,}\\\\ {\\sigma^{2}h/n\\varepsilon,}&{\\mathrm{if~}\\sqrt{\\sigma^{2}h/\\varepsilon\\rho}\\geq n}\\end{array}\\right.\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "According to (15), there are three time complexity regimes: i) slow communication, i.e., $\\sqrt{\\sigma^{2}h/\\varepsilon\\rho}\\le$ 1, this inequality means that $\\rho$ is so large, that communication between workers will not increase the convergence speed, and the best strategy is to work with only one worker!, i) medium communication, i.e., $n>\\sqrt{\\sigma^{2}h/\\varepsilon\\rho}>1$ , more than one worker will participate in the optimization proces; however, not all of them!, some workers will not contribute since their distances $\\tau_{j}*_{\\rightarrow}$ . to the pivot worker $j^{*}$ are large, ii) fast communication, i.e., ${\\sqrt{\\sigma^{2}h/\\varepsilon\\rho}}\\geq n$ , all $n$ workers will participate in optimization because $\\rho$ is small. ", "page_idx": 9}, {"type": "text", "text": "As far as we know, the result (15) is new even for such a simple structure as a line. Note that these regimes are fundamental and can not be improved due to our lower bound (up to logarithmic factors). For Circle graphs, the result is the same up to a constant factor. ", "page_idx": 9}, {"type": "text", "text": "7 Heterogeneous Setup ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In Section C (in more details), we consider and analyze the problem ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb{R}^{d}}\\Big\\{f(x):=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}_{\\xi_{i}\\sim\\mathcal{D}_{i}}\\left[f_{i}(x;\\xi_{i})\\right]\\Big\\},\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where $f_{i}\\,:\\,\\mathbb{R}^{d}\\times\\mathbb{S}_{\\xi_{i}}\\rightarrow\\mathbb{R}^{d}$ and $\\xi_{i}$ are random variables with some distributions $\\mathcal{D}_{i}$ on $\\mathbb{S}_{\\xi_{i}}$ . For all $i\\in[n]$ , worker $i$ can only acces $f_{i}$ . We show that the optimal time complexity is ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Theta\\left(\\frac{L\\Delta}{\\varepsilon}\\operatorname*{max}\\left\\{\\underset{i,j\\in[n]}{\\operatorname*{max}}\\mu_{i\\to j},\\underset{i\\in[n]}{\\operatorname*{max}}h_{i},\\frac{\\sigma^{2}}{n\\varepsilon}\\left(\\frac{1}{n}\\sum_{i=1}^{n}h_{i}\\right)\\right\\}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "in the heterogeneous setting achieved by a new method, Amelie SGD (Algorithm 5). Amelie SGD is closely related to Rennala SGD but with essential algorithmic changes to make it work with heterogeneous functions. The obtained complexity (16) is worse than (12), which is expected because the heterogeneous setting is more challenging than the homogeneous setting. ", "page_idx": 9}, {"type": "text", "text": "8 Highlights of Experiments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In Section J, we present experiments with quadratic optimization problems, logistic regression, and a neural network to substantiate our theoretical findings. Here, we focus on highlighting the results from the logistic regression experiments: ", "page_idx": 9}, {"type": "image", "img_path": "IXRa8adMHX/tmp/e9b7fd0acb2caed3b64a1308d8660401b2ddc80403df7ca5539acffc66b6c547.jpg", "img_caption": ["Figure 3: The communication time $\\rho=10$ seconds (Slow communication) in 2D-Mesh "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "On MNIST dataset (LeCun et al., 2010) with 100 workers, Fragile SGD is much faster and has better test accuracy than Minibatch SGD. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The research reported in this publication was supported by funding from King Abdullah University of Science and Technology (KAUST): i) KAUST Baseline Research Scheme, ii) Center of Excellence for Generative AI, under award number 5940, i) SDAIA-KAUST Center of Excellence in Artificial Intelligence and Data Science. The work of A.T. was partially supported by the Analytical center under the RF Government (subsidy agreement 000000D73032iP5Q0002, Grant No. 70-2021-00145 02.11.2021). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Arjevani, Y, Carmon, Y, Duchi, J. C., Foster, D. J., Srebro, N., and Woodworth, B. (2022). Lower bounds for non-convex stochastic optimization. Mathematical Programming, pages 1-50.   \nBornstein, M., Rabbani, T, Wang, E., Bedi, A. S., and Huang, E. (2023). SWIFT: Rapid decentralized federated learning via wait-free model communication. In The 1lth International Conference on Learning Representations (ICLR).   \nCarmon, Y., Duchi, J. C., Hinder, O., and Sidford, A. (2020). Lower bounds for nding stationary points i. Mathematical Programming, 184(1):71-120.   \nCohen, A., Daniely, A., Drori, Y., Koren, T., and Schain, M. (2021). Asynchronous stochastic optimization robust to arbitrary delays. Advances in Neural Information Processing Systems, 34:9024-9035.   \nDuchi, J. C., Agarwal, A., and Wainwright, M. J. (2011). Dual averaging for distributed optimization: Convergence analysis and network scaling. IEEE Transactions on Automatic control, 57(3):592- 606.   \nEven, M., Koloskova, A., and Massoulie, L. (2024). Asynchronous SGD on graphs: A unified framework for asynchronous decentralized and federated optimization. In Artificial Intelligence and Statistics. PMLR.   \nFloyd, R. W. (1962). Algorithm 97: shortest path. Communications of the ACM, 5(6):345-345.   \nGhadimi, S. and Lan, G. (2013). Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341-2368.   \nHuang, X., Chen, Y., Yin, W, and Yuan, K. (2022). Lower bounds and nearly optimal algorithms in distributed learning with communication compression. Advances in Neural Information Processing Systems (NeurIPS).   \nKhaled, A. and Richtarik, P. (2022). Better theory for SGD in the nonconvex world. Transactions on Machine Learning Research.   \nKoloskova, A. (2024). Optimization algorithms for decentralized, distributed and collaborative machine learning. Technical report, EPFL.   \nKoloskova, A., Lin, T, and Stich, S. U. (2021). An improved analysis of gradient tracking for decentralized machine learning. Advances in Neural Information Processing Systems, 34:11422- 11435.   \nKoloskva, , Stich, .U, and Jaggi,M. 222) harper convergene guarantes for ashrnu SGD for distributed and federated learning. Advances in Neural Information Processing Systems (NeurIPS).   \nLan, G. (2020). First-order and stochastic optimization methods for machine learning. Springer.   \nLeCun, Y., Cortes, C., and Burges, C. (2010). Mnist handwritten digit database. ATT Labs [Online . Available: http://yann.lecun.com/exdb/mnist, 2.   \nLian, X., Zhang, W.,Zhang, C., and Liu, J. (2018). Asynchronous decentralized parallel stochastic gradient descent. In International Conference on Machine Learning, pages 3043-3052. PMLR.   \nLiu, Y., Lin, T., Koloskova, A., and Stich, S. U. (2024). Decentralized gradient tracking with local steps. Optimization Methods and Software, pages 1-28.   \nLu, Y. and De Sa, C. (2021). Optimal complexity in decentralized training. In International Conference on Machine Learning, pages 7111-7123. PMLR.   \nMishchenko, K., Bach, F., Even, M., and Woodworth, B. (2022). Asynchronous SGD beats minibatch SGD under arbitrary delays. Advances in Neural Information Processing Systems (NeurIPS).   \nNesterov, Y. (1983). A method of solving a convex programming problem with convergence rate 0 $(1/\\!\\!k^{**}\\!\\;2)$ . Doklady Akademii Nauk SSSR, 269(3):543.   \nNguyen, L., Nguyen, P. H., Dijk, M., Richtarik, P., Scheinberg, K., and Takac, M. (2018). SGD and hogwild! convergence without the bounded gradients assumption. In International Conference on Machine Learning, pages 3750-3758. PMLR.   \nRecht, B., Re, C., Wright, S., and Niu, F. (2011). Hogwild!: A lock-free approach to parallelizing stochastic gradient descent. Advances in Neural Information Processing Systems, 24.   \nScaman, K., Bach, F., Bubeck, S., Lee, Y. T., and Massoulie, L. (2017). Optimal algorithms for smooth and strongly convex distributed optimization in networks. In International Conference on Machine Learning, pages 3027-3036. PMLR.   \nShi, W., Ling, Q., Wu, G., and Yin, W. (2015). A proximal gradient algorithm for decentralized composite optimization. IEEE Transactions on Signal Processing, 63(22):6013-6023.   \nTyurin, A., Pozzi, M., Ilin, I., and Richtarik, P. (2024). Shadowheart SGD: Distributed asynchronous SGD with optimal time complexity under arbitrary computation and communication heterogeneity. arXiv preprint arXiv:2402.04785.   \nTyurin, A. and Richtarik, P. (2023). Optimal time complexities of parallel stochastic optimization methods under a fixed computation model. Advances in Neural Information Processing Systems (NeurIPS).   \nVan Handel, R. (2014). Probability in high dimension. Lecture Notes (Princeton University).   \nVogels, T., He, L., Koloskova, A., Karimireddy, S. P., Lin, T., Stich, S. U., and Jaggi, M. (2021). RelaySum for decentralized deep learning on heterogeneous data. Advances in Neural Information Processing Systems, 34.   \nYang, T., Yi, X., Wu, J., Yuan, Y., Wu, D., Meng, Z., Hong, Y., Wang, H., Lin, Z., and Johansson, K. H. (2019). A survey of distributed optimization. Annual Reviews in Control, 47:278-305. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Introduction 1.1  Decentralized setup with times ", "page_idx": 12}, {"type": "text", "text": "2Previous Results 2 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "2.1 Time complexity with one worker 2   \n2.2 Parallel optimization without communication costs 2   \n2.3 Parallel optimization with communication costs $\\tau_{i\\to j}$ 3 ", "page_idx": 12}, {"type": "text", "text": "3Contributions + ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "4  Lower Bound 4 ", "page_idx": 12}, {"type": "text", "text": "5 New Method: Fragile SGD 5 ", "page_idx": 12}, {"type": "text", "text": "5.1 Discussion 8   \n5.2 Interpretation of the upper and lower bounds (11) and (7) 8   \n5.3 Limitations 8   \n5.4 Comparison with previous methods 8   \n5.5 Time complexity with dynamic bounds 9 ", "page_idx": 12}, {"type": "text", "text": "6 Example: Line or Circle 9 ", "page_idx": 12}, {"type": "text", "text": "7  Heterogeneous Setup 10 ", "page_idx": 12}, {"type": "text", "text": "8 Highlights of Experiments 10 ", "page_idx": 12}, {"type": "text", "text": "A More Examples 15 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 ND-Mesh or ND-Torus 15   \nA.2Star graph 16   \nA.3 General case 16 ", "page_idx": 12}, {"type": "text", "text": "B  On the Connection to the Gossip Framework 16 ", "page_idx": 12}, {"type": "text", "text": "C  Heterogeneous Setup 17 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "C.1 Lower bound 17   \nC.2 Amelie SGD: optimal method in the heterogeneous setting 17   \nC.3 Discussion 19   \nC.4   Comparison with previous methods 19 ", "page_idx": 12}, {"type": "text", "text": "D  Convex Functions in the Homogeneous and Heterogeneous Setups 19 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "D.1  Assumptions in convex world 19   \nD.2 Homogeneous setup and nonsmooth case 20   \nD.3 Homogeneous setup and smooth case 20   \nD.4  Heterogeneous setup and nonsmooth case 21   \nD.5 Heterogeneous setup and smooth case 22   \nD.6  On lower bounds 23   \nD.7  Previous works 23 ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "E Lower Bound: Diving Deeper into the Construction 24 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "E.1 Description of Protocols 8 and 1 24   \nE.2 Lower bound 25   \nE.3 Proof sketch of Theorem 19 26   \nE.4 Full proof of Theorem 19 28   \nE.5 Proof of Lemma 2 31   \nE.6 Proof of Lemma 3 34 ", "page_idx": 13}, {"type": "text", "text": "F Lower Bound in the Heterogeneous Setup 36 ", "page_idx": 13}, {"type": "text", "text": "G Proof of the Time Complexity for Homogeneous Case 38 ", "page_idx": 13}, {"type": "text", "text": "H Proof of the Time Complexity for Heterogeneous Case 41 ", "page_idx": 13}, {"type": "text", "text": "Classical SGD Theory 44 ", "page_idx": 13}, {"type": "text", "text": "J Experiments 46   \nJ.1 Experiments with Logistic Regression: Fast vs Slow Communication . 48   \nJ.2 Experiments with ResNet-18 48 ", "page_idx": 13}, {"type": "image", "img_path": "IXRa8adMHX/tmp/e9730fa742a578a28b37521519e14e05a105928934becbc4ae9daf11a3da78c6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 4: Examples of ND-Mesh graphs. For all $i\\neq j\\in[n]$ ,edges $i\\rightarrow j$ and $j\\rightarrow i$ are merged and visualized with one undirected edge. ", "page_idx": 14}, {"type": "text", "text": "A More Examples ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 ND-Mesh or ND-Torus ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We now consider a generalization of Line graphs: ND-Mesh graphs. In Figures 4a and $4\\mathrm{b}$ , we present examples of 2D-Mesh and 3D-Mesh. For simplicity, assume that $\\textstyle n=({\\bar{2}}k+1)^{N}$ for some $k\\in\\mathbb N$ The computation speeds $h_{i}=h$ for all $i\\in[n]$ ,and thecommunicate speeds of thedes $\\rho_{i\\to j}=\\rho$ if workers $i$ and $j$ are connected in a mesh and $\\rho_{i\\to j}=\\infty$ for all other $i,j\\in[n]$ . Using geometrical reasoning, it is clear an index $j^{*}$ , that minimizes (11), corresponds to the worker in the middle of a graph (13 in Figures 4a and 14 in Figures 4b). Therefore, ", "page_idx": 14}, {"type": "equation", "text": "$$\nT_{\\mathrm{2D-Mesh}}=\\Theta\\left(\\frac{L\\Delta}{\\varepsilon}\\operatorname*{min}_{k\\in[n]}\\operatorname*{max}\\left\\{\\tau_{\\pi_{j^{*},k}\\rightarrow j^{*}},h,\\frac{\\sigma^{2}h}{k\\varepsilon}\\right\\}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For now, let us consider a 2D-Mesh graph. The number of workers, that have the length of the shortest pathto $j^{*}$ equals to O, is 1. The number of workers, that have the length of the shortest path to $j^{*}$ less or equal to $\\rho$ , is 5. The number of workers, that have the length of the shortest path to $j^{*}$ less or equal to $2\\rho$ , is 13. In general, the number of workers, that have the length of the shortest path to $j^{*}$ less or equal to $\\sqrt{k}\\rho$ is $\\Theta(k)$ for all $k\\in\\{0,\\ldots,\\Theta\\left(n\\right)\\}$ It means ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{\\mathrm{2D-Mesh}}=\\Theta\\left(\\frac{L\\Delta}{\\varepsilon}\\operatorname*{min}_{k\\in\\{0,\\ldots,\\Theta(n)\\}}\\operatorname*{max}\\left\\{\\sqrt{k}\\rho,h,\\frac{\\sigma^{2}h}{\\left(k+1\\right)\\varepsilon}\\right\\}\\right)}\\\\ &{\\qquad=\\Theta\\left(\\frac{L\\Delta}{\\varepsilon}\\left[h+\\left\\{\\frac{\\sigma^{2}h}{\\varepsilon},\\qquad\\qquad\\qquad\\left(\\frac{\\sigma^{2}h}{\\rho\\varepsilon}\\right)^{2/3}\\leq1,\\right.\\right.}\\\\ &{\\qquad\\qquad\\qquad\\left.\\left.\\frac{\\sigma^{2}h}{n\\varepsilon},\\qquad\\qquad\\qquad\\left(\\frac{\\sigma^{2}h}{\\rho\\varepsilon}\\right)^{2/3}>1,\\right.\\right]\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Using the same reasoning, for the general case with a ND-Mesh graph, we get ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{\\mathrm{ND-Mesh}}=\\Theta\\left(\\frac{L\\Delta}{\\varepsilon}\\operatorname*{min}_{k\\in\\{0,\\ldots,\\Theta(n)\\}}\\operatorname*{max}\\left\\{k^{1/N}\\rho,h,\\frac{\\sigma^{2}h}{(k+1)\\varepsilon}\\right\\}\\right)}\\\\ &{\\qquad=\\Theta\\left(\\frac{L\\Delta}{\\varepsilon}\\left[h+\\left\\{\\begin{array}{l l}{\\frac{\\sigma^{2}h}{\\varepsilon},}&{\\left(\\frac{\\sigma^{2}h}{\\rho\\varepsilon}\\right)^{N/(N+1)}\\leq1,}\\\\ {\\rho^{N/(N+1)}\\left(\\frac{\\sigma^{2}h}{\\varepsilon}\\right)^{1/(N+1)},}&{n>\\left(\\frac{\\sigma^{2}h}{\\rho\\varepsilon}\\right)^{N/(N+1)}>1,}\\\\ {\\frac{\\sigma^{2}h}{n\\varepsilon},}&{\\left(\\frac{\\sigma^{2}h}{\\rho\\varepsilon}\\right)^{N/(N+1)}\\geq n}\\end{array}\\right.\\right]\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "As in Line graphs, these complexities have three regimes depending on the problem's parameters. Up to a constant factor, the same conclusions apply to ND-Torus. ", "page_idx": 14}, {"type": "text", "text": "A.2  Star graph ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Let us consider Star graphs with different computation and communication speeds. Let us fix a graph with $n+1$ workers, where one worker with the index $n+1$ is in the center, and all other $n$ workers are only directly connected to worker $n+1$ . Then, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\tau_{i\\to j}=\\rho_{i\\to n+1}+\\rho_{n+1\\to j}\\quad\\forall i\\neq j\\in[n+1]\\mathrm{~and~}\\tau_{i\\to i}=0\\quad\\forall i\\in[n].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Using these constraints, we can simplify (11). There are two possible best strategies: i) a pivot worker $j^{*}$ works locally without communications, i) a pivot worker $j^{*}$ works with communications but it would necessary require to communicate through the central worker; in this case, the central worker $n+1$ is a pivot worker. Therefore, we get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\Lambda}_{\\mathrm{sur}}^{r}=\\operatorname*{min}\\Bigg[\\underset{\\varepsilon\\mathrm{\\Lambda}_{j\\in[n]}}{L\\Delta}\\operatorname*{min}\\operatorname*{max}\\left\\{h_{j},\\frac{\\sigma^{2}h_{j}}{\\varepsilon}\\right\\}}\\\\ &{\\qquad\\qquad\\frac{L\\Delta}{\\varepsilon}\\underset{k\\in[n]}{\\operatorname*{min}}\\operatorname*{max}\\left\\{\\operatorname*{max}\\{\\rho_{\\pi_{n+1,k}\\rightarrow n+1}+\\rho_{n+1\\rightarrow\\pi_{n+1,k}},h_{\\pi_{n+1,k}}\\},\\frac{\\sigma^{2}}{\\varepsilon}\\left(\\underset{i=1}{\\overset{k}{\\sum}}\\frac{1}{h_{\\pi_{n+1,i}}}\\right)^{-1}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "since $\\tau_{n+1\\rightarrow\\pi_{n+1,k}}=\\rho_{n+1\\rightarrow\\pi_{n+1,k}}$ for all $k\\in[n+1]$ Let us slightly simplify the result and assume that broadcasting from the central worker is fast, i.e., $\\rho_{n+1\\to i}\\leq\\rho_{i\\to n+1}$ for all $i\\in[n+1]$ , then ", "page_idx": 15}, {"type": "image", "img_path": "IXRa8adMHX/tmp/74856a43acbe04d64ae0696169853ec53aa952309851c9fc1c6561ba3149ba99.jpg", "img_caption": ["Tyurin et al. (2024) also considered Star graphs and showed that $T_{\\mathrm{fast\\,comm.}}$ is the optimal time complexity for methods that communicate through the central worker. Our result is more general since we also consider decentralized methods and capture the term $T_{\\mathrm{slow\\;comm}}$ that can be potentially smaller if communication is slow. Tyurin et al. (2024) conjectured that $T_{\\mathrm{star}}$ is the optimal bound, and we proved it (up to log factor) here. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.3 General case ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We show how one can use our generic result (11) to get an explicit formula in some cases. For the general case with arbitrary communication and computation times, the minimizers $j$ and $k$ in (11) can be found in the following way. First, we have to find $\\tau_{i\\to j}$ using any algorithm that solves the all-pairs shortet path problem (e.g, FloydWarshall algorithm (Floyd, 1962). Once we know $\\tau_{i\\to j}$ \uff0c we should sort $\\{\\operatorname*{max}\\{\\tau_{i\\to j}+\\tau_{j\\to i},h_{i}\\}\\}_{k=1}^{n}$ for all $j\\in[n]$ to find the permutations. Finally, we have enoughinformationtocalculate $t^{*}$ from Definition 2. ", "page_idx": 15}, {"type": "text", "text": "B On the Connection to the Gossip Framework ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Most of the previous methods were designed for a different setting, gossip-type communication. In fact, our setting is more general than the gossip communication. Indeed, recall that in the gossip communication, worker $i$ is allowed to get vectors from other workers through the operation $\\scriptstyle\\sum_{j=1}^{n}w_{i j}x_{j}$ \uff0c where $w_{i j}\\,\\in\\,\\{0,1\\}$ . This is equivalent to our setting for the case when the communication time $\\rho_{i j}=\\infty$ when $w_{i j}$ is zero, and $\\rho_{i j}=1$ if $w_{i j}$ is not zero, and worker $i$ sums the received vectors. But our setting is richer since we allow different communication and computation times and allow workers to do whatever they like with vectors (not only to sum). ", "page_idx": 15}, {"type": "text", "text": "Moreover, the gossip framework codes the communication graph through the matrix $\\{w_{i j}\\}$ We propose to code graphs through the times $\\{\\rho_{i j}\\}\\;(\\{\\tau_{i j}\\})$ . Our approach is closer to real scenarios because, as we explained previously, it includes the gossip framework. ", "page_idx": 15}, {"type": "text", "text": "C Heterogeneous Setup ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We now consider the heterogeneous setting. The only difference is instead of (1), we consider the following problem. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb{R}^{d}}\\Big\\{f(x):=\\frac{1}{n}\\sum_{i=1}^{n}\\underbrace{\\mathbb{E}_{\\xi_{i}\\sim\\mathcal{D}_{i}}\\left[f_{i}(x;\\xi_{i})\\right]}_{f_{i}(x):=}\\Big\\},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $f_{i}:\\mathbb{R}^{d}\\times\\mathbb{S}_{\\xi_{i}}\\rightarrow\\mathbb{R}^{d}$ and $\\xi_{i}$ are random variables with some distributions $\\mathcal{D}_{i}$ on $\\mathbb{S}_{\\xi_{i}}$ Wenow present our upper and lower bounds and discuss them. ", "page_idx": 16}, {"type": "text", "text": "C.1  Lower bound ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In Section F, we prove the following lower bound. ", "page_idx": 16}, {"type": "text", "text": "Theorem 7 (Lower Bound; Simplified Presentation of Theorem 20). Consider Protocol 1 with $\\nabla f_{i}(\\cdot;\\cdot)$ . We take any $h_{i}\\geq0$ and $\\tau_{i\\to j}\\geq0$ for all $i,j\\in[n]$ such that $\\tau_{i\\to j}\\leq\\tau_{i\\to k}+\\tau_{k\\to j}$ for all $i,k,j\\in[n].\\;W e\\,f\\!i x\\,L,\\Delta,\\varepsilon,\\sigma^{2}>0$ that satisfy the inequality $\\varepsilon<c L\\Delta$ forsomeuniversalconstant $c$ For any (zero-respecting) algorithm, there exists a function $\\textstyle f={\\frac{1}{n}}\\sum_{i=1}^{n}f_{i}$ , which satisfy Assumptions 1, 2 and $f(0)-f^{*}\\,\\leq\\,\\Delta_{}$ andstochasticgradientmappings $\\nabla f_{i}(\\cdot;\\cdot)$ : which satisfy Assumption 3 $(\\mathbb{E}_{\\xi}[\\nabla f_{i}(x;\\xi)]=\\nabla f_{i}(x)$ and $\\mathbb{E}_{\\xi}[\\|\\nabla f_{i}(\\bar{x};\\xi)-\\nabla f_{i}(\\bar{x})\\|^{2}]\\overset{}{\\leq}\\sigma^{2},$ ),suchthattherequiredtimetofind $\\varepsilon$ -solutionis ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Omega\\left(\\frac{L\\Delta}{\\varepsilon}\\operatorname*{max}\\left\\{\\operatorname*{max}_{i,j\\in[n]}\\tau_{i\\to j},\\operatorname*{max}_{i\\in[n]}h_{i},\\frac{\\sigma^{2}}{n\\varepsilon}\\left(\\frac{1}{n}\\sum_{i=1}^{n}h_{i}\\right)\\right\\}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "seconds. ", "page_idx": 16}, {"type": "text", "text": "C.2 Amelie SGD: optimal method in the heterogeneous setting ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We now present a new method based on Malenia SGD from (Tyurin and Richtarik, 2023) and our Fragile SGD. As in Fragile SGD, we also have $n+1$ processes running in all workers. The main idea is that all workers calculate stochastic gradients in parallel and accumulate them locally. Process 0 zerowaitsfor the moment when . Process $i$ calculates $b_{i}$ in Line 20 of Algorithm 7, and $\\mathfrak{n e x t}_{s t,j^{*}}(i)$ .Thus, $b_{j^{*}}$ acumulatesthesumn $\\textstyle\\sum_{i=1}^{n}{\\frac{1}{s_{i}^{k}}}$ Wwhihdereases withtime since $s_{i}^{k}$ is the number of calculated stochastic gradients in worker $i$ in different moments of time. Therefore, $\\begin{array}{r}{\\frac{n}{b_{j^{\\ast}}}\\geq\\frac{S}{n}}\\end{array}$ wil hold a some poin oftme f workers continue compting gradients.Finaly Algorithm 6 runs all reduce and does the update of $x^{k}$ ", "page_idx": 16}, {"type": "text", "text": "Theorem 8. Let Assumptions 1 and 2 hold for the function $f$ and Assumption 3 holds for the functions $f_{i}$ for all $i\\in[n]$ . We take $\\gamma=1/2L$ , the parameter $S=\\operatorname*{max}\\{\\left\\lceil\\sigma^{2}/\\varepsilon\\right\\rceil,\\dot{n}\\}$ , any pivot worker $j^{*}\\in[n]$ and any spanning trees $\\overline{{s t}}$ and $\\overline{{s t}}_{\\mathrm{bc}}$ , in Algorithm 5. For all iterations number $K\\geq16L\\Delta/\\varepsilon$ , we get $\\begin{array}{r}{\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\|\\nabla f(x^{k})\\right\\|^{2}\\right]\\leq\\varepsilon.}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "Note that Theorem 8 states the convergence of Amelie SGD even if the computation and communication speeds are unbounded. ", "page_idx": 16}, {"type": "text", "text": "Theorem 9. Consider the assumptions and the parameters from Theorem 8. For any pivot worker $j^{*}\\in[n]$ andanyspanningtrees $\\overline{{s t}}$ and $\\overline{{s t}}_{\\mathrm{bc}}$ ,Algorithm5converges after atmost ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Theta\\left(\\frac{L\\Delta}{\\varepsilon}\\operatorname*{max}\\left\\{\\operatorname*{max}_{i,j\\in[n]}\\mu_{i\\to j},\\operatorname*{max}_{i\\in[n]}h_{i},\\frac{\\sigma^{2}}{n\\varepsilon}\\left(\\frac{1}{n}\\sum_{i=1}^{n}h_{i}\\right)\\right\\}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "seconds, where $\\mu_{i\\to j^{*}}^{k}\\left(\\mu_{j^{*}\\to i}^{k}\\right)$ is an upper bound ntimes requiredto send avectorfromworke ito worker $j^{*}$ (from worker $j^{*}$ to worker $i$ )along the spanning tree $\\overline{{s t}}$ (spanning tree $\\overline{{s t}}_{\\mathrm{bc}}$ for all $i\\in[n]$ ", "page_idx": 16}, {"type": "text", "text": "Algorithm 5 Amelie SGD ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1: Input: starting point $x^{0}$ , stepsize $\\gamma$ parameter $S$ , pivot worker $j^{*}$ , spanning trees $\\overline{{s t}}$ and $\\overline{{s t}}_{\\mathrm{bc}}$   \n2: Start Process 0 (Alg. 6) in worker $j^{*}$   \n3: Start Process $i$ (Alg. 7) in all workers for all $i\\in[n]$ (including worker $j^{*}$ ", "page_idx": 17}, {"type": "text", "text": "Algorithm 6 Process 0 (running in worker $j^{*}$ ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1: for $k=0,1,\\ldots,K-1$ do   \n2: Broadcast $x^{k}$ to all workers using the spanning tree $\\overline{{s t}}_{\\mathrm{bc}}$   \n3: Init $s^{k}=0$   \n4: while $\\begin{array}{r}{s^{k}<\\frac{S}{n}}\\end{array}$ do   \n5: Wait for a message $b_{j^{*}}$ from Process $j^{*}$   \n6: Calculate $\\begin{array}{r}{s^{k}=\\frac{n}{b_{j^{*}}}}\\end{array}$   \n7: end while   \n8: Run all reduce with $\\textstyle\\{{\\frac{1}{s_{i}^{k}}}g_{i}^{k}\\}_{i=1}^{n}$ to find $g^{k}={\\textstyle{\\frac{1}{n}}}\\sum_{i=1}^{n}{\\textstyle{\\frac{1}{s_{i}^{k}}}}g_{i}^{k}$ 9gs using the spaning tre t   \n9: $x^{k+1}=x^{k}-\\gamma g^{k}$   \n10: end for ", "page_idx": 17}, {"type": "text", "text": "Algorithm 7 Process i (running in worker i) ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1: while True do   \n2: Get a new point $x^{k}$ broadcasted by Process 0   \n3: Init $(g_{i}^{k},s_{i}^{k})=(0,0)$   \n4: Init $b_{i,p}=\\infty$ for all $p\\in[n]$ s.t. $\\mathsf{n e x t}_{s t,j^{*}}(\\boldsymbol{p})=i$   \n5: Run the following three functions in parallel and go to Line 24   \n6: function CalculateStochasticGradients   \n7: while True   \n8: Calculate $\\nabla f_{i}(x^{k};\\xi),\\quad\\xi\\sim\\mathcal{D}_{i}$   \n9: Run atomic add $g_{i}^{k}=g_{i}^{k}+\\nabla f_{i}(x^{k};\\xi),s_{i}^{k}=s_{i}^{k}+1$   \n10: end while   \n11: end function   \n12: function ReceiveCountersFromPrevious Workers   \n13: while True   \n14: Wait for a message $b_{p}$ from any Process $p$ s.t. $\\mathfrak{n e x t}_{s t,j^{*}}(p)=i$   \n15: Run atomic update $\\bar{b_{i,p}}=b_{p}$   \n16: end while   \n17: end function   \n18: function SendCounterToNextWorker   \n19: while True   \n20: Run atomic sum $b_{i}=\\sum_{p\\in[n]:\\mathrm{next}_{s t,j}\\ast(p)=i}b_{i,p}+\\frac{1}{s_{i}^{k}}$   \n21: Send $b_{i}\\in\\mathbb{R}\\cup\\{\\infty\\}$ (one foat) to Process $\\mathfrak{n e x t}_{\\mathit{s t},j^{*}}(i)$ and wait while it sending   \n(Process $j^{*}$ sends to Process O by the definition of $\\mathfrak{n e x t}_{\\overline{{s t}},j^{*}}(\\cdot)_{j}$   \n22: end while   \n23: end function   \n24: Wait for new point. If receives new point, stop all computations in functions, and continue   \n25: Ignore all non-received messages   \n26: end while ", "page_idx": 17}, {"type": "text", "text": "Corollary 2. Consider the assumptions and the parameters from Theorem 9. Let us take any pivot worker $j^{*}\\in[n]$ andaspanningtree $\\overline{{s t}}$ (spanning tree $\\overline{{s t}}_{\\mathrm{bc}}$ ) that connects everyworker i toworker $j^{*}$ (worker $j^{*}$ to worker $i$ )withtheshortest distance $\\tau_{i\\to j^{*}}$ $(\\tau_{j^{\\ast}\\rightarrow i})$ then Algorithm 5 converges after at most ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Theta\\left(\\frac{L\\Delta}{\\varepsilon}\\operatorname*{max}\\left\\{\\operatorname*{max}_{i,j\\in[n]}\\tau_{i\\to j},\\operatorname*{max}_{i\\in[n]}h_{i},\\frac{\\sigma^{2}}{n\\varepsilon}\\left(\\frac{1}{n}\\sum_{i=1}^{n}h_{i}\\right)\\right\\}\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "seconds. ", "page_idx": 18}, {"type": "text", "text": "C.3Discussion ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Corollary 2 together with Theorem 7 states that the time complexity (18) is optimal. The result is pessimistic since the time complexity depends on the \u201cdiameter\\* $\\operatorname*{max}_{i,j\\in[n]}\\tau_{i\\to j}$ and the slowest performance $\\operatorname*{max}_{i\\in[n]}h_{i}$ . A similar dependence was observed in (Lu and De Sa, 2021; Tyurin and Richtarik, 2023). As in (Tyurin and Richtarik, 2023), the stochastic term $\\begin{array}{r}{\\sigma^{2}/n\\varepsilon\\left(1/n\\sum_{i=1}^{n}h_{i}\\right)}\\end{array}$ depends on the average of $\\{h_{i}\\}$ if $\\sigma^{2}/\\varepsilon$ is large. ", "page_idx": 18}, {"type": "text", "text": "C.4    Comparison with previous methods ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Let us consider Minibatch SGD described in Section 2.3. This method converges after ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Theta\\left(\\frac{L\\Delta}{\\varepsilon}\\operatorname*{max}_{i,j\\in[n]}\\tau_{i\\to j},\\operatorname*{max}_{i\\in[n]}h_{i},\\frac{\\sigma^{2}}{n\\varepsilon}\\operatorname*{max}\\{\\operatorname*{max}_{i,j\\in[n]}\\tau_{i\\to j},\\operatorname*{max}_{i\\in[n]}h_{i}\\}\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "seconds in the heterogeneous setting. If $\\sigma^{2}/\\varepsilon$ is large, then Amelie SGD can be at least ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\{\\operatorname*{max}_{i,j\\in[n]}\\tau_{i\\to j},\\operatorname*{max}_{i\\in[n]}h_{i}\\}/\\left(\\frac{1}{n}\\sum_{i=1}^{n}h_{i}\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "times faster than Minibatch SGD. There are many more advanced methods (Lu and De Sa, 2021; Vogels et al., 2021; Even et al., 2024) that work in decentralized stochastic heterogeneous setting. Lu and De Sa (2021) developed similar lower and upper bounds, but there are at least three main differences: i) they derived an iteration complexity instead of a time complexity and assumed the performances of all workers are the same i) the obtained lower bound holds only for one particular multigraph while our complexity holds for any multigraph ii) they assumed the smoothness of $f_{i}$ while we consider the smoothness of $f$ . The RelaySGD and Gradient Tracking methods by Vogels et al. (2021); Liu et al. (2024) wait for the slowest worker; thus, they depend on $\\operatorname*{max}_{i\\in[n]}h_{i}$ in all regimes of $\\sigma^{2}/\\varepsilon$ , unlike our method. Even et al. (2024) consider the heterogeneous asynchronous setting, but their method assumes the similarity of the functions $f_{i}$ , which is not required in our method, and Amelie SGD converges even if there is no similarity of functions. ", "page_idx": 18}, {"type": "text", "text": "D  Convex Functions in the Homogeneous and Heterogeneous Setups ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We will be slightly more brief in the convex setting since the idea, the structure of time complexities, and the general approach do not change significantly. For instance, instead of the time complexity (11) that we get for the nonconvex case, in the nonsmooth convex case, we get (20). Thus, we will get $\\begin{array}{r}{\\Theta\\left(\\frac{M^{2}R^{2}}{\\varepsilon^{2}}\\operatorname*{min}_{j\\in[n]}t^{*}\\big(\\sigma^{2}\\big/M^{2},\\dots\\big)\\right)}\\end{array}$ instead of $\\begin{array}{r}{\\Theta\\left(\\frac{L\\Delta}{\\varepsilon}\\operatorname*{min}_{j\\in[n]}t^{*}(\\sigma^{2}/\\varepsilon,\\dots)\\right)}\\end{array}$ .The same idea applies to the smooth convex case. In the convex setting, we need the following assumptions. ", "page_idx": 18}, {"type": "text", "text": "D.1  Assumptions in convex world ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Assumption 4. The function $f$ is convex and attains a minimum at some point $x^{*}\\in\\mathbb{R}^{d}$ Assumption 5. The function $f$ is $M\\cdot$ -Lipschitz, i.e., ", "page_idx": 18}, {"type": "equation", "text": "$$\n|f(x)-f(y)|\\leq M\\left\\|x-y\\right\\|,\\quad\\forall x,y\\in\\mathbb{R}^{d}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for some $M\\in(0,\\infty]$ ", "page_idx": 18}, {"type": "text", "text": "Assumption6.Forall $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ ,stochastic(sub)gradients $\\nabla f(x;\\xi)$ are unbiased and are $\\sigma^{2}$ -variancebounded, i.e,. $\\mathbb{E}_{\\xi\\sim\\cal D}\\left[\\nabla f(x;\\xi)\\right]\\in\\partial f(x)$ and $\\begin{array}{r}{\\mathbb{E}_{\\xi\\sim\\mathcal{D}}\\left[\\left\\|\\nabla f(x;\\xi)-\\mathbb{E}_{\\xi\\sim\\mathcal{D}}\\left[\\nabla f(x;\\xi)\\right]\\right\\|^{2}\\right]\\le\\sigma^{2}}\\end{array}$ where $\\sigma^{2}\\geq0$ ", "page_idx": 19}, {"type": "text", "text": "D.2  Homogeneous setup and nonsmooth case ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Theorem 10. Let Assumptions 4, 5 and $^{6}$ hold. Choose any $\\varepsilon\\,>\\,0$ . Let us take the batch size $S=\\operatorname*{max}\\left\\{\\left\\lceil\\sigma^{2}/M^{2}\\right\\rceil,1\\right\\}$ ,stepsie $\\begin{array}{r}{\\gamma\\,=\\,\\frac{\\varepsilon}{M^{2}+\\sigma^{2}/S}\\,\\in\\,\\left[\\frac{\\varepsilon}{2M^{2}},\\frac{\\dot{\\varepsilon}}{M^{2}}\\right]}\\end{array}$ ,any pivot worker $j^{*}\\in[n]$ and any spanning trees $\\overline{{s t}}$ and $\\overline{{s t}}_{\\mathrm{bc}}$ in Algorithm 2. Then after $K\\,\\geq\\,{2M^{2}R^{2}}/{\\varepsilon^{2}}$ iterations the method guarantees $\\mathbb{E}\\left[f(\\widehat{x}^{K})\\right]-f(x^{*})\\leq\\varepsilon$ where $\\begin{array}{r}{\\widehat{x}^{K}=\\frac{1}{K}\\sum_{k=0}^{K-1}x^{k}}\\end{array}$ and $R=\\|x^{*}-x^{0}\\|$ ", "page_idx": 19}, {"type": "text", "text": "Proof. The proof of Theorem 10 is almost the same as in Theorem 4 (see Section G). The proof of Theorem 4 states that the steps of Fragile SGD are equivalent to the classical SGD method. Thus, we can use the classical result from the literature (Lan, 2020). Using Theorem 22, we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f(\\widehat{x}^{K})\\right]-f(x^{*})\\leq\\varepsilon\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "if ", "page_idx": 19}, {"type": "equation", "text": "$$\nK\\geq{\\frac{2M^{2}\\left\\|x^{*}-x^{0}\\right\\|^{2}}{\\varepsilon^{2}}}\\geq{\\frac{\\left(M^{2}+{\\frac{\\sigma^{2}}{S}}\\right)\\left\\|x^{*}-x^{0}\\right\\|^{2}}{\\varepsilon^{2}}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for the stepsize ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\gamma=\\frac{\\varepsilon}{M^{2}+\\frac{\\sigma^{2}}{S}}\\in\\left[\\frac{\\varepsilon}{2M^{2}},\\frac{\\varepsilon}{M^{2}}\\right],\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we use the fact that $S\\geq\\sigma^{2}/M^{2}$ ", "page_idx": 19}, {"type": "text", "text": "Theorem 11. Consider the assumptions and the parameters from Theorem 10. For any pivot worker $j^{*}\\in[n]$ andspanningtrees $\\overline{{s t}}$ and $\\overline{{s t}}_{\\mathrm{bc}}$ ,Algorithm 2converges after atmost ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Theta\\left(\\frac{M^{2}R^{2}}{\\varepsilon^{2}}t^{*}(\\sigma^{2}/M^{2},[h_{i}]_{i=1}^{n},[\\mu_{i\\to j^{*}}+\\mu_{j^{*}\\to i}]_{i=1}^{n})\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "seconds, where $\\mu_{i\\to j^{*}}$ $(\\mu_{j^{*}\\to i})$ is an upper bound on times required to send a vector from worker i to worker $j^{*}$ (from worker $j^{*}$ to worker i) along the spanning tree $\\overline{{s t}}$ (spanning tree $\\overline{{s t}}_{\\mathrm{bc.}}$ ", "page_idx": 19}, {"type": "text", "text": "Proof. The proof is identical to the proof of Theorem 5. ", "page_idx": 19}, {"type": "text", "text": "Corollary 3. Consider the assumptions and the parameters from Theorem $_{l l}$ . Let us take a pivot worker $j^{*}=\\arg\\operatorname*{min}_{j\\in[n]}t^{*}\\big(\\sigma^{2}/\\bar{M^{2}},[h_{i}]_{i=1}^{n},[\\tau_{i\\rightarrow j}+\\tau_{j\\rightarrow i}]_{i=1}^{n}\\big)$ , and a spanning tree $\\overline{{s t}}$ (spanning tree $\\overline{{s t}}_{\\mathrm{bc}}$ ) that connects every worker $i$ to worker $j^{*}$ (worker $j^{*}$ to every worker $i$ ) with the shortest distance $\\tau_{i\\to j^{*}}\\left(\\tau_{j^{*}\\to i}\\right)$ . Then Algorithm 2 converges after at most ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Theta\\left(\\frac{M^{2}R^{2}}{\\varepsilon^{2}}\\operatorname*{min}_{j\\in[n]}t^{*}\\big(\\sigma^{2}/M^{2},[h_{i}]_{i=1}^{n},[\\tau_{i\\to j}+\\tau_{j\\to i}]_{i=1}^{n}\\big)\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "seconds. ", "page_idx": 19}, {"type": "text", "text": "D.3  Homogeneous setup and smooth case ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In the homogeneous and smooth case, we will slightly modify Fragile SGD. Instead of Line 8 of Algorithm 3, we use the following steps: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{k+1}=\\gamma\\cdot(k+1),\\quad\\alpha_{k+1}=2/(k+2)}\\\\ &{y^{k+1}=(1-\\alpha_{k+1})x^{k}+\\alpha_{k+1}u^{k},\\qquad(u^{0}=x^{0})}\\\\ &{u^{k+1}=u^{k}-\\frac{\\gamma_{k+1}}{s^{k}}g^{k},}\\\\ &{x^{k+1}=(1-\\alpha_{k+1})x^{k}+\\alpha_{k+1}u^{k+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We will call such a method the Accelerated Fragile SGD method. The idea is to use the acceleration technique from (Lan, 2020) (which is based on (Nesterov, 1983)). ", "page_idx": 19}, {"type": "text", "text": "Theorem 12. Let Assumptions 4, $^{\\,l}$ and $3$ hold. Choose any $\\varepsilon\\,>\\,0$ . Let us take the batch size $\\begin{array}{r}{S=\\operatorname*{max}\\left\\{\\left\\lceil(\\sigma^{2}R)/(\\varepsilon^{3/2}\\sqrt{L})\\right\\rceil,1\\right\\},\\gamma=\\operatorname*{min}\\left\\{\\frac{1}{4L},\\left\\lceil\\frac{3R^{2}S}{4\\sigma^{2}(K+1)(K+2)^{2}}\\right\\rceil^{1/2}\\right\\}}\\end{array}$ any pivot worker j\\*, and anyspanningtrees $\\overline{{s t}}$ and $\\overline{{s t}}_{\\mathrm{bc}}$ in Accelerated Method 2 (Accelerated Fragile SGD), then after $\\begin{array}{r}{K\\geq\\frac{8\\sqrt{L}R}{\\sqrt{\\varepsilon}}}\\end{array}$ iterations the methodguarantes that $\\mathbb{E}\\left[f(x^{K})\\right]-f(x^{*})\\leq\\varepsilon$ where $R=\\|x^{*}-x^{0}\\|$ ", "page_idx": 20}, {"type": "text", "text": "Proof. Using the same reasoning as in Theorem 4, Accelerated Fragile SGD is just the classical accelerated stochastic gradient method with a batch size greater or equal to $S$ .We can useProposition 4.4 from Lan (2020). For the stepsize ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\gamma=\\operatorname*{min}\\left\\{\\frac{1}{4L},\\left[\\frac{3R^{2}S}{4\\sigma^{2}(K+1)(K+2)^{2}}\\right]^{1/2}\\right\\},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f(x^{K})\\right]-f(x^{*})\\leq\\frac{4L R^{2}}{K^{2}}+\\frac{4\\sqrt{\\sigma^{2}R^{2}}}{\\sqrt{S K}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f(x^{K})\\right]-f(x^{*})\\leq\\varepsilon\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "if ", "page_idx": 20}, {"type": "equation", "text": "$$\nK\\geq\\frac{8\\sqrt{L}R}{\\sqrt{\\varepsilon}}\\geq8\\operatorname*{max}\\left\\{\\frac{\\sqrt{L}R}{\\sqrt{\\varepsilon}},\\frac{\\sigma^{2}R^{2}}{\\varepsilon^{2}S}\\right\\},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we use the choice of $S$ ", "page_idx": 20}, {"type": "text", "text": "Theorem 13. Consider the assumptions and the parameters from Theorem 12. For any pivot worker $j^{*}\\in[n]$ andspanningtrees $\\overline{{s t}}$ and $\\overline{{s t}}_{\\mathrm{bc}}$ , Accelerated Algorithm 2 (Accelerated Fragile SGD) converges afteratmost ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Theta\\left(\\frac{\\sqrt{L}R}{\\sqrt{\\varepsilon}}t^{*}\\left(\\frac{\\sigma^{2}R}{\\varepsilon^{3/2}\\sqrt{L}},[h_{i}]_{i=1}^{n},[\\mu_{i\\to j^{*}}+\\mu_{j^{*}\\to i}]_{i=1}^{n}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "seconds, where $\\mu_{i\\to j^{*}}\\left(\\mu_{j^{*}\\to i}\\right)$ is anupper bound ontimes required to send a vectorfrom worker i to worker $j^{*}$ (from worker $j^{*}$ to worker $i$ )along the spanning tree $\\overline{{s t}}$ (spanning tree $\\overline{{s t}}_{\\mathrm{bc}}$ ", "page_idx": 20}, {"type": "text", "text": "Proof. The proof is identical to the proof of Theorem 5. ", "page_idx": 20}, {"type": "text", "text": "Corollary 4. Consider the assumptions and the parameters from Theorem 13. Let us take a pivot worker $\\begin{array}{r}{j^{*}=\\arg\\operatorname*{min}_{j\\in[n]}t^{*}\\left(\\frac{\\sigma^{2}R}{\\varepsilon^{3/2}\\sqrt{L}},[h_{i}]_{i=1}^{n},[\\tau_{i\\rightarrow j}+\\tau_{j\\rightarrow i}]_{i=1}^{n}\\right)}\\end{array}$ (s/2,[ha]1, Ti\u2192 + T\u2192i]=1) , and aspanning tre &t(sanning tree $\\overline{{s t}}_{\\mathrm{bc}}$ )that connects every worker i toworker $j^{*}$ (worker $j^{*}$ to every worker i) with the shortest distance $\\tau_{i\\to j^{*}}\\left(\\tau_{j^{*}\\to i}\\right)$ . Then Accelerated Algorithm 2 (Accelerated Fragile SGD) converges after at most ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Theta\\left(\\frac{\\sqrt{L}R}{\\sqrt{\\varepsilon}}\\operatorname*{min}_{j\\in[n]}t^{*}\\left(\\frac{\\sigma^{2}R}{\\varepsilon^{3/2}\\sqrt{L}},[h_{i}]_{i=1}^{n},[\\tau_{i\\to j}+\\tau_{j\\to i}]_{i=1}^{n}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "seconds. ", "page_idx": 20}, {"type": "text", "text": "D.4  Heterogeneous setup and nonsmooth case ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Consider the optimization problem (17) ", "page_idx": 20}, {"type": "text", "text": "Theorem 14. Let Assumptions 4, 5 hold for the function $f$ and Assumption $^{6}$ holds for the functions $f_{i}$ for all $i\\,\\in\\,[n]$ . Choose any $\\varepsilon\\,>\\,0$ . Let us take the batch size $\\stackrel{\\cdot}{S}\\,=\\,\\mathrm{max}\\left\\{\\left\\lceil\\bar{\\sigma}^{2}/M^{2}\\right\\rceil,n\\right\\}$ $\\begin{array}{r}{\\gamma\\,=\\,\\frac{\\varepsilon}{M^{2}+\\sigma^{2}/S}\\,\\in\\,\\left[\\frac{\\varepsilon}{2M^{2}},\\frac{\\varepsilon}{M^{2}}\\right]}\\end{array}$ , any pivot worker $j^{*}\\in[n]$ , and any spanning trees $\\overline{{s t}}$ and $\\overline{{s t}}_{\\mathrm{bc}}$ \uff0c $i n$ Algorithm 5, then after $K\\geq2M^{2}R^{2}/\\varepsilon^{2}$ iterations the method guarantees that $\\mathbb{E}\\left[f(\\widehat{x}^{K})\\right]-f(x^{*})\\leq\\varepsilon$ \uff0c where $\\begin{array}{r}{\\widehat{x}^{K}=\\frac{1}{K}\\sum_{k=0}^{K-1}x^{k}}\\end{array}$ and $R=\\|x^{*}-x^{0}\\|$ ", "page_idx": 20}, {"type": "text", "text": "Proof. The proof of Theorem 14 is almost the same as in Theorems 8 (see Section H). The proof of Theorem 8 states that the steps of Amelie SGD are equivalent to the classical SGD method. Thus, we can use the classical result from the literature (Lan, 2020). Using Theorem 22, we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f(\\widehat{x}^{K})\\right]-f(x^{*})\\leq\\varepsilon\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "if ", "page_idx": 21}, {"type": "equation", "text": "$$\nK\\geq{\\frac{2M^{2}\\left\\|x^{*}-x^{0}\\right\\|^{2}}{\\varepsilon^{2}}}\\geq{\\frac{\\left(M^{2}+{\\frac{\\sigma^{2}}{S}}\\right)\\left\\|x^{*}-x^{0}\\right\\|^{2}}{\\varepsilon^{2}}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for the stepsize ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\gamma=\\frac{\\varepsilon}{M^{2}+\\frac{\\sigma^{2}}{S}}\\in\\left[\\frac{\\varepsilon}{2M^{2}},\\frac{\\varepsilon}{M^{2}}\\right],\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where we use the fact that $S\\geq\\sigma^{2}/M^{2}$ ", "page_idx": 21}, {"type": "text", "text": "Theorem 15. Consider the assumptions and the parameters from Theorem 14. For any pivot worker $j^{*}\\in[n]$ andanyspanningtrees $\\overline{{s t}}$ and $\\overline{{s t}}_{\\mathrm{bc}}$ ,Algorithm5converges after at most ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Theta\\left(\\frac{M^{2}R^{2}}{\\varepsilon^{2}}\\operatorname*{max}\\left\\{\\operatorname*{max}_{i,j\\in[n]}\\mu_{i\\to j},\\operatorname*{max}_{i\\in[n]}h_{i},\\frac{\\sigma^{2}}{n M^{2}}\\left(\\frac{1}{n}\\sum_{i=1}^{n}h_{i}\\right)\\right\\}\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "seconds, where $\\mu_{i\\to j^{*}}^{k}\\left(\\mu_{j^{*}\\to i}^{k}\\right)$ is an upperbound on times required to send a vectorfrom worker i to worker $j^{*}$ (worker $j^{*}$ to worker $i$ ) along the spanning tree $\\overline{{s t}}$ (spanning tree $\\overline{{s t}}_{\\mathrm{bc}}$ )for all $i\\in[n]$ ", "page_idx": 21}, {"type": "text", "text": "Proof. The proof is identical to the proof of Theorem 9. ", "page_idx": 21}, {"type": "text", "text": "Corollary 5. Consider the assumptions and the parameters from Theorem 15. Let us take any pivot worker $j^{*}\\in[n]$ andaspanningtree $\\overline{{s t}}$ (spanning tree $\\overline{{s t}}_{\\mathrm{bc}}$ )that connects everyworker i toworker $j^{*}$ (worker $j^{*}$ to worker $i$ )withtheshortest distance $\\tau_{i\\to j^{*}}$ $(\\tau_{j^{\\ast}\\to i})$ then Algorithm 5 converges after at most ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Theta\\left(\\frac{M^{2}R^{2}}{\\varepsilon^{2}}\\operatorname*{max}\\left\\{\\operatorname*{max}_{i,j\\in[n]}\\tau_{i\\to j},\\operatorname*{max}_{i\\in[n]}h_{i},\\frac{\\sigma^{2}}{n M^{2}}\\left(\\frac{1}{n}\\sum_{i=1}^{n}h_{i}\\right)\\right\\}\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "seconds. ", "page_idx": 21}, {"type": "text", "text": "D.5  Heterogeneous setup and smooth case ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Consider the optimization problem (17). In this section, we use the same idea as in Section D.3. We will modify Amelie SGD and, instead of Line 9 from Algorithm 6, we use the lines (21). We call such a method the Accelerated Amelie SGD method. ", "page_idx": 21}, {"type": "text", "text": "Theorem 16. Let Assumptions $^{4}$ and $^{\\,l}$ hold for the function $f$ and Assumption 3 holds for the functions $f_{i}$ . Choose any $\\varepsilon>0$ . Let us take the batch size $S=\\operatorname*{max}\\left\\{\\left\\lceil(\\sigma^{\\stackrel{\\cdot}{2}}\\!R)/(\\varepsilon^{3/2}\\sqrt{L})\\right\\rceil,n\\right\\}$ $\\begin{array}{r}{\\gamma=\\operatorname*{min}\\left\\{\\frac{1}{4L},\\left[\\frac{3R^{2}S}{4\\sigma^{2}(K+1)(K+2)^{2}}\\right]^{1/2}\\right\\}}\\end{array}$ , any pivot worker $j^{*}$ and any panning trees $\\overline{{s t}}$ and $\\overline{{s t}}_{\\mathrm{bc}}$ in Accelerated Method $^{5}$ (Accelerated Amelie SGD), then after $\\begin{array}{r}{K\\,\\geq\\,\\frac{8\\sqrt{L}R}{\\sqrt{\\varepsilon}}}\\end{array}$ iterations the method guarantees that $\\mathbb{E}\\left[f(x^{K})\\right]-f(x^{*})\\leq\\varepsilon$ ,where $R=\\|x^{*}-x^{0}\\|$ ", "page_idx": 21}, {"type": "text", "text": "Proof. Accelerated Amelie SGD is equivalent to the accelerated stochastic gradient method with a mini-batch from Lan (2020). The proof repeats the proofs of Theorem 12 and Theorem 8. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "Theorem 17. Consider the assumptions and the parameters from Theorem 16. For any pivot worker $j^{*}\\,\\in\\,[n]$ and any spanningtrees $\\overline{{s t}}$ and $\\overline{{s t}}_{\\mathrm{bc}}$ ,AcceleratedAlgorithm $^{5}$ (Accelerated Amelie SGD) convergesafteratmost ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Theta\\left(\\frac{\\sqrt{L}R}{\\sqrt{\\varepsilon}}\\operatorname*{max}{\\left\\{\\operatorname*{max}_{i,j\\in[n]}\\mu_{i\\to j},\\operatorname*{max}_{i\\in[n]}h_{i},\\frac{\\sigma^{2}R}{n\\varepsilon^{3/2}\\sqrt{L}}\\left(\\frac{1}{n}\\sum_{i=1}^{n}h_{i}\\right)\\right\\}}\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "seconds, where $\\mu_{i\\to j^{*}}^{k}\\left(\\mu_{j^{*}\\to i}^{k}\\right)$ isanupperboudontisrequredtsendavectorfrmworkt worker $j^{*}$ (worker $j^{*}$ to worker $i$ ) along the spanning tree $\\overline{{s t}}$ (spanning tree $\\overline{{s t}}_{\\mathrm{bc}}$ )for all $i\\in[n]$ ", "page_idx": 21}, {"type": "text", "text": "Corollary 6. Consider the assumptions and the parameters from Theorem 17. Let us take any pivot worker $j^{*}\\in[n]$ and a spanning tree $\\overline{{s t}}$ (spanning tree $\\overline{{s t}}_{\\mathrm{bc}}$ ) that connects everyworker i to worker $j^{*}$ (worker $j^{*}$ to worker $i$ )withtheshortestdistance $\\tau_{i\\to j^{*}}$ $(\\tau_{j^{\\ast}\\to i})$ ,thenAcceleratedAlgorithm5 (Accelerated AmelieSGD) converges after at most ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\Theta\\left(\\frac{\\sqrt{L}R}{\\sqrt{\\varepsilon}}\\operatorname*{max}{\\left\\{\\operatorname*{max}_{i,j\\in[n]}\\tau_{i\\to j},\\operatorname*{max}_{i\\in[n]}h_{i},\\frac{\\sigma^{2}R}{n\\varepsilon^{3/2}\\sqrt{L}}\\left(\\frac{1}{n}\\sum_{i=1}^{n}h_{i}\\right)\\right\\}}\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "seconds. ", "page_idx": 22}, {"type": "text", "text": "D.6 On lower bounds ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In previous subsections, we provide upper bounds on the time complexities for different classes of convex functions and optimization problems. Using the same reasoning as in Section 4 and (Tyurin and Richtarik, 2023)[Section B], we conjecture that the obtained upper bounds are tight and optimal (up to log factors in the homogeneous case). ", "page_idx": 22}, {"type": "text", "text": "D.7 Previous works ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Let us consider the time complexities (20) and (23) (accelerated rate) in the homogeneous and convex cases. When $\\tau_{i\\to j}=0$ for all $i,j\\in[n]$ , Even et al. (2024) recovers the time complexity (nonaccelerated in the smooth case) of Asynchronous SGD (Mishchenko et al., 2022; Koloskova et al., 2022) which is suboptimal (Tyurin and Richtarik, 2023). When the communication is free, we recover the time complexity (accelerated in the smooth case) of Accelerated Rennala SGD from (Tyurin and Richtarik, 2023), which is optimal if $\\tau_{i\\to j}=0$ for all $i,j\\in[n]$ ", "page_idx": 22}, {"type": "text", "text": "In the heterogeneous and convex setting, (26) improves the result from (Even et al., 2024) since (26) is an accelerated rate and does not depend on a quantity that measures the similarity of the functions $f_{i}$ .When $\\sigma=0$ , the result (26) is consistent with the lower bound from (Scaman et al., 2017). However, when $\\sigma>0$ , as far as we know, the time complexity (26) is new in the convex smooth setting. ", "page_idx": 22}, {"type": "text", "text": "E Lower Bound: Diving Deeper into the Construction ", "text_level": 1, "page_idx": 23}, {"type": "image", "img_path": "IXRa8adMHX/tmp/4566783dcb558855336c5a16d7455617da9bdd8acd2f33d0256f8dbf56b95fc9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "In Section 4, we present a brief overview and simplified theorem for the lower bound. We now provide a formal and strict mathematical construction. ", "page_idx": 23}, {"type": "text", "text": "E.1Description of Protocols 8 and 1 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "One way how we can formalize Protocol 1 is to use Protocol 8. Let us explain it. Using Protocol 8, we consider any possible method that works in our distributed asynchronous setting. The mapping $M^{k}$ of the algorithm returns the time $t^{k+1}$ (ignore for now) and $c^{k+1}$ If $c^{k+1}=0$ , then the algorithm decides to start the calculation of a stochastic gradient: it determines the index $i^{k+1}$ of a worker that will start the calculation, then, using all locally available information $g_{i^{k+1}}^{1},\\ldots,g_{i^{k+1}}^{k}$ , it calculates a new point $\\boldsymbol{x}_{i^{k+1}}^{k}$ , and passes this point to the computation oracle $O_{i^{k+1}}$ that will return a new stochastic gradient after $h_{i^{k+1}}$ seconds. If $c^{k+1}=1$ , then the algorithm decides to communicate a vector: it returns the indices $i^{k+1}$ and $j^{k+1}$ of two workers that will communicate, and the index $p^{k+1}\\in\\mathbb{N}$ of a communication oracle, calculates $v_{i^{k+1}}^{k}$ in worker $i^{k+1}$ using onlyalocally available information $g_{i^{k+1}}^{1},\\ldots,g_{i^{k+1}}^{k}$ and passes it to the communication oracle \\*+1 \u2192j\\*+1 that will send the vector after $\\tau_{i^{k+1}\\to j^{k+1}}$ seconds. ", "page_idx": 23}, {"type": "text", "text": "The computation oracles we define as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O_{i}:\\underbrace{{\\mathbb{R}}_{\\geq0}}_{\\mathrm{time}}\\times\\underbrace{{\\mathbb{R}}^{d}}_{\\mathrm{point}}\\times\\underbrace{({\\mathbb{R}}_{\\geq0}\\times{\\mathbb{R}}^{d}\\times\\{0,1\\})}_{\\mathrm{input~state}}\\rightarrow\\underbrace{({\\mathbb{R}}_{\\geq0}\\times{\\mathbb{R}}^{d}\\times\\{0,1\\})}_{\\mathrm{output~state}}\\times{\\mathbb{R}}^{d}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "such that ", "page_idx": 23}, {"type": "equation", "text": "$$\nO_{i}(t,x,(s_{t},s_{x},s_{q}))=\\left\\{\\!\\!\\begin{array}{l l}{((t,x,1),}&{0),\\quad s_{q}=0,}\\\\ {((s_{t},s_{x},1),}&{0),\\quad s_{q}=1,t<s_{t}+h_{i},}\\\\ {((0,0,0),}&{\\nabla f(s_{x};\\xi)),}&{s_{q}=1,t\\geq s_{t}+h_{i},}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\xi\\sim\\mathcal{D}$ ", "page_idx": 23}, {"type": "text", "text": "The communication oracles we define as ", "page_idx": 23}, {"type": "equation", "text": "$$\nC_{i\\to j}^{p}:\\underbrace{\\mathbb{R}_{\\geq0}}_{\\mathrm{time}}\\times\\underbrace{\\mathbb{R}^{d}}_{\\mathrm{point}}\\times\\underbrace{(\\mathbb{R}_{\\geq0}\\times\\mathbb{R}^{d}\\times\\{0,1\\})}_{\\mathrm{input~state}}\\to\\underbrace{(\\mathbb{R}_{\\geq0}\\times\\mathbb{R}^{d}\\times\\{0,1\\})}_{\\mathrm{output~state}}\\times\\mathbb{R}^{d}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\nC_{i\\rightarrow j}^{p}(t,x,(s_{t},s_{x},s_{q}))=\\left\\{\\!\\!\\begin{array}{l l}{{((t,x,1),}}&{{0),\\quad s_{q}=0,}}\\\\ {{((s_{t},s_{x},1),}}&{{0),\\quad s_{q}=1,t<s_{t}+\\tau_{i\\rightarrow j},}}\\\\ {{((0,0,0),}}&{{s_{x}),}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The idea is that the computation oracle (27) emulates the behavior of a real worker $i$ thatrequires $h_{i}$ seconds to calculate a stochastic gradient. The communication oracle emulates the behavior of a real communication channel that requires $\\tau_{i\\to j}$ seconds to send a vector from worker $i$ toworker $j$ ", "page_idx": 24}, {"type": "text", "text": "One can see that, for all $i\\neq j\\in[n]$ , an algorithm can access an infinite number of communication oraclesCh-j? $C_{i\\rightarrow j}^{1},C_{i\\rightarrow j}^{2},\\dots.$ \\*... We allow an algorithm to send as many vectors from worker to worker j in parallel as it wants. ", "page_idx": 24}, {"type": "text", "text": "We now discuss the role of $t^{k+1}$ . One can see that the time $t^{k+1}$ is returned by the algorithm, and $t^{k+1}$ is passed to the oracles $O_{i^{k+1}}$ and +\u2192+ Consider that Ok1was called with t+ for thefrst time, then it will return zero vector because $(s_{i^{k+1}}^{h,k})_{q}=0$ (see (27)) at the beginning. Then, by the construction of (27), the oracle will return a non-zero vector in the second output if only the algorithm returns a time that is greater or equal to $t^{k+1}+h_{i^{k+1}}$ . The same idea applies to $C_{i^{k+1}\\to j^{k+1}}^{p^{\\bar{k}+1}}$ +1\u2192jk+1 : if it was called $t^{k+1}$ for the frst time, then worker $j^{k+1}$ will get a non-zero vector only if the algorithm passes time that is greater or equal to $t^{k+1}+\\tau_{i^{k+1}\\rightarrow j^{k+1}}$ ", "page_idx": 24}, {"type": "text", "text": "The oracles force the algorithm to increase the time $t^{k+1}$ ; otherwise, it will not get new information $(\\nabla f(s_{x};\\xi)$ in (27)) about the function. One crucial constraint is $t^{k+1}\\;\\geq\\;t^{\\tilde{k}}$ ,meaning that the algorithm can not \u201ccheat\u201d and travel into the past. The idea of such a protocol was proposed in Tyurin and Richtarik (2023), and we refer to Sections 3-5 for a detailed description. A more readable and less formal version of Protocol 8 is presented in the Protocol 1. ", "page_idx": 24}, {"type": "text", "text": "E.2 Lower bound ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Before we state the main theorem, let us define the class of zero-respecting algorithms. ", "page_idx": 24}, {"type": "text", "text": "Definition 18 (Algorithm Class $\\mathcal{A}_{\\mathrm{zr}})$ . Let us consider Protocol 8. We say that the sequence of tuples of mappings $\\{(M^{\\bar{k}},L^{k},D^{k},P_{1}^{k},\\dots,P_{n}^{k},V_{1}^{k},\\dots,V_{n}^{k})\\}_{k=0}^{\\infty}$ is a zero-respecting algorithm, if ", "page_idx": 24}, {"type": "equation", "text": "$$\nt^{k+1}\\geq t^{k},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The set of all algorithms with this properties we define as $A_{\\mathrm{zr}}$ ", "page_idx": 24}, {"type": "text", "text": "Constraints 1-5 define domains of the mappings. Constraint 6 is required to ensure that the time sequence $t^{k}$ does not decrease. Constraint 7 is a standard assumption that an algorithm is zerorespecting (Arjevani et al., 2022). ", "page_idx": 25}, {"type": "text", "text": "Theorem 19. Consider Protocol 8. We take any $h_{i}~\\geq~0$ and $\\tau_{i\\to j}\\;\\geq\\;0$ for all $i,j\\,\\in\\,[n]$ such that $\\tau_{i\\to j}\\,\\le\\,\\tau_{i\\to k}+\\tau_{k\\to j}$ for all $i,k,j\\,\\in\\,[n]$ : We fix $L,\\Delta,\\varepsilon,\\sigma^{2}\\,>\\,0$ that satisfy the inequality $\\varepsilon<c^{\\prime}L\\bar{\\Delta}$ . For any algorithm $A\\in{\\mathcal{A}}_{\\mathrm{zr}}$ ,there exists $a$ function $f$ which satisfy Assumptions 1, 2 and $f(0)-f^{*}\\leq\\Delta,$ and a stochastic gradient mapping $\\nabla f(\\cdot;\\cdot)$ , which satisfy Assumption 3, such that $\\mathbb{E}\\left[\\operatorname*{inf}_{k\\in S_{t},i\\in[n]}\\left\\|\\nabla f(x_{i}^{k})\\right\\|^{2}\\right]>\\varepsilon$ ,where ${S_{t}:=\\left\\{k\\in\\mathbb{N}_{0}\\,|\\,t^{k}\\leq t\\right\\}}$ ", "page_idx": 25}, {"type": "equation", "text": "$$\nt=c\\times\\frac{1}{\\log n+1}\\frac{L\\Delta}{\\varepsilon}\\operatorname*{min}_{j\\in[n]}\\operatorname*{min}_{k\\in[n]}\\operatorname*{max}\\left\\{\\operatorname*{max}\\{\\tau_{\\pi_{j,k}\\to j},h_{\\pi_{j,k}}\\},\\frac{\\sigma^{2}}{\\varepsilon}\\left(\\sum_{i=1}^{k}\\frac{1}{h_{\\pi_{j,i}}}\\right)^{-1}\\right\\},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\pi_{j,}$ . is a permutation that sorts $\\operatorname*{max}\\{h_{i},\\tau_{i\\to j}\\}$ , i.e., ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\{h_{\\pi_{j,1}},\\tau_{\\pi_{j,1}\\to j}\\}\\leq\\cdot\\cdot\\cdot\\leq\\operatorname*{max}\\{h_{\\pi_{j,n}},\\tau_{\\pi_{j,n}\\to j}\\}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for all $j\\in[n]$ . The quantities $c^{\\prime}$ and $c$ are universal constants. The sequences $x^{k}$ and $t^{k}$ are defined in Protocol 8. ", "page_idx": 25}, {"type": "text", "text": "E.3Proof sketch of Theorem 19 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Let us provide a proof sketch that will give intuition behind the theorem. The full proof starts in SectionE.4. ", "page_idx": 25}, {"type": "text", "text": "Proof Sketch. ", "page_idx": 25}, {"type": "text", "text": "Part 1: Construct a function and stochastic gradient ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The first part of the proof is standard (Carmon et al., 2020; Arjevani et al., 2022; Tyurin and Richtarik, 2023; Huang et al., 2022; Lu and De Sa, 2021), and we delegate it to Section E.4. For any algorithm, we construct oracles and a \u201cworst-case\u2019\u201d function such that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{k\\in S_{t},i\\in[n]}\\left\\|\\nabla f(x_{i}^{k})\\right\\|^{2}>2\\varepsilon\\operatorname*{inf}_{k\\in S_{t},i\\in[n]}\\mathbb{1}\\left[\\mathrm{prog}(x_{i}^{k})<T\\right],\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where ${\\mathrm{prog}}(x):={\\mathrm{max}}\\{i\\geq0\\,|\\,x_{i}\\neq0\\}$ $\\left[x_{0}\\equiv1\\right]$ and $T\\approx L\\Delta/_{\\varepsilon}$ . This inequality says that while all points in Protocol 8 have the last coordinate equals 0 by the time $t$ , an algorithm can not find an $\\varepsilon$ -stationary point. ", "page_idx": 25}, {"type": "text", "text": "Sincewe assume that $A\\in A_{\\mathrm{zr}}$ is zero-respecting, the only way to discover the next non-zero coordinate is through stochastic gradients. They are constructed in the following way (Arjevani et al., 2022): ", "page_idx": 25}, {"type": "equation", "text": "$$\n[\\nabla f(x;\\xi)]_{j}:=\\nabla_{j}f(x)\\left(1+\\mathbb{1}\\left[j>\\mathrm{prog}(x)\\right]\\left(\\frac{\\xi}{p}-1\\right)\\right)\\quad\\forall x\\in\\mathbb{R}^{T},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and $\\xi\\sim\\mathrm{Bernoulli}(p)$ for all $i\\,\\in\\,[n]$ , where $p\\approx\\varepsilon/\\sigma^{2}$ . We denote $[x]_{j}$ as the $j^{\\mathrm{th}}$ index of a vector $x\\in\\mathbb{R}^{T}$ . The stochastic gradient equals to the exact gradient except for the last non-zero coordinate: it zeros out it with the high probability $1-p$ ", "page_idx": 25}, {"type": "text", "text": "Part 2: The Level Game ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In essence, the protocol is equivalent to the following collaborative game. Each worker $i$ starts with level $\\ell_{i}=0$ . The goal is to reach level $T$ with at least one worker as fast as possible. There are two ways how a worker can increase its level: i) worker $i$ flips one coin per time from $\\xi\\sim\\mathrm{Bernoulli}(p)$ it takes $h_{i}$ seconds to flip one coin, and if the worker is lucky, i.e., $\\xi=1$ , then it moves to the next level $\\ell_{i}=\\ell_{i}+1$ ; ii) worker $i$ can share its level with another worker $j$ , and it takes $\\tau_{i\\to j}$ seconds (we are allowed to run this operation again even if the previous is not finished). Both options can be executed in parallel. What is the minimum possible time to reach the game's goal? ", "page_idx": 25}, {"type": "text", "text": "Since all workers have the levels equal to O at the beginning, they should flip coins in parallel and wait for the moment when at least one worker moves to level 1. We define $\\eta_{i}^{1}$ as the number of fips in worker $i$ to get $\\xi=1$ . Clearly, $\\eta_{i}^{1}\\sim\\mathrm{Geometric}(p)$ are i.i.d. geometric random variables with the probability $p$ . With any strategy, it is necessary to wait at least ", "page_idx": 26}, {"type": "equation", "text": "$$\ny_{j}^{1}:=\\operatorname*{min}_{i\\in[n]}\\left\\{h_{i}\\eta_{i}^{1}+\\tau_{i\\to j}\\right\\}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "seconds to reach level 1 in worker $j$ because once worker $i$ flips $\\xi=1$ , it will take at least $\\tau_{i\\to j}$ seconds to share level 1 to worker $j$ due to the triangle inequality $\\tau_{i\\to j}\\,\\le\\,\\tau_{i\\to k}+\\tau_{k\\to j}$ for all $i,j,k\\,\\in\\,[n]$ , and we should minimize $h_{i}\\eta_{i}^{1}+\\tau_{i\\rightarrow j}$ over all workers. We now use mathematical induction to prove that it is necessary to wait at least ", "page_idx": 26}, {"type": "equation", "text": "$$\ny_{j}^{T}:=\\operatorname*{min}_{i\\in[n]}\\left\\{y_{i}^{T-1}+h_{i}\\eta_{i}^{T}+\\tau_{i\\to j}\\right\\}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "seconds to reach level $T$ , where we define $y_{i}^{0}:=0$ for all $i\\in[n]$ and $\\{\\eta_{i}^{T}\\}$ are i.i.d. random variables from Geometric $(p)$ . The base case is proven above. Assume that it is true for $1,\\dots,T-1$ , then worker $i$ require t least $y_{i}^{T-1}+h_{i}\\eta_{i}^{T}$ seconds tofip a coin that moves to level $T$ it takes atleast $\\tau_{i\\to j}$ seconds to share the level $T$ $\\operatorname*{min}_{i\\in[n]}\\left\\{y_{i}^{T-1}+h_{i}\\eta_{i}^{T}+\\tau_{i\\to j}\\right\\}$ seconds in worker $i$ to get level $T$ . Ultimately, the minimum possible time to reach the game's goal is ", "page_idx": 26}, {"type": "equation", "text": "$$\ny^{T}:=\\operatorname*{min}_{j\\in[n]}y_{j}^{T}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "From the previous result, we can conclude that if ${t\\leq\\frac{1}{2}y^{T}}$ , then ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{k\\in S_{t},i\\in[n]}\\left\\|\\nabla f(x_{i}^{k})\\right\\|^{2}>2\\varepsilon\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for any algorithm $A\\in{\\mathcal{A}}_{\\mathrm{zr}}$ ", "page_idx": 26}, {"type": "text", "text": "Part 3: The high probability bound of $y^{T}$ ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Let us fix any determenistic value $\\bar{y}\\in\\mathbb R$ and take ", "page_idx": 26}, {"type": "equation", "text": "$$\nt={\\frac{1}{2}}{\\bar{y}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Using (30), we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{inf}_{k\\in S_{t},i\\in[n]}\\left\\|\\nabla f(x_{i}^{k})\\right\\|^{2}\\right]\\geq\\mathbb{E}\\left[\\operatorname*{inf}_{k\\in S_{t},i\\in[n]}\\left\\|\\nabla f(x_{i}^{k})\\right\\|^{2}\\right]y^{T}>\\bar{y}\\right]\\mathbb{P}\\left(y^{T}>\\bar{y}\\right)>\\mathbb{P}\\left(y^{T}>\\bar{y}\\right)2\\varepsilon.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Thus, it is sufficient to find any $\\bar{y}\\in\\mathbb R$ such that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\boldsymbol{y}^{T}\\leq\\bar{\\boldsymbol{y}}\\right)\\leq\\frac{1}{2}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Thesequence $y^{T}$ is a well-define time series. In Lemma 2, we show that (32) holds with ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\bar{y}=\\Theta\\left(\\frac{1}{\\log n+1}\\frac{L\\Delta}{\\varepsilon}\\operatorname*{min}_{j\\in[n]}\\operatorname*{min}_{k\\in[n]}\\operatorname*{max}\\left\\{\\operatorname*{max}\\{\\tau_{\\pi_{j,k}\\to j},h_{\\pi_{j,k}}\\},\\frac{\\sigma^{2}}{\\varepsilon}\\left(\\sum_{i=1}^{k}\\frac{1}{h_{\\pi_{j,i}}}\\right)^{-1}\\right\\}\\right),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "$\\pi_{j}$ . is a permutation that sorts $\\operatorname*{max}\\{h_{i},\\tau_{i\\to j}\\}$ , i.e., ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\{h_{\\pi_{j,1}},\\tau_{\\pi_{j,1}\\to j}\\}\\leq\\cdot\\cdot\\cdot\\leq\\operatorname*{max}\\{h_{\\pi_{j,n}},\\tau_{\\pi_{j,n}\\to j}\\}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for all $j\\in[n]$ . We substitute this $\\bar{y}$ to (31) and get the result of theorem. ", "page_idx": 26}, {"type": "text", "text": "E.4 Full proof of Theorem 19 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "This section considers the standard \u201cworst case\u201d function that helps to provide lower bounds in the nonconvex world. Let us define ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathrm{prog}(x):=\\operatorname*{max}\\{i\\geq0\\,|\\,x_{i}\\neq0\\}\\quad(x_{0}\\equiv1).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For any $T\\in\\mathbb N$ , Carmon et al. (2020); Arjevani et al. (2022) define ", "page_idx": 27}, {"type": "equation", "text": "$$\nF_{T}(x):=-\\Psi(1)\\Phi(x_{1})+\\sum_{i=2}^{T}\\left[\\Psi(-x_{i-1})\\Phi(-x_{i})-\\Psi(x_{i-1})\\Phi(x_{i})\\right],\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\Psi(x)=\\left\\{\\!\\!\\begin{array}{l l}{0,}&{x\\leq1/2,}\\\\ {\\exp\\left(1-\\frac{1}{(2x-1)^{2}}\\right),}&{x\\geq1/2,}\\end{array}\\right.\\,\\,\\mathrm{and}\\quad\\Phi(x)=\\sqrt{e}\\int_{-\\infty}^{x}e^{-\\frac{1}{2}t^{2}}d t.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We will only rely on the following facts. ", "page_idx": 27}, {"type": "text", "text": "Lemma 1 (Carmon et al. (2020); Arjevani et al. (2022). The function $F_{T}$ satisfies: ", "page_idx": 27}, {"type": "text", "text": "1. $\\begin{array}{r}{F_{T}(0)-\\operatorname*{inf}_{x\\in\\mathbb{R}^{T}}F_{T}(x)\\leq\\Delta^{0}T,w h e r e\\,\\Delta^{0}=12.}\\end{array}$   \n2. The function $F_{T}$ is $l_{1}$ -smooth,where $l_{1}=152$   \n3. For all $x\\in\\mathbb{R}^{T}$ \uff0c $\\|\\nabla F_{T}(x)\\|_{\\infty}\\leq\\gamma_{\\infty}$ , where $\\gamma_{\\infty}=23$   \n4. For all $x\\in\\mathbb{R}^{T}$ \uff0c $\\mathrm{prog}(\\nabla F_{T}(x))\\leq\\mathrm{prog}(x)+1.$   \n5. For all $x\\in\\mathbb{R}^{T}$ \uff0c $i f\\mathrm{prog}(x)<T$ then $\\|\\nabla F_{T}(x)\\|>1$ ", "page_idx": 27}, {"type": "text", "text": "Theorem 19. Consider Protocol 8. We take any $h_{i}~\\geq~0$ and $\\tau_{i\\to j}\\;\\geq\\;0$ for all $i,j\\,\\in\\,[n]$ such that $\\tau_{i\\to j}\\,\\le\\,\\tau_{i\\to k}+\\tau_{k\\to j}$ for all $i,k,j\\,\\in\\,[n]$ . We fix $\\mathbf{\\partial}:L,\\Delta,\\varepsilon,\\sigma^{2}\\,>\\,0$ thatsatisfytheinequality $\\varepsilon<c^{\\prime}L\\Delta$ . For any algorithm $A\\in{\\mathcal{A}}_{\\mathrm{zr}}$ ,there exists $a$ function $f$ whichsatisfyAssumptions 1, 2and $f(0)-f^{*}\\leq\\Delta_{*}$ anda stochasticgradientmapping $\\nabla f(\\cdot;\\cdot)$ ,which satisfyAssumption3,suchthat $\\mathbb{E}\\left[\\operatorname*{inf}_{k\\in S_{t},i\\in[n]}\\left\\|\\nabla f(x_{i}^{k})\\right\\|^{2}\\right]>\\varepsilon$ where ${S_{t}:=\\left\\{k\\in\\mathbb{N}_{0}\\,|\\,t^{k}\\leq t\\right\\}}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\nt=c\\times\\frac{1}{\\log n+1}\\frac{L\\Delta}{\\varepsilon}\\operatorname*{min}_{j\\in[n]}\\operatorname*{min}_{k\\in[n]}\\operatorname*{max}\\left\\{\\operatorname*{max}\\{\\tau_{\\pi_{j,k}\\to j},h_{\\pi_{j,k}}\\},\\frac{\\sigma^{2}}{\\varepsilon}\\left(\\sum_{i=1}^{k}\\frac{1}{h_{\\pi_{j,i}}}\\right)^{-1}\\right\\},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\pi_{j,}$ . is a permutation that sorts $\\operatorname*{max}\\{h_{i},\\tau_{i\\to j}\\}$ ,i.e., ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\{h_{\\pi_{j,1}},\\tau_{\\pi_{j,1}\\to j}\\}\\leq\\cdot\\cdot\\cdot\\leq\\operatorname*{max}\\{h_{\\pi_{j,n}},\\tau_{\\pi_{j,n}\\to j}\\}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for all $j\\in[n]$ . The quantities $c^{\\prime}$ and $c$ are universal constants. The sequences $x^{k}$ and $t^{k}$ are defined in Protocol 8. ", "page_idx": 27}, {"type": "text", "text": "Proof. ", "page_idx": 27}, {"type": "text", "text": "Part 1: The Worst Case Function ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "This part of the proof mirrors the proofs from Carmon et al. (2020); Arjevani et al. (2022); Tyurin and Richtarik (2023); Huang et al. (2022); Lu and De Sa (2021). We provide it for completeness. The goal of this part to construct a hard instance. ", "page_idx": 27}, {"type": "text", "text": "Wefix $\\lambda>0$ \uff0c1 $T\\in\\mathbb N$ and take th function $\\begin{array}{r}{f(x):=\\frac{L\\lambda^{2}}{l_{1}}F_{T}\\left(\\frac{x}{\\lambda}\\right)}\\end{array}$ , where the funtion $F_{T}$ is defined in Section E.4. Note that the function $f$ is $L$ -smooth: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\nabla f(x)-\\nabla f(y)\\|={\\frac{L\\lambda}{l_{1}}}\\left\\|\\nabla F_{T}\\left({\\frac{x}{\\lambda}}\\right)-\\nabla F_{T}\\left({\\frac{y}{\\lambda}}\\right)\\right\\|\\leq L\\lambda\\left\\|{\\frac{x}{\\lambda}}-{\\frac{y}{\\lambda}}\\right\\|=L\\left\\|x-y\\right\\|\\quad\\forall x,y\\in\\mathbb{R}^{d},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $l_{1}$ -smoothness of $F_{T}$ (Lemma 1). Let us take ", "page_idx": 27}, {"type": "equation", "text": "$$\nT=\\left\\lfloor\\frac{\\Delta l_{1}}{L\\lambda^{2}\\Delta^{0}}\\right\\rfloor,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "then ", "page_idx": 28}, {"type": "equation", "text": "$$\nf(0)-\\operatorname*{inf}_{x\\in\\mathbb{R}^{T}}f(x)=\\frac{L\\lambda^{2}}{l_{1}}(F_{T}\\left(0\\right)-\\operatorname*{inf}_{x\\in\\mathbb{R}^{T}}F_{T}(x))\\leq\\frac{L\\lambda^{2}\\Delta^{0}T}{l_{1}}\\leq\\Delta.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We showed that the function $f$ satisfy Assumptions $1,2$ and $f(0)-f^{*}\\leq\\Delta$ ", "page_idx": 28}, {"type": "text", "text": "For each worker $i$ we take an oracle $O_{i}$ , from (27) with the mapping $g$ such that ", "page_idx": 28}, {"type": "equation", "text": "$$\n[\\nabla f(x;\\xi)]_{j}:=\\nabla_{j}f(x)\\left(1+\\mathbb{1}\\left[j>\\mathrm{prog}(x)\\right]\\left(\\frac{\\xi}{p}-1\\right)\\right)\\quad\\forall x\\in\\mathbb{R}^{T},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and $D_{i}=\\mathrm{{Bernouilli}}(p)$ for all $i\\in[n]$ , where $p\\in(0,1]$ . We denote $[x]_{j}$ as the $j^{\\mathrm{th}}$ index of a vector $x\\in\\mathbb{R}^{T}$ . This stochastic gradient is unbiased and $\\sigma^{2}$ -variance-bounded. We have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[[\\nabla f(x,\\xi)]_{i}\\right]=\\nabla_{i}f(x)\\left(1+\\mathbb{1}\\left[i>\\mathrm{prog}(x)\\right]\\left(\\frac{\\mathbb{E}\\left[\\xi\\right]}{p}-1\\right)\\right)=\\nabla_{i}f(x)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "for all $i\\in[T]$ , and ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert\\nabla f(x;\\xi)-\\nabla f(x)\\right\\Vert^{2}\\right]\\leq\\operatorname*{max}_{j\\in[T]}|\\nabla_{j}f(x)|^{2}\\mathbb{E}\\left[\\left({\\frac{\\xi}{p}}-1\\right)^{2}\\right]\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "because the difference is non-zero only in one coordinate. Thus ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|\\nabla f(x,\\xi)-\\nabla f(x)\\right\\|^{2}\\right]\\leq\\frac{\\left\\|\\nabla f(x)\\right\\|_{\\infty}^{2}(1-p)}{p}=\\frac{L^{2}\\lambda^{2}\\left\\|\\nabla F_{T}\\left(\\frac{x}{\\lambda}\\right)\\right\\|_{\\infty}^{2}(1-p)}{l_{1}^{2}p}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\frac{L^{2}\\lambda^{2}\\gamma_{\\infty}^{2}(1-p)}{l_{1}^{2}p}\\leq\\sigma^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where we use Lemma 1 and take ", "page_idx": 28}, {"type": "equation", "text": "$$\np=\\operatorname*{min}\\left\\{\\frac{L^{2}\\lambda^{2}\\gamma_{\\infty}^{2}}{\\sigma^{2}l_{1}^{2}},1\\right\\}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Let us take ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\lambda=\\frac{\\sqrt{2\\varepsilon}l_{1}}{L}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "to ensure that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\|\\nabla f(x)\\right\\|^{2}=\\frac{L^{2}\\lambda^{2}}{l_{1}^{2}}\\left\\|\\nabla F_{T}\\left(\\frac{x}{\\lambda}\\right)\\right\\|^{2}=2\\varepsilon\\left\\|\\nabla F_{T}\\left(\\frac{x}{\\lambda}\\right)\\right\\|^{2}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "for all $x\\in\\mathbb{R}^{T}$ . From Lemma 1, we know that if p $\\mathrm{rog}(x)<T$ then $\\|\\nabla F_{T}(x)\\|>1$ Thus, we get ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|\\nabla f(x)\\|^{2}>2\\varepsilon\\mathbb{1}\\left[\\mathrm{prog}(x)<T\\right]\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Therefore, ", "page_idx": 28}, {"type": "equation", "text": "$$\nT=\\left\\lfloor\\frac{\\Delta L}{2\\varepsilon l_{1}\\Delta^{0}}\\right\\rfloor\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and ", "page_idx": 28}, {"type": "equation", "text": "$$\np=\\operatorname*{min}\\left\\{\\frac{2\\varepsilon\\gamma_{\\infty}^{2}}{\\sigma^{2}},1\\right\\}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The inequality (34) implies ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{k\\in S_{t},i\\in[n]}\\left\\|\\nabla f(x_{i}^{k})\\right\\|^{2}>2\\varepsilon\\operatorname*{inf}_{k\\in S_{t},i\\in[n]}\\mathbb{1}\\left[\\mathrm{prog}(x_{i}^{k})<T\\right],\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "$\\{x_{i}^{k}\\}_{k=0}^{\\infty}$ ", "page_idx": 28}, {"type": "text", "text": "Part 2: Reduction to Lower Bound Time Series ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We now focus on (27). In (27), when the oracle in worker $i$ calculates a new stochastic gradient, it ", "page_idx": 28}, {"type": "text", "text": "samples a random variable $\\xi\\sim\\mathcal{D}$ . This is equivalent to the procedure if we had $T$ infinite sequences of i.i.d. Bernoulli random variables ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\xi_{i}^{1,1},\\xi_{i}^{1,2},\\dots\\quad}&{{}(\\mathrm{prog}(s_{x})=0)}\\\\ {\\xi_{i}^{2,1},\\xi_{i}^{2,2},\\dots\\quad}&{{}(\\mathrm{prog}(s_{x})=1)}\\\\ {\\dots\\quad}&{{}}\\\\ {\\xi_{i}^{T,1},\\xi_{i}^{T,2},\\dots\\quad}&{{}(\\mathrm{prog}(s_{x})=T-1)}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "for all $i\\in[n]$ and the oracle would look at the progress of $s_{x}$ and take the next non-taken Bernoulli random variable in the sequence that corresponds to that progress. For instance, if $\\mathtt{p r o g}(s_{x})=j$ for the first time in worker $i$ then the oracle will apply $\\xi_{i}^{j,1}$ in (27). The next time when it gets $\\mathrm{prog}(s_{x})=j$ it will apply $\\xi_{i}^{j,2}$ and so on. One by one, we ge i.d. Bernoullirandom variables. ", "page_idx": 29}, {"type": "text", "text": "Let us define ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\eta_{i}^{k}=\\operatorname*{inf}\\{j\\in\\mathbb{N}\\,|\\,\\xi_{i}^{k,j}=1\\}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "for all $i\\in[n]$ and $k\\in[T]$ This is thefrsternullirandmvariable fromthe sequece $\\xi_{i}^{k,1},\\xi_{i}^{k,2},\\dots$ that equals 1. The random variables $\\{\\eta_{i}^{k}\\}$ are i.i.d. geometrically distributed random variables with the probability $p$ ", "page_idx": 29}, {"type": "text", "text": "The next steps mirror Proof Sketch from Theorem 19. The oracles constructed in such a way that it takes $h_{i}$ seconds to calculate a stochastic gradient, and at least $\\tau_{i\\to j}$ seconds to send a vector from one worker to another. Using the same reasoning as in the Level Game in Proof Sketch of Theorem 19, the first time moment when worker $j$ can get a vector with the first non-zero coordinate greater or equal ", "page_idx": 29}, {"type": "equation", "text": "$$\ny_{j}^{1}:=\\operatorname*{min}_{i\\in[n]}\\left\\{h_{i}\\eta_{i}^{1}+\\tau_{i\\to j}\\right\\}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "because, at the beginning, each worker $i$ calculates stochastic gradients with $\\mathrm{prog}(s_{x})=0$ and should wait at least $h_{i}\\eta_{i}^{1}$ seconds to get a stochastic gradient with the progress equals 1. Then, it can share this vector with any other worker $j$ , but it takes at least $\\tau_{i\\to j}$ seconds. ", "page_idx": 29}, {"type": "text", "text": "As in Proof Sketch of Theorem 19, we can use mathematical induction to prove that it is necessary to waitatleast ", "page_idx": 29}, {"type": "equation", "text": "$$\ny_{j}^{T}:=\\operatorname*{min}_{i\\in[n]}\\left\\{y_{i}^{T-1}+h_{i}\\eta_{i}^{T}+\\tau_{i\\to j}\\right\\}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "seconds to get a point such that $\\mathrm{prog}(s_{x})=T$ . The base case for $y_{j}^{1}$ has been proven. Worker $i$ requires t least $y_{i}^{T-1}+h_{i}\\eta_{i}^{T}$ seconds to wai forthe moment whenthecorresponding oracle wil return a stochastic gradient with progress $T$ because $y_{i}^{T-1}$ is the first time possible time to get a vector with progress $T-1$ by the induction, and it will take at least additional $h_{i}\\eta_{i}^{T}$ seconds to calculate $\\eta_{i}^{T}$ vectors with $\\mathrm{prog}(s_{x})=T-1$ in (27). Also, it takes at least $\\tau_{i\\to j}$ seconds to share a vector, soi is necessary to wait at least $\\begin{array}{r}{\\operatorname*{min}_{i\\in[n]}\\left\\{y_{i}^{T-1}+h_{i}\\eta_{i}^{T}+\\tau_{i\\to j}\\right\\}}\\end{array}$ seconds in worker $i$ to get the first vector with progress $T$ ", "page_idx": 29}, {"type": "text", "text": "In the end, the fastest possible time to get a vector with progress $T$ is at least ", "page_idx": 29}, {"type": "equation", "text": "$$\ny^{T}:=\\operatorname*{min}_{j\\in[n]}y_{j}^{T}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Part 3: Reduction to the concentration of $y^{T}$ ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "The last statement means that pr $\\log(x_{i}^{k})<T$ for all $i\\in[n]$ and $k$ such that $t^{k}\\leq\\textstyle{\\frac{1}{2}}y^{T}$ . Therefore, we obtain ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{k\\in S_{t},i\\in[n]}\\left\\|\\nabla f(x_{i}^{k})\\right\\|^{2}>2\\varepsilon\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "for ", "page_idx": 29}, {"type": "equation", "text": "$$\nt\\leq{\\frac{1}{2}}y^{T},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where ", "page_idx": 29}, {"type": "equation", "text": "$$\nT=\\left\\lfloor\\frac{\\Delta L}{2\\varepsilon l_{1}\\Delta^{0}}\\right\\rfloor=\\left\\lfloor c_{T}\\cdot\\frac{\\Delta L}{\\varepsilon}\\right\\rfloor\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and $\\eta_{i}^{j}\\sim\\mathrm{Geometric}(p)$ with ", "page_idx": 30}, {"type": "equation", "text": "$$\np=\\operatorname*{min}\\left\\{\\frac{2\\varepsilon\\gamma_{\\infty}^{2}}{\\sigma^{2}},1\\right\\}=\\operatorname*{min}\\left\\{c_{p}\\cdot\\frac{\\varepsilon}{\\sigma^{2}},1\\right\\},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $c_{T}=3648^{-1}$ and $c_{p}=1058$ are universal constants. Let us fix any determenistic value ${\\bar{y}}\\in\\mathbb{R}$ and take ", "page_idx": 30}, {"type": "equation", "text": "$$\nt={\\frac{1}{2}}{\\bar{y}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Using (38), we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{inf}_{k\\in S_{t},i\\in[n]}\\left\\|\\nabla f(x_{i}^{k})\\right\\|^{2}\\right]\\geq\\mathbb{E}\\left[\\operatorname*{inf}_{k\\in S_{t},i\\in[n]}\\left\\|\\nabla f(x_{i}^{k})\\right\\|^{2}\\right]y^{T}>\\bar{y}\\right]\\mathbb{P}\\left(y^{T}>\\bar{y}\\right)>\\mathbb{P}\\left(y^{T}>\\bar{y}\\right)2\\varepsilon.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Thus, it is sufficient to find any $\\bar{y}\\in\\mathbb R$ such that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(y^{T}\\leq\\bar{y}\\right)\\leq\\frac{1}{2}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "In the following lemma we use the notation of this theorem. We prove it in Section E.5 ", "page_idx": 30}, {"type": "text", "text": "Lemma 2. With ", "text_level": 1, "page_idx": 30}, {"type": "equation", "text": "$$\n\\bar{y}=c\\times\\frac{1}{\\log n+1}\\frac{L\\Delta}{\\varepsilon}\\operatorname*{min}_{j\\in[n]}\\operatorname*{min}_{k\\in[n]}\\operatorname*{max}\\left\\{\\operatorname*{max}\\{\\tau_{\\pi_{j,k}\\to j},h_{\\pi_{j,k}}\\},\\frac{\\sigma^{2}}{\\varepsilon}\\left(\\sum_{i=1}^{k}\\frac{1}{h_{\\pi_{j,i}}}\\right)^{-1}\\right\\},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "we have $\\mathbb{P}\\left(\\boldsymbol{y}^{T}\\leq\\boldsymbol{\\bar{y}}\\right)\\leq\\frac{1}{2}$ , where $\\pi_{j,}$ . is a permutation that sorts ma $\\mathrm{x}\\{h_{i},\\tau_{i\\rightarrow j}\\}$ , i.e, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\{h_{\\pi_{j,1}},\\tau_{\\pi_{j,1}\\to j}\\}\\leq\\cdot\\cdot\\cdot\\leq\\operatorname*{max}\\{h_{\\pi_{j,n}},\\tau_{\\pi_{j,n}\\to j}\\}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "for all $j\\in[n]$ .The quantity $c$ is a universal constant. ", "page_idx": 30}, {"type": "text", "text": "Using Lemma 2, we can conclude that (40) holds and ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{inf}_{k\\in S_{t},i\\in[n]}\\left\\|\\nabla f(x_{i}^{k})\\right\\|^{2}\\right]>\\varepsilon\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "for ", "page_idx": 30}, {"type": "equation", "text": "$$\nt=\\frac{c}{2}\\times\\frac{1}{\\log n+1}\\frac{L\\Delta}{\\varepsilon}\\operatorname*{min}_{j\\in[n]}\\operatorname*{min}_{k\\in[n]}\\operatorname*{max}\\left\\{\\operatorname*{max}\\{\\tau_{\\pi_{j,k}\\to j},h_{\\pi_{j,k}}\\},\\frac{\\sigma^{2}}{\\varepsilon}\\left(\\sum_{i=1}^{k}\\frac{1}{h_{\\pi_{j,i}}}\\right)^{-1}\\right\\}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "E.5Proof of Lemma 2 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Lemma 2. With ", "text_level": 1, "page_idx": 30}, {"type": "equation", "text": "$$\n\\bar{y}=c\\times\\frac{1}{\\log n+1}\\frac{L\\Delta}{\\varepsilon}\\operatorname*{min}_{j\\in[n]}\\operatorname*{min}_{k\\in[n]}\\operatorname*{max}\\left\\{\\operatorname*{max}\\{\\tau_{\\pi_{j,k}\\to j},h_{\\pi_{j,k}}\\},\\frac{\\sigma^{2}}{\\varepsilon}\\left(\\sum_{i=1}^{k}\\frac{1}{h_{\\pi_{j,i}}}\\right)^{-1}\\right\\},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "we have $\\mathbb{P}\\left(\\boldsymbol{y}^{T}\\leq\\boldsymbol{\\bar{y}}\\right)\\leq\\frac{1}{2}$ , where $\\pi_{j,}$ . is a permutation that sorts m $\\mathrm{ax}\\{h_{i},\\tau_{i\\to j}\\}$ , i.e, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\{h_{\\pi_{j,1}},\\tau_{\\pi_{j,1}\\to j}\\}\\leq\\cdot\\cdot\\cdot\\leq\\operatorname*{max}\\{h_{\\pi_{j,n}},\\tau_{\\pi_{j,n}\\to j}\\}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "for all $j\\in[n]$ . The quantity $c$ is a universal constant. ", "page_idx": 30}, {"type": "text", "text": "Proof. Using the Chernoff method for any $s>0$ ,weget ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(y^{k}\\leq t\\right)=\\mathbb{P}\\left(-s y^{k}\\geq-s t\\right)=\\mathbb{P}\\left(e^{-s y^{k}}\\geq e^{-s t}\\right)\\leq e^{s t}\\mathbb{E}\\left[e^{-s y^{k}}\\right]}\\\\ {=e^{s t}\\mathbb{E}\\left[\\exp\\left(-s\\operatorname*{min}_{j\\in[n]}y_{j}^{k}\\right)\\right]=e^{s t}\\mathbb{E}\\left[\\underset{j\\in[n]}{\\operatorname*{max}}\\exp\\left(-s y_{j}^{k}\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We have a maximum operation that complicates the analysis. In response to this problem, we use a well-known trick that bounds a maximum by a sum. ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(y^{k}\\leq t\\right)\\leq e^{s t}\\sum_{j=1}^{n}\\mathbb{E}\\left[\\exp\\left(-s y_{j}^{k}\\right)\\right]\\leq n e^{s t}\\operatorname*{max}_{j\\in[n]}\\mathbb{E}\\left[\\exp\\left(-s y_{j}^{k}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We refer to (Van Handel, 2014)[Part II] for the explanation why it can be (almost) tight. This is the main reason why we get an extra $\\log n$ factor in (41). Let us consider the last exponent separately and use the same trick again: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\exp\\left(-s y_{j}^{k}\\right)\\right]=\\mathbb{E}\\left[\\exp\\left(-s\\operatorname*{min}_{i\\in[n]}\\left\\{y_{i}^{k-1}+h_{i}\\eta_{i}^{k}+\\tau_{i\\to j}\\right\\}\\right)\\right]}\\\\ {=\\mathbb{E}\\left[\\operatorname*{max}_{i\\in[n]}\\exp\\left(-s\\left\\{y_{i}^{k-1}+h_{i}\\eta_{i}^{k}+\\tau_{i\\to j}\\right\\}\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Next, we get ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\exp\\left(-s y_{j}^{k}\\right)\\right]\\leq\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\left[\\exp\\left(-s\\left\\{y_{i}^{k-1}+h_{i}\\eta_{i}^{k}+\\tau_{i\\rightarrow j}\\right\\}\\right)\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\left[e^{-s\\left(h_{i}\\eta_{i}^{k}+\\tau_{i\\rightarrow j}\\right)}\\right]\\mathbb{E}\\left[\\exp\\left(-s y_{i}^{k-1}\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "In the last equality we use the independence. We now bound $\\mathbb{E}\\left[\\exp\\left(-s y_{i}^{k-1}\\right)\\right]$ by $\\mathrm{max}_{i\\in[n]}\\,\\mathbb{E}\\left[\\mathrm{exp}\\left(-s y_{i}^{k-1}\\right)\\right]$ and get ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\exp\\left(-s y_{j}^{k}\\right)\\right]\\leq\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\left[e^{-s\\left(h_{i}\\eta_{i}^{k}+\\tau_{i\\rightarrow j}\\right)}\\right]\\mathbb{E}\\left[\\exp\\left(-s y_{i}^{k-1}\\right)\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left(\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\left[e^{-s\\left(h_{i}\\eta_{i}^{k}+\\tau_{i\\rightarrow j}\\right)}\\right]\\right)\\displaystyle\\operatorname*{max}_{i\\in[n]}\\mathbb{E}\\left[\\exp\\left(-s y_{i}^{k-1}\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Let us fix any $s_{j}>0$ for all $j\\in[n]$ and take $s=\\operatorname*{max}_{j\\in[n]}\\,s_{j}$ Then ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\exp\\left(-s y_{j}^{k}\\right)\\right]\\leq\\left(\\sum_{i=1}^{n}\\mathbb{E}\\left[e^{-s_{j}\\left(h_{i}\\eta_{i}^{k}+\\tau_{i\\rightarrow j}\\right)}\\right]\\right)\\operatorname*{max}_{i\\in[n]}\\mathbb{E}\\left[\\exp\\left(-s y_{i}^{k-1}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Let us fix $\\bar{t}_{j}>0$ and consider ", "page_idx": 31}, {"type": "equation", "text": "$$\na_{j}:=\\sum_{i=1}^{n}\\mathbb{E}\\left[e^{-s_{j}\\left(h_{i}\\eta_{i}^{k}+\\tau_{i\\rightarrow j}\\right)}\\right].\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Then, we can use the following inequalities: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{a_{j}\\leq\\frac{N}{i\\alpha}\\mathbb{E}\\left[e^{-s_{j}(h_{n}^{\\star}+\\tau_{i-\\delta})}1\\left[\\left[h_{i}h_{i}^{\\star}+\\tau_{i-\\delta}\\right]\\leq\\bar{t}_{j}\\right]+e^{-s_{j}(h_{n}^{\\star}+\\tau_{i-\\delta})}\\left(1\\right.\\right.}}\\\\ &{\\left.\\leq\\frac{N}{i\\alpha}\\mathbb{E}\\left[1\\left.\\left[h_{i}h_{i}^{\\star}+\\tau_{i-\\delta}\\right]\\leq\\bar{t}_{j}\\right]+e^{-s_{j}\\bar{t}_{j}}\\left(1-\\left.\\left[h_{i}h_{i}^{\\star}+\\tau_{i-\\delta}\\right]\\leq\\bar{t}_{j}\\right)\\right]\\right]}\\\\ &{=n e^{-s_{j}\\bar{t}_{j}}+(1-e^{-s_{j}\\bar{t}_{j}})\\frac{N}{i\\alpha}\\mathbb{E}\\left[\\left.\\left[h_{i}h_{i}^{\\star}+\\tau_{i-\\delta}\\right]\\leq\\bar{t}_{j}\\right]\\right]}\\\\ &{\\leq n e^{-s_{j}\\bar{t}_{j}}+\\frac{N}{i-1}\\mathbb{E}\\left[\\left.\\left[h_{i}h_{i}^{\\star}+\\tau_{i-\\delta}\\right]\\leq\\bar{t}_{j}\\right]\\right]}\\\\ &{\\leq n e^{-s_{j}\\bar{t}_{j}}+\\sum_{\\eta=1}^{N}\\mathbb{P}\\left(h_{i}h_{i}^{\\star}+\\tau_{i-\\delta}\\leq\\bar{t}_{j}\\right)}\\\\ &{\\leq n e^{-s_{j}\\bar{t}_{j}}+\\sum_{\\eta=1}^{N}\\mathbb{P}\\left(h_{i}h_{i}^{\\star}+\\tau_{i-\\delta}\\leq\\bar{t}_{j}\\right)}\\\\ &{\\leq n e^{-s_{j}\\bar{t}_{j}}+\\sum_{\\eta=1}^{N}\\mathbb{P}\\left(h_{i}h_{i}^{\\star}\\leq\\bar{t}_{j}\\right)\\mathbf{I}[\\tau_{i-\\delta}\\leq\\bar{t}_{j}]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Since $\\eta_{i}^{k}\\sim\\mathrm{Geometric}(p)$ , we get ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(h_{i}\\eta_{i}^{k}\\leq\\bar{t}_{j}\\right)=1-(1-p)^{\\left\\lfloor\\frac{\\bar{t}_{j}}{h_{i}}\\right\\rfloor}\\leq p\\left\\lfloor\\frac{\\bar{t}_{j}}{h_{i}}\\right\\rfloor.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Then ", "page_idx": 32}, {"type": "equation", "text": "$$\na_{j}\\leq n e^{-s_{j}\\bar{t}_{j}}+p\\sum_{i=1}^{n}\\left\\lfloor\\frac{\\bar{t}_{j}}{h_{i}}\\right\\rfloor\\mathbb{1}\\left[\\tau_{i\\rightarrow j}\\leq\\bar{t}_{j}\\right].\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "For all $j\\in[n]$ , we take any permutation $\\pi_{j},$ . that sorts $\\operatorname*{max}\\{h_{i},\\tau_{i\\to j}\\}$ , i.e., ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{max}\\{h_{\\pi_{j,1}},\\tau_{\\pi_{j,1}\\to j}\\}\\leq\\cdot\\cdot\\cdot\\leq\\operatorname*{max}\\{h_{\\pi_{j,n}},\\tau_{\\pi_{j,n}\\to j}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We have ", "page_idx": 32}, {"type": "equation", "text": "$$\na_{j}\\leq n e^{-s_{j}\\bar{t}_{j}}+p\\sum_{i=1}^{n}\\left\\lfloor\\frac{\\bar{t}_{j}}{h_{\\pi_{j,i}}}\\right\\rfloor\\mathbb{1}\\left[\\tau_{\\pi_{j,i}\\rightarrow j}\\leq\\bar{t}_{j}\\right].\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Recall that $\\bar{t}_{j}>0$ is a parameter. In the following technical lemma, we choose $\\bar{t}_{j}$ and show that the second term in (45) is \u201csmall? We prove it in Section E.6. ", "page_idx": 32}, {"type": "text", "text": "Lemma 3. For any $n\\geq1,h_{i}\\geq0,\\tau_{i\\to j}\\geq0$ for all $i,j\\in[n]$ , and $p\\in(0,1]$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\np\\sum_{i=1}^{n}\\left\\lfloor\\frac{\\bar{t}_{j}}{h_{\\pi_{j,i}}}\\right\\rfloor\\mathbb{1}\\left[\\tau_{\\pi_{j,i}\\to j}\\leq\\bar{t}_{j}\\right]\\leq\\frac{1}{8}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "for all $j\\in[n]$ , where ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\bar{t}_{j}:=\\frac{1}{8}\\operatorname*{min}_{k\\in[n]}\\operatorname*{max}\\left\\{\\operatorname*{max}\\{\\tau_{\\pi_{j,k}\\to j},h_{\\pi_{j,k}}\\},\\left(\\sum_{i=1}^{k}\\frac{p}{h_{\\pi_{j,i}}}\\right)^{-1}\\right\\}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and $\\pi_{j,}$ . is a permutation that sorts $\\operatorname*{max}\\{h_{i},\\tau_{i\\to j}\\}$ , i.e., ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\{h_{\\pi_{j,1}},\\tau_{\\pi_{j,1}\\to j}\\}\\leq\\cdot\\cdot\\cdot\\leq\\operatorname*{max}\\{h_{\\pi_{j,n}},\\tau_{\\pi_{j,n}\\to j}\\}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "for all $j\\in[n]$ ", "page_idx": 32}, {"type": "text", "text": "Using Lemma 3 and (45), we get ", "page_idx": 32}, {"type": "equation", "text": "$$\na_{j}\\le n e^{-s_{j}\\bar{t}_{j}}+\\frac{1}{8}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "for all $j\\in[n]$ . Let us take ", "page_idx": 32}, {"type": "equation", "text": "$$\ns_{j}=\\frac{\\log8n}{\\bar{t}_{j}}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "to get ", "page_idx": 32}, {"type": "equation", "text": "$$\na_{j}\\leq\\frac{1}{8}+\\frac{1}{8}\\leq e^{-1}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "for all $j\\in[n]$ . Using (44) and (43), we obtain ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\exp\\left(-s y_{j}^{k}\\right)\\right]\\leq e^{-1}\\operatorname*{max}_{i\\in[n]}\\mathbb{E}\\left[\\exp\\left(-s y_{i}^{k-1}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "for all $j\\in[n]$ . We can conclude that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i\\in[n]}\\mathbb{E}\\left[\\exp\\left(-s y_{i}^{k}\\right)\\right]\\leq e^{-1}\\operatorname*{max}_{i\\in[n]}\\mathbb{E}\\left[\\exp\\left(-s y_{i}^{k-1}\\right)\\right]\\leq e^{-k},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where use the same reasoning in the recursion and $y_{j}^{0}=0$ for all $j\\in[n]$ . We substitute the inequality to (42): ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(y^{k}\\leq t\\right)\\leq n e^{s t-k}=e^{s t-k+\\log n}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "It is sufficient to take $k=T$ and any ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle t\\leq\\frac{1}{s}\\left(T-\\log n+\\log\\frac{1}{2}\\right)}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}=\\frac{1}{8\\log8n}\\operatorname*{min}_{i\\in[n]}\\operatorname*{min}_{k\\in[n]}\\operatorname*{max}\\left\\{\\operatorname*{max}\\{\\tau_{\\pi_{j,k}\\to j},h_{\\pi_{j,k}}\\},\\left(\\sum_{i=1}^{k}\\frac{p}{h_{\\pi_{j,i}}}\\right)^{-1}\\right\\}\\left(T-\\log n+\\log\\frac{1}{2}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "to get ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\boldsymbol{y}^{T}\\leq t\\right)\\leq\\frac{1}{2},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where we use the definitions $s=\\operatorname*{max}_{i\\in[n]}\\,s_{i}$ , (46) and (47). Recall the choice of $T$ and $p$ in (35) and (36): $T=\\left\\lfloor c_{T}\\cdot{\\frac{\\Delta L}{\\varepsilon}}\\right\\rfloor$ and $\\begin{array}{r}{p=\\operatorname*{min}\\left\\lbrace c_{p}\\cdot\\frac{\\varepsilon}{\\sigma^{2}},1\\right\\rbrace}\\end{array}$ for some univeral constants $c_{T}$ and $c_{p}$ Since we have the condition $\\varepsilon<c^{\\prime}L\\Delta$ for some universal constant $c^{\\prime}$ in the conditions of Theorem 19, we can conclude that we can take ", "page_idx": 33}, {"type": "equation", "text": "$$\nt=c\\times\\frac{1}{\\log n+1}\\frac{L\\Delta}{\\varepsilon}\\operatorname*{min}_{i\\in[n]}\\operatorname*{min}_{k\\in[n]}\\operatorname*{max}\\left\\{\\operatorname*{max}\\{\\tau_{\\pi_{j,k}\\to j},h_{\\pi_{j,k}}\\},\\frac{\\sigma^{2}}{\\varepsilon}\\left(\\sum_{i=1}^{k}\\frac{1}{h_{\\pi_{j,i}}}\\right)^{-1}\\right\\},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $c$ is a universal constant. ", "page_idx": 33}, {"type": "text", "text": "E.6 Proof of Lemma 3 ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Lemma 3. For any $n\\geq1,h_{i}\\geq0,\\tau_{i\\to j}\\geq0$ for all $i,j\\in[n]$ , and $p\\in(0,1]$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\np\\sum_{i=1}^{n}\\left\\lfloor\\frac{\\bar{t}_{j}}{h_{\\pi_{j,i}}}\\right\\rfloor\\mathbb{1}\\left[\\tau_{\\pi_{j,i}\\to j}\\leq\\bar{t}_{j}\\right]\\leq\\frac{1}{8}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "for all $j\\in[n]$ , where ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\bar{t}_{j}:=\\frac{1}{8}\\operatorname*{min}_{k\\in[n]}\\operatorname*{max}\\left\\{\\operatorname*{max}\\{\\tau_{\\pi_{j,k}\\to j},h_{\\pi_{j,k}}\\},\\left(\\sum_{i=1}^{k}\\frac{p}{h_{\\pi_{j,i}}}\\right)^{-1}\\right\\}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and $\\pi_{j}$ . is a permutation that sorts n $\\mathrm{nax}\\{h_{i},\\tau_{i\\to j}\\}$ , i.e., ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\{h_{\\pi_{j,1}},\\tau_{\\pi_{j,1}\\to j}\\}\\leq\\cdot\\cdot\\cdot\\leq\\operatorname*{max}\\{h_{\\pi_{j,n}},\\tau_{\\pi_{j,n}\\to j}\\}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "for all $j\\in[n]$ ", "page_idx": 33}, {"type": "text", "text": "Let us define $k^{*}\\in[n]$ as the largest index that minimizes (46). ", "page_idx": 33}, {"type": "text", "text": "(Part 1): bound $\\bar{t}_{j}$ by $\\operatorname*{max}\\{\\tau_{\\pi_{j,k^{*}+1}\\to j},h_{\\pi_{j,k^{*}+1}}\\}$ Let us consider the case $k^{*}<\\check{n}$ We have two options: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\operatorname*{lax}\\{\\tau_{\\pi_{j,k^{*}}\\to j},h_{\\pi_{j,k^{*}}}\\}\\ge\\left(\\sum_{i=1}^{k^{*}}\\frac{p}{h_{\\pi_{j,i}}}\\right)^{-1},\\mathrm{then}}}\\\\ &{}&{\\operatorname*{max}\\{\\tau_{\\pi_{j,k^{*}+1}\\to j},h_{\\pi_{j,k^{*}+1}}\\}\\ge\\left(\\displaystyle\\sum_{i=1}^{k^{*}+1}\\frac{p}{h_{\\pi_{j,i}}}\\right)^{-1},}\\end{array}\n$$1. Let ", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "since $\\operatorname*{max}\\{\\tau_{\\pi_{j,i}\\to j},h_{\\pi_{j,i}}\\}$ are sorted. Then, we get ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{t}_{j}<\\frac{1}{8}\\operatorname*{max}\\left\\{\\operatorname*{max}\\{\\tau_{\\pi_{j,k^{*}+1}\\to j},h_{\\pi_{j,k^{*}+1}}\\},\\left(\\displaystyle\\sum_{i=1}^{k^{*}+1}\\frac{p}{h_{\\pi_{j,i}}}\\right)^{-1}\\right\\}}\\\\ &{\\;\\;\\;\\frac{(49)}{8}\\operatorname*{max}\\{\\tau_{\\pi_{j,k^{*}+1}\\to j},h_{\\pi_{j,k^{*}+1}}\\}\\leq\\operatorname*{max}\\{\\tau_{\\pi_{j,k^{*}+1}\\to j},h_{\\pi_{j,k^{*}+1}}\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "The first inequality follows from the fact that $k^{*}$ is the largest minimizer. ", "page_idx": 34}, {"type": "text", "text": "2. Letmax{T\u03c0g,k\\*\u2192j,hmg,k\\*}< 1 h\u201d\uff09\"\uff0cthen it is not possible that because it would yield the inequality ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{1}{8}\\left(\\sum_{i=1}^{k^{*}}\\frac{p}{h_{\\pi_{j,i}}}\\right)^{-1}\\geq\\frac{1}{8}\\left(\\sum_{i=1}^{k^{*}+1}\\frac{p}{h_{\\pi_{j,i}}}\\right)^{-1}=\\frac{1}{8}\\operatorname*{max}\\left\\{\\operatorname*{max}\\{\\tau_{\\pi_{j,k^{*}+1}\\to j},h_{\\pi_{j,k^{*}+1}}\\},\\left(\\sum_{i=1}^{k^{*}+1}\\frac{p}{h_{\\pi_{j,i}}}\\right)^{-1}\\right.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "The last inequality contradicts the fact that $k^{*}$ is the largest minimizer. Thus, if $k^{*}<\\ n$ and $\\begin{array}{r}{\\operatorname*{max}\\{\\tau_{\\pi_{j,k^{*}}\\to j},h_{\\pi_{j,k^{*}}}\\}<\\left(\\sum_{i=1}^{k^{*}}\\frac{p}{h_{\\pi_{j,i}}}\\right)^{-1}}\\end{array}$ , then ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\bar{t}_{j}<\\frac{1}{8}\\operatorname*{max}\\left\\{\\operatorname*{max}\\{\\tau_{\\pi_{j,k^{*}+1}\\to j},h_{\\pi_{j,k^{*}+1}}\\},\\left(\\sum_{i=1}^{k^{*}+1}\\frac{p}{h_{\\pi_{j,i}}}\\right)^{-1}\\right\\}=\\frac{1}{8}\\operatorname*{max}\\{\\tau_{\\pi_{j,k^{*}+1}\\to j},h_{\\pi_{j,k^{*}+1}}\\}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "In total, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\bar{t}_{j}<\\operatorname*{max}\\{\\tau_{\\pi_{j,k^{*}+1}\\to j},h_{\\pi_{j,k^{*}+1}}\\}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "if $k^{*}<n$ . Using this inequality, we get ", "page_idx": 34}, {"type": "equation", "text": "$$\np\\sum_{i=1}^{n}\\left\\lfloor\\frac{\\bar{t}_{j}}{h_{\\pi_{j,i}}}\\right\\rfloor\\mathbb{1}\\left[\\tau_{\\pi_{j,i}\\to j}\\leq\\bar{t}_{j}\\right]=p\\sum_{i=1}^{k^{\\ast}}\\left\\lfloor\\frac{\\bar{t}_{j}}{h_{\\pi_{j,i}}}\\right\\rfloor\\mathbb{1}\\left[\\tau_{\\pi_{j,i}\\to j}\\leq\\bar{t}_{j}\\right]\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "for any $k^{*}$ ", "page_idx": 34}, {"type": "text", "text": "(Part 2) ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "We have three options: ", "page_idx": 34}, {"type": "text", "text": "1. If $\\begin{array}{r}{\\left(\\sum_{i=1}^{k^{*}}\\frac{p}{h_{\\pi_{j,i}}}\\right)^{-1}\\geq\\operatorname*{max}\\{\\tau_{\\pi_{j,k^{*}}\\to j},h_{\\pi_{j,k^{*}}}\\}}\\end{array}$ then, using (50) and $\\lfloor x\\rfloor\\leq x$ for ll $x\\geq0$ we get ", "page_idx": 34}, {"type": "equation", "text": "$$\np\\sum_{i=1}^{n}\\left\\lfloor\\frac{\\bar{t}_{j}}{h_{\\pi_{j,i}}}\\right\\rfloor\\mathbb{1}\\left[\\tau_{\\pi_{j,i}\\to j}\\leq\\bar{t}_{j}\\right]\\leq p\\sum_{i=1}^{k^{*}}\\frac{\\bar{t}_{j}}{h_{\\pi_{j,i}}}=\\frac{1}{8}\\left(\\sum_{i=1}^{k^{*}}\\frac{p}{h_{\\pi_{j,i}}}\\right)^{-1}p\\sum_{i=1}^{k^{*}}\\frac{1}{h_{\\pi_{j,i}}}=\\frac{1}{8}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "2.If $\\begin{array}{r}{\\left(\\sum_{i=1}^{k^{*}}\\frac{p}{h_{\\pi_{j,i}}}\\right)^{-1}<\\operatorname*{max}\\{\\tau_{\\pi_{j,k^{*}}\\to j},h_{\\pi_{j,k^{*}}}\\}}\\end{array}$ and $k^{*}=1$ , then", "page_idx": 34}, {"type": "equation", "text": "$$\np\\sum_{i=1}^{n}\\left\\lfloor\\frac{\\overline{{t}}_{j}}{h_{\\pi_{j,i}}}\\right\\rfloor\\mathbb{1}\\left[\\tau_{\\pi_{j,i}\\to j}\\leq\\bar{t}_{j}\\right]=p\\left\\lfloor\\frac{\\overline{{t}}_{j}}{h_{\\pi_{j,k}*}}\\right\\rfloor\\mathbb{1}\\left[\\tau_{\\pi_{j,k}*\\to j}\\leq\\bar{t}_{j}\\right]=0\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "because ", "page_idx": 34}, {"type": "text", "text": "$\\frac{\\cdot}{j}=\\frac{1}{8}\\operatorname*{max}\\left\\{\\operatorname*{max}\\{\\tau_{\\pi_{j,k^{*}}\\to j},h_{\\pi_{j,k^{*}}}\\},\\left(\\sum_{i=1}^{k^{*}}\\frac{p}{h_{\\pi_{j,i}}}\\right)^{-1}\\right\\}=\\frac{1}{8}\\operatorname*{max}\\{\\tau_{\\pi_{j,k^{*}}\\to j},h_{\\pi_{j,k^{*}}}\\}<\\operatorname*{max}\\{\\tau_{\\pi_{j,k^{*}}\\to j},h_{\\pi_{j,k^{*}}}\\}.$ 3. If $\\begin{array}{r}{\\left(\\sum_{i=1}^{k^{*}}\\frac{p}{h_{\\pi_{j,i}}}\\right)^{-1}<\\operatorname*{max}\\{\\tau_{\\pi_{j,k^{*}}\\to j},h_{\\pi_{j,k^{*}}}\\}}\\end{array}$ and $k^{*}>1$ , then $\\begin{array}{r}{\\bar{t}_{j}\\,=\\,\\frac{1}{8}\\operatorname*{max}\\{\\tau_{\\pi_{j,k^{*}}\\to j},h_{\\pi_{j,k^{*}}}\\}}\\end{array}$ and ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\left\\lfloor\\frac{\\bar{t}_{j}}{h_{\\pi_{j,i}}}\\right\\rfloor\\mathbb{1}\\left[\\tau_{\\pi_{j,i}\\to j}\\leq\\bar{t}_{j}\\right]=0\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "for all $i\\leq k^{*}$ such that $\\operatorname*{max}\\{\\tau_{\\pi_{j,i}\\to j},h_{\\pi_{j,i}}\\}=\\operatorname*{max}\\{\\tau_{\\pi_{j,k^{*}}\\to j},h_{\\pi_{j,k^{*}}}\\}.$ If this equality holds for all $i\\leq k^{*}$ , then ", "page_idx": 34}, {"type": "equation", "text": "$$\np\\sum_{i=1}^{n}\\left\\lfloor\\frac{\\bar{t}_{j}}{h_{\\pi_{j,i}}}\\right\\rfloor\\mathbb{1}\\left[\\tau_{\\pi_{j,i}\\to j}\\leq\\bar{t}_{j}\\right]=0.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Otherwise, there exists $\\ell<k^{*}$ such that $\\operatorname*{max}\\{\\tau_{\\pi_{j,\\ell}\\to j},h_{\\pi_{j,\\ell}}\\}<\\operatorname*{max}\\{\\tau_{\\pi_{j,k^{*}}\\to j},h_{\\pi_{j,k^{*}}}\\}$ and ", "page_idx": 35}, {"type": "equation", "text": "$$\np\\sum_{i=1}^{n}\\left\\lfloor\\frac{\\bar{t}_{j}}{h_{\\pi_{j,i}}}\\right\\rfloor\\mathbb{1}\\left[\\tau_{\\pi_{j,i}\\to j}\\leq\\bar{t}_{j}\\right]=p\\sum_{i=1}^{\\ell}\\left\\lfloor\\frac{\\bar{t}_{j}}{h_{\\pi_{j,i}}}\\right\\rfloor\\mathbb{1}\\left[\\tau_{\\pi_{j,i}\\to j}\\leq\\bar{t}_{j}\\right].\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "It s not pssible that $\\begin{array}{r}{\\operatorname*{max}\\{\\tau_{\\pi_{j,\\ell}\\to j},h_{\\pi_{j,\\ell}}\\}\\geq\\left(\\sum_{i=1}^{\\ell}\\frac{p}{h_{\\pi_{j,i}}}\\right)^{-1}}\\end{array}$ because it would yield the inequality ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{t}_{j}=\\frac{1}{8}\\operatorname*{max}\\{\\tau_{\\pi_{j,k^{*}}\\to j},h_{\\pi_{j,k^{*}}}\\}>\\frac{1}{8}\\operatorname*{max}\\{\\tau_{\\pi_{j,\\ell}\\to j},h_{\\pi_{j,\\ell}}\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\frac{1}{8}\\operatorname*{max}\\left\\{\\operatorname*{max}\\{\\tau_{\\pi_{j,\\ell}\\to j},h_{\\pi_{j,\\ell}}\\},\\left(\\sum_{i=1}^{\\ell}\\frac{p}{h_{\\pi_{j,i}}}\\right)^{-1}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "that contradicts the fact that $\\bar{t_{j}}$ is the minimum (see (46)). Thus, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\bar{t}_{j}\\leq\\frac{1}{8}\\operatorname*{max}\\left\\{\\operatorname*{max}\\{\\tau_{\\pi_{j,\\ell}\\to j},h_{\\pi_{j,\\ell}}\\},\\left(\\sum_{i=1}^{\\ell}\\frac{p}{h_{\\pi_{j,i}}}\\right)^{-1}\\right\\}=\\frac{1}{8}\\left(\\sum_{i=1}^{\\ell}\\frac{p}{h_{\\pi_{j,i}}}\\right)^{-1}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{p\\sum_{i=1}^{n}\\left\\lfloor\\frac{\\bar{t}_{j}}{h_{\\pi_{j,i}}}\\right\\rfloor\\mathbb{I}\\left[\\tau_{\\pi_{j,i}\\to j}\\leq\\bar{t}_{j}\\right]=p\\sum_{i=1}^{\\ell}\\left\\lfloor\\frac{\\bar{t}_{j}}{h_{\\pi_{j,i}}}\\right\\rfloor\\mathbb{I}\\left[\\tau_{\\pi_{j,i}\\to j}\\leq\\bar{t}_{j}\\right]\\leq p\\sum_{i=1}^{\\ell}\\frac{\\bar{t}_{j}}{h_{\\pi_{j,i}}}}}\\\\ &{}&{\\leq\\frac{1}{8}\\left(\\sum_{i=1}^{\\ell}\\frac{p}{h_{\\pi_{j,i}}}\\right)^{-1}p\\sum_{i=1}^{\\ell}\\frac{1}{h_{\\pi_{j,i}}}\\leq\\frac{1}{8}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "In total, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\np\\sum_{i=1}^{n}\\left\\lfloor\\frac{\\bar{t}_{j}}{h_{\\pi_{j,i}}}\\right\\rfloor\\mathbb{1}\\left[\\tau_{\\pi_{j,i}\\to j}\\leq\\bar{t}_{j}\\right]\\leq\\frac{1}{8}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "for $\\bar{t_{j}}$ from (46). ", "page_idx": 35}, {"type": "text", "text": "F  Lower Bound in the Heterogeneous Setup ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "In this case, we consider the following oracle mappings. For all $i\\in[n]$ ,wedefine ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O_{i}:\\underbrace{{\\mathbb{R}}_{\\geq0}}_{\\mathrm{time}}\\times\\underbrace{{\\mathbb{R}}^{d}}_{\\mathrm{point}}\\times\\underbrace{({\\mathbb{R}}_{\\geq0}\\times{\\mathbb{R}}^{d}\\times\\{0,1\\})}_{\\mathrm{input~state}}\\rightarrow\\underbrace{({\\mathbb{R}}_{\\geq0}\\times{\\mathbb{R}}^{d}\\times\\{0,1\\})}_{\\mathrm{output~state}}\\times{\\mathbb{R}}^{d}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "such that ", "page_idx": 35}, {"type": "equation", "text": "$$\nO_{i}(t,x,(s_{t},s_{x},s_{q}))=\\left\\{\\!\\!\\begin{array}{l l}{((t,x,1),}&{0),\\quad s_{q}=0,}\\\\ {((s_{t},s_{x},1),}&{0),\\quad s_{q}=1,t<s_{t}+h_{i},}\\\\ {((0,0,0),}&{\\nabla f_{i}(s_{x};\\xi)),}&{s_{q}=1,t\\geq s_{t}+h_{i},}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $\\xi\\sim\\mathcal{D}$ . Unlike (27), the mapping (51) returns $\\nabla f_{i}(s_{x};\\xi)$ ", "page_idx": 35}, {"type": "text", "text": "Theorem 20. Consider Protocol 8 with the mappings (51). We take any $h_{i}\\geq0$ and $\\tau_{i\\to j}\\geq0$ for all $i,j\\in[n]$ such that $\\tau_{i\\to j}\\leq\\tau_{i\\to k}+\\tau_{k\\to j}$ for all $i,k,j\\in[n]$ . We fix $L,\\Delta,\\varepsilon,\\sigma^{2}>0$ that satisfy the inequality $\\varepsilon<c_{1}L\\Delta$ For any algorithm $A\\in{\\mathcal{A}}_{\\mathrm{zr}}$ ,there exists a function $\\textstyle f={\\frac{1}{n}}\\sum_{i=1}^{n}f_{i}$ which satisfyAssumptions $^{\\,l}$ 2and $f(0)-f^{*}\\leq\\Delta_{}$ and stochastic gradientmappings $\\nabla f_{i}(\\cdot;\\cdot)$ ,which satisfy Assumption $3\\;(\\mathbb{E}_{\\xi}[\\nabla f_{i}(x;\\xi)]=\\nabla f_{i}(x)$ and $\\mathbb{E}_{\\xi}[\\|\\nabla f_{i}(\\bar{x};\\xi)-\\nabla f_{i}(\\bar{x})\\|^{2}]\\le\\sigma^{2}\\right)$ , such that $\\mathbb{E}\\left[\\operatorname*{inf}_{k\\in S_{t},i\\in[n]}\\left\\|\\nabla f(x_{i}^{k})\\right\\|^{2}\\right]>\\varepsilon$ where ${S_{t}:=\\left\\{k\\in\\mathbb{N}_{0}\\,|\\,t^{k}\\leq t\\right\\}}$ ", "page_idx": 35}, {"type": "equation", "text": "$$\nt=c_{2}\\times\\frac{L\\Delta}{\\varepsilon}\\operatorname*{max}\\left\\{\\operatorname*{max}_{i,j\\in[n]}\\tau_{i\\to j},\\operatorname*{max}_{i\\in[n]}h_{i},\\frac{\\sigma^{2}}{n\\varepsilon}\\left(\\frac{1}{n}\\sum_{i=1}^{n}h_{i}\\right)\\right\\},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "The quantities $c_{1}$ and $c_{2}$ are universal constants. The sequences $x^{k}$ and $t^{k}$ are defined in Protocol 8. ", "page_idx": 35}, {"type": "text", "text": "Proof. The last two terms in the max follow from Theorem A.2 by Tyurin and Richtarik (2023), who considered the same setup but with $\\tau_{i\\to j}=0$ for all $i,j\\in[n]$ . It is left to prove the first term. Let us fix $\\lambda>0$ . Let us take any pair $(\\bar{i},\\bar{j})$ of workers such that $\\operatorname*{max}_{i,j\\in[n]}\\tau_{i\\to j}=\\tau_{\\bar{i}\\to\\bar{j}}$ Next, we split the blocks of the function $F_{T}(x)$ from (33) and define two new functions: ", "page_idx": 36}, {"type": "equation", "text": "$$\nF_{T,1}(x):=-\\Psi(1)\\Phi(x_{1})+\\sum_{\\substack{i\\in\\{2,\\ldots,T\\},i\\,|\\,2=1}}\\left[\\Psi(-x_{i-1})\\Phi(-x_{i})-\\Psi(x_{i-1})\\Phi(x_{i})\\right],\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "and ", "page_idx": 36}, {"type": "equation", "text": "$$\nF_{T,2}(x):=\\sum_{\\substack{i\\in\\{2,\\ldots,T\\},\\,i\\,|\\,2=0}}\\left[\\Psi(-x_{i-1})\\Phi(-x_{i})-\\Psi(x_{i-1})\\Phi(x_{i})\\right].\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We consider the following functions $f_{i}$ ", "page_idx": 36}, {"type": "equation", "text": "$$\nf_{i}(x):=\\left\\{\\begin{array}{l l}{\\frac{n L\\lambda^{2}}{l_{1}}F_{T,1}\\left(\\frac{x}{\\lambda}\\right),}&{i=\\bar{i},}\\\\ {\\frac{n L\\lambda^{2}}{l_{1}}F_{T,2}\\left(\\frac{x}{\\lambda}\\right),}&{i=\\bar{j},}\\\\ {0,}&{i\\neq\\bar{i}\\mathrm{~and~}i\\neq\\bar{j}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Then, we get ", "page_idx": 36}, {"type": "equation", "text": "$$\nf(x)={\\frac{1}{n}}\\sum_{i=1}^{n}f_{i}(x)={\\frac{1}{n}}\\left({\\frac{n L\\lambda^{2}}{l_{1}}}F_{T,1}\\left({\\frac{x}{\\lambda}}\\right)+{\\frac{n L\\lambda^{2}}{l_{1}}}F_{T,2}\\left({\\frac{x}{\\lambda}}\\right)\\right)={\\frac{L\\lambda^{2}}{l_{1}}}F_{T}\\left({\\frac{x}{\\lambda}}\\right).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Let us show that the function $f$ is $L$ -smooth: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\left\\|\\nabla f(x)-\\nabla f(y)\\right\\|=\\frac{L\\lambda}{l_{1}}\\left\\|\\nabla F_{T}\\left(\\frac{x}{\\lambda}\\right)-\\nabla F_{T}\\left(\\frac{y}{\\lambda}\\right)\\right\\|\\leq L\\left\\|x-y\\right\\|.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Let us take ", "page_idx": 36}, {"type": "equation", "text": "$$\nT=\\left\\lfloor\\frac{\\Delta l_{1}}{L\\lambda^{2}\\Delta^{0}}\\right\\rfloor,\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "then ", "page_idx": 36}, {"type": "equation", "text": "$$\nf(0)-\\operatorname*{inf}_{x\\in\\mathbb{R}^{T}}f(x)=\\frac{L\\lambda^{2}}{l_{1}}(F_{T}\\left(0\\right)-\\operatorname*{inf}_{x\\in\\mathbb{R}^{T}}F_{T}(x))\\leq\\frac{L\\lambda^{2}\\Delta^{0}T}{l_{1}}\\leq\\Delta.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We showed that the function $f$ satisfy Assumptions $1,2$ and $f(0)-f^{*}\\leq\\Delta$ ", "page_idx": 36}, {"type": "text", "text": "In the oracles $O_{i}$ , we simply take the non-stochastic mappings $\\nabla f_{i}(x;\\xi):=\\nabla f_{i}(x)$ that are unbiased and O-variance-bounded. ", "page_idx": 36}, {"type": "text", "text": "We take ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\lambda=\\frac{l_{1}\\sqrt{\\varepsilon}}{L}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "to ensure that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\left\\|\\nabla f(x)\\right\\|^{2}=\\frac{L^{2}\\lambda^{2}}{l_{1}^{2}}\\left\\|\\nabla F_{T}\\left(\\frac{x}{\\lambda}\\right)\\right\\|^{2}>\\frac{L^{2}\\lambda^{2}}{l_{1}^{2}}=\\varepsilon\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "for all $x\\in\\mathbb{R}^{T}$ such that $\\mathrm{prog}(x)<T$ In the last inequality, we use Lemma 1. Thus ", "page_idx": 36}, {"type": "equation", "text": "$$\nT=\\left\\lfloor\\frac{\\Delta L}{l_{1}\\varepsilon\\Delta^{0}}\\right\\rfloor.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Only workers $\\bar{i}$ and $\\bar{j}$ contain the information about the function $f$ . The function $f$ is a zero-chain function, and we split it between workers $\\bar{i}$ and $\\bar{j}$ . Due to this splitting, workers $\\bar{i}$ and $\\bar{j}$ have to communicate to find the next non-zero coordinate. Only worker $\\bar{i}$ can get a non-zero value in the first coordinate through the gradient of $F_{T,1}$ . Next, this worker can not get a non-zero value in the second coordinate due to the construction of (52). Thus, it has to pass a vector with a non-zero value in the first coordinate to worker $\\bar{j}$ because only this worker can get a non-zero value in the second coordinate. This communication takes at least $\\pi_{i\\rightarrow\\bar{j}}$ seconds. Using the same reasoning, worker $\\bar{j}$ has to send a vector to worker $\\bar{i}$ once worker $\\bar{j}$ has discovered a non-zero value in the second coordinate. ", "page_idx": 36}, {"type": "text", "text": "An algorithm has to repeat such communications at least $\\textstyle{\\frac{T-1}{2}}$ times to find a vector $x\\in\\mathbb{R}^{T}$ such that $\\bar{\\mathrm{prog}}(x)=T$ ", "page_idx": 37}, {"type": "text", "text": "Thus, we get ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{k\\in S_{t},i\\in[n]}\\left\\|\\nabla f(x_{i}^{k})\\right\\|^{2}>\\varepsilon\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "for ", "page_idx": 37}, {"type": "equation", "text": "$$\nt=\\tau_{\\bar{i}\\to\\bar{j}}\\left(\\frac{T-1}{2}\\right)=\\frac{\\operatorname*{max}_{i,j\\in[n]}\\tau_{i\\to j}}{2}\\left(\\left\\lfloor\\frac{\\Delta L}{l_{1}\\varepsilon\\Delta^{0}}\\right\\rfloor-1\\right).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "G  Proof of the Time Complexity for Homogeneous Case ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Theorem 4. Let Assumptions $^{\\,l}$ , 2, and 3 hold. We take $\\gamma=1/2L$ , batch size $S=\\operatorname*{max}\\lbrace\\left\\lceil\\sigma^{2}/\\varepsilon\\right\\rceil,1\\rbrace$ any pivot worker $j^{*}\\in[n]$ , and any spanning trees $\\overline{{s t}}$ and $\\overline{{s t}}_{\\mathrm{bc}}$ in Algorithm 2. For all $K\\geq16L\\Delta/\\varepsilon$ we get $\\begin{array}{r}{\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\|\\nabla f(x^{k})\\right\\|^{2}\\right]\\leq\\varepsilon}\\end{array}$ ", "page_idx": 37}, {"type": "text", "text": "Proof. Algorithm 2 produces the sequence $x^{k}$ such as $\\begin{array}{r}{x^{k+1}\\,=\\,x^{k}\\,-\\,\\frac{\\gamma}{s^{k}}g^{k}\\,=\\,x^{k}\\,-\\,\\gamma\\bar{g}^{k}\\,}\\end{array}$ where $\\begin{array}{r}{\\bar{g}^{k}:=\\frac{1}{s^{k}}g^{k}}\\end{array}$ . By the design of the algorithm, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\bar{g}^{k}=\\frac{1}{s^{k}}\\sum_{i=1}^{s^{k}}\\nabla f(x^{k};\\xi_{i}),\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where the $\\xi_{i}$ are independent random samples and $s^{k}\\geq S$ We do not dismiss the possibility that the computation and communication times are random, so $s^{k}$ can be random. Assume $\\lvert\\nu_{k}$ is a $\\sigma$ -algebra generated by all computation and communication times and $g^{0},\\ldots,g^{k-1}$ , then $s^{k}$ is $\\lvert\\lambda_{k}$ -measurable. Using the independence and Assumption 3, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\left[\\left.\\bar{g}^{k}\\right|\\mathcal{G}_{k}\\right]=\\mathbb{E}\\left[\\mathbb{E}\\left[\\left.\\frac{1}{s^{k}}\\sum_{i=1}^{s^{k}}\\nabla f(x^{k};\\xi_{i})\\right|\\mathcal{V}_{k}\\right]\\right]\\bigg|\\mathcal{G}_{k}\\right]=\\mathbb{E}\\left[\\frac{1}{s^{k}}\\sum_{i=1}^{s^{k}}\\mathbb{E}\\left[\\left.\\nabla f(x^{k};\\xi_{i})\\right|\\mathcal{V}_{k}\\right]\\bigg|\\mathcal{G}_{k}\\right]=\\nabla f(x^{k};\\xi_{i}).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert\\bar{g}^{k}-\\nabla f(x^{k})\\right\\Vert^{2}\\middle|\\mathcal{G}_{k}\\right]}\\\\ &{=\\mathbb{E}\\left[\\mathbb{E}\\left[\\left\\Vert\\frac{1}{s^{k}}\\sum_{i=1}^{s^{k}}\\nabla f(x^{k};\\xi_{i})-\\nabla f(x^{k})\\right\\Vert^{2}\\middle|\\mathcal{V}_{k}\\right]\\Bigg|\\mathcal{G}_{k}\\right]}\\\\ &{=\\mathbb{E}\\left[\\frac{1}{(s^{k})^{2}}\\sum_{i=1}^{s^{k}}\\mathbb{E}\\left[\\left\\Vert\\nabla f(x^{k};\\xi_{i})-\\nabla f(x^{k})\\right\\Vert^{2}\\middle|\\mathcal{V}_{k}\\right]\\Bigg|\\mathcal{G}_{k}\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\frac{\\sigma^{2}}{s^{k}}\\Big|\\mathcal{G}_{k}\\right]\\leq\\frac{\\sigma^{2}}{S}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $\\mathcal{G}_{k}$ isa $\\sigma$ -algebra generated by $\\bar{g}^{0},\\ldots,\\bar{g}^{k-1}$ . We can use a well-known SGD result (Ghadimi and Lan, 2013; Khaled and Richtarik, 2022). Using Theorem 21, for the stepsize ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\gamma=\\frac{1}{2L}\\operatorname*{min}\\left\\{1,\\frac{\\varepsilon S}{\\sigma^{2}}\\right\\},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\|\\nabla f(x^{k})\\right\\|^{2}\\right]\\leq\\varepsilon,\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "if ", "page_idx": 38}, {"type": "equation", "text": "$$\nK\\ge\\frac{8\\Delta L}{\\varepsilon}+\\frac{8\\Delta L\\sigma^{2}}{\\varepsilon^{2}S}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Using the choice of $S$ we get that Algorithm 2 converges after ", "page_idx": 38}, {"type": "equation", "text": "$$\nK\\geq\\frac{16\\Delta L}{\\varepsilon}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "steps with ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\gamma={\\frac{1}{2L}}\\operatorname*{min}\\left\\{1,{\\frac{\\varepsilon S}{\\sigma^{2}}}\\right\\}={\\frac{1}{2L}}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Theorem 5. Consider the assumptions and the parameters from Theorem 4. For any pivot worker $j^{*}\\in[n]$ andspanningtrees $\\overline{{s t}}$ and $\\overline{{s t}}_{\\mathrm{bc}}$ , Algorithm 2 converges after at most ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Theta\\left(\\frac{L\\Delta}{\\varepsilon}t^{*}(\\sigma^{2}/\\varepsilon,[h_{i}]_{i=1}^{n},[\\mu_{i\\to j^{*}}+\\mu_{j^{*}\\to i}]_{i=1}^{n})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "seconds, where $\\mu_{i\\to j^{*}}$ $(\\mu_{j^{*}\\to i})$ is anupper bound on the times required to send a vector from worker $i$ to worker $j^{*}$ (from worker $j^{*}$ to worker $i$ )along the spanning tree $\\overline{{s t}}$ (spanning tree $\\overline{{s t}}_{\\mathrm{bc}},$ ", "page_idx": 38}, {"type": "text", "text": "Proof. Due to Theorem 4, we know that Algorithm 3 finds an $\\varepsilon\\cdot$ -stationary point after at most $\\begin{array}{r}{K=\\Theta\\left(\\frac{L\\Delta}{\\varepsilon}\\right)}\\end{array}$ iterations. It is lefto bound the time of one iteration to prove the theorem. ", "page_idx": 38}, {"type": "text", "text": "For all $j\\in[n]$ , we define $\\pi_{j,}$ . as a permutation that sorts $\\{\\operatorname*{max}\\{\\mu_{i\\to j}+\\mu_{j\\to i},h_{i}\\}\\}_{i=1}^{n}$ as ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{max}\\{\\mu_{\\pi_{j,1}\\to j}+\\mu_{j\\to\\pi_{j,1}},h_{\\pi_{j,1}}\\}\\leq\\cdots\\leq\\operatorname*{max}\\{\\mu_{\\pi_{j,n}\\to j}+\\mu_{j\\to\\pi_{j,n}},h_{\\pi_{j,n}}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Let us define the index ", "page_idx": 38}, {"type": "equation", "text": "$$\nk^{*}=\\arg\\operatorname*{min}_{k\\in[n]}\\operatorname*{max}\\left\\{\\operatorname*{max}\\{\\mu_{\\pi_{j^{*},k}\\to j^{*}}+\\mu_{j^{*}\\to\\pi_{j^{*},k}},h_{\\pi_{j^{*},k}}\\},S\\left(\\sum_{i=1}^{k}\\frac{1}{h_{\\pi_{j^{*},i}}}\\right)^{-1}\\right\\}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "and the set ", "page_idx": 38}, {"type": "equation", "text": "$$\nA^{*}:=\\{\\pi_{j^{*},i}\\in[n]\\,|\\,i\\leq k^{*}\\}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "that represents a set of the \u201cfastest? workers that can potentially contribute to an optimization process. Wetake ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{t}:=2\\operatorname*{max}\\left\\{\\operatorname*{max}\\{\\mu_{\\pi_{j^{*},k^{*}}\\to j^{*}}+\\mu_{j^{*}\\to\\pi_{j^{*},k^{*}}},h_{\\pi_{j^{*},k^{*}}}\\},S\\left(\\sum_{i=1}^{k^{*}}\\frac{1}{h_{\\pi_{j^{*},i}}}\\right)^{-1}\\right\\}}\\\\ &{=2\\operatorname*{min}_{k\\in[n]}\\operatorname*{max}\\left\\{\\operatorname*{max}\\{\\mu_{\\pi_{j^{*},k}\\to j^{*}}+\\mu_{j^{*}\\to\\pi_{j^{*},k}},h_{\\pi_{j^{*},k}}\\},S\\left(\\sum_{i=1}^{k}\\frac{1}{h_{\\pi_{j^{*},i}}}\\right)^{-1}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "In the following steps of the proof we show that every iteration takes at most ", "page_idx": 38}, {"type": "text", "text": "\u5341 (Step 1): Calculate enough stochastic gradients (Step 2): Send stochastic gradients to Process $j^{*}$ (Step 3): Broadcast a new point ", "page_idx": 38}, {"type": "text", "text": "seconds. ", "page_idx": 38}, {"type": "text", "text": "(Step 1): Since it takes at most $h_{i}$ seconds to calculate a stochastic gradient in worker $i$ , all workers from the set $A^{*}$ will calculate at least ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\sum_{i\\in A^{*}}\\left\\lfloor\\frac{\\bar{t}}{h_{i}}\\right\\rfloor=\\sum_{i=1}^{k^{*}}\\left\\lfloor\\frac{\\bar{t}}{h_{\\pi_{j^{*},i}}}\\right\\rfloor\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "stochastic gradients after $\\bar{t}$ seconds at the point $x^{k}$ . We have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{t}}\\geq2\\operatorname*{max}\\{\\mu_{\\pi_{j^{*},k^{*}}\\to j^{*}}+\\mu_{j^{*}\\to\\pi_{j^{*},k^{*}}},h_{\\pi_{j^{*},k^{*}}}\\}\\geq2\\operatorname*{max}\\{\\mu_{\\pi_{j^{*},i}\\to j^{*}}+\\mu_{j^{*}\\to\\pi_{j^{*},i}},h_{\\pi_{j^{*},i}}\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "for all $i\\leq k^{*}$ by the definition of the permutations $\\pi._{,\\cdot}.$ Therefore, ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\bar{t}\\geq2h_{\\pi_{j^{\\ast},i}}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "for all $i\\leq k^{*}$ . Thus, using (54) and $\\lfloor x\\rfloor\\geq{\\frac{x}{2}}$ for all $x\\geq1$ , we get ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\sum_{i\\in A^{*}}\\left\\lfloor\\frac{\\bar{t}}{h_{i}}\\right\\rfloor\\geq\\sum_{i=1}^{k^{*}}\\frac{\\bar{t}}{2h_{\\pi_{j^{*},i}}}\\stackrel{(53)}{\\geq}S.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Therefore, after $\\bar{t}$ seconds the algorithm will calculate at least $S$ stochastic gradients at the point $x^{k}$ using the workers $A^{*}$ ", "page_idx": 39}, {"type": "text", "text": "(Step 2): By the design of Algorithm 4, once a stochastic gradient $\\nabla f(x^{k};\\bar{\\xi})$ is calculated, it is added to $g_{i,\\mathrm{next}}^{k}$ . Then, $g_{i,\\mathrm{next}}^{k}$ is assigned to $g_{i,\\mathrm{send}}^{k}$ which is sent to Process $\\mathfrak{n e x t}_{s t,j^{*}}(i)$ . Finally, Process $\\mathfrak{n e x t}_{s t,j^{*}}(i)$ receices $g_{i,\\mathrm{send}}^{k}$ and ads ito $g_{(\\mathrm{next}_{s t,j}*(i)),\\mathrm{next}}^{k}$ Thus,te stochasiegradent $\\nabla f(x^{k};\\bar{\\xi})$ presented in the sum of $g_{(\\mathrm{next}_{s t,j}*(i)),\\mathrm{next}}^{k}$ ofProces $\\mathfrak{n e x t}_{s t,j^{*}}(i)$ At some point Pocess in worker $j^{*}$ willreceive a vector $g_{\\cdot,\\mathrm{send}}^{k}$ where the stochastic gradient $\\nabla f(x^{k};\\bar{\\xi})$ is presented. ", "page_idx": 39}, {"type": "text", "text": "Let us bound the time required to transmit a stochastic gradient to Process O of worker $j^{*}$ : Once a stochastic gradient $\\nabla f({\\bar{x}}^{k};{\\bar{\\xi}})$ is calculated in Process $i$ from $A^{*}$ , it is added to a vector $g_{i,\\mathrm{next}}^{k}$ in Process $i$ It willtake at most $2\\rho_{i\\to\\mathrm{next}_{s t,j^{*}}(i)}$ seconds to transmit it to Process $\\mathfrak{n e x t}_{s t,j^{*}}(i)$ because it takes at most $\\rho_{i\\to\\mathrm{next}_{s t,j^{*}}(i)}$ seconds twait for the tranmissonofamessage $g_{i,\\mathrm{send}}^{k}$ where the stochastic gradient $\\nabla f(x^{k};\\bar{\\xi})$ is nt preentd, and an additnal $\\rho_{i\\to\\mathrm{next}_{s t,j^{*}}(i)}$ seconds to send the next $g_{i,\\mathrm{send}}^{k}$ Where it will preent. Aer that, Process $\\mathfrak{n e x t}_{\\overline{{s t}},j^{*}}(i)$ will receive $g_{i,\\mathrm{send}}^{k}$ , where the Sstochastic graient $\\nabla f(x^{k};\\bar{\\xi})$ presets andad the vetor $g_{i,\\mathrm{send}}^{k}$ $g_{(\\mathrm{next}_{s t,j}*(i)),\\mathrm{next}}^{k}$ Thn, it wil l take at most $2\\rho_{\\mathrm{next}_{\\overline{{s}}\\,\\varepsilon}\\,(i)\\to\\mathrm{next}(\\mathrm{next}_{\\overline{{s}}\\,\\varepsilon,j^{*}}\\left(i\\right))}$ seconds to send a vector, where the stochastic gradient $\\nabla f(x^{k};\\bar{\\xi})$ presents, to Process next $\\left(\\mathrm{next}_{\\overline{{s t}},j^{*}}\\left(i\\right)\\right)$ and so forth. In total, after a finite number of such steps a stochastic gradient calculated in Process $i$ will be transmitted to Process O of worker $j^{*}$ . From Definition 3 of $\\mathrm{next}_{s t,j^{*}}$ , we can conclude that the vector $\\nabla f(x^{k};\\bar{\\xi})$ will be transmitted through the path between workers $i$ and $j^{*}$ in the spanning tree $\\overline{{s t}}$ . Thus, it will take at most $2\\mu_{i\\to j^{*}}$ seconds by the definition of \u03bci \u2192j\\* . ", "page_idx": 39}, {"type": "text", "text": "Using (55) and the definition of $A^{*}$ , we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{t}}\\geq2\\operatorname*{max}\\{\\mu_{\\pi_{j^{*},k^{*}}\\to j^{*}}+\\mu_{j^{*}\\to\\pi_{j^{*},k^{*}}},h_{\\pi_{j^{*},k^{*}}}\\}\\geq2\\operatorname*{max}\\{\\mu_{i\\to j^{*}}+\\mu_{j^{*}\\to i},h_{i}\\}\\geq2\\mu_{i\\to j^{*}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "for all $i\\in A^{*}$ . Therefore, it will take at most $\\bar{t}$ seconds to calculate at least $S$ stochastic gradients, and at most $\\bar{t}$ seconds to send all these stochastic gradients to Process 0. ", "page_idx": 39}, {"type": "text", "text": "(Step 3): It is left to estimate the time of the broadcast steps (Lines 7-9 in Algorithm 4) through the spanning tree $\\overline{{s t}}_{\\mathrm{bc}}$ . By the definition of $\\mu_{j}{*}_{\\to}i$ , the time required to broadcast $\\bar{x^{k}}$ to Process $i$ through the spanning tree $\\overline{{s t}}_{\\mathrm{bc}}$ is less or equal to $2\\mu_{j^{*}\\to i}$ since, in all edges from $j^{*}$ to $i$ , workers wait at most $\\rho.\\L\\rightarrow$ . seconds while edges are blocked by previous communications, and additional $\\rho.\\L\\rightarrow$ . seconds to send $x^{k}$ to next workers. Using (55) and the definition of $A^{*}$ , we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{t}\\geq2\\operatorname*{max}\\{\\mu_{\\pi_{j^{*},k^{*}}\\to j^{*}}+\\mu_{j^{*}\\to\\pi_{j^{*},k^{*}}},h_{\\pi_{j^{*},k^{*}}}\\}\\geq2\\operatorname*{max}\\{\\mu_{i\\to j^{*}}+\\mu_{j^{*}\\to i},h_{i}\\}\\geq2\\mu_{j^{*}\\to i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Thus, every worker from $A^{*}$ will get $x^{k}$ after $\\bar{t}$ seconds. By_combining all times, we can conclude that every iteration in Algorithm 3 will take at most $\\bar{t}+\\bar{t}+\\bar{t}=3\\bar{t}$ seconds. ", "page_idx": 39}, {"type": "text", "text": "It left to show that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\bar{t}=\\mathrm{O}\\left(\\operatorname*{max}\\left\\{\\operatorname*{max}\\{\\mu_{\\pi_{j^{*},k^{*}}\\to j^{*}}+\\mu_{j^{*}\\to\\pi_{j^{*},k^{*}}},h_{\\pi_{j^{*},k^{*}}}\\},\\frac{\\sigma^{2}}{\\varepsilon}\\left(\\sum_{i=1}^{k^{*}}\\frac{1}{h_{\\pi_{j^{*},i}}}\\right)^{-1}\\right\\}\\right).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "If $S>1$ , then $S=\\operatorname*{max}\\{\\left\\lceil\\sigma^{2}/\\varepsilon\\right\\rceil,1\\}=\\left\\lceil\\sigma^{2}/\\varepsilon\\right\\rceil\\leq2\\sigma^{2}/\\varepsilon$ , so it is true. Otherwise, if $S\\le1$ , then ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{t}\\leq2\\operatorname*{max}\\left\\{\\operatorname*{max}\\{\\mu_{\\pi_{j^{*},k^{*}}\\to j^{*}}+\\mu_{j^{*}\\to\\pi_{j^{*},k^{*}}},h_{\\pi_{j^{*},k^{*}}}\\},\\left(\\displaystyle\\sum_{i=1}^{k^{*}}\\frac{1}{h_{\\pi_{j^{*},i}}}\\right)^{-1}\\right\\}}\\\\ &{\\ \\ \\leq2\\operatorname*{max}\\left\\{\\operatorname*{max}\\{\\mu_{\\pi_{j^{*},k^{*}}\\to j^{*}}+\\mu_{j^{*}\\to\\pi_{j^{*},k^{*}}},h_{\\pi_{j^{*},k^{*}}}\\},h_{\\pi_{j^{*},k^{*}}}\\right\\}}\\\\ &{=2\\operatorname*{max}\\{\\mu_{\\pi_{j^{*},k^{*}}\\to j^{*}}+\\mu_{j^{*}\\to\\pi_{j^{*},k^{*}}},h_{\\pi_{j^{*},k^{*}}}\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and (7) holds Notice that the rh.s of (57) equals to O0 $\\left(t^{*}(\\sigma^{2}/\\varepsilon,[h_{i}]_{i=1}^{n},[\\mu_{i\\to j^{*}}+\\mu_{j^{*}\\to i}]_{i=1}^{n})\\right),$ where is the equilibrium time defined in Definition 2. ", "page_idx": 40}, {"type": "text", "text": "Theorem 6. Consider the assumptions and the parameters from Theorem 4. In each iteration $k$ of Algorithm 3, the computation times of worker $i$ arebounded by $h_{i}^{k}$ . Let us fix any pivot worker $j^{*}\\in[n]$ and any spanning trees $\\overline{{s t}}$ and $\\dot{\\overline{{s t}}}_{\\mathrm{bc}}$ . Then Algorithm 2 converges after at most ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Theta\\left(\\sum_{k=0}^{\\lceil16L\\Delta/\\varepsilon\\rceil}t^{*}\\big(\\sigma^{2}/\\varepsilon,\\big[h_{i}^{k}\\big]_{i=1}^{n},\\big[\\mu_{i\\to j^{*}}^{k}+\\mu_{j^{*}\\to i}^{k}\\big]_{i=1}^{n}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "seconds, where $\\mu_{i\\to j^{*}}^{k}\\left(\\mu_{j^{*}\\to i}^{k}\\right)$ is an upper bound on times required to send avector from worker ito worker $j^{*}$ (from worker $j^{*}$ to worker $i$ )along the spanning tree $\\overline{{s t}}$ (spanning tree $\\overline{{s t}}_{\\mathrm{bc}}$ ) in iteration $k$ of Algorithm 3. ", "page_idx": 40}, {"type": "text", "text": "Proof. The proof is almost the same as in Theorem 5. If we fix a pivot worker $j^{*}$ , then the $k^{\\mathrm{th}}$ iteration will finish after at most ", "page_idx": 40}, {"type": "equation", "text": "$$\nc\\times t^{*}(\\sigma^{2}/\\varepsilon,[h_{i}^{k}]_{i=1}^{n},[\\mu_{i\\rightarrow j^{*}}^{k}+\\mu_{j^{*}\\rightarrow i}^{k}]_{i=1}^{n})\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "seconds,where $c$ is a universal constant. According to Theorem 4, the number of iterations is at most $\\left\\lceil{\\frac{16L\\Delta}{\\varepsilon}}\\right\\rceil$ Therefore,the total required time is at most ", "page_idx": 40}, {"type": "equation", "text": "$$\nc\\times\\sum_{k=0}^{\\left\\lceil\\frac{16L\\Delta}{\\varepsilon}\\right\\rceil}t^{\\ast}(\\sigma^{2}/\\varepsilon,[h_{i}^{k}]_{i=1}^{n},[\\mu_{i\\rightarrow j^{\\ast}}^{k}+\\mu_{j^{\\ast}\\rightarrow i}^{k}]_{i=1}^{n}).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "H  Proof of the Time Complexity for Heterogeneous Case ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Theorem 8. Let Assumptions 1 and 2 hold for the function $f$ and Assumption 3 holds for the functions $f_{i}$ for all $i\\in[n]$ . We take $\\gamma=1/2L$ , the parameter $S=\\operatorname*{max}\\{\\left\\lceil\\sigma^{2}/\\varepsilon\\right\\rceil,\\dot{n}\\}$ , any pivot worker $j^{*}\\in[n]$ \uff0c and any spanning trees $\\overline{{s t}}$ and $\\overline{{s t}}_{\\mathrm{bc}}$ , in Algorithm 5. For all iterations number $K\\geq16L\\Delta/\\varepsilon$ , we get $\\begin{array}{r}{\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\|\\nabla f(x^{k})\\right\\|^{2}\\right]\\leq\\varepsilon.}\\end{array}$ ", "page_idx": 40}, {"type": "text", "text": "Proof. Algorithm 5 produces the sequence $x^{k}$ such that ", "page_idx": 40}, {"type": "equation", "text": "$$\nx^{k+1}=x^{k}-\\gamma g^{k}=x^{k}-\\gamma\\left({\\frac{1}{n}}\\sum_{i=1}^{n}{\\frac{1}{s_{i}^{k}}}g_{i}^{k}\\right)=x^{k}-\\gamma\\left({\\frac{1}{n}}\\sum_{i=1}^{n}{\\frac{1}{s_{i}^{k}}}\\sum_{j=1}^{s_{i}^{k}}\\nabla f_{i}(x^{k};\\xi_{i j})\\right),\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "wherethe $\\xi_{i j}$ are independent random samples. Using the independence and Assumption 3, we have $\\mathbb{E}\\left[g^{k}\\big|\\,\\mathcal{G}_{k}\\right]=\\nabla f(x^{k})$ and ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|g^{k}-\\nabla f(x^{k})\\right\\|^{2}\\Big|\\mathcal{G}_{k}\\right]}\\\\ &{=\\mathbb{E}\\left[\\left\\|\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\frac{1}{s_{i}^{k}}\\sum_{j=1}^{s_{i}^{k}}\\nabla f_{i}(x^{k};\\xi_{i j})-\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\nabla f_{i}(x^{k})\\right\\|^{2}\\Bigg|\\mathcal{G}_{k}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where $\\mathcal{G}_{k}$ is a $\\sigma$ -algebra generated by $g^{0},\\ldots,g^{k-1}$ .We do not dismiss the possibility that the computation and communication times are random, so $s_{i}^{k}$ can be random. Assume $\\lvert\\nu_{k}$ is a $\\sigma$ -algebra generated by all computation and communication times and $g^{0},\\ldots,g^{k-1}$ , then $s_{i}^{k}$ .s. $\\mathcal{V}_{k}$ -measurable for all $i\\in[n]$ . Using the independence of stochastic gradients and the times and the tower property, weget ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert g^{k}-\\nabla f(x^{k})\\right\\Vert^{2}\\middle|\\mathcal{G}_{k}\\right]}\\\\ &{=\\mathbb{E}\\left[\\mathbb{E}\\left[\\left\\Vert\\frac{n}{n}\\sum_{i=1}^{n}\\frac{1}{s_{i}^{k}}\\nabla f_{i}(x^{k};\\xi_{i j})-\\frac{1}{n}\\sum_{i=1}^{n}\\nabla f_{i}(x^{k})\\right\\Vert^{2}\\middle|\\nu_{k}\\right]\\middle|\\mathcal{G}_{k}\\right]}\\\\ &{=\\frac{1}{n^{2}}\\sum_{i=1}^{n}\\mathbb{E}\\left[\\mathbb{E}\\left[\\left\\Vert\\frac{n^{k}}{s_{i}^{k}\\;\\sum_{j=1}^{k}\\nabla f_{i}(x^{k};\\xi_{i j})-\\nabla f_{i}(x^{k})}\\right\\Vert^{2}\\middle|\\nu_{k}\\right]\\middle|\\mathcal{G}_{k}\\right]}\\\\ &{=\\frac{1}{n^{2}}\\sum_{i=1}^{n}\\mathbb{E}\\left[\\frac{1}{(s_{i}^{k})^{2}}\\sum_{j=1}^{k}\\mathbb{E}\\left[\\left\\Vert\\nabla f_{i}(x^{k};\\xi_{i j})-\\nabla f_{i}(x^{k})\\right\\Vert^{2}\\middle|\\nu_{k}\\right]\\middle|\\mathcal{G}_{k}\\right]\\leq\\frac{\\sigma^{2}}{n^{2}}\\mathbb{E}\\left[\\sum_{i=1}^{n}\\frac{1}{s_{i}^{k}}\\Big|\\mathcal{G}_{k}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Algorithm 6 waits for the moment when $\\begin{array}{r}{s^{k}\\geq\\frac{S}{n}}\\end{array}$ , which is equivalent to ", "page_idx": 41}, {"type": "equation", "text": "$$\nb_{j^{\\ast}}\\leq\\frac{n^{2}}{S}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Thevalue $b_{j^{*}}$ is calculated in Line 20 of Algorithm 7. Due to the asynchronous nature of the algorithm, we can only conclude that ", "page_idx": 41}, {"type": "equation", "text": "$$\nb_{j^{*}}\\geq\\sum_{p\\in[n]:\\mathrm{next}_{s t,j^{*}}(p)=j^{*}}b_{i,p}+\\frac{1}{s_{j^{*}}^{k}}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "because $s_{j^{*}}^{k}$ can be increased by the time when Process O will receive $b_{j^{*}}$ . Using Line 15 from Algorithm? 7, we can unroll the recursion in (62) and get ", "page_idx": 41}, {"type": "equation", "text": "$$\nb_{j^{*}}\\geq\\sum_{i=1}^{n}{\\frac{1}{s_{i}^{k}}}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Let us substitute this inequality to (61) and (60) and get ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|g^{k}-\\nabla f(x^{k})\\right\\|^{2}\\right|\\mathcal{G}_{k}\\right]\\leq\\frac{\\sigma^{2}}{S}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "As in Theorem 4, we can use the classical SGD result. Using Theorem 21, for the stepsize ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\gamma=\\frac{1}{2L}\\operatorname*{min}\\left\\{1,\\frac{\\varepsilon S}{\\sigma^{2}}\\right\\},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\|\\nabla f(x^{k})\\right\\|^{2}\\right]\\leq\\varepsilon,\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "if ", "page_idx": 41}, {"type": "equation", "text": "$$\nK\\ge\\frac{8\\Delta L}{\\varepsilon}+\\frac{8\\Delta L\\sigma^{2}}{\\varepsilon^{2}S}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Using the choice of $S$ , we get that Algorithm 5 converges after ", "page_idx": 41}, {"type": "equation", "text": "$$\nK\\geq\\frac{16\\Delta L}{\\varepsilon}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "steps with ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\gamma={\\frac{1}{2L}}\\operatorname*{min}\\left\\{1,{\\frac{\\varepsilon S}{\\sigma^{2}}}\\right\\}={\\frac{1}{2L}}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Theorem 9. Consider the assumptions and the parameters from Theorem 8. For any pivot worker $j^{*}\\in[n]$ andanyspanningtrees $\\overline{{s t}}$ and $\\overline{{s t}}_{\\mathrm{bc}}$ ,Algorithm $^{5}$ convergesafteratmost ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\Theta\\left(\\frac{L\\Delta}{\\varepsilon}\\operatorname*{max}\\left\\{\\operatorname*{max}_{i,j\\in[n]}\\mu_{i\\to j},\\operatorname*{max}_{i\\in[n]}h_{i},\\frac{\\sigma^{2}}{n\\varepsilon}\\left(\\frac{1}{n}\\sum_{i=1}^{n}h_{i}\\right)\\right\\}\\right)\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "seconds, where $\\mu_{i\\to j^{*}}^{k}\\left(\\mu_{j^{*}\\to i}^{k}\\right)$ is an upperbound ontimesrequiredtosendavectorfromworke ito worker $j^{*}$ (from worker $j^{*}$ to worker i) along the spanning tree $\\overline{{s t}}$ (spanning tree $\\overline{{s t}}_{\\mathrm{bc}}$ for all $i\\in[n]$ ", "page_idx": 42}, {"type": "text", "text": "Proof. Due to Theorem 8, the algorithm converges after $K=\\Theta\\left(L\\Delta/\\varepsilon\\right)$ iterations. Thus, it left to bound the time of one iteration. At the beginning of every iteration Process O broadcasts $x^{k}$ , which takes at most $\\operatorname*{max}_{i,j\\in[n]}\\mu_{i\\to j}$ seconds. Then, Algorithm 6 waits for the moment when $s^{k}\\geq{\\frac{S}{n}}$ \uff0c Which is equivalent to $\\begin{array}{r}{\\frac{n^{2}}{S}\\geq b_{j^{*}}}\\end{array}$ . Thus, Algorithm 6 waits for the moment when $\\begin{array}{r}{\\frac{n^{2}}{S}\\geq b_{j^{*}}}\\end{array}$ .We will return to this fact later. ", "page_idx": 42}, {"type": "text", "text": "Let us consider the term ", "page_idx": 42}, {"type": "equation", "text": "$$\n{\\frac{S}{n^{2}}}\\sum_{i=1}^{n}{\\frac{1}{s_{i}^{k}}},\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $s_{i}^{k}$ is the number of stochastic gradients calculated in worker $i$ . Let us fix any time $\\bar{t}>0$ . Then, worker $i$ will calculate at least $\\left\\lfloor\\frac{\\bar{t}}{h_{i}}\\right\\rfloor$ stochastic gradients by the time $\\bar{t}$ . Using this, we get ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\frac{S}{n^{2}}\\sum_{i=1}^{n}\\frac{1}{s_{i}^{k}}\\leq\\frac{S}{n^{2}}\\sum_{i=1}^{n}\\frac{1}{\\left\\lfloor\\frac{\\bar{t}}{h_{i}}\\right\\rfloor}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Let us take ", "page_idx": 42}, {"type": "equation", "text": "$$\n{\\bar{t}}=2\\left(\\operatorname*{max}_{i\\in[n]}h_{i}+{\\frac{S}{n}}\\left({\\frac{1}{n}}\\sum_{i=1}^{n}h_{i}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Then, since $\\lfloor x\\rfloor\\geq{\\frac{x}{2}}$ fo ll $x\\geq1$ , we get $\\begin{array}{r}{\\left\\lfloor\\frac{\\bar{t}}{h_{i}}\\right\\rfloor\\ge\\frac{\\bar{t}}{2h_{i}}}\\end{array}$ and ", "page_idx": 42}, {"type": "equation", "text": "$$\n{\\frac{S}{n^{2}}}\\sum_{i=1}^{n}{\\frac{1}{s_{i}^{k}}}\\leq{\\frac{2S}{n\\bar{t}}}\\left({\\frac{1}{n}}\\sum_{i=1}^{n}h_{i}\\right).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Using $\\begin{array}{r}{\\bar{t}\\geq\\frac{2S}{n}\\left(\\frac{1}{n}\\sum_{i=1}^{n}h_{i}\\right)}\\end{array}$ , we get ", "page_idx": 42}, {"type": "equation", "text": "$$\n{\\frac{S}{n^{2}}}\\sum_{i=1}^{n}{\\frac{1}{s_{i}^{k}}}\\leq1\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "and ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}{\\frac{1}{s_{i}^{k}}}\\leq{\\frac{n^{2}}{S}}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "after at most $\\bar{t}$ seconds. ", "page_idx": 42}, {"type": "text", "text": "Recall that Algorithm 6 waits for the moment when $\\begin{array}{r}{\\frac{n^{2}}{S}\\geq b_{j^{*}}}\\end{array}$ Note that by the tme whenProce Oreceives $b_{j^{*}}$ , the counter $s_{i}^{k}$ can be the same or increased; thus, $b_{j^{*}}$ captures potentially outdated information about $s_{i}^{k}$ . We know that (64) holds after at most $\\bar{t}$ seconds. In Line 20 of Algorithm 7, Processes recursively cllet $\\frac{1}{s_{i}^{k}}$ 10 $b_{j^{*}}$ Such procedure il take at most $\\begin{array}{r}{2\\operatorname*{max}_{i\\in[n]}\\mu_{i\\to j^{*}}\\leq2\\operatorname*{max}_{i,j\\in[n]}\\mu_{i\\to j}}\\end{array}$ seconds. Thus, the value $b_{j^{*}}$ will be less or equal $\\frac{n^{2}}{S}$ after at most $\\bar{t}+2\\operatorname*{max}_{i,j\\in[n]}\\mu_{i\\to j}$ seconds. ", "page_idx": 42}, {"type": "text", "text": "The all reduce operation in (8) will take at most $\\operatorname*{max}_{i,j\\in[n]}\\mu_{i\\to j}$ seconds. Thus, the total time of one iteration can be bounded by ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underbrace{\\underset{i,j\\in[n]}{\\operatorname*{max}}\\mu_{i\\to j}}_{\\mathrm{broadcast}}+(\\bar{t}+2\\underbrace{\\underset{i,j\\in[n]}{\\operatorname*{max}}\\mu_{i\\to j}}_{i,j\\in[n]}+\\underset{i\\in\\mathcal{E}}{\\operatorname*{max}}\\ \\mu_{i\\to j}}\\\\ &{=\\mathrm{O}\\left(\\underset{i,j\\in[n]}{\\operatorname*{max}}\\mu_{i\\to j}+\\underset{i\\in[n]}{\\operatorname*{max}}h_{i}+\\frac{S}{n}\\left(\\frac{1}{n}\\sum_{i=1}^{n}h_{i}\\right)\\right)}\\\\ &{=\\mathrm{O}\\left(\\underset{i,j\\in[n]}{\\operatorname*{max}}\\ \\mu_{i\\to j}+\\underset{i\\in[n]}{\\operatorname*{max}}h_{i}+\\frac{\\sigma^{2}}{n\\varepsilon}\\left(\\frac{1}{n}\\sum_{i=1}^{n}h_{i}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "seconds. ", "page_idx": 43}, {"type": "text", "text": "Classical SGD Theory ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "We reprove the classical SGD result (Ghadimi and Lan, 2013; Khaled and Richtarik, 2022), for completeness. ", "page_idx": 43}, {"type": "text", "text": "Theorem 21. Let Assumptions 1 and 2 hold. We consider the SGD method: ", "page_idx": 43}, {"type": "equation", "text": "$$\nx^{k+1}=x^{k}-\\gamma g(x^{k}),\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\gamma=\\frac{1}{2L}\\operatorname*{min}\\left\\{1,\\frac{\\varepsilon}{\\sigma^{2}}\\right\\}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Forall $k\\geq0$ ,thevector $g(x)$ is a random vector such that $\\mathbb{E}\\left[\\left.g(x^{k})\\right|\\mathcal{G}_{k}\\right]=\\nabla f(x^{k}),$ ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|g(x^{k})-\\nabla f(x^{k})\\right\\|^{2}\\right|\\mathcal{G}_{k}\\right]\\le\\sigma^{2},\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where $\\mathcal{G}_{k}$ is a $\\sigma$ -algebra generated by $g(x^{0}),\\ldots,g(x^{k-1})$ .Then ", "page_idx": 43}, {"type": "equation", "text": "$$\n{\\frac{1}{K}}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\|\\nabla f(x^{k})\\right\\|^{2}\\right]\\leq\\varepsilon\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "for ", "page_idx": 43}, {"type": "equation", "text": "$$\nK\\ge\\frac{8\\Delta L}{\\varepsilon}+\\frac{8\\Delta L\\sigma^{2}}{\\varepsilon^{2}}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Proof. From Assumption 1, we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{f(x^{k+1})\\leq f(x^{k})+\\left\\langle\\nabla f(x^{k}),x^{k+1}-x^{k}\\right\\rangle+\\displaystyle\\frac{L}{2}\\left\\|x^{k+1}-x^{k}\\right\\|^{2}}}\\\\ {{\\displaystyle=f(x^{k})-\\gamma\\left\\langle\\nabla f(x^{k}),g(x^{k})\\right\\rangle+\\displaystyle\\frac{L\\gamma^{2}}{2}\\left\\|g(x^{k})\\right\\|^{2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "We denote $\\mathcal{G}^{k}$ as a sigma-algebra generated by $g(x^{0}),\\ldots,g(x^{k-1})$ Using unbiasedness and (65), we obtain ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[f(x^{k+1})\\big|\\,\\mathcal{G}^{k}\\right]\\leq f(x^{k})-\\gamma\\left(1-\\displaystyle\\frac{L\\gamma}{2}\\right)\\left\\Vert\\nabla f(x^{k})\\right\\Vert^{2}+\\displaystyle\\frac{L\\gamma^{2}}{2}\\mathbb{E}\\left[\\left\\Vert g(x^{k})-\\nabla f(x^{k})\\right\\Vert^{2}\\right]\\mathcal{G}^{k}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq f(x^{k})-\\gamma\\left(1-\\displaystyle\\frac{L\\gamma}{2}\\right)\\left\\Vert\\nabla f(x^{k})\\right\\Vert^{2}+\\displaystyle\\frac{L\\gamma^{2}\\sigma^{2}}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Since $\\gamma\\leq^{1}\\!/L$ , we get ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f(x^{k+1})\\big|\\,\\mathcal{G}^{k}\\right]\\leq f(x^{k})-\\frac{\\gamma}{2}\\left\\|\\nabla f(x^{k})\\right\\|^{2}+\\frac{L\\gamma^{2}\\sigma^{2}}{2}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "We subtract $f^{*}$ and take the full expectation to obtain ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f(x^{k+1})-f^{*}\\right]\\leq\\mathbb{E}\\left[f(x^{k})-f^{*}\\right]-\\frac{\\gamma}{2}\\mathbb{E}\\left[\\left\\Vert\\nabla f(x^{k})\\right\\Vert^{2}\\right]+\\frac{L\\gamma^{2}\\sigma^{2}}{2}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Next, we sum the inequality for $k\\in\\{0,\\ldots,K-1\\}$ ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[f(x^{K})-f^{*}\\right]\\leq f(x^{0})-f^{*}-\\displaystyle\\sum_{k=0}^{K-1}\\frac{\\gamma}{2}\\mathbb{E}\\left[\\left\\Vert\\nabla f(x^{k})\\right\\Vert^{2}\\right]+\\frac{K L\\gamma^{2}\\sigma^{2}}{2}}\\\\ &{=\\Delta-\\displaystyle\\sum_{k=0}^{K-1}\\frac{\\gamma}{2}\\mathbb{E}\\left[\\left\\Vert\\nabla f(x^{k})\\right\\Vert^{2}\\right]+\\frac{K L\\gamma^{2}\\sigma^{2}}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Finally, we rearrange the terms and use that $\\mathbb{E}\\left[f(x^{K})-f^{*}\\right]\\geq0$ ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\|\\nabla f(x^{k})\\right\\|^{2}\\right]\\leq\\frac{2\\Delta}{\\gamma K}+L\\gamma\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "The choice of $\\gamma$ and $K$ ensures that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\sum_{k=0}^{K-1}\\mathbb{E}\\left[\\left\\|\\nabla f(x^{k})\\right\\|^{2}\\right]\\leq\\varepsilon.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Theorem 22. Let Assumptions $^{4}$ and 5 hold. We consider the SGD method: ", "page_idx": 44}, {"type": "equation", "text": "$$\nx^{k+1}=x^{k}-\\gamma g(x^{k}),\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\gamma=\\frac{\\varepsilon}{M^{2}+\\sigma^{2}}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "For all $k\\geq0$ , the vector $g(x)$ is a random vector such that $\\mathbb{E}\\left[\\left.g(x^{k})\\right|\\mathcal{G}_{k}\\right]\\in\\partial f(x^{k})$ ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left\\|g(x^{k})-\\mathbb{E}\\left[g(x^{k})\\big|\\,\\mathcal{G}_{k}\\right]\\right\\|^{2}\\right|\\mathcal{G}_{k}\\right]\\le\\sigma^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where $\\mathcal{G}_{k}$ is a $\\sigma$ -algebra generated by $g(x^{0}),\\ldots,g(x^{k-1})$ .Then ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f\\left(\\frac{1}{K}\\sum_{k=0}^{K-1}x^{k}\\right)\\right]-f(x^{*})\\leq\\varepsilon\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "for ", "page_idx": 44}, {"type": "equation", "text": "$$\nK\\geq{\\frac{\\left(M^{2}+\\sigma^{2}\\right)\\left\\|x^{*}-x^{0}\\right\\|^{2}}{\\varepsilon^{2}}}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Proof. We denote $\\mathcal{G}^{k}$ as a sigma-algebra generated by $g(x^{0}),\\ldots,g(x^{k-1})$ Using the convexity, for all $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , we have ", "page_idx": 44}, {"type": "equation", "text": "$$\nf({\\boldsymbol{x}})\\geq f({\\boldsymbol{x}}^{k})+\\left\\langle\\mathbb{E}\\left[\\left.g({\\boldsymbol{x}}^{k})\\right|{\\mathcal{G}}^{k}\\right],{\\boldsymbol{x}}-{\\boldsymbol{x}}^{k}\\right\\rangle=f({\\boldsymbol{x}}^{k})+\\mathbb{E}\\left[\\left.\\left\\langle g({\\boldsymbol{x}}^{k}),{\\boldsymbol{x}}-{\\boldsymbol{x}}^{k}\\right\\rangle\\right|{\\mathcal{G}}^{k}\\right].\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Note that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\left\\langle g(x^{k}),x-x^{k}\\right\\rangle=\\left\\langle g(x^{k}),x^{k+1}-x^{k}\\right\\rangle+\\left\\langle g(x^{k}),x-x^{k+1}\\right\\rangle\\ }\\\\ {\\ =-\\gamma\\left\\|g(x^{k})\\right\\|^{2}+\\displaystyle\\frac{1}{\\gamma}\\left\\langle x^{k}-x^{k+1},x-x^{k+1}\\right\\rangle\\ }\\\\ {\\ =-\\gamma\\left\\|g(x^{k})\\right\\|^{2}+\\displaystyle\\frac{1}{2\\gamma}\\left\\|x^{k}-x^{k+1}\\right\\|^{2}+\\displaystyle\\frac{1}{2\\gamma}\\left\\|x-x^{k+1}\\right\\|^{2}-\\displaystyle\\frac{1}{2\\gamma}\\left\\|x-x^{k}\\right\\|^{2}}\\\\ {\\ =-\\displaystyle\\frac{\\gamma}{2}\\left\\|g(x^{k})\\right\\|^{2}+\\displaystyle\\frac{1}{2\\gamma}\\left\\|x-x^{k+1}\\right\\|^{2}-\\displaystyle\\frac{1}{2\\gamma}\\left\\|x-x^{k}\\right\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "and ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left\\|g(x^{k})\\right\\|^{2}\\middle|\\mathcal{G}^{k}\\right]=\\mathbb{E}\\left[\\left\\|g(x^{k})-\\mathbb{E}\\left[g(x^{k})\\big|\\mathcal{G}^{k}\\right]\\right\\|^{2}\\middle|\\mathcal{G}^{k}\\right]+\\left\\|\\mathbb{E}\\left[g(x^{k})\\big|\\mathcal{G}^{k}\\right]\\right\\|^{2}\\leq\\sigma^{2}+M^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Therefore, we get ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(x^{k})\\leq f(x)+\\ensuremath{{\\mathbb{E}}}\\left[\\left.\\left\\langle g(x^{k}),x^{k}-x\\right\\rangle\\right|\\mathcal{G}^{k}\\right]}\\\\ &{\\qquad=f(x)+\\frac{\\gamma}{2}\\ensuremath{{\\mathbb{E}}}\\left[\\left.\\left\\|g(x^{k})\\right\\|^{2}\\right|\\mathcal{G}^{k}\\right]+\\frac{1}{2\\gamma}\\left\\|x-x^{k}\\right\\|^{2}-\\frac{1}{2\\gamma}\\ensuremath{{\\mathbb{E}}}\\left[\\left\\|x-x^{k+1}\\right\\|^{2}\\right|\\mathcal{G}^{k}\\right]}\\\\ &{\\qquad\\leq f(x)+\\frac{\\gamma}{2}\\left(M^{2}+\\sigma^{2}\\right)+\\frac{1}{2\\gamma}\\left\\|x-x^{k}\\right\\|^{2}-\\frac{1}{2\\gamma}\\ensuremath{{\\mathbb{E}}}\\left[\\left\\|x-x^{k+1}\\right\\|^{2}\\right|\\mathcal{G}^{k}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "By taking the full expectation and summing the last inequality for $t$ from 0 to $K-1$ ,we obtain ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\displaystyle\\left[\\sum_{k=0}^{K-1}f(x^{k})\\right]\\leq K f(x)+\\displaystyle\\frac{K\\gamma}{2}\\left(M^{2}+\\sigma^{2}\\right)+\\displaystyle\\frac{1}{2\\gamma}\\left\\|x-x^{0}\\right\\|^{2}-\\displaystyle\\frac{1}{2\\gamma}\\mathbb{E}\\left[\\left\\|x-x^{K}\\right\\|^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq K f(x)+\\displaystyle\\frac{K\\gamma}{2}\\left(M^{2}+\\sigma^{2}\\right)+\\displaystyle\\frac{1}{2\\gamma}\\left\\|x-x^{0}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Let divide the last inequality by $K$ ,take $x=x^{*}$ , and use the convexity: ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f\\left(\\frac{1}{K}\\sum_{k=0}^{K-1}x^{k}\\right)\\right]-f(x^{*})\\leq\\frac{\\gamma}{2}\\left(M^{2}+\\sigma^{2}\\right)+\\frac{1}{2\\gamma K}\\left\\|x^{*}-x^{0}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "The choices of $\\gamma$ and $K$ ensure that (66) holds. ", "page_idx": 45}, {"type": "text", "text": "Experiments ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "We now consider Fragile SGD with Minibatch SGD on quadratic optimization tasks with stochastic gradients. The working environment was emulated in Python 3.8 with one Intel(R) Xeon(R) Gold 6248CPU $\\textcircled{a}\\ 2.50\\mathrm{GHz}$ . The homogeneous optimization problem (1) is constructed in the following way.Wetake ", "page_idx": 45}, {"type": "equation", "text": "$$\nf(x)={\\frac{1}{2}}x^{\\top}\\mathbf{A}x-b^{\\top}x\\quad\\forall x\\in\\mathbb{R}^{d},\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "$d=1000$ ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathbf{A}=\\frac{1}{4}\\left(\\begin{array}{l l l l}{\\ 2}&{-1}&&{0}\\\\ {-1}&{\\ddots}&{\\ddots}&\\\\ &{\\ddots}&{\\ddots}&\\\\ &&{-1}&{2}\\end{array}\\right)\\in\\mathbb{R}^{d\\times d},\\quad\\mathrm{~and~}\\quad b=\\frac{1}{4}\\left[\\begin{array}{l}{-1}\\\\ {0}\\\\ {\\vdots}\\\\ {0}\\end{array}\\right]\\in\\mathbb{R}^{d}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Let us define $[x]_{j}$ as the $j^{\\mathrm{th}}$ index of a vector $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ . A1l $n$ workers calculate the stochastic gradients ", "page_idx": 45}, {"type": "equation", "text": "$$\n[\\nabla f(x;\\xi)]_{j}:=[\\nabla f(x)]_{j}\\left(1+\\mathbb{1}\\left[j>\\mathrm{prog}(x)\\right]\\left(\\frac{\\xi}{p}-1\\right)\\right)\\quad\\forall x\\in\\mathbb{R}^{d},\\forall i\\in[n],\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where $\\xi\\sim\\mathrm{Bernouilli}(p)$ $p\\in(0,1]$ . In our experiments, we take $p=0.001$ and the starting point $\\boldsymbol{x}^{0}\\,=\\,[\\sqrt{d},0,\\cdot\\,.\\,.\\,,0]^{\\intercal}$ .We emulate our setup by considering that the $i^{\\mathrm{th}}$ worker requires $h_{i}=1$ second to calculate a stochastic gradient. And we assume that the workers have the structure of 2DMesh (see Figure 4a) and take $\\rho_{i\\rightarrow j}=\\rho\\in\\{0.1,1,10\\}$ seconds for all edges that connect workers in 2D-Mesh. We take $n=100$ In all methods we fine-tune step sizes from the set $\\{2^{i}\\mid i\\in[-20,20]\\}$ In Fragile SGD, we fine-tune the batch size $S$ from the set $\\{10,20,40,80,120\\}$ ", "page_idx": 45}, {"type": "text", "text": "The results are presented in Figures 5, 6, and 7. The plots are fully consisted with Table 1. One can see that when the communication is fast (Fig. 5), there is no big difference between the methods because both Fragile SGD and Minibatch SGD use all workers in the optimization steps. However, when we start decreasing the communication speed, we observe that Fragile SGD converges faster. ", "page_idx": 45}, {"type": "text", "text": "We looked deeper into the optimization processes of Fragile SGD in Figure 7 and observed that only 13 of 100 workers contribute to the optimization process for the batch size $S=120$ .Otherworkers are too far away from the pivot worker, and their contributions can only slow down optimization. ", "page_idx": 46}, {"type": "image", "img_path": "IXRa8adMHX/tmp/a12c91a5b28a90e7c09302db77f763f1130e63d0150eee974879cf416aa1298c.jpg", "img_caption": ["Figure 5: The communication time $\\rho=0.1$ seconds (Fast communication) "], "img_footnote": [], "page_idx": 46}, {"type": "image", "img_path": "IXRa8adMHX/tmp/155941f19416ca834c2604778a2998584cbec6eaff8f6b939f307ae0929908d8.jpg", "img_caption": ["Figure 6: The communication time $\\rho=1$ seconds (Medium speed communication) "], "img_footnote": [], "page_idx": 46}, {"type": "image", "img_path": "IXRa8adMHX/tmp/6e5ec6fd3a4180e7d7eb656a37aacce3be1a374b94e330621ff5f04a913431ad.jpg", "img_caption": ["Figure 7: The communication time $\\rho=10$ seconds (Slow communication) "], "img_footnote": [], "page_idx": 46}, {"type": "text", "text": "J.1  Experiments with Logistic Regression: Fast vs Slow Communication ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "We now repeat the previous experiments but with logistic regression on MNIST dataset (LeCun et al., 2010) with 100 workers. We consider two regimes: fast and slow communication between workers. One can see that when the communication is fast, the gap between the methods is small, which is expected and compliant with the theory. However, Fragile SGD is much faster and has better test accuracy when the communication is slow. ", "page_idx": 47}, {"type": "image", "img_path": "IXRa8adMHX/tmp/124088f9519905106f634bfc588dd851b5fd82f73ff71dff1fb5f4c06e851396.jpg", "img_caption": ["Figure 8: The communication time $\\rho=0.1$ seconds (Fast communication) "], "img_footnote": [], "page_idx": 47}, {"type": "image", "img_path": "IXRa8adMHX/tmp/a00bf19ed9e8ca9e39ddf37c1a517f86d2f6f65ba457df8330d0336a86501a29.jpg", "img_caption": ["Figure 9: The communication time $\\rho=10$ seconds (Slow communication) "], "img_footnote": [], "page_idx": 47}, {"type": "text", "text": "J.2  Experiments with ResNet-18 ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "We test algorithms on an image recognition task, CIFAR10 (Krizhevsky et al., 2009), with the ResNet18 (He et al., 2016) deep neural network (the number of parameters $d\\approx10^{7}$ ).We use the torus structure and 9 workers. We run all methods with the step sizes $\\{0.025,0.25,2.5\\}$ . Our findings from the low-scale experiments are also evident in the large-scale experiments. Fragile SGD converges faster than Minibatch SGD in terms of function values. When we compare accuracies on the test split of MNIST, the superiority of Fragile SGD is even more transparent. ", "page_idx": 47}, {"type": "image", "img_path": "IXRa8adMHX/tmp/19ad0586f01ccc79809dbbf36d6dd0e0fd1a2e76fa98bac5192b40ea6a697c75.jpg", "img_caption": ["Figure 10: ResNet-18 on CIFAR10 dataset with 9 workers and the torus structure with the communicationtime $\\rho=1$ seconds (Medium communication) "], "img_footnote": [], "page_idx": 47}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": ". Claims ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: Table 1, the main part of the paper, Section C ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 48}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Justification: Section 5.3 ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 48}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: Section1.1, and the appendix ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 49}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: Section J Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. () If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 49}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: The code in the supplementary materials. Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 50}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: Section J ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 50}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: We provide the top 3 best plots for each algorithm to reduce randomness factors in Section J. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 50}, {"type": "text", "text": "", "page_idx": 51}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: Section J ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 51}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: We have read the code of ethics, and our paper does not violate it. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 51}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Justification: We consider a mathematical problem for machine learning. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 51}, {"type": "text", "text": "", "page_idx": 52}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Justification: We consider a mathematical problem for machine learning. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 52}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Justification: ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u00b7 Ine answer NA means tnat tne paper aoes not use exisuing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 52}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 53}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: The code and documentation in the supplementary materials. Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 53}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 53}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 53}, {"type": "text", "text": "", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 53}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 53}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 53}, {"type": "text", "text": "", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 53}]