{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-06-12", "reason": "This paper introduces the Transformer architecture, which is the foundation for many modern deep learning models, including those used in this paper for video understanding."}, {"fullname_first_author": "Gedas Bertasius", "paper_title": "Is space-time attention all you need for video understanding?", "publication_date": "2021-07-12", "reason": "This paper explores the use of space-time attention in video transformers, which is directly relevant to the problem of long-form video understanding addressed in this paper."}, {"fullname_first_author": "Daniel Bolya", "paper_title": "Token Merging: Your ViT but faster", "publication_date": "2022-07-12", "reason": "This paper introduces the token merging technique which is directly adopted and extended in this work for long-form video understanding."}, {"fullname_first_author": "Chao-Yuan Wu", "paper_title": "Towards long-form video understanding", "publication_date": "2021-06-12", "reason": "This paper is focused on long-form video understanding, which is the main topic of this paper, and provides a benchmark dataset (LVU) used for evaluation."}, {"fullname_first_author": "Jue Wang", "paper_title": "Long-short temporal contrastive learning of video transformers", "publication_date": "2022-06-12", "reason": "This paper proposes a method for learning video representations using contrastive learning, which is a technique also used in this paper to improve the efficiency and effectiveness of the video understanding model."}]}