[{"figure_path": "wduRaBDRBS/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of GPU memory footprint and throughput against scene prediction accuracy on the LVU dataset (Wu & Kr\u00e4henb\u00fchl, 2021).", "description": "This figure compares the performance of different video understanding models (Ours, Baseline, S4, ViS4mer, Orthoformer, Performer) on the LVU dataset in terms of GPU memory usage and throughput (samples per second).  The x-axis represents either memory (GB) or throughput (#samples/sec) while the y-axis shows the prediction accuracy.  The results demonstrate that the proposed 'Ours' method achieves higher accuracy with significantly reduced memory usage and increased throughput compared to the other methods.", "section": "1 Introduction"}, {"figure_path": "wduRaBDRBS/figures/figures_4_1.jpg", "caption": "Figure 2: The architectures of (a) the baseline network, (b) the transformer block, and (c) the video token merging block.", "description": "This figure illustrates three different network architectures. (a) shows a baseline network for video processing using transformers.  The input video is encoded into tokens, which then pass through multiple transformer blocks before a prediction head produces the final output. (b) zooms in on a single transformer block, detailing its internal components, such as layer normalization (LN), attention, dropout, a linear layer, and a GELU activation function. (c) shows a modified transformer block that incorporates a Video Token Merging (VTM) layer, which is the core contribution of the paper.  This layer is added to increase efficiency by reducing the number of tokens before the final linear layer and GELU activation.", "section": "3.3 Video Token Merging \u2013 Exploration"}, {"figure_path": "wduRaBDRBS/figures/figures_5_1.jpg", "caption": "Figure 3: Visualizations of target tokens of different VTM methods: (a) na\u00efve VTM, (b) center-concentrated VTM, (c) motion-based VTM, and (d) learnable VTM. In (d), learnable VTM selects the target tokens around salient objects rather than backgrounds.", "description": "This figure visualizes how different video token merging (VTM) methods select target tokens for merging.  (a) shows the na\u00efve VTM, which selects tokens uniformly. (b) illustrates the center-concentrated VTM, which focuses on the central area of the video. (c) presents the motion-based VTM, which emphasizes tokens with significant motion. Finally, (d) displays the learnable VTM, which intelligently selects tokens based on saliency, prioritizing important regions over less significant backgrounds.", "section": "3.3 Video Token Merging \u2013 Exploration"}, {"figure_path": "wduRaBDRBS/figures/figures_6_1.jpg", "caption": "Figure 4: An overview of the learnable video token merging block. The auxiliary path is used during training only.", "description": "This figure shows the architecture of the learnable video token merging (VTM) block.  It consists of two main paths: a main path and an auxiliary path. The main path performs standard self-attention on the input tokens and then estimates saliency scores for each token. These scores are used to partition the tokens into target and source sets. The source tokens are then matched to the most similar target tokens and merged using average pooling. The auxiliary path is used only during training. It performs saliency-guided attention on the auxiliary tokens, merging them in a similar way to the main path. The outputs of both paths are then added together to produce the final output of the VTM block.", "section": "3.4 Learnable Video Token Merging"}, {"figure_path": "wduRaBDRBS/figures/figures_8_1.jpg", "caption": "Figure 7: Visualizations of video token merging results on the LVU dataset. Patches with same inner and border color are merged together.", "description": "This figure visualizes the results of video token merging on the LVU dataset using the proposed learnable VTM.  The top row shows example video frames from different video clips, and the bottom row displays the corresponding token merging results. Patches (tokens) with the same inner and border colors have been merged together by the algorithm.  The visualization illustrates how the algorithm groups similar visual tokens, particularly merging background or less important visual information while preserving salient and important details.", "section": "C More Visualizations"}, {"figure_path": "wduRaBDRBS/figures/figures_13_1.jpg", "caption": "Figure 6: A network architecture of the proposed learnable VTM.", "description": "This figure illustrates the detailed architecture of the proposed learnable video token merging (VTM) method. The encoder first extracts video tokens with a channel dimension of 1024.  Each VTM block reduces the channel dimension by half, resulting in a final dimension of 256 after three blocks. The figure shows the main path, responsible for standard self-attention and token merging, and the auxiliary path, used only during training to refine saliency estimation. The auxiliary path incorporates a saliency-guided attention mechanism, helping the network to assign higher saliency scores to meaningful tokens. Both paths contribute to the updated tokens, which are then used for classification. The average pooling is used before the prediction head.", "section": "A More Implementation Detail"}, {"figure_path": "wduRaBDRBS/figures/figures_14_1.jpg", "caption": "Figure 7: Visualizations of video token merging results on the LVU dataset. Patches with same inner and border color are merged together.", "description": "This figure shows three examples of video token merging on the LVU dataset using the proposed learnable VTM. Each row represents a different video clip. The top row displays frames from the video clip. The bottom row visualizes the merging process, where patches (tokens) with the same inner and border colors are merged together. This illustrates how the algorithm groups together semantically similar visual information into merged tokens.", "section": "C More Visualizations"}]