[{"figure_path": "Ur9f4hNIpN/figures/figures_2_1.jpg", "caption": "Figure 1: Illustration of several advanced numerical methods and our proposed predictor-corrector paradigm. The right part plots a 4-order method as the predictor to obtain P<sub>t+1</sub>; F<sub>t+1</sub> is then estimated via a function F(\u00b7); A 4-step method as the corrector to obtain the Y<sub>t+1</sub>.", "description": "This figure illustrates different numerical methods for solving ordinary differential equations (ODEs), which are analogous to the residual connections in neural networks.  Panel (a) shows predictor-only paradigms: 1st-order 1-step (Euler method), 1st-order multi-step, and high-order 1-step methods. Panel (b) illustrates the proposed predictor-corrector paradigm, using a high-order predictor with exponential moving average (EMA) coefficient learning, followed by a multi-step corrector. This approach aims to improve accuracy by iteratively refining the solution.", "section": "3 Predictor-Corrector Transformer"}, {"figure_path": "Ur9f4hNIpN/figures/figures_4_1.jpg", "caption": "Figure 2: Truncation errors with different intermediate approximations.", "description": "This figure compares the perplexity (a measure of how well a model predicts text) achieved using different approximation methods for the 4th-order approximation within the predictor-corrector framework. Lower perplexity indicates fewer truncation errors, and therefore, a more accurate solution.  The results show that the 4th-order approximation performs comparably well and outperforms other methods such as a vanilla approach, Runge-Kutta (RK4), and lower-order approximations (1st, 2nd, 3rd).  This supports the paper's claim that high-order predictors improve accuracy.", "section": "3.1.2 High-order Predictor and Multistep Corrector"}, {"figure_path": "Ur9f4hNIpN/figures/figures_16_1.jpg", "caption": "Figure 3: The comparison of BLEU as well as model capacities and training costs against previous state-of-the-art deep transformers.", "description": "This figure compares the BLEU scores achieved by various Transformer models, including the vanilla Transformer, Evolved Transformer, DeLight, and different variants of the PCformer model (with gated fusion, EMA, and predictor-corrector approaches). It highlights the model parameter size and the training cost (in terms of steps) required to achieve these scores.  The results illustrate the efficiency and improved performance of the PCformer models compared to the baselines.", "section": "More Analyses"}, {"figure_path": "Ur9f4hNIpN/figures/figures_17_1.jpg", "caption": "Figure 1: Illustration of several advanced numerical methods and our proposed predictor-corrector paradigm. The right part plots a 4-order method as the predictor to obtain Pt+1; Ft+1 is then estimated via a function F(\u00b7); A 4-step method as the corrector to obtain the Yt+1.", "description": "This figure illustrates different numerical methods for solving ordinary differential equations (ODEs), which are analogous to the layer-wise computations in neural networks.  It compares three predictor-only approaches (Euler, multi-step, and high-order methods) with the proposed predictor-corrector method. The predictor-corrector approach uses a high-order method for prediction and a multi-step method for correction, improving accuracy. The figure highlights the key components and flow of the predictor-corrector framework.", "section": "3 Predictor-Corrector Transformer"}, {"figure_path": "Ur9f4hNIpN/figures/figures_18_1.jpg", "caption": "Figure 5: The coefficient learning curves of independent initialization and EMA in both 2-order and 4-order scenarios. The experiments are conducted on WMT En-De.", "description": "This figure shows the learning curves of the learnable coefficients (\u03b31, \u03b32, \u03b33, \u03b34) in both 2-order and 4-order Runge-Kutta methods with two different coefficient learning strategies: independent initialization and exponential moving average (EMA).  The independent initialization strategy allows the coefficients to learn independently, while the EMA strategy assigns exponentially decaying weights to previous approximations, giving more importance to recent data. The x-axis represents the training epoch, and the y-axis represents the value of the coefficients. The results show that the EMA strategy leads to more stable and well-behaved coefficient learning, while the independent initialization leads to more erratic behavior, with some coefficients even becoming negative. These results support the authors' claim that EMA-based coefficient learning is more effective for high-order methods.", "section": "3.1.2 High-order Predictor and Multistep Corrector"}, {"figure_path": "Ur9f4hNIpN/figures/figures_18_2.jpg", "caption": "Figure 5: The coefficient learning curves of independent initialization and EMA in both 2-order and 4-order scenarios. The experiments are conducted on WMT En-De.", "description": "This figure visualizes the learning process of learnable coefficients (\u03b3) during training for both 2-order and 4-order scenarios using two different coefficient learning strategies: independent initialization and exponential moving average (EMA).  The independent initialization strategy allows coefficients to be independently initialized, while the EMA strategy assigns larger weights to more recent approximations. The plots show how these coefficients evolve over epochs (training iterations) for the WMT English-German translation task. The results demonstrate that the EMA strategy leads to a more stable and predictable coefficient learning process compared to independent initialization, resulting in improved translation performance.", "section": "3.1.2 High-order Predictor and Multistep Corrector"}, {"figure_path": "Ur9f4hNIpN/figures/figures_18_3.jpg", "caption": "Figure 5: The coefficient learning curves of independent initialization and EMA in both 2-order and 4-order scenarios. The experiments are conducted on WMT En-De.", "description": "This figure shows the learning curves of the learnable coefficients (\u03b3) in the EMA method for both 2nd-order and 4th-order models during training on the WMT English-German translation task.  It compares two scenarios: (1) independent initialization, where each coefficient is initialized independently, and (2) EMA-based initialization, where the coefficients are initialized using an exponential moving average. The plot shows how these coefficients evolve over epochs, illustrating the impact of the different initialization strategies. The results support the claim that the EMA initialization leads to better coefficient learning and thus improves model performance.", "section": "3.1.2 High-order Predictor and Multistep Corrector"}, {"figure_path": "Ur9f4hNIpN/figures/figures_18_4.jpg", "caption": "Figure 5: The coefficient learning curves of independent initialization and EMA in both 2-order and 4-order scenarios. The experiments are conducted on WMT En-De.", "description": "This figure shows the learning curves of the learnable coefficients \u03b3 in the EMA coefficient learning method for both 2-order and 4-order scenarios. The independent initialization setting allows the coefficients to be independently initialized, while the EMA method uses an exponential moving average to update the coefficients. The experiments were conducted on the WMT En-De dataset, and the results show that the EMA method leads to more stable and consistent coefficient learning curves than the independent initialization setting.", "section": "More Analyses"}]