[{"figure_path": "Ur9f4hNIpN/tables/tables_4_1.jpg", "caption": "Table 1: Comparison with the state-of-the-arts on the WMT En-De and WMT En-Fr tasks. We both report the tokenized BLEU and SacreBLEU scores for comparison with previous work.", "description": "This table compares the performance of the proposed PCformer model with several state-of-the-art models on the widely used machine translation benchmarks WMT'14 English-German and WMT'14 English-French.  The results are presented in terms of BLEU and SacreBLEU scores, showing the superiority of PCformer in terms of accuracy while using fewer parameters.", "section": "4 Experimental Results"}, {"figure_path": "Ur9f4hNIpN/tables/tables_5_1.jpg", "caption": "Table 1: Comparison with the state-of-the-arts on the WMT En-De and WMT En-Fr tasks. We both report the tokenized BLEU and SacreBLEU scores for comparison with previous work.", "description": "This table compares the performance of the proposed PCformer model with several state-of-the-art models on the widely used machine translation benchmarks, WMT En-De and WMT En-Fr.  The results are presented in terms of both tokenized BLEU and SacreBLEU scores, providing a comprehensive evaluation of the model's performance compared to existing approaches. The table includes various model configurations, layer numbers, and parameter counts, offering insights into the relationship between model architecture and translation quality. ", "section": "4 Experimental Results"}, {"figure_path": "Ur9f4hNIpN/tables/tables_5_2.jpg", "caption": "Table 2: Results on the En-Ro task.", "description": "This table presents the results of the English-Romanian (En-Ro) machine translation task.  It compares the performance of various models, including different versions of the PCformer model (with varying numbers of parameters and orders),  against baseline Transformer and other related models (RK2-block (gated), RK4-block). The metric used for evaluation is BLEU score.", "section": "4 Experimental Results"}, {"figure_path": "Ur9f4hNIpN/tables/tables_5_3.jpg", "caption": "Table 3: Average SacreBLEU on the OPUS-100.", "description": "This table presents a comparison of the PCformer model's performance against other state-of-the-art models on the OPUS-100 multilingual machine translation benchmark.  It shows the average SacreBLEU scores for translation in both directions (English to other languages, and other languages to English) for various model sizes and architectures, highlighting the improved performance of the PCformer model.", "section": "4 Experimental Results"}, {"figure_path": "Ur9f4hNIpN/tables/tables_6_1.jpg", "caption": "Table 4: ROUGE results on CNN/DailyMail summarization dataset.", "description": "This table presents the results of the abstractive summarization task on the CNN/DailyMail dataset.  It compares the performance of several models, including Surface Connection, the standard Transformer, RK2-block (gated), PCformer (2-order), RK4-block, and PCformer (4-order), in terms of ROUGE-1, ROUGE-2, and ROUGE-L scores.  The results show that PCformer consistently outperforms other baselines, indicating the effectiveness of the proposed predictor-corrector approach in this task as well.", "section": "4 Experimental Results"}, {"figure_path": "Ur9f4hNIpN/tables/tables_6_2.jpg", "caption": "Table 5: Perplexity results on Wikitext-103. Adaptive refers to Adaptive Input Transformer [3].", "description": "This table presents the perplexity results on the Wikitext-103 benchmark for various language models, including Adaptive Input Transformer, RK2-block (gated), and PCformer (2-order). It compares the perplexity scores achieved by these models on both the validation and test sets, highlighting the performance of PCformer in achieving lower perplexity scores.", "section": "4 Experimental Results"}, {"figure_path": "Ur9f4hNIpN/tables/tables_6_3.jpg", "caption": "Table 6: PCformer results against Transformer++ [58] on various configurations. All models are trained on the same subset of the SlimPajama dataset (from 6B to 100B) with the Mistral tokenizer [21]. The last column shows the average over all benchmarks that use (normalized) accuracy as the metric.", "description": "This table compares the performance of PCformer against Transformer++ on various configurations, using different sizes of the SlimPajama dataset and the Mistral tokenizer.  The results are evaluated across multiple downstream tasks, with the final column representing the average normalized accuracy across all tasks.", "section": "3.2 Improving Training Stability"}, {"figure_path": "Ur9f4hNIpN/tables/tables_7_1.jpg", "caption": "Table 7: Comparison results on the GLUE development set. COLA QQP MNLI-m/mm SST-2 STS-B QNLI RTE MRPC Avg. Mcc Acc Acc Acc Corr Acc Acc Acc", "description": "This table presents the comparison results on the GLUE benchmark's development set between the BERT model and the proposed PCformer model.  The GLUE benchmark comprises eight sub-tasks assessing various aspects of natural language understanding. The table shows the performance of each model on each sub-task, using metrics appropriate to the sub-task (e.g., accuracy, Matthews correlation coefficient, Pearson correlation). The average score across all sub-tasks is also provided. This comparison highlights the improvement in language understanding capabilities achieved by the PCformer model compared to the BERT model.", "section": "LM Evaluation Harness"}, {"figure_path": "Ur9f4hNIpN/tables/tables_7_2.jpg", "caption": "Table 8: Comparison of PPL on PTB.", "description": "This table presents the perplexity (PPL) results on the Penn Treebank (PTB) dataset for various models, including different versions of the RK-block and PCformer.  It demonstrates the reduction in PPL achieved by incorporating the exponential moving average (EMA) based coefficient learning and the predictor-corrector framework. The results are shown separately for 1-layer and 2-layer models.", "section": "3.1.2 High-order Predictor and Multistep Corrector"}, {"figure_path": "Ur9f4hNIpN/tables/tables_7_3.jpg", "caption": "Table 1: Comparison with the state-of-the-arts on the WMT En-De and WMT En-Fr tasks. We both report the tokenized BLEU and SacreBLEU scores for comparison with previous work.", "description": "This table compares the performance of the proposed PCformer model against other state-of-the-art models on the WMT'14 English-German and English-French machine translation tasks.  It shows the number of layers, number of parameters, number of training steps, BLEU scores, and SacreBLEU scores for each model.", "section": "4 Experimental Results"}, {"figure_path": "Ur9f4hNIpN/tables/tables_8_1.jpg", "caption": "Table 10: Ablation on the several choices of the predictor and corrector on four translation tasks.", "description": "This table presents the ablation study on the predictor-corrector framework. It shows the BLEU scores achieved by different combinations of predictors (First-order Baseline, ODE Transformer, RK2-block with EMA, Multistep Method) and correctors (Multistep Method, Backward Euler Method) on four machine translation tasks (En-De, En-Fr, En-Ro, OPUS). The results demonstrate the impact of the choice of predictor and corrector on the overall performance.", "section": "3.2 Improving Training Stability"}, {"figure_path": "Ur9f4hNIpN/tables/tables_8_2.jpg", "caption": "Table 1: Comparison with the state-of-the-arts on the WMT En-De and WMT En-Fr tasks. We both report the tokenized BLEU and SacreBLEU scores for comparison with previous work.", "description": "This table compares the performance of the proposed PCformer model with other state-of-the-art models on the WMT English-German and English-French machine translation tasks.  It shows the number of layers, the number of parameters, the number of training steps, the BLEU score, and the SacreBLEU score for each model.  The results demonstrate the superior performance of the PCformer model, especially when using a larger model size.", "section": "4 Experimental Results"}, {"figure_path": "Ur9f4hNIpN/tables/tables_8_3.jpg", "caption": "Table 12: Comparison of inference speed (sentences/s) and memory consumption (GB) between the vanilla Transformer and numerical Transformers.", "description": "This table compares the inference speed and memory consumption of vanilla Transformers and numerical Transformers (ODE Transformer and PCformer) with varying numbers of layers.  It shows that while the numerical methods are slower, they achieve comparable or better BLEU scores with less memory usage.", "section": "4 Experimental Results"}, {"figure_path": "Ur9f4hNIpN/tables/tables_15_1.jpg", "caption": "Table 1: Comparison with the state-of-the-arts on the WMT En-De and WMT En-Fr tasks. We both report the tokenized BLEU and SacreBLEU scores for comparison with previous work.", "description": "This table compares the performance of the proposed PCformer model with various state-of-the-art models on the WMT English-German and English-French machine translation tasks.  The table shows the number of layers, number of parameters, number of training steps, BLEU scores, and SacreBLEU scores for each model.  The results demonstrate the superior performance of the PCformer model compared to other models.", "section": "4 Experimental Results"}, {"figure_path": "Ur9f4hNIpN/tables/tables_18_1.jpg", "caption": "Table 14: Comparison of Flowformer and PCformer on different datasets.", "description": "This table compares the performance of the PCformer model against the Flowformer model on ten different time-series forecasting datasets.  The datasets cover various domains, including ethanol concentration, face detection, handwriting, heartbeat, Japanese vowels, traffic flow (PEMS-SF), self-regulation, spoken Arabic digits, and UWAVE gesture library. For each dataset, the table shows the average score achieved by each model.  The average score across all ten datasets is also provided, indicating an overall improvement in performance for the PCformer model.", "section": "4 Experimental Results"}]