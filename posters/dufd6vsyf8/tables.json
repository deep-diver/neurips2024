[{"figure_path": "DUFD6vsyF8/tables/tables_2_1.jpg", "caption": "Table 1: Iteration complexities of NPG and FedNPG (ours) methods to reach \u025b-accuracy of the vanilla and entropy-regularized problems, where we assume exact gradient evaluation, and only keep the dominant terms w.r.t. \u025b. The policy estimates in the t-iteration are \u03c0(t) and \u03c0(t) for NPG and FedNPG, respectively, where T is the number of iterations. Here, N is the number of agents, \u03c4 < 1 is the regularization parameter, \u03c3 \u2208 [0, 1] is the spectral radius of the network, \u03b3 \u2208 [0, 1) is the discount factor, A is the size of the action space, and \u03b7 > 0 is the learning rate. The iteration complexities of FedNPG reduce to their centralized counterparts when \u03c3 = 0. For vanilla FedNPG, the learning rate is set as 1/3; for entropy-regularized FedNPG, the learning rate satisfies 0 < \u03b7 < \u03b7o = (1\u2212\u03b3)(1\u2212\u03c3)2 log |A|/32TN2\u03c3.", "description": "This table presents the iteration complexities for both centralized and federated NPG methods to achieve an \n\u03b5-accurate solution for the vanilla and entropy-regularized multi-task reinforcement learning problems. The table shows how the iteration complexities depend on various factors, including the number of agents (N), the regularization parameter (\u03c4), the spectral radius of the network (\u03c3), the discount factor (\u03b3), the size of the action space (|A|), and the learning rate (\u03b7). For the federated method, the complexities reduce to their centralized counterparts when the network is fully connected (\u03c3 = 0).", "section": "3 Federated NPG methods for multi-task RL"}]