[{"type": "text", "text": "Linear Uncertainty Quantification of Graphical Model Inference ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Chenghua $\\mathbf{Guo}^{1^{*}}$ , Han $\\mathbf{Y}\\mathbf{u}^{2^{*}}$ , Jiaxin $\\mathbf{Liu}^{3}$ , Chao Chen4, Qi $\\mathbf{Li}^{5}$ , Sihong $\\mathbf{Xie}^{6\\dagger}$ , Xi Zhang1\u2020 ", "page_idx": 0}, {"type": "text", "text": "1Key Laboratory of Trustworthy Distributed Computing and Service (MoE), Beijing University of Posts and Telecommunications, China   \n2College of Artificial Intelligence, Beijing University of Posts and Telecommunications, China 3Department of Computer Science and Engineering, Lehigh University, USA   \n4School of Computer Science and Technology, Harbin Institute of Technology (Shenzhen), China 5Department of Computer Science, Iowa State University, USA 6AI Thrust, The Hong Kong University of Science and Technology (Guangzhou), China {chenghuaguo, zhangx}@bupt.edu.cn, yuhan2021@bupt.edu.cn, jilb17@lehigh.edu, cha01nbox@gmail.com, qli@iastate.edu, xiesihong1@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Uncertainty Quantification (UQ) is vital for decision makers as it offers insights into the potential reliability of data and model, enabling more informed and risk-aware decision-making. Graphical models, capable of representing data with complex dependencies, are widely used across domains. Existing sampling-based UQ methods are unbiased but cannot guarantee convergence and are time-consuming on largescale graphs. There are fast UQ methods for graphical models with closed-form solutions and convergence guarantee but with uncertainty underestimation. We propose LinUProp, a UQ method that utilizes a novel linear propagation of uncertainty to model uncertainty among related nodes additively instead of multiplicatively, to offer linear scalability, guaranteed convergence, and closed-form solutions without underestimating uncertainty. Theoretically, we decompose the expected prediction error of the graphical model and prove that the uncertainty computed by LinUProp is the generalized variance component of the decomposition. Experimentally, we demonstrate that LinUProp is consistent with the sampling-based method but with linear scalability and fast convergence. Moreover, LinUProp outperforms competitors in uncertainty-based active learning on four real-world graph datasets, achieving higher accuracy with a lower labeling budget. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graphical models are known for their capability to represent data with complex dependencies. These models have been extensively applied to various fields, ranging from social networks [9, 39], fraud detection [26, 8], recommendation [14, 37] and crowdsourcing [25, 16], to more recent applications in enhancing graph neural networks (GNNs) [10, 28] and large language models (LLMs) [15, 34]. Among the many inference techniques in graphical models [19, 13, 12, 17], Belief Propagation (BP) [27] stands out as a powerful iterative message-passing algorithm. BP takes an initial guess (or \u201cprior belief\u201d) of each node and refines it using information propagation, resulting in an updated, more accurate estimate called \u201cposterior belief\u201d. However, a crucial limitation of BP is that it only provides point estimates for these posterior beliefs [7] and does not capture their potential uncertainty, leading to decisions unaware of the underlying data and model unreliability. For example, in a social network, a higher inferred probability of Alice\u2019s interest in sports over Bob\u2019s may lead to erroneous decisions if, in fact, Alice\u2019s inferred probability has high uncertainty and only a slightly higher interest. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To handle the uncertainty present in the beliefs, some well-known sampling-based uncertainty quantification (UQ) tools, such as Monte Carlo (MC) simulations, can provide unbiased UQ. However, these techniques are time-consuming for large-scale graphs and cannot ensure convergence within a reasonable time frame in practice [21]. Existing works [7, 35], based on Bayesian theory, have derived UQ methods with provable convergence and scalability by modeling beliefs as Dirichlet distributions and treating neighboring nodes as observations. However, this means that any neighbor of a node will necessarily reduce the uncertainty, even neighbors with noise or missing information. As illustrated in Figure 1, consider a user A with no preference information about being a music enthusiast, and A has two friends who are music enthusiasts (represented by a strong preference with Beta distribution parameters like $B(3,1)$ , where the parameters represent positive and negative preference counts). If A gains an additional friend D who has no preference information (represented by a uniform distribution $B(1,1);$ ), the existing methods still reduce A\u2019s uncertainty, incorrectly suggesting increased confidence that A is a music enthusiast. This results in an underestimation of posterior uncertainty. Furthermore, none of existing works provided a theoretical relationship between the computed uncertainty and the expected model prediction error, making it difficult for decision-makers to understand and trust the UQ results. ", "page_idx": 1}, {"type": "text", "text": "To address the challenges mentioned above, we introduce Linear Uncertainty Propagation (LinUProp), a method to quantify the uncertainty in posterior beliefs resulting from multiple iterations of propagation of uncertainty in node priors, which offers the following advantages: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Methodologically, we propose LinUProp (Sec. 3), a novel linear method that spreads the uncertainty from each node to the entire graph and additively aggregates this uncertainty to avoid underestimating posterior uncertainty. LinUProp offers interpretability due to the additive of neighbor uncertainty (Fig. 1), enabling tracking the contributions of other nodes to the computed uncertainty and allowing users to understand its sources. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Theoretically, LinUProp possesses a closedform solution, provable convergence related to the spectral radius of a matrix representing the dependencies between nodes (Sec. 4.1), and proven linear scalability (Appendix A.3). Moreover, by employing the bias-variance decomposition, we demonstrate that the posterior uncertainty is a generalized variance component of the expected model prediction error (Sec. 4.2). ", "page_idx": 1}, {"type": "image", "img_path": "XOVks7JHQA/tmp/fe1fa19bf8e524a124b8a3d6a5586949e9335aa950cb63cc9fe6fcda856355ac.jpg", "img_caption": ["Figure 1: The impact of more neighbors on posterior uncertainty. NETCONF [7] or SocNL [35] underestimates posterior uncertainty of $A$ in $G_{2}$ by considering high-uncertainty node $D$ as neighbor that reduce uncertainty. LinUProp represents the uncertainty of a posterior using interval widths, where increased uncertainty from more neighbors leads to higher posterior uncertainty. The proposed interval ways can be interpret as a generalized variance component of the expected prediction error (Sec. 4.2). Furthermore, LinUProp is interpretable due to "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "\u2022 Experimentally, we demonstrate the follow- the additive of neighbor uncertainty (Eq. (12)). ing with LinUProp: (1) The uncertainty quantified by LinUProp is accurate due to its strong positive correlation with MC simulations (Fig. 7 in Appendix B.1); (2) LinUProp exhibits both fast convergence and linear scalability (Fig. 4); (3) LinUProp is interpretable (Fig. 3); (4) When applied to graph active learning guided by uncertainty, LinUProp achieves superior accuracy with a smaller budget compared to the baseline method that underestimates uncertainty (Figs. 5-6) ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Belief Propagation (BP) [27] iteratively passes messages on a graph to infer posterior distributions of variables on a graphical model. We focus on the uncertainty in the inferred posteriors used for node classification on Markov Random Field (MRF). Bayesian networks are another important type of graphical models that can be converted into MRF [27]. ", "page_idx": 2}, {"type": "text", "text": "Consider a graph $\\mathcal{G}=(\\mathcal{V},\\mathcal{E})$ with $n$ random variable nodes, each with $k$ possible classes. The prior $\\mathbf{e}_{s}$ for node $s$ is a $k$ -dimension probability distribution and $e_{s}(i)$ specifies the prior probability for node $s$ belonging to class $i$ . Each edge $(s,t)$ from $\\mathcal{G}$ represents the dependencies between the two random variables $s$ and $t$ . In particular, the dependency is represented by a pairwise potential function, which is a $k\\times k$ compatibility matrix $\\mathbf{H}$ and $H(i,j)$ denotes the degree of association between class $i$ of node $s$ and class $j$ of node $t$ . We assume that $\\mathbf{H}$ is a symmetric matrix, as in [11, 7]. Specifically, for binary classification, the compatibility matrix is assumed to be ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{H}=\\left[\\boldsymbol{0.5}+\\epsilon\\quad0.5-\\epsilon\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Similarly, an example form of $\\mathbf{H}$ for multi-class problems is setting the diagonal elements to $\\begin{array}{r}{\\frac{1}{k}+(k-1)\\epsilon}\\end{array}$ and the other elements to $\\textstyle{\\frac{1}{k}}-\\epsilon.\\ |\\epsilon|$ is typically close to 0, and a positive/negative $\\epsilon$ specifies a homophily/heterophily relationship between any two connected nodes. Unlike existing methods that require all edges to have the same compatibility matrix, our proposed LinUProp can handle situations where each edge has a different compatibility matrix. ", "page_idx": 2}, {"type": "text", "text": "BP updates the $k$ -dimensional message $\\mathbf{m}_{t s}$ sent from node $t$ to node $s$ by: ", "page_idx": 2}, {"type": "equation", "text": "$$\nm_{t s}(i)\\gets\\sum_{j=1}^{k}H(i,j)e_{t}(j)\\prod_{u\\in\\mathcal{N}(t)\\backslash\\{s\\}}m_{u t}(j),i=1,\\ldots,k,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{N}(t)$ is the set of neighboring nodes of $t$ . Eq. (2) is applied iteratively until convergence or a maximum number of iterations is reached. Then the posterior ${\\bf b}_{s}$ for node $s$ is ", "page_idx": 2}, {"type": "equation", "text": "$$\nb_{s}(i)\\gets\\frac{1}{Z_{s}}e_{s}(i)\\prod_{t\\in\\mathcal{N}(s)}m_{t s}(i),\\ \\ i=1,\\ldots,k,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $Z_{s}$ is for normalization such that $\\begin{array}{r}{\\sum_{i=1}^{k}b_{s}(i)=1}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "Centered BP [11] is a linearized version of Eqs. (2-3): ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{m}_{t s}\\left(i\\right)\\gets k\\sum_{j=1}^{k}\\hat{H}\\left(i,j\\right)\\left(\\hat{b}_{t}\\left(j\\right)-\\frac{1}{k}\\hat{m}_{s t}\\left(j\\right)\\right),\\ \\ \\left(4\\right)\\quad\\ \\hat{b}_{s}\\left(i\\right)\\gets\\hat{e}_{s}\\left(i\\right)+\\frac{1}{k}\\sum_{t\\in\\mathcal{N}\\left(s\\right)}\\hat{m}_{t s}(i),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\begin{array}{r}{\\hat{e}_{s}(i)=e_{s}(i)-\\frac{1}{k}}\\end{array}$ , $\\begin{array}{r}{\\hat{b}_{s}(i)=b_{s}(i)-\\frac{1}{k}}\\end{array}$ , $\\begin{array}{r}{\\hat{m}_{t s}(i)=m_{t s}(i)-1,\\hat{H}\\left(i,j\\right)=H\\left(i,j\\right)-\\frac{1}{k}}\\end{array}$ , are the centralized version of the prior, belief, message, and compatibility matrix. ", "page_idx": 2}, {"type": "text", "text": "NETCONF [7] models both the belief and uncertainty of each node using a Dirichlet distribution. The certainty of a node is represented by the sum of its Dirichlet parameters, where a higher sum indicates greater certainty. Each node $u$ is initialized with a prior Dirichlet belief vector $\\check{\\mathbf{e}}_{u}$ . The posterior Dirichlet belief $\\check{\\mathbf{b}}_{u}$ is updated using multinomial messages from its neighbors: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\check{\\mathbf{b}}_{u}\\gets\\check{\\mathbf{e}}_{u}+\\sum_{v\\in\\mathcal{N}(u)}\\check{\\mathbf{m}}_{v u},\\qquad\\check{\\mathbf{m}}_{v u}\\gets\\mathbf{M}\\left(\\check{\\mathbf{e}}_{v}+\\sum_{w\\in\\mathcal{N}(v)\\backslash u}\\check{\\mathbf{m}}_{w v}\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\check{\\mathbf{m}}_{v u}$ is the message from node $v$ to $u$ , and $\\mathbf{M}$ is a modulation matrix derived from $\\mathbf{H}$ ", "page_idx": 2}, {"type": "text", "text": "Problem 1 (Quantifying Uncertainty in Posteriors). ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given: $(I)$ An undirected graph $\\mathcal{G}=(\\mathcal{V},\\mathcal{E})$ consisting of n nodes and an adjacency matrix A, (2) $|\\mathcal{E}|$ compatibility matrices, where each matrix illustrates the dependency relationship between a pair of connected nodes, and (3) $n\\ k$ -dimensional prior uncertainty vectors $\\Phi_{i}(i=1,\\ldots,n)$ , with each vector representing the uncertainty in prior beliefs for a node as interval widths across the $k$ classes. Find n $k$ -dimensional posterior uncertainty vectors $\\mathbb{b}_{i}(i=1,\\ldots,n)$ , where each vector represents the uncertainty of posterior beliefs for $k$ classes of a node based on interval width, with wider interval indicating higher uncertainty. ", "page_idx": 2}, {"type": "image", "img_path": "XOVks7JHQA/tmp/1d5f955334ec2b144242407ad121bc4c462f45f2e4e7470872171178a88288fa.jpg", "img_caption": ["Figure 2: An illustration of LinUProp quantifying uncertainty in posterior beliefs for each node in a 3-node chain. Inputs: (1) Uncertainty in prior beliefs of each node represented as interval widths $(\\mathbb{e}_{1},\\mathbb{e}_{2},\\mathbb{e}_{3})$ (2) Edge potentials ( ${\\bf{H}}_{12}$ and $\\mathbf{H}_{23}$ , with $\\mathbf{H}_{21}=\\mathbf{H}_{12}$ and $\\mathbf{H}_{32}=\\mathbf{H}_{23}$ due to the undirected graph). Outputs: uncertainty in posterior beliefs of each node also represented as interval widths. LinUProp can set a different compatibility matrix for each edge, allowing it to handle edge-dependent potentials, while previous methods cannot do this. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3 Linear Bound Propagation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Recall that our objective is to quickly quantify the uncertainty in posterior beliefs and not assume that neighbors necessarily reduce uncertainty, in order to avoid uncertainty underestimation. Interval arithmetic [6] is a non-probabilistic method for quantifying uncertainty without assumptions regarding neighbors. However, it is unclear how to directly apply interval arithmetic rules to Eqs. (2-3) of BP while meeting the above objectives. ", "page_idx": 3}, {"type": "text", "text": "A solution to these challenges lies in linearization. The linearized BP is more amenable to interval arithmetic due to its exclusive reliance on interval addition, sidestepping the interval multiplication that may underestimate uncertainty. Consequently, this facilitates the derivation of closed-form solution, endowing the method with linear scalability, interpretability, and guaranteed convergence. ", "page_idx": 3}, {"type": "text", "text": "We will next introduce LinUProp and its iterative variant, which is computationally efficient in practical applications. Prior to that, we need some additional notation pertinent to LinUProp. Let vec denotes the operation that vertically concatenates the rows of a given matrix into a single column vector, and let Diag denote the transformation of an $n k\\times k$ block matrix into a block diagonal matrix. $\\mathbb{E}=\\left[\\mathbb{e}_{1},\\mathbb{e}_{2},\\cdot\\cdot\\cdot\\,,\\mathbb{e}_{n}\\right]^{T}$ and $\\mathbb{B}=\\left[\\mathbf{\\mathcal{b}}_{1},\\mathbf{\\mathcal{b}}_{2},\\cdot\\cdot\\cdot,\\mathbf{\\mathcal{b}}_{n}\\right]^{T}$ respectively represent the uncertainties of prior and posterior beliefs for all $n$ nodes, each of dimension $n\\times k$ . $\\mathbf{Q}$ is an $n k\\times k$ matrix formed by vertically stacking $n$ identity matrices, each of size $k\\times k$ . $\\Psi_{1}^{'}$ and $\\Psi_{2}^{'}$ are $n k\\times n k$ block matrices formed from $\\hat{\\mathbf{H}}_{s t}^{'}$ and $\\hat{\\mathbf{H}}_{s t}^{'2}$ , respectively. For any given edge $(s,t)$ , $\\hat{\\mathbf{H}}_{s t}^{'}$ denotes the centralized compatibility matrix corresponding to that edge, with entries $\\hat{\\mathbf{H}}_{s t}^{'}$ defined as $\\begin{array}{r}{\\hat{H}_{s t}^{'}(i,j)=|H_{s t}(i,j)-\\frac{1}{k}|}\\end{array}$ . In cases where the edge $(s,t)$ does not exist, the matrix defaults to zero. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{Diag}\\left(\\left[\\mathbf{\\bar{A}}_{1}\\right]\\right)=\\left[\\begin{array}{c c c}{\\mathbf{\\bar{A}}_{1}}&&\\\\ &{\\ddots}&\\\\ &&{\\mathbf{L}_{n}}\\end{array}\\right]\\quad\\Psi_{1}^{\\prime}=\\left[\\begin{array}{c c c}{\\hat{\\mathbf{H}}_{11}^{\\prime}}&{\\cdots}&{\\hat{\\mathbf{H}}_{1n}^{\\prime}}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {\\hat{\\mathbf{H}}_{n1}^{\\prime}}&{\\cdots}&{\\hat{\\mathbf{H}}_{n n}^{\\prime}}\\end{array}\\right]\\quad\\Psi_{2}^{\\prime}=\\left[\\begin{array}{c c c}{\\hat{\\mathbf{H}}_{11}^{\\prime}}&{\\cdots}&{\\hat{\\mathbf{H}}_{1n}^{\\prime2}}&{\\cdots}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {\\hat{\\mathbf{H}}_{n1}^{\\prime2}}&{\\cdots}&{\\hat{\\mathbf{H}}_{n n}^{\\prime2}}\\end{array}\\right]\\quad\\hat{\\mathbf{E}}_{2}=\\left[\\begin{array}{c c c}{\\hat{\\mathbf{H}}_{11}^{\\prime2}}&{\\cdots}&{\\hat{\\mathbf{H}}_{1n}^{\\prime2}}&{\\cdots}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {\\hat{\\mathbf{H}}_{n1}^{\\prime2}}&{\\cdots}&{\\hat{\\mathbf{H}}_{n n}^{\\prime2}}\\end{array}\\right]\\quad\\hat{\\mathbf{E}}_{3}=\\left[\\begin{array}{c}{\\hat{\\mathbf{H}}_{21}^{\\prime}}\\\\ {\\hat{\\mathbf{H}}_{22}^{\\prime}}\\\\ {\\hat{\\mathbf{H}}_{33}^{\\prime2}}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Theorem 3.1 (LinUProp). For a multi-class node classification task on an MRF, given the matrix $\\mathbb{E}$ which represents the prior belief uncertainty of all nodes, matrices $\\Psi_{1}^{'}$ and $\\Psi_{2}^{'}$ which denote the dependencies among these nodes. The posterior belief uncertainty for all nodes, represented by $\\mathbb{B}$ and in terms of interval widths, is determined by the linear equation system: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname{vec}(\\mathbb{B})=\\operatorname{vec}(\\mathbb{E})+\\left(\\Psi_{1}^{'}+\\operatorname{Diag}\\left(\\Psi_{2}^{'}\\mathbf{Q}\\right)\\right)\\operatorname{vec}(\\mathbb{B}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Figure (2) illustrates Eq. (6) using a 3-node chain as an example. From this, it can be observed that LinUProp can be implemented using matrix multiplication without computing the uncertainty propagated between each pair of nodes. In practice, a more effective strategy for computing the ", "page_idx": 3}, {"type": "text", "text": "posterior belief interval widths vec(B) is to use an iterative update version of LinUProp: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname{vec}(\\mathbb{B})^{(l+1)}=\\operatorname{vec}(\\mathbb{E})+\\left(\\Psi_{1}^{'}+\\operatorname{Diag}\\left(\\Psi_{2}^{'}\\mathbf{Q}\\right)\\right)\\operatorname{vec}(\\mathbb{B})^{(l)},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $l$ denotes the iteration round and $\\mathrm{vec}(\\mathbb{B})^{(0)}$ can be set to $\\operatorname{vec}(\\mathbb{E})$ . Although Eq. (6) is similar in form to another method, LinBP (Eq. (13), [11]), which is a point estimation inference approach rather than a UQ method, simply replacing prior and posterior beliefs in LinBP with interval widths does not yield LinUProp. LinUProp is specifically designed to quantify uncertainty through derivations involving the upper and lower bounds of messages and beliefs. For detailed derivations, please refer to Appendix A.1. Its guaranteed convergence is proven in Sec. 4.1. The proof of its linear scalability is provided in Appendix A.3, showing that the time complexity per iteration is $\\mathcal{O}(|\\mathcal{V}|)$ when $|\\gamma|>|\\mathcal{E}|$ , otherwise $\\mathcal{O}(|\\mathcal{E}|)$ . ", "page_idx": 4}, {"type": "text", "text": "4 Analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We present theoretical analyses of LinUProp, including the derivation of a closed-form solution, the proof of its convergence, and the provision of an interpretable approach to the uncertainty quantified by LinUProp. Furthermore, by employing the bias-variance decomposition, we addressed the question of what the uncertainty computed by LinUProp represents and established the connection between this uncertainty and the expected prediction error. ", "page_idx": 4}, {"type": "text", "text": "4.1 Theoretical Analysis of LinUProp ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Closed-form solution. By simplifying Eq. (6), we can derive a closed-form solution for LinUProp: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{vec}(\\mathbb{B})=\\underbrace{(\\mathbf{I}-\\boldsymbol{\\Psi}_{1}^{'}-\\mathrm{Diag}(\\boldsymbol{\\Psi}_{2}^{'}\\mathbf{Q}))^{-1}}_{\\mathbf{F}^{'}}\\mathrm{vec}(\\mathbb{E}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where I represents the identity matrix. This closed-form solution is primarily used for subsequent theoretical analysis; in practice, the iterative version Eq. (7) is employed, which does not require the computation of the matrix inverse. ", "page_idx": 4}, {"type": "text", "text": "Convergence. Let $\\Psi_{1}^{'}+\\mathrm{Diag}(\\Psi_{2}^{'}\\mathbf{Q})$ be denoted as $\\mathbf{T}$ . The sufficient and necessary criteria for the convergence of LinUProp is that the spectral radius of $\\mathbf{T}$ is less than 1: ", "page_idx": 4}, {"type": "equation", "text": "$$\nL i n U P r o p\\;\\mathrm{converges}\\;\\Longleftrightarrow\\;\\rho(\\mathbf{T})<1\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Proof. The closed-form solution of LinUProp (Eq. (8)) conforms to a general linear equation system, $\\mathbf{y}=\\left(\\mathbf{I}-\\mathbf{P}\\right)^{-1}\\mathbf{x}$ , where $\\mathbf{y},\\mathbf{P}$ , and $\\mathbf{x}$ are generic terms. Such linear equation systems can be solved by the Jacobi method [29], which converges if and only if the spectral radius of $\\mathbf{P}$ is less than 1. ", "page_idx": 4}, {"type": "text", "text": "From the above convergence condition, LinUProp has a limitation: in large graphs with strong global (most edges) homophily/heterophily, $\\rho(\\mathbf{T})$ may be large, leading to non-convergence. However, LinUProp can still converge if such strong homophily/heterophily is only local (a few edges). ", "page_idx": 4}, {"type": "text", "text": "Interpretability. When LinUProp converges, with $\\rho\\left(\\mathbf{T}\\right)\\,<\\,1$ , we can expand the closed-form solution Eq. (8) using the Neumann series, yielding: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname{vec}(\\mathbb{B})=\\left(\\mathbf{I}+\\mathbf{T}+\\mathbf{T}^{2}+\\cdot\\cdot\\cdot\\right)\\operatorname{vec}(\\mathbb{E}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The uncertainty of the posterior belief for a certain class of a node, $\\mathrm{vec}(\\mathbb{B})_{v}$ can be expanded as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{vec}({\\mathbb B})_{v}=\\mathrm{vec}({\\mathbb E})_{v}+{\\mathbf T}_{v}\\mathrm{vec}({\\mathbb E})+\\left({\\mathbf T}^{2}\\right)_{v}\\mathrm{vec}({\\mathbb E})+\\cdot\\cdot\\cdot\\cdot\\mathbf{\\Gamma},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{T}_{\\boldsymbol{v}}\\mathbf{vec}(\\mathbf{E})$ , $\\left(\\mathbf{T}^{2}\\right)_{v}\\mathbf{vec}(\\mathbb{E})$ , etc. can be expanded to $\\begin{array}{r}{\\sum_{w}\\mathbf{T}_{v,w}\\mathbf{vec}(\\mathbb{E})_{w}}\\end{array}$ , $\\sum_{w}\\left(\\mathbf{T}^{2}\\right)_{v,w}\\operatorname{vec}(\\mathbb{E})_{w}$ and so on, the subscript $v$ represents the $v$ -th row of matrices $\\mathbf{T},\\mathbf{T}^{2},\\cdots,$ while the subscript $w$ represents the $w$ -th column of matrices $\\mathbf{T},\\mathbf{T}^{2},\\cdots$ and the $w$ -th component of $\\operatorname{vec}(\\mathbb{E})$ . Finally, we derive the contribution of the prior uncertainty of a certain class of any other variable node $\\mathrm{vec}(\\mathbb{E})_{w}$ towards the posterior belief uncertainty $\\mathrm{vec}(\\mathbb{B})_{v}$ as ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nc_{w\\to v}=\\mathbf{T}_{v,w}\\mathbf{vec}(\\mathbf{{\\mathbb{E}}})_{w}+\\left(\\mathbf{T}^{2}\\right)_{v,w}\\mathbf{vec}(\\mathbf{{\\mathbb{E}}})_{w}+\\left(\\mathbf{T}^{3}\\right)_{v,w}\\mathbf{vec}(\\mathbf{{\\mathbb{E}}})_{w}+\\cdot\\cdot\\cdot\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This linear equation facilitates simple explanations of the uncertainty computed by LinUProp. Since $\\rho(\\mathbf{T})<1$ ensures convergence, the influence of higher-order terms, such as $\\dot{\\mathbf{T}}^{3}$ and $\\mathbf{T}^{4}$ , decays rapidly. Therefore, in practice, considering only the first few terms often provides an accurate and interpretable estimate of uncertainty contributions. ", "page_idx": 5}, {"type": "text", "text": "4.2 Theoretical Connection with Bias-Variance Decomposition ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To delve into the theoretical understanding of the uncertainty in the node\u2019s posterior belief, we employed the bias-variance decomposition [2], an effective tool for analyzing prediction errors and decomposing model uncertainty [41]. We demonstrated that the posterior uncertainty computed by LinUProp is a generalized variance component of the expected model prediction error, giving LinUProp a solid theoretical basis. This relationship between the uncertainty computed by UQ methods and the expected model prediction error has seldom been explored by existing work [7, 36, 35]. To begin with, we first derive a linearized BP suitable for edges with distinct potential functions to facilitate bias-variance decomposition. ", "page_idx": 5}, {"type": "text", "text": "From the update equation of message $\\hat{\\mathbf{m}}_{t s}$ as shown in Eq. (4), we can derive the update rule for the reverse message $\\hat{\\mathbf{m}}_{s t}$ and substitute it back. This leads us to the stable message $\\hat{\\mathbf{m}}_{t s}$ when the algorithm converges. By inserting the stable message into Eq. (5) and simplifying, we can derive the closed-form solution for posterior beliefs where each edge has a distinct potential function (detailed proof in Appendix A): ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{vec}(\\hat{\\mathbf{B}})=\\underbrace{(\\mathbf{I}-\\boldsymbol{\\Psi}_{1}+\\mathrm{Diag}(\\boldsymbol{\\Psi}_{2}\\mathbf{Q}))^{-1}}_{\\mathbf{F}}\\mathrm{vec}(\\hat{\\mathbf{E}}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Psi_{1}=\\left[\\begin{array}{c c c}{\\hat{\\mathbf{H}}_{11}}&{\\cdots\\cdot}&{\\hat{\\mathbf{H}}_{1n}}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {\\hat{\\mathbf{H}}_{n1}}&{\\cdots}&{\\hat{\\mathbf{H}}_{n n}}\\end{array}\\right]\\quad\\Psi_{2}=\\left[\\begin{array}{c c c}{\\hat{\\mathbf{H}}_{11}^{2}}&{\\cdots}&{\\hat{\\mathbf{H}}_{1n}^{2}}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {\\hat{\\mathbf{H}}_{n1}^{2}}&{\\cdots}&{\\hat{\\mathbf{H}}_{n n}^{2}}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where vec and Diag are the same as those in Eq. (6). $\\hat{\\bf B}\\;=\\;[\\hat{\\bf b}_{1},\\hat{\\bf b}_{2},\\cdot\\cdot\\cdot\\cdot,\\hat{\\bf b}_{n}]^{T}$ and $\\hat{\\textbf{E}}=$ $[\\hat{\\mathbf{e}}_{1},\\hat{\\mathbf{e}}_{2},\\cdot\\cdot\\cdot\\mathbf{\\nabla},\\hat{\\mathbf{e}}_{n}]^{T}$ are $n\\times k$ matrices composed of the centralized posterior and prior beliefs of all nodes. $\\mathbf{Q}$ is an $n k\\times k$ matrix formed by vertically stacking $n$ identity matrices, each of size $k\\times k$ . $\\Psi_{1}$ and $\\Psi_{2}$ are $n k\\times n k$ block matrices formed from $\\hat{{\\mathbf{H}}}_{s t}$ and $\\hat{{\\mathbf{H}}}_{s t}^{2}$ , respectively. For any given edge $(s,t)$ , $\\hat{{\\mathbf{H}}}_{s t}$ denotes the centralized compatibility matrix corresponding to that edge, with entries $\\hat{{\\mathbf{H}}}_{s t}$ defined as $\\begin{array}{r}{\\hat{H}_{s t}(i,j)=H_{s t}(i,j)-\\frac{1}{k}}\\end{array}$ . If the edge $(s,t)$ does not exist, the matrix defaults to zero. ", "page_idx": 5}, {"type": "text", "text": "This closed-form solution shows that the posterior belief can be regarded as a linear model of the prior belief. Specifically, consider the element $\\mathrm{vec}(\\hat{\\mathbf{B}})_{v}$ . This element represents the centralized posterior belief of a specific class corresponding to a particular node. It is formulated as a linear combination of all centralized prior beliefs, represented by $\\mathbf{F}_{v}\\mathrm{vec}(\\hat{\\mathbf{E}})$ . Here, ${\\bf F}_{v}$ denotes the $v$ -th row of $\\mathbf{F}$ . This linear equation enables us to derive a bias-variance decomposition of the uncertainty in the posterior belief. We define $h(\\hat{\\mathbf{E}})$ as the centralized posterior belief corresponding to a specific class for a particular node in an unknown true linear model, and represent the centralized belief about $\\hat{\\bf E}$ computed using Eq. (13) as vec $(\\hat{\\mathbf{B}})_{v}$ . Then, the expected model prediction error can be decomposed into bias and variance as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left(h(\\hat{\\mathbf{E}})-\\mathrm{vec}(\\hat{\\mathbf{B}})_{v}\\right)^{2}\\right]=\\underbrace{\\left(h(\\hat{\\mathbf{E}})-\\mathbb{E}\\left[\\mathrm{vec}(\\hat{\\mathbf{B}})_{v}\\right]\\right)^{2}}_{(\\mathrm{Bias})^{2}}+\\underbrace{\\mathbb{E}\\left[\\left(\\mathrm{vec}(\\hat{\\mathbf{B}})_{v}-\\mathbb{E}\\left[\\mathrm{vec}(\\hat{\\mathbf{B}})_{v}\\right]\\right)^{2}\\right]}_{\\mathrm{Variance}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Expanding the variance term yields ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left(\\mathrm{vec}(\\hat{\\mathbf{B}})_{v}-\\mathbb{E}[\\mathrm{vec}(\\hat{\\mathbf{B}})_{v}]\\right)^{2}\\right]=\\mathbf{F}_{v}\\boldsymbol{\\Sigma}_{\\mathrm{vec}(\\hat{\\mathbf{E}})}\\mathbf{F}_{v}^{T},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\Sigma_{\\mathrm{vec}(\\hat{\\mathbf{E}})}$ is the covariance matrix of $\\mathrm{vec}(\\hat{\\mathbf{E}})$ (proof in Appendix A.4). If we assume that the priors of different nodes and class probabilities within each node\u2019s prior are nearly independent, $\\Sigma_{\\mathrm{vec}(\\hat{\\mathbf{E}})}$ can be approximated as a diagonal matrix. Consequently, the sign of the elements in ${\\bf F}_{v}$ will not affect the result of Eq. (15). Ignoring the small term $\\mathrm{Diag}(\\cdot)$ , the variance component can be approximated as $\\mathbf{F}_{v}^{'}\\Sigma_{\\mathrm{vec}(\\hat{\\mathbf{E}})}\\mathbf{F}_{v}^{'T}$ . The posterior interval width for $v$ computed by LinUProp is $\\mathrm{vec}(\\mathbb{B})_{v}=\\mathbf{F}_{v}^{'}\\mathrm{vec}(\\mathbb{E})$ , leading to $\\operatorname{vec}(\\mathbb{B})_{v}(\\operatorname{vec}(\\mathbb{B})^{T})_{v}=\\mathbf{F}_{v}^{'}(\\operatorname{vec}(\\mathbb{E})\\operatorname{vec}(\\mathbb{E})^{T})\\mathbf{F}_{v}^{'T}$ . This implies that the variance component is a special case of LinUProp where the outer product of the prior interval width $\\mathrm{vec}(\\mathbb{E})\\bar{\\mathrm{vec}}(\\mathbb{E})^{T}$ is the covariance matrix $\\Sigma_{\\mathrm{vec}(\\hat{\\mathbf{E}})}$ . In other words, the uncertainty computed by LinUProp is a generalized variance component of the expected prediction error. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We begin with a simple case study to illustrate the correctness and interpretability of quantified uncertainty. We then present quantitative evidence of our method\u2019s ability to accurately quantify uncertainty, and demonstrate the convergence and scalability of our methods on real-world datasets. Finally, we compare the efficacy of LinUProp to other competitors in graph active learning tasks. ", "page_idx": 6}, {"type": "text", "text": "5.1 Case Study on a Simple Graph ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Correctness of quantified uncertainty. To verify the correctness of the uncertainty quantified by LinUProp for posterior beliefs, we first qualitatively analyze the belief bounds on a simple graph. Specifically, we conduct the experiment on a $4\\!\\times\\!4$ grid, as shown in Figure 3(a), with priors set to ", "page_idx": 6}, {"type": "image", "img_path": "XOVks7JHQA/tmp/539d3c56795d8b2b629f7e77e3a39a9641109c2e178683b82784c77e9037a111.jpg", "img_caption": ["Figure 3: Case Study. (a) A $4\\!\\times\\!4$ grid with two classes. The nodes colored in red and green are labeled, while the rest are unlabeled. The bold unlabeled node indicates the node we aim to explain the source of uncertainty. (b) An interpretation of the uncertainty in the belief of the bold node computed by LinUProp. The colors represent the contribution of each node to the uncertainty of the bold node, with warmer colors indicating more significant contributions; the radius of the white circles indicates the belief bound width computed by LinUProp for each node, with a larger radius indicating higher uncertainty in the beliefs. "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "XOVks7JHQA/tmp/b9d9e71275306db221b13ef7fc2c2f850327dfdfb444d2135818d6eb099add07.jpg", "img_caption": ["Figure 4: (a) Convergence of average belief bound width by LinUProp. (b) Scalability. Each data point represents the running time of LinUProp for 10 iterations with a certain number of edges. The y-axis is the running time in seconds. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "$B(9,1)$ (red nodes) or $B(1,9)$ (green node) for labeled nodes, and $B(1,1)$ for the unlabeled nodes. We use Beta distributions as priors in binary classification because they are a natural choice for binary random variables [2]. For each variable node, the prior interval width for each class is set to twice the standard deviation of the corresponding prior Beta distribution. For each edge, the compatibility matrix is set as in Eq. (1), with $\\epsilon=0.1$ . ", "page_idx": 6}, {"type": "text", "text": "Figure 3(b) shows that the more labeled nodes around each node $s$ , the smaller the uncertainty in the belief; the more unlabeled nodes around $s$ , the larger the uncertainty in the belief, which is intuitive. In addition to the qualitative analysis mentioned above, we also compared the uncertainty quantified by LinUProp with that quantified by MC simulations. MC simulations are adopted as the ground-truth due to its ability to provide accurate approximations through a sufficient amount of sampling [30], which are feasible for small-scale graphs. The experimental results show a strong positive correlation between the uncertainties quantified by the two methods $\\mathrm{(PCC=}0.9084\\$ ), which can be found in Appendix B.1. ", "page_idx": 6}, {"type": "text", "text": "Interpretability. By Eq. (12), we can compute the contribution of any node $t$ to the uncertainty in the belief of node $s$ computed by LinUProp. Figure 3(b) shows that the main source of uncertainty for the bold node is itself because it is an unlabeled node. The unlabeled nodes in the neighborhood are the secondary sources, and the unlabeled nodes within 2 hops are the tertiary sources. It demonstrates that the uncertainty in the beliefs quantified by LinUProp has good interpretability, which can enhance users\u2019 trust in UQ results computed by LinUProp. ", "page_idx": 7}, {"type": "text", "text": "5.2 Experiments on Real Data ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We validate the properties of LinUProp on three popular citation networks (Cora, Citeseer, and PubMed) [18] and a political blog hyperlink network (PolBlogs) [1]. For further details regarding these datasets and experimental configurations, please refer to Appendix B.2. Our experimental findings can be summarized into the following three aspects. ", "page_idx": 7}, {"type": "text", "text": "Convergence. We set node priors based on classification type: Beta distributions for binary and Dirichlet distributions for multi-class ( $k$ classes), both using parameter vector $_{\\alpha}$ of length $k$ . In datasets, $30\\%$ of nodes are randomly labeled; if labeled as class $i$ , $\\alpha_{i}=10$ , otherwise, entries are 1. Unlabeled nodes have $\\alpha=1$ . Prior interval widths for each class are twice the standard deviation of the distribution, capturing uncertainty by representing the interval as mean $\\pm$ std. ", "page_idx": 7}, {"type": "text", "text": "To simulate the diversity of dependencies between nodes, each edge\u2019s $\\epsilon$ is randomly selected from {1e-4, 5e-4, 1e-3, 5e-3, 1e-2} and linked to a $k\\times k$ compatibility matrix. Diagonal elements are $\\begin{array}{r}{\\frac{1}{k}+(k-1)\\epsilon}\\end{array}$ , and other elements are $\\scriptstyle{\\frac{1}{k}}\\,-\\,\\epsilon$ . We monitor LinUProp \u2019s convergence (iterative version in Eq. (7)) by measuring the average belief bound width, $\\begin{array}{r}{\\sum_{p=1}^{n}\\sum_{q=1}^{k}\\mathbb{B}(p,q)/(n*k)}\\end{array}$ . ", "page_idx": 7}, {"type": "text", "text": "Figure 4(a) shows that LinUProp converges within 10 iterations across all four datasets, demonstrating its rapid convergence. Using an iterative update version of $L i n U P r o p$ , the posterior belief bound width for each node increases over iterations due to uncertainty propagation from other nodes. ", "page_idx": 7}, {"type": "text", "text": "Scalability. We initialize the prior interval width for nodes and the compatibility matrix for edges following the same procedure as in the convergence experiment. Then we uniformly sampled different numbers of edges from four datasets and recorded the running time of LinUProp for 10 iterations. Figure 4(b) shows that the running time scales linearly in the number of edges. For more details on the runtime comparison between NETCONF and LinUProp, please refer to Appendix B.4. ", "page_idx": 7}, {"type": "text", "text": "Effectiveness. For large-scale graphs, MC sampling is computationally impractical due to the significantly increased time required for each sample. Therefore, we evaluate LinUProp\u2019s effectiveness using active learning as a downstream task. Specifically, we use uncertainty-based sampling to select the next node for label acquisition, as detailed in [23]. In this context, users seek a labeled dataset with minimal uncertainty, maximum accuracy, and minimal labeling budget. ", "page_idx": 7}, {"type": "text", "text": "We simulate a realistic scenario as in [40], using training set $\\mathcal{V}_{t r a i n}$ , validation set $\\mathcal{V}_{v a l}$ , test set $\\nu_{t e s t}$ , and unlabeled pool $\\mathcal{V}_{u l p}$ (node numbers in Table 1, Appendix B.2), with query batch size $b$ Initially, training set nodes are labeled and others are unlabeled. Node priors follow the convergence experiment procedure. Edge compatibility matrices match the correctness experiment, enabling comparisons with existing methods that cannot handle diverse potential functions. Due to noisy labeling, the unlabeled pool remains unchanged to allow re-labeling [31]. ", "page_idx": 7}, {"type": "text", "text": "For each selected node $s$ , we update its Dirichlet prior by incrementing the parameter for the given label class by 1. After each labeling iteration, all nodes undergo inference. Noisy labels may not guarantee improved inference accuracy, so we use the iteration yielding the highest validation accuracy to evaluate the test set, which determines the test accuracy for the current labeling budget. MC-based methods are unsuitable for active learning as it requires time-consuming sampling for each iteration. We use the following strategies in each iteration: ", "page_idx": 7}, {"type": "text", "text": "\u2022 Random: Select $b$ nodes randomly. ", "page_idx": 7}, {"type": "text", "text": "\u2022 Least Confidence (LC): Calculate uncertainty as $U_{L C}(s)=1-\\mathrm{argmax}_{i}\\,b_{s}(i).$ \u2022 Entropy: Calculate uncertainty as $\\begin{array}{r}{U_{E n t r o p y}(s)=-\\sum_{i}b_{s}(i)\\log b_{s}(i)}\\end{array}$ . ", "page_idx": 7}, {"type": "text", "text": "\u2022 Certainty Score (CS): Let $\\breve{b_{s}}$ be the parameter vector of the posterior Dirichlet distribution of node $s$ inferred by NETCONF [7]. Uncertainty is $\\begin{array}{r}{U_{C S}(s)=-\\sum_{i}\\breve{b_{s}}(i)}\\end{array}$ . Due to assumptions on distribution forms of priors, messages, and posterior beliefs, only applicable with NETCONF. ", "page_idx": 7}, {"type": "image", "img_path": "XOVks7JHQA/tmp/eaa9a864b8af9c592524b3db90540c4f0277a36b56e22489af32745cf3f120b9.jpg", "img_caption": ["Figure 5: Test accuracy for varying labeling budgets with BP inferring posterior beliefs. Each subplot title includes two components, which represent the labeling accuracy and dataset. Each column corresponds to a dataset. In each subplot, the node selection strategies based on LinUProp and its variant are represented by red $\\blacktriangledown$ and purple $\\blacktriangle$ . Under the same labeling budget, the higher the test accuracy, the better. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "XOVks7JHQA/tmp/f1edaa70cc7eb660c37119bf83f34531350ce6fd9358da9a0c2d67acf15706b1.jpg", "img_caption": ["Figure 6: Test accuracy for varying labeling budgets with NETCONF inferring posterior beliefs. Each subplot title includes two components, which represent the labeling accuracy and dataset. Each column corresponds to a dataset. In each subplot, the node selection strategies based on LinUProp and its variants are represented by $\\blacktriangledown$ and purple $\\blacktriangle$ , while the strategies based on the native UQ method in NETCONF and its variants are represented by brown \u25c0and pink \u25b6. NETCONF-based strategies $\\mathrm{CS/LC+CS}$ ) often prioritize labeling low-degree nodes due to their assumption that neighbors serve as evidence, leading to persistent high uncertainty in most nodes and slowly increasing accuracy. Under the same labeling budget, the higher the test accuracy, the better. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "\u2022 Belief Bound (BB): Based on LinUProp with the same prior bound width setting as in the convergence experiment, uncertainty is $\\bar{U_{B B}}(s)=\\mathbb{b}_{s}$ .   \n\u2022 $\\mathbf{LC+CS}$ : Perform Min-Max normalization on $U_{L C}(s)$ and $U_{C S}(s)$ to obtain $U_{L C}^{n o r m}(s)$ and $U_{C S}^{n o r m}(s)$ . Uncertainty is $U_{L C+C S}(s)=U_{L C}^{n o r m}(s)+\\dot{U}_{C S}^{n o r m}(s)$ .   \n\u2022 $\\mathbf{LC}{+}\\mathbf{BB}$ : Combine LC and BB. Compute uncertainty as: $U_{L C+B B}(s)=U_{L C}^{n o r m}(s)+U_{B B}^{n o r m}(s)$ . ", "page_idx": 8}, {"type": "text", "text": "We set query batch size $b=2k$ ( $k$ is the number of classes) and maximum labeling budget to $20b$ . We evaluate node selection strategy performance with annotator labeling accuracy at $70\\%$ , $80\\%$ , $90\\%$ , and $100\\%$ . To reduce randomness in results, we repeat each method ten times (re-partitioning datasets and changing random seeds) and record the mean of test set inference accuracies. To demonstrate labeling budget impact, we conduct experiments evaluating various node selection strategies under different labeling budgets on four datasets, varying the budget from $2b$ to 20b. ", "page_idx": 8}, {"type": "text", "text": "As shown in Figures 5 and 6, whether using BP or NETCONF for inference, the test accuracy of the LinUProp-based node selection strategy (BB) and its variations $(\\mathrm{LC}+\\mathrm{BB})$ ) generally grow faster than other baselines as the budget increases. This is especially the case when compared to the native UQ method in NETCONF (CS) and its variations $(\\mathrm{LC}+\\mathrm{CS})$ ). As shown in Figure 6, strategies based on NETCONF $\\mathrm{CS/LC+CS)}$ ) prioritize labeling low-degree nodes due to their inherent assumption that neighbors necessarily reduce the uncertainty. As a result, nodes with many neighbors are often mistakenly viewed as having low uncertainty and are left unlabeled, which further leads to high uncertainty in the majority of nodes. This phenomenon is more pronounced in graphs with a large number of low-degree nodes, like PolBlogs, leading to a very slow increase in accuracy. Experiments with a lower labeling accuracy yielded similar conclusions, as shown in Appendix B.3.1. ", "page_idx": 8}, {"type": "text", "text": "We also evaluate all strategies under a fixed labeling budget of $20b$ for a fair comparison, with results displayed in Appendix B.3.2. The conclusion is that whether using BP or NETCONF for inference, ", "page_idx": 8}, {"type": "text", "text": "BB and $\\mathrm{LC}{+}\\mathrm{BB}$ outperform baselines across different labeling accuracies and datasets in most cases. From the results in active learning tasks, we see node selection strategies guided by LinUProp\u2019s UQ results achieves higher labeling accuracy with lower labeling budget. This indicates that uncertainty quantified by LinUProp is accurate, effective, and insensitive to the inference method. ", "page_idx": 9}, {"type": "text", "text": "6 Related Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Uncertainty in posterior belief. In addition to traditional UQ methods like MC simulations, which require extensive sampling, some research focuses on modeling uncertainty in posterior belief by deriving closed-form solutions to incorporate uncertainty into inference results. Existing methods [7, 36, 35] exhibit scalability but limited as they assume uniform potential functions across all edges. Furthermore, these methods with closed-form solutions assume that any neighbor of a node will necessarily reduce the uncertainty even neighbors affected by noise or lacking information, potentially leading to uncertainty underestimation. Moreover, few existing studies have theoretically linked calculated uncertainty to the expected model prediction error, making it difficult for decision-makers to understand and trust the UQ results. ", "page_idx": 9}, {"type": "text", "text": "Existing works [22, 20] utilize bound propagation without the aforementioned assumptions regarding neighbors. However, they focus on quantifying the error between the posterior beliefs and the true marginal probabilities of the variable nodes, rather than quantifying uncertainty. This fundamentally differs from LinUProp, which quantifies the posterior uncertainty as the generalized variance component of the expected prediction error. ", "page_idx": 9}, {"type": "text", "text": "Human understanding of PGM inference. Humans are unlikely to adopt inference outcomes without reasonable interpretation [33]. In [24, 32], the authors studied explainable Bayesian networks. Recently, explanations of inference on Bayesian network and MRF were found by differentiation [3, 5], so that a set of important network parameters (potentials) can explain the changes in the inferred posterior distribution of a target variable. Interpretable graphical models are also studied under the hood of topic models [4] or GNN [38]. None of the aforementioned studies can provide an explanation for the uncertainty in the inference results. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we proposed LinUProp, a UQ method for graphical model inference that utilizes a novel linear propagation of uncertainty to model uncertainty among related nodes additively. LinUProp provides linear scalability, guaranteed convergence, and is interpretable. Unlike its competitors, LinUProp does not assume neighbors necessarily reduce uncertainty and thus avoids uncertainty underestimation. To gain deeper insights, we decompose the expected prediction error of the graphical model and prove that the uncertainty computed by LinUProp is the generalized variance component of the decomposition. Experimental analysis shows LinUProp possesses aforementioned properties and outperforms competitors in downstream tasks. However, the study has not yet explored the interpretability of uncertainty with human involvement in real-world decision-making processes, an area we aim to address in future research. We would also like to apply LinUProp to more applications of graphical models to demonstrate its utility in the future. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Chenghua Guo and Xi Zhang were supported by the Natural Science Foundation of China (No. 62372057). This material is based upon work supported by the National Science Foundation under Grant Number 2008155. Sihong Xie was supported in part by the National Key R&D Program of China (Grant No. 2023YFF0725001), the Guangzhou-HKUST(GZ) Joint Funding Program (Grant No. 2023A03J0008), and Education Bureau of Guangzhou Municipality. Qi Li was supported in part by the National Science Foundation under NSF Grants IIS 2007941. Chao Chen was supported by the National Key Research and Development Program of China (No. 2023YFB3106504), and Pengcheng-China Mobile Jointly Funded Project (No. 2024ZY2B0050). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] L. A. Adamic and N. Glance. The political blogosphere and the 2004 us election: divided they blog. In Proceedings of the 3rd international workshop on Link discovery, pages 36\u201343, 2005.   \n[2] C. M. Bishop and N. M. Nasrabadi. Pattern recognition and machine learning. Springer, 2006.   \n[3] H. Chan and A. Darwiche. Sensitivity analysis in markov networks. In IJCAI, 2005. [4] J. Chang, J. Boyd-Graber, C. Wang, S. Gerrish, and D. M. Blei. Reading tea leaves: How humans interpret topic models. In NeurIPS, 2009.   \n[5] A. Darwiche. A differential approach to inference in bayesian networks. Journal of the ACM (JACM), 2003.   \n[6] H. Dawood. Theories of interval arithmetic: mathematical foundations and applications. LAP Lambert Academic Publishing, 2011.   \n[7] D. Eswaran, S. G\u00fcnnemann, and C. Faloutsos. The power of certainty: A dirichlet-multinomial model for belief propagation. In SDM. SIAM, 2017.   \n[8] D. Eswaran, S. G\u00fcnnemann, C. Faloutsos, D. Makhija, and M. Kumar. Zoobp: Belief propagation for heterogeneous networks. Proceedings of the VLDB Endowment, 10(5):625\u2013636, 2017.   \n[9] A. Farasat, A. Nikolaev, S. N. Srihari, and R. H. Blair. Probabilistic graphical models in modern social network analysis. Social Network Analysis and Mining, 5:1\u201318, 2015.   \n[10] H. Gao, J. Pei, and H. Huang. Conditional random field enhanced graph convolutional neural networks. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 276\u2013284, 2019.   \n[11] W. Gatterbauer, S. G\u00fcnnemann, D. Koutra, and C. Faloutsos. Linearized and single-pass belief propagation. Proceedings of the VLDB Endowment, 2015.   \n[12] S. Geman and D. Geman. Stochastic relaxation, gibbs distributions, and the bayesian restoration of images. IEEE Transactions on pattern analysis and machine intelligence, (6):721\u2013741, 1984.   \n[13] W. R. Gilks, S. Richardson, and D. Spiegelhalter. Markov chain Monte Carlo in practice. CRC press, 1995.   \n[14] J. Ha, S.-H. Kwon, S.-W. Kim, C. Faloutsos, and S. Park. Top-n recommendation through belief propagation. In Proceedings of the 21st ACM international conference on Information and knowledge management, pages 2343\u20132346, 2012.   \n[15] Z. Hu, G. Wu, S. Mitra, R. Zhang, T. Sun, H. Huang, and V. Swaminathan. Token-level adversarial prompt detection based on perplexity measures and contextual information. arXiv preprint arXiv:2311.11509, 2023.   \n[16] Y. Jin, M. Carman, Y. Zhu, and Y. Xiang. A technical survey on statistical modelling and design methods for crowdsourcing quality control. Artificial Intelligence, 2020.   \n[17] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for graphical models. Machine learning, 37:183\u2013233, 1999.   \n[18] T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. In ICML, 2017.   \n[19] S. L. Lauritzen and D. J. Spiegelhalter. Local computations with probabilities on graphical structures and their application to expert systems. Journal of the Royal Statistical Society: Series B (Methodological), 50(2):157\u2013194, 1988.   \n[20] M. Leisink and B. Kappen. Bound propagation. JAIR, 2003.   \n[21] F. Lindsten, J. Helske, and M. Vihola. Graphical model inference: Sequential monte carlo meets deterministic approximations. Advances in Neural Information Processing Systems, 31, 2018. ", "page_idx": 10}, {"type": "text", "text": "[22] J. M. Mooij and H. Kappen. Bounds on marginal probability distributions. NeurIPS, 2008. ", "page_idx": 11}, {"type": "text", "text": "[23] V.-A. Nguyen, P. Shi, J. Ramakrishnan, N. Torabi, N. S. Arora, U. Weinsberg, and M. Tingley. Crowdsourcing with contextual uncertainty. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD \u201922, page 3645\u20133655, New York, NY, USA, 2022. Association for Computing Machinery.   \n[24] S. W. Norton. An explanation mechanism for bayesian inferencing systems. In UAI, 1986.   \n[25] J. Ok, S. Oh, J. Shin, and Y. Yi. Optimality of belief propagation for crowdsourced classification. In ICML, 2016.   \n[26] S. Pandit, D. H. Chau, S. Wang, and C. Faloutsos. Netprobe: a fast and scalable system for fraud detection in online auction networks. In WWW, 2007.   \n[27] J. Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan kaufmann, 1988.   \n[28] M. Qu, Y. Bengio, and J. Tang. Gmnn: Graph markov neural networks. In International conference on machine learning, pages 5241\u20135250. PMLR, 2019.   \n[29] Y. Saad. Iterative methods for sparse linear systems. SIAM, 2003.   \n[30] S. S. Sawilowsky. You think you\u2019ve got trivials? Journal of Modern Applied Statistical Methods, 2:218\u2013225, 2003.   \n[31] V. S. Sheng, F. Provost, and P. G. Ipeirotis. Get another label? improving data quality and data mining using multiple, noisy labelers. In SIGKDD, 2008.   \n[32] H. J. Suermondt. Explanation in Bayesian belief networks. Stanford University, 1992.   \n[33] R. L. Teach and E. H. Shortliffe. An analysis of physician attitudes regarding computer-based clinical consultation systems. Computers and Biomedical Research, 1981.   \n[34] R. Tutunov, A. Grosnit, J. Ziomek, J. Wang, and H. Bou-Ammar. Why can large language models generate correct chain-of-thoughts? arXiv preprint arXiv:2310.13571, 2023.   \n[35] Y. Yamaguchi, C. Faloutsos, and H. Kitagawa. Socnl: Bayesian label propagation with confidence. In Advances in Knowledge Discovery and Data Mining: 19th Pacific-Asia Conference, PAKDD 2015, Ho Chi Minh City, Vietnam, May 19-22, 2015, Proceedings, Part I 19, pages 633\u2013645. Springer, 2015.   \n[36] Y. Yamaguchi, C. Faloutsos, and H. Kitagawa. Camlp: Confidence-aware modulated label propagation. In Proceedings of the 2016 SIAM International Conference on Data Mining, pages 513\u2013521. SIAM, 2016.   \n[37] X. Yang, Y. Guo, and Y. Liu. Bayesian-inference-based recommendation in online social networks. IEEE Transactions on Parallel and Distributed Systems, 24(4):642\u2013651, 2012.   \n[38] Z. Ying, D. Bourgeois, J. You, M. Zitnik, and J. Leskovec. Gnnexplainer: Generating explanations for graph neural networks. In NeurIPS. 2019.   \n[39] J. Yoo, S. Jo, and U. Kang. Supervised belief propagation: Scalable supervised inference on attributed networks. In 2017 IEEE International Conference on Data Mining (ICDM), pages 595\u2013604. IEEE, 2017.   \n[40] W. Zhang, Y. Wang, Z. You, M. Cao, P. Huang, J. Shan, Z. Yang, and B. CUI. Information gain propagation: a new way to graph active learning with soft labels. In ICLR, 2022.   \n[41] X. Zhou, H. Liu, F. Pourpanah, T. Zeng, and X. Wang. A survey on epistemic (model) uncertainty in supervised learning: Recent advances and applications. Neurocomputing, 489:449\u2013465, 2022. ", "page_idx": 11}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Let vec denotes the operation that vertically concatenates the rows of a given matrix into a single column vector, and let Diag denote the transformation of an $n k\\times k$ block matrix into a block diagonal matrix. $\\mathbb{E}=[\\Phi_{1},\\Phi_{2},\\cdot\\cdot\\cdot\\,,\\Phi_{n}]^{T}$ and $\\mathbb{B}=[\\mathbf{b}_{1},\\mathbf{b}_{2},\\cdot\\cdot\\cdot,\\mathbf{b}_{n}]^{T}$ respectively represent the uncertainties of prior and posterior beliefs for all $n$ nodes, each of dimension $n\\times k$ . $\\mathbf{Q}$ is an $n k\\times k$ matrix formed by vertically stacking $n$ identity matrices, each of size $k\\times k$ . $\\Psi_{1}$ and $\\Psi_{2}$ are $n k\\times n k$ block matrices formed from $\\hat{{\\bf{H}}}_{s t}^{'}$ and $\\hat{\\mathbf{H}}_{s t}^{'2}$ , respectively. For any given edge $(s,t),\\hat{\\mathbf{H}}_{s t}^{'}$ denotes the centralized compatibility matrix corresponding to that edge, with entries $\\hat{\\mathbf{H}}_{s t}^{'}$ defined as $\\begin{array}{r}{\\hat{H}_{s t}^{'}(i,j)=|H_{s t}(i,j)-\\frac{1}{k}|}\\end{array}$ . In cases where the edge $(s,t)$ does not exist, the matrix defaults to zero. ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathrm{Diag}\\left(\\left[\\begin{array}{l}{\\mathbf{L_{1}}}\\\\ {\\vdots}\\\\ {\\mathbf{L_{n}}}\\end{array}\\right]\\right)=\\left[\\begin{array}{l l l}{\\mathbf{L_{1}}}&&\\\\ &{\\ddots}&\\\\ &&{\\mathbf{L_{n}}}\\end{array}\\right]\\quad\\Psi_{1}^{\\prime}=\\left[\\begin{array}{l l l}{\\hat{\\mathbf{H}}_{11}^{\\prime}}&{\\cdots}&{\\hat{\\mathbf{H}}_{1n}^{\\prime}}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {\\hat{\\mathbf{H}}_{n1}^{\\prime}}&{\\cdots}&{\\hat{\\mathbf{H}}_{n n}^{\\prime}}\\end{array}\\right]\\quad\\Psi_{2}^{\\prime}=\\left[\\begin{array}{l l l}{\\hat{\\mathbf{H}}_{11}^{\\prime}}&{\\cdots}&{\\hat{\\mathbf{H}}_{1n}^{\\prime2}}&{\\cdots}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {\\hat{\\mathbf{H}}_{n1}^{\\prime2}}&{\\cdots}&{\\hat{\\mathbf{H}}_{n n}^{\\prime2}}&{\\vdots}\\end{array}\\right]\\quad\\hat{\\mathbf{E}}_{2}^{\\prime}=\\left[\\begin{array}{l l l}{\\hat{\\mathbf{H}}_{11}^{\\prime2}}&{\\cdots}&{\\hat{\\mathbf{H}}_{1n}^{\\prime2}}&{\\hat{\\mathbf{H}}_{2n}^{\\prime2}}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {\\hat{\\mathbf{H}}_{n1}^{\\prime2}}&{\\cdots}&{\\hat{\\mathbf{H}}_{n n}^{\\prime2}}\\end{array}\\right],\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Theorem 3.1 (LinUProp). For a multi-class node classification task on an MRF, given the matrix $\\mathbb{E}$ which represents the prior belief uncertainty of all nodes, matrices $\\Psi_{1}^{'}$ and $\\Psi_{2}^{'}$ which denote the dependencies among these nodes. The posterior belief uncertainty for all nodes, represented by $\\mathbb{B}$ and in terms of interval widths, is determined by the linear equation system: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname{vec}(\\mathbb{B})=\\operatorname{vec}(\\mathbb{E})+\\left(\\Psi_{1}^{'}+\\operatorname{Diag}\\left(\\Psi_{2}^{'}\\mathbf{Q}\\right)\\right)\\operatorname{vec}(\\mathbb{B}).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof. Based on the linear approximation of BP messages defined in Eq. (4), we can approximate the lower and upper bounds of BP messages using interval arithmetic rules [6]: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{m}_{t s}^{-}\\left(i\\right)\\leftarrow k\\displaystyle\\sum_{j}\\hat{H}_{s t}\\left(i,j\\right)\\hat{b}_{t\\setminus s}^{(1)}\\left(j\\right),}\\\\ {\\hat{m}_{t s}^{+}\\left(i\\right)\\leftarrow k\\displaystyle\\sum_{j}\\hat{H}_{s t}\\left(i,j\\right)\\hat{b}_{t\\setminus s}^{(2)}\\left(j\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "equation", "text": "$$\n\\hat{b}_{t\\backslash s}^{(1)}\\left(j\\right)=\\left\\{\\begin{array}{l l}{\\hat{b}_{t}^{-}\\left(j\\right)-\\frac{1}{k}\\hat{m}_{s t}^{+}\\left(j\\right),}&{\\hat{H}\\left(i,j\\right)>0,}\\\\ {\\hat{b}_{t}^{+}\\left(j\\right)-\\frac{1}{k}\\hat{m}_{s t}^{-}\\left(j\\right),}&{\\hat{H}\\left(i,j\\right)<0,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "equation", "text": "$$\n\\hat{b}_{t\\backslash s}^{(2)}\\left(j\\right)=\\left\\{\\begin{array}{l l}{\\hat{b}_{t}^{-}\\left(j\\right)-\\frac{1}{k}\\hat{m}_{s t}^{+}\\left(j\\right),}&{\\hat{H}\\left(i,j\\right)<0,}\\\\ {\\hat{b}_{t}^{+}\\left(j\\right)-\\frac{1}{k}\\hat{m}_{s t}^{-}\\left(j\\right),}&{\\hat{H}\\left(i,j\\right)>0.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "The idea behind Eqs. (16-19) is that the determination of the lower and upper bounds of $\\hat{m}_{t s}(i)$ is contingent upon the sign of $\\hat{H}_{s t}(i,j)$ . Specifically, when determining the lower bound of $\\hat{m}_{t s}(i)$ , if $\\hat{H}_{s t}(i,j)<0$ , then $\\hat{b}_{t\\backslash s}(j)$ should be maximized, i.e., taking its upper bound $\\hat{b}_{t}^{+}\\left(j\\right)-\\frac{1}{k}\\hat{m}_{s t}^{-}\\left(j\\right)$ ; if $\\hat{H}_{s t}(i,j)>0$ , then $\\hat{b}_{t\\backslash s}(j)$ should be minimized, i.e., taking its lower bound $\\hat{b}_{t}^{-}\\left(j\\right)-\\frac{1}{k}\\hat{m}_{s t}^{+}\\left(j\\right)$ . A similar idea applies to the computation of the upper bound of $\\hat{m}_{t s}(i)$ . Then by subtracting Eq. (16) from Eq. (17), we obtain the interval width $\\mathrm{m}_{t s}(i)$ to reflect the uncertainty of $\\hat{m}_{t s}(i)$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathrm{m}_{t s}\\left(i\\right)\\leftarrow k\\sum_{j}\\vert\\hat{H}_{s t}\\left(i,j\\right)\\vert\\biggl(\\mathbb{b}_{t}(j)+\\frac{1}{k}\\mathrm{m}_{s t}(j)\\biggr),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\mathbf{b}_{t}$ is the interval width vector reflecting the posterior belief uncertainty at node $t$ , as defined in the \u201cProblem Definitions\u201d of Sec. 2. Eq. (20) leaves only the interval width, making the specific location irrelevant for subsequent steps. Thus, LinUProp is also unaffected by the exact location of the interval. Similarly, the uncertainty of the message in the opposite direction can be quantified by interval width as: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathrm{m}_{s t}\\left(i\\right)\\leftarrow k\\sum_{j}\\vert\\hat{H}_{s t}\\left(i,j\\right)\\vert\\left(\\mathbb{b}_{s}(j)+\\frac{1}{k}\\mathrm{m}_{t s}(j)\\right).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Substituting Eq. (21) into Eq. (20) leads to ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{m}_{t s}\\left(i\\right)\\leftarrow\\ k\\sum_{j}\\vert\\hat{H}_{s t}\\left(i,j\\right)\\vert\\Bigl(\\mathbb{b}_{t}(j)+\\sum_{g}\\vert\\hat{H}_{s t}\\left(g,j\\right)\\vert\\bigl(\\mathbb{b}_{s}(g)+\\frac{1}{k}\\mathrm{m}_{t s}(g)\\bigr)\\Bigr).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "When the algorithm converges, all messages are at a stable state, so we can treat $\\mathbf{m}_{t s}$ on both sides of Eq. (22) as equal and replace the update formula with the equation: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathrm{m}_{t s}\\left(i\\right)-\\sum_{j}\\left|\\hat{H}_{s t}\\left(i,j\\right)\\right|\\sum_{g}\\left|\\hat{H}_{s t}\\left(g,j\\right)\\right|\\mathrm{m}_{t s}(g)}}\\\\ {{\\displaystyle=k\\sum_{j}\\left|\\hat{H}_{s t}\\left(i,j\\right)|\\mathbb{b}_{t}(j)+k\\sum_{j}\\left|\\hat{H}_{s t}\\left(i,j\\right)\\right|\\sum_{g}\\left|\\hat{H}_{s t}\\left(g,j\\right)\\right|\\mathbb{b}_{s}(g).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Let us denote $\\begin{array}{r}{|H_{s t}(i,j)-\\frac{1}{k}|}\\end{array}$ as $\\hat{H}_{s t}^{'}(i,j)$ , then the stable state message bound can be simplified as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\mathbf{I}-\\hat{\\mathbf{H}}_{s t}^{'2})\\mathbf{m}_{t s}=k\\hat{\\mathbf{H}}_{s t}^{'}\\mathbb{b}_{t}+k\\hat{\\mathbf{H}}_{s t}^{'2}\\mathbb{b}_{s}\\qquad\\qquad\\qquad}\\\\ {\\mathbf{m}_{t s}=k(\\mathbf{I}-\\hat{\\mathbf{H}}_{s t}^{'2})^{-1}\\hat{\\mathbf{H}}_{s t}^{'}(\\mathbb{b}_{t}+\\hat{\\mathbf{H}}_{s t}^{'}\\mathbb{b}_{s}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the message bound width, denoted as $\\mathbf{m}_{t s}$ , and the posterior belief bound width of nodes $s$ and $t$ , denoted as $\\mathbf{b}_{s}$ and $\\mathbf{b}_{t}$ respectively, are all $k$ -dimensional vectors. Then the interval width $\\mathbf{b}_{s}$ reflecting the uncertainty of posterior belief $\\hat{\\mathbf{b}}_{s}$ can be calculated by subtracting its lower bound $\\begin{array}{r}{\\hat{\\mathbf{b}}_{s}^{-}=\\hat{\\mathbf{e}}_{s}^{-}+\\frac{1}{k}\\sum_{t\\in\\mathcal{N}(s)}\\hat{\\mathbf{m}}_{t s}^{-}}\\end{array}$ from its upper bound $\\begin{array}{r}{\\hat{\\mathbf{b}}_{s}^{+}=\\hat{\\mathbf{e}}_{s}^{+}+\\frac{1}{k}\\sum_{t\\in\\mathcal{N}(s)}\\hat{\\mathbf{m}}_{t s}^{+}}\\end{array}$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n{\\mathbb b}_{s}={\\mathbb e}_{s}+\\frac{1}{k}\\sum_{t\\in\\mathcal{N}(s)}{\\bf m}_{t s},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\Phi_{s}$ is the prior bound width vector with $k$ -dimensional. Then plugging Eq. (24) into Eq. (25): ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{b}_{s}=\\mathbb{e}_{s}+\\sum_{t\\in\\mathcal{N}(s)}(\\mathbf{I}-\\hat{\\mathbf{H}}_{s t}^{'2})^{-1}\\hat{\\mathbf{H}}_{s t}^{'}\\mathbb{b}_{t}+\\sum_{t\\in\\mathcal{N}(s)}(\\mathbf{I}-\\hat{\\mathbf{H}}_{s t}^{'2})^{-1}\\hat{\\mathbf{H}}_{s t}^{'2}\\mathbb{b}_{s}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Due to $\\hat{\\mathbf{H}}_{s t}^{'}$ being the centralized matrix, $(\\mathbf{I}-\\hat{\\mathbf{H}}_{s t}^{'2})\\approx\\mathbf{I}$ , then $(\\mathbf{I}-\\hat{\\mathbf{H}}_{s t}^{'2})^{-1}\\hat{\\mathbf{H}}_{s t}^{'}\\approx\\hat{\\mathbf{H}}_{s t}^{'}$ and we get ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{b}_{s}=\\boldsymbol{\\Phi}_{s}+\\sum_{t\\in\\mathcal{N}(s)}\\hat{\\mathbf{H}}_{s t}^{'}\\mathbb{b}_{t}+\\sum_{t\\in\\mathcal{N}(s)}\\hat{\\mathbf{H}}_{s t}^{'2}\\mathbb{b}_{s}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "By using the matrices introduced at the outset of this section, Eq. (26) can be rewritten as Eq. (6). ", "page_idx": 13}, {"type": "text", "text": "A.2 Proof of Eq. (13) ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proof. Based on the linear approximation of BP messages (Eq. (4)), we can similarly get $\\hat{\\mathbf{m}}_{s t}$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\hat{m}_{s t}\\left(i\\right)\\gets k\\sum_{j=1}^{k}\\hat{H}_{s t}\\left(i,j\\right)\\bigg(\\hat{b}_{s}\\left(j\\right)-\\frac{1}{k}\\hat{m}_{t s}\\left(j\\right)\\bigg).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Substituting Eq. (27) into Eq. (4) leads to ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\hat{m}_{t s}\\left(i\\right)\\leftarrow\\ k\\sum_{j}\\hat{H}_{s t}\\left(i,j\\right)\\left(\\hat{b}_{t}(j)-\\sum_{g}\\hat{H}_{s t}\\left(g,j\\right)\\left(\\hat{b}_{s}(g)-\\frac{1}{k}\\hat{m}_{t s}(g)\\right)\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "When the algorithm converges, all messages are at a stable state, so we can treat $\\hat{\\mathbf{m}}_{t s}$ on both sides of Eq. (28) as equal and replace the update formula with the equation: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\hat{m}_{t s}\\left(i\\right)-\\sum_{j}\\hat{H}_{s t}\\left(i,j\\right)\\sum_{g}\\hat{H}_{s t}\\left(g,j\\right)\\hat{m}_{t s}(g)}}\\\\ {{\\displaystyle=k\\sum_{j}\\hat{H}_{s t}\\left(i,j\\right)\\hat{b}_{t}(j)-k\\sum_{j}\\hat{H}_{s t}\\left(i,j\\right)\\sum_{g}\\hat{H}_{s t}\\left(g,j\\right)\\hat{b}_{s}(g).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This stable state message can be simplified as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\mathbf{I}-\\hat{\\mathbf{H}}_{s t}^{2})\\hat{\\mathbf{m}}_{t s}=k\\hat{\\mathbf{H}}_{s t}\\hat{\\mathbf{b}}_{t}-k\\hat{\\mathbf{H}}_{s t}^{2}\\hat{\\mathbf{b}}_{s}\\qquad\\qquad}\\\\ {\\hat{\\mathbf{m}}_{t s}=k(\\mathbf{I}-\\hat{\\mathbf{H}}_{s t}^{2})^{-1}\\hat{\\mathbf{H}}_{s t}(\\hat{\\mathbf{b}}_{t}-\\hat{\\mathbf{H}}_{s t}\\hat{\\mathbf{b}}_{s}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then plugging Eq. (30) into Eq. (5): ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{b}}_{s}=\\hat{\\mathbf{e}}_{s}+\\sum_{t\\in\\mathcal{N}(s)}(\\mathbf{I}-\\hat{\\mathbf{H}}_{s t}^{2})^{-1}\\hat{\\mathbf{H}}_{s t}\\hat{\\mathbf{b}}_{t}+\\sum_{t\\in\\mathcal{N}(s)}(\\mathbf{I}-\\hat{\\mathbf{H}}_{s t}^{2})^{-1}\\hat{\\mathbf{H}}_{s t}\\hat{\\mathbf{b}}_{s}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Due to $\\hat{{\\mathbf{H}}}_{s t}$ being the centralized matrix, $(\\mathbf{I}-\\hat{\\mathbf{H}}_{s t}^{2})\\approx\\mathbf{I}$ , then $({\\bf{I}}-{\\hat{{\\bf{H}}}}_{s t}^{2})^{-1}{\\hat{{\\bf{H}}}}_{s t}\\approx{\\hat{{\\bf{H}}}}_{s t}$ and we get ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{b}}_{s}=\\hat{\\mathbf{e}}_{s}+\\sum_{t\\in\\mathcal{N}(s)}\\hat{\\mathbf{H}}_{s t}\\hat{\\mathbf{b}}_{t}+\\sum_{t\\in\\mathcal{N}(s)}\\hat{\\mathbf{H}}_{s t}^{2}\\hat{\\mathbf{b}}_{s}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By using $\\Psi_{1}$ and $\\Psi_{2}$ introduced at the outset of Sec. 4.2, Eq. (31) can be rewritten as Eq. (13). ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Psi_{1}=\\left[\\begin{array}{c c c}{\\hat{\\mathbf{H}}_{11}}&{\\cdots}&{\\hat{\\mathbf{H}}_{1n}}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {\\hat{\\mathbf{H}}_{n1}}&{\\cdots}&{\\hat{\\mathbf{H}}_{n n}}\\end{array}\\right]\\quad\\Psi_{2}=\\left[\\begin{array}{c c c}{\\hat{\\mathbf{H}}_{11}^{2}}&{\\cdots}&{\\hat{\\mathbf{H}}_{1n}^{2}}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {\\hat{\\mathbf{H}}_{n1}^{2}}&{\\cdots}&{\\hat{\\mathbf{H}}_{n n}^{2}}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A.3 Proof of the Linear Scalability of LinUProp ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. To demonstrate the linear scalability of $L i n U P r o p$ , we will start with the time complexity analysis of the iterative version of LinUProp (Eq. (7)): ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname{vec}(\\mathbb{B})^{(l+1)}=\\operatorname{vec}(\\mathbb{E})+\\left(\\Psi_{1}^{'}+\\operatorname{Diag}\\left(\\Psi_{2}^{'}\\mathbf{Q}\\right)\\right)\\operatorname{vec}(\\mathbb{B})^{(l)}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Before the iteration starts, $\\operatorname{vec}(\\mathbb{E})$ and $\\Psi_{1}^{'}+\\mathrm{Diag}(\\Psi_{2}^{'}\\mathbf{Q})$ (denoted as $\\mathbf{T}$ ) are known and fixed as input. In the $l$ -th iteration, we need to compute the matrix-vector multiplication $\\mathbf{T}\\cdot\\mathbf{vec}(\\mathbb{B})^{(l)}$ and then add the result to $\\operatorname{vec}(\\mathbb{E})$ . Since $\\mathbf{T}$ is a block sparse matrix, we can use sparse matrix operations to simplify the computation. The number of non-zero elements in $\\mathbf{T}$ is $k^{2}\\ast(|\\mathcal{E}|+|\\mathcal{V}|)$ , where $\\vert\\mathcal{E}\\vert$ is the number of edges, $\\vert\\protect\\nu\\vert$ is the number of nodes and $k$ is the number of node classes. Then the time complexity of $\\mathbf{T}\\cdot\\mathbf{vec}(\\mathbb{B})^{(l)}$ is $\\mathcal{O}(k^{2}*(|\\mathcal{E}|+|\\mathcal{V}|))$ . Adding this result to $\\operatorname{vec}(\\mathbb{E})$ has a complexity of $\\mathcal{O}(k|\\mathcal{V}|)$ . Therefore, the time complexity of each iteration of LinUProp is $\\mathcal{O}(k^{2}(\\lvert\\mathcal{E}\\rvert+\\lvert\\mathcal{V}\\rvert)+k\\lvert\\mathcal{V}\\rvert)$ . From this, we can conclude that the time complexity is $\\mathcal{O}(|\\mathcal{V}|)$ when $|\\gamma|>|\\mathcal{E}|$ , otherwise $\\mathcal{O}(\\vert\\mathcal{E}\\vert)$ , which reflects the linear scalability of LinUProp. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "A.4 Proof of Eq. 15 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In order to expand the variance term, we first apply the property of variance, which states that the variance of a random variable is equal to the difference between the expected value of its square and the square of its expected value. Next, we substitute the linear function $\\mathrm{vec}(\\hat{\\mathbf{B}})_{v}=\\mathbf{F}_{v}\\mathrm{vec}(\\hat{\\mathbf{E}})$ into the equation and appropriately expand the squared terms. Following this, we manipulate and simplify the terms using the properties of expectation and matrix operations. At this point, we factor out the linear function\u2019s coefficient vector ${\\bf F}_{v}$ . The resulting expression includes the covariance matrix of the random vector, denoted as $\\Sigma_{\\mathrm{vec}(\\hat{\\mathbf{E}})}$ . Finally, we present the concise form of the expression, which demonstrates the linear relationship between the variance term of each node and the prior covariances of all nodes. The detailed proof steps are provided below: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left(\\mathrm{vec}(\\hat{\\mathbf{B}})_{v}-\\mathbb{E}|\\mathrm{vec}(\\hat{\\mathbf{B}})_{v}|\\right)^{2}\\right]}\\\\ &{=\\mathbb{E}\\left[\\left(\\mathrm{vec}(\\hat{\\mathbf{B}})_{v}\\right)^{2}\\right]-\\left(\\mathbb{E}\\left[\\mathrm{vec}(\\hat{\\mathbf{B}})_{v}\\right]\\right)^{2}}\\\\ &{=\\mathbb{E}\\left[\\left(\\mathrm{F}_{v}\\mathrm{vec}(\\hat{\\mathbf{E}})\\right)^{2}\\right]-\\left(\\mathbb{E}\\left[\\mathrm{F}_{v}\\mathrm{vec}(\\hat{\\mathbf{E}})\\right]\\right)^{2}}\\\\ &{=\\mathbb{E}\\left[\\mathrm{F}_{v}\\mathrm{vec}(\\hat{\\mathbf{E}})\\mathrm{vec}(\\hat{\\mathbf{E}})^{T}\\mathbf{F}_{v}^{T}\\right]-\\mathbb{E}\\left[\\mathrm{F}_{v}\\mathrm{vec}(\\hat{\\mathbf{E}})\\right]\\left(\\mathbb{E}\\left[\\mathrm{F}_{v}\\mathrm{vec}(\\hat{\\mathbf{E}})\\right]\\right)^{T}}\\\\ &{=\\mathbb{F}_{v}\\mathbb{E}\\left[\\mathrm{vec}(\\hat{\\mathbf{E}})\\mathrm{vec}(\\hat{\\mathbf{E}})^{T}\\right]\\mathbf{F}_{v}^{T}-\\mathbb{F}_{v}\\mathbb{E}\\left[\\mathrm{vec}(\\hat{\\mathbf{E}})\\right]\\left(\\mathbb{E}\\left[\\mathrm{vec}(\\hat{\\mathbf{E}})\\right]\\right)^{T}\\mathbf{F}_{v}^{T}}\\\\ &{=\\mathbf{F}_{v}\\left(\\mathbb{E}\\left[\\mathrm{vec}(\\hat{\\mathbf{E}})\\mathrm{vec}(\\hat{\\mathbf{E}})^{T}\\right]-\\mathbb{E}\\left[\\mathrm{vec}(\\hat{\\mathbf{E}})\\right]\\left(\\mathbb{E}\\left[\\mathrm{vec}(\\hat{\\mathbf{E}})\\right]\\right)^{T}\\right)\\mathbf{F}_{v}^{T}=\\mathbf{F}_{v}\\mathrm{Z}_{v<\\hat{\\mathbf{E}})}\\mathbf{F}_{v}^{T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "B Additional Details and Results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Quantitative Validation of Correctness on a ${\\bf4}\\!\\times\\!{\\bf4}$ Grid ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We also quantitatively verify the correctness of LinUProp by comparing it with the uncertainty computed by MC simulations on a $4\\times4$ grid. Specifically, we first sample prior beliefs based on the prior distributions of each node, and then run BP to infer the posterior beliefs of each node. After repeating the above sampling process for 100,000 times, we use the empirical standard deviation of the posterior belief of each node sampled as an estimation of the ground truth uncertainty. ", "page_idx": 15}, {"type": "text", "text": "Figure 7 shows that the uncertainties quantified by MC simulation and LinUProp have a strong positive correlation, as evidenced by a Pearson Correlation Coefficient (PCC) of 0.9084 between the uncertainties obtained for all nodes using both approaches. This confirms the consistency of the uncertainty in the beliefs quantified by our method with that of MC simulation. ", "page_idx": 15}, {"type": "image", "img_path": "XOVks7JHQA/tmp/745911a469cfffbfcf4ff4dd21ff70113b9e42d7ab7fe7ec738448db0cb88d06.jpg", "img_caption": ["Figure 7: Correctness. The two axes represent two UQ methods: the $\\mathbf{X}{\\cdot}$ -axis is the empirical standard deviation of the beliefs obtained by MC simulation (an estimation of the ground truth uncertainty), and the y-axis is the belief bound width computed by LinUProp. In fact, there are 16 points in this figure, corresponding to the 16 variable nodes in Fig. 3(a). Some points appear to overlap because they have similar uncertainties quantified by both methods. The Pearson Correlation Coefficient (PCC) between the uncertainties quantified by the two methods is 0.9084, indicating a strong positive correlation. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "B.2 More Experimental Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Running environment. We conducted convergence and scalability experiments on the Apple M2 chip, and convergence experiments on a $2.2\\:\\mathrm{GHz}$ Intel Xeon CPU. ", "page_idx": 15}, {"type": "text", "text": "Datasets. Table 1 shows the statistical information and partitioning of three citation network datasets. ", "page_idx": 15}, {"type": "text", "text": "Table 1: Statistical information and partitioning of datasets. The subsets, $\\nu_{t r a i n}$ , $\\nu_{v a l}$ and $\\nu_{t e s t}$ are sampled from the original node set. The remaining nodes are in $\\nu_{u l p}$ . We use these subsets in the effectiveness experiments. ", "page_idx": 15}, {"type": "table", "img_path": "XOVks7JHQA/tmp/6fc3d6b039db814ff44ce864f9b7dc9d0aa950846efa459e7b38edfdd2789624.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "XOVks7JHQA/tmp/8539a1ad299cd5fd4ea24b7707edde6161e501a0cd70942a65d6cbe62111f593.jpg", "img_caption": ["B.3.1 Results under Varying Labeling Budgets with More Labeling Accuracy Settings ", "Figure 8: Test accuracy for varying labeling budgets with BP inferring posterior beliefs using noisy labeled nodes. Each subplot title includes two components, which represent the labeling accuracy and dataset. Each row corresponds to a dataset, and each column corresponds to a labeling accuracy. In each subplot, the node selection strategies based on LinUProp and its variant are represented by red $\\blacktriangledown$ and purple \u25b2. Under the same labeling budget, the higher the test accuracy, the better. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "XOVks7JHQA/tmp/bf22863615bdd558e60f031eed56602ea571b533b04c9d63b870edd694126586.jpg", "img_caption": ["Figure 9: Test accuracy for varying labeling budgets with NETCONF inferring posterior beliefs using noisy labeled nodes. Each subplot title includes two components, which represent the labeling accuracy and dataset. Each row corresponds to a dataset, and each column corresponds to a labeling accuracy. In each subplot, the node selection strategies based on LinUProp and its variants are represented by $\\blacktriangledown$ and purple \u25b2, while the strategies based on the native UQ method in NETCONF and its variants are represented by brown $\\blacktriangleleft$ and pink \u25b6. NETCONF-based strategies ${\\mathrm{CS/LC}}{\\mathrm{+CS}}$ ) often prioritize labeling low-degree nodes due to their assumption that neighbors serve as evidence, leading to persistent high uncertainty in most nodes and slowly increasing accuracy. Under the same labeling budget, the higher the test accuracy, the better. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "B.3.2 Results under a fixed Labeling Budgets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Tables 2 and 3 show the mean test accuracies and standard deviations for a fixed labeling budget (20b) with BP/NETCONF inference on noisy labeled nodes. LinUProp achieves the highest mean accuracy in 28 of 32 cases, consistently outperforming its competitors across all four datasets, two inference methods, and four labeling accuracies. While the performance improvements are not always statistically significant, LinUProp maintains lower standard deviations (below $7\\%$ ) and avoids poor accuracy. In contrast, LC achieves only $55\\%$ accuracy on Polblogs in Table 3, while LinUProp consistently exceeds $80\\%$ . The Entropy method shows high standard deviations (up to $14\\%$ ) on Polblogs, whereas LinUProp generally has more stable performance, as underlined in the results. ", "page_idx": 17}, {"type": "text", "text": "Table 2: Mean test accuracies and their standard deviations for a fixed labeling budget (20b) with BP inferring posterior beliefs using noisy labeled nodes. Each cell shows the mean accuracy and its standard deviation on the test set for different labeling accuracy and node selection strategies, on different datasets. BB and $\\mathrm{LC}{+}\\mathrm{BB}$ are node selection strategies based on LinUProp. Bold values indicate the highest mean accuracy. Underlined values emphasize the method with the lower standard deviation between the LinUProp winner and the non-LinUProp winner. Superscripts indicate significant superiority between the LinUProp winner and the non-LinUProp winner (pairwise t-test at a $5\\%$ significance level (\\*), $10\\%$ significance level (\u2020)). ", "page_idx": 17}, {"type": "table", "img_path": "XOVks7JHQA/tmp/4836b1eb6c535bf8b1789c4ae87a004d6245ad3173c6a5569c659353ec6b13df.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Table 3: Mean test accuracies and their standard deviations for a fixed labeling budget (20b) with NETCONF inferring posterior beliefs using noisy labeled nodes. Each cell shows the mean accuracy and its standard deviation on the test set for different labeling accuracy and node selection strategies, on different datasets. BB and ${\\mathrm{LC}}{+}{\\mathrm{BB}}$ are node selection strategies based on LinUProp. CS and $\\mathrm{LC}+\\mathrm{CS}$ are node selection strategies based on the native UQ methods of NETCONF. Bold values indicate the highest mean accuracy. Underlined values emphasize the method with the lower standard deviation between the LinUProp winner and the non-LinUProp winner. Superscripts indicate significant superiority between the LinUProp winner and the non-LinUProp winner (pairwise t-test at a $5\\%$ significance level $(^{*})$ , $10\\%$ significance level (\u2020)). ", "page_idx": 17}, {"type": "table", "img_path": "XOVks7JHQA/tmp/7c354a6fca2d42a56d771393560c1a74ee1a655bf9ee83cc2a990f106dd5108c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "B.4 Additional Runtime Results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Under the same experimental conditions as the linear scalability experiments in Figure 4(b), we tested NETCONF on four datasets, comparing its computation time (including all edges) with that of LinUProp. As shown in Table 4, the computation times of NETCONF and LinUProp are quite similar. However, LinUProp demonstrates a significant performance advantage over the NETCONF-based UQ method, as evident from Table 3 (BB/LC+BB vs. ${\\mathrm{CS/LC}}{\\mathrm{+CS}})$ . ", "page_idx": 18}, {"type": "table", "img_path": "XOVks7JHQA/tmp/28cdb2e209eabed955ac96274a2c489af273e17101d6e170d9ec7b6114e10a55.jpg", "table_caption": ["Table 4: Runtime comparison including all edges across different datasets (in seconds) "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The abstract and introduction present the main contributions, including theoretical, methodological, and experimental contributions. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We have outlined the limitations in the conclusion and discussed convergence limitations in Sec. 4.1. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The complete and correct proofs for the theoretical analyses and results are provided in Sec. 4 and Appendix A. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: All the information needed to reproduce all the experimental results are disclosed in Sec. 5 and Appendix B.2. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Code is public at https://github.com/chenghuaguo/LinUProp. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The sufficient information concerning the experimental settings is comprehensively disclosed in Sec. 5 and Appendix B.2. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: All figures and tables related to test accuracy for varying/fixed labeling budgets have provided error bars. ", "page_idx": 19}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The sufficient information concerning the computation resources is comprehensively disclosed in Appendix B.2. ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The research conforms with the NeurIPS Code of Ethics. ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. ", "page_idx": 20}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] Justification: The paper poses no such risks. ", "page_idx": 20}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The original papers for the datasets are cited properly. ", "page_idx": 20}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 20}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 20}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 20}]