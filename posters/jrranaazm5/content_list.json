[{"type": "text", "text": "Few-Shot Diffusion Models Escape the Curse of Dimensionality ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ruofeng Yang', Bo Jiang', Cheng Chen?, Ruinan $\\mathbf{Jin^{34}}$ Baoxiang Wang34, Shuai Li\\* 1 ", "page_idx": 0}, {"type": "text", "text": "1 John Hopcroft Center for Computer Science, Shanghai Jiao Tong University 2 East China Normal University 3 The Chinese University of Hong Kong, Shenzhen 4Vector Institute {wanshuiyin, bjiang, shuaili8}@sjtu.edu.cn,   \nchchen@sei.ecnu.edu.cn, {jinruinan,bxiangwang?@cuhk.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "While diffusion models have demonstrated impressive performance, there is a growing need for generating samples tailored to specific user-defined concepts. The customized requirements promote the development of few-shot diffusion models, which use limited $n_{t a}$ target samples to fine-tune a pre-trained diffusion model trained on $n_{s}$ source samples. Despite the empirical success, no theoretical work specifically analyzes few-shot diffusion models. Moreover, the existing results for diffusion models without a fine-tuning phase can not explain why fewshot models generate great samples due to the curse of dimensionality. In this work, we analyze few-shot diffusion models under a linear structure distribution with a latent dimension $d$ . From the approximation perspective, we prove that few-shot models have a O(n-2/d $\\widetilde{O}(n_{s}^{-2/d}+n_{t a}^{-\\dot{1}/2})$ bound to approximate the target score function, which is better than nta7 results. From the optimization perspective, we consider a latent Gaussian special case and prove that the optimization problem has a closed-form minimizer. This means few-shot models can directly obtain an approximated minimizer without a complex optimization process. Furthermore, we also provide the accuracy bound $\\widetilde{O}(1/n_{t a}+1/\\sqrt{n_{s}})$ for the empirical solution, which still has better dependence on $n_{t a}$ compared to $n_{s}$ . The results of the realworld experiments also show that the models obtained by only fine-tuning the encoder and decoder specific to the target distribution can produce novel images with the target feature, which supports our theoretical results. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In recent years, diffusion models have shown an excellent ability to generate diverse, high-quality images and show state-of-the-art performance in the large-scale, standard dataset (Rombach et al., 2022; Ho et al., 2022; Li et al., 2024). However, users often desire to generate samples that resemble the ones they provide, such as images related to their families, daily lives, or specific items. These user-provided samples are typically limited in number and do not appear frequently in large-scale datasets. Consequently, training a diffusion model from scratch using such limited, personalized samples often results in poor performance. To cater the customized requirements of users, few-shot diffusion models attract much attention. Few-shot diffusion models aim to fine-tune a pre-trained diffusion model using a limited amount of data $5\\sim10$ samples), and they have recently delivered impressive results in various domains, including image generation (Ruiz et al., 2023; Han et al., 2023; Zhu et al., 2023), vide0 generation (Chen et al., 2023b), and the medical domain (Dutt et al., 2023). ", "page_idx": 0}, {"type": "text", "text": "Before the fine-tuning phase, we first need to train a diffusion model on the large source dataset $\\{X_{s,i}\\}_{i=1}^{n_{s}}$ as the pre-trained model. A diffusion model consists of a forward process and a reverse process (Song et al., 2020). The forward process gradually converts the data distribution into Gaussian noise. The reverse process sequentially removes the noise in the data to generate samples, which relies on the gradient of logarithmic forward process density (a.k.a. score function). To run the reverse process, diffusion models use a neural network to approximate the unknown score function. ", "page_idx": 1}, {"type": "text", "text": "With a pre-trained diffusion model, the paradigm to obtain a few-shot diffusion model is to fine-tune the model using a limited target dataset $\\{\\bar{X_{t a,i}}\\}_{i=1}^{n_{t a}}$ . In earlie times, fully fine-tuned methods, such as DreamBooth (Ruiz et al., 2023), provided an important boost for developing few-shot models. However, they also show that the diffusion models suffer from the overfitting and memory phenomenon when fine-tuning all parameters. Furthermore, a fully fine-tuned method is both memory and time inefficient (Xiang et al., 2023). To avoid the above problems, many works freeze most parameters and fine-tune some key parameters, such as cross-attention layers (Kumari et al., 2023; Moon et al., 2022), some concept neurons (Liu et al., 2023) or text-embedding (Gal et al., 2022), to approximate the ground-truth target score function. These works not only preserve the prior information but also have a lower requirement for the target dataset size, which is more practical for applications. Hence, we aim to explain the great performance of these models in this work. ", "page_idx": 1}, {"type": "text", "text": "Despite the empirical success, no existing theoretical work specifically analyzes the approximation bound for few-shot diffusion models, and the following question remains open: ", "page_idx": 1}, {"type": "text", "text": "Do few-shot diffusion models with a fine-tuning phase enjoy a small approximation error with a limitedtargetdataset? ", "page_idx": 1}, {"type": "text", "text": "For the approximation error bound, some works currently analyze diffusion models without a finetuning phase (Oko et al., 2023; Chen et al., 2023c; Yuan et al., 2023; Li et al., 2023b). Importantly, when analyzing general, bounded data, these works suffer from the curse of dimensionality. More specifically, Oko et al. (2023) analyze bounded distribution and show the $n_{s}^{-s^{\\prime}/D}$ approximation bound, where $D$ is the data dimension of $X_{s}$ . Chen et al. (2023c) analyze linear structure distribution $X_{s}=A_{s}Z$ with subgaussian latent variable $Z\\in\\mathbb{R}^{d}$ and achieve $\\bar{n_{s}^{-2/d}}$ results. Since the source dataset size is large enough, the influence of dimension is tolerable. However, for the limited target dataset, i trivilly using the above technique, the bound is $n_{t a}^{-1/D}$ or nta/d, which is large and can not explain why few-shot diffusion models efficiently approximate the target score function. ", "page_idx": 1}, {"type": "text", "text": "In this work, for the first time, we propose the approximation bound specifically to few-shot diffusion models with a fine-tuning phase and prove that the few-shot diffusion model can escape the curse of dimensionality. More specifically, we show that when assuming (1) linear structure data and (2) the source and the target data share latent distribution, the few-shot diffusion models with a fine-tuning phase achieve O(n-2/d $\\widetilde{O}(\\bar{n}_{s}^{-2/d}+n_{t a}^{-1/2})$ approximation error bound, which makes the first step to explain why few-shot diffusion models have great performance in the application. Generally speaking, due to the component nta /^ , bound comparedto $n_{s}^{-2/d}$ Tosupport our augmentaion, wecaleulatetherequirement of $n_{t a}$ 10 obtain an accurate enough approximated target score function in popular datasets. Table 1 shows that the requirement of $n_{t a}$ is about $5\\sim10$ samples, matching the customized diffusion model requirement. We also do experiments on the real-world datasets and show that 10 target images are enough for few-shot models to generate novel images with the target feature (Section 6). ", "page_idx": 1}, {"type": "text", "text": "After directly using the property of the minimizer to obtain an approximation bound, we analyze how to optimize the few-shot diffusion models to obtain a minimizer. Since the score-matching objective function is highly non-convex, only a few works analyze the optimization problem of diffusion models (Shah et al., 2023; Bruno et al., 2023; Cui et al., 2023; Li et al., 2023b). Furthermore, these works either require (1) an exponential size neural network (Li et al., 2023b) or (2) a distribution determined by one variable (Shah et al., 2023; Bruno et al., 2023; Cui et al., 2023) to simplify the optimization problem. This work proves that few-shot diffusion models can simplify the optimization problem without these requirements. When analyzing the optimization problem, we focus on a Gaussian latent variable special case 2. Then, we prove that the expected few-shot objective function has a closed-form minimizer, which means the empirical solution can be directly obtained without a complex optimization process. We also prove the accuracy bound $\\widetilde{O}(1/n_{t a}+1/\\sqrt{n_{s}})$ of empirical closed-form solution, which still has better dependence on the target dataset. In conclusion, we accomplish the following results for few-shot diffusion models under linear structure distribution: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u00b7 For the approximation bound, we consider a subgaussian latent variable and prove $\\widetilde{O}(n_{s}^{-2/d}+$ $n_{t a}^{-1/2})$ bound for few-shot models, which is better than $n_{t a}^{-2/d}$ result without fne-tuning. \u00b7 For the optimization problem, we consider a latent Gaussian special case and prove that the expected few-shot objective function has a closed-form minimizer. Furthermore, we prove the accuracy bound $\\dot{\\tilde{O}}(1/n_{t a}+1/\\sqrt{n_{s}})$ for the empirical closed-form solution. \u00b7 To support our theoretical results, we do real-world experiments and show that the models obtained by only fine-tuning specific encoder and decoder can use only 10 target images to generate novel images with the target feature. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The approximation error bound. Recently, some works analyze the approximation error bound of diffusion models without a fine-tuning phase. Oko et al. (2023) analyze $s^{\\prime}$ -order bounded derivatives distribution and show the approximation err bound is $n_{s}^{-s^{\\prime}/D}$ . Chen et al. 2023c) analyze distribution with linear structure and subgaussian latent variable and show that the $n_{s}^{-2/d}$ result. The approximation error bound of the above works suffers the curse of (latent) dimensionality. To avoid this phenomenon, some works analyze special data distributions. Shah et al. (2023) and Cui et al. (2023) analyze the mixture of Gaussian (MOG) with known variance and achieve a $1/n_{s}$ approximation bound. Yuan et al. (2023) analyze linear structure distribution with Gaussian latent variable and achieve $1/\\sqrt{n_{s}}$ result. Mei & Wu (2023) analyze Ising models and prove that the term corresponds to $n_{s}$ is $1/\\dot{\\sqrt{n_{s}}}$ . However, the remaining terms do not converge to O when $n_{s}$ goes to $+\\infty$ For general bounded data distribution, Li et al. (2023b) provide a $n_{s}^{-2/5}$ approximation error bound. However, they use a 2-layer random feature network and only allow the second linear layer to be trainable. Hence, the network size is $\\exp\\left(n_{s}\\right)$ compared to $\\mathrm{Poly}(n_{s})$ size of all previous works. ", "page_idx": 2}, {"type": "text", "text": "The optimization of diffusion models. Since the score matching objective function is highly non-convex, only a few works analyze how to optimize it to obtain a minimizer (Shah et al., 2023; Cui et al., 2023; Bruno et al., 2023; Li et al., 2023b). These works either make assumptions about data distribution or network size to guarantee only one optimization variable, leading to a simpler optimization problem. For special data distributions, Bruno et al. (2023) and Cui et al. (2023) analyze a Gaussian with fixed variance and a 2-mode mixture of Gaussian (MOG) with equal, trainable mean and fixed variance, respectively. Shah et al. (2023) analyze a multi-mode MOG with a fixed variance and prove a local convergence guarantee. Since they assume the distance between any two modes is large enough and a good enough initialization, the optimization problem is similar to optimizing a Gaussian distribution. For the large neural network size, Li et al. (2023b) analyze a general, bounded distribution with a 2-layer NN. Note that they require $\\exp\\left(n_{s}\\right)$ hidden neurons and only allow the linear layer to be trainable, which also leads the optimization problem to a convex optimization. ", "page_idx": 2}, {"type": "text", "text": "3  The Introduction of Few-shot Diffusion Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "With pre-trained models, the paradigm to obtain a few-shot diffusion model is to freeze most parameters and fine-tune some key parameters corresponding to the target data distribution. Since the analysis of few-shot diffusion models relies heavily on the pre-trained model, this section first provides a concise overview of the fundamental concepts and notations associated with diffusion models. Then, we introduce the paradigm of few-shot diffusion models in Section 3.2. ", "page_idx": 2}, {"type": "text", "text": "3.1  The Forward and Reverse Process ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Let $q_{0}$ be the data distribution. Given $X_{0}\\sim q_{0}\\in\\mathbb{R}^{D}$ , non-decreasing function $f(X_{t},t)$ and $g(t)$ \uff0c the forward process is defined by: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{d}X_{t}=f(X_{t},t)\\mathrm{d}t+g(t)\\mathrm{d}B_{t}\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\{B_{t}\\}_{t\\in[0,T]}$ isa $D$ -dimensional Brownian motion. In this work, we choose $f(X_{t},t)\\;=\\;$ $-1/2X_{t}$ and $g(t)=1$ , which corresponds to variance preserving (VP) forward process and is widely used in practice 3(Shah et al., 2023; Song et al., 2020). Let $q_{t}$ be the density function of $X_{t}$ . Once a forward process is chosen, the conditional distribution of $X_{t}|X_{0}$ is $q_{t}(X_{t}|X_{0})=\\mathcal{N}(m_{t}X_{0},\\sigma_{t}^{2}I_{D})$ where $m_{t}=e^{-t/2},\\sigma_{t}^{2}=1-e^{-t}$ Note that when $t$ goes to $+\\infty$ $q_{t}$ converges to $\\mathcal{N}(0,I_{D})$ , which is helpful in choosing the initial distribution for the sampling process. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "To generate samples, diffusion models first reverse the forward SDE to obtain the reverse process. Then, we approximate the ground-truth score function $\\nabla\\log q_{t}(\\cdot)$ by using a neural network $\\mathbf{s}(\\cdot,t)$ due to the requirement of the reverse process (see Section 3.2). Finally, diffusion models choose a discretization scheme to discretize the continuous reverse process to obtain an implementable algorithm. Let $t_{0}\\,\\leq\\,t_{1}\\,\\leq\\,\\cdots\\,\\leq\\,t_{K}\\,=\\,T$ be the discretization points in the forward time and $h_{k}=t_{k}-t_{k-1}$ be the $k$ -th stepsize. When considering the reverse time, we define $t_{k}^{\\prime}=T-t_{K-k}$ In this work, we choose the exponential integrator (El) discretization scheme, which has great performance (Zhang & Chen, 2022). The EI discretization freezes the approximated score at $t_{k}^{\\prime}$ and runs the following process in the reverse time: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\widehat{Y}_{t}=\\left[f(\\widehat{Y}_{t},T-t)+g(T-t)^{2}\\mathbf{s}(\\widehat{Y}_{t_{k}^{\\prime}},T-t_{k}^{\\prime})\\right]\\mathrm{d}t+g(T-t)\\mathrm{d}B_{t}\\,,t\\in[t_{k}^{\\prime},t_{k+1}^{\\prime}]\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\widehat{Y}_{0}\\sim\\mathcal{N}(0,I_{D})$ due to the stationary distribution of the forward process. ", "page_idx": 3}, {"type": "text", "text": "While the discretization complexity $K$ has been well-studied with an accurate enough score function (Benton et al., 2023; Li et al., 2023a), there is a lack of analysis for the score-matching process. Therefore, this work focuses on the score approximation and the optimization problem of the few-shot score-matching objective function. ", "page_idx": 3}, {"type": "text", "text": "3.2  The Score Matching Objective Function ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this work, we specifically analyze few-shot diffusion models, which involve two datasets: (1) the sourcedataset $\\{\\bar{X_{s,i}}\\}_{i=1}^{n_{s}}$ 2) thetargetdataset $\\{X_{t a,i}\\}_{i=1}^{n_{t a}}$ Theapprachinvolvesfistraininga pre-trained diffusion model on the source dataset and then freezing the backbone network to fine-tune the diffusion models on the target dataset. ", "page_idx": 3}, {"type": "text", "text": "For data distributions, we assume that the source distribution $q^{s}$ and the target distribution $q^{t a}$ are both supported on a low-dimensional linear subspace. The low-dimensional structures have been discovered on many popular image datasets (Pope et al., 2021; Gong et al., 2019; Tenenbaum et al., 2000) due to the locally connected and symmetrical property, and it is crucial for diffusion models. For image generation, current popular diffusion models, such as Stable Diffusion (Rombach et al., 2022), transform images to a latent space and run diffusion models in the latent space. Except for the image generation, Chen et al. (2024) recently show the latent dimension plays an important role in diffusion models to work well in self-supervised learning, and linear subspace is enough. ", "page_idx": 3}, {"type": "text", "text": "We further assume that the source and target data share the same latent distribution. Note that this is a common assumption in few-shot learning. In particular, previous theoretical works in the context of supervised few-shot learning often assume that the source and target distributions have a common latent representation (Du et al., 2020; Chua et al., 2021; Meunier et al., 2023). ", "page_idx": 3}, {"type": "text", "text": "Assumption 3.1. The source datapoints $X_{s}$ and target datapoint $X_{t a}$ admit a low dimensional linear structure and share the same latent distribution $X_{s}=A_{s}Z$ and $X_{t a}=A_{t a}Z$ where $A_{s},A_{t a}\\in\\mathbb{R}^{D\\times d}$ with orthonormal columns and $Z\\sim q_{z}\\in\\mathbb{R}^{d}$ ", "page_idx": 3}, {"type": "text", "text": "As mentioned in Chen et al. (2023c), when assuming linear distribution, the ground-truth score function is decomposed into the latent score function $\\bar{\\nabla}\\log{q_{t}^{\\mathrm{LD}}(Z^{\\prime})}$ and linear encoder and decoder: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\nabla\\log q_{t}^{s}(X)=A_{s}\\nabla\\log q_{t}^{\\mathrm{LD}}\\left(A_{s}^{\\top}X\\right)-\\frac{1}{\\sigma_{t}^{2}}\\left(I_{D}-A_{s}A_{s}^{\\top}\\right)X\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\begin{array}{r}{q_{t}^{\\mathrm{LD}}\\left(Z^{\\prime}\\right)=\\int q_{t}\\left(Z^{\\prime}|Z\\right)q_{z}(Z)\\mathrm{d}Z}\\end{array}$ and $q_{t}(\\cdot|Z)=\\mathcal{N}(m_{t}Z,\\sigma_{t}^{2}I_{d})$ . This form indicates that the diffusion process happens in the latent subspace. A conceptual way to approximate the score function is to minimize the following loss on a function class $\\mathcal{S}_{N N}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{s\\in S_{N N}}\\int_{0}^{T}w(t)\\mathbb{E}_{X_{t}\\sim q_{t}^{s}}\\left\\|\\nabla\\log q_{t}^{s}\\left(X_{t}\\right)-{\\mathbf s}\\left(X_{t},t\\right)\\right\\|_{2}^{2}\\mathrm{d}t\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Wwhere $w(t)$ is a weight function. However, the above objective function is intractable since $\\nabla\\log q_{t}(\\cdot)$ is unknown. Vincent (2011) propose the following implementable loss: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{s}(\\mathbf{s})=\\int_{0}^{T}w(t)\\mathbb{E}_{X_{0}}\\left[\\mathbb{E}_{X_{t}|X_{0}}\\left\\|\\nabla\\log q_{t}^{s}\\left(X_{t}|X_{0}\\right)-\\mathbf{s}\\left(X_{t},t\\right)\\right\\|_{2}^{2}\\right]\\mathrm{d}t\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Due to the forward process, $\\nabla\\log q_{t}^{s}\\left(X_{t}|X_{0}\\right)$ has an analytical form and is equal to $-(X_{t}\\mathrm{~-~}$ $m_{t}X_{0})/\\sigma_{t}^{2}$ . Vincent (2011) also prove that this objective function only has a constant difference compared to the above one. The empirical loss with the source datasets $\\{X_{s,i}\\}_{i=1}^{n_{s}}$ is defined by: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{s}_{V,\\theta}\\in S_{N N}}\\widehat{\\mathcal{L}}_{s}(\\mathbf{s}_{V,\\theta})=\\frac{1}{n_{s}(T-\\delta)}\\sum_{i=1}^{n_{s}}\\int_{\\delta}^{T}\\ell_{t}^{s}\\,(X_{s,i};\\mathbf{s}_{V,\\theta})\\,\\mathrm{d}t\\,,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\ell_{t}^{s}\\left(X_{s,i};\\mathbf{s}\\right)=\\mathbb{E}_{X_{t}\\mid X_{0}=X_{s,i}}\\left[\\left\\Vert\\nabla\\log q_{t}^{s}\\left(X_{t}|X_{0}\\right)-\\mathbf{s}\\left(X_{t},t\\right)\\right\\Vert_{2}^{2}\\right]\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\mathcal{S}}_{\\mathrm{NN}}=\\left\\{\\mathbf{s}_{V,\\theta}(X,t)={\\frac{1}{\\sigma_{t}^{2}}}V\\mathbf{f}_{\\theta}\\left(V^{\\top}X,t\\right)-{\\frac{1}{\\sigma_{t}^{2}}}X\\cdot V\\in\\mathbb{R}^{D\\times d}{\\mathrm{~with~orthonormal~columns}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Note that we take $w(t)=1/(T-\\delta)$ for simplicity, where $\\delta$ is the early stopping parameter to avoid the blow-up phenomenon of score functions at the end of reverse process. Furthermore, we take the integral over the forward time instead of discretizing the timeline since $X_{t}$ is easy to generate. ", "page_idx": 4}, {"type": "text", "text": "The linear encoder and decoder structure and the shortcut connection in $\\mathcal{S}_{N N}$ is due to the form of the ground-truth score function. The specific parameters for $\\mathbf{f}_{\\theta}$ , such as its length and width, are identical to those used in Chen et al. (2023c). Generally, with a given network accuracy parameters $\\epsilon$ the network size is $\\mathrm{Poly}(1/\\epsilon)$ . We show the parameter of neural network in Appendix A. ", "page_idx": 4}, {"type": "text", "text": "The diffusion models minimize the empirical loss to obtain a pre-trained approximated score function. Let the minimizer of Equation (1) be $(\\widehat{V}_{s},\\widehat{\\theta})$ . Chen et al. (2023c) show that $(\\widehat{V}_{s},\\widehat{\\theta})$ leads a $n_{s}^{-2/d}$ approximation error bound. If trivially replacing $n_{s}$ with $n_{t a}$ , we obtain a $n_{t a}^{-2/d}$ bound for the target dataset without the fine-tuning phase. Note that this bound suffers from the influence of the latent dimension $d$ , which is still large in popular datasets (Table 1). Hence, this results in a large approximation error bound. In the next paragraph, we introduce the few-shot diffusion models with a fine-tuning phase and show that the dependence on $n_{t a}$ .s $n_{t a}^{-1/2}$ in the error bound (Theorem 4.3). ", "page_idx": 4}, {"type": "text", "text": "The Few shot Diffusion Models with a Fine-tuning Phase. Since the source and target distribution share the same latent data distribution, we freeze $\\hat{\\theta}$ and only fine-tune the low-rank linear encoder and decoderlayer $V_{t a}$ . This method can significantly reduce the fine-tuning parameters and is similar to LoRA (Hu et al., 2021), which also fine-tunes two low-rank matrices and is widely used in fine-tuning the stable diffusion (Rombach et al., 2022). ", "page_idx": 4}, {"type": "text", "text": "Let $\\ell_{t}^{t a}$ be the loss function of the target dataset at time $t$ , which has similar definition compared to $\\ell_{t}^{s}$ The optimization problem for the target dataset is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{s}_{V_{t a},\\hat{\\theta}}\\in\\mathcal{Q}_{N N}(\\hat{\\theta})}\\widehat{\\mathcal{L}}_{t a}(\\mathbf{s}_{V_{t a},\\hat{\\theta}})=\\frac{1}{n_{t a}(T-\\delta)}\\sum_{i=1}^{n_{t a}}\\int_{\\delta}^{T}\\ell_{t}^{t a}\\left(X_{t a,i};\\mathbf{s}_{V_{t a},\\hat{\\theta}}\\right)\\mathrm{d}t\\,,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\mathcal{Q}}_{\\mathrm{NN}}(\\theta)=\\left\\{\\mathbf{s}_{V,\\theta}(X,t)={\\frac{1}{\\sigma_{t}^{2}}}V\\mathbf{f}_{\\theta}\\left(V^{\\top}X,t\\right)-{\\frac{1}{\\sigma_{t}^{2}}}X:V\\in\\mathbb{R}^{D\\times d}\\:\\mathbf{v}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Similarly, we define the minimizer of the few-shot objective function as $(\\widehat{V}_{t a},\\widehat{\\theta})$ ", "page_idx": 4}, {"type": "text", "text": "Notations. We denote by $I_{D}$ the $D$ -dimensional identity matrix. For $X\\in\\mathbb{R}^{D}$ and $A\\in\\mathbb{R}^{D\\times d}$ we denoteby $\\|X\\|_{2}$ the Euclidean norm for vector and $\\|A\\|_{F}$ the Frobenius norm for matrix. We denote by $\\|X\\|_{L^{2}(q)}^{2}$ the expectation of $X$ .n $L_{2}$ norm $\\mathbb{E}_{X\\sim q}[\\|X\\|_{2}^{2}]$ ", "page_idx": 4}, {"type": "text", "text": "Table 1: The requirement of $n_{t a}$ in popular datasets. We use latent dimension in Pope et al. (2021). ", "page_idx": 5}, {"type": "table", "img_path": "JrraNaaZm5/tmp/ecdc36b2742f1b96404cc1fea702f9c6fea77827ebf0303826f6a4f420ad10df.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "4  Few-shot Diffusion Models Enjoy Better Approximation Error Bound ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we show that few-shot diffusion models with a fine-tuning phase escape the curse of Ilaten dimensionality and havea $\\widetilde{O}(n_{s}^{-2/d}+n_{t a}^{-1/2})$ approximaion boundt Thisresutmakesthe first step to explain why few-shot models have great performance with a limited target dataset. ", "page_idx": 5}, {"type": "text", "text": "Before showing our results, we first introduce standard assumptions on the latent distribution and the on-support ground-truth score function. We first assume that $Z$ has a subgaussian tail and the minimum eigenvalue of $Z$ is lowerbound by $c_{\\mathrm{0}}$ , also used in Chen et al. (2023c). ", "page_idx": 5}, {"type": "text", "text": "Assumption 4.1. $q_{z}>0$ is twice continuously differentiable, $\\lambda_{\\operatorname*{min}}(\\mathbb{E}\\left[Z Z^{\\top}\\right])\\geq c_{0}$ and $\\mathbb{E}\\|Z\\|_{2}^{2}\\leq$ $C_{Z}$ . Moreover, there exist positive constants $B,C_{1},C_{2}$ such that when $\\|Z\\|_{2}\\;\\ge\\;B$ $q_{z}(Z)\\ \\leq$ $(2\\pi)^{-d/2}C_{1}\\exp\\left(-C_{2}\\|Z\\|_{2}^{2}/2\\right)$ ", "page_idx": 5}, {"type": "text", "text": "Assumption 4.2. The on-support ground truth score $A_{s}\\nabla\\log q_{t}^{\\mathrm{LD}}\\left(Z\\right)$ and $A_{t a}\\nabla\\log q_{t}^{\\mathrm{LD}}\\left(Z\\right)$ is $\\beta$ Lipschitz in $Z\\in\\mathbb{R}^{d}$ for any $t\\in[0,T]$ ", "page_idx": 5}, {"type": "text", "text": "Note that different from previous works directly assume $\\nabla\\log q_{t}(\\cdot)$ is Lipschitz (Chen et al., 2022, 2023d),the $\\beta$ -Lipschitz on-support score function assumption does not conflict with the blow-up phenomenon when $t$ goes to O due to the existence of $(I_{D}^{\\star}-A A^{\\top})X/\\sigma_{t}^{2}$ . With these assumptions, we prove the approximation bound for few-shot models with a fine-tuning phase. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.3 Let $\\begin{array}{r}{\\alpha(n)=\\frac{d\\log\\log n}{\\log n}}\\end{array}$ \uff0c $\\begin{array}{r}{F=\\frac{(d+C_{Z})d^{2}\\beta^{2}}{\\delta^{2}c_{0}}}\\end{array}$ and etwork paromnter $\\epsilon=n_{t a}^{-1/2}$ Assume Assumption3.1,4.1,4.2 and na(r\u2265nsThen, withprobabilty $1-\\delta_{1}$ , the following inequality holds (hiding logarithmic factors) ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{1}{\\Gamma-\\delta}\\int_{\\delta}^{T}\\left\\|\\mathbf{s}_{\\widehat{V}_{t a},\\widehat{\\theta}}(\\cdot,t)-\\nabla\\log q_{t}^{t a}(\\cdot)\\right\\|_{L^{2}(q_{t}^{t a})}^{2}\\,\\mathrm{d}t\\leq\\tilde{O}\\left(\\left(\\frac{(1+\\beta)^{2}D d^{3}}{\\delta\\left(T-\\delta\\right)\\sqrt{n_{t a}}}+F n_{s}^{-\\frac{2-2\\alpha(n_{s})}{d+5}}\\right)\\log\\left(\\frac{1}{\\delta_{1}}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The dependence of $\\delta$ is due to the blow-up property of the score function. Note that when $n_{s}$ is sufficiently large, $\\alpha(n_{s})$ is negligible. Then, the approximation error bound for few-shot diffusion models is O(n-2/d $\\tilde{O}(n_{s}^{-2/d}+n_{t a}^{-1/2})$ Compared taroximaionrorboud $n_{t a}^{-2/d}$ phase, it is clear that few-shot diffusion models escape the curse of the (latent) dimensionality. Remark 4.4 (The discussion on the coefficient in Theorem 4.3). The goal of the fine-tuning phase is to achieve the same order error bound compared with the pre-trained models, which means that we consider the relative relationship between $n_{t a}$ and $n_{s}$ . Hence, if the coefficient of $n_{t a}$ and $n_{s}$ has the same order, we can only consider $1/\\sqrt{n_{t a}}$ and $n_{s}^{-2/d}$ . To support the above augmentation, we calculate the coefficient of $n_{s}$ and $n_{t a}$ in detail. The dominated term of coefficient for $n_{t a}$ and $n_{s}$ are $D d^{3}/\\delta$ and $d^{3}/(\\delta^{2}c_{0})$ , respectively. The classic choice for the early stopping parameter $\\delta$ and forward time $T$ are $\\mathrm{i0^{-3}}$ and 10, respectively (Karras et al., 2022). Then, with $D=256\\times256\\times3$ as an example 5 , $D d^{3}/\\delta=d^{3}\\times20\\stackrel{.}{\\times}10^{6}$ and $d^{3}/(\\delta^{2}c_{0})=d^{3}\\times10^{6}/c_{0}$ , which has the same order. Hence, we consider the relative relationship between $1/\\sqrt{n_{t a}}$ and $n_{s}^{-2/d}$ ", "page_idx": 5}, {"type": "text", "text": "4.1  Discussion on the Approximation Bound ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The relationship to empirical phenomenon. In applications, current few-shot diffusion models onlyrequire $5\\sim10$ target images to achieve great performance. Theorem 4.3 makes the first step to explain why the few-shot diffusion models have great performance with a limited target $n_{t a}$ .More specifically, with known source dataset size $n_{s}$ and the corresponding latent dimension $d$ ,we can calculate the inequality nta Aat-a(ra)2 ns6 to obtain the requiremen of $n_{t a}$ to achieve the same accuracy compared to the pre-trained diffusion models. Combined with the latent dimension of popular datasets (Pope et al., 2021), Table 1 shows the requirement of $n_{t a}$ . It is clear that we only need less than 10 target images to obtain an accurate enough few-shot diffusion model that matches the performance in reality. The real-world experiments also support our discussion (Section 6). ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Table 1 shows that the requirement of $n_{t a}$ is heavily influenced by the latent dimension $d$ When $d$ is large (e.g. ImageNet), the approximation bound of pre-trained models is influenced by latent dimension and has a large approximation error even with large-size source data. We only need a few target data to achieve the same error in this setting. When $d$ is small (e.g. CIFAR-10), pre-trained models have a small approximation error. We need a slightly larger target data size. ", "page_idx": 6}, {"type": "text", "text": "The approximation error of the fully fine-tuned method. As shown in our real experiment Section 6 and DreamBooth (Ruiz et al., 2023), when fine-tuning all parameters with a small target dataset, models tend to overfit and lose the prior information from the pre-trained model. In our theorem, this phenomenon means that in the fine-tuning phase, the model does not use 0 learned by the pre-trained model and achieves a $n_{t a}^{-2/d}$ approximation erorbound, which suffers from the curse of dimensionality. From an intuitive perspective, the probability density function (PDF) of a distribution learned by an overfitting model is only positive at the interval around the target dataset, which is far away from the PDF of true distribution and leads to a large error term. We also note that it is possible to avoid this phenomenon by using a specific loss (Ruiz et al., 2023) or carefully choosing the optimization epochs (Li et al., 2023b). We leave them as interesting future works. ", "page_idx": 6}, {"type": "text", "text": "Proof sketch. The first step is to prove that in ${\\mathcal{Q}}_{N N}({\\widehat{\\theta}})$ , there exists a solution $(\\bar{V}_{t a},\\widehat{\\theta})$ has the following inequality (only focusing on $n_{s}$ and $n_{t a}$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{1}{T-\\delta}\\int_{\\delta}^{T}\\left\\lVert\\mathbf{s}_{\\bar{V}_{t a},\\hat{\\theta}}(X,t)-\\nabla\\log q_{t}^{t a}(X)\\right\\rVert_{L^{2}(q_{t}^{t a})}^{2}\\mathrm{d}t\\le O\\left(\\epsilon^{2}+n_{s}^{-\\frac{2-2\\alpha(n_{s})}{d+5}}\\log\\left(\\frac{1}{\\delta_{1}}\\right)\\right)\\,.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "To prove the above inequality, we first do the following decomposition: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\left\\lVert\\mathbf{s}_{\\bar{V}_{t a},\\bar{\\theta}}(\\cdot,t)-\\nabla\\log q_{t}^{t a}(\\cdot)\\right\\rVert_{2}^{2}+\\left\\lVert\\mathbf{s}_{\\bar{V}_{t a},\\widehat{\\theta}}(\\cdot,t)-\\mathbf{s}_{\\bar{V}_{t a},\\bar{\\theta}}(\\cdot,t)\\right\\rVert_{2}^{2}\\,,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $(\\bar{V}_{t a},\\bar{\\theta})\\;\\in\\;{\\cal S}_{N N}$ is a constructed solution. The first term is due to the accuracy of the constructive neural network with network accuracy parameter $\\epsilon$ . For the second term, since the latent score function is shared and few-shot diffusion models directly use 0, it is control by the approximation bound of the pre-trained diffusion models. Then, by using the inequality ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{s_{V_{t a},\\hat{\\theta}}\\in\\mathcal{Q}(\\widehat{\\theta})}\\widehat{\\mathcal{L}}_{t a}\\left(\\mathbf{s}_{V_{t a},\\widehat{\\theta}}\\right)\\leq\\widehat{\\mathcal{L}}_{t a}\\left(\\mathbf{s}_{\\bar{V}_{t a},\\widehat{\\theta}}\\right)\\,,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "we build the bridge between SVta, and Svta,0. ", "page_idx": 6}, {"type": "text", "text": "The second step is using the concentration to control the error between empirical $\\widehat{\\mathcal{L}}_{t a}$ and expected $\\mathcal{L}_{t a}$ . Roughly speaking, we have that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{t a}\\left(\\mathbf{s}_{\\widehat{V}_{t a},\\widehat{\\theta}}\\right)-\\widehat{\\mathcal{L}}_{t a}\\left(\\mathbf{s}_{\\widehat{V}_{t a},\\widehat{\\theta}}\\right)\\leq\\frac{1}{n_{t a}\\epsilon^{2}}\\log\\left(\\!N\\left(1/n_{t a},\\boldsymbol{Q}_{\\mathrm{NN}}(\\widehat{\\theta}),\\|\\cdot\\|_{2}\\right)/\\delta_{1}\\right)\\,,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathcal{N}(1/n_{t a},\\mathcal{Q}_{N N}(\\widehat{\\theta}),\\|\\,\\cdot\\,\\|_{2})$ isthe coveringnumber of ${\\mathcal{Q}}_{N N}({\\widehat{\\theta}})$ in $L_{2}$ norm. Since only $V\\in\\mathbb{R}^{D\\times d}$ can be optimized and $\\widehat{\\theta}$ is fixed in ${\\mathcal{Q}}_{N N}({\\widehat{\\theta}})$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\log\\left(\\!N\\left(1/n_{t a},\\mathcal{Q}_{\\mathrm{NN}}(\\widehat{\\theta}),\\Vert\\cdot\\Vert_{2}\\right)\\!\\right)=\\widetilde{O}(D d\\log(1/n_{t a}))\\,.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Then, we balance different terms and achieve the final bound by choosing $\\epsilon^{2}=1/\\sqrt{n_{t a}}$ ", "page_idx": 6}, {"type": "text", "text": "5  The Few-shot Diffusion Model Have a Closed-form Minimizer ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "This section focuses on how to optimize the few-shot diffusion model. When considering the optimization problem, we assume the shared latent distribution admits an isotropic Gaussian distribution $q_{z}=\\mathcal{N}(\\dot{0},\\lambda^{2}I_{d})$ With $\\lambda^{2}>0$ , which indicates the score function has the following formulation: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\nabla\\log q_{t}^{t a}(X)=-\\frac{1}{\\lambda^{2}}A_{t a}A_{t a}^{\\top}X-\\frac{1}{\\sigma_{t}^{2}}\\left(I_{D}-A_{t a}A_{t a}^{\\top}\\right)X\\,.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Note that though $q_{z}=\\mathcal{N}(0,\\lambda^{2}I_{d})$ is a special case of Assumption 4.1, we still need to know $\\lambda^{2}$ and $A_{t a}$ to generate samples, which indicates the previous optimization analysis for diffusion models without a fine-tuning phase can not be used. ", "page_idx": 7}, {"type": "text", "text": "We fix a $t\\in[\\delta,T]$ for the few-shot objective function since the matrix $A_{t a}$ is independent of time $t$ . More specifically, with an approximated latent distribution $\\widehat{q}_{z}=\\mathcal{N}(0,\\widehat{\\Sigma})$ ,where $\\widehat{\\Sigma}=\\widehat{\\lambda}^{2}I_{d}$ ,the expected few-shot objective function at a fixed time $t$ is ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{s}_{V_{t a},\\widehat{\\Sigma}}\\in\\widetilde{\\mathcal{Q}}_{N N}(\\widehat{\\Sigma})}\\mathcal{L}_{t a,t}(\\mathbf{s}_{V_{t a},\\widehat{\\Sigma}})=\\mathbb{E}_{X_{t a}\\sim q^{t a}}\\left[\\ell_{t}^{t a}\\left(X_{t a};\\mathbf{s}_{V_{t a},\\widehat{\\Sigma}}\\right)\\right]\\,.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{Q}}_{\\mathrm{NN}}(\\Sigma)=\\left\\{\\mathbf{s}_{V,\\theta}(X,t)=\\frac{1}{\\sigma_{t}^{2}}V\\mathbf{f}_{\\Sigma}\\left(V^{\\top}X,t\\right)-\\frac{1}{\\sigma_{t}^{2}}X:V\\in\\mathbb{R}^{D\\times d}\\mathrm{~with~rank}(V)=d.\\right\\},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In this case, $\\mathbf{f}_{\\widehat{\\Sigma}}(Z,t)=(I_{d}-\\sigma_{t}^{2}\\widehat{\\Sigma}_{t}^{-1})Z$ where $\\widehat{\\Sigma}_{t}=m_{t}^{2}\\widehat{\\Sigma}+\\sigma_{t}^{2}I_{d}$ . The constraint $\\operatorname{rank}(V)=d$ is used to guarantee that the few-shot diffusion models learn meaningful subspace instead of $\\mathbf{0}^{D\\times d}$ Note that $\\operatorname{rank}(V)\\,=\\,d$ is a weaker constraint than $V^{\\top}V\\,=\\,I_{d}$ since the pre-trained diffusion model has already learned the length information. This weaker constraint means we need less prior knowledge compared to $\\mathcal{Q}(\\theta)$ , which is more user-friendly. Let $\\widetilde{V}_{t a}$ be a minimizer of the above expected few-shot objective function. We show that $\\widetilde{V}_{t a}$ has a closed form and good property. ", "page_idx": 7}, {"type": "text", "text": "Lemma 5.1. Assume Assumption 3.1 and $q_{z}=\\mathcal{N}(0,\\lambda^{2}I_{d})$ Let $C=\\mathbb{E}_{X_{t a}\\sim q^{t a}}$ $\\left[X_{t a}X_{t a}^{\\top}\\right]$ be the expected data covariance matrix. Then, $\\widetilde{V}_{t a}$ has a closed form: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\widetilde{V}_{t a}\\widetilde{V}_{t a}^{\\top}=\\frac{m_{t}^{2}\\widehat\\lambda^{2}+\\sigma_{t}^{2}}{\\widehat\\lambda^{2}}\\left(C+\\sigma_{t}^{2}I_{D}\\right)^{-1}C\\,.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Lemma 5.1 indicates that few-shot diffusion models can directly obtain an approximation of the minimizer without a complex optimization process. Furthermore, this minimizer has good properties and exactly_recovers the subspace spanned by $A_{t a}$ . More specifically, the expected minimizer indicates $\\|\\widetilde{V}_{t a}\\widetilde{V}_{t a}^{\\top}-A_{t a}A_{t a}^{\\top}\\|_{F}^{2}=0$ when $n_{s}$ and $n_{t a}$ are infnitwever, th soucdata $n_{s}$ and target datasets $n_{t a}$ are finite, we analyze the empirical closed-form solution ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\bar{\\widetilde{V}}_{t a}\\bar{\\widetilde{V}}_{t a}^{\\top}=\\frac{m_{t}^{2}\\widehat{\\lambda}^{2}+\\sigma_{t}^{2}}{\\widehat{\\lambda}^{2}}(m_{t}^{2}\\bar{C}+\\sigma_{t}^{2}I_{D})^{-1}\\bar{C}\\,,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\begin{array}{r}{\\bar{C}=\\frac{1}{n_{t a}}\\sum_{i=1}^{n_{t a}}X_{t a,i}X_{t a,i}^{\\top}}\\end{array}$ is the empirical covariance matrix. ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.2. Assume Assumption 3.1 and $q_{z}\\,=\\,\\mathcal{N}(0,\\lambda^{2}I_{d})$ : Let $\\widehat{q}_{z}$ be the latent distribution generaedbad $(\\widehat{V}_{t a},\\widehat{\\Sigma})$ $\\begin{array}{r}{M_{\\cdot}=\\frac{d^{2}\\beta^{2}\\left(d+\\lambda^{2}\\right)}{\\lambda}\\sqrt{D d\\log\\left(D d n_{s}\\right)\\left(d^{2}\\vee D\\right)}}\\end{array}$ Then, with probability $1-\\delta_{1}$ we have that for any $t\\in[\\delta,T]$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left\\|\\bar{\\tilde{V}}_{t a}\\bar{\\tilde{V}}_{t a}^{\\top}-A_{t a}A_{t a}^{\\top}\\right\\|_{F}^{2}\\leq\\tilde{O}\\left(\\frac{d\\log(\\frac{1}{\\delta_{1}})}{m_{t}^{2}\\lambda^{2}+\\sigma_{t}^{2}}\\left(\\frac{M}{d\\delta\\sqrt{n_{s}}}+\\frac{d}{n_{t a}}(m_{t}^{2}\\lambda^{2}+\\sigma_{t}^{2})^{2}\\right)\\right)\\,.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The above result indicates that the few-shot diffusion models can still recover the true subspace with finite $n_{s}$ and $n_{t a}$ . Note that when the latent distribution is Gaussian distribution, the approximation error bound for the source dataset is $n_{s}^{-1/2}$ instead of $n_{s}^{-2/d}$ (Yuan et al., 2023). Hence, $n_{s}$ in Theorem 5.2 do not depend on latent dimension $d$ ", "page_idx": 7}, {"type": "text", "text": "Remark 5.3. The bound of $\\|V V^{T}-A A^{T}\\|_{F}^{2}$ only guarantees the subspace spanned by $V$ and $A$ is close, which still holds after an orthogonal transformation on $V$ . Hence, this bound does not indicate $\\|V-A\\|_{F}^{2}$ is small. Since all previous works (Chen et al., 2023c; Yuan et al., 2023) consider $\\|V V^{T}-A A^{T}\\|_{F}^{2}$ we alsouse this metricto measure the subspacerecovery. However, ouresults are stronger due to the closed-form solution, where previous works do not consider how to obtain $V V^{T}$ ", "page_idx": 7}, {"type": "image", "img_path": "JrraNaaZm5/tmp/bbb93a9e2e0a9790e526158959eae40bb25bf601a858227479b8e1d27546207d.jpg", "img_caption": ["Figure 1: The experiments on CelebA64 dataset "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.1 Discussion on the Closed-form Minimizer ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Better dependence on $n_{t a},\\delta$ and $d$ . Note that Theorem 5.2 has better $1/n_{t a}$ dependence on the target dataset compared to $1/\\sqrt{n_{s}}$ dependence on the source dataset. Furthermore, the coefficient of $n_{s}$ term is dependent on the early stopping parameter $\\delta$ and $D$ . This is due to the $\\delta$ and $D$ dependence of the approximation bound, which is used in generating $\\widehat{q}_{z}$ . However, the $n_{t a}$ term only has $d$ dependence. Hence, even in the latent Gaussian setting, we still need a larger source dataset than the target dataset to obtain a sufficiently accurate closed-form solution. ", "page_idx": 8}, {"type": "text", "text": "The relationship with principal component analysis (PCA). The expected few-shot score matching objective can be simplified to ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{V_{t a}\\in\\tilde{\\mathcal{Q}}_{N N}(\\widehat{\\Sigma})}1/\\sigma_{t}^{4}\\mathbb{E}_{X_{t}|X_{0}=X_{t a,i}}\\left[\\|V_{t a}\\widehat{G}_{t}V_{t a}^{\\top}X_{t}-m_{t}X_{0}\\|_{2}^{2}\\right]\\,,\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\widehat{G}_{t}=I_{d}-\\sigma_{t}^{2}\\widehat{\\Sigma}_{t}^{-1}$ Note tatwhenignoring $1/\\sigma_{t}^{4}$ and chosing $t=0$ problem is similar to PCA. This suggests that few-shot models implicitly optimize an objective function akin to PCA. However, few-shot models extend beyond traditional PCA. More specifically, when $\\lambda^{2}$ is large, classical PCA suffers from the influence of $\\bar{\\lambda^{2}}$ . In contrast, due to $(m_{t}^{2}\\lambda^{2}\\,{+}\\,\\sigma_{t}^{2})/n_{t a}$ term, few-shot models can select a large $t$ to mitigate the impact of $\\lambda^{2}$ and achieve a $1/n_{t a}$ ", "page_idx": 8}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To corroborate our theoretical findings, we conducted experiments utilizing real-world datasets. These experiments show that the new model obtained by only fine-tuning appropriate encoder and decoder layers on target datasets can produce novel images with the target feature, which shows the effectiveness of the methods and supports our theoretical results. ", "page_idx": 8}, {"type": "text", "text": "Datasets and benchmark. Note that human face images tend to exhibit similarity in their latent space, primarily due to shared facial features, while differing in specific features. Hence, we initially pre-train a model using the CelebA64 dataset, focusing on distinct hairstyle features as the goal for the fine-tuning phase. For the source data, we construct a large dataset (6400 images) with different hairstyles (without the bald feature). For the target data, we choose the bald feature as the target feature and select 10 images with this feature to constitute the target dataset, which are much smaller than the size of target dataset (Figure 1 (a)). To show the effectiveness of our methods, we also fine-tune all parameters of the pre-trained models as the benchmark. ", "page_idx": 8}, {"type": "text", "text": "Discussion. As shown in Figure 1, the results obtained by only fine-tuning the encoder and decoder layers can generate novel face images with the bald feature. Conversely, when fine-tuning all parameters, the models suffer from memory phenomenon and can only generate images that slightly modify the brightness and angle of the target dataset. This phenomenon indicates that only fine-tuning the appropriate encoder and decoder will result in a model with a generalization property. ", "page_idx": 8}, {"type": "text", "text": "We note that these experiments aim to verify the effectiveness of the methods instead of achieving state-of-the-art performance since previous works carefully select specific parameters, such as specific cross-attention layers (Kumari et al., 2023) or special neurons (Liu et al., 2023), to fine-tune pretrained models. However, we simply fine-tune all encoder and decoder layers simultaneously. There are more experiments on cat faces and more discussion on why Assumption 3.1 is satisfied in our experiments. We refer to Appendix E for more details. ", "page_idx": 8}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work aims to provide a deeper understanding of few-shot diffusion models from a theoretical perspective. Our analysis is conducted from two key perspectives: the approximation and optimization aspects, all under linear structure distribution and shared latent space assumptions. ", "page_idx": 9}, {"type": "text", "text": "From the approximation error bound, we consider general subgaussian latent variable and prove that few-shot models have a $\\widetilde{O}\\left(n_{s}^{-2/d}+n_{t a}^{-1/2}\\right)$ approximation bound, which is better than $n_{t a}^{-2/d}$ results of diffusion models without a fine-tuning phase and escape the curse of dimensionality. This result also makes the first step to explain why few-shot diffusion models only require $5\\sim10$ images to generate great samples. The experiments on the real-world dataset also show that the fine-tuning phase only requires 10 images to generate novel images with the target feature. ", "page_idx": 9}, {"type": "text", "text": "When analyzing the optimization process, we consider a more special, shared Gaussian latent variable and prove that the expected score matching has a closed-form minimizer, which indicates that the few-shot diffusion models can simplify the optimization problem. Furthermore, we prove that the empirical closed-form solution has a $\\widetilde{\\cal O}\\left(1/n_{t a}+1/\\left(\\delta\\sqrt{n_{s}}\\right)\\right)$ accuracy bound, which still has better $1/n_{t a}$ target data dependence compared to $1/\\left(\\delta\\sqrt{n_{s}}\\right)$ dependence on the source data. ", "page_idx": 9}, {"type": "text", "text": "Future work and limitation. When considering the approximation bound, we assume a distribution with a linear structure. Though it has been supported by much empirical evidence, it is not as general as bounded distribution. After that, we plan to consider a general, bounded distribution and show the advantage of few-shot diffusion models. One possible way is to analyze the mixture of low-rank Gaussian (Wang et al., 2024), which is more general than the linear subspace assumption. ", "page_idx": 9}, {"type": "text", "text": "We focus on a special Gaussian latent distribution when considering the optimization problem. As a next step, we plan to consider a more general latent distribution, such as a log-concave distribution. In this setting, we can not directly obtain the closed-form solution. However, due to the shared information and simplified landscape, it is still possible to use some optimization algorithms, such as gradient descent, to optimize the few-shot objective function to achieve the convergence guarantee. ", "page_idx": 9}, {"type": "text", "text": "Broader Impact. This paper presents work whose goal is to understand few-shot diffusion models from the theoretical perspective. A noteworthy societal impact is that few-shot diffusion models may be used to imitate the style of artists and generate fake images, thereby infringing on the rights of artists (Mirsky & Lee, 2021). We recommend adding watermarks to images to determine whether the image was generated by a generative model (Fernandez et al., 2023). The other societal impact is the same as general generative models (Mishkin et al., 2022). ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The author Bo Jiang is supported by National Natural Science Foundation of China (62072302). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Benton, J., De Bortoli, V., Doucet, A., and Deligiannidis, G. Linear convergence bounds for diffusion models via stochastic localization. arXiv preprint arXiv:2308.03686, 2023.   \nBruno, S., Zhang, Y., Lim, D.-Y., Akyildiz, O. D., and Sabanis, S. On diffusion-based generative models and their error bounds: The log-concave case with full convergence estimates. arXiv preprint arXiv:2311.13584,2023.   \nChen, H., Lee, H., and Lu, J. Improved analysis of score-based generative modeling: User-friendly bounds under minimal smoothness assumptions. In International Conference on Machine Learning, Pp. 4735-4763. PMLR, 2023a.   \nChen, H., Wang, X., Zeng, G., Zhang, Y., Zhou, Y., Han, F, and Zhu, W. Videodreamer: Customized multi-subject text-to-video generation with disen-mix finetuning. arXiv preprint arXiv:2311.00990, 2023b.   \nChen, M., Huang, K., Zhao, T., and Wang, M. Score approximation, estimation and distribution recovery of diffusion models on low-dimensional data. arXiv preprint arXiv:2302.07194, 2023c.   \nChen, S., Chewi, S., Li, J., Li, Y., Salim, A., and Zhang, A. R. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. arXiv preprint arXiv:2209.11215, 2022.   \nChen, S., Daras, G., and Dimakis, A. G. Restoration-degradation beyond linear diffusions: A non-asymptotic analysis for dim-type samplers. arXiv preprint arXiv:2303.03384, 2023d.   \nChen, X., Liu,Z., Xie, S., and He, K. Deconstructing denoising diffusion models for self-supervised learning, 2024.   \nChua, K., Lei, Q., and Lee, J. D. How fine-tuning allows for effective meta-learning. Advances in Neural Information Processing Systems, 34:8871-8884, 2021.   \nCui, H., Krzakala, F., Vanden-Eijnden, E., and Zdeborova, L. Analysis of learning a fow-based generative model from limited sample complexity. arXiv preprint arXiv:2310.03575, 2023.   \nDu, S. S., Hu, W., Kakade, S. M., Lee, J. D., and Lei, Q. Few-shot learning via learning the representation, provably. arXiv preprint arXiv:2002.09434, 2020.   \nDutt, R,Er,Sa,,Tari . a al, Tarf for medical image analysis: The missed opportunity. arXiv preprint arXiv:2305.08252, 2023.   \nFernandez, P, Couairon, G, Jgou, H., Douze, M., and Furon, T. The stable signature: Rooting watermarks in latent diffusion models. InProceedings of the IEEE/CVF International Conference on Computer Vision, pp. 22466-22477, 2023.   \nGal, R., Alaluf, Y., Atzmon, Y, Patashnik, O., Bermano, A. H., Chechik, G., and Cohen-Or, D. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022.   \nGong, S., Boddeti, V. N., and Jain, A. K. On the intrinsic dimensionality of image representations. InProceedings of the IEEE/CVF Conference on Computer Vsion and Pattern Recognition, Pp. 3987-3996, 2019.   \nHan, L, Li, Y, Zhang, H, Milanfar, , Mtaxas, D, and Yang,F Sdiff Compact parameter sac for diffusion fine-tuning. arXiv preprint arXiv:2303.11305, 2023.   \nHo, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., and Fleet, D. J. Video diffusion models. arXiv preprint arXiv:2204.03458, 2022.   \nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.   \nKarras, T., Laine, S., and Aila, T. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, Pp. 4401-4410, 2019.   \nKarras, T, Aittala, M., Aila, T., and Laine, S. Elucidating the design space of diffusion-based generative models. Advances in Neural Information Processing Systems, 35:26565-26577, 2022.   \nKumari, N., Zhang, B., Zhang, R., Shechtman, E., and Zhu, J.-Y. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1931-1941, 2023.   \nLi, G., Wei, Y, Chen, Y, and Chi, Y. Towards faster non-asymptotic convergence for diffusion-based generative models. arXiv preprint arXiv:2306.09251, 2023a.   \nLi, H., Shi, H., Zhang, W., Wu, W., Liao, Y, Wang, L., Lee, L.-h., and Zhou, P. Dreamscene: 3d gaussian-based text-to-3d scene generation via formation pattern sampling. arXiv preprint arXiv:2404.03575, 2024.   \nLi, P., Li, Z., Zhang, H., and Bian, J. On the generalization properties of diffusion models. arXiv preprint arXiv:2311.01797, 2023b.   \nLiu, Z., Feng, R., Zhu, K., Zhang, Y., Zheng, K., Liu, Y., Zhao, D., Zhou, J., and Cao, Y. Cones: Concept neurons in diffusion models for customized generation. arXiv preprint arXiv:2303.05125, 2023.   \nMei, S. and Wu, Y. Deep networks as denoising algorithms: Sample-efcient learning of diffusion models in high-dimensional graphical models. arXiv preprint arXiv:2309.11420, 2023.   \nMeunier, D., Li, Z., Gretton, A., and Kpotufe, S. Nonlinear meta-learning can guarantee faster rates. arXiv preprint arXiv:2307.10870, 2023.   \nMirsky, Y. and Lee, W. The creation and detection of deepfakes: A survey. ACM computing surveys (CSUR), 54(1):1-41, 2021.   \nMishkin, P, Ahmad, L., Brundage, M., Krueger, G., and Sastry, G. Dall e 2 preview-risks and limitations. Noudettu, 28:2022, 2022.   \nMoon, T, Choi, M., Lee, G., Ha, J.-W., and Lee, J. Fine-tuning diffusion models with limited data. In NeurIPS 2022 Workshop on Score-Based Methods, 2022.   \nOko, K., Akiyama, S., and Suzuki, T. Diffusion models are minimax optimal distribution estimators. arXiv preprint arXiv:2303.01861, 2023.   \nPope, P., Zhu, C., Abdelkader, A., Goldblum, M., and Goldstein, T. The intrinsic dimension of images and its impact on learning. arXiv preprint arXiv:2104.08894, 2021.   \nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10684-10695, 2022.   \nRuiz, N., Li, Y, Jampani, V., Pritch, Y., Rubinstein, M., and Aberman, K. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 22500-22510, 2023.   \nShah, K., Chen,S. and Klivans,A. Leaning mixtures of gaussians using the ddpm objective. arXiv preprint arXiv:2307.01178, 2023.   \nSong, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.   \nTenenbaum, J. B., Silva, V. d., and Langford, J. C. A global geometric framework for nonlinear dimensionality reduction. science, 290(5500):2319-2323, 2000.   \nVincent, P. A connection between score matching and denoising autoencoders. Neural computation, 23(7):1661-1674, 2011.   \nWang, P., Zhang, H., Zhang, Z., Chen, S., Ma, Y., and Qu, Q. Diffusion models learn low-dimensional distributions via subspace clustering. arXiv preprint arXiv:2409.02426, 2024.   \nXiang, C., Bao, F., Li, C., Su, H., and Zhu, JI. A closer look at parameter-efficient tuning in diffusion models. arXiv preprint arXiv:2303.18181, 2023.   \nYuan, H., Huang, K., Ni, C., Chen, M., and Wang, M. Reward-directed conditional diffusion: Provable distribution estimation and reward improvement. arXiv preprint arXiv:2307.07055, 2023.   \nZhang, Q.and Chen, Y. Fast sampling of diffusion models with exponential integrator. arXiv preprint arXiv:2204.13902, 2022.   \nZhu, J., Ma, H., Chen, J., and Yuan, J. Domainstudio: Fine-tuning diffusion models for domain-driven image generation using limited data. arXiv preprint arXiv:2306.14153, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "AThe Neural Network Structure ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this section, we introduce the multi-layer ReLU network ${\\bf{f}}_{\\theta}\\in\\mathrm{NN}\\left(L,M^{\\prime},J,K_{1},\\kappa,\\gamma,\\gamma_{t}\\right)$ in $S_{\\mathrm{NN}}$ We note that the following setting is exactly the same as the one in Chen et al. (2023c), and we show the structure for completeness. We denote by N $\\lceil\\mathrm{N}\\left(L,M^{\\prime},J,K_{1},\\kappa,\\gamma,\\gamma_{t}\\right)$ the following neural network: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname{NN}\\left(L,M^{\\prime},J,K_{1},\\kappa,\\gamma,\\gamma_{t}\\right)=\\{\\mathbf{f}(Z,t)=W_{L}\\sigma\\left(\\cdot\\cdot\\sigma\\left(W_{1}\\left[Z^{\\top},t\\right]^{\\top}+\\mathbf{b}_{1}\\right)\\cdot\\cdot\\cdot\\right)+\\mathbf{b}_{L}:}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\mathrm{network~width~bounded~by~}M^{\\prime},\\underset{z,t}{\\operatorname{sup}}\\left\\|\\mathbf{f}(z,t)\\right\\|_{2}\\leq K_{1},}\\\\ &{\\qquad\\qquad\\qquad\\operatorname*{max}\\left\\{\\left\\|\\mathbf{b}_{i}\\right\\|_{\\infty},\\left\\|W_{i}\\right\\|_{\\infty}\\right\\}\\leq\\kappa\\,\\mathrm{for}~i=1,\\ldots,L,}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\left[\\left\\|W_{i}\\right\\|_{0}+\\left\\|\\mathbf{b}_{i}\\right\\|_{0}\\right)\\leq J,}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\left\\|\\mathbf{f}\\left(Z_{1},t\\right)-\\mathbf{f}\\left(Z_{2},t\\right)\\right\\|_{2}\\leq\\gamma\\left\\|Z_{1}-Z_{2}\\right\\|_{2}\\mathrm{~for~any~}t\\in[0,T],}\\\\ &{\\qquad\\qquad\\qquad\\quad\\left\\|\\mathbf{f}\\left(Z,t_{1}\\right)-\\mathbf{f}\\left(Z,t_{2}\\right)\\right\\|_{2}\\leq\\gamma_{t}\\left\\|t_{1}-t_{2}\\right\\|\\mathrm{~for~any~}Z\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\sigma$ is the ReLU activation function. Given an network accuracy $\\epsilon>0$ , the parameters is defined by ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L=\\mathcal{O}\\left(\\log\\frac{1}{\\epsilon}+d\\right),K_{1}=\\mathcal{O}\\left(2d^{2}\\log\\left(\\frac{d}{\\delta\\epsilon}\\right)\\right),}\\\\ &{M^{\\prime}=\\mathcal{O}\\left((1+\\beta)^{d}T\\tau d^{d/2+1}\\epsilon^{-(d+1)}\\log^{d/2}\\left(\\frac{d}{\\delta\\epsilon}\\right)\\right),}\\\\ &{J=\\mathcal{O}\\left((1+\\beta)^{d}T\\tau d^{d/2+1}\\epsilon^{-(d+1)}\\log^{d/2}\\left(\\frac{d}{\\delta\\epsilon}\\right)\\left(\\log\\frac{1}{\\epsilon}+d\\right)\\right),}\\\\ &{\\kappa=\\mathcal{O}\\left(\\operatorname*{max}\\left\\{2(1+\\beta)\\sqrt{d\\log\\left(\\frac{d}{\\delta\\epsilon}\\right)},T\\tau\\right\\}\\right),}\\\\ &{\\gamma=10d(1+\\beta),\\gamma_{t}=10\\tau\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tau=\\operatorname*{sup}_{A\\in\\{A_{s},A_{t a}\\}}\\operatorname*{sup}_{t\\in[\\delta,T]}\\operatorname*{sup}_{\\lVert z\\rVert_{\\infty}\\leq\\sqrt{d\\log\\frac{d}{\\delta\\epsilon}}}\\Big\\lVert\\frac{\\partial}{\\partial t}\\left[\\sigma_{t}A\\nabla\\log q_{t}^{\\mathrm{LD}}\\left(z\\right)\\right]\\Big\\rVert_{2}.}\\end{array}\n$$where ", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "B The Proof of the Approximation Error Bound ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Let $(\\widehat{V}_{s},\\widehat{\\theta})$ be the minimizer of the pre-trained objective function. The few-shot diffusion model freezes the bottleneck network and fine-tunes $V_{t a}\\in\\mathbb{R}^{D\\times d}$ to obtain the minimizer $(\\widehat{V}_{t a},\\widehat{\\theta})$ of the few-shot objective function. As the first step, we show that with the bottleneck parameterized by $\\widehat{\\theta}$ there also exists a solution $(\\bar{V}_{t a},\\widehat{\\theta})\\in\\mathcal{Q}_{N N}(\\widehat{\\theta})$ achieve the $\\epsilon^{2}+n_{s}^{-2/d}$ error bound. ", "page_idx": 12}, {"type": "text", "text": "Lemma B.1. If $\\dot{\\epsilon}\\leq n_{s}^{-\\frac{1-\\alpha(n_{s})}{d+5}}$ ,where $\\begin{array}{r}{\\alpha(n)=\\frac{d\\log\\log n}{\\log n}}\\end{array}$ dloglog n, then with probability 1 - o1,there exists a solution $(\\bar{V}_{t a},\\hat{\\theta})\\in\\mathcal{Q}_{N N}(\\hat{\\theta})$ suchthat ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\int_{\\delta}^{T}\\mathbb{E}_{X\\sim q_{t}^{t a}}\\left[\\left\\Vert\\mathbf{s}_{\\tilde{V}_{t a},\\hat{\\theta}}(X,t)-\\nabla\\log q_{t}^{t a}(X)\\right\\Vert_{2}^{2}\\right]\\mathrm{d}t\\leq O\\left(\\frac{d}{\\delta}\\epsilon^{2}+\\frac{(T-\\delta)(d+C_{Z})d^{2}\\beta^{2}}{\\delta^{2}c_{0}}n_{s}^{-\\frac{2-2\\alpha(n_{s})}{d+5}}\\log\\epsilon^{2}\\right)\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof. As shown in Theorem 1 of Chen et al. (2023c), there exists a solution $(\\bar{V}_{s},\\bar{\\theta})$ in $\\mathcal{S}_{N N}$ such that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{s}_{\\bar{V}_{s},\\bar{\\theta}}(\\cdot,t)-\\nabla\\log q_{t}^{s}(\\cdot)\\right\\|_{2}\\leq\\frac{\\sqrt{d}+1}{\\sigma_{t}^{2}}\\epsilon\\,,\\forall t\\in[\\delta,T]\\,.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Hence, we do the following decomposition: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\mathbf{s}_{\\bar{V}_{t a},\\hat{\\theta}}(\\cdot,t)-\\nabla\\log q_{t}^{t a}(\\cdot)\\right\\|_{2}^{2}\\lesssim\\left\\|\\mathbf{s}_{\\bar{V}_{t a},\\bar{\\theta}}(\\cdot,t)-\\nabla\\log q_{t}^{t a}(\\cdot)\\right\\|_{2}^{2}+\\left\\|\\mathbf{s}_{\\bar{V}_{t a},\\hat{\\theta}}(\\cdot,t)-\\mathbf{s}_{\\bar{V}_{t a},\\bar{\\theta}}(\\cdot,t)\\right\\|_{2}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "For the encoder and decoder layer, we choose $\\bar{V}_{t a}\\,=\\,A_{t a}$ . The first term is bounded due to the construction of the neural network. We first show that $\\mathbf{s}_{\\bar{V}_{t a},\\bar{\\theta}}$ is $\\epsilon$ -close to the true score function $\\nabla\\log{q_{t}^{t a}}$ : Since the encoder and decoder have been chosen, we only need to focus on the latent bottleneck. For the latent bottleneck, we need to use $\\mathbf{f}_{\\theta}(Z,t)$ to approximate ground-truth function $h(Z,t)=\\sigma_{t}^{2}\\nabla\\log q_{t}^{\\mathrm{LD}}(Z)+Z$ for $Z\\in\\mathbb{R}^{d}$ . Chen et al. (2023c) show that for any latent variable $Z^{\\prime}\\in\\mathbb{R}^{d}$ with subgaussian tail, we have that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|h\\left(Z^{\\prime},t\\right)-\\mathbf{f}_{\\bar{\\theta}}\\left(Z^{\\prime},t\\right)\\|_{L^{2}\\left(q_{t}^{\\mathrm{LD}}\\right)}\\leq(\\sqrt{d}+1)\\epsilon\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then, we have that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{s}_{\\bar{V}_{t a},\\bar{\\theta}}(\\cdot,t)-\\nabla\\log q_{t}^{t a}(\\cdot)\\right\\|_{2}^{2}\\leq\\frac{d}{\\sigma_{t}^{4}}\\epsilon^{2}\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "For the second term, we know that with probability $1-\\delta_{1}$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int_{\\delta}^{T}\\mathbb{E}_{X\\sim q_{t}}\\left[\\left\\|\\mathbf{s}_{\\bar{V}_{t a},\\bar{\\theta}}(\\cdot,t)-\\mathbf{s}_{\\bar{V}_{t a},\\hat{\\theta}}(\\cdot,t)\\right\\|_{2}^{2}\\right]}\\\\ &{\\leq\\int_{\\delta}^{T}\\frac{1}{\\sigma_{t}^{4}}\\mathbb{E}_{Z\\sim q_{t}^{\\mathrm{LD}}}\\left[\\|\\mathbf{f}_{\\hat{\\theta}}(Z)-\\mathbf{f}_{\\bar{\\theta}}(Z)\\|_{2}^{2}\\right]\\mathrm{d}t}\\\\ &{\\leq O\\left(\\frac{T-\\delta}{\\delta^{2}}(\\frac{\\delta}{c_{0}}\\left((T-\\log\\delta)\\,d\\cdot\\gamma^{2}+d\\beta\\right)+\\frac{\\gamma^{2}\\cdot C_{Z}}{c_{0}})n_{s}^{-\\frac{2-2\\alpha(n_{s})}{d+5}}\\right)\\log\\left(\\frac{1}{\\delta_{1}}\\right)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\begin{array}{r}{\\alpha(n)\\,=\\,\\frac{d\\log\\log n}{\\log n}}\\end{array}$ The first inequlity folows $A_{t a}$ is a matrix with orthonormal colums. Since we assume e < nd , the network has good enough ability to obtan an accurate enough $\\hat{\\theta}$ Hence, we can use Appendix C.4 of Chen et al. (2023c) to obtain the second inequality. Since we directly use the true matrix $\\bar{V}_{t a}=A_{t a}$ instead of the approximate $\\hat{V}_{t a}$ , we do not need orthogonal transformation and can choose $U\\,=\\,I_{d}$ in the Appendix C.4 of Chen et al. (2023c). Then, we complete our proof. \u25a0 ", "page_idx": 13}, {"type": "text", "text": "To prove Theorem 4.3, we need to do the following decomposition for the population loss of the target datasets ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{\\dot{\\Xi}}_{t a}\\left(\\mathbf{s}_{\\widehat{\\hat{V}}_{t a},\\widehat{\\theta}}\\right)}\\\\ &{=\\mathcal{L}_{t a}\\left(\\mathbf{s}_{\\widehat{\\hat{V}}_{t a},\\widehat{\\theta}}\\right)-(1+a)\\widehat{\\mathcal{L}}_{t a}\\left(\\mathbf{s}_{\\widehat{\\hat{V}}_{t a},\\widehat{\\theta}}\\right)+(1+a)\\widehat{\\mathcal{L}}_{t a}\\left(\\mathbf{s}_{\\widehat{\\hat{V}}_{t a},\\widehat{\\theta}}\\right)}\\\\ &{\\leq\\underbrace{\\mathcal{L}_{t a}^{\\mathrm{tranc}}\\left(\\mathbf{s}_{\\widehat{\\hat{V}}_{t a},\\widehat{\\theta}}\\right)-(1+a)\\widehat{\\mathcal{L}}_{t a}^{\\mathrm{tranc}}\\left(\\mathbf{s}_{\\widehat{\\hat{V}}_{t a},\\widehat{\\theta}}\\right)}_{(a)}+\\underbrace{\\mathcal{L}_{t a}\\left(\\mathbf{s}_{\\widehat{\\hat{V}}_{t a},\\widehat{\\theta}}\\right)-\\mathcal{L}_{t a}^{\\mathrm{tranc}}\\left(\\mathbf{s}_{\\widehat{\\hat{V}}_{t a},\\widehat{\\theta}}\\right)}_{(b)}+(1+a)\\underbrace{\\operatorname*{inf}_{s_{\\underbrace{V_{t a},\\widehat{\\theta}}\\in\\mathbb{Q}(\\widehat{\\theta})}}\\widehat{\\mathcal{L}}_{t a}}_{(b)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $a\\in(0,1)$ and $\\mathcal{L}_{t a}^{\\mathrm{trunc}}$ is defined as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{L}_{t a}^{\\mathrm{truc}}\\,\\left(\\mathbf{s}_{\\hat{V}_{t a},\\hat{\\theta}}\\right)=\\mathbb{E}_{x\\sim q_{0}}\\left[\\ell_{t a}^{\\mathrm{truc}}\\,\\left(x;\\mathbf{s}_{\\hat{V}_{t a},\\hat{\\theta}}\\right)\\right]=\\mathbb{E}_{x\\sim q_{0}}\\left[\\ell_{t a}\\left(x;\\mathbf{s}_{\\hat{V}_{t a},\\hat{\\theta}}\\right)\\mathbf{1}\\left\\{\\|x\\|_{2}\\leq R\\right\\}\\mathrm{d}t\\right]\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "In this  section, we  take $\\begin{array}{r l r}{R}&{{}=}&{\\mathcal{O}\\left(\\sqrt{d\\log d+\\log K_{1}+\\log\\frac{n_{t a}}{\\delta_{1}}}\\right)}\\end{array}$ to guarantee $\\mathbb{P}_{X_{t a,i}\\sim q^{t a}}$ $\\left.\\|X_{t a,i}\\|_{2}\\leq R\\right.$ for all $i=1,\\dots,n_{t a})\\geq1-\\delta_{1}$ , where $K_{1}$ is defined in Appendix A. ", "page_idx": 13}, {"type": "text", "text": "Term (a). Similar to Chen et al. (2023c), we define a function class $\\begin{array}{r l r}{\\mathcal{G}(\\hat{\\theta})}&{{}=}&{}\\end{array}$ $\\left\\{\\ell_{t a}^{\\mathrm{trunc}}\\left(\\cdot;\\mathbf s_{V,\\hat{\\theta}}\\right):\\mathbf s_{V,\\hat{\\theta}}\\in\\mathcal{Q}_{\\mathrm{NN}}(\\hat{\\theta})\\right\\}$ which is induced by ${\\mathcal{Q}}({\\widehat{\\theta}})$ . For the upper bound of $\\mathcal G(\\hat{\\theta})$ ,we directly use the augmentation of Chen et al. (2023c) to obtain that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\ell_{t a}^{\\mathrm{trunc}}\\left(X;\\mathbf{s}_{V,\\hat{\\theta}}\\right)\\leq\\mathcal{O}\\left(\\frac{K_{1}^{2}+R^{2}}{\\delta\\left(T-\\delta\\right)}\\right)\\,,\\mathrm{for~any~}\\mathbf{s}_{V,\\hat{\\theta}}\\in\\mathcal{Q}_{\\mathrm{NN}}(\\hat{\\theta})\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then, by using Lemma D.1, we know that with probability $1-\\delta_{1}$ , term (a) is bounded by ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{O}\\left(\\frac{(1+3/a)\\left((1+\\beta)^{2}d^{2}\\log\\frac{d}{\\delta\\epsilon}+\\log\\frac{n_{t a}}{\\delta}\\right)}{n_{t a}\\delta\\left(T-\\delta\\right)}\\log\\frac{\\mathcal{N}\\left(\\tau_{1},\\mathcal{G}(\\hat{\\theta}),\\|\\cdot\\|_{\\infty}\\right)}{\\delta_{1}}+\\tau_{1}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "To bound the above term, we need to calculate the covering number of $\\mathcal G(\\hat{\\theta})$ \uff0cwhich is related to a $\\iota$ covering of $\\mathcal{Q}_{N N}({\\hat{\\theta}})$ .Suppose that given ${\\bf s}_{V_{1},\\hat{\\theta}}$ and ${\\bf s}_{V_{2},\\hat{\\theta}}$ with $\\begin{array}{r}{\\operatorname*{sup}_{\\|\\boldsymbol{x}\\|_{2}\\leq3R+\\sqrt{D\\log D},t\\in[\\delta,T]}\\left\\|\\mathbf{s}_{V_{1},\\hat{\\theta}}(\\boldsymbol{x},t)-\\mathbf{s}_{V_{2},\\hat{\\theta}}(\\boldsymbol{x},t)\\right\\|_{2}\\leq\\iota,}\\end{array}$ we need to bound ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\|\\ell_{t a}^{\\mathrm{trunc}}\\ \\left(\\cdot;\\mathbf{s}_{V_{1},\\hat{\\theta}}\\right)-\\ell_{t a}^{\\mathrm{trunc}}\\ \\left(\\cdot;\\mathbf{s}_{V_{2},\\hat{\\theta}}\\right)\\right\\|_{\\infty}\\ .\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By using the same calculation compared to Term (A) of Chen et al. (2023c), we know that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\vert\\ell_{t a}^{\\mathrm{uno}}\\,\\left(\\cdot;{\\bf s}_{V_{1},\\hat{\\theta}}\\right)-\\ell_{t a}^{\\mathrm{tranc}}\\,\\left(\\cdot;{\\bf s}_{V_{2},\\hat{\\theta}}\\right)\\right\\vert\\right\\vert_{\\infty}\\le\\mathcal{O}\\left(\\frac{\\iota}{T-\\delta}(K_{1}+R)\\log\\frac{T}{\\delta}+\\frac{4K_{1}(K_{1}+R)}{\\delta\\left(T-\\delta\\right)}(R/D)^{D-2}\\,\\mathrm{ence}\\right)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The above inequality indicates that a $\\iota$ -covering of $\\mathcal{Q}_{N N}(\\hat{\\theta})$ in $L_{2}$ norm leads a $\\begin{array}{r}{\\frac{\\iota}{T-\\delta}(K_{1}\\!+\\!R)\\log\\frac{T}{\\delta}+}\\end{array}$ $\\begin{array}{r}{\\frac{4K_{1}(K_{1}+R)}{\\delta(T-\\delta)}(R/D)^{D-2}\\exp\\left(-\\frac{1}{\\sigma_{t}^{2}}R^{2}\\right)}\\end{array}$ covering ofof $\\mathcal G(\\hat{\\theta})$ .m\u00e4 $L_{\\infty}$ norm. ", "page_idx": 14}, {"type": "text", "text": "By taking R=0(\u221alogd+logK+log), K =O(2d log (), = mar-) we know that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\tau_{1}\\leq\\frac{d^{2}}{n_{t a}\\delta}\\log(\\frac{T}{\\delta})\\log(\\frac{d}{\\delta\\epsilon})\\log(\\frac{n_{t a}}{\\delta})\\,,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which indicates with probability $1-\\delta_{1}$ , term (a) is bounded by ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\gamma\\left(\\frac{(1+3/a)\\left((1+\\beta)^{2}d^{2}\\log\\frac{d}{\\delta\\epsilon}+\\log\\frac{n_{t a}}{\\delta}\\right)}{n_{t a}\\delta\\left(T-\\delta\\right)}\\log\\frac{N\\left(\\frac{1}{n_{t a}\\delta(T-\\delta)},Q_{\\mathrm{NN}}(\\hat{\\theta}),\\|\\cdot\\|_{2}\\right)}{\\delta_{1}}+\\frac{d^{2}}{n_{t a}\\delta}\\log(\\frac{T}{\\delta})\\log\\frac{1}{\\delta}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "After that, we need to determine the covering number of $\\mathcal{Q}_{N N}(\\hat{\\theta})$ with a truncated $X$ to bound term (a). ", "page_idx": 14}, {"type": "text", "text": "Lemma B.2. The logarithmic covering number of $\\mathcal{Q}_{N N}(\\theta)$ for $\\|X\\|_{2}\\le3R+\\sqrt{D\\log D},t\\in[\\delta,T]$ is ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\log\\mathcal{N}\\left(\\iota,\\mathcal{Q}_{N N}(\\theta),\\|\\cdot\\|_{2}\\right)=\\mathcal{O}\\left(2D d\\cdot\\log\\left(1+\\frac{6K\\gamma\\sqrt{d}(3R+\\sqrt{D\\log D})}{\\delta\\iota}\\right)\\right)\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. Suppose that there exists two orthonormal column matrix $V_{1},V_{2}$ such that $\\lVert V_{1}-V_{2}\\rVert_{\\mathrm{F}}\\leq\\delta_{2}$ then we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{|X|_{2}\\leq3R+\\sqrt{D\\log D},t\\in[\\delta,T]}{\\operatorname*{sup}}\\left\\|s_{V_{1},\\theta}(X,t)-\\mathbf{s}_{V_{2},\\theta}(X,t)\\right\\|_{2}}\\\\ &{=\\frac{1}{\\sigma_{t}^{2}}\\underset{|X|_{2}\\leq3R+\\sqrt{D\\log D},t\\in[\\delta,T]}{\\operatorname*{sup}}\\left[\\left\\|V_{1}\\mathbf{f}_{\\theta}\\left(V_{1}^{\\top}X,t\\right)-V_{1}\\mathbf{f}_{\\theta}\\left(V_{2}^{\\top}X,t\\right)\\right\\|_{2}+\\left\\|V_{1}\\mathbf{f}_{\\theta}\\left(V_{2}^{\\top}x,t\\right)-V_{2}\\mathbf{f}_{\\theta}\\left(V_{2}^{\\top}X,t\\right)\\right\\|_{2}\\right]}\\\\ &{\\leq\\frac{1}{\\sigma_{t}^{2}}\\left(\\gamma\\delta_{1}\\sqrt{d}(3R+\\sqrt{D\\log D})+\\delta_{1}K\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For set $\\left\\{V\\in\\mathbb{R}^{D\\times d}:\\|V\\|_{2}\\leq1\\right\\}$ the $\\delta_{2}$ -covering umber is $\\left(1+2\\frac{\\sqrt{d}}{\\delta_{2}}\\right)^{D d}$ . Then we know that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\log\\mathcal{N}\\left(\\iota,S_{\\mathrm{NN}},\\|\\cdot\\|_{2}\\right)=\\mathcal{O}\\left(2D d\\cdot\\log\\left(1+\\frac{6K\\gamma\\sqrt{d}(3R+\\sqrt{D\\log D})}{\\delta\\iota}\\right)\\right)\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Term (b). For the term (b), the proof of Theorem 2 in Chen et al. (2023c) shows that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{Term}\\left({\\bf b}\\right)\\le\\frac{1}{n_{t a}\\delta\\left(T-\\delta\\right)}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Term (c). For the term (c), we know that it is bounded by the constructed solution $(\\bar{V}_{t a},\\hat{\\theta})$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{s_{V_{t a},\\delta}\\in Q(\\delta)}\\widehat{\\mathcal{L}}_{t a}\\left(\\mathrm{s}_{V_{t a},\\hat{\\theta}}\\right)\\leq\\underbrace{\\widehat{\\mathcal{L}}_{t a}\\left(\\mathbf{s}_{\\bar{V}_{t a},\\hat{\\theta}}\\right)-(1+a)\\mathcal{L}^{\\mathrm{tmoc}}\\left(\\mathbf{s}_{\\bar{V}_{t a},\\hat{\\theta}}\\right)}_{(C_{1})}+(1+a)\\underbrace{\\mathcal{L}^{\\mathrm{trux}}\\left(\\mathbf{s}_{\\bar{V}_{t a},\\hat{\\theta}}\\right)}_{(C_{2})}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For the term (C.1), since $\\mathbf{s}_{\\bar{V}_{t a},\\hat{\\theta}}$ is a fixed function, we directly use the results of (Chen et al., 2023): ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{Term}(C_{1})\\leq O\\left(\\frac{(1+6/a)\\left((1+\\beta)^{2}d^{2}\\log\\frac{d}{\\delta\\epsilon}+\\log\\frac{n}{\\delta}\\right)}{n_{t a}\\delta\\left(T-\\delta\\right)}\\log\\frac{1}{\\delta_{1}}\\right)\\,,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with probability $1-\\delta_{1}$ . For the term (C.2), we know that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{t a}^{\\mathrm{tunc}}\\left(\\mathbf{s}_{\\bar{V}_{t a},\\hat{\\theta}}\\right)\\leq\\mathscr{L}\\left(\\mathbf{s}_{\\bar{V}_{t a},\\hat{\\theta}}\\right)=\\!\\frac{1}{T-\\delta}\\int_{\\delta}^{T}\\left\\|\\mathbf{s}_{\\bar{V}_{t a},\\hat{\\theta}}(\\cdot,t)-\\nabla\\log q_{t}^{t a}(\\cdot)\\right\\|_{L^{2}(q_{t})}^{2}\\,\\mathrm{d}t}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\underbrace{\\mathcal{L}\\left(\\mathbf{s}_{\\bar{V}_{t a},\\hat{\\theta}}\\right)-\\frac{1}{T-\\delta}\\int_{\\delta}^{T}\\left\\|\\mathbf{s}_{\\bar{V}_{t a},\\hat{\\theta}}(\\cdot,t)-\\nabla\\log q_{t}^{t a}(\\cdot)\\right\\|_{L^{2}(q_{t})}^{2}\\mathrm{d}t}_{(\\mathcal{L})}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "As we show in Section 3.2, the two terms in $\\mathcal{E}$ are both the score matching objective function and have a constant different $E$ , which is independent of the trainable parameters $(V,\\theta)$ . We denote by this ifference $E$ and $\\begin{array}{r}{F=\\frac{(d+C_{Z})d^{2}\\beta^{2}}{\\delta^{2}c_{0}}}\\end{array}$ .With probabity $1-\\delta_{1}$ Lemma B.1 shows tha tem (C.2) is bounded by ", "page_idx": 15}, {"type": "equation", "text": "$$\nO\\left(\\frac{d}{\\delta(T-\\delta)}\\epsilon^{2}+F n_{s}^{-\\frac{2-2\\alpha(n_{s})}{d+5}}\\log\\left(\\frac{1}{\\delta_{1}}\\right)\\right)+E\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "After bounding these three terms, we prove Theorem 4.3. ", "page_idx": 15}, {"type": "text", "text": "Theorem .3 Let $\\begin{array}{r}{\\alpha(n)=\\frac{d\\log\\log n}{\\log n}}\\end{array}$ \uff0c $\\begin{array}{r}{F=\\frac{(d+C_{Z})d^{2}\\beta^{2}}{\\delta^{2}c_{0}}}\\end{array}$ and evork parameter $\\epsilon=n_{t a}^{-1/2}$ Assunme Assumption 3.1 4.1 42 and $n_{t a}^{\\frac{d+5}{4(1-\\alpha(n_{s}))}}\\geq n_{s}$ . Then, with probability $1-\\delta_{1}$ the following inequaliy holds (hiding logarithmic factors) ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{1}{\\Gamma-\\delta}\\int_{\\delta}^{T}\\left\\|\\mathbf{s}_{\\widehat{V}_{t a},\\widehat{\\theta}}(\\cdot,t)-\\nabla\\log q_{t}^{t a}(\\cdot)\\right\\|_{L^{2}(q_{t}^{t a})}^{2}\\,\\mathrm{d}t\\leq\\tilde{O}\\left(\\left(\\frac{(1+\\beta)^{2}D d^{3}}{\\delta\\left(T-\\delta\\right)\\sqrt{n_{t a}}}+F n_{s}^{-\\frac{2-2\\alpha(n_{s})}{d+5}}\\right)\\log\\left(\\frac{1}{\\delta_{1}}\\right)\\right)\\,\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Equipped with the bound of the term (a), (b), and (c) and hiding the logarithmic term (except for the covering number term), with probability $1-\\delta_{1}$ ,wehave that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left({\\bf s}_{\\tilde{V}_{t a},\\delta}\\right)\\leq(1+a)^{2}E+\\tilde{O}\\left(\\frac{((1+\\beta)^{2}d^{2}\\log\\frac{d}{\\delta\\epsilon}+\\log\\frac{n_{t a}}{\\delta})}{a\\delta\\,(T-\\delta)\\,n_{t a}}\\log\\frac{N\\left(\\frac{1}{n_{t a}\\delta(T-\\delta)},\\,Q_{\\mathrm{NN}}(\\hat{\\theta}),\\,\\|\\cdot\\|_{2}\\right)}{\\delta_{1}}+\\frac{1}{\\delta_{2}}\\right)}\\\\ &{\\qquad\\qquad+\\,\\frac{1}{n_{t a}\\delta(T-\\delta)}+\\frac{d}{\\delta(T-\\delta)}\\epsilon^{2}+F n_{s}^{-\\frac{2-2\\alpha(n_{s})}{d+5}}\\log\\left(\\frac{1}{\\delta_{1}}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $\\begin{array}{r}{\\mathcal{L}_{t a}\\left(\\mathbf{s}_{\\hat{V}_{t a},\\hat{\\theta}}\\right)-E=\\frac{1}{T-\\delta}\\int_{\\delta}^{T}\\left\\|\\mathbf{s}_{\\hat{V}_{t a},\\hat{\\theta}}(\\cdot,t)-\\nabla\\log q_{t}^{t a}(\\cdot)\\right\\|_{L^{2}(q_{t})}^{2}}\\end{array}$ , we have that the following inequality when choosing $a=\\epsilon^{2}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\displaystyle-}{\\delta}\\int_{\\delta}^{T}\\left\\|\\mathrm{s}_{\\bar{\\ell}_{u,\\delta}}\\dot{(\\cdot,t)}-\\nabla\\log q_{t}^{\\mu}\\dot{(\\cdot)}\\right\\|_{L^{2}(q_{t})}^{2}}\\\\ &{\\displaystyle\\mathfrak{I}\\left(\\frac{(1+\\beta)^{2}d^{2}}{\\epsilon^{2}\\delta\\,(T-\\delta)\\,n_{t a}}\\log\\frac{{\\cal N}\\left(\\frac{1}{n_{t a}\\delta(T-\\delta)},Q_{\\mathrm{XN}}(\\hat{\\theta}),\\|\\cdot\\|_{2}\\right)}{\\delta_{1}}+\\frac{d}{\\delta(T-\\delta)}\\epsilon^{2}+\\frac{d^{2}}{n_{t a}\\delta}+F n_{s}^{-\\frac{2-2\\alpha(n_{s})}{d+5}}\\log\\left(\\frac{1}{\\delta_{1}}\\right)\\right)}\\\\ &{\\displaystyle\\mathfrak{I}\\left(\\frac{(1+\\beta)^{2}D d^{3}}{\\delta\\,(T-\\delta)\\,n_{t a}\\epsilon^{2}}\\log\\left(1+\\frac{6K\\gamma\\sqrt{d}(3R+\\sqrt{D\\log D})n_{t a}\\delta(T-\\delta)}{\\delta_{1}}\\right)+\\frac{d}{\\delta(T-\\delta)}\\epsilon^{2}+\\frac{d^{2}}{n_{t a}\\delta}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+F n_{s}^{-\\frac{2-2\\alpha(n_{s})}{d+5}}\\log\\left(\\frac{1}{\\delta_{1}}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the second inequality follows the convering number of $\\mathcal{Q}_{N N}({\\hat{\\theta}})$ for $\\|X\\|\\leq3R+{\\sqrt{D\\log D}}$ With $\\begin{array}{r}{R=\\mathcal{O}\\left(\\sqrt{d\\log d+\\log K_{1}+\\log\\frac{n_{t a}}{\\delta_{1}}}\\right)}\\end{array}$ and the network parameters is defined in Appendix A. Finally, we choose $\\epsilon^{2}=1/\\sqrt{n_{t a}}$ , then we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{1}{\\Gamma-\\delta}\\int_{\\delta}^{T}\\left\\|\\mathbf{s}_{\\hat{V}_{t a},\\hat{\\theta}}(\\cdot,t)-\\nabla\\log q_{t}^{t a}(\\cdot)\\right\\|_{L^{2}(q_{t})}^{2}\\leq\\tilde{O}\\left(\\frac{(1+\\beta)^{2}D d^{3}}{\\delta\\left(T-\\delta\\right)\\sqrt{n_{t a}}}\\log\\left(\\frac{1}{\\delta_{1}}\\right)+\\frac{d^{2}}{n_{t a}\\delta}+F n_{s}^{-\\frac{2-2\\alpha(2)}{d+5}}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "As we require in Lemma B.1, we need $\\epsilon=1/n_{t a}^{1/4}\\,\\leq\\,n_{s}^{-\\frac{1-\\alpha(n_{s})}{d+5}}$ , which indicates nta $n_{t a}^{\\frac{d+5}{4(1-\\alpha(n_{s}))}}\\geq$ $n_{s}$ ", "page_idx": 16}, {"type": "text", "text": "C The Proof of the Optimization Problem ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1  The Pre-trained Diffusion Model Generate Accurate Enough Latent Distribution ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Since we need to use the approximated latent distribution in the few-shot phase, we show that the pre-trained diffusion models with solution $(\\widehat{V}_{s},\\widehat{\\theta})$ can generate an accurate enough latent distribution. As shown in Section 5, when considering the optimization perspective of diffusion models, we assume the latent distribution is a Gaussian distribution $q_{z}=\\mathcal{N}(0,\\Sigma)$ With $\\Sigma=\\operatorname{diag}\\left(\\lambda_{1}^{2},\\dots,\\lambda_{d}^{2}\\right)\\succ0$ Yuan et al. (2023) show that in the setting, the approximation error bound (Lemma D.3) for the target dataset is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{1}{\\Gamma-\\delta}\\int_{\\delta}^{T}\\Big\\|\\nabla\\log q_{t}^{s}\\left(\\cdot\\right)-\\mathbf{s}_{\\hat{V}_{s},\\hat{\\theta}}\\left(\\cdot,t\\right)\\Big\\|_{L^{2}(q_{t}^{s})}^{2}\\,\\mathrm{d}t\\leq O\\left(\\frac{1}{\\delta}\\sqrt{\\frac{\\left(d^{2}+D d\\right)\\log\\left(D d n_{s}\\right)\\left(d^{2}\\vee D\\right)\\log\\frac{1}{\\delta_{1}}}{n_{s}}}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "To generate latent distribution, we first introduce the reverse process in the latent space. The introduction mainly follows the outline of Appendix C.2 of Chen et al. (2023c). For $X_{t}$ ,wecan do the following decomposition: $X_{t}=A_{s}Z_{t}+X_{t,\\perp}$ ,where $Z_{t}=A_{s}^{\\top}X_{t}$ .With $Z_{t}^{\\leftarrow}=Z_{T-t}$ , the reverse process in the latent space is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{d}Z_{t}^{\\leftarrow}=\\left[\\frac{1}{2}Z_{t}^{\\leftarrow}+\\nabla\\log q_{T-t}^{\\mathrm{{LD}}}\\left(Z_{t}^{\\leftarrow}\\right)\\right]\\mathrm{d}t+\\mathrm{d}\\left(A_{s}^{\\top}B_{t}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "As shown in Theorem 3 of Chen et al. (2023c), the solution $(\\widehat{V}_{s},\\widehat{\\theta})$ of the pre-trained diffusion models only guarantee $\\|\\widehat{V}_{s}\\widehat{V}_{s}^{\\top}-A_{s}A_{s}^{\\top}\\|_{F}^{2}$ is small instead of $\\|\\widehat{V}_{s}-A_{s}\\|_{F}^{2}$ is small. Hence, Theorem 3 of Chen et al. (2023c) assume there exists an orthogonal matrix $\\bar{U_{s}}\\in\\bar{\\mathbb{R}}^{d\\times d}$ and do an orthogonal transformation on $\\widehat{V}_{s}$ to obtain $\\widehat{V}_{s}U_{s}$ , which can guarantee $\\|\\widehat{V}_{s}U_{s}-A_{s}\\|_{F}^{2}$ is small. After such orthogonal transformation, the reverse process with an approximated score function and an approximated reversing beginning distribution $\\widetilde{Z}_{0}^{\\leftarrow,r}\\sim\\mathcal{N}\\left(0,I_{d}\\right)$ is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{d}\\widetilde{Z}_{t}^{\\leftarrow,r}=\\left[\\frac{1}{2}\\widetilde{Z}_{t}^{\\leftarrow,r}+\\mathbf{s}_{U_{s},\\widehat{\\theta}}^{\\mathrm{LD}}\\left(\\widetilde{Z}_{t}^{\\leftarrow,r},T-t\\right)\\right]\\mathrm{d}t+\\mathrm{d}\\left(U_{s}^{\\top}\\widehat{V}_{s}^{\\top}B_{t}\\right)\\,,\\widetilde{Z}_{0}^{\\leftarrow,r}\\sim\\mathrm{N}\\left(0,I_{d}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\widetilde{Z}_{t}^{\\leftarrow,r}=U_{s}^{\\top}\\widetilde{Z}_{t}^{\\leftarrow}\\mathrm{and}\\;\\mathbf{s}_{U_{s},\\widehat{\\theta}}^{\\mathrm{LD}}(Z,t)=\\frac{1}{\\sigma_{t}^{2}}\\left[-Z+U_{s}^{\\top}\\mathbf{f}_{\\widehat{\\theta}}(U_{s}Z,t)\\right]\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, we discretize the above process with the exponential integrator (El) discretization scheme (Zhang & Chen, 2022) to obtain an implementable algorithm: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{d}\\widetilde{Z}_{t}^{\\leftarrow,r}=\\left[\\frac{1}{2}\\widetilde{Z}_{t}^{\\leftarrow,r}+\\mathbf{s}_{U_{s},\\widehat{\\theta}}^{\\mathrm{LD}}\\left(\\widetilde{Z}_{k\\eta}^{\\leftarrow,r},T-t_{k}^{\\prime}\\right)\\right]\\mathrm{d}t+\\mathrm{d}\\left(U_{s}^{\\top}\\widehat{V}_{s}^{\\top}B_{t}\\right)\\,,\\mathrm{where~}t\\in\\left[t_{k}^{\\prime},t_{k+1}^{\\prime}\\right].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "As shown in Appendix C.4 of Chen et al. (2023c), if the target ground truth score function has a $L_{2}$ -accurate approximated score: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{1}{T-\\delta}\\int_{\\delta}^{T}\\left\\lVert\\mathbf{s}_{\\widehat{V}_{s},\\widehat{\\theta}}(\\cdot,t)-\\nabla\\log q_{t}^{s}(\\cdot)\\right\\rVert_{L^{2}(q_{t}^{s})}^{2}\\mathrm{d}t\\leq\\epsilon^{2}\\,,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "the latent score function also has an $L_{2}$ norm bound latent-score, which is determined by e: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\epsilon_{\\mathrm{latent-score}}=\\epsilon\\cdot{\\mathcal{O}}\\left(\\left[{\\frac{\\delta}{c_{0}}}\\left(\\left(T-\\log\\delta\\right)d\\cdot\\gamma^{2}+d\\beta\\right)+{\\frac{\\gamma^{2}\\cdot C_{Z}}{c_{0}}}\\right]\\right)\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The remaining term is to determined $\\epsilon$ . Since we assume Gaussian latent variable instead of subGaussian one. Hence, we do not use $\\epsilon$ in Chen et al. (2023c). We use $\\epsilon$ in Yuan et al. (2023) (Theorem 4.5 of Yuan et al. (2023)), which also considers Gaussian latent variable, to achieve the final results. ", "page_idx": 17}, {"type": "text", "text": "Finally, we have that with probability $1-\\delta_{1}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{T-\\delta}\\int_{\\delta}^{T}\\left\\|\\nabla\\log q_{t}^{\\mathrm{LD}}\\left(\\cdot\\right)-\\mathbf{s}_{U_{s},\\widehat{\\theta}}^{\\mathrm{LD}}(\\cdot,t)\\right\\|_{L^{2}(q_{t}^{\\mathrm{LD}})}^{2}\\mathrm{d}t}\\\\ &{\\leq O\\left(\\frac{d^{2}\\beta^{2}\\,\\left(d+\\lambda_{\\operatorname*{max}}^{2}\\right)}{\\lambda_{\\operatorname*{min}}\\delta}\\sqrt{\\frac{\\left(d^{2}+D d\\right)\\log\\left(D d n_{s}\\right)\\,\\left(d^{2}\\vee D\\right)\\log\\frac{1}{\\delta_{1}}}{n_{s}}}\\right)\\triangleq\\epsilon_{\\mathrm{latent}\\cdot\\mathrm{score}}^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let $p_{t}^{\\mathrm{LD}}$ be the distribution of the algorithm (the above discretization process). In the following lemma, we adopt Theorem 5 of Chen et al. (2023a) and show that the pre-trained diffusion model can obtain an accurate enough latent distribution with the above $L_{2}$ -accurate latent score function. ", "page_idx": 17}, {"type": "text", "text": "Lemma C.1. With Eaen-scoe defined in Equation (4), $\\begin{array}{r c l}{T}&{=}&{\\log\\left(\\frac{\\lambda_{m a x}^{2}+d}{\\epsilon_{l a t e n t\\cdot s c o r e}^{2}}\\right)}\\end{array}$ and $K\\quad=$ $\\begin{array}{r}{\\Theta\\left(\\frac{d^{2}(T+\\log(\\lambda_{m a x}^{2}))^{2}}{\\epsilon_{l a t e n t\\cdot s c o r e}^{2}}\\right)}\\end{array}$ ,by using the exponentially decreasing (then the constant) stepsize $h_{k}\\ =$ $\\begin{array}{r}{c\\operatorname*{min}\\left\\{\\operatorname*{max}\\left\\{t_{k},\\frac{1}{\\lambda_{m a x}^{2}}\\right\\},1\\right\\},c=\\frac{\\log(\\lambda_{m a x}^{2})+T}{K}}\\end{array}$ log(X2)+T, the results pP of samplig algorithm Equation (3) has the following guarantee with probability $1-\\delta_{1}$ (hiding the logarithmic factor): ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}\\left(q_{0}\\Vert\\hat{p}_{T}^{\\mathrm{LD}}\\right)\\leq\\widetilde{O}(\\epsilon_{l a t e n t\\cdot s c o r e}^{2})}\\\\ &{\\qquad\\qquad=\\widetilde{O}\\left(\\frac{d^{2}\\beta^{2}\\,\\left(d+\\lambda_{m a x}^{2}\\right)}{\\lambda_{m i n}\\delta}\\sqrt{\\frac{\\left(d^{2}+D d\\right)\\log\\left(D d n_{s}\\right)\\,\\left(d^{2}\\vee D\\right)\\log\\frac{1}{\\delta_{1}}}{n_{s}}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. The Theorem 5 of Chen et al. (2023a) show that if $\\nabla\\log{q_{0}^{\\mathrm{LD}}}$ .s $L$ Lipschitz, diffusion models with a $L_{2}$ accuraecangenerae $\\widehat{p}_{T}^{\\mathrm{LD}}$ which is close to $q_{0}$ .n $\\mathrm{KL}$ divergence.Since $q_{z}=\\mathcal{N}(0,\\Sigma)$ is easy to verify $L=\\lambda_{\\operatorname*{max}}^{\\breve{2}}$ . Then, we complete the proof. ", "page_idx": 17}, {"type": "text", "text": "C.2 The Closed-form Minimizer of Few-shot Diffusion Models ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "When the latent distribution is a Gaussian distribution $q_{z}=\\mathcal{N}(0,\\Sigma)$ with $\\Sigma=\\mathrm{diag}\\left(\\lambda_{1}^{2},\\dots,\\lambda_{d}^{2}\\right)\\succ$ O, the ground truth score function for the target dataset is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\nabla\\log q_{t}^{t a}(X)=-A_{t a}\\Sigma_{t}^{-1}A_{t a}^{\\top}X-{\\frac{1}{\\sigma_{t}^{2}}}\\left(I_{D}-A_{t a}A_{t a}^{\\top}\\right)X\\,,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\Sigma_{t}=\\operatorname{diag}\\left(\\dots,m_{t}^{2}\\lambda_{k}^{2}+\\sigma_{t}^{2},\\dots\\right)$ .Since the matrix $A_{t a}$ is independent of time $t$ , we fix a $t\\in[\\delta,T]$ and minimize the few-shot objective function at this time. With an approximated $\\widehat{\\Sigma}$ , which is learned by the pre-trained diffusion models, $\\mathbf{f}_{\\widehat{\\theta}}(Z,t)=(I_{d}-\\sigma_{t}^{2}\\widehat{\\Sigma}_{t}^{-1})Z$ , where $\\widehat{\\Sigma}_{t}=m_{t}^{2}\\widehat{\\Sigma}+\\sigma_{t}^{2}I_{d}$ Hence, the expected objective function for the few-shot diffusion models at a fixed time $t$ is: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{s}_{V_{t a},\\hat{\\theta}}\\in\\tilde{\\mathcal{Q}}_{N N}(\\hat{\\theta})}\\mathcal{L}_{t a,t}(\\mathbf{s}_{V_{t a},\\hat{\\theta}})=\\mathbb{E}_{X_{t a}\\sim q_{t a}}\\left[\\ell_{t}^{t a}\\left(X_{t a};\\mathbf{s}_{V_{t a},\\hat{\\theta}}\\right)\\right]\\,,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{Q}}_{\\mathrm{NN}}(\\theta)=\\left\\{\\mathbf{s}_{V,\\theta}(X,t)=\\frac{1}{\\sigma_{t}^{2}}V\\mathbf{f}_{\\theta}\\left(V^{\\top}X,t\\right)-\\frac{1}{\\sigma_{t}^{2}}X:V\\in\\mathbb{R}^{D\\times d}\\;\\mathrm{with}\\;\\mathrm{rank}(V)=d.\\right\\},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that the constraint $\\operatorname{rank}(V)=d$ is a weaker constraint than $V^{\\top}V=I_{d}$ since $\\operatorname{rank}(V)=d$ does not involve length information. ", "page_idx": 18}, {"type": "text", "text": "Lemma C.2. Assume Assumption 3.1 and $q_{z}=\\mathcal{N}(0,\\lambda^{2}I_{d})$ .Let $C=\\mathbb{E}_{X_{t a}\\sim q^{t a}}\\left[X_{t a}X_{t a}^{\\top}\\right]$ bethe expected data covariance matrix. Then, $\\widetilde{V}_{t a}$ has a closed form: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\widetilde{V}_{t a}\\widetilde{V}_{t a}^{\\top}=\\frac{m_{t}^{2}\\widehat\\lambda^{2}+\\sigma_{t}^{2}}{\\widehat\\lambda^{2}}\\left(C+\\sigma_{t}^{2}I_{D}\\right)^{-1}C\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Let $\\widehat{G}_{t}=I_{d}-\\sigma_{t}^{2}\\widehat{\\Sigma}_{t}^{-1}$ , then we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell_{t}^{t a}\\left(X_{t a,i};\\mathbf{s}_{V_{t a},\\hat{\\theta}}\\right)=\\mathbb{E}_{X_{t}|X_{0}=X_{t a,i}}\\left[\\Vert\\frac{1}{\\sigma_{t}^{2}}V_{t a}\\widehat G_{t}V_{t a}^{\\top}X_{t}-\\frac{1}{\\sigma_{t}^{2}}X_{t}-\\nabla\\log q_{t}^{t a}(X_{t}|X_{0})\\Vert_{2}^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\frac{1}{\\sigma_{t}^{4}}\\mathbb{E}_{X_{t}|X_{0}=X_{t a,i}}\\left[\\Vert V_{t a}\\widehat G_{t}V_{t a}^{\\top}X_{t}-m_{t}X_{0}\\Vert_{2}^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the second egualit folws $\\begin{array}{r}{\\nabla\\log q_{t}^{t a}\\left(X_{t}|X_{0}\\right)=-\\frac{X_{t}-m_{t}X_{0}}{\\sigma_{t}^{2}}}\\end{array}$ Let $C=\\mathbb{E}_{X_{t a}\\sim q_{t a}}\\left[X_{t a}X_{t a}^{\\top}\\right]$ be the expected covariance matrix of the target dataset. With the fact $\\mathbb{E}_{X_{t}|X_{0}}[X_{t}X_{t}^{\\top}]=m_{t}^{2}X_{0}X_{0}^{\\top}+$ $\\sigma_{t}^{2}I_{D}$ and $\\mathbb{E}_{X_{t}|X_{0}}[X_{0}X_{t}^{\\top}]=m_{t}X_{0}X_{0}^{\\top}$ , the optimization problem can be simplified to the following form (without misunderstanding, we ignore the subscript $t a$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{V\\in\\mathbb{R}^{D\\times d},\\mathrm{rank}(V)=d}\\mathcal{L}(V)=\\|(m_{t}^{2}C+\\sigma_{t}^{2}I_{D})^{\\frac{1}{2}}V\\widehat{G}_{t}V^{\\top}\\|_{F}^{2}-2m_{t}^{2}\\mathrm{tr}(V\\widehat{G}_{t}V^{\\top}C)\\,,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $(m_{t}^{2}C+\\sigma_{t}^{2}I_{D})^{\\frac{1}{2}}$ is meaningful since $(m_{t}^{2}C+\\sigma_{t}^{2}I_{D})$ is positive-definite matrix. Let $\\widetilde{V}$ be the solution of the above minimization problem. We first ignore the constraint $\\operatorname{rank}(V)=d$ and calculate $\\partial\\mathcal{L}(V)/\\partial V=0$ (since $\\widetilde{V}$ also satisfied $\\partial\\mathcal{L}(V)/\\partial V\\vert_{V=\\widetilde{V}}=0)$ ,we know that $\\widetilde{V}$ satisfies the following equality: ", "page_idx": 18}, {"type": "equation", "text": "$$\n(m_{t}^{2}C+\\sigma_{t}^{2}I_{D})V\\widehat{G}_{t}V^{\\top}V\\widehat{G}_{t}=m_{t}^{2}C V\\widehat{G}_{t}\\,,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which indicate ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\big((m_{t}^{2}C+\\sigma_{t}^{2}I_{D})V\\widehat{G}_{t}V^{\\top}-m_{t}^{2}C\\big)(V\\widehat{G}_{t})=\\mathrm{O}_{D\\times d}\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The above equality mean $\\mathrm{rank}((m_{t}^{2}C+\\sigma_{t}^{2}I_{D})V\\widehat{G}_{t}V^{\\top}\\,-\\,m_{t}^{2}C)\\,+\\,\\mathrm{rank}(V\\widehat{G}_{t})~\\le~d.$ Since $\\operatorname{rank}(V)=d$ and $\\mathrm{rank}(\\widehat{G}_{t})=d$ , then we have that $\\operatorname{rank}(V{\\widehat{G}}_{t})=d$ and ", "page_idx": 18}, {"type": "equation", "text": "$$\n(m_{t}^{2}C+\\sigma_{t}^{2}I_{D})V\\widehat{G}_{t}V^{\\top}=m_{t}^{2}C\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In Section 5, we assume the latent distribution is a isotropic Gaussian distribution $q_{z}=\\mathcal{N}(0,\\lambda^{2}I_{d})$ In this setting, $\\widehat{\\Sigma}$ is equal to $\\widehat{\\lambda}^{2}I_{d}$ and $\\begin{array}{r}{\\widehat{G}_{t}=\\frac{m_{t}^{2}\\widehat{\\lambda}^{2}}{m_{t}^{2}\\widehat{\\lambda}^{2}+\\sigma_{t}^{2}}}\\end{array}$ , which indicate the closed form solution of V is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\widetilde{V}\\widetilde{V}^{\\top}={\\frac{m_{t}^{2}\\widehat\\lambda^{2}+\\sigma_{t}^{2}}{\\widehat\\lambda^{2}}}(C+\\sigma_{t}^{2}I_{D})^{-1}C\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The last step is to prove $\\mathrm{rank}(\\widetilde{V}\\widetilde{V}^{\\top})\\,=\\,d$ . Note that $\\operatorname{rank}(C)\\,=\\,\\operatorname{rank}\\left(\\mathbb{E}_{X_{t a}\\sim q_{t a}}\\left[X_{t a}X_{t a}^{\\top}\\right]\\right)\\,=$ $\\operatorname{rank}(A{\\Sigma}A^{\\top})=d$ , which indicates ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\{\\mathrm{rank}(\\widetilde{V}\\widetilde{V}^{\\top}),\\mathrm{rank}(m_{t}^{2}C+\\sigma_{t}^{2}I_{D})\\}\\geq d\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Combined with $\\widetilde{V}\\in\\mathbb{R}^{D\\times d}$ we complete the proof. ", "page_idx": 18}, {"type": "text", "text": "C.3 The Error Bound for the empirical Closed-form Solution ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this part, we prove the accuracy bound of the empirical version closed form solution VV\\* $\\bar{\\tilde{V}}\\bar{\\tilde{V}}^{\\top}$ W.r.t. $n_{s}$ and $n_{t a}$ . The empirical solution has the following form (without misunderstanding, we ignore the subscript $t a$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\bar{\\tilde{V}}\\bar{\\tilde{V}}^{\\top}={\\frac{m_{t}^{2}\\widehat{\\lambda}^{2}+\\sigma_{t}^{2}}{\\widehat{\\lambda}^{2}}}(m_{t}^{2}\\bar{C}+\\sigma_{t}^{2}I_{D})^{-1}\\bar{C}\\,,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\begin{array}{r}{\\bar{C}=\\frac{1}{n_{t a}}\\sum_{i=1}^{n_{t a}}X_{t a,i}X_{t a,i}^{\\top}}\\end{array}$ ", "page_idx": 19}, {"type": "text", "text": "Theorem 5.2. Assume Assumption 3.1 and $q_{z}\\,=\\,\\mathcal{N}(0,\\lambda^{2}I_{d})$ : Let $\\widehat{q}_{z}$ be the latent distribution generated by the pre-trained models with (Vta,) and M = B\u03b2(+x2)Ddlog (Ddns) (d2 V D). Then, with probability $1-\\delta_{1}$ we have that for any $t\\in[\\delta,T]$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\|\\bar{\\tilde{V}}_{t a}\\bar{\\tilde{V}}_{t a}^{\\top}-A_{t a}A_{t a}^{\\top}\\right\\|_{F}^{2}\\leq\\tilde{O}\\left(\\frac{d\\log(\\frac{1}{\\delta_{1}})}{m_{t}^{2}\\lambda^{2}+\\sigma_{t}^{2}}\\left(\\frac{M}{d\\delta\\sqrt{n_{s}}}+\\frac{d}{n_{t a}}(m_{t}^{2}\\lambda^{2}+\\sigma_{t}^{2})^{2}\\right)\\right)\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. The empirical solution indicates that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\bigl(m_{t}^{2}\\bar{C}+\\sigma_{t}^{2}I_{D}\\bigr)\\bar{\\tilde{V}}\\bar{\\tilde{V}}^{\\top}=\\frac{m_{t}^{2}\\widehat{\\lambda}^{2}+\\sigma_{t}^{2}}{\\widehat{\\lambda}^{2}}\\bar{C}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "To analyze this equality, we first show that $\\widehat{\\lambda}^{2}$ is accurate enough. We know that $\\mathrm{KL}\\left(q_{0}||\\hat{p}_{T}^{\\mathrm{LD}}\\right)=$ $d\\left(\\lambda^{2}/\\widehat{\\lambda}^{2}-\\log(\\lambda^{2}/\\widehat{\\lambda}^{2})-1\\right)$ . Let $\\begin{array}{r}{M_{1}=\\frac{d^{2}\\beta^{2}\\left(d+\\lambda_{\\operatorname*{max}}^{2}\\right)}{\\lambda_{\\operatorname*{min}}\\delta}\\sqrt{\\left(d^{2}+D d\\right)\\log\\left(D d n_{s}\\right)\\left(d^{2}\\vee D\\right)}}\\end{array}$ Then, Lemma C.1 show that with probability $1-\\delta_{1}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{KL}\\left(q_{0}\\|\\hat{p}_{T}^{\\mathrm{LD}}\\right)=d\\left(\\lambda^{2}/\\hat{\\lambda}^{2}-\\log(\\lambda^{2}/\\hat{\\lambda}^{2})-1\\right)\\le\\widetilde{O}\\left(M_{1}\\sqrt{\\frac{\\log(1/\\delta_{1})}{n_{s}}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Combined with the above inequality, we know that with probability $1\\,-\\,\\delta_{1}$ $|\\lambda^{2}/\\widehat\\lambda^{2}\\,-\\,1|\\ \\leq$ $\\sqrt{\\frac{M_{1}\\sqrt{\\log(1/\\delta_{1})}}{d\\sqrt{n_{s}}}}$ by using the Taylor Expansion. By using Lemma D.2, we know that with probability $\\dot{1}-\\delta_{1}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{n_{t}^{2}\\widehat{\\lambda}^{2}+\\sigma_{t}^{2}}{\\widehat{\\lambda}^{2}}\\bar{C}\\leq\\widetilde{O}\\left(\\left(m_{t}^{2}\\lambda^{2}+\\frac{\\lambda^{2}\\sigma_{t}^{2}}{\\widehat{\\lambda}^{2}}\\right)\\left(1+\\frac{2\\sqrt{d+\\log\\left(1/\\delta_{1}\\right)}}{\\sqrt{n_{t a}}}\\right)A A^{\\top}\\right)}\\\\ &{\\qquad\\qquad\\leq\\widetilde{O}\\left(\\left(m_{t}^{2}\\lambda^{2}+\\sigma_{t}^{2}+\\sqrt{\\frac{M_{1}\\sqrt{\\log\\left(1/\\delta_{1}\\right)}}{d\\sqrt{n_{s}}}}\\sigma_{t}^{2}\\right)\\left(1+\\frac{2\\sqrt{d+\\log\\left(1/\\delta_{1}\\right)}}{\\sqrt{n_{t a}}}\\right)A A^{\\top}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For the left hand of Equation (6), we know that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left(m_{t}^{2}\\bar{C}+\\sigma_{t}^{2}I_{D}\\right)\\bar{\\tilde{V}}\\bar{\\tilde{V}}^{\\top}\\geq\\left(m_{t}^{2}\\lambda^{2}A A^{\\top}-m_{t}^{2}\\lambda^{2}\\frac{2\\sqrt{d+\\log\\left(1/\\delta_{1}\\right)}}{\\sqrt{n_{t a}}}+\\sigma_{t}^{2}I_{D}\\right)\\bar{\\tilde{V}}\\bar{\\tilde{V}}^{\\top}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Combined with Equation (7) and Equation (8), we have that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(m_{t}^{2}\\lambda^{2}A A^{\\top}+\\sigma_{t}^{2}I_{D}\\right)\\left(\\bar{\\tilde{V}}\\bar{\\tilde{V}}^{\\top}-A A^{\\top}\\right)}\\\\ &{\\leq\\tilde{O}\\Bigg(\\left(\\sqrt{\\frac{M_{1}\\sqrt{\\log(1/\\delta_{1})}}{d\\sqrt{n_{s}}}}+\\frac{2\\sqrt{d+\\log(1/\\delta_{1})}}{\\sqrt{n_{t a}}}\\left(m_{t}^{2}\\lambda^{2}+\\sigma_{t}^{2}+\\sqrt{\\frac{M_{1}\\sqrt{\\log(1/\\delta_{1})}}{d\\sqrt{n_{s}}}}\\sigma_{t}^{2}\\right)\\right)A A^{\\top}}\\\\ &{\\qquad\\qquad+\\;2m_{t}^{2}\\lambda^{2}\\frac{\\sqrt{d+\\log(1/\\delta_{1})}}{\\sqrt{n_{t a}}}\\bar{\\tilde{V}}\\bar{\\tilde{V}}^{\\top}\\Bigg)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Le $\\begin{array}{r}{\\mathrm{~t~}M_{2}(n_{s},n_{t a})=\\sqrt{\\frac{M_{1}\\sqrt{\\log(1/\\delta_{1})}}{d\\sqrt{n_{s}}}}+\\frac{2\\sqrt{d+\\log(1/\\delta_{1})}}{\\sqrt{n_{t a}}}\\left(m_{t}^{2}\\lambda^{2}+\\sigma_{t}^{2}+\\sqrt{\\frac{M_{1}\\sqrt{\\log(1/\\delta_{1})}}{d\\sqrt{n_{s}}}}\\sigma_{t}^{2}\\right).}\\end{array}$ According to symmetry, we know that $\\begin{array}{r l}{\\left|(m_{t}^{2}\\lambda^{2}A A^{\\top}+\\sigma_{t}^{2}I_{D})\\left(\\bar{\\tilde{V}}\\bar{\\tilde{V}}^{\\top}-A A^{\\top}\\right)\\right|\\right|_{F}^{2}\\le\\widetilde{O}\\left(M_{2}(n_{s},n_{t a})^{2}\\|A A^{\\top}\\|_{F}^{2}+\\frac{m_{t}^{4}\\lambda^{4}\\left(d+\\log\\left(1/\\delta_{1}\\right)\\right)}{n_{t a}}\\right.}&{}\\\\ {\\left.\\le\\widetilde{O}\\left(D^{2}(M_{2}(n_{s},n_{t a})^{2}\\right)\\,.}\\end{array}$ The last inequality follows that each element of VV is bounded by some constant due to the form $\\bar{\\tilde{V}}\\bar{\\tilde{V}}^{\\top}$ of the empirical closed form solution and ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|A A^{\\top}\\|_{F}^{2}=t r(A A^{\\top}A A^{\\top})=t r(A A^{\\top})=t r(I_{d})=d\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For the right hand of the above inequality, we know that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\left(m_{t}^{2}\\lambda^{2}A A^{\\top}+\\sigma_{t}^{2}I_{D}\\right)\\left(\\bar{\\tilde{V}}\\bar{\\tilde{V}}^{\\top}-A A^{\\top}\\right)\\right\\|_{F}^{2}}\\\\ &{=\\mathrm{Tr}\\left(\\left(\\bar{\\tilde{V}}\\bar{\\tilde{V}}^{\\top}-A A^{\\top}\\right)\\left(\\bar{\\tilde{V}}\\bar{\\tilde{V}}^{\\top}-A A^{\\top}\\right)\\left(m_{t}^{2}\\lambda^{2}A A^{\\top}+\\sigma_{t}^{2}I_{D}\\right)\\left(m_{t}^{2}\\lambda^{2}A A^{\\top}+\\sigma_{t}^{2}I_{D}\\right)\\right)}\\\\ &{\\geq\\left(m_{t}^{2}\\lambda^{2}+\\sigma_{t}^{2}\\right)\\left\\|\\left(\\bar{\\tilde{V}}\\bar{\\tilde{V}}^{\\top}-A A^{\\top}\\right)\\right\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then, we complete the proof ", "page_idx": 20}, {"type": "text", "text": "D Auxiliary Lemmas ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The following concentration lemma comes from Lemma 15 of Chen et al. (2023c). ", "page_idx": 20}, {"type": "text", "text": "Lemma D.1 (Lemma 15, (Chen et al., 2023c)). Let $\\mathcal{G}$ be a bounded function class, i.e., there exists a constant $B$ such that any $g\\in\\mathcal{G}:\\mathbb{R}^{d}\\mapsto[0,B]$ Let $\\mathbf{z}_{1},\\ldots,\\mathbf{z}_{n}\\in\\mathbb{R}^{\\bar{d}}$ be i.i.d. random variables. For any $\\delta\\in(0,1),a\\leq1,$ and $\\tau>0$ we have $\\begin{array}{l}{\\mathbb{P}\\left(\\displaystyle\\operatorname*{sup}_{g\\in\\mathcal{G}}\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}g\\left(\\mathbf{z}_{i}\\right)-(1+a)\\mathbb{E}[g(\\mathbf{z})]>\\displaystyle\\frac{(1+3/a)B}{3n}\\log\\displaystyle\\frac{\\mathcal{N}\\left(\\boldsymbol{\\tau},\\mathcal{G},\\|\\cdot\\|_{\\infty}\\right)}{\\delta_{1}}+(2+a)\\boldsymbol{\\tau}\\right)\\leq\\delta_{1}}\\\\ {\\mathbb{P}\\left(\\displaystyle\\operatorname*{sup}_{g\\in\\mathcal{G}}\\mathbb{E}[g(\\mathbf{z})]-\\displaystyle\\frac{1+a}{n}\\displaystyle\\sum_{i=1}^{n}g\\left(\\mathbf{z}_{i}\\right)>\\displaystyle\\frac{(1+6/a)B}{3n}\\log\\displaystyle\\frac{\\mathcal{N}\\left(\\boldsymbol{\\tau},\\mathcal{G},\\|\\cdot\\|_{\\infty}\\right)}{\\delta_{1}}+(2+a)\\boldsymbol{\\tau}\\right)\\leq\\delta_{1}\\,.}\\end{array}$ and ", "page_idx": 20}, {"type": "text", "text": "In the following lemma, we show the concentration of the data covariance matrix. Note that the proof sketch of the following lemma mainly follows Lemma 6 of (Du et al., 2020). We prove a concentration bound that depends on $n$ instead of a constant bound with a large enough $n$ ", "page_idx": 20}, {"type": "text", "text": "Lemma D.2 (The Modified Lemma A.6, (Du et al., 2020)). Let $\\pmb{a}_{1},\\dots,\\pmb{a}_{n}$ be i.i.d. $d$ dimensional randomvectors such that E $\\left[{\\pmb a}_{i}\\right]={\\bf0}$ E $\\left[\\pmb{a}_{i}\\pmb{a}_{i}^{\\top}\\right]=I$ and $\\pmb{a}_{i}$ $\\rho^{2}$ -subgaussian.Then with probability atleast $1-\\delta_{1}$ we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left(1-\\frac{2\\rho^{2}\\sqrt{d+\\log\\left(1/\\delta_{1}\\right)}}{\\sqrt{n}}\\right)I\\preceq\\frac{1}{n}\\sum_{i=1}^{n}{a_{i}a_{i}^{\\top}}\\preceq\\left(1+\\frac{2\\rho^{2}\\sqrt{d+\\log\\left(1/\\delta_{1}\\right)}}{\\sqrt{n}}\\right)I\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Let $\\begin{array}{r}{A=\\frac{1}{n}\\sum_{i=1}^{n}{{a_{i}}{\\pmb{a}}_{i}^{\\top}}-I,}\\end{array}$ Similr to Du e al 20), we sean $\\epsilon$ -net argumen for the unit sphere $S^{d-1}=\\left\\{\\pmb{v}\\in\\mathbb{R}^{d}:\\|\\pmb{v}\\|=1\\right\\}$ . For any $\\pmb{v}\\in S^{d-1}$ , we know that $\\left(v^{\\top}a_{i}\\right)^{2}-1$ is zero-mean and $16\\rho^{2}$ -subgaussian. By using the Bernstein inequality, we have for any $\\epsilon>0$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\left|v^{\\top}A v\\right|>\\epsilon\\right]\\leq2\\exp\\left(-\\frac{n}{2}\\operatorname*{min}\\left\\{\\frac{\\epsilon^{2}}{\\left(16\\rho^{2}\\right)^{2}},\\frac{\\epsilon}{16\\rho^{2}}\\right\\}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Next, we take a $\\frac{1}{5}$ -net $\\mathcal{N}\\subset S^{d-1}$ of $S^{d-1}$ with size $|{\\mathcal{N}}|\\leq e^{O(d)}$ . By using the union bound, we know that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\operatorname*{Pr}\\left[\\underset{v\\in\\mathcal{N}}{\\operatorname*{max}}\\left|v^{\\top}A v\\right|>\\epsilon\\right]\\leq2|\\mathcal{N}|\\exp\\left(-\\frac{n}{2}\\operatorname*{min}\\left\\{\\frac{\\epsilon^{2}}{\\left(16\\rho^{2}\\right)^{2}},\\frac{\\epsilon}{16\\rho^{2}}\\right\\}\\right)}\\\\ &{}&{\\leq\\exp\\left(O(d)-\\frac{n}{2}\\operatorname*{min}\\left\\{\\frac{\\epsilon^{2}}{\\left(16\\rho^{2}\\right)^{2}},\\frac{\\epsilon}{16\\rho^{2}}\\right\\}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Let the right hand of the above inequality equals to $\\delta_{1}$ . We know that with probability $1-\\delta_{1}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pmb{v}\\in\\mathcal{N}}|\\pmb{v}^{\\top}A\\pmb{v}|\\leq\\frac{\\rho^{2}\\sqrt{d+\\log{(1/\\delta_{1})}}}{\\sqrt{n}}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that for any $\\pmb{u}\\in S^{d-1}$ , there exists $\\pmb{u}^{\\prime}\\in\\mathcal{N}$ such that $\\begin{array}{r}{\\|\\pmb{u}-\\pmb{u}^{\\prime}\\|\\leq\\frac{1}{5}}\\end{array}$ . Then, we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\vert\\boldsymbol{u}^{\\top}\\boldsymbol{A}\\boldsymbol{u}\\vert\\le\\vert\\left(\\boldsymbol{u}^{\\prime}\\right)^{\\top}\\boldsymbol{A}\\boldsymbol{u}^{\\prime}\\vert+2\\vert\\left(\\boldsymbol{u}-\\boldsymbol{u}^{\\prime}\\right)^{\\top}\\boldsymbol{A}\\boldsymbol{u}^{\\prime}\\vert+\\vert\\left(\\boldsymbol{u}-\\boldsymbol{u}^{\\prime}\\right)^{\\top}\\boldsymbol{A}\\left(\\boldsymbol{u}-\\boldsymbol{u}^{\\prime}\\right)\\vert}}\\\\ &{}&{\\le\\frac{\\rho^{2}\\sqrt{d+\\log\\left(1/\\delta_{1}\\right)}}{\\sqrt{n}}+\\frac{1}{2}\\Vert\\boldsymbol{A}\\Vert_{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\left\\Vert A\\right\\Vert_{2}$ is the operator norm of matrix $A$ . Taking a supreme over $u\\in S^{d-1}$ , we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|A\\|_{2}\\leq\\frac{\\rho^{2}\\sqrt{d+\\log{\\left(1/\\delta_{1}\\right)}}}{\\sqrt{n}}+\\frac{1}{2}\\|A\\|_{2}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then, we complete the proof. ", "page_idx": 21}, {"type": "text", "text": "In Section 5, we assume the latent distribution is a Gaussian distribution instead of a subgaussian one. Yuan et al. (2023) show that in this setting, the approximation error bound has better dependence on $n_{s}$ ", "page_idx": 21}, {"type": "text", "text": "Lemma D.3 (Lemma C.1, (Yuan et al., 2023). Assume the latent distribution is a Gaussian distribution $q_{z}=\\mathcal{N}(0,\\Sigma)$ with $\\Sigma=\\operatorname{diag}\\left(\\lambda_{1}^{2},\\dots,\\lambda_{d}^{2}\\right)\\succ0$ Then, the solution $(\\widehat{V}_{s},\\widehat{\\theta})$ of Equation (1) has the following approximation error bound with probability $1-\\delta_{1}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{\\Gamma-\\delta}\\int_{\\delta}^{T}\\Big\\|\\nabla\\log q_{t}^{s}\\left(\\cdot\\right)-\\mathbf{s}_{\\hat{V}_{s},\\hat{\\theta}}\\left(\\cdot,t\\right)\\Big\\|_{L^{2}(q_{t}^{s})}^{2}\\,\\mathrm{d}t\\leq O\\left(\\frac{1}{\\delta}\\sqrt{\\frac{\\left(d^{2}+D d\\right)\\log\\left(D d n_{s}\\right)\\left(d^{2}\\vee D\\right)\\log\\frac{1}{\\delta_{1}}}{n_{s}}}\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "E Additional Experiments ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we do experiments on real-world datasets to show that the new model obtained by only fine-tuning appropriate encoder and decoder layers on target datasets with only 10 images can generate novel images with the target dataset feature. On the contrary, if all parameters can be fine-tuned, the model will suffer from memory phenomenon and only generate the ten images in the target dataset. This phenomenon indicates that only fine-tuning the appropriate encoder and decoder will result in a model with a generalization property. ", "page_idx": 21}, {"type": "text", "text": "Setting. In this experiment, we use a U-net network with attention layers, which contains 11 downblocks, 2 middleblocks, and 15 upblocks. When only fine-tuning the encoder and decoder layers, we fine-tune the first 4 downblock layers (encoder) and 4 upblock layers (decoder) instead of only using linear layers as the encoder and decoder (discuss in the later discussion paragraph). ", "page_idx": 21}, {"type": "text", "text": "The above experiments are conduct on a GeForce RTX 4090. We train the neural network using AdamW optimizer with learning rate 0.0001. For the pre-trained phase, we train the models for 200 epochs with batch size 20. It takes 5 hours to obtain a pre-trained diffusion models. For the fine-tuning phase, we fine-tune the pre-trained models for 400 epochs with batch size 2. It take 3 minutes to fine-tune the pre-trained models. ", "page_idx": 21}, {"type": "image", "img_path": "JrraNaaZm5/tmp/af50fb42d429898c989b479b07797d1a477291069cf3e2c8b1b847c885d55fa2.jpg", "img_caption": ["Figure 2: The experiments on cat face dataset "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Dataset. Our experiments use 2 real-world datasets: the CelebA64 dataset and the cat face dataset. ", "page_idx": 22}, {"type": "text", "text": "\u00b7 CelebA64 (size $3*64*64)$ (a) Source dataset: 6400 images of faces with different hairstyles (without the bald feature). (b) Target dataset: 10 images with the bald feature in CelebA64.   \n\u00b7 Cat face images (size $3*64*64$ (a) Source dataset: 4200 cat images with different colors (without black color cat). (b) Target dataset: 10 black color cat images (The color black constitutes more than $70\\%$ of the image's composition.). ", "page_idx": 22}, {"type": "text", "text": "Discussion on results. The experiment results of CelebA64 have been discussed in Section 6. The experiment phenomenon is similar for the cat face images, which means the models obtained by only fine-tuning the encoder and decoder can generate novel images with the target feature (Figure 2). We note that when choosing the target cat face dataset if the color black constitutes more than $70\\%$ Of the image's composition, we view this cat image as the black cat. Hence, different colors exist for cats, such as white and grey, due to the target dataset containing a small number of these colors (such as images 1, 3, 4, 6, 8). As a result, our fine-tuning results also contain these colors. However, our results do not contain colors other than those in the target dataset and can produce novel samples, which also proves the effectiveness of our fine-tuning method. ", "page_idx": 22}, {"type": "text", "text": "Discussion on linear encoder and decoder.  Assumption 3.1 assumes the linear subspace, which indicates linear encoder and decoder. However, we fine-tune the first 4 downblock layers (encoder) and 4 upblock layers (decoder) instead of only using linear layers as the encoder and decoder. We note that this operation does not conflict with our Assumption. Recall that in Stable Diffusion (Rombach et al., 2022), the diffusion models run in the VAE embedding space 7. Hence, we can view the first 3 downblock layers and the last 3 upblock layers as the VAE encoder and VAE decoder. Then, we can obtain $X$ in this paper by running the VAE encoder. The remaining 1 downblock and 1 upblock layer can be viewed as linear encoder and decoder $A$ . As mentioned in Section 4.2 of StyleGAN (Karras et al., 2019), the feature of $X$ obtained by running a good-enough VAE encoder has linear separability, which also supports our Assumption 3.1. ", "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: This work aims to explain the reason why few-shot diffusion models can achieve great performance with a limited dataset from the theoretical perspective. We achieve this goal by analyzing the approximation and optimization perspective (Theorem 4.3 and Theorem 5.2). We also do real-world experiments to support our theoretical results (Section6). ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We discuss future work and limitation at Section 7 ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should refect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We have shown all assumptions, theorem and proof sketch in the main paper.   \nThe detailed proof appears in the appendix. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We has shown all experiments detail in Appendix E. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [No] ", "page_idx": 25}, {"type": "text", "text": "Justification: As a theoretical work, we simply train and fine-tune a diffusion models on the datasets in Appendix E. All detail is shown in Appendix E. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We has shown all experiments detail including dataset and training detail in AppendixE. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [No] ", "page_idx": 25}, {"type": "text", "text": "Justification: Since this work is a theoretical work, the experiments are qualitative experiments on the dataset we constructed and are used to support our theoretical results. Since we only use 10 images to fine-tune the models to generate images with specific target features, it is hard to calculate quantitative metrics such as FID. However, our experiment results still show that our methods can generate novel images with the target feature compared to the benchmark. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. ", "page_idx": 25}, {"type": "text", "text": "\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have shown the compute works and computation time in Appendix E. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have checked the code of ethics and make sure that our work satisfies the codeof ethics. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have discussed the broader impacts of our work at the end of main paper. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This paper does not use existing assets. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset isused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 28}, {"type": "text", "text": "\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]