[{"figure_path": "Cc0ckJlJF2/tables/tables_5_1.jpg", "caption": "Table 1: Comparison of inference modules. For each, we highlight its prerequisite abstraction model, the target feature the abstraction model aims to predict, and its consistency in MDPs and POMDPs.", "description": "The table compares three different inference modules (Naive, IBU, TDM) used in the paper's proposed deep RL framework for reward machines in noisy environments.  For each module, it specifies the type of abstraction model required, the target feature the abstraction model is trying to predict (either the ground truth labelling function or the RM state), and whether the module is consistent for both fully observable Markov Decision Processes (MDPs) and Partially Observable MDPs (POMDPs). Consistency here refers to whether the module can perfectly recover the target feature given an ideal abstraction model.", "section": "5 Method"}, {"figure_path": "Cc0ckJlJF2/tables/tables_19_1.jpg", "caption": "Table 2: Hyperparameters for deep RL experiments", "description": "This table presents the hyperparameter settings used in the deep reinforcement learning experiments for three different environments: Traffic Light, Kitchen, and Colour Matching.  It shows the parameters for both the main PPO (Proximal Policy Optimization) algorithm and the abstraction model training process.  The PPO parameters control aspects of the reinforcement learning process, such as the learning rate, discount factor, and entropy coefficient, while the abstraction model hyperparameters govern the training of the models used to estimate the truth values of propositions within the environments.", "section": "6.3 Abstraction Models"}, {"figure_path": "Cc0ckJlJF2/tables/tables_21_1.jpg", "caption": "Table 1: Comparison of inference modules. For each, we highlight its prerequisite abstraction model, the target feature the abstraction model aims to predict, and its consistency in MDPs and POMDPs.", "description": "This table compares three different inference modules (Naive, IBU, TDM) used in the paper's proposed deep RL framework for reward machines in uncertain environments.  For each module, it specifies the type of abstraction model required as input (mapping from history to propositional evaluations or belief over RM states), the target feature that the abstraction model is intended to predict (ground truth propositional evaluations or RM state), and whether the inference method is theoretically consistent (meaning it can perfectly recover the true belief over the RM state) in both fully observable (MDP) and partially observable (POMDP) environments.  This helps to understand the strengths and weaknesses of each approach for handling uncertainty in the interpretation of domain-specific vocabulary within reward machines.", "section": "5 Method"}]