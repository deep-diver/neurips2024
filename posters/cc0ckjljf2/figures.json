[{"figure_path": "Cc0ckJlJF2/figures/figures_2_1.jpg", "caption": "Figure 1: The Noisy Reward Machine Environment framework. Blue elements highlight differences with respect to a standard RL framework. Dashed lines (----) indicate that an element is required during training but not deployment.", "description": "This figure illustrates the architecture of a noisy reward machine environment.  The agent interacts with the environment, receiving observations and taking actions.  Crucially, the agent does not have direct access to the ground-truth labelling function (which maps environment states to propositions used by the reward machine). Instead, it relies on an abstraction model, which provides noisy or uncertain interpretations of the relevant propositions.  The dashed lines indicate components only used during training, not during deployment.", "section": "3 Problem Framework"}, {"figure_path": "Cc0ckJlJF2/figures/figures_3_1.jpg", "caption": "Figure 2: The Gold Mining Problem is a Noisy RM Environment where the agent's interpretation of the vocabulary is uncertain. Left: The four rightmost cells yield gold () while two cells in the second column yield iron pyrite, which has no value. The agent cannot reliably distinguish between the two metals-cells are labelled with the probability the agent believes it yields gold. Right: The RM emits a (non-Markovian) reward of 1 for collecting gold and delivering it to the depot", "description": "This figure illustrates the Gold Mining Problem, used as a running example in the paper.  The left panel shows a grid world where the agent (robot) must collect gold. The numbers in each cell represent the probability that the cell contains gold (the agent cannot distinguish with certainty between gold and iron pyrite). The goal is to collect at least one gold and deliver it to the depot (bottom left). The right panel shows the Reward Machine (RM), a finite state automaton that defines the reward structure for this task. The RM transitions depend on whether the robot digs gold and delivers it to the depot, demonstrating a temporally extended reward.", "section": "3.2 Running Example"}, {"figure_path": "Cc0ckJlJF2/figures/figures_6_1.jpg", "caption": "Figure 3: Traffic Light (top left) and Kitchen (bottom left), are MiniGrids with image observations, where key propositions are partially observable. Colour Matching (right) is a MuJoCo robotics environment where the agent must identify colour names by their RGB values to solve a task.", "description": "This figure shows three different reinforcement learning environments used in the paper's experiments.  The \"Traffic Light\" and \"Kitchen\" environments are from the MiniGrid suite and use image-based partial observability; agents must infer key features (e.g., traffic light color, kitchen cleanliness) from noisy observations. The \"Colour Matching\" environment, on the other hand, is a MuJoCo robotics task requiring the agent to associate colors with RGB values.  These diverse environments test the robustness and efficiency of the proposed algorithms in various settings.", "section": "6.1 Environments"}, {"figure_path": "Cc0ckJlJF2/figures/figures_7_1.jpg", "caption": "Figure 4: RL curves averaged over 8 runs (error bars show standard error). TDM exhibits strong performance in all domains without access to the labelling function, while Recurrent PPO fails.", "description": "This figure shows the results of reinforcement learning experiments comparing different methods in four environments: Gold Mining, Traffic Light, Kitchen, and Colour Matching.  The x-axis represents training steps in millions, and the y-axis shows the average return achieved by different RL algorithms. The algorithms compared include an oracle (with access to the true labeling function), a memory-only recurrent PPO approach, and three proposed methods: TDM, IBU, and Naive. The shaded areas represent the standard error of the mean across eight runs. The key finding is that the TDM method consistently performs well, even without access to the ground-truth labels, significantly outperforming the memory-only method.", "section": "6 Experiments"}, {"figure_path": "Cc0ckJlJF2/figures/figures_8_1.jpg", "caption": "Figure 5: Accuracy of inference modules measured by log-likelihood (higher is better) of the true RM state, averaged over 8 runs with lines showing standard error. TDM predicts the RM state belief more accurately than Naive and IBU.", "description": "This figure compares the accuracy of three different inference modules (TDM, Naive, and IBU) in predicting the belief over Reward Machine states.  The accuracy is measured using log-likelihood, with higher values indicating better accuracy. The results are averaged over 8 runs, and error bars represent the standard error.  The figure shows that TDM significantly outperforms Naive and IBU in accurately predicting the RM state belief across all four experimental environments.", "section": "6.5 Results: RM State Belief Inference"}, {"figure_path": "Cc0ckJlJF2/figures/figures_17_1.jpg", "caption": "Figure 6: Precision and recall of a classifier trained to predict occurrences of propositions. Key propositions in each domain are uncertain. Values are averaged over 8 training runs, with lines showing standard error.", "description": "This figure shows the precision and recall for a classifier trained to predict the occurrence of key propositions in three different environments: Traffic Light, Kitchen, and Colour Matching.  The results are averaged over eight training runs, and error bars represent the standard error.  The fact that some key propositions show low precision and recall highlights the inherent uncertainty in observing these propositions in real-world environments.", "section": "6.3 Abstraction Models"}, {"figure_path": "Cc0ckJlJF2/figures/figures_18_1.jpg", "caption": "Figure 7: An RM for Traffic Light. The goal is to pick up the package and return home (each stage gives a reward of 1). If the agent crosses a red light, it gets a delayed penalty upon returning home. To simplify formulas on edge transitions, we assume all propositions occur mutually exclusively, and if no transition condition is satisfied, the RM remains in the same state, receiving 0 reward.", "description": "This figure shows a Reward Machine (RM) for the Traffic Light environment. The RM has four states, representing different stages of the task: initial state, reaching the package, reaching home, and a terminal state. Transitions between states are triggered by the propositions (red light, package, home).  Rewards are associated with each transition. The agent receives a reward of 1 for picking up the package and arriving home and a penalty for running a red light.", "section": "B.2 MiniGrid Experiments"}, {"figure_path": "Cc0ckJlJF2/figures/figures_19_1.jpg", "caption": "Figure 8: (Left:) Accuracy of various inference modules measured by log-likelihood (higher is better) when predicting RM states given trajectories generated by a random policy. (SL:) abstraction model is trained via supervised learning from ground-truth data. (GPT4:) abstraction model is zero-shot GPT-40. (Random:) abstraction model is a neural network with random weights. (Right:) An RGB rendering of the Traffic Light MiniGrid environment used to query GPT-40. GPT-40 is given the prompt: \"The red triangle is contained in a cell of this grid. What is the color of the cell? Answer in a single word.\" Note that the light colour is only visible when entering the intersection in the forward direction.", "description": "This figure compares the accuracy of different inference modules (TDM, Naive, IBU) in predicting the Reward Machine (RM) state.  It uses three types of abstraction models: one trained via supervised learning (SL), one using zero-shot GPT-40, and one with randomly initialized weights. The left panel shows the log-likelihood of the true RM state under the predicted belief, demonstrating TDM's superior accuracy. The right panel shows a screenshot of the Traffic Light environment used for prompting GPT-40, illustrating the partial observability challenge.", "section": "6.6 Results: Vision-Language Models as Zero-Shot Abstraction Models"}]