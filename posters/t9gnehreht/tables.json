[{"figure_path": "t9gNEhreht/tables/tables_5_1.jpg", "caption": "Table 1: Comparison of SELMA and different text-to-image alignment methods on text faithfulness and human preference (see Sec. 5.1 for discussion). SELMA achieves the best performance in all five metrics when adapted on different base models (i.e., SD v1.4, SD v2, and SDXL). Best scores for each model are in bold.", "description": "This table compares SELMA's performance against other text-to-image alignment methods across three different state-of-the-art diffusion models (SD v1.4, SD v2, and SDXL).  The comparison includes training-free methods, reinforcement learning (RL) based methods, and another automatic data generation method (DreamSync).  The metrics used evaluate both text faithfulness (how well the generated image matches the text description) and human preference (how much humans prefer the generated images).  The table highlights that SELMA consistently achieves the best results across all metrics and models, demonstrating its effectiveness in improving the alignment between text descriptions and generated images.", "section": "5 Results and Analysis"}, {"figure_path": "t9gNEhreht/tables/tables_6_1.jpg", "caption": "Table 2: Comparison of single LoRA and LoRA Merging (see Sec. 5.2 for discussion). We use SD v2 as our base model and train models with our automatically generated image-text pairs. DATASELMA: auto-generated image-text pairs where prompts are generated with LLMs with three prompt examples from DATA that are not included in DSG test prompts (see Sec. 4.3 for details). LN: Localized Narratives; CB: CountBench; DDB: DiffusionDB. Best/2nd best scores are bolded/underlined.", "description": "This table compares different training methods for the text-to-image model using automatically generated image-text pairs.  It contrasts the performance of training a single LoRA (Low-Rank Adaptation) model across all datasets versus training multiple skill-specific LoRA experts and then merging them. The results are evaluated using several metrics including text faithfulness and human preference scores on the DSG benchmark.  The table shows that merging multiple skill-specific LoRA models generally outperforms training a single LoRA model across all datasets, indicating the effectiveness of the proposed SELMA method in mitigating knowledge conflict during multi-skill learning.", "section": "5.2 Effectiveness of Learning & Merging Skill-Specific Experts"}, {"figure_path": "t9gNEhreht/tables/tables_7_1.jpg", "caption": "Table 3: Comparison of different image generators for creating training images. In addition to using the same model being trained as an image generator, we also experiment with using a smaller model as an image generator (No. 4.). SDXL is bigger/stronger than SD v2. See Sec. 5.4 for discussion.", "description": "This table compares the performance of different text-to-image models when fine-tuned using images generated by themselves or by a weaker model.  It shows that fine-tuning a stronger model (SDXL) with images from a weaker model (SD v2) can achieve comparable or even better results in terms of text faithfulness and human preference compared to fine-tuning with images generated by the same stronger model. This suggests a promising \"weak-to-strong\" generalization capability in text-to-image models.", "section": "5 Results and Analysis"}, {"figure_path": "t9gNEhreht/tables/tables_7_2.jpg", "caption": "Table 4: DSG and TIFA accuracy of SDXL fine-tuned with prompt data generated with LLaMA3 and GPT-3.5.", "description": "This table compares the performance of SDXL models fine-tuned using prompts generated by two different large language models (LLMs): LLaMA3 and GPT-3.5.  It shows the impact of different prompt generation methods on the model's performance as measured by the DSG and TIFA metrics.  Two image generators (SDv2 and SDXL) were also used to generate the image data for training. The results highlight how prompt generation and the image source for fine-tuning training data impact the final performance.", "section": "5.5 Comparison with Prompt Generation with LLaMA3"}, {"figure_path": "t9gNEhreht/tables/tables_8_1.jpg", "caption": "Table 5: Comparison with different fine-tuning methods on SD v2 with our auto-generated data, in text faithfulness and human preference. See Sec. 5.7 for discussion.", "description": "This table compares the performance of different fine-tuning methods on the Stable Diffusion v2 model using the automatically generated image-text data.  It specifically contrasts the performance of the baseline SD v2 model against three variations:  1) SELMA with LoRA Merging, 2) SELMA with LoRA Merging and Direct Preference Optimization (DPO), and 3) SELMA with Mixture of Lora Experts (MoE-LoRA).  The comparison uses text faithfulness metrics (DSGmPLUG, TIFA BLIP2) and human preference metrics (PickScore, ImageReward, HPS) on the Diffusion Scene Graph (DSG) dataset.  It shows that simple LoRA Merging, as implemented in SELMA, achieves the best overall performance.", "section": "5.7 Training Method Ablations"}, {"figure_path": "t9gNEhreht/tables/tables_8_2.jpg", "caption": "Table 2: Comparison of single LoRA and LoRA Merging (see Sec. 5.2 for discussion). We use SD v2 as our base model and train models with our automatically generated image-text pairs. DATASELMA: auto-generated image-text pairs where prompts are generated with LLMs with three prompt examples from DATA that are not included in DSG test prompts (see Sec. 4.3 for details). LN: Localized Narratives; CB: CountBench; DDB: DiffusionDB. Best/2nd best scores are bolded/underlined.", "description": "This table compares different training methods for the SELMA model, specifically focusing on the impact of using a single LoRA (Low-Rank Adaptation) versus merging multiple LoRAs.  It shows the performance on various metrics (DSG, TIFA, BLIP, PickScore, ImageReward, HPS) using different combinations of automatically generated datasets (LN, CB, DDB, Whoops, COCO).  The results highlight the effectiveness of merging multiple skill-specific LoRAs to mitigate knowledge conflict and improve overall performance.", "section": "5.2 Effectiveness of Learning & Merging Skill-Specific Experts"}, {"figure_path": "t9gNEhreht/tables/tables_18_1.jpg", "caption": "Table 1: Comparison of SELMA and different text-to-image alignment methods on text faithfulness and human preference (see Sec. 5.1 for discussion). SELMA achieves the best performance in all five metrics when adapted on different base models (i.e., SD v1.4, SD v2, and SDXL). Best scores for each model are in bold.", "description": "This table compares SELMA's performance against other text-to-image alignment methods across multiple metrics.  It evaluates both text faithfulness (how accurately the generated image matches the text description) and human preference (how aesthetically pleasing and relevant the generated images are to humans). The comparison is done using three different state-of-the-art text-to-image models (Stable Diffusion v1.4, v2, and XL) as baselines.  The table highlights that SELMA consistently achieves the best results across all metrics and base models.", "section": "5 Results and Analysis"}, {"figure_path": "t9gNEhreht/tables/tables_18_2.jpg", "caption": "Table 8: Detailed skill-specific comparison of SD models vs. SD models+SELMA on TIFA benchmark.", "description": "This table presents a detailed comparison of the performance of Stable Diffusion (SD) models (versions 1.4, v2, and XL) and their corresponding SELMA-enhanced versions across various image generation skills evaluated by the TIFA benchmark.  It breaks down the accuracy scores for each model on specific skills like recognizing animals/humans, objects, locations, activities, colors, spatial relationships, attributes, food items, counts, materials, other elements, shapes, and provides an overall average score for each model.", "section": "5 Results and Analysis"}, {"figure_path": "t9gNEhreht/tables/tables_19_1.jpg", "caption": "Table 2: Comparison of single LoRA and LoRA Merging (see Sec. 5.2 for discussion). We use SD v2 as our base model and train models with our automatically generated image-text pairs. DATASELMA: auto-generated image-text pairs where prompts are generated with LLMs with three prompt examples from DATA that are not included in DSG test prompts (see Sec. 4.3 for details). LN: Localized Narratives; CB: CountBench; DDB: DiffusionDB. Best/2nd best scores are bolded/underlined.", "description": "This table compares different training methods for the text-to-image model using automatically generated data. It shows the results of training with single LoRA models on individual skill-specific datasets, training with a single LoRA model on a mix of datasets and using LoRA merging with multiple skill-specific LoRA experts.  The metrics used are text faithfulness (DSGMPLUG, TIFA, BLIP2) and human preference (PickScore, ImageReward, HPS) on the DSG benchmark. The best performing methods are highlighted.", "section": "5.2 Effectiveness of Learning & Merging Skill-Specific Experts"}, {"figure_path": "t9gNEhreht/tables/tables_19_2.jpg", "caption": "Table 2: Comparison of single LoRA and LoRA Merging (see Sec. 5.2 for discussion). We use SD v2 as our base model and train models with our automatically generated image-text pairs. DATASELMA: auto-generated image-text pairs where prompts are generated with LLMs with three prompt examples from DATA that are not included in DSG test prompts (see Sec. 4.3 for details). LN: Localized Narratives; CB: CountBench; DDB: DiffusionDB. Best/2nd best scores are bolded/underlined.", "description": "This table compares different training methods for the Stable Diffusion v2 model using automatically generated image-text datasets.  It contrasts single LoRA (Low-Rank Adaptation) training on different skill-specific datasets with the approach of training multiple skill-specific LoRAs separately and then merging them.  The table shows the results across several metrics, evaluating text faithfulness and human preference using various datasets (LN, CB, DDB, Whoops, COCO).  The results demonstrate the effectiveness of the LoRA merging technique.", "section": "5.2 Effectiveness of Learning & Merging Skill-Specific Experts"}, {"figure_path": "t9gNEhreht/tables/tables_22_1.jpg", "caption": "Table 1: Comparison of SELMA and different text-to-image alignment methods on text faithfulness and human preference (see Sec. 5.1 for discussion). SELMA achieves the best performance in all five metrics when adapted on different base models (i.e., SD v1.4, SD v2, and SDXL). Best scores for each model are in bold.", "description": "This table compares SELMA against several other text-to-image alignment methods across various metrics.  These metrics measure how well the generated images match the input text descriptions (text faithfulness) and how well they align with human preferences. The comparison is done using three different state-of-the-art text-to-image models (Stable Diffusion v1.4, v2, and XL) as baselines.  The table highlights that SELMA consistently outperforms other methods across all metrics and base models.", "section": "5 Results and Analysis"}]