[{"figure_path": "deZpmEfmTo/figures/figures_1_1.jpg", "caption": "Figure 1: A comparison of the domain adaptation performance of our method against existing methods. Our method outperforms the state-of-the-art consistently on 11 widely studied downstream detection datasets in terms of AP50 improvements. The results of all methods are acquired with the same baseline [3].", "description": "This radar chart compares the performance of the proposed Knowledge Graph Distillation (KGD) method with several existing domain adaptation methods across 11 different downstream datasets.  The datasets represent various domains and conditions including autonomous driving under different weather and lighting, common objects, intelligent surveillance, and artistic illustrations. The chart shows that KGD consistently outperforms state-of-the-art methods, achieving significant improvements in average precision (AP50).", "section": "1 Introduction"}, {"figure_path": "deZpmEfmTo/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of the proposed Knowledge Graph Distillation (KGD). KGD comprises two consecutive stages including Knowledge Graph Extraction (KGExtract) and Knowledge Graph Encapsulation (KGEncap). KGExtract employs CLIP to encode downstream data as nodes and considers their feature distances as edges, explicitly constructing KGs including language knowledge graph (LKG) and vision knowledge graph (VKG) that inherit the rich semantic relations in CLIP. The dashed reddish lines between LKG and VKG represent the cross-modal edges that connect the nodes between vision and language modalities, enabling the integration of both language and visual information. KGEncap transfers the extracted KGs into the large-vocabulary object detector to enable accurate object classification over downstream data. Besides, KGD works for both image and text data and allow extracting and transferring vision KG (VKG) and language KG (LKG), providing complementary knowledge for adapting large-vocabulary object detectors for handling various unlabelled downstream domains.", "description": "This figure illustrates the Knowledge Graph Distillation (KGD) framework.  KGD is a two-stage process: First, it extracts knowledge graphs (KGs) from CLIP, a vision-language model.  These KGs include a language KG (LKG) based on text and a vision KG (VKG) based on image features. Second, KGD encapsulates the extracted KGs into a large-vocabulary object detector to improve object classification.  The figure shows the data flow, highlighting the use of WordNet to enrich the LKG, CLIP for encoding data and computing feature distances, and GCN for incorporating knowledge graph information into object detection.", "section": "3 Method"}, {"figure_path": "deZpmEfmTo/figures/figures_23_1.jpg", "caption": "Figure 2: Overview of the proposed Knowledge Graph Distillation (KGD). KGD comprises two consecutive stages including Knowledge Graph Extraction (KGExtract) and Knowledge Graph Encapsulation (KGEncap). KGExtract employs CLIP to encode downstream data as nodes and considers their feature distances as edges, explicitly constructing KGs including language knowledge graph (LKG) and vision knowledge graph (VKG) that inherit the rich semantic relations in CLIP. The dashed reddish lines between LKG and VKG represent the cross-modal edges that connect the nodes between vision and language modalities, enabling the integration of both language and visual information. KGEncap transfers the extracted KGs into the large-vocabulary object detector to enable accurate object classification over downstream data. Besides, KGD works for both image and text data and allow extracting and transferring vision KG (VKG) and language KG (LKG), providing complementary knowledge for adapting large-vocabulary object detectors for handling various unlabelled downstream domains.", "description": "This figure illustrates the Knowledge Graph Distillation (KGD) method. KGD consists of two stages: Knowledge Graph Extraction (KGExtract) and Knowledge Graph Encapsulation (KGEncap).  KGExtract uses CLIP to create knowledge graphs (KGs) from downstream data, including a Language KG (LKG) and a Vision KG (VKG). These KGs capture semantic relationships from CLIP. KGEncap then integrates these KGs into a large-vocabulary object detector to improve object classification across domains.  The figure highlights the flow of information, showing how image and text data are processed by CLIP to generate the KGs, which are then used to enhance the object detector's performance.", "section": "3 Method"}, {"figure_path": "deZpmEfmTo/figures/figures_23_2.jpg", "caption": "Figure 2: Overview of the proposed Knowledge Graph Distillation (KGD). KGD comprises two consecutive stages including Knowledge Graph Extraction (KGExtract) and Knowledge Graph Encapsulation (KGEncap). KGExtract employs CLIP to encode downstream data as nodes and considers their feature distances as edges, explicitly constructing KGs including language knowledge graph (LKG) and vision knowledge graph (VKG) that inherit the rich semantic relations in CLIP. The dashed reddish lines between LKG and VKG represent the cross-modal edges that connect the nodes between vision and language modalities, enabling the integration of both language and visual information. KGEncap transfers the extracted KGs into the large-vocabulary object detector to enable accurate object classification over downstream data. Besides, KGD works for both image and text data and allow extracting and transferring vision KG (VKG) and language KG (LKG), providing complementary knowledge for adapting large-vocabulary object detectors for handling various unlabelled downstream domains.", "description": "This figure illustrates the Knowledge Graph Distillation (KGD) framework.  KGD consists of two main stages:  Knowledge Graph Extraction (using CLIP to create language and vision knowledge graphs) and Knowledge Graph Encapsulation (transferring the knowledge graphs into a large-vocabulary object detector for improved classification). The figure shows how CLIP encodes downstream data, building both language and vision knowledge graphs, and how these graphs are integrated and used for improved object detection accuracy.", "section": "3 Method"}, {"figure_path": "deZpmEfmTo/figures/figures_23_3.jpg", "caption": "Figure 2: Overview of the proposed Knowledge Graph Distillation (KGD). KGD comprises two consecutive stages including Knowledge Graph Extraction (KGExtract) and Knowledge Graph Encapsulation (KGEncap). KGExtract employs CLIP to encode downstream data as nodes and considers their feature distances as edges, explicitly constructing KGs including language knowledge graph (LKG) and vision knowledge graph (VKG) that inherit the rich semantic relations in CLIP. The dashed reddish lines between LKG and VKG represent the cross-modal edges that connect the nodes between vision and language modalities, enabling the integration of both language and visual information. KGEncap transfers the extracted KGs into the large-vocabulary object detector to enable accurate object classification over downstream data. Besides, KGD works for both image and text data and allow extracting and transferring vision KG (VKG) and language KG (LKG), providing complementary knowledge for adapting large-vocabulary object detectors for handling various unlabelled downstream domains.", "description": "This figure illustrates the Knowledge Graph Distillation (KGD) framework. KGD consists of two main stages: Knowledge Graph Extraction and Knowledge Graph Encapsulation. In the extraction stage, CLIP is used to encode downstream data into nodes and compute feature distances as edges to create both language (LKG) and vision (VKG) knowledge graphs.  These graphs capture semantic relationships present in CLIP. The encapsulation stage then transfers these KGs into the LVD for improved cross-domain object classification.  The framework supports both image and text data, using LKG and VKG to offer complementary knowledge for better LVD adaptation.", "section": "3 Method"}, {"figure_path": "deZpmEfmTo/figures/figures_24_1.jpg", "caption": "Figure 2: Overview of the proposed Knowledge Graph Distillation (KGD). KGD comprises two consecutive stages including Knowledge Graph Extraction (KGExtract) and Knowledge Graph Encapsulation (KGEncap). KGExtract employs CLIP to encode downstream data as nodes and considers their feature distances as edges, explicitly constructing KGs including language knowledge graph (LKG) and vision knowledge graph (VKG) that inherit the rich semantic relations in CLIP. The dashed reddish lines between LKG and VKG represent the cross-modal edges that connect the nodes between vision and language modalities, enabling the integration of both language and visual information. KGEncap transfers the extracted KGs into the large-vocabulary object detector to enable accurate object classification over downstream data. Besides, KGD works for both image and text data and allow extracting and transferring vision KG (VKG) and language KG (LKG), providing complementary knowledge for adapting large-vocabulary object detectors for handling various unlabelled downstream domains.", "description": "This figure illustrates the Knowledge Graph Distillation (KGD) method proposed in the paper.  KGD has two main stages: Knowledge Graph Extraction and Knowledge Graph Encapsulation.  The extraction stage uses CLIP to create knowledge graphs (KGs) from downstream data, representing both language (LKG) and visual (VKG) aspects.  These KGs capture semantic relationships learned by CLIP.  The encapsulation stage integrates these KGs into a large-vocabulary object detector (LVD) to improve its object classification ability. The process is designed to handle multiple unlabeled downstream domains by using both language and visual information.", "section": "3 Method"}, {"figure_path": "deZpmEfmTo/figures/figures_24_2.jpg", "caption": "Figure 2: Overview of the proposed Knowledge Graph Distillation (KGD). KGD comprises two consecutive stages including Knowledge Graph Extraction (KGExtract) and Knowledge Graph Encapsulation (KGEncap). KGExtract employs CLIP to encode downstream data as nodes and considers their feature distances as edges, explicitly constructing KGs including language knowledge graph (LKG) and vision knowledge graph (VKG) that inherit the rich semantic relations in CLIP. The dashed reddish lines between LKG and VKG represent the cross-modal edges that connect the nodes between vision and language modalities, enabling the integration of both language and visual information. KGEncap transfers the extracted KGs into the large-vocabulary object detector to enable accurate object classification over downstream data. Besides, KGD works for both image and text data and allow extracting and transferring vision KG (VKG) and language KG (LKG), providing complementary knowledge for adapting large-vocabulary object detectors for handling various unlabelled downstream domains.", "description": "This figure illustrates the Knowledge Graph Distillation (KGD) framework.  KGD consists of two main stages: Knowledge Graph Extraction (using CLIP to encode data as nodes and distances as edges to create LKG and VKG) and Knowledge Graph Encapsulation (transferring the extracted knowledge graphs into the object detector for improved classification).  The figure highlights the interplay between language and vision knowledge graphs, showing how they complement each other for better cross-domain adaptation.", "section": "3 Method"}]