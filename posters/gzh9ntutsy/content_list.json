[{"type": "text", "text": "Least Squares Regression Can Exhibit Under-Parameterized Double Descent ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xinyue Li Rishi Sonthalia Applied Math, Yale University Math, Boston College xinyue.li.xl728@yale.edu rishi.sonthalia@bc.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The relationship between the number of training data points, the number of parameters, and the generalization capabilities of models has been widely studied. Previous work has shown that double descent can occur in the over-parameterized regime and that the standard bias-variance trade-off holds in the under-parameterized regime. These works provide multiple reasons for the existence of the peak. We postulate that the location of the peak depends on the technical properties of both the spectrum as well as the eigenvectors of the sample covariance. We present two simple examples that provably exhibit double descent in the under-parameterized regime and do not seem to occur for reasons provided in prior work. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper demonstrates interesting new phenomena that suggest that our understanding of the relationship between the number of data points, the number of parameters, and the generalization error is incomplete, even for simple linear models. The classical bias-variance theory postulates that the generalization risk versus the number of parameters for a fixed number of training data points is U-shaped (Figure $1\\mathrm{a}^{1}$ ). However, modern machine learning has shown that if we keep increasing the number of parameters, the generalization error eventually starts decreasing again [2, 3] (Figure $1\\mathfrak{b}^{2}.$ ). This second descent has been termed as double descent and occurs in the over-parameterized regime, which is when the number of parameters exceeds the number of data points. Understanding the location and the cause of such peaks in the generalization error is of significant importance. ", "page_idx": 0}, {"type": "image", "img_path": "gzh9nTUtsY/tmp/6ca9eb47ab6d76d9f278e8f8a21985ca47f0c34265d807b392766ace45e80dd1.jpg", "img_caption": ["(a) Classical Bias Variance Trade-off. ", "Figure 1: Bias-variance trade-off and double descent. ", "(b) Modern Double Descent. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Many different theories have been postulated for the appearance of the peak. The prevalent theory is that when the model is under-parameterized, the learning is constrained. This constraint on the learning results in increased variance until the interpolation point. After this point, there exists a high dimensional space of solutions, and learning methods, such as gradient descent, pick solutions that generalize well. This conjecture has been empirically validated for deep neural networks. Due to the challenges of analyzing deep neural networks, theoretical understanding of this phenomenon has focused on linear models - linear regression [4\u201313] and kernelized regression [14\u201322]. These works show that there exists a peak at the boundary between the under and over-parameterized regimes. Hence validating the above postulated theory for their setting. Careful theoretical analysis then shows that the generalization error can be decomposed into various terms, one of which is the norm of the estimator. Specifically, it has been shown that the curve for the norm of the estimator versus the dimension of the data also exhibits double descent, with the peak occurring at the same point as the peak in the generalization error curve. In most cases, this is the only term in the decomposition that exhibits double descent. This leads to a second theory for the occurrence of the peak, that is, the peak in the generalization error for linear models occurs due to the norm of the estimator blowing up. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Contributions. Since understanding the reasons that peaks occur is of critical importance, it is crucial that we have a robust theory for their appearance. However, most work focuses on the overparameterized regime and ignores the under-parameterized regime. This is because it is commonly believed that the variance is monotonic in the under-parameterized regime. We show that this is not true and present two simple examples that exhibit double descent in the under-parameterized regime. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Why does the Peak Occur. We argue that the location of the peak depends on two factors: the alignment between the targets $y$ and singular vectors $V$ of the training data matrix and the spectrum of the data. We show that by modifying these quantities appropriately, we can move the peak into the underparameterized regime. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Modifying the Alignment. For the first example, we consider a spiked covariate model, where one eigendirection dominates, and the regression target only depends on the dominant eigendirection. For this model, we consider the ridge regularized problem with ridge parameter $\\mu^{2}$ and show that the ridge parameter $\\mu^{2}$ controls the alignment between the targets $y$ and the singular vectors $V$ . We show that for $\\mu>0$ the peak occurs in the under-parameterized regime (Theorem 2). Specifically, when the ratio of the dimension to the number of training data points $c$ is equal to $(\\dot{1}+\\mu^{2})^{-1}$ $(c:=d/n=1/(1+\\mu^{2}))$ . ", "page_idx": 1}, {"type": "text", "text": "\u2022 Modifying the Spectrum For the second example, we consider training data that is a mixture of isotropic Gaussian vectors and vectors from along a fixed direction $z$ . By varying the mixture proportions $(\\pi_{1},\\pi_{2})$ , we can modify the spectrum of the covariance matrix. We show that the expected risk displays under-parameterized double descent (Theorem 4), with the peak occurring when $c:=d/n\\stackrel{=}{=}\\pi_{1}^{\\cdot}$ ). ", "page_idx": 1}, {"type": "text", "text": "\u2022 Norm of the estimator. We further analyze the first example and show that if we fix the number of training data points $n$ and vary the dimension $d$ of the problem, then for large values of $\\mu$ , the risk curve does not display a double descent. However, the curve for the norm of the estimator does display descent. Thus, the peak in the norm of the estimator does not imply a peak in the generaliation error. ", "page_idx": 1}, {"type": "text", "text": "Organization. The rest of the paper is organized as follows. Section 2 presents a quick overview of prior work on double descent for linear models. Section 3 highlights two less-studied properties that influence the location of the peak. Section 4 presents the first of the two examples of underparameterized double descent. This model also shows that a double descent in the norm of the estimator does not translate to a double descent in the risk. Finally, Section 5 presents the second example of under-parameterized double descent. ", "page_idx": 1}, {"type": "text", "text": "2 Prior Work on Double Descent ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we present the current prevailing theories for the occurrence of local maximums in the risk curve. Concretely, consider the following simple linear model that is a special case of the general models studied in [5, 8, 11, 23] amongst many other works. Let $x_{i}\\sim\\mathcal{N}(\\bar{0_{,}}\\,I_{d})$ and let $\\beta\\in\\breve{\\mathbb{R}}^{d}$ be a linear model with $\\|\\beta\\|=1$ . Let $y_{i}=\\overline{{\\beta}}^{T}x_{i}+\\overline{{\\xi}}_{i}$ where $\\xi\\sim\\mathcal{N}(0,1)$ . Then, let $\\beta_{o p t}$ be the minimum norm solution to $\\arg\\operatorname*{min}_{\\tilde{\\beta}}\\|\\beta^{T}X_{t r n}-\\tilde{\\beta}^{T}X_{t r n}+\\xi_{t r n}\\|$ , where $\\xi_{t r n}\\in\\mathbb{R}^{n\\times1}$ . One important quantity is the aspect ratio of $X$ . Specifically, for a $d\\times n$ matrix, the aspect ratio is $c:=d/n$ . With this terminology, we see that a model is under-parameterized if $c<1$ and over-parameterized when $c>1$ . ", "page_idx": 1}, {"type": "text", "text": "Table 1: Table showing various assumptions on the data and the location of the double descent peak for linear regression and denoising. We only present a subset of references for each problem setting. For the low rank setting in this paper, see Appendix F. ", "page_idx": 2}, {"type": "table", "img_path": "gzh9nTUtsY/tmp/fae5ebff57add76891d19a1fb0c11b55258d7b22e76a222a7d77271dc2a61a54.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "Finally, the interpolation point, i.e., the point at which we can exactly fit the training data, is $c=1$ , assuming we have full-rank data. ", "page_idx": 2}, {"type": "text", "text": "Then, the excess risk $\\mathcal{R}$ and the expected norm of $\\beta_{o p t}$ can be expressed as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{R}=\\left\\{\\frac{c}{1-c}\\qquad}&{c<1}\\\\ {\\frac{c-1}{c}+\\frac{1}{c-1}}&{c>1}\\end{array}\\right.\\quad\\mathrm{and}\\quad\\beta_{o p t}=\\left\\{\\frac{1+\\frac{c}{1-c}}{\\frac{c}{c}+\\frac{1}{c-1}}\\quad c<1\\right..\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In this model, there are a few important features that are ubiquitous in many prior double descent studies for linear models: ", "page_idx": 2}, {"type": "text", "text": "1. The peak happens at $c\\,=\\,1$ , on the border between the under and over-parameterized regimes.   \n2. Further, at $c=1$ the training error equals zero. Hence, this is the interpolation point.   \n3. The peak occurs due to the expected norm of the estimator $\\beta_{o p t}$ blowing up near the interpolation point. ", "page_idx": 2}, {"type": "text", "text": "This is further validated by works that study ridge regularized regression [23\u201326]. Works such as [23] have shown that optimally regularized regression no longer exhibits double descent. Further, increasing the amount of regularization from zero to the optimal amount of regularization results in the magnitude of the peak in the generalization getting smaller until a peak no longer exists. However, the location of the peak does not change by changing the amount of regularization. Further, Chen, Min, Belkin, and Karbasi [27] proved that double descent cannot take place in the under-parameterized regime for the above model. ", "page_idx": 2}, {"type": "text", "text": "Subsequently, works such as [10, 23, 28\u201331] show that there can be multiple descents in the overparameterized regime. Specifically, d\u2019Ascoli, Sagun, and Biroli [30] show that the first peak in triple descent is due to the norm of the estimator peaking and that the second peak is due to the initialization of the random features. Their results, Figure 3 in [30], show that the peaks only occur if the model is over-parameterized. Further Chen, Min, Belkin, and Karbasi [27] show that by considering a variety of product data distributions, any shaped risk curve can be observed in the over-parameterized regime. Finally, Curth, Jeffares, and van der Schaar [32] says that the peak occurs at the point of effective dimensionality of the model and not the true dimensionality. Here, we see that there are three other reasons provided for the occurrence of peaks in the risk curve. ", "page_idx": 2}, {"type": "text", "text": "1. Regularization can reduce the effective dimensionality of the model and move the peak to the right into the over-parameterized regime [32].   \n2. For random feature models, we see that the random initialization results in a second peak in the over-parameterized regime [30].   \n3. Due to the data having a complex covariance structure, any shaped risk curve is possible in the over-parameterized regime [27]. ", "page_idx": 2}, {"type": "text", "text": "Other works [33\u201336] have considered the problem for other loss models and shown a variety of different risk curves can exist. Table 1 summarizes some of the prior work. ", "page_idx": 2}, {"type": "text", "text": "Double Descent with Input Noise. There has also been prior work that studies double descent for models with input noise rather than output noise [24, 37, 38]. From these Sonthalia and Nadakuditi [24] and Kausik, Srivastava, and Sonthalia [37] consider the unregularized problem and show that the peak occurs at the boundary. Dhifallah and Lu [38] considers ridge regularization with isotropic Gaussian data and again sees that the peak occurs at the boundary. ", "page_idx": 3}, {"type": "text", "text": "3 Spectral Properties of the Data Affect the Peak Location ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we identify two important spectral properties that govern the location of the peak. In later sections, we delve into two examples that modify these properties and move the peak into the under-parameterized regime. We begin with definitions and notations. Throughout the paper, we assume that training data $X=[x_{1}\\ .\\ .\\ .\\ x_{n}]\\in\\mathbb{R}^{d\\times n}$ and $\\boldsymbol{y}=[y_{1}\\ .\\ .\\ .\\ ,y_{n}]\\in\\mathbb{R}^{k\\times n}$ . We are interested in the standard ridge regularized least squares problem. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\hat{\\beta}}\\|y-\\hat{\\beta}^{T}X\\|_{F}^{2}+\\mu^{2}\\|\\hat{\\beta}\\|^{2}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In the unregularized case, the minimum norm solution is given by $\\hat{\\beta}^{T}\\,=\\,y X^{\\dagger}$ , where $X^{\\dagger}$ is the Moore-Penrose Pseudoinverse of $X$ . Prior work on linear models has shown that a double descent in the risk is due to a double descent in the norm of the estimator. Suppose $X=U\\Sigma V^{T}$ is the SVD, $\\hat{\\beta}\\in\\mathbb{R}^{d\\times1}$ , then using unitary invariance, we have that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\|\\hat{\\beta}\\|^{2}=\\sum_{i=1}^{\\operatorname{rank}(X)}\\frac{(y V)_{i}^{2}}{\\sigma_{i}^{2}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\sigma_{i}$ is the $i$ th singular value. Hence, this is controlled by ", "page_idx": 3}, {"type": "text", "text": "1. The alignment between $y$ and $V$ . Here $V$ are the eigenvectors of the data gram matrix.   \n2. The spectrum of the gram matrix $X^{T}X$ . ", "page_idx": 3}, {"type": "text", "text": "Many prior works deal with the alignment in one of two ways. If $y=\\beta^{T}{\\cal X}+\\xi$ , with $\\xi$ having an isotropic distribution, then prior works either assume that $\\beta$ has an isotropic distribution [4, 5, 42] or they assume that $X$ is isotropic Gaussian or that $x_{i}=\\check{\\Sigma}^{1/2}z_{i}$ , where $z_{i}$ is from an isotropic Gaussian [23, 43] and $\\check{\\Sigma}$ is a deterministic matrix. For example, if $\\beta$ has an isotropic distribution, taking the expectation with respect to $\\beta,\\xi$ we get that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\beta,\\xi}\\left[\\|\\hat{\\beta}\\|^{2}\\right]=\\mathbb{E}[\\beta_{1}^{2}]\\|X X^{\\dagger}\\|_{F}^{2}+\\mathbb{E}[\\xi_{1}^{2}]\\sum_{i=1}^{\\operatorname{rank}(X)}\\frac{1}{\\sigma_{i}^{2}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This quantity is then studied by looking at the distribution of the spectrum as $d,n\\to\\infty$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 1. Given a random matrix $A$ , if $\\lambda_{1},\\ldots,\\lambda_{k}$ are its eigenvalues. Then the empirical spectral distribution $(E S D)$ is the following sum of Dirac delta measure ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\nu^{k}=\\frac{1}{k}\\sum_{i=1}^{k}\\delta_{\\lambda_{i}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and the limiting spectral distribution $\\nu$ is a measure such that $\\nu^{k}\\to\\nu$ weakly almost surely. ", "page_idx": 3}, {"type": "text", "text": "In general, the limiting distribution $\\nu_{c}$ depends on the limiting aspect ratio $(i.e.,d/n\\rightarrow c)$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 2. Given a measure $\\nu_{c}$ that is supported on the interval $J\\subset\\mathbb{R},$ , the Stieljtes transform is ", "page_idx": 3}, {"type": "equation", "text": "$$\nm_{\\nu_{c}}(\\zeta)=\\mathbb{E}_{\\lambda\\sim\\nu_{c}}\\left[\\frac{1}{\\lambda-\\zeta}\\right],\\,\\,\\zeta\\in\\mathbb{C}\\setminus J.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "One common assumption is that the limiting distribution of the empirical spectral distribution is the Marchenko-Pastur distribution [44]. Other limiting distributions have been considered in [5, 45, 46]. For the Marchenko-Pastur distribution, it is known (see, for example, Lemma 5 in [24]) that ", "page_idx": 3}, {"type": "equation", "text": "$$\nm_{\\nu_{c}}(0)=\\mathbb{E}_{\\lambda\\sim\\nu_{c}}\\left[\\frac{1}{\\lambda}\\right]=\\left\\{\\frac{c}{1-c}\\quad c<1\\atop\\frac{1}{c-1}\\right..\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Hence, the risk is governed by the value of the Stieljtes transform of the limiting spectrum at $\\zeta=0$ . In particular, for the above example, the location of the peak of the risk as a function of $c$ depended on the location of the peak of ", "page_idx": 4}, {"type": "equation", "text": "$$\nc\\mapsto m_{\\nu_{c}}(0)=:G(c).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Hence the risk depends on both the spectrum and the alignment between $y$ and $V$ . Thus, the peak occurs at $c=1$ because of the following two conditions. ", "page_idx": 4}, {"type": "table", "img_path": "gzh9nTUtsY/tmp/1e4cd3373ab6757c22e437ce7fa2b226e4c4ecb20f1392c8362376035fb13cb0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "In this paper, we show that violating either one of the above two assumptions can move the peak from the interpolation point into the under-parameterized regime. ", "page_idx": 4}, {"type": "text", "text": "4 Alignment Mismatch ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we present the first example that exhibits double descent in the under-parameterized regime. This model violates Assumption 1. ", "page_idx": 4}, {"type": "text", "text": "4.1 Model Assumptions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "For any $k\\leq d$ , let $\\beta\\in\\mathbb{R}^{d\\times k}$ be fixed such that the operator norm $||\\beta^{T}||$ is $\\Theta(1)$ . Let $X_{t r n}\\in\\mathbb{R}^{d\\times n}$ be the signal matrix and $A_{t r n}\\in\\mathbb{R}^{d\\times n}$ be the noise matrix. Then, the ridge regularized least square estimator $W_{o p t}$ is the minimum norm solution to ", "page_idx": 4}, {"type": "equation", "text": "$$\nW_{o p t}:=\\underset{W}{\\arg\\operatorname*{min}}\\;\\|\\beta^{T}X_{t r n}-W(X_{t r n}+A_{t r n})\\|_{F}^{2}+\\mu^{2}\\|W\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Given test data $X_{t s t}+A_{t s t}$ , the mean squared generalization error is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{R}(W_{o p t})=\\mathbb{E}_{A_{t r n},A_{t s t}}\\left[\\frac{\\|\\beta^{T}X_{t s t}-W_{o p t}(X_{t s t}+A_{t s t})\\|_{F}^{2}}{n_{t s t}}\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Assumption 3. Let $\\mathcal{U}\\subset\\mathbb{R}^{d}$ be a one dimensional space with a unit basis vector $u$ . Then let $X_{t r n}=\\Bar{\\sigma}_{t r n}u v_{t r n}^{T}\\in\\mathbb{R}^{d\\times n}$ and $\\boldsymbol{X_{t s t}}=\\sigma_{t s t}u\\boldsymbol{v}_{t s t}^{T}\\in\\mathbb{R}^{d\\times{n_{t s t}}}$ be \u221athe respective SVDs\u221a for the training data and test data matrices. We further assume that $\\sigma_{t r n}=O({\\sqrt{n}})$ and $\\sigma_{t s t}=O(\\sqrt{n_{t s t}})$ . ", "page_idx": 4}, {"type": "text", "text": "There are no assumptions on the distribution of $v_{t r n},v_{t s t}$ besides having unit norm. First, we see that the data $X+A$ has a spiked covariance, with the dominant eigendirection closely aligned with $u$ . Since the targets only depend on $X$ , we consider $A$ to represent noise. Even with the rank 1 assumption, the model captures many different scenarios. If $k=1$ , then the problem is similar to error-in-variables regression. If $k=d$ and $\\beta=I$ , then this is the supervised denoising problem. If the columns of $X_{t r n}$ are all $\\pm u$ and $\\beta=u$ , then this captures the binary classification problem (with MSE loss) for two Gaussian clusters centered at $u$ and $-u$ with labels $\\pm1$ . ", "page_idx": 4}, {"type": "text", "text": "Assumptions about $A$ . The analysis works for general assumptions in [24]. For simplicity, we assume that the matrix $A$ has I.I.D. entries drawn from a normalized Gaussian. ", "page_idx": 4}, {"type": "text", "text": "Assumption 4. The entries of the matrices $A\\in\\mathbb{R}^{d\\times n}$ are I.I.D. from $\\mathcal{N}(0,1/d)$ . ", "page_idx": 4}, {"type": "text", "text": "4.2 Expected Risk and Peak Location ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We begin by providing a formula for the generalization error given by Equation 2 for the least squares solution given by Equation 1. All proofs are in Appendix E. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1 (Generalization Error Formula). Suppose the training data $X_{t r n}$ and test data $X_{t s t}$ satisfy Assumption $^3$ and the noise $A_{t r n},A_{t s t}$ satisfy Assumption 4. Let $\\mu$ be the regularization parameter. Then for the under-parameterized regime (i.e., $c<1$ ) for the solution $W_{o p t}$ to Problem $^{\\,l}$ , the generalization error or risk given by Equation 2 is given by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathfrak{L}(c,\\mu)=\\frac{c\\sigma_{t r n}^{2}(\\sigma_{t r n}^{2}+1))}{2d\\tau^{2}}\\frac{1+c+\\mu^{2}c}{\\sqrt{(1-c+\\mu^{2}c)^{2}+4\\mu^{2}c^{2}}}-\\tau^{-2}\\frac{c\\sigma_{t r n}^{2}(\\sigma_{t r n}^{2}+1))}{2d}+\\tau^{-2}\\frac{\\sigma_{t s t}^{2}}{n_{t s t}}+o\\left(\\frac{1}{d}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{1}{\\tau}=\\frac{2||\\beta^{T}u||}{2+\\sigma_{t r n}^{2}(1+c+\\mu^{2}c-\\sqrt{(1-c+\\mu^{2}c)+4\\mu^{2}c^{2}})}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Sketch. The proof proceeds in four key steps. First, we use the Sherman-Morrison formula for pseudoinverses [47]. Next, we decompose the error into constituent dependent quadratic forms. Through random matrix theory and concentration of measure arguments, we demonstrate that each quadratic form concentrates around a deterministic value characterized by the Stieltjes transform of the limiting empirical spectral distribution. Finally, we establish concentration bounds for the products and sums of these dependent forms, yielding the desired error rate. \u53e3 ", "page_idx": 5}, {"type": "text", "text": "Since the focus is on the under-parameterized regime, Theorem 1 only presents the underparameterized case. The over-parameterized case can be found in Appendix E.3. Due to the complexity of the expression, it is difficult to discern how the risk scales with respect to the training data signal strength $\\dot{\\sigma}_{t r n}^{2}$ , the regularization strength $\\mu$ , or the aspect ratio $c$ . Since the focus of the paper is the scaling with respect to $c$ , we present the connection between the risk curve and $c$ in the main text. However, the shape of the risk curve with respect to the other parameters is also interesting and can be found in Appendix D. ", "page_idx": 5}, {"type": "text", "text": "To understand the shape of the risk curve as $c$ varies, we first consider that data scaling regime. That is, fix $d$ and change $n$ . The following theorem 2 shows that the risk curve is theoretically guaranteed to have a peak at $\\begin{array}{r}{\\bar{c}=\\frac{1}{1+\\mu^{2}}}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 2 (Under-Parameterized Peak). Let $\\mu\\in\\mathbb{R}_{>0}$ , $\\sigma_{t r n}^{2}=n=d/c$ and $\\sigma_{t s t}^{2}=n_{t s t}$ , and $d$ is sufficiently large, so that the error term $o(1/d)$ is small, then the risk $\\mathcal{R}(c)$ from Theorem $^{\\,l}$ , as $a$ function of c, has a local maximum in the under-parameterized regime at c =1+1\u00b52 . ", "page_idx": 5}, {"type": "text", "text": "Theorem 2 contrasts with prior works, in which double descent occurs in the over-parameterized regime or on the boundary between the two regimes. We numerically verify the predictions from Theorems 1 and 2. Figure 2 shows that the theoretically predicted risk matches the numerical risk, thus verifying that double descent occurs in the under-parameterized regime.3 ", "page_idx": 5}, {"type": "image", "img_path": "gzh9nTUtsY/tmp/3fe61ca034cfc788b96ec2394363ac4c005a080ef27dc3c82e1ca7837156025d.jpg", "img_caption": ["Figure 2: Figure showing the theoretical risk curve from Theorem 1 and empirical values in the d\u221aata scaling \u221aregime for different values of $\\mu$ [(L) $\\mu=0.1$ , (C) $\\mu=1$ , (R) $\\mu=2$ ]. Here $\\sigma_{t r n}=$ $\\sqrt{n},\\sigma_{t s t}=\\sqrt{n_{t s t}}$ , $d=1000$ , $n_{t s t}=1000$ . For each empirical point, we ran at least 100 trials. More details can be found in Appendix G. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "4.3 The Peak Occurs Due to Alignment Mismatch ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now show that the peak occurs due to a mismatch between the target vector and the right singular vectors of the input data. To begin, note that the ridge regularized problem can be written as follows ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\beta^{T}\\underbrace{\\left[X_{t r n}\\;\\mathbf{0}_{d\\times d}\\right]}_{\\hat{X}_{t r n}}-W([X_{t r n}\\;\\mathbf{0}_{d\\times d}]+\\underbrace{[A_{t r n}\\;\\mu I]]}_{\\hat{A}_{t r n}})\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In this expression, $y=\\beta^{T}\\hat{X}_{t r n}=(\\beta^{T}u)[v_{t r n}^{T}\\;\\mathbf{0}_{p}]$ . Hence the direction is given by $\\hat{v}_{t r n}^{T}=[v_{t r n}^{T}\\,\\mathbf{0}_{p}]$ . The right singular vectors of $\\hat{X}_{t r n}+\\hat{A}_{t r n}$ are more difficult to compute,thus we use proxies. Since $\\hat{X}_{t r n}$ is rank 1, we use the right singular vectors of $\\hat{A}_{t r n}$ as a proxy. Lemma 5 in the appendix, shows that if $A\\,=\\,U\\Sigma V^{T}$ , then we can express $\\hat{A}_{t r n}$ as $\\hat{U}\\hat{\\Sigma}\\hat{V}^{T}$ , where $\\hat{U}=U$ , $\\hat{\\Sigma}^{2}\\,=\\,\\bar{\\Sigma^{2}}+\\mu^{2}I$ , and $\\hat{V}=\\left[\\overset{V_{1:p}\\Sigma\\hat{\\Sigma}^{-1}}{\\mu U\\hat{\\Sigma}^{-1}}\\right]\\in\\mathbb{R}^{n+d\\times d}$ . Here $V_{1:d}$ are the first $d$ columns of $V$ . Then, ", "page_idx": 6}, {"type": "equation", "text": "$$\ny\\hat{V}=(\\beta^{T}u)(\\hat{v}_{t r n}^{T}V_{1:d})\\Sigma\\hat{\\Sigma}^{-1}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Since $V_{1:d}$ came from a Gaussian random matrix, $(\\hat{v}_{t r n}^{T}V_{1:d})$ has isotropic entries. However, the diagonal matrix $\\Sigma\\hat{\\Sigma}^{-1}$ results in the entries of $y\\hat{V}$ not being isotropic. Note when $\\mu=0$ , $\\Sigma\\hat{\\Sigma}^{-1}=I$ hence it is isotropic. Hence, $\\mu$ controls the deviation from isotropy. ", "page_idx": 6}, {"type": "text", "text": "We use $\\hat{\\Sigma}^{T}\\hat{\\Sigma}I$ as a proxy for the spectrum of the sample covariance. By Lemma 5, we have that $\\hat{\\Sigma}^{T}\\hat{\\Sigma}=\\Sigma^{T}\\Sigma\\!+\\!\\mu^{2}$ . We know that the limiting spectrum for $\\Sigma^{T}\\Sigma$ is the Marchenko-Pastur distribution for which the map $G(c)=m_{\\nu_{c}}(0)$ has a maximum for $c=1$ . Shifting the spectrum changes the magnitude of the peak but does not change the location. This suggests that the peak occurs due to the misalignment between the target vector and the right singular vectors of the input data. ", "page_idx": 6}, {"type": "text", "text": "Ablation experiment To verify that the location of the peak is due to the misalignment, we conduct two experiments. First, we solve the unregularized problem. However, we change the spectrum of the noise matrix $A_{t r n}$ . That is, instead of using $A_{t r n}=U\\Sigma V^{T}$ , we use $\\tilde{A}={\\bar{U(}}\\Sigma^{2}+{\\bar{\\mu^{2}}}I)^{1/2}V^{T}$ . If the spectrum was the primary factor determining the location of the peak, the peak should occur at $c=1/(1+\\mu^{2})$ . However, as seen in Figure 3a it still occurs at $c=1$ . Second, we replace $\\hat{V}$ with a uniformly random orthogonal matrix $Q$ 4. Clearly, $y Q$ is now isotropic. Figure 3b shows that, in this case, the peak moves to the over-parameterized regime. ", "page_idx": 6}, {"type": "image", "img_path": "gzh9nTUtsY/tmp/7d28da9889b33a9f53ed5c6697b593981a015601c02065411ce3937b2520726f.jpg", "img_caption": ["Figure 3: Risk for the ablation experiment. Left: Empirical Expected Risk when using $\\tilde{A}$ for the noise. Right: Empirical risk when we replace $\\hat{V}$ with a random orthogonal matrix. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Connection to Prior Double Descent Theory Prior double descent theory postulates that the peak for the ridge regularized model occurs at the interpolation point for the unregularized model or further to the right into the over-parameterized regime. Hence, this model goes against prior expectations, with the peak moving to the left into the under-parameterized regime. However, there might still be some connection between the training error and the location of the peak. For example, the peak may correspond to a local minimum of the training loss. We explore this in Appendix C and see only a weak connection with the third derivative of the training error. ", "page_idx": 6}, {"type": "text", "text": "4.4 Peak in the Norm of the Estimator Does Not Imply a Peak in the Risk ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Prior double descent theories suggest that double descent occurs due to the norm of the estimator increasing and then decreasing. This is true for the above model where we fixed $d$ and varied $n$ . However, the connection breaks if we fix $n$ and vary $d$ instead (parameter scaling regime). This difference between the two regimes is due to the normalization considered and has been observed before [30]. ", "page_idx": 7}, {"type": "text", "text": "Figure 4 shows that for the parameter scaling regime, for small values of $\\mu$ , we see underparameterized double descent. However, as we increase $\\mu$ , the risk curve becomes monotonic. Nevertheless, as shown in Figure 5, for larger values of $\\mu$ , there is still a peak in the curve for the norm of the estimator $\\|\\boldsymbol{W_{o p t}}\\|_{F}^{2}$ . Hence, the curve for the norm of the estimator exhibits underparameterized double descent even if the risk does not. This is further highlighted in Figure 6. Here, we see that even though the variance is non-monotonic, the risk is dominated by the bias term. Thus, we show that a peak in the generalization error for linear models does not imply a peak in the norm of the estimator. The following theorem provides a local maximum in the E $[\\|\\bar{W_{o p t}}\\|_{F}^{2}]$ curve for $c<1$ . ", "page_idx": 7}, {"type": "text", "text": "Theorem 3 $\\mathit{\\check{\\Psi}}[|W_{o p t}||_{F}$ Peak). If $\\sigma_{t s t}=\\sqrt{n_{t s t}},$ , $\\sigma_{t r n}=\\sqrt{n}$ and $\\mu$ is such that $p(\\mu)<0,$ , then for fixed n that is sufficiently large enough, we have that $\\mathbb{E}\\left[\\Vert W_{o p t}\\Vert_{F}\\right]$ versus $c=d/n$ curve has a local maximum in the under-parameterized regime at $c=(\\mu^{2}+1)^{-1}$ . ", "page_idx": 7}, {"type": "image", "img_path": "gzh9nTUtsY/tmp/4b7803430ea615ff9b86570a4fa6e6ebe13fc0c477b2a626fd5542f6cf1ab269.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 4: Figure showing the theoretical risk curve from Theorem 1 and empirical values in the parameter scaling regime for different values of $\\mu$ [ $\\mathrm{(L)}$ $\\mu=0.1$ , (C) $\\mu=0.2$ , (R) $\\mu=0.5]$ . Here, only $\\mu=0.1$ has a local peak. Here $n=n_{t s t}=1000$ and $\\sigma_{t r n}=\\sigma_{t s t}=\\sqrt{1000}$ . Each empirical point is an average of 100 trials. ", "page_idx": 7}, {"type": "image", "img_path": "gzh9nTUtsY/tmp/decca2cd74c3136919c6bc964ec6be752d63411d000b6e7a90344a729226d133.jpg", "img_caption": ["Figure 5: Figure showing generalization error versus $\\mathbb{E}\\left[\\Vert W_{o p t}\\Vert_{F}^{2}\\right]$ for the parameter scaling regime for three different values of $\\mu$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "5 Shifting Local Maximum for Stieljtes Transform as a Function of $c$ ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we present the next example of under-parameterized double descent that violates Assumption 2. In particular, the maximum of the map $c\\mapsto m_{\\nu_{c}}(0)$ does not occur at $c=1$ . We show that the maximum can be chosen to be any value in $(0,1)$ . We consider the following mixture model. Let $\\pi_{1},\\pi_{2}$ be mixture weights such that $\\pi_{1}+\\pi_{2}=1$ . Then, with probability $\\pi_{1}$ , the data is sampled from $\\textstyle{\\mathcal{N}}(0,{\\frac{1}{d}}I)$ and with probability $\\pi_{2}$ , the data point is $\\alpha z$ for fixed $z\\in\\bar{\\mathbb{R}}^{d}$ and $\\alpha\\sim\\mathcal{N}(0,1)$ . For this model, the uncentered covariance matrix is given by ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}[x x^{T}]=\\pi_{1}\\mathbb{E}_{x\\sim\\mathcal{N}(0,\\frac{1}{d}I)}[x x^{T}]+\\pi_{2}\\mathbb{E}[\\alpha^{2}z z^{T}]=\\frac{\\pi_{1}}{d}I+\\pi_{2}z z^{T}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "image", "img_path": "gzh9nTUtsY/tmp/deceadaf638ccedacde385295db515117724570b00554cf6d8ee0eb89a3cd7fc.jpg", "img_caption": ["Figure 6: Figure showing the $\\mathbb{E}\\left[\\Vert W_{o p t}\\Vert_{F}^{2}\\right]$ , and the generalization error in the parameter scaling regime for $\\mu\\,=\\,1\\,$ , $\\sigma_{t r n}\\,=\\,\\sqrt{n}$ , and $\\sigma_{t s t}\\,=\\,\\sqrt{n_{t s t}}$ . Here $n\\,=\\,1000$ and $n_{t s t}\\,=\\,1000$ . For each empirical data point, we ran at least 100 trials. More details can be found in Appendix $\\mathrm{G}$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Then, the expected excess risk for a solution $\\hat{\\beta}$ compared to $\\beta$ is ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathcal{R}=\\mathbb{E}[\\|\\beta^{T}x-\\hat{\\beta}^{T}x\\|^{2}|X]=\\mathbb{E}\\left[\\frac{\\pi_{1}}{d}\\|\\beta^{T}-\\hat{\\beta}^{T}\\|^{2}+\\pi_{2}\\|(\\beta-\\hat{\\beta})^{T}z\\|^{2}|X\\right].\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Let $X_{t r n}=[A\\ z v^{T}]\\in\\mathbb{R}^{d\\times n}$ , where the $A\\in\\mathbb{R}^{d\\times n-k}$ with each column a data point sampled I.I.D. from $\\textstyle N(0,{\\frac{1}{d}}I)$ and $\\boldsymbol{v}\\in\\mathbb{R}^{k}$ is the vector with random coefficients in front of $z$ . Let $\\beta\\in\\mathbb{R}^{d}$ be the target regressor functio n and let $y^{T}=\\beta^{T}X_{t r n}+\\xi_{t r n}^{T}$ , where $\\xi_{t r n}^{T}$ has I.I.D. entries from a standard normal. Let $\\beta_{o p t}^{T}=y^{T}X_{t r n}^{T}(X_{t r n}X_{t r n}^{T})^{-1}$ be the minimum norm. Then, Theorem 4 shows that the peak occurs when $d/n=c=\\pi_{1}<1$ . To experimentally verify Theorem 4, we consider two cases, one where we enforce $\\beta\\perp z$ and one where we do not. As seen in Figure 7, Theorem 4 is accurate for both cases. This suggests that the $\\beta\\perp z$ assumption seems to only be needed for simplifying the proof. ", "page_idx": 8}, {"type": "image", "img_path": "gzh9nTUtsY/tmp/f5271637dfa72ad22c78e2ac139d8dd92421c4bbc49322b48f0ddc7e3486fce7.jpg", "img_caption": ["Figure 7: Figure showing under-parameterized double descent. (Left) We have $\\beta=d\\cdot z$ . (Right) We have $\\beta\\perp z$ . The solid blue line represents the theoretical estimate from Theorem 4 and the scatter points are from empirical experiments with $d=600$ . For the empirical points, we average over 50 trials. The dashed vertical purple line is $\\pi_{1}$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Theorem 4 (Under-parametrized Peak). For the above model, if $k/n\\rightarrow\\pi_{2}$ , and $d/n\\to c,$ , then the expected risk is given by $\\begin{array}{r}{\\mathcal{R}=\\left\\{\\!\\!\\!\\begin{array}{l l}{\\frac{\\pi_{1}c}{\\pi_{1}-c}}&{c<\\pi_{1}}\\\\ {\\pi_{1}\\left(\\frac{\\pi_{1}}{c-\\pi_{1}}+\\left(1-\\frac{\\pi_{1}}{c}\\right)\\left(\\frac{\\left\\|\\beta\\right\\|^{2}}{d}-\\frac{\\left(\\beta^{T}z\\right)^{2}}{\\|z\\|^{2}d}\\right)\\right)}&{c>\\pi_{1}}\\end{array}\\!\\!.\\right.}\\end{array}$ ", "page_idx": 8}, {"type": "text", "text": "Theorem 4 is quite surprising as it shows that the only peak in the risk curve occurs in the underparameterized regime. One might assume that is due to the low rankness of the data from the second mixture. While this is true, prior work does not indicate that this is the reason. Specifically, Huang, Hogg, and Villar [41] shows that projecting onto low-dimensional versions of the data acts as a regularizer and removes the peak altogether. $\\mathrm{\\DeltaXu}$ and Hsu [39], also looks at a similar problem, but they consider isotropic Gaussian data and project onto the first $p$ components. In this case, the data is artificially high-dimensional (since only the first $k$ coordinates are non-zero). ", "page_idx": 8}, {"type": "text", "text": "They again see a peak at the interpolation point $(n=p$ ). Wu and $\\mathrm{Xu}$ [40] also looks at a version of Principal Component Regression in which the data dimension is reduced. That is, the data is not embedded in high-dimensional space anymore. Wu and $\\mathrm{Xu}$ [40] sees a peak at the boundary between the under and over-parameterized regions. Finally, Sonthalia and Nadakuditi [24] and Kausik, Srivastava, and Sonthalia [37] look at the denoising problem for low-dimensional data and have peaks at $c=1$ . Therefore, prior work does not immediately imply that low dimensional data results in under-parameterized double descent. If we had only low-dimensional data, then the peak \u201cshould\u201d move into the over-parameterized regime. This is because if the true dimensionality of the data is $r<d$ . Then, one might think that the peak occurs when the number of training data points $n$ equals $r$ since that is the interpolation point5. We are in the over-parameterized regime since $d>r=n$ . ", "page_idx": 9}, {"type": "text", "text": "We can understand this phenomenon as follows. The data from the second mixture does not affect the smallest eigenvalue of the covariance matrix. This is because the second mixture lives in a one-dimensional space. Hence, it only affects the top eigenvalue. Since the Stieljtes transform at 0 is dominated by the behavior of the smallest non-zero eigenvalue, data from the second mixture has very little effect on the Stieljtes transform of the ESD at 0. We expect the above intuition to hold, even when replacing rank 1 with rank $r$ for any fixed small $r$ . ", "page_idx": 9}, {"type": "text", "text": "Connection to Prior Double Descent Theory For this example, it is easy to show that there is a strong connection to prior double descent theory. Specifically, even though we cannot interpolate the data, the minimum training error will occur at $c=\\pi_{1}$ . Further, we see that the blow-up in the excess risk is due to the norm of the estimator blowing up. ", "page_idx": 9}, {"type": "text", "text": "Additionally, we see that comparing with the result from [11] (which is the case when $\\pi_{2}=0$ ), we see that the $\\pi_{2}>0$ results in sifting the peak to $\\pi_{1}$ and rescales the variance by $\\pi_{1}$ . However, we also see an additional correction term in the overparameterized regime: ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\left(1-\\frac{\\pi_{1}}{c}\\right)\\left(-\\frac{(\\beta^{T}z)^{2}}{\\|z\\|^{2}d}\\right)\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Here we see that the term depends on the alignment between the target $\\beta$ and the spike direction $z$ . ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper presents two simple models with double descent in the under-parameterized regime. While such peaks seem limited to special cases, understanding the cause is important for a complete understanding of the double descent phenomenon. Our analysis reveals that the location of peaks depends critically on two properties: the alignment between targets and the eigenvectors of the training data gram matrix and the behavior of the Stieltjes transform of the limiting empirical spectral distribution. ", "page_idx": 9}, {"type": "text", "text": "We demonstrate that violating either of these properties can shift the peak into the under-parameterized regime. The first model shows that ridge regularization can create a misalignment between targets and singular vectors, leading to a peak at $c=1/(1+\\mu^{2})$ . The second model, using a mixture of isotropic Gaussian vectors and directional vectors, demonstrates that modifying the spectrum can result in a peak at $c=\\pi_{1}$ . These findings challenge several prevailing theories about double descent. They show that peaks need not occur at or beyond the interpolation threshold and that a peak in the estimator\u2019s norm does not necessarily imply a peak in the generalization error. ", "page_idx": 9}, {"type": "text", "text": "Investigating the interaction between spectral properties and generalization in deep neural networks to provide a general theory of double descent is an important avenue for future work. Understanding whether similar phenomena occur in other architectures and the implications for model selection and regularization remain open questions. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Scott Fortmann-Roe. Understanding the Bias-Variance Tradeoff, June 2012. URL: http: //scott.fortmann-roe.com/docs/BiasVariance.html (cited on page 1). ", "page_idx": 9}, {"type": "text", "text": "[2] Manfred Opper and Wolfgang Kinzel. Statistical Mechanics of Generalization. Models of Neural Networks III: Association, Generalization, and Representation, 1996 (cited on page 1). [3] Mikhail Belkin, Daniel J. Hsu, Siyuan Ma, and Soumik Mandal. Reconciling Modern MachineLearning Practice and the Classical Bias\u2013Variance Trade-off. Proceedings of the National Academy of Sciences, 2019 (cited on page 1).   \n[4] Madhu S. Advani, Andrew M. Saxe, and Haim Sompolinsky. High-dimensional Dynamics of Generalization Error in Neural Networks. Neural Networks, 2020 (cited on pages 2, 4).   \n[5] Edgar Dobriban and Stefan Wager. High-dimensional asymptotics of prediction: ridge regression and classification. The Annals of Statistics, 2018 (cited on pages 2\u20134).   \n[6] Gabriel Mel and Surya Ganguli. A Theory of High Dimensional Regression with Arbitrary Correlations Between Input Features and Target Functions: Sample Complexity, Multiple Descent Curves and a Hierarchy of Phase Transitions. In Proceedings of the 38th International Conference on Machine Learning, 2021 (cited on page 2).   \n[7] Vidya Muthukumar, Kailas Vodrahalli, and Anant Sahai. Harmless Interpolation of Noisy Data in Regression. 2019 IEEE International Symposium on Information Theory (ISIT), 2019 (cited on page 2).   \n[8] Peter Bartlett, Philip M. Long, G\u00e1bor Lugosi, and Alexander Tsigler. Benign Overfitting in Linear Regression. Proceedings of the National Academy of Sciences, 2020 (cited on pages 2, 3).   \n[9] Mikhail Belkin, Daniel J. Hsu, and Ji Xu. Two Models of Double Descent for Weak Features. SIAM Journal on Mathematics of Data Science, 2020 (cited on page 2).   \n[10] Michal Derezinski, Feynman T Liang, and Michael W Mahoney. Exact Expressions for Double Descent and Implicit Regularization Via Surrogate Random Design. In Advances in Neural Information Processing Systems, 2020 (cited on pages 2, 3).   \n[11] Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J. Tibshirani. Surprises in HighDimensional Ridgeless Least Squares Interpolation. The Annals of Statistics, 2022 (cited on pages 2, 3, 10).   \n[12] Bruno Loureiro, Gabriele Sicuro, Cedric Gerbelot, Alessandro Pacco, Florent Krzakala, and Lenka Zdeborova. Learning Gaussian Mixtures with Generalized Linear Models: Precise Asymptotics in High-dimensions. In Advances in Neural Information Processing Systems, 2021 (cited on page 2).   \n[13] Chen Cheng and Andrea Montanari. Dimension Free Ridge Regression. arXiv preprint arXiv:2210.08571, 2022 (cited on page 2).   \n[14] Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Generalization Error of Random Feature and Kernel Methods: Hypercontractivity and Kernel Matrix Concentration. Applied and Computational Harmonic Analysis, 2022 (cited on page 2).   \n[15] Song Mei and Andrea Montanari. The Generalization Error of Random Features Regression: Precise Asymptotics and the Double Descent Curve. Communications on Pure and Applied Mathematics, 75, 2021 (cited on page 2).   \n[16] Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A Mean Field View of the Landscape of Two-layer Neural Networks. Proceedings of the National Academy of Sciences of the United States of America, 2018 (cited on page 2).   \n[17] Nilesh Tripuraneni, Ben Adlam, and Jeffrey Pennington. Covariate Shift in High-Dimensional Random Feature Regression. ArXiv, 2021 (cited on page 2).   \n[18] Federica Gerace, Bruno Loureiro, Florent Krzakala, Marc Mezard, and Lenka Zdeborova. Generalisation Error in Learning with Random Features and the Hidden Manifold Model. In Proceedings of the 37th International Conference on Machine Learning, 2020 (cited on page 2).   \n[19] Blake Woodworth, Suriya Gunasekar, Jason D. Lee, Edward Moroshko, Pedro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro. Kernel and Rich Regimes in Overparametrized Models. In Proceedings of Thirty Third Conference on Learning Theory, 2020 (cited on page 2).   \n[20] Bruno Loureiro, C\u2019edric Gerbelot, Hugo Cui, Sebastian Goldt, Florent Krzakala, Marc M\u2019ezard, and Lenka Zdeborov\u2019a. Learning Curves of Generic Features Maps for Realistic Datasets with a Teacher-Student Model. In NeurIPS, 2021 (cited on page 2).   \n[21] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Limitations of Lazy Training of Two-layers Neural Network. In Advances in Neural Information Processing Systems, 2019 (cited on page 2).   \n[22] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When Do Neural Networks Outperform Kernel Methods? In Advances in Neural Information Processing Systems, 2020 (cited on page 2).   \n[23] Preetum Nakkiran, Prayaag Venkat, Sham M. Kakade, and Tengyu Ma. Optimal Regularization can Mitigate Double Descent. In International Conference on Learning Representations, 2020 (cited on pages 2\u20134, 17).   \n[24] Rishi Sonthalia and Raj Rao Nadakuditi. Training data size induced double descent for denoising feedforward neural networks and the role of training noise. Transactions on Machine Learning Research, 2023 (cited on pages 3\u20135, 10, 17, 20, 21, 24, 26, 31).   \n[25] Dmitry Kobak, Jonathan Lomond, and Benoit Sanchez. The Optimal Ridge Penalty for RealWorld High-Dimensional Data Can Be Zero or Negative Due to the Implicit Ridge Regularization. Journal of Machine Learning Research, 2022 (cited on page 3).   \n[26] Fatih Furkan Yilmaz and Reinhard Heckel. Regularization-Wise Double Descent: Why it Occurs and How to Eliminate it. In IEEE International Symposium on Information Theory, 2022 (cited on page 3).   \n[27] Lin Chen, Yifei Min, Mikhail Belkin, and Amin Karbasi. Multiple Descent: Design Your Own Generalization Curve. Advances in Neural Information Processing Systems, 2021 (cited on page 3).   \n[28] Lechao Xiao and Jeffrey Pennington. Precise learning curves and higher-order scaling limits for dot product kernel regression. arXiv preprint arXiv:2205.14846, 2022 (cited on page 3).   \n[29] Ben Adlam and Jeffrey Pennington. The Neural Tangent Kernel in High Dimensions: Triple Descent and a Multi-Scale Theory of Generalization. In International Conference on Machine Learning, 2020 (cited on page 3).   \n[30] St\u00e9phane d\u2019Ascoli, Levent Sagun, and Giulio Biroli. Triple Descent and the Two Kinds of Overftiting: Where and Why Do They Appear? In Advances in Neural Information Processing Systems, 2020 (cited on pages 3, 8).   \n[31] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Linearized Two-layers Neural Networks in High Dimension. The Annals of Statistics, 2021 (cited on page 3).   \n[32] Alicia Curth, Alan Jeffares, and Mihaela van der Schaar. A u-turn on double descent: rethinking parameter counting in statistical learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL: https://openreview.net/forum?id= O0Lz8XZT2b (cited on page 3).   \n[33] Zeyu Deng, Abla Kammoun, and Christos Thrampoulidis. A model of double descent for high-dimensional binary linear classification. Information and Inference: A Journal of the IMA, 11(2):435\u2013495, 2022 (cited on page 3).   \n[34] Ganesh Ramachandra Kini and Christos Thrampoulidis. Analytic study of double descent in binary classification: the impact of loss. In 2020 IEEE International Symposium on Information Theory (ISIT), pages 2527\u20132532. IEEE, 2020 (cited on page 3).   \n[35] Pragya Sur and Emmanuel J Cand\u00e8s. A modern maximum-likelihood theory for highdimensional logistic regression. Proceedings of the National Academy of Sciences, 116(29):14516\u201314525, 2019 (cited on page 3).   \n[36] Francesca Mignacco, Florent Krzakala, Yue Lu, Pierfrancesco Urbani, and Lenka Zdeborova. The role of regularization in classification of high-dimensional noisy gaussian mixture. In International conference on machine learning, pages 6874\u20136883. PMLR, 2020 (cited on page 3).   \n[37] Chinmaya Kausik, Kashvi Srivastava, and Rishi Sonthalia. Generalization Error without Independence: Denoising, Linear Regression, and Transfer Learning, 2023 (cited on pages 3, 4, 10, 42).   \n[38] Oussama Dhifallah and Yue Lu. On the inherent regularization effects of noise injection during training. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 2665\u20132675. PMLR, 18\u201324 Jul 2021. URL: https://proceedings.mlr.press/ v139/dhifallah21a.html (cited on pages 3, 4, 14).   \n[39] Ji Xu and Daniel J Hsu. On the number of variables to use in principal component regression. Advances in neural information processing systems, 2019 (cited on pages 3, 9).   \n[40] Denny Wu and Ji Xu. On the Optimal Weighted \\ell_2 Regularization in Overparameterized Linear Regression. Advances in Neural Information Processing Systems, 2020 (cited on pages 3, 10).   \n[41] Ningyuan Huang, David W. Hogg, and Soledad Villar. Dimensionality reduction, regularization, and generalization in overparameterized regressions. SIAM Journal on Mathematics of Data Science, 2022 (cited on pages 3, 9).   \n[42] Wei Hu, Zhiyuan Li, and Dingli Yu. Simple and Effective Regularization Methods for Training on Noisily Labeled Data with Generalization Guarantee. In International Conference on Learning Representations, 2020 (cited on page 4).   \n[43] Yutong Wang, Rishi Sonthalia, and Wei Hu. Near-interpolators: rapid norm growth and the trade-off between interpolation and generalization. In Sanjoy Dasgupta, Stephan Mandt, and Yingzhen Li, editors, Proceedings of The 27th International Conference on Artificial Intelligence and Statistics, volume 238 of Proceedings of Machine Learning Research, pages 4483\u2013 4491. PMLR, February 2024. URL: https://proceedings.mlr.press/v238/ wang24k.html (cited on page 4).   \n[44] Vladimir Marcenko and Leonid Pastur. Distribution of Eigenvalues for Some Sets of Random Matrices. Mathematics of The Ussr-sbornik, 1967 (cited on pages 4, 28).   \n[45] Lucas Benigni and Sandrine P\u00e9ch\u00e9. Eigenvalue distribution of some nonlinear models of random matrices. Electronic Journal of Probability, 26:1\u201337, 2021 (cited on page 4).   \n[46] Olivier Ledoit and Sandrine P\u00e9ch\u00e9. Eigenvectors of some large sample covariance matrix ensembles. Probability Theory and Related Fields, 151(1):233\u2013264, 2011 (cited on page 4).   \n[47] Carl D. Meyer Jr. Generalized Inversion of Modified Matrices. SIAM Journal on Applied Mathematics, 1973 (cited on pages 6, 22, 24).   \n[48] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: an imperative style, highperformance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\u2019Alch\u00e9- Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8024\u20138035. Curran Associates, Inc., 2019. URL: http://papers.neurips.cc/ paper/9015-pytorch-an-imperative-style-high-performance-deeplearning-library.pdf.   \n[49] Chris M. Bishop. Training with Noise is Equivalent to Tikhonov Regularization. Neural Computation, 1995 (cited on page 17).   \n[50] Friedrich G\u00f6tze and Alexander Tikhomirov. The Rate of Convergence for Spectra of GUE and LUE Matrix Ensembles. Central European Journal of Mathematics, 2005 (cited on page 28).   \n[51] Friedrich G\u00f6tze and Alexander Tikhomirov. Rate of Convergence to the Semi-Circular Law. Probability Theory and Related Fields, 2003 (cited on page 28).   \n[52] Friedrich G\u00f6tze and Alexander Tikhomirov. Rate of Convergence in Probability to the Marchenko-Pastur Law. Bernoulli, 2004 (cited on page 28).   \n[53] Z. Bai, Baiqi. Miao, and Jian-Feng. Yao. Convergence Rates of Spectral Distributions of Large Sample Covariance Matrices. SIAM Journal on Matrix Analysis and Applications, 2003 (cited on page 28). ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "image", "img_path": "gzh9nTUtsY/tmp/5bc66f97c6387935b74b30e7232ca95dfaa83207194d02e11e6e3cca1aba3c55.jpg", "img_caption": ["Figure 8: Generalization Error for low-dimension MNIST using a linear denoiser. For the left figure, we use 10 dimensions and $\\mu=0.1$ . For the central figure, we use 5 dimensions and $\\mu=1$ . For the right figure, we use 784 dimensions and $\\mu=1$ . "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "A Higher Rank for Denoising Model ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "One might be led to believe that the restriction that the data lie on a line embedded in high dimensional space is crucial to the appearance of this phenomenon. However, this is not true. As long as the rank of the data is relatively low, for the input noise setting, we can see this phenomenon. Hence, we extend our results beyond the one-dimensional case to the low-dimensional case. Due to space constraints, the conjectured formula for the risk is in Appendix F.6 We verify the conjectured formula as well as the role of low dimensionality using MNIST data. Specifically, we project the data onto a $r$ -dimensional subspace. We then add noise to the low-dimensional representation and then solve the denoising problem. The left two figures in Figure 8 show that the phenomenon exists for low-dimensional data. That is, we see that a peak occurs in the under-parameterized regime, and the location of the peak seems to occur at $\\frac{\\mathfrak{f}}{\\mu^{2}\\!+\\!1}$ . However, running the same experiment without projecting to a low-dimensional space (right figure) results in very different phenomena. We no longer see double descent at all. Hence we see that if a peak occurs, then its location does not depend on the dimension. However, the occurrence of the peak does depend on the dimension. Thus, we see that this complements the results in [38]. ", "page_idx": 13}, {"type": "image", "img_path": "gzh9nTUtsY/tmp/d77a6a392cceb2e67492cd6ede47f7a309b7aafc0fa51513e1b2d062a51141f4.jpg", "img_caption": ["Figure 9: Generalization Risk and Training error for denoising a low-dimensional version of MNIST using a 1-hidden layer neural network. Here $d=2\\cdot784$ . "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "B Non-Linear Model ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To show that under-parameterized peak occurs for non-linear models. We conducted an experiment on MNIST with a 1-hidden layer neural network with a width of 784. The network has no bias, so has $2\\cdot784^{2}$ parameters. Let $\\mathcal{V}$ be a random 10-dimensional subspace. We project our data onto $\\mathcal{V}$ , add noise to the low dimensional representation, and train our network to remove this noise. We use full batch gradient descent with weight decay of 0.001 to train the model for 500 epochs with a learning rate of $2\\times10^{-2}$ . We test it on the complete MNIST test data. Figure 9 shows the training error and generalization error as a function of the number of training data points. As seen in the figure, we have multiple peaks in the under-parameterized regime, and the peaks in the risk correspond to local minimums in the training error. Interestingly, the risk curve here exhibits 4 peaks! ", "page_idx": 14}, {"type": "text", "text": "There are, however, many crucial differences between the peaks for this neural network case and the linear model case. First, the location of the peak does not seem to depend on the strength of the ridge regularization. Second, the peak seems to directly correspond to the local minimums of the training error. Hence, while we see peaks in the under-parameterized regime, the mechanisms that create these peaks are likely to be different. ", "page_idx": 14}, {"type": "image", "img_path": "gzh9nTUtsY/tmp/ff670740068d7e7c746d1c99c686ac9e8834bbd749f57fed59c399656fcbf86c.jpg", "img_caption": ["Figure 10: Figure showing the training error, the third derivative of the training error, and the location of the peak of the generalization error for different values of $\\mu$ [(L) $\\mu=1$ , (C) $\\mu=2$ ] for the data scaling regime. (R) shows the location of the local minimum of the third derivative and $\\textstyle{\\frac{1}{\\mu^{2}+1}}$ . "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "C Training Error ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "As seen in the prior section, the peak occurs in the interior of the under-parameterized regime and not on the border between the under-parameterized and over-parameterized regimes. We have also seen that it does not necessarily occur whenever there is a peak in the norm of the estimator. The final postulate for where the peak occurs is that it occurs when we first hit zero training error. ", "page_idx": 15}, {"type": "text", "text": "In this section, we explore the connection between the training error and the risk. Theorem 5 derives a formula for the training error in the under-parameterized regime. ", "page_idx": 15}, {"type": "text", "text": "Theorem 5 (Training Error). Let $\\tau$ be as in Theorem 1. The training error for $c<1$ is given by ", "page_idx": 15}, {"type": "text", "text": "$\\begin{array}{r}{\\mathbb{E}_{A_{t r n}}[\\|X_{t r n}-W_{o p t}(X_{t r n}+A_{t r n})\\|_{F}^{2}]=\\tau^{-2}\\left(\\sigma_{t r n}^{2}\\left(1-c\\cdot T_{1}\\right)+\\sigma_{t r n}^{4}T_{2}\\right)+o(1),}\\end{array}$   \nwher $;T_{1}={\\frac{\\mu^{2}}{2}}\\left({\\frac{1+c+\\mu^{2}c}{\\sqrt{(1-c+\\mu^{2}c)^{2}+4\\mu^{2}c^{2}}}}-1\\right)+{\\frac{1}{2}}+{\\frac{1+\\mu^{2}c-\\sqrt{(1-c+\\mu^{2}c)^{2}+4c^{2}\\mu^{2}}}{2c}},$   \nand $T_{2}=(\\mu^{2}c+c-1-\\sqrt{(1-c+\\mu^{2}c)^{2}+4c^{2}\\mu^{2}})^{2}\\left(\\frac{\\mu^{2}c+c+1}{2\\sqrt{(1-c+\\mu^{2}c)^{2}+4c^{2}\\mu^{2}}}+\\frac12\\right).$ ", "page_idx": 15}, {"type": "text", "text": "Since we are studying the ridge regularized problem, it is impossible for the training error to be exactly equal to 0. Hence, we may expect the peak to correspond to other features of the training error curve. Given the analytical formula for the training error, we can compute the derivatives. We found that the training error curve does not seem to signal the location of the peak in the generalization error curve. ", "page_idx": 15}, {"type": "text", "text": "Since we are studying the ridge regularized problem, it is impossible for the training error to be exactly equal to 0. Hence, we may expect the peak to correspond to other features of the training error curve. For example, the peak could correspond to a local minimum of the training error, or it could correspond to a point where the training error or its derivatives suddenly change. The first, a local minimum, is easy to see from the plot of the training error, but the second can be more subtle as we do not usually have access to the derivatives. However, since we have an analytical formula for the training error, we can compute the derivatives. ", "page_idx": 15}, {"type": "text", "text": "Figure 10 plots the location of the peak and the training error. Here, the figure shows that the training error curve does not seem to signal the location of the peak in the generalization error curve. However, it shows that for the data scaling regime, the peak roughly corresponds to a local minimum of the third derivative of the training error. While the minimum of the third derivative is difficult to interpret, as we can see from the third plot in Figure 10, the minimum seems to closely track the location of the peak. ", "page_idx": 15}, {"type": "text", "text": "D Regularization Trade-off ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "It has been seen in prior work that the amount of noise added to the input data can be viewed as a regularizer [24, 49]. In our setup, we have two different regularizers: the amount of noise added to the data (since we are dealing with linear models, this is equivalent to the strength of the signal $\\sigma_{t r n}$ ) and the strength of the ridge regularizer $\\mu$ . It is interesting to analyze the trade-off between the two regularizers and the generalization error. ", "page_idx": 16}, {"type": "image", "img_path": "gzh9nTUtsY/tmp/1e7ce0466b55de66fce3da68d09bfecce99a2dbd0f18846e966b435e94a19869.jpg", "img_caption": ["Figure 11: The first two figures show the $\\sigma_{t r n}$ versus risk curve for $c=0.5$ , $\\mu=1$ and $c=2$ , $\\mu=0.1$ with $d=1000$ . The second two figures show the risk when training using the optimal $\\sigma_{t r n}$ for the data scaling and parameter scaling regimes. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Optimal $\\sigma_{t r n}$ First, we fix $\\mu$ and determine the optimal $\\sigma_{t r n}$ . Figure 11 displays the generalization error versus $\\sigma_{t r n}^{2}$ curve. The figure shows that the error is initially large but then decreases until the optimal generalization error. The generalization error when using the optimal $\\sigma_{t r n}$ is also shown in Figure 11. Here, unlike [23], picking the optimal value of $\\sigma_{t r n}$ does not mitigate double descent. ", "page_idx": 16}, {"type": "text", "text": "Proposition 1 (Optimal $\\sigma_{t r n}$ ). The optimal value of $\\sigma_{t r n}^{2}$ for $c<1$ is given by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sigma_{t r n}^{2}=\\frac{\\sigma_{t s t}^{2}d[2c(\\mu^{2}+1)^{2}-2T(c\\mu^{2}+c+1)+2(c\\mu^{2}-2c+1)]+N_{t s t}(\\mu^{2}c^{2}+c^{2}+1-T)}{N_{t s t}(c^{3}(\\mu^{2}+1)^{2}-T(\\mu^{2}c^{2}+c^{2}-1)-2c^{2}-1)}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Additionally, it is interesting to determine how the optimal value of $\\sigma_{t r n}$ depends on both $\\mu$ and $c$ Figure 12 shows that for small values of $\\mu\\in(0.1,0.5)$ , as $c$ changes, there exists an (inverted) double descent curve for the optimal value of $\\sigma_{t r n}$ . However, for the data scaling regime, the minimum of this double descent curve does not match the location for the peak of the generalization error. Further, as the amount of ridge regularization increases, the optimal amount of noise regularization decreases proportionally; optimal $\\overline{{\\sigma_{t r n}^{2}\\approx d\\mu^{2}}}$ . Thus, for higher values of ridge regularization, it is preferable to have higher-quality data. ", "page_idx": 16}, {"type": "text", "text": "Optimal Value of $\\mu$ We now explore the effect of fixing $\\sigma_{t r n}$ and then changing $\\mu$ . Figure 13, shows a $U$ shaped curve for the generalization error versus $\\mu$ , suggesting that there is an optimal value of $\\mu$ , which should be used to minimize the generalization error. Next, we compute the optimal value of $\\mu$ using grid search and plot it against $c$ . Figure 14 shows double descent for the optimal value of $\\mu$ for small values of $\\sigma_{t r n}$ . Thus, for low SNR data, we see a double descent, but we do not for high SNR data. ", "page_idx": 16}, {"type": "text", "text": "Finally, for a given value of $\\mu$ and $c$ , we compute the optimal $\\sigma_{t r n}$ . We then compute the generalization error (when using the optimal $\\sigma_{t r n.}$ ) and plot the generalization error versus $\\mu$ curve. Figure 15 displays a very different trend from Figure 13. Instead of having a $U$ -shaped curve, we have a monotonically decreasing generalization error curve. This suggests that we can improve generalization by using higher-quality training while compensating for this by increasing the amount of ridge regularization. ", "page_idx": 16}, {"type": "image", "img_path": "gzh9nTUtsY/tmp/3900f213a939d8d1bcb674d5b541d1e7b468d0564e364a942c1d9651078de5f5.jpg", "img_caption": ["Figure 12: The first figure plots the optimal $\\sigma_{t r n}^{2}/n$ versus $\\mu$ curve. The middle figure plots the optimal $\\sigma_{t r n}^{2}/n$ versus $c$ in the data scaling regime for $\\mu=0.5$ , and the last figure plots the optimal $\\sigma_{t r n}^{\\bar{2}}/n$ versus $c$ in the parameter scaling regime for $\\mu=0.1$ . "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "gzh9nTUtsY/tmp/4d0b40521b6197c418f2352feb8406d43f406977b34a578a382ff28df18d7fb2.jpg", "img_caption": ["Figure 13: Figure showing the generalization error versus $\\mu$ for $\\sigma_{t r n}^{2}=n$ and $\\sigma_{t s t}^{2}=N_{t s t}$ "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "gzh9nTUtsY/tmp/b3d1bed367654fb29ba4f1c5f76d01223c44d90898d50e8835f570b7f6bda594.jpg", "img_caption": ["Figure 14: Figure for the optimal value of $\\mu$ verses for different values of $\\sigma_{t r n}$ "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "gzh9nTUtsY/tmp/f0edb0076fa4d07cef07db84b512b5d63bbfd61186675bf01f2221ad35638527.jpg", "img_caption": ["Figure 15: Figure showing the generalization error versus $\\mu$ for the optimal $\\sigma_{t r n}^{2}$ and $\\sigma_{t s t}^{2}=N_{t s t}$ . "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Interaction Between the Regularizers T\u221ahe optimal values of $\\mu$ and $\\sigma_{t r n}$ are jointly computed using grid search for $\\mu\\in(0,100]$ and $\\sigma_{t r n}/\\sqrt{n}\\,\\bar{\\in}\\,(0,10]$ . Figure 16 shows the results. Specifically, $\\sigma_{t r n}$ is at the highest possible value (so best quality data), and then the model regularizes purely using the ridge regularizer. This results in a monotonically decreasing generalization error curve. Thus, in the data scaling model, there is an implicit bias that favors one regularizer over the other. Specifically, the model\u2019s implicit bias is to use higher quality data while using ridge regularization to regularize the model appropriately. It is surprising that the two regularizers are not balanced. ", "page_idx": 17}, {"type": "image", "img_path": "gzh9nTUtsY/tmp/b49ca085e4e4ddfabc67f4e2ad04b813ab42163ca376d06ecfa0461efb48ffae.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 16: Trade-off between the regularizers. The left column is the optimal $\\sigma_{t r n}$ , the central column is the optimal $\\mu$ , and the right column is the generalization error for these parameter restrictions. ", "page_idx": 18}, {"type": "image", "img_path": "gzh9nTUtsY/tmp/3e3f454f57fcd6256ac4298dcd75e72e0993720da3813539b7c5a7c71f537c9a.jpg", "img_caption": ["Figure 17: Trade-off between the regularizers. The left column is the optimal $\\sigma_{t r n}$ , the central column is the optimal $\\mu$ , and the right column is the generalization error for these parameter restrictions "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Next we look at the trade-off between $\\sigma_{t r n}$ and $\\mu$ for the parameter scaling regime. We again see, Figure 17, that the model implicitly prefers regularizing via ridge regularization and not via input data noise regularizer. ", "page_idx": 18}, {"type": "text", "text": "E Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "E.1 Linear Regression ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We begin by noting, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\beta^{T}=(\\beta_{o p t}^{T}X+\\xi_{t r n})X_{t r n}^{\\dag}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, we have, ", "page_idx": 19}, {"type": "text", "text": "\u2225\u03b2\u22252 = Tr(\u03b2T \u03b2) $=\\mathrm{Tr}(\\beta_{o p t}^{T}X_{t r n}X_{t r n}^{\\dag}(X_{t r n}^{\\dag})^{T}X_{t r n}\\beta_{o p t})+\\mathrm{Tr}(\\xi_{t r n}X_{t r n}^{\\dag}(X_{t r n}^{\\dag})^{T}\\xi_{t r n}^{T})+2\\,\\mathrm{Tr}(\\beta_{o p t}^{T}X_{t r n}X_{t r n}^{\\dag}X_{t r n}^{\\dag})$ n)T \u03betTrn ", "page_idx": 19}, {"type": "text", "text": "Taking the expectation, with respect to $\\xi_{t r n}$ , we see that the last term vanishes. ", "page_idx": 19}, {"type": "text", "text": "Letting $X_{t r n}=U_{X}\\Sigma_{X}V_{X}^{T}$ . We see that using the rotational invariance of $X,U_{X},V_{X}$ are independent and uniformly random. Thus, $s:=\\beta_{o p t}^{T}U_{X}$ is a uniformly random unit vector. ", "page_idx": 19}, {"type": "text", "text": "Thus, we see, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X_{t r n},\\xi_{t r n}}\\left[{\\mathrm{Tr}}(\\beta_{o p t}^{T}X_{t r n}X_{t r n}^{\\dagger}(X_{t r n}^{\\dagger})^{T}X_{t r n}\\beta_{o p t})\\right]=\\sum_{i=1}^{\\operatorname*{min}(d,n)}\\mathbb{E}[s_{i}^{2}]=\\operatorname*{min}\\left(1,{\\frac{1}{c}}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Similarly, we see, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X_{t r n},\\xi_{t r n}}\\left[\\xi_{t r n}X_{t r n}^{\\dagger}(X_{t r n}^{\\dagger})^{T}\\xi_{t r n}^{T}\\right]=\\sum_{i=1}^{\\operatorname*{min}(d,n)}\\mathbb{E}\\left[\\frac{1}{\\sigma_{i}(X_{t r n})^{2}}\\right]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Multiplying and dividing by $d$ , normalizes the singular values squared of $X_{t r n}$ so that the limiting distribution is the Marchenko Pastur distribution with shape $c$ . Thus, we can estimate using Lemma 5 from Sonthalia and Nadakuditi [24] to get, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\frac{c}{1-c}+o(1)}&{c<1}\\\\ {\\frac{1}{c-1}+o(1)}&{c>1}\\end{array}.\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Finally, the cross-term has an expectation equal to zero. Thus, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X_{t r n},\\xi_{t r n}}[\\|\\beta_{o p t}\\|^{2}]=\\left\\{\\frac{1+\\frac{c}{1-c}}{\\frac{1}{c}+\\frac{1}{c-1}}\\quad c>1\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then we have, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\beta^{T}\\beta_{o p t}=\\beta_{o p t}^{T}X_{t r n}X_{t r n}^{\\dagger}\\beta_{o p t}+\\xi_{t r n}X_{t r n}^{\\dagger}\\beta_{o p t}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The second term has an expectation equal to zero, and the first term is similar to before and has an expectation equal to $\\operatorname*{min}\\left(1,{\\frac{1}{c}}\\right)$ ", "page_idx": 19}, {"type": "text", "text": "E.2 Proof for Output Noise Model ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Theorem 4 (Under-parametrized Peak). For the above model, $i f k/n\\to\\pi_{2}$ , and $d/n\\to c,$ , then the expected risk is given by $\\begin{array}{r}{\\mathcal{R}=\\left\\{\\!\\!\\!\\begin{array}{l l}{\\frac{\\pi_{1}c}{\\pi_{1}-c}}&{c<\\pi_{1}}\\\\ {\\pi_{1}\\left(\\frac{\\pi_{1}}{c-\\pi_{1}}+\\left(1-\\frac{\\pi_{1}}{c}\\right)\\left(\\frac{\\|\\beta\\|^{2}}{d}-\\frac{(\\beta^{T}z)^{2}}{\\|z\\|^{2}d}\\right)\\right)}&{c>\\pi_{1}}\\end{array}\\!\\!.\\right.}\\end{array}$ ", "page_idx": 19}, {"type": "text", "text": "Proof. We begin with $c<\\pi_{1}$ . Here, since $A A^{T}$ is invertible, we can see that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\hat{\\beta}^{T}=\\left(\\beta^{T}\\left[A\\right.\\right.\\left.z v^{T}\\right]+\\xi^{T}\\right)\\left[_{v z^{T}}^{A^{T}}\\right]\\left(A A^{T}+\\|v\\|^{2}z z^{T}\\right)^{-1}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let us focus on the term without the $\\xi$ . Using the Sherman Morrison formula and the orthogonality of $\\beta$ and $z$ , we see that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left(\\beta^{T}\\left[A}&{{}z v^{T}\\right]\\right)\\left[\\overset{A^{T}}{v z^{T}}\\right]\\left(A A^{T}+\\|v\\|^{2}z z^{T}\\right)^{-1}=\\beta^{T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, we get that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\hat{\\beta}^{T}=\\beta^{T}+\\xi^{T}\\left[\\!\\!\\begin{array}{l}{A^{T}}\\\\ {v z^{T}}\\end{array}\\!\\!\\right]\\left(A A^{T}+\\|v\\|^{2}z z^{T}\\right)^{-1}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For this model, the uncentered covariance matrix is given by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}[x x^{T}]=\\pi_{1}\\mathbb{E}_{x\\sim\\mathcal{N}(0,\\frac{1}{d}I)}[x x^{T}]+\\pi_{2}\\mathbb{E}[\\alpha^{2}z z^{T}]=\\frac{\\pi_{1}}{d}I+\\pi_{2}z z^{T}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then, the expected excess risk for a solution $\\hat{\\beta}$ compared to $\\beta$ is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{R}=\\mathbb{E}[\\|\\beta^{T}x-\\hat{\\beta}^{T}x\\|^{2}|X]=\\mathbb{E}\\left[\\frac{\\pi_{1}}{d}\\|\\beta^{T}-\\hat{\\beta}^{T}\\|^{2}+\\pi_{2}\\|(\\beta-\\hat{\\beta})^{T}z\\|^{2}|X\\right].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The first term is given by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\|\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}}^{T}\\|^{2}|\\boldsymbol{X}]=\\mathbb{E}\\left[\\left\\|\\xi^{T}\\left[\\overset{A^{T}}{v}\\right]\\left(A A^{T}+\\|v\\|^{2}z z^{T}\\right)^{-1}\\right\\|^{2}|\\boldsymbol{X}\\right]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Taking the expectation over $\\xi$ , we get that the expected excess risk is given by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{Tr}\\left(\\left[\\!\\!\\begin{array}{l}{A^{T}}\\\\ {v z^{T}}\\end{array}\\!\\!\\right]\\left(A A^{T}+\\|v\\|^{2}z z^{T}\\right)^{-2}\\left[\\!\\!\\begin{array}{l l}{A}&{z v^{T}\\right]\\right)=\\mathrm{Tr}\\left(\\left(A A^{T}+\\|v\\|^{2}z z^{T}\\right)^{-1}\\right)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Again, using the Sherman-Morrison formula, we get that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{Tr}((A A^{T}+\\|v\\|z z^{T})^{-1})=\\mathrm{Tr}((A A^{T})^{-1}-\\frac{\\|v\\|^{2}}{1+\\|v\\|^{2}z^{T}(A A^{T})^{-1}z}\\,\\mathrm{Tr}\\left(z^{T}(A A^{T})^{-2}z\\right)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Suppose $\\nu$ is the limiting distribution of the empirical spectral distribution for the non-zero eigenvalues. Then using the concentration results from [24], we see that the error can be expressed ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{Tr}\\big((A A^{T})^{-1}-\\frac{\\|v\\|^{2}}{1+\\|v\\|^{2}z^{T}(A A^{T})^{-1}z}\\,\\mathrm{Tr}\\,\\big(z^{T}(A A^{T})^{-2}z\\big)=d\\cdot\\mathbb{E}_{\\lambda\\sim\\nu}\\,\\bigg[\\frac{1}{\\lambda}\\bigg]-\\frac{\\|v\\|^{2}}{1+\\|v\\|^{2}\\mathbb{E}_{\\lambda\\sim\\nu}\\,\\left[\\frac{1}{\\lambda}\\right]}\\mathbb{E}_{\\lambda\\sim\\nu}\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The second term of the expected risk is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\|\\xi^{T}\\left[\\overset{A^{T}}{v}\\right]\\left(A A^{T}+\\|v\\|z z^{T}\\right)^{-1}z\\right\\|^{2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Again, if we take the expectation over $\\xi$ and write as a trace, we get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{Tr}(z^{T}\\left((A A^{T})^{-1}-\\frac{(A A^{T})^{-1}\\|v\\|^{2}z z^{T}(A A^{T})^{-1}}{1+\\|v\\|^{2}z^{T}(A A^{T})^{-1}z}\\right)z)=\\frac{z^{T}(A A^{T})^{-1}z}{1+\\|v\\|^{2}z^{T}(A A^{T})^{-1}z}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Again, this can expressed as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\mathbb{E}_{\\lambda\\sim\\nu}\\left[\\frac{1}{\\lambda}\\right]}{1+\\|v\\|^{2}\\mathbb{E}_{\\lambda\\sim\\nu}\\left[\\frac{1}{\\lambda}\\right]}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Putting it all together, the expected excess risk is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\pi_{1}\\mathbb{E}_{\\lambda\\sim\\nu}\\left[{\\frac{1}{\\lambda}}\\right]+\\pi_{2}{\\frac{\\mathbb{E}_{\\lambda\\sim\\nu}\\left[{\\frac{1}{\\lambda}}\\right]}{1+\\|v\\|^{2}\\mathbb{E}_{\\lambda\\sim\\nu}\\left[{\\frac{1}{\\lambda}}\\right]}}-{\\frac{\\pi_{1}}{d}}{\\frac{\\|v\\|^{2}}{1+\\|v\\|^{2}\\mathbb{E}_{\\lambda\\sim\\nu}\\left[{\\frac{1}{\\lambda}}\\right]}}\\mathbb{E}_{\\lambda\\sim\\nu}\\left[{\\frac{1}{\\lambda^{2}}}\\right]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "If we then send $n,k,d$ to infinity, while noting that $\\|v\\|\\rightarrow\\infty$ , then we get then limiting risk ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\pi_{1}\\mathbb{E}_{\\lambda\\sim\\nu}\\left[{\\frac{1}{\\lambda}}\\right]={\\frac{\\pi_{1}c}{\\pi_{1}-c}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For the $d>n-k$ case, $A A^{T}$ is no longer invertible. Hence, we need to replace the inverse with pseudoinverse. Hence, we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\hat{\\beta}^{T}=\\left(\\beta^{T}\\left[A\\quad z v^{T}\\right]+\\xi^{T}\\right)\\left[\\boldsymbol{\\omega}_{z}^{T}\\right]\\left(A A^{T}+\\|v\\|^{2}z z^{T}\\right)^{\\dagger}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We now expand the pseudoinverse using Theorem 1 from [47]. To write is succinctly, let $M\\,=$ $A A^{T},P=\\dot{(}I-M\\dot{M^{\\dag}})$ , $\\gamma=z^{T}P z$ , and $\\boldsymbol{\\tau}=z^{T}M^{\\dagger}z$ to get that ", "page_idx": 21}, {"type": "equation", "text": "$$\n(A A^{T}+\\|v\\|^{2}z z^{T})^{\\dagger}=(A A^{T})^{\\dagger}-{\\frac{1}{\\gamma}}\\left((A A^{T})^{\\dagger}z z^{T}P+P z z^{T}(A A^{T})^{\\dagger}\\right)+{\\frac{(1+\\|v\\|^{2})\\tau}{\\|v\\|^{2}\\gamma^{2}}}P z z^{T}P\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that $P$ is the projection onto the orthogonal complement of the range of $A$ . Hence we see that $M P=M^{\\dagger}P=0$ . Thus, multiplying through, and setting terms to zero, we see that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(M+\\|v\\|^{2}z z^{T})(M+\\|v\\|^{2}z z^{T})^{\\dagger}=M M^{\\dagger}-\\frac{1}{\\gamma}M M^{\\dagger}z z^{T}P+\\|v\\|^{2}z z^{T}M^{\\dagger}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad-\\,\\frac{1}{\\gamma}\\|v\\|^{2}z z^{T}M^{\\dagger}z z^{T}P-\\frac{1}{\\gamma}\\|v\\|^{2}z z^{T}P z z^{T}M^{\\dagger}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\frac{(1+\\|v\\|^{2}\\tau)}{\\gamma^{2}}z z^{T}P z z^{T}P}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Using the fact that $z^{T}P z=\\gamma,z^{T}M^{\\dagger}z=\\tau$ and cancelling, we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\big(M+\\|v\\|^{2}z z^{T}\\big)(M+\\|v\\|^{2}z z^{T})=M M^{t}-\\frac{1}{\\gamma}M M^{t}z z^{T}P}\\\\ {-\\frac{1}{\\gamma}\\|v\\|^{2}z z^{T}M^{t}z_{2}^{T}P+\\frac{(1+\\|v\\|^{2}\\tau)}{\\gamma}z_{2}^{T}P}\\\\ {=M M^{t}-\\frac{1}{\\gamma}M M^{t}z z^{T}P}\\\\ {-\\frac{1}{\\gamma}\\|v\\|^{2}z\\tau^{2}P+\\frac{(1+\\|v\\|^{2}\\tau)}{\\gamma}z_{2}^{T}P}\\\\ {=M M^{t}-\\frac{1}{\\gamma}M M^{t}z z^{T}P+\\frac{1}{\\gamma}z_{2}^{T}P}\\\\ {=M M^{t}+\\Big[(I-M M)^{\\frac{1}{2}}z z^{T}P\\Big]}\\\\ {=M M^{t}+\\Big[\\frac{1}{\\gamma}p z_{2}^{T}P}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, the first two terms in the error are ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\pi_{1}}{d}\\|\\boldsymbol{\\beta}-\\boldsymbol{\\hat{\\beta}}\\|^{2}+\\pi_{2}\\|\\boldsymbol{\\beta}^{T}\\boldsymbol{z}-\\boldsymbol{\\hat{\\beta}}^{T}\\boldsymbol{z}\\|^{2}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Let us look at the second term first. Here we see that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\hat{\\beta}^{T}z=\\beta^{T}\\left(M M^{\\dagger}+\\frac{1}{\\gamma}P z z^{T}P\\right)z=\\beta^{T}(M M^{\\dagger}z+P z)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, we see that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta^{T}z-\\hat{\\beta}^{T}z=\\beta^{T}z-\\beta^{T}(M M^{\\dagger}z+P z)}\\\\ &{\\qquad\\qquad\\qquad=\\beta^{T}(I-M M^{\\dagger})z-\\beta^{T}(I-M M^{\\dagger})z}\\\\ &{=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For the first term, we first note that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\beta^{T}-\\hat{\\beta}^{T}=\\beta^{T}-\\beta^{T}\\left(M M^{\\dagger}+\\frac{1}{\\gamma}P z z^{T}P\\right)}\\\\ {=\\beta^{T}P-\\frac{1}{\\gamma}\\beta^{T}P z z^{T}P\\,\\,\\,\\,\\,\\,\\,\\,\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "To compute the norm, we expand and get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\|\\beta^{T}P\\right\\|^{2}+\\frac{1}{\\gamma^{2}}\\operatorname{Tr}\\left(\\beta^{T}P z z^{T}P P z z^{T}P\\beta\\right)-\\frac{2}{\\gamma}\\operatorname{Tr}\\left(\\beta^{T}P z z^{T}P P\\beta\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Noting that $P$ is a projection matrix, so $P P\\,=\\,P$ and using $z^{T}P Z\\,=\\,\\gamma$ , we get that this term simplifies to ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left\\|\\beta^{T}P\\right\\|^{2}-\\frac{1}{\\gamma}\\operatorname{Tr}\\left(\\beta^{T}P z z^{T}P\\beta\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that the subspace for $P$ comes from a Gaussian random matrix. Hence is uniformly random. Hence ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|\\beta^{T}P\\right\\|^{2}\\right]=\\left(1-\\frac{n-k}{d}\\right)\\|\\beta\\|^{2}=\\left(1-\\frac{\\pi_{1}}{c}\\right)\\|\\beta\\|^{2}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Similarly, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[-\\frac{1}{\\gamma}\\operatorname{Tr}\\left(\\beta^{T}P z z^{T}P\\beta\\right)\\right]=\\left(1-\\frac{\\pi_{1}}{c}\\right)\\frac{(\\beta^{T}z)^{2}}{\\|z\\|^{2}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For the next two terms, we need to first simplify ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left[\\!\\!\\begin{array}{l}{A^{T}}\\\\ {v z^{T}}\\end{array}\\!\\!\\right](A A^{T}+\\|v\\|^{2}z z^{T})^{\\dagger}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Substituting in our formulas and noticing that ", "page_idx": 22}, {"type": "equation", "text": "$$\nA^{T}P=0\\;{\\mathrm{and~}}A^{T}M^{\\dagger}=A^{\\dagger}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "we get that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left[\\!\\!\\begin{array}{c}{A^{T}}\\\\ {v z^{T}}\\end{array}\\!\\!\\right](A A^{T}+\\|v\\|^{2}z z^{T})^{\\dagger}=\\left[\\!\\!\\begin{array}{c}{A^{\\dagger}-\\frac{1}{\\gamma}A^{\\dagger}z z^{T}P-0+0}\\\\ {v z^{T}M^{\\dagger}-\\frac{1}{\\gamma}\\tau v z^{T}P-v z^{T}M^{\\dagger}+\\frac{1+\\|v\\|^{2}\\tau}{\\|v\\|^{2}\\gamma}v z^{T}P\\!\\!\\right]}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~A^{\\dagger}-\\frac{1}{\\gamma}A^{\\dagger}z z^{T}P}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}\\\\ {-\\frac{1}{\\gamma}\\tau v z^{T}P+\\frac{1}{\\gamma}\\tau v z^{T}P+\\frac{1}{\\|v\\|^{2}\\gamma}v z^{T}P\\!\\!\\!\\right]}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~=\\left[\\!\\!\\begin{array}{c}{A^{\\dagger}-\\frac{1}{\\gamma}A^{\\dagger}z z^{T}P}\\\\ {\\frac{1}{\\|v\\|^{2}\\gamma}v z^{T}P}\\end{array}\\!\\!\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\xi\\left[\\left\\|\\xi^{T}\\left[v_{z^{T}}^{A T}\\right](A A^{T}+\\|v\\|^{2}z z^{T})^{\\dagger}\\right\\|^{2}\\right]=\\left\\|\\left[A^{T}\\right](A A^{T}+\\|v\\|^{2}z z^{T})^{\\dagger}\\right\\|^{2}}&{}\\\\ {=\\mathrm{Tr}\\left(\\left[A^{\\dagger}-\\frac{1}{\\gamma}A^{\\dagger}z z^{T}P\\right]^{T}\\left[A^{\\dagger}-\\frac{1}{\\gamma}A^{\\dagger}z z^{T}P\\right]\\right)}&{}\\\\ {=\\mathrm{Tr}\\left(M^{\\dagger}-\\frac{1}{\\gamma}m^{\\dagger}z z^{T}P-\\displaystyle\\frac{1}{\\gamma}P z z^{t}M^{\\dagger}+\\frac{1}{\\|v\\|^{2}\\gamma^{2}}P z z^{T}P\\right.}&{}\\\\ {\\left.=\\mathrm{Tr}\\left(M^{\\dagger}-\\mathrm{Tr}\\left(\\frac{1}{\\gamma}M^{\\dagger}z z^{T}P\\right)-\\mathrm{Tr}\\left(\\frac{1}{\\gamma}P z z^{T}M^{\\dagger}\\right)+\\frac{1}{\\|v\\|^{2}\\gamma^{2}}P z z^{T}P\\right]\\right.}&{}\\\\ {\\left.=\\mathrm{Tr}\\left(M^{\\dagger}\\right)-\\mathrm{Tr}\\left(\\frac{1}{\\gamma}M^{\\dagger}z z^{T}P\\right)-\\mathrm{Tr}\\left(\\frac{1}{\\gamma}P z z^{T}M^{\\dagger}\\right)+\\mathrm{Tr}\\left(\\frac{1}{\\gamma}P z z^{T}M^{\\dagger}\\right)\\right.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Using the cycling invariance of the trace and the fact that $M^{\\dag}P=0$ , we get that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert\\xi^{T}\\left[\\overset{A^{T}}{\\underset{v z^{T}}{\\prod}}\\right](A A^{T}+\\Vert v\\Vert^{2}z z^{T})^{\\dagger}\\right\\Vert^{2}\\right]=\\operatorname{Tr}\\left(M^{\\dagger}\\right)+\\frac{1+\\Vert v\\Vert^{2}\\tau}{\\Vert v\\Vert^{2}\\gamma}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The last equality is obtained using cyclic invariance, the fact that $P^{2}=P$ , and $\\gamma=z^{T}P z$ . Using Lemma 6 for $p=n-k$ and $q=d$ , we get that asymptotically, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{\\pi_{1}}{d}\\operatorname{Tr}(M^{\\dag})=\\frac{\\pi_{1}^{2}}{c-\\pi_{1}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that we similarly get that asymptotically, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{\\pi_{1}}{d}\\tau=\\frac{1}{d}\\frac{\\pi_{1}^{2}}{c-\\pi_{1}}\\rightarrow0.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus, the contribution to the risk from this term is $\\frac{\\pi_{1}^{2}}{c\\!-\\!\\pi_{1}}$ ", "page_idx": 23}, {"type": "text", "text": "Finally, for the last term, we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left[\\overset{A^{T}}{\\underset{v z^{T}}{A^{T}}}\\right](A A^{T}+\\|v\\|^{2}z z^{T})^{\\dagger}z=\\left[\\overset{\\cdot}{\\underset{\\Vert v\\Vert^{2}\\gamma}{\\underset{\\Vert v\\Vert^{2}\\gamma}{\\sum}}}v z^{T}P z\\right]}\\\\ &{=\\left[\\overset{0}{\\underset{\\Vert v\\Vert^{2}}{\\prod}}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Thus, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|\\xi^{T}\\left[\\overset{A^{T}}{\\underset{v z^{T}}{\\sim}}\\right](A A^{T}+\\|v\\|^{2}z z^{T})^{\\dagger}z\\right\\|^{2}\\right]=\\frac{1}{\\|v\\|^{2}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that $\\|\\boldsymbol{v}\\|^{2}\\approx k$ and hence this term is aympotitcally zero. Putting it all together, we get the needed result. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "E.3 Proofs for Theorem 1 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The proof structure closely follows that of [24]. ", "page_idx": 23}, {"type": "text", "text": "E.3.1 Step 1: Decompose the error into bias and variance terms. ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "First, we decompose the error. Since we are not in the supervised learning setup, we do not have standard definitions of bias/variance. However, we will call the following terms the bias/variance of the model. First, we recall the following from [24]. ", "page_idx": 23}, {"type": "text", "text": "Lemma 1 (Sonthalia and Nadakuditi [24]). If $A_{t s t}$ has mean $O$ entries and $A_{t s t}$ is independent of $X_{t s t}$ and $W$ , then ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{A_{t s t}}[\\|X_{t s t}-W Y_{t s t}\\|_{F}^{2}]=\\underbrace{\\mathbb{E}_{A_{t s t}}[\\|X_{t s t}-W X_{t s t}\\|_{F}^{2}]}_{B i a s}+\\underbrace{\\mathbb{E}_{A_{t s t}}[\\|W A_{t s t}\\|_{F}^{2}]}_{V a r i a n c e}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "E.3.2 Step 2: Formula for $W_{o p t}$ ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Here, we compute the explicit formula for $W_{o p t}$ in Problem 1. Let $\\begin{array}{r}{\\hat{A}_{t r n}=[A_{t r n}\\quad\\mu I]}\\end{array}$ , $\\hat{X}_{t r n}=$ $\\left[X_{t r n}\\quad0\\right]$ , and $\\hat{Y}_{t r n}=\\hat{X}_{t r n}+\\hat{A}_{t r n}$ . Then solving $\\begin{array}{r}{\\arg\\operatorname*{min}_{W}\\|X_{t r n}-W Y_{t r n}\\|_{F}^{2}+\\mu^{2}\\|W\\|_{F}^{2}}\\end{array}$ is equivalent to solving $\\mathrm{arg\\,min}_{W}\\parallel\\hat{X}_{t r n}-W\\hat{Y}_{t r n}\\parallel_{F}^{2}$ . Thus, $W_{o p t}=\\arg\\operatorname*{min}_{W}\\|\\hat{X}_{t r n}-W\\hat{Y}_{t r n}\\|_{F}^{2}=$ $\\hat{X}_{t r n}\\hat{Y}_{t r n}^{\\dag}$ . Expanding this out, we get the following formula for $\\hat{W}$ . Let $\\hat{u}$ be the left singular vector and $\\hat{v}_{t r n}$ be the right singular vectors of $\\hat{X}_{t r n}$ . Note that the left singular does not change after ridge regularization, so $\\hat{u}~=~u$ . Let $\\hat{h}\\,=\\,\\hat{v}_{t r n}^{T}\\hat{A}_{t r n}^{\\dagger},\\,\\hat{k}\\,=\\,\\hat{A}_{t r n}^{\\dagger}u,\\,\\hat{s}\\,=\\,(I\\,-\\,\\hat{A}_{t r n}\\hat{A}_{t r n}^{\\dagger})u$ , $\\begin{array}{r}{\\hat{t}=\\hat{v}_{t r n}(I-\\hat{A}_{t r n}^{\\dagger}\\hat{A}_{t r n}),\\hat{\\gamma}=1+\\sigma_{t r n}\\hat{v}_{t r n}^{T}\\hat{A}_{t r n}^{\\dagger}u,\\hat{\\tau}=}\\end{array}$ $\\hat{\\tau}=\\sigma_{t r n}^{2}\\|\\hat{t}\\|^{2}\\|\\hat{k}\\|^{2}+\\hat{\\gamma}^{2}$ . ", "page_idx": 23}, {"type": "text", "text": "Proposition 2. If $\\hat{\\gamma}\\neq0$ and $A_{t r n}$ has full rank then ", "page_idx": 23}, {"type": "equation", "text": "$$\nW_{o p t}=\\frac{\\sigma_{t r n}\\hat{\\gamma}}{\\hat{\\tau}}u\\hat{h}+\\frac{\\sigma_{t r n}^{2}||\\hat{t}||^{2}}{\\hat{\\tau}}u\\hat{k}^{T}\\hat{A}_{t r n}^{\\dagger}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Here we know that $u$ is arbitrary. We have that $\\hat{A}_{t r n}$ has full rank. Thus, the rank of $\\hat{A}_{t r n}$ is $d$ , and the range of $\\hat{A}_{t r n}$ is the whole space. Thus, $u$ lives in the range of $\\hat{A}_{t r n}$ . In this case, we want Theorem 3 from [47]. We define ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\hat{p}=-\\frac{\\sigma_{t r n}^{2}\\|\\hat{k}\\|^{2}}{\\hat{\\gamma}}\\hat{t}^{T}-\\sigma_{t r n}\\hat{k}\\mathrm{~and~}\\hat{q}^{T}=-\\frac{\\sigma_{t r n}\\|\\hat{t}\\|^{2}}{\\hat{\\gamma}}\\hat{k}^{T}\\hat{A}_{t r n}^{\\dagger}-\\hat{h}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then we have, ", "page_idx": 23}, {"type": "equation", "text": "$$\n(\\hat{A}_{t r n}+\\sigma_{t r n}u\\hat{v}_{t r n}^{T})^{\\dagger}=\\hat{A}_{t r n}^{\\dagger}+\\frac{\\sigma_{t r n}}{\\hat{\\gamma}}\\hat{t}^{T}\\hat{k}^{T}\\hat{A}_{t r n}^{\\dagger}-\\frac{\\hat{\\gamma}}{\\hat{\\tau}}\\hat{p}\\hat{q}^{T}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that, by our assumptions, we have $\\hat{t}=\\hat{v}_{t r n}(I-\\hat{A}_{t r n}^{\\dagger}\\hat{A}_{t r n})$ , and $(I-\\hat{A}_{t r n}^{\\dagger}\\hat{A}_{t r n})$ is a projection matrix, thus ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{v}_{t r n}^{T}\\hat{t}^{T}=\\hat{v}_{t r n}^{T}(I-\\hat{A}_{t r n}^{\\dagger}\\hat{A}_{t r n})^{T}\\hat{v}_{t r n}^{T}}\\\\ &{\\qquad\\quad=\\hat{v}_{t r n}^{T}(I-\\hat{A}_{t r n}^{\\dagger}\\hat{A}_{t r n})^{T}(I-\\hat{A}_{t r n}^{\\dagger}\\hat{A}_{t r n})^{T}\\hat{v}_{t r n}^{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "To compute $W_{o p t}\\;=\\;\\hat{X}_{t r n}\\big(\\hat{X}_{t r n}\\,+\\,\\hat{A}_{t r n}\\big)^{\\dagger}\\;=\\;\\sigma_{t r n}u\\hat{v}_{t r n}^{T}\\big(\\hat{A}_{t r n}\\,+\\,\\sigma_{t r n}u\\hat{v}_{t r n}^{T}\\big)^{\\dagger}$ , using $\\hat{\\gamma}\\mathrm{~-~}1\\mathrm{~=~}$ $\\sigma_{t r n}\\hat{v}_{t r n}^{T}\\hat{A}_{t r n}^{\\dagger}u=\\sigma_{t r n}\\hat{h}u$ , we multiply this through. ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma_{t r n}u\\dot{v}_{t r n}^{T}(\\hat{A}_{t r n}+\\sigma_{t r n}u\\hat{v}_{t r n}^{T})^{\\dagger}=\\sigma_{t r n}u\\dot{v}_{t r n}^{T}(\\hat{A}_{t r n}^{\\dagger}+\\frac{\\sigma_{t r n}}{\\hat{\\gamma}}\\dot{u}^{T}\\hat{k}^{T}\\hat{A}_{t r n}^{\\dagger}-\\frac{\\hat{\\gamma}}{\\hat{\\gamma}}\\hat{p}\\dot{q}^{T})}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\sigma_{t r n}u\\hat{b}+\\frac{\\sigma_{t r n}^{2}\\lVert\\hat{\\zeta}\\rVert^{2}}{\\hat{\\gamma}}\\boldsymbol{u}\\hat{k}^{T}\\hat{A}_{t r n}^{\\dagger}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\frac{\\sigma_{t r n}\\hat{\\gamma}}{\\hat{\\gamma}}u\\hat{v}_{t r n}^{T}\\left(\\frac{\\sigma_{t r n}^{2}\\lVert\\hat{b}\\rVert^{2}}{\\hat{\\gamma}}\\dot{t}^{T}+\\sigma_{t r n}\\hat{k}\\right)\\dot{q}^{T}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\sigma_{t r n}u\\hat{b}+\\frac{\\sigma_{t r n}^{2}\\lVert\\hat{\\zeta}\\rVert^{2}}{\\hat{\\gamma}}u\\hat{k}^{T}\\hat{A}_{t r n}^{\\dagger}+\\frac{\\sigma_{t r n}^{3}\\lVert\\hat{k}\\rVert^{2}\\lVert\\hat{t}\\rVert^{2}}{\\hat{\\gamma}}u\\hat{q}^{T}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\frac{\\sigma_{t r n}\\hat{\\gamma}(\\hat{\\gamma}-1)}{\\hat{\\gamma}}u\\hat{q}^{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then we have, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\sigma_{t r n}^{3}\\|\\hat{k}\\|^{2}\\|\\hat{t}\\|^{2}}{\\hat{\\tau}}u\\hat{q}^{T}=\\frac{\\sigma_{t r n}^{3}\\|\\hat{k}\\|^{2}\\|\\hat{t}\\|^{2}}{\\hat{\\tau}}u\\left(-\\frac{\\sigma_{t r n}\\|\\hat{t}\\|^{2}}{\\hat{\\gamma}}\\hat{k}^{T}\\hat{A}_{t r n}^{\\dagger}-\\hat{h}\\right)}\\\\ &{\\qquad\\qquad\\qquad=-\\frac{\\sigma_{t r n}^{4}\\|\\hat{k}\\|^{2}\\|\\hat{t}\\|^{4}}{\\hat{\\tau}\\hat{\\gamma}}u\\hat{k}^{T}\\hat{A}_{t r n}^{\\dagger}-\\frac{\\sigma_{t r n}^{3}\\|\\hat{k}\\|^{2}\\|\\hat{t}\\|^{2}}{\\hat{\\tau}}u\\hat{h}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\sigma_{t r n}\\hat{\\gamma}(\\hat{\\gamma}-1)}{\\hat{\\tau}}u\\hat{q}^{T}=\\frac{\\sigma_{t r n}\\hat{\\gamma}(\\hat{\\gamma}-1)}{\\hat{\\tau}}u\\left(-\\frac{\\sigma_{t r n}\\|\\hat{t}\\|^{2}}{\\hat{\\gamma}}\\hat{k}^{T}\\hat{A}_{t r n}^{\\dagger}-\\hat{h}\\right)}\\\\ &{\\qquad\\qquad\\qquad=-\\frac{\\sigma_{t r n}^{2}\\|\\hat{t}\\|^{2}(\\hat{\\gamma}-1)}{\\hat{\\tau}}u\\hat{k}^{T}\\hat{A}_{t r n}^{\\dagger}-\\frac{\\sigma_{t r n}\\hat{\\gamma}(\\hat{\\gamma}-1)}{\\hat{\\tau}}u\\hat{h}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Substituting back in and collecting like terms, we get, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma_{t r n}u\\hat{v}_{t r n}^{T}(\\mathcal{\\hat{A}}_{t r n}+\\sigma_{t r n}u\\hat{v}_{t r n}^{T})^{\\dagger}=\\sigma_{t r n}\\left(1-\\frac{\\sigma_{t r n}^{2}\\|\\hat{k}\\|^{2}\\|\\hat{t}\\|^{2}}{\\hat{\\tau}}-\\frac{\\hat{\\gamma}(\\hat{\\gamma}-1)}{\\hat{\\tau}}\\right)u\\hat{h}+}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\sigma_{t r n}^{2}\\left(\\frac{\\|\\hat{t}\\|^{2}}{\\hat{\\gamma}}-\\frac{\\sigma_{t r n}^{2}\\|\\hat{k}\\|^{2}\\|\\hat{t}\\|^{4}}{\\hat{\\tau}\\hat{\\gamma}}-\\frac{\\|\\hat{t}\\|^{2}(\\hat{\\gamma}-1)}{\\hat{\\tau}}\\right)u\\hat{k}^{T}\\hat{A}_{t r n}^{\\dagger}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We can then simplify the constants as follows. ", "page_idx": 24}, {"type": "equation", "text": "$$\n1-\\frac{\\sigma_{t r n}^{2}\\|\\hat{k}\\|^{2}\\|\\hat{t}\\|^{2}}{\\hat{\\tau}}-\\frac{\\hat{\\gamma}(\\hat{\\gamma}-1)}{\\hat{\\tau}}=\\frac{\\hat{\\tau}-\\sigma_{t r n}^{2}\\|\\hat{k}\\|^{2}\\|\\hat{t}\\|^{2}-\\gamma^{2}+\\gamma}{\\hat{\\tau}}=\\frac{\\hat{\\gamma}}{\\hat{\\tau}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{\\|\\hat{t}\\|^{2}}{\\hat{\\gamma}}-\\frac{\\sigma_{t r n}^{2}\\|\\hat{k}\\|^{2}\\|\\hat{t}\\|^{4}}{\\hat{\\tau}\\hat{\\gamma}}-\\frac{\\|\\hat{t}\\|^{2}(\\hat{\\gamma}-1)}{\\hat{\\tau}}=\\frac{\\|\\hat{t}\\|^{2}\\left(\\hat{\\tau}-\\sigma_{t r n}^{2}\\|\\hat{k}\\|^{2}\\|\\hat{t}\\|^{2}-\\hat{\\gamma}^{2}+\\hat{\\gamma}\\right)}{\\hat{\\tau}\\hat{\\gamma}}=\\frac{\\|\\hat{t}\\|^{2}}{\\hat{\\tau}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "E.3.3 Step 3: Decompose the terms into a sum of various trace terms. ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "For the bias and variance terms, we have the following two Lemmas. Lemma 2. If $W_{o p t}$ is the solution to Equation $^{\\,I}$ , then ", "page_idx": 25}, {"type": "equation", "text": "$$\nX_{t s t}-W_{o p t}X_{t s t}=\\frac{\\hat{\\gamma}}{\\hat{\\tau}}X_{t s t}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. To see this, note that we have $n+M>M$ . ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{X_{t s t}-W_{o p t}X_{t s t}=X_{t s t}-\\frac{\\sigma_{t r n}\\hat{\\gamma}}{\\hat{\\tau}}u\\hat{h}u v_{t s t}^{T}-\\frac{\\sigma_{t r n}^{2}\\|\\hat{t}\\|^{2}}{\\hat{\\tau}}u\\hat{k}^{T}\\hat{A}_{t r n}^{\\dagger}u v_{t s t}^{T}}\\\\ &{\\qquad\\qquad\\qquad=X_{t s t}-\\frac{\\hat{\\sigma}_{t r n}\\hat{\\gamma}}{\\hat{\\tau}}u\\hat{v}_{t r n}^{T}\\hat{A}_{t r n}^{\\dagger}u v_{t s t}^{T}-\\frac{\\sigma_{t r n}^{2}\\|\\hat{t}\\|^{2}}{\\hat{\\tau}}u\\hat{k}^{T}\\hat{A}_{t r n}^{\\dagger}u v_{t s t}^{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Note that $\\hat{\\gamma}=1+\\sigma_{t r n}\\hat{v}_{t r n}^{T}\\hat{A}_{t r n}^{\\dagger}u$ . Thus, we have that $\\sigma_{t r n}\\hat{v}_{t r n}^{T}\\hat{A}_{t r n}^{\\dagger}u=\\hat{\\gamma}-1$ . Substituting this into the second term, we get, ", "page_idx": 25}, {"type": "equation", "text": "$$\nX_{t s t}-W_{o p t}X_{t s t}=X_{t s t}-\\frac{\\hat{\\gamma}(\\hat{\\gamma}-1)}{\\hat{\\tau}}u v_{t s t}^{T}-\\frac{\\sigma_{t r n}^{2}\\lVert\\hat{t}\\rVert^{2}}{\\hat{\\tau}}u\\hat{k}^{T}\\hat{A}_{t r n}^{\\dagger}u v_{t s t}^{T}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For the third term, since $\\hat{k}=\\hat{A}_{t r n}^{\\dagger}u,\\hat{k}^{T}\\hat{A}_{t r n}^{\\dagger}u=\\hat{k}^{T}\\hat{k}=\\|\\hat{k}\\|^{2}$ . Substituting this into the expression, we get that ", "page_idx": 25}, {"type": "equation", "text": "$$\nX_{t s t}-W_{o p t}X_{t s t}=X_{t s t}-\\frac{\\hat{\\gamma}(\\hat{\\gamma}-1)}{\\hat{\\tau}}u v_{t s t}^{T}-\\frac{\\sigma_{t r n}^{2}\\|\\hat{t}\\|^{2}\\|\\hat{k}\\|^{2}}{\\hat{\\tau}}u v_{t s t}^{T}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since $X_{t s t}=u v_{t s t}^{T}$ , we get, ", "page_idx": 25}, {"type": "equation", "text": "$$\nX_{t s t}-W_{o p t}X_{t s t}=X_{t s t}\\left(1-\\frac{\\hat{\\gamma}(\\hat{\\gamma}-1)}{\\hat{\\tau}}-\\frac{\\sigma_{t r n}^{2}\\|\\hat{t}\\|^{2}\\|\\hat{k}\\|^{2}}{\\hat{\\tau}}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Simplify the constants using $\\hat{\\tau}=\\sigma_{t r n}^{2}\\|\\hat{t}\\|^{2}\\|\\hat{k}\\|^{2}+\\hat{\\gamma}^{2}$ , we get, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{\\hat{\\tau}+\\hat{\\gamma}-\\hat{\\gamma}^{2}-\\sigma_{t r n}^{2}\\|\\hat{t}\\|^{2}\\|\\hat{k}\\|^{2}}{\\hat{\\tau}}=\\frac{\\hat{\\gamma}}{\\hat{\\tau}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Lemma 3 (Sonthalia and Nadakuditi [24]). If the entries of $A_{t s t}$ are independent with mean $\\mathcal{O},$ , and variance $1/d,$ then we have that $\\begin{array}{r}{\\mathbb{E}_{A_{t s t}}[\\|\\dot{W}_{o p t}\\dot{A}_{t s t}\\|^{2}]=\\frac{\\bar{N_{t s t}}}{d}\\|W_{o p t}\\|^{2}}\\end{array}$ . ", "page_idx": 25}, {"type": "text", "text": "Lemma 4. If $\\hat{\\gamma}\\neq0$ and $A_{t r n}$ has full rank, then we have that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|W_{o p t}\\|_{F}^{2}=\\frac{\\sigma_{t r n}^{2}\\hat{\\gamma}^{2}}{\\tau^{2}}\\operatorname{Tr}(\\hat{h}^{T}\\hat{h})+2\\frac{\\sigma_{t r n}^{3}\\|\\hat{t}\\|^{2}\\hat{\\gamma}}{\\hat{\\tau}^{2}}\\operatorname{Tr}(\\hat{h}^{T}\\hat{k}^{T}\\hat{A}_{t r n}^{\\dag})+\\frac{\\sigma_{t r n}^{4}\\|\\hat{t}\\|^{4}}{\\hat{\\tau}^{2}}\\underbrace{\\operatorname{Tr}((\\hat{A}_{t r n}^{\\dag})^{T}\\hat{k}\\hat{k}^{T}\\hat{A}_{t r n}^{\\dag})}_{\\rho}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. We have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|W_{o p t}\\|_{F}^{2}=\\operatorname{Tr}(W_{o p t}^{T}W_{o p t})}\\\\ &{\\qquad\\qquad=\\operatorname{Tr}\\left(\\left(\\frac{\\sigma_{t r n}\\hat{Y}}{\\hat{T}}u\\hat{h}+\\frac{\\sigma_{t r n}^{2}\\|\\hat{t}\\|^{2}}{\\hat{T}}u\\hat{k}^{T}\\hat{A}_{t r n}^{\\dagger}\\right)^{T}\\left(\\frac{\\sigma_{t r n}\\hat{Y}}{\\hat{T}}u\\hat{h}+\\frac{\\sigma_{t r n}^{2}\\|\\hat{t}\\|^{2}}{\\hat{T}}u\\hat{k}^{T}\\hat{A}_{t r n}^{\\dagger}\\right)\\right)}\\\\ &{\\qquad\\qquad=\\frac{\\sigma_{t r n}^{2}\\hat{Y}^{2}}{\\hat{T}^{2}}\\operatorname{Tr}(\\hat{h}^{T}u^{T}u\\hat{h})+2\\frac{\\sigma_{t r n}^{3}\\|\\hat{t}\\|^{2}\\hat{\\gamma}}{\\hat{T}^{2}}\\operatorname{Tr}(\\hat{h}^{T}u^{T}u\\hat{k}^{T}\\hat{A}_{t r n}^{\\dagger})}\\\\ &{\\qquad\\qquad+\\frac{\\sigma_{t r n}^{4}\\|\\hat{t}\\|^{4}}{\\hat{T}^{2}}\\operatorname{Tr}((\\hat{A}_{t r n}^{\\dagger})^{T}\\hat{k}u^{T}u\\hat{k}^{T}\\hat{A}_{t r n}^{\\dagger})}\\\\ &{\\qquad\\qquad=\\frac{\\sigma_{t r n}^{2}\\hat{Y}^{2}}{\\hat{T}^{2}}\\operatorname{Tr}(\\hat{h}^{T}\\hat{h})+2\\frac{\\sigma_{t r n}^{3}\\|\\hat{t}\\|^{2}\\hat{\\gamma}}{\\hat{T}^{2}}\\operatorname{Tr}(\\hat{h}^{T}\\hat{k}^{T}\\hat{A}_{t r n}^{\\dagger})+\\frac{\\sigma_{t r n}^{4}\\|\\hat{t}\\|^{4}}{\\hat{T}^{2}}\\operatorname{Tr}((\\hat{A}_{t r n}^{\\dagger})^{T}\\hat{k}\\hat{k}^{T}\\hat{A}_{t r n}^{\\dagger}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Where the last inequality is true due to the fact that $\\|u\\|^{2}=1$ . ", "page_idx": 25}, {"type": "text", "text": "E.3.4 Step 4: Estimate With Random Matrix Theory ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Lemma 5. Let $A$ be a $p\\times q$ matrix and let $\\hat{A}=[A\\quad\\mu I]\\in\\mathbb{R}^{p\\times q+p}$ . Suppose $A=U\\Sigma V^{T}$ be the singular value decomposition of $A$ . If $\\hat{A}=\\hat{U}\\hat{\\Sigma}\\hat{V}^{T}$ is the singular value decomposition of $\\hat{A}$ , then U\u02c6 = U and if p < q ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\hat{\\Sigma}=\\left[\\begin{array}{c c c c}{\\sqrt{\\sigma_{1}(A)^{2}+\\mu^{2}}}&{0}&{\\cdots}&{0}\\\\ {0}&{\\sqrt{\\sigma_{2}(A)^{2}+\\mu^{2}}}&{}&{0}\\\\ {\\vdots}&{}&{\\ddots}&{}\\\\ {0}&{0}&{\\cdots}&{\\sqrt{\\sigma_{p}(A)^{2}+\\mu^{2}}}\\end{array}\\right]\\in\\mathbb{R}^{p\\times p},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{V}=\\left[V_{1:p}\\Sigma\\hat{\\Sigma}^{-1}\\right]\\in\\mathbb{R}^{q+p\\times p}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Here $V_{1:p}$ are the first p columns of $V$ . ", "page_idx": 26}, {"type": "text", "text": "Proof. Since $p<q$ , we have that $U\\in\\mathbb{R}^{p\\times p}$ , $\\boldsymbol{\\Sigma}\\in\\mathbb{R}^{p\\times p}$ are invertible. Here also consider the form of the SVD in which $V^{T}\\in\\mathbb{R}^{p\\times q}$ . ", "page_idx": 26}, {"type": "text", "text": "We start by nothing that ${\\hat{U}}{\\hat{\\Sigma}}^{2}{\\hat{U}}^{T}={\\hat{A}}{\\hat{A}}^{T}=A A^{T}\\!+\\!\\mu^{2}I=U(\\Sigma^{2}\\!+\\!\\mu^{2}I_{p})U^{T}$ . Thus, we immediately see that $\\sigma_{i}(\\hat{A})^{2}=\\sigma_{i}(A)^{2}+\\mu^{2}$ and that $\\hat{U}=U$ . ", "page_idx": 26}, {"type": "text", "text": "Finally, we see, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\hat{V}^{T}=\\hat{\\Sigma}^{-1}U^{T}\\hat{A}=\\left[\\hat{\\Sigma}^{-1}\\Sigma V_{1:p}^{T}\\quad\\mu\\hat{\\Sigma}^{-1}U^{T}\\right]\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Lemma 6. Let $A$ be a $p\\times q$ matrix and let $\\hat{A}=[A\\quad\\mu I]\\in\\mathbb{R}^{p\\times q+p}$ . Suppose $A=U\\Sigma V^{T}$ be the singular value decomposition of $A$ . If $\\hat{A}=\\hat{U}\\hat{\\Sigma}\\hat{V}^{T}$ is the singular value decomposition of $\\hat{A}$ , then U\u02c6 = U and if p > q ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\hat{\\Sigma}=\\left[\\begin{array}{c c c c c c}{\\sqrt{\\sigma_{1}(A)^{2}+\\mu^{2}}}&{0}&{\\cdots}&{0}&{\\cdots}&{0}\\\\ {0}&{\\sqrt{\\sigma_{2}(A)^{2}+\\mu^{2}}}&{}&{0}&{}&{}\\\\ {\\vdots}&{}&{}&{\\ddots}&{}&{\\vdots}&{}\\\\ {0}&{0}&{\\cdots}&{\\sqrt{\\sigma_{q}(A)^{2}+\\mu^{2}}}&{}&{0}\\\\ {}&{}&{}&{}&{}&{\\mu}&{}\\\\ {\\vdots}&{}&{}&{}&{}&{}&{\\ddots}&{0}\\\\ {0}&{}&{}&{0}&{\\cdots}&{0}&{\\cdots}&{0}\\end{array}\\right]\\in\\mathbb{R}^{p\\times p}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Here we will denote the upper left $q\\times q$ block by $C$ . Further, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{V}=\\left[\\overset{V\\Sigma_{1:q,1:q}^{T}C^{-1}}{\\mu U_{1:q}C^{-1}}\\right.\\quad0\\quad\\bigg]\\in\\mathbb{R}^{q+p\\times p}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. Since $p\\,>\\,q$ , we have that $U\\,\\in\\,\\mathbb{R}^{p\\times p}$ and we have that $\\Sigma\\,\\in\\,\\mathbb{R}^{p\\times q}$ . Here $V^{T}\\,\\in\\,\\mathbb{R}^{q\\times q}$ is invertible. ", "page_idx": 26}, {"type": "text", "text": "We start with nothing, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\hat{U}\\hat{\\Sigma}^{2}\\hat{U}^{T}=\\hat{A}\\hat{A}^{T}=A A^{T}+\\mu^{2}I=U\\left(\\left[\\begin{array}{c c}{{\\Sigma_{1:q,1:q}^{2}}}&{{0}}\\\\ {{0}}&{{0_{q-p}}\\end{array}\\right]+\\mu^{2}I_{q}\\right)U^{T}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Thus, we immediately see that for $i=1,\\dots,p\\;\\sigma_{i}(\\hat{A})^{2}=\\sigma_{i}(A)^{2}+\\mu^{2}$ and for $i=p+1,\\ldots,q$ , we have that $\\sigma_{i}(\\hat{A})^{2}=\\mu^{2}$ and that $\\hat{U}=U$ . ", "page_idx": 26}, {"type": "text", "text": "Then, we see, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\hat{V}^{T}=\\hat{\\Sigma}^{-1}U^{T}\\hat{A}=\\left[\\hat{\\Sigma}^{-1}\\Sigma V^{T}\\quad\\mu\\hat{\\Sigma}^{-1}U^{T}\\right].\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Note that $\\Sigma$ has 0 for the last $p-q$ entries. Thus, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\hat{\\Sigma}^{-1}\\Sigma V=\\left[{\\cal C}^{-1}\\Sigma_{1:q,1:q}V\\right].\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Similarly, due to the structure of $\\hat{\\Sigma}$ , we see, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mu\\hat{\\Sigma}^{-1}U^{T}=[\\mu C^{-1}U_{1:q}^{T}\\quad\\mu\\frac{1}{\\mu}U_{q+1:p}^{T}].\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Lemma 7. Suppose $A$ is an $p$ by $q$ matrix such that $p<q$ , the entries of $A$ are independent and have mean $\\boldsymbol{O}$ , variance $1/p,$ , and bounded fourth moment. Let $c=p/q$ . Let $\\hat{A}=[A\\quad\\mu I]\\in\\mathbb{R}^{p\\times q+p}$ . Let $W_{p}=\\hat{A}\\hat{A}^{T}$ and let $W_{q}=\\hat{A}^{T}\\hat{A}$ . Suppose $\\lambda_{p}$ is a random non-zero eigenvalue from the largest $p$ eigenvalues of $W_{p}$ , and $\\lambda_{q}$ is a random non-zero eigenvalue of $W_{q}$ . Then ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I.\\ \\mathbb{E}\\left[\\frac{1}{\\lambda_{p}}\\right]=\\mathbb{E}\\left[\\frac{1}{\\lambda_{q}}\\right]=\\frac{\\sqrt{(1+\\mu^{2}c-c)^{2}+4\\mu^{2}c^{2}}-1-\\mu^{2}c+c}{2\\mu^{2}c}+o(1).}\\\\ &{2.\\ \\mathbb{E}\\left[\\frac{1}{\\lambda_{p}^{2}}\\right]=\\mathbb{E}\\left[\\frac{1}{\\lambda_{q}^{2}}\\right]=\\frac{\\mu^{2}c^{2}+c^{2}+\\mu^{2}c-2c+1}{2\\mu^{4}c\\sqrt{4\\mu^{2}c^{2}+(1-c+\\mu^{2}c)^{2}}}+\\frac{1}{2\\mu^{4}}\\left(1-\\frac{1}{c}\\right)+o(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. First, we note that the non-zero eigenvalues of $W_{p}$ and $W_{q}$ are the same. Hence we focus on $W_{p}$ . $W_{p}$ is nearly a Wishart matrix but is not normalized by the correct value. However, $c W_{p}$ does have the correct normalization. ", "page_idx": 27}, {"type": "text", "text": "Due to the assumptions on $A$ , we have that the eigenvalues of $c A A^{T}$ converge to the MarchenkoPastur. Hence since the eigenvalues of $c W_{p}$ are ", "page_idx": 27}, {"type": "equation", "text": "$$\n(c\\lambda_{p})_{i}=c\\sigma_{i}(A)^{2}+c\\mu^{2},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "we can estimate them by estimating $c\\sigma_{i}(A)^{2}$ with the Marchenko-Pastur [44, 50\u201353]. In particular, we want the expectation of the inverse. We need to use the Stieljes transform. We know that if $m_{c}(z)$ is the Stieljes transform for the Marchenko-Pastur with shape parameter $c$ , then if $\\lambda$ is sampled from the Marchenko-Pastur distribution, then ", "page_idx": 27}, {"type": "equation", "text": "$$\nm_{c}(z)=\\mathbb{E}_{\\lambda}\\left[{\\frac{1}{\\lambda-z}}\\right].\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, we have that the expected inverse of the eigenvalue can be approximated $m(-c\\mu^{2})$ . We know that the Steiljes transform: ", "page_idx": 27}, {"type": "equation", "text": "$$\nm_{c}(z)=-\\frac{1-z-c-\\sqrt{(1-z-c)^{2}-4c z}}{-2z c}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, we have, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\frac{1}{c\\lambda_{p}}\\right]=m(-c\\mu^{2})=\\frac{\\sqrt{(1+\\mu^{2}c-c)^{2}+4\\mu^{2}c^{2}}-1-\\mu^{2}c+c}{2\\mu^{2}c^{2}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Canceling $1/c$ from both sides, we get, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\frac{1}{\\lambda_{p}}\\right]=\\frac{\\sqrt{(1+\\mu^{2}c-c)^{2}+4\\mu^{2}c^{2}}-1-\\mu^{2}c+c}{2\\mu^{2}c}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then for the estimate of $\\mathbb{E}\\left[1/\\lambda_{p}^{2}\\right]$ , we need to compute the derivative of the $m_{c}(z)$ and evaluate it at $-c\\mu^{2}$ . Hence, we see, ", "page_idx": 27}, {"type": "equation", "text": "$$\nm_{c}^{\\prime}(z)=\\frac{(c-z+\\sqrt{-4c z+(1-c-z)^{2}}-1)(c+z+\\sqrt{-4c z+(1-c-z)^{2}}-1)}{4c z^{2}\\sqrt{-4c z+(1-c-z)^{2}}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\tau\\left[\\frac{1}{c^{2}\\lambda_{p}^{2}}\\right]=m_{c}^{\\prime}(-c\\mu^{2})}}\\\\ {{\\ ~~~~~~~~~~~~=\\frac{(c+\\mu^{2}c+\\sqrt{4\\mu^{2}c^{2}+(1-c+\\mu^{2}c)^{2}}-1)(c-\\mu^{2}c+\\sqrt{4\\mu^{2}c^{2}+(1-c+\\mu^{2}c)^{2}}-1)}{4\\mu^{4}c^{3}\\sqrt{4\\mu^{2}c^{2}+(1-c+\\mu^{2}c)^{2}}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Canceling the $1/c^{2}$ from both sides, we get, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathfrak{z}\\left[\\frac{1}{\\lambda_{p}^{2}}\\right]=\\frac{(c+\\mu^{2}c+\\sqrt{4\\mu^{2}c^{2}+(1-c+\\mu^{2}c)^{2}}-1)(c-\\mu^{2}c+\\sqrt{4\\mu^{2}c^{2}+(1-c+\\mu^{2}c)^{2}}-1)}{4\\mu^{4}c\\sqrt{4\\mu^{2}c^{2}+(1-c+\\mu^{2}c)^{2}}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Multiplying out and simplifying ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\frac{1}{\\lambda_{p}^{2}}\\right]=\\frac{\\mu^{2}c^{2}+c^{2}+\\mu^{2}c-2c+1}{2\\mu^{4}c\\sqrt{4\\mu^{2}c^{2}+(1-c+\\mu^{2}c)^{2}}}+\\frac{1}{2\\mu^{4}}\\left(1-\\frac{1}{c}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Lemma 8. Suppose $A$ is an $p$ by $q$ matrix such that $p>q$ , the entries of $A$ are independent and have mean $\\boldsymbol{O}$ , variance $1/p,$ , and bounded fourth moment. Let $c=p/q$ . Let $\\hat{A}=[A\\quad\\mu I]\\in\\mathbb{R}^{p\\times q+p}$ . Let $W_{p}=\\hat{A}\\hat{A}^{T}$ and let $W_{q}=\\hat{A}^{T}\\hat{A}$ . Suppose $\\lambda_{p}$ is a random non-zero eigenvalue of $W_{p}$ , and $\\lambda_{q}$ is $a$ random eigenvalue from the largest q eigenvalues of $W_{q}$ . Then ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I.\\ \\mathbb{E}\\left[\\frac{1}{\\lambda_{q}}\\right]=\\mathbb{E}\\left[\\frac{1}{\\lambda_{p}}\\right]=\\frac{\\sqrt{4\\mu^{2}c+(-1+c+\\mu^{2}c)^{2}}-c-\\mu^{2}c+1}{2\\mu^{2}}+o(1).}\\\\ &{2.\\ \\mathbb{E}\\left[\\frac{1}{\\lambda_{q}^{2}}\\right]=\\mathbb{E}\\left[\\frac{1}{\\lambda_{p}^{2}}\\right]=\\frac{1-2c+c^{2}+\\mu^{2}c+\\mu^{2}c^{2}}{2\\mu^{4}\\sqrt{4\\mu^{2}c+(-1+c+\\mu^{2}c)^{2}}}+(1-c)\\frac{1}{2\\mu^{4}}+o(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. First, we note that the non-zero eigenvalues of $W_{p}$ and $W_{q}$ are the same. Hence we focus on $W_{p}$ . Due to the assumptions on $A$ , we have that the eigenvalues of $A^{T}A$ converge to the MarchenkoPastur with shape $c^{-1}$ . Hence if $\\lambda_{p}$ is one of the first $q$ eigenvalues of $W_{p}$ , we see, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\frac{1}{\\lambda_{p}}\\right]=m_{c^{-1}}(\\mu^{2})=\\frac{\\sqrt{(1+\\mu^{2}-1/c)^{2}+4\\mu^{2}/c}-1-\\mu^{2}+1/c}{2\\mu^{2}/c}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Then for the estimate of $\\mathbb{E}\\left[1/\\lambda_{p}^{2}\\right]$ , we need to compute the derivative of the $m_{c^{-1}}(z)$ and evaluate it at $-\\mu^{2}$ . Hence, we see, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\ z\\left[\\frac{1}{\\lambda_{p}^{2}}\\right]=\\frac{(1/c+\\mu^{2}+\\sqrt{4\\mu^{2}/c+(1-1/c+\\mu^{2})^{2}}-1)(1/c-\\mu^{2}+\\sqrt{4\\mu^{2}/c+(1-1/c+\\mu^{2})^{2}}-c)}{4\\mu^{4}/c\\sqrt{4\\mu^{2}/c+(1-1/c+\\mu^{2})^{2}}}}\\\\ &{}&{=\\frac{(1+\\mu^{2}c+c\\sqrt{4\\mu^{2}/c+(1-1/c+\\mu^{2})^{2}}-c)(1-\\mu^{2}c+c\\sqrt{4\\mu^{2}/c+(1-1/c+\\mu^{2})^{2}}-c)}{4\\mu^{4}c\\sqrt{4\\mu^{2}/c+(1-1/c+\\mu^{2})^{2}}}}\\\\ &{}&{=\\frac{(1+\\mu^{2}c+\\sqrt{4\\mu^{2}c+(-1+c+\\mu^{2}c)^{2}}-c)(1-\\mu^{2}c+\\sqrt{4\\mu^{2}c+(-1+c+\\mu^{2}c)^{2}}-c)}{4\\mu^{4}\\sqrt{4\\mu^{2}c+(-1+c+\\mu^{2}c)^{2}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "This can be further simplified to ", "page_idx": 28}, {"type": "equation", "text": "$$\n{\\frac{1-2c+c^{2}+\\mu^{2}c+\\mu^{2}c^{2}}{2\\mu^{4}{\\sqrt{4\\mu^{2}c+(-1+c+\\mu^{2}c)^{2}}}}}+(1-c){\\frac{1}{2\\mu^{4}}}+o(1)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We will also need to estimate some other terms. ", "page_idx": 28}, {"type": "text", "text": "Lemma 9. Suppose $A$ is an $p$ by $q$ matrix such that the entries of $A$ are independent and have mean $O_{i}$ , variance $1/p,$ , and bounded fourth moment. Let $\\hat{A}=[A\\quad\\mu I]\\in\\mathbb{R}^{p\\times q+p}$ . Let $W_{p}=\\hat{A}\\hat{A}^{T}$ and let $W_{q}=\\hat{A}^{T}\\hat{A}.$ . Suppose $\\lambda_{p},\\lambda_{q}$ are random non-zero eigenvalues of $W_{p},W_{q}$ from the largest $\\operatorname*{min}(p,q)$ eigenvalues of $W_{p},W_{q}$ . Then ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{I.\\ I f p>q,\\mathbb{E}\\left[\\frac{\\lambda_{p}-\\mu^{2}}{\\lambda_{p}}\\right]=c\\left(\\frac{1}{2}+\\frac{1+\\mu^{2}c-\\sqrt{(-1+c+\\mu^{2}c)^{2}+4\\mu^{2}c}}{2c}\\right)+o(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{2.\\ I f p<q,\\mathbb{E}\\left[\\frac{\\lambda_{q}-\\mu^{2}}{\\lambda_{q}}\\right]=\\frac{1}{2}+\\frac{1+\\mu^{2}c-\\sqrt{(1-c+\\mu^{2}c)^{2}+4c^{2}\\mu^{2}}}{2c}+o(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{3.\\ I f p>q,\\mathbb{E}\\left[\\frac{\\lambda_{p}-\\mu^{2}}{\\lambda_{p}^{2}}\\right]=c\\left(\\frac{1+c+\\mu^{2}c}{2\\sqrt{(-1+c+\\mu^{2}c)^{2}+4\\mu^{2}c}}-\\frac{1}{2}\\right)+o(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$\\begin{array}{r}{I f p<q,\\,\\mathbb{E}\\left[\\frac{\\lambda_{q}-\\mu^{2}}{\\lambda_{q}^{2}}\\right]=\\frac{1+c+\\mu^{2}c}{2\\sqrt{(1-c+c\\mu^{2})^{2}+4c^{2}\\mu^{2}}}-\\frac{1}{2}+o(1).}\\end{array}$ ", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. Notice that if $\\lambda$ is an eigenvalue of $A$ (so unshifted). ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{\\lambda}{\\lambda+\\mu^{2}}=1-\\frac{\\mu^{2}}{\\lambda+\\mu^{2}}\\;\\mathrm{and}\\;\\frac{\\lambda}{(\\lambda+\\mu^{2})^{2}}=\\frac{1}{\\lambda+\\mu^{2}}-\\frac{\\mu^{2}}{(\\lambda+\\mu^{2})^{2}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then use Lemmas 7, and 8 to finish the proof. ", "page_idx": 29}, {"type": "text", "text": "Bounding the Variance. ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Lemma 10. Let $\\eta_{n}$ be a uniform measure on n numbers $a_{1},\\ldots,a_{n}$ such that $\\eta^{n}\\rightarrow\\eta$ weakly in probability. Then for any bounded continuous function $f$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n{\\frac{1}{n}}\\sum_{i=1}^{n-1}f(a_{i})\\to\\mathbb{E}_{x\\sim\\eta}[f(x)].\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. Using weak convergence ", "page_idx": 29}, {"type": "equation", "text": "$$\n{\\frac{1}{n}}\\sum_{i=1}^{n}f(a_{i})\\to\\mathbb{E}_{x\\sim\\eta}[f(x)].\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then using the boundedness of $f$ , we get, ", "page_idx": 29}, {"type": "equation", "text": "$$\n{\\frac{1}{n}}\\sum_{i=1}^{n-1}f(a_{i})-{\\frac{1}{n}}\\sum_{i=1}^{n}f(a_{i})=-{\\frac{1}{n}}f(a_{n})\\to0.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Lemma 11. Let $\\eta_{n}$ be a uniform measure on n numbers $a_{1},\\ldots,a_{n}$ such that $\\eta_{n}\\rightarrow\\eta$ weakly in probability. Let s be a uniformly random unit vector in $\\mathbb{R}^{m}$ independent of $\\eta_{n}$ . Suppose $n/m\\rightarrow\\zeta\\in$ $(0,1]$ . Then for any bounded function $f$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}_{s}\\left[\\sum_{i=1}^{n}s_{i}^{2}f(a_{i})\\right]\\to\\zeta\\mathbb{E}_{x\\sim\\eta}[f(x)]\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}_{s}\\left[\\left(\\sum_{i=1}^{n}s_{i}^{2}f(a_{i})\\right)^{2}\\right]-\\mathbb{E}_{s}\\left[\\sum_{i=1}^{n}s_{i}^{2}f(a_{i})\\right]^{2}\\rightarrow0.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. The first limit comes directly from weak convergence. ", "page_idx": 29}, {"type": "text", "text": "For the second, notice, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{n}s_{i}^{2}f(a_{i})\\right)^{2}=\\sum_{i=1}^{n}s_{i}^{4}f(a_{i})^{2}+\\sum_{i\\neq j}s_{i}^{2}s_{j}^{2}f(a_{i})f(a_{j})=\\sum_{i=1}^{n}s_{i}^{4}f(a_{i})^{2}+\\sum_{i=1}^{n}s_{i}^{2}f(a_{i})\\sum_{j\\neq i}s_{j}^{2}f(a_{j}).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Taking the expectation with respect to $s$ we get, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}_{s}\\left[\\left(\\sum_{i=1}^{n}s_{i}^{2}f(a_{i})\\right)^{2}\\right]=\\frac{1}{m^{2}+O(m)}\\sum_{i=1}^{n}f(a_{i})^{2}+\\frac{1}{m^{2}+O(m)}\\sum_{i=1}^{n}f(a_{i})\\sum_{j\\neq i}f(a_{j})\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then using Lemma 10 for any fixed $i$ , we have, ", "page_idx": 29}, {"type": "equation", "text": "$$\n{\\frac{1}{m}}\\sum_{j\\neq i}f(a_{j})\\to\\zeta\\mathbb{E}_{x\\sim\\eta}[f(x)].\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Thus, as $n\\to\\infty$ , we have, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}_{s}\\left[\\left(\\sum_{i=1}^{n}s_{i}^{2}f(a_{i})\\right)^{2}\\right]\\rightarrow\\zeta^{2}\\mathbb{E}_{x\\sim\\eta}[f(x)]^{2}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then since ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}_{s}\\left[\\sum_{i=1}^{n}s_{i}^{2}f(a_{i})\\right]^{2}\\rightarrow\\zeta^{2}\\mathbb{E}_{x\\sim\\eta}[f(x)]^{2}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Thus, the variance goes to zero. ", "page_idx": 30}, {"type": "text", "text": "The interpretation of the above Lemma is that the variance of the sum decays to zero as $m\\rightarrow\\infty$ . ", "page_idx": 30}, {"type": "text", "text": "Lemma 12. Suppose $A$ is an $p$ by $q$ matrix such that the entries of $A$ are independent and have mean $\\boldsymbol{O}$ , variance $1/p,$ , and bounded fourth moment. Let $\\hat{A}=[A\\quad\\mu I]\\in\\mathbb{R}^{p\\times q+p}$ . Let $x\\in\\mathbb{R}^{p}$ and $\\hat{y}\\in\\mathbb{R}^{p+q}$ be unit norm vectors such that $\\hat{y}^{T}=[y^{T}\\quad0_{p}]$ . Then ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\mathrm{Tr}(x^{T}(\\hat{A}\\hat{A}^{T})^{\\dagger}x]=\\frac{\\sqrt{(1-c+\\mu^{2}c)^{2}+4\\mu^{2}c^{2}}-1-\\mu^{2}c+c}{2\\mu^{2}c}+o(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "equation", "text": "$\\begin{array}{r}{\\mathbb{E}[\\mathrm{Tr}(x^{T}(\\hat{A}\\hat{A}^{T})^{\\dagger}x]=\\frac{\\sqrt{(-1+c+\\mu^{2}c)^{2}+4\\mu^{2}c}-1-\\mu^{2}c+c}{2\\mu^{2}c}+o(1).}\\end{array}$ ", "text_format": "latex", "page_idx": 30}, {"type": "equation", "text": "$\\begin{array}{r}{\\mathbb{E}[\\mathrm{Tr}(\\hat{y}^{T}(\\hat{A}^{T}\\hat{A})^{\\dag}\\hat{y}]=c\\left(\\frac{1+c+\\mu^{2}c}{2\\sqrt{(-1+c+\\mu^{2}c)^{2}+4\\mu^{2}c}}-\\frac{1}{2}\\right)+o(1).}\\end{array}$ ", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "The variance of each above is $o(1)$ . ", "page_idx": 30}, {"type": "text", "text": "Proof. Let us start with $p<q$ . ", "page_idx": 30}, {"type": "text", "text": "Let $\\hat{A}=\\hat{U}\\hat{\\Sigma}\\hat{V}^{T}$ , where $\\hat{\\Sigma}$ is $p\\times p$ . Then we see, ", "page_idx": 30}, {"type": "equation", "text": "$$\n(\\hat{A}\\hat{A}^{T})^{\\dagger}=\\hat{U}\\hat{\\Sigma}^{-2}\\hat{U}^{T}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Where $\\hat{U}$ is uniformly random. Thus similar to [24], we can use Lemma 7 to get, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathrm{Tr}(x^{T}(\\hat{A}\\hat{A}^{T})^{\\dagger}x]=\\frac{\\sqrt{(1+\\mu^{2}c-c)^{2}+4\\mu^{2}c^{2}}-1-\\mu^{2}c+c}{2\\mu^{2}c}+o(1).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "On the other hand, for $p>q$ , we have that only the first $q$ eigenvalues have the expectation in Lemma 8 The other $p-q$ are equal to $\\textstyle{\\frac{1}{\\mu^{2}}}$ . Thus, we see, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathrm{Tr}(x^{T}(\\hat{A}\\hat{A}^{T})^{\\dagger}x]=\\frac{1}{c}\\left(\\frac{\\sqrt{4\\mu^{2}c+(-1+c+\\mu^{2}c)^{2}}-c-\\mu^{2}c+1}{2\\mu^{2}}+o(1)\\right)+\\left(1-\\frac{1}{c}\\right)\\frac{1}{\\mu^{2}}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Again let us first consider the case when $p<q$ . Then we have, ", "page_idx": 30}, {"type": "equation", "text": "$$\n(\\hat{A}^{T}\\hat{A})^{\\dag}=\\hat{V}\\hat{\\Sigma}^{-2}\\hat{V}^{T}=\\left[\\overset{V_{1:p}\\Sigma\\hat{\\Sigma}^{-1}}{\\mu U\\hat{\\Sigma}^{-1}}\\right]\\hat{\\Sigma}^{-2}\\left[\\hat{\\Sigma}^{-1}\\Sigma V_{1:p}^{T}\\quad\\mu\\hat{\\Sigma}^{-1}U^{T}\\right].\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Since $\\hat{y}$ has zeros in the last $p$ coordinates, we see, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\hat{y}^{T}(\\hat{A}^{T}\\hat{A})^{\\dagger}\\hat{y}=y^{T}V_{1:p}\\Sigma\\hat{\\Sigma}^{-4}\\Sigma V_{1:p}^{T}y.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Thus, we can use Lemma 9 to estimate this as, ", "page_idx": 30}, {"type": "equation", "text": "$$\nc\\left(\\frac{1+c+\\mu^{2}c}{2\\sqrt{(1-c+c\\mu^{2})^{2}+4c^{2}\\mu^{2}}}-\\frac{1}{2}\\right)+o(1).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "The extra factor of $c$ comes from the sum of $p$ coordinates of a uniformly unit vector in $q$ dimensional space. And for $p>q$ , we have that the estimate is ", "page_idx": 31}, {"type": "equation", "text": "$$\nc\\left(\\frac{1+c+\\mu^{2}c}{2\\sqrt{(1+\\mu^{2}-1/c)^{2}+4\\mu^{2}/c}}-\\frac{1}{2}\\right)+o(1).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "For the variance term, use Lemma 11. For three of the cases, the limiting distribution is the MarchenkoPastur distribution. For the other case, the limiting measure is a mixture of the Marchenko-Pastur and a dirac delta at $1/\\mu^{2}$ . \u53e3 ", "page_idx": 31}, {"type": "text", "text": "The rest of the lemmas in this section are used to compute the mean and variance of the various terms that appear in the formula of $W_{o p t}$ . ", "page_idx": 31}, {"type": "text", "text": "Lemma 13. We have that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}_{A_{t r n}}\\left[\\|\\hat{h}\\|^{2}\\right]=\\left\\{c\\left(\\frac{1+c+\\mu^{2}c}{2\\sqrt{(1-c+\\mu^{2}c)^{2}+4\\mu^{2}c^{2}}}-\\frac{1}{2}\\right)+o(1)\\quad c<1\\right.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and that $\\mathbb{V}(\\|\\hat{h}\\|^{2})=o(1)$ . ", "page_idx": 31}, {"type": "text", "text": "Proof. Here we see that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\|\\hat{h}\\|^{2}=\\mathrm{Tr}(\\hat{v}_{t r n}^{T}(\\hat{A}_{t r n}^{T}\\hat{A}_{t r n})^{\\dagger}\\hat{v}_{t r n}^{T}).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Thus, using the Lemma 12 we get that if $c<1$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\|\\hat{h}\\|^{2}]=c\\left(\\frac{1+c+\\mu^{2}c}{2\\sqrt{(1-c+\\mu^{2}c)^{2}+4\\mu^{2}c^{2}}}-\\frac{1}{2}\\right)+o(1)\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and if $c>1$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\|\\hat{h}\\|^{2}]=c\\left(\\frac{1+c+\\mu^{2}c}{2\\sqrt{(-1+c+\\mu^{2}c)^{2}+4\\mu^{2}c}}-\\frac{1}{2}\\right)+o(1).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Lemma 14. We have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{A_{t r n}}\\left[\\|\\hat{k}\\|^{2}\\right]=\\left\\{\\frac{\\sqrt{(1-c+\\mu^{2}c)^{2}+4\\mu^{2}c^{2}}-1-\\mu^{2}c+c}{2\\mu^{2}c}+o(1)\\quad c<1\\ }\\\\ {\\frac{\\sqrt{(-1+c+\\mu^{2}c)^{2}+4\\mu^{2}c}-1-\\mu^{2}c+c}{2\\mu^{2}c}+o(1)\\quad c>1}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and that $\\mathbb{V}(\\|\\hat{k}\\|^{2})=o(1)$ . ", "page_idx": 31}, {"type": "text", "text": "Proof. Since $\\hat{k}=\\hat{A}_{t r n}^{\\dagger}u$ , we have that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\Vert\\hat{k}\\Vert^{2}=\\mathrm{Tr}(u^{T}(\\hat{A}_{t r n}\\hat{A}_{t r n}^{T})^{\\dagger}u).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "According to the Lemma 12, if $c<1$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\|\\hat{k}\\|^{2}]=\\frac{\\sqrt{(1-c+\\mu^{2}c)^{2}+4\\mu^{2}c^{2}}-1-\\mu^{2}c+c}{2\\mu^{2}c}+o(1)\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and if $c>1$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\|\\hat{k}\\|^{2}]=\\frac{\\sqrt{(-1+c+\\mu^{2}c)^{2}+4\\mu^{2}c}-1-\\mu^{2}c+c}{2\\mu^{2}c}+o(1).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Lemma 15. We have that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}_{A_{t r n}}\\left[\\|\\hat{t}\\|^{2}\\right]=\\left\\{\\begin{array}{l l}{\\frac{1}{2}\\left(1-c-\\mu^{2}c+\\sqrt{(1-c+\\mu^{2}c)^{2}+4c^{2}\\mu^{2}}\\right)+o(1)}&{c<1}\\\\ {\\frac{1}{2}\\left(1-c-\\mu^{2}c+\\sqrt{(-1+c+\\mu^{2}c)^{2}+4\\mu^{2}c}\\right)+o(1)}&{c>1}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and we have that $\\mathbb{V}(\\|\\hat{t}\\|^{2})=o(1)$ ", "page_idx": 31}, {"type": "text", "text": "Proof. Here we see that $\\hat{t}=\\hat{v}_{t r n}(I-\\hat{A}_{t r n}^{\\dagger}\\hat{A}_{t r n})$ . Thus, we see that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|\\b{\\hat{t}}\\|^{2}=\\|\\b{v_{t r n}}\\|^{2}-\\hat{v}_{t r n}^{T}\\hat{A}_{t r n}^{\\dag}\\hat{A}_{t r n}\\hat{v}_{t r n}=1-\\hat{v}_{t r n}^{T}\\hat{A}_{t r n}^{\\dag}\\hat{A}_{t r n}\\hat{v}_{t r n}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "If $\\hat{V}\\in\\mathbb{R}^{p+q\\times p+q}$ , we have that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\hat{A}_{t r n}^{\\dagger}\\hat{A}_{t r n}=\\hat{V}\\left[I_{p}\\quad0\\right]\\hat{V}^{T}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Then if $p<q$ using Lemma 6 and the fact that the last $p$ coordinates of $\\hat{v}_{t r n}$ are 0, we see that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{v}_{t r n}^{T}\\hat{A}_{t r n}^{\\dagger}\\hat{A}_{t r n}\\hat{v}_{t r n}=v_{t r n}^{T}V_{1:p}\\Sigma\\hat{\\Sigma}^{-2}\\Sigma V_{1:p}^{T}v_{t r n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Then using Lemma 9 to estimate the middle diagonal matrix, we get that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\mathbb{E}[\\|\\hat{t}\\|^{2}]=1-c\\left(\\frac{1}{2}+\\frac{1+\\mu^{2}c-\\sqrt{(1+\\mu^{2}c-c)^{2}+4c^{2}\\mu^{2}}}{2c}\\right)}\\\\ {=\\displaystyle\\frac{1}{2}\\left(1-c-\\mu^{2}c+\\sqrt{(1-c+\\mu^{2}c)^{2}+4c^{2}\\mu^{2}}\\right)+o(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Similarly for $c>1$ , we have that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}[\\|\\hat{t}\\|^{2}]=1-\\left(\\frac{1}{2}+\\frac{c+\\mu^{2}c-c\\sqrt{(1+\\mu^{2}-1/c)^{2}+4\\mu^{2}/c}}{2}\\right)+o(1)}\\\\ {=\\frac{1}{2}\\left(1-c-\\mu^{2}c+\\sqrt{(-1+c+\\mu^{2}c)^{2}+4\\mu^{2}c}\\right)+o(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The variance of $\\hat{A}_{t r n}^{\\dagger}\\hat{A}_{t r n}$ is also $o(1)$ using Lemma 11. ", "page_idx": 32}, {"type": "text", "text": "Lemma 16. We have that $\\mathbb{E}_{A_{t r n}}\\left[\\hat{\\gamma}\\right]=1$ and $\\mathbb{V}(\\gamma)=O(\\sigma_{t r n}^{2}/d).$ . ", "page_idx": 32}, {"type": "text", "text": "Proof. Noting that $\\hat{A}=U\\hat{\\Sigma}\\hat{V}^{T}$ , we have that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\hat{\\gamma}=1+\\sigma_{t r n}\\hat{v}_{t r n}^{T}\\hat{A}_{t r n}^{\\dag}u=1+\\sigma_{t r n}\\sum_{i=1}^{\\operatorname*{min}(n,d)}\\sigma_{i}(\\hat{A})^{-1}\\hat{a}_{i}b_{i}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Here $\\hat{a}^{T}=\\hat{v}_{t r n}^{T}\\hat{V}$ and $b=U^{T}u$ . $U$ is a uniformly random rotation matrix that is independent of $\\hat{\\Sigma}$ and $\\hat{V}$ . Thus, taking the expectation with respect to $A_{t r n}$ , we get that the expectation is equal to zero. For the variance, let us first consider the case when $c<1$ . For this case, we have that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{V}=\\left[V_{1:d}\\Sigma\\hat{\\Sigma}^{-1}\\right].}\\\\ {\\mu U\\hat{\\Sigma}^{-1}\\;}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Thus, letting $a^{T}=v_{t r n}^{T}V_{1:d}$ , we get that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\hat{\\gamma}=1+\\sum_{i=1}^{d}\\frac{\\sigma_{i}(A)}{\\sigma_{i}^{2}(A)+\\mu^{2}}a_{i}b_{i}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Squaring and taking the expectation, we see that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\gamma^{2}]=1+\\frac{\\sigma_{t r n}^{2}}{n}\\mathbb{E}_{\\lambda\\sim\\mu_{c}}\\left[\\frac{\\lambda}{(\\lambda+\\mu^{2})^{2}}\\right]+o\\left(\\frac{\\sigma_{t r n}^{2}}{n}\\right).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Similarly for $c>1$ , we have that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\gamma^{2}]=1+\\frac{\\sigma_{t r n}^{2}}{d}\\mathbb{E}_{\\lambda\\sim\\mu_{c}}\\left[\\frac{\\lambda}{(\\lambda+\\mu^{2})^{2}}\\right]+o\\left(\\frac{\\sigma_{t r n}^{2}}{d}\\right).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Lemma 17. We have that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\mathrm{Tr}((\\hat{A}_{t r n}^{\\dagger})^{T}\\hat{k}\\hat{k}^{T}\\hat{A}_{t r n}^{\\dagger})\\right]=\\mathbb{E}\\left[\\rho\\right]=\\left\\{\\frac{\\mu^{2}c^{2}+e^{2}+\\mu^{2}c-2c+1}{2\\mu^{4}c\\sqrt{4\\mu^{2}c^{2}+(1-c+\\mu^{2}c)^{2}}}+\\frac{1}{2\\mu^{4}}\\left(1-\\frac{1}{c}\\right)+o(1)\\quad c<1\\mathrm{.~}}\\\\ {\\frac{1-2c+c^{2}+\\mu^{2}c+\\mu^{2}c^{2}}{2\\mu^{4}c\\sqrt{4\\mu^{2}c+(-1+c+\\mu^{2}c)^{2}}}+\\left(1-\\frac{1}{c}\\right)\\frac{1}{2\\mu^{4}}+o(1)\\quad c>1\\mathrm{.~}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and that $\\mathbb{V}(\\rho)=o(1)$ . ", "page_idx": 33}, {"type": "text", "text": "Proof. Here we have that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\rho=\\mathrm{Tr}(\\hat{k}^{T}(\\hat{A}_{t r n}^{T}\\hat{A}_{t r n})^{\\dag}\\hat{k})=\\mathrm{Tr}(u^{T}(\\hat{A}_{t r n}\\hat{A}_{t r n}^{T})^{\\dag}(\\hat{A}_{t r n}\\hat{A}_{t r n}^{T})^{\\dag}u).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We first notice that ", "page_idx": 33}, {"type": "equation", "text": "$$\n(\\hat{A}_{t r n}\\hat{A}_{t r n}^{T})^{\\dagger}(\\hat{A}_{t r n}\\hat{A}_{t r n}^{T})^{\\dagger}=\\hat{U}^{T}\\hat{\\Sigma}^{2}\\hat{U}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Thus using Lemmas 7 and 8, we see that if $c<1$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\rho]=\\frac{\\mu^{2}c^{2}+c^{2}+\\mu^{2}c-2c+1}{2\\mu^{4}c\\sqrt{4\\mu^{2}c^{2}+(1-c+\\mu^{2}c)^{2}}}+\\frac{1}{2\\mu^{4}}\\left(1-\\frac{1}{c}\\right)\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and if $c>1$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\mathbb{E}[\\rho]={\\frac{1}{c}}\\left({\\frac{1-2c+c^{2}+\\mu^{2}c+\\mu^{2}c^{2}}{2\\mu^{4}{\\sqrt{4\\mu^{2}c+(-1+c+\\mu^{2}c)^{2}}}}}+(1-c){\\frac{1}{2\\mu^{4}}}\\right)+\\left(1-{\\frac{1}{c}}\\right){\\frac{1}{\\mu^{4}}}}\\\\ &{\\qquad={\\frac{1-2c+c^{2}+\\mu^{2}c+\\mu^{2}c^{2}}{2\\mu^{4}c{\\sqrt{4\\mu^{2}c+(-1+c+\\mu^{2}c)^{2}}}}}+\\left(1-{\\frac{1}{c}}\\right){\\frac{1}{2\\mu^{4}}}.}\\end{array}}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "The variance being $o(1)$ comes from Lemma 11 again. ", "page_idx": 33}, {"type": "text", "text": "Lemma 18. We have that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{E}_{A_{t r n}}\\left[{\\mathrm{Tr}}({\\hat{h}}^{T}{\\hat{k}}^{T}{\\hat{A}}_{t r n}^{\\dagger})\\right]=0\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and the variance is $o(1)$ . ", "page_idx": 33}, {"type": "text", "text": "Proof. Letting $\\hat{A}=U\\hat{\\Sigma}\\hat{V}^{T}$ , we get that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname{Tr}(\\hat{h}^{T}\\hat{k}^{T}\\hat{A}^{T})=u^{T}U\\hat{\\Sigma}^{-3}\\hat{V}^{T}\\hat{v}_{t r n}^{T}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Then again since $U$ is uniformly random and independent of $\\hat{\\Sigma}$ and $\\hat{V}$ , the expectation is equal to zero. The variance is computed similarly to Lemma 16. ", "page_idx": 33}, {"type": "text", "text": "E.3.5 Step 5: Putting it together ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Lemma 19. We have that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\frac{\\tau}{\\sigma_{t r n}^{2}}\\right]=\\left\\{\\!\\!\\!\\begin{array}{l l}{\\frac{1}{\\sigma_{t r n}^{2}}+\\frac{1}{2}\\left(1+\\mu^{2}c+c-\\sqrt{(1-c+\\mu^{2}c)^{2}+4\\mu^{2}c^{2}}\\right)+o(1)}&{{c<1}}\\\\ {\\frac{1}{\\sigma_{t r n}^{2}}+\\frac{1}{2}\\left(1+\\mu^{2}c+c-\\sqrt{(-1+c+\\mu^{2}c)^{2}+4\\mu^{2}c}\\right)+o(1)}&{{c>1}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and that $\\mathbb{V}(\\tau/\\sigma_{t r n}^{2})=o(1)$ . ", "page_idx": 33}, {"type": "text", "text": "Proof. Using the fact that all of the quantities concentrate, we can use the previous estimates. Specifically, we use that ", "page_idx": 33}, {"type": "equation", "text": "$$\n|\\mathbb{E}[X Y]-\\mathbb{E}[X]\\mathbb{E}[Y]|\\leq{\\sqrt{\\mathbb{V}[X]\\mathbb{V}[Y]}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Thus, since our variances decay, we can use the product of the expectations. Further, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{V}[X Y]|=|\\mathbb{V}[X]\\mathbb{V}[Y]+\\mathbb{E}[X]^{2}\\mathbb{V}[Y]+\\mathbb{E}[Y]^{2}\\mathbb{V}[X]-2\\mathbb{E}[X]\\mathbb{E}[Y]\\mathrm{Cov}(X,Y)+\\mathrm{Cov}(X^{2},Y^{2})-\\mathrm{Cov}(Y^{2})}\\\\ {\\le|\\mathbb{V}[X]\\mathbb{V}[Y]+\\mathbb{E}[X]^{2}\\mathbb{V}[Y]+\\mathbb{E}[Y]^{2}\\mathbb{V}[X]|+2|\\mathbb{E}[X]\\mathbb{E}[Y]|\\sqrt{\\mathbb{V}[X]\\mathbb{V}[Y]}+|\\mathbb{V}[X]\\mathbb{V}[Y]|+|}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Thus, since the variances individually go to 0, we see that the variance of the product also goes to 0. Then using Lemma 15 and 14, we have that if $c<1$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Vert\\hat{t}\\Vert^{2}\\Vert\\hat{k}\\Vert^{2}\\right]=\\frac{1}{2}\\left(1+\\mu^{2}c+c-\\sqrt{(1-c+\\mu^{2}c)^{2}+4\\mu^{2}c^{2}}\\right)+o(1)\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and $\\mathbb{V}(\\|\\hat{t}\\|^{2}\\|\\hat{k}\\|^{2})=o(1)$ . Then since ", "page_idx": 34}, {"type": "equation", "text": "$$\n|\\mathbb{V}[X+Y]|\\leq|\\mathbb{V}[X]+\\mathbb{V}[Y]|+2{\\sqrt{\\mathbb{V}[X]\\mathbb{V}[Y]}}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "we have that using Lemma 16, that if $c<1$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\frac{\\tau}{\\sigma_{t r n}^{2}}\\right]=\\frac{1}{\\sigma_{t r n}^{2}}+\\frac{1}{2}\\left(1+\\mu^{2}c+c-\\sqrt{(1-c+\\mu^{2}c)^{2}+4\\mu^{2}c^{2}}\\right)+o(1)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and that that variance is $o(1)$ . If $c>1$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\frac{\\tau}{\\sigma_{t r n}^{2}}\\right]=\\frac{1}{\\sigma_{t r n}^{2}}+\\frac{1}{2}\\left(1+\\mu^{2}c+c-\\sqrt{(-1+c+\\mu^{2}c)^{2}+4\\mu^{2}c}\\right)+o(1).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Lemma 20. We have that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}_{A_{t r n}}\\left[\\frac{1}{\\sigma_{t r n}^{2}}\\|\\hat{h}\\|^{2}+\\|\\hat{t}\\|^{4}\\rho\\right]=\\left\\{\\frac{c(1+\\sigma_{t r n}^{-2})}{2}\\left(\\frac{\\mu^{2}c+c+1}{\\sqrt{(1-c+\\mu^{2}c)^{2}+4\\mu^{2}c^{2}}}-1\\right)+o(1)\\quad c<1\\medskip}{\\frac{c(1+\\sigma_{t r n}^{-2})}{2}\\left(\\frac{\\mu^{2}c+c+1}{\\sqrt{(1+c+\\mu^{2}c)^{2}+4\\mu^{2}c}}-1\\right)+o(1)\\,\\,\\,\\,\\,c>1}\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and that the variance is $o(1)$ . ", "page_idx": 34}, {"type": "text", "text": "Proof. Similar to Lemma 19, we can multiply the expectations since the variances are small. For $c<1$ , simplifying, we get that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}_{A_{t r n}}\\left[\\frac{1}{\\sigma_{t r n}^{2}}\\|\\hat{h}\\|^{2}+\\|\\hat{t}\\|^{4}\\rho\\right]=\\frac{c(1+\\sigma_{t r n}^{-2})}{2}\\left(\\frac{\\mu^{2}c+c+1}{\\sqrt{(1-c+\\mu^{2}c)^{2}+4\\mu^{2}c^{2}}}-1\\right)+o(1)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and if $c>1$ , we get that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}_{A_{t r n}}\\left[\\frac{1}{\\sigma_{t r n}^{2}}\\|\\hat{h}\\|^{2}+\\|\\hat{t}\\|^{4}\\rho\\right]=\\frac{c(1+\\sigma_{t r n}^{-2})}{2}\\left(\\frac{\\mu^{2}c+c+1}{\\sqrt{(-1+c+\\mu^{2}c)^{2}+4\\mu^{2}c}}-1\\right)+o(1)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and the variance decays since the variances decay individually. ", "page_idx": 34}, {"type": "text", "text": "Lemma 21. We have that ", "text_level": 1, "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{A_{t r n}}\\left[\\|W_{o p t}\\|_{F}^{2}\\right]=\\frac{\\sigma_{t r n}^{4}}{\\tau^{2}}\\left\\{\\frac{c(1+\\sigma_{t r n}^{-2})}{2}\\left(\\frac{\\mu^{2}c+c+1}{\\sqrt{(1-c+\\mu^{2}c)^{2}+4\\mu^{2}c^{2}}}-1\\right)+o(1)\\quad c<1\\right.}\\\\ {\\left.\\frac{c(1+\\sigma_{t r n}^{-2})}{2}\\left(\\frac{\\mu^{2}c+c+1}{\\sqrt{(-1+c+\\mu^{2}c)^{2}+4\\mu^{2}c}}-1\\right)+o(1)\\quad c>1}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and that $\\mathbb{V}(\\|W_{o p t}\\|_{F}^{2})=o(1)$ . ", "page_idx": 34}, {"type": "text", "text": "Proof. Follows immediately from Lemmas 4, 17, 18, and 20. ", "page_idx": 34}, {"type": "text", "text": "Theorem 1 (Generalization Error Formula). Suppose the training data $X_{t r n}$ and test data $X_{t s t}$ satisfy Assumption 3 and the noise $A_{t r n},A_{t s t}$ satisfy Assumption 4. Let $\\mu$ be the regularization parameter. Then for the under-parameterized regime (i.e., $c<1$ ) for the solution $W_{o p t}$ to Problem $^{\\,l}$ , the generalization error or risk given by Equation 2 is given by ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathfrak{L}(c,\\mu)=\\frac{c\\sigma_{t r n}^{2}(\\sigma_{t r n}^{2}+1))}{2d\\tau^{2}}\\frac{1+c+\\mu^{2}c}{\\sqrt{(1-c+\\mu^{2}c)^{2}+4\\mu^{2}c^{2}}}-\\tau^{-2}\\frac{c\\sigma_{t r n}^{2}(\\sigma_{t r n}^{2}+1))}{2d}+\\tau^{-2}\\frac{\\sigma_{t s t}^{2}}{n_{t s t}}+o\\left(\\frac{1}{d}\\right),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{1}{\\tau}=\\frac{2||\\beta^{T}u||}{2+\\sigma_{t r n}^{2}(1+c+\\mu^{2}c-\\sqrt{(1-c+\\mu^{2}c)+4\\mu^{2}c^{2}})}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proof. Rewriting \u03c4\u03b3\u02c6 2 as \u03c4\u03b3\u02c6 2//\u03c3\u03c3t4rn , we can the concentration from Lemmas 16 and 19. Then using Lemma 21 we get the needed result. \u53e3 ", "page_idx": 34}, {"type": "text", "text": "Theorem 6. For the over-parameterized case, we have that the generalization error is given by ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathcal{R}(c,\\mu)=\\tau^{-2}\\left(\\frac{\\sigma_{t s t}^{2}}{N_{t s t}}+\\frac{c\\sigma_{t r n}^{2}(\\sigma_{t r n}^{2}+1))}{2d}\\left(\\frac{1+c+\\mu^{2}c}{\\sqrt{(-1+c+\\mu^{2}c)^{2}+4\\mu^{2}c}}-1\\right)\\right)+o\\left(\\frac{1}{d}\\right),\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. Rewriting \u03c42 as $\\frac{\\hat{\\gamma}^{2}/\\sigma_{t r n}^{4}}{\\tau^{2}/\\sigma_{t r n}^{4}}$ , we can the concentration from Lemmas 16 and 19. Then using Lemma 21 we get the needed result. \u53e3 ", "page_idx": 35}, {"type": "text", "text": "E.4 Proof of Theorem 2 ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Theorem 2 (Under-Parameterized Peak). Let $\\mu\\in\\mathbb{R}_{>0}$ , $\\sigma_{t r n}^{2}=n=d/c$ and $\\sigma_{t s t}^{2}=n_{t s t}$ , and $d$ is sufficiently large, so that the error term $o(1/d)$ is small, then the risk $\\mathcal{R}(c)$ from Theorem $^{\\,l}$ , as $a$ function of c, has a local maximum in the under-parameterized regime at $\\begin{array}{r}{\\dot{c}=\\frac{1}{1+\\mu^{2}}}\\end{array}$ . ", "page_idx": 35}, {"type": "text", "text": "Proof. First, we compute the derivative of the risk. We do so using SymPy and get the following expression. ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\operatorname{i}c\\left(4c\\mu^{2}+\\left(\\mu^{2}-1\\right)\\left(c\\mu^{2}-c+1\\right)-\\left(\\mu^{2}+1\\right)\\sqrt{4c^{2}\\mu^{2}+\\left(c\\mu^{2}-c+1\\right)^{2}}\\right)}{d\\left(4c^{2}\\mu^{2}+\\left(c\\mu^{2}-c+1\\right)^{2}\\right)\\left(c\\mu^{2}+c-\\sqrt{4c^{2}\\mu^{2}+\\left(c\\mu^{2}-c+1\\right)^{2}}+1\\right)^{2}}}\\\\ &{+\\frac{2c\\left(\\left(\\mu^{2}+1\\right)\\left(4c^{2}\\mu^{2}+\\left(c\\mu^{2}-c+1\\right)^{2}\\right)-\\left(4c\\mu^{2}+\\left(\\mu^{2}-1\\right)\\left(c\\mu^{2}-c+1\\right)\\right)\\right)\\left(c\\mu^{2}+c+1\\right)\\right)}{d\\left(4c^{2}\\mu^{2}+\\left(c\\mu^{2}-c+1\\right)^{2}\\right)^{\\frac{3}{2}}\\left(c\\mu^{2}+c-\\sqrt{4c^{2}\\mu^{2}+\\left(c\\mu^{2}-c+1\\right)^{2}}+1\\right)^{2}}}\\\\ &{+\\frac{2\\left(\\left(4c^{2}\\mu^{2}+\\left(c\\mu^{2}-c+1\\right)^{2}\\right)^{3}\\left(c\\mu^{2}+c-\\sqrt{4c^{2}\\mu^{2}+\\left(c\\mu^{2}-c+1\\right)^{2}}+1\\right)^{2}\\right)}{M\\left(4c^{2}\\mu^{2}+\\left(c\\mu^{2}-c+1\\right)^{2}\\right)^{\\frac{5}{2}}\\left(c\\mu^{2}+c-\\sqrt{4c^{2}\\mu^{2}+\\left(c\\mu^{2}-c+1\\right)^{2}}+1\\right)^{7}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "We can then compute the limit as $c\\to0^{+}$ . Again using SymPy we see that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{c\\rightarrow0^{+}}\\frac{\\partial}{\\partial c}\\mathcal{R}(c,\\mu^{2};\\sigma_{t r n}^{2}=d/c)=\\frac{1}{d}>0.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Let ", "page_idx": 35}, {"type": "equation", "text": "$$\nT(c,\\mu)=c^{2}\\mu^{4}+2c^{2}\\mu^{2}+c^{2}+2c\\mu^{2}-2c+1\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "To find the critical point, we shall write the derivative as one fraction of the form ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\partial_{c}\\mathcal{R}(c,\\mu)=-2\\frac{(c\\mu^{2}+c-1)P(c,\\mu,d,T)}{Q(c,\\mu,d,T)}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Here we see that ", "page_idx": 35}, {"type": "equation", "text": "$$\nP(c,\\mu,d,T)=c^{2}\\mu^{4}+2c^{2}\\mu^{2}+c^{2}+2c\\mu^{2}+1-(c\\mu+c+1)\\sqrt{T}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and ", "page_idx": 35}, {"type": "equation", "text": "$$\nQ(c,\\mu,d,T)=d(c\\mu^{2}+c+1-\\sqrt{T})^{2}T^{3/2}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "We can see that the numerator is zero at $c=(1+\\mu^{2})^{-1}$ . To evaluate the denominator at this point, we first get that ", "page_idx": 35}, {"type": "equation", "text": "$$\nT((1+\\mu^{2})^{-1},\\mu^{2})=\\frac{4\\mu^{2}}{\\mu^{2}+1}<4\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Thus, we see that the denominator is ", "page_idx": 36}, {"type": "equation", "text": "$$\nd\\left(2-\\frac{2\\mu}{\\sqrt{\\mu^{2}+1}}\\right)^{2}\\left(\\frac{4\\mu^{2}}{\\mu^{2}+1}\\right)^{3/2}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Since $T((1+\\mu^{2})^{-1},\\mu^{2}<4$ , we have that ", "page_idx": 36}, {"type": "equation", "text": "$$\n2-\\frac{2\\mu}{\\sqrt{\\mu^{2}+1}}>0\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Hence the denominator is non-zero. Thus, we see that $c=(1+\\mu^{2})^{-1}$ is a critical point. ", "page_idx": 36}, {"type": "text", "text": "Next we want to show that this point is a local maximum. To do so, we compute the second derivative and evaluate at $c=(1+\\mu^{2})^{-\\dot{1}}$ . Using SymPy, we get that the value of the second derivative at this point is ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\frac{(\\mu^{2}+1)^{4}\\cdot(4\\mu^{3}+3\\mu-4\\mu^{2}\\sqrt{\\mu^{2}+1}-\\sqrt{\\mu^{2}+1})}{8d\\cdot\\mu^{3}\\cdot(\\mu^{2}-\\mu\\sqrt{\\mu^{2}+1}+1)^{3}}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "To see that it is a maximum, we need to show that the above is negative. We begin by showing that the denominator is positive. Since $8d\\mu^{3}>0$ , we only need to look at the second term. Using arithmetic mean and geometric mean inequality, we see that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mu{\\sqrt{\\mu^{2}+1}}={\\sqrt{\\mu^{2}(\\mu^{2}+1)}}\\leq{\\frac{\\mu^{2}+\\mu^{2}+1}{2}}=\\mu^{2}+{\\frac{1}{2}}<\\mu^{2}+1\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Hence the denominator is positive. To show that the numerator is negative, we have the following ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mu^{3}+3\\mu-4\\mu^{2}\\sqrt{\\mu^{2}+1}-\\sqrt{\\mu^{2}+1}<0\\iff4\\mu^{2}+3<(4\\mu^{2}+1)\\frac{\\sqrt{\\mu^{2}+1}}{\\mu}}\\\\ {\\iff16\\mu^{4}+9+24\\mu^{2}<(16\\mu^{4}+1+8\\mu^{2})\\cdot\\left(1+\\frac{1}{\\mu^{2}}\\right)}\\\\ {\\iff16\\mu^{2}+8<\\frac{1}{\\mu^{2}}(16\\mu^{4}+8\\mu^{2}+1)}\\\\ {\\iff0<\\frac{1}{\\mu^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Where we are allowed to square both sides because both quantities are non-negative. Thus, we see get the needed result. \u53e3 ", "page_idx": 36}, {"type": "text", "text": "E.5 Proof of Theorem 3 ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Theorem 3 $\\iiint_{O p t}\\!\\left\\|\\boldsymbol{F}\\right\\|$ Peak). If $\\sigma_{t s t}=\\sqrt{n_{t s t}},\\,\\sigma_{t r n}=\\sqrt{n}$ and $\\mu$ is such that $p(\\mu)<0,$ , then for fixed n that is sufficiently large enough, we have that E $[\\|W_{o p t}\\|_{F}]$ versus $c=d/n$ curve has a local maximum in the under-parameterized regime at $c=(\\mu^{2}+1)^{-1}$ . ", "page_idx": 36}, {"type": "text", "text": "Proof. Here we note that the expression for the norm of $W_{o p t}$ is given by Lemma 21. We follow the same proof structure as Theorem 2. Differentiating with respect to $c$ , we see that the numerator is if the form ", "page_idx": 36}, {"type": "equation", "text": "$$\n(c\\mu^{2}+c-1)P(c,\\mu,T)\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "When the denominator is ", "page_idx": 36}, {"type": "equation", "text": "$$\n(c\\mu^{2}+c+1-\\sqrt(T))^{7}T^{7/2}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Where is as is in the \u221aproof of Theorem 2. Again at when $c=1/(\\mu^{2}+1)$ . We see that the denominator is positive because $\\sqrt{T}<2$ . Hence again, we have a critical point $c=(1+\\mu^{2})^{-1}$ . \u53e3 ", "page_idx": 36}, {"type": "text", "text": "E.6 Proof of Theorem 5 ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Theorem 5 (Training Error). Let $\\tau$ be as in Theorem 1. The training error for $c<1$ is given by ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{A_{t r n}}[\\|X_{t r n}-W_{o p t}(X_{t r n}+A_{t r n})\\|_{F}^{2}]=\\tau^{-2}\\left(\\sigma_{t r n}^{2}\\left(1-c\\cdot T_{1}\\right)+\\sigma_{t r n}^{4}T_{2}\\right)+o(1),}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "wher $;T_{1}={\\frac{\\mu^{2}}{2}}\\left({\\frac{1+c+\\mu^{2}c}{\\sqrt{(1-c+\\mu^{2}c)^{2}+4\\mu^{2}c^{2}}}}-1\\right)+{\\frac{1}{2}}+{\\frac{1+\\mu^{2}c-\\sqrt{(1-c+\\mu^{2}c)^{2}+4c^{2}\\mu^{2}}}{2c}},$ and ", "page_idx": 37}, {"type": "equation", "text": "$$\nT_{2}=(\\mu^{2}c+c-1-\\sqrt{(1-c+\\mu^{2}c)^{2}+4c^{2}\\mu^{2}})^{2}\\left(\\frac{\\mu^{2}c+c+1}{2\\sqrt{(1-c+\\mu^{2}c)^{2}+4c^{2}\\mu^{2}}}+\\frac12\\right).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. Note that we have: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{A_{t r n}}\\left[\\frac{\\|X_{t r n}-W_{o p t}Y_{t r n}\\|_{F}^{2}}{n}\\right]=\\frac{1}{n}\\mathbb{E}_{A_{t r n}}\\left[\\|X_{t r n}-W_{o p t}(X_{t r n}+A_{t r n}))\\|_{F}^{2}\\right]}\\\\ &{\\phantom{=}=\\frac{1}{n}\\mathbb{E}[\\|X_{t r n}-W_{o p t}X_{t r n}\\|^{2}]+\\frac{1}{n}\\mathbb{E}[\\|W_{o p t}A_{t r n}\\|^{2}]}\\\\ &{\\phantom{=}+\\frac{2}{n}\\mathbb{E}\\left[\\mathrm{Tr}((X_{t r n}-W_{o p t}X_{t r n})^{T}W_{o p t}A_{t r n})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "First, by Lemma 2, we have $\\begin{array}{r}{X_{t r n}\\mathrm{~-~}W_{o p t}X_{t r n}\\;=\\;\\frac{\\hat{\\gamma}}{\\hat{\\tau}}X_{t r n}}\\end{array}$ . Then, $\\mathbb{E}[\\|X_{t r n}\\textrm{--}W_{o p t}X_{t r n}\\|^{2}]\\;=$ $\\begin{array}{r}{\\frac{\\hat{\\gamma}^{2}}{\\hat{\\tau}^{2}}\\mathbb{E}[\\|X_{t r n}\\|^{2}]=\\frac{\\hat{\\gamma}^{2}\\sigma_{t r n}^{2}}{\\hat{\\tau}^{2}}}\\end{array}$ . Then, let us look at the $\\mathbb{E}_{A_{t r n}}[||W_{o p t}A_{t r n}||_{F}^{2}]$ term. ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\lambda_{x}}\\lefteqn{\\Vert\\mathbf{B}_{\\theta\\theta\\theta}^{\\theta}\\mathbf{A}_{\\theta+1}\\Vert_{\\mathrm{L}^{2}}^{2}\\mathrm{L}\\mathrm{T}[\\left({\\mathbf{J}}_{\\theta}^{T}\\mathrm{B}_{\\theta}^{T}\\right)\\mathrm{R}_{+}^{2}\\mathrm{L}\\mathrm{R}_{+}^{2},\\lambda_{x}^{\\prime}]}}\\\\ &{=\\frac{\\theta^{2}\\tilde{\\mu}_{x}^{2}\\tilde{\\mu}_{y}^{2}\\mathrm{L}[\\left(\\lambda_{x}^{\\prime}\\mathrm{R}_{+}^{2}\\mathrm{R}_{+}^{2}\\mathrm{R}_{+}^{2}\\mathrm{R}_{+}^{2}\\mathrm{R}_{+}^{2}\\right)\\mathrm{L}_{+}^{2}]}\\\\ &{+\\frac{\\theta^{2}\\tilde{\\mu}_{y}^{2}\\tilde{\\mu}_{y}^{2}}{3}\\mathrm{L}[\\left(\\lambda_{y}^{\\prime}\\mathrm{R}_{+}^{2}\\mathrm{R}_{+}^{2}\\mathrm{R}_{+}^{2}\\mathrm{R}_{+}^{2}\\mathrm{R}_{+}^{2}\\mathrm{A}_{y}^{\\prime}\\right)\\mathrm{L}_{+}^{2}]}\\\\ &{+\\frac{\\theta^{2}\\tilde{\\mu}_{y}^{2}\\tilde{\\mu}_{y}^{2}}{3}\\mathrm{L}[\\left(\\lambda_{y}^{\\prime}\\mathrm{A}_{\\theta}^{T}\\right)\\mathrm{R}_{+}^{2}\\mathrm{R}_{+}^{2}\\mathrm{R}_{+}^{2}]}\\\\ &{+\\frac{\\theta^{2}\\tilde{\\mu}_{x}^{2}\\tilde{\\mu}_{y}^{2}}{3}\\mathrm{L}[\\left(\\lambda_{x}^{\\prime}\\mathrm{R}_{+}^{2}\\mathrm{R}_{+}^{2}\\mathrm{R}_{+}^{2}\\mathrm{A}_{y}^{\\prime}\\right)\\mathrm{L}_{+}^{2}\\mathrm{R}_{+}^{2}]}\\\\ &{+\\frac{\\theta^{2}\\tilde{\\mu}_{y}^{2}}{3}\\mathrm{L}[\\left(\\lambda_{y}^{\\prime}\\mathrm{R}_{+}^{2}\\mathrm{R}_{+}^{2}\\mathrm{R}_{+}^{2}\\mathrm{A}_{y}^{\\prime}\\right)\\mathrm{L}_{ \n$$$$\n\\begin{array}{r l}{\\Phi_{i}}&{=-\\operatorname*{inf}_{\\{1,\\ell\\}}-\\lambda_{i}\\pi_{1}(k_{\\perp},\\ensuremath{\\mathbb{R}}_{T})\\Phi_{i}}\\\\ &{+\\frac{\\alpha_{i}}{2}\\mu_{1}\\mu_{2}^{2}\\mathbf{R}[\\alpha_{i}(k_{\\perp},\\ensuremath{\\mathbb{R}}_{T})^{2}\\bar{\\Phi}_{i}]}\\\\ &{+\\frac{\\alpha_{i}}{2}\\frac{\\beta_{i}}{2}\\mu_{2}^{3}\\mathbf{R}[\\alpha_{i}(k_{\\perp},\\ensuremath{\\mathbb{R}}_{T})^{2}\\bar{\\Phi}_{i}]}\\\\ &{+\\frac{\\alpha_{i}}{2}\\frac{\\beta_{i}}{2}\\mu_{1}^{2}\\mathbf{R}[\\alpha_{i}(k_{\\perp},\\ensuremath{\\mathbb{R}}_{T})^{2}\\bar{\\Phi}_{i}]}\\\\ &{=\\frac{\\alpha_{i}}{2}\\mu_{1}^{2}\\frac{\\beta_{i}}{2}\\mathbf{R}[\\alpha_{i}(k_{\\perp},\\ensuremath{\\mathbb{R}}_{T})^{2}\\bar{\\Phi}_{i}]}\\\\ &{+\\frac{\\alpha_{i}}{2}\\frac{\\beta_{i}}{2}\\mu_{2}^{3}\\mathbf{R}[\\alpha_{i}(k_{\\perp},\\ensuremath{\\mathbb{R}}_{T})^{2}\\bar{\\Phi}_{i}]}\\\\ &{+\\frac{\\alpha_{i}}{2}\\frac{\\beta_{i}}{2}\\mu_{1}^{2}\\mathbf{R}[\\alpha_{i}(k_{\\perp},\\ensuremath{\\mathbb{R}}_{T})^{2}\\bar{\\Phi}_{i}]}\\\\ &{+\\frac{\\alpha_{i}}{2}\\frac{\\beta_{i}}{2}\\mu_{2}^{3}\\mathbf{R}[\\alpha_{i}(k_{\\perp},\\ensuremath{\\mathbb{R}}_{T})^{2}\\bar{\\Phi}_{i}]}\\\\ &{+\\frac{\\alpha_{i}}{2}\\frac{\\beta_{i}}{2}\\mu_{1}^{3}\\mathbf{R}[\\alpha_{i}(k_{\\perp},\\ensuremath{\\mathbb{R}}_{T})^{2}\\bar{\\Phi}_{i}]}\\\\ &{+\\frac{\\alpha_{i}}{2}\\frac{\\beta_{i}}{2}\\mu_{2}^{3}\\mathbf{R}[\\alpha_{i}(k_{\\perp},\\ensuremath{\\mathbb{R}}_{T} \n$$", "text_format": "latex", "page_idx": 37}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Then, we look at the $\\operatorname{Tr}((X_{t r n}\\,-\\,W_{o p t}X_{t r n})^{T}W_{o p t}A_{t r n})$ term. By Lemma 2, we have $X_{t r n}\\perp$ $\\begin{array}{r}{W_{o p t}X_{t r n}=\\frac{\\hat{\\gamma}}{\\hat{\\tau}}X_{t r n}}\\end{array}$ . Then, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{\\dot{\\gamma}}{\\hbar}\\mathrm{T}\\left(X_{t=\\mathrm{b}}^{T}W_{q\\neq\\varepsilon}A_{t r\\alpha}\\right)=\\frac{\\dot{\\gamma}}{\\hbar}\\mathrm{T}\\left(X_{t=\\mathrm{b}}^{T}\\left(\\frac{\\sigma_{t\\neq\\varepsilon}\\dot{\\gamma}_{\\varepsilon}}{\\dot{\\gamma}}i\\hbar\\dot{\\gamma}+\\frac{\\sigma_{t\\varepsilon}^{2}\\vert\\dot{\\gamma}\\vert^{2}}{\\dot{\\gamma}}w_{t}^{2}k^{\\prime}\\hat{A}_{t r\\alpha}\\right)A_{t r\\alpha}\\right)}\\\\ &{=\\frac{\\sigma_{t\\varepsilon}\\dot{\\gamma}^{2}}{\\hbar^{2}\\hbar^{2}}\\mathrm{Tr}\\left(X_{t=\\mathrm{b}}^{T}\\hbar\\hat{A}_{t r\\alpha}\\right)}\\\\ &{\\quad+\\frac{\\sigma_{t\\varepsilon}^{2}\\dot{\\gamma}_{\\varepsilon}^{2}}{\\hbar^{2}}\\mathrm{Tr}\\left(X_{t=\\mathrm{b}}^{T}k^{T}\\hat{A}_{t r\\alpha}^{\\dagger}A_{t r\\alpha}\\right)}\\\\ &{=\\frac{\\sigma_{t\\varepsilon}\\dot{\\gamma}^{2}}{\\hbar^{2}}\\mathrm{Tr}\\left(\\sigma_{t\\varepsilon}n_{\\mathrm{ra}}^{2}k^{\\prime}\\hat{A}_{t r\\alpha}^{\\dagger}A_{t r\\alpha}\\right)}\\\\ &{\\quad+\\frac{\\sigma_{t\\varepsilon}^{2}\\dot{\\gamma}_{\\varepsilon}^{2}}{\\hbar^{2}}\\mathrm{Tr}\\left(\\sigma_{t\\varepsilon}n_{\\mathrm{ra}}\\nu_{t}^{2}\\hat{A}_{t r\\alpha}^{\\dagger}\\right)\\!\\!-\\!\\frac{\\dot{\\gamma}}{\\hbar^{2}\\hbar^{2}}\\mathrm{Tr}\\left(\\right.}\\\\ &{=\\frac{\\sigma_{t\\varepsilon}^{2}\\dot{\\gamma}_{\\varepsilon}^{2}}{\\hbar^{2}}\\mathrm{Tr}\\left(\\hat{\\gamma}_{\\varepsilon}^{T}\\hat{A}_{t r\\alpha}^{\\dagger}A_{t r\\alpha}\\right)}\\\\ &{\\quad+\\frac{\\sigma_{t\\varepsilon}^{2}\\dot{\\gamma}_{\\varepsilon}^{2}}{\\hbar^{2}}\\mathrm{Tr}\\left(\\hat{\\gamma}_{\\varepsilon}^{T}\\hat{A}_{t r\\alpha}^{\\dagger}A_{t r\\alpha}\\right)}\\\\ &{=\\frac{\\sigma_{t\\varepsilon}^{2}\\dot{\\gamma}_{\\varepsilon}^{2}}{\\hbar^{2}}\\mathrm{Tr}\\left(\\hat{\\gamma}_{\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "In conclusion, we have the training error: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{A_{t r n}}\\left[\\frac{\\|X_{t r n}-W_{o p t}Y_{t r n}\\|_{F}^{2}}{n}\\right]=\\frac{\\hat{\\gamma}^{2}\\sigma_{t r n}^{2}}{n\\hat{\\tau}^{2}}+\\frac{\\sigma_{t r n}^{2}\\hat{\\gamma}^{2}}{n\\hat{\\tau}^{2}}\\mathbb{E}[\\mathrm{Tr}(\\hat{v}_{t r n}^{T}\\hat{A}_{t r n}^{\\dagger}A_{t r n}A_{t r n}^{T}(\\hat{A}_{t r n}^{\\dagger})^{T}\\hat{v}_{t r n}^{T})]}\\\\ &{\\phantom{\\quad}+\\frac{\\sigma_{t r n}^{4}\\|\\hat{t}\\|^{4}}{n\\hat{\\tau}^{2}}\\mathbb{E}[\\mathrm{Tr}(u^{T}(\\hat{A}_{t r n}^{\\dagger})^{T}\\hat{A}_{t r n}^{\\dagger}A_{t r n}A_{t r n}^{T}(\\hat{A}_{t r n}^{\\dagger})^{T}\\hat{A}_{t r n}^{\\dagger}u)]}\\\\ &{\\phantom{\\quad}+2\\frac{\\sigma_{t r n}^{2}\\hat{\\gamma}^{2}}{n\\hat{\\tau}^{2}}\\mathbb{E}\\left[\\mathrm{Tr}\\left(\\hat{v}_{t r n}^{T}\\hat{A}_{t r n}^{\\dagger}A_{t r n}v_{t r n}\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Now we estimate the above terms using random matrix theory. Here we focus on the $c<1$ case. For $c<1$ , we note that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\hat{A}_{t r n}^{\\dagger}A_{t r n}A_{t r n}^{T}(\\hat{A}_{t r n}^{\\dagger})^{T}=\\hat{V}\\hat{\\Sigma}^{-1}\\Sigma\\Sigma^{T}\\hat{\\Sigma}^{-1}\\hat{V}^{T}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Thus, for $c<1$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\hat{v}_{t r n}^{T}\\hat{A}_{t r n}^{\\dagger}A_{t r n}A_{t r n}^{T}(\\hat{A}_{t r n}^{\\dagger})^{T}\\hat{v}_{t r n}=\\sum_{i=1}^{d}a_{i}^{2}\\frac{\\sigma_{i}(A)^{4}}{(\\sigma_{i}(A)^{2}+\\mu^{2})^{2}}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $a^{T}=v_{t r n}^{T}V_{1:d}$ . Taking the expectation, and using Lemma 9 we get that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{L}_{A_{t r n}}\\left[\\hat{v}_{t r n}^{T}\\hat{A}_{t r n}^{\\dagger}A_{t r n}A_{t r n}^{T}(\\hat{A}_{t r n}^{\\dagger})^{T}\\hat{v}_{t r n}\\right]=}\\\\ &{\\cdot\\left(\\frac{1}{2}+\\frac{1+\\mu^{2}c-\\sqrt{(1-c+\\mu^{2}c)^{2}+4c^{2}\\mu^{2}}}{2c}+\\mu^{2}\\left(\\frac{1+c+\\mu^{2}c}{2\\sqrt{(1-c+c\\mu^{2})^{2}+4c^{2}\\mu^{2}}}-\\frac{1}{2}\\right)\\right)+o(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Using Lemma 11, we see that the variance is $o(1)$ . Similarly, we have that ", "page_idx": 38}, {"type": "equation", "text": "$$\n(\\hat{A}_{t r n}^{\\dagger})^{T}\\hat{A}_{t r n}^{\\dagger}A_{t r n}A_{t r n}^{T}(\\hat{A}_{t r n}^{\\dagger})^{T}\\hat{A}_{t r n}^{\\dagger}=U\\hat{\\Sigma}^{-2}\\Sigma\\Sigma^{T}\\hat{\\Sigma}^{-2}U^{T}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Thus, again, using a similar argument, we see that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{\\xi}_{A_{t r n}}\\left[\\mathrm{Tr}(u^{T}(\\hat{A}_{t r n}^{\\dagger})^{T}\\hat{A}_{t r n}^{\\dagger}A_{t r n}A_{t r n}^{T}(\\hat{A}_{t r n}^{\\dagger})^{T}\\hat{A}_{t r n}^{\\dagger}u)\\right]=\\frac{1+c+\\mu^{2}c}{2\\sqrt{(1-c+c\\mu^{2})^{2}+4c^{2}\\mu^{2}}}-\\frac{1}{2}+o(1)\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "and again using Lemma 11, the variance is $o(1)$ . Finally, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{A}_{t r n}^{\\dagger}A_{t r n}=\\hat{V}\\hat{\\Sigma}^{-1}\\Sigma V.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Thus, ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathrm{Tr}(\\hat{v}_{t r n}^{T}\\hat{A}_{t r n}^{\\dagger}A_{t r n}v_{t r n}=\\sum_{i=1}^{d}a_{i}^{2}\\frac{\\sigma_{i}(A)^{2}}{\\sigma_{i}(A)^{2}+\\mu^{2}}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Thus, using Lemma 9, we get that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathbb{E}_{A_{t r n}}\\left[\\mathrm{Tr}(\\hat{v}_{t r n}^{T}\\hat{A}_{t r n}^{\\dagger}A_{t r n}v_{t r n}\\right]=\\frac{1}{2}+\\frac{1+\\mu^{2}c-\\sqrt{(1-c+\\mu^{2}c)^{2}+4c^{2}\\mu^{2}}}{2c}+o(1)\\right]\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and using Lemma 11, the variance is $o(1)$ . Then, similar to the proof of Theorem 1, we can simplify the above expression to get the final result. ", "page_idx": 39}, {"type": "text", "text": "E.7 Proof of Proposition 1 ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Proposition 1 (Optimal $\\sigma_{t r n}$ ). The optimal value of $\\sigma_{t r n}^{2}$ for $c<1$ is given by ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\sigma_{t r n}^{2}=\\frac{\\sigma_{t s t}^{2}d[2c(\\mu^{2}+1)^{2}-2T(c\\mu^{2}+c+1)+2(c\\mu^{2}-2c+1)]+N_{t s t}(\\mu^{2}c^{2}+c^{2}+1-T)}{N_{t s t}(c^{3}(\\mu^{2}+1)^{2}-T(\\mu^{2}c^{2}+c^{2}-1)-2c^{2}-1)}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Proof. Let $\\sigma:=\\sigma_{t r n}^{2}$ and ", "page_idx": 39}, {"type": "equation", "text": "$$\nF=\\tau^{-2}\\left(\\frac{\\sigma_{t s t}^{2}}{N_{t s t}}+\\frac1d(\\sigma\\|\\hat{h}\\|_{2}^{2}+\\sigma^{2}\\|\\hat{t}\\|_{2}^{4}\\rho)\\right).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Notice that only $\\tau$ is a function of $\\sigma,\\,\\|\\hat{h}\\|_{2}^{2},\\,\\|\\hat{t}\\|_{2}^{2}$ , and $\\|\\hat{k}\\|_{2}^{2}$ are all functions of $\\mu$ . Then ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial{\\cal F}}{\\partial\\sigma}=\\tau^{-2}\\frac{1}{d}(\\|\\hat{h}\\|_{2}^{2}+2\\sigma\\|\\hat{t}\\|_{2}^{4}\\rho)-2\\tau^{-3}\\frac{\\partial\\tau}{\\partial\\sigma}\\left(\\frac{\\sigma_{t s t}^{2}}{N_{t s t}}+\\frac{1}{d}\\left(\\sigma\\|\\hat{h}\\|_{2}^{2}+\\sigma^{2}\\|\\hat{t}\\|_{2}^{4}\\rho\\right)\\right)}\\\\ {\\displaystyle\\qquad=\\tau^{-2}\\frac{1}{d}(\\|\\hat{h}\\|_{2}^{2}+2\\sigma\\|\\hat{t}\\|_{2}^{4}\\rho)-2\\tau^{-3}\\|\\hat{t}\\|_{2}^{2}\\|\\hat{k}\\|_{2}^{2}\\left(\\frac{\\sigma_{t s t}^{2}}{N_{t s t}}+\\frac{1}{d}(\\sigma\\|\\hat{h}\\|_{2}^{2}+\\sigma^{2}\\|\\hat{t}\\|_{2}^{4}\\rho)\\right)}\\\\ {\\displaystyle\\qquad=\\tau^{-2}\\left(\\frac{1}{d}(\\|\\hat{h}\\|_{2}^{2}+2\\sigma\\|\\hat{t}\\|_{2}^{4}\\rho)-2\\tau^{-1}\\|\\hat{t}\\|_{2}^{2}\\|\\hat{k}\\|_{2}^{2}\\left(\\frac{\\sigma_{t s t}^{2}}{N_{t s t}}+\\frac{1}{d}(\\sigma\\|\\hat{h}\\|_{2}^{2}+\\sigma^{2}\\|\\hat{t}\\|_{2}^{4}\\rho)\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "The optimal $\\sigma^{*}$ satisfies $\\scriptstyle{\\frac{\\partial F}{\\partial\\sigma}}|_{\\sigma=\\sigma^{*}}\\;=\\;0$ . Thus, we can solve the equation ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\tau^{-2}=0\\quad\\mathrm{~or~}\\quad\\frac{1}{d}(\\|\\hat{h}\\|_{2}^{2}+2\\sigma\\|\\hat{t}\\|_{2}^{4}\\rho)-2\\tau^{-1}\\|\\hat{t}\\|_{2}^{2}\\|\\hat{k}\\|_{2}^{2}\\left(\\frac{\\sigma_{t s t}^{2}}{N_{t s t}}+\\frac{1}{d}(\\sigma\\|\\hat{h}\\|_{2}^{2}+\\sigma^{2}\\|\\hat{t}\\|_{2}^{4}\\rho)\\right).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Let $\\begin{array}{r}{\\alpha:=\\|\\widehat{t}\\|_{2}^{2}\\|\\widehat{k}\\|_{2}^{2},\\delta:=d\\frac{\\sigma_{t s t}^{2}}{N_{t s t}}}\\end{array}$ \u03c3tst. Then ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\tau^{-2}=0\\implies\\sigma=-\\frac{1}{\\lVert t\\rVert_{2}^{2}\\lVert k\\rVert_{2}^{2}}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Notice that $\\sigma<0$ implies $\\sigma_{t r n}$ is an imaginary number, something we don\u2019t want. Thus, we look at the other expression. ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{0=\\displaystyle\\frac{1}{d}(\\|\\hat{h}\\|_{2}^{2}+2\\sigma\\|\\hat{t}\\|_{2}^{4}\\rho)-2\\tau^{-1}\\|\\hat{t}\\|_{2}^{2}\\|k\\|_{2}^{2}\\left(\\frac{\\sigma_{t s t}^{2}}{N_{t s t}}+\\frac{1}{d}(\\sigma\\|\\hat{h}\\|_{2}^{2}+\\sigma^{2}\\|\\hat{t}\\|_{2}^{4}\\rho)\\right)}}&{{}}\\\\ {{\\quad=\\displaystyle\\frac{1}{d}(\\|\\hat{h}\\|_{2}^{2}+2\\sigma\\|\\hat{t}\\|_{2}^{4}\\rho)-2\\tau^{-1}\\alpha\\left(\\frac{\\delta}{d}+\\frac{1}{d}(\\sigma\\|\\hat{h}\\|_{2}^{2}+\\sigma^{2}\\|\\hat{t}\\|_{2}^{4}\\rho)\\right).}}&{{[\\alpha=\\|\\hat{t}\\|_{2}^{2}\\|\\hat{k}\\|_{2}^{2}]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Then multiplying through by $d$ and $\\tau$ ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0=(1+\\alpha\\sigma)(\\|\\hat{h}\\|_{2}^{2}+2\\sigma\\|\\hat{t}\\|_{2}^{4}\\rho)-2\\alpha(\\delta+\\sigma\\|\\hat{h}\\|_{2}^{2}+\\sigma^{2}\\|\\hat{t}\\|_{2}^{4}\\rho)}&{\\left[\\tau=1+\\alpha\\sigma\\right]}\\\\ &{\\quad=\\|\\hat{h}\\|_{2}^{2}+2\\|\\hat{t}\\|_{2}^{4}\\rho\\sigma+\\alpha\\|\\hat{h}\\|_{2}^{2}\\sigma+2\\alpha\\|\\hat{t}\\|_{2}^{4}\\rho\\sigma^{2}-2\\alpha\\delta-2\\alpha\\|\\hat{h}\\|_{2}^{2}\\sigma-2\\alpha\\|\\hat{t}\\|_{2}^{4}\\rho\\sigma^{2}}\\\\ &{\\quad=\\|\\hat{h}\\|_{2}^{2}+2\\|\\hat{t}\\|_{2}^{4}\\rho\\sigma+\\alpha\\|\\hat{h}\\|_{2}^{2}\\sigma-2\\alpha\\delta-2\\alpha\\|\\hat{h}\\|_{2}^{2}\\sigma.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Then solving for $\\sigma$ , we get that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\sigma=\\frac{2\\alpha\\delta-\\|\\hat{h}\\|^{2}}{2\\|t\\|^{4}\\rho-\\alpha\\|\\hat{h}\\|^{2}}=\\frac{2d\\|\\hat{t}\\|_{2}^{2}\\|\\hat{k}\\|_{2}^{2}\\sigma_{t s t}^{2}-\\|\\hat{h}\\|^{2}N_{t s t}}{N_{t s t}(2\\|\\hat{t}\\|_{2}^{4}\\rho-\\|\\hat{t}\\|_{2}^{2}\\|\\hat{k}\\|_{2}^{2}\\|\\hat{h}\\|_{2}^{2})}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Then we use the random matrix theory lemmas to estimate this quantity. ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "F Proof of low-rank case ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Similar to the proof in [37], we conducted the low rank case. ", "page_idx": 41}, {"type": "text", "text": "We begin by defining some notation. Let $\\hat{X}_{t r n}=U\\Sigma_{t r n}V_{t r n}^{T}$ . Here $U$ is $d\\times r$ with $U^{T}U=I$ , $\\Sigma_{t r n}$ is $r\\times r$ , and $V_{t r n}$ is $r\\times(d+N)$ . All of the following matrices are full rank. ", "page_idx": 41}, {"type": "text", "text": "1. $\\hat{X}_{t r n}$ and $\\hat{X}_{t s t}$ is $d\\times(d+N)$ with rank $r$ . $\\hat{X}_{t r n}=[X_{t r n}\\quad0]$   \n2. $\\hat{X}_{t r n}=U\\Sigma V$ , by the singular value decomposition. Let $\\hat{X}_{t s t}=U L$ .   \n3. $U$ is $d\\times r$ with $U^{T}U=I_{r\\times r}\\mathrm{\\boldmath~\\backslash~}$ $V^{T}$ is is $r\\times(d+N)$ .   \n4. $\\Sigma_{t r n}$ is $r\\times r$ , with rank $r$ .   \n5. $\\begin{array}{r}{\\hat{A}_{t r n}=[A_{t r n}\\quad\\mu I]}\\end{array}$ .   \n6. $\\hat{A}_{t r n}$ is $d\\times(N+d)$ with rank $d$ .   \n7. $\\hat{A}_{t r n}^{\\dagger}\\hat{A}_{t r n}$ is $(N+d)\\times(N+d)$ .   \n8. $H$ is $r\\times(N+d)$ , with rank $r$ .   \n9. $K$ is $(N+d)\\times r$ , with rank $r$ .   \n10. $Z$ is $r\\times r$ , with rank $r$ .   \n11. $H_{1}$ is $r\\times r$ , with rank $r$ .   \n12. $\\hat{A}_{t r n}=\\eta_{t r n}\\hat{U}\\hat{\\Sigma}\\hat{V}^{T}$ .   \n13. $\\hat{U}$ is $d\\times d$ unitary.   \n14. $\\hat{\\Sigma}$ is $d\\times d$ . ", "page_idx": 41}, {"type": "text", "text": "For rank $r$ data and $r<N$ , with $\\begin{array}{r}{c=\\frac{d}{N}}\\end{array}$ , the following is true. ", "page_idx": 41}, {"type": "text", "text": "1. We denote the minimum norm linear denoiser $W_{o p t}$ by just $W$ in this subsection. It is given by $W_{o p t}=-U\\Sigma_{t r n}H_{1}^{-1}K^{T}\\hat{A}_{t r n}^{\\dagger}+U\\Sigma_{t r n}H_{1}^{-1}Z^{T}(Q Q^{T})^{-1}H$ ", "page_idx": 41}, {"type": "text", "text": "2. The test error when $X_{t s t}=U L$ is given by ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\hat{A}_{t r n}}\\left[\\frac{1}{N_{t s t}}\\|U\\Sigma_{t r n}H_{1}^{-1}Z^{T}(Q Q^{T})^{-1}\\Sigma_{t r n}^{-1}L\\|_{F}^{2}+\\frac{\\sigma_{t s t}^{2}}{d}\\|W_{o p t}\\|_{F}^{2}\\right],\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "wher $\\begin{array}{r}{\\Im\\,Q_{\\rightarrow}=V^{T}(I_{\\rightarrow}-\\hat{A}_{t_{T}\\!\\mathscr{n}}^{\\dagger}\\hat{A}_{t^{\\prime\\prime}\\!\\mathscr{n}}),\\,H=V_{t r\\!\\mathscr{n}}^{T}\\hat{A}_{t r\\!\\mathscr{n}}^{\\dagger},\\,\\boldsymbol{1}\\,K=-\\hat{A}_{t r\\!\\mathscr{n}}^{\\dagger}U\\Sigma_{t r\\!\\mathscr{n}},\\,Z=I+V_{t r\\!\\mathscr{n}}^{T}\\hat{A}_{t r\\!\\mathscr{n}}^{\\dagger}U\\Sigma_{t r\\!\\mathscr{n}},}\\end{array}$ $H_{1}={K^{T}K}+\\dot{Z}^{T}(Q\\dot{Q}^{T})^{-1}Z$ . ", "page_idx": 41}, {"type": "text", "text": "For $c<1$ , we have that if $d<N$ then ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\Sigma_{t r n}^{-1}K^{T}K\\Sigma_{t r n}^{-1}]=\\left(\\frac{\\sqrt{(1+\\mu^{2}c-c)^{2}+4\\mu^{2}c^{2}}-1-\\mu^{2}c+c}{2\\mu^{2}c}+o(1)\\right)I_{r}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "and if $d>N$ then ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\Sigma_{t r n}^{-1}K^{T}K\\Sigma_{t r n}^{-1}]=\\left(\\frac{\\sqrt{4\\mu^{2}c+(-1+c+\\mu^{2}c)^{2}}-1-\\mu^{2}c+c}{2\\mu^{2}c}+o(1)\\right)I_{r}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "When $d<N$ then ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathfrak{L}[\\Sigma_{t r n}^{-1}K^{T}\\hat{A}_{t r n}^{\\dagger}(\\hat{A}_{t r n}^{\\dagger})^{T}K\\Sigma_{t r n}^{-1}]=\\frac{\\mu^{2}c^{2}+c^{2}+\\mu^{2}c-2c+1}{2\\mu^{4}c\\sqrt{4\\mu^{2}c^{2}+(1-c+\\mu^{2}c)^{2}}}I_{r}+\\frac{1}{2\\mu^{4}}\\left(1-\\frac{1}{c}\\right)I_{r}+o(1),\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "if $d>N$ then ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\Sigma_{t r n}^{-1}K^{T}\\hat{A}_{t r n}^{\\dagger}(\\hat{A}_{t r n}^{\\dagger})^{T}K\\Sigma_{t r n}^{-1}]=\\frac{1-2c+c^{2}+\\mu^{2}c+\\mu^{2}c^{2}}{2c\\mu^{4}\\sqrt{4\\mu^{2}c+(-1+c+\\mu^{2}c)^{2}}}I_{r}+(1-\\frac{1}{c})\\frac{1}{2\\mu^{4}}I_{r}+o(1).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "We have that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{E}[Q Q^{T}]=c\\left(\\frac{1}{2}+\\frac{1+\\mu^{2}c-\\sqrt{(-1+c+\\mu^{2}c)^{2}+4\\mu^{2}c}}{2c}\\right)+o(1).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "and ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{E}[(Q Q^{T})^{-1}]=\\frac{2}{1+\\mu^{2}c+c-\\sqrt{(-1+c+\\mu^{2}c)^{2}+4\\mu^{2}c}}+o(1).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "When $d<N$ we have that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{E}[H H^{T}]=c\\left(\\frac{1+c+\\mu^{2}c}{2\\sqrt{(1-c+\\mu^{2}c)^{2}+4c^{2}\\mu^{2}}}-\\frac{1}{2}\\right)I_{r}+o(1)\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "and when $d>N$ , we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{E}[H H^{T}]=c\\left(\\frac{1+c+\\mu^{2}c}{2\\sqrt{(-1+c+\\mu^{2}c)^{2}+4\\mu^{2}c}}-\\frac{1}{2}\\right)I_{r}+o(1).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "When $d<N$ , we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\varepsilon\\|\\|W\\|_{F}^{2}]=\\left(\\frac{\\mu^{2}c^{2}+c^{2}+\\mu^{2}c-2c+1}{2\\mu^{4}c\\sqrt{4\\mu^{2}c^{2}+(1-c+\\mu^{2}c)^{2}}}+\\frac{1}{2\\mu^{4}}\\left(1-\\frac{1}{c}\\right)\\right)}}\\\\ &{}&{\\mathrm{Tr}\\left(\\left(\\frac{\\sqrt{(1+\\mu^{2}c-c)^{2}+4\\mu^{2}c^{2}}-1-\\mu^{2}c+c}{2\\mu^{2}c}-I_{r}+\\frac{2}{1+\\mu^{2}c+c-\\sqrt{(-1+c+\\mu^{2}c)^{2}}}\\right.\\right.}\\\\ &{}&{\\left.\\left.+\\left(\\frac{2}{1+\\mu^{2}c+c-\\sqrt{(-1+c+\\mu^{2}c)^{2}+4\\mu^{2}c}}\\right)^{2}c\\left(\\frac{1+c+\\mu^{2}c}{2\\sqrt{(1-c+\\mu^{2}c)^{2}+4c^{2}\\mu^{2}}}-\\frac{1}{2}\\right)\\right),}\\\\ &{}&{\\mathrm{Tr}\\left(\\left(\\frac{\\sqrt{(1+\\mu^{2}c-c)^{2}+4\\mu^{2}c^{2}}-1-\\mu^{2}c+c}{2\\mu^{2}c}-I_{r}+\\frac{2}{1+\\mu^{2}c+c-\\sqrt{(-1+c+\\mu^{2}c)^{2}}}\\right.\\right.}\\\\ &{}&{\\left.\\left.+o(1),}\\end{array}\\right.\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "when $d>N$ this is estimated by ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\varepsilon[\\|W\\|_{F}^{2}]=\\left(\\frac{1-2c+c^{2}+\\mu^{2}c+\\mu^{2}c^{2}}{2c\\mu^{4}\\sqrt{4\\mu^{2}c+(-1+c+\\mu^{2}c)^{2}}}+(1-\\frac{1}{c})\\frac{1}{2\\mu^{4}}\\right)}\\\\ &{\\quad\\quad\\times\\left(\\left(\\frac{\\sqrt{4\\mu^{2}c+(-1+c+\\mu^{2}c)^{2}}-1-\\mu^{2}c+c}{2\\mu^{2}c}+c_{I}+\\frac{2}{1+\\mu^{2}c+c-\\sqrt{(-1+c+\\mu^{2}c)^{2}}}\\right.\\right.}\\\\ &{\\quad\\quad\\left.\\left.+\\left(\\frac{2}{1+\\mu^{2}c+c-\\sqrt{(-1+c+\\mu^{2}c)^{2}+4\\mu^{2}c}}\\right)^{2}c\\left(\\frac{1+c+\\mu^{2}c}{2\\sqrt{(-1+c+\\mu^{2}c)^{2}+4\\mu^{2}c}}-\\frac{1}{2}\\right)\\right.\\right.}\\\\ &{\\quad\\quad\\left.\\left.\\mathrm{Tr}\\left(\\left(\\frac{\\sqrt{4\\mu^{2}c+(-1+c+\\mu^{2}c)^{2}}-1-\\mu^{2}c+c}{2\\mu^{2}c}\\right.\\right.\\right.}\\\\ &{\\quad\\quad\\quad\\left.\\left.+o(1).\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "When $d<N$ the test error $\\mathcal{R}(W,X_{t s t})$ for $W=W_{o p t}$ is given by ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Big|={\\frac{1}{N_{t}}}\\frac{\\partial^{2}}{\\partial t}(\\frac{\\sqrt{4}\\eta^{2}+(-1+\\epsilon+\\mu^{2})^{2}-1}{2\\mu^{2}}-1-\\mu^{2}+c_{2}}{2\\mu}_{\\mathrm{L}},}\\\\ &{+\\frac{1}{1+\\mu^{2}\\epsilon}+c_{2}\\sqrt{-(1+\\epsilon+\\mu^{2})^{2}+4\\mu^{2}c^{2}}}\\\\ &{{\\frac{2}{1+\\mu^{2}\\epsilon+c_{1}}}\\frac{\\partial^{2}}{\\partial t}_{\\mathrm{L}},}\\\\ &{+\\frac{\\sigma_{\\mu}^{2}}{\\mu}_{\\mathrm{L}}\\Big(\\frac{\\mu^{2}+\\epsilon^{2}+\\mu^{2}c_{2}}{2\\mu\\epsilon\\sqrt{4}\\mu^{2}c^{2}+(1-\\epsilon+\\mu^{2})^{2}}+\\frac{1}{2\\mu}\\Big(1-\\frac{1}{\\epsilon}\\Big)\\Big)}\\\\ &{\\mathrm{Tr}\\left(\\frac{\\sqrt{(1+\\mu^{2}c-\\epsilon)^{2}+4\\mu^{2}c^{2}}-1-\\mu^{2}+c_{2}}{2\\mu\\epsilon}{\\epsilon}_{\\mathrm{L}}-\\epsilon\\frac{1}{1+\\mu^{2}\\epsilon+c_{2}}{\\epsilon}_{\\mathrm{L}}+\\frac{2}{1+\\mu^{2}c+\\epsilon^{2}}{\\epsilon}_{\\mathrm{L}}\\right)}\\\\ &{+\\frac{\\sigma_{\\mu}^{2}}{\\mu}_{\\mathrm{L}}\\left(\\frac{2}{1+\\mu^{2}\\epsilon+c_{2}}\\frac{2}{\\epsilon-\\epsilon+c_{2}}+4\\mu^{2}c^{2}\\right)^{2}\\epsilon\\left(\\frac{1+\\epsilon+\\mu^{2}c}{2\\sqrt{(1-\\epsilon+\\mu^{2}c)^{2}+4\\mu^{2}c^{2}}}-\\frac{1}{2}\\right)\\Omega}\\\\ &{\\mathrm{Tr}\\left(\\frac{\\sqrt{(1+\\mu^{2}c-\\epsilon)^{2}+4\\mu^{2}c^{2}}-1-\\mu^{2}c+c_{2}}{2\\mu\\epsilon}{\\epsilon}_{\\mathrm{L}}-\\epsilon\\frac{2}{1+\\mu^{2}c+\\epsilon-\\sqrt{(-1+\\epsilon+\\mu^{2}c^{2})+4\\mu^{2}c^{2}}}\\right)}\\\\ \n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "when $d>N$ this is estimated by ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\begin{array}{r l}&{\\frac{1}{N_{t}}\\frac{\\mathrm{Te}^{\\tau}\\left(1-(\\tau+1+c+\\mu^{\\tau})^{2}-1-1-\\mu^{\\tau}+c+c_{I}\\right)}{2\\mu_{C}^{2}c}}\\\\ &{+\\frac{1}{1+\\mu^{\\tau}c}+c-\\sqrt{(-1+c+\\mu^{\\tau})^{2}+4\\mu_{C}^{2}c^{2}}-2^{-2})}\\\\ &{\\frac{1}{1+\\mu^{\\tau}c}+c-\\sqrt{(-1+c+\\mu^{\\tau}c)^{2}+4\\mu_{C}^{2}c^{2}}}\\\\ &{+\\frac{\\sigma_{\\mu}c}{d}\\left(\\frac{1-2c+\\sigma^{2}+\\mu^{\\tau}c+\\mu^{\\tau}c^{2}}{2\\mu_{C}^{2}\\sqrt{4\\mu_{C}^{2}c}+(-1+c+\\mu^{\\tau}c^{2})^{2}}+(1-c)\\frac{1}{2}\\right)}\\\\ &{\\mathrm{Te}\\left(\\left(\\frac{\\sqrt{4\\mu^{\\tau}c}+(1-c+\\mu^{\\tau}c)^{2}-1-\\mu^{\\tau}c+c}{2\\mu_{C}^{2}c}+c_{I}+\\frac{2}{1+\\mu^{\\tau}c+c}\\sqrt{(-1+c+\\mu^{\\tau}c)^{2}+4\\mu_{C}^{2}c}\\right.\\right.}\\\\ &{+\\left.\\left.\\frac{\\sigma_{\\mu}c}{d}\\left(\\frac{1}{1+\\mu^{\\tau}c+c-\\sqrt{(-1+c+\\mu^{\\tau}c)^{2}+4\\mu^{\\tau}c}}\\right)^{2}+\\left(\\frac{1+c+\\mu^{\\tau}c}{2\\sqrt{(-1+c+\\mu^{\\tau}c)^{2}+4\\mu^{\\tau}c}}-\\frac{1}{2}\\right)\\right)}\\\\ &{\\mathrm{Te}\\left(\\left(\\sqrt{4\\mu^{\\tau}c}+(1+c+\\mu^{\\tau}c)^{2}-1-\\mu^{\\tau}c+c_{I}\\right.+\\left.\\frac{2}{1+\\mu^{\\tau}c+c}\\frac{2}{\\sqrt{(-1+c+\\mu^{\\tau}c)^{2}+4\\mu^{\\tau}c}}\\right.\\right.}\\\\ &{\\left.\\left.+o(1).}\\end{array}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "G Experiments ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "All experiments were conducted using Pytorch and run on Google Colab using an A100 GPU. For each empirical data point, we did at least 100 trials. The maximum number of trials for any experiment was 20000 trials. ", "page_idx": 43}, {"type": "text", "text": "For each configuration of the parameters, $N_{t r n},N_{t s t},d,\\sigma_{t r n},\\sigma_{t s t}$ , and $\\mu$ . For each trial, we sampled $u,v_{t r n},v_{t s t}$ uniformly at random from the appropriate dimensional sphere. We also sampled new training and test noise for each trial. ", "page_idx": 43}, {"type": "text", "text": "For the data scaling regime, we kept $d\\,=\\,1000$ and for the parameter scaling regime, we kept $N_{t r n}=1000$ . For all experiments, $N_{t s t}=1000$ . ", "page_idx": 44}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: Both the abstract and introduction reference the main two Theorems (Theorem 4 and Theorem 2). Additionally, both put them in the context of prior work. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 44}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Justification: We believe the main purpose of the paper is to show that a certain phenomenon exists and are very careful with our assumptions. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 44}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: All statements have corresponding detailed proofs. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 45}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: We have very few experiments. However, for each one we have a section in the appendix with the needed details and have provided code as part of the supplementary material. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. ", "page_idx": 45}, {"type": "text", "text": "In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 46}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We include the code as part of the submission. All data used is synthetic or already open source. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 46}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We have very few experiments. However, for each one we have a section in the appendix with the needed details and have provided code as part of the supplementary material. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 46}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 46}, {"type": "text", "text": "Answer: [No] ", "page_idx": 46}, {"type": "text", "text": "Justification: The paper is a theory paper about mean behavior under a variety of concentration results. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 47}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: Google Collab with an A100 was used. Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 47}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: ", "page_idx": 47}, {"type": "text", "text": "Guidelines: The paper conforms with the code of ethics. ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 47}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: This is a theoretical work that helps build an understanding of existing phenomena. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 48}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Justification: This is a theoretical work Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 48}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: We credit pytorch. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 48}, {"type": "text", "text": "", "page_idx": 49}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: This is a theoretical work ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 49}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: This is a theoretical work ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 49}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: This is a theoretical work ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 49}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 50}]