[{"figure_path": "erQDc72vyi/tables/tables_4_1.jpg", "caption": "Table 1: Effect of image queries with different detector backbone.", "description": "This table presents an ablation study on the impact of adding image queries to different detector backbones (ImageNet-1k R50 and CLIP R50).  It shows the Average Precision (AP) and its variations (AP50, AP75, etc.) with and without the addition of image queries. The results demonstrate that integrating image queries improves the model's performance across various metrics, regardless of the backbone used.", "section": "4 Experiments"}, {"figure_path": "erQDc72vyi/tables/tables_5_1.jpg", "caption": "Table 2: Effect of using different foundation models to extract image queries.", "description": "This table presents the results of an ablation study comparing different foundation models used for extracting image queries.  The goal is to determine which pre-trained model provides the best high-level image understanding for enhancing object detection.  The models compared include several popular vision transformers trained using both supervised and self-supervised methods.  The table shows that OpenAI CLIP and DEIT-III perform comparably well, outperforming other models.", "section": "4 Experiments"}, {"figure_path": "erQDc72vyi/tables/tables_5_2.jpg", "caption": "Table 3: Effect of different implementations to extract image queries.", "description": "This table presents the ablation study on different methods of extracting image queries. The methods compared are: 1) cropping sub-images and feeding them into the foundation model individually to get class tokens as image queries; 2) using mean features of patch tokens as image queries; 3) using multiple class tokens as image queries constrained by attention masks.  The table shows the AP, AP50, AP75, APs, APm, API, training time, and inference speed for each method with one image query or multiple image queries (1+4).", "section": "3 Foundation Models as Feature Enhancers"}, {"figure_path": "erQDc72vyi/tables/tables_5_3.jpg", "caption": "Table 4: Ablation studies on feature fusion in the encoder.", "description": "This table presents the ablation study results on feature fusion within the encoder. It compares the performance of the baseline model (no foundation model) against three variations: adding 5 image queries, incorporating patch tokens into the encoder, and adding patch tokens to both the encoder and the decoder.  The results are evaluated using metrics such as AP, AP50, AP75, APs, APm, and APl, along with memory usage (Mem), GFLOPs, and FPS.", "section": "4 Experiments"}, {"figure_path": "erQDc72vyi/tables/tables_6_1.jpg", "caption": "Table 5: Ablation studies on the input image size of the foundation model.", "description": "This table presents the ablation study on the input image size of the foundation model used in Frozen-DETR.  Different input image sizes (224, 336, 448, and 560) were tested, and the resulting Average Precision (AP) metrics, along with breakdowns by AP50, AP75, APs, APm, and API, are reported.  Additionally, the table shows the GFLOPs (giga floating-point operations) and FPS (frames per second) for each input size, illustrating the computational cost and speed tradeoffs associated with different input resolutions.", "section": "4 Experiments"}, {"figure_path": "erQDc72vyi/tables/tables_6_2.jpg", "caption": "Table 6: Ablation studies on the model size of the foundation model.", "description": "This table presents the ablation study results on the model size of the foundation model used in Frozen-DETR.  It shows how the detector's performance (measured by Average Precision (AP) and its variants) changes when using different sized foundation models (R101, ViT-B-16, ViT-L-14). The table also includes the computational cost (GFLOPS) and speed (FPS) for each model size.", "section": "4 Experiments"}, {"figure_path": "erQDc72vyi/tables/tables_6_3.jpg", "caption": "Table 7: Ablation studies on whether fine-tuning the foundation model (CLIP R101).", "description": "This table presents the results of ablation studies conducted to evaluate the impact of fine-tuning the CLIP R101 foundation model on the performance of the object detection model.  The table compares three scenarios: 1) No foundation model used, 2) The foundation model is trainable (fine-tuned), and 3) The foundation model is frozen (not fine-tuned).  The metrics used for comparison are Average Precision (AP), AP at 50% IoU (AP50), AP at 75% IoU (AP75), AP for small objects (APs), AP for medium objects (APm), AP for large objects (APl), and memory consumption (Mem). The results indicate the performance differences based on training the foundation model or keeping it frozen.", "section": "4.2 Patch Tokens Enhance Feature Fusion in Encoder"}, {"figure_path": "erQDc72vyi/tables/tables_7_1.jpg", "caption": "Table 8: Comparisons with other query-based detectors on COCO minival set. *: the input size of the foundation model is 448. \u2020: The single-scale detector uses standard attention in the encoder while Frozen-DETR uses deformable attention to fuse multi-scale features.", "description": "This table compares the performance of Frozen-DETR with other query-based object detectors on the COCO minival dataset.  It shows the backbone used, number of training epochs, and various metrics including Average Precision (AP) at different Intersection over Union (IoU) thresholds (AP50, AP75), as well as AP for small, medium, and large objects.  The table also notes when the foundation model input size is 448 and highlights the use of deformable attention in Frozen-DETR for multi-scale feature fusion.  It demonstrates Frozen-DETR's performance gains over existing methods.", "section": "4.3 Main Results on COCO dataset"}, {"figure_path": "erQDc72vyi/tables/tables_7_2.jpg", "caption": "Table 9: Results on LVIS v1 training with full annotations. *: Our implementation.", "description": "This table presents the results of different object detection methods on the LVIS v1 dataset, trained with full annotations.  The results show average precision (AP), average precision for rare classes (APr), average precision for common classes (APc), and average precision for frequent classes (APf). The table highlights the performance of the proposed Frozen-DETR method in comparison to several other state-of-the-art detectors.", "section": "4.4 Dose Frozen-DETR Work under Large Vocabulary Settings?"}, {"figure_path": "erQDc72vyi/tables/tables_8_1.jpg", "caption": "Table 10: Results on open-vocabulary LVIS. *: Our implementation.", "description": "This table presents the results of different open-vocabulary object detection methods on the LVIS dataset.  The results are broken down by several metrics, including AP (average precision), APr (average precision for rare classes), APc (average precision for common classes), and APf (average precision for frequent classes).  The table highlights that the proposed Frozen-DETR method significantly outperforms other state-of-the-art methods, demonstrating its effectiveness in open-vocabulary object detection scenarios.", "section": "4.4 Dose Frozen-DETR Work under Large Vocabulary Settings?"}, {"figure_path": "erQDc72vyi/tables/tables_8_2.jpg", "caption": "Table 11: Results of combining multiple foundation models on COCO.", "description": "This table shows the performance of the DINO-det-4scale model with different combinations of foundation models (CLIP and DINOv2).  It demonstrates that incorporating multiple foundation models can lead to further performance improvements.  The AP, AP50, AP75, APs, APm and API metrics are provided, along with the absolute and relative gains in AP compared to the baseline (DINO-det-4scale).", "section": "4.5 Combining Multiple Foundation Models"}, {"figure_path": "erQDc72vyi/tables/tables_9_1.jpg", "caption": "Table 1: Effect of image queries with different detector backbone.", "description": "This table presents an ablation study on the impact of adding image queries to different detector backbones (ResNet-50 and CLIP ResNet-50).  It shows the Average Precision (AP), and its variations (AP50, AP75, APS, APM, API)  for object detection with and without the addition of image queries.  The results demonstrate the effectiveness of image queries in improving detection performance, regardless of the backbone used.", "section": "4 Experiments"}, {"figure_path": "erQDc72vyi/tables/tables_14_1.jpg", "caption": "Table 14: Comparisons with different methods to improve the performance.", "description": "This table compares the performance and computational cost of four different methods for improving object detection performance.  The baseline is DINO-det-4scale.  The table shows the memory usage (Mem) and training time per epoch for each method, as well as the inference memory usage and frames per second (FPS), and the GFLOPS (floating point operations per second) for each method.  The methods compared are:\n\n*   **DINO-det-4scale baseline:** The original DINO-det-4scale model serves as the baseline for comparison.\n*   **Frozen-DETR (DINO-det-4scale):**  The proposed Frozen-DETR method applied to DINO-det-4scale.\n*   **DINO-det-5scale:** The DINO-det model with 5 scales.\n*   **DINO-det-4scale + ViT-L backbone:** The DINO-det-4scale model, but uses the ViT-L as a backbone instead of the standard backbone.", "section": "A Comparisons between Using Foundation Models as a Backbone and as a Plug-and-Play Module"}, {"figure_path": "erQDc72vyi/tables/tables_16_1.jpg", "caption": "Table 15: Results of combining foundation models with registers.", "description": "This table presents the results of experiments combining multiple foundation models (CLIP and DINOv2) with and without registers, demonstrating the impact of these additions on the performance metrics (AP, AP50, AP75, APS, APm, API).  The baseline is DINO-det-4scale.  The table shows a progressive improvement in performance metrics as more foundation models and registers are included, highlighting the benefits of integrating these components.", "section": "4.7 How does Frozen-DETR work?"}]