[{"figure_path": "qf1ncViBr5/figures/figures_0_1.jpg", "caption": "Figure 1: Three state-of-the-art architectures and their associated derivation trees within einspace. Top row shows the architectures where the black node is the input tensor and the red is the output. Bottom row shows derivation trees where the top node represents the starting symbol, the grey internal nodes the non-terminals and the leaf nodes the terminal operations. See Section 3.1 for details on other node colouring. Best viewed with digital zoom.", "description": "This figure shows three state-of-the-art neural network architectures (ResNet18, ViT, and MLP-Mixer) and their corresponding derivation trees within the proposed einspace search space.  The top row displays the architectures, where the black node represents the input tensor and the red node signifies the output. The bottom row illustrates the derivation trees generated by the context-free grammar used to define einspace.  In the derivation trees, the top node represents the starting symbol of the grammar, grey nodes represent non-terminal symbols (intermediate steps in architecture generation), and leaf nodes depict the terminal operations (the actual layers in the neural network). The color-coding of the nodes provides information about the type of operation represented (explained in section 3.1 of the paper).", "section": "3 einspace: A Search Space of Fundamental Operations"}, {"figure_path": "qf1ncViBr5/figures/figures_1_1.jpg", "caption": "Figure 1: Three state-of-the-art architectures and their associated derivation trees within einspace. Top row shows the architectures where the black node is the input tensor and the red is the output. Bottom row shows derivation trees where the top node represents the starting symbol, the grey internal nodes the non-terminals and the leaf nodes the terminal operations. See Section 3.1 for details on other node colouring. Best viewed with digital zoom.", "description": "This figure shows three state-of-the-art neural network architectures (ResNet18, ViT, and MLP-Mixer) and their corresponding derivation trees within the proposed einspace search space.  The top row displays the architectures visually, with the input tensor represented by a black node and the output tensor by a red node. The bottom row illustrates the derivation trees generated by the parameterized probabilistic context-free grammar (PCFG) that defines einspace.  Each derivation tree shows how the architecture is constructed from fundamental operations, with the top node as the starting symbol, gray internal nodes representing non-terminal operations, and leaf nodes representing terminal operations. Different colors are used to distinguish the type of operation (branching, aggregation, routing, computation) as explained in Section 3.1.", "section": "3 einspace: A Search Space of Fundamental Operations"}, {"figure_path": "qf1ncViBr5/figures/figures_3_1.jpg", "caption": "Figure 1: Three state-of-the-art architectures and their associated derivation trees within einspace. Top row shows the architectures where the black node is the input tensor and the red is the output. Bottom row shows derivation trees where the top node represents the starting symbol, the grey internal nodes the non-terminals and the leaf nodes the terminal operations. See Section 3.1 for details on other node colouring. Best viewed with digital zoom.", "description": "This figure shows three state-of-the-art neural network architectures (ResNet18, ViT, and MLP-Mixer) and their corresponding derivation trees within the proposed einspace search space. Each architecture is represented visually with the input tensor in black and the output in red.  The derivation trees illustrate how these architectures are constructed from fundamental operations within einspace, using a context-free grammar. The top node of the tree represents the starting symbol of the grammar. Internal nodes are non-terminals and represent modules, while the leaf nodes are terminal operations, each colored to represent their operational role.", "section": "1 Introduction"}, {"figure_path": "qf1ncViBr5/figures/figures_4_1.jpg", "caption": "Figure 1: Three state-of-the-art architectures and their associated derivation trees within einspace. Top row shows the architectures where the black node is the input tensor and the red is the output. Bottom row shows derivation trees where the top node represents the starting symbol, the grey internal nodes the non-terminals and the leaf nodes the terminal operations. See Section 3.1 for details on other node colouring. Best viewed with digital zoom.", "description": "This figure shows three state-of-the-art neural network architectures (ResNet18, ViT, and MLP-Mixer) and their corresponding derivation trees within the proposed einspace search space.  The top row displays the architectures visually, with the input tensor represented by a black node and the output tensor by a red node. The bottom row illustrates the derivation trees for each architecture, showing how they are generated by the context-free grammar used to define einspace. The top node in each derivation tree is the starting symbol, grey nodes represent non-terminal operations, and leaf nodes represent the terminal operations used in the architecture. Different colors are used to visually represent different groups of operations in the derivation trees.", "section": "3 einspace: A Search Space of Fundamental Operations"}, {"figure_path": "qf1ncViBr5/figures/figures_4_2.jpg", "caption": "Figure 3: Example derivation tree of a traditional convolutional block with a skip connection.", "description": "This figure shows the derivation tree for a simple convolutional block with a skip connection, constructed using the einspace context-free grammar. The tree visually represents how fundamental operations (leaf nodes) combine to form larger modules (internal nodes) and ultimately, the complete architecture (root node). It illustrates the hierarchical and recursive nature of the grammar, enabling the generation of diverse and complex architectures.", "section": "3.3 Search Space as a Context-Free Grammar"}, {"figure_path": "qf1ncViBr5/figures/figures_6_1.jpg", "caption": "Figure 4: To ensure our CFG is consistent and does not generate infinite architectures, we make sure the branching rate is in the sub-critical region by setting p(M\u2192C|M)>0.31.", "description": "This figure shows a graph illustrating the relationship between the branching rate and the probability of selecting a computation module (p(M\u2192C|M)) in the CFG.  The x-axis represents p(M\u2192C|M), and the y-axis represents the branching rate. The curve represents the boundary between the super-critical and sub-critical regions. The super-critical region (pink) indicates a high branching rate where the CFG can generate infinitely long architectures. The sub-critical region (green) indicates a low branching rate, ensuring that the CFG generates finite architectures. A dashed vertical line indicates a threshold probability of p(M\u2192C|M) = 0.31. A black point on the curve shows that with a p(M\u2192C|M) slightly below 0.31, the branching rate is slightly above 1. Therefore, the probability p(M\u2192C|M) is set to be > 0.31 to guarantee that the CFG is in the sub-critical region and does not generate infinitely long architectures.", "section": "3.7 Balancing Architecture Complexity"}, {"figure_path": "qf1ncViBr5/figures/figures_7_1.jpg", "caption": "Figure 5: The top RE(Mix) architecture on AddNIST, found in einspace.", "description": "This figure showcases the top-performing architecture discovered by the Regularized Evolution (RE) algorithm initialized with a mix of state-of-the-art architectures (RE(Mix)) on the AddNIST dataset, within the einspace search space.  The architecture's structure and complexity are visually represented, highlighting the capacity of einspace to find diverse and high-performing architectures.", "section": "4.3 Evolutionary Search from Existing SOTA Architectures"}, {"figure_path": "qf1ncViBr5/figures/figures_8_1.jpg", "caption": "Figure 8: The top architectures found by RE(Mix) in einspace on Unseen NAS. From left to right, row by row: AddNIST, Language; MultNIST, CIFARTile; Gutenberg, Isabella; GeoClassing, Chesseract.", "description": "This figure visualizes the top-performing neural network architectures discovered by the regularized evolution (RE) algorithm initialized with a mix of state-of-the-art architectures (RE(Mix)) in the einspace search space. Each sub-figure represents a different dataset from the Unseen NAS benchmark: AddNIST, Language, MultNIST, CIFAR-Tile, Gutenberg, Isabella, GeoClassing, and Chesseract. The architectures are depicted graphically, with each node representing a specific operation (e.g., convolution, normalization, activation), and the color indicating the node type. The figure demonstrates the diversity of architectures generated by the einspace search space, highlighting its capability to discover novel and high-performing architectures across various tasks.", "section": "4.3 Evolutionary Search from Existing SOTA Architectures"}, {"figure_path": "qf1ncViBr5/figures/figures_17_1.jpg", "caption": "Figure 1: Three state-of-the-art architectures and their associated derivation trees within einspace. Top row shows the architectures where the black node is the input tensor and the red is the output. Bottom row shows derivation trees where the top node represents the starting symbol, the grey internal nodes the non-terminals and the leaf nodes the terminal operations. See Section 3.1 for details on other node colouring. Best viewed with digital zoom.", "description": "This figure shows three state-of-the-art neural architectures (ResNet18, ViT, MLP-Mixer) and how they are represented as derivation trees within the proposed einspace search space.  The top row displays the architectures visually, highlighting input and output tensors. The bottom row presents the corresponding derivation trees generated by the context-free grammar, illustrating how the architectures are constructed from fundamental operations.  Each node in the tree represents an operation or module, with the terminal nodes signifying fundamental operations and non-terminal nodes representing more complex modules. The node colors represent different types of operations. This visualization demonstrates the expressiveness of einspace in representing diverse architectures.", "section": "3 einspace: A Search Space of Fundamental Operations"}, {"figure_path": "qf1ncViBr5/figures/figures_26_1.jpg", "caption": "Figure 1: Three state-of-the-art architectures and their associated derivation trees within einspace. Top row shows the architectures where the black node is the input tensor and the red is the output. Bottom row shows derivation trees where the top node represents the starting symbol, the grey internal nodes the non-terminals and the leaf nodes the terminal operations. See Section 3.1 for details on other node colouring. Best viewed with digital zoom.", "description": "This figure illustrates how three different state-of-the-art neural network architectures (ResNet18, ViT, and MLP-Mixer) are represented within the einspace search space using derivation trees.  Each architecture's derivation tree shows how the architecture is built up from fundamental operations, illustrating the expressiveness of the proposed search space. The top row displays the architectures, while the bottom row shows their respective derivation trees.  The nodes in the derivation trees represent different operations and their connections in the architecture, using different colors to distinguish various types of operations.", "section": "3 einspace: A Search Space of Fundamental Operations"}, {"figure_path": "qf1ncViBr5/figures/figures_27_1.jpg", "caption": "Figure 1: Three state-of-the-art architectures and their associated derivation trees within einspace. Top row shows the architectures where the black node is the input tensor and the red is the output. Bottom row shows derivation trees where the top node represents the starting symbol, the grey internal nodes the non-terminals and the leaf nodes the terminal operations. See Section 3.1 for details on other node colouring. Best viewed with digital zoom.", "description": "This figure shows three state-of-the-art neural network architectures (ResNet18, ViT, and MLP-Mixer) and their corresponding derivation trees within the *einspace* search space.  The top row displays the architectures themselves, illustrating the flow of tensors from input (black) to output (red). The bottom row shows the derivation trees generated by the probabilistic context-free grammar (PCFG) that defines the *einspace* search space. These trees break down each architecture into a hierarchical structure of fundamental operations (leaf nodes), non-terminal operations (grey nodes), and the starting symbol (top node). The color-coding of nodes in the trees provides further information about the operation type (detailed in Section 3.1), showing how complex architectures are built from simpler operations within the grammar.", "section": "3 einspace: A Search Space of Fundamental Operations"}]