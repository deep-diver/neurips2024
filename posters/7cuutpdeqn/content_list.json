[{"type": "text", "text": "Analytically deriving Partial Information Decomposition for affine systems of stable and convolution-closed distributions ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Chaitanya Goswami ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Amanda Merkley ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Electrical & Computer Engineering, Carnegie Mellon University, Pittsburgh, PA-15213. cgoswami@andrew.cmu.edu ", "page_idx": 0}, {"type": "text", "text": "Electrical & Computer Engineering, Carnegie Mellon University, Pittsburgh, PA-15213. amerkley@andrew.cmu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Bivariate partial information decomposition (PID) has emerged as a promising tool for analyzing interactions in complex systems, particularly in neuroscience. PID achieves this by decomposing the information that two sources (e.g., different brain regions) have about a target (e.g., a stimulus) into unique, redundant, and synergistic terms. However, the computation of PID remains a challenging problem, often involving optimization over distributions. While several works have been proposed to compute PID terms numerically, there is a surprising dearth of work on computing PID terms analytically. The only known analytical PID result is for jointly Gaussian distributions. In this work, we present two theoretical advances that enable analytical calculation of the PID terms for numerous wellknown distributions, including distributions relevant to neuroscience, such as Poisson, Cauchy, and binomial. Our first result generalizes the analytical Gaussian PID result to the much larger class of stable distributions. We also discover a theoretical link between PID and the emerging fields of data thinning and data fission. Our second result utilizes this link to derive analytical PID terms for two more classes of distributions: convolution-closed distributions and a sub-class of the exponential family. Furthermore, we provide an analytical upper bound for approximately computing PID for convolution-closed distributions, whose tightness we demonstrate in simulation. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Bivariate partial information decomposition1 (PID) is an information-theoretic framework developed for answering a central inquiry in many neuroscientific and machine learning studies: how do two sources, $X$ and $Y$ , jointly process information about a target $M$ ? PID answers this question by quantifying the $M$ -specific information contained in different interactions between $X$ and $Y$ . Specifically, it decomposes the total information $X$ and $Y$ have about $M$ into four components: (i) the information about $M$ contained uniquely in $X$ , (ii) the information about $M$ contained uniquely in $Y$ , (iii) the redundant information about $M$ contained in both $X$ and $Y$ , and (iv) the synergistic information about $M$ which arises from the interaction between $X$ and $Y$ . ", "page_idx": 0}, {"type": "text", "text": "The PID terms offer novel insights for understanding interactions within complex systems, particularly in neuroscience. For instance, PID has been used to understand the firing patterns of grid cells [1, 2] and to study the flow of information in the visual cortex [3]. The following list of works [1, 3, 4, 5, 6, 7, 8] demonstrate the application of PID in studying diverse neuroscientific questions. ", "page_idx": 0}, {"type": "text", "text": "Beyond neuroscience, PID has found applications in multimodal learning for interpreting model predictions [9, 10], fair machine learning for defining and quantifying bias [11, 12, 13, 14], and understanding financial markets [15]. ", "page_idx": 1}, {"type": "text", "text": "However, the primary constraint hindering broader adoption of PID is the difficulty of computing and estimating the PID terms. BROJA-PID [16], a widely applied PID framework, requires solving a constrained minimization problem over a set of probability distributions (see Sec. 2). This minimization problem can pose a considerable challenge, particularly when the underlying distributions are continuous. As a result, several recent works have been dedicated to providing numerical algorithms that solve the aforementioned minimization problem exactly or approximately [3, 9, 17, 18, 19, 20], with more emphasis on the case of discrete distributions. ", "page_idx": 1}, {"type": "text", "text": "Despite significant progress in numerically calculating PID (typically through discrete approximations), very few works exist on analytically2calculating PID. As of this writing, the only known analytical PID expressions exist for jointly Gaussian $M$ , $X$ , and $Y$ [21, 22]. A fundamental property of the Gaussian system is that one of the unique information (UI) terms in its PID is guaranteed to be zero. This property greatly simplifies the computation of PID for Gaussian systems, as the rest of the PID terms can be easily derived by solving desirable linear equations specified in many PID frameworks (see Sec 2), bypassing the need for optimizing over distributions (see Sec. 3). ", "page_idx": 1}, {"type": "text", "text": "In this work, we show that numerous systems of random variables $M,\\,X$ , and $Y$ expressing a particular \u201caffine dependence structure\u201d also exhibit the same fundamental property that at least one of the UI terms is zero. Consequently, we expand significantly on the existing Gaussian PID result by using this fundamental property to analytically compute PID for various systems of $M,X$ , and $Y$ employing well-known distributions such as Poisson, exponential, gamma, beta, negative binomial, multinomial, Cauchy, L\u00e9vy-stable and more. The main contributions of this work are: ", "page_idx": 1}, {"type": "text", "text": "1. We extend the Gaussian PID result to a much larger class of distributions, known as the stable distribution family [23, 24] in Sec. 4. These results provide the first known analytical PID for fat-tailed distributions.   \n2. We highlight a theoretical link between PID calculation and the fields of data thinning [25] and data fission [26] in Sec. 5. We utilize this link to derive analytical PID terms for two more distribution families: convolution-closed distributions [25] and certain exponential family distributions [26].   \n3. For convolution-closed distributions, we further derive an analytical upper bound on the objective of the minimization used for computing BROJA-PID. We use this upper bound to approximately compute PID for systems of $M,X$ , and $Y$ having a non-affine dependence structure. We show the goodness of our approximation by a simulation study in Sec. 6. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Notation: We denote the set of all natural numbers, real numbers, and positive real numbers as $\\mathbb{N}$ , $\\mathbb{R}$ , and $\\mathbb{R}^{+}$ , respectively. Vectors are denoted by bold-faced font and an arrow, and matrices are denoted by bold-faced font. Define $\\mathbb{N}_{0}=\\mathbb{N}\\cup\\{0\\}$ . Let $\\mathbf{I}_{d}$ be the identity matrix of size $d\\times d$ and $[d]=\\{1,\\dots,d\\}\\,\\forall\\,d\\in\\mathbb{N}$ . We denote $\\vec{1}_{d}$ and $\\vec{\\mathbf{0}}_{d}$ as $d$ -dimensional vectors having all elements as 1 and 0, respectively. For brevity, the probability notations of the form $P(A|B)$ and $P(A)$ are always understood to be as $P(A=a|B=\\bar{b})$ and $P(A=a)$ , respectively. The general term \u2018distribution\u2019 is used to refer to both probability density function (p.d.f.) and probability mass function (p.m.f.) (whichever is appropriate depending upon the context). The symbol $A\\perp\\!\\!\\!\\perp B|C$ denotes that $A$ and $B$ are conditionally independent given $C$ , and similarly, $A\\perp\\!\\!\\!\\perp B$ implies $A$ and $B$ are independent. The $L_{p}$ -norm is denoted by $\\|(\\cdot)\\|_{p}\\,\\mathbf{\\bar{\\forall}}\\,p\\in[0,\\infty)$ , and $|(\\cdot)|$ denotes the absolute function. ", "page_idx": 1}, {"type": "text", "text": "PID Background: Suppose $M,X,Y$ are random variables with joint distribution $P(M,X,Y)$ . According to [16, 27], three desirable equalities for a bivariate PID are as follows: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{P}(M;[X,Y])=U I(M;X\\backslash Y)+U I(M;Y\\backslash X)+R I(M;X,Y)+S I(M;X,Y),}\\\\ &{I_{P}(M;X)\\!=\\!U I(M;X\\backslash Y)\\!+\\!R I(M;X,Y),}\\\\ &{I_{P}(M;Y)\\!=\\!U I(M;Y\\backslash X)\\!+\\!R I(M;X,Y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Here, $I_{P}(M;X)$ and $I_{P}(M;Y)$ denote the mutual information [28] between $M$ and $X$ , and $M$ and $Y$ , respectively, under the distribution $P(M,X,Y)$ . Similarly, $I_{P}(M;[X,Y])$ denotes the joint mutual information between $M$ and $[X,Y]$ under the distribution $P(M,X,Y)$ . $\\dot{U}I(M;X\\backslash Y)$ and $U I(M;Y\\backslash X)$ are the unique information about $M$ in $X$ and $Y$ , respectively. $S I(M;X,Y)$ and $R I(M;X,Y)$ denote the respective synergistic and redundant information shared between $X$ and $Y$ about $M$ . The linear system defined in (1) contains four variables in three equations. Hence, only one of $U I,R I$ , or $S I$ need be defined to evaluate all four PID terms. Proposing a suitable definition of PID is the focus of much research [16, 27, 29, 30, 31, 32, 33, 34, 35] and all our theoretical results are applicable for any Blackwellian PID [22, 36] or a PID definition satisfying the assumption $(*)$ of Bertschinger et al. [16] (see Appx. B for a formal justification). For example, our results are applicable for the PID definitions proposed in [16, 22, 27, 29, 37]. In this work, we discuss our results in the context of the BROJA-PID [16, 38], which is the definition of PID adopted in previous works [3, 9, 20]: ", "page_idx": 2}, {"type": "text", "text": "where $\\Delta_{P}{=}\\{Q(M,X,Y){:}Q(M,X){=}P(M,X),Q(M,Y)\\,{=}P(M,Y)\\}$ and $I_{Q}(M;X|Y)$ is the conditional mutual information under the distribution $Q(M,X,Y)$ . Note that the minimizing distributions for both problems shown in (2) are the same [16], and consequently, both minimization problems are equivalent. We refer the reader to [39] for a review on PID. In this work, we define \u201canalytically calculating PID\u201d as analytically solving (2) by providing an explicit construction of the minimizing distribution $Q^{*}(M,X,{\\dot{Y}})$ that minimizes (2). Note that only the distributions $Q^{*}(M,X,Y)$ and $P(M,X,Y)$ are needed to compute the BROJA-PID terms. ", "page_idx": 2}, {"type": "text", "text": "We briefly describe two distribution families used in Sec. 4 and Sec. 5 (see Appx. N for more details). ", "page_idx": 2}, {"type": "text", "text": "Stable distribution family: Stable distributions are a family of distributions that naturally arise in the context of generalized central limit theorems. Some well-known members of this family are the Gaussian, Cauchy, Poisson, and L\u00e9vy distributions. A defining feature of stable distributions is that the sum of two independent copies of a random variable $X$ , denoted as $X_{1}$ and $X_{2}$ , has the same distribution as a translated and scaled version of $X$ [23, 24, 40]. In this work, we consider five sub-classes of stable distributions: ", "page_idx": 2}, {"type": "text", "text": "(i) Continuous stable distributions are parameterized through four parameters: stability parameter $\\alpha\\in(0,2]$ , skewness parameter $\\beta\\in[-1,1]$ , scale parameter $\\gamma\\in(0,\\infty)$ , and location parameter $\\mu\\in\\mathbb{R}$ . We denote its p.d.f. as $p_{C S}(\\alpha,\\beta,\\gamma,\\mu)$ . Note that all continuous stable distribution (except Gaussian) are fat-tailed. ", "page_idx": 2}, {"type": "text", "text": "(ii) Independent component multivariate stable distributions describe the distribution of $\\vec{\\bf X}$ consisting of $d$ independent random variables $\\{X_{j}\\}_{j=1}^{d}$ , such that each $X_{j}\\sim p_{C S}(\\alpha,\\beta_{j},\\gamma_{j},\\mu_{j})$ . We denote its p.d.f. as $p_{C S-I C}(\\alpha,\\vec{\\beta},\\vec{\\gamma},\\vec{\\mu})$ . ", "page_idx": 2}, {"type": "text", "text": "(iii) Elliptically-contoured multivariate stable distributions are the distributions of continuous stable random vectors whose p.d.f. has elliptical contours, e.g., the multivariate Gaussian distribution. We denote its p.d.f. as $p_{C S-E C}(\\alpha,\\Sigma,\\vec{\\mu})$ . Here, $\\Sigma$ is a positive definite matrix, $\\vec{\\pmb{\\mu}}\\in\\mathbb{R}^{d}$ , and $\\alpha\\in(0,2]$ . ", "page_idx": 2}, {"type": "text", "text": "(iv) Discrete stable distributions are the discrete analogues of the continuous stable distributions. The p.m.f. of discrete stable distributions, denoted as $\\bar{P}_{D S}(\\nu,\\tau)$ , are parameterized through two parameters: rate parameter $\\tau>0$ and exponent $0<\\nu\\leq1$ . Discrete stable distributions do not have well-known multivariate generalizations except the Poisson distribution. ", "page_idx": 2}, {"type": "text", "text": "(v) Multivariate Poisson distribution: We use the multivariate Poisson distribution proposed in [41, 42, 43, 44], denoted as $\\mathrm{Poisson}(d,d^{\\prime},\\vec{\\Lambda})$ , which represents each random variable in the random vector $\\vec{\\bf N}$ as a sum of independent Poisson random variables. Formally, $\\vec{\\bf N}\\sim\\mathrm{Poisson}(d,d^{\\prime},\\vec{\\bf\\Lambda})$ if $\\vec{\\bf N}={\\bf A}\\vec{\\bf N}^{g}$ , where $\\mathbf{A}=\\left[\\mathbf{A}_{1}\\ldots\\mathbf{A}_{d^{\\prime}}\\right]$ . Here, $\\mathbf{A}_{i}$ denotes a $d\\times\\left(\\O_{i}^{d}\\right)$ submatrix having no duplicate columns, where each of its columns contain exactly $i$ ones and $(d-i)$ zeros [44]. The vector ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\vec{\\bf N}^{g}=[N_{1}^{g}\\cdot\\cdot\\cdot N_{d}^{g}\\;N_{12}^{g}\\cdot\\cdot\\cdot\\;N_{d-(d^{\\prime}-1)\\ldots d}^{g}]^{T}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "with mutually independent $N_{i_{1}\\ldots i_{j}}^{g}\\sim\\mathrm{Poisson}(\\lambda_{i_{1}\\ldots i_{j}})\\;\\forall\\;(i_{1},\\ldots,i_{j})\\in\\mathbb{A}_{j}^{d},j\\in[d^{\\prime}],$ where $\\mathbb{A}_{i}^{d}=$ $\\{(j_{1},\\dots,j_{i})\\colon j_{1}<j_{2}<\\dots<j_{i}$ , and $j_{1},\\ldots,j_{i}\\in[d]\\}$ , e.g., $\\mathbb{A}_{2}^{3}=\\{(1,2),(1,3),(2,3)\\}$ . Note that $d^{\\prime}\\leq d$ , and $\\vec{\\bf\\cal A}=\\left[\\lambda_{1}\\;\\ldots\\;\\lambda_{d-(d^{\\prime}-1)\\ldots d}\\right]^{T}$ . ", "page_idx": 2}, {"type": "text", "text": "We refer the reader to [23, 24, 40, 42, 44, 45, 46, 47] for more details on stable distributions. ", "page_idx": 3}, {"type": "text", "text": "Convolution-closed distributions: Convolution-closed distributions form a large class of distributions that are closed under convolution in some parameter $\\delta$ . Formally, we define the convolutionclosed distribution as follows: Let $\\mathcal{F}_{\\mathcal{D}}$ denote a family of distributions, where each member distribution $f(\\delta)\\in\\mathcal{F}_{\\mathcal{D}}$ is indexed by a parameter $\\delta\\,\\in\\,{\\mathcal{D}}$ . Consider $X_{1}\\sim\\,f(\\delta_{1})$ , $X_{2}\\sim\\,f(\\delta_{2})$ , and $X_{1}$ \u22a5\u22a5 $X_{2}$ for some $\\delta_{1},\\delta_{2}\\in\\mathcal{D}$ . Then, $\\mathcal{F}_{\\mathcal{D}}$ is convolution-closed in the parameter $\\delta$ if ", "page_idx": 3}, {"type": "equation", "text": "$$\nX_{1}+X_{2}\\sim f(\\delta_{1})*f(\\delta_{2})=f(\\delta_{1}+\\delta_{2})\\,\\forall\\,\\delta_{1},\\delta_{2}\\in\\mathcal{D}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $^*$ denotes the convolution operator. Many well-known distributions can be considered convolution-closed in some parameter [25], such as the Poisson distribution, Gaussian distribution, gamma distribution, etc. Table 1 in [25] lists various examples of convolution-closed distributions. ", "page_idx": 3}, {"type": "text", "text": "3 A sufficient condition for computing PID terms analytically ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The main focus of this work is to analyze the cases where (2) is analytically solvable. A sufficient condition for solving (2) is to show that $\\Delta_{P}$ contains a distribution $Q_{M C}(M,X,Y)$ with the Markovian structure $M\\rightarrow X\\rightarrow Y$ or $M\\rightarrow Y\\rightarrow X$ . We briefly discuss the argument justifying this sufficient condition. Consider the case where $Q_{M C}(M,X,Y)\\,\\overset{\\cdot}{\\in}\\Delta_{P}$ and has the Markovian structure $M\\rightarrow X\\rightarrow Y$ . First, we note that $\\begin{array}{r}{U I(M;Y\\backslash X)=\\operatorname*{min}_{Q\\in\\Delta_{P}}I_{Q}(M;Y|X)\\ge0}\\end{array}$ due to the nonnegativity of conditional mutual information [28]. Second, $Y$ \u22a5\u22a5 $M|X$ due to the Markovian structure of $Q_{M C}(M,X,Y)$ , which implies that $I_{Q_{M C}}(M;Y|X)=0$ (conditionally independent random variables have zero conditional mutual information [28]). Hence, $Q_{M C}(M,X,Y)$ achieves the lower bound of zero for the minimization problem $\\mathrm{min}_{Q\\in\\Delta_{P}}\\,I_{Q}(M;Y|X)$ , showing that $Q_{M C}(M,X,Y)$ indeed minimizes (2). A similar argument can be made for the case when $Q_{M C}(M,X,Y)$ has the Markovian structure $M\\rightarrow Y\\rightarrow X$ . Proposition 1 in Appx. A formalizes the above argument. ", "page_idx": 3}, {"type": "text", "text": "The above sufficient condition provides an easy way to analytically calculate the PID terms, as it ensures one of the unique information terms is always zero. Consequently, the remaining PID terms can be calculated by substituting zero for the appropriate unique information term in (1) and solving the resultant linear system. Hence, if applicable, the above sufficient condition considerably simplifies the calculation of PID terms by circumventing the need for optimizing over a set of distributions. Surprisingly, many well-known distribution families allow intuitive constructions of $P(M,X,Y)$ , for which the above sufficient condition is applicable. In the following sections, we provide theorems that specify sufficient conditions under which the existence of these Markov chains in $\\Delta_{P}$ can be guaranteed for these $P(M,X,Y)$ . ", "page_idx": 3}, {"type": "text", "text": "4 Computing PID for stable distributions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we extend existing results for analytical PID computation of jointly Gaussian $M,X$ , and $Y$ [21, 22] to the much larger class of stable distributions. Our results utilize two key observations to provide the generalization of the Gaussian results: (i) We first identify that the analytical computation of PID for jointly Gaussian systems is due to their particular \u201caffine dependence\u201d structure; (ii) We show that these particular affine dependence structures are not unique to Gaussian systems, but rather extend to many members of the stable distribution family. ", "page_idx": 3}, {"type": "text", "text": "For jointly Gaussian $M,X$ , and $Y$ , the conditional distributions $P(X|M){=}\\mathcal{N}(a M+b,\\sigma_{X}^{2})$ and $P(\\bar{Y}|M)\\!=\\!\\mathcal{N}(c M+d,\\sigma_{Y}^{2})$ are also Gaussian distributions, where their means are an affine function of $M$ and their variances are fixed with respect to $M$ . This particular affine dependence of $X$ and $Y$ on $M$ is the key to the analytical calculation of their PID, as it guarantees existence of a Markov chain $Q_{M C}(M,\\dot{X},Y)\\in\\Delta_{P}$ . Thus, we can apply the sufficient condition described in Sec. 3 to compute the PID terms. We illustrate through an example: consider $P(X|M){=}{\\mathcal{N}}(M,\\sigma_{X}^{2})$ and $P(\\dot{Y[M)}{=}\\mathcal{N}(M,\\sigma_{Y}^{2})$ where $\\sigma_{X}^{2}<\\sigma_{Y}^{2}$ , and $M\\sim P(M)$ for some appropriate $P(M)$ . Then, we can explicitly construct $\\bar{Q}_{M C}(M,\\bar{X^{\\prime}}Y)\\in\\Delta_{P}$ with the Markovian structure $M\\rightarrow X\\rightarrow Y$ as follows: Choose $Q_{M C}(M,X,Y){=}P(M)P(X|M)Q_{M C}(Y|X)$ where $Q_{M C}(Y|X)$ is specified through the addition of independent Gaussian noise, i.e., $Y{=}X+\\epsilon$ . Here, $\\epsilon\\sim\\mathcal{N}(0,\\bar{\\sigma}_{Y}^{2}\\!-\\!\\sigma_{X}^{2})$ and $\\epsilon$ \u22a5\u22a5 $(M,X)$ . It is easy to verify that $Q_{M C}(Y|M){=}\\mathcal{N}(M,\\sigma_{Y}^{2})$ , which implies that $Q_{\\bar{M}C}(\\bar{M_{,}^{\\prime}}Y){=}P(M,Y)$ . By construction, $Q_{M C}(M,X){=}{\\cal P}(M,X)$ , and hence, $Q_{M C}(M,X,Y)\\in\\Delta_{P}$ . ", "page_idx": 3}, {"type": "text", "text": "The above example is a special case of a well-known result where, for a scalar $M$ , one can always construct a lower signal-to-noise ratio (SNR) \u201cGaussian channel\u201d, i.e. $P(Y|M)$ in our example, by adding independent Gaussian noise to a higher SNR \u201cGaussian channel\u201d [22, 48], i.e. $P(X{\\bar{|}}M)$ in our example. Surprisingly, the technique of adding independent noise to construct Markov chains contained in $\\Delta_{P}$ can be extended to $\\bar{P}(X|M)$ and $P({\\bar{Y}}|M)$ as members of stable distributions. Theorems 1 and 4 generalize the above construction for $P(X|M)$ and $P(Y|M)$ as members of univariate continuous and univariate discrete stable distributions, respectively. Theorems 2, 3, and 5 consider the case of multivariate stable distributions (see Sec. 2). The key technique for proving these theorems is that a Markov chain $Q_{M C}(M,X,Y)\\in\\Delta_{P}$ can always be constructed by adding appropriate independent noise to a higher SNR $P(X|M)$ to obtain a lower SNR $P(Y|M)$ , similar to our above example. ", "page_idx": 4}, {"type": "text", "text": "4.1 PID of univariate affine continuous stable system ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Theorem 1 can be viewed as a direct generalization of Barrett\u2019s Gaussian PID result [21] to stable distributions, showing one of the UI terms is always zero. We begin by formally describing the univariate affine continuous stable system that generalizes the Gaussian system of Barrett\u2019s [21]. Let the joint distribution of $M,X$ , and $Y$ , denoted as $P(M,X,Y)$ , satisfy the following properties: (i) $M\\overset{\\cdot}{\\sim}P(M)$ with support set ${\\mathcal{M}}\\subseteq\\mathbb{R}$ ; (ii) $P(X|M)$ and $P(Y|M)$ are univariate continuous stable distributions with an affine dependence on $M$ , i.e., $P(X|M)\\!=\\!p_{C S}(\\alpha,\\beta_{X},\\gamma_{X},a M+b)$ and $P(Y|M)\\!=\\!p_{C S}(\\alpha,\\beta_{Y}\\mathrm{sgn}(a c),\\gamma_{Y},\\bar{c}M+d)$ , where $a,b,c,d\\in\\mathbb{R}$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Let $M,X$ , and $Y$ be random variables whose joint distribution $P(M,X,Y)$ is described by a univariate affine continuous stable system. Without the loss of generality, assume $|a|/_{\\gamma_{X}}\\geq|c|/_{\\gamma_{Y}}$ . $\\begin{array}{r}{I\\!\\!\\!\\dot{f}\\,1-\\beta_{Y}\\,\\geq\\,\\left(\\gamma\\dot{x}\\,|c|\\!\\!\\left/\\gamma_{Y}\\,|a|\\right)^{\\alpha}\\left(1-\\beta_{X}\\right)}\\end{array}$ and $1+\\beta_{Y}\\geq\\left(\\gamma_{X}\\lvert c\\rvert\\Big/\\gamma_{Y}\\lvert a\\rvert\\right)^{\\!\\!\\!\\alpha}\\left(1+\\beta_{X}\\right)\\!\\!,$ , then $\\Delta_{P}$ contains $a$ Markov chain of the form $M\\rightarrow X\\rightarrow Y$ and $U I(M;Y\\backslash X)=0$ . ", "page_idx": 4}, {"type": "text", "text": "Proof. See Appx. F for the proof. Here, $|a|/_{\\gamma x}$ and $|c|/\\gamma_{Y}$ are the SNR analogues. ", "page_idx": 4}, {"type": "text", "text": "4.2 PID of multivariate affine continuous stable systems ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We analyze two multivariate generalizations of Theorem 1 in Theorems 2 and 3. Namely, we consider independent component multivariate stable distribution (where all the components of the random vector are independent) and elliptically-contoured multivariate stable distribution (where the p.d.f. is elliptically contoured, similar to multivariate Gaussian distributions). We construct two systems employing these two sub-classes of multivariate continuous stable distributions and show that one of the UI terms is always zero for these systems. For both cases, denote the joint distribution of $M,{\\vec{\\mathbf{X}}}$ , and $\\vec{\\bf Y}$ as $P(M,{\\vec{\\mathbf{X}}},{\\vec{\\mathbf{Y}}})$ , where the support set of $M$ is ${\\mathcal{M}}\\subseteq\\mathbb{R}$ . The dimensions of $M,{\\vec{\\mathbf{X}}}$ , and $\\vec{\\bf Y}$ are 1, $d_{X}$ , and $d_{Y}$ , respectively. ", "page_idx": 4}, {"type": "text", "text": "System 1: The random vectors $\\vec{\\bf X}$ and $\\vec{\\bf Y}$ satisfy the following equations: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\vec{\\bf X}=\\vec{\\bf H}_{X}M+{\\bf A}_{X}\\vec{\\bf Z}_{X}+\\vec{\\bf b}_{X}{\\mathrm{~and~}}\\vec{\\bf Y}=\\vec{\\bf H}_{Y}M+{\\bf A}_{Y}\\vec{\\bf Z}_{Y}+{\\bf b}_{Y}^{\\backprime},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\vec{\\bf Z}_{X}\\ \\sim\\ p_{C S-I C}(\\alpha,\\vec{\\bf0}_{d_{X}},\\vec{\\bf1}_{d_{X}},\\vec{\\bf0}_{d_{X}}),\\ \\vec{\\bf Z}_{Y}\\ \\sim\\ p_{C S-I C}(\\alpha,\\vec{\\bf0}_{d_{Y}},\\vec{\\bf1}_{d_{Y}},\\vec{\\bf0}_{d_{Y}}),\\ {\\bf A}_{X}$ and $\\mathbf{A}_{Y}$ are invertible matrices, $\\vec{\\bf H}_{X},\\vec{\\bf b}_{X}\\in\\mathbb{R}^{d_{X}}$ , and $\\vec{\\bf H}_{Y},\\vec{\\bf b}_{Y}\\in\\mathbb{R}^{d_{Y}}$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 2. Let System $^{\\,l}$ describe the joint distribution $P(M,{\\vec{\\mathbf{X}}},{\\vec{\\mathbf{Y}}})$ of $M,\\,{\\vec{\\mathbf{X}}}$ , and $\\vec{\\bf Y}$ . Without the loss of generality, assume $\\|\\mathbf{A}_{Y}^{-1}\\vec{\\mathbf{H}}_{Y}\\|_{\\kappa}\\,\\leq\\,\\|\\mathbf{A}_{X}^{-1}\\vec{\\mathbf{H}}_{X}\\|_{\\kappa}$ , where $\\kappa\\,=\\,\\alpha/\\alpha\\!-\\!1\\ \\forall\\ \\alpha\\,\\in\\,(1,2]$ and $\\kappa\\;=\\;\\infty\\;\\forall\\;\\alpha\\;\\in\\;(0,1]$ . Then, $\\Delta_{P}$ contains a Markov chain of the form $M\\,\\rightarrow\\,\\vec{\\textbf{X}}\\rightarrow\\,\\vec{\\textbf{Y}}$ and $U I(M;\\vec{\\bf Y}\\backslash\\vec{\\bf X})=0$ . ", "page_idx": 4}, {"type": "text", "text": "Proof. See Appx. G for the proof. Here, $\\lVert\\mathbf{A}_{Y}^{-1}\\vec{\\mathbf{H}}_{Y}\\rVert_{\\kappa}$ and $\\|\\mathbf{A}_{X}^{-1}\\vec{\\mathbf{H}}_{X}\\|_{\\kappa}$ are the SNR analogues. ", "page_idx": 4}, {"type": "text", "text": "System 2: The conditional distribution of $\\vec{\\bf X}$ and $\\vec{\\bf Y}$ conditioned on $M$ are as follows: $P(\\vec{\\bf X}|M)=$ $\\overline{{p_{C S-E C}}}(\\alpha,\\Sigma_{X},\\vec{\\bf H}_{X}M+\\vec{\\bf b}_{X})$ and $P(\\vec{\\bf Y}|M)=p_{C S-E C}(\\alpha,\\Sigma_{Y},\\vec{\\bf H}_{Y}M+\\vec{\\bf b}_{Y})$ , where $\\Sigma_{X}$ and $\\Sigma_{Y}$ are positive definite matrices, $\\vec{\\bf H}_{X},\\vec{\\bf b}_{X}\\in\\mathbb{R}^{d_{X}}$ , and $\\vec{\\bf H}_{Y},\\vec{\\bf b}_{Y}\\in\\mathbb{R}^{d_{Y}}$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 3. Let System 2 describe the joint distribution $P(M,{\\vec{\\mathbf{X}}},{\\vec{\\mathbf{Y}}})$ of $M,{\\vec{\\mathbf{X}}}$ , and $\\vec{\\bf Y}$ . Define $\\Sigma_{X}^{-1/2}$ and $\\Sigma_{Y}^{-1/2}$ as the respective inverses of the matrices $\\Sigma_{X}^{^{1/2}}$ and $\\Sigma_{Y}^{^{1/2}}$ w\u2212hich satisfy: $(\\pmb{\\Sigma}_{X}^{1/2})^{T}\\pmb{\\Sigma}_{X}^{1/2}=\\-\\-\\pmb{\\dot{\\Sigma}}_{X}$ , and $(\\boldsymbol{\\Sigma}_{Y}^{1/2})^{T}\\boldsymbol{\\Sigma}_{Y}^{1/2}=\\boldsymbol{\\Sigma}_{Y}$ . Without the loss of generality, assume $\\lVert\\boldsymbol{\\Sigma}_{Y}^{-1/2}\\vec{\\mathbf{H}}_{Y}\\rVert_{2}\\leq\\lVert\\boldsymbol{\\Sigma}_{X}^{-1/2}\\vec{\\mathbf{H}}_{X}\\rVert_{2}$ . Then, $\\Delta_{P}$ contains a Markov chain of the form $M\\rightarrow\\vec{\\mathbf{X}}\\rightarrow\\vec{\\mathbf{Y}}$ and $U I(M;{\\vec{\\mathbf{Y}}}\\backslash{\\vec{\\mathbf{X}}})=0$ . ", "page_idx": 5}, {"type": "text", "text": "Proof. See Appx. H for the proof. Here, $\\lVert\\boldsymbol{\\Sigma}_{Y}^{-1/2}\\vec{\\mathbf{H}}_{Y}\\rVert_{2}$ and $\\|\\pmb{\\Sigma}_{X}^{-1/2}\\vec{\\mathbf{H}}_{X}\\|_{2}$ are the SNR analogues. ", "page_idx": 5}, {"type": "text", "text": "4.3 PID of univariate affine discrete stable system ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The univariate affine discrete stable system is the discrete counterpart of the univariate affine continuous stable system described in Sec. 4.1. The formal description of the univariate affine discrete stable system is as follows. Let the joint distribution of $M,X$ , and $Y$ , denoted as $P(M,X,Y)$ , satisfy the following properties: (i) $M\\,\\sim\\,P(M)$ with support set ${\\mathcal{M}}\\,\\subseteq\\,(0,\\infty)$ ; (ii) $P(X|M)$ and $P(Y|M)$ are univariate discrete stable distributions with an affine dependence on $M$ , i.e., $P(X|M\\!=\\!m)\\!=\\!P_{D S}(\\nu,a m+b)$ and $P(Y|M{=}m){=}P_{D S}(\\nu,c m+d)$ , where $a,b,c,d\\in(0,\\infty)$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 4. Let $M,X$ , and $Y$ be random variables whose joint distribution $P(M,X,Y)$ is described by a univariate affine discrete stable system. Without the loss of generality, assume $a\\geq c.$ . $H^{a}/b\\geq c/d,$ , then $\\Delta_{P}$ contains a Markov chain of the form $M\\rightarrow X\\rightarrow Y$ and $U I(M;Y\\backslash X)=0$ . ", "page_idx": 5}, {"type": "text", "text": "Proof. See Appx. I for the proof. Here, $a$ and $c$ are the SNR analogues. ", "page_idx": 5}, {"type": "text", "text": "4.4 PID of multivariate linear Poisson system ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The Poisson distribution is the only the discrete stable distribution with a well-known multivariate extension. Hence, we analyze vector-generalizations of Theorem 4 only for the Poisson distribution. We now describe the multivariate linear Poisson system: Let the joint distribution $P(M,{\\vec{\\mathbf{X}}},{\\vec{\\mathbf{Y}}})$ of $M,{\\vec{\\mathbf{X}}}$ , and $\\vec{\\bf Y}$ satisfy the following properties: (i) $M\\sim P(M)$ with support set ${\\mathcal{M}}\\subseteq(0,\\infty)$ ; (ii) $P(\\vec{\\bf X}|M){=}\\,\\mathrm{Poisson}(d_{X},d_{X}^{\\prime},\\vec{\\bf A}_{X})$ and $P(\\vec{\\bf Y}|M){=}\\mathrm{Poisson}(d_{Y},d_{Y}^{\\prime},\\vec{\\bf\\Lambda}_{Y})$ , with: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\vec{\\Lambda}_{X}\\!=\\left[\\lambda_{1}^{X}\\phantom{+}\\!\\!\\!\\dots\\!\\!\\!\\lambda_{d_{X}-(d_{X}^{\\prime}-1)\\dots d_{X}}^{X}\\right]^{T},\\lambda_{i_{1}\\dots i_{j}}^{X}=\\gamma_{i_{1}\\dots i_{j}}^{X}M^{j}\\,\\forall\\,j\\in[d_{X}]\\mathrm{~and~}(i_{1},\\dots,i_{j})\\in\\mathbb{A}_{j}^{d_{X}},}\\\\ &{\\vec{\\Lambda}_{Y}=\\left[\\lambda_{1}^{Y}\\phantom{+}\\!\\!\\!\\dots\\!\\!\\!\\lambda_{d_{Y}-(d_{Y}^{\\prime}-1)\\dots d_{Y}}^{Y}\\right]^{T},\\lambda_{i_{1}\\dots i_{j}}^{Y}=\\gamma_{i_{1}\\dots i_{j}}^{Y}M^{j}\\,\\forall\\,j\\in[d_{Y}]\\mathrm{~and~}(i_{1},\\dots,i_{j})\\in\\mathbb{A}_{j}^{d_{Y}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Theorem 5. Let the joint distribution $P(M,{\\vec{\\mathbf{X}}},{\\vec{\\mathbf{Y}}})$ of $M,\\,{\\vec{\\mathbf{X}}}$ , and $\\vec{\\textbf{Y}}$ be described by the multivariate linear Poisson system defined above. Without the loss of generality, assume $d_{X}^{\\bar{\\prime}}\\geq d_{Y}^{\\prime}$ . If $\\begin{array}{r}{\\sum_{(i_{1},\\dotsc,i_{j})\\in\\mathbb{A}_{j}^{d_{X}}}\\gamma_{i_{1}\\dotsc i_{j}}^{X}\\geq\\sum_{(i_{1},\\dotsc,i_{j})\\in\\mathbb{A}_{j}^{d_{Y}}}\\gamma_{i_{1}\\dotsc i_{j}}^{Y}\\ \\forall\\ j\\in[d_{Y}^{\\prime}],}\\end{array}$ , then $\\Delta_{P}$ contains a Markov chain of the form $M\\rightarrow\\vec{\\mathbf{X}}\\rightarrow\\vec{\\mathbf{Y}}$ and $U I(M;\\vec{\\bf Y}\\backslash\\vec{\\bf X})=0.$ . Proof. See Appx. J for the proof. Here, $\\gamma_{1}^{X},\\dots,\\gamma_{d_{X}-(d_{X}^{\\prime}-1)\\dots d_{X}}^{X},\\gamma_{1}^{Y},\\dots,\\gamma_{d_{Y}-(d_{Y}^{\\prime}-1)\\dots d_{Y}}^{Y}\\in\\mathbb{R}^{+}$ are the SNR analogues. \u53e3 ", "page_idx": 5}, {"type": "text", "text": "5 Computing PID using data thinning and data fission strategies ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Data thinning and data fission are emerging fields in machine learning and statistics dedicated to studying the procedures of splitting a random variable $X$ into $N$ different component random variables. These splitting procedures provide an attractive alternative to the standard splitting of datasets into training, validation, and test splits for performing cross-validation to select statistical model parameters, as they enable users to perform cross-validation even for the extreme case of a dataset containing a single datapoint. We refer the reader to the recent works of Neufeld et al. [25] and Leiner et al. [26] for a more comprehensive discussion on data thinning and fission, respectively. ", "page_idx": 5}, {"type": "text", "text": "The fields of data thinning and data fission share an important theoretical link with analytically calculating PID terms. Specifically, the tools developed for splitting random variables for data thinning and data fission can be readily used to calculate PID terms analytically. To give an intuition, suppose $X$ contains more information about $M$ than $Y$ . Then, we can employ data fission and thinning strategies to decompose $X$ into two components, $f_{1}(X)$ and $f_{2}(X)$ , such that $f_{2}(X)$ follows the same distribution as $Y$ . Then, $f_{2}(X)$ and $Y$ convey the same information about $M$ as they are identically distributed and represent the redundant component of the PID terms. Similarly, $f_{1}(X)$ contains the information uniquely contained in $X$ about $M$ . Thus, $f_{1}(X)$ represents the unique information term. Theorems 6 and 7 utilize the data thinning and data fission proposed in [25, 26] to construct a Markov chain $Q_{M C}(M,X,Y)\\in\\Delta_{P}$ for several systems of random variables $M,X$ , and $Y$ . This allows us to use the sufficient condition discussed in Sec. 3 to compute their PID. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "5.1 PID for convolution-closed distribution based on data thinning strategies of Neufeld et al. ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Neufeld et al. [25] introduces data thinning for a large family of distributions known as convolutionclosed distributions (see Sec. 2). An attractive property of convolution-closed distributions is that they provide a natural way to define a dilation/thinning operation. Formally, let $X\\sim\\ f(\\delta)$ , then we define $X_{\\epsilon}$ as the $\\epsilon$ -dilated version of $X$ if $X_{\\epsilon}\\;\\sim\\;\\bar{f}(\\bar{\\epsilon}\\delta)$ for some $\\epsilon\\,\\in\\,(0,1)$ such that $\\epsilon\\delta\\,\\in\\,\\mathcal{D}$ . Furthermore, if we assume $(1-\\epsilon)\\delta\\in\\mathcal{D}$ , then $P(X_{\\epsilon}|X){=}P(X_{\\epsilon}|X_{\\epsilon}+X_{1-\\epsilon})$ , where $X_{1-\\epsilon}\\;\\sim\\;f((1\\:-\\:\\epsilon)\\delta)$ and $X_{\\epsilon}$ \u22a5\u22a5 $X_{1-\\epsilon}$ (see lemma 19 for a formal justification). We denote $P(X_{\\epsilon}|X=x)=P(X_{\\epsilon}|X_{\\epsilon}+X_{1-\\epsilon}=x)$ as $G(\\epsilon\\delta,(1-\\epsilon)\\delta,x)$ . This dilation operation forms the basis of data thinning, as it enables $X$ to be split into its dilated components $X_{\\epsilon}$ and $X_{1-\\epsilon}$ , such that $X=X_{\\epsilon}+X_{1-\\epsilon}$ , where $X_{\\epsilon}\\perp\\!\\!\\!\\perp X_{1-\\epsilon}$ . We utilize this dilation operation for analytically calculating the PID terms of the following linear convolution-closed system: ", "page_idx": 6}, {"type": "text", "text": "Linear convolution-closed system: Let $\\mathcal{F}_{\\mathcal{D}}$ be a convolution-closed distribution family as described in Sec. 2. The joint distribution $P(M,X,Y)$ of the random variables $M,X$ , and $Y$ describes a linear convolution-closed system if the distributions $P(X|M)$ and $P(Y|M)$ are defined as follows: ", "page_idx": 6}, {"type": "text", "text": "where $\\mathcal{M}$ is the support of $M$ . Furthermore, we assume $\\delta_{m}^{X}=\\gamma\\delta_{m}^{Y}\\,\\forall\\,m\\in\\mathcal{M}$ for some $\\gamma\\in\\mathbb{R}^{+}$ ", "page_idx": 6}, {"type": "text", "text": "Theorem 6. Let the joint density $P(M,X,Y)$ of random variables $M,\\,X$ , and $Y$ be described by $a$ linear convolution-closed system. Without the loss of generality, assume $\\gamma\\le1$ . If ", "page_idx": 6}, {"type": "equation", "text": "$(1-\\gamma)\\delta_{m}^{Y}\\in\\mathcal{D}\\,\\forall\\,m\\in\\mathcal{M},$ ", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "then $\\Delta_{P}$ contains a Markov chain of the form $M\\rightarrow X\\rightarrow Y$ and $U I(M;Y\\backslash X)=0.$ . ", "page_idx": 6}, {"type": "text", "text": "The proof of Theorem 6 is provided in Appx. K. Theorem 6 enables PID calculation for several well-known distributions such as gamma, Poisson, beta etc. (see Appx. C for a (non-exhaustive) list). ", "page_idx": 6}, {"type": "text", "text": "5.2 PID for distributions based on data fission strategies of Leiner et al. ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Leiner et al. [26] propose a \u201cconjugate-prior reversal\u201d strategy for splitting a random variable $X$ into two components, $f(X)$ and $g(X)$ , for certain exponential family distributions. The distributions proposed for performing conjugate-prior reversal provide natural descriptions of $P(M,X,Y)$ (specified in theorem 7) for which PID can be calculated analytically. We briefly describe the distributions used in the conjugate-prior reversal strategy. Let $X\\sim p_{e x p1}(X)$ , where ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p_{e x p1}(X=x;\\theta_{1},\\theta_{2})=H(\\theta_{1},\\theta_{2})\\exp(\\theta_{1}^{T}x-\\theta_{2}^{T}A(x)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for some appropriately defined $H(\\cdot,\\cdot),A(\\cdot),\\theta_{1}$ and $\\theta_{2}$ . Furthermore, define a random variable $Y$ through its conditional density $p(Y|X=x)$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\np(Y=y|X=x;\\theta_{3})=h(y)\\exp\\left(x^{T}T(y)-\\theta_{3}^{T}A(x)\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for some $h(\\cdot),T(\\cdot)$ , and $\\theta_{3}$ , such that $p(Y=y|X=x;\\theta_{1},\\theta_{2})$ is a well-defined distribution. Then, the decomposition terms $f(X)$ and $g(X)$ are $Y$ and $X$ , respectively, in the conjugate-prior reversal strategy. Furthermore, the marginal distribution of $Y$ is expressed as: ", "page_idx": 6}, {"type": "equation", "text": "$$\np_{e x p2}(Y=y;\\theta_{1},\\theta_{2},\\theta_{3})=h(y)H(\\theta_{1},\\theta_{2})/H(\\theta_{1}+T(y),\\theta_{2}+\\theta_{3})\\;\\mathrm{(reverse)}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Theorem 7. Let $M,X$ , and $Y$ be random variables having the joint distribution $P(M,X,Y)$ . Furthermore, the conditional distribution of $X$ and $Y$ conditioned on $M$ are as follows: $P(X|M{=}m){=}p_{e x p1}(X;\\theta_{1}(m),\\theta_{2}(m))$ and $P(Y|M{=}m){=}p_{e x p2}(Y;\\theta_{1}(m),\\theta_{2}(m),\\theta_{3})$ . Then, $\\Delta_{P}$ contains a Markov chain of the form $M\\rightarrow X\\rightarrow Y$ and $U I({\\bar{M}};Y\\backslash X)=0$ . ", "page_idx": 6}, {"type": "text", "text": "The proof of Theorem 7 can be found in Appx. L. The proof essentially stems from observing that the joint distribution $Q_{M C}(M,X,Y){=}P(\\bar{M)}P(X|M)\\bar{Q}_{M C}(Y|X)$ lies in $\\Delta_{P}$ , where $Q_{M C}(Y|X){=}$ $h(y)\\exp\\left(x^{T}T(y)-\\theta_{3}^{\\dot{T}}A(x)\\right)$ . A (non-exhaustive) list of well-known distributions for which theorem 7 is applicable is provided in Appx. D. Leiner et al. [26] also discuss some more strategies for performing data fission that do not follow the conjugate-prior reversal strategy. We provide the corresponding results for computing PID for these remaining data fission strategies and for additional miscellaneous distributions in Appx. E. ", "page_idx": 7}, {"type": "text", "text": "6 Upper bound for convolution-closed distributions ", "text_level": 1, "page_idx": 7}, {"type": "image", "img_path": "7CUUtpDeqN/tmp/a248c9d7bb50796ecd364bc3a3d0f33ece9f5d395186e96a861d9ac202b54c18.jpg", "img_caption": ["Figure 1: a and $\\mathbf{b}$ , respectively, show the box plot of the difference $I_{Q_{A}}(M,X,Y)-I_{Q_{N}}(M,X,Y)$ and the corresponding values of $I_{Q_{N}}(M,\\bar{X_{}}Y)$ for the 20 different function pairs across the 75 different $P(M)$ distributions. The light-blue dots show the corresponding data points used for making the box plots. c shows the ratio of the median difference $I_{Q_{A}}(M,X,Y)-I_{Q_{N}}(M,X,Y)$ and the median value of $I_{Q_{N}}(M,X,Y)$ in percentage, for each function pair. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Several numerical methods, such as [3, 9, 20], approximately solve (2) by considering a smaller constraint set or minimizing an appropriate upper bound for calculating the PID terms. These numerical methods employ general approximations that rely on weak assumptions on the underlying distributions for solving (2), as their goal is to estimate PID for a large class of distributions. However, for specific applications, it is possible to make stronger assumptions on the underlying distributions (e.g., assuming the Poisson distribution for modeling neural spikes). This section illustrates how our theoretical analysis can benefti these numerical algorithms by providing more refined approximations for solving (2) that harness these stronger assumptions. Specifically, we construct an upper bound for the objective of (2) for convolution-closed distributions and show that, under certain assumptions, the upper bound can be analytically minimized over $\\Delta_{P}$ in Sec. 6.1. Note that these upper bounds are applicable for more general cases than our theoretical results, as they do not require the sources $X$ and $Y$ to have an affine dependence on $M$ . Consequently, our upper bound is also applicable in cases where both of the UI terms in the PID are non-zero, unlike our results in Sec. 4 and 5 We demonstrate the tightness of our upper bound through a simulation study in Sec. 6.2. ", "page_idx": 7}, {"type": "text", "text": "Notations and assumptions: We consider the minimization problem $\\mathrm{min}_{Q\\in\\Delta_{P}}\\,I_{Q}(M;[X,Y])$ for deriving the upper bound, as it was shown to be equivalent to (2) in [16]. The distribution $P(M,X,{\\bar{Y}})$ for which we will construct our upper bound is specified as follows. The random vofa rsioabmlee $M$ nhvaosl ustiuopnp-ocrlto soevde r $\\mathcal{M}$ r,i tbhueti coon nfdaitmiiolny $\\mathcal{F}_{\\mathcal{D}}$ ,s tarnibd utthioernes $P(X|M)$ ea $\\delta_{b i a s}^{X},\\delta_{b i a s}^{Y}\\in\\mathcal{D}$ $P(Y|M)$ smuecmh btherast ", "page_idx": 7}, {"type": "text", "text": "$P(X|M{=}m){=}f(\\delta_{m}^{X})$ and $P(Y|M\\!=\\!m)\\!=\\!f(\\delta_{m}^{Y})$ where $\\delta_{m}^{X},\\delta_{m}^{Y}\\in{\\cal D}\\,\\forall\\,m\\in{\\cal M}$ , with $\\big(\\delta_{m}^{Y}-\\delta_{b i a s}^{Y}\\big),\\big(\\delta_{m}^{X}-\\delta_{b i a s}^{X}\\big),\\big(\\delta_{m}^{X}-\\delta_{m}^{Y}-\\big(\\delta_{b i a s}^{X}-\\delta_{b i a s}^{Y}\\big)\\big)\\in\\mathcal{D}$ and $\\delta_{m}^{X}-\\delta_{b i a s}^{X}\\!=\\!\\epsilon_{m}^{(1)}(\\delta_{m}^{Y}\\!-\\delta_{b i a s}^{Y}),$ , $\\delta_{m}^{Y}=\\epsilon_{m}^{(2)}(\\delta_{m}^{Y}-\\delta_{b i a s}^{Y})$ , and $\\delta_{m}^{X}=\\epsilon_{m}^{(3)}(\\delta_{m}^{X}-\\delta_{b i a s}^{X})$ for $\\epsilon_{m}^{(1)},\\epsilon_{m}^{(2)},\\epsilon_{m}^{(3)}\\in[0,1]\\,\\forall\\,m\\in\\mathcal{M}.$ (9) ", "page_idx": 7}, {"type": "text", "text": "The above assumptions ensure that $X$ and $Y$ can always be decomposed into new random variables $X^{\\prime},Y^{\\prime\\prime},n_{X},Y^{\\prime}$ , and $n_{Y}$ (see (10)). We utilize these decomposed random variables to construct our upper bound. Hence, our upper bound is only applicable for systems satisfying (9). The above assumptions describe a large class of systems. For example, in the Poisson case, our upper bound is applicable for any $P(M,X,Y)$ having $P(X|M)\\,=\\,{\\mathrm{Poisson}}(f_{1}(M))$ and $P(Y|M)\\;=\\;$ Poisson $\\langle f_{2}(M)\\rangle$ , as long as $f_{1}(M)\\geq f_{2}(M)$ over $\\mathcal{M}$ . In Appx. C, we provide numerous examples of systems for which (9) holds to illustrate the assumptions of (9). ", "page_idx": 7}, {"type": "text", "text": "First, we consider an arbitrary distribution $Q(M,X,Y)\\quad\\in\\quad\\Delta_{P}$ . Therefore, we know $Q(X|M{=}m){=}P(X|M{=}m){=}\\bar{f}(\\delta_{m}^{X})$ and $Q(Y|M{=}m){=}P(Y|M{=}m){=}f(\\delta_{m}^{Y})$ . We use the dilation properties of convolution-closed distributions (Sec. 5.1) to decompose $X$ and $Y$ into their respective dilated versions: $(X^{\\prime},Y^{\\prime\\prime},n_{X})$ and $(Y^{\\prime},n_{Y})$ . From the results of Appx. M.1, we know that $X=X^{\\prime}+Y^{\\prime\\prime}+n_{X}$ and $Y=Y^{\\prime}+n_{Y}$ . Furthermore, $(X^{\\prime},Y^{\\prime\\prime},n_{X})$ are mutually conditionally independent given $M$ and $Y^{\\prime}\\perp\\!\\!\\!\\perp n_{Y}|M$ . Hence, we can construct the following Markov chain for any arbitrary $\\begin{array}{r}{\\dot{Q}(M,X,Y)\\in\\Delta_{P}}\\end{array}$ : ", "page_idx": 8}, {"type": "equation", "text": "$$\nM\\rightarrow\\left[X^{\\prime}\\quad Y^{\\prime\\prime}\\quad Y^{\\prime}\\quad n_{X}\\quad n_{Y}\\quad\\right]^{T}\\rightarrow\\left[X^{\\prime}+Y^{\\prime\\prime}+n_{X}\\quad Y^{\\prime}+n_{Y}\\right]^{T}=\\left[X\\quad Y\\right]^{T}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "We denote the joint distribution of $\\left(M,X^{\\prime},Y^{\\prime\\prime},Y^{\\prime},n_{X},n_{Y}\\right)$ as $\\bar{Q}(M,X^{\\prime},Y^{\\prime\\prime},Y^{\\prime},n_{X},n_{Y})$ . We appropriately choose the dilation amounts for $X$ and $Y$ such that the respective conditional distributions of ${\\bar{(}}X^{\\prime},Y^{\\prime\\prime},Y^{\\prime},n_{X},n_{Y})$ are as follows: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{Q}(X^{\\prime}|M\\!=\\!m)\\!=\\!f(\\delta_{m}^{X}\\!-\\!\\delta_{m}^{Y}-(\\delta_{b i a s}^{X}\\!-\\!\\delta_{b i a s}^{Y})),\\;\\;\\bar{Q}(Y^{\\prime\\prime}|M\\!=\\!m)=f(\\delta_{m}^{Y}-\\delta_{b i a s}^{Y}),}\\\\ &{\\bar{Q}(Y^{\\prime}|M\\!=\\!m)\\!=\\!f(\\delta_{m}^{Y}-\\delta_{b i a s}^{Y}),\\;\\;\\bar{Q}(n_{X}|M)=f(\\delta_{b i a s}^{X}),\\;\\mathrm{and}\\;\\bar{Q}(n_{Y}|M)=f(\\delta_{b i a s}^{Y}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The distributions in (11) are well-defined due to (9). Appx. M.1 formally shows that a $\\bar{Q}$ , defined as above, exists for each $Q\\in\\Delta_{P}$ . We use data-processing inequality and (10) to conclude: ", "page_idx": 8}, {"type": "equation", "text": "$$\nI_{\\bar{Q}}(M;[X^{\\prime},Y^{\\prime\\prime},n_{X},Y^{\\prime},n_{Y}])\\ge I_{Q}(M;[X,Y])\\lor Q(M,X,Y)\\in\\Delta_{P}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Hence, (12) provides us the desired upper bound for our objective $I_{Q}(M;[X,Y])$ . Furthermore, the minimization problem $\\mathrm{min}_{Q\\in\\Delta_{P}}\\,I_{\\bar{Q}}(M;[X^{\\prime},Y^{\\prime\\prime},n_{X},Y^{\\prime},n_{Y}])$ is analytically solvable, and the minimizing distribution $\\bar{Q}^{*}$ has the structure: ", "page_idx": 8}, {"type": "equation", "text": "$$\nY^{\\prime}=Y^{\\prime\\prime}\\;\\mathrm{and}\\;(n_{X},n_{Y})\\perp\\;\\parallel(M,X^{\\prime},Y^{\\prime}).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The corresponding distribution of $(M,X,Y)$ , denoted as $Q_{A}(M,X,Y)$ , can be found by appropriately manipulating $\\bar{Q}^{*}$ , as $X=X^{\\prime}+Y^{\\prime\\prime}+n_{X}$ and $Y=Y^{\\prime}+n_{Y}$ . The distribution $Q_{A}$ serves as an approximate solution for the problem $\\mathrm{min}_{Q\\in\\Delta_{P}}\\;I_{Q}(M;[X,Y])$ and, consequently, (2). Note that if multiple $\\delta_{b i a s}^{X}$ and $\\delta_{b i a s}^{Y}$ exist satisfying (9), we optimize over the pairs $(\\delta_{b i a s}^{X},\\delta_{b i a s}^{Y})$ to further refine our approximate solution of (2). ", "page_idx": 8}, {"type": "text", "text": "6.2 Simulation study for numerically validating the upper bound ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We illustrate the tightness of our upper bound through a simulation study on the Poisson distribution (a convolution-closed distribution). The reason for choosing the Poisson distribution is two-fold: (i) Poisson distribution is easily approximated as a discrete distribution over finite support, enabling calculation of ground-truth PID terms through numerical solvers such as [10, 17, 18], which are not readily available for continuous distributions; (ii) Many practical applications of PID have been in neuroscience, and the Poisson distribution is frequently used for modeling neural spikes in neuroscientific studies [49]. ", "page_idx": 8}, {"type": "text", "text": "We compare the performance of our analytical estimate $Q_{A}(M,X,Y)$ with the numerical groundtruth estimate $Q_{N}(M,X,Y)$ for the Poisson distribution. The simulation setup is as follows: We choose a $P(M,X,Y)$ such that $P(X|M)={\\mathrm{Poisson}}(f_{1}(M))$ and $P(Y|M)\\,=\\,{\\mathrm{Poisson}}(f_{2}(M))$ . We chose 20 different pairs of non-linear $f_{1}(\\cdot)$ and $f_{2}(\\cdot)$ (enumerated in Table 1 in Appx. M), such that the assumptions in (9) are satisfied. The distribution $P(M)$ is chosen to be a discrete distribution. For each function pair, we compare $Q_{A}(M,X,Y)$ and $Q_{N}(M,X,Y)$ over 75 different distributions of $P(M)$ as shown in Fig. 1a. The 75 distributions are further sub-divided into three groups of 25 based on the number of the outcomes of $M$ , which are 2, 4, and 8. For each of the 75 distributions, the values of $M$ are randomly sampled from [1, 4] and the values of $P(M)$ are randomly sampled from a simplex of appropriate dimensions. We compare the mutual information $I_{Q_{A}}(\\dot{M};[\\bar{X_{}^{}}Y])$ calculated using $Q_{A}$ with the numerical ground-truth $I_{Q_{N}}(M;[X,Y])$ calculated using numerical solvers (Fig. 1b). For calculating $I_{Q_{N}}(M;[X,Y])$ , we approximate the Poisson distribution as a finite discrete distribution by terminating its support at the smallest integer where its cumulative distribution is greater than 0.99. We use the code provided in [50] (under Apache 2.0-license) to numerically solve (2) using this discrete approximation of the Poisson distribution. Fig. 1c demonstrates that the analytical upper bound provided by $Q_{A}$ is very tight for the tested function pairs (within $<1\\%$ of the numerical ground-truth for 16 function pairs) and serves as a good approximation for solving (2). Furthermore, the tightness of our analytical upper bound suggests that $Q_{A}$ might be an analytical solution of (2) for a larger class of systems of $M,\\,X$ , and $Y$ having non-affine dependence on $M$ . Similar results demonstrating the tightness of upper-bound for negative-binomial and binomial distributions are presented in Appx. M.4. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "7 Discussion and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we analytically compute PID for large classes of distributions, greatly expanding upon the analytical result for the Gaussian system. We provide the first known analytical PID for systems employing Poisson, gamma, exponential, Cauchy, beta, Dirichlet, L\u00e9vy-stable, binomial, multinomial, negative binomial, and uniform distributions. Furthermore, we generalize the previous Gaussian PID result [21] in an additional way by showing the target $M$ need not be Gaussian. Our stable distribution results provide the first known analytical computation of PID for fat-tailed distributions (all continuous stable distributions have infinite variance except the Gaussian distribution). A practical utility of our analytical results is that they provide a large test bed in which the performance of numerical PID estimators can be compared and evaluated. This test bed may benefti future works on numerical PID estimation by enabling more comprehensive testing of PID estimators. ", "page_idx": 9}, {"type": "text", "text": "Our results on the Poisson, Cauchy, and binomial are of particular relevance in the neuroscientific context. Poisson and Cauchy distributions are widely used to model neural spikes [49] and network dynamics in the brain [51]. Binomial thinning is a frequently-used operator in neuroscience [49]. Our generalization of the Gaussian result could be helpful in refining approximations already used in computing PID for neural data [3], e.g., by relaxing the assumption of joint Gaussianity. As continuous stable distributions have been shown to better model Magnetic Resonance Imaging data [52, 53], our results may also be helpful in computing PID in this application. ", "page_idx": 9}, {"type": "text", "text": "We also connect the fields of data thinning and data fission with PID by using their decomposition strategies for analytically computing the PID of systems based on convolution-closed distributions (see Sec. 5). Conversely, our PID results also suggest decomposition strategies for data thinning/fission purposes, e.g., our stable distribution results suggest that stable random variables can be decomposed by adding independent noise (similar to the Gaussian case discussed in [26]). Convolution-closed distributions are particularly promising for studying PID as they allow intuitive construction of upper bounds that can be analytically minimized (see Sec. 6). These upper bounds can complement the work on numerical estimation of PID by providing more refined approximations. Another promising avenue is to combine our upper bounds with lower bounds (e.g., from [54]) to create branch and bound algorithms [55] for solving (2). Overall, our analytical results greatly facilitate the computation of PID, either by directly using the analytical expressions or by providing refined approximations for numerical methods. ", "page_idx": 9}, {"type": "text", "text": "Limitations and Future Work: We study PID for univariate $M,X$ , and $Y$ and provide some vector extensions. More vector extensions of our results are a promising direction for subsequent works. Most of our analytical results require $P(X|M)$ and $P(Y|M)$ to depend on some affine functions of $M$ . The existence of analytical solutions for the cases where $P(X|M)$ and $P(Y|M)$ depend on non-affine functions of $M$ remains an open question. The upper bound discussed in Sec. 6 can be further refined with more careful analysis, and more rigorous testing of these upper bounds is required to understand in which cases the upper bound is a good approximation for solving (2). We defer the testing and refinement of these upper bounds for subsequent works, as our primary goal in this work is to study analytical solutions of PID. Niu & Quinn [56] also propose a duality result between the synergistic and redundant components in the Gaussian broadcast and multiple-access channels utilizing the analytical PID expressions of the Gaussian system. It may be possible to derive similar duality results for appropriately defined broadcast and multiple-access channels employing other distributions (e.g., Poisson or Cauchy) with our theoretical results. ", "page_idx": 9}, {"type": "text", "text": "Broader Impact: Due to their theoretical nature, our results\u2019 negative impact primarily depends on how they are used and interpreted. Our theoretical results are applicable only under the specific assumptions outlined in this work, and using these results without ensuring that the theorem\u2019s assumptions are satisfied can lead to incorrect scientific conclusions. We also caution reader against naively using PID to draw causal inferences, as PID is inherently a correlational quantity. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Authors would like to thank Prof. Pulkit Grover, Dr. Praveen Venkatesh, Prof. Alireza Chamanzar, and Neil Ashim Mehta for helpful discussions. This work was supported by National Science Foundation (NSF). Authors declare no competing interests. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Praveen Venkatesh and Pulkit Grover. Understanding encoding and redundancy in grid cells using partial information decomposition. Comput. Syst. Neurosci.(Cosyne), 2020.   \n[2] Ariel K Feldman, Praveen Venkatesh, Douglas J Weber, and Pulkit Grover. Information-theoretic tools to understand distributed source coding in neuroscience. IEEE Journal on Selected Areas in Information Theory, 2024.   \n[3] Praveen Venkatesh, Corbett Bennett, Sam Gale, Tamina Ramirez, Greggory Heller, Severine Durand, Shawn Olsen, and Stefan Mihalas. Gaussian partial information decomposition: Bias correction and application to high-dimensional data. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 74602\u201374635. Curran Associates, Inc., 2023.   \n[4] Nigel Colenbier, Frederik Van de Steen, Lucina Q Uddin, Russell A Poldrack, Vince D Calhoun, and Daniele Marinazzo. Disambiguating the role of blood flow and global signal with partial information decomposition. NeuroImage, 213:116699, 2020.   \n[5] Tjeerd W Boonstra, Luca Faes, Jennifer N Kerkman, and Daniele Marinazzo. Information decomposition of multichannel EMG to map functional interactions in the distributed motor system. NeuroImage, 202:116093, 2019.   \n[6] Jana Krohova, Luca Faes, Barbora Czippelova, Zuzana Turianikova, Nikoleta Mazgutova, Riccardo Pernice, Alessandro Busacca, Daniele Marinazzo, Sebastiano Stramaglia, and Michal Javorka. Multiscale information decomposition dissects control mechanisms of heart rate variability at rest and during physiological stress. Entropy, 21(5):526, 2019.   \n[7] Giuseppe Pica, Eugenio Piasini, Houman Safaai, Caroline Runyan, Christopher Harvey, Mathew Diamond, Christoph Kayser, Tommaso Fellin, and Stefano Panzeri. Quantifying how much sensory information in a neural code is relevant for behavior. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.   \n[8] Praveen Venkatesh, Sanghamitra Dutta, and Pulkit Grover. Information flow in computational systems. IEEE Transactions on Information Theory, 66(9):5456\u20135491, 2020.   \n[9] Paul Pu Liang, Yun Cheng, Xiang Fan, Chun Kai Ling, Suzanne Nie, Richard Chen, Zihao Deng, Nicholas Allen, Randy Auerbach, Faisal Mahmood, Russ R Salakhutdinov, and Louis-Philippe Morency. Quantifying & modeling multimodal interactions: An information decomposition framework. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 27351\u201327393. Curran Associates, Inc., 2023.   \n[10] Paul Pu Liang, Yun Cheng, Ruslan Salakhutdinov, and Louis-Philippe Morency. Multimodal fusion interactions: A study of human and automatic quantification. In Proceedings of the 25th International Conference on Multimodal Interaction, pages 425\u2013435, 2023.   \n[11] Sanghamitra Dutta, Praveen Venkatesh, Piotr Mardziel, Anupam Datta, and Pulkit Grover. An information-theoretic quantification of discrimination with exempt features. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 3825\u20133833, 2020.   \n[12] Sanghamitra Dutta and Faisal Hamman. A review of partial information decomposition in algorithmic fairness and explainability. Entropy, 25(5):795, 2023.   \n[13] Faisal Hamman and Sanghamitra Dutta. Demystifying local & global fairness trade-offs in federated learning using partial information decomposition. In The Twelfth International Conference on Learning Representations, 2024.   \n[14] Faisal Hamman and Sanghamitra Dutta. A unified view of group fairness tradeoffs using partial information decomposition. In 2024 IEEE International Symposium on Information Theory (ISIT), pages 214\u2013219, 2024.   \n[15] Tomas Scagliarini, Luca Faes, Daniele Marinazzo, Sebastiano Stramaglia, and Rosario N Mantegna. Synergistic information transfer in the global system of financial markets. Entropy, 22(9):1000, 2020.   \n[16] Nils Bertschinger, Johannes Rauh, Eckehard Olbrich, J\u00fcrgen Jost, and Nihat Ay. Quantifying unique information. Entropy, 16(4):2161\u20132183, 2014.   \n[17] Abdullah Makkeh, Dirk Oliver Theis, and Raul Vicente. Bivariate partial information decomposition: The optimization perspective. Entropy, 19(10):530, 2017.   \n[18] Pradeep Kr Banerjee, Johannes Rauh, and Guido Mont\u00fafar. Computing the unique information. In 2018 IEEE International Symposium on Information Theory (ISIT), pages 141\u2013145. IEEE, 2018.   \n[19] Abdullah Makkeh, Dirk Oliver Theis, and Raul Vicente. BROJA-2PID: A robust estimator for bivariate partial information decomposition. Entropy, 20(4):271, 2018.   \n[20] Ari Pakman, Amin Nejatbakhsh, Dar Gilboa, Abdullah Makkeh, Luca Mazzucato, Michael Wibral, and Elad Schneidman. Estimating the unique information of continuous variables. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 20295\u201320307. Curran Associates, Inc., 2021.   \n[21] Adam B Barrett. Exploration of synergistic and redundant information sharing in static and dynamical Gaussian systems. Physical Review E, 91(5):052802, 2015.   \n[22] Praveen Venkatesh and Gabriel Schamberg. Partial information decomposition via deficiency for multivariate Gaussians. In 2022 IEEE International Symposium on Information Theory (ISIT), pages 2892\u20132897. IEEE, 2022.   \n[23] John P Nolan. Univariate stable distributions. Springer Series in Operations Research and Financial Engineering, 10:978\u20133, 2020.   \n[24] Fred W Steutel and Klaas van Harn. Discrete analogues of self-decomposability and stability. The Annals of Probability, pages 893\u2013899, 1979.   \n[25] Anna Neufeld, Ameer Dharamshi, Lucy L. Gao, and Daniela Witten. Data thinning for convolution-closed distributions. Journal of Machine Learning Research, 25(57):1\u201335, 2024.   \n[26] James Leiner, Boyan Duan, Larry Wasserman, and Aaditya Ramdas. Data fission: Splitting a single data point. Journal of the American Statistical Association, 0(0):1\u201312, 2023.   \n[27] Paul L Williams and Randall D Beer. Nonnegative decomposition of multivariate information. arXiv preprint arXiv:1004.2515, 2010.   \n[28] Thomas M. Cover and Joy A. Thomas. Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing). Wiley-Interscience, USA, 2006.   \n[29] Malte Harder, Christoph Salge, and Daniel Polani. Bivariate measure of redundant information. Physical Review E, 87(1):012130, 2013.   \n[30] Pradeep Kr Banerjee, Eckehard Olbrich, J\u00fcrgen Jost, and Johannes Rauh. Unique informations and deficiencies. In 2018 56th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 32\u201338. IEEE, 2018.   \n[31] Jim W Kay and Robin AA Ince. Exact partial information decompositions for Gaussian systems based on dependency constraints. Entropy, 20(4):240, 2018.   \n[32] Conor Finn and Joseph T Lizier. Pointwise partial information decomposition using the specificity and ambiguity lattices. Entropy, 20(4):297, 2018.   \n[33] Xueyan Niu and Christopher J Quinn. A measure of synergy, redundancy, and unique information using information geometry. In 2019 IEEE International Symposium on Information Theory (ISIT), pages 3127\u20133131. IEEE, 2019.   \n[34] Fernando E Rosas, Pedro AM Mediano, Borzoo Rassouli, and Adam B Barrett. An operational information decomposition via synergistic disclosure. Journal of Physics A: Mathematical and Theoretical, 53(48):485001, 2020.   \n[35] Keerthana Gurushankar, Praveen Venkatesh, and Pulkit Grover. Extracting unique information through Markov relations. In 2022 58th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 1\u20136. IEEE, 2022.   \n[36] David Blackwell. Equivalent comparisons of experiments. The Annals of Mathematical Statistics, pages 265\u2013272, 1953.   \n[37] Praveen Venkatesh, Keerthana Gurushankar, and Gabriel Schamberg. Capturing and interpreting unique information. In 2023 IEEE International Symposium on Information Theory (ISIT), pages 2631\u20132636. IEEE, 2023.   \n[38] Virgil Griffith and Christof Koch. Quantifying synergistic mutual information. Guided selforganization: inception, pages 159\u2013190, 2014.   \n[39] Joseph T Lizier, Nils Bertschinger, J\u00fcrgen Jost, and Michael Wibral. Information decomposition of target effects from multi-source interactions: Perspectives on previous, current and future work. Entropy, 20(4):307, 2018.   \n[40] John P Nolan. Stable distributions. 2012.   \n[41] Jeremie Fish, Jie Sun, and Erik Bollt. Interaction networks from discrete event data by Poisson multivariate mutual information estimation and information flow with applications from gene expression data. Applied Network Science, 7(1):1\u201322, 2022.   \n[42] AS Krishnamoorthy. Multivariate binomial and Poisson distributions. Sankhy\u00afa: The Indian Journal of Statistics, pages 117\u2013124, 1951.   \n[43] DM Mahamunulu. A note on regression in the multivariate Poisson distribution. Journal of the American Statistical Association, 62(317):251\u2013258, 1967.   \n[44] Dimitris Karlis and Loukia Meligkotsidou. Multivariate Poisson regression with covariance structure. Statistics and Computing, 15(4):255\u2013265, 2005.   \n[45] Murad S Taqqu. Stable non-Gaussian random processes: stochastic models with infinite variance. Chapman & Hall, 1994.   \n[46] S James Press. Multivariate stable distributions. Journal of Multivariate Analysis, 2(4):444\u2013462, 1972.   \n[47] Norman Lloyd Johnson, Samuel Kotz, and Narayanaswamy Balakrishnan. Discrete multivariate distributions, volume 165. Wiley New York, 1997.   \n[48] Xiaohu Shang and H Vincent Poor. Noisy-interference sum-rate capacity for vector Gaussian interference channels. IEEE Transactions on Information Theory, 59(1):132\u2013153, 2012.   \n[49] Peter Dayan and Laurence F Abbott. Theoretical neuroscience: computational and mathematical modeling of neural systems. MIT press, 2005.   \n[50] Abdullah Makkeh, Dirk Oliver Theis, and Raul Vicente. Broja-2pid: A robust estimator for bertschinger et al.\u2019s bivariate partial information decomposition. Entropy, 20(4):271, 2018.   \n[51] Viktoras Pyragas and Kestutis Pyragas. Mean-field equations for neural populations with $\\mathbf{q}$ -Gaussian heterogeneities. Physical Review E, 105(4):044402, 2022.   \n[52] Diego Salas-Gonz\u00e1lez, JM G\u00f3rriz, Javier Ram\u00edrez, Matthias Schloegl, Elmar Wolfgang Lang, and Andr\u00e9s Ortiz. Parameterization of the distribution of white and grey matter in MRI using the $\\alpha$ -stable distribution. Computers in biology and medicine, 43(5):559\u2013567, 2013.   \n[53] Diego Salas-Gonzalez, Juan M G\u00f3rriz, Javier Ram\u00edrez, and Elmar W Lang. Why using the alphastable distribution in NeuroImage? In 2014 International Conference on Signal Processing and Multimedia Applications (SIGMAP), pages 297\u2013301. IEEE, 2014.   \n[54] Seiya Tokui and Issei Sato. Disentanglement analysis with partial information decomposition. arXiv preprint arXiv:2108.13753, 2021.   \n[55] Narendra and Fukunaga. A branch and bound algorithm for feature subset selection. IEEE Transactions on Computers, 100(9):917\u2013922, 1977.   \n[56] Xueyan Niu and Christopher J Quinn. Synergy and redundancy duality between Gaussian multiple access and broadcast channels. In 2020 International Symposium on Information Theory and Its Applications (ISITA), pages 6\u201310. IEEE, 2020.   \n[57] Robert B Ash and Catherine A Dol\u00e9ans-Dade. Probability and measure theory. Academic press, 2000.   \n[58] Gilbert Strang. Introduction to linear algebra. SIAM, 2022.   \n[59] Alan V Oppenheim, Alan S Willsky, Syed Hamid Nawab, and Jian-Jiun Ding. Signals and systems, volume 2. Prentice hall Upper Saddle River, NJ, 1997.   \n[60] Jagadesh Chandramohan and Lung-Kuang Liang. Bernoulli, multinomial and Markov chain thinning of some point processes and some results about the superposition of dependent renewal processes. Journal of Applied Probability, 22(4):828\u2013835, 1985.   \n[61] Sergio Verd\u00fa. Poisson communication theory. International Technion Communication Day in Honor of Israel Bar-David, 66, 1999.   \n[62] Christian Walck et al. Hand-book on statistical distributions for experimentalists. Stockholms universitet, 1996.   \n[63] Kalimuthu Krishnamoorthy. Handbook of statistical distributions with applications. Chapman and Hall/CRC, 2006.   \n[64] Lenka Sl\u00e1mov\u00e1 and Lev B Klebanov. Generalized definitions of discrete stability. arXiv preprint arXiv:1502.02588, 2015. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Proposition 1. For the minimization problem defined in (2), if there exists a joint density $Q_{M C}(M,X,Y)\\in\\Delta_{P}$ , having the Markovian structure of the form $M\\rightarrow X\\rightarrow Y$ or $M\\rightarrow Y\\rightarrow X$ , then $Q_{M C}(M,X,Y)$ minimizes (2). Furthermore, ", "page_idx": 14}, {"type": "text", "text": "Proof. Case 1: $Q_{M C}(M,X,Y)\\in\\Delta_{P}$ having the Markovian structure $M\\rightarrow X\\rightarrow Y$ ", "page_idx": 14}, {"type": "text", "text": "By the definition of the Markov chain, we know that $M$ and $Y$ are conditionally independent given $X$ . Denote $Q_{M C}(M,X,Y)$ as the joint distribution of the Markov chain $M\\rightarrow X\\rightarrow Y$ . Then, ", "page_idx": 14}, {"type": "equation", "text": "$$\nI_{Q_{M C}}(M;Y|X)=0,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $I_{Q_{M C}}(M;Y|X)$ is the conditional mutual information between $M$ and $Y$ given $X$ under the distribution $Q_{M C}(M,X,Y)$ . Equation (13) follows from a well-known property of conditional mutual information which states that $I(A;B|C)=0$ iff $A\\perp\\!\\!\\!\\perp B|C$ [28]. ", "page_idx": 14}, {"type": "text", "text": "Since $I_{Q}(M;Y|X)\\geq0\\,\\forall\\,Q\\in\\Delta_{P}$ , due to the non-negativity of conditional mutual information [28], we can conclude that $\\mathrm{min}_{Q\\in\\Delta_{P}}\\,I_{Q}(M;Y|X)\\geq0$ . The joint distribution $Q_{M C}(M,X,Y)$ having the Markovian structures $M\\rightarrow X\\rightarrow Y$ achieves the lower bound of 0 for the minimization problem $\\mathrm{min}_{Q\\in\\Delta_{P}}\\,I_{Q}(M;Y|X)$ implying $Q_{M C}(M,X,Y)$ minimizes (2). ", "page_idx": 14}, {"type": "text", "text": "Case 2: $Q_{M C}(M,X,Y)\\in\\Delta_{P}$ having the Markovian structure $M\\rightarrow Y\\rightarrow X$ . ", "page_idx": 14}, {"type": "text", "text": "A similar argument as case 1, with the random variables $X$ and $Y$ switched, would allow us to conclude that the joint distribution $Q_{M C}(M,X,Y)$ specifying the Markov chain $M\\rightarrow Y\\rightarrow X$ minimizes (2). \u53e3 ", "page_idx": 14}, {"type": "text", "text": "B Other PID definitions for which theorems 1-7 are valid ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Our theoretical results (Theorems 1-7) hold for a wide range of PID measures. Particularly, our results hold for two broad classes of PID definitions: PID definitions satisfying assumption $(*)$ in Bertschinger et al. [16] and Blackwellian PIDs (see Appendix B-C of Venkatesh & Schamberg [22] for a deeper discussion on differences between these two families of PID definition). Some examples of PID definitions satisfying assumption $(*)$ in Bertschinger et al. [16] are the Williams & Beer\u2019s PID definition [27], the PID definition proposed in Harder et al. [29], and I-PID proposed in Venkatesh et al. [37]. Similarly, an example of a Blackwellian PID is the $\\delta$ -PID discussed in Venkatesh & Schamberg [22]. Section III-F of Venkatesh et al. [37] further discusses a family of Blackwellian PID definitions for which our theorems are also applicable. We now provide formal arguments showing that our theoretical results are applicable for PID definitions that either satisfy assumption $(*)$ of Bertschinger et al. [16] or are Blackwellian ", "page_idx": 14}, {"type": "text", "text": "PID definitions satisfying assumption $(*)$ : We invoke the argument presented in the proof discussed in Section 4.2 of Barrett [21] to show that our results hold for any PID definition satisfying assumption $(*)$ in Bertschinger et al. [16]. Barrett\u2019s argument relies on the key observation that the UI calculated using BROJA-PID upper bounds the UI calculated using any other PID definition satisfying assumption $(*)$ in Bertschinger et al. Entropy\u201914. Consequently, if one of the UI atoms of the BROJA-PID is zero then the corresponding UI atom of the other PID definitions must also be zero due to BROJA PID upper-bounding them and the non-negativity of the PID atoms. Since the remaining PID atoms are calculated using (1), it must be the case that the PID atoms calculated using any PID definition satisfying assumption $(*)$ must be the same as the PID atoms calculated using BROJA-PID whenever one of the UI terms of BROJA-PID is zero. ", "page_idx": 14}, {"type": "text", "text": "Blackwellian PID: The defining feature of a Blackwellian PID is that the UI atom is zero iff it is possible to construct the Markovian structure between the random variables (i.e., $M\\rightarrow X\\rightarrow Y$ or $M\\rightarrow Y\\rightarrow X)$ ), while preserving their pairwise marginals (see Venkatesh et al. [22] for more details). In all our theoretical results, we show that the random variables in the analyzed system admit a Markovian structure, while preserving the pairwise marginal structure. Hence, the UI atom for the systems analyzed in Theorems 1-7 is zero for any Blackwellian PID. Consequently, as the rest of the PID terms are calculated using (1), it must be the case that the PID atoms calculated using any Blackwellian PID must be the same as the PID atoms calculated using BROJA-PID in our results. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "C Lists of Systems that can solved using Theorem 6 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we provide examples of systems of random variables $(M,X,Y)$ employing wellknown distributions whose PID terms can be obtained through theorem 6. The following list is modified from Table 2 of [25]. Note that for the joint distribution $P(M,X,Y)$ of the system of random variable $(M,X,Y)$ , we only specify $P(M)$ , $P(X|M)$ , and $P(Y|M)$ , as any $P(\\dot{M},X,Y)$ having these marginals can be solved through theorem 6. We also discuss some possible extensions of the corresponding systems for which the upper bound discussed in Sec. 6 is applicable. Note that the extension of the systems provided are not exhaustive, and there exists more systems for which the upper bound is applicable. In the following examples, we assume $a>b$ . ", "page_idx": 15}, {"type": "text", "text": "1. System: The distributions of $P(M),P(X|M)$ , and $P(Y|M)$ are as follows: ", "page_idx": 15}, {"type": "text", "text": "(a) $M\\sim P(M)$ having support $(0,\\infty)$ .   \n(b) $P(X|M){=}\\mathrm{Poisson}(a M)$ .   \n(c) $P(Y|M)=\\operatorname{Poisson}(b M)$ .   \nThe corresponding $G(\\gamma\\delta_{m}^{X},(1-\\gamma)\\delta_{m}^{X},x)$ specified in theorem 6 is Binomial $(x,\\gamma)$ with $\\gamma=b/a$ , and $a,b>0$ .   \nResult: The Markov chain $M\\to X\\to Y\\in\\Delta_{P}$ and $U I(M;Y\\backslash X)=0$ .   \nAn extension of the system for which the upper bound of Sec. 6 in applicable: The distributions of $P(M),{\\dot{P}}(X|M)$ , and $P(Y|M)$ are as follows:   \n(a) $M\\sim P(M)$ having support $\\mathcal{M}$ .   \n(b) $P(X|M){=}\\mathbf{Poisson}(f_{1}(M))$ , for $f_{1}(M)>0\\,\\forall\\,m\\in\\mathcal{M}$ .   \n(c) $P(Y|M)=\\operatorname{Poisson}(f_{2}(M))$ , for $f_{2}(M)>0\\,\\forall\\,m\\in\\mathcal{M}$ .   \n(d) $f_{1}(m)\\geq f_{2}(m)\\,\\forall\\,m\\in\\mathcal{M}$ and all the underlying distributions are well-defined. ", "page_idx": 15}, {"type": "text", "text": "2. System: The distributions of $P(M),P(X|M)$ , and $P(Y|M)$ are as follows: ", "page_idx": 15}, {"type": "text", "text": "(a) $M\\sim P(M)$ having support $(-\\infty,\\infty)$ .   \n(b) $P(X|M){=}N(a M,a\\sigma^{2})$ .   \n(c) $P(Y|M)=\\mathcal{N}(b M,b\\sigma^{2})$ .   \nThe corresponding $G(\\gamma\\delta_{m}^{X},(1-\\gamma)\\delta_{m}^{X},x)$ specified in theorem 6 is $\\mathcal{N}(\\gamma x,\\gamma(1-\\gamma)\\sigma^{2})$ with $\\gamma=b/a$ , and $a,b>0$ .   \nResult: The Markov chain $M\\to X\\to Y\\in\\Delta_{P}$ and $U I(M;Y\\backslash X)=0$ . An extension of the system for which the upper bound of Sec. 6 in applicable: The distributions of $P(M),{\\dot{P}}(X|M)$ , and $P(Y|M)$ are as follows:   \n(a) $M\\sim P(M)$ having support $\\mathcal{M}$ .   \n(b) $P(X|M){=}\\mathrm{Gaussian}(f_{1}(M),f_{1}(M)),$ for $f_{1}(M)>0\\,\\forall\\,m\\in\\mathcal{M}.$ .   \n(c) $P(Y|M)=\\operatorname{Gaussian}(f_{2}(M),f_{2}(M))$ , for $f_{2}(M)>0\\,\\forall\\,m\\in\\mathcal{M}$ .   \n(d) $f_{1}(m)\\geq f_{2}(m)\\,\\forall\\,m\\in\\mathcal{M}$ and all the underlying distributions are well-defined. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "3. System: The distributions of $P(M),P(X|M)$ , and $P(Y|M)$ are as follows: ", "page_idx": 15}, {"type": "text", "text": "(a) $M\\sim P(M)$ having support $(0,1]$ .   \n(b) $P(X|M){=}\\mathbf{N}\\,$ egative Binomial $(a,M)$ .   \n(c) $P(Y|M)=\\mathbf{N}\\mathrm{{}}$ egative Binomial $(b,M)$ .   \nThe corresponding $G(\\gamma\\delta_{m}^{X},(1\\mathrm{~-~}\\gamma)\\delta_{m}^{X},x)$ specified in theorem 6 is Beta-Binomial(x, $\\gamma a,(1\\!-\\!\\gamma)\\bar{a})$ with $\\gamma=b/a$ , and $a,b\\in\\mathbb{N}$ .   \nResult: The Markov chain $M\\to X\\to Y\\in\\Delta_{P}$ , and $U I(M;Y\\backslash X)=0$ .   \nAn extension of the system for which the upper bound of Sec. 6 in applicable: The distributions of $P(M),{\\dot{P}}(X|M)$ , and $P(Y|M)$ are as follows:   \n(a) $M\\sim P(M)$ having support $\\mathcal{M}$ . (b) $P(X|M){=}\\mathbf{N}$ egative Binomial $(f_{1}(M),p)$ , for $f_{1}(M)\\in\\mathbb{N}_{0}\\,\\forall\\,m\\in\\mathcal{M}$ and $p\\in[0,1]$ .   \n(c) $P(Y|M){=}\\mathbf{Ne}$ gative Binomial $(f_{2}(M),p)$ , for $f_{2}(M)\\in\\mathbb{N}_{0}\\,\\forall\\,m\\in\\mathcal{M}$ .   \n(d) $f_{1}(m)\\geq f_{2}(m)\\,\\forall\\,m\\in\\mathcal{M}$ and all the underlying distributions are well-defined. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "4. System: The distributions of $P(M),P(X|M)$ , and $P(Y|M)$ are as follows: (a) $M\\sim P(M)$ having support $(0,\\infty)$ . (b) $P(X|M){=}\\mathbf{Gamma}(a,M)$ . (c) $P(Y|M)=\\operatorname{Gamma}(b,M)$ . ", "page_idx": 16}, {"type": "text", "text": "The corresponding $G(\\gamma\\delta_{m}^{X},(1-\\gamma)\\delta_{m}^{X},x)$ specified in theorem 6 is obtained as follows: $G(\\gamma\\delta_{m}^{X},(1_{-}^{'}\\gamma)\\delta_{m}^{\\breve{X}},x){=}x^{\\breve{Z}}$ , $Z\\sim\\mathrm{Beta}(b,(1-b))$ with $\\gamma=b/a$ , and $a,b\\in(0,\\infty)$ . Note that we are following the $\\mathrm{Gamma}(\\alpha,\\beta)$ notation.   \nResult: The Markov chain $M\\to X\\to Y\\in\\Delta_{P}$ and $U I(M;Y\\backslash X)=0$ .   \nAn extension of the system for which the upper bound of Sec. 6 in applicable: The distributions of $P(M),{\\dot{P}}(X|M)$ , and $P(Y|M)$ are as follows:   \n(a) $M\\sim P(M)$ having support $\\mathcal{M}$ .   \n(b) $P(X|M){=}\\mathrm{Gamma}(f_{1}(M),\\beta)$ , for $f_{1}(M)>0\\,\\forall\\,m\\in\\mathcal{M}$ and $\\beta>0$ .   \n(c) $P(Y|M)=\\operatorname{Gamma}(f_{2}(M),\\beta)$ , for $f_{2}(M)>0\\,\\forall\\,m\\in\\mathcal{M}$ .   \n(d) $f_{1}(m)\\geq f_{2}(m)\\,\\forall\\,m\\in\\mathcal{M}$ and all the underlying distributions are well-defined. ", "page_idx": 16}, {"type": "text", "text": "5. System: The distributions of $P(M),P(X|M)$ , and $P(Y|M)$ are as follows: ", "page_idx": 16}, {"type": "text", "text": "(a) $M\\sim P(M)$ having support $(0,\\infty)$ .   \n(b) $P(X|M){=}\\mathrm{Exponential}(M)$ .   \n(c) $P(Y|M)=\\operatorname{Gamma}(b,M).$ . ", "page_idx": 16}, {"type": "text", "text": "The corresponding $G(\\gamma\\delta_{m}^{X},(1-\\gamma)\\delta_{m}^{X},x)$ specified in theorem 6 is obtained as follows: $G(\\gamma\\delta_{m}^{X},(\\dot{1}-\\gamma)\\delta_{m}^{\\breve{X}},x)\\dot{=}x Z$ , $Z\\,\\sim\\,\\mathrm{Beta}(b,(1\\,-\\,b))$ , and $0\\,<\\,b\\,\\leq\\,1$ . Note that we are following the ${\\mathrm{Gamma}}(\\alpha,\\beta)$ notation. ", "page_idx": 16}, {"type": "text", "text": "Result: The Markov chain $M\\to X\\to Y\\in\\Delta_{P}$ and $U I(M;Y\\backslash X)=0$ . ", "page_idx": 16}, {"type": "text", "text": "An extension of the system for which the upper bound of Sec. 6 in applicable: The distributions of $P(M),{\\dot{P}}(X|M)$ , and $P(Y|M)$ are as follows: ", "page_idx": 16}, {"type": "text", "text": "(a) $M\\sim P(M)$ having support $\\mathcal{M}$ .   \n(b) $P(X|M){=}\\mathrm{Exponential}(f_{1}(M)),\\operatorname{f}$ or $f_{1}(M)>0\\,\\forall\\,m\\in\\mathcal{M}$ .   \n(c) $P(Y|M)=\\operatorname{Gamma}(f_{2}(M),f_{1}(M))$ , for $f_{2}(M)>0\\,\\forall\\,m\\in\\mathcal{M}$ .   \n(d) $f_{2}(m)\\leq1\\,\\forall\\,m\\in\\mathcal{M}$ and all the underlying distributions are well-defined. ", "page_idx": 16}, {"type": "text", "text": "6. System: The distributions of $P(M),P(X|M)$ , and $P(Y|M)$ are as follows: ", "page_idx": 16}, {"type": "text", "text": "(a) $M\\sim P(M)$ having support $[0,1]$ .   \n(b) $P(X|M){=}\\mathrm{Binomial}(a,M)$ .   \n(c) $P(Y|M)=\\operatorname{Binomial}(b,M)$ . ", "page_idx": 16}, {"type": "text", "text": "The corresponding $G(\\gamma\\delta_{m}^{X},(1-\\gamma)\\delta_{m}^{X},x)$ specified in theorem 6 is Hypergeometric $(x,$ $\\gamma a,(1-\\gamma)\\bar{a})$ with $\\gamma=b/a$ , and $a,b\\in\\mathbb{N}$ . ", "page_idx": 16}, {"type": "text", "text": "Result: The Markov chain $M\\to X\\to Y\\in\\Delta_{P}$ and $U I(M;Y\\backslash X)=0$ . ", "page_idx": 16}, {"type": "text", "text": "An extension of the system for which the upper bound of Sec. 6 in applicable: The distributions of $P(M),P(X|M)$ , and $P(Y|M)$ are as follows: ", "page_idx": 16}, {"type": "text", "text": "(a) $M\\sim P(M)$ having support $\\mathcal{M}$ .   \n(b) $P(X|M){=}\\mathrm{Binomial}(f_{1}(M),p).$ , for $f_{1}(M)\\in\\mathbb{N}_{0}\\,\\forall\\,m\\in\\mathcal{M}$ and $p\\in[0,1]$ .   \n(c) $P(Y|M)=\\mathrm{Binomial}(f_{2}(M),p)$ , for $f_{2}(M)\\in\\mathbb{N}_{0}\\,\\forall\\,m\\in\\mathcal{M}$ .   \n(d) $f_{1}(m)\\geq f_{2}(m)\\,\\forall\\,m\\in\\mathcal{M}$ and all the underlying distributions are well-defined. ", "page_idx": 16}, {"type": "text", "text": "7. System: The distributions of $P(\\vec{\\bf M}),P(\\vec{\\bf X}|\\vec{\\bf M})$ , and $P(\\vec{\\mathbf{Y}}\\vert\\vec{\\mathbf{M}})$ are as follows: ", "page_idx": 16}, {"type": "text", "text": "(a) $\\vec{\\bf M}\\sim P(\\vec{\\bf M})$ having support over a $k$ -dimensional simplex. (b) $P(\\vec{\\bf X}|\\vec{\\bf M})\\!=\\!{\\bf M u l t i n o m i a l}_{k}(a,\\vec{\\bf M}).$   \n(c) $P(\\vec{\\bf Y}|\\vec{\\bf M})={\\bf M u l t i n o m i a l}_{k}(b,\\vec{\\bf M})$ . ", "page_idx": 16}, {"type": "text", "text": "The corresponding $\\langle G(\\gamma\\delta_{m}^{X},(1-\\gamma)\\delta_{m}^{X},x)$ specified in theorem 6 is Multivariate Hypergeometric $(\\vec{\\bf x},\\gamma_{Y})$ with $\\gamma=b/a$ , and $a,b\\in\\mathbb{N}$ . ", "page_idx": 17}, {"type": "text", "text": "Result: The Markov chain $\\vec{\\bf M}\\rightarrow\\vec{\\bf X}\\rightarrow\\vec{\\bf Y}\\in\\Delta_{P}$ and $U I(\\vec{\\bf M};\\vec{\\bf Y}\\backslash\\vec{\\bf X})=0$ . An extension of the system for which the upper bound of Sec. 6 in applicable: The distributions of $P(M),P(\\vec{\\bf X}|M)$ , and $P(\\vec{\\mathbf{Y}}|M)$ are as follows: ", "page_idx": 17}, {"type": "text", "text": "(a) $M\\sim P(M)$ having support $\\mathcal{M}$ .   \n(b) $P(\\vec{\\mathbf{X}}|M)\\!=\\!\\mathbf{M}\\mathbf{u}\\mathbf{ltinomial}_{k}(f_{1}(M),\\vec{\\mathbf{p}}),\\mathbf{for}\\ f_{1}(M)\\in\\mathbb{N}_{0}\\,\\forall\\,m\\in\\mathcal{M}.$ .   \n(c) $P(\\vec{\\mathbf{Y}}|M)=\\mathbf{Multinomial}(f_{2}(M),\\vec{\\mathbf{p}})$ , for $f_{2}(M)\\in\\mathbb{N}_{0}\\,\\forall\\,m\\in\\mathcal{M}$ .   \n(d) $f_{1}(m)\\,\\geq\\,f_{2}(m)\\;\\forall\\;m\\,\\in\\,\\mathcal{M},\\,\\bar{\\mathtt{P}}$ $\\vec{\\bf p}$ is a valid probability vector, and all the underlying distributions are well-defined. ", "page_idx": 17}, {"type": "text", "text": "D Lists of Systems that can solved using Theorem 7 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we provide examples of systems of random variables $(M,X,Y)$ employing wellknown distributions whose PID terms can be obtained through theorem 7. The following list is appropriately modified from Appendix A of [26]. Similar to Appx. C, we only specify specify $\\dot{P}(M)$ , $P(X|M)$ , and $P(Y|M)$ of the joint distribution $P(M,X,Y)$ , as any $P(M,X,Y)$ having the same marginals can be solved through theorem 7. In the following examples, we assume $a>b$ . ", "page_idx": 17}, {"type": "text", "text": "1. System: The distributions of $P(M),P(X|M)$ , and $P(Y|M)$ are as follows: ", "page_idx": 17}, {"type": "text", "text": "(a) $M\\sim P(M)$ having support $(0,\\infty)$ .   \n(b) $P(X|M){=}\\mathrm{Exponential}(M)$ .   \n(c) $P(Y|M)=\\mathrm{Geometric}\\big(M\\big/(\\tau\\!+\\!M)\\big)$ , where $\\tau\\in(0,\\infty)$ . ", "page_idx": 17}, {"type": "text", "text": "Results: The Markov chain $M\\to X\\to Y\\in\\Delta_{P}$ , and $U I(M;Y\\backslash X)=0$ . The corresponding $Q_{M C}(Y|X)$ required for constructing the Markov chain is $\\operatorname{Poisson}(\\tau X)$ . ", "page_idx": 17}, {"type": "text", "text": "2. System: The distributions of $P(M),P(X|M)$ , and $P(Y|M)$ are as follows: (a) $M\\sim P(M)$ having support $\\mathcal{M}$ . (b) $P(X|M){=}\\mathrm{Gamma}(f_{1}(M),f_{2}(M))$ , where $f_{1}(m),f_{2}(m)>0\\,\\forall\\,m\\in\\mathcal{M}$ . (c) $P(Y|M)=\\mathbf{N}\\mathrm{{}}$ egative Binomial ${\\big(}f_{1}(M),f_{2}(M){\\big/}(f_{2}(M)\\!+\\!\\tau){\\big)}$ , where $\\tau\\in(0,\\infty)$ ", "page_idx": 17}, {"type": "text", "text": "Results: The Markov chain $M\\to X\\to Y\\in\\Delta_{P}$ , and $U I(M;Y\\backslash X)=0$ . The corresponding $Q_{M C}(Y|X)$ required for constructing the Markov chain is is $\\mathrm{Poisson}(\\tau X)$ . ", "page_idx": 17}, {"type": "text", "text": "3. System: The distributions of $P(M),P(X|M)$ , and $P(Y|M)$ are as follows: (a) $M\\sim P(M)$ having support $(0,\\infty)$ . (b) $P(X|M){=}\\mathrm{Beta}(M,1)$ . (c) $P(Y|M)$ is a discrete distribution with over support set $\\{0,1,\\ldots,N\\}$ , with ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "equation", "text": "$$\nP(Y=y|M=m)=m\\frac{\\Gamma(y+m)N!}{\\Gamma(N+1+m)y!},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\Gamma(.)$ is the Gamma function. ", "page_idx": 17}, {"type": "text", "text": "Results: The Markov chain $M\\to X\\to Y\\in\\Delta_{P}$ , and $U I(M;Y\\backslash X)=0$ . The corresponding $Q_{M C}(Y|X)$ required for constructing the Markov chain is Binomial $(N,X)$ . ", "page_idx": 17}, {"type": "text", "text": "4. System: The distributions of $P(M),P(X|M)$ , and $P(Y|M)$ are as follows: (a) $M\\sim P(M)$ having support $(0,\\infty)$ . (b) $P(X|M){=}\\mathrm{Beta}(1,M)$ . (c) $P(Y|M)$ is a discrete distribution with over support set $\\{0,1,\\ldots,N\\}$ , with ", "page_idx": 17}, {"type": "equation", "text": "$$\nP(Y=y|M=m)=m\\frac{\\Gamma(y+m)N!}{\\Gamma(N+1+m)y!},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\Gamma(.)$ is the Gamma function. ", "page_idx": 17}, {"type": "text", "text": "Results: The Markov chain $M\\to X\\to Y\\in\\Delta_{P}$ , and $U I(M;Y\\backslash X)=0$ . The corresponding $Q_{M C}(Y|X)$ required for constructing the Markov chain is is Binomial $(N,1-\\bar{X)}$ . ", "page_idx": 17}, {"type": "text", "text": "5. System: The distributions of $P(\\vec{\\bf M}),P(\\vec{\\bf X}|\\vec{\\bf M})$ , and $P(\\vec{\\mathbf{Y}}\\vert\\vec{\\mathbf{M}})$ are as follows: ", "page_idx": 18}, {"type": "text", "text": "(a) $\\vec{\\bf M}\\sim P(\\vec{\\bf M})$ having support $(0,\\infty)^{d}$ for some dimension $d$ .   \n(b) $P(\\vec{\\mathbf{X}}|\\vec{\\mathbf{M}}){=}\\mathrm{Dirichlet}(M_{1},\\ldots,M_{d})$ .   \n(c) $P(\\vec{\\mathbf{Y}}|\\vec{\\mathbf{M}})$ is a discrete distribution having support $\\{y_{i}\\;\\;\\in\\;\\;\\{1,...\\,,N\\}\\;\\;\\forall\\;\\;i\\;\\;\\in$ $[d]|\\sum_{i=1}^{d}y_{i}=N\\}$ , and ", "page_idx": 18}, {"type": "equation", "text": "$$\nP(\\vec{\\bf Y}|\\vec{\\bf M})=\\frac{\\Gamma\\left(\\sum_{i=1}^{d}M_{i}\\right)N!}{\\Gamma\\left(\\sum_{i=1}^{d}M_{i}+N\\right)}\\prod_{i=1}^{d}\\frac{\\Gamma\\left(M_{i}+Y_{i}\\right)}{\\Gamma\\left(M_{i}\\right)Y_{i}!},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\Gamma(.)$ is the Gamma function. ", "page_idx": 18}, {"type": "text", "text": "Results: The Markov chain $\\vec{\\bf M}\\to\\vec{\\bf X}\\to\\vec{\\bf Y}\\in\\Delta_{P}$ , and $U I(\\vec{\\bf M};\\vec{\\bf Y}\\backslash\\vec{\\bf X})=0$ . The corresponding $Q_{M C}(\\vec{\\bf Y}|\\vec{\\bf X})$ required for constructing the Markov chain is Multinomial ${\\bf\\Gamma}_{d}(N,\\vec{\\bf X})$ . ", "page_idx": 18}, {"type": "text", "text": "E Miscellaneous PID Results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "This section explores the analytical calculation of PID terms for five more systems of random variables $(M,X,Y)$ with joint distribution $P(M,X,Y)$ . These five systems\u2019 $\\dot{P}(M,X,Y)$ employ wellknown distributions, but their respective analytical PID calculation does not result from Theorem 1- Theorem 7. Of note are the results of $P(M,{\\dot{X}},Y)$ employing exponential and uniform distribution, which states that these systems will always have both unique information terms zero. ", "page_idx": 18}, {"type": "text", "text": "E.1 PID for the miscellaneous system M1. ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Miscellaneous system M1: Let $(M,X,Y)$ be a system of random variables with joint distribution $\\overline{{P(M,X,Y)}}$ . The joint distribution $P(M,X,Y)$ describes the miscellaneous system M1 if it satisfies the following two properties: ", "page_idx": 18}, {"type": "text", "text": "1. $M\\sim P(M)$ , and has support $(0,\\infty)$ .   \n2. P(X|M)=Exponential $(\\gamma_{X}M)$ , and $P(Y|M){=}\\mathrm{Exponential}(\\gamma_{Y}M)$ , where $\\gamma_{X},\\gamma_{Y}\\;\\;\\in$ $(0,\\infty)$ . ", "page_idx": 18}, {"type": "text", "text": "Proposition 2. Let $M,X$ , and $Y$ be random variables whose joint distribution $P(M,X,Y)$ describes the miscellaneous system M1 defined above. Then, $\\Delta_{P}$ contains a distribution $Q_{M C}(M,X,Y)$ , which admits both Markovian structures, i.e., $M\\rightarrow X\\rightarrow Y$ and $M\\rightarrow Y\\rightarrow X$ . Consequently, both $U I(M;Y\\backslash X)=U I(M;X\\backslash Y)=0$ . ", "page_idx": 18}, {"type": "text", "text": "Proof. We prove proposition 2 by explicitly constructing a joint distribution $Q_{M C}(M,X,Y)\\in\\Delta_{P}$ that admits both Markovian structures, namely, $M\\rightarrow X\\rightarrow Y$ and $M\\rightarrow Y\\rightarrow X$ . A necessary and sufficient condition for the distribution $Q_{M C}(M,X,Y)$ to admit both Markovian structures is that it should satisfy (14). ", "page_idx": 18}, {"type": "equation", "text": "$$\nQ_{M C}(M,X,Y)\\!=\\!Q_{M C}(M)Q_{M C}(X|M)Q_{M C}(Y|X)\\!=\\!Q_{M C}(M)Q_{M C}(Y|M)Q_{M C}(X|Y).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We now briefly outline the structure of the proof. The rest of the proof can be divided into three parts: ", "page_idx": 18}, {"type": "text", "text": "1. In the first part, we explicitly construct $Q_{M C}(M,X,Y)$ .   \n2. In the second part, we show that the $Q_{M C}(M,X,Y)$ constructed in the first part lies in $\\Delta_{P}$ .   \n3. In the third part, we conclude our proof by showing that $Q_{M C}(M,X,Y)$ satisfies (14) and, consequently, admits both Markovian structures. ", "page_idx": 18}, {"type": "text", "text": "Part 1: Specifying $Q_{M C}(M,X,Y)$ ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We specify $Q_{M C}(M,X,Y)$ by individually specifying $Q_{M C}(M),Q_{M C}(X|M)$ , and $Q_{M C}(Y|X)$ . We choose $Q_{M C}(M)$ and $Q_{M C}(X|M)$ as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\nQ_{M C}(M)=P(M),\\mathrm{~and~}Q_{M C}(X|M)=P(X|M).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The distribution $Q_{M C}(Y|X)$ is specified through the following deterministic transformation: ", "page_idx": 19}, {"type": "equation", "text": "$$\nY={\\frac{\\gamma_{X}}{\\gamma_{Y}}}X.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Part 2: Showing $Q_{M C}(M,X,Y)\\in\\Delta_{P}$ ", "page_idx": 19}, {"type": "text", "text": "For showing $Q_{M C}(M,X,Y)\\in\\Delta_{P}$ , we first need to calculate $Q_{M C}(Y|M)$ : ", "page_idx": 19}, {"type": "text", "text": "Calculating $Q_{M C}(Y|M)$ : By lemma 1, if $X\\,\\sim\\,\\mathrm{Exp}(\\lambda)$ , then $a X\\,\\sim\\,\\mathrm{Exponential}(\\lambda/a)$ . Since $Q_{M C}(X|M){=}\\mathrm{E}$ xponential $(\\gamma_{X}M)$ , we can use the result of lemma 1 to calculate $Q_{M C}(Y|M)$ as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{Q_{M C}(Y|M)=Q_{M C}\\,\\left(\\left.{\\frac{\\gamma_{X}}{\\gamma_{Y}}}X\\right|M\\right)={\\mathrm{Exponential}}\\left({\\frac{\\gamma_{Y}}{\\gamma_{X}}}\\gamma_{X}M\\right)}\\\\ &{\\qquad\\qquad={\\mathrm{Exponential}}\\left(\\gamma_{Y}M\\right)=P(Y|M).}\\end{array}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, from (15) and (17), we can conclude: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{M C}(M,X)=Q_{M C}(M)Q_{M C}(X|M)=P(M)P(X|M)=P(M,X),}\\\\ &{Q_{M C}(M,Y)=Q_{M C}(M)Q_{M C}(Y|M)=P(M)P(Y|M)=P(M,Y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence, $Q_{M C}(M,X,Y)\\in\\Delta_{P}$ . ", "page_idx": 19}, {"type": "text", "text": "Part 3: Showing $Q_{M C}(M,X,Y)$ satisfies (14) ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "It is trivial to see that by the construction of $Q_{M C}(M,X,Y)$ given in Part 1, we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\nQ_{M C}(M,X,Y)=Q_{M C}(M)Q_{M C}(X|M)Q_{M C}(Y|X).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By the chain rule, we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\nQ_{M C}(M,X,Y)=Q_{M C}(M)Q_{M C}(Y|M)Q_{M C}(X|Y,M).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence, for showing that $Q_{M C}(M,X,Y)$ satisfies (14), we only need to show that $Q_{M C}(X|Y,M)=Q_{M C}^{'}(X|Y)$ . ", "page_idx": 19}, {"type": "text", "text": "Showing $Q_{M C}(X|Y,M)=Q_{M C}(X|Y)$ : Observe that $Y=\\left.\\gamma_{X}\\right/_{\\gamma_{Y}}X$ by construction. Therefore, we can equivalently express $X=\\left.\\gamma_{Y}\\right/_{\\gamma_{X}}Y$ . Consequently, ", "page_idx": 19}, {"type": "equation", "text": "$$\nQ_{M C}(X|Y,M)=\\mathbb{I}[X={\\gamma}{\\boldsymbol{\\gamma}}_{X}Y]{\\mathrm{~as~}}X={\\gamma}{\\boldsymbol{\\gamma}}_{X}Y,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\mathbb{I}[\\cdot]$ is the indicator function. Since R.H.S of the above equation does not depend on $M$ , we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\nQ_{M C}(X|Y,M)=\\mathbb{I}[X=\\gamma_{Y\\left/\\gamma_{X}}Y]=Q_{M C}(X|Y).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Combining (21) and (23), we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\nQ_{M C}(M,X,Y)=Q_{M C}(M)Q_{M C}(Y|M)Q_{M C}(X|Y).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Combining (20) and (24), we can show that $Q_{M C}(M,X,Y)$ satisfies (14). ", "page_idx": 19}, {"type": "text", "text": "Conclusion: Combining the results of parts 2 and 3, we can conclude that the $Q_{M C}(M,X,Y)\\in$ $\\Delta_{P}$ and contains both Markovian structure $M\\ \\rightarrow\\ X\\ \\rightarrow\\ Y$ and $M\\ \\rightarrow\\ Y\\ \\rightarrow\\ X$ . A unique implication of $Q_{M C}(M,X,Y)$ admitting both Markovian structures is that both $I_{Q_{M C}}(M;X|Y)=$ $\\bar{I_{Q_{M C}}}(M;Y|X)=\\dot{0}$ . Consequently, both minimization problems presented in (2) are minimized by $Q_{M C}(M,X,Y)$ with $\\begin{array}{r}{U I(M;X\\backslash Y)\\;=\\;\\operatorname*{min}_{Q\\in\\Delta_{P}}I_{Q}(M;X|Y)\\;=\\;0}\\end{array}$ and $U I(M;Y\\backslash X)\\;=$ $\\mathrm{min}_{Q\\in\\Delta_{P}}\\,I_{Q}(M;Y|X)=0$ , completing our proof. \u53e3 ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\cal I}f\\,X\\sim E x p o n e n t i a l{(\\lambda)}\\,{w i t h}\\,\\lambda\\in(0,\\infty),t h e n\\,\\gamma X\\sim E x p o n e n t i a l{(\\lambda/\\gamma)}\\,f o r\\,\\gamma\\in(0,\\infty).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. We denote the p.d.f. of the random variable $X$ as $p_{X}(\\cdot)$ and, by lemma statement, we know: ", "page_idx": 19}, {"type": "equation", "text": "$$\np_{X}(X=x)=\\operatorname{Exponential}(\\lambda)={\\left\\{\\begin{array}{l l}{\\lambda e^{-x\\lambda}}&{~~x\\geq0}\\\\ {~~0}&{{\\mathrm{otherwise}}}\\end{array}\\right.}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Denote the p.d.f. of the function $f(X)$ of the random variable $X$ as $p_{f(X)}(\\cdot)$ . We want to derive $p_{f(X)}(\\cdot)$ for $f(X)=\\gamma X$ . Since, $f(X)=\\gamma X$ is an invertible function, we can use the change-ofvariable technique for deriving $p_{f(X)}(\\cdot)$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{f(X)}(f(X)=y)=p_{X}(X=f^{-1}(y))\\left|\\frac{d f^{-1}(y)}{d y}\\right|=\\left\\{\\begin{array}{c c}{\\frac{\\lambda}{\\gamma}e^{-y\\lambda/\\gamma}}&{y\\geq0}\\\\ {0}&{\\mathrm{otherwise}}\\end{array}\\right.}\\\\ &{\\qquad\\qquad\\qquad=\\mathrm{Exponential}(\\lambda/\\gamma).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "E.2 PID for the miscellaneous system M2. ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Miscellaneous System M2: Let $(M,X,Y)$ be a system of random variables with joint distribution $\\overline{{P(M,X,Y)}}$ . The joint distribution $P(M,X,Y)$ describes the miscellaneous system M2 if it satisfies the following two properties: ", "page_idx": 20}, {"type": "text", "text": "1. $M\\sim P(M)$ and has support $(0,\\infty)$ .   \n2. $P(X|M){=}^{\\frac{}{}}$ Negative Binomial $(M,p_{X})$ , and $P(Y|M){=}]$ Negative Binomial $(M,p_{Y})$ . ", "page_idx": 20}, {"type": "text", "text": "This system is adapted from the negative binomial decomposition listed in Appendix A [26]. ", "page_idx": 20}, {"type": "text", "text": "Proposition 3. Let $M,X$ , and $Y$ be random variables whose joint distribution $P(M,X,Y)$ describes the miscellaneous system $M2$ defined above. ", "page_idx": 20}, {"type": "text", "text": "Proof. We only provide an explicit proof of condition 1. The proof of condition 2 follows the same steps as the proof of condition 1 with parameters of $X$ and $Y$ switched. ", "page_idx": 20}, {"type": "text", "text": "Proof of condition 1: We briefly outline the proof structure. The proof can be divided into two parts: ", "page_idx": 20}, {"type": "text", "text": "1. In the first part, we explicitly construct a joint distribution $Q_{M C}(M,X,Y)$ having the Markovian structure $M\\rightarrow X\\rightarrow Y$ .   \n2. In the second part, we conclude our proof by showing the $Q_{M C}(M,X,Y)$ constructed in the first part, lies in $\\Delta_{P}$ . ", "page_idx": 20}, {"type": "text", "text": "Part 1: Specifying $Q_{M C}(M,X,Y)$ ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Since $Q_{M C}(M,X,Y)$ is the joint distribution of a Markov chain $M\\ \\rightarrow\\ X\\ \\rightarrow\\ Y$ , we have $Q_{M C}(M,X,Y)=Q_{M C}(M)Q_{M C}(X|M)Q_{M C}(Y|X)$ . Taking advantage of this special structure of $Q_{M C}(M,X,Y)$ , we specify $Q_{M C}(M,X,Y)$ by individually specifying $Q_{M C}(M),Q_{M C}(X|M)$ , and $Q_{M C}(Y|X)$ . We choose $Q_{M C}(M),Q_{M C}(X|M)$ and $Q_{M C}(Y|X)$ as follows: ", "page_idx": 20}, {"type": "equation", "text": "$Q_{M C}(M)=P(M),Q_{M C}(X|M)=P(X|M)$ ", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Part 2: Showing $Q_{M C}(M,X,Y)\\in\\Delta_{P}$ ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "For showing $Q_{M C}(M,X,Y)\\in\\Delta_{P}$ , we first need to calculate $Q_{M C}(Y|M)$ : ", "page_idx": 20}, {"type": "text", "text": "Calculating $Q_{M C}(Y|M)$ : Since $Q_{M C}(X|M)={\\mathrm{Negative~Binomial}}(M,p_{X})$ and $Q_{M C}(Y|X)=$ Binomial $(N,p)$ , we can use lemma 2 for calculating $Q(Y|M)$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\nQ_{M C}(Y|M)={\\mathrm{Negative~Binomial}}(M,p_{Y})=P(Y|M).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then, from (27) and (28), we can conclude: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{M C}(M,X)=Q_{M C}(M)Q_{M C}(X|M)=P(M)P(X|M)=P(M,X),}\\\\ &{Q_{M C}(M,Y)=Q_{M C}(M)Q_{M C}(Y|M)=P(M)P(Y|M)=P(M,Y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Hence, $Q_{M C}(M,X,Y)\\;\\in\\;\\Delta_{P}$ and consequently, by proposition 1, $U I(M;Y\\backslash X)\\,=\\,0$ . This concludes our proof. $\\sqsupset$ ", "page_idx": 21}, {"type": "text", "text": "Lemma 2. Let $\\textit{X}\\sim$ Negative Binomia $\\left(N,p_{X}\\right)$ and $P(Y|X)\\ =\\ B i n o m i a l(X,p)$ with $p\\ =$ ppYX ((11\u2212\u2212ppYX )), pY \u2265pX, pY , pX \u2208[0, 1], and N \u2208N0. Then, Y \u223cNegative Binomial(N, pY ). ", "page_idx": 21}, {"type": "text", "text": "Proof. First, in order for $P(Y|X)$ to be a legitimate distribution, we need to ensure that $p\\in[0,1]$ , i.e., we need to show that: ", "page_idx": 21}, {"type": "equation", "text": "$$\n0\\leq p\\leq1.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since $p_{X},(1-p_{Y}),(1-p_{X}),p_{Y}\\geq0$ as $p_{X},p_{Y}\\in[0,1]$ , we can easily verify the first inequality: ", "page_idx": 21}, {"type": "equation", "text": "$$\np=\\frac{p_{X}(1-p_{Y})}{(1-p_{X})p_{Y}}\\geq0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For verifying $p\\leq1$ , we will use the assumption that $p_{Y}\\geq p_{X}$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\np_{X}\\leq p_{Y}\\Rightarrow\\frac{1}{p_{Y}}\\leq\\frac{1}{p_{X}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Subtracting 1 from both sides of the above inequality. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{p_{Y}}-1\\leq\\frac{1}{p_{X}}-1\\Rightarrow\\frac{1-p_{Y}}{p_{Y}}\\leq\\frac{1-p_{X}}{p_{X}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Rearranging terms, we obtain: ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\frac{p_{X}(1-p_{Y})}{(1-p_{X})p_{Y}}}\\leq1\\Rightarrow p\\leq1\\left({\\mathrm{as~}}p={\\frac{p_{X}(1-p_{Y})}{(1-p_{X})p_{Y}}}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For calculating $P(Y)$ , we borrow the result of proposition 13 in [26], which states that if $\\begin{array}{r l}{X}&{{}\\sim}\\end{array}$ Negative Binomial $(M,p_{X})$ and $P(Y|X)\\quad=\\quad\\operatorname{Binomial}(X,p)$ , then $\\begin{array}{r l}{P(Y)}&{{}=}\\end{array}$ Negative Binomial N,pX+ppX\u2212ppX . Hence, ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{P(Y)=\\operatorname{Negative~Bimomial}\\left(N,{\\frac{p_{X}}{p_{X}+p-p_{Y}}}\\right)=\\operatorname{Negative~Bimomial}\\left(N,{\\frac{p_{X}}{p_{X}+p(1-p_{X})}}\\right)}\\\\ &{\\qquad=\\operatorname{Negative~Bimomial}\\left(N,{\\frac{p_{X}}{p_{X}+{\\frac{p_{X}}{(1-p_{X})p_{Y}}}(1-p_{X})}}\\right)}\\\\ &{\\qquad=\\operatorname{Negative~Bimomial}\\left(N,{\\frac{p_{X}(1-p_{X})}{p_{X}+{\\frac{p_{X}}{p_{Y}}}(1-p_{Y})}}\\right)}\\\\ &{\\qquad=\\operatorname{Negative~Bimomial}\\left(N,{\\frac{p_{X}(1-p_{Y})}{p_{X}+{\\frac{p_{X}}{p_{Y}}}(1-p_{Y})}}\\right)}\\\\ &{\\qquad=\\operatorname{Negative~Bimomial}\\left(N,{\\frac{p_{X}p_{Y}}{p_{X}p_{Y}+p_{X}(1-p_{Y})}}\\right)}\\\\ &{\\qquad=\\operatorname{Negative~Bimomial}\\left(N,{\\frac{p_{X}p_{Y}}{p_{X}}}\\right)=\\operatorname{Negative~Bimomial}\\left(N,p_{Y}\\right).}\\end{array}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "E.3 PID for the miscellaneous system M3. ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Miscellaneous System M3: Let $(M,X,Y)$ be a system of random variables with joint distribution $\\overline{{P(M,X,Y)}}$ . The joint distribution $P(M,X,Y)$ describes the miscellaneous system M3 if it satisfies the following two properties: ", "page_idx": 22}, {"type": "text", "text": "1. $M\\sim P(M)$ and has support $(0,\\infty)$ .   \n2. $P(X|M){=}\\mathrm{Uniform}(0,\\gamma_{X}M)$ and $P(Y|M){=}\\mathrm{Uniform}(0,\\gamma_{Y}M)$ , where $\\gamma_{X},\\gamma_{Y}$ $\\mathbb{R}\\backslash\\{0\\}$ . ", "page_idx": 22}, {"type": "text", "text": "Proposition 4. Let $M,X$ , and $Y$ be random variables whose joint distribution $P(M,X,Y)$ describes the miscellaneous system $M3$ defined above. Then, $\\Delta_{P}$ contains a distribution $Q_{M C}(M,X,Y)$ , which admits both Markovian structures, i.e., $M\\rightarrow X\\rightarrow Y$ and $M\\rightarrow Y\\rightarrow X$ . Consequently, both $U I(M;Y\\backslash X)=U I(M;X\\backslash Y)=0$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. We prove proposition 4 by explicitly constructing a joint distribution $Q_{M C}(M,X,Y)\\in\\Delta_{P}$ that admits both Markovian structures, namely, $M\\rightarrow X\\rightarrow Y$ and $M\\rightarrow Y\\rightarrow X$ . A necessary and sufficient condition for the distribution $Q_{M C}(M,X,Y)$ to admit both Markovian structures is that it should satisfy (37): ", "page_idx": 22}, {"type": "equation", "text": "$$\nQ_{M C}(M,X,Y)\\!=\\!Q_{M C}(M)Q_{M C}(X|M)Q_{M C}(Y|X)\\!=\\!Q_{M C}(M)Q_{M C}(Y|M)Q_{M C}(X|Y).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We now briefly outline the structure of the proof. The rest of the proof can be divided into three parts: ", "page_idx": 22}, {"type": "text", "text": "1. In the first part, we explicitly construct $Q_{M C}(M,X,Y)$ .   \n2. In the second part, we show that the $Q_{M C}(M,X,Y)$ constructed in the first part, lies in $\\Delta_{P}$ .   \n3. In the third part, we conclude our proof by showing that $Q_{M C}(M,X,Y)$ satisfies (37) and, consequently, admits both Markovian structures. ", "page_idx": 22}, {"type": "text", "text": "Part 1: Specifying $Q_{M C}(M,X,Y)$ ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We specify $Q_{M C}(M,X,Y)$ by individually specifying $Q_{M C}(M),Q_{M C}(X|M)$ , and $Q_{M C}(Y|X)$ . We choose $Q_{M C}(M)$ and $Q_{M C}(X|M)$ as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\nQ_{M C}(M)=P(M),\\mathrm{~and~}Q_{M C}(X|M)=P(X|M).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The distribution $Q_{M C}(Y|X)$ is specified through the following deterministic transformation: ", "page_idx": 22}, {"type": "equation", "text": "$$\nY={\\frac{\\gamma_{Y}}{\\gamma_{X}}}X.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Part 2: Showing $Q_{M C}(M,X,Y)\\in\\Delta_{P}$ ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "For showing $Q_{M C}(M,X,Y)\\in\\Delta_{P}$ , we first need to calculate $Q_{M C}(Y|M)$ : ", "page_idx": 22}, {"type": "text", "text": "Calculating $Q_{M C}(Y|M)$ : Since $Q_{M C}(X|M){=}\\mathrm{Uniform}(0,\\gamma_{X}M)$ , and $Y=\\left.\\gamma_{Y}\\right/_{\\gamma x}X$ , we can use the result of lemma 3 to calculate $Q_{M C}(Y|M)$ as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle Q_{M C}(Y|M)=Q_{M C}\\,\\left(\\frac{\\gamma_{Y}}{\\gamma_{X}}X\\right|M\\right)=\\mathrm{Uniform}\\left(0,\\frac{\\gamma_{Y}}{\\gamma_{X}}\\gamma_{X}M\\right)=\\mathrm{Uniform}\\left(0,\\gamma_{Y}M\\right)}\\\\ {\\displaystyle\\qquad\\qquad=P(Y|M).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then, from (38) and (40), we can conclude: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{M C}(M,X)=Q_{M C}(M)Q_{M C}(X|M)=P(M)P(X|M)=P(M,X),}\\\\ &{Q_{M C}(M,Y)=Q_{M C}(M)Q_{M C}(Y|M)=P(M)P(Y|M)=P(M,Y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence, $Q_{M C}(M,X,Y)\\in\\Delta_{P}$ . ", "page_idx": 22}, {"type": "text", "text": "Part 3: Showing $Q_{M C}(M,X,Y)$ satisfies (37) ", "page_idx": 22}, {"type": "text", "text": "It is trivial to see that by the construction of $Q_{M C}(M,X,Y)$ given in Part 1, we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\nQ_{M C}(M,X,Y)=Q_{M C}(M)Q_{M C}(X|M)Q_{M C}(Y|X).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By the chain rule, we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\nQ_{M C}(M,X,Y)=Q_{M C}(M)Q_{M C}(Y|M)Q_{M C}(X|Y,M).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Hence, for showing that $Q_{M C}(M,X,Y)$ satisfies (37), we only need to show that $Q_{M C}(X|Y,M)=$ $Q_{M C}(X|Y)$ . ", "page_idx": 23}, {"type": "text", "text": "Showing $Q_{M C}(X|Y,M)=Q_{M C}(X|Y)$ : Observe that $Y=\\left.\\gamma_{Y}\\right/_{\\gamma_{X}}X$ by construction. Therefore, we can equivalently express $X=\\gamma_{X}\\big/_{\\gamma_{Y}}Y$ . Consequently, ", "page_idx": 23}, {"type": "equation", "text": "$$\nQ_{M C}(X|Y,M)=\\mathbb{I}[X={\\gamma}x/{\\gamma}_{Y}Y]{\\mathrm{~as~}}X={\\gamma}x/{\\gamma}_{Y}Y,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\mathbb{I}[\\cdot]$ is the indicator function. Since R.H.S of the above equation does not depend on $M$ , we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\nQ_{M C}(X|Y,M)=\\mathbb{I}[X=\\gamma x/_{\\gamma_{Y}}Y]=Q_{M C}(X|Y).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Combining (44) and (46), we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\nQ_{M C}(M,X,Y)=Q_{M C}(M)Q_{M C}(Y|M)Q_{M C}(X|Y).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Combining (43) and (47), we can show that $Q_{M C}(M,X,Y)$ satisfies (37). ", "page_idx": 23}, {"type": "text", "text": "Conclusion: Combining the results of parts 2 and 3, we can conclude that $Q_{M C}(M,X,Y)\\ \\in$ $\\Delta_{P}$ and contains both Markovian structures: $M\\,\\rightarrow\\,X\\,\\rightarrow\\,Y$ and $M\\,\\rightarrow\\,Y\\,\\rightarrow\\,X$ . A unique implication of $Q_{M C}(M,X,Y)$ admitting both Markovian structures is that both $I_{Q_{M C}}(M;X|Y)=$ $I_{Q_{M C}}(M;Y|X)=0$ . Consequently, both minimization problems presented in (2) are minimized by $Q_{M C}(M,X,Y)$ with $\\begin{array}{r}{U I(M;X\\backslash Y)\\;=\\;\\operatorname*{min}_{Q\\in\\Delta_{P}}I_{Q}(M;X|Y)\\;=\\;0}\\end{array}$ and $U I(M;Y\\backslash X)\\;=$ $\\mathrm{min}_{Q\\in\\Delta_{P}}\\,I_{Q}(M;Y|X)=0$ , completing our proof. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Lemma 3. Let $X\\sim\\mathit{U n i f o r m}(0,a)$ for some $a\\in\\mathbb{R},$ , then $\\gamma X\\sim{\\cal U}n i f o r m(0,\\gamma a).$ for $\\gamma\\in\\mathbb{R}$ and $a,\\gamma\\neq0$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. We denote the p.d.f. of the random variable $X$ as $p_{X}(\\cdot)$ . By lemma statement, we know: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p_{X}(X=x)=\\mathrm{Uniform}(0,a)=\\left\\{\\begin{array}{c l}{\\frac{1}{\\left|a\\right|}}&{x\\in[0,a]}\\\\ {0}&{\\mathrm{otherwise}}\\end{array}\\right..}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Denote the p.d.f. of the function $f(X)$ of the random variable $X$ as $p_{f(X)}(\\cdot)$ . We want to derive $p_{f(X)}(\\cdot)$ for $f(X)=\\gamma X$ . Since, $f(X)=\\gamma X$ is an invertible function, we can use the change-ofvariable technique for deriving $p_{f(X)}(\\cdot)$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\np_{f(X)}(f(X)=y)=p_{X}(X=f^{-1}(y))\\left|{\\frac{d f^{-1}(y)}{d y}}\\right|=\\left\\{\\begin{array}{c l}{1/|\\gamma{\\boldsymbol{\\alpha}}|}&{y\\in[0,\\gamma a]}\\\\ {0}&{\\mathrm{otherwise}}\\end{array}\\right.=\\mathrm{Uniform}(0,\\gamma a).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "E.4 PID for the miscellaneous system M4. ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Miscellaneous System M4: Let $(M,X,Y)$ be a system of random variables with joint distribution $\\overline{{P(M,X,Y)}}$ . The joint distribution $P(M,X,Y)$ describes the miscellaneous system M4 if it satisfies the following two properties: ", "page_idx": 23}, {"type": "text", "text": "1. $M\\sim P(M)$ and has support $[0,1]$ .   \n2. $P(X|M){=}\\mathrm{Bernoulli}(M)$ , and $P(Y|M){=}\\mathrm{Bernoulli}(M+p-2p M)$ , where $p\\in[0,1]$ . ", "page_idx": 23}, {"type": "text", "text": "This system is adapted from the Bernoulli decomposition listed in Appendix A [26]. ", "page_idx": 23}, {"type": "text", "text": "Proposition 5. Let $M,X$ , and $Y$ be random variables whose joint distribution $P(M,X,Y)$ describes the miscellaneous system M4 defined above. Then, $\\Delta_{P}$ contains a Markov chain of the form $M\\rightarrow X\\rightarrow Y$ and ${\\dot{U}}I(M;Y\\backslash X)=0$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. We briefly outline the proof structure. The proof can be divided into two parts: ", "page_idx": 24}, {"type": "text", "text": "1. In the first part, we explicitly construct a joint distribution $Q_{M C}(M,X,Y)$ having the Markovian structure $M\\rightarrow X\\rightarrow Y$ .   \n2. In the second part, we conclude our proof by showing the $Q_{M C}(M,X,Y)$ constructed in the first part, lies in $\\Delta_{P}$ . ", "page_idx": 24}, {"type": "text", "text": "Part 1: Specifying $\\underline{{Q_{M C}(M,X,Y)}}$ ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Since $Q_{M C}(M,X,Y)$ is the joint distribution of a Markov chain $M\\ \\rightarrow\\ X\\ \\rightarrow\\ Y$ , we have $Q_{M C}(M,X,Y)~=~Q_{M C}(M)Q_{M C}(X|M)Q_{M C}(Y|X)$ . Leveraging the special structure of $Q_{M C}(M,X,Y)$ , we specify $Q_{M C}(M,X,Y)$ by individually specifying $Q_{M C}{\\bar{(}}M),Q_{M C}(X|M)$ , and $Q_{M C}(Y|X)$ . We choose $Q_{M C}(M)$ , $Q_{M C}(X|M)$ as follows: ", "page_idx": 24}, {"type": "equation", "text": "$$\nQ_{M C}(M)=P(M),\\mathrm{~and~}Q_{M C}(X|M)=P(X|M).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The distribution $Q_{M C}(Y|X)$ is specified through the following stochastic transformation: ", "page_idx": 24}, {"type": "equation", "text": "$$\nY=X(1-Z)+Z(1-X),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $Z\\sim\\mathrm{Bernoulli}(p)$ and $Z$ \u22a5\u22a5 $(X,M)$ .", "page_idx": 24}, {"type": "text", "text": "Part 2: Showing $Q_{M C}(M,X,Y)\\in\\Delta_{P}$ ", "page_idx": 24}, {"type": "text", "text": "For showing $Q_{M C}(M,X,Y)\\in\\Delta_{P}$ , we first need to calculate $Q_{M C}(Y|M)$ : ", "page_idx": 24}, {"type": "text", "text": "Calculating $Q_{M C}(Y|M)$ : For calculating $Q(Y|M)$ , we use the result of proposition 10 in [26], which states that if $X^{\\prime}\\sim\\mathrm{Bernoulli}(\\theta)$ , $Z^{\\prime}\\sim$ Bernoulli $\\left(p^{\\prime}\\right)$ , $Z^{\\prime}\\perp\\!\\!\\!\\perp X^{\\prime}$ , and $Y^{\\prime}\\stackrel{}{=}\\bar{X}^{\\prime}(1-Z^{\\prime})+(1-X^{\\prime})Z^{\\prime}$ , then Y \u223cBernoulli $(\\theta+p^{\\prime}-2p^{\\prime}\\theta)$ . ", "page_idx": 24}, {"type": "text", "text": "Noting that $Q_{M C}(X|M)=\\operatorname{Bernoulli}(M)$ and $Y=X(1-Z)+Z(1-X)$ , where $Z\\sim\\mathrm{Bernoulli}(p)$ and $Z\\perp\\!\\!\\!\\perp X|M$ , we can calculate $Q_{M C}(Y|M)$ as follows: ", "page_idx": 24}, {"type": "equation", "text": "$$\nQ_{M C}(Y|M)=\\mathrm{Bernoulli}(M+p-2p M)=P(Y|M).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then, from (49) and (51), we can conclude: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{M C}(M,X)=Q_{M C}(M)Q_{M C}(X|M)=P(M)P(X|M)=P(M,X),}\\\\ &{Q_{M C}(M,Y)=Q_{M C}(M)Q_{M C}(Y|M)=P(M)P(Y|M)=P(M,Y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Hence, $Q_{M C}(M,X,Y)\\;\\in\\;\\Delta_{P}$ and consequently, by proposition 1, $U I(M;Y\\backslash X)\\,=\\,0$ . This concludes our proof. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "E.5 PID for the miscellaneous system M5. ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Miscellaneous System M5: Let $(\\vec{\\bf M},X,Y)$ be a system of random variables with joint distribution $\\overline{{P(\\vec{\\bf M},X,Y)}}$ . The joint distribution $P(\\vec{\\bf M},X,Y)$ describes the miscellaneous system M5 if it satisfies the following two properties: ", "page_idx": 24}, {"type": "text", "text": "1. $\\vec{\\bf M}\\sim P(\\vec{\\bf M})$ and has support over simplex of dimension $d$ . ", "page_idx": 24}, {"type": "text", "text": "2. $P(X|\\vec{\\mathbf{M}}){=}\\mathrm{Categorical}(M_{1},\\ldots,M_{d})$ , and $P(Y|\\vec{\\bf M})\\!=\\!\\mathrm{categorical}(\\phi_{1},\\ldots,\\phi_{d})$ , where $\\phi_{i}=(1-p)M_{i}+p/d$ . ", "page_idx": 24}, {"type": "text", "text": "This system is adapted from the categorical decomposition listed in Appendix A [26]. ", "page_idx": 24}, {"type": "text", "text": "Proposition 6. Let $\\vec{\\bf M},X$ , and $Y$ be random variables whose joint distribution $P(\\vec{\\bf M},X,Y)$ describes the miscellaneous system M5 defined above. Then, $\\Delta_{P}$ contains a Markov chain of the form $\\vec{\\bf M}\\rightarrow X\\rightarrow Y$ and $U I({\\vec{\\mathbf{M}}};Y\\backslash X)=0$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. We briefly outline the proof structure. The proof can be divided into two parts: ", "page_idx": 24}, {"type": "text", "text": "1. In the first part, we explicitly construct a joint distribution $Q_{M C}(\\vec{\\bf M},X,Y)$ having the Markovian structure $\\vec{\\bf M}\\rightarrow X\\rightarrow Y$ . ", "page_idx": 24}, {"type": "text", "text": "2. In the second part, we conclude our proof by showing the $Q_{M C}(\\vec{\\bf M},X,Y)$ constructed in the first part, lies in $\\Delta_{P}$ . ", "page_idx": 25}, {"type": "text", "text": "Part 1: Specifying $Q_{M C}(\\vec{\\bf M},X,Y)$ ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Since $Q_{M C}(\\vec{\\bf M},X,Y)$ is the joint distribution of a Markov chain $\\vec{\\textbf{M}}\\to\\textbf{\\textit{X}}\\to\\textbf{\\textit{Y}}$ , we have $Q_{M C}(\\vec{\\bf M},X,Y)\\,=\\,Q_{M C}(\\vec{\\bf M})Q_{M C}(X|\\vec{\\bf M})Q_{M C}(Y|X).$ . Taking advantage of this special structure of $Q_{M C}(\\vec{\\bf M},X,Y)$ , we specify $Q_{M C}(\\vec{\\bf M},X,Y)$ by individually specifying $Q_{M C}(\\vec{\\bf M})$ , $Q_{M C}(X|\\vec{\\bf M})$ , and $Q_{M C}(Y|X)$ . We choose $Q_{M C}(\\vec{\\mathbf{M}})$ , $Q_{M C}(X|\\vec{\\bf M})$ as follows: ", "page_idx": 25}, {"type": "equation", "text": "$$\nQ_{M C}(\\vec{\\bf M})=P(\\vec{\\bf M}),\\;\\mathrm{and}\\;Q_{M C}(X|\\vec{\\bf M})=P(X|\\vec{\\bf M}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The distribution $Q_{M C}(Y|X)$ is specified through the following transformation: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Y=X(1-Z)+Z D,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where Z \u223cBernoulli $(p)$ , $D\\sim$ Categorical $\\textstyle\\left({\\frac{1}{d}},\\ldots,{\\frac{1}{d}}\\right)$ , and $D,Z$ , and $\\left(X,{\\vec{\\mathbf{M}}}\\right)$ are jointly independent. ", "page_idx": 25}, {"type": "text", "text": "Part 2: Showing $Q_{M C}(\\vec{\\bf M},X,Y)\\in\\Delta_{P}$ ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "For showing $Q_{M C}(\\vec{\\bf M},X,Y)\\in\\Delta_{P}$ , we first need to calculate $Q_{M C}(Y|\\vec{\\mathbf{M}})$ : ", "page_idx": 25}, {"type": "text", "text": "Calculating $Q_{M C}(Y|\\vec{\\mathbf{M}})$ : For calculating $Q_{M C}(Y|\\vec{\\mathbf{M}})$ , we use the result of proposition 11 in [26], which states that: ", "page_idx": 25}, {"type": "text", "text": "If $X^{\\prime}\\sim$ Categorical $(\\theta_{1},\\ldots,\\theta_{d})$ , $Z^{\\prime}\\sim\\mathrm{Bernoulli}(p^{\\prime}).$ , $D^{\\prime}\\sim$ Categorical $\\left(1/d,\\ldots,1/d\\right)$ , and $Y^{\\prime}=$ $X^{\\prime}(1-Z^{\\prime})+Z^{\\prime}D^{\\prime}$ with $X^{\\prime},Z^{\\prime}$ , and $D^{\\prime}$ jointly independent, then $Y^{\\prime}\\sim$ Categorical $(\\phi_{1}^{\\prime},\\ldots,\\phi_{d}^{\\prime})$ with $\\phi_{i}^{\\prime}=(1-p^{\\prime})\\theta_{i}+p^{\\prime}/d$ . ", "page_idx": 25}, {"type": "text", "text": "Noting that $Q_{M C}(X|\\vec{\\mathbf{M}})=\\mathrm{Categorical}(M_{1},\\dots,M_{d})$ and $Y=X(1-Z)+Z D$ , with $X,D$ , and $Z$ jointly conditionally independent conditioned on $M$ , we have: ", "page_idx": 25}, {"type": "equation", "text": "$$\nQ_{M C}(Y|\\vec{\\bf M})=\\mathrm{Categorical}(\\phi_{1},\\ldots,\\phi_{d}),~\\mathrm{with}~\\phi_{i}=M_{i}(1-p)+\\frac{p}{d}=P(Y|\\vec{\\bf M}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "From (54) and (56), we can conclude: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{M C}(\\vec{\\bf M},X)=Q_{M C}(\\vec{\\bf M})Q_{M C}(X|\\vec{\\bf M})=P(M)P(X|\\vec{\\bf M})=P(\\vec{\\bf M},X),}\\\\ &{Q_{M C}(\\vec{\\bf M},Y)=Q_{M C}(\\vec{\\bf M})Q_{M C}(Y|\\vec{\\bf M})=P(M)P(Y|\\vec{\\bf M})=P(\\vec{\\bf M},Y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Hence, $Q_{M C}(\\vec{\\bf M},X,Y)\\;\\in\\;\\Delta_{P}$ and consequently, by proposition 1, $U I(\\vec{\\bf M};Y\\backslash X)\\,=\\,0$ . This concludes our proof. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "E.6 PID for the miscellaneous system M6. ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Miscellaneous System M6: Let $(M,\\vec{\\bf X},\\vec{\\bf Y})$ be a system of random variables with joint distribution $\\overline{{P(M,\\vec{\\bf X},\\vec{\\bf Y})}}$ . The joint distribution $P(M,{\\vec{\\mathbf{X}}},{\\vec{\\mathbf{Y}}})$ describes the miscellaneous system M6 if it satisfies the following two properties: ", "page_idx": 25}, {"type": "text", "text": "1. $M\\sim P(M)$ and has support $\\mathbb{N}$ .   \n2. $P(\\vec{\\mathbf{X}}|M)\\mathrm{{=}}\\mathbf{M}\\mathrm{{ultinomial}}_{d_{X}}(\\vec{\\mathbf{X}};M,\\vec{\\mathbf{p}}_{X})$ , and $P(\\vec{\\mathbf{Y}}\\vert M)\\mathrm{=}\\mathrm{Multinomial}_{d_{Y}}(\\vec{\\mathbf{Y}};M,\\vec{\\mathbf{p}}_{Y}).$ ", "page_idx": 25}, {"type": "text", "text": "Proposition 7. Let $M$ be a random variable. Furthermore, assume $\\vec{\\bf X}$ and $\\vec{\\bf Y}$ are random vectors of size $d_{X}$ and $d_{Y}$ , respectively, such that $P(M,{\\vec{\\mathbf{X}}},{\\vec{\\mathbf{Y}}})$ describes miscellaneous system M6. ", "page_idx": 25}, {"type": "text", "text": "1. If, $\\begin{array}{r}{\\operatorname*{min}_{i\\in[d_{Y}]}p_{i}^{Y}\\geq\\operatorname*{min}_{i\\in[d_{X}]}p_{i}^{X}}\\end{array}$ , then there exists a Markov chain of the form $M\\rightarrow\\vec{\\mathbf{X}}\\rightarrow$ $\\vec{\\bf Y}$ in $\\Delta_{P}$ , and $U I(M;{\\vec{\\mathbf{Y}}}\\backslash{\\vec{\\mathbf{X}}})=0$ .   \n2. If, $\\begin{array}{r}{\\operatorname*{min}_{i\\in[d_{Y}]}p_{i}^{Y}\\leq\\operatorname*{min}_{i\\in[d_{X}]}p_{i}^{X}}\\end{array}$ , then there exists a Markov chain of the form $M\\rightarrow\\vec{\\mathbf{Y}}\\rightarrow$ $\\vec{\\bf X}$ in $\\Delta_{P}$ , and $U I(M;{\\vec{\\mathbf{X}}}\\backslash{\\vec{\\mathbf{Y}}})=0$ . ", "page_idx": 25}, {"type": "text", "text": "Proof. We only provide an explicit proof of condition 1. The proof of condition 2 follows the same steps as the proof of condition 1 with parameters of $\\vec{\\bf X}$ and $\\vec{\\bf Y}$ switched. ", "page_idx": 26}, {"type": "text", "text": "We briefly outline the proof structure. The proof can be divided into two major parts: ", "page_idx": 26}, {"type": "text", "text": ". In the first part, we explicitly construct a joint distribution $Q_{M C}(M,{\\vec{\\mathbf{X}}},{\\vec{\\mathbf{Y}}})$ having the Markovian structure $M\\rightarrow\\vec{\\mathbf{X}}\\rightarrow\\vec{\\mathbf{Y}}$ .   \n2. In the second part, we conclude our proof by showing the $Q_{M C}(M,{\\vec{\\mathbf{X}}},{\\vec{\\mathbf{Y}}})$ constructed in the first part, lies in $\\Delta_{P}$ . ", "page_idx": 26}, {"type": "text", "text": "Part 1: Specifying $Q_{M C}(M,{\\vec{\\mathbf{X}}},{\\vec{\\mathbf{Y}}})$ ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Since $Q_{M C}(M,{\\vec{\\mathbf{X}}},{\\vec{\\mathbf{Y}}})$ is the joint distribution of a Markov chain $M\\;\\rightarrow\\;\\vec{\\textbf{X}}\\;\\rightarrow\\;\\vec{\\textbf{Y}}$ , we have $Q_{M C}(M,\\vec{\\mathbf{X}},\\vec{\\mathbf{Y}})\\,=\\,Q_{M C}(M)Q_{M C}(\\vec{\\mathbf{X}}|M)Q_{M C}(\\vec{\\mathbf{Y}}|\\vec{\\mathbf{X}})$ . Taking advantage of this special structure of $Q_{M C}(M,{\\vec{\\mathbf{X}}},{\\vec{\\mathbf{Y}}})$ , we specify $Q_{M C}(M,{\\vec{\\mathbf{X}}},{\\vec{\\mathbf{Y}}})$ by individually specifying $Q_{M C}(M)$ , $Q_{M C}(\\vec{\\bf X}|M)$ , and $Q_{M C}(\\vec{\\bf Y}|\\vec{\\bf X})$ . We choose $Q_{M C}(M),Q_{M C}(\\vec{\\bf X}|M)$ as follows: ", "page_idx": 26}, {"type": "equation", "text": "$$\nQ_{M C}(M)=P(M),\\;\\mathrm{and}\\;Q_{M C}(\\vec{\\bf X}|M)=P(\\vec{\\bf X}|M).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Specifying ${\\cal Q}(\\vec{\\bf Y}|\\vec{\\bf X})$ : Let $\\begin{array}{l l l}{k_{X}^{*}}&{=}&{\\arg\\operatorname*{min}_{i\\in[d_{X}]}p_{i}^{X}}\\end{array}$ , and $\\begin{array}{r c l}{k_{Y}^{*}}&{=}&{\\arg\\operatorname*{min}_{i\\in[d_{Y}]}p_{i}^{Y}}\\end{array}$ . Then, $\\\\\\\\\\\\b{Q}(\\vec{\\mathbf{Y}}|\\vec{\\mathbf{X}})\\!\\!=\\!\\!\\mathbf{M}\\mathbf{ultinomial}_{d_{Y}}\\left(\\vec{\\mathbf{Y}};\\sum_{i\\in[d_{X}]\\backslash k_{X}^{*}}x_{i},\\mathbf{p}_{Y}^{*}\\right)$ , where ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{p}_{Y}^{*}\\!=\\left[\\frac{p_{1}^{Y}}{\\sum_{i\\in[d_{X}]\\backslash k_{X}^{*}}p_{i}^{X}}\\,\\cdot\\cdot\\cdot\\frac{p_{d_{Y}-1}^{Y}}{\\sum_{i\\in[d_{X}]\\backslash k_{X}^{*}}p_{i}^{X}}\\,\\,1-\\frac{\\sum_{i=1}^{d_{Y}-1}p_{i}^{Y}}{\\sum_{i\\in[d_{X}]\\backslash k_{X}^{*}}p_{i}^{X}}\\right]^{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We first show the proposed $Q_{M C}(\\vec{\\bf Y}|\\vec{\\bf X})$ is a valid distribution. By condition 1: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{i\\in[d_{Y}]}{\\operatorname*{min}}\\,p_{i}^{Y}\\geq\\underset{i\\in d_{X}}{\\operatorname*{min}}\\,p_{i}^{X}}\\\\ &{\\Rightarrow\\underset{i\\in[d_{X}]\\backslash k_{X}^{*}}{\\sum}\\,p_{i}^{X}\\geq\\underset{i\\in[d_{Y}]\\backslash k_{Y}^{*}}{\\sum}p_{i}^{Y}\\geq p_{j}^{Y},\\:\\forall\\,j\\in[d_{Y}]}\\\\ &{\\Rightarrow1\\geq\\frac{p_{j}^{Y}}{\\sum_{i\\in[d_{X}]\\backslash k_{X}^{*}}p_{i}^{X}}\\geq0,\\:\\forall\\,j\\in[d_{Y}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the last inequality is due to $\\vec{\\bf p}_{X}$ and $\\vec{\\bf p}_{Y}$ forming valid multinomial distributions. ", "page_idx": 26}, {"type": "text", "text": "For showing $Q_{M C}(M,\\vec{\\bf X},\\vec{\\bf Y})\\in\\Delta_{P}$ , we first need to calculate $Q_{M C}(\\vec{\\mathbf{Y}}|M)$ : ", "page_idx": 26}, {"type": "text", "text": "Calculating $Q_{M C}(\\vec{\\mathbf{Y}}|M)$ : We use lemma 4 to calculate $Q(\\vec{\\mathbf{Y}}|M)$ . Note that $Q_{M C}(\\vec{\\bf X}|M)\\;=\\;$ Multinomia $.d_{X}\\left(M,\\vec{{\\bf p}}_{X}\\right)$ , hence we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\nQ_{M C}(Y|M)=\\mathbf{Multinomial}_{d_{Y}}(M,\\vec{\\mathbf{p}}_{Y}^{\\prime})\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\vec{\\mathbf{p}}_{Y}^{\\prime}=\\left[\\frac{p_{1}^{Y}\\sum_{i\\in[d_{X}]\\setminus k_{X}^{*}}p_{i}^{X}}{\\sum_{i\\in[d_{X}]\\setminus k_{X}^{*}}p_{i}^{X}}\\quad\\dotsc\\quad\\frac{p_{d_{Y}-1}^{Y}\\sum_{i\\in[d_{X}]\\setminus k_{X}^{*}}p_{i}^{X}}{\\sum_{i\\in[d_{X}]\\setminus k_{X}^{*}}p_{i}^{X}}\\quad1-\\frac{\\sum_{i\\in[d_{X}]\\setminus k_{X}^{*}}p_{i}^{X}\\sum_{i=1}^{d_{Y}-1}p_{i}^{Y}}{\\sum_{i\\in[d_{X}]\\setminus k_{X}^{*}}p_{i}^{X}}\\right]}\\\\ &{\\Rightarrow\\vec{\\mathbf{p}}_{Y}^{\\prime}=\\left[p_{1}^{Y}\\quad\\dotsc\\quad p_{d_{Y}-1}^{Y}\\quad1-\\sum_{i\\in[d_{Y}]}p_{i}^{Y}\\right]=\\vec{\\mathbf{p}}_{Y}}\\\\ &{\\Rightarrow Q_{M C}(Y|M)=\\mathbf{M}\\mathrm{ultinomial}_{d_{Y}}(M,\\vec{\\mathbf{p}}_{Y})=P(Y|M).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then, from (59) and (64), we can conclude: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{M C}(M,\\vec{\\mathbf{X}})=Q_{M C}(M)Q_{M C}(\\vec{\\mathbf{X}}|M)=P(M)P(\\vec{\\mathbf{X}}|M)=P(M,\\vec{\\mathbf{X}}),}\\\\ &{Q_{M C}(M,\\vec{\\mathbf{Y}})=Q_{M C}(M)Q_{M C}(\\vec{\\mathbf{Y}}|M)=P(M)P(\\vec{\\mathbf{Y}}|M)=P(M,\\vec{\\mathbf{Y}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Hence, $Q_{M C}(M,\\vec{\\bf X},\\vec{\\bf Y})\\,\\in\\,\\Delta_{P}$ and consequently, by proposition 1, $U I(M;\\vec{\\mathbf{Y}}\\backslash\\vec{\\mathbf{X}})\\,=\\,0$ . This concludes our proof. $\\sqsubset$ ", "page_idx": 26}, {"type": "text", "text": "Lemma 4. Suppose $\\vec{\\bf X}\\ \\sim\\ M u l t i n o m i a l_{d_{X}}(n,\\vec{\\bf p}_{X})$ and $\\begin{array}{r}{\\vec{\\bf Y}|\\vec{\\bf X}\\;\\sim\\;M u l t i n o m i a l_{d_{Y}}\\,\\Big(\\sum_{j\\in\\mathbb{Z}}X_{j},\\vec{\\bf q}\\Big),}\\end{array}$ , where $\\mathcal{T}\\subset\\{1,\\ldots,d_{X}\\}$ . Then $\\vec{\\bf Y}\\sim M u l t i n o m i a l_{d_{Y}}\\;(n,\\vec{\\bf q}^{*}),$ where ", "page_idx": 27}, {"type": "equation", "text": "$$\n{q_{i}}^{*}=\\left\\{\\frac{\\left[\\sum_{j\\in\\mathbb{Z}}{p_{j}}\\right]\\cdot q_{i},}{1-\\left[\\sum_{j\\in\\mathbb{Z}}{p_{j}}\\right]\\cdot\\left[\\sum_{j=1}^{d_{Y}-1}q_{j}\\right],}\\right.\\quad i=d_{Y}\\,.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. Without loss of generality, reshuffle the class indices of $\\vec{\\bf Y}$ such that $\\mathbb{Z}{=}\\{i{:}1\\,\\leq\\,i\\,\\leq\\,w\\}$ , where $|\\mathcal{T}|\\!=\\!w$ . Let $\\begin{array}{r}{C\\!=\\!\\left\\{\\vec{\\bf x}^{\\prime}\\in\\mathbb{N}_{0}^{d_{X}-1}\\colon\\!\\sum_{i=1}^{d_{X}-1}x_{i}\\le n\\right\\}}\\end{array}$ . By the law of total probability, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle P(\\vec{\\bf Y}=\\vec{\\bf y})=\\sum_{\\vec{\\bf x}\\in C}P(\\vec{\\bf X}=\\vec{\\bf x})P(\\vec{\\bf Y}=\\vec{\\bf y}|\\vec{\\bf X}=\\vec{\\bf x})}\\ ~}\\\\ {{\\displaystyle~~~~~~~~~~~~~=\\sum_{\\vec{\\bf x}\\in C}\\frac{n!}{\\prod_{i=1}^{d_{X}}x_{i}!}\\prod_{i=1}^{d_{X}}p_{i}^{x_{i}}}\\cdot\\frac{\\Bigl(\\sum_{j\\in\\cal Z}x_{j}\\Bigr)!}{\\prod_{i=1}^{d_{Y}}y_{i}!}\\prod_{i=1}^{d_{Y}}q_{i}^{y_{i}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Remove from the sum all terms that do not depend on $\\vec{\\bf x}$ . Note that $\\begin{array}{r}{y_{d_{Y}}\\!=\\sum_{j\\in\\mathcal{Z}}x_{j}-\\sum_{i=1}^{d_{Y}-1}y_{i}}\\end{array}$ and $\\begin{array}{r}{q_{d_{Y}}^{y_{d_{Y}}}=\\left(1-\\sum_{i=1}^{d_{Y}}q_{i}\\right)^{y_{d_{Y}}}}\\end{array}$ are both functions of $\\vec{\\bf y}$ . Let $\\begin{array}{r}{y_{s}\\!=\\sum_{i=1}^{d_{Y}-1}y_{i}}\\end{array}$ and $\\begin{array}{r}{q_{s}\\!=\\sum_{i=1}^{d_{Y}-1}q_{i}}\\end{array}$ . ", "page_idx": 27}, {"type": "equation", "text": "$$\nP(\\vec{\\bf Y}=\\vec{\\bf y})=\\left(\\frac{n!}{\\prod_{i=1}^{d_{Y}-1}y_{i}!}\\prod_{i=1}^{d_{Y}-1}q_{i}^{y_{i}}\\right)\\sum_{\\vec{\\bf x}\\in\\cal C}\\frac{\\prod_{i=1}^{d_{X}}p_{i}^{x_{i}}\\cdot\\left(\\sum_{j\\in\\cal Z}x_{j}\\right)!}{\\prod_{i=1}^{d_{X}}x_{i}!\\left(\\sum_{j\\in\\cal Z}x_{j}-y_{s}\\right)!}\\left(1-q_{s}\\right)^{\\sum_{j\\in\\cal Z}x_{j}-y_{s}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Form a multinomial coefficient outside the sum: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle P(\\vec{\\bf Y}=\\vec{\\bf y})=\\left(\\frac{n!}{\\prod_{i=1}^{d_{Y}-1}y_{i}!\\cdot(n-y_{s})!}\\prod_{i=1}^{d_{Y}-1}q_{i}^{y_{i}}\\right)}\\\\ {\\displaystyle\\times\\sum_{\\vec{\\bf x}\\in C}\\frac{(n-y_{s})!\\prod_{i=1}^{d_{X}}p_{i}^{x_{i}}\\cdot\\left(\\sum_{j\\in\\mathbb{Z}}x_{j}\\right)!}{\\prod_{i=1}^{d_{X}}x_{i}!\\left(\\sum_{j\\in\\mathbb{Z}}x_{j}-y_{s}\\right)!}\\left(1-q_{s}\\right)\\!\\!\\sum_{j\\in\\mathbb{Z}}x_{j}-y_{s}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Let ${\\mathcal{T}}^{0}\\,=\\,\\{i\\,{:}w\\,<\\,i\\,\\leq\\,d_{X}\\}$ be the set of indices that are not taken in the number of trials for $\\vec{\\bf Y}$ . Consider the sum over $\\vec{\\bf x}\\in C$ alone in (71). Separating terms that belong to $\\mathcal{T}$ and $\\mathcal{Z}^{0}$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{\\vec{\\kappa}\\in C}\\frac{(n-y_{s})!}{\\prod_{j\\in\\mathbb{Z}^{0}}x_{j}!\\left(\\sum_{j\\in\\mathbb{Z}}x_{j}-y_{s}\\right)!}\\prod_{j\\in\\mathbb{Z}^{0}}p_{j}^{x_{j}}\\left(1-q_{s}\\right)\\sum_{j\\in\\mathbb{Z}^{\\nu}}x_{j}-y_{s}\\cdot\\frac{\\left(\\sum_{j\\in\\mathbb{Z}}x_{j}\\right)!}{\\prod_{j\\in\\mathbb{Z}}x_{j}!}\\prod_{j\\in\\mathbb{Z}}p_{j}^{x_{j}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Let $\\begin{array}{r}{C^{\\prime}=\\{\\vec{\\mathbf{x}}^{\\prime}\\in\\mathbb{N}_{0}^{d_{X}-1}\\colon\\sum_{i=1}^{d_{X}-1}x_{i}-\\sum_{i=1}^{d_{Y}-1}y_{i}\\leq n-\\sum_{i=1}^{d_{Y}-1}y_{i}\\}}\\end{array}$ . Note $\\vec{\\bf x}\\in C\\Leftrightarrow\\vec{\\bf x}\\in C^{\\prime}$ , so we can equivalently sum ove r elements in $C^{\\prime}$ . Perform a change of variable with $\\begin{array}{r}{u=\\sum_{j\\in\\mathcal{Z}}x_{j}-y_{s}}\\end{array}$ and define $\\begin{array}{r}{B=\\{x_{j}\\in\\vec{\\bf x}\\colon j\\in\\mathcal{Z},\\sum_{j\\in\\mathcal{Z}}x_{j}=u+y_{s}\\}}\\end{array}$ . Then the sum becomes ", "page_idx": 27}, {"type": "equation", "text": "$$\n=\\sum_{\\vec{\\kappa}\\in C^{\\prime}}\\frac{(n-y_{s})!}{\\prod_{j\\in\\mathbb{Z}^{0}}x_{j}!u!}\\prod_{j\\in\\mathbb{Z}^{0}}p_{j}^{x_{j}}\\,(1-q_{s})^{u}\\sum_{x_{j}\\in B}\\binom{u+y_{s}}{x_{j_{1}},...,x_{j_{m}}}\\prod_{j\\in\\mathbb{Z}}p_{j}^{x_{j}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Note that the inner sum simplifies by the multinomial theorem: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle=\\sum_{\\vec{\\bf x}\\in C^{\\prime}}\\frac{(n-y_{s})!}{\\prod_{j\\in\\mathbb{Z}^{0}}x_{j}!u!}\\prod_{j\\in\\mathbb{Z}^{0}}p_{j}^{x_{j}}\\left(1-q_{s}\\right)^{u}\\left(\\sum_{j\\in\\mathbb{Z}}p_{j}\\right)^{u+y_{s}}}}\\\\ {{\\displaystyle=\\left(\\sum_{j\\in\\mathbb{Z}}p_{j}\\right)^{y_{s}}\\sum_{\\vec{\\bf x}\\in C^{\\prime}}\\frac{(n-y_{s})!}{\\prod_{j\\in\\mathbb{Z}^{0}}x_{j}!\\cdot u!}\\prod_{j\\in\\mathbb{Z}^{0}}p_{j}^{x_{j}}\\left(\\sum_{j\\in\\mathbb{Z}}p_{j}-q_{s}\\sum_{j\\in\\mathbb{Z}}p_{j}\\right)^{u}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Reapply the multinomial theorem to the sum of $\\vec{\\bf x}$ over $C^{\\prime}$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle=\\left(\\sum_{j\\in\\mathbb{Z}}p_{j}\\right)^{y_{s}}\\left(\\sum_{j\\in\\mathbb{Z}^{0}}p_{j}+\\sum_{j\\in\\mathbb{Z}}p_{j}-q_{s}\\sum_{j\\in\\mathbb{Z}}p_{j}\\right)^{n-y_{s}}}}\\\\ {{\\displaystyle=\\left(\\sum_{j\\in\\mathbb{Z}}p_{j}\\right)^{y_{s}}\\left(1-q_{s}\\sum_{j\\in\\mathbb{Z}}p_{j}\\right)^{n-y_{s}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Inserting (76) into (71), the marginal becomes ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle P(\\vec{\\bf Y}=\\vec{\\bf y})=\\frac{n!\\prod_{i=1}^{d_{Y}-1}q_{i}^{y_{i}}}{\\prod_{i=1}^{d_{Y}-1}y_{i}!\\cdot(n-y_{s})!}\\left(\\sum_{j\\in\\mathbb{Z}}p_{j}\\right)^{y_{s}}\\left(1-q_{s}\\sum_{j\\in\\mathbb{Z}}p_{j}\\right)^{n-y_{s}}}}\\\\ {{\\displaystyle=\\frac{n!\\prod_{i=1}^{d_{Y}-1}\\left(\\sum_{j\\in\\mathbb{Z}}p_{j}\\cdot q_{i}\\right)^{y_{i}}}{\\prod_{i=1}^{d_{Y}-1}y_{i}!\\cdot(n-y_{s})!}\\left(1-q_{s}\\sum_{j\\in\\mathbb{Z}}p_{j}\\right)^{n-y_{s}}}}\\\\ {{\\displaystyle=\\mathrm{Multinomial}_{d_{Y}}\\left(n,\\vec{\\bf q}^{*}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\vec{\\bf q}^{*}$ is defined as in (67). ", "page_idx": 28}, {"type": "text", "text": "F Proof of Theorem 1 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this section, we provide the proof of theorem 1. Since theorem 1 provides the analytical PID terms for the univariate affine continuous stable system defined in Sec. 4, we briefly restate certain key properties of the univariate stable distributions and the corresponding univariate affine continuous stable system for convenience. ", "page_idx": 28}, {"type": "text", "text": "F.1 Univariate continuous stable distribution ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Univariate continuous stable distribution are a large class of distributions that naturally arise in the context of generalized central limit theorems. We refer the reader to Appx. N.2 for more details on univariate continuous stable distributions. We now list certain key properties of univariate continuous stable distributions that we make use of in the proof of theorem 1: ", "page_idx": 28}, {"type": "text", "text": "1. If $X$ is distributed according to a univariate continuous stable distribution, then the sum of two independent copies of $X$ , denoted as $X_{1}$ and $X_{2}$ , follows the same univariate continuous stable distribution upto a scaling and translation operation, i.e., $a X_{1}+b X_{2}\\stackrel{d}{=}c X+d$ for $a,b,c>0$ and $d\\in\\mathbb R$ .   \n2. The p.d.f. of univariate continuous stable distributions is characterized by four parameters: stability parameter denoted as $\\alpha\\in(0,2]$ , skewness parameter denoted as $\\beta\\in[-1,1]$ , scale parameters denoted as $\\gamma\\in(0,\\infty)$ , and location parameter denoted as $\\mu\\in\\mathbb{R}$ . We denote the p.d.f. of a univariate continuous stable distribution as $p_{C S}(\\alpha,\\beta,\\gamma,\\mu)$ . ", "page_idx": 28}, {"type": "text", "text": "F.2 Definition of univariate affine continuous stable system ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Let $(M,X,Y)$ be a system of random variables with joint distribution $P(M,X,Y)$ . The joint distribution $P(M,X,Y)$ describes the univariate affine continuous stable system if it satisfies the following two properties: ", "page_idx": 28}, {"type": "text", "text": "1. $M\\sim P(M)$ having some support set ${\\mathcal{M}}\\subseteq\\mathbb{R}$ .   \n2. The conditional distributions of random variables $X$ and $Y$ conditioned on $M$ can be expressed through univariate continuous stable distributions with an affine dependence on $M$ , i.e., $P(X|M){=}p_{C S}(\\alpha,\\beta_{X},\\gamma_{X},a M{+}b)$ and $P(Y|M)\\!=\\!p_{C S}(\\alpha,\\beta_{Y}\\mathrm{sgn}(a c),\\gamma_{Y},c M\\!+\\!d)$ , where $a,b,c,d\\in\\mathbb{R}$ . ", "page_idx": 28}, {"type": "text", "text": "F.3 Formal proof of Theorem 1 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We first discuss and provide the provide the proofs of lemma 5 and lemma 6 that we will use for proving theorem 1. In the following proofs, $\\operatorname{sgn}(x)$ denotes the sign function: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathrm{sgn}(x)=\\left\\{\\begin{array}{c c}{{1}}&{{x>0}}\\\\ {{0}}&{{x=0}}\\\\ {{-1}}&{{x<0}}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Lemma 5. If $X\\sim p_{C S}(\\alpha,\\beta,\\gamma,\\mu),$ , then for any $\\eta\\neq0$ and $\\kappa\\in\\mathbb R$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\eta X+\\kappa\\sim\\left\\{\\begin{array}{c c}{p_{C S}(\\alpha,s g n(\\eta)\\beta,|\\eta|\\gamma,\\eta\\mu+\\kappa)}&{\\alpha\\neq1}\\\\ {p_{C S}\\left(\\alpha,s g n(\\eta)\\beta,|\\eta|\\gamma,\\eta\\mu+\\kappa-2/\\pi\\beta\\gamma\\eta\\log(|\\eta|)\\right)}&{\\alpha=1}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. See the proof of proposition 1.4 part (b) in [23]. ", "page_idx": 29}, {"type": "text", "text": "Lemma 6. If $X_{1}\\sim p_{C S}(\\alpha,\\beta_{1},\\gamma_{1},\\mu_{1})$ and $X_{2}\\sim p_{C S}(\\alpha,\\beta_{2},\\gamma_{2},\\mu_{2})$ are independent, then $X_{1}+$ $X_{2}\\sim p_{C S}(\\alpha,\\beta,\\gamma,\\mu)$ where ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\beta=\\frac{\\beta_{1}\\gamma_{1}^{\\alpha}+\\beta_{2}\\gamma_{2}^{\\alpha}}{\\gamma_{1}^{\\alpha}+\\gamma_{2}^{\\alpha}},\\,\\,\\,\\gamma^{\\alpha}=\\gamma_{1}^{\\alpha}+\\gamma_{2}^{\\alpha},\\,\\,a n d\\,\\mu=\\mu_{1}+\\mu_{2}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. See the proof of proposition 1.4 part (c) in [23]. ", "page_idx": 29}, {"type": "text", "text": "Theorem 1. Let $M,X$ , and $Y$ be random variables whose joint distribution $P(M,X,Y)$ describes a univariate affine continuous stable system. Without the loss of generality, assume $|a|/\\gamma_{X}\\geq|c|/\\gamma_{Y}$ . $\\begin{array}{r}{I f1-\\beta_{Y}\\geq\\left(\\gamma_{X}|c|\\middle/\\gamma_{Y}|a|\\right)^{\\alpha}\\left(1-\\beta_{X}\\right)\\!,}\\end{array}$ , and $1+\\beta_{Y}\\geq\\left(\\gamma_{X}\\lvert c\\rvert\\Big/\\gamma_{Y}\\overset{\\cdot}{\\lvert a\\rvert}\\right)^{\\alpha}\\left(1+\\overset{\\cdot}{\\beta}_{X}\\right)\\!,$ , then $\\Delta_{P}$ contains $a$ Markov chain of the form $M\\rightarrow X\\rightarrow Y$ and $U I(M;Y\\backslash X)=0$ . ", "page_idx": 29}, {"type": "text", "text": "Proof. We first note that we can always assume $|a|/_{\\gamma_{X}}\\geq|c|/_{\\gamma_{Y}}$ without the loss of generality because if $|c|\\bar{\\big/}_{\\gamma_{Y}}\\geq|a|/\\gamma_{X}$ , then we can always switch our nomenclature to refer to $Y$ as $X$ , and $X$ as $Y$ . ", "page_idx": 29}, {"type": "text", "text": "We now briefly outline the proof structure. We divide the proof into two cases $|a|/_{\\gamma x}>|c|/_{\\gamma Y}$ and $|a|/_{\\gamma_{X}}=|c|/_{\\gamma_{Y}}$ . The proof for both cases follows essentially the same structure consisting of two major parts: ", "page_idx": 29}, {"type": "text", "text": "1. In the first part, we explicitly construct a joint distribution $Q_{M C}(M,X,Y)$ having the Markovian structure $M\\rightarrow X\\rightarrow Y$ . 2. In the second part, we show that the $Q_{M C}(M,X,Y)$ constructed in the first part, lies in $\\Delta_{P}$ . Therefore, we can then apply the result of proposition 1 to conclude $U I({\\bar{M}};Y\\backslash X)=0$ . ", "page_idx": 29}, {"type": "text", "text": "Case 1: $|a|/_{\\gamma_{X}}>|c|/_{\\gamma_{Y}}$ ", "page_idx": 29}, {"type": "text", "text": "Part 1: Specifying $Q_{M C}(M,X,Y)$ ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We explicitly construct the desired Markov chain $M\\rightarrow X\\rightarrow Y$ by constructing a larger Markov chain $M\\to X\\to X^{\\prime}\\to Y^{\\prime}\\to Y$ and then marginalizing the larger Markov chain to obtain the desired Markov chain $M\\rightarrow X\\rightarrow Y$ . ", "page_idx": 29}, {"type": "text", "text": "Denote the joint distribution of the Markov chain $M\\ \\ \\to\\ X\\ \\ \\to\\ \\ X^{\\prime}\\ \\ \\to\\ \\ Y^{\\prime}\\ \\ \\to\\ \\ Y\\$ as $Q_{M C}(M,X;X^{\\prime},Y^{\\prime},Y)$ . We can decompose $Q_{M C}(M,X,X^{\\prime},Y^{\\prime},Y)$ by utilizing its Markovian structure as follows: ", "page_idx": 29}, {"type": "equation", "text": "$$\nQ_{M C}(M,X,X^{\\prime},Y^{\\prime},Y)=Q_{M C}(M)Q_{M C}(X|M)Q_{M C}(X^{\\prime}|X)Q_{M C}(Y^{\\prime}|X^{\\prime})Q_{M C}(Y|Y^{\\prime}).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Consequently, we can specify/construct the distribution $Q_{M C}(M,X,X^{\\prime},Y^{\\prime},Y)$ by individually specifying $\\bar{Q_{M C}}(M),\\bar{Q_{M C}}(\\bar{X}|M),Q_{M C}(X^{\\prime}|X),Q_{M C}(Y^{\\prime}|X^{\\prime})$ , and $Q_{M C}(Y|Y^{\\prime})$ . ", "page_idx": 29}, {"type": "text", "text": "Specifying $Q_{M C}(M)$ and $Q_{M C}(X|M)$ : We choose $Q_{M C}(M)$ and $Q_{M C}(X|M)$ as follows: ", "page_idx": 29}, {"type": "equation", "text": "$$\nQ_{M C}(M)=P(M){\\mathrm{~and~}}Q_{M C}(X|M)=P(X|M),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $P(M)$ and $P(X|M)$ are marginal distributions derived from the original joint distribution $P(M,X,Y)$ (discussed in the theorem statement) over which the bivariate PID is being calculated. ", "page_idx": 29}, {"type": "text", "text": "Specifying $Q_{M C}(X^{\\prime}|X)$ : The distribution $Q_{M C}(X^{\\prime}|X)$ is specified by the following deterministic transformation: ", "page_idx": 30}, {"type": "equation", "text": "$$\nX^{\\prime}=\\left\\{\\begin{array}{c l}{{\\frac{1}{a}X}}&{{\\alpha\\neq1}}\\\\ {{\\frac{1}{a}X+(2\\beta_{X}\\gamma_{X}\\log(\\lvert1/a\\rvert))/(\\pi a)}}&{{\\alpha=1}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We will also derive the distribution $Q(X^{\\prime}|M)$ before proceeding with our construction, as we will need it later in the proof to show that the constructed $Q_{M C}(M,X,Y)\\in\\Delta_{P}$ . ", "page_idx": 30}, {"type": "text", "text": "Deriving $Q_{M C}(X^{\\prime}|M)$ : Since $Q_{M C}(X|M=m)=p_{C S}(\\alpha,\\beta_{X},\\gamma_{X},a m+b)$ for a fixed $m$ and $X^{\\prime}$ is scaled and translated version of $X$ , we can use lemma 5 to derive $Q_{M C}(X^{\\prime}|M)$ . The exact expression of $Q_{M C}(X^{\\prime}|M=m)$ is provided in (83). ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{M C}(X^{\\prime}|M=m)=p_{C S}(\\alpha,\\beta_{X^{\\prime}},\\gamma_{X^{\\prime}},\\mu_{X^{\\prime}}),\\mathrm{~where~}}\\\\ &{\\beta_{X^{\\prime}}=\\left\\{\\begin{array}{l l}{\\mathrm{sgn}(1/a)\\beta_{X}}&{\\alpha\\neq1}\\\\ {\\mathrm{sgn}(1/a)\\beta_{X}}&{\\alpha=1}\\end{array}\\right.=\\mathrm{sgn}(1/a)\\beta_{X}=\\mathrm{sgn}(a)\\beta_{X},}\\\\ &{\\gamma_{X^{\\prime}}=\\left\\{\\begin{array}{l l}{|1/a|\\gamma_{X}}&{\\alpha\\neq1}\\\\ {|1/a|\\gamma_{X}}&{\\alpha=1}\\end{array}\\right.=\\frac{\\gamma_{X}}{|a|},}\\\\ &{\\mu_{X^{\\prime}}=\\left\\{\\begin{array}{l l}{\\medskip}&{\\frac{1}{a}(a m+b)}\\\\ {\\frac{1}{a}(a m+b)+2\\beta_{X}\\gamma_{X}\\log(1/a)/\\pi a-2\\beta_{X}\\gamma_{X}\\log(1/a)/\\pi a}&{\\alpha=1}\\end{array}\\right.=m+\\frac{b}{a},}\\\\ &{\\Rightarrow Q_{M C}(X^{\\prime}|M=m)=p_{C S}(\\alpha,\\beta_{X}\\mathrm{sgn}(a),\\gamma_{X}/a|,m+b/a).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Specifying $Q_{M C}(Y^{\\prime}|X^{\\prime})$ : In order to define $Q_{M C}(Y^{\\prime}|X^{\\prime})$ , we need to define an auxiliary variable $\\epsilon$ The random variable $\\overline{{\\epsilon\\sim p_{C S}}}(\\epsilon;\\alpha,\\beta^{\\prime},\\gamma^{\\prime},\\mu^{\\prime})$ , where ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\beta^{\\prime}=\\mathrm{sgn}(a)\\frac{\\left(\\gamma\\gamma/|c|\\right)^{\\alpha}\\beta_{Y}-\\left(\\gamma x/|a|\\right)^{\\alpha}\\beta_{X}}{\\left(\\gamma x/|c|\\right)^{\\alpha}-\\left(\\gamma x/|a|\\right)^{\\alpha}},\\enspace\\gamma^{\\prime}=\\left(\\left(\\frac{\\gamma_{Y}}{|c|}\\right)^{\\alpha}-\\left(\\frac{\\gamma_{X}}{|a|}\\right)^{\\alpha}\\right)^{\\frac{1}{\\alpha}},\\enspace\\mu^{\\prime}=\\frac{d}{c}-\\frac{b}{a}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "In order for $\\epsilon$ to have a legitimate continuous stable distribution, we need to ensure that $\\beta^{\\prime},\\gamma^{\\prime}$ , and $\\mu^{\\prime}$ lie within their appropriate bounds as specified in Appx. F.1. It is trivial to see that $\\mu^{\\prime}\\in\\mathbb{R}$ . ", "page_idx": 30}, {"type": "text", "text": "Showing $\\gamma^{\\prime}\\in(0,\\infty)$ . Under the assumption of the case 1, we know that: ", "page_idx": 30}, {"type": "equation", "text": "$$\n{\\frac{|a|}{\\gamma_{X}}}>{\\frac{|c|}{\\gamma_{Y}}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Manipulating the above inequality, we obtain: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac{\\gamma_{Y}}{\\left|c\\right|}>\\frac{\\gamma_{X}}{\\left|a\\right|}\\Rightarrow\\left(\\frac{\\gamma_{Y}}{\\left|c\\right|}\\right)^{\\alpha}>\\left(\\frac{\\gamma_{X}}{\\left|a\\right|}\\right)^{\\alpha}\\Rightarrow\\left(\\frac{\\gamma_{Y}}{\\left|c\\right|}\\right)^{\\alpha}-\\left(\\frac{\\gamma_{X}}{\\left|a\\right|}\\right)^{\\alpha}>0}\\\\ &{\\Rightarrow\\left(\\left(\\frac{\\gamma_{Y}}{\\left|c\\right|}\\right)^{\\alpha}-\\left(\\frac{\\gamma_{X}}{\\left|a\\right|}\\right)^{\\alpha}\\right)^{\\frac{1}{\\alpha}}=\\gamma^{\\prime}>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Hence, (85) shows that $\\gamma^{\\prime}\\in(0,\\infty)$ . ", "page_idx": 30}, {"type": "text", "text": "Lastly, showing that $\\beta^{\\prime}\\in[-1,1]$ . Note that the condition $\\beta^{\\prime}\\in[-1,1]$ can be equivalently expressed as $|\\beta^{\\bar{\\prime}}|\\leq1$ . For showing that $\\beta^{\\prime}\\in[-1,1]$ , we will show that the inequality $|\\beta^{\\prime}|\\leq1$ is equivalent to the inequalities: $\\left(\\gamma x|c|/|a|\\gamma_{Y}\\right)^{\\alpha}\\left(1\\stackrel{!}{-}\\beta_{X}\\right)\\stackrel{!}{\\leq}1-\\beta_{Y}$ and $\\left(1+\\beta_{X}\\right)\\left(\\gamma x\\,|c|/|a|\\gamma_{Y}\\right)^{\\alpha}\\leq1+\\beta_{Y}$ , where the last two inequalities hold by the theorem assumptions: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{|\\beta^{\\prime}|\\le1\\Rightarrow\\left|\\operatorname{sgn}(a)\\frac{\\left(\\gamma\\gamma/|c|\\right)^{\\alpha}\\beta_{Y}-\\left(\\gamma x/|a|\\right)^{\\alpha}\\beta_{X}}{\\left(\\gamma\\gamma/|c|\\right)^{\\alpha}-\\left(\\gamma x/|a|\\right)^{\\alpha}}\\right|\\le1\\Rightarrow\\left|\\frac{\\left(\\gamma\\gamma/|c|\\right)^{\\alpha}\\beta_{Y}-\\left(\\gamma x/|a|\\right)^{\\alpha}\\beta_{X}}{\\left(\\gamma\\gamma/|c|\\right)^{\\alpha}-\\left(\\gamma x/|a|\\right)^{\\alpha}}\\right|\\le1,}\\\\ &{}&{\\Rightarrow-1\\le\\frac{\\left(\\gamma x/|c|\\right)^{\\alpha}\\beta_{Y}-\\left(\\gamma x/|a|\\right)^{\\alpha}\\beta_{X}}{\\left(\\gamma x/|c|\\right)^{\\alpha}-\\left(\\gamma x/|a|\\right)^{\\alpha}}\\le1.\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(8}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Multiplying $\\left(\\gamma_{Y}{\\big/}|c|\\right)^{\\alpha}-\\left(\\gamma x{\\big/}|a|\\right)^{\\alpha}$ on both sides of the inequalities in (86). ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left(\\gamma x/|a|\\right)^{\\alpha}-\\left(\\gamma x/|c|\\right)^{\\alpha}\\leq\\left(\\gamma x/|c|\\right)^{\\alpha}\\beta_{Y}-\\left(\\gamma x/|a|\\right)^{\\alpha}\\beta_{X}\\leq\\left(\\gamma x/|c|\\right)^{\\alpha}-\\left(\\gamma x/|a|\\right)^{\\alpha}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Adding $\\left(\\gamma_{X}\\big/|a|\\right)^{\\alpha}\\beta_{X}$ on both sides of the inequalities in (87). ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\left(\\gamma x/|\\mathfrak{a}|\\right)^{\\alpha}+\\left(\\gamma x/|\\mathfrak{a}|\\right)^{\\alpha}\\beta_{X}-\\left(\\gamma x/|\\mathfrak{c}|\\right)^{\\alpha}\\leq\\left(\\gamma x/|\\mathfrak{c}|\\right)^{\\alpha}\\beta_{Y}\\leq\\left(\\gamma x/|\\mathfrak{c}|\\right)^{\\alpha}-\\left(\\gamma x/|\\mathfrak{a}|\\right)^{\\alpha}+\\left(\\gamma x/|\\mathfrak{a}|\\right)^{\\alpha}\\beta_{X},}\\\\ &{}&{\\Rightarrow\\left(1+\\beta_{X}\\right)\\left(\\gamma x/|\\mathfrak{a}|\\right)^{\\alpha}-\\left(\\gamma x/|\\mathfrak{c}|\\right)^{\\alpha}\\leq\\left(\\gamma x/|\\mathfrak{c}|\\right)^{\\alpha}\\beta_{Y}\\leq\\left(\\gamma x/|\\mathfrak{c}|\\right)^{\\alpha}+\\left(\\gamma x/|\\mathfrak{a}|\\right)^{\\alpha}\\left(\\beta_{X}-1\\right).\\qquad\\qquad(8\\delta_{X})=\\left(\\gamma x/|\\mathfrak{c}|\\right)^{\\alpha}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Dividing $\\left(\\gamma_{Y}/{\\vert c\\vert}\\right)^{\\alpha}$ on both sides of the inequalities in (88). ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(1+\\beta_{X}\\right)\\left(\\gamma_{X}|c|/|a|\\gamma_{Y}\\right)^{\\alpha}-1\\leq\\beta_{Y}\\leq1+\\left(\\gamma_{X}|c|/|a|\\gamma_{Y}\\right)^{\\alpha}\\left(\\beta_{X}-1\\right)\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Simplifying the inequalities in (89). ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(1+\\beta_{X}\\right)\\left(\\gamma x\\,|c|/|a|\\gamma_{Y}\\right)^{\\alpha}\\leq1+\\beta_{Y},}\\\\ {\\beta_{Y}-1\\leq\\left(\\gamma x\\,|c|/|a|\\gamma_{Y}\\right)^{\\alpha}(\\beta_{X}-1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Multiplying $-1$ on both sides in (91): ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\left(\\gamma_{X}|c|\\middle/|a|\\gamma_{Y}\\right)^{\\alpha}\\left(1-\\beta_{X}\\right)\\leq1-\\beta_{Y}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Hence, by above analysis we can conclude that the inequality $|\\beta^{\\prime}|\\leq1$ is equivalent to the inequalities: $\\left(\\gamma_{X}|c|{\\Big/}|a|{\\bar{\\gamma}}_{Y}\\right)^{\\alpha}\\left(1-\\beta_{X}^{\\cdot}\\right)\\leq1-\\beta_{Y}$ and $\\left(1+\\beta_{X}\\right)\\left(\\gamma_{X}|c|/|a|\\gamma_{Y}\\right)^{\\dot{\\alpha}}\\leq1+\\beta_{Y}$ , which ensures $\\beta^{\\prime}\\in[-1,1]$ . As $\\beta^{\\prime}\\,\\in\\,[-1,1]$ , $\\gamma^{\\prime}\\;\\in\\;(0,\\infty)$ and $\\mu^{\\prime}\\,\\in\\,\\mathbb{R}$ , we can conclude $\\epsilon$ follows a legitimate univariate continuous stable distribution. Furthermore, we define $\\epsilon$ $\\perp\\mathbf{\\Psi}(M,X,X^{\\prime})$ , i.e., $\\mathrm{Pr}(\\epsilon|M,X,X^{\\prime})=$ $\\operatorname*{Pr}(\\epsilon)$ and $\\operatorname*{Pr}(M,X,X^{\\prime}|\\epsilon)=\\operatorname*{Pr}(M,X,X^{\\prime})$ . ", "page_idx": 31}, {"type": "text", "text": "We now use $\\epsilon$ to specify $Q_{M C}(Y^{\\prime}|X^{\\prime})$ as follows: ", "page_idx": 31}, {"type": "equation", "text": "$$\nY^{\\prime}=X^{\\prime}+\\epsilon.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We also derive the distribution $Q(Y^{\\prime}|M)$ before proceeding with our construction, as we will need it later in the proof to show that the constructed $\\bar{Q_{M C}}(M,\\bar{X_{}}Y)\\in\\Delta_{P}$ . ", "page_idx": 31}, {"type": "text", "text": "Deriving $Q_{M C}(Y^{\\prime}|M)$ : We will use the distributions $Q_{M C}(X^{\\prime}|M)$ and $Q_{M C}(\\epsilon)$ to derive the distribution of $Q_{M C}(\\dot{Y^{\\prime}}|M)$ : ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{M C}(Y^{\\prime}\\!=\\!y^{\\prime}|M\\!=\\!m)=Q_{M C}(X^{\\prime}+\\epsilon\\!=\\!y^{\\prime}|M\\!=\\!m),}\\\\ &{\\qquad\\qquad\\qquad\\quad=\\int_{k=-\\infty}^{\\infty}Q_{M C}(X^{\\prime}\\!=\\!k,\\epsilon\\!=\\!y^{\\prime}-k|M\\!=\\!m)d k,}\\\\ &{\\qquad\\qquad\\quad=\\int_{k=-\\infty}^{\\infty}Q_{M C}(\\epsilon=y^{\\prime}\\!-\\!k|M\\!=\\!m)Q_{M C}(X^{\\prime}\\!=\\!k|\\epsilon=y^{\\prime}\\!-\\!k,M\\!=\\!m)d k.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Using the fact that $\\epsilon$ \u22a5\u22a5 $M$ and $\\epsilon$ \u22a5\u22a5 $X^{\\prime}|M$ , we have: ", "page_idx": 31}, {"type": "equation", "text": "$$\nQ_{M C}(Y^{\\prime}=y^{\\prime}|M=m)=\\int_{k=-\\infty}^{\\infty}Q_{M C}(\\epsilon=y^{\\prime}-k)Q_{M C}(X^{\\prime}\\!=\\!k|M\\!=\\!m)d k.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Since $Q_{M C}(\\epsilon)$ and $Q_{M C}(X^{\\prime}|M\\,=\\,m)$ are univariate continuous stable distributions for a fixed $m$ (see (84) and (83)), equation (94) describes a convolution of two univariate stable distributions for a fixed $m$ . Convolution of two univariate continuous stable distribution is akin to summing two independent random variable having the same univariate continuous stable distributions, and consequently $Q_{M C}(Y^{\\prime}|M)$ can be derived using lemma 6. Hence, ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\mu}=m+\\displaystyle\\frac{b}{a}+\\mu^{\\prime}=m+\\displaystyle\\frac{b}{a}+\\frac{d}{c}-\\frac{b}{a}=m+\\displaystyle\\frac{d}{c},}\\\\ &{\\tilde{\\gamma}=\\left(\\left(\\frac{\\gamma\\chi}{\\|a\\|}\\right)^{\\alpha}+\\gamma^{\\alpha}\\right)^{\\frac{1}{\\alpha}}=\\left(\\left(\\frac{\\gamma\\chi}{\\|a\\|}\\right)^{\\alpha}+\\left(\\frac{\\gamma\\gamma}{\\|c\\|}\\right)^{\\alpha}-\\left(\\frac{\\gamma\\chi}{\\|a\\|}\\right)^{\\alpha}\\right)^{\\frac{1}{\\alpha}}=\\frac{\\gamma\\gamma}{\\left|c\\right|},}\\\\ &{\\tilde{\\beta}=\\frac{(\\gamma x/\\lfloor a\\rfloor)^{\\alpha}\\operatorname{sgn}(a)\\beta\\chi+\\gamma^{\\alpha}\\beta^{\\prime}}{(\\gamma x/\\lfloor a\\rfloor)^{\\alpha}+\\gamma^{\\alpha}}}\\\\ &{=\\frac{(\\gamma x/\\lfloor a\\rfloor)^{\\alpha}\\operatorname{sgn}(a)\\beta\\chi+((\\gamma x/\\lfloor c\\rfloor)^{\\alpha}-(\\gamma x/\\lfloor a\\rfloor)^{\\alpha})\\left(\\operatorname{sgn}(a)\\frac{(\\gamma x/\\lfloor c\\rfloor)^{\\alpha}\\beta\\gamma-(\\gamma x/\\lfloor a\\rfloor)^{\\alpha}\\beta}{(\\gamma x/\\lfloor c\\rfloor)^{\\alpha}}\\right)}{(\\gamma x/\\lfloor c\\rfloor)^{\\alpha}+(\\gamma x/\\lfloor c\\rfloor)^{\\alpha}-(\\gamma x/\\lfloor a\\rfloor)^{\\alpha}}}\\\\ &{=\\operatorname{sgn}(a)\\frac{(\\gamma x/\\lfloor a\\rfloor)^{\\alpha}\\beta\\chi+(\\gamma\\gamma/\\lfloor c\\rfloor)^{\\alpha}\\beta\\gamma-(\\gamma x/\\lfloor c\\rfloor)^{\\alpha}\\beta\\chi}{(\\gamma x/\\lfloor c\\rfloor)^{\\alpha}}=s\\operatorname{gn}(a)\\frac{(\\gamma\\vee/\\gamma\\rfloor\\vee)^{\\alpha}\\beta\\gamma}{(\\gamma x/\\lfloor c\\rfloor)^{\\alpha}}}\\\\ &{=s\\operatorname{gn}(a)\\beta\\gamma.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Specifying $Q_{M C}(Y|Y^{\\prime})$ : $Q_{M C}(Y|Y^{\\prime})$ is specified by the following deterministic transformation: ", "page_idx": 32}, {"type": "equation", "text": "$$\nY={\\left\\{\\begin{array}{l l}{c Y^{\\prime}}&{\\alpha\\not=1}\\\\ {c Y^{\\prime}+(2\\mathrm{sgn}(a)c\\beta_{Y}\\gamma_{Y}\\log(|c|))/\\pi}&{\\alpha=1}\\end{array}\\right.}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Finally, we construct the desired $Q_{M C}(M,X,Y)$ from $Q_{M C}(M,X,X^{\\prime},Y^{\\prime},Y)$ by marginalizing $X^{\\prime}$ and $Y^{\\prime}$ . ", "page_idx": 32}, {"type": "text", "text": "Part 2: Showing $Q_{M C}(M,X,Y)\\in\\Delta_{P}$ ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "For showing that $Q_{M C}(M,X,Y)\\in\\Delta_{P}$ , we first need to calculate $Q_{M C}(Y|M)$ . ", "page_idx": 32}, {"type": "text", "text": "Calculating $Q(Y|M)$ : We use lemma 5 to calculate $Q(Y|M)$ as from (95), we know that $Q_{M C}(Y^{\\prime}|M)\\,=\\,p_{C S}(Y^{\\prime};\\alpha,{\\bf s g n}(a)\\beta_{Y},\\gamma x\\big/|c|,M+\\,d/c)$ is a continuous stable distribution for a fixed $m$ , and $Y$ a translated and scaled version of $Y^{\\prime}$ . The exact expression of $Q_{M C}(Y|M=m)$ is provided in (97). ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall M C\\{\\imath\\ |\\ m=m\\jmath=p c S(\\alpha,\\rho_{Y},\\gamma_{Y},\\mu_{Y}),\\ \\mathrm{wners}}\\\\ &{\\tilde{\\beta}_{Y}=\\left\\{\\begin{array}{l l}{\\mathrm{sgn}(c)\\mathrm{sgn}(c)\\beta_{Y}\\quad\\alpha\\neq1}\\\\ {\\mathrm{sgn}(c)\\mathrm{sgn}(a)\\beta_{Y}\\quad\\alpha=1}\\end{array}\\right.=\\mathrm{sgn}(a)\\mathrm{sgn}(c)\\beta_{Y}=\\mathrm{sgn}(a c)\\beta_{Y},}\\\\ &{\\tilde{\\gamma}_{Y}=\\left\\{\\begin{array}{l l}{|c|\\frac{\\gamma_{Y}}{|c|}}&{\\alpha\\neq1}\\\\ {|c|\\frac{\\gamma_{Y}}{|c|}}&{\\alpha=1}\\end{array}\\right.=\\gamma_{Y},}\\\\ &{\\tilde{\\mu}_{Y}=\\left\\{\\begin{array}{l l}{\\alpha\\neq1}&{c(m+\\frac{d}{c})}\\\\ {c(m+\\frac{d}{c})+2\\mathrm{sgn}(a)c\\beta_{Y}\\gamma_{Y}\\log(|c|)/\\pi-2\\mathrm{sgn}(a)c\\beta_{Y}\\gamma_{Y}\\log(|c|)/\\pi}&{\\alpha=1}\\end{array}\\right.=c m+d,}\\\\ &{\\Rightarrow Q_{M C}(Y|M=m)=p_{C S}(\\alpha,\\beta_{Y}\\mathrm{sgn}(a c),\\gamma_{Y},c m+d)=P(Y|M).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "From (81) and (97), we can conclude: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{M C}(M,X)=Q_{M C}(M)Q_{M C}(X|M)=P(M)P(X|M)=P(M,X),}\\\\ &{Q_{M C}(M,Y)=Q_{M C}(M)Q_{M C}(Y|M)=P(M)P(Y|M)=P(M,Y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Hence, $Q_{M C}(M,X,Y)\\in\\Delta_{P}$ and consequently, by proposition 1, $U I(M;Y\\backslash X)=0$ , concluding our proof for case 1. ", "page_idx": 32}, {"type": "text", "text": "The proof of case 2 is extremely similar to case 1. For case 2, we are able to directly construct the distribution $Q_{M C}(M,X,Y)$ without the need of specifying a larger Markov chain. ", "page_idx": 32}, {"type": "text", "text": "Part 1: Specifying $Q_{M C}(M,X,Y)$ ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Similarly to case 1, we specify $Q_{M C}(M,X,Y)$ by individually specifying $Q_{M C}(M)$ , $Q_{M C}(X|M)$ and $Q_{M C}(Y|X)$ . The distributions $Q_{M C}(M)$ and $Q_{M C}(X|M)$ are as follows: ", "page_idx": 32}, {"type": "text", "text": "The distribution $Q_{M C}(Y|X)$ is defined using the following deterministic transformation: ", "page_idx": 33}, {"type": "equation", "text": "$$\nY=\\left\\{\\begin{array}{c l}{c/a X-c b/a+d}&{\\alpha\\neq1}\\\\ {\\frac{c}{a}X+2c/a\\pi\\beta\\gamma_{X}\\log\\left(\\left|c/a\\right|\\right)-c b/a+d}&{\\alpha=1}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Part 2: Showing $Q_{M C}(M,X,Y)\\in\\Delta_{P}$ ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "For showing that $Q_{M C}(M,X,Y)\\in\\Delta_{P}$ , we first need to calculate $Q_{M C}(Y|M)$ . ", "page_idx": 33}, {"type": "text", "text": "Calculating $Q(Y|M)$ : We use lemma 5 to calculate $Q(Y|M)$ as $Q_{M C}(X|M{=}m)$ is a continuous stable distribution for a fixed $m$ , and $Y$ a translated and scaled version of $X$ . The exact expression of $Q_{M C}(Y|M=m)$ is provided in (102). ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{M C}(Y|M=m)=p_{C S}(y;\\alpha,\\Tilde{\\beta},\\Tilde{\\gamma},\\Tilde{\\mu}),}\\\\ &{\\Tilde{\\mu}=\\left\\{\\begin{array}{l l}{\\qquad}&{\\qquad c a/a m+\\frac{c b}{a}-\\frac{b c}{a}+d}\\\\ {\\qquad}&{\\qquad\\stackrel{\\alpha b}{a}-\\frac{b c}{a}+d-2c/a\\pi\\beta\\gamma_{X}\\log\\left(\\lvert c/a\\rvert\\right)+2c/a\\pi\\beta\\gamma_{X}\\log\\left(\\lvert c/a\\rvert\\right)}&{\\alpha=1}\\end{array}\\right.}\\\\ &{\\quad=c m+d,}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\tilde{\\beta}=\\beta\\mathbf{sgn}(c/a)=\\beta\\mathbf{sgn}(a c),}\\\\ &{\\Rightarrow Q_{M C}(Y|M=m)=p_{C S}(\\alpha,\\mathbf{sgn}(a c)\\beta_{Y},\\gamma_{Y},c m+d)=P(Y|M).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "From (100) and (102), we can conclude: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{M C}(M,X)=Q_{M C}(M)Q_{M C}(X|M)=P(M)P(X|M)=P(M,X),}\\\\ &{Q_{M C}(M,Y)=Q_{M C}(M)Q_{M C}(Y|M)=P(M)P(Y|M)=P(M,Y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Hence, $Q_{M C}(M,X,Y)\\in\\Delta_{P}$ and consequently, by proposition 1, $U I(M;Y\\backslash X)=0$ , concluding our proof for case 2 and theorem 1. ", "page_idx": 33}, {"type": "text", "text": "G Proof of Theorem 2 ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In this section, we provide the proof of theorem 2. Since theorem 2 provides the analytical PID terms for system 1 of the multivariate affine continuous stable system defined in Sec. 4, we briefly restate certain key properties of the corresponding independent component multivariate stable distributions and the corresponding multivariate affine continuous stable system for convenience. ", "page_idx": 33}, {"type": "text", "text": "G.1 Independent component multivariate stable distribution ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Independent component multivariate stable distributions are a specific multivariate generalization of univariate continuous stable distributions describing a collection of independent random variables, with each random variable distributed according to a univariate continuous stable distribution. We refer the reader to Appx. N.3 for more details on multivariate continuous stable distributions. We now list certain key properties of independent component multivariate stable distributions that we make use of in the proof of theorem 2: ", "page_idx": 33}, {"type": "text", "text": "1. If $\\vec{\\bf X}$ is distributed according to an independent component multivariate stable distribution, then the sum of two independent copies of $\\vec{\\bf X}$ , denoted as $\\vec{\\bf X}_{1}$ and $\\vec{\\bf X}_{2}$ , follows an independent component multivariate stable distribution up to a scaling and translation operation, i.e., $a\\vec{\\bf X}_{1}+b\\vec{\\bf X}_{2}\\stackrel{d}{=}c\\vec{\\bf X}+\\vec{\\bf d}$ for $a,b,c>0$ and $\\vec{\\bf d}\\in\\mathbb{R}$ .   \n2. We denote the p.d.f. of the independent component multivariate stable continuous distribution as $p_{C S-I C}(\\alpha,\\vec{\\beta},\\vec{\\gamma},\\vec{\\mu})$ , where $\\begin{array}{r l}&{\\vec{\\beta}=\\left[\\beta_{1}\\quad\\cdot\\,\\cdot\\quad\\beta_{d}\\right]^{T},\\ \\mathrm{with}\\ \\beta_{j}\\in\\left[-1,1\\right],\\vec{\\gamma}=\\left[\\gamma_{1}\\quad\\cdot\\,\\cdot\\quad\\gamma_{d}\\right]^{T}\\ \\mathrm{with}\\ \\gamma_{j}\\in(0,\\infty),}\\\\ &{\\vec{\\mu}=\\left[\\mu_{1}\\quad\\cdot\\,\\cdot\\quad\\mu_{d}\\right]^{T}\\in\\mathbb{R}^{d},\\ \\mathrm{and}\\ \\alpha\\in(0,2].}\\end{array}$ ", "page_idx": 33}, {"type": "text", "text": "In general, the p.d.f. of independent component multivariate stable distribution do not have a closed-form analytical expression, and are expressed through their characteristic function. ", "page_idx": 33}, {"type": "text", "text": "3. The characteristic function of a $d$ -dimensional random vector $\\vec{\\bf X}$ having independent component multivariate distribution $p_{C S-I C}(\\alpha,\\vec{\\beta},\\vec{\\gamma},\\vec{\\mu})$ is expressed as follows: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb E\\left[e^{i\\vec{\\bf t}^{T}\\vec{\\bf X}}\\right]=\\exp\\left(-\\displaystyle\\sum_{j=1}^{d}|\\gamma_{j}t_{j}|^{\\alpha}(1-i\\beta_{j}{\\bf s g n}(t_{j})\\Phi(\\alpha))+i\\vec{\\bf t}^{T}\\vec{\\mu}\\right)\\ \\forall\\,\\vec{\\bf t}\\in\\mathbb R^{d},}\\\\ &{\\mathrm{where~}\\Phi(\\alpha)=\\left\\{\\begin{array}{l l}{\\mathrm{tan}\\left(\\frac{\\pi\\alpha}{2}\\right)}&{\\alpha\\neq1}\\\\ {\\frac{-2}{\\pi}\\log(|t|)}&{\\alpha=1}\\end{array}\\right.,\\ \\ \\mathrm{sgn}(t)=\\left\\{\\begin{array}{l l}{-1}&{t<0}\\\\ {0}&{t=0}\\end{array}\\right..}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "4. The random vector ${\\vec{\\mathbf{X}}}\\,=\\,\\left[X_{1}\\quad.\\ldots\\quad X_{d}\\right]^{T}$ having the distribution $p_{C S-I C}(\\alpha,\\vec{\\beta},\\vec{\\gamma},\\vec{\\mu})$ is essentially a collection of independent random variables $\\{X_{j}\\}_{j=1}^{d}$ , where $X_{j}~\\sim$ $p_{C S}(\\alpha,\\beta_{j},\\gamma_{j},\\mu_{j})$ . ", "page_idx": 34}, {"type": "text", "text": "G.2 Definition of the system 1 of the multivariate affine continuous stable system ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Let the random variable $M\\sim P(M)$ with support ${\\mathcal{M}}\\subseteq\\mathbb{R}$ . $\\vec{\\bf X}$ and $\\vec{\\bf Y}$ are $d_{X}$ -dimensional and $d_{Y}$ -dimensional random vectors, respectively. The joint distribution $P(M,{\\vec{\\mathbf{X}}},{\\vec{\\mathbf{Y}}})$ describes system 1 of the multivariate affine continuous stable system if it satisfies the following two properties ", "page_idx": 34}, {"type": "text", "text": "1. $M\\sim P(M)$ having some support set ${\\mathcal{M}}\\subseteq\\mathbb{R}$ .   \n2. The conditional distributions of random variables $\\vec{\\bf X}$ and $\\vec{\\bf Y}$ conditioned on $M$ are expressed as follows: $\\vec{\\bf X}\\!=\\!\\vec{\\bf H}_{X}M+{\\bf A}_{X}\\vec{\\bf Z}_{X}+\\vec{\\bf b}_{X}{\\mathrm{~and~}}\\vec{\\bf Y}\\!=\\!\\vec{\\bf H}_{Y}M+{\\bf A}_{Y}\\vec{\\bf Z}_{Y}+{\\bf b}_{Y}^{\\backprime},$ (106) where $\\vec{\\bf Z}_{X}\\,\\sim\\,p_{C S-I C}(\\alpha,\\vec{\\bf0}_{d_{X}},\\vec{\\bf1}_{d_{X}},\\vec{\\bf0}_{d_{X}})$ , $\\vec{\\cal Z}_{Y}\\,\\sim\\,p_{C S-I C}(\\alpha,\\vec{0}_{d_{Y}},\\vec{1}_{d_{Y}},\\vec{0}_{d_{Y}})$ , $\\mathbf{A}_{X}$ and $\\mathbf{A}_{Y}$ are invertible matrices, $\\vec{\\bf H}_{X},\\vec{\\bf b}_{X}\\in\\mathbb{R}^{d_{X}}$ , and $\\vec{\\bf H}_{Y}$ , $\\vec{\\bf b}_{Y}\\in\\mathbb{R}^{d_{Y}}$ . ", "page_idx": 34}, {"type": "text", "text": "G.3 Formal proof of Theorem 2 ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Theorem 2. Let the joint distribution $P(M,{\\vec{\\mathbf{X}}},{\\vec{\\mathbf{Y}}})$ of $M,{\\vec{\\mathbf{X}}}$ , and $\\vec{\\bf Y}$ describe the system $^{\\,l}$ of the multivariate affine continuous stable system. Without the loss of generality, assume $\\|\\mathbf{A}_{Y}^{-1}\\vec{\\mathbf{H}}_{Y}\\|_{\\kappa}\\leq$ $\\|\\mathbf{A}_{X}^{-1}\\vec{\\mathbf{H}}_{X}\\|_{\\kappa}$ , where $\\kappa=\\alpha/\\alpha\\!-\\!1\\,\\forall\\,\\alpha\\in(1,2]$ and $\\kappa=\\infty\\,\\forall\\,\\alpha\\in(0,1]$ . Then, $\\Delta_{P}$ contains a Markov chain of the form $M\\rightarrow\\vec{\\mathbf{X}}\\rightarrow\\vec{\\mathbf{Y}}$ and $U I(M;{\\vec{\\mathbf{Y}}}\\backslash{\\vec{\\mathbf{X}}})=0$ . ", "page_idx": 34}, {"type": "text", "text": "Proof. We first note that we can always assume $\\|\\mathbf{A}_{Y}^{-1}\\vec{\\mathbf{H}}_{Y}\\|_{\\kappa}\\,\\leq\\,\\|\\mathbf{A}_{X}^{-1}\\vec{\\mathbf{H}}_{X}\\|_{\\kappa}$ without the loss of generality because if $\\|\\mathbf{A}_{Y}^{-1}\\vec{\\mathbf{H}}_{Y}\\|_{\\kappa}\\geq\\|\\mathbf{A}_{X}^{-1}\\vec{\\mathbf{H}}_{X}\\|_{\\kappa}$ , then we can always switch our nomenclature to refer to $\\vec{\\bf Y}$ as $\\vec{\\bf X}$ , and $\\vec{\\bf X}$ as $\\vec{\\bf Y}$ . ", "page_idx": 34}, {"type": "text", "text": "The proof of above theorem relies on the result of lemma 7 and the fact that the linear system described in (106) can always be reduced to the special case of the linear system used in lemma 7. We briefly outline the proof structure. ", "page_idx": 34}, {"type": "text", "text": "1. In the first part, we explicitly construct a joint distribution $Q_{M C}(M,{\\vec{\\mathbf{X}}},{\\vec{\\mathbf{Y}}})$ having the Markovian structure $M\\rightarrow\\vec{\\mathbf{X}}\\rightarrow\\vec{\\mathbf{Y}}$ . 2. In the second part, we show that the $Q_{M C}(M,{\\vec{\\mathbf{X}}},{\\vec{\\mathbf{Y}}})$ constructed in the first part, lies in $\\Delta_{P}$ . Therefore, we can then apply the result of proposition 1 to conclude $U I(M;{\\vec{\\mathbf{Y}}}\\backslash{\\vec{\\mathbf{X}}})=0$ . ", "page_idx": 34}, {"type": "text", "text": "Part 1: Specifying $Q_{M C}(M,{\\vec{\\mathbf{X}}},{\\vec{\\mathbf{Y}}})$ ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "We explicitly construct the desired Markov chain $M\\rightarrow\\vec{\\mathbf{X}}\\rightarrow\\vec{\\mathbf{Y}}$ by constructing a larger Markov chain $M\\rightarrow\\bar{\\mathbf{X}}\\rightarrow\\vec{\\mathbf{X}}^{\\prime}\\rightarrow\\vec{\\mathbf{Y}}^{\\prime}\\rightarrow\\vec{\\mathbf{Y}}^{\\prime}$ and then marginalizing the larger Markov chain to obtain the desired Markov chain $M\\rightarrow\\vec{\\mathbf{X}}\\rightarrow\\vec{\\mathbf{Y}}$ . ", "page_idx": 34}, {"type": "text", "text": "Denote the joint distribution of the Markov chain $M\\ \\to\\ {\\vec{\\mathbf{X}}}\\ \\to\\ {\\vec{\\mathbf{X}}}^{\\prime}\\ \\to\\ {\\vec{\\mathbf{Y}}}^{\\prime}\\ \\to\\ {\\vec{\\mathbf{Y}}}^{\\prime}\\ \\to\\ {\\vec{\\mathbf{Y}}}$ as $Q_{M C}(M,\\vec{\\bf X},\\vec{\\bf X}^{\\prime},\\vec{\\bf Y}^{\\prime},\\vec{\\bf Y})$ . We can decompose $Q_{M C}(M,\\vec{\\bf X},\\vec{\\bf X}^{\\prime},\\vec{\\bf Y}^{\\prime},\\vec{\\bf Y})$ by utilizing its Markovian structure as follows: ", "page_idx": 35}, {"type": "equation", "text": "$$\nQ_{M C}(M,\\vec{\\mathbf{X}},\\vec{\\mathbf{X}}^{\\prime},\\vec{\\mathbf{Y}}^{\\prime},\\vec{\\mathbf{Y}})=Q_{M C}(M)Q_{M C}(\\vec{\\mathbf{X}}|M)Q_{M C}(\\vec{\\mathbf{X}}^{\\prime}|\\vec{\\mathbf{X}})Q_{M C}(\\vec{\\mathbf{Y}}^{\\prime}|\\vec{\\mathbf{X}}^{\\prime})Q_{M C}(\\vec{\\mathbf{Y}}|\\vec{\\mathbf{Y}}^{\\prime}).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Consequently, we can specify/construct the distribution $Q_{M C}(M,\\vec{\\bf X},\\vec{\\bf X}^{\\prime},\\vec{\\bf Y}^{\\prime},\\vec{\\bf Y})$ by individually specifying $Q_{M C}(M)$ $\\mathbf{\\Phi}^{\\prime}),\\,Q_{M C}(\\vec{\\mathbf{X}}|M),\\,Q_{M C}(\\vec{\\mathbf{X}}^{\\prime}|\\vec{\\mathbf{X}}),\\,Q_{M C}(\\vec{\\mathbf{Y}}^{\\prime}|\\vec{\\mathbf{X}}^{\\prime}).$ , and $Q_{M C}(\\vec{\\bf Y}|\\vec{\\bf Y}^{\\prime})$ . We specify $Q_{M C}(M,\\vec{\\bf X})=P(M,\\vec{\\bf X})$ and $Q(\\vec{\\bf X}^{\\prime}|\\vec{\\bf X})$ using the following deterministic transformation: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\vec{\\bf X}^{\\prime}={\\bf A}_{X}^{-1}\\left(\\vec{\\bf X}-\\vec{\\bf b}_{X}\\right).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Substituting (106) in the above equation: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\vec{\\bf X}^{\\prime}={\\bf A}_{X}^{-1}\\left(\\vec{\\bf H}_{X}M+{\\bf A}_{X}\\vec{\\bf Z}_{X}+\\vec{\\bf b}_{X}-\\vec{\\bf b}_{X}\\right)={\\bf A}_{X}^{-1}\\vec{\\bf H}_{X}M+\\vec{\\bf Z}_{X}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Specifying $Q_{M C}(\\vec{\\bf Y}^{\\prime}|\\vec{\\bf X}^{\\prime})$ : We define $\\vec{\\bf Y^{\\prime}}$ according to the following affine system: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\vec{\\bf Y}^{\\prime}={\\bf A}_{Y}^{-1}\\vec{\\bf H}_{Y}M+\\vec{\\bf Z}_{Y}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Observe that both $\\vec{\\bf X}^{\\prime}$ and $\\vec{\\bf Y^{\\prime}}$ can be described as an affine function of $M$ satisfying the special structure described in lemma 7 (see (115)). Furthermore, $Q_{M C}(M)\\,=\\,P(M)$ by construction. Consequently, $Q_{M C}(M)$ satisfies the properties required by the distribution of $M$ outlined in lemma 7, as $P(M)$ in both lemma 7 and theorem 2 follow the same properties. Since $\\|\\mathbf{A}_{Y}^{-1}\\vec{\\mathbf{H}}_{Y}\\|_{\\kappa}\\leq$ $\\|\\mathbf{A}_{X}^{-1}\\vec{\\mathbf{H}}_{X}\\|_{\\kappa}$ by the assumption in the theorem, we can apply the result of lemma 7 to construct $Q_{M C}(\\vec{\\bf Y}^{\\prime}|\\vec{\\bf X}^{\\prime})$ , where $\\vec{\\bf{H}}_{X}$ and $\\vec{\\bf H}_{Y}$ would be replaced by ${\\bf A}_{X}^{-1}\\vec{\\bf H}_{X}$ and ${\\bf A}_{Y}^{-1}\\vec{\\bf H}_{Y}$ , respectively. ", "page_idx": 35}, {"type": "text", "text": "Lastly, we specify $Q_{M C}(\\vec{\\bf Y}|\\vec{\\bf Y}^{\\prime})$ through the following linear system: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\vec{\\bf Y}={\\bf A}_{Y}\\vec{\\bf Y}^{\\prime}+\\vec{\\bf b}_{Y}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Finally, we construct the desired $Q_{M C}(M,{\\vec{\\mathbf{X}}},{\\vec{\\mathbf{Y}}})$ from $Q_{M C}(M,\\vec{\\bf X},\\vec{\\bf X}^{\\prime},\\vec{\\bf Y}^{\\prime},\\vec{\\bf Y})$ by marginalizing $\\vec{\\bf X}^{\\prime}$ and $\\vec{\\bf Y^{\\prime}}$ . ", "page_idx": 35}, {"type": "text", "text": "Part 2: Showing $Q_{M C}(M,\\vec{\\bf X},\\vec{\\bf Y})\\in\\Delta_{P}$ ", "page_idx": 35}, {"type": "text", "text": "For showing that $Q_{M C}(M,\\vec{\\bf X},\\vec{\\bf Y})\\in\\Delta_{P}$ , we first need to derive $Q_{M C}(\\vec{\\mathbf{Y}}|M)$ . ", "page_idx": 35}, {"type": "text", "text": "Deriving $Q(\\vec{\\mathbf{Y}}|M)$ : Substituting (109) in (110) we obtain: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\vec{\\bf Y}={\\bf A}_{Y}\\big({\\bf A}_{Y}^{-1}\\vec{\\bf H}_{Y}M+\\vec{\\bf Z}_{Y}\\big)+\\vec{\\bf b}_{Y}=\\vec{\\bf H}_{Y}M+{\\bf A}_{Y}\\vec{\\bf Z}_{Y}+\\vec{\\bf b}_{Y}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Inspecting the above equation, we can conclude: ", "page_idx": 35}, {"type": "equation", "text": "$$\nQ_{M C}(\\vec{\\mathbf{Y}}|M)=P(\\vec{\\mathbf{Y}}|M).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "From (112), we can conclude: ", "page_idx": 35}, {"type": "equation", "text": "$$\nQ_{M C}(M,\\vec{\\mathbf{Y}})=Q_{M C}(M)Q_{M C}(\\vec{\\mathbf{Y}}|M)=P(M)P(\\vec{\\mathbf{Y}}|M)=P(M,\\vec{\\mathbf{Y}}).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "We know that by construction: ", "page_idx": 35}, {"type": "equation", "text": "$$\nQ_{M C}(M,\\vec{\\bf X})=P(M,\\vec{\\bf X}).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Hence, $Q_{M C}(M,\\vec{\\bf X},\\vec{\\bf Y})\\in\\Delta_{P}$ and consequently, by proposition 1, $U I(M;{\\vec{\\mathbf{Y}}}\\backslash{\\vec{\\mathbf{X}}})=0$ , concluding our proof. \u5382 ", "page_idx": 35}, {"type": "text", "text": "Lemma 7. Let the joint distribution of $M$ , $\\vec{\\bf X}$ and $\\vec{\\bf Y}$ , denoted as $P(M,{\\vec{\\mathbf{X}}},{\\vec{\\mathbf{Y}}})$ , describe the system $^{\\,l}$ of the multivariate affine continuous stable system. Furthermore, assume $\\mathbf{A}_{X}=\\mathbf{I}_{d_{X}}$ , $\\mathbf{A}_{Y}=\\mathbf{I}_{d_{Y}}$ , $\\vec{\\mathbf{b}}_{X}=\\vec{\\mathbf{0}}_{d_{X}}$ and $\\vec{\\mathbf{b}}_{Y}=\\vec{\\mathbf{0}}_{d_{Y}}$ , where $\\mathbf{I}_{d}$ is a $d\\times d$ identity matrix and $\\vec{\\mathbf{0}}_{d}$ is $d$ -dimensional vector of zeros. Then, ", "page_idx": 35}, {"type": "text", "text": "1. If $\\lVert\\vec{\\bf H}_{Y}\\rVert_{\\kappa}\\,\\le\\,\\lVert\\vec{\\bf H}_{X}\\rVert_{\\kappa}.$ , where $\\kappa=\\alpha/\\alpha\\!-\\!1\\,\\forall\\,\\alpha\\in\\,(1,2]$ and $\\kappa=\\infty\\,\\forall\\,\\alpha\\in(0,1],$ , then $\\Delta_{P}$ contains a Markov chain of the form $M\\rightarrow\\vec{\\mathbf{X}}\\rightarrow\\vec{\\mathbf{Y}}$ and $U I(M;\\vec{\\bf Y}/\\vec{\\bf X})=0$ . ", "page_idx": 36}, {"type": "text", "text": "Proof. We only provide an explicit proof of condition 1. Condition 2 is essentially the same as condition 1, with the parameters of $\\Chi$ and $\\mathrm{\\bfY}$ switched. Consequently, the proof of condition 2 follows the same steps as the proof of condition 1, with parameters about $\\Chi$ and $\\mathrm{\\bfY}$ switched. ", "page_idx": 36}, {"type": "text", "text": "Proof of condition 1: ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "We briefly outline the proof structure. ", "page_idx": 36}, {"type": "text", "text": "1. In the first part, we explicitly construct a joint distribution $Q_{M C}(M,{\\vec{\\mathbf{X}}},{\\vec{\\mathbf{Y}}})$ having the Markovian structure $\\bar{M^{\\mathrm{~}}}\\to\\bar{\\vec{\\mathbf{X}}^{}}\\to\\vec{\\mathbf{Y}}$ . 2. In the second part, we show that the $Q_{M C}(M,{\\vec{\\mathbf{X}}},{\\vec{\\mathbf{Y}}})$ constructed in the first part, lies in $\\Delta_{P}$ . Therefore, we can then apply the result of proposition 1 to conclude $U I(M;{\\vec{\\mathbf{Y}}}\\backslash{\\vec{\\mathbf{X}}})=0$ . ", "page_idx": 36}, {"type": "text", "text": "Part 1: Specifying $Q_{M C}(M,{\\vec{\\mathbf{X}}},{\\vec{\\mathbf{Y}}})$ ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "We explicitly construct the desired Markov chain $M\\rightarrow\\vec{\\mathbf{X}}\\rightarrow\\vec{\\mathbf{Y}}$ by constructing a larger Markov chain $\\mathbf{\\bar{{M}}}\\to\\mathbf{\\bar{X}}\\to\\vec{\\mathbf{X}}^{\\prime}\\to\\vec{\\mathbf{Y}}$ and then marginalizing the larger Markov chain to obtain the desired Markov chain $M\\rightarrow\\vec{\\mathbf{X}}\\rightarrow\\vec{\\mathbf{Y}}$ . ", "page_idx": 36}, {"type": "text", "text": "Denote the joint distribution of the Markov chain $M\\rightarrow\\vec{\\mathbf{X}}\\rightarrow\\vec{\\mathbf{X}}^{\\prime}\\rightarrow\\vec{\\mathbf{Y}}$ as $Q_{M C}(M,\\vec{\\bf X},\\vec{\\bf X}^{\\prime},\\vec{\\bf Y})$ . We can decompose $Q_{M C}(M,\\vec{\\bf X},\\vec{\\bf X}^{\\prime},\\vec{\\bf Y})$ by utilizing its Markovian structure as follows: ", "page_idx": 36}, {"type": "equation", "text": "$$\nQ_{M C}(M,\\vec{\\mathbf{X}},\\vec{\\mathbf{X}}^{\\prime},\\vec{\\mathbf{Y}})=Q_{M C}(M)Q_{M C}(\\vec{\\mathbf{X}}|M)Q_{M C}(\\vec{\\mathbf{X}}^{\\prime}|\\vec{\\mathbf{X}})Q_{M C}(\\vec{\\mathbf{Y}}|\\vec{\\mathbf{X}}^{\\prime}).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Consequently, we can specify/construct the distribution $Q_{M C}(M,\\vec{\\bf X},\\vec{\\bf X}^{\\prime},\\vec{\\bf Y})$ by individually specifying $Q_{M C}(M),Q_{M C}(\\vec{\\bf X}|M),Q_{M C}(\\vec{\\bf X}^{\\prime}|\\vec{\\bf X})$ , and $Q_{M C}(\\vec{\\bf Y}|\\vec{\\bf X}^{\\prime})$ . ", "page_idx": 36}, {"type": "text", "text": "Under the assumptions of lemma 7, we can simplify the linear system describing $P(M,{\\vec{\\mathbf{X}}},{\\vec{\\mathbf{Y}}})$ shown in (106) as follows: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\vec{\\bf X}{=}\\vec{\\bf H}_{X}M+\\vec{\\bf Z}_{X}\\;\\mathrm{and}\\;\\vec{\\bf Y}{=}\\vec{\\bf H}_{Y}M+\\vec{\\bf Z}_{Y}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Correspondingly, we can write the characteristic functions of conditional distributions $P(\\vec{\\mathbf{X}}|M=m)$ ) and $P(\\vec{\\mathbf{Y}}|M=m)$ for a fixed $m$ by employing lemma 9: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[e^{i\\vec{\\bf t}^{T}\\vec{\\bf X}}|M{=}m]=\\exp\\left(-\\displaystyle\\sum_{j=1}^{n}|t_{j}|^{\\alpha}+i\\vec{\\bf t}^{T}\\vec{\\bf H}_{X}m\\right),}\\\\ &{\\mathbb{E}[e^{i\\vec{\\bf t}^{T}\\vec{\\bf Y}}|M{=}m]=\\exp\\left(-\\displaystyle\\sum_{j=1}^{n}|t_{j}|^{\\alpha}+i\\vec{\\bf t}^{T}\\vec{\\bf H}_{Y}m\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where the characteristic function can be derived by realizing that for a given $m$ , $\\vec{\\bf{H}}_{X}m$ and $\\vec{\\bf{H}}_{Y}m$ are constants being added to the random vectors $\\vec{\\bf Z}_{X}\\;\\sim\\;p_{C S-I C}(\\alpha,\\vec{\\bf0}_{d_{x}},\\vec{\\bf1}_{d_{x}},\\vec{\\bf0}_{d_{x}})$ and $\\vec{\\mathbf{Z}}_{Y}\\,\\sim$ $p_{C S-I C}(\\alpha,\\vec{\\bf0}_{d_{y}},\\vec{\\bf1}_{d_{y}},\\vec{\\bf0}_{d_{y}})$ , respectively, and then using the result of lemma 9. ", "page_idx": 36}, {"type": "text", "text": "Specifying $Q_{M C}(M)$ and $\\underline{{Q}}_{M C}(\\vec{\\bf X}|M)$ : We specify $Q_{M C}(M)$ and $Q_{M C}(\\vec{\\bf X}|M)$ as follows: ", "page_idx": 36}, {"type": "equation", "text": "$$\nQ_{M C}(M)=P(M)\\;\\mathrm{and}\\;\\;Q_{M C}(\\vec{\\bf X}|M)=P(\\vec{\\bf X}|M),\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $P(M)$ and $P(\\vec{\\bf X}|M)$ are marginal distributions derived from the original joint distribution $P(M,{\\vec{\\mathbf{X}}},{\\vec{\\mathbf{Y}}})$ (discussed in the lemma statement) over which the bivariate PID is being calculated. ", "page_idx": 36}, {"type": "text", "text": "Specifying $Q_{M C}(\\vec{\\bf X}^{\\prime}|\\vec{\\bf X})$ : We specify $Q_{M C}(\\vec{\\bf X}^{\\prime}|\\vec{\\bf X})$ through the following deterministic transformation: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\vec{\\bf X}^{\\prime}=\\vec{\\bf H}_{Y}(\\vec{\\bf H}_{X}^{k})^{T}\\vec{\\bf X},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $\\vec{\\bf H}_{X}^{k}$ is as defined in lemma 10, and $k\\in[0,\\infty]$ . ", "page_idx": 37}, {"type": "text", "text": "We will also derive the distribution $Q(\\vec{\\mathbf{X}}^{\\prime}|M)$ before proceeding with our construction, as we will need it later in the proof to show that the constructed $Q_{M C}(M,\\vec{\\bf X},\\vec{\\bf Y})\\in\\Delta_{P}$ . ", "page_idx": 37}, {"type": "text", "text": "Deriving $Q_{M C}(\\vec{\\bf X}^{\\prime}|M)$ : To derive $Q_{M C}(\\vec{\\bf X}^{\\prime}|M)$ , we represent $\\vec{\\bf X}^{\\prime}$ as function of $M$ and $\\vec{\\bf Z}_{X}$ in a similar manner to $\\vec{\\bf X}$ in (115), and then use the result of lemma 9 to derive its conditional characteristic function $\\mathbb{E}[e^{\\vec{\\mathbf{t}}^{T}\\vec{\\mathbf{x}}^{\\prime}}|M]$ , and correspondingly the desired $Q_{M C}(\\vec{\\bf X}^{\\prime}|M)$ . ", "page_idx": 37}, {"type": "text", "text": "We multiply $\\vec{\\bf{H}}_{Y}(\\vec{\\bf{H}}_{X}^{k})^{T}$ on both sides of equality in (115) to obtain the following linear equation: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\vec{\\mathbf{H}}_{Y}(\\vec{\\mathbf{H}}_{X}^{k})^{T}\\vec{\\mathbf{X}}=\\vec{\\mathbf{H}}_{Y}(\\vec{\\mathbf{H}}_{X}^{k})^{T}(\\vec{\\mathbf{H}}_{X}M+\\vec{\\mathbf{Z}}_{X})=\\vec{\\mathbf{H}}_{Y}(\\vec{\\mathbf{H}}_{X}^{k})^{T}\\vec{\\mathbf{H}}_{X}M+\\vec{\\mathbf{H}}_{Y}(\\vec{\\mathbf{H}}_{X}^{k})^{T}\\vec{\\mathbf{Z}}_{X}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Substituting (118) in the above equation, we obtain: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\vec{\\mathbf{X}}^{\\prime}=\\vec{\\mathbf{H}}_{Y}(\\vec{\\mathbf{H}}_{X}^{k})^{T}\\vec{\\mathbf{X}}=\\vec{\\mathbf{H}}_{Y}(\\vec{\\mathbf{H}}_{X}^{k})^{T}\\vec{\\mathbf{H}}_{X}M+\\vec{\\mathbf{H}}_{Y}(\\vec{\\mathbf{H}}_{X}^{k})^{T}\\vec{\\mathbf{Z}}_{X}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Using the result of lemma 10, i.e., $\\vec{\\bf{H}}_{Y}(\\vec{\\bf{H}}_{X}^{k})^{T}\\vec{\\bf{H}}_{X}=\\vec{\\bf{H}}_{Y}$ , in the above equation we obtain: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\vec{\\bf X}^{\\prime}=\\vec{\\bf H}_{Y}M+\\vec{\\bf H}_{Y}(\\vec{\\bf H}_{X}^{k})^{T}\\vec{\\bf Z}_{X}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "From (120), we can observe that $\\vec{\\bf X}^{\\prime}$ is an affine function of $\\vec{\\bf Z}_{X}$ for a fixed $m$ . Hence, we can employ the result of lemma 9 to derive the distribution of $Q_{M C}(\\vec{\\bf X}^{\\prime}|M)$ (by deriving its corresponding characteristic function). ", "page_idx": 37}, {"type": "text", "text": "Note that the $j$ -th column of $\\vec{\\bf{H}}_{Y}(\\vec{\\bf{H}}_{X}^{k})^{T}$ can be expressed as $h_{j}^{\\prime}\\vec{\\mathbf{H}}_{Y}$ , where $h_{j}^{\\prime}$ is the $j$ -th component of $\\vec{\\bf H}_{X}^{k}$ . Consequently: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb E\\left[\\left.e^{i\\overline{{t}}^{T}\\overline{{\\mathbf X}}^{\\prime}}\\right\\rvert M=m\\right]=\\exp\\left(-\\displaystyle\\sum_{j=1}^{d_{X}}\\left\\lvert h_{j}^{\\prime}\\vec{\\mathbf H}_{Y}^{T}\\right\\rvert^{\\alpha}+i\\overline{{\\mathbf t}}^{T}\\vec{\\mathbf H}_{Y}m\\right)}\\\\ &{\\qquad\\qquad=\\exp\\left(-\\displaystyle\\sum_{j=1}^{d_{X}}\\left\\lvert h_{j}^{\\prime}\\right\\rvert^{\\alpha}\\left\\lvert\\vec{\\mathbf H}_{Y}^{T}\\vec{\\mathbf t}\\right\\rvert^{\\alpha}+i\\vec{\\mathbf t}^{T}\\vec{\\mathbf H}_{Y}m\\right)}\\\\ &{\\qquad\\qquad=\\exp\\left(-\\left\\lvert\\vec{\\mathbf H}_{Y}^{T}\\vec{\\mathbf t}\\right\\rvert^{\\alpha}\\left(\\displaystyle\\sum_{j=1}^{d_{X}}\\left\\lvert h_{j}^{\\prime}\\right\\rvert^{\\alpha}\\right)+i\\vec{\\mathbf t}^{T}\\vec{\\mathbf H}_{Y}m\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We substitute the value $h_{j}^{\\prime}$ in (121) using the definition of $\\vec{\\bf H}_{X}^{k}$ given in lemma 10. We will divide the substitution into two cases: ", "page_idx": 37}, {"type": "text", "text": "Case 1: $k$ is finite, i.e., $k\\in[0,\\infty)$ . Then, we have $h_{j}^{\\prime}=|h_{j}^{X}|^{k}/||\\vec{\\bf H}_{X}||_{1+k}^{1+k}$ , which implies: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[e^{i\\overline{{\\mathbf{t}}}^{T}\\overline{{\\mathbf{X}}}^{\\prime}}\\Big\\vert\\,M=m\\right]=\\exp\\left(-\\left\\vert\\overrightarrow{\\mathbf{H}}_{Y}^{T}\\boldsymbol{\\mathsf{t}}\\right\\vert^{\\alpha}\\sum_{j=1}^{M_{X}}\\frac{\\left\\vert h_{j}^{X}\\right\\vert^{k\\alpha}}{\\left\\vert\\left\\vert\\overrightarrow{\\mathbf{H}}_{X}\\right\\vert\\right\\vert_{1+k}^{\\alpha+k\\alpha}}+i\\overline{{\\mathbf{t}}}^{T}\\overrightarrow{\\mathbf{H}}_{Y}m\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\exp\\left(-\\frac{\\left\\vert\\overrightarrow{\\mathbf{H}}_{Y}^{T}\\right\\vert^{\\alpha}}{\\left\\vert\\left\\vert\\overrightarrow{\\mathbf{H}}_{X}\\right\\vert\\right\\vert_{1+k}^{\\alpha+k\\alpha}}\\sum_{j=1}^{M_{X}}\\left\\vert h_{j}^{X}\\right\\vert^{k\\alpha}+i\\overrightarrow{\\mathbf{t}}^{T}\\overrightarrow{\\mathbf{H}}_{Y}m\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\exp\\left(-\\frac{\\left\\vert\\overrightarrow{\\mathbf{H}}_{Y}^{T}\\right\\vert^{\\alpha}\\left\\vert\\left\\vert\\overrightarrow{\\mathbf{H}}_{X}\\right\\vert\\right\\vert_{1+k}^{k\\alpha}}{\\left\\vert\\left\\vert\\overrightarrow{\\mathbf{H}}_{X}\\right\\vert\\right\\vert_{1+k}^{\\alpha+k\\alpha}}+i\\overrightarrow{\\mathbf{t}}^{T}\\overrightarrow{\\mathbf{H}}_{Y}m\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Case 2: $k$ is infinite, i.e., $k=\\infty$ . Then, we have $h_{j}^{\\prime}=0\\,\\forall\\,j\\neq j^{*}$ , and $h_{j\\ast}^{\\prime}={}^{1}/h_{1}^{X}$ , which implies: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb E\\left[\\left.e^{i\\vec{\\bf t}^{T}\\vec{\\bf X}^{\\prime}}\\right\\rvert M=m\\right]=\\exp\\left(-\\left\\lvert\\vec{\\bf H}_{Y}^{T}\\vec{\\bf t}\\right\\rvert^{\\alpha}\\Bigg\\rvert\\frac{1}{h_{j^{*}}^{X}}\\right\\rvert^{\\alpha}+i\\vec{\\bf t}^{T}\\vec{\\bf H}_{Y}m\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\exp\\left(-\\frac{\\left\\lvert\\vec{\\bf H}_{Y}^{T}\\vec{\\bf t}\\right\\rvert^{\\alpha}}{\\left\\lvert\\left\\lvert\\vec{\\bf H}_{X}\\right\\rvert\\right\\rvert_{\\infty}^{\\alpha}}+i\\vec{\\bf t}^{T}\\vec{\\bf H}_{Y}m\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Combining (122) and (123), we can re-write $\\mathbb{E}\\left[e^{i\\vec{\\mathbf{t}}\\,^{T}\\vec{\\mathbf{X}}^{\\prime}}\\Big|\\,M=m\\right]$ as: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left.e^{i\\vec{\\mathbf{t}}^{T}\\vec{\\mathbf{X}}^{\\prime}}\\right\\rvert M=m\\right]=\\exp\\left(-f(k)\\left\\lvert\\vec{\\mathbf{H}}_{Y}^{T}\\vec{\\mathbf{t}}\\right\\rvert^{\\alpha}+i\\vec{\\mathbf{t}}^{T}\\vec{\\mathbf{H}}_{Y}m\\right),\\mathrm{~where~}}\\\\ &{f(k)=\\left\\{\\begin{array}{r l}{\\lVert\\vec{\\mathbf{H}}_{X}\\rVert_{k\\alpha}^{k\\alpha}/\\lVert\\vec{\\mathbf{H}}_{X}\\rVert_{1+k}^{\\alpha+\\alpha k}}&{\\ k\\mathrm{~flnite}}\\\\ {1/\\lVert\\vec{\\mathbf{H}}_{X}\\rVert_{\\infty}^{\\alpha}}&{k\\mathrm{~infinite}}\\end{array}\\right..}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Specifying $Q_{M C}(\\vec{\\bf Y}|\\vec{\\bf X}^{\\prime})$ : We specify $Q_{M C}(\\vec{\\bf Y}|\\vec{\\bf X}^{\\prime})$ through the following stochastic transformation: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\vec{\\mathbf{Y}}=\\vec{\\mathbf{X}}^{\\prime}+\\vec{\\pmb{\\epsilon}},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $\\vec{\\epsilon}$ follows a multivariate stable distribution. Furthermore, we assume that $\\vec{\\epsilon}$ is jointly independent from $(\\vec{\\bf X},\\vec{\\bf X}^{\\prime},M)$ , i.e., $\\vec{\\epsilon}$ \u22a5\u22a5 $(\\vec{\\bf X},\\vec{\\bf X}^{\\prime},M)$ . The characteristic function of $\\vec{\\epsilon}$ is defined as follows: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[e^{i\\vec{\\bf t}^{T}\\vec{\\epsilon}}\\right]=\\exp\\left(-\\sum_{j=1}^{d_{Y}}|t_{j}|^{\\alpha}+f(k)\\left|\\vec{\\bf H}_{Y}^{T}\\vec{\\bf t}\\right|^{\\alpha}\\right).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "In order for $Q_{M C}(\\vec{\\bf Y}|\\vec{\\bf X}^{\\prime})$ to be a valid distribution, we need to ensure that the distribution of $\\vec{\\epsilon}$ defined in (126) is a legitimate multivariate stable distribution. We will utilize property 1 for showing that $\\vec{\\epsilon}$ follows a stable multivariate stable distribution. Property 1 states a random vector is distributed according to an independent component multivariate stable distribution iff every 1-dimensional projection of this random vector follows a univariate stable continuous distribution. Hence, to show that $\\vec{\\epsilon}$ is distributed according to a legitimate multivariate stable distribution, we will show that every 1-dimensional projection of $\\vec{\\epsilon}$ follows a univariate stable continuous distribution. ", "page_idx": 38}, {"type": "text", "text": "Let $\\vec{\\bf t}\\in\\mathbb{R}^{d_{Y}}$ , then the characteristic function of the 1-dimensional projection of $\\vec{\\epsilon}$ along $\\vec{\\bf t}$ , i.e., $\\vec{\\bf t}^{T}\\vec{\\pmb{\\epsilon}}$ can be trivially deduced using (126). ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{E}\\left[e^{i\\vec{\\bf t}^{T}\\vec{\\epsilon}}\\right]=\\exp\\left(-\\displaystyle\\sum_{j=1}^{d_{Y}}|t_{j}|^{\\alpha}+f(k)\\left|\\vec{\\bf H}_{Y}^{T}\\vec{\\bf t}\\right|^{\\alpha}\\right)}\\\\ &{}&{=\\exp\\left(-\\left\\|\\vec{\\bf t}\\right\\|_{\\alpha}^{\\alpha}+f(k)\\left|\\vec{\\bf H}_{Y}^{T}\\vec{\\bf t}\\right|^{\\alpha}\\right).\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Comparing the characteristic function of $\\vec{\\bf t}^{T}\\vec{\\pmb{\\epsilon}}$ described in (127) with the standard characteristic function of a univariate stable characteristic function defined in (323), we can conclude $\\vec{\\bf t}^{T}\\vec{\\pmb{\\epsilon}}\\sim$ $p_{C S}(\\alpha,\\beta(\\vec{\\mathbf{t}}),\\gamma(\\vec{\\mathbf{t}}),\\mu(\\vec{\\mathbf{t}}))$ , where: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\gamma(\\vec{\\mathbf{t}})=\\left\\lVert\\vec{\\mathbf{t}}\\right\\rVert_{\\alpha}^{\\alpha}-f(k)\\left\\lvert\\vec{\\mathbf{H}}_{Y}^{T}\\vec{\\mathbf{t}}\\right\\rvert^{\\alpha},\\mu(\\vec{\\mathbf{t}})=0,\\mathrm{~and~}\\beta(\\vec{\\mathbf{t}})=0.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "In order for $\\vec{\\bf t}^{T}\\vec{\\pmb{\\epsilon}}$ to have a legitimate univariate stable distribution, we just need to show that $\\gamma(\\vec{\\mathbf{t}})\\,\\geq\\,0\\,\\,\\forall\\,\\,\\vec{\\mathbf{t}}\\,\\in\\,\\mathbb{R}^{d_{Y}}$ . Note that $\\gamma({\\vec{\\mathbf{t}}})\\,=\\,0$ would correspond to the case where all the mass of the distribution is centered at 0, which does satisfy definition 2, and is an example of a degenerate univariate stable distribution. Let us analyze the function $\\gamma(\\vec{\\bf t})$ : ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\gamma(\\vec{\\bf t})=\\left\\lVert\\vec{\\bf t}\\right\\rVert_{\\alpha}^{\\alpha}-f(k)\\left|\\vec{\\bf H}_{Y}^{T}\\vec{\\bf t}\\right|^{\\alpha},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Applying lemma 8 on the term $\\left|\\vec{\\mathbf{H}}_{Y}^{T}\\vec{\\mathbf{t}}\\right|$ , we obtain: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\vec{\\bf H}_{Y}^{T}\\vec{\\bf t}\\right|\\leq\\left\\{\\begin{array}{l l}{\\|\\vec{\\bf t}\\|_{\\alpha}^{\\alpha}\\|\\vec{\\bf H}_{Y}\\|_{\\alpha/\\alpha-1}^{\\alpha}}&{\\alpha\\in(1,2]}\\\\ {\\|\\vec{\\bf t}\\|_{1}^{\\alpha}\\|\\vec{\\bf H}_{Y}\\|_{\\infty}^{\\alpha}}&{\\alpha\\in(0,1]}\\end{array}\\right..}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Applying (130) in (129), we obtain: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\gamma(\\vec{\\mathbf{t}})\\geq\\left\\{\\begin{array}{l l}{\\lVert\\vec{\\mathbf{t}}\\rVert_{\\alpha}^{\\alpha}-f(k)\\lVert\\vec{\\mathbf{t}}\\rVert_{\\alpha}^{\\alpha}\\lVert\\vec{\\mathbf{H}}_{Y}\\rVert_{\\alpha/\\alpha-1}^{\\alpha}}&{\\alpha\\in(1,2]}\\\\ {\\lVert\\vec{\\mathbf{t}}\\rVert_{\\alpha}^{\\alpha}-f(k)\\lVert\\vec{\\mathbf{t}}\\rVert_{1}^{\\alpha}\\lVert\\vec{\\mathbf{H}}_{Y}\\rVert_{\\infty}^{\\alpha}}&{\\alpha\\in(0,1]}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Denote $l(\\vec{\\bf t},\\alpha)$ as follows: ", "page_idx": 39}, {"type": "equation", "text": "$$\nl(\\vec{\\mathfrak{t}},\\alpha)=\\left\\{\\begin{array}{l l}{\\|\\vec{\\mathfrak{t}}\\|_{\\alpha}^{\\alpha}-f(k)\\|\\vec{\\mathfrak{t}}\\|_{\\alpha}^{\\alpha}\\|\\vec{\\pmb{\\Pi}}_{Y}\\|_{\\alpha/\\alpha-1}^{\\alpha}}&{\\alpha\\in(1,2]}\\\\ {\\|\\vec{\\mathfrak{t}}\\|_{\\alpha}^{\\alpha}-f(k)\\|\\vec{\\mathfrak{t}}\\|_{1}^{\\alpha}\\|\\vec{\\pmb{\\Pi}}_{Y}\\|_{\\infty}^{\\alpha}}&{\\alpha\\in(0,1]}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "In order to show $\\gamma(\\vec{\\mathbf{t}})\\geq0\\,\\forall\\,\\vec{\\mathbf{t}}\\in\\mathbb{R}^{d_{Y}}$ , it suffices to show that $l(\\vec{\\bf t},\\alpha)\\geq0\\,\\forall\\,\\vec{\\bf t}\\in\\mathbb R^{d_{Y}}$ and $\\alpha\\in(0,2]$ .   \nCase 1 $:\\alpha\\in(1,2]$ . ", "page_idx": 39}, {"type": "text", "text": "For $\\alpha\\in(1,2]$ , we have: ", "page_idx": 39}, {"type": "equation", "text": "$$\nl(\\vec{\\bf t},\\alpha)=\\left\\lVert\\vec{\\bf t}\\right\\rVert_{\\alpha}^{\\alpha}-f(k)\\left\\lVert\\vec{\\bf t}\\right\\rVert_{\\alpha}^{\\alpha}\\left\\lVert\\vec{\\bf H}_{Y}\\right\\rVert_{\\alpha/\\alpha-1}^{\\alpha}=\\left\\lVert\\vec{\\bf t}\\right\\rVert_{\\alpha}^{\\alpha}\\left(1-f(k)\\left\\lVert\\vec{\\bf H}_{Y}\\right\\rVert_{\\alpha/\\alpha-1}^{\\alpha}\\right).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Since, the above equation holds for any $k\\in[0,\\infty]$ , we choose $k=1/(\\alpha\\!-\\!1)$ . Substituting the form $f(k)$ from (124) in the above equation, we obtain: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle l(\\vec{\\mathbf{t}},\\alpha)=\\left\\lVert\\vec{\\mathbf{t}}\\right\\rVert_{\\alpha}^{\\alpha}\\left(1-\\frac{\\left\\lVert\\vec{\\mathbf{H}}_{Y}\\right\\rVert_{\\alpha/\\alpha-1}^{\\alpha}\\left\\lVert\\vec{\\mathbf{H}}_{X}\\right\\rVert_{\\alpha/\\alpha-1}^{\\alpha/\\alpha-1}}{\\left\\lVert\\vec{\\mathbf{H}}_{X}\\right\\rVert_{1}^{\\alpha}\\left\\lVert\\mathbf{1}_{+\\frac{1}{\\alpha-1}}^{\\alpha(1+\\frac{1}{\\alpha})}}\\right)=\\left\\lVert\\vec{\\mathbf{t}}\\right\\rVert_{\\alpha}^{\\alpha}\\left(1-\\frac{\\left\\lVert\\vec{\\mathbf{H}}_{Y}\\right\\rVert_{\\alpha/\\alpha-1}^{\\alpha}\\left\\lVert\\vec{\\mathbf{H}}_{X}\\right\\rVert_{\\alpha/\\alpha-1}^{\\alpha/\\alpha-1}}{\\left\\lVert\\vec{\\mathbf{H}}_{X}\\right\\rVert_{\\alpha/\\alpha-1}^{\\alpha(\\frac{\\alpha}{\\alpha-1})}}\\right)}\\\\ {\\displaystyle=\\left\\lVert\\vec{\\mathbf{t}}\\right\\rVert_{\\alpha}^{\\alpha}\\left(1-\\frac{\\left\\lVert\\vec{\\mathbf{H}}_{Y}\\right\\rVert_{\\alpha/\\alpha-1}^{\\alpha}}{\\left\\lVert\\vec{\\mathbf{H}}_{X}\\right\\rVert_{\\alpha/\\alpha-1}^{\\alpha(\\frac{\\alpha}{\\alpha-1})-\\frac{\\alpha}{\\alpha-1}}}\\right)=\\left\\lVert\\vec{\\mathbf{t}}\\right\\rVert_{\\alpha}^{\\alpha}\\left(1-\\frac{\\left\\lVert\\vec{\\mathbf{H}}_{Y}\\right\\rVert_{\\alpha/\\alpha-1}^{\\alpha}}{\\left\\lVert\\vec{\\mathbf{H}}_{X}\\right\\rVert_{\\alpha/\\alpha-1}^{\\alpha}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Note that $\\left\\|{\\vec{\\mathbf{t}}}\\right\\|_{\\alpha}^{\\alpha}\\geq0$ , and by condition 1 we know that $\\|\\vec{\\mathbf{H}}_{Y}\\|_{\\alpha/\\alpha-1}\\leq\\|\\vec{\\mathbf{H}}_{X}\\|_{\\alpha/\\alpha-1}\\Rightarrow\\|\\vec{\\mathbf{H}}_{Y}\\|_{\\alpha/\\alpha-1}^{\\alpha}\\leq$ $\\|\\vec{\\mathbf{H}}_{X}\\|_{\\alpha_{\\!/\\alpha-1}}^{\\alpha}$ . Hence, using these two previous facts and the above equation, we can conclude: ", "page_idx": 39}, {"type": "equation", "text": "$$\nl(\\vec{\\bf t},\\alpha)=\\underbrace{\\left\\|\\vec{\\bf t}\\right\\|_{\\alpha}^{\\alpha}}_{\\geq0}\\underbrace{\\left(1-\\frac{\\left\\|\\vec{\\bf H}_{Y}\\right\\|_{\\alpha}^{\\alpha}}{\\left\\|\\vec{\\bf H}_{X}\\right\\|_{\\alpha/\\alpha-1}^{\\alpha}}\\right)}_{\\geq0}\\Rightarrow l(\\vec{\\bf t},\\alpha)\\geq0.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Case 2: $\\alpha\\in(0,1]$ . ", "page_idx": 39}, {"type": "equation", "text": "$$\nl(\\vec{\\mathbf{t}},\\alpha)=\\left\\lVert\\vec{\\mathbf{t}}\\right\\rVert_{\\alpha}^{\\alpha}-f(k)\\left\\lVert\\vec{\\mathbf{t}}\\right\\rVert_{1}^{\\alpha}\\left\\lVert\\vec{\\mathbf{H}}_{Y}\\right\\rVert_{\\infty}^{\\alpha}\\stackrel{(a)}{\\geq}\\left\\lVert\\vec{\\mathbf{t}}\\right\\rVert_{\\alpha}^{\\alpha}-f(k)\\left\\lVert\\vec{\\mathbf{t}}\\right\\rVert_{\\alpha}^{\\alpha}\\left\\lVert\\vec{\\mathbf{H}}_{Y}\\right\\rVert_{\\infty}^{\\alpha},\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where $(a)$ is derived using the monotonicity of $L_{p}$ norms in $p$ , see [57]. More precisely, we use the inequality $\\|\\vec{\\mathbf{t}}\\|_{\\alpha}\\geq\\|\\vec{\\mathbf{t}}\\|_{1}\\,\\forall\\,\\alpha\\in(0,1]$ . Simplifying the above equation, we have: ", "page_idx": 39}, {"type": "equation", "text": "$$\nl(\\vec{\\bf t},\\alpha)\\geq\\left\\lVert\\vec{\\bf t}\\right\\rVert_{\\alpha}^{\\alpha}\\left(1-f(k)\\left\\lVert\\vec{\\bf H}_{Y}\\right\\rVert_{\\infty}^{\\alpha}\\right).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Since, the above equation holds for any $k\\in[0,\\infty]$ , we choose $k=\\infty$ . Substituting the form $f(k)$ from (124) in the above equation, we obtain: ", "page_idx": 40}, {"type": "equation", "text": "$$\nl(\\vec{\\bf t},\\alpha)\\geq\\left\\|\\vec{\\bf t}\\right\\|_{\\alpha}^{\\alpha}\\left(1-\\frac{\\|\\vec{\\bf H}_{Y}\\|_{\\infty}^{\\alpha}}{\\|\\vec{\\bf H}_{X}\\|_{\\infty}^{\\alpha}}\\right).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Note that $\\left\\|{\\vec{\\mathbf{t}}}\\right\\|_{\\alpha}^{\\alpha}\\geq0$ , and by condition 1 we know that $\\|\\vec{\\mathbf{H}}_{Y}\\|_{\\infty}\\leq\\|\\vec{\\mathbf{H}}_{X}\\|_{\\infty}\\Rightarrow\\|\\vec{\\mathbf{H}}_{Y}\\|_{\\infty}^{\\alpha}\\leq\\|\\vec{\\mathbf{H}}_{X}\\|_{\\infty}^{\\alpha}$ . Hence, using these two facts and the above equation, we can conclude: ", "page_idx": 40}, {"type": "equation", "text": "$$\nl(\\vec{\\mathbf{t}},\\alpha)\\geq\\underbrace{\\left\\|\\vec{\\mathbf{t}}\\right\\|_{\\alpha}^{\\alpha}}_{\\geq0}\\underbrace{\\left(1-\\frac{\\left\\|\\vec{\\mathbf{H}}_{Y}\\right\\|_{\\infty}^{\\alpha}}{\\left\\|\\vec{\\mathbf{H}}_{X}\\right\\|_{\\infty}^{\\alpha}}\\right)}_{\\geq0}\\Rightarrow l(\\vec{\\mathbf{t}},\\alpha)\\geq0.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "From (135) and (139), we can conclude that $l(\\vec{\\bf t},\\alpha)\\geq0\\,\\forall\\,\\vec{\\bf t}\\in\\mathbb R^{d_{Y}}$ and $\\alpha\\in(0,2]$ , which implies $\\gamma(\\vec{\\mathbf{t}})\\geq0\\,\\forall\\,\\vec{\\mathbf{t}}\\in\\mathbb{R}^{d_{Y}}$ . Hence, $\\vec{\\epsilon}$ follows a legitimate multivariate stable distribution. ", "page_idx": 40}, {"type": "text", "text": "Finally, we construct the desired $Q_{M C}(M,{\\vec{\\mathbf{X}}},{\\vec{\\mathbf{Y}}})$ from $Q_{M C}(M,\\vec{\\bf X},\\vec{\\bf X}^{\\prime},\\vec{\\bf Y})$ by marginalizing $\\vec{\\bf X}^{\\prime}$ . ", "page_idx": 40}, {"type": "text", "text": "Part 2: Showing $Q_{M C}(M,\\vec{\\bf X},\\vec{\\bf Y})\\in\\Delta_{P}$ ", "page_idx": 40}, {"type": "text", "text": "For showing that $Q_{M C}(M,\\vec{\\bf X},\\vec{\\bf Y})\\in\\Delta_{P}$ , we first need to derive $Q_{M C}(\\vec{\\mathbf{Y}}|M)$ . ", "page_idx": 40}, {"type": "text", "text": "Deriving $Q(\\vec{\\mathbf{Y}}|M)$ : We will derive $Q_{M C}(\\vec{\\mathbf{Y}}|M)$ by using the fact that $\\vec{\\epsilon}\\perp\\vec{\\mathbf{X}}^{\\prime}|M$ , hence their conditional characteristic functions would just result in multiplication. ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb E\\left[e^{i\\vec{\\bf t}^{T}\\vec{\\bf Y}}\\Big|\\,M=m\\right]=\\mathbb E\\left[e^{i\\vec{\\bf t}^{T}(\\vec{\\bf X}^{\\prime}+\\vec{\\epsilon})}\\Big|\\,M=m\\right]=\\mathbb E\\left[e^{i\\vec{\\bf t}^{T}\\vec{\\bf X}^{\\prime}+i\\vec{\\bf t}^{T}\\vec{\\epsilon}}\\Big|\\,M=m\\right]}\\\\ &{\\,=\\mathbb E\\left[e^{i\\vec{\\bf t}^{T}\\vec{\\bf X}^{\\prime}}e^{i\\vec{\\bf t}^{T}\\vec{\\epsilon}}\\Big|\\,M=m\\right]\\overset{(a)}{=}\\mathbb E\\left[e^{i\\vec{\\bf t}^{T}\\vec{\\bf X}^{\\prime}}\\Big|\\,M=m\\right]\\mathbb E\\left[e^{i\\vec{\\bf t}^{T}\\vec{\\epsilon}}\\Big|\\,M=m\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where $(a)$ is due to $\\vec{\\epsilon}\\perp\\vec{\\mathbf{X}}^{\\prime}|M$ . Substituting the characteristic functions of $\\vec{\\bf X}^{\\prime}$ and $\\vec{\\epsilon}$ from (124) and (126), respectively, in (140): ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\end{array}\\left(-\\sum_{j=1}^{d_{Y}}|t_{j}|^{\\alpha}+f(k)\\left|\\vec{\\bf H}_{Y}^{T}\\vec{\\bf t}\\right|^{\\alpha}\\right)\\displaystyle\\exp\\left(-f(k)\\left|\\vec{\\bf H}_{Y}^{T}\\vec{\\bf t}\\right|^{\\alpha}+i\\vec{\\bf t}^{T}\\vec{\\bf H}_{Y}m\\right)}\\\\ {\\displaystyle}\\\\ {\\displaystyle=\\exp\\left(-\\sum_{j=1}^{d_{Y}}|t_{j}|^{\\alpha}+f(k)\\left|\\vec{\\bf H}_{Y}^{T}\\vec{\\bf t}\\right|^{\\alpha}-f(k)\\left|\\vec{\\bf H}_{Y}^{T}\\vec{\\bf t}\\right|^{\\alpha}+i\\vec{\\bf t}^{T}\\vec{\\bf H}_{Y}m\\right)}\\\\ {\\displaystyle}\\\\ {\\displaystyle=\\exp\\left(-\\sum_{j=1}^{d_{Y}}|t_{j}|^{\\alpha}+i\\vec{\\bf t}^{T}\\vec{\\bf H}_{Y}m\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "By comparing the characteristic function shown in (141) with the characteristic function derived using $P(\\vec{\\mathbf{Y}}|M)$ shown in (116), we can conclude: ", "page_idx": 40}, {"type": "equation", "text": "$$\nQ_{M C}(\\vec{\\mathbf{Y}}|M)=P(\\vec{\\mathbf{Y}}|M).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "From (142) and (117), we can conclude: ", "page_idx": 40}, {"type": "equation", "text": "$$\nQ_{M C}(M,\\vec{\\mathbf{Y}})=Q_{M C}(M)Q_{M C}(\\vec{\\mathbf{Y}}|M)=P(M)P(\\vec{\\mathbf{Y}}|M)=P(M,\\vec{\\mathbf{Y}}).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "We know that by construction: ", "page_idx": 40}, {"type": "equation", "text": "$$\nQ_{M C}(M,\\vec{\\bf X})=P(M,\\vec{\\bf X}).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Hence, $Q_{M C}(M,\\vec{\\bf X},\\vec{\\bf Y})\\in\\Delta_{P}$ and consequently, by proposition 1, $U I(M;{\\vec{\\mathbf{Y}}}\\backslash{\\vec{\\mathbf{X}}})=0$ , concluding our proof. ", "page_idx": 40}, {"type": "text", "text": "Lemma 8. Let $\\vec{\\mathbf{a}}=[a_{1}\\quad.\\dots\\quad a_{d}]\\in\\mathbb{R}^{d}$ and $\\vec{\\mathbf{b}}=[b_{1}\\quad.\\dots\\quad b_{d}]\\in\\mathbb{R}^{d},$ , then we have: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\vec{\\bf a}^{T}\\vec{\\bf b}\\right|^{\\alpha}\\leq\\left\\{\\begin{array}{l l}{\\|\\vec{\\bf a}\\|_{\\alpha}^{\\alpha}\\|\\vec{\\bf b}\\|_{\\alpha/(\\alpha-1)}^{\\alpha}}&{\\alpha\\in(1,2]}\\\\ {\\|\\vec{\\bf a}\\|_{1}^{\\alpha}\\|\\vec{\\bf b}\\|_{\\infty}^{\\alpha}}&{\\alpha\\in(0,1]}\\end{array}\\right.,}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $\\|(\\cdot)\\|_{p}$ is the standard $L_{p}$ norm. ", "page_idx": 41}, {"type": "text", "text": "Proof. Case 1: $\\alpha\\in(1,2]$ ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Analysing the LHS of (145) ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\left|\\vec{\\mathbf{a}}^{T}\\vec{\\mathbf{b}}\\right|=\\left|\\sum_{i=1}^{d}a_{i}b_{i}\\right|\\overset{(a)}{\\leq}\\sum_{i=1}^{d}\\left|a_{i}b_{i}\\right|\\overset{(b)}{\\leq}\\left(\\sum_{i=1}^{d}\\left|a_{i}\\right|^{\\alpha}\\right)^{\\frac{1}{\\alpha}}\\left(\\sum_{i=1}^{d}\\left|b_{i}\\right|^{\\frac{\\alpha}{(\\alpha-1)}}\\right)^{\\frac{(\\alpha-1)}{\\alpha}}=\\|\\vec{\\mathbf{a}}\\|_{\\alpha}\\|\\vec{\\mathbf{b}}\\|_{\\alpha/\\alpha-1},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $(a)$ is due to the sub-additivity of $|(\\cdot)|$ operator, and $(b)$ is due to H\u00f6lder\u2019s inequality [57]. Raising both sides of the inequality to $\\alpha$ in (146) gives us the desired inequality. Since, $\\alpha>0$ raising both sides of the inequalities to power $\\alpha$ does not change the direction of inequality. ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\left|\\vec{\\mathbf{a}}^{T}\\vec{\\mathbf{b}}\\right|^{\\alpha}\\leq\\|\\vec{\\mathbf{a}}\\|_{\\alpha}^{\\alpha}\\|\\vec{\\mathbf{b}}\\|_{\\alpha/\\alpha-1}^{\\alpha}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Analysing the LHS of (145) ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\left\\vert\\vec{\\mathbf{a}}^{T}\\vec{\\mathbf{b}}\\right\\vert=\\left\\vert\\sum_{i=1}^{d}a_{i}b_{i}\\right\\vert\\overset{(a)}{\\leq}\\sum_{i=1}^{d}\\left\\vert a_{i}b_{i}\\right\\vert\\overset{(b)}{\\leq}\\left(\\sum_{i=1}^{d}\\left\\vert a_{i}\\right\\vert\\right)\\underset{i\\in\\{1,\\ldots,d\\}}{\\operatorname*{max}}\\left\\vert b_{i}\\right\\vert=\\left\\|\\vec{\\mathbf{a}}\\right\\|_{1}\\left\\|\\vec{\\mathbf{b}}\\right\\|_{\\infty},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $(a)$ is again due to the sub-additivity of $|(\\cdot)|$ operator, and $(b)$ is due to the fact that $|a_{i}b_{i}|\\leq$ $|a_{i}|\\,\\mathrm{max}_{i\\in\\{1,\\ldots,d\\}}\\,|b_{i}|\\,\\forall\\,i\\in\\{1,\\ldots,d\\}$ . Raising both sides of the inequality to $\\alpha$ in (148) gives us the desired inequality. Since, $\\alpha>0$ raising both sides of the inequalities to power $\\alpha$ does not change the direction of inequality. ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\left|\\vec{\\bf a}^{T}\\vec{\\bf b}\\right|^{\\alpha}\\leq\\|\\vec{\\bf a}\\|_{1}^{\\alpha}\\|\\vec{\\bf b}\\|_{\\infty}^{\\alpha}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Combining (147) and (149), we obtain the desired inequality. ", "page_idx": 41}, {"type": "text", "text": "Lemma 9. Let $\\vec{\\bf Z}\\sim p_{C S-I C}(\\alpha,\\vec{\\bf0}_{d},\\vec{\\bf1}_{d},\\vec{\\bf0}_{d})$ be a $d$ -dimensional random vector, $\\mathbf{A}=[a_{i j}]_{i,j=1}^{n,d}\\in$ $\\mathbb{R}^{n\\times d}$ be a $n\\times d$ matrix, and $\\vec{\\mathbf{b}}\\in\\mathbb{R}^{d}$ . Then, the characteristic function of $\\mathbf{A}\\vec{\\mathbf{Z}}+\\vec{\\mathbf{b}}$ is given by (150) ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[e^{i\\vec{\\mathbf{t}}^{T}\\left(\\mathbf{A}\\vec{\\mathbf{Z}}+\\vec{\\mathbf{b}}\\right)}\\right]=\\exp\\left(-\\sum_{j=1}^{d}\\left|\\vec{\\mathbf{a}}_{j}^{T}\\vec{\\mathbf{t}}\\right|^{\\alpha}+i\\vec{\\mathbf{t}}^{T}\\vec{\\mathbf{b}}\\right)\\ \\forall\\,\\vec{\\mathbf{t}}\\in\\mathbb{R}^{d},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $\\vec{\\bf a}_{i}$ is the $i$ -th column of A ", "page_idx": 41}, {"type": "text", "text": "Proof. Analyzing L.H.S of (150) ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[e^{i\\vec{\\bf t}^{T}({\\bf A}\\vec{\\bf Z}+\\vec{\\bf b})}\\right]=\\mathbb{E}\\left[e^{i\\vec{\\bf t}^{T}{\\bf A}\\vec{\\bf Z}+\\vec{\\bf t}^{T}\\vec{\\bf b}}\\right]=\\mathbb{E}\\left[e^{i\\vec{\\bf t}^{T}{\\bf A}\\vec{\\bf Z}}e^{i\\vec{\\bf t}^{T}\\vec{\\bf b}}\\right]\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Using the fact that $e^{i\\vec{\\mathbf{t}}^{\\tau}\\vec{\\mathbf{b}}}$ is constant w.r.t expectation over $\\vec{\\bf Z}$ , and linearity of expectation, we have: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[e^{i\\vec{\\mathbf{t}}^{T}\\left(\\mathbf{A}\\vec{\\mathbf{Z}}+\\vec{\\mathbf{b}}\\right)}\\right]=e^{i\\vec{\\mathbf{t}}^{T}\\vec{\\mathbf{b}}}\\mathbb{E}\\left[\\exp\\left(i\\vec{\\mathbf{t}}^{T}\\mathbf{A}\\vec{\\mathbf{Z}}\\right)\\right],}\\\\ &{\\qquad\\qquad\\qquad=e^{i\\vec{\\mathbf{t}}^{T}\\vec{\\mathbf{b}}}\\mathbb{E}\\left[\\exp\\left(i\\vec{\\mathbf{t}}^{T}\\left[\\vec{\\mathbf{a}}_{1}\\quad\\dots\\quad\\vec{\\mathbf{a}}_{d}\\right]\\vec{\\mathbf{Z}}\\right)\\right],}\\\\ &{\\qquad\\qquad=e^{i\\vec{\\mathbf{t}}^{T}\\vec{\\mathbf{b}}}\\mathbb{E}\\left[\\exp\\left(i\\left[\\vec{\\mathbf{t}}^{T}\\vec{\\mathbf{a}}_{1}\\quad\\dots\\quad\\vec{\\mathbf{t}}^{T}\\vec{\\mathbf{a}}_{d}\\right]\\vec{\\mathbf{Z}}\\right)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Simplifying the term $\\left[\\vec{\\mathbf{t}}^{T}\\vec{\\mathbf{a}}_{1}\\right.\\right.\\ldots\\left.\\vec{\\mathbf{t}}^{T}\\vec{\\mathbf{a}}_{d}\\right]\\vec{\\mathbf{Z}}$ by substituting $\\mathbf{Z}=\\left[Z_{1}\\quad\\ldots\\quad Z_{d}\\right]^{T}$ , we obtain: ", "page_idx": 42}, {"type": "equation", "text": "$$\n=e^{i\\vec{\\bf t}^{_T}\\vec{\\bf b}}\\mathbb{E}\\left[\\exp\\left(i\\sum_{j=1}^{d}(\\vec{\\bf t}^{_T}\\vec{\\bf a}_{j})Z_{j}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Using properties of exponentials, we obtain: ", "page_idx": 42}, {"type": "equation", "text": "$$\n=e^{i\\vec{\\bf t}^{\\ T}\\vec{\\bf b}}\\mathbb{E}\\left[\\prod_{j=1}^{d}\\exp\\left(i\\vec{\\bf t}^{T}\\vec{\\bf a}_{j}Z_{j}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Using the fact $\\{Z_{j}\\}_{j=1}^{d}$ are jointly independent we obtain: ", "page_idx": 42}, {"type": "equation", "text": "$$\n=e^{i\\vec{\\bf t}^{\\ T}\\vec{\\bf b}}\\prod_{j=1}^{d}\\mathbb{E}\\left[\\exp\\left(i\\vec{\\bf t}^{\\ T}\\vec{\\bf a}_{j}Z_{j}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Using the fact that $Z_{j}\\ \\sim\\ p_{C S}(\\alpha,0,1,0)$ and the result of lemma 5, we know that $\\vec{\\bf t}^{T}\\vec{\\bf a}_{j}Z_{j}\\,\\sim$ $p_{C S}(\\alpha,0,|\\vec{\\mathbf{t}}^{T}\\vec{\\mathbf{a}}_{j}|,0)$ . Substituting the corresponding characteristic function of $\\vec{\\bf t}^{T}\\vec{\\bf a}_{j}Z_{j}$ using (323) ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle=\\,e^{i\\vec{\\bf t}\\,^{T}\\vec{\\bf b}}\\prod_{j=1}^{d}\\exp\\left(-\\left|\\vec{\\bf t}\\,^{T}\\vec{\\bf a}_{i}\\right|^{\\alpha}\\right)=e^{i\\vec{\\bf t}\\,^{T}\\vec{\\bf b}}\\exp\\left(-\\displaystyle\\sum_{j=1}^{d}\\left|\\vec{\\bf t}\\,^{T}\\vec{\\bf a}_{i}\\right|^{\\alpha}\\right)}\\ ~}\\\\ {{\\displaystyle=\\exp\\left(-\\sum_{j=1}^{d}\\left|\\vec{\\bf t}\\,^{T}\\vec{\\bf a}_{i}\\right|^{\\alpha}+i\\vec{\\bf t}\\,^{T}\\vec{\\bf b}\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Lemma 10. Let $\\vec{\\mathbf{H}}_{1}=\\left[h_{1}^{(1)}\\quad.\\ldots\\quad h_{d_{1}}^{(1)}\\right]^{T}\\in\\mathbb{R}^{d_{1}}$ and $\\vec{\\bf H}_{2}=\\left[h_{1}^{(2)}\\quad.\\dots\\quad h_{d_{2}}^{(2)}\\right]^{T}\\in\\mathbb{R}^{d_{2}}$ . Define $\\vec{\\bf H}_{1}^{k}$ as follows: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\vec{\\mathbf{H}}_{1}^{k}=\\frac{1}{\\|\\vec{\\mathbf{H}}_{1}\\|_{1}^{1+k}}\\left[s g n(h_{1}^{(1)})|h_{1}^{(1)}|^{k}\\quad.\\,.\\,.\\quad s g n(h_{d_{1}}^{(1)})|h_{d_{1}}^{(1)}|^{k}\\right]^{T}\\,\\forall\\,k\\in[0,\\infty).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Furthermore, define $\\vec{\\bf{H}}_{1}^{\\infty}$ as follows: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{j^{*}=\\underset{j\\in\\{1,\\dots,d_{1}\\}}{\\arg\\operatorname*{max}}\\left|h_{j}^{(1)}\\right|,}}\\\\ {{\\vec{\\mathbf{H}}_{1}^{\\infty}=\\left[h_{1}^{\\infty}\\quad\\dots\\quad h_{d_{1}}^{\\infty}\\right]^{T},\\ w i t h\\;h_{j}^{\\infty}=0\\,\\forall\\,j\\in\\{1,\\dots,d_{1}\\}\\backslash\\{j^{*}\\},\\;a n d\\,h_{j}^{\\infty}=\\frac{1}{h_{i^{*}}^{(1)}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Then, we have: ", "page_idx": 42}, {"type": "text", "text": "where $\\|(\\cdot)\\|_{1+k}$ is the standard $L_{1+k}$ norm [57]. We can alternatively state (154) in a more compact notation: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\vec{\\bf H}_{2}(\\vec{\\bf H}_{1}^{k})^{T}\\vec{\\bf H}_{1}=\\vec{\\bf H}_{2}\\,\\forall\\,k\\in[0,\\infty],\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where the case of $k=\\infty$ is to be understood as $\\vec{\\bf{H}}_{1}^{k}=\\vec{\\bf{H}}_{1}^{\\infty}$ . ", "page_idx": 42}, {"type": "text", "text": "Proof. Case 1: $k\\in[0,\\infty)$ , i.e., $k$ is finite, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\vec{\\bf H}_{2}(\\vec{\\bf H}_{1}^{k})^{T}\\vec{\\bf H}_{1}=\\frac{1}{\\|\\vec{\\bf H}_{1}\\|_{1+k}^{1+k}}\\vec{\\bf H}_{2}\\left[\\mathrm{sgn}(h_{1}^{(1)})|h_{1}^{(1)}|^{k}\\quad\\dots\\quad\\mathrm{sgn}(h_{d_{1}}^{(1)})|h_{d_{1}}^{(1)}|^{k}\\right]\\left[\\begin{array}{l}{{h_{1}^{(1)-1}}}\\\\ {{\\vdots}}\\\\ {{h_{d_{1}}^{(1)}}}\\end{array}\\right]}}\\\\ {{\\displaystyle=\\frac{1}{\\|\\vec{\\bf H}_{1}\\|_{1+k}^{1+k}}\\left(\\sum_{j=1}^{d}|h_{j}|^{1+k}\\right)\\vec{\\bf H}_{2}=\\vec{\\bf H}_{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Case 2: $k=\\infty$ , ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\vec{\\mathbf{H}}_{2}(\\vec{\\mathbf{H}}_{1}^{\\infty})^{T}\\vec{\\mathbf{H}}_{1}=\\vec{\\mathbf{H}}_{2}\\left(\\sum_{j=1}^{d_{1}}h_{j}^{\\infty}h_{j}^{(1)}\\right)\\stackrel{(a)}{=}\\vec{\\mathbf{H}}_{2}\\left(h_{j*}^{\\infty}h_{j*}^{(1)}\\right)\\stackrel{(b)}{=}\\vec{\\mathbf{H}}_{2}\\frac{1}{h_{j*}^{(1)}}h_{j*}^{(1)}=\\vec{\\mathbf{H}}_{2},\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where $(a)$ is due to the fact that all elements of $\\vec{\\bf{H}}_{1}^{\\infty}$ are 0 except the $j^{*}$ element, and $(b)$ is due to the fact that $\\begin{array}{r}{h_{j\\ast}^{\\infty}={^{1}\\!/}h_{j\\ast}^{(1)}}\\end{array}$ . \u53e3 ", "page_idx": 43}, {"type": "text", "text": "H Proof of Theorem 3 ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "In this section, we provide the proof of theorem 3. Since theorem 3 provides the analytical PID terms for the system 2 of the multivariate affine continuous stable system defined in Sec. 4, we briefly restate certain key properties of the corresponding elliptically-contoured multivariate stable distributions and the corresponding multivariate affine continuous stable system for convenience. ", "page_idx": 43}, {"type": "text", "text": "H.1 Elliptically-contoured multivariate stable distribution ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Elliptically-contoured multivariate stable distributions are another example of a multivariate generalization of univariate continuous stable distributions. As the name suggests, the defining features of these distributions is that the corresponding p.d.f. has elliptical contours, similar to the multivariate Gaussian distribution. We refer the reader to Appx. N.3 for more details on multivariate continuous stable distributions. We now list certain key properties of elliptically-contoured multivariate stable distributions that we make use of in the proof of theorem 3: ", "page_idx": 43}, {"type": "text", "text": "1. We denote the p.d.f. of the elliptically-contoured continuous stable distribution as $p_{C S-E C}(\\alpha,\\Sigma,\\bar{\\mu})$ , where $\\Sigma$ is a positive definite matrix, $\\vec{\\mu}\\,\\in\\,\\mathbb R^{d}$ , and $\\alpha\\,\\in\\,(0,2]$ . In general, the p.d.f. of Elliptically-contoured multivariate stable distribution do not have a closed-form analytical expression, and are expressed through their characteristic function.   \n2. The characteristic function of a $d$ -dimensional random vector $\\vec{\\bf X}$ having ellipticallycontoured multivariate distribution $p_{C S-E C}(\\alpha,\\Sigma,\\vec{\\mu})$ is expressed as follows: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[e^{i\\vec{\\bf t}^{T}\\vec{\\bf x}}\\right]=\\exp\\left(-\\left(\\vec{\\bf t}^{T}\\Sigma\\vec{\\bf t}\\right)^{\\alpha/2}+i\\vec{\\bf t}^{T}\\vec{\\pmb{\\mu}}\\right)~\\forall\\,\\vec{\\bf t}\\in\\mathbb{R}^{d}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "H.2 Definition of the system 2 of the multivariate affine continuous stable system ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Let the random variable $M\\sim P(M)$ with support ${\\mathcal{M}}\\subseteq\\mathbb{R}$ . $\\vec{\\bf X}$ and $\\vec{\\bf Y}$ are $d_{X}$ -dimensional and $d_{Y}$ -dimensional random vectors, respectively. The joint distribution $P(M,{\\vec{\\mathbf{X}}},{\\vec{\\mathbf{Y}}})$ describes the second system of the multivariate affine continuous stable distribution if it satisfies the following two properties: ", "page_idx": 43}, {"type": "text", "text": "1. $M\\sim P(M)$ having some support set ${\\mathcal{M}}\\subseteq\\mathbb{R}$ . ", "page_idx": 43}, {"type": "text", "text": "2. The conditional distributions of $\\vec{\\bf X}$ and $\\vec{\\textbf{Y}}$ are expressed as follows: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P(\\vec{\\bf X}|M)=p_{C S-E C}(\\alpha,\\Sigma_{X},\\vec{\\bf H}_{X}M+\\vec{\\bf b}_{X}),}\\\\ &{P(\\vec{\\bf Y}|M)=p_{C S-E C}(\\alpha,\\Sigma_{Y},\\vec{\\bf H}_{Y}M+\\vec{\\bf b}_{Y}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where $\\Sigma_{X}$ and $\\Sigma_{Y}$ are positive definite matrices, $\\vec{\\bf H}_{X},\\vec{\\bf b}_{X}\\in\\mathbb{R}^{d_{X}}$ , and $\\vec{\\bf H}_{Y},\\vec{\\bf b}_{Y}\\in\\mathbb{R}^{d_{Y}}$ . ", "page_idx": 43}, {"type": "text", "text": "H.3 Formal proof of theorem 3 ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Theorem 3. Let the joint distribution $P(M,{\\vec{\\mathbf{X}}},{\\vec{\\mathbf{Y}}})$ of $M,{\\vec{\\mathbf{X}}}$ , and $\\vec{\\bf Y}$ describe the system 2 of the multivariate affine continuous stable system. Define \u03a3\u2212X1/2 and $\\Sigma_{Y}^{-1/2}$ as the respective inverses of the matrices \u03a31X/2 and $\\Sigma_{Y}^{^{1/2}}$ which satisfy: $(\\pmb{\\Sigma}_{X}^{1/2})^{T}\\pmb{\\Sigma}_{X}^{1/2}=\\pmb{\\Sigma}_{X}$ , and $\\begin{array}{r}{(\\pmb{\\Sigma}_{Y}^{1/2})^{T}\\pmb{\\Sigma}_{Y}^{1/2}=\\pmb{\\Sigma}_{Y}}\\end{array}$ . Without the loss of generality, assume $\\lVert\\boldsymbol{\\Sigma}_{Y}^{-1/2}\\vec{\\mathbf{H}}_{Y}\\rVert_{2}\\leq\\lVert\\boldsymbol{\\Sigma}_{X}^{-1/2}\\vec{\\mathbf{H}}_{X}\\rVert_{2}$ . Then, $\\Delta_{P}$ contains a Markov chain of the form $M\\rightarrow\\vec{\\mathbf{X}}\\rightarrow\\vec{\\mathbf{Y}}$ and $U I(M;{\\vec{\\mathbf{Y}}}\\backslash{\\vec{\\mathbf{X}}})=0$ . ", "page_idx": 43}, {"type": "text", "text": "1. We can always assume $\\lVert\\boldsymbol{\\Sigma}_{Y}^{-1/2}\\vec{\\mathbf{H}}_{Y}\\rVert_{2}\\leq\\lVert\\boldsymbol{\\Sigma}_{X}^{-1/2}\\vec{\\mathbf{H}}_{X}\\rVert_{2}$ without the loss generality because if $\\|\\Sigma_{Y}^{-1/2}\\vec{\\mathbf{H}}_{Y}\\|_{2}\\geq\\|\\Sigma_{X}^{-1/2}\\vec{\\mathbf{H}}_{X}\\|_{2}$ , then we can always switch our nomenclature to refer to $\\vec{\\bf Y}$ as $\\vec{\\bf X}$ , and $\\vec{\\bf X}$ as $\\vec{\\bf Y}$ . ", "page_idx": 44}, {"type": "text", "text": "2. Properties of positive definite matrices guarantee the existence of an invertible $\\Sigma_{X}^{^{1/2}}$ and $\\Sigma_{Y}^{^{1/2}}$ [58]. ", "page_idx": 44}, {"type": "text", "text": "The proof of this theorem is a generalization of the proof used for the deriving a similar result for the special case of the multivariate Gaussian distribution discussed in [22], which borrows known results from Gaussian interference channels discussed in [48]. This proof follows the same structure as our earlier proofs. ", "page_idx": 44}, {"type": "text", "text": "1. In the first part, we explicitly construct a joint distribution $Q_{M C}(M,{\\vec{\\mathbf{X}}},{\\vec{\\mathbf{Y}}})$ having the Markovian structure $M\\rightarrow\\vec{\\mathbf{X}}\\rightarrow\\vec{\\mathbf{Y}}$ . ", "page_idx": 44}, {"type": "text", "text": "2. In the second part, we show that the $Q_{M C}(M,{\\vec{\\mathbf{X}}},{\\vec{\\mathbf{Y}}})$ constructed in the first part, lies in $\\Delta_{P}$ . Therefore, we can then apply the result of proposition 1 to conclude $U I(M;{\\vec{\\mathbf{Y}}}\\backslash{\\vec{\\mathbf{X}}})=0$ . ", "page_idx": 44}, {"type": "text", "text": "Part 1: Specifying $Q_{M C}(M,{\\vec{\\mathbf{X}}},{\\vec{\\mathbf{Y}}})$ ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "We explicitly construct the desired Markov chain $M\\rightarrow\\vec{\\mathbf{X}}\\rightarrow\\vec{\\mathbf{Y}}$ by constructing a larger Markov chain $M\\rightarrow\\overleftrightarrow{\\mathbf{X}}\\rightarrow\\vec{\\mathbf{X}}^{\\prime}\\rightarrow\\vec{\\mathbf{X}}^{\\prime\\prime}\\rightarrow\\vec{\\mathbf{Y}}^{\\prime}\\rightarrow\\vec{\\mathbf{Y}}^{\\prime}$ , and then marginalizing the larger Markov chain to obtain the desired Markov chain $M\\rightarrow\\vec{\\mathbf{X}}\\rightarrow\\vec{\\mathbf{Y}}$ . ", "page_idx": 44}, {"type": "text", "text": "Denote the joint distribution of the Markov chain $M\\ \\to\\ \\vec{\\mathbf{X}}\\ \\to\\ \\vec{\\mathbf{X}}^{\\prime}\\ \\to\\ \\vec{\\mathbf{X}}^{\\prime\\prime}\\ \\to\\ \\vec{\\mathbf{Y}}^{\\prime}\\ \\to\\ \\vec{\\mathbf{Y}}^{\\prime}\\ \\to$ $\\vec{\\bf Y}$ as $Q_{M C}(M,\\vec{\\mathbf{X}},\\vec{\\mathbf{X}}^{\\prime},\\vec{\\mathbf{X}}^{\\prime\\prime},\\vec{\\mathbf{Y}}^{\\prime},\\vec{\\mathbf{Y}})$ . We can decompose $Q_{M C}(M,\\vec{\\mathbf{X}},\\vec{\\mathbf{X}}^{\\prime},\\vec{\\mathbf{X}}^{\\prime\\prime},\\vec{\\mathbf{Y}}^{\\prime},\\vec{\\mathbf{Y}})$ by utilizing its Markovian structure as follows: ${\\cal Q}_{M C}(M,\\vec{\\bf X},\\vec{\\bf X}^{\\prime},\\vec{\\bf X}^{\\prime\\prime},\\vec{\\bf Y}^{\\prime},\\vec{\\bf Y})\\,=\\,{\\cal Q}_{M C}(M){\\cal Q}_{M C}(\\vec{\\bf X}|M)$ $Q_{M C}(\\vec{\\mathbf{X}}^{\\prime}|\\vec{\\mathbf{X}})\\,Q_{M C}(\\vec{\\mathbf{X}}^{\\prime\\prime}|\\vec{\\mathbf{X}}^{\\prime})Q_{M C}(\\vec{\\mathbf{Y}}^{\\prime}|\\vec{\\mathbf{X}}^{\\prime\\prime})Q_{M C}(\\vec{\\mathbf{Y}}|\\vec{\\mathbf{Y}}^{\\prime})$ . Consequently, we can specify/construct the distribution $Q_{M C}(M,\\vec{\\mathbf{X}},\\vec{\\mathbf{X}}^{\\prime},\\vec{\\mathbf{X}}^{\\prime\\prime},\\vec{\\mathbf{Y}}^{\\prime},\\vec{\\mathbf{Y}})$ by individually specifying $Q_{M C}(M)$ , $Q_{M C}(\\vec{\\bf X}|M)$ , $Q_{M C}(\\vec{\\bf X}^{\\prime}|\\vec{\\bf X}),Q_{M C}(\\vec{\\bf X}^{\\prime\\prime}|\\vec{\\bf X}^{\\prime}),Q_{M C}(\\vec{\\bf Y}^{\\prime}|\\vec{\\bf X}^{\\prime\\prime}).$ , and $Q_{M C}(\\vec{\\bf Y}|\\vec{\\bf Y}^{\\prime})$ . ", "page_idx": 44}, {"type": "text", "text": "Specifying $Q_{M C}(M)$ and $Q_{M C}(\\vec{\\bf X}|M)$ : We choose $Q_{M C}(M)$ and $Q_{M C}(\\vec{\\bf X}|M)$ as follows: ", "page_idx": 44}, {"type": "equation", "text": "$$\nQ_{M C}(M)=P(M)\\;\\mathrm{and}\\;Q_{M C}(\\vec{\\mathbf{X}}|M)=P(\\vec{\\mathbf{X}}|M).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Specifying $Q_{M C}(\\vec{\\bf X}^{\\prime}|\\vec{\\bf X})$ : $Q_{M C}(\\vec{\\bf X}^{\\prime}|\\vec{\\bf X})$ is specified through the following deterministic transformation: ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\vec{\\bf X}^{\\prime}=(\\Sigma_{X}^{-1/2})^{T}(\\vec{\\bf X}-\\vec{\\bf b}_{X}).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "We will also derive the distribution $Q(\\vec{\\mathbf{X}}^{\\prime}|M)$ before proceeding with our construction, as we will need it later in the proof to show that the constructed $Q_{M C}(M,\\vec{\\bf X},\\vec{\\bf Y})\\in\\Delta_{P}$ . ", "page_idx": 44}, {"type": "text", "text": "Deriving $Q_{M C}(\\vec{\\bf X}^{\\prime}|M)$ : We use lemma 11 to derive $Q_{M C}(\\vec{\\bf X}^{\\prime}|M=m)$ , as $Q_{M C}(\\vec{\\bf X}|M=m)$ is an elliptically contoured multivariate stable distribution for a fixed $m$ , and (159) defines a scaling and ", "page_idx": 44}, {"type": "text", "text": "translation operation. ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{Q_{M C}(\\vec{\\bf X}^{\\prime}|M=m)=p_{C S-E C}\\left(\\alpha,({\\bf{\\Sigma}}_{X}^{-1/2})^{T}{\\bf{\\Sigma}}_{X}{\\bf{\\Sigma}}_{X}^{-1/2},({\\bf{\\Sigma}}_{X}^{-1/2})^{T}\\vec{\\bf H}_{X}m+({\\bf{\\Sigma}}_{X}^{-1/2})^{T}(\\vec{\\bf b}_{X}-\\vec{\\bf b}_{X})\\right)}\\\\ &{\\Rightarrow Q_{M C}(\\vec{\\bf X}^{\\prime}|M=m)=p_{C S-E C}\\left(\\alpha,({\\bf{\\Sigma}}_{X}^{-1/2})^{T}{\\bf{\\Sigma}}_{X}{\\bf{\\Sigma}}_{X}^{-1/2},({\\bf{\\Sigma}}_{X}^{-1/2})^{T}\\vec{\\bf H}_{X}m\\right),}\\\\ &{\\Rightarrow Q_{M C}(\\vec{\\bf X}^{\\prime}|M=m)=p_{C S-E C}\\left(\\alpha,({\\bf{\\Sigma}}_{X}^{-1/2})^{T}({\\bf{\\Sigma}}_{X}^{1/2})^{T}{\\bf{\\Sigma}}_{X}^{1/2}{\\bf{\\Sigma}}_{X}^{-1/2},({\\bf{\\Sigma}}_{X}^{-1/2})^{T}\\vec{\\bf H}_{X}m\\right),}\\\\ &{\\Rightarrow Q_{M C}(\\vec{\\bf X}^{\\prime}|M=m)=p_{C S-E C}\\left(\\alpha,({\\bf{\\Sigma}}_{X}^{1/2}{\\bf{\\Sigma}}_{X}^{-1/2})^{T}{\\bf{\\Sigma}}_{X}^{1/2}{\\bf{\\Sigma}}_{X}^{-1/2},({\\bf{\\Sigma}}_{X}^{-1/2})^{T}\\vec{\\bf H}_{X}m\\right),}\\\\ &{\\Rightarrow Q_{M C}(\\vec{\\bf X}^{\\prime}|M=m)=p_{C S-E C}\\left(\\alpha,{\\bf{\\Sigma}}_{{\\bf{I}}_{d}},({\\bf{\\Sigma}}_{X}^{-1/2})^{T}\\vec{\\bf H}_{X}m\\right),}\\\\ &{\\Rightarrow Q_{M C}(\\vec{\\bf X}^{\\prime}|M=m)=p_{C S-E C}\\left(\\alpha,{\\bf{\\Sigma}}_{{\\bf{I}}_{d}},({\\bf{\\Sigma}}_{X}^{-1/2})^{\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where $\\mathbf{I}_{d_{X}}$ is a $d_{X}\\times d_{X}$ identity matrix, and $\\vec{\\bf K}_{X}=(\\Sigma_{X}^{-1/2})^{T}\\vec{\\bf H}_{X}$ . ", "page_idx": 45}, {"type": "text", "text": "Specifying $Q_{M C}(\\vec{\\bf X}^{\\prime\\prime}|\\vec{\\bf X}^{\\prime})$ : $Q_{M C}(\\vec{\\bf X}^{\\prime\\prime}|\\vec{\\bf X}^{\\prime})$ is again specified through a deterministic transformation: ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\vec{\\bf X}^{\\prime\\prime}=\\vec{\\bf K}_{Y}(\\vec{\\bf K}_{X}^{1})^{T}\\vec{\\bf X}^{\\prime},\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where $\\vec{\\bf K}_{Y}\\,=\\,({\\pmb{\\Sigma}}_{Y}^{-1/2})^{T}\\vec{\\bf H}_{Y}$ and $\\vec{\\bf K}_{X}^{1}\\,=\\,{}^{1}\\/||\\vec{\\bf K}_{X}\\,||_{2}^{2}\\vec{\\bf K}_{X}$ . We also derive the distribution $Q(\\vec{\\mathbf{X}}^{\\prime}|M)$ which will be needed for showing that the constructed $Q_{M C}(M,\\vec{\\bf X},\\vec{\\bf Y})\\in\\Delta_{P}$ . ", "page_idx": 45}, {"type": "text", "text": "Deriving $Q_{M C}(\\vec{\\mathbf{X}}^{\\prime\\prime}|M\\ =\\ m)$ : We again use lemma 11 to calculate $Q_{M C}(\\vec{\\mathbf{X}}^{\\prime\\prime}|M\\ =\\ m)$ , as ${\\cal Q}_{M C}({\\vec{\\bf X}}^{\\prime}|M\\,=\\,m)$ is an elliptically contoured multivariate stable distribution (see (160)) for a fixed $m$ , and (159) defines a scaling and translation operation. ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{M C}(\\vec{\\bf X}^{\\prime}|M=m)=p_{C S-E C}\\left(\\alpha,(\\vec{\\bf K}_{Y}(\\vec{\\bf K}_{X}^{1})^{T}){\\bf I}_{d_{X}}(\\vec{\\bf K}_{Y}(\\vec{\\bf K}_{X}^{1})^{T})^{T},\\vec{\\bf K}_{Y}(\\vec{\\bf K}_{X}^{1})^{T}\\vec{\\bf K}_{X}m\\right),}\\\\ &{\\qquad\\qquad\\qquad=p_{C S-E C}\\left(\\alpha,(\\vec{\\bf K}_{Y}(\\vec{\\bf K}_{X}^{1})^{T})(\\vec{\\bf K}_{Y}(\\vec{\\bf K}_{X}^{1})^{T})^{T},\\vec{\\bf K}_{Y}(\\vec{\\bf K}_{X}^{1})^{T}\\vec{\\bf K}_{X}m\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "From lemma 10, we know that $\\vec{\\bf K}_{Y}(\\vec{\\bf K}_{X}^{1})^{T}\\vec{\\bf K}_{X}=\\vec{\\bf K}_{Y}$ , which implies ", "page_idx": 45}, {"type": "equation", "text": "$$\nQ_{M C}(\\vec{\\bf X}^{\\prime}|M=m)=p_{C S-E C}\\left(\\alpha,(\\vec{\\bf K}_{Y}(\\vec{\\bf K}_{X}^{1})^{T})(\\vec{\\bf K}_{Y}(\\vec{\\bf K}_{X}^{1})^{T})^{T},\\vec{\\bf K}_{Y}m\\right).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Specifying $Q_{M C}(\\vec{\\bf Y}^{\\prime}|\\vec{\\bf X}^{\\prime\\prime})$ : We specify $Q_{M C}(\\vec{\\bf Y}^{\\prime}|\\vec{\\bf X}^{\\prime\\prime})$ through the following stochastic transformation: ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\vec{\\mathbf{Y}}^{\\prime}=\\vec{\\mathbf{X}}^{\\prime\\prime}+\\vec{\\epsilon},\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where $\\vec{\\epsilon}$ follows a multivariate stable distribution. Furthermore, we assume that $\\vec{\\epsilon}$ is jointly independent from $(\\vec{\\bf X},\\vec{\\bf X}^{\\prime},\\vec{\\bf X}^{\\prime\\prime},M)$ , i.e., $\\vec{\\epsilon}$ \u22a5\u22a5 $(\\vec{\\bf X},\\vec{\\bf X}^{\\prime},\\vec{\\bf X}^{\\prime\\prime},M)$ . The characteristic function of $\\vec{\\epsilon}$ is defined as follows: ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[e^{i\\vec{\\bf t}^{T}\\vec{\\epsilon}}\\right]=\\exp\\left(-\\left(\\vec{\\bf t}^{T}{\\bf I}_{d_{Y}}\\vec{\\bf t}\\right)^{\\alpha/2}+\\left(\\vec{\\bf t}^{T}(\\vec{\\bf K}_{Y}(\\vec{\\bf K}_{X}^{1})^{T})(\\vec{\\bf K}_{Y}(\\vec{\\bf K}_{X}^{1})^{T})^{T}\\vec{\\bf t}\\right)^{\\alpha/2}\\right).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "In order for $Q_{M C}(\\vec{\\bf Y}^{\\prime}|\\vec{\\bf X}^{\\prime\\prime})$ to be a valid distribution, we need to ensure that the distribution of $\\vec{\\epsilon}$ defined in (164) is a legitimate multivariate stable distribution. Similarly to the proof of theorem 2, we will utilize property 1 for showing that $\\vec{\\epsilon}$ follows a stable multivariate stable distribution. Property 1 states a random vector is distributed according to a multivariate stable distribution iff every 1- dimensional projection of this random vector follows a univariate stable continuous distribution. Hence, to show that $\\vec{\\epsilon}$ is distributed according to a legitimate multivariate stable distribution, we will show that every 1-dimensional projection of $\\vec{\\epsilon}$ follows a univariate stable continuous distribution. ", "page_idx": 45}, {"type": "text", "text": "Let $\\vec{\\bf t}\\in\\mathbb{R}^{d_{Y}}$ , then the characteristic function of the 1-dimensional projection of $\\vec{\\epsilon}$ along $\\vec{\\bf t}$ , i.e., $\\vec{\\bf t}^{T}\\vec{\\pmb{\\epsilon}}$ can be trivially derived from (164). ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[e^{i\\vec{\\bf t}^{T}\\vec{\\epsilon}}\\right]=\\exp\\left(-\\left(\\vec{\\bf t}^{T}{\\bf I}_{d_{Y}}\\vec{\\bf t}\\right)^{\\alpha/2}+\\left(\\vec{\\bf t}^{T}(\\vec{\\bf K}_{Y}(\\vec{\\bf K}_{X}^{1})^{T})(\\vec{\\bf K}_{Y}(\\vec{\\bf K}_{X}^{1})^{T})^{T}\\vec{\\bf t}\\right)^{\\alpha/2}\\right).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Comparing the characteristic function of $\\vec{\\bf t}^{T}\\vec{\\pmb{\\epsilon}}$ described in (127) with the standard characteristic function of a univariate stable characteristic function defined in (323), we can conclude $\\vec{\\bf t}^{T}\\vec{\\pmb{\\epsilon}}\\sim$ $p_{C S}(\\alpha,\\beta(\\vec{\\mathbf{t}}),\\gamma(\\vec{\\mathbf{t}}),\\mu(\\vec{\\mathbf{t}}))$ , where: ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\gamma(\\vec{\\mathbf t})=\\left(\\vec{\\mathbf t}^{T}\\mathbf I_{d\\gamma}\\vec{\\mathbf t}\\right)^{\\alpha/2}-\\left(\\vec{\\mathbf t}^{T}(\\vec{\\mathbf K}_{Y}(\\vec{\\mathbf K}_{X}^{1})^{T})(\\vec{\\mathbf K}_{Y}(\\vec{\\mathbf K}_{X}^{1})^{T})^{T}\\vec{\\mathbf t}\\right)^{\\alpha/2},\\mu(\\vec{\\mathbf t})=0,\\mathrm{~and~}\\beta(\\vec{\\mathbf t})=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "In order for $\\vec{\\bf t}^{T}\\vec{\\pmb{\\epsilon}}$ to have a legitimate univariate stable distribution, we just need to show that $\\gamma(\\vec{\\mathbf{t}})\\,\\geq\\,0\\,\\,\\forall\\,\\,\\vec{\\mathbf{t}}\\,\\in\\,\\mathbb{R}^{d_{Y}}$ . Note that $\\gamma({\\vec{\\mathbf{t}}})\\,=\\,0$ would correspond to the case where all the mass of the distribution is centered at 0, which does satisfy definition 2, and is an example of a degenerate univariate stable distribution. ", "page_idx": 46}, {"type": "text", "text": "By the assumption in theorem, $\\|\\vec{\\mathbf{K}}_{Y}\\|_{2}=\\|\\Sigma_{Y}^{-1/2}\\vec{\\mathbf{H}}_{Y}\\|_{2}\\le\\|\\vec{\\mathbf{K}}_{X}\\|_{2}=\\|\\Sigma_{X}^{-1/2}\\vec{\\mathbf{H}}_{X}\\|_{2}$ . Hence, we can use the result of lemma 12, which shows that $(\\vec{\\bf K}_{Y}(\\vec{\\bf K}_{X}^{1})^{T})(\\vec{\\bf K}_{Y}(\\vec{\\bf K}_{X}^{1})^{T})^{T}\\preceq{\\bf I}_{d_{Y}}$ . Using the fact that $(\\mathbf{I}_{d_{Y}}-(\\vec{\\mathbf{K}}_{Y}(\\vec{\\mathbf{K}}_{X}^{1})^{T})(\\vec{\\mathbf{K}}_{Y}(\\vec{\\mathbf{K}}_{X}^{1})^{T})^{T})$ is a positive semi-definite matrix, we have: ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\vec{\\mathbf{t}}^{T}\\left(\\mathbf{I}_{d_{Y}}-(\\vec{\\mathbf{K}}_{Y}(\\vec{\\mathbf{K}}_{X}^{1})^{T})(\\vec{\\mathbf{K}}_{Y}(\\vec{\\mathbf{K}}_{X}^{1})^{T})^{T}\\right)\\vec{\\mathbf{t}}\\ge0\\,\\forall\\,\\vec{\\mathbf{t}}\\in\\mathbb{R}^{d_{Y}}\\,,}\\\\ &{\\Rightarrow\\vec{\\mathbf{t}}^{T}\\mathbf{I}_{d_{Y}}\\vec{\\mathbf{t}}\\ge\\vec{\\mathbf{t}}^{T}(\\vec{\\mathbf{K}}_{Y}(\\vec{\\mathbf{K}}_{X}^{1})^{T})(\\vec{\\mathbf{K}}_{Y}(\\vec{\\mathbf{K}}_{X}^{1})^{T})^{T}\\vec{\\mathbf{t}}\\,\\forall\\,\\vec{\\mathbf{t}}\\in\\mathbb{R}^{d_{Y}}\\,,}\\\\ &{\\Rightarrow\\left(\\vec{\\mathbf{t}}^{T}\\mathbf{I}_{d_{Y}}\\vec{\\mathbf{t}}\\right)^{\\alpha/2}\\ge\\left(\\vec{\\mathbf{t}}^{T}(\\vec{\\mathbf{K}}_{Y}(\\vec{\\mathbf{K}}_{X}^{1})^{T})(\\vec{\\mathbf{K}}_{Y}(\\vec{\\mathbf{K}}_{X}^{1})^{T})^{T}\\vec{\\mathbf{t}}\\right)^{\\alpha/2}\\,\\forall\\,\\vec{\\mathbf{t}}\\in\\mathbb{R}^{d_{Y}}\\,,}\\\\ &{\\Rightarrow\\gamma(\\vec{\\mathbf{t}})=\\left(\\vec{\\mathbf{t}}^{T}\\mathbf{I}_{d_{Y}}\\vec{\\mathbf{t}}\\right)^{\\alpha/2}-\\left(\\vec{\\mathbf{t}}^{T}(\\vec{\\mathbf{K}}_{Y}(\\vec{\\mathbf{K}}_{X}^{1})^{T})(\\vec{\\mathbf{K}}_{Y}(\\vec{\\mathbf{K}}_{X}^{1})^{T})^{T}\\vec{\\mathbf{t}}\\right)^{\\alpha/2}\\ge0\\,\\forall\\,\\vec{\\mathbf{t}}\\in\\mathbb{R}^{d_{Y}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Hence, we know that $\\vec{\\epsilon}$ follows a legitimate multivariate stable distribution. We also derive $Q_{M C}(\\vec{\\mathbf{Y}}^{\\prime}|M)$ which is needed for showing $Q_{M C}(M,\\vec{\\bf X},\\vec{\\bf Y})\\in\\Delta_{P}$ . ", "page_idx": 46}, {"type": "text", "text": "Deriving $Q_{M C}(\\vec{\\mathbf{Y}}^{\\prime}|M)$ : We will derive $Q_{M C}(\\vec{\\mathbf{Y}}^{\\prime}|M)$ using the fact that $\\vec{\\epsilon}\\perp\\vec{\\bf X}^{\\prime\\prime}|M=m$ , hence their conditional characteristic functions would just result in multiplication. ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb E\\left[e^{i\\vec{\\bf t}^{T}\\vec{\\bf Y}^{\\prime}}\\Big|\\,M=m\\right]=\\mathbb E\\left[e^{i\\vec{\\bf t}^{T}(\\vec{\\bf X}^{\\prime\\prime}+\\vec{\\epsilon})}\\Big|\\,M=m\\right]=\\mathbb E\\left[e^{i\\vec{\\bf t}^{T}\\vec{\\bf X}^{\\prime\\prime}+i\\vec{\\bf t}^{T}\\vec{\\epsilon}}\\Big|\\,M=m\\right],}\\\\ &{\\,=\\mathbb E\\left[e^{i\\vec{\\bf t}^{T}\\vec{\\bf X}^{\\prime\\prime}}e^{i\\vec{\\bf t}^{T}\\vec{\\epsilon}}\\Big|\\,M=m\\right]\\overset{(a)}=\\mathbb E\\left[e^{i\\vec{\\bf t}^{T}\\vec{\\bf X}^{\\prime\\prime}}\\Big|\\,M=m\\right]\\mathbb E\\left[e^{i\\vec{\\bf t}^{T}\\vec{\\epsilon}}\\Big|\\,M=m\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where $(a)$ is due to $\\vec{\\epsilon}\\perp\\vec{\\mathbf{X}}^{\\prime\\prime}|M$ . Substituting the characteristic functions of $\\vec{\\bf X}^{\\prime\\prime}$ and $\\vec{\\epsilon}$ from (162) and (164), respectively, in (168) ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\exp\\left(-\\left(\\vec{\\bf t}^{T}(\\vec{\\bf K}_{Y}(\\vec{\\bf K}_{X}^{1})^{T})(\\vec{\\bf K}_{Y}(\\vec{\\bf K}_{X}^{1})^{T})^{T}\\vec{\\bf t}\\right)^{\\alpha/2}+i\\vec{\\bf t}^{T}\\vec{\\bf K}_{Y}m\\right)\\times}\\\\ &{\\quad\\exp\\left(-\\left(\\vec{\\bf t}^{T}{\\bf I}_{d_{Y}}\\vec{\\bf t}\\right)^{\\alpha/2}+\\left(\\vec{\\bf t}^{T}(\\vec{\\bf K}_{Y}(\\vec{\\bf K}_{X}^{1})^{T})(\\vec{\\bf K}_{Y}(\\vec{\\bf K}_{X}^{1})^{T})^{T}\\vec{\\bf t}\\right)^{\\alpha/2}\\right),}\\\\ &{=\\exp\\left(-\\left(\\vec{\\bf t}^{T}{\\bf I}_{d_{Y}}\\vec{\\bf t}\\right)^{\\alpha/2}+i\\vec{\\bf t}^{T}\\vec{\\bf K}_{Y}m\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "By inspecting the characteristic function shown in (169), we can conclude: ", "page_idx": 46}, {"type": "equation", "text": "$$\nQ_{M C}(\\vec{\\mathbf{Y}}^{\\prime}|M=m)=p_{C S-E C}(\\alpha,\\mathbf{I}_{d_{Y}},\\vec{\\mathbf{K}}_{Y}m).\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Specifying $Q_{M C}(\\vec{\\bf Y}|\\vec{\\bf Y}^{\\prime})$ : Lastly, we specify $Q_{M C}(\\vec{\\bf Y}|\\vec{\\bf Y}^{\\prime})$ through the following deterministic transformation: ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\vec{\\bf Y}=(\\Sigma_{Y}^{1/2})^{T}\\vec{\\bf Y}^{\\prime}+\\vec{\\bf b}_{Y}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Finally, we construct the desired $Q_{M C}(M,{\\vec{\\mathbf{X}}},{\\vec{\\mathbf{Y}}})$ from $Q_{M C}(M,\\vec{\\mathbf{X}},\\vec{\\mathbf{X}}^{\\prime},\\vec{\\mathbf{X}}^{\\prime\\prime},\\vec{\\mathbf{Y}}^{\\prime},\\vec{\\mathbf{Y}})$ by marginalizing $\\vec{\\mathbf{X}}^{\\prime},\\vec{\\mathbf{X}}^{\\prime\\prime}$ , and $\\vec{\\bf Y^{\\prime}}$ . ", "page_idx": 46}, {"type": "text", "text": "Part 2: Showing $Q_{M C}(M,\\vec{\\bf X},\\vec{\\bf Y})\\in\\Delta_{P}$ ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "For showing that $Q_{M C}(M,\\vec{\\bf X},\\vec{\\bf Y})\\in\\Delta_{P}$ , we first need to derive $Q_{M C}(\\vec{\\mathbf{Y}}|M)$ . ", "page_idx": 46}, {"type": "text", "text": "Deriving $\\underline{{Q}}(\\vec{\\mathbf{Y}}|M)$ : We use lemma 11 to calculate $Q_{M C}(\\vec{\\mathbf{Y}}\\vert M=m)$ , as $Q_{M C}(\\vec{\\bf X}|M=m)$ is an elliptically contoured multivariate continuous stable distribution for a fixed $m$ (see (170)), and (171) defines a scaling and translation operation. ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{M C}(\\vec{\\bf X}^{\\prime}|M=m)=p_{C S-E C}\\left(\\alpha,(\\Sigma_{Y}^{1/2})^{T}\\Sigma_{Y}^{1/2},(\\Sigma_{Y}^{1/2})^{T}\\vec{\\bf K}_{Y}m+\\vec{\\bf b}_{Y}\\right),}\\\\ &{\\overset{(a)}{=}p_{C S-E C}\\left(\\alpha,\\Sigma_{Y},(\\Sigma_{Y}^{1/2})^{T}\\vec{\\bf K}_{Y}m+\\vec{\\bf b}_{Y}\\right),}\\\\ &{\\overset{(b)}{=}p_{C S-E C}\\left(\\alpha,\\Sigma_{Y},(\\Sigma_{Y}^{1/2})^{T}(\\Sigma_{Y}^{-1/2})^{T}\\vec{\\bf H}_{Y}m+\\vec{\\bf b}_{Y}\\right),}\\\\ &{=p_{C S-E C}\\left(\\alpha,\\Sigma_{Y},\\vec{\\bf H}_{Y}m+\\vec{\\bf b}_{Y}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where $(a)$ is due to the fact that $(\\boldsymbol{\\Sigma}_{Y}^{1/2})^{T}\\boldsymbol{\\Sigma}_{Y}^{1/2}=\\boldsymbol{\\Sigma}_{Y}$ , and $(b)$ is due to the fact that $\\vec{\\bf K}_{Y}=\\Sigma_{Y}^{-1/2}\\vec{\\bf H}_{Y}$ . Comparing (173) with $P(\\vec{\\mathbf{Y}}|M=\\dot{m})$ shown in Appx. H.2, we can conclude: ", "page_idx": 47}, {"type": "equation", "text": "$$\nQ_{M C}(\\vec{\\mathbf{Y}}|M)=P(\\vec{\\mathbf{Y}}|M).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "From (174) and (158), we can conclude: ", "page_idx": 47}, {"type": "equation", "text": "$$\nQ_{M C}(M,\\vec{\\mathbf{Y}})=Q_{M C}(M)Q_{M C}(\\vec{\\mathbf{Y}}|M)=P(M)P(\\vec{\\mathbf{Y}}|M)=P(M,\\vec{\\mathbf{Y}}).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "We know that by construction: ", "page_idx": 47}, {"type": "equation", "text": "$$\nQ_{M C}(M,\\vec{\\bf X})=P(M,\\vec{\\bf X}).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Hence, $Q_{M C}(M,\\vec{\\bf X},\\vec{\\bf Y})\\in\\Delta_{P}$ and consequently, by proposition 1, $U I(M;{\\vec{\\mathbf{Y}}}\\backslash{\\vec{\\mathbf{X}}})=0$ , concluding our proof. ", "page_idx": 47}, {"type": "text", "text": "Lemma 11. Let $\\vec{\\mathbf{X}}\\sim p_{C S-E C}(\\alpha,\\Sigma,\\vec{\\mu})$ be a $d$ -dimensional random vector, where $\\Sigma$ is a $d\\times d$ positive definite matrix, $\\vec{\\mu}\\in\\mathbb R^{d}$ , and $\\alpha\\in(0,2]$ . Furthermore, let $\\mathbf{A}\\in\\mathbb{R}^{n\\times d}$ and $\\vec{\\mathbf{b}}\\in\\mathbb{R}^{d}$ . Then, ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathbf{A}\\vec{\\mathbf{X}}+\\vec{\\mathbf{b}}\\sim p_{C S-E C}(\\alpha,\\mathbf{A}\\Sigma\\mathbf{A}^{T},\\mathbf{A}\\vec{\\mu}+\\vec{\\mathbf{b}}).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Proof. Calculating the characteristic function of $\\mathbf{A}\\vec{\\mathbf{X}}+\\vec{\\mathbf{b}}$ : ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\exp\\left(i\\bar{\\mathbf{t}}^{T}(\\mathbf{A}\\vec{\\mathbf{X}}+\\vec{\\mathbf{b}})\\right)\\right]=\\mathbb{E}\\left[\\exp\\left(i\\bar{\\mathbf{t}}^{T}\\mathbf{A}\\vec{\\mathbf{X}}+i\\vec{\\mathbf{t}}^{T}\\vec{\\mathbf{b}}\\right)\\right]=\\mathbb{E}\\left[\\exp\\left(i\\bar{\\mathbf{t}}^{T}\\mathbf{A}\\vec{\\mathbf{X}}\\right)\\exp\\left(i\\vec{\\mathbf{t}}^{T}\\vec{\\mathbf{b}}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Employing linearity of expectation to pull the term $\\exp\\left(i\\vec{\\mathbf{t}}^{T}\\vec{\\mathbf{b}}\\right)$ out of the expectation, as it is constant with respect to $\\vec{\\bf X}$ : ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\exp\\left(i\\vec{\\mathbf{t}}^{T}(\\mathbf{A}\\vec{\\mathbf{X}}+\\vec{\\mathbf{b}})\\right)\\right]=\\exp\\left(i\\vec{\\mathbf{t}}^{T}\\vec{\\mathbf{b}}\\right)\\mathbb{E}\\left[\\exp\\left(i\\vec{\\mathbf{t}}^{T}\\mathbf{A}\\vec{\\mathbf{X}}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Substituting $\\vec{\\bf t}^{\\prime}=\\vec{\\bf t}^{T}{\\bf A}$ in (177): ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\exp\\left(i\\vec{\\mathbf{t}}^{T}(\\mathbf{A}\\vec{\\mathbf{X}}+\\vec{\\mathbf{b}})\\right)\\right]=\\exp\\left(i\\vec{\\mathbf{t}}^{T}\\vec{\\mathbf{b}}\\right)\\mathbb{E}\\left[\\exp\\left(i(\\vec{\\mathbf{t}}^{\\prime})^{T}\\vec{\\mathbf{X}}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Substituting the formula of the characteristic function of an Elliptically-contoured multivariate stable distribution (given in (156)) for calculating $\\mathbb{E}\\left[\\exp\\left(i(\\vec{\\mathbf{t}}^{\\prime})^{\\,T}\\vec{\\mathbf{X}}\\right)\\right]$ as $\\vec{\\mathbf{X}}\\sim p_{C S-E C}(\\alpha,\\Sigma,\\vec{\\mu})$ : ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\exp\\left(i\\vec{\\mathbf{t}}^{T}(\\mathbf{A}\\vec{\\mathbf{X}}+\\vec{\\mathbf{b}})\\right)\\right]=\\exp\\left(i\\vec{\\mathbf{t}}^{T}\\vec{\\mathbf{b}}\\right)\\exp\\left(-\\left((\\vec{\\mathbf{t}}^{\\prime})^{T}\\Sigma\\vec{\\mathbf{t}}^{\\prime}\\right)^{\\alpha/2}+i(\\vec{\\mathbf{t}}^{\\prime})^{T}\\vec{\\mu}\\right).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Re-substituting $\\vec{\\bf t}^{\\prime}=\\vec{\\bf t}^{T}{\\bf A}$ in the above equation ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\exp\\left(i\\vec{\\bf t}^{T}\\vec{\\bf b}\\right)\\exp\\left(-\\left(\\vec{\\bf t}^{T}{\\bf A}\\Sigma{\\bf A}^{T}\\vec{\\bf t}\\right)^{\\alpha/2}+i\\vec{\\bf t}^{T}{\\bf A}\\vec{\\pmb{\\mu}}\\right),}\\\\ &{=\\exp\\left(-\\left(\\vec{\\bf t}^{T}{\\bf A}\\Sigma{\\bf A}^{T}\\vec{\\bf t}\\right)^{\\alpha/2}+i\\vec{\\bf t}^{T}({\\bf A}\\vec{\\pmb{\\mu}}+\\vec{\\bf b})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "By inspecting the characteristic function described in (178), we can determine that A\u20d7X + \u20d7b is distributed according to an elliptically-contoured multivariate stable distribution, i.e., $\\mathbf{A}\\vec{\\mathbf{X}}+\\vec{\\mathbf{b}}\\sim$ $p_{C S-E C}\\left(\\alpha,\\mathbf{A}\\Sigma\\mathbf{A}^{\\bar{T}},\\mathbf{A}\\vec{\\mu}+\\vec{\\mathbf{b}}\\right)$ \u53e3 ", "page_idx": 48}, {"type": "text", "text": "Lemma 12. Let $\\vec{\\bf H}_{1}\\,\\in\\,\\mathbb{R}^{d_{1}}$ and $\\vec{\\bf{H}}_{2}\\,\\in\\,\\mathbb{R}^{d_{2}}$ . Then, $\\|\\vec{\\bf H}_{1}\\|_{2}\\;\\geq\\;\\|\\vec{\\bf H}_{2}\\|_{2}$ if and only $i f\\exists~a$ matrix $\\mathbf{A}\\in\\mathbb{R}^{d_{2}\\times d_{1}}$ such that $\\vec{\\bf H}_{2}={\\bf A}\\vec{\\bf H}_{1}$ , and ${\\mathbf{A}}{\\mathbf{A}}^{T}\\preceq{\\mathbf{I}}_{d_{2}}$ , with ${\\bf A}=\\vec{\\bf H}_{2}(\\vec{\\bf H}_{1}^{1})^{T}$ , where $\\mathbf{B}\\preceq\\mathbf{C}$ means ${\\mathbf B}-{\\mathbf C}$ is a positive semi-definite matrix, $\\vec{\\bf H}_{1}^{1}$ is as defined in lemma $I O_{;}$ , and $\\mathbf{I}_{d_{2}}$ is $d_{2}\\times d_{2}$ identity matrix. ", "page_idx": 48}, {"type": "text", "text": "Proof. See the proof of lemma 5 in [48] for the special case $t=1$ . ", "page_idx": 48}, {"type": "text", "text": "I Proof of Theorem 4 ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "In this section, we provide the proof of theorem 4. Since theorem 4 provides the analytical PID terms for the univariate affine discrete stable system defined in Sec. 4, we briefly restate certain key properties of the univariate discrete stable distributions and the corresponding univariate affine discrete stable system for convenience. ", "page_idx": 48}, {"type": "text", "text": "I.1 Univariate discrete stable distribution ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Univariate discrete stable distributions are the discrete analogues of univariate continuous stable distributions. We refer the reader to Appx. N.4 for more details on univariate discrete stable distributions. We now list certain key properties of univariate continuous stable distributions that we make use of in the proof of theorem 4: ", "page_idx": 48}, {"type": "text", "text": "1. If $X$ is distributed according to a univariate discrete stable distribution, then $X\\overset{d}{=}\\gamma\\circ X_{1}+$ $(1-\\gamma^{\\nu})^{1/\\nu}\\circ X_{2}$ , where $X_{1}$ and $X_{2}$ are two independent copies of $X$ , $\\nu\\in(0,1]$ , $\\gamma\\in[0,1]$ , and $\\circ$ denotes the binomial thinning operation.   \n2. The p.m.f. of discrete stable distributions are characterized by two parameters: exponent $\\nu\\in(0,1]$ , and rate parameter $\\tau\\in(0,\\infty)$ . We denote the p.m.f. of a univariate discrete stable distribution as $P_{D S}(\\nu,\\tau)$ . In general, discrete stable distributions do not have a \u201cnice\u201d analytical form consisting of well-known elementary functions. Consequently, the univariate discrete stable distribution are typically expressed through their probability generating function.   \n3. The probability generating function of a discrete random variable $N$ having a stable discrete distribution is given by (179): ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\mathbb{P}_{N}(z)=\\exp(-\\tau(1-z)^{\\gamma}).\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "I.2 Definition of univariate affine discrete stable system ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Let $M,X$ , and $Y$ be a system of random variables with the joint distribution $P(M,X,Y)$ . The joint distribution $P(M,X,Y)$ describes the univariate affine discrete stable system if it satisfies the following two properties: ", "page_idx": 48}, {"type": "text", "text": "1. $M\\sim P(M)$ having some support set ${\\mathcal{M}}\\subseteq(0,\\infty)$ .   \n2. The conditional distributions of random variables $X$ and $Y$ conditioned on $M$ can be expressed through discrete stable family distributions with an affine dependence on $M$ . Formally, $P(X|M{=}m){=}P_{D S}(\\nu,a m\\stackrel{.}{+}b)$ and $P(Y|M\\!=\\!m)\\!=\\!P_{D S}(\\nu,\\stackrel{.}{c}\\!m+d)$ , where $a,b,c,d\\in(0,\\infty)$ . ", "page_idx": 48}, {"type": "text", "text": "I.3 Formal proof of Theorem 4 ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Theorem 4. Let $M,X$ , and $Y$ be random variables whose joint distribution $P(M,X,Y)$ describes a univariate affine discrete stable system. Without the loss of generality, assume $a\\geq c$ . If ${a}/{b}\\geq c/d,$ then $\\Delta_{P}$ contains a Markov chain of the form $M\\rightarrow X\\rightarrow Y$ and $U I(M;Y\\backslash X)=0$ . ", "page_idx": 48}, {"type": "text", "text": "Proof. We first note that we can always assume $a\\geq c$ without the loss of generality because if $a\\leq c$ , then we can always switch our nomenclature to refer to $Y$ as $X$ , and $X$ as $Y$ . ", "page_idx": 49}, {"type": "text", "text": "We briefly outline the proof structure. ", "page_idx": 49}, {"type": "text", "text": "1. In the first part, we explicitly construct a joint distribution $Q_{M C}(M,X,Y)$ having the Markovian structure $M\\rightarrow X\\rightarrow Y$ . 2. In the second part, we show that the $Q_{M C}(M,X,Y)$ constructed in the first part lies in $\\Delta_{P}$ . Therefore, we can then apply the result of proposition 1 to conclude $U I(M;Y\\backslash X)=0$ . ", "page_idx": 49}, {"type": "text", "text": "Part 1: Specifying $Q_{M C}(M,X,Y)$ ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "We explicitly construct the desired Markov chain $M\\rightarrow X\\rightarrow Y$ by constructing a larger Markov chain $M\\rightarrow X\\rightarrow X^{\\prime}\\rightarrow Y$ and then marginalizing the larger Markov chain to obtain the desired Markov chain $M\\rightarrow X\\rightarrow Y$ . ", "page_idx": 49}, {"type": "text", "text": "Denote the joint distribution of the Markov chain $M\\rightarrow X\\rightarrow X^{\\prime}\\rightarrow Y$ as $Q_{M C}(M,X,X^{\\prime},Y)$ . We can decompose $Q_{M C}(M,X,X^{\\prime},Y)$ by utilizing its Markovian structure as follows: ", "page_idx": 49}, {"type": "equation", "text": "$$\nQ_{M C}(M,X,X^{\\prime},Y)=Q_{M C}(M)Q_{M C}(X|M)Q_{M C}(X^{\\prime}|X)Q_{M C}(Y|X^{\\prime}).\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Consequently, we can specify/construct the distribution $Q_{M C}(M,X,X^{\\prime},Y)$ by individually specifying $\\bar{Q_{M C}}(\\bar{M}),Q_{M C}(\\bar{X}|M),Q_{M C}(X^{\\prime}|X)$ , and $Q_{M C}(Y|X^{\\prime})$ . ", "page_idx": 49}, {"type": "text", "text": "Specifying $Q_{M C}(M)$ and $Q_{M C}(X|M)$ : We choose $Q_{M C}(M)$ and $Q_{M C}(X|M)$ as follows: ", "page_idx": 49}, {"type": "equation", "text": "$$\nQ_{M C}(M)=P(M)~\\mathrm{and}~Q_{M C}(X|M)=P(X|M),\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where $P(M)$ and $P(X|M)$ are marginal distributions derived from the original joint distribution $P(M,X,Y)$ (discussed in the theorem statement) over which the bivariate PID is being calculated. ", "page_idx": 49}, {"type": "text", "text": "Specifying $Q_{M C}(X^{\\prime}|X)$ : $Q_{M C}(X^{\\prime}|X)$ is specified through a binomial thinning operation, specifically: ", "page_idx": 49}, {"type": "equation", "text": "$$\nX^{\\prime}=X\\circ\\left(c/a\\right)^{1/\\nu}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Note that the above binomial thinning operation is valid as $c\\leq a$ by the assumption in the theorem statement. We also derive $Q_{M C}(X^{\\prime}|M)$ as it is needed for showing $Q_{M C}(M,X,Y)\\in\\Delta_{P}$ . ", "page_idx": 49}, {"type": "text", "text": "Deriving $Q_{M C}(X^{\\prime}|M)$ : We use lemma 13 to derive $Q_{M C}(X^{\\prime}|M\\,=\\,m)$ as $Q_{M C}(X|M\\,=\\,m)$ describes a univariate discrete stable distribution for a fixed $m$ , and $X^{\\prime}$ is a binomially-thinned version of $X$ . The exact expression of $Q_{M C}(X^{\\prime}|M=m)$ is provided in (182). ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{Q_{M C}(X^{\\prime}|M=m)=P_{D S}(\\nu,(c/a)^{\\nu/\\nu}(a m+b/a)),}}\\\\ {{=P_{D S}(\\nu,c m+c b/a).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Specifying $Q_{M C}(Y|X^{\\prime})$ : $Q_{M C}(Y|X^{\\prime})$ is specified through the following stochastic transformation: ", "page_idx": 49}, {"type": "equation", "text": "$$\nY=X^{\\prime}+\\epsilon,\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where $\\epsilon\\sim P_{D S}\\left(\\nu,d-c b/a\\right)$ . For $Q_{M C}(Y|X^{\\prime})$ to be a valid distribution, we need to ensure that $\\epsilon$ is distributed according to a legitimate discrete stable distribution. In order to show $\\epsilon$ is distributed according to a legitimate discrete stable distribution, we need to ensure that $\\nu$ and $d-c b/a$ lie within their appropriate bounds as specified in Sec. I.1. It is trivial to see $\\nu\\in(0,1]$ . ", "page_idx": 49}, {"type": "text", "text": "From the assumptions in the theorem statement, we know that ", "page_idx": 49}, {"type": "equation", "text": "$$\n{\\frac{a}{b}}\\geq{\\frac{c}{d}}\\Rightarrow d\\geq{\\frac{b c}{a}}\\Rightarrow d-{\\frac{b c}{a}}\\geq0.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Hence, $\\epsilon$ follows a legitimate discrete stable distribution. Furthermore, we choose $\\epsilon$ \u22a5\u22a5 $(M,X,X^{\\prime})$ .   \nFinally, we construct the desired $Q_{M C}(M,X,Y)$ from $Q_{M C}(M,X,X^{\\prime},Y)$ by marginalizing $X^{\\prime}$ . ", "page_idx": 49}, {"type": "text", "text": "Part 2: Showing $Q_{M C}(M,X,Y)\\in\\Delta_{P}$ ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "For showing that $Q_{M C}(M,X,Y)\\in\\Delta_{P}$ , we first need to derive $Q_{M C}(Y|M)$ . ", "page_idx": 49}, {"type": "text", "text": "Deriving $Q(Y|M)$ : Note that $\\epsilon$ follows discrete stable distribution $P_{D S}(\\nu,d\\textrm{--}\\ b c/a)$ and $\\overline{{Q_{M C}(X^{\\prime}|M\\;=\\;m)}}$ is also a discrete stable distribution $P_{D S}(X^{\\prime};\\nu,c m\\,+\\,b c/a)$ for a fixed $m$ (see (182)). Furthermore, $\\epsilon\\perp\\!\\!\\!\\perp X^{\\prime}|M$ . Hence, using lemma 14, we have ", "page_idx": 50}, {"type": "equation", "text": "$$\nQ_{M C}(Y|M=m)=P_{D S}(\\nu,c m+b c/a-b c/a+d)=P_{D S}(\\nu,c m+d).\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "As $P(Y|M)=P_{D S}(\\nu,c M+d)$ (see Appx. I.2), we have: ", "page_idx": 50}, {"type": "equation", "text": "$$\nQ(Y|M)=P(Y|M).\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "From (180) and (186), we can conclude: ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{M C}(M,X)=Q_{M C}(M)Q_{M C}(X|M)=P(M)P(X|M)=P(M,X),}\\\\ &{Q_{M C}(M,Y)=Q_{M C}(M)Q_{M C}(Y|M)=P(M)P(Y|M)=P(M,Y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Hence, $Q_{M C}(M,X,Y)\\in\\Delta_{P}$ and consequently, by proposition 1, $U I(M;Y\\backslash X)=0$ , concluding our proof. ", "page_idx": 50}, {"type": "text", "text": "Lemma 13. Let $N\\sim P_{D S}(\\nu,\\tau)$ . Then, $N_{1}=\\gamma\\circ N\\sim P_{D S}(\\nu,\\gamma^{\\nu}\\tau)\\ \\forall\\,\\gamma\\in\\,[0,1],$ , where $\\circ$ is the binomial thinning operation. ", "page_idx": 50}, {"type": "text", "text": "Proof. The probability generating function of $N$ , denoted as $\\mathbb{P}_{N}(z)$ , can be described using (179): ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}_{N}\\!\\left(z\\right)=\\exp\\left(-\\tau(1-z)^{\\nu}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "For calculating the probability generating function of $N_{1}$ , we use the following relationship between the probability generating function of a random variable $R$ and its binomially-thinned version $R\\circ p$ , with $p\\in[0,1]$ : ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\mathbb{P}_{R o p}(z)=\\mathbb{P}_{R}(1-p(1-z))\\ (\\mathrm{see\\equation\\3.3\\in\\}24]).\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "From (189) and (190), we can calculate the probability generating function of $N_{1}$ as follows: ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathbb{P}}_{N_{1}}(z)={\\mathbb{P}}_{N o\\gamma}(z)={\\mathbb{P}}_{N}(1-\\gamma(1-z))=\\exp\\left(-\\tau\\left(1-(1-\\gamma(1-z))\\right)^{\\nu}\\right)}\\\\ &{\\qquad\\qquad=\\exp\\left(-\\tau\\left(\\gamma(1-z)\\right)^{\\nu}\\right)=\\exp\\left(-\\tau\\gamma^{\\nu}\\left(1-z\\right)^{\\nu}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Inspecting the probability generating function of $N_{1}$ in (191), we can conclude that $N_{1}~\\sim$ $P_{D S}(\\nu,\\gamma^{\\nu}\\tau)$ . \u53e3 ", "page_idx": 50}, {"type": "text", "text": "Lemma 14. Let ${\\cal N}_{1}\\sim{\\cal P}_{D S}(\\nu,\\tau_{1}),\\,{\\cal N}_{2}\\sim{\\cal P}_{D S}(\\nu,\\tau_{2})$ , and $N_{1}$ \u22a5\u22a5 $N_{2}$ . Then, $N_{1}+N_{2}=N\\sim$ $P_{D S}(n;\\nu,\\tau_{1}+\\tau_{2})$ . ", "page_idx": 50}, {"type": "text", "text": "Proof. The probability generating function of $N_{1}$ and $N_{2}$ , denoted as $\\mathbb{P}_{N_{1}}(z)$ and $\\mathbb{P}_{N_{2}}(z)$ , can be described using (179): ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\mathbb{P}_{N_{1}}(z)=\\exp\\left(-\\tau_{1}(1-z)^{\\nu}\\right)\\mathrm{~and~}\\mathbb{P}_{N_{2}}(z)=\\exp\\left(-\\tau_{2}(1-z)^{\\nu}\\right).\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Calculating the probability generating function of $N=N_{1}+N_{2}$ , ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}_{N}(z)=\\mathbb{P}_{N_{1}+N_{2}}(z)\\overset{(a)}{=}\\mathbb{P}_{N_{1}}(z)\\mathbb{P}_{N_{2}}(z)\\overset{(b)}{=}\\exp\\left(-\\tau_{1}\\left(1-z\\right)^{\\nu}\\right)\\exp\\left(-\\tau_{2}\\left(1-z\\right)^{\\nu}\\right),}\\\\ &{\\qquad\\quad=\\exp\\left(-(\\tau_{1}+\\tau_{2})\\left(1-z\\right)^{\\nu}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where $(a)$ is due to the fact that $N_{1}\\perp\\!\\!\\!\\perp N_{2}$ , and $(b)$ is achieved by substituting the explicit forms of $\\mathbb{P}_{N_{1}}(z)$ and $\\mathbb{P}_{N_{2}}(z)$ from (192). Inspecting the probability generating function of $N_{1}+N_{2}$ in (193), we can conclude that $N_{1}+N_{2}=N\\sim P_{D S}(n_{1};\\nu,\\tau_{1}+\\tau_{2})$ . \u53e3 ", "page_idx": 50}, {"type": "text", "text": "J Proof of Theorem 5 ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "In this section, we provide the proof of theorem 5. Since theorem 5 provides the analytical PID terms for the multivariate linear Poisson system defined in Sec. 4, we briefly restate certain key properties of multivariate Poisson distributions and the multivariate linear Poisson system for convenience. ", "page_idx": 50}, {"type": "text", "text": "J.1 Multivariate Poisson Distribution ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "We use the multivariate Poisson distribution proposed in [42]. For more details, see Appx. N.5. We list certain key properties of multivariate Poisson distributions that we make use of in the proof of theorem 5: ", "page_idx": 51}, {"type": "text", "text": "1. Under the definition of the multivariate Poisson distribution proposed in [42], each component of the random vector is a sum of independent Poisson random, i.e., ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\vec{\\bf N}={\\bf A}\\vec{\\bf N}^{g},\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "where $\\mathbf{A}$ is an appropriate matrix of $0\\,\\mathrm{\\dot{s}}$ and 1\u2019s. We can decompose $\\mathbf{A}=[\\mathbf{A}_{1}\\quad.\\dots\\quad\\mathbf{A}_{d^{\\prime}}]$ , where $\\mathbf{A}_{i}$ is a $d\\times\\binom{d}{i}$ submatrix having no duplicate columns, and each of its columns containing exactly $i$ ones and $(d\\mathrm{~-~}i)$ zeros [44], and $\\vec{\\bf N}^{g}\\ =\\ [N_{1}^{g}\\,.\\,.\\,.\\,N_{d}^{g}\\ \\,N_{12}^{g}\\,.\\,.\\,.$ $N_{(d-1)d}^{g}\\cdot\\cdot\\cdot\\ N_{d-(d^{\\prime}-1)\\ldots d}^{g}]^{T}$ N dg\u2212(d\u2032\u22121)...d]T , with N ig1... $N_{i_{1}\\ldots i_{j}}^{g}\\sim\\mathrm{Poisson}(\\lambda_{i_{1}\\ldots i_{j}})\\,\\forall\\,(i_{1},\\ldots,i_{j})\\in\\mathbb{A}_{j}^{d},j\\in[d^{\\prime}].$ Furthermore, the random variables $\\left\\{N_{1},\\ldots,N_{d-\\left(d^{\\prime}-1\\right)\\ldots d}\\right\\}$ are mutually independent. ", "page_idx": 51}, {"type": "text", "text": "2. We denote the p.m.f. of the multivariate Poisson distribution as Poisson $(d,d^{\\prime},\\vec{\\Lambda})$ , where $d\\geq d^{\\prime}$ , and ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\vec{\\bf A}=\\left[\\lambda_{1}\\;.\\;.\\;.\\;\\lambda_{d}\\;\\lambda_{12}\\;\\ldots\\lambda_{d-(d^{\\prime}-1)\\ldots d}\\right]^{T}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "3. For $d^{\\prime}\\,=\\,1$ , we have that $\\vec{\\bf N}$ is a collection of independent Poisson random variables, and when both $d\\,=\\,d^{\\prime}\\,=\\,1$ , we recover the scalar Poisson distribution: $\\operatorname*{Pr}(N=n)=$ n!\u03bb , \u2200n \u2208N0. ", "page_idx": 51}, {"type": "text", "text": "4. Let $\\vec{\\bf N}\\sim\\mathrm{Poisson}(d,d^{\\prime},\\vec{\\bf\\Lambda})$ , where $\\vec{\\bf N}$ is a $d$ -dimensional random vector. Then the corresponding p.m.f. of $\\vec{\\bf N}$ is described below: ", "page_idx": 51}, {"type": "text", "text": "Let $\\vec{\\bf n}^{\\prime}\\!=\\left[n_{12}\\,\\dots\\,n_{(d-1)d}\\,\\dots\\,n_{d-(d^{\\prime}-1)\\dots d}\\right]^{T}$ , and $d_{\\vec{\\mathbf{n}}^{\\prime}}$ be the dimension of $\\vec{\\bf n}^{\\prime}$ , then: ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle P(\\vec{\\bf N}\\!=\\!\\vec{\\bf n})\\!=\\!e^{-\\vec{\\bf1}^{T}\\vec{\\Lambda}}\\prod_{i=1}^{d}\\lambda_{i}^{n_{i}}\\sum_{{\\bf\\vec{n}}^{\\prime}\\in C}\\left(\\prod_{(i_{1},i_{2})\\in\\mathbb{A}_{2}^{d}}\\left(\\!\\frac{\\lambda_{i_{1}i_{2}}}{\\lambda_{i_{1}}\\lambda_{i_{2}}}\\right)^{n_{i_{1}i_{2}}}\\!\\times\\!\\!\\!}}\\\\ {{\\displaystyle\\dots\\times\\prod_{(i_{1},\\dots,i_{d^{\\prime}})\\in\\mathbb{A}_{d^{\\prime}}^{d}}\\left(\\!\\frac{\\lambda_{i_{1}\\dots i_{d^{\\prime}}}}{\\prod_{j=1}^{d^{\\prime}}\\lambda_{i_{j}}}\\!\\right)^{n_{i_{1}\\dots i_{d^{\\prime}}}}\\!\\times Q(\\vec{\\bf n},\\vec{\\bf n}^{\\prime})\\!\\right),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "where $C=\\{\\vec{\\bf n}^{\\prime}\\in\\mathbb{N}_{0}^{d_{\\vec{\\bf n}^{\\prime}}}{\\bf:}(\\vec{\\bf a}_{i}^{\\prime})^{T}\\vec{\\bf n}^{\\prime}\\leq n_{i}\\;\\forall\\;i\\in[d]\\},$ , and ", "page_idx": 51}, {"type": "equation", "text": "$$\nQ(\\vec{\\bf n},\\vec{\\bf n}^{\\prime})=\\prod_{i=1}^{d}{\\frac{1}{(n_{i}-\\vec{\\bf a}_{i}^{\\prime T}\\vec{\\bf n}^{\\prime})!}}\\prod_{j=2}^{d^{\\prime}}\\prod_{(i_{1},\\dots,i_{j})\\in\\mathbb{A}_{j}^{d}}{\\frac{1}{n_{i_{1}\\dots i_{j}}^{g}!}},\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "with $\\vec{\\bf a}_{i}^{\\prime}$ being the $i$ -th row of the matrix $\\mathbf{A}^{\\prime}=\\left[\\mathbf{A}_{2}\\ldots\\mathbf{A}_{d^{\\prime}}\\right]$ . ", "page_idx": 51}, {"type": "text", "text": "J.2 Definition of the multivariate linear Poisson system ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Let the random variable $M\\sim P(M)$ . $\\vec{\\bf X}$ and $\\vec{\\bf Y}$ are $d_{X}$ -dimensional and $d_{Y}$ -dimensional random vectors, respectively. The joint distribution $P(M,{\\vec{\\mathbf{X}}},{\\vec{\\mathbf{Y}}})$ describes the multivariate linear Poisson system if it satisfies the following two properties: ", "page_idx": 51}, {"type": "text", "text": "1. $M\\sim P(M)$ having some support set ${\\mathcal{M}}\\subseteq(0,\\infty)$ . ", "page_idx": 51}, {"type": "text", "text": "2. The conditional distributions of random vectors $\\vec{\\bf X}$ and $\\vec{\\bf Y}$ are multivariate Poisson distributions, i.e., $P(\\vec{\\bf X}|M){=}\\mathrm{Poisson}(d_{X},d_{X}^{\\prime},\\vec{\\bf A}_{X})$ and $P(\\vec{\\bf Y}|M){=}\\mathrm{Poisson}(d_{Y},d_{Y}^{\\prime},\\vec{\\bf\\Lambda}_{Y})$ , with: $\\begin{array}{r l}&{\\vec{\\Lambda}_{X}\\!=\\!\\left[\\lambda_{1}^{X}\\,\\cdot\\,\\cdot\\,\\cdot\\,\\lambda_{d_{X}-(d_{X}^{\\prime}-1)\\dots d_{X}}^{X}\\right]^{T},\\lambda_{i_{1}\\dots i_{j}}^{X}=\\gamma_{i_{1}\\dots i_{j}}^{X}M^{j}\\,\\forall\\,j\\in[d_{X}]\\,\\mathbf{a}}\\\\ &{\\mathbf{\\Lambda}_{\\mathbf{X}}=\\left[\\lambda_{1}^{Y}\\,\\cdot\\,\\dots\\,\\,\\lambda_{d_{Y}-(d_{Y}^{\\prime}-1)\\dots d_{Y}}^{Y}\\right]^{T},\\lambda_{i_{1}\\dots i_{j}}^{Y}=\\gamma_{i_{1}\\dots i_{j}}^{Y}M^{j}\\,\\forall\\,j\\in[d_{Y}]\\,\\mathbf{a}}\\end{array}$ nd $(i_{1},...,i_{j})\\in\\mathbb{A}_{j}^{d_{X}}$ , nd $(i_{1},...,i_{j})\\in\\mathbb{A}_{j}^{d_{Y}}$ . Here, $\\gamma_{1}^{X},\\dots,\\gamma_{d_{X}-(d_{X}^{\\prime}-1)\\dots d_{X}}^{X},\\gamma_{1}^{Y},\\dots,\\gamma_{d_{Y}-(d_{Y}^{\\prime}-1)\\dots d_{Y}}^{Y}\\in\\mathbb{R}^{+}.$ ", "page_idx": 51}, {"type": "text", "text": "Furthermore, let $\\mathbf{A}_{X}$ , and $\\mathbf{A}_{Y}$ be the corresponding A-matrices (defined in (194)) associated with $P(\\vec{\\bf X}|M)$ and $P(\\vec{\\mathbf{Y}}|M)$ , respectively. ", "page_idx": 52}, {"type": "text", "text": "J.3 Formal proof of Theorem 5 ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Theorem 5. Let the joint distribution $P(M,{\\vec{\\mathbf{X}}},{\\vec{\\mathbf{Y}}})$ of $M,{\\vec{\\mathbf{X}}}$ , and $\\vec{\\bf Y}$ describe the multivariate linear Poisson system. Without the loss of generality, assume $d_{X}^{\\prime}\\;\\geq\\;d_{Y}^{\\prime}$ . If $\\begin{array}{r l}{\\sum_{(i_{1},\\dotsc,i_{j})\\in\\mathbb{A}_{j}^{d_{X}}}\\gamma_{i_{1}\\dotsc i_{j}}^{X}\\ \\geq}&{{}}\\end{array}$ (i1,...,ij)\u2208AdY \u03b3iY1...ij \u2200j \u2208[d\u2032Y ], then \u2206P contains a Markov chain of the form M \u2192 \u20d7X \u2192 \u20d7Y and $U I(M;{\\vec{\\mathbf{Y}}}\\backslash{\\vec{\\mathbf{X}}})=0$ . ", "page_idx": 52}, {"type": "text", "text": "Proof. We first note that we can always assume $d_{X}^{\\prime}\\geq d_{Y}^{\\prime}$ without the loss of generality because if $d_{X}^{\\prime}\\leq d_{Y}^{\\prime}$ , then we can always switch our nomenclature to refer to $\\vec{\\bf Y}$ as $\\vec{\\bf X}$ , and $\\vec{\\bf X}$ as $\\vec{\\bf Y}$ . ", "page_idx": 52}, {"type": "text", "text": "We provide an explicit construction of the Markov chain $M\\to\\vec{\\bf X}\\to\\vec{\\bf X}^{g}\\to\\vec{\\bf Y}^{g}\\to\\vec{\\bf Y}^{\\chi}$ having the marginals $P(M,{\\vec{\\mathbf{X}}})$ , and $P(M,{\\vec{\\mathbf{Y}}})$ . Denote the joint density of $M\\to{\\vec{\\mathbf{X}}}\\to{\\vec{\\mathbf{X}}}^{g}\\to{\\vec{\\mathbf{Y}}}^{g}\\to{\\vec{\\mathbf{Y}}}^{\\vec{\\mathbf{\\mu}}}$ as $Q(M,\\vec{\\bf X},\\vec{\\bf X}^{g},\\vec{\\bf Y}^{g},\\vec{\\bf Y})$ . For $Q(M)$ and $Q(\\vec{\\bf X}|M)$ , we choose them to be equal to $P(M)$ and $P(\\vec{\\bf X}|M)$ , i.e. $Q(M)\\,=\\,P(M)$ , and $Q(\\vec{\\bf X}|M)\\,=\\,P(\\vec{\\bf X}|M)$ . Note that due to this construction, $P(M,{\\vec{\\mathbf{X}}})=Q(M,{\\vec{\\mathbf{X}}})$ holds trivially. ", "page_idx": 52}, {"type": "text", "text": "For $Q(\\vec{\\mathbf{X}}^{g}|\\vec{\\mathbf{X}})$ , we use the result described in lemma 17. Note that the construction for $Q(\\vec{\\mathbf{X}}^{g}|\\vec{\\mathbf{X}})$ is not explicit but rather implicit. Let $d_{\\vec{\\Lambda}_{X}}$ be the dimension of $\\vec{\\Lambda}_{X}$ , then we explicitly choose: $Q(\\vec{\\bf X}|\\vec{\\bf X}^{g},M)\\;=\\;\\delta_{K}(\\vec{\\bf X}\\;=\\;{\\bf A}_{X}\\vec{\\bf X}^{g})$ , and $Q(\\vec{\\bf Y}^{g}|M){=}\\mathrm{Poisson}(d_{\\vec{\\bf A}_{X}},1,\\vec{\\bf A}_{\\bf Y})$ . Note that $Q(\\vec{\\bf X}|M){=}\\mathrm{Poisson}(d_{X},d_{X}^{\\prime},\\vec{\\bf A}_{X})$ by construction, and we derive ${\\cal Q}(\\vec{\\bf X}^{g}|\\vec{\\bf X},M)$ through the Bayes theorem. Here, $\\delta_{K}(\\cdot)$ is the Kronecker delta function [59]. By lemma 17, we know that ${\\cal Q}(\\vec{\\bf X}^{g}|\\vec{\\bf X},M)={\\cal Q}(\\vec{\\bf X}^{g}|\\vec{\\bf X})$ , and hence we have the Markov chain $M\\rightarrow\\vec{\\mathbf{X}}\\rightarrow\\vec{\\mathbf{X}}^{g}$ . ", "page_idx": 52}, {"type": "text", "text": "For choosing $Q(\\vec{\\mathbf{Y}}^{g}|\\vec{\\mathbf{X}}^{g})$ , we rely on the result of lemma 18. Let us define the random vectors: ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\vec{\\bf X}_{j}\\!=\\left[X_{i_{1}...i_{j}}^{g}\\right]_{(i_{1},...,i_{j})\\in\\mathbb{A}_{j}^{d_{1}}}^{T}\\,,\\;\\mathrm{and}\\;\\vec{\\bf Y}_{j}\\!=\\left[Y_{i_{1}...i_{j}}^{g}\\right]_{(i_{1},...,i_{j})\\in\\mathbb{A}_{j}^{d_{2}}}^{T}\\,,\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "i.e., \u20d7Xj and \u20d7Yj are random vectors containing all terms of the form Xig1,...,ij, and Y kg1,...,kj, where $(i_{1},\\ldots,i_{j})\\;\\;\\in\\;\\;\\mathbb{A}_{j}^{d_{1}}$ and $(k_{1},\\ldots,k_{j})\\;\\;\\in\\;\\;\\mathbb{A}_{j}^{d_{2}}$ , respectively. Note that we can write \u20d7Xg= \u20d7X1T . . . \u20d7XdT\u2032 T, and we define $\\vec{\\bf Y}^{g}\\;=\\;\\left[\\vec{\\bf Y}_{1}^{T}\\,\\ldots\\,\\vec{\\bf Y}_{d_{2}^{\\prime}}^{T}\\right]^{T}$ . Then, we construct $Q(\\vec{\\mathbf{Y}}^{g}|\\vec{\\mathbf{X}}^{g})$ as a product of $d_{Y}^{\\prime}$ multinomial distributions, described below: ", "page_idx": 52}, {"type": "equation", "text": "$$\nQ(\\vec{\\bf Y}^{g}|\\vec{\\bf X}^{g})=\\prod_{j=1}^{d_{Y}^{\\prime}}Q(\\vec{\\bf Y}_{j}|\\vec{\\bf X}_{j}),\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "where $Q({\\vec{\\mathbf{Y}}}_{j}\\!=\\!{\\vec{\\mathbf{y}}}_{j}|{\\vec{\\mathbf{X}}}_{j}\\!=\\!{\\vec{\\mathbf{x}}}_{j})={\\mathrm{Multinomial}}(\\mathbf{k}_{j};N_{j},\\mathbf{p}_{i})$ , and ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{N_{j}=\\vec{\\bf1}^{T}\\vec{\\bf x}_{j},\\,\\vec{\\bf k}_{j}=\\left[\\vec{\\bf y}_{j}^{T}\\,\\;\\;\\,\\vec{\\bf1}^{T}\\vec{\\bf x}_{j}-\\vec{\\bf1}^{T}\\vec{\\bf y}_{j}\\right]^{T},}\\\\ &{\\vec{\\bf p}_{i}=\\left[p_{1\\dots j}\\,\\cdots\\,p_{d_{2}-(j-1)\\dots d_{2}}\\;1-\\sum_{(i_{1},\\dots,i_{j})\\in\\mathbb{A}_{j}^{d_{2}}}p_{i_{1}\\dots i_{j}}\\right]^{T},}\\\\ &{p_{i_{1}\\dots i_{j}}=\\frac{\\gamma_{i_{1}\\dots i_{j}}^{Y}}{\\sum_{(i_{1},\\dots,i_{j})\\in\\mathbb{A}_{j}^{d_{1}}}\\gamma_{i_{1}\\dots i_{j}}^{X}}\\,\\forall\\,(i_{1},\\dots,i_{j})\\in\\mathbb{A}_{j}^{d_{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "By construction $\\vec{\\bf X}^{g}$ consists of mutually conditionally independent Poisson random variables (conditioned on $M$ ), and the condition of lemma 18 (specified in (219)) is satisfied by condition 1. Therefore, using the result of lemma 18 on the Markov chain $M\\rightarrow\\vec{\\mathbf{X}}^{g}\\rightarrow\\vec{\\mathbf{Y}}^{g}$ (obtained by marginalizing $\\vec{\\bf X}$ ), we conclude that $Q(\\vec{\\bf Y}^{g}|M)=\\mathrm{Poisson}(1,d_{\\vec{\\bf A}_{Y}},\\vec{\\bf A}_{Y})$ , where $d_{\\vec{\\Lambda}_{Y}}$ is the dimension of $\\mathbf{\\bar{A}}_{Y}$ . ", "page_idx": 52}, {"type": "text", "text": "We specify $Q(\\vec{\\mathbf{Y}}|\\vec{\\mathbf{Y}}^{g})$ through the following deterministic transformation $\\vec{\\mathbf{Y}}=\\mathbf{A}_{Y}\\vec{\\mathbf{Y}}^{g}$ to obtain the Markov chain $M\\to{\\vec{\\mathbf{X}}}\\to{\\vec{\\mathbf{X}}}^{g}\\to{\\vec{\\mathbf{Y}}}^{g}\\to{\\vec{\\mathbf{Y}}}^{\\vec{\\mathbf{\\mu}}}$ . Marginalizing $\\vec{\\bf X}$ and $\\vec{\\bf X}^{g}$ in the above Markov chain, we get the following Markov chain: $M\\rightarrow\\vec{\\mathbf{Y}}^{g}\\rightarrow\\vec{\\mathbf{Y}}$ . Now, since $\\vec{\\bf Y}={\\bf A}_{Y}\\vec{\\bf Y}^{g}$ and $P(\\vec{\\bf Y}^{g}|M)=$ Poisson $(d_{\\vec{\\mathbf{A}}_{Y}},1,\\vec{\\mathbf{A}}_{Y})$ , $P(\\vec{\\bf Y}|M)=\\mathrm{Poisson}(d_{Y},d_{Y}^{\\prime},\\Lambda_{Y})$ (by definition of the multivariate Poisson discussed in Sec. J.1). Since we have $Q(M)=P(M)$ and $Q({\\vec{\\mathbf{Y}}}|M)=P({\\vec{\\mathbf{Y}}}|M)$ , we also have $Q(M,{\\vec{\\mathbf{Y}}})\\,=\\,P(M,{\\vec{\\mathbf{Y}}})$ . Marginalizing $\\vec{\\bf X}^{g}$ and $\\vec{\\bf Y}^{g}$ from the Markov chain $M\\\\to{\\vec{\\mathbf{X}}}\\to{\\vec{\\mathbf{X}}}^{g}\\to$ $\\vec{\\bf Y}^{g}\\,\\rightarrow\\,\\vec{\\bf Y}$ results in the desired Markov chain $M\\rightarrow\\vec{\\mathbf{X}}\\rightarrow\\vec{\\mathbf{Y}}$ having $Q(M,\\vec{\\bf X}){=}P(M,\\vec{\\bf X})$ and $Q(M,{\\vec{\\mathbf{Y}}}){=}P(M,{\\vec{\\mathbf{Y}}})$ , concluding our proof. \u53e3 ", "page_idx": 52}, {"type": "text", "text": "", "page_idx": 53}, {"type": "text", "text": "Lemma 15. If $N_{j}\\sim P o i s s o n(\\lambda_{j})\\;\\forall\\;j\\;\\in\\;\\{1,\\ldots,d\\}$ , and all $N_{j}$ \u2019s are mutually independent, then $P(N_{1},\\ldots,N_{j}|\\sum_{j=1}^{d}N_{j}=N)=M u l t i n o m i a l(N_{1},\\ldots,N_{j};N,\\left[\\lambda_{1}/\\sum_{i=1}^{d}\\lambda_{j}\\right])$ . . . $\\lambda_{d}{\\Big/}{\\textstyle\\sum_{i=1}^{d}\\lambda_{j}}{\\Big]}$ ). ", "page_idx": 53}, {"type": "text", "text": "Proof. See Section 3 of [60]. ", "page_idx": 53}, {"type": "text", "text": "Lemma 16. Let $\\vec{\\bf N}_{1}=\\left[N_{1}^{(1)}\\right.\\left.\\ .\\ .\\ N_{d_{1}}^{(1)}\\right]^{T}$ , where $N_{j}\\sim P o i s s o n(\\lambda_{j}^{(1)})\\;\\forall\\;j\\in\\{1,\\ldots,d_{1}\\}$ and all $N_{j}$ \u2019s are mutually independent from each other. Define $P(\\vec{\\bf N}_{2}|\\vec{\\bf N}_{1})=M u l t i n o m$ ial $\\left(\\vec{\\bf k};N,\\vec{\\bf p}\\right)$ , where ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{N=\\displaystyle\\sum_{j=1}^{d_{1}}N_{j}^{(1)},\\;\\vec{\\mathbf{k}}=\\left[N_{1}^{(2)}\\,\\cdot\\,\\cdot\\,\\cdot\\,\\,N_{d_{2}}^{(2)}\\,\\sum_{j=1}^{d_{1}}N_{j}^{(1)}-\\sum_{j=1}^{d_{2}}N_{j}\\right]^{T},}\\\\ &{\\vec{\\mathbf{p}}=\\left[\\frac{\\lambda_{1}^{(2)}}{\\sum_{j=1}^{d_{1}}\\lambda_{j}^{(1)}}\\,\\cdot\\,\\cdot\\,\\cdot\\,\\frac{\\lambda_{1}^{(2)}}{\\sum_{j=1}^{d_{1}}\\lambda_{j}^{(1)}}\\,\\,1-\\frac{\\sum_{j=1}^{d_{2}}\\lambda_{j}^{(2)}}{\\sum_{j=1}^{d_{1}}\\lambda_{j}^{(1)}}\\right]^{T},}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "where $\\{\\lambda_{j}^{(2)}\\}_{j=1}^{d_{2}}$ are some positive constants, i.e., $\\lambda_{j}^{(2)}\\in(0,\\infty)$ , such that $\\begin{array}{r}{\\sum_{i=1}^{d_{2}}\\lambda_{j}^{(2)}\\le\\sum_{j=1}^{d_{1}}\\lambda_{j}^{(1)}}\\end{array}$ . Then, $\\vec{\\bf N}_{2}=\\left[N_{1}^{(2)}\\right.\\,\\left.\\dots N_{d_{2}}^{(2)}\\right]^{T}$ , where $N_{j}^{(2)}\\sim P o i s s o n(\\lambda_{j}^{(2)})$ , and all $N_{j}^{(2)}{}^{,}$ are mutually independent form each other. ", "page_idx": 53}, {"type": "text", "text": "Proof. Lemma 16 is a straight-forward generalization of lemma 15. Denote the probability of $\\vec{\\bf N}_{2}$ shown in the lemma statement as $P(\\bar{\\bf N}_{2})$ . We will prove the above lemma by constructing a Markov chain $\\vec{\\bf N}_{1}\\rightarrow N_{1}^{\\prime}\\rightarrow N_{2}^{\\prime}\\rightarrow\\vec{\\bf N}_{2}$ with joint density ${\\cal Q}(\\vec{\\bf N}_{1},N_{1}^{\\prime},N_{2}^{\\prime},\\vec{\\bf N}_{2})$ , and showing that ${\\cal Q}(\\vec{\\bf N}_{2})={\\cal P}(\\vec{\\bf N}_{2})$ and $Q(\\vec{\\bf N}_{2}|\\vec{\\bf N}_{1})=P(\\vec{\\bf N}_{2}|\\vec{\\bf N}_{1})$ . ", "page_idx": 53}, {"type": "text", "text": "Specifying $Q(N_{1}^{\\prime}|\\vec{\\mathbf{N}}_{1})$ : We specify $Q(N_{1}^{\\prime}|\\vec{\\mathbf{N}}_{1})$ through the following deterministic transformation: ", "page_idx": 53}, {"type": "equation", "text": "$$\nN_{1}^{\\prime}=\\vec{\\bf1}^{T}\\vec{\\bf N}_{1}=\\sum_{j=1}^{d_{1}}N_{j}^{(1)}.\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Calculating $Q(N_{1}^{\\prime})$ : We use the property that the sum of independent Poisson random variables is a Poisson random variable with its rate parameter being the sum of the rate parameters of its summands [61]. Hence, $\\begin{array}{r}{Q(N_{1}^{\\prime})=\\mathrm{Poisson}(\\sum_{j=1}^{d_{1}}\\lambda_{j}^{(1)})}\\end{array}$ . ", "page_idx": 53}, {"type": "text", "text": "Specifying $Q(N_{2}^{\\prime}|N_{1}^{\\prime})$ : We specify $Q(N_{2}^{\\prime}|N_{1}^{\\prime}=n_{1}^{\\prime})$ as follows: ", "page_idx": 53}, {"type": "equation", "text": "$$\nQ(N_{2}^{\\prime}|N_{1}^{\\prime}=n_{1}^{\\prime})=\\mathrm{Binomial}\\left(N_{2}^{\\prime};n_{1}^{\\prime},\\sum_{j=1}^{d_{2}}\\lambda_{j}^{(2)}\\big/{\\sum_{j=1}^{d_{1}}\\lambda_{j}^{(1)}}\\right).\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Calculating $Q(N_{2}^{\\prime})$ : We use lemma 15 to calculate $Q(N_{2}^{\\prime})$ . Note that $\\begin{array}{r}{Q(N_{1}^{\\prime})=\\mathrm{Poisson}(\\sum_{j=1}^{d_{1}}\\lambda_{j}^{(1)})}\\end{array}$ , and $Q(N_{2}^{\\prime}|N_{1}^{\\prime})=\\mathrm{Binomial}\\left(N_{2}^{\\prime};n_{1}^{\\prime},\\sum_{j=1}^{d_{2}}\\lambda_{j}^{(2)}\\Big/{\\sum_{j=1}^{d_{1}}\\lambda_{j}^{(1)}}\\right)$ . Hence, by lemma 15, we have: ", "page_idx": 53}, {"type": "equation", "text": "$$\nQ(N_{2}^{\\prime})=\\mathrm{Poisson}\\left(\\sum_{j=1}^{d_{1}}\\lambda_{j}^{(1)}\\times\\frac{\\sum_{j=1}^{d_{2}}\\lambda_{j}^{(2)}}{\\sum_{j=1}^{d_{1}}\\lambda_{j}^{(1)}}\\right)=\\mathrm{Poisson}\\left(\\sum_{j=1}^{d_{2}}\\lambda_{j}^{(2)}\\right).\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Specifying $Q(N_{2}^{\\prime}|N_{1}^{\\prime})$ : We specify $Q(\\vec{\\bf N}_{2}|N_{2}^{\\prime}=n_{2}^{\\prime})$ as follows: ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Q(\\vec{\\bf N}_{2}|N_{2}^{\\prime}=n_{2}^{\\prime})=\\mathrm{Multinomial}\\left(\\vec{\\bf N}_{2}^{\\prime};n_{2}^{\\prime},\\left[\\frac{\\lambda_{1}^{(2)}}{\\sum_{j=1}^{d_{2}}\\lambda_{j}^{(2)}}\\quad\\cdot\\cdot\\cdot\\quad\\frac{\\lambda_{d_{2}}^{(2)}}{\\sum_{j=1}^{d_{2}}\\lambda_{j}^{(2)}}\\right]^{T}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Calculating $Q(\\vec{\\bf N}_{2})$ : We again use lemma 15 to calculate $Q(\\vec{\\bf N}_{2})$ . Note that, ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal Q}(N_{2}^{\\prime})=\\mathrm{Poisson}\\left(\\sum_{j=1}^{d_{2}}\\lambda_{j}^{(2)}\\right),\\,\\,\\mathrm{and}}}\\\\ {{\\displaystyle{\\cal Q}(\\vec{\\bf N}_{2}|N_{2}^{\\prime}=n_{2}^{\\prime})=\\mathrm{Multinomial}\\left(\\vec{\\bf N}_{2}^{\\prime};n_{2}^{\\prime},\\left[\\frac{\\lambda_{1}^{(2)}}{\\sum_{j=1}^{d_{2}}\\lambda_{j}^{(2)}}\\quad\\cdot\\cdot\\cdot\\quad\\frac{\\lambda_{d_{2}}^{(2)}}{\\sum_{j=1}^{d_{2}}\\lambda_{j}^{(2)}}\\right]^{T}\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Hence, by lemma 15, we have: ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\vec{\\bf N}_{2}=\\left[N_{1}^{(2)}\\quad.\\dots\\quad N_{d_{2}}^{(2)}\\right]^{T},\\,\\,\\mathrm{where}\\,\\,N_{j}^{(2)}\\sim\\mathrm{Poisson}(\\lambda_{j}^{(2)})\\,\\forall\\,j\\in[d_{2}].\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Hence, by (202), we know that ${\\cal Q}(\\vec{\\bf N}_{2})={\\cal P}(\\vec{\\bf N}_{2})$ . ", "page_idx": 54}, {"type": "text", "text": "Calculating $Q(\\vec{\\bf N}_{2}|\\vec{\\bf N}_{1})$ : ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q(\\vec{\\bf N}_{2}=\\vec{\\bf n}_{2}|\\vec{\\bf N}_{1}=\\vec{\\bf n}_{1})=\\displaystyle\\sum_{n_{1}^{\\prime},n_{2}^{\\prime}=0}^{\\infty}Q(N_{1}^{\\prime}=n_{1}^{\\prime},N_{2}^{\\prime}=n_{2}^{\\prime},\\vec{\\bf N}_{2}=\\vec{\\bf n}_{2}|\\vec{\\bf N}_{1}=\\vec{\\bf n}_{1}),}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\overset{(a)}{=}\\displaystyle\\sum_{n_{1}^{\\prime},n_{2}^{\\prime}=0}^{\\infty}Q(n_{1}^{\\prime}|\\vec{\\bf n}_{2}^{\\prime})Q(n_{2}^{\\prime}|n_{1}^{\\prime})Q(\\vec{\\bf n}_{2}|n_{2}^{\\prime}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "where $(a)$ is due to the Markovian nature of ${\\cal Q}(\\vec{\\bf N}_{1},N_{1}^{\\prime},N_{2}^{\\prime},\\vec{\\bf N}_{2})$ . We drop the random variable notation for brevity. Let \u20d7n1 = n(11) $\\vec{\\bf n}_{1}=\\left[n_{1}^{(1)}\\quad.\\dots\\quad n_{d_{1}}^{(1)}\\right]^{T}$ , and $\\vec{\\bf n}_{2}=\\left[n_{1}^{(2)}\\quad.\\dots\\quad n_{d_{2}}^{(2)}\\right]^{T}$ . Then, substituting the exact form of $Q(n_{1}^{\\prime}|\\vec{\\bf n}_{1})\\overline{{,}}Q(n_{2}^{\\prime}|n_{1}^{\\prime})$ , and $\\bar{Q}(\\vec{\\bf n}_{2}|n_{2}^{\\prime})$ in the above equation: ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle=\\sum_{n_{1}^{\\prime},n_{2}^{\\prime}=0}^{\\infty}\\mathbb{I}\\left[n_{1}^{\\prime}=\\sum_{j=1}^{d_{1}}n_{j}^{(1)}\\right]\\frac{(n_{1}^{\\prime})!}{(n_{1}^{\\prime}-n_{2}^{\\prime})!(n_{2}^{\\prime})!}\\left(\\frac{\\sum_{j=1}^{d_{2}}\\lambda_{j}^{(2)}}{\\sum_{j=1}^{d_{1}}\\lambda_{j}^{(1)}}\\right)^{n_{2}^{\\prime}}\\left(1-\\frac{\\sum_{j=1}^{d_{2}}\\lambda_{j}^{(2)}}{\\sum_{j=1}^{d_{1}}\\lambda_{j}^{(1)}}\\right)^{n_{1}^{\\prime}-n_{2}^{\\prime}}\\times},}}\\\\ {{\\displaystyle\\mathbb{I}\\left[\\sum_{j=1}^{d_{2}}n_{j}^{(2)}=n_{2}^{\\prime}\\right]\\frac{(n_{2}^{\\prime})!}{\\prod_{j=1}^{d_{2}}(n_{j}^{(2)})!}\\prod_{j=1}^{d_{2}}\\left(\\frac{\\lambda_{j}^{(2)}}{\\sum_{j=1}^{d_{2}}\\lambda_{j}^{(2)}}\\right)^{n_{j}^{(2)}},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "where $\\mathbb{I}[\\cdot]$ is an indicator function. Simplifying the above equation by using the fact that only non-zero term in the above summation is $\\begin{array}{r}{n_{1}^{\\prime}=\\sum_{j=1}^{d_{1}}n_{j}^{(1)}}\\end{array}$ and $\\begin{array}{r}{n_{2}^{\\prime}=\\sum_{j=1}^{d_{1}}n_{j}^{(2)}}\\end{array}$ . ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\frac{\\left(\\sum_{j=1}^{d_{1}}n_{j}^{(1)}\\right)!}{\\left(\\sum_{j=1}^{d_{1}}n_{j}^{(1)}-\\sum_{j=1}^{d_{2}}n_{j}^{(2)}\\right)!\\left(\\sum_{j=1}^{d_{2}}n_{j}^{(2)}\\right)!}\\left(\\frac{\\sum_{j=1}^{d_{2}}\\lambda_{j}^{(2)}}{\\sum_{j=1}^{d_{1}}\\lambda_{j}^{(1)}}\\right)^{\\sum_{j=1}^{d_{2}}n_{j}^{(2)}}\\times}\\\\ &{\\left(1-\\frac{\\sum_{j=1}^{d_{2}}\\lambda_{j}^{(2)}}{\\sum_{j=1}^{d_{1}}\\lambda_{j}^{(1)}}\\right)^{\\sum_{j=1}^{d_{1}}n_{j}^{(1)}-\\sum_{j=1}^{d_{1}}n_{j}^{(2)}}\\frac{\\left(\\sum_{j=1}^{d_{2}}n_{j}^{(2)}\\right)!}{\\prod_{j=1}^{d_{2}}(n_{j}^{(2)})!}\\prod_{j=1}^{d_{2}}\\left(\\frac{\\lambda_{j}^{(2)}}{\\sum_{j=1}^{d_{2}}\\lambda_{j}^{(2)}}\\right)^{n_{j}^{(2)}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Collecting all factorial terms together, and rearranging some terms: ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\!\\frac{\\left(\\sum_{j=1}^{d_{1}}n_{j}^{(1)}\\right)!}{\\left(\\sum_{j=1}^{d_{1}}n_{j}^{(1)}-\\sum_{j=1}^{d_{2}}n_{j}^{(2)}\\right)!\\prod_{j=1}^{d_{2}}(n_{j}^{(2)})!}\\left(1-\\frac{\\sum_{j=1}^{d_{2}}\\lambda_{j}^{(2)}}{\\sum_{j=1}^{d_{1}}\\lambda_{j}^{(1)}}\\right)^{\\sum_{j=1}^{d_{1}}n_{j}^{(1)}-\\sum_{j=1}^{d_{2}}n_{j}^{(2)}}\\times}\\\\ &{\\!\\!\\!\\!\\!\\!\\left(\\frac{\\sum_{j=1}^{d_{2}}\\lambda_{j}^{(2)}}{\\sum_{j=1}^{d_{1}}\\lambda_{j}^{(1)}}\\right)^{\\sum_{j=1}^{d_{2}}n_{j}^{(2)}}\\displaystyle\\prod_{j=1}^{d_{2}}\\left(\\frac{\\lambda_{j}^{(2)}}{\\sum_{j=1}^{d_{2}}\\lambda_{j}^{(2)}}\\right)^{n_{j}^{(2)}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Substituting $\\begin{array}{r}{\\left(\\sum_{j=1}^{d_{2}}\\lambda_{j}^{(2)}\\big/\\sum_{j=1}^{d_{1}}\\lambda_{j}^{(1)}\\right)^{\\sum_{j=1}^{d_{2}}n_{j}^{(2)}}=\\prod_{j=1}^{d_{2}}\\left(\\sum_{j=1}^{d_{2}}\\lambda_{j}^{(2)}\\big/{\\sum_{j=1}^{d_{1}}\\lambda_{j}^{(1)}}\\right)^{n_{j}^{(2)}}}\\end{array}$ ) =  jd2=1  jd2=1 \u03bb(j2 )/ jd1=1 \u03bb(j1) n(j2)in the above equation: ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\begin{array}{r l}&{=(\\frac{\\sum_{j=1}^{m}\\lambda_{j}\\psi_{j}^{(1)}}{\\sum_{j=1}^{m}\\psi_{j}^{(1)}-\\sum_{j=1}^{m}\\psi_{j}^{(2)}})!\\prod_{j=1}^{m}\\left(n_{j}^{2}!\\right)^{\\frac{n-j}{2}}}\\\\ &{\\frac{\\sinh{\\left(\\sum_{j=1}^{m}\\lambda_{j}\\psi_{j}^{(1)}\\right)}^{2}}{\\prod_{j=1}^{m}\\sum_{j=1}^{m}\\lambda_{j}^{(1)}}\\Biggr|\\prod_{h=1}^{m}\\left(\\sum_{j=1}^{m}\\lambda_{j}^{(1)}\\right)}\\end{array}}\\\\ &{\\begin{array}{r l}&{\\frac{\\sinh{\\left(\\sum_{j=1}^{m}\\lambda_{j}\\psi_{j}^{(2)}\\right)}^{2}}{\\prod_{j=1}^{m}\\sum_{j=1}^{m}\\lambda_{j}^{(1)}}\\Biggr|\\begin{array}{r l}&{\\mathrm{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\}}\\\\ {\\frac{\\sinh{\\left(\\sum_{j=1}^{m}\\lambda_{j}\\psi_{j}^{(1)}\\right)}^{2}}{\\prod_{j=1}^{m}\\left(\\sum_{j=1}^{m}\\lambda_{j}^{(1)}\\right)^{2}}}\\\\ &{\\frac{\\sinh{\\left(\\sum_{j=1}^{m}\\lambda_{j}\\psi_{j}^{(2)}\\right)}^{2}}{\\prod_{j=1}^{m}\\left(\\sum_{j=1}^{m}\\lambda_{j}^{(1)}\\right)^{2}}\\Biggr|\\prod_{h=1}^{m}\\left(\\frac{\\sum_{j=1}^{m}\\lambda_{j}^{(2)}}{\\sum_{j=1}^{m}\\lambda_{j}^{(1)}}\\right)^{\\frac{n-j}{2}}}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ }\\\\ &{\\frac{\\sinh{\\left(\\sum_{j=1}^{m}\\lambda_{j}^{(1)}\\right)}^{2}}{\\prod_{j=1}^{m}\\sum_{j=1}^\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "By inspecting (203), we can see that $Q(\\vec{\\mathbf{N}}_{2}|\\vec{\\mathbf{N}}_{1})=\\mathbf{Multinomial}(\\vec{\\mathbf{k}};N,\\vec{\\mathbf{p}})$ with, ", "page_idx": 55}, {"type": "equation", "text": "$$\n{\\cal N}=\\sum_{j=1}^{d_{1}}N_{j}^{(1)},\\;\\vec{\\bf k}=\\left[N_{1}^{(2)}\\,\\cdot\\cdot\\cdot\\,\\,N_{d_{2}}^{(2)}\\,\\sum_{j=1}^{d_{1}}N_{j}^{(1)}-\\sum_{j=1}^{d_{2}}N_{j}\\right]^{T},\n$$", "text_format": "latex", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\vec{\\bf p}=\\left[\\lambda_{1}^{(2)}/\\sum_{j=1}^{d_{1}}\\lambda_{j}^{(1)}\\,\\cdot\\,\\cdot\\,\\cdot\\,\\,\\lambda_{1}^{(2)}/\\sum_{j=1}^{d_{1}}\\lambda_{j}^{(1)}\\,\\,1-\\sum_{j=1}^{d_{2}}\\lambda_{j}^{(2)}/\\sum_{j=1}^{d_{1}}\\lambda_{j}^{(1)}\\right]^{T},}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Hence, by (205), we can conclude that $Q(\\vec{\\bf N}_{2}|\\vec{\\bf N}_{1})=P(\\vec{\\bf N}_{2}|\\vec{\\bf N}_{1})$ finishing our proof. ", "page_idx": 55}, {"type": "text", "text": "Lemma 17. Let $\\vec{\\bf N}$ be a $d_{\\cdot}$ -dimensional vector and $M$ be a positive random variable, i.e. $M\\sim$ $P(M),P(M\\leq0)=0$ . Let $P(\\vec{\\bf N}|M)=P o i s s o n(d,d^{\\prime},\\vec{\\bf\\Lambda}),$ , where: ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\vec{\\Lambda}=\\left[\\lambda_{1}\\,\\dots\\,\\lambda_{d}\\,\\,\\lambda_{12}\\,\\,.\\dots\\lambda_{d-(d^{\\prime}-1)\\dots d}\\right]^{T},}\\\\ &{\\lambda_{i_{1}\\dots i_{j}}=\\gamma_{i_{1}\\dots i_{j}}M^{j}\\,\\forall\\,j\\in[d^{\\prime}]\\,a n d\\,\\big(i_{1},\\dots,i_{d^{\\prime}}\\big)\\in\\mathbb{A}_{j}^{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Let $\\begin{array}{r l r}{\\vec{\\bf N}^{g}}&{{}=}&{\\left[N_{1}^{g}\\;.\\;.\\;.\\;\\;N_{d}^{g}\\;N_{12}^{g}\\;.\\;.\\;.\\;\\;N_{(d-1)d}^{g}\\;.\\;.\\;.\\;\\;N_{d-(d^{\\prime}-1)...d}^{g}\\right]^{T}}\\end{array}$ where $\\begin{array}{r l}{P(\\vec{\\bf N}^{g}|M)}&{{}\\;=}\\end{array}$ $P o i s s o n(d_{\\vec{\\mathbf{A}}},1,\\vec{\\mathbf{\\Lambda}})$ and $\\vec{\\bf N}\\;=\\;{\\bf A}\\vec{\\bf N}^{g}$ , where $d_{\\Lambda}$ is the dimension of $\\vec{\\pmb{\\Lambda}}_{;}$ , and $\\mathbf{A}\\ =\\ [\\mathbf{A}_{1}\\ldots\\mathbf{A}_{d^{\\prime}}]$ as defined in Sec. J.1. Then $P(\\vec{\\bf N}^{g}|\\vec{\\bf N},M)\\,=\\,P(\\vec{\\bf N}^{g}|\\vec{\\bf N})$ , i.e. $M,\\vec{\\bf N}^{g}$ and $\\vec{\\bf N}$ form the following Markov chain $M\\rightarrow\\vec{\\mathbf{N}}\\rightarrow\\vec{\\mathbf{N}}^{g}$ . ", "page_idx": 55}, {"type": "text", "text": "Proof. To prove lemma 17, all we need to show is that $P(\\vec{\\bf N}^{g}|\\vec{\\bf N},M)$ does not depend on $M$ . Let us to calculate $P(\\vec{\\bf N}^{g}|\\vec{\\bf N},M)$ . By Bayes Theorem, we know: ", "page_idx": 55}, {"type": "equation", "text": "$$\nP(\\vec{\\bf N}^{g}|\\vec{\\bf N},M)=\\frac{P(\\vec{\\bf N}^{g}|M)P(\\vec{\\bf N}|\\vec{\\bf N}^{g},M)}{P(\\vec{\\bf N}|M)}.\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Let us now write the expression for $P(\\vec{\\bf N}|M)$ using (196): ", "page_idx": 55}, {"type": "equation", "text": "$$\nP(\\vec{\\bf N}\\!=\\!\\vec{\\bf n}|M)\\!=\\!Q(\\vec{\\bf n},\\vec{\\bf n}^{\\prime})e^{-\\vec{\\bf1}^{T}\\tilde{\\Lambda}}\\prod_{i=1}^{d}(\\gamma_{i}M)^{n_{i}}\\left(\\sum_{\\vec{\\bf n^{\\prime}}\\in C}\\prod_{j=2}^{d^{\\prime}}\\prod_{(i_{1},\\dots,i_{j})\\in{\\cal k}_{j}^{d}}\\left(\\frac{\\gamma_{i_{1}\\dots i_{j}}M^{j}}{\\prod_{l=1}^{j}\\gamma_{i_{l}}M}\\right)^{n_{i_{1}\\dots i_{j}}^{\\prime}}\\right).\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Canceling $M$ in the above equation for the terms inside the summation, we get: ", "page_idx": 56}, {"type": "equation", "text": "$$\nP(\\vec{\\bf N}\\!=\\!\\vec{\\bf n}|M)\\!=\\!Q(\\vec{\\bf n},\\vec{\\bf n}^{\\prime})e^{-\\vec{\\bf1}^{T}\\vec{\\Lambda}}\\prod_{i=1}^{d}(\\gamma_{i}M)^{n_{i}}\\left(\\sum_{\\vec{\\bf n^{\\prime}}\\in C}\\prod_{j=2}^{d^{\\prime}}\\prod_{(i_{1},\\dots,i_{j})\\in\\mathbb{A}_{j}^{d}}\\left(\\frac{\\gamma_{i_{1}\\dots i_{j}}}{\\prod_{l=1}^{j}\\gamma_{i_{l}}}\\right)^{n_{i_{1}\\dots i_{j}}^{\\prime}}\\right).\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Absorbing all terms that do not depend upon $M$ into $B$ , we have ", "page_idx": 56}, {"type": "equation", "text": "$$\nB=\\sum_{\\vec{\\bf n^{\\prime}}\\in C}\\prod_{j=2}^{d^{\\prime}}\\prod_{(i_{1},\\dots,i_{j})\\in\\mathbb{A}_{j}^{d}}\\left(\\frac{\\gamma_{i_{1}\\dots i_{j}}}{\\prod_{l=1}^{j}\\gamma_{i_{l}}}\\right)^{n_{i_{1}\\dots i_{j}}^{\\prime}},.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Then, we can rewrite (209) as: ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{P(\\vec{\\bf N}\\!=\\!\\vec{\\bf n}|M)=B e^{-\\vec{\\bf1}^{T}\\vec{\\Lambda}}\\displaystyle\\prod_{i=1}^{d}(\\gamma_{i}M)^{n_{i}}=B e^{-\\vec{\\bf1}^{T}\\vec{\\Lambda}}M^{\\sum_{i=1}^{d}n_{i}}\\displaystyle\\prod_{i=1}^{d}(\\gamma_{i})^{n_{i}}}}\\\\ {{\\overset{(a)}{=}B e^{-\\vec{\\bf1}^{T}\\vec{\\Lambda}}M^{\\sum_{i=1}^{d}n_{i}}\\overset{(b)}{=}B e^{-\\vec{\\bf1}^{T}\\vec{\\Lambda}}M^{\\vec{\\bf1}^{T}\\vec{\\bf n}},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "where, in (a) we further absorb $\\prod_{i=1}^{d}(\\gamma_{i})^{n_{i}}$ into $B$ since it does not depend upon $M$ , and in (b) we substitute $\\textstyle\\sum_{i=1}^{d}n_{i}={\\vec{\\mathbf{1}}}^{T}{\\vec{\\mathbf{n}}}$ . Similarly, let us write out the expression for $P(\\vec{\\bf N}^{g}|M)$ : ", "page_idx": 56}, {"type": "equation", "text": "$$\nP(\\vec{\\bf N}^{g}\\!=\\!\\vec{\\bf n}^{g}|M)=e^{-\\vec{\\bf1}^{T}\\vec{\\Lambda}}\\prod_{j=1}^{d^{\\prime}}\\prod_{(i_{1},\\dots,i_{j})\\in\\mathbb{A}_{j}^{d}}(\\gamma_{i_{1}\\dots i_{j}}M^{j})^{n_{i_{1}\\dots i_{j}}^{g}}.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Collecting all the $M$ terms, and absorbing all the terms that do not depend upon $M$ into $D$ , we obtain: ", "page_idx": 56}, {"type": "equation", "text": "$$\nP(\\vec{\\bf N}^{g}\\!=\\!\\vec{\\bf n}^{g}|M)=D e^{-\\vec{\\bf1}^{T}\\vec{\\Lambda}}M^{\\sum_{j=1}^{d^{\\prime}}\\sum_{(i_{1},\\dots,i_{j})\\in\\mathbb{A}_{j}^{d}}\\left(j n_{i_{1}\\dots i_{j}}^{g}\\right)}.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Now let us analyze the term $\\vec{\\bf1}^{T}A\\vec{\\bf n}^{g}$ : ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\vec{\\mathbf{1}}^{T}\\mathbf{A}\\vec{\\mathbf{n}}^{g}\\stackrel{(a)}{=}(\\vec{\\mathbf{n}}^{g})^{T}\\mathbf{A}^{T}\\vec{\\mathbf{1}}\\stackrel{(b)}{=}(\\vec{\\mathbf{n}}^{g})^{T}\\left[\\begin{array}{c}{\\mathbf{\\vec{A}}_{1}^{T}\\vec{\\mathbf{1}}}\\\\ {\\vdots}\\\\ {\\mathbf{\\vec{A}}_{d^{\\prime}}^{T}\\vec{\\mathbf{1}}}\\end{array}\\right]\\stackrel{(c)}{=}(\\vec{\\mathbf{n}}^{g})^{T}\\left[\\begin{array}{c}{\\vec{\\mathbf{1}}}\\\\ {2\\vec{\\mathbf{1}}}\\\\ {\\vdots}\\\\ {\\vdots}\\\\ {d^{\\prime}\\vec{\\mathbf{1}},}\\end{array}\\right],\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "where (a) uses the fact that $\\vec{1}^{T}\\mathbf{A}\\vec{\\mathbf{n}}^{g}$ is a scalar and hence is equal to its transpose, (b) uses the fact $\\mathbf{A}\\!=\\!\\left[\\mathbf{A}_{1}\\,.\\,.\\,.\\,\\mathbf{A}_{d^{\\prime}}\\right]$ , and (c) follows from the special structure of $\\mathbf{A}_{i}$ , i.e., that each column only contains $i$ ones and $d-i$ zeros, and the fact that ${\\bf A}_{1}^{T}\\vec{\\bf1}$ is akin to summing up the columns, hence $\\mathbf{\\dot{A}}_{i}^{T}\\vec{\\mathbf{1}}=i\\vec{\\mathbf{1}}$ . Equivalently we can rewrite the above equation as: ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\vec{\\bf1}^{T}{\\bf A}\\vec{\\bf n}^{g}=\\sum_{j=1}^{d^{\\prime}}\\sum_{(i_{1},\\dots,i_{j})\\in\\mathbb{A}_{j}^{d}}\\left(j n_{i_{1}\\dots i_{j}}^{g}\\right).\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Substituting (214) into (213): ", "page_idx": 56}, {"type": "equation", "text": "$$\nP(\\vec{\\bf N}^{g}|M)=D e^{-\\vec{\\bf1}^{T}\\vec{\\Lambda}}M^{\\vec{\\bf1}^{T}{\\bf A}\\vec{\\bf n}^{g}}.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Now, let us write out the expression for $P(\\vec{\\bf N}|\\vec{\\bf N}^{g},M)$ . Since $\\vec{\\bf N}\\;=\\;{\\bf A}\\vec{\\bf N}^{g}$ , $P(\\vec{\\bf N}|\\vec{\\bf N}^{g})$ can be represented as a Kronecker delta function with the condition $\\vec{\\bf N}={\\bf A}\\vec{\\bf N}^{g}$ , i.e., ", "page_idx": 56}, {"type": "equation", "text": "$$\nP(\\vec{\\bf N}=\\vec{\\bf n}|\\vec{\\bf N}^{g}=\\vec{\\bf n}^{g},M)=P(\\vec{\\bf N}=\\vec{\\bf n}|\\vec{\\bf N}^{g}=\\vec{\\bf n}^{g})=\\delta_{K}\\left(\\vec{\\bf n}={\\bf A}\\vec{\\bf n}^{g}\\right),\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "where $\\delta_{K}(\\cdot)$ is the Kronecker delta function. Substituting (211), (215) and (216) in (207), we get: ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P(\\vec{\\bf N}^{g}|\\vec{\\bf N},M)=\\frac{D e^{-\\vec{\\bf1}^{T}\\vec{\\bf A}}M^{\\vec{\\bf1}^{T}{\\bf A}\\vec{\\bf n}^{g}}\\delta_{K}(\\vec{\\bf n}={\\bf A}\\vec{\\bf n}^{g})}{B e^{-\\vec{\\bf1}^{T}\\vec{\\bf A}}M^{\\vec{\\bf1}^{T}\\vec{\\bf n}}}\\overset{(a)}{=}\\frac{D M^{\\vec{\\bf1}^{T}{\\bf A}\\vec{\\bf n}^{g}}\\delta_{K}(\\vec{\\bf n}=A\\vec{\\bf n}^{g})}{B M^{\\vec{\\bf1}^{T}\\vec{\\bf n}}},}\\\\ &{\\overset{(b)}{=}\\frac{D M^{\\vec{\\bf1}^{T}{\\bf A}\\vec{\\bf n}^{g}}\\delta_{K}(\\vec{\\bf n}={\\bf A}\\vec{\\bf n}^{g})}{B M^{\\vec{\\bf1}^{T}{\\bf A}\\vec{\\bf n}^{g}}}\\overset{(c)}{=}\\frac{D\\delta_{K}(\\vec{\\bf n}={\\bf A}\\vec{\\bf n}^{g})}{B},}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "where we obtain (a) by canceling the term $e^{-\\vec{\\bf1}^{T}\\vec{\\bf A}}$ , (b) by using the fact $\\vec{\\bf n}={\\bf A}\\vec{\\bf n}^{g}$ due to the delta function, and (c) by canceling the term $M^{\\vec{\\mathbf{1}}^{T}\\mathbf{A}\\vec{\\mathbf{n}}^{g}}$ . Since the terms $D,B$ and $\\delta_{K}(\\vec{\\bf n}={\\bf A}\\vec{\\bf n}^{g})$ do not depend upon $M$ , we can conclude $P(\\vec{\\bf N}^{g}|\\vec{\\bf N},M)$ also does not depend upon $M$ , i.e. $P(\\vec{\\bf N}^{g}|\\vec{\\bf N},M)=$ $P(\\vec{\\bf N}^{g}|\\vec{\\bf N})$ , or $M\\rightarrow\\vec{\\mathbf{N}}\\rightarrow\\vec{\\mathbf{N}}^{g}$ . \u53e3 ", "page_idx": 57}, {"type": "text", "text": "Lemma 18. Let $M\\,\\sim\\,P(M)$ have support over $(0,\\infty)$ . Define, $\\vec{\\bf X}\\,=\\,\\left[\\vec{\\bf X}_{1}^{T}\\,\\,.\\,.\\,\\,.\\,\\vec{\\bf X}_{d_{1}}^{T}\\right]^{T}$ and $\\vec{\\bf Y}=\\left[\\vec{\\bf Y}_{1}\\,\\dots\\,\\vec{\\bf Y}_{d_{2}}^{T}\\right]^{T}$ , where $\\vec{\\bf X}_{i}$ and $\\vec{\\bf Y}_{j}$ are random vectors of size $q_{i}\\in\\mathbb{N}$ and $r_{j}\\in\\mathbb{N},$ , respectively, $\\forall\\:i\\in[d_{1}]$ , and $j\\in[d_{2}]$ . Let the conditional distribution of $\\vec{\\bf X}$ and $\\vec{\\bf Y}$ conditioned on $M$ be specified as follows: ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{l}{P\\left(\\vec{\\bf X}|M\\right)=\\displaystyle\\prod_{i=1}^{d_{1}}P\\left(\\vec{\\bf X}_{i}|M\\right),\\,a n d\\,P\\left(\\vec{\\bf Y}|M\\right)=\\displaystyle\\prod_{i=1}^{d_{2}}P\\left(\\vec{\\bf Y}_{i}|M\\right),\\,w h e r e}\\\\ {P\\left(\\vec{\\bf X}_{i}|M\\right)=P o i s s o n\\left(q_{i},1,\\vec{\\Lambda}_{i}^{X}\\right),\\vec{\\Lambda}_{i}^{X}=\\left[\\gamma_{i1}^{X}M^{i}\\quad...\\quad\\gamma_{i q_{i}}^{X}M^{i}\\right]^{T}\\,\\forall\\,i\\in[d_{1}],\\,a n d}\\\\ {P\\left(\\vec{\\bf Y}_{j}|M\\right)=P o i s s o n\\left(r_{j},1,\\vec{\\Lambda}_{j}^{Y}\\right),\\vec{\\Lambda}_{i}^{Y}=\\left[\\gamma_{i1}^{Y}M^{j}\\quad...\\quad\\gamma_{i r_{i}}^{Y}M^{j}\\right]^{T}\\,\\forall\\,j\\in[d_{2}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "If $d_{1}\\geq d_{2}$ and (219) hold ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{q_{i}}\\gamma_{i j}^{X}\\geq\\sum_{j=1}^{r_{i}}\\gamma_{i j}^{Y}\\;\\forall\\;i\\in[d_{2}]\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "then the distribution $\\tilde{Q}(M,\\vec{\\bf X},\\vec{\\bf Y})$ defined in (220) lies in $\\Delta_{P}$ ", "page_idx": 57}, {"type": "equation", "text": "$$\n{\\tilde{Q}}(M,{\\vec{\\mathbf{X}}},{\\vec{\\mathbf{Y}}})=P(M)P({\\vec{\\mathbf{X}}}|M){\\tilde{Q}}({\\vec{\\mathbf{Y}}}|{\\vec{\\mathbf{X}}}),\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "where $\\tilde{Q}(\\vec{\\bf Y}|\\vec{\\bf X})$ is a product of $d_{2}$ multinomial distributions, i.e $\\begin{array}{r}{\\tilde{Q}(\\vec{\\bf Y}|\\vec{\\bf X})=\\prod_{i=1}^{d_{2}}\\tilde{Q}(\\vec{\\bf Y}_{i}|\\vec{\\bf X}_{i})}\\end{array}$ , where $\\tilde{Q}(\\vec{\\bf Y}_{i}|\\vec{\\bf X}_{i})=M u l t i n o m i a l(\\vec{\\bf k}_{i};N_{i},\\vec{\\bf p}_{i})$ and ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{N_{i}=\\displaystyle\\sum_{j=1}^{n_{i}}x_{i j},\\;\\vec{\\mathbf{k}}_{i}=\\left[y_{i1}\\cdot\\cdot\\cdot\\ y_{i m_{i}}\\,\\sum_{j=1}^{n_{i}}x_{i j}-\\sum_{j=1}^{m_{i}}y_{i j}\\right]^{T},}}\\\\ {{\\vec{\\mathbf{p}}_{i}=\\displaystyle\\left[\\frac{\\gamma_{i1}^{Y}}{\\sum_{j=1}^{n_{i}}\\gamma_{i j}^{X}}\\,\\cdot\\cdot\\cdot\\,\\frac{\\gamma_{i m_{i}}^{Y}}{\\sum_{j=1}^{n_{i}}\\gamma_{i j}^{X}}\\,\\,1-\\frac{\\sum_{j=1}^{m_{i}}\\gamma_{i j}^{Y}}{\\sum_{j=1}^{n_{i}}\\gamma_{i j}^{X}}\\right]^{T}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Proof. Lemma 18 follows from a straightforward application of lemma 16. For showing $\\tilde{Q}(M,\\vec{\\bf X},\\vec{\\bf Y})$ specified in (220) lies in $\\Delta_{P}$ , we need to show that $\\tilde{Q}(M,\\vec{\\bf X})\\;=\\;P(M,\\vec{\\bf X})$ and $\\tilde{Q}(M,\\vec{\\bf Y})\\;=\\;$ $P(M,{\\vec{\\mathbf{Y}}})$ . $\\tilde{Q}(M,\\vec{\\bf X})=P(M,\\vec{\\bf X})$ is satisfied trivially by construction of $\\tilde{Q}(M,\\vec{\\bf X},\\vec{\\bf Y})$ . ", "page_idx": 57}, {"type": "text", "text": "Proving $\\tilde{Q}(\\vec{\\bf Y},M)=P(\\vec{\\bf Y},M)$ : ", "page_idx": 57}, {"type": "text", "text": "For showing the second condition, we will explicitly show $\\tilde{Q}(\\vec{\\bf Y}|M)=P(\\vec{\\bf Y}|M)$ which, combined with the fact that $\\tilde{Q}(M)=P(M)$ (by construction), will show $\\tilde{Q}(\\vec{\\bf Y},M)=P(\\vec{\\bf Y},M)$ completing our proof. ", "page_idx": 57}, {"type": "text", "text": "Calculating $\\underline{{\\tilde{Q}}}(\\vec{\\bf Y}|\\vec{\\bf X})$ : From the law of total probability, we have: ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\hat{Q}(\\widetilde{\\mathbf{Y}}|M)-\\sum_{\\mathbf{x}=0}^{F}P(\\widetilde{\\mathbf{X}}=\\widetilde{\\mathbf{x}}|M)\\hat{Q}(\\widetilde{\\mathbf{Y}}|\\widetilde{\\mathbf{X}}=\\mathbf{x})}\\\\ {\\vdots\\;\\;\\;\\;\\;\\;\\;\\;}&{\\displaystyle\\Updownarrow}\\\\ {\\displaystyle\\triangleq\\sum_{i=1}^{F}\\prod_{i=1}^{F}P(\\widetilde{\\mathbf{X}}_{i}=\\widetilde{\\mathbf{x}}|M)\\prod_{i=1}^{F}\\hat{Q}(\\widetilde{\\mathbf{Y}}_{i}|\\widetilde{\\mathbf{X}}_{i}=\\widetilde{\\mathbf{x}}_{i})}\\\\ {\\vdots\\;\\;\\;\\;\\;\\;\\;}&{\\displaystyle\\xrightarrow{\\frac{d}{d}}\\sum_{s=1}^{F}\\prod_{i=1}^{F}P(\\widetilde{\\mathbf{X}}_{i}=\\widetilde{\\mathbf{x}}_{i}|M)\\hat{Q}(\\widetilde{\\mathbf{Y}}_{i}|\\widetilde{\\mathbf{X}}_{i}=\\boldsymbol{\\mathbf{x}}_{i})\\prod_{i=1}^{F}P(\\widetilde{\\mathbf{X}}_{i}=\\widetilde{\\mathbf{x}}_{i}|M)}\\\\ {\\vdots\\;\\;\\;\\;\\;\\;\\;\\;}&{\\displaystyle\\Updownarrow}\\\\ {\\displaystyle\\equiv\\prod_{i=1}^{G}\\left(\\sum_{s=1}^{F}P(\\widetilde{\\mathbf{X}}_{i}=\\widetilde{\\mathbf{x}}_{i}|M)\\hat{Q}(\\widetilde{\\mathbf{Y}}_{i}|\\widetilde{\\mathbf{X}}_{i}=\\boldsymbol{\\mathbf{x}}_{i})\\right)\\prod_{i=1}^{F}\\left(\\sum_{s=1}^{F}P(\\widetilde{\\mathbf{X}}_{i}=\\widetilde{\\mathbf{x}}_{i}|M)\\right)}\\\\ {\\vdots\\;\\;\\;\\;\\;\\;\\;\\;\\;}&{\\displaystyle\\Updownarrow}\\\\ {\\displaystyle\\underset{i=1}^{G}\\prod_{i=1}^{F}\\underbrace{\\sum_{s=1}^{F}P(\\widetilde{\\mathbf{X}}_{i}=\\widetilde{\\mathbf{x}}_{i}|M)\\hat{Q}(\\widetilde{\\mathbf{Y}}_{i}|\\widetilde{\\mathbf{X}}_{i}=\\widetilde{\\mathbf{x}}_{i})}_{:=0\\,\\widetilde{\\mathbf{y}}\\left\\{\\mathbf{x}}_{i}\\right\\}=\\frac{\\displaystyle\\hat{L}_{1}}{\\displaystyle\\sum_{i=1}^{F}\\hat{Q}(\\widetilde{\\mathbf{Y}}_{i}|M)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "where $(a)$ is due to the particular structure of $P(\\vec{\\bf X}|M)$ and $\\tilde{Q}(\\vec{\\bf Y}|\\vec{\\bf X})$ specified in (218) and (220), respectively, $(b)$ is just collecting the terms of $P(\\vec{\\bf X}_{i}|M)$ and $\\tilde{Q}(\\vec{\\mathbf{Y}}_{i}|\\vec{\\mathbf{X}}_{i})$ into a single product, $(c)$ is due to spreading the summation of the components of $\\tilde{\\bf x}$ over their corresponding distribution components, and $(d)$ is due to the fact that sum over all probabilities of all possible $\\tilde{\\bf x}_{i}$ is 1. ", "page_idx": 58}, {"type": "text", "text": "Note that for a fixed $m$ $\\imath,\\;\\,P\\left(\\vec{\\bf X}_{i}|M=m\\right)\\;\\sim\\;\\,\\mathrm{Poisson}\\left(n_{i},1,\\left[\\gamma_{i1}m^{i}\\quad.\\,.\\,.\\quad\\gamma_{i n_{i}}m^{i}\\right]^{T}\\right)\\!,$ and $Q(\\vec{\\bf Y}_{i}|\\vec{\\bf X}_{i})=\\mathrm{Multinomial}(\\vec{\\bf k}_{i};\\dot{N}_{i},\\vec{\\bf p}_{i})$ , where ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{N_{i}=\\displaystyle\\sum_{j=1}^{n_{i}}x_{i j},\\;\\vec{\\mathbf{k}}_{i}=\\left[y_{i1}\\cdot\\cdot\\cdot\\ y_{i m_{i}}\\,\\sum_{j=1}^{n_{i}}x_{i j}-\\sum_{j=1}^{m_{i}}y_{i j}\\right]^{T},}}\\\\ {{\\vec{\\mathbf{p}}_{i}=\\displaystyle\\left[\\frac{\\gamma_{i1}^{Y}}{\\sum_{j=1}^{n_{i}}\\gamma_{i j}^{X}}\\,\\cdot\\cdot\\cdot\\,\\frac{\\gamma_{i m_{i}}^{Y}}{\\sum_{j=1}^{n_{i}}\\gamma_{i j}^{X}}\\,\\,1-\\frac{\\sum_{j=1}^{m_{i}}\\gamma_{i j}^{Y}}{\\sum_{j=1}^{n_{i}}\\gamma_{i j}^{X}}\\right]^{T}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Note that we can equivalently write $\\vec{\\bf p}_{i}$ as follows, ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\vec{\\bf p}_{i}=\\left[\\frac{\\gamma_{i1}^{Y}M^{i}}{\\sum_{j=1}^{n_{i}}\\gamma_{i j}^{X}M^{i}}\\,\\cdot\\cdot\\cdot\\,\\frac{\\gamma_{i m_{i}}^{Y}M^{i}}{\\sum_{j=1}^{n_{i}}\\gamma_{i j}^{X}M^{i}}\\,\\,1-\\frac{\\sum_{j=1}^{m_{i}}\\gamma_{i j}^{Y}M^{i}}{\\sum_{j=1}^{n_{i}}\\gamma_{i j}^{X}M^{i}}\\right]^{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "In the setup of lemma 16, choose $d_{1}\\,=\\,q_{i}$ and $d_{2}\\,=\\,r_{i}$ , $\\lambda_{j}^{(2)}\\;=\\;\\gamma_{i j}^{Y}M^{i}\\;\\forall\\;j\\;\\in\\;[r_{i}]$ and $\\lambda_{j}^{(1)}\\,=$ $\\gamma_{i j}^{X}M^{i}\\ \\forall\\ j\\ \\in\\ [q_{i}]$ . From (219), we know the condition $\\begin{array}{r}{\\sum_{j=1}^{d_{2}}\\lambda_{j}^{(2)}\\,\\le\\,\\sum_{j=1}^{d_{2}}\\lambda_{j}^{(2)}}\\end{array}$ is satisfied by assumption. Hence, we use the result of lemma 15 to conclude $\\tilde{Q}(\\vec{\\mathbf{Y}}_{i}|M)=\\mathbf{Poisson}(r_{i},1,\\vec{\\mathbf{\\Lambda}}^{\\prime})$ , with ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\vec{\\mathbf{A}}^{\\prime}=\\left[\\lambda_{i1}\\;\\;\\;\\;.\\;.\\;,\\lambda_{i r_{i}}\\right]^{T},\\;\\mathrm{with}\\;\\lambda_{i j}=\\gamma_{i j}^{Y}M^{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Comparing $\\tilde{Q}(\\vec{\\mathbf{Y}}_{i}|M)$ specified in (223) with $P(\\vec{\\mathbf{Y}}_{i}|M)$ specified in (218), we can conclude $Q(\\vec{\\mathbf{Y}}_{i}|M)=P(\\vec{\\mathbf{Y}}_{i}|M)$ . Substituting $Q(\\vec{\\mathbf{Y}}_{i}|M)=P(\\vec{\\mathbf{Y}}_{i}|M)$ in (221), we obtain: ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\tilde{Q}(\\vec{\\bf Y}|M)=\\prod_{i=1}^{d_{2}}P(\\vec{\\bf Y}_{i}|M).\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Comparing (224) with (218), we can conclude $\\tilde{Q}(\\vec{\\bf Y}|M)=P(\\vec{\\bf Y}|M)$ , finishing our proof. ", "page_idx": 58}, {"type": "text", "text": "K Proof of Theorem 6 ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "In this section, we provide the proof of theorem 6. Since theorem 6 provides the analytical PID terms for the linear convolution-closed system defined in Sec. 5.1, we briefly restate certain key properties of convolution-closed distributions and the linear convolution-closed system for convenience. ", "page_idx": 59}, {"type": "text", "text": "K.1 Convolution-closed distributions ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Convolution-closed distributions are a large class of distributions that are defined as follows: ", "page_idx": 59}, {"type": "text", "text": "Definition 1. Let $\\mathcal{F}_{\\mathcal{D}}$ denote a family of distributions, where each member distribution, $f(\\delta)\\in\\mathcal F_{\\mathcal D}$ , is indexed by a parameter $\\delta$ . Consider $\\begin{array}{r}{\\dot{X_{1}}\\sim f(\\delta_{1}),\\,X_{2}\\sim f(\\delta_{2}),}\\end{array}$ , and $X_{1}$ \u22a5\u22a5 $X_{2}$ for some $\\delta_{1},\\delta_{2}\\in\\mathcal{D}$ . Then, $\\mathcal{F}_{\\mathcal{D}}$ is convolution-closed in the parameter $\\delta$ , $i f$ ", "page_idx": 59}, {"type": "equation", "text": "$$\nX_{1}+X_{2}\\sim f(\\delta_{1})*f(\\delta_{2})=f(\\delta_{1}+\\delta_{2})\\,\\forall\\,\\delta_{1},\\delta_{2}\\in\\mathcal{D}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "where $^*$ denotes the convolution operator. ", "page_idx": 59}, {"type": "text", "text": "Convolution-closed distributions define a natural dilation/thinning operator. Formally, let $X\\sim f(\\delta)$ . Then, we define $X_{\\epsilon}$ as the $\\epsilon$ -dilated version of $X$ if $X\\sim f(\\epsilon\\delta)$ for some $\\epsilon\\in(0,1)$ such that $\\epsilon\\delta\\in\\mathcal{D}$ . Furthermore, if we assume $(1-\\epsilon)\\delta\\in\\mathcal{D}$ , then $P(X_{\\epsilon}|X)$ can be defined as $P(X_{\\epsilon}|X_{\\epsilon}+X_{(1-\\epsilon)})$ , where $X_{1-\\epsilon}\\sim f((1-\\epsilon)\\delta)$ and $X_{\\epsilon}$ \u22a5\u22a5 $X_{1-\\epsilon}$ . Denote, $P(X_{\\epsilon}|X=x)=P(X_{\\epsilon}|X_{\\epsilon}+X_{(1-\\epsilon)}=$ $x)=G(\\epsilon\\delta,(1-\\epsilon)\\delta,x)$ . ", "page_idx": 59}, {"type": "text", "text": "K.2 Definition of linear convolution-closed system ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Let $\\mathcal{F}_{\\mathcal{D}}$ be a convolution-closed distribution family in parameter $\\delta\\in\\mathcal{D}$ , and $M$ be a target/message random variable having distribution $P(M)$ over some support set $\\mathcal{M}$ . Define the conditional distribution of random variables $X$ and $Y$ conditioned on $M$ as follows: ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r}{P(Y|M{=}m)=f(\\delta_{m}^{Y})\\ \\mathrm{{such}\\ t h a t}\\ \\delta_{m}^{X},\\delta_{m}^{Y}\\in\\mathcal{D}\\,\\forall\\,m\\in\\mathcal{M}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Furthermore, we assume that there exists $\\gamma\\in(0,\\infty)$ such that: $\\delta_{m}^{X}=\\gamma\\delta_{m}^{Y}\\,\\forall\\,m\\in\\mathcal{M}$ . We denote such a system of random variables $(M,X,Y)$ , having the joint distribution $P(M,X,Y)$ , a linear convolution-closed system. ", "page_idx": 59}, {"type": "text", "text": "K.3 Formal proof of theorem 6 ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Theorem 6. Let the joint density $P(M,X,Y)$ of random variables $M,\\,X$ , and $Y$ describe a linear convolution-closed system. Without the loss of generality, assume $\\gamma\\le1$ . If ", "page_idx": 59}, {"type": "text", "text": "(a) $(1-\\gamma)\\delta_{m}^{Y}\\in\\mathcal{D}\\,\\forall\\,m\\in\\mathcal{M},$ (b) $P(X_{\\gamma}|X_{\\gamma}+\\,X_{1-\\gamma}\\!=\\!x,M\\!=\\!m)\\ =\\ G(\\gamma\\delta_{m}^{X},(1\\,-\\,\\gamma)\\delta_{m}^{X},x)$ ) does not depend on $m$ , where $P(X_{\\gamma}|M)=f(\\gamma\\delta_{m}^{X}),\\,P(X_{1-\\gamma}|M)=f((1-\\gamma)\\delta_{m}^{X})$ and $X_{\\gamma}$ \u22a5\u22a5 $X_{1-\\gamma}|M$ , ", "page_idx": 59}, {"type": "text", "text": "then $\\Delta_{P}$ contains a Markov chain of the form $M\\rightarrow X\\rightarrow Y$ and $U I(M;Y\\backslash X)=0.$ . ", "page_idx": 59}, {"type": "text", "text": "Proof. We first note that we can always assume $\\gamma\\le1$ without the loss of generality because if $\\gamma\\geq1$ , then we can always switch our nomenclature to refer to $Y$ as $X$ , and $X$ as $Y$ . ", "page_idx": 59}, {"type": "text", "text": "We briefly outline the proof structure. ", "page_idx": 59}, {"type": "text", "text": "1. In the first part, we explicitly construct a joint distribution $Q_{M C}(M,X,Y)$ having the Markovian structure $M\\rightarrow X\\rightarrow Y$ . 2. In the second part, we show that the $Q_{M C}(M,X,Y)$ constructed in the first part, lies in $\\Delta_{P}$ . Therefore, we can then apply the result of proposition 1 to conclude $U I({\\bar{M}};Y\\backslash X)=0$ . ", "page_idx": 59}, {"type": "text", "text": "Part 1: Specifying $Q_{M C}(M,X,Y)$ ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Denote the joint distribution of the Markov chain $M\\,\\rightarrow\\,X\\,\\rightarrow\\,Y$ as $Q_{M C}(M,X,Y)$ . We can decompose $\\dot{Q}_{M C}(M,X,Y)$ by utilizing its Markovian structure as follows: ", "page_idx": 60}, {"type": "equation", "text": "$$\nQ_{M C}(M,X,Y)=Q_{M C}(M)Q_{M C}(X|M)Q_{M C}(Y|X).\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Consequently, we can construct the distribution $Q_{M C}(M,X,Y)$ by individually specifying $Q_{M C}(M)$ , $\\dot{Q}_{M C}(X|M)$ , and $Q_{M C}(Y|X)$ . ", "page_idx": 60}, {"type": "text", "text": "Specifying $Q_{M C}(M)$ and $Q_{M C}(X|M)$ : We specify $Q_{M C}(M)$ and $Q_{M C}(X|M)$ as follows: ", "page_idx": 60}, {"type": "equation", "text": "$$\nQ_{M C}(M)=P(M){\\mathrm{~and~}}Q_{M C}(X|M)=P(X|M).\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Specifying $Q_{M C}(Y|X)$ : By assumption (b) in the theorem, we know that $G(\\gamma\\delta_{m}^{X},(1-\\gamma)\\delta_{m}^{X},x)$ does not depend on $m$ , and consequently, does not depend on $\\delta_{m}^{X}$ . Hence, we can simplify notation of $G(\\gamma\\delta_{m}^{X},\\bar{(}1-\\gamma)\\delta_{m}^{X},m)$ as $G(\\gamma,(1-\\gamma),x)$ . We specify $Q({\\dot{Y}}|X)$ as follows: ", "page_idx": 60}, {"type": "equation", "text": "$$\nQ(Y|X=x)=G(\\gamma,1-\\gamma,x).\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Part 2: Showing $Q_{M C}(M,X,Y)\\in\\Delta_{P}$ ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "For showing that $Q_{M C}(M,X,Y)\\in\\Delta_{P}$ , we first need to derive $Q_{M C}(Y|M)$ . ", "page_idx": 60}, {"type": "text", "text": "Deriving $Q_{M C}(Y|M)$ : We use the result of lemma 19 to derive $Q_{M C}(Y|M)$ . As $Q_{M C}(X|M=$ $m)\\;=\\;f(\\delta_{m}^{X})$ is a convolution-closed distribution for a fixed $m$ , and $Q_{M C}(Y\\,=\\,y|X\\,=\\,x)\\,=$ $G(\\gamma,1-\\gamma,x)$ , the result of lemma 19 shows that $Q_{M C}(Y|M)$ can be expressed as: ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Q_{M C}(Y|M=m)=f(\\gamma\\times\\delta_{m}^{X})=f(\\gamma\\delta_{m}^{X})\\overset{(a)}{=}f(\\delta_{m}^{Y})=P(Y|M=m),}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "where $(a)$ is due to the assumption $\\delta_{m}^{X}=\\gamma\\delta_{m}^{Y}$ (see Appx. K.2). ", "page_idx": 60}, {"type": "text", "text": "From (227) and (229), we can conclude: ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{M C}(M,X)=Q_{M C}(M)Q_{M C}(X|M)=P(M)P(X|M)=P(M,X),}\\\\ &{Q_{M C}(M,Y)=Q_{M C}(M)Q_{M C}(Y|M)=P(M)P(Y|M)=P(M,Y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Hence, $Q_{M C}(M,X,Y)\\in\\Delta_{P}$ and consequently, by proposition 1, $U I(M;Y\\backslash X)=0$ , concluding our proof for case 1. ", "page_idx": 60}, {"type": "text", "text": "Lemma 19. Let $\\mathcal{F}_{\\mathcal{D}}$ be a convolution-closed distribution family convolution-closed in the parameter $\\delta$ . For a $\\delta\\in\\mathcal{D}$ , assume there exists $\\epsilon\\in(0,1)$ such that $\\epsilon\\delta$ $\\delta,(\\mathrm{i}-\\epsilon)\\delta\\in\\mathcal{D}$ . Consider the following random variables: ", "page_idx": 60}, {"type": "equation", "text": "$$\nX_{\\epsilon}^{\\prime}\\sim f(\\epsilon\\delta)\\in\\mathcal{F}_{\\mathcal{D}},\\ \\ X_{1-\\epsilon}^{\\prime}\\sim f((1-\\epsilon)\\delta)\\in\\mathcal{F}_{\\mathcal{D}},\\ \\ a n d\\,X\\sim f(\\delta)\\in\\mathcal{F}_{\\mathcal{D}},\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "such that $X_{\\epsilon}^{\\prime}\\perp\\!\\!\\!\\perp X_{1-\\epsilon}^{\\prime}$ . If $P(X_{\\epsilon}=x_{\\epsilon}|X=x)=P(X_{\\epsilon}^{\\prime}=x_{\\epsilon}|X_{\\epsilon}^{\\prime}+X_{1-\\epsilon}^{\\prime}=x)$ ), then we have $P(X_{\\epsilon}=x_{\\epsilon})=P(X_{\\epsilon}^{\\prime}=x_{\\epsilon})=f(x_{\\epsilon};\\epsilon\\delta)$ or $X_{\\epsilon}\\overset{d}{=}X_{\\epsilon}^{\\prime}$ , where $\\underline{{\\underline{{d}}}}$ implies equality in distribution $I57J$ . ", "page_idx": 60}, {"type": "text", "text": "Proof. First let us derive the distribution of $X^{\\prime}=X_{1-\\epsilon}^{\\prime}+X_{\\epsilon}^{\\prime}$ . Since, $X_{\\epsilon}^{\\prime}$ \u22a5\u22a5 $X_{1-\\epsilon}^{\\prime}$ , we can write $P(X^{\\prime})$ as follows: ", "page_idx": 60}, {"type": "equation", "text": "$$\nP(X^{\\prime})=f(\\epsilon\\delta)*f((1-\\epsilon)\\delta)\\stackrel{(a)}{=}f(\\epsilon\\delta+(1-\\epsilon)\\delta)=f(\\delta)\\stackrel{(b)}{=}P(X),\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "where $^*$ is the convolution operator, $(a)$ is due to the properties of convolution-closed distribution (see Appx. K.1), and $(b)$ is due to the definition of $X$ in the lemma statement. From (232), we can conclude $P(X={\\mathit{x}})=P(X^{\\prime}=x)$ , and by assumption we have $P(X_{\\epsilon}|X)=P(X_{\\epsilon}^{\\prime}|X^{\\prime})=$ $P(X_{\\epsilon}^{\\prime}|X_{\\epsilon}^{\\prime}+X_{(1-\\epsilon)}^{\\prime})$ . Hence, we have: ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{P(X=x,X_{\\epsilon}=x_{\\epsilon})=P(X=x)P\\left(X_{\\epsilon}=x_{\\epsilon}|X=x\\right)=P(X^{\\prime}=x)P\\left(X_{\\epsilon}^{\\prime}=x_{\\epsilon}|X^{\\prime}=x\\right),}&{{}}\\\\ {=P(X^{\\prime}=x)P\\left(X_{\\epsilon}^{\\prime}=x_{\\epsilon}|X_{\\epsilon}^{\\prime}+X_{1-\\epsilon}^{\\prime}=x\\right)}&{{}}\\\\ {=P\\left(X_{\\epsilon}^{\\prime}+X_{1-\\epsilon}^{\\prime}=x,X_{\\epsilon}^{\\prime}=x_{\\epsilon}\\right).}&{{}}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "From (233), we can conclude that $P(X_{\\epsilon})=P(X_{\\epsilon}^{\\prime})=f(\\epsilon\\delta)$ . ", "page_idx": 60}, {"type": "text", "text": "L Proof of Theorem 7 ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "In this section, we provide the proof of theorem 7. ", "page_idx": 61}, {"type": "text", "text": "L.1 Definition of pexp1 and pexp2 ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "We restate the definitions of $p_{e x p1}$ and $p_{e x p2}$ defined in Sec. 5.2, which is used in the proof of theorem 7. Let $X\\sim p_{e x p1}(X)$ , where ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p_{e x p1}(X=x;\\theta_{1},\\theta_{2})=H(\\theta_{1},\\theta_{2})\\exp(\\theta_{1}^{T}x-\\theta_{2}^{T}A(x)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "for some appropriately defined $H(\\cdot,\\cdot),A(\\cdot),\\theta_{1}$ and $\\theta_{2}$ . Furthermore, define a random variable $Y$ through its conditional density $p(Y|X=x)$ : ", "page_idx": 61}, {"type": "equation", "text": "$$\np(Y=y|X=x;\\theta_{1},\\theta_{2})=h(y)\\exp\\left(x^{T}T(y)-\\theta_{3}^{T}A(x)\\right),\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "for some $h(\\cdot),\\,T(\\cdot)$ , and $\\theta_{3}$ , such that $p(Y\\;=\\;y|X\\;=\\;x;\\theta_{1},\\theta_{2})$ is a well-defined distribution. Furthermore, the marginal distribution of $Y$ is expressed as: ", "page_idx": 61}, {"type": "equation", "text": "$$\np_{e x p2}(Y=y;\\theta_{1},\\theta_{2},\\theta_{3})=h(y)\\frac{H(\\theta_{1},\\theta_{2})}{H(\\theta_{1}+T(y),\\theta_{2}+\\theta_{3})}\\mathrm{~(see~proof~of~theorem~1~in~}[26]).\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "L.2 Formal proof of theorem 7 ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Theorem 7. Let $M,X$ , and $Y$ be random variables having the joint distribution $P(M,X,Y)$ . Furthermore, the conditional distribution of $X$ and $Y$ conditioned on $M$ are as follows: $P(X|M{=}m){=}p_{e x p1}(X;\\theta_{1}(m),\\theta_{2}(m))$ and $P(Y|M{=}m){=}p_{e x p2}(Y;\\theta_{1}(m),\\theta_{2}(m),\\theta_{3})$ . Then, $\\Delta_{P}$ contains a Markov chain of the form $M\\rightarrow X\\rightarrow Y$ and $U I({\\bar{M}};Y\\backslash X)=0$ . ", "page_idx": 61}, {"type": "text", "text": "Proof. We prove theorem 7 by explicitly constructing a Markov chain $M\\rightarrow X\\rightarrow Y$ , having joint distribution $Q_{M C}(M,X,Y)$ , and showing that $Q_{M C}(M,X,Y)\\in\\Delta_{P}$ . ", "page_idx": 61}, {"type": "text", "text": "Specifying $Q_{M C}(M)$ and $Q_{M C}(X|M)$ : We specify $Q_{M C}(M)$ and $Q_{M C}(X|M)$ as follows: ", "page_idx": 61}, {"type": "equation", "text": "$Q_{M C}(X|M)=P(X|M).$ ", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Specifying $Q_{M C}(Y|X)$ : We specify $Q_{M C}(Y|X)=h(y)\\exp(x^{T}T(y)-\\theta_{3}^{T}A(x))$ . Then, by (236), we know that ", "page_idx": 61}, {"type": "equation", "text": "$$\nQ(Y=y|M=m)=p_{e x p2}(y;\\theta_{1}(m),\\theta_{2}(m),\\theta_{3})=\\frac{h(y)H(\\theta_{1}(m),\\theta_{2}(m))}{H(\\theta_{1}(m)+T(y),\\theta_{2}(m)+\\theta_{3}(m))}.\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "From (237) and (238), we can conclude: ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Q_{M C}(M,Y)=Q_{M C}(Y|M)Q_{M C}(M)=P(Y|M)P(M)=P(M,Y),}\\\\ {Q_{M C}(M,X)=Q_{M C}(X|M)Q_{M C}(M)=P(X|M)P(M)=P(M,X).}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Hence, $Q_{M C}(M,X,Y)\\in\\Delta_{P}$ and consequently, by proposition 1, $U I(M;Y\\backslash X)=0$ , concluding our proof. ", "page_idx": 61}, {"type": "text", "text": "M Additional proofs and details for Sec. 6 ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "In this section, we provide the proofs of certain statements made in Sec. 6, as well as additional details regarding the simulation study presented in the same section. ", "page_idx": 61}, {"type": "text", "text": "M.1 Generating $\\bar{Q}$ for every $Q\\in\\Delta_{P}$ ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Our goal in this section is to show that there exists a $\\bar{Q}$ (as described in Sec. 6) for every $Q\\in\\Delta_{P}$ , where the corresponding $P(M,X,Y)$ is as defined in Sec. 6. We first briefly restate the assumptions on $P(M,X,Y)$ described in Sec. 6. The distribution $P(M,X,Y)$ has the following properties: the random variable $M$ has support over $\\mathcal{M}$ , the conditional distributions $P(X|M)$ and $\\bar{P(Y|M)}$ are members of some convolution-closed distribution family $\\mathcal{F}_{\\mathcal{D}}$ , and there exists some $\\delta_{b i a s}^{X},\\delta_{b i a s}^{Y}\\in\\mathcal{D}$ such that: ", "page_idx": 61}, {"type": "text", "text": "", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta_{m}^{X}-\\delta_{b i a s}^{X}=\\epsilon_{m}^{(1)}(\\delta_{m}^{Y}-\\delta_{b i a s}^{Y})\\mathrm{~for~some~}\\epsilon_{m}^{(1)}\\in[0,1]\\,\\forall\\,m\\in\\mathcal{M},}\\\\ &{\\qquad\\quad\\delta_{m}^{Y}=\\epsilon_{m}^{(2)}(\\delta_{m}^{Y}-\\delta_{b i a s}^{Y})\\mathrm{~for~some~}\\epsilon_{m}^{(2)}\\in[0,1]\\,\\forall\\,m\\in\\mathcal{M},}\\\\ &{\\qquad\\quad\\delta_{m}^{X}=\\epsilon_{m}^{(3)}(\\delta_{m}^{X}-\\delta_{b i a s}^{X})\\mathrm{~for~some~}\\epsilon_{m}^{(3)}\\in[0,1]\\,\\forall\\,m\\in\\mathcal{M},\\mathrm{~and}}\\\\ &{\\qquad\\quad(\\delta_{m}^{Y}-\\delta_{b i a s}^{Y}),\\;(\\delta_{m}^{X}-\\delta_{b i a s}^{X}),\\;(\\delta_{m}^{X}-\\delta_{m}^{Y}-(\\delta_{b i a s}^{X}-\\delta_{b i a s}^{Y}))\\in\\mathcal{D}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "As ${Q(M,X){=}P(M,X)}$ and $Q(M,Y)=P(M,Y)$ due to $Q\\in\\Delta_{P}$ , we have: ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{Q}(X|M\\!=\\!m)\\!=\\!f(\\delta_{m}^{X}),\\mathrm{~and~}Q(Y|M\\!=\\!m)\\!=\\!f(\\delta_{m}^{Y})\\mathrm{~where~}\\delta_{m}^{X},\\delta_{m}^{Y}\\in\\mathcal{D}\\,\\forall\\,m\\in\\mathcal{M}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "M.1.1 Explicitly constructing a $\\bar{Q}$ for every $Q\\in\\Delta_{P}$ ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "We show existence of a $\\bar{Q}$ for every $Q\\in\\Delta_{P}$ by explicitly constructing a $\\bar{Q}$ for each $Q\\in\\Delta_{P}$ . We define two vectors to simplify our notations: ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\vec{\\mathbf{A}}=\\left[X\\quad Y\\right]^{T},\\ \\ \\vec{\\mathbf{B}}=\\left[X^{\\prime}\\quad Y^{\\prime\\prime}\\quad n_{X}\\quad Y^{\\prime}\\quad n_{Y}\\right]^{T},\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "where $X$ and $Y$ are the random variables defined above. $(X^{\\prime},Y^{\\prime\\prime},n_{X})$ , and $(Y^{\\prime},n_{Y})$ are the corresponding dilated versions of $X$ and $Y$ as defined in Sec. 6. Note that the joint distribution of $(M\\bar{,}X^{\\prime},Y^{\\bar{\\prime}\\prime},n_{X},Y^{\\prime},n_{Y})$ is ${\\bar{Q}}(M,X^{\\prime},Y^{\\prime\\prime},n_{X},Y^{\\prime},n_{Y})$ . We reiterate the two main properties of ${\\bar{Q}}(X^{\\prime},Y^{\\prime\\prime},n_{X},Y^{\\prime},n_{Y})$ , mentioned in Sec. 6, that we want to prove: ", "page_idx": 62}, {"type": "text", "text": "1. The marginal conditional distributions of $(X^{\\prime},Y^{\\prime\\prime},n_{X},Y^{\\prime},n_{Y})$ given $M$ are as follows: ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{Q}(X^{\\prime}|M\\!=\\!m)\\!=\\!f(\\delta_{m}^{X}\\!-\\!\\delta_{m}^{Y}-(\\delta_{b i a s}^{X}\\!-\\!\\delta_{b i a s}^{Y})),\\;\\bar{Q}(Y^{\\prime\\prime}|M\\!=\\!m)=f(\\delta_{m}^{Y}-\\delta_{b i a s}^{Y}),}\\\\ &{\\bar{Q}(Y^{\\prime}|M\\!=\\!m)\\!=\\!f(\\delta_{m}^{Y}-\\delta_{b i a s}^{Y}),\\bar{Q}(n_{X}|M)=f(\\delta_{b i a s}^{X}),\\bar{Q}(n_{Y}|M)=f(\\delta_{b i a s}^{Y}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "2. The random variables $(X^{\\prime},Y^{\\prime\\prime},n_{X})$ are jointly conditionally independent given $M$ . Similarly, the random variables $(Y^{\\prime},n_{Y})$ are also conditionally independent given $M$ . ", "page_idx": 62}, {"type": "text", "text": "We construct $\\vec{\\mathbf B}$ from $\\vec{\\mathbf A}$ by specifying the conditional distribution $\\bar{Q}(\\vec{\\bf B}|\\vec{\\bf A},M)$ . Note that given the values of $\\vec{\\mathbf A}$ and $M$ , we can always use $\\bar{Q}(\\vec{\\bf B}|\\vec{\\bf A},M)$ to construct the random variables $\\vec{\\mathbf B}$ . The goal of this remaining section is to show that the corresponding distribution $\\bar{Q}(\\vec{\\bf B},M)$ (specified through $\\bar{Q}(\\vec{\\bf B}|\\vec{\\bf A},M))$ ) satisfies the above two properties. We first explicitly state the structure of $\\bar{Q}(\\vec{\\bf B}|\\vec{\\bf A},M)$ . ", "page_idx": 62}, {"type": "text", "text": "Specifying $\\bar{Q}(\\vec{\\bf B}|\\vec{\\bf A},M)$ : ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "We first impose the following structure on $\\bar{Q}(\\vec{\\bf B}|\\vec{\\bf A},M)$ as follows: ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\bar{Q}(\\vec{\\bf B}|\\vec{\\bf A},M)=\\bar{Q}(X^{\\prime},Y^{\\prime\\prime},n_{X}|X,M)\\bar{Q}(Y^{\\prime},n_{Y}|Y,M),\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "where ${\\bar{Q}}(X^{\\prime},Y^{\\prime\\prime},n_{X}|X,M)$ and $\\bar{Q}(Y^{\\prime},n_{Y}|Y,M)$ are the conditional distributions through which we specify $\\bar{Q}(\\vec{\\bf B}|\\vec{\\bf A},M)$ . Since $\\bar{Q}(\\vec{\\bf B}|\\vec{\\bf A},M)$ is a product of two well-defined distributions, it is a valid distribution. Let us now analyze the joint distribution $\\bar{Q}(M,\\vec{\\bf A},\\vec{\\bf B})$ . By chain rule, we have: ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\bar{Q}(M,\\vec{\\bf A},\\vec{\\bf B})=\\bar{Q}(M,\\vec{\\bf A})\\bar{Q}(\\vec{\\bf B}|\\vec{\\bf A},M).\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "Substituting the fact that $(M,\\vec{\\bf A})\\,=\\,(M,X,Y)$ and using the structure of $\\bar{Q}(\\vec{\\bf B}|\\vec{\\bf A},M)$ specified in (244), we have: ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\bar{Q}(M,\\vec{\\bf A},\\vec{\\bf B})=\\bar{Q}(M)\\bar{Q}(X,Y|M)\\bar{Q}(X^{\\prime},Y^{\\prime\\prime},n_{X}|X,M)\\bar{Q}(Y^{\\prime},n_{Y}|Y,M).\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "We now specify the two components of $\\bar{Q}(\\vec{\\bf B}|\\vec{\\bf A},M)$ . Specifying $\\bar{Q}(Y^{\\prime},n_{Y}|Y,M)$ : ", "page_idx": 62}, {"type": "text", "text": "Using the chain rule on $\\bar{Q}(Y^{\\prime},n_{Y}|Y,M)$ , we obtain ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\bar{Q}(Y^{\\prime},n_{Y}|Y,M)=\\bar{Q}(Y^{\\prime}|Y,M)\\bar{Q}(n_{Y}|Y,Y^{\\prime},M).\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Choose: ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{Q}(Y^{\\prime}|Y\\!=\\!y,M\\!=\\!m)=G(\\epsilon_{m}^{(2)}\\delta_{m}^{Y},(1-\\epsilon_{m}^{(2)})\\delta_{m}^{Y},y),\\mathrm{~where~}\\delta_{m}^{Y}\\!=\\!\\epsilon_{m}^{(2)}(\\delta_{m}^{Y}-\\delta_{b i a s}^{Y})\\vee m\\in\\mathcal{M},}\\\\ &{\\bar{Q}(n_{Y}|Y\\!=\\!y,Y^{\\prime}\\!=\\!y^{\\prime},M\\!=\\!m)=\\delta_{K}(n_{Y}=y-y^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Here, $\\delta_{K}(\\cdot)$ denotes the Kronecker delta function [59]. Marginalizing $(X,X^{\\prime},Y^{\\prime\\prime},n_{X})$ from (246), we obtain: ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\bar{Q}(M,Y,Y^{\\prime},n_{Y})=\\bar{Q}(M)\\bar{Q}(Y|M)\\bar{Q}(Y^{\\prime},n_{Y}|M,Y)=\\bar{Q}(M)\\bar{Q}(Y,Y^{\\prime},n_{Y}|M).\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Consider the distribution ${\\bar{Q}}(Y,Y^{\\prime},n_{Y}|M)$ . Then, we know: ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{Q}(Y|M{=}m)=f(\\delta_{m}^{Y}),}\\\\ &{\\bar{Q}(n_{Y}|Y{=}y,Y^{\\prime}{=}y^{\\prime}M{=}m)=\\delta_{K}(n_{Y}=y-y^{\\prime}),\\mathrm{~and~}}\\\\ &{\\bar{Q}(Y^{\\prime}{=}y^{\\prime}|Y{=}y,M{=}m)=G(\\epsilon_{m}^{(2)}\\delta_{m}^{Y},(1-\\epsilon_{m}^{(2)})\\delta_{m}^{Y},y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Comparing the above equation with the structure of $\\bar{Q}(Y,Y^{\\prime},n_{Y}|M)$ specified in proposition 8, we can conclude that $(Y^{\\prime},n_{Y})$ are just dilated versions of $Y$ . Consequently, using the result of proposition 8, we have: ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{Q}(Y^{\\prime}|M\\!=\\!m)=f(\\delta_{m}^{Y}-\\delta_{b i a s}^{Y}),\\bar{Q}(n_{Y}|M)=f(\\delta_{b i a s}^{Y}),\\mathrm{~and~}Y^{\\prime}\\perp\\!\\!\\!\\perp n_{Y}|M.}\\end{array}\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Specifying ${\\bar{Q}}(X^{\\prime},Y^{\\prime\\prime},n_{X}|X,M)$ : ", "page_idx": 63}, {"type": "text", "text": "We similarly decompose ${\\bar{Q}}(X^{\\prime},Y^{\\prime\\prime},n_{X}|X,M)$ using the chain rule as follows: ", "page_idx": 63}, {"type": "text", "text": "$\\begin{array}{r}{\\bar{Q}(X^{\\prime},Y^{\\prime\\prime},n_{X}|X,M)=\\bar{Q}(n_{X}|X,M)\\bar{Q}(X^{\\prime}|X,n_{X},M)\\bar{Q}(Y^{\\prime\\prime}|X,n_{X},X^{\\prime},M).}\\end{array}$ We choose: ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{Q}(X^{\\prime}|X\\!=\\!x,n_{X}\\!=\\!x_{n_{X}},M\\!=\\!m)=G\\big((1-\\epsilon_{m}^{(1)})\\big(\\delta_{m}^{X}-\\delta_{b i a s}^{X}\\big),\\epsilon_{m}^{(1)}\\big(\\delta_{m}^{X}-\\delta_{b i a s}\\big),x-x_{n_{X}}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 63}, {"type": "equation", "text": "$\\delta_{m}^{X}-\\delta_{b i a s}^{X}=\\epsilon_{m}^{(1)}(\\delta_{m}^{Y}-\\delta_{b i a s}^{Y})\\,\\forall\\,m\\in\\mathcal{M}$ ", "text_format": "latex", "page_idx": 63}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{Q}(Y^{\\prime\\prime}|X\\!=\\!x,n_{X}\\!=\\!x_{n_{X}},X^{\\prime}\\!=\\!x^{\\prime},M\\!=\\!m)=\\delta_{K}\\!\\left(n_{X}=x-x^{\\prime}-x_{n_{X}}\\right)\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Here $\\delta_{K}(\\cdot)$ denotes the Kronecker delta function [59]. Marginalizing $(Y,Y^{\\prime},n_{Y})$ from (246), we obtain: ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\bar{Q}(M,X,X^{\\prime},Y^{\\prime\\prime},n_{X})=\\bar{Q}(M)\\bar{Q}(X|M)\\bar{Q}(X^{\\prime},n_{X},Y^{\\prime\\prime}|M,X)=\\bar{Q}(M)\\bar{Q}(X,X^{\\prime},Y^{\\prime\\prime},n_{X}|M).\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Consider the distribution ${\\bar{Q}}(X,X^{\\prime},n_{X},Y^{\\prime\\prime}|M)$ . Then, we know: ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\bar{Q}(n_{Y}|X,M),\\bar{Q}(X^{\\prime}|X,n_{X},M),\\mathrm{~and~}\\bar{Q}(Y^{\\prime\\prime}|X,n_{X},\\ldots,X^{\\prime},M)\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Comparing the above equation with the structure of ${\\bar{Q}}(X,X^{\\prime},n_{X},Y^{\\prime\\prime}|M)$ specified in proposition 9, we can conclude that $({\\bar{X}}^{\\prime},Y^{\\prime\\prime},n_{X})$ are just dilated versions of $X$ . Consequently, using the result of proposition 9, we have: ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{Q}(X^{\\prime}|M{=}m)=f(\\delta_{m}^{X}-\\delta_{m}^{Y}-(\\delta_{b i a s}^{X}-\\delta_{b i a s}^{Y})),}\\\\ &{\\bar{Q}(n_{X}|M)=f(\\delta_{b i a s}^{X}),}\\\\ &{\\bar{Q}(Y^{\\prime\\prime}|M)=f(\\delta_{m}^{Y}-\\delta_{b i a s}^{Y}),\\;\\mathrm{and}}\\\\ &{(X^{\\prime},Y^{\\prime\\prime},n_{X})\\;\\mathrm{are\\;jointly\\;conditionally\\;independe1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "This concludes our construction of $\\bar{Q}(\\vec{\\bf B}|\\vec{\\bf A},M)$ . We obtain our desired ${\\bar{Q}}(M,X^{\\prime},Y^{\\prime\\prime},n_{X},Y^{\\prime},n_{Y})$ by marginalizing $\\vec{\\mathbf A}$ out of $\\bar{Q}(M,\\vec{\\bf A},\\vec{\\bf B})$ . From (251) and (255) we know that $\\bar{Q}(M,X^{\\overline{{{\\prime}}}},Y^{\\prime\\prime},n_{X}\\bar{,}Y^{\\prime},n_{Y})$ satisfies the two main properties specified earlier. Since we did not make any specific assumptions on $Q$ aside from the fact that it lies in $\\Delta_{P}$ , the above construction can always be used to construct $(X^{\\prime},n_{X},Y^{\\prime\\prime},Y^{\\prime},n_{Y})$ from $(M,X,Y)$ using the $\\bar{Q}(\\vec{\\bf B}|\\vec{\\bf A},M)$ . Note that due to the particular construction of $\\bar{Q}(\\vec{\\bf B}|\\vec{\\bf A},M)$ , we have: ", "page_idx": 63}, {"type": "text", "text": "M.1.2 Propositions required for constructing $\\bar{Q}$ ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "We describe two important propositions that we use to show the existence of a $\\bar{Q}$ for every $Q\\in\\Delta_{P}$ . Proposition 8. Suppose the random variables $M$ and $Y$ have the joint distribution $Q(M,Y)$ such that ", "page_idx": 64}, {"type": "equation", "text": "$$\nQ(M)=P(M)\\,a n d\\,Q(Y|M\\!=\\!m)=f(\\delta_{m}^{Y})\\,\\forall\\,m\\in\\mathcal{M},\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "where $\\mathcal{M}$ denotes the support of $M$ and $f(\\delta_{m}^{Y})\\in\\mathcal{F}_{\\mathcal{D}}$ for some convolution-closed distribution family $\\mathcal{F}_{\\mathcal{D}}$ . Furthermore, there also exists a $\\delta_{b i a s}^{Y}\\in\\mathcal{D}$ such that ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\delta_{m}^{Y}=\\epsilon_{m}^{(2)}\\big(\\delta_{m}^{Y}-\\delta_{b i a s}^{Y}\\big)f o r\\,s o m e\\,\\epsilon_{m}^{(2)}\\in[0,1]\\;a n d\\,\\big(\\delta_{m}^{Y}-\\delta_{b i a s}^{Y}\\big)\\in\\mathcal{D}\\,\\forall\\,m\\in\\mathcal{M}.\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "Consider two dilated versions of $Y$ , denoted as $Y^{\\prime}$ and $n_{Y}$ , defined as follows: ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\bar{Q}(Y^{\\prime}|Y\\!=\\!y,M\\!=\\!m)=G(\\epsilon_{m}^{(2)}\\delta_{m}^{Y},(1-\\epsilon_{m}^{(2)})\\delta_{m}^{Y},y),\\;a n d\\,n_{Y}=Y-Y^{\\prime}.\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "Then, we have ", "page_idx": 64}, {"type": "equation", "text": "$$\nI.~Y^{\\prime}\\perp\\!\\!\\!\\perp n_{Y}|M\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "2. The conditional distributions ${\\bar{Q}}(Y^{\\prime}|M)$ and $\\bar{Q}(n_{Y}|M)$ are specified as follows: ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{Q}(Y^{\\prime}|M=m)=f(\\epsilon_{m}^{(2)}\\delta_{m}^{Y})=f(\\delta_{m}^{Y}-\\delta_{b i a s}^{Y})\\;a n d}\\\\ &{\\bar{Q}(n_{Y}|M{=}m)=f((1-\\epsilon_{m}^{(2)})\\delta_{m}^{Y})=f(\\delta_{b i a s}^{Y})\\,\\forall\\,m\\in\\mathcal{M}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "Proof. We essentially modify the argument given in theorem 1 of [25] to prove the above proposition. Consider the conditional distribution of $Y,~Y^{\\prime}$ , and $n_{Y}$ conditioned on $M$ , denoted as $\\bar{Q}(Y,Y^{\\prime},n_{Y}|M)$ . Then, by chain rule, we have $\\bar{Q}(Y,Y^{\\prime},n_{Y}|M)~~=$ $\\bar{Q}(Y|M)\\bar{Q}(Y^{\\prime}|M,Y)\\bar{Q}(\\stackrel{.}{n_{Y}}|Y,Y^{\\prime},M)$ . By the assumptions in (257) and (259), we know that ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{Q}(Y|M\\!=\\!m)=f(\\delta_{m}^{Y}),\\bar{Q}(Y^{\\prime}|M\\!=\\!m,Y\\!=\\!y)=G(\\epsilon_{m}^{(2)}\\delta_{m}^{Y},(1-\\epsilon_{m}^{(2)})\\delta_{m}^{Y},y),}\\\\ &{\\bar{Q}(n_{Y}|Y\\!=\\!y,Y^{\\prime}\\!=\\!y^{\\prime},M)=\\delta_{K}(n_{Y}=y-y^{\\prime}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "where $\\delta_{K}(\\cdot)$ is the Kronecker delta function [59]. ", "page_idx": 64}, {"type": "text", "text": "Observation 1: Our first key observation is that the distribution $\\bar{Q}(Y^{\\prime},Y,n_{Y}|M\\!=\\!m)$ is completely specified through the distributions $\\bar{Q}(Y|M)$ and $\\bar{Q}(Y^{\\prime}|Y,M)$ , as $n_{Y}$ is completely determined by $Y^{\\prime}$ and $Y$ and $\\breve{\\bar{Q}}(n_{Y}|Y,Y^{\\prime},M)$ is specified through a Kronecker delta function. ", "page_idx": 64}, {"type": "text", "text": "Similarly, consider a new pair of random variables $Y_{n e w}^{\\prime},n_{Y}^{n e w}$ such that $Y_{n e w}^{\\prime}$ \u22a5\u22a5 $n_{Y}^{n e w}|M$ and the respective conditional distributions of $Y_{n e w}^{\\prime}$ and $n_{Y}^{n e w}$ are specified as follows: ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\bar{Q}_{n e w}(Y_{n e w}^{\\prime}|M\\!=\\!m)=f(\\epsilon_{m}^{(2)}\\delta_{m}^{Y}),\\mathrm{~and~}\\bar{Q}_{n e w}(n_{Y}^{n e w}|M\\!=\\!m)=f((1-\\epsilon_{m}^{(2)})\\delta_{m}^{Y})\\forall\\,m\\in\\mathcal{M}.\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "We will now show two key properties for the random variables $(Y_{n e w}^{\\prime},n_{Y}^{n e w},Y_{n e w}^{\\prime}+n_{Y}^{n e w})$ ew, Y n\u2032ew + nYnew) that will form the crux of our argument: ", "page_idx": 64}, {"type": "text", "text": "Property 1: $\\bar{Q}_{n e w}(Y_{n e w}^{\\prime}+n_{Y}^{n e w}\\!=\\!y|M\\!=\\!m)=\\bar{Q}(Y\\!=\\!y|M\\!=\\!m).$ ", "page_idx": 64}, {"type": "text", "text": "Let us now calculate the conditional distribution $\\bar{Q}(Y_{n e w}^{\\prime}+n_{Y}^{n e w}=y|M)$ : ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\bar{Q}_{n e w}(Y_{n e w}^{\\prime}+n_{Y}^{n e w}=y|M)=\\int_{k=-\\infty}^{\\infty}\\bar{Q}_{n e w}(Y_{n e w}^{\\prime}=k,n_{Y}^{n e w}=y-k|M).\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "Using conditional dependence of $Y_{n e w}^{\\prime}$ and $n_{Y}^{n e w}$ , we have $\\bar{Q}_{n e w}(Y_{n e w}^{\\prime}=k,n_{Y}^{n e w}=y-k|M)=$ $\\bar{Q}_{n e w}(Y_{n e w}^{\\prime}=k|M)\\bar{Q}_{n e w}(n_{Y}^{n e w}=\\overleftarrow{y}-k|M)\\bar{)}$ : ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\bar{Q}_{n e w}(Y_{n e w}^{\\prime}+n_{Y}^{n e w}=y|M)=\\int_{k=-\\infty}^{\\infty}\\bar{Q}_{n e w}(Y_{n e w}^{\\prime}=k|M)\\bar{Q}_{n e w}(n_{Y}^{n e w}=y-k|M).\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "Note that the R.H.S of the above equation defines a convolution of $\\bar{Q}_{n e w}(Y_{n e w}^{\\prime}|M)$ and $\\bar{Q}(n_{Y}^{n e w}|M)$ hence: ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\bar{Q}_{n e w}(Y_{n e w}^{\\prime}+n_{Y}^{n e w}=y|M)=\\bar{Q}_{n e w}(Y_{n e w}^{\\prime}=k|M)*\\bar{Q}_{n e w}(n_{Y}^{n e w}=y-k|M),\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "where $^*$ denotes the convolution operator. Substituting (262) in the above equation: ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\bar{Q}_{n e w}(Y_{n e w}^{\\prime}+n_{Y}^{n e w}{=}y|M{=}m)=f(\\epsilon_{m}^{(2)}\\delta_{m}^{Y})*f((1-\\epsilon_{m}^{(2)})\\delta_{m}^{Y}).\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "Using the properties of convolution-closed distributions: ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{Q}_{n e w}(Y_{n e w}^{\\prime}+n_{Y}^{n e w}\\!=\\!y|M\\!=\\!m)=f(\\epsilon_{m}^{(2)}\\delta_{m}^{Y}+(1-\\epsilon_{m}^{(2)})\\delta_{m}^{Y})=f(\\delta_{m}^{Y}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "Hence, by comparing above equation with (261) we have: ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\bar{Q}_{n e w}(Y_{n e w}^{\\prime}+n_{Y}^{n e w}=y|M=m)=Q(Y=y|M=m).\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "Property 2: $\\bar{Q}(Y_{n e w}^{\\prime}\\!=\\!y^{\\prime}|Y_{n e w}^{\\prime}+n_{Y}^{n e w}\\!=\\!y,M\\!=\\!m)=G(\\epsilon_{m}^{(2)}\\delta_{m}^{Y},1-\\epsilon_{m}^{(2)}\\delta_{m}^{Y},y)$ By definition of $G(\\epsilon_{m}^{(2)}\\delta_{m}^{Y},(1-\\epsilon_{m}^{(2)})\\delta_{m}^{Y},z)$ given in Sec. 5.1, we have: ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{G(\\epsilon_{m}^{(2)}\\delta_{m}^{Y},(1-\\epsilon_{m}^{(2)})\\delta_{m}^{Y},z)=\\operatorname*{Pr}(Z_{\\epsilon_{m}^{(2)}}|Z_{\\epsilon_{m}^{(2)}}+Z_{1-\\epsilon_{m}^{(2)}}=z),\\mathrm{~where~}}\\\\ &{\\operatorname*{Pr}(Z_{\\epsilon_{m}^{(2)}})=f(\\epsilon_{m}^{(2)}\\delta_{m}^{Y}),\\operatorname*{Pr}(Z_{1-\\epsilon_{m}^{(2)}})=f((1-\\epsilon_{m}^{(2)})\\delta_{m}^{Y}),\\mathrm{~and~}Z_{\\epsilon_{m}^{(2)}}\\mathrm{~}\\perp\\!\\!\\!\\perp Z_{1-\\epsilon_{m}^{(2)}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "In the following steps, we slightly modify the notation of $G(\\epsilon_{m}^{(2)}\\delta_{m}^{Y},(1\\ -\\ \\epsilon_{m}^{(2)})\\delta_{m}^{Y},z)$ to $G(z_{\\epsilon_{m}^{(2)}};\\epsilon_{m}^{(2)}\\delta_{m}^{Y},(1-\\epsilon_{m}^{(2)})\\delta_{m}^{Y},z)$ to make our notations more explicit as follows: ", "page_idx": 65}, {"type": "equation", "text": "$$\nG(z_{\\epsilon_{m}^{(2)}};\\epsilon_{m}^{(2)}\\delta_{m}^{Y},(1-\\epsilon_{m}^{(2)})\\delta_{m}^{Y},z)=\\operatorname*{Pr}(Z_{\\epsilon_{m}^{(2)}}=z_{\\epsilon_{m}^{(2)}}|Z_{\\epsilon_{m}^{(2)}}+Z_{1-\\epsilon_{m}^{(2)}}=z).\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "We use the definition of the conditional distribution to express $G(\\epsilon_{m}^{(2)}\\delta_{m}^{Y},(1-\\epsilon_{m}^{(2)})\\delta_{m}^{Y},z)$ ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{G(z_{\\epsilon_{m}^{(2)}};\\epsilon_{m}^{(2)}\\delta_{m}^{Y},(1-\\epsilon_{m}^{(2)})\\delta_{m}^{Y},z)=\\frac{\\operatorname*{Pr}(Z_{\\epsilon}=z_{\\epsilon_{m}^{(2)}},Z_{\\epsilon_{m}^{(2)}}+Z_{1-\\epsilon_{m}^{(2)}}=z)}{\\operatorname*{Pr}(Z_{\\epsilon_{m}^{(2)}}+Z_{1-\\epsilon_{m}^{(2)}}=z)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\frac{\\operatorname*{Pr}(Z_{\\epsilon}=z_{\\epsilon_{m}^{(2)}})\\operatorname*{Pr}(Z_{\\epsilon_{m}^{(2)}}+Z_{1-\\epsilon_{m}^{(2)}}=z|Z_{\\epsilon_{m}^{(2)}}=z_{\\epsilon_{m}^{(2)}})}{\\operatorname*{Pr}(Z_{\\epsilon_{m}^{(2)}}+Z_{1-\\epsilon_{m}^{(2)}}=z)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": ":hat $\\mathrm{Pr}(Z_{\\epsilon_{m}^{(2)}}\\,+\\,Z_{1-\\epsilon_{m}^{(2)}}\\,=\\,z|Z_{\\epsilon_{m}^{(2)}}\\,=\\,z_{\\epsilon_{m}^{(2)}})\\,=\\,\\mathrm{Pr}(Z_{1-\\epsilon_{m}^{(2)}}\\,=\\,z\\,-\\,z_{\\epsilon_{m}^{(2)}})$ , since $Z_{\\epsilon_{m}^{(2)}}\\perp Z_{1-\\epsilon_{m}^{(2)}}$ ", "page_idx": 65}, {"type": "equation", "text": "$$\n=\\frac{\\operatorname*{Pr}(Z_{\\epsilon}=z_{\\epsilon_{m}^{(2)}})\\operatorname*{Pr}(Z_{1-\\epsilon_{m}^{(2)}}=z-z_{\\epsilon_{m}^{(2)}})}{\\operatorname*{Pr}(Z_{\\epsilon_{m}^{(2)}}+Z_{1-\\epsilon_{m}^{(2)}}=z)}.\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "Note that by properties of convolution-closed distributions, $\\mathrm{Pr}(Z_{\\epsilon_{m}^{(2)}}+Z_{1-\\epsilon_{m}^{(2)}}=z))=f(z;\\delta_{m}^{Y})$ , where $f(z;\\delta_{m}^{Y})$ is to be understood as the distribution $f(\\delta_{m}^{Y})$ evaluated at $z$ . Substituting (269) in the above equation: ", "page_idx": 65}, {"type": "equation", "text": "$$\n=\\frac{f(z_{\\epsilon_{m}^{(2)}};\\epsilon_{m}^{(2)}\\delta_{m}^{Y})f(z-z_{\\epsilon_{m}^{(2)}};(1-\\epsilon_{m}^{(2)})\\delta_{m}^{Y})}{f(z;\\delta_{m}^{Y})}.\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "Now, note that by (262) and (267), we have: ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\ \\bar{Q}_{n e w}(Y_{n e w}^{\\prime}\\!=\\!z_{\\epsilon}|M\\!=\\!m)=f(z_{\\epsilon_{m}^{(2)}};\\epsilon_{m}^{(2)}\\delta_{m}^{Y}),}\\\\ &{\\quad\\ \\bar{Q}_{n e w}(n_{Y}^{n e w}\\!=\\!z-z_{\\epsilon}|M\\!=\\!m)=f(z-z_{\\epsilon_{m}^{(2)}};(1-\\epsilon_{m}^{(2)})\\delta_{m}^{Y}),\\ \\mathrm{and}}\\\\ &{\\bar{Q}_{n e w}(Y_{n e w}^{\\prime}\\!+\\!n_{Y}^{n e w}\\!=\\!z|M\\!=\\!m)=f(z;\\delta_{m}^{Y})\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "Hence, combining the above equation with (274), and renaming $z_{\\epsilon_{m}^{(2)}}$ as $y^{\\prime}$ and $z$ as $y$ , we have: ", "page_idx": 65}, {"type": "equation", "text": "$$\nG(\\epsilon_{m}^{(2)}\\delta_{m}^{Y},(1-\\epsilon_{m}^{(2)})\\delta_{m}^{Y},y)=\\frac{\\bar{Q}_{n e w}(Y_{n e w}^{\\prime}\\!=\\!y^{\\prime}|M\\!=\\!m)\\bar{Q}_{n e w}(n_{Y}^{n e w}\\!=\\!y-y^{\\prime}|M\\!=\\!m)}{\\bar{Q}_{n e w}(Y_{n e w}^{\\prime}+n_{Y}^{n e w}\\!=\\!y|M\\!=\\!m)}.\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "Noting the fact that $\\bar{Q}_{n e w}(n_{Y}^{n e w}\\!=\\!y-y^{\\prime}|M\\!=\\!m)=\\bar{Q}_{n e w}(Y_{n e w}^{\\prime}+n_{Y}^{n e w}\\!=\\!y|M\\!=\\!m,Y_{n e w}^{\\prime}=y^{\\prime})$ ) due to $Y_{n e w}^{\\prime}$ \u22a5\u22a5 $n_{Y}^{n e w}|M$ and abbreviating notation pertaining to $y^{\\prime}$ , we have: ", "page_idx": 66}, {"type": "equation", "text": "$$\nG(\\epsilon_{m}^{(2)}\\delta_{m}^{Y},(1-\\epsilon_{m}^{(2)})\\delta_{m}^{Y},y)=\\frac{\\bar{Q}_{n e w}(Y_{n e w}^{\\prime}|M\\!=\\!m)\\bar{Q}_{n e w}(Y_{n e w}^{\\prime}+n_{Y}^{n e w}\\!=\\!y|M\\!=\\!m)}{\\bar{Q}_{n e w}(Y_{n e w}^{\\prime}+n_{Y}^{n e w}\\!=\\!y|M\\!=\\!m)}.\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "Using Bayes rule in the above equation provides us with our desired property: ", "page_idx": 66}, {"type": "equation", "text": "$$\nG(\\epsilon_{m}^{(2)}\\delta_{m}^{Y},(1-\\epsilon_{m}^{(2)})\\delta_{m}^{Y},y)=\\bar{Q}_{n e w}(Y_{n e w}^{\\prime}|M\\!=\\!m,Y_{n e w}^{\\prime}+n_{Y}^{n e w}\\!=\\!y)\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "Furthermore, by comparing (259) with the above equation, we have: ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\bar{Q}_{n e w}(Y_{n e w}^{\\prime}=y^{\\prime}|Y_{n e w}^{\\prime}+n_{Y}^{n e w}=y,M=m)=\\bar{Q}(Y^{\\prime}=y^{\\prime}|Y=y,M=m).\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "Observation 2: Note that the distribution $\\bar{Q}(Y_{n e w}^{\\prime},n_{Y}^{n e w},Y_{n e w}^{\\prime}+n_{Y}^{n e w}|M)$ is also completely determined by specifying $\\bar{Q}(Y_{n e w}^{\\prime}+n_{Y}^{n e w}|M)$ and $\\bar{Q}(Y_{n e w}^{\\prime}|Y_{n e w}^{\\prime}+n_{Y}^{n e w},M)$ since $n_{Y}^{n e w}$ is completely determined by $Y_{n e w}^{\\prime}+n_{Y}^{n e w}$ and $Y_{n e w}^{\\prime}$ . ", "page_idx": 66}, {"type": "text", "text": "By property 1 and (279), we know that ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\bar{Q}_{n e w}(Y_{n e w}^{\\prime}+n_{Y}^{n e w}|M)=Q(Y|M),\\mathrm{~and~}\\bar{Q}_{n e w}(Y_{n e w}^{\\prime}|Y_{n e w}^{\\prime}+n_{Y}^{n e w},M)=\\bar{Q}(Y^{\\prime}|Y,M).\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "Therefore, combining the above equation with observations 1 and 2,we have: ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\bar{Q}(Y^{\\prime},Y,n_{Y}|M)=\\bar{Q}(Y_{n e w}^{\\prime},Y_{n e w}^{\\prime}+n_{Y}^{n e w},n_{Y}^{n e w}|M).\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "Consequently the distribution $\\bar{Q}(Y^{\\prime},n_{Y}|M)=\\bar{Q}_{n e w}(Y_{n e w}^{\\prime},n_{Y}^{n e w}|M)$ . Since, $Y_{n e w}^{\\prime}\\perp\\!\\!\\!\\perp n_{Y}^{n e w}|M,$ we also have $Y^{\\prime}\\perp\\!\\!\\!\\perp n_{Y}|M$ . Furthermore, by (262), ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\bar{Q}_{n e w}(Y_{n e w}^{\\prime}|M\\!=\\!m)=\\bar{Q}(Y^{\\prime}|M)=f(\\epsilon_{m}^{(2)}\\delta_{m}^{Y})\\,\\forall\\,m\\in\\mathcal{M},\\mathrm{~and}}\\\\ &{}&{\\bar{Q}_{n e w}(n_{Y}^{n e w}|M\\!=\\!m)=\\bar{Q}(n_{Y}|M)=f((1-\\epsilon_{m}^{(2)})\\delta_{m}^{Y})\\,\\forall\\,m\\in\\mathcal{M}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "By (258), we know that $\\epsilon_{m}^{(2)}\\delta_{m}^{Y}=\\delta_{m}^{Y}-\\delta_{b i a s}^{Y}$ and $(1-\\epsilon_{m}^{(2)})\\delta_{m}^{Y}=\\delta_{b i a s}^{Y}$ . Hence, ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\bar{Q}(Y^{\\prime}|M)=f(\\epsilon_{m}^{(2)}\\delta_{m}^{Y})=f(\\delta_{m}^{Y}-\\delta_{b i a s}^{Y})\\,\\forall\\,m\\in\\mathcal{M},\\;\\mathrm{and}}\\\\ &{}&{\\bar{Q}(n_{Y}|M)=f((1-\\epsilon_{m}^{(2)})\\delta_{m}^{Y})=f(\\delta_{b i a s}^{Y})\\,\\forall\\,m\\in\\mathcal{M}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "Proposition 9. Suppose the random variables $M$ and $X$ have the joint distribution $Q(M,X)$ , such that ", "page_idx": 66}, {"type": "equation", "text": "$$\nQ(M)=P(M)\\;a n d\\;Q(X|M\\!=\\!m)=f(\\delta_{m}^{X})\\;\\forall\\;m\\in\\mathcal{M},\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "where $\\mathcal{M}$ denotes the support of $M$ and $f(\\delta_{m}^{X})\\in\\mathcal{F}_{\\mathcal{D}}$ for some convolution-closed distribution family $\\mathcal{F}_{\\mathcal{D}}$ . Furthermore, there also exist $\\delta_{b i a s}^{X}\\in\\ddot{D}$ , and $\\delta_{m}^{Y}\\in\\mathcal{D}\\,\\forall\\,m\\in\\mathcal{M}$ such that ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta_{m}^{X}-\\delta_{b i a s}^{X}=\\epsilon_{m}^{(1)}(\\delta_{m}^{Y}-\\delta_{b i a s}^{Y})\\,f o r\\,s o m e\\,\\epsilon_{m}^{(1)}\\in[0,1],}\\\\ &{\\delta_{m}^{X}=\\epsilon_{m}^{(3)}(\\delta_{m}^{X}-\\delta_{b i a s}^{X})\\,f o r\\,s o m e\\,\\epsilon_{m}^{(3)}\\in[0,1],\\,a n d}\\\\ &{(\\delta_{m}^{X}-\\delta_{m}^{Y}-(\\delta_{b i a s}^{X}-\\delta_{b i a s}^{Y})),\\;(\\delta_{m}^{X}-\\delta_{b i a s}^{X}),\\;(\\delta_{m}^{Y}-\\delta_{b i a s}^{Y})\\in\\mathcal{D}\\,\\forall\\,m\\in\\mathcal{M}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "Then consider three dilated versions of $X$ , denoted as $X^{\\prime},Y^{\\prime\\prime}$ , and $n_{X}$ , defined as follows: ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{Q}(X^{\\prime},Y^{\\prime\\prime},n_{X}|X,M)=\\bar{Q}(n_{X}|X,M)\\bar{Q}(X^{\\prime}|X,n_{X},M)\\bar{Q}(Y^{\\prime\\prime}|X,n_{X},M,X^{\\prime}),\\;w h e r e=0,\\;w h e r e=0,\\;w h e r e=1,\\;w h e r e=0,\\;w h e r e=1,\\;w h e r e=0,\\;w h e r e=1,\\;w h e r e=1,\\;w h e r e=0,\\;w h e r e=1,\\;w h e r e=1,\\;w h e r e=1,\\;w h e r e=1,\\;w h e r e=1,\\;w h e r e=1,\\;w h e r e=1,\\;w h e r e=1,\\;w h e r e=1,\\;w h e r e=1,\\;w h e r e=1,\\;w h e r e=1,\\;w h e r e=1.}\\\\ {\\bar{Q}(X^{\\prime}|X=x,n_{X}=x_{n X},M=m)=G((1-\\epsilon_{m}^{(1)})(\\delta_{m}^{X}-\\delta_{b i a s}^{X}),\\epsilon_{m}^{(1)}(\\delta_{m}^{X}-\\delta_{b i a s}^{X}),x=n_{X}),\\;a n d\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;}\\end{array}\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "Then, we have ", "page_idx": 66}, {"type": "text", "text": "1. $(n_{X},X^{\\prime},Y^{\\prime\\prime})$ are jointly conditionally independent conditioned on $M$ . ", "page_idx": 66}, {"type": "text", "text": "2. The conditional distributions $\\bar{Q}(X^{\\prime}|M),\\,\\bar{Q}(Y^{\\prime\\prime}|M)$ , and $\\bar{Q}(n_{X}|M)$ are specified as follows: ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{Q}(X^{\\prime}|M)=f(\\delta_{m}^{X}-\\delta_{m}^{Y}-(\\delta_{b i a s}^{X}-\\delta_{b i a s}^{Y})),}\\\\ &{\\bar{Q}(Y^{\\prime\\prime}|M)=f(\\delta_{m}^{X}-\\delta_{b i a s}^{Y}),\\;a n d}\\\\ &{\\bar{Q}(n_{X}|M)=f(\\delta_{b i a s}^{X}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "Proof. The proof essentially consists of recursively applying the proof of proposition 8. Consider a new auxiliary random variable $Z\\,=\\,X\\,-\\,n_{X}$ . Then, the distribution $\\dot{\\bar{Q}}(\\dot{X^{}},n_{X},Z|M)$ has the following properties: ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\bar{Q}(X|M)\\stackrel{(a)}{=}f(\\delta_{m}^{X}),\\bar{Q}(n_{X}|M\\!=\\!m,X\\!=\\!x)\\stackrel{(b)}{=}G((1-\\epsilon_{m}^{(3)})\\delta_{m}^{X},\\epsilon_{m}^{(3)}\\delta_{m}^{X},x),\\mathrm{~and~}}\\\\ &{}&{\\bar{Q}(Z|X\\!=\\!x,n_{X}\\!=\\!x_{n_{X}},M=m)=\\delta_{K}(Z=x-x_{n_{X}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "where $\\delta_{K}(\\cdot)$ is the Kronecker delta function, and the equalities $(a)$ and $(b)$ follow from (284) and (285), respectively. Note that the distribution $\\bar{Q}(X,n_{X},Z|M)$ has exactly the same structure as $\\bar{Q}(Y,Y^{\\prime},n_{Y}|\\bar{M})$ described in (261), hence $Z$ and $n_{Y}$ are just dilated versions of $X$ . Consequently, from the result of proposition 8, we know that $Z$ \u22a5\u22a5 $n_{X}|M$ , and the conditional distributions $\\bar{Q}(Z|M)$ and $\\bar{Q}(n_{Y}|M)$ are described as follows: ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{Q}(n_{X}|M\\!=\\!m)=f\\big((1-\\epsilon_{m}^{(3)})\\delta_{m}^{X}\\big)\\mathrm{,~and}\\;\\bar{Q}(Z|M\\!=\\!m)=f\\big(\\epsilon_{m}^{(3)}\\delta_{m}^{X}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "From (285), we have $\\epsilon_{m}^{(3)}\\delta_{m}^{X}=\\delta_{m}^{X}-\\delta_{b i a s}^{X}$ and $(1-\\epsilon_{m}^{(3)})\\delta_{m}^{X}=\\delta_{b i a s}^{X}$ . Consequently, ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{Q}(n_{X}|M\\!=\\!m)=f(\\delta_{b i a s}^{X}),\\;\\mathrm{and}\\;\\bar{Q}(Z|M\\!=\\!m)=f(\\delta_{m}^{X}-\\delta_{b i a s}^{X}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "We alternatively express $\\tilde{Q}(X^{\\prime}|n_{X},X,M)$ as follows: ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{Q}(X^{\\prime}|n_{X}\\!=\\!x_{n_{X}},X\\!=\\!x,M\\!=\\!m)=G\\big((1-\\epsilon_{m}^{(1)})(\\delta_{m}^{X}-\\delta_{b i a s}^{X}),\\epsilon_{m}^{(1)}(\\delta_{m}^{X}-\\delta_{b i a s}^{X}),x-x_{n_{X}}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "Note that the distribution $\\bar{Q}(X^{\\prime}|n_{X}{=}x_{n_{X}},X{=}x,M{=}m)$ only depends on $X$ and $n_{X}$ through their difference $X-n_{X}$ . Hence, substituting $Z=X-n_{X}$ in the above equation: ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{Q}(X^{\\prime}|n_{X}\\!=\\!x_{n_{X}},X\\!=\\!x,M\\!=\\!m)=\\bar{Q}(X^{\\prime}|Z\\!=\\!x-x_{n_{X}},M\\!=\\!m)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=G((1-\\epsilon_{m}^{(1)})(\\delta_{m}^{X}-\\delta_{b i a s}^{X}),\\epsilon_{m}^{(1)}(\\delta_{m}^{X}-\\delta_{b i a s}^{X}),x-n_{X}),}\\\\ &{\\qquad\\qquad\\Rightarrow\\bar{Q}(X^{\\prime}|Z\\!=\\!z,M\\!=\\!m)=G((1-\\epsilon_{m}^{(1)})(\\delta_{m}^{X}-\\delta_{b i a s}^{X}),\\epsilon_{m}^{(1)}(\\delta_{m}^{X}-\\delta_{b i a s}^{X}),z).}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "Hence, consider the following distribution ${\\bar{Q}}(Z,X^{\\prime},Y^{\\prime\\prime}|M)$ , where from (290) and (293), we have: ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{Q}(Z|M\\!=\\!m)=f(\\delta_{m}^{X}-\\delta_{b i a s}^{X}),\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}\\\\ {\\bar{Q}(X^{\\prime}|Z=z,M=m)=G((1-\\epsilon_{m}^{(1)})(\\delta_{m}^{X}-\\delta_{b i a s}^{X}),\\epsilon_{m}^{(1)}(\\delta_{m}^{X}-\\delta_{b i a s}^{X}),z).}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "Furthermore, by (286), we have $Y^{\\prime\\prime}=X-n_{X}-X^{\\prime}\\overset{\\left(c\\right)}{=}Z-X^{\\prime}$ , where $(c)$ is due to $Z=X-n_{X}$ . Hence, ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\bar{Q}(Y^{\\prime\\prime}|Z{=}z,M,X^{\\prime}{=}x)=\\delta_{K}(Y^{\\prime\\prime}=z-x).\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "Comparing (295) and (294) with (261), we can again conclude that $X^{\\prime\\prime}$ and $Y^{\\prime}$ are just dilated versions of $Z$ . Hence, again using the result of proposition 8, we have $X^{\\prime}\\perp\\!\\!\\!\\perp Y^{\\prime\\prime}|M$ , and ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{Q}(X^{\\prime}|M\\!=\\!m)=f\\big(\\!(1-\\epsilon_{m}^{(1)})\\!\\big(\\delta_{m}^{X}-\\delta_{b i a s}^{X})\\big),\\mathrm{~and~}\\bar{Q}(Y^{\\prime\\prime}|M\\!=\\!m)=f\\big(\\epsilon_{m}^{(1)}(\\delta_{m}^{X}-\\delta_{b i a s}^{X})\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "From (285), we have $\\epsilon_{m}^{(1)}(\\delta_{m}^{X}-\\delta_{b i a s}^{X})\\,=\\,\\delta_{m}^{Y}\\,-\\,\\delta_{b i a s}^{Y}$ and $\\bigl(1-\\epsilon_{m}^{(1)}\\bigr)\\bigl(\\delta_{m}^{X}-\\delta_{b i a s}^{Y}\\bigr)\\,=\\,\\delta_{m}^{X}\\,-\\,\\delta_{M}^{Y}\\,-\\,$ . Consequently, ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{Q}(X^{\\prime}|M\\!=\\!m)=f(\\delta_{m}^{X}-\\delta_{m}^{Y}-(\\delta_{b i a s}^{X}-\\delta_{b i a s}^{Y}),\\mathrm{~and~}\\bar{Q}(Y^{\\prime\\prime}|M\\!=\\!m)=f(\\delta_{m}^{Y}-\\delta_{b i a s}^{Y}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "Hence, by (290) and (297), we can conclude that the conditional distributions $\\bar{Q}(X^{\\prime}|M),\\bar{Q}(Y^{\\prime\\prime}|M)$ , and $\\bar{Q}(n\\dot{\\boldsymbol{x}}|M)$ are specified as follows: ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{Q}(X^{\\prime}|M=m)=f(\\delta_{m}^{X}-\\delta_{m}^{Y}-(\\delta_{b i a s}^{X}-\\delta_{b i a s}^{Y})),}\\\\ &{\\quad\\quad\\bar{Q}(Y^{\\prime\\prime}|M)=f(\\delta_{m}^{X}-\\delta_{b i a s}^{Y}),\\mathrm{~and~}}\\\\ &{\\quad\\quad\\bar{Q}(n_{X}|M)=f(\\delta_{b i a s}^{X}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "Showing joint conditional independence of $(n_{Y},X^{\\prime},Y^{\\prime\\prime})$ given $M$ : ", "page_idx": 68}, {"type": "text", "text": "Consider the distribution $\\bar{Q}(n_{X},Z,X^{\\prime},Y^{\\prime\\prime}|M)$ . Using the chain rule, we have: ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{Q}(n_{X},Z,X^{\\prime},Y^{\\prime\\prime}|M)=\\bar{Q}(n_{X},Z|M)Q(X^{\\prime},Y^{\\prime\\prime}|M,n_{X},Z),}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\overset{(b)}{=}\\bar{Q}(n_{X}|M)\\bar{Q}(Z|M)Q(X^{\\prime},Y^{\\prime\\prime}|M,n_{X},Z),}\\\\ &{\\qquad\\qquad\\qquad\\overset{(c)}{=}\\bar{Q}(n_{X}|M)\\bar{Q}(Z|M)Q(X^{\\prime},Y^{\\prime\\prime}|M,Z),}\\end{array}\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "where $(b)$ follows from $n_{X}$ \u22a5\u22a5 $Z|M$ . For showing $(c)$ , observe that $Y^{\\prime\\prime}=Z-X^{\\prime}$ , and hence we can write $\\bar{Q}(Y^{\\prime\\prime}|M,Z,n_{Y},X^{\\prime})=\\bar{Q}(Y^{\\prime\\prime}|Z,X^{\\prime})$ . Furthermore, by analyzing $\\bar{Q}(X^{\\prime}|n_{X},Z,M)$ , we can conclude: ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\bar{Q}(X^{\\prime}|n_{X},Z,M)=\\bar{Q}(X^{\\prime}|n_{X},X-n_{X},M)=\\bar{Q}(X^{\\prime}|n_{X},X,M)=\\bar{Q}(X^{\\prime}|Z,M).\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "Marginalizing $Z$ out of (299), we obtain: ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\bar{Q}(n_{X},X^{\\prime},Y^{\\prime\\prime}|M)=\\bar{Q}(n_{X}|M)\\bar{Q}(X^{\\prime},Y^{\\prime\\prime}|M)\\overset{(d)}{=}\\bar{Q}(n_{X}|M)Q(X^{\\prime}|M)\\bar{Q}(Y^{\\prime\\prime}|M),\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "where $(d)$ follows from $X^{\\prime}\\perp\\!\\!\\!\\perp Y^{\\prime\\prime}|M$ . By (301), we know that $(n_{X},X^{\\prime\\prime},Y^{\\prime})$ are jointly conditionally independent given $M$ , concluding our proof. ", "page_idx": 68}, {"type": "text", "text": "M.2 Analytically optimizing the upper bound ", "text_level": 1, "page_idx": 68}, {"type": "text", "text": "Proposition 10. Let ${\\bar{Q}}(M,X^{\\prime},Y^{\\prime\\prime},n_{X},Y^{\\prime},n_{Y})$ be the distribution derived from a distribution $Q\\in\\Delta_{P}$ using the construction scheme specified in Appx. M.1. Then, the minimization problem specified in (302) is analytically solvable with the optimizing $\\bar{Q}^{*}$ given by (303). ", "page_idx": 68}, {"type": "equation", "text": "$\\bar{Q}$ $$\n\\begin{array}{r l}&{\\underset{Q\\in\\Delta_{P}}{\\operatorname*{mun}}\\ \\ I_{\\bar{Q}}(M;|\\boldsymbol{X}^{\\prime},\\boldsymbol{Y}^{\\prime},\\boldsymbol{n}_{X},\\boldsymbol{Y}^{\\prime},\\boldsymbol{n}_{Y}|),\\ (n o t e\\,t h a t\\,Q\\,v\\ s\\,a\\,f u n c t{\\iota o n\\,o f\\,}Q)}\\\\ &{\\bar{Q}^{*}(M,X^{\\prime},Y^{\\prime\\prime},n_{X},\\boldsymbol{n}_{Y},Y^{\\prime})=\\bar{Q}^{*}(M)\\bar{Q}^{*}(X^{\\prime}|M)\\bar{Q}^{*}(Y^{\\prime\\prime}|M)\\bar{Q}^{*}(n_{X})\\bar{Q}^{*}(n_{Y})\\bar{Q}^{*}(Y^{\\prime}|Y^{\\prime\\prime})}\\\\ &{\\bar{Q}^{*}(M)=P(M),\\bar{Q}^{*}(X^{\\prime}|M{=}m)=f(\\delta_{m}^{X}-\\delta_{m}^{Y}-(\\delta_{b i a s}^{X}-\\delta_{b i a s}^{Y})),\\bar{Q}^{*}(n_{X})=f(\\delta_{b i a s}^{X}),}\\\\ &{\\bar{Q}^{*}(Y^{\\prime\\prime}|M{=}m)=f(\\delta_{m}^{Y}-\\delta_{b i a s}^{Y}),\\bar{Q}^{*}(n_{Y})=f(\\delta_{b i a s}^{Y}),\\ a n d\\,\\bar{Q}^{*}(Y^{\\prime}|Y^{\\prime\\prime})=\\mathbb{I}[Y^{\\prime\\prime}=Y^{\\prime}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "where $\\mathbb{I}[\\cdot]$ is the indicator function. Note that for the minimizing $\\bar{Q}^{*}$ , we have $(n_{X},n_{Y},(M,X^{\\prime},Y^{\\prime\\prime}))$ are jointly independent and $Y^{\\prime}=Y^{\\prime\\prime}$ . ", "page_idx": 68}, {"type": "text", "text": "Proof. First note that by the construction scheme defined in Appx. M.1, we know that there is a unique $\\bar{Q}$ for every $Q\\in\\Delta_{P}$ . Hence, we can interpret $\\bar{Q}$ as a function of $Q$ , and consequently the minimization problem in (302) is well-defined. Using the chain rule of mutual information, we derive the following lower bound for $I_{\\bar{Q}}(M;[X^{\\prime},Y^{\\prime\\prime},n_{X},\\bar{Y}^{\\prime},n_{Y}])$ : ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ \\ I_{\\bar{Q}}(M;[X^{\\prime},Y^{\\prime\\prime},n_{X},Y^{\\prime},n_{Y}])=I_{\\bar{Q}}(M;[X^{\\prime},Y^{\\prime\\prime}])+I_{\\bar{Q}}(M;[n_{X},Y^{\\prime},n_{Y}]|[X^{\\prime},Y^{\\prime}])}\\\\ &{\\Rightarrow\\!I_{\\bar{Q}}(M;[X^{\\prime},Y^{\\prime\\prime},n_{X},Y^{\\prime},n_{Y}])\\stackrel{(a)}{\\geq}I_{\\bar{Q}}(M;[X^{\\prime},Y^{\\prime\\prime}]),}\\end{array}\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "where $(a)$ is due to the non-negativity of conditional mutual information. Furthermore, note that by properties of $\\bar{Q}$ derived in Appx. M.1, we know that $X^{\\prime}\\perp\\perp Y^{\\prime\\prime}|M\\,\\forall\\,Q\\in\\Delta_{P}$ . Consequently, the term $I_{\\bar{Q}}(M;[X^{\\prime},Y^{\\prime\\prime}])$ does note vary as we choose different $Q$ from $\\Delta_{P}$ as the corresponding $\\bar{Q}(M,X^{\\prime},\\bar{Y}^{\\prime\\prime})=\\bar{Q}(M)\\bar{Q}(X^{\\prime}|M)\\bar{Q}(Y^{\\prime\\prime}|M)$ and the terms: ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{Q}(M)=Q(M)=P(M),\\bar{Q}(X^{\\prime}|M\\!=\\!m)=f(\\delta_{m}^{X}-\\delta_{m}^{Y}-(\\delta_{b i a s}^{X}-\\delta_{b i a s}^{Y})),\\mathrm{~and~}}\\\\ &{\\bar{Q}(Y^{\\prime\\prime}|M)=f(\\delta_{m}^{Y}-\\delta_{b i a s}^{Y})}\\end{array}\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "do not vary for all $Q\\in\\Delta_{P}$ (see Appx. M.1). Therefore, we have: ", "page_idx": 69}, {"type": "equation", "text": "$$\nI(M;[X^{\\prime},Y^{\\prime\\prime}])\\leq I_{\\bar{Q}}(M;[X^{\\prime},Y^{\\prime\\prime},n_{X},Y^{\\prime},n_{Y}])\\,\\forall\\,Q\\in\\Delta_{P},\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "where we forgo the subscript $\\bar{Q}$ for $I_{\\bar{Q}}(M;[X^{\\prime\\prime},Y^{\\prime}])$ to indicate that the term $I_{\\bar{Q}}(M;[X^{\\prime\\prime},Y^{\\prime}])$ does not vary over $\\Delta_{P}$ . Hence, if we show that the distribution $\\bar{Q}^{*}(M,X^{\\prime},\\dot{Y^{\\prime\\prime}},n_{X},Y^{\\prime},n_{Y})$ specified in (303) achieves the lower bound in (307), then ${\\bar{Q}}^{*}(M,X^{\\prime},Y^{\\prime\\prime},n_{X},Y^{\\prime},n_{Y})$ should minimize the value of $I_{\\bar{Q}}(M;[X^{\\prime},Y^{\\prime\\prime},n_{X},Y^{\\prime},n_{Y}])$ over the set $\\Delta_{P}$ . We now calculate $I_{\\bar{Q}^{\\ast}}(M;[X^{\\prime},Y^{\\prime\\prime},n_{X},Y^{\\prime},n_{Y}^{\\prime}])$ . By the chain rule of mutual information: ", "page_idx": 69}, {"type": "equation", "text": "$$\nI_{\\bar{Q}^{*}}(M;[X^{\\prime},Y^{\\prime\\prime},n_{X},Y^{\\prime},n_{Y}])=I_{\\bar{Q}^{*}}(M;[n_{X},n_{Y},X^{\\prime},Y^{\\prime\\prime}])+I_{\\bar{Q}^{*}}(M;Y^{\\prime}|n_{X},n_{Y},X,Y^{\\prime\\prime}).\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "Using the fact that $Y^{\\prime}\\;\\;=\\;\\;Y^{\\prime\\prime}$ , we know that $I_{\\bar{Q}^{\\ast}}(M;Y^{\\prime}|n_{X},n_{Y},X,Y^{\\prime\\prime})$ is zero. A simple justification follows from the fact $I_{\\bar{Q}^{*}}(M;Y^{\\prime}|n_{X},n_{Y},X,Y^{\\prime\\prime})\\;=\\;H(Y^{\\prime}|n_{X},n_{Y},X$ , Y \u2032\u2032) \u2212 $H(Y^{\\prime}|n_{X},n_{Y},X,Y^{\\prime\\prime},M)$ , where $H(.|.)$ is the conditional entropy function [28]. Observe that both conditional entropy terms are zero as $Y^{\\prime}=Y^{\\prime\\prime}$ , and condition entropy of a random variable conditioned on itself is zero. Therefore, we can simplify the above equation as: ", "page_idx": 69}, {"type": "equation", "text": "$$\nI_{\\bar{Q}^{*}}(M;[X^{\\prime},Y^{\\prime\\prime},n_{X},Y^{\\prime},n_{Y}])=I_{\\bar{Q}^{*}}(M;[n_{X},n_{Y},X^{\\prime},Y^{\\prime\\prime}]).\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "Applying the chain rule of mutual information again, we obtain: ", "page_idx": 69}, {"type": "equation", "text": "$$\nI_{\\bar{Q}^{*}}(M;[X^{\\prime},Y^{\\prime\\prime},n_{X},Y^{\\prime},n_{Y}])=I_{\\bar{Q}^{*}}(M;[n_{X},n_{Y}])+I_{\\bar{Q}^{*}}(M;[X^{\\prime},Y^{\\prime\\prime}]|n_{X},n_{Y}).\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "Using the fact that $(n_{X},n_{Y})$ \u22a5\u22a5 $(M,X^{\\prime},Y^{\\prime\\prime})$ , we know that $I(M;[n_{X},n_{Y}])\\;=\\;0$ as mutual information between independent random variables is zero [28]. Consequently, ", "page_idx": 69}, {"type": "equation", "text": "$$\nI_{\\bar{Q}^{*}}(M;[X^{\\prime},Y^{\\prime\\prime},n_{X},Y^{\\prime},n_{Y}])=I_{\\bar{Q}^{*}}(M;[X^{\\prime},Y^{\\prime\\prime}]|n_{X},n_{Y})\\stackrel{(b)}{=}I_{\\bar{Q}^{*}}(M;[X^{\\prime},Y^{\\prime\\prime}]),\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "where $(b)$ again follows from the fact that $(n_{X},n_{Y})$ \u22a5\u22a5 $(M,X^{\\prime},Y^{\\prime\\prime})$ , and, consequently, conditioning $(M,X^{\\prime},\\bar{Y^{\\prime\\prime}})$ on $(n_{X},n_{Y})$ has no effect. From (310), we see the proposed $\\bar{Q}^{*}$ in (303) does indeed achieve the lower bound specified in (307), and consequently minimizes (302). ", "page_idx": 69}, {"type": "text", "text": "The final step remaining in our proof is to ensure that the proposed $\\bar{Q}^{*}$ corresponds to a valid $Q_{U B}^{*}(M,X,Y)$ lying in $\\Delta_{P}$ , i.e., there is some $Q_{U B}^{*}(M,X,Y)\\in\\Delta_{P}$ that generates $\\bar{Q}^{*}$ using the construction scheme outlined in Appx. M.1. We know that the relationship between the random variables for $(M,X^{\\prime},Y^{\\prime\\prime},n_{X},Y^{\\prime},n_{Y})$ having the joint distribution $\\bar{Q}$ and $(M,X,Y)$ having the distribution $Q$ can be expressed as follows: $X\\,{\\overset{\\bullet}{=}}\\,X^{\\bar{\\prime}}+Y^{\\prime\\prime}+n_{X}$ and $Y=Y^{\\prime}+n_{Y}$ . Hence, we use these relationships for calculating the conditional marginals $Q_{U B}^{*}(X|M)$ and $Q_{U B}^{*}(Y|M)$ of the corresponding distribution $Q_{U B}^{*}(M,X,Y)$ for the proposed distribution ${\\bar{Q}}^{*}(M,X^{\\prime},Y^{\\prime\\prime},n_{X},Y^{\\prime},n_{Y})$ : ", "page_idx": 69}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{U B}^{*}(X|M=m)=\\bar{Q}^{*}(X^{\\prime}+Y^{\\prime\\prime}+n_{X}|M=m)}\\\\ &{\\overset{(c)}{=}\\bar{Q}^{*}(X^{\\prime}|M=m)*\\bar{Q}^{*}(Y^{\\prime\\prime}|M=m)*\\bar{Q}^{*}(n_{X}|M=m)}\\\\ &{=f(\\delta_{m}^{X}-\\delta_{m}^{Y}-(\\delta_{b i a s}^{X}-\\delta_{b i a s}^{Y}))*f(\\delta_{m}^{Y}-\\delta_{b i a s}^{Y})*f(\\delta_{b i a s}^{X})}\\\\ &{\\overset{(d)}{=}f(\\delta_{m}^{X}-\\delta_{m}^{Y}-(\\delta_{b i a s}^{X}-\\delta_{b i a s}^{Y})+\\delta_{m}^{Y}-\\delta_{b i a s}^{Y}+\\delta_{b i a s}^{X})}\\\\ &{=f(\\delta_{m}^{X})=P(X|M=m).}\\end{array}\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "where $^*$ denotes the convolution operator, and $(c)$ follows from the fact that $(X^{\\prime},Y^{\\prime\\prime},n_{X})$ are jointly conditionally independent given $M$ under $\\bar{Q}^{*}$ , and $(d)$ follows from the properties of convolutionclosed distributions. Similarly, calculating $Q_{U B}^{*}(Y|M=m)$ : ", "page_idx": 69}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{U B}^{*}(Y|M=m)=\\bar{Q}^{*}(Y^{\\prime}+n_{Y}|M=m)}\\\\ &{\\qquad\\qquad\\qquad\\stackrel{(e)}{=}\\bar{Q}^{*}(Y^{\\prime\\prime}|M=m)*\\bar{Q}^{*}(n_{Y}|M=m)}\\\\ &{\\qquad\\quad=f(\\delta_{m}^{Y}-\\delta_{b i a s}^{Y})*f(\\delta_{b i a s}^{Y})}\\\\ &{\\qquad\\quad=f(\\delta_{m}^{Y}-\\delta_{b i a s}^{Y}+\\delta_{b i a s}^{X})}\\\\ &{\\qquad\\quad=f(\\delta_{m}^{Y})=P(Y|M=m),}\\end{array}\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "where $(e)$ follows from the fact that $Y^{\\prime}$ \u22a5\u22a5 $n_{Y}|M$ under $\\bar{Q}^{*}$ . It is easy to see that $Q_{U B}^{*}(M)\\,=$ $\\bar{Q}(M)\\;\\;=\\;\\;P(M)$ . Hence, we have $Q_{U B}^{*}(M,X)~~=~~P(M)P(X|M)~~=~~P(M,X)$ and $Q_{U B}^{*}(M,Y)\\,=\\,P(M)P(X\\,=\\,Y|M)\\,=\\,\\tilde{P}(\\tilde{M},Y)$ , which implies $Q_{U B}^{*}(M,X,Y)\\,\\in\\,\\Delta_{P}$ , concluding our proof. \u53e3 ", "page_idx": 69}, {"type": "text", "text": "M.3 Additional details regarding simulation ", "text_level": 1, "page_idx": 70}, {"type": "text", "text": "The list of different function pairs used in the simulation study are listed in Table 1. The function pairs were chosen with the only requirement being that they satisfied the assumptions in (9). For constructing the analytical estimate $\\hat{\\hat{Q}}_{A}(M,X,Y)$ , the corresponding $\\delta_{b i a s}^{X}$ and $\\delta_{b i a s}^{Y}$ were chosen as follows: ", "page_idx": 70}, {"type": "text", "text": ". For each distribution and function pair, we first determine the range of possible values for $\\delta_{b i a s}^{X}$ and \u03b4bYias. ", "page_idx": 70}, {"type": "text", "text": "2. For $\\delta_{b i a s}^{Y}$ , we choose the range as $[0,\\operatorname*{min}_{m\\in\\mathcal{M}}f_{2}(m))$ , where $\\mathcal{M}$ is the support set of $M$ for that distribution. Note that we cannot choose $\\delta_{b i a s}^{Y}>\\mathrm{min}_{m\\in\\mathcal{M}}\\;f_{2}(m)$ as the corresponding conditional distribution $\\bar{Q}^{*}(Y^{\\prime}|M)=\\mathrm{Poisson}(\\bar{f}_{2}(M)-\\delta_{b i a s}^{Y})$ (required for the optimal $\\bar{Q}^{*}$ from which $Q_{A}$ is constructed, see Appx. M.2) would have a negative rate parameter, which is not allowed. Within this range, we choose 10 different, uniformly spaced values for \u03b4bYias. ", "page_idx": 70}, {"type": "text", "text": "3. For each chosen value of $\\delta_{b i a s}^{Y}$ , the corresponding range for $\\delta_{b i a s}^{X}$ is then chosen as $[0,\\operatorname*{min}\\{\\operatorname*{min}_{m\\in\\mathcal{M}}f_{1}(m),\\operatorname*{min}_{m\\in\\mathcal{M}}(f_{1}(m)-f_{2}(m)+\\delta_{b i a s}^{Y})\\}]$ . Note that choosing the upper limit of $\\delta_{b i a s}^{X}$ ${\\mathrm{as~min}}\\{\\operatorname*{min}_{m\\in{\\mathcal{M}}}f_{1}(m),\\operatorname*{min}_{m\\in{\\mathcal{M}}}(f_{1}(m)-f_{2}(m)+\\delta_{b i a s}^{Y})\\}$ ensures that the distribution $\\bar{Q}^{*}(X^{\\prime}|M)\\ =\\ \\mathrm{Poisson}((f_{1}(M)\\,-\\,\\delta_{b i a s}^{X})\\,-\\,(f_{2}(M)\\,-\\,\\delta_{b i a s}^{Y}))$ ) and $\\bar{Q}^{*}(Z|M)=\\mathrm{Poisson}(f_{1}(M)-\\delta_{b i a s}^{X})$ (required for specifying the optimal $\\bar{Q}^{*}$ from which $Q_{A}$ is constructed, see Appx. M.1 and Appx. M.2) have positive rate parameters. For each value of $\\delta_{b i a s}^{Y}$ , we again choose 10 different values of $\\delta_{b i a s}^{X^{-}}$ within the above specified range. In total, we chose 100 different pairs of $(\\delta_{b i a s}^{X},\\delta_{b i a s}^{Y})$ . ", "page_idx": 70}, {"type": "text", "text": "For each pair of $(\\delta_{b i a s}^{X},\\delta_{b i a s}^{Y})$ , we calculate the analytical $Q_{A}(M,X,Y)$ as follows: ", "page_idx": 70}, {"type": "text", "text": "First, we construct the optimal $\\bar{Q}^{*}$ distribution using the structure provided in proposition 10. The exact specification of $\\bar{Q}^{\\ast}$ is described below. We simplify the structure of $\\bar{Q}^{*}$ by using the fact that $Y^{\\prime}=\\bar{Y}^{\\prime\\prime}$ : ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\bar{Q}^{*}(M,X^{\\prime},Y^{\\prime},n_{X},n_{Y},Y^{\\prime\\prime})\\equiv\\bar{Q}^{*}(M,X^{\\prime},Y^{\\prime},n_{X},n_{Y}),}&{}\\\\ {=\\bar{Q}^{*}(M)\\bar{Q}^{*}(X^{\\prime}|M)\\bar{Q}^{*}(Y^{\\prime}|M)\\bar{Q}^{*}(n_{X})\\bar{Q}^{*}(n_{Y}),\\;\\forall}&{}\\\\ {\\bar{Q}^{*}(M)=P(M),}&{}\\\\ {\\bar{Q}^{*}(X^{\\prime}|M)=\\mathrm{Poisson}(f_{1}(M)-f_{2}(M)-(\\delta_{b i a s}^{X}-\\delta_{b i a s}^{Y})),}&{}\\\\ {\\bar{Q}^{*}(n_{X})=\\mathrm{Poisson}(\\delta_{b i a s}^{X}),}&{}\\\\ {\\bar{Q}^{*}(Y^{\\prime\\prime}|M)=\\mathrm{Poisson}(f_{2}(M)-\\delta_{b i a s}^{Y}),}&{}\\\\ {\\bar{Q}^{*}(n_{Y})=\\mathrm{Poisson}(\\delta_{b i a s}^{Y}).}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "Then, we use the property that $X=X^{\\prime}+Y^{\\prime}+n_{X}$ , and $Y=Y^{\\prime}+n_{Y}$ , and use that to appropriately modify $\\bar{Q}^{*}$ to obtain $Q_{A}$ as follows: ", "page_idx": 70}, {"type": "text", "text": "\u2022 We calculate a new distribution ${\\bar{Q}}^{*}(M,X^{\\prime}+Y^{\\prime},Y^{\\prime},n_{X},n_{Y});$ : ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\bar{Q}^{*}(M,X^{\\prime}+Y^{\\prime},Y^{\\prime},n_{X},n_{Y})=\\bar{Q}^{*}(M)\\bar{Q}^{*}(n_{X})\\bar{Q}^{*}(n_{Y})\\bar{Q}^{*}(X^{\\prime}+Y^{\\prime}|M)\\times}&{}\\\\ {\\bar{Q}^{*}(Y^{\\prime}|X^{\\prime}+Y^{\\prime},M).}&{}\\\\ {=\\bar{Q}^{*}(M)\\bar{Q}^{*}(n_{X})\\bar{Q}^{*}(n_{Y})\\bar{Q}^{*}(Z|M)\\bar{Q}^{*}(Y^{\\prime}|Z,M),}\\end{array}\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "where $Z=X^{\\prime}+Y^{\\prime}$ and alternatively also $Z=X-n_{X}$ (as $X=X^{\\prime}+Y^{\\prime}+n_{X})$ . First note that $\\bar{Q}^{*}(Z|M)$ from Appx. M.2 has the form $f(\\delta_{m}^{X}-\\delta_{b i a s}^{X})$ , which for the Poisson case is Poisson $(f_{1}(M)-\\delta_{b i a s}^{X})$ . Furthermore, we can recognize that $\\bar{Q}^{*}(Y^{\\prime}=y|Z=z,M=$ $m)=\\bar{Q}^{*}(Y^{\\prime}=y^{\\prime}|X^{\\prime}+Y^{\\prime}=x^{\\prime}+y^{\\prime},M=m)=G(y^{\\prime};\\epsilon_{m}^{(1)},1-\\epsilon_{m}^{(1)},x^{\\prime}+y^{\\prime})$ , which for the Poisson case can be written as Binomial $(Z,p_{m})$ where $p_{m}=f_{2}(M)\\!-\\!\\delta_{b i a s\\Big/f_{1}(M)-\\delta_{b i a s}^{X}}^{Y}$ . ", "page_idx": 70}, {"type": "text", "text": "\u2022 Now define $\\mathbb{S}_{x}^{m}\\;=\\;\\{x_{n_{X}}\\;\\in\\;\\mathcal{X}_{n_{X}}^{m}\\,,z\\,\\in\\,\\mathcal{Z}_{m}\\,:\\,x_{n_{X}}\\perp z\\,=\\,x\\}$ , where $\\mathcal{X}_{n_{X}}^{m}$ and ${\\mathcal{Z}}_{m}$ are the corresponding support sets of the distributions $\\bar{Q}^{*}(n_{X}|M=m)$ and $\\bar{Q}^{\\bar{*}}(Z|M=m)$ , respectively. Then, using the fact that $X=Z+n_{X}$ , we have: ", "page_idx": 70}, {"type": "text", "text": "", "page_idx": 71}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{Q}^{*}(M\\!=\\!m,X\\!=\\!x,n_{Y},Y^{\\prime})=\\bar{Q}^{*}(M,Z\\!+\\!n_{X},n_{Y},Y^{\\prime})=\\bar{Q}^{*}(M=m)\\bar{Q}^{*}(n_{Y})\\times}\\\\ &{\\displaystyle\\sum_{(x_{n_{X}},z)\\in\\mathbb{S}_{x}^{m}}\\bar{Q}^{*}(n_{X}=x_{n_{X}})\\bar{Q}^{*}(Z=z|M=m)\\bar{Q}^{*}(Y^{\\prime}|Z=z,M=m),\\qquad\\qquad(3)}\\end{array}\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "where we know that for the Poisson case: $\\begin{array}{r l}&{\\bar{Q}^{*}(M)=P(M),\\bar{Q}^{*}(n_{Y})=\\mathrm{Poisson}(\\delta_{b i a s}^{Y}),\\bar{Q}^{*}(Z|M)=\\mathrm{Poisson}(f_{1}(M)-\\delta_{b i a s}^{X}),}\\\\ &{\\bar{Q}^{*}(n_{X})=\\mathrm{Poisson}(\\delta_{b i a s}^{X}),\\mathrm{~and~}\\bar{Q}^{*}(Y^{\\prime}|Z,M)=\\mathrm{Binomial}\\left(Z,\\frac{f_{2}(M)-\\delta_{b i a s}^{Y}}{f_{1}(M)-\\delta_{b i a s}^{X}}\\right).\\mathrm{~(3)}}\\end{array}$ 16) Using a similar procedure, we derive $Q_{A}(M,X,Y)$ from $\\bar{Q}^{*}(M,X,Y^{\\prime},n_{Y})$ as $Y=$ $n_{Y}+Y^{\\prime}$ . Define $\\mathbb{S}_{y}^{m}=\\{y_{n_{y}}\\in\\mathcal{Y}_{n_{X}}^{m},z\\in\\mathcal{Y}_{m}^{\\prime}:y_{n_{X}}+y^{\\prime}=y\\}$ , then: $Q_{A}(M,X,Y=y)=\\bar{Q}(M,X,n_{Y}+Y^{\\prime})=\\sum_{(x_{n_{Y}},y^{\\prime})\\in\\mathbb{S}_{y}^{m}}\\bar{Q}^{*}(M,X,Y^{\\prime}=y^{\\prime},n_{Y}=x_{n_{Y}}).$ \u2022 After analytically calculating $Q_{A}$ for all 100 bias pairs of $(\\delta_{b i a s}^{X},\\delta_{b i a s}^{Y})$ , we chose $Q_{A}$ corresponding to the pair $(\\delta_{b i a s}^{X},\\delta_{b i a s}^{Y})$ which results in the smallest value of the corresponding $I_{Q_{A}}(M;[X,Y])$ . ", "page_idx": 71}, {"type": "text", "text": "", "page_idx": 71}, {"type": "table", "img_path": "7CUUtpDeqN/tmp/f2c151bbc15df94655cc1483cfb9e669c7ae05e6fe89ff79c9bc04ae85669e6a.jpg", "table_caption": [], "table_footnote": ["Table 1: Table 1 lists the 20 different function pairs used in the simulation study described in Sec. 6 for comparing the analytical estimate of the minimizing distribution of (2) proposed in Sec. 6 with the corresponding ground-truth numerical estimate. The index in Table 1 maps the corresponding function pair to the results shown in Fig. 1. To illustrate, the number 1 on the $x$ -axis of all the plots shown in Fig. 1 corresponds to the function pair with index 1 in Table 1. "], "page_idx": 71}, {"type": "text", "text": "M.4 Additional results and analysis ", "text_level": 1, "page_idx": 71}, {"type": "text", "text": "We report the tightness of the upper-bound for systems employing binomial and negative-binomial distributions in Fig. 2(a). We find that the results for binomial and negative-binomial remain consistent with the Poisson study performed in Sec. 6 and the upper-bound continues to remain tight. We also performed further analysis to understand how our upper-bound performs when both of the UI terms are non-zero. We sub-selected the trials in the 20 pairs (analyzed in Section 6) that had both $U I_{x}>0.01$ bits and $U I_{y}>0.01$ bits, and report the corresponding median error of these sub-selected trials in Fig. 2(d). ", "page_idx": 71}, {"type": "text", "text": "N Distributions ", "text_level": 1, "page_idx": 71}, {"type": "text", "text": "We provide brief descriptions of the distribution families and the corresponding distributions used in this work. We also provide citations to relevant works for the reader interested in learning more about these distributions. ", "page_idx": 71}, {"type": "image", "img_path": "7CUUtpDeqN/tmp/a361279239c2e069a7d98ae9c0a24a2dc23c71d7d97e5a3e2824142b19596d50.jpg", "img_caption": ["(a) Upper Bound (UB): Binomial & Neg. Binomial "], "img_footnote": [], "page_idx": 72}, {"type": "image", "img_path": "7CUUtpDeqN/tmp/95ea26bf7055785d35220a019703afe26b68a72f2387ac754eff1df0a523184d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 72}, {"type": "image", "img_path": "7CUUtpDeqN/tmp/45b9b4462f458f0de5ec67f48ae36f0d3f2747bba1a609cb645eef35d0ef6de0.jpg", "img_caption": ["Figure 2: (a) shows additional results of simulation study performed in Section 6 of the manuscript for the distributions: Negative Binomial and Binomial. The experimental setup is the same as described in Section 6, with the differences being that only 10 function pairs were tested, the number of outcomes of $M$ was restricted to 2 and 4, and the support of $M$ was appropriately modified for Binomial and Negative Binomial distributions (see caption of Table 2 below). The function pairs tested are provided in Table 2. (b) and (c) show the unique information atoms of the systems having non-zero unique information in both $X$ and $Y$ tested in the simulation study presented in Section 6 of the manuscript. More specifically, of the 20 pairs analyzed in Section 6, we only selected trials that had both $U I_{x}>0.01$ bits and $U I_{y}>0.01$ bits. (d) provides the corresponding median error for the subselected trials (originally shown in Fig.1c in the manuscript for all trials) for the systems shown in ${\\bf(b)}$ and (c). "], "img_footnote": [], "page_idx": 72}, {"type": "table", "img_path": "7CUUtpDeqN/tmp/ce4da93e0540b6e3b28b691fd5d7484e2d1fbcc2666b7291c35f710a536b8ff9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 72}, {"type": "text", "text": "Table 2: For negative binomial the outcomes of $M$ were uniformly randomly sampled from the interval [5, 10], and for the binomial experiment the outcomes of $M$ were uniformly chosen from the set $\\{5,6;...,1{\\dot{4}}\\}$ . The symbol $\\lfloor\\cdot\\rceil$ denotes the rounding to closest integer operation. The corresponding system models of binomial and negative binomial can be found in Appendix C (systems 3 and 6, respectively) of the manuscript. ", "page_idx": 72}, {"type": "text", "text": "N.1 Common distributions used in this work ", "text_level": 1, "page_idx": 73}, {"type": "text", "text": "We employed various well-known distributions in this work. We refer the reader to [62, 63] for a review of these distributions. We use the most commonly used notations in literature to represent these distributions. Regardless, we provide explicit definitions of our notations for these distributions to remove ambiguity. ", "page_idx": 73}, {"type": "text", "text": "Poisson Distribution Poisson distribution is a discrete distribution having support over all nonnegative integers. The p.m.f. of the Poisson distribution is described by a single rate parameter $\\bar{\\lambda}\\in[0,\\infty)$ . We define the Poisson distribution with the rate parameter $\\lambda=0$ as a degenerate point distribution with all its mass at 0. We use Poisson $(\\lambda)$ to denote the p.m.f. of a random variable $X$ distributed according to the Poisson distribution. The exact definition of the p.m.f. of $X$ is provided below: ", "page_idx": 73}, {"type": "equation", "text": "$$\nP(X=x)=\\mathrm{Poisson}(x;\\lambda)=\\frac{e^{-\\lambda}\\lambda^{x}}{x!}\\,\\forall\\,x\\in\\mathbb{N}_{0}.\n$$", "text_format": "latex", "page_idx": 73}, {"type": "text", "text": "Gaussian Distribution Gaussian distribution is a continuous distribution having support over $\\mathbb{R}$ . The p.d.f. of the Gaussian distribution is described by its mean $\\mu\\in\\mathbb{R}$ and variance $\\bar{\\sigma}^{2}\\in[0,\\infty)$ . We define the Gaussian distribution with the variance $\\sigma^{\\bar{2}}=0$ as a degenerate point distribution with all its mass at $\\mu$ . We use $\\mathcal{N}(\\mu,\\sigma^{2})$ to denote the p.d.f. of a random variable $X$ distributed according to the Gaussian distribution. The exact definition of the p.d.f. of $X$ is provided below: ", "page_idx": 73}, {"type": "equation", "text": "$$\np(X=x)=\\mathcal{N}(x;\\mu,\\sigma^{2})=\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}e^{-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}}\\,\\forall\\,x\\in\\mathbb{R}.\n$$", "text_format": "latex", "page_idx": 73}, {"type": "text", "text": "Negative Binomial Distribution Negative binomial distribution is a discrete distribution having support over all non-negative integers. The p.m.f. of the negative binomial distribution is described by two parameters: the number of successes $r\\in\\mathbb{N}$ , and the probability of success $p\\in(0,1]$ . We use Negative Binomial $(r,p)$ to denote the p.m.f. of a random variable $X$ distributed according to the negative binomial distribution. The exact definition of the p.m.f. of $X$ is provided below: ", "page_idx": 73}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P(X=x)=\\mathrm{Negative\\;Binomial}(x;r,p)=\\binom{x+r-1}{x}(1-p)^{x}p^{r}\\;\\forall\\;x\\in\\mathbb{N}_{0},}\\\\ &{\\mathrm{\\where}\\left(\\binom{x+r-1}{x}=\\frac{(x+r-1)!}{x!(r-1)!}.\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 73}, {"type": "text", "text": "Geometric Distribution Geometric distribution is a special case of negative binomial distribution with $r=1$ . The p.m.f. of the geometric distribution is described by a success probability $p\\in(0,1]$ . We use Geometric $(p)$ to denote the p.m.f. of a random variable $X$ distributed according to the geometric distribution. The exact definition of the p.m.f. of $X$ is provided below: ", "page_idx": 73}, {"type": "equation", "text": "$$\nP(X=x)={\\mathrm{Geometric}}(x;p)=p(1-p)^{x}\\,\\forall\\,x\\in\\mathbb{N}_{0}.\n$$", "text_format": "latex", "page_idx": 73}, {"type": "text", "text": "Beta-Binomial Distribution Beta-binomial distribution is a discrete distribution having support over $\\{0,\\ldots,N\\}$ for some $N\\in\\ensuremath{\\mathbb{N}}_{0}$ . The p.m.f. of the beta-binomial distribution is described by three parameters: $N$ specifies the support, $\\alpha\\in(0,\\infty)$ and $\\beta\\in(0,\\infty)$ . We use Beta-Binomia $(\\bar{N},\\alpha,\\beta)$ to denote the p.m.f. of a random variable $X$ distributed according to the beta-binomial distribution. The exact definition of the p.m.f. of $X$ is provided below: ", "page_idx": 73}, {"type": "text", "text": "$P(X=x)={\\mathrm{Beta-Binomial}}(x;N,\\alpha,\\beta)={\\binom{N}{x}}{\\frac{B(x+\\alpha,M-x+\\beta)}{B(\\alpha,\\beta)}}\\vee n\\in\\{0,\\ldots,N\\},$ where $B(\\alpha,\\beta)={\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}}$ is the beta function and $\\Gamma(\\cdot)$ is the Gamma function. ", "page_idx": 73}, {"type": "text", "text": "Gamma Distribution Gamma distribution is a continuous distribution having support over $(0,\\infty)$ . The p.d.f. of the gamma distribution is described by its shape parameter $\\alpha>0$ and rate parameter $\\beta>0$ . We use ${\\mathrm{Gamma}}(\\alpha,\\beta)$ to denote the p.d.f. of a random variable $X$ distributed according to the gamma distribution. The exact definition of the p.d.f. of $X$ is provided below: ", "page_idx": 73}, {"type": "equation", "text": "$$\np(X=x)=\\mathbf{Gamma}(x;\\alpha,\\beta)={\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}}x^{\\alpha-1}e^{-\\beta x}\\,\\forall\\,x\\in(0,\\infty),\n$$", "text_format": "latex", "page_idx": 73}, {"type": "text", "text": "where $\\Gamma(\\cdot)$ is the Gamma function. ", "page_idx": 73}, {"type": "text", "text": "Exponential Distribution Exponential distribution is special case of the Gamma distribution with $\\alpha=1$ . The p.d.f. of the exponential distribution is described by its rate parameter $\\lambda>0$ . We use Exponential $(\\lambda)$ to denote the p.d.f. of a random variable $X$ distributed according to the exponential distribution. The exact definition of the p.d.f. of $X$ is provided below: ", "page_idx": 74}, {"type": "equation", "text": "$$\np(X=x)=\\operatorname{Exponential}(x;\\lambda)=\\lambda e^{-\\lambda x}\\,\\forall\\,x\\in[0,\\infty).\n$$", "text_format": "latex", "page_idx": 74}, {"type": "text", "text": "Beta Distribution Beta distribution is a continuous distribution having support over [0, 1] or $(0,1)$ depending upon the values of its parameter. The p.d.f. of the beta distribution is described by two shape parameters: $\\alpha>0$ and $\\beta>0$ . We use Beta $(\\alpha,\\beta)$ to denote the p.d.f. of a random variable $X$ distributed according to the beta distribution. The exact definition of the p.d.f. of $X$ is provided below: ", "page_idx": 74}, {"type": "equation", "text": "$$\np(X=x)=\\mathtt{B e t a}(x;\\alpha,\\beta)=\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}\\,\\forall\\,x\\in(0,1),\n$$", "text_format": "latex", "page_idx": 74}, {"type": "text", "text": "where $\\Gamma(\\cdot)$ is the Gamma function. The end-point 0 is included in the support of the beta distribution if $\\alpha\\geq1$ and the end-point 1 is included in the support of the beta distribution if $\\beta\\geq1$ . ", "page_idx": 74}, {"type": "text", "text": "Dirichlet Distribution Dirichlet distribution is a multivariate generalization of the beta distribution. The p.d.f. of Dirichlet distribution is described by $d$ -shape parameters: $\\{\\alpha_{1},\\ldots,\\alpha_{d}\\}$ , where $\\alpha_{i}\\in$ $(0,\\infty)\\ \\forall\\ i\\,\\in\\ [d]$ . We use Dirichle $\\cdot(\\alpha_{1},\\ldots,\\alpha_{d})$ to denote the p.d.f. of a $d$ -dimensional random vector $\\vec{\\bf X}$ distributed according to the Dirichlet distribution. The exact definition of the p.d.f. of $X$ is provided below: ", "page_idx": 74}, {"type": "equation", "text": "$$\np(\\vec{\\bf X}=\\vec{\\bf x})=\\mathrm{Dirichlet}(\\vec{\\bf x};\\alpha_{1},\\ldots,\\alpha_{d})=\\frac{\\Gamma\\left(\\sum_{i=1}^{d}\\alpha_{i}\\right)}{\\prod_{i=1}^{d}\\Gamma(\\alpha_{i})}\\prod_{i=1}^{d}x_{i}^{\\alpha_{i}-1},\n$$", "text_format": "latex", "page_idx": 74}, {"type": "text", "text": "where $\\Gamma(\\cdot)$ is the Gamma function, and $\\vec{\\mathbf{x}}=\\left[x_{1}\\quad\\ldots\\quad x_{d}\\right]^{T}$ , with $x_{i}\\in[0,1]$ and $\\textstyle\\sum_{i=1}^{d}x_{i}=1$ . ", "page_idx": 74}, {"type": "text", "text": "Binomial Distribution Binomial distribution is a discrete distribution having support over $\\{0,\\ldots,N\\}$ for some $N\\,\\in\\,\\mathbb{N}_{0}$ . The p.m.f. of the binomial distribution is described by two parameters: the total number of trials, $N$ , and the success probability, $p$ . We use Binomia $(N,p)$ to denote the p.m.f. of a random variable $X$ distributed according to the binomial distribution. The exact definition of the p.m.f. of $X$ is provided below: ", "page_idx": 74}, {"type": "equation", "text": "$$\nP(X=x)={\\mathrm{Binomial}}(x;N,p)={\\binom{N}{x}}p^{x}(1-p)^{n-x}\\,\\forall\\,\\,n\\in\\{0,\\ldots,N\\},\n$$", "text_format": "latex", "page_idx": 74}, {"type": "text", "text": "Multinomial Distribution Multinomial distribution is a multivariate generalization of the binomial distribution. The p.m.f. of the multinomial distribution is described by two parameters: the total number of trials, $N$ , and the success probability vector, \u20d7p. We use Multinomial $\\bar{\\mathbf{\\Gamma}}_{d}(N,\\vec{\\mathbf{p}})$ to denote the p.m.f. of a $d$ -dimensional random vector $\\vec{\\bf X}$ distributed according to the multinomial distribution. The exact definition of the p.m.f. of $\\vec{\\bf X}$ is provided below: ", "page_idx": 74}, {"type": "equation", "text": "$$\nP({\\vec{\\mathbf{X}}}={\\vec{\\mathbf{x}}})={\\mathrm{Multinomial}}_{d}({\\vec{\\mathbf{x}}};N,{\\vec{\\mathbf{p}}})={\\frac{N!}{\\prod_{i=1}^{d}x_{i}!}}\\prod_{i=1}^{d}p_{i}^{x_{i}},\n$$", "text_format": "latex", "page_idx": 74}, {"type": "text", "text": "where, $\\vec{\\mathbf{p}}=\\left[\\begin{array}{l l l l}{p_{1}}&{\\ldots}&{p_{d}}\\end{array}\\right]^{T}$ is a $d$ -dimensional probability vector, i.e. $0\\leq p_{i}\\leq1,\\;\\forall\\;1\\leq i\\leq d$   \nand $\\textstyle\\sum_{i=1}^{d}p_{i}\\,=\\,1$ , and ${\\vec{\\mathbf{x}}}\\,=\\,{\\left[\\begin{array}{l l l l}{x_{1}}&{\\dots}&{x_{d}}\\end{array}\\right]}^{T}$ is a $d$ -dimensional categorical vector, i.e. $x_{i}\\in$   \n$\\mathbb{N}_{0}$ $,\\forall\\,1\\leq i\\leq d$ , and $\\textstyle\\sum_{i=1}^{d}x_{i}=n$ . ", "page_idx": 74}, {"type": "text", "text": "Hypergeometric Distribution The p.m.f. of the hypergeometric distribution is described by three parameters: the total number of success states in the population $S\\in\\ensuremath{\\mathbb{N}}_{0}$ , the total number of failure states $F\\in\\ensuremath{\\mathbb{N}}_{0}$ and the number of draws $n\\in\\{0,\\dots,S+F\\}$ . We use Hypergeometric $(S,K,n)$ to ", "page_idx": 74}, {"type": "text", "text": "denote the p.m.f. of a random variable $X$ distributed according to the hypergeometric distribution. The exact definition of the p.m.f. of $X$ is provided below: ", "page_idx": 75}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P(X=x)=\\mathrm{{Hypergeometric}}(x;S,K,n)}\\\\ &{\\qquad\\qquad=\\frac{{\\binom{S}{x}}{\\binom{F}{n-x}}}{\\binom{S+F}{n}}\\,\\forall\\,x\\in\\{\\operatorname*{max}\\{0,n-F\\},...,\\operatorname*{min}\\{n,S\\}\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 75}, {"type": "text", "text": "Multivariate Hypergeometric Distribution Multivariate hypergeometric distribution is a multivariate generalization of hypergeometric distribution. The p.m.f. of the multivariate hypergeometric distribution is described by the following parameters: the total number of object types $d\\in\\ensuremath{\\mathbb{N}}_{0}$ , the size of population of each type $\\{n_{1},\\ldots,n_{d}\\}$ with $n_{i}\\in\\ensuremath{\\mathbb{N}}_{0}$ , and the number of draws $n\\in\\{0,\\dots,\\sum_{i=1}^{d}n_{i}\\}$ . Define $\\vec{\\bf n}=[n_{1}\\mathrm{~\\boldmath~\\sigma~}...\\,n_{d}]$ . Then, we use Multivariate Hypergeometric $(\\vec{\\bf n},n)$ to denote the p.m.f. of a $d$ -dimensional random vector \u20d7X distributed according to the multivariate hypergeometric distribution. The exact definition of the p.m.f. of $\\vec{\\bf X}$ is provided below: ", "page_idx": 75}, {"type": "equation", "text": "$$\n\\begin{array}{r}{P(\\vec{\\mathbf{X}}=\\vec{\\mathbf{x}})=\\vec{\\mathbf{M}}\\mathbf{ultivariate}\\;\\mathrm{Hypergeometric}(\\vec{\\mathbf{x}};\\vec{\\mathbf{n}},n),}\\\\ {=\\frac{\\prod_{i=1}^{d}\\binom{n_{i}}{x_{i}}}{\\binom{\\sum_{i=1}^{d}n_{i}}{n}},\\;\\;\\;}\\end{array}\n$$", "text_format": "latex", "page_idx": 75}, {"type": "text", "text": "where $\\textstyle{\\binom{A}{B}}\\;=\\;{\\frac{A!}{B!(A-B)!}}$ for some $A,B\\,\\in\\,\\mathbb{N}_{0}$ and $B\\,\\leq\\,A$ , and ${\\vec{\\mathbf{x}}}\\,=\\,\\left[x_{1}\\quad\\ldots\\quad x_{d}\\right]^{T}$ with $x_{i}\\in$ $\\{0,n_{i}\\}$ and $\\textstyle\\sum_{i=1}^{d}x_{i}=n$ . ", "page_idx": 75}, {"type": "text", "text": "Uniform Distribution Uniform distribution is a continuous distribution having support over $(a,b)$ where $a,b\\,\\in\\,\\mathbb{R}$ . We use Uniform $(a,b)$ to denote the p.d.f. of a random variable $X$ distributed according to the uniform distribution. The exact definition of the p.d.f. of $X$ is provided below: ", "page_idx": 75}, {"type": "equation", "text": "$$\np(X=x)={\\mathrm{Uniform}}(x;a,b)={\\frac{1}{|a-b|}}\\;\\forall\\;x\\in[a,b].\n$$", "text_format": "latex", "page_idx": 75}, {"type": "text", "text": "For uniform distribution, we abuse notation by not ensuring $a<b$ for representing the interval $[a,b]$ . In the context of uniform distribution, the notation $[a,b]$ is to be understood as $[a,b]$ if $a\\leq b$ and $[b,a]$ if $a\\geq b$ . ", "page_idx": 75}, {"type": "text", "text": "Bernoulli Distribution The p.m.f. of the random variable $X$ distributed according to the Bernoulli distribution is denoted as Bernoulli $(p)$ for $p\\,\\in\\,[0,1]$ . The exact definition of the p.d.f. of $X$ is provided below: ", "page_idx": 75}, {"type": "equation", "text": "$$\nP(X=0)=p,{\\mathrm{~and~}}P(X=1)=1-p.\n$$", "text_format": "latex", "page_idx": 75}, {"type": "text", "text": "Categorical Distribution The p.m.f. of a random variable $X$ distributed according to the categorical distribution is denoted as Categorica $(p_{1},\\ldots,p_{d})$ for $p_{i}\\,\\in\\,[0,1]\\;\\forall\\;i\\,\\in\\,[d]$ and $\\textstyle\\sum_{i=1}^{d}p_{i}\\,=\\,1$ . The exact definition of the p.d.f. of $X$ is provided below: ", "page_idx": 75}, {"type": "equation", "text": "$$\nP(X=i)=p_{i}\\;\\forall\\;i\\in[d].\n$$", "text_format": "latex", "page_idx": 75}, {"type": "text", "text": "N.2 Univariate Stable Distribution Definition ", "text_level": 1, "page_idx": 75}, {"type": "text", "text": "We employ the definition of a univariate stable distribution discussed in [23]: ", "page_idx": 75}, {"type": "text", "text": "Definition 2. $A$ random variable $X$ is said to have a stable continuous distribution if, for two independent copies of $X$ , denoted as $X_{1}$ and $X_{2}$ , and positive constants $a>0$ and $b>0$ , (322) holds for some $c>0$ and $d\\in\\mathbb R$ . ", "page_idx": 75}, {"type": "equation", "text": "$$\na X_{1}+b X_{2}\\stackrel{d}{=}c X+d,\\ w h e r e\\stackrel{d}{=}m e a n s\\ e q u a l i t y\\ i n\\ d i s t r i b u t i o n\\ [57].\n$$", "text_format": "latex", "page_idx": 75}, {"type": "text", "text": "Continuous stable distributions are typically expressed through their characteristic functions, as the analytical form of many continuous stable distributions is not known. The characteristic function of a random variable $X$ having a continuous stable distribution is typically parameterized by four parameters: stability parameter denoted as $\\alpha\\in(0,2]$ , skewness parameter denoted as $\\beta\\in[-1,1]$ , scale parameter denoted as $\\gamma\\in(0,\\infty)$ , and location parameter denoted as $\\mu\\in\\mathbb{R}$ . The exact form of the characteristic function is given in (323). ", "page_idx": 76}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[e^{i t X}\\right]=\\phi(t;\\alpha,\\beta,\\gamma,\\mu)=\\exp\\left(i t\\mu-|\\gamma t|^{\\alpha}(1-i\\beta\\mathrm{sgn}(t)\\Phi(\\alpha))\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 76}, {"type": "equation", "text": "$$\n{\\mathrm{where~}}\\Phi(\\alpha)={\\left\\{\\begin{array}{l l}{\\tan\\left({\\frac{\\pi\\alpha}{2}}\\right)}&{\\alpha\\neq1}\\\\ {{\\frac{-2}{\\pi}}\\log(|t|)}&{\\alpha=1}\\end{array}\\right.},\\ \\ \\operatorname{sgn}(t)={\\left\\{\\begin{array}{l l}{-1}&{t<0}\\\\ {0}&{t=0}\\\\ {1}&{t>0}\\end{array}\\right.}.\n$$", "text_format": "latex", "page_idx": 76}, {"type": "text", "text": "We further denote the p.d.f. of $X$ having the characteristic function defined in (323) as $p_{C S}(X;\\alpha,\\beta,\\gamma,\\mu)$ . Some well-known examples of continuous stable distribution are the Gaussian/normal distribution, Cauchy distribution, and L\u00e9vy distribution. All univariate continuous stable distributions have infinite variance except the Gaussian distribution, and their tails follow a power law behavior [23]. Univariate stable distributions have been widely used to model many financial and physical phenomena. Chapter 2 of [23] provides a list of applications where stable distributions have been used. For a more in-depth treatment of stable distributions, we refer the reader to [23]. ", "page_idx": 76}, {"type": "text", "text": "N.3 Multivariate Stable Distribution Definition ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "A multivariate stable distribution is defined similarly to a univariate stable distribution described in Appx. N.2. We employ the definition of multivariate distribution given in [23]. ", "page_idx": 76}, {"type": "text", "text": "Definition 3. A non-generate $d$ -dimensional random variable $\\vec{\\bf X}$ is said to have a multivariate stable continuous distribution if, for two independent copies of $\\vec{\\bf X}_{:}$ , denoted as $\\vec{\\bf X}_{1}$ and $\\vec{\\bf X}_{2}$ , and positive constants $a>0$ and $b>0$ , (324) holds for some $c>0$ and $\\vec{\\bf d}\\in\\mathbb{R}^{d}$ . ", "page_idx": 76}, {"type": "equation", "text": "$$\na\\vec{\\mathbf{X}}_{1}+b\\vec{\\mathbf{X}}_{2}\\stackrel{d}{=}c\\vec{\\mathbf{X}}+\\vec{\\mathbf{d}},\\;w h e r e\\stackrel{d}{=}m e a n s\\;e q u a l i t y\\;i n\\;d i s t r i b u t i o n\\;[57].\n$$", "text_format": "latex", "page_idx": 76}, {"type": "text", "text": "A main source of complexity while analyzing multivariate continuous stable distributions is the wide range of dependence structures this family of distribution admits [23]. We refer the reader [46, 45] for a more general treatment of multivariate stable distributions. In this work, we focus on two special classes of multivariate stable distributions: independent component multivariate stable continuous distribution denoted as $p_{C S-I C}(\\alpha,\\vec{\\beta},\\vec{\\gamma},\\vec{\\mu})$ and elliptically contoured multivariate stable distribution denoted as $p_{C S-E C}(\\alpha,\\Sigma,\\vec{\\mu})$ . Similarly to the univariate continuous stable distribution, both independent component and elliptically contoured multivariate stable distributions do not have explicit general analytical forms and are expressed through their characteristic functions. ", "page_idx": 76}, {"type": "text", "text": "The characteristic function of a $d$ -dimensional random vector $\\vec{\\bf X}$ having independent component multivariate distribution $p_{C S-I C}(\\alpha,\\vec{\\beta},\\vec{\\gamma},\\vec{\\mu})$ is expressed as follows: ", "page_idx": 76}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[e^{i\\bar{\\mathbf{t}}^{T}\\bar{\\mathbf{x}}}\\right]=\\exp\\left(-\\sum_{j=1}^{d}|\\gamma_{j}t_{j}|^{\\alpha}(1-i\\beta_{j}\\mathbf{sgn}(t_{j})\\Phi(\\alpha))+i\\bar{\\mathbf{t}}^{T}\\vec{\\mu}\\right)\\ \\forall\\,\\vec{\\mathbf{t}}\\in\\mathbb{R}^{d},\n$$", "text_format": "latex", "page_idx": 76}, {"type": "text", "text": "where $\\Phi(\\alpha)$ and $\\operatorname{sgn}({\\mathord{\\cdot}})$ are as defined in (323), $\\vec{\\beta}\\:=\\:\\left[\\beta_{1}\\:\\:\\:\\ldots\\:\\:\\:\\:\\beta_{d}\\right]^{T}$ with $\\beta_{j}~\\in~[-1,1]$ , $\\Vec{\\gamma}\\;=$ $\\left[\\gamma_{1}\\quad.\\ldots\\quad\\gamma_{d}\\right]^{T}$ with $\\gamma_{j}\\in(0,\\infty),\\,\\vec{\\mu}=\\left[\\mu_{1}\\quad.\\dots\\quad\\mu_{d}\\right]^{T}\\in\\mathbb{R}^{d}$ , and $\\alpha\\in(0,2]$ . One can also alternatively think of the random vector $\\vec{\\mathbf{X}}=\\left[X_{1}\\quad\\ldots\\quad X_{d}\\right]^{T}$ having the distribution $p_{C S-I C}(\\alpha,\\vec{\\beta},\\vec{\\gamma},\\vec{\\mu})$ as a collection of independent random variables $\\{X_{j}\\}_{j=1}^{d}$ , where $X_{j}\\sim p_{C S}(\\alpha,\\beta_{j},\\gamma_{j},\\mu_{j})$ . ", "page_idx": 76}, {"type": "text", "text": "The characteristic function of a $d$ -dimensional random vector $\\vec{\\bf X}$ having elliptically-contoured multivariate distribution $p_{C S-E C}(\\alpha,\\Sigma,\\vec{\\mu})$ is expressed as follows: ", "page_idx": 76}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[e^{i\\vec{\\bf t}^{T}\\vec{\\bf x}}\\right]=\\exp\\left(-\\left(\\vec{\\bf t}^{T}\\Sigma\\vec{\\bf t}\\right)^{\\alpha/2}+i\\vec{\\bf t}^{T}\\vec{\\pmb{\\mu}}\\right)~\\forall\\,\\vec{\\bf t}\\in\\mathbb{R}^{d},\n$$", "text_format": "latex", "page_idx": 76}, {"type": "text", "text": "where $\\Sigma$ is a positive definite matrix, $\\vec{\\pmb{\\mu}}=\\left[\\mu_{1}\\quad.\\;.\\quad\\mu_{d}\\right]^{T}\\in\\mathbb{R}^{d}$ , and $\\alpha\\in(0,2]$ . Note that allowing $\\Sigma$ to be positive semi-definite will violate the fact that $\\vec{\\bf X}$ should be a non-degenerate $d$ -dimensional random variable. We also list an important property of multivariate continuous stable distributions used in this paper. ", "page_idx": 76}, {"type": "text", "text": "Property 1. A $d$ -dimensional random vector $\\vec{\\bf X}$ has a multivariate stable continuous distribution if and only if every 1-dimensional projection has a univariate continuous stable distribution, i.e., $\\vec{\\mathbf{t}}^{T}\\vec{\\mathbf{X}}\\sim p_{C S}(\\alpha,\\beta(\\vec{\\mathbf{t}}),\\gamma(\\vec{\\mathbf{t}}),\\mu(\\vec{\\mathbf{t}}))\\,\\forall\\,\\vec{\\mathbf{t}}\\in\\mathbb{R}^{d}$ . ", "page_idx": 77}, {"type": "text", "text": "N.4 Univariate Discrete Stable Distributions ", "text_level": 1, "page_idx": 77}, {"type": "text", "text": "Discrete stable distributions form the discrete analogues of the continuous stable distributions. We employ the following definition for a univariate discrete distributions taken from [24]: ", "page_idx": 77}, {"type": "text", "text": "Definition 4. A random variable $X$ is said to have a discrete stable distribution with exponent $\\nu\\in(0,1]$ if for two independent copies of $X$ , denoted as $X_{1}$ and $X_{2}$ , (322) holds for all $\\gamma\\in[0,1]$ . ", "page_idx": 77}, {"type": "equation", "text": "$$\n\\gamma\\circ X_{1}+\\left(1-\\gamma^{\\nu}\\right)^{1/\\nu}\\circ X_{2}\\overset{d}{=}X,\n$$", "text_format": "latex", "page_idx": 77}, {"type": "text", "text": "where $\\underline{{\\underline{{d}}}}$ means equality in distribution $I57J$ , and $\\circ$ implies a binomial thinning operation defined as follows: $\\begin{array}{r}{\\gamma\\circ X=\\sum_{j=1}^{X}N_{j}}\\end{array}$ , where $N_{j}\\sim B e r n o u l l i(\\gamma)$ , and all $N_{j}^{\\prime}s$ are jointly independent. ", "page_idx": 77}, {"type": "text", "text": "Similar to univariate continuous stable distributions, univariate discrete stable distribution do not have explicit analytical forms, and are typically expressed through their probability generating functions and/or characteristic functions . The characteristic function of a discrete random variable $N$ having a stable discrete distribution is given by (328): ", "page_idx": 77}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[e^{i t N}\\right]=\\exp\\left(\\tau(e^{i t}-1)^{\\nu}\\right)=\\phi(t;\\nu,\\tau)\n$$", "text_format": "latex", "page_idx": 77}, {"type": "text", "text": "where $\\mathrm{P}_{N}(z;\\nu,\\tau)$ is the probability generating function of $N$ , $i=\\sqrt{-1}$ , $\\tau>0$ , and $0<\\nu\\leq1$ . We further denote the probability mass function of $N$ as $P_{D S}(N=n;\\nu,\\tau)$ . $P_{D S}(N;1,\\lambda)$ is a standard Poisson distribution with mean $\\lambda$ [24]. All discrete stable distribution asymptotically follow a power law distribution [64]. ", "page_idx": 77}, {"type": "text", "text": "N.5 Multivariate Poisson Distribution ", "text_level": 1, "page_idx": 77}, {"type": "text", "text": "To our knowledge, there are no well-known multivariate generalizations of univariate discrete stable distributions, except for the special case of the Poisson distribution. In this section, we describe a popular multivariate extension of the Poisson distribution. ", "page_idx": 77}, {"type": "text", "text": "An intuitive way to define multivariate Poisson distribution is to represent each random variable in the multivariate Poisson distribution as a sum of independent Poisson random variables [41, 42, 43, 44]. To illustrate, let us construct a bivariate Poisson random vector $\\vec{\\bf N}=\\left[N_{1}\\quad N_{2}\\right]^{T}$ , where ", "page_idx": 77}, {"type": "equation", "text": "$$\nN_{1}=N_{1}^{g}+N_{12}^{g},\\;N_{2}=N_{2}^{g}+N_{12}^{g},\n$$", "text_format": "latex", "page_idx": 77}, {"type": "text", "text": "and $N_{1}^{g},N_{2}^{g}$ , and $N_{12}^{g}$ are mutually independent Poisson random variables with rates $\\lambda_{1},\\lambda_{2}$ , and $\\lambda_{12}$ , respectively. Here, the dependence between $N_{1}$ and $N_{2}$ is expressed through $N_{12}^{g}$ , with covariance between $N_{1}$ and $N_{2}$ being equal to $\\lambda_{12}$ [47]. For dimensions $d>2$ , the Poisson random vector: ", "page_idx": 77}, {"type": "equation", "text": "$$\n\\vec{\\bf N}={\\bf A}\\vec{\\bf N}^{g},\n$$", "text_format": "latex", "page_idx": 77}, {"type": "text", "text": "where A is an appropriate matrix of 0\u2019s and 1\u2019s. We can decompose $\\mathbf{A}=[\\mathbf{A}_{1}\\quad...\\quad\\mathbf{A}_{d^{\\prime}}].$ , where $\\mathbf{A}_{i}$ is a $d\\times(\\O_{i}^{d})$ submatrix having no duplicate columns, and each of its columns containing exactly $i$ ones and $(d-i)$ zeros [44], and $\\vec{\\bf N}^{g}\\,=\\,[N_{1}^{g}\\cdot\\cdot\\cdot N_{d}^{g}\\,\\,N_{12}^{g}\\cdot\\cdot\\cdot\\,\\,N_{(d-1)d}^{g}\\cdot\\cdot\\cdot\\,\\,N_{d-(d^{\\prime}-1)\\dots d}^{g}]^{T}$ , with $N_{i_{1}\\dots i_{j}}^{g}\\ \\sim\\ \\mathsf{P o i s s o n}(\\lambda_{i_{1}\\dots i_{j}})\\ \\forall\\ (i_{1},\\dots,i_{j})\\ \\in\\ \\mathbb{A}_{j}^{d},j\\ \\in\\ [d^{\\prime}]$ . Furthermore, the random variables $\\left\\{N_{1},\\ldots,N_{d-\\left(d^{\\prime}-1\\right)\\ldots d}\\right\\}$ are mutually independent. An intuitive way to think about this multivariate Poisson distribution is to interpret $\\{N_{i}^{g}\\}_{i=1}^{d}$ as the \u201cmain effect\u201d, $\\{N_{i_{1}i_{2}}^{g}\\}_{(i_{1},i_{2})\\in\\mathbb{A}_{2}^{d}}$ as the \u201ctwo-way covariance effects\u201d and so on until $\\{N_{i_{1}...i_{d^{\\prime}}}^{g}\\}_{(i_{1},...,i_{d^{\\prime}})\\in\\mathbb{A}_{d^{\\prime}}^{d}}$ as the $d^{\\prime}$ -way covariance effect in an ANOVA-like fashion [44]. For a more detailed discussion, see [43, 44, 47, 42]. Note that $d^{\\prime}\\leq d$ . In this work, we denote the $d_{\\cdot}$ -dimensional random vector $\\vec{\\bf N}$ having a multivariate Poisson distribution with parameters $\\vec{\\bf A}=\\left[\\lambda_{1}\\;.\\;.\\;.\\;\\lambda_{d}\\;\\lambda_{12}\\;\\ldots\\lambda_{d-(d^{\\prime}-1)\\ldots d}\\right]^{T}$ specifying up to $d^{\\prime}$ -way covariance effects (where $d^{\\prime}\\leq d)$ , as $\\vec{\\bf N}\\sim\\mathrm{Poisson}(d,d^{\\prime},\\vec{\\bf\\Lambda})$ . Note that for $d^{\\prime}=1$ , we have that $K$ is a collection of independent Poisson random variables, and when both $d=d^{\\prime}=1$ , we recover the scalar Poisson distribution: Pr(K = k) = e\u2212k\u03bb!\u03bbk, \u2200k \u2208N0. ", "page_idx": 77}, {"type": "text", "text": "Deriving the p.m.f. of the Multivariate Poisson Distribution ", "text_level": 1, "page_idx": 78}, {"type": "text", "text": "Let $\\vec{\\bf N}\\sim\\mathrm{Poisson}(d,d^{\\prime},\\vec{\\bf\\Lambda})$ , where $\\vec{\\bf N}$ is a $d$ -dimensional random vector. Then the corresponding p.m.f. of $\\vec{\\bf N}$ is described below: ", "page_idx": 78}, {"type": "text", "text": "Let $\\vec{\\bf n}^{\\prime}\\!=\\!\\left[n_{12}\\,\\dots\\,n_{(d-1)d}\\,\\dots\\,n_{d-(d^{\\prime}-1)\\dots d}\\right]^{T}$ , and $d_{\\vec{\\mathbf{n}}^{\\prime}}$ be the dimension of $\\vec{\\bf n}^{\\prime}$ , then: ", "page_idx": 78}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle P(\\vec{\\bf N}\\!=\\!\\vec{\\bf n})\\!=\\!e^{-{\\bf1}^{T}\\vec{\\Lambda}}\\prod_{i=1}^{d}\\lambda_{i}^{n_{i}}\\sum_{{\\bf{\\vec{n}^{\\prime}}}\\in{\\cal C}}\\left(\\prod_{(i_{1},i_{2})\\in\\mathbb{A}_{2}^{d}}\\left(\\!\\frac{\\lambda_{i_{1}i_{2}}}{\\lambda_{i_{1}}\\lambda_{i_{2}}}\\right)^{n_{i_{1}i_{2}}}\\!\\times\\!\\!\\!}}\\\\ {{\\displaystyle\\dots\\times\\prod_{(i_{1},\\dots,i_{d^{\\prime}})\\in\\mathbb{A}_{d^{\\prime}}^{d}}\\left(\\!\\frac{\\lambda_{i_{1}\\dots i_{d^{\\prime}}}}{\\prod_{j=1}^{d^{\\prime}}\\lambda_{i_{j}}}\\!\\right)^{n_{i_{1}\\dots i_{d^{\\prime}}}}\\!\\times Q(\\vec{\\bf n},\\vec{\\bf n}^{\\prime})\\!\\right),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 78}, {"type": "text", "text": "where $C=\\{\\vec{\\bf n}^{\\prime}\\in\\mathbb{N}_{0}^{d_{\\vec{\\bf n}^{\\prime}}}{\\bf:}(\\vec{\\bf a}_{i}^{\\prime})^{T}\\vec{\\bf n}^{\\prime}\\leq n_{i}\\;\\forall\\;i\\in[d]\\}.$ , and ", "page_idx": 78}, {"type": "equation", "text": "$$\nQ(\\vec{\\bf n},\\vec{\\bf n}^{\\prime})=\\prod_{i=1}^{d}{\\frac{1}{(n_{i}-\\vec{\\bf a}_{i}^{\\prime}T\\vec{\\bf n}^{\\prime})!}}\\prod_{j=2}^{d^{\\prime}}\\prod_{(i_{1},\\dots,i_{j})\\in\\mathbb{A}_{j}^{d}}{\\frac{1}{n_{i_{1}\\dots i_{j}}^{g}!}},\n$$", "text_format": "latex", "page_idx": 78}, {"type": "text", "text": "with $\\vec{\\bf a}_{i}^{\\prime}$ being the $i$ -th row of the matrix $\\mathbf{A}^{\\prime}=\\left[\\mathbf{A}_{2}\\ldots\\mathbf{A}_{d^{\\prime}}\\right]$ . ", "page_idx": 78}, {"type": "text", "text": "Proof of $\\vec{\\bf N}\\sim\\mathrm{Poisson}(d,d^{\\prime},\\vec{\\bf\\Lambda})$ having the p.m.f. described in (331). ", "page_idx": 78}, {"type": "text", "text": "We know that $\\vec{\\bf N}={\\bf A}\\vec{\\bf N}_{g}$ , where $\\mathbf{A}=\\left[\\mathbf{A}_{1}\\ldots\\mathbf{A}_{d^{\\prime}}\\right]$ . Let the dimension of $\\vec{\\bf N}^{g}$ be $d_{\\vec{\\mathbf{N}}^{g}}$ and $S_{\\vec{\\mathbf{n}}}=$ $\\left\\{\\vec{\\bf n}^{g}\\in\\mathbb{N}_{0}^{d_{\\vec{\\bf N}^{g}}}{\\bf:k}=A{\\bf k}^{g}\\right\\}$ . Then, using the fact $\\vec{\\bf N}=A\\vec{\\bf N}^{g}$ , the p.m.f. of $\\vec{\\bf N}$ can be expressed using the p.m.f. of $\\vec{\\bf N}^{g}$ in the following manner: ", "page_idx": 78}, {"type": "equation", "text": "$$\nP(\\vec{\\bf N}=\\vec{\\bf n})=\\sum_{\\vec{\\bf n}^{g}\\in S_{\\vec{\\bf n}}}P(\\vec{\\bf N}^{g}=\\vec{\\bf n}^{g}).\n$$", "text_format": "latex", "page_idx": 78}, {"type": "text", "text": "Since $\\vec{\\bf N}^{g}$ is just a collection of mutually independent Poisson random variables, we can write $P(\\vec{\\mathbf{N}}^{g}=\\vec{\\mathbf{n}}^{g})$ as a product of scalar Poisson distributions, i.e., ", "page_idx": 78}, {"type": "equation", "text": "$$\nP(\\vec{\\bf N}^{g}=\\vec{\\bf n}^{g})=\\prod_{j=1}^{d^{\\prime}}\\prod_{(i_{1},\\dots,i_{j})\\in\\mathbb{A}_{j}^{d}}e^{\\lambda_{i_{1}\\dots i_{j}}}{\\frac{\\lambda_{i_{1}\\dots i_{j}}^{g}}{n_{i_{1}\\dots i_{j}}^{g}!}}.\n$$", "text_format": "latex", "page_idx": 78}, {"type": "text", "text": "Substituting (333) into (332), and collecting all $e^{\\lambda_{i_{1}\\dots i_{j}}}$ terms we obtain: ", "page_idx": 78}, {"type": "equation", "text": "$$\nP(\\vec{\\bf N}\\!=\\!\\vec{\\bf n})\\!=\\!e^{-{\\bf1}^{T}\\vec{\\Lambda}}\\sum_{\\vec{\\bf n}^{g}\\in{\\cal S}_{\\vec{\\bf n}}}\\prod_{j=1}^{d^{\\prime}}\\prod_{(i_{1},\\ldots,i_{j})\\in\\mathbb{A}_{j}^{d}}\\!\\frac{\\lambda_{i_{1}\\ldots i_{j}}^{n_{i_{1}\\ldots i_{j}}^{g}}}{n_{i_{1}\\ldots i_{j}}^{g}!}.\n$$", "text_format": "latex", "page_idx": 78}, {"type": "text", "text": "Decompose the matrix $\\mathbf{A}=\\left[\\mathbf{A}_{1}\\mathbf{\\Lambda}\\mathbf{A}^{\\prime}\\right]$ , where $\\mathbf{A}^{\\prime}=\\left[\\mathbf{A}_{2}\\ldots\\mathbf{A}_{d^{\\prime}}\\right]$ and $\\vec{\\bf n}^{g}\\,=\\,[(\\vec{\\bf n}_{1}^{g})^{T}\\,\\,\\vec{\\bf n}^{\\prime T}]^{T}$ , where $\\vec{\\bf n}_{1}^{g}=[n_{1}^{g}\\cdot\\cdot\\cdot n_{d}^{g}]^{T}$ and $\\vec{\\bf n}^{\\prime}$ contains the rest of the elements in $\\vec{\\bf n}^{g}$ . Using the fact that $\\vec{\\bf n}^{g}\\in S_{\\vec{\\bf n}}\\Rightarrow$ $\\vec{\\bf n}={\\bf A}\\vec{\\bf n}^{g}$ we have: $\\vec{\\bf n}={\\bf A}_{1}\\vec{\\bf n}_{1}^{g}+{\\bf A}^{\\prime}\\vec{\\bf n}^{\\prime}$ . Combining this with the fact that ${\\bf A}_{1}$ is an identity matrix, we obtain: ", "page_idx": 78}, {"type": "equation", "text": "$$\n\\vec{\\bf n}=\\vec{\\bf n}_{1}^{g}+{\\bf A}^{\\prime}\\vec{\\bf n}^{\\prime}\\Rightarrow\\vec{\\bf n}_{1}^{g}=\\vec{\\bf n}-{\\bf A}^{\\prime}\\vec{\\bf n}^{\\prime}\\Rightarrow n_{i}^{g}=n_{i}-(\\vec{\\bf a}_{i}^{\\prime})^{T}\\vec{\\bf n}^{\\prime},\n$$", "text_format": "latex", "page_idx": 78}, {"type": "text", "text": "where $\\vec{\\bf a}_{i}^{\\prime}$ is the $i$ -th row of $\\mathbf{A}^{\\prime}$ and $i\\in[d]$ . Substituting (335) into (334): ", "page_idx": 78}, {"type": "equation", "text": "$$\nP(\\vec{\\bf N}\\!=\\!\\vec{\\bf n})\\!=\\!e^{-\\vec{\\bf1}^{T}\\vec{\\Lambda}}\\sum_{\\vec{\\bf n}^{\\prime}\\in\\mathcal{S}_{\\vec{\\bf n}^{\\prime}}}\\left[\\prod_{i_{1}\\in\\mathbb{A}_{1}^{d}}\\frac{\\lambda_{i_{1}}^{n_{i}-(\\vec{\\bf a}_{i}^{\\prime})^{T}\\vec{\\bf n}^{\\prime}}}{(n_{i}-(\\vec{\\bf a}_{i}^{\\prime})^{T}\\vec{\\bf n}^{\\prime})!}\\prod_{j=2}^{d^{\\prime}}\\prod_{(i_{1},\\dots,i_{j})\\in\\mathbb{A}_{j}^{d}}\\frac{\\lambda_{i_{1}\\dots i_{j}}^{n_{i_{1}\\dots i_{j}}}}{n_{i_{1}\\dots i_{j}}!}\\right]\n$$", "text_format": "latex", "page_idx": 78}, {"type": "text", "text": "where the summation constraint is transformed to $S_{\\vec{\\mathbf{n}}^{\\prime}}\\,=\\,\\{\\vec{\\mathbf{n}}^{\\prime}\\,\\in\\,\\mathbb{N}_{0}^{d_{\\vec{\\mathbf{n}}^{\\prime}}}\\colon(\\vec{\\mathbf{a}}_{i}^{\\prime})^{T}\\vec{\\mathbf{n}}^{\\prime}\\,\\le\\,n_{i}\\;\\forall\\;i\\,\\in\\,[d]\\}$ . The equivalence of the summations over the sets $S_{\\mathbf{k}^{\\prime}}$ and $S_{\\mathbf{k}}$ can be derived by considering that ", "page_idx": 78}, {"type": "text", "text": "$n_{i}^{g}=n_{i}-(\\vec{\\bf a}_{i}^{\\prime})^{T}\\vec{\\bf n}^{\\prime}$ , and $n_{i}^{g}\\ge0$ . Collecting all the factorial terms, we get: ", "page_idx": 79}, {"type": "equation", "text": "$$\nQ(\\vec{\\bf n},\\vec{\\bf n^{\\prime}})=\\prod_{i=1}^{d}{\\frac{1}{(n_{i}-\\vec{\\bf a}_{i}^{\\prime}T\\vec{\\bf n^{\\prime}})!}}\\prod_{j=2}^{d^{\\prime}}\\prod_{(i_{1},\\dots,i_{j})\\in\\mathbb{A}_{j}^{d}}\\frac{1}{n_{i_{1}\\dots i_{j}}^{g}!}.\n$$", "text_format": "latex", "page_idx": 79}, {"type": "text", "text": "Substituting (337) into (336), we obtain: ", "page_idx": 79}, {"type": "equation", "text": "$$\nP(\\vec{\\bf N}\\!=\\!\\vec{\\bf n})\\!=\\!e^{-\\vec{\\bf1}^{T}\\vec{\\Lambda}}\\sum_{\\vec{\\bf n}^{\\prime}\\in\\mathcal{S}_{\\vec{\\bf n}^{\\prime}}}\\left[\\prod_{i_{1}\\in\\hat{\\bf x}_{1}^{d}}\\lambda_{i_{1}}^{n_{i}-(\\vec{\\bf a}_{i}^{\\prime})^{T}\\vec{\\bf n}^{\\prime}}\\;\\prod_{j=2}^{d^{\\prime}}\\prod_{(i_{1},\\dots,i_{j})\\in\\mathbb{A}_{j}^{d}}\\lambda_{i_{1}\\dots i_{j}}^{n_{i_{1}\\dots i_{j}}^{g}}\\right]Q(\\vec{\\bf n},\\vec{\\bf n}^{\\prime}).\n$$", "text_format": "latex", "page_idx": 79}, {"type": "text", "text": "Taking the term $\\prod_{i_{1}\\in\\mathbb{A}_{1}}\\lambda_{i_{1}}^{n_{i}}$ out of the summation as it does not depend upon $\\vec{\\bf n}^{\\prime}$ , we obtain: ", "page_idx": 79}, {"type": "equation", "text": "$$\nP(\\vec{\\bf N}\\!=\\!\\vec{\\bf n})\\!=\\!e^{-\\vec{\\bf1}^{T}\\vec{\\Lambda}}\\prod_{i_{1}\\in\\mathbb{A}_{1}^{d}}\\lambda_{i_{1}}^{n_{i}}\\sum_{\\vec{\\bf n}^{\\prime}\\in\\mathcal{S}_{\\boldsymbol{\\mathfrak{n}}^{\\prime}}}\\left[\\prod_{i_{1}\\in\\mathbb{A}_{1}^{d}}\\lambda_{i_{1}}^{-(\\vec{\\bf a}_{i}^{\\prime})^{T}\\vec{\\bf n}^{\\prime}}\\prod_{j=2}^{d^{\\prime}}\\prod_{(i_{1},\\dots,i_{j})\\in\\mathbb{A}_{j}^{d}}\\lambda_{i_{1}\\dots i_{j}}^{n_{i_{1}\\dots i_{j}}^{g}}\\right]Q(\\vec{\\bf n},\\vec{\\bf n}^{\\prime}).\n$$", "text_format": "latex", "page_idx": 79}, {"type": "text", "text": "Notice that $(a_{i}^{\\prime})^{T}\\vec{\\mathbf{n}}^{\\prime}$ contains all the elements of $\\vec{\\bf n}^{\\prime}$ which have $i$ in their subscript. Hence, expanding the term $\\prod_{i_{1}\\in\\mathbb{A}_{1}^{d}}\\lambda_{i_{1}}^{-(\\vec{\\mathbf{a}}_{i}^{\\prime})^{T}\\vec{\\mathbf{n}}^{\\prime}}$ and distributing over the product, we obtain the desired form: ", "page_idx": 79}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle P(\\vec{\\bf N}\\!=\\!\\vec{\\bf n})\\!=\\!e^{-\\vec{\\bf1}^{T}\\vec{\\Lambda}}\\prod_{i=1}^{d}\\lambda_{i}^{n_{i}}\\sum_{{\\bf{\\Lambda}}_{\\vec{{\\bf n}^{\\prime}}\\in{\\cal C}}^{\\bf{\\Lambda}}}\\left(\\prod_{(i_{1},i_{2})\\in\\mathbb{A}_{2}^{d}}\\left(\\frac{\\lambda_{i_{1}i_{2}}}{\\lambda_{i_{1}}\\lambda_{i_{2}}}\\right)^{n_{i_{1}i_{2}}}\\times\\right.}}\\\\ {{\\displaystyle\\left.\\dots\\times\\prod_{(i_{1},\\dots,i_{d^{\\prime}})\\in\\mathbb{A}_{d^{\\prime}}^{d}}\\left(\\frac{\\lambda_{i_{1}\\dots i_{d^{\\prime}}}}{\\prod_{j=1}^{d^{\\prime}}\\lambda_{i_{j}}}\\right)^{n_{i_{1}\\dots i_{d^{\\prime}}}}\\times Q(\\vec{\\bf n},\\vec{\\bf n}^{\\prime})\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 79}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 80}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 80}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 80}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 80}, {"type": "text", "text": "Justification: The three main results of this work are clearly enumerated in Section 1 (Introduction) that demonstrate the main claim of the work which is analytically computing partial information decomposition for numerous systems employing well-known distributions. ", "page_idx": 80}, {"type": "text", "text": "Guidelines: ", "page_idx": 80}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 80}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 80}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 80}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 80}, {"type": "text", "text": "Justification: We have provided a discussion on the limitations of this work in Section 7. Guidelines: ", "page_idx": 80}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 80}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 80}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 80}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 81}, {"type": "text", "text": "Justification: A formal proof of each theorem is provided in the Appx. F- L, with clearly stated assumptions. A formal justification of the arguments used in Sec. 6 is provided in Appx. M. Section 3 discusses the general proof technique used in proving our main theorems. We also provide informal discussions on the constructions used for proving theorems 1- 7 in their respective sections, namely Sec. 4 and Sec. 5. ", "page_idx": 81}, {"type": "text", "text": "Guidelines: ", "page_idx": 81}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 81}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 81}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 81}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 81}, {"type": "text", "text": "Justification: The details of our simulation results are provided in Section 6, followed by more details in Appendix M.3. We have also provided the exact code used for running the simulation study. ", "page_idx": 81}, {"type": "text", "text": "Guidelines: ", "page_idx": 81}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. ", "page_idx": 81}, {"type": "text", "text": "In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 82}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 82}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 82}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 82}, {"type": "text", "text": "Justification: We have provided our code with the supplementary materials. Guidelines: ", "page_idx": 82}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 82}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 82}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 82}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 82}, {"type": "text", "text": "Justification: We have provided all simulation details in Section 6 and Appendix L.3. ", "page_idx": 82}, {"type": "text", "text": "Guidelines: ", "page_idx": 82}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 82}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 82}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 82}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 82}, {"type": "text", "text": "Justification:We have provided a box-plot of our results accompanied by the actual data points used for computing the box-plot in Figure 1. The factors of variability are clearly stated in Sec. 6. ", "page_idx": 82}, {"type": "text", "text": "Guidelines: ", "page_idx": 82}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 82}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 83}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 83}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 83}, {"type": "text", "text": "Answer: [No] ", "page_idx": 83}, {"type": "text", "text": "Justification: We did not feel the need to provide information on the compute resources required to run our simulation, as our main results are of theoretical nature. Furthermore, all our simulation results can be reproduced on any ordinary personal laptop and do not require any special compute resources. ", "page_idx": 83}, {"type": "text", "text": "Guidelines: ", "page_idx": 83}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 83}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 83}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 83}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 83}, {"type": "text", "text": "Justification: The research conducted is mainly theoretical in nature and we believe it does not violate the NeurIPS code of ethics. ", "page_idx": 83}, {"type": "text", "text": "Guidelines: ", "page_idx": 83}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 83}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 83}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 83}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 84}, {"type": "text", "text": "Justification: While our research is mainly theoretical, we have tried to anticipate how it might affect the society negatively and positively. ", "page_idx": 84}, {"type": "text", "text": "Guidelines: ", "page_idx": 84}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 84}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 84}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 84}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 84}, {"type": "text", "text": "Justification: We believe our work poses no such risk. ", "page_idx": 84}, {"type": "text", "text": "Guidelines: ", "page_idx": 84}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 84}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 84}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 84}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 84}, {"type": "text", "text": "Justification: We have provided the citation and the corresponding license for the code we used for running our simulation in Section 6. ", "page_idx": 84}, {"type": "text", "text": "Guidelines: ", "page_idx": 84}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 84}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 85}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 85}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 85}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 85}, {"type": "text", "text": "Justification: The paper does not release any new assets. ", "page_idx": 85}, {"type": "text", "text": "Guidelines: ", "page_idx": 85}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 85}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 85}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 85}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 85}, {"type": "text", "text": "Justification: Our work does not involve crowdsourcing nor human subjects. ", "page_idx": 85}, {"type": "text", "text": "Guidelines: ", "page_idx": 85}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 85}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 85}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 85}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 85}, {"type": "text", "text": "Justification: Our work does not involve crowdsourcing nor human subjects. ", "page_idx": 85}, {"type": "text", "text": "Guidelines: ", "page_idx": 86}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 86}]