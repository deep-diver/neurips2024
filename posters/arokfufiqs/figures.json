[{"figure_path": "aRokfUfIQs/figures/figures_1_1.jpg", "caption": "Figure 1: An efficient and provable generalization of the DeepSets polynomial to vector features.", "description": "This figure illustrates how the DeepSets polynomial, originally designed for scalar features, can be generalized to handle vector features efficiently and provably.  The left side shows the original DeepSets polynomial representation using scalar features (u, v, w). The right side demonstrates the generalization to vector features (u\u00b9, u\u00b2, u\u00b3, u\u2074; v\u00b9, v\u00b2, v\u00b3, v\u2074; w\u00b9, w\u00b2, w\u00b3, w\u2074), showing how each vector is treated as a polynomial of another variable (z), allowing for a polynomial representation with a polynomial size in terms of the number of vectors and dimensionality.  This efficient generalization addresses a significant limitation of the original DeepSets polynomial in handling high-dimensional data.", "section": "4 SSMA-Sequential Signal Mixing Aggregation"}, {"figure_path": "aRokfUfIQs/figures/figures_6_1.jpg", "caption": "Figure 3: Visualization of the Sequential Signal Mixing Aggregation. Left: demonstration of the aggregation stage in an off-the-shelf MPGNN layer. The goal is to create a compressed view of t's incoming neighbors. Right: our proposed aggregation. We convert the neighbor features into two-dimensional discrete signals. We then apply 2D circular convolution by applying 2D FFT, performing pointwise multiplication and transforming back using IFFT. Finally, we compress the result back into a d-dimensional vector using a multi-layer perceptron as a universal compressor.", "description": "The figure visualizes the Sequential Signal Mixing Aggregation (SSMA) method.  The left side shows a standard Message Passing Graph Neural Network (MPGNN) layer aggregating neighbor features. The right side illustrates the SSMA approach, which converts neighbor features into 2D signals, applies 2D circular convolution using Fast Fourier Transform (FFT) and Inverse FFT (IFFT), and finally compresses the result using a Multi-Layer Perceptron (MLP).", "section": "4 SSMA-Sequential Signal Mixing Aggregation"}, {"figure_path": "aRokfUfIQs/figures/figures_7_1.jpg", "caption": "Figure 4: SUMOFGRAM train and test regression L\u2081 errors for different activation functions. The sum aggregator (not dashed) performs poorly and fails to scale with the capacity of the aggregation module, even when used in conjunction with analytic activations. On the contrary, SSMA (dashed) consistently achieves low regression errors and scales well with the number of learnable parameters.", "description": "This figure compares the performance of sum-based aggregators and SSMA on the SUMOFGRAM task with different activation functions (GELU, ReLU, SILU) and varying numbers of trainable parameters.  The results show that sum-based aggregators perform poorly and fail to scale, even with analytic activations known to provide theoretical separation capabilities.  In contrast, SSMA consistently achieves low regression errors and scales well with increasing model size.", "section": "5.1 Synthetic task"}, {"figure_path": "aRokfUfIQs/figures/figures_24_1.jpg", "caption": "Figure 2: Visualization of the higher order notion of neighbor mixing. We visualize the convolution result h for 3-dimensional features, considering 2 neighbors u, v (left) and 3 neighbors u, v, w (right). We demonstrate for each n-tuple matching a feature per node, the corresponding n-th order derivative of exactly one entry of h is 1.", "description": "This figure visualizes how circular convolution in SSMA achieves higher-order neighbor mixing compared to sum-based aggregators.  It shows that while sum-based aggregators only sum neighbor features, the circular convolution mixes features from distinct neighbors in a higher-order way, which is represented by the n-th order derivative calculation. This higher-order mixing ability is crucial for handling downstream tasks that require considering interactions between multiple neighbors.", "section": "4 SSMA-Sequential Signal Mixing Aggregation"}, {"figure_path": "aRokfUfIQs/figures/figures_25_1.jpg", "caption": "Figure 6: SSMA achieves peak performance with significantly lower hidden dimensions.", "description": "This figure shows the results of ablation studies on the IMDB-B and MUTAG datasets comparing SSMA to sum-based aggregators across various hidden dimensions and MPGNN layer types.  The plots illustrate that SSMA outperforms sum-based aggregators in all parameter regimes and reaches peak performance with significantly lower hidden dimensions than its counterparts.", "section": "E Ablation studies"}, {"figure_path": "aRokfUfIQs/figures/figures_26_1.jpg", "caption": "Figure 3: Visualization of the Sequential Signal Mixing Aggregation. Left: demonstration of the aggregation stage in an off-the-shelf MPGNN layer. The goal is to create a compressed view of t's incoming neighbors. Right: our proposed aggregation. We convert the neighbor features into two-dimensional discrete signals. We then apply 2D circular convolution by applying 2D FFT, performing pointwise multiplication and transforming back using IFFT. Finally, we compress the result back into a d-dimensional vector using a multi-layer perceptron as a universal compressor.", "description": "This figure illustrates the proposed Sequential Signal Mixing Aggregation (SSMA) method and compares it to a standard message-passing graph neural network (MPGNN) layer.  The left side shows a typical MPGNN layer, aiming to create a compressed representation of a node's neighbors.  The right side details the SSMA approach, which transforms neighbor features into 2D signals, applies circular convolution using FFT and IFFT, and then compresses the result with a multi-layer perceptron (MLP).", "section": "4 SSMA-Sequential Signal Mixing Aggregation"}]