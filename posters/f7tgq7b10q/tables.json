[{"figure_path": "F7tGQ7b10q/tables/tables_7_1.jpg", "caption": "Table 1: Improvements in honesty rate and H\u00b2 scores for Llama3-8b and Mistral-7b after the proposed two-stage fine-tuning.", "description": "This table presents the results of the two-stage fine-tuning method proposed in the paper for enhancing the honesty and helpfulness of LLMs.  It shows the improvement in honesty rate and the H\u00b2 (honest and helpful) score for Llama3-8b and Mistral-7b models after applying the method, comparing the results with those before fine-tuning ('raw'). The table breaks down the scores for different levels of performance (1-3 (Poor), 4-6 (Medium), 7-10 (Excellent)) and provides the overall gain achieved after fine-tuning.", "section": "5.2 Main Results"}, {"figure_path": "F7tGQ7b10q/tables/tables_8_1.jpg", "caption": "Table 1: Improvements in honesty rate and H\u00b2 scores for Llama3-8b and Mistral-7b after the proposed two-stage fine-tuning.", "description": "This table presents the results of the two-stage fine-tuning method proposed in the paper, showing the improvements achieved in honesty rate and H\u00b2 (Honest and Helpful) scores for Llama3-8b and Mistral-7b models.  It compares the performance of these models before fine-tuning ('raw') and after each stage of fine-tuning, as well as after direct fine-tuning using a combined dataset from both stages ('opt'). The table is divided into two sections: Proprietary Model and Open-Source Model, and shows the improvement in scores for each model across different stages in the fine-tuning process.  For each model and stage, it presents the 'raw' and 'optimized' scores for different ranges of the H\u00b2 assessment (Poor, Medium, Excellent).", "section": "5.2 Main Results"}, {"figure_path": "F7tGQ7b10q/tables/tables_8_2.jpg", "caption": "Table 1: Improvements in honesty rate and H2 scores for Llama3-8b and Mistral-7b after the proposed two-stage fine-tuning.", "description": "This table presents the results of applying the proposed two-stage fine-tuning method to enhance the honesty and helpfulness of Llama3-8b and Mistral-7b LLMs.  It shows the honesty rate and H\u00b2 score (a comprehensive measure of honesty and helpfulness) for each model under different conditions:\n\n* **Raw:** The original model's performance without any modifications.\n* **Direct:** The model after direct fine-tuning (a single-stage process, for comparison).\n* **Stage-1:** The model after the first stage of fine-tuning, focused on distinguishing between honest and dishonest responses.\n* **Stage-2:** The model after the second stage of fine-tuning, aimed at improving the overall quality and helpfulness of responses.\n\nThe table highlights the percentage improvement in both honesty rate and H\u00b2 score for each stage of the fine-tuning process, demonstrating the effectiveness of the two-stage approach.", "section": "5.2 Main Results"}, {"figure_path": "F7tGQ7b10q/tables/tables_9_1.jpg", "caption": "Table 6: Token usage comparison across different methods. Merged and. is the optimized answer based on the confusion.", "description": "This table compares the average number of tokens used by different LLMs across various methods: raw responses, responses showing the confusion of the model, merged answers incorporating confusion and the proposed method's responses.  It demonstrates the computational cost of the different approaches to generating honest and helpful responses, particularly the increased token usage introduced by the proposed method.", "section": "5.1 Experimental Setup"}, {"figure_path": "F7tGQ7b10q/tables/tables_9_2.jpg", "caption": "Table 1: Improvements in honesty rate and H\u00b2 scores for Llama3-8b and Mistral-7b after the proposed two-stage fine-tuning.", "description": "This table presents the results of a two-stage fine-tuning process on Llama3-8b and Mistral-7b models.  It shows the improvement in honesty rate and H\u00b2 (honest and helpful) scores after the fine-tuning. The \"raw\" column represents the performance before fine-tuning, and \"opt.\" indicates the performance after optimization.  The \"gain\" column shows the percentage improvement in the H\u00b2 score after fine-tuning.", "section": "5.2 Main Results"}, {"figure_path": "F7tGQ7b10q/tables/tables_9_3.jpg", "caption": "Table 5: Refusal rate in jailbreak evaluation on TrustLLM [34]. Each jailbreak category includes 100 samples. Ori. is the original performance.", "description": "This table shows the refusal rate for various jailbreak categories in the TrustLLM benchmark, both before and after applying the fine-tuning method proposed in the paper.  The \"Ori.\" column represents the original refusal rate of the language model in each category, while the \"Fine-Tuning (Ours)\" column shows the refusal rate after the fine-tuning process.  An increase in refusal rate indicates better safety performance, as the model becomes more resistant to jailbreak attempts.", "section": "5.4 Computing Budgets"}, {"figure_path": "F7tGQ7b10q/tables/tables_18_1.jpg", "caption": "Table 8: Examples of complex queries in different domains that challenge LLMs' professional capability (Professional Capability in Specific Domains).", "description": "This table presents example queries from various domains (Math, Biology and Medicine, Chemistry, Economics, Computer Science, and Physics) designed to assess the professional capabilities of Large Language Models (LLMs). These queries go beyond the typical capabilities of LLMs, requiring specialized knowledge and problem-solving skills in their respective fields.", "section": "Dataset Analysis"}, {"figure_path": "F7tGQ7b10q/tables/tables_20_1.jpg", "caption": "Table 9: Honesty rate for each category in the raw responses of the HONESET.", "description": "This table presents the honesty rates for each of the six categories in the HONESET dataset before any enhancement methods were applied.  The categories represent different types of queries designed to challenge LLMs' honesty in various ways.  The honesty rate indicates the proportion of responses deemed honest by human evaluators. The table allows for a comparison of the raw honesty rates across different LLMs, separating proprietary and open-source models. It is a baseline for measuring the effectiveness of subsequent honesty enhancement techniques.", "section": "5.2 Main Results"}, {"figure_path": "F7tGQ7b10q/tables/tables_21_1.jpg", "caption": "Table 1: Improvements in honesty rate and H\u00b2 scores for Llama3-8b and Mistral-7b after the proposed two-stage fine-tuning.", "description": "This table presents the results of the two-stage fine-tuning method on two specific large language models, Llama3-8b and Mistral-7b.  It shows the improvement in honesty rate and H\u00b2 scores (which measures both honesty and helpfulness) after applying the fine-tuning method. The table breaks down the results, comparing the performance before fine-tuning (\"raw\") and after fine-tuning (\"opt.\") for different levels of quality (1-3 Poor, 4-6 Medium, 7-10 Excellent) and then provides an overall improvement in the H\u00b2 score.  The \"gain\" column shows the percentage increase in the H\u00b2 score after optimization.", "section": "5.2 Main Results"}, {"figure_path": "F7tGQ7b10q/tables/tables_21_2.jpg", "caption": "Table 1: Improvements in honesty rate and H2 scores for Llama3-8b and Mistral-7b after the proposed two-stage fine-tuning.", "description": "This table presents the results of the two-stage fine-tuning method on Llama3-8b and Mistral-7b models. It shows the improvements in honesty rate and H2 scores (a comprehensive evaluation metric combining honesty and helpfulness) after applying the proposed method.  The table is divided into two sections: proprietary models and open-source models. Each model's raw performance (before fine-tuning), performance after fine-tuning with the proposed two-stage method, and the percentage gain are shown for both the honesty rate and the H2 score. The results highlight significant improvements in both metrics across all evaluated models, demonstrating the effectiveness of the fine-tuning approach in enhancing honesty and helpfulness.", "section": "5.2 Main Results"}, {"figure_path": "F7tGQ7b10q/tables/tables_22_1.jpg", "caption": "Table 12: Average scores for each Category in the optimized response across models", "description": "This table presents the average H\u00b2 scores for each of the six categories in the HONESET dataset after applying the training-free method.  The scores range from 1-10, with higher scores indicating better performance in honesty and helpfulness.  The table is divided into proprietary and open-source models to allow for easy comparison.", "section": "5.2 Main Results"}, {"figure_path": "F7tGQ7b10q/tables/tables_25_1.jpg", "caption": "Table 1: Improvements in honesty rate and H\u00b2 scores for Llama3-8b and Mistral-7b after the proposed two-stage fine-tuning.", "description": "This table presents the results of a two-stage fine-tuning process applied to Llama3-8b and Mistral-7b large language models.  It shows the improvement in honesty rate and H\u00b2 score (a combined measure of honesty and helpfulness) for these models after undergoing the proposed fine-tuning. The table compares the raw (original) performance of the models with their performance after the first stage of fine-tuning, the second stage, and a combined, direct fine-tuning approach.  The 'gain' column indicates the percentage increase in the H\u00b2 score after fine-tuning.", "section": "5.2 Main Results"}]