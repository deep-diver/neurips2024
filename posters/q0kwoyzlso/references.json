{"references": [{"fullname_first_author": "Emmanuel Abbe", "paper_title": "On the non-universality of deep learning: quantifying the cost of symmetry", "publication_date": "2022-MM-DD", "reason": "This paper introduces the concept of leap complexity, a key element in understanding the complexity of learning sparse functions with gradient-based methods."}, {"fullname_first_author": "Emmanuel Abbe", "paper_title": "Sgd learning on neural networks: leap complexity and saddle-to-saddle dynamics", "publication_date": "2022-MM-DD", "reason": "This paper extends the analysis of leap complexity to the context of stochastic gradient descent (SGD), providing a crucial link between theoretical complexity and practical algorithms."}, {"fullname_first_author": "Emmanuel Abbe", "paper_title": "The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks", "publication_date": "2022-MM-DD", "reason": "This paper provides a nearly sufficient condition for SGD learning of sparse functions, further advancing the understanding of the relationship between theoretical complexity and practical learning."}, {"fullname_first_author": "Zeyuan Allen-Zhu", "paper_title": "Backward feature correction: How deep learning performs deep learning", "publication_date": "2020-MM-DD", "reason": "This paper offers insights into the mechanisms by which deep learning achieves its efficiency, providing a valuable context for understanding the challenges of learning sparse functions."}, {"fullname_first_author": "Alberto Bietti", "paper_title": "On learning gaussian multi-index models with gradient flow", "publication_date": "2023-MM-DD", "reason": "This paper explores the complexity of learning multi-index models using gradient flow, offering a relevant extension to the study of sparse function learning."}]}