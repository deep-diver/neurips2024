[{"type": "text", "text": "On the Complexity of Learning Sparse Functions with Statistical and Gradient Queries ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Nirmit Joshi Toyota Technological Institute at Chicago nirmit@ttic.edu ", "page_idx": 0}, {"type": "text", "text": "Theodor Misiakiewicz Toyota Technological Institute at Chicago theodor.misiakiewicz@ttic.edu ", "page_idx": 0}, {"type": "text", "text": "Nathan Srebro Toyota Technological Institute at Chicago nati@ttic.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The goal of this paper is to investigate the complexity of gradient algorithms when learning sparse functions (juntas). We introduce a type of Statistical Queries (SQ), which we call Differentiable Learning Queries (DLQ), to model gradient queries on a specified loss with respect to an arbitrary model. We provide a tight characterization of the query complexity of $\\mathsf{D}\\mathsf{L}\\mathsf{Q}$ for learning the support of a sparse function over generic product distributions. This complexity crucially depends on the loss function. For the squared loss, $\\mathsf{D}\\mathsf{L}\\mathsf{Q}$ matches the complexity of Correlation Statistical Queries (CSQ)\u2014potentially much worse than SQ. But for other simple loss functions, including the $\\ell_{1}$ loss, $\\mathsf{D}\\mathsf{L}\\mathsf{Q}$ always achieves the same complexity as SQ. We also provide evidence that $\\mathsf{D}\\mathsf{L}\\mathsf{Q}$ can indeed capture learning with (stochastic) gradient descent by showing it correctly describes the complexity of learning with a two-layer neural network in the mean field regime and linear scaling. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In recent years, major efforts have been devoted to understanding which distributions can be learned efficiently using gradient-type algorithms on generic models [Abbe and Sandon, 2020, Allen-Zhu and Li, 2020, Malach et al., 2021, Damian et al., 2022, Abbe et al., 2021b,a, 2022, 2023, Bietti et al., 2023, Glasgow, 2023, Dandi et al., 2023, 2024, Edelman et al., 2024, Kou et al., 2024]. In this paper, we focus on learning sparse functions (i.e. \u201cjuntas\u201d [Blum and Langley, 1997]), that is functions that depend only on a small number $P$ out of a much larger set $d\\gg P$ of input coordinates. The challenge in this setting is to identify the few relevant coordinates. For some sparse functions, such as noisy parities, learning is believed to require $O(d^{P})$ runtime [Kearns, 1998], while others, such as linear functions, are easy to learn in $\\tilde{O}(d)$ time. Which functions are easy to learn and which are hard? What is the complexity of learning a specific sparse function? Recent works [Abbe et al., 2022, 2023] unveiled a rich \u201cleap\u201d hierarchical structure and saddle-to-saddle dynamics that drives learning in this setting. The goal of the present paper is to provide a general characterization for the complexity of learning sparse functions that go beyond $(i)$ hypercube data and Fourier analysis, and (ii) CSQ (see below) and focusing only on the squared loss. ", "page_idx": 0}, {"type": "text", "text": "The notion of complexity we consider is the Statistical Query (SQ) complexity, which studies learning by measuring expectations up to some worst-case tolerance (see [Kearns, 1998, Bshouty and Feldman, 2002, Reyzin, 2020] and Section 3). Although based on worst-case error (or almost equivalently, additive independent noise) rather than the sampling error encountered in practice, statistical query complexity has been proven to be a useful guideline for studying the complexity of learning. In particular, gradient computations are a special case of statistical queries. In specific cases which include binary functions or gradients on the squared or cross-entropy loss, gradient queries are equivalent1 to the restricted class of Correlation Statistical Queries (CSQ) which are strictly less powerful than general statistical queries. Lower bounds on the CSQ complexity have thus been seen as corresponding to the complexity of gradient-based learning2 in these restricted cases. Part of the motivation for this paper is to emphasize that this relationship is limited to very specific loss functions and does not hold more generally. In order to study the complexity of gradient algorithms for general output and loss functions, we introduce a type of statistical query which we call Differentiable Learning Query (DLQ). These queries are defined with respect to a specific loss $\\ell$ \u2014denoted by ${\\sf D}{\\sf L}{\\sf Q}_{\\ell}$ \u2014and are given by gradients on a loss $\\ell$ with respect to an arbitrary model. Specifically, ${\\sf D}{\\sf L}{\\sf Q}_{\\ell}$ algorithms correspond to SQ algorithms with queries of the type ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\n\\phi(y,\\pmb{x})=\\frac{\\partial}{\\partial\\omega}\\ell(f(\\pmb{x},\\omega),y)\\Big|_{\\omega=0},\\quad\\mathrm{where}\\quad f:\\mathcal{X}^{d}\\times\\mathbb{R}\\rightarrow\\mathbb{R}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Depending on the loss $\\ell$ and target distributions, learning with $\\mathsf{D}\\mathsf{L}\\mathsf{Q}_{\\ell}$ can be less, equal, or more powerful than CSQ. ", "page_idx": 1}, {"type": "text", "text": "For inputs on the hypercube $\\pmb{x}\\sim\\mathrm{Unif}(\\{\\pm1\\}^{d})$ and CSQ, Abbe et al. [2022, 2023] showed that the complexity of learning sparse functions is sharply captured by a leap exponent defined in terms of the non-zero Fourier coefficients of the sparse function. Informally, it states that $\\Theta(d^{k_{*}})$ queries are necessary and sufficient to learn with $\\mathsf{C S Q}$ , where $k_{*}$ is the minimum number of coordinates one need to add at once to \u201cleap\u201d between non-zero Fourier coefficients. E.g., consider the following two sparse functions: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{y_{1}=x_{s_{*}(1)}+x_{s_{*}(1)}x_{s_{*}(2)}+x_{s_{*}(1)}x_{s_{*}(2)}x_{s_{*}(3)}+x_{s_{*}(1)}x_{s_{*}(2)}x_{s_{*}(3)}x_{s_{*}(4)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{y_{2}=x_{s_{*}(1)}x_{s_{*}(2)}x_{s_{*}(3)}+x_{s_{*}(1)}x_{s_{*}(2)}x_{s_{*}(4)}+x_{s_{*}(1)}x_{s_{*}(3)}x_{s_{*}(4)}+x_{s_{*}(2)}x_{s_{*}(3)}x_{s_{*}(4)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Both functions depend on an (unknown) subset of $P=4$ coordinates $\\{s_{*}(1),s_{*}(2),s_{*}(3),s_{*}(4)\\}$ . For $y_{1}$ , the monomials are ordered such that we add only one new coordinate to the support at a time: the leap exponent is 1 and $\\Theta(d)$ queries are sufficient to learn $y_{1}$ . For $y_{2}$ , we need to add three new coordinates at once: the leap exponent is 3 and $\\Theta(d^{3})$ queries are required to learn $y_{2}$ . ", "page_idx": 1}, {"type": "text", "text": "We extend and generalize this charactarization in several significant ways: ", "page_idx": 1}, {"type": "text", "text": "(a) We go beyond binary input space and allow for arbitrary measurable space with product distribution. In particular, the leap complexity arises from the structure of the permutation group rather than a specific functional basis.   \n$(b)$ We go beyond $\\mathsf{C S Q}$ and squared loss, and tightly characterize the query complexity for learning sparse functions with $\\mathsf{S Q}$ and $\\mathsf{D}\\mathsf{L}\\mathsf{Q}_{\\ell}$ algorithms with any loss $\\ell$ . This complexity is in terms of a leap exponent defined analogously as above, but now over a set system $\\mathcal{C}_{\\sf A}\\subseteq2^{[P]}$ of \u201cdetectable\u201d subsets $S\\subseteq[P]$ , where \u201cdetectability\u201d depends on the type of queries $\\mathsf{A}\\in\\{\\mathsf{S Q},\\mathsf{C S Q},\\mathsf{D L Q}_{\\ell}\\}$ . Learning with different loss functions are not necessarily comparable to $\\mathsf{C S Q}$ or $\\mathsf{S Q}$ and depend on the sparse function. However, we show that some losses, such as $\\ell_{1}$ or exponential loss, are \u201cgeneric\u201d, i.e. $\\mathcal{C}_{\\sf D L Q_{\\ell}}=\\mathcal{C}_{\\sf S Q}$ for any sparse function, and always match the $\\mathsf{S Q}$ -leap exponent. This shows that differentiable learning with these losses is as powerful as learning with $\\mathsf{S Q}$ for juntas.   \n(c) Finally, we introduce a cover exponent\u2014defined on the same system of detectable sets $\\mathcal{C}_{\\sf A}$ \u2014which captures learning with $\\mathsf{A}\\in\\{\\mathsf{S Q},\\mathsf{C S Q},\\mathsf{D L Q}_{\\ell}\\}$ when the queries are chosen non-adaptively, i.e., without adapting to the responses of previous queries. This can be roughly thought of as learning with a single gradient step, versus many consecutive steps in the adaptive case. The contrast between the two helps understand to what extent learning relies on adaptivity and can help frame other related results\u2014see discussion in Section 8. ", "page_idx": 1}, {"type": "text", "text": "We summarize the complexity of learning a sparse function with SQ, CSQ, $\\mathsf{D}\\mathsf{L}\\mathsf{Q}_{\\ell}$ and adaptive/nonadaptive queries in Table 1. To see some of these notions play out, consider again the two examples in Eq. (2): ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l r l}&{y_{1}:}&&{\\mathsf{L e a p}(\\mathscr C_{\\mathsf C S Q})=1,\\quad\\mathsf{C o v e r}(\\mathscr C_{\\mathsf C S Q})=4,}&&{\\mathsf{L e a p}(\\mathscr C_{\\mathsf S Q})=\\mathsf{C o v e r}(\\mathscr C_{\\mathsf S Q})=1,}\\\\ &{y_{2}:}&&{\\mathsf{L e a p}(\\mathscr C_{\\mathsf C S Q})=\\mathsf{C o v e r}(\\mathscr C_{\\mathsf C S Q})=3,}&&{\\mathsf{L e a p}(\\mathscr C_{\\mathsf S Q})=\\mathsf{C o v e r}(\\mathscr C_{\\mathsf S Q})=1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "table", "img_path": "Q0KwoyZlSo/tmp/4dcf3f3a6e3cd4055cd78e9445d14e91ea36a4f1bc60178b32d6f60d868f5d61.jpg", "table_caption": ["Table 1: Complexity of learning a sparse function over $d$ input coordinates with different query types based on Theorem 5.1. $\\mathsf{D}\\mathsf{L}\\mathsf{Q}_{\\ell}$ is defined in Section 2, Leap and Cover of a set system in Definition 1, and systems $\\mathcal{C}_{\\sf A}$ of detectable sets in Definition 2 based on test functions depending on query type A, as specified in Eq. (14). "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "For $y_{1}$ , $\\mathsf{D}\\mathsf{L}\\mathsf{Q}_{\\ell}$ will require $\\Theta(d)$ adaptive or $\\Theta(d^{4})$ non-adaptive queries to learn with squared loss\u2014 equivalent to $\\mathsf{C S Q}$ , while $\\Theta(d)$ adaptive/non-adaptive queries suffice with $\\ell_{1}$ -loss\u2014equivalent to SQ. For $y_{2}$ , $\\mathsf{D}\\mathsf{L}\\mathsf{Q}_{\\ell}$ will only require $\\Theta(d)$ queries, either adaptive or non-adaptive, with $\\ell_{1}$ -loss, compared to $\\Theta(d^{3})$ queries with the squared loss. Proposition 6.2.(c) provides an example where $\\mathsf{L e a p}(\\mathcal{C}_{\\mathsf{S Q}})<\\mathsf{L e a p}(\\bar{\\mathcal{C}}_{\\mathsf{D L Q}_{\\ell}})<\\mathsf{L e a p}(\\mathcal{C}_{\\mathsf{C S Q}}).$ . ", "page_idx": 2}, {"type": "text", "text": "$\\mathsf{D}\\mathsf{L}\\mathsf{Q}_{\\ell}$ algorithms are quite different than (stochastic) gradient descent algorithms. On one hand, $\\mathsf{D}\\mathsf{L}\\mathsf{Q}_{\\ell}$ allows evaluating arbitrary gradients rather than following a trajectory, and is thus potentially more powerful than (S)GD on generic models3. On the other hand, the lower bound on $\\mathsf{D}\\mathsf{L}\\mathsf{Q}_{\\ell}$ algorithms in Table 1 is against worst-case noise\u2014much more pessimistic than sampling noise encountered in SGD or GD-on-the-training-set [Abbe et al., 2021b]. Nevertheless, ${\\sf D}{\\sf L}{\\sf Q}_{\\ell}$ does provide useful guidance and we expect that it does capture important aspects of the computational complexity of GD in a number of settings. To demonstrate this, Section 7 considers learning sparse functions on the hypercube using online SGD on a two-layer neural network. We show that in this setting, the $\\mathsf{D}\\mathsf{L}\\mathsf{Q}_{\\ell}$ -leap exponent correctly captures learnability in the mean-field regime and linear scaling. Namely, for ${\\sf D}{\\sf L}{\\sf Q}_{\\ell}$ -leap 1 functions, SGD on loss $\\ell$ learns in $O(d)$ -steps, while for leap greater than 1, the dynamics remains stuck in a suboptimal saddle subspace and require $\\omega(d)$ -steps to escape. ", "page_idx": 2}, {"type": "text", "text": "2 Setting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Junta Recovery Problem. A sparse recovery (junta learning) problem with $P$ relevant coordinates is defined in terms of a coordinate input space $\\mathcal{X}$ , an output space $\\mathcal{V}^{4}$ , a marginal distribution $\\mu_{x}$ over $\\mathcal{X}$ and a link distribution $\\mu_{y|z}$ over $\\boldsymbol{\\wp}$ given elements of $\\chi^{P}$ . This specifies a joint distribution $\\mu_{y,z}$ over a measurable space $\\mathcal{V}\\times\\mathcal{X}^{P}$ , where the marginal distribution $z\\sim\\mu_{x}^{P}$ is the product distribution, and $y\\mid z\\sim\\mu_{y\\mid z}$ . We further denote $\\mu_{y}$ to be the marginal distribution over $\\boldsymbol{\\wp}$ . We further denote the junta problem by a tuple $\\boldsymbol{\\mu}:=(\\mu_{x},\\mu_{y|z})$ , where the support size $P$ and the spaces $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ are implicit. Consider some examples, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{z\\sim\\mathrm{Unif}(\\{\\pm1\\}^{P}),\\quad y=h_{*}(z)+\\varepsilon\\quad\\mathrm{for~some~}\\:h_{*}:\\{\\pm1\\}^{P}\\rightarrow\\mathbb{R}\\;\\;\\mathrm{and~noise}\\;\\varepsilon;}\\\\ &{z\\sim\\mathsf{N}(\\mathbf{0},\\mathbf{I}_{P}),\\quad y\\sim\\mathrm{Bernoulli}(\\mathrm{sigmoid}(h_{*}(z)))\\quad\\mathrm{for~some~target~}\\:h_{*}:\\mathbb{R}^{P}\\rightarrow\\mathbb{R}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "For $d\\geq P$ , we define the junta recovery problem as learning the family of distributions ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\mathcal{H}}_{\\mu}^{d}:=\\left\\{{\\mathcal{D}}_{\\mu,s}^{d}:s\\in{\\mathsf{P}}([d],P)\\right\\}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathsf{P}([d],P)$ is the set of non-repeating sequences from $[d]$ of length $P$ , and the distribution $\\mathcal{D}_{\\mu,s}^{d}$ is a distribution over $\\mathcal{V}\\times\\mathcal{X}^{d}$ such that ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{x\\sim\\mu_{x}^{d}\\ \\ \\mathrm{and}\\ \\ \\ y\\ |\\ \\big(x_{s(1)},\\cdot\\cdot\\cdot,x_{s(P)}\\big)\\sim\\mu_{y|z}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In words, $\\mathcal{H}_{\\mu}^{d}$ is the set of distributions on $(y,x_{1},\\ldots,x_{d})$ where $\\textbf{\\em x}$ follows the product distribution $\\mu_{x}^{d}$ and the output $y$ only depends on an (unknown) sequence of $P$ coordinates. For ease of notation, we will denote $\\mathcal{D}_{s}^{d}=\\mathcal{D}_{\\mu,s}^{d}$ when the junta problem $\\boldsymbol{\\mu}=\\left(\\mu_{x},\\mu_{y|z}\\right)$ is clear from context. ", "page_idx": 2}, {"type": "text", "text": "Success as Support Recovery. For any sequence $s\\ \\in\\ \\mathsf{P}([d],P)$ , we additionally denote the associated unordered set of its elements by $S\\;:=\\;\\{s(i)\\;:\\;i\\;\\bar{\\in}\\;\\left[P\\right]\\}$ . We consider learning as \u201csucceeding\u201d if it outputs the correct index set. That is, we say a junta learning algorithm \u201csucceeds\u201d in learning $\\mathcal{H}_{\\mu}^{d}$ if for every $s_{*}\\in\\mathtt{P}([d],P)$ , it outputs $\\hat{S}\\subseteq[d]$ such that $\\hat{S}=S_{*}=\\{s_{*}(i):i\\in[P]\\}$ . ", "page_idx": 3}, {"type": "text", "text": "For our purpose, just the recovery of relevant coordinates is the objective as we only care about the complexity as a scaling of $d$ . Once we recover the relevant coordinates $S_{*}$ , learning the ordered sequence $s_{*}$ corresponds to a problem of size $P$ , and thus its complexity is independent of $d$ . Note that the precise ordering $s_{*}$ may not be identifiable if for some other $s_{*}^{\\prime}$ with $S_{*}\\,=\\,S_{*}^{\\prime}$ we have $\\mathcal{D}_{s_{*}}^{d}=\\bar{\\mathcal{D}}_{s_{*}^{\\prime}}^{d}$ ; this is possible when the measure $\\mu_{y|z}$ has symmetries with respect to coordinates of $_{\\textit{z}}$ . We further emphasize that even the support $S_{*}$ may not be identifiable, when $y$ is independent of some coordinates of $_{\\textit{z}}$ or when only using a restricted query access to the distribution\u2014we will return to this issue in Section 5. ", "page_idx": 3}, {"type": "text", "text": "Does the learner know the link $\\mu_{y|z}?$ We always assume that the learner knows $\\mu_{x}$ . In our formulation, we also assume that the learner additionally knows the link $\\mu_{y|z}$ , and the only unknown is $s_{*}$ . Indeed, our lower bounds hold even for this easier setting. But under mild assumptions, our upper bounds can be obtained with an algorithm that does not require knowing $\\mu_{y|z}$ (the complexity still depends on the problem $\\mu$ )\u2014see discussion in Section 5. ", "page_idx": 3}, {"type": "text", "text": "Well Behaved Distributions. For our lower bounds, we will consider junta problems $\\mu=$ $\\left(\\mu_{x},\\mu_{y\\vert z}\\right)$ , which are \u201cwell-behaved\u201d in a way that is standard in the hypothesis testing literature. Let $\\mu_{y,z}$ be the induced joint distribution on $(y,z)$ where $z\\sim\\mu_{x}^{P}$ and $y\\mid z\\sim\\mu_{y\\mid z}$ . For any subset $U\\subseteq[P]$ , let $\\mu_{y,z,U}$ be the marginal distribution of $(y,(z_{i})_{i\\in U})$ and ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mu_{y,z,U}^{0}=\\mu_{y,z,U}\\otimes\\mu_{\\chi}^{P-|U|},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "meaning that $\\left(y,z_{1},\\ldots,z_{P}\\right)\\sim\\mu_{y,z,U}^{0}$ has $(y,(z_{i})_{i\\in U})\\sim\\mu_{y,z,U}$ and $(z_{i})_{i\\in[P]\\backslash U}\\overset{\\mathrm{i.i.d.}}{\\sim}\\mu_{x}$ independently of $(y,(z_{i})_{i\\in U})$ (we replace $z_{i}$ for $i\\notin U$ with independent draws from the marginal). The marginal distribution of $_{\\textit{z}}$ under $\\mu_{y,z,U}^{0}$ is still $\\mu_{x}^{P}$ . ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.1. For any $U\\subseteq[P]$ , we have $\\mu_{y,z}\\ll\\mu_{y,z,U}^{0}$ , and the Radon-Nikodym derivative $\\mathrm{d}\\mu_{y,z}/\\mathrm{d}\\mu_{y,z,U}^{0}$ is square integrable w.r.t. $\\mu_{y,z,U}^{0}$ , i.e. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mu_{y,z}}{\\mathrm{d}\\mu_{y,z,U}^{0}}\\in L^{2}(\\mu_{y,z,U}^{0})\\ \\,\\,f o r\\,a l l\\ \\ U\\subseteq[P].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This is a standard and implicit assumption in the hypothesis testing literature whenever a corresponding null distribution is considered (here all $\\mu_{y,z,U}^{0}$ , i.e., with label $y$ depending only on a strict subset $U\\subsetneq[P])$ ; more specifically for statistical query lower bounds [Feldman et al., 2017, Damian et al., 2024b], low-degree likelihood ratio [Hopkins, 2018, Kunisky et al., 2019], or contiguity lower bounds [Perry et al., 2018]. It always holds when $\\mathcal{X}$ is finitely supported. We further comment on the necessity of this assumption in Appendix A.1. ", "page_idx": 3}, {"type": "text", "text": "3 Statistical and Differentiable Learning Queries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We will consider three classes of learning algorithms, all based on the statistical query paradigm, but differing in the type of queries allowed, as captured by a set $\\mathcal{Q}$ of allowed queries. ", "page_idx": 3}, {"type": "text", "text": "For a number of queries $q$ , tolerance $\\tau~>~0$ and a set $\\mathcal{Q}\\,\\subseteq\\,\\mathbb{R}^{\\mathcal{V}\\times\\mathcal{X}^{d}}$ of measurable functions $\\mathcal{V}\\times\\mathcal{X}^{d}\\rightarrow\\mathbb{R}$ , a $\\mathcal{Q}$ -restricted statistical query algorithm $A\\in\\mathcal{Q}^{-}{\\mathsf{S Q}}(q,\\tau)$ for junta learning takes an input distribution $\\mathcal{D}$ over $\\mathcal{V}\\times\\mathcal{X}^{d}$ and operates in $q$ rounds where at each round $t\\in\\{1,\\ldots,q\\}$ , it issues a query $\\phi_{t}\\in\\mathcal{Q}$ , and receives a response $v_{t}$ such that ", "page_idx": 3}, {"type": "equation", "text": "$$\n|v_{t}-\\mathbb{E}_{\\mathcal{D}}\\left[\\phi_{t}(y,\\pmb{x})\\right]|\\leq\\tau\\sqrt{\\mathbb{E}_{\\mathcal{D}_{0}}[\\phi_{t}(y,\\pmb{x})^{2}]},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{D}_{0}=\\mathcal{D}_{y}\\otimes\\mathcal{D}_{x}$ is the associated decoupled \u201cnull\u201d distribution where $\\textbf{\\em x}$ and $y$ are independent, but follow their marginals5. The query $\\phi_{t}$ can depend on the past responses $v_{1},\\ldots,v_{t-1}$ . After issuing $q$ queries, the learner $\\boldsymbol{\\mathcal{A}}$ outputs $\\hat{S}\\subseteq[d]$ . We say that $\\boldsymbol{\\mathcal{A}}$ succeeds in learning $\\mathcal{H}_{\\mu}^{d}$ if for any $\\mathcal{D}_{s_{*}}\\in\\mathcal{H}_{\\mu}^{d}$ and any responses $v_{t}$ satisfying (8) for $\\mathcal{D}=\\mathcal{D}_{s_{*}}$ , $\\boldsymbol{\\mathcal{A}}$ outputs $\\hat{S}=S_{*}$ . ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Above we allow the queries to be chosen adaptively, i.e., depending on past responses. We also consider $\\mathcal{Q}$ -restricted non-adaptive statistical query algorithms, which we denote by $\\mathcal{Q}{\\sf-n a S Q}(q,\\tau)$ , where the query functions $\\{\\phi_{t}\\}_{t\\in[q]}$ are fixed in advance and do not depend on the past responses. I.e. a non-adaptive algorithm is specified by a list of queries and a mapping from the responses to an output $\\hat{S}\\subseteq[\\bar{d}]$ . ", "page_idx": 4}, {"type": "text", "text": "Statistical Queries (SQ): In regular, unrestricted Statistical Query learning, the allowed query set, denoted by $\\mathcal{Q}_{\\sf S Q}$ , is the set of all measurable functions. With slight overloading of notation, we refer to the class of these algorithms simply as ${\\sf S Q}(q,\\tau)$ and $\\mathsf{n a S Q}(\\bar{q_{1}},\\tau)$ . ", "page_idx": 4}, {"type": "text", "text": "Correlation Statistical Queries (CSQ): It is a special subclass of statistical queries, which require $\\mathcal{V}\\subseteq\\mathbb{R}$ (usually we allow the input and output spaces to be abstract), and are restricted to: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{Q}_{\\sf C S Q}=\\left\\{\\phi(y,\\pmb{x})=y\\cdot\\tilde{\\phi}(\\pmb{x})\\mid\\tilde{\\phi}:\\pmb{x}\\to\\mathbb{R}\\mathrm{~measurable}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We denote the class of adaptive and non-adaptive algorithms making such queries as $\\mathsf{C S Q}(q,\\tau)$ and $\\mathsf{n a C S Q}(q,\\tau)$ respectively. ", "page_idx": 4}, {"type": "text", "text": "Differentiable Learning Queries (DLQ) with Loss $\\ell$ : Let ${\\mathcal{F}}\\subseteq V$ , which is an open subset of some normed vector space $V$ , be the output space of our models, e.g. usually $V\\,=\\,\\mathcal{F}\\,=\\,\\mathbb{R}$ for models with a single output unit, but we may have $V=\\mathcal{F}=\\mathbb{R}^{r}$ with multiple output units. We consider a loss function $\\ell:\\mathcal{F}\\times\\mathcal{Y}\\rightarrow\\mathbb{R}$ that is locally Lipschitz continuous in its first argument for every $y\\in\\mathcal{V}$ . The loss is additionally equipped with a derivative operator $\\nabla\\ell:\\mathcal{F}\\times\\mathcal{Y}\\rightarrow V$ as a part of the loss definition such that for any $\\pmb{u}\\in\\mathcal{F}$ and $y\\in\\mathcal{V}$ , we have $\\nabla\\ell(\\boldsymbol{\\mathbf{\\mathit{u}}},y)\\in\\partial_{1}\\ell(\\boldsymbol{\\mathbf{\\mathit{u}}},y)$ , the set of generalized Clarke subderivatives of $\\ell(\\cdot,y)$ at $\\pmb{u}\\in\\mathcal{F}$ . This is a standard generalization of derivatives to non-differentiable and non-convex losses; in particular, note that $(i)$ for differentiable losses, $\\partial_{1}\\ell(u,y)$ is a singleton with the true gradient of $\\ell(\\cdot,y)$ at $\\pmb{u}\\in\\mathcal{F}$ , and $(i i)$ for convex losses (in the first argument), $\\partial_{1}\\ell(\\boldsymbol{u},\\boldsymbol{y})$ is the set of subderivatives of $\\ell(\\cdot,y)$ at $\\pmb{u}\\in\\mathcal{F}$ . Finally, let ${\\mathcal{M}}:=\\{f:{\\mathcal{X}}^{d}\\times\\mathbb{R}\\to{\\mathcal{F}}\\mid f({\\boldsymbol{x}},{\\boldsymbol{\\omega}})$ is differentiable at $\\omega=0$ for all $\\pmb{x}\\in\\mathcal{X}^{d}\\}$ be the set of allowed models6. Then the allowed differentiable learning query set is: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{Q}_{\\sf D L Q_{\\ell}}=\\left\\{\\phi(y,\\pmb{x})=\\left[\\frac{\\mathrm{d}}{\\mathrm{d}\\omega}f(\\pmb{x},\\omega)\\right]_{\\omega=0}^{\\top}\\nabla\\ell(f(\\pmb{x},0),y)\\ \\middle|\\ f\\in\\mathcal{M}\\right\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "That is, at each round the algorithm chooses a parametric model $f(\\pmb{x},\\omega)$ , parameterized by a single scalar $\\omega$ , and the query corresponds to the derivative (with respect to the single parameter $\\omega$ ) of the loss applied to the model, at $\\omega=0$ . This captures the first gradient calculation for a single-parameter model initialized at zero. But the derivative at any other point can also be obtained by querying at a shifted model $f_{(\\nu)}({\\pmb x},\\omega)=f({\\pmb x},\\nu+\\omega)$ , and the gradient with respect to a model $f({\\boldsymbol{x}},{\\boldsymbol{w}})$ with $r$ parameters $\\pmb{w}\\in\\mathbb{R}^{r}$ can be obtained by issuing $r$ queries, one for each coordinate, of the form $f_{({\\pmb w},i)}({\\pmb x},\\omega)=f({\\pmb x},{\\pmb w}+\\omega e_{i})$ , where $e_{i}$ is the standard basis vector. Queries of the form $\\mathcal{Q}_{\\sf D L Q_{\\ell}}$ can thus be used to implement gradient calculations for any differentiable model, noting that the number of queries $q$ is the number of gradient calculations times the number of parameters. Finally, observe that, for differentiable losses, the queries of the form (1) are equivalent to the form mentioned in (10) due to the chain rule. ", "page_idx": 4}, {"type": "text", "text": "We denote the class of adaptive and non-adaptive algorithms making such queries as $\\mathsf{D L Q}_{\\ell}(q,\\tau){:=}\\mathcal{Q}_{\\mathsf{D L Q}_{\\ell}}{-}\\mathsf{S Q}(\\bar{q},\\tau)$ and $\\mathsf{n a D L Q}_{\\ell}(q,\\tau){:=}\\mathcal{Q}_{\\mathsf{D L Q}_{\\ell}}{\\mathsf{n a S Q}}(\\bar{q},\\tau)$ . ", "page_idx": 4}, {"type": "text", "text": "Remark 3.1. More common in the $S Q$ literature is to restrict the query in $L^{\\infty}$ (or equivalently, require precision relative to $L^{\\infty}$ ). Precision relative to $L^{2}(\\mathcal{D}_{0})$ is more similar to VSTAT [Feldman et al., 2017], and is more powerful than relative to $L^{\\infty}$ , and our lower bounds hold against this stronger notion. In our algorithms and upper bounds we only need this additional power when $y\\mapsto\\nabla\\ell({\\pmb u},y)$ is unbounded. If we further assume that these functions are bounded, e.g., the labels $y$ are bounded and ${\\boldsymbol\\nabla}{\\boldsymbol\\ell}$ continuous, our queries have bounded $L^{\\infty}$ and thus operate in the more familiar $S Q$ setting. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "4 Leap and Cover Complexities ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Definition 1. We define the leap and cover complexities for any system of subsets ${\\mathcal{C}}\\subseteq2^{[P]}$ . ", "page_idx": 5}, {"type": "text", "text": "(i) For any system of subsets ${\\mathcal{C}}\\subseteq2^{[P]}$ , its leap complexity is defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathsf{L e a p}(\\mathcal{C}):=\\operatorname*{min}_{\\stackrel{r_{1},\\ldots,U_{r}\\in\\mathcal{C}}{\\bigcup_{i=1}^{r}U_{i}=[P]}}\\operatorname*{max}_{i\\in[r]}\\,|U_{i}\\setminus\\cup_{j=0}^{i-1}U_{j}|.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "(ii) We define the cover complexity of $\\mathcal{C}$ to be ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\mathsf{C o v e r}}({\\mathcal{C}}):=\\operatorname*{max}_{i\\in[P]}\\ \\operatorname*{min}_{U\\in{\\mathcal{C}},i\\in U}\\ |U|.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Remark 4.1. We always have Lea $\\mathsf{p}(\\mathcal{C})\\leq\\mathsf{C o v e r}(\\mathcal{C})$ . Both Leap and Cover complexities are closed under taking the union of subsets in $\\mathcal{C}$ . Also, when $\\mathrm{supp}(\\mathcal{C})\\,=\\,\\cup_{U\\in\\mathcal{C}}U\\,\\ne\\,[\\dot{P}],$ , then we use the convention $\\bar{\\mathsf{L e a p}}(\\mathcal{C})=\\mathsf{C o v e r}(\\mathcal{C})=\\infty$ . See a discussion about this convention in Appendix A.3 and in particular, the definition of the relative leap and cover complexities in Definition 3. ", "page_idx": 5}, {"type": "text", "text": "Here $\\mathcal{C}$ is the system of subsets which are \u201cdetectable\u201d, and will depend on the the query access model. Intuitively, the leap and cover complexities of $\\mathcal{C}$ capture the exponent of $d$ in the query complexity when recovering the support of an unknown $s_{*}\\in\\mathtt{P}([d],P)$ , for adaptive and non-adaptive algorithms respectively. To discover the relevant coordinates of $s_{*}$ , that correspond to $U\\in{\\mathcal{C}}$ , one needs to enumerate over $\\Theta(d^{|U|})$ . Hence, a non-adaptive algorithm, which fixes the queries in advance, requires $\\Theta(d^{k_{i}})$ queries to discover $i^{\\mathrm{th}}$ relevant coordinate i.e. $s_{*}(i)$ , where $k_{i}=\\operatorname*{min}_{i\\in U\\in{\\mathcal{C}}}|U|$ . Therefore, non-adaptive algorithms need a total number of queries that scales as $\\Theta(d^{\\mathsf{C o v e r}({\\mathcal{C}})})$ to learn $\\operatorname{supp}(s_{*})$ . On the other hand, adapting queries using previous answers can greatly reduce this complexity as seen in the example in (2) in Section 1. This is captured by the leap complexity, which measures the maximum number of coordinates we need to discover at once. Finally, the set system of detectable subsets will depend on the type of allowed queries. ", "page_idx": 5}, {"type": "text", "text": "Definition 2 (Detectable Subsets). Let $\\mu$ be a junta problem. Denote ", "page_idx": 5}, {"type": "equation", "text": "$$\nL_{0}^{2}(\\mu_{x})=\\left\\{T\\in L^{2}(\\mu_{x}):\\mathbb{E}_{z\\sim\\mu_{x}}[T(z)]=0\\right\\}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "the set of zero-mean functions. For a set of test function $\\Psi\\,\\subseteq\\,L_{2}(\\mu_{y})$ we say that $U\\subseteq\\left[P\\right]$ is $\\Psi$ -detectable iff ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\exists T\\in\\Psi,\\ \\exists T_{i}\\in L_{0}^{2}(\\mu_{x})\\{\\mathrm{or}\\ \\mathrm{each}\\,i\\in U\\quad{s u c h}\\ t h a t\\quad\\mathbb{E}_{\\mu_{y},z}\\left[T(y)\\prod_{i\\in U}T_{i}(z_{i})\\right]\\neq0.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We denote $\\mathcal C_{\\Psi}(\\mu)$ the set of $\\Psi$ -detectable sets, i.e. those sets satisfying (13). ", "page_idx": 5}, {"type": "text", "text": "The set of relevant test functions depend on the query types allowed and we define: ", "page_idx": 5}, {"type": "text", "text": "For CSQ (recall $\\mathcal{V}\\subseteq\\mathbb{R}$ ), $\\Psi_{\\mathsf{C S Q}}=\\{y\\mapsto y\\}$ (just the identity). ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Psi_{\\sf D L Q_{\\ell}}=\\{y\\mapsto\\pmb{v}^{\\sf T}\\nabla\\ell(\\pmb{u},y):\\pmb{u}\\in\\mathcal{F},\\pmb{v}\\in V\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "While the queries of the form (13), where $\\Psi_{\\mathsf{A}}$ for $\\mathsf{A}\\in\\{\\mathsf{S Q},\\mathsf{C S Q},\\mathsf{D L Q}_{\\ell}\\}$ is given by (14), are less general than $\\phi(y,x)$ with $\\phi\\in\\mathcal{Q}_{\\mathsf{A}}$ , they can be implemented by the corresponding query types $\\mathcal{Q}_{\\sf A}$ and are sufficient for deciding between \u201c $^{\\cdot}S\\subseteq[d]$ maps to the corresponding $U\\subseteq[P]^{*}$ or $^{\\bullet}S\\not\\subseteq S_{*}{}^{,}$ . The sets $\\Psi_{5\\mathsf{Q}},\\Psi_{\\mathsf{C S Q}},\\Psi_{\\mathsf{D L Q}_{\\ell}}$ from (14) used for detectibility arise naturally in the proof of the lower bounds of Theorem 5.1. ", "page_idx": 5}, {"type": "text", "text": "To ease notation, for query type A, we use the shorthand $\\mathcal{C}_{\\sf A}:=\\mathcal{C}_{\\Psi_{\\sf A}}$ , and $\\begin{array}{r}{\\mathbf{eap}_{\\mathbb{A}}(\\mu):=\\mathsf{L e a p}(\\mathcal{C}_{\\mathbb{A}}(\\mu))=\\mathsf{L e a p}(\\mathcal{C}_{\\Psi_{\\mathbb{A}}}(\\mu)),\\qquad\\mathsf{C o v e r}_{\\mathbb{A}}(\\mu):=\\mathsf{C o v e r}(\\mathcal{C}_{\\mathbb{A}}(\\mu))=\\mathsf{C o v e r}(\\mathcal{C}_{\\Psi_{\\mathbb{A}}}(\\mu)).}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "We refer to these as the A-leap exponent and A-cover exponent of the problem $\\mu$ . ", "page_idx": 5}, {"type": "text", "text": "5 Main Result: Characterizing the Complexity of Learning Juntas ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Theorem 5.1. For any junta problem $\\mu$ and any loss $\\ell$ , there exists $C>c>0$ (that depend on $P,\\mu$ and the loss, but not on $d$ ), such that for query types $\\mathsf{A}\\in\\{\\mathsf{S Q},\\mathsf{C S Q},\\mathsf{D L Q}_{\\ell}\\}$ with corresponding test function sets $\\Psi_{\\mathsf{A}}$ as defined in (14): ", "page_idx": 6}, {"type": "text", "text": "Adaptive. Let $k_{*}=\\mathsf{L e a p_{A}(\\mu)}$ . There exists an algorithm $A\\in\\mathsf{A}(q,\\tau)$ that succeeds in learning $\\mathcal{H}_{\\mu}^{d}$ with $\\tau=c$ and $q=C d^{k_{*}}$ . And if $\\mu$ satisfies Assumption 2.1, then for any $(q,\\tau)$ such that $q/\\tau^{2}\\leq c d^{k_{*}}$ , no algorithm $A\\in\\mathsf{A}(q,\\tau)$ succeeds at learning $\\mathcal{H}_{\\mu}^{d}$ . ", "page_idx": 6}, {"type": "text", "text": "Non-adaptive. Let $k_{*}\\,=\\,\\mathsf{C o v e r_{A}}(\\mu)$ . There exists an algorithm $A\\in\\mathsf{n a A}(q,\\tau)$ that succeeds in learning $\\mathcal{H}_{\\mu}^{d}$ with $\\tau=c$ and $q=C d^{k_{*}}$ . And if $\\mu$ satisfies Assumption 2.1, then for any $(q,\\tau)$ such that $q/\\tau^{2}\\leq c d^{k*}$ , no algorithm $\\mathcal{A}\\in\\mathsf{n a A}(q,\\tau)$ succeeds at learning $\\mathcal{H}_{\\mu}^{d}$ . ", "page_idx": 6}, {"type": "text", "text": "Remark 5.2. In the positive results in Theorem 5.1, we used all the allowed complexity to have many $(q=\\Theta(d^{k_{*}}))$ queries, and kept the tolerance constant. More generally, it is possibly to trade off between the number of queries $q$ and tolerance $\\tau$ , at a cost of a log-factor: For $k_{*}=\\mathsf{L e a p_{A}(\\mu)}$ and $k_{*}=\\mathsf{C o v e r_{A}}(\\mu)$ , respectively, there exists algorithms $A\\in\\mathsf{A}(q,\\tau)$ and $A\\in\\mathsf{n a A}(q,\\tau)$ that learn $\\mathcal{H}_{\\mu}^{d}$ for any $q\\geq C\\log(d)$ and $\\tau\\leq c$ with $q/\\tau^{2}\\geq C d^{k_{*}}\\log(d)$ . ", "page_idx": 6}, {"type": "text", "text": "The proof of this theorem is deferred to Appendix B. Theorem 5.1 shows that the leap and cover complexities sharply capture the scaling in $d$ of statistical query algorithms when learning $\\mathcal{H}_{\\mu}^{d}$ . ", "page_idx": 6}, {"type": "text", "text": "Remark 5.3. The above upper bound uses that $\\mu$ and therefore the $T,T_{i}$ in Definition 2 are known. In the case when $\\mu$ is unknown, one can follow a similar strategy as in Damian et al. [2024b] and randomize the transformations $T$ and $T_{i}$ over a sufficiently large (but finite independent of $d$ ) linear combination of functions in $\\Psi_{\\mathsf{S Q}}$ and $L_{0}^{2}(\\mu_{x})$ . Under some regularity assumption on $\\mu$ and $\\ell$ , one can show by anti-concentration that with constant probability, the expectation in condition (13) is bounded away from 0 by a constant independent of the dimension. ", "page_idx": 6}, {"type": "text", "text": "6 Relationship Between SQ, CSQ and $\\mathsf{D L Q}_{\\ell}$ ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Obviously, ${\\mathsf{C S Q}}\\subseteq{\\mathsf{S Q}}$ , and indeed we see that $\\mathcal{C}_{\\sf C S Q}\\ \\subseteq\\ \\mathcal{C}_{\\sf S Q}$ in Definition 2 because of which $\\mathsf{L e a p}_{\\mathsf{C S Q}}\\,\\geq\\,\\mathsf{L e a p}_{\\mathsf{S Q}}$ and $\\mathsf{C o v e r c s Q\\Sigma\\geq C o v e r s Q}$ . For binary $\\boldsymbol{\\wp}$ , these query models collapse, but otherwise there can be an arbitrary gap. ", "page_idx": 6}, {"type": "text", "text": "Proposition 6.1 (SQ versus CSQ). For any $\\mu_{;}$ , let $\\mathcal{C}_{5\\mathsf{Q}}:=\\mathcal{C}_{5\\mathsf{Q}}(\\mu)$ , and $\\mathcal{C}_{\\sf C S Q}:=\\mathcal{C}_{\\sf C S Q}(\\mu)$ . If $|\\mathcal{Y}|=2$ (binary output), then we always have $\\mathcal{C}_{\\sf C S Q}=\\mathcal{C}_{\\sf S Q}$ and $\\mathsf{L e a p}_{\\mathsf{S Q}}=\\mathsf{L e a p}_{\\mathsf{C S Q}}\\,a n d\\,\\mathsf{C o v e r}_{\\mathsf{S Q}}=\\mathsf{C o v e r}_{\\mathsf{C S Q}}.$ On the other hand if $|\\mathcal{V}|>2$ , the $\\mathsf{S Q}$ -exponents can be much smaller than the CSQ-exponents: e.g., there exist a setting with $\\mathsf{L e a p}_{\\mathsf{S Q}}=\\mathsf{C o v e r}_{\\mathsf{S Q}}=1$ and $\\mathsf{L e a p}_{\\mathsf{C S Q}}=\\mathsf{C o v e r}_{\\mathsf{C S Q}}=P$ . ", "page_idx": 6}, {"type": "text", "text": "Similarly, $\\mathsf{D L Q}_{\\ell}\\subseteq\\mathsf{S Q}$ by definition, and thus, ${\\mathsf{L e a p}}_{\\mathsf{D L Q}_{\\ell}}\\geq{\\mathsf{L e a p}}_{\\mathsf{S Q}}$ and ${\\mathsf{C o v e r\\mathsf{D L Q}}}_{\\ell}\\geq{\\mathsf{C o v e r\\mathsf{S Q}}}$ ", "page_idx": 6}, {"type": "text", "text": "Proposition 6.2 $\\mathrm{\\DeltaDLQ}_{\\ell}$ versus SQ and CSQ). Consider any $\\boldsymbol{\\mu}=\\left(\\mu_{y},\\mu_{y|z}\\right)$ . ", "page_idx": 6}, {"type": "text", "text": "(a) Let Y, ${\\mathcal{F}}\\subseteq\\mathbb{R}$ . For the squared loss $\\ell:(u,y)\\mapsto(u-y)^{2}$ , we always have $\\mathcal{C}_{\\sf D L Q_{\\ell}}=\\mathcal{C}_{\\sf C S Q}$ , and thus, $\\mathsf{L e a p}_{\\mathsf{C S Q}}=\\mathsf{L e a p}_{\\mathsf{D L Q}_{\\ell}}$ and ${\\mathsf{C o v e r c s Q}}={\\mathsf{C o v e r}}_{\\mathsf{D L Q}_{\\ell}}$ .   \n(b) A sufficient condition for $\\mathcal{C}_{5\\sf Q}=\\mathcal{C}_{\\sf D L Q,\\ell}$ is to have $\\operatorname{span}(\\Psi_{\\mathsf{D L Q}_{\\ell}})$ dense in $L_{0}^{2}(\\mu_{y})$ . Conversely, if there exists nonzero $T~\\in~L_{0}^{2}(\\mu_{y})$ bounded with $T(y)$ orthogonal in $L^{2}(\\mu_{y})$ to any functions in $\\Psi_{\\mathsf{D L Q}_{\\ell}}$ , then there exists a problem $\\mu$ such that ${\\mathsf{C o v e r}}_{\\mathsf{D L Q}_{\\ell}}\\,=\\,{\\mathsf{L e a p}}_{\\mathsf{D L Q}_{\\ell}}\\,>$ CoverSQ = LeapSQ.   \n(c) There exists a loss $\\ell$ and a junta problem $\\mu$ such that $\\mathsf{L e a p}_{\\mathsf{S Q}}(\\mu)\\ <\\ \\mathsf{L e a p}_{\\mathsf{D L Q}_{\\ell}}(\\mu)\\ <$ $\\mathsf{L e a p}_{\\mathsf{C S Q}}(\\mu)$ . Similarly, we can have $\\mathsf{L e a p}_{\\mathsf{D L Q}_{\\ell}}>\\mathsf{L e a p}_{\\mathsf{C S Q}}$ . ", "page_idx": 6}, {"type": "text", "text": "The condition in Proposition 6.2.(b) can be seen as a universal approximation property of neural networks with activation $\\boldsymbol{y}\\mapsto\\nabla\\ell(\\mathbf{u},\\boldsymbol{y})$ [Cybenko, 1989, Hornik, 1991, Sonoda and Murata, 2017]. The next lemma gives a few examples of losses with $\\mathsf{D}\\mathsf{L}\\mathsf{Q}_{\\ell}=\\mathsf{S}\\mathsf{Q}$ . For concreteness, we consider ${\\mathcal{F}}=\\mathbb{R}$ and $\\mathcal{V}\\subseteq\\mathbb{R}$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 6.3. For $\\ell\\in\\{\\ell_{1}:(u,y)\\mapsto|u-y|,\\ell_{h i n g e}:(u,y)\\mapsto\\operatorname*{max}(1-u y,0)\\}$ , then $\\mathcal{C}_{\\sf D L Q_{\\ell}}(\\mu)=$ $\\mathcal{C}_{\\sf S Q}(\\mu)$ for all7 problems $\\mu,$ . If we further assume ${\\mathcal{V}}\\subseteq[-M,M]$ for $M\\geq0,$ , then $\\ell(u,y)=e^{-u y}$ (exponential loss) has $\\mathcal{C}_{\\sf D L Q_{\\ell}}(\\mu)=\\mathcal{C}_{\\sf S Q}(\\mu)$ for all problems $\\mu$ . ", "page_idx": 7}, {"type": "text", "text": "The cases of $\\ell_{1}$ and Hinge loss follow directly from universal approximation of neural networks with linear threshold activation. The proofs of the above propositions and lemma can be found in Appendix C. Propositions 6.1 and 6.2 combined with Theorems 6.3 and 5.1 directly imply a number of separation results between adaptive and non-adaptive algorithms and between different loss functions. See examples (2) in the introduction, and further examples in Appendix C. ", "page_idx": 7}, {"type": "text", "text": "7 Gradient Descent on Neural Networks ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The goal of this section is to connect the complexity of $\\mathsf{D}\\mathsf{L}\\mathsf{Q}$ to gradient descent on standard neural networks. We focus on the simple case of $\\bar{\\mathbf{x}}\\sim\\operatorname{Unif}(\\{+1,-1\\}^{d})$ uniformly distributed on the hypercube, and $\\mathcal{V}\\subseteq\\mathbb{R}$ and ${\\mathcal{F}}=\\mathbb{R}$ . In this setting, condition (13) in Definition 2 simplifies to: there exists $T\\in\\Psi_{\\mathsf{A}}$ such that $\\begin{array}{r}{\\mathbb{E}_{\\mu_{y,z}}\\left[T(y)\\prod_{i\\in U}z_{i}\\right]=\\mathbb{E}_{\\mu_{y,z}}\\left[T(y)\\chi_{U}(z)\\right]\\neq0}\\end{array}$ , where $\\bar{\\chi}_{U}(z):=$ $\\textstyle\\prod_{i\\in U}z_{i}$ denote the standard Fourier-Walsh basis. In particular, the set $\\mathcal{C}_{\\sf C S Q}$ contains exactly all non-zero Fourier coefficients of $h_{*}(z):=\\mathbb{E}[y|z]$ , and we recover the leap exponent of Abbe et al. [2023] as discussed in the introduction. ", "page_idx": 7}, {"type": "text", "text": "We train a standard two layer neural network (see (NN1) in Appendix D) of width $M$ , using online SGD with a loss function $\\ell:\\mathbb{R}\\times\\mathcal{Y}\\rightarrow\\mathbb{R}_{\\ge0}$ , i.e. SGD on $\\mathbb{E}_{(\\pmb{x},y)\\sim\\mathcal{D}_{s_{*}}^{d}}[\\ell(f_{\\mathsf{N N}}(\\pmb{x};\\pmb{\\Theta}),y)]$ for $\\mathcal{D}_{s_{*}}^{\\tilde{d}}\\in\\mathcal{H}_{\\mu}^{d}$ . More specifically, we train the parameters $\\Theta=\\{\\pmb{\\theta}_{j}:j\\in[M]\\}$ using batch-SGD with loss $\\ell$ and batch size $b$ from initialization $(\\pmb{\\theta}_{j})_{j\\in[M]}\\stackrel{\\mathrm{i.i.d.}}{\\sim}\\rho_{0}$ specified in (69). At each step, given samples $(\\{(x_{k i},y_{k i}):i\\in[b]\\})_{k\\geq0}$ , the weights are updated using ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\theta_{j}^{k+1}=\\theta_{j}^{k}-\\frac{\\eta}{b}\\left(\\sum_{i\\in[b]}\\ell^{\\prime}(f_{\\mathsf{N N}}(\\boldsymbol{x}_{k i};\\Theta^{t}),y_{k i})\\nabla_{\\theta}\\sigma_{*}(\\boldsymbol{x}_{k i};\\theta_{j}^{k})+\\lambda\\,\\theta_{j}^{k}\\right),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\eta$ is the step-size and we allow for a $\\ell_{2}$ regularization with parameter $\\lambda\\in\\mathbb{R}_{+}$ . Recall that $\\ell^{\\prime}(u,y)\\in\\partial_{1}\\ell(u,y)$ is the defined derivative of $\\ell(\\cdot,y)$ at $u\\in\\mathbb{R}$ . We define the test error ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{R}(f)=\\mathbb{E}_{\\mathcal{D}_{s_{*}}^{d}}\\left[\\ell(f(\\pmb{x}),y)\\right],\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "and further introduce the excess test error $\\overline{{\\mathcal{R}}}(f)=\\mathcal{R}(f)-\\operatorname*{inf}_{\\bar{f}:\\{\\pm1\\}^{d}\\rightarrow\\mathbb{R}}\\mathcal{R}(\\bar{f}).$ ", "page_idx": 7}, {"type": "text", "text": "Dimension-free dynamics. In the junta learning setting, when $y$ only depends on $P\\ll d$ coordinates, following Abbe et al. [2022, Secion 3], the SGD dynamics ( $\\boldsymbol{\\ell}$ -bSGD) concentrates on an effective dimension-free (DF) dynamics as $M,d\\rightarrow\\infty$ and $\\eta\\rightarrow0$ . This equivalence holds under a certain assumption on the loss function, and other assumptions on the initialization and activation that are similar to the setup of Abbe et al. [2022] (see Appendix D for details, especially DF-PDE). ", "page_idx": 7}, {"type": "text", "text": "Dimension-free dynamics\u2019 alignment with the support. In the above limiting regime, $\\mathsf{L e a p}_{\\mathsf{D L Q}_{\\ell}}$ crisply characterizes (DF-PDE) dynamics\u2019 alignment with the support. ", "page_idx": 7}, {"type": "text", "text": "Theorem 7.1 (Informal version of Theorem D.6). $\\begin{array}{r}{\\lceil f\\lfloor\\tt e a p_{D L Q_{\\ell}}=1}\\end{array}$ then for some time $t$ , the output of the model at time $t$ of (DF-PDE) dynamics depends on all coordinates $z_{i}$ . On the other hand, if $\\mathsf{L e a p}_{\\mathsf{D L Q}_{\\ell}}>1$ , then there exists a coordinate i such that for any time $t\\geq0$ , the output of the model at time $t$ of (DF-PDE) dynamics does no depend on $z_{i}$ . ", "page_idx": 7}, {"type": "text", "text": "This establishes that the ability of (DF-PDE) dynamics (comparable to ( $\\mathcal{E}$ -bSGD) in the linear scaling) to learn all relevant coordinates depends on $\\mathsf{L e a p}_{\\mathsf{D L Q}_{\\ell}}(\\mu)=1$ . That is if $\\mathsf{L e a p}_{\\mathsf{D L Q}_{\\ell}}>1$ , then (DF-PDE) remains stuck in a suboptimal saddle subspace. On the other hand, if ${\\mathsf{L e a p}}_{\\mathsf{D L Q}_{\\ell}}=1$ , then (DF-PDE) dynamics escapes this subspace and the weights align with the entire support. ", "page_idx": 7}, {"type": "text", "text": "Learning of ${\\mathsf{L e a p}}_{\\mathsf{D L Q}_{\\ell}}=1$ with finite SGD. Showing directly that (DF-PDE) dynamics indeed reach a near global minimizers of the test error remains challenging. Alternatively, we show that a specific layer-wise training dynamics similar to Abbe et al. [2022] achieves a vanishing excess error for $\\mathsf{L e a p\\_p}_{\\mathsf{D L Q}_{\\ell}}(\\mu)=1$ settings in the linear scaling of samples. ", "page_idx": 8}, {"type": "text", "text": "Roughly speaking, we train the first layer for $k_{1}=P$ steps and then the second layer weights for $k_{2}=O_{d}(1)$ steps using batch-SGD with batch size $b=O_{d}(d)$ , both for a loss $\\ell$ . We consider a polynomial activation $\\bar{\\sigma}(x)\\,=\\,(1+x)^{L}$ of degree $L\\ \\geq2^{8P}$ . The most notable difference from Abbe et al. [2022] is that we further slightly perturb step-sizes for each coordinate $\\eta^{w_{i}}=\\eta\\kappa_{i}$ with $\\kappa_{i}\\in[1/2,3/2]$ , and denote $\\pmb{\\kappa}=(\\kappa_{1},...\\,,\\kappa_{d})\\in\\mathbb{R}^{d}$ for the first layer training. This perturbation is necessary to break possible coordinate symmetries; see Remark 7.3. ", "page_idx": 8}, {"type": "text", "text": "Theorem 7.2 (Informal version of Theorem E.1). For a convex and analytic loss $\\ell$ , almost surely over the perturbation $\\kappa$ and the initial bias $\\bar{c}\\in\\mathbb R$ the following holds. If $\\begin{array}{r}{{\\bf\\nabla}\\cdot{\\sf L e a p}_{\\sf D L Q}_{\\ell}(\\mu)=1,}\\end{array}$ , then the above layer-wise SGD training dynamics with total sample size $n=\\Theta_{d}(d)$ and $M=\\Theta_{d}(1)$ achieves excess test error $\\overline{{\\mathcal{R}}}\\big(f_{\\mathsf{N N}}(\\cdot;\\Theta^{k_{1}+k_{2}})\\big)=o(1)$ with high probability. ", "page_idx": 8}, {"type": "text", "text": "The formal statement and the precise training specifications can be found in Appendix D. This result generalizes Abbe et al. [2022, Theorem 9] beyond squared loss. ", "page_idx": 8}, {"type": "text", "text": "Remark 7.3. A slight coordinate-wise perturbation in the step-sizes for the first layer training is necessary to break the potential coordinate symmetries in the output $y$ \u2014see discussion in Abbe et al. [2022, Appendix A]. This can be removed by either stating the theorem for all but some measure zero set of Leap-1 functions as in Abbe et al. [2022], or by studying the dynamics for $O(\\log d)$ steps. ", "page_idx": 8}, {"type": "text", "text": "The query complexity of $b$ -batch SGD on $M$ neurons for $\\Upsilon$ SGD-steps is $T_{\\mathsf{C}}\\,=\\,\\Theta(b M\\Upsilon d)$ . The above theorems show that for $\\mathsf{L e a p}_{\\mathsf{D L Q}_{\\ell}}(\\mu)=1$ , $\\Upsilon=\\Theta(d/b)$ steps with $M=\\Theta(1)$ neurons\u2014and therefore $T_{\\mathsf{C}}=\\Theta(d^{2})$ \u2014suffices to learn the support and minimize the excess test error. Furthermore, for $\\mathsf{L e a p}_{\\mathsf{D L Q}_{\\ell}}(\\mu)>1$ and neural networks trained in the mean-field regime, $\\Upsilon=\\Theta(d)$ (and therefore $T_{\\mathsf C}=\\Theta(d^{2}))$ is not enough. We further comment on the general conjectural picture in Appendix A.2. ", "page_idx": 8}, {"type": "text", "text": "Numerical Simulation. We consider a function similar to $y_{2}$ in (2) with $\\mathsf{L e a p c s Q}=3$ but $\\mathsf{L e a p}_{\\mathsf{S Q}}=$ 1. Specifically, we set $P=4$ and $\\mathcal{C}=\\{\\{1,2,3\\},\\{1,2,4\\},\\{1,3,4\\},\\{2,3,4\\}\\}$ , and define $y=h_{*}\\dot{(z)}$ where ", "page_idx": 8}, {"type": "equation", "text": "$$\nh_{*}(z)=\\sum_{U\\in{\\mathcal{C}}}{\\widehat{h}}_{*}(U)\\chi_{U}(z),{\\mathrm{~where~}}{\\widehat{h}}_{*}(U)\\sim\\mathrm{Unif}([-2,2]){\\mathrm{~for~all~}}U\\in{\\mathcal{C}}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "We train with online 1-batch SGD ( $\\boldsymbol{\\ell}$ -bSGD) on a two-layer net with different loss functions (without any regularization) and stepsize $\\eta\\ \\propto\\ 1/d$ , where we consider ambiant dimensions $d\\in\\{100,300,500\\}$ . In Figure 1, we plot the test mean squared error versus $\\eta\\times\\mathrm{SGD}$ -iterations (thus also scaled with $1/d)$ , over 10 trials. Additionally, we also plot the continuous time (DF-PDE) in (dashed black line) that corresponds to the limit $d\\to\\infty$ . ", "page_idx": 8}, {"type": "image", "img_path": "Q0KwoyZlSo/tmp/4d504e02cbb30c81131519a5a3167f0a5f220e240f8ce96151639e024c70a4d5.jpg", "img_caption": ["Figure 1: The function $h_{*}(z)$ in (16) has $\\mathsf{L e a p}_{\\mathsf{C S Q}}\\,=\\,3$ but $\\mathsf{L e a p}_{\\mathsf{S Q}}\\,=\\,1$ . For the squared loss (left plot), (DF-PDE) remains stuck at initialization (no learning), and to escape the saddle, SGD requires a number of iterations that increases faster than $O(d)$ . For the absolute loss (center plot) or the other loss (right plot), we have $\\mathsf{L e a p}_{\\mathsf{D L Q}_{\\ell}}\\,=\\,\\mathsf{L e a p}_{\\mathsf{S Q}}\\,=\\,1$ , and the SGD dynamics learns in $\\Theta(d)$ steps and (DF-PDE) learns in $O(1)$ continuous time. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "8 Conclusion and Outlook ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we considered learning juntas over general product distributions with statistical query algorithms. To capture learning with gradient evaluations over a general loss and arbitrary model, we introduced Differentiable Learning Queries $(\\mathsf{D L Q}_{\\ell})$ , which can be seen as a generalization of correlation statistical queries beyond squared loss. We then showed that the complexity of learning juntas with either SQ, CSQ, or $\\mathsf{D}\\mathsf{L}\\mathsf{Q}_{\\ell}$ algorithms is sharply characterized in terms of a leap exponent (adaptive queries) or a cover exponent (non-adaptive queries). These exponents are defined in terms of a minimal combination of detectable sets to cover the support, where the system of detectable sets depends on the allowed queries. In general, the leap and cover exponents for different losses are not comparable. However, we identify \u201cgeneric\u201d losses, including $\\ell_{1}$ , where $\\mathsf{D}\\mathsf{L}\\mathsf{Q}_{\\ell}$ algorithms are as powerful as SQ algorithms for learning juntas. We further showed that ${\\sf D}{\\sf L}{\\sf Q}_{\\ell}$ can indeed capture the complexity of learning with SGD in the case of data on the hypercube. ", "page_idx": 9}, {"type": "text", "text": "Worst-case v.s. One-pass v.s. Multi-pass SGD. $\\mathsf{D}\\mathsf{L}\\mathsf{Q}_{\\ell}$ (like SQ) is defined in terms of worst-case noise. It is well understood that worst-case noise is theoretically very different from estimating population gradients based on samples (either independent samples as in one-pass (S)GD, or with repeated use of samples as in full-batch or multi-pass) when highly specialized models are allowed [Abbe et al., 2021b]. However, we expect $\\mathsf{D}\\mathsf{L}\\mathsf{Q}_{\\ell}$ to still capture the complexity of one-pass SGD in many settings of interest\u2014with \u201cregular\u201d models\u2014such as in Section 7. With sample reuse (e.g. multi-pass) the situation is more complex: Dandi et al. [2024] showed that two steps of full-batch gradient descent with square loss goes beyond $\\mathsf{C S Q}$ . Heuristically, and in light of our work, two steps on the same batch can be seen as a single gradient evaluation on a modified loss, thus going beyond $\\mathsf{C S Q}=\\mathsf{D L Q}_{\\ell_{s q}}$ , but remaining inside $\\mathsf{D L Q}_{\\widetilde{\\ell}}\\subseteq\\mathsf{S Q}$ for some perturbed non-quadratic loss $\\bar{\\tilde{\\ell}}$ . Indeed, we expect generally that multi-pass SGD on a \u201cregular\u201d model will remain in SQ. ", "page_idx": 9}, {"type": "text", "text": "Multi-index and beyond. In this paper, we focused on learning sparse functions. We hope the modular nature of our analysis framework (defining detectable sets in terms of test functions, and leap and cover complexities of set systems), our definition of DLQ, and the distinctions we emphasize between $\\mathsf{C S Q}$ , SQ and $\\mathsf{D}\\mathsf{L}\\mathsf{Q}$ and between adaptive and non-adaptive complexities, will be helpful in guiding and contextualizing analysis in other settings such as learning single-index or multi-index functions [e.g. Refinetti et al., 2021, Mousavi-Hosseini et al., 2022, Abbe and Boix-Adsera, 2022, Damian et al., 2022, Bietti et al., 2022, 2023, Damian et al., 2024a]. For example, the information exponent for single-index [Arous et al., 2021] can be seen as analogous to our $\\mathsf{C S Q}$ -cover exponent, the generative exponent for single-index [Damian et al., 2024b] as analogous to our SQ-cover exponent, and the isoLeap exponent for multi-index [Abbe et al., 2023, Dandi et al., 2023, Bietti et al., 2023] as analogous to our CSQ-leap exponent. It would be interesting to obtain a unified understanding of these specialized treatments and extend our general framework to multi-index models, and also to learning under other invariances beyond permutations and rotations. ", "page_idx": 9}, {"type": "text", "text": "In our setup, we emphasize generic input and output spaces, without a field structure. This emphasizes that when learning juntas, polynomials or degree of input or output coordinates is irrelevant. Defining multi-index models and introducing rotational invariance necessitates a field structure and gives rise to the relevance of polynomial degrees and decomposition. ", "page_idx": 9}, {"type": "text", "text": "An important point is that when considering permutation (as in juntas) vs. rotational (as in multi-index models) invariance, one must consider not only the invariance structure of the target function class, but also the input distribution (i.i.d. coordinates as in our case, or more generally exchangeable vs. spherical) and learning rule. E.g., learning a parity over input coordinates requires only $\\Theta(\\log d)$ samples, but a rotationally equivariant algorithm effectively learns parities also over rotated axis, which requires $\\Omega(d)$ samples [Glasgow, 2023] (and thus $\\dot{\\Omega}(d^{2})$ runtime). This also explains the need for $\\Theta(d)$ steps and thus $\\Theta(d^{2})$ runtime to learn leap-1 functions using SGD on a rotationally invariant neural net in Section 7. In order to break the $\\Omega(d)$ -sample lower bound, we need to break the rotation-equivariance, e.g. using sparse initialization and $\\ell_{1}$ regularization, which indeed can achieve $\\Theta(\\log(d))$ sample complexity. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement. This research was done as part of the NSF-Simons Sponsored Collaboration on the Theoretical Foundations of Deep Learning and the NSF Tripod Institute on Data, Econometrics, Algorithms, and Learning (IDEAL). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Emmanuel Abbe and Enric Boix-Adsera. On the non-universality of deep learning: quantifying the cost of symmetry. Advances in Neural Information Processing Systems, 35:17188\u201317201, 2022. ", "page_idx": 10}, {"type": "text", "text": "Emmanuel Abbe and Colin Sandon. On the universality of deep learning. Advances in Neural Information Processing Systems, 33:20061\u201320072, 2020.   \nEmmanuel Abbe, Enric Boix-Adsera, Matthew S Brennan, Guy Bresler, and Dheeraj Nagaraj. The staircase property: How hierarchical structure can guide deep learning. Advances in Neural Information Processing Systems, 34:26989\u201327002, 2021a.   \nEmmanuel Abbe, Pritish Kamath, Eran Malach, Colin Sandon, and Nathan Srebro. On the power of differentiable learning versus pac and sq learning. Advances in Neural Information Processing Systems, 34:24340\u201324351, 2021b.   \nEmmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks. In Conference on Learning Theory, pages 4782\u20134887. PMLR, 2022.   \nEmmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. Sgd learning on neural networks: leap complexity and saddle-to-saddle dynamics. In The Thirty Sixth Annual Conference on Learning Theory, pages 2552\u20132623. PMLR, 2023.   \nZeyuan Allen-Zhu and Yuanzhi Li. Backward feature correction: How deep learning performs deep learning. arXiv e-prints, pages arXiv\u20132001, 2020.   \nGerard Ben Arous, Reza Gheissari, and Aukosh Jagannath. Online stochastic gradient descent on non-convex losses from high-dimensional inference. Journal of Machine Learning Research, 22 (106):1\u201351, 2021.   \nAlberto Bietti, Joan Bruna, Clayton Sanford, and Min Jae Song. Learning single-index models with shallow neural networks. Advances in Neural Information Processing Systems, 35:9768\u20139783, 2022.   \nAlberto Bietti, Joan Bruna, and Loucas Pillaud-Vivien. On learning gaussian multi-index models with gradient flow. arXiv preprint arXiv:2310.19793, 2023.   \nAvrim L Blum and Pat Langley. Selection of relevant features and examples in machine learning. Artificial intelligence, 97(1-2):245\u2013271, 1997.   \nNader H Bshouty and Vitaly Feldman. On using extended statistical queries to avoid membership queries. Journal of Machine Learning Research, 2(Feb):359\u2013395, 2002.   \nS\u00e9bastien Bubeck et al. Convex optimization: Algorithms and complexity. Foundations and Trends\u00ae in Machine Learning, 8(3-4):231\u2013357, 2015.   \nMichael Celentano, Theodor Misiakiewicz, and Andrea Montanari. Minimum complexity interpolation in random features models. arXiv preprint arXiv:2103.15996, 2021.   \nLenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized models using optimal transport. Advances in neural information processing systems, 31, 2018.   \nGeorge Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals and systems, 2(4):303\u2013314, 1989.   \nAlex Damian, Eshaan Nichani, Rong Ge, and Jason D Lee. Smoothing the landscape boosts the signal for sgd: Optimal sample complexity for learning single index models. Advances in Neural Information Processing Systems, 36, 2024a.   \nAlex Damian, Loucas Pillaud-Vivien, Jason D Lee, and Joan Bruna. The computational complexity of learning gaussian single-index models. arXiv preprint arXiv:2403.05529, 2024b.   \nAlexandru Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with gradient descent. In Conference on Learning Theory, pages 5413\u20135452. PMLR, 2022. ", "page_idx": 10}, {"type": "text", "text": "Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan. Learning two-layer neural networks, one (giant) step at a time. arXiv preprint arXiv:2305.18270, 2023. ", "page_idx": 11}, {"type": "text", "text": "Yatin Dandi, Emanuele Troiani, Luca Arnaboldi, Luca Pesce, Lenka Zdeborov\u00e1, and Florent Krzakala. The beneftis of reusing batches for gradient descent in two-layer networks: Breaking the curse of information and leap exponents. arXiv preprint arXiv:2402.03220, 2024. ", "page_idx": 11}, {"type": "text", "text": "Richard M Dudley. Real analysis and probability. Chapman and Hall/CRC, 2018. ", "page_idx": 11}, {"type": "text", "text": "Benjamin Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang. Pareto frontiers in deep feature learning: Data, compute, width, and luck. Advances in Neural Information Processing Systems, 36, 2024.   \nVitaly Feldman, Elena Grigorescu, Lev Reyzin, Santosh S Vempala, and Ying Xiao. Statistical algorithms and a lower bound for detecting planted cliques. Journal of the ACM (JACM), 64(2): 1\u201337, 2017.   \nMargalit Glasgow. Sgd finds then tunes features in two-layer neural networks with near-optimal sample complexity: A case study in the xor problem. In The Twelfth International Conference on Learning Representations, 2023.   \nSamuel Hopkins. Statistical inference and the sum of squares method. PhD thesis, Cornell University, 2018.   \nKurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4(2): 251\u2013257, 1991.   \nMichael Kearns. Efficient noise-tolerant learning from statistical queries. Journal of the ACM (JACM), 45(6):983\u20131006, 1998.   \nYiwen Kou, Zixiang Chen, Quanquan Gu, and Sham M Kakade. Matching the statistical query lower bound for $\\mathbf{k}$ -sparse parity problems with stochastic gradient descent. arXiv preprint arXiv:2404.12376, 2024.   \nDmitriy Kunisky, Alexander S Wein, and Afonso S Bandeira. Notes on computational hardness of hypothesis testing: Predictions using the low-degree likelihood ratio. In ISAAC Congress (International Society for Analysis, its Applications and Computation), pages 1\u201350. Springer, 2019.   \nEran Malach, Pritish Kamath, Emmanuel Abbe, and Nathan Srebro. Quantifying the benefti of using differentiable learning over tangent kernels. In International Conference on Machine Learning, pages 7379\u20137389. PMLR, 2021.   \nSong Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of twolayer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665\u2013E7671, 2018.   \nAlireza Mousavi-Hosseini, Sejun Park, Manuela Girotti, Ioannis Mitliagkas, and Murat A Erdogdu. Neural networks efficiently learn low-dimensional representations with sgd. arXiv preprint arXiv:2209.14863, 2022.   \nDJ Newman and Morton Slater. Waring\u2019s problem for the ring of polynomials. Journal of Number Theory, 11(4):477\u2013487, 1979.   \nAmelia Perry, Alexander S Wein, Afonso S Bandeira, and Ankur Moitra. Optimality and suboptimality of pca i: Spiked random matrix models. The Annals of Statistics, 46(5):2416\u20132451, 2018.   \nMaria Refinetti, Sebastian Goldt, Florent Krzakala, and Lenka Zdeborov\u00e1. Classifying highdimensional gaussian mixtures: Where kernel methods fail and neural networks succeed. In International Conference on Machine Learning, pages 8936\u20138947. PMLR, 2021.   \nLev Reyzin. Statistical queries and statistical algorithms: Foundations and applications. arXiv preprint arXiv:2004.00557, 2020.   \nGrant M Rotskoff and Eric Vanden-Eijnden. Neural networks as interacting particle systems: Asymptotic convexity of the loss landscape and universal scaling of the approximation error. stat, 1050:22, 2018.   \nJustin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A central limit theorem. Stochastic Processes and their Applications, 130(3):1820\u20131852, 2020.   \nLe Song, Santosh Vempala, John Wilmes, and Bo Xie. On the complexity of learning neural networks. Advances in neural information processing systems, 30, 2017.   \nSho Sonoda and Noboru Murata. Neural network with unbounded activation functions is universal approximator. Applied and Computational Harmonic Analysis, 43(2):233\u2013268, 2017.   \nGregory Valiant. Finding correlations in subquadratic time, with applications to learning parities and juntas. In 2012 IEEE 53rd Annual Symposium on Foundations of Computer Science, pages 11\u201320. IEEE, 2012.   \nSantosh Vempala and John Wilmes. Gradient descent for one-hidden-layer neural networks: Polynomial convergence and sq lower bounds. In Conference on Learning Theory, pages 3115\u20133117. PMLR, 2019. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Additional discussions from the main text ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Necessity of Assumption 2.1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We further elaborate on the necessity of Assumption 2.1 in our setting. Informally, we require the label to have \u201cenough noise\u201d. This is necessary to obtain meaningful lower-bounds for general SQ algorithms, as noted in Valiant [2012], Song et al. [2017], Vempala and Wilmes [2019]. When learning real-valued target function classes, allowing for general measurable queries is too weak (note that arbitrary measurable functions are not practical anyway). Indeed, Vempala and Wilmes [2019] showed that any finite set $\\mathcal{H}$ of (noiseless) functions can be learned with $\\log\\left|\\mathcal{H}\\right|$ (bounded) queries and constant tolerance8. They identify three possible approaches to address this challenge: (i) Require $y\\mapsto\\phi(y,\\pmb{x})$ to be Lipschitz for every fixed $\\textbf{\\em x}$ ; (ii) Insist on noisy concepts, e.g., $\\bar{y}=f(\\pmb{x})+\\zeta$ , with $\\zeta\\sim\\mathsf{N}(0,\\sigma^{2})$ , which is equivalent to Lipschitz queries; (iii) Restrict the form of the queries, such as in CSQ. Our Assumption 2.1 can be viewed as a quantitative version of approach (ii). For approach (iii), if we restrict ourselves to CSQ or $\\mathsf{D}\\mathsf{L}\\mathsf{Q}_{\\ell}$ with $\\ell$ piecewise analytic, we can indeed remove Assumption 2.1 by modifying the proof of Theorem 5.1. ", "page_idx": 13}, {"type": "text", "text": "A.2 The conjectural picture of complexity of learning (\u2113-bSGD) ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "For $k_{*}:=\\mathsf{L e a p}_{\\mathsf{D L Q}_{\\ell}}(\\mu)$ , learning the junta problem with online SGD on loss $\\ell$ requires ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{k_{*}=1:}&{\\quad}&{n=\\Theta(d),\\qquad T_{\\mathsf{C}}=\\Theta(d^{2}),}\\\\ {k_{*}=2:}&{\\quad}&{n=\\Theta(d\\log d),\\qquad T_{\\mathsf{C}}=\\widetilde{\\Theta}(d^{2}),}\\\\ {k_{*}>2:}&{\\quad}&{n=\\widetilde{\\Theta}(d^{k_{*}-1}),\\qquad T_{\\mathsf{C}}=\\widetilde{\\Theta}(d^{k_{*}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Partial evidence was provided towards this conjecture for the square loss in Abbe et al. [2023], Bietti et al. [2023]: they showed that learning in this setting happens through a saddle-to-saddle dynamics, where each time the neural network needs to align to $k>1$ new coordinates, a saddle appears and SGD requires $\\widetilde{\\Theta}(d^{k-1})$ steps to escape. ", "page_idx": 13}, {"type": "image", "img_path": "Q0KwoyZlSo/tmp/229848b0f46ce88dab5e54f3c513656bd9f4c30789628f8c2f305d21f4239975.jpg", "img_caption": ["Figure 2: We repeat the same experiment, with the step size $\\eta\\propto1/d$ as in Figure 1 but now considering the order 3 parity function $h_{*}(z)\\stackrel{\\_}{=}z_{1}z_{2}z_{3}$ . Since $y=\\{-1,+1\\}$ , by Proposition 6.1, we have $\\mathsf{L e a p}_{\\mathsf{S Q}}=$ $\\mathsf{L e a p}_{\\mathsf{C S Q}}=\\mathsf{L e a p}_{\\mathsf{D L Q}_{\\ell}}=3$ for all the three loss functions considered. We observe that online SGD learns in $O(d^{2})$ iterations for all the losses. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "A.3 Identifiability and Relative Leap and Cover Complexities ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The Leap and Cover complexity may be infinite when $\\operatorname{supp}(\\mathcal{C}_{\\mathsf{A}})=\\cup_{U\\in\\mathcal{C}_{\\mathsf{A}}}U\\subsetneq[P]$ , in which case Theorem 5.1 (correctly) implies that we cannot recover the relevant coordinates that correspond to $[P]\\backslash\\operatorname{supp}(\\mathcal{C}_{\\mathsf{A}})$ using queries of type A (even with perfect precision). In this case, we can characterize the complexity of recovering $\\operatorname{supp}(\\mathcal{C}_{\\mathsf{A}})$ instead, and this is captured by the following \u201crelative\u201d complexities: ", "page_idx": 13}, {"type": "text", "text": "Definition 3. For any system of subsets ${\\mathcal{C}}\\subseteq2^{[P]}$ the relative leap and cover complexities are: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{r e l L e a p}(\\mathcal{C}):=\\displaystyle\\operatorname*{min}_{U_{1},\\ldots,U_{r}\\in\\mathcal{C}}\\quad\\operatorname*{max}_{i\\in[r]}\\,\\,|U_{i}\\setminus\\cup_{j=0}^{i-1}U_{j}|,}\\\\ &{\\underbrace{\\overset{r}{\\bigcup}_{i=1}^{r}U_{i}\\mathrm{=supp}(\\mathcal{C})}_{\\mathsf{r e l}\\mathsf{C o v e r}(\\mathcal{C}):=\\displaystyle\\operatorname*{max}_{i\\in\\mathsf{s u p p}(\\mathcal{C})}\\,\\,U\\in\\mathcal{C},i\\in U}\\,\\,|U|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A slight variant of Theorem 5.1 can then be shown, where relLea $\\rightthreetimes$ and relCoverA $(\\mu)$ for $\\mathsf{A}\\in$ $\\{\\mathsf{S Q},\\mathsf{C S Q},\\mathsf{D L Q}_{\\ell}\\}$ characterizes the complexity of recovering $\\operatorname{supp}(\\mathcal{C}_{\\mathsf{A}})$ . ", "page_idx": 14}, {"type": "text", "text": "For SQ, we may have $\\operatorname{supp}(\\mathcal{C}_{5\\mathbb{Q}})\\ \\subsetneq\\ [P]$ only when $y$ doesn\u2019t actually depend on some of the coordinates in $[P]$ (i.e. $y\\mid z=y|(z_{i})_{i\\in\\operatorname{supp}({\\mathcal{C}})})$ . In this case, once we recover $\\operatorname{supp}({\\mathcal{C}})$ (and possibly enumerating over permutations of its coordinates) we can also recover the conditional $y|x$ . That is, $\\mathsf{r e l L e a p}_{\\mathsf{S Q}}$ and relCoverSQ characterize the complexity of learning the distribution $\\mathcal{D}_{s_{*}}$ , even if not the (unidentifiable) set $S_{*}$ . ", "page_idx": 14}, {"type": "text", "text": "For ${\\sf D}{\\sf L}{\\sf Q}_{\\ell}$ , including $\\mathsf{C S Q}$ , we may not be able to identify some coordinates even if $y$ does depend on them. Consider for example the junta problem of size $P=2$ where $y|z=z_{1}\\!+\\!\\mathcal{N}(0,z_{2}^{2})$ . Although $y$ does depend on the second coordinate $z_{2}$ , it is not identifiable using $\\mathsf{C S Q}$ queries, $\\operatorname{supp}(\\mathcal{C}_{\\sf C S Q})=\\{1\\}$ , and using $\\mathsf{C S Q}$ queries we cannot recover the conditional distribution $\\mathcal{D}_{s_{*}}$ (even if we know the link $\\mu_{y|z})$ . What we can say for $\\mathsf{D}\\mathsf{L}\\mathsf{Q}_{\\ell}$ after recovering $\\hat{S}=\\mathrm{supp}(\\mathcal{C}_{\\sf D L Q_{\\ell}})$ , is that although we might not know $\\mathcal{D}_{s_{*}}$ , we can find the $\\ell$ -risk minimizer $f_{*}=\\arg\\operatorname*{min}_{f:\\mathcal{X}\\to\\mathcal{F}}\\mathbb{E}_{(y,\\mathbf{x})\\sim\\mathcal{D}_{s_{*}}}\\left[\\ell(f(x),y)\\right]$ . That is, relLea $\\mathsf{p}_{\\mathsf{D L Q}_{\\ell}}$ and relCoverDL $Q_{\\ell}$ characterize the complexity of $\\ell$ -risk minimization. ", "page_idx": 14}, {"type": "text", "text": "A.4 Property of SQ Leap and Cover Complexities ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We show a useful property of the $\\mathsf{S Q}$ -exponent. This characterization is similar to the definition of the generative exponent for single-index models on Gaussian data in [Damian et al., 2024b, Definition 2.4], while the detectable set Definition 2 is similar to their variational representation [Damian et al., 2024b, Proposition 2.6]. ", "page_idx": 14}, {"type": "text", "text": "Proposition A.1 (Property of SQ-detectable sets). $U\\in\\mathcal{C}_{\\mathsf{S Q}}$ if and only if there exists $T_{i}\\in L_{0}^{2}(\\mu_{x})$ for all $i\\in U$ such that $\\|\\xi_{U}\\|_{L^{2}(\\mu_{y})}>0$ where ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\xi_{U}(y)=\\mathbb{E}_{\\mu_{y,z}}\\Big[\\prod_{i\\in U}T_{i}(z_{i})\\Big|y\\Big].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof of Proposition A.1. In the forward direction, consider any $U\\in\\mathcal{C}_{\\mathsf{S Q}}$ , then by Definition 2, there exists $\\dot{T}_{i}\\in\\bar{L}_{0}^{2}(\\mu_{x})$ and $T\\in L^{2}(\\mu_{y})$ such that ", "page_idx": 14}, {"type": "equation", "text": "$$\n0\\neq\\mathbb{E}_{\\mu_{y,z}}\\left[T(y)\\prod_{i\\in U}T_{i}(z_{i})\\right]=\\mathbb{E}_{\\mu_{y}}\\left[T(y)\\xi_{U}(y)\\right],\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we defined $\\begin{array}{r}{\\xi_{U}(y):=\\mathbb{E}_{\\mu_{y,z}}\\,\\big[\\prod_{i\\in U}T_{i}(z_{i})\\mid y\\big].}\\end{array}$ . We then have ", "page_idx": 14}, {"type": "equation", "text": "$0<|\\mathbb{E}_{\\mu_{y}}\\left[T(y)\\xi_{U}(y)\\right]|\\leq\\|T\\|_{L^{2}(\\mu_{y})}\\|\\xi_{U}\\|_{L^{2}(\\mu_{y})}$ ", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, we conclude that $\\|\\xi_{U}\\|_{L^{2}(\\mu_{y})}>0$ as desired. In the opposite direction, let us consider $U\\subseteq[P]$ such that there exists $T_{i}\\in L_{0}^{2}(\\mu_{x})$ for every $i\\in U$ such that $\\|\\xi_{U}\\|_{L^{2}(\\mu_{y})}>0$ , where ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\xi_{U}(y)=\\mathbb{E}_{\\mu_{y,z}}\\left[\\prod_{i\\in U}T_{i}(z_{i})\\mid y\\right].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then let $T(y):=\\xi_{U}(y)$ . It is straightforward to verify that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Vert T\\Vert_{\\mu_{y}}^{2}=\\mathbb{E}_{\\mu_{y}}\\left[T(y)^{2}\\right]=\\mathbb{E}_{\\mu_{y}}\\left[\\mathbb{E}_{\\mu_{y},\\mathbf{z}}\\left[\\prod_{i\\in U}T_{i}(z_{i})\\mid y\\right]^{2}\\right]\\leq\\mathbb{E}_{\\mu_{y},\\mathbf{z}}\\left[\\prod_{i\\in U}T_{i}(z_{i})^{2}\\right]<\\infty.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We verified $T(y)\\in L^{2}(\\mu_{y})$ . Moreover, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbb{E}_{\\boldsymbol{\\mu}_{y}}\\!\\!\\!\\!}&{}&{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\mathbb{E}_{\\boldsymbol{\\mu}_{y}}\\left[T(\\boldsymbol{y})\\prod_{i\\in U}T_{i}(\\boldsymbol{z}_{i})\\right]=\\mathbb{E}_{\\boldsymbol{\\mu}_{y}}\\left[T(\\boldsymbol{y})\\mathbb{E}_{\\boldsymbol{\\mu}_{y},\\boldsymbol{z}}\\left[\\prod_{i\\in U}T_{i}(\\boldsymbol{z}_{i})\\mid\\boldsymbol{y}\\right]\\right]}\\\\ &{}&{=\\mathbb{E}_{\\boldsymbol{\\mu}_{y}}\\left[\\xi_{U}(\\boldsymbol{y})\\xi_{U}(\\boldsymbol{y})\\right]=\\|\\xi_{U}\\|_{L^{2}(\\boldsymbol{\\mu}_{y})}^{2}\\neq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, by Definition 2, we obtain $U\\in\\mathcal{C}_{\\mathsf{S Q}}$ . ", "page_idx": 15}, {"type": "text", "text": "B Proof of Theorem 5.1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Preliminaries ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Tensor basis of $L^{2}(\\mathcal{X}^{k},\\mu_{x}^{k})$ : Since we assumed that $(\\mathcal{X},\\mu_{x})$ is a Polish probability space, $L^{2}(\\mathcal{X},\\mu_{x})$ is separable [Dudley, 2018]. Consider $\\{\\psi_{i}\\}_{i\\in\\mathbb{Z}}$ , $\\mathcal{T}\\subseteq\\mathbb{N}$ , an orthonormal basis of $L^{2}(\\mathcal{X},\\mu_{x})$ such that $\\psi_{0}\\,=\\,1$ without loss of generality. In particular, the space $L^{2}(\\mathcal{X}^{k},\\mu_{x}^{k})$ admits the following tensor basis ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\{\\psi_{i}:=\\psi_{i_{1}}\\otimes\\psi_{i_{2}}\\otimes...\\otimes\\psi_{i_{k}}\\ :\\ i=(i_{1},...\\,,i_{k})\\in{\\mathbb{Z}}^{k}\\}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We will denote $\\operatorname{supp}(i)=\\{j\\in[d]:i_{j}>0\\}$ . ", "page_idx": 15}, {"type": "text", "text": "Distributions $\\nu_{S}^{\\sigma}$ : For any non-repeating sequence $\\sigma\\;\\in\\;{\\sf P}([d],P)$ of length $P$ , a subset $U\\subseteq$ $[P]$ , and a system of subsets $\\mathcal{C}\\,=\\,\\{U_{1},\\dots,U_{m}\\}\\,\\subseteq\\,2^{[P]}$ , we denote $\\sigma(U)\\;:=\\;\\{\\sigma(i)\\,:\\,i\\,\\in\\,U\\}$ and $\\sigma({\\mathcal{C}})\\;:=\\;\\{\\sigma(U_{1}),\\ldots,\\sigma(U_{m})\\}$ . The following analysis holds for fix $\\mu$ and hence we will often omit the subscript $\\mu$ by denoting Dd\u00b5,\u03c3 = D\u03c3d for any \u03c3 \u2208 P([d], P). For \u03c3 = Id, i.e. $\\sigma(1)=1,\\dots,\\sigma(P)=P$ , we will further omit writing Id and denote $\\mathcal{D}^{d}=\\mathcal{D}_{\\mathrm{Id}}^{d}$ . For $U\\subseteq[P]$ , $\\mu_{y,z,U}^{0}\\in\\mathcal{P}(\\mathcal{V}\\times\\mathcal{X}^{P})$ is the distribution of $(y_{U},z)$ corresponding to decoupling $y$ from $(z_{i})_{i\\notin U}$ in $\\mu_{y,z}$ following Eq. (6), i.e., $(y_{U},(z_{i})_{i\\in U})\\sim\\mu_{y,z,U}$ and $(z_{i})_{i\\notin U}\\sim\\mu_{x}^{P-|U|}$ \u00b5xP \u2212|U|independently. ", "page_idx": 15}, {"type": "text", "text": "For clarity, for every $U\\subseteq[P]$ and $\\sigma\\in\\mathsf{P}([d],P)$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nu_{U}^{\\sigma}:=[\\mathrm{Id}\\otimes\\sigma]_{\\#}[\\mu_{y,z,U}^{0}\\otimes\\mu_{x}^{d-P}],\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we abuse the notation and view $\\sigma$ as a permutation in $\\Pi_{d}$ , by extending the sequence $\\sigma\\in$ $\\mathsf{P}([d],P)$ to an ordering of irrelevant coordinates, as notice that it yields the same $\\nu_{U}^{\\sigma}$ . In words, $\\nu_{U}^{\\sigma}$ is the distribution of $(y,x)$ where we decoupled $y$ from $(x_{\\sigma(i)})_{i\\notin U}$ in $\\mathcal{D}_{\\sigma}^{d}$ . For $\\sigma=\\mathrm{Id}$ , we denote $\\nu_{U}=\\nu_{U}^{\\mathrm{Id}}$ . For $U=[P]$ , we simply denote $\\nu^{\\sigma}:=\\mathcal{D}_{\\sigma}^{d}$ . Note that $\\nu_{0}:=\\nu_{\\varnothing}=\\mu_{y}\\otimes\\mu_{x}^{d}$ and $\\nu_{U}^{\\sigma}=\\nu_{U}$ if $\\sigma(i)=i$ for all $i\\in U$ . ", "page_idx": 15}, {"type": "text", "text": "Basic properties under Assumption 2.1. Recall that our lower bounds in Theorem 5.1 are under Assumption 2.1. It states that the junta problem $\\boldsymbol{\\mu}\\,=\\,(\\mu_{x},\\mu_{y|z})$ is such that Eq. (7) holds. An immediate consequence of this assumption, using Jensen\u2019s inequality is that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mu_{y,z,U}^{0}}{\\mathrm{d}\\mu_{y}\\otimes\\mu_{x}^{P}}\\in L^{2}(\\mu_{y}\\otimes\\mu_{x}^{P}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Indeed, this follows from ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\|\\frac{\\mathrm{d}\\mu_{y,z,U}^{0}}{\\mathrm{d}\\mu_{y}\\otimes\\mu_{x}^{P}}\\right\\|_{L^{2}(\\mu_{y}\\otimes\\mu_{x}^{P})}^{2}\\le\\quad\\left\\|\\frac{\\mathrm{d}\\mu_{y,z}}{\\mathrm{d}\\mu_{y}\\otimes\\mu_{x}^{P}}\\right\\|_{L^{2}(\\mu_{y}\\otimes\\mu_{x}^{P})}^{2}<\\infty,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the first inequality follows from Jensen\u2019s inequality and the second by (7). ", "page_idx": 15}, {"type": "text", "text": "Another consequence of this assumption is that, for any $U\\subseteq\\;[P]$ and $\\sigma\\,\\in\\,{\\tt P}([d],P)$ , we have $L^{2}(\\nu_{0})\\subseteq L^{1}(\\dot{\\nu_{U}^{\\sigma}})$ . This is because for any $\\phi\\in L^{2}(\\nu_{0})$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\phi\\big\\|_{L^{1}(\\nu_{U}^{\\sigma})}=\\mathbb{E}_{(y,\\mathbf{x})\\sim\\nu_{U}^{\\sigma}}\\left[|\\phi(y,\\mathbf{x})|\\right]=\\mathbb{E}_{(y,\\mathbf{x})\\sim\\nu_{0}}\\left[\\frac{\\mathrm{d}\\nu_{U}^{\\sigma}}{\\mathrm{d}\\nu_{0}}(y,\\mathbf{x})\\cdot|\\phi(y,\\mathbf{x})|\\right]\\leq\\left\\|\\frac{\\mathrm{d}\\nu_{U}^{\\sigma}}{\\mathrm{d}\\nu_{0}}\\right\\|_{L^{2}(\\nu_{0})}\\|\\phi\\|_{L^{2}(\\nu_{0})}<\\phi.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we used Cauchy-Schwarz inequality and Eq. (21). ", "page_idx": 15}, {"type": "text", "text": "Algorithms in A: Throughout the proof, we consider $\\mathsf{A}\\in\\{\\mathsf{S Q},\\mathsf{C S Q},\\mathsf{D L Q}_{\\ell}\\}$ and $\\mathcal{C}_{\\mathsf{A}}:=\\mathcal{C}_{\\mathsf{A}}(\\mu)$ defined in Definition 2. First, observe that one can alternatively define $\\mathcal{Q}_{5\\mathrm{Q}}=L^{2}\\dot{(\\nu_{0})}$ because for any measurable $\\phi\\notin L^{2}(\\nu_{0})$ , the received query response can infinite by (8) and the learner does not gain any information. Therefore, it suffices to prove lower bound against the query set $\\mathcal{Q}_{5\\mathsf{Q}}=L^{2}\\bar{\\left(\\nu_{0}\\right)}$ . For the same reason, consider the allowed query sets, $\\mathcal{Q}_{\\sf C S Q}\\subseteq L^{\\breve{2}}(\\nu_{0})$ containing queries of the form $\\phi(y,\\pmb{x})=y\\tilde{\\phi}(\\pmb{x})$ , and $\\begin{array}{r l r}{\\lefteqn{\\mathcal{Q}_{\\mathrm{DLQ}_{\\ell}}\\subseteq L^{2}(\\nu_{0})}}\\end{array}$ with queries of the form ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\phi(y,\\pmb{x})=\\left[\\frac{\\mathrm{d}}{\\mathrm{d}\\omega}f(\\pmb{x},\\omega)\\right]_{\\omega=0}^{\\top}\\nabla\\ell(f(\\pmb{x},0),y).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We claim that, for $i\\in\\mathcal{T}^{d}$ and $\\psi_{i}$ defined in Eq. (19) and any $\\phi\\in\\mathcal{Q}_{\\mathsf{A}}$ , Definition 2 implies that if $\\operatorname{supp}(i)\\not\\in\\sigma(\\mathcal{C}_{\\mathsf{A}})$ , then almost surely over $\\tilde{\\pmb{x}}\\sim\\mu_{x}^{d}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}_{(y,\\pmb{x})\\sim\\nu^{\\sigma}}\\left[\\phi(y,\\pmb{\\tilde{x}})\\psi_{i}(\\pmb{x})\\right]=0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This follows from Definition 2 of $\\Psi$ -detectibility with Eq. (14), and using the fact that $\\psi_{i}$ is the product of $\\psi_{i_{j}}(x_{j})$ with $\\psi_{i_{j}}\\,\\in\\,L_{0}^{2}(\\mu_{x})$ for $j\\,\\in\\,\\mathrm{supp}(i)$ . More specifically, since $\\phi\\in L^{2}(\\nu_{0})$ , the function $\\phi(\\cdot,\\tilde{{\\pmb x}})\\in L^{2}(\\mu_{y})$ almost surely over $\\tilde{\\pmb{x}}\\sim\\mu_{x}^{d}$ , and In particular, almost surely over $\\tilde{\\pmb{x}}\\sim\\mu_{x}^{d}$ : ", "page_idx": 16}, {"type": "text", "text": "For any $\\phi\\in\\mathcal{Q}_{\\mathsf{S Q}}$ , we have $\\phi(\\cdot,\\tilde{{\\pmb x}})\\in L^{2}(\\mu_{y})=\\Psi_{\\mathsf{S Q}},$ For any $\\phi\\in\\mathcal{Q}_{\\mathsf{C S Q}}$ , we have $\\phi(\\cdot,\\tilde{\\pmb{x}})=y\\tilde{\\phi}(\\tilde{\\pmb{x}})$ (i.e. a scaled identity), For any \u03d5 \u2208QDLQ\u2113, we have $\\phi(\\cdot,\\tilde{{\\pmb x}})\\in\\Psi_{\\mathsf{D L Q}_{\\ell}}$ (i.e. $\\phi(\\cdot,\\tilde{{\\pmb x}})$ of the form $\\boldsymbol{v}^{\\top}\\nabla\\ell({\\boldsymbol{u}},{\\boldsymbol{y}}))$ . ", "page_idx": 16}, {"type": "text", "text": "B.2 Theorem 5.1.(a): lower bound for adaptive queries ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Definition of set $\\boldsymbol{S}_{*}$ : Let $k_{*}=\\mathsf{L e a p}(\\mathcal{C}_{\\mathsf{A}})$ . Define $\\mathcal{C}_{*}\\subset\\mathcal{C}_{\\mathsf{A}}$ to be the maximal subset of $\\mathcal{C}_{\\sf A}$ such that $\\mathsf{L e a p}(\\mathcal{C}_{*})\\,\\leq\\,k_{*}\\,-\\,1$ , and $\\begin{array}{r}{S_{*}=\\cup_{U\\in\\mathcal{C}_{*}}U}\\end{array}$ . In words, $\\boldsymbol{S}_{*}$ is the collection of all coordinates in the support that can be reached by adding at most $k_{*}-1$ coordinates at a time. In particular, by definition of $\\mathcal{C}_{*}$ , we have $|U\\setminus S_{*}|\\ge k_{*}$ for every $U\\,\\in\\,{\\mathcal{C}}_{\\mathsf{A}}\\setminus{\\mathcal{C}}_{*}$ . Denote $r\\,=\\,|S_{*}|$ and without loss of generality, assume that $S_{*}=\\{1,\\ldots,r\\}$ . We decompose the covariate into $\\pmb{x}=(\\pmb{x}_{S_{\\ast}},\\pmb{x}_{S_{\\ast}^{c}})$ where $S_{*}^{c}\\,=\\,[d]\\,\\backslash\\,S_{*}$ . Using these notations, $(y_{S_{*}}^{\\sigma},x)\\,\\sim\\,\\nu_{S_{*}}^{\\sigma}$ has $(y_{S_{*}}^{\\sigma},x_{\\sigma(S_{*})})\\,\\sim\\,\\mathcal{D}_{y,z,S_{*}}$ and $\\mathbf{\\Delta}\\pmb{x}_{\\sigma(S_{*}^{c})}\\sim\\mu_{x}^{d-r}$ independently. ", "page_idx": 16}, {"type": "text", "text": "Definition of $\\mathcal{H}_{\\mu,S_{*}}^{d}$ : We introduce $\\mathsf{P}([d],P,S_{*})\\subseteq\\mathsf{P}([d],P)$ the subset of non-repeating sequence of length $[P]$ from elements $[d]$ such that $\\sigma(i)=i$ for all $i\\in S_{*}$ , and $\\mathcal{H}_{\\mu,S_{*}}^{d}\\subseteq\\mathcal{H}_{\\mu}^{d}$ the subset of hypotheses $\\mathcal{D}_{\\sigma}^{d}$ with $\\sigma\\in\\mathsf{P}([d],P,S_{*})$ . Recall that for all $\\sigma\\in\\mathsf{P}([d],P,S_{*})$ , we have $\\nu_{S_{*}}^{\\sigma}=\\nu_{S_{*}}$ . To prove our lower bound, we will lower bound the complexity of learning $\\mathcal{H}_{\\mu,S_{*}}^{d}$ which implies a lower bound on $\\mathcal{H}_{\\mu}^{d}$ . Specifically, we will show that with $q/\\tau^{2}\\leq c d^{k*}$ for some constant $c>0$ that only depends on $\\bar{P},\\mu$ (and the loss $\\ell$ for ${\\sf D L Q}_{\\ell},$ ), statistical query algorithms in $\\mathsf{A}$ cannot distinguish for all $\\sigma\\in\\mathsf{P}([d],P,S_{*})$ between $\\nu^{\\sigma}=\\mathcal{D}_{\\sigma}^{d}$ and $\\nu_{S_{*}}$ . ", "page_idx": 16}, {"type": "text", "text": "Proof outline: For any $\\sigma\\in\\mathsf{P}([d],P,S_{*})$ and query $\\phi\\in\\mathcal{Q}_{\\mathsf{A}}\\subseteq L^{2}(\\nu_{0})$ , define ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Delta_{\\sigma}(\\phi):=\\mathbb{E}_{\\nu^{\\sigma}}[\\phi]-\\mathbb{E}_{\\nu_{S_{\\ast}}}[\\phi].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We will show below that there exists a constant $C>0$ that only depends on $\\mu$ such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\phi\\in\\mathcal{Q}_{\\mathsf{A}}}\\frac{\\mathbb{E}_{\\sigma}[\\Delta_{\\sigma}(\\phi)^{2}]}{\\|\\phi\\|_{L^{2}(\\nu_{0})}^{2}}\\leq C d^{-k_{*}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\mathbb{E}_{\\sigma}$ denote the expectation with respect to $\\sigma\\,\\sim\\,{\\mathrm{Unif}}\\left({\\mathsf{P}}([d],P,S_{*})\\right)$ . The intuition behind this crucial bound in (25) is that we defined the null distribution as the distribution with matching marginals $(y,x)$ where the label only depends on the support of $\\textbf{\\em x}$ discoverable with \u201cleap\" $k_{*}$ . Hence, to make any progress, any new detectable set added to the support needs to contain at least $k_{*}$ new coordinates. In the second moment, to have a non-zero contribution, we need the permutation to map the support to a detectable subset, i.e. the permutation to map to a subset of size at least $k_{*}$ , which has probability $O(d^{-k_{*}})$ over permutation. ", "page_idx": 16}, {"type": "text", "text": "Let $\\phi_{1},...,\\phi_{q}$ be the sequence of adaptive queries with responses $\\mathbb{E}_{\\nu_{S*}}[\\phi_{t}]$ . Then, by Markov\u2019s inequality, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\sigma}\\left(\\exists t\\in[q],\\ |\\Delta_{\\sigma}(\\phi_{t})|>\\tau\\|\\phi\\|_{L^{2}(\\nu_{0})}\\right)\\le\\frac{q}{\\tau^{2}}\\operatorname*{sup}_{\\phi\\in\\mathcal{Q}_{\\mathrm{A}}}\\frac{\\mathbb{E}_{\\sigma}[\\Delta_{\\sigma}(\\phi)^{2}]}{\\|\\phi\\|_{L^{2}(\\nu_{0})}^{2}}\\le C\\frac{q d^{-k_{*}}}{\\tau^{2}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Hence, for $q/\\tau^{2}\\;<\\;d^{k_{*}}/C$ , there exists $\\sigma\\,\\in\\,\\mathsf{P}([d],P,S_{*})$ such that for every $t\\ \\in\\ [q]$ , we have $|\\mathbb{E}_{\\nu^{\\sigma}}[\\phi_{t}]-\\mathbb{E}_{\\nu_{S_{*}}}[\\phi_{t}]|\\,\\le\\,\\tau\\|\\phi_{t}\\|_{L^{2}(\\nu_{0})}$ . Therefore Using Eq (26), we obtain that that there are two members from $\\#_{\\mu}^{d}$ whose query responses are compatible with the null $\\nu_{S_{*}}$ , and hence also mutually compatible and indistinguishable from each other. This then establishes the failure of the learning algorithm in recovering the correct support. ", "page_idx": 17}, {"type": "text", "text": "Decomposing $\\Delta_{\\sigma}(\\phi)$ : We rewrite $\\Delta_{\\sigma}(\\phi)$ in terms of $\\mathbb{E}_{\\nu_{S_{*}}}$ using the Radon-Nikodym derivative: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Delta_{\\sigma}(\\phi)=\\mathbb{E}_{\\nu_{S_{*}}}\\left[\\frac{\\mathrm{d}\\nu^{\\sigma}}{\\mathrm{d}\\nu_{S_{*}}}\\phi\\right]-\\mathbb{E}_{\\nu_{S_{*}}}[\\phi]=\\mathbb{E}_{\\nu_{S_{*}}}\\left[\\left(\\frac{\\mathrm{d}\\nu^{\\sigma}}{\\mathrm{d}\\nu_{S_{*}}}(y_{S_{*}},\\pmb{x})-1\\right)\\phi(y_{S_{*}},\\pmb{x})\\right].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Recall that $(y_{S_{*}},x_{S_{*}})$ is independent of $\\mathbf{\\boldsymbol{x}}_{\\sigma(S_{*}^{c})}\\sim\\mu_{x}^{d-r}$ under $\\nu_{S_{*}}$ , and that $\\mathrm{d}\\nu^{\\sigma}/\\mathrm{d}\\nu_{S_{*}}\\in L^{2}(\\nu_{S_{*}})$ by Assumption 2.1. Therefore, we have the following orthogonal decomposition in $L^{2}(\\nu_{S_{*}})$ using the tensor basis (19): ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\nu^{\\sigma}}{\\mathrm{d}\\nu_{S_{*}}}(y_{S_{*}},\\pmb{x})-1=\\sum_{i\\in\\mathbb{Z}^{d-r}\\backslash\\{\\mathbf{0}\\}}\\xi_{i}^{\\sigma}(y_{S_{*}},x_{S_{*}})\\psi_{i}(x_{S_{*}^{c}}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\xi_{i}^{\\sigma}(y_{\\mathcal{S}_{\\ast}},x_{\\mathcal{S}_{\\ast}}):=\\mathbb{E}_{\\nu_{S_{\\ast}}}\\left[\\frac{\\mathrm{d}\\nu^{\\sigma}}{\\mathrm{d}\\nu_{S_{\\ast}}}(y_{S_{\\ast}},x)\\psi_{i}(x_{S_{\\ast}^{c}})\\Big\\vert y_{\\mathcal{S}_{\\ast}},x_{\\mathcal{S}_{\\ast}}\\right]=\\mathbb{E}_{\\nu^{\\sigma}}\\left[\\psi_{i}(x_{S_{\\ast}^{c}})\\Big\\vert y_{S_{\\ast}},x_{S_{\\ast}}\\right].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Similarly, we decompose $\\phi$ orthogonally in $L^{2}(\\nu_{S_{*}})$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\phi(y s_{\\ast},\\pmb{x})=\\sum_{i\\in\\mathbb{Z}^{d-r}}\\alpha_{i}(y s_{\\ast},\\pmb{x}s_{\\ast})\\psi_{i}(\\pmb{x}_{S_{\\ast}^{c}}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\alpha_{i}(y_{S_{\\ast}},x_{S_{\\ast}}):=\\mathbb{E}_{\\nu_{S_{\\ast}}}\\left[\\phi(y_{S_{\\ast}},\\pmb{x})\\psi_{i}(\\pmb{x}_{S_{\\ast}^{c}})\\middle|y_{S_{\\ast}},\\pmb{x}_{S_{\\ast}}\\right].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We deduce that we can decompose $\\Delta_{\\sigma}(\\phi)$ as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Delta_{\\sigma}(\\phi)=\\sum_{i\\in\\mathbb{Z}^{d-r}\\backslash\\{\\mathbf{0}\\}}\\mathbb{E}_{\\nu_{S_{\\ast}}}\\left[\\xi_{i}^{\\sigma}(y_{S_{\\ast}},x_{S_{\\ast}})\\alpha_{i}(y_{S_{\\ast}},x_{S_{\\ast}})\\right].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For convenience, denote $m_{i}^{\\sigma}$ the summand. From the expressions (29) and (31) and by Fubini\u2019s theorem, we can rewrite $m_{i}^{\\sigma}$ as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{m_{i}^{\\sigma}=\\mathbb{E}_{y_{S_{*}},x_{S_{*}}}\\left[\\mathbb{E}_{\\nu^{\\sigma}}\\left[\\psi_{i}({x_{S_{*}^{c}}})|y_{S_{*}},{x_{S_{*}}}\\right]\\cdot\\mathbb{E}_{\\tilde{x}_{S_{*}^{c}}\\sim\\mu_{x}^{P-r}}\\left[\\phi(y_{S_{*}},{x_{S_{*}}},\\tilde{x}_{S_{*}})\\psi_{i}(\\tilde{x}_{S_{*}^{c}})\\right]\\right]}\\\\ &{\\quad\\quad=\\mathbb{E}_{(y,x,\\tilde{x}_{S_{*}^{c}})\\sim\\nu^{\\sigma}\\otimes\\mu_{x}^{P-r}}\\left[\\psi_{i}({x_{S_{*}^{c}}})\\phi(y,{x_{S_{*}}},\\tilde{x}_{S_{*}^{c}})\\psi_{i}(\\tilde{x}_{S_{*}^{c}})\\right]}\\\\ &{\\quad\\quad=\\mathbb{E}_{\\tilde{x}_{S_{*}^{c}}\\sim\\mu_{x}^{P-r}}\\left[\\psi_{i}(\\tilde{x}_{S_{*}^{c}})\\mathbb{E}_{(y,x)\\sim\\nu^{\\sigma}}\\left[\\phi(y,{x_{S_{*}}},\\tilde{x}_{S_{*}^{c}})\\psi_{i}(x_{S_{*}^{c}})\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let us decouple $y$ from $\\pmb{x}_{S_{*}}$ inside the query $\\phi$ using the Radon-Nikodym derivative: conditional on $\\tilde{\\pmb{x}}_{S_{\\ast}^{c}}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{(y,x)\\sim\\nu^{\\sigma}}\\left[\\phi(y,x_{S_{\\ast}},\\tilde{x}_{S_{\\ast}^{c}})\\psi_{i}(x_{S_{\\ast}^{c}})\\right]}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{(y_{S_{\\ast}^{c}},x)\\sim\\nu_{S_{\\ast}^{c}}^{\\sigma}}\\left[\\displaystyle\\frac{\\mathrm{d}\\nu^{\\sigma}}{\\mathrm{d}\\nu_{S_{\\ast}^{c}}^{\\sigma}}(y_{S_{\\ast}^{c}},x)\\cdot\\phi(y_{S_{\\ast}^{c}},x_{S_{\\ast}},\\tilde{x}_{S_{\\ast}^{c}})\\psi_{i}(x_{S_{\\ast}^{c}})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Similarly to Eq. (28), we have the following orthogonal decomposition in $L^{2}(\\nu_{S_{\\ast}^{c}}^{\\sigma})$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\nu^{\\sigma}}{\\mathrm{d}\\nu_{S_{*}^{c}}^{\\sigma}}(y_{S_{*}^{c}},\\pmb{x})=\\sum_{j\\in\\mathbb{Z}^{r}}\\xi_{j}^{\\sigma}(y_{S_{*}^{c}},\\pmb{x}_{S_{*}^{c}})\\psi_{j}(\\pmb{x}_{S_{*}}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\xi_{j}^{\\sigma}(y_{S_{*}^{c}},x_{S_{*}^{c}}):=\\mathbb{E}_{\\boldsymbol{\\nu}^{\\sigma}}\\left[\\psi_{j}(\\mathbf{\\boldsymbol{x}}_{S_{*}})\\middle|y_{S_{*}^{c}},x_{S_{*}^{c}}\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By Fubini\u2019s theorem, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{(y_{S_{*}^{c}},\\tilde{x}_{S_{*}},x_{S_{*}^{c}})\\sim\\nu_{S_{*}^{c}}^{\\sigma}}\\left[\\xi_{j}^{\\sigma}\\big(y_{S_{*}^{c}},x_{S_{*}^{c}}\\big)\\psi_{j}\\big(\\tilde{x}_{S_{*}}\\big)\\cdot\\phi\\big(y_{S_{*}^{c}},\\tilde{x}_{S_{*}},\\tilde{x}_{S_{*}^{c}}\\big)\\psi_{i}\\big(x_{S_{*}^{c}}\\big)\\right]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n=\\mathbb{E}_{{\\tilde{x}}_{S_{\\ast}}\\sim\\mu_{x}^{r}}\\left[\\psi_{j}({\\tilde{x}}_{S_{\\ast}})\\mathbb{E}_{(y,x)\\sim\\nu^{\\sigma}}\\left[\\phi(y,{\\tilde{x}}_{S_{\\ast}},{\\tilde{x}}_{S_{\\ast}^{c}})\\psi_{j}(x_{S_{\\ast}})\\psi_{i}(x_{S_{\\ast}^{c}})\\right]\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Combining Eqs. (34), (35), and (37) into Eq. (33) yields the expression ", "page_idx": 18}, {"type": "equation", "text": "$$\nm_{i}^{\\sigma}=\\sum_{j\\in\\mathbb{Z}^{r}}\\mathbb{E}_{\\tilde{x}\\sim\\mu_{x}^{d}}\\left[\\psi_{j}(\\tilde{x}_{S_{*}})\\psi_{i}(\\tilde{x}_{S_{*}^{c}})\\mathbb{E}_{(y,x)\\sim\\nu^{\\sigma}}\\left[\\phi(y,\\tilde{x})\\psi_{j}(x_{S_{*}})\\psi_{i}(x_{S_{*}^{c}})\\right]\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "From Eq. (32), we deduce the following simple decomposition ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Delta_{\\sigma}(\\phi)=\\sum_{i\\in{\\cal Z}^{d},\\mathrm{\\scriptsize~supp}(i)\\subseteq{\\cal S}_{*}}\\mathbb{E}_{\\tilde{x}\\sim\\mu_{x}^{d}}\\left[\\psi_{i}(\\tilde{x})\\mathbb{E}_{(y,x)\\sim\\nu^{\\sigma}}\\left[\\phi(y,\\tilde{x})\\psi_{i}(x)\\right]\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Bounding $\\mathbb{E}_{\\sigma}[\\Delta_{\\sigma}(\\phi)^{2}]$ : If $\\operatorname{supp}(i)\\not\\in\\sigma(\\mathcal{C}_{\\mathsf{A}}\\setminus\\mathcal{C}_{*})$ , we have by Eq. (23) that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{{\\tilde{\\mathbf{x}}}\\sim\\mu_{x}^{d}}\\left[\\psi_{i}({\\tilde{\\mathbf{x}}})\\mathbb{E}_{(y,\\mathbf{x})\\sim\\nu^{\\sigma}}\\left[\\phi(y,{\\tilde{\\mathbf{x}}})\\psi_{i}(\\mathbf{x})\\right]\\right]=0.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore we can rewrite the sum in Eq. (39) with ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\Delta_{\\sigma}(\\phi)=\\displaystyle\\sum_{i\\in\\mathbb{Z}^{d}}\\,\\displaystyle\\sum_{S\\in\\mathcal{C}_{\\mathrm{A}}\\backslash\\mathcal{C}_{*}}\\mathbb{E}_{(y,x,\\tilde{x})\\sim\\nu^{\\sigma}\\otimes\\mu_{x}^{d}}\\big[\\psi_{i}(\\tilde{x})\\phi(y,\\tilde{x})\\psi_{i}(x)\\big]\\cdot\\mathbb{1}[\\mathrm{supp}(i)=\\sigma(S)]}\\\\ &{}&{\\quad=\\displaystyle\\sum_{i\\in\\mathbb{Z}^{d}}\\,\\displaystyle\\sum_{S\\in\\mathcal{C}_{\\mathrm{A}}\\backslash\\mathcal{C}_{*}}\\mathbb{E}_{(y,x,\\tilde{x})\\sim\\nu\\otimes\\mu_{x}^{d}}\\big[\\psi_{i}(\\tilde{x})\\phi(y,\\tilde{x})\\psi_{i}(x)\\big]\\cdot\\mathbb{1}[\\sigma(\\mathrm{supp}(i))=S].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Recall that for every $S\\in{\\mathcal{C}}_{\\mathsf{A}}\\setminus{\\mathcal{C}}_{*}$ by construction. Hence, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\sigma}\\left[\\sum_{S_{1},S_{2}\\in\\mathcal{C}_{\\mathsf{A}}\\backslash\\mathcal{C}_{*}}\\mathbb{1}[\\sigma(\\operatorname{supp}(i))=S_{1},\\sigma(\\operatorname{supp}(j))=S_{2}]\\right]\\leq C d^{-k_{*}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By squaring Eq. (41) and the previous display, we deduce that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq C d^{-k}\\cdot\\Bigg(\\displaystyle\\sum_{i\\in\\mathcal{I}_{k}^{n}}\\Big|\\sum_{\\{i,\\ell\\},\\lambda_{i}\\sim\\nu_{i}\\approx\\mathcal{I}_{k}^{n}\\Big\\}\\Big(\\frac{\\Delta_{\\ell}}{\\Delta_{\\ell}}(\\hat{y}_{1}(\\hat{y}_{1})\\hat{y}_{1})(x_{1}(x))\\Big|\\Big)^{2}}\\\\ &{\\leq C d^{-k}\\cdot\\left(\\displaystyle\\sum_{i\\in\\mathcal{I}_{k}^{n}}\\Big|\\sum_{\\{i,\\ell\\},\\lambda_{i}\\sim\\nu_{i}\\in\\mathcal{I}_{k}^{n}\\Big\\}\\left[\\underline{{\\hat{\\Delta}_{\\ell}}}_{(\\hat{y}_{1})}(y_{1},x)|\\hat{v}_{1}(x)\\right]\\Big|\\sum_{\\ell\\in\\mathcal{I}_{k}^{n}}\\Big|\\hat{y}_{1}(y_{1},\\hat{x})|\\hat{v}_{1}(\\hat{x})\\Big|\\right]\\Bigg)^{2}}\\\\ &{\\leq C d^{-k}\\cdot\\left(\\displaystyle\\sum_{i\\in\\mathcal{I}_{k}^{n}}\\mathbb{E}_{\\nu_{i}^{n}}\\left[\\mathbb{E}_{\\nu_{i}^{n}}\\left[\\frac{\\Delta_{\\ell}}{\\Delta_{\\ell}}(y_{1},x)|\\hat{v}_{1}(x)\\right]\\right]^{2}\\right]^{1/2}\\mathbb{E}_{\\nu_{i}\\sim\\nu_{i}\\in\\mathcal{I}_{k}^{n}\\Big\\}\\big|\\hat{y}_{1}(y_{1},\\hat{x})|\\hat{v}_{1}(\\hat{x})\\big|\\hat{y}_{1}^{2}\\Bigg)^{1/2}\\Bigg)^{2}}\\\\ &{\\leq C d^{-k}\\cdot\\left(\\displaystyle\\sum_{i\\in\\mathcal{I}_{k}^{n}}\\mathbb{E}_{\\nu_{i}^{n}}\\left[\\mathbb{E}_{\\nu_{i}^{n}}\\left[\\frac{\\Delta_{\\ell}}{\\Delta_{\\ell}}(y_{1},x)|\\hat{v}_{1}(x)\\right]^{2}\\right]\\right)\\left(\\displaystyle\\sum_{i\\in\\mathcal{I}_{k}^{n}}\\mathbb{E}_{\\nu_{i}\\sim\\nu_{i}\\in\\mathcal{I}_{k}^{n}\\Big\\}\\big(\\hat{y}_{1}(y_{1},\\hat{x})|\\hat{v}_{1}(x)\\big|^{2\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we used in the third and fourth line Cauchy-Schwarz inequality, and in the last line ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\nu}{\\mathrm{d}\\nu_{0}}=\\frac{\\mathrm{d}\\mu_{y,z}}{\\mathrm{d}\\mu_{y}\\otimes\\mu_{x}^{P}}\\in L^{2}(\\nu_{0}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "by Assumption 2.1. Note that the above bound is uniform over all $\\phi\\in L^{2}(\\nu_{0})$ . This concludes the proof. ", "page_idx": 18}, {"type": "text", "text": "Remark B.1. We remark that indeed these proofs, after alternatively defining $k_{*}=\\mathsf{r e l L e a p}(\\mathcal{C}_{\\mathsf{A}})$ and $\\mathcal{C}_{*}$ to be the maximal subset of $\\mathcal{C}_{\\sf A}$ such that relLeap $\\iota(\\mathcal{C}_{*})=k_{*}-1$ , shows that if $q/\\tau^{2}\\leq c d^{k_{*}}$ , then mthods in A fail to recover coordinates from $\\mathcal{C}_{\\sf A}\\setminus\\mathcal{C}_{*}$ . ", "page_idx": 19}, {"type": "text", "text": "B.3 Theorem 5.1.(b): lower bound for non-adaptive queries ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The proof follows from a similar argument to the adaptive case. ", "page_idx": 19}, {"type": "text", "text": "Proof outline: Let $k_{*}=\\mathsf{C o v e r}(\\mathcal{C}_{\\mathsf{A}})$ and $l_{*}$ that maximizes $\\mathrm{min}_{S\\in{\\cal c}_{\\mathsf{A}},l\\in S}\\left|S\\right|$ . By definition of the cover complexity, $|S|\\geq k_{*}$ for all $S\\in{\\mathcal{C}}_{\\mathsf{A}}$ with $l_{*}\\in S$ . Let $S_{*}:=[P]\\setminus\\{l_{*}\\}$ . We define in the non-adaptive case for all $\\phi\\in\\mathcal{Q}_{\\mathsf{A}}$ and $\\sigma\\in\\mathsf{P}([d],P)$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Delta_{\\sigma}(\\phi)=\\mathbb{E}_{\\nu^{\\sigma}}[\\phi]-\\mathbb{E}_{\\nu_{S_{*}}^{\\sigma}}[\\phi],\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "meaning that we compare the query expectation between $\\nu^{\\sigma}$ and $\\nu_{S_{*}}^{\\sigma}$ where we decoupled the label $y$ from coordinate $\\sigma(l_{*})$ . We show again that the bound (25) holds on the second moment of $\\Delta_{\\sigma}\\bar{(\\phi)}/\\|\\phi\\|_{L^{2}(\\nu_{0})}$ . Therefore, for any set of $q$ queries $\\{\\phi_{t}\\}_{t\\in[q]}\\,\\subset\\,\\mathcal{Q}_{\\mathsf{A}}$ , we get a similar bound (26) on the probability of all queries being less or equal to $\\tau$ in absolute value. We deduce that for $q/\\tau^{2}\\leq c d^{k_{*}}$ , the algorithm will not be able to distinguish for all $\\sigma\\in{\\sf P}([d],P)$ between $\\nu^{\\sigma}$ and $\\nu_{S_{*}}^{\\sigma}$ , and therefore recover the coordinate $\\sigma(l_{*})$ in the support. ", "page_idx": 19}, {"type": "text", "text": "Bounding $\\mathbb{E}_{\\sigma}[\\Delta_{\\sigma}(\\phi)^{2}]$ : Following the same decomposition of $\\Delta_{\\sigma}(\\phi)$ as above, we first obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Delta_{\\sigma}(\\phi)=\\sum_{i\\in\\mathbb{Z}\\backslash\\{0\\}}\\mathbb{E}_{\\nu_{S_{*}}^{\\sigma}}\\left[\\xi_{i}^{\\sigma}(y_{S_{*}}^{\\sigma},{x_{\\sigma(S_{*})}})\\alpha_{i}(y_{S_{*}}^{\\sigma},{x_{\\sigma(S_{*})}})\\right],\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and then ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Delta_{\\sigma}(\\phi)=\\sum_{\\substack{i\\in{\\cal Z}^{d},\\,\\sigma(l_{*})\\in\\mathrm{supp}(i)\\mathbb{Z}\\sigma(S_{*})}}\\mathbb{E}_{\\widetilde{x}\\sim\\mu_{x}^{d}}\\left[\\psi_{i}(\\widetilde{x})\\mathbb{E}_{(y,x)\\sim\\nu^{\\sigma}}\\left[\\phi(y,\\widetilde{x})\\psi_{i}(x)\\right]\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, by Eq. (23), the summand is non zero only if $\\mathrm{supp}(i)\\subseteq\\sigma(\\mathcal{C}_{\\mathsf{A}})$ with $\\sigma(l_{*})\\in\\mathrm{supp}(i)$ . Recall that in this case $|\\mathrm{supp}(i)|\\geq k_{*}$ by the choice of $l_{*}$ . The rest of the proof follows using the same computation as in the previous section. ", "page_idx": 19}, {"type": "text", "text": "B.4 Upper Bounds for Adaptive and Non-Adaptive Queries ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Define ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\beta=\\operatorname*{min}_{U\\in\\mathcal{C}_{\\mathsf{A}}}\\left|\\mathbb{E}_{\\mu_{y,z}}\\left[T^{U}(y)\\prod_{i\\in U}T_{i}^{U}(z_{i})\\right]\\right|,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with $T^{U}\\in\\Psi_{\\mathsf{A}}$ and $T_{i}^{U}\\in L_{0}^{2}(\\mu_{x})$ chosen to make the expectation non-zero, where without loss of generality we normalize them so that $\\begin{array}{r}{\\|T^{U}\\|_{L^{2}(\\mu_{y})}^{2}\\prod_{i\\in S}\\|T_{i}^{U}\\|_{L^{2}(\\mu_{x})}=1}\\end{array}$ . We can then emulate the support recovery algorithm described below Definition 1 in Section 4 using for every $U_{*}\\in{\\mathcal{C}}_{\\mathsf{A}}$ and $\\begin{array}{r}{|S|=|U_{*}|,\\phi_{S}(y,\\pmb{x})=T^{U_{*}}(y)\\prod_{i\\in S}T_{i}^{U_{*}}(x_{i})}\\end{array}$ (note that here $S$ is an ordered subset) and precision $\\tau<\\beta/2$ , so that if the response $|v_{t}|^{\\bar{}}>\\beta/2$ , then $S\\in\\sigma_{*}(\\mathcal{C}_{\\mathsf{A}})$ . Note that $\\phi_{S}(y,x)$ are indeed in $\\mathcal{Q}_{\\sf A}$ : for SQ, CSQ this is direct, for ${\\sf D}{\\sf L}{\\sf Q}_{\\ell}$ , we can take $\\begin{array}{r}{f_{t}(\\pmb{x};\\omega)=\\pmb{u}+\\omega\\cdot\\pmb{v}\\prod_{i\\in S}T_{i}^{U_{*}}(x_{i})}\\end{array}$ . ", "page_idx": 19}, {"type": "text", "text": "If we take $\\tau<\\beta/2$ , the number of queries only needs to scale as $O(d^{\\mathsf{C o v e r_{A}}})$ or $O(d^{\\tt L e a p_{\\tt A}})$ . We can further trade-off the precision and accuracy and get the result of several of the above queries with one query with small enough $\\tau$ . For example, consider testing $d$ subset $\\{i\\}$ to find $s_{*}(1)$ . Then, one can group each of this query in $\\Theta(\\log(d))$ groups $G_{k}$ following the binary representation of $i$ . For each of these groups, consider the query ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{d}}\\sum_{s\\in G_{k}}T^{\\{1\\}}(y)T_{1}^{\\{1\\}}(z_{i}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that this is indeed a query in $\\Psi_{\\mathsf{A}}$ : it has $L^{2}(\\nu_{0})$ norm $O(1)$ (indeed $\\mathbb{E}[T^{(i)}(z_{i})T^{(j)}(z_{j})]=0$ for any $i\\neq j$ with at least one not in the support). With precision $O(1/{\\sqrt{d}})$ , one can test if one of those expectation is non-zero. Repeating for each of the $\\Theta(\\log(d))$ groups allows to recover the binary representation of $\\{s_{*}(1)\\}$ . This idea allows to trade-off the query and precision for any $q/\\tau^{2}\\geq C d^{k_{*}}\\log(d)$ . ", "page_idx": 19}, {"type": "text", "text": "C Proofs of Technical Results ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proposition 6.1: Denote ${\\boldsymbol{\\mathcal{V}}}\\,=\\,\\{a,b\\}$ . For any $U\\,\\in\\,{\\mathcal{C}}_{{\\mathsf{S Q}}}(\\mu)$ , by Definition 2, there exists $T\\ \\in$ $L^{2}(\\bar{\\mu}_{y})$ and $\\{T_{i}\\in L_{0}^{2}(\\mu_{x})\\}_{i\\in S}$ such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{0\\not=\\mathbb E\\Big[T(y)\\displaystyle\\prod_{i\\in U}T_{i}(z_{i})\\Big]}}\\\\ {{\\mathrm{}}}\\\\ {{\\mathrm{}}=T(a)\\mu_{y,z}(y=a)\\mathbb E\\Big[\\displaystyle\\prod_{i\\in U}T_{i}(z_{i})\\Big|y=a\\Big]+T(b)\\mu_{y,z}(y=b)\\mathbb E\\Big[\\displaystyle\\prod_{i\\in U}T_{i}(z_{i})\\Big|y=b\\Big]}}\\\\ {{\\mathrm{}}=(T(b)-T(a))\\mu_{y,z}(y=b)\\mathbb E\\Big[\\displaystyle\\prod_{i\\in U}T_{i}(z_{i})\\Big|y=b\\Big],}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we used that $\\mathbb{E}[T_{i}(z_{i})]=0$ . Hence, this expectation is non-zero for any mapping $T(b)\\neq T(a)$ , in particular $T=\\mathrm{{Id}}$ . We deduce that $U\\in{\\mathcal{C}}_{\\mathsf{C S Q}}(\\mu)$ too. ", "page_idx": 20}, {"type": "text", "text": "Consider $\\mathcal{X}\\,=\\,\\mathbb{R}$ and $\\mu_{x}\\ =\\ \\gamma$ the standard Gaussian measure. We assume that the label $y=$ $z_{1}z_{2}\\cdot\\cdot\\cdot z_{P}$ . Then $\\mathcal{C}_{\\sf C S Q}$ only contains the entire support $[P]$ and $\\mathsf{C o v e r}_{\\mathsf{C S Q}}(\\mu)=\\mathsf{L e a p}_{\\mathsf{C S Q}}(\\mu)=P$ On the other hand, consider the mappings $T(y)=\\mathbb{1}[|y|\\geq\\tau]$ for some $\\tau>0$ , and $T_{i}(z_{i})=z_{i}^{2}-1$ Then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}[T(y)T(z_{i})]=\\mathbb{E}_{z_{-i}}\\left[\\mathbb{E}_{z_{i}}\\left[(z_{i}^{2}-1)\\mathbb{1}\\left[|z_{i}|\\cdot\\prod_{j\\neq i}|z_{j}|\\geq\\tau\\right]\\right]\\right]>0,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and $\\mathcal{C}_{\\sf S Q}(\\mu)$ contains all the singletons $\\{i\\}$ for $i\\in[P]$ . We deduce $\\mathsf{C o v e r}_{\\mathsf{S Q}}(\\mu)=\\mathsf{L e a p}_{\\mathsf{S Q}}(\\mu)=1$ . ", "page_idx": 20}, {"type": "text", "text": "Proposition 6.2.(a): Note that $\\ell^{\\prime}(u,y)=(u-y)$ for the squared loss. For any $U\\subseteq[P]$ , and any $\\{T_{i}^{\\;\\;}\\mathbf{\\dot{\\in}}\\;L_{2}^{0}(\\mu_{x}):i\\in[P]\\}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{S}_{\\mu_{y,z}}\\left[(u-y)\\prod_{i\\in U}T_{i}(z_{i})\\right]=\\mathbb{E}_{\\mu_{y,z}}\\left[(u\\prod_{i\\in U}T_{i}(z_{i})\\right]-\\mathbb{E}_{\\mu_{y,z}}\\left[y\\prod_{i\\in U}T_{i}(z_{i})\\right]=-\\mathbb{E}_{\\mu_{y,z}}\\left[y\\prod_{i\\in U}T_{i}(z_{i})\\right].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mu_{y,z}}\\left[(u-y)\\prod_{i\\in U}T_{i}(z_{i})\\right]\\neq0\\mathrm{~if~and~only~if~}\\mathbb{E}_{\\mu_{y,z}}\\left[(y\\prod_{i\\in U}T_{i}(z_{i})\\right].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This immediately implies $\\mathcal{C}_{\\sf D L Q_{\\ell}}=\\mathcal{C}_{\\sf S Q}$ by Definition 2, and as a consequence $\\mathsf{L e a p}_{\\mathsf{C S Q}}=\\mathsf{L e a p}_{\\mathsf{D L Q}_{\\ell}}$ and CoverCSQ = CoverDLQ\u2113. ", "page_idx": 20}, {"type": "text", "text": "Proposition 6.2.(b): Consider $U\\in\\mathcal{C}_{\\mathsf{S Q}}$ with ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mu_{y,z}}\\Big[T(y)\\prod_{i\\in U}T_{i}(z_{i})\\Big]\\neq0.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that without loss of generality we can assume $T\\in L_{0}^{2}(\\mu_{y})$ . If $\\operatorname{span}(\\Psi_{\\mathsf{D L Q}_{\\ell}})$ is dense in $L_{0}^{2}(\\mu_{y})$ , then for every $\\varepsilon>0$ , there exists $M_{\\varepsilon}\\in\\mathbb N$ and $(v_{j},u_{j})_{j\\in[M_{\\varepsilon}]}\\subseteq\\mathcal{V}\\times\\mathcal{F}$ such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\tilde{T}_{M_{\\varepsilon}}(y)=\\sum_{j\\in[M_{\\varepsilon}]}\\boldsymbol{v}_{j}^{\\mathsf{T}}\\nabla\\ell(\\boldsymbol{u}_{j},\\boldsymbol{y})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "has $\\|T-\\tilde{T}_{M_{\\varepsilon}}\\|_{L^{2}(\\mu_{y})}\\leq\\varepsilon$ . In particular, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{\\mu_{y,z}}\\left[\\tilde{T}_{M_{\\varepsilon}}(y)\\prod_{i\\in U}T_{i}(z_{i})\\right]-\\mathbb{E}_{\\mu_{y,z}}\\left[T(y)\\prod_{i\\in U}T_{i}(z_{i})\\right]\\right|\\leq\\|T-\\tilde{T}_{M_{\\varepsilon}}\\|_{L^{2}(\\mu_{y})}\\prod_{i\\in U}\\|T_{i}\\|_{L^{2}(\\mu_{x})}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore taking $\\varepsilon$ sufficiently small, $\\tilde{T}_{M_{\\varepsilon}}$ has non zero correlation with $\\textstyle\\prod_{i\\in U}T_{i}$ , and in particular, one of the $\\pmb{v}_{j}^{\\top}\\nabla\\ell(\\pmb{u}_{j},\\{y\\})$ must have non-zero correlation too. We deduce that $\\mathcal{C}_{5\\sf Q}\\subseteq\\mathcal{C}_{\\sf D L Q,\\ell}$ and we conclude using $\\mathcal{C}_{\\sf D L Q_{\\ell}}\\subseteq\\mathcal{C}_{\\sf S Q}$ . ", "page_idx": 20}, {"type": "text", "text": "Let us assume that there exists nonzero bounded $T~\\in~L_{0}^{2}(\\mu_{y})$ , which we take without loss of generality $T(y)\\subseteq[-1,1]$ , such that $\\mathbb{E}[\\pmb{v}^{\\top}\\nabla\\ell(\\pmb{u},y)T(y)]=0$ for all $\\pmb{u}\\in\\mathcal{F},\\pmb{v}\\in V$ . The goal is to ", "page_idx": 20}, {"type": "text", "text": "define $\\mu_{y,z}$ such that $\\mathcal{C}_{\\sf D L Q_{\\ell}}\\subsetneq\\mathcal{C}_{\\sf S Q}$ . Specifically, we will construct a joint distribution on $(y,z_{i})$ with $\\{i\\}\\in{\\mathcal{C}}_{{\\mathsf{S}}{\\mathsf{Q}}}^{}$ but $\\{i\\}\\not\\in{\\cal C}_{\\sf D L Q_{\\ell}}$ . Let $A\\subset\\mathcal{X}$ with ${\\dot{\\mu}}_{x}(A)\\in(0,1)$ and consider $y$ that only depends on $z_{i}$ through $\\mathbb{1}[z_{i}\\in A]$ . Denote $P(A|y)=\\mathbb{P}(z_{i}\\in A|y)$ . For any $T_{i}\\in L_{0}^{2}(\\mu_{x})$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}[T_{i}(z_{i})|y]=P(A|y)\\mathbb{E}[T_{i}(z_{i})|z_{i}\\in A]+P(A^{c}|y)\\mathbb{E}[T_{i}(z_{i})|z_{i}\\notin A].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Denote $\\kappa(A)=\\mathbb{E}[T_{i}(z_{i})|z_{i}\\in A]$ . By definition $\\mathbb{E}[T_{i}(z_{i})]=0=\\mu_{x}(A)\\kappa(A)+\\mu_{x}(A^{c})\\kappa(A^{c})$ , and therefore $\\kappa(A^{c})=-\\mu_{x}(A)\\kappa(A)/(1-\\mu_{x}(A))$ . Hence, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}[T_{i}(z_{i})|y]={\\frac{\\kappa(A)}{1-\\mu_{x}(A)}}\\left[P(A|y)-\\mu_{x}(A)\\right].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Taking $\\lambda>(1-\\mu_{x}(A))/\\mu_{x}(A)$ , we can set ", "page_idx": 21}, {"type": "equation", "text": "$$\nP(A|y):=\\lambda^{-1}(1-\\mu_{x}(A))T(y)+\\mu_{x}(A)\\in(0,1).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We then define the joint distribution of $(y,z_{i})$ as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mu_{y,z_{i}}:=[\\mu_{x}(z_{i}|z_{i}\\in A)P(A|y)+\\mu_{x}(z_{i}|z_{i}\\not\\in A)P(A^{c}|y)]\\mu_{y}(y).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that $\\mathbb{E}[P(A|y)]\\,=\\,\\mu_{x}(A)$ using that $\\mathbb{E}[T(y)]\\,=\\,0$ , and therefore the marginals of $\\mu_{y,z_{i}}$ are indeed $\\mu_{y}$ and $\\mu_{x}$ . For any $T_{i}\\in L_{0}^{2}(\\mu_{x})$ and $(\\boldsymbol{u},\\boldsymbol{v})\\in\\mathcal{F}\\times V$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mu_{y,z_{i}}}[\\pmb{v}^{\\top}\\nabla\\ell(\\pmb{u},y)T_{i}(z_{i})]=\\frac{\\kappa(A)}{\\lambda}\\mathbb{E}_{\\mu_{y}}[\\pmb{v}^{\\top}\\nabla\\ell(\\pmb{u},y)T(y)]=0,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and therefore $\\{i\\}\\not\\in{\\mathcal{C}}_{\\mathsf{D L Q}_{\\ell}}$ . On the other hand $T\\in L^{2}(\\mu_{y})$ and therefore $\\{i\\}\\in\\mathcal{C}_{\\mathsf{S Q}}$ . We conclude that the joint distribution $(y,z_{i})$ has $\\mathsf{L e a p}_{\\mathsf{D L Q}_{\\ell}}=\\mathsf{C o v e r}_{\\mathsf{D L Q}_{\\ell}}=\\infty$ while $\\mathsf{L e a p}_{\\mathsf{S Q}}=\\mathsf{C o v e r}_{\\mathsf{S Q}}=1$ . ", "page_idx": 21}, {"type": "text", "text": "Proposition 6.2.(c): We construct a simple example with $\\mathcal{X}=\\{+1,-1\\}$ and $\\mu_{x}=\\operatorname{Unif}({\\mathcal{X}})$ , i.e., uniform on the discrete hypercube. Consider $P=2k$ and label ", "page_idx": 21}, {"type": "equation", "text": "$$\ny=z_{1}z_{2}\\dots z_{2k}\\left(\\sum_{i\\in[2k]}z_{i}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The set $\\mathcal{C}_{\\sf C S Q}(\\mu)$ contains all subsets $[2k]\\setminus\\{i\\}$ for all $i\\in[2k]$ , and therefore ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\mathsf{C o v e r}}_{\\mathsf{C S Q}}(\\mu)={\\mathsf{L e a p}}_{\\mathsf{C S Q}}(\\mu)=2k-1.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Consider the loss function $\\begin{array}{r}{\\ell(u,y)=\\frac{1}{2}(y-u)^{2}+\\frac{1}{4}(y-u)^{4}}\\end{array}$ . The derivative is given by ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\ell^{\\prime}(u,y)=u-y+u^{3}-3u^{2}y+3u y^{2}-y^{3}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We have ", "page_idx": 21}, {"type": "equation", "text": "$$\ny^{2}=2k+2\\sum_{i<j}z_{i}z_{j},\\qquad y^{3}=z_{1}z_{2}\\ldots z_{2k}\\left(\\sum_{i\\in[2k]}z_{i}\\right)^{3}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For $k\\geq3$ , $y^{3}$ only contains monomials over $\\geq2k-3\\geq3$ coordinates. Therefore $\\mathcal{C}_{\\sf D L Q_{\\ell}}(\\mu)$ contains all pairs $\\{i,j\\}$ but no singleton. Hence, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathsf{C o v e r}_{\\mathsf{D L Q}_{\\ell}}(\\mu)=\\mathsf{L e a p}_{\\mathsf{D L Q}_{\\ell}}(\\mu)=2.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Finally, for $\\mathcal{C}_{\\sf S Q}$ , taking $T(y)=y^{2k-1}$ , it contains all singleton $\\{i\\}$ and therefore ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\mathsf{C o v e r}}_{\\mathsf{S Q}}(\\mu)={\\mathsf{L e a p}}_{\\mathsf{S Q}}(\\mu)=1.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "C.1 Proof of Theorem 6.3 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Note that $\\mathcal{C}_{\\sf A}$ does not change if we add or remove constant functions to $\\Psi_{\\mathsf{A}}$ . ", "page_idx": 21}, {"type": "text", "text": "$\\ell_{1}$ -loss: First consider $\\ell(u,y)=|y-u|$ . We have $\\nabla\\ell(u,y):=\\ell^{\\prime}(u,y)=\\mathrm{sign}(u-y)$ , where we can set without loss of generality $\\mathrm{sign}(0)=0$ . From Hornik [1991, Theorem 5] and Hornik [1991, Theorem 1], we directly conclude that for any probability measure $\\mu_{y}$ on $\\mathbb{R}$ , we have $\\operatorname{span}(\\Psi_{\\mathsf{A}})$ dense in $L^{2}(\\mu_{y})$ and we conclude using Proposition 6.2.(b). ", "page_idx": 21}, {"type": "text", "text": "Hinge loss: Consider the Hinge loss $\\ell(u,y)=\\operatorname*{max}(1-y u,0)$ . We get $\\ell^{\\prime}(u,y)=-y\\mathbb{1}_{u y\\leq1}$ where without loss of generality, we set $\\ell^{\\prime}(1/y,y)=-y$ . For $a>0$ and $b<0$ , we have $\\ell^{\\prime}(a,y)\\!-\\!\\bar{\\ell^{\\prime}}(b,y)=$ $y\\mathbb{1}[y>a]-y\\bar{\\mathbb{1}}[y<b]$ . Taking $b\\rightarrow-\\infty$ , we have $\\ell^{\\prime}(a,y)-\\ell^{\\prime}(b,y)$ that converges to $y\\mathbb{1}[y>a]$ in $L^{2}(\\mu_{y})$ (recall that we assume that the second moment of $\\mu_{y}$ is bounded). Similarly, we can construct $y\\mathbb{1}[y>a]$ for $a\\leq0$ . Following the proof of Hornik [1991, Theorem 1], if $\\operatorname{span}(\\Psi_{\\mathsf{A}})$ is not dense in $L^{2}\\dot{(\\mu_{y})}$ , there exists $g\\in L^{2}(\\bar{\\mu_{y}})$ such that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\int y\\mathbb{1}[y>u]g(y)\\mathrm{d}\\mu_{y}=0,\\qquad\\forall u\\in\\mathbb{R}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Define the signed measure $\\begin{array}{r l r}{\\sigma(B)}&{{}=}&{\\int_{B}y g(y)\\mathrm{d}\\mu_{y}}\\end{array}$ . It is finite since $\\begin{array}{r l}{\\int|y g(y)|\\mathrm{d}\\mu_{y}}&{{}\\leq}\\end{array}$ $\\|y\\|_{L^{2}(\\mu_{y})}\\|g\\|_{L^{2}(\\mu_{y})}<\\infty$ by assumption. We deduce that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\int\\mathbb{1}[a y-u>0]\\mathrm{d}\\sigma(y)=0\\qquad\\forall(a,u)\\in\\mathbb{R}^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "(The case $a=0$ is obtained by taking $u\\rightarrow-\\infty$ in Eq. (65).) We then use that $\\mathbb{1}[a y-u>0]$ is discriminatory and therefore no such finite signed measure exists. ", "page_idx": 22}, {"type": "text", "text": "Exponential loss: Consider ${\\mathcal{V}}\\subseteq[-M,M]$ and $\\ell(u,y)=e^{-u y}$ , so that $\\ell^{\\prime}(u,y)=-y e^{-y u}$ . Any Borel probability measure $\\mu_{y}$ on $[-M,M]$ is regular, and in particular continuous functions and therefore polynomials are dense in $L^{2}(\\mu_{y})$ . To show that span $\\{y e^{-y u}:u\\in\\mathbb{R}\\}$ is dense in $L^{2}(\\mu_{y})$ , it is sufficient to show that we can approximate any monomial $y^{k}$ in $L^{2}(\\mu_{y})$ . This is readily proven recursively, using that ", "page_idx": 22}, {"type": "equation", "text": "$$\ny e^{-y u}=\\sum_{l=0}^{k-1}{\\frac{(-1)^{l}}{l!}}u^{l}y^{l+1}+O(u^{l})\\cdot e^{M}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This concludes the proof of this theorem. ", "page_idx": 22}, {"type": "text", "text": "D SGD, mean-field and limiting dimension-free dynamics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Fix a link distribution $y\\vert z\\sim\\mu_{y\\vert z}$ and consider data distribution $(y,\\pmb{x})\\sim\\mathcal{D}_{s_{*}}^{d}\\in\\mathcal{H}_{\\mu}^{d}$ with $\\mu=$ $\\left(\\mu_{x},\\mu_{y\\vert z}\\right)$ and (unknown) $s_{*}\\in\\mathtt{P}([d],P)$ . We consider learning using a two-layer neural network with parameters $\\Theta\\in\\mathbb{R}^{M(d+2)+1}$ and an activation $\\sigma:\\mathbb{R}\\rightarrow\\mathbb{R}$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\nf_{\\mathsf{N N}}(\\boldsymbol{x};\\boldsymbol{\\Theta})=c+\\frac{1}{M}\\sum_{\\boldsymbol{j}\\in[M]}a_{\\boldsymbol{j}}\\sigma(\\langle\\boldsymbol{w}_{j},\\boldsymbol{x}\\rangle\\!+\\!b_{j}),\\qquad c,a_{j},b_{j},\\boldsymbol{\\in}\\mathbb{R},\\quad\\boldsymbol{w}_{j}\\in\\mathbb{R}^{d},\\;\\mathrm{for}\\;j\\in[M]\\;\\;(\\mathsf{N N}])\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For convenience, we will reparametrize the neural network for $\\pmb{\\Theta}=(\\pmb{\\theta}_{j})_{j\\in[M]}\\,\\in\\mathbb{R}^{M(d+3)}$ with $\\pmb{\\theta}_{j}=(a_{j},\\pmb{w}_{j},b_{j},c_{j})\\in\\mathbb{R}^{d+3}$ and ", "page_idx": 22}, {"type": "equation", "text": "$$\nf_{\\mathsf{N N}}(\\boldsymbol{x};\\boldsymbol{\\Theta})=\\frac{1}{M}\\sum_{j\\in[M]}\\sigma_{\\ast}(\\boldsymbol{x};\\pmb{\\theta}_{j}),\\qquad\\sigma_{\\ast}(\\boldsymbol{x};\\pmb{\\theta}_{j})=a_{j}\\sigma(\\langle\\pmb{w}_{j},\\pmb{x}\\rangle+b_{j})+c_{j}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Indeed, the two neural networks (NN1) and (NN2) remain equal throughout training under the initialization c1 = . . . = cM = c. ", "page_idx": 22}, {"type": "text", "text": "Also, recall the definitions of the risk and the excess risk, respectively, for a data distribution $(y,x)\\sim\\mathcal{D}$ , with respect to a loss function $\\ell:\\mathbb{R}\\times\\mathcal{Y}\\rightarrow\\mathbb{R}_{\\ge0}$ . ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{R}(f)=\\mathbb{E}_{\\mathcal{D}}\\left[\\ell(f(\\pmb{x}),y)\\right],\\quad\\mathrm{and}\\quad\\overline{{\\mathcal{R}}}(f)=\\mathcal{R}(f)\\ -\\operatorname*{inf}_{\\substack{\\bar{f}:\\{\\pmb{\\Omega}\\}^{d}\\rightarrow\\mathbb{R}}}\\mathcal{R}(\\bar{f}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We start with the initialization specified by $\\rho_{0}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n(a^{0},b^{0},\\sqrt{d}\\cdot{\\pmb w}^{0},c^{0})\\sim\\mu_{a}\\otimes\\mu_{b}\\otimes\\mu_{w}^{\\otimes d}\\otimes\\delta_{c=\\overline{{{c}}}}:=\\rho_{0}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Batch Stochastic Gradient Descent. We train the parameters $\\Theta$ using batch-SGD with loss $\\ell$ and the batch size $b$ . Even in more generality than stated in (\u2113-bSGD), we allow for time-varying step size $(\\eta_{k})_{k\\geq0}$ , where $\\pmb{\\eta}_{k}\\in\\mathbb{R}^{d+3}$ can be different for different parameters (e.g., different layers), and $\\ell_{2}$ -regularization $\\lambda\\in\\mathbb{R}^{d+3}$ . We initialize the weights $(\\pmb{\\theta}_{j})_{j\\in[M]}\\stackrel{\\mathrm{i.i.d.}}{\\sim}\\rho_{0}$ from (69), and at each step, given samples $(\\{(x_{k i},y_{k i}):i\\in[b]\\})_{k\\geq0}$ , the weights are updated using ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\theta_{j}^{k+1}=\\theta_{j}^{k}-\\frac{1}{b}\\sum_{i\\in[b]}\\ell^{\\prime}(f_{\\mathrm{NN}}(\\mathbf{x}_{k i};\\mathbf{\\Theta}^{t}),y_{k i})\\cdot H_{k}\\nabla_{\\theta}\\sigma_{\\ast}(x_{k i};\\theta_{j}^{k})-H_{k}\\Lambda\\theta_{j}^{k},\\qquad(\\ell\\mathrm{-}\\mathrm{bSGD-g})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we introduced $\\begin{array}{r}{\\pmb{H}_{k}:=\\mathrm{diag}(\\pmb{\\eta}_{k})}\\end{array}$ and $\\begin{array}{r}{\\mathbf{A}:=\\mathrm{diag}(\\lambda)}\\end{array}$ . ", "page_idx": 23}, {"type": "text", "text": "Mean-Field Dynamics. A rich line work [Chizat and Bach, 2018, Mei et al., 2018, Rotskoff and Vanden-Eijnden, 2018, Sirignano and Spiliopoulos, 2020] has established a crisp approximation between the dynamics of one-pass-batch-SGD ( $\\boldsymbol{\\ell}$ -bSGD- $\\mathrm{\\bfg}$ ) and a continuous dynamics in the space of probability distributions in $\\mathbb{R}^{\\hat{d}+3}$ . To any distribution $\\rho\\in\\mathcal{P}(\\mathbb{R}^{d+3})$ , we associate the infinite-width neural network ", "page_idx": 23}, {"type": "equation", "text": "$$\nf_{\\mathsf{N N}}(\\pmb{x};\\pmb{\\rho})=\\int\\sigma_{*}(\\pmb{x};\\pmb{\\theta})\\rho(\\mathrm{d}\\pmb{\\theta}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In particular, we recover the finite-width neural network (NN2) by taking $\\begin{array}{r}{\\hat{\\rho}^{(M)}:=\\frac{1}{M}\\sum_{j\\in[M]}\\delta_{\\pmb{\\theta}_{j}}}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "Denote \u03c1\u02c6(kM)the empirical distribution of the weights {\u03b8jk }j\u2208[M] after k-steps of (\u2113-bSGD-g). In our ( $\\ell$ -bSGD-g) dynamics, we allow for a generic step-size schedule that is captured by ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\eta_{k}=\\eta\\pmb{\\xi}(k\\eta)\\quad\\mathrm{~for~some~function~}\\quad\\pmb{\\xi}:\\mathbb{R}\\to\\mathbb{R}_{\\ge0}^{d+3}\\,,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and that the data at each step are sampled i.i.d. from $\\mathcal{D}$ ; the case of $\\mathcal{D}$ empirical distribution on training data corresponds to multi-pass batch-SGD, while $\\mathcal{D}$ population distribution corresponds to online batch-SGD. Under some reasonable assumptions (that we will list below on the activation, step-size schedule $\\xi$ , initialization $\\rho_{0}$ , and the loss $\\boldsymbol{\\ell}$ ) on taking $M$ sufficiently large and $\\eta$ sufficiently small, setting $k\\,=\\,t/\\eta,\\,\\hat{\\rho}^{(M)}$ is well approximated by a distribution $\\rho_{t}\\,\\in\\,\\mathcal{P}(\\mathbb{R}^{d+3})$ that evolves according to the PDE: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\partial_{t}\\rho_{t}=\\nabla_{\\pmb{\\theta}}\\cdot(\\rho_{t}\\pmb{H}(t)\\nabla_{\\pmb{\\theta}}\\psi(\\pmb{\\theta};\\rho_{t})),}\\\\ &{\\qquad\\psi(\\pmb{\\theta};\\rho_{t})=\\mathbb{E}_{\\mathcal{D}}[\\ell^{\\prime}(f_{\\mathsf{N N}}(\\pmb{x};\\rho_{t}),y)\\sigma_{\\ast}(\\pmb{x};\\pmb{\\theta})]+\\frac{1}{2}\\pmb{\\theta}^{\\top}\\Lambda\\pmb{\\theta},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the initial distribution is $\\rho_{0}$ and we defined $H(t)=\\mathrm{diag}(\\pmb{\\xi}(t))$ . We will refer to the distribution dynamics (MF-PDE) as the mean-field (MF) dynamics. We now specify the assumptions on hyperparameters under which a non-asymptotic equivalence can be derived. ", "page_idx": 23}, {"type": "text", "text": "A D.1 (Activation). Our $\\sigma:\\mathbb{R}\\rightarrow\\mathbb{R}$ is three times differentiable with $\\|\\sigma^{(k)}\\|_{\\infty}\\leq K$ , $k=0,\\ldots,3$ . A D.2 (Bounded, Lipschitz step-size schedule). $t\\mapsto\\pmb{\\xi}(t)=(\\xi_{i}(t))_{i\\in[d+3]}$ has bounded Lipschitz entries: $\\|\\xi_{i}\\|_{\\infty}\\leq K$ and $\\|\\xi_{i}\\|_{\\mathrm{Lip}}\\leq K$ . ", "page_idx": 23}, {"type": "text", "text": "A D.3 (Initialization). The initial distribution $\\rho_{0}\\in\\mathcal{P}(\\mathbb{R}^{d+3})$ is of the form: ", "page_idx": 23}, {"type": "equation", "text": "$$\n(a^{0},b^{0},\\sqrt{d}\\cdot w^{0},c^{0})\\sim\\mu_{a}\\otimes\\mu_{b}\\otimes\\mu_{w}^{\\otimes d}\\otimes\\delta_{c=\\overline{{{c}}}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\mu_{a},\\mu_{w},\\mu_{b}$ are independent of $d,$ . We further assume that $\\mu_{a}$ supported on $|a|\\leq K,\\,|\\overline{{c}}|\\leq K,$ , and $\\mu_{w}$ is symmetric and $\\bar{K}^{2}$ -sub-Gaussian. ", "page_idx": 23}, {"type": "text", "text": "A D.4 (Loss). The loss $\\ell:\\mathbb{R}\\times\\mathcal{Y}\\rightarrow\\mathbb{R}_{\\ge0}$ is twice-differentiable in its first argument for all $y\\in\\mathcal{V}$ and satisfies ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\ell^{\\prime}(u,y)|\\leq K(1+|u|)\\,a n d\\,|\\ell^{\\prime\\prime}(u,y)|\\leq K\\,f o r\\,a l l\\,y\\in\\mathcal{Y}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Assumptions D.1-D.3 are similar to Abbe et al. [2022]. Informally, Assumption D.4 states that loss has a quadratic upper bound (which is analogues to the smoothness assumption in the convex optimization literature). Later, to show the learnability of leap one junta settings, we will further require $\\ell(\\cdot,y)$ to be convex and analytic. We note that together these assumptions holds for $(i)$ the standard logistic loss for the classification setting when $\\bar{y}\\in\\{\\pm1\\}$ , and $(i i)$ for the squared loss and any of its \u201canalytic perturbations\u201d for the regression setting when $y\\subseteq[-B,B]$ . In particular, we would like to mention $\\ell(u,y)=(u-y)\\mathrm{archsinh}(u-y)$ , for which we have ${\\mathsf{L e a p}}_{\\mathsf{D L Q}_{\\ell}}={\\mathsf{L e a p}}_{\\mathsf{S Q}}$ and these assumptions also hold, and hence our results apply. For classification, by Proposition 6.1, anyway $\\mathsf{L e a p S Q}=\\mathsf{L e a p C S Q}=\\mathsf{L e a p}_{\\mathsf{D L Q}_{\\ell}}$ for any non-degenerate loss (including the standard logistic loss for which our results hold). ", "page_idx": 23}, {"type": "text", "text": "Limiting Dimension-Free Dynamics. In the junta learning setting, when $y$ only depends on $P\\ll$ $d$ coordinates, following Abbe et al. [2022, Secion 3], the SGD dynamics $\\boldsymbol{\\ell}$ -bSGD) concentrates on an effective dimension-free (DF) dynamics as $M,d\\rightarrow\\infty$ and $\\eta\\rightarrow0$ . This equivalence holds under a certain assumption on the loss function, and other assumptions on the initialization and activation that are similar to the setup of Abbe et al. [2022]. ", "page_idx": 24}, {"type": "text", "text": "Consider the isotropic parameters $H_{k}=\\mathrm{diag}(\\eta_{k}^{a},\\eta_{k}^{w}{\\bf I}_{d},\\eta_{k}^{b},\\eta_{k}^{c})$ and ${\\pmb{\\Lambda}}=\\mathrm{diag}(\\lambda^{a},\\lambda^{w}{\\bf I}_{d},\\lambda^{b},\\lambda^{c})$ . Using technical ideas from Abbe et al. [2022, Secion 3], the ( $\\ell$ -bSGD- $\\mathrm{\\bfg}$ ) concentrates on an effective dimension-free dynamics in the limit $M,d\\rightarrow\\infty$ and $\\eta\\rightarrow0$ . This limiting dynamics corresponds to the gradient flow on $\\mathcal{R}(f)+\\pmb{\\theta}^{\\top}\\pmb{\\Lambda}\\pmb{\\theta}$ of the following effective infinite-width neural network (recall $z\\in\\mathbf{\\overline{{R}}}^{P}$ is the support) ", "page_idx": 24}, {"type": "equation", "text": "$$\nf_{\\sf N N}(z;\\bar{\\rho}_{t})=\\int\\overline{{\\sigma}}_{*}(z;\\bar{\\theta}^{t})\\bar{\\rho}_{t}(\\mathrm{d}\\bar{\\theta}^{t}),\\qquad\\overline{{\\sigma}}_{*}(z;\\bar{\\theta}^{t})=c^{t}+a^{t}\\mathbb{E}_{G}[\\sigma(\\langle z,u^{t}\\rangle+s^{t}G+b^{t})],\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $G\\sim\\mathsf{N}(0,1)$ and $\\bar{\\rho}_{t}\\in\\mathcal{P}(\\mathbb{R}^{P+4})$ is the distribution over $\\bar{\\pmb{\\theta}}^{t}=(a^{t},b^{t},\\pmb{u}^{t},c^{t},s^{t})$ with $\\pmb{u}^{t}\\in\\mathbb{R}^{P}$ . ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\partial_{t}\\bar{\\rho}_{t}=\\nabla_{\\bar{\\theta}}\\cdot\\left(\\bar{\\rho}_{t}\\overline{{H}}(t)\\cdot\\nabla_{\\bar{\\theta}}\\psi(\\bar{\\pmb{\\theta}},\\bar{\\rho}_{t})\\right),}\\\\ {\\displaystyle\\psi(\\bar{\\pmb{\\theta}},\\bar{\\rho}_{t})=\\mathbb{E}_{\\mu_{y,z}}\\left[\\ell^{\\prime}(f_{\\mathsf{N N}}(z;\\bar{\\rho}_{t}),y)\\overline{{\\sigma}}_{*}(z;\\bar{\\pmb{\\theta}})\\right]+\\frac{1}{2}\\bar{\\pmb{\\theta}}^{\\mathsf{T}}\\overline{{\\Lambda}}\\bar{\\pmb{\\theta}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\overline{{{\\cal H}}}(t)=\\mathrm{diag}(\\xi^{a},\\xi^{b},\\xi^{w}{\\bf I}_{P},\\xi^{c},\\xi^{w})(t)$ and $\\overline{{{\\mathbf{\\cal{A}}}}}=\\mathrm{diag}(\\lambda^{a},\\lambda^{b},\\lambda^{w}\\mathbf{I}_{P},\\lambda^{w})$ from initialization ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\bar{\\rho}_{0}:=\\mu_{a}\\otimes\\mu_{b}\\otimes\\delta_{{\\pmb u}^{0}={\\pmb0}}\\otimes\\delta_{c^{0}=\\overline{{{c}}}}\\otimes\\delta_{s^{0}=\\sqrt{m_{2}^{w}}},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $m_{2}^{w}\\,=\\,\\mathbb{E}_{W\\sim\\mu_{w}}[W^{2}]^{1/2}$ is the second moment of $\\mu_{w}$ . The non-asymptotic equivalence is characterized by the following theorem that can be seen as a generalization of [Abbe et al., 2022, Theorem 5] to non-squared losses. ", "page_idx": 24}, {"type": "text", "text": "Theorem D.5. Fix activation, step-size schedule, initialization, and the loss such that Assumptions D.1 to D.4 hold and consider any $T\\geq1$ independent of $d.$ Let $(\\bar{\\rho}_{t})_{t\\geq0}$ be the solution of (DF-PDE), and $\\{\\Theta_{k}\\}_{k\\ge0}$ the trajectory of SGD $\\boldsymbol{\\ell}$ -bSGD-g) with initialization $\\{\\pmb{\\theta}_{j}^{0}\\}_{j\\in[M]}\\stackrel{i.i.d.}{\\sim}\\rho_{0}$ . Then there exist a constant $C_{K,T}>0$ that only depends on $K,T$ , such that for any $d,M,\\eta>0$ with $M\\leq e^{d}$ and $0<\\eta\\leq1/[C_{K,T}(d+\\log(M))]$ , and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left|\\mathcal{R}(f_{\\mathsf{N N}}(\\cdot;\\Theta^{k}))-\\mathcal{R}(f_{\\mathsf{N N}}(\\cdot;\\bar{\\rho}_{\\eta k}))\\right|\\le C_{K,T}\\left\\{\\sqrt{\\frac{P}{d}}+\\sqrt{\\frac{\\log(M)}{M}}+\\sqrt{\\eta}\\sqrt{\\frac{d+\\log(M)}{b}\\vee1}\\right\\}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for all $k\\leq T/\\eta_{-}$ , with probability at least $1-1/M$ . ", "page_idx": 24}, {"type": "text", "text": "This theorem follows by a straightforward modification of the proof of [Abbe et al., 2022, Theorem 16, Proposition 15] using propagation of chaos argument. We note that the only property about the loss that is needed is Assumption D.4. ", "page_idx": 24}, {"type": "text", "text": "D.1 DF-PDE alignment with the support. ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this subsection, we provide a formal statement of Theorem 7.1 and its proof. The following is an extension of [Abbe et al., 2022, Theorem 7] from square loss to general loss. ", "page_idx": 24}, {"type": "text", "text": "Theorem D.6 (Formal statement of Theorem 7.1). Assume that $\\mathsf{L e a p\\scriptscriptstyle D L Q}_{\\ell}(\\mu)\\,>\\,1$ . Then there exists $U\\subset[P]$ , $|U|\\ge2,$ , such that for all $t\\geq0$ , the $D F$ dynamics solution remains independent of coordinates $(z_{i})_{i\\in U}$ , i.e., $f_{\\mathsf{N N}}(z;\\rho_{t})\\,=\\,f_{\\mathsf{N N}}((z_{i})_{i\\notin U};\\rho_{t})$ , and therefore fails at recovering the support. ", "page_idx": 24}, {"type": "text", "text": "Conversely, $i f\\mathsf{L e a p}_{\\mathsf{D L Q}_{\\ell}}(\\mu)=1$ and $\\ell$ is analytic with respect to its first argument, then almost surely over $\\overline{{c}},$ , there exists $t>0$ such that $f_{\\mathsf{N N}}(z;\\rho_{t})$ depends on all $[P]$ . ", "page_idx": 24}, {"type": "text", "text": "Proof of Theorem D.6. The proof follows similarly to the proof of [Abbe et al., 2022, Theorem 7]. Assume $\\mathsf{L e a p}_{\\mathsf{D L Q}_{\\ell}}(\\mu)>1$ . Consider $\\mathcal{C}_{*}\\subset\\mathcal{C}_{\\sf D L Q_{\\ell}}$ the maximal subset such that $\\mathsf{L e a p}(\\mathcal{C}_{*})=1$ and denote $\\begin{array}{r}{U_{*}=\\bigcup_{U\\in\\mathcal{C}_{*}}U}\\end{array}$ . In words, $U_{*}$ contains the subset of coordinates that are reachable by doing leaps of at most 1. By the assumption that $\\mathsf{L e a p}_{\\mathsf{D L Q}_{\\ell}}(\\mu)>1$ , we must have $|[P]\\setminus U_{*}|\\ge2$ . Let us show that the weights $u_{i}$ for $i\\in[P]\\setminus U_{*}$ remain equal to 0 throughout the dynamics. ", "page_idx": 24}, {"type": "text", "text": "For simplicity, we forget about the other parameters which we set to $b=c=s=0$ . For convenience, denote $\\bar{\\Omega}=\\left[P\\right]\\backslash U_{*}$ . First, we can bound by Gronwall\u2019s inequality, for $t\\leq C$ , $|a^{t}|\\leq K C^{\\prime}$ for all $a^{t}$ on the support of $\\rho_{t}$ . From the proof of Theorem 5.1, conditioning on already explored coordinates does not change the leap-complexity. In particular, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\ell^{\\prime}(g(z_{U_{\\ast}}),y)z_{i}\\vert z_{U_{\\ast}}]=0,\\qquad\\forall i\\in\\Omega.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Consider the time derivative of $u_{i}^{t}$ for $i\\in\\Omega$ . We have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad|\\ell^{\\prime}(f_{\\mathsf{N N}}(z_{U_{\\bullet}},z_{\\Omega};\\rho_{t}),y)\\sigma^{\\prime}(\\langle u_{U_{\\bullet}}^{t},z_{U_{\\bullet}}\\rangle+\\langle u_{\\Omega}^{t},z_{\\Omega}\\rangle)-\\ell^{\\prime}(f_{\\mathsf{N N}}(z_{U_{\\bullet}},\\mathbf{0};\\rho_{t}),y)\\sigma^{\\prime}(\\langle u_{U_{\\bullet}}^{t},z_{U_{\\bullet}}\\rangle)|}\\\\ &{\\le K\\|\\sigma\\|_{\\infty}\\displaystyle\\int|\\tilde{a}^{t}||\\sigma(\\langle\\tilde{u}_{U_{\\bullet}}^{t},z_{U_{\\bullet}}\\rangle+\\langle\\tilde{u}_{\\Omega}^{t},z_{\\Omega}\\rangle)-\\sigma(\\langle\\tilde{u}_{U_{\\bullet}}^{t},z_{U_{\\bullet}}\\rangle)|\\rho_{t}(\\mathrm{d}\\tilde{\\theta})}\\\\ &{\\phantom{\\le}+K\\|f_{\\mathsf{N N}}(\\cdot;\\rho_{t})\\|_{\\infty}|\\sigma(\\langle u_{U_{\\bullet}}^{t},z_{U_{\\bullet}}\\rangle+\\langle u_{\\Omega}^{t},z_{\\Omega}\\rangle)-\\sigma(\\langle u_{U_{\\bullet}}^{t},z_{U_{\\bullet}}\\rangle)|}\\\\ &{\\le K\\displaystyle\\operatorname*{sup}_{j\\in\\Omega,u_{j}^{t}\\in\\operatorname{supp}(\\rho_{t})}|u_{j}^{t}|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where we used Assumptions D.1 and D.4. Hence, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\cfrac{\\mathrm{d}}{\\mathrm{d}t}u_{i}^{t}\\right|=\\left|a^{t}\\mathbb{E}[\\ell^{\\prime}(f_{\\mathsf{N N}}(z,\\rho_{t}),y)\\sigma^{\\prime}(\\langle u^{t},z\\rangle)z_{i}]\\right|}\\\\ &{\\qquad\\qquad\\leq\\left|a^{t}\\mathbb{E}[\\ell^{\\prime}(f_{\\mathsf{N N}}(z_{U_{\\bullet}},\\mathbf{0};\\rho_{t}),y)\\sigma^{\\prime}(\\langle u_{U_{\\bullet}}^{t},z_{U_{\\bullet}}\\rangle)z_{i}]\\right|+K\\operatorname*{sup}_{j\\in\\Omega,u_{j}^{t}\\in\\mathrm{supp}(\\rho_{t})}|u_{j}^{t}|}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The first expectation is equal to 0. Note that $\\begin{array}{r}{m_{\\Omega}^{t}\\;:=\\;\\mathrm{sup}_{j\\in\\Omega,u_{j}^{t}\\in\\mathrm{supp}(\\rho_{t})}\\;|u_{j}^{t}|}\\end{array}$ has $m_{\\Omega}^{0}\\,=\\,0$ and therefore $m_{\\Omega}^{t}=0$ for all $t\\geq0$ . \u53e3 ", "page_idx": 25}, {"type": "text", "text": "E Learning Leap One Juntas with SGD: Formalizing Theorem 7.2 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we provide our precise layer-wise training algorithm and the proof that it succeeds in learning ${\\mathsf{L e a p}}_{\\mathsf{D L Q}_{\\ell}}=1$ junta problems. This section follows the similar training procedure from Abbe et al. [2022] for the layer-wise training but adapted to training with general loss function $\\ell:\\mathbb{R}\\times\\mathcal{Y}\\rightarrow\\mathbb{R}_{\\ge0}$ to show Theorem 7.2. The key difference in the training algorithm from Abbe et al. [2022] is the use of coordinate-wise perturbed step-sizes for the first layer weights to break the symmetry between the coordinates. This allows us to learn even some degenerate cases having coordinate symmetries with leap one, in contrast to Abbe et al. [2022]. ", "page_idx": 25}, {"type": "text", "text": "Discrete Dimension-free Dynamics. We first consider the dimension-free dynamics similar to (DF-PDE) but for discrete step-size regime (see [Abbe et al., 2022, Appendix C]). We initialize to ", "page_idx": 25}, {"type": "equation", "text": "$$\n(a^{0},b^{0},{\\pmb u}^{0},c^{0},s^{0})\\sim\\bar{\\rho}_{0}\\in\\mathcal{P}(\\mathbb{R}^{P+4}),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and $(\\bar{\\rho}_{k})_{k\\in\\mathbb{N}}$ are induced distribution on $(a^{0},b^{0},{\\pmb u}^{0},c^{0},s^{0})$ recursively defined as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a^{k+1}=a^{k}-\\eta_{k}^{a}\\left(\\mathbb{E}_{z,G}[\\ell^{\\prime}(\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}_{k}),y)\\sigma(\\langle u^{k},z\\rangle+s^{k}G+b^{k})]+\\lambda^{a}a^{k}\\right)}\\\\ &{u^{k+1}=u^{k}-\\eta_{k}^{u}\\circ\\left(\\mathbb{E}_{z,G}[\\ell^{\\prime}(\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}_{k}),y)a^{k}\\sigma^{\\prime}(\\langle u^{k},z\\rangle+s^{k}G+b^{k})z]+\\lambda^{w}u^{k}\\right)}\\\\ &{s^{k+1}=s^{k}-\\eta_{k}^{w}\\left(\\mathbb{E}_{z,G}[\\ell^{\\prime}(\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}_{k}),y)a^{k}\\sigma^{\\prime}(\\langle u^{k},z\\rangle+s^{k}G+b^{k})G]+\\lambda^{w}s^{k}\\right)\\quad(\\mathrm{d-DF-PDE})}\\\\ &{b^{k+1}=b^{k}-\\eta_{k}^{b}\\left(\\mathbb{E}_{z,G}[\\ell^{\\prime}(\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}_{k}),y)a^{k}\\sigma^{\\prime}(\\langle u^{k},z\\rangle+s^{k}G+b^{k})]+\\lambda^{b}b^{k}\\right)}\\\\ &{c^{k+1}=c^{k}-\\eta_{k}^{c}\\left(\\mathbb{E}_{z,G}[\\ell^{\\prime}(\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}_{k}),y)]+\\lambda^{c}c^{k}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "E.1 Training Algorithm ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Choose a loss $\\ell:\\mathbb{R}\\times\\mathcal{Y}\\rightarrow\\mathbb{R}_{\\ge0}$ and the activation $\\sigma(x)=(1+x)^{L}$ for $L\\geq2^{8P}$ . We train for $\\Theta(1)$ total steps in two phases and the batch size of $\\Theta(d)$ with the following choice of hyperparameters. ", "page_idx": 25}, {"type": "text", "text": "\u2022 No regularization: set the regularization parameters $\\lambda^{w}=\\lambda^{a}=\\lambda^{b}=\\lambda^{c}=0$ . ", "page_idx": 25}, {"type": "text", "text": "\u2022 Initialization: Initialize the first layer weights and biases to deterministically 0. ", "page_idx": 26}, {"type": "equation", "text": "$$\n(b_{j}^{0},\\sqrt{d}{\\pmb w}_{j}^{0})\\stackrel{\\mathrm{i.i.d.}}{\\sim}\\mu_{b}\\otimes\\mu_{w}^{\\otimes d}\\equiv\\delta_{b=0}\\otimes\\delta_{w=0}^{\\otimes d}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The second layer weights are sampled uniformly from $[-1,1]$ , e.g. $a_{j}\\stackrel{\\mathrm{~\\tiny~{i.i.d.}~}}{\\sim}\\mu_{a}\\;\\;\\equiv$   \n$\\mathrm{Unif}([-1,1])$ . Finally, choose $c^{0}\\sim\\delta_{c=\\bar{c}}$ for the given global bias choice $\\bar{c}\\in\\mathbb R$ . ", "page_idx": 26}, {"type": "text", "text": "\u2022 For the dimension-free dynamics, this corresponds to taking ", "page_idx": 26}, {"type": "equation", "text": "$$\n(a^{0},b^{0},\\pmb{u}^{0},c^{0},s^{0})\\sim\\bar{\\rho}^{0}\\equiv\\mathrm{Unif}([-1,1])\\otimes\\delta_{b=0}\\otimes\\delta_{w=0}^{\\otimes P}\\otimes\\delta_{c=\\bar{c}}\\otimes\\delta_{s=0}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "\u2022 We deploy a two-phase training procedure. Given parameters $\\eta$ and $\\kappa\\in[1/2,3/2]^{d}$ : ", "page_idx": 26}, {"type": "text", "text": "1. Phase 1: For all $k\\in\\{0,\\ldots,k_{1}-1\\}$ , set $\\eta_{k}^{a}=\\eta_{k}^{b}=\\eta_{k}^{c}=0$ and $\\eta_{k}^{w}\\in\\mathbb{R}^{d}$ such that $\\eta_{k}^{w_{i}}=\\eta\\kappa_{i}$ for all $i\\in[d]$ . For the dimension-free dynamics, train the first layer weights $\\pmb{u}^{k}$ for $k_{1}$ steps, while keeping other parameters fixed, i.e $a^{k}=a^{0},b^{k}=0,c^{k}=\\bar{c}$ . 2. Phase 2: Set $\\eta_{k}^{a}=\\eta$ and $\\pmb{\\eta}_{k}^{w}=\\mathbf{0},\\eta_{k}^{b}=\\eta_{k}^{c}=0$ for $k\\in\\{k_{1},\\ldots,k_{1}+k_{2}-1\\}$ . In words, train the second layer weights $a^{k}$ for $k_{2}$ steps, while keeping the first layer weights fixed at $\\pmb{u}^{k}=\\pmb{u}^{k_{1}}$ . ", "page_idx": 26}, {"type": "text", "text": "We will take $\\eta>0$ to be a small constant to be specified layer and $\\kappa_{i}\\,\\in\\,\\mathrm{Unif}([1/2,3/2])$ be the random perturbation. We also let $b=\\Omega(d)$ , hiding constants in $\\eta,\\varepsilon,P,K,\\mu$ . For the first phase, it suffices to train for $k_{1}=P$ time steps. For the second phase, we train for $k_{2}=k_{2}(\\eta,\\varepsilon,P,K,\\mu)=$ $\\Theta(1)$ time steps to be specified later as we analyze. Note that only unspecified hyperparameter is the global bias initialization $\\bar{c}$ and the results holds almost surely over this choice. ", "page_idx": 26}, {"type": "text", "text": "Theorem E.1 (Formal statement of Theorem 7.2). Assume $\\ell$ is analytic, convex and that Assumption D.4 holds. Then for any ${\\mathsf{L e a p}}_{\\mathsf{D L Q}_{\\ell}}(\\mu)\\,=\\,1$ setting, for any $\\mathcal{D}_{\\mu,s_{*}}^{d}\\in\\mathcal{H}_{\\mu}^{d}$ , almost surely over the initialization $\\bar{c}\\in\\mathbb R$ , and $\\kappa\\in[1/2,3/2]^{d}$ , the following holds. For any $\\varepsilon>0$ , with ", "page_idx": 26}, {"type": "equation", "text": "$$\nb\\geq\\Omega(d),M=\\Omega(1),\\,a n d\\,k_{2}=\\Omega(1),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "using $\\Omega(\\cdot)$ to hide constants in $\\varepsilon,K,P,\\mu,\\eta,\\prime$ \u03ba and $\\bar{c},$ , the above specified layer-wise training dynamics reaches $\\overline{{\\mathcal{R}}}(f_{\\mathsf{N N}}(\\cdot;\\Theta^{k_{1}+k_{2}}))\\leq\\varepsilon$ with probability at least $9/10$ . ", "page_idx": 26}, {"type": "text", "text": "E.2 Proof of Theorem E.1 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "The proof of the theorem proceeds by first showing a non-asymptotic approximation guarantee between (d-DF-PDE) and (\u2113-bSGD-g). To this end, we first start by noting a generalization of [Abbe et al., 2022, Theorem 23] to any loss under satisfying Assumption D.4. ", "page_idx": 26}, {"type": "text", "text": "Proposition E.2. Assume that Assumption $D.4$ on the loss holds. Then for any $0<\\eta\\le K$ and for any $\\Upsilon\\in\\mathbb{N}$ , there exists a constant $C(K,\\Upsilon)$ such that for (\u2113-bSGD-g) and (d-DF-PDE) of the specified layer-wise training dynamics, with probability $1-1/M$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{=0,\\ldots,\\Upsilon}\\left|\\mathcal{R}(\\hat{f}_{\\mathsf{N N}}(\\cdot;\\Theta^{k}))-\\mathcal{R}(\\hat{f}_{\\mathsf{N N}}(\\cdot;\\bar{\\rho}_{k}))\\right|\\leq C(K,\\Upsilon)\\left(\\sqrt{\\frac{P+\\log d}{d}}+\\sqrt{\\frac{\\log M}{M}}+\\sqrt{\\frac{d+\\log M}{b}}\\right)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The proof of the proposition follows the similar arguments like propagation of chaos used in Theorem D.5. A direct corollary of this proposition is that it suffices to focus only on the (d-DF-PDE) to show Theorem E.1. ", "page_idx": 26}, {"type": "text", "text": "Corollary E.3. Assume that Assumption D.4 holds. Then for total $\\Upsilon$ number of (\u2113-bSGD-g) $i t$ - erations on a neural network with $\\bar{M}\\geq C_{0}(K,\\Upsilon,\\varepsilon)$ and the batch-size $b\\geq C_{1}(K,\\Upsilon,\\varepsilon)\\cdot d,$ the aforementioned layer-wise training dynamics, with probability at least $9/10$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{k=0,\\dots,\\Upsilon}\\left|\\mathcal{R}(\\hat{f}_{\\sf N N}(\\cdot;\\Theta^{k}))-\\mathcal{R}(\\hat{f}_{\\sf N N}(\\cdot;\\bar{\\rho}_{k}))\\right|\\leq\\frac{\\varepsilon}{2}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. It is easy to see that it is possible to choose $M\\geq C_{0}(K,\\Upsilon,\\varepsilon)$ and $b\\geq C_{1}(K,\\Upsilon,\\varepsilon)d$ for sufficiently large constants such that the right hand side of equality in Proposition E.2 is bounded by $\\varepsilon/2$ and $\\dot{1/M}\\,\\dot{\\leq}\\,1/10$ . The corollary then follows from Proposition E.2. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "In light of Corollary E.3, it suffices to simply focus on (d-DF-PDE) and show that it achieves the excess error of at most $\\varepsilon/2$ within total steps $\\Upsilon=O(1)$ hiding constants in $K,\\mu,P,\\varepsilon$ . This will be done by analyzing each phase separately\u2014 $(i)$ showing that the training of the top layer weights for steps $k\\in\\{k_{1},\\ldots,k_{1}+k_{2}-1\\}$ in Phase 2 is a linear model trained with a convex loss so it succeeds in reaching a vanishing risk as long as the corresponding kernel matrix of the feature map is non-degenerate; $(i i)$ indeed showing that the corresponding kernel matrix of the feature map after $k_{1}=P$ steps of Phase 1 training on the first layer weights is non-degenerate. ", "page_idx": 27}, {"type": "text", "text": "Simplified dynamics. We first make a simple observation that $b^{k}\\;=\\;0,s^{k}\\;=\\;0$ and $c^{k}\\ =\\ \\bar{c}$ throughout; this allows us to analyze a simpler dynamics. To see this, for $(b^{k},c^{k})$ , we use $\\eta^{c}=\\eta^{b}=0$ throughout, and thus, they remain the same from the initialization. We start with $s^{0}=0$ and using (d-DF-PDE) (assuming $s^{\\check{k}}=0$ ), gives us ", "page_idx": 27}, {"type": "equation", "text": "$$\ns^{k+1}=0-\\eta_{k}^{w}\\left(\\mathbb{E}_{z,G}[\\ell^{\\prime}(\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}_{k}),y)a^{k}\\sigma^{\\prime}(\\langle u^{k},z\\rangle+b^{k})G]+\\lambda^{w}0\\right)=0.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "E.3 Phase 2 (linear training) ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We first analyze a slightly simpler Phase 2 of training. For a convex loss $\\ell$ , the second layer training just corresponds to linear dynamics with kernel $\\begin{array}{r}{K^{k_{1}}:\\{+1,-1\\}^{P}\\times\\{+1,-1\\}^{P}\\to\\mathbb{R}}\\end{array}$ given by ", "page_idx": 27}, {"type": "equation", "text": "$$\n{\\cal K}^{k_{1}}(z,z^{\\prime})=\\mathbb{E}_{a\\sim\\mu_{a}}[\\sigma(\\langle{\\pmb u}^{k_{1}}(a),z\\rangle)\\sigma(\\langle{\\pmb u}^{k_{1}}(a),z^{\\prime}\\rangle)].\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We first show some helper lemmas, and using them, show the main lemma. We will then show the proof of the helper lemmas. We start by proving the existence of a sparse function with a small excess error. ", "page_idx": 27}, {"type": "text", "text": "Lemma E.4. For any $\\varepsilon>0,$ , there exists a function $f_{*}:\\{\\pm1\\}^{d}\\rightarrow\\mathbb{R}$ such that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\overline{{\\mathcal{R}}}(f_{*})=\\left[\\mathcal{R}(f_{*})-\\operatorname*{inf}_{\\bar{f}:\\{\\pm1\\}^{d}\\rightarrow\\mathbb{R}}\\mathcal{R}(\\bar{f})\\right]\\leq\\frac{\\varepsilon}{4}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Moreover, $f_{*}({\\pmb x})=h_{*}({\\pmb z})$ for some $h_{*}:\\{\\pm1\\}^{P}\\rightarrow\\mathbb{R}$ indpendent of $d_{\\cdot}$ , where $_{\\textit{z}}$ is the support of $\\mathcal{D}_{s_{*}}^{d}$ ", "page_idx": 27}, {"type": "text", "text": "Approximation. The neural network output after training $\\textbf{\\em u}$ for $k_{1}$ steps is given by: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}_{k_{1}})=\\bar{c}+\\int a\\sigma(\\langle{u^{k_{1}}(a)},z\\rangle)\\mathrm{d}\\mu_{a}=\\bar{c}+\\mathbb{E}_{a\\sim\\mu_{a}}\\left[a\\sigma(\\langle{u^{k_{1}}(a)},z\\rangle)\\right].\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Observing that for the next $k_{2}$ steps, only $a$ is trained while keeping $\\pmb{u}^{k_{1}}$ fixed, the neural network output is of the following form ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho})=\\bar{c}+\\int a\\sigma(\\langle{\\pmb u},{z}\\rangle)\\mathrm{d}\\bar{\\rho}(a,{\\pmb u})=\\bar{c}+\\int a\\sigma(\\langle{\\pmb u},{z}\\rangle)\\mathrm{d}\\bar{\\rho}(a({\\pmb u}))\\mathrm{d}\\bar{\\rho}_{k_{1}}({\\pmb u}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "now start by showing that our neural network, there exists $\\bar{\\rho}_{\\ast}$ such that we can exactly represent $h_{*}(z)$ as long as the resultant (kernel) after $1^{\\mathrm{st}}$ layer training is non-degenerate. ", "page_idx": 27}, {"type": "text", "text": "Lemma E.5. If $\\lambda_{\\operatorname*{min}}(K^{k_{1}})\\geq c$ for some constant $c,$ , then there exists $\\bar{\\rho}_{*}\\,\\in\\,\\mathcal{P}(\\mathbb{R}^{P+1})$ such that $\\mathrm{d}\\bar{\\rho}_{*}(a,\\pmb{u})=\\mathrm{d}\\bar{\\rho}(a_{*}(\\pmb{u}))\\mathrm{d}\\bar{\\rho}_{k_{1}}(\\pmb{u})$ and for all $z\\in\\{\\pm1\\}^{P}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\nh_{*}(z)=\\hat{f}_{\\sf N N}(z;\\bar{\\rho}_{*})=\\bar{c}+\\int a_{*}\\sigma(\\langle{\\pmb u},{z}\\rangle)\\mathrm{d}\\bar{\\rho}_{*}(a_{*}({\\pmb u}))\\mathrm{d}\\bar{\\rho}_{k_{1}}({\\pmb u}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Moreover, $\\begin{array}{r}{\\|a^{*}\\|_{\\bar{\\rho}_{*}}^{2}=\\int a_{*}^{2}(\\pmb{u})\\mathrm{d}\\bar{\\rho}_{*}(a_{*}(\\pmb{u}))\\leq c_{1}}\\end{array}$ for some constant $c_{1}:=c_{1}(\\lambda_{\\operatorname*{min}}(K^{k_{1}}),h_{*},\\bar{c}).$ . ", "page_idx": 27}, {"type": "text", "text": "Convexity and Smoothness. Finally, we will show that the risk objective in terms of $\\bar{\\rho}$ (of the form (75)) is convex and smooth in terms of $\\bar{\\rho}$ . The convexity follows from the convexity of $\\ell(\\cdot,y)$ and observing that the neural network output in (75) is linear in $\\bar{\\rho}(a({\\pmb u}))$ . The smoothness follows from Assumption D.4. ", "page_idx": 27}, {"type": "text", "text": "Lemma E.6. Consider $\\ell:\\mathbb{R}\\times\\mathcal{Y}\\rightarrow\\mathbb{R}_{\\ge0}$ such that $\\ell(\\cdot,y)$ is convex for every $y\\in\\mathcal{V}$ and that Assumption $D$ .4 holds. Then ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{R}(\\bar{\\rho})=\\mathbb{E}[\\ell(\\hat{f}_{\\sf N N}(z;\\bar{\\rho}),y)],\\quad w h e r e\\:\\mathrm{d}\\bar{\\rho}(a,{\\bf u})=\\mathrm{d}\\bar{\\rho}(a({\\bf u}))\\mathrm{d}\\bar{\\rho}_{k_{1}}({\\bf u}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "is convex and $H$ -smooth with $H=K^{3}$ . ", "page_idx": 28}, {"type": "text", "text": "While we denote $\\mathcal{R}(\\bar{\\rho})$ as an objective in terms of the joint measure $\\bar{\\rho}(a,\\boldsymbol{u})$ , it is actually an objective in $\\bar{\\rho}(a(\\pmb{u})))$ since it is of the form where $\\mathrm{d}(a,\\pmb{u})\\,\\stackrel{.}{=}\\,\\mathrm{d}\\bar{\\rho}(a(\\pmb{u}))\\mathrm{d}\\bar{\\rho}_{k_{1}}(\\pmb{u})$ . Also, the convexity and smoothness is in terms of the argument $\\bar{\\rho}(a(\\bar{\\pmb u}))$ . Having established the convexity and smoothness of the $2^{\\mathrm{nd}}$ layer weights training, we will directly apply the convergence rate guarantees for convex and smooth objectives. ", "page_idx": 28}, {"type": "text", "text": "Lemma E.7. Consider any convex, differentiable objective $f(x)$ that is $H$ -smooth. Then for any step-size $\\begin{array}{r}{\\eta\\le\\frac{1}{H}}\\end{array}$ , the gradient descent from initialization $x^{0}$ reaches an iterate $x^{k}$ after $k$ steps such that for any $\\tilde{x}$ (not necessarily a minimizer), we have ", "page_idx": 28}, {"type": "equation", "text": "$$\nf(x^{T})-f(\\tilde{x})\\leq\\frac{\\|x^{0}-\\tilde{x}\\|^{2}}{2\\eta k}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "This is a classical convergence rate guarantee for gradient descent on convex smooth objective [Bubeck et al., 2015, Theorem 3.3], but noting that a proof generalizes even when $\\tilde{x}$ is not necessarily a minimizer. We will apply this guarantees with $\\tilde{x}$ as a near minimizer of $f$ , especially when the actual minimizer may not exist (for example with the logistic loss, the infimum is never achieved). We provide the proof with other lemmas for completeness. ", "page_idx": 28}, {"type": "text", "text": "Lemma E.8. Assume that $\\lambda_{\\operatorname*{min}}(K^{k_{1}})\\,>\\,c>\\,0$ for some constant $c_{:}$ , where $\\pmb{K}^{k_{1}}$ is the (kernel) matrix. Then discrete dimension-free dynamics (d-DF-PDE) in Phase 2 of layer-wise training with step-size $\\begin{array}{r}{\\eta\\le\\frac{1}{K^{3}}}\\end{array}$ as specified before after $k_{2}:=k_{2}(\\eta,\\varepsilon,P,K,\\mu,\\bar{c})$ steps achieves ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\overline{{\\mathcal{R}}}(\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}^{k_{1}+k_{2}}))=\\bigg[\\mathcal{R}(\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}^{k_{1}+k_{2}}))-\\operatorname*{inf}_{\\bar{f}:\\{\\pm1\\}^{d}\\to\\mathbb{R}}\\mathcal{R}(\\bar{f})\\bigg]\\leq\\frac{\\varepsilon}{2}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. First of all, using Lemma E.4, there exists $\\bar{\\rho}_{*}$ such that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\overline{{\\mathcal{R}}}(\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}_{*}))=\\mathcal{R}(\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}_{*}))-\\operatorname*{inf}_{\\bar{f}:\\{\\pm1\\}^{d}\\rightarrow\\mathbb{R}}\\mathcal{R}(\\bar{f})\\leq\\frac{\\varepsilon}{4}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Moreover, by Lemma E.6, the objective $\\mathcal{R}(\\bar{\\rho})$ is convex and $K^{3}$ -smooth, and observe that (d-DF-PDE) are performing gradient descent with step-size $\\begin{array}{r}{\\eta_{k}^{a}=\\eta\\le\\frac{1}{K^{3}}}\\end{array}$ on this objective. Thus, using Lemma E.7 ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\Big[\\mathcal{R}(\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}^{k_{1}+k}))-\\mathcal{R}(\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}_{*}))\\Big]:=\\big[\\mathcal{R}(\\bar{\\rho}^{k_{1}+k})-\\mathcal{R}(\\bar{\\rho}_{*})\\big]\\leq\\frac{\\|a^{k_{1}}-a^{*}\\|_{\\bar{\\rho}_{k_{1},\\bar{\\rho}_{*}}}^{2}}{2\\eta k}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Recall that $a^{k_{1}}\\sim\\mu_{a}\\equiv\\mathrm{Unif}([-1,1])$ . ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|a^{k_{1}}-a^{*}\\|_{\\bar{\\rho}_{k_{1}},\\bar{\\rho}_{*}}^{2}=\\int(a-a^{*}(u))^{2}\\mathrm{d}\\mu_{a}\\mathrm{d}\\bar{\\rho}_{*}(a^{*}(u))\\mathrm{d}\\bar{\\rho}_{k_{1}}(u)\\leq1+2\\|a^{*}\\|_{\\bar{\\rho}_{*}}+\\|a^{*}\\|_{\\bar{\\rho}_{*}}^{2}\\leq c_{2},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "for some constant $c_{2}:=c_{2}\\big(\\lambda_{\\operatorname*{min}}(K^{k_{1}}),P,\\mu,\\bar{c}\\big)$ . Therefore, choosing $\\begin{array}{r}{k_{2}\\geq\\frac{2c_{2}}{\\eta\\varepsilon}}\\end{array}$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left[\\mathcal{R}(\\hat{f}_{\\sf N N}(z;\\bar{\\rho}^{k_{1}+k_{2}}))-\\mathcal{R}(\\hat{f}_{\\sf N N}(z;\\bar{\\rho}_{*}))\\right]\\leq\\frac{\\varepsilon}{4}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Combining (78) and (76) ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{\\mathscr{R}}}\\big(\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}^{k_{1}+k_{2}})\\big)=\\bigg[\\mathcal{R}\\big(\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}^{k_{1}+k_{2}})\\big)-\\mathcal{R}\\big(\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}_{*})\\big)\\bigg]}\\\\ &{\\qquad\\qquad\\qquad+\\left[\\mathcal{R}(\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}_{*}))-\\operatorname*{inf}_{\\bar{f}:\\{\\pm1\\}^{d}\\to\\mathbb{R}}\\mathcal{R}(\\bar{f})\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{\\varepsilon}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Thus, if we can show that $\\lambda_{\\operatorname*{min}}(K^{k_{1}})\\geq c>0$ for some constant $c:=c(\\eta,P,K,\\mu)$ for a sufficiently small constant $\\eta>0$ that only depends on $K,P,\\mu$ , then it immediately implies the desired result by Corollary E.3 with $\\Upsilon=k_{1}+k_{2}$ . The goal of Appendix E.4 is to show that this holds after $k_{1}=P$ steps of the first layer weight training. We now return to the deferred proofs of helper lemmas. ", "page_idx": 28}, {"type": "text", "text": "E.3.1 Proof of helper lemmas ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Proof of Lemma E.4. Note that Eq. (74) simply follows from the definition of infimum and the excess risk functional $\\overline{{\\mathcal{R}}}(\\cdot)$ . To show that $f_{*}({\\pmb x})=h_{*}({\\pmb z})$ , we make an observation that for any function $f:\\{\\pm1\\}^{d}\\rightarrow\\mathbb{R}$ , one can define $h_{*}:\\{\\pm1\\}^{P}\\rightarrow\\mathbb{R}$ that only depend on the support and achieve risk no worse than $f_{*}$ . Define $h_{*}(z)=\\mathbb{E}_{z^{c}}[f_{*}(x)]$ , where ${\\pmb z}^{c}={\\pmb x}\\setminus{\\pmb z}$ (the coordinates outside the support). In other words, $h_{*}$ is the boolean function after ignoring the monominals that do not depend on $_{z}$ . Then ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathcal R}(f_{*})={\\mathbb E}_{(y,x)\\sim{\\mathcal D}_{s_{*}}^{d}}\\left[\\ell(f_{*}(x),y)\\right]={\\mathbb E}_{\\mu_{y,z}}[{\\mathbb E}_{\\mu_{z^{c}|y,z}}[\\ell(f_{*}(x),y)]]}\\\\ &{\\qquad\\qquad\\geq{\\mathbb E}_{\\mu_{y,z}}[\\ell({\\mathbb E}_{\\mu_{z^{c}|y,z}}[f_{*}(x)],y)]={\\mathbb E}[\\ell(h_{*}(z),y)]={\\mathcal R}(h),}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the only inequality follows from Jensen\u2019s inequality and convexity of $\\ell(\\cdot,y)$ . ", "page_idx": 29}, {"type": "text", "text": "Proof of Lemma $E.5$ . Define $g_{*}(z)=h_{*}(z)-\\bar{c}$ . Our goal is to show the existence of $\\bar{\\rho}_{*}$ such that $g_{*}(z)=\\mathbb{E}_{(a_{*},\\pmb{u})\\sim\\bar{\\rho}_{*}}[a_{*}\\sigma(\\langle\\pmb{u},z\\rangle)]$ .   \nConsider the following objective least square objective for the domain ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathcal{L}(a(\\pmb{u});\\lambda)=\\sum_{z\\in\\{\\pm1\\}^{P}}\\left(\\mathbb{E}_{\\pmb{u}\\sim\\tilde{\\rho}_{k_{1}}(\\pmb{u})}[a(\\pmb{u})\\sigma(\\langle\\pmb{u},z\\rangle)]-g_{*}(z)\\right)^{2}+\\lambda\\int a^{2}(\\pmb{u})\\mathrm{d}\\bar{\\rho}_{k_{1}}(\\pmb{u})\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "While this is an infinite dimensional problem, the Representer\u2019s theorem holds and the interpolating solution exists. We refer the reader to [Celentano et al., 2021] for a detailed analysis of interpolation with the random feature model. Moreover, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\|a^{*}\\|_{\\bar{\\rho}_{*}}\\leq\\lambda_{\\operatorname*{min}}(\\pmb{K}^{k_{1}})^{-1}\\sqrt{\\sum_{z\\in\\{\\pm1\\}^{P}}g_{*}(z)^{2}}\\leq\\ c_{1},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "for some constant $c_{1}$ that only depends on $\\lambda_{\\operatorname*{min}}(K^{k_{1}}),h_{*},P$ and $\\bar{c}$ . ", "page_idx": 29}, {"type": "text", "text": "Proof of Lemma E.6. For $t\\in[0,1]$ and any $\\bar{\\rho}^{(1)}$ and $\\bar{\\rho}^{(2)}$ , consider the density $\\bar{\\rho}=t\\bar{\\rho}^{(1)}\\!+\\!(1\\!-\\!t)\\bar{\\rho}^{(2)}$ . Then ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\mathcal{R}(\\bar{\\rho})=\\mathcal{R}(t\\bar{\\rho}^{(1)}+(1-t)\\bar{\\rho}^{(2)})=\\mathbb{E}[\\ell(\\hat{f}_{\\mathsf{N N}}(z;t\\bar{\\rho}^{(1)}+(1-t)\\bar{\\rho}^{(2)}),y)]}&\\\\ &{\\qquad=\\mathbb{E}[\\ell(t\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}^{(1)})+(1-t)\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}^{(2)}),y)]}&\\\\ &{\\qquad\\leq\\mathbb{E}[t\\ell(\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}^{(1)}),y)+(1-t)\\ell(\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}^{(2)}),y)]}&{(\\mathrm{using~convexity}}\\\\ &{\\qquad=t\\mathbb{E}\\left[\\ell(\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}^{(1)}),y)\\right]+(1-t)\\mathbb{E}\\left[\\ell(\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}^{(2)}),y)\\right]=t\\mathcal{R}(\\bar{\\rho}^{(1)})+(1-t)\\mathcal{R}(\\bar{\\rho}^{(2)}).}&\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "To show smoothness, first observe that a direct consequence of Assumption D.4 (the condition $|\\ell^{\\prime\\prime}(u,y)|\\le K)$ is that for any $u_{1},u_{2}\\in\\mathbb{R}$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\ell(u_{1},y)\\leq\\ell(u_{2},y)+\\ell^{\\prime}(u_{2},y)(u_{1}-u_{2})+\\frac{K}{2}(u_{2}-u_{1})^{2}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Using this, for any $\\bar{\\rho}^{(1)}$ and $\\bar{\\rho}^{(2)}$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathcal{R}\\big(\\bar{\\rho}^{(1)}\\big)=\\mathbb{E}\\big[\\ell(\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}^{(1)}),y)\\big]}}\\\\ &{}&{\\leq\\mathbb{E}\\big[\\ell(\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}^{(2)}),y)\\big]+\\mathbb{E}\\left[\\ell^{\\prime}(\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}^{(2)}),y)\\left(\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}^{(1)})-\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}^{(2)})\\right)\\right]}\\\\ &{}&{\\quad{\\mathrm{(using~o~}}}\\\\ &{}&{\\qquad+\\,\\frac{K}{2}\\mathbb{E}\\left[\\Big(\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}^{(1)})-\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}^{(2)})\\Big)^{2}\\right]}\\\\ &{}&{\\displaystyle\\mathrm{e}\\ \\ \\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}^{(1)})-\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}^{(2)})=\\mathbb{E}_{a^{(1)},a^{(2)},u}\\left[(a^{(1)}-a^{(2)})\\sigma(\\langle u,z\\rangle)\\right],\\,\\mathrm{further~simplifying~}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Sinc the above ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\ell^{\\prime}\\big(\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}^{(2)}),y\\big)\\left(\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}^{(1)})-\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}^{(2)})\\right)\\right]}\\\\ &{=\\mathbb{E}_{(y,z)\\sim\\mu_{y,z}}\\left[\\ell^{\\prime}\\big(\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}^{(2)}),y)\\mathbb{E}_{a^{(1)},a^{(2)},u}\\left[(a^{(1)}-a^{(2)})\\sigma(\\langle u,z\\rangle)\\right]\\right]}\\\\ &{=\\mathbb{E}_{a^{(1)},a^{(2)},u}\\left[\\mathbb{E}_{(y,z)\\sim\\mu_{y,z}}\\left[\\ell^{\\prime}\\big(\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}^{(2)}),y)\\sigma(\\langle u,z\\rangle)\\right](a^{(1)}-a^{(2)})\\right]}\\\\ &{=\\Big\\langle\\nabla_{a}\\mathcal{R}(\\bar{\\rho}^{(2)}),a^{(1)}-a^{(2)}\\Big\\rangle_{\\bar{\\rho}^{(1)},\\bar{\\rho}^{(2)}}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Also, simplifying the third term of (80) ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{K}{2}\\mathbb{E}\\left[\\left(\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}^{(1)})-\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}^{(2)})\\right)^{2}\\right]=\\frac{K}{2}\\mathbb{E}_{(y,z)\\sim\\mu_{y,z}}\\left[\\mathbb{E}_{a^{(1)},a^{(2)},u}\\left[(a^{(1)}-a^{(2)})\\sigma(\\langle u,z\\rangle)\\right]^{2}\\right]}}\\\\ &{}&{\\leq\\frac{K}{2}\\mathbb{E}_{(y,z)\\sim\\mu_{y,z}}\\mathbb{E}_{a^{(1)},a^{(2)},u}\\left[(a^{(1)}-a^{(2)})^{2}\\sigma^{2}(\\langle u,z\\rangle)\\right]}{(\\mathrm{by~Jensen}^{\\ast}\\operatorname*{ineqaulity})}}\\\\ &{}&{\\leq\\frac{K^{3}}{2}\\|a^{(1)}-a^{(2)}\\|_{\\bar{\\rho}^{(1)},\\bar{\\rho}^{(2)}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Finally, combining (81) and (82) with (80), we obtain ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathcal{R}(\\bar{\\rho}^{(1)})=\\mathcal{R}(\\bar{\\rho}^{(2)})+\\left\\langle\\nabla_{a}\\mathcal{R}(\\bar{\\rho}^{(2)}),a^{(1)}-a^{(2)}\\right\\rangle_{\\bar{\\rho}^{(1)},\\bar{\\rho}^{(2)}}+\\frac{K^{3}}{2}\\|a^{(1)}-a^{(2)}\\|_{\\bar{\\rho}^{(1)},\\bar{\\rho}^{(2)}}^{2},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which, by definition, gives us $H$ -smoothness with $H=K^{3}$ . ", "page_idx": 30}, {"type": "text", "text": "Proof of Lemma E.7. First, the classical descent lemma holds due to $H$ -smoothness for step-size \u03b7 \u2264H . ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{f({x}^{j+1})\\le f({x}^{j})+\\langle\\nabla f({x}^{j}),{x}^{j+1}-{x}^{j}\\rangle+\\displaystyle\\frac{H}{2}\\|{x}^{j+1}-{x}^{j}\\|_{2}^{2}}\\\\ {\\quad\\quad\\quad=f({x}^{j})-\\eta\\left(1-\\frac{\\eta H}{2}\\right)\\|\\nabla f({x}^{j})\\|_{2}^{2}\\le f({x}^{j})-\\displaystyle\\frac{\\eta}{2}\\|\\nabla f({x}^{j})\\|_{2}^{2}\\le f({x}^{j}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By convexity, for any $\\tilde{x}$ (not necessarily a minimizer), we also have ", "page_idx": 30}, {"type": "equation", "text": "$$\nf(x^{j})\\leq f(\\tilde{x})+\\langle\\nabla f(x^{j}),x^{j}-\\tilde{x}\\rangle.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Substituting this in (83) ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(x^{j+1})\\leq f(\\tilde{x})+\\langle\\nabla f(x^{j}),x^{j}-\\tilde{x}\\rangle-\\displaystyle\\frac{\\eta}{2}\\|\\nabla f(x^{j})\\|_{2}^{2}}\\\\ &{\\qquad\\quad=f(\\tilde{x})+\\displaystyle\\frac{1}{2\\eta}\\left(2\\eta\\langle\\nabla f(x^{j}),x^{j}-\\tilde{x}\\rangle-\\eta^{2}\\|\\nabla f(x^{j})\\|_{2}^{2}\\right)}\\\\ &{\\qquad=f(\\tilde{x})+\\displaystyle\\frac{1}{2\\eta}\\left(2\\eta\\langle\\nabla f(x^{j}),x^{j}-\\tilde{x}\\rangle-\\eta^{2}\\|\\nabla f(x^{j})\\|_{2}^{2}-\\|x^{j}-\\tilde{x}\\|_{2}^{2}+\\|x^{j}-\\tilde{x}\\|_{2}^{2}\\right)}\\\\ &{\\qquad=f(\\tilde{x})+\\displaystyle\\frac{1}{2\\eta}\\left(\\|x^{j}-\\tilde{x}\\|_{2}^{2}-\\|x^{j+1}-\\tilde{x}^{2}\\|_{2}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Therefore, summing over $j\\in\\{0,\\ldots,k-1\\}$ , since the right hand side is a telescopic sum ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{j=0}^{k-1}\\big(f(x^{j+1})-f(\\widetilde{x})\\big)\\leq\\frac{1}{2\\eta}\\left(\\|x^{0}-\\widetilde{x}\\|_{2}^{2}-\\|x^{k}-\\widetilde{x}\\|_{2}^{2}\\right)\\leq\\frac{\\|x^{0}-\\widetilde{x}\\|_{2}^{2}}{2\\eta}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Note that the lemma statements trivially holds if $f(x^{k})-f(\\tilde{x})\\leq0$ already. Therefore, in the case when $f(x^{k})>f({\\tilde{x}})$ and using the descent lemma, we finally conclude the proof. ", "page_idx": 30}, {"type": "equation", "text": "$$\nf(x^{k})-f(\\tilde{x})\\leq\\frac{1}{k}\\sum_{j=0}^{k-1}\\left(f(x^{j})-f(\\tilde{x})\\right)\\leq\\frac{\\|x^{0}-\\tilde{x}\\|^{2}}{2\\eta k}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "E.4 Phase 1 (non-linear training) ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We now analyze Phase 1 of the algorithm and show that $\\lambda_{\\operatorname*{min}}(K^{k_{1}})\\;\\geq\\;c$ , for a constant $c:=$ $c(\\eta,\\mu,P,K)$ where $\\eta$ is also a small constant (in terms of $\\mu,P,K)$ . ", "page_idx": 30}, {"type": "text", "text": "Writing the weight evolution with a polynomial. We will by considering a general polynomial activation $\\begin{array}{r}{\\sigma(x)=\\sum_{l=0}^{L}m_{l}x^{l}}\\end{array}$ of degree $L$ , whose coefficients are given by $\\pmb{m}=(m_{0},\\dots,m_{L})$ . ", "page_idx": 31}, {"type": "text", "text": "Lemma E.9. (Training dynamics given by a polynomial) Let $\\pmb{\\xi}=(\\xi_{S,k})_{S\\subseteq[P],0\\leq k\\leq k_{1}-1}\\in\\mathbb{R}^{2^{P}k_{1}}$ , $\\zeta\\in\\mathbb{R},$ , $\\pmb{\\rho}\\in\\mathbb{R}^{L+1},\\pmb{\\gamma}\\in\\mathbb{R}^{P}$ be variables. For each $i\\in[P]$ , define $p_{0,i}(\\zeta,\\pmb{\\xi},\\pmb{\\rho},\\gamma)\\equiv0$ . For each, $0\\leq k\\leq k_{1}-1$ , ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle p_{k+1,i}(\\zeta,\\xi,\\rho,\\gamma)=p_{k,i}(\\zeta,\\xi,\\rho,\\gamma)+\\zeta\\gamma_{i}\\rho_{1}\\xi_{\\{i\\},k}}\\ ~}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}+\\zeta\\gamma_{i}\\displaystyle\\sum_{r=1}^{L-1}\\frac{\\rho_{r+1}}{r!}\\sum_{{\\scriptstyle(i_{1},\\ldots,i_{r})\\in[P]^{r}}}^{\\xi_{i(\\phi i_{1}\\phi\\cdots\\phi i_{r},k})}\\prod_{l=1}^{r}p_{k,i}(\\zeta,\\xi,\\rho,\\gamma)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Then there is a constant $c:=c(k_{1},P,K)$ such that for any $0<\\eta<c$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{u_{i}^{k}(a)=p_{k,i}(\\eta a,\\beta,{\\pmb m},\\kappa),}}\\\\ {{\\beta=(\\beta_{S,k})_{S\\subseteq[P],0\\leq k\\leq k_{1}-1}\\ i s\\ g i v e n\\ b y\\ \\beta_{S,k}=-{\\mathbb E}[\\ell^{\\prime}(\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}_{k}),y)\\chi_{S}(z)].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The term $\\hat{f}_{\\mathsf{N N}}(\\cdot;\\bar{\\rho}_{k})$ evolves non-linearly, and difficult to analyze directly. However, for sufficiently small step size $\\eta$ , the interaction term $\\hat{f}_{\\mathsf{N N}}(\\cdot;\\bar{\\rho}_{k})$ is small, and we show that it can be ignored. Formally, we define the simplified dynamics $\\hat{\\pmb u}^{k}(a)$ for each $0\\leq k\\leq k_{1}$ by letting $\\hat{\\boldsymbol{u}}^{0}(a)=\\mathbf{0}$ and inductively setting for each $\\bar{k}\\in\\{0,\\bar{\\ldots},k_{1}-1\\}$ , ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{u}}^{k+1}(a)=\\hat{\\mathbf{u}}^{k}(a)-\\eta\\kappa\\circ\\mathbb{E}[\\ell^{\\prime}(\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}_{0}),y)\\cdot a\\cdot\\sigma^{\\prime}(\\langle\\hat{\\mathbf{u}}^{k}(a),z\\rangle)z].\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Using a similar argument, we may show: ", "page_idx": 31}, {"type": "text", "text": "Lemma E.10 (Simplified training dynamics are given by a polynomial). There is a constant $c>0$ depending only on $k_{1},P,K,$ , such that for any $0<\\eta<c,$ any $i\\in[P]$ and any $0\\leq k\\leq k_{1}$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\hat{u}_{i}^{k}(a)=p_{k,i}(\\eta a,\\alpha,m,\\kappa),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where we abuse notation (since $\\pmb{\\alpha}=(\\alpha_{S})_{S\\subseteq[P]}$ otherwise) and let $\\pmb{\\alpha}=(\\alpha_{S,k})_{S\\subseteq[P],0\\leq k\\leq k_{1}-1}$ be given by ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\alpha_{S,k}=\\alpha_{S}=\\mathbb{E}[-\\ell^{\\prime}(\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}_{0}),y)\\chi_{S}(z)]\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Reducing to analyzing simplified dynamics We lower-bound $\\lambda_{\\operatorname*{min}}(K^{k_{1}})$ in terms of the determinant of a certain random matrix. Let $\\zeta=[\\zeta_{1},\\dotsc,\\zeta_{2^{P}}]$ be a vector of $2^{P}$ variables. Define $M=M(\\zeta,\\xi,\\rho,\\gamma)\\in\\mathbb{R}^{2^{P}\\times2^{P}}$ to be the matrix indexed by $z\\in\\{+1,-1\\}^{P}$ and $j\\,\\in\\,[2^{P}]$ with entries ", "page_idx": 31}, {"type": "equation", "text": "$$\nM_{z,j}(\\zeta,\\xi,\\rho,\\gamma)=\\sum_{r=0}^{L}\\frac{\\rho_{r}}{r!}\\left(\\sum_{i=1}^{P}z_{i}p_{k,i}(\\zeta_{j},\\xi,\\rho,\\gamma)\\right)^{r}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "This matrix is motivated by the following fact: ", "page_idx": 31}, {"type": "text", "text": "Lemma E.11. There is a constant $c>0$ depending only on $k_{1},P,K$ , such that for any $0<\\eta<c,$ , and any $\\mathbf{\\boldsymbol{a}}=[a_{1},\\dots,a_{2^{P}}]\\in[-1,1]^{2^{P}}$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{M_{z,j}(\\eta\\mathbf{a},\\beta,m,\\kappa)=\\sigma(\\langle\\pmb{u}^{k_{1}}(a_{j}),z\\rangle)}\\\\ {M_{z,j}(\\eta\\mathbf{a},\\alpha,m,\\kappa)=\\sigma(\\langle\\hat{\\pmb{u}}^{k_{1}}(a_{j}),z\\rangle)}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Under this notation, using the exactly same analysis as in [Abbe et al., 2022, Lemma E.9], we can also show ", "page_idx": 31}, {"type": "text", "text": "Lemma E.12. There is a constant $c>0$ depending on $K,P$ such that for any $0<\\eta<c,$ , ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{min}}(K^{k_{1}})\\geq c\\mathbb{E}_{\\mathbf{a}\\sim\\mu_{a}^{\\otimes2^{P}}}[\\operatorname*{det}(M(\\eta\\mathbf{a},\\beta,m,\\kappa))^{2}].\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "On the other hand, we can prove a lower-bound on $\\mathbb{E}[\\operatorname*{det}(M(\\eta\\mathbf{a},\\beta,m,\\kappa))^{2}]$ simply by lowerbounding the sum of magnitudes of coefficients of $\\operatorname*{det}(M(\\zeta,\\alpha,m,\\kappa))$ when viewed as a polynomial in $\\zeta$ almost surely over the choice of $\\bar{c}$ . This is because of (a) the fact that $\\operatorname*{det}(M(\\zeta,\\alpha,m,\\kappa))$ and $\\operatorname*{det}(M(\\zeta,\\beta,m,\\kappa))$ have coefficients in $\\zeta$ that are ${\\cal O}(\\eta)$ -close for $\\eta$ small, and (b) the fact that polynomials anti-concentrate over random inputs. ", "page_idx": 31}, {"type": "text", "text": "Analysis of simplified dynamics. The proof is now reduced to showing that $\\operatorname*{det}(\\bar{\\boldsymbol{M}}(\\zeta,\\alpha,m,\\kappa)\\bar{\\rangle}~\\neq~0$ , as a polynomial in $\\zeta$ . In other words, by Lemma E.11, we are now focusing on only the simplified dynamics $\\hat{\\pmb u}^{k}$ . If we can show that $\\operatorname*{det}(M(\\zeta,\\alpha,m,\\kappa))\\neq0$ almost surely over the choice of $\\kappa\\in[1/2,3/2]^{P}$ , then Theorem E.1 follows. Therefore, introducing the variables $\\gamma\\in\\mathbb{R}^{P}$ , it suffices to show that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{det}(M(\\zeta,\\alpha,m,\\gamma))\\neq0,\\mathrm{\\as\\a\\,polynomial\\in}\\ \\zeta,\\gamma.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We start by following simple observation. ", "page_idx": 32}, {"type": "text", "text": "Claim 1. Almost surely over the choice of initialization c\u00af, the set of subsets $K:=\\{S:\\alpha_{S}\\neq0\\}$ has Lea $\\mathfrak{z}(\\mathcal{K})=1$ . ", "page_idx": 32}, {"type": "text", "text": "Proof. This basically follows from the fact that $\\ell$ is piecewise analytic. The argument is as follows: we have that $\\mathsf{L e a p}_{\\mathsf{D L Q}_{\\ell}}=\\mathsf{L e a p}(\\mathcal{C}_{\\mathsf{D L Q}_{\\ell}})=1$ , where $\\overset{\\cdot}{C_{\\sf D L Q_{\\ell}}}=\\{S:\\exists u\\in\\mathbb{R}$ such that $\\mathbb{E}[\\ell^{\\prime}(u,y)\\chi_{S}(z)]\\neq$ $0\\}$ . Since $\\ell$ is piece-wise analytic, so is $\\ell^{\\prime}(u,y)$ in the first argument. Thus, for any $S\\in{\\mathcal{C}}_{\\mathsf{D L Q}_{\\ell}}$ , the set $\\{u:\\mathbb{E}[\\ell^{\\prime}(u,y)\\chi_{S}(z)]=0\\}$ has measure 0. Thus, almost surely over the choice of $\\bar{c}=\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}_{0})$ , we have that for any $S\\in{\\mathcal{C}}_{\\mathsf{D L Q}_{\\ell}}$ ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\ell^{\\prime}(\\bar{c},y)\\chi_{S}(z)]=\\mathbb{E}[\\ell^{\\prime}(\\hat{f}_{\\sf N N}(z;\\bar{\\rho}_{0}),y)\\chi_{S}(z)]=-\\alpha_{S}\\neq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "This implies that $S\\in\\kappa$ as well. We finally conclude that $\\mathsf{L e a p}_{\\mathsf{D L Q}_{\\ell}}=\\mathsf{L e a p}(K)=1.$ ", "page_idx": 32}, {"type": "text", "text": "Using ideas similar to Abbe et al. [2022], we now prove (84) by analyzing the recurrence relations for $p_{k,i}$ to show that to first-order the polynomials $p_{k_{1},i}$ are distinct for all $i\\in[P]$ , but now doing smooth analysis over $\\gamma$ instead. Formally we show the following lemma. ", "page_idx": 32}, {"type": "text", "text": "Lemma E.13. Suppose that $L\\geq2^{8P}$ and let $m_{i}=i!\\binom{L}{i}$ for all $0\\leq i\\leq L,$ , which correspond to the activation function $\\sigma(x)=(1+x)^{L}$ . Let $k_{1}=P$ . Then $\\operatorname*{det}(M(\\zeta,\\alpha,m,\\gamma))\\neq0,$ , i.e. (84) holds. ", "page_idx": 32}, {"type": "text", "text": "Claim 2. For any time step $0\\leq k\\leq k_{1}$ any $j\\in[N]$ , a learning rate $\\eta<1/(4K^{2}(1+K)P k)$ , and any $a\\in[-1,1]$ we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\pmb{u}^{k}(a)\\|_{1},\\|\\hat{\\pmb{u}}^{k}(a)\\|_{1}\\leq2\\eta K^{2}(1+K)P k\\leq1/2.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. We prove this by induction on $k$ . For $k\\;=\\;0$ , we have $\\pmb{u}^{0}\\;=\\;\\hat{\\pmb{u}}^{0}\\;=\\;\\pmb{0}$ and the claim holds trivially. For the inductive step, $\\hat{f}_{\\mathsf{N N}}\\big(z;\\bar{\\rho}_{k}\\big)\\leq\\mathbb{E}_{a\\sim\\mu_{a}}[|a||\\sigma(\\langle{\\pmb u}^{k},{\\pmb z}\\rangle)|]\\leq\\|\\sigma\\|_{\\infty}\\leq K$ , since $a\\sim\\operatorname{Unif}([-1,1])$ . Therefore ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{u^{k+1}(a)\\|_{1}\\leq\\|u^{k}(a)\\|_{1}+\\eta\\|\\kappa\\circ\\mathbb{E}_{y,z}[-\\ell^{\\prime}(\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}_{k}),y)a\\sigma^{\\prime}(\\langle u^{k},z\\rangle)z]\\|_{1}}\\\\ &{\\leq\\|u^{k}(a)\\|_{1}+\\eta\\|\\kappa\\|_{\\infty}\\mathbb{E}_{y,z}[\\|-\\ell^{\\prime}(\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}_{k}),y)a\\sigma^{\\prime}(\\langle u^{k},z\\rangle)z\\|_{1}]}\\\\ &{\\leq\\|u^{k}(a)\\|_{1}+2\\eta K(1+K)\\cdot K\\cdot\\mathbb{E}_{z}[\\|z\\|_{1}]\\leq\\|u^{k}(a)\\|_{1}+2\\eta K^{2}(1+K)P\\leq2\\eta K^{2}(1+K)P k.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Note that here we used $\\|\\kappa\\|_{\\infty}\\leq2$ , $|\\ell^{\\prime}(\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}_{k}),y)|\\leq K(1+|\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}_{k})|)\\leq K(1+K)$ using A2, and $|a\\sigma^{\\prime}(\\langle\\pmb{u}^{k},z\\rangle)|\\leq|a|\\cdot|\\sigma^{\\prime}(\\langle\\pmb{u}^{k},z\\rangle)|\\leq K$ . The bound for $\\lVert\\hat{\\boldsymbol{u}}^{k}(\\boldsymbol{a})\\rVert_{1}$ follows exactly the same argument but now we use $|\\ell^{\\prime}(\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}_{0}),y)|\\le K$ . \u53e3 ", "page_idx": 32}, {"type": "text", "text": "Proof of Lemmas $E.9$ and E.10. We have $\\zeta\\,\\in\\,\\mathbb{R}$ , $\\pmb{\\xi}=(\\xi_{S,k})_{S\\subseteq[P],k\\in\\{0,\\dots,k_{1}-1\\}}$ , $\\rho\\in\\mathbb{R}^{L+1}$ , and $\\gamma\\,\\in\\,\\mathbb{R}^{P}$ as variables. Let $s_{0},\\dotsc,s_{k_{1}-1}\\ :\\ \\{+1,-1\\}^{P}\\ \\rightarrow\\ \\mathbb{R}$ be $\\begin{array}{r}{s_{k}(z)\\:=\\:\\sum_{\\cal S\\subseteq[P]}\\xi_{{\\cal S},k}\\chi_{{\\cal S}}(z)}\\end{array}$ . Consider the recurrence relation $\\pmb{\\nu}^{k}\\in\\mathbb{R}^{P}$ , where we initialize $\\pmb{\\nu}^{0}=\\mathbf{0}$ and, for $0\\leq k\\leq k_{1}-1$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\nu^{k+1}=\\nu^{k}+\\zeta\\gamma\\circ\\mathbb{E}_{z}\\Big[s_{k}\\big(z\\big)\\sum_{r=0}^{L-1}\\frac{\\rho_{r+1}}{r!}\\langle\\nu^{k},z\\rangle^{r}z\\Big].\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "This recurrence is satisfied by $\\zeta=\\eta a,\\pmb\\rho=m,\\pmb\\gamma=\\pmb\\kappa$ , and $\\begin{array}{r}{s_{k}(z)=\\sum_{S}\\beta_{S,k}\\chi_{S}(z)}\\end{array}$ . This is because ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{u^{k+1}=u^{k}-\\eta\\cdot\\kappa\\circ\\mathbb{E}[\\ell^{\\prime}(\\hat{f}_{\\mathsf{M N}}(z;\\bar{\\rho}_{k}),y)a\\sigma^{\\prime}(\\langle u^{k},z\\rangle)z]}\\\\ &{\\qquad=u^{k}+\\eta a\\cdot\\kappa\\circ\\mathbb{E}[-\\ell^{\\prime}(\\hat{f}_{\\mathsf{M N}}(z;\\bar{\\rho}_{k}),y)\\underset{r=0}{\\overset{L-1}{\\sum}}\\frac{m_{r+1}}{r!}\\langle u^{k},z\\rangle^{r}z]}\\\\ &{\\qquad=u^{k}-\\eta\\cdot\\kappa\\circ\\mathbb{E}[\\ell^{\\prime}(\\hat{f}_{\\mathsf{M N}}(z;\\bar{\\rho}_{k}),y)a\\sigma^{\\prime}(\\langle u^{k},z\\rangle)z]}\\\\ &{\\qquad=u^{k}+\\eta a\\cdot\\kappa\\circ\\mathbb{E}[-\\ell^{\\prime}(\\hat{f}_{\\mathsf{M N}}(z;\\bar{\\rho}_{k}),y)\\underset{r=0}{\\overset{L-1}{\\sum}}\\frac{m_{r+1}}{r!}\\langle u^{k},z\\rangle^{r}z]}\\\\ &{\\qquad=u^{k}+\\eta a\\cdot\\kappa\\circ\\mathbb{E}_{z}[\\mathbb{E}_{y\\mid z}[-\\ell^{\\prime}(\\hat{f}_{\\mathsf{M N}}(z;\\bar{\\rho}_{k}),y)\\underset{r=0}{\\overset{L-1}{\\sum}}\\frac{m_{r+1}}{r!}\\langle u^{k},z\\rangle^{r}z]}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Finally, noting that $\\begin{array}{r}{\\mathbb{E}_{y|z}[-\\ell^{\\prime}(\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}_{k}),y)]\\,=\\,\\sum_{S}\\beta_{S,k}\\chi_{S}(z)}\\end{array}$ , we showed that recurrence (85) holds with specified value of variables. Similarly, $\\hat{\\pmb u}^{k}(a)$ with $\\begin{array}{r}{s_{k}(z)=\\sum_{S}\\alpha_{S}\\chi_{S}(z)}\\end{array}$ for $\\alpha_{S}\\,=$ $-\\mathbb{E}[\\ell^{\\prime}(\\hat{f}_{\\mathsf{N N}}(z;\\bar{\\rho}_{0},y)\\chi_{S}(z)]$ . This is because $|\\langle\\boldsymbol{u}^{k},\\boldsymbol{z}\\rangle|,|\\langle\\hat{\\boldsymbol{u}}^{k},\\boldsymbol{z}\\rangle|\\leq1/2<1$ by Claim 2 and in the interval $(-1,1)$ , we have $\\begin{array}{r}{\\sigma(x)=\\sum_{r=0}^{L}{\\frac{m_{r}}{r!}}x^{r}}\\end{array}$ . It finally remains to show that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\nu_{i}^{k}=p_{k,i}(\\zeta,\\pmb{\\xi},\\pmb{\\rho},\\gamma).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "The proof is by induction on $k$ . For $k=0$ , it is true that $p_{0,i}(\\zeta,\\xi,\\rho,\\gamma)=0=\\nu_{i}^{0}$ . For the inductive step, for any $r\\geq1$ and $i\\in[d]$ , we can write ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{i}\\cdot\\mathbb{E}_{z}\\big[s_{k}(z)\\langle\\nu^{k},z\\rangle^{r}z_{i}\\big]=\\gamma_{i}\\mathbb{E}_{z}\\bigg[s_{k}(z)z_{i}\\underbrace{\\sum_{\\substack{(i_{1},\\ldots,i_{r})\\in[P]^{r}\\,l=1}}\\prod_{\\substack{l=1}}^{r}\\nu_{i_{l}}^{k}z_{i_{l}}\\bigg]}_{\\mathbb{E}_{z}\\bigg[s_{k}(z)\\chi_{i}(z)\\displaystyle\\prod_{l=1}^{r}\\chi_{i_{l}}(z)\\displaystyle\\prod_{l=1}^{r}p_{k,i_{l}}(\\zeta,\\xi,\\rho,\\gamma)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad\\mathbb{E}_{\\{i\\}\\oplus\\{i_{1}\\}\\oplus\\cdots\\oplus\\{i_{r}\\},k}\\displaystyle\\prod_{l=1}^{r}p_{k,i_{l}}(\\zeta,\\xi,\\rho,\\gamma),}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and $\\gamma_{i}\\mathbb{E}_{z}[s_{k}(z)\\langle\\pmb{\\nu}^{k},z\\rangle^{0}z_{i}]=\\gamma_{i}\\mathbb{E}_{z}[s_{k}(z)z_{i}]=\\gamma_{i}\\xi_{\\{i\\},k}.$ . The inductive step follows by linearity of expectation. \u53e3 ", "page_idx": 33}, {"type": "text", "text": "E.5 Proof of Lemma E.13 ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In Lemma E.13, we have already fixed $m\\in\\mathbb{R}^{L+1}$ to be $m_{i}=i!{\\binom{L}{i}}$ for all $i\\in\\{0,\\ldots,L\\}$ . This corresponds to the activation function $\\sigma(x)=(1+x)^{L}$ . ", "page_idx": 33}, {"type": "text", "text": "E.5.1 Reducing to minimal Leap 1 structure ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "To show that $\\operatorname*{det}(M(\\zeta,\\alpha,m,\\gamma))\\not\\equiv0$ as a polynomial in $\\zeta,\\gamma$ , we first show that it suffices to consider \u201cminimal\u201d Leap 1 set structure, without loss of generality. ", "page_idx": 33}, {"type": "text", "text": "Claim 3. Let $K^{\\prime}\\subseteq K$ such that L $\\mathsf{a p}(K^{\\prime})=1$ . Then if ", "page_idx": 33}, {"type": "text", "text": "we have ", "page_idx": 33}, {"type": "text", "text": "Proof. Directly substituting $\\alpha_{S}=0$ for all $S\\in{\\mathcal{K}}\\setminus K^{\\prime}$ . ", "page_idx": 33}, {"type": "text", "text": "Therefore, without loss of generality (up to relabelling the indices of the variables), we assume that ", "page_idx": 33}, {"type": "text", "text": "Otherwise, we could remove a set from $\\kappa$ while still having $\\mathsf{L e a p}(K)=1$ . ", "page_idx": 33}, {"type": "text", "text": "E.5.2 Weights to leading order ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Let us define the polynomials $q_{k,i}$ in variables $\\zeta,\\phi,\\rho,\\gamma$ where $\\phi~=~(\\phi_{S})_{S\\in K}$ . For all $k\\ \\in$ $\\{0,\\ldots,k_{1}-1\\}$ and $i\\in[P]$ , ", "page_idx": 34}, {"type": "text", "text": "Therefore $M(\\zeta,\\phi,\\rho,\\gamma)$ has entries $\\begin{array}{r}{M_{z,j}(\\zeta,\\phi,\\rho,\\gamma)=\\sum_{r=0}^{L}\\frac{\\rho_{r}}{r!}\\left(\\sum_{i=1}^{P}q_{k,i}(\\zeta_{j},\\phi,\\rho,\\gamma)\\right)^{r}}\\end{array}$ . We will explicitly compute the lowest degree term in $\\zeta$ . First, we show that many terms are zero. To this end, consider the following notation: Recursively define ", "page_idx": 34}, {"type": "equation", "text": "$$\no_{i}=1+\\sum_{i^{\\prime}\\in S_{i}\\setminus\\{i\\}}o_{i^{\\prime}}\\;{\\mathrm{for~all~}}i\\in[P],\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where the sum over an empty set is 0 by convention. Additionally, let $\\tilde{q}_{k,i}(\\phi,\\rho,\\gamma)$ to be the coefficient of the term in $q_{k,i}(\\zeta,\\phi,\\rho,\\gamma)$ whose degree in $\\zeta$ is $o_{i}$ . Furthermore, recursively define ", "page_idx": 34}, {"type": "equation", "text": "$$\ng_{i}(\\gamma)=\\gamma_{i}\\operatorname{if}\\,\\{i\\}\\in\\mathcal{K},\\mathrm{~and~otherwise~}g_{i}(\\gamma)=\\gamma_{i}\\prod_{i^{\\prime}\\in S_{i}\\setminus\\{i^{\\prime}\\}}g_{i^{\\prime}}(\\gamma).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Lemma E.14. Under the above notation, we have that $q_{k,i}(\\zeta,\\phi,m,\\gamma)$ has no nonzero terms of degree less than $o_{i}$ in $\\zeta$ . Furthermore, $\\tilde{q}_{k,i}(\\phi,\\rho,\\gamma)$ can be decomposed as $\\tilde{q}_{k,i}(\\phi,\\rho,\\gamma)\\;=\\;$ $g_{i}(\\gamma)\\cdot\\hat{q}_{k,i}(\\phi,\\pmb{\\rho})$ for some $\\hat{q}_{k,i}(\\phi,\\rho)$ . ", "page_idx": 34}, {"type": "text", "text": "Proof. We will prove this by induction on $k$ . The base case for $k=0$ trivially holds because $q_{0,i}\\equiv0$ . For the inductive step, we assume the statement holds for all $k^{\\prime}\\,\\in\\,\\{0,\\ldots,k\\}$ and we prove the statement for $k+1$ . By the recurrence relation of the polynomials $q_{k,i}$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\iota_{k+1,i}(\\zeta,\\phi,\\rho,\\gamma)=q_{k,i}(\\zeta,\\phi,\\rho,\\gamma)+\\gamma_{i}\\zeta\\rho_{1}\\phi_{\\{i\\}}\\mathbb{1}(\\{i\\}\\in K)}}\\\\ {{\\displaystyle\\quad+\\gamma_{i}\\zeta\\sum_{r=1}^{L-1}\\frac{\\rho_{r+1}}{r!}\\sum_{{\\scriptstyle(i_{1},\\ldots,i_{r})\\in[P]^{r}}}\\phi_{\\{i\\}\\oplus\\{i_{1}\\}\\oplus\\cdots\\oplus\\{i_{r}\\}}\\mathbb{1}(\\{i\\}\\oplus\\{i_{1}\\}\\oplus\\cdots\\oplus\\{i_{r}\\}\\in K)\\prod_{l=1}^{r}q_{k,i_{l}}(\\zeta,\\phi,\\rho,\\gamma)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "It suffices to show that the claim holds for each one of the three terms above. ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{i}\\left(\\frac{\\rho_{r^{\\prime}+1}}{r^{\\prime}!}\\phi_{S_{i}}\\prod_{i^{\\prime}\\in S_{i}\\setminus\\{i\\}}^{r}\\tilde{q}_{k,i^{\\prime}}(\\phi,\\rho,\\gamma)\\sum_{(i_{1},\\ldots,i_{r^{\\prime}})\\in[P]^{r^{\\prime}}}1(\\{i,i_{1},\\ldots,i_{r^{\\prime}}\\}=S_{i})\\right)}\\\\ &{=\\gamma_{i}\\left(\\frac{\\rho_{r^{\\prime}+1}}{r^{\\prime}!}\\phi_{S_{i}}\\prod_{i^{\\prime}\\in S_{i}\\setminus\\{i\\}}^{r}g_{i^{\\prime}}(\\gamma)\\widehat{q}_{k,i^{\\prime}}(\\rho,\\gamma)\\sum_{(i_{1},\\ldots,i_{r^{\\prime}})\\in[P]^{r^{\\prime}}}1(\\{i,i_{1},\\ldots,i_{r^{\\prime}}\\}=S_{i})\\right)}\\\\ &{=\\gamma_{i}\\prod_{i^{\\prime}\\in S_{i}\\setminus\\{i^{\\prime}\\}}g_{i^{\\prime}}(\\gamma)\\left(\\frac{\\rho_{r^{\\prime}+1}}{r^{\\prime}!}\\phi_{S_{i}}\\prod_{i^{\\prime}\\in S_{i}\\setminus\\{i\\}}^{r}\\widehat{q}_{k,i^{\\prime}}(\\rho,\\gamma)\\sum_{(i_{1},\\ldots,i_{r^{\\prime}})\\in[P]^{r^{\\prime}}}1(\\{i,i_{1},\\ldots,i_{r^{\\prime}}\\}=S_{i})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Case $^b$ . If $\\{i\\}\\oplus\\{i_{1}\\}\\oplus\\cdots\\oplus\\{i_{r}\\}=S_{i^{\\prime}}$ for some $i^{\\prime}\\neq i$ , then either $i\\in\\{i_{1},\\ldots,i_{r}\\}$ , in which case $1+\\textstyle\\sum_{l=1}^{r}o_{i_{l}}>o_{i}$ . Otherwise, we must have $i^{\\prime}>i$ . But in this case $o_{i^{\\prime}}>o_{i}$ since $i\\in S_{i^{\\prime}}$ , so we also have $\\sum_{l=1}^{r}o_{i_{l}}>o_{i}$ and again no new terms of degree less than $o_{i}$ are added. In fact, only terms  of degree strictly more than $o_{i}$ are added. Thus, in either case the coefficient of the term with degree $o_{i}$ in $\\zeta$ is simply 0. ", "page_idx": 35}, {"type": "text", "text": "Finally, we showed that each of the three terms have at least the degree $o_{i}$ in $\\zeta$ . Moreover, each term can be decomposed as $g_{i}(\\gamma)\\hat{q}(\\pmb{\\rho},\\gamma)$ for a certain $\\hat{q}$ . Thus, overall we can decompose ", "page_idx": 35}, {"type": "text", "text": "Claim 4. Let ", "page_idx": 35}, {"type": "equation", "text": "$$\nr_{z}(\\zeta,\\phi,\\pmb{\\rho},\\gamma)=\\sum_{i}z_{i}q_{k_{1},i}(\\zeta,\\phi,\\pmb{\\rho},\\gamma).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Then, for each distinct pair $z,z^{\\prime}\\in\\{+1,-1\\}^{P}$ , we have $r_{z}(\\zeta,\\phi,m)-r_{z^{\\prime}}(\\zeta,\\phi,m)\\not\\equiv0$ as a polynomial in $\\zeta$ and $\\gamma$ . ", "page_idx": 35}, {"type": "text", "text": "Proof. Recall the definition of $o_{i}$ from Lemma E.14. Fix $i\\in[P]$ be such that $z_{i}\\neq z_{i}^{\\prime}$ and $o_{i}$ is minimized. It is always possible to choose such an $i$ since $z\\neq z^{\\prime}$ . Again $\\tilde{r}_{z}(\\phi,\\rho,\\gamma)$ is the term with degree $o_{i}$ in $\\zeta$ . Using Lemma E.14 then ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\tilde{r}_{z}(\\phi,\\rho,\\gamma)-\\tilde{r}_{z^{\\prime}}(\\phi,\\rho,\\gamma)=\\sum_{\\substack{i^{\\prime}\\mathrm{~s.t.~}o_{i^{\\prime}}=o_{i},z_{i^{\\prime}}\\neq z_{i^{\\prime}}^{\\prime}}}(z_{i^{\\prime}}-z_{i^{\\prime}}^{\\prime})\\tilde{q}_{k_{1},i^{\\prime}}(\\phi,\\rho,\\gamma),\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "but $\\tilde{q}_{k_{1},i^{\\prime}}$ are non-zero polynomials in $\\gamma$ with different dependence that is captured by $g_{i^{\\prime}}(\\gamma)$ according to Lemma E.14. Thus, $r_{z}(\\zeta,\\phi,m)-r_{z^{\\prime}}(\\zeta,\\phi,m)\\not\\equiv0$ . \u53e3 ", "page_idx": 35}, {"type": "text", "text": "E.5.3 Linear Independence of Powers of Polynomials ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Similar to Abbe et al. [2022], we finish our proof using the classical result of Newman and Slater [1979] about the linear independence of large powers of polynomials. ", "page_idx": 35}, {"type": "text", "text": "Proposition E.15 (Remark 5.2 in Newman and Slater [1979]). Let $R_{1},\\ldots,R_{m}\\,\\in\\,\\mathbb{C}[\\zeta]$ be nonconstant polynomials such that for all $i\\neq i^{\\prime}\\in[m]$ we have $R_{i}(\\zeta)$ is not a constant multiple of $R_{i^{\\prime}}(\\zeta)$ . Then for $L\\geq8m^{2}$ we have that $(\\overset{\\cdot}{R}_{1})^{L},\\,\\cdot\\,\\cdot\\,,(R_{m})^{L}\\in\\mathbb{C}[\\zeta]$ are $\\mathbb{C}$ -linearly independent. ", "page_idx": 35}, {"type": "text", "text": "We will finally show that $\\operatorname*{det}(M(\\zeta,\\alpha,m,\\gamma))\\neq0$ . ", "page_idx": 35}, {"type": "text", "text": "Proof of Lemma E.13. Let us fix $\\kappa\\,\\in\\,\\mathbb{R}^{P}$ such that for all $z\\neq z^{\\prime}$ , we have $r_{z}(\\zeta,\\alpha,m,\\kappa)\\textsuperscript{-}$ $r_{z^{\\prime}}(\\zeta,\\alpha,m,\\kappa)\\not\\equiv0$ as polynomials in $\\zeta$ . This can be ensured by drawing $\\kappa\\sim\\operatorname{Unif}([0.5,1.5]^{P})$ , since for all $z\\neq z^{\\prime}$ we have $r_{z}(\\zeta,\\alpha,m,\\gamma)-r_{z^{\\prime}}(\\zeta,\\alpha,m,\\gamma)\\not\\equiv0$ as polynomials in $\\zeta,\\gamma$ by Claim 4. We now rewrite $\\tilde{r}_{z}(\\zeta)=r_{z}(\\zeta,\\alpha,m,\\kappa)$ to further indicate that the variables $\\phi=\\alpha,\\rho=m$ and $\\gamma\\,=\\,\\kappa$ are instantiated, and that we are looking at a polynomial over $\\zeta$ . We have also chosen $m_{i}=i!{\\binom{L}{i}}$ for all $i\\in\\{0,\\ldots,L\\}$ . We then have ", "page_idx": 35}, {"type": "equation", "text": "$$\nM_{z,j}(\\zeta,\\alpha,m,\\kappa)=(1+\\tilde{r}_{z}(\\zeta_{j}))^{L}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Also, observe from the recurrence relations $\\zeta$ divides $q_{k_{1},i}(\\zeta,\\alpha,m,\\kappa)$ for each $i\\in[P]$ , so $\\zeta_{j}$ divides $\\begin{array}{r}{\\tilde{r}_{z}(\\zeta_{j})=\\sum_{i=1}^{P}z_{i}q_{k_{1},i}(\\zeta_{j},\\alpha,m,\\kappa)}\\end{array}$ . Therefore, polynomials $(1+\\tilde{r}_{z}(\\zeta_{j}))$ and , $\\left(1+\\tilde{r}_{z^{\\prime}}(\\zeta_{j})\\right)$ are not constant multiples of each other for any $z\\neq z^{\\prime}$ . ", "page_idx": 35}, {"type": "text", "text": "Now see $\\zeta$ as variables, and construct the Wronskian matrix over the $L$ -th power polynomials $\\{(1+\\tilde{r}_{z}(\\zeta))^{L}\\}_{z\\in\\{+1,-1\\}^{P}}$ . This is a $2^{P}\\times2^{P}$ matrix $H(\\zeta)$ whose entries are indexed by $_{z}$ and $l\\in[2^{P}]$ and defined by: ", "page_idx": 35}, {"type": "equation", "text": "$$\nH_{z,l}(\\zeta)=\\frac{\\partial^{l-1}}{\\partial\\zeta^{l-1}}(1+\\tilde{r}_{z}(\\zeta))^{L}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Applying Proposition E.15 implies the polynomials $\\{(1\\,+\\,\\tilde{r}_{z}(\\zeta))^{L}\\}_{z\\in\\{+1,-1\\}^{P}}$ are linearlyindependent, and so the Wronskian determinant is nonzero as a polynomial in $\\zeta$ : ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\operatorname*{det}(H(\\zeta))\\not\\equiv0.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Also, observe that we can write det(H(\u03b6)) =\u2202\u2202\u03b62\u2202\u2202\u03b622 . . $\\begin{array}{r}{\\operatorname*{det}(H(\\zeta))=\\frac{\\partial}{\\partial\\zeta_{2}}\\frac{\\partial^{2}}{\\partial\\zeta_{3}^{2}}\\cdot\\cdot\\cdot\\frac{\\partial^{2^{P}-1}}{\\partial\\zeta_{2^{P}}^{2P-1}}\\operatorname*{det}(M(\\zeta,\\alpha,m,\\kappa))\\mid_{\\zeta=\\zeta_{1}=\\cdots=\\zeta_{2^{P}}}.}\\end{array}$ This finally gives us $\\operatorname*{det}(M(\\zeta,\\alpha,\\boldsymbol{m},\\kappa))\\quad\\neq\\quad0$ as a polynomial in $\\zeta$ , and thus, $\\operatorname*{det}(M(\\zeta,\\bar{\\alpha},\\bar{m},\\gamma))\\not\\equiv0$ as a polynomial in $\\zeta$ and $\\gamma$ . \u53e3 ", "page_idx": 36}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 37}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 37}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 37}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 37}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 37}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 37}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 37}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: Yes, it does. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 37}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We do specify the limitations of our work. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 38}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: Yes, we do provide all the assumptions and correct proofs. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 38}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We do provide the code and the experiments are fully reproducible. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 38}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 39}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: We attach the code in the supplementary material. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 39}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 40}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: We specify our training algorithm and related details. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 40}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: We do report the error bars whenever necessary. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 40}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: All experiments are relatively small scale and can be reproduced under 1 hr of compute on a standard computer with 16 GB RAM. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 40}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 41}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: It does. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 41}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: This is a theoretical work with no societal impact. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 41}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: This paper does not have models or data with safety concerns. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 41}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 42}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: The paper does not use any existing assets. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 42}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: The paper does not release new asset. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 42}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: This does not require crowd sourcing nor research with human subject. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 43}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: Our study does not require IRB approval. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 43}]