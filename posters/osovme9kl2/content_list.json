[{"type": "text", "text": "Implicit Regularization of Sharpness-Aware Minimization for Scale-Invariant Problems ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Bingcong Li Liang Zhang Niao He ", "page_idx": 0}, {"type": "text", "text": "Department of Computer Science ETH Zurich, Switzerland {bingcong.li, liang.zhang, niao.he}@inf.ethz.ch ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sharpness-aware minimization (SAM) improves generalization of various deep learning tasks. Motivated by popular architectures such as LoRA, we explore the implicit regularization of SAM for scale-invariant problems involving two groups of variables. Instead of focusing on commonly used sharpness, this work introduces a concept termed balancedness, defined as the difference between the squared norm of two variables. This allows us to depict richer global behaviors of SAM. In particular, our theoretical and empirical findings reveal that i) SAM promotes balancedness; and ii) the regularization on balancedness is data-responsive \u2013 outliers have stronger impact. The latter coincides with empirical observations that SAM outperforms SGD in the presence of outliers. Leveraging the implicit regularization, we develop a resource-efficient SAM variant, balancedness-aware regularization (BAR), tailored for scale-invariant problems such as finetuning language models with LoRA. BAR saves $95\\%$ computational overhead of SAM, with enhanced test performance across various tasks on RoBERTa, GPT2, and OPT-1.3B. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sharpness-aware minimization (SAM) is emerging as an appealing optimizer, because it enhances generalization performance on various downstream tasks across vision and language applications (Foret et al., 2021; Chen et al., 2022; Bahri et al., 2022). The success of SAM is typically explained using its implicit regularization (IR) toward a flat solution (Wen et al., 2023a). ", "page_idx": 0}, {"type": "text", "text": "However, existing results only characterize sharpness/flatness near local minima (Wen et al., 2023a). Little is known about early convergence, despite its crucial role in SAM\u2019s implicit regularization (Agarwala and Dauphin, 2023). In addition, theoretical understanding of SAM highly hinges upon the existence of positive eigenvalues of Hessians (Wen et al., 2023a), leaving gaps in nonconvex scenarios where the Hessian can be negative definite. The limitations above lead to our first question (Q1): can we broaden the scope of implicit regularization to depict global behaviors in SAM? ", "page_idx": 0}, {"type": "text", "text": "Moreover, scenarios where SAM popularizes often involve certain form of data anomalies, such as outliers and large data variance. SAM has provable generalization beneftis on sparse coding problems in the small signal-to-noise ratio (SNR) regime (Chen et al., 2023). Remarkable performance of SAM is also observed under distributional shifts, e.g., domain adaptation (Wang et al., 2023), meta-learning (Abbas et al., 2022), and transfer learning in language models (Bahri et al., 2022; Sherborne et al., 2023). Evidences above motivate our second question (Q2): can implicit regularization of SAM reflect its enhanced performance under data anomalies? ", "page_idx": 0}, {"type": "text", "text": "This work answers both Q1 and Q2 within a class of scale-invariant problems. The focus on scaleinvariance is motivated by its prominence in deep learning architectures. Consider variables $\\mathbf{x}\\in\\mathbb{R}^{d_{1}}$ ", "page_idx": 0}, {"type": "image", "img_path": "oSOVME9kl2/tmp/b082a8a018193c51940accabeadaa845712ce18e82142f22d8af3381c09ec181.jpg", "img_caption": ["(a) non-overparametrized (NOP) ", "(b) overparametrized (OP) "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Implicit regularization of SAM on balancedness. The losses for NOP and OP are $\\mathbb{E}[\\|\\mathbf{x}\\mathbf{y}^{\\top}-$ $(\\mathbf{A}+\\alpha\\mathbf{N})\\lVert^{2}\\rVert$ and $\\mathbb{E}[\\|\\mathbf{x}^{\\top}\\mathbf{y}-(a+\\alpha n)\\|^{2}]$ , respectively. Here, A is the ground truth matrix, $\\mathbf{N}$ is the Gaussian noise, and $\\alpha$ controls the SNR. Left of (a) and (b): $\\lvert\\lvert\\mathbf{x}_{t}\\rvert\\rvert^{2}-\\lvert\\lvert\\mathbf{y}_{t}\\rvert\\rvert^{2}\\rvert$ vs. iteration. Right of (a) and (b): $||\\mathbf{g}_{\\mathbf{x}_{t}}||^{2}-||\\mathbf{g}_{\\mathbf{y}_{t}}||^{2}|$ vs. iteration, where $(\\mathbf{g}_{\\mathbf{x}_{t}},\\mathbf{g}_{\\mathbf{y}_{t}})$ denotes stochastic gradients. ", "page_idx": 1}, {"type": "text", "text": "and $\\textbf{y}\\in\\mathbb{R}^{d_{2}}$ , both in high-dimensional space. The problems of interest can be categorized into non-overparametrization (NOP) and overparametrization (OP), based on whether the dimension of variables $(d_{1}+d_{2})$ is greater than dimension of dom $f$ , ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbf{NOP:}}&{\\underset{\\mathbf{x},\\mathbf{y}}{\\mathrm{min}}\\,f_{n}(\\mathbf{xy^{\\top}})=\\mathbb{E}_{\\boldsymbol{\\xi}\\sim\\mathcal{D}}\\big[f_{n}^{\\boldsymbol{\\xi}}(\\mathbf{xy^{\\top}})\\big],}\\\\ {\\mathbf{OP:}}&{\\underset{\\mathbf{x},\\mathbf{y}}{\\mathrm{min}}\\,f_{o}(\\mathbf{x}^{\\top}\\mathbf{y})=\\mathbb{E}_{\\boldsymbol{\\xi}\\sim\\mathcal{D}}\\big[f_{o}^{\\boldsymbol{\\xi}}(\\mathbf{x}^{\\top}\\mathbf{y})\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Here, $d_{1}=d_{2}$ is assumed for OP, and $\\mathcal{D}$ denotes the training data. For both cases, the losses are nonconvex in $(\\mathbf{x},\\mathbf{y})$ . Scale-invariance refers to that $\\left(\\alpha\\mathbf{x},\\mathbf{y}/\\alpha\\right)$ share the same objective value $\\forall\\alpha\\neq0$ . It naturally calls for implicit regularization from optimization algorithms to determine the value of $\\alpha$ . We focus on two-variable problems in the main text for simplicity and generalize the results to multi-layer cases in the appendix. Problems (1a) and (1b) are inspired by widely-adopted modules in deep learning, where low rank adapters (LoRA) for finetuning language models is NOP, and softmax in attention falls in OP framework (Hu et al., 2022; Vaswani et al., 2017). ", "page_idx": 1}, {"type": "text", "text": "This work studies SAM\u2019s implicit regularization on balancedness, defined as $\\begin{array}{r}{B_{t}=\\frac{1}{2}\\left(\\|\\mathbf{x}_{t}\\|^{2}-\\|\\mathbf{y}_{t}\\|^{2}\\right)_{.}}\\end{array}$ . Balancedness is a useful alternative to sharpness for (1) because: i) it enables us to go beyond local minima and describe the behavior over SAM\u2019s entire trajectory; ii) analyses and assumptions can be significantly simplified when working with $B_{t}$ ; and, iii) it enables a data-driven perspective for understanding SAM. Building on balancedness, we answer our major questions. ", "page_idx": 1}, {"type": "text", "text": "For Q1, we prove that even with imbalanced initialization, SAM drives $|B_{t}|\\,\\to\\,0$ for OP, while ensuring a small $|\\boldsymbol{\\mathcal{B}}_{t}|$ in NOP. In contrast, we also prove that balancedness of SGD is unchanged over iterations. This clear distinction between SAM and SGD is illustrated in Fig. 1. Thanks to the adoption of balancedness, our results on implicit regularization have no requirement on the batchsize compared to (Wen et al., 2023a) and can be extended to explain $m$ -sharpness in (Foret et al., 2021). ", "page_idx": 1}, {"type": "text", "text": "Regarding Q2, we present analytical and empirical evidences that data anomalies (e.g., samples with large noise) have stronger impact on balancedness for both NOP and OP. Fig. 1 showcases an example where SAM is applied on the same problem with different SNRs. Smaller SNR (i.e., larger $\\alpha$ ) promotes balancedness faster. Being more balanced with noisy data also aligns well with previous studies (Chen et al., 2023; Wang et al., 2023), which show that SAM performs better than SGD under data anomalies. This data-driven behavior of SAM is well depicted through balancedness. ", "page_idx": 1}, {"type": "text", "text": "Our theoretical understanding on balancedness also cultivates practical tools. In particular, we explicify the implicit regularization of SAM as a data-driven regularizer. When applied on top of, e.g., SGD, it enables a computationally efficient variant of SAM, balancedness-aware regularization (BAR), suited for scale-invariant problems such as finetuning language models with LoRA $\\mathrm{Hu}$ et al., 2022). BAR eliminates the need to compute the second gradient in SAM, thereby significantly reducing overhead in large-scale settings. BAR improves the test performance of LoRA on three representative downstream tasks on RoBERTa, GPT2, and OPT, while saving $95\\%$ computational overhead of SAM. Moreover, this is the first efficient SAM approach derived from SAM\u2019s implicit regularization. In a nutshell, our contribution can be summarized as: ", "page_idx": 1}, {"type": "text", "text": "O Theories. Balancedness is introduced as a new metric for implicit regularization in SAM. Compared to sharpness, balancedness enables us to depict richer behaviors \u2013 SAM favors balanced solutions for both NOP and OP, and data anomalies have stronger regularization on balancedness. O Practice. Implicit regularization of SAM is made explicit for practical merits. The resulting approach, balancedness-aware regularization (BAR), improves accuracy for finetuning language models with LoRA, while significantly saving computational overhead of SAM. ", "page_idx": 2}, {"type": "text", "text": "Notation. Bold lowercase (capital) letters denote column vectors (matrices); $\\Vert\\cdot\\Vert$ stands for $\\ell_{2}$ (Frobenius) norm of a vector (matrix), and $(\\cdot)^{\\top}$ refers to transpose. ", "page_idx": 2}, {"type": "text", "text": "1.1 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Related topics are streamlined here, with comprehensive discussions deferred to Apdx. A.2. ", "page_idx": 2}, {"type": "text", "text": "Scale-invariance in deep learning. Scale-invariant modules are prevalent in modern neural networks, such as LoRA, ReLU networks, and softmax in attention. However, scale-invariant problems are not yet fully understood, especially from a theoretical perspective. Neyshabur et al. (2018) develop scale-invariant PAC-Bayesian bounds for ReLU networks. A scale-invariant SGD is developed in (Neyshabur et al., 2015), and this approach becomes more practical recently in (Gonon et al., 2024). Linear neural networks entail scale-invariance and overparametrization simultaneously, and IR of (S)GD on quadratic loss is established in (Arora et al., 2018; Du et al., 2018; Gidel et al., 2019). IR of GD for softmax attention in transformers is studied in (Sheen et al., 2024) assuming linearly separable data. It is pointed out in (Dinh et al., 2017) that sharpness is sensitive to scaling, while our results indicate that when taking the training trajectory into account, SAM excludes extreme scaling. ", "page_idx": 2}, {"type": "text", "text": "Mechanism behind SAM. To theoretically explain the success of SAM, Bartlett et al. (2023) analyze sharpness on quadratic losses. Wen et al. (2023a) focus on sharpness of SAM near the solution manifold on smooth loss functions, requiring batchsize to be 1 in the stochastic case. Andriushchenko and Flammarion (2022) consider sparsity of SAM on (overparametrized) diagonal linear networks on a regression problem. Chen et al. (2023) study the benign overfitting of SAM on a two-layer ReLU network. In general, existing studies on SAM\u2019s implicit regularization focus more on sharpness and do not fully capture scale-invariance. In comparison, our results i) are Hessian-free and hence sharpness-free; ii) have no constraint on batchsize; and iii) hold for both NOP and OP. ", "page_idx": 2}, {"type": "text", "text": "SAM variants. Approaches in (Kim et al., 2022; Kwon et al., 2021) modify SAM for efficiency under coordinate-wise ill-scaling, while our results suggest that SAM favors balancedness between layers. Computationally efficient SAM variants are developed through reusing or sparsifying gradients (Liu et al., 2022; Mi et al., 2022); stochastic perturbation (Du et al., 2022a); switching to SGD (Jiang et al., 2023); and connecting with distillation (Du et al., 2022b). Our BAR can be viewed as resource-efficient SAM applied specifically for scale-invariant problems such as LoRA. Different from existing works, BAR is the first to take inspiration from the implicit regularization of SAM. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This section briefly reviews SAM and then compares sharpness with balancedness. For a smoother presentation, our main numerical benchmark, LoRA (Hu et al., 2022), is revisited in Sec. 5. ", "page_idx": 2}, {"type": "text", "text": "2.1 Recap of SAM ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Sharpness-aware minimization (SAM) is designed originally to seek for solutions in flat basins. The idea is formalized by enforcing small loss around the entire neighborhood in parameter space, i.e., $\\begin{array}{r}{\\operatorname*{min}_{\\mathbf{w}}\\operatorname*{max}_{\\|\\epsilon\\|\\leq\\rho}h(\\mathbf{w}+}\\end{array}$ $\\epsilon$ ), where $\\rho$ is the radius of considered neighborhood, and $h(\\mathbf{w}):=\\mathbb{E}_{\\xi}[h^{\\xi}(\\mathbf{w})]$ . Practical implementation of SAM is summarized under Alg. 1. It is proved in (Wen et al., 2023a) ", "page_idx": 2}, {"type": "table", "img_path": "oSOVME9kl2/tmp/3911df423be5ca8e2d3b82f1f5bd90bf86660d62f10836668a28a1994ee028d5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "that $\\|\\nabla h_{t}(\\mathbf{w})\\|\\neq0$ (in line 5) holds for any $\\rho$ under most initialization. Based on this result and similar to (Dai et al., 2023), we assume that SAM iterates are well-defined. ", "page_idx": 2}, {"type": "text", "text": "Limitation of sharpness. Coming naturally with SAM is the so-termed sharpness, given by ${\\cal{S}}({\\bf{w}}):=$ $\\operatorname*{max}_{\\lVert\\epsilon\\rVert\\leq\\rho}h(\\mathbf{w}+\\bar{\\epsilon})\\mathrm{~-~}h(\\mathbf{w})$ . When $\\|\\nabla\\bar{h}(\\mathbf{w})\\|\\,\\to\\,0$ , ${\\cal{S}}({\\bf\\tilde{w}})$ can be approximated using (scaled) largest eigenvalue of Hessian (Zhuang et al., 2022). This approximation is widely exploited in literature to study the implicit regularization of SAM. Consequently, most results only hold locally \u2013 behaviors near $\\|\\nabla h(\\mathbf{w})\\|\\rightarrow0$ are studied. In addition, sharpness (the largest eigenvalue) is not always informative for scale-invariant problems (1). Consider $h(x,y)\\,=\\,{\\bar{x}}y$ for example. The sharpness is 1 for any $(x,y)$ \u2013 these points are not distinguishable in terms of sharpness. ", "page_idx": 3}, {"type": "text", "text": "2.2 Prelude on Balancedness ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Balancedness $\\begin{array}{r}{\\mathcal{B}_{t}:=\\frac{1}{2}\\left(\\|\\mathbf{x}_{t}\\|^{2}-\\|\\mathbf{y}_{t}\\|^{2}\\right)}\\end{array}$ turns out to be an intriguing alternative to sharpness on the scale-invariant problem (1). Being a global metric, balancedness is capable of describing the entire trajectory of an algorithm, regardless of proximity to critical points or definiteness of Hessian. ", "page_idx": 3}, {"type": "text", "text": "How does $B_{t}$ evolve in different algorithms? To set a comparing benchmark of SAM, we first borrow results from previous works on SGD. Following implicit regularization literature such as (Arora et al., 2018, 2019b; Wen et al., 2023a), we consider SGD with infinitesimally small learning rate $\\eta\\rightarrow0$ for the NOP problem (1a) ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{x}_{t+1}=\\mathbf{x}_{t}-\\eta\\mathbf{g}_{\\mathbf{x}_{t}},\\quad\\mathbf{y}_{t+1}=\\mathbf{y}_{t}-\\eta\\mathbf{g}_{\\mathbf{y}_{t}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Theorem 1 ((Arora et al., 2018, 2019a; Ji and Telgarsky, 2019; Ahn et al., 2023)). When applying SGD on the NOP (1a), the limiting flow with $\\eta\\rightarrow0$ satisfies $\\|\\mathbf{x}_{t}\\|^{2}-\\|\\mathbf{y}_{t}\\|^{2}=\\|\\mathbf{x}_{0}\\|^{2}-\\|\\bar{\\mathbf{y}_{0}}\\|^{\\dot{2}}$ for all $t>0$ . In other words, $\\begin{array}{r}{\\frac{d B_{t}}{d t}=0}\\end{array}$ holds. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1 shows that $B_{t}\\equiv B_{0}$ given $\\eta\\rightarrow0$ . A graphical illustration can be found in Fig. 1 (a). Another interesting observation is that given the same initialization, $B_{t}$ is fixed for SGD regardless of training datasets. This suggests that SGD is less adaptive to data. A similar result of Theorem 1 can be established for SGD on OP. The full statement is deferred to Apdx. C.1; see also Fig. 1 (b). ", "page_idx": 3}, {"type": "text", "text": "Merits of being balance. Because $B_{0}$ is preserved, SGD is sensitive to initialization. For example, $(\\mathbf{x}_{0},\\mathbf{y}_{0})$ and $(2\\mathbf{x}_{0},0.5\\mathbf{y}_{0})$ can result in extremely different trajectories, although the same objective value is shared at initialization. Most of existing works initialize $B_{0}\\approx0$ to promote optimization benefits, because the variance of stochastic gradient is small and the local curvature is harmonized around a balanced solution. Take the stochastic gradient of NOP on minibatch $\\mathcal{M}$ for example ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{g}_{\\mathbf{x}}=\\frac{1}{|\\mathcal{M}|}\\Big[\\sum_{\\boldsymbol{\\xi}\\in\\mathcal{M}}\\nabla f_{n}^{\\boldsymbol{\\xi}}(\\mathbf{x}\\mathbf{y}^{\\top})\\Big]\\mathbf{y},\\quad\\mathbf{g}_{\\mathbf{y}}=\\frac{1}{|\\mathcal{M}|}\\Big[\\sum_{\\boldsymbol{\\xi}\\in\\mathcal{M}}\\nabla f_{n}^{\\boldsymbol{\\xi}}(\\mathbf{x}\\mathbf{y}^{\\top})\\Big]^{\\top}\\mathbf{x}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Assuming bounded variance $\\begin{array}{r}{\\mathbb{E}[\\|\\frac{1}{|\\mathcal{M}|}\\sum_{\\boldsymbol{\\xi}\\in\\mathcal{M}}\\nabla f_{n}^{\\boldsymbol{\\xi}}(\\mathbf{x}\\mathbf{y}^{\\top})-\\nabla f_{n}(\\mathbf{x}\\mathbf{y}^{\\top})\\|^{2}]\\leq\\sigma^{2}}\\end{array}$ , it can be seen that the variance of $[\\mathbf{g}_{\\mathbf{x}},\\mathbf{g}_{\\mathbf{y}}]$ is bounded by $\\sigma^{2}(||\\mathbf{x}||^{2}+||\\mathbf{y}||^{2})$ . In other words, among $\\{(\\mathbf{x},\\mathbf{y})|\\mathbf{x}\\mathbf{y}^{\\top}=\\mathbf{W}\\}$ , gradient variance is minimized if $\\|\\mathbf{x}\\|\\,=\\,\\|\\mathbf{y}\\|$ , i.e., being balance. Moreover, block smoothness parameters $L_{n}^{\\mathbf{x}}$ and $L_{n}^{\\mathbf{y}1}$ also hint upon the difficulties for optimization, where large values typically correspond to slow convergence (Bottou et al., 2018; Nesterov, 2004). With the help of Assumption 1 (in the next subsection), it can be seen that $L_{n}^{\\mathbf{x}}=L_{n}\\|\\mathbf{y}\\|^{2}$ and $L_{n}^{\\mathbf{y}}=L_{n}\\|\\mathbf{x}\\|^{2}$ . In other words, a large $|\\boldsymbol{\\mathcal{B}}_{t}|$ implies difficulty for optimizing one variable than the other. For these reasons, balancedness is well-appreciated in domains such as matrix factorization/sensing \u2013 a special case of (1a) (Tu et al., 2016; Bartlett et al., 2018; Du et al., 2018; Ge et al., 2017). It is also observed that balanced neural networks are easier to optimize relative to unbalanced ones (Neyshabur et al., 2015). ", "page_idx": 3}, {"type": "text", "text": "2.3 Assumptions and Prerequisites ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To gain theoretical insights of scale-invariant problems in (1), we assume that the loss has Lipschitz continuous gradient on dom $f$ following common nonconvex optimization and SAM analyses (Bottou et al., 2018; Andriushchenko and Flammarion, 2022; Wen et al., 2023a). ", "page_idx": 3}, {"type": "text", "text": "Assumption 1. Let $\\mathbf{W}\\in\\mathbb{R}^{d_{1}\\times d_{2}}$ , and $w\\in\\mathbb{R}$ . For each $\\xi_{i}$ , $f_{n}^{\\xi}(\\mathbf{W})$ and $f_{o}^{\\xi}(w)$ in (1) have $L_{n}$ , and $L_{o}$ Lipschitz continuous gradient, respectively. ", "page_idx": 3}, {"type": "text", "text": "Scale-invariant problems are challenging to solve even on simple problems in Fig. 1. Even GD can diverge on some manually crafted initialization (De Sa et al., 2015; Arora et al., 2019a). With proper hyperparameters this rarely happens in practice; hence, we focus on scenarios where SGD and SAM do not diverge. This assumption is weaker than the global convergence needed in (Andriushchenko and Flammarion, 2022), and is similar to the assumption on existence (Wen et al., 2023a). ", "page_idx": 4}, {"type": "text", "text": "3 SAM for Non-Overparametrized Problems ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "This section tackles the implicit regularization of SAM on NOP (1a). Motivated by practical scenarios such as LoRA, we focus on cases initialized with large $|\\beta_{0}|$ . ", "page_idx": 4}, {"type": "text", "text": "When ambiguity is absent, the subscript in $f_{n}$ and $L_{n}$ is ignored in this section for convenience. Applying Alg. 1 on NOP, the update of SAM can be written as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\mathbf{x}}_{t}=\\mathbf{x}_{t}+\\rho u_{t}\\mathbf{g}_{\\mathbf{x}_{t}},\\quad\\tilde{\\mathbf{y}}_{t}=\\mathbf{y}_{t}+\\rho u_{t}\\mathbf{g}_{\\mathbf{y}_{t}}}\\\\ &{\\mathbf{g}_{\\tilde{\\mathbf{x}}_{t}}=\\nabla f_{t}\\big(\\tilde{\\mathbf{x}}_{t}\\tilde{\\mathbf{y}}_{t}^{\\top}\\big)\\tilde{\\mathbf{y}}_{t},\\quad\\mathbf{g}_{\\tilde{\\mathbf{y}}_{t}}=\\big[\\nabla f_{t}\\big(\\tilde{\\mathbf{x}}_{t}\\tilde{\\mathbf{y}}_{t}^{\\top}\\big)\\big]^{\\top}\\tilde{\\mathbf{x}}_{t}}\\\\ &{\\quad\\mathbf{x}_{t+1}=\\mathbf{x}_{t}-\\eta\\mathbf{g}_{\\tilde{\\mathbf{x}}_{t}},\\quad\\mathbf{y}_{t+1}=\\mathbf{y}_{t}-\\eta\\mathbf{g}_{\\tilde{\\mathbf{y}}_{t}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\rho>0$ is the radius of SAM perturbation; $u_{t}:=1/\\sqrt{\\|\\mathbf{g}_{\\mathbf{x}_{t}}\\|^{2}+\\|\\mathbf{g}_{\\mathbf{y}_{t}}\\|^{2}}$ ; and $f_{t},\\nabla f_{t}$ denote the loss, stochastic gradient on minibatch $\\mathcal{M}_{t}$ , respectively. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2. (Dynamics of SAM.) Suppose that Assumption $^{\\,l}$ holds. Consider SAM for NOP in (4) with a sufficiently small $\\rho$ . Let $\\begin{array}{r}{{\\mathcal{B}}_{t}:=\\frac{\\cdot}{2}\\big(\\|\\mathbf{x}_{t}\\|^{2}-\\|\\mathbf{y}_{t}\\|^{\\hat{2}}\\big)}\\end{array}$ . For some $|\\mathcal{A}_{t}|=\\mathcal{O}(\\rho^{2}L)$ and $\\eta\\rightarrow0$ , the limiting flow of SAM guarantees that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{d\\mathcal{B}_{t}}{d t}=\\rho\\frac{\\|\\mathbf{g}_{\\mathbf{x}_{t}}\\|^{2}-\\|\\mathbf{g}_{\\mathbf{y}_{t}}\\|^{2}}{\\sqrt{\\|\\mathbf{g}_{\\mathbf{x}_{t}}\\|^{2}+\\|\\mathbf{g}_{\\mathbf{y}_{t}}\\|^{2}}}+A_{t}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Moreover, the change on $B_{t}$ depends on the difference of stochastic gradients on $\\mathbf{x}_{t}$ and $\\mathbf{y}_{t}$ , i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\rho\\big|\\|\\mathbf{g}_{\\mathbf{x}_{t}}\\|-\\|\\mathbf{g}_{\\mathbf{y}_{t}}\\|\\big|-\\mathcal{O}(\\rho^{2}L)\\leq\\big|\\frac{d B_{t}}{d t}\\big|\\leq\\rho\\sqrt{\\|\\mathbf{g}_{\\mathbf{x}_{t}}\\|^{2}-\\|\\mathbf{g}_{\\mathbf{y}_{t}}\\|^{2}}\\big|+\\mathcal{O}(\\rho^{2}L).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Unlike SGD for which $\\begin{array}{r}{\\frac{\\mathrm{d}\\mathcal{B}_{t}}{\\mathrm{d}t}=0}\\end{array}$ , Theorem 2 states that the balancedness for SAM is driven by gradient difference $\\lVert\\mathbf{g}_{\\mathbf{x}_{t}}\\rVert^{2}-\\lVert\\mathbf{g}_{\\mathbf{y}_{t}}\\rVert^{2}$ . To gain some intuition, if we estimate $\\|\\mathbf{g}_{\\mathbf{x}_{t}}\\|^{2}-\\|\\mathbf{g}_{\\mathbf{y}_{t}}\\|^{2}\\propto\\|\\mathbf{y}_{t}\\|^{2}-\\|\\mathbf{x}_{t}\\|^{2}$ based on (3) and ignore $\\boldsymbol{A}_{t}$ , it can be seen that $\\begin{array}{r}{\\frac{\\mathrm{d}\\mathcal{B}_{t}}{\\mathrm{d}t}\\propto-\\rho\\mathcal{B}_{t}}\\end{array}$ . This indicates the contraction on $|\\boldsymbol{\\mathcal{B}}_{t}|$ A graphical illustration on decreasing $|\\boldsymbol{\\mathcal{B}}_{t}|$ , and its relation with gradient difference can be found in Figs. 1 (a) and 2 (a). Moreover, this implicit regularization on balancedness is global as it holds for all $t$ regardless of whether $\\left(\\mathbf{x}_{t},\\mathbf{y}_{t}\\right)$ is close to local optima. Thanks to adopting balancedness as the metric, Theorem 2 also poses no requirement on the batchsize. ", "page_idx": 4}, {"type": "text", "text": "SAM promotes balancedness. As discussed in Section 2.2, unbalancedness is burdensome for optimization. SAM overcomes this by implicitly favoring relatively balanced solutions. ", "page_idx": 4}, {"type": "text", "text": "Corollary 1. (Informal.) Under some regularity conditions, there exists $\\bar{\\beta}_{t}^{\\rho}\\geq0$ such that whenever $|B_{t}|>\\bar{B}_{t}^{\\bar{\\rho}}$ , the magnitude of $B_{t}$ shrinks, where $\\mathbf{\\bar{\\mathcal{B}}}_{t}^{\\rho}$ can be found in (21) at appendix. ", "page_idx": 4}, {"type": "text", "text": "Corollary 1 shows that SAM promotes balancedness until $|\\boldsymbol{\\mathcal{B}}_{t}|$ reaches lower bounds $\\bar{\\mathcal{B}}_{t}^{\\rho}$ . Because $\\bar{\\mathcal{B}}_{t}^{\\rho}$ depends on SAM\u2019s trajectory, we plot $\\begin{array}{r}{\\frac{1}{T}\\int_{0}^{T}\\bar{B}_{t}^{\\rho}d t}\\end{array}$ using dotted lines for better visualization in Fig. 2 (a). It can be seen that our calculation on $\\bar{\\mathcal{B}}_{t}^{\\rho}$ almost matches the balancedness of SAM after sufficient convergence. Being balance also reveals that the benefti of SAM can come from optimization, which is a perspective typically ignored in literature. ", "page_idx": 4}, {"type": "text", "text": "Noisy data have stronger impact on balancedness. Although our discussions extend to more general problems, for simplicity we consider the example in Fig. 2 (a), i.e., $\\mathbb{E}[\\|\\mathbf{x}\\mathbf{y}^{\\top}-(\\mathbf{A}+\\alpha\\mathbf{N})\\|^{2}]$ , where $\\mathbf{A}$ is ground truth; $\\mathbf{N}$ is data noise; and $\\alpha$ determines SNR. For this problem, noisy data directly lead to noisy gradients. It can be seen in Fig. 2 (a) that smaller SNR coincides with faster decreasing of $|\\boldsymbol{\\mathcal{B}}_{t}|$ . To explain such a data-responsive behavior in implicit regularization, Theorem 2 states that balancedness changes largely when the difference of $\\|\\mathbf{g}_{\\mathbf{y}_{t}}\\|$ and $\\|{\\bf g}_{{\\bf x}_{t}}\\|$ is large. Since $\\mathbb{E}[||\\mathbf{g}_{\\mathbf{y}_{t}}||^{2}-||\\mathbf{g}_{\\mathbf{x}_{t}}||^{2}]\\propto\\alpha^{2}$ if assuming elements of $\\mathbf{N}$ to be iid unit Gaussian variables, it thus implies that a small SNR (large $\\alpha$ ) offers large regularization on balancedness. ", "page_idx": 4}, {"type": "image", "img_path": "oSOVME9kl2/tmp/6c466f2edb6c2a4cdbfd22eee8225535739a698d10db27e8dc5936d283a61c46.jpg", "img_caption": ["Figure 2: Implicit regularization of SAM on NOP $\\mathbb{E}[\\|\\mathbf{x}\\mathbf{y}^{\\top}-(\\mathbf{A}+\\alpha\\mathbf{N})\\|^{2}]$ , where $\\alpha$ controls SNR. (a) the threshold of balancedness $\\bar{B}_{t}^{\\rho}$ in Corollary 1; (b) implicit vs. explicit regularization. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Extension to LoRA (multi-layer two-variable NOP). For LoRA, the objective is to minimize $D$ blocks of variables simultaneously, i.e., $\\operatorname*{min}\\mathbb{E}_{\\xi}[f^{\\xi}(\\{\\mathbf{x}_{l}\\mathbf{y}_{l}^{\\top}\\}_{l=1}^{D})]$ . It is established in Theorem 5 in appendix that SAM cultivates balancedness in a layer-wise fashion, i.e., the magnitud\u221ae of $\\begin{array}{r}{\\mathcal{B}_{t,l}\\,:=\\,\\frac{1}{2}\\left(\\|\\mathbf{x}_{t,l}\\|^{2}-\\|\\mathbf{y}_{t,l}\\|^{2}\\right)}\\end{array}$ cannot be large for each $l$ . However, the $|\\mathrm{d}\\boldsymbol{B}_{t,l}/\\mathrm{d}t|$ can be $\\mathcal{O}(\\sqrt{D})$ times smaller than Theorem 2 in the worst case because of the additional variables. ", "page_idx": 5}, {"type": "text", "text": "Validation of IR on modern architectures. Going beyond the infinitesimally small step size, we adopt $\\eta=0.1$ on modern language models to validate our theoretical findings. We consider finetuning a RoBERTa-large with LoRA for few-shot learning tasks. More details can be found later in Section 6.1. Balancedness of SAM and SGD on different layers in various datasets are plotted in Fig. 3. SAM has a clear trend of promoting balancedness, aligning well with our theoretical predictions. ", "page_idx": 5}, {"type": "text", "text": "4 SAM for Overparametrized Problems ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Next, we focus on SAM\u2019s implicit regularization on OP (1b). Overparametrization enables SAM to have stronger regularization on balancedness. Subscripts in $f_{o}$ and $L_{o}$ are omitted for convenience. SAM\u2019s per iteration update for OP can be summarized as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\mathbf{x}}_{t}=\\mathbf{x}_{t}+\\rho u_{t}\\mathbf{y}_{t},\\quad\\tilde{\\mathbf{y}}_{t}=\\mathbf{y}_{t}+\\rho u_{t}\\mathbf{x}_{t}}\\\\ &{\\mathbf{g}_{\\tilde{\\mathbf{x}}_{t}}=f_{t}^{\\prime}\\big(\\tilde{\\mathbf{x}}_{t}^{\\top}\\tilde{\\mathbf{y}}_{t}\\big)\\tilde{\\mathbf{y}}_{t},\\ \\ \\mathbf{g}_{\\tilde{\\mathbf{y}}_{t}}=f_{t}^{\\prime}\\big(\\tilde{\\mathbf{x}}_{t}^{\\top}\\tilde{\\mathbf{y}}_{t}\\big)\\tilde{\\mathbf{x}}_{t}}\\\\ &{\\mathbf{x}_{t+1}=\\mathbf{x}_{t}-\\eta\\mathbf{g}_{\\tilde{\\mathbf{x}}_{t}},\\ \\ \\mathbf{y}_{t+1}=\\mathbf{y}_{t}-\\eta\\mathbf{g}_{\\tilde{\\mathbf{y}}_{t}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $u_{t}:=\\mathrm{sgn}(f_{t}^{\\prime}(\\mathbf{x}_{t}^{\\top}\\mathbf{y}_{t}))/\\sqrt{\\|\\mathbf{x}_{t}\\|^{2}+\\|\\mathbf{y}_{t}\\|^{2}}$ ; $f_{t}$ and $f_{t}^{\\prime}$ denote the loss, stochastic gradient on minibatch $\\mathcal{M}_{t}$ , respectively. Different from NOP, SAM has stronger regularization on balancedness, where $|\\boldsymbol{\\mathcal{B}}_{t}|$ decreases whenever the norm of stochastic gradient is large. To see this, it is convenient to define $\\mathcal{C}_{t}:=|\\|\\mathbf{x}_{t}\\|-\\|\\mathbf{y}_{t}\\||$ . Note that $\\mathcal{C}_{t}\\leq\\sqrt{2|\\mathcal{B}_{t}|}$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 3. Consider $\\eta\\rightarrow0$ for (7). The limiting flow of SAM on $O P$ ensures a decreasing magnitude of $B_{t}$ whenever $|f_{t}^{\\prime}(\\mathbf{x}_{t}^{\\top}\\mathbf{y}_{t})|\\cdot\\bar{C}_{t}>\\mathcal{O}(\\rho L|\\mathcal{B}_{t}|)$ . Moreover, the speed of decrease can be lower- and upper- bounded as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\rho|f_{t}^{\\prime}(\\mathbf{x}_{t}^{\\top}\\mathbf{y}_{t})|\\cdot\\mathcal{C}_{t}-\\mathcal{O}(\\rho^{2}L|\\mathcal{B}_{t}|)\\leq|\\frac{d\\mathcal{B}_{t}}{d t}|\\leq\\rho|f_{t}^{\\prime}(\\mathbf{x}_{t}^{\\top}\\mathbf{y}_{t})|\\sqrt{2|\\mathcal{B}_{t}|}+\\mathcal{O}(\\rho^{2}L|\\mathcal{B}_{t}|).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Given $\\rho\\rightarrow0$ and sufficiently noisy data, Theorem 3 implies that $|B_{t}|\\to0$ . Moreover, Theorem 3 also states that the regularization power on balancedness is related to both gradient norm and balancedness itself. The elbow-shaped curve of $|{\\boldsymbol{{B}}}_{t}|$ in Fig. 1 (b) demonstrates that the regularization power is reducing, as both gradient norm and balancedness shrink over time. ", "page_idx": 5}, {"type": "text", "text": "Noisy data have stronger impact on balancedness. As shown in Fig. 1 (b), balancedness is promoted faster on problems with lower SNR. This data-responsive behavior can be already seen from Theorem 3, because $|\\mathrm{d}\\boldsymbol{B}_{t}/\\mathrm{d}t|$ is directly related with $|f_{t}^{\\bar{\\prime}}(\\mathbf{x}_{t}^{\\top}\\mathbf{y}_{t})|$ , and $\\mathbb{E}[|f_{t}^{\\prime}(\\mathbf{x}_{t}^{\\top}\\mathbf{y}_{t})|]$ is clearly larger when data are more noisy. In other words, SAM exploits noisy data for possible optimization merits from balancedness (see discussions in Sec. 2.2). Overall, the implicit regularization on balancedness aligns well with the empirical observations in presence of data anomalies (Wang et al., 2023; Sherborne et al., 2023), where SAM outperforms SGD by a large margin. ", "page_idx": 5}, {"type": "image", "img_path": "oSOVME9kl2/tmp/8faa930fc13c267d11d2a1fad2982afb029d0fd5c21c87a2fcb355c48f4e4c4e.jpg", "img_caption": ["Figure 3: Implicit regularization of SAM on LoRA. We consider few shot learning with LoRA on a RoBERTa-large. For datasets RTE, SST-5, and MNLI, 1st, 12th and 24th query layers\u2019 $2|\\beta_{t,l}|$ are plotted, respectively. The layers are chosen to represent early, middle, and final stages of RoBERTa. The averaged $\\bar{\\mathcal{B}}_{t,l}^{\\rho}$ in Corollary 1 is 0.37, 0.21, and 0.29, respectively. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Extension to $m$ -sharpness. $m$ -sharpness is a variant of SAM suitable for distributed training. It is observed to empirically improve SAM\u2019s performance (Foret et al., 2021). $m$ -sharpness evenly divides minibatch $\\mathcal{M}_{t}$ into $m$ disjoint subsets, i.e., $\\{f_{t,j}\\}_{j=1}^{m}$ , and perform SAM update independently on each subset; see (38) in appendix. It turns out that $m$ -sharpness can also be explained using balancedness. With formal proofs in Apdx. C.3, the IR of $m$ -sharpness amounts to substitute $\\left|{f_{t}^{\\prime}}(\\mathbf{x}_{t}^{\\top}\\mathbf{y}_{t})\\right|$ in Theorem 3 with $\\begin{array}{r}{\\frac{1}{m}\\sum_{j=1}^{m}|f_{t,j}^{\\prime}(\\mathbf{x}_{t}^{\\top}\\mathbf{y}_{t})|}\\end{array}$ . This means that the regularization on balancedness from $m$ -sharpness is more profound than vanilla SAM, because $\\begin{array}{r}{\\frac{1}{m}\\sum_{j=1}^{m}|f_{t,j}^{\\prime}(\\mathbf{x}_{t}^{\\top}\\mathbf{y}_{t})|\\ge|f_{t}^{\\prime}(\\mathbf{x}_{t}^{\\top}\\mathbf{y}_{t})|}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "Finally, we connect balancedness with sharpness on local minima of OP. ", "page_idx": 6}, {"type": "text", "text": "Lemma 1. Let $\\mathcal{W}^{*}\\,=\\,\\{({\\bf x},{\\bf y})|{\\bf x}^{\\top}{\\bf y}\\,=\\,w,f^{\\prime}(w)\\,=\\,0,f^{\\prime\\prime}(w)\\,>\\,0\\}$ be non-empty. For the OP problem (1b), minimizing sharpness within $\\mathcal{W}^{*}$ is equivalent to finding $B=0$ in $\\mathcal{W}^{*}$ . ", "page_idx": 6}, {"type": "text", "text": "This link showcases that by studying balancedness we can also obtain the implicit regularization on sharpness for free. A concurrent work also links balancedness with sharpness (the largest eigenvalue) for some one-hidden layer neural networks (Singh and Hofmann, 2024). Compared with (Wen et al., 2023a), this is achieved with less assumptions and simplified analyses. More importantly, balancedness enables us to cope with arbitrary batchsize, to explain SAM\u2019s stronger regularization with noisy data, and to extend results to $m$ -sharpness. ", "page_idx": 6}, {"type": "text", "text": "5 Implicit Regularization Made Explicit ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Next, insights from our theoretical understanding of SAM are leveraged to build practical tools. We adopt LoRA (Hu et al., 2022) as our major numerical benchmark for scale-invariant problems given its prevalence in practice. More diverse examples on both OP and NOP can be found in Apdx. A.3. Compared to full parameter-tuning, LoRA is more economical in terms of memory not only for finetuning, but also for serving multiple downstream tasks. LoRA and its variants are actively developed and well welcomed by the community; see e.g., HuggingFace\u2019s PEFT codebase.2 ", "page_idx": 6}, {"type": "text", "text": "5.1 Overview of LoRA ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Given a pretrained model with frozen weight $\\mathbf{W}_{l}\\in\\mathbb{R}^{d_{1}\\times d_{2}}$ on a particular layer $l$ , the objective of LoRA is to find low rank matrices $\\mathbf{X}_{l}\\in\\bar{\\mathbb{R}}^{d_{1}\\times r}$ , and $\\mathbf{Y}_{l}\\in\\mathbb{R}^{d_{2}\\times r}$ with $r\\ll\\operatorname*{min}\\{d_{1},d_{2}\\}$ such that the loss is minimized for a downstream task, i.e., ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\{\\mathbf{X}_{l},\\mathbf{Y}_{l}\\}_{l}}\\mathcal{L}\\big(\\{\\mathbf{W}_{l}+\\mathbf{X}_{l}\\mathbf{Y}_{l}^{\\top}\\}_{l}\\big).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "LoRA enjoys parameter efficiency for finetuning thanks to the low-rank matrices $\\mathbf{X}_{l}$ and $\\mathbf{Y}_{l}$ . For instance, it only requires $0.8\\mathrm{M}$ trainable parameters to finetune a $355\\mathrm{M}.$ -parameter RoBERTa-large (Hu et al., 2022). The outer product of $\\mathbf{X}_{l}$ and $\\mathbf{Y}_{l}$ induces scale-invariance, and the number of variables renders it NOP. The downside of LoRA, on the other hand, is the drop on test performance due to the parsimony on trainable parameters. Unbalancedness is also unavoidable for LoRA, due to the need of initializing at $\\mathbf{X}_{l}\\sim\\mathcal{N}(0,\\sigma^{2}),\\mathbf{Y}_{l}=\\mathbf{0}$ ; see an example of RoBERTa-large in Fig. 3. The unbalancedness leads to instability of LoRA when finetuning RoBERTa on datasets SST-2 and MNLI; see more details in Apdx. D.4. ", "page_idx": 6}, {"type": "table", "img_path": "oSOVME9kl2/tmp/513dbdacdafa83315bab35642c4d99ee72ee6eb3d9e3ff8ecd5625e454aa761f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Integrating SAM with LoRA is a case with mutual benefits \u2013 LoRA reduces the additional memory requirement of SAM, while SAM not only overcomes the distributional shift in finetuning (Zhou et al., 2022), but also mitigates the possible inefficiency associated with LoRA\u2019s unbalancedness. ", "page_idx": 7}, {"type": "text", "text": "5.2 Balancedness-Aware Regularization (BAR) ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "However, directly applying SAM variants on LoRA exhibits two concerns: i) SAM doubles computational cost due to the need of two gradients; and ii) additional efforts are required to integrate SAM with gradient accumulation and low-precision training (HuggingFace), which are common techniques for memory and runtime efficiency in large-scale finetuning. Note that concern i) is annoying given the size of language models, especially in setups involving model parallelism. ", "page_idx": 7}, {"type": "text", "text": "Our balancedness-aware regularization (BAR) is a highly efficient approach to address both concerns, and it fixes the accuracy drop of LoRA relative to full-parameter finetuning. BAR is also the first efficient SAM variant derived from implicit regularization. The key observation for our algorithm design is that SAM\u2019s implicit regularization on balancedness can be achieved with an explicit regularizer $\\alpha_{t}|\\mathbf{x}^{\\top}\\mathbf{x}-\\mathbf{y}^{\\top}\\mathbf{\\dot{y}}|$ . This regularizer originates from matrix sensing; see e.g., (Tu et al., 2016; Ge et al., 2017). For OP, choosing $\\alpha_{t}:=\\mathcal{O}(|f^{\\prime}(\\mathbf{x}_{t}^{\\top}\\mathbf{y}_{t})|/\\sqrt{\\|\\mathbf{x}_{t}\\|^{2}+\\|\\mathbf{y}_{t}\\|^{2}})$ recovers SAM\u2019s dynamic on $B_{t}$ up to an error of ${\\mathcal{O}}(\\rho^{2})$ ; cf. Lemma 2 in appendix. By ignoring this error, it can be seen that $B_{t}$ decreases when $\\left\\|\\mathbf{x}_{t}\\right\\|\\geq\\left\\|\\mathbf{y}_{t}\\right\\|$ . Following this dynamic, we regulate balancedness based on whether $\\|\\mathbf x_{t}\\|\\geq\\|\\mathbf y_{t}\\|$ . The resultant approach is termed as overparamterized BAR (oBAR) to reflect its source in OP. ", "page_idx": 7}, {"type": "text", "text": "On the other hand, because LoRA is NOP inherently, we take inspiration from Theorem 2 \u2013 dropping the term $A_{t}$ and mimicking dynamics of SAM. In particular, we regulate the objective with $\\alpha_{t}(\\mathbf{x}^{\\top}\\mathbf{x}-$ $\\mathbf{y}^{\\top}\\mathbf{y})$ if $\\|\\mathbf{g}_{\\mathbf{x}_{t}}\\|^{2}<\\|\\mathbf{g}_{\\mathbf{y}_{t}}\\|^{2}$ ; otherwise $\\alpha_{t}(\\mathbf{y}^{\\top}\\mathbf{y}-\\mathbf{\\bar{x}}^{\\top}\\mathbf{x})$ . The resultant approach is termed as nBAR. A graphical illustration can be found in Fig. 2 (b). It can be observed that nBAR shares similar performance as SAM on NOP. Both nBAR and oBAR can be implemented in the same manner as weight decay, and their detailed steps are summarized in Algs. 2 and 3, respectively. ", "page_idx": 7}, {"type": "text", "text": "Another benefit of BAR, in additional to the lightweight computation, is that it can be applied individually on each LoRA layer. As previously discussed (cf. Theorem 5), the number of layers has a negative impact on balancedness. By overcoming this \u201ccurse of multi-layer\u201d, BAR can induce better test performance over SAM. ", "page_idx": 7}, {"type": "text", "text": "Schedule of $\\alpha_{t}$ . In both nBAR and oBAR, one can employ a decreasing scheduler for $\\alpha_{t}$ for algorithmic flexibility. This is motivated by the fact that for both NOP and OP problems, the implicit regularization of SAM is less powerful after sufficient balancedness or near optimal. Commonly adopted cosine and linear schedules work smoothly. ", "page_idx": 7}, {"type": "table", "img_path": "oSOVME9kl2/tmp/75fcbc6ddbea5c6222fd189f6fefa99fd2cfd43156f166f68649cbd610c43231.jpg", "table_caption": ["Table 1: Few shot learning on RoBERTa (355M). \u2020 denotes results reported by (Malladi et al., 2023) "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 2: Runtime of BAR (normalized to LoRA, 1x) on OPT-1.3B. SAM relies on FP32 for stability. LoRA and BAR adopt FP16 training since this is the default choice for large models. nBAR and oBAR share similar runtime, hence reported together. ", "page_idx": 8}, {"type": "table", "img_path": "oSOVME9kl2/tmp/b6d07f750b0c2963598388bde0a1835bd3ed40eeb81b3d6c18bb56e3ffe1acc8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "6 Numerical Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To demonstrate the effectiveness of BAR, numerical experiments are conducted on various deep learning tasks using language models (LMs). Bold and underlined numbers are used to highlight the best and second best performance, respectively. More experimental details can be found in Apdx. D. Code is available at https://github.com/BingcongLi/BAR. ", "page_idx": 8}, {"type": "text", "text": "6.1 Few-shot Learning with RoBERTa-large and OPT-1.3B ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The first task to consider is few-shot learning with LoRA (Malladi et al., 2023), where the goal is to finetune a language model with a small training set. We follow the settings in (Malladi et al., 2023), and choose the backbones as RoBERTa-large, a masked LM with 355M parameters, and OPT-1.3B, an autoregressive LM (Liu et al., 2019; Zhang et al., 2022). ", "page_idx": 8}, {"type": "text", "text": "Results of the proposed oBAR and nBAR on RoBERTa-large are summarized in Table 1. As indicated by the zero-shot performance, the distributional shift between finetuning and pretraining datasets is obvious. This is a natural setting suitable for SAM and BAR. The averaged test accuracy is improved by 0.9 and 1.2 via oBAR and nBAR, respectively. The performance of nBAR is close to SAM. Moreover, BAR saves $74\\%$ additional runtime of SAM; see more details in Table 7 in the appendix. ", "page_idx": 8}, {"type": "text", "text": "The proposed nBAR and oBAR perform even better when scaling up to OPT-1.3B. BAR reduces the overhead of SAM by more than $95\\%$ because of its compatibility with FP16 training; see Table 2. Note that applying FP16 directly with SAM leads to underflow; see more in Apdx. D. This signifies the flexibility of BAR over SAM when scaling to large problems, as FP16 is the default choice for LMs. Prefix tuning (Li and Liang, 2021) is also included as a benchmark for comparisons on test performance. We report F1 score for SQuAD and accuracy for other datasets in Table 3. The averaged improvement over LoRA is 0.9 and 1.6 from oBAR and nBAR, respectively, both outperforming SAM. We conjecture that the performance gap between SAM and BAR comes from their different effectiveness in regularizing balancedness. Balancedness of a particular layer is decreasing slower in SAM due to multiple layers, as shown in Theorem 5, while BAR promotes balancedness faster as it can be applied individually on each LoRA layer. Comparing the absolute improvement for RoBERTa-large (355M) and OPT-1.3B, it is conjectured that BAR has more potential for larger models, and the verification is left for future due to hardware constraints. ", "page_idx": 8}, {"type": "text", "text": "6.2 Finetuning with RoBERTa-large ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Having demonstrated the power of BAR in few-shot learning, we then apply it to finetune RoBERTalarge with LoRA. The results can be found in Table 4. It can be observed that nBAR and oBAR improve the performance of LoRA and prefix tuning (Li and Liang, 2021) on most of tested datasets. ", "page_idx": 8}, {"type": "table", "img_path": "oSOVME9kl2/tmp/25e4bdb0a88f4079d78453173849a0706c1a532ce95ae5ecfd4e2261bd14becb.jpg", "table_caption": ["Table 3: Performance of BAR for few shot learning using OPT-1.3B. "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "oSOVME9kl2/tmp/8dc19a8ffe7ca053090d776c41fdef38fce4ba5d0a5980194fcc935ab1a0e3b8.jpg", "table_caption": ["Table 4: Finetuning RoBERTa (355M) with BAR. Results marked with $\\dagger$ are taken from (Hu et al., 2022), and those with $^*$ refer to AdapterP in (Hu et al., 2022). "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "oSOVME9kl2/tmp/862c02eb4959fed93b9ec85411c28182409a2b86751676917a70da0ddbd578d7.jpg", "table_caption": ["Table 5: Finetuning GPT2 (345M) with BAR on WebNLG. Results of prefix tuning and full-parameter finetuning are obtained from (Hu et al., 2022). "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "On average, oBAR leads to a gain of 0.4, and nBAR raises the test performance by 0.6. BAR thereby fills the gap of test performance between LoRA (0.8M) and full-parameter (355M) finetuning. ", "page_idx": 9}, {"type": "text", "text": "6.3 Text Generation on GPT2-medium ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Lastly, we consider BAR on a text-generation problem using GPT2-medium, a model with 345M parameters. Results on WebNLG (Gardent et al., 2017) are reported in Table 5. It can be seen that oBAR matches the performance of prefix tuning, while nBAR achieves the best BLEU score. ", "page_idx": 9}, {"type": "text", "text": "7 Discussions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work provides theoretical and empirical evidence on the implicit regularization of SAM for both scale-invariant NOP and OP problems. Balancedness, as an alternative to commonly adopted sharpness, is employed as the metric to capture global and data-responsive behaviors of SAM. We find that i) SAM promotes variables to have (relatively) balanced norms; and ii) noisy data have stronger impact on balancedness. Lastly, we explicify the implicit regularization as a data-driven regularizer to foster the design of a computationally efficient SAM variant, termed BAR. The effectiveness of BAR is demonstrated using various tasks on RoBERTa-large, GPT2 and OPT. BAR saves $95\\%$ overhead of SAM and enhances the accuracy of LoRA to the level of full-parameter finetuning. ", "page_idx": 9}, {"type": "text", "text": "Limitation and Future directions. Our approach, BAR, is best applied on scale-invariant modules in neural networks. Finetuning language models with LoRA, as a popular option in practice, is a setting naturally suitable for our approach. However, our approach does not apply for linear models, e.g., logistic regression. Regarding future directions, an interesting one is whether SAM has other forms of implicit regularization beyond balancedness and sharpness. The exploration of other scale-invariant architectures beyond LoRA, e.g., the softmax function in attention, is also deferred to future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank anonymous reviewers for their suggestions. BL is supported by Swiss National Science Foundation (SNSF) Project Funding No. 200021-207343. LZ gratefully acknowledges funding by the Max Planck ETH Center for Learning Systems (CLS). NH is supported by ETH research grant funded through ETH Zurich Foundations and SNSF Project Funding No. 200021-207343. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Momin Abbas, Quan Xiao, Lisha Chen, Pin-Yu Chen, and Tianyi Chen. Sharp-MAML: Sharpnessaware model-agnostic meta learning. In Proc. Int. Conf. Machine Learning, pages 10\u201332. PMLR, 2022.   \nAtish Agarwala and Yann Dauphin. SAM operates far from home: eigenvalue regularization as a dynamical phenomenon. In Proc. Int. Conf. Machine Learning, pages 152\u2013168. PMLR, 2023.   \nKwangjun Ahn, S\u00e9bastien Bubeck, Sinho Chewi, Yin Tat Lee, Felipe Suarez, and Yi Zhang. Learning threshold neurons via edge of stability. In Proc. Adv. Neural Info. Processing Systems, volume 36, 2023.   \nMaksym Andriushchenko and Nicolas Flammarion. Towards understanding sharpness-aware minimization. In Proc. Int. Conf. Machine Learning, pages 639\u2013668. PMLR, 2022.   \nSanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit acceleration by overparameterization. In Proc. Int. Conf. Machine Learning, pages 244\u2013253. PMLR, 2018.   \nSanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient descent for deep linear neural networks. In Proc. Int. Conf. Learning Represention, 2019a.   \nSanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix factorization. In Proc. Adv. Neural Info. Processing Systems, volume 32, 2019b.   \nSanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In Proc. Int. Conf. Machine Learning, pages 322\u2013332. PMLR, 2019c.   \nSanjeev Arora, Zhiyuan Li, and Abhishek Panigrahi. Understanding gradient descent on the edge of stability in deep learning. In Proc. Int. Conf. Machine Learning, pages 948\u20131024. PMLR, 2022.   \nDara Bahri, Hossein Mobahi, and Yi Tay. Sharpness-aware minimization improves language model generalization. In Proc. Conf. Assoc. Comput. Linguist. Meet., pages 7360\u20137371, 2022.   \nDavid Barrett and Benoit Dherin. Implicit gradient regularization. In Proc. Int. Conf. Learning Represention, 2021.   \nPeter Bartlett, Dave Helmbold, and Philip Long. Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks. In Proc. Int. Conf. Machine Learning, pages 521\u2013530. PMLR, 2018.   \nPeter Bartlett, Philip Long, and Olivier Bousquet. The dynamics of sharpness-aware minimization: Bouncing across ravines and drifting towards wide minima. J. Mach. Learn. Res., 24(316):1\u201336, 2023.   \nL\u00e9on Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. SIAM Review, 60(2):223\u2013311, 2018.   \nSamuel Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large annotated corpus for learning natural language inference. In Proc. Conf. Empir. Methods Nat. Lang. Process., pages 632\u2013642, 2015.   \nDaniel Cer, Mona Diab, Eneko Agirre, I\u00f1igo Lopez-Gazpio, and Lucia Specia. SemEval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. In Proc. Int. Workshop Semant. Eval., pages 1\u201314. ACL, 2017.   \nPratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-SGD: Biasing gradient descent into wide valleys. In Proc. Int. Conf. Learning Represention, 2017.   \nXiangning Chen, Cho-Jui Hsieh, and Boqing Gong. When vision transformers outperform ResNets without pre-training or strong data augmentations. In Proc. Int. Conf. Learning Represention, 2022.   \nYukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. LongLoRA: Efficient fine-tuning of long-context large language models. In Proc. Int. Conf. Learning Represention, 2024.   \nZixiang Chen, Junkai Zhang, Yiwen Kou, Xiangning Chen, Cho-Jui Hsieh, and Quanquan Gu. Why does sharpness-aware minimization generalize better than SGD? In Proc. Adv. Neural Info. Processing Systems, volume 36, 2023.   \nYan Dai, Kwangjun Ahn, and Suvrit Sra. The crucial role of normalization in sharpness-aware minimization. In Proc. Adv. Neural Info. Processing Systems, volume 36, 2023.   \nMarie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The CommitmentBank: Investigating projection in naturally occurring discourse. Proc. Sinn und Bedeutung, 23(2):107\u2013 124, 2019.   \nChristopher De Sa, Christopher Re, and Kunle Olukotun. Global convergence of stochastic gradient descent for some non-convex matrix problems. In Proc. Int. Conf. Machine Learning, pages 2332\u20132341. PMLR, 2015.   \nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA: Efficient finetuning of quantized LLMs. In Proc. Adv. Neural Info. Processing Systems, volume 36, 2023.   \nLaurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep nets. In Proc. Int. Conf. Machine Learning, pages 1019\u20131028. PMLR, 2017.   \nBill Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Proc. Int. Workshop Paraphrasing, 2005.   \nJiawei Du, Hanshu Yan, Jiashi Feng, Joey Tianyi Zhou, Liangli Zhen, Rick Siow Mong Goh, and Vincent Y. F. Tan. Efficient sharpness-aware minimization for improved training of neural networks. In Proc. Int. Conf. Learning Represention, 2022a.   \nJiawei Du, Daquan Zhou, Jiashi Feng, Vincent Y. F. Tan, and Joey Tianyi Zhou. Sharpness-aware training for free. In Proc. Adv. Neural Info. Processing Systems, 2022b.   \nSimon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. In Proc. Adv. Neural Info. Processing Systems, volume 31, 2018.   \nGintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. In Proc. Conf. Uncerntainty in Artif. Intel., 2017.   \nPierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. In Proc. Int. Conf. Learning Represention, 2021.   \nClaire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. The WebNLG challenge: Generating text from RDF data. In Proc. Int. Conf. Nat. Lang. Gener., pages 124\u2013133. ACL, 2017.   \nRong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A unified geometric analysis. In Proc. Int. Conf. Machine Learning, pages 1233\u20131242. PMLR, 2017.   \nGauthier Gidel, Francis Bach, and Simon Lacoste-Julien. Implicit regularization of discrete gradient dynamics in linear neural networks. In Proc. Adv. Neural Info. Processing Systems, volume 32, 2019.   \nAntoine Gonon, Nicolas Brisebarre, Elisa Riccietti, and R\u00e9mi Gribonval. A path-norm toolkit for modern networks: consequences, promises and challenges. In Proc. Int. Conf. Learning Represention, 2024.   \nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In Proc. Int. Conf. Machine Learning, pages 2790\u20132799. PMLR, 2019.   \nEdward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In Proc. Int. Conf. Learning Represention, 2022.   \nHuggingFace. Gradient accumulation. URL https://huggingface.co/docs/accelerate/en/ usage_guides/gradient_accumulation.   \nPavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry P. Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. In Proc. Conf. Uncerntainty in Artif. Intel., pages 876\u2013885, 2018.   \nStanis\u0142aw Jastrze\u02dbbski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amos Storkey. Three factors influencing minima in SGD. arXiv:1711.04623, 2017.   \nZiwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. In Proc. Int. Conf. Learning Represention, 2019.   \nWeisen Jiang, Hansi Yang, Yu Zhang, and James Kwok. An adaptive policy to employ sharpnessaware minimization. In Proc. Int. Conf. Learning Represention, 2023.   \nYiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic generalization measures and where to find them. In Proc. Int. Conf. Learning Represention, 2020.   \nNitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In Proc. Int. Conf. Learning Represention, 2016.   \nMinyoung Kim, Da Li, Shell Xu Hu, and Timothy M. Hospedales. Fisher SAM: Information geometry and sharpness aware minimisation. In Proc. Int. Conf. Machine Learning, pages 11148\u201311161, 2022.   \nDawid Jan Kopiczko, Tijmen Blankevoort, and Yuki M Asano. VeRA: Vector-based random matrix adaptation. In Proc. Int. Conf. Learning Represention, 2024.   \nJungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. ASAM: Adaptive sharpnessaware minimization for scale-invariant learning of deep neural networks. In Proc. Int. Conf. Machine Learning, pages 5905\u20135914. PMLR, 2021.   \nBingcong Li and Georgios B Giannakis. Enhancing sharpness-aware optimization through variance suppression. In Proc. Adv. Neural Info. Processing Systems, volume 36, 2023.   \nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proc. Conf. Assoc. Comput. Linguist. Meet., pages 4582\u20134597, 2021.   \nZhiyuan Li, Tianhao Wang, and Sanjeev Arora. What happens after SGD reaches zero loss? \u2013 A mathematical framework. In Proc. Int. Conf. Learning Represention, 2022.   \nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692, 2019.   \nYong Liu, Siqi Mai, Xiangning Chen, Cho-Jui Hsieh, and Yang You. Towards efficient and scalable sharpness-aware minimization. In Proc. Conf. Computer Vision and Pattern Recognition, pages 12350\u201312360, 2022.   \nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Proc. Int. Conf. Learning Represention, 2019.   \nKaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks. In Proc. Int. Conf. Learning Represention, 2020.   \nSadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D. Lee, Danqi Chen, and Sanjeev Arora. Fine-tuning language models with just forward passes. In Proc. Adv. Neural Info. Processing Systems, volume 36, 2023.   \nPeng Mi, Li Shen, Tianhe Ren, Yiyi Zhou, Xiaoshuai Sun, Rongrong Ji, and Dacheng Tao. Make sharpness-aware minimization stronger: A sparsified perturbation approach. In Proc. Adv. Neural Info. Processing Systems, volume 35, 2022.   \nYurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer Science & Business Media, 2004.   \nBehnam Neyshabur, Russ R Salakhutdinov, and Nati Srebro. Path-SGD: Path-normalized optimization in deep neural networks. In Proc. Adv. Neural Info. Processing Systems, volume 28, 2015.   \nBehnam Neyshabur, Srinadh Bhojanapalli, David Mcallester, Nathan Srebro, and Nati Srebro. Exploring generalization in deep learning. In Proc. Adv. Neural Info. Processing Systems, volume 30, pages 5947\u20135956, 2017.   \nBehnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-bayesian approach to spectrally-normalized margin bounds for neural networks. In Proc. Int. Conf. Learning Represention, 2018.   \nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: $100{,}000{+}$ questions for machine comprehension of text. In Proc. Conf. Empir. Methods Nat. Lang. Process., pages 2383\u20132392, 2016.   \nPranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don\u2019t know: Unanswerable questions for SQuAD. In Proc. Conf. Assoc. Comput. Linguist. Meet., pages 784\u2013789, 2018.   \nMelissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In AAAI Spring Symposium Series, 2011.   \nHeejune Sheen, Siyu Chen, Tianhao Wang, and Harrison H Zhou. Implicit regularization of gradient flow on one-layer softmax attention. arXiv preprint arXiv:2403.08699, 2024.   \nTom Sherborne, Naomi Saphra, Pradeep Dasigi, and Hao Peng. TRAM: Bridging trust regions and sharpness aware minimization. In Proc. Int. Conf. Learning Represention, 2023.   \nDongkuk Si and Chulhee Yun. Practical sharpness-aware minimization cannot converge all the way to optima. In Proc. Adv. Neural Info. Processing Systems, volume 36, 2023.   \nSidak Pal Singh and Thomas Hofmann. Closed form of the hessian spectrum for some neural networks. In High-dimensional Learning Dynamics 2024: The Emergence of Structure and Reasoning, 2024.   \nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew $\\mathbf{Y}\\mathbf{N}\\mathbf{g}$ , and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proc. Conf. Empir. Methods Nat. Lang. Process., pages 1631\u20131642, 2013.   \nBehrooz Tahmasebi, Ashkan Soleymani, Dara Bahri, Stefanie Jegelka, and Patrick Jaillet. A universal class of sharpness-aware minimization algorithms. arXiv preprint arXiv:2406.03682, 2024.   \nStephen Tu, Ross Boczar, Max Simchowitz, Mahdi Soltanolkotabi, and Ben Recht. Low-rank solutions of linear matrix equations via procrustes flow. In Proc. Int. Conf. Machine Learning, pages 964\u2013973. PMLR, 2016.   \nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proc. Adv. Neural Info. Processing Systems, volume 30, 2017.   \nEllen M Voorhees and Dawn M Tice. Building a question answering test collection. In Proc. Annu. Int. ACM SIGIR Conf. Res. Dev. Inf. Retr., pages 200\u2013207, 2000.   \nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. In Proc. Adv. Neural Info. Processing Systems, volume 32, 2019a.   \nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proc. Int. Conf. Learning Represention, 2019b.   \nPengfei Wang, Zhaoxiang Zhang, Zhen Lei, and Lei Zhang. Sharpness-aware gradient matching for domain generalization. In Proc. Conf. Computer Vision and Pattern Recognition, pages 3769\u20133778, 2023.   \nZiqiao Wang and Yongyi Mao. On the generalization of models trained with SGD: Informationtheoretic bounds and implications. In Proc. Int. Conf. Learning Represention, 2022.   \nAlex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments. Trans. Assoc. Comput. Linguist., 7:625\u2013641, 2019.   \nKaiyue Wen, Tengyu Ma, and Z hiyuan Li. How does sharpness-aware minimization minimizes sharpness. In Proc. Int. Conf. Learning Represention, 2023a.   \nKaiyue Wen, Tengyu Ma, and Zhiyuan Li. Sharpness minimization algorithms do not only minimize sharpness to achieve better generalization. In Proc. Adv. Neural Info. Processing Systems, volume 36, 2023b.   \nAdina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proc. Conf. North Am. Chapter Assoc. Comput. Linguist., pages 1112\u20131122, 2018.   \nBlake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. In Proc. Annual Conf. Learning Theory, pages 3635\u20133673. PMLR, 2020.   \nDongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust generalization. In Proc. Adv. Neural Info. Processing Systems, volume 33, pages 2958\u20132969, 2020.   \nWenhan Xia, Chengwei Qin, and Elad Hazan. Chain of LoRA: Efficient fine-tuning of language models via residual learning. arXiv preprint arXiv:2401.04151, 2024.   \nQingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. In Proc. Int. Conf. Learning Represention, 2023a.   \nRuipeng Zhang, Ziqing Fan, Jiangchao Yao, Ya Zhang, and Yanfeng Wang. Domain-inspired sharpness aware minimization under domain shifts. In Proc. Int. Conf. Learning Represention, 2023b.   \nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. ReCoRD: Bridging the gap between human and machine commonsense reading comprehension. arXiv preprint arXiv:1810.12885, 2018.   \nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.   \nYang Zhao, Hao Zhang, and Xiuyuan Hu. Penalizing gradient norm for efficiently improving generalization in deep learning. In Proc. Int. Conf. Machine Learning, pages 26982\u201326992, 2022.   \nWenxuan Zhou, Fangyu Liu, Huan Zhang, and Muhao Chen. Sharpness-aware minimization with dynamic reweighting. In Proc. Conf. Empir. Methods Nat. Lang. Process., pages 5686\u20135699, 2022.   \nJuntang Zhuang, Boqing Gong, Liangzhe Yuan, Yin Cui, Hartwig Adam, Nicha Dvornek, Sekhar Tatikonda, James Duncan, and Ting Liu. Surrogate gap minimization improves sharpness-aware training. In Proc. Int. Conf. Learning Represention, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Supplementary Document for ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "\u201cImplicit Regularization of Sharpness-Aware Minimization for Scale-Invariant Problems\u201d ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Broad Impact ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The theories and approaches are applicable across various scenarios. The proposed algorithmic tool simplifies finetuning language models, improves performance of downstream tasks, and consumes less resource compared to SAM. For tasks such as sentiment classification, our approach facilitates real world systems such as recommendation by improving accuracy. However, caution is advised when the downstream tasks of language models involve generation. For these tasks, users should thoroughly review generated content and consider to implement gating methods to ensure safety and trustworthiness. ", "page_idx": 15}, {"type": "text", "text": "A.2 More on Related Work ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Sharpness and generalization. Sharpness is observed to relate with generalization of SGD in deep learning (Keskar et al., 2016). It is found that sharpness varies with the ratio between learning rate and batchsize in SGD (Jastrze\u02dbbski et al., 2017). Large scale experiments also indicate sharpness-based measures align with generalization in practical scenarios (Jiang et al., 2020; Chen et al., 2022). Theoretical understandings on generalization error using sharpness-related metrics can be found in e.g., (Dziugaite and Roy, 2017; Neyshabur et al., 2017; Wang and Mao, 2022). There is a large body of literature exploring sharpness for improved generalization. Entropy SGD leverages local entropy in search of a flat valley (Chaudhari et al., 2017). A similar approach as SAM is also developed in (Wu et al., 2020) while putting more emphases on adversarial robustness. Stochastic weight averaging is proposed for finding flatter minima in (Izmailov et al., 2018). It is shown later in (Wen et al., 2023b) that the interplay between sharpness and generalization subtly depends on data distributions and model architectures, and there are unveiled reasons beyond sharpness for the benefit of SAM. ", "page_idx": 15}, {"type": "text", "text": "SAM variants. Although SAM is successful in various deep learning tasks, it can be improved further by leveraging local geometry in a fine-grained manner. For example, results in (Zhao et al., 2022; Barrett and Dherin, 2021) link SAM with gradient norm penalization. Zhuang et al. (2022) optimize sharpness gap and training loss jointly. A more accurate manner to solve inner maximization in SAM is developed in (Li and Giannakis, 2023). SAM and its variants are also widely applied to domain generalization problems; see e.g., (Zhang et al., 2023b; Wang et al., 2023). ", "page_idx": 15}, {"type": "text", "text": "Other perspectives for SAM. The convergence of SAM is comprehensively studied in (Si and Yun, 2023). Agarwala and Dauphin (2023) focus on the edge-of-stability-like behavior of unnormalized SAM on quadratic problems. Dai et al. (2023) argue that the normalization in SAM, i.e., line 5 of Alg. 1, is critical. Sharpness measure is generalized to any functions of Hessian in (Tahmasebi et al., 2024). However, even the generalized sharpness cannot provide implicit regularization for simple functions such as $h(x,y)=\\bar{x}y$ , because the Hessian is the same for all $(x,y)$ . In addition, when Hessian is negative definite, some of the generalized sharpness measures (e.g., determinate of Hessian) may not be necessarily meaningful. ", "page_idx": 15}, {"type": "text", "text": "Implicit regularization. The regularization effect can come from optimization algorithms rather than directly from the regularizer in objective functions. This type of the behavior is termed as implicit regularization or implicit bias of the optimizer. The implicit regularization of (S)GD is studied from multiple perspectives, such as margin (Ji and Telgarsky, 2019; Lyu and Li, 2020), kernel (Arora et al., 2019c), and Hessian (Li et al., 2022; Arora et al., 2022). Initialization can also determine the implicit regularization (Woodworth et al., 2020). Most of these works explore the overparametrization regime. ", "page_idx": 15}, {"type": "text", "text": "LoRA and parameter-efficient finetuning. LoRA (Hu et al., 2022), our major numerical benchmark, is an instance of parameter-efficient finetuning (PEFT) approaches. PEFT reduces the resource requirement for large language models on various downstream tasks, at the cost of possible accuracy drops on test performance. The latter, together with the transfer learning setup jointly motivate the adoption of SAM. Other commonly adopted PEFT methods include, e.g., adapters (Houlsby et al., 2019) and prefix tuning (Li and Liang, 2021). There are also various efforts to further improve ", "page_idx": 15}, {"type": "text", "text": "LoRA via adaptivity (Zhang et al., 2023a), chaining (Xia et al., 2024), aggressive parameter saving (Kopiczko et al., 2024), low-bit training (Dettmers et al., 2023), and modifications for long-sequences (Chen et al., 2024). Most of these efforts are orthogonal to BAR proposed in this work. ", "page_idx": 16}, {"type": "text", "text": "A.3 Additional Applications of Scale-Invariant Problems in Deep Learning ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Attention in transformers. Attention is one of the backbones of modern neural networks (Vaswani et al., 2017). Given the input $\\mathbf{D}$ , attention can be written as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{Q},\\mathbf{K},\\mathbf{V}}\\ \\mathrm{softmax}\\left(\\frac{1}{\\alpha}\\mathbf{D}\\mathbf{Q}\\mathbf{K}^{\\top}\\mathbf{D}^{\\top}\\right)\\mathbf{D}\\mathbf{V}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\{\\mathbf{Q},\\mathbf{K},\\mathbf{V}\\}$ are query, key, and value matrices to be optimized. This is a scale-invariant problem because scaling $\\{\\mathbf{Q},\\mathbf{K}\\}$ does not modify the objective function. Considering the number of variables, the optimization of $\\{\\overset{.}{\\mathbf{Q}},\\mathbf{K}\\}$ is considered as OP. ", "page_idx": 16}, {"type": "text", "text": "Two-layer linear neural networks. This problem is a simplified version of two-layer ReLU neural nets, and its objective can be defined as ", "page_idx": 16}, {"type": "equation", "text": "$$\nf(\\mathbf{W}_{1},\\mathbf{W}_{2})=\\frac{1}{2}\\mathbb{E}_{(\\mathbf{a},\\mathbf{b})}\\big[\\|\\mathbf{W}_{1}\\mathbf{W}_{2}\\mathbf{a}-\\mathbf{b}\\|^{2}\\big].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This is usually adopted as an example for overparametrization, and can be extended to deeper linear neural networks; see e.g., (Arora et al., 2019a). Moreover, it is known that the optimization for such problem is quite challenging, and GD can fail to converge if $\\mathbf{W}_{1}$ and $\\mathbf{W}_{2}$ are not initialized with balancedness (Arora et al., 2019a). An extension of (10) is two-layer ReLU networks, which are widely adopted in theoretical frameworks to understand the behavior of neural networks. ReLU networks are scale-invariant, but only when the scaling factor is positive. ", "page_idx": 16}, {"type": "text", "text": "Other examples. For ResNets, two-variable scale-invariant submodules also include affine BatchNorm and the subsequent convolutional layer. For transformers, scale-invariant submodules besides attention include LayerNorm and its subsequent linear layer. ", "page_idx": 16}, {"type": "text", "text": "A.4 SAM Pays More Attention to Difficult Examples ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Testing example for NOP. The problem presented below is adopted in Fig. 1 (a) and Fig. 2 for visualization of SAM\u2019s behavior on NOP. We consider a special case of problem (1a), where the goal is to fit (rank-1) matrices by minimizing ", "page_idx": 16}, {"type": "equation", "text": "$$\nf_{n}(\\mathbf{x},\\mathbf{y})=\\mathbb{E}_{\\xi}\\big[\\|\\mathbf{x}\\mathbf{y}^{\\top}-(\\mathbf{A}+\\alpha\\mathbf{N}_{\\xi})\\|^{2}\\big]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\mathbf{A}\\in\\mathbb{R}^{3\\times3}:=\\mathrm{diag}[0.5,0,0]$ and $\\mathbf{N}_{\\xi}\\in\\mathbb{R}^{3\\times3}$ denote the ground truth and Gaussian noise, respectively; and $\\alpha$ controls the SNR. Here we choose $\\mathbf{N}_{\\xi}:=\\mathrm{diag}[1.0,0.8,0.5]\\mathbf{U}_{\\xi}$ , where entries of $\\mathbf{U}_{\\xi}$ are unit Gaussian random variables. ", "page_idx": 16}, {"type": "text", "text": "In our simulation of Fig. 1 (a), we set the step size to be $\\eta=10^{-4}$ and the total number of iterations as $T=10^{5}$ for both SGD and SAM. Parameter $\\rho$ is chosen as 0.1 for SAM. For both algorithms, initialization is $\\mathbf{x}_{0}=[0.2,-0.1,0.3]^{\\top}$ and $\\mathbf{y}_{0}=-3\\mathbf{x}_{0}$ . Note that we choose a small step size to mimic the settings of our theorems. ", "page_idx": 16}, {"type": "text", "text": "Testing example for OP. The problem presented below is adopted in Fig. 1 $(b)$ for visualization of SAM on OP. A special case of problem (1b) is considered with objective function ", "page_idx": 16}, {"type": "equation", "text": "$$\nf_{o}(\\mathbf{x},\\mathbf{y})=\\mathbb{E}_{\\xi}\\left[\\|\\mathbf{x}^{\\top}\\mathbf{y}-(a+\\alpha n_{\\xi})\\|^{2}\\right]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $a\\in\\mathbb{R}$ and $n_{\\xi}\\in\\mathbb{R}$ denote the ground truth and Gaussian noise, respectively. We choose $a=0.5$ and $n_{\\xi}$ as a unit Gaussian random variable. Here, $\\alpha$ controls the SNR of this problem. ", "page_idx": 16}, {"type": "text", "text": "In our simulation of Fig. 1 (b), we set $\\eta=10^{-4}$ and $T=10^{5}$ for both SGD and SAM. Parameter $\\rho$ is set as 0.2 for SAM. For both algorithms, initialization is $\\mathbf{x}_{0}=[0.2,-0.1,0.3]^{\\top}$ and $\\mathbf{y}_{0}=-3\\mathbf{x}_{0}$ . ", "page_idx": 16}, {"type": "text", "text": "A.5 Scale-Invariance in OP ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Scale-invariance also bothers OP in the same fashion as it burdens NOP. For completeness, the scale-invariance of OP can be verified by ", "page_idx": 16}, {"type": "equation", "text": "$$\nf_{o}(\\mathbf{x}^{\\top}\\mathbf{y})=f_{o}\\Big((\\alpha\\mathbf{x})^{\\top}(\\frac{1}{\\alpha}\\mathbf{y})\\Big),\\forall\\alpha\\neq0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "An optimizer has to determine $\\alpha$ for OP despite it does not influence objective value. Hence, scaling is redundant for OP. ", "page_idx": 17}, {"type": "text", "text": "Similar to NOP, the (stochastic) gradient of OP is not scale-invariant. In particular, given a minibatch of data $\\mathcal{M}$ , the stochastic gradient for OP (1b) can be written as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{g}_{\\mathbf{x}}=\\frac{1}{|\\mathcal{M}|}\\Big[\\sum_{\\xi\\in\\mathcal{M}}(f_{o}^{\\xi})^{\\prime}(\\mathbf{x}^{\\top}\\mathbf{y})\\Big]\\mathbf{y},\\quad\\mathbf{g}_{\\mathbf{y}}=\\frac{1}{|\\mathcal{M}|}\\Big[\\sum_{\\xi\\in\\mathcal{M}}(f_{o}^{\\xi})^{\\prime}(\\mathbf{x}^{\\top}\\mathbf{y})\\Big]\\mathbf{x}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Consequently, being balance also brings optimization benefits for OP as discussed previously in Section 2.2 . ", "page_idx": 17}, {"type": "text", "text": "A.6 BAR in Detail ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "BAR is inspired jointly from the balancedness-promoting regularizer $\\|\\|\\mathbf{x}_{t}\\|^{2}-\\|\\mathbf{\\dot{y}}_{t}\\|^{2}|$ and the dynamics of SAM on both NOP and OP. The implementation of BAR is similar as weight decay in AdamW (Loshchilov and Hutter, 2019). ", "page_idx": 17}, {"type": "text", "text": "Here we use nBAR as an example. If ignoring $\\boldsymbol{A}_{t}$ in Theorem 2, it can be seen that $B_{t}$ for NOP decreases whenever $\\|\\mathbf{g}_{\\mathbf{x}_{t}}\\|<\\|\\mathbf{g}_{\\mathbf{y}_{t}}\\|$ . In other words, the balancedness of SAM is driven by the difference between the gradient norms at $\\mathbf{x}_{t}$ and $\\mathbf{y}_{t}$ . nBAR mimics this and triggers balancedness when stochastic gradients $\\mathbf{g}_{\\mathbf{x}_{t}}$ and $\\mathbf{g}_{\\mathbf{y}_{t}}$ are not balanced; see Alg. 2. ", "page_idx": 17}, {"type": "text", "text": "Finally, we illustrate more on the reasons for employing regularization in OP rather than posing $\\left\\|\\mathbf{x}_{t}\\right\\|=\\left\\|\\mathbf{y}_{t}\\right\\|$ as a hard constraint or initializing in a balanced manner, i.e., $\\|\\mathbf{x}_{0}\\|\\;=\\;\\|\\mathbf{y}_{0}\\|$ . First, it is quite clear that $\\|\\mathbf{x}\\|\\,=\\,\\|\\mathbf{y}\\|$ is a nonconvex set and how to project on such a set is still debatable. Second, the \u2018symmetry\u2019 associated with the scale-invariant problems does not always favor this ", "page_idx": 17}, {"type": "image", "img_path": "oSOVME9kl2/tmp/4b50500d88efa482513ed584df570051536377b2ab47ab5ce03a6b922ba6720c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 4: The value of $f(x,y)$ . Once SGD reaches the dotted line, i.e., the hard constraint $|x|\\;=\\;|y|$ , it can only converge to a saddle point $(0,0)$ . ", "page_idx": 17}, {"type": "text", "text": "constraint. For the purpose of graphical illustration, we consider a 2-dimensional example $f(x,y)=$ $30000(x y-0.005)^{2}$ . It is quite clear that the objective is symmetric regarding the line $x=-y$ , which satisfies $|x|=|y|$ ; see Fig. 4. However, it is not hard to see that SGD can never leave $x=-y$ once it reaches this line via a hard constraint or initialized on this line. In other words, directly adding $\\|\\mathbf{x}\\|=\\|\\mathbf{y}\\|$ as a constraint can trap the algorithm at saddle points. This symmetric pattern is even more complicated in high dimension, i.e., symmetry over multiple lines or hyperplanes. Hence, one should be extremely careful about this hard constraint, and regularization is a safer and more practical choice. ", "page_idx": 17}, {"type": "text", "text": "B Missing Proofs for NOP ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. For notational convenience, we let $\\mathbf{G}_{t}:=\\nabla f_{t}(\\mathbf{x}_{t}\\mathbf{y}_{t}^{\\top})$ . Then, we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\|\\mathbf{x}_{t}\\|^{2}}{\\mathrm{d}t}=2\\mathbf{x}_{t}^{\\top}\\frac{\\mathrm{d}\\mathbf{x}_{t}}{\\mathrm{d}t}=-2\\mathbf{x}_{t}^{\\top}\\mathbf{g}_{\\mathbf{x}_{t}}=-2\\mathbf{x}_{t}^{\\top}\\mathbf{G}_{t}\\mathbf{y}_{t}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Similarly, we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\|\\mathbf{y}_{t}\\|^{2}}{\\mathrm{d}t}=2\\mathbf{y}_{t}^{\\top}\\frac{\\mathrm{d}\\mathbf{y}_{t}}{\\mathrm{d}t}=-2\\mathbf{y}_{t}^{\\top}\\mathbf{g}_{\\mathbf{y}_{t}}=-2\\mathbf{y}_{t}^{\\top}\\mathbf{G}_{t}^{\\top}\\mathbf{x}_{t}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Combining these two inequalities, we arrive at ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\|\\mathbf{x}_{t}\\|^{2}}{\\mathrm{d}t}-\\frac{\\mathrm{d}\\|\\mathbf{y}_{t}\\|^{2}}{\\mathrm{d}t}=0.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The proof is thus completed. ", "page_idx": 17}, {"type": "text", "text": "B.2 Extension to Stochastic Normalized Gradient Descent (SNGD) ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Next, we extend Theorem 1 to SNGD, whose updates can be written as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{x}_{t+1}=\\mathbf{x}_{t}-\\eta{\\frac{\\mathbf{g}_{\\mathbf{x}_{t}}}{{\\sqrt{\\|\\mathbf{g}_{\\mathbf{x}_{t}}\\|^{2}+\\|\\mathbf{g}_{\\mathbf{y}_{t}}\\|^{2}}}}},\\ \\ \\ \\ \\ \\mathbf{y}_{t+1}=\\mathbf{y}_{t}-\\eta{\\frac{\\mathbf{g}_{\\mathbf{y}_{t}}}{{\\sqrt{\\|\\mathbf{g}_{\\mathbf{x}_{t}}\\|^{2}+\\|\\mathbf{g}_{\\mathbf{y}_{t}}\\|^{2}}}}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Theorem 4. When applying SNGD (15) on NOP problem (1a), the limiting flow with $\\eta~\\rightarrow~0$ guarantees that $\\|\\mathbf{x}_{t}\\|^{2}-\\|\\mathbf{y}_{t}\\|^{2}=\\|\\mathbf{x}_{0}\\|^{2}-\\|\\mathbf{y}_{0}\\|^{2}$ for all $t>0$ . In other words, $\\begin{array}{r}{\\frac{\\dot{a}B_{t}}{d t}=0}\\end{array}$ holds. ", "page_idx": 18}, {"type": "text", "text": "Proof. For notational convenience, we let $\\mathbf{G}_{t}:=\\nabla f_{t}(\\mathbf{x}_{t}\\mathbf{y}_{t}^{\\top})$ . Then, we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\mathbf{d}\\|\\mathbf{x}_{t}\\|^{2}}{\\mathrm{d}t}=2\\mathbf{x}_{t}^{\\top}\\frac{\\mathbf{d}\\mathbf{x}_{t}}{\\mathrm{d}t}=-2\\frac{\\mathbf{x}_{t}^{\\top}\\mathbf{g}_{\\mathbf{x}_{t}}}{\\sqrt{\\|\\mathbf{g}_{\\mathbf{x}_{t}}\\|^{2}+\\|\\mathbf{g}_{\\mathbf{y}_{t}}\\|^{2}}}=-2\\frac{\\mathbf{x}_{t}^{\\top}\\mathbf{G}_{t}\\mathbf{y}_{t}}{\\sqrt{\\|\\mathbf{g}_{\\mathbf{x}_{t}}\\|^{2}+\\|\\mathbf{g}_{\\mathbf{y}_{t}}\\|^{2}}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Similarly, we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\mathbf{d}\\|\\mathbf{y}_{t}\\|^{2}}{\\mathrm{d}t}=2\\mathbf{y}_{t}^{\\top}\\frac{\\mathbf{d}\\mathbf{y}_{t}}{\\mathrm{d}t}=-2\\frac{\\mathbf{y}_{t}^{\\top}\\mathbf{g}_{\\mathbf{y}_{t}}}{\\sqrt{\\|\\mathbf{g}_{\\mathbf{x}_{t}}\\|^{2}+\\|\\mathbf{g}_{\\mathbf{y}_{t}}\\|^{2}}}=-2\\frac{\\mathbf{y}_{t}^{\\top}\\mathbf{G}_{t}^{\\top}\\mathbf{x}_{t}}{\\sqrt{\\|\\mathbf{g}_{\\mathbf{x}_{t}}\\|^{2}+\\|\\mathbf{g}_{\\mathbf{y}_{t}}\\|^{2}}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Combining these two inequalities, we arrive at ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\|\\mathbf{x}_{t}\\|^{2}}{\\mathrm{d}t}-\\frac{\\mathrm{d}\\|\\mathbf{y}_{t}\\|^{2}}{\\mathrm{d}t}=0.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The proof is thus completed. ", "page_idx": 18}, {"type": "text", "text": "B.3 Proof of Theorem 2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. Denote $\\mathbf{G}_{t}\\,=\\,\\nabla f_{t}\\big(\\mathbf{x}_{t}\\mathbf{y}_{t}^{\\top}\\big)$ and $\\tilde{\\mathbf{G}}_{t}\\,=\\,\\nabla f_{t}\\big(\\tilde{\\mathbf{x}}_{t}\\tilde{\\mathbf{y}}_{t}^{\\top}\\big)$ for notational convenience. Following SAM updates in (4) and setting $\\eta\\rightarrow0$ , we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mathbf{x}_{t}}{\\mathrm{d}t}=-\\tilde{\\mathbf{G}}_{t}(\\mathbf{y}_{t}+\\rho u_{t}\\mathbf{G}_{t}^{\\top}\\mathbf{x}_{t}),\\;\\;\\;\\frac{\\mathrm{d}\\mathbf{y}_{t}}{\\mathrm{d}t}=-\\tilde{\\mathbf{G}}_{t}^{\\top}(\\mathbf{x}_{t}+\\rho u_{t}\\mathbf{G}_{t}\\mathbf{y}_{t}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This gives that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{\\mathrm{i}}{2}\\frac{\\mathrm{d}\\left(\\left\\|\\mathbf{x}_{t}\\right\\|^{2}-\\left\\|\\mathbf{y}_{t}\\right\\|^{2}\\right)}{\\mathrm{d}t}=\\rho u_{t}\\bigg[\\mathbf{y}_{t}^{\\top}\\tilde{\\mathbf{G}}_{t}^{\\top}\\mathbf{G}_{t}\\mathbf{y}_{t}-\\mathbf{x}_{t}^{\\top}\\tilde{\\mathbf{G}}_{t}\\mathbf{G}_{t}^{\\top}\\mathbf{x}_{t}\\bigg]}}\\\\ &{}&{=\\rho u_{t}\\bigg[\\left\\|\\mathbf{g}_{\\mathbf{x}_{t}}\\right\\|^{2}-\\left\\|\\mathbf{g}_{\\mathbf{y}_{t}}\\right\\|^{2}\\bigg]+\\underbrace{\\rho u_{t}\\bigg[\\mathbf{y}_{t}^{\\top}(\\tilde{\\mathbf{G}}_{t}-\\mathbf{G}_{t})^{\\top}\\mathbf{g}_{\\mathbf{x}_{t}}-\\mathbf{x}_{t}^{\\top}(\\tilde{\\mathbf{G}}_{t}-\\mathbf{G}_{t})\\mathbf{g}_{\\mathbf{y}_{t}}\\bigg]}_{:=A_{t}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The second term in (16b) is $\\boldsymbol{A}_{t}$ in Theorem 2. Next, we give upper bound on $|\\mathcal{A}_{t}|$ . Using Assumption 1, we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\tilde{\\mathbf{G}}_{t}-\\mathbf{G}_{t}\\|\\leq L\\|\\tilde{\\mathbf{x}}_{t}\\tilde{\\mathbf{y}}_{t}^{\\top}-\\mathbf{x}_{t}\\mathbf{y}_{t}^{\\top}\\|}\\\\ &{\\qquad\\qquad=L\\|\\rho u_{t}(\\mathbf{x}_{t}\\mathbf{g}_{\\mathbf{y}_{t}}^{\\top}+\\mathbf{g}_{\\mathbf{x}_{t}}\\mathbf{y}_{t}^{\\top})+\\rho^{2}u_{t}^{2}\\mathbf{g}_{\\mathbf{x}_{t}}\\mathbf{g}_{\\mathbf{y}_{t}}^{\\top}\\|}\\\\ &{\\qquad\\qquad\\overset{(a)}{\\leq}L\\rho\\frac{\\left\\|\\mathbf{x}_{t}\\mathbf{g}_{\\mathbf{y}_{t}}^{\\top}+\\mathbf{g}_{\\mathbf{x}_{t}}\\mathbf{y}_{t}^{\\top}\\right\\|}{\\sqrt{\\left\\|\\mathbf{g}_{\\mathbf{x}_{t}}\\right\\|^{2}+\\left\\|\\mathbf{g}_{\\mathbf{y}_{t}}\\right\\|^{2}}}+L\\rho^{2}\\frac{\\left\\|\\mathbf{g}_{\\mathbf{x}_{t}}\\mathbf{g}_{\\mathbf{y}_{t}}^{\\top}\\right\\|}{\\left\\|\\mathbf{g}_{\\mathbf{x}_{t}}\\right\\|^{2}+\\left\\|\\mathbf{g}_{\\mathbf{y}_{t}}\\right\\|^{2}}}\\\\ &{\\qquad\\qquad\\overset{(b)}{\\leq}L\\rho(\\|\\mathbf{x}_{t}\\|+\\|\\mathbf{y}_{t}\\|)+\\frac{L\\rho^{2}}{2}=\\mathcal{O}(L\\rho)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where (a) uses the definition of $u_{t}$ ; (b) follows from $\\|\\mathbf{a}\\mathbf{b}^{\\top}\\|=\\|\\mathbf{a}\\|\\|\\mathbf{b}\\|$ and the finite convergence assumption. To bound $\\boldsymbol{A}_{t}$ , we also have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\rho u_{t}\\big|\\mathbf{y}_{t}^{\\top}(\\tilde{\\mathbf{G}}_{t}-\\mathbf{G}_{t})^{\\top}\\mathbf{g}_{\\mathbf{x}_{t}}\\big|=\\rho\\frac{\\left|\\mathbf{y}_{t}^{\\top}(\\tilde{\\mathbf{G}}_{t}-\\mathbf{G}_{t})^{\\top}\\mathbf{g}_{\\mathbf{x}_{t}}\\right|}{\\sqrt{\\|\\mathbf{g}_{\\mathbf{x}_{t}}\\|^{2}+\\|\\mathbf{g}_{\\mathbf{y}_{t}}\\|^{2}}}\\leq\\rho\\frac{\\left|\\mathbf{y}_{t}^{\\top}(\\tilde{\\mathbf{G}}_{t}-\\mathbf{G}_{t})^{\\top}\\mathbf{g}_{\\mathbf{x}_{t}}\\right|}{\\left\\|\\mathbf{g}_{\\mathbf{x}_{t}}\\right\\|}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\rho\\|\\tilde{\\mathbf{G}}_{t}-\\mathbf{G}_{t}\\|\\|\\mathbf{y}_{t}\\|=\\mathcal{O}(L\\rho^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the last line also uses the finite convergence. We can bound $\\rho u_{t}|\\mathbf{x}_{t}^{\\top}(\\tilde{\\mathbf{G}}_{t}-\\mathbf{G}_{t})\\mathbf{g}_{\\mathbf{y}_{t}}|=\\mathcal{O}(\\rho^{2}L)$ in a similar manner. Combining (17) with (16b) gives the bound on $|\\mathcal{A}_{t}|=\\mathcal{O}(\\rho^{2}L)$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "B.4 Proof of Corollary 1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Here, we prove the formal version of Corollary 1. ", "page_idx": 19}, {"type": "text", "text": "Corollary 2. Suppose that $\\|\\mathbf{g}_{\\mathbf{x}_{t}}\\|>0$ and $\\|\\mathbf{g}_{\\mathbf{y}_{t}}\\|>0$ and $\\rho\\rightarrow0$ , then there exists $\\bar{\\beta}_{t}$ such that the magnitude of $B_{t}$ shrinks whenever $\\left|\\boldsymbol{B}_{t}\\right|>\\bar{\\boldsymbol{B}}_{t}$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. Without loss of generality, we suppose that $B_{t}>0$ , i.e., $\\|\\mathbf x_{t}\\|>\\|\\mathbf y_{t}\\|>0$ . Let $\\bar{\\mathbf{x}}_{t}$ and $\\bar{\\mathbf{y}}_{t}$ be the scaled version of $\\mathbf{x}_{t}$ and $\\mathbf{y}_{t}$ such that $\\left\\|\\bar{\\mathbf x}_{t}\\right\\|=\\left\\|\\bar{\\mathbf y}_{t}\\right\\|$ and $\\bar{\\mathbf x}_{t}\\bar{\\mathbf y}_{t}^{\\top}=\\mathbf x_{t}\\mathbf y_{t}^{\\top}$ are satisfied. This suggests that $\\mathbf{x}_{t}=\\alpha_{t}\\bar{\\mathbf{x}}_{t}$ and $\\mathbf{y}_{t}=\\bar{\\mathbf{y}}_{t}/\\alpha_{t}$ , where $\\alpha_{t}=\\sqrt{\\|\\mathbf{x}_{t}\\|/\\|\\mathbf{y}_{t}\\|}$ . Next, we show that whenever $B_{t}$ is large enough, we have that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mathcal{B}_{t}}{\\mathrm{d}t}=\\rho\\frac{\\|\\mathbf{g}_{\\mathbf{x}_{t}}\\|^{2}-\\|\\mathbf{g}_{\\mathbf{y}_{t}}\\|^{2}}{\\sqrt{\\|\\mathbf{g}_{\\mathbf{x}_{t}}\\|^{2}+\\|\\mathbf{g}_{\\mathbf{y}_{t}}\\|^{2}}}+\\mathcal{O}(\\rho^{2}L)<0.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since $\\rho\\rightarrow0$ , we only need to show that for some small $\\epsilon=O(\\rho L)\\geq0$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\lVert\\mathbf{g}_{\\mathbf{x}_{t}}\\rVert^{2}-\\lVert\\mathbf{g}_{\\mathbf{y}_{t}}\\rVert^{2}}{\\sqrt{\\lVert\\mathbf{g}_{\\mathbf{x}_{t}}\\rVert^{2}+\\lVert\\mathbf{g}_{\\mathbf{y}_{t}}\\rVert^{2}}}<-\\epsilon.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By the definition of $\\mathbf{g}_{\\mathbf{x}_{t}},\\mathbf{g}_{\\mathbf{y}_{t}}$ and $\\bar{\\mathbf{x}}_{t},\\bar{\\mathbf{y}}_{t}$ , we have that (19) can be rewritten as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\alpha_{t}^{2}\\|\\mathbf{G}_{t}^{\\top}\\bar{\\mathbf{x}}_{t}\\|^{2}-\\|\\mathbf{G}_{t}\\bar{\\mathbf{y}}_{t}\\|^{2}/\\alpha_{t}^{2}}{\\sqrt{\\alpha_{t}^{2}\\|\\mathbf{G}_{t}^{\\top}\\bar{\\mathbf{x}}_{t}\\|^{2}+\\|\\mathbf{G}_{t}\\bar{\\mathbf{y}}_{t}\\|^{2}/\\alpha_{t}^{2}}}>\\epsilon.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that the function $h(z)\\::=\\:(a z\\mathrm{~-~}b/z)/\\sqrt{a z+b/z}$ is monotonically increasing in $z$ when $a,b>0$ and $z>0$ as $h^{\\prime}(z)=(a^{2}z+6a b/z+b^{2}/z^{3})/(2(a z+b/z)^{3/2})>0$ . This implies that $h(z)>0$ when $z>\\sqrt{b/a}$ , and thus the condition in (20) can be satisfied for $\\epsilon=\\mathcal{O}(\\rho L)\\to0$ when $\\alpha_{t}^{2}>\\bar{\\alpha}^{2}$ , where $\\bar{\\alpha}^{2}:=\\|\\mathbf{G}_{t}\\bar{\\mathbf{y}}_{t}\\|/\\|\\mathbf{G}_{t}^{\\top}\\bar{\\mathbf{x}}_{t}\\|$ . This condition on $\\alpha_{t}$ is equivalent to ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{\\mathcal B_{t}=\\frac{1}{2}\\big(\\|{\\bf x}_{t}\\|^{2}-\\|{\\bf y}_{t}\\|^{2}\\big)}}\\\\ {\\displaystyle\\quad=\\frac{1}{2}\\big(\\|\\alpha_{t}\\bar{\\bf x}_{t}\\|^{2}-\\|\\bar{\\bf y}_{t}/\\alpha_{t}\\|^{2}\\big)}\\\\ {\\displaystyle\\quad>\\frac{1}{2}\\big(\\|\\bar{\\alpha}\\bar{\\bf x}_{t}\\|^{2}-\\|\\bar{\\bf y}_{t}/\\bar{\\alpha}\\|^{2}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Combining everything together, we have that $\\begin{array}{r}{\\frac{\\mathrm{d}\\mathcal{B}_{t}}{\\mathrm{d}t}<0}\\end{array}$ if ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{B}_{t}>\\bar{\\mathcal{B}}_{t}:=\\frac{1}{2}\\big(\\|\\bar{\\alpha}\\bar{\\mathbf{x}}_{t}\\|^{2}-\\|\\bar{\\mathbf{y}}_{t}/\\bar{\\alpha}\\|^{2}\\big).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The proof is thus completed. We also note that in the case of $\\rho>0$ , the same condition as (21) can be derived by obtaining the inverse function of $h(z)$ evaluated at $\\epsilon=\\mathcal{O}(\\rho L)$ , and the corresponding $\\bar{\\alpha}_{\\rho}$ and $\\bar{\\mathcal{B}}_{t}^{\\rho}$ can be defined similarly. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "B.5 Extension to LoRA (layer-wise NOP problem) ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Let $l\\,\\in\\,\\{1,2,\\dots,D\\}$ be the layer index. Denote $f_{t}$ as the loss function on minibatch $\\mathcal{M}_{t}$ . To simplify the notation, we also let $\\begin{array}{r}{\\mathbf{\\check{G}}_{t,l}:=\\nabla_{\\mathbf{x}_{t,l}\\mathbf{y}_{t,l}^{\\top}}f_{t}\\big(\\{\\mathbf{x}_{t,l},\\mathbf{y}_{t,l}\\}_{l}\\big),\\;\\tilde{\\mathbf{G}}_{t,l}:=\\nabla_{\\tilde{\\mathbf{x}}_{t,l}\\mathbf{\\tilde{y}}_{t,l}^{\\top}}f_{t}\\big(\\{\\tilde{\\mathbf{x}}_{t,l},\\tilde{\\mathbf{y}}_{t,l}\\}_{l}\\big),}\\end{array}$ , and $\\begin{array}{r}{u_{t}:=1/\\sqrt{\\sum_{l=1}^{D}\\left(\\|\\mathbf{g}_{\\mathbf{x}_{t},l}\\|^{2}+\\|\\mathbf{g}_{\\mathbf{y}_{t,l}}\\|^{2}\\right)}}\\end{array}$ . The update of SAM for layer $l$ can be written as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\tilde{\\mathbf{x}}_{t,l}=\\mathbf{x}_{t,l}+\\rho u_{t}\\mathbf{G}_{t,l}\\mathbf{y}_{t,l},}&{\\tilde{\\mathbf{y}}_{t,l}=\\mathbf{y}_{t,l}+\\rho u_{t}\\mathbf{G}_{t,l}^{\\top}\\mathbf{x}_{t,l}}\\\\ {\\mathbf{g}_{\\tilde{\\mathbf{x}}_{t,l}}=\\tilde{\\mathbf{G}}_{t,l}\\tilde{\\mathbf{y}}_{t,l},}&{\\mathbf{g}_{\\tilde{\\mathbf{y}}_{t,l}}=\\tilde{\\mathbf{G}}_{t,l}^{\\top}\\tilde{\\mathbf{x}}_{t,l}}\\\\ {\\mathbf{x}_{t+1,l}=\\mathbf{x}_{t,l}-\\eta\\mathbf{g}_{\\tilde{\\mathbf{x}}_{t,l}},}&{\\mathbf{y}_{t+1,l}=\\mathbf{y}_{t,l}-\\eta\\mathbf{g}_{\\tilde{\\mathbf{y}}_{t,l}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Refined assumption for LoRA. Direct translating Assumption 1 to our multi-layer setting gives ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\nabla f_{t}(\\{\\mathbf{x}_{l}\\mathbf{y}_{l}^{\\top}\\}_{l})-\\nabla f_{t}(\\{\\mathbf{a}_{l}\\mathbf{b}_{l}^{\\top}\\}_{l})\\|^{2}\\leq L^{2}\\sum_{l=1}^{D}\\|\\mathbf{x}_{l}\\mathbf{y}_{l}^{\\top}-\\mathbf{a}_{l}\\mathbf{b}_{l}^{\\top}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "However, the above assumption is loose, and our proof only needs block-wise smoothness, i.e., ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|\\nabla_{l}f_{t}(\\mathbf{x}_{l}\\mathbf{y}_{l}^{\\top})-\\nabla_{l}f_{t}(\\mathbf{a}_{l}\\mathbf{b}_{l}^{\\top})\\|^{2}\\leq\\hat{L}^{2}\\|\\mathbf{x}_{l}\\mathbf{y}_{l}^{\\top}-\\mathbf{a}_{l}\\mathbf{b}_{l}^{\\top}\\|^{2},\\forall l\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\nabla_{l}$ refers to the gradient on $\\mathbf{x}_{l}\\mathbf{y}_{l}^{\\top}$ . It can be seen that $\\sqrt{D}\\hat{L}\\geq L$ , but one can assume that $\\sqrt{D}\\hat{L}\\approx L$ for intuitive understandings. ", "page_idx": 20}, {"type": "text", "text": "Theorem 5. Suppose that block smoothness assumption in (24) holds. Consider the limiting flow of SAM in (22) with $\\eta\\:\\rightarrow\\:0$ and a sufficiently small $\\rho$ . Let $\\begin{array}{r l}{\\mathscr{B}_{t,l}\\;:=\\;\\frac{1}{2}\\Big(\\|\\mathbf{x}_{t,l}\\|^{2}-\\|\\mathbf{y}_{t,l}\\|^{2}\\Big)}\\end{array}$ and $\\begin{array}{r}{\\mathcal{B}_{t}=\\sum_{l=1}^{D}\\mathcal{B}_{t,l}}\\end{array}$ . For some $|\\mathcal{A}_{t}|=\\mathcal{O}(\\rho^{2}\\hat{L})$ , SAM guarantees that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{d\\mathcal{B}_{t}}{d t}=\\rho\\frac{\\sum_{l=1}^{D}\\|\\mathbf{g}_{\\mathbf{x}_{t,l}}\\|^{2}-\\sum_{l=1}^{D}\\|\\mathbf{g}_{\\mathbf{y}_{t,l}}\\|^{2}}{\\sqrt{\\sum_{l=1}^{D}\\|\\mathbf{g}_{\\mathbf{x}_{t,l}}\\|^{2}+\\sum_{l=1}^{D}\\|\\mathbf{g}_{\\mathbf{y}_{t,l}}\\|^{2}}}+\\mathcal{A}_{t}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Furthermore, for per layer balancedness it satisfies that for some $|\\mathcal{A}_{t,l}|=\\mathcal{O}(\\rho^{2}\\hat{L})$ . ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{d\\mathcal{B}_{t,l}}{d t}=\\rho\\frac{\\Vert\\mathbf{g}_{\\mathbf{x}_{t,l}}\\Vert^{2}-\\Vert\\mathbf{g}_{\\mathbf{y}_{t,l}}\\Vert^{2}}{\\sqrt{\\sum_{l=1}^{D}\\Vert\\mathbf{g}_{\\mathbf{x}_{t,l}}\\Vert^{2}+\\sum_{l=1}^{D}\\Vert\\mathbf{g}_{\\mathbf{y}_{t,l}}\\Vert^{2}}}+A_{t,i}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Understanding Theorem 5. $\\boldsymbol{A}_{t,i}$ and $A_{t}$ are at the same order because of the possible unbalancedness among gradient norms for different layers. Comparing per layer b\u221aalancedness $B_{t,l}$ with Theorem 2, it can be roughly estimate that the regularization power is $\\mathcal{O}(\\sqrt{D})$ times smaller in $B_{t,l}$ . This estimation comes from $\\hat{L}\\approx L/\\sqrt{D}$ , and the first term is also $\\mathcal{O}(\\sqrt{D})$ smaller than the \u221asame term in Theorem 2. In other words, the regularization on balancedness can be reduced by $\\mathcal{O}(\\sqrt{D})$ times in LoRA in the worst case, and the worst case comes from gradient unbalancedness among layers. ", "page_idx": 20}, {"type": "text", "text": "Proof. Following (22) and setting $\\eta\\rightarrow0$ , we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mathbf{x}_{t,l}}{\\mathrm{d}t}=-\\tilde{\\mathbf{G}}_{t,l}(\\mathbf{y}_{t,l}+\\rho u_{t}\\mathbf{G}_{t,l}^{\\top}\\mathbf{x}_{t,l}),\\;\\;\\;\\frac{\\mathrm{d}\\mathbf{y}_{t,l}}{\\mathrm{d}t}=-\\tilde{\\mathbf{G}}_{t,l}^{\\top}(\\mathbf{x}_{t,l}+\\rho u_{t}\\mathbf{G}_{t,l}\\mathbf{y}_{t,l}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This gives that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mathcal{B}_{t,l}}{\\mathrm{d}t}=\\rho u_{t}\\left[\\mathbf{y}_{t,l}^{\\top}\\tilde{\\mathbf{G}}_{t,l}^{\\top}\\mathbf{G}_{t,l}\\mathbf{y}_{t,l}-\\mathbf{x}_{t,l}^{\\top}\\tilde{\\mathbf{G}}_{t,l}\\mathbf{G}_{t,l}^{\\top}\\mathbf{x}_{t,l}\\right]\\qquad\\qquad\\qquad\\qquad\\qquad(27)}\\\\ {=\\rho u_{t}\\left[\\left\\Vert\\mathbf{g}_{\\mathbf{x}_{t,l}}\\right\\Vert^{2}-\\left\\Vert\\mathbf{g}_{\\mathbf{y}_{t,l}}\\right\\Vert^{2}\\right]+\\underbrace{\\rho u_{t}\\left[\\mathbf{y}_{t,l}^{\\top}(\\tilde{\\mathbf{G}}_{t,l}-\\mathbf{G}_{t,l})^{\\top}\\mathbf{g}_{\\mathbf{x}_{t,l}}-\\mathbf{x}_{t,l}^{\\top}(\\tilde{\\mathbf{G}}_{t,l}-\\mathbf{G}_{t,l})\\mathbf{g}_{\\mathbf{y}_{t,l}}\\right]}_{:=\\mathcal{A}_{t,l}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof for (25). Let $\\begin{array}{r}{\\mathcal A_{t}:=\\sum_{l}\\mathcal A_{t,l}}\\end{array}$ . To start with, we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\tilde{\\mathbf{G}}_{t,l}-\\mathbf{G}_{t,l}\\|\\leq\\hat{L}\\|\\tilde{\\mathbf{x}}_{t,l}\\tilde{\\mathbf{y}}_{t,l}^{\\top}-\\mathbf{x}_{t,l}\\mathbf{y}_{t,l}^{\\top}\\|}\\\\ &{\\qquad\\qquad\\qquad=\\hat{L}\\|\\rho u_{t}(\\mathbf{x}_{t,l}\\mathbf{g}_{\\mathbf{y}_{t,l}}^{\\top}+\\mathbf{g}_{\\mathbf{x}_{t,l}}\\mathbf{y}_{t,l}^{\\top})+\\rho^{2}u_{t}^{2}\\mathbf{g}_{\\mathbf{x}_{t,l}}\\mathbf{g}_{\\mathbf{y}_{t,l}}^{\\top}\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Next, based on finite convergence assumption, we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\rho u_{t}\\frac{D}{L\\tau_{i}}\\Big\\vert\\mathbf{y}_{t,i}^{\\top}(\\bar{\\mathbf{G}}_{t,i}-\\mathbf{G}_{t,i})^{\\top}\\mathbf{g}_{\\mathbf{x}_{t,i}}\\Big\\vert}\\\\ &{\\leq\\displaystyle\\sum_{l=1}^{D}C\\sigma\\Big(\\rho u_{t}\\Big\\vert\\bar{\\mathbf{G}}_{t,l}-\\mathbf{G}_{t,i}\\Big\\vert\\cdot\\Big\\vert\\mathbf{g}_{\\mathbf{x}_{t,i}}\\Big\\vert\\Big)}\\\\ &{\\overset{(a)}{\\leq}\\displaystyle\\sum_{l=1}^{D}C\\sigma\\Big(\\rho^{2}u_{t}^{2}\\bar{L}\\big\\Vert\\mathbf{x}_{t,i}\\mathbf{g}_{\\mathbf{y}_{t,i}^{\\top}}^{\\top}+\\mathbf{g}_{\\mathbf{x}_{t,i}}\\bar{\\mathbf{y}}_{t,i}^{\\top}\\Big\\vert\\cdot\\big\\Vert\\mathbf{g}_{\\mathbf{x}_{t,i}}\\Big\\vert\\Big)}\\\\ &{\\overset{(b)}{\\leq}\\displaystyle\\sum_{l=1}^{D}C\\left(\\rho^{2}u_{t}^{2}\\bar{L}\\big(\\|\\mathbf{g}_{\\mathbf{x}_{t,i}}\\|+\\big\\Vert\\mathbf{g}_{\\mathbf{x}_{t,i}}\\big\\Vert\\cdot\\big\\Vert\\mathbf{g}_{\\mathbf{x}_{t,i}}\\big)\\right)}\\\\ &{=\\rho^{2}\\hat{L}\\cdot O\\Big(\\frac{\\sum_{l=1}^{D}\\big\\Vert\\mathbf{g}_{\\mathbf{x}_{t,i}}\\big\\Vert^{2}}{\\sum_{l=1}^{D}\\big(\\|\\mathbf{g}_{\\mathbf{x}_{t,i}}\\|^{2}+\\big\\Vert\\mathbf{g}_{\\mathbf{x}_{t,i}}\\|^{2}\\big)}+\\frac{\\sum_{l=1}^{D}\\big\\Vert\\mathbf{g}_{\\mathbf{x}_{t,i}}\\big\\Vert\\big\\Vert\\mathbf{g}_{\\mathbf{y}_{t,i}}\\big\\Vert}{\\sum_{l=1}^{D}\\big(\\|\\mathbf{g}_{\\mathbf{x}_{t,i}}\\|^{2}+\\big\\Vert\\mathbf{g}_{\\mathbf{y}_{t,i}}\\|^{2}\\big)}\\Big)}\\\\ &{=\\sigma(\\rho^{2}\\hat{L})}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where in (a) we use the fact that $\\rho$ is chosen small; (b) uses finite convergence assumption and $\\|\\mathbf{ab}^{\\top}\\|=\\|\\mathbf{a}\\|\\|\\mathbf{b}\\|$ . Using similar arguments, we can bound $\\mathcal{A}_{t}=\\mathcal{O}(\\rho^{2}\\hat{L})$ . ", "page_idx": 21}, {"type": "text", "text": "Proof for (26). Next, we give upper bound on $|\\boldsymbol{\\mathcal{A}}_{t,l}|$ . Using similar argument as (28), we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\rho u_{t}\\bigl|\\mathbf{y}_{t,l}^{\\top}(\\tilde{\\mathbf{G}}_{t,l}-\\mathbf{G}_{t,l})^{\\top}\\mathbf{g}_{\\mathbf{x}_{t,l}}\\bigr|}\\\\ &{\\leq\\mathcal{O}\\biggl(\\rho^{2}u_{t}^{2}\\hat{L}(\\|\\mathbf{g}_{\\mathbf{y}_{t,l}}\\|+\\|\\mathbf{g}_{\\mathbf{x}_{t,l}}\\|)\\cdot\\|\\mathbf{g}_{\\mathbf{x}_{t,l}}\\|\\biggr)}\\\\ &{=\\rho^{2}\\hat{L}\\cdot\\mathcal{O}\\biggl(\\frac{\\|\\mathbf{g}_{\\mathbf{x}_{t,l}}\\|^{2}}{\\sum_{l=1}^{D}(\\|\\mathbf{g}_{\\mathbf{x}_{t,l}}\\|^{2}+\\|\\mathbf{g}_{\\mathbf{y}_{t,l}}\\|^{2})}+\\frac{\\|\\mathbf{g}_{\\mathbf{x}_{t,l}}\\|\\|\\mathbf{g}_{\\mathbf{y}_{t,l}}\\|}{\\sum_{l=1}^{D}(\\|\\mathbf{g}_{\\mathbf{x}_{t,l}}\\|^{2}+\\|\\mathbf{g}_{\\mathbf{y}_{t,l}}\\|^{2})}\\biggr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Using (29), we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|A_{t,l}|\\leq\\rho^{2}\\hat{L}\\cdot\\mathcal{O}\\bigg(\\frac{\\|\\mathbf{g}_{\\mathbf{x}_{t,l}}\\|^{2}+\\|\\mathbf{g}_{\\mathbf{y}_{t,l}}\\|^{2}}{\\sum_{l=1}^{D}(\\|\\mathbf{g}_{\\mathbf{x}_{t,l}}\\|^{2}+\\|\\mathbf{g}_{\\mathbf{y}_{t,l}}\\|^{2})}+\\frac{\\|\\mathbf{g}_{\\mathbf{x}_{t,l}}\\|\\|\\mathbf{g}_{\\mathbf{y}_{t,l}}\\|}{\\sum_{l=1}^{D}(\\|\\mathbf{g}_{\\mathbf{x}_{t,l}}\\|^{2}+\\|\\mathbf{g}_{\\mathbf{y}_{t,l}}\\|^{2})}\\bigg)}\\\\ &{\\qquad=\\mathcal{O}(\\rho^{2}\\hat{L}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The proof is is thus completed. ", "page_idx": 21}, {"type": "text", "text": "C Missing Proofs for OP ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "C.1 Unbalancedness of SGD in OP ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Theorem 6. Applied SGD or SNGD on problem (1b), both of them ensure that $\\|\\mathbf{x}_{t}\\|^{2}-\\|\\mathbf{y}_{t}\\|^{2}=$ $\\|\\mathbf{x}_{0}\\|^{2}-\\|\\mathbf{y}_{0}\\|^{2}$ for all $t>0$ . In other words, $B_{t}$ keeps unchanged. ", "page_idx": 21}, {"type": "text", "text": "Proof. We consider SGD and NSGD separately. ", "page_idx": 21}, {"type": "text", "text": "SGD. It is straightforward to see that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\|\\mathbf{x}_{t}\\|^{2}}{\\mathrm{d}t}=-2f_{t}^{\\prime}(\\mathbf{x}_{t}^{\\top}\\mathbf{y}_{t})\\mathbf{x}_{t}^{\\top}\\mathbf{y}_{t}=\\frac{\\mathrm{d}\\|\\mathbf{y}_{t}\\|^{2}}{\\mathrm{d}t}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This completes the proof of SGD. ", "page_idx": 21}, {"type": "text", "text": "NSGD. The gradient update of NSGD is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mathbf{x}_{t}}{\\mathrm{d}t}=-\\frac{\\mathbf{g}_{\\mathbf{x}_{t}}}{\\sqrt{\\|\\mathbf{g}_{\\mathbf{x}_{t}}\\|^{2}+\\|\\mathbf{g}_{\\mathbf{y}_{t}}\\|^{2}}},\\quad\\frac{\\mathrm{d}\\mathbf{y}_{t}}{\\mathrm{d}t}=-\\frac{\\mathbf{g}_{\\mathbf{y}_{t}}}{\\sqrt{\\|\\mathbf{g}_{\\mathbf{x}_{t}}\\|^{2}+\\|\\mathbf{g}_{\\mathbf{y}_{t}}\\|^{2}}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then we have that for NSGD, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\|\\mathbf{x}_{t}\\|^{2}}{\\mathrm{d}t}=-2f_{t}^{\\prime}(\\mathbf{x}_{t}^{\\top}\\mathbf{y}_{t})\\frac{\\mathbf{x}_{t}^{\\top}\\mathbf{y}_{t}}{\\sqrt{\\|\\mathbf{g}_{\\mathbf{x}_{t}}\\|^{2}+\\|\\mathbf{g}_{\\mathbf{y}_{t}}\\|^{2}}}=\\frac{\\mathrm{d}\\|\\mathbf{y}_{t}\\|^{2}}{\\mathrm{d}t}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This gives the result for SNGD. ", "page_idx": 21}, {"type": "text", "text": "C.2 Proof of Theorem 3 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "To prove this theorem, we first focus on the dynamic of SAM. ", "page_idx": 22}, {"type": "text", "text": "Lemma 2. Suppose that Assumption $^{\\,l}$ holds. Consider the limiting flow of SAM in (7) with $\\eta\\rightarrow0$ . Let $\\begin{array}{r}{B_{t}:=\\frac{1}{2}\\bigl(\\|\\dot{\\mathbf{x}_{t}}\\|^{2}-\\|\\mathbf{y}_{t}\\|^{2}\\bigr)}\\end{array}$ and $\\rho$ be small. Then, for some $|\\mathcal{A}_{t}|\\stackrel{=}-O(\\rho^{2}L|B_{t}|)$ , SAM guarantees ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{d\\mathcal{B}_{t}}{d t}=-2\\rho\\frac{\\vert f_{t}^{\\prime}(\\mathbf{x}_{t}^{\\top}\\mathbf{y}_{t})\\vert}{\\sqrt{\\Vert\\mathbf{x}_{t}\\Vert^{2}+\\Vert\\mathbf{y}_{t}\\Vert^{2}}}\\mathcal{B}_{t}+\\mathcal{A}_{t}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. For notational convenience, we write $f_{t}^{\\prime}:=f_{t}^{\\prime}(\\mathbf{x}_{t}^{\\top}\\mathbf{y}_{t})$ and $\\tilde{f}_{t}^{\\prime}:=f_{t}^{\\prime}(\\tilde{\\mathbf{x}}_{t}^{\\top}\\tilde{\\mathbf{y}}_{t})$ . Using similar arguments as Theorem 2, we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{1}{2}\\frac{\\mathrm{d}}{\\mathrm{d}t}\\bigg(\\|\\mathbf{x}_{t}\\|^{2}-\\|\\mathbf{y}_{t}\\|^{2}\\bigg)=-\\rho u_{t}\\tilde{f}_{t}^{\\prime}\\cdot\\big(\\|\\mathbf{x}_{t}\\|^{2}-\\|\\mathbf{y}_{t}\\|^{2}\\big)}&{}\\\\ {=-\\rho\\frac{\\mathrm{sgn}(f_{t}^{\\prime})\\tilde{f}_{t}^{\\prime}}{\\sqrt{\\|\\mathbf{x}_{t}\\|^{2}+\\|\\mathbf{y}_{t}\\|^{2}}}\\cdot\\big(\\|\\mathbf{x}_{t}\\|^{2}-\\|\\mathbf{y}_{t}\\|^{2}\\big)}&{}\\\\ {=-\\rho\\frac{\\vert f_{t}^{\\prime}\\vert}{\\sqrt{\\|\\mathbf{x}_{t}\\|^{2}+\\|\\mathbf{y}_{t}\\|^{2}}}\\cdot\\big(\\|\\mathbf{x}_{t}\\|^{2}-\\|\\mathbf{y}_{t}\\|^{2}\\big)}&{}\\\\ {+\\underbrace{\\rho\\frac{\\mathrm{sgn}(f_{t}^{\\prime})(f_{t}^{\\prime}-\\tilde{f}_{t}^{\\prime})}{\\sqrt{\\|\\mathbf{x}_{t}\\|^{2}+\\|\\mathbf{y}_{t}\\|^{2}}}\\cdot\\big(\\|\\mathbf{x}_{t}\\|^{2}-\\|\\mathbf{y}_{t}\\|^{2}\\big)}_{:=A_{t}}.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Next we bound $\\left|\\boldsymbol{\\mathcal{A}}_{t}\\right|$ . To start with, we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\tilde{\\mathbf{x}}_{t}^{\\top}\\tilde{\\mathbf{y}}_{t}-\\mathbf{x}_{t}^{\\top}\\mathbf{y}_{t}\\right|=\\left|\\rho^{2}u_{t}^{2}\\mathbf{x}_{t}^{\\top}\\mathbf{y}_{t}+\\rho u_{t}\\|\\mathbf{x}_{t}\\|^{2}+\\rho u_{t}\\|\\mathbf{y}_{t}\\|^{2}\\right|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\rho^{2}\\frac{\\left|\\mathbf{x}_{t}^{\\top}\\mathbf{y}_{t}\\right|}{\\left\\|\\mathbf{x}_{t}\\right\\|^{2}+\\left\\|\\mathbf{y}_{t}\\right\\|^{2}}+\\rho\\sqrt{\\|\\mathbf{x}_{t}\\|^{2}+\\left\\|\\mathbf{y}_{t}\\right\\|^{2}}}\\\\ &{\\qquad\\qquad\\leq\\frac{\\rho^{2}}{2}+\\rho\\sqrt{\\left\\|\\mathbf{x}_{t}\\right\\|^{2}+\\left\\|\\mathbf{y}_{t}\\right\\|^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Using Assumption 1 and (34), we arrive at ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|f_{t}^{\\prime}-\\tilde{f}_{t}^{\\prime}|\\leq L\\big|\\tilde{\\mathbf{x}}_{t}^{\\top}\\tilde{\\mathbf{y}}_{t}-\\mathbf{x}_{t}^{\\top}\\mathbf{y}_{t}\\big|=\\mathcal{O}(\\rho L\\sqrt{\\|\\mathbf{x}_{t}\\|^{2}+\\|\\mathbf{y}_{t}\\|^{2}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence, we arrive at ", "page_idx": 22}, {"type": "equation", "text": "$$\n|\\mathcal{A}_{t}|\\leq\\rho|f_{t}^{\\prime}-\\tilde{f}_{t}^{\\prime}|\\bigg|\\frac{\\|\\mathbf{x}_{t}\\|^{2}-\\|\\mathbf{y}_{t}\\|^{2}}{\\sqrt{\\|\\mathbf{x}_{t}\\|^{2}+\\|\\mathbf{y}_{t}\\|^{2}}}\\bigg|=\\mathcal{O}(\\rho^{2}L|\\mathcal{B}_{t}|).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The proof is thus completed. ", "page_idx": 22}, {"type": "text", "text": "Next, the proof of Theorem 3 is provided. ", "page_idx": 22}, {"type": "text", "text": "Proof. Lemma 2 has already indicated the concentration of $B_{t}$ towards 0, if the magnitude of the first term is larger than $\\left|\\boldsymbol{\\mathcal{A}}_{t}\\right|$ . To see this, notice that we can lower bound $2|\\mathcal{B}_{t}|/\\sqrt{\\|\\mathbf{x}_{t}\\|^{2}+\\|\\mathbf{y}_{t}\\|^{2}}$ by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left|{\\frac{\\|\\mathbf{x}_{t}\\|^{2}-\\|\\mathbf{y}_{t}\\|^{2}}{{\\sqrt{\\|\\mathbf{x}_{t}\\|^{2}+\\|\\mathbf{y}_{t}\\|^{2}}}}}\\right|=\\left|{\\frac{(\\|\\mathbf{x}_{t}\\|+\\|\\mathbf{y}_{t}\\|)(\\|\\mathbf{x}_{t}\\|-\\|\\mathbf{y}_{t}\\|)}{{\\sqrt{\\left|\\|\\mathbf{x}_{t}\\|^{2}+\\|\\mathbf{y}_{t}\\|^{2}}}}}\\right|\\geq\\left|\\|\\mathbf{x}_{t}\\|-\\|\\mathbf{y}_{t}\\|\\right|={\\mathcal{C}}_{t}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence, long as $\\rho|f_{t}^{\\prime}(\\mathbf{x}_{t}^{\\top}\\mathbf{y}_{t})|\\cdot\\mathcal{C}_{t}>\\mathcal{O}(\\rho^{2}L|\\mathcal{B}_{t}|)$ , we have the first term dominating the dynamic of SAM, leading to contraction of $B_{t}$ . This completes the proof to the first part. ", "page_idx": 22}, {"type": "text", "text": "Next we prove the second part, which is the lower- and upper- bound on $B_{t}$ . The lower bound can be seen from (36). For the upper bound, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left|{\\frac{\\|\\mathbf{x}_{t}\\|^{2}-\\|\\mathbf{y}_{t}\\|^{2}}{{\\sqrt{\\|\\mathbf{x}_{t}\\|^{2}+\\|\\mathbf{y}_{t}\\|^{2}}}}}\\right|\\leq\\left|{\\frac{\\|\\mathbf{x}_{t}\\|^{2}-\\|\\mathbf{y}_{t}\\|^{2}}{{\\sqrt{\\|\\mathbf{x}_{t}\\|^{2}-\\|\\mathbf{y}_{t}\\|^{2}}}}}\\right|={\\sqrt{2|{\\boldsymbol{B}}_{t}|}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Plugging (37) into (33) finishes the proof. ", "page_idx": 22}, {"type": "text", "text": "C.3 $m$ -sharpness for OP ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "$m$ -sharpness is a variant of SAM that is empirically observed to improve generalization, and it is especially useful for distributed training on multiple GPUs (Foret et al., 2021). However, the reason behind the improved performance is not fully understood. (Andriushchenko and Flammarion, 2022) show that $m$ -sharpness is more sparse-promoting for diagonal linear neural networks minimized via a quadratic loss. However, diagonal linear networks are not scale-invariant. ", "page_idx": 23}, {"type": "text", "text": "For consistent notation with (7), we use $f_{t}(\\cdot)$ to denote the loss function on minibatch $\\mathcal{M}_{t}$ . In $m$ -sharpness, the minibatch $\\mathcal{M}_{t}$ is divided into $m$ disjoint subsets. Without loss of generality, we also assume that the minibatch is evenly divided. We denote the loss function on each subset as $f_{t,i},i\\in\\{1,2,\\ldots,m\\}$ . Note that we have $\\textstyle{\\frac{1}{m}}\\sum_{i=1}^{m}f_{t,i}=f_{t}$ . With these definitions, the update of $m$ -sharpness can be written as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\quad\\tilde{\\mathbf{x}}_{t,i}=\\mathbf{x}_{t}+\\rho u_{t,i}\\mathbf{y}_{t},\\quad\\tilde{\\mathbf{y}}_{t,i}=\\mathbf{y}_{t}+\\rho u_{t,i}\\mathbf{x}_{t}}\\\\ &{\\quad\\mathbf{g}_{\\tilde{\\mathbf{x}}_{t,i}}^{i}=f_{t,i}^{\\prime}(\\tilde{\\mathbf{x}}_{t,i}^{\\top}\\tilde{\\mathbf{y}}_{t,i})\\tilde{\\mathbf{y}}_{t,i},\\quad\\mathbf{g}_{\\tilde{\\mathbf{y}}_{t,i}}^{i}=f_{t,i}^{\\prime}(\\tilde{\\mathbf{x}}_{t,i}^{\\top}\\tilde{\\mathbf{y}}_{t,i})\\tilde{\\mathbf{x}}_{t,i}}\\\\ &{\\mathbf{x}_{t+1}=\\mathbf{x}_{t}-\\eta\\frac{1}{m}\\sum_{i=1}^{m}\\mathbf{g}_{\\tilde{\\mathbf{x}}_{t,i}}^{i},\\quad\\mathbf{y}_{t+1}=\\mathbf{y}_{t}-\\eta\\frac{1}{m}\\sum_{i=1}^{m}\\mathbf{g}_{\\tilde{\\mathbf{y}}_{t,i}}^{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $u_{t,i}:=\\mathrm{sgn}(f_{t,i}^{\\prime}(\\mathbf{x}_{t}^{\\top}\\mathbf{y}_{t}))/\\sqrt{\\|\\mathbf{x}_{t}\\|^{2}+\\|\\mathbf{y}_{t}\\|^{2}}$ . Comparing with the SAM update for OP in (7), the difference is that perturbed gradient is calculated on each $f_{t,i}$ . Next, we analyze the dynamic of SAM with $m$ -sharpness. ", "page_idx": 23}, {"type": "text", "text": "Lemma 3. Suppose that Assumption $^{\\,l}$ holds. Consider the limiting flow of SAM in (38) with $\\eta\\rightarrow0$ . Let $\\begin{array}{r}{B_{t}:=\\frac{1}{2}\\bigl(\\|\\dot{\\mathbf{x}}_{t}\\|^{2}-\\|\\mathbf{y}_{t}\\|^{2}\\bigr)}\\end{array}$ and $\\rho$ be small. Then, for some $|{\\mathcal{A}}_{t}|={\\mathcal{O}}(\\rho^{2}L)$ , SAM guarantees that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{d\\mathcal{B}_{t}}{d t}=-2\\frac{\\rho}{m}\\frac{\\sum_{i=1}^{m}|f_{t,i}^{\\prime}(\\mathbf{x}_{t}^{\\top}\\mathbf{y}_{t})|}{\\sqrt{\\left\\|\\mathbf{x}_{t}\\right\\|^{2}+\\left\\|\\mathbf{y}_{t}\\right\\|^{2}}}\\mathcal{B}_{t}+\\mathcal{A}_{t}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. For notational convenience, we write $f_{t,i}^{\\prime}:=f_{t,i}^{\\prime}(\\mathbf{x}_{t}^{\\top}\\mathbf{y}_{t})$ and $\\tilde{f}_{t,i}^{\\prime}:=f_{t,i}^{\\prime}(\\tilde{\\mathbf{x}}_{t,i}^{\\top}\\tilde{\\mathbf{y}}_{t,i})$ . Then, we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{1}{2}\\frac{\\mathrm{d}}{\\mathrm{d}t}\\bigg(\\|\\mathbf{x}_{t}\\|^{2}-\\|\\mathbf{y}_{t}\\|^{2}\\bigg)=-\\frac{\\rho}{m}\\sum_{i=1}^{m}u_{t,i}\\tilde{f}_{t,i}^{\\prime}\\cdot\\big(\\|\\mathbf{x}_{t}\\|^{2}-\\|\\mathbf{y}_{t}\\|^{2}\\big)}&{}\\\\ {=-\\frac{\\rho}{m}\\sum_{i=1}^{m}\\frac{8\\mathrm{gn}(f_{t,i}^{\\prime})\\tilde{f}_{t,i}^{\\prime}}{\\sqrt{\\|\\mathbf{x}_{t}\\|^{2}+\\|\\mathbf{y}_{t}\\|^{2}}}\\cdot\\big(\\|\\mathbf{x}_{t}\\|^{2}-\\|\\mathbf{y}_{t}\\|^{2}\\big)}&{}\\\\ {=-\\frac{\\rho}{m}\\frac{\\sum_{i=1}^{m}|f_{t,i}^{\\prime}|}{\\sqrt{\\|\\mathbf{x}_{t}\\|^{2}+\\|\\mathbf{y}_{t}\\|^{2}}}\\cdot\\big(\\|\\mathbf{x}_{t}\\|^{2}-\\|\\mathbf{y}_{t}\\|^{2}\\big)}&{}\\\\ {+\\frac{\\rho}{m}\\sum_{i=1}^{m}\\underbrace{\\frac{\\mathrm{Sgn}(f_{t,i}^{\\prime})(f_{t,i}^{\\prime}-\\tilde{f}_{t,i}^{\\prime})}{\\sqrt{\\|\\mathbf{x}_{t}\\|^{2}+\\|\\mathbf{y}_{t}\\|^{2}}}\\cdot\\big(\\|\\mathbf{x}_{t}\\|^{2}-\\|\\mathbf{y}_{t}\\|^{2}\\big)}_{:=A_{t,i}}.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Next, using (34) and Assumption 1, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|f_{t,i}^{\\prime}-\\tilde{f}_{t,i}^{\\prime}|\\leq L|\\tilde{\\mathbf{x}}_{t,i}^{\\top}\\tilde{\\mathbf{y}}_{t,i}-\\mathbf{x}_{t}^{\\top}\\mathbf{y}_{t}|=\\mathcal{O}(\\rho L\\sqrt{\\|\\mathbf{x}_{t}\\|^{2}+\\|\\mathbf{y}_{t}\\|^{2}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Hence, we can bound $|\\mathcal{A}_{t,i}|$ as ", "page_idx": 23}, {"type": "equation", "text": "$$\n|A_{t,i}|\\leq|f_{t,i}^{\\prime}-\\tilde{f}_{t,i}^{\\prime}|\\left|\\frac{\\|\\mathbf{x}_{t}\\|^{2}-\\|\\mathbf{y}_{t}\\|^{2}}{\\sqrt{\\|\\mathbf{x}_{t}\\|^{2}+\\|\\mathbf{y}_{t}\\|^{2}}}\\right|=\\mathcal{O}(\\rho L|\\mathcal{B}_{t}|).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The proof is thus completed by plugging $|\\mathcal{A}_{t,i}|$ into (40). ", "page_idx": 23}, {"type": "text", "text": "C.4 Extension to Layer-wise OP ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We start with the notation. Let $l\\,\\in\\,\\{1,2,\\dots,D\\}$ be the layer index. Denote $f_{t}$ as the loss on minibatch $\\mathcal{M}_{t}$ . Let $f_{t,l}^{\\prime}:=\\nabla_{l}f_{t}\\big(\\{\\mathbf{x}_{t,l}^{\\top}\\mathbf{y}_{t,l}\\}_{l}\\big)$ , i.e., the $l$ -th entry of gradient (w.r.t. the variable $\\mathbf{x}_{t,l}^{\\top}\\mathbf{y}_{t,l}),\\,\\tilde{f}_{t,l}^{\\prime}:=\\nabla_{l}f_{t}(\\{\\tilde{\\mathbf{x}}_{t,l}^{\\top}\\tilde{\\mathbf{y}}_{t,l}\\}_{l})$ , and $\\begin{array}{r}{u_{t}:=1/\\sqrt{\\sum_{l=1}^{D}|f_{t,l}^{\\prime}|^{2}\\left[\\|\\mathbf{x}_{t,l}\\|^{2}+\\|\\mathbf{y}_{t,l}\\|^{2}\\right]}}\\end{array}$ . The update of SAM for layer $l$ can be written as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\tilde{\\mathbf{x}}_{t,l}=\\mathbf{x}_{t,l}+\\rho u_{t}f_{t,l}^{\\prime}\\mathbf{y}_{t,l},}&{\\tilde{\\mathbf{y}}_{t,l}=\\mathbf{y}_{t,l}+\\rho u_{t}f_{t,l}^{\\prime}\\mathbf{x}_{t,l},}\\\\ {\\mathbf{g}_{\\tilde{\\mathbf{x}}_{t,l}}=\\tilde{f}_{t,l}^{\\prime}\\tilde{\\mathbf{y}}_{t,l},}&{\\mathbf{g}_{\\tilde{\\mathbf{y}}_{t,l}}=\\tilde{f}_{t,l}^{\\prime}\\tilde{\\mathbf{x}}_{t,l}}\\\\ {\\mathbf{x}_{t+1,l}=\\mathbf{x}_{t,l}-\\eta\\mathbf{g}_{\\tilde{\\mathbf{x}}_{t,l}},}&{\\mathbf{y}_{t+1,l}=\\mathbf{y}_{t,l}-\\eta\\mathbf{g}_{\\tilde{\\mathbf{y}}_{t,l}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Refined assumption for LoRA. Our proof only needs block-wise smoothness, i.e., ", "page_idx": 24}, {"type": "equation", "text": "$$\n|\\nabla_{l}f_{t}(\\mathbf{x}_{l}^{\\top}\\mathbf{y}_{l})-\\nabla_{l}f_{t}(\\mathbf{a}_{l}^{\\top}\\mathbf{b}_{l})|^{2}\\leq\\hat{L}^{2}|\\mathbf{x}_{l}^{\\top}\\mathbf{y}_{l}-\\mathbf{a}_{l}^{\\top}\\mathbf{b}_{l}|^{2},\\;\\forall l,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\nabla_{l}$ refers to the gradient on $\\mathbf{x}_{l}^{\\top}\\mathbf{y}_{l}$ . It can be seen that $\\sqrt{D}\\hat{L}\\geq L$ , but one can assume that $\\sqrt{D}\\hat{L}\\approx L$ for more clear intuition. ", "page_idx": 24}, {"type": "text", "text": "Theorem 7. Suppose that block smoothness assumption in (42) holds. Consider the limiting flow of SAM in (41) with $\\eta\\:\\rightarrow\\:0$ and a sufficiently small $\\rho$ . Let $\\begin{array}{r l}{\\mathscr{B}_{t,l}\\;:=\\;\\frac{1}{2}\\Big(\\|\\mathbf{x}_{t,l}\\|^{2}-\\|\\mathbf{y}_{t,l}\\|^{2}\\Big)}\\end{array}$ and $B_{t}^{\\operatorname*{max}}=\\operatorname*{max}_{l}\\,|B_{t,l}|$ . For some $\\vert\\mathcal{A}_{t}\\vert=\\mathcal{O}(\\rho^{2}\\hat{L}\\mathcal{B}_{t}^{\\operatorname*{max}})$ , SAM guarantees that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{d\\mathcal{B}_{t}}{d t}=-\\rho\\frac{\\sum_{l=1}^{D}|f_{t,l}^{\\prime}|^{2}\\big(\\|\\mathbf{x}_{t,l}\\|^{2}-\\|\\mathbf{y}_{t,l}\\|^{2}\\big)}{\\sqrt{\\sum_{l=1}^{D}|f_{t,l}^{\\prime}|^{2}\\big[\\|\\mathbf{x}_{t,l}\\|^{2}+\\|\\mathbf{y}_{t,l}\\|^{2}\\big]}}+\\mathcal{A}_{t}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Furthermore, for some $|A_{t,l}|=\\mathcal{O}(\\rho^{2}\\hat{L}|B_{t,l}|),$ , per layer balancedness satisfies that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{d\\mathcal{B}_{t,l}}{d t}=-\\rho\\frac{\\vert f_{t,l}^{\\prime}\\vert^{2}\\big(\\Vert\\mathbf{x}_{t,l}\\Vert^{2}-\\Vert\\mathbf{y}_{t,l}\\Vert^{2}\\big)}{\\sqrt{\\sum_{l=1}^{D}\\vert f_{t,l}^{\\prime}\\vert^{2}\\big[\\Vert\\mathbf{x}_{t,l}\\Vert^{2}+\\Vert\\mathbf{y}_{t,l}\\Vert^{2}\\big]}}+A_{t,i}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. Using a similar derivation as before, we have that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{2}\\frac{\\mathrm{d}}{\\mathrm{d}t}\\bigg(\\|\\mathbf{x}_{t,l}\\|^{2}-\\|\\mathbf{y}_{t,l}\\|^{2}\\bigg)=-\\rho u_{t}|f_{t,l}^{\\prime}|^{2}\\cdot\\big(\\|\\mathbf{x}_{t,l}\\|^{2}-\\|\\mathbf{y}_{t,l}\\|^{2}\\big)}\\\\ {+\\underbrace{\\rho u_{t}f_{t,l}^{\\prime}(f_{t,l}^{\\prime}-\\tilde{f}_{t,l}^{\\prime})\\cdot\\big(\\|\\mathbf{x}_{t,l}\\|^{2}-\\|\\mathbf{y}_{t,l}\\|^{2}\\big)}_{:=A_{t,l}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Next, based on (42), we have that ", "page_idx": 24}, {"type": "text", "text": "$\\begin{array}{r}{|f_{t,l}^{\\prime}-\\tilde{f}_{t,l}^{\\prime}|\\leq\\hat{L}\\big|\\tilde{\\mathbf{x}}_{t,l}^{\\top}\\tilde{\\mathbf{y}}_{t,l}-\\mathbf{x}_{t,l}^{\\top}\\mathbf{y}_{t,l}\\big|\\leq\\rho\\hat{L}u_{t}|f_{t,l}^{\\prime}|\\big(\\|\\mathbf{x}_{t,l}\\|^{2}+\\|\\mathbf{y}_{t,l}\\|^{2}\\big)+\\rho^{2}\\hat{L}u_{t}^{2}|f_{t,l}^{\\prime}|^{2}|\\mathbf{x}_{t,l}^{\\top}\\mathbf{y}_{t,l}|.}\\end{array}$ . Combining these two equations, and applying similar argument as Theorem 5, it is not difficult to arrive at $|A_{t,i}|=\\mathcal{O}(\\rho^{2}\\hat{L}|B_{t,l}|)$ and $\\vert\\mathcal{A}_{t}\\vert=\\mathcal{O}(\\rho^{2}\\hat{L}\\mathcal{B}_{t}^{\\operatorname*{max}})$ . \u53e3 ", "page_idx": 24}, {"type": "text", "text": "C.5 Proof of Lemma 1 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Proof. Within $\\mathcal{W}^{*}$ , the Hessian on $(\\mathbf{x},\\mathbf{y})$ can be calculated as $f^{\\prime\\prime}(\\mathbf{x}^{\\top}\\mathbf{y})[\\mathbf{y}^{\\top},\\mathbf{x}^{\\top}]^{\\top}[\\mathbf{y}^{\\top},\\mathbf{x}^{\\top}]$ . The largest eigenvalue is $f^{\\prime\\prime}(w)\\left(\\lvert\\lvert\\mathbf{x}\\rvert\\rvert^{2}+\\lvert\\lvert\\mathbf{y}\\rvert\\rvert^{2}\\right)$ . By the AM-GM inequality, it can be seen that the largest eigenvalue is minimized when $\\|\\mathbf{x}\\|=\\|\\mathbf{y}\\|$ , whose balancedness is 0. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "D Missing Experimental Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We mainly focus on finetuning LMs with LoRA. This setting naturally includes distributional shift \u2013 the finetuning dataset does not usually have the same distribution as the pretraining dataset as validated through zero-shot performance. All experiments are performed on a server with AMD EPYC 7742 CPUs and NVIDIA GeForce RTX 3090 GPUs each with 24GiB memory. All numerical results from Section 6 report test performance (e.g., accuracy, F1 scores, or BLEU scores) and the standard deviation across multiple runs. ", "page_idx": 24}, {"type": "text", "text": "D.1 Details on Datasets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Our evaluations are carried out on commonly-used datasets in the literature. ", "page_idx": 25}, {"type": "text", "text": "GLUE benchmark. GLUE is designed to provide a general-purpose evaluation of language understanding (Wang et al., 2019b). Those adopted in our work include MNLI (inference, (Williams et al., 2018)), SST-2 (sentiment analysis, (Socher et al., 2013)), MRPC (paraphrase detection, (Dolan and Brockett, 2005)), CoLA (linguistic acceptability (Warstadt et al., 2019)), QNLI (inference (Rajpurkar et al., 2018)), $\\mathsf{Q Q P}^{3}$ (question-answering), $\\dot{\\mathrm{RTE^{4}}}$ (inference), and STS-B (textual similarity (Cer et al., 2017)). These datasets are released under different permissive licenses. ", "page_idx": 25}, {"type": "text", "text": "SuperGLUE benchmark. SuperGLUE (Wang et al., 2019a) is another commonly adopted benchmark for language understanding and is more challenging compared with GLUE. The considered datasets include CB (inference, (De Marneffe et al., 2019)), ReCoRD (multiple-choice question answering (Zhang et al., 2018)), COPA (question answering (Roemmele et al., 2011)). These datasets are released under different permissive licenses. ", "page_idx": 25}, {"type": "text", "text": "WebNLG Challenge. This dataset is commonly used for data-to-text evaluation (Gardent et al., 2017). It has 22K examples in total with 14 distinct categories. Among them, 9 are seen during training, and the unseen training data are used to test the generalization performance. The dataset is released under license CC BY-NC-SA 4.0. ", "page_idx": 25}, {"type": "text", "text": "Additional datasets. We also use SQuAD (question answering (Rajpurkar et al., 2016)) in our experiments, which is released under license CC BY-SA 4.0. Other datasets include TREC (topic classification (Voorhees and Tice, 2000)) and SNLI (inference (Bowman et al., 2015)). Both of them are licensed under CC BY-SA 4.0. ", "page_idx": 25}, {"type": "text", "text": "D.2 Details on Language Models ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We summarize the adopted language models in our evaluation. All model checkpoints are obtained from HuggingFace. ", "page_idx": 25}, {"type": "text", "text": "RoBERTa-large. This is a $355\\mathrm{M}$ parameter model. The model checkpoint5 is released under the MIT license. ", "page_idx": 25}, {"type": "text", "text": "OPT-1.3B. The model checkpoint6 is released under a non-commercial license. ", "page_idx": 25}, {"type": "text", "text": "GPT2-medium. This is a 345M parameter model. Its checkpoint8 is under MIT License. ", "page_idx": 25}, {"type": "text", "text": "D.3 Few-shot Learning with RoBERTa and OPT ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Experiments on RoBERTa-large. We follow the $k$ -shot learning setup in (Malladi et al., 2023) and focus on classification tasks. The training set contains $k=512$ samples per class while the test set has 1000 samples. We also employ prompts for finetuning; where the adopted prompts are the same as those in (Malladi et al., 2023, Table 13). AdamW is adopted as the base optimizer, and hyperparameters are tuned from those in Table 6. Our experiments are averaged over 3 random trials. The estimated runtime is about 5 minutes per dataset. ", "page_idx": 25}, {"type": "text", "text": "The per-iteration runtime on the SST-5 dataset of BAR, SAM, and the baseline optimizer are compared in Table 7. It can be seen that SAM is much more slower than the baseline approach, and BAR reduces $74\\%$ additional runtime of SAM, while achieving comparable accuracy. We believe that this runtime saving can be even larger with additional engineering efforts such as kernel fusion, which we leave for future work. This validates the computational efficiency of BAR. ", "page_idx": 25}, {"type": "text", "text": "Table 6: Hyperparameters used for few-shot learning with RoBERTa-large. ", "page_idx": 26}, {"type": "table", "img_path": "oSOVME9kl2/tmp/50137bb03908d01550437b99c819fac87d843ae7eaa980d2790505a7f60aa764.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "oSOVME9kl2/tmp/08faebc5b9e47b94d0792acd6db64c55a15cdf8f042532741246d9a7f1a335b8.jpg", "table_caption": ["Table 7: Per-iteration runtime for finetuning RoBERTa-large on SST5. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "Experiments on OPT. For OPT-1.3B, we consider tasks from the SuperGLUE benchmark covering classification and multiple-choice. We also consider generation tasks on SQuAD. Following (Malladi et al., 2023), we randomly sample 1000 data for training and the other 1000 for testing. AdamW is adopted as base optimizer. The hyperparameters adopted are searched over values in Table 8. Estimated runtime is less than or around 10 minutes, depending on the dataset. ", "page_idx": 26}, {"type": "text", "text": "If we directly apply FP16 training with SAM, underflow can happen if one does not take care of the gradient scaling on the two gradients calculated per iteration. This means that SAM is not flexible enough to be integrated with the codebase for large scale training, as FP16 is the default choice for finetuning LMs. We employ FP32 to bypass the issue with SAM. Consequently, the training speed is significantly slowed down; see a summary in Table 9. It further demonstrates the effectiveness of BAR for large scale-training. ", "page_idx": 26}, {"type": "text", "text": "Overall, the results for few-shot learning indicate that given limited data, BAR can effectively improve generalization using significantly reduced computational resources relative to SAM. ", "page_idx": 26}, {"type": "table", "img_path": "oSOVME9kl2/tmp/3a2e2678e90dc062c208f391940f90955fb45b5d1c8d1b780f231b70ce662ad2.jpg", "table_caption": ["Table 8: Hyperparameters used for few-shot learning with OPT-1.3B. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "D.4 Finetuning with RoBERTa-large ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Our implementation is inspired from (Hu et al., $2022)^{9}$ , which is under MIT License. The hyperparameters are chosen the same as provided in its GitHub Repo. AdamW is adopted as the base optimizer. However, we employ single GPU rather than multiple ones and use gradient accumulation rather than parallelism due to memory constraint. We also note that there could be failure cases for LoRA using certain seed, e.g., SST-2 with seed 1 and MNLI with seed 2. These cases are ignored when comparing. We consider the GLUE benchmark and report the mismatched accuracy for MNLI, Matthew\u2019s correlation for CoLA, Pearson correlation for STS-B, and accuracy for other datasets. Larger values indicate better results for all datasets. For LoRA, we employ $r\\,=\\,8$ and $\\alpha\\,=\\,16$ . ", "page_idx": 26}, {"type": "table", "img_path": "oSOVME9kl2/tmp/3fbdd79a7325f5d2e1302fb98050da1e1e82f84e99fadf8f6b62bf0445d322c2.jpg", "table_caption": ["Table 9: Per-iteration runtime for finetuning OPT-1.3B on RTE. "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "oSOVME9kl2/tmp/466c754939caf55c781624e8b1b04f499f26eb149e5c80989af79b8354969aee.jpg", "table_caption": ["Table 10: Experiments on finetuning RoBERTa (355M). Results marked with $\\dagger$ are taken from (Hu et al., 2022), and those with $^*$ refer to AdapterP in (Hu et al., 2022). "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Experiments are conducted over three random trials for all datasets, with the exception of QQP, for which only two trials are performed due to its large size. The results of final test performance can be found in Table 10. Estimated runtime varies for different datasets from 2 to 15 hours, except for QQP which takes 3 days on our device. ", "page_idx": 27}, {"type": "text", "text": "For the hyperparameters of oBAR and nBAR, $\\mu_{0}$ is typically chosen from $\\{0.2,0.5,1.0\\}$ ; however, for QQP, a value of 0.05 is used. The scheduler is chosen from linear and constant. We also observe that for datasets such as COLA and RTE, setting weight decay as 0 works best for BAR. ", "page_idx": 27}, {"type": "text", "text": "D.5 GPT2 medium on WebNLG Challenge ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "AdamW is adopted as base optimizer. The hyperparameters can be found in Table 11. Our results are obtained from three random trials. Each trial takes roughly 8 hours on our hardware. ", "page_idx": 27}, {"type": "table", "img_path": "oSOVME9kl2/tmp/7a3736667fa6ca2859b21aaa5b333fd5f89b14b3d3c98c252fea02af76584a37.jpg", "table_caption": ["Table 11: Hyperparameters used for GPT2. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our claims are supported by theoretical results in Sections 3 and 4 and numerical experiments in Sections 5 and 6. Due to space limitation, missing proofs and implementation details can be found in the appendix. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: Limitation is discussed in Section 7. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: All assumptions are stated along with theorems. The proofs are listed in appendix. All theories and equations are properly referenced. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The details on proposed algorithms can be found in Section 5 and Appendix A.6. Experimental details on setups, datasets, architectures, and hyperparameters can be found in Section 6 and Appendix D. Code can be found in our github repo. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The code is open-sourced on github. The code gathers details for reproducing our experiments. For example, the exact environment is listed in environment.yml. Instructions on preparation (e.g., data, packages, etc) and commands to use are stated in ReadMe.md. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: Experimental details can be found in Section 6 and Appendix D. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Mean and standard deviation are obtained by three random trials for most of experiments. See details in Appendix D. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 30}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Hardware for our experiments is detailed in Appendix D. Estimated runtime is also provided in Appendix D. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: NeurIPS Code of Ethics is followed. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Broader impacts can be found in Appendix A.1. It is put in appendix due to space limitation. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 31}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This paper does not release new data or models. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The datasets and model checkpoints used in this work are widely adopted ones in the field. Their licenses are listed in Appendix D. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our code is released under MIT license, unless model checkpoints and datasets are under more restrictive licenses; see more in supplementary materials. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}]