{"references": [{"fullname_first_author": "Peter Bartlett", "paper_title": "The dynamics of sharpness-aware minimization: Bouncing across ravines and drifting towards wide minima", "publication_date": "2023-00-00", "reason": "This paper provides a theoretical analysis of sharpness-aware minimization, explaining its implicit regularization and dynamic behavior, which is a central focus of the current paper."}, {"fullname_first_author": "Gintare Karolina Dziugaite", "paper_title": "Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data", "publication_date": "2017-00-00", "reason": "This paper is foundational for understanding generalization in deep learning, particularly when the number of parameters exceeds the training data size, which is highly relevant to the context of implicit regularization."}, {"fullname_first_author": "Sanjeev Arora", "paper_title": "On the optimization of deep networks: Implicit acceleration by overparameterization", "publication_date": "2018-00-00", "reason": "This paper is crucial for establishing the theoretical underpinnings of implicit regularization, a key concept explored in the current work's analysis of SAM."}, {"fullname_first_author": "Pierre Foret", "paper_title": "Sharpness-aware minimization for efficiently improving generalization", "publication_date": "2021-00-00", "reason": "This paper introduces Sharpness-Aware Minimization (SAM), the optimization method that is the primary subject of the current paper's analysis."}, {"fullname_first_author": "L\u00e9on Bottou", "paper_title": "Optimization methods for large-scale machine learning", "publication_date": "2018-00-00", "reason": "This review paper provides a comprehensive overview of optimization methods used in large-scale machine learning, which is the foundational context for the development and analysis of SAM in this paper."}]}