[{"figure_path": "oSOVME9kl2/tables/tables_2_1.jpg", "caption": "Table 1: Few shot learning on RoBERTa (355M). \u2020 denotes results reported by (Malladi et al., 2023)", "description": "This table presents the results of few-shot learning experiments conducted on the RoBERTa-large language model (355M parameters).  It compares the performance of LoRA (Low-Rank Adaptation), LoRA combined with SAM (Sharpness-Aware Minimization), and LoRA combined with the proposed BAR (Balancedness-Aware Regularization) methods.  The results are shown for several downstream tasks (RTE, SST-2, SST-5, SNLI, MNLI, and TREC), with the average performance across tasks also provided. The 'Zero-Shot' row shows the performance without any fine-tuning, serving as a baseline.", "section": "6 Numerical Experiments"}, {"figure_path": "oSOVME9kl2/tables/tables_7_1.jpg", "caption": "Table 1: Few shot learning on RoBERTa (355M). \u2020 denotes results reported by (Malladi et al., 2023)", "description": "This table presents the results of few-shot learning experiments conducted using a RoBERTa-large language model (355M parameters).  It compares the performance of different approaches: LoRA (Low-Rank Adaptation), LoRA combined with SAM (Sharpness-Aware Minimization), LoRA with oBAR (Overparametrized Balancedness-Aware Regularization), and LoRA with nBAR (Non-Overparametrized Balancedness-Aware Regularization). The results are shown for various downstream tasks (RTE, SST-2, SST-5, SNLI, MNLI, and TREC) and are reported as average accuracy with standard deviation. A zero-shot baseline is included for reference.", "section": "6.1 Few-shot Learning with RoBERTa-large and OPT-1.3B"}, {"figure_path": "oSOVME9kl2/tables/tables_8_1.jpg", "caption": "Table 1: Few shot learning on RoBERTa (355M). \u2020 denotes results reported by (Malladi et al., 2023)", "description": "This table presents the results of few-shot learning experiments using the RoBERTa-large language model.  The results compare the performance of LoRA (Low-Rank Adaptation) with three variations: LoRA-SAM (Sharpness-Aware Minimization), LoRA-oBAR (Overparametrized Balancedness-Aware Regularization), and LoRA-nBAR (Non-Overparametrized Balancedness-Aware Regularization).  The table shows the average accuracy achieved on seven different downstream tasks (RTE, SST-2, SST-5, SNLI, MNLI, RTE, TREC), along with the zero-shot performance (without any fine-tuning) for comparison.  The results demonstrate the effectiveness of BAR in achieving comparable or better performance than SAM while significantly reducing computational overhead.", "section": "6.1 Few-shot Learning with RoBERTa-large and OPT-1.3B"}, {"figure_path": "oSOVME9kl2/tables/tables_8_2.jpg", "caption": "Table 2: Runtime of BAR (normalized to LoRA, 1x) on OPT-1.3B. SAM relies on FP32 for stability. LORA and BAR adopt FP16 training since this is the default choice for large models. nBAR and OBAR share similar runtime, hence reported together.", "description": "This table shows the runtime of Balancedness-Aware Regularization (BAR) compared to LoRA and SAM on the OPT-1.3B model.  The runtime is normalized relative to LoRA, with 1x representing the runtime of LoRA.  The table highlights that BAR is significantly faster than SAM, while maintaining similar performance.  The use of FP16 precision for LORA and BAR contributes to their speed advantage over SAM, which uses FP32 for stability reasons.", "section": "6 Numerical Experiments"}, {"figure_path": "oSOVME9kl2/tables/tables_9_1.jpg", "caption": "Table 3: Performance of BAR for few shot learning using OPT-1.3B.", "description": "This table presents the results of few-shot learning experiments using the OPT-1.3B language model.  The performance of LoRA (Low-Rank Adaptation), LoRA with SAM (Sharpness-Aware Minimization), LoRA with oBAR (Overparametrized Balancedness-Aware Regularization), and LoRA with nBAR (Non-Overparametrized Balancedness-Aware Regularization) are compared. The results are shown for several datasets (SST-2, CB, RTE, COPA, ReCoRD, and SQUAD), along with a zero-shot baseline.  The average performance across all datasets is also given.  The table highlights the improved performance of BAR compared to LoRA and LoRA with SAM, demonstrating BAR's effectiveness in few-shot learning scenarios.", "section": "6.1 Few-shot Learning with RoBERTa-large and OPT-1.3B"}, {"figure_path": "oSOVME9kl2/tables/tables_9_2.jpg", "caption": "Table 4: Finetuning RoBERTa (355M) with BAR. Results marked with \u2020 are taken from (Hu et al., 2022), and those with * refer to Adapter in (Hu et al., 2022).", "description": "This table shows the results of finetuning a RoBERTa-large language model (355M parameters) using different methods: full finetuning (FT+), Adapter, LoRA, LoRA with OBAR, and LoRA with nBAR.  The results are presented for several downstream tasks (STS-B, RTE, MRPC, CoLA, QQP) and an average score across all tasks.  The table demonstrates the performance improvement of the proposed balancedness-aware regularization (BAR) methods compared to standard LoRA and other baselines.", "section": "6 Numerical Experiments"}, {"figure_path": "oSOVME9kl2/tables/tables_9_3.jpg", "caption": "Table 5: Finetuning GPT2 (345M) with BAR on WebNLG. Results of prefix tuning and full-parameter finetuning are obtained from (Hu et al., 2022).", "description": "This table shows the BLEU scores achieved by different methods for text generation on the WebNLG dataset using the GPT2-medium language model.  It compares the performance of full fine-tuning (FT), prefix tuning, LoRA, LoRA with OBAR, and LoRA with nBAR.  The results highlight the improved performance of LoRA when combined with BAR compared to standard LoRA and other methods.", "section": "6.3 Text Generation on GPT2-medium"}, {"figure_path": "oSOVME9kl2/tables/tables_26_1.jpg", "caption": "Table 6: Hyperparameters used for few-shot learning with RoBERTa-large.", "description": "This table shows the hyperparameter settings used for few-shot learning experiments with the RoBERTa-large model.  It lists the values tested for several key hyperparameters, including LoRA rank, LoRA alpha, number of training iterations, batch size, learning rate, sharpness parameter (p) for SAM, and the mu0 and scheduler values for BAR.", "section": "6 Numerical Experiments"}, {"figure_path": "oSOVME9kl2/tables/tables_26_2.jpg", "caption": "Table 7: Per-iteration runtime for finetuning RoBERTa-large on SST5.", "description": "This table shows the per-iteration runtime for training a RoBERTa-large model on the SST5 dataset using three different optimization methods: a baseline method, Sharpness-Aware Minimization (SAM), and Balancedness-Aware Regularization (BAR).  The results demonstrate the computational efficiency of BAR compared to SAM, while maintaining comparable performance.", "section": "6.1 Few-shot Learning with RoBERTa-large and OPT-1.3B"}, {"figure_path": "oSOVME9kl2/tables/tables_26_3.jpg", "caption": "Table 6: Hyperparameters used for few-shot learning with RoBERTa-large.", "description": "This table shows the hyperparameters used for few-shot learning experiments with the RoBERTa-large model.  The hyperparameters include the rank and scaling factor for LoRA, the number of training iterations, batch size, learning rate for the optimizer, the radius parameter (\u03c1) for SAM, the regularization coefficient (\u03bc0) for BAR, and the scheduling strategy used for BAR (linear or cosine).  These settings were likely tuned to optimize the performance of the model on the specific tasks.", "section": "6 Numerical Experiments"}, {"figure_path": "oSOVME9kl2/tables/tables_27_1.jpg", "caption": "Table 9: Per-iteration runtime for finetuning OPT-1.3B on RTE.", "description": "This table compares the per-iteration runtime of three different optimization methods: baseline, SAM, and BAR, when fine-tuning the OPT-1.3B model on the RTE dataset.  It highlights the significant reduction in runtime achieved by BAR compared to SAM while maintaining comparable precision using FP16.", "section": "6.1 Few-shot Learning with RoBERTa-large and OPT-1.3B"}, {"figure_path": "oSOVME9kl2/tables/tables_27_2.jpg", "caption": "Table 4: Finetuning RoBERTa (355M) with BAR. Results marked with \u2020 are taken from (Hu et al., 2022), and those with * refer to Adapter in (Hu et al., 2022).", "description": "This table shows the results of finetuning a RoBERTa-large language model (355M parameters) using three different methods: LoRA, LoRA-OBAR, and LoRA-nBAR.  The performance is evaluated on several downstream tasks, including STS-B, RTE, MRPC, MNLI, COLA, and QQP. The results are compared against a full finetuning approach (FT) and an Adapter-based approach. The table highlights the improved accuracy achieved by the proposed balancedness-aware regularization (BAR) methods while maintaining comparable efficiency to LoRA.", "section": "6 Numerical Experiments"}, {"figure_path": "oSOVME9kl2/tables/tables_27_3.jpg", "caption": "Table 3: Performance of BAR for few shot learning using OPT-1.3B.", "description": "This table presents the results of applying Balancedness-Aware Regularization (BAR) to few-shot learning tasks using the OPT-1.3B language model.  It compares the performance of BAR against LoRA (a low-rank adaptation technique), LoRA combined with SAM (Sharpness-Aware Minimization), and a zero-shot baseline.  The results are shown for several different datasets and metrics, demonstrating the improvement in performance achieved by BAR.", "section": "6.1 Few-shot Learning with RoBERTa-large and OPT-1.3B"}]