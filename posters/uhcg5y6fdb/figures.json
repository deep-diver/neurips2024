[{"figure_path": "uHcG5Y6fdB/figures/figures_2_1.jpg", "caption": "Figure 1: In-context generalization error (with standard deviation) of kernel ridge regression, neural network + gradient descent, and pretrained transformer. The target function is a polynomial single-index model. We fix r = 8 and vary d = 16, 32.", "description": "The figure shows the in-context generalization error for three different learning algorithms: kernel ridge regression, a two-layer neural network trained with gradient descent, and a pretrained transformer.  The task was to learn a polynomial single-index model, where the dimensionality of the index feature vector (r) was fixed at 8 while the ambient dimensionality (d) was varied (16 and 32). The plot illustrates the prediction risk as a function of the number of in-context examples (N*).  It demonstrates that the pretrained transformer achieves lower prediction risk with fewer in-context examples compared to the baseline algorithms.", "section": "Synthetic Experiments"}, {"figure_path": "uHcG5Y6fdB/figures/figures_9_1.jpg", "caption": "Figure 2: In-context sample complexity of GPT-2 model pretrained on Gaussian single-index function (see Section 4.1 for details) of degree-4 polynomial. Observe that (a) the ICL risk curve overlaps for different ambient dimensions d but the same target (subspace) dimensionality r, and (b) the required sample size N* becomes larger as r increases.", "description": "This figure displays the in-context sample complexity results for a GPT-2 model trained on Gaussian single-index functions with a degree-4 polynomial.  The left subplot (a) demonstrates the model's performance across varying ambient dimensions (d) while keeping the target subspace dimensionality (r) constant.  The results show that the in-context sample complexity is nearly independent of the ambient dimension,  suggesting adaptability to the underlying low-dimensional structure. The right subplot (b) shows the impact of varying the target subspace dimensionality (r) while keeping the ambient dimension (d) constant. As expected, increasing the target dimensionality increases the required number of in-context examples for effective learning.", "section": "4 Synthetic Experiments"}, {"figure_path": "uHcG5Y6fdB/figures/figures_43_1.jpg", "caption": "Figure 1: In-context generalization error (with standard deviation) of kernel ridge regression, neural network + gradient descent, and pretrained transformer. The target function is a polynomial single-index model. We fix r = 8 and vary d = 16, 32.", "description": "This figure compares the in-context generalization error for three different methods: kernel ridge regression, a two-layer neural network trained with gradient descent, and a pretrained transformer.  The experiment uses a polynomial single-index model as the target function, keeping the subspace dimensionality (r) constant at 8 while varying the ambient dimensionality (d) between 16 and 32. The x-axis represents the number of in-context examples used, and the y-axis represents the prediction risk (generalization error). The shaded areas represent the standard deviations.", "section": "Synthetic Experiments"}]