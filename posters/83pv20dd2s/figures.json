[{"figure_path": "83pV20DD2s/figures/figures_1_1.jpg", "caption": "Figure 1: Framework of SCG. SCG has two components. One is to promote the network to spontaneously specialize different functional modules through our designed modular equivariance constraint; The other is to perform self-supervised controllable generation through pattern completion.", "description": "This figure illustrates the framework of the Self-supervised Controllable Generation (SCG) method.  It highlights the two main components: 1) A modular autoencoder that promotes spontaneous functional specialization in different modules through an equivariance constraint, and 2) a self-supervised pattern completion approach for controllable generation. The illustration shows how the network is designed to mimic the brain's modularity and pattern completion capabilities for associative generation, making it capable of self-supervised learning.", "section": "1 Introduction"}, {"figure_path": "83pV20DD2s/figures/figures_3_1.jpg", "caption": "Figure 2: Detail architecture of proposed Modular Autoencoder. The latent space is divided into several modules. We use a prediction matrix M(i)(\u03b4) to build relationship on latent space between input image pairs. M(i) is the learnable codebooks for each modules.", "description": "This figure details the architecture of the Modular Autoencoder, a key component of the Self-Supervised Controllable Generation (SCG) framework.  The latent space is divided into modules, each with its own learnable codebook (M(i)). A prediction matrix M(i)(\u03b4) connects the latent space representations of input image pairs (I and I\u2019), where I\u2019 is a transformed version of I (e.g., translated or rotated). This design promotes intra-module correlation and inter-module independence, encouraging functional specialization within the modules.", "section": "3 Self-supervised Controllable Generation"}, {"figure_path": "83pV20DD2s/figures/figures_4_1.jpg", "caption": "Figure 3: Feature Visualization of modular autoencoder. Each panel shows all features learned by an individual model with multiple modules (one module each row). We trained modular autoencoder with a translation-rotation equivariance constraint on a)MNIST and b)ImageNet, respectively. c) On ImageNet, we also train an autoencoder with an additional translation equivariance constraint besides the translation-rotation equivariance constraint on each module. d) We visualize reconstructed images by features of each module in c.", "description": "This figure visualizes the features learned by a modular autoencoder trained with an equivariance constraint.  The autoencoder is trained on MNIST and ImageNet datasets.  (a) and (b) show the features learned with a translation-rotation equivariance constraint on MNIST and ImageNet, respectively.  (c) shows the features learned on ImageNet with an additional translation equivariance constraint.  (d) shows reconstructed images using features from (c), demonstrating functional specialization of the modules.", "section": "Modular Autoencoder"}, {"figure_path": "83pV20DD2s/figures/figures_6_1.jpg", "caption": "Figure 4: Images generated by SCG in MS-COCO. The upper part shows an image randomly selected in MS-COCO with a text prompt. On the right show the condition images extracted from our modular autocoder and the corresponding generated images. The last column is a generated image by ControlNet conditioned by the canny edge. The bottom part shows more generated images. The three row images are original, condition and generated images, respectively. (See more in Figure S7 and S6)", "description": "This figure shows examples of images generated using the Self-Supervised Controllable Generation (SCG) method on the MS-COCO dataset.  The top row displays a randomly chosen image and its corresponding text prompt. Below, the second row presents the condition images derived from specialized modules (HC0-HC3) of the modular autoencoder. These modules capture different aspects of the image, such as color, brightness, and edges.  The third row shows the images generated by SCG using each condition image. For comparison, the far-right column displays an image generated by the ControlNet method conditioned using a Canny edge map. The bottom section provides more examples of generated images using different input conditions and modules.", "section": "Self-supervised Controllable Generation on MS-COCO"}, {"figure_path": "83pV20DD2s/figures/figures_6_2.jpg", "caption": "Figure 5: Subjective evaluation on zero-shot oil painting association generation.", "description": "This figure shows the results of a subjective evaluation comparing ControlNet and SCG on zero-shot oil painting association generation.  The winning rates for both fidelity and aesthetics are displayed for each method.  The bars represent the percentage of times participants preferred ControlNet (gray bars) or SCG (purple bars) for each aspect.  The results show that SCG has a significantly higher winning rate in aesthetics and comparable results in fidelity, indicating that SCG generates more aesthetically pleasing results while maintaining comparable fidelity to the original oil paintings.", "section": "Experiments and Results"}, {"figure_path": "83pV20DD2s/figures/figures_7_1.jpg", "caption": "Figure 6: Association generation on manual sketches. The original sketches are from ControlNet[55].", "description": "This figure shows a comparison between ControlNet and the proposed SCG method on associative generation tasks using manual sketches.  The first row displays the original sketches, which are taken from the ControlNet paper. The second row presents the results generated by ControlNet, using the Canny edge detector as a condition. The third row illustrates the results generated by the SCG approach, employing HC3 (one of its specialized modules) as the condition. The figure highlights the capability of SCG to perform associative generation by associating sketches with photorealistic images, indicating its potential for zero-shot generalization.", "section": "5 Conditional Associative Generation"}, {"figure_path": "83pV20DD2s/figures/figures_7_2.jpg", "caption": "Figure 7: Associative generation on oil painting (top) and wash and ink painting (bottom). (See more generation results in Figure S10 and Figure S9)", "description": "This figure demonstrates the model's ability to perform associative generation, specifically associating sketches with realistic images. The top row shows the results for oil paintings and the bottom row shows the results for wash and ink paintings. Each set shows the original image, the generated image by ControlNet using Canny edge detection, and two generated images by the proposed SCG method using different modules (HC1 and HC3) as conditions.  The results illustrate the model's capability to handle diverse artistic styles and generate images with high fidelity and aesthetics.", "section": "5 Conditional Associative Generation"}, {"figure_path": "83pV20DD2s/figures/figures_8_1.jpg", "caption": "Figure 8: Association generation on ancient graffiti on rock. (See more generations in Figure S8)", "description": "This figure presents a comparison of associative generation results on ancient rock graffiti between ControlNet and the proposed SCG method.  The left column shows the original graffiti, the middle column shows the results using ControlNet conditioned by the Canny edge detector, and the right column shows results from SCG using module HC3 (sensitive to brightness) as the condition.  Below each set of image results are bar charts showing the win rates (in percent) for fidelity and aesthetics for each method as determined by human evaluators.  The figure demonstrates that SCG's results are more visually appealing and faithful to the original image compared to ControlNet, particularly when dealing with the inherent noise present in the ancient graffiti.", "section": "5 Conditional Associative Generation"}, {"figure_path": "83pV20DD2s/figures/figures_14_1.jpg", "caption": "Figure S1: Ablation study on equivariance constraint.", "description": "This figure shows the ablation study on the effect of the equivariance constraint on the modular autoencoder. The left panel (a) shows the feature maps and reconstruction results obtained with the equivariance constraint. The learned features exhibit clear functional specialization, with each module focusing on different aspects of the input image (e.g., orientation, frequency). The right panel (d) shows the results without the equivariance constraint, illustrating a lack of functional specialization and less organized feature representation. The middle panels (b, c) provide a more detailed view of the feature maps and reconstructions from one of the hypercolumns for better understanding.  This demonstrates that the equivariance constraint plays a crucial role in enabling the network to spontaneously specialize different functional modules.", "section": "A Appendix / supplemental material"}, {"figure_path": "83pV20DD2s/figures/figures_15_1.jpg", "caption": "Figure S2: Ablation study on Modular Autoencoder.", "description": "This figure presents an ablation study on the modular autoencoder, showing the effects of removing the equivariance constraint, rotation, translation, and symmetry loss. It compares the learned features with those obtained using PCA, demonstrating the effectiveness of the proposed equivariance constraint in achieving functional specialization and the importance of each component in the modular autoencoder architecture.", "section": "A Appendix / supplemental material"}, {"figure_path": "83pV20DD2s/figures/figures_16_1.jpg", "caption": "Figure S3: Color antagonism and center-surround receptive fields.", "description": "This figure visualizes the color antagonism and center-surround receptive fields found in the learned features of the modular autoencoder. It shows visualizations of positive and negative weights, along with the input image and the deconvolution of each hypercolumn. The results demonstrate brain-like features such as color antagonism and center-surround receptive fields, which contribute to the model's robustness to noise.", "section": "A.3 Brain-like Characteristics"}, {"figure_path": "83pV20DD2s/figures/figures_16_2.jpg", "caption": "Figure 3: Feature Visualization of modular autoencoder. Each panel shows all features learned by an individual model with multiple modules (one module each row). We trained modular autoencoder with a translation-rotation equivariance constraint on a)MNIST and b)ImageNet, respectively. c) On ImageNet, we also train an autoencoder with an additional translation equivariance constraint besides the translation-rotation equivariance constraint on each module. d) We visualize reconstructed images by features of each module in c.", "description": "This figure visualizes the features learned by a modular autoencoder trained with different equivariance constraints.  It shows how the model spontaneously develops specialized functional modules for processing various aspects of images such as orientation, color, and brightness.  The visualization demonstrates brain-like characteristics such as orientation selectivity and color antagonism, highlighting the effectiveness of the proposed approach.", "section": "Modular Autoencoder"}, {"figure_path": "83pV20DD2s/figures/figures_17_1.jpg", "caption": "Figure 1: Framework of SCG. SCG has two components. One is to promote the network to spontaneously specialize different functional modules through our designed modular equivariance constraint; The other is to perform self-supervised controllable generation through pattern completion.", "description": "This figure illustrates the framework of the Self-supervised Controllable Generation (SCG) method proposed in the paper.  SCG comprises two main components:\n\n1.  **Modular Equivariance Constraint:** This component aims to encourage the network to automatically develop specialized functional modules.  The design uses an equivariance constraint to achieve this spontaneous specialization.\n\n2.  **Self-Supervised Pattern Completion:** This component employs a self-supervised learning approach that leverages pattern completion for controllable generation. It doesn't rely on annotated training data, making it more scalable.", "section": "1 Introduction"}, {"figure_path": "83pV20DD2s/figures/figures_18_1.jpg", "caption": "Figure 1: Framework of SCG. SCG has two components. One is to promote the network to spontaneously specialize different functional modules through our designed modular equivariance constraint; The other is to perform self-supervised controllable generation through pattern completion.", "description": "This figure illustrates the framework of the Self-Supervised Controllable Generation (SCG) method.  SCG consists of two main parts: a modular autoencoder which encourages functional specialization in its various modules via an equivariance constraint and a pattern completion module, which leverages a pre-trained diffusion model to perform controllable generation in a self-supervised manner (without the need for labeled data). The modular autoencoder is designed to separate the input image's features (modalities) into independent modules, mimicking the brain's modular structure. These specialized modules then serve as inputs or conditions for the pattern completion module, which completes the input image.", "section": "1 Introduction"}, {"figure_path": "83pV20DD2s/figures/figures_19_1.jpg", "caption": "Figure 3: Feature Visualization of modular autoencoder. Each panel shows all features learned by an individual model with multiple modules (one module each row). We trained modular autoencoder with a translation-rotation equivariance constraint on a)MNIST and b)ImageNet, respectively. c) On ImageNet, we also train an autoencoder with an additional translation equivariance constraint besides the translation-rotation equivariance constraint on each module. d) We visualize reconstructed images by features of each module in c.", "description": "This figure visualizes the features learned by a modular autoencoder trained with an equivariance constraint.  It shows how different modules specialize in different aspects of image features (MNIST and ImageNet are used). The results demonstrate that the modules develop orientation selectivity, color antagonism, and center-surround receptive fields, similar to those observed in the visual cortex.  The additional translation equivariance constraint on ImageNet leads to improved feature specialization, as visualized by reconstructed images from individual modules.", "section": "Modular Autoencoder"}, {"figure_path": "83pV20DD2s/figures/figures_19_2.jpg", "caption": "Figure 8: Association generation on ancient graffiti on rock. (See more generations in Figure S8)", "description": "This figure compares the results of ControlNet and SCG on generating images based on ancient graffiti.  The top row shows an example of ancient graffiti depicting a horse, followed by the results from ControlNet using a Canny edge detector as a condition, and then the results from SCG using its learned modules as conditions. The bottom row shows a similar comparison but with a different piece of ancient graffiti. The figure highlights the superior robustness of SCG in generating high-fidelity and aesthetically pleasing images compared to ControlNet, especially when dealing with the noise inherent in ancient rock art.", "section": "5 Conditional Associative Generation"}, {"figure_path": "83pV20DD2s/figures/figures_20_1.jpg", "caption": "Figure 7: Associative generation on oil painting (top) and wash and ink painting (bottom). (See more generation results in Figure S10 and Figure S9)", "description": "This figure demonstrates the zero-shot generalization capabilities of the proposed SCG framework on associative generation tasks.  The top row shows examples of associative generation applied to Western-style oil paintings. The original oil painting is shown, along with a Canny edge map (used as a condition in ControlNet), and the generated images from both ControlNet and SCG (using HC1 as a condition).  The bottom row shows a similar experiment, but with Eastern-style wash and ink paintings.  This section highlights the SCG's superior performance, particularly in handling the complexities and noise often present in oil paintings, where ControlNet struggles to maintain fidelity and aesthetics due to the Canny edge map's inability to accurately represent texture and detail.", "section": "5 Conditional Associative Generation"}, {"figure_path": "83pV20DD2s/figures/figures_21_1.jpg", "caption": "Figure 4: Images generated by SCG in MS-COCO. The upper part shows an image randomly selected in MS-COCO with a text prompt. On the right show the condition images extracted from our modular autocoder and the corresponding generated images. The last column is a generated image by ControlNet conditioned by the canny edge. The bottom part shows more generated images. The three row images are original, condition and generated images, respectively. (See more in Figure S7 and S6)", "description": "This figure demonstrates the performance of the Self-supervised Controllable Generation (SCG) framework on the MS-COCO dataset.  The top row shows example images from MS-COCO with accompanying text prompts. The middle row displays the control images generated by the modular autoencoder, representing different features (color, brightness, edges). The bottom row shows the images generated by SCG using these control images as conditions and compares the outcome to an image generated by ControlNet which used Canny edge detection as a control condition. The figure illustrates the SCG's ability to perform zero-shot conditional image generation, by successfully reconstructing images from partial or incomplete information.", "section": "Self-supervised Controllable Generation on MS-COCO"}, {"figure_path": "83pV20DD2s/figures/figures_21_2.jpg", "caption": "Figure 1: Framework of SCG. SCG has two components. One is to promote the network to spontaneously specialize different functional modules through our designed modular equivariance constraint; The other is to perform self-supervised controllable generation through pattern completion.", "description": "This figure illustrates the framework of the Self-supervised Controllable Generation (SCG) method.  It highlights two key components:\n\n1. **Modular Equivariance Constraint:** This component focuses on promoting spontaneous specialization within the network's functional modules.  This specialization is achieved through a modular design and the application of an equivariance constraint.\n\n2. **Self-supervised Controllable Generation:** This component uses a pattern completion approach to generate controllable images. This self-supervised learning method is designed to avoid the need for labeled training data and enables the model to associate different visual attributes in a brain-like manner.", "section": "1 Introduction"}, {"figure_path": "83pV20DD2s/figures/figures_22_1.jpg", "caption": "Figure S12: Zero-shot conditional generation of SCG on more tasks. With green box are conditions and with red box are generated images.", "description": "This figure demonstrates the zero-shot generalization capabilities of the Self-supervised Controllable Generation (SCG) model on various tasks such as LineArt, Super-resolution, Dehazing and Sketch.  For each task, the top row shows the input condition (e.g., a line drawing, a low-resolution image, a hazy image, or a sketch) and the bottom row presents the corresponding generated high-quality images by SCG. The green boxes highlight the input conditions while the red boxes indicate the generated outputs. The results visually showcase SCG's ability to perform different tasks without explicit training for each.", "section": "A.4.5 More Generations"}]