[{"figure_path": "GVlJVX3iiq/figures/figures_2_1.jpg", "caption": "Figure 1: The framework of FMCSC. Initially, each client conducts cross-client consensus pre-training to alleviate model misalignment (Section 3.2). Then, all clients begin training using the designed local-synergistic contrast (Section 3.3) and upload their local models to the server. The server performs global-specific weighting aggregation and distributes multiple heterogeneous global models to all clients (Section 3.4). Finally, leveraging global models received from the server, clients discover complementary cluster structures across all clients.", "description": "This figure illustrates the FMCSC framework, which involves three main stages: 1) Cross-Client Consensus Pre-training to align local models, 2) Local-Synergistic Contrast for single-view and multi-view clients to learn consistent features, and 3) Global-Specific Weighting Aggregation to combine local models into heterogeneous global models. The final stage involves using these global models to discover cluster structures.", "section": "3 Methodology"}, {"figure_path": "GVlJVX3iiq/figures/figures_7_1.jpg", "caption": "Figure 2: ACC vs. Tm and Tp.", "description": "This figure shows the impact of temperature parameters Tm and Tp on the clustering accuracy (ACC) of the FMCSC method.  The x and y axes represent Tm and Tp respectively, ranging from 0.1 to 1.0.  Each bar represents the ACC for a given combination of Tm and Tp values. The figure demonstrates the sensitivity of the clustering performance to these hyperparameters and helps determine optimal settings for Tm and Tp. Different colors might represent different datasets or experimental conditions.", "section": "4.2 Results and Analysis"}, {"figure_path": "GVlJVX3iiq/figures/figures_8_1.jpg", "caption": "Figure 3: Visualization on model misalignment.", "description": "This figure visualizes the impact of consensus pre-training on model alignment using t-SNE.  (a) shows the feature space without consensus pre-training, demonstrating feature mixing and poor separability. (b) shows the feature space with consensus pre-training, revealing distinct and separable features indicating effective alignment.", "section": "3.3 Local-Synergistic Contrast"}, {"figure_path": "GVlJVX3iiq/figures/figures_8_2.jpg", "caption": "Figure 7: Scalability with the number of clients.", "description": "This figure demonstrates the impact of the number of clients on the clustering performance of FMCSC across different datasets (MNIST-USPS, BDGP, and NUSWIDE). Each sub-figure displays the accuracy (ACC), normalized mutual information (NMI), and adjusted rand index (ARI) against the number of clients. It shows that FMCSC maintains relatively stable performance even as the number of clients increases, indicating robustness and scalability. However, a slight decline in performance is observed for MNIST-USPS when the client number reaches 50, which is attributed to insufficient samples per client.", "section": "4.3 Attributes of Federated Learning"}, {"figure_path": "GVlJVX3iiq/figures/figures_21_1.jpg", "caption": "Figure 5: Comparison strategies.", "description": "This figure illustrates the data distribution strategies in the heterogeneous hybrid view scenario of federated multi-view clustering. The left side represents the multi-view clients, who have complete data across all views (View 1 to View V). The right side represents the single-view clients, who only have partial data. In this scenario, multi-view clients have complete data for all views, while single-view clients only have data for some views. This data distribution creates challenges in federated multi-view clustering because the clients have varying amounts of data, and the data is not uniformly distributed among the clients.", "section": "4 Experiments"}, {"figure_path": "GVlJVX3iiq/figures/figures_22_1.jpg", "caption": "Figure 6: Convergence analysis on four datasets.", "description": "This figure presents the convergence analysis of the reconstruction loss, consistency loss, and total loss for multi-view and single-view clients on four datasets: MNIST-USPS, BDGP, Multi-Fashion, and NUSWIDE.  Each subfigure shows the loss values over the number of epochs for different loss types (Cm, CP, Lm, Lp). The plots visually demonstrate the training process, showing how the losses decrease and eventually reach a stable state. This visual representation supports the stability and effectiveness of the proposed FMCSC method.", "section": "E.1 Convergence Analysis"}, {"figure_path": "GVlJVX3iiq/figures/figures_22_2.jpg", "caption": "Figure 7: Scalability with the number of clients.", "description": "This figure shows the impact of the number of clients on the clustering performance for three datasets: MNIST-USPS, BDGP, and NUSWIDE.  Each subfigure displays the accuracy (ACC), normalized mutual information (NMI), and adjusted rand index (ARI) as the number of clients increases from 2 to 50. Error bars represent the standard deviation across multiple runs. The results demonstrate that the performance of FMCSC remains generally stable even when the number of clients increases, though a slight decrease in performance is observed for MNIST-USPS when the number of clients reaches 50, which is likely due to insufficient samples per client in that scenario.", "section": "4.3 Attributes of Federated Learning"}, {"figure_path": "GVlJVX3iiq/figures/figures_22_3.jpg", "caption": "Figure 8: Sensitivity under privacy constraints when M/S = 2:1.", "description": "This figure visualizes the impact of differential privacy on the clustering performance of FMCSC across four datasets (MNIST-USPS, BDGP, Multi-Fashion, NUSWIDE).  The results show the NMI (Normalized Mutual Information) and ARI (Adjusted Rand Index) for three different privacy levels (\u03b5=10, \u03b5=50, No Privacy) when the ratio of multi-view clients to single-view clients is 2:1 (M/S = 2:1).  It demonstrates how the addition of differential privacy (with varying levels of noise) affects the clustering accuracy, highlighting the trade-off between privacy and performance.", "section": "4.2 Results and Analysis"}]