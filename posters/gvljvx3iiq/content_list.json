[{"type": "text", "text": "Bridging Gaps: Federated Multi-View Clustering in Heterogeneous Hybrid Views ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xinyue Chen1 Yazhou $\\mathbf{Ren^{1,2,*}}$ Jie $\\mathbf{X}\\mathbf{u}^{1}$ Fangfei Lin1 Xiaorong ${\\bf P}{\\bf u}^{1,2}$ Yang Yang1 ", "page_idx": 0}, {"type": "text", "text": "1School of Computer Science and Engineering, University of Electronic Science and Technology of China, China; 2Shenzhen Institute for Advanced Study, University of Electronic Science and Technology of China, China ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recently, federated multi-view clustering (FedMVC) has emerged to explore cluster structures in multi-view data distributed on multiple clients. Many existing approaches tend to assume that clients are isomorphic and all of them belong to either single-view clients or multi-view clients. While these methods have succeeded, they may encounter challenges in practical FedMVC scenarios involving heterogeneous hybrid views, where a mixture of single-view and multi-view clients exhibit varying degrees of heterogeneity. In this paper, we propose a novel FedMVC framework, which concurrently addresses two challenges associated with heterogeneous hybrid views, i.e., client gap and view gap. To address the client gap, we design a local-synergistic contrastive learning approach that helps single-view clients and multi-view clients achieve consistency for mitigating heterogeneity among all clients. To address the view gap, we develop a global-specific weighting aggregation method, which encourages global models to learn complementary features from hybrid views. The interplay between local-synergistic contrastive learning and global-specific weighting aggregation mutually enhances the exploration of the data cluster structures distributed on multiple clients. Theoretical analysis and extensive experiments demonstrate that our method can handle the heterogeneous hybrid views in FedMVC and outperforms state-of-the-art methods. The code is available at https://github.com/5Martina5/FMCSC. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent advancements in sensors and the internet have allowed many distributed clients to collect unlabeled data from multiple views/modalities[10, 17, 25]. Utilizing these unlabeled data while considering the need for data privacy among clients has given rise to an emerging field of federated multi-view clustering (FedMVC) [8], which enables multiple clients to collaboratively train consistent clustering models without exposing private data. The clustering models across distributed clients can be applied in many applications (e.g., recommendation [15] and medicine [5]) and thus FedMVC attracts increasing research interest [14, 26, 35]. ", "page_idx": 0}, {"type": "text", "text": "Existing FedMVC methods tend to assume that clients are isomorphic and all of them belong to either single-view clients or multi-view clients. For instance, FedDMVC [8] assumes that a dataset with $V$ views is distributed across $V$ single-view clients, each having the same set of samples. FedMVFPC [14] assumes that the data are distributed among multiple multi-view clients, with each client having $V$ views and non-overlapping samples among clients. Despite the achieved success, they may encounter challenges when handling some practical FedMVC scenarios involving heterogeneous hybrid views, where different clients are heterogeneous such that single-view clients and multi-view clients are hybrid. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "This scenario is prevalent in real world situations [6, 47], for example, hospitals in metropolitan areas employ CT, X-ray, and EHR for disease detection, whereas remote areas usually rely on a single detection method. Similarly, smartphones can simultaneously capture audio and images, but recording devices are limited to collecting audio data only. ", "page_idx": 1}, {"type": "text", "text": "The presence of heterogeneous hybrid views might limit the applicability of previous FedMVC methods. We decompose these challenges into two issues. (a) Client gap. Multi-view clients collect multiple views that have the opportunity to learn comprehensive cluster partitions by leveraging multi-view information. Conversely, single-view clients only own single views and easily obtain biased cluster partitions if the single view only contains marginal information. (b) View gap. Hybrid views collected by different clients have quality differences (e.g., images might contain richer visual information than texts, but texts could have semantic information), and extracting complementary information from different views across all clients is not trivial. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we introduce a novel FedMVC method, namely Federated Multi-view Clustering via Synergistic Contrast (FMCSC), which can simultaneously leverage the single-view and multi-view data across heterogeneous clients to mine clustering structures from hybrid views. Figure 1 shows an overview of our FMCSC framework. First, we note that lacking unified information supervision, naive aggregation of local models may lead to model misalignment. Therefore, we propose crossclient consensus pre-training to align the local models on all clients to avoid their misalignment. Second, to address the client gap, we design local-synergistic contrastive learning that helps to mitigate the heterogeneity between single-view clients and multi-view clients. In particular, we leverage feature-level and model-level contrastive learning to align multi-view clients and singleview clients, respectively. Third, to tackle the view gap, we develop the global-specific weighting aggregation which encourages global models to learn robust features from hybrid views, further exploring complementary cluster structures. The local-synergistic contrastive learning and globalspecific weighting aggregation promote each other to explore the data cluster structures distributed on multiple clients. Overall, FMCSC effectively facilitates all clients in bridging the client gap and view gap within the heterogeneous hybrid views through theoretical and experimental analysis. ", "page_idx": 1}, {"type": "text", "text": "Our main contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose a novel FedMVC method that can handle the heterogeneous hybrid views and explain the success mechanism of the proposed method through theoretical analysis from the perspective of bridging client and view gaps. \u2022 We design local-synergistic contrastive learning and global-specific weighting aggregation, using mutual information as a bridge to connect local and global models, together to explore the cluster structures in multi-view data distributed on different clients. \u2022 Theoretical and experimental analyses verify the effectiveness of FMCSC, which shows excellent clustering performance under various federated learning scenarios. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Federated multi-view clustering (FedMVC) has emerged to explore cluster structures in multi-view data distributed on multiple clients. Existing FedMVC methods can be classified into two categories based on the partitioning of multi-view data among clients. (1) Vertical FedMVC assumes that a dataset with $V$ views is distributed across $V$ single-view clients, each having the same set of samples. Robust federated multi-view learning (FedMVL) [16] addresses high communication costs, fault tolerance, and stragglers. Federated deep multi-view clustering (FedDMVC) [8] focuses on addressing the challenges of feature heterogeneity and incompleteness. Existing vertical FedMVC methods still rely on the idealistic assumption that different views of the same sample can be aligned across clients, which warrants further investigation. (2) Horizontal FedMVC assumes that the data are distributed among multiple multi-view clients, with each client having $V$ views and non-overlapping samples among clients. Federated multi-view fuzzy c-means consensus prototypes clustering (FedMVFPC) [14] utilizes federated learning mechanisms to perform fuzzy c-means clustering on multi-view data. Horizontal federated multi-view learning (H-FedMV) [5] aims to improve the local disease prediction performance by sharing training models among clients. Although existing approaches have been ", "page_idx": 1}, {"type": "image", "img_path": "GVlJVX3iiq/tmp/c18af0ef5fb54c714e596fefce344629d0bc2c0689c4e0bb6752b26da320684b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 1: The framework of FMCSC. Initially, each client conducts cross-client consensus pretraining to alleviate model misalignment (Section 3.2). Then, all clients begin training using the designed local-synergistic contrast (Section 3.3) and upload their local models to the server. The server performs global-specific weighting aggregation and distributes multiple heterogeneous global models to all clients (Section 3.4). Finally, leveraging global models received from the server, clients discover complementary cluster structures across all clients. ", "page_idx": 2}, {"type": "text", "text": "successful, they may encounter challenges in practical FedMVC scenarios with heterogeneous hybrid views. Specifically, heterogeneity refers to the differences among clients, where single-view and multi-view clients coexist. The hybrid views indicate the uncertainty in the number and quality of views involved in training. Our proposed FMCSC is a variant of horizontal FedMVC, and can handle such scenarios by bridging the client gap and the view gap among clients. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The key goal of FMCSC is to bridge client and view gaps. On the one hand, multi-view clients have the opportunity to learn comprehensive cluster partitions by leveraging multi-view information. Singleview clients aim to bridge the client gap between themselves and multi-view clients, thereby avoiding obtaining biased clustering partitions. On the other hand, considering the inherent discrepancies in data quality among different views, our objective is to extract complementary information from hybrid views across all clients. ", "page_idx": 2}, {"type": "text", "text": "3.1 Problem Definition ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider a heterogeneous federated learning setting where $M$ multi-view clients and $S$ single-view clients collaborate to train and mine complementary clustering structures using multiple heterogeneous global models $\\left\\{f_{g}^{1}\\left(\\cdot;\\mathbf{w}^{1}\\right),\\cdot\\cdot\\cdot,f_{g}^{V^{\\star}}\\!\\!\\left(\\cdot;\\mathbf{w}^{V}\\right),\\bar{f}_{g}\\left(\\cdot;\\mathbf{w}\\right)\\right\\}$ . Here, $f_{g}^{v}\\left(\\cdot;\\mathbf{w}^{v}\\right)$ represents the global model capable of handling the $v$ -th view type, and $f_{g}\\left(\\cdot;\\mathbf{w}\\right)$ represents the global model capable of hwahnedrlei rmeuplrtei-sveinetsw t hdea -.t hE saacmh pslien golf et-hvei w-t hc lsiienngt $p\\in[S]$ c lhieasn ti tcso lplreicvtaetde  fdraotmas teht $\\mathcal{S}_{p}\\,=\\,\\{\\mathbf{x}_{i}^{v}\\}_{i=1}^{|S_{p}|}$ $\\mathbf{x}_{i}^{v}$ $i$ $p$ $v$ It adopts a small model $f_{p}\\left(\\,\\cdot\\,;\\mathbf{w}_{p}^{v}\\right):\\mathbb{R}^{D_{v}}\\rightarrow\\mathbb{R}^{d}$ based on its local view types. The multi-view client m \u2208[M] has its local dataset Mm =  xi1 , xi2 , . . . , xiV  |i=M1m|, w here $\\left(\\mathbf{x}_{i}^{1},\\mathbf{x}_{i}^{2},\\ldots,\\mathbf{x}_{i}^{V}\\right)$ represents the -th sample of the -th multi-view client collected from different view types. It trains a small model $f_{m}\\left(\\cdot;\\mathbf{w}_{m}\\right):\\mathbb{R}^{\\sum_{v=1}^{V}D_{v}}\\,\\to\\,\\mathbb{R}^{d}$ based on its local data. For simplicity, we assume that the output features of all models have the same dimension $d$ . ", "page_idx": 2}, {"type": "text", "text": "3.2 Cross-Client Consensus Pre-training ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Multi-view datasets often contain redundancy and random noise. Current mainstream methods employ self-supervised autoencoder models, such as autoencoder (AE) [13] and variational autoencoder (VAE) [20] to learn high-level features from raw data. In FMCSC, we employ an encoder-decoder pair, denoted $E_{\\phi^{v}}^{v}(\\cdot)$ and $D_{\\theta^{v}}^{v}(\\cdot)$ with learnable parameters $\\phi^{v}$ and $\\theta^{v}$ , for each view in each client. For multi-view client $m$ , we define $\\mathbf{z}_{i}^{v}=E_{\\phi^{v}}^{v}\\left(\\mathbf{x}_{i}^{v}\\right)\\in\\mathbb{R}^{d_{v}}$ as the $d_{v}$ -dimensional latent feature of the $i$ -th sample, and the output of the autoencoder is $\\hat{\\mathbf{x}}_{i}^{v}\\,=\\,D_{\\theta^{v}}^{v}\\left(\\mathbf{z}_{i}^{v}\\right)\\,\\in\\,\\mathbb{R}^{D_{v}}$ . We calculate the reconstruction loss between the input $\\mathbf{x}_{i}^{v}$ and the output $\\hat{\\mathbf{x}}_{i}^{v}$ for all samples in this client. Additionally, the local model of this client consists of $V$ encoder-decoder pairs, which can be pre-trained by minimizing the reconstruction objective: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{r}^{m}=\\frac{1}{\\left|\\mathcal{M}_{m}\\right|}\\sum_{\\upsilon=1}^{V}\\sum_{i=1}^{\\left|\\mathcal{M}_{m}\\right|}\\left\\|\\mathbf{x}_{i}^{\\upsilon}-D_{\\theta^{\\upsilon}}^{\\upsilon}\\left(E_{\\phi^{\\upsilon}}^{\\upsilon}\\left(\\mathbf{x}_{i}^{\\upsilon}\\right)\\right)\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Similarly, for single-view client $p$ that contains a type of view data locally, it is sufficient to construct an encoder-decoder pair. Pre-training can be performed using the same objective as in Eq. (1): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{r}^{p}=\\frac{1}{\\vert\\mathcal{S}_{p}\\vert}\\sum_{i=1}^{\\vert\\mathcal{S}_{p}\\vert}\\left\\Vert\\mathbf{x}_{i}^{v}-D_{\\theta^{v}}^{v}\\left(E_{\\phi^{v}}^{v}\\left(\\mathbf{x}_{i}^{v}\\right)\\right)\\right\\Vert_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Key Observations: In federated learning, diverse local data distributions frequently lead to model drift, causing slow and unstable convergence [19, 47]. In FMCSC, single-view clients have never encountered data from other view types, thus intensifying the issue of model drift due to heterogeneous hybrid views. Moreover, the absence of uniformly labeled data across all clients allows the reconstruction objective of autoencoders to optimize from multiple different directions. In feature space, this problem manifests itself as angular deviations among features [48], and from the perspective of model aggregation, it manifests itself as model misalignment. In other words, direct aggregation of models can blur the feature distinctions captured by local models, leading to inseparability among features, as shown in Figure 3. ", "page_idx": 3}, {"type": "text", "text": "A direct strategy to alleviate model misalignment is through alignment. Based on this naive idea, we propose that the multi-view client that finishes training first should distribute its network parameters $\\left\\{E_{\\phi^{1}}^{1}(\\cdot),\\ldots,E_{\\phi^{V}}^{V}(\\cdot)\\right\\}$ and $\\left\\{D_{\\theta^{1}}^{1}(\\cdot),\\ldots,D_{\\theta^{V}}^{V}(\\cdot)\\right\\}$ to the remaining clients. Each client then performs pre-training based on this model, thereby alleviating the model misalignment caused by unsupervised training. Notably, as pre-training solely involves training the autoencoder, the construction of local models is inherently dependent on the view type. Therefore, single-view clients can still refer to the network parameters of multi-view clients. This process facilitates consensus pre-training among the clients, ultimately leading to the uploading of pre-trained model parameters to the server. The server initializes global models based on the models uploaded by clients. ", "page_idx": 3}, {"type": "text", "text": "3.3 Local-Synergistic Contrast ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "During pre-training, the features extracted through the reconstruction objective usually contain both common semantics and view-private information. The latter is often meaningless or even misleading, leading to poor clustering effectiveness. To mitigate the adverse effects of view-private information, each client also needs to design a consistent objective during training to learn common semantics. ", "page_idx": 3}, {"type": "text", "text": "For multi-view client $m$ , it possesses information from multiple views. Inspired by many previous works of MVC [40, 42, 44, 45], we employ feature contrastive learning to achieve consistency objectives. Considering the conflict between consistency and reconstruction objectives, we coders $\\overline{{\\left\\{\\left(\\mathbf{z}_{i}^{1},\\mathbf{z}_{i}^{2},\\ldots,\\mathbf{z}_{i}^{V}\\right)\\right\\}_{i=1}^{|\\mathcal{M}_{m}|}}}$ as low-level features. Moreover, we stack $V$ non-linear mappings $\\left\\{\\mathcal{H}^{1}\\left(\\mathbf{Z}^{1};\\Psi^{1}\\right),\\ldots,\\mathcal{H}^{V}\\left(\\mathbf{Z}^{V};\\Psi^{V}\\right)\\right\\}$ to obtain high-level features $\\left\\{\\left(\\mathbf{h}_{i}^{1},\\mathbf{h}_{i}^{2},\\ldots,\\mathbf{h}_{i}^{V}\\right)\\right\\}_{i=1}^{|\\mathcal{M}_{m}|}$ . Additionally, a non-linear mapping $\\mathcal{H}\\left(\\mathbf{Z};\\Psi\\right):\\mathbb{R}^{\\sum_{v=1}^{V}d_{v}}\\rightarrow\\mathbb{R}^{d}$ is constructed by: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{H}=\\mathcal{H}\\left(\\mathbf{Z};\\Psi\\right)=\\mathcal{H}\\left(\\left[\\mathbf{Z}^{1},\\mathbf{Z}^{2},...\\,,\\mathbf{Z}^{V}\\right];\\Psi\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\{\\mathbf{h}_{i}\\}_{i=1}^{|\\mathcal{M}_{m}|}=\\mathbf{H}\\in\\mathbb{R}^{|\\mathcal{M}_{m}|\\times d}$ , and $\\begin{array}{r}{\\mathbf{Z}\\in\\mathbb{R}^{|\\mathcal{M}_{m}|\\times\\sum_{v=1}^{V}d_{v}}}\\end{array}$ . We aim to preserve the representative capacity of low-level features to prevent model collapse while learning the common semantics $\\mathbf{H}$ among views in the high-level feature space. ", "page_idx": 4}, {"type": "text", "text": "In the high-level feature space, the common semantics $\\mathbf{H}$ are learned from all views and should be very similar to the common semantics learned from individual views. Based on this, we define hi, hjv v=1,..., V as V positive feature pairs, and the remaining hi, hjv jv\u0338==i1,...,V a re $V(|\\mathcal{M}_{m}|-1)$ negative feature pairs. Then, we use cosine similarity to measure the similarity of feature pairs: ", "page_idx": 4}, {"type": "equation", "text": "$$\ns i m\\left(\\mathbf{h}_{i},\\mathbf{h}_{j}^{v}\\right)=\\frac{\\left<\\mathbf{h}_{i},\\mathbf{h}_{j}^{v}\\right>}{\\left|\\left|\\mathbf{h}_{i}\\right|\\right|\\left|\\mathbf{h}_{j}^{v}\\right|\\right|},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\langle\\cdot,\\cdot\\rangle$ is dot product operator. We introduce the temperature parameter $\\tau_{m}$ to moderate the effect of similarity. Subsequently, the feature contrastive loss is formulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{c}^{m}=-\\frac{1}{|\\mathcal{M}_{m}|}\\sum_{v=1}^{V}\\sum_{i=1}^{|\\mathcal{M}_{m}|}\\log\\frac{e^{s i m(\\mathbf{h}_{i},\\mathbf{h}_{i}^{v})/\\tau_{m}}}{\\sum_{j\\neq i}e^{s i m\\left(\\mathbf{h}_{i},\\mathbf{h}_{j}^{v}\\right)/\\tau_{m}}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Moreover, multi-view clients aim to assist single-view clients in bridging client gaps and discarding view-private information detrimental to clustering. They optimize multiple heterogeneous global models using local data, ensuring that even the global model designed for single-view processing acquires generalized common semantics. Such common semantics are also advantageous for uncovering complementary clustering structures across clients. Concretely, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\{\\mathbf{w}_{m}^{v}\\}_{v=1}^{V}}\\sum_{v=1}^{V}\\|f_{m}^{v}\\left(\\cdot;\\mathbf{w}_{m}^{v}\\right)-f_{m}\\left(\\cdot;\\mathbf{w}_{m}\\right)\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $f_{m}^{v}\\left(\\cdot;\\mathbf{w}_{m}^{v}\\right)$ represents the local model that can handle the $v$ -th type of view, which is initialized by the global model $f_{g}^{v}\\,(\\,\\cdot\\,;\\mathbf{w}^{v}).\\ f_{m}\\,(\\,\\cdot\\,;\\mathbf{w}_{m}\\big)$ represents the local model of multi-view client $m$ , where multi-view clients possess data of all view types. ", "page_idx": 4}, {"type": "text", "text": "For single-view client $p$ , where each sample only has a single view, we design a model contrastive learning to achieve consistency objectives. Specifically, client $p$ contains data of the $v\\cdot$ -th view type $\\{\\mathbf{x}_{i}^{v}\\}_{i=1}^{|S_{p}|}$ , with its local model as $f_{p}\\left(\\cdot;\\mathbf{w}^{v}\\right)$ . To explore common semantics,v  wev adovpt the same approach as multi-view clients by constructing two non-linear mappings and ${\\mathcal{H}}(\\mathbf{Z};{\\bar{\\Psi}})$ to obtain high-level features ${\\bf H}^{v}$ and common semantics $\\mathbf{H}$ . The global m $\\mathrm{odel}f_{g}\\left(\\cdot;\\mathbf{w}^{v}\\right)$ after aggregation further enhances its ability to learn generalized common semantics. To encourage the local model of client $p$ to approach the more generalized global model, we formulate the model contrastive loss: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{c}^{p}=-\\frac{1}{|\\mathcal{S}_{p}|}\\sum_{i=1}^{|\\mathcal{S}_{p}|}\\log\\frac{e^{s i m\\left(\\mathbf{h}_{i},\\mathbf{h}_{i}^{g}\\right)/\\tau_{p}}}{e^{s i m\\left(\\mathbf{h}_{i},\\mathbf{h}_{i}^{g}\\right)/\\tau_{p}}+e^{s i m\\left(\\mathbf{h}_{i},\\mathbf{z}_{i}^{v}\\right)/\\tau_{p}}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\{\\mathbf{h}_{i}^{g}\\}_{i=1}^{|S_{p}|}$ represents the output of the local data after being processed by the global model, and $\\tau_{p}$ denotes the temperature parameter. The significance of Eq. (7) lies in treating $\\{\\mathbf{h}_{i},\\mathbf{h}_{i}^{g}\\}_{i=1}^{|S_{p}|}$ as positive pairs and $\\{\\mathbf h_{i},\\mathbf z_{i}^{v}\\}_{i=1}^{|S_{p}|}$ as negative pairs. This allows the local model of client $p$ to converge towards the global model while amplifying the differences between the reconstruction and consistency objectives in the local model. ", "page_idx": 4}, {"type": "text", "text": "During training, the respective total losses for multi-view client $m$ and single-view client $p$ are: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}^{m}=\\mathcal{L}_{r}^{m}+\\mathcal{L}_{c}^{m},~~~~\\mathcal{L}^{p}=\\mathcal{L}_{r}^{p}+\\mathcal{L}_{c}^{p}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In the optimization of FMCSC, $\\mathcal{L}_{r}^{m}$ and ${\\mathcal{L}}_{r}^{p}$ are utilized as reconstruction losses to learn representations for each view individually. Meanwhile, $\\mathcal{L}_{c}^{m}$ and ${\\mathcal{L}}_{c}^{p}$ are employed to discover common semantics across views, facilitating the exploration of complementary clustering structures across clients. ", "page_idx": 4}, {"type": "text", "text": "3.4 Global-Specific Weighting Aggregation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To bridge the view gap and aggregate heterogeneous models, we design a weighted specific aggregation strategy on the server, yielding multiple heterogeneous global models. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Assume $\\delta_{m},\\delta_{p}\\ \\in\\ (0,1)$ such that $p\\left(\\mathbf{h}_{i}^{v}\\mid\\mathbf{h}_{i}\\right)~>~\\delta_{m}$ , $i\\;=\\;1,2,\\cdots\\,,|{\\mathcal{M}}_{m}|\\;$ and $p\\left(\\mathbf{h}_{i}^{g}\\mid\\mathbf{h}_{i}\\right)\\;=\\;p\\left(\\mathbf{z}_{i}^{v}\\mid\\mathbf{h}_{i}\\right)\\;\\stackrel{!}{>}\\;\\delta_{p},$ , $i\\;=\\;1,2,\\cdots\\;,|S_{p}|$ hold. The following inequality establishes the relationship between the consistency objectives and the mutual information of the multi-view client m and single-view client $p$ , respectively: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{v=1}^{V}I\\left(\\mathbf{H},\\mathbf{H}^{v}\\right)\\geq V\\log\\left|\\mathcal{M}_{m}\\right|-\\delta_{m}\\mathcal{L}_{c}^{m},}\\\\ &{I\\left(\\mathbf{H},\\mathbf{H}^{g}\\right)-I\\left(\\mathbf{H},\\mathbf{Z}^{v}\\right)\\leq-2\\delta_{p}\\mathcal{L}_{c}^{p}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proofs of all the theorems in this paper are provided in Appendix $\\mathbf{C}$ due to space limit. Theorem 1, Eq. (5), and Eq. (7) indicate that minimizing contrastive loss $\\mathcal{L}_{c}^{m}$ and ${\\mathcal{L}}_{c}^{p}$ are equal to maximizing mutual information. Such connection has also been discussed in [42, 51]. ", "page_idx": 5}, {"type": "text", "text": "Through theoretical analysis, we use mutual information $\\begin{array}{r}{\\sum_{v=1}^{V}I\\left(\\mathbf{H},\\mathbf{H}^{v}\\right)}\\end{array}$ and $I\\left(\\mathbf{H},\\mathbf{H}^{g}\\right){-}I\\left(\\mathbf{H},\\mathbf{Z}^{v}\\right)$ as weights to evaluate the quality of the models from multi-view clients and single-view clients, respectively. A higher level of mutual information indicates better model quality, leading to higher weights during aggregation. ", "page_idx": 5}, {"type": "text", "text": "Considering the heterogeneity of the models, we aggregate client models with the same architecture on the server, referred to as specific aggregation. Specifically, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{f_{g}\\left(\\,\\cdot;\\mathbf{w}\\right)=\\sum_{m=1}^{M}\\alpha_{m}f_{m}\\left(\\,\\cdot;\\mathbf{w}_{m}\\right)}\\,,}}\\\\ {{\\displaystyle{f_{g}^{v}\\left(\\,\\cdot;\\mathbf{w}^{v}\\right)=\\sum_{m=1}^{M}\\alpha_{m}^{v}f_{m}^{v}\\left(\\,\\cdot;\\mathbf{w}_{m}^{v}\\right)+\\sum_{p=1}^{S}\\alpha_{p}f_{p}\\left(\\,\\cdot;\\mathbf{w}_{p}^{v}\\right)}\\,,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $v=1,2,\\cdots\\,,V$ , $\\alpha_{m}$ and $\\alpha_{p}$ represent the weights for model aggregation of multi-view client $m$ and single-view client $p$ respectively. In this scenario, there are a total of $M$ multi-view clients and $S$ single-view clients, resulting in $(V+1)$ heterogeneous global models. ", "page_idx": 5}, {"type": "text", "text": "For global models that handle multiple view types simultaneously, the expected risk is defined as ${\\mathcal{L}}_{M}(f)$ and optimized by minimizing the empirical risk $\\widehat{\\mathcal{L}}_{M}(f)$ . Similarly, for global models dealing with processing a single view type, such as the $v$ -th vie w type, the expected and empirical risks are defined as $\\mathcal{L}_{S^{v}}(f)$ and $\\widehat{\\mathcal{L}}_{S^{v}}(f)$ respectively. Inspired by previous works [23, 37] on the generalization bound of clustering ap proaches, we obtain the following theorem by analyzing the generalization bound of the proposed FMCSC method. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. Suppose that for any $\\textbf{x}\\in\\mathbf{\\Sigma}\\mathcal{X}$ and $f\\ \\in\\ {\\mathcal{F}}$ , there exists $D\\ <\\ \\infty$ such that $\\left\\|\\mathbf{x}\\right\\|,\\left\\|f_{x}\\left(\\mathbf{x}\\right)\\right\\|,\\left\\|\\bar{f}_{z}^{v}\\left(\\mathbf{x}\\right)\\right\\|,\\left\\|\\underline{{f}}_{h}^{v}\\left(\\mathbf{x}\\right)\\right\\|,\\left\\|f_{h}^{0}\\left(\\mathbf{x}\\right)\\right\\|\\in\\left[0,D\\right]$ hold. With probability $1-\\delta$ for any $f\\in\\mathcal F$ , the following inequality holds ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{M}(f)\\leq\\widehat{\\mathcal{L}}_{M}(f)+\\frac{12V D^{2}}{\\sqrt{M\\left|\\mathcal{M}_{m}\\right|}}+9V D^{2}\\sqrt{\\frac{\\log\\frac{1}{\\delta}}{2M\\left|\\mathcal{M}_{m}\\right|}},}\\\\ &{L_{S^{v}}(f)\\leq\\!\\widehat{L}_{S^{v}}(f)+\\!\\frac{10D^{2}}{\\sqrt{S^{v}\\left|\\mathcal{S}_{p}\\right|}}+8D^{2}\\sqrt{\\frac{\\log\\frac{1}{\\delta}}{2S^{v}\\left|\\mathcal{S}_{p}\\right|}}}\\\\ &{\\qquad\\qquad+\\sqrt{\\frac{4}{M\\left|\\mathcal{M}_{m}\\right|}\\left(d\\log\\frac{2e M\\left|\\mathcal{M}_{m}\\right|}{d}+\\log\\frac{4}{\\delta}\\right)}+d_{\\mathcal{F}}\\left(\\tilde{\\mathcal{D}}_{v},\\tilde{\\mathcal{D}}\\right)+\\lambda_{v},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $M$ is the number of multi-view participating clients, $S^{v}$ is the number of single-view clients with $v$ -th view type that participated in the training, $|\\mathcal{M}_{m}|$ and $\\left|\\mathcal{S}_{p}\\right|$ are the number of samples in each multi-view client and single-view client, respectively. $d_{\\mathcal{F}}\\left(\\tilde{\\mathcal{D}}_{v},\\tilde{\\mathcal{D}}\\right)$ measures the difference between data from the v-th view distribution $\\tilde{\\mathcal{D}}_{v}$ and the multi-view data distribution $\\tilde{\\mathcal{D}}$ . ", "page_idx": 5}, {"type": "text", "text": "Three key implications can be derived from Theorem 2: i) For global models capable of handling multi-view data, having more samples from multi-view clients, such as increasing the number of samples per client or adding multi-view participating clients, contributes to the improvement of generalization performance. ii) For global models dealing with single-view data, having more samples from both multi-view and single-view clients enhances their generalization performance, which is a reflection of multi-view clients helping single-view clients to bridge the client gap. iii) Although the view gap is mitigated, the high dissimilarity among views still leads to high distribution divergence $d_{\\mathcal{F}}\\left(\\tilde{\\mathcal{D}}_{v},\\tilde{\\mathcal{D}}\\right)$ , which impairs the quality of the global model. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Finally, each client applies the $K$ -means [30] on the common semantics $\\mathbf{H}$ obtained from the corresponding global model to calculate the cluster centroids and obtain their local clustering results. For example, for multi-view client $m$ , letting $\\left\\{\\mathbf{c}_{j}\\right\\}_{j=1}^{K}$ denote the $K$ cluster centroids, we have: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{{\\bf c}_{1},{\\bf c}_{2},\\ldots,{\\bf c}_{K}}\\,\\sum_{i=1}^{|\\mathcal{M}_{m}|}\\sum_{j=1}^{K}\\left\\|{\\bf h}_{i}-{\\bf c}_{j}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The clustering result for the $i$ -th sample is $\\begin{array}{r}{y_{i}=\\arg\\operatorname*{min}_{j}\\left\\|\\mathbf{h}_{i}-\\mathbf{c}_{j}\\right\\|^{2}}\\end{array}$ . By concatenating clustering results from all clients, we obtain the overall clustering results. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Experimental Settings ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets. Our experiments are carried out on four multi-view datasets. Specifically, MNIST-USPS [34] comprises 5000 samples collected from two handwritten digital image datasets, which are considered as two views. BDGP [4] consists of 2500 samples across 5 drosophila categories, with each sample having textual and visual views. Multi-Fashion [41] contains images from 10 categories, where we treat three different styles of one object as three views, resulting in 10000 samples. NUSWIDE [9] consists of 5000 samples obtained from web images with 5 views. Considering the sample quantities, we allocate BDGP to 12 clients, MNIST-USPS and NUSWIDE to 24 clients respectively, and Multi-Fashion to 48 clients to simulate the federated learning settings. ", "page_idx": 6}, {"type": "text", "text": "Comparison Methods. We select 9 state-of-the-art methods, including HCP-IMSC [24], IMVC-CBG [39], DSIMVC [37], LSIMVC [27], ProImp [22], JPLTD [29], CPSPAN [18], FedDMVC [8] and FCUIF [36]. Among them, apart from FedDMVC and FCUIF, which are FedMVC methods, all the other comparison methods are centralized incomplete multi-view clustering methods. To ensure fair comparisons, we concatenate the data distributed among the clients and use them as the input for centralized methods. Among these, the data from multi-view clients can be regarded as complete data, while the data from single-view clients can be considered as missing data. ", "page_idx": 6}, {"type": "text", "text": "Implementation Details. For an encoder-decoder pair, the encoder structure is Input- $\\mathrm{Fc_{500}\\mathrm{~-~}}$ $\\mathrm{Fc_{500}-F c_{2000}-F c_{200}}.$ , and the decoder is symmetric with the encoder. Also, we set temperature parameters $\\tau_{m}=\\tau_{p}=0.5$ and use the batch size of 256. The output dimension $d$ is set to 20 for all local and global models and communication rounds $R$ is set to 5. All experiments in the paper involving FMCSC that are not mentioned are performed when $M/S=1{:}1$ . ", "page_idx": 6}, {"type": "text", "text": "4.2 Results and Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Clustering Results. Table 1 presents a quantitative comparison under various heterogeneous hybrid view scenarios. Each experiment is independently conducted five times, reporting average values and standard deviations. We construct different scenarios by adjusting the ratio of multi-view clients to single-view clients. Remarkably, FMCSC achieves exceptional performance in various heterogeneous hybrid view scenarios, surpassing recent methods. This indicates our ability to achieve satisfactory clustering performance while preserving data privacy. Furthermore, with the increasing proportion of single-view clients, all methods exhibit varying degrees of performance decline, aligning with common expectations. Even when the number of single-view clients is twice that of multi-view clients, FMCSC still achieves superior clustering performance, which is encouraging. ", "page_idx": 6}, {"type": "text", "text": "Ablation Studies. Components A and D represent the consensus pre-training process and weighted aggregation, respectively. Component B indicates that multi-view clients bring the global models closer as Eq. (6). Component C indicates that model comparison in single-view clients as Eq. (7). Table 2 shows that the impact of component A on clustering performance is dependent on the dataset. Additionally, note that the results in Item-1 are obtained after running four times the number of communication rounds compared to FMCSC. Although the initial misalignment of the model on MNIST-USPS and Multi-Fashion can be mitigated through multiple communication rounds. Component A still plays a crucial role in our training process, facilitating consensus among clients during pre-training to alleviate model misalignment and accelerate convergence effectively. Item-3, which lacks component C, involves both single-view and multi-view clients carrying out the same operation of replacing their local models with global models. We observe a substantial improvement in Item-3 compared to Item-2, indicating that the success of model comparison in component C is attributed to high-quality global models. This achievement requires collaborative efforts from both multi-view and single-view clients. Additionally, the inclusion of the weighted aggregation in component D enhances the benefits of collaborative training among all clients. ", "page_idx": 6}, {"type": "table", "img_path": "GVlJVX3iiq/tmp/f82b533f0e3b6f5c4adb723fdf2e38117c05b44ac144fcaa3e6baaebd81379ed.jpg", "table_caption": ["Table 1: Clustering results (mean\u00b1std $\\%$ ) of all methods on four datasets. The best and second best results are denoted in bold and underline. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "GVlJVX3iiq/tmp/15b81ff090f5aa90b5da4549326502100db55d0b4453380367d6e5dc56da467f.jpg", "table_caption": ["Table 2: Ablation studies on four datasets when M/ $S{=}1{:}1$ . "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "GVlJVX3iiq/tmp/21a76e3a9be04dda2ad1afa403666bd741fcea9ad01da2b05d417db1c98bb82a.jpg", "img_caption": ["Figure 2: ACC vs. $\\tau_{m}$ and $\\tau_{p}$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Parameter Analysis. We investigate the sensitivity of our clustering performance on MNIST-USPS dataset to two primary hyperparameters in local-synergistic contrastive learning: $\\tau_{m}$ and $\\tau_{p}$ , as shown in Figure 2. The values of $\\tau_{m}$ and $\\tau_{p}$ are tuned within the range of 0.1, 0.3, 0.5, 0.7, 1. Our observations include: i) When $\\tau_{m}$ and $\\tau_{p}$ are set to small values, such as 0.1, the clustering performance of the proposed FMCSC decreases. This may be attributed to an excessive emphasis on view consistency, potentially resulting in an inseparable intrinsic feature space. ii) As the values of $\\tau_{m}$ and $\\tau_{p}$ increase, the clustering results gradually recover, and they exhibit insensitivity within the range of 0.3 to 1. Empirically, we set $\\tau_{m}=\\tau_{p}=0.5$ for all datasets. ", "page_idx": 8}, {"type": "text", "text": "Qualitative Study on Model Misalignment. To further quantify the impact of model misalignment and the effectiveness of our proposed strategy, we visualize the model outputs, i.e., the consensus semantics $\\mathbf{H}$ , both without consensus pre-training and with consensus pre-training. ", "page_idx": 8}, {"type": "text", "text": "Figure 3 displays the t-SNE [38] visualizations generated from a randomly selected multi-view client, where different colors represent different classes. In Figure 3 (a), we observe that the output of the global model without consensus pretraining shows mixed features that cannot be clearly distinguished. This confirms our viewpoint that direct parameter aggregation leads to model misalignment, manifested as feature confusion in the feature space. In contrast, Figure 3 (b) demonstrates that FMCSC produces more distinct and separable features, mitigating the negative impact of model aggregation. ", "page_idx": 8}, {"type": "image", "img_path": "GVlJVX3iiq/tmp/27a7216e17d144f54e9ef644267872f7eefefda6ad3838c8f67569eba34fc189.jpg", "img_caption": ["Figure 3: Visualization on model misalignment. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "GVlJVX3iiq/tmp/960ffd30d991553b8dc486ed8784dd6fa3d4d783a2b5b44a95a28823b677f033.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 4: (a) Effect of samples per client on generalization performance. (b) Scalability with the number of clients on Multi-Fashion. (c) Sensitivity under privacy constraints when $M/S=2{\\cdot}1$ . ", "page_idx": 8}, {"type": "text", "text": "4.3 Attributes of Federated Learning ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Generalization Analysis. To check the validity of our theory for the proposed method, we investigate the impact of the number of samples per client and client participation rate on the generalization performance in the clustering task, as shown in Figure 4 (a) and Table 3. In this setting, the total ", "page_idx": 8}, {"type": "text", "text": "number of clients is fixed. We find that: i) Increasing the number of samples per client effectively raises the total number of samples involved in training, which enhances the model\u2019s generalization performance. ii) As the proportion of participat", "page_idx": 8}, {"type": "table", "img_path": "GVlJVX3iiq/tmp/f8ae281901132344a43fde237b3f68d3d7f65a3b0ffa1d813c88d3ad5ee92f3e.jpg", "table_caption": ["Table 3: Effect of participation rates on generalization performance. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "ing clients increases, the accuracy of non-participating clients also improves, and the performance gap between participating and non-participating clients narrows. These observations are consistent with the theoretical understanding in Theorem 2, i.e., the generalization performance of the model is enhanced as both the number of samples per client and the number of participating clients increase. ", "page_idx": 8}, {"type": "text", "text": "Number of Clients. We next consider the effects of changing the number of clients as shown in Figure 4 (b). It is observed that as the number of clients increases, the performance of FMCSC experiences a slight decline but remains generally stable. Only when the client number reaches 100, does a noticeable decline in performance occur, which is attributed to the insufficient number of samples within each client. ", "page_idx": 9}, {"type": "text", "text": "Privacy. FMCSC, by design, does not share any raw data between clients and the server. Only the model parameters on each client are shared with the server. To further protect client privacy, we adopt differential privacy [1] by adding noise to the model parameters uploaded from the client to the server. Figure 4 (c) illustrates the clustering accuracy of FMCSC under different privacy bounds $\\varepsilon$ . We observe that FMCSC achieves both high performance and privacy at $\\varepsilon=50$ . However, as the level of noise increases at $\\varepsilon=10$ , the performance of FMCSC unavoidably degrades. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose FMCSC that can handle practical scenarios with heterogeneous hybrid views and explore the data cluster structures distributed on multiple clients. First, we propose cross-client consensus pre-training to align the local models on all clients to avoid their misalignment. Then, local-synergistic contrast and global-specific weighting aggregation are designed to bridge the client gap and the view gap across distributed clients and explore the cluster structures in multi-view data distributed on different clients. Theoretical analysis and extensive experiments demonstrate that FMCSC outperforms state-of-the-art methods across diverse heterogeneous hybrid views and various federated learning scenarios. In future work, we will use the method for more downstream tasks and some real-world situations, such as medical analysis and financial forecasting. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported in part by National Natural Science Foundation of China (No. 62476052), Sichuan Science and Technology Program (No. 2024NSFSC1473), and Shenzhen Science and Technology Program (No. JCYJ20230807115959041). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In ACM CCS, pages 308\u2013318, 2016.   \n[2] Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Machine learning, 79:151\u2013175, 2010.   \n[3] Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for domain adaptation. In NeurIPS, pages 1\u20138, 2006.   \n[4] Xiao Cai, Hua Wang, Heng Huang, and Chris Ding. Joint stage recognition and anatomical annotation of drosophila gene expression patterns. Bioinformatics, 28(12):i16\u2013i24, 2012.   \n[5] Sicong Che, Zhaoming Kong, Hao Peng, Lichao Sun, Alex Leow, Yong Chen, and Lifang He. Federated multi-view learning for private medical data integration and analysis. TIST, 13(4):1\u201323, 2022.   \n[6] Jiayi Chen and Aidong Zhang. Fedmsplit: Correlation-adaptive federated multi-task learning across multimodal split networks. In KDD, pages 87\u201396, 2022.   \n[7] Man-Sheng Chen, Ling Huang, Chang-Dong Wang, and Dong Huang. Multi-view clustering in latent embedding space. In AAAI, pages 3513\u20133520, 2020.   \n[8] Xinyue Chen, Jie Xu, Yazhou Ren, Xiaorong Pu, Ce Zhu, Xiaofeng Zhu, Zhifeng Hao, and Lifang He. Federated deep multi-view clustering with global self-supervision. In ACM MM, pages 3498\u20133506, 2023.   \n[9] Tat-Seng Chua, Jinhui Tang, Richang Hong, Haojie Li, Zhiping Luo, and Yantao Zheng. Nuswide: a real-world web image database from national university of singapore. In ACM CIVR, pages 1\u20139, 2009.   \n[10] Chenhang Cui, Yazhou Ren, Jingyu Pu, Jiawei Li, Xiaorong Pu, Tianyi Wu, Yutao Shi, and Lifang He. A novel approach for effective multi-view clustering with information-theoretic perspective. In NeurIPS, pages 1\u201313, 2023.   \n[11] Jonas Geiping, Hartmut Bauermeister, Hannah Dr\u00f6ge, and Michael Moeller. Inverting gradientshow easy is it to break privacy in federated learning? In NeurIPS, pages 16937\u201316947, 2020.   \n[12] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In AISTATS, pages 315\u2013323, 2011.   \n[13] Geoffrey E Hinton and Richard Zemel. Autoencoders, minimum description length and helmholtz free energy. NeurIPS, 6, 1993.   \n[14] Xingchen Hu, Jindong Qin, Yinghua Shen, Witold Pedrycz, Xinwang Liu, and Jiyuan Liu. An efficient federated multi-view fuzzy c-means clustering method. TFS, 32(4):1886\u20131899, 2023.   \n[15] Mingkai Huang, Hao Li, Bing Bai, Chang Wang, Kun Bai, and Fei Wang. A federated multi-view deep learning framework for privacy-preserving recommendations. arXiv preprint arXiv:2008.10808, 2020.   \n[16] Shudong Huang, Wei Shi, Zenglin Xu, Ivor W Tsang, and Jiancheng Lv. Efficient federated multi-view learning. Pattern Recognition, 131:108817, 2022.   \n[17] Weitian Huang, Sirui Yang, and Hongmin Cai. Generalized information-theoretic multi-view clustering. In NeurIPS, pages 1\u201313, 2023.   \n[18] Jiaqi Jin, Siwei Wang, Zhibin Dong, Xinwang Liu, and En Zhu. Deep incomplete multiview clustering with cross-view partial sample and prototype alignment. In CVPR, pages 11600\u201311609, 2023.   \n[19] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In ICML, pages 5132\u20135143, 2020.   \n[20] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.   \n[21] Rafa\u0142 Lata\u0142a and Krzysztof Oleszkiewicz. On the best constant in the khinchin-kahane inequality. Studia Mathematica, 109(1):101\u2013104, 1994.   \n[22] Haobin Li, Yunfan Li, Mouxing Yang, Peng Hu, Dezhong Peng, and Xi Peng. Incomplete multi-view clustering via prototype-based imputation. In IJCAI, pages 3911\u20133919, 2023.   \n[23] Shaojie Li and Yong Liu. Sharper generalization bounds for clustering. In ICML, pages 6392\u20136402, 2021.   \n[24] Zhenglai Li, Chang Tang, Xiao Zheng, Xinwang Liu, Wei Zhang, and En Zhu. High-order correlation preserved incomplete multi-view subspace clustering. TIP, 31:2067\u20132080, 2022.   \n[25] Paul Pu Liang, Zihao Deng, Martin Q Ma, James Y Zou, Louis-Philippe Morency, and Ruslan Salakhutdinov. Factorized contrastive learning: Going beyond multi-view redundancy. In NeurIPS, pages 1\u201328, 2023.   \n[26] Yi-Ming Lin, Yuan Gao, Mao-Guo Gong, Si-Jia Zhang, Yuan-Qiao Zhang, and Zhi-Yuan Li. Federated learning on multimodal data: A comprehensive survey. MIR, pages 1\u201315, 2023.   \n[27] Chengliang Liu, Zhihao Wu, Jie Wen, Yong Xu, and Chao Huang. Localized sparse incomplete multi-view clustering. TMM, 25:5539\u20135551, 2022.   \n[28] Xinwang Liu, Miaomiao Li, Chang Tang, Jingyuan Xia, Jian Xiong, Li Liu, Marius Kloft, and En Zhu. Efficient and effective regularized incomplete multi-view clustering. TPAMI, 43(8):2634\u20132646, 2020.   \n[29] Wei Lv, Chao Zhang, Huaxiong Li, Xiuyi Jia, and Chunlin Chen. Joint projection learning and tensor decomposition-based incomplete multiview clustering. TNNLS, pages 1\u201312, 2023.   \n[30] J MacQueen. Classification and analysis of multivariate observations. In BSMSP, pages 281\u2013297, 1967.   \n[31] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT press, 2018.   \n[32] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.   \n[33] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019.   \n[34] Xi Peng, Zhenyu Huang, Jiancheng Lv, Hongyuan Zhu, and Joey Tianyi Zhou. Comic: Multiview clustering without parameter selection. In ICML, pages 5092\u20135101, 2019.   \n[35] Dong Qiao, Chris Ding, and Jicong Fan. Federated spectral clustering via secure similarity reconstruction. In NeurIPS, pages 1\u201336, 2023.   \n[36] Yazhou Ren, Xinyue Chen, Jie Xu, Jingyu Pu, Yonghao Huang, Xiaorong Pu, Ce Zhu, Xiaofeng Zhu, Zhifeng Hao, and Lifang He. A novel federated multi-view clustering method for unaligned and incomplete data fusion. Information Fusion, 108:1\u201310, 2024.   \n[37] Huayi Tang and Yong Liu. Deep safe incomplete multi-view clustering: Theorem and algorithm. In ICML, pages 21090\u201321110, 2022.   \n[38] Van, Laurens der Maaten, and Geoffrey Hinton. Visualizing data using t-sne. JMLR, 9(11), 2008.   \n[39] Siwei Wang, Xinwang Liu, Li Liu, Wenxuan Tu, Xinzhong Zhu, Jiyuan Liu, Sihang Zhou, and En Zhu. Highly-efficient incomplete large-scale multi-view clustering with consensus bipartite graph. In CVPR, pages 9776\u20139785, 2022.   \n[40] Song Wu, Yan Zheng, Yazhou Ren, Jing He, Xiaorong Pu, Shudong Huang, Zhifeng Hao, and Lifang He. Self-weighted contrastive fusion for deep multi-view clustering. TMM, 2024.   \n[41] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.   \n[42] Jie Xu, Shuo Chen, Yazhou Ren, Xiaoshuang Shi, Heng Tao Shen, Gang Niu, and Xiaofeng Zhu. Self-weighted contrastive learning among multiple views for mitigating representation degeneration. In NeurIPS, 2023.   \n[43] Jie Xu, Yazhou Ren, Xiaolong Wang, Lei Feng, Zheng Zhang, Gang Niu, and Xiaofeng Zhu. Investigating and mitigating the side effects of noisy views for self-supervised clustering algorithms in practical multi-view scenarios. In CVPR, pages 22957\u201322966, 2024.   \n[44] Jie Xu, Huayi Tang, Yazhou Ren, Liang Peng, Xiaofeng Zhu, and Lifang He. Multi-level feature learning for contrastive multi-view clustering. In CVPR, pages 16051\u201316060, 2022.   \n[45] Weiqing Yan, Yuanyang Zhang, Chenlei Lv, Chang Tang, Guanghui Yue, Liang Liao, and Weisi Lin. Gcfagg: Global and cross-view feature aggregation for multi-view clustering. In CVPR, pages 19863\u201319872, 2023.   \n[46] Mouxing Yang, Yunfan Li, Peng Hu, Jinfeng Bai, Jiancheng Lv, and Xi Peng. Robust multi-view clustering with incomplete information. TPAMI, 45(1):1055\u20131069, 2022.   \n[47] Qiying Yu, Yang Liu, Yimu Wang, Ke Xu, and Jingjing Liu. Multimodal federated learning via contrastive representation ensemble. In ICLR, 2023.   \n[48] Fengda Zhang, Kun Kuang, Long Chen, Zhaoyang You, Tao Shen, Jun Xiao, Yin Zhang, Chao Wu, Fei Wu, Yueting Zhuang, et al. Federated unsupervised representation learning. FITEE, 24(8):1181\u20131193, 2023.   \n[49] Zheng Zhang, Li Liu, Fumin Shen, Heng Tao Shen, and Ling Shao. Binary multi-view clustering. TPAMI, 41(7):1774\u20131782, 2018.   \n[50] Handong Zhao, Hongfu Liu, and Yun Fu. Incomplete multi-modal visual data grouping. In IJCAI, pages 2392\u20132398, 2016.   \n[51] Huasong Zhong, Chong Chen, Zhongming Jin, and Xian-Sheng Hua. Deep robust clustering by contrastive learning. arXiv preprint arXiv:2008.03030, 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "We provide more details and results about our work in the appendices. Here are the contents: ", "page_idx": 13}, {"type": "text", "text": "\u2022 Appendix A: Framework of the proposed algorithm.   \n\u2022 Appendix B: More related work.   \n\u2022 Appendix C: Proofs of Theorem 1 and Theorem 2.   \n\u2022 Appendix D: More details about experimental settings.   \n\u2022 Appendix E: Additional experiment results.   \n\u2022 Appendix F: Broader impacts of our proposed method.   \n\u2022 Appendix G: Limitations of our proposed method. ", "page_idx": 13}, {"type": "text", "text": "A Framework of the Proposed Algorithm ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Algorithm 1 outlines the execution flow for both clients and the server in FMCSC. Initially, crossclient consensus pre-training aligns the unsupervised local models of each client (Lines 1-2). During the training, feature contrastive learning is employed for multi-view clients to explore common semantics among different views (Lines 5-12). For single-view clients, model contrastive learning is designed between local models and global models, promoting the extraction of more generalized common semantics from client models (Lines 13-19). Subsequently, the server develops globalspecific weighting aggregation to aggregate multiple high-quality models (Lines 21-23). Finally, complementary cluster structures are discovered using the global models across all clients (Line 25). ", "page_idx": 13}, {"type": "text", "text": "Input: Data with $V$ views distributed among $M$ multi-view clients and $S$ single-view clients,   \ncommunication rounds $R$ , Local epoch $E$ .   \nOutput: Overall clustering results.   \n1: Pre-train all client models by Eqs. (1)-(2).   \n2: server receives pre-trained models from all clients and initializes the global models.   \n3: while not reaching $R$ rounds do   \n4: for $c=1$ to $(M+S)$ do in parallel   \n5: if $c\\in[M]$ then \u25b7Multi-view clients   \n6: while not reach the maximum iterations $E$ do   \n7: Learn common semantics by Eqs. (4)-(5).   \n8: Optimize the total loss by Eq. (8).   \n9: end while   \n10: Optimize multiple global models by Eq. (6).   \n11: Upload $\\{f_{m}^{v}\\,(\\cdot;\\mathbf{w}_{m}^{v})\\}_{v=1}^{V}$ and $f_{m}\\left(\\cdot;\\mathbf{w}_{m}\\right)$ to the server.   \n12: else if $c\\in[S]$ then \u25b7Single-view clients   \n13: while not reach the maximum iterations $E$ do   \n14: Learn common semantics by Eq. (7).   \n15: Optimize the total loss by Eq. (8).   \n16: end while   \n17: Upload $f_{p}\\left(\\cdot;\\mathbf{w}_{p}^{v}\\right)$ to the server.   \n18: end if   \n19: end for \u25b7Server   \n20: Assigning weights to local models by Eq. (9).   \n21: Aggregate models among all clients by Eq. (10).   \n22: Distribute multiple global models to each client.   \n23: end while   \n24: Calculate the clustering results by Eq. (12). ", "page_idx": 13}, {"type": "text", "text": "B More Related Work ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Multi-view clustering (MVC) methods leverage consistency and complementary information between multiple views to enhance clustering effectiveness. Based on their ability to handle missing data, existing MVC methods can be classified into two categories. Complete multi-view clustering [7, 43, 49] which uncovers hidden patterns and structures by leveraging complete multi-view data for clustering. The success of existing complete multi-view clustering relies on the assumption of sample integrity across multiple views. However, in real-world scenarios, samples of multi-view are partially available due to data corruption or sensor failure, which leads to incomplete multi-view clustering studies [28, 46, 50]. Incomplete multi-view clustering via prototype-based imputation (ProImp) [22] employs a dual attention layer and a dual contrastive learning loss to learn view-specific prototypes and recover missing data. Cross-view partial sample and prototype alignment network (CPSPAN) [18] employs complete data to guide sample reconstruction and proposes shifted prototype alignment to calibrate prototype sets across views. However, the aforementioned MVC methods assume that multi-view data are stored within a single entity, thus lacking the concept of heterogeneous clients, and only addressing the hybrid views scenarios we mentioned. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "C Theoretical Analysis ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this part, we want to prove that minimizing contrastive loss $\\mathcal{L}_{c}^{m}$ and ${\\mathcal{L}}_{c}^{p}$ are equal to maximizing mutual information. The proof is motivated by [32, 51]. ", "page_idx": 14}, {"type": "text", "text": "Proof. $\\mathcal{L}_{c}^{m}$ and ${\\mathcal{L}}_{c}^{p}$ as the contrastive loss, denoting the consistency objectives of the multi-view client $m$ and single-view client $p$ respectively, i.e., ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{L}_{c}^{m}=-\\frac{1}{|\\mathcal{M}_{m}|}\\sum_{v=1}^{V}\\sum_{i=1}^{|\\mathcal{M}_{m}|}\\log\\frac{e^{s i m(\\mathbf{h}_{i},\\mathbf{h}_{i}^{v})/\\tau_{m}}}{\\sum_{j\\neq i}e^{s i m\\left(\\mathbf{h}_{i},\\mathbf{h}_{j}^{v}\\right)/\\tau_{m}}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{L}_{c}^{p}=-\\frac{1}{|S_{p}|}\\sum_{i=1}^{|S_{p}|}\\log\\frac{e^{s i m\\left(\\mathbf{h}_{i},\\mathbf{h}_{i}^{g}\\right)/\\tau_{p}}}{e^{s i m\\left(\\mathbf{h}_{i},\\mathbf{h}_{i}^{g}\\right)/\\tau_{p}}+e^{s i m\\left(\\mathbf{h}_{i},\\mathbf{z}_{i}^{v}\\right)/\\tau_{p}}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "When $j\\neq i$ , we assume that $p\\left(\\mathbf{h}_{i},\\mathbf{h}_{j}^{v}\\right)=p\\left(\\mathbf{h}_{i}\\right)p\\left(\\mathbf{h}_{j}^{v}\\right)$ , which means that $\\begin{array}{r}{\\frac{p\\left(\\mathbf{h}_{i},\\mathbf{h}_{j}^{v}\\right)}{p\\left(\\mathbf{h}_{i}\\right)p\\left(\\mathbf{h}_{j}^{v}\\right)}\\,=\\,1}\\end{array}$ . Let $\\begin{array}{r}{\\mathcal{N}_{i}=\\sum_{j=1}^{|\\mathcal{M}_{m}|}\\frac{p\\left(\\mathbf{h}_{i},\\mathbf{h}_{j}^{v}\\right)}{p\\left(\\mathbf{h}_{i}\\right)p\\left(\\mathbf{h}_{j}^{v}\\right)}}\\end{array}$ , we can then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{{\\cal I}\\left({{\\bf H}};{{\\bf H}^{v}}\\right)=\\displaystyle\\sum_{i=1}^{\\left|{\\cal M}_{m}\\right|\\,\\left|{\\cal M}_{m}\\right|}p\\left({{\\bf h}}_{i},{{\\bf h}_{j}^{v}}\\right)\\log\\frac{p\\left({{\\bf h}}_{i},{{\\bf h}_{j}^{v}}\\right)}{p\\left({{\\bf h}}_{i}\\right)p\\left({{\\bf h}_{j}^{v}}\\right)}}\\\\ {=}&{\\displaystyle\\sum_{i=1}^{\\left|{\\cal M}_{m}\\right|}p\\left({{\\bf h}}_{i},{{\\bf h}_{i}^{v}}\\right)\\log\\frac{p\\left({{\\bf h}}_{i},{{\\bf h}_{i}^{v}}\\right)}{p\\left({{\\bf h}}_{i}\\right)p\\left({{\\bf h}_{i}^{v}}\\right)}+\\sum_{i=1}^{\\left|{\\cal M}_{m}\\right|}\\sum_{j\\ne i}p\\left({{\\bf h}}_{i},{{\\bf h}_{j}^{v}}\\right)\\log\\frac{p\\left({{\\bf h}}_{i},{{\\bf h}_{j}^{v}}\\right)}{p\\left({{\\bf h}_{i}}\\right)p\\left({{\\bf h}_{j}^{v}}\\right)}}\\\\ {=}&{\\displaystyle\\sum_{i=1}^{\\left|{\\cal M}_{m}\\right|}p\\left({{\\bf h}}_{i},{{\\bf h}_{i}^{v}}\\right)\\log\\left(\\frac{p\\left({{\\bf h}}_{i},{{\\bf h}_{i}^{v}}\\right)}{p\\left({{\\bf h}}_{i}\\right)p\\left({{\\bf h}_{i}^{v}}\\right)}\\cdot{{\\cal N}_{i}^{\\prime}}\\right)}\\\\ &{\\displaystyle=\\sum_{i=1}^{\\left|{\\cal M}_{m}\\right|}p\\left({{\\bf h}}_{i},{{\\bf h}_{i}^{v}}\\right)\\log\\frac{p\\left({{\\bf h}}_{i},{{\\bf h}_{i}^{v}}\\right)}{p\\left({{\\bf h}}_{i}\\right)p\\left({{\\bf h}_{i}^{v}}\\right)}+\\sum_{i=1}^{\\left|{\\cal M}_{m}\\right|}p\\left({{\\bf h}}_{i},{{\\bf h}_{i}^{v}}\\right)\\log{{\\cal N}_{i}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since positive pairs are correlated, we have the estimate: $p\\left(\\mathbf{h}_{i},\\mathbf{h}_{i}^{v}\\right)\\geq p\\left(\\mathbf{h}_{i}\\right)p\\left(\\mathbf{h}_{i}^{v}\\right)$ . In addition, we assume that there exists a constant $\\delta_{m}\\in(0,1)$ such that $p\\left(\\mathbf{h}_{i}^{v}\\mid\\mathbf{h}_{i}\\right)>\\delta_{m}$ , $i=1,2,\\cdots\\,,|\\mathcal{M}_{m}|$ holds. According to [42] and with the estimation, we have $\\begin{array}{r}{p\\left(\\mathbf{h}_{i}\\right)\\approx\\frac{1}{\\left|\\mathcal{M}_{m}\\right|},i=1,2,\\cdots\\,,\\left|\\mathcal{M}_{m}\\right|}\\end{array}$ , and $\\begin{array}{r}{e^{s i m\\left(\\mathbf{h}_{i},\\mathbf{h}_{j}^{v}\\right)/\\tau_{m}}\\propto\\frac{p\\left(\\mathbf{h}_{i},\\mathbf{h}_{j}^{v}\\right)}{p\\left(\\mathbf{h}_{i}\\right)p\\left(\\mathbf{h}_{j}^{v}\\right)}}\\end{array}$ , the following inequality holds: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{=1}^{V}{I\\left({\\bf{H}},{\\bf{H}}^{v}\\right)}=\\sum_{v=1}^{V}{\\sum_{i=1}^{V}{p\\left({\\bf{h}}_{i},{\\bf{h}}_{i}^{v}\\right)\\log\\frac{\\bar{p}({\\bf{h}}_{i},{\\bf{h}}_{i}^{v})}{{\\mathcal{N}}_{i}}}+\\sum_{v=1}^{V}{\\sum_{i=1}^{V}{p\\left({\\bf{h}}_{i},{\\bf{h}}_{i}^{v}\\right)\\log\\left(\\sqrt{\\displaystyle\\sum_{j=1}^{{N}\\alpha_{i}}{\\frac{p\\left({\\bf{h}}_{i},{\\bf{h}}_{j}^{v}\\right)}{p\\left({\\bf{h}}_{i}\\right)p\\left({\\bf{h}}_{j}^{v}\\right)}}\\right)}}}}\\\\ {\\displaystyle\\approx\\sum_{v=1}^{V}{\\sum_{i=1}^{M_{m}}{\\frac{1}{{\\left|\\mathcal{M}_{m}\\right|}^{p}\\left({{\\bf{h}}_{i}^{v}\\mid{\\bf{h}}_{i}}\\right)\\log\\frac{\\bar{p}({\\bf{h}}_{i},{\\bf{h}}_{i}^{v})}{{\\mathcal{N}}_{i}}}}+\\sum_{v=1}^{V}\\log\\left({\\left|\\mathcal{M}_{m}\\right|-1+\\frac{p\\left({\\bf{h}}_{i},{\\bf{h}}_{i}^{v}\\right)}{p\\left({\\bf{h}}_{i}\\right)p\\left({\\bf{h}}_{i}^{v}\\right)}}\\right)}}\\\\ {\\displaystyle\\geq\\frac{\\delta_{m}}{\\left|\\mathcal{M}_{m}\\right|}\\sum_{v=1}^{V}{\\sum_{i=1}^{|\\mathcal{M}_{m}|}{\\log\\frac{e^{s i m{\\left({\\bf{h}}_{i},{\\bf{h}}_{i}^{v}\\right)/\\tau_{m}}}}{{\\sum_{j\\neq i}e^{s i m{\\left({\\bf{h}}_{i},{\\bf{h}}_{j}^{v}\\right)/\\tau_{m}}}+e^{s i m{\\left({\\bf{h}}_{i},{\\bf{h}}_{i}^{v}\\right)/\\tau_{m}}}}}}+V\\log\\left|\\mathcal{M}_{m}\\right|}}\\\\ {\\displaystyle}&{\\displaystyle\\geq V\\log\\left|\\mathcal{M}_{m}\\right|-\\delta_{m}\\\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Similarly, we assume that there exists a constant $\\delta_{p}\\in(0,1)$ such that $p\\left(\\mathbf{h}_{i}^{g}\\mid\\mathbf{h}_{i}\\right)=p\\left(\\mathbf{z}_{i}^{v}\\mid\\mathbf{h}_{i}\\right)>\\delta_{p},$ , $\\textit{i}=\\,1,2,\\cdots\\,,|S_{p}|$ holds. And we have $\\begin{array}{r l r}{p\\left(\\mathbf{h}_{i}\\right)\\!}&{{}\\approx}&{\\!\\frac{1}{\\left|S_{p}\\right|}}\\end{array}$ , $\\begin{array}{r l r}{e^{s i m\\left(\\mathbf{h}_{i},\\mathbf{h}_{i}^{g}\\right)/\\tau_{p}}}&{\\propto}&{\\frac{p\\left(\\mathbf{h}_{i},\\mathbf{h}_{i}^{g}\\right)}{p\\left(\\mathbf{h}_{i}\\right)p\\left(\\mathbf{h}_{i}^{g}\\right)}}\\end{array}$ and $\\begin{array}{r}{e^{s i m(\\mathbf{h}_{i},\\mathbf{z}_{i}^{v})/\\tau_{p}}\\propto\\frac{p(\\mathbf{h}_{i},\\mathbf{z}_{i}^{v})}{p(\\mathbf{h}_{i})p\\left(\\mathbf{z}_{i}^{v}\\right)}}\\end{array}$ , the following inequality holds: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\langle\\mathbf{H},\\mathbf{H}^{\\mathrm{p}}\\right\\rangle-I\\left(\\mathbf{H},\\mathbf{Z}^{\\mathrm{p}}\\right)}&{=\\frac{\\left\\langle S_{\\mathrm{p}}\\right\\rangle}{\\sum_{i=1}^{N}}p\\left(\\mathbf{h}_{i,i}\\mathbf{h}_{i}^{\\mathrm{q}}\\right)\\log\\frac{p\\left(\\mathbf{h}_{i,i}\\mathbf{h}_{i}^{\\mathrm{q}}\\right)}{p\\left(\\mathbf{h}_{i}\\right)p\\left(\\mathbf{h}_{i}^{\\mathrm{q}}\\right)}\\sum_{\\mathbf{\\mu}=\\nu}^{\\infty/\\infty}\\left\\{p\\left(\\mathbf{h}_{i,i}\\mathbf{\\sigma}_{i}^{\\mathrm{q}}\\right)\\log\\frac{p\\left(\\mathbf{h}_{i,i}\\mathbf{\\sigma}_{i}^{\\mathrm{q}}\\right)}{p\\left(\\mathbf{h}_{i}\\right)p\\left(\\mathbf{\\sigma}_{i}^{\\mathrm{q}}\\right)}\\right.}\\\\ &{\\left.\\underset{\\left.\\times\\nu_{i-1}^{N}\\right\\rangle_{i}^{2}}{\\underbrace{\\left\\langle\\mathbf{\\sigma}_{\\mathbf{h}_{i}^{\\mathrm{p}}}\\right\\rangle\\left(\\mathbf{h}_{i}^{\\mathrm{q}}\\right)\\log\\frac{p\\left(\\mathbf{h}_{i,\\mathbf{h}}\\right)^{q}}{p\\left(\\mathbf{h}_{i}\\right)p\\left(\\mathbf{\\sigma}_{i}^{\\mathrm{q}}\\right)}-\\sum_{i=1}^{N}\\frac{1}{\\left|\\mathbf{\\sigma}_{i}^{\\mathrm{p}}\\right\\rangle}p\\left(\\mathbf{h}_{i}^{\\mathrm{q}}\\right)\\log\\frac{p\\left(\\mathbf{h}_{i,i}\\mathbf{\\sigma}_{i}^{\\mathrm{p}}\\right)}{p\\left(\\mathbf{h}_{i}\\right)p\\left(\\mathbf{h}_{i}\\right)p\\left(\\mathbf{\\sigma}_{i}^{\\mathrm{q}}\\right)}}}\\\\ &{\\phantom{\\frac{....}{\\left.\\int_{0}^{\\infty}}\\left|\\frac{\\mathbf{\\sigma}_{\\mathbf{\\mu}}^{\\mathrm{p}}}{\\omega_{i}}\\right|\\ln_{i}^{2}\\left|\\mathbf{\\sigma}_{i}^{\\mathrm{q}}\\right\\rangle\\ln_{i}^{2}p\\left(\\mathbf{h}_{i}\\right)\\frac{p\\left(\\mathbf{h}_{i,i}\\mathbf{\\sigma}_{i}^{\\mathrm{q}}\\right)}{p\\left(\\mathbf{h}_{i}\\right)p\\left(\\mathbf{\\sigma}_{i}^{\\mathrm{q}}\\right)}}\\\\ &{\\left.\\underset{\\left.\\\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "C.2 Proof of Theorem 2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We consider a heterogeneous federated learning setting where $M$ multi-view clients and $S$ single-view clients. For each multi-view client, we define the empirical risk and its expectation as $\\bar{\\mathcal{L}}_{m}(f)$ and $\\mathcal{L}_{m}(f)$ , respectively. Similarly, for each single-view client, the empirical risk and its  expectation are denoted as $\\widehat{\\mathcal{L}}_{p}(f)$ and $\\mathcal{L}_{p}(f)$ . Let $f:\\mathcal{X}\\to\\mathbb{R}^{D_{v}}\\times\\mathbb{R}^{d_{v}}\\times\\mathbb{R}^{d_{v}}$ denotes the function that maps input samples into reconstruction samples, low-level features and high-level features. Then, the reconstruction samples, low-level features and high-level features are given by $\\hat{\\mathbf{x}}_{i}^{v}:=f_{x}\\left(\\mathbf{x}_{i}^{v}\\right)\\in\\mathbb{R}^{D_{v}}$ , $\\mathbf{z}_{i}^{v}:=f_{z}^{v}\\left(\\mathbf{x}_{i}^{v}\\right)\\in\\mathbb{R}^{d_{v}}$ , and $\\mathbf{h}_{i}^{v}:=f_{h}^{v}\\left(\\mathbf{x}_{i}^{v}\\right)\\in\\mathbb{R}^{d_{v}}$ , respectively. Additionally, we define $\\left\\{\\mathbf{x}_{i}^{v}\\right\\}_{v=1}^{V}:=$ $\\mathbf{x}_{i}$ , then common semantics are denoted as $\\mathbf{h}_{i}\\,:=\\,f_{h}^{0}\\left(\\mathbf{x}_{i}\\right)\\,\\in\\,\\mathbb{R}^{d}$ . To prove Theorem 2, we first introduce the following three lemmas. ", "page_idx": 15}, {"type": "text", "text": "Lemma 1. We define the empirical risk $\\widehat{\\mathcal{L}}_{m}(f)$ for multi-view client m as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\widehat{\\mathcal{L}}_{m}({f})=\\frac{1}{\\vert\\mathcal{M}_{m}\\vert}\\sum_{v=1}^{V}\\sum_{i=1}^{\\vert\\mathcal{M}_{m}\\vert}\\left[\\Vert\\mathbf{x}_{i}^{v}-f_{x}\\left(\\mathbf{x}_{i}^{v}\\right)\\Vert^{2}-\\log\\frac{e^{s i m\\left(\\mathbf{h}_{i},\\mathbf{h}_{i}^{v}\\right)/\\tau_{m}}}{\\sum_{j\\neq i}e^{s i m\\left(\\mathbf{h}_{i},\\mathbf{h}_{j}^{v}\\right)/\\tau_{m}}}\\right]}\\\\ {\\displaystyle=\\frac{1}{\\vert\\mathcal{M}_{m}\\vert}\\sum_{v=1}^{V}\\sum_{i=1}^{\\vert\\mathcal{M}_{m}\\vert}\\left[\\Vert\\mathbf{x}_{i}^{v}-f_{x}\\left(\\mathbf{x}_{i}^{v}\\right)\\Vert^{2}-\\log\\frac{e^{s i m\\left(f_{h}^{0}\\left(\\mathbf{x}_{i}\\right),f_{h}^{v}\\left(\\mathbf{x}_{i}^{v}\\right)\\right)/\\tau_{m}}}{\\sum_{j\\neq i}e^{s i m\\left(f_{h}^{0}\\left(\\mathbf{x}_{i}\\right),f_{h}^{v}\\left(\\mathbf{x}_{j}^{v}\\right)\\right)/\\tau_{m}}}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Let $\\mathcal{L}_{m}(f)$ be the expectation of $\\widehat{\\mathcal{L}}_{m}(f)$ . Suppose that for any $\\textbf{x}\\in\\:\\mathcal{X}$ and $f\\,\\in\\,{\\mathcal{F}}$ , there exists $D<\\infty$ such that $\\left\\|\\mathbf{x}\\right\\|,\\left\\|f_{x}\\left(\\mathbf{x}\\right)\\right\\|,\\left\\|f_{h}^{v}\\left(\\mathbf{x}\\right)\\right\\|,\\left\\|f_{h}^{0}\\left(\\mathbf{x}\\right)\\right\\|\\in\\left[0,D\\right]$ hold. With probability $1-\\delta$ for any $f\\in\\mathcal F$ , the following inequality holds ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{L}_{m}(f)\\leq\\widehat{\\mathcal{L}}_{m}(f)+\\frac{12V D^{2}}{\\sqrt{\\left|\\mathcal{M}_{m}\\right|}}+9V D^{2}\\sqrt{\\frac{\\log\\frac{1}{\\delta}}{2\\left|\\mathcal{M}_{m}\\right|}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. This proof is inspired by [37]. For simplicity, we define $\\begin{array}{r l}{f_{h}\\left(\\mathbf{x}_{i},\\mathbf{x}_{j}\\right)}&{{}:=}\\end{array}$ sim $\\left(f_{h}^{0}\\left(\\mathbf{x}_{i}\\right),f_{h}^{v}\\left(\\mathbf{x}_{j}^{v}\\right)\\right)/\\tau_{m}$ and $f_{h}\\left(\\mathbf{x}_{i}\\right):=\\,s i m\\left(f_{h}^{0}\\left(\\mathbf{x}_{i}\\right),f_{h}^{v}\\left(\\mathbf{x}_{i}^{v}\\right)\\right)/\\tau_{m}$ , respectively. Then, the empirical risk and its expectation can be formulated as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\Sigma}_{m}(f)=\\frac{1}{\\left|M_{m}\\right|}\\displaystyle\\sum_{v=1}^{V}\\sum_{i=1}^{\\left|M_{m}\\right|}\\left[\\left\\|\\mathbf{x}_{i}^{v}-f_{x}\\left(\\mathbf{x}_{i}^{v}\\right)\\right\\|^{2}-\\log\\frac{e^{f_{h}\\left(\\mathbf{x}_{i}\\right)}}{\\sum_{j\\neq i}e^{f_{h}\\left(\\mathbf{x}_{i},\\mathbf{x}_{j}\\right)}}\\right]}\\\\ &{\\qquad=\\frac{1}{\\left|M_{m}\\right|}\\displaystyle\\sum_{v=1}^{V}\\sum_{i=1}^{\\left|M_{m}\\right|}\\left[\\left\\|\\mathbf{x}_{i}^{v}-f_{x}\\left(\\mathbf{x}_{i}^{v}\\right)\\right\\|^{2}+\\log\\left(\\displaystyle\\sum_{j\\neq i}\\exp\\left\\{f_{h}\\left(\\mathbf{x}_{i},\\mathbf{x}_{j}\\right)\\right\\}\\right)-f_{h}\\left(\\mathbf{x}_{i}\\right)\\right]}\\\\ &{\\qquad=\\frac{1}{\\left|M_{m}\\right|}\\displaystyle\\sum_{v=1}^{V}\\sum_{i=1}^{\\left|M_{m}\\right|}\\left[\\left\\|\\mathbf{x}_{i}^{v}-f_{x}\\left(\\mathbf{x}_{i}^{v}\\right)\\right\\|^{2}-f_{h}\\left(\\mathbf{x}_{i}\\right)\\right]+\\frac{1}{\\left|M_{m}\\right|}\\displaystyle\\sum_{v=1}^{V}\\sum_{i=1}^{\\left|M_{m}\\right|}\\log\\sum_{j\\neq i}\\exp\\left\\{f_{h}\\left(\\mathbf{x}_{i},\\mathbf{x}_{j}\\right)\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{L}_{m}(f)=\\sum_{v=1}^{V}\\mathbb{E}_{\\mathbf{x}}\\left[\\left\\Vert x_{i}^{v}-f_{x}\\left(x_{i}^{v}\\right)\\right\\Vert^{2}-f_{h}\\left(\\mathbf{x}_{i}\\right)\\right]+\\sum_{v=1}^{V}\\mathbb{E}_{\\mathbf{x}}\\left[\\log\\sum_{j\\neq i}\\exp\\left\\{f_{h}\\left(\\mathbf{x}_{i},\\mathbf{x}_{j}\\right)\\right\\}\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Let $\\bar{\\mathcal{M}}_{m}$ be the sample set that different from $\\mathcal{M}_{m}$ by only one set of data $\\bar{\\mathbf{x}}_{r}:=\\left\\{\\bar{\\mathbf{x}}_{r}^{v}\\right\\}_{v=1}^{V}$ . The empirical risk of the function $f$ on $\\bar{\\mathcal{M}}_{m}$ is denoted as $\\widehat{\\mathcal{L}}_{m}^{\\prime}(f)$ . We have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\displaystyle\\left|\\operatorname*{sup}_{f\\in\\mathcal{F}}\\left|\\mathcal{L}_{n}(f)-\\hat{\\mathcal{L}}_{n}(f)\\right|-\\operatorname*{sup}_{i\\in\\mathcal{F}}\\left|\\mathcal{L}_{n}(f)-\\hat{\\mathcal{L}}_{n}^{c}(f)\\right|\\right|}\\\\ &{\\le\\operatorname*{sup}_{f\\in\\mathcal{F}}\\left|\\hat{\\mathcal{L}}_{n}(f)-\\hat{\\mathcal{L}}_{n}(f)\\right|}\\\\ &{\\le\\operatorname*{sup}_{f\\in\\mathcal{F}}\\left|\\displaystyle\\left|\\frac{1}{M_{\\mathrm{min}}}\\right|\\sum_{i=1}^{V}\\left(\\|\\mathbf{x}_{r}^{*}-f_{\\varepsilon}(\\mathbf{x}_{r}^{*})\\|^{2}-\\|\\mathbf{x}_{r}^{*}-f_{\\varepsilon}(\\mathbf{x}_{r}^{*})\\|^{2}-(f_{h}(\\mathbf{x}_{r})-f_{h}(\\mathbf{x}_{r}))\\right)\\right|}\\\\ &{\\quad+\\operatorname*{sup}_{f\\in\\mathcal{F}}\\displaystyle\\left|\\frac{1}{M_{\\mathrm{min}}}\\sum_{i=1}^{V}\\left(\\log_{\\sqrt{n}}\\exp\\left\\{f_{h}(\\mathbf{x}_{r},\\mathbf{x}_{g})\\right\\}-\\log_{\\sqrt{n}}\\exp\\left\\{f_{h}(\\mathbf{x}_{r},\\mathbf{x}_{g})\\right\\}\\right)\\right|}\\\\ &{\\le\\operatorname*{sup}_{f\\in\\mathcal{F}}\\displaystyle\\left|\\frac{1}{M_{\\mathrm{min}}}\\sum_{i=1}^{V}\\left|\\|\\mathbf{x}_{r}^{*}\\|^{2}-\\|\\mathbf{x}_{r}^{*}\\|^{2}+\\|f_{x}(\\mathbf{x}_{r}^{*})\\|^{2}-\\|f_{x}(\\mathbf{x}_{r}^{*})\\|^{2}+2\\|\\mathbf{x}_{r}^{*}\\|\\|f_{x}(\\mathbf{x}_{r}^{*})\\|+2\\|\\mathbf{x}_{r}^{*}\\|\\|f_{x}(\\mathbf{x}_{r}^{*})\\|}\\\\ &{\\quad+\\operatorname*{sup}_{f\\in\\mathcal{F}}\\displaystyle\\left|\\frac{1}{M_{\\mathrm{min}}}\\sum_{i=1}^{V}\\left|f_{h}(\\mathbf{x}_{r})-f_{h}(\\mathbf{x}_{r})\\right|+\\operatorname*{sup}_{f\\in\\mathcal{F}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, we analyze the upper bound of the expectation term $\\mathbb{E}\\operatorname*{sup}_{f\\in{\\mathcal{F}}}|{\\widehat{\\mathcal{L}}}_{m}(f)\\,-\\,{\\mathcal{L}}_{m}(f)|$ . Let $\\sigma_{1},\\ldots,\\sigma_{|\\mathcal{M}_{m}|}$ be i.i.d. independent random variables taking values in $\\{-1,1\\}$ and $\\bar{\\mathcal{M}}_{m}$ be the independent copy of $\\mathcal{M}_{m}$ . We have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}_{\\left(-\\infty,\\infty,\\infty,\\infty,\\infty,\\infty\\right)}\\left|\\left|\\frac{1}{N k_{0}}\\sum_{i=1}^{\\infty}\\left|\\left(N_{0}^{c}-f_{i}(x,x_{i}^{*})\\right|^{2}-\\left|\\mathcal{E}_{\\left(-\\infty,\\infty,\\infty,\\frac{\\infty}{N_{0}}\\right)/2}\\right|\\right)\\right|}\\\\ &{\\ \\ \\ \\ \\ +\\mathcal{E}_{\\left(-\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\right)}\\left|\\left|\\frac{1}{N k_{0}}\\sum_{i=1}^{\\infty}\\left|\\left(f_{i}(x)-f_{i}(x)\\right)\\right|^{2}}\\\\ &{\\ \\ \\ \\ \\ +\\mathcal{E}_{\\left(-\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\right)}\\left|\\left|\\frac{1}{N k_{0}}\\sum_{i=1}^{\\infty}\\frac{\\left|\\left(\\log\\left(x\\right)\\right)\\right|^{2}}{\\sum_{j=1}^{\\infty}\\left|\\left(\\log\\left(x\\right)\\right)\\right|^{2}}-1\\right|+\\infty\\log\\frac{\\exp\\left(f_{i}(x,x_{i}^{*})\\right)}{\\sum_{j=1}^{\\infty}\\left|\\left(\\log\\left(x\\right)\\right)\\right|^{2}}\\right|}\\\\ &{\\ \\ \\ \\ \\ +\\mathcal{E}_{\\left(-\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\ \\ \\ }\\\\ &{\\ \\ \\ \\ \\ \\ +\\mathcal{E}_{\\left(-\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty,\\infty\\right)}\\left|\\left|\\frac{1}{N k_{0}}\\sum_{i=1}^{\\infty}\\left|\\left(f_{i}(x)-f_{i}(x)\\right)\\right|^{2}+2\\5\\alpha_{4}\\alpha_{4}\\exp\\left|\\left(\\frac{1}{\\sqrt{N k_{0}}}\\sum_{i=1}^{\\infty}\\alpha_{2}f_{i}(x,x_{i}^{*})\\right)\\right|}\\\\ &{\\ \\ \\ \\ \\ +25\\alpha_{4}\\exp\\left|\\left|\\left(\\frac{1}{\\sqrt{N k_{0}}}\\sum_{i=1}^{\\infty}\\left|\\frac{\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the second last inequality inequality is obtained by the Khintchine-Kahane inequality[21]. Thus, according to the McDiarmid inequality [31], with probability at least $1-\\delta$ for any $f\\in\\mathcal F$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{L}_{m}(f)\\leq\\widehat{\\mathcal{L}}_{m}(f)+\\frac{12V D^{2}}{\\sqrt{\\left|\\mathcal{M}_{m}\\right|}}+9V D^{2}\\sqrt{\\frac{\\log\\frac{1}{\\delta}}{2\\left|\\mathcal{M}_{m}\\right|}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Due to the presence of view gaps among different views, let the data from the $v$ -th view follow the distribution $\\mathcal{D}_{v}$ , and the multi-view data follow the distribution $\\mathcal{D}$ . We analyze the generalization bounds for $f_{m}^{v}\\left(\\cdot;\\mathbf{w}_{m}^{v}\\right)$ in multi-view client $m$ , which is built upon prior works from domain adaptation. ", "page_idx": 17}, {"type": "text", "text": "Lemma 2 (Generalization Bounds for Domain Adaptation [2, 3]). Consider the v-th view data domain $\\mathcal{D}_{v}$ and the multi-view data domain $\\mathcal{D}$ , respectively. Given a feature extraction function $\\mathcal{R}:\\mathcal{X}\\mapsto\\mathcal{H}$ that shared between $\\mathcal{D}_{v}$ and $\\mathcal{D}$ . Let $\\mathcal{F}$ be a set of hypothesis with VC-dimension $d,$ . Then, for every $f\\in\\mathcal F$ , with probability at least $1-\\delta$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{L}_{m}(f)\\leq\\mathcal{L}_{m^{\\upsilon}}(f)+\\sqrt{\\frac{4}{|\\mathcal{M}_{m}|}\\left(d\\log\\frac{2e\\,|\\mathcal{M}_{m}|}{d}+\\log\\frac{4}{\\delta}\\right)}+d_{\\mathcal{F}}\\left(\\tilde{\\mathcal{D}}_{v},\\tilde{\\mathcal{D}}\\right)+\\lambda_{v},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $d_{\\mathcal{F}}\\left(\\tilde{\\mathcal{D}}_{v},\\tilde{\\mathcal{D}}\\right)$ denotes the divergence measured over a symmetric-difference hypothesis space. $\\tilde{\\mathcal{D}}_{v}$ and $\\tilde{\\mathcal{D}}$ are the induced distributions of $\\mathcal{D}_{v}$ and $\\mathcal{D}$ under ${\\mathcal R}$ , respectively, s.t. $\\mathbb{E}_{h\\sim\\tilde{\\mathcal{D}}_{v}}[B(h)]=$ $\\mathbb{E}_{x\\sim\\mathcal{D}_{v}}[B(\\mathcal{R}(x))]$ given a probability event $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ , and so for $\\tilde{\\mathcal{D}}$ $.\\ \\lambda_{v}:=\\operatorname*{min}_{f}\\mathcal L_{m^{v}}(f)+\\mathcal L_{m}(f)$ denotes an oracle performance. ", "page_idx": 17}, {"type": "text", "text": "Lemma 3. We define the empirical risk $\\widehat{\\mathcal{L}}_{p}(f)$ for single-view client p as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\hat{\\mathcal{L}}_{p}(f)=\\frac{1}{|S_{p}|}\\sum_{i=1}^{|S_{p}|}\\left[\\|\\mathbf{x}_{i}^{v}-f_{x}\\left(\\mathbf{x}_{i}^{v}\\right)\\|^{2}-\\log\\frac{e^{s i m\\left(\\mathbf{h}_{i},\\mathbf{h}_{i}^{v}\\right)/\\tau_{p}}}{e^{s i m\\left(\\mathbf{h}_{i},\\mathbf{h}_{i}^{v}\\right)/\\tau_{p}}+e^{s i m\\left(\\mathbf{h}_{i},\\mathbf{z}_{i}^{v}\\right)/\\tau_{p}}}\\right]}\\\\ {\\displaystyle=\\frac{1}{|S_{p}|}\\sum_{i=1}^{|S_{p}|}\\left[\\|\\mathbf{x}_{i}^{v}-f_{x}\\left(\\mathbf{x}_{i}^{v}\\right)\\|^{2}-\\log\\frac{e^{s i m\\left(f_{h}^{0}(\\mathbf{x}_{i}),f_{g}^{0}(\\mathbf{x}_{i})\\right)/\\tau_{p}}}{e^{s i m\\left(f_{h}^{0}(\\mathbf{x}_{i}),f_{g}^{0}(\\mathbf{x}_{i})\\right)/\\tau_{p}}+e^{s i m\\left(f_{h}^{0}(\\mathbf{x}_{i}),f_{z}^{v}\\left(\\mathbf{x}_{i}^{v}\\right)\\right)/\\tau_{p}}}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Let $\\mathcal{L}_{p}(f)$ be the expectation of $\\widehat{\\mathcal{L}}_{p}(f)$ . Suppose that for any $\\mathbf{x}\\in\\mathcal{X}$ and $f\\in{\\mathcal{F}}$ , there exists $D<\\infty$ such that $\\left\\|\\mathbf{x}\\right\\|,\\left\\|f_{x}\\left(\\mathbf{x}\\right)\\right\\|,\\left\\|f_{z}^{v}\\left(\\mathbf{x}\\right)\\right\\|,\\left\\|f_{h_{-}}^{0}(\\mathbf{x})\\right\\|,\\left\\|f_{g}^{0}\\left(\\mathbf{x}\\right)\\right\\|\\in\\left[0,D\\right]$ hold. With probability $1-\\delta$ for any $f\\in\\mathcal F$ , the following inequality holds ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{L}_{p}(f)\\leq\\widehat{\\mathcal{L}}_{p}(f)+\\frac{10D^{2}}{\\sqrt{|S_{p}|}}+8D^{2}\\sqrt{\\frac{\\log\\frac{1}{\\delta}}{2\\left|S_{p}\\right|}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. For simplicity, we define $f_{g}\\left(\\mathbf{x}_{i}\\right)\\ \\ :=\\ \\,s i m\\left(f_{h}^{0}\\left(\\mathbf{x}_{i}\\right),f_{g}^{0}\\left(\\mathbf{x}_{i}\\right)\\right)/\\tau_{p}$ and $f_{h,z}\\left(\\mathbf{x}_{i}\\right)\\mathbf{\\Sigma}:=$ sim $\\left(f_{h}^{0}\\left(\\mathbf{x}_{i}\\right),f_{z}^{v}\\left(\\mathbf{x}_{i}^{v}\\right)\\right)/\\tau_{p}$ , respectively. Then, the empirical risk and its expectation can be formulated as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\mathcal{L}}_{p}(f)=\\displaystyle\\frac{1}{|S_{p}|}\\sum_{i=1}^{|S_{p}|}\\left[\\|\\mathbf{x}_{i}^{v}-f_{x}\\left(\\mathbf{x}_{i}^{v}\\right)\\|^{2}-\\log\\frac{e^{f_{g}\\left(\\mathbf{x}_{i}\\right)}}{e^{f_{g}\\left(\\mathbf{x}_{i}\\right)}+e^{f_{h,z}\\left(\\mathbf{x}_{i}\\right)}}\\right]}\\\\ &{\\qquad=\\displaystyle\\frac{1}{|S_{p}|}\\sum_{i=1}^{|S_{p}|}\\left[\\|\\mathbf{x}_{i}^{v}-f_{x}\\left(\\mathbf{x}_{i}^{v}\\right)\\|^{2}+\\log\\left(e^{f_{g}\\left(\\mathbf{x}_{i}\\right)}+e^{f_{h,z}\\left(\\mathbf{x}_{i}\\right)}\\right)-f_{g}\\left(\\mathbf{x}_{i}\\right)\\right]}\\\\ &{\\qquad=\\displaystyle\\frac{1}{|S_{p}|}\\sum_{i=1}^{|S_{p}|}\\left[\\|\\mathbf{x}_{i}^{v}-f_{x}\\left(\\mathbf{x}_{i}^{v}\\right)\\|^{2}-f_{g}\\left(\\mathbf{x}_{i}\\right)\\right]+\\displaystyle\\frac{1}{|S_{p}|}\\sum_{i=1}^{|S_{p}|}\\log\\left(e^{f_{g}\\left(\\mathbf{x}_{i}\\right)}+e^{f_{h,z}\\left(\\mathbf{x}_{i}\\right)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{L}_{p}(f)=\\mathbb{E}_{\\mathbf{x}}\\left[\\left\\Vert x_{i}^{v}-f_{x}\\left(x_{i}^{v}\\right)\\right\\Vert^{2}-f_{h}\\left(\\mathbf{x}_{i}\\right)\\right]+\\mathbb{E}_{\\mathbf{x}}\\left[\\log\\left(e^{f_{g}\\left(\\mathbf{x}_{i}\\right)}+e^{f_{h,z}\\left(\\mathbf{x}_{i}\\right)}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Let $\\bar{S}_{p}$ be the sample set that different from ${\\mathcal S}_{p}$ by only one sample point $\\bar{\\bf x}_{r}$ . The empirical risk of the function $f$ on $\\bar{S}_{p}$ is denoted as $\\widehat{\\mathcal{L}}_{p}^{\\prime}(f)$ . We have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\bigg|\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\left|\\mathcal{L}_{p}(f)-\\hat{\\mathcal{L}}_{p}(f)\\right|-\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\left|\\mathcal{L}_{p}(f)-\\hat{\\mathcal{L}}_{p}(f)\\right|\\bigg|}\\\\ &{\\leq\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\left|\\hat{\\mathcal{L}}_{p}(f)-\\hat{\\mathcal{L}}_{p}(f)\\right|}\\\\ &{\\leq\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\left|\\frac{1}{|\\mathcal{S}_{p}|}\\bigg([\\kappa_{\\mathfrak{p}}^{*}-f_{*}(\\kappa_{\\mathfrak{p}}^{*})]^{2}-\\left\\Vert\\mathbf{k}_{\\mathfrak{p}}^{*}-f_{*}(\\kappa_{\\mathfrak{p}}^{*})\\right\\Vert^{2}-(f_{\\mathfrak{p}}(\\kappa_{\\mathfrak{p}})-f_{*}(\\kappa_{\\mathfrak{p}}))\\bigg)\\right|}\\\\ &{\\quad+\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\left|\\frac{1}{|\\mathcal{S}_{p}|}\\left(\\log\\left(e^{f_{*}(\\kappa_{\\mathfrak{p}}^{*})}+e^{f_{*}(\\kappa_{\\mathfrak{p}}^{*})}\\right)-\\log\\left(e^{f_{*}(\\kappa_{\\mathfrak{p}}^{*})}+e^{f_{*}(\\kappa_{\\mathfrak{p}}^{*})}\\right)\\right)\\right|}\\\\ &{\\leq\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\frac{1}{|\\mathcal{S}_{p}|}\\left|\\left|\\mathbf{k}_{\\mathfrak{p}}^{*}\\right|^{2}-\\left\\Vert\\mathbf{g}_{\\mathcal{F}}^{*}\\right\\Vert^{2}-\\left\\Vert\\mathbb{L}_{p}\\left(\\mathbf{x}_{\\mathfrak{p}}^{*}\\right)\\right|^{2}-\\left\\Vert\\mathbb{L}_{p}^{*}\\right\\Vert\\left\\Vert f_{\\mathcal{R}}\\left(\\mathbf{x}_{\\mathfrak{p}}^{*}\\right)\\right\\Vert+2\\left\\Vert\\mathbf{k}_{\\mathfrak{p}}^{*}\\right\\Vert\\left\\Vert f_{\\mathcal{R}}\\left(\\mathbf{x}_{\\mathfrak{p}}^{*}\\right)\\right\\Vert}\\\\ &{\\quad+\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\frac{1}{|\\mathcal{S}_{p}|}\\left|f_{\\mathcal{F}}\\left(\\mathbf{x}_{\\mathfrak{p\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, we analyze the upper bound of the expectation term $\\mathbb{E}\\operatorname*{sup}_{f\\in\\mathcal{F}}|\\widehat{\\mathcal{L}}_{p}(f)\\,-\\,\\mathcal{L}_{p}(f)|$ . Let $\\sigma_{1},\\ldots,\\sigma_{|S_{p}|}$ be i.i.d. independent random variables taking values in $\\{-1,1\\}$ and $\\bar{S}_{p}$ be the independent copy of ${\\cal S}_{p}$ . We have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\underset{0\\leq t\\leq T}{\\operatorname*{sup}}\\left\\{\\sum_{j=0}^{H}\\int_{\\mathbb{D}_{t}}^{\\infty}\\left\\lvert\\mathcal{F}_{j}(t)-\\mathcal{E}_{j}(t)\\right\\rvert\\right\\}}\\\\ &{\\leq\\mathbb{E}_{\\theta\\in\\mathcal{S}_{m}}\\frac{1}{p}\\Bigg[\\frac{1}{|\\mathcal{F}_{t}|}\\frac{\\mathbb{S}_{t}^{\\perp}}{\\sum_{i=1}^{K}\\left(\\left\\lvert\\mathcal{K}_{i}\\right\\rvert^{2}\\right)^{2}}\\left\\lbrace(\\mathbf{x}_{t}^{*}-{f}_{i}(\\mathbf{x}_{t}^{*}))\\right\\rbrace^{2}-\\left\\lbrace\\mathbf{X}_{t}^{*}-{f}_{i}(\\mathbf{x}_{t}^{*})\\right\\rbrace^{2}\\Bigg]+\\mathbb{E}_{\\theta\\in\\mathcal{S}_{m}}\\frac{1}{p(\\theta)}\\Bigg[\\frac{1}{|\\mathcal{F}_{t}|}\\frac{\\mathbb{S}_{t}^{\\perp}}{\\sum_{i=1}^{K}\\left(\\left\\lvert\\mathcal{F}_{t}\\right\\rvert^{2}\\right)^{2}}\\Bigg]}\\\\ &{\\ \\ \\ +\\mathbb{E}_{\\theta\\in\\mathcal{S}_{m}}\\frac{1}{p}\\Bigg[\\frac{1}{|\\mathcal{F}_{t}|}\\frac{\\mathbb{S}_{t}^{\\perp}}{\\sum_{i=1}^{K}\\left(\\left\\lvert\\mathcal{F}_{t}\\right\\rvert^{2}+(\\left\\lvert\\mathcal{F}_{t}\\right\\rvert^{2}+(\\left\\lvert\\mathcal{K}_{t}\\right\\rvert^{2})\\right)-\\log\\left(\\frac{1}{|\\mathcal{F}_{t}|}(\\mathbf{x}_{t}^{*})+\\ell_{*}(\\mathbf{x}_{t}^{*})\\right)\\right)\\Bigg]}\\\\ &{\\leq2\\mathbb{E}_{\\theta\\in\\mathcal{F}_{m}}\\frac{1}{p}\\Bigg[\\Bigg\\lvert\\mathcal{S}_{t}\\Bigg\\rvert\\Bigg]\\frac{1}{|\\mathcal{F}_{t}|}\\frac{\\mathbb{S}_{t}^{\\perp}}{\\sum_{i=1}^{K}\\left(\\left\\lvert\\mathcal{F}_{t}\\right\\rvert^{2}+(\\left\\lvert\\mathcal{F}_{t}\\right\\rvert^{2}+(\\left\\lvert\\mathcal{F}_{t}\\right\\rvert^{2})\\right\\rvert\\right)^{2}}+\\mathbb{E}_{\\theta\\in\\mathcal{F}_{m}}\\frac{\\mu}{p}\\Bigg[\\Bigg\\lvert\\mathcal\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, according to McDiarmid inequality [31], with probability at least $1-\\delta$ for any $f\\in\\mathcal F$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{L}_{p}(f)\\leq\\widehat{\\mathcal{L}}_{p}(f)+\\frac{10D^{2}}{\\sqrt{|S_{p}|}}+8D^{2}\\sqrt{\\frac{\\log\\frac{1}{\\delta}}{2\\left|S_{p}\\right|}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Our objective is to collaboratively train all clients to obtain multiple heterogeneous global models capable of handling different view types. This is manifested in the optimization of multiple global objective functions. For the global objective of handling multiple view types simultaneously, expected risk is defined as ${\\mathcal{L}}_{M}(f)$ , typically optimized in the form of empirical risk minimization, defined as: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\widehat{\\mathcal{L}}_{M}(f)=\\frac{1}{M}\\sum_{m=1}^{M}\\widehat{\\mathcal{L}}_{m}(f).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Similarly, for handling individual view types, such as the $v$ -th view type, the global objective entails defining the expected risk as $\\mathcal{L}_{S^{v}}(f)$ , with empirical risk defined as: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\widehat{\\mathcal{L}}_{S^{v}}(f)=\\frac{1}{S^{v}}\\sum_{p\\in[S^{v}]}\\widehat{\\mathcal{L}}_{p}(f)+\\frac{1}{M}\\sum_{m=1}^{M}\\widehat{\\mathcal{L}}_{m^{v}}(f).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now we give the proof of Theorem 2. ", "page_idx": 19}, {"type": "text", "text": "Proof. According to Lemma 1, with probability at least $1-\\delta$ for any $f\\in\\mathcal F$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{L}_{M}(f)\\leq\\widehat{\\mathcal{L}}_{M}(f)+\\frac{12V D^{2}}{\\sqrt{M\\left|\\mathcal{M}_{m}\\right|}}+9V D^{2}\\sqrt{\\frac{\\log\\frac{1}{\\delta}}{2M\\left|\\mathcal{M}_{m}\\right|}},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $M$ is the number of multi-view clients and $|\\mathcal{M}_{m}|$ is the number of samples in each multi-view client. Additionally, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{1}{M}\\sum_{m=1}^{M}d_{\\mathcal{F}}\\left(\\tilde{\\mathcal{D}}_{v},\\tilde{\\mathcal{D}}\\right)=d_{\\mathcal{F}}\\left(\\tilde{\\mathcal{D}}_{v},\\tilde{\\mathcal{D}}\\right)=\\frac{2}{M}\\sum_{m=1}^{M}\\operatorname*{sup}_{f\\in\\mathcal{F}}\\left|\\operatorname*{Pr}_{\\tilde{\\mathcal{D}}_{v}}\\left[f\\right]-\\operatorname*{Pr}_{\\tilde{\\mathcal{D}}}\\left[f\\right]\\right|.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "According to Lemma 2 and Lemma 3, with probability at least $1-\\delta$ for any $f\\in\\mathcal F$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{S^{v}}(f)\\leq\\!\\!\\widehat{L}_{S^{v}}(f)+\\frac{10D^{2}}{\\sqrt{S^{v}\\left|{\\cal S}_{p}\\right|}}+8D^{2}\\sqrt{\\frac{\\log\\frac1\\delta}{2S^{v}\\left|{\\cal S}_{p}\\right|}}}\\\\ &{\\qquad\\qquad+\\,\\sqrt{\\frac{4}{M\\left|\\mathcal{M}_{m}\\right|}\\left(d\\log\\frac{2e M\\left|\\mathcal{M}_{m}\\right|}{d}+\\log\\frac4\\delta\\right)}+d_{\\mathcal{F}}\\left(\\tilde{\\mathcal{D}}_{v},\\tilde{\\mathcal{D}}\\right)+\\lambda_{v},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $S^{v}$ is the number of single-view clients with $v$ -th view type and $|\\mathcal{S}_{p}|$ is the number of samples in each single-view client. ", "page_idx": 20}, {"type": "text", "text": "D Experimental Settings ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "D.1 Datasets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We conduct the experiments on the following public datasets. MNIST-USPS [34] is a widely-used dataset for handwritten digits (0-9) and consists of 5000 examples with two views of digital images. The MNIST feature size is $28\\times28$ , while the USPS feature size is $16\\times16$ . BDGP [4] comprises 2500 examples related to drosophila embryos, each represented by a 1750-dimensional visual feature and a 79-dimensional textual feature. Multi-Fashion [41] is an image dataset featuring products like Coats, Dresses, and T-shirts, with images sized at $28\\times28$ . Following the approach in [10], which constructs a three-view version using 30,000 images, each instance includes three different images belonging to the same class. Consequently, the three views of each instance represent the same product with three distinct styles. NUSWIDE [9] is a web image dataset offering multiple views, including 65-dimensional color histogram, 226-dimensional block-wise color moments, 145-dimensional color correlogram, 74-dimensional edge direction histogram, and 129-dimensional wavelet texture. We utilize a total of 5000 samples for the evaluation of our proposed method. ", "page_idx": 20}, {"type": "table", "img_path": "GVlJVX3iiq/tmp/916b0013636ac052c158eb7ab5bc3ce87603b111a42a38f2972e99a504538227.jpg", "table_caption": ["Table 4: The statistics of experimental datasets "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "D.2 Implementation Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The models of all methods are implemented on the PyTorch [33] platform using NVIDIA RTX-3090 GPUs. For an encoder-decoder pair, the encoder structure follows Input- $\\mathrm{Fc_{500}\\mathrm{~-~}}$ $\\mathrm{Fc_{500}-F c_{2000}-F c_{20}}$ , and the decoder is symmetric to the encoder. The non-linear mappings $\\left\\{\\mathcal{H}^{1}\\left(\\mathbf{Z}^{1};\\Psi^{1}\\right),\\ldots,\\mathcal{H}^{V}\\left(\\mathbf{Z}^{V};\\Psi^{V}\\right)\\right\\}$ and ${\\mathcal{H}}(\\mathbf{Z};\\Psi)$ adopt network architectures of $\\mathrm{Fc_{20}-F c_{256}-}$ $\\mathrm{\\dot{F}c_{20}}$ and $\\mathrm{Fc}_{20V}-\\mathrm{Fc}_{256}-\\mathrm{Fc}_{20}$ , respectively. The activation function is ReLU [12], and the optimizer uses Adam. For all the datasets used, the learning rate is fixed at 0.0003, the batch size is set to 256, and the temperature parameters $\\tau_{m}$ and $\\tau_{p}$ are both set to 0.5. Local pre-training is performed for 250 epochs on all datasets. After each communication round between the server and clients, local training is conducted for 10 epochs for the BDGP dataset and 25 epochs for other datasets on each client. The communication rounds between the server and clients are set to $R=5$ . All experiments in the paper involving FMCSC that are not mentioned are performed when $M/S=1{:}1$ . ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "We report some results that number of parameters and runtime by FMCSC to give the reader some information about the computational resources used by the method. Table 5 shows that the number of parameters and runtime by FMCSC are small and easy to reproduce. ", "page_idx": 21}, {"type": "table", "img_path": "GVlJVX3iiq/tmp/d4fd8b856574c5ffb575a790ad375075afdb1ac85f19de684e1e6a475672b336.jpg", "table_caption": ["Table 5: Number of parameters and runtime by FMCSC. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "D.3 Comparison Methods ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We select 9 state-of-the-art methods, including HCP-IMSC [24], IMVC-CBG [39], DSIMVC [37], LSIMVC [27], ProImp [22], JPLTD [29], CPSPAN [18], FedDMVC [8] and FCUIF [36]. Among them, apart from FedDMVC and FCUIF, which are FedMVC methods, all the other comparison methods are centralized incomplete multi-view clustering methods. To ensure fair comparisons, we simplify the heterogeneous hybrid views scenario in our paper into a hybrid views scenario. Specifically, we concatenate the data distributed among the clients and use them as input for centralized methods, as shown in Figure 5. Among these, the data from multi-view clients can be considered complete data, while the data from single-view clients can be regarded as missing data. It\u2019s worth noting that in our reported results, our method operates under the heterogeneous hybrid views scenario, whereas the other comparative methods operate under the hybrid views scenario. Although existing solutions can bypass the challenge of heterogeneous clients by simply concatenating data, the exposure of raw data, due to privacy concerns, may cause more data owners to refuse to participate in collaborative training. In contrast, our method can extract complementary clustering structures across clients without exposing their raw data, offering better privacy protection and performance improvement than current state-of-the-art methods. ", "page_idx": 21}, {"type": "image", "img_path": "GVlJVX3iiq/tmp/dfb60de33b8a05571eeb2917465b27568b4379991e00d720dbc35a3e4e6846ea.jpg", "img_caption": ["Figure 5: Comparison strategies. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "E Additional Experiment Results ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "E.1 Convergence Analysis ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Convergence analysis of the reconstruction loss, consistency loss, and total loss for multi-view clients and single-view clients is conducted using MNIST-USPS, BDGP, Multi-Fashion, and NUSWIDE. As illustrated in Figure 6, the increasing number of epochs leads to a gradual convergence of all loss functions, ultimately reaching a stable state. This clear observation serves as compelling evidence for the stability and effectiveness of our proposed model. ", "page_idx": 21}, {"type": "image", "img_path": "GVlJVX3iiq/tmp/9781cf7ad0cef609fe35e7e8eea0580390317cc02d1ec6f58dc8be017f45289b.jpg", "img_caption": ["Figure 6: Convergence analysis on four datasets. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "E.2 Attributes of Federated Learning ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we present additional experimental results of FMCSC in various federated learning scenarios, including the number of clients and privacy. Figure 7 presents the impact of the number of clients on clustering performance for MNIST-USPS, BDGP, and NUSWIDE. It is observed that, with an increasing number of clients, the performance of FMCSC shows a slight decline but remains generally stable. However, on MNIST-USPS, as the number of clients reaches 50, the clustering performance experiences an unavoidable decrease due to the insufficient number of samples for each client. ", "page_idx": 22}, {"type": "image", "img_path": "GVlJVX3iiq/tmp/909230c89f87786ac76462d3207c935852f752c95c3a9afd5f1fa6f0839f74c5.jpg", "img_caption": ["Figure 7: Scalability with the number of clients. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "GVlJVX3iiq/tmp/9dbe377ef13afa48a1d820253169ba63a99c58a7f77654e967428cbfdb9584f4.jpg", "img_caption": ["Figure 8: Sensitivity under privacy constraints when $M/S=2{\\cdot}1$ . "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "In FMCSC, we assume that all participating parties are semi-honest and do not collude with each other. An attacker faithfully executes the training protocol but may launch privacy attacks to infer the private data of other parties. Previous studies have demonstrated that sharing gradients can leak information about raw data [11]. To address this concern, we utilize differential privacy to further enhance the privacy guarantee of FMCSC. Differential privacy algorithms aim to introduce noise during individual data processing to protect against the disclosure of private information [1]. Formally, let $\\boldsymbol{\\mathcal{A}}$ be a random mechanism that takes dataset $D$ as input and belongs to set $S$ . Assuming $D_{1}$ and $D_{2}$ are two neighboring datasets differing in only one point, $\\boldsymbol{\\mathcal{A}}$ is $(\\varepsilon,\\delta)$ differentially private if: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[A\\left({\\mathcal{D}}_{1}\\right)\\in S\\right]\\leq\\exp(\\varepsilon)\\cdot\\operatorname*{Pr}\\left[A\\left({\\mathcal{D}}_{2}\\right)\\in S\\right]+\\delta\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Here, $\\varepsilon$ and $\\delta$ quantify the individual impact on the overall output in differential privacy. $\\varepsilon$ represents the strength of differential privacy, with smaller values indicating stronger privacy protection at the potential cost of increased noise. $\\begin{array}{r}{\\dot{\\delta}\\sim\\mathcal{O}\\left(\\frac{1}{\\mathrm{number\\,of\\,samples}}\\right)}\\end{array}$ is used to handle probabilistic exceptional cases. We implement Laplace noise to achieve differential privacy, and Figure 8 reports the clustering performance of FMCSC under different privacy strengths $\\varepsilon$ . Our observations reveal that FMCSC achieves both high performance and privacy at $\\varepsilon=50$ , especially on the BDGP and Multi-Fashion datasets. However, as the level of noise increases at $\\varepsilon=10$ , the performance of FMCSC unavoidably degrades. ", "page_idx": 23}, {"type": "text", "text": "F Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Heterogeneous hybrid views are prevalent in real-world scenarios. Our proposed method extends the application domain of existing FedMVC approaches to address complex scenarios such as healthcare and the Internet of Things (IoT). For example, hospitals in metropolitan areas use CT, X-ray, and EHR for disease detection, whereas remote areas often rely on a single detection method. Similarly, smartphones can capture both audio and images simultaneously, while recording devices are limited to collecting audio data only. Furthermore, this research is not expected to introduce any new negative societal impacts beyond those already known. ", "page_idx": 23}, {"type": "text", "text": "G Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Our model addresses heterogeneous hybrid views in FedMVC, but it idealistically categorizes clients into two types: single-view clients and multi-view clients. In more realistic scenarios, multi-view clients can be further classified into full-view clients and partial-view clients based on the number of view types they have. Such a detailed categorization could encourage more heterogeneous devices to participate in federated learning, thereby enhancing the model\u2019s generalization ability and accelerating its application in fields like healthcare and finance. We will continue to explore this problem in future work and apply our findings to real-world scenarios. ", "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: In the abstract and introduction, we first introduce the relevant concepts of FedMVC and summarize existing methods, analyzing their shortcomings. Notably, existing methods assume isomorphic clients, prompting us to propose the concept of heterogeneous hybrid views. We focus on addressing the client gap and view gap that arise in this scenario. Our proposed method, which designs local-synergistic contrastive learning and globalspecific weighting aggregation, aims to bridge these gaps and explore cluster structures. Additionally, we analyze the method\u2019s generalization performance and its effectiveness across various federated learning scenarios through theoretical and experimental evaluation. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Please see Appendix G. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Please see Appendix C. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Please refer to Appendix D.2 for information on reproducing the main experimental results of the paper. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have uploaded the code for the new proposed method and the datasets used in the experiments in the Supplementary Material. The baseline methods compared in the paper are open source and can be reproduced directly after following the comparison strategy mentioned in Appendix D.3. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: Please refer to Appendix D for more experimental setting/details. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We report error bars in Table 1 and Figure 4 (b), which are standard deviations of the results obtained after five independent runs. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 26}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Please see Appendix D.2. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We have read the NeurIPS Code of Ethics and confirm that the paper complies. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Please see Appendix F. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 27}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We have cite the original paper that produced datasets see Appendix D.1. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]