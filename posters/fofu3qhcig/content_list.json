[{"type": "text", "text": "TuneTables: Context Optimization for Scalable Prior-Data Fitted Networks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Benjamin Feuer1, Robin Tibor Schirrmeister2, Valeriia Cherepanova \u22173, Chinmay Hegde1, Frank Hutter2, Micah Goldblum1, Niv Cohen\u20201, Colin White\u20204 ", "page_idx": 0}, {"type": "text", "text": "1 New York University, 2 University of Freiburg, 3 University of Maryland, 4 Abacus.AI ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "While tabular classification has traditionally relied on from-scratch training, a recent breakthrough called prior-data fitted networks (PFNs) challenges this approach. Similar to large language models, PFNs make use of pretraining and in-context learning to achieve strong performance on new tasks in a single forward pass. However, current PFNs have limitations that prohibit their widespread adoption. Notably, TabPFN achieves very strong performance on small tabular datasets but is not designed to make predictions for datasets of size larger than 1000. In this work, we overcome these limitations and substantially improve the performance of PFNs via context optimization. We introduce TuneTables, a parameter-efficient fine-tuning strategy for PFNs that compresses large datasets into a smaller learned context. We conduct extensive experiments on nineteen algorithms over 98 datasets and find that TuneTables achieves the best performance on average, outperforming boosted trees such as CatBoost, while optimizing fewer than $5\\%$ of TabPFN\u2019s parameters. Furthermore, we show that TuneTables can be used as an interpretability tool and can even be used to mitigate biases by optimizing a fairness objective. We open-source our code and raw results at https://github.com/penfever/TuneTables. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Tabular data, or data organized into rows and columns consisting of distinct features, are the oldest and one of the most ubiquitous types of data in machine learning in practice [10, 67]. Tabular data has numerous applications across medicine [41, 71], online advertising [33, 52, 64], finance [7, 18], and other areas [11, 12, 72]. ", "page_idx": 0}, {"type": "text", "text": "Competitive classification algorithms for tabular data include gradient-boosted decision trees [16, 61] and deep neural networks [31, 42, 69]. Both approaches fti their respective models on a labeled dataset containing samples from a distribution reflecting the task at hand. A recent breakthrough, prior-data fitted networks (PFNs) [35, 55], are are a specific type of neural process which learn to perform approximate Bayesian inference in a single forward pass using in-context learning [50]. PFNs do not require optimizing parameters or fitting a model on downstream training data, instead feeding training data into the context and conditioning on it. In particular, TabPFN achieved state-of-the-art classification on small tabular datasets [35, 51]. ", "page_idx": 0}, {"type": "text", "text": "The in-context learning approach of PFNs parallels that of large language models (LLMs) [82]. Both approaches can be viewed as approximate Bayesian inference, whether implicitly [79] or explicitly [55]. While researchers have successfully used various context optimization strategies for enhancing LLM performance [49], no prior work has studied context optimization strategies for PFNs. Furthermore, although TabPFN achieves very strong performance on small datasets, its limitations currently prohibit its widespread adoption: it only runs on datasets whose number of training samples, number of features, and number of classes are at most 1000, 100, and 10, respectively. ", "page_idx": 0}, {"type": "image", "img_path": "FOfU3qhcIG/tmp/f923b9ac7b8954cefd22ad45c106a045507341f0880c823ad784474b846e09a7.jpg", "img_caption": ["Figure 1: TuneTables: a novel prompt-tuning technique for prior-data fitted networks. TuneTables performs prompt tuning on a pre-trained prior-fitted network (TabPFN) to distill real-world datasets into learned embeddings, allowing for stronger performance and faster inference time than TabPFN in many cases. TuneTables also expands the capabilities of pre-trained PFNs; by way of example, we demonstrate its effectiveness for bias mitigation, and as an interpretability tool. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this work, we perform the first investigation into context optimization strategies for PFNs, allowing us to substantially improve their performance when scaled to large datasets. Specifically, we introduce TuneTables, a novel parameter-efficient fine-tuning technique for PFNs that compresses large datasets into a smaller learned context (Figure 1). We conduct an extensive empirical investigation into TuneTables\u2019 performance on the TabZilla Benchmark Suite, the largest benchmark considered by recent tabular data literature [51]. Over 98 datasets and 19 algorithms, we find that TuneTables achieves the best average performance and is the best-performing method on 30 of them. ", "page_idx": 1}, {"type": "text", "text": "Because TuneTables effectively compresses the contents of large tabular datasets into the tuned prompt, no training data is needed in the context during inference, significantly speeding up inference time (similar to works on neural processes; see Section 6). We also show that the learned prompt can be used as a tool for interpretability. Finally, we show how to use TuneTables for multi-objective optimization, such as optimizing both accuracy and fairness simultaneously, allowing users to mitigate biased predictions of a pretrained PFNs with just a lightweight tuning procedure. We open-source our code and raw results at https://github.com/penfever/TuneTables. ", "page_idx": 1}, {"type": "text", "text": "Our contributions. We describe our main contributions below. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce TuneTables, a parameter-efficient fine-tuning technique for PFNs. TuneTables achieves the highest number of wins (30) when compared to 19 algorithms over 98 datasets, while requiring less inference time than TabPFN.   \n\u2022 We show how to use prompt tuning for multi-objective optimization, such as optimizing both accuracy and fairness simultaneously, allowing users to mitigate biased predictions of a pretrained PFNs with just a lightweight tuning procedure.   \n\u2022 We show that TuneTables\u2019 condensed representations can be used as an interpretability tool.   \n\u2022 We conduct an extensive study on context optimization strategies for PFNs by performing an ablation study on TuneTables, as well as studying sketching and feature selection techniques.   \n\u2022 In order to better manage the limitations of our method, we introduce TuneTables-medium and TuneTables-light, which achieve strong tradeoffs between precision and speed. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "PFNs: review and limitations. In this section, we give a background on PFNs and discuss their limitations. For a complete description, see [35, 55, 58]. Assume that we have a classification problem with features $\\bar{\\boldsymbol{\\chi}}\\subseteq\\mathbb{R}^{d}$ and labels $\\boldsymbol{\\wp}$ . Given a dataset $D=D_{\\mathrm{train}}\\cup D_{\\mathrm{test}}$ , where $D_{\\mathrm{train}}=$ $\\left\\{(x_{1},y_{1}),\\ldots,(x_{n},y_{n})\\right\\}$ and $D_{\\mathrm{test}}~=~\\{(x_{\\mathrm{test}},y_{\\mathrm{test}})\\}$ , our goal is to predict the conditional class probabilities $p(\\cdot\\ \\mid x_{\\mathrm{test}})$ . In the Bayesian framework for supervised learning, the mechanism for generating the data distribution is a hypothesis $\\varphi$ , drawn from $\\Phi$ , the space of all hypotheses. $\\Phi$ encodes our prior beliefs on the system before observing data. In this framework, datasets $D$ are generated by first drawing $\\varphi\\sim\\Phi$ , and then drawing i.i.d. samples according to $\\varphi$ . The posterior predictive distribution (PPD) for a test sample $x_{\\mathrm{{test}}}$ is the label distribution $p(\\cdot\\ \\mid x_{\\mathrm{test}},D_{\\mathrm{train}})$ that follows from our prior. We can obtain the PPD by integrating over the space of hypotheses $\\Phi$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\np(\\boldsymbol{y}\\mid\\boldsymbol{x},D)\\propto\\int_{\\Phi}p(\\boldsymbol{y}\\mid\\boldsymbol{x},\\varphi)p(D\\mid\\varphi)p(\\varphi)d\\varphi.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "A PFN is a transformer-based architecture trained to approximate the PPD via synthetic prior-ftiting. Given a prior, we first sample hypotheses $\\varphi\\sim p(\\varphi)$ and then synthetic datasets $D\\sim p(D\\mid\\varphi)$ . We optimize the parameters of the PFN by predicting the class labels of $D_{\\mathrm{test}}\\subseteq D$ , conditioned on $D_{\\mathrm{train}}=D\\setminus D_{\\mathrm{test}}$ . We compute the loss by: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{PFN}}=\\mathbb{E}_{D\\sim p(D)}\\left[-\\log q_{\\theta}(y_{\\mathrm{test}}\\mid x_{\\mathrm{test}},D_{\\mathrm{train}})\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "for simplicity assuming all training and test sets are size $n$ and 1, respectively. We then approximately solve this optimization problem, $\\begin{array}{r}{\\hat{\\theta}=\\arg\\operatorname*{min}_{\\theta}\\mathcal{L}_{\\mathrm{PFN}}}\\end{array}$ , allowing $q_{\\theta}$ to approximate Equation (1): ", "page_idx": 2}, {"type": "equation", "text": "$$\nq_{\\theta}(y_{\\mathrm{test}}\\mid x_{\\mathrm{test}},D_{\\mathrm{train}})\\approx p(y_{\\mathrm{test}}\\mid x_{\\mathrm{test}},D_{\\mathrm{train}}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Scaling challenges. While PFNs, specifically TabPFN, have shown remarkable success in classification by in-context learning, several important obstacles constrain their more widespread adoption: ", "page_idx": 2}, {"type": "text", "text": "1. PFNs only accept a fixed number of features. The current design of PFNs fixes the quantity of features at the time of pretraining. This quantity cannot be changed without retraining the PFN.   \n2. PFNs scale poorly with the dataset size. While PFN accuracy can improve with more real-data samples at inference time [35, 58], the memory requirements scale with the context length, making extensions beyond a certain number of samples impractical.   \n3. PFNs only select from a fixed number of classes. The MLP decoder head co-trained with the PFN fixes the number of classes that can be identified at test time. ", "page_idx": 2}, {"type": "text", "text": "A motivating example. In Figure 3 (left), we present a comparison between CatBoost and TabPFNs3000 from [51], a version of TabPFN that, for datasets above 3000 data points, uses a random subset of 3000 data points, for fitting; this version also evaluates 30 feature subsets based on mutual information (requiring $30\\times$ more time). We ablate our feature and sample subselection strategies for TabPFNs3000 in Appendix Table 13. Although TabPFNs3000 performs very well on datasets with fewer than 1000 datapoints and 100 features, it significantly underperforms CatBoost beyond those constraints. ", "page_idx": 2}, {"type": "text", "text": "Approach. We propose sketching, feature selection, and fine-tuning as an attempt to remedy $(I)$ and (2). Then in Section 3, we describe novel prompt-tuning and class-extension strategies to create TuneTables, a robust classifier which remedies (1)-(3). ", "page_idx": 2}, {"type": "text", "text": "2.1 Classical sketching and feature selection ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Sketching. The number of samples that a PFN can handle is limited to around 3000 by conventional GPU sizes. However, in the real world, datasets are often much larger. Going forward, we refer the maximum allowable context size of the PFN as $n$ , and to the size of the real-world dataset as $N$ . ", "page_idx": 2}, {"type": "text", "text": "Given a training dataset $D_{\\mathrm{train}}$ of size $N>>n$ , one option is to select a representative subset of the dataset, $D_{\\mathrm{compact}}\\subseteq D_{\\mathrm{train}}$ , to use as the context. In general, researchers have studied a variety of data ", "page_idx": 2}, {"type": "text", "text": "summarization techniques, often called sketching, for tabular data [56]. In the context of a pretrained PFN $q_{\\theta}$ , the goal is to find a sketching function $s:\\mathbb{R}^{N\\times d}\\mapsto\\mathbb{R}^{n\\times\\tilde{d}}$ such that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~~\\mathbb{E}_{D\\sim p(D)}\\left[-\\log q_{\\theta}(y_{\\mathrm{test}}\\mid x_{\\mathrm{test}},s(D_{\\mathrm{train}}))\\right]}\\\\ &{\\approx\\,\\mathbb{E}_{D\\sim p(D)}\\left[-\\log q_{\\theta}(y_{\\mathrm{test}}\\mid x_{\\mathrm{test}},D_{\\mathrm{train}})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, we consider three sketching methods for TabPFN: random, in which we select a random subset of $n$ datapoints from the full training set; $k$ -means, in which we compute the $n$ -means clustering [3] of $D_{\\mathrm{train}}$ and select the $n$ centers; and CoreSet, in which we compute a core-set of size $n$ [2]. ", "page_idx": 3}, {"type": "text", "text": "Feature selection. In addition to the limitation on dataset size, PFNs, such as TabPFN, also impose a limitation on the number of features $d$ . Similar to sketching, given a dataset with $D>>d$ features, we can perform feature selection or summarization in order to adhere to the constraint (formally, finding a function that reduces the feature dimension and approximates an expression similar to Equation (4)). Feature selection is a critical part of tabular classification, and there are several popular feature selection methods [13, 17]. We investigate three different methods: random, in which we randomly select a set of $d$ features; mutual information, in which we select $d$ features with the highest mutual information of the target dataset [75]; and principal component analysis $(P C A)$ , in which we take the $d$ first principal components. In Section 4, we find that all sketching and feature selection methods plateau in performance well before approaching parity with GBDTs, which motivates our investigation of new scaling techniques. ", "page_idx": 3}, {"type": "text", "text": "2.2 Fine-tuning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We conclude this section with a discussion of another potential approach for scaling PFNs: finetuning. In this approach, given a dataset of size $N>>n$ , we use gradient descent to continue training all parameters of the PFN. However, we show in Section 4 that fine-tuning takes up considerable memory resources while still not achieving competitive accuracy on large datasets; we postulate that for most datasets, the synthetic prior of TabPFN is more robust to overfitting than the actual training data; fine-tuning overftis to the validation set. This is borne out by the observation that when fine-tuning TabPFN, validation accuracy continues to improve even as test accuracy declines. To remedy this, we present new parameter-efficient solutions for scaling PFNs in the next section. ", "page_idx": 3}, {"type": "text", "text": "3 TuneTables ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Motivated by the strong performance of prompt tuning, we introduce TuneTables, a new tabular classification algorithm. Using a pretrained PFN (TabPFN, in our experiments) as a base model, TuneTables overcomes the limitations of PFNs, allowing them to be applied to any tabular classification problem. ", "page_idx": 3}, {"type": "text", "text": "For the full details of the algorithm, please refer to Appendix D. We summarize here: (a) if a dataset is small enough to run with the original zero-shot version of TabPFN, we include it in our search, as the TabPFN is already highly optimized for such datasets; The reason why we do not only use TabPFN is that the exact size at which TuneTables outperforms TabPFN is dataset-dependent (with an average transition of around 800 samples). $(b)$ if there are more than 100 features, we perform grid search over a set of feature subselection methods and select the one which performs best on the validation set; (c) orthogonally, if there are too many labels, we fti a new decoder to a frozen TabPFN; $(d)$ we optimize over a search space of tuned prompts, both with and without real-data context during training, fitted to the dataset; (e) we report the best-performing model according to accuracy. ", "page_idx": 3}, {"type": "text", "text": "Prompt tuning as a scalable context for PFNs. Motivated by the limitations of sketching for large contexts, we explore soft prompt tuning as an alternative. In soft prompt tuning [46], given a dataset $D=D_{\\mathrm{train}}\\cup D_{\\mathrm{test}}.$ ,r  ae pmabreadmdeitnegri zdiedm emnastiroinx $D_{\\mathrm{tune}}^{p\\times e}$ iiss  ap rheyppeenrdpeadr atom ettheer i\u2013nthpeu ts iezme boef dtdhien tgu $D_{\\mathrm{train}}^{n\\times e}$ r, owmhpetr.e $e$ $p$ ", "page_idx": 3}, {"type": "text", "text": "The paper [46] demonstrates the effectiveness of soft prompt tuning for NLP tasks by prepending a small task-specific prompt $(p\\approx5)$ . These task-specific learned tokens prove effective at the extremely large model scales commonly encountered in NLP. Somewhat surprisingly, we show that prompt tuning is effective at similar scales of $p$ even for the much smaller tabular data models (see Appendix D and Appendix F). However, prompt tuning increases in effectiveness when $p$ is larger. ", "page_idx": 3}, {"type": "image", "img_path": "FOfU3qhcIG/tmp/34629fa222d772bb6ca5ffb0359be4d97da84d22de17f4db1b818cd1c1f08326.jpg", "img_caption": ["Figure 2: TuneTables and state-of-the-art tabular models. A critical difference plot according to mean accuracy rank across the 98 datasets in Table 1 of [51]. Algorithms which are not significantly different $(p>0.05)$ are connected with a horizontal black bar. TuneTables achieves the highest mean rank of any algorithm. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Soft prompt tuning for tabular data. Unlike in NLP, transformers for tabular data, including PFNs, generally accept two input embeddings; a continuous-valued embedding $D_{\\mathrm{train}X}$ , and a categoricallyvalued $D_{\\mathrm{train}y}$ which is passed through directly. We adapt the method of [46] by ftiting the parameters of $D_{\\mathrm{tune}\\,X}^{p\\times e}$ to $D_{\\mathrm{train}X}$ and randomly initializing $D_{\\mathrm{tune\\,}y}^{p\\times1}$ with an equal number of labels from each class in $D_{\\mathrm{train}}$ . These synthetic datapoints are optimized on the entire labeled set, and therefore give PFNs access to a much larger training set not accessible by existing methods. ", "page_idx": 4}, {"type": "text", "text": "We further adjust the method of [46] to allow for the possibility that $D_{\\mathrm{tune}}$ has learned, in essence, a distilled version of $D_{\\mathrm{train}}$ ; at test time, we evaluate two settings, hereafter referred to as $C$ (\u2018context\u2019) and $N C$ (\u2018no context\u2019). In the $C$ setting, following [46], we have $D_{\\mathrm{tune}}^{p\\times e}$ prepended to the input embedding $D_{\\mathrm{train}}^{n\\times e}$ . In the $N C$ setting, we provide only $D_{\\mathrm{tune}}^{p\\times e}$ at test time. In Section 4, we empirically evaluate both approaches. We ablate specifics of our implementation choices in Appendix D. Unlike prompt tuning for NLP, we show that the $N C$ setting is often competitive with, if not better than, the $C$ setting. We also ablate this choice during training; see Appendix D for implementation specifics. ", "page_idx": 4}, {"type": "text", "text": "Extending the number of predicted classes. TabPFN uses a pretrained transformer, with a two-layer MLP as a final classifier. The pretraining procedures limit the na\u00efve use of TabPFN to classification tasks with at most 10 classes. Yet, in many cases, datasets of interest might contain a larger number of classes, which would require pretraining a new PFN from scratch. ", "page_idx": 4}, {"type": "text", "text": "Following the method of last-layer retraining [44, 45], for such datasets, we fti a PFN to new posteriors given by real-world classification datasets with more than 10 classes, freezing all weights except for those of the decoder MLP and the tuned prompt (see Appendix D for more implementation details). Even after this modification, our method remains highly parameter-efficient, optimizing fewer than $5\\%$ of TabPFN\u2019s parameters. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Algorithms and datasets used. We compare TuneTables to nineteen algorithms, including three GBDTs: CatBoost [61], LightGBM [43], and XGBoost [16]; 11 neural networks: DANet [14], FT-Transformer [31], two MLPs [31], NODE [60], ResNet [31], SAINT [69], STG [80], TabNet [6], TabPFN [35], VIME [81], and ExcelFormer [15]; and five baselines: Decision Tree [62], KNN [21], Logistic Regression [22], Random Forest [48], and SVM [20]. For all algorithms, we use their official implementation; see Appendix C for more details. We also compare to TabPFNs3000. We run the algorithms on the TabZilla Benchmark Suite introduced in [51]. This suite consists of 98 classification datasets from OpenML [74] with a diversity of sizes and number of features [51]. See Table 4 in Appendix C for a list of all datasets with their statistics. ", "page_idx": 4}, {"type": "text", "text": "Experimental setup. For all algorithms other than TuneTables, we perform light hyperparameter tuning by running one default setting and 29 iterations of random search using Optuna [4]); see Appendix C for details. Following [51], all of the algorithms come with their default set of hyperparameters used in the official implementation, and we used all of these settings. For TuneTables, we optimize via a grid search described in Appendix D. ", "page_idx": 4}, {"type": "image", "img_path": "FOfU3qhcIG/tmp/147015fd1dfebd95c6d4e65506a096ba81a11ef92cfc571b3aca4b6e3cf240b4.jpg", "img_caption": ["Figure 3: TuneTables addresses TabPFN\u2019s limitations. (Left) Motivating example (using the subset of [51]on which both CatBoost and TabPFNs3000 report results): TabPFNs3000 is best on small datasets, but when scaled past 3000 datapoints and 100 features, TabPFNs3000 significantly underperforms. (Middle) CatBoost vs. TuneTables on LARGESCALETABLES $:$ By contrast, TuneTables is competitive with CatBoost on all datasets, mitigating the limitations of TabPFN. (Right) TabPFNs3000 vs. TuneTables on LARGESCALETABLES : TuneTables outperforms TabPFNs3000 on datasets with a high number of datapoints or features. The colorbar on the y axis represents the comparative change in per-dataset accuracy between two algorithms (A: blue, B: red). Positive numbers represent the absolute gain in accuracy of B w.r.t. A, negative numbers represent the absolute gain in accuracy of A w.r.t. B. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "We fti for up to 100 epochs with early stopping. For all algorithms, we report the test performance of the hyperparameter set with the best performance on the validation set and cross-validate on three train/test folds from OpenML. We conduct our experiments on an NVIDIA L4 TPU with 24GB VRAM. We summarize our results across datasets by reporting mean accuracy as well as the mean normalized accuracy (after Z-score normalization) for each algorithm. ", "page_idx": 5}, {"type": "text", "text": "Algorithm comparison. We compute statistically significant performance differences among algorithms averaged across all datasets, as done in prior work [42]. We first use a Friedman test to check whether performance differences between all algorithms are significant [30]. We reject the null hypothesis for $p<0.05$ . Then we use a Wilcoxon signed-rank test to check which pairs of algorithms have significant performance differences [19]. We use a Holm-Bonferroni correction to account for multiple comparisons [36]. See Figure 2. We find that TuneTables achieves the highest average rank, although the difference among the top three algorithms is not statistically significant. In Table 1, we present the accuracy, rank, and ${\\cal Z}_{}$ -score of all algorithms, averaged across all datasets, and find that TuneTables performs very well on all metrics but runtime. ", "page_idx": 5}, {"type": "text", "text": "Large datasets. One limitation of the TabZilla Benchmark Suite is that even the largest dataset in the comparison contains only 45,211 samples. This scale is modest by the standards of modern tabular problems. In order to better understand the performance of TuneTables on extremely large datasets, we curate from [51] a novel benchmark, LARGESCALETABLES , consisting of 29 datasets with up to 1 900 000 samples, and 7200 features. We curate the datasets from OpenML, omitting image classification datasets. Since TabPFN generally outperforms boosted trees on these smaller datasets, and TuneTables extends TabPFN, we also heuristically select smaller datasets to LARGESCALETABLES so as not to favor either TabPFN or boosted trees. TuneTables achieves the highest average accuracy of any algorithm, and achieves the best performance on poker-hand, a dataset of size 1 025 009. The complete results can be found in Appendix Table 8 and Figure 3 (right). In order to assess the performance of TuneTables on datasets with many classes, we curate another subset of [51], presenting results on fifteen datasets with up to 100 classes. In appendix Table 7 we show that despite the large divergence from the PFN\u2019s pretraining, TuneTables achieves a mean rank of 2.52, ahead of CatBoost and a ResNet, second only to XGBoost, whose mean rank is 2.0. ", "page_idx": 5}, {"type": "text", "text": "Runtime comparison. We divide our consideration of runtime into inference and training. At inference time, TuneTables is around 9x faster than TabPFNs3000, on average; see Appendix Table 10 for the per-dataset details. However, the end-to-end runtime of TuneTables is over $7\\mathbf{x}$ that of CatBoost and XGBoost, and also slower than TabPFNs3000, because of the increased training time. ", "page_idx": 5}, {"type": "text", "text": "In order to better understand the trade-off between accuracy and runtime, we introduce efficient variants of our method. TuneTables-medium utilizes a more efficient adaptive sequence size (i.e., the number of real data points received at train time) which scales with the size of the dataset, validates on a subset of the available validation set when the validation set is large, employs lower patience for early stopping, omits a zero-shot TabPFN grid search over 30 random seeds which TuneTablesstandard uses to find more optimal feature selection subsets, and, most impactfully, omits ensembling for datasets larger than a cutoff hyperparameter (which we fix at 150 000 samples). TuneTables-light includes all of the efficiency modifications of TuneTables-medium, trains for just one epoch, and uses TabPFN zero-shot to preselect the feature selection method rather than performing a grid search using TuneTables itself. In Table 2, we compare these lighter methods to TuneTables-standard on the LARGESCALETABLES benchmark. TuneTables-medium decreases runtime by $72\\%$ compared to TuneTables-standard, while the accuracy decreases by less than a quarter of a percent. Furthermore, the runtime of TuneTables-light is comparable to CatBoost; however, performance also degrades by $5\\%$ when going from TuneTables-medium to TuneTables-light. Still, TuneTables-light shows stronger performance than TabPFNs3000 $78.7\\%$ vs. $78.1\\%$ accuracy) while having a lower inference time. ", "page_idx": 5}, {"type": "table", "img_path": "FOfU3qhcIG/tmp/eec41e00f8ff8431747c1cb8489b79543c2e5804c6edac4cf92d7a05e8970236.jpg", "table_caption": ["Table 1: TuneTables matches SOTA algorithms on 98 datasets. In this table, we compare algorithms over the 98 datasets in the TabZilla benchmark suite from [51]. For each algorithm, we compute its mean accuracy and mean rank in terms of accuracy. We also compute the mean Z-score, computed by normalizing the set of results on each dataset (by mean 0 std. 1), so that each dataset has the same weight, and averaging each algorithm\u2019s normalized performances. Std. Z-Score is computed with respect to random splits and averaged across datasets. Num. wins values are averaged over three splits per dataset. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "FOfU3qhcIG/tmp/474d581df42d9c99f4ea63ba99f59890f5873dc2844f87b2358baf43de35f145.jpg", "table_caption": ["Table 2: TuneTables-medium and TuneTables-light are substantially faster with only a modest decrease in accuracy. We compare the average accuracy and runtime (in seconds) of three versions of TuneTables and find that the medium and light versions of the algorithm are substantially faster on large datasets; we also find that TuneTables-medium sacrifices little accuracy. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Ablations. In order to better understand the significance of the changes we introduce in our method, we separately ablate the tuned prompt size and the use of ensembles in Table 12 and Table 13, finding that smaller datasets pair well with smaller prompts and rarely benefit from ensembling. ", "page_idx": 6}, {"type": "text", "text": "We also compare TuneTables with and without keeping the real data as additional context (referred to as C and NC, respectively, in Section 3); see Appendix Table 12. We find that smaller datasets cannot always be fully learned by the tuned prompt, but for larger datasets, the tuned prompt alone suffices, and in some cases even outperforms the real data. ", "page_idx": 6}, {"type": "text", "text": "Table 3: TuneTables significantly improves accuracy and demographic parity. In these multiobjective optimization experiments, we consider prompt tuning for mitigating predictive bias, comparing TabPFN to TuneTables, tuning for accuracy alone vs. accuracy and demographic parity. TuneTables improves over TabPFN with respect to both objectives. ", "page_idx": 7}, {"type": "table", "img_path": "FOfU3qhcIG/tmp/e1328fc2201b50cdc57bc9087261b4d5903a0f8a9ab4342ab52e3a6599ebeb15.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Sketching and feature selection Finally, in Appendix Table 6 we give a study on the three sketching and three feature selection techniques described in Section 2. As described earlier, TabPFNs3000\u2019s performance when relying on feature selection plateaus well before approaching parity with CatBoost, a top-performing GBDT, on seven very large datasets. ", "page_idx": 7}, {"type": "text", "text": "5 TuneTables Extensions ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Mitigating bias with prompt tuning. Many real-world applications of machine learning involve a set of protected attributes (such as race or gender) that partition the dataset into groups, in which some have higher model performance than others. Removing the sensitive attributes does not fix the algorithmic bias, because the sensitive attributes are often non-trivially correlated with other attributes in the dataset. Due to this issue, researchers have put in significant effort into mitigating the bias of ML models, with the majority of techniques devising new training strategies [9, 53]. ", "page_idx": 7}, {"type": "text", "text": "Given TabPFN\u2019s pretrained nature and considerable retraining cost, the only options for mitigating biased predictions are to run a postprocessing routine on the output predictions, which generally do not perform as well as inprocessing strategies [66]. We show how to use prompt tuning to substantially reduce the bias of predictions while also improving accuracy. ", "page_idx": 7}, {"type": "text", "text": "We conduct experiments on four datasets widely used for research in fairness: the Adult Census Income database (with sex as the sensitive attribute) [8], speed dating (with same race as the sensitive attribute) [83], COMPAS (with sex as the sensitive attribute) [5], and National Longitudinal Survey (with gender as the sensitive attribute) [73]. To quantify bias, we use demographic parity [25, 76], which measures the difference in probability of a positive outcome among the protected and unprotected groups. Formally, given protected group $G_{1}$ , unprotected group $G_{0}$ , and protected attribute $x._{,a}$ , it can be computed as ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "FOfU3qhcIG/tmp/269b3f132307aacebd6b101fc13f7ad8712b8c90f597ba8ba3888cf1fbd96c04.jpg", "img_caption": ["Figure 4: Dataset with high accuracies from just two datapoints. Shown is a two-example prompt dataset for the breast cancer dataset [78]. Malign class example has higher values for all features than benign class. "], "img_footnote": [], "page_idx": 7}, {"type": "equation", "text": "$$\nP_{(x_{i},y_{i})\\in G_{0}}(y_{i}=1\\mid x_{i,a})-P_{(x_{i},y_{i})\\in G_{1}}(y_{i}=1\\mid x_{i,a}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "During prompt tuning, we employ a demographic parity regularizer that aims to minimize the difference in positive outcome probabilities between the two groups: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left|\\sum_{(x_{i},y_{i})\\in G_{0}}P(y_{i}=1\\mid x_{i,a})-\\sum_{(x_{i},y_{i})\\in G_{1}}P(y_{i}=1\\mid x_{i,a})\\right|\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We use the same experimental setup as in Section 4, except that we report one shift rather than the average of three, and TuneTables is fitted to a single prompt rather than ensembled. We compare the default TabPFN to TuneTables, fine-tuning for accuracy alone vs. accuracy and demographic parity. The latter significantly improves demographic parity, compared to the default TabPFN and TuneTables fine-tuned for accuracy, and enhances accuracy relative to the default TabPFN across all datasets but one; see Table 3. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Summarizing and understanding datasets with prompt tuning. While we have demonstrated that TuneTables scales and improves the performance of PFNs, now we show that it can also help understand the discriminative features in a dataset. Often, besides a good predictive model for a dataset, users want to gain further insights into the dataset. In prompt tuning, the tuned smaller dataset can be seen as a summary of the complete dataset that emphasizes discriminative features for the given task. As an example, in Figure 4 we show that on the Breast Cancer dataset [78], a prompt with just two synthetic examples is enough to reach high accuracies and at the same time allows an understanding of the predictive features. For example, in the Breast Cancer dataset, the malign example has higher values for all features compared to the benign example, suggesting high feature values indicate malignancy. We give further examples in Appendix F. ", "page_idx": 8}, {"type": "text", "text": "6 Related work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Neural Processes and Prior-Data Fitted Networks. Prior-data Fitted Networks (PFNs) [35, 55] are a recently-proposed paradigm for machine learning, which show that fast approximate Bayesian inference is possible by training a neural network to mimic the posterior predictive distribution (PPD) in a single forward pass using in-context learning [54, 55, 58]. PFNs were shown to yield state-of-the-art empirical performance on small tabular datasets [35, 51]. PFNs have been used in other applications, including Bayesian optimization [54] learning curve extrapolation [1], and as foundation models for hypernetworks [57]. ", "page_idx": 8}, {"type": "text", "text": "PFNs are neural processes (NPs) [55]. Recent advances in NP are similar in nature to our work, especially recent works that aim to scale attention-based methods to larger contexts. Feng et al. [27] propose Latent Bottlenecked Attentive Neural Processes (LBANPs), a transformer-based neural process which overcomes the quadratic complexity of transformers by encoding the context dataset into a constant number of latent vectors. Guo et al. [34] propose Versatile Neural Processes (VNP), which increases the capability of NPs to handle compex signals by using a new bottleneck encoder. [63] introduces semi-parametric inducing point networks (SPIN), which can attend to a training set at inference time with linear complexity via inducing point methods. [28] introduce Constant Memory Attention Block (CMAB), an attention block that is permutation-invariant and has constant memory complexity when computing the output, as well as Constant Memory Attentive Neural Processes (CMANPs), a NP that requires constant memory. For additional related work, see Appendix B. ", "page_idx": 8}, {"type": "text", "text": "7 Conclusions, limitations, and future work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this work, we gave the first investigation into context optimization techniques for PFNs, allowing us to substantially improve their performance when scaled to large datasets. In particular, we introduced TuneTables, which uses a novel prompt-tuning technique to achieve strong performance on large datasets. We demonstrate that TuneTables mitigates the constraints of TabPFN on the dataset size, number of features, and number of class labels. Additionally, we use prompt tuning to mitigate bias without retraining TabPFN and as an interpretability tool. We open-source our code, results, and all other materials needed to reproduce our work. As PFN models improve, the context optimization techniques explored in our work will allow researchers to further optimize and scale. For example, a next-generation TabPFN might have a longer total context length, and prompt tuning will allow us to push the dataset size even further. ", "page_idx": 8}, {"type": "text", "text": "Limitations. No current TuneTables is on par with GBDTs in terms of both accuracy and runtime simultaneously. We therefore emphasize that while we achieve strong performance metrics, we do not claim practical superiority of our method over gradient boosting, when taking into account training time. However, given the novelty of our method, we expect future research to further improve the accuracy-runtime tradeoff of TuneTables. It also does not improve on TabPFN for small datasets (fewer than 1000 samples, 100 features and 10 classes); we postulate that this is a result of overftiting. ", "page_idx": 8}, {"type": "text", "text": "Future work. Parameter-efficient fine-tuning can be used with PFNs to ensure high-quality, differentially private predictions, given that the pretraining is done on purely synthetic data, and prompt tuning only updates a small number of parameters. Low-rank adaptation (LoRA) [37] and quantized low-rank adaptation (QLoRA) [23] have been used successfully for large language models and would be a promising technique for parameter-efficient fine-tuning of PFNs. Designing a sparse mixture-of-experts PFN using router networks is another promising technique, due to its success in the field of large language models [39]. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Steven Adriaensen, Herilalaina Rakotoarison, Samuel M\u00fcller, and Frank Hutter. Efficient bayesian learning curve extrapolation using prior-data fitted networks. In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS), 2023.   \n[2] Pankaj K Agarwal, Sariel Har-Peled, Kasturi R Varadarajan, et al. Geometric approximation via coresets. Combinatorial and computational geometry, 2005.   \n[3] Mohiuddin Ahmed, Raihan Seraj, and Syed Mohammed Shamsul Islam. The k-means algorithm: A comprehensive survey and performance evaluation. Electronics, 2020.   \n[4] Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: A next-generation hyperparameter optimization framework. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pages 2623\u20132631, 2019.   \n[5] Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. Machine bias. In Ethics of data and analytics, pages 254\u2013264. Auerbach Publications, 2022.   \n[6] Sercan \u00d6 Arik and Tomas Pfister. Tabnet: Attentive interpretable tabular learning. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2021.   \n[7] Kumar Arun, Garg Ishan, and Kaur Sanmeet. Loan approval prediction based on machine learning approach. IOSR J. Comput. Eng, 18(3):18\u201321, 2016.   \n[8] Arthur Asuncion and David Newman. Uci machine learning repository, 2007.   \n[9] Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness and machine learning: Limitations and opportunities. MIT Press, 2023.   \n[10] Vadim Borisov, Tobias Leemann, Kathrin Se\u00dfler, Johannes Haug, Martin Pawelczyk, and Gjergji Kasneci. Deep neural networks and tabular data: A survey. arXiv preprint arXiv:2110.01889, 2021.   \n[11] Anna L Buczak and Erhan Guven. A survey of data mining and machine learning methods for cyber security intrusion detection. IEEE Communications surveys & tutorials, 18(2):1153\u20131176, 2015.   \n[12] Varun Chandola, Arindam Banerjee, and Vipin Kumar. Anomaly detection: A survey. ACM computing surveys (CSUR), 41(3):1\u201358, 2009.   \n[13] Girish Chandrashekar and Ferat Sahin. A survey on feature selection methods. Computers & Electrical Engineering, 2014.   \n[14] Jintai Chen, Kuanlun Liao, Yao Wan, Danny Z Chen, and Jian Wu. Danets: Deep abstract networks for tabular data classification and regression. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2022.   \n[15] Jintai Chen, Jiahuan Yan, Qiyuan Chen, Danny Ziyi Chen, Jian Wu, and Jimeng Sun. Excelformer: A neural network surpassing gbdts on tabular data, 2024.   \n[16] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, pages 785\u2013794, 2016.   \n[17] Valeriia Cherepanova, Roman Levin, Gowthami Somepalli, Jonas Geiping, C Bayan Bruss, Andrew Gordon Wilson, Tom Goldstein, and Micah Goldblum. A performance-driven benchmark for feature selection in tabular deep learning. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023.   \n[18] Jillian M Clements, Di Xu, Nooshin Yousef,i and Dmitry Efimov. Sequential deep learning for credit risk monitoring with tabular financial data. arXiv preprint arXiv:2012.15330, 2020.   \n[19] William Jay Conover. Practical nonparametric statistics, volume 350. john wiley & sons, 1999.   \n[20] Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning, 1995.   \n[21] Thomas Cover and Peter Hart. Nearest neighbor pattern classification. IEEE transactions on information theory, 1967.   \n[22] David R Cox. The regression analysis of binary sequences. Journal of the Royal Statistical Society: Series B (Methodological), 1958.   \n[23] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS), 2023.   \n[24] Samuel Dooley, Gurnoor Singh Khurana, Chirag Mohapatra, Siddartha Venkat Naidu, and Colin White. Forecastpfn: Synthetically-trained zero-shot forecasting. In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS), 2023.   \n[25] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In Proceedings of the 3rd innovations in theoretical computer science conference, 2012.   \n[26] Y. Eldar, M. Lindenbaum, M. Porat, and Y.Y. Zeevi. The farthest point strategy for progressive image sampling. In Proceedings of the 12th IAPR International Conference on Pattern Recognition, Vol. 2 - Conference B: Computer Vision and Image Processing. (Cat. No.94CH3440-5), pages 93\u201397 vol.3, 1994.   \n[27] Leo Feng, Hossein Hajimirsadeghi, Yoshua Bengio, and Mohamed Osama Ahmed. Latent bottlenecked attentive neural processes. In The Eleventh International Conference on Learning Representations, 2023.   \n[28] Leo Feng, Frederick Tung, Hossein Hajimirsadeghi, Yoshua Bengio, and Mohamed Osama Ahmed. Memory efficient neural processes via constant memory attention block. OpenReview, 2023.   \n[29] Jerome H Friedman. Greedy function approximation: a gradient boosting machine. Annals of statistics, pages 1189\u20131232, 2001.   \n[30] Milton Friedman. The use of ranks to avoid the assumption of normality implicit in the analysis of variance. Journal of the american statistical association, 1937.   \n[31] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko. Revisiting deep learning models for tabular data. Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS), 2021.   \n[32] Leo Grinsztajn, Edouard Oyallon, and Gael Varoquaux. Why do tree-based models still outperform deep learning on typical tabular data? In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022.   \n[33] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. Deepfm: a factorization-machine based neural network for ctr prediction. In IJCAI, 2017.   \n[34] Zongyu Guo, Cuiling Lan, Zhizheng Zhang, Yan Lu, and Zhibo Chen. Versatile neural processes for learning implicit neural representations. In The Eleventh International Conference on Learning Representations, 2023.   \n[35] Noah Hollmann, Samuel M\u00fcller, Katharina Eggensperger, and Frank Hutter. Tabpfn: A transformer that solves small tabular classification problems in a second. In Proceedings of the International Conference on Learning Representations (ICLR), 2023.   \n[36] Sture Holm. A simple sequentially rejective multiple test procedure. Scandinavian journal of statistics, pages 65\u201370, 1979.   \n[37] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In Proceedings of the International Conference on Learning Representations (ICLR), 2022.   \n[38] Xin Huang, Ashish Khetan, Milan Cvitkovic, and Zohar Karnin. Tabtransformer: Tabular data modeling using contextual embeddings. arXiv preprint arXiv:2012.06678, 2020.   \n[39] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.   \n[40] Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what language models know? Transactions of the Association for Computational Linguistics, 2020.   \n[41] Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii, a freely accessible critical care database. Scientific data, 3(1):1\u20139, 2016.   \n[42] Arlind Kadra, Marius Lindauer, Frank Hutter, and Josif Grabocka. Well-tuned simple nets excel on tabular datasets. Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS), 34, 2021.   \n[43] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. Lightgbm: A highly efficient gradient boosting decision tree. In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS), 2017.   \n[44] Polina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson. Last layer re-training is sufficient for robustness to spurious correlations. arXiv preprint arXiv:2204.02937, 2022.   \n[45] Phuong Quynh Le, J\u00f6rg Schl\u00f6tterer, and Christin Seifert. Is last layer re-training truly sufficient for robustness to spurious correlations? arXiv preprint arXiv:2308.00473, 2023.   \n[46] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021.   \n[47] Roman Levin, Valeriia Cherepanova, Avi Schwarzschild, Arpit Bansal, C Bayan Bruss, Tom Goldstein, Andrew Gordon Wilson, and Micah Goldblum. Transfer learning with deep tabular models. ICLR, 2023.   \n[48] Andy Liaw, Matthew Wiener, et al. Classification and regression by randomforest. R news, 2(3):18\u201322, 2002.   \n[49] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 2023.   \n[50] Renqian Luo, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu. Neural architecture optimization. In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS), 2018.   \n[51] Duncan McElfresh, Sujay Khandagale, Jonathan Valverde, Ganesh Ramakrishnan, Vishak Prasad, Micah Goldblum, and Colin White. When do neural nets outperform boosted trees on tabular data? In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS), 2023.   \n[52] H Brendan McMahan, Gary Holt, David Sculley, Michael Young, Dietmar Ebner, Julian Grady, Lan Nie, Todd Phillips, Eugene Davydov, Daniel Golovin, et al. Ad click prediction: a view from the trenches. In Proceedings of the Annual Conference on Knowledge Discovery and Data Mining (KDD), pages 1222\u20131230, 2013.   \n[53] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A survey on bias and fairness in machine learning. ACM computing surveys (CSUR), 2021.   \n[54] Samuel M\u00fcller, Matthias Feurer, Noah Hollmann, and Frank Hutter. Pfns4bo: In-context learning for bayesian optimization. In Proceedings of the International Conference on Machine Learning (ICML), 2023.   \n[55] Samuel M\u00fcller, Noah Hollmann, Sebastian Pineda Arango, Josif Grabocka, and Frank Hutter. Transformers can do bayesian inference. In Proceedings of the International Conference on Learning Representations (ICLR), 2022.   \n[56] Alexander Munteanu and Chris Schwiegelshohn. Coresets-methods and history: A theoreticians design pattern for approximation and streaming algorithms. KI-K\u00fcnstliche Intelligenz, 2018.   \n[57] Andreas M\u00fcller, Carlo Curino, and Raghu Ramakrishnan. Mothernet: A foundational hypernetwork for tabular classification, 2023.   \n[58] Thomas Nagler. Statistical foundations of prior-data fitted networks. In Proceedings of the International Conference on Machine Learning (ICML), 2023.   \n[59] Fabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikitlearn: Machine learning in python. the Journal of machine Learning research, 12:2825\u20132830, 2011.   \n[60] Sergei Popov, Stanislav Morozov, and Artem Babenko. Neural oblivious decision ensembles for deep learning on tabular data. In Proceedings of the International Conference on Learning Representations (ICLR), 2020.   \n[61] Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush, and Andrey Gulin. Catboost: unbiased boosting with categorical features. Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS), 2018.   \n[62] J. Ross Quinlan. Induction of decision trees. Machine learning, 1986.   \n[63] Richa Rastogi, Yair Schiff, Alon Hacohen, Zhaozhi Li, Ian Lee, Yuntian Deng, Mert R Sabuncu, and Volodymyr Kuleshov. Semi-parametric inducing point networks and neural processes. In The Eleventh International Conference on Learning Representations, 2023.   \n[64] Matthew Richardson, Ewa Dominowska, and Robert Ragno. Predicting clicks: estimating the click-through rate for new ads. In Proceedings of the 16th international conference on World Wide Web, pages 521\u2013530, 2007.   \n[65] Ivan Rubachev, Artem Alekberov, Yury Gorishniy, and Artem Babenko. Revisiting pretraining objectives for tabular deep learning. arXiv preprint arXiv:2207.03208, 2022.   \n[66] Yash Savani, Colin White, and Naveen Sundar Govindarajulu. Intra-processing methods for debiasing neural networks. Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS), 2020.   \n[67] Ravid Shwartz-Ziv and Amitai Armon. Tabular data: Deep learning is not all you need. Information Fusion, 81:84\u201390, 2022.   \n[68] Jack W Smith, James E Everhart, WC Dickson, William C Knowler, and Robert Scott Johannes. Using the adap learning algorithm to forecast the onset of diabetes mellitus. In Proceedings of the annual symposium on computer application in medical care, page 261. American Medical Informatics Association, 1988.   \n[69] Gowthami Somepalli, Micah Goldblum, Avi Schwarzschild, C Bayan Bruss, and Tom Goldstein. Saint: Improved neural networks for tabular data via row attention and contrastive pre-training. arXiv preprint arXiv:2106.01342, 2021.   \n[70] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multimodal few-shot learning with frozen language models. Advances in Neural Information Processing Systems, 34:200\u2013212, 2021.   \n[71] Dennis Ulmer, Lotta Meijerink, and Giovanni Cin\u00e0. Trust issues: Uncertainty estimation does not enable reliable ood detection on medical tabular data. In Machine Learning for Health, pages 341\u2013354. PMLR, 2020.   \n[72] Christopher J Urban and Kathleen M Gates. Deep learning: A primer for psychologists. Psychological Methods, 2021.   \n[73] US Bureau of Labor Statistics. National longitudinal surveys of youth data set, 2023.   \n[74] Joaquin Vanschoren, Jan N Van Rijn, Bernd Bischl, and Luis Torgo. Openml: networked science in machine learning. ACM SIGKDD Explorations Newsletter, 2014.   \n[75] Jorge R Vergara and Pablo A Est\u00e9vez. A review of feature selection methods based on mutual information. Neural computing and applications, 24:175\u2013186, 2014.   \n[76] Sahil Verma and Julia Rubin. Fairness definitions explained. In Proceedings of the international workshop on software fairness, pages 1\u20137, 2018.   \n[77] Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. Universal adversarial triggers for attacking and analyzing nlp. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 2019.   \n[78] William Wolberg, Olvi Mangasarian, Nick Street, and W. Street. Breast Cancer Wisconsin (Diagnostic). UCI Machine Learning Repository, 1995. DOI: https://doi.org/10.24432/C5DW2B.   \n[79] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. In International Conference on Learning Representations, 2022.   \n[80] Yutaro Yamada, Ofir Lindenbaum, Sahand Negahban, and Yuval Kluger. Feature selection using stochastic gates. In Proceedings of the International Conference on Machine Learning (ICML), 2020.   \n[81] Jinsung Yoon, Yao Zhang, James Jordon, and Mihaela van der Schaar. Vime: Extending the success of self-and semi-supervised learning to tabular domain. Advances in Neural Information Processing Systems, 33:11033\u201311043, 2020.   \n[82] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023.   \n[83] Yong Zheng, Tanaya Dave, Neha Mishra, and Harshit Kumar. Fairness in reciprocal recommendations: A speed-dating study. In Adjunct publication of the 26th conference on user modeling, adaptation and personalization, 2018.   \n[84] Zexuan Zhong, Dan Friedman, and Danqi Chen. Factual probing is [mask]: Learning vs. learning to recall. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2021.   \n[85] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.   \n[86] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. International Journal of Computer Vision, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Broader Societal Impact Statement ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The goal of our work is to investigate context optimization strategies for PFNs, such as prompt-tuning and sketching, which we use to scale and improve the performance of TabPFN. We do not see any negative broader societal impacts of our work that do not already exist in other classification methods. In fact, our work may further facilitate the adoption of TabPFN, which has the benefit of being pretrained and therefore having a lower carbon footprint compared to most deep learning approaches that must be trained from scratch. ", "page_idx": 14}, {"type": "text", "text": "Furthermore, we demonstrate that our prompt-tuning strategy makes it possible to mitigate the bias of TabPFN while only fine-tuning a small set of embedding vectors; the authors of TabPFN mentioned that it is critical to study and improve TabPFN under the lens of algorithmic fairness and other dimensions of trustworthy AI [35]. This may allow practitioners to use TabPFN in sensitive settings for the first time, in which the original TabPFN would lead to biased preditions. Overall, our hope is that our work will have a positive impact for both practitioners and researchers, by facilitating the adoption of a model with a low carbon footprint, and by providing the tools to mitigate the bias of PFNs. Likewise, we hope that the possibility to summarize datasets with prompt tuning will add to the toolbox of machine learning practitioners aiming to analyze and interpret their data better, and therefore may have a positive impact on the trustworthieness of machine learning. ", "page_idx": 14}, {"type": "text", "text": "B Additional Related Work ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Tabular classification. Tabular datasets are the oldest and among the most widely used dataset types in machine learning [10, 67]. GBDTs [29] build an ensemble of decision trees, with each tree ftiting the residual of the loss from the previous tree. XGBoost [16] and CatBoost [61] are two of the most widely-used and highest-performing GBDTs. Researchers have also explored many methods based on neural nets [31, 38, 42]. ", "page_idx": 14}, {"type": "text", "text": "There is an active debate in the community on which family of methods perform best on tabular data: neural nets [6, 42, 47, 60, 65] or GBDTs [10, 31, 32, 67], with the exception of small datasets, on which TabPFN performs the best [35, 51]. Finally, sketching and feature selection methods have been extensively studied in prior works [13, 56]. ", "page_idx": 14}, {"type": "text", "text": "Neural Processes and Prior-Data Fitted Networks. Prior-data Fitted Networks (PFNs) [35, 55] are a recently-proposed paradigm for machine learning, which show that fast approximate Bayesian inference is possible by training a neural network to mimic the posterior predictive distribution (PPD) in a single forward pass using in-context learning [54, 55, 58]. PFNs were shown to yield state-of-the-art empirical performance on small tabular datasets [35, 51]. PFNs have been used in other applications, including Bayesian optimization [54], forecasting [24], and learning curve extrapolation [1]. ", "page_idx": 14}, {"type": "text", "text": "PFNs are neural processes (NPs) [55]. Recent advances in NP are similar in nature to our work, especially recent works that aim to scale attention-based methods to larger contexts. Feng et al. [27] propose Latent Bottlenecked Attentive Neural Processes (LBANPs), a transformer-based neural process which overcomes the quadratic complexity of transformers by encoding the context dataset into a constant number of latent vectors. Guo et al. [34] propose Versatile Neural Processes (VNP), which increases the capability of NPs to handle compex signals by using a new bottleneck encoder. [63] introduces semi-parametric inducing point networks (SPIN), which can attend to a training set at inference time with linear complexity via inducing point methods. [28] introduce Constant Memory Attention Block (CMAB), an attention block that is permutation-invariant and has constant memory complexity when computing the output, as well as Constant Memory Attentive Neural Processes (CMANPs), a NP that requires constant memory. ", "page_idx": 14}, {"type": "text", "text": "For additional related work, see Appendix B. ", "page_idx": 14}, {"type": "text", "text": "Prompt tuning. Researchers have extensively studied prompting techniques for large language models (LLMs) [49]. In such methods, one modifies the input into a prompt, and the LLM predicts the subsequent tokens, from which it derives the output predictions. Prompt tuning techniques typically start with a pretrained LLM and use the training data of a downstream task to find the best prompt for this task. \u2018Hard\u2019 prompt tuning involves finding the best natural language prompts using discrete search techniques [40, 77], while \u2018soft\u2019 prompt tuning optimizes a prompt directly in the embedding space of the model [46, 84]. Soft prompt tuning has also been applied to multi-modal models such as vision-language models [70, 85, 86]. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "C Additional experimental details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Hyperparameter settings for baselines. For the baselines, we use the same hyperparameter search space as in prior work [51] (which is itself similar to other works [42]). We choose the best configuration among the default parameter sets and the additional 29 sets, using $\\mathbf{k}$ -fold crossvalidation with 10 folds. We follow the best configurations of [51] with exceptions; for CatBoost and XGBoost, we increase the range of tree depth by a factor of 10, since a high depth is often more appropriate for very large datasets. We find that this improves performance on large datasets, but sometimes harms performance on small datasets, and increases the runtime \u2013 see Appendix G for the raw results. For each dataset, each algorithm is allowed a maximum runtime of 10 hours for the entire search process. For ExcelFormer, we decrease the number of dimensions and depth of the default transformer, as we find that without this modification, it times out on even medium-sized datasets. ", "page_idx": 15}, {"type": "text", "text": "Hyperparameter settings for TuneTables. Consistent with the experimental design in [51], we perform hyperparameter optimization of TuneTables for up to 30 variants; however, we utilize a grid search rather than Optuna. We use a batch size of 1 and no gradient aggregation. The space over which we conduct grid search in TuneTables is conditioned on metadata; number of features, number of samples, number of classes. The feature and class splits are set by the limits of the particular TabPFN checkpoint we optimize. When we reach a leaf node, TuneTables-standard and TuneTables-medium conduct a grid search over a fixed range of configurations. The size of our search space is always $<$ 30, and usually $<10$ . For feature-large datasets, TuneTables-light conducts additional optimization in the feature space prior to grid search. Further details on our search algorithm can be found in the repository associated with the paper. TuneTables hyperparameter settings can be found in chart form in Table 5. ", "page_idx": 15}, {"type": "text", "text": "C.1 Memory Efficiency ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In our experiments, we find that we exceed hardware limitations around 3000 samples with a batch size of 1 when using TabPFN on a GPU with 48GB of VRAM. While FlashAttention, a popular optimization, is memory-linear in sequence length, but it has not yet been implemented in the public release of TabPFN, which relies on an older version of PyTorch. Flash attention may be integrated into a future release, in which case any benefits will carry over to our method. Even so, inference time continues to be quadratic, and there are other overheads on GPU memory. Overall, there remains a considerable need for new algorithmic methods which can be scaled up to the sequence lengths required by large tabular datasets. ", "page_idx": 15}, {"type": "text", "text": "C.2 Feature selection and sketching ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We ablate strategies for both sketching (subsampling) and feature selection for scaling TabPFN. We consider three sketching methods (random, $k$ -means, and CoreSet) and three feature selection methods (random, PCA, mutual information) as described in Section 2. In addition to sketching, we consider two different methods for sampling class labels: equal and proportional. ", "page_idx": 15}, {"type": "text", "text": "For efficient coreset selection, we use a variant of Farthest Point Sampling [26]; after selecting an initial set of $\\scriptstyle{\\mathrm{n}=5}$ random points, we compute the distance of each point in the dataset to the set of already selected points. Next, we add to the set of selected points the point whose distance to the selected points is maximal. Finally, we update the distances of all the points according to the updated selected set; and continue iteratively. ", "page_idx": 15}, {"type": "text", "text": "We limit our algorithmic comparison to TabPFN to CatBoost, which is the overall best-performing model in [51]. We compare all combinations of sketching and feature selection with both CatBoost and TabPFN; see Table 6. Interestingly, we find that random sketching performs just as well as the more involved algorithms, $k$ -means and CoreSet. On the other hand, PCA significantly outperforms mutual information and random feature selection methods, when the original number of features is large. ", "page_idx": 15}, {"type": "image", "img_path": "FOfU3qhcIG/tmp/17810a0f7caa91834c7e09d2f6ec023f4c3f813963e7d0d0e3a45bb1f173430e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 5: TuneTables is competitive with state-of-the-art tabular models. A critical difference plot according to mean accuracy rank across all LARGESCALETABLES datasets with fewer than 50 000 samples. Algorithms which are not significantly different $(p>0.05)$ are connected with a horizontal black bar. TuneTables achieves the highest mean rank of any algorithm. This plot is similar to Figure 2, but the search spaces for XGBoost and CatBoost are expanded to include more trees. ", "page_idx": 16}, {"type": "image", "img_path": "FOfU3qhcIG/tmp/f9a9e741c1f1ba77385809b01b1d35a53150bba11cb73ace1b07a751e0370f7f.jpg", "img_caption": ["Figure 6: We present a critical difference plot according to mean accuracy rank on LARGESCALETABLES . Algorithms which are not significantly different $(p>0.05)$ are connected with a horizontal black bar. Across these datasets, TuneTables achieves performance that is not significantly different from CatBoost or XGBoost. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "C.3 Additional results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "First, we present the large class table from Section 4. See Table 7. ", "page_idx": 16}, {"type": "text", "text": "In Table 1, we gave aggregate statistics for TuneTables and baselines across all datasets of size less than 50 000. Now, in Table 8, we present results for TuneTables, CatBoost, and XGBoost on all datasets individually, including datasets of size nearly two million. Note that we report the end-to-end runtime of all methods, including 30 iterations of search (for example, TabPFNs3000 takes about $30\\times$ more time than TabPFN as reported in prior work [51]). In Figure 6, we present a critical difference diagram similar to Figure 2 but over all 29 datasets. Across these 29 datasets, TuneTables achieves performance that is not statistically different from CatBoost or XGBoost. ", "page_idx": 16}, {"type": "text", "text": "Neural net comparison. In Table 1 and Table 8, we compared TuneTables to GBDTs and two other neural nets. Now, we compare TuneTables to two additional neural nets: MLP and TabTransformer [38] (for four total neural net baselines). Since transformer-based methods have a higher memory consumption, we obtained full results on 17 total datasets. See Table 9. We find that TuneTables substantially outperforms all other neural nets on average across all datasets, achieving the lowest average rank at 1.93, and achieving the highest average Z-Score of 0.85, far above the second-place neural net\u2019s value of 0.18. ", "page_idx": 16}, {"type": "text", "text": "Inference Time We give the full details for inference time; see Table 10. ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Full results for TuneTables-medium and TuneTables-light Recall at the end of Section 4, we introduced TuneTables-medium and TuneTables-light, showing that they can substantially decrease the runtime of the standard TuneTables with just a small reduction in accuracy. While we presented summary results in Table 2, now we present the full results in Table 11. For descriptions of TuneTables variants, see Section 4. ", "page_idx": 16}, {"type": "text", "text": "C.4 Ablation study ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We report our results ablating the core components of our methods: the prompt length, its use during training and inference, the use of ensembles, and the prompt tuning procedure itself (against regular finetuning). Beginning with the prompt length, we see in Table 12 that while some dataset results are not sensitive to the prompt length, others vary significantly, and generally enjoy longer prompts. Ablating the importance of having real-data context during inference (same table) we find that it is important for smaller datasets. On the larger datasets, the variant with no real data at all is better in specific cases. However, having no such data during inference, a model would perform better not having it during training as well. Introducing real data first during inference is usually harmless, but significantly deteriorates the results on specific cases. Fine tuning the entire model is not only significantly more memory-intensive but also underperforms TuneTables in terms of accuracy. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "In Table 13 we show the results of our method with or without ensembles, and with or without context (in training and inference). Our ensembling strategy is beneficial in both cases. When using ensembles, the results with or without context are generally similar, although one variant may outperform the other on specific datasets. ", "page_idx": 17}, {"type": "text", "text": "D TuneTables additional details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Tuned prompt implementation details. We implement our tuned prompts by prepending randomly initialized vectors of identical dimension to the TabPFN encoder to the real data points fed to TabPFN as context during training. ", "page_idx": 17}, {"type": "text", "text": "In the CT (\u2018context in training\u2019) setting, we continue to provide real data points as part of the training context; the quantity of real data provided for each batch is drawn with uniform probability from a random variable which ranges from zero to a fixed upper bound, which is passed to the model as a hyperparameter. Usually that upper bound is 1152, the default setting for TabPFN, unless the dataset is extremely small, in which case we select it according to the limitations of that dataset. ", "page_idx": 17}, {"type": "text", "text": "In the NCT (\u2018no context in training\u2019) setting, no real data is provided as part of the training context; all data in the training set is used to fit the randomly initialized prompt. There are always a fixed number of data points provided, and it is the same for every batch. Usually that number is 128, unless the dataset is extremely small, in which case we select it according to the limitations of that dataset. ", "page_idx": 17}, {"type": "text", "text": "We note that it is also possible to evaluate models trained in either setting either with context (C) or without (NC). In our experiments, we always evaluate all models on both, and report the setting with higher validation accuracy at test time. ", "page_idx": 17}, {"type": "text", "text": "We find that on smaller datasets, the setting with context (C) sometimes outperforms the setting without context (NC), even after the prompt has been ftited, perhaps because the tuned prompt overftis on a small amount of available training data. However, when using ensembling, for datasets with more than 3000 samples in the training set, the NC setting is as good or better than the C setting. See Table 13. We leave further investigation of this phenomenon to future work. ", "page_idx": 17}, {"type": "text", "text": "Loss function. Most TuneTables experiments are optimized using the cross-entropy loss between the labels assigned to the training data (P) and TuneTables\u2019s outputs (Q): ", "page_idx": 17}, {"type": "equation", "text": "$$\nH(P,Q)=-\\sum_{x}P(x)\\log Q(x)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "To reduce the effects of overfitting on datasets with fewer samples, we extend our grid search to include the choice of loss function. We find that many such datasets benefit from fitting the tuned prompt via the KL divergence loss between the PFN\u2019s outputs (P) and TuneTables (Q). ", "page_idx": 17}, {"type": "equation", "text": "$$\nK L(P||Q)=\\sum_{x}P(x)\\log\\left({\\frac{P(x)}{Q(x)}}\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Compute. We conduct all of our prompt tuning experiments on Google Cloud Platform (GCP), using a single NVIDIA L4 TPU with 24GB VRAM for each experiment. ", "page_idx": 17}, {"type": "text", "text": "Size of tuned prompts. We ablate the size of tuned prompts in Table 12 and find that the larger tuned prompts generally perform better on datasets with more samples, while smaller tuned prompts excel on datasets with few samples. For this reason, the experimental results in Section 4 are reported on prompts of size 10 or 1000, conditioned on the number of samples in the input dataset. ", "page_idx": 17}, {"type": "text", "text": "Duration of training. Our experiments run for up to 100 epochs. We employ early stopping if our key metric (Accuracy) fails to improve after a fixed number of epochs. ", "page_idx": 18}, {"type": "text", "text": "Evaluation of tuned prompts. We validate our tuned prompts every epoch on a subset of the entire validation set if the validation set is large. At a fixed interval, we run the entire validation and test sets. After the experiment concludes, we report results from the epoch with the best key metric score. ", "page_idx": 18}, {"type": "text", "text": "Fine-tuned setting. In the fine-tuned setting, our parameters are the same, except that we use a lower fixed learning rate of $1e-3$ . ", "page_idx": 18}, {"type": "text", "text": "Ensembling over tuned prompts. While individual tuned prompts are already a substantial improvement on TabPFN for sample-large datasets, we find that these improvements sometimes compound when we ensemble over multiple tuned prompts. ", "page_idx": 18}, {"type": "text", "text": "We draw the inspiration for our ensembling approach from the internal ensembling used in TabPFN, which averages predictions over permutations of feature indices and label indices[35]. For the results presented in Section 4, we ensemble over ten permutations of each dataset, averaging the top two ensemble member predictions (as measured by NC accuracy on the validation set in the NCT setting, or C accuracy in the CT setting). ", "page_idx": 18}, {"type": "text", "text": "In a TuneTables ensemble, each ensemble member ftis its own tuned prompt to the data. Variance in the ensemble members is introduced by differences in the random initialization of the tuned prompt, as well as permuting the order of features and labels, a la TabPFN, one time before each tuned prompt is fitted. ", "page_idx": 18}, {"type": "text", "text": "See Table 13 for ablation studies of the effectiveness of ensembling. ", "page_idx": 18}, {"type": "text", "text": "E Regression experiments ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we show that using prompt tuning, we are also able to extend PFNs to perform well on regression datasets, further highlighting the flexibility of our approach. ", "page_idx": 18}, {"type": "text", "text": "E.1 Adapting TuneTables to regression problems. ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Adapting TuneTables to regression problems introduces new challenges, as TabPFN was not designed for use with such problems. In particular, we find that end-to-end fine-tuning outperforms prompt tuning in this setting, and that the most effective grid search for regression problems utilizes a space of PFN base models. Put another way, TuneTables-regression does not use prompt tuning, and does not always use TabPFN as its foundation model. Rather, it searches a space of foundation models to find the best performer for a particular regression problem. We search over three foundation models; TabPFN, the checkpoint released in [55] and a new PFN we train from scratch for 10 epochs on a synthetic prior of regression-specific datasets. ", "page_idx": 18}, {"type": "text", "text": "E.2 Details on the datasets used for the experiments and baseline experimental design ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Similar to [51], each algorithm is tuned for each dataset by maximizing the R-squared (R2) metric. Each dataset corresponds to an OpenML task, and can be preprocessed exactly like the classification datasets used in other experiments. The 15 datasets used in these experiments are \u201cBankNote-Authentication- UCI\u201d (OpenML task 361002), \u201cEgyptianSkulls\u201d (5040), \u201cWine\u201d (190420), \u201cWisconsin-breast- cancer-cytology-features\u201d (361003), \u201cbodyfat\u201d (5514), \u201ccalifornia\u201d (361089), \u201cchscase-foot\u201d (5012), \u201ccleveland\u201d (2285), \u201ccolleges\u201d (359942), \u201ccpu-small\u201d (4883), \u201cliver-disorders\u201d (52948), \u201cmeta\u201d (4729), \u201cmv\u201d (4774), \u201cpbc\u201d (4850), and \u201cveteran\u201d (4828). Unlike prior work, we report non-normalized averages, as we believe it gives a more realistic indication of real-world performance at a glance. ", "page_idx": 18}, {"type": "text", "text": "We use 12 deep learning and boosted tree algorithms as baselines; VIME, TabTransformer, TabNet, DANet, STG, MLP, LightGBM, CatBoost, XGBoost, NODE, DeepFM. ", "page_idx": 18}, {"type": "image", "img_path": "FOfU3qhcIG/tmp/a254f3210fd3bd2ca8c50c4b03ee8b635497186a35347d7ccb778fd0d6fd01d8.jpg", "img_caption": ["Figure 7: TuneTables is competitive with state-of-the-art tabular models on regression. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "FOfU3qhcIG/tmp/1eeff4406cc0b4da841b5e1e8cc135017633300e9cf85ad115984e80effee557.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 8: Decision boundaries on 2D datasets. Shown are prompt-tuned TabPFN decision boundaries as well as the regular zero-shot TabPFN decision boundaries. Larger dots in middle row represent the tuned points for the two classes. ", "page_idx": 19}, {"type": "text", "text": "E.3 Results of regression experiments ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In Table 14 results and Figure 7, we show the results of our experiments. Overall, we see that TuneTables is competitive with top-performing algorithms across a variety of metrics; mean win rank, average $R^{2}$ score, etc. Equally importantly, TuneTables is significantly stronger on several datasets than both boosted trees and deep baselines. ", "page_idx": 19}, {"type": "text", "text": "F Summarization details and decision boundaries of prompt-tuned TabPFN on toy 2D Problems ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "For the experiments with prompts of only two examples, we prompt-tune one example per class for 500 epochs. The prompt is input directly into TabPFN without undergoing any preprocessing steps that the original data went through. Therefore, the tuned feature values do not correspond directly to the original values in the dataset. For the breast cancer dataset, it reaches $100\\%$ accuracy; for the diabetes dataset, it reaches $78.7\\%$ accuracy. ", "page_idx": 19}, {"type": "text", "text": "We also show decision boundaries of TabPFN and prompt-tuned TabPFN on toy 2d classification problems using scikit-learn [59] in Fig. 8. The prompt contains four datapoints for each class. One can see how the points are tuned to recover the decision boundaries. Due to the very low prompt length and the low dataset size, the prompt-tuned decision boundaries are slightly less accurate than TabPFN (consistent with our results for small datasets in Table 12). ", "page_idx": 19}, {"type": "text", "text": "G Results for Deeper GBDTs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In the results in Section 4, we compare TuneTables to the GBDTs, XGBoost and CatBoost, using a search space in which the number of trees can range from 1 to 1000. Now, we compare TuneTables to XGBoost and CatBoost when they have expanded search spaces, where the number of trees can range from 1 to 10 000. We re-compute the critical difference plot and table results: see Figure 5, Table 15, and Table 16. ", "page_idx": 19}, {"type": "image", "img_path": "FOfU3qhcIG/tmp/0eef1f278117e48171eb51f1ead7fbb4072b87e9389a58b337d8c5c0e70f6066.jpg", "img_caption": ["Figure 9: Diabetes dataset [68] with high accuracies from just two samples. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "In Figure 5 and Table 15, we see that TuneTables still performs favorably compared to CatBoost and XGBoost on datasets of size up to 50 000. Note that the larger search space may perform better on larger datasets, it is also more challenging to search through in 30 iterations. We present the full results on all datasets in Table 16. ", "page_idx": 20}, {"type": "table", "img_path": "FOfU3qhcIG/tmp/a85750bc0b72b1867da8ebbfb454326fc7ec965e542b78bd44a0a1f3e881dee0.jpg", "table_caption": ["Table 4: List of all datasets used in Section 4 and Appendix C. LARGESCALETABLES datasets are listed in part one. Datasets in part two were used for the class extension experiments in Table 7. Datasets in part three were used for fairness experiments in Section 5. The additional dataset in part four was used for intepretability experiments in Section 5. Datasets in part 5 were used for the ablations on sketching and feature selection in Appendix C. For the list of 98 used in Table 8, see https://github.com/penfever/TuneTables/blob/main/datasets-used.csv. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "FOfU3qhcIG/tmp/837e62fd4d8f83d932a486ca61bf227b695b213cc9ae17891c29376efc96e049.jpg", "table_caption": ["Table 5: TuneTables and TabPFNs3000 hyperparameter configurations based on number of samples. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 6: Comparative performance of TabPFN and CatBoost with sketching, feature selection, and sampling methods. On a distinct subset of the datasets in [51] selected to emphasize datasets with many features or many samples, we compare CatBoost and TabPFNs3000. When both models are limited to 3000 samples, TabPFNs3000 performs better on 12 of 17 datasets where significant differences exist. When CatBoost is allowed access to the entire training data, the win rate is identical. In most cases, random sample selection is sufficient for optimal performance. Both models benefit from PCA and mutual information dimension reduction when the feature space is large. The columns labeled SKT / FTS / SMP list the best performing method for sketching, feature subsampling and label-aware sketching technique, respectively. Label-aware sketching refers to a strategy where we either sample instances proportionate to their labels, or we oversample minority classes with replacement to create a class-balanced distribution. While the choice of label-aware sketching strategy is often impactful (and we use it in TuneTables), and the choice of feature subselection method can be important for some datasets, in all but one case, no sketching method we test outperforms random sampling. Bold indicates the best-performing model(s). ", "page_idx": 22}, {"type": "table", "img_path": "FOfU3qhcIG/tmp/a0dcb872a604241c49fda0303d30987f6d45afee81dd2ae14a50fd2f7be9ec14.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "FOfU3qhcIG/tmp/93b1ab4f351c829c7fb6c79d831d269283f52d902e8dfeb36b6af0d7daeec32c.jpg", "table_caption": ["Table 7: Comparison of algorithms on datasets with a large number of classes. TuneTables can effectively handle datasets with more classes than the ones used for pretraining, which was not possible with TabPFN. For each algorithm, we compute its mean test accuracy, and mean rank in terms of accuracy. We also compute the mean $Z_{\\cdot}$ -score, computed by normalizing the set of results on each dataset (by mean 0 std. 1), so that each dataset has the same weight, and averaging each algorithm\u2019s normalized performances. We see that TuneTables performs the best across all performance-oriented metrics. Fractional num. wins values are averaged over three splits per dataset, and reflect the presence of multi-way ties on certain splits. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Table 8: Comparison of the top-performing methods on LARGESCALETABLES . For each algorithm, we show its test accuracy and end-to-end runtime in seconds. In this table, Table 11 and Table 16, we report the summed runtime for all random seeds (30 in the case of all algorithms except for TuneTables, which generally requires fewer than 30). Since TuneTables uses TabPFN predictions on small datasets, we conservatively report its end to end runtime as the sum of TabPFNs3000 runtime and TuneTables grid search runtime. This prediction is also extremely conservative for TabPFNs3000, as 30 random seeds are unnecessary on datasets with fewer than 100 features. We report runtime in this manner to mitigate concerns that we are advantaging PFNs over boosted trees. We also show the number of samples in each dataset. All datasets above the line were included in Table 1. ", "page_idx": 23}, {"type": "table", "img_path": "FOfU3qhcIG/tmp/1f525286db62f3f73c28785071c8fca48d0eb88e769e65e2994f9330caab629a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "FOfU3qhcIG/tmp/1c4842569ce94b3a1bb9a972787607635bf01eb945b6a8ab1daf577459762ba7.jpg", "table_caption": ["Table 9: Comparison of neural nets on LARGESCALETABLES . We compare TuneTables to other prominent deep learning methods for tabular data on the 17 datasets in LARGESCALETABLES for which all algorithms reported results. For each algorithm, we compute its different metrics of accuracy and rank. We also compute the mean Z-score, computed by normalizing the set of results on each dataset (by mean 0 std. 1), so that each dataset has the same weight, and averaging each algorithm\u2019s normalized performances. We see that TuneTables performs the best across all performance-oriented metrics. Fractional num. wins values are averaged over three splits per dataset, and reflect the presence of multi-way ties on certain splits. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "FOfU3qhcIG/tmp/6b54676a42fe187b1664c828d2036be6a7fa139d1d9034cfe2573233f6453db8.jpg", "table_caption": ["Table 10: Comparison of inference times across datasets. In this table, we compare the inference time of TabPFNs3000 (which uses up to 3000 points of real data as context, when available) and four common configurations for TuneTables. We report inference time for the entire test set, as well as per 1000 samples, estimated from the inference time for the entire test set. At inference time, TuneTables does not require ensembling to achieve high accuracy, while TabPFNs3000 does. Therefore, TuneTables is considerably faster. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "FOfU3qhcIG/tmp/d230cc2e84174919a8eada87011e483b52062a9c07b65cb3241d426847a90d2c.jpg", "table_caption": ["Table 11: TuneTables-medium and TuneTables-light are substantially faster with only a modest decrease in accuracy. We compare the average accuracy and runtime in seconds of three versions of TuneTables, across all 19 datasets of size $<50\\mathrm{K}$ , as well as all 29 datasets. All datasets above the line were included in Table 1. "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "FOfU3qhcIG/tmp/73fd3345ba5d55de862d65d356144a05d017aced7d78443baec5b657dd205f79.jpg", "table_caption": ["Table 12: Ablation on TuneTables variants. Comparison of the different variants of our method (TabPFN-PT) and the entire backbone fine-tuning (TabPFN-FT) to TuneTables itself on the LARGESCALETABLES benchmark. TuneTables is our full method described in Section 3. For each variant, we show its test accuracy. For all methods in this table except for TuneTables, there is no hyperparameter optimization (HPO). For TuneTables, we use our standard grid search. "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "FOfU3qhcIG/tmp/c07e0deca23739c52b15d3a78b9ff516e26ac964335f8cdca4c98fbece99e5c6.jpg", "table_caption": ["Table 13: Ensembled models outperform models trained on a single tuned prompt; with ensembling, the training and testing without real-data context (NC) setting matches or exceeds the setting with context (C). Runs with prompt tuned only once are noted as TabPFN-PT, and ensembles of such runs as TabPFN-PT-Ens. Although the improvements are often quite small, we find that ensembles generally outperform single tuned prompts in both C and NC settings. We also find that ensembles trained without additional real data context at train time or test time are as good or better than ensembles trained and tested with real data context on datasets larger than 3000 samples. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Table 14: TuneTables is competitive with strong baselines on a range of regression datasets. In this table, we compare 7 algorithms on 15regression datasets from [51]. The terminology and methods of this table follow Table 1, except that we report mean $R^{2}$ score instead of mean accuracy as our primary statistic. TuneTables ties with XGBoost for the number of wins, has the highest average $R^{2}$ score of any algorithm, and the second-highest mean rank after LightGBM. ", "page_idx": 27}, {"type": "table", "img_path": "FOfU3qhcIG/tmp/6eb6c9e8c5c5db11f24a7ba3934f25b67844a726ae071624a3043cea53af6827.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Table 15: TuneTables matches SOTA algorithms on small and medium-sized datasets. In this table, we compare algorithms over 19 datasets in LARGESCALETABLES with at most 50 000 samples. For each algorithm, we compute its mean accuracy, mean runtime, and mean rank in terms of accuracy. We also compute the mean Z-score, computed by normalizing the set of results on each dataset (by mean 0 std. 1), so that each dataset has the same weight, and averaging each algorithm\u2019s normalized performances. Std. Z-Score is computed with respect to random splits and averaged across datasets. Fractional num. wins values are averaged over three splits per dataset, and reflect the presence of multi-way ties on certain splits. This table is similar to Table 1, but the benchmark is LARGESCALETABLES , and the search spaces for XGBoost and CatBoost are expanded to include more trees. ", "page_idx": 28}, {"type": "table", "img_path": "FOfU3qhcIG/tmp/d59405ea5fef88ce90f4ac49032f8d913fbe93ad82ab2b693811ca9d1b1cd76d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "FOfU3qhcIG/tmp/3569543cbffc80e650990aee1f341f00029492d95f33b114049a53ee22c5bbb2.jpg", "table_caption": ["Table 16: Comparison of top-performing methods on the LARGESCALETABLES benchmark. For each algorithm, we show its test accuracy and runtime (seconds). We also show the number of samples in each dataset. All datasets above the line were included in Table 15. This table is similar to Table 8, but the search spaces for XGBoost and CatBoost are expanded to include more trees. "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our abstract and introduction both state the main claims in the paper. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We discuss the limitations of our work in Section 7. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our work does not include theoretical results. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Section 4 and Appendix C lay out all information needed to reproduce the experimental results. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We do release all code and materials needed to reproduce our results. We provided this link in the introduction: https://anonymous.4open.science/r/ TuneTables. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Section 4 and Appendix C specify all experimental settings and details. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Our results in Section 4 do include standard deviations, and we also include critical difference plots in order to compute the statistical significance of our results. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Section 4 does provide information on the compute resources needed to reproduce the experiments. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper conforms to the NeurIPS Code of Ethics. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We discuss the societal impacts in Appendix A. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: As discussed in Appendix A, our paper does not pose such risks. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We credit the original owners of the resources we use in the supplementary material. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 33}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We release code which is documented with a readme and comments. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]