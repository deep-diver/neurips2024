[{"type": "text", "text": "eXponential FAmily Dynamical Systems (XFADS): Large-scale nonlinear Gaussian state-space modeling ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Matthew Dowling Champalimaud Research, Champalimaud Foundation, Portugal matthew.dowling@research.fchampalimaud.org ", "page_idx": 0}, {"type": "text", "text": "Yuan Zhao National Institute of Mental Health, USA yuan.zhao@nih.gov ", "page_idx": 0}, {"type": "text", "text": "Il Memming Park Champalimaud Research, Champalimaud Foundation, Portugal memming.park@research.fchampalimaud.org ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "State-space graphical models and the variational autoencoder framework provide a principled apparatus for learning dynamical systems from data. State-of-the-art probabilistic approaches are often able to scale to large problems at the cost of flexibility of the variational posterior or expressivity of the dynamics model. However, those consolidations can be detrimental if the ultimate goal is to learn a generative model capable of explaining the spatiotemporal structure of the data and making accurate forecasts. We introduce a low-rank structured variational autoencoding framework for nonlinear Gaussian state-space graphical models capable of capturing dense covariance structures that are important for learning dynamical systems with predictive capabilities. Our inference algorithm exploits the covariance structures that arise naturally from sample based approximate Gaussian message passing and low-rank amortized posterior updates \u2013 effectively performing approximate variational smoothing with time complexity scaling linearly in the state dimensionality. In comparisons with other deep state-space model architectures our approach consistently demonstrates the ability to learn a more predictive generative model. Furthermore, when applied to neural physiological recordings, our approach is able to learn a dynamical system capable of forecasting population spiking and behavioral correlates from a small portion of single trials. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "State-space models (SSM) are invaluable for understanding the temporal structure of complex natural phenomena through their underlying dynamics1\u20133. While engineering or physics problems often assume the dynamical laws of the system of interest are known to a high degree of accuracy, in an unsupervised data-driven investigation, they have to be learned from the observed data. The variational autoencoder (VAE) framework makes it possible to jointly learn the parameters of the state-space description and an inference network to amortize posterior computation of the unknown latent state4\u20137. However, it can be challenging to structure the variational approximation and design an inference network that permits fast evaluation of the loss function (evidence lower bound or ELBO) while preserving the temporal structure of the posterior. ", "page_idx": 0}, {"type": "text", "text": "In this work, we develop a structured variational approximation, approximate ELBO, and inference network architecture for generative models specified by nonlinear dynamical systems with Gaussian state noise. Our main contributions are as follows, ", "page_idx": 1}, {"type": "text", "text": "(i) A structured amortized variational approximation that combines the prior dynamics with low-rank data updates to parameterize Gaussian distributions with dense covariance matrices,   \n(ii) Conceptualizing the approximate smoothing problem as an approximate filtering problem for pseudo-observations that encode a representation of current and future data, and,   \n(iii) An inference algorithm that scales $\\mathcal{O}(\\dot{T L}(S r+S^{2}+r^{2}))$ \u2013 made possible by exploiting the low-rank structure of the amortization network as well as Monte Carlo integration of the latent state through the dynamics. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "State-space models are probabilistic graphical models where observations $\\mathbf{y}_{t}$ in discrete time are conditionally independent given a continuous latent state, $\\mathbf{z}_{t}$ , evolving according to Markovian dynamics, so that the complete data likelihood for $T$ consecutive observations factorizes as, ", "page_idx": 1}, {"type": "equation", "text": "$$\np(\\mathbf{y}_{1:T},\\mathbf{z}_{1:T})=p_{\\theta}(\\mathbf{z}_{1})\\,p_{\\psi}(\\mathbf{y}_{1}\\,|\\,\\mathbf{z}_{1})\\times\\prod_{t=2}^{T}p_{\\psi}(\\mathbf{y}_{t}\\,|\\,\\mathbf{z}_{t})\\,p_{\\theta}(\\mathbf{z}_{t}\\,|\\,\\mathbf{z}_{t-1})\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\mathbf{z}_{t}\\in\\mathbb{R}^{L}$ are real-valued latent states, $\\pmb{\\theta}$ parameterizes the dynamics and initial condition, and $\\psi$ parameterizes the observation model. When the generative model, $(\\theta,\\psi)$ , is known, the statistical inference problem is to compute the smoothing posterior, $p(\\mathbf{z}_{1:T}\\,|\\,\\mathbf{y}_{1:T})^{\\,2}$ . Otherwise, $(\\theta,\\psi)$ have to be learned from the data \u2013 known as system identification8. ", "page_idx": 1}, {"type": "text", "text": "Variational inference makes it possible to accomplish these goals in a harmonious way. The variational expectation maximization (vEM) algorithm iterates two steps: first, we maximize a lower bound to the log-marginal likelihood \u2013 the ELBO \u2013 with respect to the parameters of an approximate posterior, $q(\\overline{{\\mathbf{z}_{1:T}}})\\approx\\bar{p}(\\mathbf{z}_{1:T}\\,|\\,\\mathbf{y}_{1:T})$ ; then, with the approximate posterior fixed, the ELBO is maximized with respect to parameters of the generative model9. For large scale problems, vEM can be slow due to the need to fully optimize the variational parameters before taking gradient steps on parameters of the generative model. Therefore, the variational autoencoder (VAE) is better suited for large scale problems for its ability to simultaneous learn the generative model and inference network \u2013 an expressive parametric function that maps data to the parameters of approximate posterior10,11. ", "page_idx": 1}, {"type": "text", "text": "Model specifications. Although our approach is applicable to any exponential family state-space process, given their ubiquity, we focus on dynamical systems driven by Gaussian noise so that, ", "page_idx": 1}, {"type": "equation", "text": "$$\np_{\\boldsymbol{\\theta}}(\\mathbf{z}_{t}\\,|\\,\\mathbf{z}_{t-1})=\\mathcal{N}(\\mathbf{z}_{t}\\,|\\,\\mathbf{m}_{\\boldsymbol{\\theta}}(\\mathbf{z}_{t-1}),\\mathbf{Q}_{\\boldsymbol{\\theta}})\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\mathbf{m}_{\\theta}:\\mathbb{R}^{L}\\rightarrow\\mathbb{R}^{L}$ might be a nonlinear neural network function with learnable parameters $\\pmb{\\theta}$ , and $\\mathbf{Q}_{\\theta}\\in\\mathbb{R}^{L\\times L}$ is a learnable state-noise covariance matrix. Given the favorable properties of exponential family distributions12\u201315, especially in the context of variational inference, we write the prior dynamics in their exponential family representation (natural parameter form), ", "page_idx": 1}, {"type": "equation", "text": "$$\np_{\\theta}(\\mathbf{z}_{t}\\vert\\mathbf{z}_{t-1})=h(\\mathbf{z}_{t})\\exp\\left(\\mathcal{T}(\\mathbf{z}_{t})^{\\top}\\lambda_{\\theta}(\\mathbf{z}_{t-1})-A(\\lambda_{\\theta}(\\mathbf{z}_{t-1}))\\right)\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $h$ is the base measure, ${\\mathcal{T}}(\\mathbf{z}_{t})$ the sufficient statistics, $A(\\cdot)$ the log-partition function, and $\\lambda_{\\theta}(\\cdot)$ is a map $\\mathbb{R}^{L}\\mapsto\\mathbb{R}^{L^{2}+L}$ that transforms $\\mathbf{Z}_{t-1}$ to natural parameters for $\\mathbf{z}_{t}$ . For a Gaussian distribution, the sufficient statistics can be defined as $\\begin{array}{r l}{\\dot{T}(\\mathbf{\\bar{z}}_{t})^{\\top}=\\left[\\mathbf{z}_{t}^{\\top}\\right.}&{{}\\left.-\\frac{1}{2}\\mathbf{z}_{t}\\mathbf{z}_{t}^{\\top}\\right]}\\end{array}$ , so that $\\lambda_{\\theta}(\\cdot)$ for (1) is given by, ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\lambda_{\\theta}(\\mathbf{z}_{t-1})=\\binom{\\mathbf{Q}_{\\theta}^{-1}\\mathbf{m}_{\\theta}(\\mathbf{z}_{t-1})}{\\mathbf{Q}_{\\theta}^{-1}}\\qquad\\mathrm{(dynamics~model~in~natural~paramter~form)}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "As it will simplify subsequent analysis, the mean parameter mapping corresponding to this natural parameter mapping (guaranteed to exist as long as the exponential family is minimal ) is given by ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mu_{\\theta}(\\mathbf{z}_{t-1})=\\mathbb{E}_{p_{\\theta}(\\mathbf{z}_{t}|\\mathbf{z}_{t-1})}\\left[\\mathcal{T}(\\mathbf{z}_{t})\\right]=\\left[-\\frac{\\mathbf{m}_{\\theta}(\\mathbf{z}_{t-1})}{2}\\mathbf{m}_{\\theta}(\\mathbf{z}_{t-1})\\mathbf{m}_{\\theta}(\\mathbf{z}_{t-1})^{\\top}+\\mathbf{Q}_{\\theta})\\right]\\qquad\\left(\\mathrm{paraneter}\\right).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Furthermore, we make the following assumptions i) the state-noise, $\\mathbf{Q}_{\\theta}$ , is diagonal or structured for efficient matrix-vector multiplications. ii) $\\mathbf{m}_{\\theta}(\\cdot)$ , is a nonlinear smooth function. ii) the likelihood, $p_{\\psi}(\\mathbf{y}_{t}\\,|\\,\\mathbf{z}_{t})$ , may be non-conjugate. iv) $L$ may be large enough so that $L^{3}$ is comparable to $T$ . ", "page_idx": 1}, {"type": "text", "text": "Amortized inference for state-space models. A useful property of SSMs is that, $\\mathbf{z}_{t}$ conditioned on $\\mathbf{Z}_{t-1}$ and $\\mathbf{y}_{t:T}$ , is independent of $\\mathbf{y}_{1:t-1}$ , i.e., $p(\\mathbf{z}_{t}\\,|\\,\\mathbf{z}_{t-1},\\mathbf{\\bar{y}}_{1:T})=p(\\mathbf{z}_{t}\\,|\\,\\mathbf{z}_{t-1},\\mathbf{y}_{t:T})^{5,16},$ . It thus suffices to construct an approximate posterior that factorizes forward in time, ", "page_idx": 2}, {"type": "equation", "text": "$$\nq(\\mathbf{z}_{1:T})=q(\\mathbf{z}_{1})\\prod q(\\mathbf{z}_{t}\\,|\\,\\mathbf{z}_{t-1})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "and introduce learnable function approximators to amortize inference by mapping $\\mathbf{z}_{t-1}$ and $\\mathbf{y}_{t:T}$ to the parameters of $q\\big(\\mathbf{z}_{t}\\,|\\,\\mathbf{z}_{t-1}\\big)$ . This makes it simple to sample $\\mathbf{z}_{1:T}$ from the approximate posterior (using the reparameterization trick) and evaluate the ELBO (a.k.a. negative variational free energy), ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}(q)=\\sum\\mathbb{E}_{q_{t}}\\left[\\log p(\\mathbf{y}_{t}\\left|\\mathbf{z}_{t})\\right]-\\mathbb{E}_{q_{t-1}}\\left[\\mathbb{D}_{\\mathrm{KL}}(q(\\mathbf{z}_{t}\\left|\\mathbf{z}_{t-1}\\right)\\right|\\left|\\,p_{\\theta}(\\mathbf{z}_{t}\\left|\\mathbf{z}_{t-1}\\right))\\right]\\leq\\log p(\\mathbf{y}_{1:T})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbb{D}_{\\mathrm{KL}}(\\cdot||\\cdot)$ is the Kullback-Leibler (KL) divergence and $\\mathbb{E}_{q_{t}}\\equiv\\mathbb{E}_{\\mathbf{z}_{t}\\sim q(\\mathbf{z}_{t};\\mathbf{y}_{1:T})}$ , so that the generative model and inference network parameters can be learned through stochastic backpropagation. Many works for Gaussian $q(\\mathbf{z}_{t}\\,|\\,\\mathbf{z}_{t-1}\\bar{})$ , such as Krishnan et al. 6, Alaa and van der Schaar 17, Girin et al. 18, Hafner et al. 19, construct inference networks that parameterize the variational posterior as ", "page_idx": 2}, {"type": "equation", "text": "$$\nq(\\mathbf{z}_{t}\\,|\\,\\mathbf{z}_{t-1})=\\mathcal{N}(\\mathbf{m}_{\\phi}(\\mathbf{z}_{t-1},\\mathbf{y}_{1:T}),\\mathbf{P}_{\\phi}(\\mathbf{z}_{t-1},\\mathbf{y}_{1:T})).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "There are limitless ways to construct $\\mathbf{m}_{\\phi}(\\cdot)$ and $\\mathbf{P}_{\\phi}(\\cdot)$ so $\\phi$ can be learned through gradient ascent on the ELBO, but a straightforward and illustrative approach6,17 is to transform future data, $\\mathbf{y}_{t:T}$ , using a recurrent neural network (RNN), or any efficient autoregressive sequence to sequence model, and then mapping the preceding latent state, $\\mathbf{Z}_{t-1}$ , using a feed-forward neural network, so that a complete inference network description could be, ", "page_idx": 2}, {"type": "equation", "text": "$$\n(\\mathbf{m}_{\\phi}(\\mathbf{z}_{t-1},\\mathbf{y}_{t:T}),\\mathbf{P}_{\\phi}(\\mathbf{z}_{t-1},\\mathbf{y}_{t:T}))=\\mathrm{NN}([\\mathbf{z}_{t-1},\\mathbf{u}_{t}]),\\qquad\\quad\\mathbf{u}_{t}=\\mathrm{S2S}([\\mathbf{u}_{t+1},\\mathbf{y}_{t}])\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathrm{S2S(\\cdot)}$ is a parametric sequence-to-sequence function that maintains a hidden state $\\mathbf{u}_{t}$ and takes as input $\\mathbf{y}_{t}$ , and $\\mathrm{{NN}}(\\cdot)$ is a parametric function designed to output approximate posterior parameters. This leads to a backward-forward algorithm, meaning that data $\\mathbf{y}_{1:T}$ are mapped to $\\mathbf{u}_{1:T}$ in reverse time, and then samples are drawn from $q\\!\\left(\\mathbf{z}_{t}\\left|\\mathbf{z}_{t-1}\\right)$ forward in time. ", "page_idx": 2}, {"type": "text", "text": "Possible drawbacks of this inference framework are i) missing observations obstruct inference (the example networks cannot naturally accommodate missing data); ii) sampling entire trajectories to approximate the expected KL term can potentially lead to high-variance gradient estimators, and iii) statistics of the marginals (e.g. second moments) can only be approximated through sample averages. ", "page_idx": 2}, {"type": "text", "text": "3 Related works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Many existing works also explore inference and data-driven learning for state-space graphical models within the VAE framework. We highlight the most closely related studies and note specific limitations that our work seeks to address. The structured variational autoencoder (SVAE)20 makes it possible to efficiently evaluate the ELBO while preserving the temporal structure of the posterior by restricting the prior to a linear dynamical system (LDS) and then constructing the approximation as $\\begin{array}{r}{q(\\mathbf{z}_{1:T})\\,\\,\\bar{\\propto}\\,p_{\\theta}(\\mathbf{\\dot{z}}_{1:T})\\prod\\exp(t(\\mathbf{z}_{t})^{\\top}\\boldsymbol{\\psi}(\\mathbf{y}_{t}))}\\end{array}$ so that its statistics can be obtained using efficient message passing algorithms. However, the SVAE is not directly applicable when the dynamics are nonlinear since the joint prior will no longer be Gaussian (thereby not allowing for efficient conjugate updates). Recently, Zhao and Linderman 2 1 expanded on the SVAE framework by exploiting the LDS structure and associative scan operations to improve its scalability. ", "page_idx": 2}, {"type": "text", "text": "The deep Kalman filter $(\\mathrm{DKF})^{6}$ uses black-box inference networks to make drawing joint samples from the full posterior simple. However, pure black-box amortization networks such as those can make learning the parameters of the generative model dynamics difficult because their gradients will not propagate through the expected log-likelihood term5. In contrast, we consider inference networks inspired by the fundamental importance of the prior for evaluating Bayesian conjugate updates. The deep variational Bayes filter (DVBF) also considers inference and learning in state-space graphical models5. Difficulties of learning the generative model that arise as a result of more standard VAE implementations defining inference networks independent of the prior are handled by forcing samples from the approximate posterior to traverse through the dynamics. Our work extends this concept, by directly specifying the parameters of the variational approximation in terms of the prior. ", "page_idx": 2}, {"type": "text", "text": "Our approach constructs an inference network infused with the prior similar to the SVAE and DVBF but i) avoids restrictions to LDS and ii) affords easy access to approximations of the marginal statistics (such as the dense latent state covariances) without having to average over sampled trajectories (or store them directly which would be prohibitive as the latent dimensionality becomes large). ", "page_idx": 2}, {"type": "text", "text": "4 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "An alternative to constructing variational approximations through specification of conditional distributions, as in Eq. (7), involves the use of data-dependent Gaussian potentials, that we refer to as pseudo-observations: ", "page_idx": 3}, {"type": "equation", "text": "$$\np(\\tilde{\\mathbf{y}}_{t}\\mid\\mathbf{z}_{t})\\propto\\exp(\\tilde{\\lambda}_{\\phi}(\\mathbf{y}_{1:T})^{\\top}\\mathcal{T}(\\mathbf{z}_{t}))\\equiv\\exp(\\tilde{\\lambda}_{t}^{\\top}\\mathcal{T}(\\mathbf{z}_{t}))=\\exp\\left(\\mathbf{k}_{t}^{\\top}\\mathbf{z}_{t}-\\frac{1}{2}||\\mathbf{K}_{t}\\mathbf{z}_{t}||^{2}\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "These Gaussian potentials can then be combined with the prior through Bayes\u2019 rule, yielding the approximation ", "page_idx": 3}, {"type": "equation", "text": "$$\nq(\\mathbf{z}_{1:T})=\\frac{\\prod p(\\tilde{\\mathbf{y}}_{t}\\,|\\,\\mathbf{z}_{t})p_{\\theta}(\\mathbf{z}_{1:T})}{p(\\tilde{\\mathbf{y}}_{1:T})}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "A benefit of this formulation, is that it inherently imposes the latent dependency structure of the generative model onto the amortized posterior. This parameterization, was introduced in Johnson et al. 20, where an important point highlighted, is that the Gaussian potentials can encode any arbitrary subset of observations; for example, $p\\big(\\tilde{\\mathbf{y}}_{t}\\,|\\,\\mathbf{z}_{t}\\big)$ could be made to depend on $\\mathbf{y}_{t}$ alone, or even the entire dataset, $\\mathbf{y}_{1:T}$ . Regardless of that particular design choice, the corresponding ELBO for the variational approximation of Eq. (10) is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}(q)=\\sum\\mathbb{E}_{q_{t}}\\left[\\log p(\\mathbf{y}_{t}\\,|\\,\\mathbf{z}_{t})\\right]-\\mathbb{E}_{q_{t}}\\left[\\log p(\\tilde{\\mathbf{y}}_{t}\\,|\\,\\mathbf{z}_{t})\\right]+\\log p(\\tilde{\\mathbf{y}}_{1:T})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For linear Gaussian latent dynamics, conjugate potentials could be efficiently integrated with the prior using exact message passing, yielding flitered and smoothed marginal statistics. The smoothed statistics can be used to evaluate the first two terms on the right-hand side, while the flitered statistics can be used to evaluate the final term, the log-marginal likelihood of the pseudo-observations. ", "page_idx": 3}, {"type": "text", "text": "However, this approach does not directly apply to nonlinear dynamical systems, where the variational posterior is no longer Gaussian. Since evaluating the smoothed marginals and the log-marginal likelihood of pseudo-observations relies on first obtaining the filtered marginals, a logical starting point is to develop a method for approximating these filtered marginals. To this end, we propose a differentiable approximate message passing algorithm specifically designed to compute filtered posterior statistics in models characterized by nonlinear latent dynamics and observations represented as Gaussian potentials. Building on this foundation, we then return to the subsequent challenges of efficiently computing smoothed posterior statistics and evaluating the ELBO. ", "page_idx": 3}, {"type": "text", "text": "Differentiable nonlinear filtering. Bayesian filtering is often conceptualized as a two step procedure2. In the predict step, our belief of the latent state is integrated through the dynamics, yielding $\\begin{array}{r}{q(\\mathbf{z}_{t}\\mid\\tilde{\\mathbf{y}}_{1:t-1}\\mid=\\int p_{\\theta}(\\bar{\\mathbf{z}_{t}}\\mid\\mathbf{z}_{t-1})q(\\mathbf{z}_{t-1}\\mid\\tilde{\\mathbf{y}}_{1:t-1})\\,\\mathrm{d}\\mathbf{z}_{t-1}}\\end{array}$ (a.k.a. the predictive distribution). Then, applying Bayes\u2019 rule, $q(\\mathbf{z}_{t}\\,|\\,\\tilde{\\mathbf{y}}_{1:t})\\propto p(\\tilde{\\mathbf{y}}_{t}\\,|\\,\\mathbf{z}_{t})q(\\mathbf{z}_{t}\\,|\\,\\tilde{\\mathbf{y}}_{1:t-1})$ , we update our belief. Evidently then, developing an approximate filtering algorithm that exploits the conjugacy of the pseudo observations can be recast as the problem: given an approximation $\\pi(\\mathbf{z}_{t-1})\\approx\\dot{q}(\\mathbf{z}_{t-1}\\mid\\mathbf{\\bar{y}}_{1:t-1})$ , find an approximation to the predictive distribution, $\\bar{\\pi}(\\mathbf{\\bar{z}}_{t})\\approx q(\\mathbf{z}_{t}\\,|\\,\\tilde{\\mathbf{y}}_{1:t-1})$ . The recursion would continue forward by updating our belief analytically, setting $\\pi(\\mathbf{z}_{t})\\propto p(\\tilde{\\mathbf{y}}_{t}\\,|\\,\\mathbf{z}_{t})\\bar{\\pi}(\\mathbf{z}_{t})$ , then finding a Gaussian approximation of $\\bar{\\pi}(\\mathbf{z}_{t+1})$ and so forth. ", "page_idx": 3}, {"type": "text", "text": "With the problem restated this way, we propose an approximate flitering solution designed to exploit two key factors at play i) the approximate beliefs are constrained to the same exponential family as the latent state transitions ii) the pseudo observations are encoded as conjugate potentials. Our approach involves recursively solving intermediary variational problems (their fixed point solutions on the right), ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\pi}(\\mathbf{z}_{t})=\\operatorname{argmin}\\,\\mathbb{D}_{\\mathrm{KL}}\\!\\left(\\mathbb{E}_{\\pi_{t-1}}\\left[p_{\\theta}(\\mathbf{z}_{t}\\,|\\,\\mathbf{z}_{t-1})\\right]\\right|\\bar{\\pi}(\\mathbf{z}_{t})\\right)\\,\\Longrightarrow\\,\\,\\bar{\\mu}_{t}=\\mathbb{E}_{\\pi_{t-1}}\\left[\\mu_{\\theta}(\\mathbf{z}_{t-1})\\right]}\\\\ &{\\pi(\\mathbf{z}_{t})=\\operatorname{argmin}\\,\\mathbb{D}_{\\mathrm{KL}}(\\pi(\\mathbf{z}_{t})||\\,p(\\tilde{\\mathbf{y}}_{t}\\,|\\,\\mathbf{z}_{t})\\bar{\\pi}(\\mathbf{z}_{t}))\\,\\Longrightarrow\\,\\,\\lambda_{t}=\\bar{\\lambda}_{t}+\\tilde{\\lambda}_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Steps (i) and (ii) can be thought of as variational analogues of the predict/update steps of Bayesian filtering, and importantly, finding their fixed point solutions does not require an iterative procedure because of our problem specifications. Reassuringly, iterating (i) and (ii) in the case of an LDS generative model would exactly recover the information form Kalman flitering equations. In the case of nonlinear dynamical systems, directly taking the expectation in (i) is intractable. We can overcome this by employing the reparameterization trick to obtain a differentiable sample approximation of the parameters, $\\Bar{\\pmb{\\mu}}_{t}$ , of the fixed point solution. Naturally now, the statistics of the approximate filtered beliefs can be used to approximate the log-marginal likelihood of the pseudo observations as, ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\log p(\\tilde{\\mathbf{y}}_{1:T})=\\sum\\log\\int p(\\tilde{\\mathbf{y}}_{t}\\,|\\,\\mathbf{z}_{t})q(\\mathbf{z}_{t}\\,|\\,\\tilde{\\mathbf{y}}_{1:t-1})\\,\\mathrm{d}\\mathbf{z}_{t}\\approx\\sum\\log\\int p(\\tilde{\\mathbf{y}}_{t}\\,|\\,\\mathbf{z}_{t})\\bar{\\pi}(\\mathbf{z}_{t})\\,\\mathrm{d}\\mathbf{z}_{t}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the last integral can be evaluated analytically as a result of the Gaussian forms of the approximations and pseudo observations. While formulating step (ii) as a variational problem may appear superfluous given the conjugate structure, it hints at the possibility of approximating smoothed posterior marginal statistics within a variational framework. However, pursuing this idea further reveals significant challenges. Trying to develop a backward recursion for the distribution $q_{t}$ minimizing $\\ \\overline{{\\mathbb{D}}}_{\\mathrm{KL}}\\big(\\mathbb{E}_{q_{t+1}}[q_{t|t+1}]\\big|\\,\\big|\\,q_{t}\\big)$ , by using the forward KL divergence (as in step (i)), leads to an intractable problem because the backward Markov transitions, $q_{t|t+1}$ , are not conditionally Gaussian. Conversely, a fixed point solution of the reverse KL objective (as in step(ii)), $\\mathbb{D}_{\\mathrm{KL}}\\bigl(q_{t}\\,\\big|\\,\\big|\\,\\mathbb{E}_{q_{t+1}}\\bar{[}q_{t\\,|\\,t+1}\\big]\\bigr)$ , necessitates an iterative procedure, which can be computationally expensive. ", "page_idx": 4}, {"type": "text", "text": "Smoothing as filtering. In light of these difficulties, we offer a simple solution that exploits the flexibility in choosing the pseudo observation data dependence: define the parameters, $\\mathbf{k}_{t}$ and $\\mathbf{K}_{t}$ , of each pseudo observation, $\\tilde{\\mathbf{y}}_{t}$ , to be a function of current and future data, $\\mathbf{y}_{t:T}$ , so that, ", "page_idx": 4}, {"type": "equation", "text": "$$\np(\\tilde{\\mathbf{y}}_{t}\\,|\\,\\mathbf{z}_{t})\\propto\\exp(\\tilde{\\lambda}_{\\phi}(\\mathbf{y}_{t:T})^{\\top}T(\\mathbf{z}_{t}))\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "With this choice, filtered statistics of the latent state\u2014relative to the pseudo-observations\u2014can be used to approximate posterior smoothed marginals, i.e. $\\pi(\\mathbf{z}_{t})\\approx q(\\mathbf{z}_{t}\\mid\\tilde{\\mathbf{y}}_{1:t})\\approx p(\\mathbf{z}_{t}\\mid\\mathbf{y}_{1:T})$ . This solution circumvents the challenges associated with backward message computation and only requires a single pass through the pseudo observations to obtain approximate smoothed posterior statistics. Substituting, $\\pi_{t}$ , as an approximation to $q_{t}$ , in Eq. (11), leads to the following approximation of the ELBO. ", "page_idx": 4}, {"type": "text", "text": "Variational smoothing ELBO ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\mathcal{L}}(\\pi)=\\displaystyle\\sum\\mathbb{E}_{\\pi_{t}}\\left[\\log p(\\mathbf{y}_{t}\\,|\\,\\mathbf{z}_{t})\\right]-\\mathbb{E}_{\\pi_{t}}\\left[\\log p(\\tilde{\\mathbf{y}}_{t}\\,|\\,\\mathbf{z}_{t})\\right]+\\log\\mathbb{E}_{\\bar{\\pi}_{t}}\\left[p(\\tilde{\\mathbf{y}}_{t}\\,|\\,\\mathbf{z}_{t})\\right]}\\\\ &{\\quad\\quad\\quad=\\displaystyle\\sum\\mathbb{E}_{\\pi_{t}}\\left[\\log p(\\mathbf{y}_{t}\\,|\\,\\mathbf{z}_{t})\\right]-\\mathbb{D}_{\\mathrm{KL}}(\\pi(\\mathbf{z}_{t})|\\,|\\,\\bar{\\pi}(\\mathbf{z}_{t}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "By expressing the approximate ELBO compactly as Eq.(17), we highlight that it promotes learning models where the posterior at time $t$ aligns closely with the one-step posterior predictive at that time (which depends on the generative model and the posterior at time $t-1)$ ). The KL term in Eq.(17) can be evaluated in closed form, while the expected log-likelihood term can be approximated using the reparameterization trick4. However, a point of practical importance should be raised now: every filtering step and evaluation of the KL term has a time complexity of $\\mathcal{O}(L^{3})$ , which may become a bottleneck for large $L$ . In the following discussion, we will explore effective strategies to parameterize the Gaussian potential inference network that produces $\\tilde{\\lambda}_{1:T}$ , to reduce the computational burden that filtering and evaluating $\\hat{\\mathcal{L}}(\\pi)$ pose in the large $L$ regime. ", "page_idx": 4}, {"type": "text", "text": "Local and backward encoders. For state-space models, inferences about the latent state should be possible even with missing observations. To enable the amortized inference network to process missing observations in a principled way, we decompose the natural parameter update into two additive components: i) a local encoder, $\\alpha_{\\phi}(\\cdot)$ , for current observation, and ii) a backward encoder, $\\beta_{\\phi}(\\cdot)$ , for future observations, i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\lambda}_{\\phi}(\\mathbf{y}_{t:T})=\\alpha_{\\phi}(\\mathbf{y}_{t})+\\beta_{\\phi}(\\mathbf{y}_{t+1:T})\\quad(\\mathrm{or~for~the~sake~of~brevity})\\quad\\tilde{\\lambda}_{t}=\\alpha_{t}+\\beta_{t+1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Furthermore, by building the dependence of $\\beta_{\\phi}(\\cdot)$ on $\\mathbf{y}_{t+1:T}$ through their representation as $\\alpha_{t+1:T}$ , so that $\\begin{array}{r}{\\beta_{\\phi}(\\mathbf{y}_{t+1:T})=\\bar{\\beta}_{\\phi}(\\bar{\\alpha}_{t+1:T})}\\end{array}$ , a missing observation at time $t$ is handled by setting $\\alpha_{t}=\\mathbf{0}$ While a data dependent natural parameter update of 0 faithfully represents a missing observation \u2013 in the absence of data, the prior should not be updated \u2013 alternatively setting ${\\bf y}_{t}={\\bf0}$ would introduce a harmful inductive bias into the inference network, since an observation of 0 can be arbitrarily informative. Given the impracticality of $\\mathcal{O}(T L^{2})$ memory requirements, it is appealing to consider a low-rank parameterization for the local and backward encoders \u2013 we consider ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\alpha_{t}=\\left(\\mathbf{A}_{t}\\mathbf{A}_{t}^{\\top}\\right):=\\left(\\mathbf{A}(\\mathbf{y}_{t})\\mathbf{A}(\\mathbf{y}_{t})^{\\top}\\right)\\qquad\\beta_{t}=\\left(\\mathbf{b}_{t}\\mathbf{B}_{t}^{\\top}\\right):=\\left(\\mathbf{B}(\\alpha_{t:T})\\mathbf{B}(\\alpha_{t:T})^{\\top}\\right)\\qquad\\beta_{t}=\\left(\\mathbf{b}_{t}\\mathbf{B}_{t}^{\\top}\\right)^{\\top},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "image", "img_path": "Ln8ogihZ2S/tmp/bcae9c432684fa440ab02d0bf1ee8be1d7b7829f73cb361e48bc909309011da6.jpg", "img_caption": ["Figure 1: Smoothing and predictive performance on bouncing ball and pendulum. To the left of the red line are samples from the posterior during the data window projected to image space, to the right of the red line are samples unrolled from $p_{\\theta}(\\mathbf{z}_{t}\\mid\\mathbf{z}_{t-1})$ . a) while all methods are adept at smoothing in the context window, our methods predictive performance is better by a noticeable margin as measured by the $R^{2}$ . b) similar results hold for the bouncing ball dataset. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{A}_{t}\\in\\mathbb{R}^{L\\times r_{\\alpha}}$ with $\\mathbf{B}_{t}\\in\\mathbb{R}^{L\\times r_{\\beta}}$ parameterize low-rank local/backward precision updates, and $\\mathbf{a}_{t}\\in\\mathbb{R}^{L}$ with $\\mathbf{b}_{t}\\in\\mathbb{R}^{L}$ parameterize local/backward precision-scaled mean updates. Using these descriptions and the additive decomposition (18), the parameters of a single pseudo observation are, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tilde{\\lambda}_{t}=\\binom{\\mathbf{k}_{t}}{{\\mathbf{K}_{t}}{\\mathbf{K}_{t}}^{\\top}}:=\\binom{{\\mathbf{k}}({\\mathbf{y}}_{t:T})}{{\\mathbf{K}}({\\mathbf{y}}_{t:T}){\\mathbf{K}}({\\mathbf{y}}_{t:T})^{\\top}}=\\binom{{\\mathbf{a}}_{t}+{\\mathbf{b}}_{t}}{[{\\mathbf{A}}_{t}\\ {\\mathbf{B}}_{t}]\\,[{\\mathbf{A}}_{t}\\ {\\mathbf{B}}_{t}]^{\\top}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{K}\\in\\mathbb{R}^{L\\times r}$ if $\\boldsymbol{r}=\\boldsymbol{r}_{\\alpha}+\\boldsymbol{r}_{\\beta}$ and $\\mathbf{k}\\in\\mathbb{R}^{L}$ . The low-rank structure of the natural parameter updates will be a key component to develop an efficient approximate message passing algorithm for obtaining sufficient statistics of the approximate posterior and evaluating the ELBO. Analogous to the inference network description (8), a differentiable architecture producing $\\alpha_{1:T}$ and $\\beta_{1:T}$ could be, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\alpha_{t}=\\mathrm{NN}(\\mathbf{y}_{t})\\qquad\\qquad\\qquad\\qquad\\beta_{t}=\\mathrm{S2S}([\\beta_{t+1},\\alpha_{t}]),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "which overall defines the map ${\\bf y}_{1:T}\\mapsto(\\alpha_{1:T},\\beta_{1:T})$ . In addition, the separation of local and backward encoders can reduce the complexity of the backward encoder for $L<N$ . Those familiar with sequential Monte-Carlo (SMC) methods22 can view the backward encoder similar to twisting functions used to combine future information with filtered state beliefs to produce smoothing approximations23,24. ", "page_idx": 5}, {"type": "text", "text": "Exploiting structure for efficient flitering. A benefti of using the forward KL to design a variational analogue to the exact Bayesian predict step is immediate access to the fixed point solution. While nonlinear specification of the latent dynamical system make the expectation of Eq. (12) intractable, using the reparameterization trick with $\\mathbf{z}_{t-1}^{s}\\sim\\pi(\\mathbf{z}_{t-1})$ , gives the approximation, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\bar{\\mu}_{t}=1/s\\sum_{s=1}^{S}\\left[{-\\frac{1}{2}\\left(\\mathbf{m}_{\\theta}(\\mathbf{z}_{t-1}^{s})\\mathbf{m}_{\\theta}(\\mathbf{z}_{t-1}^{s})^{\\top}+\\mathbf{Q}_{\\theta}\\right)}\\right]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $S$ is the total number of samples. Converting this finite sample estimate from mean parameter coordinates to a mean/covariance representation, we get that, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{m}}_{t}=1/s\\sum_{s=1}^{S}\\mathbf{m}_{\\theta}(\\mathbf{z}_{t-1}^{s})\\qquad\\qquad\\qquad\\bar{\\mathbf{P}}_{t}=\\bar{\\mathbf{M}}_{t}^{c}\\bar{\\mathbf{M}}_{t}^{c\\top}+\\mathbf{Q}_{\\theta}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\bar{\\mathbf{M}}_{t}^{c}$ is the $L\\times S$ matrix of samples passed through the dynamics function, then centered by the mean, defined for convenience as, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{M}}_{t}^{c}={1}/{\\sqrt{S}}\\left[\\mathbf{m}_{\\theta}(\\mathbf{z}_{t-1}^{1})-\\bar{\\mathbf{m}}_{t},\\cdot\\cdot\\cdot\\mathbf{\\mu},\\mathbf{m}_{\\theta}(\\mathbf{z}_{t-1}^{S})-\\bar{\\mathbf{m}}_{t}\\right]\\in\\mathbb{R}^{L\\times S}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Writing the covariance estimate as it is in Eq. (23), reveals that it can alternatively be represented by the pair $(\\bar{\\mathbf{M}}_{t}^{c},\\mathbf{Q}_{\\theta})$ . In the regime where $L>S$ , significant computational savings can be afforded by capitalizing on the low-rank structure of the covariance as estimated via the reparameterization trick. This structure can be exploited for efficient linear algebraic operations involving $\\bar{\\mathbf{P}}_{t}$ (and its inverse, after application of the Woodbury identity). Consequently, the natural parameters of $\\pi_{t}$ , after updating $\\bar{\\pi}_{t}$ , are ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{P}_{t}^{-1}\\mathbf{m}_{t}=\\bar{\\mathbf{P}}_{t}^{-1}\\bar{\\mathbf{m}}_{t}+\\mathbf{k}_{t}\\qquad\\qquad\\qquad\\mathbf{P}_{t}^{-1}=\\bar{\\mathbf{P}}_{t}^{-1}+\\mathbf{K}_{t}\\mathbf{K}_{t}^{\\top}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "image", "img_path": "Ln8ogihZ2S/tmp/3ca2b6e2af2f3ab5ec715559e4d070febbde1318e6f77e7f6ac16f6d1516b4a6.jpg", "img_caption": ["Figure 2: a) Empirical time complexity scaling. Since complexity is a function of $L,S,$ , and $r$ , we vary $L$ (top) for fixed $r=10$ and (bottom) for fixed $S=5$ ; we examine several values of the variable not fixed. Examining wall-clock time shows empirically our implementation scales linearly in $L$ ; on the (bottom) we plot wall-clock time for a Kalman fliter implementation, showing the standard cubic dependence on $L$ . b) (top) Negative ELBO as a function of training epoch when $N=L$ (bottom) when $N=L/5$ ; the left column shows the case $L=50$ and the right when $L=100$ . Different colors indicate different settings of the local/backward encoder rank; zooming in for $L=100$ , shows low-rank updates can match diagonal ones. c) Peristimulus time histogram (PSTH) for the DMFC RSG dataset for different trial condition averages; we consider a context window of 1.3s and a prediction window of 1.3s. d) BPS for each method for context/prediction windows. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "and also admit structured representations. Without both the sample approximation structure (during the variational predict step) and low-rank parameterization (during the variational update step), the cost of approximate filtering each time-step would be dominated by an $\\mathcal{O}(L^{3})$ cost. Instead, recognizing the potential computational advantages of exploiting these structures, and never instantiating the predictive/updated covariance and precision matrices, makes it possible to develop an approximate filtering algorithm, where in the case $L$ is significantly larger than $S$ or $r$ , has complexity of $\\mathcal{O}(L(S r+r^{2}+S^{2}))$ per step. More details regarding time complexity are in App. B.5. ", "page_idx": 6}, {"type": "text", "text": "Efficient sampling and ELBO evaluation. When $\\mathbb{E}_{\\pi_{t}}\\left[\\log p(\\mathbf{y}_{t}\\,|\\,\\mathbf{z}_{t})\\right]$ can not be evaluated in closed form, Monte-Carlo integration can be used as a differentiable approximation. To sample from $\\pi(\\mathbf{z}_{t})$ without explicitly constructing $\\mathbf{P}_{t}$ , we can take $\\bar{\\mathbf{z}}_{t}^{s}\\sim\\mathcal{N}(\\mathbf{0},\\bar{\\mathbf{P}}_{t})$ and $\\mathbf{w}_{t}^{s}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I}_{L+S})$ and set, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{z}_{t}^{s}=\\mathbf{m}_{t}+\\bar{\\mathbf{z}}_{t}^{s}-\\mathbf{K}_{t}\\mathbf{Y}_{t}\\mathbf{Y}_{t}^{\\top}(\\mathbf{K}_{t}^{\\top}\\bar{\\mathbf{z}}_{t}^{s}+\\mathbf{w}_{t}^{s}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "While more details are provided in App. B.4, this can be done efficiently since samples can be drawn cheaply from $\\bar{\\boldsymbol{\\pi}}(\\mathbf{z}_{t})$ using Eq. (23). Whereas Monte-Carlo approximations of the expected log-likelihood term might be unavoidable, the closed form solution for the KL between two Gaussian distributions should be used to avoid further stochastic approximations. The only difficulty, is that the time complexity of naively evaluating the KL term, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{D}_{\\mathrm{KL}}(\\pi(\\mathbf{z}_{t})||\\,\\bar{\\pi}(\\mathbf{z}_{t}))=\\frac{1}{2}\\big[(\\bar{\\mathbf{m}}_{t}-\\mathbf{m}_{t})^{\\top}\\bar{\\mathbf{P}}_{t}^{-1}(\\bar{\\mathbf{m}}_{t}-\\mathbf{m}_{t})+\\mathrm{tr}(\\bar{\\mathbf{P}}_{t}^{-1}\\mathbf{P}_{t})+\\log(|\\bar{\\mathbf{P}}_{t}|/|\\mathbf{P}_{t}|)-L\\big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "scales $\\mathcal{O}(L^{3})$ . However, since matrix vector multiplies with $\\bar{\\mathbf{P}}_{t}^{-1}$ can be performed efficiently and the trace/log-determinant terms can be rewritten using the square-root factors acquired during the forward pass, as we describe in App. C.1, it is possible to evaluate the KL in $\\mathcal{O}(L S r+L S^{2}+L r^{2})$ time. After a complete forward pass through the encoded data, we acquire the samples $\\mathbf{z}_{1:T}^{1:S}$ and all necessary quantities for efficient ELBO evaluation. We detail the variational filtering algorithm in Alg. 2 in App. C.2 and the complete end-to-end learning procedure in Alg. 1. ", "page_idx": 6}, {"type": "text", "text": "Causal amortized inference for streaming data. In constructing a fully differentiable variational approximation, the parameters of the approximate marginals were effectively amortized according to a recursion in the natural parameter space by iterating Eqs. (12) and (13). This recursion can be recognized more easily by introducing the function, $\\mathfrak{F}_{\\theta}(\\cdot)$ , and writing ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\lambda_{t}=\\mathfrak{F}_{\\theta}(\\lambda_{t-1})+\\alpha_{t}+\\beta_{t+1}\\mathrm{~with~}\\mathfrak{F}_{\\theta}(\\lambda_{t-1})=\\nabla A^{*}\\left(\\int\\pi(\\mathbf{z}_{t-1};\\lambda_{t-1})\\mu_{\\theta}(\\mathbf{z}_{t-1})\\,\\mathrm{d}\\mathbf{z}_{t-1}\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Input: y1:T   \nwhile not converged do for $t=T$ to 1 do $\\alpha_{t}=\\mathrm{NN}(\\mathbf{y}_{t})$ # local encoder $\\beta_{t}=\\mathrm{S}2\\mathrm{S}([\\beta_{t+1}\\ \\alpha_{t}])$ # backward encoder $\\mathbf{k}_{t}=\\mathbf{a}_{t}+\\mathbf{b}_{t}$ $\\mathbf{K}_{t}=\\left[\\mathbf{A}_{t}\\;\\mathbf{B}_{t}\\right]$ end for $\\begin{array}{r l}&{\\mathbf{z}_{1:T}^{1:S},\\mathbf{m}_{1:T},\\bar{\\mathbf{m}}_{1:T},\\mathbf{\\mathcal{T}}_{1:T}=\\mathrm{Alg.\\nabla}2(\\mathbf{k}_{1:T},\\mathbf{K}_{1:T})}\\\\ &{\\hat{\\mathcal{L}}(\\pi)=\\sum[S^{-1}\\sum\\log p(\\mathbf{y}_{t}\\,|\\,\\mathbf{z}_{t}^{s})-\\mathbb{D}_{\\mathrm{KL}}(\\pi_{t}|\\,|\\,\\bar{\\pi}_{t})]}\\\\ &{(\\phi,\\theta,\\psi)\\leftarrow(\\phi,\\theta,\\psi)-\\nabla\\hat{\\mathcal{L}}(\\pi)}\\end{array}$   \nend while   \nOutput: $\\mathbf{z}_{1:T}^{1:S}$ , $\\mathbf{m}_{1:T},\\,\\bar{\\mathbf{m}}_{1:T},\\,\\mathbf{Y}_{1:T}$ ", "page_idx": 7}, {"type": "text", "text": "$\\nabla A^{*}:(\\mathbf{m},-{\\textstyle\\frac{1}{2}}(\\mathbf{P}+\\mathbf{mm}^{\\top}))\\,\\mapsto\\,(\\mathbf{P}^{-1}\\mathbf{m},\\mathbf{P}^{-1}),$ , and $A^{*}(\\cdot)$ is the convex conjugate of the logpartition function12. So that, $\\mathfrak{F}_{\\theta}(\\cdot)$ can be thought of as mapping $\\lambda_{t-1}$ forward in time by first taking the expectation of (4) with respect to $\\pi(\\mathbf{z}_{t-1};\\lambda_{t-1})$ , and then applying the mean-to-natural coordinate transformation. ", "page_idx": 7}, {"type": "text", "text": "One limitation of amortizing inference through the recursion (28) is its inability to produce approximations for the flitering distributions, $p(\\mathbf{z}_{t}\\,|\\,\\mathbf{y}_{1:t})$ , which can be valuable in streaming or online settings, as well as for testing hypotheses of causality. However, since (17) only depends on the posterior and posterior predictive marginal statistics, we have the freedom to alter our inference network in a way such that filtered marginal statistics are a by-product of obtaining smoothed marginal statistics. For example, an alternative sequence-to-sequence map for $\\lambda_{t}$ could be defined by, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lambda_{t}=\\mathfrak{F}_{\\theta}\\big(\\underbrace{\\lambda_{t-1}-\\beta_{t}}_{\\mathrm{~smoothed-future}}+\\alpha_{t}+\\beta_{t+1},}\\\\ {\\mathrm{~smoothed-future}=\\mathrm{filtered}}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "so that $\\breve{\\lambda}_{t}\\equiv\\lambda_{t}-\\beta_{t+1}$ obey the recursion $\\breve{\\lambda}_{t}\\,=\\,\\mathfrak{F}_{\\theta}\\big(\\breve{\\lambda}_{t-1}\\big)+\\alpha_{t}$ and are natural parameters of an approximate filtering distribution, $\\breve{\\pi}(\\mathbf{z}_{t})\\,\\approx\\,p\\big(\\mathbf{z}_{t}\\mid\\breve{\\mathbf{y}}_{1:t}\\big)$ . Consequently the approximations to posterior and predictive distributions will have a more complicated relationship than they previously did; while efficient sampling and ELBO evaluation are more intricate as a result \u2013 linear time scaling in the state-dimension can still be achieved with additional algebraic manipulations, as we show in App. C.2. ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Time complexity & low-rank precision updates. We first investigated the properties of low-rank variational Gaussian approximations in the large $L$ regime. To guide us, we had several questions in mind such as: i) how does the performance of low-rank approximations compare to full-rank and diagonal covariance approximations, ii) how large compared to $L$ should the rank of precision updates be to achieve satisfactory results, and iii) how does convergence using low-rank approximations compare to diagonal approximations, considering that they require a larger number of parameters. We expect that full-rank approximations would perform best (given a sufficient amount of data), since the true posterior will have dense second-order statistics due to the interactions of latent states in both the dynamics and observation models. However, it remains unclear how many dimensions are necessary for a low-rank approximation to achieve similar performance and whether this number will be practical. ", "page_idx": 7}, {"type": "text", "text": "We simulated data from 50D and 100D linear dynamical systems and compared the convergence between dense and diagonal approximations (Fig. 2b); we examined the ELBO for different rank parameterizations in two regimes i) observations and states are of the same dimensionality, $N=L$ (Fig. 2b - top), and ii) observations are lower dimensional than states, $N=L/5$ (Fig. 2b - bottom). While not surprising that dense variational Gaussian approximations achieve superior performance, message passing in latent Gaussian models with dense covariance scales like $\\mathcal{O}(\\dot{L}^{3})^{25}$ and becomes prohibitive for large $L$ ; thus it is reassuring that in both regimes, low-rank approximate posterior parameterizations achieve comparable results for precision matrix updates of relatively low rank compared to $L$ . To empirically examine time complexity scaling as a function of $L,\\,S$ , and $r$ , in Fig. 2a, we plot wall-clock times for fixed $r$ while varying $S$ and $L$ (bottom), and for fixed $S$ while varying $r$ and $L$ (top); reassuringly, inference time complexity scales $\\mathcal{O}(L)$ . ", "page_idx": 7}, {"type": "image", "img_path": "Ln8ogihZ2S/tmp/ba8c2fee68a46c712e0d00e14739cb57159b4a477a575e4a0e513088eb0259d3.jpg", "img_caption": ["Figure 3: Predict behavior from a causally inferred initial condition. a) Actual reaches. b) (top) Reaches linearly decoded from smoothed $\\dot{R}^{2}=0.89)$ ), causally filtered $R^{2}=0.88_{\\rightmoon}$ ), & predicted $/R^{2}=0.74)$ latent trajectories starting from an initial condition causally inferred during the preparatory period. (bottom) Top 3 principal latent dimensions per regime (smoothing/filtering/prediction) for three example trials. c) bps / $\\dot{R^{2}}$ of predicted hand velocity using rates inferred from the $700\\mathrm{ms}$ context window and the $500\\mathrm{ms}$ prediction window. d) Velocity decoding $R^{2}$ using predicted trajectories as a function of how far into the trial the latent state was filtered until it was only sampled from the autonomous dynamics; by the the movement onset, behavioral predictions using latent trajectory predictions are nearly on par with behavior decoded from the smoothed posterior. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Baseline comparisons \u2013 pendulum & bouncing ball. Next, we wondered how our approach fared against other modern deep state-space models when it came to learning complex dynamical systems from data. To explore this, we considered two popular datasets: i) a pendulum system26 and ii) a bouncing ball27,28. Each dataset consists of sequences of observations that are $16\\times16$ pixel images that are governed by a low-dimensional dynamical system. An interesting aspect of these datasets is that images can be reconstructed with impartial knowledge of the latent state, but for accurate long-term predictions, the dynamics will need to propagate features of the latent state that are irrelevant to the likelihood (e.g. pendulum angular velocity). For benchmarks, three other deep SSM approaches were included: i) deep variational Bayes fliter(DVBF)5 ii) deep Kalman fliter(DKF)29 iii) structured VAE(SVAE)20. We denote our causal amortization network with (c) and the noncausal version with (n). ", "page_idx": 8}, {"type": "text", "text": "We trained all models in context windows of 50 consecutive images and then sampled future $50\\,/\\,25$ time-step latent states from the learned dynamical system for pendulum / bouncing ball. To measure quality of learned latent representation and dynamics, we fti angular velocity / position decoders from training set latent states inferred from pendulum / bouncing ball observations. Then, on held-out test data, we measured the $R^{2}$ of velocity / position predictions during the context (smoothing) and forecast (prediction) windows. Fig. 1 shows that all methods are able to reconstruct well in context windows, however, when prediction is concerned, where the underlying dynamics would need to be learned well for accurate forecasts, our method consistently performs better. ", "page_idx": 8}, {"type": "text", "text": "Neural population dynamics. We consider two neuroscientific datasets where previous studies have shown the importance of population dynamics in generating plausible hypothesis about underlying neural computation. In addition to DKF, DVBF, and SVAE, we include the LFADS method7. First, we considered recordings from motor cortex of a monkey performing a reaching task30 and evaluate each methods\u2019 ability to forecast neural spiking and behavioral correlates. We measure the performance by bits-per-spike (BPS) using inferred spike-train rates31 and $R^{2}$ for decoding hand velocity. Similar to the previous experiment, we evaluate the performance in two regimes: i) a 700ms context window and ii) a $500\\mathrm{ms}$ prediction window following an initial context window of $200\\mathrm{ms}$ . Fig. 3c shows that, while all the methods excel at smoothing in the context window, our method makes more informative predictions in terms of $R^{2}$ and BPS. Next, we examined how well the monkey\u2019s behavior could be predicted given only causal estimates of the latent state; we trained a model using the causal amortized inference network, given by Eq. (29), then use learned inference network to infer latent states to predict behavior in three regimes: smoothing, filtering, and prediction. Fig. 3b shows that hand velocity can be decoded nearly as well in the flitering regime (without access to future data) as in the smoothing regime. In Fig. 3d, we plot how the quality of predictions change as filtered latent states are unrolled through the learned dynamics at different points in the trial; showing that forecasts starting prior to movement onset exhibit strong predictive capability. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Next, we investigated our method\u2019s performance with data exhibiting a more intricate trial structure. Specifically, we analyzed physiological recordings from the DMFC region of a monkey engaged in a timing interval reproduction task32. During this task, the monkey observes a random interval of time (termed the \u2018ready\u2019-\u2018set\u2019 period) demarcated by two cues, and the goal of the monkey is to reproduce that interval (termed the \u2018set\u2019-\u2018go\u2019 period). We perform a similar procedure as before, but for this experiment we use the period before \u2018set\u2019 as the context window, and use the learned dynamics to make predictions onward; in Fig. 2d we show the BPS measured on test data during the context and forecast windows. To further investigate the predictive capabilities, we examined condition averaged PSTH produced by samples from the latent state posterior during the joint context/prediction windows. Using the trained model, we sample spike valued observations for the context/prediction windows and then computed condition averaged PSTHs; the results shown in Fig. 2c, show that PSTHs sampled from the model remain true to the data, even during the lengthy prediction window. ", "page_idx": 9}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We presented a new approximate variational flitering/smoothing algorithm, variational learning objective, and Gaussian inference network parameterization for nonlinear state-space models. Focusing on approximations parameterized by dense covariances forced us to consider strategies that ensured computational feasibility for inference and learning with large $L$ . The introduced approximate variational filtering algorithm, while especially useful for the nonlinear dynamics we considered, could also be applied to high-dimensional linear systems where exact computations might be infeasible for large $L$ . Although our variational objective loses the property of lower-bounding the original data log-marginal likelihood, experiments showed that our method consistently outperforms approaches using potentially tight lower bounds. Quantifying this gap or considering potential corrections present interesting directions for future work. Furthermore, while the same variational approach is applicable to any exponential family dynamical system, specific distributions will have their own associated challenges, offering avenues for further research. ", "page_idx": 9}, {"type": "text", "text": "Given that neural computation is inherently nonlinear, system identification methods capable of modeling nonlinear dynamical systems are essential for advancing neuroscience. General SSMs can perform well on inferring smoothed latent state trajectories without learning a good model of the nonlinear dynamics. Our proposed method, XFADS, can not only perform efficient system identification and smoothing but also forecast future state evolution for population recordings\u2014a hallmark of a meaningful nonlinear dynamical model; using a causal inference network, XFADS can be used for real-time monitoring, feedback control, and online optimal experimental design, opening the door for new kinds of basic and clinical neuroscience experiments. Future work will focus on developing network architectures for precision matrix updates that are more parameter efficient when the rank those updates become moderate. Moreover, while Alg. 1 remains applicable in the confines of the generative model constraints considered, in certain scenarios, such as when $S>L$ , modifications will need to be made to Alg. 2 to minimize time complexity. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We would like to thank the anonymous reviewers and Scott Linderman for their insightful comments and constructive feedback, which greatly improved the quality of this paper. We thank Mahmoud Elmakki for help with organizing the code base and developing helpful demos. Yuan Zhao was supported in part by the National Institute of Mental Health Intramural Research Program (ZICMH002968). This work was supported by NIH RF1-DA056404 and the Portuguese Recovery and Resilience Plan (PPR), through project number 62, Center for Responsible AI, and the Portuguese national funds, through FCT \u2013 Funda\u00e7\u00e3o para a Ci\u00eancia e a Tecnologia \u2013 in the context of the project UIDB/04443/2020. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Kailath, T., Sayed, A. H. & Hassibi, B. Linear estimation. Prentice Hall, 2000.   \n[2] S\u00e4rkk\u00e4, S. Bayesian filtering and smoothing. Cambridge University Press, 2013. ISBN 9781107619289.   \n[3] Anderson, B. D. O. & Moore, J. B. Optimal Filtering. Prentice-Hall, Englewood Cliffs, N.J., 1979. ISBN 978-0-13-638122-8.   \n[4] Kingma, D. P. & Welling, M. Auto-Encoding variational bayes. In International Conference on Learning Representation, May 2014.   \n[5] Karl, M., Soelch, M., Bayer, J. & Smagt, P.van der . Deep variational Bayes fliters: Unsupervised learning of state space models from raw data. In International Conference on Learning Representations, 2017. [6] Krishnan, R. G., Shalit, U. & Sontag, D. A. Structured inference networks for nonlinear state space models. In AAAI Conference on Artificial Intelligence, 2016.   \n[7] Pandarinath, C., O\u2019Shea, D. J., Collins, J., Jozefowicz, R., Stavisky, S. D., Kao, J. C., Trautmann, E. M., Kaufman, M. T., Ryu, S. I., Hochberg, L. R. & others, . Inferring single-trial neural population dynamics using sequential auto-encoders. Nature methods, 15(10):805\u2013815, 2018.   \n[8] Van Overschee, P. & De Moor, B. Subspace identification for linear systems. Springer US, Boston, MA, 1996. ISBN 9781461380610,9781461304654.   \n[9] Turner, R. E. & Sahani, M. Two problems with variational expectation maximisation for time-series models. In Barber, D., Cemgil, T. & Chiappa, S., editors, Bayesian Time series models, chapter 5, pages 109\u2013130. Cambridge University Press, 2011.   \n[10] Kingma, D. P. & Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[11] Rezende, D. J., Mohamed, S. & Wierstra, D. Stochastic backpropagation and approximate inference in deep generative models. In International conference on machine learning, pages 1278\u20131286. PMLR, 2014.   \n[12] Wainwright, M. J. & Jordan, M. I. Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning, 1(1\u20132):1\u2013305, 2008.   \n[13] Diaconis, P. & Ylvisaker, D. Conjugate priors for exponential families. The Annals of statistics, pages 269\u2013281, 1979.   \n[14] Seeger, M. Expectation propagation for exponential families. Technical report, University of California at Berkeley, 2005.   \n[15] Khan, M. & Lin, W. Conjugate-Computation Variational Inference : Converting Variational Inference in Non-Conjugate Models to Inferences in Conjugate Models. In Singh, A. & Zhu, J., editors, Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, volume 54 of Proceedings of Machine Learning Research, pages 878\u2013887. PMLR, 20\u201322 Apr 2017.   \n[16] Beal, M. J. Variational algorithms for approximate Bayesian inference. University of London, University College London (United Kingdom), 2003.   \n[17] Alaa, A. M. & Schaar, M.van der . Attentive state-space modeling of disease progression. Advances in neural information processing systems, 32, 2019.   \n[18] Girin, L., Leglaive, S., Bie, X., Diard, J., Hueber, T. & Alameda-Pineda, X. Dynamical variational autoencoders: A comprehensive review. arXiv preprint arXiv:2008.12595, 2020.   \n[19] Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H. & Davidson, J. Learning latent dynamics for planning from pixels. In International conference on machine learning, pages 2555\u20132565. PMLR, 2019.   \n[20] Johnson, M. J., Duvenaud, D., Wiltschko, A. B., Datta, S. R. & Adams, R. P. Structured VAEs: Composing probabilistic graphical models and variational autoencoders. arXiv preprint arXiv:1603.06277, 2:2016, 2016.   \n[21] Zhao, Y. & Linderman, S. Revisiting structured variational autoencoders. In International Conference on Machine Learning, pages 42046\u201342057. PMLR, 2023.   \n[22] Douc, R., Moulines, E. & Stoffer, D. Nonlinear time series: theory, methods and applications with R examples. CRC press, 2014.   \n[23] Whiteley, N. & Lee, A. Twisted particle filters. 2014.   \n[24] Lawson, D., Ravent\u00f3s, A., Warrington, A. & Linderman, S. Sixo: Smoothing inference with twisted objectives. Advances in Neural Information Processing Systems, 35:38844\u201338858, 2022.   \n[25] Cseke, B. & Heskes, T. Approximate marginals in latent gaussian models. The Journal of Machine Learning Research, 12:417\u2013454, 2011.   \n[26] Botev, A., Jaegle, A., Wirnsberger, P., Hennes, D. & Higgins, I. Which priors matter? Benchmarking models for learning latent dynamics. arXiv preprint arXiv:2111.05458, 2021.   \n[27] Jiang, X., Missel, R., Li, Z. & Wang, L. Sequential latent variable models for few-shot highdimensional time-series forecasting. In The Eleventh International Conference on Learning Representations.   \n[28] Missel, R. Torch-Neural-SSM. https://github.com/qu-gg/torchssm, 2022.   \n[29] Krishnan, R. G., Shalit, U. & Sontag, D. Deep Kalman fliters. arXiv preprint arXiv:1511.05121, 2015.   \n[30] Churchland, M. M., Cunningham, J. P., Kaufman, M. T., Foster, J. D., Nuyujukian, P., Ryu, S. I. & Shenoy, K. V. Neural population dynamics during reaching. Nature, 487(7405):51\u201356, 2012.   \n[31] Pei, F., Ye, J., Zoltowski, D. M., Wu, A., Chowdhury, R. H., Sohn, H., O\u2019Doherty, J. E., Shenoy, K. V., Kaufman, M. T., Churchland, M., Jazayeri, M., Miller, L. E., Pillow, J., Park, I. M., Dyer, E. L. & Pandarinath, C. Neural latents benchmark \u201921: evaluating latent variable models of neural population activity. In Advances in Neural Information Processing Systems (NeurIPS), Track on Datasets and Benchmarks, 2021.   \n[32] Sohn, H., Narain, D. & Jazayeri, N. M. M. Bayesian computation through cortical latent dynamics. Neuron, 103(5):934\u2013947, sep 2019.   \n[33] Khan, M. E. & Rue, H. The bayesian learning rule. arXiv preprint arXiv:2107.04562, 2021.   \n[34] Cong, Y., Chen, B. & Zhou, M. Fast simulation of hyperplane-truncated multivariate normal distributions. 2017.   \n[35] Elfwing, S., Uchibe, E. & Doya, K. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural Networks, 107:3\u201311, 2018.   \n[36] Jiang, X., Missel, R., Li, Z. & Wang, L. Sequential latent variable models for few-shot highdimensional time-series forecasting. In The Eleventh International Conference on Learning Representations, 2023.   \n[37] Li, X., Wong, T.-K. L., Chen, R. T. & Duvenaud, D. Scalable gradients for stochastic differential equations. In International Conference on Artificial Intelligence and Statistics, pages 3870\u20133882. PMLR, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A Nomenclature 14 ", "page_idx": 12}, {"type": "text", "text": "B Variational filtering 14 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "B.1 Variational predict step . . . 14   \nB.2 Variational update step . . 15   \nB.3 Variational smoothing step . . 15   \nB.4 Efficiently sampling structured marginals 16   \nB.5 Efficient filtering 16 ", "page_idx": 12}, {"type": "text", "text": "C Evaluating the KL using low-rank structure 16 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "C.1 Smoothing inference network . 16   \nC.2 Causal/streaming inference network 17 ", "page_idx": 12}, {"type": "text", "text": "D Comparison method details 18 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "D.1 SVAE 18   \nD.2 DVBF 19   \nD.3 DKF 19 ", "page_idx": 12}, {"type": "text", "text": "E Experimental details 19 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "E.1 High-dimensional linear dynamical system . 19   \nE.2 Time complexity . . 19   \nE.3 Pendulum 19   \nE.4 Bouncing ball . . 20   \nE.5 MC_Maze . 21   \nE.6 DMFC_RSG 22   \nE.7 Collated results 23 ", "page_idx": 12}, {"type": "text", "text": "F Useful expressions 23 ", "page_idx": 12}, {"type": "text", "text": "G Forward KL fixed point 25 ", "page_idx": 12}, {"type": "text", "text": "H Linear Gaussian (information form) smoothing 25 ", "page_idx": 12}, {"type": "table", "img_path": "Ln8ogihZ2S/tmp/392f782e27ec9fb0fc819f2fd62efe4acc65a3ba0946f91bbc84caa076c77267.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "B Variational filtering ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Principles of Bayesian inference make it straightforward to write down an algorithm recursively computing the filtering posterior, $p(\\mathbf{z}_{t}\\mid\\mathbf{y}_{1:t})^{\\frac{}{2}}$ . Given, $p(\\mathbf{z}_{t-1}\\mid\\mathbf{y}_{1:t-1})$ , updating our belief to $p(\\mathbf{z}_{t}\\,|\\,\\mathbf{y}_{1:t})$ after observing $\\mathbf{y}_{t}$ can be broken down into two steps: first, we marginalize $p(\\mathbf{z}_{t-1}|\\,\\mathbf{y}_{1:t-1})$ through the dynamics to obtain the predictive distribution, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\bar{p}(\\mathbf{z}_{t}\\,|\\,\\mathbf{y}_{1:t-1})=\\mathbb{E}_{p(\\mathbf{z}_{t-1}\\mid\\mathbf{y}_{1:t-1})}\\left[p_{\\theta}(\\mathbf{z}_{t}\\,|\\,\\mathbf{z}_{t-1})\\right]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then, we update our belief by incorporating $\\mathbf{y}_{t}$ through Bayes\u2019 rule, ", "page_idx": 13}, {"type": "equation", "text": "$$\np(\\mathbf{z}_{t}\\,|\\,\\mathbf{y}_{1:t})\\propto p_{\\psi}(\\mathbf{y}_{t}\\,|\\,\\mathbf{z}_{t})\\bar{p}(\\mathbf{z}_{t}\\,|\\,\\mathbf{y}_{1:t-1})\\qquad(\\mathrm{update~step})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "With all of the filtered/predictive beliefs, the smoothing step is given by, ", "page_idx": 13}, {"type": "equation", "text": "$$\np(\\mathbf{z}_{t}\\left|\\mathbf{y}_{1:T})=\\mathbb{E}_{p(\\mathbf{z}_{t+1}\\mid\\mathbf{y}_{1:T})}\\left[p(\\mathbf{z}_{t}\\left|\\mathbf{z}_{t+1},\\mathbf{y}_{1:t})\\right]=p(\\mathbf{z}_{t}\\left|\\mathbf{y}_{1:t}\\right)\\!\\frac{\\mathbb{E}_{p(\\mathbf{z}_{t+1}\\mid\\mathbf{y}_{1:T})}\\left[p_{\\theta}(\\mathbf{z}_{t+1}\\mid\\mathbf{z}_{t})\\right]}{\\mathbb{E}_{p(\\mathbf{z}_{t}\\mid\\mathbf{y}_{1:t})}\\left[p_{\\theta}(\\mathbf{z}_{t+1}\\mid\\mathbf{z}_{t})\\right]}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "However, these steps can usually not be evaluated in closed form when we depart from assumptions of Gaussianity and linearity. For nonlinear Gaussian dynamics the predict step can not be carried out exactly, and for nonlinear or non-Gaussian observations neither can the update step. ", "page_idx": 13}, {"type": "text", "text": "Alternatively, by considering variational analogues of the predict / update steps, we can develop a recursive and fully differentiable procedure for finding approximations $\\pi(\\overline{{\\mathbf{z}_{t}}})\\approx p(\\mathbf{z}_{t}\\mid\\mathbf{y}_{1:t})$ . In developing the variational analogues, it is assumed the approximations belong to an exponential family of distributions, (i.e. $\\pi\\in\\mathcal{Q}$ where $\\mathcal{Q}$ is an exponential family distribution) \u2013 not necessarily Gaussian. ", "page_idx": 13}, {"type": "text", "text": "B.1 Variational predict step ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Similar to developing a recursive algorithm as in the exact case, given $\\pi(\\mathbf{z}_{t-1})\\approx p(\\mathbf{z}_{t-1}\\,|\\,\\mathbf{y}_{1:t-1})$ , we approximately marginalize $\\pi(\\mathbf{z}_{t-1})$ through the dynamics, by solving the following variational (forward KL / moment-matching) problem, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\bar{\\pi}(\\mathbf{z}_{t})=\\underset{\\bar{\\pi}\\in\\mathcal{Q}}{\\mathrm{argmin}}~\\mathbb{D}_{\\mathrm{KL}}\\big(\\mathbb{E}_{\\pi(\\mathbf{z}_{t-1})}\\left[p_{\\theta}(\\mathbf{z}_{t}\\,|\\,\\mathbf{z}_{t-1})\\right]\\big|\\,\\big|\\,\\bar{\\pi}(\\mathbf{z}_{t})\\big)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "So that if $p_{\\pmb{\\theta}}(\\mathbf{z}_{t}\\,|\\,\\mathbf{z}_{t-1})\\in\\mathcal{Q}$ , the optimization problem is minimized when the mean parameters of $\\bar{\\boldsymbol{\\pi}}\\big(\\mathbf{z}_{t}\\big)$ , denoted $\\textstyle{\\bar{\\pmb{\\mu}}}_{t}$ , are set to the expected mean parameter transformation under $\\pi(\\mathbf{z}_{t-1})$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\bar{\\pmb{\\mu}}_{t}=\\mathbb{E}_{\\pi(\\mathbf{z}_{t-1})}\\left[\\pmb{\\mu}_{\\theta}(\\mathbf{z}_{t-1})\\right]}&{{}}&{\\mathrm{(variational\\;predict\\;step)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "For a LGSSM with $p_{\\boldsymbol\\theta}(\\mathbf{z}_{t}\\,|\\,\\mathbf{z}_{t-1})=\\mathcal{N}(\\mathbf F\\mathbf{z}_{t-1},\\mathbf Q)$ , using the fact that, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu_{\\boldsymbol\\theta}(\\mathbf{z}_{t-1})=\\left[{\\frac{\\mathbf{F}\\mathbf{z}_{t-1}}{-{\\frac{1}{2}}\\,\\left(\\mathbf{F}\\mathbf{z}_{t-1}\\mathbf{z}_{t-1}^{\\top}\\mathbf{F}^{\\top}+\\mathbf{Q}_{\\boldsymbol\\theta}\\right)}}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "means that if $\\pi(\\mathbf{z}_{t-1})=\\mathcal{N}(\\mathbf{m}_{t-1},\\mathbf{P}_{t-1})$ , setting the mean and variance of $\\bar{\\boldsymbol{\\pi}}(\\mathbf{z}_{t})$ to, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\mathbf{m}}_{t}=\\mathbf{F}\\mathbf{m}_{t-1}}\\\\ &{\\bar{\\mathbf{P}}_{t}=\\mathbf{F}\\mathbf{P}_{t-1}\\mathbf{F}^{\\top}+\\mathbf{Q}_{\\theta}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "minimizes the forward KL objective, and reassuringly, recovers the familiar Kalman filter predict step. ", "page_idx": 14}, {"type": "text", "text": "B.2 Variational update step ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For the variational analogue of the filtering update step, we use $\\bar{\\boldsymbol{\\pi}}\\big(\\mathbf{z}_{t}\\big)$ as a prior for the latest observation, $\\mathbf{y}_{t}$ , and solve the following variational (reverse KL) problem, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\pi(\\mathbf{z}_{t})=\\underset{\\pi\\in\\mathcal{Q}}{\\mathrm{argmin}}~\\mathbb{D}_{\\mathrm{KL}}\\big(\\pi(\\mathbf{z}_{t})\\big|\\big|\\,p_{\\psi}(\\mathbf{y}_{t}\\,|\\,\\mathbf{z}_{t})\\bar{\\pi}(\\mathbf{z}_{t})\\big)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "If we denote the natural parameters of $\\pi(\\mathbf{z}_{t})$ by $\\lambda_{t}$ , then the optimal $\\lambda_{t}$ satisfy the implicit equation33, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\lambda_{t}=\\nabla_{\\mu_{t}}\\mathbb{E}_{\\pi_{t}}\\left[\\log p_{\\psi}(\\mathbf{y}_{t}\\,|\\,\\mathbf{z}_{t})\\right]+\\bar{\\lambda}_{t}\\qquad(\\mathrm{variational\\,\\,update\\,\\,step})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This usually requires an iterative optimization procedure, except when the likelihood is conjugate to $\\pi(\\mathbf{z}_{t})$ in which case, the likelihood must take the following form with respect to $\\mathbf{z}_{t}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\np_{\\psi}(\\mathbf{y}_{t}\\,|\\,\\mathbf{z}_{t})\\propto\\exp(\\mathcal{T}(\\mathbf{z}_{t})^{\\top}\\tilde{\\lambda}_{t})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "so that, as expected, the natural parameters of the solution are given as Bayes\u2019 rule would suggest \u2013 by adding the data dependent update to the natural parameters of the prior so that, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\lambda_{t}=\\tilde{\\lambda}_{t}+\\bar{\\lambda}_{t}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For a LGSSM with $p_{\\psi}(\\mathbf{y}_{t}\\,|\\,\\mathbf{z}_{t})=\\mathcal{N}(\\mathbf{C}\\mathbf{z}_{t},\\mathbf{R})$ , this results in the following updates, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{h}_{t}=\\bar{\\mathbf{h}}_{t}+\\mathbf{C}^{\\top}\\mathbf{R}^{-1}\\mathbf{y}_{t}}\\\\ {\\mathbf{J}_{t}=\\bar{\\mathbf{J}}_{t}+\\mathbf{C}^{\\top}\\mathbf{R}^{-1}\\mathbf{C}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which reassuringly recover the information form of the Kalman filter update step16. ", "page_idx": 14}, {"type": "text", "text": "B.3 Variational smoothing step ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Letting $\\breve{q}({\\bf z}_{t})\\approx p\\big({\\bf z}_{t}\\:|\\:{\\bf y}_{1:t}\\big)$ and $q(\\mathbf{z}_{t+1})\\approx p(\\mathbf{z}_{t+1}\\mid\\mathbf{y}_{1:T})$ , we can calculate the statistics of $q{\\bf(z}_{t})$ approximating the smoothed marginal posterior, by minimizing the following objective with respect to the marginal distribution, $q{\\bf(z}_{t})$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{S}(\\check{q})=\\mathbb{D}_{\\mathrm{KL}}\\big(\\check{q}(\\mathbf{z}_{t})\\big|\\,\\big|\\,\\mathbb{E}_{\\check{q}(\\mathbf{z}_{t+1})}\\,\\big[q(\\mathbf{z}_{t}\\,|\\,\\mathbf{z}_{t+1})\\big]\\big)}\\\\ &{\\qquad\\qquad=\\mathbb{D}_{\\mathrm{KL}}\\big(\\check{q}(\\mathbf{z}_{t})\\big|\\,\\big|\\,q(\\mathbf{z}_{t})\\big)-\\mathbb{E}_{\\check{q}(\\mathbf{z}_{t})}\\left[\\log\\mathbb{E}_{\\check{q}(\\mathbf{z}_{t+1})}\\left[\\frac{p_{\\theta}\\big(\\mathbf{z}_{t+1}\\,|\\,\\mathbf{z}_{t}\\big)}{\\mathbb{E}_{q(\\mathbf{z}_{t})}\\,\\big[p_{\\theta}\\big(\\mathbf{z}_{t+1}\\,|\\,\\mathbf{z}_{t}\\big)\\big]}\\right]\\right]}\\\\ &{\\qquad\\qquad\\approx\\mathbb{D}_{\\mathrm{KL}}\\big(\\check{q}(\\mathbf{z}_{t})\\big|\\,\\big|\\,q(\\mathbf{z}_{t})\\big)-\\mathbb{E}_{\\check{q}(\\mathbf{z}_{t})}\\left[\\log\\mathbb{E}_{\\check{q}(\\mathbf{z}_{t+1})}\\left[\\frac{p_{\\theta}\\big(\\mathbf{z}_{t+1}\\,|\\,\\mathbf{z}_{t}\\big)}{\\bar{q}\\big(\\mathbf{z}_{t+1}\\big)}\\right]\\right]:=\\widehat{\\mathcal{L}}_{S}(\\check{q})}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "taking natural gradients, we find that at a fixed point, the smoothed marginal posterior parameters satisfy the implicit relationship, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\lambda_{t}=\\check{\\lambda}_{t}+\\nabla_{\\mu_{t}}\\mathbb{E}_{q(\\mathbf{z}_{t})}\\left[A\\left(\\lambda_{\\theta}(\\mathbf{z}_{t})+\\lambda_{t+1}-\\bar{\\lambda}_{t+1}\\right)-A\\left(\\lambda_{\\theta}(\\mathbf{z}_{t})\\right)\\right]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Algorithm 2 Nonlinear variational filtering Input: $\\mathbf{k}_{1:T}$ , $\\mathbf{K}_{1:T}$ for $t=1$ to $T$ do $\\begin{array}{r}{\\bar{\\mathbf{m}}_{t}=S^{-1}\\sum\\mathbf{m}_{\\theta}(\\mathbf{z}_{t-1}^{s})}\\end{array}$ $\\bar{\\mathbf{M}}_{t}^{c}={S}^{-1/2}\\left[\\mathbf{m}_{\\theta}(\\mathbf{z}_{t-1}^{1})-\\bar{\\mathbf{m}}_{t}\\cdot\\cdot\\cdot\\mathbf{m}_{\\theta}(\\mathbf{z}_{t-1}^{S})-\\bar{\\mathbf{m}}_{t}\\right]$ $\\bar{\\mathbf{Y}}_{t}=\\mathrm{Cholesky}(\\mathbf{I}_{S}+\\bar{\\mathbf{M}}_{t}^{c\\top}\\mathbf{Q}^{-1}\\bar{\\mathbf{M}}_{t}^{c})^{-1}$ $\\bar{\\mathbf{h}}_{t}=\\bar{\\mathbf{P}}_{t}^{-1}\\bar{\\mathbf{m}}_{t}$ # using $[\\mathbf{Q},\\bar{\\mathbf{M}}_{t}^{c},\\bar{\\mathbf{Y}}_{t}]$ and Eq. (50) $\\mathbf{\\Upsilon}\\mathbf{\\Upsilon}\\Upsilon_{t}=\\underline{{\\mathbf{\\Upsilon}}}\\mathbf{\\mathrm{Cholesky}}(\\mathbf{I}_{r}+\\mathbf{K}_{t}^{\\top}\\bar{\\mathbf{P}}_{t}\\mathbf{K}_{t})^{-1}$ # using $[\\mathbf{Q},\\bar{\\mathbf{M}}_{t}^{c}]$ and Eq. (23) $\\mathbf{h}_{t}=\\bar{\\mathbf{h}}_{t}+\\mathbf{k}_{t}$ $\\mathbf{m}_{t}=\\mathbf{P}_{t}\\mathbf{h}_{t}$ # using $[\\mathbf{Q},\\bar{\\mathbf{M}}_{t}^{c},\\mathbf{K}_{t},\\mathbf{Y}_{t}]$ and Eqs. (52) and (23) $\\bar{\\mathbf{w}}_{t}^{s}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I}_{L+S})$ $\\bar{\\mathbf{z}}_{t}^{s}=\\bar{\\mathbf{P}}_{t}^{1/2}\\bar{\\mathbf{w}}_{t}^{s}$ # using $[\\mathbf{Q},\\bar{\\mathbf{M}}_{t}^{c}]$ and Eq. (23) $\\mathbf{w}_{t}^{s}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I}_{r})$ $\\begin{array}{r}{\\mathbf{z}_{t}^{s}=\\mathbf{m}_{t}+\\bar{\\mathbf{z}}_{t}^{s}-\\mathbf{K}_{t}\\mathbf{Y}_{t}\\mathbf{Y}_{t}^{\\top}(\\mathbf{K}_{t}^{\\top}\\bar{\\mathbf{z}}_{t}^{s}+\\mathbf{w}_{t}^{s})}\\end{array}$ end for   \nOutput: $\\mathbf{z}_{1:T}^{1:S},\\mathbf{m}_{1:T},\\bar{\\mathbf{m}}_{1:T},\\bar{\\mathbf{T}}_{1:T}$ ", "page_idx": 15}, {"type": "text", "text": "While $\\mathbf{P}_{t}$ has a structured representation, drawing samples from $\\pi(\\mathbf{z}_{t})$ is not straightforward because we do not have a structured representation for a square-root of $\\mathbf{P}_{t}$ . However, using the factorization of $\\bar{\\mathbf{P}}_{t}$ in Eq. (23), it is possible to sample from $\\mathcal{N}(\\mathbf{0},\\bar{\\mathbf{P}}_{t})$ efficiently since $\\bar{\\mathbf{P}}_{t}^{1/2}=\\mathbf{\\bar{\\left[M_{t}^{c}\\bar{\\mathbf{Q}}^{1/2}\\right]}}$ . Now, combining the fact that the posterior marginal can be written as, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pi(\\mathbf{z}_{t})=\\mathcal{N}(\\mathbf{m}_{t},\\mathbf{P}_{t})}\\\\ &{\\quad=\\mathcal{N}(\\mathbf{m}_{t},(\\bar{\\mathbf{P}}_{t}^{-1}+\\mathbf{K}_{t}\\mathbf{K}_{t}^{\\top})^{-1})}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with the result from Cong et al. 34, stating that sampling $\\mathbf{z}_{t}^{s}\\sim\\,\\pi(\\mathbf{z}_{t})$ is equivalent to sampling $\\mathbf{w}_{t}^{s}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I}_{L+S})$ and $\\bar{\\mathbf{z}}_{t}^{s}\\sim\\mathcal{N}(\\mathbf{0},\\bar{\\mathbf{P}}_{t})$ , and then setting ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{z}_{t}^{s}=\\mathbf{m}_{t}+\\bar{\\mathbf{z}}_{t}^{s}-\\mathbf{K}_{t}\\mathbf{Y}_{t}\\mathbf{Y}_{t}^{\\top}(\\mathbf{K}_{t}^{\\top}\\bar{\\mathbf{z}}_{t}^{s}+\\mathbf{w}_{t}^{s}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "makes it possible to efficiently draw samples from the posterior marginal. ", "page_idx": 15}, {"type": "text", "text": "B.5 Efficient filtering ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Evaluating $\\bar{\\mathbf{h}}_{t}\\,=\\,\\bar{\\mathbf{P}}_{t}^{-1}\\bar{\\mathbf{m}}_{t}$ and MVMs with $\\bar{\\mathbf{P}}_{t}^{-1}$ , can be carried out in $O(L S+S^{2})$ time, after an initial cost of $\\mathcal{O}(L S^{2}+S^{3})$ to factorize $\\bar{\\mathbf{T}}_{t}\\bar{\\mathbf{T}}_{t}^{\\top}\\,=\\,(\\mathbf{I}_{S}+\\bar{\\mathbf{M}}_{t}^{c\\top}\\mathbf{Q}_{\\theta}^{-1}\\bar{\\mathbf{M}}_{t}^{c})^{-1}$ , by applying the Woodbury identity to (23), ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{P}}_{t}^{-1}=\\mathbf{Q}_{\\theta}^{-1}-\\mathbf{Q}_{\\theta}^{-1}\\bar{\\mathbf{M}}_{t}^{c}\\bar{\\mathbf{T}}_{t}\\bar{\\mathbf{Y}}_{t}^{\\top}\\bar{\\mathbf{M}}_{t}^{c\\top}\\mathbf{Q}_{\\theta}^{-1}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since this specifies all quantities that characterize $\\bar{\\boldsymbol{\\pi}}\\big(\\mathbf{z}_{t}\\big)$ , following (13), next is to update our belief by adding the information from the pseudo observation to them, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{h}_{t}=\\bar{\\mathbf{h}}_{t}+\\mathbf{k}_{t}\\qquad\\qquad\\qquad\\mathbf{P}_{t}^{-1}=\\bar{\\mathbf{P}}_{t}^{-1}+\\mathbf{K}_{t}\\mathbf{K}_{t}^{\\top}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "As a result, the multiplication in $\\mathbf{m}_{t}=\\mathbf{P}_{t}\\mathbf{h}_{t}$ requires $\\mathcal{O}(L S+L r)$ time, and, analogous to (50), the square root factor $\\mathbf{\\boldsymbol{\\Upsilon}}_{t}$ in ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{P}_{t}=\\bar{\\mathbf{P}}_{t}-\\bar{\\mathbf{P}}_{t}\\mathbf{K}_{t}\\mathbf{Y}_{t}\\mathbf{Y}_{t}^{\\top}\\mathbf{K}_{t}^{\\top}\\bar{\\mathbf{P}}_{t}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "requires $\\mathcal{O}(r^{3}+L S r+S^{2}r)$ time where $\\mathbf{\\Upsilon}\\mathbf{\\Upsilon}\\mathbf{\\Upsilon}_{t}\\mathbf{\\Upsilon}\\mathbf{\\Upsilon}\\mathbf{\\Upsilon}_{t}^{\\top}=(\\mathbf{I}_{r}+\\mathbf{K}_{t}^{\\top}\\bar{\\mathbf{P}}_{t}\\mathbf{K}_{t})^{-1}.$ . ", "page_idx": 15}, {"type": "text", "text": "C Evaluating the KL using low-rank structure ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1 Smoothing inference network ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Efficient training of the generative model and inference networks require efficient numerical evaluation of the ELBO. We take advantage of the structured precision matrices arising from low-rank updates ", "page_idx": 15}, {"type": "text", "text": "and sample approximations. The expected log likelihood can be evaluated using a Monte Carlo approximation from samples during the filtering pass. The KL term, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{D}_{\\mathrm{KL}}(\\pi(\\mathbf{z}_{t})||\\,\\bar{\\pi}(\\mathbf{z}_{t}))=\\frac{1}{2}\\left[(\\bar{\\mathbf{m}}_{t}-\\mathbf{m}_{t})^{\\top}\\bar{\\mathbf{P}}_{t}^{-1}(\\bar{\\mathbf{m}}_{t}-\\mathbf{m}_{t})+\\mathrm{tr}(\\bar{\\mathbf{P}}_{t}^{-1}\\mathbf{P}_{t})+\\log\\frac{|\\bar{\\mathbf{P}}_{t}|}{|\\mathbf{P}_{t}|}-L\\right]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "can be evaluated in closed form and we can expand each term as, ", "page_idx": 16}, {"type": "text", "text": "Log-determinant. Writing ", "text_level": 1, "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log|\\mathbf{P}_{t}|=-\\log|\\bar{\\mathbf{P}}_{t}^{-1}+\\mathbf{K}_{t}\\mathbf{K}_{t}^{\\top}|}\\\\ &{\\qquad\\qquad=\\log|\\bar{\\mathbf{P}}_{t}|-\\log|\\mathbf{I}+\\mathbf{K}_{t}^{\\top}\\bar{\\mathbf{P}}_{t}\\mathbf{K}_{t}|}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "gives ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\log\\frac{|\\bar{\\mathbf{P}}_{t}|}{|\\mathbf{P}_{t}|}=\\log|\\mathbf{I}+\\mathbf{K}_{t}^{\\top}\\bar{\\mathbf{P}}_{t}\\mathbf{K}_{t}|}\\\\ {=-2\\sum_{i=1}^{r}[\\mathbf{Y}_{t}]_{i,i}\\quad\\~}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Trace. Writing, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{tr}(\\bar{\\mathbf{P}}_{t}^{-1}\\mathbf{P}_{t})=\\mathrm{tr}(\\mathbf{I}_{L}-\\mathbf{K}_{t}(\\mathbf{I}+\\mathbf{K}_{t}^{\\top}\\bar{\\mathbf{P}}_{t}\\mathbf{K}_{t})^{-1}\\mathbf{K}_{t}^{\\top}\\bar{\\mathbf{P}}_{t})}\\\\ &{\\phantom{\\mathrm{tr}}=L-\\mathrm{tr}(\\bar{\\mathbf{P}}_{t}^{\\top/2}\\mathbf{K}_{t}(\\mathbf{I}+\\mathbf{K}_{t}^{\\top}\\bar{\\mathbf{P}}_{t}\\mathbf{K}_{t})^{-1}\\mathbf{K}_{t}^{\\top}\\bar{\\mathbf{P}}_{t}^{1/2})}\\\\ &{\\phantom{\\mathrm{tr}}=L-\\mathrm{tr}(\\mathbf{Y}_{t}^{\\top}\\mathbf{K}_{t}^{\\top}\\bar{\\mathbf{P}}_{t}\\mathbf{K}_{t}\\mathbf{Y}_{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which by taking $\\mathbf{\\Delta}\\mathbf{\\Upsilon}_{t}$ to be an $r\\times r$ square root such that, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{\\Upsilon}\\mathbf{\\Upsilon}\\mathbf{\\Upsilon}\\mathbf{\\Upsilon}_{t}\\mathbf{\\Upsilon}\\mathbf{\\Upsilon}_{t}^{\\top}=(\\mathbf{I}+\\mathbf{K}_{t}^{\\top}\\bar{\\mathbf{P}}_{t}\\mathbf{K}_{t})^{-1}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "further simplifies to ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{tr}(\\bar{\\mathbf{P}}_{t}^{-1}\\mathbf{P}_{t})=L-\\mathrm{tr}(\\bar{\\mathbf{M}}_{t}^{c^{\\top}}\\mathbf{K}_{t}\\mathbf{Y}_{t}\\mathbf{Y}_{t}^{\\top}\\mathbf{K}_{t}^{\\top}\\bar{\\mathbf{M}}_{t}^{c})-\\mathrm{tr}(\\mathbf{Q}^{\\top/2}\\mathbf{K}_{t}\\mathbf{Y}_{t}\\mathbf{Y}_{t}^{\\top}\\mathbf{K}_{t}^{\\top}\\mathbf{Q}^{1/2})}\\\\ {=L-\\mathrm{tr}(\\bar{\\mathbf{M}}_{t}^{c^{\\top}}\\mathbf{K}_{t}\\mathbf{Y}_{t}\\mathbf{Y}_{t}^{\\top}\\mathbf{K}_{t}^{\\top}\\bar{\\mathbf{M}}_{t}^{c})-\\mathrm{tr}(\\mathbf{Y}_{t}^{\\top}\\mathbf{K}_{t}^{\\top}\\mathbf{Q}^{1/2}\\mathbf{Q}^{\\top/2}\\mathbf{K}_{t}\\mathbf{Y}_{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "note that the size of the triple product, $\\bar{\\mathbf{M}}_{t}^{c^{\\top}}\\mathbf{K}_{t}\\mathbf{Y}_{t}$ , is $S\\times r$ . ", "page_idx": 16}, {"type": "text", "text": "C.2 Causal/streaming inference network ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "When the real-time parameterization of the inference network is used the expressions become slightly more complicated due to the more intricate relationship between the posterior at time $t$ and the posterior predictive at time $t$ . ", "page_idx": 16}, {"type": "text", "text": "Log-determinant. We need to first find $\\log|\\bar{\\mathbf{P}}_{t|T}|-\\log|\\mathbf{P}_{t}|$ so begin with using the matrixdeterminant lemma to write, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\bar{\\mathbf{P}}_{t|T}|=|\\mathbf{M}_{t|T}^{c}\\mathbf{M}_{t|T}^{c\\top}+\\mathbf{Q}|}\\\\ &{\\qquad\\quad=|\\mathbf{I}_{S}+\\mathbf{M}_{t|T}^{c\\top}\\mathbf{Q}^{-1}\\mathbf{M}_{t|T}^{c}|\\times|\\mathbf{Q}|}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "then expand the smoothed covariance to write, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\mathbf{P}_{t}|=|(\\check{\\mathbf{P}}_{t}^{-1}+\\mathbf{B}_{t}\\mathbf{B}_{t}^{\\top})^{-1}|}\\\\ &{\\qquad=|\\check{\\mathbf{P}}_{t}^{-1}+\\mathbf{B}_{t}\\mathbf{B}_{t}^{\\top}|^{-1}}\\\\ &{\\qquad=(|\\mathbf{I}_{r_{\\beta}}+\\mathbf{B}_{t}^{\\top}\\check{\\mathbf{P}}_{t}\\mathbf{B}_{t}|\\times|\\check{\\mathbf{P}}_{t}^{-1}|)^{-1}}\\\\ &{\\qquad=|\\mathbf{I}_{r_{\\beta}}+\\mathbf{B}_{t}^{\\top}\\check{\\mathbf{P}}_{t}\\mathbf{B}_{t}|^{-1}|\\check{\\mathbf{P}}_{t}|}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and another time to write, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\breve{\\mathbf{P}}_{t}^{-1}|=|\\bar{\\mathbf{P}}_{t}^{-1}+\\mathbf{A}_{t}\\mathbf{A}_{t}^{\\top}|}\\\\ &{\\qquad\\quad=|\\mathbf{I}_{r_{\\alpha}}+\\mathbf{A}_{t}^{\\top}\\bar{\\mathbf{P}}_{t}\\mathbf{A}_{t}|\\times|\\bar{\\mathbf{P}}_{t}^{-1}|}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and another time, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\bar{\\mathbf{P}}_{t}|=|\\mathbf{M}_{t}^{c}\\mathbf{M}_{t}^{c^{\\top}}+\\mathbf{Q}|}\\\\ &{\\qquad=|\\mathbf{I}_{S}+\\mathbf{M}_{t}^{c^{\\top}}\\mathbf{Q}^{-1}\\mathbf{M}_{t}^{c}|\\times|\\mathbf{Q}|}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "When combined we are finally able to write ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\log|\\bar{\\mathbf{P}}_{t|T}|-\\log|\\mathbf{P}_{t}|=\\log|\\mathbf{I}_{S}+\\mathbf{M}_{t|T}^{c\\top}\\mathbf{Q}^{-1}\\mathbf{M}_{t|T}^{c}|+\\log|\\mathbf{I}_{r_{\\beta}}+\\mathbf{B}_{t}^{\\top}\\bar{\\mathbf{P}}_{t}\\mathbf{B}_{t}|}\\\\ {+\\log|\\mathbf{I}_{r_{\\alpha}}+\\mathbf{A}_{t}^{\\top}\\bar{\\mathbf{P}}_{t}\\mathbf{A}_{t}|-\\log|\\mathbf{I}_{S}+\\mathbf{M}_{t}^{c\\top}\\mathbf{Q}^{-1}\\mathbf{M}_{t}^{c}|}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For the initial condition we have, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\log|\\bar{\\mathbf{P}}_{1}|-\\log|\\mathbf{P}_{1}|=\\log\\left|\\mathbf{I}_{r_{\\beta}}+\\mathbf{B}_{1}^{\\top}\\check{\\mathbf{P}}_{1}\\mathbf{B}_{1}\\right|+\\log\\left|\\mathbf{I}_{r_{\\alpha}}+\\mathbf{A}_{1}^{\\top}\\bar{\\mathbf{P}}_{1}\\mathbf{A}_{1}\\right|\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Trace. For the trace, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{tr}(\\bar{\\mathbf{P}}_{t|T}^{-1}\\mathbf{P}_{t})=\\mathrm{tr}(\\mathbf{Q}^{-1}\\mathbf{P}_{t})-\\mathrm{tr}(\\mathbf{Q}^{-1}\\mathbf{M}_{t|T}^{c}(\\mathbf{I}_{S}+\\mathbf{M}_{t|T}^{c\\top}\\mathbf{Q}^{-1}\\mathbf{M}_{t|T}^{c})^{-1}\\mathbf{M}_{t|T}^{c\\top}\\mathbf{Q}^{-1}\\mathbf{P}_{t})}\\\\ &{\\qquad\\qquad=\\mathrm{tr}(\\mathbf{Q}^{-1}\\mathbf{P}_{t})-\\mathrm{tr}(\\mathbf{Q}^{-1}\\mathbf{M}_{t|T}^{c}\\bar{\\mathbf{T}}_{t|T}\\bar{\\mathbf{T}}_{t|T}^{\\top}\\mathbf{M}_{t|T}^{c\\top}\\mathbf{Q}^{-1}\\mathbf{P}_{t})}\\\\ &{\\qquad\\qquad=\\mathrm{tr}(\\mathbf{P}_{t}\\mathbf{Q}^{-1})-\\mathrm{tr}([\\mathbf{P}_{t}\\mathbf{Q}^{-1}\\mathbf{M}_{t|T}^{c}\\bar{\\mathbf{T}}_{t|T}]\\,\\bar{\\mathbf{T}}_{t|T}^{\\top}\\mathbf{M}_{t|T}^{c\\top}\\mathbf{Q}^{-1})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we expand the first rhs term for a numerically efficient implementation by writing ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{tr}(\\mathbf{P}_{t}\\mathbf{Q}^{-1})=\\mathrm{tr}(\\bar{\\mathbf{P}}_{t}\\mathbf{Q}^{-1})-\\mathrm{tr}(\\bar{\\mathbf{P}}_{t}\\mathbf{A}_{t}(\\mathbf{I}_{r_{\\alpha}}+\\mathbf{A}_{t}^{\\top}\\bar{\\mathbf{P}}_{t}\\mathbf{A}_{t})^{-1}\\mathbf{A}_{t}^{\\top}\\bar{\\mathbf{P}}_{t}\\mathbf{Q}^{-1})}\\\\ {-\\mathrm{tr}(\\breve{\\mathbf{P}}_{t}\\mathbf{B}_{t}(\\mathbf{I}_{r_{\\beta}}+\\mathbf{B}_{t}^{\\top}\\breve{\\mathbf{P}}_{t}\\mathbf{B}_{t})^{-1}\\mathbf{B}_{t}^{\\top}\\breve{\\mathbf{P}}_{t}\\mathbf{Q}^{-1})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "To reduce notational clutter we define, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\mathbf{Y}}_{t}\\bar{\\mathbf{Y}}_{t}^{\\top}=(\\mathbf{I}_{S}+\\mathbf{M}_{t}^{c^{\\top}}\\mathbf{Q}^{-1}\\mathbf{M}_{t}^{c})^{-1}}\\\\ &{\\bar{\\mathbf{Y}}_{t\\mid T}\\bar{\\mathbf{Y}}_{t\\mid T}^{\\top}=(\\mathbf{I}_{S}+\\mathbf{M}_{t\\mid T}^{c^{\\top}}\\mathbf{Q}^{-1}\\mathbf{M}_{t\\mid T}^{c})^{-1}}\\\\ &{\\quad\\breve{\\mathbf{Y}}_{t}\\breve{\\mathbf{Y}}_{t}^{\\top}=(\\mathbf{I}_{r_{\\alpha}}+\\mathbf{A}_{t}^{\\top}\\breve{\\mathbf{P}}_{t}\\mathbf{A}_{t})^{-1}}\\\\ &{\\quad\\mathbf{Y}_{t}\\mathbf{Y}_{t}^{\\top}=(\\mathbf{I}_{r_{\\beta}}+\\mathbf{B}_{t}^{\\top}\\bar{\\mathbf{P}}_{t}\\mathbf{B}_{t})^{-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and so the trace is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{tr}(\\bar{\\mathbf{P}}_{t|T}^{-1}\\mathbf{P}_{t})=L+\\mathrm{tr}(\\mathbf{M}_{t}^{c^{\\top}}\\mathbf{Q}^{-1}\\mathbf{M}_{t}^{c})-\\mathrm{tr}(\\mathbf{Q}^{-1/2}\\bar{\\mathbf{P}}_{t}\\mathbf{A}_{t}\\breve{\\mathbf{Y}}_{t}\\breve{\\mathbf{Y}}_{t}^{\\top}\\mathbf{A}_{t}^{\\top}\\bar{\\mathbf{P}}_{t}\\mathbf{Q}^{-1/2})}\\\\ &{\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,-\\mathrm{tr}(\\mathbf{Q}^{-1/2}\\breve{\\mathbf{P}}_{t}\\mathbf{B}_{t}\\Upsilon_{t}\\Upsilon_{t}\\mathbf{Y}_{t}^{\\top}\\mathbf{B}_{t}^{\\top}\\breve{\\mathbf{P}}_{t}\\mathbf{Q}^{-1/2})}\\\\ &{\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,-\\mathrm{tr}([\\mathbf{P}_{t}\\mathbf{Q}^{-1}\\mathbf{M}_{t|T}^{c}\\bar{\\mathbf{Y}}_{t|T}]\\,\\bar{\\mathbf{Y}}_{t|T}^{\\top}\\mathbf{M}_{t|T}^{c\\top}\\mathbf{Q}^{-1})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which is now in a form that is easy to handle using fast MVMs with $\\bar{\\mathbf{P}}_{t},\\breve{\\mathbf{P}}_{t},\\mathbf{P}_{t}$ . ", "page_idx": 17}, {"type": "text", "text": "For the initial condition term we use the fact that $\\bar{\\mathbf{P}}_{1}$ is diagonal, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{tr}(\\mathbf{P}_{1}\\bar{\\mathbf{P}}_{1}^{-1})=L-\\mathrm{tr}(\\bar{\\mathbf{P}}_{1}^{1/2}\\mathbf{A}_{1}\\check{\\mathbf{T}}_{1}\\check{\\mathbf{T}}_{1}^{\\top}\\mathbf{A}_{1}^{\\top}\\bar{\\mathbf{P}}_{1}^{1/2})-\\mathrm{tr}(\\bar{\\mathbf{P}}_{1}^{-1/2}\\check{\\mathbf{P}}_{1}\\mathbf{B}_{1}\\mathbf{Y}_{1}\\mathbf{Y}_{1}^{\\top}\\mathbf{B}_{1}^{\\top}\\check{\\mathbf{P}}_{1}\\bar{\\mathbf{P}}_{1}^{-1/2})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "D Comparison method details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "D.1 SVAE ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "For the $S\\mathrm{VAE}^{20}$ , the latent state prior is a linear dynamical system parameterized as, ", "page_idx": 17}, {"type": "equation", "text": "$$\np_{\\theta}(\\mathbf{z}_{t}\\,|\\,\\mathbf{z}_{t-1})=\\mathcal{N}(\\mathbf{z}_{t}\\,|\\,\\mathbf{F}\\mathbf{z}_{t-1},\\mathbf{Q})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Using conjugate potentials, with likelihood $p(\\tilde{\\mathbf{y}}_{t}\\,|\\,\\mathbf{z}_{t})=\\exp(t(\\mathbf{z}_{t})^{\\top}\\alpha(\\mathbf{y}_{t}))$ , the approximate posterior is given by $\\begin{array}{r}{\\bar{q}(\\mathbf{z}_{1:T})=\\prod p(\\tilde{\\mathbf{y}}_{t}\\,|\\,\\mathbf{z}_{t})p_{\\pmb{\\theta}}(\\mathbf{z}_{1:T})}\\end{array}$ so that its statistics can be found by applying Kalman filtering/smoothing to the pseudo-observations. In this case, the ELBO can be evaluated as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\boldsymbol{q})=\\sum\\mathbb{E}_{\\boldsymbol{q}_{t}}\\left[\\log p(\\mathbf{y}_{t}\\left|\\mathbf{z}_{t})\\right]-\\mathbb{E}_{\\boldsymbol{q}_{t}}\\left[\\log p(\\tilde{\\mathbf{y}}_{t}\\left|\\mathbf{z}_{t}\\right.)\\right]+\\log\\mathbb{E}_{\\bar{\\boldsymbol{q}}_{t}}\\left[p(\\tilde{\\mathbf{y}}_{t}\\left|\\mathbf{z}_{t}\\right.)\\right]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\bar{q}_{t}:=\\bar{q}(\\mathbf{z}_{t})=p(\\mathbf{z}_{t}\\,|\\,\\tilde{\\mathbf{y}}_{1:t-1})$ is the filtering predictive distribution. These expressions can be evaluated in closed form and written concisely in terms of natural/mean parameters and log-partition functions as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{L}(q)=\\sum\\mathbb{E}_{q_{t}}\\left[\\log p(\\mathbf{y}_{t}\\,|\\,\\mathbf{z}_{t})\\right]-\\mu_{t}^{\\top}\\alpha_{t}+A(\\bar{\\lambda}_{t}+\\alpha_{t})-A(\\bar{\\lambda}_{t})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\mu_{t}=\\nabla A(\\bar{\\lambda}_{t}+\\alpha_{t}+\\beta_{t+1})$ . Using the identities given in App. F, these expressions can be written in more familiar mean/covariance parameters. ", "page_idx": 17}, {"type": "text", "text": "D.2 DVBF ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For the $\\mathrm{DVBF}^{5}$ , we parameterize the latent state prior using a nonlinear dynamical system of the same form as Eq. (1). Then, using an inference network that encodes data in reverse-time to produce the parameters of a diagonal Gaussian distribution, $\\mathbf{w}_{t}\\sim q(\\mathbf{w}_{t})=\\mathcal{N}(\\mathbf{m}_{t},\\mathrm{diag}(\\mathbf{s}_{t}))$ , we sample the latent trajectory forward using the recursion, $\\begin{array}{r}{\\mathbf{z}_{t}=\\mathbf{m}_{\\theta}\\big(\\mathbf{z}_{t-1}\\big)+\\mathbf{Q}^{1/2}\\mathbf{w}_{t}}\\end{array}$ . Parameters of the generative model/inference network are learned jointly by minimizing the ELBO, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{L}(q)=\\sum\\mathbb{E}_{q(\\mathbf{z}_{t})}\\left[\\log p_{\\psi}(\\mathbf{y}_{t}\\,|\\,\\mathbf{z}_{t})\\right]-\\mathbb{D}_{\\mathrm{KL}}(q(\\mathbf{w}_{t})||\\,p(\\mathbf{w}_{t}))\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where, $p(\\mathbf{w}_{t})=\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ . ", "page_idx": 18}, {"type": "text", "text": "D.3 DKF ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For the $\\mathrm{DKF}^{6}$ , the latent state is also parameterized using a nonlinear dynamical system of the same form as Eq. (1). We follow the parameterization outlined in the text with S2S(\u00b7) implemented using a recurrent neural network. We sample trajectories using the inference network and jointly train all parameters on the ELBO, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{L}(q)=\\sum\\mathbb{E}_{q_{t}}\\left[\\log p(\\mathbf{y}_{t}\\left|\\mathbf{z}_{t})\\right]-\\mathbb{E}_{q_{t-1}}\\left[\\mathbb{D}_{\\mathrm{KL}}(q(\\mathbf{z}_{t}\\left|\\mathbf{z}_{t-1}\\right)\\right|\\left|\\,p_{\\theta}(\\mathbf{z}_{t}\\left|\\mathbf{z}_{t-1}\\right))\\right]\\leq\\log p(\\mathbf{y}_{1:T})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "E Experimental details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In describing the neural network architectures used to parameterize the inference model, it will be useful to define the following multilayer perceptron (MLP), with SiLU nonlinearity35, that gets used repeatedly: ", "page_idx": 18}, {"type": "equation", "text": "$$\n-\\ \\mathrm{MLP}(n_{\\mathrm{in}},n_{\\mathrm{hidden}},n_{\\mathrm{out}}):[\\mathrm{Linear}(n_{\\mathrm{in}},n_{\\mathrm{hidden}}),\\mathrm{SiLU}(),\\mathrm{Linear}(n_{\\mathrm{hidden}},n_{\\mathrm{out}})]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{L}(q)=\\sum\\mathbb{E}_{q_{t}}\\left[\\log p(\\mathbf{y}_{t}\\left|\\mathbf{z}_{t})\\right]-\\mathbb{E}_{q_{t-1}}\\left[\\mathbb{D}_{\\mathrm{KL}}(q(\\mathbf{z}_{t}\\left|\\mathbf{z}_{t-1}\\right)\\right|\\left|\\,p_{\\theta}(\\mathbf{z}_{t}\\left|\\mathbf{z}_{t-1}\\right))\\right]\\leq\\log p(\\mathbf{y}_{1:T})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "E.1 High-dimensional linear dynamical system ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We simulated data from an LDS generative model (with dynamics restricted to the set of matrices with singular values less than 1) for latent dimensions $L\\in[20,50,100]$ over 3 random seeds for two scenarios i) $N=L$ and ii) $N=L/5$ . For each scenario, we also vary the rank of the local/backward encoder precision updates. ", "page_idx": 18}, {"type": "text", "text": "E.2 Time complexity ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We generate random LDS systems of appropriate dimension and measure the average time to complete one forward pass and take a gradient step. The system used for benchmarking wall-clock time was an RTX 4090 with 128GB of RAM with an AMD 5975WX processor. ", "page_idx": 18}, {"type": "text", "text": "E.3 Pendulum ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We consider the pendulum system from26. We generate 500/150/150 trials of length 100 for training/validation/testing. All methods are trained for 5000 epochs for 3 different random seeds. We consider a context window of 50 images and a forecast window of 50 images. A decoder was fti from the latent state on the training set during the context window; then, for held out data, we examine performance of the decoder during the context and forecast windows. ", "page_idx": 18}, {"type": "text", "text": "The generative model is parameterized as: ", "page_idx": 18}, {"type": "text", "text": "\u2022 L = 4 \u2022 $N=256$ \u2022 $T=50$ \u2022 likelihood ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\ p_{\\psi}(\\mathbf{y}_{t}\\,|\\,\\mathbf{z}_{t})=\\mathcal{N}(\\mathbf{z}_{t}\\,|\\,\\mathbf{C}_{\\psi}(\\mathbf{z}_{t})+\\mathbf{b},\\mathbf{R})}\\\\ &{-\\ \\mathbf{C}_{\\psi}:\\mathrm{MLP}(4,128,256)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For each method, inference is amortized using the following neural network architectures: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-r_{\\alpha}=4}\\\\ &{\\quad-r_{\\beta}=4}\\\\ &{\\quad-\\alpha_{\\phi}\\cdot\\mathrm{MF}(256,128,20)}\\\\ &{\\quad-\\beta_{\\phi}\\cdot\\mathrm{[GRU(128),Linear(128,20)]}}\\\\ &{\\quad+\\mathrm{DKF}\\operatorname*{inf}_{\\mathrm{Frerence~network}}}\\\\ &{\\quad-\\textbf{u}_{\\phi}\\cdot\\mathrm{GRU(128)}}\\\\ &{\\quad-\\textbf{(m_{\\phi})}_{\\mathrm{lop}}\\cdot\\mathrm{DF}_{\\phi}\\backslash\\mathrm{MF}(132,128,8)}\\\\ &{\\quad-\\textbf{D}\\mathrm{DF}\\mathrm{inf}_{\\mathrm{Frence~network}}}\\\\ &{\\quad-\\textbf{u}_{\\phi}\\cdot\\mathrm{[GRU(128)]}}\\\\ &{\\quad-\\textbf{(}\\mu_{\\phi}\\mathrm{,log}\\sigma_{\\phi}\\mathrm{)}\\cdot\\mathrm{MF}(132,128,8)}\\\\ &{\\quad-\\textbf{S V}\\mathrm{A}\\mathrm{DF}\\mathrm{ernce~network}}\\\\ &{\\quad-\\textbf{\\textup{\\texttt{n+}}}\\mathrm{MD}(256,128,20)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Optimization and training details: ", "page_idx": 19}, {"type": "text", "text": "\u2013 optimizer: Adam(lr = 0.001) \u2013 batch size: 128 ", "page_idx": 19}, {"type": "text", "text": "ours(c) 0000008008088000080000080000009000000880 \u2192\u2192 time ", "page_idx": 19}, {"type": "text", "text": "Figure 4: Learned covariance of nonlinear SSMs. (top) single trial of a pendulum. Below are the posterior covariances output by the causal and non-causal variants of XFADS. ", "page_idx": 19}, {"type": "text", "text": "E.4 Bouncing ball ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We consider a bouncing ball dataset commonly used as a baseline to benchmark the performance of inference and learning in deep state-space models26,36,36. For this dataset we take $500\\bar{/}150/150$ trials of length 75 for training/validation/testing. All methods are trained for 5000 epochs for 3 different random seeds. The generative model is parameterized as: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\,\\,L=8}\\\\ &{\\bullet\\,\\,N=256}\\\\ &{\\bullet\\,\\,T=50}\\\\ &{\\bullet\\,\\,\\mathrm{likelihood}}\\\\ &{\\quad\\quad-\\,\\,p_{\\psi}(\\mathbf{y}_{t}\\,|\\,\\mathbf{z}_{t})=\\mathcal{N}(\\mathbf{z}_{t}\\,|\\,\\mathbf{C}_{\\psi}(\\mathbf{z}_{t})+\\mathbf{b},\\mathbf{R})}\\\\ &{\\quad\\quad-\\,\\,\\mathbf{C}_{\\psi}:\\mathrm{MLP}(8,128,256)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For each method, inference is amortized using the following neural network architectures: ", "page_idx": 19}, {"type": "text", "text": "our inference network $\\begin{array}{l}{-\\ r_{\\alpha}=8}\\\\ {-\\ r_{\\beta}=4}\\\\ {-\\ \\alpha_{\\phi}{\\colon}\\operatorname{MLP}(256,128,72)}\\end{array}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad-\\beta_{\\phi}\\colon[\\mathrm{GRU}(128),\\mathrm{Linear}(128,40)]}\\\\ {*}&{\\mathrm{DKF~inference~network}}\\\\ &{\\qquad-\\mathrm{~u_{\\phi}\\cdot G R U}(128)}\\\\ &{\\qquad-\\mathrm{~(m_{\\phi},l o g~P_\\phi)\\colon M L P}(132,128,16)}\\\\ {*}&{\\mathrm{DVBF~inference~network}}\\\\ &{\\qquad-\\mathrm{~u_{\\phi}\\cdot[G R U(128)]}}\\\\ &{\\qquad-\\mathrm{~(}\\mu_{\\phi},\\log\\sigma_{\\phi}\\mathrm{)\\colonMLP}(132,128,16)}\\\\ {*}&{\\mathrm{SVAE~inference~network}}\\\\ &{\\qquad-\\alpha_{\\phi}\\colon\\mathrm{MLP}(256,128,72)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Optimization and training details: ", "page_idx": 20}, {"type": "text", "text": "\u2013 optimizer: Adam(lr = 0.001) \u2013 batch size: 128 ", "page_idx": 20}, {"type": "text", "text": "ours(o) 09089800000000008N500NN9000808N988000000 DKF 00OOONNNOOUNODNNNNNNNNNNNOOONONNONNNNNNN   \nDVBF NNONNNNNONOOONNNNNONNODOONONNNONONNNNONC \u2192 time ", "page_idx": 20}, {"type": "text", "text": "Figure 5: Learned covariance of nonlinear SSMs. (top) single trial of a bouncing ball. Below are the posterior covariances output by the nonlinear SSMs considered \u2013 qualitatively, we observe more complex covariance structures arise when the ball hits the wall that diagonal approximations cannot capture. ", "page_idx": 20}, {"type": "text", "text": "E.5 MC_Maze ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The monkey reaching dataset of30 was the first real dataset examined in the main text. For this dataset, we partitioned 1800/200/200 training/validation/testing trials sampled at 20ms per bin. All methods are trained for 1000 epochs for 3 different random seeds. The generative model is parameterized as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\,\\,L=40}\\\\ &{\\bullet\\,\\,N=182}\\\\ &{\\bullet\\,\\,T=35}\\\\ &{\\bullet\\,\\,\\mathrm{likelihood}}\\\\ &{\\quad\\,-\\,\\,p_{\\psi}(\\mathbf{y}_{t}\\,|\\,\\mathbf{z}_{t})=\\mathrm{Poisson}(\\mathbf{z}_{t}\\,|\\,\\mathbf{C}_{\\psi}(\\mathbf{z}_{t})+\\mathbf{b})}\\\\ &{\\quad\\,-\\,\\,\\mathbf{C}_{\\psi}:\\mathrm{Linear}(40,182)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For each method, inference is amortized using the following neural network architectures: ", "page_idx": 20}, {"type": "text", "text": "our inference network $\\begin{array}{r l}&{-\\ r_{\\alpha}=15}\\\\ &{-\\ r_{\\beta}=5}\\\\ &{-\\ \\alpha_{\\phi}\\colon\\mathrm{MLP}(182,128,640)}\\\\ &{-\\ \\beta_{\\phi}\\colon\\mathrm{[GRU}(128),\\mathrm{Linear}(128,240)]}\\end{array}$   \n\u2022 DKF inference network \u2013 $\\mathbf{u}_{\\phi}$ : GRU(128) \u2013 (m\u03c6, log P\u03c6): MLP(128, 128, 80) ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{\\mu}-\\mathbf{\\deltau}_{\\phi}\\colon[\\mathrm{GRU}(128)]}\\\\ &{\\phantom{=}-\\mathbf{\\mu}(\\mathbf{m}_{\\phi},\\log\\mathbf{P}_{\\phi})\\colon\\mathrm{MLP}(128,128,80)}\\\\ &{\\phantom{=}\\bullet\\mathrm{\\normalfont~SVAE~inference~network}}\\\\ &{\\phantom{=}-\\mathbf{\\mu}\\alpha_{\\phi}\\colon\\mathrm{MLP}(182,256,1640)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Optimization and training details: ", "page_idx": 21}, {"type": "text", "text": "\u2013 optimizer: Adam(lr = 0.001) \u2013 batch size: 128 ", "page_idx": 21}, {"type": "image", "img_path": "Ln8ogihZ2S/tmp/ed18cfe14628da8e6fe1fec9f06d47fac72da1aa9efe4fb7462f8d5dbe2f5ac5.jpg", "img_caption": ["Figure 6: XFADS predictive capabilities on real data. On the left are example reaches the monkey made during several trials. Using the learned model from the monkey reaching experiment, we filtered neural activity starting from $-240\\mathrm{ms}$ up until $-80\\mathrm{ms}$ before movement onset (each vertical panel represents the result from filtering more and more data) and unroll the final filtered states through the dynamics (starting from the dashed red line) to make predictions. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "E.6 DMFC_RSG ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The second real dataset examined was the timing interval reproduction task of32 samples at $10\\mathrm{ms}$ bins. For this dataset, we partitioned 700/150/150 training/validation/testing trials. All methods are trained for 1000 epochs for 3 different random seeds. The generative model is parameterized as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{{\\bullet}\\,\\,L=40}\\\\ &{\\bullet\\,\\,N=54}\\\\ &{\\bullet\\,\\,T=130}\\\\ &{\\bullet\\,\\,{\\mathrm{likelihood}}}\\\\ &{\\quad\\,-\\,\\,p_{\\psi}(\\mathbf{y}_{t}\\,|\\,\\mathbf{z}_{t})={\\mathrm{Poisson}}(\\mathbf{z}_{t}\\,|\\,\\mathbf{C}_{\\psi}(\\mathbf{z}_{t})+\\mathbf{b})}\\\\ &{\\quad\\,-\\,\\,\\mathbf{C}_{\\psi}:\\mathrm{Linear}(40,54)}\\end{array}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For each method, inference is amortized using the following neural network architectures: ", "page_idx": 21}, {"type": "text", "text": "\u2022 our inference network $\\begin{array}{r l}{\\phantom{\\omega}}&{=\\,r_{\\alpha}=15}\\\\ &{-\\phantom{\\omega}r_{\\beta}=5}\\\\ &{-\\phantom{\\omega}\\alpha_{\\phi}\\colon\\mathrm{MLP}(54,128,640)}\\\\ &{-\\phantom{\\omega}\\beta_{\\phi}\\colon\\mathrm{[GRU}(128),\\mathrm{Linear}(128,240)]}\\end{array}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{~\\,~\\,-~}\\mathbf{u}_{\\phi}\\colon\\mathrm{GRU}(128)}\\\\ &{\\mathrm{~\\,~\\,-~}\\left(\\mathbf{m}_{\\phi},\\log\\mathbf{P}_{\\phi}\\right)\\colon\\mathrm{MLP}(128,128,80)}\\\\ {*}&{\\mathrm{DVBF~inference~network}}\\\\ &{\\mathrm{~\\,~\\,~-~}\\mathbf{u}_{\\phi}\\colon\\mathrm{[GRU(128)]}}\\\\ &{\\mathrm{~\\,~\\,~-~}\\left(\\mu_{\\phi},\\log\\sigma_{\\phi}\\right)\\colon\\mathrm{MLP}(128,128,80)}\\\\ {*}&{\\mathrm{SVAE~inference~network}}\\\\ &{\\mathrm{~\\,~\\,~-~}\\alpha_{\\phi}\\colon\\mathrm{MLP}(256,128,1640)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Optimization and training details: ", "page_idx": 22}, {"type": "text", "text": "\u2013 optimizer: Adam(lr = 0.001) \u2013 batch size: 128 ", "page_idx": 22}, {"type": "table", "img_path": "Ln8ogihZ2S/tmp/367335cd44257769a81224abff34237fee2f8e0228ab91bff86bb286117ad49c.jpg", "table_caption": ["E.7 Collated results "], "table_footnote": ["Figure 7: Collated results including the latent SDE $({\\bf L}{\\bf-}{\\bf S}{\\bf D}{\\bf E})^{37}$ . We ran an additional baseline on pendulum/bouncing ball/monkey reaching datasets. L-SDE achieves results with a similar level of performance as the other models. Listed are the $R^{2}$ values for decoding angular velocity/x-y position/reach velocity from the latent representations learned for each dataset in both smoothing and prediction regimes. "], "page_idx": 22}, {"type": "text", "text": "F Useful expressions ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "mean/natural parameter inner product One common expression that frequently arises is the inner product between a mean and natural parameter, i.e. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mu_{t}^{\\top}\\alpha_{t}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where in the Gaussian case if the mean/natural parameter coordinates are, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mu_{t}=\\left({\\begin{array}{c}{\\mathbf{m}_{t}}\\\\ {-{\\frac{1}{2}}(\\mathbf{P}_{t}+\\mathbf{m}_{t}\\mathbf{m}_{t}^{\\top})}\\end{array}}\\right)\\qquad\\qquad\\qquad\\qquad\\alpha_{t}=\\left({\\begin{array}{c}{\\mathbf{a}_{t}}\\\\ {\\mathbf{A}_{t}\\mathbf{A}_{t}^{\\top}}\\end{array}}\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "then, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu_{t}^{\\top}\\alpha_{t}=\\mathbf{m}_{t}^{\\top}\\mathbf{a}_{t}-\\frac{1}{2}||\\mathbf{A}_{t}^{\\top}\\mathbf{m}_{t}||^{2}-\\frac{1}{2}\\operatorname{tr}(\\mathbf{A}_{t}^{\\top}\\mathbf{P}_{t}\\mathbf{A}_{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "difference of log partition functions Another common expression that frequently arises is given by, ", "page_idx": 23}, {"type": "equation", "text": "$$\nA(\\bar{\\pmb{\\lambda}}_{t}+\\alpha_{t})-A(\\bar{\\pmb{\\lambda}}_{t})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "so that if, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\bar{\\lambda}_{t}=\\left(\\bar{\\mathbf{P}}_{\\bar{\\mathbf{P}}_{t}^{-1}}^{-1}\\bar{\\mathbf{m}}_{t}\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "then, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A(\\bar{\\lambda}_{t}+\\alpha_{t})\\!-\\!A(\\bar{\\lambda}_{t})}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!=\\frac{1}{2}\\check{\\mathbf{m}}_{t}^{\\top}({\\bar{\\mathbf{P}}}_{t}^{-1}+\\mathbf{A}_{t}\\mathbf{A}_{t}^{\\top})\\check{\\mathbf{m}}_{t}-\\frac{1}{2}\\log|\\bar{\\mathbf{P}}_{t}^{-1}+\\mathbf{A}_{t}\\mathbf{A}_{t}^{\\top}|}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!-\\frac{1}{2}\\bar{\\mathbf{m}}_{t}{\\bar{\\mathbf{P}}}_{t}^{-1}\\bar{\\mathbf{m}}_{t}+\\frac{1}{2}\\log|\\bar{\\mathbf{P}}_{t}^{-1}|}\\\\ &{=\\frac{1}{2}\\left(||\\check{\\mathbf{m}}_{t}||_{\\bar{\\mathbf{P}}_{t}^{-1}}^{2}-||\\bar{\\mathbf{m}}_{t}||_{\\bar{\\mathbf{P}}_{t}^{-1}}^{2}+||\\mathbf{A}_{t}^{\\top}\\check{\\mathbf{m}}_{t}||^{2}-\\log|\\mathbf{I}+\\mathbf{A}_{t}^{\\top}\\bar{\\mathbf{P}}_{t}\\mathbf{A}_{t}|\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "conditional linear Gaussian log partition For a Gaussian distribution, the log-partition function in terms of the natural parameters, $\\pmb{\\lambda}=(\\mathbf{h},\\mathrm{vec}(\\mathbf{J}))$ , is given by ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A(\\mathbf{\\lambda})=\\frac{1}{2}\\mathbf{h}^{\\top}\\mathbf{J}^{-1}\\mathbf{h}-\\frac{1}{2}\\log|\\mathbf{J}|}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "so that, when ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{F}={\\binom{\\mathbf{Q}^{-1}\\mathbf{A}\\quad\\mathbf{0}}{\\mathbf{0}}}\\qquad\\mathbf{f}={\\binom{\\mathbf{0}}{\\mathbf{Q}^{-1}}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and $\\mathbf{u}^{\\top}=\\left(\\mathbf{u}_{1}^{\\top}\\quad\\mathrm{vec}(\\mathbf{u}_{2})^{\\top}\\right)$ is an arbitrary constant, we can write $A(\\mathbf{F}t(\\mathbf{z})+\\mathbf{f}+\\mathbf{u})=\\mathbf{a}^{\\top}t(\\mathbf{z})+b$ for some a and $b$ . To show this, we can just expand the log partition function ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A(\\mathbf{F}t(\\mathbf{z})+\\mathbf{f}+\\mathbf{u})=\\frac{1}{2}\\mathbf{z}^{\\top}\\mathbf{A}^{\\top}\\mathbf{Q}^{-1}(\\mathbf{Q}^{-1}+\\mathbf{u}_{2})^{-1}\\mathbf{Q}^{-1}\\mathbf{A}\\mathbf{z}+\\mathbf{z}^{\\top}\\mathbf{A}^{\\top}\\mathbf{Q}^{-1}(\\mathbf{Q}^{-1}+\\mathbf{u}_{2})^{-1}\\mathbf{u}_{1}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\frac{1}{2}\\mathbf{u}_{1}^{\\top}(\\mathbf{Q}^{-1}+\\mathbf{u}_{2})^{-1}\\mathbf{u}_{1}-\\frac{1}{2}\\log|\\mathbf{u}_{2}+\\mathbf{Q}^{-1}|}\\\\ &{\\qquad\\qquad\\qquad=\\mathbf{a}^{\\top}t(\\mathbf{z})+b}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "then a and $b$ can be identified as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbf{a}=\\left(-\\mathbf{A}^{\\top}\\mathbf{Q}^{-1}(\\mathbf{Q}^{-1}+\\mathbf{U}_{2})^{-1}\\mathbf{u}_{1}\\right)}&{{}b=\\frac{1}{2}\\mathbf{u}_{1}^{\\top}(\\mathbf{Q}^{-1}+\\mathbf{u}_{2})^{-1}\\mathbf{u}_{1}-\\frac{1}{2}\\log|\\mathbf{Q}^{-1}+\\mathbf{u}_{2}|}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then, for a LGSSM, we can evaluate the following expression which describes the difference between predictive and smoothed marginals, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda_{t}-\\bar{\\lambda}_{t}=\\alpha_{t}+\\nabla_{\\mu_{t}}\\mathbb{E}_{q(\\mathbf{z}_{t})}\\left[A\\left(\\lambda_{\\theta}(\\mathbf{z}_{t})+\\lambda_{t+1}-\\bar{\\lambda}_{t+1}\\right)-A\\left(\\lambda_{\\theta}(\\mathbf{z}_{t})\\right)\\right]}\\\\ &{\\qquad\\qquad=\\alpha_{t}+\\left(\\mathbf{A}^{\\top}\\mathbf{Q}^{-1}(\\mathbf{Q}^{-1}+\\mathbf{P}_{t+1}^{-1}-\\bar{\\mathbf{P}}_{t+1}^{-1})^{-1}(\\mathbf{h}_{t+1}-\\bar{\\mathbf{h}}_{t+1})\\right)}\\\\ &{\\qquad\\qquad=\\alpha_{t}+\\beta_{t+1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\mathbf{S}_{t+1}=\\mathbf{Q}^{-1}-\\mathbf{Q}^{-1}(\\mathbf{Q}^{-1}+\\mathbf{P}_{t+1}^{-1}-\\Bar{\\mathbf{P}}_{t+1}^{-1})^{-1}\\mathbf{Q}^{-1}.$ ", "page_idx": 23}, {"type": "text", "text": "G Forward KL fixed point ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Using the moment matching property of the fixed point solution to the forward KL objective, we can write ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\mu}_{t+1}^{*}=\\int t({\\mathbf{z}}_{t+1})\\mathbb{E}_{q_{t}}\\left[p_{\\theta}({\\mathbf{z}}_{t+1}\\mid{\\mathbf{z}}_{t})\\right]\\mathrm{d}{\\mathbf{z}}_{t+1}\\qquad\\qquad\\qquad\\qquad(112)}\\\\ &{\\qquad=\\int t({\\mathbf{z}}_{t+1})}&{(113)}\\\\ &{\\qquad\\times\\left[\\int h({\\mathbf{z}}_{t})h({\\mathbf{z}}_{t+1})\\exp\\left(t({\\mathbf{z}}_{t+1})^{\\top}{\\lambda}_{\\theta}({\\mathbf{z}}_{t})+t({\\mathbf{z}}_{t})^{\\top}{\\lambda}_{t}-A({\\boldsymbol{\\lambda}}_{t})-A({\\boldsymbol{\\lambda}}_{\\theta}({\\mathbf{z}}_{t})\\right))\\,\\mathrm{d}{\\mathbf{z}}_{t}\\right]\\mathrm{d}{\\mathbf{z}}_{t+1}}\\\\ &{\\qquad=\\int h({\\mathbf{z}}_{t})\\exp\\left(t({\\mathbf{z}}_{t})^{\\top}{\\lambda}_{t}-A({\\boldsymbol{\\lambda}}_{t})\\right)}&{(114)}\\\\ &{\\qquad\\times\\left[\\int t({\\mathbf{z}}_{t+1})h({\\mathbf{z}}_{t+1})\\exp\\left(t({\\mathbf{z}}_{t+1})^{\\top}{\\lambda}_{\\theta}({\\mathbf{z}}_{t})-A({\\boldsymbol{\\lambda}}_{\\theta}({\\mathbf{z}}_{t}))\\right)\\,\\mathrm{d}{\\mathbf{z}}_{t+1}\\right]\\mathrm{d}{\\mathbf{z}}_{t}}\\\\ &{\\qquad=\\mathbb{E}_{q_{t}}\\left[\\mu_{{\\mathbf{z}}}({\\mathbf{z}}_{t})\\right]}&{(115)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This result holds for any setting where stochastic transitions, $p_{t+1|t}$ , and the approximate marginal distributions, $q_{t}$ , are restricted to the same exponential family distribution. ", "page_idx": 24}, {"type": "text", "text": "H Linear Gaussian (information form) smoothing ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Using the derived relation, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbf{P}_{t}^{-1}=\\bar{\\mathbf{P}}_{t}^{-1}+\\mathbf{C}^{\\top}\\mathbf{R}^{-1}+\\mathbf{F}^{\\top}\\mathbf{S}_{t+1}\\mathbf{F}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "means that, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbf{P}_{t}^{-1}-\\bar{\\mathbf{P}}_{t}^{-1}=\\mathbf{C}^{\\top}\\mathbf{R}^{-1}\\mathbf{C}+\\mathbf{F}^{\\top}\\mathbf{S}_{t+1}\\mathbf{F}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "so that using the definition of $\\mathbf{S}_{t}$ (rewritten for convenience) then plugging in, we get, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{S}_{t}=\\mathbf{Q}^{-1}-\\mathbf{Q}^{-1}(\\mathbf{Q}^{-1}+\\mathbf{P}_{t}^{-1}-\\Bar{\\mathbf{P}}_{t}^{-1})^{-1}\\mathbf{Q}^{-1}}\\\\ &{\\quad=\\mathbf{Q}^{-1}-\\mathbf{Q}^{-1}(\\mathbf{Q}^{-1}+\\mathbf{C}^{\\top}\\mathbf{R}^{-1}\\mathbf{C}+\\mathbf{F}^{\\top}\\mathbf{S}_{t+1}\\mathbf{F})\\mathbf{Q}^{-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which gives a recurrence backward in time for $\\mathbf{S}_{1:T}$ starting from $\\mathbf{S}_{T+1}=\\mathbf{0}$ . Now since, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbf{h}_{t}-\\bar{\\mathbf{h}}_{t}=\\mathbf{F}^{\\top}\\mathbf{Q}^{-1}(\\mathbf{Q}^{-1}+\\mathbf{P}_{t+1}^{-1}-\\bar{\\mathbf{P}}_{t+1}^{-1})^{-1}(\\mathbf{h}_{t+1}-\\bar{\\mathbf{h}}_{t+1})\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and, ", "page_idx": 24}, {"type": "equation", "text": "$$\n(\\mathbf{Q}^{-1}+\\mathbf{C}^{\\top}\\mathbf{R}^{-1}\\mathbf{C}+\\mathbf{F}^{\\top}\\mathbf{S}_{t+1}\\mathbf{F})^{-1}=\\mathbf{Q}-\\mathbf{Q}\\mathbf{S}_{t}\\mathbf{Q}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "we have that, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbf{F}^{\\top}\\mathbf{s}_{t}=\\mathbf{F}^{\\top}\\mathbf{Q}^{-1}(\\mathbf{Q}^{-1}+\\mathbf{C}^{\\top}\\mathbf{R}^{-1}\\mathbf{C}+\\mathbf{F}^{\\top}\\mathbf{S}_{t+1}\\mathbf{F})^{-1}(\\mathbf{C}^{\\top}\\mathbf{R}^{-1}\\mathbf{y}_{t+1}+\\mathbf{F}^{\\top}\\mathbf{s}_{t+1})\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which simplifies into a recursion for $\\mathbf{s}_{1:T}$ backward in time starting from $\\mathbf s_{T+1}=\\mathbf0$ , given by, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{s}_{t}=\\mathbf{Q}^{-1}(\\mathbf{Q}^{-1}+\\mathbf{C}^{\\top}\\mathbf{R}^{-1}\\mathbf{C}+\\mathbf{F}^{\\top}\\mathbf{S}_{t+1}\\mathbf{F})^{-1}(\\mathbf{C}^{\\top}\\mathbf{R}^{-1}\\mathbf{y}_{t+1}+\\mathbf{F}^{\\top}\\mathbf{s}_{t+1})}\\\\ &{\\quad=(\\mathbf{I}-\\mathbf{S}_{t}\\mathbf{Q})(\\mathbf{C}^{\\top}\\mathbf{R}^{-1}\\mathbf{y}_{t+1}+\\mathbf{F}^{\\top}\\mathbf{s}_{t+1})}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Justification: we try to clearly state our contributions. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We discuss potential limitations of the work. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and   \na complete (and correct) proof?   \nAnswer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We list assumptions about the types of generative models we consider. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results. \u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. ", "page_idx": 25}, {"type": "text", "text": "\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems. \u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. \u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We provide experimental details in the appendix that include the architectures and training regimes.   \nGuidelines:   \n\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes]   \nJustification: We provide sufficient instructions and algorithmic details. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. ", "page_idx": 26}, {"type": "text", "text": "\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We provide details about splits, optimization, and parameterizations in the appendix.   \nGuidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate   \ninformation about the statistical significance of the experiments?   \nAnswer: [Yes] Justification: We discuss the neuroscientific implications of the results our model could be used to discover from neural data.   \nGuidelines: ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: we provide our computing setup. Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide the specifications of our computing setup. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: we adhere to the code of ethics. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Justification: We discuss the broader impacts. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?   \nAnswer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: the paper poses no such risks Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: we produced all code to run the experiments, we cite real data from where they were procured.   \nGuidelines:   \n\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation   \nprovided alongside the assets?   \nAnswer: [NA]   \nJustification: the paper does not release new assets   \nGuidelines:   \n\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?   \nAnswer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing or human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA]   \nJustification: this paper does not involve human subjects. Guidelines:   \n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}]