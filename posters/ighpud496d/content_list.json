[{"type": "text", "text": "Provable Editing of Deep Neural Networks using Parametric Linear Relaxation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhe Tao University of California, Davis Davis, CA 95616, USA zhetao@ucdavis.edu ", "page_idx": 0}, {"type": "text", "text": "Aditya V. Thakur University of California, Davis Davis, CA 95616, USA avthakur@ucdavis.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ensuring that a DNN satisfies a desired property is critical when deploying DNNs in safety-critical applications. There are efficient methods that can verify whether a DNN satisfies a property, as seen in the annual DNN verification competition (VNN-COMP). However, the problem of provably editing a DNN to satisfy a property remains challenging. We present PREPARED,1the first efficient technique for provable editing of DNNs. Given a DNN $\\mathcal{N}$ with parameters $\\theta$ , input polytope $\\mathrm{P}$ , and output polytope Q, PREPARED finds new parameters $\\dot{\\pmb{\\theta}}$ such that $\\forall\\mathbf{x}\\in\\mathrm{P}$ . $\\mathcal{N}(\\mathbf{x};\\dot{\\pmb{\\theta}})\\in\\mathrm{Q}$ while minimizing the changes $\\lVert\\dot{{\\boldsymbol{\\theta}}}-{\\boldsymbol{\\theta}}\\rVert$ . Given a DNN and a property it violates from the VNN-COMP benchmarks, PREPARED is able to provably edit the DNN to satisfy this property within 45 seconds. PREPARED is efficient because it relaxes the NP-hard provable editing problem to solving a linear program. The key contribution is the novel notion of Parametric Linear Relaxation, which enables PREPARED to construct tight output bounds of the DNN that are parameterized by the new parameters $\\dot{\\theta}$ . We demonstrate that PREPARED is more efficient and effective compared to prior DNN editing approaches i) using the VNN-COMP benchmarks, ii) by editing CIFAR10 and TINYIMAGENET image-recognition DNNs, and BERT sentiment-classification DNNs for local robustness, and iii) by training a DNN to model a geodynamics process and satisfy physics constraints. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ensuring that a DNN is correct and avoids harmful behaviors is critical when deploying them, especially in safety-critical contexts [8]. In such scenarios, it is vital to guarantee that a DNN satisfies a given property, e.g., a safety specification [35]. Towards this end, DNN verifiers aim to determine whether a DNN satisfies the given property [1]. There are efficient methods for DNN verification that can handle a wide-range of DNN architectures and properties, as seen in the annual DNN verification competition (VNN-COMP) [5]. However, the success of DNN verification reveals the next challenge: provably editing a DNN to satisfy a property. Formally, we define the provable editing problem as: ", "page_idx": 0}, {"type": "text", "text": "Definition 1.1. Given a DNN $\\mathcal{N}$ with parameters $\\theta$ , an input polytope $\\mathrm{P}\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\left\\{\\mathbf{x}\\mid\\mathbf{Dx}\\leq\\mathbf{e}\\right\\}$ and an output polytope ${\\textsc{Q}}{\\stackrel{\\mathrm{def}}{=}}\\left\\{\\mathbf{y}\\mid\\mathbf{A}\\mathbf{y}\\leq\\mathbf{b}\\right\\}$ , the provable editing problem is to find new parameters $\\dot{\\pmb\\theta}$ that ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}{\\|\\dot{\\theta}-\\theta\\|}\\quad\\mathrm{s.t.}\\quad\\forall\\mathbf{x}\\in\\mathrm{P.}\\,\\mathcal{N}(\\mathbf{x};\\dot{\\theta})\\in\\mathrm{Q}\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "Provable editing can be used, for instance, to ensure that a classifier DNN is locally robust for an input $\\mathbf{x}$ . In this case, the input polytope $\\mathrm{P}\\stackrel{\\mathrm{def}}{=}\\{\\mathbf{x}^{\\prime}\\mid\\|\\mathbf{x}-\\mathbf{x}^{\\prime}\\|_{\\infty}\\leq\\varepsilon\\}$ is the $L^{\\infty}$ ball around $\\mathbf{x}$ with radius $\\varepsilon$ , and the output polytope $\\mathrm{Q}\\ensuremath{\\stackrel{\\mathrm{def}}{=}}\\left\\{\\mathbf{y}\\mid\\bigwedge_{j\\neq l}\\mathbf{y}_{j}<\\mathbf{y}_{l}\\right\\}$ requires that the output label is $l$ ; viz., arg max $\\mathbf{y}=l$ . Moreover, provable editing can be used either during training to guarantee that the DNN satisfies a property, or post-deployment to repair a given DNN. ", "page_idx": 0}, {"type": "image", "img_path": "IGhpUd496D/tmp/4a3e015bc20797446137fc0fc0ebed612019defb95c3a71a3d8d265adf7b293a.jpg", "img_caption": ["Figure 1: Linear relaxations for ${\\pmb y}\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\mathrm{ReLU}({\\pmb x})$ . and denote the upper and lower bounds, denotes the relaxation. (a) Relaxation in prior approaches [49, 39], where the bounds are overapproximated using linear functions, and the upper bound is only sound within given input bounds $[{\\underline{{\\pmb x}}},{\\overline{{\\pmb x}}}]$ ; (b) This work exactly represents the upper bound for any input upper bound variable $\\dddot{x}$ , results in a tighter approximation; (c) Illustration of how this work exactly represents the upper bound $\\ddot{y}$ ( ) by capturing the epigraph $\\left(\\!\\!\\left[::::\\right]\\!\\!\\right)$ of $\\mathtt{R e L U}(\\ddot{{\\boldsymbol{x}}})$ with the linear constraint $\\pmb{\\ddot{y}}\\geq0\\wedge\\pmb{\\ddot{y}}\\geq\\pmb{\\ddot{x}}$ . The lower bound $\\dot{\\underline{{y}}}\\left(-\\mathrm{-}\\right)$ uses a linear relaxation $\\dot{\\underline{{\\pmb y}}}\\ {\\stackrel{\\mathrm{def}}{=}}\\ c\\dot{\\underline{{x}}}$ with a constant $\\pmb{c}\\in[0,1]$ . "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The provable editing problem is challenging due to the presence of the universal quantifier; viz., finding new parameters such that for all inputs in the polytope $\\mathrm{P}$ , the output of the edited network lies in the polytope Q. There are no existing approaches for efficiently and effectively solving the provable editing problem. Regularization-based approaches [26, 10, 28, 24] encode the property into the loss during training, but are unable to guarantee the property-satisfaction. SMT-based approaches [14, 16] provide guarantees, but are not efficient because they directly solve an NP-hard problem. Prior LP-based approaches [41, 42] are efficient because they can only handle a restricted class of provable editing problems; e.g., they assume that the vertices of the input polytope or the linear regions of the DNN can be efficiently enumerated, or the DNN architecture can be modified. ", "page_idx": 1}, {"type": "text", "text": "This paper presents PREPARED, the first efficient technique for provable editing of DNNs that runs in polynomial time (Theorem 3.4). Given a DNN and a property it violates from the VNN $-\\mathsf{C O M P}^{\\prime}22$ benchmarks, PREPARED is able to provably edit the DNN to satisfy this property within 45 seconds. ", "page_idx": 1}, {"type": "text", "text": "PREPARED approaches the problem of provable editing by constructing tight parametric bounds of the output $\\mathcal{N}(\\ensuremath{\\mathbf{x}};\\dot{\\pmb{\\theta}})$ for all inputs xxx in the input polytope $\\mathrm{P}$ in terms of the parameters $\\dot{\\pmb{\\theta}}$ , and then constraining these parametric bounds to lie within the output polytope Q. These parametric bounds are constructed compositionally per layer of the DNN, and are expressed as linear constraints, so that efficient Linear Programming (LP) solvers can be used to find an optimal solution. ", "page_idx": 1}, {"type": "text", "text": "Our key insight is to represent the parametric bounds indirectly via underapproximations of the epigraph and hypograph of the DNN layer. Consider the ReLU layer ${\\pmb y}\\ \\overset{\\mathrm{def}}{=}\\ \\mathrm{ReLU}({\\pmb x})$ . Prior approaches [49, 39] overapproximate its upper bound with a linear function ( in Figure 1(a)) using constant input bounds $[{\\underline{{\\pmb x}}},{\\overline{{\\pmb x}}}]$ . Such a linear relaxation is loose, and is only sound within the given constant input bounds $[{\\underline{{x}}},{\\overline{{\\mathbf{x}}}}]$ . Our approach is able to exactly represent the upper bound of ReLU output for any input upper bound variable $\\dddot{\\pmb{x}}$ ( in Figure 1(b)). This is done by capturing the epigraph ( in Figure 1(c)) of $\\mathtt{R e L U}(\\overleftarrow{\\boldsymbol{x}})$ with a linear constraint. ", "page_idx": 1}, {"type": "text", "text": "Our key contribution is a novel Parametric Linear Relaxation for DNN layers, which defines tight parametric bounds using linear constraints in terms of the layer parameters. Consider vvv= xxxwww, which is a simplified representation of an Affine layer. We would like to construct bounds for all $\\pmb{x}\\in[\\underline{{\\pmb{x}}},\\overline{{\\pmb{x}}}]$ in terms of the variable parameter $\\dot{w}$ . Prior approaches could achieve a sound, but loose linear relaxation that is not in terms of the layer parameters (Figure 2(a)). In contrast, our approach exactly represents the output bounds without any relaxation (Figure 2(b)) building upon our key insight of capturing the epigraph and hypograph of the DNN layer in terms of the parameters (Figure 2(c)). ", "page_idx": 1}, {"type": "image", "img_path": "IGhpUd496D/tmp/7746e6d1b47a294f3728038415cb29434090d634b84f65e6dee2a1422c83209b.jpg", "img_caption": ["Figure 2: Linear relaxations for bounding $\\pmb{v}\\ \\frac{\\mathrm{def}}{\\mathrm{\\pmb{\\sigma}}}\\pmb{x}\\dot{w}$ for all ${\\pmb x}\\in[{\\underline{{\\pmb x}}},{\\overline{{\\pmb x}}}]$ , with variable parameter $\\dot{w}$ and denote the upper and lower bound overapproximations. $\\sqsubset$ denotes the relaxation. (a) A loose relaxation for all $\\pmb{x}\\in[\\underline{{\\pmb{x}}},\\overline{{\\pmb{x}}}]$ and for all $\\pmb{\\dot{w}}\\in[\\underline{{\\pmb{w}}},\\overline{{\\pmb{w}}}]$ , where $\\dot{\\pmb w}$ is not treated as a parameter but another input within constant bounds $[\\underline{{w}},\\overline{{w}}]$ . (b) This work exactly represents the bounds for all inputs $\\pmb{x}\\in[\\pmb{\\underline{{x}}},\\overline{{\\pmb{x}}}]$ , parameterized by variable $\\dot{\\pmb{w}}$ , without any relaxation. Here we plot a case when $\\dot{w}<0$ , while our parametric bounds are exact for any $\\dot{w}$ . (c) Illustration of how our parametric bounds are defined for the function $\\pmb{v}\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\pmb{x}\\dot{w}$ in terms of the variable parameter $\\dot{\\pmb{w}}$ . denotes the true output region for all slope (input) $\\pmb{x}\\in[\\underline{{\\pmb{x}}},\\overline{{\\pmb{x}}}]$ . The upper bound $\\dddot{v}$ ( ) is exactly captured by the epigraph $(\\![::::]\\!)$ , defined by linear constraint $(\\\"\\!\\!{v}\\geq\\underline{{x}}\\dot{w}\\wedge\\\"\\!\\!{v}\\geq\\overline{{x}}\\dot{w})$ ); the lower bound $\\underline{{\\dot{v}}}\\left(-\\rule{0ex}{0ex}\\right)$ is exactly captured by the hypograph $\\left(\\mathbb{E}^{:::}\\right)$ , defined by linear constraint $(\\underline{{\\dot{v}}}\\leq\\underline{{x}}\\dot{w}\\wedge\\underline{{\\dot{v}}}\\leq\\overline{{x}}\\dot{w})$ ). "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "We evaluate PREPARED i) using the VNN-COMP benchmarks, ii) by editing CIFAR10 and TINYIMAGENET image-recognition DNNs, and BERT sentiment-classification DNNs for local robustness, and iii) by training a DNN to model a geodynamics process and satisfy physics constraints. Because there were no prior efficient approaches for provable editing, we implemented new provable finetuning baselines by integrating prior DNN editing approaches with a verifier in the loop. PREPARED outperformed such baselines demonstrating its effectiveness and efficiency. ", "page_idx": 2}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Regularized training [20, 45, 30, 10, 19, 22, 47] incorporates constraints as regularization into the training loss, but does not guarantee constraint-satisfaction in the trained DNN. For the DNN editing problem we are addressing, which involves universally-quantified logical formulas, DL2 [10] is the state-of-the-art regularized training method. ", "page_idx": 2}, {"type": "text", "text": "Certified training [26, 15, 36, 2, 31, 28, 24] is a type of regularized training geared towards adversarial robustness. SABR [28] and STAPS [24] are state-of-the-art certified training approaches. However, they also do not guarantee constraint-satisfaction. ", "page_idx": 2}, {"type": "text", "text": "SMT-based editing approaches [14, 23, 16] directly solve the NP-hard provable editing problem using an SMT solver. Thus, they are inefficient and do not scale beyond small DNNs. For instance, the recent DeepSaDe approach [16] incorporates an SMT solver during training, but only uses the SMT solver to edit the last layer. However, it still can take $500\\times$ longer than regularized training. ", "page_idx": 2}, {"type": "text", "text": "LP-based editing approaches [41, 42] relax the provable editing problem to solving an LP problem, which makes them efficient. However, prior approaches can only handle a restricted class of provable editing problems. The state-of-the-art provable editing approach APRNN [42] is efficient for editing input points. However, it does not scale to the general provable editing problem of Definition 1.1 due to the need for enumerating vertices of the input polytope. PRDNN [41] was the precursor to APRNN and suffers from similar limitations; moreover, PRDNN modifies the architecture of DNN and requires enumeration of linear regions to edit an input polytope. REASSURE [12] repairs a linear region of a DNN by adding a patch DNN for each such linear region. Thus, it also requires enumeration of linear regions to edit an input polytope. Further, it has been shown to be unsound and incomplete (Section 5 of [42]), and suffers from significant runtime and memory overheads (Section 7.8 of [42]). ", "page_idx": 2}, {"type": "text", "text": "DNN verification [38, 39, 49, 44, 37, 48, 46, 9, 4] aims to determine whether a DNN satisfies a given property. As shown in Section 4, provable editing can be used when a verifier determines that a DNN violates a property. Most verification techniques use linear relaxations to bound the DNN output. However, these linear relaxations cannot be used for provable editing because they are not parameterized in terms of the DNN parameters. Instead, verification techniques assume that the parameters are constant and focus on designing linear relaxations for activation layers like ReLU [34, 43]. However, it is not clear how even these activation layer relaxations could be used for provable editing. For instance, these relaxations require constant input bounds; loose constant input bounds result in loose linear relaxations. In the context of provable editing, if the ReLU layer follows an Affine layer with editable parameters, then the constant input bounds to the ReLU layer would be very loose (e.g., $[-\\infty,\\infty])$ in order to be sound for all possible edits. Hence, the resulting linear relaxation would also be loose. In contrast, our technique does not require constant input bounds for any layer, and produces an exact upper bound for the ReLU layer (Theorem 3.9). Applying our Parametric Linear Relaxation to DNN verification remains future work. ", "page_idx": 3}, {"type": "text", "text": "3 Provable editing of DNNs using Parametric Linear Relaxation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We use $\\pmb{x}\\in\\mathbb{R}$ to denote a scalar, $\\mathbf{x}\\in\\mathbb{R}^{m}$ to denote a column vector, and $\\mathbf{W}\\in\\mathbb{R}^{n\\times m}$ to denote a matrix. Blue-colored variables with a dot denote LP decision variables, e.g., xxx, xxx, WWW, bbb, etc. The proofs for all theorems can be found in Appendix A. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.1 (DNN). Let $\\mathbf{y}\\ {\\overset{\\underset{\\mathrm{def}}{}}{=}}\\ {\\mathcal{N}}(\\mathbf{x};\\theta)$ denote an $L$ -layer deep neural network (DNN) $\\mathcal{N}$ with parameters $\\theta$ , input $\\mathbf{x}$ and output yyy. The DNN is defined iteratively as $\\mathbf{x}^{(\\ell+1)}\\,{\\overset{\\mathrm{def}}{=}}\\,{\\mathcal{N}}^{(\\ell)}(\\mathbf{x}^{(\\ell)};\\theta^{(\\ell)})$ for each layer $\\mathcal{N}^{(\\ell)}$ with parameters $\\theta^{(\\ell)}$ , where $\\mathbf{x}^{(0)}\\,{\\stackrel{\\mathrm{def}}{=}}\\,\\mathbf{x},\\mathbf{y}\\,{\\stackrel{\\mathrm{def}}{=}}\\,\\mathbf{x}^{(L)}$ and $0\\leq\\ell<L$ . \u25a0 ", "page_idx": 3}, {"type": "text", "text": "For ease of exposition, we defer our approach for the general provable editing problem of Definition 1.1 to Appendix B. In this section, we consider the following provable interval editing problem, where the input and output constraints are constant interval bounds: ", "page_idx": 3}, {"type": "text", "text": "Definition 3.2. Given a DNN $\\mathcal{N}$ with parameters $\\theta$ , constant input bounds $[\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}}]$ and constant output bounds $[\\underline{{\\mathbf{y}}},\\overline{{\\mathbf{y}}}]$ , the provable interval editing problem is to find new parameters $\\dot{\\pmb{\\theta}}$ that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\|\\dot{{\\boldsymbol\\theta}}-{\\boldsymbol\\theta}\\|\\quad\\mathrm{s.t.~}\\quad\\forall\\mathbf{x}\\in[\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}}].\\,\\mathcal{N}(\\mathbf{x};\\dot{{\\boldsymbol\\theta}})\\in[\\underline{{\\mathbf{y}}},\\overline{{\\mathbf{y}}}]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3.1 Parametric Linear Relaxation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Consider a DNN $\\mathcal{N}$ with variable parameters $\\dot{\\pmb{\\theta}}$ and input bounds [xxx,xxx], our goal is to construct sound parametric output bounds [yyy,yyy] of $\\mathcal{N}$ in terms of $\\dot{\\pmb\\theta}$ and [xxx,xxx]. The exact definition is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\dot{\\underline{{\\mathbf{y}}}}\\leq\\operatorname*{min}\\left\\{{\\mathcal{N}}(\\mathbf{x};\\dot{\\theta})~\\left|~\\dot{\\underline{{\\mathbf{x}}}}\\leq\\mathbf{x}\\leq\\ddot{\\mathbf{x}}\\right\\}\\wedge\\ddot{\\mathbf{y}}\\geq\\operatorname*{max}\\left\\{{\\mathcal{N}}(\\mathbf{x};\\dot{\\theta})~\\left|~\\dot{\\underline{{\\mathbf{x}}}}\\leq\\mathbf{x}\\leq\\ddot{\\mathbf{x}}\\right\\}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "However, this universally-quantified non-linear formula is expensive to solve [6, 7]. Our key contribution is a novel Parametric Linear Relaxation, a poly-size linear formula that implies Equation 3. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.3. The Parametric Linear Relaxation $\\underline{{\\overline{{\\varphi}}}}_{N}(\\underline{{\\dot{\\mathbf{y}}}},\\underline{{\\ddot{\\mathbf{y}}}},\\underline{{\\dot{\\mathbf{x}}}},\\underline{{\\ddot{\\mathbf{x}}}};\\dot{\\pmb{\\theta}})$ for $\\mathcal{N}$ is a poly-size linear formula that implies $\\forall\\mathbf{x}\\in$ [xxx,xxx]. $\\mathcal{N}(\\mathbf{x};\\dot{\\pmb{\\theta}})\\in\\mathcal{\\dot{\\lfloor}{\\pmb{\\underline{{\\mathbf{y}}}}}}$ ,yyy]. ", "page_idx": 3}, {"type": "text", "text": "In other words, any satisfying assignment $(\\underline{{\\mathbf{y}}},\\overline{{\\mathbf{y}}},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\theta)$ of the formula $\\underline{{\\overline{{\\varphi}}}}_{N}(\\underline{{\\dot{\\bf y}}},\\underline{{\\ddot{\\bf y}}},\\underline{{\\dot{\\bf x}}},\\underline{{\\ddot{\\bf x}}};\\dot{\\pmb{\\theta}})$ yields sound output bounds $[\\underline{{\\mathbf{y}}},\\overline{{\\mathbf{y}}}]$ for $\\mathcal{N}$ , viz., $\\forall\\mathbf{x}\\in[\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}}]$ . $\\mathcal{N}(\\mathbf{x};\\theta)\\;\\in\\;[\\underline{{\\mathbf{y}}},\\overline{{\\mathbf{y}}}]$ . The size of the formula $\\underline{{\\overline{{\\varphi}}}}_{N}(\\underline{{\\dot{\\bf y}}},\\underline{{\\ddot{\\bf y}}},\\underline{{\\dot{\\bf x}}},\\underline{{\\ddot{\\bf x}}};\\dot{\\theta})$ is polynomial in the size of the DNN, i.e., the number of parameters, layers, and the input and output dimensions of each layer. ", "page_idx": 3}, {"type": "text", "text": "3.2 Provable editing via Parametric Linear Relaxation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The following theorem shows how Parametric Linear Relaxation can be used to solve the provable interval editing problem. ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.4. Given a provable interval editing problem (Definition 3.2) for DNN N and parameters $\\theta$ with input bounds $[\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}}]$ and output bounds $[\\underline{{\\mathbf{y}}},\\overline{{\\mathbf{y}}}]$ , let $\\underline{{\\overline{{\\varphi}}}}_{N}(\\underline{{\\dot{\\bf y}}},\\ddot{\\underline{{\\bf y}}},\\underline{{{\\bf x}}},\\overline{{{\\bf x}}};\\dot{\\theta})$ be a Parametric Linear Relaxation for $\\mathcal{N}$ . Then the following linear program can be solved in polynomial time in the size of the DNN $\\mathcal{N}$ , and whose solution is a solution to the provable interval editing problem: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\|\\dot{\\theta}-\\theta\\|\\quad s.t.\\quad\\overline{{\\underline{{\\varphi}}}}_{N}\\big(\\underline{{\\dot{\\bf y}}},\\underline{{\\ddot{\\bf y}}},\\underline{{\\underline{{\\bf x}}}},\\overline{{\\underline{{\\bf x}}}};\\dot{\\theta}\\big)\\wedge\\big(\\underline{{\\mathbf{y}}}\\leq\\underline{{\\dot{\\bf y}}}\\wedge\\ddot{\\mathbf{y}}\\leq\\overline{{\\mathbf{y}}}\\big)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3.3 Compositional construction of Parametric Linear Relaxation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Definition 3.5. Given an $L$ -layer DNN $\\mathcal{N}$ with parameters $\\dot{\\pmb{\\theta}}$ , constant input bounds $[\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}}]$ and Parametric Linear Relaxation $\\overline{{\\underline{{\\varphi}}}}_{N^{\\left(\\ell\\right)}}$ for each layer ${\\cal N}^{(\\ell)}.$ , we define the compositional Parametric Linear Relaxation $\\underline{{\\overline{{\\varphi}}}}_{N}(\\underline{{\\dot{\\mathbf{y}}}},\\underline{{\\ddot{\\mathbf{y}}}},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}};\\dot{\\boldsymbol{\\theta}})$ as follows, where [yyy,yyy]d=ef $[\\underline{{\\dot{\\bf x}}}^{(L)},\\ddot{\\bf x}^{(L)}]$ and $[\\underline{{\\dot{\\bf x}}}^{(0)},\\underline{{\\ddot{\\bf x}}}^{(0)}]\\stackrel{\\mathrm{def}}{=}[\\underline{{\\bf x}},\\overline{{\\bf x}}]$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\underline{{\\overline{{\\varphi}}}}_{N}\\big(\\underline{{\\dot{y}}},\\underline{{\\ddot{y}}},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}};\\dot{\\theta}\\big)\\overset{\\mathrm{def}}{=}\\bigwedge_{0\\leq\\ell<L}\\overline{{\\underline{{\\varphi}}}}_{N^{(\\ell)}}\\big(\\underline{{\\dot{\\mathbf{x}}}}^{(\\ell+1)},\\ddot{\\mathbf{x}}^{(\\ell+1)},\\underline{{\\dot{\\mathbf{x}}}}^{(\\ell)},\\ddot{\\mathbf{x}}^{(\\ell)};\\dot{\\theta}^{(\\ell)}\\big)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Theorem 3.6. Definition 3.5 is a Parametric Linear Relaxation for the DNN $\\mathcal{N}$ . ", "page_idx": 4}, {"type": "text", "text": "By Theorems 3.4 and 3.6, we see that to solve the provable editing problem, we just need to construct Parametric Linear Relaxations $\\underline{{\\overline{{\\varphi}}}}_{N^{(\\ell)}}\\big(\\underline{{\\dot{\\mathbf{y}}}},\\underline{{\\ddot{\\mathbf{y}}}},\\underline{{\\dot{\\mathbf{x}}}},\\ddot{\\mathbf{x}};\\dot{\\theta}\\big)$ for each DNN layer ${\\mathcal N}^{(\\ell)}$ . In practice, $\\underline{{\\overline{{\\varphi}}}}_{N^{(\\ell)}}\\big(\\underline{{\\dot{\\mathbf{y}}}},\\underline{{\\ddot{\\mathbf{y}}}},\\underline{{\\dot{\\mathbf{x}}}},\\ddot{\\mathbf{x}};\\dot{\\pmb{\\theta}}\\big)$ is defined by separate formulas $\\overline{{\\varphi}}_{\\mathcal{N}^{(\\ell)}}\\big(\\mathbf{\\ddot{y}},\\mathbf{\\dot{\\underline{{x}}}},\\mathbf{\\ddot{x}};\\dot{\\theta}\\big)$ and $\\underline{{\\varphi}}_{\\mathcal{N}^{(\\ell)}}\\big(\\underline{{\\dot{\\mathbf{y}}}},\\underline{{\\dot{\\mathbf{x}}}},\\underline{{\\ddot{\\mathbf{x}}}};\\dot{\\pmb{\\theta}}\\big)$ for the upper and lower bounds, respectively: $\\underline{{\\overline{{\\varphi}}}}_{N^{(\\ell)}}\\big(\\underline{{\\dot{\\mathbf{y}}}},\\underline{{\\ddot{\\mathbf{y}}}},\\underline{{\\dot{\\mathbf{x}}}},\\underline{{\\ddot{\\mathbf{x}}}};\\dot{\\theta}\\big)\\,\\stackrel{\\mathrm{def}}{=}\\,\\overline{{\\varphi}}_{N^{(\\ell)}}\\big(\\ddot{\\mathbf{y}},\\underline{{\\dot{\\mathbf{x}}}},\\ddot{\\mathbf{x}};\\dot{\\theta}\\big)\\,\\wedge\\,\\underline{{\\varphi}}_{N^{(\\ell)}}\\big(\\underline{{\\dot{\\mathbf{y}}}},\\underline{{\\dot{\\mathbf{x}}}},\\ddot{\\mathbf{x}};\\dot{\\theta}\\big)$ . For brevity, we describe how to construct Parametric Linear Relaxation for ReLU and Affine layers. Appendix C presents Parametric Linear Relaxations for Tanh, Sigmoid and ELU layers. ", "page_idx": 4}, {"type": "text", "text": "3.4 Parametric Linear Relaxation for ReLU layer ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Definition 3.7. $\\mathbf{y}\\ {\\overset{\\mathrm{def}}{=}}\\ \\mathrm{ReLU}(\\mathbf{x})$ with input $\\mathbf{x}\\in\\mathbb{R}^{m}$ and output $\\mathbf{y}\\in\\mathbb{R}^{m}$ is defined as ${\\mathbf{y}}\\,{\\stackrel{\\mathrm{def}}{=}}\\,\\operatorname*{max}\\{{\\mathbf{x}},0\\}$ . ", "page_idx": 4}, {"type": "text", "text": "Definition 3.8. For a ReLU layer with variable input bounds [xxx,xxx], its Parametric Linear Relaxation is defined as $\\begin{array}{r}{\\overline{{\\mathcal{L}}}_{\\mathtt{R e L U}}(\\underline{{\\dot{\\bf y}}},\\underline{{\\ddot{\\bf y}}},\\underline{{\\dot{\\bf x}}},\\underline{{\\ddot{\\bf x}}})\\overset{\\mathrm{def}}{=}\\overline{{\\varphi}}_{\\mathtt{R e L U}}(\\overline{{\\bf y}},\\underline{{\\ddot{\\bf x}}})\\wedge\\underline{{\\varphi}}_{\\mathtt{R e L U}}(\\underline{{\\dot{\\bf y}}},\\underline{{\\dot{\\bf x}}})}\\end{array}$ where: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\overline{{\\varphi}}_{\\mathtt{R e L U}}(\\mathbf{\\ddot{y}},\\ddot{\\mathbf{x}})\\ \\stackrel{\\mathrm{def}}{=}\\,\\ddot{\\mathbf{y}}\\geq\\ddot{\\mathbf{x}}\\wedge\\ddot{\\mathbf{y}}\\geq0\\qquad\\underline{{\\varphi}}_{\\mathtt{R e L U}}(\\underline{{\\dot{y}}},\\dot{\\underline{{\\mathbf{x}}}})\\ \\stackrel{\\mathrm{def}}{=}\\ \\underline{{\\dot{\\mathbf{y}}}}=\\mathbf{c}\\dot{\\underline{{\\mathbf{x}}}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and $\\mathbf{c}\\in\\mathbb{R}^{m}$ are constants such that $0\\leq\\mathbf{c}_{i}\\leq1$ . ", "page_idx": 4}, {"type": "text", "text": "Because ReLU is a convex function, as seen in Figure 1(c), the upper bound constraint $\\ddot{\\mathbf{y}}_{i}\\geq\\ddot{\\mathbf{x}}_{i}\\wedge\\ddot{\\mathbf{y}}_{i}\\geq0$ for each $\\mathbf{\\ddot{y}}_{i}$ exactly captures the epigraph $(\\mathbb{E}::::)$ of $\\mathbb{R}{\\in}\\mathrm{LU}(\\mathbf{\\ddot{x}}_{i})$ . For each lower bound $\\dot{\\underline{{\\mathbf{y}}}}_{i}$ , we use a linear relaxation $\\dot{\\underline{{\\mathbf{y}}}}_{i}=\\mathbf{c}_{i}\\dot{\\underline{{\\mathbf{x}}}}_{i}$ ( ) of $\\mathtt{R e L U}(\\dot{\\underline{{x}}})_{i}$ with a constant $0\\leq{\\bf c}_{i}\\leq1$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.9. Definition 3.8 is a Parametric Linear Relaxation for the ReLU layer that captures the exact upper bound. ", "page_idx": 4}, {"type": "text", "text": "3.5 Parametric Linear Relaxation for Affine layer ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Definition 3.10. $\\mathbf{y}\\overset{\\mathrm{def}}{=}\\mathtt{A f f}\\mathtt{i n e}(\\mathbf{x};\\mathbf{W},\\mathbf{b})$ with input $\\mathbf{x}\\in\\mathbb{R}^{m}$ , output $\\mathbf{y}\\in\\mathbb{R}^{n}$ , weight $\\mathbf{W}\\in\\mathbb{R}^{n\\times m}$ and bias $\\mathbf{b}\\in\\mathbb{R}^{n}$ is defined as $\\mathbf{y}\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\mathbf{W}\\mathbf{x}+\\mathbf{b}$ , which expands to $\\begin{array}{r}{\\mathbf{y}_{j}\\overset{\\mathrm{def}}{=}\\sum_{i=0}^{m}\\mathbf{x}_{i}\\mathbf{W}_{j i}+\\mathbf{b}_{j}}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "In DNN verification, bounding Affine layers is straightforward, because the parameters are constants. However, in provable editing, this is challenging, because the parameters are variables. In particular, we need to bound the multiplication $\\pmb{x}\\dot{\\pmb{w}}$ between the universally-quantified input $\\pmb{x}\\in[\\underline{{\\pmb x}},\\bar{\\pmb x}]$ and the variable weight $\\dot{w}$ . As seen in Figure 2(a), if the bounds for $\\pmb{x}\\dot{w}$ are not parameterized by $\\dot{w}$ , a linear relaxation would require constant bounds $[\\underline{{w}},\\overline{{w}}]$ for $\\dot{\\pmb{w}}$ , resulting in loose bounds. Our approach constructs tight parametric output bounds using Parametric Linear Relaxation. We now describe our approach for an Affine layer i) with constant input bounds $[\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}}]$ when it is the first layer of a DNN, and ii) with variable input bounds [xxx,xxx] when it is not. ", "page_idx": 4}, {"type": "image", "img_path": "IGhpUd496D/tmp/c1cc6ee06d78ba4789d5750a427f4172b36dbe4bc62bcad1812871d0c6e3ac31.jpg", "img_caption": ["Figure 3: Illustration of Parameterized Linear Relaxations for Tanh, Sigmoid and ELU. and denote the upper and lower bound approximations; and denotes the convex underapproximations $\\overline{{\\varphi}}(\\mathbf{\\ddot{y}},\\mathbf{\\tilde{x}})$ and $\\underline{{\\varphi}}(\\underline{{\\dot{y}}},\\dot{\\underline{{x}}})$ for the epigraph and hypograph; denotes relaxation. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "3.5.1 Affine layer with constant input bounds ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Definition 3.11. For an Affine layer with constant input bounds $[\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}}]$ and variable parameters WWW, bbb, its Parametric Linear Relaxation is defined as $\\underline{{\\varphi}}_{\\mathrm{Aff}}\\left(\\underline{{\\dot{\\bf y}}},\\underline{{\\ddot{\\bf y}}},\\underline{{\\bf x}},\\overline{{\\bf x}},\\dot{\\bf W},\\dot{\\bf b}\\right)\\overset{\\mathrm{def}}{=}\\overline{{\\varphi}}_{\\mathrm{Aff}}\\left(\\ddot{\\bf y},\\underline{{\\bf x}},\\overline{{\\bf x}},\\dot{\\bf W},\\dot{\\bf b}\\right)\\wedge$ \u03c6 (yyy,xxx,xxx,WWW,bbb) where: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{\\varphi}}_{\\mathrm{Aff}}\\left(\\ddot{\\mathbf{y}},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\dot{\\mathbf{W}},\\dot{\\mathbf{b}}\\right)\\overset{\\mathrm{def}}{=}\\bigwedge\\ddot{\\mathbf{y}}_{j}=\\sum_{i}\\overline{{\\mathbf{V}}}_{j i}+\\dot{\\mathbf{b}}_{j}\\,\\wedge\\,\\ddot{\\mathbf{V}}\\ge\\dot{\\mathbf{W}}_{\\underline{{\\mathbf{X}}}}\\,\\wedge\\,\\ddot{\\mathbf{V}}\\ge\\dot{\\mathbf{W}}\\overline{{\\mathbf{x}}}}\\\\ {\\underline{{\\varphi}}_{\\mathrm{Aff}}\\left(\\dot{\\underline{{\\mathbf{y}}}},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\dot{\\mathbf{W}},\\dot{\\mathbf{b}}\\right)\\overset{\\mathrm{def}}{=}\\bigwedge\\underset{j}{\\dot{\\mathbf{y}}}_{j}=\\sum_{i}\\dot{\\underline{{\\mathbf{V}}}}_{j i}+\\dot{\\mathbf{b}}_{j}\\,\\wedge\\,\\dot{\\mathbf{V}}\\le\\dot{\\mathbf{W}}_{\\underline{{\\mathbf{X}}}}\\,\\wedge\\,\\dot{\\underline{{\\mathbf{V}}}}\\le\\dot{\\mathbf{W}}\\overline{{\\mathbf{x}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The core of this definition is constructing parametric bounds $[\\underline{{\\dot{v}}},\\ddot{v}]$ for the multiplication $\\mathbf{\\Delta}x\\dot{w}$ for all $\\pmb{x}\\in[\\underline{{\\pmb{x}}},\\overline{{\\pmb{x}}}]$ in terms of the variable weight $\\dot{w}$ . As seen in Figure 2(c), for all $\\pmb{x}\\in[\\underline{{\\pmb{x}}},\\overline{{\\pmb{x}}}]$ , the upper bound ( ) of $\\mathbf{\\Delta}x\\dot{\\boldsymbol{w}}$ is defined by a piecewise-linear convex function $\\overline{{f}}(\\dot{\\pmb w})\\ \\stackrel{\\mathrm{def}}{=}\\operatorname*{max}\\{\\underline{{\\pmb x}}\\dot{\\pmb w},\\overline{{\\pmb x}}\\dot{\\pmb w}\\}$ . Hence, the upper bound constraint $\\ddot{\\pmb{v}}\\geq\\underline{{x}}\\dot{\\pmb{w}}\\wedge\\ddot{\\pmb{v}}\\geq\\overline{{\\pmb{x}}}\\dot{\\pmb{w}}$ exactly captures the epigraph $\\left(\\!\\!\\left[::::\\right]\\!\\!\\right)$ of $\\overline{{f}}(\\dot{\\boldsymbol{w}})$ . Similarly, the lower bound ( ) of $\\pmb{x}\\dot{w}$ is defined by a piecewise-linear concave function $\\underline{{f}}(\\dot{\\pmb w})\\ \\stackrel{\\mathrm{def}}{=}\\operatorname*{min}\\{\\underline{{x}}\\dot{\\pmb w},\\overline{{\\pmb x}}\\dot{\\pmb w}\\}$ Hence, the lower bound constraint ${\\underline{{\\dot{v}}}}\\leq{\\underline{{x}}}{\\dot{w}}\\wedge{\\underline{{\\dot{v}}}}\\leq{\\overline{{x}}}{\\dot{w}}$ exactly captures the hypograph $(\\![::::]\\!)$ of $\\underline{{f}}(\\dot{w})$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.12. Definition 3.11 is a Parametric Linear Relaxation for the Affine layer with constant input bounds $[\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}}]$ that captures the exact lower and upper bounds. ", "page_idx": 5}, {"type": "text", "text": "3.5.2 Affine layer with variable input bounds ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We freeze the parameter weight $\\dot{\\mathbf{W}}$ to be constant $\\mathbf{W}$ (e.g., the original weight) when the input bounds are variable to avoid non-linearity. ", "page_idx": 5}, {"type": "text", "text": "Definition 3.13. For an Affine layer with variable input bounds [xxx,xxx], constant weight $\\mathbf{W}$ and variable bias $\\dot{\\mathbf{b}}$ , its Parametric Linear Relaxation is defined as $\\underline{{\\varphi}}_{\\mathrm{Aff}}(\\underline{{\\dot{\\bf y}}},\\ddot{\\bf y},\\dot{\\underline{{\\bf x}}},\\ddot{\\bf x},{\\bf W},\\dot{\\bf b})\\overset{\\mathrm{def}}{=}\\overline{{\\varphi}}_{\\mathrm{Aff}}(\\dddot{\\bf y},\\underline{{\\dot{\\bf x}}},\\ddot{\\bf x},{\\bf W},\\dot{\\bf b})\\wedge$ \u03c6 (yyy,xxx,xxx,WWW,bbb) where: $\\begin{array}{r l}&{\\overline{{\\varphi}}_{k t f}(\\vec{\\bf y},\\pm,\\vec{\\bf x},\\mathbb{W},\\dot{\\bf b})\\overset{\\mathrm{def}}{=}\\bigwedge\\vec{y}_{j}\\!=\\!\\sum_{i}\\!\\overline{{\\bf V}}_{j i}+\\dot{\\bf b}_{j}}\\\\ &{\\qquad\\underbrace{\\varphi}_{k t f}(\\underline{{\\dot{\\bf y}}},\\pm,\\vec{\\bf x},\\mathbb{W},\\dot{\\bf b})\\overset{\\mathrm{def}}{=}\\bigwedge\\!\\bigwedge_{j}\\!=\\!\\!\\sum_{i}\\!\\underline{{\\dot{\\bf Y}}}_{j i}+\\dot{\\bf b}_{j}}\\end{array}\\mathrm{where}\\left\\{\\begin{array}{l l}{\\underline{{\\bf V}}_{j i}\\overset{\\mathrm{def}}{=}\\dot{\\bf\\underline{{x}}}_{j}\\!\\textbf{W}_{j i}\\mathrm{~and~}\\overline{{\\bf V}}_{j i}\\overset{\\mathrm{def}}{=}\\ddot{\\bf x}_{i}\\!\\textbf{W}_{j i}\\mathrm{~if~}\\ \\mathbb{W}_{j i}\\ge0}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\nabla_{j i}\\overset{\\mathrm{def}}{=}\\ddot{\\bf x}_{j}\\!\\textbf{W}_{j i}\\mathrm{~and~}\\overline{{\\bf V}}_{j i}\\overset{\\mathrm{def}}{=}\\dot{\\underline{{x}}}_{i}\\!\\textbf{W}_{j i}\\mathrm{~otherwise}}\\end{array}\\right.}\\\\ &{\\qquad\\underbrace{\\varphi}_{k t f}(\\underline{{\\dot{\\bf y}}},\\dot{\\underline{{\\bf x}}},\\vec{\\bf x},\\mathbb{W},\\dot{\\bf b})\\overset{\\mathrm{def}}{=}\\bigwedge\\!\\bigwedge_{j}\\!=\\!\\sum_{i}\\!\\underline{{\\dot{\\bf V}}}_{j i}+\\dot{\\bf b}_{j}}\\end{array}$ (8) \u25a0 ", "page_idx": 5}, {"type": "text", "text": "Using constant weight $\\mathbf{W}$ avoids non-linearity due to the multiplication $\\mathbf{\\nabla}x\\mathbf{w}$ where $\\textbf{\\em x}$ is universallyquantified in variable bounds [xxx,xxx]. Thus, the bounds $[\\underline{{\\pmb v}},\\overline{{\\pmb v}}]$ of $_{\\mathcal{X W}}$ for all $\\pmb{x}\\in[\\underline{{\\dot{x}}},\\ddot{x}]$ are determined by the sign of the constant $\\pmb{w}$ ; viz., $[\\underline{{v}},\\overline{{v}}]\\stackrel{\\mathrm{def}}{=}[\\underline{{\\dot{x}}}\\pmb{w},\\ddot{x}\\pmb{w}]$ if $\\pmb{w}\\geq0$ , otherwise $[\\underline{{v}},\\overline{{v}}]\\stackrel{\\mathrm{def}}{=}[\\ddot{x}w,\\underline{{\\dot{x}}}w]$ . ", "page_idx": 5}, {"type": "image", "img_path": "IGhpUd496D/tmp/3df0ae3dc585128d2ed51b48057d1b87a86450d5952731504c1ff048a608d6e1.jpg", "img_caption": ["(b) Left: cactus plot of runtime for 66 all-properties editing problems, with 3600 seconds time limit for each problem. PREPARED succeeded on 62 problems, while PFT\u27e8\u27e8\u27e8DL2\u27e9\u27e9\u27e9succeeded on 45, and PFT\u27e8\u27e8\u27e8APRNN\u27e9\u27e9\u27e9succeeded 48. Middle: speed up for PREPARED vs PFT\u27e8\u27e8\u27e8DL2\u27e9\u27e9\u27e9. Right: speed up for PREPARED vs PFT\u27e8\u27e8\u27e8APRNN\u27e9\u27e9\u27e9. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 4: Results of (a) single-property and (b) all-properties editing problems from VNN-COMP\u203222. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.14. Definition 3.13 is a Parametric Linear Relaxation for the Affine layer with variable input bounds [xxx,xxx] that captures the exact upper and lower bounds. ", "page_idx": 6}, {"type": "text", "text": "3.6 Parametric Linear Relaxation for Tanh, Sigmoid and ELU layers ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our approach can also handle other activation layers like Tanh, Sigmoid and ELU. Figure 3 illustrates their Parametric Linear Relaxations with detailed description deferred to Appendix C. ", "page_idx": 6}, {"type": "text", "text": "3.7 On scalability ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As described in Appendix B, our approach can restrict the edits to only the last $k$ layers of the DNN $\\mathcal{N}$ , freezing the parameters of the first $L-k$ layers. Consequently, the resulting Parametric Linear Relaxation is a linear formula whose size is polynomial in the size of only the last $k$ editable layers $\\scriptstyle{\\mathcal{N}}^{(k:L)}$ instead of the entire DNN $\\mathcal{N}$ . This flexibility enables our approach to scale to large DNNs and BERT transformers, as demonstrated in Section 4. ", "page_idx": 6}, {"type": "text", "text": "4 Experimental evaluation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We have implemented PREPARED in PyTorch [32] and use Gurobi [17] as the LP solver. We demonstrate the effectiveness and efficiency of PREPARED on different tasks that use a wide-range of DNN architectures and properties. All experiments were run on a machine with Dual Intel Xeon Silver 4216 Processor 16-Core 2.1GHz with 384 GB of memory, SSD and RTX-A6000 with 48 GB of GPU memory running Ubuntu 20.04. Additional details about these experiments are in Appendix D. ", "page_idx": 6}, {"type": "text", "text": "Baselines. Because there were no efficient prior approaches for provable editing, we implemented provable fine-tuning PFT\u27e8\u27e8\u27e8\u00b7\u27e9\u27e9\u27e9baselines that combine prior (non-provable) DNN editing approaches with a verifier in the loop: the editing stops when the verifier confirms that the DNN satisfies the property (Appendix D.1). We consider the state-of-the-art DNN editing approaches DL2 [10], APRNN [42], SABR [28] and STAPS [24] as introduced in Section 2. We use PFT\u27e8\u27e8\u27e8DL2\u27e9\u27e9\u27e9to denote the provable finetuning baseline instantiated with DL2. We use verifiers $\\alpha,\\beta$ -CROWN [46, 44], MN-BaB [9], or DeepT [4], depending on the task. To the best of our knowledge, ours is the first to provide a comprehensive evaluation of such verifier-in-the-loop baselines for provable editing. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.1 Provable editing on VNN competition benchmarks ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Setup. We compare PREPARED against PFT\u27e8\u27e8\u27e8DL2\u27e9\u27e9\u27e9and PFT\u27e8\u27e8\u27e8APRNN\u27e9\u27e9\u27e9on $\\mathsf{U N N-C O M P^{\\prime}}22$ benchmarks [29]. The VNN- ${\\mathsf{C O M P}}^{\\prime}22$ benchmarks consist of DNNs along with one or more of their associated safety properties. For verification, the task would be to determine whether a DNN satisfies each of its properties. In the context of provable editing, we use these benchmarks in two scenarios: i) Single-property editing: Given a DNN and a property it violates, edit it to satisfy this single property with a time limit of 600 seconds. There are 423 such DNN-property instances. ii) All-properties editing: Given a DNN that violates at least one of its associated properties, edit it to satisfy the conjunction of all (satisfied or violated) properties associated with it with a time limit of 3600 seconds. There are 66 such DNN-property instances. ", "page_idx": 7}, {"type": "text", "text": "PFT\u27e8\u27e8\u27e8SABR\u27e9\u27e9\u27e9and PFT\u27e8\u27e8\u27e8STAPS\u27e9\u27e9\u27e9are not used because they do not handle all properties and DNN architectures in this experiment. In particular, SABR and STAPS have not discussed how to handle general logical formula with disjunctions, and their current implementations are designed for local robustness training. PFT\u27e8\u27e8\u27e8SABR\u27e9\u27e9\u27e9and PFT\u27e8\u27e8\u27e8STAPS\u27e9\u27e9\u27e9are used for local robustness editing in Section 4.2. ", "page_idx": 7}, {"type": "text", "text": "Metrics. The effectiveness is measured using the number of provably edited instances, and the efficiency is measured using the runtime. We were unable to determine the impact on the predictive performance (accuracy), because VNN-COMP does not come with such evaluation metrics and data. ", "page_idx": 7}, {"type": "text", "text": "Results. As shown in Figure 4, PREPARED is the best provable editing approach, significantly outperforming PFT\u27e8\u27e8\u27e8DL2\u27e9\u27e9\u27e9and PFT\u27e8\u27e8\u27e8APRNN\u27e9\u27e9\u27e9in terms of both effectiveness and efficiency. As shown in the cactus plots (Left in Figures 4(a) and 4(b)), PREPARED can provably edit more instances in less time. As shown in the speed-up plots (Middle and Right in Figures 4(a) and 4(b)), PREPARED completes most problems in 10 seconds, taking a maximum of 45 seconds for single-property editing; it achieves from $10\\times$ to more than $100\\times$ speed-up over PFT\u27e8\u27e8\u27e8DL2\u27e9\u27e9\u27e9and PFT\u27e8\u27e8\u27e8APRNN\u27e9\u27e9\u27e9. Specifically, PREPARED succeeds on all 423 single-property editing problems, while PFT\u27e8\u27e8\u27e8DL2\u27e9\u27e9\u27e9and PFT\u27e8\u27e8\u27e8APRNN\u27e9\u27e9\u27e9 succeed on 185 and 117 problems, respectively. PREPARED succeeds on 62 out of 66 multi-property editing problems, while PFT\u27e8\u27e8\u27e8DL2\u27e9\u27e9\u27e9and PFT\u27e8\u27e8\u27e8APRNN\u27e9\u27e9\u27e9only succeed on 45 and 48 problems, respectively. ", "page_idx": 7}, {"type": "text", "text": "4.2 Local robustness editing for image-recognition DNNs ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Definition 4.1. Consider a classifier DNN $\\mathcal{N}:\\mathbb{R}^{m}\\,\\rightarrow\\,\\mathbb{R}^{n}$ and a dataset $\\mathcal{D}$ of input-label pairs $(\\mathbf{x},l)\\in\\mathbb{R}^{m}\\times\\mathbb{Z}$ . The DNN $\\mathcal{N}$ correctly classifies an input-label pair $({\\bf x},l)$ iff arg max $\\bar{\\mathcal{N}}({\\bf x})\\bar{\\bf\\mu}=l$ , and the standard accuracy of a DNN on a dataset $\\mathcal{D}$ is the percentage of correctly classified input-label pairs in $\\mathcal{D}$ . Given a perturbation $\\varepsilon\\in\\mathbb R$ , the DNN $\\mathcal{N}$ is $L^{\\infty}$ -locally-robust on an input-label pair $({\\bf x},l)$ iff $\\mathcal{N}$ correctly classifies all perturbed inputs $\\mathbf{x}^{\\prime}$ in the $L^{\\infty}$ box centered at $\\mathbf{x}$ with perturbation $\\varepsilon$ , viz., $\\forall\\mathbf{x}^{\\prime}\\in\\mathbb{R}^{m}$ . $\\|\\dot{\\mathbf{x}^{\\prime}}-\\mathbf{x}\\|_{\\infty}\\leq\\varepsilon\\implies$ arg max $\\mathcal{N}(\\mathbf{x}^{\\prime})=l$ . The certified accuracy of a DNN on a dataset $\\mathcal{D}$ is the percentage of $L^{\\infty}$ -locally-robust input-label pairs in $\\mathcal{D}$ . ", "page_idx": 7}, {"type": "text", "text": "Setup. We compare PREPARED against PFT\u27e8\u27e8\u27e8DL2\u27e9\u27e9\u27e9, PFT\u27e8\u27e8\u27e8SABR\u27e9\u27e9\u27e9, and APRNN on $L^{\\infty}$ -local-robustness editing for image-recognition DNNs. We edit CNN7 DNNs for CIFAR10 [21] and TINYIMAGENET [33], trained in prior work [28], to be locally-robust for images in the edit set. The CIFAR10 DNN has 17.2M parameters, $79.24\\%$ standard accuracy and $75.77\\%$ certified accuracy $\\langle\\varepsilon=0.5/255\\rangle$ ). The TinyImageNet DNN has 51.9M parameters, $28.85\\%$ standard accuracy and $20.46\\%$ certified accuracy $(\\varepsilon=1/255)$ . For CIFAR10 and TINYIMAGENET, the edit sets consist of 50 misclassified fog-corrupted images from CIFAR10-C [18] and TINYIMAGENET-C [18], respectively. The generalization sets consist of 200 variant images with different corruption-levels, four variants per image in the edit set. We use MN-BaB [9] to compute the certified accuracy. ", "page_idx": 7}, {"type": "text", "text": "Metrics. Efficacy is measured using the certified accuracy (Definition 4.1) on the edit set. The original certified accuracy on the edit set is $0\\%$ . Efficacy is the most important metric: a provable edit must guarantee $I O O\\%$ efficacy. The standard accuracy and certified accuracy on the full ", "page_idx": 7}, {"type": "table", "img_path": "IGhpUd496D/tmp/3bbcc25b54bdd6df6cd71a9381fc51413d1801dda1783332d2a616ba936ec4ce.jpg", "table_caption": ["Table 1: Local robustness editing for image-recognition DNNs. Comparison of efficacy (Effic.), standard (Acc.), certified (Cert.) and generalization (Gen.) accuracy. Rows with ${<}100\\%$ efficacy are shaded. Best results are highlighted. "], "table_footnote": ["\u2217Timeout in four hours. "], "page_idx": 8}, {"type": "text", "text": "Table 2: Local robustness editing for sentiment-classification transformers. Comparison of the lower bound of efficacy before (Og. Effic.) and after edit (Effic.), as well as the stand accuracy (Acc.). Rows with ${<}100\\%$ efficacy are shaded. Best results are highlighted. ", "page_idx": 8}, {"type": "table", "img_path": "IGhpUd496D/tmp/0b1ba482481053a96b5bde604510dd58076677c5abf40743a643e7ff8db38ea3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "test set are also important metrics: a good provable edit should have high accuracy. However, those accuracy metrics are relevant only if efficacy is $100\\%$ . The generalization accuracy on the generalization set measures how well the edit generalizes to inputs that are similar to the edit set. However, the generalization accuracy is relevant only if the efficacy is $100\\%$ and the standard and certified accuracies are good; that is, the predictive power of the edited DNN should not be sacrificed for better generalization. The runtime metric measures the time taken to edit the DNN. ", "page_idx": 8}, {"type": "text", "text": "Results. As shown in Table 1, PREPARED is overall the best approach. PREPARED achieves the best standard and certified accuracy, good generalization and short runtime. In particular, PFT\u27e8\u27e8\u27e8SABR\u27e9\u27e9\u27e9 and PFT\u27e8\u27e8\u27e8STAPS\u27e9\u27e9\u27e9have significantly lower standard and certified accuracy; PFT\u27e8\u27e8\u27e8DL2\u27e9\u27e9\u27e9takes significantly longer time, and was unable to achieve $100\\%$ efficacy in four hours in the TINYIMAGENET experiment; APRNN was unable to achieve $100\\%$ efficacy. ", "page_idx": 8}, {"type": "text", "text": "4.3 Local robustness editing for sentiment classification BERT transformers ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Setup. We compare PREPARED against PFT\u27e8\u27e8\u27e8DL2\u27e9\u27e9\u27e9and APRNN on provable $L^{\\infty}$ local-robustness editing for BERT sentiment-classification transformers. We conduct this experiment on the Stanford Sentiment Treebank (SST) [40] DNNs. We use DeepT [4] as the verifier in this experiment. We take the \u201cwider\u201d 12-layer BERT transformer used in the DeepT paper [4]. The standard accuracy of this network is $84.07\\%$ . Because DeepT cannot handle all sentences in the SST dataset with arbitrary $\\varepsilon$ , we construct two editing sets from the SST validation set: all 66 verifiable sentences with $\\varepsilon=1\\mathrm{e}{-4}$ , where 60 of them are certified to be locally-robust; all 26 verifiable sentences with $\\varepsilon=5\\mathrm{e}{-4}$ , where 24 of them are certified to be locally-robust. PFT\u27e8\u27e8\u27e8SABR\u27e9\u27e9\u27e9and PFT\u27e8\u27e8\u27e8STAPS\u27e9\u27e9\u27e9are not compared in this experiment because SABR and STAPS do not handle the BERT transformer architecture. ", "page_idx": 8}, {"type": "text", "text": "Metrics. Efficacy is measured using the certified accuracy (Definition 4.1) on the edit set. Because DeepT is incomplete, we can only compute a lower bound of the efficacy. Efficacy is the most important metric: a provable edit must guarantee $I O O\\%$ efficacy. The original efficacy (Og. Effic.) is also presented in Table 2. The standard accuracy on the full test set is also an important metric: a good provable edit should have high accuracy. However, the standard accuracy is relevant only if efficacy is $100\\%$ . The runtime metric measures the time taken to edit the DNN. ", "page_idx": 8}, {"type": "text", "text": "Results. As seen in Table 2, PREPARED is the only approach that achieves $100\\%$ efficacy in this task. PREPARED also achieves good accuracy and short runtime. Both PFT\u27e8\u27e8\u27e8DL2\u27e9\u27e9\u27e9and APRNN were unable to achieve $100\\%$ efficacy, and decreased the efficacy in most experiments. ", "page_idx": 8}, {"type": "text", "text": "Table 3: Global physics property repair for geodynamics DNN. Comparison of relative error (Rel. Err.), continuity (Cont. Err.) and boundary-condition (BC Err.) errors. Errors on the test set (Ref.) are shaded. Best results are highlighted. ", "page_idx": 9}, {"type": "table", "img_path": "IGhpUd496D/tmp/a7575a28857ddb9bd422494d60bec95c5f94406aefbdfa5678d615b13d4c1886.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "4.4 Provable training for physics-plausible DNNs ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Numerical model. We consider a classic model of a buoyancy-driven mantle flow with a central circular plume [13], which is an incompressible flow with variable viscosity. The state of the system is a vector field $\\mathbf{F}\\ {\\stackrel{\\mathrm{def}}{=}}\\ (u,v,\\eta,\\rho)$ of velocity field $\\mathbf{U}\\ensuremath{\\stackrel{\\mathrm{def}}{=}}(u,v)$ , viscosity $\\eta$ and density $\\rho$ , over space $(x,y)\\in\\Omega$ and time $t\\in\\mathcal T$ . We consider the following two physics constraints: i) Conservation of mass $\\nabla\\cdot\\mathbf{U}=0$ from the continuity equation; ii) Dirichlet boundary condition $\\mathbf{U}\\cdot\\mathbf{n}=0$ on the boundary $\\partial\\Omega$ , where $\\mathbf{n}$ is the outward normal of $\\partial\\Omega$ . ", "page_idx": 9}, {"type": "text", "text": "Setup. We compare GD\u27e8\u27e8\u27e8PREPARED\u27e9\u27e9\u27e9, a combination of gradient-descent (GD) and PREPARED, against DL2, GD and GD\u27e8\u27e8\u27e8APRNN\u27e9\u27e9\u27e9on the task of provably training a DNN to model the geodynamics process and satisfy physics constraints. We discretize the space into a $31\\!\\times\\!31$ grid and implement the numerical model to generate data for 150 time steps. We evenly split the data into training, validation and test sets. We train a ReLU ResNet with one single residual block of four conv2d layers with 32 channels and $3{\\times}3$ kernel size. For DL2, we add the physics constraints as an unsupervised-learning regularization to the supervised training. For GD\u27e8\u27e8\u27e8APRNN\u27e9\u27e9\u27e9and GD\u27e8\u27e8\u27e8PREPARED\u27e9\u27e9\u27e9, we first use vanilla gradientdescent (GD) to train a base model, then edit it with the constraints. Specifically, GD\u27e8\u27e8\u27e8APRNN\u27e9\u27e9\u27e9edits all training points, and GD\u27e8\u27e8\u27e8PREPARED\u27e9\u27e9\u27e9edits the convex-hull of all training points to satisfy the constraints. ", "page_idx": 9}, {"type": "text", "text": "Metrics. The relative error on the test set, the constraint-satisfaction errors, viz., continuity error, for the conservation of mass, and boundary-condition error, and the runtime are the metrics. ", "page_idx": 9}, {"type": "text", "text": "Results. As shown in Table 3, GD\u27e8\u27e8\u27e8PREPARED\u27e9\u27e9\u27e9is overall the best approach. GD\u27e8\u27e8\u27e8PREPARED\u27e9\u27e9\u27e9has good relative error, and its constraint-satisfaction error is significantly better than other approaches. Notably, its continuity error is at the same magnitude as the reference data, close to the theoretical value, zero, and is $\\mathrm{i0^{5}}$ to $10^{8}$ times better than other approaches. GD achieves the best relative error and the shortest runtime. DL2 has the worst errors and takes significantly longer amount of time. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have presented PREPARED, the first efficient approach for provable editing of DNNs that runs in polynomial time. We presented novel methods for constructing tight parametric bounds for the DNN output using Parametric Linear Relaxation, enabling PREPARED to use an LP solver to find an edit. To demonstrate its effectiveness and efficiency, PREPARED was used to provably edit DNNs and properties from the VNN-COMP\u203222 benchmark, provably edit CIFAR10 and TINYIMAGENET imagerecognition DNNs, and BERT sentiment-classification DNNs for local robustness, and provably train a geodynamics DNN to satisfy physics constraints. ", "page_idx": 9}, {"type": "text", "text": "Societal impact. PREPARED represents a step towards the goal of ensuring safety, trustworthiness, and reliability of DNNs, which is crucial given their use in autonomous safety-critical systems. However, being a general technique for editing DNNs, it could be used to remove or add malicious behavior such as bias, backdoor attacks, etc. ", "page_idx": 9}, {"type": "text", "text": "Limitations. PREPARED requires as input a property that we wish the DNN to satisfy. These properties would need to formally encode the notions of safety, reliability, or trustworthiness, which may not always be possible or easy to do. e.g., defining safety properties for LLM-based chatbots. However, the machine learning and formal methods communities are making progress towards this goal; e.g., developing proxy specifications for complex properties that are easier to define and learning safety specifications from data [8]. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We would like to thank the anonymous reviewers for their feedback and suggestions, which have greatly improved the quality of the paper. This work is supported in part by NSF grant CCF-2048123 and DOE Award DE-SC0022285. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Aws Albarghouthi. Introduction to neural network verification. Found. Trends Program. Lang., 7(1-2): 1\u2013157, 2021. doi: 10.1561/2500000051. URL https://doi.org/10.1561/2500000051.   \n[2] Mislav Balunovic and Martin Vechev. Adversarial training and provable defenses: Bridging the gap. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum? id=SJxSDxrKDr.   \n[3] Gregory Bonaert, Dimitar I. Dimitrov, Maximilian Baader, and Martin T. Vechev. Fast and precise certification of transformers. https://github.com/eth-sri/DeepT/tree/ 16ffe4075f1f8a7c87fa2a187d8c46cfd51e07bf, 2021.   \n[4] Gregory Bonaert, Dimitar I. Dimitrov, Maximilian Baader, and Martin T. Vechev. Fast and precise certification of transformers. In Stephen N. Freund and Eran Yahav, editors, PLDI \u201921: 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation, Virtual Event, Canada, June 20-25, 2021, pages 466\u2013481. ACM, 2021. doi: 10.1145/3453483.3454056. URL https://doi.org/10.1145/3453483.3454056.   \n[5] Christopher Brix, Mark Niklas M\u00fcller, Stanley Bak, Taylor T. Johnson, and Changliu Liu. First three years of the international verification of neural networks competition (vnn-comp), 2023.   \n[6] George E. Collins. Quantifier elimination for real closed fields by cylindrical algebraic decompostion. In H. Brakhage, editor, Automata Theory and Formal Languages, pages 134\u2013183, Berlin, Heidelberg, 1975. Springer Berlin Heidelberg. ISBN 978-3-540-37923-2.   \n[7] George E. Collins and Hoon Hong. Partial cylindrical algebraic decomposition for quantifier elimination. Journal of Symbolic Computation, 12(3):299\u2013328, 1991. ISSN 0747-7171. doi: https://doi.org/10. 1016/S0747-7171(08)80152-6. URL https://www.sciencedirect.com/science/article/pii/ S0747717108801526.   \n[8] David Dalrymple, Joar Skalse, Yoshua Bengio, Stuart Russell, Max Tegmark, Sanjit Seshia, Steve Omohundro, Christian Szegedy, Ben Goldhaber, Nora Ammann, Alessandro Abate, Joe Halpern, Clark Barret, Ding Zhao, Tan Zhi-Xuan, Jeannette Wing, and Joshua Tenenbaum. Towards guaranteed safe ai: A framework for ensuring robust and reliable AI systems. arXiv preprint arXiv:2405.06624, 2024.   \n[9] Claudio Ferrari, Mark Niklas M\u00fcller, Nikola Jovanovic\u00b4, and Martin Vechev. Complete verification via multineuron relaxation guided branch-and-bound. In International Conference on Learning Representations, 2022.   \n[10] Marc Fischer, Mislav Balunovic, Dana Drachsler-Cohen, Timon Gehr, Ce Zhang, and Martin T. Vechev. DL2: training and querying neural networks with logic. In Proceedings of the 36th International Conference on Machine Learning (ICML), volume 97 of Proceedings of Machine Learning Research. PMLR, 2019.   \n[11] Marc Fischer, Mislav Balunovic, Dana Drachsler-Cohen, Timon Gehr, Ce Zhang, and Martin T. Vechev. DL2: Training and querying neural networks with logic. https://github.com/eth-sri/dl2/tree/ 05e2d3fb4e2b00ef5968bf28d26219750a580908, 2023.   \n[12] Feisi Fu and Wenchao Li. Sound and complete neural network repair with minimality and locality guarantees. In 10th International Conference on Learning Representations, ICLR 2022, Lisbon, Portugal, Oct 27-28, 2022. OpenReview.net, 2022. URL https://arxiv.org/abs/2110.07682.   \n[13] Taras Gerya. Introduction to numerical geodynamic modelling. Cambridge University Press, 2019.   \n[14] Ben Goldberger, Guy Katz, Yossi Adi, and Joseph Keshet. Minimal modifications of deep neural networks using verification. In Elvira Albert and Laura Kov\u00e1cs, editors, LPAR 2020: 23rd International Conference on Logic for Programming, Artificial Intelligence and Reasoning, Alicante, Spain, May 22-27, 2020, volume 73 of EPiC Series in Computing, pages 260\u2013278. EasyChair, 2020. doi: 10.29007/699q. URL https://doi.org/10.29007/699q.   \n[15] Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Uesato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval bound propagation for training verifiably robust models, 2019.   \n[16] Kshitij Goyal, Sebastijan Dumancic, and Hendrik Blockeel. Deepsade: Learning neural networks that guarantee domain constraint satisfaction. Proceedings of the AAAI Conference on Artificial Intelligence, 38 (11):12199\u201312207, Mar. 2024. doi: 10.1609/aaai.v38i11.29109. URL https://ojs.aaai.org/index. php/AAAI/article/view/29109.   \n[17] Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2022. URL https://www.gurobi. com.   \n[18] Dan Hendrycks and Thomas G. Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum? id=HJz6tiCqYm.   \n[19] Nick Hoernle, Rafael Michael Karampatsis, Vaishak Belle, and Kobi Gal. Multiplexnet: Towards fully satisfied logical constraints in neural networks. Proceedings of the AAAI Conference on Artificial Intelligence, 36(5):5700\u20135709, Jun. 2022. doi: 10.1609/aaai.v36i5.20512. URL https: //ojs.aaai.org/index.php/AAAI/article/view/20512.   \n[20] Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy, and Eric Xing. Harnessing deep neural networks with logic rules. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2410\u20132420, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1228. URL https://aclanthology.org/P16-1228.   \n[21] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced research). URL http://www.cs.toronto.edu/\\~kriz/cifar.html.   \n[22] Zenan Li, Zehua Liu, Yuan Yao, Jingwei Xu, Taolue Chen, Xiaoxing Ma, and Jian L\\\u201d{u}. Learning with logical constraints but without shortcut satisfaction. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=M2unceRvqhh.   \n[23] Dmitry Malioutov and Kuldeep S Meel. Mlic: A maxsat-based framework for learning interpretable classification rules. In International Conference on Principles and Practice of Constraint Programming, pages 312\u2013327. Springer, 2018.   \n[24] Yuhao Mao, Mark Niklas Mueller, Marc Fischer, and Martin Vechev. Connecting certified and adversarial training. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https: //openreview.net/forum?id $\\equiv$ T2lM4ohRwb.   \n[25] Yuhao Mao, Mark Niklas Mueller, Marc Fischer, and Martin Vechev. TAPS: Connecting certified and adversarial training. https://github.com/eth-sri/TAPS/tree/ 6b5c5d9d3453c482f56f00e6f57b99b07b11ca4f, 2023.   \n[26] Matthew Mirman, Timon Gehr, and Martin T. Vechev. Differentiable abstract interpretation for provably robust neural networks. In Jennifer G. Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pages 3575\u20133583. PMLR, 2018. URL http://proceedings.mlr.press/v80/mirman18b.html.   \n[27] Mark Niklas Mueller, Franziska Eckert, Marc Fischer, and Martin Vechev. SABR: Small adversarial bounding boxes. https://github.com/eth-sri/sabr, 2023.   \n[28] Mark Niklas Mueller, Franziska Eckert, Marc Fischer, and Martin Vechev. Certified training: Small boxes are all you need. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=7oFuxtJtUMH.   \n[29] Mark Niklas M\u00fcller, Christopher Brix, Stanley Bak, Changliu Liu, and Taylor T. Johnson. The third international verification of neural networks competition (vnn-comp 2022): Summary and results, 2022. URL https://arxiv.org/abs/2212.10376.   \n[30] Yatin Nandwani, Abhishek Pathak, Mausam, and Parag Singla. A primal dual formulation for deep learning with constraints. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/ cf708fc1decf0337aded484f8f4519ae-Paper.pdf. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[31] Alessandro De Palma, Rudy Bunel, Krishnamurthy Dvijotham, M. Pawan Kumar, and Robert Stanforth. Ibp regularization for verified adversarial robustness via branch-and-bound, 2023. ", "page_idx": 12}, {"type": "text", "text": "[32] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Z. Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. CoRR, abs/1912.01703, 2019. URL http://arxiv.org/abs/1912.01703.   \n[33] Hadi Pouransari and Saman Ghili. Tiny imagenet visual recognition challenge. CS 231N, 7, 2014.   \n[34] Hadi Salman, Greg Yang, Huan Zhang, Cho-Jui Hsieh, and Pengchuan Zhang. A convex relaxation barrier to tight robustness verification of neural networks. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\u2019Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 9832\u20139842, 2019. URL https://proceedings. neurips.cc/paper/2019/hash/246a3c5544feb054f3ea718f61adfa16-Abstract.html.   \n[35] Sanjit A. Seshia, Dorsa Sadigh, and S. Shankar Sastry. Toward verified artificial intelligence. Commun. ACM, 65(7):46\u201355, 2022. doi: 10.1145/3503914. URL https://doi.org/10.1145/3503914.   \n[36] Zhouxing Shi, Yihan Wang, Huan Zhang, Jinfeng Yi, and Cho-Jui Hsieh. Fast certified robust training with short warmup. In ICML 2021 Workshop on Adversarial Machine Learning, 2021. URL https: //openreview.net/forum?id=_jUobmvki51.   \n[37] Zhouxing Shi, Qirui Jin, J Zico Kolter, Suman Jana, Cho-Jui Hsieh, and Huan Zhang. Formal verification for neural networks with general nonlinearities via branch-and-bound. 2023.   \n[38] Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus P\u00fcschel, and Martin T. Vechev. Fast and effective robustness certification. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada, pages 10825\u201310836, 2018. URL https://proceedings.neurips.cc/paper/ 2018/hash/f2f446980d8e971ef3da97af089481c3-Abstract.html.   \n[39] Gagandeep Singh, Timon Gehr, Markus P\u00fcschel, and Martin T. Vechev. An abstract domain for certifying neural networks. Proc. ACM Program. Lang., 3(POPL):41:1\u201341:30, 2019. doi: 10.1145/3290354. URL https://doi.org/10.1145/3290354.   \n[40] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631\u20131642, 2013.   \n[41] Matthew Sotoudeh and Aditya Thakur. Prdnn. https://github.com/95616ARG/PRDNN, 2021.   \n[42] Zhe Tao, Stephanie Nawas, Jacqueline Mitchell, and Aditya V. Thakur. Architecture-preserving provable repair of deep neural networks. Proc. ACM Program. Lang., 7(PLDI), jun 2023. doi: 10.1145/3591238. URL https://doi.org/10.1145/3591238.   \n[43] Christian Tjandraatmadja, Ross Anderson, Joey Huchette, Will Ma, Krunal Patel, and Juan Pablo Vielma. The convex relaxation barrier, revisited: Tightened single-neuron relaxations for neural network verification. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, MariaFlorina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ f6c2a0c4b566bc99d596e58638e342b0-Abstract.html.   \n[44] Shiqi Wang, Huan Zhang, Kaidi Xu, Xue Lin, Suman Jana, Cho-Jui Hsieh, and J. Zico Kolter. Betacrown: Efficient bound propagation with per-neuron split constraints for neural network robustness verification. In Marc\u2019Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 29909\u201329921, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/ fac7fead96dafceaf80c1daffeae82a4-Abstract.html.   \n[45] Jingyi Xu, Zilu Zhang, Tal Friedman, Yitao Liang, and Guy Van den Broeck. A semantic loss function for deep learning with symbolic knowledge. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 5502\u20135511. PMLR, 10\u201315 Jul 2018. URL https://proceedings.mlr.press/v80/ xu18h.html.   \n[46] Kaidi Xu, Huan Zhang, Shiqi Wang, Yihan Wang, Suman Jana, Xue Lin, and Cho-Jui Hsieh. Fast and complete: Enabling complete neural network verification with rapid and massively parallel incomplete verifiers. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=nVZtXBI6LNn.   \n[47] Ziwei Xu, Yogesh Rawat, Yongkang Wong, Mohan S Kankanhalli, and Mubarak Shah. Don't pour cereal into coffee: Differentiable temporal logic for temporal action segmentation. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 14890\u201314903. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ 5f96a21345c138da929e99871fda138e-Paper-Conference.pdf.   \n[48] Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efficient neural network robustness certification with general activation functions. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada, pages 4944\u20134953, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/ d04863f100d59b3eb688a11f95b0ae60-Abstract.html.   \n[49] Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efficient neural network robustness certification with general activation functions. Advances in neural information processing systems, 31, 2018.   \n[50] Huan Zhang, Zhouxing Shi, Kaidi Xu, Yihan Wang, Shiqi Wang, Linyi Li, Jinqi (Kathryn) Chen, and Zhuolin Yang. auto_LiRPA: An automatic linear relaxation based perturbation analysis library for neural networks and general computational graphs. https://github.com/Verified-Intelligence/auto_ LiRPA, 2023. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Theorem 3.4. Given a provable interval editing problem (Definition 3.2) for DNN $\\mathcal{N}$ and parameters $\\theta$ with input bounds [xxx,xxx] and output bounds [yyy,yyy], let $\\underline{{\\overline{{\\varphi}}}}_{N}(\\underline{{\\dot{\\bf y}}},\\ddot{\\underline{{\\bf y}}},\\underline{{{\\bf x}}},\\overline{{{\\bf x}}};\\dot{\\theta})$ be a Parametric Linear Relaxation for $\\mathcal{N}$ . Then the following linear program can be solved in polynomial time in the size of the DNN $\\mathcal{N}$ , and whose solution is a solution to the provable interval editing problem: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\|\\dot{\\theta}-\\theta\\|\\quad s.t.\\quad\\overline{{\\underline{{\\varphi}}}}_{N}\\big(\\underline{{\\dot{\\bf y}}},\\underline{{\\ddot{\\bf y}}},\\underline{{\\underline{{\\bf x}}}},\\overline{{\\hat{\\bf x}}};\\dot{\\theta}\\big)\\wedge\\big(\\underline{{\\mathbf{y}}}\\le\\underline{{\\dot{\\bf y}}}\\wedge\\overline{{\\mathbf{y}}}\\le\\overline{{\\mathbf{y}}}\\big)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. Equation 4 is a linear program that can be solved in polynomial in the size of the DNN $\\mathcal{N}$ because by Definition 3.3, $\\underline{{\\overline{{\\varphi}}}}_{\\mathcal{N}}(\\underline{{\\dot{\\mathbf{y}}}},\\underline{{\\ddot{\\mathbf{y}}}},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}};\\dot{\\boldsymbol{\\theta}})$ is a poly-size linear formula whose size is polynomial in the size of the DNN $\\mathcal{N}$ , i.e., the number of parameters, layers, and the input and output dimensions of each layer. ", "page_idx": 14}, {"type": "text", "text": "Equation 4 is a solution to the provable interval editing problem (Definition 3.2) because by Definition 3.3, $\\underline{{\\overline{{\\varphi}}}}_{N}(\\underline{{\\dot{\\bf y}}},\\ddot{\\bf y},\\underline{{{\\bf x}}},\\overline{{{\\bf x}}};\\dot{\\pmb\\theta})$ implies $\\forall\\mathbf{x}\\in[\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}}]$ . $\\mathcal{N}(\\mathbf{x};\\dot{\\pmb{\\theta}})\\in$ [yyy,yyy]. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Theorem 3.6. Definition 3.5 is a Parametric Linear Relaxation for the DNN $\\mathcal{N}$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. Recall Definition 3.5: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\underline{{\\overline{{\\varphi}}}}_{\\mathcal{N}}\\big(\\underline{{\\dot{y}}},\\underline{{\\ddot{\\mathbf{y}}}},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}};\\dot{\\theta}\\big)\\overset{\\mathrm{def}}{=}\\bigwedge_{0\\leq\\ell<L}\\overline{{\\underline{{\\varphi}}}}_{\\mathcal{N}^{(\\ell)}}\\big(\\underline{{\\dot{\\mathbf{x}}}}^{(\\ell+1)},\\ddot{\\mathbf{x}}^{(\\ell+1)},\\underline{{\\dot{\\mathbf{x}}}}^{(\\ell)},\\ddot{\\mathbf{x}}^{(\\ell)};\\dot{\\theta}^{(\\ell)}\\big)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $[\\dot{\\underline{{\\mathbf{y}}}},\\ddot{\\underline{{\\mathbf{y}}}}]\\,\\overset{\\mathrm{def}}{=}\\,[\\dot{\\underline{{\\mathbf{x}}}}^{(L)},\\ddot{\\underline{{\\mathbf{x}}}}^{(L)}]$ and $[\\underline{{\\dot{\\bf x}}}^{(0)},\\underline{{\\ddot{\\bf x}}}^{(0)}]\\,\\stackrel{\\mathrm{def}}{=}\\,[\\underline{{\\bf x}},\\overline{{\\bf x}}]$ . Our goal is to prove that $\\underline{{\\overline{{\\varphi}}}}_{N}(\\underline{{\\dot{\\bf y}}},\\ddot{\\bf y},\\underline{{{\\bf x}}},\\overline{{{\\bf x}}};\\dot{\\pmb\\theta})$ is a poly-size linear formula that implies ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\forall\\mathbf{x}\\in[\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}}].\\,\\mathcal{N}\\big(\\mathbf{x};\\dot{\\theta}\\big)\\in[\\underline{{\\dot{\\mathbf{y}}}},\\ddot{\\mathbf{y}}]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We first show that $\\underline{{\\overline{{\\varphi}}}}_{N}(\\underline{{\\dot{\\mathbf{y}}}},\\underline{{\\ddot{\\mathbf{y}}}},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}};\\dot{\\boldsymbol{\\theta}})$ is a poly-size linear formula. For Parametric Linear Relaxation $\\underline{{\\boldsymbol{\\overline{{\\varphi}}}}}_{\\mathcal{N}^{(\\ell)}}\\big(\\underline{{\\boldsymbol{\\dot{x}}}}^{(\\ell+1)},\\ddot{\\mathbf{x}}^{(\\ell+1)},\\underline{{\\dot{\\mathbf{x}}}}^{(\\ell)},\\ddot{\\mathbf{x}}^{(\\ell)};\\dot{\\pmb{\\theta}}^{(\\ell)}\\big)$ for any layer ${\\mathcal{N}}^{(\\ell)}.$ , by Definition 3.3, we have $\\underline{{\\boldsymbol{\\overline{{\\varphi}}}}}_{N^{(\\ell)}}\\big(\\underline{{\\boldsymbol{\\dot{x}}}}^{(\\ell+1)},\\ddot{\\mathbf{x}}^{(\\ell+1)},\\underline{{\\dot{\\mathbf{x}}}}^{(\\ell)},\\ddot{\\mathbf{x}}^{(\\ell)};\\dot{\\pmb{\\theta}}^{(\\ell)}\\big)$ is a poly-size linear formula whose size is polynomial in the number of parameters, input and output dimensions of layer ${\\cal N}^{(\\ell)}.$ . Therefore, we have $\\underline{{\\overline{{\\varphi}}}}_{N}(\\underline{{\\dot{\\mathbf{y}}}},\\underline{{\\ddot{\\mathbf{y}}}},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}};\\dot{\\boldsymbol{\\theta}})$ is a poly-size linear formula whose size is polynomial in the number of parameters, layers, and the input and output dimensions of each layer. ", "page_idx": 14}, {"type": "text", "text": "Then we show that $\\underline{{\\overline{{\\varphi}}}}_{N}(\\underline{{\\dot{\\bf y}}},\\ddot{\\bf y},\\underline{{{\\bf x}}},\\overline{{{\\bf x}}};\\dot{\\pmb\\theta})$ implies $\\forall\\mathbf{x}\\in[\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}}]$ . $\\mathcal{N}(\\mathbf{x};\\dot{\\pmb{\\theta}})\\in\\mathcal{[}\\dot{\\mathbf{y}}$ ,yyy]. By Definition 3.3, we have that the Parametric Linear Relaxation \u03c6N (\u2113) xxx(\u2113+1),xxx(\u2113+1),xxx(\u2113),xxx(\u2113);\u03b8\u03b8\u03b8(\u2113) for any layer N (\u2113) implies the following formula: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\forall\\mathbf{x}^{(\\ell)}\\in[\\underline{{\\dot{\\mathbf{x}}}}^{(\\ell)},\\ddot{\\mathbf{x}}^{(\\ell)}].\\,\\mathcal{N}^{(\\ell)}(\\mathbf{x}^{(\\ell)};\\dot{\\pmb{\\theta}}^{(\\ell)})\\in[\\underline{{\\dot{\\mathbf{x}}}}^{(\\ell+1)},\\ddot{\\mathbf{x}}^{(\\ell+1)}]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, we have $\\begin{array}{r}{\\underline{{\\overline{{\\varphi}}}}_{N}\\big(\\underline{{\\dot{y}}},\\underline{{\\ddot{y}}},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}};\\dot{\\theta}\\big)\\overset{\\mathrm{def}}{=}\\bigwedge_{0\\le\\ell<L}\\underline{{\\overline{{\\varphi}}}}_{N^{(\\ell)}}\\big(\\underline{{\\dot{\\mathbf{x}}}}^{(\\ell+1)},\\ddot{\\mathbf{x}}^{(\\ell+1)},\\underline{{\\dot{\\mathbf{x}}}}^{(\\ell)},\\ddot{\\mathbf{x}}^{(\\ell)};\\dot{\\theta}^{(\\ell)}\\big)}\\end{array}$ implies the following formula: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\bigwedge_{0\\leq\\ell<L}\\forall\\mathbf{x}^{(\\ell)}\\in[\\dot{\\underline{{\\mathbf{x}}}}^{(\\ell)},\\ddot{\\mathbf{x}}^{(\\ell)}].\\,\\mathcal{N}^{(\\ell)}(\\mathbf{x}^{(\\ell)};\\dot{\\boldsymbol{\\theta}}^{(\\ell)})\\in[\\dot{\\underline{{\\mathbf{x}}}}^{(\\ell+1)},\\ddot{\\mathbf{x}}^{(\\ell+1)}]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where [yyy,yyy] $\\underline{{\\underline{{\\mathrm{def}}}}}\\quad[\\underline{{\\dot{\\bf x}}}^{(L)},\\underline{{\\ddot{\\bf x}}}^{(L)}]$ and $[\\underline{{\\dot{\\bf x}}}^{(0)},\\ddot{\\bf x}^{(0)}]\\ \\stackrel{\\mathrm{def}}{=}\\ [{\\bf x},\\overline{{\\bf x}}]$ . By induction, the formula above implies $\\forall\\mathbf{x}\\in[\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}}]$ . $\\mathcal{N}(\\mathbf{x};\\dot{\\boldsymbol{\\theta}})\\in$ [yyy,yyy]. Therefore, we proved that $\\underline{{\\overline{{\\varphi}}}}_{\\mathcal{N}}(\\underline{{\\dot{\\mathbf{y}}}},\\ddot{\\mathbf{y}},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}};\\dot{\\pmb{\\theta}})$ is a poly-size linear formula that implies $\\forall\\mathbf{x}\\in[\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}}]$ . $\\mathcal{N}(\\mathbf{x};\\dot{\\pmb{\\theta}})\\in\\mathcal{\\dot{\\lfloor}{\\pmb{\\mathfrak{y}}}}$ ,yyy]. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "A.1 Proofs for Parametric Linear Relaxation of ReLU layers ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Lemma A.1. $\\underline{{\\overline{{\\varphi}}}}_{\\mathtt{R e L U}}(\\underline{{\\dot{\\bf y}}},\\underline{{\\ddot{\\bf y}}},\\underline{{\\dot{\\bf x}}},\\underline{{\\ddot{\\bf x}}})$ defined in Definition 3.8 is a poly-size linear formula. ", "page_idx": 15}, {"type": "text", "text": "Proof. As seen in Definition 3.8, $\\underline{{\\overline{{\\varphi}}}}_{\\mathtt{R e L U}}(\\underline{{\\dot{\\bf y}}},\\ddot{\\underline{{\\bf y}}},\\dot{\\underline{{\\bf x}}},\\ddot{\\underline{{\\bf x}}})$ is a linear formula. Let $m$ be the number of input dimensions of ReLU. $\\underline{{\\overline{{\\varphi}}}}_{\\mathtt{R e L U}}(\\underline{{\\dot{\\bf y}}},\\underline{{\\ddot{\\bf y}}},\\underline{{\\dot{\\bf x}}},\\underline{{\\ddot{\\bf x}}})$ has $3m$ linear constraints over $4m$ variables $(\\dot{\\underline{{\\mathbf{x}}}}\\in\\mathbb{R}^{m}$ , $\\breve{\\mathbf{x}}\\in\\mathbb{R}^{m},\\dot{\\underline{{\\mathbf{y}}}}\\in\\mathbb{R}^{m}$ and $\\mathbf{\\ddot{y}}\\in\\mathbb{R}^{m}$ ). Hence, $\\underline{{\\overline{{\\varphi}}}}_{\\mathtt{R e L U}}(\\underline{{\\dot{\\bf y}}},\\underline{{\\ddot{\\bf y}}},\\underline{{\\dot{\\bf x}}},\\ddot{\\underline{{\\tilde{\\bf x}}}})$ is a poly-size linear formula whose size is polynomial in the number of input dimensions of the ReLU layer. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Lemma A.2. $\\begin{array}{r l}{\\underline{{\\varphi}}_{\\mathtt{R e L U}}(\\underline{{\\dot{\\bf y}}},\\underline{{\\ddot{\\bf y}}},\\underline{{\\dot{\\bf x}}},\\underline{{\\ddot{\\bf x}}})}&{\\stackrel{d e f}{=}\\;\\overline{{\\varphi}}_{\\mathtt{R e L U}}(\\overline{{\\dot{\\bf y}}},\\underline{{\\ddot{\\bf x}}})\\;\\wedge\\;\\underline{{\\varphi}}_{\\mathtt{R e L U}}(\\underline{{\\dot{\\bf y}}},\\underline{{\\dot{\\bf x}}})}\\end{array}$ defined in Definition 3.8 implies $\\forall\\mathbf{x}\\in$ [xxx,xxx]. $\\mathtt{R e L U}(\\mathbf{x})\\in[$ [yyy,yyy]. ", "page_idx": 15}, {"type": "text", "text": "Proof. For the upper bound, recall that by Definition 3.8, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\overline{{\\varphi}}}_{\\mathtt{R e L U}}({\\mathfrak{F}},{\\mathfrak{F}})\\ {\\stackrel{\\mathrm{def}}{=}}\\ {\\mathfrak{F}}\\geq{\\ddot{\\mathbf{x}}}\\wedge{\\ddot{\\mathbf{y}}}\\geq0\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Our goal is to prove that $\\overline{{\\varphi}}_{\\mathtt{R e L U}}(\\mathfrak{F},\\mathfrak{X})$ implies ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\forall\\mathbf{x}\\in[\\dot{\\underline{{\\mathbf{x}}}},\\ddot{\\mathbf{x}}].\\,\\ddot{\\mathbf{y}}\\geq\\mathrm{ReLU}(\\mathbf{x})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which is equivalent to ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\smash{\\widetilde{\\mathbf{y}}\\geq\\operatorname*{max}\\left\\{\\operatorname{ReLU}(\\mathbf{x})~|~\\dot{\\mathbf{x}}\\leq\\mathbf{x}\\leq\\ddot{\\mathbf{x}}\\right\\}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By the definition of the ReLU layer (Definition 3.7), we expand the formula above to ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\left.\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\mathbf{\\ddot{y}}\\geq\\operatorname*{max}\\left\\{\\operatorname*{max}\\{\\operatorname*{max}\\{\\mathbf{x},0\\}\\}\\;\\big|\\;\\underline{{\\dot{\\mathbf{x}}}}\\leq\\mathbf{x}\\leq\\ddot{\\mathbf{x}}\\right\\}\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Because $\\operatorname*{max}\\{\\mathbf{x},0\\}$ increases monotonically with respect to $\\mathbf{x}$ , the above formula is equivalent to ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\pmb{\\ddot{\\mathbf{y}}}\\geq\\operatorname*{max}\\{\\pmb{\\Tilde{\\mathbf{x}}},0\\}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Because $\\operatorname*{max}\\{\\aleph,0\\}$ is a convex function, the above formula is equivalent to ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{y}}\\geq\\tilde{\\mathbf{x}}\\wedge\\tilde{\\mathbf{y}}\\geq0\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which is the definition of $\\overline{{\\varphi}}_{\\mathtt{R e L U}}(\\mathfrak{F},\\mathfrak{X})$ in Definition 3.8. Therefore, we proved that $\\overline{{\\varphi}}_{\\mathtt{R e L U}}(\\mathfrak{F},\\mathfrak{X})$ implies $\\forall\\mathbf{x}\\in$ [xxx,xxx]. $\\widetilde{\\mathbf{y}}\\geq\\mathtt{R e L U}(\\mathbf{x})$ . ", "page_idx": 15}, {"type": "text", "text": "For the lower bound, recall that by Definition 3.8, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\underline{{\\varphi}}_{\\mathrm{ReLU}}(\\underline{{\\dot{\\mathbf{y}}}},\\underline{{\\dot{\\mathbf{x}}}})\\overset{\\mathrm{def}}{=}\\underline{{\\dot{\\mathbf{y}}}}=\\mathbf{c}\\underline{{\\dot{\\mathbf{x}}}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\mathbf{c}\\in\\mathbb{R}^{m}$ are constants such that $0\\leq{\\bf c}_{i}\\leq1$ . Our goal is to prove that \u03c6ReLU(yyy,xxx) implies ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\forall\\mathbf{x}\\in[\\dot{\\underline{{\\mathbf{x}}}},\\ddot{\\mathbf{x}}].\\,\\dot{\\underline{{\\mathbf{y}}}}\\leq\\mathrm{ReLU}(\\mathbf{x})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which is equivalent to ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\dot{\\underline{{\\mathbf{y}}}}\\leq\\operatorname*{min}\\left\\{{\\mathrm{ReLU}}(\\mathbf{x})~\\middle|~\\dot{\\underline{{\\mathbf{x}}}}\\leq\\mathbf{x}\\leq\\ddot{\\mathbf{x}}\\right\\}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By the definition of the ReLU layer (Definition 3.7), we expand the formula above to ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\dot{\\underline{{\\mathbf{y}}}}\\leq\\operatorname*{min}\\left\\lbrace\\operatorname*{max}\\{\\mathbf{x},0\\}\\ \\big|\\ \\dot{\\underline{{\\mathbf{x}}}}\\leq\\mathbf{x}\\leq\\ddot{\\mathbf{x}}\\right\\rbrace\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Because $\\operatorname*{max}\\{\\mathbf{x},0\\}$ increases monotonically with respect to $\\mathbf{x}$ , the above formula is equivalent to ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\underline{{\\dot{\\mathbf{y}}}}\\leq\\operatorname*{max}\\{\\underline{{\\dot{\\mathbf{x}}}},0\\}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which is equivalent to ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\underline{{\\dot{\\mathbf{y}}}}\\leq\\underline{{\\dot{\\mathbf{x}}}}\\vee\\underline{{\\dot{\\mathbf{y}}}}\\leq0\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Given constant vector ccc where $0\\,\\leq\\,{\\mathfrak{c}}\\,\\leq\\,1$ , $\\underline{{\\varphi}}_{\\mathrm{ReLU}}(\\underline{{\\dot{\\bf y}}},\\underline{{\\dot{\\bf x}}})\\,\\stackrel{\\mathrm{def}}{=}\\,\\underline{{\\dot{\\bf y}}}\\,=\\,{\\bf c}\\underline{{\\dot{\\bf x}}}$ implies the above formula. Therefore, we proved that $\\underline{{\\varphi}}_{\\mathrm{ReLU}}(\\dot{\\underline{{\\mathbf{y}}}},\\dot{\\underline{{\\mathbf{x}}}})$ implies $\\forall\\mathbf{x}\\in$ [xxx,xxx]. $\\dot{\\underline{{\\mathbf{y}}}}\\le\\mathtt{R e L U}(\\mathbf{x})$ . \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Lemma A.3. $\\begin{array}{r}{\\underline{{\\varphi}}_{\\mathtt{R e L U}}(\\underline{{\\dot{\\mathbf{y}}}},\\ddot{\\mathbf{y}},\\dot{\\underline{{\\mathbf{x}}}},\\ddot{\\mathbf{x}})\\overset{d e f}{=}\\overline{{\\varphi}}_{\\mathtt{R e L U}}(\\ddot{\\mathbf{y}},\\ddot{\\mathbf{x}})\\wedge\\underline{{\\varphi}}_{\\mathtt{R e L U}}(\\dot{\\underline{{\\mathbf{y}}}},\\dot{\\underline{{\\mathbf{x}}}})}\\end{array}$ defined in Definition 3.8 is implied by $\\ddagger=\\operatorname*{max}\\left\\{\\operatorname{ReLU}(\\mathbf{x})\\ |\\ \\dot{\\mathbf{x}}\\leq\\mathbf{x}\\leq\\ddagger\\right\\}$ , hence captures the exact upper bound. ", "page_idx": 16}, {"type": "text", "text": "Proof. Recall that by Definition 3.8, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\overline{{\\varphi}}}_{\\mathtt{R e L U}}({\\mathfrak{F}},{\\mathfrak{F}})\\ {\\stackrel{\\mathrm{def}}{=}}\\ {\\mathfrak{F}}\\geq{\\ddot{\\mathbf{x}}}\\wedge{\\ddot{\\mathbf{y}}}\\geq0\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For any solution $(\\overline{{\\mathbf{y}}}^{*},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}})$ to $\\ddagger=\\operatorname*{max}\\left\\{\\operatorname{ReLU}(\\mathbf{x})\\ |\\ \\underline{{\\dot{\\mathbf{x}}}}\\leq\\mathbf{x}\\leq\\ddagger,$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\overline{{\\mathbf{y}}}^{*}\\stackrel{\\mathrm{def}}{=}\\operatorname*{max}\\left\\{{\\mathrm{ReLU}}(\\mathbf{x})~|~\\mathbf{x}\\leq\\overline{{\\mathbf{x}}}\\right\\}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By the definition of ReLU layer (Definition 3.7), because ReLU is a monotonically increasing function, the definition above is equivalent to ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\overline{{\\mathbf{y}}}^{*}\\ \\overset{\\mathrm{def}}{=}\\operatorname*{max}\\left\\{\\overline{{\\mathbf{x}}},0\\right\\}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "To prove that $\\overline{{\\varphi}}_{\\mathtt{R e L U}}(\\mathfrak{F},\\mathfrak{X})$ captures the exact upper bound, we will show that $(\\overline{{\\mathbf{y}}}^{*},\\overline{{\\mathbf{x}}})$ is a solution to $\\overline{{\\varphi}}_{\\mathtt{R e L U}}(\\mathfrak{F},\\ddot{\\mathbf{x}})$ . In other words, the following instantiated formula $\\overline{{\\varphi}}_{\\mathtt{R e L U}}(\\overline{{\\mathbf{y}}}^{*},\\overline{{\\mathbf{x}}})$ is true: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\overline{{\\varphi}}_{\\mathtt{R e L U}}(\\overline{{\\mathbf{y}}}^{*},\\overline{{\\mathbf{x}}})\\overset{\\mathrm{def}}{=}\\overline{{\\mathbf{y}}}^{*}\\ge\\overline{{\\mathbf{x}}}\\wedge\\overline{{\\mathbf{y}}}^{*}\\ge0\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By substituting $\\overline{{\\mathbf{y}}}^{*}$ defined in Equation 9 above, our goal becomes ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left\\{\\overline{{\\mathbf{x}}},0\\right\\}\\geq\\overline{{\\mathbf{x}}}\\land\\operatorname*{max}\\left\\{\\overline{{\\mathbf{x}}},0\\right\\}\\geq0\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which is true. Hence, we have proved that $\\underline{{\\overline{{\\varphi}}}}_{\\mathtt{R e L U}}(\\underline{{\\dot{\\bf y}}},\\underline{{\\ddot{\\bf y}}},\\underline{{\\dot{\\bf x}}},\\underline{{\\ddot{\\bf x}}})$ captures the exact upper bound. ", "page_idx": 16}, {"type": "text", "text": "Theorem 3.9. Definition 3.8 is a Parametric Linear Relaxation for the ReLU layer that captures the exact upper bound. ", "page_idx": 16}, {"type": "text", "text": "Proof. By Lemma A.1 and Lemma A.2, we proved that $\\underline{{\\overline{{\\varphi}}}}_{\\mathtt{R e L U}}(\\underline{{\\dot{\\bf y}}},\\underline{{\\ddot{\\bf y}}},\\underline{{\\dot{\\bf x}}},\\underline{{\\ddot{\\bf x}}})$ is a poly-size linear formula that implies $\\forall\\mathbf{x}~\\in$ [xxx,xxx]. $\\mathtt{R e L U}(\\mathbf{x})\\ \\in\\ [\\underline{{\\dot{\\mathbf{y}}}},\\ddot{\\mathbf{y}}]$ , hence is a Parametric Linear Relaxation (Definition 3.3) for ReLU. By Lemma A.3, we proved that $\\underline{{\\overline{{\\varphi}}}}_{\\mathtt{R e L U}}(\\underline{{\\dot{\\bf y}}},\\underline{{\\ddot{\\bf y}}},\\underline{{\\dot{\\bf x}}},\\underline{{\\ddot{\\bf x}}})$ is implied by $\\smash{\\dddot{\\mathbf{y}}=\\operatorname*{max}\\left\\{\\operatorname{ReLU}(\\mathbf{x})\\,\\left|\\,\\dot{\\mathbf{\\Sigma}}\\leq\\mathbf{x}\\leq\\ddot{\\mathbf{x}}\\right\\}}$ , hence captures the exact upper bound. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "A.2 Proofs for Parametric Linear Relaxation of Affine layers with constant input bounds ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lemma A.4. $\\underline{{\\boldsymbol{\\overline{{\\varphi}}}}}_{\\mathrm{Aff}}\\left(\\underline{{\\boldsymbol{\\dot{y}}}},\\ddot{\\mathbf{y}},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\dot{\\mathbf{W}},\\dot{\\mathbf{b}}\\right)$ defined in Definition 3.11 is a poly-size linear formula. ", "page_idx": 16}, {"type": "text", "text": "Proof. As seen in Definition 3.11, $\\underline{{\\overline{{\\varphi}}}}_{\\mathtt{A f f}}\\left(\\underline{{\\dot{\\mathbf{y}}}},\\ddot{\\mathbf{y}},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\dot{\\mathbf{W}},\\dot{\\mathbf{b}}\\right)$ is a linear formula. Let $m$ and $n$ be the number of input and output dimensions of the Affine layer. $\\underline{{\\boldsymbol{\\overline{{\\varphi}}}}}_{\\mathrm{Aff}}\\left(\\underline{{\\boldsymbol{\\dot{y}}}},\\ddot{\\mathbf{y}},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\dot{\\mathbf{W}},\\dot{\\mathbf{b}}\\right)$ has $2n+4m n$ linear constraints over $2m+3n+3m n$ variables $\\mathbf{\\dot{x}}\\in\\mathbb{R}^{m}$ , $\\mathbf{\\ddot{x}}\\in\\mathbb{R}^{m}$ , $\\dot{\\mathbf b}\\in\\mathbb R^{n},\\dot{\\mathbf y}\\in\\mathbb R^{n},\\dddot{\\mathbf y}\\in\\mathbb R^{n}$ , $\\dot{\\mathbf{W}}\\in\\mathbb{R}^{n\\times m}$ , $\\dot{\\mathbf{\\Sigma}}\\dot{\\mathbf{\\Sigma}}\\in\\mathbb{R}^{n\\times m}$ and $\\mathbf{\\ddot{V}}\\in\\mathbb{R}^{n\\times m},$ ). Hence, $\\underline{{\\boldsymbol{\\overline{{\\varphi}}}}}_{\\mathrm{Aff}}\\left(\\underline{{\\boldsymbol{\\dot{y}}}},\\ddot{\\mathbf{y}},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\dot{\\mathbf{W}},\\dot{\\mathbf{b}}\\right)$ is a poly-size linear formula whose size is polynomial in the number of input and output dimensions of the Affine layer. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Lemma A.5. $\\underline{{\\varphi}}_{\\mathrm{Aff}}\\left(\\underline{{\\dot{\\mathbf{y}}}},\\underline{{\\ddot{\\mathbf{y}}}},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\dot{\\mathbf{W}},\\dot{\\mathbf{b}}\\right)\\overset{d e f}{=}\\overline{{\\varphi}}_{\\mathrm{Aff}}\\left(\\dddot{\\mathbf{y}},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\dot{\\mathbf{W}},\\dot{\\mathbf{b}}\\right)\\wedge\\underline{{\\varphi}}_{\\mathrm{Aff}}\\left(\\underline{{\\dot{\\mathbf{y}}}},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\dot{\\mathbf{W}},\\dot{\\mathbf{b}}\\right)$ defined in Definition 3.11 implies $\\forall\\mathbf{x}\\in[\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}}]$ . Affi $\\mathbf{\\Sigma}_{\\mathbf{\\Sigma}}(\\mathbf{x};\\dot{\\mathbf{W}},\\dot{\\mathbf{b}})\\in[\\mathbf{\\Psi}$ yyy,yyy]. ", "page_idx": 16}, {"type": "text", "text": "Proof. For the upper bound, recall that by Definition 3.11, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{\\varphi}}_{\\mathtt{A f f}}\\left(\\mathtt{\\ddot{y}},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\dot{\\mathbf{W}},\\dot{\\mathbf{b}}\\right)\\overset{\\mathrm{def}}{=}\\bigwedge_{j}\\dddot{\\mathbf{y}}_{j}=\\sum_{i}\\dddot{\\mathbf{V}}_{j i}+\\dot{\\mathbf{b}}_{j}\\;\\wedge\\;\\dddot{\\mathbf{V}}\\ge\\dot{\\mathbf{W}}\\underline{{\\mathbf{x}}}\\;\\wedge\\;\\dddot{\\mathbf{V}}\\ge\\dot{\\mathbf{W}}\\overline{{\\mathbf{x}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "our goal is to prove that $\\overline{{\\varphi}}_{\\mathrm{Aff}}\\left(\\ddot{\\mathbf{y}},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\dot{\\mathbf{W}},\\dot{\\mathbf{b}}\\right)$ implies ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\forall\\mathbf{x}\\in[\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}}].\\,\\mathbf{\\ddot{y}}\\geq\\mathsf{A f f\\,i n e}(\\mathbf{x};\\dot{\\mathbf{W}},\\dot{\\mathbf{b}})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We prove the above formula by proving for each $\\ddot{\\mathbf{y}}_{j}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\ddagger_{j}\\geq\\operatorname*{max}\\left\\{\\mathsf{A f f}\\,\\mathtt{i n e}(\\mathbf{x};\\dot{\\mathbf{W}},\\dot{\\mathbf{b}})_{j}\\ \\middle|\\ \\underline{{\\mathbf{x}}}\\leq\\mathbf{x}\\leq\\overline{{\\mathbf{x}}}\\right\\}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By the definition of the Affine layer (Definition 3.10), we expand the formula above to ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\ddagger_{j}\\geq\\operatorname*{max}\\left\\{\\sum_{i=0}^{m}\\mathbf{x}_{i}\\dot{\\mathbf{W}}_{j i}+\\dot{\\mathbf{b}}_{j}\\ \\big|\\ \\mathbf{x}\\leq\\mathbf{x}\\leq\\overline{{\\mathbf{x}}}\\right\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which is implied by the following formula ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\overleftrightarrow{\\mathbf{y}}_{j}\\geq\\sum_{i=0}^{m}\\operatorname*{max}\\left\\{\\mathbf{x}_{i}\\dot{\\mathbf{W}}_{j i}\\ \\middle|\\ \\mathbf{x}_{i}\\leq\\mathbf{x}_{i}\\leq\\overline{{\\mathbf{x}}}_{i}\\right\\}+\\dot{\\mathbf{b}}_{j}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Because $\\overline{{\\varphi}}_{\\mathrm{Aff}}\\left(\\ddot{\\mathbf{y}},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\dot{\\mathbf{W}},\\dot{\\mathbf{b}}\\right)$ implies $\\begin{array}{r}{\\dddot{\\mathbf{y}}_{j}=\\sum_{i=0}^{m}\\breve{\\mathbf{V}}_{j i}+\\dot{\\mathbf{b}}_{j}}\\end{array}$ , by substituting the left-hand-side $\\mathbf{\\ddot{y}}_{j}$ of the formula above with $\\textstyle\\sum_{i=0}^{m}\\bar{\\mathbf{V}}_{j i}+\\dot{\\mathbf{b}}_{j}$ , our goal becomes: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{m}\\overbar{\\mathbf{V}}_{j i}+\\dot{\\mathbf{b}}_{j}\\ge\\sum_{i=0}^{m}\\operatorname*{max}\\left\\{\\mathbf{x}_{i}\\dot{\\mathbf{W}}_{j i}~\\middle|~\\mathbf{\\underline{{x}}}_{i}\\le\\mathbf{x}_{i}\\le\\overline{{\\mathbf{x}}}_{i}\\right\\}+\\dot{\\mathbf{b}}_{j}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By subtracting $\\dot{\\mathbf{b}}_{j}$ from both sides, our goal becomes: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{m}\\breve{\\mathbf{V}}_{j i}\\geq\\sum_{i=0}^{m}\\operatorname*{max}\\left\\{\\mathbf{x}_{i}\\dot{\\mathbf{W}}_{j i}\\ \\big|\\ \\mathbf{x}_{i}\\leq\\mathbf{x}_{i}\\leq\\overline{{\\mathbf{x}}}_{i}\\right\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then we prove the above formula by proving for each $i$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\ddot{\\mathbf{V}}_{j i}\\geq\\operatorname*{max}\\left\\lbrace\\mathbf{x}_{i}\\dot{\\mathbf{W}}_{j i}\\mid\\mathbf{\\underline{{x}}}_{i}\\leq\\mathbf{x}_{i}\\leq\\overline{{\\mathbf{x}}}_{i}\\right\\rbrace}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Because $\\mathbf{x}_{i}\\dot{\\mathbf{W}}_{j i}$ changes monotonically with respect to $\\mathbf{x}_{i}$ , $\\operatorname*{nax}\\left\\{\\mathbf{x}_{i}\\dot{\\mathbf{W}}_{j i}\\mid\\underline{{\\mathbf{x}}}_{i}\\leq\\mathbf{x}_{i}\\leq\\overline{{\\mathbf{x}}}_{i}\\right\\}$ is taken when $\\mathbf{x}_{i}=\\underline{{\\mathbf{x}}}_{i}$ or $\\mathbf{x}_{i}=\\overline{{\\mathbf{x}}}_{i}$ . In other words, max $\\left\\{\\mathbf{x}_{i}\\dot{\\mathbf{W}}_{j i}\\mid\\underline{{\\mathbf{x}}}_{i}\\leq\\mathbf{x}_{i}\\leq\\overline{{\\mathbf{x}}}_{i}\\right\\}=\\operatorname*{max}\\left\\{\\underline{{\\mathbf{x}}}_{i}\\dot{\\mathbf{W}}_{j i},\\,\\overline{{\\mathbf{x}}}_{i}\\dot{\\mathbf{W}}_{j i}\\,\\right\\}$ . By substituting the right-hand-side of the above formula, our goal becomes: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{V}}_{j i}\\geq\\operatorname*{max}\\left\\{\\underline{{\\mathbf{x}}}_{i}\\dot{\\mathbf{W}}_{j i},\\,\\overline{{\\mathbf{x}}}_{i}\\dot{\\mathbf{W}}_{j i}\\right\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The formula above is equivalent to $\\begin{array}{r l r l r}{\\bar{\\mathbf{V}}_{j i}}&{{}\\geq}&{\\underline{{\\mathbf{x}}}_{i}\\dot{\\mathbf{W}}_{j i}}&{\\land}&{\\bar{\\mathbf{V}}_{j i}}&{\\geq}&{\\overline{{\\mathbf{x}}}_{i}\\dot{\\mathbf{W}}_{j i}}\\end{array}$ , which is implied by $\\overline{{\\varphi}}_{\\mathrm{Aff}}\\left(\\ddot{\\mathbf{y}},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\dot{\\mathbf{W}},\\dot{\\mathbf{b}}\\right)$ . Therefore, we have proved that $\\overline{{\\varphi}}_{\\mathrm{Aff}}\\left(\\ddot{\\mathbf{y}},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\dot{\\mathbf{W}},\\dot{\\mathbf{b}}\\right)$ implies $\\forall\\mathbf{x}\\in[\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}}].\\;\\mathbf{\\\"}\\mathbf{\\forall}\\geq\\mathsf{A f f\\,i n e}(\\mathbf{x};\\dot{\\mathbf{W}},\\dot{\\mathbf{b}})$ . ", "page_idx": 17}, {"type": "text", "text": "For the lower bound, recall that by Definition 3.11, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\underline{{\\varphi}}_{\\mathtt{A f f}}\\left(\\underline{{\\dot{\\mathbf{y}}}},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\dot{\\mathbf{W}},\\dot{\\mathbf{b}}\\right)\\overset{\\mathrm{\\scriptsize~def}}{=}\\bigwedge_{j}\\underline{{\\dot{\\mathbf{y}}}}_{j}=\\sum_{i}\\dot{\\underline{{\\mathbf{V}}}}_{j i}+\\dot{\\mathfrak{b}}_{j}\\;\\wedge\\;\\dot{\\underline{{\\mathbf{V}}}}\\le\\dot{\\mathbf{W}}\\underline{{\\mathbf{x}}}\\;\\wedge\\;\\dot{\\underline{{\\mathbf{V}}}}\\le\\dot{\\mathbf{W}}\\overline{{\\mathbf{x}}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "our goal is to prove that $\\underline{{\\varphi}}_{\\mathrm{Aff}}(\\underline{{\\dot{\\mathbf{y}}}},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\dot{\\mathbf{W}},\\dot{\\mathbf{b}})$ implies ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\forall\\mathbf{x}\\in[\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}}]\\cdot\\underline{{\\dot{\\mathbf{y}}}}\\leq\\mathtt{A f f}\\,\\mathtt{i n e}(\\mathbf{x};\\dot{\\mathbf{W}},\\dot{\\mathbf{b}})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "by proving for each $j$ where $\\dot{\\underline{{\\mathbf{y}}}}_{j}$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\dot{\\underline{{\\mathbf{y}}}}_{j}\\le\\operatorname*{min}\\left\\{\\operatorname{Aff}\\mathtt{i n e}(\\mathbf{x};\\dot{\\mathbf{W}},\\dot{\\mathbf{b}})_{j}\\ \\big|\\ \\underline{{\\mathbf{x}}}\\le\\mathbf{x}\\le\\overline{{\\mathbf{x}}}\\right\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By the definition of the Affine layer (Definition 3.10), we expand the formula above to ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\dot{\\underline{{\\mathbf{y}}}}_{j}\\le\\operatorname*{min}\\left\\{\\sum_{i=0}^{m}\\mathbf{x}_{i}\\dot{\\mathbf{W}}_{j i}+\\dot{\\mathbf{b}}_{j}\\ \\big|\\ \\underline{{\\mathbf{x}}}\\le\\mathbf{x}\\le\\overline{{\\mathbf{x}}}\\right\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which is implied by the following formula ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\underline{{\\dot{\\bf y}}}_{j}\\leq\\sum_{i=0}^{m}\\operatorname*{min}\\left\\{{\\bf x}_{i}\\dot{\\hat{\\bf W}}_{j i}~\\middle|~{\\bf\\underline{{x}}}_{i}\\leq{\\bf x}_{i}\\leq\\overline{{\\bf x}}_{i}\\right\\}+\\dot{\\bf b}_{j}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Because $\\begin{array}{r}{\\underline{{\\varphi}}_{\\mathrm{Aff}}\\left(\\underline{{\\dot{\\bf y}}},\\underline{{{\\bf x}}},\\overline{{\\bf x}},\\dot{\\bf W},\\dot{\\bf b}\\right)\\,\\mathrm{implies}\\,\\underline{{\\dot{\\bf y}}}_{j}=\\sum_{i=0}^{m}\\dot{\\bf\\underline{{V}}}_{j i}+\\dot{\\bf b}_{j},}\\end{array}$ , by substituting the left-hand-side $\\dot{\\underline{{\\mathbf{y}}}}_{j}$ of the formula above with $\\scriptstyle\\sum_{i=0}^{m}{\\dot{\\underline{{\\mathbf{V}}}}}_{j i}\\,+\\,{\\dot{\\mathbf{b}}}_{j}$ , our goal becomes: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{m}\\dot{\\underline{{\\mathbf{V}}}}_{j i}+\\dot{\\mathbf{b}}_{j}\\le\\sum_{i=0}^{m}\\operatorname*{min}\\left\\{\\mathbf{x}_{i}\\dot{\\mathbf{W}}_{j i}\\ \\big|\\ \\underline{{\\mathbf{x}}}_{i}\\le\\mathbf{x}_{i}\\le\\overline{{\\mathbf{x}}}_{i}\\right\\}+\\dot{\\mathbf{b}}_{j}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By subtracting $\\dot{\\mathbf{b}}_{j}$ from both sides, our goal becomes: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{m}\\dot{\\underline{{\\mathbf{V}}}}_{j i}\\leq\\sum_{i=0}^{m}\\operatorname*{min}\\left\\{\\mathbf{x}_{i}\\dot{\\mathbf{W}}_{j i}\\mid\\mathbf{x}_{i}\\leq\\mathbf{x}_{i}\\leq\\overline{{\\mathbf{x}}}_{i}\\right\\}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then we will prove for each $i$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\underline{{\\dot{\\bf V}}}_{j i}\\leq\\operatorname*{min}\\left\\{{\\bf x}_{i}\\dot{\\bf W}_{j i}\\ \\big|\\ {\\bf x}_{i}\\leq{\\bf x}_{i}\\leq\\overline{{\\bf x}}_{i}\\right\\}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Because $\\mathbf{x}_{i}\\dot{\\mathbf{W}}_{j i}$ changes monotonically with respect to $\\mathbf{x}_{i}$ , $\\operatorname*{min}\\left\\{\\mathbf{x}_{i}\\dot{\\mathbf{W}}_{j i}\\mid\\underline{{\\mathbf{x}}}_{i}\\leq\\mathbf{x}_{i}\\leq\\overline{{\\mathbf{x}}}_{i}\\right\\}$ is taken when $\\mathbf{x}_{i}=\\underline{{\\mathbf{x}}}_{i}$ or $\\mathbf{x}_{i}=\\overline{{\\mathbf{x}}}_{i}$ . In other words, $\\operatorname*{min}\\Big\\{\\mathbf{x}_{i}\\dot{\\boldsymbol{\\mathsf{W}}}_{j i}~\\big|~\\underline{{\\mathbf{x}}}_{i}\\le\\mathbf{x}_{i}\\le\\overline{{\\mathbf{x}}}_{i}\\Big\\}=\\operatorname*{min}\\Big\\{\\underline{{\\mathbf{x}}}_{i}\\dot{\\boldsymbol{\\mathsf{W}}}_{j i},~\\overline{{\\mathbf{x}}}_{i}\\dot{\\boldsymbol{\\mathsf{W}}}_{j i}\\Big\\}.$ . By substituting the right-hand-side of the above formula, our goal becomes: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\underline{{\\dot{\\bf V}}}_{j i}\\le\\operatorname*{min}\\left\\{{\\underline{{\\bf x}}}_{i}\\dot{\\bf W}_{j i},\\,\\overline{{\\bf x}}_{i}\\dot{\\bf W}_{j i}\\right\\}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The formula above is equivalent to $\\underline{{\\dot{\\bf V}}}_{j i}\\;\\;\\le\\;\\;{\\bf{x}}_{i}\\dot{\\bf W}_{j i}\\;\\;\\;\\;\\land\\;\\;\\;\\;\\underline{{\\dot{\\bf V}}}_{j i}\\;\\;\\le\\;\\;\\overline{{\\bf{x}}}_{i}\\dot{\\bf W}_{j i}$ , which is implied by $\\underline{{\\varphi}}_{\\mathrm{Aff}}(\\underline{{\\dot{\\mathbf{y}}}},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\dot{\\mathbf{W}},\\dot{\\mathbf{b}})$ . Therefore, we have proved that $\\underline{{\\varphi}}_{\\mathrm{Aff}}\\,(\\underline{{\\dot{\\mathbf{y}}}},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\dot{\\mathbf{W}},\\dot{\\mathbf{b}})$ implies $\\forall\\mathbf{x}\\in[\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}}].\\;\\underline{{\\dot{\\mathbf{y}}}}\\leq\\mathtt{A f f\\,i n e}(\\mathbf{x};\\dot{\\mathbf{W}},\\dot{\\mathbf{b}})$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Lemma A.6. $\\begin{array}{r}{\\underline{{\\overline{{\\varphi}}}}_{\\mathtt{A f f}}(\\underline{{\\dot{\\mathbf{y}}}},\\underline{{\\ddot{\\mathbf{y}}}},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\dot{\\mathbf{W}},\\dot{\\mathbf{b}})\\quad\\quad\\stackrel{d e f}{=}\\quad\\overline{{\\varphi}}_{\\mathtt{A f f}}(\\ddot{\\mathbf{y}},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\dot{\\mathbf{W}},\\dot{\\mathbf{b}})\\quad\\wedge\\underset{n i t i o n}{\\underbrace{\\varphi}}(\\underline{{\\dot{\\mathbf{y}}}},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\dot{\\mathbf{W}},\\dot{\\mathbf{b}})}\\\\ {\\textnormal{\\Delta}\\quad\\cdots\\quad\\cdots\\quad\\ i s\\quad\\operatorname*{implied}\\quad b y\\quad\\ddot{\\mathbf{y}}=\\operatorname*{max}\\left\\{\\mathtt{A f f}\\;\\mathtt{i n e}(\\mathbf{x};\\dot{\\mathbf{W}},\\dot{\\mathbf{b}})\\;\\right\\}\\;\\underline{{\\mathbf{x}}}\\leq\\mathbf{x}\\leq\\overline{{\\mathbf{x}}}\\right\\}}\\end{array}$ defined in Def $\\wedge$ $\\underline{{\\dot{\\mathbf{y}}}}=\\operatorname*{min}\\left\\{\\mathop{\\mathtt{A f f}}\\mathop{\\mathtt{f i n e}}(\\mathbf{x};\\dot{\\mathbf{W}},\\dot{\\mathbf{b}})~|~\\underline{{\\mathbf{x}}}\\le\\mathbf{x}\\le\\overline{{\\mathbf{x}}}\\right\\}$ , hence captures the exact upper and lower bounds. ", "page_idx": 18}, {"type": "text", "text": "Proof. For the upper bound, recall that by Definition 3.11, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\overline{{\\varphi}}_{\\mathtt{A f f}}\\left(\\ddot{\\mathtt{y}},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\dot{\\mathtt{W}},\\dot{\\mathtt{b}}\\right)\\overset{\\mathrm{def}}{=}\\bigwedge_{j}\\ddot{\\mathtt{y}}_{j}=\\sum_{i}\\ddot{\\mathtt{V}}_{j i}+\\dot{\\mathtt{b}}_{j}\\;\\wedge\\;\\ddot{\\mathtt{V}}\\ge\\dot{\\mathtt{W}}\\underline{{\\mathbf{x}}}\\;\\wedge\\;\\ddot{\\mathtt{V}}\\ge\\dot{\\mathtt{W}}\\overline{{\\mathbf{x}}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Given constant input bounds $[\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}}]$ , for any solution $(\\overline{{\\mathbf{y}}}^{*},\\mathbf{W},\\mathbf{b})$ to $\\ddagger=\\operatorname*{max}\\left\\{\\mathtt{A f f}\\,\\mathtt{i n e}(\\mathbf{x};\\dot{\\mathbf{W}},\\dot{\\mathbf{b}})\\ |\\ \\underline{{\\mathbf{x}}}\\leq\\mathbf{x}\\leq\\overline{{\\mathbf{x}}}\\right\\}$ , we have the exact constant output upper bound defined as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\overline{{\\mathbf{y}}}^{*}\\stackrel{\\mathrm{def}}{=}\\operatorname*{max}\\left\\{\\mathbf{Aff}\\,\\mathbf{ine}\\big(\\mathbf{x};\\mathbf{W},\\mathbf{b}\\big)\\ \\big|\\ \\underline{{\\mathbf{x}}}\\leq\\mathbf{x}\\leq\\overline{{\\mathbf{x}}}\\right\\}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By the definition of Affine layer (Definition 3.10), it is equivalent to ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\overline{{\\mathbf{y}}}_{j}^{*}\\stackrel{\\mathrm{def}}{=}\\sum_{i}\\operatorname*{max}\\left\\{\\mathbf{x}_{i}\\mathbf{W}_{j i}\\ \\middle|\\ \\mathbf{x}\\leq\\mathbf{x}\\leq\\overline{{\\mathbf{x}}}\\right\\}+\\mathbf{b}_{j}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To prove that $\\overline{{\\varphi}}_{\\tt A f f}$ captures the exact upper bound, we will show that by substituting $(\\overline{{\\mathbf{y}}}^{*},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\mathbf{W},\\mathbf{b})$ in $\\overline{{\\varphi}}_{\\mathrm{Aff}}\\left(\\underline{{\\dot{\\mathbf{y}}}},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\dot{\\mathbf{W}},\\dot{\\mathbf{b}}\\right)$ , the instantiated formula $\\boldsymbol{\\overline{{\\varphi}}}_{\\mathrm{Aff}}\\left(\\overline{{\\mathbf{y}}}^{*},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\mathbf{W},\\mathbf{b}\\right)$ , as shown below, is satisfiable: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\overline{{\\varphi}}_{\\mathtt{A f f}}\\left(\\overline{{\\mathbf{y}}}^{*},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\mathbf{W},\\mathbf{b}\\right)\\overset{\\mathrm{def}}{=}\\bigwedge_{j}\\overline{{\\mathbf{y}}}_{j}^{*}=\\sum_{i}\\overline{{\\mathbf{V}}}_{j i}+\\mathbf{b}_{j}\\;\\wedge\\;\\overleftarrow{\\mathbf{V}}\\ge\\dot{\\mathbf{W}}\\underline{{\\mathbf{x}}}\\;\\wedge\\;\\overleftarrow{\\mathbf{V}}\\ge\\dot{\\mathbf{W}}\\overline{{\\mathbf{x}}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which is equivalent to prove the following formula for any index $j$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\overline{{\\mathbf{y}}}_{j}^{*}=\\sum_{i}\\overline{{\\mathbf{V}}}_{j i}+\\mathbf{b}_{j}\\,\\mathrm{~\\wedge~}\\,\\overleftarrow{\\mathbf{V}}\\geq\\dot{\\mathbf{W}}\\underline{{\\mathbf{x}}}\\,\\mathrm{~\\wedge~}\\,\\overleftarrow{\\mathbf{V}}\\ge\\dot{\\mathbf{W}}\\overline{{\\mathbf{x}}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By substituting $\\overline{{\\mathbf{y}}}_{j}^{*}$ defined in Equation 10 above, our goal becomes ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{i}\\operatorname*{max}\\left\\lbrace\\mathbf{x}_{i}\\mathbf{W}_{j i}\\ \\middle|\\ \\mathbf{x}\\leq\\mathbf{x}\\leq\\overline{{\\mathbf{x}}}\\right\\rbrace+\\mathbf{b}_{j}=\\sum_{i}\\overline{{\\mathbf{V}}}_{j i}+\\mathbf{b}_{j}\\ \\wedge\\ \\widetilde{\\mathbf{V}}\\geq\\dot{\\mathbf{W}}\\mathbf{x}\\ \\wedge\\ \\overleftarrow{\\mathbf{V}}\\geq\\dot{\\mathbf{W}}\\overline{{\\mathbf{x}}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which is implied by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left\\lbrace\\mathbf{x}_{i}\\mathbf{W}_{j i}\\;\\middle|\\;\\underline{{\\mathbf{x}}}\\leq\\mathbf{x}\\leq\\overline{{\\mathbf{x}}}\\right\\rbrace=\\overline{{\\mathbf{V}}}_{j i}\\;\\wedge\\;\\overleftarrow{\\mathbf{V}}_{j i}\\;\\geq\\mathbf{W}_{j i}\\underline{{\\mathbf{x}}}_{i}\\;\\wedge\\;\\overleftarrow{\\mathbf{V}}_{j i}\\geq\\mathbf{W}_{j i}\\overline{{\\mathbf{x}}}_{i}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "If $\\mathbf{{W}}_{j i}\\,\\geq\\,0$ , $\\overline{{\\mathbf{V}}}_{j i}^{*}\\ \\stackrel{\\mathrm{def}}{=}\\mathbf{W}_{j i}\\overline{{\\mathbf{x}}}_{i}$ is a solution to this formula; otherwise $\\overline{{\\mathbf{V}}}_{j i}^{*}\\ \\stackrel{\\mathrm{def}}{=}\\mathbf{W}_{j i}\\mathbf{\\underline{{X}}}_{i}$ is a solution to this formula. Hence, we have proved that $\\boldsymbol{\\overline{{\\varphi}}}_{\\mathrm{Aff}}\\left(\\mathbf{\\overline{{y}}}^{*},\\underline{{\\mathbf{x}}},\\mathbf{\\overline{{x}}},\\mathbf{W},\\mathbf{b}\\right)$ is satisfiable and $\\overline{{\\varphi}}_{\\mathrm{Aff}}(\\underline{{\\dot{\\bf y}}},\\underline{{\\bf x}},\\overline{{\\bf x}},\\dot{\\bf W},\\dot{\\bf b})$ captures the exact upper bound. ", "page_idx": 19}, {"type": "text", "text": "For the lower bound, recall that by Definition 3.11, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\underline{{\\varphi}}_{\\mathtt{A f f}}\\left(\\underline{{\\dot{\\mathbf{y}}}},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\dot{\\mathbf{W}},\\dot{\\mathbf{b}}\\right)\\overset{\\mathrm{\\scriptsize~def}}{=}\\bigwedge_{j}\\dot{\\underline{{\\mathbf{y}}}}_{j}=\\sum_{i}\\dot{\\underline{{\\mathbf{V}}}}_{j i}+\\dot{\\mathfrak{b}}_{j}\\;\\wedge\\;\\dot{\\underline{{\\mathbf{V}}}}\\le\\dot{\\mathbf{W}}\\underline{{\\mathbf{x}}}\\;\\wedge\\;\\dot{\\underline{{\\mathbf{V}}}}\\le\\dot{\\mathbf{W}}\\overline{{\\mathbf{x}}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Given constant input bounds $[\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}}]$ , for any solution $(\\underline{{\\mathbf{y}}}^{*},\\bar{\\mathbf{W}},\\mathbf{b})$ to $\\underline{{\\dot{\\mathbf{y}}}}=\\operatorname*{min}\\left\\{\\mathop{\\mathtt{A f f}}\\mathtt{i n e}(\\mathbf{x};\\dot{\\mathbf{W}},\\dot{\\mathbf{b}})~|~\\underline{{\\mathbf{x}}}\\le\\mathbf{x}\\le\\overline{{\\mathbf{x}}}\\right\\}$ , we have the exact constant output lower bound defined as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\underline{{\\mathbf{y}}}^{*}\\stackrel{\\mathrm{def}}{=}\\operatorname*{min}\\left\\{\\mathbf{Aff}\\,\\mathbf{ine}\\big(\\mathbf{x};\\mathbf{W},\\mathbf{b}\\big)\\ \\big|\\ \\underline{{\\mathbf{x}}}\\leq\\mathbf{x}\\leq\\overline{{\\mathbf{x}}}\\right\\}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By the definition of Affine layer (Definition 3.10), it is equivalent to ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\underline{{\\mathbf{y}}}_{j}^{*}\\stackrel{\\mathrm{def}}{=}\\sum_{i}\\operatorname*{min}\\left\\{\\mathbf{x}_{i}\\mathbf{W}_{j i}\\mid\\underline{{\\mathbf{x}}}\\leq\\mathbf{x}\\leq\\overline{{\\mathbf{x}}}\\right\\}+\\mathbf{b}_{j}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "To prove that $\\underline{{\\varphi}}_{\\mathtt{A f f}}$ captures the exact lower bound, we will show that by substituting $(\\underline{{\\mathbf{y}}}^{*},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\mathbf{W},\\mathbf{b})$ in $\\underline{{\\varphi}}_{\\mathrm{Aff}}(\\underline{{\\dot{\\mathbf{y}}}},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\dot{\\mathbf{W}},\\dot{\\mathbf{b}})$ , the instantiated formula $\\underline{{\\varphi}}_{\\mathrm{Aff}}\\left(\\underline{{\\mathbf{y}}}^{*},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\mathbf{W},\\mathbf{b}\\right)$ , as shown below, is satisfiable: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\underline{{\\varphi}}_{\\mathrm{Aff}}\\left(\\underline{{\\mathbf{y}}}^{*},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\mathbf{W},\\mathbf{b}\\right)\\overset{\\mathrm{def}}{=}\\bigwedge_{j}\\underline{{\\mathbf{y}}}_{j}^{*}=\\sum_{i}\\dot{\\underline{{\\mathbf{V}}}}_{j i}+\\mathbf{b}_{j}\\;\\wedge\\;\\dot{\\underline{{\\mathbf{V}}}}\\le\\mathbf{W}\\underline{{\\mathbf{x}}}\\;\\wedge\\;\\dot{\\underline{{\\mathbf{V}}}}\\le\\mathbf{W}\\overline{{\\mathbf{x}}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which is equivalent to prove the following formula for any index $j$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{\\underline{{y}}}_{j}^{*}=\\sum_{i}\\underline{{\\dot{\\mathbf{V}}}}_{j i}+\\mathbf{b}_{j}\\;\\wedge\\;\\underline{{\\dot{\\mathbf{V}}}}\\leq\\mathbf{W}\\underline{{\\mathbf{x}}}\\;\\wedge\\;\\underline{{\\dot{\\mathbf{V}}}}\\leq\\mathbf{W}\\overline{{\\mathbf{x}}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By substituting $\\underline{{\\mathbf{y}}}_{j}^{*}$ defined in Equation 11 above, our goal becomes ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{i}\\operatorname*{min}\\left\\{\\mathbf{x}_{i}\\mathbf{W}_{j i}~\\middle|~\\underline{{\\mathbf{x}}}\\leq\\mathbf{x}\\leq\\overline{{\\mathbf{x}}}\\right\\}+\\mathbf{b}_{j}=\\sum_{i}\\dot{\\underline{{\\mathbf{V}}}}_{j i}+\\mathbf{b}_{j}~\\wedge~\\dot{\\underline{{\\mathbf{V}}}}\\leq\\mathbf{W}\\underline{{\\mathbf{x}}}~\\wedge~\\dot{\\underline{{\\mathbf{V}}}}\\leq\\mathbf{W}\\overline{{\\mathbf{x}}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which is implied by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\left\\lbrace\\mathbf{x}_{i}\\mathbf{W}_{j i}\\;\\middle|\\;\\underline{{\\mathbf{x}}}\\leq\\mathbf{x}\\leq\\overline{{\\mathbf{x}}}\\right\\rbrace=\\dot{\\underline{{\\mathbf{V}}}}_{j i}\\;\\wedge\\;\\dot{\\underline{{\\mathbf{V}}}}_{j i}\\leq\\mathbf{W}_{j i}\\underline{{\\mathbf{x}}}_{i}\\;\\wedge\\;\\dot{\\underline{{\\mathbf{V}}}}_{j i}\\leq\\mathbf{W}_{j i}\\overline{{\\mathbf{x}}}_{i}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "If $\\begin{array}{r}{\\mathbf{W}_{j i}\\,\\geq\\,0,\\underline{{\\mathbf{V}}}_{j i}^{*}\\,\\overset{\\mathrm{def}}{=}\\,\\mathbf{W}_{j i}\\\\underline{{\\mathbf{x}}}_{i}}\\end{array}$ is a solution to this formula; otherwise $\\underline{{\\mathbf{V}}}_{j i}^{*}\\ \\underline{{\\stackrel{\\mathrm{def}}{=}}}\\ \\mathbf{W}_{j i}\\overline{{\\mathbf{x}}}_{i}$ is a solution to this formula. Hence, we have proved that $\\underline{{\\varphi}}_{\\mathrm{Aff}}\\left(\\underline{{\\mathbf{y}}}^{*},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\mathbf{W},\\mathbf{b}\\right)$ is satisfiable and $\\underline{{\\varphi}}_{\\mathrm{Aff}}(\\underline{{\\dot{\\mathbf{y}}}},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\dot{\\mathbf{W}},\\dot{\\mathbf{b}})$ captures the exact lower bound. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Theorem 3.12. Definition 3.11 is a Parametric Linear Relaxation for the Affine layer with constant input bounds [xxx,xxx] that captures the exact lower and upper bounds. ", "page_idx": 19}, {"type": "text", "text": "Proof. By Lemma A.4 and Lemma A.5, we proved that $\\underline{{\\boldsymbol{\\overline{{\\varphi}}}}}_{\\mathrm{Aff}}\\left(\\underline{{\\boldsymbol{\\dot{y}}}},\\underline{{\\boldsymbol{\\ddot{y}}}},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\dot{\\mathbf{W}},\\dot{\\mathbf{b}}\\right)$ is a poly-size linear formula that implies $\\forall\\mathbf{x}\\in[\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}}]$ . Affin $\\mathbf{\\Psi}\\circ(\\mathbf{x};\\dot{\\mathbf{W}},\\dot{\\mathbf{b}})\\,\\in\\,[\\!.$ [yyy,yyy], hence is a Parametric Linear Relaxation (Definition 3.3) for the Affine layer. By Lemma A.6, we proved that $\\underline{{\\boldsymbol{\\overline{{\\varphi}}}}}_{\\mathrm{Aff}}\\left(\\underline{{\\boldsymbol{\\dot{y}}}},\\ddot{\\mathbf{y}},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\dot{\\mathbf{W}},\\dot{\\mathbf{b}}\\right)$ is implied by $\\ddagger=\\operatorname*{max}\\left\\{\\mathtt{A f f\\,i n e}(\\mathbf{x};\\dot{\\mathbb{W}},\\dot{\\mathbf{b}})\\,\\left|\\,\\underline{{\\mathtt{x}}}\\leq\\mathbf{x}\\leq\\overline{{\\mathtt{x}}}\\right\\}\\wedge\\underline{{\\dot{\\mathtt{y}}}}=\\operatorname*{min}\\left\\{\\mathtt{A f f\\,i n e}(\\mathbf{x};\\dot{\\mathbb{W}},\\dot{\\mathbf{b}})\\,\\left|\\,\\underline{{\\mathtt{x}}}\\leq\\mathbf{x}\\leq\\overline{{\\mathtt{x}}}\\right\\}\\wedge\\mathtt{y}.$ , hence captures the exact upper and lower bounds. \u5382 ", "page_idx": 19}, {"type": "text", "text": "A.3 Proofs for Parametric Linear Relaxation of Affine layers with variable input bounds ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Lemma A.7. $\\underline{{\\overline{{\\varphi}}}}_{\\mathtt{A f f}}\\left(\\underline{{\\dot{\\mathbf{y}}}},\\ddot{\\mathbf{y}},\\dot{\\underline{{\\mathbf{x}}}},\\ddot{\\mathbf{x}},\\mathbf{W},\\dot{\\mathbf{b}}\\right)$ defined in Definition 3.13 is a poly-size linear formula. ", "page_idx": 20}, {"type": "text", "text": "Proof. As seen in Definition 3.13, $\\underline{{\\overline{{\\varphi}}}}_{\\mathtt{A f f}}\\left(\\underline{{\\dot{\\mathbf{y}}}},\\ddot{\\mathbf{y}},\\dot{\\underline{{\\mathbf{x}}}},\\ddot{\\mathbf{x}},\\mathbf{W},\\dot{\\mathbf{b}}\\right)$ is a linear formula. Let $m$ and $n$ be the number of input and output dimensions of the Affine layer. $\\underline{{\\boldsymbol{\\overline{{\\varphi}}}}}_{\\mathrm{Aff}}\\left(\\underline{{\\boldsymbol{\\dot{y}}}},\\ddot{\\mathbf{y}},\\underline{{\\dot{\\mathbf{x}}}},\\ddot{\\mathbf{x}},\\mathbf{W},\\dot{\\mathbf{b}}\\right)$ has $2n$ linear constraints over $2m+3n$ variables $(\\underline{{\\dot{\\mathbf{x}}}}\\in\\mathbb{R}^{m},\\,\\ddot{\\mathbf{x}}\\in\\mathbb{R}^{m},\\,\\dot{\\mathbf{b}}\\in\\mathbb{R}^{n},\\,\\underline{{\\dot{\\mathbf{y}}}}\\in\\mathbb{R}^{n}$ and $\\mathbf{\\ddot{y}}^{\\textit{\\scriptsize e}}\\in\\mathbb{R}^{n}$ ). Hence, $\\underline{{\\overline{{\\varphi}}}}_{\\mathtt{A f f}}\\left(\\underline{{\\dot{\\mathbf{y}}}},\\ddot{\\mathbf{y}},\\dot{\\underline{{\\mathbf{x}}}},\\ddot{\\mathbf{x}},\\mathbf{W},\\dot{\\mathbf{b}}\\right)$ is a poly-size linear formula whose size is polynomial in the number of input and output dimensions of the Affine layer. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Lemma A.8. $\\underline{{\\varphi}}_{\\mathrm{Aff}}\\left(\\underline{{\\dot{\\mathbf{y}}}},\\underline{{\\ddot{\\mathbf{y}}}},\\underline{{\\dot{\\mathbf{x}}}},\\underline{{\\ddot{\\mathbf{x}}}},\\mathbf{W},\\dot{\\mathbf{b}}\\right)\\overset{d e f}{=}\\overline{{\\varphi}}_{\\mathrm{Aff}}\\left(\\underline{{\\ddot{\\mathbf{y}}}},\\underline{{\\dot{\\mathbf{x}}}},\\underline{{\\ddot{\\mathbf{x}}}},\\mathbf{W},\\dot{\\mathbf{b}}\\right)\\wedge\\underline{{\\varphi}}_{\\mathrm{Aff}}\\left(\\underline{{\\dot{\\mathbf{y}}}},\\underline{{\\dot{\\mathbf{x}}}},\\underline{{\\ddot{\\mathbf{x}}}},\\mathbf{W},\\dot{\\mathbf{b}}\\right)$ defined in Definition 3.13 implies that $\\forall\\mathbf{x}\\in$ [xxx,xxx]. Affine $(\\mathbf{x};\\mathbf{W},\\dot{\\mathbf{b}})\\in$ [yyy,yyy]. ", "page_idx": 20}, {"type": "text", "text": "Proof. For the upper bound, recall that by Definition 3.13, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\overline{{\\varphi}}_{\\mathrm{Aff}}\\left(\\ddot{\\mathbf{y}},\\underline{{\\dot{\\mathbf{x}}}},\\ddot{\\mathbf{x}},\\mathbf{W},\\dot{\\mathbf{b}}\\right)\\stackrel{\\mathrm{def}}{=}\\bigwedge_{j}\\ddot{\\mathbf{y}}_{j}=\\sum_{i}\\overline{{\\mathbf{V}}}_{j i}+\\dot{\\mathbf{b}}_{j}\\,\\mathrm{where}\\left\\{\\begin{array}{l l}{\\overline{{\\mathbf{V}}}_{j i}\\stackrel{\\mathrm{def}}{=}\\ddot{\\mathbf{x}}_{i}\\mathbf{W}_{j i}\\,\\,\\,\\mathrm{if}\\,\\,\\,\\mathbf{W}_{j i}\\geq0}\\\\ {\\overline{{\\mathbf{V}}}_{j i}\\stackrel{\\mathrm{def}}{=}\\dot{\\underline{{\\mathbf{x}}}}_{i}\\mathbf{W}_{j i}\\,\\,\\,\\mathrm{otherwise}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Our goal is to prove that $\\overline{{\\varphi}}_{\\mathtt{A f f}}(\\ddot{\\mathbf{y}},\\dot{\\underline{{\\mathbf{x}}}},\\ddot{\\mathbf{x}},\\mathbf{W},\\dot{\\mathbf{b}})$ implies ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\forall\\mathbf{x}\\in[\\underline{{\\dot{\\mathbf{x}}}},\\ddot{\\mathbf{x}}].\\,\\dddot{\\mathbf{y}}\\geq\\mathtt{A f f\\,i n e}(\\mathbf{x};\\mathbf{W},\\dot{\\mathbf{b}})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We prove the above formula by proving for each $\\ddot{\\mathbf{y}}_{j}$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\ddagger_{j}\\geq\\operatorname*{max}\\left\\{\\mathsf{A f f}\\,\\mathtt{i n e}(\\mathbf{x};\\mathbf{W},\\dot{\\mathbf{b}})_{j}\\ \\big|\\ \\dot{\\underline{{\\mathbf{x}}}}\\leq\\mathbf{x}\\leq\\ddagger\\right\\}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By the definition of the Affine layer (Definition 3.10), we expand the formula above to ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\displaystyle\\overleftarrow{\\mathbf{y}}_{j}\\geq\\operatorname*{max}\\left\\{\\sum_{i=0}^{m}\\mathbf{x}_{i}\\mathbf{W}_{j i}+\\dot{\\mathbf{b}}_{j}~\\middle|~\\dot{\\mathbf{x}}\\leq\\mathbf{x}\\leq\\ddot{\\mathbf{x}}\\right\\}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which is implied by the following formula ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\widetilde{\\mathbf{y}}_{j}\\geq\\sum_{i=0}^{m}\\operatorname*{max}\\left\\{\\mathbf{x}_{i}\\mathbf{W}_{j i}\\;\\middle|\\;\\dot{\\underline{{\\mathbf{x}}}}_{i}\\leq\\mathbf{x}_{i}\\leq\\dddot{\\mathbf{x}}_{i}\\right\\}+\\dot{\\mathbf{b}}_{j}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By Definition 3.13, $\\overline{{\\varphi}}_{\\mathrm{Aff}}\\left(\\ddot{\\mathbf{y}},\\dot{\\underline{{\\mathbf{x}}}},\\ddot{\\mathbf{x}},\\mathbf{W},\\dot{\\mathbf{b}}\\right)$ implies $\\begin{array}{r}{\\ddot{\\bf y}_{j}=\\sum_{i=0}^{m}\\overline{{\\bf V}}_{j i}+\\dot{\\bf b}_{j}}\\end{array}$ . By substituting the left-handside $\\ddot{\\mathbf{y}}_{j}$ of the formula above with $\\scriptstyle\\sum_{i=0}^{m}\\overline{{\\mathbf{V}}}_{j i}+\\dot{\\mathbf{b}}_{j}$ , our goal becomes: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{m}\\overline{{\\mathbf{V}}}_{j i}+\\dot{\\mathbf{b}}_{j}\\ge\\sum_{i=0}^{m}\\operatorname*{max}\\left\\{\\mathbf{x}_{i}\\mathbf{W}_{j i}\\ \\big|\\ \\dot{\\underline{{\\mathbf{x}}}}_{i}\\le\\mathbf{x}_{i}\\le\\dddot{\\mathbf{x}}_{i}\\right\\}+\\dot{\\mathbf{b}}_{j}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By subtracting $\\dot{\\mathbf{b}}_{j}$ from both sides, our goal becomes: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{m}\\overline{{\\mathbf{V}}}_{j i}\\geq\\sum_{i=0}^{m}\\operatorname*{max}\\left\\{\\mathbf{x}_{i}\\mathbf{W}_{j i}\\ \\big|\\ \\underline{{\\dot{\\mathbf{x}}}}_{i}\\leq\\mathbf{x}_{i}\\leq\\dddot{\\mathbf{x}}_{i}\\right\\}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then we prove the above formula by proving for each $i$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\overline{{\\mathbf{V}}}_{j i}\\geq\\operatorname*{max}\\left\\lbrace\\mathbf{x}_{i}\\mathbf{W}_{j i}\\;\\middle|\\;\\dot{\\mathbf{\\Sigma}}_{\\underline{{{\\mathbf{x}}}}_{i}}\\leq\\mathbf{x}_{i}\\leq\\ddot{\\mathbf{x}}_{i}\\right\\rbrace\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For the left-hand-side, we have $\\begin{array}{r l r}{\\overline{{\\mathbf{V}}}_{j i}}&{\\stackrel{\\mathrm{def}}{=}}&{\\left\\{\\ddot{\\mathbf{x}}_{i}\\mathbf{W}_{j i}\\quad\\mathbf{W}_{j i}\\geq0\\right.}\\\\ &{}&{\\left.\\left\\{\\dot{\\mathbf{x}}_{i}\\mathbf{W}_{j i}\\quad\\mathrm{otherwise}\\right.\\right.}\\end{array}$ For the right-hand-side, because $\\mathbf{x}_{i}\\mathbf{W}_{j i}$ changes monotonically with respect to $\\mathbf{x}_{i}$ , and $\\mathbf{W}_{j i}$ is a given constant, we ", "page_idx": 20}, {"type": "text", "text": "have max $\\begin{array}{r l r}{\\left\\{\\mathbf{x}_{i}\\mathbf{W}_{j i}\\;\\middle|\\;\\underline{{\\dot{\\mathbf{x}}}}_{i}\\leq\\mathbf{x}_{i}\\leq\\dddot{\\mathbf{x}}_{i}\\right\\}}&{=}&{\\left\\{\\ddot{\\mathbf{x}}_{i}\\mathbf{W}_{j i}\\quad\\mathbf{W}_{j i}\\geq0\\right.}\\\\ &{}&{\\left.\\left\\{\\dot{\\mathbf{x}}_{i}\\mathbf{W}_{j i}\\quad\\mathrm{otherwise}\\right.\\right.}\\end{array}$ which is the same as the def\u201d   \ninition of the left-hand-side $\\overline{{\\mathbf{V}}}_{j i}$ . Thus, we have proved that $\\overline{{\\varphi}}_{\\mathtt{A f f}}(\\ddot{\\mathbf{y}},\\dot{\\underline{{\\mathbf{x}}}},\\ddot{\\mathbf{x}},\\mathbf{w},\\dot{\\mathbf{b}})$ implies   \n$\\forall\\mathbf{x}\\in$ [xxx,xxx]. $\\\"\\bar{\\mathbf{y}}\\geq\\mathsf{A f f\\,i n e}(\\mathbf{x};\\mathbf{W},\\dot{\\mathbf{b}})$ . ", "page_idx": 21}, {"type": "text", "text": "For the lower bound, recall that by Definition 3.13, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\underline{{\\varphi}}_{\\mathrm{Aff}}\\left(\\underline{{\\dot{\\bf y}}},\\underline{{\\dot{\\bf x}}},\\overline{{\\ddot{\\bf x}}},\\mathbf{W},\\dot{\\mathbf{b}}\\right)\\stackrel{\\mathrm{def}}{=}\\bigwedge_{j}\\underline{{\\dot{\\bf y}}}_{j}=\\sum_{i}\\mathbf{\\underline{{V}}}_{j i}+\\dot{\\bf b}_{j}\\;\\mathrm{where}\\left\\{\\begin{array}{l l}{\\underline{{\\nabla_{j i}}}\\stackrel{\\mathrm{def}}{=}\\underline{{\\dot{\\bf x}}}_{j}\\mathbf{W}_{j i}\\;\\;\\mathrm{if}\\;\\;\\mathbf{W}_{j i}\\ge0}\\\\ {\\underline{{\\nabla_{j i}}}\\stackrel{\\mathrm{def}}{=}\\vec{\\bf x}_{j}\\mathbf{W}_{j i}\\;\\;\\mathrm{otherwise}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Our goal is to prove that $\\underline{{\\varphi}}_{\\mathrm{Aff}}(\\underline{{\\dot{\\mathbf{y}}}},\\underline{{\\dot{\\mathbf{x}}}},\\underline{{\\ddot{\\mathbf{x}}}},\\mathbf{W},\\dot{\\mathbf{b}})$ implies ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\forall\\mathbf{x}\\in[\\underline{{\\dot{\\mathbf{x}}}},\\ddot{\\mathbf{x}}]\\mathbf{\\cdot}\\,\\underline{{\\dot{\\mathbf{y}}}}\\leq\\mathtt{A f f}\\,\\mathtt{i n e}(\\mathbf{x};\\mathbf{W},\\dot{\\mathbf{b}})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We prove the above formula by proving for each $\\dot{\\underline{{\\mathbf{y}}}}_{j}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\dot{\\underline{{\\mathbf{y}}}}_{j}\\leq\\operatorname*{min}\\left\\{\\operatorname{Aff}\\mathtt{i n e}(\\mathbf{x};\\mathbf{W},\\dot{\\mathbf{b}})_{j}~\\middle|~\\dot{\\underline{{\\mathbf{x}}}}\\leq\\mathbf{x}\\leq\\ddot{\\mathbf{x}}\\right\\}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By the definition of the Affine layer (Definition 3.10), we expand the formula above to ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\dot{\\underline{{\\mathbf{y}}}}_{j}\\leq\\operatorname*{min}\\left\\{\\sum_{i=0}^{m}\\mathbf{x}_{i}\\mathbf{W}_{j i}+\\dot{\\mathbf{b}}_{j}\\ \\big|\\ \\dot{\\underline{{\\mathbf{x}}}}\\leq\\mathbf{x}\\leq\\ddot{\\mathbf{x}}\\right\\}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which is implied by the following formula ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\underline{{\\dot{\\bf y}}}_{j}\\leq\\sum_{i=0}^{m}\\operatorname*{min}\\left\\{\\mathbf{x}_{i}\\mathbf{W}_{j i}\\ \\middle|\\ \\underline{{\\dot{\\bf x}}}_{i}\\leq\\mathbf{x}_{i}\\leq\\dddot{\\mathbf{x}}_{i}\\right\\}+\\dot{\\mathbf{b}}_{j}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By Definition 3.13, $\\underline{{\\varphi}}_{\\mathrm{Aff}}\\,(\\underline{{\\dot{\\mathbf{y}}}},\\underline{{\\dot{\\mathbf{x}}}},\\underline{{\\ddot{\\mathbf{x}}}},\\mathbf{W},\\dot{\\mathbf{b}})$ implies $\\begin{array}{r}{\\underline{{\\dot{\\bf y}}}_{j}=\\sum_{i=0}^{m}{\\bf\\underline{{V}}}_{j i}+\\dot{{\\bf b}}_{j}}\\end{array}$ . By substituting the left-handside $\\dot{\\underline{{\\mathbf{y}}}}_{j}$ of the formula above with $\\begin{array}{r}{\\sum_{i=0}^{m}\\underline{{\\mathbf{V}}}_{j i}+\\dot{\\mathbf{b}}_{j}}\\end{array}$ , our goal becomes: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{m}\\underline{{\\mathbf{V}}}_{j i}+\\dot{\\mathbf{b}}_{j}\\le\\sum_{i=0}^{m}\\operatorname*{min}\\left\\{\\mathbf{x}_{i}\\mathbf{W}_{j i}\\ \\big|\\ \\dot{\\underline{{\\mathbf{x}}}}_{i}\\le\\mathbf{x}_{i}\\le\\dddot{\\mathbf{x}}_{i}\\right\\}+\\dot{\\mathbf{b}}_{j}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By subtracting $\\dot{\\mathbf{b}}_{j}$ from both sides, our goal becomes: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{m}\\underline{{\\mathbf{V}}}_{j i}\\leq\\sum_{i=0}^{m}\\operatorname*{min}\\left\\{\\mathbf{x}_{i}\\mathbf{W}_{j i}\\ \\big|\\ \\underline{{\\dot{\\mathbf{x}}}}_{i}\\leq\\mathbf{x}_{i}\\leq\\dddot{\\mathbf{x}}_{i}\\right\\}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then we prove the above formula by proving for each $i$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\underline{{\\mathbf{V}}}_{j i}\\leq\\operatorname*{min}\\left\\{\\mathbf{x}_{i}\\mathbf{W}_{j i}\\;\\big|\\;\\dot{\\underline{{\\mathbf{x}}}}_{i}\\leq\\mathbf{x}_{i}\\leq\\ddot{\\mathbf{x}}_{i}\\right\\}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For the left-hand-side, we have VVVji d=ef  xxxxiWWWWji WWWji \u22650. For the right-hand-side, because $\\mathbf{x}_{i}\\mathbf{W}_{j i}$ changes monotonically with respect to $\\mathbf{x}_{i}$ , and $\\mathbf{W}_{j i}$ is a given constant, we have $\\begin{array}{r l r}{\\operatorname*{min}\\left\\{\\mathbf{x}_{i}\\mathbf{W}_{j i}\\mid\\dot{\\mathbf{\\underline{{x}}}}_{i}\\leq\\mathbf{x}_{i}\\leq\\ddot{\\mathbf{x}}_{i}\\right\\}}&{=}&{\\left\\{\\underline{{\\dot{\\mathbf{\\underline{{x}}}}}}\\mathbf{W}_{j i}\\quad\\mathbf{W}_{j i}\\geq0\\right.}\\\\ &{}&{\\left.\\left.\\left.\\mathrm{\\underline{{\\ddot{x}}}}\\mathbf{W}_{j i}\\quad\\mathrm{otherwise}\\right.\\right.}\\end{array}$ which is the same as the definition of the left-hand-side $\\mathbf{\\underline{{V}}}_{j i}$ . Thus, we have proved that $\\underline{{\\varphi}}_{\\mathrm{Aff}}(\\underline{{\\dot{\\mathbf{y}}}},\\underline{{\\dot{\\mathbf{x}}}},\\underline{{\\ddot{\\mathbf{x}}}},\\mathbf{W},\\dot{\\mathbf{b}})$ implies $\\forall\\mathbf{x}\\in$ [xxx,xxx]. $\\underline{{\\dot{\\mathbf{y}}}}\\leq\\tt A f f i n e(\\mathbf{x};\\mathbf{W},\\dot{\\mathbf{b}})$ . \u53e3 ", "page_idx": 21}, {"type": "text", "text": "Lemma A.9. $\\begin{array}{r l r l}{(\\underline{{\\dot{\\bf y}}},\\underline{{\\ddot{\\bf y}}},\\underline{{\\dot{\\bf x}}},\\underline{{\\ddot{\\bf x}}},\\underline{{\\bf W}},\\underline{{\\dot{\\bf b}}})}&{{}\\stackrel{\\scriptscriptstyle\\mathrm{def}}{=}}&{{}\\;\\overline{{\\varphi}}_{\\mathtt{A f f}}(\\ddot{\\underline{{\\mathbf{y}}}},\\underline{{\\dot{\\bf x}}},\\ddot{\\underline{{\\mathbb{x}}}},\\underline{{\\mathbb{W}}},\\underline{{\\dot{\\mathbf{b}}}})\\quad\\wedge\\quad\\underline{{\\varphi}}_{\\mathtt{A f f}}(\\underline{{\\dot{\\bf y}}},\\underline{{\\dot{\\bf x}}},\\ddot{\\underline{{\\mathbb{x}}}},\\underline{{\\mathbb{w}}},\\underline{{\\dot{\\mathbf{b}}}})}\\end{array}$ defined in Definition 3.13 is implied by $\\ddagger=\\operatorname*{max}\\left\\{\\mathtt{A f f}\\,\\mathtt{i n e}(\\mathbf{x};\\mathbf{W},\\dot{\\mathbf{b}})\\ |\\ \\dot{\\underline{{\\mathbf{x}}}}\\leq\\mathbf{x}\\leq\\ddagger\\right\\}$ $\\wedge$ $\\dot{\\underline{{\\mathbf{y}}}}=\\operatorname*{min}\\left\\{\\operatorname{Aff}\\dot{\\mathbf{\\pi}}\\mathrm{ine}\\big(\\mathbf{x};\\mathbf{W},\\dot{\\mathbf{b}}\\big)~\\big|~\\dot{\\underline{{\\mathbf{x}}}}\\le\\mathbf{x}\\le\\ddot{\\mathbf{x}}\\right\\}$ , hence captures the exact upper and lower bounds. ", "page_idx": 21}, {"type": "text", "text": "Proof. For the upper bound, recall that by Definition 3.13, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\overline{{\\varphi}}_{\\mathrm{Aff}}\\left(\\ddot{\\mathbf{y}},\\dot{\\underline{{\\mathbf{x}}}},\\ddot{\\mathbf{x}},\\mathbf{W},\\dot{\\mathbf{b}}\\right)\\stackrel{\\mathrm{def}}{=}\\bigwedge_{j}\\ddot{\\mathbf{y}}_{j}=\\sum_{i}\\overline{{\\mathbf{V}}}_{j i}+\\dot{\\mathbf{b}}_{j}\\,\\mathrm{where}\\left\\{\\begin{array}{l l}{\\overline{{\\mathbf{V}}}_{j i}\\stackrel{\\mathrm{def}}{=}\\ddot{\\mathbf{x}}_{i}\\mathbf{W}_{j i}\\,\\,\\,\\mathrm{if}\\,\\,\\,\\mathbf{W}_{j i}\\geq0}\\\\ {\\overline{{\\mathbf{V}}}_{j i}\\stackrel{\\mathrm{def}}{=}\\dot{\\underline{{\\mathbf{x}}}}_{i}\\mathbf{W}_{j i}\\,\\,\\,\\mathrm{otherwise}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For any solution $(\\overline{{\\mathbf{y}}}^{*},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\mathbf{b})$ to $\\ddagger=\\operatorname*{max}\\left\\{\\mathtt{A f f}\\,\\mathtt{i n e}(\\mathbf{x};\\mathbf{W},\\dot{\\mathbf{b}})\\ \\big|\\ \\dot{\\mathbf{z}}\\leq\\mathbf{x}\\leq\\ddagger\\right\\}$ , we have the exact constant output upper bound defined as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\overline{{\\mathbf{y}}}^{*}\\stackrel{\\mathrm{def}}{=}\\operatorname*{max}\\left\\{\\mathbf{Aff}\\,\\mathbf{ine}\\big(\\mathbf{x};\\mathbf{W},\\mathbf{b}\\big)\\ \\big|\\ \\underline{{\\mathbf{x}}}\\leq\\mathbf{x}\\leq\\overline{{\\mathbf{x}}}\\right\\}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By the definition of Affine layer (Definition 3.10), it is equivalent to ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\overline{{\\mathbf{y}}}_{j}^{*}\\stackrel{\\mathrm{def}}{=}\\sum_{i}\\operatorname*{max}\\left\\{\\mathbf{x}_{i}\\mathbf{W}_{j i}\\ \\middle|\\ \\mathbf{x}\\leq\\mathbf{x}\\leq\\overline{{\\mathbf{x}}}\\right\\}+\\mathbf{b}_{j}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "To prove that $\\overline{{\\varphi}}_{\\tt A f f}$ captures the exact upper bound, we will show that $(\\overline{{\\mathbf{y}}}^{*},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\mathbf{b})$ is a solution to $\\overline{{\\varphi}}_{\\mathrm{Aff}}\\left(\\ddot{\\mathbf{y}},\\dot{\\underline{{\\mathbf{x}}}},\\ddot{\\mathbf{x}},\\mathbf{W},\\dot{\\mathbf{b}}\\right)$ . In other words, the following instantiated formula $\\boldsymbol{\\overline{{\\varphi}}}_{\\mathrm{Aff}}\\left(\\mathbf{\\overline{{y}}}^{*},\\underline{{\\mathbf{x}}},\\mathbf{\\overline{{x}}},\\mathbf{W},\\mathbf{b}\\right)$ is true: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\overline{{\\varphi}}_{\\mathsf{A f f}}\\left(\\overline{{\\mathbf{y}}}^{*},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\mathbf{W},\\mathbf{b}\\right)\\overset{\\mathrm{def}}{=}\\bigwedge_{j}\\overline{{\\mathbf{y}}}_{j}^{*}=\\sum_{i}\\overline{{\\mathbf{V}}}_{j i}+\\mathbf{b}_{j}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\overline{{\\mathbf{V}}}_{j i}\\ \\overset{\\mathrm{def}}{=}\\overline{{\\mathbf{x}}}_{j}\\mathbf{W}_{j i}$ if $\\mathbf{{w}}_{j i}\\ge0$ , otherwise $\\overline{{\\mathbf{V}}}_{j i}\\overset{\\mathrm{def}}{=}\\underline{{\\mathbf{x}}}_{j}\\mathbf{W}_{j i}$ . ", "page_idx": 22}, {"type": "text", "text": "This formula is equivalent to prove the following formula for any index $j$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\overline{{\\mathbf{y}}}_{j}^{*}=\\sum_{i}\\overline{{\\mathbf{V}}}_{j i}+{\\mathbf{b}}_{j}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By substituting $\\overline{{\\mathbf{y}}}_{j}^{*}$ defined in Equation 12 above, our goal becomes ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{i}\\operatorname*{max}\\left\\{\\mathbf{x}_{i}\\mathbf{W}_{j i}\\mid\\underline{{\\mathbf{x}}}\\leq\\mathbf{x}\\leq\\overline{{\\mathbf{x}}}\\right\\}+\\mathbf{b}_{j}=\\sum_{i}\\overline{{\\mathbf{V}}}_{j i}+\\mathbf{b}_{j}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which is implied by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left\\{\\mathbf{x}_{i}\\mathbf{W}_{j i}\\;\\middle|\\;\\underline{{\\mathbf{x}}}\\leq\\mathbf{x}\\leq\\overline{{\\mathbf{x}}}\\right\\}=\\overline{{\\mathbf{V}}}_{j i}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "If $\\mathbf{\\Delta}\\mathbf{W}_{j i}\\ge0$ , max $\\left\\{\\mathbf{x}_{i}\\mathbf{W}_{j i}\\;\\middle|\\;\\mathbf{x}\\leq\\mathbf{x}\\leq\\overline{{\\mathbf{x}}}\\right\\}=\\overline{{\\mathbf{x}}}_{j}\\mathbf{W}_{j i}=\\overline{{\\mathbf{V}}}_{j i}$ , the above formula is true; Otherwise, max $\\left\\{\\mathbf{x}_{i}\\mathbf{W}_{j i}\\;\\middle|\\;\\mathbf{x}\\leq\\mathbf{x}\\leq\\overline{{\\mathbf{x}}}\\right\\}=\\underline{{\\mathbf{x}}}_{j}\\mathbf{W}_{j i}=\\overline{{\\mathbf{V}}}_{j i}$ , the above formula is true. Hence, we have proved that $\\boldsymbol{\\overline{{\\varphi}}}_{\\mathrm{Aff}}\\left(\\mathbf{\\overline{{y}}}^{*},\\underline{{\\mathbf{x}}},\\mathbf{\\overline{{x}}},\\mathbf{W},\\mathbf{b}\\right)$ is true and $\\overline{{\\varphi}}_{\\mathrm{Aff}}(\\ddot{\\mathbf{y}},\\dot{\\underline{{\\mathbf{x}}}},\\ddot{\\mathbf{x}},\\mathbf{W},\\dot{\\mathbf{b}})$ captures the exact upper bound. ", "page_idx": 22}, {"type": "text", "text": "For the lower bound, recall that by Definition 3.13, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\underline{{\\varphi}}_{\\mathrm{Aff}}\\left(\\underline{{\\dot{\\bf y}}},\\underline{{\\dot{\\bf x}}},\\overline{{\\ddot{\\bf x}}},\\mathbf{W},\\dot{\\mathbf{b}}\\right)\\stackrel{\\mathrm{def}}{=}\\bigwedge_{j}\\underline{{\\dot{\\bf y}}}_{j}=\\sum_{i}\\mathbf{\\underline{{V}}}_{j i}+\\dot{\\bf b}_{j}\\;\\mathrm{where}\\left\\{\\begin{array}{l l}{\\underline{{\\nabla_{j i}}}\\stackrel{\\mathrm{def}}{=}\\underline{{\\dot{\\bf x}}}_{j}\\mathbf{W}_{j i}\\;\\;\\mathrm{if}\\;\\;\\mathbf{W}_{j i}\\ge0}\\\\ {\\underline{{\\nabla_{j i}}}\\stackrel{\\mathrm{def}}{=}\\ddot{\\bf x}_{j}\\mathbf{W}_{j i}\\;\\;\\mathrm{otherwise}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For any solution $(\\underline{{\\mathbf{y}}}^{*},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\mathbf{b})$ to $\\underline{{\\dot{\\mathbf{y}}}}=\\operatorname*{min}\\left\\{\\mathbf{Aff\\,ine}(\\mathbf{x};\\mathbf{W},\\dot{\\mathbf{b}})\\ |\\ \\underline{{\\dot{\\mathbf{x}}}}\\leq\\mathbf{x}\\leq\\dddot{\\mathbf{x}}\\right\\}$ , we have the exact constant output lower bound defined as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\underline{{\\mathbf{y}}}^{*}\\stackrel{\\mathrm{def}}{=}\\operatorname*{min}\\left\\{\\mathbf{Aff}\\,\\mathbf{ine}\\big(\\mathbf{x};\\mathbf{W},\\mathbf{b}\\big)\\ \\big|\\ \\underline{{\\mathbf{x}}}\\leq\\mathbf{x}\\leq\\overline{{\\mathbf{x}}}\\right\\}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By the definition of Affine layer (Definition 3.10), it is equivalent to ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\underline{{\\mathbf{y}}}_{j}^{*}\\stackrel{\\mathrm{def}}{=}\\sum_{i}\\operatorname*{min}\\left\\{\\mathbf{x}_{i}\\mathbf{W}_{j i}\\mid\\underline{{\\mathbf{x}}}\\leq\\mathbf{x}\\leq\\overline{{\\mathbf{x}}}\\right\\}+\\mathbf{b}_{j}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "To prove that $\\underline{{\\varphi}}_{\\mathtt{A f f}}$ captures the exact lower bound, we will show that $(\\underline{{\\mathbf{y}}}^{*},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\mathbf{b})$ is a solution to $\\underline{{\\varphi}}_{\\mathrm{Aff}}(\\underline{{\\dot{\\mathbf{y}}}},\\underline{{\\dot{\\mathbf{x}}}},\\underline{{\\ddot{\\mathbf{x}}}},\\mathbf{W},\\dot{\\mathbf{b}})$ . In other words, the following instantiated formula $\\underline{{\\varphi}}_{\\mathrm{Aff}}\\left(\\underline{{\\mathbf{y}}}^{*},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\mathbf{W},\\mathbf{b}\\right)$ is true: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\underline{{\\varphi}}_{\\mathsf{A f f}}\\left(\\underline{{\\mathbf{y}}}^{*},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\mathbf{W},\\mathbf{b}\\right)\\overset{\\mathrm{def}}{=}\\bigwedge_{j}\\underline{{\\mathbf{y}}}_{j}^{*}=\\sum_{i}\\underline{{\\mathbf{V}}}_{j i}+\\mathbf{b}_{j}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\underline{{\\mathbf{V}}}_{j i}\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\underline{{\\mathbf{x}}}_{j}\\mathbf{W}_{j i}$ if $\\mathbf{{w}}_{j i}\\ge0$ , otherwise $\\underline{{\\mathbf{V}}}_{j i}\\ {\\stackrel{\\mathrm{def}}{=}}\\ {\\overline{{\\mathbf{x}}}}_{j}\\mathbf{W}_{j i}$ . ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "This formula is equivalent to prove the following formula for any index $j$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\underline{{\\mathbf{y}}}_{j}^{*}=\\sum_{i}\\underline{{\\mathbf{V}}}_{j i}+\\mathbf{b}_{j}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By substituting $\\underline{{\\mathbf{y}}}_{j}^{*}$ defined in Equation 13 above, our goal becomes ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{i}\\operatorname*{min}\\left\\{\\mathbf{x}_{i}\\mathbf{W}_{j i}\\;\\middle|\\;\\mathbf{x}\\leq\\mathbf{x}\\leq\\overline{{\\mathbf{x}}}\\right\\}+\\mathbf{b}_{j}=\\sum_{i}\\mathbf{\\underline{{V}}}_{j i}+\\mathbf{b}_{j}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which is implied by ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\left\\{\\mathbf{x}_{i}\\mathbf{W}_{j i}\\;\\middle|\\;\\underline{{\\mathbf{x}}}\\leq\\mathbf{x}\\leq\\overline{{\\mathbf{x}}}\\right\\}=\\underline{{\\mathbf{V}}}_{j i}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "If $\\bar{\\mathbf{W}}_{j i}\\,\\geq\\,0$ , $\\operatorname*{min}\\left\\{\\mathbf{x}_{i}\\mathbf{W}_{j i}\\;\\middle|\\;\\underline{{\\mathbf{x}}}\\leq\\mathbf{x}\\leq\\overline{{\\mathbf{x}}}\\right\\}=\\underline{{\\mathbf{x}}}_{j}\\mathbf{W}_{j i}=\\underline{{\\mathbf{V}}}_{j i}$ , the above formula is true; Otherwise, m $\\ln\\left\\{\\mathbf{x}_{i}\\mathbf{W}_{j i}\\mid\\underline{{\\mathbf{x}}}\\leq\\mathbf{x}\\leq\\overline{{\\mathbf{x}}}\\right\\}=\\overline{{\\mathbf{x}}}_{j}\\mathbf{W}_{j i}=\\underline{{\\mathbf{V}}}_{j i}$ , the above formula is true. Hence, we have proved that $\\underline{{\\varphi}}_{\\mathrm{Aff}}\\left(\\underline{{\\mathbf{y}}}^{*},\\underline{{\\mathbf{x}}},\\overline{{\\mathbf{x}}},\\mathbf{W},\\mathbf{b}\\right)$ is true and $\\underline{{\\varphi}}_{\\mathrm{Aff}}(\\underline{{\\dot{\\mathbf{y}}}},\\underline{{\\dot{\\mathbf{x}}}},\\underline{{\\ddot{\\mathbf{x}}}},\\mathbf{W},\\dot{\\mathbf{b}})$ captures the exact lower bound. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Theorem 3.14. Definition 3.13 is a Parametric Linear Relaxation for the Affine layer with variable input bounds [xxx,xxx] that captures the exact upper and lower bounds. ", "page_idx": 23}, {"type": "text", "text": "Proof. By Lemma A.7 and Lemma A.8, we proved that $\\underline{{\\boldsymbol{\\overline{{\\varphi}}}}}_{\\mathrm{Aff}}\\left(\\underline{{\\boldsymbol{\\dot{y}}}},\\underline{{\\boldsymbol{\\ddot{y}}}},\\underline{{\\boldsymbol{\\dot{x}}}},\\underline{{\\boldsymbol{\\ddot{x}}}},\\mathbf{W},\\underline{{\\boldsymbol{\\dot{b}}}}\\right)$ is a poly-size linear formula that implies $\\forall\\mathbf{x}\\in$ [xxx,xxx]. Affine $\\mathbf{\\Psi}_{:}(\\mathbf{x};\\mathbf{W},\\dot{\\mathbf{b}})\\,\\in\\,[\\dot{\\mathbf{y}}$ yyy,yyy], hence is a Parametric Linear Relaxation (Definition 3.3) for the Affine layer. By Lemma A.9, we proved that $\\underline{{\\boldsymbol{\\overline{{\\varphi}}}}}_{\\mathrm{Aff}}\\left(\\underline{{\\boldsymbol{\\dot{y}}}},\\ddot{\\mathbf{y}},\\dot{\\underline{{\\mathbf{x}}}},\\ddot{\\mathbf{x}},\\mathbf{W},\\dot{\\mathbf{b}}\\right)$ is implied b $\\!\\!\\langle\\,{\\ddag}\\,\\rangle\\!=\\operatorname*{max}\\big\\{\\!\\operatorname{Aff}\\,\\mathrm{ine}\\big(\\mathbf{x};\\!\\mathbf{W},{\\dot{\\mathbf{b}}}\\big)\\,\\big|\\,{\\dot{\\underline{{\\mathbf{x}}}}}\\leq\\mathbf{x}\\leq{\\ddot{\\mathbf{x}}}\\big\\}\\wedge\\!{\\dot{\\underline{{\\mathbf{y}}}}}=\\operatorname*{min}\\big\\{\\operatorname{Aff}\\,\\mathrm{ine}\\big(\\mathbf{x};\\!\\mathbf{W},{\\dot{\\mathbf{b}}}\\big)\\,\\big|\\,{\\dot{\\mathbf{z}}}\\leq\\mathbf{x}\\leq{\\ddot{\\mathbf{x}}}\\big\\},$ , hence captures the exact upper and lower bounds. ", "page_idx": 23}, {"type": "text", "text": "B General provable editing via Parametric Linear Relaxation ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section we present our approach for solving the general provable editing problem (Definition 1.1): ", "page_idx": 23}, {"type": "text", "text": "Definition B.1. Given Parametric Linear Relaxation $\\underline{{\\overline{{\\varphi}}}}_{N}(\\underline{{\\dot{\\bf y}}},\\underline{{\\ddot{\\bf y}}},\\underline{{{\\bf x}}},\\overline{{{\\bf x}}};\\dot{\\pmb{\\theta}})$ for DNN $\\mathcal{N}$ with parameters $\\theta$ , an input polytope $\\mathrm{P}\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\left\\{\\mathbf{x}\\ |\\ \\mathbf{D}\\mathbf{x}\\leq\\mathbf{e}\\right\\}$ , an output polytope ${\\textsc{Q}}{\\stackrel{\\mathrm{def}}{=}}\\left\\{\\mathbf{y}\\mid\\mathbf{A}\\mathbf{y}\\leq\\mathbf{b}\\right\\}$ . Given a hyperparameter $k$ so that we only edit the DNN layers $\\scriptstyle{\\mathcal{N}}^{(k:L)}$ starting from the $k$ th layer. Then a solution to the following linear program is a solution to the provable editing problem of Definition 1.1: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\|\\dot{\\theta}-\\theta\\|\\quad\\mathrm{s.t.}\\quad\\overline{{\\underline{{\\varphi}}}}_{N^{(k;L)}}\\big(\\underline{{\\dot{\\bf y}}},\\ddot{\\bf y},\\underline{{\\mathbf x}}^{(k)},\\overline{{\\mathbf x}}^{(k)};\\dot{\\theta}^{(k;L)}\\big)\\wedge\\psi(\\underline{{\\dot{\\bf y}}},\\ddot{\\bf y},\\mathbf A,\\mathbf b)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where a sound constant interval bound $\\big[\\underline{{\\mathbf{x}}}^{(k)},\\overline{{\\mathbf{x}}}^{(k)}\\big]$ for the input $\\mathbf{x}^{(k)}$ to the slice $\\mathcal{N}^{(k:L)}$ to edit is computed by a sound bound propagation tool of DNNs like DeepPoly [39], auto_LiRPA [50] and DeepT [4]: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left[\\underline{{\\mathbf{x}}}^{(k)},\\overline{{\\mathbf{x}}}^{(k)}\\right]\\stackrel{\\mathrm{def}}{=}\\mathsf{C o m p u t e\\_B o u n d}_{\\mathcal{N}^{(0:k)}}\\big(\\mathrm{P};\\boldsymbol{\\theta}^{(0:k)}\\big)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The output constraint $\\psi(\\underline{{\\dot{\\bf y}}},\\underline{{\\ddot{\\bf y}}},{\\bf A},{\\bf b})$ encodes $\\mathbf{A}\\mathbf{y}\\leq\\mathbf{b}$ for all $\\mathbf{y}\\in\\left[\\dot{\\mathbf{y}},\\ddot{\\mathbf{y}}\\right]$ using the parametric output bounds yyy,yyy is defined as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\psi(\\underline{{\\underline{{\\psi}}}},\\underline{{\\underline{{\\psi}}}},\\mathbf{A},\\mathbf{b})\\overset{\\mathrm{def}}{=}\\bigwedge_{i=0}^{m}\\sum_{j=0}^{n}\\mathbf{E}_{i j}\\leq\\mathbf{b}_{i}\\quad\\mathrm{where}\\quad\\mathbf{E}_{i j}\\overset{\\mathrm{def}}{=}\\left\\{\\mathbf{A}_{i j}\\overset{\\mathrm{di}}{\\underline{{\\breve{\\nabla}}}}_{j}\\quad\\mathbf{A}_{i j}\\geq0\\right.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "B.1 Provable editing for multiple properties via Parametric Linear Relaxation ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Given $n$ properties defined by input polytopes $\\mathrm{P}_{i}\\stackrel{\\mathrm{def}}{=}\\left\\{\\mathbf{x}\\mid\\mathbf{\\omega}_{i}\\mathbf{D}\\mathbf{x}\\leq\\mathbf{\\omega}_{i}\\mathbf{e}\\right\\}$ , and output polytopes $\\mathrm{Q}_{i}\\stackrel{\\mathrm{def}}{=}\\left\\{\\mathbf{y}\\mid{}_{i}\\mathbf{A}\\mathbf{y}\\leq{}_{i}\\mathbf{b}\\right\\}$ . This work can directly generalize to provably edit multiple properties: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\|\\dot{{\\boldsymbol\\theta}}-{\\boldsymbol\\theta}\\|\\quad\\mathrm{s.t.~}\\quad\\bigwedge_{0\\leq i<n}\\forall{\\mathbf{x}}\\in\\mathrm{P}_{i}.\\,\\mathcal{N}({\\mathbf{x}};\\dot{{\\boldsymbol\\theta}})\\in\\mathrm{Q}_{i}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "In particular, most of our experiments in Section 4 involve editing DNNs for multiple properties altogether. Here we present our formulation for provably editing multiple properties, which encodes the conjunction of the constraints for each property. ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\|\\dot{\\theta}-\\theta\\|\\quad\\mathrm{~s.t.~}\\quad\\bigwedge_{0\\leq i<n}\\overline{{\\mathcal{L}}}_{N^{(k;L)}}\\big(_{i}\\dot{\\underline{{\\mathbf{y}}}},\\,i\\ddot{\\mathbf{y}},\\,i\\underline{{\\mathbf{x}}}^{(k)},\\,i\\overline{{\\mathbf{x}}}^{(k)};\\dot{\\theta}^{(k;L)}\\big)\\wedge\\psi\\big(_{i}\\dot{\\underline{{\\mathbf{y}}}},\\,i\\ddot{\\mathbf{y}},\\,i\\mathbf{A},\\,i\\mathbf{b}\\big)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\begin{array}{r}{\\left[{_{i}\\underline{{\\mathbf{x}}}^{(k)}},{_{i}\\overline{{\\mathbf{x}}}^{(k)}}\\right]\\stackrel{\\mathrm{def}}{=}\\mathrm{Compute\\_Bound}_{\\mathcal{N}^{(0:k)}}\\!\\left(\\mathrm{P}_{i};{\\theta^{(0:k)}}\\right)\\!.}\\end{array}$ ", "page_idx": 24}, {"type": "text", "text": "B.2 Provable editing for disjunctive properties via Parametric Linear Relaxation ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Given a property defined by an input polytope $\\mathrm{P}\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\left\\{\\mathbf{x}\\mid\\mathbf{Dx}\\leq\\mathbf{e}\\right\\}$ , and $n$ possible output polytopes $\\mathrm{Q}_{j}\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\left\\{\\mathbf{y}\\mid{}_{j}\\mathbf{A}\\mathbf{y}\\leq{}_{j}\\mathbf{b}\\right\\}$ , this work can directly generalize to provably edit disjunctive property: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\|\\dot{{\\boldsymbol\\theta}}-{\\boldsymbol\\theta}\\|\\quad\\mathrm{s.t.}\\quad\\forall\\mathbf{x}\\in\\mathrm{P}_{j}.\\quad\\bigvee_{0\\leq j<n}\\mathcal{N}(\\mathbf{x};\\dot{{\\boldsymbol\\theta}})\\in\\mathrm{Q}_{j}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "by using a mixed-integer linear programming (MILP) solver. In particular, some properties in VNN-COMP (Section 4.1) involve editing DNNs for such disjunctive properties. Here we present our formulation: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\|\\dot{\\theta}-\\theta\\|\\quad\\mathrm{s.t.}\\quad\\overline{{\\underline{{\\varphi}}}}_{N^{(k:L)}}\\big(\\underline{{\\dot{\\bf y}}},\\underline{{\\ddot{\\bf y}}},\\underline{{\\underline{{\\bf x}}}}^{(k)},\\underline{{\\bar{\\bf x}}}^{(k)};\\dot{\\theta}^{(k:L)}\\big)\\wedge\\bigvee_{0\\leq j<n}\\psi(\\underline{{\\dot{\\bf y}}},\\underline{{\\ddot{\\bf y}}},j\\mathbf{A},j\\mathbf{b})\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\begin{array}{r}{\\big[\\underline{{\\mathbf{x}}}^{(k)},\\overline{{\\mathbf{x}}}^{(k)}\\big]\\overset{\\mathrm{def}}{=}\\mathtt{C o m p u t e\\_B o u n d}_{\\mathcal{N}^{(0:k)}}\\big(\\mathrm{P};\\theta^{(0:k)}\\big).}\\end{array}$ ", "page_idx": 24}, {"type": "text", "text": "C Parametric Linear Relaxation for Tanh, Sigmoid and ELU layers ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "C.1 Parametric Linear Relaxation for Tanh layers ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Definition C.1. ${\\mathbf{y}}\\ {\\stackrel{\\mathrm{def}}{=}}\\ {\\mathrm{Tanh}}({\\mathbf{x}})$ with input $\\mathbf{x}\\in\\mathbb{R}^{m}$ and output $\\mathbf{y}\\in\\mathbb{R}^{m}$ is defined as $\\mathbf{y}_{i}\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\operatorname{tanh}(\\mathbf{x}_{i})$ . ", "page_idx": 24}, {"type": "text", "text": "Definition C.2. For Tanh layer with variable input bounds [xxx,xxx], its Parametric Linear Relaxation is defined as $\\underline{{\\varphi}}_{\\mathrm{Tanh}}(\\underline{{\\dot{\\mathbf{y}}}},\\underline{{\\ddot{\\mathbf{y}}}},\\underline{{\\dot{\\mathbf{x}}}},\\ddot{\\mathbf{x}})\\stackrel{\\mathrm{def}}{=}\\overline{{\\varphi}}_{\\mathrm{Tanh}}(\\underline{{\\ddot{\\mathbf{y}}}},\\ddot{\\mathbf{x}})\\wedge\\underline{{\\varphi}}_{\\mathrm{Tanh}}(\\underline{{\\dot{\\mathbf{y}}}},\\dot{\\underline{{\\mathbf{x}}}})$ where: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{\\varphi}}_{\\mathrm{Tanh}}(\\mathbf{\\ddot{y}},\\mathbf{\\ddot{x}})\\;\\stackrel{\\mathrm{def}}{=}\\;\\mathbf{\\ddot{y}}\\geq\\mathrm{Tanh}(\\mathbf{x}^{u})\\,\\wedge\\,\\mathbf{\\ddot{y}}\\geq\\mathbf{a}^{u}\\mathbf{\\ddot{x}}+\\mathbf{b}^{u}}\\\\ &{\\underline{{\\varphi}}_{\\mathrm{Tanh}}(\\mathbf{\\dot{y}},\\mathbf{\\dot{\\underline{{x}}}})\\;\\stackrel{\\mathrm{def}}{=}\\;\\mathbf{\\dot{y}}\\leq\\mathrm{Tanh}(\\mathbf{x}^{l})\\,\\wedge\\,\\mathbf{\\dot{\\underline{{y}}}}\\leq\\mathbf{a}^{l}\\mathbf{\\dot{\\underline{{x}}}}+\\mathbf{b}^{l}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\mathbf{x}^{u}\\in\\mathbb{R}^{m},\\mathbf{a}^{u}\\in\\mathbb{R}^{m},\\mathbf{b}^{u}\\in\\mathbb{R}^{m}$ are constants that satisfy the following two conditions: i) let $f(\\pmb{x})=\\mathbf{a}_{i}^{u}\\pmb{x}+\\mathbf{b}_{i}^{u}$ , the line defined by function $f({\\pmb x})$ is tangent to the concave piece of tanh at some $\\pmb{x}\\ge0$ ; ii) $\\mathbf{a}^{u}\\mathbf{x}^{u}+\\mathbf{b}^{u}=\\operatorname{Tanh}(\\mathbf{x}^{u})$ ; viz., the line defined by function $f(\\pmb{x})$ intersects with convex piece of tanh at $\\mathbf{x}_{i}^{u}$ where $\\mathbf{x}_{i}^{u}\\le0$ . ", "page_idx": 24}, {"type": "text", "text": "Similarly, $\\mathbf{x}^{l}\\in\\mathbb{R}^{m}$ , $\\mathbf{a}^{l}\\in\\mathbb{R}^{m}$ , $\\mathbf{b}^{l}\\in\\mathbb{R}^{m}$ are constants that satisfy the following two conditions: i) let $f(\\pmb{x})=\\mathbf{\\dot{a}}_{i}^{l}\\pmb{x}+\\mathbf{b}_{i}^{l}$ , the line defined by function $f({\\pmb x})$ is tangent to the convex piece of tanh at some $\\pmb{x}\\leq0$ ; ii) $\\mathbf{a}^{l}\\mathbf{x}^{l}+\\mathbf{\\dot{b}}^{l}=\\operatorname{Tanh}(\\mathbf{x}^{l})$ ; viz., the line defined by function $f({\\pmb x})$ intersects with concave piece of tanh at $\\mathbf{x}_{i}^{l}$ where $\\mathbf{x}_{i}^{l}\\ge0$ . ", "page_idx": 24}, {"type": "text", "text": "Theorem C.3. Definition $C.2$ is a Parametric Linear Relaxation for the Tanh layer. ", "page_idx": 24}, {"type": "text", "text": "Proof. As seen in Definition C.2, $\\underline{{\\overline{{\\varphi}}}}_{\\mathtt{T a n h}}(\\underline{{\\dot{\\mathbf{y}}}},\\underline{{\\ddot{\\mathbf{y}}}},\\underline{{\\dot{\\mathbf{x}}}},\\underline{{\\ddot{\\mathbf{x}}}})$ is a linear formula. Let $m$ be the number of input dimensions of Tanh. $\\underline{{\\overline{{\\varphi}}}}_{\\mathtt{T a n h}}(\\underline{{\\dot{\\mathbf{y}}}},\\ddot{\\mathbf{y}},\\underline{{\\dot{\\mathbf{x}}}},\\ddot{\\mathbf{x}})$ has $4m$ linear constraints over $4m$ variables $(\\dot{\\underline{{\\mathbf{x}}}}\\in\\mathbb{R}^{m}$ , $\\breve{\\mathbf{x}}\\in\\mathbb{R}^{m},\\dot{\\underline{{\\mathbf{y}}}}\\in\\mathbb{R}^{m}$ annudm $\\mathbf{\\ddot{y}}\\in\\mathbb{R}^{m}.$ ).p uHt ednicme,e $\\underline{{\\overline{{\\varphi}}}}_{\\mathtt{T a n h}}(\\underline{{\\dot{\\mathtt{y}}}},\\underline{{\\ddot{\\mathtt{y}}}},\\underline{{\\dot{\\mathtt{x}}}},\\underline{{\\ddot{\\mathtt{x}}}})$ n ihs  laa ypeorl.y-size linear formula, whose size is ", "page_idx": 25}, {"type": "text", "text": "Now our goal is to prove that $\\underline{{\\overline{{\\varphi}}}}_{\\mathtt{T a n h}}(\\underline{{\\dot{\\mathtt{y}}}},\\underline{{\\ddot{\\mathtt{y}}}},\\underline{{\\dot{\\mathtt{x}}}},\\underline{{\\ddot{\\mathtt{x}}}})$ defined in Definition C.2 implies ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\forall\\mathbf{x}\\in[\\dot{\\underline{{\\mathbf{x}}}},\\ddot{\\mathbf{x}}].\\,\\dot{\\underline{{\\mathbf{y}}}}\\leq\\operatorname{Tanh}(\\mathbf{x})\\leq\\ddot{\\mathbf{y}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We will discuss the upper and lower bounds separately. ", "page_idx": 25}, {"type": "text", "text": "For the upper bound constraint $\\overline{{\\varphi}}_{\\mathrm{Tanh}}(\\mathbf{\\ddot{y}},\\mathbf{\\ddot{x}})\\overset{\\mathrm{def}}{=}\\mathbf{\\ddot{y}}\\,\\geq\\mathbf{\\mathrm{Tanh}}(\\mathbf{x}^{u})\\,\\wedge\\,\\mathbf{\\ddot{y}}\\geq\\mathbf{a}^{u}\\mathbf{\\ddot{x}}+\\mathbf{b}^{u}$ , for the case when $\\dot{\\mathbf{x}}_{i}\\leq\\mathbf{x}_{i}^{u}$ , we have $\\pmb{\\ddot{\\mathbf{y}}}_{i}\\geq\\operatorname{tanh}(\\mathbf{x}_{i}^{u})\\geq\\operatorname{tanh}(\\pmb{\\dot{\\mathbf{x}}}_{i})$ because tanh is monotonically increasing. For the case when $\\dot{\\mathbf{x}}_{i}>\\mathbf{x}_{i}^{u}$ , we have $\\ddot{\\mathbf{y}}_{i}\\geq\\mathbf{a}_{i}^{u}\\ddot{\\mathbf{x}}_{i}+\\mathbf{b}_{i}^{u}\\geq\\operatorname{tanh}(\\dot{\\mathbf{x}}_{i})$ because $f(\\pmb{x})=\\mathbf{a}_{i}^{u}\\pmb{x}+\\mathbf{b}_{i}^{u}$ is tangent to the concave piece of tanh in $[0,\\infty]$ , and tanh is monotonically increasing in $[\\mathbf{x}_{i}^{u},0]$ . Therefore, $\\overline{{\\varphi}}_{\\mathrm{Tanh}}(\\mathbf{\\ddot{y}},\\mathbf{\\ddot{x}})$ defined in Definition C.2 implies that $\\forall\\mathbf{x}\\in$ [xxx,xxx]. $\\operatorname{Tanh}(\\mathbf{x})\\leq\\ddot{\\mathbf{y}}$ . ", "page_idx": 25}, {"type": "text", "text": "For the lower bound constraint $\\underline{{\\varphi}}_{\\mathrm{Tanh}}(\\underline{{\\dot{\\mathbf{y}}}},\\underline{{\\dot{\\mathbf{x}}}})\\overset{\\mathrm{def}}{=}\\underline{{\\dot{\\mathbf{y}}}}\\leq\\mathrm{Tanh}(\\mathbf{x}^{l})\\,\\wedge\\,\\underline{{\\dot{\\mathbf{y}}}}\\leq\\mathbf{a}^{l}\\underline{{\\dot{\\mathbf{x}}}}+\\mathbf{b}^{l}$ , for the case when $\\dot{\\mathbf{x}}_{i}\\geq\\mathbf{x}_{i}^{l}$ , we have $\\dot{\\underline{{\\mathbf{y}}}}_{i}\\leq\\operatorname{tanh}(\\mathbf{x}_{i}^{l})\\leq\\operatorname{tanh}(\\dot{\\mathbf{x}}_{i})$ because tanh is monotonically increasing. For the case when $\\dot{\\mathbf{x}}_{i}<\\mathbf{x}_{i}^{l}$ , we have $\\underline{{\\dot{\\mathbf{y}}}}\\leq\\mathbf{a}_{i}^{l}\\dot{\\mathbf{\\underline{{x}}}}_{i}+\\mathbf{b}_{i}^{l}\\leq\\operatorname{tanh}(\\dot{\\mathbf{x}}_{i})$ because $f(\\pmb{x})=\\mathbf{a}_{i}^{l}\\pmb{x}+\\mathbf{b}_{i}^{l}$ is tangent to the convex piece of tanh in $[-\\infty,0]$ , and tanh is monotonically increasing in $[0,\\mathbf{x}_{i}^{l}]$ . Therefore, $\\underline{{\\varphi}}_{\\mathrm{Tanh}}(\\ddot{\\mathbf{y}},\\ddot{\\mathbf{x}})$ defined in Definition C.2 implies that $\\forall\\mathbf{x}\\in$ [xxx,xxx]. $\\dot{\\underline{{\\mathbf{y}}}}\\leq\\mathtt{T a n h}(\\mathbf{x})$ . ", "page_idx": 25}, {"type": "text", "text": "C.2 Parametric Linear Relaxation for Sigmoid layers ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Definition C.4. $\\mathbf{y}\\ {\\overset{\\underset{\\mathrm{def}}{}}{=}}\\ \\mathbf{S}\\mathtt{i g m o i d}(\\mathbf{x})$ with input $\\mathbf{x}\\in\\mathbb{R}^{m}$ and output $\\mathbf{y}\\in\\mathbb{R}^{m}$ is defined as $\\begin{array}{r}{\\mathbf{y}_{i}\\overset{\\mathrm{def}}{=}\\frac{1}{1+e^{-\\mathbf{x}_{i}}}}\\end{array}$ ", "page_idx": 25}, {"type": "text", "text": "Definition C.5. For Sigmoid layer with variable input bounds [xxx,xxx], its Parametric Linear Relaxation is defined as $\\underline{{\\overline{{\\varphi}}}}_{\\mathrm{Sigmoid}}(\\underline{{\\dot{\\bf y}}},\\underline{{\\ddot{\\bf y}}},\\underline{{\\dot{\\bf x}}},\\underline{{\\ddot{\\bf x}}})\\stackrel{\\mathrm{def}}{=}\\overline{{\\varphi}}_{\\mathrm{Sigmoid}}(\\underline{{\\ddot{\\bf y}}},\\underline{{\\ddot{\\bf x}}})\\wedge\\underline{{\\varphi}}_{\\mathrm{Sigmoid}}(\\underline{{\\dot{\\bf y}}},\\underline{{\\dot{\\bf x}}})$ where: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{\\varphi}}_{\\mathrm{sigmoid}}(\\ddot{\\mathbf{y}},\\ddot{\\mathbf{x}})\\;\\stackrel{\\mathrm{def}}{=}\\,\\ddot{\\mathbf{y}}\\geq\\mathrm{Sigmoid}(\\mathbf{x}^{u})\\,\\wedge\\,\\ddot{\\mathbf{y}}\\geq\\mathbf{a}^{u}\\ddot{\\mathbf{x}}+\\mathbf{b}^{u}}\\\\ &{\\underline{{\\varphi}}_{\\mathrm{Sigmoid}}(\\underline{{\\dot{\\mathbf{y}}}},\\dot{\\underline{{\\mathbf{x}}}})\\;\\stackrel{\\mathrm{def}}{=}\\,\\dot{\\mathbf{y}}\\leq\\mathrm{Sigmoid}(\\mathbf{x}^{l})\\,\\wedge\\,\\dot{\\mathbf{y}}\\leq\\mathbf{a}^{l}\\dot{\\underline{{\\mathbf{x}}}}+\\mathbf{b}^{l}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\mathbf{x}^{u}\\in\\mathbb{R}^{m}$ , $\\mathbf{a}^{u}\\in\\mathbb{R}^{m}$ , $\\mathbf{b}^{u}\\in\\mathbb{R}^{m}$ are constants that satisfy the following two conditions: i) let $f(\\pmb{x})=\\mathbf{a}_{i}^{u}\\pmb{x}+\\mathbf{b}_{i}^{u}$ , the line defined by function $f({\\pmb x})$ is tangent to the concave piece of sigmoid at some $\\pmb{x}\\ge0$ ; ii) $\\mathbf{a}^{u}\\mathbf{x}^{u}+\\mathbf{b}^{u}=\\mathtt{S i g m o i d}(\\mathbf{x}^{u})$ ; viz., the line defined by function $f({\\pmb x})$ intersects with convex piece of sigmoid at $\\mathbf{x}_{i}^{u}$ where $\\mathbf{x}_{i}^{u}\\le0$ . ", "page_idx": 25}, {"type": "text", "text": "Similarly, $\\mathbf{x}^{l}\\in\\mathbb{R}^{m}$ , $\\mathbf{a}^{l}\\in\\mathbb{R}^{m}$ , $\\mathbf{b}^{l}\\in\\mathbb{R}^{m}$ are constants that satisfy the following two conditions: i) let $f(\\mathbf{x})=\\mathbf{\\dot{a}}_{i}^{l}\\mathbf{x}+\\mathbf{b}_{i}^{l}$ , the line defined by function $f({\\pmb x})$ is tangent to the convex piece of sigmoid at some $\\pmb{x}\\leq0$ ; ii) $\\mathbf{a}^{l}\\mathbf{x}^{l}+\\mathbf{b}^{l}=\\mathrm{sigmoid}(\\mathbf{x}^{l})$ ; viz., the line defined by function $f({\\pmb x})$ intersects with concave piece of sigmoid at $\\mathbf{x}_{i}^{l}$ where $\\mathbf{x}_{i}^{l}\\ge0$ . ", "page_idx": 25}, {"type": "text", "text": "Theorem C.6. Definition C.5 is a Parametric Linear Relaxation for the Sigmoid layer. ", "page_idx": 25}, {"type": "text", "text": "Proof. As seen in Definition C.5, $\\underline{{\\overline{{\\varphi}}}}_{\\mathtt{S i g m o i d}}(\\underline{{\\dot{\\bf y}}},\\ddot{\\underline{{\\bf y}}},\\dot{\\underline{{\\bf x}}},\\ddot{\\underline{{\\bf x}}})$ is a linear formula. Let $m$ be the number of input dimensions of Sigmoid. $\\underline{{\\overline{{\\varphi}}}}_{\\mathtt{S i g m o i d}}(\\underline{{\\dot{\\bf y}}},\\ddot{\\underline{{\\bf y}}},\\dot{\\underline{{\\bf x}}},\\ddot{\\underline{{\\bf x}}})$ has $4m$ linear constraints over $4m$ variables $(\\underline{{\\dot{\\bf x}}}\\in\\mathbb{R}^{m},\\ddot{\\bf x}\\in\\mathbb{R}^{m},\\underline{{\\dot{\\bf y}}}\\in\\mathbb{R}^{m}$ and $\\mathbf{\\ddot{y}}\\in\\mathbb{R}^{m}.$ ). Hence, $\\underline{{\\overline{{\\varphi}}}}_{\\mathtt{S i g m o i d}}(\\underline{{\\dot{\\bf y}}},\\ddot{\\underline{{\\bf y}}},\\dot{\\underline{{\\bf x}}},\\ddot{\\underline{{\\bf x}}})$ is a poly-size linear formula, whose size is polynomial in the number of input dimensions of the Sigmoid layer. ", "page_idx": 25}, {"type": "text", "text": "Now our goal is to prove that $\\underline{{\\overline{{\\varphi}}}}_{\\mathtt{S i g m o i d}}(\\underline{{\\dot{\\bf y}}},\\ddot{\\underline{{\\bf y}}},\\dot{\\underline{{\\bf x}}},\\ddot{\\underline{{\\bf x}}})$ defined in Definition C.5 implies ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\forall\\mathbf{x}\\in[\\underline{{\\dot{\\mathbf{x}}}},\\ddot{\\mathbf{x}}]\\mathbf{\\cdot}\\,\\underline{{\\dot{\\mathbf{y}}}}\\leq\\mathrm{sigmoid}(\\mathbf{x})\\leq\\ddot{\\mathbf{y}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We will discuss the upper and lower bounds separately. ", "page_idx": 25}, {"type": "text", "text": "For the upper bound constraint $\\overline{{\\varphi}}_{\\mathtt{S i g m o i d}}(\\ddot{\\mathtt{y}},\\ddot{\\mathtt{x}})\\overset{\\mathrm{def}}{=}\\ddot{\\mathtt{y}}\\ge\\mathtt{S i g m o i d}(\\mathbf{x}^{u})\\,\\wedge\\,\\dddot{\\mathtt{y}}\\ge\\mathbf{a}^{u}\\ddot{\\mathtt{x}}+\\mathbf{b}^{u}$ , for the case when $\\dot{\\bf x}_{i}\\le{\\bf x}_{i}^{u}$ , we have $\\begin{array}{r}{\\varPsi_{i}\\geq\\,\\mathrm{sigmoid}(\\mathbf{x}_{i}^{u})\\geq\\,\\mathrm{sigmoid}(\\dot{\\mathbf{x}}_{i})}\\end{array}$ because sigmoid is monotonically increasing. For the case when $\\dot{\\mathbf{x}}_{i}>\\mathbf{x}_{i}^{u}$ , we have $\\tilde{\\mathbf{y}}_{i}^{}\\ge\\mathbf{a}_{i}^{u}\\tilde{\\mathbf{x}}_{i}+\\mathbf{b}_{i}^{u}\\ge$ sigmoid $({\\dot{\\bf x}}_{i})$ because $f(\\pmb{x})\\,=\\,\\pmb{\\mathrm{a}}_{i}^{u}\\pmb{x}+\\pmb{\\mathrm{b}}_{i}^{u}$ is tangent to the concave piece of sigmoid in $[0,\\infty]$ , and sigmoid is monotonically increasing in $[\\mathbf{x}_{i}^{u},0]$ . Therefore, $\\overline{{\\varphi}}_{\\mathtt{S i g m o i d}}(\\mathbf{\\ddot{y}},\\mathbf{\\ddot{x}})$ defined in Definition C.5 implies that $\\forall\\mathbf{x}\\in$ [xxx,xxx]. $\\mathtt{S i g m o i d}(\\mathbf{x})\\le\\ddot{\\mathbf{y}}$ . ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "For the lower bound constraint $\\underline{{\\varphi}}_{\\mathrm{sigmoid}}(\\underline{{\\dot{\\mathbf{y}}}},\\underline{{\\dot{\\mathbf{x}}}})\\,\\stackrel{\\mathrm{def}}{=}\\,\\underline{{\\dot{\\mathbf{y}}}}\\,\\leq\\,\\mathrm{Sigmoid}(\\mathbf{x}^{l})\\,\\land\\,\\underline{{\\dot{\\mathbf{y}}}}\\,\\leq\\,\\mathbf{a}^{l}\\underline{{\\dot{\\mathbf{x}}}}+\\mathbf{b}^{l}$ , for the case when $\\dot{\\mathbf{x}}_{i}\\geq\\mathbf{x}_{i}^{l}$ , we have $\\dot{\\mathbf{y}}_{\\iota}\\leq\\mathbf{s}\\mathtt{i g m o i d}(\\mathbf{x}_{\\iota}^{\\iota})\\leq\\mathbf{s}\\mathtt{i g m o i d}(\\dot{\\mathbf{x}}_{i})$ because sigmoid is monotonically increasing. For the case when $\\dot{\\mathbf{x}}_{i}<\\mathbf{x}_{i}^{l}$ , we have $\\underline{{\\dot{\\mathbf{y}}}}_{i}\\leq\\mathbf{a}_{i}^{l}\\underline{{\\dot{\\mathbf{x}}}}_{i}\\!+\\!\\mathbf{b}_{i}^{l}\\leq\\mathbf{s}\\mathrm{i}\\,\\mathtt{g m o i d}(\\dot{\\mathbf{x}}_{i})$ because $f(\\mathbf{x})=\\mathbf{a}_{i}^{l}x+$ $\\mathbf{b}_{i}^{l}$ is tangent to the convex piece of sigmoid in $[-\\infty,0]$ , and sigmoid is monotonically increasing in $[0,\\mathbf{x}_{i}^{l}]$ . Therefore, $\\underline{{\\varphi}}_{\\mathtt{S i g m o i d}}(\\widetilde{\\mathbf{y}},\\widetilde{\\mathbf{x}})$ defined in Definition C.5 implies that $\\forall\\mathbf{x}\\in$ [xxx,xxx]. $\\dot{\\underline{{\\mathbf{y}}}}\\leq\\mathtt{S i g m o i d}(\\mathbf{x})$ . ", "page_idx": 26}, {"type": "text", "text": "C.3 Parametric Linear Relaxation for ELU layers ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Definition C.7. $\\mathbf{y}\\ {\\overset{\\mathrm{def}}{=}}\\ \\mathsf{E L U}(\\mathbf{x})$ with input $\\mathbf{x}\\in\\mathbb{R}^{m}$ and output $\\mathbf{y}\\in\\mathbb{R}^{m}$ is defined as $\\mathbf{y}_{i}\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\mathsf{e l u}(\\mathbf{x}_{i})$ $\\operatorname{elu}(\\pmb{x})\\stackrel{\\mathrm{def}}{=}\\left\\{\\begin{array}{l l}{\\pmb{x}}&{\\pmb{x}\\geq0}\\\\ {\\alpha(e^{\\pmb{x}}-1)}&{\\pmb{x}<0}\\end{array}\\right.$ and $\\alpha>0$ . ", "page_idx": 26}, {"type": "text", "text": "Definition C.8. For ELU layer with variable input bounds [xxx,xxx], its Parametric Linear Relaxation is defined as $\\underline{{\\varphi}}_{\\mathtt{E L U}}(\\underline{{\\dot{\\mathbf{y}}}},\\underline{{\\ddot{\\mathbf{y}}}},\\underline{{\\dot{\\mathbf{x}}}},\\underline{{\\ddot{\\mathbf{x}}}})\\overset{\\mathrm{def}}{=}\\overline{{\\varphi}}_{\\mathtt{E L U}}(\\overline{{\\mathbf{y}}},\\underline{{\\ddot{\\mathbf{x}}}})\\wedge\\underline{{\\varphi}}_{\\mathtt{E L U}}(\\underline{{\\dot{\\mathbf{y}}}},\\underline{{\\dot{\\mathbf{x}}}})$ where: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{\\varphi}}_{\\mathtt{E L U}}(\\ddot{\\mathtt{y}},\\ddot{\\mathtt{x}})\\ \\overset{\\mathrm{def}}{=}\\ \\ddot{\\mathtt{y}}\\geq\\ddot{\\mathtt{x}}\\ \\wedge\\ddot{\\mathtt{y}}\\geq0}\\\\ &{\\underline{{\\varphi}}_{\\mathtt{E L U}}(\\dot{\\underline{{y}}},\\dot{\\underline{{\\mathtt{x}}}})\\ \\overset{\\mathrm{def}}{=}\\ \\dot{\\underline{{\\mathtt{y}}}}\\leq{\\mathtt{a}}\\dot{\\underline{{x}}}+\\mathtt{b}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\mathbf{a}\\in\\mathbb{R}^{m}$ , $\\mathbf{b}\\in\\mathbb{R}^{m}$ are constants such that the line defined by $f({\\bf{x}})={\\bf{a}}_{i}{\\bf{x}}+{\\bf{b}}_{i}$ for any $i$ is tangent to elu. \u25a0 ", "page_idx": 26}, {"type": "text", "text": "Theorem C.9. Definition $C.8$ is a Parametric Linear Relaxation for the ELU layer. ", "page_idx": 26}, {"type": "text", "text": "Proof. As seen in Definition C.8, $\\underline{{\\overline{{\\varphi}}}}_{\\mathtt{E L U}}(\\underline{{\\dot{\\bf y}}},\\underline{{\\ddot{\\bf y}}},\\underline{{\\dot{\\bf x}}},\\underline{{\\ddot{\\bf x}}})$ is a linear formula. Let $m$ be the number of input dimensions of ELU. $\\underline{{\\overline{{\\varphi}}}}_{\\mathtt{E L U}}(\\underline{{\\dot{\\mathbf{y}}}},\\underline{{\\ddot{\\mathbf{y}}}},\\underline{{\\dot{\\mathbf{x}}}},\\underline{{\\ddot{\\mathbf{x}}}})$ has $3m$ linear constraints over $4m$ variables $(\\dot{\\underline{{\\mathbf{x}}}}\\in\\mathbb{R}^{m}$ , $\\mathbf{\\ddot{x}}\\in\\mathbb{R}^{m}$ , $\\dot{\\underline{{\\mathbf{y}}}}\\in\\mathbb{R}^{m}$ and $\\mathbf{\\ddot{y}}\\in\\mathbb{R}^{m}$ ). Hence, $\\underline{{\\overline{{\\varphi}}}}_{\\mathtt{E L U}}(\\underline{{\\dot{\\bf y}}},\\underline{{\\ddot{\\bf y}}},\\underline{{\\dot{\\bf x}}},\\underline{{\\ddot{\\bf x}}})$ is a poly-size linear formula, whose size is polynomial in the number of input dimensions of the ELU layer. ", "page_idx": 26}, {"type": "text", "text": "Now our goal is to prove that $\\underline{{\\overline{{\\varphi}}}}_{\\mathtt{E L U}}(\\underline{{\\dot{\\bf y}}},\\ddot{\\underline{{\\bf y}}},\\dot{\\underline{{\\bf x}}},\\ddot{\\underline{{\\bf x}}})$ defined in Definition C.8 implies ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\forall\\mathbf{x}\\in[\\dot{\\underline{{\\mathbf{x}}}},\\ddot{\\mathbf{x}}]\\mathbf{\\cdot}\\,\\dot{\\underline{{\\mathbf{y}}}}\\leq\\mathtt{E L U}(\\mathbf{x})\\leq\\ddot{\\mathbf{y}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We will discuss the upper and lower bounds separately. ", "page_idx": 26}, {"type": "text", "text": "For the upper bound constraint $\\overline{{\\varphi}}_{\\mathtt{E L U}}(\\mathbf{\\ddot{y}},\\mathbf{\\ddot{x}})\\overset{\\mathrm{def}}{=}\\ddot{\\mathbf{y}}\\ge\\ddot{\\mathbf{x}}\\wedge\\ddot{\\mathbf{y}}\\ge0$ , for the case when $\\dot{\\bf x}_{i}\\geq0$ , by Definition C.7 we have $\\ddot{\\mathbf{y}}_{i}\\geq\\ddot{\\mathbf{x}}_{i}=\\mathsf{e l u}(\\ddot{\\mathbf{x}}_{i})$ . For the case $\\dot{\\bf x}_{i}<0$ , because elu is monotonically increasing and $\\mathtt{e l u}(0)=0$ , we have $\\tilde{\\mathbf{y}}_{i}\\geq0\\geq\\mathsf{e l u}(\\ddot{\\mathbf{x}}_{i})$ . Therefore, $\\overline{{\\varphi}}_{\\mathtt{E L U}}(\\mathbf{\\widetilde{y}},\\mathbf{\\widetilde{x}})$ defined in Definition C.8 implies that $\\forall\\mathbf{x}\\in$ [xxx,xxx]. $\\begin{array}{r}{\\mathrm{ELU}(\\mathbf{x})\\leq\\ddot{\\mathbf{y}}}\\end{array}$ . ", "page_idx": 26}, {"type": "text", "text": "For the lower bound constraint $\\underline{{\\varphi}}_{\\mathtt{E L U}}(\\underline{{\\dot{\\mathbf{y}}}},\\underline{{\\dot{\\mathbf{x}}}})\\ \\ \\stackrel{\\mathrm{def}}{=}\\ \\underline{{\\dot{\\mathbf{y}}}}\\ \\leq\\ \\mathbf{a}\\underline{{\\dot{\\mathbf{x}}}}+\\mathbf{b}.$ , because the line defined by $f(\\pmb{x})=\\mathbf{a}_{i}\\pmb{x}+\\mathbf{b}_{i}$ is tangent to elu and elu is a convex function, we have $\\underline{{\\dot{\\mathbf{y}}}}_{i}\\leq\\mathbf{a}_{i}\\underline{{\\dot{\\mathbf{x}}}}_{i}+\\mathbf{b}_{i}\\leq\\mathbf{e}\\mathrm{1u}(\\underline{{\\dot{\\mathbf{x}}}}_{i})$ . Therefore, $\\underline{{\\varphi}}_{\\mathtt{E L U}}(\\tilde{\\mathbf{y}},\\ddot{\\mathbf{x}})$ defined in Definition C.8 implies that $\\forall\\mathbf{x}\\in$ [xxx,xxx]. $\\dot{\\underline{{\\mathbf{y}}}}\\leq\\mathtt{E L U}(\\mathbf{x})$ . \u53e3 ", "page_idx": 26}, {"type": "text", "text": "D Experiment Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "D.1 Provable fine-tuning PFT\u27e8\u27e8\u27e8\u00b7\u27e9\u27e9\u27e9 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Because there are no efficient prior approaches for provable editing, we implemented provable fine-tuning ${\\tt P F T}\\langle\\cdot\\rangle$ baselines that combine prior (non-provable) DNN editing approaches with a verifier in the loop: the editing stops when the verifier confirms that the DNN satisfies the property (Appendix D.1). Specifically: 1) In each epoch, provable fine-tuning first updates the DNN parameters with the DNN editing approach; 2) If the editing approach or adversarial attack cannot find any constraint violation, we call a verifier to check if all constraints are satisfied; 3) If all constraints are satisfied, the loop terminates; otherwise the loop continues. According to the DNN architecture and editing property, we use the best verifier from (1) $\\alpha,\\beta$ -CROWN [46, 44]: winner of the VNN Competition VNN-COMP 2021, 2022 and 2023; (2) MN-BaB [9]: runner-up of VNN-COMP 2022 and 2023; and (3) DeepT [4]: state-of-the-art (incomplete) transformer verifier. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "In order to improve the performance of provable fine-tuning with a non-provable DNN editing approach, viz., preserve the DNN\u2019s accuracy, we a) freeze all batch normalization layers, b) only edit the last few layers, c) incorporate changes of parameters as regularization, and d) sample points and incorporate the changes in their outputs as regularization. ", "page_idx": 27}, {"type": "text", "text": "D.2 Provable Editing on VNN Competition Benchmarks ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section we present the further details for Section 4.1. ", "page_idx": 27}, {"type": "text", "text": "Benchmarks. We take 12 out of 15 benchmarks from VNN-COMP\u203222 [29], excluding i) \u201ccarvana_unet_2022\u201d and \u201cnn4sys\u201d: the support for the sigmoid activation function is not implemented in PREPARED, and is not supported by APRNN. ii) \u201cvggnet16_2022\u201d: the bound computation for intermediate layers runs out of CPU and GPU memory using auto_LiRPA. ", "page_idx": 27}, {"type": "text", "text": "Setup details. For both PFT\u27e8\u27e8\u27e8DL2\u27e9\u27e9\u27e9and PFT\u27e8\u27e8\u27e8APRNN\u27e9\u27e9\u27e9, we use DL2 to find counterexamples to edit, and we use $\\alpha,\\beta$ -CROWN, the winner of VNN-COMP, as the verifier. We only call the verifier when DL2 cannot find counterexamples. For all approaches, we edit the last few layers, which differs for different DNNs. For PREPARED, we use auto_LiRPA or DeepPoly to compute the constant bounds for the input to the first layer to edit. ", "page_idx": 27}, {"type": "text", "text": "Hyperparameters for PFT\u27e8\u27e8\u27e8DL2\u27e9\u27e9\u27e9. SGD optimizer with a learning rate of 0.001, DL2 weight of 0.2, batch size of 32 for the single-property setting, and 256 for the all-properties setting. ", "page_idx": 27}, {"type": "text", "text": "Hyperparameters for PFT\u27e8\u27e8\u27e8APRNN\u27e9\u27e9\u27e9. For different DNNs, APRNN edits from the last six layers to the last layer. we set a bound of [-10, 10] for the change of each parameter. We use DL2 to find counterexamples for APRNN to edit in each epoch, and use the same hyperparameters as the DL2 experiments for the DL2 oracle. ", "page_idx": 27}, {"type": "text", "text": "Hyperparameters for PREPARED. For different DNNs, PREPARED edits from the last eight layers to the last layer. We use auto_LiRPA [50] or DeepPoly [39] to compute the input bounds to the first layer to edit. we set a bound of [-10, 10] for the change of each parameter. ", "page_idx": 27}, {"type": "text", "text": "Results details. In Table 4 we present the number of succeed instances for provably editing single-properties and all-properties instances in each benchmark. ", "page_idx": 27}, {"type": "text", "text": "D.3 Local robustness editing for image-recognition DNNs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section we present the experiment details for Section 4.2. ", "page_idx": 27}, {"type": "text", "text": "Property. Given an $L^{\\infty}$ local robustness perturbation $\\varepsilon$ for an DNN $\\mathcal{N}$ and a pair of input point $\\mathbf{x}$ and output label $l$ , the repair specification is $\\forall\\mathbf{x}^{\\prime}$ . $\\|\\mathbf{x}-\\mathbf{x}^{\\prime}\\|_{\\infty}\\implies$ arg max $\\bar{\\mathcal{N}}(\\mathbf{x}^{\\prime})=l$ . In other words, for all inputs $\\mathbf{x}^{\\prime}$ in the $L^{\\infty}$ ball around $\\mathbf{x}$ with radius $\\varepsilon$ , the DNN $\\mathcal{N}$ should classify them as label $l$ . ", "page_idx": 27}, {"type": "text", "text": "Setup details. The validation set is the same $10\\%$ of the training set, randomly selected with seed 0. For PFT\u27e8\u27e8\u27e8DL2\u27e9\u27e9\u27e9, PFT\u27e8\u27e8\u27e8SABR\u27e9\u27e9\u27e9and PFT\u27e8\u27e8\u27e8STAPS\u27e9\u27e9\u27e9, we use MN-BaB, the complete verifier used by SABR and STAPS, as the verifier in the loop. MN-BaB is the runner-up of VNN-COMP. We only call the verifier when DL2, SABR or STAPS cannot find counterexamples. For PREPARED, we use DeepPoly to compute the constant bounds for the input to the first layer to edit. ", "page_idx": 27}, {"type": "text", "text": "Hyperparameters for $\\mathsf{P F T}\\langle\\mathtt{S A B R}\\rangle$ . We take the base hyperparameters from the SABR training script [27]. We freeze all batch normalization layers, and search the hyperparameters: i) learning rate in $\\{5\\mathrm{e}{-4},\\,1\\mathrm{e}{-4},\\,5\\mathrm{e}{-5},\\,1\\mathrm{e}{-5}\\}$ ; ii) batch size in $\\{5,10,25\\}$ ; iii) lambda in $\\{0.1,0.2,\\ldots,0.9\\}$ ; iv) end_epoch_eps in $\\{80,60,40,20\\};$ ; v) fine-tune the entire DNN, the last three layers, or the last layer to find the edit with the highest efficacy, then the best validation set accuracy. Other hyperparameters are the same as used for training the CIFAR10 $\\underline{{\\epsilon}}=2/255)$ and TINYIMAGENET $\\left\\langle\\varepsilon=1/255\\right\\rangle$ CNN7 in the SABR paper. We use the following early-stopping criteria: i) CIFAR10: validation accuracy drops below $60\\%$ ; ii) TINYIMAGENET: validation accuracy drops below $20\\%$ . The final hyperparameters for CIFAR10 are: Adam optimizer with a learning rate of 5e-4, batch size of 5, ", "page_idx": 27}, {"type": "text", "text": "Table 4: Comparison of the number of succeed instances for provably editing single-instances and all-properties instances in each benchmark. ", "page_idx": 28}, {"type": "table", "img_path": "IGhpUd496D/tmp/259422ee912afc8c830a4d104e34eba3e4b9534c4ec41625ae77fce004fa9b8d.jpg", "table_caption": ["(a) Comparison of the number of instances succeeded for provably editing single-instances instances in each benchmark. "], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "IGhpUd496D/tmp/b9d75de59b3d9b40bddfc9d82af5f62667b42e1c4d5c179d249b843ac5fce456.jpg", "table_caption": ["(b) Comparison of the number of instances succeeded for provably editing all-properties instances in each benchmark. "], "table_footnote": ["\u2217The DNNs that violate the properties in these benchmarks only have one property, hence are subsumed in the prior single-property setting and excluded in this all-properties setting. "], "page_idx": 28}, {"type": "text", "text": "lambda of 0.1, end_epoch_eps of 80, and fine-tune the last three layers. The final hyperparameters for TINYIMAGENET are: Adam optimizer with a learning rate of 5e-4, batch size of 5, lambda of 0.1, end_epoch_eps of 80, and fine-tune the last three layers. ", "page_idx": 28}, {"type": "text", "text": "Hyperparameters for PFT\u27e8\u27e8\u27e8STAPS\u27e9\u27e9\u27e9. We take the base hyperparameters from the STAPS training script in the TAPS code [25]. We freeze all batch normalization layers, and search the following hyperparameters: i) learning rate in $\\{5\\mathrm{e}{-4},\\,1\\mathrm{e}{-4},\\,5\\mathrm{e}{-5},\\,1\\mathrm{e}{-5}\\}$ ; ii) batch size in $\\{5,10,25\\}$ ; iii) reg_lambda in $\\{0.1,0.2,\\ldots,0.9\\}$ ; iv) end_epoch_eps in $\\{80,60,40,20\\}$ . v) fine-tune the entire DNN, the last three layers, or the last layer. to find the edit with the highest efficacy, then the best validation set accuracy. Other hyperparameters are the same as used for training CIFAR10 $\\underline{{\\epsilon}}=2/255)$ and TINYIMAGENET $(\\varepsilon\\,=\\,1/255)$ CNN7 using STAPS in the TAPS paper We use the following early-stopping criteria: i) CIFAR10: validation accuracy drops below $60\\%$ ; ii) TINYIMAGENET: validation accuracy drops below $20\\%$ . The final hyperparameters for CIFAR10 are: Adam optimizer with a learning rate of 1e-5, batch size of 10, reg_lambda of 0.9, end_epoch_eps of 80, and fine-tune the last three layers. The final hyperparameters for TINYIMAGENET are: Adam optimizer with a Learning rate of 1e-5, batch size of 10, reg_lambda of 0.8, end_epoch_eps of 80, and fine-tune the last three layers. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "Hyperparameters for PFT\u27e8\u27e8\u27e8DL2\u27e9\u27e9\u27e9. We take the base hyperparameters from the RobustnessG training script in the DL2 code [11]. We freeze all the batch normalization layers, and search for the following hyperparameters: i) learning rate in $\\{1{\\mathrm{e}}{-}3,5{\\mathrm{e}}{-}4,1{\\mathrm{e}}{-}4,5{\\mathrm{e}}{-}5\\}$ ; ii) batch size in $\\{5,10,25\\}$ ; iii) fine-tune the entire DNN, the last layer or the last three layers. to find the edit with the highest efficacy, then the best validation set accuracy. We use the following early-stopping criteria: i) CIFAR10: validation accuracy drops below $70\\%$ ; ii) TINYIMAGENET: validation accuracy drops below $20\\%$ . The final hyperparameters for CIFAR10 are: SGD optimizer with learning rate of 1e-4, batch size of 10, DL2 weight of 0.2, fine-tune the last three layers. The final hyperparameters for TINYIMAGENET are: SGD optimizer with learning rate of 1e-4, batch size of 10, DL2 weight of 0.2, fine-tune the last three layers. ", "page_idx": 29}, {"type": "text", "text": "Hyperparameters for APRNN. We edit up to the fifth last layer, and set a bound [-10, 10] for the change of each DNN parameter. For both CIFAR10 and TINYIMAGENET, the final result comes from editing the last layer. ", "page_idx": 29}, {"type": "text", "text": "Hyperparameters for PREPARED. We edit up to the fifth last layer, and set a bound [-10, 10] for the change of each DNN parameter. For both CIFAR10 and TINYIMAGENET, the final result comes from editing the last layer. ", "page_idx": 29}, {"type": "text", "text": "D.4 Local robustness editing for sentiment classification BERT transformers ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this section we present the experiment details for Section 4.3. ", "page_idx": 29}, {"type": "text", "text": "Setup details. We use the default validation set used in the DeepT training scripts [3]. For PFT\u27e8\u27e8\u27e8DL2\u27e9\u27e9\u27e9, we use DeepT as the verifier in the loop. We only call the verifier when DL2 cannot find counterexamples. For PREPARED, we use DeepT to compute the constant bounds for the input to the first layer to edit. ", "page_idx": 29}, {"type": "text", "text": "Hyperparameters for $\\mathsf{P F T}\\langle\\mathsf{D L}2\\rangle$ . We incorporate the default training script and hyperparameters from DeepT [4]. We search the hyperparameters i) fine-tune the last three layers or only the last layer; ii) batch size in $\\{4,8,16,32\\}$ ; iii) learning rate in $\\{1{\\mathrm{e}}{-}3,5{\\mathrm{e}}{-}3,1{\\mathrm{e}}{-}4,5{\\mathrm{e}}{-}4\\}$ ; to find the edit with the best efficacy, then the best validation accuracy. For $\\varepsilon{=}1\\mathrm{e}{-}4$ , the final hyperparameters are: fine-tune the last layer, batch size of 32, Adam optimizer with learning rate of 1e-4, DL2 weight of 0.1 For $\\varepsilon{=}5\\mathrm{e}{-}4$ , the final hyperparameters are: fine-tune the last layer, batch size of 16, Adam optimizer with learning rate of 1e-4, DL2 weight of 0.1. ", "page_idx": 29}, {"type": "text", "text": "Hyperparameters for PFT\u27e8\u27e8\u27e8APRNN\u27e9\u27e9\u27e9. We edit up to the third last layer, and set a bound [-10, 10] for the change of each DNN parameter. The final result comes from editing the last layer. ", "page_idx": 29}, {"type": "text", "text": "Hyperparameters for PREPARED. We edit up to the third last layer, and set a bound [-10, 10] for the change of each DNN parameter. The final result comes from editing the last layer. ", "page_idx": 29}, {"type": "text", "text": "D.5 Provable training for physics-plausible DNNs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this section we present the experiment details for Section 4.4. ", "page_idx": 29}, {"type": "text", "text": "Hyperparameters for DL2. We search the hyperparameters i) batch size in $\\{4,8,16,32\\}$ ; ii) learning rate in {1e-3, 5e-3, 1e-4, 5e-4}; We train 2000 epochs and take the epoch with the highest validation accuracy. The final hyperparameters are: batch size of 8, SGD optimizer with lr of 1e-3, DL2 weight of 0.2, 1241 epochs. ", "page_idx": 29}, {"type": "text", "text": "Hyperparameters for GD. We search the hyperparameters i) batch size in $\\{4,8,16,32\\}$ ; ii) learning rate in {1e-3, 5e-3, 1e-4, 5e-4}; We train 2000 epochs and take the epoch with the highest validation accuracy. The final hyperparameters are: batch size of 32, SGD optimizer with lr of 1e-3, 635 epochs. ", "page_idx": 29}, {"type": "text", "text": "Hyperparameters for GD\u27e8\u27e8\u27e8APRNN\u27e9\u27e9\u27e9. We edit up to the first layer, and set a bound [-10, 10] for the change of each DNN parameter. The final result comes from editing the last layer. ", "page_idx": 29}, {"type": "text", "text": "Hyperparameters for GD\u27e8\u27e8\u27e8PREPARED\u27e9\u27e9\u27e9. We edit up to the first layer, and set a bound [-10, 10] for the change of each DNN parameter. We use DeepPoly to compute the input bounds to the first layer to edit. The final result comes from editing the last three layers. ", "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The theoretical claims in the abstract and introduction are proved in Section 3 as well as Appendix A, B. The empirical claims in the abstract and introduction are justified in Section 4. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: The limitations are discussed in Section 5 ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The full set of assumptions and informal proof sketch are provided in Section 3 complemented by formal proofs in Appendix A. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Section 3 complemented by Appendix B fully disclose all information needed to reproduce the technique, and Section 4 complemented by Appendix D fully disclose all information needed to reproduce the experimental results. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [No] ", "page_idx": 33}, {"type": "text", "text": "Justification: The experiments Section 4.1\u20134.3 use open-access benchmarks. We plan to release on acceptance the data we generated for Section 4.4. For the implementation of our tool, we could not make the code open access due to ongoing IP restrictions. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: The experimental settings are presented in Section 4 complemented by further details in Appendix D. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [No] ", "page_idx": 33}, {"type": "text", "text": "Justification: Because the proposed algorithm is deterministic, we instead evaluate our approach on a wide-variety of benchmarks to demonstrate its efficacy and efficiency. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 33}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The evaluation platform and runtime are provided in Section 4. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: This is discussed in Sections 1 and 5. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 34}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: This paper proposes an efficient and provable DNN editing approach, which does not involve releasing of data and models that have a high risk for misuse. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Section 4 cites the original papers that produced the baselines, tools, models and benchmarks used in the paper. Appendix D cites commits for open-access code and scripts used in the paper. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: We do not introduce new assets in the paper. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 36}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 37}]