[{"type": "text", "text": "Decentralized Noncooperative Games with Coupled Decision-Dependent Distributions ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Wenjing Yan Xuanyu Cao \\*   \nDepartment of Electronic and Computer Engineering   \nThe Hong Kong University of Science and Technology wj.yan@connect.ust.hk,eexcao@ust.hk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Distribution variations in machine learning, driven by the dynamic nature of deployment environments, significantly impact the performance of learning models. This paper explores endogenous distribution shifts in learning systems, where deployed models influence environments, which in turn alters the data distributions that the learning models rely on. This phenomenon is formulated by a decision-dependent distribution mapping within the recently introduced framework of performative prediction (PP) (Perdomo et al., 2020). Our study investigates the performative effect in a decentralized noncooperative game, where players aim to minimize private cost functions while simultaneously managing coupled inequality constraints. In this context, we examine two equilibrium concepts for the studied game: performative stable equilibrium (PSE) and Nash equilibrium (NE), and establish sufficient conditions for their existence and uniqueness. Notably, we provide the first upper bound on the distance between the PSE and NE in the literature, which is challenging to evaluate due to the absence of strong convexity on the joint cost function. Furthermore, we develop a decentralized stochastic primal-dual algorithm for efficiently computing the PSE point. By rigorously bounding the performative effect, we prove that the proposed algorithm achieves sublinear convergence rates for both performative regret and constraint violations and maintains the same order of convergence rate as the case without performativity. Numerical experiments further confirm the effectiveness of our algorithm and theoretical results. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Machine learning aims to generalize models trained on given datasets to make accurate predictions or decisions on new, unseen data (El Naqa and Murphy, 2015). The effectiveness of those models depends on the alignment between the training datasets and deployment environments (Quinonero-Candela et al., 2o08). However, real-world environments are seldom static and often exhibit fluctuations that can severely degrade model performance (Zhou, 2022). In particular, shifts in data-generating distributions, driven by the dynamic nature of real-world conditions, present significant challenges for model deployment. ", "page_idx": 0}, {"type": "text", "text": "Distribution shifts in machine learning can occur exogenously or endogenously. Exogenous distribution shifts are driven by external factors beyond the control of the learning platforms, such as environmental changes (Chan et al., 2020) or policy amendments (Wu et al., 2021). In contrast, endogenous shifts arise from the system's inherent dynamics and interactions, where the deployed models affect environments, which in turn alters the data distributions that the learning models rely on (Dong et al., 2018). For instance, an increase in commodity prices may decrease user interest, thereby impacting sales. The key distinction lies in the controllability of endogenous shifts, providing an opportunity for designers to either exploit these shifts for improved performance or mitigate unintended consequences (Dean et al., 2023). ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "While substantial efforts have been made to address exogenous distribution changes, such as covariate shift (Chan et al., 2020), label shift (Wu et al., 2021), and concept drift (Lu et al., 2018), relatively little attention has been paid to the challenges posed by endogenous distribution shifts. Tackling these endogenous shifts is particularly challenging as data distributions are intrinsically linked to the decisions made by the learning model itself (Perdomo et al., 2020). As a result, addressing endogenous shifts may require the explicit modeling of feedback loops, consideration of causal relationships, and the adaptation of models to dynamic environments. ", "page_idx": 1}, {"type": "text", "text": "A notable advancement in this area is the recently proposed framework of \u201c\"performative prediction (PP) (Perdomo et al., 2020), also referred to as \u201cdecision-dependent learning\u201d (Drusvyatskiy and Xiao, 2023). This framework elegantly captures the dynamic interplay between decisions and data distributions through a decision-dependent mapping, denoted by $\\mathcal{D}(\\pmb{\\theta})$ where $\\pmb{\\theta}$ represents the decision variable. By linking $\\pmb{\\theta}$ to the data distribution, this formulation bridges the gap between model deployment and parameter optimization. Following the seminal work of (Perdomo et al., 2020), a growing body of research has emerged, focusing on stability and optimality analysis (Piliouras and Yu, 2023; Miller et al., 2021), as well as algorithmic design for various settings, including reinforcement learning (Mandal et al., 2023), online learning (Wood et al., 2021), bandit problems (Jagadeesan et al., 2022), and bilevel optimization (Lu, 2023). ", "page_idx": 1}, {"type": "text", "text": "This paper investigates endogenous distribution shifts in a decentralized noncooperative game, where players aim to minimize private cost functions while simultaneously managing coupled inequality constraints. To contextualize this setting, consider scenarios where strategic responses exhibit in learning environments and competitive interactions occur among players. For example, in autonomous vehicular networks, multiple vehicles compete to select their routes under constraints such as road capacities, traffic congestion, and travel costs. The route choices of each vehicle influence traffic patterns and consequently affect the travel times experienced by other vehicles (Mori et al., 2015). Similarly, in finance, traders compete to maximize profits under constraints like market capacities and inventory levels. The trading strategies of these participants impact market volatility and the distribution of asset prices, creating a dynamic pricing landscape (Fattouh and Mahadeva, 2014). These dynamics extend to other domains, such as electricity market competition (Moshari et al., 2010), ride-sharing platforms (Narang et al., 2023), natural resource extraction (Cust and Poelhekke, 2015), and online advertising auctions (Varian, 2009). ", "page_idx": 1}, {"type": "text", "text": "Despite its pervasiveness, this performative phenomenon has largely been overlooked in the studies of decentralized noncooperative games. This paper addresses the problem by formulating performativity using coupled decision-dependent distributions, following the PP framework of (Perdomo et al., 2020). However, the intricate interplay between decentralized players and endogenous distribution shifts presents challenging theoretical and algorithmic questions: How do strategic responses in learning environments infuence the game's equilibrium?How can players adapt their strategies effectively when confronted with coupled decision-dependent distributions?How canwe design algorithms to exploit these dynamics for optimal decision-making? These questions form the core of our investigation, guiding us toward more resilient, adaptive, and efficient learning outcomes in decentralized games, especially in environments characterized by continuously evolving data and decision-making processes. Our main contributions are summarized below: ", "page_idx": 1}, {"type": "text", "text": "\u00b7 We initially formulate the problem of decentralized noncooperative games with data performativity, where selfish players seek to minimize individual costs while managing coupled inequality constraints. Under this setting, we examine two equilibrium concepts: performative stable equilibrium (PSE) and Nash equilibrium (NE), and establish sufficient conditions for their existence and uniqueness. Compared to conventional games, this examination is more complicated due to the interplay between decision-making and distribution changes. Notably, we make a significant contribution by providing the first upper bound on the distance between the PSE and NE in the literature. Computing this distance in PP games is challenging due to the absence of strong convexity on the joint cost function, an essential property for determining the optimality gap of performative stable points in previous work. Instead, we characterize the distance by leveraging relations from strong duality and derive a result comparable to the findings of the prior work (Perdomo et al., 2020; Lu, 2023). ", "page_idx": 1}, {"type": "text", "text": "\u00b7 To compute the PSE point of the PP-game, we propose a decentralized stochastic primal-dual algorithm based on repeated risk minimization (RRM). The development and convergence analysis of this algorithm face two primary challenges. First, there is a complex interaction between decentralized competition and endogenous distribution shifts. Second, players only have partial observation, as they communicate solely with neighbors, despite their private cost functions being infuenced by the strategies of all players. We evaluate the performance of our algorithm by two commonly used metrics: performative regret, which measures the suboptimality of the strategy sequence generated by RRM relative to the PSE point, and constraint violation. By rigorously bounding the performative effect, we prove that the proposed algorithm achieves sublinear convergence rates for both metrics. Furthermore, our results show that while the performative effect slows down convergence, it does not degrade the order of performative regret compared to the case without performativity (Lu et al.,2020). ", "page_idx": 2}, {"type": "text", "text": "Finally, we conduct numerical experiments on a networked Cournot game and a ride-share market. The simulation results confirm the sublinear convergence of our algorithm. Furthermore, the results demonstrate that while greater performative strength leads to a wider gap between the PSE and NE, the discrepancy between these two equilibria remains marginal. This verifies both the effectiveness of the PSE solutions and the accuracy of our distance analysis between the PSE and NE. ", "page_idx": 2}, {"type": "text", "text": "Related Work: Among the numerous existing studies, two closely related works (Narang et al., 2023) and (Wang et al., 2023) have considered performative behaviors in games. A key distinction in our work is that our model requires all players\u2019 collective strategies to adhere to the constraints of the learning system, whereas both (Narang et al., 2023) and (Wang et al., 2023) address unconstrained settings. This difference results in fundamentally distinct algorithmic designs and convergence analyses. Our approach employs a primal-dual technique and requires consensus, whereas their methods only rely on local stochastic gradient descent. Additionally, we consider a mathematically richer model compared to (Wang et al., 2023), whose framework is structured in a specific form involving local costs dependent solely on individual strategies and a regularizer quantifying similarity among neighboring strategies. Furthermore, our algorithm design accounts for practical constraints where players can only communicate with their immediate neighbors, while (Narang et al., 2023) assumes full accessibility to all players\u2019 strategies across the entire network. Importantly, our work makes a significant contribution by providing the first upper bound on the distance between the performative stable equilibrium (PSE) and Nash equilibrium (NE)a gap not previously addressed. Other related works such as (Li et al., 2022) and (Piliouras and Yu, 2023), have studied performative prediction in decentralized multi-agent optimization. The former focuses on consensus-seeking agents, while the latter is restricted to location-scale families. Finally, (Yan and Cao, 2024b) considers the constrained performative prediction problem in a single-agent setting, whereas our paper addresses decentralized noncooperative games. A more comprehensive literature review is provided in Appendix A. ", "page_idx": 2}, {"type": "text", "text": "2 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Consider a decentralized noncooperative game with $n$ players. Each player $i$ selects a strategy (or, interchangeably, decision, action), denoted as $\\theta_{i}$ , from its feasible set $\\bar{\\Omega_{i}}\\subseteq\\mathbb{R}^{d}$ . Let the collective decisions of all players be denoted as $\\pmb{\\theta}:=\\mathrm{col}\\left(\\pmb{\\theta}_{1},\\cdot\\cdot\\cdot\\mathbf{\\nabla},\\pmb{\\theta}_{n}\\right)$ , and the collective decisions of all players except player $i$ be represented as $\\pmb{\\theta}_{-i}:=\\operatorname{col}\\left(\\pmb{\\theta}_{1},\\cdot\\cdot\\cdot\\mathbf{\\theta},\\pmb{\\theta}_{i-1},\\pmb{\\theta}_{i+1},\\cdot\\cdot\\cdot\\mathbf{\\theta},\\pmb{\\theta}_{n}\\right)$ for any $i\\in[n]$ , where $[n]$ denotes the set of integers $\\{1,2,\\ldots,n\\}$ . Each player $i$ has a private cost function $J_{i}(\\xi_{i};\\bar{\\pmb{\\theta}}_{i},\\pmb{\\theta}_{-i})$ which depends on the random variable $\\xi_{i}\\in\\Xi_{i}$ , the player's private decision $\\theta_{i}$ , and the decisions of all other players $\\theta_{-i}$ . This paper considers a scenario where the underlying populations strategically respond to the players? decisions, causing shifts in data distributions. This interplay is modeled by a decision-dependent distribution mapping $\\pmb{\\xi}_{i}\\sim\\mathcal{D}_{i}\\left(\\pmb{\\theta}_{i},\\pmb{\\theta}_{-i}\\right)$ for all $i\\in[n]$ . The objective of each player $i$ is to selfishly minimize its performative risk $\\mathbb{E}_{\\pmb{\\xi}_{i}\\sim\\mathcal{D}_{i}(\\pmb{\\theta}_{i},\\pmb{\\theta}_{-i})}J_{i}(\\pmb{\\xi}_{i};\\pmb{\\theta}_{i},\\pmb{\\theta}_{-i})$ (abbreviated as $\\mathrm{PR}_{i}(\\pmb{\\theta}_{i},\\pmb{\\theta}_{-i}))$ , subject to a coupled constriant $\\begin{array}{r}{\\sum_{i=1}^{n}\\pmb{g}_{i}(\\pmb{\\theta}_{i})\\preceq\\mathbf{0}}\\end{array}$ i.e., ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\theta_{i}\\in\\Omega_{i}}{\\mathrm{min}}}&{\\mathbb{E}_{\\pmb{\\xi}_{i}\\sim\\mathcal{D}_{i}(\\pmb{\\theta}_{i},\\pmb{\\theta}_{-i})}J_{i}\\left(\\pmb{\\xi}_{i};\\pmb{\\theta}_{i},\\pmb{\\theta}_{-i}\\right)}\\\\ {\\mathrm{subject\\,to}}&{\\pmb{g}_{i}(\\pmb{\\theta}_{i})+\\sum_{j\\neq i}\\pmb{g}_{j}(\\pmb{\\theta}_{j})\\preceq\\mathbf{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Both $J_{i}(\\cdot)$ and $g_{i}(\\cdot)$ are only locally accessible to player $i$ for all $i\\in[n]$ . In the game (1), each player solves its private optimization problem to determine the best strategy, given the current strategies of all the other players. An equilibrium of the game (1) corresponds to a set of strategies where no player can improve its performance by deviating unilaterally from its strategy. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Denote by $\\pmb{\\xi}~:=~\\operatorname{col}\\left(\\pmb{\\xi}_{1},\\cdot\\cdot\\cdot~,\\pmb{\\xi}_{n}\\right)$ the concatenation of the variables $\\xi_{i}$ and by. $J\\left(\\xi;\\theta\\right)\\;:=\\;$ $\\operatorname{col}\\left(J_{1}\\left(\\xi_{1};\\pmb{\\theta}\\right),\\cdot\\cdot\\cdot\\mathbf{\\nabla},J_{n}\\left(\\pmb{\\xi}_{n};\\pmb{\\theta}\\right)\\right)$ the concatenation of the cost functions $J_{i}(\\cdot)$ for all $\\textit{i}\\in$ $[n]$ (A stochastic pseudogradient mapping of $J\\left(\\xi;\\theta\\right)$ is defined as_ $\\begin{array}{r l}{\\nabla J\\left(\\xi;\\theta\\right)}&{{}:=}\\end{array}$ $\\overset{!}{\\underset{}{\\vert}}\\left(\\nabla_{\\pmb{\\theta}_{1}}J_{1}\\left(\\pmb{\\xi}_{1};\\pmb{\\theta}\\right),\\cdot\\cdot\\cdot\\right)\\nabla_{\\pmb{\\theta}_{n}}J_{n}^{-}\\left(\\pmb{\\xi}_{n};\\pmb{\\theta}\\right)\\rbrace$ . We have the following assumption on $\\nabla J\\left(\\xi;\\theta\\right)$ ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.1. There exists a constant $\\mu>0$ such that the stochastic gradient mapping $\\nabla J\\left(\\xi;\\theta\\right)$ is $\\mu$ -strongly monotone, i.e., $\\left\\langle\\nabla J\\left(\\xi;\\theta\\right)-\\nabla J\\left(\\xi;\\theta^{\\prime}\\right),\\theta-\\theta^{\\prime}\\right\\rangle\\geq\\mu\\Vert\\bar{\\theta}-\\theta^{\\prime}\\Vert_{2}^{2},\\forall\\xi\\stackrel{\\cdot}{\\in}\\Xi,\\theta,\\theta^{\\prime}\\in\\Omega,$ where $\\Xi:=\\Xi_{1}\\times\\cdot\\cdot\\times\\Xi_{n}$ and $\\Omega:=\\Omega_{1}\\times\\cdots\\times\\Omega_{n}$ ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.1 is commonly made in the literature of game theory. It suffices to guarantee the existence of Nash equilibrium for a stochastic game with fixed data distributions (Facchinei and Pang, 2003, Theorem 2.3.3(b)). However, in our paper, since the data distributions are decision-dependent, Assumption 2.1 does not imply the monotonicity of the gradient mapping of the joint performative risk, denoted by $\\operatorname{PR}(\\cdot):={\\bar{\\mathrm{col}}}\\left(\\operatorname{PR}_{1}(\\cdot),\\cdot\\cdot\\cdot\\,,\\operatorname{PR}_{n}(\\cdot)\\right)$ . Therefore, the existence and uniqueness (E&U) conditions for the Nash equilibrium of the game (1) need further investigation. ", "page_idx": 3}, {"type": "text", "text": "We define a graph $\\mathcal{G}(\\mathbf{P})$ to represent the impact of players? decisions on the data distributions of different players. In $\\mathcal{G}(\\mathbf{P})$ , the weight $p_{i j}>0$ if player $j$ 's decision affects player $i$ 's data distribution, and $p_{i j}=0$ otherwise. Particularly, $p_{i i}$ represents the weight of self-influence. These weights are normalized as $\\textstyle\\sum_{j=1}^{n}p_{i j}=1$ ,foral $i\\in[n]$ Clearly, t aer twe $p_{i j}$ thesronger the effet of player $j$ 's decision on the data distribution of player ", "page_idx": 3}, {"type": "text", "text": "Let $\\mathcal{W}_{1}\\left(\\mathcal{D},\\mathcal{D}^{\\prime}\\right)$ represent the Wasserstein-1 distance between two probability measures $\\mathcal{D}$ and $\\mathcal{D}^{\\prime}$ Following Wang et al.,2023), we impose the following assumption on the distributions $\\{{\\mathcal{D}}_{i}\\}_{i\\in[n]}$ ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.2. For any $i~\\in~[n]$ , there exists a constant $\\varepsilon_{i}~\\geq~0$ such that, $\\forall\\pmb{\\theta},\\pmb{\\theta}^{\\prime}\\;\\in\\;\\Omega$ the distribution mapping $\\mathcal{D}_{i}$ is constrained by $\\begin{array}{r}{\\mathcal{W}_{1}\\left(\\mathcal{D}_{i}\\left(\\pmb{\\theta}\\right),\\mathcal{D}_{i}\\left(\\pmb{\\theta}^{\\prime}\\right)\\right)\\leq\\varepsilon_{i}\\sqrt{\\sum_{j=1}^{n}p_{i j}\\left\\|\\pmb{\\theta}_{j}-\\pmb{\\theta}_{j}^{\\prime}\\right\\|_{2}^{2}}}\\end{array}$ ", "page_idx": 3}, {"type": "text", "text": "For any $i\\in[n]$ , the parameter $\\varepsilon_{i}$ bounds the sensitivity of player $i$ 's distribution with respect to (w.r.t.) the decision variations of all players. This $\\varepsilon$ -sensitivity property of distributions is conceptually akin to the Lipschitz continuity of functions that quantifies the variation of function values w.r.t argument changes.We also require the following assumptions. ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.3. For any $i\\in[n]$ , the non-empty feasible set $\\Omega_{i}$ is closed, convex, and bounded, i.e. there exists a constant $C\\geq0$ such that, $\\forall\\pmb{\\theta}_{i}\\in\\Omega_{i}$ \uff0c $\\lVert\\pmb{\\theta}_{i}\\rVert_{2}\\leq C$ ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.4. For any $\\textit{i}\\in\\ [n]$ and $\\theta_{i}~\\in~\\Omega_{i}$ , the cost function $J_{i}(\\xi_{i};\\pmb{\\theta}_{i},\\pmb{\\theta}_{-i})$ is convex w.r.t. $\\theta_{i}$ . Moreover, there exists a constant $L_{i}~\\ge~0$ such that $J_{i}\\left(\\xi_{i};\\theta\\right)$ is $L_{i}$ -smooth, i.e, $\\begin{array}{r}{\\left\\|\\nabla J_{i}\\left(\\xi_{i};\\theta\\right)-\\nabla J_{i}\\left(\\xi_{i}^{\\prime};\\theta^{\\prime}\\right)\\right\\|_{2}\\le L_{i}\\left(\\left\\|\\xi_{i}-\\xi_{i}^{\\prime}\\right\\|_{2}+\\left\\|\\theta-\\theta^{\\prime}\\right\\|_{2}\\right),\\forall\\xi_{i},\\xi_{i}^{\\prime}\\in\\Xi_{i},\\theta,\\theta^{\\prime}\\in\\Omega}\\end{array}$ ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.5. For any $i\\in[n]$ and $\\pmb\\theta_{i}\\in\\Omega_{i}$ , the constraint function $g_{i}(\\pmb\\theta_{i})$ is convex w.r.t. $\\theta_{i}$ Moreover, there exist a constant $G_{g}\\ge0$ such that $g_{i}(\\cdot)$ is $G_{g}$ -Lipschitz, i.e., $\\left\\|\\pmb{\\mathscr{g}}_{i}(\\pmb{\\theta}_{i})-\\pmb{\\mathscr{g}}_{i}(\\pmb{\\theta}_{i}^{\\prime})\\right\\|_{2}\\leq$ $G_{g}\\|\\pmb{\\theta}_{i}-\\pmb{\\theta}_{i}^{\\prime}\\|_{2},\\forall\\pmb{\\theta}_{i},\\pmb{\\theta}_{i}^{\\prime}\\in\\Omega_{i}.$ ", "page_idx": 3}, {"type": "text", "text": "Assumptions 2.3 and 2.5 are widely used in constrained optimization (Bertsekas, 2014; Yan and Cao, 2024a), and Assumption 2.4 is standard in the PP literature. From Yan and Cao (2024a, Proposition 1), under Assumptions 2.3 and 2.4, the cost function $J_{i}(\\pmb{\\xi}_{i};\\pmb{\\theta}),\\forall i\\in[n]$ is Lipschitz continuous, i.e., there exist a constant $G_{i}\\geq0$ such that $|J_{i}(\\pmb{\\xi}_{i};\\pmb{\\theta})-J_{i}(\\pmb{\\xi}_{i}^{\\prime};\\pmb{\\theta}^{\\prime})|\\leq G_{i}$ $(\\left\\lVert\\bar{\\pmb{\\xi}_{i}}-\\pmb{\\xi}_{i}^{\\prime}\\right\\rVert_{2}+\\left\\lVert\\pmb{\\theta}-\\pmb{\\theta}^{\\prime}\\right\\rVert_{2})\\mathrm{,~}\\forall\\pmb{\\xi}_{i},\\pmb{\\xi}_{i}^{\\prime}\\in$ $\\Xi_{i},\\pmb{\\theta},\\pmb{\\theta}^{\\prime}\\in\\Omega$ . Moreover, Assumptions 2.3 and 2.5 imply the boundedness of $\\lVert g_{i}(\\pmb{\\theta}_{i})\\rVert_{2}$ , i.e., there exists a constant $B\\geq0$ such that $\\lVert\\pmb{g}_{i}(\\pmb{\\theta}_{i})\\rVert_{2}\\leq B,\\forall\\pmb{\\theta}_{i}\\in\\Omega_{i},i\\in[n]$ ", "page_idx": 3}, {"type": "text", "text": "3  Equilibrium of the PP-Game ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This section examines two fundamental equilibrium concepts of the performative game (1): Nash equilibrium (NE) and performative stable equilibrium (PSE), as defined below. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.1 (Nash Equilibrium). A vector $\\pmb{\\theta}^{\\mathrm{ne}}:=\\mathrm{col}\\left(\\pmb{\\theta}_{1}^{\\mathrm{ne}},\\dots,\\pmb{\\theta}_{n}^{\\mathrm{ne}}\\right)$ achieves an NE of the game (1) if it holds for any $i\\in[n]$ that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\pmb{\\theta}_{i}^{\\mathrm{ne}}\\in\\arg\\operatorname*{min}_{\\pmb{\\theta}_{i}\\sim\\mathcal{D}_{i}\\left(\\pmb{\\theta}_{i},\\pmb{\\theta}_{-i}^{\\mathrm{ne}}\\right)}J_{i}\\left(\\pmb{\\xi}_{i};\\pmb{\\theta}_{i},\\pmb{\\theta}_{-i}^{\\mathrm{ne}}\\right)}&{}\\\\ {\\mathrm{subject}\\mathrm{~to}}&{\\pmb{g}_{i}(\\pmb{\\theta}_{i})+\\sum_{j\\neq i}g_{j}(\\pmb{\\theta}_{j}^{\\mathrm{ne}})\\preceq\\mathbf{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Defnition 3.2Perfomative Stable Equilrium).A vet $\\pmb{\\theta}^{\\mathrm{pse}}:=\\mathrm{col}\\left(\\pmb{\\theta}_{1}^{\\mathrm{pse}},\\dots,\\pmb{\\theta}_{n}^{\\mathrm{pse}}\\right)$ achieves a PSE of the game (1) if it holds for any $i\\in[n]$ that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\pmb{\\theta}_{i}^{\\mathrm{pse}}\\in\\arg\\operatorname*{min}_{\\pmb{\\theta}_{i}\\in\\Omega_{i}}}&{\\mathbb{E}_{\\pmb{\\xi}_{i}\\sim\\mathcal{D}_{i}(\\pmb{\\theta}^{\\mathrm{pse}})}J_{i}\\left(\\pmb{\\xi}_{i};\\pmb{\\theta}_{i},\\pmb{\\theta}_{-i}^{\\mathrm{pse}}\\right)}\\\\ {\\mathrm{subject}\\mathrm{to}}&{\\pmb{g}_{i}(\\pmb{\\theta}_{i})+\\sum_{j\\neq i}\\pmb{g}_{j}(\\pmb{\\theta}_{j}^{\\mathrm{pse}})\\preceq\\mathbf{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "NE is a fundamental concept in game theory. At NE, each player's strategy optimally aligns with its own interest, given the strategies of other players. Hence, no player has an incentive to deviate from its strategy unilaterally. In the case of performative games, the computation of NE needs to take into account the data distributions $\\mathcal{D}_{i}(\\cdot)$ for all $i\\in[n]$ , as they are parameterized by the optimization variable $\\pmb{\\theta}$ . However, this information is often unavailable in practice. Instead, at PSE, the data distribution of each player $i\\in[n]$ is fixed at $\\mathcal{D}_{i}\\left(\\theta^{\\mathrm{pse}}\\right)$ and the PSE point achieves an NE of the game (1) under the fixed data distribution of its own deployment. This formulation draws benign properties akin to problems with fixed data distributions, facilitating the adaptation of existing algorithms. Therefore, PSE is more frequently chosen as a performance metric in the literature of PP. ", "page_idx": 4}, {"type": "text", "text": "3.1  Existence and Uniqueness of PSE ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We first establish the condition for the $\\mathrm{E}\\&\\mathrm{U}$ of the PSE of the game (1). Our approach relies on repeated risk minimization (RRM) for closed-loop retraining. First, we define a mapping ${\\mathcal{T}}(\\pmb{\\theta}):=$ $\\{\\bar{\\mathcal{T}}_{i}(\\theta)\\}_{i\\in[n]}$ that, for any $i\\in[n]$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\pmb{\\theta}_{i}^{\\prime}=\\mathcal{T}_{i}(\\pmb{\\theta}):=\\arg\\operatorname*{min}_{\\pmb{u}_{i}\\in\\Omega_{i}}}&{\\mathbb{E}_{\\pmb{\\xi}_{i}\\sim\\mathcal{D}_{i}(\\pmb{\\theta}_{i},\\pmb{\\theta}_{-i})}J_{i}\\left(\\pmb{\\xi}_{i};\\pmb{u}_{i},\\pmb{\\theta}_{-i}^{\\prime}\\right)}\\\\ {\\mathrm{subject~to}}&{g_{i}(\\pmb{u}_{i})+\\sum_{j\\neq i}g_{j}\\left(\\pmb{\\theta}_{j}^{\\prime}\\right)\\preceq\\mathbf{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The mapping $\\tau(\\pmb\\theta)$ outputs the NE of the game (1) under the fixed data distributions $\\mathcal{D}_{i}(\\pmb{\\theta}_{i},\\pmb{\\theta}_{-i})$ for all $i\\in[n]$ . With Assumption 2.1, the $\\mathrm{E}\\&\\mathrm{U}$ of this NE is guaranteed, thereby ensuring the validity of the mapping $\\tau(\\pmb\\theta)$ . Based on $\\tau(\\pmb\\theta)$ , the RRM updates $\\pmb{\\theta}_{i}^{t}$ at each iteration $t$ by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{\\theta}_{i}^{t+1}=T_{i}(\\pmb{\\theta}^{t}),\\forall i\\in[n].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Clearly, $\\pmb{\\theta}^{t+1}$ is an NE of the game (1) under the deployment of $\\pmb{\\theta}^{t}$ Additionally, we have that any fixed point of (2) achieves an PSE for the game (i), i.e., $\\theta^{\\mathrm{pse}}=\\mathcal{T}(\\theta^{\\mathrm{pse}})$ . By investigating the convergence the iterative equation (2), we have the following sufficient condition for the $\\mathrm{E}\\&\\mathrm{U}$ of the PSE of the game (1). ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.3. Suppose that Assumptions 2.1-2.5 hold. Then, for any $\\theta,\\delta\\in\\Omega_{*}$ themapping $\\tau(\\pmb\\theta)$ satisfies ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathcal{T}(\\theta)-\\mathcal{T}(\\delta)\\|_{2}\\leq\\frac{1}{\\mu}\\sqrt{\\sum_{i=1}^{n}L_{i}^{2}\\varepsilon_{i}^{2}\\operatorname*{max}_{j\\in[n]}p_{i j}}\\,\\|\\theta-\\delta\\|_{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Thus, if it is satisfed that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{\\mu}\\sqrt{\\sum_{i=1}^{n}L_{i}^{2}\\varepsilon_{i}^{2}\\operatorname*{max}_{j\\in[n]}p_{i j}}<1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "the sequence generated by the RRM (2) converges to a unique PSE point $\\pmb{\\theta}^{\\mathrm{pse}}$ at a linear rate that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\theta^{t+1}-\\theta^{\\mathrm{pse}}\\|_{2}\\leq\\left(\\frac{1}{\\mu}\\sqrt{\\sum_{i=1}^{n}L_{i}^{2}\\varepsilon_{i}^{2}\\operatorname*{max}_{j\\in[n]}p_{i j}}\\right)^{t}\\left\\|\\theta^{1}-\\theta^{\\mathrm{pse}}\\right\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The proof of Theorem 3.3 is provided in Appendix B. According to Theorem 3.3, under Assumptions 2.1-2.5, when condition (3) holds, we have that: (i) the game (1) admits a unique PSE, and (i) the RRM method (2) converges linearly to the PSE. ", "page_idx": 4}, {"type": "text", "text": "Since the influence weights $\\{p_{i j}\\}_{j\\in[n]}$ are normalized, with $\\textstyle\\sum_{j=1}^{n}p_{i j}\\ =\\ 1$ for all $i~\\in~[n]$ \uff0cwe generally have that $\\begin{array}{r}{p_{i j}=\\mathcal{O}\\big(\\frac{1}{n}\\big)}\\end{array}$ . Therefore, the contraction condition (3) exhibits good scalability w.r.t. the number of players. Moreover, according to the proof in Appendix $\\mathbf{B}$ , if for any player $i\\in[n]$ , its distribution $\\mathcal{D}_{i}(\\cdot)$ depends only on its own decision $\\theta_{i}$ , i.e., $p_{i j}=0$ for all $j\\neq i$ , then we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|{\\mathcal{T}}(\\pmb{\\theta})-{\\mathcal{T}}(\\pmb{\\delta})\\|_{2}\\leq\\frac{1}{\\mu}\\operatorname*{max}_{i\\in[n]}L_{i}\\varepsilon_{i}\\,\\|\\pmb{\\delta}-\\pmb{\\theta}\\|_{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The contraction of the above iterative equation only requires that $\\begin{array}{r}{\\frac{1}{\\mu}\\operatorname*{max}_{i\\in[n]}L_{i}\\varepsilon_{i}<1}\\end{array}$ Furthermore, if all players exhibit equivalent model parameters that $L_{1}=\\cdot\\cdot\\stackrel{.}{-}=L_{n}=L$ and $\\varepsilon_{1}=\\cdot\\cdot\\cdot=\\varepsilon_{n}=\\varepsilon$ and $\\begin{array}{r}{p_{i j}^{\\dot{}{}}=\\frac{1}{n}}\\end{array}$ for all $i,j\\,\\bar{\\in}\\,[n]$ , condition (3) reduces to $\\begin{array}{r}{\\frac{L\\varepsilon}{\\mu}<1}\\end{array}$ ,recovering thecontraction requirmt of (Perdomo et al., 2020) for a single-agent PP case. ", "page_idx": 5}, {"type": "text", "text": "3.2  Existence and Uniqueness of NE ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "First, we define a gradient mapping $G_{\\pmb{\\theta}}^{(i)}(\\delta_{i},\\delta_{-i}):=\\mathbb{E}_{\\pmb{\\xi}_{i}\\sim\\mathcal{D}_{i}(\\pmb{\\theta})}\\nabla_{\\delta_{i}}J_{i}\\left(\\pmb{\\xi}_{i};\\delta_{i},\\delta_{-i}\\right)$ for any $i\\in[n]$ and $G_{\\pmb\\theta}(\\pmb\\delta):=\\mathrm{col}\\left(G_{\\pmb\\theta}^{(1)}(\\pmb\\delta),\\dots,G_{\\pmb\\theta}^{(n)}(\\pmb\\delta)\\right)$ . Moreover, for any $i\\in[n]$ , define ", "page_idx": 5}, {"type": "equation", "text": "$$\nH_{\\pmb{\\theta}_{i},\\pmb{\\theta}_{-i}}^{(i)}(\\pmb{\\delta}):=\\nabla_{\\pmb{u}_{i}}\\mathbb{E}_{\\pmb{\\xi}_{i}\\sim\\mathcal{D}_{i}(\\pmb{u}_{i},\\pmb{\\theta}_{-i})}\\left[J_{i}\\left(\\pmb{\\xi}_{i};\\pmb{\\delta}\\right)\\right]\\rvert_{\\pmb{u}_{i}=\\pmb{\\theta}_{i}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and $H_{\\boldsymbol\\theta}(\\boldsymbol\\delta)\\,:=\\,\\operatorname{col}\\left(H_{\\boldsymbol\\theta_{1},\\boldsymbol\\theta_{-1}}^{(1)}(\\boldsymbol\\delta),\\dots,H_{\\boldsymbol\\theta_{n},\\boldsymbol\\theta_{-n}}^{(n)}(\\boldsymbol\\delta)\\right)$ Then, forary $i~\\in~[n]$ performative risk $\\mathrm{PR}_{i}(\\pmb{\\theta}_{i},\\pmb{\\theta}_{-i})$ w.r.t. $\\theta_{i}$ is given by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\nabla_{\\pmb{\\theta}_{i}}\\mathrm{PR}_{i}(\\pmb{\\theta}_{i},\\pmb{\\theta}_{-i})=G_{\\pmb{\\theta}_{i},\\pmb{\\theta}_{-i}}^{(i)}(\\pmb{\\theta}_{i},\\pmb{\\theta}_{-i})+H_{\\pmb{\\theta}_{i},\\pmb{\\theta}_{-i}}^{(i)}(\\pmb{\\theta}_{i},\\pmb{\\theta}_{-i}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Define $\\nabla\\mathrm{PR}(\\pmb\\theta):=\\mathrm{col}\\left(\\nabla_{\\pmb\\theta_{1}}\\mathrm{PR}_{i}(\\pmb\\theta),\\cdot\\cdot\\cdot\\right.,\\nabla_{\\pmb\\theta_{n}}\\mathrm{PR}_{n}(\\pmb\\theta)\\right)$ , we further have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\nabla\\mathrm{PR}(\\pmb\\theta)=G_{\\pmb\\theta}(\\pmb\\theta)+H_{\\pmb\\theta}(\\pmb\\theta).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "From Facchinei and Pang (2003, Theorem 2.3.3(b), to prove the $\\mathrm{E}\\&\\mathrm{U}$ of the NE of the (1), we require the strongly monotonivity of the gradient mapping ${\\boldsymbol\\nabla}\\mathrm{PR}(\\pmb\\theta)$ . Therefore, we have the following sufficient condition for the $\\mathrm{E}\\&\\mathrm{U}$ of the NE of the game (1). ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.4. Suppose that Assumptions 2.1-2.5 hold. If it is satisfied that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu-\\sum_{i=1}^{n}L_{i}\\varepsilon_{i}\\operatorname*{max}_{j\\in[n]}\\sqrt{p_{i j}}-\\sqrt{\\sum_{i=1}^{n}L_{i}^{2}\\varepsilon_{i}^{2}p_{i i}}>0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "then,the $P P$ -game (1) is strongly monotone and admits a unique NE. ", "page_idx": 5}, {"type": "text", "text": "The proof of Theorem 3.4 is presented in Appendix C. Since $p_{i j}$ characterizes the influence of player $j$ 's decision on the data distribution of player $i$ , we typically have $p_{i j}\\,\\leq\\,p_{i i}$ for $j\\neq i$ and thus $\\mathrm{max}_{j\\,\\in[n]}\\,p_{i j}\\,=\\,p_{i i}$ for all $i\\,\\in\\,[n]$ . Then, the condition (4) reduces to $\\begin{array}{r}{\\mu-\\sum_{i=1}^{n}L_{i}\\varepsilon_{i}p_{i i}\\mathrm{~-~}}\\end{array}$ $\\sqrt{\\textstyle\\sum_{i=1}^{n}L_{i}^{2}\\varepsilon_{i}^{2}p_{i i}}>0$ Similaly when $L_{1}=\\cdot\\cdot\\cdot=L_{n}=L$ $\\varepsilon_{1}=\\cdots=\\varepsilon_{n}=\\varepsilon$ and $\\begin{array}{r}{p_{i j}=\\frac{1}{n}}\\end{array}$ for all $i,j\\in[n]$ , we require that $\\mu-2L\\varepsilon>0$ , i.e., $\\varepsilon\\leq\\frac{\\mu}{2L}$ , which recovers the condition to guarantee the convexity of the performative risk $\\mathrm{PR}(\\cdot)$ , and thereby the $\\mathrm{E}\\&\\mathrm{U}$ of the performative optimal point of (Miller et al., 2021) for single-agent PP. ", "page_idx": 5}, {"type": "text", "text": "3.3  Distance Between PSE and NE ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Theorem  3.5. Define $\\begin{array}{r l r l r l r}{\\widetilde{\\mu}}&{{}}&{:=}&{{}}&{\\mu}&{{}-}&{\\sum_{i=1}^{n}L_{i}\\varepsilon_{i}\\operatorname*{max}_{j\\in[n]}\\sqrt{p_{i j}}}\\end{array}$ and $\\alpha\\qquad\\qquad:=$ $\\begin{array}{r}{\\sum_{i=1}^{n}G_{i}\\left(1+\\varepsilon_{i}\\operatorname*{max}_{j\\in[n]}\\sqrt{p_{i j}}\\right)}\\end{array}$ Suppose that Assumptions 2.1-25 hold and $\\smash{\\widetilde{\\mu}}\\ >\\ 0$ Then, for every $P S E$ point andNEpoint,wehavethefollowingrelations: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\theta^{\\mathrm{pse}}-\\theta^{\\mathrm{ne}}\\|_{2}\\leq\\frac{1}{\\tilde{\\mu}}\\sqrt{\\sum_{i=1}^{n}G_{i}^{2}\\varepsilon_{i}^{2}p_{i i}}\\quad a n d\\quad|\\mathrm{PR}(\\theta^{\\mathrm{pse}})-\\mathrm{PR}(\\theta^{\\mathrm{ne}})|\\leq\\frac{\\alpha}{\\tilde{\\mu}}\\sqrt{\\sum_{i=1}^{n}G_{i}^{2}\\varepsilon_{i}^{2}p_{i i}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The proof of Theorem 3.5 is presented in Appendix D. According to Theorem 3.5, the distance between the PSE and NE of the game (1) depends on the cost functions\u2019 parameters $\\mu,\\,\\{G_{i}\\},\\,\\{L_{i}\\}$ as well as the sensitivity of the data distributions $\\{\\varepsilon_{i}\\}$ . Larger sensitivity parameters widen the gap between the PSE and NE, while a bigger monotonicity parameter $\\mu$ reduces it. Notably, when the sensitivity parameter $\\varepsilon_{i}=0$ for all $i\\in[n]$ , the game (1) reduces to a conventional stochastic game with fixed data distributions, and as a result, the PSE and NE converge to the same point. ", "page_idx": 5}, {"type": "text", "text": "To the best of our knowledge, this is the first result on the distance between PSE and NE of PP-games. Characterizing this distance is challenging in games due to the lack of strong convexity on the joint cost function $J(\\cdot)$ , which is an essential property for determining the optimality gap of performative stable points in previous work (Perdomo et al., 2020; Lu, 2023). In this paper, we characterize this gap by leveraging relations from strong duality (Boyd and Vandenberghe, 2004; Facchinei and Pang, 2010). Our result is comparable to the findings in (Perdomo et al., 2020) for single-agent PP problems wheri this optimality gapis boundedby $\\frac{2L\\varepsilon}{\\mu}$ Im ourcase, when $G_{1}=\\cdot\\cdot\\cdot=G_{n}=G$ $\\varepsilon_{1}=\\cdot\\cdot\\cdot=\\varepsilon_{n}=\\varepsilon$ $\\begin{array}{r}{p_{i j}=\\frac{1}{n}}\\end{array}$ foral $i,j\\in[n]$ we have $\\begin{array}{r}{\\|\\pmb{\\theta}^{\\mathrm{{pse}}}-\\pmb{\\theta}^{\\mathrm{{ne}}}\\|_{2}\\leq\\frac{G\\varepsilon}{\\mu-L\\varepsilon}}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "1: Initialize $\\theta_{i}^{1}\\in\\Xi_{i}$ arbitrarily Set $\\lambda_{i}^{1}=\\mathbf{0}$ and $\\widehat{\\pmb{\\theta}}_{i h}^{1}={\\bf0}$ for all $h\\neq i$   \n2: for $t=1$ to $T$ do   \n3:Exchange $\\pmb{\\theta}_{i}^{t}$ \uff0c $\\widehat{\\pmb{\\theta}}_{i}^{t}$ , and $\\lambda_{i}^{t}$ with all neighbors;   \n4: Update the estimate $\\widehat{\\pmb{\\theta}}_{i h}^{t}$ for ll $h\\neq i$ by: $\\begin{array}{r}{\\widehat{\\pmb{\\theta}}_{i h}^{t+1}=\\sum_{k\\neq h}a_{i k}\\widehat{\\pmb{\\theta}}_{k h}^{t}+a_{i h}\\pmb{\\theta}_{h}^{t}}\\end{array}$   \n5: Deploy the model $\\theta_{i}^{t}$ and sample $\\pmb{\\xi}_{i}^{t}\\sim\\mathcal{D}_{i}(\\pmb{\\theta}_{i}^{t},\\pmb{\\theta}_{-i}^{t})$   \n6: Udate he primal variableby: $\\pmb{\\theta}_{i}^{t+1}=P_{\\Omega_{i}}\\left[\\pmb{\\theta}_{i}^{t}-\\gamma_{t}\\left(\\nabla_{\\pmb{\\theta}_{i}}J_{i}\\left(\\pmb{\\xi}_{i}^{t};\\pmb{\\theta}_{i}^{t},\\widehat{\\pmb{\\theta}}_{i}^{t}\\right)+\\gamma_{t}\\nabla\\pmb{g}_{i}(\\pmb{\\theta}_{i}^{t})^{\\top}\\pmb{\\lambda}_{i}^{t}\\right)\\right]$   \n7: Update the dual ariableby: $\\begin{array}{r}{\\pmb{\\lambda}_{i}^{t+1}=\\left[\\left(1-\\gamma_{t}^{2}\\right)\\sum_{j\\in\\mathcal{N}_{i}}a_{i j}\\pmb{\\lambda}_{j}^{t}+\\gamma_{t}\\pmb{g}_{i}\\left(\\pmb{\\theta}_{i}^{t}\\right)\\right]_{+}}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "4  Computation of the PSE ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Although RRM theoretically has the capability to find a PSE point, how to perform risk minimization at its each update remains unknown. Moreover, RRM requires the computation of an NE for each deployment, which is computationally intensive. In this section, we present a decentralized stochastic primal-dual algorithm for effciently computing the PSE of the game (1). Theoretical analysis is also provided on the convergence of the proposed algorithm. ", "page_idx": 6}, {"type": "text", "text": "4.1  Algorithm Development ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "For each player $i\\in[n]$ , define a regularized Lagrangian as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\delta}^{(i)}(\\theta_{i},\\theta_{-i},\\lambda)=\\mathbb{E}_{\\xi_{i}\\sim\\mathcal{D}_{i}(\\delta)}J_{i}\\left(\\xi_{i};\\theta_{i},\\theta_{-i}\\right)+\\left<\\lambda,g_{i}(\\theta_{i})+\\sum_{j\\neq i}g_{j}\\left(\\theta_{j}\\right)\\right>,}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\pmb{\\lambda}\\in\\mathbb{R}_{+}^{m}$ is the dual variable. Denote by $\\nabla g_{i}(\\cdot)$ the Jacobian matrix of $g_{i}(\\cdot)$ . From the primal-dual theory (Boyd and Vandenberghe, 2004; Facchinei and Pang, 2010), for any $\\gamma>0$ ,there exists a bounded Lagrangian multiplier $\\bar{\\lambda^{\\mathrm{pse}}}$ such that the following condition holds: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta_{i}^{\\mathrm{pse}}=\\!P_{\\Omega_{i}}\\left[\\theta_{i}^{\\mathrm{pse}}-\\gamma\\left(G_{\\theta^{\\mathrm{pse}}}^{(i)}\\left(\\theta^{\\mathrm{pse}},\\lambda^{\\mathrm{pse}}\\right)+\\gamma\\nabla g_{i}(\\theta_{i}^{\\mathrm{pse}})^{\\top}\\lambda^{\\mathrm{pse}}\\right)\\right],\\quad\\forall i\\in[n],}\\\\ &{\\lambda^{\\mathrm{pse}}=\\left[\\lambda^{\\mathrm{pse}}+\\gamma\\left(g_{i}(\\theta_{i}^{\\mathrm{pse}})+\\sum_{j\\neq i}g_{j}\\left(\\theta_{j}^{\\mathrm{pse}}\\right)\\right)\\right]_{+},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\gamma$ is a control parameter. Thus, given $\\pmb{\\theta}_{-i}^{\\mathrm{pse}}$ and under $\\pmb{\\xi}_{i}\\sim\\mathcal{D}_{i}(\\pmb{\\theta}^{\\mathrm{pse}}),(\\pmb{\\theta}_{i}^{\\mathrm{pse}},\\pmb{\\lambda}^{\\mathrm{pse}})$ is a saddle pointo the Lagrangian $\\mathcal{L}_{\\theta^{\\mathrm{pse}}}^{(i)}(\\pmb{\\theta}_{i},\\pmb{\\theta}_{-i}^{\\mathrm{pse}},\\lambda)$ forany $i\\in[n]$ The joint sadl point $(\\theta^{\\mathrm{pse}},\\lambda^{\\mathrm{pse}})$ achieve the PSE of the game (1) under strong duality (Boyd and Vandenberghe, 2004). ", "page_idx": 6}, {"type": "text", "text": "In the decentralized noncooperative game (1), each player can only communicate with its neighbors. We use $\\mathcal{G}(\\mathbf{A})$ to denote the communication graph of the network, where $\\mathbf{A}=(a_{i j})_{n\\times n}$ represents a weight matrix. In $\\mathcal{G}(\\mathbf{A})$ \uff0c $a_{i j}=a_{j i}>0$ if there is a communication link between player $i$ and play $j$ and $a_{i j}=a_{j i}=0$ otherwise. Let ${\\mathcal N}_{i}$ be the set containing player $i$ and all its neighbors such that $j\\in\\breve{\\mathscr{N}}_{i}$ if $a_{i j}>0$ . We assume that the communication graph $\\mathscr{G}(\\mathbf{A})$ is connected and the weight matrix A is doubly stochastic. ", "page_idx": 6}, {"type": "text", "text": "To find the saddle point $(\\pmb{\\theta}^{\\mathrm{pse}},\\pmb{\\lambda}^{\\mathrm{pse}})$ , we develop a decentralized stochastic primal-dual algorithm, as presented in Algorithm 1. The basic idea of Algorithm 1 is to perform gradient update on the primal variables $\\theta_{i}$ for all $i\\in[n]$ and the dual variable $\\lambda$ . In the decentralized noncooperative game, each player $i\\in[n]$ only observes information from its neighbors. However, its private cost funtion $J_{i}(\\xi_{i};\\pmb{\\theta}_{i},\\pmb{\\theta}_{-i})$ involves all players\u2032 strategies. To solve this problem, we let each player $i$ store an estimate for the strategies of all the other players, denoted by $\\widehat{\\pmb{\\theta}}_{i h}$ , for all $h\\neq i$ . Define a vector $\\widehat{\\pmb{\\theta}}_{i}$ that concatenates all the estimates $\\widehat{\\pmb{\\theta}}_{i h}$ . In each iteration $t$ , neighbors exchange strategy $\\pmb{\\theta}_{i}^{t}$ , estimate ${\\widehat{\\pmb{\\theta}}}_{i}^{t}$ and dual varible $\\lambda_{i}^{t}$ with each other. Then, each player $i$ updates the estimates $\\widehat{\\pmb{\\theta}}_{i h}$ , for all $h\\neq i$ by weighted average in Step 4. The primal variable $\\pmb{\\theta}_{i}^{t}$ is updated by gradient descent by Step 6, and the dual variable $\\lambda_{i}^{t}$ is updated by gradient ascent by Step 7. The coefficient $\\gamma_{t}$ is the stepsize at the tth iteration for all $\\dot{t}\\in[T]$ ", "page_idx": 6}, {"type": "text", "text": "4.2 Performance Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Before analyzing the performance of Algorithm 1, we define the performance metrics adopted in this paper. The first metric is performative regret. For any player $i\\in[n]$ , its performative regret over $T$ iterations is defined as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{R}_{i}(T)\\!:=\\!\\sum_{t=1}^{T}\\left(\\mathbb{E}_{\\xi_{i}\\sim\\mathcal{D}_{i}(\\theta^{\\mathrm{pse}})}J_{i}\\left(\\xi_{i};\\theta_{i}^{t},\\theta_{-i}^{\\mathrm{pse}}\\right)-\\mathrm{PR}_{i}\\left(\\theta^{\\mathrm{pse}}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The regret $\\mathcal{R}_{i}(T)$ measures the suboptimality of the sequence of decisions $\\{\\pmb\\theta_{i}^{1},\\cdot\\cdot\\cdot,\\pmb\\theta_{i}^{T}\\}$ takenby play $i$ relative to $\\pmb{\\theta}_{i}^{\\mathrm{pse}}$ . Besides, since the decisions of all players are subject to constraints, another performance metric of constraint violation, denoted by $\\mathcal{R}_{g}(T)$ , is required, defined as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{R}_{g}(T)=\\left\\|\\left[\\sum_{t=1}^{T}\\sum_{i=1}^{n}\\pmb{g}_{i}\\left(\\pmb{\\theta}_{i}^{t}\\right)\\right]_{+}\\right\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Any online or learning algorithm is regarded as \u201cgood\" if both the time-average regret and the time-average constraint violation are sublinear, i.e., $\\begin{array}{r}{\\operatorname*{lim}_{T\\rightarrow\\infty}\\mathcal{R}_{i}(T)/T\\leq o(1)}\\end{array}$ forany $i\\in[n]$ and $\\begin{array}{r}{\\operatorname*{lim}_{T\\to\\infty}\\bar{\\mathcal{R}_{g}}(T)/T\\le o(1)}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "For analysis, we make the following assumption on the variance of the stochastic gradient $\\nabla_{\\pmb{\\theta}_{i}}J_{i}\\,(\\pmb{\\xi}_{i};\\pmb{\\delta}),\\forall i\\in[n]$ ", "page_idx": 7}, {"type": "text", "text": "Assumption 4.1. The stochastic gradient $\\nabla_{\\delta_{i}}J_{i}\\left(\\pmb{\\xi}_{i};\\pmb{\\delta}_{i},\\pmb{\\delta}_{-i}\\right)$ is unbiased  that $\\overline{{\\mathbb{E}_{\\pmb{\\xi}_{i}\\sim\\mathcal{D}_{i}}}}(\\pmb{\\theta})\\nabla_{\\pmb{\\delta}_{i}}J_{i}\\left(\\pmb{\\xi}_{i};\\pmb{\\delta}_{i},\\pmb{\\delta}_{-i}\\right)\\;\\;=\\;\\;G_{\\pmb{\\theta}}^{(i)}\\left(\\pmb{\\delta}_{i},\\pmb{\\delta}_{-i}\\right)$ and there exist constants $\\sigma_{0},\\sigma_{1}\\mathrm{~\\,~\\!~\\geq~\\,~0~}$ such that $\\begin{array}{r}{\\sum_{i=1}^{n}\\mathbb{E}_{\\xi_{i}\\sim\\mathcal{D}_{i}(\\theta)}\\left\\|\\nabla_{\\delta_{i}}J_{i}\\left(\\xi_{i};\\delta_{i},\\delta_{-i}\\right)-G_{\\theta}^{(i)}\\left(\\delta_{i},\\delta_{-i}\\right)\\right\\|_{2}^{2}\\leq\\sigma_{0}^{2}+\\sigma_{1}^{2}\\left\\|\\theta-\\theta^{\\mathrm{pse}}\\right\\|_{2}^{2},\\forall\\theta,\\delta\\in\\Omega.}\\end{array}$ Theorem 4.2.  Define $\\begin{array}{r l r l r l r}{\\widetilde{\\mu}}&{{}}&{:=}&{{}}&{\\mu}&{{}-}&{\\sum_{i=1}^{n}L_{i}\\varepsilon_{i}\\operatorname*{max}_{j\\in[n]}\\sqrt{p_{i j}}}\\end{array}$ and . $\\nu\\quad\\quad\\quad:=$ $\\begin{array}{r}{3\\left(\\sigma_{1}^{2}+3\\sum_{i=1}^{n}L_{i}^{2}\\left(1+\\varepsilon_{i}^{2}\\operatorname*{max}_{j\\in[n]}p_{i j}\\right)\\right)}\\end{array}$ .Suppose that Assumptions 2.1-2.5 and 4.1 hold and $\\widetilde{\\mu}>0$ By Algorithm $^{\\,l}$ if the stepsize satisfies $\\begin{array}{r}{\\operatorname*{sup}_{t\\in[T]}\\gamma_{t}\\leq\\frac{\\widetilde\\mu}{\\nu}}\\end{array}$ then, the performative regret of the game (1) is bounded by ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{R}_{i}(T)\\leq\\mathcal{O}\\left(\\sqrt{\\frac{T}{\\tilde{\\mu}}\\left(\\frac{1}{\\gamma T}+\\sum_{t=1}^{T}\\gamma_{t}\\right)}\\right),\\forall i\\in[n].}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Further, the constraint violation is bounded by ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{R}_{g}(T)\\leq\\mathcal{O}\\left(\\frac{1}{\\gamma T}\\sqrt{\\left(\\frac{1}{\\gamma T}+\\sum_{t=1}^{T}\\gamma_{t}\\right)\\left(1+\\sum_{t=1}^{T}\\gamma_{t}^{2}\\right)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "For a sequence of diminishing stepsize $\\gamma_{t}=\\tau_{1}^{\\eta}(\\tau_{2}t+\\tau_{1})^{-\\eta}$ , where $\\tau_{1},\\tau_{2}>0$ and $0<\\eta<1$ we have that: 1) $\\begin{array}{r}{\\sum_{t=1}^{T}\\gamma_{t}\\le\\mathcal{O}\\left(T^{1-\\eta}\\right);2)\\sum_{t=1}^{T}\\gamma^{2}(t)\\le\\mathcal{O}\\left(T^{1-2\\eta}\\right)}\\end{array}$ Plugging the above results into Theorem 4.2 yields ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{R}_{i}(T)\\le\\mathcal{O}\\left(T^{\\frac{1+\\eta}{2}}+T^{1-\\frac{\\eta}{2}}\\right),i\\in[n]\\quad\\mathrm{and}\\quad\\mathcal{R}_{g}(T)\\le\\mathcal{O}\\left(T^{\\frac{3}{2}\\eta}+T^{\\frac{1+\\eta}{2}}+T^{1-\\frac{\\eta}{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Based on the above two inequalities, the best choice of $\\eta$ is $\\frac{1}{2}$ such that $\\mathcal{R}_{i}(T)\\leq\\mathcal{O}(T^{\\frac{3}{4}}),\\forall i\\in[n]$ and $\\mathcal{R}_{g}(T)\\leq\\mathcal{O}(T^{\\frac{3}{4}})$ . This convergence speed matches that of the decentralized noncooperative game without performativity (Lu et al., 2020). ", "page_idx": 7}, {"type": "text", "text": "The proof of Theorem 4.2 is provided in Appendix E. According to Theorem 4.2, the performative effect reduces the convergence rate by amplifying the coefficient $\\frac{1}{\\widetilde{\\mu}}$ in the regret bounds. Specifically, as the sensitivity parameters $\\varepsilon_{i}$ increase, the coefficient $\\widetilde{\\mu}$ decreases, leading to a slower convergence rate of $\\mathcal{R}_{i}(T)$ for all $i\\,\\in\\,[n]$ . This occurs because a larger $\\varepsilon_{i}$ indicates a stronger performative influence, which more significantly impacts the algorithm's convergence. Nevertheless, the performative effect does not degrade the convergence order of Algorithm 1 compared to the case without performativity (Lu et al., 2020). ", "page_idx": 7}, {"type": "text", "text": "5 Numerical Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we evaluate the effectiveness of our algorithm and theoretical results by conducting numerical experiments on a networked Cournot game (Abolhassani et al., 2014), which is a foundational model in economic theory (Allaz and Vila, 1993) for analyzing oligopolistic competitions. We consider a networked Cournot game with five firms selling a single commodity across three markets. Each firm aims to maximize its profit by determining the quantities it serves in all markets. The total accommodated quantity in each market is limited by its market capacity. The simulation details and additional numerical results are presented in Appendix F.1. We also provide an additional experiment on a ride-share market in Appendix F.2. ", "page_idx": 7}, {"type": "image", "img_path": "KqgSzXbufw/tmp/742b4720c582e5f406e5120bc203fb695989d611bee1051376512c505ec3227f.jpg", "img_caption": ["Figure 1: Convergence of time-average regrets and time-average constraint violations. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "KqgSzXbufw/tmp/dc525f9cbe44b13ee77323ce99155f476e755c2500e2e4e7ff65d7a4724b8d95.jpg", "img_caption": ["Figure 2: (a). Normalized distance between $\\pmb{\\theta}^{t}$ and $\\pmb{\\theta}^{\\mathrm{ne}}$ . (b). Total revenue at PSE and NE. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Fig. 1 illustrates the convergence of the time-average regrets of five firms, denoted by $\\mathcal{R}_{i}(t)/t$ $\\forall i\\ \\in$ [5], and the convergence of the time-average constraint violations of three markets, denotedby $\\begin{array}{r}{\\frac{1}{t}\\sum_{t^{\\prime}=1}^{t}\\sum_{i=1}^{n}g_{i j}(\\pmb{\\theta}_{i}^{t^{\\prime}}),}\\end{array}$ $\\forall j\\ \\in$ [3]. The results demonstrate that both $\\mathcal{R}_{i}(t)/t$ and $\\begin{array}{r}{\\frac{1}{t}\\sum_{t^{\\prime}=1}^{t}\\sum_{i=1}^{n}g_{i j}(\\pmb{\\theta}_{i}^{t^{\\prime}})}\\end{array}$ approach zero as the ieraions incrase. This erfes the subinear convergence of the regrets and constraint violations in Theorem 4.2. ", "page_idx": 8}, {"type": "text", "text": "Fig. 2 (a) compares the normalized distance between $\\pmb{\\theta}^{t}$ , generated by Algorithm 1, and the NE point $\\pmb{\\theta}^{\\mathrm{ne}}$ , denoted as $\\lVert{\\boldsymbol{\\theta}}^{t}-{\\boldsymbol{\\theta}}^{\\mathrm{ne}}\\rVert_{2}/\\lVert{\\boldsymbol{\\theta}}^{t}\\rVert_{2}$ . The NE point is computed based on perfect knowledge of $\\{{\\mathcal{D}}_{i}\\}_{i\\in[n]}$ .We consider three different performative strengths: $\\varepsilon\\;=\\;0.2,\\;0.4.$ , and 0.6. It is observed that $\\lVert{\\boldsymbol{\\theta}}^{t}-{\\boldsymbol{\\theta}}^{\\mathrm{ne}}\\rVert_{2}/\\lVert{\\boldsymbol{\\theta}}^{t}\\rVert_{2}$ stabilizes at values approximately equal to or smaller than $10^{-1}$ with iterations, varifying the effectiveness of Algorithm 1. Additionally, a larger performative strength leads to a wider normalized distance between the convergent point of $\\pmb{\\theta}^{\\tilde{t}}$ and $\\pmb{\\theta}^{\\mathrm{ne}}$ . In Fig. 2 (b), wempatrt $-\\textstyle\\sum_{i=1}^{5}\\operatorname{PR}_{i}({\\bar{\\pmb{\\theta}}}^{t})$ under the same three $\\varepsilon$ setings. We consider two scenarios: 1). \u201cpse\", where $\\pmb{\\theta}^{t}$ is generated by Algorithm 1; 2). \u201cne\", where $\\pmb{\\theta}^{t}$ is generated by performing the same procedures as Algorithm 1 but with perfect information on the distributions $\\{{\\bar{D}}_{i}(\\pmb\\theta)\\}_{i\\in[n]}$ . The result demonstrates the close performance of the \u201cpse\u201d approach and the \u201cne\u201d approach. More numerical results can be found in Appendix F. ", "page_idx": 8}, {"type": "text", "text": "Conclusions: We have studied the performative phenomenon in a decentralized noncooperative game where selfish players seek to maximize their individual profits while adhering to coupled inequality constraints. We have derived sufficient conditions for the E&U of both PSE and NE and provided the first upper bound on the distance between these two equilibria. Furthermore, we have developed a decentralized stochastic primal-dual algorithm for efficiently computing of the PSE point. Theoretical analysis has demonstrated the same order of convergence speed of our algorithm as the case without performativity. Finally, numerical simulations have been provided to verify the effectiveness of our algorithm and theoretical results. ", "page_idx": 8}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Melika Abolhassani, Mohammad Hossein Bateni, MohammadTaghi Hajiaghayi, Hamid Mahini, and Anshul Sawant. 2014. Network cournot competition. In International Conference on Web and Internet Economics. Springer,15-29.   \nBlaise Allaz and Jean-Luc Vila. 1993. Cournot competition, forward markets and efciency. Journal of Economic theory 59, 1 (1993), 1-16.   \nDimitri P Bertsekas. 2014. Constrained optimization and Lagrange multiplier methods. Academic press.   \nStephen P Boyd and Lieven Vandenberghe. 2004. Convex optimization. Cambridge university press.   \nAlex Chan, Ahmed Alaa, Zhaozhi Qian, and Mihaela Van Der Schar. 2020. Unlabelled data improves bayesian uncertainty calibration under covariate shift. In International conference on machine learning. PMLR, 1392-1402.   \nJames Cust and Steven Poelhekke. 2015. The local economic impacts of natural resource extraction. Annu. Rev. Resour. Econ. 7, 1 (2015), 251-268.   \nSarah Dean, Mihaela Curmei, Lillian J. Ratliff, Jamie Morgenstern, and Maryam Fazel. 2023. Emergent segmentation from participation dynamics and multi-learner retraining. arXiv preprint arXiv:2206.02667 (2023).   \nJinshuo Dong, Aaron Roth, Zachary Schutman, Bo Waggoner, and Zhiwei Steven Wu. 208. Strategic classification from revealed preferences. In Proceedings of the 2018 ACM Conference on Economics and Computation. 55-70.   \nDmitriy Drusvyatskiy and Lin Xiao. 2023. Stochastic optimization with decision-dependent distributions. Mathematics of Operations Research 48,2 (2023),954-998.   \nIssam El Naqa and Martin J Murphy. 2015. What is machine learning? Springer.   \nFrancisco Facchinei and Jong-Shi Pang. 2003. Finite-dimensional variational inequalities and complementarity problems. Springer.   \nFranciscoFacchinei and Jong-Shi Pang. 2010.Nash equilibria: the ariational approach. Convex optimization n signal processing and communications (2010), 443.   \nBassam Fattouh and Lavan Mahadeva. 2014. Causes and implications of shifts in financial participation in commodity markets. Journal of Futures Markets 34, 8 (2014), 757-787.   \nYiguang Hong, Jiangping Hu, and Linxin Gao. 2006. Tracking control for multi-agent consensus with an active leader and variable topology. Automatica 42, 7 (2006), 1177-1182.   \nRoger A Horn and Charles R Johnson. 2012. Matrix analysis. Cambridge university press.   \nZachary Izzo, Lexing Ying, and James Zou. 2021. How to learn when data reacts to your model: performative gradient descent. In International Conference on Machine Learning. PMLR, 4641-4650.   \nMeena Jagadeesan, Tijana Zrnic, and Celestine Mender-Dunner. 2022. Regret minimization with performative feedback. In International Conference on Machine Learning. PMLR, 9760-9785.   \nQiang Li, Chng-Yu Yau, and HoToWai 2022.Multi-agent perfrmative prdition with gredy dloyet and consensus seeking agents. Advances in Neural Information Processing Systems 35 (2022), 38449-38460.   \nJieLu,njnLi,FanDong,Feng Gu,JoaGama,andGuangquanZhang. 208Leainder onct dift: A review. IEEE transactions on knowledge and data engineering 31, 12 (2018), 2346-2363.   \nKaihong Lu, Guangqi Li, and Long Wang. 2020. Online distributed algorithms for seeking generalized Nash equilibria in dynamic environments. IEEE Trans. Automat. Control 66, 5 (2020), 2289-2296.   \nSongtao Lu. 2023. Bilevel optimization with coupled decision-dependent distributions. In International Conference on Machine Learning. PMLR, 22758-22789.   \nDebmalya Mandal, Stelios Triantafyllou, and Goran Radanovic. 2023. Performative reinforcement learning. In International Conference on Machine Learning. PMLR, 23642-23680.   \nJohn P Miller, Juan C Perdomo, and Tijana Zrnic. 2021. Outside the echo chamber: Optimizing the performative risk. In International Conference on Machine Learning. PMLR, 7710-7720.   \nUsue Mori, Alexander Mendiburu, Maite Alvarez, and Jose A Lozano. 2015. A review of travel time estimation and forecasting for advanced traveller information systems. Transportmetrica A: Transport Science 11, 2 (2015), 119-157.   \nAmir Moshari, GR Yousefi, Akbar Ebrahimi, and Saeid Haghbin. 2010. Demand-side behavior in the smart grid environment. In 2010 IEEE PES Innovative Smart Grid Technologies Conference Europe (ISGT Europe). IEEE,1-7.   \nAdhyyan Narang, Evan Faulkner, Dmitriy Drusvyatskiy, Maryam Fazel, and Lillian J Ratliff. 2023. Multiplayer performative prediction: Learning in decision-dependent games. Journal of Machine Learning Research 24, 202 (2023), 1-56.   \nJuan Perdomo, Tijana Zrnic, Celestine Mendler-Dunner, and Moritz Hardt. 2020. Performative prediction. In Proceedings of the 37th International Conference on Machine Learning (ICML 2020). PMLR, 7599-7609.   \nGeorgios Piliouras and Fang-Yi Yu. 2023. Multi-agent performative prediction: From global stability and optimality to chaos. In Proceedings of the 24th ACM Conference on Economics and Computation. 1047- 1074.   \nJoaquin Quinonero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. 2008. Dataset shift in machine learning. Mit Press.   \nJia-Wei Shan, Peng Zhao, and Zhi-Hua Zhou. 2023. Beyond Performative Prediction: Open-environment Learning with Presence of Corruptions. In International Conference on Artificial Intelligence and Statistics. PMLR, 7981-7998.   \nHal R Varian. 2009. Online ad auctions. American Economic Review 99, 2 (2009), 430-434.   \nXiaolu Wang, Chung- Yiu Yau, and Hoi To Wai. 2023. Network effects in performative prediction games. In International Conference on MachineLearning.PMLR,36514-36540.   \nKillian Wood, Gianluca Bianchin, and Emiliano Dall' Anese. 2021. Online projected gradient descent for stochastic optimization with decision-dependent distributions. IEEE Control Systems Letters 6 (2021), 1646-1651.   \nRuihan Wu, Chuan Guo, Yi Su, and Kilian Q Weinberger. 2021. Online adaptation to label distribution shift. Advances in Neural Information Processing Systems 34 (2021), 11340-11351.   \nWenjing Yan and Xuanyu Cao. 2024a. Decentralized Multi-Task Online Convex Optimization Under Random Link Failures. IEEE Transactions on Signal Processing (2024).   \nWenjing Yan and Xuanyu Cao. 2024b. Zero-regret performative prediction under inequality constraints. Advances in Neural InformationProcessingSystems 36(2024).   \nZhi-Hua Zhou. 2022. Open-environment machine learning. National Science Review 9, 8 (2022), nwac123. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "A Related Work ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "In recent years, the exploration of distribution shifts in machine learning systems has been extended beyond traditional exogenous shifts (Quinonero-Candela et al., 2008), such as covariate (Chan et al., 2020), label (Wu et al., 2021), and concept (Lu et al., 2018) drifts, to include endogenous shifts resulting from strategic behaviors within the learning platforms themselves. Perdomo et al. (2020) introduced the framework of performative prediction, which captures the platform's strategic responses using decision-dependent distribution mappings. Following this seminal work, significant research effort has been dedicated to investigating the phenomenon of performativity in various scenarios. In particular, (Shan et al., 2023) studied the endogenous distribution change in open environments, where data are obtained from a corrupted decision-dependent distribution. They proposed an effective algorithm with theoretical guarantees by decoupling the two sources of effects. Lu (2023) investigated the presence of performativity in bilevel optimization. They first established sufficient conditions for the existence of performatively stable solutions and then developed a stochastic algorithm to find the PS point. In (Mandal et al., 2023), the authors examined the performative effect in a regularized reinforcement learning problem and showed that repeatedly optimizing this objective converges to a performatively stable policy under reasonable assumptions on the transition dynamics. It is demonstrated in (Drusvyatskiy and Xiao, 2023) that typical gradientbased stochastic algorithms can be applied to find performative stable equilibria with a biased gradient Oracle. ", "page_idx": 11}, {"type": "text", "text": "While most existing work focused on finding performative stable points, there are studies aimed at identifying the optimal solutions for performative prediction problems (Miller et al., 2021; Izzo et al., 2021; Jagadeesan et al., 2022). The optimality gap of performative stable points was first presented in (Perdomo et al., 2020), where their bound is proportional to the strong convexity parameter and inversely proportional to the smoothness parameter of cost functions and the sensitivity parameter of the decision-dependent distributions. The primary challenges in computing optimal points in performative prediction problems lie in the unknown decision-dependent data distributions. To address this challenge, a commonly used method is to make parametric assumptions on the data distributions and then design algorithms to estimate them. For instance, (Miller et al., 2021) proposed a two-stage algorithm to find the performative optima for distribution maps in the location family. Izzo et al. (2021) proposed a PerfGD algorithm by exploiting the exponential structure of the underlying distribution maps. ", "page_idx": 11}, {"type": "text", "text": "Among the numerous existing studies, (Narang et al., 2023) and (Wang et al., 2023) are, at a conceptual level, the closest papers to our own since they have considered performative behaviors in games. On a technical level, however, these two works are quite distinct from ours since we study completely different problem settings. One defining distinction is that, in our model, the collective strategies of all players must adhere to the learning system's constraints, whereas both (Narang et al., 2023) and (Wang et al., 2023) are unconstrained. Constraints are unavoidable in certain game scenarios, such as safety and cost constraints in transportation, relevance and diversity constraints in advertising, and risk tolerance and portfolio constraints in financial trading. The constrained problem in our work results in a fundamentally different algorithm design and convergence analysis from these two papers. Our work utilizes the primal-dual technique and necessitates consensus, whereas their approach only requires local stochastic gradient descent. Additionally, there are distinctions in the problem settings. In (Wang et al., 2023), the private cost function of each player is structured in a specific form, involving a local cost depending solely on its own strategy and a regularizer quantifying the similarity of strategies among neighbors. In contrast, we consider a mathematically richer setting where each player's private cost function depends on the strategies of all players in the game, thus encompassing the model in (Wang et al., 2023). Moreover, our algorithm design takes into account the practical implementation where players can only communicate with their neighbors, while (Narang et al., 2023) assumes that the strategies of all players are publicly accessible across the entire network. This more practical setting poses challenges for each player in observing the entire network. More importantly, although (Narang et al., 2023) and (Wang et al., 2023) demonstrated the existence and uniqueness of the PSE and NE for their respective game settings, neither of them offers insights into the distance between these two equilibria. This paper makes a significant contribution by presenting the first upper bound on this distance. ", "page_idx": 11}, {"type": "text", "text": "Furthermore, there are works on decentralized optimization of multiagent performative prediction (Li et al., 2022; Piliouras and Yu, 2023). Specifically, (Li et al., 2022) focused on decentralized optimization with consensus-seeking agents, where the data distribution of each agent depends only on its own decision. Although (Piliouras and Yu, 2023) considers multiagent, their study is in a centralized fashion and their data distributions are restricted to location-scale families. Lastly, it is worth mentioning that one paper (Yan and Cao, 2024b) has considered constrained optimization in the context of performative prediction. However, (Yan and Cao, 2024b) studied the single-agent case, while this work considers a more complex model with decentralized noncooperative players and partially observed information about competitors\u2019 strategies. Additionally, this paper contributes to the evaluation of equilibria, whereas such analysis has not been involved in (Yan and Cao, 2024b). ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "B  Existence and Uniqueness of Performative Stable Equilibrium ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "From the definition of the mapping $\\mathcal{T}(\\pmb{\\theta})$ , we have that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\gamma_{i}^{\\prime}={\\mathcal{T}}_{i}(\\theta)=\\underset{u_{i}\\in\\Omega_{i}}{\\operatorname{arg\\,min}}\\ }&{\\mathbb{E}_{\\xi_{i}\\sim{\\mathcal{D}}_{i}(\\theta)}J_{i}\\left(\\xi_{i};u_{i},\\theta_{-i}^{\\prime}\\right)}&{\\mathrm{s.t.}\\ \\ g_{i}(u_{i})+\\displaystyle\\sum_{j\\neq i}g_{j}\\left(\\theta_{j}^{\\prime}\\right)\\leq0,\\quad\\forall i\\in[n],}\\\\ {\\xi_{i}^{\\prime}={\\mathcal{T}}_{i}(\\delta)=\\underset{u_{i}\\in\\Omega_{i}}{\\operatorname{arg\\,min}}\\ }&{\\mathbb{E}_{\\xi_{i}\\sim{\\mathcal{D}}_{i}(\\delta)}J_{i}\\left(\\xi_{i};u_{i},\\delta_{-i}^{\\prime}\\right)}&{\\mathrm{s.t.}\\ \\ g_{i}(u_{i})+\\displaystyle\\sum_{j\\neq i}g_{j}\\left(\\delta_{j}^{\\prime}\\right)\\leq0,\\quad\\forall i\\in[n].}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Define $\\mathbb{E}_{{\\pmb\\xi}_{i}\\sim\\mathcal{D}_{i}({\\pmb\\theta})}\\nabla_{{\\pmb\\theta}_{i}}J_{i}\\left({\\pmb\\xi}_{i};{\\pmb\\theta}_{i}^{\\prime},{\\pmb\\theta}_{-i}^{\\prime}\\right):=G_{{\\pmb\\theta}}^{(i)}({\\pmb\\theta}_{i}^{\\prime},{\\pmb\\theta}_{-i}^{\\prime})$ From the optimalitycondition of constrained optimization, we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\left\\langle G_{\\pmb{\\theta}}^{(i)}\\left(\\pmb{\\theta}^{\\prime}\\right),\\pmb{\\theta}_{i}^{\\prime}-\\delta_{i}^{\\prime}\\right\\rangle\\leq0,\\quad\\forall i\\in[n].\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Define a vector $G_{\\pmb\\theta}(\\pmb\\theta^{\\prime}):=\\mathrm{col}\\left(G_{\\pmb\\theta}^{(1)}(\\pmb\\theta^{\\prime}),\\cdot\\cdot\\cdot\\mathrm{\\boldmath~},G_{\\pmb\\theta}^{(n)}(\\pmb\\theta^{\\prime})\\right)$ that concatenates ll the $G_{\\pmb\\theta}^{(i)}(\\pmb\\theta^{\\prime}),i\\in[n].$ Then, we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\left\\langle G_{\\theta}\\left(\\theta^{\\prime}\\right),\\theta^{\\prime}-\\delta^{\\prime}\\right\\rangle\\leq0.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Similarly, we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\left\\langle G_{\\delta}\\left(\\delta^{\\prime}\\right),\\theta^{\\prime}-\\delta^{\\prime}\\right\\rangle\\geq0.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Further, from the monotoniticy of the gradient mapping $\\nabla J\\left(\\xi;\\theta\\right)$ in Assumption 2.1, we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\langle{G_{\\theta}(\\theta^{\\prime})-G_{\\theta}(\\delta^{\\prime}),\\theta^{\\prime}-\\delta^{\\prime}}\\right\\rangle=\\mathbb{E}_{\\xi\\sim\\mathcal{D}(\\theta)}\\left\\langle{\\nabla J\\left(\\xi;\\theta^{\\prime}\\right)-\\nabla J\\left(\\xi;\\delta^{\\prime}\\right),\\theta^{\\prime}-\\delta^{\\prime}}\\right\\rangle\\geq\\mu\\|\\theta^{\\prime}-\\delta^{\\prime}\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\mathcal{D}(\\pmb{\\theta}):=\\mathcal{D}_{1}(\\pmb{\\theta})\\times\\dots\\times\\mathcal{D}_{n}(\\pmb{\\theta})$ . Plugging (A1) and (A2) into (A3) gives ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu\\|\\pmb{\\theta}^{\\prime}-\\pmb{\\delta}^{\\prime}\\|_{2}^{2}\\leq\\left\\langle-G_{\\pmb{\\theta}}\\left(\\pmb{\\delta}^{\\prime}\\right),\\pmb{\\theta}^{\\prime}-\\pmb{\\delta}^{\\prime}\\right\\rangle}\\\\ &{\\qquad\\qquad\\leq\\left\\langle G_{\\pmb{\\delta}}\\left(\\pmb{\\delta}^{\\prime}\\right)-G_{\\pmb{\\theta}}\\left(\\pmb{\\delta}^{\\prime}\\right),\\pmb{\\theta}^{\\prime}-\\pmb{\\delta}^{\\prime}\\right\\rangle}\\\\ &{\\qquad\\qquad\\leq\\left\\|G_{\\pmb{\\delta}}\\left(\\pmb{\\delta}^{\\prime}\\right)-G_{\\pmb{\\theta}}\\left(\\pmb{\\delta}^{\\prime}\\right)\\right\\|_{2}\\left\\|\\pmb{\\theta}^{\\prime}-\\pmb{\\delta}^{\\prime}\\right\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "From Assumption 2.2, $\\begin{array}{r}{\\mathcal{W}_{1}\\left(\\mathcal{D}_{i}\\left(\\pmb{\\theta}\\right),\\mathcal{D}_{i}\\left(\\pmb{\\theta}^{\\prime}\\right)\\right)\\leq\\varepsilon_{i}\\sqrt{\\sum_{j=1}^{n}p_{i j}\\left\\|\\pmb{\\theta}_{j}-\\pmb{\\theta}_{j}^{\\prime}\\right\\|_{2}^{2}}}\\end{array}$ . Along with Assumption 2.4, we have that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|{G_{\\delta}\\left(\\delta^{\\prime}\\right)-G_{\\theta}\\left(\\delta^{\\prime}\\right)}\\right\\|_{2}^{2}\\le\\displaystyle\\sum_{i=1}^{n}\\sum_{j=1}^{n}L_{i}^{2}\\varepsilon_{i}^{2}p_{i j}\\left\\|{\\delta_{j}-\\theta_{j}}\\right\\|_{2}^{2}}\\\\ {\\le\\displaystyle\\sum_{i=1}^{n}L_{i}^{2}\\varepsilon_{i}^{2}\\operatorname*{max}_{j\\in[n]}p_{i j}\\left\\|{\\delta-\\theta}\\right\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Plugging the above result into (A4) yields ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\|\\pmb{\\theta}^{\\prime}-\\delta^{\\prime}\\|_{2}\\leq\\frac{1}{\\mu}\\sqrt{\\sum_{i=1}^{n}L_{i}^{2}\\varepsilon_{i}^{2}\\operatorname*{max}_{j\\in[n]}p_{i j}}\\;\\|\\pmb{\\delta}-\\pmb{\\theta}\\|_{2}\\,.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "From the RRM procedure, we know that $\\pmb{\\theta}^{t+1}=\\mathcal{T}(\\pmb{\\theta}^{t})$ and the PSE satisfies $\\theta^{\\mathrm{pse}}=\\mathcal{T}(\\theta^{\\mathrm{pse}})$ . Then, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|{\\boldsymbol\\theta}^{t+1}-{\\boldsymbol\\theta}^{\\mathrm{pse}}\\right\\|_{2}\\leq\\frac{1}{\\mu}\\sqrt{\\displaystyle\\sum_{i=1}^{n}L_{i}^{2}\\varepsilon_{i}^{2}\\operatorname*{max}_{j\\in[n]}p_{i j}}\\left\\|{\\boldsymbol\\theta}^{t}-{\\boldsymbol\\theta}^{\\mathrm{pse}}\\right\\|_{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left(\\frac{1}{\\mu}\\sqrt{\\displaystyle\\sum_{i=1}^{n}L_{i}^{2}\\varepsilon_{i}^{2}\\operatorname*{max}_{j\\in[n]}p_{i j}}\\right)^{t}\\left\\|{\\boldsymbol\\theta}^{1}-{\\boldsymbol\\theta}^{\\mathrm{pse}}\\right\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Further, if for any player $i$ , its distribution $\\mathcal{D}_{i}$ depends only on its own decision $\\theta_{i}$ , i.e., $p_{i j}=0$ and $p_{i i}=1$ for all $i,j\\in[n]$ and $j\\neq i$ , then, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left\\|\\left(G_{\\delta}\\left(\\delta^{\\prime}\\right)-G_{\\theta}\\left(\\delta^{\\prime}\\right)\\right)\\right\\|_{2}\\leq\\sqrt{\\sum_{i=1}^{n}L_{i}^{2}\\varepsilon_{i}^{2}\\left\\|\\delta_{i}-\\pmb\\theta_{i}\\right\\|_{2}^{2}}\\leq\\operatorname*{max}_{i\\in[n]}L_{i}\\varepsilon_{i}\\left\\|\\delta-\\pmb\\theta\\right\\|_{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Plugging (A5) into (A4) yields ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\|\\theta^{\\prime}-\\delta^{\\prime}\\|_{2}\\leq\\frac{1}{\\mu}\\operatorname*{max}_{i\\in[n]}L_{i}\\varepsilon_{i}\\left\\|\\delta-\\theta\\right\\|_{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Correspondingly, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left\\lVert\\theta^{t+1}-\\theta^{\\mathrm{pse}}\\right\\rVert_{2}\\leq\\left(\\frac{1}{\\mu}\\operatorname*{max}_{i\\in[n]}L_{i}\\varepsilon_{i}\\right)^{t}\\left\\lVert\\theta^{1}-\\theta^{\\mathrm{pse}}\\right\\rVert_{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "C  Existence and Uniqueness of Nash Equilibrium ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Based on the results in Facchinei and Pang (2003, Theorem 2.3.3(b)), to show the existence and uniqueness of NE, we need to prove that the gradient mapping ${\\boldsymbol\\nabla}\\mathrm{PR}(\\pmb\\theta)$ of the performative game (1) is strongly monotone, i.e., there exists a $\\alpha~>~0$ such that $\\langle\\nabla\\mathrm{PR}(\\pmb{\\theta})-\\bar{\\nabla}\\mathrm{PR}(\\pmb{\\theta}),\\pmb{\\theta}-\\bar{\\delta}\\rangle\\ \\geq$ $\\alpha\\left|\\left|\\pmb{\\theta}-\\pmb{\\delta}\\right|\\right|_{2}^{2}$ where $\\alpha$ denotes the strongly-monotone parameter. Since $\\nabla\\mathrm{PR}(\\pmb\\theta)=G_{\\pmb\\theta}(\\pmb\\theta)+H_{\\pmb\\theta}(\\pmb\\theta)$ wehave ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\langle\\nabla\\mathrm{PR}(\\theta)-\\nabla\\mathrm{PR}(\\delta),\\theta-\\delta\\rangle=\\langle G_{\\theta}(\\theta)-G_{\\delta}(\\delta),\\theta-\\delta\\rangle+\\langle H_{\\theta}(\\theta)-H_{\\delta}(\\delta),\\theta-\\delta\\rangle\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "From Assumption 2.2, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\langle G_{\\theta}(\\theta)-G_{\\delta}(\\theta),\\theta-\\delta\\rangle\\geq-\\sum_{i=1}^{n}L_{i}\\varepsilon_{i}\\operatorname*{max}_{j\\in[n]}\\sqrt{p_{i j}}\\left\\|\\theta-\\delta\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Moreover, from the monotonicity of the gradient mapping $\\nabla J\\left(\\xi;\\pmb{\\theta}\\right)$ in Assumption 2.1, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle G_{\\delta}(\\pmb{\\theta})-G_{\\delta}(\\delta),\\pmb{\\theta}-\\delta\\rangle=\\mathbb{E}_{\\pmb{\\xi}\\sim\\mathcal{D}(\\delta)}\\,\\langle\\nabla J(\\pmb{\\xi};\\pmb{\\theta})-\\nabla J(\\pmb{\\xi};\\pmb{\\delta}),\\pmb{\\theta}-\\delta\\rangle\\geq\\mu\\,\\|\\pmb{\\theta}-\\pmb{\\delta}\\|_{2}^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Combining the above two inequalities yields ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle{\\boldsymbol G}_{\\theta}(\\theta)-{\\boldsymbol G}_{\\delta}(\\delta),\\theta-\\delta\\rangle=\\langle{\\boldsymbol G}_{\\theta}(\\theta)-{\\boldsymbol G}_{\\delta}(\\theta),\\theta-\\delta\\rangle+\\langle{\\boldsymbol G}_{\\delta}(\\theta)-{\\boldsymbol G}_{\\delta}(\\delta),\\theta-\\delta\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\geq\\left(\\mu-\\displaystyle\\sum_{i=1}^{n}L_{i}\\varepsilon_{i}\\displaystyle\\operatorname*{max}_{j\\in[n]}\\sqrt{p_{i j}}\\right)\\|\\theta-\\delta\\|_{2}^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Further, let $\\gamma(s)=\\pmb\\theta^{\\prime}+s\\left(\\pmb\\theta-\\pmb\\theta^{\\prime}\\right)$ for $s\\in(0,1)$ . Then, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J_{i}\\left(\\xi_{i};\\theta\\right)-J_{i}\\left(\\xi_{i};\\theta^{\\prime}\\right)=\\displaystyle\\int_{0}^{1}\\left\\langle\\nabla J_{i}\\left(\\xi_{i};\\theta^{\\prime}+s\\left(\\theta-\\theta^{\\prime}\\right)\\right),\\theta-\\theta^{\\prime}\\right\\rangle\\mathrm{d}s}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\int_{0}^{1}\\left\\langle\\nabla J_{i}\\left(\\xi_{i};\\gamma(s)\\right),\\theta-\\theta^{\\prime}\\right\\rangle\\mathrm{d}s.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "From the definition of $H_{\\pmb\\theta}^{(i)}(\\pmb\\delta)$ that $H_{\\pmb\\theta}^{(i)}(\\pmb\\delta):=\\left.\\nabla_{\\pmb u_{i}}\\mathbb{E}_{\\pmb\\xi_{i}\\sim\\mathcal{D}_{i}(\\pmb u_{i},\\pmb\\theta_{-i})}\\left[J_{i}\\left(\\pmb\\xi_{i};\\pmb\\delta\\right)\\right]\\right|_{\\pmb u_{i}=\\pmb\\theta_{i}}$ , we have that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{H_{\\theta}^{(i)}(\\theta)-H_{\\theta}^{(i)}(\\theta^{\\prime})=\\left.\\nabla_{u_{i}}\\mathbb{E}_{\\xi_{i}\\sim\\mathcal{D}_{i}(u_{i},\\theta_{-i})}\\left[\\int_{0}^{1}\\left<\\nabla J_{i}\\left(\\xi_{i};\\gamma(s)\\right),\\theta-\\theta^{\\prime}\\right>\\mathrm{d}s\\right]\\right|_{u_{i}=\\theta_{i}}}\\\\ {=\\left.\\displaystyle\\int_{0}^{1}\\nabla_{u_{i}}\\mathbb{E}_{\\xi_{i}\\sim\\mathcal{D}_{i}(u_{i},\\theta_{-i})}\\left<\\nabla J_{i}\\left(\\xi_{i};\\gamma(s)\\right),\\theta-\\theta^{\\prime}\\right>\\right|_{u_{i}=\\theta_{i}}\\mathrm{d}s.~~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "From Assumption 2.4, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\mathbb{E}_{\\pmb{\\xi}_{i}\\sim\\mathcal{D}_{i}}\\nabla J_{i}\\left(\\pmb{\\xi}_{i};\\pmb{\\theta}\\right)-\\mathbb{E}_{\\pmb{\\xi}_{i}^{\\prime}\\sim\\mathcal{D}_{i}^{\\prime}}\\nabla J_{i}\\left(\\pmb{\\xi}_{i}^{\\prime};\\pmb{\\theta}\\right)\\right\\|_{2}\\leq L_{i}\\mathcal{W}_{1}(\\mathcal{D}_{i},\\mathcal{D}_{i}^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Along with Assumption 2.2, we know that the function $\\mathbb{E}_{\\pmb{\\xi}_{i}\\sim\\mathcal{D}_{i}(\\pmb{\\theta}_{i},\\pmb{\\theta}_{-i})}\\nabla J_{i}\\left(\\pmb{\\xi}_{i};\\pmb{\\theta}^{\\prime}\\right)$ is $L_{i}\\varepsilon_{i}p_{i i}$ Lipschitz continuous w.r.t $\\theta_{i}$ , and thus its gradient satisfies ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bigg\\|\\nabla_{u_{i}}\\mathbb{E}_{\\pmb{\\xi}_{i}\\sim\\mathcal{D}_{i}(u_{i},\\pmb{\\theta}_{-i})}\\left[\\nabla J_{i}\\left(\\pmb{\\xi}_{i};\\gamma(s)\\right)\\right]\\rvert_{u_{i}=\\pmb{\\theta}_{i}}\\bigg\\|_{2}\\leq L_{i}\\varepsilon_{i}p_{i i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Combing (A8) and (A9) gives ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|H_{\\theta}^{(i)}(\\theta)-H_{\\theta}^{(i)}(\\theta^{\\prime})\\right\\|_{2}\\le\\int_{0}^{1}\\left\\|\\nabla_{u_{i}}\\mathbb{E}_{\\xi_{i}\\sim\\mathcal{D}_{i}(u_{i},\\theta_{-i})}\\left[\\nabla J_{i}\\left(\\xi_{i};\\gamma(s)\\right)\\right]\\right|_{u_{i}=\\theta_{i}}\\right\\|_{2}\\left\\|\\theta-\\theta^{\\prime}\\right\\|_{2}\\ensuremath{\\mathrm{d}}s}&{}\\\\ {\\le L_{i}\\varepsilon_{i}p_{i i}\\left\\|\\theta-\\theta^{\\prime}\\right\\|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the first inequality holds due to the Cauchy-Schwartz inequality. This further implies that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|{H_{\\theta}}(\\theta)-{H_{\\theta}}(\\theta^{\\prime})\\right\\|_{2}=\\sqrt{\\displaystyle\\sum_{i=1}^{n}\\left\\|{H_{\\theta}^{(i)}}(\\theta)-{H_{\\theta}^{(i)}}(\\theta^{\\prime})}\\right\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\sqrt{\\displaystyle\\sum_{i=1}^{n}L_{i}^{2}\\varepsilon_{i}^{2}p_{i i}}\\left\\|{\\theta-\\theta^{\\prime}}\\right\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Following prior work (Narang et al., 2023) and (Wang et al., 2023) on performative games, we assume that the mapping $H_{\\delta}(\\pmb\\theta)$ is monotone w.r.t $\\delta$ i.e., $\\langle H_{\\theta}(\\pmb\\theta)-H_{\\delta}(\\pmb\\theta),\\pmb\\theta-\\pmb\\delta\\rangle\\geq0$ . Then, we have that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\nabla\\mathrm{PR}(\\theta)-\\nabla\\mathrm{PR}(\\delta),\\theta-\\delta\\rangle=\\langle G_{\\theta}(\\theta)-G_{\\delta}(\\delta),\\theta-\\delta\\rangle+\\langle H_{\\theta}(\\theta)-H_{\\delta}(\\delta),\\theta-\\delta\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\langle G_{\\theta}(\\theta)-G_{\\delta}(\\theta),\\theta-\\delta\\rangle+\\langle H_{\\theta}(\\theta)-H_{\\delta}(\\theta),\\theta-\\delta\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\,\\langle G_{\\delta}(\\theta)-G_{\\delta}(\\delta),\\theta-\\delta\\rangle+\\langle H_{\\delta}(\\theta)-H_{\\delta}(\\delta),\\theta-\\delta\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\geq\\left(\\mu-\\displaystyle\\sum_{i=1}^{n}L_{i}\\varepsilon_{i}\\displaystyle\\operatorname*{max}_{j\\in[n]}\\sqrt{p_{i j}}-\\sqrt{\\displaystyle\\sum_{i=1}^{n}L_{i}^{2}\\varepsilon_{i}^{2}p_{i i}}\\right)\\|\\theta-\\delta\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Based on the classical result that a strongly monotone game over a non-empty, closed, and convex set admits a unique NE Facchinei and Pang (2003, Theorem 2.3.3(b)), we have the $\\mathrm{E}\\&\\mathrm{U}$ conditionfor the NE of the game (1) as given in theorem 3.4. ", "page_idx": 14}, {"type": "text", "text": "D Distance Between PSE and NE ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The computation on the distance between the PSE and NE of the game (1) is based on the strong duality (Boyd and Vandenberghe, 2004; Facchinei and Pang, 2010). Recall the definitions in Section 4.1 that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\delta}^{(i)}(\\theta_{i},\\theta_{-i},\\lambda):=\\mathbb{E}_{\\xi_{i}\\sim\\mathcal{D}_{i}(\\delta)}J_{i}\\left(\\xi_{i};\\theta_{i},\\theta_{-i}\\right)+\\left\\langle\\lambda,g_{i}(\\theta_{i})+\\sum_{j\\neq i}g_{j}\\left(\\theta_{j}\\right)\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Moreover,define a gradient mapping $\\phi_{i}(\\pmb{\\xi}_{i};\\pmb{\\theta},\\pmb{\\lambda}):=\\nabla\\pmb{\\theta}_{i}J_{i}\\left(\\pmb{\\xi}_{i};\\pmb{\\theta}\\right)+\\nabla\\pmb{g}_{i}(\\pmb{\\theta}_{i})^{\\top}\\pmb{\\lambda}$ and a concatenation vector $\\phi:=[\\phi_{1},\\cdots,\\phi_{n}]^{\\top}$ . For any $i\\in[n]$ , since $(\\pmb{\\theta}_{i}^{\\mathrm{pse}},\\lambda^{\\mathrm{pse}})$ is a sadle point of the Lagrangian $\\mathcal{L}_{\\theta^{\\mathrm{Pse}}}^{(i)}(\\pmb{\\theta}_{i},\\pmb{\\theta}_{-i}^{\\mathrm{pse}},\\lambda)$ under $\\pmb{\\xi}_{i}\\sim\\mathcal{D}_{i}(\\pmb{\\theta}^{\\mathrm{pse}})$ , we have that' ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\theta^{\\mathrm{pse}}}^{(i)}\\left(\\theta_{i}^{\\mathrm{pse}},\\theta_{-i}^{\\mathrm{pse}},\\lambda\\right)\\leq\\mathcal{L}_{\\theta^{\\mathrm{pse}}}^{(i)}\\left(\\theta_{i}^{\\mathrm{pse}},\\theta_{-i}^{\\mathrm{pse}},\\lambda^{\\mathrm{pse}}\\right)\\leq\\mathcal{L}_{\\theta^{\\mathrm{pse}}}^{(i)}\\left(\\theta_{i},\\theta_{-i}^{\\mathrm{pse}},\\lambda^{\\mathrm{pse}}\\right)\\quad\\forall\\theta_{i}\\in\\Omega_{i},\\lambda\\in\\mathbb{R}_{+}^{m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Similarly, for any $\\begin{array}{r l r}{i}&{{}\\in}&{[n]}\\end{array}$ $(\\pmb{\\theta}_{i}^{\\mathrm{ne}},\\lambda^{\\mathrm{ne}})$ the saddle point of the regularized Lagrangian $\\mathcal{L}_{\\pmb{\\theta}_{i},\\pmb{\\theta}_{-i}^{\\mathrm{ne}}}^{(i)}(\\pmb{\\theta}_{i},\\pmb{\\theta}_{-i}^{\\mathrm{ne}},\\pmb{\\lambda})$ with decision-dependent distribution $\\pmb{\\xi}_{i}\\;\\sim\\;\\mathcal{D}_{i}(\\pmb{\\theta}_{i},\\pmb{\\theta}_{-i}^{\\mathrm{ne}})$ . Setting $\\lambda\\,=\\,\\lambda^{\\mathrm{ne}}$ in the first part of the proceeding inequality, we obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{l l}&{\\mathbf{0}\\leq\\mathcal{L}_{\\theta^{\\mathrm{pse}}}^{(i)}\\left(\\theta_{i}^{\\mathrm{pse}},\\theta_{-i}^{\\mathrm{pse}},\\lambda^{\\mathrm{pse}}\\right)-\\mathcal{L}_{\\theta^{\\mathrm{pse}}}^{(i)}\\left(\\theta_{i}^{\\mathrm{pse}},\\theta_{-i}^{\\mathrm{pse}},\\lambda^{\\mathrm{ne}}\\right)=\\left(\\lambda^{\\mathrm{pse}}-\\lambda^{\\mathrm{ne}}\\right)^{\\top}g\\left(\\theta^{\\mathrm{pse}}\\right),\\forall i\\in[n],}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Where $\\begin{array}{r}{(\\boldsymbol{\\lambda}^{\\mathrm{pse}}-\\boldsymbol{\\lambda}^{\\mathrm{ne}})^{\\top}\\,\\boldsymbol{g}\\,(\\boldsymbol{\\theta}^{\\mathrm{pse}})=\\sum_{j=1}^{m}\\big(\\boldsymbol{\\lambda}_{j}^{\\mathrm{pse}}-\\boldsymbol{\\lambda}_{j}^{\\mathrm{ne}}\\big)\\,\\big(\\sum_{i=1}^{n}g_{j i}\\,(\\theta_{i}^{\\mathrm{pse}})\\big)}\\end{array}$ By theconvexity of $g_{j i}(\\cdot)$ for all $j\\in[m],i\\in[n]$ ,we have that? ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{n}g_{j i}\\left(\\pmb{\\theta}_{i}^{\\mathrm{pse}}\\right)\\leq\\displaystyle\\sum_{i=1}^{n}\\left(g_{j i}\\left(\\pmb{\\theta}_{i}^{\\mathrm{ne}}\\right)+\\left\\langle\\nabla g_{j i}\\left(\\pmb{\\theta}_{i}^{\\mathrm{pse}}\\right),\\pmb{\\theta}_{i}^{\\mathrm{pse}}-\\pmb{\\theta}_{i}^{\\mathrm{ne}}\\right\\rangle\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\displaystyle\\sum_{i=1}^{n}\\left\\langle\\nabla g_{j i}\\left(\\pmb{\\theta}_{i}^{\\mathrm{pse}}\\right),\\pmb{\\theta}_{i}^{\\mathrm{pse}}-\\pmb{\\theta}_{i}^{\\mathrm{ne}}\\right\\rangle,\\forall j\\in[m],}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where thelastqultyfwsromha $\\begin{array}{r}{g_{j}(\\pmb{\\theta}_{\\mathrm{~\\tiny~\\cdot~}}^{\\mathrm{ne}})=\\sum_{i=1}^{n}g_{j i}\\left(\\pmb{\\theta}_{i}^{\\mathrm{ne}}\\right)\\leq0}\\end{array}$ Multiplying the preceding inequality with $\\lambda_{j}^{\\mathrm{pse}}$ and adding over all $j\\in[m]$ we obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{j=1}^{m}\\sum_{i=1}^{n}\\lambda_{j}^{\\mathrm{pse}}g_{j i}\\left(\\theta_{i}^{\\mathrm{pse}}\\right)=\\left(\\lambda^{\\mathrm{pse}}\\right)^{\\top}g\\left(\\theta^{\\mathrm{pse}}\\right)\\leq\\displaystyle\\sum_{i=1}^{n}\\left\\langle\\sum_{j=1}^{m}\\lambda_{j}^{\\mathrm{pse}}\\nabla g_{j i}\\left(\\theta_{i}^{\\mathrm{pse}}\\right),\\theta_{i}^{\\mathrm{pse}}-\\theta_{i}^{\\mathrm{ne}}\\right\\rangle}\\\\ &{\\displaystyle=\\sum_{i=1}^{n}\\left\\langle\\nabla g_{i}(\\theta_{i}^{\\mathrm{pse}})^{\\top}\\lambda^{\\mathrm{pse}},\\theta_{i}^{\\mathrm{pse}}-\\theta_{i}^{\\mathrm{ne}}\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By the definition of the mapping $\\phi_{i}(\\cdot)$ , for any $\\boldsymbol{\\xi}_{i}\\in\\Xi_{i}$ , we have that, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla g_{i}(\\pmb{\\theta}_{i}^{\\mathrm{pse}})^{\\top}\\lambda^{\\mathrm{pse}}=\\phi_{i}(\\pmb{\\xi}_{i};\\pmb{\\theta}^{\\mathrm{pse}},\\pmb{\\lambda}^{\\mathrm{pse}})-\\nabla_{\\pmb{\\theta}_{i}}J_{i}\\left(\\pmb{\\xi}_{i};\\pmb{\\theta}^{\\mathrm{pse}}\\right),\\forall i\\in[n].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Plugging (A11) into (A10) gives ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left(\\lambda^{\\mathrm{pse}}\\right)^{\\top}g\\left(\\theta^{\\mathrm{pse}}\\right)\\leq\\sum_{i=1}^{n}\\left\\langle\\phi_{i}(\\pmb{\\xi}_{i};\\theta^{\\mathrm{pse}},\\lambda^{\\mathrm{pse}})-\\nabla_{\\theta_{i}}J_{i}\\left(\\pmb{\\xi}_{i};\\theta^{\\mathrm{pse}}\\right),\\theta_{i}^{\\mathrm{pse}}-\\theta_{i}^{\\mathrm{ne}}\\right\\rangle,\\forall i\\in[n].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Likewise, we have the following inequality based on the convexity of the functions $\\{g_{j i}(\\cdot)\\}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\ng_{j i}\\left(\\theta_{i}^{\\mathrm{pse}}\\right)\\geq g_{j i}\\left(\\theta_{i}^{\\mathrm{ne}}\\right)+\\langle\\nabla g_{j i}\\left(\\theta_{i}^{\\mathrm{ne}}\\right),\\theta_{i}^{\\mathrm{pse}}-\\theta_{i}^{\\mathrm{ne}}\\rangle\\,,\\forall j\\in[m],i\\in[n].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Multiplying the preceding inequality with $-\\lambda_{j}^{\\mathrm{ne}}$ and summing over $j\\in[m]$ , we obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\displaystyle\\sum_{j=1}^{m}\\lambda_{i}^{\\mathrm{ne}}\\sum_{i=1}^{n}g_{j i}\\left(\\theta_{i}^{\\mathrm{pse}}\\right)\\leq-\\displaystyle\\sum_{j=1}^{m}\\lambda_{j}^{\\mathrm{ne}}\\sum_{i=1}^{n}g_{j i}\\left(\\theta_{i}^{\\mathrm{ne}}\\right)-\\displaystyle\\sum_{i=1}^{n}\\left\\langle\\sum_{j=1}^{m}\\lambda_{j}^{\\mathrm{ne}}\\nabla g_{j i}\\left(\\theta_{i}^{\\mathrm{ne}}\\right),\\theta_{i}^{\\mathrm{pse}}-\\theta_{i}^{\\mathrm{ne}}\\right\\rangle}\\\\ &{\\displaystyle=\\sum_{i=1}^{n}\\left\\langle\\nabla g_{i}\\left(\\theta_{i}^{\\mathrm{ne}}\\right)^{\\top}\\lambda^{\\mathrm{ne}},\\theta_{i}^{\\mathrm{ne}}-\\theta_{i}^{\\mathrm{pse}}\\right\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the equality follows from that $\\begin{array}{r}{\\sum_{j=1}^{m}\\lambda_{j}^{\\mathrm{ne}}\\sum_{i=1}^{n}g_{j i}\\left(\\pmb{\\theta}_{i}^{\\mathrm{ne}}\\right)=\\left(\\pmb{\\lambda}^{\\mathrm{ne}}\\right)^{\\top}\\pmb{g}\\left(\\pmb{\\theta}^{\\mathrm{ne}}\\right)=0}\\end{array}$ , which holds by the complementary slackness condition of the Lagrangian $\\mathcal{L}_{\\pmb{\\theta}_{i},\\pmb{\\theta}_{-i}^{\\mathrm{ne}}}^{(i)}(\\pmb{\\theta}_{i},\\pmb{\\theta}_{-i}^{\\mathrm{ne}},\\pmb{\\lambda})$ for all $i\\,\\in\\,[n]$ Similar to (A12), we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n-\\left(\\boldsymbol{\\lambda}^{\\mathrm{ne}}\\right)^{\\top}g\\left(\\theta^{\\mathrm{pse}}\\right)\\leq\\sum_{i=1}^{n}\\left\\langle\\phi_{i}\\big(\\pmb{\\xi}_{i};\\theta^{\\mathrm{ne}},\\boldsymbol{\\lambda}^{\\mathrm{ne}}\\big)-\\nabla_{\\theta_{i}}J_{i}\\left(\\pmb{\\xi}_{i};\\theta^{\\mathrm{ne}}\\right),\\theta_{i}^{\\mathrm{ne}}-\\theta_{i}^{\\mathrm{pse}}\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Combining (A12) and (A13) yields ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\left(\\lambda^{\\mathrm{pse}}-\\lambda^{\\mathrm{ne}}\\right)^{\\top}g\\left(\\theta^{\\mathrm{pse}}\\right)\\leq\\displaystyle\\sum_{i=1}^{n}\\left\\langle\\phi_{i}(\\xi_{i};\\theta^{\\mathrm{pse}},\\lambda^{\\mathrm{pse}})-\\phi_{i}(\\xi_{i};\\theta^{\\mathrm{ne}},\\lambda^{\\mathrm{ne}}),\\theta_{i}^{\\mathrm{pse}}-\\theta_{i}^{\\mathrm{ne}}\\right\\rangle}&{}&\\\\ {-\\displaystyle\\sum_{i=1}^{n}\\left\\langle\\nabla_{\\theta_{i}}J_{i}\\left(\\xi_{i};\\theta^{\\mathrm{pse}}\\right)-\\nabla_{\\theta_{i}}J_{i}\\left(\\xi_{i};\\theta^{\\mathrm{ne}}\\right),\\theta_{i}^{\\mathrm{pse}}-\\theta_{i}^{\\mathrm{ne}}\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Taking expectation on both sides of the above inequality over the distribution $\\mathcal{D}_{i}(\\pmb{\\theta}^{\\mathrm{pse}})$ forall $i\\in[n]$ gives ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\lambda^{\\mathrm{pse}}-\\lambda^{\\mathrm{ne}}\\right)^{\\top}g\\left(\\theta^{\\mathrm{pse}}\\right)\\leq\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}_{\\xi_{i}\\sim\\mathcal{D}_{i}\\left(\\theta^{\\mathrm{pse}}\\right)}\\left\\langle\\phi_{i}\\big(\\xi_{i};\\theta^{\\mathrm{pse}},\\lambda^{\\mathrm{pse}}\\big)-\\phi_{i}\\big(\\xi_{i};\\theta^{\\mathrm{ne}},\\lambda^{\\mathrm{ne}}\\big),\\theta_{i}^{\\mathrm{pse}}-\\theta_{i}^{\\mathrm{ne}}\\right\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad-\\displaystyle\\sum_{i=1}^{n}\\left\\langle G_{\\theta^{\\mathrm{pse}}}^{(i)}\\left(\\theta^{\\mathrm{pse}}\\right)-G_{\\theta^{\\mathrm{pse}}}^{(i)}\\left(\\theta^{\\mathrm{ne}}\\right),\\theta_{i}^{\\mathrm{pse}}-\\theta_{i}^{\\mathrm{ne}}\\right\\rangle.\\qquad\\qquad\\qquad\\mathrm{(A1)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since $(\\pmb{\\theta}_{i}^{\\mathrm{pse}},\\lambda^{\\mathrm{pse}})$ is a saddle point of the Lagrangian $\\mathcal{L}_{\\theta^{\\mathrm{pse}}}^{(i)}(\\theta^{\\mathrm{pse}},\\lambda^{\\mathrm{pse}})$ given $\\pmb{\\xi}_{i}\\sim\\mathcal{D}_{i}(\\pmb{\\theta}^{\\mathrm{pse}})$ we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\xi_{i}\\sim\\mathcal{D}_{i}(\\theta^{\\mathrm{pse}})}\\left\\langle\\phi_{i}(\\xi_{i};\\theta^{\\mathrm{pse}},\\lambda^{\\mathrm{pse}}),\\theta_{i}^{\\mathrm{pse}}-\\theta_{i}^{\\mathrm{ne}}\\right\\rangle\\leq0,\\forall i\\in[n].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Furthermore, for any $i\\in[n]$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{{\\pmb\\xi}_{i}\\sim\\mathcal{D}_{i}({\\pmb\\theta}^{\\mathrm{pse}})}\\phi_{i}({\\pmb\\xi}_{i};{\\pmb\\theta}^{\\mathrm{ne}},\\lambda^{\\mathrm{ne}})=G_{\\theta^{\\mathrm{pse}}}^{(i)}\\left(\\theta^{\\mathrm{ne}}\\right)+\\nabla g_{i}(\\theta_{i}^{\\mathrm{ne}})^{\\top}\\lambda^{\\mathrm{ne}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\nabla_{{\\pmb\\theta}_{i}}\\mathrm{PR}_{i}(\\theta_{i}^{\\mathrm{ne}},\\theta_{-i}^{\\mathrm{ne}})-\\nabla_{{\\pmb\\theta}_{i}}\\mathrm{PR}_{i}(\\theta_{i}^{\\mathrm{ne}},\\theta_{-i}^{\\mathrm{ne}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since $(\\pmb{\\theta}_{i}^{\\mathrm{ne}},\\lambda^{\\mathrm{ne}})$ is a saddle point of the Lagrangian $\\mathcal{L}_{\\pmb{\\theta}_{i},\\pmb{\\theta}_{-i}^{\\mathrm{ne}}}^{(i)}(\\pmb{\\theta}_{i},\\pmb{\\theta}_{-i}^{\\mathrm{ne}},\\pmb{\\lambda}^{\\mathrm{ne}})$ with decision-dependent distribution $\\mathcal{D}_{i}(\\pmb{\\theta}_{i},\\pmb{\\theta}_{-i}^{\\mathrm{ne}})$ , we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{-\\left\\langle\\nabla_{\\theta_{i}}\\mathrm{PR}_{i}(\\theta_{i}^{\\mathrm{ne}},\\theta_{-i}^{\\mathrm{ne}})+\\nabla g_{i}(\\theta_{i}^{\\mathrm{ne}})^{\\top}\\lambda^{\\mathrm{ne}},\\theta_{i}^{\\mathrm{pse}}-\\theta_{i}^{\\mathrm{ne}}\\right\\rangle\\leq0,\\forall i\\in[n].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Plugging (A15), (A16), and (A17) into (A14) yields ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0\\leq(\\lambda^{\\mathrm{pse}}-\\lambda^{\\mathrm{ne}})^{\\top}\\,g\\,(\\theta^{\\mathrm{pse}})}\\\\ &{\\quad\\leq\\displaystyle\\sum_{i=1}^{n}\\left\\langle\\nabla_{i}\\mathrm{PR}_{i}(\\theta_{i}^{\\mathrm{ne}},\\theta_{-i}^{\\mathrm{ne}})-G_{\\theta^{\\mathrm{pse}}}^{(i)}\\left(\\theta^{\\mathrm{ne}}\\right),\\theta_{i}^{\\mathrm{pse}}-\\theta_{i}^{\\mathrm{ne}}\\right\\rangle}\\\\ &{\\quad\\quad-\\displaystyle\\sum_{i=1}^{n}\\left\\langle G_{\\theta^{\\mathrm{pse}}}^{(i)}\\left(\\theta^{\\mathrm{pse}}\\right)-G_{\\theta^{\\mathrm{pse}}}^{(i)}\\left(\\theta^{\\mathrm{ne}}\\right),\\theta_{i}^{\\mathrm{pse}}-\\theta_{i}^{\\mathrm{ne}}\\right\\rangle}\\\\ &{\\quad=\\displaystyle\\sum_{i=1}^{n}\\left\\langle H_{\\theta^{\\mathrm{ne}}}^{(i)}(\\theta^{\\mathrm{ne}})+G_{\\theta^{\\mathrm{ne}}}^{(i)}\\left(\\theta^{\\mathrm{ne}}\\right)-G_{\\theta^{\\mathrm{pse}}}^{(i)}\\left(\\theta^{\\mathrm{pse}}\\right),\\theta_{i}^{\\mathrm{pse}}-\\theta_{i}^{\\mathrm{ne}}\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\langle G_{\\theta^{\\mathrm{pse}}}\\left(\\theta^{\\mathrm{pse}}\\right)-G_{\\theta^{\\mathrm{ne}}}\\left(\\theta^{\\mathrm{ne}}\\right),\\theta^{\\mathrm{pse}}-\\theta^{\\mathrm{ne}}\\right\\rangle\\leq\\left\\langle H_{\\theta^{\\mathrm{ne}}}(\\theta^{\\mathrm{ne}}),\\theta^{\\mathrm{pse}}-\\theta^{\\mathrm{ne}}\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "From the result in (A6) and the Cauchy-Schwarz inequality, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left(\\mu-\\sum_{i=1}^{n}L_{i}\\varepsilon_{i}\\operatorname*{max}_{j\\in[n]}\\sqrt{p_{i j}}\\right)\\|\\theta^{\\mathrm{pse}}-\\theta^{\\mathrm{ne}}\\|_{2}^{2}\\leq\\|H_{\\theta^{\\mathrm{ne}}}(\\theta^{\\mathrm{ne}})\\|_{2}\\|\\theta^{\\mathrm{pse}}-\\theta^{\\mathrm{ne}}\\|_{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since the cost function $J_{i}(\\cdot)$ .s $G_{i}$ Lipschitz for any $i\\in[n]$ , along with Assumption 2.2, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|H_{\\theta^{\\mathrm{ne}}}(\\theta^{\\mathrm{ne}})\\|_{2}=\\sqrt{\\sum_{i=1}^{n}\\|H_{\\theta_{i}^{\\mathrm{ne}},\\theta_{-i}^{\\mathrm{ne}}}^{(i)}(\\theta_{i}^{\\mathrm{ne}},\\theta_{-i}^{\\mathrm{ne}})\\|_{2}^{2}}\\leq\\sqrt{\\sum_{i=1}^{n}G_{i}^{2}\\varepsilon_{i}^{2}p_{i i}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Combining the above results yields ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|\\theta^{\\mathrm{pse}}-\\theta^{\\mathrm{ne}}\\|_{2}\\leq\\frac{\\sqrt{\\sum_{i=1}^{n}G_{i}^{2}\\varepsilon_{i}^{2}p_{i i}}}{\\mu-\\sum_{i=1}^{n}L_{i}\\varepsilon_{i}\\operatorname*{max}_{j\\in[n]}\\sqrt{p_{i j}}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Further, from Assumption 2.2, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\mathrm{PR}_{i}(\\pmb{\\theta}^{\\mathrm{pse}})-\\mathrm{PR}_{i}(\\pmb{\\theta}^{\\mathrm{ne}})|\\leq G_{i}\\|\\pmb{\\theta}^{\\mathrm{pse}}-\\pmb{\\theta}^{\\mathrm{ne}}\\|_{2}+G_{i}\\varepsilon_{i}\\sqrt{\\displaystyle\\sum_{j=1}^{n}p_{i j}\\left\\|\\pmb{\\theta}_{j}^{\\mathrm{pse}}-\\pmb{\\theta}_{j}^{\\mathrm{ne}}\\right\\|_{2}^{2}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq G_{i}\\left(1+\\varepsilon_{i}\\displaystyle\\operatorname*{max}_{j\\in[n]}\\sqrt{p_{i j}}\\right)\\|\\pmb{\\theta}^{\\mathrm{pse}}-\\pmb{\\theta}^{\\mathrm{ne}}\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{|\\mathrm{PR}(\\theta^{\\mathrm{pse}})-\\mathrm{PR}(\\theta^{\\mathrm{ne}})|=\\displaystyle\\sum_{i=1}^{n}|\\mathrm{PR}_{i}(\\theta^{\\mathrm{pse}})-\\mathrm{PR}_{i}(\\theta^{\\mathrm{ne}})|}}\\\\ &{}&{\\leq\\left(\\displaystyle\\sum_{i=1}^{n}G_{i}\\left(1+\\varepsilon_{i}\\operatorname*{max}_{j\\in[n]}\\sqrt{p_{i j}}\\right)\\right)\\frac{\\sqrt{\\sum_{i=1}^{n}G_{i}^{2}\\varepsilon_{i}^{2}p_{i i}}}{\\mu-\\sum_{i=1}^{n}L_{i}\\varepsilon_{i}\\operatorname*{max}_{j\\in[n]}\\sqrt{p_{i j}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "E  Convergence of the Decentralized Stochastic Primal-Dual Algorithm ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The proof of this section utilizes the following supporting lemmas. ", "page_idx": 17}, {"type": "text", "text": "Lemma E.1. Based on the update rule of the dual variable $\\lambda$ in Algorithm $^{\\,l}$ for any $\\gamma_{t}~\\geq~0$ $\\lambda_{i}^{t}\\in\\mathbb{R}_{+}^{m}$ \uff0c $i\\in[n]$ and $t\\in[T]$ we have that $\\begin{array}{r}{\\sum_{i=1}^{n}\\|\\gamma_{t}\\lambda_{i}^{t}\\|_{2}^{2}\\leq n B^{2}}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "Lemma E.2. Define $\\begin{array}{r}{\\overline{{\\lambda}}^{t}:=\\frac{1}{n}\\sum_{i=1}^{n}\\lambda_{i}^{t}}\\end{array}$ teaverageftdlalevellyeat t iteration. Then, for any $\\gamma_{t}\\geq0$ and $\\bar{t}\\in[T]$ we have the following relationship: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle-\\sum_{t=1}^{T}\\sum_{i=1}^{n}\\gamma_{t}({\\boldsymbol{X}}_{i}^{t})^{\\top}g_{i}(\\theta_{i}^{t})\\leq-\\sum_{t=1}^{T}\\sum_{i=1}^{n}\\gamma_{t}\\lambda^{\\top}g_{i}\\left(\\theta_{i}^{t}\\right)+\\frac{n}{2}\\left(1+\\displaystyle\\sum_{t=1}^{T}\\gamma_{t}^{2}\\right)\\|\\lambda\\|_{2}^{2}+\\displaystyle\\frac{9}{2}\\sum_{t=1}^{T}\\sum_{i=1}^{n}\\left\\|\\lambda_{i}^{t}-\\overline{{\\boldsymbol{X}}}^{t}\\right\\|_{L^{2}}}\\\\ {\\displaystyle+\\,2(1+\\sqrt{n})B\\sum_{t=1}^{T}\\gamma_{t}\\sum_{i=1}^{n}\\left\\|\\lambda_{i}^{t}-\\overline{{\\boldsymbol{X}}}^{t}\\right\\|_{2}+4n B^{2}\\sum_{t=1}^{T}\\gamma_{t}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Moreover, we require the following Lemma on the weight matrix A. ", "page_idx": 17}, {"type": "text", "text": "Lemma E.3. Let $\\sigma_{2}(\\mathbf{A})$ denote the second-largest eigenvalue of the weight matrix A. Since A is assumed to be doubly stochastic, it holds that $\\sigma_{2}(\\mathbf{A})<1$ (Horn and Johnson, 2012). Furthermore, for any $i\\in[n]$ ,we construct a weight matrix $\\mathbf{A}_{i}^{-}$ by removing the ith row and column of A. Let $\\beta$ represent the maximum eigenvalue of $\\mathbf{A}_{i}^{-}$ for all $i\\in[n]$ . It has been established in Hong et al. (2006, Lemma 3) that $\\beta<1$ ", "page_idx": 17}, {"type": "text", "text": "With Lemma E.3, we have the following results. ", "page_idx": 17}, {"type": "text", "text": "Lemma E.4. Define $\\pmb{e}_{i h}^{t}:=\\widehat{\\pmb{\\theta}}_{i h}^{t}-\\pmb{\\theta}_{h}^{t}$ the estimation error of player i on the decision of player $h$ at the tth iteration, for all $i,h\\in[n]$ and $t\\,\\in\\,[T]$ . Let $e_{h}^{t}$ denote the concatenation of $e_{i h}^{t}$ that $\\pmb{e}_{h}^{t}:=\\mathrm{col}\\left(\\pmb{e}_{1h}^{t},\\cdot\\cdot\\cdot\\;,\\pmb{e}_{(h-1)h}^{t},\\pmb{e}_{(h+1)h}^{t},\\cdot\\cdot\\cdot\\;,\\pmb{e}_{n h}^{t}\\right)$ Then, the sum of $\\|e_{h}^{t}\\|_{2}$ over $h\\in[n]$ and $t\\in[T]$ satisfies ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\sum_{h=1}^{n}\\mathbb{E}\\|e_{h}^{t}\\|_{2}\\leq\\frac{n C}{1-\\beta}+\\frac{n\\sqrt{n-1}(G+\\sqrt{n}B G_{g})}{1-\\beta}\\sum_{t=1}^{T}\\gamma_{t}=\\mathcal{O}\\left(\\sum_{t=1}^{T}\\gamma_{t}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Moreover, the sum of $\\lVert e_{i h}^{t}\\rVert_{2}^{2}$ over $h\\in[n]$ and $t\\in[T]$ satisfies ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\sum_{h=1}^{n}\\mathbb{E}\\|e_{h}^{t}\\|_{2}^{2}\\leq{\\frac{2n C^{2}}{1-\\beta}}+{\\frac{2n(n-1)(G+{\\sqrt{n}}B G_{g})^{2}}{(1-\\beta)^{2}}}\\sum_{t=1}^{T}\\gamma_{t}={\\mathcal{O}}\\left(\\sum_{t=1}^{T}\\gamma_{t}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma E.5. With the definition $\\begin{array}{r}{\\overline{{\\lambda}}^{t}:=\\;\\frac{1}{n}\\sum_{i=1}^{n}\\lambda_{i}^{t}}\\end{array}$ we have the follwing relationship on the consensus error of the dual variable $\\lambda_{i}^{t}$ given by $\\lambda_{i}^{t}-\\overline{{\\lambda}}^{t}$ for all $i\\in[n]$ and $t\\in[T]$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\displaystyle\\sum_{i=1}^{n}\\left\\|\\lambda_{i}^{t}-\\overline{{\\lambda}}^{t}\\right\\|_{2}\\leq\\displaystyle\\frac{2(n+\\sqrt{n})B}{1-\\sigma_{2}(\\mathbf A)}\\sum_{t=1}^{T}\\gamma_{t}=\\mathcal{O}\\left(\\sum_{t=1}^{T}\\gamma_{t}\\right),}\\\\ &{\\displaystyle\\sum_{t=1}^{T}\\displaystyle\\sum_{i=1}^{n}\\left\\|\\lambda_{i}^{t}-\\overline{{\\lambda}}^{t}\\right\\|_{2}^{2}\\leq\\displaystyle\\frac{4(n+\\sqrt{n})^{2}B^{2}}{(1-\\sigma_{2}(\\mathbf A))^{2}}\\sum_{t=1}^{T}\\gamma_{t}=\\mathcal{O}\\left(\\sum_{t=1}^{T}\\gamma_{t}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Next, we start the proof of Theorem 4.2. For ease of proposition, we define the following gradient mappings: for any $t~\\in~[T]$ $\\begin{array}{r}{\\phi_{i}^{t}(\\pmb{\\xi}_{i};\\pmb{\\theta}_{i},\\pmb{\\theta}_{-i},\\pmb{\\lambda})\\;:=\\;\\mathbf{\\bar{\\nabla}}_{\\b{i}}J_{i}\\left(\\pmb{\\xi}_{i};\\pmb{\\theta}_{i},\\pmb{\\theta}_{-i},\\pmb{\\theta}\\right)+\\gamma_{t}\\nabla\\pmb{g}_{i}(\\pmb{\\theta}_{i})^{\\top}\\pmb{\\lambda},}\\end{array}$ $\\phi^{t}(\\cdot)~~:=~~[\\phi_{1}^{t}(\\cdot),\\cdot\\cdot\\cdot\\cdot,\\phi_{n}^{t}(\\cdot)]^{\\intercal}$ \uff0c $\\Phi_{\\delta}^{i,t}(\\pmb\\theta,\\lambda)\\;:=\\;\\;G_{\\delta}^{(i)}\\left(\\pmb\\theta\\right)\\;+\\;\\gamma_{t}\\nabla g_{i}(\\pmb\\theta_{i})^{\\top}\\lambda$ and $\\Phi_{\\delta}^{t}(\\pmb{\\theta},\\lambda)~~:=$ $\\left[\\Phi_{\\delta}^{1,t}(\\pmb{\\theta},\\lambda),\\cdot\\cdot\\cdot\\mathbf{\\Phi},\\Phi_{\\delta}^{n,t}(\\pmb{\\theta},\\lambda)\\right]^{\\mathrm{~\\tiny~1~}}$ . Then, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\lVert\\theta^{t+1}-\\theta^{\\mathrm{pse}}\\right\\rVert_{2}^{2}=\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\left\\lVert P_{\\Omega_{i}}\\left[\\theta_{i}^{t}-\\gamma_{t}\\phi_{i}^{t}\\left(\\xi_{i}^{t};\\theta_{i}^{t},\\widehat{\\theta}_{i}^{t},\\lambda_{i}^{t}\\right)\\right]-P_{\\Omega_{i}}\\left[\\theta_{i}^{\\mathrm{pse}}-\\gamma_{t}\\Phi_{\\theta^{\\mathrm{pse}}}^{i,t}\\left(\\theta^{\\mathrm{pse}},\\lambda^{\\mathrm{pse}}\\right)\\right]\\right.}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathbb{E}\\left\\lVert\\theta^{t}-\\theta^{\\mathrm{pse}}\\right\\rVert_{2}^{2}+\\gamma_{t}^{2}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\left\\lVert\\phi_{i}^{t}\\left(\\xi_{i}^{t};\\theta_{i}^{t},\\widehat{\\theta}_{i}^{t},\\lambda_{i}^{t}\\right)-\\Phi_{\\theta^{\\mathrm{pse}}}^{i,t}\\left(\\theta^{\\mathrm{pse}},\\lambda^{\\mathrm{pse}}\\right)\\right\\rVert_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\left.-\\displaystyle2\\gamma_{t}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\left\\langle\\theta_{i}^{t}-\\theta_{i}^{\\mathrm{pse}},\\phi_{i}^{t}\\left(\\xi_{i}^{t};\\theta_{i}^{t},\\widehat{\\theta}_{i}^{t},\\lambda_{i}^{t}\\right)-\\Phi_{\\theta^{\\mathrm{pse}}}^{i,t}\\left(\\theta^{\\mathrm{pse}},\\lambda^{\\mathrm{pse}}\\right)\\right\\rangle\\cdot\\mathrm{\\boldmath~\\Gamma~}(\\mathrm{Al}\\,\\mathfrak{B})}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The second term on the right side of (A18) is handled as follows. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\varepsilon_{i}^{2}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\bigg\\|\\phi_{i}^{t}\\left(\\xi_{i}^{t},\\theta_{i}^{t},\\hat{\\theta}_{i}^{t},\\lambda_{i}^{t}\\right)-\\Phi_{\\theta^{\\!\\ i}=0}^{i,t}\\left(\\theta^{\\mathrm{pse}},\\lambda^{\\mathrm{pse}}\\right)\\bigg\\|_{2}^{2}}\\\\ &{=\\gamma_{i}^{2}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\bigg\\|\\phi_{i}^{t}\\left(\\xi_{i}^{t},\\theta_{i}^{t},\\hat{\\theta}_{i}^{t},\\lambda_{i}^{t}\\right)-\\Phi_{\\theta^{\\!\\ i}}^{t,t}\\left(\\theta_{i}^{t},\\hat{\\theta}_{i}^{t},\\lambda_{i}^{t}\\right)+\\Phi_{\\theta^{\\!\\ i}}^{t,t}\\left(\\theta_{i}^{t},\\hat{\\theta}_{i}^{t},\\lambda_{i}^{t}\\right)-\\Phi_{\\theta^{\\!\\ i}=0}^{i,t}\\left(\\theta^{\\mathrm{pse}},\\lambda^{\\mathrm{pse}}\\right)\\bigg\\|_{2}^{2}}\\\\ &{\\leq\\underbrace{3\\gamma_{i}^{2}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\bigg\\|\\phi_{i}^{t}\\left(\\xi_{i}^{t},\\theta_{i}^{t},\\hat{\\theta}_{i}^{t},\\lambda_{i}^{t}\\right)-\\Phi_{\\theta^{\\!\\ i}}^{t,t}\\left(\\theta_{i}^{t},\\hat{\\theta}_{i}^{t},\\lambda_{i}^{t}\\right)\\bigg\\|_{2}^{2}}_{(\\mathrm{~i~})}+\\underbrace{3\\gamma_{i}^{2}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\bigg\\|G_{\\theta^{\\!\\ i}}^{t}\\left(\\theta_{i}^{t},\\hat{\\theta}_{i}^{t}\\right)-G_{\\theta^{\\!\\ i\\infty}}^{t,t}\\left(\\theta^{\\mathrm{pse}},\\phi^{\\!\\theta^{\\!i}}\\right)\\bigg\\|_{2}^{2}}_{(\\mathrm{~i~})}}\\\\ &{\\qquad+\\underbrace{3\\gamma_{i}^{4}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\bigg\\|\\nabla g_{i}\\left(\\theta_{i}^{t}\\right)^{\\top}\\lambda_{i}^{t} \n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We have the following results on these three terms in the last inequality of (A19). ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(a)=3\\gamma_{t}^{2}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\left\\|\\nabla_{\\theta_{i}}J_{i}\\left(\\xi_{i}^{t};\\theta_{i}^{t},\\widehat{\\pmb{\\theta}}_{i}^{t}\\right)-G_{\\theta^{t}}^{(i)}\\left(\\pmb{\\theta}_{i}^{t},\\widehat{\\pmb{\\theta}}_{i}^{t}\\right)\\right\\|_{2}^{2}}\\\\ &{\\quad\\leq3\\gamma_{t}^{2}\\left(\\sigma_{0}^{2}+\\sigma_{1}^{2}\\mathbb{E}\\left\\|\\pmb{\\theta}^{t}-\\pmb{\\theta}^{\\mathrm{pse}}\\right\\|_{2}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{b)=3\\gamma_{t}^{2}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\left\\|G_{\\theta^{t}}^{(i)}\\left(\\theta_{i}^{t},\\widehat{\\theta}_{i}^{t}\\right)-G_{\\theta^{t}}^{(i)}\\left(\\theta^{t}\\right)+G_{\\theta^{t}}^{(i)}\\left(\\theta^{t}\\right)-G_{\\theta^{t}}^{(i)}\\left(\\theta^{\\mathrm{pse}}\\right)+G_{\\theta^{t}}^{(i)}\\left(\\theta^{\\mathrm{pse}}\\right)-G_{\\theta^{\\mathrm{pse}}}^{(i)}\\left(\\theta^{\\mathrm{pse}}\\right)\\right\\|}\\\\ &{\\quad\\le9\\gamma_{t}^{2}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\left(\\left\\|G_{\\theta^{t}}^{(i)}\\left(\\theta_{i}^{t},\\widehat{\\theta}_{i}^{t}\\right)-G_{\\theta^{t}}^{(i)}\\left(\\theta^{t}\\right)\\right\\|_{2}^{2}+\\left\\|G_{\\theta^{t}}^{(i)}\\left(\\theta^{t}\\right)-G_{\\theta^{t}}^{(i)}\\left(\\theta^{\\mathrm{pse}}\\right)\\right\\|_{2}^{2}+\\left\\|G_{\\theta^{t}}^{(i)}\\left(\\theta^{\\mathrm{pse}}\\right)-G_{\\theta^{\\mathrm{ps}}}^{(i)}\\left(\\theta^{\\mathrm{pse}}\\right)\\right\\|_{2}^{2}\\right.}\\\\ &{\\quad\\le9\\gamma_{t}^{2}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\left(L_{i}^{2}\\left\\|\\widehat{\\theta}_{i}^{t}-\\theta_{-i}^{t}\\right\\|_{2}^{2}+L_{i}^{2}\\left\\|\\theta^{t}-\\theta^{\\mathrm{pse}}\\right\\|_{2}^{2}+L_{i}^{2}\\varepsilon_{i}^{2}\\displaystyle\\operatorname*{max}_{j\\in[n]}p_{i j}\\left\\|\\theta^{t}-\\theta^{\\mathrm{pse}}\\right\\|_{2}^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the last inequality is based on Assumptions 2.2 and 2.4. Further, since the constriant function $g_{i}(\\cdot)$ is $G_{g}$ Lipschitz for all $i\\in[n]$ ,wehavethat ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left(c\\right)\\leq6\\gamma_{t}^{4}\\sum_{i=1}^{n}\\mathbb{E}\\left\\|\\nabla g_{i}\\left(\\pmb{\\theta}_{i}^{t}\\right)^{\\top}\\lambda_{i}^{t}\\right\\|_{2}^{2}+6\\gamma_{t}^{4}\\sum_{i=1}^{n}\\mathbb{E}\\left\\|\\nabla g_{i}\\left(\\pmb{\\theta}_{i}^{\\mathrm{pse}}\\right)^{\\top}\\lambda^{\\mathrm{pse}}\\right\\|_{2}^{2}}\\\\ {\\displaystyle\\leq6\\gamma_{t}^{2}G_{g}^{2}\\sum_{i=1}^{n}\\mathbb{E}\\|\\gamma_{t}\\pmb{\\lambda}_{i}^{t}\\|_{2}^{2}+6\\gamma_{t}^{4}n G_{g}^{2}\\|\\pmb{\\lambda}^{\\mathrm{pse}}\\|_{2}^{2}}\\\\ {\\displaystyle\\leq6\\gamma_{t}^{2}n B^{2}G_{g}^{2}+6\\gamma_{t}^{4}n G_{g}^{2}\\|\\pmb{\\lambda}^{\\mathrm{pse}}\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the last inequality is based on Lemma E.1. ", "page_idx": 19}, {"type": "text", "text": "Plugging the results of $(a),(b)$ , and $(c)$ into (A19) gives ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{t}^{2}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\left\\|\\phi_{i}^{t}\\left(\\xi_{i}^{t};\\theta_{i}^{t},\\widehat{\\theta}_{i}^{t},\\lambda_{i}^{t}\\right)-\\Phi_{\\theta^{\\mathrm{pse}}}^{i,t}\\left(\\theta^{\\mathrm{pse}},\\lambda^{\\mathrm{pse}}\\right)\\right\\|_{2}^{2}}\\\\ &{\\,\\,\\leq3\\gamma_{t}^{2}\\sigma_{0}^{2}+3\\gamma_{t}^{2}\\left(\\sigma_{1}^{2}+3\\displaystyle\\sum_{i=1}^{n}L_{i}^{2}\\left(1+\\varepsilon_{i}^{2}\\displaystyle\\operatorname*{max}_{j\\in[n]}p_{i j}\\right)\\right)\\mathbb{E}\\left\\|\\theta^{t}-\\theta^{\\mathrm{pse}}\\right\\|_{2}^{2}}\\\\ &{\\,\\,\\,\\,\\,+\\,9\\gamma_{t}^{2}\\displaystyle\\sum_{i=1}^{n}L_{i}^{2}\\mathbb{E}\\left\\|\\widehat{\\theta}_{i}^{t}-\\theta_{-i}^{t}\\right\\|_{2}^{2}+6\\gamma_{t}^{2}n B^{2}G_{g}^{2}+6\\gamma_{t}^{4}n G_{g}^{2}\\|\\lambda^{\\mathrm{pse}}\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Next, we deal with the last term on the right side of (A18). First, we have the following inequality: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb E\\left[\\phi_{i}^{t}\\left(\\pmb{\\xi}_{i}^{t};\\pmb{\\theta}_{i}^{t},\\widehat{\\pmb{\\theta}}_{i}^{t},\\lambda_{i}^{t}\\right)-\\Phi_{\\theta^{\\mathrm{pse}}}^{i,t}\\left(\\theta^{\\mathrm{pse}},\\lambda^{\\mathrm{pse}}\\right)\\right]}\\\\ &{\\ =\\mathbb E\\left[G_{\\theta^{t}}^{(i)}\\left(\\pmb{\\theta}_{i}^{t},\\widehat{\\pmb{\\theta}}_{i}^{t}\\right)-G_{\\theta^{\\mathrm{pse}}}^{(i)}\\left(\\theta^{\\mathrm{pse}}\\right)\\right]+\\gamma_{t}\\mathbb E\\left[\\nabla g_{i}\\left(\\pmb{\\theta}_{i}^{t}\\right)^{\\top}\\lambda_{i}^{t}-\\nabla g_{i}\\left(\\theta_{i}^{\\mathrm{pse}}\\right)^{\\top}\\lambda^{\\mathrm{pse}}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Moreover, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\quad-\\,2\\gamma_{t}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\left\\langle\\theta_{i}^{t}-\\theta_{i}^{\\mathrm{pse}},G_{\\theta^{t}}^{(i)}\\left(\\theta_{i}^{t},\\widehat{\\theta}_{i}^{t}\\right)-G_{\\theta^{\\mathrm{pse}}}^{(i)}\\left(\\theta^{\\mathrm{pse}}\\right)\\right\\rangle}&\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{=-2\\gamma_{t}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\left\\langle\\theta_{i}^{t}-\\theta_{i}^{\\mathrm{pse}},G_{\\theta^{t}}^{(i)}\\left(\\theta_{i}^{t},\\widehat{\\theta}_{i}^{t}\\right)-G_{\\theta^{t}}^{(i)}\\left(\\theta^{t}\\right)\\right\\rangle-2\\gamma_{t}\\mathbb{E}\\left\\langle\\theta^{t}-\\theta^{\\mathrm{pse}},G_{\\theta^{t}}\\left(\\theta^{t}\\right)-G_{\\theta^{t}}\\left(\\theta^{\\mathrm{pse}}\\right)\\right\\rangle}&\\\\ &{\\quad\\quad-\\,2\\gamma_{t}\\mathbb{E}\\left\\langle\\theta^{t}-\\theta^{\\mathrm{pse}},G_{\\theta^{t}}\\left(\\theta^{\\mathrm{pse}}\\right)-G_{\\theta^{\\mathrm{pse}}}\\left(\\theta^{\\mathrm{pse}}\\right)\\right\\rangle}&\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\times}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the last inequality is from Assumptions 2.2, 2.3, 2.4 and the Cauchy-Schwarz inequality. Further,wehave ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\;2\\gamma_{t}^{2}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\left\\langle\\theta_{i}^{t}-\\theta_{i}^{\\mathrm{pse}},\\nabla g_{i}\\left(\\theta_{i}^{t}\\right)^{\\top}\\lambda_{i}^{t}-\\nabla g_{i}\\left(\\theta_{i}^{\\mathrm{pse}}\\right)^{\\top}\\lambda^{\\mathrm{pee}}\\right\\rangle}\\\\ &{\\leq2\\gamma_{t}^{2}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\left\\langle\\theta_{i}^{\\mathrm{pse}}-\\theta_{i}^{t},\\nabla g_{i}\\left(\\theta_{i}^{t}\\right)^{\\top}\\lambda_{i}^{t}\\right\\rangle+4\\gamma_{t}^{2}C G_{g}\\|\\lambda^{\\mathrm{pee}}\\|_{2}}\\\\ &{\\leq2\\gamma_{t}^{2}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\left[\\left(g_{i}\\left(\\theta_{i}^{\\mathrm{pse}}\\right)-g_{i}\\left(\\theta_{i}^{t}\\right)\\right)^{\\top}\\lambda_{i}^{t}\\right]+4\\gamma_{t}^{2}C G_{g}\\|\\lambda^{\\mathrm{pee}}\\|_{2}}\\\\ &{\\leq2\\gamma_{t}^{2}\\mathbb{E}\\left[\\displaystyle\\sum_{i=1}^{n}g_{i}\\left(\\theta_{i}^{\\mathrm{pse}}\\right)^{\\top}\\left(\\lambda_{i}^{t}-\\overline{{\\lambda}}^{t}\\right)+g\\left(\\theta^{\\mathrm{pse}}\\right)^{\\top}\\overline{{\\lambda}}^{t}-\\displaystyle\\sum_{i=1}^{n}g_{i}\\left(\\theta_{i}^{t}\\right)^{\\top}\\lambda_{i}^{t}\\right]+4\\gamma_{t}^{2}C G_{g}\\|\\lambda^{\\mathrm{pee}}\\|_{2}}\\\\ &{\\leq2\\gamma_{t}^{2}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\left[\\|g_{i}\\left(\\theta_{i}^{\\mathrm{pse}}\\right)\\|_{2}\\left\\|\\lambda_{i}^{t}-\\overline{{\\lambda}}^{t}\\right\\|_{2}-g_{i}\\left(\\theta_{i}^{t}\\right)^{\\top}\\lambda_{i}^{t}\\right]+4\\gamma_{t}^{2}C G_{g}\\|\\lambda^{\\mathrm{pee}}\\|_{2},\\qquad\\qquad(\\mathrm{AZ})}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the last inequality uses the fact that $g\\left(\\theta^{\\mathrm{pse}}\\right)^{\\top}\\overline{{\\lambda}}^{t}\\leq0$ ", "page_idx": 19}, {"type": "text", "text": "Define $\\begin{array}{r}{\\widetilde{\\mu}:=\\mu-\\sum_{i=1}^{n}L_{i}\\varepsilon_{i}\\operatorname*{max}_{j\\in[n]}\\sqrt{p_{i j}}}\\end{array}$ $\\begin{array}{r}{\\nu:=3\\left(\\sigma_{1}^{2}+3\\sum_{i=1}^{n}L_{i}^{2}\\left(1+\\varepsilon_{i}^{2}\\operatorname*{max}_{j\\in[n]}p_{i j}\\right)\\right)}\\end{array}$ ), and $\\pi:=3\\sigma_{0}^{2}+6n B^{2}G_{g}^{2}+6n G_{g}^{2}\\|\\lambda^{\\mathrm{pse}}\\|_{2}^{2}+4C G_{g}\\|\\lambda^{\\mathrm{pse}}\\|$ . Plugging the results in (A20), (A21), and ", "page_idx": 19}, {"type": "text", "text": "(A22) into (A18) yields ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\|\\theta^{t+1}-\\theta^{\\mathrm{pse}}\\right\\|_{2}^{2}}\\\\ &{\\leq\\left(1-2\\gamma_{t}\\widetilde{\\mu}+\\nu\\gamma_{t}^{2}\\right)\\mathbb{E}\\left\\|\\theta^{t}-\\theta^{\\mathrm{pse}}\\right\\|_{2}^{2}+4C\\gamma_{t}\\displaystyle\\sum_{i=1}^{n}L_{i}\\mathbb{E}\\left\\|\\widehat{\\theta}_{i}^{t}-\\theta_{-i}^{t}\\right\\|_{2}+9\\gamma_{t}^{2}\\displaystyle\\sum_{i=1}^{n}L_{i}^{2}\\mathbb{E}\\left\\|\\widehat{\\theta}_{i}^{t}-\\theta_{-i}^{t}\\right\\|_{2}^{2}}\\\\ &{\\quad+\\displaystyle2\\gamma_{t}^{2}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\left[B\\left\\|\\lambda_{i}^{t}-\\overline{{\\lambda}}^{t}\\right\\|_{2}-g_{i}\\left(\\theta_{i}^{t}\\right)^{\\top}\\lambda_{i}^{t}\\right]+\\pi\\gamma_{t}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let $\\begin{array}{r}{\\operatorname*{sup}_{t\\geq1}\\gamma_{t}\\leq\\frac{\\widetilde\\mu}{\\nu}}\\end{array}$ , then $1-2\\widetilde{\\mu}\\gamma_{t}+\\nu\\gamma_{t}^{2}\\leq1-\\widetilde{\\mu}\\gamma_{t}$ Thus, we have ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\because}\\\\ &{\\leq\\displaystyle\\frac{1}{\\widetilde{\\mu}\\gamma_{t}}\\left(\\mathbb{E}\\left\\|\\theta^{t}-\\theta^{\\mathrm{pse}}\\right\\|_{2}^{2}-\\mathbb{E}\\left\\|\\theta^{t+1}-\\theta^{\\mathrm{pse}}\\right\\|_{2}^{2}\\right)+\\displaystyle\\frac{4C}{\\widetilde{\\mu}}\\sum_{i=1}^{n}L_{i}\\mathbb{E}\\left\\|\\widehat{\\theta}_{i}^{t}-\\theta_{-i}^{t}\\right\\|_{2}}\\\\ &{\\quad+\\displaystyle\\frac{9\\gamma_{t}}{\\widetilde{\\mu}}\\sum_{i=1}^{n}L_{i}^{2}\\mathbb{E}\\left\\|\\widehat{\\theta}_{i}^{t}-\\theta_{-i}^{t}\\right\\|_{2}^{2}+\\displaystyle\\frac{2\\gamma_{t}}{\\widetilde{\\mu}}\\sum_{i=1}^{n}\\mathbb{E}\\left[B\\left\\|\\lambda_{i}^{t}-\\overline{{\\lambda}}^{t}\\right\\|_{2}-g_{i}\\left(\\theta_{i}^{t}\\right)^{\\top}\\lambda_{i}^{t}\\right]+\\frac{\\pi\\gamma_{t}}{\\widetilde{\\mu}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Summing the above inequality over $t\\in[T]$ and plugging into the result of Lemma E.2 yields ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{i=1}^{T}\\mathbb{E}\\left\\|\\theta^{t}-\\theta^{\\mathrm{pse}}\\right\\|_{2}^{2}\\leq\\displaystyle\\sum_{t=1}^{T}\\displaystyle\\sum_{\\tilde{\\mu}\\gamma_{t}}^{1}\\Big(\\mathbb{E}\\left\\|\\theta^{t}-\\theta^{\\mathrm{pse}}\\right\\|_{2}^{2}-\\mathbb{E}\\left\\|\\theta^{t+1}-\\theta^{\\mathrm{pse}}\\right\\|_{2}^{2}\\Big)+\\displaystyle\\frac{4C}{\\tilde{\\mu}}\\displaystyle\\sum_{t=1}^{T}\\sum_{i=1}^{n}L_{i}\\mathbb{E}\\left\\|\\hat{\\theta}_{i}^{t}-\\theta_{-}^{t}\\right.}\\\\ &{\\displaystyle\\left.+\\frac{9}{\\tilde{\\mu}}\\sum_{t=1}^{T}\\gamma_{t}\\sum_{i=1}^{T}L_{i}^{2}\\mathbb{E}\\left\\|\\hat{\\theta}_{i}^{t}-\\theta_{-i}^{t}\\right\\|_{2}^{2}+\\displaystyle\\frac{2}{\\tilde{\\mu}}\\left(3+2\\sqrt{n}\\right)B\\sum_{t=1}^{T}\\gamma_{t}\\sum_{i=1}^{n}\\mathbb{E}\\left\\|\\lambda_{i}^{t}-\\overline{{\\lambda}}^{t}\\right\\|_{2}\\right.}\\\\ &{\\displaystyle\\left.+\\frac{9}{\\tilde{\\mu}}\\displaystyle\\sum_{t=1}^{T}\\sum_{i=1}^{n}\\left\\|\\lambda_{i}^{t}-\\overline{{\\lambda}}^{t}\\right\\|_{2}^{2}+\\displaystyle\\frac{\\pi}{\\tilde{\\mu}}\\displaystyle\\sum_{t=1}^{T}\\gamma_{t+}+\\frac{8n B^{2}}{\\tilde{\\mu}}\\displaystyle\\sum_{t=1}^{T}\\gamma_{t}^{2}\\right.}\\\\ &{\\displaystyle\\left.-\\,\\displaystyle\\frac{2}{\\tilde{\\mu}}\\sum_{t=1}^{T}\\sum_{i=1}^{n}\\gamma_{t}\\Delta^{\\tau}g_{i}^{t}(\\theta_{i}^{t})+\\frac{\\pi}{\\tilde{\\mu}}\\left(1+\\displaystyle\\sum_{t=1}^{T}\\gamma_{t}^{2}\\right)\\|\\lambda\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $\\left\\lVert\\theta^{t}-\\theta^{\\mathrm{pse}}\\right\\rVert_{2}^{2}\\leq4C^{2}$ , we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\frac{1}{\\gamma_{t}}\\left(\\mathbb{E}\\left\\|\\theta^{t}-\\theta^{\\mathrm{pse}}\\right\\|_{2}^{2}-\\mathbb{E}\\left\\|\\theta^{t+1}-\\theta^{\\mathrm{pse}}\\right\\|_{2}^{2}\\right)}\\\\ &{\\displaystyle=\\frac{1}{\\gamma_{1}}\\mathbb{E}\\left\\|\\theta^{1}-\\theta^{\\mathrm{pse}}\\right\\|_{2}^{2}-\\frac{1}{\\gamma_{T}}\\mathbb{E}\\left\\|\\theta^{T+1}-\\theta^{\\mathrm{pse}}\\right\\|_{2}^{2}+\\displaystyle\\sum_{t=2}^{T}\\left(\\frac{1}{\\gamma_{t}}-\\frac{1}{\\gamma_{t-1}}\\right)\\mathbb{E}\\left\\|\\theta^{t}-\\theta^{\\mathrm{pse}}\\right\\|_{2}^{2}}\\\\ &{\\displaystyle\\leq\\frac{1}{\\gamma_{1}}4C^{2}+\\displaystyle\\sum_{t=2}^{T}\\left(\\frac{1}{\\gamma_{t}}-\\frac{1}{\\gamma_{t-1}}\\right)4C^{2}\\leq\\frac{4C^{2}}{\\gamma_{T}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where in th las iequality s aed on the fathat $\\begin{array}{r}{\\frac{1}{\\gamma_{t}}-\\frac{1}{\\gamma_{t-1}}\\geq0}\\end{array}$ because $\\gamma_{t}$ is a non-incrasing sequence. Further, we have the following relations: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{n}L_{i}^{2}\\mathbb{E}\\left\\|\\hat{\\theta}_{i}^{t}-\\theta_{-i}^{t}\\right\\|_{2}^{2}\\leq\\operatorname*{max}_{i}L_{i}\\displaystyle\\sum_{i=1}^{n}\\sum_{h\\neq i}\\left\\|\\hat{\\theta}_{i h}^{t}-\\theta_{h}^{t}\\right\\|_{2}^{2}=\\operatorname*{max}_{i}L_{i}\\displaystyle\\sum_{h=1}^{n}\\|e_{h}^{t}\\|}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\displaystyle\\sum_{i=1}^{n}L_{i}\\mathbb{E}\\left\\|\\hat{\\theta}_{i}^{t}-\\theta_{-i}^{t}\\right\\|_{2}\\leq\\operatorname*{max}_{i}L_{i}\\displaystyle\\sum_{i=1}^{n}\\sqrt{\\displaystyle\\sum_{h\\neq i}\\left\\|\\hat{\\theta}_{i h}^{t}-\\theta_{h}^{t}\\right\\|_{2}^{2}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\operatorname*{max}_{i}L_{i}\\sqrt{\\displaystyle\\sum_{i=1}^{n}\\sum_{h\\neq i}\\left\\|\\hat{\\theta}_{i h}^{t}-\\theta_{h}^{t}\\right\\|_{2}^{2}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\operatorname*{max}_{i}L_{i}\\sqrt{n}\\displaystyle\\sum_{h=1}^{n}\\|e_{h}^{t}\\|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last inequality is based on the fact that ${\\sqrt{a+b+c}}\\leq{\\sqrt{a}}+{\\sqrt{b}}+{\\sqrt{c}}$ for any $a,b,c\\geq0$ Plugging (A25), (A26) and (A27) into (A24) and utilizing the results in Lemmas E.4 and E.5, we havethat ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\left\\|\\pmb{\\theta}^{t}-\\pmb{\\theta}^{\\mathrm{pse}}\\right\\|_{2}^{2}+\\frac{2}{\\widetilde{\\mu}}\\sum_{t=1}^{T}\\sum_{i=1}^{n}\\gamma_{t}\\pmb{\\lambda}^{\\top}\\pmb{g}_{i}\\left(\\pmb{\\theta}_{i}^{t}\\right)-\\frac{n}{\\widetilde{\\mu}}\\left(1+\\displaystyle\\sum_{t=1}^{T}\\gamma_{t}^{2}\\right)\\|\\pmb{\\lambda}\\|_{2}^{2}}\\\\ &{\\le\\mathcal{O}\\left(\\frac{1}{\\widetilde{\\mu}\\gamma_{T}}+\\displaystyle\\frac{1}{\\widetilde{\\mu}}\\sum_{t=1}^{T}\\gamma_{t}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since any $\\lambda\\in\\mathbb{R}_{+}^{m}$ satisfies the above nequality, by seting $\\begin{array}{r}{\\lambda=\\frac{\\left[\\sum_{t=1}^{T}\\gamma_{t}\\sum_{i=1}^{n}g_{i}\\left(\\pmb{\\theta}_{i}^{t}\\right)\\right]_{+}}{n\\left(1+\\sum_{t=1}^{T}\\gamma_{t}^{2}\\right)}}\\end{array}$ we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{2}{\\widetilde{\\mu}}\\lambda^{\\top}\\left(\\sum_{t=1}^{T}\\gamma_{t}\\sum_{i=1}^{n}\\pmb{g}_{i}\\left(\\pmb{\\theta}_{i}^{t}\\right)\\right)-\\frac{n}{\\widetilde{\\mu}}\\left(1+\\sum_{t=1}^{T}\\gamma_{t}^{2}\\right)\\left\\Vert\\pmb{\\lambda}\\right\\Vert_{2}^{2}=\\frac{\\left\\Vert\\left[\\sum_{t=1}^{T}\\gamma_{t}\\sum_{i=1}^{n}\\pmb{g}_{i}\\left(\\pmb{\\theta}_{i}^{t}\\right)\\right]_{+}\\right\\Vert_{2}^{2}}{\\widetilde{\\mu}n\\left(1+\\sum_{t=1}^{T}\\gamma_{t}^{2}\\right)}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "As the terms in (A29) is non-negative, omitting it in (A28) gives ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}\\left\\lVert\\pmb{\\theta}^{t}-\\pmb{\\theta}^{\\mathrm{pse}}\\right\\rVert_{2}^{2}\\leq\\mathcal{O}\\left(\\frac{1}{\\widetilde{\\mu}\\gamma_{T}}+\\frac{1}{\\widetilde{\\mu}}\\sum_{t=1}^{T}\\gamma_{t}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Furthermore, since $\\mathbb{E}_{\\xi_{i}\\sim\\mathcal{D}(\\theta^{\\mathrm{pse}})}\\big|J_{i}(\\xi_{i};\\theta_{i}^{t},\\theta_{-i}^{\\mathrm{pse}})-J_{i}(\\xi_{i};\\theta^{\\mathrm{pse}})\\big|\\le G_{i}\\left\\|\\theta_{i}^{t}-\\theta_{i}^{\\mathrm{pse}}\\right\\|_{2}$ for any $i\\in[n]$ we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathcal R}_{i}(T)=\\displaystyle\\sum_{t=1}^{T}\\left(\\mathbb E_{\\xi_{i}\\sim\\mathcal D(\\theta^{\\mathrm{pse}})}\\left[J\\left(\\xi_{i};\\theta_{i}^{t},\\theta_{-i}^{\\mathrm{pse}}\\right)-J\\left(\\xi_{i};\\theta^{\\mathrm{pse}}\\right)\\right]\\right)}\\\\ &{\\qquad\\le G_{i}\\displaystyle\\sum_{t=1}^{T}\\|\\theta_{i}^{t}-\\theta_{i}^{\\mathrm{pse}}\\|_{2}}\\\\ &{\\qquad\\le G_{i}\\sqrt{T\\displaystyle\\sum_{t=1}^{T}\\|\\theta_{i}^{t}-\\theta_{i}^{\\mathrm{pse}}\\|_{2}^{2}}}\\\\ &{\\qquad\\le\\mathcal O\\left(\\sqrt{\\displaystyle\\frac{T}{\\tilde{\\mu}}\\left(\\frac{1}{\\gamma T}+\\displaystyle\\sum_{t=1}^{T}\\gamma_{t}\\right)}\\right),\\forall i\\in[n].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "On the other  hand,  plugging(A29)  into  (A28) and  omitting  the  non-negtive  term $\\begin{array}{r}{\\sum_{t=1}^{T}\\mathbb{E}\\left\\lVert\\pmb{\\theta}^{t}-\\pmb{\\theta}^{\\mathrm{pse}}\\right\\rVert_{2}^{2}}\\end{array}$ wehave ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\left\\|\\left[\\sum_{t=1}^{T}\\gamma_{t}\\sum_{i=1}^{n}g_{i}\\left(\\pmb{\\theta}_{i}^{t}\\right)\\right]_{+}\\right\\|_{2}^{2}}{\\widetilde{\\mu}n\\left(1+\\sum_{t=1}^{T}\\gamma_{t}^{2}\\right)}\\leq\\mathcal{O}\\left(\\frac{1}{\\widetilde{\\mu}\\gamma_{T}}+\\frac{1}{\\widetilde{\\mu}}\\sum_{t=1}^{T}\\gamma_{t}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\|\\left[\\sum_{t=1}^{T}\\gamma_{t}\\sum_{i=1}^{n}\\pmb{g}_{i}\\left(\\pmb{\\theta}_{i}^{t}\\right)\\right]_{+}\\right\\|_{2}\\le\\mathcal{O}\\left(\\sqrt{\\left(\\frac{1}{\\gamma_{T}}+\\sum_{t=1}^{T}\\gamma_{t}\\right)\\left(1+\\sum_{t=1}^{T}\\gamma_{t}^{2}\\right)}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then, we prove that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{R}_{g}(T)\\leq\\mathcal{O}\\left(\\frac{1}{\\gamma_{T}}\\sqrt{\\left(\\frac{1}{\\gamma_{T}}+\\sum_{t=1}^{T}\\gamma_{t}\\right)\\left(1+\\sum_{t=1}^{T}\\gamma_{t}^{2}\\right)}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "E.1 Proof of Lemma E.1 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "From the update rule of the dual variables, for any $\\lambda_{i}^{t}\\in\\mathbb{R}_{+}^{m}$ $i\\in[n]$ , and $t\\in[T]$ , we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{i=1}^{n}\\left\\|X_{i}^{t+1}\\right\\|_{2}^{2}\\leq\\sum_{i=1}^{n}\\left\\|\\sum_{j=1}^{n}a_{i j}\\left[(1-\\gamma_{i}^{2})\\lambda_{j}^{t}+\\gamma_{i}g_{i}(\\theta_{i}^{t})\\right]\\right\\|_{2}^{2}}}\\\\ &{}&{\\leq\\sum_{i=1}^{n}\\sum_{j=1}^{n}a_{i j}\\left\\|(1-\\gamma_{i}^{2})\\lambda_{j}^{t}+\\gamma_{i}^{2}\\frac{g_{i}(\\theta_{i}^{t})}{\\gamma_{i}}\\right\\|_{2}^{2}}\\\\ &{}&{\\leq\\sum_{i=1}^{n}\\sum_{j=1}^{n}a_{i j}\\left[(1-\\gamma_{i}^{2})\\left\\|X_{i}^{t}\\right\\|_{2}^{2}+\\left\\|g_{i}(\\theta_{i}^{t})\\right\\|_{2}^{2}\\right]}\\\\ &{}&{\\leq(1-\\gamma_{i}^{2})\\sum_{i=1}^{n}\\left\\|X_{i}^{t}\\right\\|_{2}^{2}+\\sum_{i=1}^{n}\\left\\|g_{i}(\\theta_{i}^{t})\\right\\|_{2}^{2}}\\\\ &{}&{\\leq(1-\\gamma_{i}^{2})\\sum_{i=1}^{n}\\left\\|X_{i}^{t}\\right\\|_{2}^{2}+n B^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We next bound $\\begin{array}{r}{\\sum_{i=1}^{n}\\left\\|\\pmb{\\lambda}_{i}^{t}\\right\\|_{2}^{2},\\forall t\\in[T]}\\end{array}$ by deduction. First, since $\\lambda_{i}^{1}=\\mathbf{0}$ $\\gamma_{1}\\leq1$ and $\\lVert\\pmb{g}_{i}(\\pmb{\\theta}_{i}^{1})\\rVert_{2}^{2}\\leq$ $B^{2},\\,\\forall i\\ \\in\\ [n]$ , we have that $\\begin{array}{r}{\\sum_{i=1}^{n}\\left\\|\\boldsymbol{\\lambda}_{i}^{2}\\right\\|_{2}^{2}\\;\\leq\\;\\sum_{i=1}^{n}\\left\\|g_{i}(\\pmb{\\theta}_{i}^{1})\\right\\|_{2}^{2}\\;\\leq\\;n B^{2}\\;\\leq\\;\\frac{n B^{2}}{\\gamma_{1}^{2}}}\\end{array}$ Assume that $\\begin{array}{r}{\\sum_{i=1}^{n}\\|\\boldsymbol{\\lambda}_{i}^{t}\\|_{2}^{2}\\leq\\frac{n B^{2}}{\\gamma_{t-1}^{2}}}\\end{array}$ Since $\\{\\gamma_{t}\\}_{t\\in[T]}$ $\\begin{array}{r}{\\sum_{i=1}^{n}\\|{\\bf\\lambda}_{i}^{t}\\|_{2}^{\\bar{2}}\\leq\\frac{n B^{2}}{\\gamma_{t-1}^{2}}\\leq\\frac{n B^{2}}{\\gamma_{t}^{2}}}\\end{array}$ and thus $\\begin{array}{r}{\\sum_{i=1}^{n}\\left\\|{\\lambda_{i}^{t+1}}\\right\\|_{2}^{2}\\,\\le\\,(1-\\gamma_{t}^{2})\\frac{n B^{2}}{\\gamma_{t}^{2}}+n B^{2}\\,=\\,\\frac{n B^{2}}{\\gamma_{t}^{2}}}\\end{array}$ . Therefore, for any $t\\,\\in\\,[T]$ , we have $\\begin{array}{r}{\\sum_{i=1}^{n}\\|{\\boldsymbol{\\lambda}}_{i}^{t+1}\\|_{2}^{2}\\leq\\frac{n B^{2}}{\\gamma_{t}^{2}}\\leq\\frac{n B^{2}}{\\gamma_{t+1}^{2}}}\\end{array}$ i.e, $\\begin{array}{r}{\\sum_{i=1}^{n}\\|\\gamma_{t}\\lambda_{i}^{t}\\|_{2}^{2}\\leq n B^{2}}\\end{array}$ , which completes the proof. ", "page_idx": 22}, {"type": "text", "text": "E.2Proof of Lemma E.2 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "From the update rule of the dual variables $\\lambda_{i}$ , for any $\\lambda\\in\\mathbb{R}_{+}^{m}$ , we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{i=1}^{n}\\left\\|{\\boldsymbol X}_{i}^{t+1}-{\\boldsymbol X}_{1}^{1}\\right\\|_{2}^{2}=\\displaystyle\\sum_{i=1}^{n}\\left\\|\\left[\\left(1-\\gamma_{i}^{2}\\right)\\sum_{j\\in{\\mathcal N}_{i}}a_{i j}\\lambda_{j}^{t}+\\gamma_{i}g_{i}\\left(\\theta_{i}^{t}\\right)\\right]_{+}-{\\boldsymbol X}\\right\\|_{2}^{2}}\\\\ {\\displaystyle\\leq\\sum_{i=1}^{n}\\left\\|\\left(1-\\gamma_{i}^{2}\\right)\\sum_{j\\in{\\mathcal N}_{i}}a_{i j}\\left(\\lambda_{j}^{t}-\\lambda_{i}^{t}\\right)+\\left(\\lambda_{i}^{t}-\\lambda\\right)+\\gamma_{i}\\left(g_{i}\\left(\\theta_{i}^{t}\\right)-\\gamma_{i}\\lambda_{i}^{t}\\right)\\right\\|_{2}^{2}}\\\\ {\\displaystyle\\leq\\sum_{i=1}^{n}\\left(\\sum_{j\\in{\\mathcal N}_{i}}a_{i j}\\left\\|{\\boldsymbol X}_{j}^{t}-{\\boldsymbol X}_{i}^{t}\\right\\|_{2}^{2}+\\left\\|{\\boldsymbol X}_{i}^{t}-{\\boldsymbol X}_{1}^{t}\\right\\|_{2}^{2}+\\gamma_{i}^{2}\\left\\|\\mathbf{g}_{i}\\left(\\theta_{i}^{t}\\right)-\\gamma_{i}{\\boldsymbol X}_{i}^{t}\\right\\|_{2}^{2}\\right.}\\\\ {\\displaystyle\\left.+2\\sum_{j\\in{\\mathcal N}_{i}}a_{i j}\\left\\langle{\\boldsymbol X}_{j}^{t}-{\\boldsymbol X}_{i}^{t},{\\boldsymbol X}_{i}^{t}-\\lambda\\right\\rangle+2\\gamma_{i}\\left\\langle{\\boldsymbol X}_{i}^{t}-{\\boldsymbol X}_{2}^{t},\\theta_{i}\\left(\\theta_{i}^{t}\\right)-\\gamma_{i}{\\boldsymbol X}_{i}^{t}\\right\\rangle\\right.}\\\\ {\\displaystyle+2\\gamma_{i}\\sum_{j\\in{\\mathcal N}_{i}}a_{i j}\\left\\|{\\boldsymbol X}_{j}^{t}-{\\boldsymbol X}_{i}^{t}\\right\\|_{2}\\left\\|\\theta_{i}\\left(\\theta_{i}^{t}\\right)-\\gamma_{i}{\\boldsymbol X}_{i}^{t}\\right\\|_{2}\\right),}&{~\\mathrm{(A}{\\boldsymbol X})}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we use the fact $1-\\gamma_{t}^{2}\\leq1$ in (A30). Next, we simplify the terms in (A30). First, based on the inequality $(a-b)^{2}\\leq2\\left(a^{2}+b^{2}\\right)$ for any $a,b\\ge0$ , we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\sum_{j=1}^{n}a_{i j}\\left\\|\\boldsymbol{\\lambda}_{j}^{t}-\\boldsymbol{\\lambda}_{i}^{t}\\right\\|_{2}^{2}=\\sum_{i=1}^{n}\\sum_{j=1}^{n}a_{i j}\\left(\\left\\|\\left(\\boldsymbol{\\lambda}_{j}^{t}-\\overline{{\\boldsymbol{\\lambda}}}^{t}\\right)-\\left(\\boldsymbol{\\lambda}_{i}^{t}-\\overline{{\\boldsymbol{\\lambda}}}^{t}\\right)\\right\\|_{2}^{2}\\right)\\leq4\\sum_{i=1}^{n}\\left\\|\\boldsymbol{\\lambda}_{i}^{t}-\\overline{{\\boldsymbol{\\lambda}}}^{t}\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In addition, with the result in Lemma E.1, we know that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{n}\\left\\|g_{i}\\left(\\theta_{i}^{t}\\right)-\\gamma_{t}\\lambda_{i}^{t}\\right\\|_{2}^{2}\\leq2\\displaystyle\\sum_{i=1}^{n}\\left\\|g_{i}\\left(\\theta_{i}^{t}\\right)\\right\\|_{2}^{2}+2\\displaystyle\\sum_{i=1}^{n}\\left\\|\\gamma_{t}\\lambda_{i}^{t}\\right\\|_{2}^{2}\\leq2n B^{2}+2n B^{2}=4n B^{2},}\\\\ &{\\left\\|g_{i}\\left(\\theta_{i}^{t}\\right)-\\gamma_{t}\\lambda_{i}^{t}\\right\\|_{2}\\leq\\left\\|g_{i}\\left(\\theta_{i}^{t}\\right)\\right\\|_{2}+\\left\\|\\gamma_{t}\\lambda_{i}^{t}\\right\\|_{2}\\leq B+\\sqrt{n}B=(1+\\sqrt{n})B.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Moreover, based on the fact that $\\begin{array}{r}{\\sum_{i=1}^{n}\\sum_{j=1}^{n}a_{i j}\\left\\langle\\lambda_{j}^{t}\\;-\\lambda_{i}^{t},z\\right\\rangle=0}\\end{array}$ for any $z\\in\\mathbb{R}^{m}$ , we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{i=1}^{n}\\sum_{j=1}^{n}a_{i j}\\left\\langle\\lambda_{j}^{t}-\\lambda_{i}^{t},\\lambda_{i}^{t}-\\lambda\\right\\rangle=\\sum_{i=1}^{n}\\sum_{j=1}^{n}a_{i j}\\left\\langle\\lambda_{j}^{t}-\\lambda_{i}^{t},\\lambda_{i}^{t}-\\overline{{\\lambda}}^{t}\\right\\rangle}}\\\\ &{}&{\\leq\\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}a_{i j}\\left(\\left\\|\\lambda_{j}^{t}-\\lambda_{i}^{t}\\right\\|_{2}^{2}+\\left\\|\\lambda_{i}^{t}-\\overline{{\\lambda}}^{t}\\right\\|_{2}^{2}\\right)}\\\\ &{}&{\\leq\\frac{5}{2}\\sum_{i=1}^{n}\\left\\|\\lambda_{i}^{t}-\\overline{{\\lambda}}^{t}\\right\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Furthermore, notice that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle\\lambda_{i}^{t}-\\lambda,g_{i}\\left(\\theta_{i}^{t}\\right)-\\gamma_{t}\\lambda_{i}^{t}\\right\\rangle=\\left\\langle\\lambda_{i}^{t}-\\lambda,g_{i}\\left(\\theta_{i}^{t}\\right)\\right\\rangle-\\gamma_{t}\\|\\lambda_{i}^{t}\\|_{2}^{2}+\\gamma_{t}\\lambda^{\\top}\\lambda_{i}^{t}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\left\\langle\\lambda_{i}^{t}-\\lambda,g_{i}\\left(\\theta_{i}^{t}\\right)\\right\\rangle+\\frac{\\gamma_{t}}{2}\\left(\\|\\lambda\\|_{2}^{2}-\\|\\lambda_{i}^{t}\\|_{2}^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the last inequality follows that $\\begin{array}{r}{\\lambda^{\\top}\\lambda_{i}^{t}=\\frac{1}{2}(\\|\\pmb{\\lambda}\\|_{2}^{2}+\\|\\pmb{\\lambda}_{i}^{t}\\|_{2}^{2})}\\end{array}$ . We also have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\sum_{j=1}^{n}a_{i j}\\left\\|\\pmb{\\lambda}_{j}^{t}-\\pmb{\\lambda}_{i}^{t}\\right\\|_{2}\\leq2\\sum_{i=1}^{n}\\left\\|\\pmb{\\lambda}_{i}^{t}-\\overline{{\\pmb{\\lambda}}}^{t}\\right\\|_{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Plugging all the above results into (A30), we obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{n}\\left\\|\\pmb{\\lambda}_{i}^{t+1}-\\pmb{\\lambda}\\right\\|_{2}^{2}\\leq\\displaystyle\\sum_{i=1}^{n}\\left(\\left\\|\\pmb{\\lambda}_{i}^{t}-\\pmb{\\lambda}\\right\\|_{2}^{2}+9\\left\\|\\pmb{\\lambda}_{i}^{t}-\\pmb{\\overline{{\\lambda}}}^{t}\\right\\|_{2}^{2}\\right.}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\left.+\\displaystyle2\\gamma_{t}\\left\\langle\\pmb{\\lambda}_{i}^{t}-\\pmb{\\lambda},\\pmb{g}_{i}\\left(\\pmb{\\theta}_{i}^{t}\\right)\\right\\rangle+\\gamma_{t}^{2}\\left(\\|\\pmb{\\lambda}\\|_{2}^{2}-\\left\\|\\pmb{\\lambda}_{i}^{t}\\right\\|_{2}^{2}\\right)\\right.}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\left.+4\\gamma_{t}(1+\\sqrt{n})B\\left\\|\\pmb{\\lambda}_{i}^{t}-\\pmb{\\overline{{\\lambda}}}^{t}\\right\\|_{2}\\right)+4n B^{2}\\gamma_{t}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Rearranging the terms in the above inequality and summing over $t\\in[T]$ gives ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\displaystyle\\sum_{i=1}^{n}\\gamma_{i}\\left<\\lambda_{i}^{t}-\\lambda,g_{i}\\left(\\theta_{i}^{t}\\right)\\right>+\\displaystyle\\sum_{t=1}^{T}\\displaystyle\\frac{n\\gamma_{i}^{t}}{2}\\|\\lambda\\|_{2}^{2}}\\\\ &{\\geq\\displaystyle\\frac{1}{2}\\displaystyle\\sum_{t=1}^{T}\\sum_{i=1}^{n}\\left(\\left\\|\\lambda_{i}^{t+1}-\\lambda\\right\\|_{2}^{2}-\\left\\|\\lambda_{i}^{t}-\\lambda\\right\\|_{2}^{2}\\right)}\\\\ &{\\displaystyle~~-\\frac{9}{2}\\displaystyle\\sum_{t=1}^{T}\\displaystyle\\sum_{i=1}^{n}\\left\\|\\lambda_{i}^{t}-\\overline{{\\lambda}}^{t}\\right\\|_{2}^{2}-4n B^{2}\\displaystyle\\sum_{t=1}^{T}\\gamma_{t}^{2}}\\\\ &{-\\displaystyle2(1+\\sqrt{n})B\\displaystyle\\sum_{t=1}^{T}\\gamma_{i}\\sum_{i=1}^{n}\\left\\|\\lambda_{i}^{t}-\\overline{{\\lambda}}^{t}\\right\\|_{2}+\\displaystyle\\sum_{t=1}^{T}\\sum_{i=1}^{n}\\frac{\\gamma_{i}^{t}}{2}\\left\\|\\lambda_{i}^{t}\\right\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The last term on the right side of the above inequality is non-negative and can be omitted. Besides, since $\\lambda_{i}^{1}=\\mathbf{0}$ for all $i\\in[n]$ .then $\\begin{array}{r}{\\sum_{t=1}^{T}\\left(\\left\\|\\mathbf{\\boldsymbol{\\lambda}}_{i}^{t+1}-\\mathbf{\\boldsymbol{\\lambda}}\\right\\|_{2}^{2}-\\left\\|\\mathbf{\\boldsymbol{\\lambda}}_{i}^{t}-\\mathbf{\\boldsymbol{\\lambda}}\\right\\|_{2}^{2}\\right)\\geq-\\|\\mathbf{\\boldsymbol{\\lambda}}\\|_{2}^{2}}\\end{array}$ for any $\\lambda_{i}^{T+1}\\in$ ", "page_idx": 23}, {"type": "text", "text": "$\\mathbb{R}_{+}^{m}$ . Thus, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\sum_{i=1}^{n}\\gamma_{t}\\left((\\lambda_{i}^{t})^{\\top}g_{i}(\\theta_{i}^{t})-\\lambda^{\\top}g_{i}(\\theta_{i}^{t})\\right)}\\\\ &{\\displaystyle\\geq-\\frac{n}{2}\\left(1+\\sum_{t=1}^{T}\\gamma_{t}^{2}\\right)\\|\\lambda\\|_{2}^{2}-\\frac{9}{2}\\sum_{t=1}^{T}\\sum_{i=1}^{n}\\left\\|\\lambda_{i}^{t}-\\overline{{\\lambda}}^{t}\\right\\|_{2}^{2}}\\\\ &{\\displaystyle\\quad-\\,2(1+\\sqrt{n})B\\sum_{t=1}^{T}\\gamma_{t}\\sum_{i=1}^{n}\\left\\|\\lambda_{i}^{t}-\\overline{{\\lambda}}^{t}\\right\\|_{2}-4n B^{2}\\sum_{t=1}^{T}\\gamma_{t}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Rearranging the terms in the above inequality yields ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{-\\displaystyle\\sum_{t=1}^{T}\\displaystyle\\sum_{i=1}^{n}\\gamma_{t}(\\boldsymbol{X}_{i}^{t})^{\\top}g_{i}(\\theta_{i}^{t})\\le-\\displaystyle\\sum_{t=1}^{T}\\displaystyle\\sum_{i=1}^{n}\\gamma_{t}\\boldsymbol{\\lambda}^{\\top}g_{i}(\\theta_{i}^{t})+\\displaystyle\\frac{n}{2}\\left(1+\\displaystyle\\sum_{t=1}^{T}\\gamma_{t}^{2}\\right)\\|\\boldsymbol{\\lambda}\\|_{2}^{2}+\\displaystyle\\frac{9}{2}\\displaystyle\\sum_{t=1}^{T}\\sum_{i=1}^{n}\\Big\\|\\boldsymbol{\\lambda}_{i}^{t}-\\overline{{\\boldsymbol{\\lambda}}}^{t}\\Big\\|_{L^{2}}}&\\\\ &{\\displaystyle}&{\\qquad+\\displaystyle\\,2(1+\\sqrt{n})B\\displaystyle\\sum_{t=1}^{T}\\gamma_{t}\\displaystyle\\sum_{i=1}^{n}\\Big\\|\\boldsymbol{\\lambda}_{i}^{t}-\\overline{{\\boldsymbol{\\lambda}}}^{t}\\Big\\|_{2}+4n B^{2}\\displaystyle\\sum_{t=1}^{T}\\gamma_{t}^{2},\\qquad}&\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which completes the proof. ", "page_idx": 24}, {"type": "text", "text": "E.3Proof of Lemma E.4 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Based on the update rule of $\\widehat{\\pmb{\\theta}}_{i h}^{t}$ that $\\begin{array}{r}{\\widehat{\\pmb{\\theta}}_{i h}^{t+1}=\\sum_{k\\neq h}a_{i k}\\widehat{\\pmb{\\theta}}_{k h}^{t}+a_{i h}\\pmb{\\theta}_{h}^{t},\\forall h\\neq i}\\end{array}$ and $i,h\\in[n]$ , we have that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{e_{i h}^{t+1}:=\\widehat{\\pmb{\\theta}}_{i h}^{t+1}-\\pmb{\\theta}_{h}^{t+1}=\\displaystyle\\sum_{k\\not=h}a_{i k}\\widehat{\\pmb{\\theta}}_{k h}^{t}+a_{i h}\\pmb{\\theta}_{h}^{t}-\\pmb{\\theta}_{h}^{t+1}+\\pmb{\\theta}_{h}^{t}-\\pmb{\\theta}_{h}^{t}}}\\\\ {{=\\displaystyle\\sum_{k\\not=h}a_{i k}e_{k h}^{t}-\\left(\\pmb{\\theta}_{h}^{t+1}-\\pmb{\\theta}_{h}^{t}\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Recall that ${\\bf A}_{h}^{-}$ is the weight matrix formed by removing the $h$ th row and $h$ th column of the weight matrix Aforany h E [n], andeh := col (eth\\*. ,e(n-1)e(h+1)h\\* ,enh). . Then, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{e_{h}^{t+1}=(\\mathbf{A}_{h}^{-}\\otimes\\mathbf{I}_{d})e_{h}^{t}+\\mathbf{1}_{n-1}\\otimes\\left(\\pmb{\\theta}_{h}^{t+1}-\\pmb{\\theta}_{h}^{t}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Since $\\beta$ is the maximium eigenvalue of ${\\bf A}_{h}^{-}$ for all $h\\in[n]$ , we have that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}\\|e_{h}^{t+1}\\|_{2}\\leq\\mathbb{E}\\left\\|\\left(\\mathbf{A}_{h}^{-}\\otimes\\mathbf{I}_{d}\\right)e_{h}^{t}\\right\\|_{2}+\\mathbb{E}\\left\\|\\mathbf{1}_{n-1}\\otimes\\left(\\theta_{h}^{t+1}-\\theta_{h}^{t}\\right)\\right\\|_{2}}}\\\\ &{}&{\\leq\\beta\\mathbb{E}\\|e_{h}^{t}\\|_{2}+\\sqrt{n-1}\\gamma_{t}\\mathbb{E}\\left\\|\\phi_{h}^{t}\\left(\\xi_{h}^{t};\\theta_{h}^{t},\\hat{\\theta}_{h}^{t},\\lambda_{h}^{t}\\right)\\right\\|_{2}}\\\\ &{}&{\\leq\\beta\\mathbb{E}\\|e_{h}^{t}\\|_{2}+\\sqrt{n-1}\\gamma_{t}\\mathbb{E}\\left\\|\\nabla_{\\theta_{h}}J_{h}\\left(\\xi_{h}^{t};\\theta_{h}^{t},\\hat{\\theta}_{h}^{t}\\right)\\right\\|_{2}+\\sqrt{n-1}\\gamma_{t}^{2}\\mathbb{E}\\left\\|\\nabla g_{h}(\\theta_{h})^{\\top}\\lambda_{h}^{t}\\right\\|_{2}}\\\\ &{}&{\\leq\\beta\\mathbb{E}\\|e_{h}^{t}\\|_{2}+\\sqrt{n-1}\\gamma_{t}(G+G_{g}\\mathbb{E}\\|\\gamma_{t}\\lambda_{h}^{t}\\|_{2})}\\\\ &{}&{\\leq\\beta^{t}\\mathbb{E}\\|e_{h}^{1}\\|_{2}+\\sqrt{n-1}\\sum_{k=0}^{t-1}\\beta^{k}\\gamma_{t-k}(G+\\sqrt{n}B G_{g}).~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~(\\mathrm{A}31-\\sqrt{n}B G_{g}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Further, since ${\\pmb\\theta}_{i h}^{1}\\,=\\,{\\bf0}$ for any $i,h\\,\\in\\,[n]$ , then, from Assumption 2.3, $\\mathbb{E}\\|e_{i h}^{1}\\|_{2}\\,=\\,\\|\\pmb{\\theta}_{h}^{1}\\|_{2}\\,\\le\\,C$ Summing the above inequality over $t\\in[T]$ and $h\\in[n]$ , we obtain ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\displaystyle\\sum_{h=1}^{n}\\mathbb{E}\\|e_{h}^{t}\\|_{2}\\leq n C\\displaystyle\\sum_{t=1}^{T}\\beta^{t-1}+n\\sqrt{n-1}(G+\\sqrt{n}B G_{g})\\displaystyle\\sum_{t=1}^{T}\\sum_{k=0}^{t-2}\\beta^{k}\\gamma_{t-k-1}}\\\\ &{\\leq\\displaystyle\\frac{n C}{1-\\beta}+n\\sqrt{n-1}(G+\\sqrt{n}B G_{g})\\displaystyle\\sum_{k=1}^{T}\\sum_{t=k+1}^{T}\\beta^{t-k-1}\\gamma_{k}}\\\\ &{\\leq\\displaystyle\\frac{n C}{1-\\beta}+\\frac{n\\sqrt{n-1}(G+\\sqrt{n}B G_{g})}{1-\\beta}\\displaystyle\\sum_{k=1}^{T}\\gamma_{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "On the other hand, taking square on both sides of (A31), we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Vert e_{h}^{t}\\Vert_{2}^{2}\\leq2\\beta^{t}\\mathbb{E}\\left\\Vert e_{h}^{1}\\right\\Vert_{2}^{2}+2(n-1)(G+\\sqrt{n}B G_{g})^{2}\\left(\\sum_{k=0}^{t-2}\\beta^{k}\\gamma_{t-k-1}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Using the Cauchy-Schwarz inequality yields ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left(\\sum_{k=0}^{t-2}\\beta^{k}\\gamma_{t-k-1}\\right)^{2}\\leq\\left(\\sum_{k=0}^{t-2}\\beta^{k}\\right)\\left(\\sum_{k=0}^{t-2}\\beta^{k}\\gamma_{t-k-1}^{2}\\right)\\leq\\frac{\\sum_{k=0}^{t-2}\\beta^{k}\\gamma_{t-k-1}}{1-\\beta}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Plugging (A33) into (A32) and summing over $t\\in[T]$ , we have that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\displaystyle\\sum_{h=1}^{n}\\mathbb{E}\\|e_{h}^{t}\\|_{2}^{2}\\leq2n C^{2}\\displaystyle\\sum_{t=1}^{T}\\beta^{t}+\\frac{2n(n-1)(G+\\sqrt{n}B G_{g})^{2}}{1-\\beta}\\left(\\displaystyle\\sum_{t=1}^{T}\\sum_{k=0}^{t-2}\\beta^{k}\\gamma_{t-k-1}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{2n C^{2}}{1-\\beta}+\\frac{2n(n-1)(G+\\sqrt{n}B G_{g})^{2}}{(1-\\beta)^{2}}\\displaystyle\\sum_{k=1}^{T}\\gamma_{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "E.4 Proof of Lemma E.5 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Let $\\begin{array}{r}{\\omega_{i}^{t}:=\\Big[\\big(1-\\gamma_{t}^{2}\\big)\\sum_{j\\in\\mathcal{N}_{i}}a_{i j}\\pmb{\\lambda}_{j}^{t}+\\gamma_{t}\\pmb{g}_{i}\\left(\\pmb{\\theta}_{i}^{t}\\right)\\Big]_{+}-\\sum_{j\\in\\mathcal{N}_{i}}a_{i j}\\pmb{\\lambda}_{j}^{t}}\\end{array}$ Then, for any $i\\in[n]$ , we have that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\left\\|{\\boldsymbol{\\omega}_{i}^{t}}\\right\\|_{2}=\\left\\|{\\left[{\\left({1-\\gamma_{t}^{2}}\\right)\\sum_{j\\in{\\mathcal{N}}_{i}}{a_{i j}\\lambda_{j}^{t}}+\\gamma_{t}g_{i}\\left({\\boldsymbol{\\theta}_{i}^{t}}\\right)}\\right]_{+}-\\sum_{j\\in{\\mathcal{N}}_{i}}{a_{i j}\\lambda_{j}^{t}}}\\right\\|_{2}}\\\\ {\\ }&{\\leq\\left\\|{-\\gamma_{t}\\sum_{j\\in{\\mathcal{N}}_{i}}{a_{i j}\\gamma_{t}\\lambda_{j}^{t}}+\\gamma_{t}g_{i}\\left({\\boldsymbol{\\theta}_{i}^{t}}\\right)}\\right\\|_{2}}\\\\ {\\ }&{\\leq\\gamma_{t}\\sum_{j\\in{\\mathcal{N}}_{i}}{a_{i j}\\left\\|\\gamma_{t}\\lambda_{j}^{t}\\right\\|_{2}+\\gamma_{t}\\left\\|g_{i}\\left({\\boldsymbol{\\theta}_{i}^{t}}\\right)\\right\\|_{2}}}\\\\ {\\ }&{\\leq\\gamma_{t}(\\sqrt{n}+1)B.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The first inequality in (A34) results from the nonexpansive property of projection, and the third inequality holds by using Lemma E.1. By the update rule of $\\lambda_{i}$ for any $i\\in[n]$ ,we havethat ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\lambda_{i}^{t+1}=\\sum_{j\\in\\mathcal{N}_{i}}a_{i j}\\pmb{\\lambda}_{j}^{t}+\\omega_{i}^{t}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Define concatenation vectors $\\boldsymbol{\\lambda}_{o}^{t}=\\operatorname{col}\\left(\\boldsymbol{\\lambda}_{1}^{t},\\cdot\\cdot\\cdot\\mathbf{\\Omega},\\boldsymbol{\\lambda}_{n}^{t}\\right)$ and $\\pmb{\\omega}_{o}^{t}=\\mathrm{col}\\left(\\pmb{\\omega}_{1}^{t},\\cdot\\cdot\\cdot\\mathbf{\\Omega},\\pmb{\\omega}_{n}^{t}\\right)$ . Then, for any $t\\in[T]$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boldsymbol{\\lambda}_{o}^{t+1}=\\left(\\mathbf{A}\\otimes\\mathbf{I}_{m}\\right)\\boldsymbol{\\lambda}_{o}^{t}+\\boldsymbol{\\omega}_{o}^{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since $\\begin{array}{r}{\\overline{{\\pmb{\\lambda}}}^{t}=\\frac{1}{n}\\sum_{i=1}^{n}\\lambda_{i}^{t}}\\end{array}$ we have that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\Delta^{t}:=\\lambda_{o}^{t}-\\left(\\mathbf{1}_{n}\\otimes\\mathbf{I}_{m}\\right)\\overline{{\\lambda}}^{t}=\\left(\\left(\\mathbf{I}_{n}-\\frac{\\mathbf{1}_{n}\\mathbf{1}_{n}^{T}}{n}\\right)\\otimes\\mathbf{I}_{m}\\right)\\lambda_{o}^{t},\\forall t\\in[T].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Combining (A35) and (A36) yields ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\Delta^{t+1}=\\left(\\mathbf{A}\\otimes\\mathbf{I}_{m}\\right)\\Delta^{t}+\\left(\\left(\\mathbf{I}-{\\frac{\\mathbf{11}^{T}}{n}}\\right)\\otimes\\mathbf{I}_{m}\\right)\\omega_{o}^{t},\\forall t\\in[T].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "image", "img_path": "KqgSzXbufw/tmp/60ea3dce1d86df16c5824604a6545d00b98c5fbaa703f7e6354890655e3e6995.jpg", "img_caption": ["Figure 3: A networked Cournot game with five firms and three markets. "], "img_footnote": [], "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{i=1}^{n}\\left\\|\\boldsymbol{\\lambda}_{i}^{t+1}-\\boldsymbol{\\bar{\\lambda}}^{t+1}\\right\\|_{2}=\\left\\|\\boldsymbol{\\Delta}^{t+1}\\right\\|_{2}=\\left\\|\\left(\\boldsymbol{\\Delta}^{\\otimes}\\boldsymbol{\\mathrm{I}}_{m}\\right)\\boldsymbol{\\Delta}^{t}+\\left(\\left(\\mathbf{I}-\\frac{\\mathbf{1}^{1T}}{n}\\right)\\otimes\\mathbf{I}_{m}\\right)\\omega_{o}^{t}\\right\\|_{2}}\\\\ &{\\leq\\left\\|\\left(\\boldsymbol{\\Delta}\\otimes\\boldsymbol{\\mathrm{I}}_{m}\\right)\\boldsymbol{\\Delta}^{t}\\right\\|_{2}+\\left\\|\\left(\\left(\\mathbf{I}-\\frac{\\mathbf{1}^{1T}}{n}\\right)\\otimes\\mathbf{I}_{m}\\right)\\omega_{o}^{t}\\right\\|_{2}}\\\\ &{\\leq\\sigma_{2}(\\boldsymbol{\\mathrm{A}})\\left\\|\\boldsymbol{\\Delta}^{t}\\right\\|_{2}+2\\left\\|\\omega_{o}^{t}\\right\\|_{2}}\\\\ &{\\leq2\\displaystyle\\sum_{k=0}^{t-1}\\sigma_{2}(\\boldsymbol{\\mathrm{A}})^{k}\\left\\|\\omega_{o}^{t-k}\\right\\|_{2}}\\\\ &{\\leq2(n+\\sqrt{n})B\\displaystyle\\sum_{k=0}^{t-1}\\sigma_{2}(\\boldsymbol{\\mathrm{A}})^{k}\\gamma_{t-k},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the last inequality is based on the result in (A34). Summing the above inequality over $t\\in[T]$ yields ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\displaystyle\\sum_{i=1}^{n}\\left\\|\\pmb{\\lambda}_{i}^{t}-\\overline{{\\pmb{\\lambda}}}^{t}\\right\\|_{2}\\leq2(n+\\sqrt{n})B\\displaystyle\\sum_{t=1}^{T}\\sum_{k=0}^{t-2}\\sigma_{2}(\\pmb{\\Delta})^{k}\\gamma_{t-1-k}}\\\\ &{\\quad\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{2(n+\\sqrt{n})B}{1-\\sigma_{2}(\\pmb{\\Delta})}\\sum_{k=1}^{T}\\gamma_{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Similarly to the calculation of $\\begin{array}{r}{\\sum_{t=1}^{T}\\sum_{h=1}^{n}\\|e_{h}^{t}\\|_{2}^{2}}\\end{array}$ in Section E.3, we have that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\sum_{i=1}^{n}\\left\\|\\lambda_{i}^{t}-\\overline{{\\lambda}}^{t}\\right\\|_{2}^{2}\\leq\\frac{4(n+\\sqrt{n})^{2}B^{2}}{(1-\\sigma_{2}(\\mathbf A))^{2}}\\sum_{k=1}^{T}\\gamma_{k}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "F Simulation Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "F.1  Networked Cournot Game ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "The Cournot game is a foundational model in economic theory (Allaz and Vila, 1993) for analyzing oligopolistic competition, where a limited number of firms dominate a specific market. In Cournot games, all firms sell a homogeneous commodity and aim to maximize their individual profits by independently and simultaneously determining optimal production quantities. The total quantity produced by all firms is constrained by factors such as market capacity, raw material availability, and environmental considerations. The profit of each firm depends not only on its own production quantity but also on the quantities chosen by its competitors, as they influence the demand price determined by the market's demand curve and the total production quantity. There are strategic interactions between firms and markets in the Cournot game. According to the law of supply and demand, an increased production quantity drives down the demand price, and vice versa. The Cournot game model has diverse applications in various fields, including supply chain management, electricity market competition, natural resource extraction, online advertising auctions, and the telecommunications industry. ", "page_idx": 26}, {"type": "image", "img_path": "KqgSzXbufw/tmp/41827aa050163a6949caa233337fda92060fa59c44997a94ac09bba6d0582b93.jpg", "img_caption": ["Figure 5: The serving quantities of five firms to three markets. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "In this experiment, we consider a networked Cournot game comprising $n$ firms selling a single commodity across $m$ markets, as illustrated in Fig. 3. Each firm $i\\in[n]$ determines its output quantity $\\pmb{\\theta}_{i}=\\mathrm{col}\\left(\\theta_{i1},\\cdot\\cdot\\cdot\\mathbf{\\theta},\\theta_{i m}\\right)$ subject to the constraint of its production capacity $Q_{i}$ that $\\textstyle\\sum_{j=1}^{m^{-}}\\theta_{i j}\\leq Q_{i}$ Here, $\\theta_{i j}$ denotes the quantity of player $i$ sold to the $j$ th market. The total quantity allocated to market $j$ is limited by its market capacity $B_{j}$ , satisfying the condition that $\\begin{array}{r}{\\sum_{i=1}^{n}\\!\\!\\!\\!\\theta_{i j}\\leq B_{j}\\;\\forall j\\in[m]}\\end{array}$ . Thus, the local constraint of player $i$ associated with market $j$ is ", "page_idx": 27}, {"type": "equation", "text": "$$\ng_{i j}(\\pmb\\theta_{i})=\\theta_{i j}-B_{j}/n,\\forall i\\in[n],j\\in[m].\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Let $\\pmb{g}_{i}(\\pmb{\\theta}_{i}):=\\operatorname{col}\\big(g_{i1}\\big(\\pmb{\\theta}_{i}\\big),\\cdot\\cdot\\cdot\\mathbf{\\nabla},g_{i m}(\\pmb{\\theta}_{i})\\big),\\forall i\\in[n]$ ", "page_idx": 27}, {"type": "text", "text": "The cost function of firm $i$ is defined as ", "page_idx": 28}, {"type": "equation", "text": "$$\nJ_{i}=\\pmb{d}_{i}^{\\top}\\pmb{\\theta}_{i}-\\sum_{j=1}^{m}p_{j}\\theta_{i j},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\pmb{d}_{i}=\\mathrm{col}\\left(d_{i1},\\cdot\\cdot\\cdot\\,,d_{i m}\\right)$ and $d_{i j}$ represents the cost that firm $i$ sells a unit of its product to the $j$ th market, $\\forall i\\in[n],j\\in[m]$ $d_{i}$ includes the cost of raw material, transportation, maintenance, etc. In $J_{i}$ , the term $p_{j}$ denotes the unit demand price of market $j$ determined by its market demand curve and the total production quantity, given by ", "page_idx": 28}, {"type": "equation", "text": "$$\np_{j}=\\xi_{j}+\\Lambda_{j}\\left(c_{j}+\\frac{1}{d_{j}}\\sum_{i=1}^{n}\\theta_{i j}\\right)^{-\\frac{1}{\\tau_{j}}},\\forall j\\in[m],\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $c_{j},d_{j}\\perp_{j}$ , and $\\tau_{j}>0$ are constants, $\\xi_{j}$ is a random variable. Due to the interaction between firms and markets, the demand price can fluctuate with production quantities, represented by $\\xi_{j}\\sim$ $\\mathcal{D}_{j}(\\pmb{\\theta})$ . Note that the quantity-dependent distributions $\\mathcal{D}_{j}(\\pmb{\\theta})$ for all $j\\in[m]$ are unknown by players. For any $j\\in[m]$ , the variable $\\xi_{j}$ is defined as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\xi_{j}=\\xi_{j}^{o}+\\varepsilon\\frac{\\alpha_{j}}{\\sum_{j^{\\prime}=1}^{m}\\alpha_{j^{\\prime}}}\\left(\\sum_{i=1}^{n}\\theta_{i j}\\right),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\xi_{j}^{o}$ is the random base component, $\\varepsilon\\ge0$ represents the performative strength of markets, and $\\alpha_{j}$ is the relative strength of market $j$ for any $j\\in[m]$ . According to the law of supply and demand, an increased production quantity generally decreases a market's demand price, which corresponds to the setup that $\\alpha_{j}\\leq0$ for all $j\\in[m]$ . Thus, the objective of each play $i\\in[n]$ in the network Cournot game is formulated by ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\pmb{\\theta}_{i}\\in\\Omega_{i}}{\\mathrm{min}}}&{\\mathbb{E}_{p_{j}\\sim\\mathcal{D}_{j}(\\theta_{i j},\\forall i\\in[n]),j\\in[m]}\\left[\\pmb{d}_{i}^{\\top}\\pmb{\\theta}_{i}-\\sum_{j=1}^{m}p_{j}\\theta_{i j}\\right]}\\\\ {\\mathrm{subject~to}}&{\\theta_{i j}+\\displaystyle\\sum_{i^{\\prime}\\neq i}\\theta_{i^{\\prime}j}\\leq B_{j},\\forall j\\in[m].}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "In the simulation, we set $n=5$ and $m=3$ . The network structure is as depicted in Fig. 3. Each element of the communication weight matrix $A=(a_{i j})_{n\\times n}$ is se to be $\\begin{array}{r}{a_{i j}\\overset{\\cdot}{=}\\frac{1}{|\\mathcal{N}_{i}|}}\\end{array}$ and $|\\mathcal{N}_{i}|$ is the cardinality of ${\\mathcal{N}}_{i}$ . The production capacity $Q_{i}$ is randomly and uniformly drawn from [10, 12] for all $i\\in[5]$ , and the market's capacity $B_{j}$ is randomly and uniformly drawn from [10, 15] for all $\\bar{j}\\in[m]$ All entries in $\\pmb{d}_{i}$ $\\forall i\\in[n]$ are randomly and uniformly drawn from [1, 1.5]. The distribution of $\\xi_{j}^{o}$ is set to $\\operatorname*{min}(\\operatorname*{max}(\\mathcal{N}(2.5,1),2.5),^{\\prime}$ 7.5). The performative power $\\alpha_{j}$ is randomly and uniformly drawn from $(-1,0]$ , for all $j\\in[3]$ . Other settings are: $\\Lambda_{j}=10$ $c_{j}=10$ \uff0c $d_{j}=5$ and $\\tau_{j}=2,\\forall j\\in[3]$ ", "page_idx": 28}, {"type": "text", "text": "Fig. 4 compares the demand prices of three markets at PSE and NE with performative strength $\\varepsilon=0.2$ , 0.4, and 0.6 and Fig. 5 compares the corresponding serving quantities of five firms to these three markets. The results suggest that, although a larger performative strength leads to a wider gap, the difference in these two indicators between the PSE and NE remains insignificant. This confirms the effectiveness of PSE solutions and our distance analysis between the PSE and NE as stated in Theorem3.5. ", "page_idx": 28}, {"type": "text", "text": "F.2 Ride-Share Market ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We further examine an example of a ride-share market, where multiple platforms compete to maximize their individual revenue by offering shared rides in competitive areas, taking into account operational constraints and market demands. This experiment builds upon the semi-synthetic simulation conducted in (Narang et al., 2023), adapting it to our constrained noncooperative game setting. ", "page_idx": 28}, {"type": "text", "text": "Consider a ride-share market with $n$ platforms competing in $m$ areas. Each platform $i\\in[n]$ aims to maximize its revenue by determining the quantities it offers at the $j$ th area, denoted as $\\theta_{i j}$ , for all $j\\in[m]$ . Let $\\pmb{\\theta}_{i}=[\\theta_{i1},\\cdots\\,,\\theta_{i m}]^{\\top}$ . The total number of rides provided by each platform $i$ cannot exceed a predefined limit $Q_{i}$ , given by $\\begin{array}{r}{\\sum_{j=1}^{m}\\theta_{i j}\\leq Q_{i},\\forall i\\in[\\'\\Tilde{n}]}\\end{array}$ . Let $p_{j}$ denote the demand price at the $j$ th location, which fuctuates with the total offered quantity at the area following the law of supply and demand. We adopt the same model for $\\{p_{j}\\}$ as in the network Cournot game, given by (A37). Additionally, the maintenance costs associated with platform operations may vary across locations due to factors such as distance or labor costs. Let $\\pmb{d}_{i}\\in\\mathbb{R}^{m}$ represent the cost vector of platform $i$ at all areas. Then, the inverse of the revenue function for each platform can be expressed as ", "page_idx": 28}, {"type": "image", "img_path": "KqgSzXbufw/tmp/df7af166938ddf700b0758a4d1a4ab5e7f90d5a06dfcba055eafb2707ad00b7e.jpg", "img_caption": ["Figure 6: Convergence of the time-average revenues of three platforms. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "KqgSzXbufw/tmp/b4956c132adcdda1043bf7ea9a43254f21e54dc940004b42aa006b6183b4a511.jpg", "img_caption": ["Figure 7: Convergence of the time-average constraint violations at eight areas. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "equation", "text": "$$\nJ_{i}=-\\sum_{j=1}^{m}p_{j}\\theta_{i j}+\\mathbf{d}_{i}^{\\top}\\pmb{\\theta}_{i},\\forall i\\in[n].\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Assume that each platform only offers one type of ride. Considering the diverse ride characteristics, such as shape and speed, we use $h_{i}$ to denote the spatial occupancy of each ride offered by platform $i$ The accommodated ride quantity at each location is constrained by $B_{j}$ due to parking availability and road conditions, such that $\\begin{array}{r}{\\sum_{i=1}^{n}h_{i}\\theta_{i j}\\leq B_{j}}\\end{array}$ Then, the objective of each latform $\\bar{i}\\in[n]$ inthe ride-share market is formulated as ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\theta_{i}\\in\\Omega_{i}}{\\mathrm{min}}}&{\\mathbb{E}_{p_{j}\\sim\\mathcal{D}_{j}(\\theta_{i j},\\forall i\\in[n]),\\forall j\\in[m]}\\left[-\\displaystyle\\sum_{j=1}^{m}p_{j}\\theta_{i j}+d_{i}^{\\top}\\theta_{i}\\right]}\\\\ {\\mathrm{subject~to}}&{h_{i}\\theta_{i j}+\\displaystyle\\sum_{i^{\\prime}\\neq i}h_{i^{\\prime}}\\theta_{i^{\\prime}j}\\leq B_{j},\\forall j\\in[m].}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The simulation setup is based on dataset from a prior Kaggle competition.2 Our study focuses on three ride-share platforms (Uber, Lyft, and Via) and eight competing areas within New York. We randomly and uniformly assign the total number of rides, $Q_{i}$ , from the range [200, 400] for each platform $i\\in[3]$ . Similarly, the accommodated capacity, $B_{j}$ , is randomly and uniformly drawn from [50, 150] for all $j\\in$ [8]. All entries in $d_{i}$ $_i,\\forall i\\in[n]$ are randomly and uniformly drawn from [0.2, 2.2]. The distribution of $\\xi_{j}^{o}$ is set as $\\operatorname*{min}(\\operatorname*{max}(\\mathcal{N}(1,1),1),5)$ . Additionally, we set the following values for all areas $j\\in$ [8]: $\\Lambda_{j}=5$ $c_{j}=5$ $d_{j}=5$ , and $\\tau_{j}=2$ ", "page_idx": 29}, {"type": "text", "text": "Fig. 6 compares the convergence of the time-average revenues of these three platforms: Uber, Lyft, and Via, denoted by $\\begin{array}{r}{-\\frac{1}{t}\\sum_{t^{\\prime}=1}^{t}\\mathbb{E}_{\\pmb{p}^{t}\\sim\\mathcal{D}(\\pmb{\\theta}^{t})}[J_{i}(\\pmb{p}^{t};\\pmb{\\theta}^{t^{\\prime}})]}\\end{array}$ . We consider three performative strengths: ", "page_idx": 29}, {"type": "image", "img_path": "KqgSzXbufw/tmp/3c7f3aa474ff0ba9cb59a0d85937a2fdb3fe3f7a9404537d6021b22dcc84be1a.jpg", "img_caption": ["Figure 8: The normalized distance between $\\pmb{\\theta}^{t}$ and $\\theta^{\\mathrm{ne}}$ "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "$\\varepsilon=0.1,0.2$ , and 0.3. Similarly to Fig. 2 (b), we compare the performance of Algorithm 1, represented by \u201c\"pse\", and the performance of Algorithm 1with perfect knowledge of data distributions $\\mathcal{D}_{j}(\\pmb{\\theta})$ for all $j\\in[m]$ . It is observed that, with a mild performative strength $\\varepsilon$ , the revenues achieved by the \u201cpse\u201d are close to those of the \u201cne\u201d for all three platforms. However, as $\\varepsilon$ increases, the gap between the two approaches widens, although it remains relatively small. This observation confirms the analytical result presented in Theorem 3.5. ", "page_idx": 30}, {"type": "text", "text": "Fig. 7 shows the convergence of the time-average constraint violations at eight areas by Algorithm 1, denoted by $\\begin{array}{r}{\\frac{1}{t}\\sum_{t^{\\prime}=1}^{t}\\sum_{i=1}^{\\bar{3}}g_{i j}(\\pmb{\\theta}_{i}^{t^{\\prime}}),j=1,\\cdots\\bar{,}8}\\end{array}$ with performative strengths of $\\varepsilon=0.1,0.2$ and 0.3. The constraints hold for all three performative strengths. However, as $\\varepsilon$ increases, the platform tends to allocate fewer rides. This may be attributed to larger market fluctuations associated with a higher $\\varepsilon$ , leading to a more conservative allocation. ", "page_idx": 30}, {"type": "text", "text": "Fig. 8 compares the normalized distance between $\\pmb{\\theta}^{t}$ and the NE point $\\theta^{\\mathrm{ne}}$ , denoted as $\\lVert\\pmb{\\theta}^{t}-$ ${\\theta}^{\\mathrm{ne}}\\lVert_{2}/\\rVert{\\theta}^{t}\\rVert_{2}$ , with performative strengths: $\\varepsilon\\;=\\;0.1,\\;0.2$ , and 0.3. The result is quantitatively analogous to the findings presented in Fig. 8. Firstly, $\\pmb{\\theta}^{t}$ gradually approaches $\\pmb{\\theta}^{\\mathrm{ne}}$ with iterations. Secondly, a higher performative strength leads to a wider normalized distance between the convergent point of $\\mathbf{\\ddot{\\theta}}^{t}$ and $\\pmb{\\theta}^{\\mathrm{ne}}$ ", "page_idx": 30}, {"type": "text", "text": "Fig. 9 compares the demand prices of eight areas and the ride quantities offered to them by three platforms at PSE and NE. We consider performative strengths $\\varepsilon=0.1$ and $\\varepsilon=0.3$ .It isobserved that the values of these indicators at the PSE and NE are close to each other when $\\varepsilon=0.1$ However, a noticeable discrepancy arises when $\\varepsilon=0.3$ ", "page_idx": 30}, {"type": "text", "text": "Additionally, we display the demand prices of eight areas in New York in Fig. 10, with different performative strengths: $\\varepsilon=0.1$ , 0.2, and 0.3. It can be observed that, while prices vary by location, smallervalues of $\\varepsilon$ generally correspond to higher prices. The offered quantities of these three platforms to the eight locations are illustrated in Fig. 11. The results indicate a conservative allocation as the performative strength increases. Furthermore, with the cost of these three platforms at different locations in Fig. 12, we obtain the revenues of the platforms Uber, Lyft, and Via in different areas, as illustrated in Fig. 13. Clearly, performativity has an inverse effect on revenues, and the stronger the performative strength, the lower the revenues. ", "page_idx": 30}, {"type": "image", "img_path": "KqgSzXbufw/tmp/d4dcf8c2f074a00d6028b26fd0cb78999210aa905cbc22c9c3a31b83e2821c5d.jpg", "img_caption": ["Figure 9: The demand prices of eight areas and the ride quantities offered to them by three platforms. "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "KqgSzXbufw/tmp/74cacd0b0972fac13e5dfc7d8feac0f8d08f689a448c05ba0ef4c6f491680b12.jpg", "img_caption": ["Figure 10: The demand prices of different areas. "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "KqgSzXbufw/tmp/da46a1fca6b490a2d6cb5bc058f513f9f8b770caa65bc12a9f3c66aae784aeb2.jpg", "img_caption": ["Figure 11: The quantities of platforms offered to different areas. "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "KqgSzXbufw/tmp/2baaa76eb41ef23bfb70afe5096f629ca4bc9e96da1626a1d6597aa80569fcf1.jpg", "img_caption": ["Figure 12: The cost of platforms in different areas. "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "KqgSzXbufw/tmp/4818c543b43da36fb9750c16758a54b8a045e5ea0bc8b4c3490f4bccf23706e0.jpg", "img_caption": ["Figure 13: The revenues of platforms in different areas. "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "2. Limitations: ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Does the paper discuss the limitations of the work performed by the authors? [No] ", "page_idx": 34}, {"type": "text", "text": "Answer: [No] ", "page_idx": 34}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 34}, {"type": "text", "text": "Answer: [No] ", "page_idx": 34}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [No] ", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}]