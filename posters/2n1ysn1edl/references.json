{"references": [{"fullname_first_author": "A. Gu", "paper_title": "Mamba: Linear-time sequence modeling with selective state spaces", "publication_date": "2023-12-00", "reason": "This paper introduces the Mamba architecture, which is the focus of the current research and the basis for the proposed MambaLRP method."}, {"fullname_first_author": "S. Bach", "paper_title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation", "publication_date": "2015-00-00", "reason": "This paper introduces Layer-wise Relevance Propagation (LRP), a crucial method for explaining model predictions that the current research extends to Mamba models."}, {"fullname_first_author": "G. Montavon", "paper_title": "Methods for interpreting and understanding deep neural networks", "publication_date": "2018-00-00", "reason": "This review paper provides a broad overview of methods for interpreting deep learning models, providing important context and background for the current research on explainable AI."}, {"fullname_first_author": "A. Ali", "paper_title": "XAI for transformers: Better explanations through conservative propagation", "publication_date": "2022-00-00", "reason": "This paper explores explainable AI techniques for Transformer models, offering insights and approaches that are relevant to the current research on explaining sequence models."}, {"fullname_first_author": "A. Gu", "paper_title": "Efficiently modeling long sequences with structured state spaces", "publication_date": "2022-00-00", "reason": "This paper introduces structured state space sequence models (SSMs), a class of models that the current research builds upon and extends with explainability methods."}]}