[{"figure_path": "U3hQoqgQDJ/tables/tables_6_1.jpg", "caption": "Table 2: Benchmark on Generalizable multi-modal understanding tasks with one model architecture joint training for all. *Unlike Mask2Former and SEEM, FIND is not trained with a deformable vision encoder. We report un-ensemble/ensemble results for X-Decoder, and the finetuned/pre-trained results for blip2. Note that we compute the ITC score for blip2 instead of ITM. unless specified as SAM. The language backbone is a fixed LLaMA-7B, unless specified as UniCL. During training, we train the FIND-Interface jointly on all the tasks unless specified.", "description": "This table compares FIND's performance on various multi-modal understanding tasks against several strong baselines.  It highlights FIND's generalizability by evaluating its performance across generic segmentation, grounded segmentation, interactive segmentation, and image-text retrieval tasks.  Key metrics include mIoU, cIoU, PQ, MAP, and IR@1/TR@1. The table also notes specific details about the baseline models' training setups (e.g., use of deformable vision encoders, language backbones, and ensemble methods) to provide a more nuanced comparison.  It showcases that FIND, even without a deformable vision encoder, achieves competitive performance, demonstrating the effectiveness of its unified training approach.", "section": "4.1 Main Results"}, {"figure_path": "U3hQoqgQDJ/tables/tables_7_1.jpg", "caption": "Table 2: Benchmark on Generalizable multi-modal understanding tasks with one model architecture joint training for all. *Unlike Mask2Former and SEEM, FIND is not trained with a deformable vision encoder. We report un-ensemble/ensemble results for X-Decoder, and the finetuned/pre-trained results for blip2. Note that we compute the ITC score for blip2 instead of ITM. unless specified as SAM. The language backbone is a fixed LLaMA-7B, unless specified as UniCL. During training, we train the FIND-Interface jointly on all the tasks unless specified.", "description": "This table compares FIND's performance on various multi-modal understanding tasks against several strong baselines.  It highlights FIND's generalizability by showing results across generic segmentation, grounded segmentation, interactive segmentation, and image-text retrieval.  The table also notes key differences in training methods between FIND and other models (e.g., the use of a deformable vision encoder).", "section": "4.1 Main Results"}, {"figure_path": "U3hQoqgQDJ/tables/tables_7_2.jpg", "caption": "Table 4: Ablation study on different foundation model architectures.", "description": "This table presents the ablation study comparing the performance of FIND using different foundation models.  Specifically, it shows the results for various tasks (generic segmentation, grounding, interactive segmentation, and retrieval) when using different vision and language encoders (X-Decoder, SAM, UniCL, and LLaMA).  The table helps to understand the impact of each model's capabilities on the overall performance of FIND.", "section": "4.1 Main Results"}, {"figure_path": "U3hQoqgQDJ/tables/tables_8_1.jpg", "caption": "Table 3: Benchmark on interleaved understanding with the jointly trained model on all tasks with one set of weights. We evaluate interleave grounding, retrieval, and generic segmentation.", "description": "This table presents the results of a benchmark evaluating the performance of a jointly trained model on three interleaved understanding tasks: grounding, retrieval, and generic segmentation.  The model uses a single set of weights for all tasks. The results are broken down by task and dataset (COCO, g-Ref, Entity, VOC, Karpathy), and  metrics include  cIoU, mIoU, AP50, IR@5, IR@10, PQ, and mAP.", "section": "4.1 Main Results"}, {"figure_path": "U3hQoqgQDJ/tables/tables_12_1.jpg", "caption": "Table 6: Task specific FIND Interface. We define each task under the prototype of the FIND interface that enables a shared embedding space, and a unified and flexible architecture for future tasks. Where p, q stands for prompts, queries, and arrows stand for attention direction. The colors red, blue, and olive are the embeddings of vision, language, and interleave modality.", "description": "This table details the design choices for the task-specific FIND interface. It breaks down each task (Generic Segmentation, Grounded Segmentation, Image-Text Retrieval, Interactive Segmentation, Interleave Grounding, and Interleave Retrieval) into its components: prompts (input embeddings), queries (learnable embeddings), content attention (information flow from prompts to queries), conditional attention (internal reasoning within prompts and queries), and projection (mapping queries to final outputs). The table also indicates the types of embeddings used (vision, language, or interleaved) and the final output type (Pixel or Semantic).", "section": "A Method Details"}]