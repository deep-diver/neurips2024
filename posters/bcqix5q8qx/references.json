{"references": [{"fullname_first_author": "A. Athalye", "paper_title": "Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples", "publication_date": "2018-00-00", "reason": "This paper is crucial as it highlights the limitations of existing adversarial defenses and motivates the need for more robust attack methods."}, {"fullname_first_author": "P. Bartlett", "paper_title": "Adversarial examples in multi-layer random ReLU networks", "publication_date": "2021-00-00", "reason": "This paper provides a theoretical foundation for understanding the phenomenon of adversarial examples in neural networks, particularly within the context of ReLU activation functions."}, {"fullname_first_author": "S. Bubeck", "paper_title": "A universal law of robustness via isoperimetry", "publication_date": "2021-00-00", "reason": "This paper introduces a novel theoretical framework for analyzing the robustness of classifiers and offers a general explanation for the existence of adversarial examples."}, {"fullname_first_author": "F. Croce", "paper_title": "Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks", "publication_date": "2020-00-00", "reason": "This paper introduces AutoAttack, a state-of-the-art adversarial attack method that serves as a benchmark and comparison point for the proposed attack in this paper."}, {"fullname_first_author": "I. Goodfellow", "paper_title": "Explaining and harnessing adversarial examples", "publication_date": "2014-00-00", "reason": "This seminal paper introduced the concept of adversarial examples and presented early explanations based on the local linearity of neural networks, providing a foundation for subsequent research."}]}