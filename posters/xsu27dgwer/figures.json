[{"figure_path": "xSU27DgWEr/figures/figures_8_1.jpg", "caption": "Figure 3: Comparison between D and DhH. The y-axis is the estimated corresponding f-divergence and the x-axis is the number of iterations.", "description": "This figure compares the absolute f-divergence with and without absolute value function. The x-axis represents the number of iterations, and the y-axis shows the estimated f-divergence. The plots show that using the absolute value function leads to an overestimation of the f-divergence, and that the absolute discrepancy measure can easily explode during training. This highlights the importance of using the proposed f-DD measure in domain adaptation.", "section": "6.2 Experiments"}, {"figure_path": "xSU27DgWEr/figures/figures_27_1.jpg", "caption": "Figure 2: Illustration of the adversarial training framework for f-DD-based UDA. The framework includes the representation network (hrep), the main classifier (hcls), and the auxiliary classification network (h'cls). It jointly minimizes the empirical risk on the source domain and the approximated f-DD between the source and target domains.", "description": "This figure illustrates the adversarial training framework used in the paper for unsupervised domain adaptation (UDA).  It shows how the model learns by minimizing both the empirical risk (error) on the labeled source data and the f-domain discrepancy (f-DD) between the source and target domains.  The f-DD, a measure of the difference in data distributions, is approximated using an adversarial approach. The figure highlights the two main components of the model: the representation network (hrep) and the classification network (hcls), along with their counterparts in the adversarial component (h'rep and h'cls).", "section": "6.1 Domain Adversarial Learning Algorithm"}, {"figure_path": "xSU27DgWEr/figures/figures_29_1.jpg", "caption": "Figure 3: Comparison between D and DhH. The y-axis is the estimated corresponding f-divergence and the x-axis is the number of iterations.", "description": "The figure compares the absolute and non-absolute versions of KL and chi-square f-divergences across four different experimental settings (KL on Office-31, KL on Office-Home, chi-square on Office-31, chi-square on Office-Home).  It shows that the absolute value version of the discrepancy tends to overestimate the f-divergence, leading to a breakdown in the training process.", "section": "6.2 Experiments"}, {"figure_path": "xSU27DgWEr/figures/figures_29_2.jpg", "caption": "Figure 4: Visualization results of representations obtained by using t-SNE. The source domain (blue points) is U and the target domain (orange points) is M.", "description": "This figure visualizes the results of applying t-SNE to the representations learned by four different domain adaptation methods: f-DAL, f-DD using chi-squared divergence, f-DD using KL divergence, and f-DD using Jeffreys divergence.  The source domain (USPS) is represented by blue points, and the target domain (MNIST) is represented by orange points. The visualization shows how well each method aligns the representations of the source and target domains. A better alignment indicates a more successful domain adaptation. The figure aims to show the improved representation alignment from f-DD compared to f-DAL.", "section": "6 Algorithms and Experimental Results"}]