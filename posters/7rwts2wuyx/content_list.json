[{"type": "text", "text": "Revisiting Ensembling in One-Shot Federated Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Youssef Allouah1 Akash Dhasade1\u2217 Rachid Guerraoui1 Nirupam Gupta2 ", "page_idx": 0}, {"type": "text", "text": "Anne-Marie Kermarrec1 Rafael Pinot3 Rafael Pires1 Rishi Sharma1 ", "page_idx": 0}, {"type": "text", "text": "1EPFL 2University of Copenhagen 3Sorbonne Universit\u00e9 and Universit\u00e9 Paris Cit\u00e9, CNRS, LPSM ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Federated learning (FL) is an appealing approach to training machine learning models without sharing raw data. However, standard FL algorithms are iterative and thus induce a significant communication cost. One-shot federated learning (OFL) trades the iterative exchange of models between clients and the server with a single round of communication, thereby saving substantially on communication costs. Not surprisingly, OFL exhibits a performance gap in terms of accuracy with respect to FL, especially under high data heterogeneity. We introduce FENS, a novel federated ensembling scheme that approaches the accuracy of FL with the communication efficiency of OFL. Learning in FENS proceeds in two phases: first, clients train models locally and send them to the server, similar to OFL; second, clients collaboratively train a lightweight prediction aggregator model using FL. We showcase the effectiveness of FENS through exhaustive experiments spanning several datasets and heterogeneity levels. In the particular case of heterogeneously distributed CIFAR-10 dataset, FENS achieves up to a $26.9\\%$ higher accuracy over state-of-the-art (SOTA) OFL, being only $3.1\\%$ lower than FL. At the same time, FENS incurs at most $4.3\\times$ more communication than OFL, whereas FL is at least $10.9\\times$ more communication-intensive than FENS. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "FL is a widely adopted distributed machine learning (ML) approach, enabling clients to collaboratively train a common model over their collective data without sharing raw data with a central server [27]. Clients in FL engage in iterative parameter exchanges with the server over several communication rounds to train a model. While providing high accuracy, this process incurs substantial communication cost [19]. One-shot federated learning (OFL) [11] has been introduced to address the communication challenges in FL by reducing the exchange of models to a single round. Not surprisingly, this came with a loss of accuracy with respect to FL. ", "page_idx": 0}, {"type": "text", "text": "Typical OFL methods execute local training at the clients up to completion and form an ensemble of locally trained models at the server [7, 10, 46, 11]. The ensemble is distilled into a single model, through means of either auxiliary public dataset [10, 11] or synthetic data generated at the server [7, 14, 46]. While these OFL methods address communication challenges by reducing model exchanges to a single round, they often achieve lower accuracy compared to iterative FL. This is especially true when data distribution across clients is highly heterogeneous as OFL methods typically rely on simple prediction aggregation schemes such as averaging [11, 46], weighted averaging [7, 10] or voting [8]. ", "page_idx": 0}, {"type": "image", "img_path": "7rWTS2wuYX/tmp/3e1e787193e529422d672a495bed3af8504d6ffc199d6c2b3838b9953348fc7e.jpg", "img_caption": ["Figure 1: FENS in comparison to iterative and one-shot federated learning. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "We introduce FENS, a hybrid of OFL and standard FL. FENS aims to approach both the accuracy of iterative FL as well as the communication cost of OFL. Learning in FENS proceeds in two phases. In the first phase, similar to OFL, clients upload their locally-trained models to the server. Instead of using the traditional OFL aggregation, FENS employs a second phase of FL: the server constructs an ensemble with a prediction aggregator model stacked on top of the locally trained models. This advanced aggregation function is then trained by the clients in a lightweight FL training phase. The overall learning procedure is illustrated in Figure 1, alongside iterative and one-shot FL. ", "page_idx": 1}, {"type": "text", "text": "1.1 Our Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We show for the first time, to the best of our knowledge, that a shallow neural network for the aggregator model suffices to satisfactorily bridge the gap between OFL and FL. Leveraging a shallow aggregator model enables two major benefits: first, it induces significantly lower communication cost in the iterative phase, and second, the iterative refinement of this aggregator model significantly improves accuracy over existing OFL methods. By utilizing elements from both OFL and FL in this novel ensembling scheme, FENS achieves the best of both worlds: accuracy of FL and communication efficiency of OFL. ", "page_idx": 1}, {"type": "text", "text": "Through extensive evaluations on several benchmark datasets (CIFAR100, CIFAR-10, SVHN, and AG-News) across different heterogeneity levels, we demonstrate the efficacy of FENS in achieving FL-like accuracy at OFL-like communication cost. We extend our empirical evaluations to the FLamby benchmark [32], a realistic cross-silo FL dataset for healthcare applications. Our results show that in heterogeneous settings where even iterative FL algorithms struggle, FENS remains a strong competitor. We then conduct an extensive study of different aggregator models and highlight the accuracy vs. communication trade-off. Lastly, we show that FENS maintains high accuracy even with a comparable memory footprint. ", "page_idx": 1}, {"type": "image", "img_path": "7rWTS2wuYX/tmp/ab75de2822013c45ad56557abc556efb5446c82ea84c0d0d3b7e29cb50c79e4c.jpg", "img_caption": ["Bar labels indicate the normalized comm. cost w.r.t. OFL "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "To showcase FENS\u2019s performance, we compare its accuracy and communication costs against Co-Boosting [7], a state-of-the-art OFL method, and FEDADAM [33], a popular iterative FL algorithm, as shown in Figure 2. These evaluations are performed on the CIFAR-10 dataset with 20 clients across three heterogeneity levels: $\\alpha=0.01$ (very high), $\\alpha=0.05$ (high), and $\\alpha=0.1$ (moderate). Co-Boosting exhibits an accuracy gap ", "page_idx": 1}, {"type": "text", "text": "Figure 2: Test accuracy and communication cost of OFL, FENS and FL on CIFAR-10 dataset under high data heterogeneity. ", "page_idx": 1}, {"type": "text", "text": "of $13.7\\mathrm{~-~}26.9\\%$ compared to FEDADAM. FENS closes this accuracy gap, being only $0-3.1\\%$ lower than FEDADAM. To achieve this, FENS incurs only $3.8-4.3\\times$ more communication than Co-Boosting whereas FEDADAM is $10.9-22.1\\times$ more expensive than FENS. ", "page_idx": 1}, {"type": "text", "text": "1.2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "One-shot Federated Learning. Guha et al. [11] introduced one-shot FL, which limits communication to a single round. They proposed two main methods: $(i)$ heuristic selection for final ensemble clients, and $(i i)$ knowledge distillation (KD) for ensemble aggregation into a single model at the server using an auxiliary dataset. Subsequent methods based on KD [10, 22] require large, publicly available datasets similar to local client data for good performance, which are often difficult to obtain [50]. To address this, synthetic data generation using generative adversarial networks (GAN)s has been proposed [7, 46]. The SOTA Co-Boosting algorithm [7] iteratively generates and refines synthetic data and the ensemble model. In FEDCVAE-ENS [14], clients train variational autoencoders (VAEs) locally and upload decoders to the server, which generates synthetic samples for classifier training. FEDOV [8] trains an open-set classifier at each client to predict \u201cunknown\u201d classes, with the server ensembling these models and using open-set voting for label prediction. Other OFL approaches either do not fully consider data heterogeneity [22, 37], or face difficulties under high data heterogeneity [48]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Another line of research in OFL focuses on aggregating fully trained client model parameters [42, 45]. PFNM [45] matches neurons across client models for fully-connected networks, while FEDMA [42] extends this to convolutional neural networks (CNNs) and LSTMs. However, the performance of these methods drops with more complex models. Few theoretical works exist, such as [18], which analyze global model loss for overparameterized ReLU networks. Despite the advances, OFL still exhibits accuracy gap with iterative FL. We show that FENS narrows this accuracy gap while preserving communication efficiency. ", "page_idx": 2}, {"type": "text", "text": "Ensembles in Federated Learning. Ensembles have been previously studied in FL for a variety of different goals. FEDDF [24] performs robust model fusion of client ensembles to support model heterogeneity. The FEDBE algorithm [5] uses Bayesian Model Ensemble to aggregate parameters in each global round, improving over traditional parameter averaging. Hamer et al. propose FEDBOOST [12] that constructs the ensemble using simple weighted averaging and analyze its optimality for density estimation tasks. However, these works are designed for standard FL and rely on substantial iterative communication. In the decentralized edge setting, [38] show that collaborative inference via neighbor averaging can achieve higher accuracy over local inference alone. However, they assume a setting where clients can exchange query data during inference and consider only IID data replicated on all edge devices. The idea of learning an aggregator model closely resembles late fusion techniques in multimodal deep learning [25]. The key difference is that FENS focuses on fusing single modality models trained on heterogeneous data under the communication constraints of federated settings. ", "page_idx": 2}, {"type": "text", "text": "2 Description of FENS ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider a classification task represented by a pair of input and output spaces $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ , respectively. The system comprises $M$ clients, represented by $[M]=\\{1,\\dots,M\\}$ and a central server. Each client $i$ holds a local dataset $\\mathcal{D}_{i}\\,\\subset\\,\\mathcal{X}\\times\\mathcal{Y}$ . For a model $h_{\\theta}:\\mathcal{X}\\rightarrow\\mathcal{Z}$ parameterized by $\\theta\\in\\Theta\\subseteq\\mathbb{R}^{d}$ , each data point $(x,y)\\in\\mathcal{X}\\times\\mathcal{Y}$ incurs a loss of $\\ell(h_{\\theta}(x),y)$ where $\\ell:\\mathcal{Z}\\times\\mathcal{Y}\\to\\mathbb{R}$ . Denoting by $\\textstyle D:=\\bigcup_{i\\in[M]}D_{i}$ the union of all local datasets, the objective is to solve the empirical risk minimization (ERM) problem: $\\begin{array}{r}{\\operatorname*{min}_{\\theta\\in\\Theta}\\frac{1}{|\\mathcal{D}|}\\sum_{(x,y)\\in\\mathcal{D}}\\ell\\left(h_{\\theta}(x),y\\right)\\!.}\\end{array}$ ", "page_idx": 2}, {"type": "text", "text": "2.1 Federated Learning (FL) and One-shot FL (OFL) ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "FL algorithms, such as FedAvg [27], are iterative methods that enable the clients to solve the above ERM problem, without having to share their local data. In each iteration $t$ , the server broadcasts the current model parameter $\\theta_{t}$ to a subset of clients $S_{t}\\subseteq[M]$ . Each client $i\\in S_{t}$ updates the parameter locally over its respective dataset $\\mathcal{D}_{i}$ using an optimization method, typically stochastic gradient descent (SGD). Clients send back to the server their locally updated model parameters $\\{\\theta_{t}^{(i)},\\;i\\in S_{t}\\}$ . Lastly, the server updates the global model parameter to $\\begin{array}{r}{\\theta_{t+1}:=\\frac{1}{|S_{t}|}\\sum_{i\\in S_{t}}\\theta_{t}^{(i)}}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "In One-shot Federated Learning (OFL), the iterative exchanges in $\\mathrm{FL}$ are replaced with a one-shot communication of local models. Specifically, each client $i$ seeks a model $\\bar{\\theta}^{(i)}$ that approximately solves the ERM problem on their local data: $\\begin{array}{r}{\\operatorname*{min}_{\\theta\\in\\Theta}\\frac{1}{|\\mathcal{D}_{i}|}\\sum_{(x,y)\\in\\mathcal{D}_{i}}\\ell\\left(h_{\\theta}(x),y\\right)}\\end{array}$ , and sends ${\\theta}^{\\left(i\\right)}$ to the server. Upon receiving the local parameter ${\\theta}^{\\left(i\\right)}$ , corresponding to parametric model $\\pi_{i}=h_{\\theta^{(i)}}$ , the server builds an ensemble model of the form $\\begin{array}{r}{\\pi(x)=\\sum_{i\\in[M]}w_{i}\\pi_{i}(x)}\\end{array}$ . This ensemble model is then distilled into a single global model at the server using either a public dataset or synthetic data (generated by the server). Existing $\\mathrm{OFL}$ algorithms choose weights $w_{1},\\cdot\\cdot\\cdot,w_{M}$ in three different ways: (i) uniformly at random [46], $(i i)$ based on local label distributions [10], and $(i i i)$ dynamically adjusted based on generated synthetic data [7]. ", "page_idx": 2}, {"type": "text", "text": "2.2 FENS ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In FENS, the server builds the ensemble model using a generic aggregator $f_{\\lambda}:\\mathcal{Z}^{M}\\to\\mathcal{Z}$ , parameterized by $\\lambda\\in\\Lambda\\subset\\mathbb{R}^{q}$ to obtain a global model $\\pi:\\mathcal{X}\\rightarrow\\mathcal{Z}$ defined to be ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pi(x):=f_{\\lambda}(\\pi_{1}(x),\\ldots,\\pi_{M}(x)).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In case of standard aggregators such as weighted averaging, $q=M$ and $\\begin{array}{r}{\\lambda\\in(w\\in\\mathbb{R}_{+}^{M}\\,\\Big|\\sum_{i=1}^{M}w_{i}=}\\end{array}$ 1), and . In general, $f_{\\lambda}$ can be a non-linear trainable model such as a neural network. The overall learning procedure in FENS comprises two phases: ", "page_idx": 3}, {"type": "text", "text": "1. Local training and one-shot communication: Each client $i$ does local training to compute ${\\theta}^{\\left(i\\right)}$ , identical to OFL, and sends it to the server. 2. Iterative aggregator training: Upon receiving the local parameters ${\\theta}^{\\left(i\\right)}$ , the server reconstructs $\\pi_{i}:=h_{\\theta^{(i)}}$ , and obtains\u03bb  that approximately solves the following ERM problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\lambda\\in\\Lambda}\\quad\\frac{1}{|\\mathcal{D}|}\\sum_{(x,y)\\in\\mathcal{D}}\\ell\\left(f_{\\lambda}(\\pi_{1}(x),\\ldots,\\pi_{M}(x)),y\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The above ERM problem is solved using an iterative FL scheme (described above). For doing so, the server transmits the set of local models $\\{\\pi_{1},\\ldots,\\pi_{M}\\}$ to all the clients. The final model is given by $\\pi(x):=f_{\\widehat{\\lambda}}\\left(\\pi_{1}(x),\\ldots,\\pi_{M}(x)\\right)$ . ", "page_idx": 3}, {"type": "text", "text": "When solving for (2) using iterative $\\mathrm{FL}$ , only the aggregator parameters are transferred between the server and the clients. As we show through experiments, in the subsequent section, training an aggregator model is much simpler than training the local models $\\pi_{i}$ , and a shallow neural network suffices for $f_{\\lambda}$ . Algorithms 1 and 2 (Appendix C) provide the pseudo for FENS. ", "page_idx": 3}, {"type": "text", "text": "Connection with stacked generalization. The use of a trainable aggregator corresponds to an instance of stacked generalization [44] in the centralized ensemble literature, wherein the aggregation function is regarded as level 1 generalizer, while the clients\u2019 models are regarded as level 0 generalizers. It has been shown that level 1 generalizer serves the role of correcting the biases of level 0 generalizers, thereby improving the overall learning performance of the ensemble [44]. While stacked generalization has been primarily studied in centralized settings, through FENS we show that this scheme can be efficiently extended to an FL setting. ", "page_idx": 3}, {"type": "text", "text": "3 Experiments ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We split our evaluation into the following sections: (i) FENS vs OFL in Section 3.2; (ii) FENS vs FL and analysis of when FENS can match FL in Section 3.3; (iii) FENS on real-world cross-silo FLamby benchmark [32] in Section 3.4; (iv) FENS on language dataset in Section 3.5; (v) dissecting components of FENS in Section 3.6; and (vi) enhancing FENS efficiency in Section 3.7. ", "page_idx": 3}, {"type": "text", "text": "3.1 Experimental setup ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Datasets. We consider three standard vision datasets with varying level of difficulty, including SVHN [30], CIFAR-10 [30] and CIFAR-100 [30], commonly used in several OFL works [7, 10, 46] as well as one language dataset AG-News [47]. Vision experiments involve 20 clients, except in the scalability study, where client numbers vary; and AG-News uses 10 clients. The original training splits of these datasets are partitioned across clients using the Dirichlet distribution $\\mathtt{D i r}_{20}(\\alpha)$ , in line with previous works [7, 10, 14]. The parameter $\\alpha$ determines the degree of heterogeneity, with lower values leading to more heterogeneous distributions (see Appendix A, Figure 9). For our experiments involving the realistic healthcare FLamby benchmark, we experiment with 3 datasets: Fed-Camelyon16, Fed-Heart-Disease, and Fed-ISIC2019. Table 5 (Appendix A) presents an overview of the selected tasks. The datasets consist of a natural non independent and identically distributed (non-IID) partitioning across clients. In FENS, each client performs local training using $\\mathrm{\\dot{90}\\%}$ of their local training data while reserving $10\\%$ for the iterative aggregator training. We observed that by splitting the datasets, we achieve better performance than reusing the training data for aggregator training. For fairness, OFL and FL baselines run with each client using $100\\bar{\\%}$ of their dataset for local training. The testing set of each dataset is split $(50\\!-\\!50\\%)$ for validation and testing. We use the validation set to tune hyperparameters and always report the accuracy on the testing split. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "One-shot FL baselines. We compare FENS against 6 one-shot baselines: $(i)$ one-round FEDAVG [27]; (ii) FEDENS [11], the first one-shot method constituting an ensemble with uniform weights; (iii) FEDKD [10], based on auxiliary dataset; $(i\\nu)$ one-shot version of FED-ET [6]; (v) the data-free FEDCVAE-ENS [14]; and (vi) Co-Boosting [7], based on synthetic data generation. We use the best-reported hyperparameters in each work for the respective datasets wherever applicable or tune them. Appendix B.1 provides additional details regarding the one-shot baselines. ", "page_idx": 4}, {"type": "text", "text": "Iterative FL baselines. For comparison with FL, we consider 6 algorithms: (i) FEDAVG [27]; (ii) FEDPROX [23]; (iii) FEDNOVA [43]; $(i\\nu)$ SCAFFOLD [20]; (v) FEDYOGI [33]; and (vi) FEDADAM [33]. We tune learning rates for each algorithm. In addition to these baselines, we implement gradient compression with FEDAVG STC, following the sparsification and quantization schemes of STC [29]. In particular, we set the quantization precision to 16-bit and sparsity level to $50\\%$ , to reduce the communication cost of FEDAVG by $4\\times$ and keep the remaining setup to the same as above baselines. For the FLamby benchmark experiments, we use the reported hyperparameters which were obtained after extensive tuning, except with one difference. The authors purposefully restricted the number of rounds to be approximately the same as the number of epochs required to train on pooled data (see [32]). Since this might not reflect true FL performance, we rerun all FL strategies to convergence using the reported tuned parameters. Precisely, we run up to $10\\times$ more communication rounds than evaluated in the FLamby benchmark. We include more details on FL baselines in Appendix B.2. ", "page_idx": 4}, {"type": "text", "text": "FENS.2 For the CIFAR-10 and CIFAR-100 datasets, each client conducts local training for 500 epochs utilizing SGD as the local optimizer with an initial learning rate of 0.0025. For the SVHN and AG-News datasets, local training extends to 50 and 20 epochs respectively with a learning rate of 0.01. The learning rate is decayed using Cosine Annealing across all datasets. For the FLamby benchmark experiments, each client in FENS performs local training with the same hyperparameters as the client local baselines of FLamby. We experiment with two aggregator models, a 2-layer perceptron with ReLU activations and another that learns per-client per-class weights. We train the aggregator model using the FEDADAM algorithm where the learning rate is separately tuned for each dataset (Table 8, Appendix B.3). To reduce the communication costs corresponding to the ensemble download, we employ post-training model quantization at the server from FP32 to INT8. Appendix B.3 provides more details on FENS. ", "page_idx": 4}, {"type": "text", "text": "Configurations. In line with related work [10, 24, 35], we use ResNet-8 [13] as the client local model for our vision tasks and fine-tune DistilBert [34] for our language task. Our FLamby experiments use the same models as defined in the benchmark for each task (see Table 5, Appendix A). We report the average results across at least 3 random seeds. For iterative FL baselines, the communication cost corresponds to the round in which the best accuracy is achieved. ", "page_idx": 4}, {"type": "text", "text": "3.2 FENS vs OFL ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To assess FENS\u2019s efficacy, we experiment in non-IID settings, varying $\\alpha\\in\\{0.01,0.05,0.1\\}$ , and present results across datasets and baselines in Table 1. Our observations reveal challenges for oneshot methods under high heterogeneity, with the optimal baseline differing across settings. FEDAVG with one round exhibits the poorest performance. While FEDCVAE-ENS maintains consistent accuracy across various heterogeneity levels and datasets, it struggles particularly with CIFAR-10 and CIFAR-100, indicating challenges in learning effective local decoder models. Regarding distillationbased methods, FEDKD and Co-Boosting demonstrate similar performance on SVHN and CIFAR-10. However, FEDKD outperforms Co-Boosting on CIFAR-100, facilitated by the auxiliary public dataset for KD while Co-Boosting arduously generates its synthetic transfer dataset. FED-ET improves over FEDENS and is also competitive to FEDKD. Notably, FENS consistently outperforms the best baseline in each scenario, except for SVHN at $\\alpha=0.01$ , where FEDCVAE-ENS excels. FENS achieves significant accuracy gains, surpassing the best baseline by $11.4-26.9\\%$ on CIFAR-10 and $8.7-15.4\\%$ on CIFAR-100, attributed to its advanced aggregator model. ", "page_idx": 4}, {"type": "text", "text": "We chart the client communication costs incurred by all algorithms in Figure 3. The clients in FENS expend $3.6-4.3\\times$ more than one-shot algorithms owing to the ensemble download and iterative ", "page_idx": 4}, {"type": "text", "text": "Table 1: FENS vs one-shot FL for various heterogeneity levels across datasets. The highest achieved accuracy is presented as bold and the top-performing baseline is underlined. The rightmost column presents the performance difference between FENS and the top-performing baseline. ", "page_idx": 5}, {"type": "image", "img_path": "7rWTS2wuYX/tmp/6c251bdf2ec8a9b542d28f226d5dbe2f2663c106b470e1bc49f2d3d876920c6a.jpg", "img_caption": ["Figure 3: Total communication cost of FENS against OFL baselines. The clients in FENS expend roughly $3.7-4.3\\times$ more than OFL in communication costs. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "aggregator training. While these costs are greater than OFL, they are significantly lower than the costs incurred by iterative FL baselines as shown in Section 3.3. FEDCVAE-ENS has the lowest cost since the clients only upload the decoder component of their VAE model to the server. However, it also suffers from a significant performance gap with respect to other OFL baselines and FENS on the CIFAR-10 and CIFAR-100 datasets. ", "page_idx": 5}, {"type": "text", "text": "Varying number of clients. We also assess the performance of different baselines by varying the number of clients. We consider the CIFAR-10 dataset with $\\alpha=0.1$ and present the results in Table 2. FENS achieves the best accuracy surpassing the best-performing baseline FEDKD by $5.9-11.4\\%$ in accuracy points. This again demonstrates the beneftis of utilizing an advanced ensemble model with a trainable aggregator function. ", "page_idx": 5}, {"type": "table", "img_path": "7rWTS2wuYX/tmp/4f4cc57bd913543974d438e02817f7ae1eeead2c474a8173dd56cddf1562a427.jpg", "table_caption": ["Table 2: FENS vs one-shot FL on CIFAR-10 with varying number of clients. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "3.3 FENS vs Iterative FL ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We now compare the accuracy and communication cost of FENS against iterative FL baselines. After our extensive evaluation of all 6 iterative FL baselines (Table 10, Appendix D) on the CIFAR-10 dataset across various heterogeneity levels, we find that FEDADAM and FEDYOGI consistently perform the best. Hence, we show FEDADAM in our remaining evaluations. Figure 4 presents the results, showing FEDADAM as the representative of FL, FEDAVG STC as the representative of gradient compression, FEDKD as the representative of OFL, and FENS. Moreover, we show two versions of FEDKD, one additional with multi-round support to match the communication cost of FENS (details in Appendix B.2). We also show two versions of FEDADAM, one achieving the accuracy of FENS and the other with its maximum accuracy to facilitate effective comparison of communication costs. ", "page_idx": 5}, {"type": "image", "img_path": "7rWTS2wuYX/tmp/3214324d61a28a1f72d9be04f42be6cb3ca67b200ee46712b2247ad344b52620.jpg", "img_caption": ["Figure 4: FENS against iterative FL. The R indicates the number of global rounds, signifying the multi-round version of the OFL baseline. FENS achieves accuracy properties of iterative FL (FEDADAM) with a modest increase in communication cost compared to OFL (FEDKD). Numerical accuracy results are included in Table 11 (Appendix D). ", "Bar labels indicate the normalized communication cost w.r.t. OFL "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "We observe that FENS with its iteratively trained aggregator significantly closes the accuracy gap between OFL (FEDKD) and FL (FEDADAM) across all datasets and heterogeneity levels. Remarkably, the boost achieved is sufficient to match FEDADAM\u2019s accuracy at $\\alpha=\\{0.\\bar{0}1,0.\\dot{0}5\\}$ on the CIFAR-10 dataset. This comes at only a modest increase in communication costs which are $\\approx4\\times$ that of OFL across all cases. We observe that FEDADAM incurs $30-80\\times$ more communication than OFL to reach the same accuracy as FENS. Even adding multi-round support to FEDKD only marginally improves its performance. While the best accuracy achieved by FEDADAM still remains higher, it also comes at significant communication costs of $47-96\\times$ that of OFL. Furthermore, we observe that communication compression (FEDAVG STC) fails to preserve the accuracy of FL under high heterogeneity. Thus FENS achieves the best accuracy vs. communication trade-off, demonstrating accuracy properties of iterative FL while retaining communication efficiency of OFL. ", "page_idx": 6}, {"type": "text", "text": "3.3.1 When can FENS match iterative FL? ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we aim to understand when FENS can match iterative FL. The performance of ensembles depends upon $(i)$ the quality of local models; and (ii) data heterogeneity. The quality of local models in turn depends on the amount of local data held by the clients. As local models improve at generalizing locally, the overall performance of the ensemble is enhanced. In contrast, FL struggles to generate a good global model when the local datasets of clients significantly differ. Thus more volume of data does not analogously benefit FL due to high data heterogeneity. However, as heterogeneity reduces, FL excels and beneftis significantly from collaborative updates. This suggests that FENS under sufficiently large local datasets and high heterogeneity can match iterative FL\u2019s performance. We confirm this intuition through the following experiments on the SVHN dataset. ", "page_idx": 6}, {"type": "text", "text": "Setup. We study the performance of FL and FENS by progressively increasing the volume of data held by the clients. To this end, we consider the classification task on the SVHN dataset due to the availability of an extended training set of 604 388 samples, i.e., $\\approx10\\times$ bigger than the default SVHN dataset. We then experiment with fractions ranging from 10 to $100\\%$ of the total training set. Each client locally utilizes $90\\%$ for one-shot local model training and reserves $10\\%$ for iterative aggregator training, similar to previous sections. We then compare FENS with FEDAVG (FL baseline) on three levels of heterogeneity: $\\alpha=\\{0.05,0.1,1\\}$ , varying from highly non-IID to IID. We tune the learning rate for FEDAVG (details in Appendix B.2) and keep the remaining setup as in previous experiments. ", "page_idx": 6}, {"type": "image", "img_path": "7rWTS2wuYX/tmp/e28f4e9058b6fb059eff37e5e5998c687463b3b5c11936c39f9a7259ac44ef5c.jpg", "img_caption": ["Figure 5: Accuracy of FENS for increasing dataset size. Performance of FENS rapidly increases as the data volume increases. At high data heterogeneity, FENS matches iterative FL\u2019s accuracy. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Results. Figure 5 shows the results and confirms our prior insight behind the effective performance of FENS. Specifically, we observe that the growing volume of training data benefits FENS much more than FEDAVG. When the data distribution is homogeneous $(\\alpha=1)$ ), the performance of FENS improves faster than FEDAVG, but still remains behind. On the other hand, under high heterogeneity ( $(\\alpha=0.01)$ ), FENS quickly catches up with the performance of FEDAVG, matching the same accuracy when using the full training set. We conclude that under regimes of high heterogeneity and sufficiently large local datasets, FENS presents a practical alternative to communication expensive iterative FL. ", "page_idx": 7}, {"type": "text", "text": "3.4 Performance on real-world datasets ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we evaluate the performance of FENS on the real-world cross-silo FLamby benchmark [32]. Specifically, we present 5 iterative baselines and 2 one-shot baselines along with the client local baselines (Figure 6). For the one-shot FEDAVG and FEDPROX OFL baselines, we additionally tune the number of local updates. FEDKD is infeasible in these settings since it requires a public dataset for distillation, unavailable in the medical setting. FEDCVAEENS and Co-Boosting are also infeasible due to the difficulty in learning good decoder models or synthetic dataset generators for medical input data, a conclusion supported by their poor performance on the comparatively simpler CIFAR100 task (Table 1). Figure 6 shows the results with the first row comparing FENS against iterative FL algorithms and the second row against one-shot FL and the client local baselines. ", "page_idx": 7}, {"type": "text", "text": "When comparing to iterative FL, we observe that FENS is on par for the Fed-Heart-Disease dataset and performs better for Fed-Camelyon16. The iterative FL performance is affected by high heterogeneity [32] where the deterioration is more ", "page_idx": 7}, {"type": "image", "img_path": "7rWTS2wuYX/tmp/03881db5bb390ed134c0485c0015c93744c7974bbaf48652b13f9e2910ca554e.jpg", "img_caption": ["Figure 6: FENS in FLamby. FENS is on par with iterative FL (row-1), except when local models are weak (Fed-ISIC2019) while remaining superior in the one-shot setting (row-2). Numerical results included in Tables 12 to 17 (Appendix D). "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "significant for Fed-Camelyon16, which learns on large breast slides $(10000\\!\\times\\!2048)$ ) than for Fed-HeartDisease, which learns on tabular data. In such scenarios of heterogeneity, FENS can harness diverse local classifiers through the aggregator model to attain good performance. On the Fed-ISIC2019 dataset, however, FENS does not reach the accuracy of iterative FL. Clients in Fed-ISIC2019 exhibit high variance in local data amounts and model performance, with the largest client having $12\\mathrm{k}$ samples and the smallest only 225 (Table 5, Appendix A). We thus speculate that the Fed-ISIC2019 dataset falls within the low local training fraction regime depicted in Figure 5, exhibiting a larger accuracy gap compared to iterative FL. However, we note that FENS achieves superior performance over one-shot FEDAVG and one-shot FEDPROX, while performing at least as well as the best client local baseline across all datasets. Overall, these observations for FL algorithms have spurred new interest in developing a better understanding of performance on heterogeneous cross-silo datasets [32]. We show that FENS remains a strong competitor in such settings. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "3.5 Performance on language dataset ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We now study the performance of FENS on the AG-News dataset, comparing it against top-performing baselines FEDADAM and FEDKD in the iterative and one-shot categories, respectively. Figure 7 shows the results: at $\\alpha=0.1$ , FEDKD achieves $71.\\bar{5}\\%$ accuracy, leaving a gap to FEDADAM at $82.3\\%$ . FENS effectively closes this gap, reaching $78.8\\%$ . As heterogeneity reduces at $\\alpha=0.3$ , all algorithms achieve higher accuracies. FENS improves upon FEDKD from $79.3\\%$ to $84.5\\bar{\\%}$ while FEDADAM achieves $88.3\\%$ . Thus we observe consistent results on the language task as our vision benchmarks. ", "page_idx": 8}, {"type": "text", "text": "3.6 Dissecting FENS ", "text_level": 1, "page_idx": 8}, {"type": "image", "img_path": "7rWTS2wuYX/tmp/4c18069b4c44338f9c2fc99567e86bce2ff3a4f09dbbdbf73208a8c1c0dca5f2.jpg", "img_caption": ["AG-News dataset. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "We extensively evaluate various aggregation functions (details in Appendix B.4) on the CIFAR-10 dataset across diverse heterogeneity levels. ", "page_idx": 8}, {"type": "text", "text": "In particular, we assess static aggregation rules including averaging and weighted averaging, parametric aggregator models including a linear model, and a shallow neural network. We also evaluate an advanced version of voting [2] which involves computing competency matrices to reach a collective decision. In addition, we evaluate the Mixture-of-Experts (MoE) aggregation rule [36] where only the gating function is trained via federation. Figure 8 illustrates the accuracy, communication costs, and breakdown for all aggregations. Trained aggregator models outperform static aggregations, incurring additional costs for ensemble download and iterative training. The NN aggregator emerges as the top performer, offering the best accuracy vs. communication trade-off. Notably, the iterative training cost of the NN aggregator model for several rounds is lower than the OFL phase itself. Regarding accuracy, only MoE outperforms NN at $\\alpha\\,=\\,0.01$ , where extreme heterogeneity induces expert specialization, while the trained gating network accurately predicts the right expert. However, MoE\u2019s performance declines as heterogeneity decreases while its communication costs remain higher due to the size of the gating network. ", "page_idx": 8}, {"type": "image", "img_path": "7rWTS2wuYX/tmp/bded446bbfb6784c84a78c8648ad14f6733b0a2e73a33b4f458e47926012ae94.jpg", "img_caption": ["Figure 8: Accuracy of different aggregation functions on the CIFAR-10 dataset. NN offers the best accuracy vs. communication trade-off, with its iterative training taking up only a fraction of the total cost. Numerical accuracy values are included in Table 9 (Appendix D). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "3.7 Enhancing FENS efficiency ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The FENS global model comprises the aggregator model stacked atop the ensemble of client local models. Although FENS achieves strong accuracy, the ensemble model can be computationally and memory intensive. We used FP32 to INT8 quantization in our previous experiments which reduces the memory costs by $4\\times$ (Appendix B.3). In this section, we explore two additional approaches to reduce FENS\u2019s overheads. ", "page_idx": 8}, {"type": "text", "text": "What if we distill FENS into a single model? To enable efficient inference, we can distill the FENS global model into a single model at the server using KD once the training is completed. Specifically, we distill the FENS ensemble model comprising 20 ResNet-8 client models and the shallow aggregator neural network into a single ResNet-8 model. Table 3 presents the results on the CIFAR-10 dataset for $\\alpha=\\{0.01,0.05,0.1\\}$ distilled using CIFAR-100 as the auxiliary dataset. We observe a slight accuracy drop arising from the process of distillation, which is standard behavior [39]. While we distill using the standard distillation algorithm [15], we note that this accuracy gap can be further reduced through the use of more advanced distillation methods [16, 49]. ", "page_idx": 9}, {"type": "table", "img_path": "7rWTS2wuYX/tmp/429fe0e91ed3597bf6d5b9d7a80a1dd5259b4c313e47083139f656d8e0b3a7fc.jpg", "table_caption": ["Table 3: Accuracy of FENS after distillation on the CIFAR-10 dataset. "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "7rWTS2wuYX/tmp/ce01a0f6092249682c1a2b3bfd006d8f62af0497d8806ceed398149f6f067100.jpg", "table_caption": ["Table 4: FENS vs FEDADAM under similar memory footprint on CIFAR-10. DS stands for downsized. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "What if we match the memory footprint of FENS to FEDADAM? While the above approach enables efficient inference, we still need to mitigate training time costs. We now consider a downsized (DS) version of ResNet-8, where the width of a few layers is reduced so that the total size of the FENS downsized (FENS-DS) model approximately matches the size of the single model in FEDADAM. Table 4 presents the results on the CIFAR-10 dataset for various heterogeneity levels. Note that no quantization is considered for FENS and FENS-DS, hence the contrast in values with Table 3. Under a comparable memory footprint, FENS-DS remains competitive with the original FEDADAM, with only a slight drop in accuracy compared to FENS. On the other hand, using the downsized model as the global model in FEDADAM-DS results in a significant accuracy drop (from $39.32\\%$ to $29.69\\%$ ) under high data heterogeneity $\\alpha=0.01)$ . Thus, the memory overhead of FENS can be alleviated while retaining its communication benefits without too much impact on accuracy. ", "page_idx": 9}, {"type": "text", "text": "4 Discussion and Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Limitations. One limitation is the memory required on client devices to store the ensemble model for aggregator training. We explored quantization and downsizing to mitigate this issue. Future work could investigate aggregator models that do not require access to all client models in the ensemble. This memory issue is only present during training; after training, FENS can be distilled into a single global model on the server, enabling efficient inference as shown in Section 3.7. Another limitation is the increased vulnerability to attacks during iterative aggregator training, unlike OFL, which limits the attack surface to one round. However, this only affects the aggregator model, since the client local models are still uploaded in one shot. Privacy can be further enhanced in FENS through techniques such as differential privacy [9] or trusted execution environments [28]. Specifically, clients can use differentially private SGD [1] for local training, providing a differentially private local model for the ensemble, while the aggregator training could leverage a differentially private FL algorithm [31]. ", "page_idx": 9}, {"type": "text", "text": "Benefits. In addition to low communication costs and good accuracy, FENS provides three important advantages. First, it supports model heterogeneity, allowing different model architectures across federated clients [21]. Second, FENS enables rapid client unlearning [4], towards the goal of the right to be forgotten in GDPR [26]. In FENS, unlearning a client can be achieved by simply re-executing the lightweight aggregator training by excluding the requested clients\u2019 model from the ensemble. This is more efficient than traditional FL, where disincorporating knowledge from a single global model can be costly. Lastly, if a small server-side dataset is available, such as a proxy dataset for bootstrapping FL [3, 19], FENS can train the aggregator model on the server. This makes FENS applicable in model market scenarios of OFL [7, 41] where clients primarily offer pre-trained models. ", "page_idx": 9}, {"type": "text", "text": "To conclude, we introduce FENS, a hybrid approach combining OFL and FL. FENS emphasizes local training and one-shot model sharing, similar to OFL, which limits communication costs. It then performs lightweight aggregator training in an iterative FL-like fashion. Our experiments on diverse tasks demonstrated that FENS is highly effective in settings with high data heterogeneity, nearly achieving FL accuracy while maintaining the communication efficiency of OFL. Additionally, FENS supports model heterogeneity, rapid unlearning, and is applicable to model markets. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Nirupam is partly supported by Swiss National Science Foundation (SNSF) project 200021_200477, \u201cControlling The Spread of Epidemics: A Computing Perspective\u201d. The authors are thankful to Milos Vujasinovic and Sayan Biswas for their helpful discussions, and to the anonymous reviewers of NeurIPS 2024 for their valuable time and constructive inputs that shaped the final version of this work. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer and communications security, pages 308\u2013318, 2016.   \n[2] Ruth Ben-Yashar and Jacob Paroush. Optimal decision rules for fixed-size committees in polychotomous choice situations. Social Choice and Welfare, 18(4):737\u2013746, 2001.   \n[3] Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir Ivanov, Chlo\u00e9 Kiddon, Jakub Kone\u02c7cn\u00fd, Stefano Mazzocchi, Brendan McMahan, Timon Van Overveldt, David Petrou, Daniel Ramage, and Jason Roselander. Towards federated learning at scale: System design. In MLSys, 2019.   \n[4] Lucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. Machine unlearning. In 2021 IEEE Symposium on Security and Privacy (SP), pages 141\u2013159. IEEE, 2021.   \n[5] Hong-You Chen and Wei-Lun Chao. Fed{be}: Making bayesian model ensemble applicable to federated learning. In International Conference on Learning Representations, 2021.   \n[6] Yae Jee Cho, Andre Manoel, Gauri Joshi, Robert Sim, and Dimitrios Dimitriadis. Heterogeneous ensemble knowledge transfer for training large models in federated learning. In Lud De Raedt, editor, Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22, pages 2881\u20132887. International Joint Conferences on Artificial Intelligence Organization, 7 2022. Main Track.   \n[7] Rong Dai, Yonggang Zhang, Ang Li, Tongliang Liu, Xun Yang, and Bo Han. Enhancing oneshot federated learning through data and ensemble co-boosting. In The Twelfth International Conference on Learning Representations, 2024. [8] Yiqun Diao, Qinbin Li, and Bingsheng He. Towards addressing label skews in one-shot federated learning. In International Conference on Learning Representations, 2023.   \n[9] Robin C Geyer, Tassilo Klein, and Moin Nabi. Differentially private federated learning: A client level perspective. arXiv preprint arXiv:1712.07557, 2017.   \n[10] Xuan Gong, Abhishek Sharma, Srikrishna Karanam, Ziyan Wu, Terrence Chen, David Doermann, and Arun Innanje. Preserving privacy in federated learning with ensemble crossdomain knowledge distillation. Proceedings of the AAAI Conference on Artificial Intelligence, 36(11):11891\u201311899, Jun. 2022.   \n[11] Neel Guha, Ameet Talwalkar, and Virginia Smith. One-shot federated learning. arXiv preprint arXiv:1902.11175, 2019.   \n[12] Jenny Hamer, Mehryar Mohri, and Ananda Theertha Suresh. Fedboost: A communicationefficient algorithm for federated learning. In International Conference on Machine Learning, pages 3973\u20133983. PMLR, 2020.   \n[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[14] Clare Elizabeth Heinbaugh, Emilio Luz-Ricca, and Huajie Shao. Data-free one-shot federated learning under very high statistical heterogeneity. In The Eleventh International Conference on Learning Representations, 2023.   \n[15] Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. In NIPS Deep Learning and Representation Learning Workshop, 2015.   \n[16] Fotis Iliopoulos, Vasilis Kontonis, Cenk Baykal, Gaurav Menghani, Khoa Trinh, and Erik Vee. Weighted distillation with unlabeled examples. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 7024\u20137037. Curran Associates, Inc., 2022.   \n[17] Maximilian Ilse, Jakub Tomczak, and Max Welling. Attention-based deep multiple instance learning. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 2127\u20132136. PMLR, 10\u201315 Jul 2018.   \n[18] Divyansh Jhunjhunwala, Shiqiang Wang, and Gauri Joshi. Towards a theoretical and practical understanding of one-shot federated learning with fisher information. In Federated Learning and Analytics in Practice: Algorithms, Systems, Applications, and Opportunities, 2023.   \n[19] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aur\u00e9lien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. Foundations and Trends\u00ae in Machine Learning, 14(1\u20132):1\u2013210, 2021.   \n[20] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In International Conference on Machine Learning, pages 5132\u20135143. PMLR, 2020.   \n[21] Daliang Li and Junpu Wang. Fedmd: Heterogenous federated learning via model distillation. NeurIPS Workshop on Federated Learning for Data Privacy and Confidentiality, 2019.   \n[22] Qinbin Li, Bingsheng He, and Dawn Song. Practical one-shot federated learning for cross-silo setting. In Zhi-Hua Zhou, editor, Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pages 1484\u20131490. International Joint Conferences on Artificial Intelligence Organization, 8 2021. Main Track.   \n[23] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. Proceedings of Machine learning and systems, 2:429\u2013450, 2020.   \n[24] Tao Lin, Lingjing Kong, Sebastian U Stich, and Martin Jaggi. Ensemble distillation for robust model fusion in federated learning. Advances in Neural Information Processing Systems, 33:2351\u20132363, 2020.   \n[25] Kuan Liu, Yanen Li, Ning Xu, and Prem Natarajan. Learn to combine modalities in multimodal deep learning. arXiv preprint arXiv:1805.11730, 2018.   \n[26] Alessandro Mantelero. The eu proposal for a general data protection regulation and the roots of the \u2018right to be forgotten\u2019. Computer Law & Security Review, 29(3):229\u2013235, 2013.   \n[27] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial Intelligence and Statistics, pages 1273\u20131282. PMLR, 2017.   \n[28] Aghiles Ait Messaoud, Sonia Ben Mokhtar, Vlad Nitu, and Valerio Schiavoni. Shielding federated learning systems against inference attacks with arm trustzone. In Proceedings of the 23rd ACM/IFIP International Middleware Conference, pages 335\u2013348, 2022.   \n[29] Alessio Mora, Luca Foschini, and Paolo Bellavista. Structured sparse ternary compression for convolutional layers in federated learning. In 2022 IEEE 95th Vehicular Technology Conference: (VTC2022-Spring), pages 1\u20135, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[30] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. 2011. ", "page_idx": 12}, {"type": "text", "text": "[31] Maxence Noble, Aur\u00e9lien Bellet, and Aymeric Dieuleveut. Differentially private federated learning on heterogeneous data. In International Conference on Artificial Intelligence and Statistics, pages 10110\u201310145. PMLR, 2022.   \n[32] Jean Ogier du Terrail, Samy-Safwan Ayed, Edwige Cyffers, Felix Grimberg, Chaoyang He, Regis Loeb, Paul Mangold, Tanguy Marchand, Othmane Marfoq, Erum Mushtaq, Boris Muzellec, Constantin Philippenko, Santiago Silva, Maria Tele\u00b4nczuk, Shadi Albarqouni, Salman Avestimehr, Aur\u00e9lien Bellet, Aymeric Dieuleveut, Martin Jaggi, Sai Praneeth Karimireddy, Marco Lorenzi, Giovanni Neglia, Marc Tommasi, and Mathieu Andreux. Flamby: Datasets and benchmarks for cross-silo federated learning in realistic healthcare settings. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 5315\u20135334. Curran Associates, Inc., 2022.   \n[33] Sashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konec\u02c7n\u00fd, Sanjiv Kumar, and Hugh Brendan McMahan. Adaptive federated optimization. In International Conference on Learning Representations, 2021.   \n[34] Victor Sanh, L Debut, J Chaumond, and T Wolf. Distilbert, a distilled version of bert: Smaller, faster, cheaper and lighter. arxiv 2019. arXiv preprint arXiv:1910.01108, 2019.   \n[35] Felix Sattler, Arturo Marban, Roman Rischke, and Wojciech Samek. Cfd: Communicationefficient federated distillation via soft-label quantization and delta coding. IEEE Transactions on Network Science and Engineering, 9(4):2025\u20132038, 2022.   \n[36] Noam Shazeer, \\*Azalia Mirhoseini, \\*Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-ofexperts layer. In International Conference on Learning Representations, 2017.   \n[37] MyungJae Shin, Chihoon Hwang, Joongheon Kim, Jihong Park, Mehdi Bennis, and Seong-Lyun Kim. Xor mixup: Privacy-preserving data augmentation for one-shot federated learning. arXiv preprint arXiv:2006.05148, 2020.   \n[38] Nir Shlezinger, Erez Farhan, Hai Morgenstern, and Yonina C. Eldar. Collaborative inference via ensembles on the edge. In ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8478\u20138482, 2021.   \n[39] Samuel Don Stanton, Pavel Izmailov, Polina Kirichenko, Alexander A Alemi, and Andrew Gordon Wilson. Does knowledge distillation really work? In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021.   \n[40] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning, pages 6105\u20136114. PMLR, 2019.   \n[41] Manasi Vartak, Harihar Subramanyam, Wei-En Lee, Srinidhi Viswanathan, Saadiyah Husnoo, Samuel Madden, and Matei Zaharia. Modeldb: a system for machine learning model management. In Proceedings of the Workshop on Human-In-the-Loop Data Analytics, HILDA \u201916, New York, NY, USA, 2016. Association for Computing Machinery.   \n[42] Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni. Federated learning with matched averaging. In International Conference on Learning Representations, 2020.   \n[43] Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective inconsistency problem in heterogeneous federated optimization. Advances in neural information processing systems, 33:7611\u20137623, 2020.   \n[44] David H. Wolpert. Stacked generalization. Neural Networks, 5(2):241\u2013259, 1992.   \n[45] Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. In International conference on machine learning, pages 7252\u20137261. PMLR, 2019.   \n[46] Jie Zhang, Chen Chen, Bo Li, Lingjuan Lyu, Shuang Wu, Shouhong Ding, Chunhua Shen, and Chao Wu. DENSE: Data-free one-shot federated learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.   \n[47] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015.   \n[48] Yanlin Zhou, George Pu, Xiyao Ma, Xiaolin Li, and Dapeng Wu. Distilled one-shot federated learning. arXiv preprint arXiv:2009.07999, 2020.   \n[49] Yichen Zhu, Ning Liu, Zhiyuan Xu, Xin Liu, Weibin Meng, Louis Wang, Zhicai Ou, and Jian Tang. Teach less, learn more: On the undistillable classes in knowledge distillation. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.   \n[50] Zhuangdi Zhu, Junyuan Hong, and Jiayu Zhou. Data-free knowledge distillation for heterogeneous federated learning. In International Conference on Machine Learning, pages 12878\u201312889. PMLR, 2021. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Organization of the Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Datasets   \nB Additional Experimental Details B.1 One-shot FL baselines B.2 Iterative FL baselines B.3 FENS B.4 Aggregation rules   \nC FENS pseudo code   \nD Numerical results   \nE Compute resources   \nF Broader impact ", "page_idx": 14}, {"type": "text", "text": "A Datasets ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "As mentioned in Section 3.1, we focus on classification tasks and experiment with 3 datasets in FLamby benchmark including Fed-Camelyon16, Fed-Heart-Disease and Fed-ISIC2019. Table 5 overviews the selected tasks in this work. ", "page_idx": 14}, {"type": "table", "img_path": "7rWTS2wuYX/tmp/3c73ba14a070b37fa5a16c3dfccd298f611d03e6873fa40d4779c518d0223a52.jpg", "table_caption": ["Table 5: Overview of selected datasets and tasks in FLamby. We defer additional details to [32]. "], "table_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "7rWTS2wuYX/tmp/3dbc4c32e189e7c643c592392dfc8e0a455c1d293a86952fe33ed0e9b2634cd9.jpg", "img_caption": ["Figure 9: Visualizing the effect of changing $\\alpha$ on the CIFAR-10 dataset. Dot size corresponds to the number of samples of a given class in a given node. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "B Additional Experimental Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 One-shot FL baselines ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The client local training for all OFL baselines (except FEDCVAE-ENS) as well as FENS is conducted alike, using the parameters reported in [10] for the CIFAR-10 and CIFAR-100 datasets. For the SVHN and AG-News datasets, local training is conducted using the SGD optimizer with a learning rate of 0.01 for 50 and 20 local epochs respectively and decayed using Cosine Annealing. All vision datasets use a batch size of 16 while AG-News uses a batch-size of 8 for local training. For FEDCVAE-ENS, we use the same CVAE architecture and local training parameters as reported by the authors [14]. However, for the distillation at the server, we use ResNet-8 as the classifier model at the server to maintain fairness with other baselines. For the distillation phase of FEDKD, we use the setup described by the authors [10] without inference quantization. For the one-shot version of FED-ET, we set the diversity regularization parameter to the best value of $\\lambda=0.05$ . For both FEDKD and FED-ET, we use CIFAR-10, CIFAR-100, and TinyImageNet as the auxiliary datasets for distillation for SVHN, CIFAR-10, and CIFAR-100 datasets respectively. We consider a $60-40\\%$ split for the AG-News dataset where local training is conducted on the $60\\%$ split while the remaining $40\\%$ is treated as the auxiliary dataset for distillation at the server in FEDKD. Distillation in Co-Boosting using synthetically generated data also uses the best-reported hyperparameters [7]. For one-round FEDAVG, we additionally tune the number of local epochs performed before aggregation by considering $\\{1,2,5,10,15,20\\}$ epochs. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "B.2 Iterative FL baselines ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We experiment with 6 different baselines including FEDAVG, FEDPROX, FEDNOVA, FEDADAM, FEDYOGI, and SCAFFOLD. We perform extensive hyperparameter tuning on the CIFAR-10 dataset for all levels of heterogeneity as detailed below. The server assumes full client participation, i.e., all clients participate in each round. For our vision benchmarks, each client performs 2 local epochs per round using a batch size of 16. For our language task, clients train for 50 local steps in each round using a batch-size of 8. We run FL training until convergence and report the best accuracy achieved. We found all algorithms to converge in less than 100 communication rounds on the vision tasks and in 150 rounds on the language task. ", "page_idx": 15}, {"type": "text", "text": "Hyperparameter tuning. Below we describe our tuning procedure derived from several previous works [23, 32, 33]. For the FEDAVG algorithm, we tune the client learning rate $(\\eta_{l})$ over the values $\\{0.1,0.01,0.001,0.0001\\}$ separately for every $\\alpha\\,\\in\\,\\{0.01,0.05,0.1\\}$ . For the FEDPROX algorithm, our grid space was $\\left\\{0.1,0.01\\right\\}$ and $\\{1,0.1,0.01\\}$ for the client learning rate $(\\eta_{l})$ and the proximal parameter $(\\mu)$ respectively. This was again separately tuned for every value of $\\alpha\\in$ $\\{0.{\\bar{0}}1,0.05,0.{\\bar{1}}\\}$ . For the FEDYOGI and the FEDADAM algorithm, we consider the grid space of $\\{0.1,0.01,0.001,0.0001\\}$ and $\\{10,1,0.1,0.01,0.001\\}$ for the client learning rate $(\\eta_{l})$ and the server learning rate $(\\eta_{s})$ respectively. This explodes the search significantly when tuning for every value of $\\alpha$ . From our tuning results for the FEDAVG and the FEDPROX algorithm, we noticed that the optimal parameter values were the same within the following two subgroups of $\\alpha-\\{0.01,0.05\\}$ and $\\bar{\\{0.1\\}}$ (Table 6). Hence, to keep the tuning tractable, we tune only for one $\\alpha$ in each subgroup and reuse the values for other alphas within the same subgroup. For the FEDNOVA algorithm, we use the version with both client and global momentum which was reported to perform the best [43]. We consider the search space $\\left\\{0.00\\bar{5},0.01,0.02,0.05,0.08\\right\\}$ for the client learning rate $(\\eta_{l})$ as done by the authors [43] and tune separately for every value of $\\alpha$ . Finally, for the SCAFFOLD algorithm, we consider the search spaces $\\{0.1,0.01,0.001,0.0001\\}$ and $\\{1.0,0.1,0.01\\}$ for $\\eta_{l}$ and $\\eta_{s}$ respectively and also tune separately for every $\\alpha$ . We first conduct the tuning procedure on the CIFAR-10 dataset and report the obtained hyperparameters in Table 6. ", "page_idx": 15}, {"type": "table", "img_path": "7rWTS2wuYX/tmp/325339c1da03b5923830e694d94c83a67aa474349cf33280a8dbb1665561f4f7.jpg", "table_caption": ["Table 6: Best hyperparameters obtained for the different algorithms on the CIFAR-10 dataset. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "We run all iterative FL baselines using the above parameters and present the results in Table 10. Based on our results in the Table 10 for CIFAR-10, we observe that FEDADAM and FEDYOGI consistently perform the best. Hence, to keep the experiments tractable, we tune and present just FEDADAM as a representative of the iterative FL family for our evaluations on the SVHN, CIFAR-100 datasets in Section 3.3 and AG-News in Section 3.5. For FEDADAM on the AG-News dataset, we note that the training is conducted using only the $60\\%$ split (see Appendix B.1) to achieve a fair comparison with FEDKD. For our experiments involving the extended SVHN dataset in Section 3.3.1, we again tune the client learning rate $(\\eta_{l})$ for the FEDAVG algorithm over the search space $\\{0.1,0.01,0.001,0.0001\\}$ separately for every $\\dot{\\alpha}\\in\\{0.01,0.05,0.1\\}$ . ", "page_idx": 15}, {"type": "text", "text": "FEDAVG with gradient compression. To implement FEDAVG with gradient compression, we followed the sparsification and quantization schemes of STC [29]. We use the quantization level of 16-bit and sparsity of $50\\%$ . This results in a communication cost reduction of $4\\times$ against standard FEDAVG in every round. For each dataset and heterogeneity level, we tune the learning rate over the search space $\\{0.{\\dot{1}},0.05,0.01,0.001\\}$ . We keep the remaining setup the same as FEDAVG. ", "page_idx": 16}, {"type": "text", "text": "Multi-round FEDKD. Since FENS incurs a communication cost four times that of OFL, we also evaluate FEDKD with multi-round support. We explore two approaches: i) pre-training for 3 rounds using FEDAVG, then applying FEDKD, and $i i$ ) using FEDKD followed by 3 fine-tuning rounds with FEDAVG. In the first case, each FEDKD client begins training from the global model produced by FEDAVG, while in the second, FEDAVG starts from the FEDKD model. We observe that pre-training with FEDAVG offers little improvement, likely due to the forgetting effect from multiple local training epochs, whereas fine-tuning with FEDAVG boosts FEDKD performance. For our experiments in Section 3.3, we thus present the multi-round version of FEDKD with fine-tuning. We remark that this multi-round support is still insufficient to match the performance of FENS, which achieves significantly higher accuracy as shown in Table 7 while incurring similar communication costs. ", "page_idx": 16}, {"type": "table", "img_path": "7rWTS2wuYX/tmp/1a9f271ad0b7c53f2bbdf82706180d225501d6dd26b1fbd21bbc621e55d0551a.jpg", "table_caption": ["Table 7: FEDKD under multi-round support on the CIFAR-10 dataset. "], "table_footnote": ["B.3 FENS "], "page_idx": 16}, {"type": "text", "text": "Clients in FENS perform local training similar to OFL baselines as described in Section 3.1 and appendix B.1. Let $\\dot{z}_{j}\\in\\mathbb{R}^{C}$ denote the logits obtained from each client model $\\pi_{j}$ for all $j\\in[M]$ on a given input where $C$ is the number of classes. We use one of the two aggregator models as follows. The first one is a multilayer perceptron using ReLu activations and a final classifier head as follows: $f=W_{2}^{T}\\sigma(W_{1}^{T}z)$ where $\\dot{W}_{1}\\in\\mathbb{R}^{M C\\times k}$ $\\mathbf{\\check{W}}_{2}\\in\\mathbb{R}^{k\\times C},z=\\mathrm{concat}(z_{1},\\dots,z_{M})\\in\\mathbb{R}^{M C}$ is the concatenated logit vector and $\\sigma(x)=\\operatorname*{max}\\{x,0\\}$ is the ReLU function. The parameter $k$ determines the number of units in the hidden layer of this perceptron model. The second one is $\\begin{array}{r}{f=\\sum_{i=1}^{M}\\lambda_{i}\\odot z_{i}}\\end{array}$ where $\\lambda_{1},\\dots,\\lambda_{M}\\in\\mathbb{R}^{C}$ are weight vectors and $\\odot$ denotes coordinate-wise product. This model learns per-class per-client weights as the model parameters. For all datasets, the aggregator model is trained using the FEDADAM algorithm where the learning rate is separately tuned for each dataset. Table 8 presents the tuned learning rate and tuned training parameters per dataset. ", "page_idx": 16}, {"type": "table", "img_path": "7rWTS2wuYX/tmp/ceb388f777b72f5447cad24c2ba9e9ef69a5602ff2fed6cc21c3b8611f610ac3.jpg", "table_caption": ["Table 8: Aggregator training in FENS. We use FEDADAM as the FL algorithm with the following client $(\\eta_{l})$ and server $(\\eta_{s})$ learning rates. The parameter $k$ corresponds to the weight matrices $W_{1}$ and $W_{2}$ . "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Model quantization in FENS. Clients in FENS incur a critical cost of downloading the ensemble model from the server to initiate the aggregator training. To reduce the communication burden on the clients, the server employs post-training model quantization of all received client local models from FP32 to INT8, reducing the download costs by $4\\times$ . Alternatively, the quantization can also be executed on the client side. The quantization results in a drop of $\\approx1-2\\%$ test accuracy for every client model compared to the corresponding non-quantized model. However, the subsequent aggregator training phase in FENS provides resilience to this drop in the accuracy of client models in the ensemble. In fact, we observe that the final accuracy achieved after aggregator training is slightly higher when using the quantized models as compared to unquantized models due to the regularising effect of quantization on generated logits. The model quantization also provides reduced memory usage on client devices during aggregator training. We use the standard PyTorch quantization library to implement quantization in FENS. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "B.4 Aggregation rules ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Averaging. Averaging corresponds to the following static aggregation rule: $\\begin{array}{r}{f=\\sum_{i=1}^{M}\\lambda_{i}\\odot z_{i}}\\end{array}$ where $\\begin{array}{r}{\\lambda_{i}=[\\frac{1}{M},\\cdot\\cdot\\cdot,\\frac{1}{M}]}\\end{array}$ and $\\odot$ denotes coordinate-wise product. ", "page_idx": 17}, {"type": "text", "text": "Weighted Averaging. In weighted averaging, the $\\lambda_{1},\\dots,\\lambda_{M}\\in\\mathbb{R}^{C}$ are typically assigned based on local training dataset statistics. In FEDKD [10], $\\begin{array}{r}{\\pmb{\\lambda_{i}}=[\\frac{n_{i}^{1}}{\\sum_{i\\in[M]}n_{i}^{1}},\\frac{n_{i}^{2}}{\\sum_{i\\in[M]}n_{i}^{2}},\\pmb{\\cdot}\\cdot\\cdot,\\frac{n_{i}^{C}}{\\sum_{i\\in[M]}n_{i}^{C}}]}\\end{array}$ where $n_{i}^{j}$ corresponds to the number of samples of class $j$ with client $i$ . ", "page_idx": 17}, {"type": "text", "text": "Linear. This aggregation corresponds to having a single learnable scalar weight for each client $\\textstyle f=\\sum_{i=1}^{M}w_{i}z_{i}$ . The learnable parameters in this case consist of the vector $[w_{1},w_{2},\\ldots,w_{M}]^{T}$ . ", "page_idx": 17}, {"type": "text", "text": "Neural network (NN). Let $z_{j}\\in\\mathbb{R}^{C}$ denote the logits obtained from each client model $\\pi_{j}$ for all $j\\in[M]$ on a given input where $C$ is the number of classes. The NN aggregation corresponds to any neural network-based model $f:\\mathcal{Z}^{M}\\to\\mathcal{Z}$ that operates on the logits produced by the client models. Denoting $z=\\mathrm{concat}(z_{1},\\dots,z_{M})\\in\\mathbb{R}^{M C}$ as the concatenated vector of logits, $f$ corresponds to the following 2 layer neural network $f=W_{2}^{T}\\sigma(W_{1}^{T}z)$ where $W_{1}\\in\\mathbb{R}^{M\\bar{C}\\times k}$ , $\\dot{W}_{2}\\in\\mathbb{R}^{\\bar{k}\\times C}$ and $\\sigma(x)\\,=\\,\\operatorname*{max}\\{x,{\\dot{0}}\\}$ is the ReLU function. Here $k$ determines the number of units in the hidden layer and controls the expressivity of the network. This aggregation is much more powerful than the previously mentioned aggregations, owing to its ability to discern complex patterns across all $M\\times C$ logits. The learnable parameters comprise the weight matrices $\\{W_{1},\\bar{W}_{2}\\}$ . ", "page_idx": 17}, {"type": "text", "text": "Polychotomous Voting. Polychotomous voting [2], was originally developed in social choice theory to reach a collective decision when offered $C$ alternatives (classification labels in our case) in a committee of $M$ experts (clients in our case). This method requires as input: $(i)$ classwise \u201ccompetency\u201d scores of each client: $P_{i}^{c}(r)$ indicating the probability of $i^{t h}$ -client to vote for label $c$ when the ground truth is $r;(i i)\\,p_{p r i o r}(r)$ : prior probability distribution over correct alternatives $r$ ; and $(i i i)$ the \u201cbenefti\u201d vector of the committee: $B(c|r)$ indicating the committee\u2019s benefti in choosing label $c$ when the correct class is $r$ . Given this information, Ben-Yashar and Paroush [2] derive a criterion for the optimal decision that maximizes expected utility. This criterion is not computed using a closed-form expression, and we generically express it as ", "page_idx": 17}, {"type": "equation", "text": "$$\nf(\\pi_{1},\\dots,\\pi_{M};P_{1},\\dots,P_{M};{p_{p r i o r}};B)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $f$ corresponds to a procedure that evaluates and compares benefits for each choice $c\\in[C]$ given the competency matrices $\\{P_{i}\\}_{i=1}^{M}$ , the priors $p_{p r i o r}$ and the benefit function $B$ . Since the competency matrices for each client model are not directly available in our distributed setting, we learn them by federation in the network. More specifically, each client computes the competency matrix for every client model in the ensemble on its local data and transfers them to the server. The server then aggregates the received competency matrix to produce the final competency matrix per client to be used in decision-making. We further use a simple benefit function for our experiments that assigns $B(c/r)=1$ when $c=r$ (correct choice) and $B(c/r)=0$ when $c\\neq r$ (incorrect choice) and set the prior to be uniform over the set of labels. ", "page_idx": 17}, {"type": "text", "text": "Mixture-of-Experts (MoE). The MoE aggregation [36] considers both the input and the logits in the following form: $\\begin{array}{r}{f\\,=\\,\\sum_{i\\in[M]}G(x)_{i\\cdot\\pi_{i}}\\bar{(x)}}\\end{array}$ . Thus, MoE aggregation $f:\\dot{(\\boldsymbol{\\mathcal{X}},\\boldsymbol{\\mathcal{Z}}^{M})}\\rightarrow\\bar{\\boldsymbol{\\mathcal{Z}}}$ is more expressive compared to other aggregations which only consider logits $f:\\mathcal{Z}^{M}\\to\\mathcal{Z}$ . Here, $G:\\mathcal{X}\\overset{\\bullet}{\\rightarrow}[0,1]^{M}$ is called a gating network that generates scalar weights for every expert $\\pi_{1},\\cdot\\cdot\\cdot,\\pi_{M}$ based on the input $x\\in\\mathscr{X}$ . In FENS with MoE aggregation, only the gating network is trained via the federation in the network while $\\{\\pi_{i}\\}_{i=1}^{M}$ correspond to the locally trained client models. We use a simple CNN with two convolutional blocks comprising ReLU activation and max pooling layers followed by $2\\,\\mathrm{FC}$ layers with ReLU activation, which in turn is followed by the final classification head. Despite its expressivity, learning a good gating network incurs significant communication costs and remains difficult under heterogeneous data in federated settings. ", "page_idx": 17}, {"type": "table", "img_path": "7rWTS2wuYX/tmp/89a1c1e9b0a8ea0d29c83861962ce6c450e0d3796a5a12650a4ce432a01eed90.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Algorithm 2: FENS from the clients perspective ", "page_idx": 18}, {"type": "text", "text": "Require :Local dataset $\\mathcal{D}_{i}$ , loss function $\\ell$ , local steps $K$ and client learning rate $\\eta_{l}$   \n1 Procedure FENS_CLIENT():   \n2 Split $\\mathcal{D}_{i}$ randomly into $90\\%$ $\\mathcal{D}_{i1}$ and $10\\%$ $\\mathcal{D}_{i2}$   \n3 Receive parameters $\\theta$ from the server   \n4 Obtain ${\\theta}^{\\left(i\\right)}$ through local training of $\\theta$ on $D_{i1}$   \n5 Send converged model parameters ${\\theta}^{\\left(i\\right)}$ to server (one-shot)   \n6 Receive $\\{\\theta^{(i)},\\,i\\in[M]\\}$ from the server   \n7 while Receive aggregator model parameters $\\lambda_{t}$ from the server do   \n8 Initialize $\\lambda_{t}^{(i)}\\leftarrow\\lambda_{t}$   \n9 for $k=0,1,\\ldots,K$ do   \n10 Sample mini-batch $b\\in\\mathcal{D}_{i2}$   \n11 $\\begin{array}{r}{\\ell_{b}\\gets\\frac{1}{|b|}\\sum_{(x,y)\\in b}\\ell\\left(f_{\\lambda_{t}^{(i)}}(\\pi_{1}(x),\\ldots,\\pi_{M}(x)),y\\right)}\\end{array}$   \n12 $\\lambda_{t}^{(i)}\\leftarrow\\lambda_{t}^{(i)}-\\eta_{l}\\nabla\\ell_{b}$   \n13 Send $\\lambda_{t}^{(i)}$ back to the server ", "page_idx": 18}, {"type": "text", "text": "C FENS Algorithm ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Algorithm 1 outlines the role of the server in FENS. The process begins with the server initializing the parameter $\\theta$ corresponding to the parametric model $\\pi=h_{\\theta}$ , which it sends to all clients for local training (lines 2-3). Once clients complete their local training, they return their updated models to the server (line 4). If quantized is enabled, the server quantizes all the local models from FP32 to INT8 using a quantization algorithm (line 6). The server then redistributes all models back to the clients (line 7), enabling each to contribute to the aggregation process that follows. In the final stage, the server iteratively trains an aggregator model in FL fashion, which is designed to combine client models into a single, improved global model (lines 9-12). During each round, the server selects a subset of clients to update the aggregator model (line 10), refining it further with each iteration until convergence. ", "page_idx": 18}, {"type": "text", "text": "Algorithm 2 explains the client-side process in FENS. Each client starts by splitting its local dataset into two parts: one for one-shot local training and a smaller part for the iterative aggregator training (line 2). Using the received model, clients first train on $\\mathcal{D}_{i1}$ (line 4) and send their converged local model back to the server (line 5). In subsequent rounds, clients receive from the server the aggregator model (line 7) which they refine it locally using $\\mathcal{D}_{i2}$ (lines 8-12). Finally, clients send the updated aggregator parameters back to the server (line 13), contributing to the global aggregation process. ", "page_idx": 18}, {"type": "text", "text": "D Numerical Results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we include the numerical values in Tables 9 to 17 corresponding to the plots presented in Sections 3.3, 3.4 and 3.6 for a complete reference. ", "page_idx": 19}, {"type": "table", "img_path": "7rWTS2wuYX/tmp/0bd48fd5f54c3aebb7baea18791a8ddb7a0895dc1ee9732a585fdb3a63871cc4.jpg", "table_caption": ["Table 9: FENS aggregation methods on CIFAR-10. Results of Figure 8. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "7rWTS2wuYX/tmp/7d307717a2c09549079789059f00702d6a43ce61aae116718d101237711fc0dd.jpg", "table_caption": ["Table 10: FENS vs SOTA FL algorithms on the CIFAR-10 dataset. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "7rWTS2wuYX/tmp/fa73ad95f234d3bbbeba3109a4fa2ca6a720730878cef9db9df1bc66a9299e14.jpg", "table_caption": ["Table 11: FENS vs. iterative FL. Results from Figure 4. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Table 12: Figure 6 results. FENS vs. iterative FL \u2013 Fed-Camelyon16 (row 1). ", "page_idx": 19}, {"type": "table", "img_path": "7rWTS2wuYX/tmp/3787fd8ededb20aa36c5ec31be730136c8b94cbdfb91bfdd05500015cc72f298.jpg", "table_caption": ["Table 13: Figure 6 results. FENS vs. one-shot FL \u2013 Fed-Camelyon16 (row 2). "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "7rWTS2wuYX/tmp/94347244371dee6e1ceb6646f9a73adb2b2f8af82107f194dd436785c8a97d6f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "7rWTS2wuYX/tmp/f61c4173a9b576cdbe6f1ac69a0860a66d776014d19bbc931ddbb2e9fccabfd2.jpg", "table_caption": ["Table 14: Figure 6 results. FENS vs. iterative FL \u2013 Fed-Heart-Disease (row 1). "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "7rWTS2wuYX/tmp/cabb915bf1486134b52367a7ad7d24021682f88ba3254072ce9af595de9c8a6e.jpg", "table_caption": ["Table 15: Figure 6 results. FENS vs. one-shot FL \u2013 Fed-Heart-Disease (row 2). "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "7rWTS2wuYX/tmp/48703aa61f138d3d01ae17ac9d13d76738c242f3bd9886617011606f997efbe7.jpg", "table_caption": ["Table 16: Figure 6 results. FENS vs. iterative FL \u2013 Fed-ISIC2019 (row 1). "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "7rWTS2wuYX/tmp/e08d90ec086ab9ebc2e08e316b01ef19498368e8d753233b06aaafa935bcf8d1.jpg", "table_caption": ["Table 17: Figure 6 results. FENS vs. one-shot FL \u2013 Fed-ISIC2019 (row 2). "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "E Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We use a cluster comprising a mix of $2\\mathbf{x}$ Intel Xeon Gold 6240 $@$ 2.5 GHz of 36 hyper-threaded cores and $2\\mathbf{x}$ AMD EPYC 7302 $@$ 3 GHz of 64 hyper-threaded cores, equipped with $4\\mathbf{x}$ NVIDIA Tesla V100 32G and $8\\mathbf{x}$ NVIDIA Tesla A100 40G GPU respectively. Training of local models can take up to 2.5 hours in wall-clock time depending on the dataset, while FENS aggregator model training executes in under 30 minutes in wall-clock time. The time required for executing the baselines varies significantly depending on the baseline, with up to 24 hours in wall clock time for Co-Boosting. Across all experiments that are presented in this article, including different seeds and hyperparameter tuning, the total virtual CPU and GPU time is approximately 3500 and 8000 hours respectively. ", "page_idx": 20}, {"type": "text", "text": "F Broader Impact ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Federated Learning (FL) has significantly advanced privacy-preserving machine learning, particularly in sensitive domains like healthcare and finance, by facilitating collaborative model training without sharing raw data. The development of FENS, which combines FL\u2019s accuracy with the communication efficiency of One-shot FL (OFL), carries numerous positive implications. By simultaneously reducing communication costs and maintaining high accuracy, FENS enhances the accessibility and practicality of FL, thereby promoting wider adoption, especially in resource-constrained environments. This has the potential to catalyze advancements in privacy-sensitive sectors like healthcare and finance, where FL is extensively utilized. ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Our claims in the abstract and the introductions are appropriately scoped and well supported through our extensive empirical assessments spanning multiple datasets, baselines and settings. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We have discussed the limitations of our work in Section 4. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not include theoretical results. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have included elaborate descriptions regarding the setup and hyperparameters needed to reproduce the paper in Appendix B, complementing our description in Section 3.1. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We open-source our code at https://github.com/sacs-epfl/fens. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Yes, we have included all details in Section 3.1 and Appendix B. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Each of our experiments is repeated with at least three random seeds. We report standard deviations in all tables and display the $95\\%$ confidence intervals in our plots. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We have included the details on compute resources in Appendix E. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We have reviewed the Code of Ethics and believe our work adheres to these guidelines. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We have discussed the broad impacts of our work in Appendix F. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 24}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We affirm that the medical datasets employed in our experiments within the FLamby benchmark were acquired and utilized in strict accordance with their respective licensing agreements and ethical guidelines. We obtained the necessary permissions and approvals from the appropriate authorities and/or institutions responsible for data collection, and we adhered to all relevant ethical standards and regulations. The owners of the original assets used in this paper were properly cited and attributed. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We open-source our code at: https://github.com/sacs-epfl/fens. Our repository is well documented with all the instructions to run the code. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 26}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]