{"importance": "This paper is important because it presents **FENS**, a novel approach that significantly improves the accuracy of one-shot federated learning while maintaining its communication efficiency. This addresses a critical challenge in federated learning, paving the way for more efficient and accurate distributed machine learning systems.  The findings are particularly relevant to researchers working on resource-constrained environments or dealing with high data heterogeneity. The techniques used in FENS, such as the shallow neural network for aggregation, may inspire new directions in federated ensemble learning.", "summary": "FENS: a novel federated ensembling scheme that boosts one-shot federated learning accuracy to near iterative FL levels, while maintaining low communication costs.", "takeaways": ["FENS achieves significantly higher accuracy than state-of-the-art one-shot federated learning methods.", "FENS approaches the accuracy of iterative federated learning with significantly lower communication overhead.", "A lightweight prediction aggregator model is sufficient to bridge the performance gap between one-shot and iterative federated learning."], "tldr": "Federated learning (FL) is an efficient way to train machine learning models on decentralized data without sharing raw data. However, standard FL algorithms are iterative, leading to high communication costs. One-shot federated learning (OFL) reduces communication to a single round, but it typically suffers from accuracy loss, especially under high data heterogeneity. \nThis paper introduces FENS, a novel federated ensembling scheme that combines the communication efficiency of OFL with the accuracy of FL. FENS proceeds in two phases: 1) clients train models locally and send them to a server, and 2) clients collaboratively train a lightweight prediction aggregator model using FL. Extensive experiments show that FENS significantly outperforms existing OFL methods and approaches the accuracy of FL with minimal extra communication, making it suitable for resource-constrained settings and highly heterogeneous data distributions.", "affiliation": "EPFL", "categories": {"main_category": "Machine Learning", "sub_category": "Federated Learning"}, "podcast_path": "7rWTS2wuYX/podcast.wav"}