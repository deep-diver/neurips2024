[{"figure_path": "7rWTS2wuYX/figures/figures_1_1.jpg", "caption": "Figure 1: FENS in comparison to iterative and one-shot federated learning.", "description": "This figure compares three different approaches to federated learning: standard iterative federated learning, one-shot federated learning, and the proposed FENS method.  In iterative FL, the server and clients exchange partially trained models over multiple rounds. In one-shot FL, fully trained models are sent from clients to the server in a single round. FENS combines aspects of both:  clients initially send fully trained models to the server (like one-shot FL), but then the server uses those models to train a lightweight aggregator model iteratively with the clients (like standard FL). The figure visually represents the flow of models and data between the server and clients in each approach, highlighting the key differences in communication rounds and model states.", "section": "1 Introduction"}, {"figure_path": "7rWTS2wuYX/figures/figures_1_2.jpg", "caption": "Figure 2: Test accuracy and communication cost of OFL, FENS and FL on CIFAR-10 dataset under high data heterogeneity.", "description": "This figure compares the performance of three federated learning approaches: One-shot federated learning (OFL), the proposed FENS method, and iterative federated learning (FL).  The comparison is done across three different levels of data heterogeneity (\u03b1 = 0.01, 0.05, 0.1) on the CIFAR-10 dataset. The top panel shows the test accuracy achieved by each method, while the bottom panel illustrates the client communication cost in gigabytes (GiB).  The bar labels in the bottom panel additionally indicate the normalized communication cost relative to OFL.  The results illustrate that FENS achieves significantly higher accuracy than OFL, while maintaining a relatively low communication cost compared to FL.", "section": "3 Experiments"}, {"figure_path": "7rWTS2wuYX/figures/figures_5_1.jpg", "caption": "Figure 3: Total communication cost of FENS against OFL baselines. The clients in FENS expend roughly 3.7 \u2013 4.3\u00d7 more than OFL in communication costs.", "description": "This figure compares the communication costs of FENS with various one-shot federated learning (OFL) baselines across three datasets (CIFAR-100, CIFAR-10, and SVHN) under different levels of data heterogeneity.  It shows that while FENS incurs a higher communication cost than the OFL baselines (3.7-4.3 times higher), this cost is significantly lower than that of iterative federated learning methods (not shown in this figure). The bar chart visually represents the normalized communication costs, illustrating that the overhead of FENS is relatively modest compared to the substantial improvements in accuracy achieved by using its novel federated ensembling scheme.", "section": "3.2 FENS vs OFL"}, {"figure_path": "7rWTS2wuYX/figures/figures_6_1.jpg", "caption": "Figure 4: FENS against iterative FL. The R indicates the number of global rounds, signifying the multi-round version of the OFL baseline. FENS achieves accuracy properties of iterative FL (FEDADAM) with a modest increase in communication cost compared to OFL (FEDKD). Numerical accuracy results are included in Table 11 (Appendix D).", "description": "This figure compares the performance of FENS against iterative federated learning (FL) and one-shot federated learning (OFL) baselines.  It shows test accuracy and communication costs across different datasets (CIFAR-100, CIFAR-10, SVHN) and heterogeneity levels.  The key takeaway is that FENS achieves accuracy comparable to iterative FL with significantly lower communication overhead than traditional FL, and higher accuracy than one-shot OFL methods. The multi-round version of FEDKD, an OFL baseline, is included to provide context for the communication cost comparison.", "section": "3.3 FENS vs Iterative FL"}, {"figure_path": "7rWTS2wuYX/figures/figures_7_1.jpg", "caption": "Figure 5: Accuracy of FENS for increasing dataset size. Performance of FENS rapidly increases as the data volume increases. At high data heterogeneity, FENS matches iterative FL's accuracy.", "description": "This figure shows the results of an experiment designed to understand when FENS can match iterative FL. The experiment varies the amount of training data available to the clients, while also varying the level of data heterogeneity (\u03b1). The results show that as the amount of training data increases, the performance of FENS improves significantly faster than that of FEDAVG, especially under high heterogeneity (\u03b1 = 0.05). When the data distribution is homogeneous (\u03b1 = 1), the performance of FENS still remains lower than that of FEDAVG. However, under high heterogeneity and sufficiently large local datasets, FENS is able to match the accuracy of FEDAVG (iterative FL).", "section": "3.3 FENS vs Iterative FL"}, {"figure_path": "7rWTS2wuYX/figures/figures_7_2.jpg", "caption": "Figure 6: FENS in FLamby. FENS is on par with iterative FL (row-1), except when local models are weak (Fed-ISIC2019) while remaining superior in the one-shot setting (row-2). Numerical results included in Tables 12 to 17 (Appendix D).", "description": "This figure compares the performance of FENS against various iterative and one-shot federated learning baselines on three real-world datasets from the FLamby benchmark (Fed-Camelyon16, Fed-Heart-Disease, Fed-ISIC2019).  The top row shows the comparison against iterative methods, indicating FENS performs competitively with iterative FL, except on the Fed-ISIC2019 dataset where local models are weaker. The bottom row compares FENS against one-shot methods and client-side local baselines, highlighting FENS's consistent superiority.  Detailed numerical results are referenced in Appendix D.", "section": "3.4 Performance on real-world datasets"}, {"figure_path": "7rWTS2wuYX/figures/figures_8_1.jpg", "caption": "Figure 7: FENS on the AG-News dataset.", "description": "The figure compares the performance of FENS, FedKD (one-shot), and FedAdam (iterative) on the AG-News dataset under two different levels of data heterogeneity (\u03b1 = 0.1 and \u03b1 = 0.3).  It shows that FENS significantly improves accuracy compared to FedKD, while achieving comparable performance to FedAdam, especially at higher heterogeneity (\u03b1 = 0.3).  This highlights the efficacy of the proposed approach, particularly in challenging, non-IID settings.", "section": "3.5 Performance on language dataset"}, {"figure_path": "7rWTS2wuYX/figures/figures_8_2.jpg", "caption": "Figure 8: Accuracy of different aggregation functions on the CIFAR-10 dataset. NN offers the best accuracy vs. communication trade-off, with its iterative training taking up only a fraction of the total cost. Numerical accuracy values are included in Table 9 (Appendix D).", "description": "This figure compares various aggregation methods used in the FENS model, evaluating their performance based on test accuracy and client communication costs across different levels of data heterogeneity.  It shows that the neural network (NN) aggregator provides the best balance between accuracy and communication efficiency.  While methods like MoE (Mixture of Experts) achieve high accuracy under high heterogeneity, they are significantly more communication intensive. The figure highlights the trade-off between accuracy and communication cost for each method. The breakdown of communication costs (one-shot local training, ensemble download, and aggregator training) for each method is also visualized.", "section": "3.6 Dissecting FENS"}, {"figure_path": "7rWTS2wuYX/figures/figures_14_1.jpg", "caption": "Figure 9: Visualizing the effect of changing \u03b1 on the CIFAR-10 dataset. Dot size corresponds to the number of samples of a given class in a given node.", "description": "This figure shows how data heterogeneity changes with different values of alpha (\u03b1) in the CIFAR-10 dataset. Each plot represents a different alpha value, ranging from highly non-IID (\u03b1 = 0.01) to IID (\u03b1 = 100). The x-axis represents the node identifier, and the y-axis represents the class.  The size of each dot corresponds to the number of samples of that class in that node.  It visually demonstrates how data distribution across clients (nodes) becomes more uniform as alpha increases, transitioning from highly skewed distributions to more balanced ones.", "section": "A Datasets"}]