[{"type": "text", "text": "Generalizability of experimental studies ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Experimental studies are a cornerstone of machine learning (ML) research. A com  \n2 mon, but often implicit, assumption is that the results of a study will generalize   \n3 beyond the study itself, e.g. to new data. That is, there is a high probability that   \n4 repeating the study under different conditions will yield similar results. Despite the   \n5 importance of the concept, the problem of measuring generalizability remains open.   \n6 This is probably due to the lack of a mathematical formalization of experimental   \n7 studies. In this paper, we propose such a formalization and develop a quantifiable   \n8 notion of generalizability. This notion allows to explore the generalizability of   \n9 existing studies and to estimate the number of experiments needed to achieve the   \n10 generalizability of new studies. To demonstrate its usefulness, we apply it to two   \n11 recently published benchmarks to discern generalizable and non-generalizable   \n12 results. We also publish a Python module that allows our analysis to be repeated   \n13 for other experimental studies. ", "page_idx": 0}, {"type": "text", "text": "14 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "15 Due to the importance of experimental studies, the machine learning (ML) community advocates for   \n16 high methodological standards [20, 12, 13, 17, 8, 31, 32, 44]. Failure to meet these standards can   \n17 have significant consequences, such as the ongoing reproducibility crisis [6, 47, 50, 51, 30].   \n18 Reproducibility is not the only desirable property of a study. For example, the reader expects that the   \n19 best encoders of categorical features identified in [41] will not only remain the best when the study   \n20 is reproduced, but will also outperform their competitors on new datasets. This property of getting   \n21 the same results from different data is known as replicability [46, 48]. Replicability is a special case   \n22 of generalizability, the property of obtaining the same results with any change in the inputs. The   \n23 assumption of generalizability is arguably the main motivation for extensive experimental studies and   \n24 benchmarks. However, existing definitions of generalizability do not quantify how well the results of   \n25 a study can be transferred to other contexts. This hinders the usefulness of such studies and leads to   \n26 confusion. For example, articles [38, 41, 49, 42] and [19, 8, 11, 13, 29, 43] report that the results of   \n27 experimental studies are often contradictory.   \n28 Quantifying generalizability can also help determine the appropriate size of experimental studies. For   \n29 example, one dataset is unlikely to be sufficient to draw far-reaching conclusions, but $10^{6}$ datasets   \n30 are likely enough. Of course, such large studies are usually not practical: it is crucial to determine the   \n31 minimum amount of data needed to achieve generalizability. This principle also applies to decisions   \n32 other than the number of datasets, such as the choice of quality metric and the initialization seed.   \n33 A notion similar to generalizability is model replicability [1, 22, 23, 24, 33, 36, 37]. A model is   \n34 $\\rho$ -replicable if, given i.i.d. samples from the same data distribution, the trained models are the same   \n35 with probability $1-\\rho$ [33]. Adapting this definition to quantify generalizability is not trivial, as   \n36 it requires formalizing experimental studies. The latter must take into account several aspects: the   \n37 research question, the results of a study, and how to compare the results. Regarding the problem of   \n38 defining the size of experimental studies, the current literature addresses the (crucial, but orthogonal)   \n39 problem of choosing appropriate experimental factors [20, 12, 13, 17, 8, 31, 32, 44]. While these   \n40 studies recommend varying the factors, they do not help decide how many of the factor levels are   \n41 enough. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "42 Our contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "43 1. we formalize experimental studies and their results;   \n44 2. we propose a quantifiable definition of the generalizability of experimental studies;   \n45 3. we develop an algorithm to estimate the size of a study to obtain generalizable results;   \n46 4. we consider two recent experimental studies on categorical encoders [41] and Large Lan  \n47 guage Models [55] and show how their results may or may not be generalizable.   \n48 5. we will publish the GENEXPY1 Python module to repeat our analysis in other studies.   \n49 Paper outline: Section 2 is related work, Section 3 formalizes experimental studies, Section 4 defines   \n50 generalizability and provides the algorithm to estimate the required size of a study for generalizability,   \n51 Section 5 contains the case studies, Section 6 describes limitations and concludes. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "52 2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "53 We first discuss the literature related to the motivation we are tackling, i.e., why experimental studies   \n54 may not generalize. Second, we overview the existing concept of model replicability, closely related   \n55 to our work. Finally, we show other meanings that these words can assume in other domains.   \n56 Non-generalizable results. It is well known that experimental results can significantly vary based   \n57 on design choices [38, 41, 49, 42]. Possible reasons include an insufficient number of datasets [19,   \n58 41, 3, 12] as well as differences in hyperparameter tuning [13, 41], initialization seed [30], and   \n59 hardware [56]. As a result, the statistical benchmarking literature advocates for experimenters to   \n60 motivate their design choices [7, 43, 11, 13, 44] and clearly state the conclusions they are attempting   \n61 to draw from their study [7, 45].   \n62 Replicability and generalizability in ML. Our work formalizes the definitions of replicability and   \n63 generalizability given in [48, 46]. Intuitively, replicable work consists of repeating an experiment   \n64 on different data, while generalizable work varies other factors as well \u2014 e.g., quality metric,   \n65 implementation. A recent line of work, initiated by [33], has linked replicability to model stability: a   \n66 $\\rho$ -replicable model learns (with probability $1-\\rho)$ the same parameters from different i.i.d. samples.   \n67 This definition has later been adapted and applied to other learning algorithms [23], clustering [24],   \n68 reinforcement learning [22, 37], convex optimization [1], and learning rules [36]. Recent efforts   \n69 have been bridging the gap between replicability, differential privacy, generalization error, and global   \n70 stability [15, 16, 26, 45, 21]. However, these applications remain limited to model replicability.   \n71 Replicability and generalizability in Science. In other fields of Science, generalizability and   \n72 replicability take different meanings. In social sciences, generalizability theory is a tool to quantify   \n73 the effect of different factors on numerical responses [14]. In medicine, the replicability proposed   \n74 in [34] is the probability of observing a positive treatment effect in a meta-study. Although these   \n75 concepts are related to generalizability of experimental studies, they are limited to purely numerical   \n76 responses or specific study designs. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "77 3 Experiments and experimental studies ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "78 An experimental study is a set of experiments comparing the same alternatives under different   \n79 experimental conditions. An experimental condition is a tuple of levels of experimental factors, the   \n80 parameters defining the experiments. Different factors play different roles in the study: the design   \n81 and held-constant factors are fixed by design, while the generalizability of a study is defined in terms   \n82 of the allowed-to-vary factors. The study aims at answering a research question, which defines its   \n83 scope and goals.   \n84 Example 3.1. (The \u201ccheckmate-in-one\u201d task, cf. Figure 1) An experimenter wants to compare three   \n85 Large Language Models (LLMs), the alternatives, on the \u201ccheckmate-in-one\u201d task [55, 2, 5, 4, 18].   \n86 The assignment is to find the unique checkmating move from a position of pieces on a chessboard: an   \n87 LLM succeeds if and only if it outputs the correct move. The experimenter considers two experimental   \n88 factors: the number of shots, $n$ , and the initial position on the chessboard, $\\mathbf{pos}_{l}$ . The number of shots   \n89 is a design factor, while the initial position is an allowed-to-vary factor. The experimenter wants to   \n90 find if $\\mathrm{LLM_{1}}$ ranks consistently against the other two LLMs when changing the initial position, for a   \n91 fixed number of shots. ", "page_idx": 1}, {"type": "image", "img_path": "aYJ2T5TXoX/tmp/e1d694b190d6b17b0ff8a82be0ea7415f5cb472ec4a29285bd9d5b81c64394cf.jpg", "img_caption": ["Figure 1: Two empirical studies on the checkmate-in-one task, cf. Example 3.1. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "92 The rest of this section defines the terms introduced above. ", "page_idx": 2}, {"type": "text", "text": "93 3.1 Experiments ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "94 An experiment evaluates all the considered alternatives under a valid experimental condition. ", "page_idx": 2}, {"type": "text", "text": "95 Alternatives. An alternative $a\\in A$ is an object compared in the study, like an LLM in Example 3.1.   \n96 Here, $A$ is the set of alternatives considered in the study, with cardinality $n_{a}$ .   \n97 Experimental factors. An experimental factor is anything that could, in principle, affect the result   \n98 of an experiment. $i$ denotes a factor, $C_{i}$ the (possibly infinite) set of levels $i$ can take, $c\\in C_{i}$ a level   \n99 of $i$ , and $I$ the set of all factors. We adapt Montgomery\u2019s classification of experimental factors [44,   \n100 Chapter 1] and discern between design factors, held-constant factors, and allowed-to-vary factors.   \n101 \u2022 Design factors, e.g., whether and how to tune the hyperparameters, quality metrics, number   \n102 of shots, are chosen by the experimenter.   \n103 \u2022 Held-constant factors, e.g., implementation, initialization seed, number of cross-validated   \n104 folds, may affect the outcome but are not in the scope of the experiment and are fixed by the   \n105 experimenter.   \n106 \u2022 Allowed-to-vary factors, e.g., \u201cdataset\u201d or \u201cchessboard position\u201d in Example 3.1, may affect   \n107 the outcome but cannot be held constant: the experimenter expects results to generalize w.r.t.   \n108 these factors; $I_{\\mathrm{atv}}$ denotes them.   \n109 Experimental conditions. An experimental condition c is a tuple of levels of experimental factors,   \n110 $\\begin{array}{r}{\\mathbf{c}\\bar{=}\\,(c_{i})_{i\\in I}\\in C\\subseteq\\prod_{i\\in I}C_{i}}\\end{array}$ . We endow $C$ with a probability $\\mu$ , as we will need to sample from it to   \n111 define the result of a study in Section ??. The probability space $(C,{\\mathcal{F}},\\mu)$ is the universe of valid   \n112 experimental conditions. $C$ may not coincide with $\\textstyle\\prod_{i\\in I}{\\dot{C}}_{i}$ as some experimental conditions may be   \n113 invalid, i.e., illegal or not of interest. Validity has to be assessed on a case-by-case basis. For instance,   \n114 in Example 3.1, $C=\\{(\\mathsf{p o s}_{l},n)\\}_{l,n}$ , where $\\mathbf{pos}_{l}$ is a legal configuration of pieces on a chessboard   \n115 and $m$ is the non-negative number of shots.   \n116 Experimental results. The experiment function $E$ evaluates the alternatives $A$ under a valid   \n117 experimental condition $\\mathbf{c}\\in C$ . Unless necessary, we consider $A$ fixed and omit it in our notation.   \n118 We require that $E:C\\to\\mathcal{R}_{n_{a}}$ is a measurable function, for some fixed $A$ . Finally, the result of an   \n119 experiment $E\\left(A,\\mathbf{c}\\right)$ is a ranking on $A$ .   \n120 Definition 3.1 (Ranking (with ties)). A ranking $r$ on $A$ is a transitive and reflexive binary endorelation   \n121 on $A$ . Equivalently, $r$ is a totally ordered partition of $A$ into tiers of equivalent alternatives. $r(a)$   \n122 denotes the rank of $a\\in A$ , i.e., the position of the tier of $a$ in the ordering. W.l.o.g. $(\\mathcal{R}_{n_{a}},\\mathcal{P}(\\mathcal{R}_{n_{a}})\\dot{)}$   \n123 denotes the measure space of all rankings of $n_{a}$ objects, where $\\mathcal{P}$ indicates the power set.   \n124 Example 3.1 (Continued). The result of an experiment on $({\\sf p o s}_{l},n)$ is a ranking of the three LLMs,   \n125 according to whether or not they output the checkmating move. Suppose that only $\\mathrm{LLM_{1}}$ and $\\mathrm{LLM_{2}}$   \n126 output the correct move. Then $E({\\sf p o s}_{l},n)$ ranks $\\mathrm{LLM_{1}}$ and $\\mathrm{LLM_{2}}$ tied as best and $\\mathrm{LLM_{3}}$ as worst. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "127 3.2 Experimental studies ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "128 A study is defined by its research question $\\mathcal{Q}$ , i.e., its scope and goals. The scope consists of the   \n129 alternatives $A$ , the valid experimental conditions $C$ , and the allowed-to-vary factors $I_{\\mathrm{atv}}$ . The goal is   \n130 the kind of conclusions one is attempting to draw from the study. For now, the goal is a statement of   \n131 interests, i.e., a set of strings.   \n132 Definition 3.2 (Research question). The research question $\\mathcal{Q}=(A,C,I_{\\mathrm{atv}}$ , goals) is a tuple contain  \n133 ing the set of alternatives $A$ , the experimental conditions $C$ , the set of allowed-to-vary-factors $I_{\\mathrm{atv}}$ ,   \n134 and the goals of the study.   \n135 Example 3.1 (Continued). The research question of the \u201ccheckmate-in-one\u201d study is as follows.   \n136 The scope is $\\left(A=\\left\\{\\mathrm{LLM}_{a}\\right\\}_{a=1,2,3},C=\\left\\{(\\mathrm{pos}_{l},n)\\right\\}_{l,n},I_{\\mathrm{atv}}=\\left\\{{^{\\ast}\\mathrm{position}^{\\ast}}\\right\\}\\right)$ . The goal is \u201cDoes   \n137 $\\mathrm{LLM_{1}}$ rank consistently against the other LLMs?\u201d   \n138 A crucial element of our formalization is the distinction between ideal and empirical studies. An   \n139 ideal study exhausts its research question; however, its result is not observable. An empirical study is   \n140 an observable sample of an ideal study. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "141 3.2.1 Ideal studies ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "142 The ideal study on a research question $\\mathcal{Q}=(A,C,I_{\\mathrm{atv}},\\mathrm{goals})$ is the experimental study consist  \n143 ing of an experiment for each valid experimental condition $\\textbf{c}\\in{\\cal C}$ . We say that such a study   \n144 exhausts $\\mathcal{Q}$ . Hence, there exists exactly one ideal study on $\\mathcal{Q}$ . The result of an ideal study is   \n145 the probability distribution of the results of its experiments. Recall that the experiment function   \n146 $E:\\,(C,\\mathcal{F},\\mu)\\,\\rightarrow\\,(\\mathcal{R}_{n_{a}},\\mathcal{P}\\,(\\mathcal{R}_{n_{a}}))$ is measurable.   \n147 Definition 3.3 (Result of an ideal study). The result of an ideal study with research question   \n148 $\\mathcal{Q}=(A,C,I_{\\mathrm{atv}}$ , goals) is ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{c}{S\\left(\\mathcal{Q}\\right)=\\mathbb{P}:\\mathcal{R}_{n_{a}}\\rightarrow\\left[0,1\\right]}\\\\ {r\\mapsto\\mathbb{P}\\left(r\\right):=\\mu\\left(E^{-1}(r)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "149 where $E^{-1}(r)=\\{\\mathbf{c}:E(\\mathbf{c})=r\\}\\subseteq C$ is the preimage of $r$ through $E$ . ", "page_idx": 3}, {"type": "text", "text": "150 In general, multiple experiments of a study may yield identical results. Definition 3.3 supports this by   \n151 assigning a higher probability mass to results that occur more often. ", "page_idx": 3}, {"type": "text", "text": "152 3.2.2 Empirical studies ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "153 Consider again a research question $\\mathcal{Q}=(A,C,I_{\\mathrm{atv}}$ , goals). In practice, as $C$ might be infinite or too   \n154 large, one can only run experiments on a sample of valid experimental conditions $\\{\\mathbf{c}_{j}\\}_{j=1}^{N}\\overset{\\mathrm{iid}}{\\sim}(C,\\mu)$ .   \n155 The study performed on $\\left\\{{\\bf c}_{j}\\right\\}_{j=1}^{N}$ is an empirical study on $\\mathcal{Q}$ , of size $N$ . As for ideal studies, the   \n156 result of an empirical study is the probability distribution of the results of its experiments. ", "page_idx": 3}, {"type": "text", "text": "157 Definition 3.4 (Result of an empirical study). The result of an empirical study on $\\mathcal{Q}$ is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{S}_{N}\\left(\\mathcal{Q}\\right):\\mathcal{R}_{n_{a}}\\rightarrow\\left[0,1\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad r\\mapsto\\#\\left\\{j\\in\\{\\mathbf{c}_{j}\\}_{j=1}^{N}:E\\left(A,\\mathbf{c}_{j}\\right)=r\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "158 Where $\\mathcal{Q}_{\\mathrm{.}}$ , $\\left\\{{\\bf c}_{j}\\right\\}_{j=1}^{N}$ is a research question and a set of valid experimental conditions as above. ", "page_idx": 3}, {"type": "text", "text": "159 The result of an empirical study can be thought of as the empirical distribution of a sample following   \n160 the distribution of the result of the corresponding ideal study. With a slight abuse of notation,   \n161 indicating both the sample and its empirical distribution as $\\hat{S}_{N}\\,(\\mathcal{Q})$ , we write ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{S}_{N}\\left(\\mathcal{Q}\\right)\\stackrel{\\mathrm{iid}}{\\sim}S\\left(\\mathcal{Q}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "162 4 Generalizability of experimental studies ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "163 The currently accepted definition of generalizability is the property of two independent studies with   \n164 the same research question to yield similar results [46, 48]. Although intuitive, this notion is not   \n165 directly applicable as it does not provide a way to measure the generalizability of a study. We now   \n166 introduce a quantifiable notion of generalizability of experimental studies, as the probability that any   \n167 two empirical studies approximating the same ideal study yield similar results.   \n168 Definition 4.1 (Generalizability). Let $\\mathcal{Q}=(A,C,I_{\\mathrm{{atv}}},\\kappa)$ be the research question of an ideal study,   \n169 let $\\mathbb{P}=S(\\mathcal{Q})$ be the result of that study, and let $d$ be some distance between probability distributions.   \n170 The generalizability of the ideal study on $\\mathcal{Q}$ is ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\mathrm{Gen}}\\left(\\mathcal{Q};\\varepsilon,n\\right):=\\mathbb{P}^{n}\\otimes\\mathbb{P}^{n}\\left(\\left(X_{j},Y_{j}\\right)_{j=1}^{n}:d(X,Y)\\leq\\varepsilon\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "171 where $\\varepsilon\\in\\mathbb{R}^{+}$ is a similarity threshold. ", "page_idx": 4}, {"type": "text", "text": "172 As the result of an ideal study is usually unobservable (cf. Section 3.2), we do not know the true   \n173 distribution $\\mathbb{P}$ . However, we can observe the result of an empirical study, $\\hat{\\mathbb{P}}_{N}\\,=\\,\\hat{S}_{N}\\left(\\boldsymbol{\\mathcal{Q}}\\right)$ , which   \n174 approximates $\\mathbb{P}$ under the assumption that the experimental conditions are i.i.d. samples from $C$ . As   \n175 the sample size $N$ increases (the empirical study becomes larger), $\\hat{\\mathbb{P}}_{N}$ converges in distribution to $\\mathbb{P}$ .   \n176 Definition (1) requires a distance $d$ between probability distributions. In the next sections, we propose   \n177 to use a generalizability based on kernels and Maximum Mean Discrepancy (MMD) [27], as it allows   \n178 to compute generalizability w.r.t. different research questions. The underlying idea is that we can   \n179 capture the goal of a study with an appropriate kernel. We conclude this section with an algorithm to   \n180 estimate the number of experimental conditions required to obtain generalizable results. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "181 4.1 Similarity between rankings \u2014 kernels ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "182 Whether two experimental results (i.e., rankings) are similar or not ultimately depends on the goal of   \n183 the study. For instance, consider two rankings on $A=\\{a_{1},a_{2},a_{3}\\}$ , $\\mathbf{r}=(1,2,3)$ and $\\mathbf{r}^{\\prime}=(\\bar{1},3,2)$ ,   \n184 where $r_{i}$ is the tier of alternative $a_{i}$ . The conclusions drawn from $r$ and $r^{\\prime}$ are identical if one\u2019s goal is   \n185 to find the best alternative, but very different if one\u2019s goal is to obtain an ordering of the alternatives.   \n186 One can use kernels to quantify the similarity between experimental results. Kernels are suitable to   \n187 formalize the aspects of the result of a study one wants to generalize, i.e., the goals of the study. For   \n188 instance, one kernel is suitable to identify the best tier while another kernel focuses on the position of   \n189 a specific alternative. In the following, we describe three representative kernels that cover a wide   \n190 spectrum of possible goals.   \n191 Borda kernel. The Borda kernel is suitable for goals in the form \u201cIs the alternative $a^{*}$ consistently   \n192 ranked the same?\u201d. It uses the Borda count: the number of alternatives (weakly) dominated by a given   \n193 one [9]. For a pair of rankings, we compute the Borda counts of $a^{*}$ , and then take their difference. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\kappa_{b}^{a^{*},\\nu}\\left(r_{1},r_{2}\\right)=e^{-\\nu\\left|b_{1}-b_{2}\\right|},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "194 where $b_{l}=\\{a\\in A:r_{l}(a)\\geq r_{l}(a^{*})\\}$ is the number of alternatives dominated by $a^{*}$ in $r_{l}$ and $\\nu\\in\\mathbb{R}$   \n195 is the kernel bandwidth. The Borda kernel takes values in $\\left[e^{(-\\nu n_{a})},1\\right]$ . If $\\nu$ is too large compared to   \n196 $1\\big/|b_{1}-b_{2}|$ , the kernel is oversensitive and will penalize every deviation too much. On the contrary, if $\\nu$   \n197 is too small, the kernel is undersensitive and will not penalize deviations unless they are very large.   \n198 As $\\left|b_{1}-b_{2}\\right|\\in\\left[0,n_{a}\\right]$ , we recommend $\\nu=1/n_{a}$ .   \n199 Jaccard kernel. The Jaccard kernel is suitable for goals in the form \u201cAre the best alternatives   \n200 consistently the same ones?\u201d. As it measures the similarity between sets [25, 10], we use it to compare ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "201 the top- $k$ tiers of two rankings. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\kappa_{j}^{k}\\left(r_{1},r_{2}\\right)=\\frac{\\left|r_{1}^{-1}([k])\\cap r_{2}^{-1}([k])\\right|}{\\left|r_{1}^{-1}([k])\\cup r_{2}^{-1}([k])\\right|},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "202 where $r^{-1}([k])=\\{a\\in A:r_{1}(a)\\leq k\\}$ is the set of alternatives whose rank is better than or equal to   \n203 $k$ . The Jaccard kernel takes values in $[0,1]$ .   \n204 Mallows kernel. The Mallows kernel is suitable for goals in the form \u201cAre the alternatives ranked   \n205 consistently?\u201d. It measures the overall similarity between rankings [35, 40, 39]. We adapt the original   \n206 definition in [39] for ties, ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\kappa_{m}^{\\nu}(r_{1},r_{2})=e^{-\\nu n_{d}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "207 where $\\begin{array}{r}{n_{d}=\\sum_{a_{1},a_{2}\\in A}\\left|\\mathrm{sign}\\left(r_{1}(a_{1})-r_{1}(a_{2})\\right)-\\mathrm{sign}\\left(r_{2}(a_{1})-r_{2}(a_{2})\\right)\\right|}\\end{array}$ is the number of discor  \n208 dant pairs and $\\nu\\in\\mathbb{R}$ is the kernel bandwidth. If a pair is tied in one ranking but not in the other, one   \n209 counts it as half a discordant pair. The Mallows kernel takes values in $\\bar{\\left[\\exp\\left(-2\\nu{\\binom{n_{a}}{2}}\\right)\\right.},1\\right].$ . If $\\nu$ is   \n210 too large compared to $^{1}\\!/\\!n_{d}$ , the kernel is oversensitive and it will penalize every deviation too much.   \n211 On the contrary, if $\\nu$ is too small, the kernel is undersensitive and will not penalize deviations unless   \n212 they are very large. As $n_{d}\\in\\left[0,\\bar{\\left(\\begin{array}{l}{n_{a}}\\\\ {2}\\end{array}\\right)}\\right]$ , we recommend $\\nu=1/{\\left({n_{a}}\\right)}$ . ", "page_idx": 5}, {"type": "text", "text": "213 4.2 Distance between distributions \u2014 Maximum Mean Discrepancy ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "214 Having sorted out how to measure the similarity between the results of experiments, we now   \n215 discuss how to measure the distance between the results of studies. We chose the Maximum Mean   \n216 Discrepancy (MMD) [27], for the following reasons. First, MMD is compatible with the kernels   \n217 described in Section 4.1, i.e., it takes into consideration the goal of the studies. Second, it handles   \n218 sparse distributions well; this is needed as empirical studies are typically small compared to the   \n219 number of all possible rankings, which grows exponentially in the number of alternatives. 2 Finally,   \n220 it comes with bounds and theoretical guarantees, which we will use in Section 4.3.   \n221 Definition 4.2 (MMD (empirical distributions)). Let $X$ be a set with a kernel $\\kappa$ , and let $\\mathbb{Q}_{1}$ and $\\mathbb{Q}_{2}$   \n222 be two probability distributions on ${\\mathcal{R}}_{n_{a}}$ . Let $\\mathbf{x}=\\left(x_{i}\\right)_{i=1}^{n},\\mathbf{y}=\\left(y_{i}\\right)_{i=1}^{m}$ be two i.i.d. samples from   \n223 $\\mathbb{Q}_{1}$ and $\\mathbb{Q}_{2}$ respectively. Then, ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{MMD}\\left(\\mathbf{x},\\mathbf{y}\\right)^{2}:=\\frac{1}{n^{2}}\\sum_{i,j=1}^{n}\\kappa(x_{i},x_{j})+\\frac{1}{m^{2}}\\sum_{i,j=1}^{m}\\kappa(y_{i},y_{j})-\\frac{2}{m n}\\sum_{i=1\\ldots n\\atop j=1\\ldots m}\\kappa(x_{i},y_{j}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "224 Proposition 4.1. MMD takes values in $\\left[0,\\sqrt{2\\cdot\\left(\\kappa_{s u p}-\\kappa_{i n f}\\right)}\\right]$ , where $\\kappa_{s u p}=\\operatorname*{sup}_{x,y\\in X}\\kappa(x,y)$ and   \n225 $\\kappa_{i n f}=\\operatorname{inf}_{x,y\\in X}\\kappa(x,y)$ . ", "page_idx": 5}, {"type": "text", "text": "226 4.3 How many experiments ensure generalizability? ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "227 When designing a study, an experimenter has to decide how many experiments to run in order to   \n228 obtain generalizable results. In other words, they need to choose a (minimum) sample size $n^{*}$ that   \n229 achieves the desired generalizability $\\alpha^{*}$ and the desired similarity $\\varepsilon^{*}$ . ", "page_idx": 5}, {"type": "equation", "text": "$$\nn^{*}=\\operatorname*{min}\\left\\{n\\in\\mathbb{N}_{0}:\\operatorname{Gen}\\left(\\mathbb{P};\\varepsilon^{*},n\\right)\\geq\\alpha^{*}\\right\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "230 To estimate $n^{*}$ we make use of a linear dependency between the logarithms of the sample size $n$ and   \n231 the logarithm of the $\\alpha^{*}$ -quantile of MMD $\\varepsilon_{n}^{\\alpha^{*}}$ that we have observed in our experiments. ", "page_idx": 5}, {"type": "text", "text": "232 Proposition 4.2. $\\forall\\alpha^{*}$ , there exist $\\beta_{0}\\geq0,\\beta_{1}\\leq0$ s.t. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\log(n)\\approx\\beta_{1}\\log\\left({\\varepsilon_{n}^{\\alpha}}^{*}\\right)+\\beta_{0}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "233 Appendix A.3.2 provides a proof for a simplified case. Proposition 4.2 suggests that one can use a   \n234 small set of $N$ preliminary experiments to estimate $n^{*}$ . One can then iteratively improve that estimate   \n235 with the results of additional experiments.   \n236 Our algorithm, shown in detail in Appendix A.3.3, requires specifying the desired generalizability,   \n237 $\\alpha^{*}$ , and the similarity threshold between the studies results, $\\varepsilon^{*}$ . Then, it performs the following steps:   \n238 1. it estimates the $\\alpha^{*}$ -quantile of MMD for all $n$ less than some budget $n_{\\mathrm{max}}$ . If there exists an   \n239 $n$ less than $n_{\\mathrm{max}}$ that satisfies the condition in (2), we return it as $n^{*}$ ;   \n240 2. it then fits the linear model in (3), computing the coefficients $\\beta_{0}$ and $\\beta_{1}$ ;   \n241 3. finally, it outputs $n^{*}=\\exp\\left(\\beta_{1}\\log\\left(\\varepsilon_{n}^{\\alpha^{*}}\\right)+\\beta_{0}\\right)$ , which satisfies the condition in (2) thanks   \n242 to Proposition 4.2.   \n243 In practice, choosing $\\varepsilon^{*}$ is hardly interpretable as it is a threshold on MMD. To solve this, we propose   \n244 choosing $\\varepsilon^{*}$ as a function of another parameter $\\delta^{*}$ , such that ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "image", "img_path": "aYJ2T5TXoX/tmp/b817a96d207f3cb24c58b14430fa7bb5a6c1182682ec1ca912771a99b26ef1e3.jpg", "img_caption": ["Figure 2: Predicted $n^{*}$ for categorical encoders. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\varepsilon^{*}(\\delta^{*})=\\sqrt{2(\\kappa_{\\mathrm{sup}}-f_{\\kappa}(\\delta^{*}))}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "245 Here, $\\delta^{*}$ represents the distance between two rankings as computed by the kernel and $f_{\\kappa}$ is the   \n246 function linking the distance to the kernel value. For instance, for the Jaccard kernel, $\\delta^{*}$ is simply   \n247 the Jaccard coefficient between the top- ${\\cdot k}$ tiers of two rankings, $f_{\\kappa}(\\delta^{*})\\,=\\,1\\,-\\,\\delta^{*}$ , and $\\varepsilon^{*}(\\delta^{*})\\bar{=}$   \n248 $\\sqrt{2(1-\\left(1-\\delta^{*}\\right))}$ . For the Mallows kernel (with our recommendation for $\\nu$ ), $\\delta^{*}$ is the fraction of   \n249 discordant pairs, $f_{\\kappa}(x)\\,=\\,e^{-x}$ , and $\\varepsilon^{*}(\\delta^{*})\\,=\\,\\sqrt{2(1-e^{-\\delta^{*}})}$ . As a concrete example, achieving   \n250 $\\left(\\alpha^{*}=0.99,\\delta^{*}=0.05\\right)$ -generalizable results for the Jaccard kernel means that, with probability 0.99,   \n251 the average Jaccard coefficient between two rankings drawn from the results is 0.95. ", "page_idx": 6}, {"type": "text", "text": "252 5 Case studies ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "253 5.1 Case Study 1: A benchmark of categorical encoders ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "254 We now evaluate the generalizability of a recent study [41] that analyzes the performance of encoders   \n255 for categorical data. The performance of an encoder is approximated by the quality of a model trained   \n256 on the encoded data. The design factors are the model, the tuning strategy for the pipeline, and the   \n257 quality metric for the model, while the only allowed-to-vary factor is the dataset. We impute missing   \n258 values in the results of the study by assigning the worst rank. We evaluate how well the results of the   \n259 study generalize w.r.t. three goals:   \n260 $\\left(g_{1}\\right)$ Find out if One-Hot encoder (a popular encoder) ranks consistently amongst its competitors,   \n261 using the Borda kernel with $\\nu=1/n_{a}$ .   \n262 $\\left(g_{2}\\right)$ Investigate if some encoders outperform all the others using the Jaccard kernel with $k=1$ .   \n263 $\\left(g_{3}\\right)$ Evaluate whether the encoders are typically ranked in a similar order, using the Mallows   \n264 kernel with $\\nu=1/\\!\\left({n_{a}\\atop2}\\right)$ .   \n265 Figure 2 shows the predicted $n^{*}$ for different choices of $\\alpha^{*}$ and $\\delta^{*}$ , the other one fixed at 0.95   \n266 and 0.05 respectively. The variance in the boxes comes from variance in the design factors. For   \n267 example, the results for the design factors \u201cdecision tree, full tuning, accuracy\u201d have a different   \n268 $(\\alpha^{*},\\delta^{*})$ -generalizability than the results for \u201cSVM, no tuning, accuracy\u201d. We observe on the left   \n269 that \u2014 as expected \u2014 obtaining generalizable results requires more experiments as the desired   \n270 generalizability $\\alpha^{*}$ increases. We can also see that the variance of the boxes increases with $\\alpha^{*}$ . This   \n271 means that the choice of the design factors has a larger influence on the achieved generalizability.   \n272 We observe the same when decreasing $\\delta^{*}$ , as it corresponds to a stricter similarity condition on the   \n273 rankings. In the rather extreme cases of $\\alpha^{*}=0.7$ or $\\delta^{*}=0.3$ , even less than 10 datasets are enough   \n274 to achieve $(\\alpha^{*},\\delta^{*})$ -generalizability.   \n275 Consider now goal $g_{2}$ for two different choices of design factors: (A): \u201cdecision tree, full tuning,   \n276 accuracy\u201d and (B): \u201cSVM, full tuning, balanced accuracy\u201d. Furthermore, let $\\left(\\alpha^{*},\\delta^{*}\\right)=\\left(0.95,0.05\\right)$ :   \n277 we estimate $n^{*}=28$ for (A) and ${n}^{*}=34$ for (B), corresponding to the bottom and top whiskers of   \n278 the corresponding box in Figure 2. As both (A) and (B) were evaluated using $n=30$ experiments,   \n279 we conclude that the results of (A) are (barely) (0.95, 0.05)-generalizable, while those of (B) are not.   \n280 Hence, one should run more experiments with fixed factors (B) to make the study generalizable. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "aYJ2T5TXoX/tmp/daca13048a5e9ee3e82daf869987f63dd6e6eb6ef27b11db889e0f8077d55e08.jpg", "img_caption": ["Figure 3: Predicted $n^{*}$ for LLMs. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "281 5.2 Case study 2: BIG-bench \u2014 A benchmark of Large Language Models ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "282 We now evaluate the generalizability of BIG-bench [55], a collaborative benchmark of Large Lan  \n283 guage Models (LLMs). The benchmark compares LLMs on different tasks, such as the checkmate  \n284 in-one task (cf. Example 3.1), and for different numbers of shots. Task and number of shots are the   \n285 design factors. Every task has a number of subtasks, which is the allowed-to-vary factor. We stick to   \n286 the preferred scoring for each subtask. As the results have too many missing values to impute them,   \n287 we only consider the experimental conditions where at least $80\\%$ of the LLMs had results, and to the   \n288 LLMs whose results cover at least $80\\%$ of the conditions. ", "page_idx": 7}, {"type": "text", "text": "289 Similar to before, we define the three goals as follows: ", "page_idx": 7}, {"type": "text", "text": "290 $\\left(g_{1}\\right)$ Find out if GPT3 (to date, one of the most popular LLMs) ranks consistently amongst its   \n291 competitors, using the Borda kernel with $\\nu=1/n_{a}$ .   \n292 $\\left(g_{2}\\right)$ Investigate if some encoders outperform all the others using the Jaccard kernel with $k=1$ .   \n293 $\\left(g_{3}\\right)$ Evaluate whether the LLMs are typically ranked in a similar order, using the Mallows kernel   \n294 with $\\nu=1/\\!\\left({n_{a}\\atop2}\\right)$ .   \n295 Figure 3 shows the predicted $n^{*}$ for different choices of $\\alpha^{*}$ and $\\delta^{*}$ , the other one fixed at 0.95 and   \n296 0.05 respectively. Again, the variance in the boxes comes from variance in the design factors, i.e., the   \n297 task and the number of shots. As before, increasing $\\alpha^{*}$ or decreasing $\\delta^{*}$ leads to higher $n^{*}$ . Unlike in   \n298 the previous section, $n^{*}$ for $g_{2}$ greatly depends on the combination of fixed factors, as we now detail.   \n299 Consider now goal $g_{2}$ for two different choices of design factors: (A): \u201cconlang_translation, 0 shots\u201d,   \n300 and (B): \u201carithmetic, 2 shots\u201d. Furthermore, let $(\\alpha^{*},\\delta^{*})\\,=\\,(0.95,0.05)$ . For this choice of of   \n301 parameters, we estimate ${n}^{*}=44$ for (A), corresponding to the top whisker of the corresponding box   \n302 in Figure 2. As the study evaluates (A) on 10 subtasks, it is therefore not (0.95, 0.05)-generalizable.   \n303 In fact, we estimate that this would require 34 more subtasks. For (B), on the other hand, we estimate   \n304 ${n^{*}}=1$ : the best 2-shot LLM for the observed subtasks is always PALM 535B. Hence, the result of a   \n305 single experiment is enough to achieve (0.95, 0.05)-generalizability.   \n306 Note that, although we correctly estimated ${n^{*}}=1$ for (B), this estimate relies on 10 preliminary   \n307 experiments. In other words, our algorithm was able to quantify in hindsight that a single experiment   \n308 would have been enough to obtain generalizable results. Of course, however, one cannot trust an   \n309 estimate of $n^{*}$ based on only one experiment. The next section thus investigates how the number of   \n310 preliminary experiments influences the estimate of $n^{*}$ . ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "aYJ2T5TXoX/tmp/650e74749ccfa476558d5452979f5a64299b25211c4e1376ccef25c2290a541a.jpg", "img_caption": ["Figure 4: Relative error in the estimate of $n^{*}$ against $n_{50}^{*}$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "311 5.3 How many preliminary experiments? ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "312 This section evaluates the influence of the number of preliminary experiments $N$ on $n^{*}$ . For each   \n313 study, we consider the design factor combinations for which we have at least 50 experiments. This   \n314 results in 23 out of 48 combinations for the categorical encoders and 9 out of 24 combinations for   \n315 the LLMs. For each of those combinations, we consider the estimate $n_{50}^{*}$ made at $N=50$ as the   \n316 ground truth and observe how the estimates of $n^{*}$ for $N<50$ differ. Figure 4 shows the relative error   \n317 $\\bar{|}n_{N}^{*}\\!-\\!n_{50}^{*}|\\big/n_{50}^{*}$ , for different goals: the relative errors behave very differently. For goal $g_{3}$ (Mallows   \n318 kernel), even $n_{10}^{*}$ is close to $n_{50}^{*}$ for a majority of the design factor combinations. On the contrary,   \n319 one needs 20 to 30 preliminary experiments for goal $g_{1}$ (Borda kernel). This means that knowing the   \n320 goals of a study when performing preliminary experiments can help understand how trustworthy the   \n321 estimate of $n^{*}$ is. ", "page_idx": 8}, {"type": "text", "text": "322 6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "323 Limitations & future work. First, we dealt with experimental results as rankings. Other forms of   \n324 results, e.g., the absolute performance of alternatives according to some quality measure, will require   \n325 the development of appropriate kernels. Second, our approach uses kernels to compute the similarity   \n326 of experimental results and MMD the distance between the results of studies. There are, however.   \n327 other possible choices. Third, we processed missing evaluations by either dropping them or imputing   \n328 them. One could analyze different solutions, for instance by adapting the kernels to missing values.   \n329 Fourth, we estimate the distribution of the MMD by sampling multiple times from the results. A   \n330 non-asymptotic theory of MMD, at least for some kernels, might yield more insights in improving   \n331 this procedure. Fifth, we plan to investigate the possibility of actively selecting experiments to obtain   \n332 good estimates of the required size $n^{*}$ with less preliminary experiments. Sixth and related to the   \n333 previous one, we intend to obtain some guarantees on the convergence of $n^{*}$ to the true value.   \n334 Conclusions. An experimental study is generalizable if, with high probability, its findings will hold   \n335 under different experimental conditions, e.g., on unseen datasets. Non-generalizable studies might be   \n336 of limited use or even misleading. This study is, to our knowledge, the first to develop a quantifiable   \n337 notion for the generalizability of experimental studies. To achieve this, we formalize experiments,   \n338 experimental studies and their results \u2014 rankings and distributions over rankings. Our approach   \n339 allows us to estimate the number of experiments needed to achieve a desired level of generalizability   \n340 in new experimental studies. We demonstrate its utility showing generalizable and non-generalizable   \n341 results in two recent experimental studies. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "342 Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "343 ", "page_idx": 9}, {"type": "text", "text": "344 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "345 [1] Kwangjun Ahn et al. \u201cReproducibility in Optimization: Theoretical Framework and Limits\u201d.   \n346 In: NeurIPS. 2022.   \n347 [2] Scott Alexander. \u201cA very unlikely chess game, 2020\u201d. In: URL https://slatestarcodex.   \n348 com/2020/01/06/a-very-unlikely-chessgame/.(cited on pp. 29 and 30) ().   \n349 [3] Maxime Alvarez et al. \u201cA Revealing Large-Scale Evaluation of Unsupervised Anomaly   \n350 Detection Algorithms\u201d. In: CoRR abs/2204.09825 (2022).   \n351 [4] Prithviraj Ammanabrolu et al. \u201cBringing stories alive: Generating interactive fiction worlds\u201d.   \n352 In: Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital   \n353 Entertainment. Vol. 16. 1. 2020, pp. 3\u20139.   \n354 [5] Prithviraj Ammanabrolu et al. \u201cToward automated quest generation in text-adventure games\u201d.   \n355 In: arXiv preprint arXiv:1909.06283 (2019).   \n356 [6] Monya Baker. \u201c1,500 scientists lift the lid on reproducibility\u201d. In: Nature 533.7604 (2016).   \n357 [7] Thomas Bartz-Beielstein et al. \u201cBenchmarking in Optimization: Best Practice and Open   \n358 Issues\u201d. In: CoRR abs/2007.03488 (2020).   \n359 [8] Alessio Benavoli et al. \u201cTime for a Change: a Tutorial for Comparing Multiple Classifiers   \n360 Through Bayesian Analysis\u201d. In: J. Mach. Learn. Res. 18 (2017), 77:1\u201377:36.   \n361 [9] JC de Borda. \u201cM\u2019emoire sur les\u2019 elections au scrutin\u201d. In: Histoire de l\u2019Acad\u2019emie Royale des   \n362 Sciences (1781).   \n363 [10] Mathieu Bouchard, Anne-Laure Jousselme, and Pierre-Emmanuel Dor\u00e9. \u201cA proof for the   \n364 positive definiteness of the Jaccard index matrix\u201d. In: International Journal of Approximate   \n365 Reasoning 54.5 (2013), pp. 615\u2013626.   \n366 [11] Anne-Laure Boulesteix, Rory Wilson, and Alexander Hapfelmeier. \u201cTowards evidence-based   \n367 computational statistics: lessons from clinical research on the role and design of real-data   \n368 benchmark studies\u201d. In: BMC Medical Research Methodology 17 (2017), pp. 1\u201312.   \n369 [12] Anne-Laure Boulesteix et al. \u201cA statistical framework for hypothesis testing in real data   \n370 comparison studies\u201d. In: The American Statistician 69.3 (2015), pp. 201\u2013212.   \n371 [13] Xavier Bouthillier et al. \u201cAccounting for Variance in Machine Learning Benchmarks\u201d. In:   \n372 MLSys. mlsys.org, 2021.   \n373 [14] Robert L Brennan. \u201cGeneralizability theory\u201d. In: Educational Measurement: Issues and Prac  \n374 tice 11.4 (1992), pp. 27\u201334.   \n375 [15] Mark Bun et al. \u201cStability Is Stable: Connections between Replicability, Privacy, and Adaptive   \n376 Generalization\u201d. In: STOC. ACM, 2023, pp. 520\u2013527.   \n377 [16] Zachary Chase, Shay Moran, and Amir Yehudayoff. \u201cStability and Replicability in Learning\u201d.   \n378 In: FOCS. IEEE, 2023, pp. 2430\u20132439.   \n379 [17] Giorgio Corani et al. \u201cStatistical comparison of classifiers through Bayesian hierarchical   \n380 modelling\u201d. In: Mach. Learn. 106.11 (2017), pp. 1817\u20131837.   \n381 [18] Sahith Dambekodi et al. \u201cPlaying text-based games with common sense\u201d. In: arXiv preprint   \n382 arXiv:2012.02757 (2020).   \n383 [19] Mostafa Dehghani et al. \u201cThe Benchmark Lottery\u201d. In: CoRR abs/2107.07002 (2021).   \n384 [20] Janez Demsar. \u201cStatistical Comparisons of Classifiers over Multiple Data Sets\u201d. In: J. Mach.   \n385 Learn. Res. 7 (2006), pp. 1\u201330.   \n386 [21] Peter Dixon et al. \u201cList and Certificate Complexities in Replicable Learning\u201d. In: NeurIPS.   \n387 2023.   \n388 [22] Eric Eaton et al. \u201cReplicable Reinforcement Learning\u201d. In: NeurIPS. 2023.   \n389 [23] Hossein Esfandiari et al. \u201cReplicable Bandits\u201d. In: ICLR. OpenReview.net, 2023.   \n390 [24] Hossein Esfandiari et al. \u201cReplicable Clustering\u201d. In: NeurIPS. 2023.   \n391 [25] Thomas G\u00e4rtner, Quoc Viet Le, and Alex J Smola. \u201cA short tour of kernel methods for graphs\u201d.   \n392 In: Under Preparation (2006).   \n393 [26] Badih Ghazi et al. \u201cOn User-Level Private Convex Optimization\u201d. In: ICML. Vol. 202. Pro  \n394 ceedings of Machine Learning Research. PMLR, 2023, pp. 11283\u201311299.   \n395 [27] Arthur Gretton et al. \u201cA Kernel Method for the Two-Sample-Problem\u201d. In: NIPS. MIT Press,   \n396 2006, pp. 513\u2013520.   \n397 [28] Arthur Gretton et al. \u201cA Kernel Two-Sample Test\u201d. In: J. Mach. Learn. Res. 13 (2012), pp. 723\u2013   \n398 773.   \n399 [29] Odd Erik Gundersen, Kevin L. Coakley, and Christine R. Kirkpatrick. \u201cSources of Irrepro  \n400 ducibility in Machine Learning: A Review\u201d. In: CoRR abs/2204.07610 (2022).   \n401 [30] Odd Erik Gundersen et al. \u201cOn Reporting Robust and Trustworthy Conclusions from Model   \n402 Comparison Studies Involving Neural Networks and Randomness\u201d. In: ACM-REP. ACM,   \n403 2023, pp. 37\u201361.   \n404 [31] Torsten Hothorn et al. \u201cThe design and analysis of benchmark experiments\u201d. In: Journal of   \n405 Computational and Graphical Statistics 14.3 (2005), pp. 675\u2013699.   \n406 [32] Karl Huppler. \u201cThe Art of Building a Good Benchmark\u201d. In: TPCTC. Vol. 5895. Lecture Notes   \n407 in Computer Science. Springer, 2009, pp. 18\u201330.   \n408 [33] Russell Impagliazzo et al. \u201cReproducibility in learning\u201d. In: STOC. ACM, 2022, pp. 818\u2013831.   \n409 [34] Iman Jaljuli et al. \u201cQuantifying replicability and consistency in systematic reviews\u201d. In:   \n410 Statistics in Biopharmaceutical Research 15.2 (2023), pp. 372\u2013385.   \n411 [35] Yunlong Jiao and Jean-Philippe Vert. \u201cThe Kendall and Mallows Kernels for Permutations\u201d.   \n412 In: IEEE Trans. Pattern Anal. Mach. Intell. 40.7 (2018), pp. 1755\u20131769.   \n413 [36] Alkis Kalavasis et al. \u201cStatistical Indistinguishability of Learning Algorithms\u201d. In: ICML.   \n414 Vol. 202. Proceedings of Machine Learning Research. PMLR, 2023, pp. 15586\u201315622.   \n415 [37] Amin Karbasi et al. \u201cReplicability in Reinforcement Learning\u201d. In: NeurIPS. 2023.   \n416 [38] Fred Lu, Edward Raff, and James Holt. \u201cA Coreset Learning Reality Check\u201d. In: AAAI. AAAI   \n417 Press, 2023, pp. 8940\u20138948.   \n418 [39] Colin L Mallows. \u201cNon-null ranking models. I\u201d. In: Biometrika 44.1/2 (1957), pp. 114\u2013130.   \n419 [40] Horia Mania et al. \u201cOn kernel methods for covariates that are rankings\u201d. In: (2018).   \n420 [41] Federico Matteucci, Vadim Arzamasov, and Klemens B\u00f6hm. \u201cA benchmark of categorical   \n421 encoders for binary classification\u201d. In: NeurIPS. 2023.   \n422 [42] Duncan C. McElfresh et al. \u201cOn the Generalizability and Predictability of Recommender   \n423 Systems\u201d. In: NeurIPS. 2022.   \n424 [43] Iven Van Mechelen et al. \u201cA white paper on good research practices in benchmarking: The   \n425 case of cluster analysis\u201d. In: WIREs Data. Mining. Knowl. Discov. 13.6 (2023).   \n426 [44] Douglas C Montgomery. Design and analysis of experiments. John wiley & sons, 2017.   \n427 [45] Shay Moran, Hilla Schefler, and Jonathan Shafer. \u201cThe Bayesian Stability Zoo\u201d. In: NeurIPS.   \n428 2023.   \n429 [46] Engineering National Academies of Sciences, Medicine, et al. Reproducibility and replicability   \n430 in science. National Academies Press, 2019.   \n431 [47] Roger D Peng. \u201cReproducible research in computational science\u201d. In: Science 334.6060 (2011),   \n432 pp. 1226\u20131227.   \n433 [48] Joelle Pineau et al. \u201cImproving Reproducibility in Machine Learning Research(A Report from   \n434 the NeurIPS 2019 Reproducibility Program)\u201d. In: J. Mach. Learn. Res. 22 (2021), 164:1\u2013   \n435 164:20.   \n436 [49] Zhen Qin et al. \u201cRD-Suite: A Benchmark for Ranking Distillation\u201d. In: NeurIPS. 2023.   \n437 [50] Edward Raff. \u201cDoes the Market of Citations Reward Reproducible Work?\u201d In: ACM-REP.   \n438 ACM, 2023, pp. 89\u201396.   \n439 [51] Edward Raff. \u201cResearch Reproducibility as a Survival Analysis\u201d. In: AAAI. AAAI Press, 2021,   \n440 pp. 469\u2013478.   \n441 [52] Isaac J Schoenberg. \u201cMetric spaces and positive definite functions\u201d. In: Transactions of the   \n442 American Mathematical Society 44.3 (1938), pp. 522\u2013536.   \n443 [53] Bernhard Sch\u00f6lkopf. \u201cThe kernel trick for distances\u201d. In: Advances in neural information   \n444 processing systems 13 (2000).   \n445 [54] J Laurie Snell and John G Kemeny. \u201cMathematical models in the social sciences\u201d. In: (No   \n446 Title) (1962).   \n447 [55] Aarohi Srivastava et al. \u201cBeyond the Imitation Game: Quantifying and extrapolating the   \n448 capabilities of language models\u201d. In: CoRR abs/2206.04615 (2022).   \n449 [56] Donglin Zhuang et al. \u201cRandomness in Neural Network Training: Characterizing the Impact   \n450 of Tooling\u201d. In: MLSys. mlsys.org, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "451 A Details for Section 4 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "452 A.1 Details for Section 4.1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "453 This section contains the proofs to show that the similarities introduced in Section 4.1 are kernels,   \n454 i.e., symmetric and positive definite functions. As symmetry is a clear property of all of them, we   \n455 only discuss their positive definiteness. Our proofs for the Borda and Mallows kernels follow that   \n456 in [35]: we define a distance $d$ on the set of rankings ${\\mathcal{R}}_{n_{a}}$ and show that $(\\mathcal{R}_{n_{a}},d)$ is isometric to an   \n457 $L_{2}$ space. This ensures that $d$ is a conditionally positive definite (c.p.d.) function and, thus, that $e^{-\\nu d}$   \n458 is positive definite [52, 53]. Our proof for the Jaccard kernel, instead, follows without much effort   \n459 from previous results. For ease of reading, we restate the definitions as well. ", "page_idx": 12}, {"type": "text", "text": "Definition A.1 (Borda kernel). ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\kappa_{b}^{a^{\\ast},\\nu}\\left(r_{1},r_{2}\\right)=e^{-\\nu\\left|d_{1}-d_{2}\\right|},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "460 where $d_{l}=\\{a\\in A:r_{l}(a)\\geq r_{l}(a^{*})\\}$ is the number of alternatives dominated by $a^{*}$ in $r_{\\mathit{l}}$ and $\\nu\\in\\mathbb{R}$ .   \n461 Proposition A.1. The Borda kernel as defined in (4) is a kernel. ", "page_idx": 12}, {"type": "text", "text": "462 Proof. Define a distance ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\small}&{{}d:\\mathcal{R}_{n_{a}}\\times\\mathcal{R}_{n_{a}}\\rightarrow\\mathbb{R}^{+}}\\\\ {\\quad}&{{}\\left(r_{1},r_{2}\\right)\\mapsto\\left|d_{1},d_{2}\\right|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "463 where $d_{l}\\,=\\,\\{a\\in A:r_{l}(a)\\geq r_{l}(a^{*})\\}$ is the number of alternatives dominated by $a^{*}$ in $r_{l}$ . Now,   \n464 $(\\mathcal{R}_{n_{a}},d)$ is isometric to $(\\mathbb{R},\\|\\cdot\\|_{2})$ via the map $r_{l}\\mapsto d_{l}$ . Hence, $d$ is c.p.d. and $\\kappa_{b}$ is a kernel. \u53e3 ", "page_idx": 12}, {"type": "text", "text": "Definition A.2 (Jaccard kernel). ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\kappa_{j}^{k}\\left(r_{1},r_{2}\\right)=\\frac{\\left|r_{1}^{-1}([k])\\cap r_{2}^{-1}([k])\\right|}{\\left|r_{1}^{-1}([k])\\cup r_{2}^{-1}([k])\\right|},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "465 where $r^{-1}([k])=\\{a\\in A:r_{1}(a)\\leq k\\}$ is the set of alternatives whose rank is better than or equal to   \n466 $k$ . ", "page_idx": 12}, {"type": "text", "text": "467 Proposition A.2. The Jaccard kernel as defined in (5) is a kernel. ", "page_idx": 12}, {"type": "text", "text": "468 Proof. It is already know that the Jaccard coefficients for sets is a kernel [25, 10]. As the Jaccard   \n469 kernel for rankings is equivalent to the Jaccard coefficient for the $k$ -best tiers of said rankings, the   \n470 former is also a kernel. \u53e3 ", "page_idx": 12}, {"type": "text", "text": "Definition A.3 (Mallows kernel). ", "text_level": 1, "page_idx": 12}, {"type": "equation", "text": "$$\n\\kappa_{m}^{\\nu}(r_{1},r_{2})=e^{-\\nu n_{d}},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "471 where $\\begin{array}{r}{n_{d}=\\sum_{a_{1},a_{2}\\in A}\\left|\\mathrm{sign}\\left(r_{1}(a_{1})-r_{1}(a_{2})\\right)-\\mathrm{sign}\\left(r_{2}(a_{1})-r_{2}(a_{2})\\right)\\right|}\\end{array}$ is the number of discor  \n472 dant pairs and $\\nu\\in\\mathbb{R}$ is the kernel bandwidth. ", "page_idx": 12}, {"type": "text", "text": "473 Proposition A.3. The Mallows kernel as defined in (6) is a kernel. ", "page_idx": 12}, {"type": "text", "text": "474 Proof. The number of discordant pairs $n_{d}$ is a distance on $\\mathcal{R}_{n_{a}}$ [54]. Consider now the mapping of a   \n475 ranking into its adjacency matrix, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Phi:\\mathcal{R}_{n_{a}}\\to\\{0,1\\}^{n_{a}\\times n_{a}}\\qquad\\qquad\\qquad\\quad}\\\\ {r\\mapsto(\\mathrm{sign}\\,(r(i)-r(j)))_{i,j=1}^{n_{a}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "476 Then, ", "page_idx": 12}, {"type": "equation", "text": "$$\nn_{d}=\\|\\Phi(r_{1})-\\Phi(r_{2})\\|_{1}=\\|\\Phi(r_{1})-\\Phi(r_{2})\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "477 where $\\left\\|\\cdot\\right\\|_{p}$ indicates the entry-wise matrix $p$ -norm and the equality holds because the entries of the   \n478 matrices are either 0 or 1. As a consequence, $(\\mathcal{R}_{n_{a}},n_{d})$ is isometric to $\\left(\\mathbb{R}^{n_{a}\\times n_{a}},\\lVert\\cdot\\rVert_{2}\\right)$ via $\\Phi$ . Hence,   \n479 $n_{d}$ is c.p.d. and $\\kappa_{m}$ is a kernel. \u53e3   \n481 Proposition 4.1. MMD takes values in $\\left[0,\\sqrt{2\\cdot\\left(\\kappa_{s u p}-\\kappa_{i n f}\\right)}\\right]$ , where $\\kappa_{s u p}=\\operatorname*{sup}_{x,y\\in X}\\kappa(x,y)$ and   \n482 $\\kappa_{i n f}=\\operatorname*{inf}_{x,y\\in X}\\kappa(x,y)$ . ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Proof. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle0\\leq\\mathrm{MMD}_{\\kappa}\\left(\\mathbf{x},\\mathbf{y}\\right)^{2}=\\frac{1}{n^{2}}\\sum_{i,j=1}^{n}\\kappa(x_{i},x_{j})+\\frac{1}{m^{2}}\\sum_{i,j=1}^{m}\\kappa(y_{i},y_{j})-\\frac{2}{m n}\\sum_{i=1\\ldots n}\\kappa(x_{i},y_{j})}\\\\ {\\displaystyle\\leq\\frac{1}{n^{2}}\\sum_{i,j=1}^{n}\\kappa_{\\mathrm{sup}}+\\frac{1}{m^{2}}\\sum_{i,j=1}^{n}\\kappa_{\\mathrm{sup}}-\\frac{2}{m n}\\sum_{i=1\\ldots n}\\kappa_{\\mathrm{inf}}}\\\\ {\\displaystyle=2(\\kappa_{\\mathrm{sup}}-\\kappa_{\\mathrm{inf}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "483 ", "page_idx": 13}, {"type": "text", "text": "484 A.3 Details for Section 4.3 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "485 A.3.1 Choice of $\\alpha^{*},\\varepsilon^{*}$ , and $\\delta^{*}$ ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "486 Consider a research question $\\mathcal{Q}\\,=\\,(A,C,I_{\\mathrm{atv}},\\kappa)$ and the corresponding ideal study with result   \n487 $\\mathbb{P}$ . The algorithm introduced in Section 4.3 aims at finding the minimum $n^{*}$ such that, given two   \n488 independent empirical studies on $\\mathcal{Q}$ , they achieve similar results. It has two hyperparameters, $\\alpha^{*}$ and   \n489 $\\varepsilon^{*}$ . $\\bar{\\alpha}^{*}\\in[0,1]$ is the generalizability that one wants to achieve from the study, i.e., the probability   \n490 that two independent realizations of the same ideal study will yield similar results. $\\varepsilon^{\\ast}\\in\\mathbb{R}^{+}$ is a   \n491 similarity threshold: the results of two empirical studies $\\mathbf{x},\\mathbf{y}\\overset{\\mathrm{iid}}{\\sim}\\mathbb{P}$ are similar if $\\mathbf{MMD}_{\\kappa}(\\mathbf{x},\\mathbf{y})\\le\\varepsilon^{*}$ .   \n492 However, as it is, $\\varepsilon^{*}$ is not interpretable. Instead, adapting the proof of Proposition 4.1, we can bound   \nMMD by imposing a condition on the kernel, as we\u2019ll now illustrate. The key remark is that we are   \n494 looking for a condition in the form ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{MMD}_{\\kappa}\\left(\\mathbf{x},\\mathbf{y}\\right)\\leq\\varepsilon^{*}=\\sqrt{2(\\kappa_{\\mathrm{sup}}-\\delta^{\\prime})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "495 where $\\delta^{\\prime}\\in[0,\\kappa_{\\mathrm{sup}}]$ replaces the third summatory in (7). In other terms, we can interpret $\\delta^{\\prime}$ as the   \n496 minimum acceptable value for the average of the kernel, $\\mathbb{E}_{\\mathbb{P}^{2}}\\left[\\kappa(x,y)\\right]$ . We now go a step further and   \n497 compute $\\delta^{\\prime}$ (a condition on the kernel) from $\\delta^{*}\\in[0,1]$ (a condition on the rankings). The relation   \n498 between $\\delta^{\\prime}$ and $\\delta^{*}$ changes with the kernel, and so does the interpretation of $\\delta^{*}$ . For the three kernels   \n499 we discuss in Section 4.1:   \n500   \n501   \n502   \n503 ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "\u2022 Mallows kernel with $\\nu=1/\\binom{n}{2}$ : $\\delta^{*}$ is the fraction of discordant pairs, $\\delta^{\\prime}=e^{-\\delta^{*}}$ . \u2022 Jaccard kernel: $\\delta^{*}$ is the intersection over union of the top $k$ tiers, $\\delta^{\\prime}=1-\\delta^{*}$ . \u2022 Borda kernel with $\\nu=1/n_{a}\\colon\\delta^{*}$ is the difference in relative position of $a^{*}$ in the rankings, normalized to the length of the rankings, \u03b4\u2032 = e\u2212\u03b4\u2217 ", "page_idx": 13}, {"type": "text", "text": "504 A.3.2 Proof of proposition 4.2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "505 Proposition 4.2. $\\forall\\alpha^{*}$ , there exist $\\beta_{0}\\geq0,\\beta_{1}\\leq0$ s.t. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\log(n)\\approx\\beta_{1}\\log\\left({\\varepsilon_{n}^{\\alpha}}^{*}\\right)+\\beta_{0}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "506 Proof. We provide a proof replacing MMD with the distribution-free bound defined in [28]. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\mathbb{P}^{n}\\otimes\\mathbb{P}^{n}\\left((X_{j},Y_{j})_{j=1}^{n}:\\mathrm{MMD}(X,Y)-\\left(\\frac{2\\kappa_{\\operatorname*{sup}}}{n}\\right)>\\varepsilon\\right)<\\exp\\left(-\\frac{n\\varepsilon^{2}}{4\\kappa_{\\operatorname*{sup}}}\\right)}\\\\ {\\displaystyle\\overset{(1)}{\\Longrightarrow}\\mathbb{P}^{n}\\otimes\\mathbb{P}^{n}\\left((X_{j},Y_{j})_{j=1}^{n}:\\mathrm{MMD}(X,Y)>\\varepsilon^{\\downarrow}\\right)<\\exp\\left(-\\frac{n\\left(\\varepsilon^{\\prime}-\\left(\\frac{2\\kappa_{\\operatorname*{sup}}}{n}\\right)\\right)^{2}}{4\\kappa_{\\operatorname*{sup}}}\\right)}\\\\ {\\displaystyle\\overset{(2)}{\\Longrightarrow}\\mathbb{P}^{n}\\otimes\\mathbb{P}^{n}\\left((X_{j},Y_{j})_{j=1}^{n}:\\mathrm{MMD}(X,Y)>n^{-\\frac{1}{2}}\\left(\\sqrt{-\\log\\left(1-\\alpha\\right)4\\kappa_{\\operatorname*{sup}}}\\right)+\\sqrt{2\\kappa_{\\operatorname*{sup}}}\\right)<1-\\alpha.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{(3)}{\\cos\\mathbb{P}^{n}}\\mathbb{P}^{n}\\otimes\\mathbb{P}^{n}\\left((X_{j},Y_{j})_{j=1}^{n}:\\mathbf{M}\\mathbf{M}\\mathbb{D}(X,Y)\\leq n^{-\\frac{1}{2}}\\left(\\sqrt{-\\log\\left(1-\\alpha\\right)4\\kappa_{\\mathrm{sup}}}\\right)+\\sqrt{2\\kappa_{\\mathrm{sup}}}\\right)\\geq\\alpha}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "507 where: ", "page_idx": 14}, {"type": "text", "text": "508 ", "page_idx": 14}, {"type": "text", "text": "509 ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\varepsilon^{\\prime}=\\varepsilon+\\sqrt{2\\kappa_{\\mathrm{sup}}/n}.}\\\\ &{1-\\alpha=\\exp{\\left(-\\frac{n\\left(\\varepsilon^{\\prime}-\\left(\\frac{2\\kappa_{\\mathrm{sup}}}{n}\\right)\\right)^{2}}{4\\kappa_{\\mathrm{sup}}}\\right)}\\mathrm{~and~}\\varepsilon^{*}=n^{-\\frac{1}{2}}\\left(\\sqrt{-\\log{(1-\\alpha)}\\,4\\kappa_{\\mathrm{sup}}}+\\sqrt{2\\kappa_{\\mathrm{sup}}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "510 ", "page_idx": 14}, {"type": "text", "text": "(3) Take the complementary event. ", "page_idx": 14}, {"type": "text", "text": "511 Now, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad q_{n}^{\\alpha}=n^{-\\frac{1}{2}}\\left(\\sqrt{-\\log\\left(1-\\alpha\\right)4\\kappa_{\\mathrm{sup}}}\\right)+\\sqrt{2\\kappa_{\\mathrm{sup}}}}\\\\ &{\\Rightarrow\\hfill n=(q_{n}^{\\alpha})^{-2}\\left(\\sqrt{-4\\kappa_{\\mathrm{sup}}\\log\\left(1-\\alpha\\right)}+\\sqrt{2\\kappa_{\\mathrm{sup}}}\\right)^{2}}\\\\ &{\\Rightarrow\\log(n)=-2\\log(q_{n}^{\\alpha})+2\\log\\left(\\sqrt{-4\\kappa_{\\mathrm{sup}}\\log\\left(1-\\alpha\\right)}+\\sqrt{2\\kappa_{\\mathrm{sup}}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "512 concluding the proof. ", "page_idx": 14}, {"type": "text", "text": "513 Remark. Altohugh theoretically sound, using the abovementioned bound instead of MMD leads to   \n514 excessively conservative estimates for $n^{*}$ , roughly one order of magnitude greater than the empirical   \n515 estimate. ", "page_idx": 14}, {"type": "table", "img_path": "aYJ2T5TXoX/tmp/7c2ae27e7177ec8a4d08f22d3905b81db7fbd4e4a2cf730c40d2ed8a89813072.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "517 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "518 1. Claims   \n519 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n520 paper\u2019s contributions and scope?   \n521 Answer: [Yes]   \n522 Justification: In order, our claims are: the formalization in Section 3; the definition gen  \n523 eralizability in Section 4; the algorithm for study size in Section 4.3, the case studies in   \n524 Section 5, and we provide a link to the anonymized GitHub repository for the module.   \n525 Guidelines:   \n526 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n527 made in the paper.   \n528 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n529 contributions made in the paper and important assumptions and limitations. A No or   \n530 NA answer to this question will not be perceived well by the reviewers.   \n531 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n532 much the results can be expected to generalize to other settings.   \n533 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n534 are not attained by the paper. ", "page_idx": 16}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: In Section 6. ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 16}, {"type": "text", "text": "566 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "567 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n568 a complete (and correct) proof?   \n72 \u2022 The answer NA means that the paper does not include theoretical results.   \n73 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n74 referenced.   \n75 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n76 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n77 they appear in the supplemental material, the authors are encouraged to provide a short   \n78 proof sketch to provide intuition.   \n79 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n80 by formal proofs provided in appendix or supplemental material.   \n81 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "582 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "583 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n584 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n585 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "587 Justification: On GitHub. ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "620 5. Open access to data and code ", "page_idx": 17}, {"type": "text", "text": "21 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n22 tions to faithfully reproduce the main experimental results, as described in supplemental   \n23 material?   \n24 Answer: [Yes]   \n25 Justification: On GitHub.   \n26 Guidelines:   \n27 \u2022 The answer NA means that paper does not include experiments requiring code.   \n28 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n29 public/guides/CodeSubmissionPolicy) for more details.   \n30 \u2022 While we encourage the release of code and data, we understand that this might not be   \n31 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n32 including code, unless this is central to the contribution (e.g., for a new open-source   \n33 benchmark).   \n34 \u2022 The instructions should contain the exact command and environment needed to run to   \n35 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n36 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n37 \u2022 The authors should provide instructions on data access and preparation, including how   \n38 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n39 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n40 proposed method and baselines. If only a subset of experiments are reproducible, they   \n41 should state which ones are omitted from the script and why.   \n42 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n43 versions (if applicable).   \n44 \u2022 Providing as much information as possible in supplemental material (appended to the   \n45 paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 18}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: In Section 5. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 18}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "659 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n660 information about the statistical significance of the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The boxplots in Section 5 show the variability for the choice of fixed factors. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) ", "page_idx": 18}, {"type": "text", "text": "3 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n4 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n5 of the mean.   \n76 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \npreferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n8 of Normality of errors is not verified.   \n9 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n0 figures symmetric error bars that would yield results that are out of range (e.g. negative   \nerror rates).   \n2 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n3 they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 19}, {"type": "text", "text": "684 8. Experiments Compute Resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "685 Question: For each experiment, does the paper provide sufficient information on the com  \n686 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n687 the experiments?   \n88 Answer: [No]   \n89 Justification: The analysis we showcase in Section 5 executes very fast, requiring in total   \n90 less than 4 hours on a standard office laptop. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 19}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 19}, {"type": "text", "text": "04 Justification:   \n05 Guidelines:   \n06 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n07 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n08 deviation from the Code of Ethics.   \n09 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n10 eration due to laws or regulations in their jurisdiction). ", "page_idx": 19}, {"type": "text", "text": "711 10. Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "712 Question: Does the paper discuss both potential positive societal impacts and negative   \n713 societal impacts of the work performed?   \n14 Answer: [NA]   \n15 Justification:   \n16 Guidelines:   \n17 \u2022 The answer NA means that there is no societal impact of the work performed.   \n18 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n19 impact or why the paper does not address societal impact.   \n20 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n21 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n22 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n23 groups), privacy considerations, and security considerations.   \n724 \u2022 The conference expects that many papers will be foundational research and not tied   \n725 to particular applications, let alone deployments. However, if there is a direct path to   \n726 any negative applications, the authors should point it out. For example, it is legitimate   \n727 to point out that an improvement in the quality of generative models could be used to   \n728 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n729 that a generic algorithm for optimizing neural networks could enable people to train   \n730 models that generate Deepfakes faster.   \n731 \u2022 The authors should consider possible harms that could arise when the technology is   \n732 being used as intended and functioning correctly, harms that could arise when the   \n733 technology is being used as intended but gives incorrect results, and harms following   \n734 from (intentional or unintentional) misuse of the technology.   \n735 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n736 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n737 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n738 feedback over time, improving the efficiency and accessibility of ML).   \n739 11. Safeguards   \n740 Question: Does the paper describe safeguards that have been put in place for responsible   \n741 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n742 image generators, or scraped datasets)?   \n743 Answer: [NA]   \n744 Justification:   \n745 Guidelines:   \n746 \u2022 The answer NA means that the paper poses no such risks.   \n747 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n748 necessary safeguards to allow for controlled use of the model, for example by requiring   \n749 that users adhere to usage guidelines or restrictions to access the model or implementing   \n750 safety filters.   \n751 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n752 should describe how they avoided releasing unsafe images.   \n753 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n754 not require this, but we encourage authors to take this into account and make a best   \n755 faith effort.   \n756 12. Licenses for existing assets   \n757 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n758 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n759 properly respected?   \n760 Answer: [Yes]   \n761 Justification: To the best of our knowledge, we referenced all sources in the appropriate way.   \n762 Guidelines:   \n763 \u2022 The answer NA means that the paper does not use existing assets.   \n764 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n765 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n766 URL.   \n767 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n768 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n769 service of that source should be provided.   \n770 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n771 package should be provided. For popular datasets, paperswithcode.com/datasets   \n772 has curated licenses for some datasets. Their licensing guide can help determine the   \n773 license of a dataset.   \n774 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n775 the derived asset (if it has changed) should be provided.   \n776 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n777 the asset\u2019s creators.   \n778 13. New Assets   \n779 Question: Are new assets introduced in the paper well documented and is the documentation   \n780 provided alongside the assets?   \n781 Answer: [Yes]   \n782 Justification: Our Python module is documented on GitHub.   \n783 Guidelines:   \n784 \u2022 The answer NA means that the paper does not release new assets.   \n785 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n786 submissions via structured templates. This includes details about training, license,   \n787 limitations, etc.   \n788 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n789 asset is used.   \n790 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n791 create an anonymized URL or include an anonymized zip file.   \n792 14. Crowdsourcing and Research with Human Subjects   \n793 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n794 include the full text of instructions given to participants and screenshots, if applicable, as   \n795 well as details about compensation (if any)?   \n796 Answer: [NA]   \n797 Justification:   \n798 Guidelines:   \n799 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n800 human subjects.   \n801 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n802 tion of the paper involves human subjects, then as much detail as possible should be   \n803 included in the main paper.   \n804 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n805 or other labor should be paid at least the minimum wage in the country of the data   \n806 collector. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 21}]