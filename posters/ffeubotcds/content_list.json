[{"type": "text", "text": "Persistent Test-time Adaptation in Recurring Testing Scenarios ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Trung-Hieu Hoang1 Duc Minh $\\mathbf{V_{O}}^{2}$ Minh N. Do1,3 ", "page_idx": 0}, {"type": "text", "text": "1Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign 2The University of Tokyo 3VinUni-Illinois Smart Health Center, VinUniversity {hthieu, minhdo}@illinois.edu vmduc@nlab.ci.i.u-tokyo.ac.jp ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Current test-time adaptation (TTA) approaches aim to adapt a machine learning model to environments that change continuously. Yet, it is unclear whether TTA methods can maintain their adaptability over prolonged periods. To answer this question, we introduce a diagnostic setting - recurring TTA where environments not only change but also recur over time, creating an extensive data stream. This setting allows us to examine the error accumulation of TTA models, in the most basic scenario, when they are regularly exposed to previous testing environments. Furthermore, we simulate a TTA process on a simple yet representative $\\epsilon$ -perturbed Gaussian Mixture Model Classifier, deriving theoretical insights into the dataset- and algorithm-dependent factors contributing to gradual performance degradation. Our investigation leads us to propose persistent TTA (PeTTA), which senses when the model is diverging towards collapse and adjusts the adaptation strategy, striking a balance between the dual objectives of adaptation and model collapse prevention. The supreme stability of PeTTA over existing approaches, in the face of lifelong TTA scenarios, has been demonstrated over comprehensive experiments on various benchmarks. Our project page is available at https://hthieu166.github.io/petta. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Machine learning (ML) models have demonstrated significant achievements in various areas [18, 38, 47, 23]. Still, they are inherently susceptible to distribution-shift [46, 13, 48, 21, 6] (also known as the divergence between the training and testing environments), leading to a significant degradation in model performance. The ability to deviate from the conventional testing setting appears as a crucial aspect in boosting ML models\u2019 adaptability when confronted with a new testing environment that has been investigated [30, 53, 14]. Among common domain generalization methods [58, 24, 1], test-time adaptation (TTA) takes the most challenging yet rewarding path that leverages unlabeled data available at test time for self-supervised adaptation prior to the final inference [57, 39, 8, 41, 59]. ", "page_idx": 0}, {"type": "text", "text": "Early TTA studies have concentrated on a simply ideal adaptation scenario where the test samples come from a fixed single domain [57, 39, 41]. As a result, such an assumption is far from the everchanging and complex testing environments. To confront continually changing environments [59, 12], Yuan et al. [61] proposed a practical TTA scenario where distribution changing and correlative sampling occur [15] simultaneously. Though practical TTA is more realistic than what the previous assumptions have made, it still assumes that any environment only appears once in the data stream, a condition which does not hold true. Taking a surveillance camera as an example, it might accommodate varying lighting conditions recurringly day after day (Fig. 1-left). Based on this reality, we hypothesize that the recurring of those conditions may reveal the error accumulation phenomenon in TTA, resulting in performance degradation over a long period. To verify our hypothesis, we simulate a recurring testing environment and observe the increasing error rate by recurringly adapting to the test set of CIFAR-10-C [19] multiple times. We showcase the testing error of RoTTA [61] after 20 cycles of adaptation in Fig. 1-right. As expected, RoTTA can successfully adapt and deliver encouraging outcomes within the first few passes. However, this advantage is short-lived as our study uncovers a significant issue: TTA approaches in this setting may experience severe and persistent degradation in performance. Consequently, the testing error of RoTTA gradually escalates over time and quickly surpasses the model without adaptation. This result confirms the risk of TTA deployment in our illustrative scenario, as an algorithm might work well in the first place and gradually degenerate. Therefore, ensuring sustainable quality is crucial for real-world applications, especially given the recurring nature of testing environments. ", "page_idx": 0}, {"type": "image", "img_path": "ffeUBoTcdS/tmp/a8f1373480132a2691390fc61b4fdcbf3aed80626be848d543e5ded72507cbba.jpg", "img_caption": ["Figure 1: Recurring Test-time Adaption (TTA). (left) Testing environments may change recurringly and preserving adaptability when visiting the same testing condition is not guaranteed. (right) The testing error of RoTTA [61] progressively raises (performance degradation) and exceeds the error of the source model (no TTA) while our PeTTA demonstrates its stability when adapting to the test set of CIFAR-10-C [19] 20 times. The bold lines denote the running mean and the shaded lines in the background represent the testing error on each domain (excluding the source model, for clarity). "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "This study examines whether the adaptability of a TTA algorithm persists over an extended testing stream. Specifically, in the most basic scenario, where the model returns to a previously encountered testing environment after undergoing various adjustments. We thus propose a more general testing scenario than the practical TTA [61], namely recurring TTA, where the environments not only change gradually but also recur in a correlated manner over time. We first analyze a simulation using the \u03f5\u2212perturbed Gaussian Mixture Model Classifier $(\\epsilon{-}G M M C)$ on a synthesized dataset and derive a theoretical analysis to confirm our findings, offering insights to tackle similar issues in deep neural networks. The analysis provides hints for reasoning the success of many recent robust continual TTA approaches [61, 12, 59, 15] and leading us to propose a simple yet effective baseline to avoid performance degradation, namely Persistent TTA (PeTTA). PeTTA continuously monitors the chance of collapsing and adjusts the adaptation strategy on the fly, striking a balance between the two objectives: adaptation and collapse prevention. Our contributions can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 First, this work proposes a testing scenario - recurring TTA, a simple yet sufficient setup for diagnosing the overlooked gradual performance degradation phenomenon of TTA. \u2022 Second, we formally define the phenomenon of TTA collapsing and undertake a theoretical analysis on an $\\epsilon$ -GMMC, shedding light on dataset-dependent and algorithm-dependent factors that contribute to the error accumulation during TTA processes. \u2022 Third, we introduce persistent TTA (PeTTA) - a simple yet effective adaptation scheme that surpasses all baseline models and demonstrates a persisting performance. ", "page_idx": 1}, {"type": "text", "text": "For more context on related work, readers are directed to visit our discussions in Appdx. A. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Test-time Adaptation (TTA). A TTA algorithm operates on an ML classifier $f_{t}:\\mathcal{X}\\to\\mathcal{Y}$ with parameter $\\theta_{t}\\in\\Theta$ (parameter space) gradually changing over time $(t\\in T)$ that maps an input image $\\pmb{x}\\in\\mathcal{X}$ to a category (label) $y\\in\\mathcal{V}$ . Let the capital letters $(X_{t},Y_{t})\\in\\mathcal{X}\\times\\mathcal{Y}$ denote a pair of random variables with the joint distribution $P_{t}(\\underline{{\\mathbf{x}}},y)\\,\\in\\,\\mathcal{P}_{d},t\\,\\in\\,\\mathcal{T}$ . Here, $\\mathcal{P}_{d}$ belongs to collection of $D$ sets of testing scenarios (domains) $\\{\\mathcal{P}_{d}\\}_{d=1}^{D}$ . The covariate shift [46] is assumed: $P_{t}({\\pmb x})$ and $P_{t^{\\prime}}(x)$ ", "page_idx": 1}, {"type": "text", "text": "could be different but $P_{t}(y|x)=P_{t^{\\prime}}(y|x)$ holds $\\forall t\\neq t^{\\prime}$ . At $t=0$ , $\\theta_{0}$ is initialized by a supervised model trained on $P_{0}\\in\\mathcal{P}_{0}$ (source dataset). The model then explores an online stream of testing data. For each $t>0$ , it receives $X_{t}$ (typically in form of a batch of $N_{t}$ testing samples) for adapting itself $f_{t-1}\\rightarrow f_{t}$ before making the final prediction $f_{t}\\left(X_{t}\\right)$ . ", "page_idx": 2}, {"type": "text", "text": "TTA with Mean Teacher Update. To achieve a stable optimization process, the main (teacher) model $f_{t}$ are updated indirectly through a student model with parameters $\\theta_{t}^{\\prime}$ [57, 61, 12, 15, 55]. At first, the teacher model in the previous step introduces a pseudo label [28] $\\hat{Y_{t}}$ for each $X_{t}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{Y}_{t}=f_{t-1}(X_{t}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "With a classification loss $\\mathcal{L}_{\\mathrm{CLS}}$ (e.g., cross-entropy [16]), and a model parameters regularizer $\\mathcal{R}$ , the student model is first updated with a generic optimization operator Optim, followed by an exponential moving average (EMA) update of the teacher model parameter $\\theta_{t-1}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta_{t}^{\\prime}=0\\mathrm{ptim}\\mathbb{E}_{P_{t}}\\left[\\mathcal{L}_{\\mathrm{CLS}}\\left(\\hat{Y}_{t},X_{t};\\theta^{\\prime}\\right)\\right]+\\lambda\\mathcal{R}(\\theta^{\\prime}),}\\\\ &{\\theta_{t}=(1-\\alpha)\\theta_{t-1}+\\alpha\\theta_{t}^{\\prime},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "with $\\alpha\\in(0,1)$ - the update rate of EMA, and $\\lambda\\in\\mathbb{R}^{+}$ - the weighting coefficient of the regularization term, are the two hyper-parameters. ", "page_idx": 2}, {"type": "text", "text": "Practical TTA. In practical TTA [61], two characteristics of the aforementioned distribution of data stream are noticeable. Firstly, Pt\u2019s can be partitioned by td\u2019s in which {Pt}ttd=td\u22121 . Here, each partition of consecutive steps follows the same underlying distribution which will change continually through $D$ domains [59] $(\\mathcal{P}_{1}\\to\\mathcal{P}_{2}\\cdot\\cdot\\cdot\\to\\mathcal{P}_{D})$ ). Secondly, the category distribution in each testing batch is temporally correlated [15]. This means within a batch, a small subset of categories is dominant over others, making the marginal distribution $P_{t}(y)=0,\\forall y\\notin\\mathcal{V}_{t}\\subset\\mathcal{V}$ even though the category distribution over all batches are balanced. Optimizing under this low intra-batch diversity $(|\\mathcal{D}_{t}\\bar{|}\\ll|\\mathcal{D}|)$ situation can slowly degenerate the model [7]. ", "page_idx": 2}, {"type": "text", "text": "3 Recurring TTA and Theoretical Analysis ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This section conducts a theoretical analysis on a concrete failure case of a simple TTA model. The results presented at the end of Sec. 3.2 will elucidate the factors contributing to the collapse (Sec. 3.1), explaining existing good practices (Sec. 3.3) and give insights into potential solutions (Sec. 4). ", "page_idx": 2}, {"type": "text", "text": "3.1 Recurring TTA and Model Collapse ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Recurring TTA. To study the gradual performance degradation (or model collapse), we propose a new testing scenario based on practical TTA [61]. Conducting a single pass through $D$ distributions, as done in earlier studies [61, 59], may not effectively identify the degradation. To promote consistency, our recurring TTA performs revisiting the previous distributions $K$ times to compare the incremental error versus the previous visits. For example, a sequence with $K=2$ could be $\\mathcal{P}_{1}\\to\\mathcal{P}_{2}\\to\\cdot\\cdot\\cdot\\to$ $\\mathcal{P}_{D}\\rightarrow\\mathcal{P}_{1}\\rightarrow\\mathcal{P}_{2}\\rightarrow\\dots\\rightarrow\\mathcal{P}_{D}$ . Appdx. D extends our justifications on constructing recurring TTA. ", "page_idx": 2}, {"type": "text", "text": "Definition 1 (Model Collapse). A model is said to be collapsed from step $\\tau\\in\\mathcal{T},\\tau<\\infty$ if there exists a non-empty subset of categories $\\tilde{\\mathcal{D}}\\subset\\mathcal{D}$ such that $\\operatorname*{Pr}\\{Y_{t}\\,\\in\\,\\tilde{\\mathcal{P}}\\}\\,>\\,0$ but the marginal $\\operatorname*{Pr}\\{\\hat{Y}_{t}\\in\\tilde{\\mathcal{Y}}\\}$ converges to zero in probability: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\rightarrow\\tau}\\operatorname*{Pr}\\{\\hat{Y}_{t}\\in\\tilde{\\mathcal{Y}}\\}=0.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here, upon collapsing, a model tends to ignore almost categories in $\\tilde{\\mathcal{D}}$ . As it is irrecoverable once collapsed, the only remedy would be resetting all parameters back to $\\theta_{0}$ . ", "page_idx": 2}, {"type": "text", "text": "3.2 Simulation of Failure and Theoretical Analysis ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Collapsing behavior varies across datasets and the adaptation processes. Formally studying this phenomenon on a particular real dataset and a TTA algorithm is challenging. Therefore, we propose a theoretical analysis on $\\epsilon$ -perturbed binary Gaussian Mixture Model Classifier $\\epsilon$ -GMMC) that shares the typical characteristics by construction and demonstrates the same collapsing pattern in action (Sec. 5.1) as observed on real continual TTA processes (Sec. 5.3). ", "page_idx": 2}, {"type": "text", "text": "Figure 2: $\\epsilon$ -perturbed binary Gaussian Mixture Model Classifier, imitating a continual TTA algorithm for theoretical analysis. Two main components include a pseudo-label predictor (Eq. 1), and a mean teacher update (Eqs. 2, 3). The predictor is perturbed for retaining a false negative rate of $\\epsilon_{t}$ to simulate an undesirable TTA testing stream. ", "page_idx": 3}, {"type": "image", "img_path": "ffeUBoTcdS/tmp/b7dbc1bfab9b0b025677cf23a6f7c74420c452e77415a4aaea3f7adc05f64176.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Simulated Testing Stream. Observing a testing stream with $(X_{t},Y_{t})\\in\\mathcal{X}\\times\\mathcal{Y}=\\mathbb{R}\\times\\{0,1\\}$ and the underlying joint distribution $P_{t}(x,\\bar{y})=p_{y,t}:\\mathcal{N}(x;\\mu_{y},\\sigma_{y}^{2})$ . The main task is predicting $X_{t}$ was sampled from cluster 0 or 1 (negative or positive). Conveniently, let $p_{y,t}\\triangleq P_{t}(y)=\\operatorname*{Pr}(Y_{t}=y)$ and $\\hat{p}_{y,t}\\overset{\\Delta}{=}\\operatorname*{Pr}(\\hat{Y}_{t}=y)$ be the marginal distribution of the true label $Y_{t}$ and pseudo label $\\hat{Y}_{t}$ . ", "page_idx": 3}, {"type": "text", "text": "GMMC and TTA. GMMC first implies an equal prior distribution by construction which is desirable for the actual TTA algorithms (e.g., category-balanced sampling strategies in [61, 15]). Thus, it simplifies $f_{t}$ into a maximum likelihood estimation $f_{t}(x)=\\operatorname{argmax}_{y\\in y}\\operatorname{\\bar{P}r}(x|y;\\theta_{t})$ with $\\mathrm{Pr}(x|y;\\theta_{t})=\\mathcal N(x;\\hat{\\mu}_{y,t},\\hat{\\sigma}_{y,t}^{2})$ . The goal is estimating a set of parameters $\\theta_{t}=\\{\\hat{\\mu}_{y,t},\\hat{\\sigma}_{y,t}^{2}\\}_{y\\in\\mathcal{y}}.$ . A perfect classifier $\\theta_{0}=\\{\\mu_{y},\\sigma_{y}^{2}\\}_{y\\in\\mathcal{y}}$ is initialized at $t=0$ . For the consecutive steps, the simplicity of GMMC allows solving the Optim (for finding $\\theta_{t}^{\\prime}$ , Eq. 2) perfectly by computing the empirical mean and variance of new samples, approximating $\\mathbb{E}_{P_{t}}$ . The mean teacher update (Eq. 3) for GMMC is: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mu}_{y,t}=\\left\\{\\begin{array}{l l}{(1-\\alpha)\\hat{\\mu}_{y,t-1}+\\alpha\\mathbb{E}_{P_{t}}\\left[X_{t}|\\hat{Y}_{t}\\right]}&{\\mathrm{if~}\\hat{Y}_{t}=y}\\\\ {\\hat{\\mu}_{y,t-1}}&{\\mathrm{otherwise}}\\end{array}\\right..}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The update of $\\hat{\\sigma}_{y,t}^{2}$ is similar. $\\hat{Y}_{t}=f_{t-1}(X_{t})$ can be interpreted as a pseudo label (Eq. 1). ", "page_idx": 3}, {"type": "text", "text": "$\\epsilon$ -GMMC. Severe distribution shifts or low intra-batch category diversity of recurring TTA/practical TTA both result in an increase in the error rate of the predictor. Instead of directly modeling the dynamic changes of $p_{y,t}$ (which can be complicated depending on the dataset), we study an $\\epsilon-$ pertubed GMMC ( $\\epsilon{\\mathrm{-GMMC}})$ , where $p_{y,t}$ is assumed to be static (defined below) and the pseudolabel predictor of this model is perturbed to simulate undesirable effects of the testing stream on the predictor. Two kinds of errors appear in a binary classifier [4]. Let ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\epsilon_{t}=\\mathrm{Pr}\\{Y_{t}=1|\\hat{Y}_{t}=0\\}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "be the false negative rate (FNR) of the model at step $t$ . Without loss of generality, we study the increasing type $I I$ collapse of $\\epsilon$ -GMMC. By intentionally flipping the true positive pseudo labels in simulation, an FNR of $\\epsilon_{t}$ is maintained (Fig. 2). ", "page_idx": 3}, {"type": "text", "text": "Assumption 1 (Static Data Stream). The marginal distribution of the true label follows the same Bernoulli distribution $\\mathrm{Ber}(p_{0})$ : $p_{0,t}=p_{0}$ , $(p_{1,t}=p_{1}=1-p_{0})$ ), $\\forall t\\in\\mathcal{T}$ . ", "page_idx": 3}, {"type": "text", "text": "Lemma 1 (Increasing FNR). Under Assumption $^{\\,l}$ , a binary $\\epsilon$ -GMMC would collapsed (Def. 1) with $\\operatorname*{lim}_{t\\rightarrow\\tau}\\hat{p}_{1,t}=0$ (or $\\operatorname*{lim}_{t\\rightarrow\\tau}\\hat{p}_{0,t}=1$ , equivalently) if and only $\\eta\\operatorname*{lim}_{t\\to\\tau}\\epsilon_{t}=p_{1}$ . ", "page_idx": 3}, {"type": "text", "text": "Lemma 1 states the negative correlation between $\\hat{p}_{1,t}$ and $\\epsilon_{t}$ . Unsurprisingly, towards the collapsing point where all predictions are zeros, the FNR also increases at every step and eventually reaches the highest possible FNR of $p_{1}$ . ", "page_idx": 3}, {"type": "text", "text": "Lemma 2 ( $\\epsilon$ -GMMC After Collapsing). For a binary $\\epsilon$ -GMMC model, with Assumption 1, if $\\operatorname*{lim}_{t\\rightarrow\\tau}\\hat{p}_{1,t}=0$ (collapsing), the cluster $\\boldsymbol{O}$ in GMMC converges in distribution to a single-cluster GMMC with parameters: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{N}(\\hat{\\mu}_{0,t},\\hat{\\sigma}_{0,t}^{2})\\xrightarrow{d_{\\dagger}}\\mathcal{N}(p_{0}\\mu_{0}+p_{1}\\mu_{1},p_{0}\\sigma_{0}^{2}+p_{1}\\sigma_{1}^{2}+p_{0}p_{1}(\\mu_{0}-\\mu_{1})^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Lemma 2 states the resulting $\\epsilon{-}\\mathrm{GMMC}$ after collapsing. Cluster 0 now covers the whole data distribution (and assigning label 0 for all samples). Furthermore, collapsing happens when $\\hat{\\mu}_{0,t}$ moves toward $\\mu_{1}$ . We next investigate the factors and conditions for this undesirable convergence. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1 (Convergence of $\\epsilon$ \u2212GMMC). For a binary $\\epsilon$ -GMMC model, with Assumption 1, let the distance from $\\hat{\\mu}_{0,t}$ toward $\\mu_{1}$ is $d_{t}^{0\\to1}=|\\mathbb{E}_{P_{t}}\\left[\\hat{\\mu}_{0,t}\\right]-\\dot{\\mu}_{1}|,$ , then: ", "page_idx": 4}, {"type": "equation", "text": "$$\nd_{t}^{0\\to1}-d_{t-1}^{0\\to1}\\leq\\alpha\\cdot p_{0}\\cdot\\bigg(|\\mu_{0}-\\mu_{1}|-\\frac{d_{t-1}^{0\\to1}}{1-\\epsilon_{t}}\\bigg)\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "From Thm. 1, we observe that the distance $d_{t}^{0\\to1}$ \u2019s converges (also indicating the convergence to the distribution in Lemma 2) if $d_{t}^{0\\to1}<d_{t-1}^{0\\to1}$ dt0\u2212\u219211 . The model collapse happens when this condition holds for a sufficiently long period. ", "page_idx": 4}, {"type": "text", "text": "Corollary 1 (A Condition for $\\epsilon$ \u2212GMMC Collapse). With fixed $p_{0},\\,\\alpha,\\mu_{0},\\mu_{1}$ , \u03f5\u2212GMMC is collapsed if there exists a sequence of $\\{\\epsilon_{t}\\}_{\\tau-\\Delta_{\\tau}}^{\\tau}\\,(\\tau\\geq\\Delta_{\\tau}>0)$ such that: ", "page_idx": 4}, {"type": "equation", "text": "$$\np_{1}\\geq\\epsilon_{t}>1-\\frac{d_{t-1}^{0\\rightarrow1}}{\\vert\\mu_{0}-\\mu_{1}\\vert},\\quad t\\in[\\tau-\\Delta_{\\tau},\\tau].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Corollary 1 introduces a condition $\\epsilon$ -GMMC collapse. Here, $\\epsilon_{t}$ \u2019s are non-decreasing, $\\operatorname*{lim}_{t\\to\\tau}\\epsilon_{t}=p_{1}$ . ", "page_idx": 4}, {"type": "text", "text": "Remarks. Thm. 1 concludes two sets of factors contributing to collapse: (i) data-dependent factors: the prior data distribution $\\left(p_{0}\\right)$ , the nature difference between two categories $(|\\mu_{0}-\\mu_{1}|)$ ; and (ii) algorithm-dependent factors: the update rate $(\\alpha)$ , the FNR at each step $\\left(\\epsilon_{t}\\right)$ . $\\epsilon$ -GMMC analysis sheds light on explaining model collapse on real datasets (Sec. 5.3), reasons the existing approaches (Sec. 3.3) and motivates the development of our baseline (Sec. 4). ", "page_idx": 4}, {"type": "text", "text": "3.3 Connection to Existing Solutions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Prior TTA algorithms have already incorporated implicit mechanisms to mitigate model collapse.   \nThe theoretical results in the previous section explain the rationale behind these effective strategies. ", "page_idx": 4}, {"type": "text", "text": "Regularization Term for $\\theta_{t}$ . Knowing that $f_{0}$ is always well-behaved, an attempt is restricting the divergence of $\\theta_{t}$ from $\\theta_{0}$ , e.g. using $\\mathcal{R}(\\theta_{t})\\triangleq\\|\\theta_{0}-\\theta_{t}\\|_{2}^{2}$ regularization [40]. The key idea is introducing a penalty term to avoid an extreme divergence as happening in Thm. 1. ", "page_idx": 4}, {"type": "text", "text": "Memory Bank for Harmonizing $P_{t}(x)$ . Upon receiving $X_{t}$ , samples in this batch are selectively updated to a memory bank $\\mathcal{M}$ (which already contains a subset of some instances of $X_{t^{\\prime}},t^{\\prime}<t$ in the previous steps). By keeping a balanced number of samples from each category, distribution $P_{t}^{\\mathcal{M}}(y)$ of samples in $\\mathcal{M}$ is expected to have less zero entries than $P_{t}(y)$ , making the optimization step over $P_{t}^{M}$ more desirable. From Thm. 1, $\\mathcal{M}$ moderates the extreme value of the category distribution $[p_{0}$ term) which typically appears on batches with low intra-batch category diversity. ", "page_idx": 4}, {"type": "text", "text": "4 Persistent Test-time Adaptation (PeTTA) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Now we introduce our Persistent TTA (PeTTA) approach. Further inspecting Thm. 1, while $\\epsilon_{t}$ (Eq. 5) is not computable without knowing the true labels, the measure of divergence from the initial distribution (analogously to $d_{t-1}^{0\\to1}$ term) can provide hints to fine-tune the adaptation process. ", "page_idx": 4}, {"type": "text", "text": "Key Idea. A proper adjustment toward the TTA algorithm can break the chain of monotonically increasing $\\epsilon_{t}$ \u2019s in Corollary 1 to prevent the model collapse. In the mean teacher update, the larger value of $\\lambda$ (Eq. 2) prioritizes the task of preventing collapse on one hand but also limits its adaptability to the new testing environment. Meanwhile, $\\alpha$ (Eq. 3) controls the weight on preserving versus changing the model from the previous step. Drawing inspiration from the exploration-exploitation tradeoff [49, 25] encountered in reinforcement learning [54], we introduce a mechanism for adjusting $\\lambda$ and \u03b1 on the fly, balancing between the two primary objectives: adaptation and preventing model collapse. Our strategy is prioritizing collapse prevention (increasing $\\lambda$ ) and preserving the model from previous steps (decreasing $\\alpha$ ) when there is a significant deviation from $\\theta_{0}$ . ", "page_idx": 4}, {"type": "text", "text": "In [40, 61, 59], $\\lambda$ and $\\alpha$ were fixed through hyper-parameter tuning. This is suboptimal due to varying TTA environments and the lack of validation set [62]. Furthermore, Thm. 1 suggests the convergence rate quickly escalates when $\\epsilon_{t}$ increases, making constant $\\lambda,\\alpha$ insufficient to prevent collapse. ", "page_idx": 4}, {"type": "text", "text": "Sensing the Divergence of $\\theta_{t}$ . We first equip PeTTA with a mechanism for measuring its divergence from $\\theta_{0}$ . Since $f_{t}(\\pmb{x})=$ argmax ${\\bf\\Gamma}_{y\\in\\mathcal{Y}}\\operatorname*{Pr}(y|{\\pmb x};{\\pmb\\theta}_{t})$ , we can decompose $\\operatorname*{Pr}(y|\\pmb{x};\\theta_{t})\\stackrel{-}{=}[h\\left(\\phi_{\\theta_{t}}(\\pmb{\\bar{x}})\\right)]_{y}$ , with $\\phi_{\\theta_{t}}(\\cdot)$ is a $\\theta_{t}$ -parameterized deep feature extractor followed by a fixed classification head (a linear and softmax layer) $h(\\cdot)$ . The operator $[\\cdot]_{y}$ extracts the $y^{\\mathrm{th}}$ component of a vector. ", "page_idx": 4}, {"type": "text", "text": "Since $h(\\cdot)$ remains unchanged, instead of comparing the divergence in the parameter space $\\left(\\Theta\\right)$ or between the output probability $\\operatorname*{Pr}(y|\\pmb{x};\\pmb{\\theta}_{t})$ and $\\bar{\\mathrm{Pr}}(y|\\bar{\\boldsymbol{x}};\\boldsymbol{\\theta}_{0})$ , we suggest an inspection over the feature embedding space that preserves a maximum amount of information in our case (data processing inequality [9]). Inspired by [31] and under Gaussian assumption, the Mahalanobis distance of the first moment of the feature embedding vectors is compared. Let ${z}\\,=\\,\\phi_{\\theta_{t}}({\\pmb x})$ , we keep track of a collection of the running mean of feature vector $_{z}$ : $\\{\\bar{\\mu}_{t}^{y}\\}_{y\\in\\mathcal{Y}}$ in which $\\hat{\\pmb{\\mu}}_{t}^{y}$ is EMA updated with vector $_{\\textit{z}}$ if $f_{t}(\\pmb{x})=y$ . The divergence of $\\theta_{t}$ at step $t$ , evaluated on class $y$ is defined as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\gamma_{t}^{y}=1-\\exp\\left(-(\\hat{\\mu}_{t}^{y}-\\pmb{\\mu}_{0}^{y})^{T}\\left(\\pmb{\\Sigma}_{0}^{y}\\right)^{-1}(\\hat{\\mu}_{t}^{y}-\\pmb{\\mu}_{0}^{y})\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\pmb{\\mu}_{0}^{y}$ and $\\Sigma_{0}^{y}$ are the pre-computed empirical mean and covariant matrix of feature vectors in the source dataset $(P_{0})$ . The covariant matrix here is diagonal for simplicity. In practice, without directly accessing the training set, we assume a small set of unlabeled samples can be drawn from the source distribution for empirically computing these values (visit Appdx. E.4 for further details). ", "page_idx": 5}, {"type": "text", "text": "Here, we implicitly expect the independence of each entry in $_{\\textit{z}}$ and TTA approaches learn to align feature vectors of new domains back to the source domain $(P_{0})$ . Therefore, the accumulated statistics of these feature vectors at each step should be concentrated near the vectors of the initial model. The value of $\\gamma_{t}^{y}\\in[0,1]$ is close to 0 when $\\theta_{t}=\\theta_{0}$ and increases exponentially as $\\hat{\\pmb{\\mu}}_{t}^{y}$ diverging from $\\pmb{\\mu}_{0}^{y}$ . ", "page_idx": 5}, {"type": "text", "text": "Adaptive Regularization and Model Update. With $\\alpha_{0},\\lambda_{0}$ are initial values, utilizing $\\gamma_{t}^{y}$ derived in Eq. 6, a pair of $(\\lambda_{t},\\alpha_{t})$ is adaptively chosen at each step: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\bar{\\gamma}_{t}=\\displaystyle\\frac{1}{|\\hat{\\mathcal{V}}_{t}|}\\sum_{y\\in\\hat{\\mathcal{Y}}_{t}}\\gamma_{t}^{y},\\quad\\hat{\\mathcal{Y}}_{t}=\\Big\\{\\hat{Y}_{t}^{(i)}|i=1,\\cdots,N_{t}\\Big\\}\\,;}\\\\ {\\lambda_{t}=\\bar{\\gamma}_{t}\\cdot\\lambda_{0},\\quad\\quad\\alpha_{t}=\\left(1-\\bar{\\gamma}_{t}\\right)\\cdot\\alpha_{0},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "$\\hat{\\mathcal{V}}_{t}$ is a set of unique pseudo labels in a testing batch $\\hat{Y}_{t}^{(i)}$ is the $i^{\\mathrm{th}}$ realization of $\\hat{Y_{t}}$ ). ", "page_idx": 5}, {"type": "text", "text": "Anchor Loss. Penalizing the divergence with regular vector norms in high-dimensional space $\\left(\\Theta\\right)$ is insufficient (curse of dimensionality [5, 51]), especially with a large model and limited samples. Anchor loss $\\mathcal{L}_{\\mathrm{AL}}$ can nail down the similarity between $f_{t}$ and $f_{0}$ in the probability space [32, 12]: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{AL}}(X_{t};\\theta)=-\\sum_{y\\in\\mathcal{Y}}\\operatorname*{Pr}(y|X_{t};\\theta_{0})\\log\\operatorname*{Pr}(y|X_{t};\\theta),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "which is equivalent to minimizing the KL divergence $D_{K L}\\left(\\operatorname*{Pr}(y|X_{t};\\theta_{0})\\Vert\\operatorname*{Pr}(y|X_{t};\\theta)\\right).$ ", "page_idx": 5}, {"type": "text", "text": "Persistent TTA. Having all the ingredients, we design our approach, PeTTA, following the convention setup of the mean teacher update, with the category-balanced memory bank and the robust batch normalization layer from [61]. Appdx. E.1 introduces the pseudo code of PeTTA. For $\\mathcal{L}_{\\mathrm{CLS}}$ , either the self-training scheme [12] or the regular cross-entropy [16] is adopted. With ${\\mathcal{R}}(\\theta)$ , cosine similarity or L2 distance are both valid metrics for measuring the distance between $\\theta$ and $\\theta_{0}$ in the parameter space. Fisher regularizer coefficient [40, 27] can also be used, optionally. To sum up, the teacher model update of PeTTA is an elaborated version of EMA with $\\lambda_{t},\\alpha_{t}$ (Eq. 7) and $\\mathcal{L}_{\\mathrm{AL}}$ (Eq. 8): ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta_{t}^{\\prime}=0\\mathbf{p}\\mathbf{t}\\mathrm{im}\\,\\mathbb{E}_{P_{t}}\\left[\\mathcal{L}_{\\mathrm{CLS}}\\left(\\hat{Y}_{t},X_{t};\\theta^{\\prime}\\right)+\\mathcal{L}_{\\mathrm{AL}}\\left(X_{t};\\theta^{\\prime}\\right)\\right]+\\lambda_{t}\\mathcal{R}(\\theta^{\\prime}),}\\\\ &{\\theta_{t}=(1-\\alpha_{t})\\theta_{t-1}+\\alpha_{t}\\theta_{t}^{\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "5 Experimental Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "5.1 \u03f5\u2212MMC Simulation Result ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Simulation Setup. A total of 6000 samples from two Gaussian distributions: $\\mathcal{N}(\\mu_{0}=0,\\sigma_{0}^{2}=1)$ and $\\mathcal{N}(\\mu_{1}=2,\\bar{\\sigma}_{1}^{2}=1)$ with $\\begin{array}{r}{p_{0}=p_{1}\\overset{\\cdot}{=}\\frac{1}{2}}\\end{array}$ are synthesized and gradually released in a batch of $B=10$ samples. For evaluation, an independent set of 2000 samples following the same distribution is used for computing the prediction frequency, and the false negative rate (FNR). \u03f5\u2212GMMC update follows Eq. 4 with $\\bar{\\alpha}=5\\bar{e}^{-2}$ . To simulate model collapse, the predictor is intercepted and $10\\%$ of the true-postive pseudo labels at each testing step are randomly flipped (Corollary 1). ", "page_idx": 5}, {"type": "text", "text": "Simulation Result. In action, both the likelihood of predicting class 0 (Fig. 3a-left) and the $\\epsilon_{t}$ (Eq. 5) (Fig. 3c-right, solid line) gradually increases over time as expected (Lemma 1). After collapsing, $\\epsilon$ -GMMC merges the two initial clusters, resulting in a single one (Fig. 3b-left) with parameters that match Lemma 2. The distance from $\\hat{\\mu}_{0,t}$ (initialized at $\\mu_{0}$ ) towards $\\mu_{1}$ converges (Fig. 3c-left, solid line), coincided with the analysis in Thm. 1 when $\\epsilon_{t}$ is chosen following Corollary 1 (Fig. 3c, dashed line). GMMC (perturbed-free) stably produces accurate predictions (Fig. 3a-right) and approximates the true data distribution (Fig. 3b-right). The simulation empirically validates our analysis (Sec. 3.2), confirming the vulnerability of TTA models when the pseudo labels are inaccurately estimated. ", "page_idx": 5}, {"type": "image", "img_path": "ffeUBoTcdS/tmp/1d7bcb8642f087a9b5ac3c3d243b2f9d69fa1fc90fad0a1d6779e0bfafeffda6.jpg", "img_caption": ["Figure 3: Simulation result on \u03f5-perturbed Gaussian Mixture Model Classifier (\u03f5-GMMC) and GMMC (perturbed-free). (a) Histogram of model predictions through time. A similar prediction frequency pattern is observed on CIFAR-10-C (Fig. 5a-left). (b) The probability density function of the two clusters after convergence versus the true data distribution. The initial two clusters of $\\epsilon$ -GMMC collapsed into a single cluster with parameters stated in Lemma 2. In the perturbed-free, GMMC converges to the true data distribution. (c) Distance toward $\\mu_{1}$ $(|\\mathbb{E}_{P_{t}}\\left[\\hat{\\mu}_{0,t}\\right]-\\mu_{1}|)$ and falsenegative rate $\\left(\\epsilon_{t}\\right)$ in simulation coincides with the result in Thm. 1 (with $\\epsilon_{t}$ following Corollary 1). "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "5.2 Setup - Benchmark Datasets ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets. We benchmark the performance on four TTA classification tasks. Specifically, CIFAR10 $\\rightarrow$ CIFAR10-C, CIFAR $100\\rightarrow$ CIFAR100-C, and ImageNet $\\rightarrow$ ImageNet-C [19] are three corrupted images classification tasks (corruption level 5, the most severe). Additionally, we incorporate DomainNet [44] with 126 categories from four domains for the task real $\\rightarrow$ clipart, painting, sketch. ", "page_idx": 6}, {"type": "text", "text": "Compared Methods. Besides PeTTA, the following algorithms are investigated: CoTTA [59], EATA [40], RMT [12], MECTA [22], RoTTA [61], ROID [37] and TRIBE [52]. Noteworthy, only RoTTA is specifically designed for the practical TTA setting while others fti the continual TTA setting in general. A parameter-free approach: LAME [7] and a reset-based approach (i.e., reverting the model to the source model after adapting to every $1,000$ images): RDumb [45] are also included. ", "page_idx": 6}, {"type": "text", "text": "Recurring TTA. Following the practical TTA setup, multiple testing scenarios from each testing set will gradually change from one to another while the Dirichlet distribution (Dir(0.1) for CIFAR10- C, DomainNet, and ImageNet-C, and $\\operatorname{Dir}(0.01)$ for CIFAR100-C) generates category temporally correlated batches of data. For all experiments, we set the number of revisits $K=20$ (times) as this number is sufficient to fully observe the gradual degradation on existing TTA baselines. ", "page_idx": 6}, {"type": "text", "text": "Implementation Details. We use PyTorch [43] for implementation. RobustBench [10] and torchvision [35] provide pre-trained source models. Hyper-parameter choices are kept as close as possible to the original selections of authors. Visit Sec. G for more implementation details. Unless otherwise noted, for all PeTTA experiments, the EMA update rate for robust batch normalization [61] and feature embedding statistics is set to $5e^{-2}$ ; $\\alpha_{0}=1e^{-3}$ and cosine similarity regularizer is used. On CIFAR10/100-C and ImageNet-C we use the self-training loss in [12] for $\\mathcal{L}_{\\mathrm{CLS}}$ and $\\lambda_{0}=10$ while the regular cross-entropy loss [13] and $\\lambda_{0}~=~1$ (severe domain shift requires prioritizing adaptability) are applied in DomainNet experiments. In Appdx. F.5, we provide a sensitivity analysis on the choice of hyper-parameter $\\lambda_{0}$ in PeTTA. ", "page_idx": 6}, {"type": "table", "img_path": "ffeUBoTcdS/tmp/60fe96e5b17329829a495a93cf06296b27611a13976d20f4e77e1e73643ffee5.jpg", "table_caption": ["Table 1: Average classification error of the task CIFAR- $10\\rightarrow$ CIFAR-10-C in recurring TTA. The lowest error is in bold,(\u2217)average value across 5 runs (different random seeds) is reported for PeTTA. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5.3 Result - Benchmark Datasets ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Recurring TTA Performance. Fig. 1-right presents the testing error on CIFAR-10-C in recurring TTA setting. RoTTA [61] exhibits promising performance in the first several visits but soon raises and eventually exceeds the source model (no TTA). The classification error of compared methods on CIFAR- $10{\\rightarrow}1$ CIFAR-10-C, and ImageNet $\\rightarrow$ ImageNet-C [19] tasks are shown in Tab. 1, and Tab. 2. Appdx. F.1 provides the results on the other two datasets. The observed performance degradation of CoTTA [59], EATA [40], RoTTA [61], and TRIBE [52] confirms the risk of error accumulation for an extensive period. While RMT [12], MECTA [22], and ROID [37] remain stable, they failed to adapt to the temporally correlated test stream at the beginning, with a higher error rate than the source model. LAME [7] (parameter-free TTA) and RDumb [45] (reset-based TTA) do not suffer from collapsing. However, their performance is lagging behind, and knowledge accumulation is limited in these approaches that could potentially favor a higher performance as achieved by PeTTA. Furthermore, LAME [7] is highly constrained by the source model, and selecting a precise reset frequency in RDumb [45] is challenging in practice (see Appdx. F.3 for a further discussion). ", "page_idx": 7}, {"type": "image", "img_path": "ffeUBoTcdS/tmp/f4662423f17ea2e9c57c0b34e8c5f056076e41a7818cdc2938afc2e01e3e3207.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 4: Classification error of TRIBE [52] and PeTTA (ours) of the task CIFAR- $^{10\\rightarrow}$ CIFAR10-C task in recurring TTA with 40 visits. ", "page_idx": 7}, {"type": "text", "text": "In average, PeTTA outperforms almost every baseline approaches and persists across 20 visits over the three datasets. The only exception is at the case of TRIBE [52] on CIFAR-10- C. While this state-of-the-art model provides stronger adaptability, outweighing the PeTTA, and baseline RoTTA [61] in several recurrences, the risk of the model collapsing still presents in TRIBE [52]. This can be clearly observed when we increase the observation period to 40 recurring visits in Fig. 4. As the degree of freedom for adaptation in PeTTA is more constrained, it takes a bit longer for adaptation but remains stable afterward. Fig. 5b-bottom exhibits the confusion matrix at the last visit with satisfactory accuracy. The same results are also observed when shuffilng the order of domain shifts within each recurrence (Appdx. D.3), or extending the number of recurrences to 40 visits (Appdx. F.4). ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Continuously Changing Corruption (CCC) [45] Performance. Under CCC [45], Tab. 3 reveals the supreme performance of PeTTA over RoTTA [61] and RDumb [45]. Here, we report the average classification error between two consecutive adaptation step intervals. An adaptation step in this table corresponds to a mini-batch of data with 64 images. The model is adapted to 80, 000 steps in total with more than 5.1M images, significantly longer than 20 recurring TTA visits. Undoubtedly, PeTTA still achieves good performance where the corruptions are algorithmically generated, non-cyclic with two or more corruption types can happen simultaneously. This experiment also empirically justifies the construction of our recurring TTA as a diagnostic tool (Appdx. D.2) where similar observations are concluded on the two settings. Obviously, our recurring TTA is notably simpler than CCC [45]. ", "page_idx": 7}, {"type": "table", "img_path": "ffeUBoTcdS/tmp/9b2db0d1ec58406367dca6e762275957c99cdaaa34d9f34a004a28f513a31273.jpg", "table_caption": ["Table 2: Average classification error of the task ImageNet $\\rightarrow$ ImageNet-C in recurring TTA scenario. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 3: Average classification error on CCC [45] setting. Each column presents the average error within an adaptation interval (e.g., the second column provides the average error between the 6701 and 13400 adaptation steps). Each adaptation step here is performed on a mini-batch of 64 images. ", "page_idx": 8}, {"type": "table", "img_path": "ffeUBoTcdS/tmp/16cb858a9bf3f9c5261404da84647835787f897d306c3bd63dd36ad996ae1799.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "ffeUBoTcdS/tmp/bb0b2448115220a55afba82c1515165f176392a7f41ec9b0b05cec5447eee140.jpg", "img_caption": ["Figure 5: Recurring TTA (20 visits) on CIFAR- $^{10\\rightarrow}$ CIFAR10-C task. (a) Histogram of model predictions (10 labels are color-coded). PeTTA achieves a persisting performance while RoTTA [61] degrades. (b) Confusion matrix at the last visit, RoTTA classifies all samples into a few categories (e.g., 0: airplane, 4: deer). (c) Force-directed graphs showing (left) the most prone to misclassification pairs (arrows indicating the portion and pointing from the true to the misclassified category); (right) similar categories tend to be easily collapsed. Edges denote the average cosine similarity of feature vectors (source model), only the highest similar pairs are shown. Best viewed in color. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Collapsing Pattern. The rise in classification error (Fig. 1-right) can be reasoned by the prediction frequency of RoTTA [61] in an recurring TTA setting (Fig. 5a-left). Similar to $\\epsilon$ -GMMC, the likelihood of receiving predictions on certain categories gradually increases and dominates the others. Further inspecting the confusion matrix of a collapsed model (Fig. 5b-top) reveals two major groups of categories are formed and a single category within each group represents all members, thereby becoming dominant. To see this, Fig. 5c-left simplifies the confusion matrix by only visualizing the ", "page_idx": 8}, {"type": "text", "text": "Table 4: Average (across 20 visits) error of multiple variations of PeTTA: without (w/o) $\\mathcal{R}(\\theta),\\mathcal{L}_{\\mathrm{AL}};\\mathcal{L}_{\\mathrm{AL}}$ only; fixed regularization coefficient $\\lambda$ ; adaptive coefficient $\\lambda_{t}$ , update rate $\\alpha_{t}$ ; using anchor loss $\\mathcal{L}_{\\mathrm{AL}}$ . ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "table", "img_path": "ffeUBoTcdS/tmp/1bad440bc432dc678b86666901b6c7d39f9a7cbfaf4cd9682f43901deb125e3e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "ffeUBoTcdS/tmp/60e6a6cc0cffb56d618949b69d03ae6eb7ee0f75a150606314e1474f3f20768a.jpg", "table_caption": ["Table 5: Average (across 20 visits) error of PeTTA. PeTTA favors various choices of regularizers ${\\mathcal{R}}(\\theta)$ : L2 and cosine similarity in conjunction with Fisher [27, 40] coefficient. "], "table_footnote": ["CF: CIFAR, DN: DomainNet, IN: ImageNet "], "page_idx": 9}, {"type": "text", "text": "top prone-to-misclassified pair of categories. Here, label deer is used for almost every living animal while airplane represents transport vehicles. The similarity between categories in the feature space of the source model (Fig. 5c-right) is correlated with the likelihood of being merged upon collapsing. As distance in feature space is analogous to $|\\mu_{0}-\\mu_{1}|$ (Thm. 1), closer clusters are at a higher risk of collapsing. This explains and showcases that the collapsing behavior is predictable up to some extent. ", "page_idx": 9}, {"type": "text", "text": "5.4 Ablation Study ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Effect of Each Component. Tab. 4 gives an ablation study on PeTTA, highlighting the use of a regularization term $({\\mathcal{R}}(\\theta))$ with a fixed choice of $\\lambda,\\alpha$ not only fails to mitigate model collapse but may also introduce a negative effect (rows 2-3). Trivially applying the anchor loss $(\\mathcal{L}_{\\mathrm{AL}})$ alone is also incapable of eliminating the lifelong performance degradation in continual TTA (row 4). Within PeTTA, adopting the adaptive $\\lambda_{t}$ scheme alone (row 5) or in conjunction with either $\\alpha_{t}$ or anchor loss $\\mathcal{L}_{\\mathrm{AL}}$ (rows 6-7) partially stabilizes the performance. Under the drastic domain shifts with a larger size of categories or model parameters (e.g., on CIFAR-100-C, DomainNet, ImageNet-C), restricting $\\alpha_{t}$ adjustment limits the ability of PeTTA to stop undesirable updates while a common regularization term without $\\mathcal{L}_{\\mathrm{AL}}$ is insufficient to guide the adaptation. Thus, leveraging all elements secures the persistence of PeTTA (row 8). ", "page_idx": 9}, {"type": "text", "text": "Various Choices of Regularizers. The design of PeTTA is not coupled with any specific regularization term. Demonstrated in Tab. 5, PeTTA works well for the two common choices: L2 and cosine similarity. The conjunction use of Fisher coefficent [27, 40] for weighting the model parameter importance is also studied. While the benefti (in terms of improving accuracy) varies across datasets, PeTTA accommodates all choices, as the model collapse is not observed in any of the options. ", "page_idx": 9}, {"type": "text", "text": "6 Discussions and Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "On a Potential Risk of TTA in Practice. We provide empirical and theoretical evidence on the risk of deploying continual TTA algorithms. Existing studies fail to detect this issue with a single pass per test set. The recurring TTA could be conveniently adopted as a straightforward evaluation, where its challenging test stream magnifies the error accumulation that a model might encounter in practice. ", "page_idx": 9}, {"type": "text", "text": "Limitations. PeTTA takes one step toward mitigating the gradual performance degradation of TTA. Nevertheless, a complete elimination of error accumulation cannot be guaranteed rigorously through regularization. Future research could delve deeper into expanding our efforts to develop an algorithm that achieves error accumulation-free by construction. Furthermore, as tackling the challenge of the temporally correlated testing stream is not the focus of PeTTA, using a small memory bank as in [61, 15] is necessary. It also assumes the features statistics from the source distribution are available (Appdx. E.3, E.4). These constraints potentially limit its scalability in real-world scenarios. ", "page_idx": 9}, {"type": "text", "text": "Conclusion. Towards trustworthy and reliable TTA applications, we rigorously study the performance degradation problem of TTA. The proposed recurring TTA setting highlights the limitations of modern TTA methods, which struggle to prevent the error accumulation when continuously adapting to demanding test streams. Theoretically inspecting a failure case of $\\epsilon{-}G M M C$ paves the road for designing PeTTA- a simple yet efficient solution that continuously assesses the model divergence for harmonizing the TTA process, balancing adaptation, and collapse prevention. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported by the Jump ARCHES Endowment through the Health Care Engineering Systems Center, JSPS/MEXT KAKENHI JP24K20830, ROIS NII Open Collaborative Research 2024-24S1201, in part by the National Institute of Health (NIH) under Grant R01 AI139401, and in part by the Vingroup Innovation Foundation under Grant VINIF.2021.DA00128. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Kartik Ahuja, Ethan Caballero, Dinghuai Zhang, Jean-Christophe Gagnon-Audet, Yoshua Bengio, Ioannis Mitliagkas, and Irina Rish. Invariance principle meets information bottleneck for out-of-distribution generalization. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=jlchsFOLfeF. [2] Rahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars, Laurent Charlin, Massimo Caccia, Min Lin, and Lucas Page-Caccia. Online continual learning with maximal interfered retrieval. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32, 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/ file/15825aee15eb335cc13f9b559f166ee8-Paper.pdf.   \n[3] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample selection for online continual learning. In Advances in Neural Information Processing Systems, volume 32, 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/ e562cd9c0768d5464b64cf61da7fc6bb-Paper.pdf.   \n[4] Amitav Banerjee, U. B. Chitnis, S. L. Jadhav, J. S. Bhawalkar, and S. Chaudhury. Hypothesis testing, type I and type II errors. Industrial Psychiatry Journal, 18(2):127\u2013131, 2009. ISSN 0972-6748. doi: 10.4103/0972-6748.62274. URL https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2996198/.   \n[5] Richard Bellman. Dynamic Programming. Princeton University Press, Princeton, NJ, USA, 1957.   \n[6] Arno Blaas, Andrew Miller, Luca Zappella, Joern-Henrik Jacobsen, and Christina Heinze-Deml. Considerations for distribution shift robustness in health. In ICLR 2023 Workshop on Trustworthy Machine Learning for Healthcare, 2023. URL https://openreview.net/forum?id=y7XveyWYzIB. [7] Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8334\u20138343, 2022. doi: 10.1109/CVPR52688.2022.00816.   \n[8] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In Proceedings of the IEEE International Conference on Computer Vision, 2022.   \n[9] Thomas M. Cover and Joy A. Thomas. Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing). Wiley-Interscience, USA, 2006. ISBN 0471241954.   \n[10] Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustness benchmark. In Thirty-ffith Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2021. URL https://openreview.net/forum?id=SSKZPJCt7B.   \n[11] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(7):3366\u20133385, 2022. doi: 10.1109/ TPAMI.2021.3057446.   \n[12] Mario D\u00f6bler, Robert A. Marsden, and Bin Yang. Robust mean teacher for continual and gradual test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 7704\u20137714, June 2022.   \n[13] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 1180\u20131189, Lille, France, 07\u201309 Jul 2015. PMLR. URL https://proceedings.mlr.press/v37/ganin15.html.   \n[14] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\u00e7ois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-Adversarial Training of Neural Networks, pages 189\u2013 209. Springer International Publishing, 2017. doi: 10.1007/978-3-319-58347-1_10. URL https: //doi.org/10.1007/978-3-319-58347-1_10.   \n[15] Taesik Gong, Jongheon Jeong, Taewon Kim, Yewon Kim, Jinwoo Shin, and Sung-Ju Lee. NOTE: Robust continual test-time adaptation against temporal correlation. In Advances in Neural Information Processing Systems, 2022.   \n[16] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In L. Saul, Y. Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems, volume 17, 2004. URL https://proceedings.neurips.cc/paper_files/paper/2004/file/ 96f2b50b5d3613adf9c27049b2a888c7-Paper.pdf.   \n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. arXiv preprint arXiv:1512.03385, 2015.   \n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1026\u20131034, 2015.   \n[19] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. Proceedings of the International Conference on Learning Representations, 2019.   \n[20] Dan Hendrycks, Norman Mu, Ekin D. Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. AugMix: A simple data processing method to improve robustness and uncertainty. Proceedings of the International Conference on Learning Representations (ICLR), 2020.   \n[21] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 8320\u20138329, 2021. doi: 10.1109/ICCV48922.2021.00823.   \n[22] Junyuan Hong, Lingjuan Lyu, Jiayu Zhou, and Michael Spranger. MECTA: Memory-economic continual test-time model adaptation. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=N92hjSf5NNh.   \n[23] Fabian Isensee, Paul F. Jaeger, Simon A. A. Kohl, Jens Petersen, and Klaus H. Maier-Hein. nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature Methods, 18(2):203\u2013211, February 2021. ISSN 1548-7105. doi: 10.1038/s41592-020-01008-z. URL https: //www.nature.com/articles/s41592-020-01008-z.   \n[24] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier adjustment module for model-agnostic domain generalization. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 2427\u20132440, 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/ 1415fe9fea0fa1e45dddcff5682239a0-Paper.pdf.   \n[25] Michael N. Katehakis and Arthur F. Veinott. The multi-armed bandit problem: Decomposition and computation. Mathematics Operations Research, 12:262\u2013268, 1987. URL https://api.semanticscholar. org/CorpusID:656323.   \n[26] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412. 6980.   \n[27] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521\u20133526, 2017. doi: 10.1073/pnas.1611835114. URL https://www.pnas.org/doi/abs/10.1073/pnas.1611835114.   \n[28] Dong-Hyun Lee. Pseudo-label : The simple and efficient semi-supervised learning method for deep neural networks. ICML 2013 Workshop : Challenges in Representation Learning (WREPL), 07 2013.   \n[29] T. Lee, S. Chottananurak, T. Gong, and S. Lee. Aetta: Label-free accuracy estimation for test-time adaptation. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 28643\u201328652, Los Alamitos, CA, USA, jun 2024. IEEE Computer Society. doi: 10.1109/CVPR52733. 2024.02706. URL https://doi.ieeecomputersociety.org/10.1109/CVPR52733.2024.02706.   \n[30] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C. Kot. Domain generalization with adversarial feature learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.   \n[31] Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou. Revisiting batch normalization for practical domain adaptation. In International Conference on Learning Representations Workshop, 2017. URL https://openreview.net/forum?id=BJuysoFeg.   \n[32] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(12):2935\u20132947, 2018. doi: 10.1109/TPAMI.2017.2773081.   \n[33] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? Source hypothesis transfer for unsupervised domain adaptation. In International Conference on Machine Learning (ICML), pages 6028\u20136039, 2020.   \n[34] Sen Lin, Peizhong Ju, Yingbin Liang, and Ness Shroff. Theory on forgetting and generalization of continual learning. In Proceedings of the 40th International Conference on Machine Learning, ICML\u201923, 2023.   \n[35] TorchVision maintainers and contributors. Torchvision: Pytorch\u2019s computer vision library. https: //github.com/pytorch/vision, 2016.   \n[36] Robert A Marsden, Mario D\u00f6bler, and Bin Yang. Gradual test-time adaptation by self-training and style transfer. arXiv preprint arXiv:2208.07736, 2022.   \n[37] Robert A Marsden, Mario D\u00f6bler, and Bin Yang. Universal test-time adaptation through weight ensembling, diversity weighting, and prior correction. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2555\u20132565, 2024.   \n[38] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. In Proceedings of the European Conference on Computer Vision (ECCV), 2020.   \n[39] A. Tuan Nguyen, Thanh Nguyen-Tang, Ser-Nam Lim, and Philip Torr. TIPI: Test time adaptation with transformation invariance. In Conference on Computer Vision and Pattern Recognition 2023, 2023. URL https://openreview.net/forum?id=NVh1cy37Ge.   \n[40] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test-time model adaptation without forgetting. In The Internetional Conference on Machine Learning, 2022.   \n[41] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, and Mingkui Tan. Towards stable test-time adaptation in dynamic wild world. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=g2YraF75Tj.   \n[42] K. R. Parthasarathy. Introduction to Probability and Measure, volume 33 of Texts and Readings in Mathematics. Hindustan Book Agency, Gurgaon, 2005. ISBN 978-81-85931-55-5 978-93-86279-27-9. doi: 10.1007/978-93-86279-27-9. URL http://link.springer.com/10.1007/978-93-86279-27-9.   \n[43] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library, 2019.   \n[44] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In Proceedings of the IEEE International Conference on Computer Vision, pages 1406\u20131415, 2019.   \n[45] Ori Press, Steffen Schneider, Matthias Kuemmerer, and Matthias Bethge. RDumb: A simple approach that questions our progress in continual test-time adaptation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=VfP6VTVsHc.   \n[46] Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D. Lawrence. Dataset Shift in Machine Learning. The MIT Press, 2009. ISBN 0262170051.   \n[47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 8748\u20138763. PMLR, 18\u201324 Jul 2021. URL https://proceedings. mlr.press/v139/radford21a.html.   \n[48] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet classifiers generalize to ImageNet? In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 5389\u20135400. PMLR, 09\u201315 Jun 2019. URL https://proceedings.mlr.press/v97/ recht19a.html.   \n[49] Mooweon Rhee and Tohyun Kim. Exploration and Exploitation, pages 543\u2013546. Palgrave Macmillan UK, London, 2018. ISBN 978-1-137-00772-8. doi: 10.1057/978-1-137-00772-8_388. URL https: //doi.org/10.1057/978-1-137-00772-8_388.   \n[50] Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, , and Gerald Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interference. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id= B1gTShAct7.   \n[51] Tanin Sirimongkolkasem and Reza Drikvandi. On Regularisation Methods for Analysis of High Dimensional Data. Annals of Data Science, 6(4):737\u2013763, December 2019. ISSN 2198-5812. doi: 10.1007/s40745-019-00209-4. URL https://doi.org/10.1007/s40745-019-00209-4.   \n[52] Yongyi Su, Xun Xu, and Kui Jia. Towards real-world test-time adaptation: Tri-net self-training with balanced normalization. Proceedings of the AAAI Conference on Artificial Intelligence, 38(13):15126\u2013 15135, 2024.   \n[53] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In Hal Daum\u00e9 III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 9229\u20139248. PMLR, 13\u201318 Jul 2020. URL https://proceedings. mlr.press/v119/sun20b.html.   \n[54] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, 2018.   \n[55] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS\u201917, page 1195\u20131204, 2017. ISBN 9781510860964.   \n[56] Daniel Vela, Andrew Sharp, Richard Zhang, Trang Nguyen, An Hoang, and Oleg S. Pianykh. Temporal quality degradation in AI models. Scientific Reports, 12(1):11654, July 2022. ISSN 2045-2322. doi: 10.1038/s41598-022-15245-z. URL https://www.nature.com/articles/s41598-022-15245-z.   \n[57] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=uXl3bZLkr3c.   \n[58] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, and Tao Qin. Generalizing to unseen domains: A survey on domain generalization. In Zhi-Hua Zhou, editor, Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pages 4627\u20134635. International Joint Conferences on Artificial Intelligence Organization, 8 2021. doi: 10.24963/ijcai.2021/628. URL https://doi.org/10. 24963/ijcai.2021/628. Survey Track.   \n[59] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 7201\u20137211, June 2022.   \n[60] Zachary Young and Robert Steele. Empirical evaluation of performance degradation of machine learning-based predictive models \u2013 a case study in healthcare information systems. International Journal of Information Management Data Insights, 2(1):100070, 2022. ISSN 2667-0968. doi: https: //doi.org/10.1016/j.jjimei.2022.100070. URL https://www.sciencedirect.com/science/article/ pii/S2667096822000143.   \n[61] Longhui Yuan, Binhui Xie, and Shuang Li. Robust test-time adaptation in dynamic scenarios. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15922\u2013 15932, 2023.   \n[62] Hao Zhao, Yuejiang Liu, Alexandre Alahi, and Tao Lin. On pitfalls of test-time adaptation. In ICLR 2023 Workshop on Pitfalls of limited data and computation for Trustworthy ML, 2023. URL https: //openreview.net/forum?id=0Go_RsG_dYn. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Persistent Test-time Adaptation in Recurring Testing Scenarios Technical Appendices ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Related Work 16 ", "page_idx": 14}, {"type": "text", "text": "B Proof of Lemmas and Theorems 16 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Proof of Lemma 1 17   \nB.2 Proof of Lemma 2. 17   \nB.3 Proof of Theorem 1 and Corollary 1. 18 ", "page_idx": 14}, {"type": "text", "text": "C Further Justifications on Gaussian Mixture Model Classifier 19 ", "page_idx": 14}, {"type": "text", "text": "D.1 Recurring TTA Follows the Design of a Practical TTA Stream 20   \nD.2 Recurring TTA as a Diagnostic Tool . 20   \nD.3 Recurring TTA with Random Orders 20 ", "page_idx": 14}, {"type": "text", "text": "E Further Justifications on Persistent TTA (PeTTA) 21 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "E.1 Pseudo Code . 21   \nE.2 Anchor Loss . 22   \nE.3 The Use of the Memory Bank 23   \nE.4 Empirical Mean and Covariant Matrix of Feature Vectors on the Source Dataset 23   \nE.5 Novelty of PeTTA 24 ", "page_idx": 14}, {"type": "text", "text": "F Additional Experimental Results of PeTTA 24 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "F.1 Performance of PeTTA Versus Compared Methods 24   \nF.2 An Inspection of PeTTA . . 25   \nF.3 Does Model Reset Help? 25   \nF.4 PeTTA with 40 Recurring Visits . 27   \nF.5 The Sensitivity of Hyper-parameter Choices in PeTTA 27   \nF.6 More Details on the Ablation Study . . . 27   \nF.7 More Confusion Matrices in Recurring TTA Setting 29 ", "page_idx": 14}, {"type": "text", "text": "G Experimental Details 29 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "G.1 Computing Resources 29   \nG.2 Experiments on CCC Testing Stream 29   \nG.3 Test-time Adaptation Methods . 29   \nG.4 The Use of Existing Assets 30 ", "page_idx": 14}, {"type": "text", "text": "A Related Work ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Towards Robust and Practical TTA. While forming the basis, early single-target TTA approaches [53, 57, 39, 41, 33] is far from practice. Observing the dynamic of many testing environments, a continual TTA setting is proposed where an ML model continuously adapts to a sequence of multiple shifts [36, 59]. Meanwhile, recent studies [15, 7] point out that the category distribution realistic streams is highly temporally correlated. Towards real-world TTA setting, Yuan et al. [61] launch the practical TTA which considers the simultaneous occurrence of the two aforementioned challenges. ", "page_idx": 15}, {"type": "text", "text": "For a robust and gradual adaptation, an update via the mean teacher [55] mechanism is exploited in many continual TTA algorithms [59, 61, 12, 22]. To moderate the temporally correlated test stream, common approaches utilize a small memory bank for saving a category-balanced subset of testing samples [15, 61], inspired by the replay methods [50, 2] to avoid forgetting in the task of continual learning [34, 3, 11]. Our study emphasizes another perspective: beyond a supreme performance, a desirable TTA should also sustain it for an extended duration. ", "page_idx": 15}, {"type": "text", "text": "Temporal Performance Degradation. By studying the quality of various ML models across multiple industry applications [56, 60] the issue of AI \u201caging\" with the temporal model degradation progress, even with data coming from a stable process has been confirmed. In TTA, the continuous changes of model parameters through gradient descent aggravate the situation, as also recently noticed in [45]. Apart from observation, we attempt to investigate and provide theoretical insights towards the mechanism of this phenomenon. ", "page_idx": 15}, {"type": "text", "text": "Accumulated Errors in TTA. In TTA, the issue of accumulated error has been briefly acknowledged. Previous works strive to avoid drastic changes to model parameters as a good practice. Up to some degree, it helps to avoid performance degradation. Nevertheless, it is still unclear whether their effectiveness truly eliminates the risk. To preserve in-distribution performance, regularization [27, 40] or replaying of training samples at test-time [12] have been used. Other studies explore reset (recovering the initial model parameters) strategies [59, 45], periodically or upon the running entropy loss approaches a threshold [41]. Unfortunately, knowledge accumulated in the preceding steps will vanish, and a bad heuristic choice of threshold or period leads to highly frequent model resets. Noteworthy, tuning those hyper-parameters is exceedingly difficult due to the unavailability of the validation set [62]. LAME [7] suggests a post-processing step for adaptation (without updating the parameters). This approach, however, still limits the knowledge accumulation. Our PeTTA is reset-free by achieving an adaptable continual test-time training. ", "page_idx": 15}, {"type": "text", "text": "B Proof of Lemmas and Theorems ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we prove the theoretical results regarding the $\\epsilon-$ perturbed Gaussian Mixture Model Classifier ( $\\left(\\epsilon{-}\\mathrm{GMMC}\\right)$ introduced in Sec. 3.2. We first briefly summarize the definition of model collapse and the static data stream assumption: ", "page_idx": 15}, {"type": "text", "text": "Definition 1 (Model Collapse). A model is said to be collapsed from step $\\tau\\in\\mathcal{T},\\tau<\\infty$ if there exists a non-empty subset of categories $\\tilde{\\mathcal{D}}\\subset\\mathcal{D}$ such that $\\operatorname*{Pr}\\{Y_{t}\\,\\in\\,\\tilde{\\mathcal{P}}\\}\\,>\\,0$ but the marginal $\\operatorname*{Pr}\\{\\hat{Y}_{t}\\in\\tilde{\\mathcal{Y}}\\}$ converges to zero in probability: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\rightarrow\\tau}\\operatorname*{Pr}\\{\\hat{Y}_{t}\\in\\tilde{\\mathcal{Y}}\\}=0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Assumption 1 (Static Data Stream). The marginal distribution of the true label follows the same Bernoulli distribution $\\mathrm{Ber}(p_{0}){\\colon}\\,p_{0,t}=p_{0},(p_{1,t}=p_{1}=1-p_{0}),\\forall t\\in\\mathcal{T}$ . ", "page_idx": 15}, {"type": "text", "text": "Preliminary. Following the same set of notations introduced in the main text, recall that we denoted $p_{y,t}\\triangleq\\mathrm{Pr}\\{Y_{t}=y\\},\\hat{p}_{y,t}\\triangleq\\mathrm{Pr}\\{\\hat{Y}_{t}=y\\}$ (marginal distribution of the true label $Y_{t}$ and pseudo label $\\hat{Y}_{t}$ receiving label $y$ , respectively) and $\\epsilon_{t}=\\mathrm{Pr}\\{Y_{t}=1|\\hat{Y}_{t}=0\\}$ (the false negative rate (FNR) of ", "page_idx": 15}, {"type": "text", "text": "\u03f5\u2212GMMC). At testing step $t$ , we obtain the following relations: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{P_{t}}\\left[X_{t}|\\hat{Y}_{t}=0\\right]=(1-\\epsilon_{t})\\mu_{0}+\\epsilon_{t}\\mu_{1},}\\\\ &{\\quad\\mathbb{E}_{P_{t}}\\left[X_{t}|\\hat{Y}_{t}=1\\right]=\\mu_{1},}\\\\ &{\\mathrm{Var}_{P_{t}}\\left(X_{t}|\\hat{Y}_{t}=0\\right)=(1-\\epsilon_{t})\\sigma_{0}^{2}+\\epsilon_{t}\\sigma_{1}^{2}+\\epsilon_{t}(1-\\epsilon_{t})(\\mu_{0}-\\mu_{1})^{2},}\\\\ &{\\quad\\mathrm{Var}_{P_{t}}\\left(X_{t}|\\hat{Y}_{t}=1\\right)=\\sigma_{1}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In addition, under Assumption 1, the marginal distribution $P_{t}(x)$ (also referred as data distribution in our setup) is: ", "page_idx": 16}, {"type": "equation", "text": "$$\nP_{t}(x)=\\mathcal{N}(x;p_{0}\\mu_{0}+p_{1}\\mu_{1},p_{0}\\sigma_{0}^{2}+p_{1}\\sigma_{1}^{2}+p_{0}p_{1}(\\mu_{0}-\\mu_{1})^{2})\\qquad\\forall t\\in\\mathcal{T}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B.1 Proof of Lemma 1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lemma 1 (Increasing FNR). Under Assumption 1, a binary $\\epsilon$ -GMMC would collapsed (Def. 1) with $\\operatorname*{lim}_{t\\rightarrow\\tau}\\hat{p}_{1,t}=0$ (or $\\operatorname*{lim}_{t\\rightarrow\\tau}\\hat{p}_{0,t}=1$ , equivalently) $i f$ and only i $f\\operatorname*{lim}_{t\\to\\tau}\\epsilon_{t}=p_{1}$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. Under Assumption 1, we have $\\mathbb{E}_{P_{t}}\\left[X_{t}\\right]=p_{0}\\mu_{0}+(1-p_{0})\\mu_{1}$ . Also note that: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P_{t}}\\left[X_{t}\\right]=\\mathbb{E}_{P_{t}}\\left[\\mathbb{E}_{P_{t}}\\left[X_{t}|\\hat{Y}_{t}\\right]\\right]}\\\\ &{\\phantom{\\mathbb{E}_{P_{t}}}=\\mathbb{E}_{P_{t}}\\left[X_{t}|\\hat{Y}_{t}=0\\right]\\hat{p}_{0,t}+\\mathbb{E}_{P_{t}}\\left[X_{t}|\\hat{Y}_{t}=1\\right]\\hat{p}_{1,t}}\\\\ &{\\phantom{\\mathbb{E}_{P_{t}}}=[(1-\\epsilon_{t})\\mu_{0}+\\epsilon_{t}\\mu_{1}]\\,\\hat{p}_{0,t}+\\mu_{1}(1-\\hat{p}_{0,t})}\\\\ &{\\phantom{\\mathbb{E}_{P_{t}}}=[(1-\\epsilon_{t})\\hat{p}_{0,t}]\\,\\mu_{0}+[1-\\hat{p}_{0,t}(1-\\epsilon_{t})]\\,\\mu_{1}}\\\\ &{\\phantom{\\mathbb{E}_{P_{t}}}=p_{0}\\mu_{0}+(1-p_{0})\\mu_{1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the second equality follows Eqs. 9-10. Therefore: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\hat{p}_{0,t}=\\frac{p_{0}}{1-\\epsilon_{t}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Eq. 15 shows positive correlation between $\\hat{p}_{0,t}$ and $\\epsilon_{t}$ . Given $\\operatorname*{lim}_{t\\to\\tau}\\epsilon_{t}=p_{1}$ , taking the limit introduces: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\to\\tau}\\hat{p}_{0,t}=\\operatorname*{lim}_{t\\to\\tau}\\frac{p_{0}}{1-\\epsilon_{t}}=\\frac{p_{0}}{1-p_{1}}=1.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Similarly, having $\\operatorname*{lim}_{t\\rightarrow\\tau}\\hat{p}_{0,t}=1$ , the false negative rate $\\epsilon_{t}$ when $t\\rightarrow\\tau$ is: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\rightarrow\\tau}=1-p_{0}=p_{1}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since $\\hat{p}_{0,t}+\\hat{p}_{1,t}=1,\\operatorname*{lim}_{t\\to\\tau}\\hat{p}_{1,t}=0$ , equivalently. Towards the collapsing point, the model tends to predict a single label (class 0 in the current setup). In addition, the FNR of the model $\\epsilon_{t}$ also raises correspondingly. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "B.2 Proof of Lemma 2. ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lemma 2 ( $\\epsilon$ -GMMC After Collapsing). For a binary \u03f5-GMMC model, with Assumption 1, if $\\operatorname*{lim}_{t\\rightarrow\\tau}\\hat{p}_{1,t}=0$ (collapsing), the cluster $\\boldsymbol{O}$ in GMMC converges in distribution to a single-cluster GMMC with parameters: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{N}(\\hat{\\mu}_{0,t},\\hat{\\sigma}_{0,t}^{2})\\overset{d_{\\star}}{\\rightarrow}\\mathcal{N}(p_{0}\\mu_{0}+p_{1}\\mu_{1},p_{0}\\sigma_{0}^{2}+p_{1}\\sigma_{1}^{2}+p_{0}p_{1}(\\mu_{0}-\\mu_{1})^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. From Eqs. 9-10, under the increasing type II collapse of \u03f5\u2212GMMC setting, the perturbation does not affect the approximation of $\\mu_{1}$ . Meanwhile, when $\\epsilon_{t}$ increases, one can expect that $\\hat{\\mu}_{0,t}$ ", "page_idx": 16}, {"type": "text", "text": "moves further away from $\\mu_{0}$ toward $\\mu_{1}$ . Frist, the mean teacher model of GMMC (Eq. 4, main text) gives: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P_{t}}\\left[\\hat{\\mu}_{0,t}|\\hat{Y}_{t}=1\\right]=\\mathbb{E}_{P_{t-1}}\\left[\\hat{\\mu}_{0,t-1}\\right],}\\\\ &{\\mathbb{E}_{P_{t}}\\left[\\hat{\\mu}_{0,t}|\\hat{Y}_{t}=0\\right]=(1-\\alpha)\\mathbb{E}_{P_{t-1}}\\left[\\hat{\\mu}_{0,t-1}|\\hat{Y}_{t}=0\\right]+\\alpha\\mathbb{E}_{P_{t}}\\left[X_{t}|\\hat{Y}_{t}=0\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=(1-\\alpha)\\mathbb{E}_{P_{t-1}}\\left[\\hat{\\mu}_{0,t-1}\\right]+\\alpha\\left(\\mathbb{E}_{P_{t}}\\left[X_{t}|\\hat{Y}_{t}=0\\right]\\right),}\\\\ &{\\mathbb{E}_{P_{t}}\\left[\\hat{\\mu}_{1,t}|\\hat{Y}_{t}=1\\right]=(1-\\alpha)\\mathbb{E}_{P_{t-1}}\\left[\\hat{\\mu}_{1,t-1}|\\hat{Y}_{t}=1\\right]+\\alpha\\mathbb{E}_{P_{t}}\\left[X_{t}|\\hat{Y}_{t}=1\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=(1-\\alpha)\\mathbb{E}_{P_{t-1}}\\left[\\hat{\\mu}_{1,t-1}\\right]+\\alpha\\left(\\mathbb{E}_{P_{t}}\\left[X_{t}|\\hat{Y}_{t}=1\\right]\\right),}\\\\ &{\\mathbb{E}_{P_{t}}\\left[\\hat{\\mu}_{1,t}|\\hat{Y}_{t}=0\\right]=\\mathbb{E}_{P_{t-1}}\\left[\\hat{\\mu}_{1,t-1}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By defining $u_{y,t}=\\mathbb{E}_{P_{t}}\\left[\\hat{\\mu}_{y,t}\\right]$ , we obtain the following recurrence relation between $u_{0,t}$ and $u_{0,t-1}$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{u_{0,t}=\\mathbb{E}_{P_{t}}\\left[\\hat{\\mu}_{0,t}\\vert\\hat{Y}_{t}=0\\right]\\hat{p}_{0,t}+\\mathbb{E}_{P_{t}}\\left[\\hat{\\mu}_{0,t}\\vert\\hat{Y}_{t}=1\\right]\\hat{p}_{1,t}}\\\\ &{\\qquad=\\left((1-\\alpha)u_{0,t-1}+\\alpha\\mathbb{E}_{P_{t}}\\left[X_{t}\\vert\\hat{Y}_{t}=0\\right]\\right)\\hat{p}_{0,t}+u_{0,t-1}\\hat{p}_{1,t}}\\\\ &{\\qquad=\\left[(1-\\alpha)\\hat{p}_{0,t}+\\hat{p}_{1,t}\\right]u_{0,t-1}+\\alpha\\hat{p}_{0,t}\\mathbb{E}_{P_{t}}\\left[X_{t}\\vert\\hat{Y}_{t}=0\\right]}\\\\ &{\\qquad=(1-\\alpha\\hat{p}_{0,t})u_{0,t-1}+\\alpha\\hat{p}_{0,t}\\mathbb{E}_{P_{t}}\\left[X_{t}\\vert\\hat{Y}_{t}=0\\right]}\\\\ &{\\qquad=(1-\\alpha\\hat{p}_{0,t})u_{0,t-1}+\\alpha\\hat{p}_{0,t}\\left[(1-\\epsilon_{t})\\mu_{0}+\\epsilon_{t}\\mu_{1}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Given $\\operatorname*{lim}_{t\\rightarrow\\tau}\\hat{p}_{0,t}=1$ , it follows that $\\operatorname*{lim}_{t\\to\\tau}\\epsilon_{0,t}=p_{1}$ by Lemma 1. From this point: ", "page_idx": 17}, {"type": "equation", "text": "$$\nu_{0,t}=(1-\\alpha)u_{0,t-1}+\\alpha\\left(p_{0}\\mu_{0}+p_{1}\\mu_{1}\\right)\\qquad\\forall t>\\tau.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Taking the limit $t\\to\\infty$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{lim}_{t\\to\\infty}u_{0,t}=\\operatorname*{lim}_{t\\to\\infty}(1-\\alpha)u_{0,t-1}+\\alpha\\left(p_{0}\\mu_{0}+p_{1}\\mu_{1}\\right)}\\\\ &{\\qquad\\qquad=\\displaystyle\\operatorname*{lim}_{t\\to\\infty}(1-\\alpha)^{t}\\hat{\\mu}_{0,0}+\\alpha\\sum_{i=1}^{t}(1-\\alpha)^{i-1}\\left(p_{0}\\mu_{0}+p_{1}\\mu_{1}\\right)}\\\\ &{\\displaystyle\\qquad=\\operatorname*{lim}_{t\\to\\infty}(1-\\alpha)^{t}\\hat{\\mu}_{0,0}+(1-(1-\\alpha)^{t})(p_{0}\\mu_{0}+p_{1}\\mu_{1})}\\\\ &{\\displaystyle=p_{0}\\mu_{0}+p_{1}\\mu_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The second equation is obtained by solving the recurrence relation. When $\\operatorname*{lim}_{t\\rightarrow\\tau}\\hat{p}_{0,t}=1,\\{\\hat{\\mu}_{y,t}\\}_{y\\in\\{0,1\\}}$ becomes a deterministic values. Hence, giving $u_{y,t}=\\mathbb{E}_{P_{t}}\\left[\\hat{\\mu}_{y,t}\\right]=\\hat{\\mu}_{0,t}(\\forall t>\\tau)$ and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\to\\infty}\\hat{\\mu}_{0,t}=\\operatorname*{lim}_{t\\to\\infty}u_{0,t}=p_{0}\\mu_{0}+p_{1}\\mu_{1}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Repeating the steps above with Eqs. 11-12 in place of Eqs. 9-10, we obtain a similar result for $\\sigma_{0,t}^{2}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\to\\infty}\\hat{\\sigma}_{0,t}^{2}=p_{0}\\sigma_{0}^{2}+p_{1}\\sigma_{1}^{2}+p_{0}p_{1}(\\mu_{0}-\\mu_{1})^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By L\u00e9vy\u2019s continuity theorem (p. 302, [42]), from Eqs. 17-18, when $t~\\rightarrow~\\infty$ , the estimated distribution of the first cluster $\\mathcal{N}(x;\\hat{\\mu}_{0,t}\\hat{\\sigma}_{0,t}^{2})$ converges to the whole data distribution $P_{t}(x)$ (Eq. 13) when collapsing. ", "page_idx": 17}, {"type": "text", "text": "B.3 Proof of Theorem 1 and Corollary 1. ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Theorem 1 (Convergence of \u03f5\u2212GMMC). For a binary $\\epsilon$ -GMMC model, with Assumption $^{\\,l}$ , let the distance from $\\hat{\\mu}_{0,t}$ toward $\\mu_{1}$ is $d_{t}^{0\\to1}=|\\mathbb{E}_{P_{t}}\\left[\\hat{\\mu}_{0,t}\\right]-\\dot{\\mu}_{1}|$ , then: ", "page_idx": 17}, {"type": "equation", "text": "$$\nd_{t}^{0\\to1}-d_{t-1}^{0\\to1}\\leq\\alpha\\cdot p_{0}\\cdot\\bigg(|\\mu_{0}-\\mu_{1}|-\\frac{d_{t-1}^{0\\to1}}{1-\\epsilon_{t}}\\bigg)\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. Substituting Eq. 15 into $\\hat{p}_{0,t}$ of Eq. 16 gives: ", "page_idx": 18}, {"type": "equation", "text": "$$\nu_{0,t}=\\left(1-\\frac{\\alpha p_{0}}{1-\\epsilon_{t}}\\right)u_{0,t-1}+\\frac{\\alpha p_{0}}{1-\\epsilon_{t}}\\left[(1-\\epsilon_{t})\\mu_{0}+\\epsilon_{t}\\mu_{1}\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Hence, we have the distance from $u_{0,t}$ toward $\\mu_{1}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle u_{0,t}-\\mu_{1}\\big|=\\left|\\left(1-\\frac{\\alpha p_{0}}{1-\\epsilon_{t}}\\right)u_{0,t-1}+\\alpha p_{0}\\mu_{0}+\\frac{\\alpha p_{0}\\epsilon_{t}\\mu_{1}}{1-\\epsilon_{t}}-\\mu_{1}\\right|}\\\\ {\\displaystyle\\qquad=\\left|\\left(1-\\frac{\\alpha p_{0}}{1-\\epsilon_{t}}\\right)\\left(u_{0,t-1}-\\mu_{1}\\right)+\\alpha p_{0}\\mu_{0}+\\frac{\\alpha p_{0}\\epsilon_{t}\\mu_{1}}{1-\\epsilon_{t}}-\\frac{\\alpha p_{0}\\mu_{1}}{1-\\epsilon_{t}}\\right|}\\\\ {\\displaystyle\\qquad=\\left|\\left(1-\\frac{\\alpha p_{0}}{1-\\epsilon_{t}}\\right)\\left(u_{0,t-1}-\\mu_{1}\\right)+\\alpha p_{0}\\mu_{0}-\\frac{\\alpha p_{0}\\mu_{1}(1-\\epsilon_{t})}{1-\\epsilon_{t}}\\right|}\\\\ {\\displaystyle\\qquad=\\left|\\left(1-\\frac{\\alpha p_{0}}{1-\\epsilon_{t}}\\right)\\left(u_{0,t-1}-\\mu_{1}\\right)+\\alpha p_{0}\\big(\\mu_{0}-\\mu_{1}\\big)\\right|}\\\\ {\\displaystyle\\qquad\\leq\\left(1-\\frac{\\alpha p_{0}}{1-\\epsilon_{t}}\\right)\\left|u_{0,t-1}-\\mu_{1}\\right|+\\alpha p_{0}\\big|\\mu_{0}-\\mu_{1}\\big|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The last inequality holds due to the triangle inequality. Equivalently, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left|u_{0,t}-\\mu_{1}\\right|-\\left|u_{0,t-1}-\\mu_{1}\\right|\\leq\\alpha\\cdot p_{0}\\cdot\\left(\\left|\\mu_{0}-\\mu_{1}\\right|-\\frac{\\left|u_{0,t-1}-\\mu_{1}\\right|}{1-\\epsilon_{t}}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Let $d_{t}^{0\\to1}=|\\mathbb{E}_{P_{t}}\\left[\\hat{\\mu}_{0,t}\\right]-\\mu_{1}|$ , we conclude that: ", "page_idx": 18}, {"type": "equation", "text": "$$\nd_{t}^{0\\to1}-d_{t-1}^{0\\to1}\\leq\\alpha\\cdot p_{0}\\cdot\\bigg(|\\mu_{0}-\\mu_{1}|-\\frac{d_{t-1}^{0\\to1}}{1-\\epsilon_{t}}\\bigg)\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Corollary 1 (A Condition for $\\epsilon$ \u2212GMMC Collapse). With fixed $p_{0},\\,\\alpha,\\mu_{0},\\mu_{1}$ , \u03f5\u2212GMMC is collapsed if there exists a sequence of $\\{\\epsilon_{t}\\}_{\\tau-\\Delta_{\\tau}}^{\\tau}\\,(\\tau\\geq\\Delta_{\\tau}>0)$ such that: ", "page_idx": 18}, {"type": "equation", "text": "$$\np_{1}\\geq\\epsilon_{t}>1-\\frac{d_{t-1}^{0\\rightarrow1}}{\\vert\\mu_{0}-\\mu_{1}\\vert},\\quad t\\in[\\tau-\\Delta_{\\tau},\\tau].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Initialized at $\\mu_{0}$ , $\\epsilon$ -GMMC is collapsing when $\\hat{\\mu}_{0,t}$ converges to the mid-point $p_{0}\\mu_{0}+p_{1}\\mu_{1}$ (Lemma 2), i.e., moving closer to $\\mu_{1}$ . From Thm. 1, the distance towards $\\mu_{1}\\ d_{t}^{0\\to1}<d_{t-1}^{0\\to1}$ if ", "page_idx": 18}, {"type": "equation", "text": "$$\n|\\mu_{0}-\\mu_{1}|-\\frac{|u_{0,t-1}-\\mu_{1}|}{1-\\epsilon_{t}}<0\\Leftrightarrow|\\mu_{0}-\\mu_{1}|<\\frac{|u_{0,t-1}-\\mu_{1}|}{1-\\epsilon_{t}}\\Leftrightarrow\\epsilon_{t}>1-\\frac{|u_{0,t-1}-\\mu_{1}|}{|\\mu_{0}-\\mu_{1}|}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "When there exists this sequence $\\{\\epsilon_{t}\\}_{\\tau-\\Delta_{\\tau}}^{\\tau}$ $\\quad\\tau\\geq\\Delta_{\\tau}>0)$ it follows that $d_{t}^{0\\to1}<d_{t-1}^{0\\to1}$ and $\\epsilon_{t}>\\epsilon_{t-1}$ is guaranteed $\\forall t\\in[\\tau-\\Delta_{\\tau},\\tau]$ . Hence, $\\operatorname*{lim}_{t\\to\\tau}\\epsilon_{t}=p_{1}$ (model collapsed, by Lemma 1). \u53e3 ", "page_idx": 18}, {"type": "text", "text": "C Further Justifications on Gaussian Mixture Model Classifier ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "One may notice that in $\\epsilon$ -GMMC (Sec. 4.2), the classifier is defined $f_{t}(x)=\\operatorname{argmax}_{y\\in\\mathcal{y}}\\operatorname*{Pr}(x|y;\\theta_{t})$ (maximum likelihood estimation) while in general, $f_{t}(x)=\\operatorname*{argmax}_{y\\in\\mathcal{y}}\\operatorname*{Pr}(y|x;\\theta_{t})$ (maximum a posterior estimation), parameterized by a neural network. In this case, since the equal prior (i.e., $\\operatorname*{Pr}(y;\\theta_{t})=\\operatorname*{Pr}(y^{\\prime};\\theta_{t}),\\forall y,y^{\\prime}\\in\\mathcal{C})$ is enforced in $\\epsilon$ -GMMC, the two definitions are equivalent. ", "page_idx": 18}, {"type": "text", "text": "Proof. Having: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{argmax}_{y\\in\\mathcal{Y}}\\operatorname*{Pr}(y|x;\\theta_{t})=\\operatorname*{argmax}_{y\\in\\mathcal{Y}}\\frac{\\operatorname*{Pr}\\left(x\\mid y;\\theta_{t}\\right)\\operatorname*{Pr}\\left(y;\\theta_{t}\\right)}{\\sum_{y^{\\prime}\\in\\mathcal{Y}}\\operatorname*{Pr}\\left(x\\mid y^{\\prime};\\theta_{t}\\right)\\operatorname*{Pr}\\left(y^{\\prime};\\theta_{t}\\right)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\operatorname*{argmax}_{y\\in\\mathcal{Y}}\\operatorname*{Pr}(x\\mid y;\\theta_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We conclude that the two definitions are equivalent. In fact, it is well-known that maximum likelihood estimation is a special case of maximum a posterior estimation when the prior is uniform. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "D Further Justifications on the Recurring Testing Scenario ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "D.1 Recurring TTA Follows the Design of a Practical TTA Stream ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Note that in recurring TTA, besides the recurrence of environments (or corruptions) as in [59, 40], the distribution of class labels is also temporally correlated (non-i.i.d.) as suggested by [15, 61] to reflect the practical testing stream better. In short, recurring TTA is formed by recurring the environments of practical TTA scenario introduced in [61] multiple times (readers are encouraged to visit the original paper for additional motivations on this scenario). ", "page_idx": 19}, {"type": "text", "text": "D.2 Recurring TTA as a Diagnostic Tool ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Noticeably, CoTTA [59] also performed 10-round repetition across multiple domain shifts to simulate a lifelong TTA testing stream just like our recurring TTA. However, the key difference is CoTTA assumes the distribution of class labels is i.i.d., which does not hold in many real-life testing scenarios as argued in [15, 61]. Our recurring TTA lifts this assumption and allows temporally correlated (non-i.i.d.) label distribution (more challenging, more practical). This extension allows recurring TTA to spot the risk of model collapse on CoTTA [59] and other methods. The over-simplicity of the repeating scheme in CoTTA for spotting performance degradation is also suggested in [45]. Clearly, it seems not to be a problem at first glance in Tab. 5 of [59] (CoTTA\u2019s 10-round repetition), but in fact, the risk in CoTTA remains, as explored in our scenario and also on CCC [45]. ", "page_idx": 19}, {"type": "text", "text": "The construction of our recurring TTA is notably simple - a technical effort to extend the testing stream. However, this simplicity is on purpose, serving as a diagnostic tool for lifelong continual TTA. Counterintuitively, our experiments on four different tasks with the latest methods verify that even if the model is exposed to the same environment (the most basic case), their adaptability and performance are still consistently reduced (demonstrated visually in Fig. 1, quantitatively in Sec. 5.3). ", "page_idx": 19}, {"type": "text", "text": "We believe that the extensive testing stream by recurrence in our setup is a simple yet sufficient scenario to demonstrate the vulnerability of existing continual TTA methods when facing the issue of model collapse (compared to CCC [45], a notably more complicated scenario than our recurring TTA). Indeed, recurring shifts are sufficient to show this failure mode and any lifelong TTA method should necessarily be able to handle recurring conditions. ", "page_idx": 19}, {"type": "text", "text": "D.3 Recurring TTA with Random Orders ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Recall that in Sec. 3.1, recurring TTA is constructed by repeating the same sequence of $D$ distributions $K$ times. For example, a sequence with $K\\,=\\,2$ could be $\\mathcal{P}_{1}\\,\\rightarrow\\,\\mathcal{P}_{2}\\,\\rightarrow\\,\\dots\\,\\rightarrow\\,\\mathcal{P}_{D}\\,\\rightarrow\\,\\mathcal{P}_{1}\\,\\rightarrow$ $\\mathcal{P}_{2}\\rightarrow\\cdot\\cdot\\cdot\\rightarrow\\mathcal{P}_{D}$ . For simplicity and consistency that promote reproducibility, the same order of image corruptions (following [61]) is used for all recurrences. This section presents supplementary experimental findings indicating that the order of image corruptions within each recurrence, indeed, does not affect the demonstration of TTA model collapse and the performance of our PeTTA. ", "page_idx": 19}, {"type": "text", "text": "Experiment Setup. We refer to the setting same-order as using one order of image corruptions in [61] for all recurrences (specifically, on CIFAR-10/100-C and ImageNet-C: motion $\\rightarrow s n o w\\rightarrow$ $f o g\\rightarrow s h o t\\rightarrow d e f o c u s\\rightarrow c o n t r a s t\\rightarrow z o o m\\rightarrow b r$ $i g h t n e s s\\rightarrow f r o s t\\rightarrow e l a s t i c\\rightarrow g l a s s\\rightarrow g a u s s i a n$ $\\rightarrow p i x e l a t e d\\rightarrow j p e g\\rightarrow i m p u l s e)$ . Conversely, in random-order, the order of image corruptions is randomly shuffled at the beginning of each recurrence. Hence, the corruption orders across $K$ recurrences are now entirely different. We redo the experiment of the second setting three times (with different random $\\mathrm{;eeds=0,1,2}$ ). Nevertheless, different TTA methods are ensured to be evaluated on the same testing stream, since it is fixed after generation. Without updating its parameters, the performance of the source model is trivially independent of the order of corruptions. ", "page_idx": 19}, {"type": "text", "text": "Experimental Result. The experimental results are visualized in Fig. 6. The first column plots the experiments under the same-order, while the remaining three columns plot the experiments in the random-order setting, with varying random seeds. Note that the message conveyed by each sub-figure entirely matches that of Fig. 1-right. ", "page_idx": 19}, {"type": "text", "text": "Discussions. Clearly, a similar collapsing pattern is observed in all three TTA tasks, with three combinations of 20 image corruption orders. This pattern also matches the easiest setting using the same order of image corruptions we promoted in recurring TTA. ", "page_idx": 19}, {"type": "image", "img_path": "ffeUBoTcdS/tmp/9ee8156e2de48208b5ce621413534bc65c1796c772c2f9d96caedacfb074a476.jpg", "img_caption": ["Figure 6: Recurring TTA with different order of corruptions. This figure plots the testing error of two TTA approaches: RoTTA - - [61], and, PeTTA- - (ours), and source model- $\\cdot\\times$ - as a reference performance under our recurring TTA (with 20 visits) across three TTA tasks. On the same-order experiments (column 1), the same order of image corruptions is applied for all 20 visits. Meanwhile, in random-order, this order is reshuffled at the beginning of each visit (columns 2-4). Random-order experiments are redone three times with different random seeds. Here, we empirically validate that using the same order of domain shifts (image corruptions) in our recurring TTA is sufficient to showcase the model collapse and evaluate the persistence of our PeTTA. Best viewed in color. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "E Further Justifications on Persistent TTA (PeTTA) ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "E.1 Pseudo Code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We summarize the key steps of our proposed PeTTA in Alg. 1, with the key part (lines 4-13) highlighted in blue. Our approach fits well in the general workflow of a TTA algorithm, enhancing the regular mean-teacher update step. Appdx. E.5 elaborates more on our contributions in PeTTA, distinguishing them from other components proposed in previous work. The notations and definitions of all components follow the main text (described in detail in Sec. 4). On line 8 of Alg. 1, as a ", "page_idx": 20}, {"type": "text", "text": "Algorithm 1 Persistent TTA (PeTTA) ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Input: Classification model $f_{t}$ and its deep feature extractor $\\phi_{\\theta_{t}}$ , both parameterized by $\\overline{{\\theta_{t}\\ \\in\\ \\Theta}}$ . Testing stream $\\{X_{t}\\}_{t=0}^{T}$ , initial model parameter $\\left(\\theta_{0}\\right)$ , initial update rate $\\left(\\alpha_{0}\\right)$ , regularization term coefficient $\\left(\\lambda_{0}\\right)$ , empirical mean $(\\bar{\\{\\mu_{0}^{y}\\}}_{y\\in\\mathcal{D}})$ and covariant matrix $\\big(\\{\\Sigma_{0}^{y}\\}_{y\\in\\mathcal{D}}\\big)$ of feature vectors in the training set, $\\hat{\\pmb{\\mu}}_{t}^{y}$ EMA update rate $(\\nu)$ . ", "page_idx": 21}, {"type": "text", "text": "1 $\\hat{\\pmb{\\mu}}_{0}^{y}\\leftarrow\\pmb{\\mu}_{0}^{y},\\forall y\\in\\mathcal{y}$ ; // Initialization   \n2 for $t\\in[1,\\cdot\\cdot\\cdot\\,,T]$ do   \n3 $\\hat{Y}_{t}\\gets f_{t-1}(X_{t})$ ; // Obtaining pseudo-labels for all samples in $X_{t}$   \n4 // Persistent TTA (PeTTA)   \n5 $\\hat{\\mathcal{V}}_{t}\\gets\\left\\{\\hat{Y}_{t}^{(i)}|i=1,\\cdots\\,,N_{t}\\right\\}$ ; // Set of (unique) pseudo-labels in $X_{t}$   \n6 $\\bar{\\gamma}_{t}\\gets0$ ;   \n7 for $y\\in\\hat{\\mathcal{V}}_{t}$ do   \n8 $\\begin{array}{r}{\\gamma_{t}^{y}\\gets1-\\exp\\left(-(\\hat{\\mu}_{t}^{y}-\\mu_{0}^{y})^{T}\\left(\\Sigma_{0}^{y}\\right)^{-1}(\\hat{\\mu}_{t}^{y}-\\mu_{0}^{y})\\right)\\,;}\\end{array}$ ; // Divergence sensing term   \non category $y$   \n9 $\\bar{\\gamma}_{t}\\leftarrow\\bar{\\gamma}_{t}+\\frac{\\gamma_{t}^{y}}{|\\hat{\\mathcal{D}}_{t}|}$ // Average divergence sensing term for step $t$   \n10 $\\hat{\\mu}_{t}^{y}\\gets(1-\\nu)\\hat{\\mu}_{t-1}^{y}+\\nu\\phi_{\\theta_{t-1}}(X_{t}|\\hat{Y}_{t}=y)$ ; // EMA update of $\\hat{\\pmb{\\mu}}_{t}^{y}$ for samples with   \nY\u02c6t = y   \n11 end   \n12 $\\lambda_{t}\\leftarrow\\bar{\\gamma}_{t}\\cdot\\lambda_{0}$ ; // Computing adaptive regularization term coefficient   \n13 $\\alpha_{t}\\gets\\left(1-\\bar{\\gamma}_{t}\\right)\\cdot\\alpha_{0}$ ; // Computing adaptive update rate   \n14 // Regular Mean-teacher Update   \n15 $\\theta_{t}^{\\prime}\\gets\\mathsf{0p t i m}\\,\\mathbb{E}_{P_{t}}\\left[\\mathcal{L}_{\\mathrm{CLS}}\\left(\\hat{Y}_{t},X_{t};\\theta^{\\prime}\\right)+\\mathcal{L}_{\\mathrm{AL}}\\left(X_{t};\\theta^{\\prime}\\right)\\right]+\\lambda_{t}\\mathcal{R}(\\theta^{\\prime})\\;;$ // Student model   \nupdate   \n6 $\\widehat{\\theta_{t}\\,\\Big\\,\\leftarrow\\,}(1-\\alpha_{t})\\theta_{t-1}+\\alpha_{t}\\theta_{t}^{\\prime}.$ ; // Teacher model update   \n17 // Final prediction   \n8 yeild $f_{t}(\\bar{X}_{t})$ ; // Returning the final inference with updated model $f_{t}$   \n19 end ", "page_idx": 21}, {"type": "text", "text": "shorthand notation, $\\phi_{\\theta_{t-1}}(X_{t}|\\hat{Y}_{t}\\,=\\,y)$ denotes the empirical mean of all feature vectors of $X_{t}^{(i)}$ (extracted by $\\phi_{\\theta_{t-1}}\\left(X_{t}^{(i)}\\right))$ if $\\hat{Y}_{t}^{(i)}=y,i=1,\\cdots\\,,N_{t}$ in the current testing batch. ", "page_idx": 21}, {"type": "text", "text": "E.2 Anchor Loss ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "KL Divergence Minimization-based Interpretation of Anchor Loss. In Sec. 4, we claimed that minimizing the anchor loss $\\mathcal{L}_{\\mathrm{AL}}$ is equivalent to minimizing the relative entropy (or KL divergence) between the output probability of two models parameterized by $\\theta_{0}$ and $\\theta$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. Having: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{K L}\\left(\\operatorname*{Pr}(y|X_{t};\\theta_{0})\\vert\\vert\\operatorname*{Pr}(y\\vert X_{t};\\theta)\\right)=\\underset{y\\in\\mathcal{Y}}{\\sum}\\operatorname*{Pr}(y|X_{t};\\theta_{0})\\log\\frac{\\operatorname*{Pr}(y\\vert X_{t};\\theta_{0})}{\\operatorname*{Pr}(y\\vert X_{t};\\theta)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\underset{y\\in\\mathcal{Y}}{\\underbrace{-\\sum_{y\\in\\mathcal{Y}}\\operatorname*{Pr}(y\\vert X_{t};\\theta_{0})\\log\\operatorname*{Pr}(y\\vert X_{t};\\theta)}}-\\underset{\\mathrm{constant}}{\\underbrace{H(\\operatorname*{Pr}(y\\vert X_{t};\\theta_{0}))}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Hence, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{argmin}_{\\theta\\in\\Theta}\\mathcal{L}_{\\mathrm{AL}}(X_{t};\\theta)=\\operatorname*{argmin}_{\\theta\\in\\Theta}D_{K L}\\left(\\operatorname*{Pr}(y|X_{t};\\theta_{0})||\\operatorname*{Pr}(y|X_{t};\\theta)\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Intuitively, a desirable TTA solution should be able to adapt to novel testing distributions on the one hand, but it should not significantly diverge from the initial model. $\\mathcal{L}_{\\mathrm{AL}}$ ftis this purpose, constraining the KL divergence between two models at each step. ", "page_idx": 22}, {"type": "text", "text": "Connections between Anchor Loss and Regularizer Term. While supporting the same objective (collapse prevention by avoiding the model significantly diverging from the source model), the major difference between Anchor loss $(\\mathcal{L}_{\\mathrm{AL}})$ and the Regularizer term $({\\mathcal{R}}(\\theta))$ is that the anchor loss operates on the probability space of model prediction while the regularizer term works on the model parameter spaces. Tab. 4 (lines 1 and 5) summarizes the ablation study when each of them is eliminated. We see the role of the regularization term is crucial for avoiding model collapse, while the anchor loss guides the adaptation under the drastic domain shift. Nevertheless, fully utilizing all components is suggested for maintaining TTA persistence. ", "page_idx": 22}, {"type": "text", "text": "E.3 The Use of the Memory Bank ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The size of Memory Bank. The size of the memory bank in PeTTA is relatively small, equal to the size of one mini-batch for update (64 images, specifically). ", "page_idx": 22}, {"type": "text", "text": "The Use of the Memory Bank in PeTTA is Fair with Respect To the Compared Methods. Our directly comparable method - RoTTA [61] also takes this advantage (referred to as category-balanced sampling, Sec. 3.2 of [61]). Hence, the comparison between PeTTA and RoTTA is fair in terms of additional memory usage. Noteworthy, the use of a memory bank is a common practice in TTA literature (e.g., [15, 8, 61]), especially in situations where the class labels are temporally correlated or non-i.i.d. distributed (as we briefly summarized in Appdx. A - Related Work section). CoTTA [59], EATA [40] and MECTA [22] (compared method) assume labels are i.i.d. distributed. Hence, a memory bank is unnecessary, but their performance under temporally correlated label distribution has dropped significantly as a trade-off. The RMT [12] (compared method) does not require a memory bank but it needs to cache a portion of the source training set for replaying (Sec. 3.3 in [12]) which even requires more resources than the memory bank. ", "page_idx": 22}, {"type": "text", "text": "Eliminating the Need for a Memory Bank. As addressing the challenge of temporally correlated label distribution on the testing stream is not the focus of PeTTA, we have conveniently adopted the use of the memory bank proposed in [61]. Since this small additional memory requirement is not universally applied in every real-world scenario, we believe that this is a reasonable assumption, and commonly adopted in TTA practices. Nevertheless, exploring alternative ways for reducing the memory size (e.g., storing the embedded features instead of the original image) would be an interesting future direction. ", "page_idx": 22}, {"type": "text", "text": "E.4 Empirical Mean and Covariant Matrix of Feature Vectors on the Source Dataset ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Two Ways of Computing $\\mu_{0}^{y}$ and $\\Sigma_{0}^{y}$ in Practice. One may notice that in PeTTA, computing $\\gamma_{t}^{y}$ requires the pre-computed empirical mean $(\\mu_{0}^{y})$ and covariance $(\\Sigma_{0}^{y})$ of the source dataset. This requirement may not be met in real-world situations where the source data is unavailable. In practice, the empirical mean and covariance matrix computed on the source distribution can be provided in the following two ways: ", "page_idx": 22}, {"type": "text", "text": "1. Most ideally, these values are computed directly by inference on the entire training set once the model is fully trained. They will be provided alongside the source-distribution pre-trained model as a pair for running TTA.   \n2. With only the source pre-trained model available, assume we can sample a set of unlabeled data from the source distribution. The (pseudo) labels for them are obtained by inferring from the source model. Since the source model is well-performed in this case, using pseudo is approximately as good as the true label. ", "page_idx": 22}, {"type": "text", "text": "Accessing the Source Distribution Assumption in TTA. In fact, the second way is typically assumed to be possible in previous TTA methods such as EATA [40], and MECTA [22] (a compared method) to estimate a Fisher matrix (for anti-forgetting regularization purposes). Our work - PeTTA follows the same second setup as the previous approaches mentioned above. A variation of RMT [12] (a compared method) approach even requires having the fully labeled source data available at test-time for source replaying (Sec. 3.3 of [12]). This variation is used for comparison in our experiments. ", "page_idx": 22}, {"type": "text", "text": "We believe that having the empirical mean and covariant matrix pre-computed on a portion of the source distribution in PeTTA is a reasonable assumption. Even in the ideal way, revealing the statistics might not severely violate the risk of data privacy leakage or require notable additional computing resources. ", "page_idx": 23}, {"type": "text", "text": "Number of Samples Needed for Computation. To elaborate more on the feasibility of setting (2) mentioned above, we perform a small additional experiment on the performance of PeTTA while varying the number of samples used for computing the empirical mean and covariant matrix on the source distribution. In this setting, we use the test set of CIFAR-10, CIFAR-100, DomainNet validation set of ImageNet (original images, without corruption, or the real domain test set of DomainNet), representing samples from the source distribution. The total number of images is 10, 000 in CIFAR-10/A00, 50, 000 in ImageNet, and 69, 622 in DomainNet. We randomly sample $25\\%$ , $50\\%$ , $75\\%$ , and $100\\%$ of the images in this set to run PeTTA for 20 rounds of recurring. The result is provided in Tab. 6 below. ", "page_idx": 23}, {"type": "table", "img_path": "ffeUBoTcdS/tmp/ffe427fc389a28d2a4183589cf4786b511f67122fc49158c4a2d0bed7846af3c.jpg", "table_caption": ["Table 6: Average classification error of PeTTA (across 20 visits) with varying sizes of source samples used for computing feature empirical mean $(\\mu_{0}^{y})$ and covariant matrix $(\\Sigma_{0}^{y})$ . "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "The default choice of PeTTA is using $100\\%$ samples of the validation set of the source dataset. However, we showcase that it is possible to reduce the number of unlabeled samples from the source distribution to compute the empirical mean and covariant matrix for PeTTA, without significantly impacting its performance. ", "page_idx": 23}, {"type": "text", "text": "E.5 Novelty of PeTTA ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "PeTTA is composed of multiple components. Among them, the anchor loss is an existing idea (examples of previous work utilizing this idea are [32, 12]). Similarly, the mean-teacher update; and regularization are well-established techniques and very useful for the continual or gradual TTA scenario. Hence, we do not aim to improve or alternate these components. ", "page_idx": 23}, {"type": "text", "text": "Nevertheless, the novelty of our contribution is the sensing of the divergence and adaptive model update, in which the importance of minimizing the loss (adaptation) and regularization (collapse prevention) is changed adaptively. In short, we propose a harmonic way of combining those elements adaptively to achieve a persistent TTA process. ", "page_idx": 23}, {"type": "text", "text": "The design of PeTTA draws inspiration from a theoretical analysis (Sec. 3.2), empirically surpassing both the conventional reset-based approach [45] (Appdx. F.3) and other continual TTA approaches [61, 12, 59, 22, 7] on our proposed recurring TTA (Sec. 3.1, Appdx. F.1), as well as the previously established CCC [45] benchmark. ", "page_idx": 23}, {"type": "text", "text": "F Additional Experimental Results of PeTTA ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "F.1 Performance of PeTTA Versus Compared Methods ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Performance on CIFAR-100-C and Domainnet Datasets. Due to the length constraint, the classification errors on the tasks CIFAR- $100\\rightarrow$ CIFAR-100-C, and real $\\rightarrow$ clipart, painting, sketch of DomainNet are provided in Tab. 7 and Tab. 8. To prevent model collapse, the adaptability of PeTTA is more constrained. As a result, it requires more time for adaptation initially (e.g., in the first visit) but remains stable thereafter. Generally, consistent trends and observations are identified across all four TTA tasks. ", "page_idx": 23}, {"type": "text", "text": "Standard Deviation of PeTTA Performance Across Multiple Runs. For PeTTA experiments marked with $({^*})$ in Tab. 1, Tab. 2, Tab. 7, and Tab. 8, the average performance across five independent runs with different random seeds is reported. Due to the space constraint, the corresponding standard deviation values are now reported in Tab. 9. Generally, the average standard deviation across runs ", "page_idx": 23}, {"type": "text", "text": "Table 7: Average classification error of the task CIFAR- $100\\rightarrow$ CIFAR-100-C in recurring TTA scenario. The lowest error is highlighted in bold, (\u2217)average value across 5 runs (different random seeds) is reported for PeTTA. ", "page_idx": 24}, {"type": "table", "img_path": "ffeUBoTcdS/tmp/a4737f39bdcefe1f868acdd5454f302db904fa862a153a22856fb9ff73a75406.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "ffeUBoTcdS/tmp/e0b9ff0acb84244c28b4f3adec0c0b6a126189b4438141bf475f0b5ad87687ac.jpg", "table_caption": ["Table 8: Average classification error of the task real $\\rightarrow$ clipart $\\rightarrow$ painting $\\rightarrow$ sketch on DomainNet dataset in recurring TTA scenario. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "stays within $\\pm0.1\\%$ for small datasets (CIFAR-10-C, CIFAR-100-C) and $\\pm0.5\\%$ for larger datasets (ImageNet-C, DomainNet). ", "page_idx": 24}, {"type": "text", "text": "Table 9: Mean and standard deviation classification error of PeTTA on the four datasets: CIFAR-10-C (CF-10-C), CIFAR-100-C (CF-100-C), DomainNet (DN), and ImageNet-C (IN-C) with recurring TTA scenario. Each experiment is run 5 times with different random seeds. ", "page_idx": 24}, {"type": "table", "img_path": "ffeUBoTcdS/tmp/2b3f755c7b90a5d0bfd33320825d9c9d2c7dbbcb67034cfbff74f17fadfb93ab.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "F.2 An Inspection of PeTTA ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In Fig. 7, we showcase an inspection of our PeTTA on the task CIFAR- $10\\rightarrow$ CIFAR-10-C [19] in a typical recurring TTA with 20 visits. Specifically, the visualizations of PeTTA parameters $(\\bar{\\gamma}_{t}$ , $\\lambda_{t}$ , and $\\alpha_{t}$ ), adaptation losses $(\\mathcal{L}_{\\mathrm{CLS}},\\mathcal{L}_{\\mathrm{AL}})$ and regularization term $({\\mathcal{R}}(\\theta))$ are provided. Here, we observe the values of adaptive parameters $\\lambda_{t}$ and $\\alpha_{t}$ continuously changing through time, as the testing scenarios evolve during recurring TTA. This proposed mechanism stabilizes the value of the loss functions, and regularization term, balancing between the two primary objectives: adaptation and preventing model collapse. Thus, the error rate persists as a result. A similar pattern is observed on other datasets (CIFAR-100-C [19] and DomainNet [44]). ", "page_idx": 24}, {"type": "text", "text": "F.3 Does Model Reset Help? ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Experiment Setup. We use the term \u201cmodel reset\u201d to represent the action of \u201creverting the current TTA model to the source model\u201d. This straightforward approach is named RDumb [45]. We thoroughly conducted experiments to compare the performance of RDumb with PeTTA. The implementation of RDumb in this setting is as follows. We employ RoTTA [61] as the base test-time adaptor due to the characteristics of the practical TTA [61] stream. The model (including model parameters, the optimizer state, and the memory bank) is reset after adapting itself to $T$ images.1For each dataset, three values of this hyper-parameter $T$ are selected: ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "\u2022 $T=1,000$ : This is the value selected by the RDumb\u2019s authors [45]. Unless specifically stated, we use this value when reporting the performance of RDumb [45] in all other tables.   \n\u2022 $T\\,=\\,10,000$ (CIFAR-10/100-C), $T\\,=\\,5,000$ (ImageNet-C) and $T\\,=\\,24,237$ (DomainNet).2 This value is equal to the number of samples in the test set of a single corruption type, i.e., the model is reset exactly after visiting each $\\mathcal{P}_{i}$ \u2019s (see Sec. 3.1 for notations). For DomainNet [44], since the number of images within each domain is unequal, the average number of images is used instead.   \n\u2022 $T=150,000$ (CIFAR-10/100-C), $T=75,000$ (ImageNet-C) and $T=72$ , 712 (DomainNet). This number is equal to the number of samples in one recurrence of our recurring TTA, i.e., the model is reset exactly after visiting $\\mathcal{P}_{1}\\rightarrow\\cdot\\cdot\\cdot\\rightarrow\\mathcal{P}_{D}$ . Here, $D=15$ - types of corruptions [19] for CIFAR-10/100-C and ImageNet-C and $D=3$ for DomainNet (clipart, painting, sketch). For example, the model is reset 20 times within a recurring TTA setting with 20 recurrences under this choice of $T$ . ", "page_idx": 25}, {"type": "text", "text": "The second and the last reset scheme could be interpreted as assuming the model has access to an oracle model with a capability of signaling the transitions between domains, or recurrences. Typically, this is an unrealistic capability in real-world scenarios, and a desirable continual TTA algorithm should be able to operate independently without knowing when the domain shift happening. ", "page_idx": 25}, {"type": "text", "text": "Experimental Results. An empirical comparison between RDumb [45] and our PeTTA are reported in Tab. 10, Tab. 11, Tab. 12 and Tab. 13 for all four tasks. ", "page_idx": 25}, {"type": "table", "img_path": "ffeUBoTcdS/tmp/66162122d288be0f0ddf980c30fe862b0840657e18d7025aec9a9d8782422673.jpg", "table_caption": ["Table 10: Average classification error comparison between RDumb [45] (a reset-based approach) with different reset frequencies and our PeTTA on CIFAR- $10\\rightarrow$ CIFAR-10-C task. "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "ffeUBoTcdS/tmp/0963862eaf8f34860f04cdfb563cdf6aff5f948857fd3abfa322663e632cc0be.jpg", "table_caption": ["Table 11: Average classification error comparison between RDumb [45] (a reset-based approach) with different reset frequencies and our PeTTA on CIFAR-100-C dataset. "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "ffeUBoTcdS/tmp/9cc695fff665d9817ce5fd0589fdb12021974d952862bc3d83e0bbdd07c4a3f3.jpg", "table_caption": ["Table 12: Average classification error comparison between RDumb [45] (a reset-based approach) with different reset frequencies and our PeTTA on DomainNet dataset. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Discussions. Across datasets and reset frequencies, our PeTTA approach is always better than RDumb [45]. The supreme performance holds even when RDumb has access to the oracle information that can reset the model exactly at the transition between each domain shift or recurrence. Importantly, this oracle information is typically unavailable in practice. ", "page_idx": 25}, {"type": "table", "img_path": "ffeUBoTcdS/tmp/dbd7935b074ebba5d9d60e7a85a80ece0bc4156ea3638e94644b9d46833e62ef.jpg", "table_caption": ["Table 13: Average classification error comparison between RDumb [45] (a reset-based approach) with different reset frequencies and our PeTTA on ImageNet-C dataset. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "Noteworthy, it is clear that the performance of RDumb varies when changing the choice of the reset frequency. For a given choice of $T$ , the better performance on one dataset does not guarantee the same performance on other datasets. For example, $T=1$ , 000 - the best empirical value found by RDumb authors [45] on CCC, does not give the best performance on our recurring TTA scenario; the second choice of $T$ negatively impact the performance on many tasks; the third choice gives the best results, but knowing this exact recurrence frequency of the testing stream is unrealistic. The result highlights the challenge in practice when tuning this parameter (too slow/frequent), especially in the TTA setting where a validation set is unavailable. Our PeTTA, in contrast, is reset-free. ", "page_idx": 26}, {"type": "text", "text": "F.4 PeTTA with 40 Recurring Visits ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "To demonstrate the persistence of PeTTA over an even longer testing stream, in Tab. 14 and Fig. 8, we provide the evaluation results of PeTTA on recurring with 40 recurrences. ", "page_idx": 26}, {"type": "text", "text": "F.5 The Sensitivity of Hyper-parameter Choices in PeTTA ", "text_level": 1, "page_idx": 26}, {"type": "table", "img_path": "ffeUBoTcdS/tmp/39860980b7448d6483bd2bfc587ce8514728fdf6e206f4c683d7a3542fea19fe.jpg", "table_caption": ["Table 15: Sensitivity of PeTTA with different choices of $\\lambda_{0}$ "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "There are two hyper-parameters in PeTTA: $\\alpha_{0}$ and $\\lambda_{0}$ . The initial learning rate of $\\alpha_{0}=1e^{-3}$ is used for all experiments. We do not tune this hyper-parameter, and the choice of $\\alpha_{0}$ is universal across all datasets, following the previous works/compared methods (e.g., RoTTA [61], CoTTA [59]). ", "page_idx": 26}, {"type": "text", "text": "Since $\\lambda_{0}$ is more specific to PeTTA, we included a sensitive analysis with different choices of $\\lambda_{0}$ on PeTTA, evaluated with images from CIFAR-10/100-C and ImageNet-C in Tab. 15. Overall, the choice of $\\lambda_{0}$ is not extremely sensitive, and while the best value is $1e^{\\bar{1}}$ on most datasets, other choices such as $5e^{0}$ or $5e^{1}$ also produce roughly similar performance. Selecting $\\lambda_{0}$ is intuitive, the larger value of $\\lambda_{0}$ stronger prevents the model from collapsing but also limits its adaptability as a trade-off. ", "page_idx": 26}, {"type": "text", "text": "In action, $\\lambda_{0}$ is an initial value and will be adaptively scaled with the sensing model divergence mechanism in PeTTA, meaning it does not require careful tuning. More generally, this hyperparameter can be tuned similarly to the hyper-parameters of other TTA approaches, via an additional validation set, or some accuracy prediction algorithm [29] when labeled data is unavailable. ", "page_idx": 26}, {"type": "text", "text": "F.6 More Details on the Ablation Study ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We provide the detailed classification error for each visit in the recurring TTA setting of each row entry in Tab. 4 (PeTTA Ablation Study): Tab. 16, Tab. 17, Tab. 18, Tab. 19; and Tab. 5 (PeTTA with various choices of regularizers): Tab. 20, Tab. 21, Tab. 22, Tab. 23. ", "page_idx": 26}, {"type": "text", "text": "Fig. 9 presents an additional examination of the ablation study conducted on the task CIFAR-100 $\\rightarrow$ CIFAR-100-C [19] for our PeTTA approach. We plot the classification error (top) and the value of $\\bar{\\gamma}_{t}$ (bottom) for various PeTTA variations. As the model diverges from the initial state, the value of $\\bar{\\gamma}_{t}$ increases. Unable to adjust $\\alpha_{t}$ or constraint the probability space via $\\mathcal{L}_{\\mathrm{AL}}$ limits the ability of PeTTA to prevent model collapse. In all variations with the model collapse in ablation studies, the rapid saturation of $\\bar{\\gamma}_{t}$ is all observed. Therefore, incorporating all components in PeTTA is necessary. ", "page_idx": 26}, {"type": "table", "img_path": "ffeUBoTcdS/tmp/d64ccdc41bca84145a12e30085af6ebe979168db18a71b565154892fcc1a92cb.jpg", "table_caption": ["Table 16: Average classification error of multiple variations of PeTTA. Experiments on CIFAR10 \u2192 CIFAR10-C [19] task. "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "ffeUBoTcdS/tmp/7e074a96a7dd6556ea704720cb850d43a41cd5095f15c9cca5f1760185b395a0.jpg", "table_caption": ["Table 17: Average classification error of multiple variations of PeTTA. Experiments on CIFAR-100 $\\rightarrow$ CIFAR100-C [19] task. "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "ffeUBoTcdS/tmp/b002d65f52d37922c02b19e34dcd7683e45b4fedfe2eed4296bc6458aca0cbf7.jpg", "table_caption": ["Table 18: Average classification error of multiple variations of PeTTA. Experiments on real $\\rightarrow$ clipart, painting, sketch task from DomainNet [44] task. "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "ffeUBoTcdS/tmp/c129c53d4a78e12a2896d5bf4c7364306ecea0ba682daca6498da803a21f2b9f.jpg", "table_caption": ["Table 19: Average classification error of multiple variations of PeTTA. Experiments on ImageNet $\\rightarrow$ ImageNet-C [19] task. "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "ffeUBoTcdS/tmp/6152b08a1263b4940e42700175afe66a814a9ac111a6b94ba636feecf5daf93a.jpg", "table_caption": ["Table 20: Average classification error of PeTTA with various choices of regularizers. Experiments on CIFAR- $10\\rightarrow$ CIFAR-10-C [19] task. "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "ffeUBoTcdS/tmp/61c6f83380bbdcec9b1e206d123daee406a1ddf32a77a6f550cf2272f14c8b9b.jpg", "table_caption": ["Table 21: Average classification error of PeTTA with various choices of regularizers. Experiments on CIFAR- $100\\rightarrow$ CIFAR-100-C [19] task. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Table 22: Average classification error of PeTTA with various choices of regularizers. Experiments on $r e a l\\rightarrow$ clipart, painting, sketch task from DomainNet [44] dataset. ", "page_idx": 28}, {"type": "table", "img_path": "ffeUBoTcdS/tmp/294148469b0e2588f692c63130ee85fb6a740e8cb3a024795c514199b6837c7c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "Table 23: Average classification error of PeTTA with various choices of regularizers. Experiments on ImageNet $\\rightarrow$ ImageNet-C [19] task. ", "page_idx": 28}, {"type": "table", "img_path": "ffeUBoTcdS/tmp/e344f65c7442bd093c607f3aaac62eb1bed4ff0a32186758c3a36057595ffbd1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "F.7 More Confusion Matrices in Recurring TTA Setting ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "For the task CIFAR- $10\\rightarrow$ CIFAR-10-C [19] in recurring TTA setting (with 20 visits), we additionally showcase the confusion matrix of RoTTA [61] (Fig. 10) and our proposed PeTTA (Fig. 11) at each visit. Our PeTTA persistently achieves competitive performance across 20 visits while RoTTA [61] gradually degrades. ", "page_idx": 28}, {"type": "text", "text": "G Experimental Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "G.1 Computing Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "A computer cluster equipped with an Intel(R) Core(TM) 3.80GHz i7-10700K CPU, 64 GB RAM, and one NVIDIA GeForce RTX 3090 GPU (24 GB VRAM) is used for our experiments. ", "page_idx": 28}, {"type": "text", "text": "G.2 Experiments on CCC Testing Stream ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this section, we further evaluate the performance of our PeTTA on the testing data stream of Continuous Changing Corruption (CCC) [45] setting. Here we use the baseline accuracy $20\\%$ , transition speed 1000, and random seed 44.3 The compared methods are source model (ResNet 50), PeTTA, RoTTA [61], and RDumb [45]. Noteworthy, different from recurring TTA, the class labels here are i.i.d. distributed. The adaptation configuration of PeTTA follows the same settings as used on ImageNet-C, while the same setting introduced in Sec. F.3, with $T=1000$ is used for RDumb [45]. ", "page_idx": 28}, {"type": "text", "text": "G.3 Test-time Adaptation Methods ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Pre-trained Model on Source Distribution. Following previous studies [57, 61, 12, 59], only the batch norm layers are updated. As stated in Sec. 5.2, RobustBench [10] and torchvision [35] provide pre-trained models trained on source distributions. Specifically, for ImageNet-C and DomainNet experiments, a ResNet50 model [17] pre-trained on ImageNet V2 (specifically, checkpoint ResNet50_Weights.IMAGENET1K_V2 of torchvision) is used. From RobustBench, the model with checkpoint Standard and Hendrycks2020AugMix_ResNeXt [20] are adopted for CIFAR10-C and CIFAR-100-C experiments, respectively. Lastly, experiments on DomainNet dataset utilize the checkpoint (best_real_2020) provided in AdaContrast [8] study.4 ", "page_idx": 28}, {"type": "text", "text": "Optimizer. Without specifically stated, Adam [26] optimizer with learning rate equal $1e^{-3}$ , and $\\vec{\\beta}\\,\\stackrel{=}{=}(0.9,0.999)$ is selected as a universal choice for all experiments. ", "page_idx": 28}, {"type": "text", "text": "More Details on PeTTA. Since designing the batch normalization layers, and the memory bank is not the key focus of PeTTA, we conveniently adopt the implementation of the Robust Batch Norm layer and the Category-balanced Sampling strategy using a memory bank introduced in RoTTA [61]. ", "page_idx": 28}, {"type": "text", "text": "G.4 The Use of Existing Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Many components of PeTTA is utilized from the official repository of RoTTA [61] 5 and RMT [12]. 6 These two assets are released under MIT license. All the datasets, including CIFAR-10-C, CIFAR100-C and ImageNet-C [19] are publicly available online, released under Apache-2.0 license.7 DomainNet dataset [44] (cleaned version) is also released for research purposes.8 ", "page_idx": 29}, {"type": "image", "img_path": "ffeUBoTcdS/tmp/70aba7ffd6e2d01c956d63d1e6e03c2c2afe585af03bfca3bac84900b255d743.jpg", "img_caption": ["Figure 7: An inspection of PeTTA on the task CIFAR- $10\\rightarrow$ CIFAR-10-C [19] in a recurring with 20 visits (visits are separated by the vertical dashed lines). Here, we visualize (rows 1-3) the dynamic of PeTTA adaptive parameters $(\\bar{\\gamma}_{t},\\lambda_{t},\\alpha_{t})$ , (rows 4-5) the value of the loss functions $(\\mathcal{L}_{\\mathrm{CLS}},\\mathcal{L}_{\\mathrm{AL}})$ and (row 6) the value of the regularization term $\\left({\\mathcal{R}}(\\theta)\\right)$ and (row 7) the classification error rate at each step. The solid line in the foreground of each plot denotes the running mean. The plots show an adaptive change of $\\lambda_{t},\\alpha_{t}$ through time in PeTTA, which stabilizes TTA performance, making PeTTA achieve a persisting adaptation process in all observed values across 20 visits. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "ffeUBoTcdS/tmp/7f453935fe83b44f0d2b98dd655233d0ac3ac8e0a7fbba5f443c1e7a8dff0e16.jpg", "img_caption": ["Figure 8: Testing error of PeTTA with 40 recurring TTA visits. "], "img_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "ffeUBoTcdS/tmp/5739af47371e2999c29bdd31765ef12aad9b2e30a93254ef36ba67c7b63f8276.jpg", "table_caption": [], "table_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "ffeUBoTcdS/tmp/b031f94f310ed1086f2c0db70b93c79cf1fb4c289a0dbee497b2523c51253284.jpg", "img_caption": ["Table 14: Average testing error of PeTTA in recurring TTA with 20 and 40 visits. PeTTA demonstrates its persistence over an extended testing time horizon beyond the $20^{t h}$ visit milestone (Fig. 8\u2019s horizontal dashed line). ", "Figure 9: An inspection on the ablation study of multiple variations of PeTTA on the task CIFAR-100 $\\rightarrow$ CIFAR-100-C [19] in an episodic TTA with 20 visits (visits are separated by the vertical dashed lines). (top): testing error of multiple variations of PeTTA. The performance of PeTTA without (w/o) ${\\mathcal{R}}(\\theta)$ , or fixed regularization coefficient $(\\lambda=\\lambda_{0}/0.1\\lambda_{0})$ degrades through time (the top 3 lines). The degradation of PeTTA - $-\\lambda_{t}$ is still happening but at a slower rate (justification below). The performance of the other three variations persists through time with PeTTA - $-\\lambda_{t}+\\alpha_{t}+\\mathcal{L}_{\\mathrm{AL}}$ achieves the best performance. (bottom): changes of $\\bar{\\gamma}_{t}$ in multiple variations of PeTTA. When limiting the degree of freedom in adjusting $\\alpha_{t}$ or lacking of supervision from $\\mathcal{L}_{\\mathrm{AL}}$ (e.g., PeTTA $-\\lambda_{t}+\\alpha_{t}$ , PeTTA $-\\lambda_{t}+\\mathcal{L}_{\\mathrm{AL}}$ , and especially PeTTA - $-\\lambda_{t}$ ), the value of $\\gamma_{t}$ , unfortunately, escalates and eventually saturated. After this point, PeTTA has the same effect as using a fixed regularization coefficient. Therefore, fully utilizing all components is necessary to preserve the persistence of PeTTA. Best viewed in color. "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "ffeUBoTcdS/tmp/19b2f66c91d15026a23ea2aa03cc0173211579eb9aad9d1932ffa216d49ac963.jpg", "img_caption": ["Figure 10: The dynamic of the confusion matrix of RoTTA [61] in episodic TTA with 20 visits. "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "ffeUBoTcdS/tmp/6aa7ee74732ea39cb5444abfe23d41dce98ebe2d49fa7e4892fcd58595957345.jpg", "img_caption": ["Figure 11: The dynamic of the confusion matrix of PeTTA (ours) in episodic TTA with 20 visits. "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We have highlighted the three main claims and contributions of our work in both the abstract (highlighted in bold font) and the introduction section (listed as bullet points). ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 34}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We have discussed the limitations and potential future work of our study in Sec. 6. Specifically, three main limitations are included: (1) Collapse prevention can not be guaranteed through regularization, PeTTA requires (2) the use of a relatively small memory bank is available and (3) the empirical mean and covariant matrix of feature vectors on the source dataset is computable. We also include discussions in Appdx. E.3 and Appdx. E.4 to further elaborate (2), and (3) respectively. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 34}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We have provided the full proof of all lemmas and theorem in Appdx. B. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 35}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: This study propose a new TTA approach - PeTTA. A full description of this approach is given in Sec. 4 with its pseudo-code provided in Appdx. E.1. The implementation of PeTTA in Python is also attached as supplemental material. Additionally, Sec. 5.2 and Appdx. G are dedicated to providing further implementation details for reproducing the main experimental results. Lastly, the construction of recurring TTA is notably simple, and can be easily extended to other TTA streams. Its configuration on each tasks is described in the Recurring TTA paragraph of Sec. 5.2. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 35}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 36}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: This study does not involve any private datasets. All datasets used in our experiments are publicly available online from previous works (more information in Appdx. G.4). The source code of PeTTA is also attached as supplemental material. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 36}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The experimental settings of the key results in the paper have been provided in Sec. 5.1 (Simulation Setup) and Sec. 5.2 (Setup - Benchmark Datasets). In the supplementary material, any additional experimental results beyond the main paper, such as those in Appdx. D.3, and Appdx. F.3, are consistently preceded by a subsection titled Experiment Setup summarizing the experimental details before presenting the results. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 36}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Due to the limited computing resources, we only extensively evaluate the performance of our proposed method (PeTTA) across 5 independent runs, with different random seeds. Specifically, the mean values in 5 runs are reported in Tab. 1, Tab. 2, Tab. 7, and Tab. 8. The corresponding standard deviation values are provided in Appdx. F.1. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 37}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We have provided the information on the computing resources used in our experiments in Appdx. G.1. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 37}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: The authors have reviewed and to the best of our judgment, this study has conformed to the NeurIPS Code of Ethics. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. \u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. ", "page_idx": 37}, {"type": "text", "text": "\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 38}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 38}, {"type": "text", "text": "Answer: [No] ", "page_idx": 38}, {"type": "text", "text": "Justification: This study advances the research in test-time adaptation area in general, and not tied to particular applications. Hence, there are no significant potential societal consequences of our work which we feel must be specifically highlighted here. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 38}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: To the best of our judgment, this study poses no risks for misuse. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 38}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: The original papers that produced the code package or dataset have been properly cited throughout the paper. Further information on the licenses of used assets are provided in Appdx. G.4. ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 39}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: This study does not release new assets. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 39}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: This study does not involve crowdsourcing nor research with human subjects. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 39}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: This study does not involve crowdsourcing nor research with human subjects. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 40}]