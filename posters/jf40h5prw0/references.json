{"references": [{"fullname_first_author": "Martin Abadi", "paper_title": "Deep learning with differential privacy", "publication_date": "2016-00-00", "reason": "This paper is foundational for the application of differential privacy to machine learning, a core technique used in the methods analyzed in the current paper."}, {"fullname_first_author": "Cynthia Dwork", "paper_title": "Differential privacy", "publication_date": "2006-00-00", "reason": "This paper is seminal to the field of differential privacy, establishing fundamental definitions and concepts used extensively in the private adaptation methods discussed in the current paper."}, {"fullname_first_author": "Nicolas Papernot", "paper_title": "Semi-supervised knowledge transfer for deep learning from private training data", "publication_date": "2017-00-00", "reason": "This paper introduced the Private Aggregation of Teacher Ensembles (PATE) mechanism, which is central to several of the private adaptation methods for closed LLMs analyzed in this work."}, {"fullname_first_author": "Nicolas Papernot", "paper_title": "Scalable private learning with PATE", "publication_date": "2018-00-00", "reason": "This paper builds upon the PATE mechanism introduced in the previous reference, making it more scalable and practical for large language model adaptation, hence highly relevant to the methods in this paper."}, {"fullname_first_author": "Xuechen Li", "paper_title": "Large language models can be strong differentially private learners", "publication_date": "2022-00-00", "reason": "This paper demonstrates the effectiveness of differentially private fine-tuning for large language models, which is a key approach for private adaptation of open LLMs as discussed and compared in the current work."}]}