[{"figure_path": "Jf40H5pRW0/tables/tables_2_1.jpg", "caption": "Table 1: Comparison of privacy protection, performance, and cost between private adaptations for closed vs open LLMs. We select the top-performing adaptations. For closed LLMs, we use DP-ICL [63] and leverage PrivateLoRA [64] on open LLMs for both tasks. We consider sentiment classification on SST2 and the dialog summarization on SAMSum. The training data is denoted by DT and the test queries by Q. Reveals represents which data are exposed to the LLM provider. The methods were trained with DP guarantees: \u03b5 = 8 and \u03b4 = 1/N, where N is the number of examples in DT. We report the Performance (higher is better) on test data (where Acc denotes the classification accuracy). The cost (in $) is computed separately for training (Train) and for answering 10k test queries (Query). Note, the (estimated) number of parameters for closed LLMs is 1.76T for GPT4 Turbo and 175B for GPT3 Davinci, while Llama3 has only 8B and BART-Large is significantly smaller with 355M parameters. The adaptation of the open LLMs is more expensive on SST2 than on SAMSum due to the larger training data size for SST2 and a larger model. DP-ICL's query cost is high due to the usage of an ensemble of 100 prompts to answer each query. In summary, open local LLM adaptations are more private, more performant, and less expensive.", "description": "This table compares the privacy, performance, and cost of using two different methods (DP-ICL and PrivateLoRA) to adapt both closed and open LLMs for two tasks (sentiment classification and dialog summarization).  It highlights that adapting open LLMs is more private, performs better, and is cheaper than adapting closed LLMs.", "section": "Comparing Open and Closed LLM Adaptations"}, {"figure_path": "Jf40H5pRW0/tables/tables_3_1.jpg", "caption": "Table 2: Comparison of properties between private LLM adaptations. The in-context learning (ICL) optimizes instructions and shots (demonstrations). Many privacy techniques include the ones designed for multi-label PATE (denoted as MLPATE) [67], exponential mechanism (EM) [44], joint exponential mechanism (JEM) [22], Gaussian Mechanism (GM), Report-Noisy-Max Mechanism (RNM), Propose-Test-Release (PTR) [20], sample-and-aggregate (SAA) [48], Limited Domain Algorithm (LDA) [18].", "description": "This table compares various private LLM adaptation methods, highlighting their privacy algorithms, optimization strategies, privatization techniques, inference types, and the required resources (e.g., open LLMs). It categorizes the methods based on whether they use in-context learning (ICL) or gradient-based optimization.  The table helps readers understand the differences in approaches and their implications for privacy and performance.", "section": "2 Background and Related Work"}, {"figure_path": "Jf40H5pRW0/tables/tables_6_1.jpg", "caption": "Table 1: Comparison of privacy protection, performance, and cost between private adaptations for closed vs open LLMs. We select the top-performing adaptations. For closed LLMs, we use DP-ICL [63] and leverage PrivateLoRA [64] on open LLMs for both tasks. We consider sentiment classification on SST2 and the dialog summarization on SAMSum. The training data is denoted by DT and the test queries by Q. Reveals represents which data are exposed to the LLM provider. The methods were trained with DP guarantees: \u03b5 = 8 and \u03b4 = 1/N, where N is the number of examples in DT. We report the Performance (higher is better) on test data (where Acc denotes the classification accuracy). The cost (in $) is computed separately for training (Train) and for answering 10k test queries (Query). Note, the (estimated) number of parameters for closed LLMs is 1.76T for GPT4 Turbo and 175B for GPT3 Davinci, while Llama3 has only 8B and BART-Large is significantly smaller with 355M parameters. The adaptation of the open LLMs is more expensive on SST2 than on SAMSum due to the larger training data size for SST2 and a larger model. DP-ICL's query cost is high due to the usage of an ensemble of 100 prompts to answer each query. In summary, open local LLM adaptations are more private, more performant, and less expensive.", "description": "This table compares the privacy, performance, and cost of different private adaptation methods for both closed and open LLMs. It focuses on two tasks: sentiment classification and dialogue summarization, using various LLMs and datasets. The table shows that open LLM adaptations generally offer better privacy, higher performance, and lower costs compared to closed LLM methods.", "section": "4.1 Comparing Privacy Protection"}, {"figure_path": "Jf40H5pRW0/tables/tables_7_1.jpg", "caption": "Table 1: Comparison of privacy protection, performance, and cost between private adaptations for closed vs open LLMs. We select the top-performing adaptations. For closed LLMs, we use DP-ICL [63] and leverage PrivateLoRA [64] on open LLMs for both tasks. We consider sentiment classification on SST2 and the dialog summarization on SAMSum. The training data is denoted by DT and the test queries by Q. Reveals represents which data are exposed to the LLM provider. The methods were trained with DP guarantees: \u03b5 = 8 and \u03b4 = 1/N, where N is the number of examples in DT. We report the Performance (higher is better) on test data (where Acc denotes the classification accuracy). The cost (in $) is computed separately for training (Train) and for answering 10k test queries (Query). Note, the (estimated) number of parameters for closed LLMs is 1.76T for GPT4 Turbo and 175B for GPT3 Davinci, while Llama3 has only 8B and BART-Large is significantly smaller with 355M parameters. The adaptation of the open LLMs is more expensive on SST2 than on SAMSum due to the larger training data size for SST2 and a larger model. DP-ICL's query cost is high due to the usage of an ensemble of 100 prompts to answer each query. In summary, open local LLM adaptations are more private, more performant, and less expensive.", "description": "This table compares the privacy, performance, and cost of private adaptations for closed vs. open LLMs.  It focuses on two tasks: sentiment classification and dialog summarization, using specific methods (DP-ICL and PrivateLoRA) and various LLMs (GPT4 Turbo, Llama3, etc.). The results show that open LLM adaptations offer superior privacy, performance, and lower costs.", "section": "4.1 Comparing Privacy Protection"}, {"figure_path": "Jf40H5pRW0/tables/tables_7_2.jpg", "caption": "Table 5: Evaluation on Question Answering with PFL-DocVQA for \u03b5 = 8", "description": "This table compares the performance of various private adaptation methods for both open and closed LLMs on the PFL-DocVQA question answering task.  The methods are evaluated based on Rouge-1, BLEU, and Levenshtein scores, along with training and query costs.  The \u03b5 value represents the privacy budget used.  The table highlights the superior performance and lower cost of private adaptations using open LLMs compared to closed LLMs for this specific task.", "section": "4.2.3 Performance of Private Adaptations for Text Generation"}, {"figure_path": "Jf40H5pRW0/tables/tables_7_3.jpg", "caption": "Table 1: Comparison of privacy protection, performance, and cost between private adaptations for closed vs open LLMs. We select the top-performing adaptations. For closed LLMs, we use DP-ICL [63] and leverage PrivateLoRA [64] on open LLMs for both tasks. We consider sentiment classification on SST2 and the dialog summarization on SAMSum. The training data is denoted by DT and the test queries by Q. Reveals represents which data are exposed to the LLM provider. The methods were trained with DP guarantees: \u03b5 = 8 and \u03b4 = 1/N, where N is the number of examples in DT. We report the Performance (higher is better) on test data (where Acc denotes the classification accuracy). The cost (in $) is computed separately for training (Train) and for answering 10k test queries (Query). Note, the (estimated) number of parameters for closed LLMs is 1.76T for GPT4 Turbo and 175B for GPT3 Davinci, while Llama3 has only 8B and BART-Large is significantly smaller with 355M parameters. The adaptation of the open LLMs is more expensive on SST2 than on SAMSum due to the larger training data size for SST2 and a larger model. DP-ICL's query cost is high due to the usage of an ensemble of 100 prompts to answer each query. In summary, open local LLM adaptations are more private, more performant, and less expensive.", "description": "This table compares the privacy, performance, and cost of private adaptations for both closed and open LLMs on two tasks: sentiment classification (SST2) and dialog summarization (SAMSum).  It shows that open LLMs, using PrivateLoRA, offer superior performance and privacy at significantly lower cost compared to closed LLMs using DP-ICL.", "section": "Comparing Open and Closed LLM Adaptations"}, {"figure_path": "Jf40H5pRW0/tables/tables_16_1.jpg", "caption": "Table 1: Comparison of privacy protection, performance, and cost between private adaptations for closed vs open LLMs. We select the top-performing adaptations. For closed LLMs, we use DP-ICL [63] and leverage PrivateLoRA [64] on open LLMs for both tasks. We consider sentiment classification on SST2 and the dialog summarization on SAMSum. The training data is denoted by DT and the test queries by Q. Reveals represents which data are exposed to the LLM provider. The methods were trained with DP guarantees: \u03b5 = 8 and \u03b4 = 1/N, where N is the number of examples in DT. We report the Performance (higher is better) on test data (where Acc denotes the classification accuracy). The cost (in $) is computed separately for training (Train) and for answering 10k test queries (Query). Note, the (estimated) number of parameters for closed LLMs is 1.76T for GPT4 Turbo and 175B for GPT3 Davinci, while Llama3 has only 8B and BART-Large is significantly smaller with 355M parameters. The adaptation of the open LLMs is more expensive on SST2 than on SAMSum due to the larger training data size for SST2 and a larger model. DP-ICL's query cost is high due to the usage of an ensemble of 100 prompts to answer each query. In summary, open local LLM adaptations are more private, more performant, and less expensive.", "description": "This table compares the privacy, performance, and cost of private adaptations for closed and open LLMs. It focuses on two tasks: sentiment classification and dialog summarization, using several different LLMs and methods.  The results highlight that open LLMs offer superior privacy, performance, and lower costs compared to their closed counterparts.", "section": "4.1 Comparing Privacy Protection"}, {"figure_path": "Jf40H5pRW0/tables/tables_17_1.jpg", "caption": "Table 1: Comparison of privacy protection, performance, and cost between private adaptations for closed vs open LLMs. We select the top-performing adaptations. For closed LLMs, we use DP-ICL [63] and leverage PrivateLoRA [64] on open LLMs for both tasks. We consider sentiment classification on SST2 and the dialog summarization on SAMSum. The training data is denoted by DT and the test queries by Q. Reveals represents which data are exposed to the LLM provider. The methods were trained with DP guarantees: \u03b5 = 8 and \u03b4 = 1/N, where N is the number of examples in DT. We report the Performance (higher is better) on test data (where Acc denotes the classification accuracy). The cost (in $) is computed separately for training (Train) and for answering 10k test queries (Query). Note, the (estimated) number of parameters for closed LLMs is 1.76T for GPT4 Turbo and 175B for GPT3 Davinci, while Llama3 has only 8B and BART-Large is significantly smaller with 355M parameters. The adaptation of the open LLMs is more expensive on SST2 than on SAMSum due to the larger training data size for SST2 and a larger model. DP-ICL's query cost is high due to the usage of an ensemble of 100 prompts to answer each query. In summary, open local LLM adaptations are more private, more performant, and less expensive.", "description": "This table compares privacy, performance, and cost of adapting closed vs. open LLMs for two tasks: sentiment classification and dialogue summarization.  It shows that open LLM adaptations, using PrivateLoRA, are more private, perform better, and are significantly cheaper than closed LLM methods (DP-ICL).", "section": "4.1 Comparing Privacy Protection"}, {"figure_path": "Jf40H5pRW0/tables/tables_17_2.jpg", "caption": "Table 1: Comparison of privacy protection, performance, and cost between private adaptations for closed vs open LLMs. We select the top-performing adaptations. For closed LLMs, we use DP-ICL [63] and leverage PrivateLoRA [64] on open LLMs for both tasks. We consider sentiment classification on SST2 and the dialog summarization on SAMSum. The training data is denoted by DT and the test queries by Q. Reveals represents which data are exposed to the LLM provider. The methods were trained with DP guarantees: \u03b5 = 8 and \u03b4 = 1/N, where N is the number of examples in DT. We report the Performance (higher is better) on test data (where Acc denotes the classification accuracy). The cost (in $) is computed separately for training (Train) and for answering 10k test queries (Query). Note, the (estimated) number of parameters for closed LLMs is 1.76T for GPT4 Turbo and 175B for GPT3 Davinci, while Llama3 has only 8B and BART-Large is significantly smaller with 355M parameters. The adaptation of the open LLMs is more expensive on SST2 than on SAMSum due to the larger training data size for SST2 and a larger model. DP-ICL's query cost is high due to the usage of an ensemble of 100 prompts to answer each query. In summary, open local LLM adaptations are more private, more performant, and less expensive.", "description": "This table compares the privacy, performance, and cost of private adaptations for both closed and open LLMs.  It focuses on two tasks: sentiment classification (SST2) and dialog summarization (SAMSum), using specific adaptation methods (DP-ICL and PrivateLoRA).  Key metrics include accuracy, Rouge-L score, training cost, and query cost, highlighting the advantages of open LLMs in terms of privacy and cost-effectiveness.", "section": "Comparing Open and Closed LLM Adaptations"}, {"figure_path": "Jf40H5pRW0/tables/tables_17_3.jpg", "caption": "Table 1: Comparison of privacy protection, performance, and cost between private adaptations for closed vs open LLMs. We select the top-performing adaptations. For closed LLMs, we use DP-ICL [63] and leverage PrivateLoRA [64] on open LLMs for both tasks. We consider sentiment classification on SST2 and the dialog summarization on SAMSum. The training data is denoted by DT and the test queries by Q. Reveals represents which data are exposed to the LLM provider. The methods were trained with DP guarantees: \u03b5 = 8 and \u03b4 = 1/N, where N is the number of examples in DT. We report the Performance (higher is better) on test data (where Acc denotes the classification accuracy). The cost (in $) is computed separately for training (Train) and for answering 10k test queries (Query). Note, the (estimated) number of parameters for closed LLMs is 1.76T for GPT4 Turbo and 175B for GPT3 Davinci, while Llama3 has only 8B and BART-Large is significantly smaller with 355M parameters. The adaptation of the open LLMs is more expensive on SST2 than on SAMSum due to the larger training data size for SST2 and a larger model. DP-ICL's query cost is high due to the usage of an ensemble of 100 prompts to answer each query. In summary, open local LLM adaptations are more private, more performant, and less expensive.", "description": "This table compares the privacy, performance, and cost of private adaptations for both closed and open LLMs.  It focuses on two tasks: sentiment classification (SST2) and dialog summarization (SAMSum), using various LLMs and adaptation methods. The results demonstrate that open LLM adaptations are more private, achieve higher performance, and have lower costs than closed LLM alternatives.", "section": "4.1 Comparing Privacy Protection"}, {"figure_path": "Jf40H5pRW0/tables/tables_18_1.jpg", "caption": "Table 1: Comparison of privacy protection, performance, and cost between private adaptations for closed vs open LLMs. We select the top-performing adaptations. For closed LLMs, we use DP-ICL [63] and leverage PrivateLoRA [64] on open LLMs for both tasks. We consider sentiment classification on SST2 and the dialog summarization on SAMSum. The training data is denoted by DT and the test queries by Q. Reveals represents which data are exposed to the LLM provider. The methods were trained with DP guarantees: \u03b5 = 8 and \u03b4 = 1/N, where N is the number of examples in DT. We report the Performance (higher is better) on test data (where Acc denotes the classification accuracy). The cost (in $) is computed separately for training (Train) and for answering 10k test queries (Query). Note, the (estimated) number of parameters for closed LLMs is 1.76T for GPT4 Turbo and 175B for GPT3 Davinci, while Llama3 has only 8B and BART-Large is significantly smaller with 355M parameters. The adaptation of the open LLMs is more expensive on SST2 than on SAMSum due to the larger training data size for SST2 and a larger model. DP-ICL's query cost is high due to the usage of an ensemble of 100 prompts to answer each query. In summary, open local LLM adaptations are more private, more performant, and less expensive.", "description": "This table compares the privacy, performance, and cost of adapting both closed and open LLMs for two tasks: sentiment classification and dialog summarization.  It shows that adapting open LLMs using PrivateLoRA is significantly more private, performs better, and is cheaper than using the state-of-the-art methods for adapting closed LLMs.", "section": "4.1 Comparing Privacy Protection"}, {"figure_path": "Jf40H5pRW0/tables/tables_18_2.jpg", "caption": "Table 1: Comparison of privacy protection, performance, and cost between private adaptations for closed vs open LLMs. We select the top-performing adaptations. For closed LLMs, we use DP-ICL [63] and leverage PrivateLoRA [64] on open LLMs for both tasks. We consider sentiment classification on SST2 and the dialog summarization on SAMSum. The training data is denoted by DT and the test queries by Q. Reveals represents which data are exposed to the LLM provider. The methods were trained with DP guarantees: \u03b5 = 8 and \u03b4 = 1/N, where N is the number of examples in DT. We report the Performance (higher is better) on test data (where Acc denotes the classification accuracy). The cost (in $) is computed separately for training (Train) and for answering 10k test queries (Query). Note, the (estimated) number of parameters for closed LLMs is 1.76T for GPT4 Turbo and 175B for GPT3 Davinci, while Llama3 has only 8B and BART-Large is significantly smaller with 355M parameters. The adaptation of the open LLMs is more expensive on SST2 than on SAMSum due to the larger training data size for SST2 and a larger model. DP-ICL's query cost is high due to the usage of an ensemble of 100 prompts to answer each query. In summary, open local LLM adaptations are more private, more performant, and less expensive.", "description": "This table compares the privacy, performance, and cost of private adaptations for both closed and open LLMs. It focuses on two tasks: sentiment classification (SST2) and dialog summarization (SAMSum).  The table highlights that open LLM adaptations are generally more private, perform better, and are less expensive than closed LLM adaptations.", "section": "Comparing Open and Closed LLM Adaptations"}, {"figure_path": "Jf40H5pRW0/tables/tables_18_3.jpg", "caption": "Table 1: Comparison of privacy protection, performance, and cost between private adaptations for closed vs open LLMs. We select the top-performing adaptations. For closed LLMs, we use DP-ICL [63] and leverage PrivateLoRA [64] on open LLMs for both tasks. We consider sentiment classification on SST2 and the dialog summarization on SAMSum. The training data is denoted by DT and the test queries by Q. Reveals represents which data are exposed to the LLM provider. The methods were trained with DP guarantees: \u03b5 = 8 and \u03b4 = 1/N, where N is the number of examples in DT. We report the Performance (higher is better) on test data (where Acc denotes the classification accuracy). The cost (in $) is computed separately for training (Train) and for answering 10k test queries (Query). Note, the (estimated) number of parameters for closed LLMs is 1.76T for GPT4 Turbo and 175B for GPT3 Davinci, while Llama3 has only 8B and BART-Large is significantly smaller with 355M parameters. The adaptation of the open LLMs is more expensive on SST2 than on SAMSum due to the larger training data size for SST2 and a larger model. DP-ICL's query cost is high due to the usage of an ensemble of 100 prompts to answer each query. In summary, open local LLM adaptations are more private, more performant, and less expensive.", "description": "This table compares the privacy, performance, and cost of private adaptations for both closed and open LLMs.  It focuses on two tasks: sentiment classification (SST2) and dialog summarization (SAMSum), using various LLMs and adaptation methods. The results demonstrate that open LLMs offer superior privacy, performance, and cost-effectiveness compared to closed LLMs in private adaptation scenarios.", "section": "4.1 Comparing Privacy Protection"}, {"figure_path": "Jf40H5pRW0/tables/tables_18_4.jpg", "caption": "Table 1: Comparison of privacy protection, performance, and cost between private adaptations for closed vs open LLMs. We select the top-performing adaptations. For closed LLMs, we use DP-ICL [63] and leverage PrivateLoRA [64] on open LLMs for both tasks. We consider sentiment classification on SST2 and the dialog summarization on SAMSum. The training data is denoted by DT and the test queries by Q. Reveals represents which data are exposed to the LLM provider. The methods were trained with DP guarantees: \u03b5 = 8 and \u03b4 = 1/N, where N is the number of examples in DT. We report the Performance (higher is better) on test data (where Acc denotes the classification accuracy). The cost (in $) is computed separately for training (Train) and for answering 10k test queries (Query). Note, the (estimated) number of parameters for closed LLMs is 1.76T for GPT4 Turbo and 175B for GPT3 Davinci, while Llama3 has only 8B and BART-Large is significantly smaller with 355M parameters. The adaptation of the open LLMs is more expensive on SST2 than on SAMSum due to the larger training data size for SST2 and a larger model. DP-ICL's query cost is high due to the usage of an ensemble of 100 prompts to answer each query. In summary, open local LLM adaptations are more private, more performant, and less expensive.", "description": "This table compares the privacy, performance, and cost of private adaptations for both closed and open LLMs. It focuses on two tasks: sentiment classification (SST2) and dialog summarization (SAMSum).  The table shows that open LLM adaptations are significantly more private, perform better, and are less expensive than their closed counterparts.", "section": "4.1 Comparing Privacy Protection"}, {"figure_path": "Jf40H5pRW0/tables/tables_19_1.jpg", "caption": "Table 1: Comparison of privacy protection, performance, and cost between private adaptations for closed vs open LLMs. We select the top-performing adaptations. For closed LLMs, we use DP-ICL [63] and leverage PrivateLoRA [64] on open LLMs for both tasks. We consider sentiment classification on SST2 and the dialog summarization on SAMSum. The training data is denoted by DT and the test queries by Q. Reveals represents which data are exposed to the LLM provider. The methods were trained with DP guarantees: \u03b5 = 8 and \u03b4 = 1/N, where N is the number of examples in DT. We report the Performance (higher is better) on test data (where Acc denotes the classification accuracy). The cost (in $) is computed separately for training (Train) and for answering 10k test queries (Query). Note, the (estimated) number of parameters for closed LLMs is 1.76T for GPT4 Turbo and 175B for GPT3 Davinci, while Llama3 has only 8B and BART-Large is significantly smaller with 355M parameters. The adaptation of the open LLMs is more expensive on SST2 than on SAMSum due to the larger training data size for SST2 and a larger model. DP-ICL's query cost is high due to the usage of an ensemble of 100 prompts to answer each query. In summary, open local LLM adaptations are more private, more performant, and less expensive.", "description": "This table compares the privacy, performance, and cost of private adaptations for closed and open LLMs. It focuses on two tasks: sentiment classification and dialog summarization.  The table highlights that open LLMs offer better privacy, performance, and lower costs compared to closed LLMs.", "section": "Comparing Open and Closed LLM Adaptations"}, {"figure_path": "Jf40H5pRW0/tables/tables_19_2.jpg", "caption": "Table 1: Comparison of privacy protection, performance, and cost between private adaptations for closed vs open LLMs. We select the top-performing adaptations. For closed LLMs, we use DP-ICL [63] and leverage PrivateLoRA [64] on open LLMs for both tasks. We consider sentiment classification on SST2 and the dialog summarization on SAMSum. The training data is denoted by DT and the test queries by Q. Reveals represents which data are exposed to the LLM provider. The methods were trained with DP guarantees: \u03b5 = 8 and \u03b4 = 1/N, where N is the number of examples in DT. We report the Performance (higher is better) on test data (where Acc denotes the classification accuracy). The cost (in $) is computed separately for training (Train) and for answering 10k test queries (Query). Note, the (estimated) number of parameters for closed LLMs is 1.76T for GPT4 Turbo and 175B for GPT3 Davinci, while Llama3 has only 8B and BART-Large is significantly smaller with 355M parameters. The adaptation of the open LLMs is more expensive on SST2 than on SAMSum due to the larger training data size for SST2 and a larger model. DP-ICL's query cost is high due to the usage of an ensemble of 100 prompts to answer each query. In summary, open local LLM adaptations are more private, more performant, and less expensive.", "description": "This table compares the privacy, performance, and cost of using private adaptations for both closed and open LLMs.  It focuses on two specific tasks: sentiment classification (SST2) and dialog summarization (SAMSum). The table highlights the data leakage to the LLM provider, the accuracy/ROUGE scores, and the training and query costs for each method.  Open LLMs using PrivateLoRA are shown to be more private, performant, and cost-effective compared to closed LLM methods like DP-ICL.", "section": "Comparing Open and Closed LLM Adaptations"}, {"figure_path": "Jf40H5pRW0/tables/tables_20_1.jpg", "caption": "Table 1: Comparison of privacy protection, performance, and cost between private adaptations for closed vs open LLMs. We select the top-performing adaptations. For closed LLMs, we use DP-ICL [63] and leverage PrivateLoRA [64] on open LLMs for both tasks. We consider sentiment classification on SST2 and the dialog summarization on SAMSum. The training data is denoted by DT and the test queries by Q. Reveals represents which data are exposed to the LLM provider. The methods were trained with DP guarantees: \u03b5 = 8 and \u03b4 = 1/N, where N is the number of examples in DT. We report the Performance (higher is better) on test data (where Acc denotes the classification accuracy). The cost (in $) is computed separately for training (Train) and for answering 10k test queries (Query). Note, the (estimated) number of parameters for closed LLMs is 1.76T for GPT4 Turbo and 175B for GPT3 Davinci, while Llama3 has only 8B and BART-Large is significantly smaller with 355M parameters. The adaptation of the open LLMs is more expensive on SST2 than on SAMSum due to the larger training data size for SST2 and a larger model. DP-ICL's query cost is high due to the usage of an ensemble of 100 prompts to answer each query. In summary, open local LLM adaptations are more private, more performant, and less expensive.", "description": "This table compares the privacy, performance, and cost of different private adaptation methods for both closed and open LLMs.  It focuses on two tasks: sentiment classification (SST2) and dialog summarization (SAMSum). The table highlights that open LLM adaptations offer superior privacy, performance, and lower costs compared to their closed counterparts.", "section": "4.1 Comparing Privacy Protection"}, {"figure_path": "Jf40H5pRW0/tables/tables_20_2.jpg", "caption": "Table 1: Comparison of privacy protection, performance, and cost between private adaptations for closed vs open LLMs. We select the top-performing adaptations. For closed LLMs, we use DP-ICL [63] and leverage PrivateLoRA [64] on open LLMs for both tasks. We consider sentiment classification on SST2 and the dialog summarization on SAMSum. The training data is denoted by DT and the test queries by Q. Reveals represents which data are exposed to the LLM provider. The methods were trained with DP guarantees: \u03b5 = 8 and \u03b4 = 1/N, where N is the number of examples in DT. We report the Performance (higher is better) on test data (where Acc denotes the classification accuracy). The cost (in $) is computed separately for training (Train) and for answering 10k test queries (Query). Note, the (estimated) number of parameters for closed LLMs is 1.76T for GPT4 Turbo and 175B for GPT3 Davinci, while Llama3 has only 8B and BART-Large is significantly smaller with 355M parameters. The adaptation of the open LLMs is more expensive on SST2 than on SAMSum due to the larger training data size for SST2 and a larger model. DP-ICL's query cost is high due to the usage of an ensemble of 100 prompts to answer each query. In summary, open local LLM adaptations are more private, more performant, and less expensive.", "description": "This table compares the privacy, performance, and cost of adapting closed LLMs (using DP-ICL) versus open LLMs (using PrivateLoRA) for two tasks: sentiment classification (SST2) and dialog summarization (SAMSum).  It shows that open LLM adaptations are significantly more private, perform better, and are less expensive.", "section": "Comparing Open and Closed LLM Adaptations"}, {"figure_path": "Jf40H5pRW0/tables/tables_20_3.jpg", "caption": "Table 1: Comparison of privacy protection, performance, and cost between private adaptations for closed vs open LLMs. We select the top-performing adaptations. For closed LLMs, we use DP-ICL [63] and leverage PrivateLoRA [64] on open LLMs for both tasks. We consider sentiment classification on SST2 and the dialog summarization on SAMSum. The training data is denoted by DT and the test queries by Q. Reveals represents which data are exposed to the LLM provider. The methods were trained with DP guarantees: \u03b5 = 8 and \u03b4 = 1/N, where N is the number of examples in DT. We report the Performance (higher is better) on test data (where Acc denotes the classification accuracy). The cost (in $) is computed separately for training (Train) and for answering 10k test queries (Query). Note, the (estimated) number of parameters for closed LLMs is 1.76T for GPT4 Turbo and 175B for GPT3 Davinci, while Llama3 has only 8B and BART-Large is significantly smaller with 355M parameters. The adaptation of the open LLMs is more expensive on SST2 than on SAMSum due to the larger training data size for SST2 and a larger model. DP-ICL's query cost is high due to the usage of an ensemble of 100 prompts to answer each query. In summary, open local LLM adaptations are more private, more performant, and less expensive.", "description": "This table compares the privacy, performance, and cost of private adaptations for both closed and open LLMs. It focuses on two tasks: sentiment classification (SST2) and dialog summarization (SAMSum), using specific methods and models for each LLM type.  The results highlight that open LLM adaptations generally offer superior privacy, performance, and lower costs compared to closed LLM adaptations.", "section": "Comparing Open and Closed LLM Adaptations"}, {"figure_path": "Jf40H5pRW0/tables/tables_20_4.jpg", "caption": "Table 1: Comparison of privacy protection, performance, and cost between private adaptations for closed vs open LLMs. We select the top-performing adaptations. For closed LLMs, we use DP-ICL [63] and leverage PrivateLoRA [64] on open LLMs for both tasks. We consider sentiment classification on SST2 and the dialog summarization on SAMSum. The training data is denoted by DT and the test queries by Q. Reveals represents which data are exposed to the LLM provider. The methods were trained with DP guarantees: \u03b5 = 8 and \u03b4 = 1/N, where N is the number of examples in DT. We report the Performance (higher is better) on test data (where Acc denotes the classification accuracy). The cost (in $) is computed separately for training (Train) and for answering 10k test queries (Query). Note, the (estimated) number of parameters for closed LLMs is 1.76T for GPT4 Turbo and 175B for GPT3 Davinci, while Llama3 has only 8B and BART-Large is significantly smaller with 355M parameters. The adaptation of the open LLMs is more expensive on SST2 than on SAMSum due to the larger training data size for SST2 and a larger model. DP-ICL's query cost is high due to the usage of an ensemble of 100 prompts to answer each query. In summary, open local LLM adaptations are more private, more performant, and less expensive.", "description": "This table compares the privacy, performance, and cost of private adaptations for both closed and open LLMs. It focuses on two tasks: sentiment classification and dialog summarization, using various LLMs and methods (DP-ICL and PrivateLoRA).  The results show that open LLMs offer better privacy, higher performance, and lower costs.", "section": "4.1 Comparing Privacy Protection"}, {"figure_path": "Jf40H5pRW0/tables/tables_21_1.jpg", "caption": "Table 1: Comparison of privacy protection, performance, and cost between private adaptations for closed vs open LLMs. We select the top-performing adaptations. For closed LLMs, we use DP-ICL [63] and leverage PrivateLoRA [64] on open LLMs for both tasks. We consider sentiment classification on SST2 and the dialog summarization on SAMSum. The training data is denoted by DT and the test queries by Q. Reveals represents which data are exposed to the LLM provider. The methods were trained with DP guarantees: \u03b5 = 8 and \u03b4 = 1/N, where N is the number of examples in DT. We report the Performance (higher is better) on test data (where Acc denotes the classification accuracy). The cost (in $) is computed separately for training (Train) and for answering 10k test queries (Query). Note, the (estimated) number of parameters for closed LLMs is 1.76T for GPT4 Turbo and 175B for GPT3 Davinci, while Llama3 has only 8B and BART-Large is significantly smaller with 355M parameters. The adaptation of the open LLMs is more expensive on SST2 than on SAMSum due to the larger training data size for SST2 and a larger model. DP-ICL's query cost is high due to the usage of an ensemble of 100 prompts to answer each query. In summary, open local LLM adaptations are more private, more performant, and less expensive.", "description": "This table compares the privacy, performance, and cost of private adaptations for closed vs. open LLMs. It focuses on two tasks: sentiment classification and dialog summarization, using several LLMs and methods.  The results show that open LLM adaptations are superior in all three aspects.", "section": "4.1 Comparing Privacy Protection"}, {"figure_path": "Jf40H5pRW0/tables/tables_22_1.jpg", "caption": "Table 1: Comparison of privacy protection, performance, and cost between private adaptations for closed vs open LLMs. We select the top-performing adaptations. For closed LLMs, we use DP-ICL [63] and leverage PrivateLoRA [64] on open LLMs for both tasks. We consider sentiment classification on SST2 and the dialog summarization on SAMSum. The training data is denoted by DT and the test queries by Q. Reveals represents which data are exposed to the LLM provider. The methods were trained with DP guarantees: \u03b5 = 8 and \u03b4 = 1/N, where N is the number of examples in DT. We report the Performance (higher is better) on test data (where Acc denotes the classification accuracy). The cost (in $) is computed separately for training (Train) and for answering 10k test queries (Query). Note, the (estimated) number of parameters for closed LLMs is 1.76T for GPT4 Turbo and 175B for GPT3 Davinci, while Llama3 has only 8B and BART-Large is significantly smaller with 355M parameters. The adaptation of the open LLMs is more expensive on SST2 than on SAMSum due to the larger training data size for SST2 and a larger model. DP-ICL's query cost is high due to the usage of an ensemble of 100 prompts to answer each query. In summary, open local LLM adaptations are more private, more performant, and less expensive.", "description": "This table compares the privacy, performance, and cost of private adaptations for both closed and open LLMs.  It focuses on two tasks: sentiment classification (SST2) and dialog summarization (SAMSum), using several LLMs and adaptation methods. The results highlight that open LLMs offer better privacy, higher performance, and lower costs compared to closed LLMs when considering private adaptations.", "section": "4.1 Comparing Privacy Protection"}, {"figure_path": "Jf40H5pRW0/tables/tables_22_2.jpg", "caption": "Table 3: Private local adaptations on open LLMs outperform their closed alternatives for classification tasks. The default privacy budget is set to \u03b5 = 8, except for PromptPATE [16], where the performance plateaus after \u03b5 = 0.3. The best result for a given task is bolded, and the 2nd best is underlined. T($) is training cost while Q($) is query cost for 10k queries (SST2), All($) is total cost.", "description": "This table compares the performance, training cost, and query cost of various private adaptation methods for both open and closed LLMs on four different text classification datasets (SST2, Trec, Mpqa, Disaster).  It highlights the superior performance and lower cost of private local adaptations using open LLMs compared to methods using closed LLMs, especially at tighter privacy budgets (lower epsilon values).  The 'Reveals' column indicates data leakage to the LLM provider. Note that the costs are reported separately for training and 10k test queries.", "section": "4.2 Comparing Performance"}, {"figure_path": "Jf40H5pRW0/tables/tables_23_1.jpg", "caption": "Table 1: Comparison of privacy protection, performance, and cost between private adaptations for closed vs open LLMs. We select the top-performing adaptations. For closed LLMs, we use DP-ICL [63] and leverage PrivateLoRA [64] on open LLMs for both tasks. We consider sentiment classification on SST2 and the dialog summarization on SAMSum. The training data is denoted by DT and the test queries by Q. Reveals represents which data are exposed to the LLM provider. The methods were trained with DP guarantees: \u03b5 = 8 and \u03b4 = 1/N, where N is the number of examples in DT. We report the Performance (higher is better) on test data (where Acc denotes the classification accuracy). The cost (in $) is computed separately for training (Train) and for answering 10k test queries (Query). Note, the (estimated) number of parameters for closed LLMs is 1.76T for GPT4 Turbo and 175B for GPT3 Davinci, while Llama3 has only 8B and BART-Large is significantly smaller with 355M parameters. The adaptation of the open LLMs is more expensive on SST2 than on SAMSum due to the larger training data size for SST2 and a larger model. DP-ICL's query cost is high due to the usage of an ensemble of 100 prompts to answer each query. In summary, open local LLM adaptations are more private, more performant, and less expensive.", "description": "This table compares the privacy, performance, and cost of private adaptations for both closed and open LLMs. It focuses on two tasks: sentiment classification (SST2) and dialog summarization (SAMSum), using various LLMs and adaptation methods.  The table highlights that open LLM adaptations generally offer better privacy, higher performance, and lower costs.", "section": "Comparing Open and Closed LLM Adaptations"}, {"figure_path": "Jf40H5pRW0/tables/tables_24_1.jpg", "caption": "Table 1: Comparison of privacy protection, performance, and cost between private adaptations for closed vs open LLMs. We select the top-performing adaptations. For closed LLMs, we use DP-ICL [63] and leverage PrivateLoRA [64] on open LLMs for both tasks. We consider sentiment classification on SST2 and the dialog summarization on SAMSum. The training data is denoted by DT and the test queries by Q. Reveals represents which data are exposed to the LLM provider. The methods were trained with DP guarantees: \u03b5 = 8 and \u03b4 = 1/N, where N is the number of examples in DT. We report the Performance (higher is better) on test data (where Acc denotes the classification accuracy). The cost (in $) is computed separately for training (Train) and for answering 10k test queries (Query). Note, the (estimated) number of parameters for closed LLMs is 1.76T for GPT4 Turbo and 175B for GPT3 Davinci, while Llama3 has only 8B and BART-Large is significantly smaller with 355M parameters. The adaptation of the open LLMs is more expensive on SST2 than on SAMSum due to the larger training data size for SST2 and a larger model. DP-ICL's query cost is high due to the usage of an ensemble of 100 prompts to answer each query. In summary, open local LLM adaptations are more private, more performant, and less expensive.", "description": "This table compares the privacy, performance, and cost of private adaptations for both closed and open LLMs.  It focuses on two tasks: sentiment classification (SST2) and dialog summarization (SAMSum), using several different LLMs.  Key metrics include accuracy, Rouge-L score, training cost, and query cost. The results show that open LLM adaptations generally offer better privacy, performance, and lower costs.", "section": "Comparing Open and Closed LLM Adaptations"}, {"figure_path": "Jf40H5pRW0/tables/tables_25_1.jpg", "caption": "Table 1: Comparison of privacy protection, performance, and cost between private adaptations for closed vs open LLMs. We select the top-performing adaptations. For closed LLMs, we use DP-ICL [63] and leverage PrivateLoRA [64] on open LLMs for both tasks. We consider sentiment classification on SST2 and the dialog summarization on SAMSum. The training data is denoted by DT and the test queries by Q. Reveals represents which data are exposed to the LLM provider. The methods were trained with DP guarantees: \u03b5 = 8 and \u03b4 = 1/N, where N is the number of examples in DT. We report the Performance (higher is better) on test data (where Acc denotes the classification accuracy). The cost (in $) is computed separately for training (Train) and for answering 10k test queries (Query). Note, the (estimated) number of parameters for closed LLMs is 1.76T for GPT4 Turbo and 175B for GPT3 Davinci, while Llama3 has only 8B and BART-Large is significantly smaller with 355M parameters. The adaptation of the open LLMs is more expensive on SST2 than on SAMSum due to the larger training data size for SST2 and a larger model. DP-ICL's query cost is high due to the usage of an ensemble of 100 prompts to answer each query. In summary, open local LLM adaptations are more private, more performant, and less expensive.", "description": "This table compares the privacy, performance, and cost of private adaptations for closed vs. open LLMs.  It focuses on two tasks: sentiment classification (SST2) and dialog summarization (SAMSum), using several different LLMs.  Key metrics include accuracy, Rouge-L score, training cost, and query cost. The table highlights that open LLMs offer superior privacy, performance, and lower cost for private adaptations.", "section": "Comparing Open and Closed LLM Adaptations"}]