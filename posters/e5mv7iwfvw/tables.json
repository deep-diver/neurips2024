[{"figure_path": "e5Mv7iWfVW/tables/tables_8_1.jpg", "caption": "Table 1: Results of Llama-2 fine-tuned on the query and value in attention module with LoRA [15] and AWM. We report the evaluation results on TruthfulQA [20], GSM8K [4], and HellaSwag [39]. We also report the threshold of AWM and the portion of the masked query weight vector pair.", "description": "This table presents the results of fine-tuning Llama-2 using LoRA and the proposed Angle-based Weight Masking (AWM) method.  It compares the performance of LoRA alone against LoRA combined with AWM across different thresholds.  The performance is measured using three benchmarks: TruthfulQA, GSM8K, and HellaSwag.  The table also shows the percentage of fixed weight vector pairs resulting from AWM at each threshold.", "section": "4 Reducing the Trainable Parameters During Fine-tuning"}, {"figure_path": "e5Mv7iWfVW/tables/tables_9_1.jpg", "caption": "Table 2: Results of LLMs fine-tuned on WikiText-2 [22] and GSM8K [4] with LoftQ [19] and AWM. Models are fine-tuned through causal language modelling on training sets and are tested on validation/test sets. We also report the threshold of AWM and the portion of the masked query weight vector pair.", "description": "This table shows the results of fine-tuning various LLMs (Llama-2-7b, Mistral-7B, and Phi-2) on WikiText-2 and GSM8K datasets using LoftQ and the proposed AWM method.  It presents the perplexity scores for WikiText-2 and accuracy scores for GSM8K.  The impact of AWM on the number of trainable parameters is also shown by reporting the threshold (\u03c4) used for masking weight vector pairs and the percentage of fixed weight vector pairs.  Lower perplexity and higher accuracy generally indicate better model performance.", "section": "4.2 Reducing the Trainable Parameters on Query and Key"}]