[{"figure_path": "xeviQPXTMU/tables/tables_1_1.jpg", "caption": "Table 1: Results of adapting the random graph-based watermarking GL method [Xu et al., 2023] to watermark FedGL models. \"MA\": main task accuracy; \"WA\": watermark accuracy.", "description": "This table presents the results of applying a random graph-based watermarking method (from Xu et al., 2023) to Federated Graph Learning (FedGL) models.  It shows the main task accuracy (MA) and watermark accuracy (WA) for different datasets (MUTAG, PROTEINS, DD, COLLAB) and FedGL models (Fed-GIN, Fed-GSAGE, Fed-GCN) under various attacks (None, Distillation, Finetuning, 1-Layer Perturbation).  The results demonstrate the limitations of directly applying existing non-FedGL watermarking techniques to FedGL.", "section": "1 Introduction"}, {"figure_path": "xeviQPXTMU/tables/tables_7_1.jpg", "caption": "Table 2: Results of our FedGMark under empirical watermark removal attacks.", "description": "This table presents the results of the FedGMark model under three different watermark removal attacks (Distillation, Finetuning, and 1-Layer Perturbation) and compares its performance to a scenario with no attack. The results are broken down by dataset (MUTAG, PROTEINS, DD, COLLAB) and FedGL model (Fed-GIN, Fed-GSAGE, Fed-GCN). The metrics used are main task accuracy (MA) and watermark accuracy (WA). The table shows that FedGMark is highly robust against these attacks, maintaining high watermark accuracy even under the attacks. This demonstrates its effectiveness in protecting the ownership of FedGL models.", "section": "4.2 Empirical Results: MA and WA"}, {"figure_path": "xeviQPXTMU/tables/tables_7_2.jpg", "caption": "Table 2: Results of our FedGMark under empirical watermark removal attacks.", "description": "This table presents the results of the proposed FedGMark model when subjected to various watermark removal attacks.  It shows the main task accuracy (MA) and watermark accuracy (WA) for three different graph neural network models (Fed-GIN, Fed-GSAGE, Fed-GCN) across four datasets (MUTAG, PROTEINS, DD, COLLAB). The attacks evaluated include distillation, finetuning, and a 1-layer perturbation attack. The table allows for a comparison of the model's robustness under these attacks by assessing the preservation of both main task accuracy and watermark accuracy.", "section": "4.2 Empirical Results: MA and WA"}, {"figure_path": "xeviQPXTMU/tables/tables_8_1.jpg", "caption": "Table 1: Results of adapting the random graph-based watermarking GL method [Xu et al., 2023] to watermark FedGL models. \"MA\": main task accuracy; \"WA\": watermark accuracy.", "description": "This table presents the results of applying a random graph-based watermarking method to federated graph learning (FedGL) models. It compares the main task accuracy (MA) and watermark accuracy (WA) of different FedGL models (Fed-GIN, Fed-GSAGE, Fed-GCN) on various datasets (MUTAG, PROTEINS, DD, COLLAB) under different attacks (none, distillation, finetuning, 1-layer perturbation). The results show that the existing method yields unsatisfactory performance, with watermark accuracy often less than 60%. This motivates the need for a more robust watermarking method for FedGL, which the paper proposes.", "section": "1 Introduction"}, {"figure_path": "xeviQPXTMU/tables/tables_14_1.jpg", "caption": "Table 1: Results of adapting the random graph-based watermarking GL method [Xu et al., 2023] to watermark FedGL models. \"MA\": main task accuracy; \"WA\": watermark accuracy.", "description": "This table presents the results of applying a random graph-based watermarking method to Federated Graph Learning (FedGL) models.  The original method, from Xu et al. (2023), was adapted for use in FedGL. The table shows the main task accuracy (MA) and watermark accuracy (WA) for various FedGL models (Fed-GIN, Fed-GSAGE, Fed-GCN) on different datasets (MUTAG, PROTEINS, DD, COLLAB) under different attacks (None, Distillation, Finetuning, 1-Layer Perturbation).  The results highlight the performance of this adapted watermarking method and its vulnerability to watermark removal attacks.", "section": "1 Introduction"}, {"figure_path": "xeviQPXTMU/tables/tables_14_2.jpg", "caption": "Table 1: Results of adapting the random graph-based watermarking GL method [Xu et al., 2023] to watermark FedGL models. \"MA\": main task accuracy; \"WA\": watermark accuracy.", "description": "This table presents the results of applying a random graph-based watermarking method to Federated Graph Learning (FedGL) models.  It compares the main task accuracy (MA) and watermark accuracy (WA) across different datasets (MUTAG, PROTEINS, DD, COLLAB) and FedGL models (Fed-GIN, Fed-GSAGE, Fed-GCN).  Different attacks (None, Distillation, Finetuning, 1-Layer Pert.) are also tested to evaluate the robustness of the watermarking method.  The low watermark accuracy across all datasets and models highlights a key limitation addressed in the paper.", "section": "1 Introduction"}, {"figure_path": "xeviQPXTMU/tables/tables_14_3.jpg", "caption": "Table 1: Results of adapting the random graph-based watermarking GL method [Xu et al., 2023] to watermark FedGL models. \"MA\": main task accuracy; \"WA\": watermark accuracy.", "description": "This table presents the results of applying a random graph-based watermarking technique from existing literature to Federated Graph Learning (FedGL) models. It compares the main task accuracy (MA) and watermark accuracy (WA) under different attack scenarios (none, distillation, finetuning, and 1-layer perturbation).  The goal is to show the limitations of directly applying existing watermarking methods to FedGL, motivating the need for a more robust approach. The table demonstrates that existing methods yield unsatisfactory performance, particularly when subjected to watermark removal attacks like distillation and finetuning. This highlights the need for a new watermarking method specifically designed for FedGL.", "section": "1 Introduction"}, {"figure_path": "xeviQPXTMU/tables/tables_15_1.jpg", "caption": "Table 2: Results of our FedGMark under empirical watermark removal attacks.", "description": "This table presents the results of the FedGMark model under various empirical watermark removal attacks, including distillation, finetuning, and a 1-layer perturbation attack.  It shows the main task accuracy (MA) and watermark accuracy (WA) for three different FedGL models (Fed-GIN, Fed-GSAGE, Fed-GCN) across four graph datasets (MUTAG, PROTEINS, DD, COLLAB). The results demonstrate the robustness of FedGMark against these attacks, maintaining high watermark accuracy even after the attacks.", "section": "4.2 Empirical Results: MA and WA"}, {"figure_path": "xeviQPXTMU/tables/tables_16_1.jpg", "caption": "Table 2: Results of our FedGMark under empirical watermark removal attacks.", "description": "This table presents the results of the FedGMark model under three different watermark removal attacks: distillation, finetuning, and a 1-layer perturbation attack.  It shows the main task accuracy (MA) and watermark accuracy (WA) for the FedGMark model on four different datasets (MUTAG, PROTEINS, DD, and COLLAB) using three different FedGL models (Fed-GIN, Fed-GSAGE, and Fed-GCN). The table compares the performance of FedGMark under no attack, with the performances after each attack.  This allows assessment of the model's robustness against attempts to remove the watermark.", "section": "4.2 Empirical Results: MA and WA"}, {"figure_path": "xeviQPXTMU/tables/tables_16_2.jpg", "caption": "Table 1: Results of adapting the random graph-based watermarking GL method [Xu et al., 2023] to watermark FedGL models. \"MA\": main task accuracy; \"WA\": watermark accuracy.", "description": "This table presents the results of applying a random graph-based watermarking method (from Xu et al., 2023) to Federated Graph Learning (FedGL) models.  It evaluates the main task accuracy (MA) and watermark accuracy (WA) across various datasets (MUTAG, PROTEINS, DD, COLLAB) and FedGL models (Fed-GIN, Fed-GSAGE, Fed-GCN) under different attacks (None, Distillation, Finetuning, 1-Layer Perturbation).  The table shows that this adaptation yields unsatisfactory watermark accuracy (mostly less than 60%), highlighting the need for more sophisticated techniques.", "section": "1 Introduction"}, {"figure_path": "xeviQPXTMU/tables/tables_16_3.jpg", "caption": "Table 12: Results of FedGMark on IID and non-IID/heterogeneous datasets.", "description": "This table compares the performance of the FedGMark model on two types of datasets: IID (independently and identically distributed) and Non-IID (non-independently and identically distributed).  The IID datasets assume that data across different clients is similar, while the Non-IID datasets have varying data distributions among clients which is more common in real world scenarios.  The results are shown in terms of Main task accuracy (MA) and Watermark accuracy (WA).  These results demonstrate the robustness and effectiveness of FedGMark in handling non-IID data, a key challenge in federated learning.", "section": "4 Experiments"}, {"figure_path": "xeviQPXTMU/tables/tables_16_4.jpg", "caption": "Table 13: Results of FedGMark against p% malicious clients whose watermarked data are mislabeled.", "description": "This table presents the results of the FedGMark model's performance against malicious clients. The experiment simulates scenarios with varying percentages (0%, 10%, 20%, 30%, and 40%) of malicious clients whose watermarked data has been mislabeled.  The table shows the Main task Accuracy (MA) and Watermark Accuracy (WA) achieved in each scenario across four different datasets (MUTAG, PROTEIN, DD, and COLLAB). This demonstrates the model's robustness to different levels of malicious attacks.", "section": "4.2.2 Impact of Hyperparameters"}, {"figure_path": "xeviQPXTMU/tables/tables_17_1.jpg", "caption": "Table 2: Results of our FedGMark under empirical watermark removal attacks.", "description": "This table presents the results of the FedGMark method under different empirical watermark removal attacks (distillation, finetuning, and 1-layer perturbation). For each of the four datasets (MUTAG, PROTEINS, DD, and COLLAB) and three FedGL models (Fed-GIN, Fed-GSAGE, and Fed-GCN), the table shows the main task accuracy (MA) and watermark accuracy (WA) under each attack.  The results demonstrate the robustness of FedGMark against these attacks, with WAs remaining high even after the attacks are applied.", "section": "4.2 Empirical Results: MA and WA"}, {"figure_path": "xeviQPXTMU/tables/tables_18_1.jpg", "caption": "Table 2: Results of our FedGMark under empirical watermark removal attacks.", "description": "This table presents the results of the FedGMark model under three different watermark removal attacks: distillation, finetuning, and a 1-layer perturbation attack.  It compares the Main task accuracy (MA) and Watermark accuracy (WA) across four different graph datasets (MUTAG, PROTEINS, DD, COLLAB) and three different Federated Graph Learning (FedGL) models (Fed-GIN, Fed-GSAGE, Fed-GCN).  The \"None\" row shows the performance without any attack.", "section": "4.2 Empirical Results: MA and WA"}, {"figure_path": "xeviQPXTMU/tables/tables_18_2.jpg", "caption": "Table 17: MA/WA on synthesized graphs for watermarking.", "description": "This table presents the Main task accuracy (MA) and Watermark accuracy (WA) achieved by FedGMark on synthesized graphs used for watermarking.  The results are shown for four different datasets: MUTAG, PROTEINS, DD, and COLLAB.  The table compares the performance on graphs used in training/testing with those on newly synthesized graphs, providing insights into the model's generalizability and robustness.", "section": "4.2 Empirical Results: MA and WA"}]