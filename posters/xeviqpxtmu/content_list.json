[{"type": "text", "text": "FedGMark: Certifiably Robust Watermarking for Federated Graph Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yuxin Yang1,2 Qiang Li1 Yuan Hong3 Binghui Wang2\u2217 ", "page_idx": 0}, {"type": "text", "text": "1College of Computer Science and Technology, Jilin University, Changchun, Jilin, China 2Department of Computer Science, Illinois Institute of Technology, Chicago, Illinois, USA 3School of Computing, University of Connecticut, Storrs, Connecticut, USA ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Federated graph learning (FedGL) is an emerging learning paradigm to collaboratively train graph data from various clients. However, during the development and deployment of FedGL models, they are susceptible to illegal copying and model theft. Backdoor-based watermarking is a well-known method for mitigating these attacks, as it offers ownership verification to the model owner. We take the first step to protect the ownership of FedGL models via backdoor-based watermarking. Existing techniques have challenges in achieving the goal: 1) they either cannot be directly applied or yield unsatisfactory performance; 2) they are vulnerable to watermark removal attacks; and 3) they lack of formal guarantees. To address all the challenges, we propose FedGMark, the first certified robust backdoor-based watermarking for FedGL. FedGMark leverages the unique graph structure and client information in FedGL to learn customized and diverse watermarks. It also designs a novel GL architecture that facilitates defending against both the empirical and theoretically worst-case watermark removal attacks. Extensive experiments validate the promising empirical and provable watermarking performance of FedGMark. Source code is available at: https://github.com/Yuxin104/FedGMark. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Federated Graph Learning (FedGL) [Xie et al., 2021; Wang et al., 2022a; Tan et al., 2023; Yao et al., 2024] leverages a server and multiple clients to collaboratively train GL methods [Kipf and Welling, 2017; Hamilton et al., 2017] via federated learning (FL) [McMahan et al., 2017; Li et al., 2021; Karimireddy et al., 2020]. In recent years, FedGL has attracted increasing interest in domains such as disease prediction [Peng et al., 2022], recommendation systems [Baek et al., 2023; Wu et al., 2022; Li et al., 2022b], and molecular classification [He et al., 2022]. In addition, several industries have deployed/open-sourced their FedGL frameworks, such as Alibaba\u2019s FederatedScope-GNN [Wang et al., 2022b] and Amazon\u2019s FedML-GNN [Vidya et al.]. However, FedGL models are typically left unprotected, rendering them vulnerable to threats like illegal copying, model theft, and malicious distribution. For instance, a business competitor may replicate a model to gain competitive advantages or a malicious user may sell the model for proftis. These threats waste the model owner\u2019s investment (e.g., labor costs, time, and energy) and infringe upon the legitimate copyrights of the model. ", "page_idx": 0}, {"type": "text", "text": "Backdoor-based watermarking [Uchida et al., 2017; Adi et al., 2018; Bansal et al., 2022] is a de facto model ownership verification technique to mitigate the above threats. This technique typically consists of two steps: 1) Embedding the target model with a watermark. The model owner injects a specific backdoor trigger (i.e., watermark) into some clean samples and trains the target model with this watermarked data along with the remaining clean data. Then the trained target (watermarked) model could have both high watermark accuracy (i.e., accurately classify testing data with the same watermark as the owner desires) and main task accuracy (i.e., accurately classify clean testing data). 2) Model ownership verification. When suspecting the target model is illegally used by others, the model owner can recruit a trusted third party for model ownership verification. Particularly, the true model owner knows how the target model behaves as expected by providing the trusted third party the carefully designed watermarked data, while the illegal parties cannot do so. Notice that, since all the clients have devoted computation and data to the training, they have a strong intention to jointly protect their ownership of the model. ", "page_idx": 0}, {"type": "table", "img_path": "xeviQPXTMU/tmp/5cc2a04acbf572e555e8864302a8b1d8bb8f2de17a96149fe236fbc911208527.jpg", "table_caption": ["Table 1: Results of adapting the random graph-based watermarking GL method [Xu et al., 2023] to watermark FedGL models. \u201cMA\u201d: main task accuracy; \u201cWA\u201d: watermark accuracy. "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this paper, we aim to protect the ownership of FedGL models via backdoor-based watermarking. We observe backdoor-based watermarking methods for protecting the ownership of FL model on non-graph data [Li et al., 2022a; Tekgul et al., 2021; Shao et al., 2022; Yang et al., 2023; Lansari et al., 2023] or centralized GL model on graph data [Xu et al., 2023] have been recently developed. However, applying these methods for protecting FedGL models faces challenges and weaknesses. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Inapplicable or ineffective: Existing methods for non-graph data cannot be directly applied for graph data. For instance, they require input data have same size, while graphs can have varying sizes; they are unable to consider the connectivity information such as edges connecting nodes in the graph data. The only method for graph data [Xu et al., 2023] uses a naive random graph (e.g., generated by the ER model [Gilbert, 1959]) as a watermark. Extending this watermark from centralized GL to FedGL models exhibits unsatisfactory performance, as shown in Table 1. For instance, the watermark accuracy is less than $60\\%$ in all the studied graph datasets and FedGL models. The core reason is the random graph watermark does not use any graph structure information or client information that are unique in FedGL. \u2022 Vulnerable to watermark removal attacks: They are vulnerable to existing watermark removal techniques such as distillation and finetuning [Bansal et al., 2022] (more details in Section 2.3). For instance, as illustrated in Table 1, distillation can reduce the watermark accuracy to less than $30\\%$ . \u2022 Lack or weak formal guarantees: All these methods do not provide formal robustness guarantees against watermark removal attacks. This could make them even vulnerable to more advanced attacks. For instance, our proposed layer-perturbation attack can further reduce the watermark accuracy, e.g., perturbing only 1-layer parameters of the watermarked model yields only $10\\%$ watermark accuracy (while main accuracy is marginally affected). Bansal et al. [2022] proposed the first certified watermark for centralized non-graph learning models against $l_{2}$ model parameter perturbation. However, its certified radius is only 1.2, meaning the $l_{2}$ norm of a (usually milliondimensional) perturbation vector cannot exceed 1.2 to maintain the watermark accuracy. ", "page_idx": 1}, {"type": "text", "text": "We address all the above issues by proposing a certified robust backdoor-based watermark method for FedGL, called FedGMark.2 FedGMark enjoys several properties: 1) Its designed watermarks can handle varying size graphs and utilize both graph structure and client information unique in FedGL models; 2) It is empirically robust to both existing watermark removal attacks and the proposed layerperturbation attack; and 3) more importantly, it is provably robust to the layer-perturbation attack (the layer parameters can be arbitrarily perturbed), when the number of the perturbed layers is bounded. ", "page_idx": 1}, {"type": "image", "img_path": "xeviQPXTMU/tmp/0607bb785509f4d6d7450891c4a772a120bebc48e70c7f7b7f7520042d067ed7.jpg", "img_caption": ["Figure 1: Overall pipeline of the proposed certified watermarks. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Specifically, as depicted in Figure 1, FedGMark consists of two modules: 1) Customized Watermark Generator (CWG): it learns the customized watermark for individual graphs and clients in FedGL, by integrating the edge information from the client graphs and the unique key features of the clients. CWG can significantly enhance the diversity and effectiveness of the generated watermarks. 2) Robust Model Loader (RML). RML designs a new GL model that consists of multiple submodels, where each submodel can be any existing GL model. It also introduces a voting classifier for assembling the submodels\u2019 predictions. Such a design can facilitate deriving the certified watermark performance against the (worst-case) layer-perturbation attack. ", "page_idx": 2}, {"type": "text", "text": "We evaluate FedGMark on four real-world graph datasets (MUTAG, PROTEINS, DD, and COLLAB) and three FedGL models including Fed-GIN, Fed-GSAGE, and Fed-GCN, whose base GL models are GIN [Xu et al., 2019], GSAGE [Hamilton et al., 2017], and GCN [Kipf and Welling, 2017], respectively. Extensive experimental results show FedGMark achieves high main accuracy and watermark accuracy under no attacks and watermark removal attacks, high certified watermark accuracy, and significantly outperforms the existing method. Such good results demonstrate the potential of FedGMark as a watermarking method to protect the ownership of FedGL models. ", "page_idx": 2}, {"type": "text", "text": "We summarize our main contributions of this paper as follows: ", "page_idx": 2}, {"type": "text", "text": "\u2022 To our best knowledge, this is the first work to protect the ownership of emerging FedGL models. \u2022 We propose a certifiably robust backdoor-based watermarking method FedGMark for FedGL. \u2022 We validate the effectiveness of FedGMark in multiple FedGL models and real-world graph datasets under no attack, existing backdoor removal attacks, and worst-case layer-perturbation attacks. ", "page_idx": 2}, {"type": "text", "text": "2 PRELIMINARIES ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Federated Graph Learning (FedGL) ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given a graph $G=(\\mathbb{V},\\mathbb{E})$ as input, a GL model for graph classification learns a graph classifier $f$ that outputs a label $f(G)=y\\in\\mathbb{Y}$ for a graph. Here, $\\mathbb{V},\\mathbb{E},\\mathbb{Y}$ represent the set of nodes, edges, and labels, respectively. $\\mathbf{A}\\in\\{0,1\\}^{|\\mathbb{V}|\\times|\\mathbb{V}|}$ is the binary adjacency matrix of $G$ , where $\\mathbf{A}[v_{j},v_{k}]=1$ if there exists an edge between nodes $v_{j}$ and $v_{k}$ , and 0 otherwise, with $\\lvert\\mathbb{V}\\rvert$ the total number of nodes. FedGL employs $\\mathrm{FL}$ techniques [McMahan et al., 2017] to collaboratively train GL models with a set of (e.g., $T$ ) clients $\\mathbb{T}=\\{1,\\cdot\\cdot\\cdot,T\\}$ and a server. Assuming each client $i\\in\\mathbb{T}$ has a set of graphs $\\mathbb{G}^{i}$ , we illustrate the training process of FedGL using the $e_{\\cdot}$ -th epoch as an example: 1) Initially, the server distributes the global model parameters $\\theta_{e}$ to a randomly selected subset of clients $\\mathbb{T}_{e}$ , where $\\mathbb{T}_{e}\\subseteq\\mathbb{T}.\\;2)$ ) Upon receiving $\\theta_{e}$ , each client $i$ trains its local model parameter $\\theta_{e}^{i}$ with its own graphs $\\mathbb{G}^{i}$ and updates its model parameters via SGD, i.e., $\\theta_{e}^{i}=\\theta_{e-1}^{i}-\\bar{\\eta}\\partial_{\\theta_{e}}L(\\theta_{e};\\breve{\\mathbb{G}}^{i})$ , where $L(\\bar{\\theta}_{e};\\bar{\\mathbb{G}}^{i})$ represents a loss function, e.g., cross-entropy loss. After training, client $i$ submits its update model parameters $\\theta_{e}^{i}$ to the server. 3) The server aggregates local model parameters of the selected clients i.e., $\\{\\theta_{e}^{i}:i\\in\\mathbb{T}_{e}\\}$ and updates the global model parameter, e.g., $\\begin{array}{r}{\\theta_{e+1}=\\frac{1}{|\\mathbb{T}_{e}|}\\sum_{i\\in\\mathbb{T}_{e}}\\theta_{e}^{i}}\\end{array}$ via the average aggregation [McMahan et al., 2017], for the next epoch. This iterative process continues until the global model converges or reaches the maximum number of epochs. ", "page_idx": 2}, {"type": "text", "text": "2.2 Backdoor-based Watermarking for GL ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Backdoor-based watermarking [Uchida et al., 2017; Adi et al., 2018; Bansal et al., 2022; Xu et al., 2023] adopts the idea of backdoor attack [Bagdasaryan et al., 2020; Wang et al., 2020; Saha et al., 2020] from the adversarial realm to facilitate model ownership verification. To watermark the GL model, assume the model owner has a set of clean graphs $\\mathbb{G}$ and selects a subset of graphs $\\mathbb{G}_{w}\\subset\\mathbb{G}$ to inject the watermark. In the existing method [Xu et al., 2023], the model owner first generates a random graph (e.g., via the ER-model) as the watermark for each to-be-watermarked graph. For instance, for a graph $G\\in\\mathbb{G}_{w}$ with label $y$ , the generated random graph is $G_{s}$ (its size is often smaller than $G$ ). The owner then attaches $G_{s}$ to $G$ to produce the watermarked graph $G_{w}$ , where nodes in $G$ are randomly chosen and the edge status of these nodes are replaced by edges in $G_{s}$ . Finally, the owner assigns a desired label different from $y$ to $G_{w}$ . The watermarked graphs together with the clean graphs are used to train the GL model. During model ownership verification, the one who can predict a high accuracy on these watermarked graphs can claim to be the model owner. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "We note this method can be extended to watermark FedGL models, where each client can generate its own random graphs as the watermark and train its local model with the watermarked graphs and clean graphs. The server then aggregates the watermarked local models to update the global model. ", "page_idx": 3}, {"type": "text", "text": "2.3 Watermark Removal Attacks ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We consider three possible watermark removal attacks aiming to infringe the FedGL model ownership: distillation and finetuning from [Shafieinejad et al., 2021], and our proposed layer-perturbation attack. In all attacks, the attacker (e.g., malicious user) is assumed to know the target watermarked model. ", "page_idx": 3}, {"type": "text", "text": "1) Distillation. This attack has access to some unlabeled data sampled from the same data distribution. To remove watermarks without affecting the target model\u2019s main task performance, the attacker uses the unlabeled data to distill the target model during training. Specifically, the attacker initializes its model with the target model and labels the unlabeled data by querying the target model. The attacker\u2019s model is then updated with these unlabeled data and their predicted labels. ", "page_idx": 3}, {"type": "text", "text": "2) Finetuning. This attack assumes the attacker has some labeled data. The attacker then leverages the labeled data to further finetune the target model in order to forget the watermark. This attack is shown to pose a greater threat than the distillation attack [Bansal et al., 2022]. ", "page_idx": 3}, {"type": "text", "text": "3) Layer-perturbation attack. This attack also assumes the attacker has some labeled data. As knowing the target watermarked model (and hence the architecture), the attacker can mimic training an unwatermarked model with the same architecture as the target model using the labeled data. To further test the model robustness, we assume the attacker also knows some true watermarked samples, similar to [Jiang et al., 2023]. Then, the attacker can replace any layer(s)\u2019 parameters of the target model with those from the unwatermarked model to maximally reduce the watermark accuracy on its watermarked samples, while maintaining the main task performance. Our results (e.g., in Table 1) show this layer-perturbation attack (even only perturbing 1 layer parameters) is much more effective than the other two attacks (even though the whole model parameters can be perturbed). ", "page_idx": 3}, {"type": "text", "text": "2.4 Threat Model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We follow existing methods [Shafieinejad et al., 2021; Bansal et al., 2022; Xu et al., 2023; Jiang et al., 2023], where the adversary is assumed to know all details of the pretrained watermarked FedGL model, but does not tamper with the training process. This means all clients and the server are benign and follow the federated training protocol, and the attack happens at the testing/inference time. We highlight this is in stark contrast to the training-time Byzantine attack on FL where some clients are malicious and they manipulate the training process. ", "page_idx": 3}, {"type": "text", "text": "Attacker\u2019s knowledge. The attacker has white-box access to the pretrained watermarked FedGL model. In addition, the attacker may also know some clean (unlabeled or labeled) training data, as well as watermarked data. Note that this setting actually makes our defense design the most challenging. If the defense can successfully defend against the strongest white-box attack on the watermarked FedGL model, it will also be effective against weaker attacks, such as black-box attacks. ", "page_idx": 3}, {"type": "text", "text": "Attacker\u2019s capability. The attacker can modify the pretrained model via leveraging its white-box access to the trained model and its hold training and watermarked data. For instance, the attacker can finetune the pretrained model via the labeled training data. More details of the capabilities of considered attacks are described in Section 2.3. ", "page_idx": 3}, {"type": "text", "text": "Attacker\u2019s goal. The attacker aims to remove the watermark based on its knowledge and capability, while maintaining the model utility. This allows it to illegally use the model without detection. ", "page_idx": 3}, {"type": "text", "text": "3 FedGMark: Our Certified Robust Watermark for FedGL ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "3.1 Motivation and Overview ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Recall that the random graph based watermark is unable to ensure high watermark accuracy for protecting FedGL (as shown in Table 1). This is because such random watermark does not use any graph structure or client information during FedGL training. Our results also show this method is vulnerable to the three watermark removal attacks. These weaknesses inspire us to design a more effective and robust watermarking method specially for FedGL model ownership verification. ", "page_idx": 4}, {"type": "text", "text": "We propose FedGMark, the first certified robust backdoor-based watermarking method for FedGL. FedGMark comprises two main components: Customized Watermark Generator (CWG) and Robust Model Loader (RML) (as depicted in Figure 1). The CWG module utilizes the unique property of each client, as different clients could have different properties (e.g., distributions of their graph data) and their optimal watermark could be different. Particularly, CWG learns the customized watermark for each graph using its structure information, and outputs a set of diversified watermarked graphs for each client. Further, inspired by existing GNNs [Xu et al., 2019], the RML module designs a new GL model that consists of multiple submodels, each being any existing GL model. It also introduces a voting classifier for aggregating the prediction results from the submodels. Under this design, FedGMark can be proved to be certified robust against the worst-case layer-perturbation attack, once the number of perturbed layers is bounded. The model owner (e.g., participating clients in FedGL) adopts the designed GL model to train the local watermarked model with the learnt watermarked graphs and the remaining clean graphs. After the server-client training terminates, the ownership of the trained FedGL model can be verified via measuring its performance on a set of testing graphs injected with the global watermark, which is the integration of all clients\u2019 local watermarks. ", "page_idx": 4}, {"type": "text", "text": "3.2 Customized Watermark Generator (CWG) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "CWG consists of two networks: GatingNet and KeyNet. GatingNet designs the watermark for each graph separately using the edge information, while KeyNet learns client-wise watermarking style using predefined keys (e.g., client ID in this paper). The customized watermark for each client\u2019s graph is then decided using the output of GatingNet and KeyNet. Detailed network architectures of CWG can be seen in Table 7 in Appendix $C.$ . We demonstrate how CWG can learn a customized watermark using a graph $G^{i}=(\\mathbb{V}^{i},\\mathbf{\\bar{E}}^{i})$ from client $i$ as an instance. The details are as follows: ", "page_idx": 4}, {"type": "text", "text": "\u2022 We first randomly select $n_{w}$ nodes $\\mathbb{V}_{w}^{i}=\\{v_{1},\\cdots,v_{n_{w}}\\}$ from $\\mathbb{V}^{i}$ as watermark nodes and construct a corresponding mask matrix $\\mathbf{M}^{i}\\in\\{0,1\\}^{|\\mathbb{V}^{i}|\\times|\\mathbb{V}^{i}|}$ such that ${\\bf M}^{i}[v_{j},v_{k}]=1$ if $v_{j},v_{k}\\in\\mathbb{V}_{w}^{i}$ , and 0 otherwise. We also update the adjacency matrix $\\mathbf{A}^{i}$ of $G^{i}$ according to $\\mathbb{V}_{w}^{i}$ , i.e., setting ${\\bf A}^{i}[v_{j},v_{k}]=$ $0,\\forall v_{j}$ , $\\boldsymbol{v}_{k}\\in\\mathbb{V}_{w}^{i}$ . This allows us focus on learning the edge status between watermarked nodes. \u2022 Given the client $i$ \u2019s ID string $k^{i}$ , we utilize a cryptographic hash function, such as MD5, to convert it into an integer (e.g., 128-bit long with the integer range $[0,2^{128}-1])$ . This integer is then employed as a seed to produce a key matrix $\\mathbf{K}^{i}\\in\\mathbb{R}^{|\\mathbb{V}^{i}|\\times|\\mathbb{V}^{i}|}$ . Then, we employ GatingNet and KeyNet to extract edge features and key features, resulting in $\\widetilde{\\mathbf{A}}^{i}=\\mathrm{GatingNet}(\\mathbf{A}^{i})\\in[0,1]^{|\\mathbb{V}^{i}|\\times|\\mathbb{V}^{i}|}$ and $\\tilde{\\mathbf{K}}^{i}=\\mathrm{KeyNet}(\\mathbf{K}^{i})\\in[0,1]^{|\\mathbb{V}^{i}|\\times|\\mathbb{V}^{i}|}$ , respectively. \u2022 We finally learn the customized watermark for $G^{i}$ by integrating $\\tilde{\\mathbf{A}}^{i},\\tilde{\\mathbf{K}}^{i}$ , and $\\mathbf{M}^{i}$ , and obtain the corresponding watermarked graph as $G_{w}^{i}=(\\mathbb{V}^{i},\\dot{\\mathbb{E}}_{w}^{i})$ . Here $\\mathbb{E}_{w}^{i}$ is the set of edges according to the updated adjacency matrix $\\mathbf{A}^{\\overline{{i}}}\\oplus\\mathbf{W}^{i}$ , where $\\bigoplus$ is the element-wise addition and $\\mathbf{W}^{i}\\,=$ $\\mathbb{I}((\\tilde{\\mathbf{A}}^{i}\\odot\\tilde{\\mathbf{K}}^{i})>0.\\dot{5})\\odot\\dot{\\mathbf{M}}^{i}$ contains the edge status between the watermark nodes $\\mathbb{V}_{w}^{i}$ . Here, $\\odot$ is the element-wise product, $\\mathbb{I}(p)$ is an indicator function returning 1 if $p$ is true, and 0 otherwise. We adopt 0.5 as a threshold to decide the presence of edges between watermarked nodes. ", "page_idx": 4}, {"type": "text", "text": "3.3 Robust Model Loader (RML) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "This module aims to design a new GL model that is provably robust to the layer-perturbation attack. Towards this end, we design a GL model architecture to incorporate multiple submodels; and devise a majority voting-based ensemble classifier on top of the predictions of these submodels. ", "page_idx": 4}, {"type": "text", "text": "Architecture of the proposed GL model. Intuitively, each client can take a base GL model (e.g., GIN [Xu et al., 2019]) and split it according to the layer indexes to obtain multiple submodels. For instance, a 8-layer GIN can be represented with layer indexes $\\{l_{1},\\cdot\\cdot\\cdot,l_{8}\\}$ . Splitting this GIN into 4 submodels $\\{\\mathrm{GIN}_{1},\\cdot\\cdot\\cdot,\\mathrm{GIN}_{4}\\}$ with layer indexes $\\{l_{1},l_{2}\\},\\cdots,\\{l_{7},l_{8}\\}$ means $\\mathrm{GIN}_{i}$ contains layers $\\{l_{2i-1},l_{2i}\\}$ , from the GIN. However, submodels splitted in this way are coupled from each other, making them unable to defend against layer-perturbation attacks. To tackle this problem, we design the novel GL model $\\theta$ that is an ensemble of a set of $S$ independent submodels $\\{\\vartheta_{1},\\vartheta_{2},\\cdot\\cdot\\cdot,\\bar{\\vartheta_{S}}\\}$ , where each submodel $\\vartheta_{i}$ is a base GL model. This approach can hence be easily adapted to any existing FedGL. Further, to prevent homogeneity, we define varying channels for these submodels to diversify them. Details of the model architecture are shown in Table 8 in Appendix C. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "A majority-voting based ensemble classifier. The designed GL model architecture inspires us to leverage the idea of ensemble classifier, which can combine the predictions of base \u201cweak\u201d classifiers. Specifically, we propose a majority voting-based ensemble classifier to combine the predictions of the submodels. Given a testing graph $G$ and a graph classifier $f$ , we denote the prediction of the submodel $\\vartheta_{i}$ for $G$ as $y=f(\\vartheta_{i},G)\\in\\mathbb{Y}$ . For a GL model $\\theta$ with $S$ submodels $\\{\\vartheta_{1},\\bar{\\vartheta}_{2},\\cdot\\cdot\\cdot,\\vartheta_{S}\\}$ , we can count the submodels that classify $G$ to be $y$ as $\\begin{array}{r}{N_{y}=\\sum_{i=1}^{S}\\!\\mathbb{I}\\!\\left(f(\\vartheta_{i},G)=y\\right)}\\end{array}$ . Then we introduce our majority-voting based ensemble classifier to classify $G$ as: $g(\\theta,G)\\,=\\,\\arg\\operatorname*{max}_{y\\in\\mathbb{Y}}\\,N_{y}$ . In cases of ties, our ensemble classifier $g$ selects the label with a smaller index. ", "page_idx": 5}, {"type": "text", "text": "3.4 Training the Proposed FedGL Model ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The overall training process consists of three iterative steps: 1) training the proposed GL model in all clients; 2) training the CWG module in watermarked clients, i.e., the clients that aim to inject watermarked graphs for protecting the model ownership; and 3) aggregating the clients\u2019 GL models to produce the target watermarked model. The final global model is the learnt watermarked FedGL model. Details of training can be seen in Algorithm 1 in the Appendix. ", "page_idx": 5}, {"type": "text", "text": "Step 1: Training the proposed GL model. Assume we have $T_{w}$ watermarked clients with indexes $[1,\\bar{T}_{w}]$ . For each watermarked client $i$ , we split its training graphs $\\mathbb{G}^{i}$ into the watermarked graphs $\\mathbb{G}_{w}^{i}$ with a target label, say $y_{w}$ , and remaining clean graphs $\\mathbb{G}_{c}^{i}$ , and then customize the watermark for each graph in $\\mathbb{G}_{w}^{i}$ using the CWG module (see Step 2). Given the client\u2019s GL model $\\theta^{i}$ with $S$ submodels $\\{\\vartheta_{1}^{i},\\vartheta_{2}^{i},\\cdots,\\vartheta_{S}^{i}\\}$ , we train each submodel $\\vartheta_{j}^{i}$ via minimizing the loss on $\\mathbb{G}_{c}^{i}$ and $\\mathbb{G}_{w}^{i}$ , i.e., $\\begin{array}{r}{\\vartheta_{j}^{i}=\\arg\\operatorname*{min}_{\\vartheta_{j}^{i}}L(\\vartheta_{j}^{i};\\mathbb{G}_{c}^{i}\\bigcup\\mathbb{G}_{w}^{i})}\\end{array}$ . For an unwatermarked client $k\\in[T_{w}+1,T]$ , we utilize all clean graphs $\\mathbb{G}^{k}$ to train each submodel $\\vartheta_{j}^{k}$ separately, i.e., $\\begin{array}{r}{\\vartheta_{j}^{k}=\\arg\\operatorname*{min}_{\\vartheta_{j}^{k}}L(\\vartheta_{j}^{k};\\mathbb{G}^{k})}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "Step 2: Training the CWG. We denote the parameters of the CWG module for a watermarked client $i\\in[1,T_{w}]$ , as $\\omega^{i}$ . The parameters include two networks, GatingNet and KeyNet. Each client $i$ trains its CWG $\\omega^{i}$ to ensure that the generated watermarks be effective and diverse. Formally, we have $\\begin{array}{r}{\\omega^{i}=\\arg\\operatorname*{min}_{\\omega^{i}}L(\\theta^{i};\\mathbb{G}_{w}^{i}),i\\in\\bar{[1,T_{w}]}}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "Step 3: Aggregating clients\u2019 GL models. The server averages GL models $\\{\\theta^{i}\\}_{i\\in T}$ to produce the global model $\\theta$ , and distributes this model to selected clients in the next iteration. ", "page_idx": 5}, {"type": "text", "text": "3.5 Model Ownership Verification ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "When suspecting the target FedGMark model $\\theta$ is illegally used by others, the model owner (all the participating clients or their representative) can recruit a trusted judge for model ownership verification. Typically, the judge requests both the true model owner and the illegal party to provide some test data for verification. Only when the one knows the predictions by the target model for the provided test data by both parties, the judge will confirm this party the model ownership. In particular, besides providing the clean data $\\overline{{\\mathbb{G}}}_{c}^{i}$ by both parties that behave normally, the true model owner especially provides the designed watermarked data $\\mathbb{G}_{w}^{i}$ that only s/he knows the model behaves on. As a result, both parties know the prediction results on $\\mathbb{G}_{c}^{i}$ , but the illegal party is hard to predict accurately on $\\mathbb{G}_{w}^{i}$ provided by the true model owner. ", "page_idx": 5}, {"type": "text", "text": "3.6 Certified Robustness Guarantees against Layer-Perturbation Attacks ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We show the above design, with any layer-perturbation attack, ensures the predictions of the learnt watermarked FedGL model and its compromised counterpart for the watermarked graphs are consistent, once the number of perturbed layers is bounded. Given the target watermarked FedGL model $\\theta$ and its $S$ submodels $\\{\\vartheta_{1},\\cdot\\cdot\\cdot,\\vartheta_{S}\\}$ , we denote $\\theta^{\\prime}$ as the comprised model and $\\{\\vartheta_{1}^{\\prime},\\cdot\\cdot\\cdot,\\vartheta_{S}^{\\prime}\\}$ as its $S$ submodels. For each watermarked graph $G_{w}$ , we use the ensemble classifier $g$ on submodels\u2019 predictions, i.e., its predictions on $\\theta$ and $\\theta^{\\prime}$ are $g(\\theta,G_{w})=\\arg\\operatorname*{max}_{y\\in\\mathbb{Y}}N_{y}$ , and $g(\\theta^{\\prime},G_{w})=\\arg\\operatorname*{max}_{y\\in\\mathbb{Y}}N_{y}^{\\prime}$ , respectively, where $\\begin{array}{r}{N_{y}=\\sum_{i=1}^{S}\\!\\mathbb{I}\\!\\left(f(\\vartheta_{i},G)=y\\right)}\\end{array}$ and $\\begin{array}{r}{N_{y}^{\\prime}=\\sum_{i=1}^{S}\\!\\mathbb{I}\\big(f(\\vartheta_{i}^{\\prime},G)=y\\big)}\\end{array}$ . Then we have the following result on guaranteeing the number of perturbed layers on the target watermarked model. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1 (Certified number of perturbed layers $r.$ .) Let $\\theta,\\,\\theta^{\\prime}$ , $g_{\\mathrm{:}}$ , and $G_{w}$ be above defined. Suppose $N_{A}$ and $N_{B}$ are the largest and second largest count outputted by $g$ on $G_{w}$ , For any layerperturbation attack, we have $\\bar{g}(\\theta,G_{w})=g(\\theta^{\\prime},G_{w}\\bar{)}$ , when the number of perturbed layers $r$ satisfies: ", "page_idx": 6}, {"type": "equation", "text": "$$\nr\\le r^{*}=\\left(N_{A}-N_{B}+\\mathbb{I}[A<B]-1\\right)/2,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathbb{I}[\\cdot]$ is the indicator function and $r$ is called the certified number of perturbed layers. ", "page_idx": 6}, {"type": "text", "text": "We also show the tightness of our derived $r^{*}$ in the following theorem: ", "page_idx": 6}, {"type": "text", "text": "Theorem 2 (Tightness of $r^{*}$ .) Without using extra information of $f$ , our derived $r^{*}$ in Theorem $^{\\,l}$ is tight. I.e., $r^{*}$ is the maximum number of perturbed layers tolerated by our target watermarked model. The proofs of Theorems 1 and 2 are deferred to Appendix A. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we comprehensively evaluate FedGMark on multiple datasets, FedGL models, attack baselines, and experimental settings. More experimental results and discussions are in Appendix. ", "page_idx": 6}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets and models. We evaluate our FedGMark on four real-world graph datasets for graph classification: MUTAG [Debnath et al., 1991], PROTEINS [Borgwardt et al., 2005], DD [Dobson and Doig, 2003], and COLLAB [Yanardag and Vishwanathan, 2015]. Details about the statistics of those datasets are shown in Table 6 in Appendix C. Following prior work Shen et al. [2022]; Xia et al. [2024], we choose the well-known GIN [Xu et al., 2019], GSAGE [Hamilton et al., 2017], and GCN [Kipf and Welling, 2017] as the GL model. All these network architectures involved in the experiments are detailed in Table 8 in Appendix C. ", "page_idx": 6}, {"type": "text", "text": "Parameter setting. We implement our method using one NVIDIA GeForce GTX 1080 Ti GPU. In FedGL, we use $T=40$ clients in total and train the model 200 iterations. The server randomly selects $50\\%$ clients in each iteration. We define the target label of watermarking graphs as 1, and each participating client randomly selects $10\\%$ graphs with labels not 1 as the watermarking graphs. We extensively validate the effectiveness and robustness of the proposed watermarking method with the following hyperparameters details: the number of submodels $S=\\{4,8,16\\}$ , the number watermarked clients $\\bar{T}_{w}\\,\\,=\\,\\{5,10,20\\}$ (both $S$ and $T_{w}$ are halved on MUTAG due to less data), the watermarked nodes $n_{w}=\\{3,4,5\\}$ , and the number of perturbed layers $r=\\{1,\\cdot\\cdot\\cdot,5\\}$ in the layer-perturbation attack. By default, we set $S=4$ , $T_{w}=10$ , $n_{w}=4$ , $r=1$ . While studying the impact of a hyperparameter, we fix the others as the default value. ", "page_idx": 6}, {"type": "text", "text": "Evaluation metric. We use three metrics for evaluation: the main task accuracy (MA), watermark accuracy (WA), and certified WA $(\\mathbf{CWA}@r)$ . An effective and robust watermarked model is expected to achieve both high MA and WA. CWA evaluates the certified robustness of FedGMark against layer-perturbation attacks. $\\mathrm{CWA@}r$ is defined as the fraction of testing graphs that are provably predicted as the target label, when at most $r$ layers in client models can be arbitrarily perturbed. ", "page_idx": 6}, {"type": "text", "text": "Attack baselines. We evaluate FedGMark against existing watermark removal attacks including distillation and finetuning, and our proposed layer-perturbation attack. In our setting, the layerperturbation attack can replace any layer(s)\u2019 parameters of the target watermarked model with those from the unwatermarked model to maximally reduce the watermark accuracy on its watermarked graphs. When perturbing multiple layers, we utilize a greedy algorithm to decide the perturbed layers\u2014we search for one optimal perturbed layer at each step. Specifically, we first traverse perturbing layers in the watermarked model and find the one that maximally reduces the watermark accuracy. We then search for the remaining layers based on this one and continue the process. ", "page_idx": 6}, {"type": "text", "text": "4.2 Empirical Results: MA and WA ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.2.1 Results under the Default Setting ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We first assess our FedGMark against empirical and layer-perturbation attacks under the default setting. Experimental results are provided in Table 2. We have the following observations: ", "page_idx": 6}, {"type": "text", "text": "\u2022 1) Our FedGMark significantly outperforms the existing method under no attack. Recall in Table 1 that [Xu et al., 2023] obtains $<60\\%$ WAs across all datasets and FedGL models. In contrast, FedGMark can achieve WAs $>$ $70\\%$ in almost all cases, while having similar MAs. ", "page_idx": 6}, {"type": "table", "img_path": "xeviQPXTMU/tmp/64c1cdca77a4188559b1e2db1b91d527bac8462cf538af94e7015bf7c61d413b.jpg", "table_caption": ["Table 2: Results of our FedGMark under empirical watermark removal attacks. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "xeviQPXTMU/tmp/127af9c775f215fa1830c5f963f073ff4d6902b316c535ef7feeacaa76a1a58f.jpg", "img_caption": ["Figure 2: Example learnt watermarks and watermarked graphs by our FedGMark. CWGs generated by different clients produce unique watermarks, characterized by distinct edge connection patterns. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "xeviQPXTMU/tmp/61ba42ecc2c7daa7e0826bb808324b03175c759c25597769f0c31ab14c18bf96.jpg", "table_caption": ["Table 3: Impact of $S$ on FedGMark against our layer-perturbation attack. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Such significant improvements confirm the superiority of our learnt watermarks over the random watermarks in [Xu et al., 2023]. Figure 2 also visualizes example learnt watermarks and we can see these watermarks are diversified due to the proposed CWG. ", "page_idx": 7}, {"type": "text", "text": "\u2022 2) Our FedGMark exhibits resistance to existing empirical attacks. As shown in Table 1, existing methods are vulnerable to distillation and finetuning attacks. In contrast, the WAs of FedGMark under these two attacks are almost the same as those under no attack, demonstrating FedGMark is robust to the existing attacks. ", "page_idx": 7}, {"type": "text", "text": "\u2022 3) Our FedGMark is resilient to the proposed layer perturbation attack. We notice that the existing method has difficulties in defending against the proposed layer-perturbation attacks and shows an unsatisfactory WA, e.g., $<25\\%$ WA in almost all cases under the 1-layer perturbation attack. Conversely, our FedGMark can obtain a close WA compared to that without attack. This verifies our ensemble classifier in FedGMark is capable of resisting the 1-layer perturbation attack. ", "page_idx": 7}, {"type": "text", "text": "4.2.2 Impact of Hyperparameters ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "This section studies the impact of hyperparameters in FedGMark against the layer-perturbation attack. ", "page_idx": 7}, {"type": "text", "text": "Impact of #submodels $S$ . We first examine the impact of $S$ on the performance of FedGMark and report the results against the 1-layer perturbation attack in Table 3. We observe that the learnt watermarked model can resist to the 1-layer perturbation attack under all $S$ and both the MAs and WAs are also similar. This indicates the number of submodels marginally affects the watermarking performance against the 1-layer perturbation attack. ", "page_idx": 7}, {"type": "text", "text": "Table 4: Impact of #perturbed layers on FedGMark against our layer-perturbation attack. Compared with Table 2, the change in MA is less than $4\\%$ in all cases. ", "page_idx": 8}, {"type": "image", "img_path": "xeviQPXTMU/tmp/c3351a6155451211d9ac91bebc22eaea429484a93ed02ca556f898c7ed02bfce.jpg", "img_caption": ["Figure 3: CWA vs. #perturbed layers $r$ in the layer-perturbation attack. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Impact of #watermarking clients $T_{w}$ . The results on $T_{w}=5$ and $T_{w}=20$ are shown in Figure 4 in Appendix. We see that: the larger number of clients generating watermarks, the better our watermarking performance. For instance, the WA improves by $11\\%$ , $9\\%$ , and $7\\%$ , respectively on the three FedGL models on PROTEINS when $T_{w}$ increases from 5 to 20. This is because more watermark clients (thus more watermarked graphs) can ensure the FedGL model better learns the relation between the watermarks and the target label. ", "page_idx": 8}, {"type": "text", "text": "Impact of the watermark size $n_{w}$ . We also investigate $n_{w}=3$ and $n_{w}\\,=5$ , and the results are presented in Figure 5 in Appendix. Similarly, FedGMark obtains higher WA with larger $n_{w}$ . This is because a larger watermark size can facilitate the trained FedGL model better learn the watermark. ", "page_idx": 8}, {"type": "text", "text": "Impact of #perturbed layers. Finally, Table 4 shows the results of FedGMark vs. different #perturbed layers. We observe that: 1) The WA decreases as increasing the #perturbed layers. This is because the attacker has more attack capability by perturbing more layers. 2) Despite the theoretically guaranteed CWA being 0 at #perturbed layers $\\geq3$ as later depicted in the Figure 3, our empirical watermarking performance can still achieves WA from $48\\%$ to $58\\%$ . Note that the change in MA is $<4\\%$ in all cases, compared with Table 2. ", "page_idx": 8}, {"type": "text", "text": "4.2.3 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "table", "img_path": "xeviQPXTMU/tmp/dbca7a4f96a4bd195d8a97ff0cf2be3d3285118b17d5ad8612986f82e4c050b0.jpg", "table_caption": ["Table 5: Impact of different modules in FedGMark. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "We investigate the contribution of each module in FedGMark, including the GatingNet and KeyNet in CWG, and the submodels used in RML. The results under the default setting and no attack are shown in Table 5. First, GatingNet plays a crucial role on generating more ", "page_idx": 8}, {"type": "text", "text": "effective watermarks and results in an improvement in WA from $8\\%$ to $21\\%$ . Second, KeyNet facilitates watermark differentiation between clients, leading to an improve on WA from $5\\%$ to $8\\%$ . Third, the submodels in RML has a negligible impact on WA/MA. However, it is important to defend against the layer-perturbation attack, as shown in Section 4.3. ", "page_idx": 8}, {"type": "text", "text": "4.3 Certified Robustness Results: Certified WA ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this section, we evaluate the certified robustness of FedGMark against the layer perturbation attack. The CWAs on the three FedGL models and four datasets are depicted in Figure 3. We have several observations: 1) FedGMark achieves promising provable robustness results against the worst-case layer perturbation attack, when the #perturbed layers is within the certified range in Eqn (1). For instance, when $S=4$ and $0<r\\le2$ , CWA is close to WA without attack $(r=0,$ ). 2) As $S$ increases, the certified #perturbed layers also increases, showing that a more number of submodules in RML can better provably defend against the layer-perturbation attack. For instance, when $S=16$ , the CWA is $86\\%$ on MUTAG even when 5 any layers in the global model are arbitrarily perturbed, while it is 0 when $S=4$ . However, this is at the cost of requiring more computational resources (e.g., GPU memory, runtime). As shown in Table 10 in Appendix, the runtime is linearly to $S$ . ", "page_idx": 9}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Backdoor-based watermarking for centralized models on non-graph data. Many backdoor-based watermarking methods [Lv et al., 2023; Yan et al., 2023; Tekgul et al., 2021; Yang et al., 2023; Li et al., 2022a; Shao et al., 2022; Lansari et al., 2023] have been proposed that can empirically protect the model ownership. These methods mainly focus on centralized learning models on non-graph (e.g., image) data. Compared with non-graph data, graph data have unique graph structure information, e.g., entities are connected by links. Similarly, compared with centralized models, FL models are collaboratively trained by multiple clients, which could have their own uniqueness. Bansal et al. [2022] is the first watermarking method for centralized non-graph models with certified guarantee. However, its certified robustness performance is unsatisfactory. ", "page_idx": 9}, {"type": "text", "text": "Backdoor-based watermarking for FL models on non-graph data. A few recent works Tekgul et al. [2021]; Li et al. [2022a]; Bansal et al. [2022] design backdoor-based watermarks for protecting the ownership of FL models on non-graph data, where the watermark can be injected into client\u2019s data or server\u2019s validation data. For instance, Tekgul et al. [2021] presented WAFFLE, an approach to watermark DNN models by incorporating a re-training step via watermarked data at the server. Li et al. [2022a] leveraged each client\u2019s private watermark to verify FL model ownership, ensuring non-conflicting watermarks across different clients. However, these techniques cannot be directly applied to graph data, as they often require fixed input data, while graph data often have varying sizes. Moreover, all these methods do not provide guaranteed watermarking performance under the attack. ", "page_idx": 9}, {"type": "text", "text": "Backdoor-based watermarking for centralized GL models on graph data. The only work [Xu et al., 2023] handling graph data uses random graph as the watermark. This method can be extended to the FedGL model, but its watermarking performance is far from satisfactory\u2013especially vulnerable to watermark removal attacks such as distillation, finetuning, and the layer perturbation attack. ", "page_idx": 9}, {"type": "text", "text": "Backdoor attacks for centralized and federated GL models on graph data. Several works [Zhang et al., 2021; Yang et al., 2024] design backdoor attacks to manipulate GL models, enabling the attacker to influence the learned model to serve its purpose\u2013the model will predict the attacker-chosen label for test graphs once they contain a predefined trigger. For instance, Zhang et al. [2021] uses random subgraphs as the trigger to backdoor centralized GL models, while Yang et al. [2024] learns subgraph trigger to backdoor FedGL models. Note that the goal of these works is orthogonal to ours. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We protect the model ownership of emerging FedGL trained on distributed graph data, and use the de facto backdoor-based watermarking method. We develop the first certifiably robust backdoor-based watermarking method FedGMark for FedGL. FedGMark demonstrates the capability of achieving high empirical watermarking performance under no attack, under existing backdoor removal attacks and the proposed stronger layer-perturbation attack. FedGMark is also provably robust against the worst-case layer-perturbation attack, once the number of perturbed layers is bounded by Theorem 1. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments. We thank all anonymous reviewers for the constructive comments. Li is partially supported by the National Natural Science Foundation of China under Grant No. 62072208, Key Research and Development Projects of Jilin Province under Grant No. 20240302090GX. Hong is partially supported by the National Science Foundation under grant No. CNS-2302689, CNS2308730, CNS-2319277 and CMMI-2326341. Wang is partially supported by the National Science Foundation under grant No. ECCS-2216926, CNS-2241713, CNS-2331302 and CNS-2339686. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Yossi Adi, Carsten Baum, Moustapha Cisse, Benny Pinkas, and Joseph Keshet. Turning your weakness into a strength: Watermarking deep neural networks by backdooring. In 27th USENIX Security Symposium (USENIX Security 18), pages 1615\u20131631, 2018.   \nJinheon Baek, Wonyong Jeong, Jiongdao Jin, Jaehong Yoon, and Sung Ju Hwang. Personalized subgraph federated learning. In International Conference on Machine Learning, pages 1396\u20131415. PMLR, 2023.   \nEugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to backdoor federated learning. In International conference on artificial intelligence and statistics, pages 2938\u20132948. PMLR, 2020.   \nArpit Bansal, Ping-yeh Chiang, Michael J Curry, Rajiv Jain, Curtis Wigington, Varun Manjunatha, John P Dickerson, and Tom Goldstein. Certified neural network watermarks with randomized smoothing. In International Conference on Machine Learning, pages 1450\u20131465. PMLR, 2022.   \nPeva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer. Machine learning with adversaries: Byzantine tolerant gradient descent. Advances in neural information processing systems, 30, 2017.   \nKarsten M Borgwardt, Cheng Soon Ong, Stefan Sch\u00f6nauer, SVN Vishwanathan, Alex J Smola, and Hans-Peter Kriegel. Protein function prediction via graph kernels. Bioinformatics, 21(suppl_1):i47\u2013 i56, 2005.   \nAsim Kumar Debnath, Rosa L Lopez de Compadre, Gargi Debnath, Alan J Shusterman, and Corwin Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity. Journal of medicinal chemistry, 34(2):786\u2013797, 1991.   \nPaul D Dobson and Andrew J Doig. Distinguishing enzyme structures from non-enzymes without alignments. Journal of molecular biology, 330(4):771\u2013783, 2003.   \nEdgar N Gilbert. Random graphs. The Annals of Mathematical Statistics, 30(4):1141\u20131144, 1959.   \nWill Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. Advances in neural information processing systems, 30, 2017.   \nChaoyang He, Emir Ceyani, Keshav Balasubramanian, Murali Annavaram, and Salman Avestimehr. Spreadgnn: Decentralized multi-task federated learning for graph neural networks on molecular data. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 6865\u2013 6873, 2022.   \nZhengyuan Jiang, Minghong Fang, and Neil Zhenqiang Gong. Ipcert: Provably robust intellectual property protection for machine learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3612\u20133621, 2023.   \nSai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In International conference on machine learning, pages 5132\u20135143. PMLR, 2020.   \nThomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations (ICLR), 2017.   \nMohammed Lansari, Reda Bellafqira, Katarzyna Kapusta, Vincent Thouvenot, Olivier Bettan, and Gouenou Coatrieux. When federated learning meets watermarking: A comprehensive overview of techniques for intellectual property protection. Machine Learning and Knowledge Extraction, 5(4):1382\u20131406, 2023.   \nTian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated learning through personalization. In International conference on machine learning, pages 6357\u2013 6368. PMLR, 2021.   \nBowen Li, Lixin Fan, Hanlin Gu, Jie Li, and Qiang Yang. Fedipr: Ownership verification for federated deep neural network models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(4):4521\u20134536, 2022.   \nZheng Li, Muhammad Bilal, Xiaolong Xu, Jielin Jiang, and Yan Cui. Federated learning-based cross-enterprise recommendation with graph neural networks. IEEE Transactions on Industrial Informatics, 19(1):673\u2013682, 2022.   \nPeizhuo Lv, Pan Li, Shengzhi Zhang, Kai Chen, Ruigang Liang, Hualong Ma, Yue Zhao, and Yingjiu Li. A robustness-assured white-box watermark in neural networks. IEEE Transactions on Dependable and Secure Computing, 2023.   \nBrendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pages 1273\u20131282. PMLR, 2017.   \nLiang Peng, Nan Wang, Nicha Dvornek, Xiaofeng Zhu, and Xiaoxiao Li. Fedni: Federated graph learning with network inpainting for population-based disease prediction. IEEE Transactions on Medical Imaging, 2022.   \nAniruddha Saha, Akshayvarun Subramanya, and Hamed Pirsiavash. Hidden trigger backdoor attacks. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 11957\u201311965, 2020.   \nMasoumeh Shafieinejad, Nils Lukas, Jiaqi Wang, Xinda Li, and Florian Kerschbaum. On the robustness of backdoor-based watermarking in deep neural networks. In Proceedings of the 2021 ACM workshop on information hiding and multimedia security, pages 177\u2013188, 2021.   \nShuo Shao, Wenyuan Yang, Hanlin Gu, Zhan Qin, Lixin Fan, Qiang Yang, and Kui Ren. Fedtracker: Furnishing ownership verification and traceability for federated learning model. arXiv preprint arXiv:2211.07160, 2022.   \nYun Shen, Xinlei He, Yufei Han, and Yang Zhang. Model stealing attacks against inductive graph neural networks. In 2022 IEEE Symposium on Security and Privacy $(S P)$ , pages 1175\u20131192. IEEE, 2022.   \nYue Tan, Yixin Liu, Guodong Long, Jing Jiang, Qinghua Lu, and Chengqi Zhang. Federated learning on non-iid graphs via structural knowledge sharing. In Proceedings of the AAAI conference on artificial intelligence, volume 37, pages 9953\u20139961, 2023.   \nBuse GA Tekgul, Yuxi Xia, Samuel Marchal, and N Asokan. Waffle: Watermarking in federated learning. In 2021 40th International Symposium on Reliable Distributed Systems (SRDS), pages 310\u2013320. IEEE, 2021.   \nYusuke Uchida, Yuki Nagai, Shigeyuki Sakazawa, and Shin\u2019ichi Satoh. Embedding watermarks into deep neural networks. In Proceedings of the 2017 ACM on international conference on multimedia retrieval, pages 269\u2013277, 2017.   \nVidya et al. Fedml supports several out-of-the-box deep learning algorithms for various data types, such as tabular, text, image, graphs, and internet of things (iot) data. https://aws.amazon.com/blogs/machine-learning/ part-2-federated-learning-on-aws-with-fedml-health-analytics-without-shar ing-sensitive-data/.   \nHongyi Wang, Kartik Sreenivasan, Shashank Rajput, Harit Vishwakarma, Saurabh Agarwal, Jy-yong Sohn, Kangwook Lee, and Dimitris Papailiopoulos. Attack of the tails: Yes, you really can backdoor federated learning. Advances in Neural Information Processing Systems, 33:16070\u2013 16084, 2020.   \nBinghui Wang, Ang Li, Meng Pang, Hai Li, and Yiran Chen. Graphf:l A federated learning framework for semi-supervised node classification on graphs. In 2022 IEEE International Conference on Data Mining (ICDM), pages 498\u2013507. IEEE, 2022.   \nZhen Wang, Weirui Kuang, Yuexiang Xie, Liuyi Yao, Yaliang Li, Bolin Ding, and Jingren Zhou. Federatedscope-gnn: Towards a unified, comprehensive and efficient package for federated graph learning. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 4110\u20134120, 2022.   \nPeter Wills and Fran\u00e7ois G Meyer. Metrics for graph comparison: a practitioner\u2019s guide. Plos one, 15(2):e0228728, 2020.   \nChuhan Wu, Fangzhao Wu, Lingjuan Lyu, Tao Qi, Yongfeng Huang, and Xing Xie. A federated graph neural network framework for privacy-preserving personalization. Nature Communications, 13(1):3091, 2022.   \nZaishuo Xia, Han Yang, Binghui Wang, and Jinyuan Jia. Gnncert: Deterministic certification of graph neural networks against adversarial perturbations. In International Conference on Learning Representations, 2024.   \nHan Xie, Jing Ma, Li Xiong, and Carl Yang. Federated graph classification over non-iid graphs. Advances in neural information processing systems, 34:18839\u201318852, 2021.   \nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019.   \nJing Xu, Stefanos Koffas, Og\u02d8uzhan Ersoy, and Stjepan Picek. Watermarking graph neural networks based on backdoor attacks. In 2023 IEEE 8th European Symposium on Security and Privacy (EuroS&P), pages 1179\u20131197. IEEE, 2023.   \nYifan Yan, Xudong Pan, Mi Zhang, and Min Yang. Rethinking $\\{\\mathrm{White}{-}\\mathrm{Box}\\}$ watermarks on deep learning models under neural structural obfuscation. In 32nd USENIX Security Symposium (USENIX Security 23), pages 2347\u20132364, 2023.   \nPinar Yanardag and SVN Vishwanathan. Deep graph kernels. In Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, pages 1365\u20131374, 2015.   \nWenyuan Yang, Shuo Shao, Yue Yang, Xiyao Liu, Ximeng Liu, Zhihua Xia, Gerald Schaefer, and Hui Fang. Watermarking in secure federated learning: A verification framework based on client-side backdooring. ACM Transactions on Intelligent Systems and Technology, 15(1):1\u201325, 2023.   \nYuxin Yang, Qiang Li, Jinyuan Jia, Yuan Hong, and Binghui Wang. Distributed backdoor attacks on federated graph learning and certified defenses. In Proceedings of the 2024 ACM SIGSAC Conference on Computer and Communications Security, 2024.   \nYuhang Yao, Weizhao Jin, Srivatsan Ravi, and Carlee Joe-Wong. Fedgcn: Convergencecommunication tradeoffs in federated training of graph convolutional networks. Advances in Neural Information Processing Systems, 36, 2024.   \nDong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Byzantine-robust distributed learning: Towards optimal statistical rates. In International conference on machine learning, pages 5650\u20135659. Pmlr, 2018.   \nZaixi Zhang, Jinyuan Jia, Binghui Wang, and Neil Zhenqiang Gong. Backdoor attacks to graph neural networks. In SACMAT, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proof of Theorem 1: Given a watermarked graph $G_{w}$ , a base graph classifier $f$ , our ensemble classifier $g$ , and the target watermarked FedGL model $\\theta\\,=\\,\\{\\vartheta_{i}\\bar{\\}_{i=1}^{S}$ . Let $\\theta^{\\prime}$ be the watermarked FedGL model after the layer-perturbation attack. We denote by $y=f(\\vartheta_{i},G_{w})$ the prediction of $G_{w}$ by submodel $i$ , and $g(\\theta,G_{w})=\\arg\\operatorname*{max}_{y\\in\\mathbb{Y}}\\,N_{y}$ the prediction of $G_{w}$ by the target model $\\theta$ , where $\\begin{array}{r}{N_{y}=\\sum_{i=1}^{S}\\!\\mathbb{I}(f(\\vartheta_{i},G_{w})=y)}\\end{array}$ and $\\boldsymbol{\\theta}=\\{\\boldsymbol{\\vartheta}_{i}\\}_{i=1}^{S}$ . Assume $N_{A}$ and $N_{B}$ are the largest and second larges t votes of the output of the ensemble classifier $g$ on the target model $\\theta$ and $G_{w}$ , respectively. Similarly, let $N_{A}^{\\prime}$ and $N_{B}^{\\prime}$ respectively denote the corresponding votes of the model $\\theta^{\\prime}$ . Under our GL model architecture, each perturbed layer affects at most 1 submodel in the worst case. Therefore, $N_{A}^{\\prime}$ and $N_{B}^{\\prime}$ satisfy the following equations when $r$ layers in $\\theta$ are perturbed: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{N_{A}-r\\leq N_{A}^{\\prime}\\leq N_{A}+r,}}\\\\ {{N_{B}-r\\leq N_{B}^{\\prime}\\leq N_{B}+r.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Thus, $g(\\theta^{\\prime},G)=g(\\theta,G)$ when $N_{A}^{\\prime}\\,>\\,N_{B}^{\\prime}$ , This means $N_{A}-r>N_{B}+r-\\mathbb{1}(A<B)$ , where $\\mathbb{I}(A<B)$ indicates we select a label with a smaller index in case of ties. Hence, $r$ satisfies: ", "page_idx": 13}, {"type": "equation", "text": "$$\nr\\leq r^{\\ast}=\\frac{N_{A}-N_{B}+\\mathbb{I}(A<B)-1}{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof of Theorem 2: We use proof by contradiction, i.e., when $r>r^{*}$ , we can build a base classifier $f^{\\prime}$ such that there exists $g(\\theta,\\bar{G}_{w})\\neq g(\\theta^{\\prime},G_{w})$ . We select the smallest $r=r^{*}+1$ for simplicity, indicating that $r^{*}+1$ layers whose model parameters can be arbitrarily perturbed. Assume there exist a graph classifier $f^{\\prime}$ under which, $(r^{*}+1)$ submodels whose original predictions on $G_{w}$ are the label $A$ and now become $B$ after $r^{*}+1$ layers are perturbed. Then we have $N_{A}^{\\prime}=N_{A}-\\left(r^{*}+1\\right)$ and $N_{B}^{\\prime}=N_{B}+(r^{*}+1)$ . Since $r^{*}$ is the largest number such that Equation (2) is satisfied, i.e., $N_{A}-r^{*}>N_{B}-\\mathbb{1}(A<B)+r^{*}$ holds. Thus, ", "page_idx": 13}, {"type": "equation", "text": "$$\nN_{A}-(r^{*}+1)\\leq N_{B}-\\mathbb{1}(A<B)+(r^{*}+1).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Or $N_{A}^{\\prime}\\leq N_{B}^{\\prime}$ . Hence, $g(\\theta,G_{w})=A\\neq B=g(\\theta^{\\prime},G_{w}),$ , if $A>B$ . ", "page_idx": 13}, {"type": "text", "text": "B Algorithm 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Algorithm 1 The training process of FedGMark ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Input: clients $[1,T]$ , watermarked clients $[1,T_{w}]$ , training graphs $\\mathbb{G}^{i}$ $(i\\in[1,T])$ , initial global model $\\theta_{1}$ , initial   \nCWG $\\omega_{1}$ , #Perturbed Layers $r$ , unique key $k^{i}$ for client $i$ $(i\\in[1,T_{w}])$ , the number of submodels $S$ .   \nOutput: Target global model $\\theta_{e p o c h}$ , and target local models $\\theta_{e p o c h}^{j}(j\\in[1,T_{w}])$ .   \n1: for each epoch $e$ in $[1,e p o c h]$ do   \n2: for each client $i\\in[1,T]$ do   \n3: if $i\\in[1,T_{w}]$ then   \n4: Divide $\\mathbb{G}^{i}$ into $\\mathbb{G}_{c}^{i}$ and $\\mathbb{G}_{w}^{i}$ . $\\vartriangleright$ clean & watermarked samples   \n5: $\\mathbb{G}_{w}^{i}=\\mathrm{CWG}(\\mathbb{G}_{w}^{i},k^{i})$   \n6: end if   \n7: $\\vartheta_{1},\\vartheta_{2},\\cdot\\cdot\\cdot\\mathbf{\\varepsilon},\\vartheta_{S}=\\mathrm{divide}(\\theta_{e})$   \n8: for each submodel $j$ in $[1,S]$ do   \n9: $\\vartheta_{j}^{i}=\\arg\\operatorname*{min}_{\\vartheta_{j}^{i}}L\\big(\\vartheta_{j};\\mathbb{G}^{i}\\big)$   \n10: end for   \n11: $\\begin{array}{r}{\\theta_{e}^{i}=\\operatorname*{joint}(\\vartheta_{1}^{i},\\vartheta_{2}^{i},\\cdot\\cdot\\cdot\\cdot,\\vartheta_{S}^{i})}\\\\ {\\mathbf{if}\\,i\\in[1,T_{w}]\\,\\mathbf{then}}\\\\ {\\omega_{e}^{i}=\\underset{\\omega^{i}}{\\mathrm{arg}\\,\\mathrm{min}}\\,L(\\theta_{e}^{i};\\mathbb{G}_{w}^{i})}\\end{array}$   \n12:   \n13:   \n14: end if   \n15: end for   \n16: $\\begin{array}{r}{\\theta_{e+1}=\\frac{1}{|\\mathbb{T}_{e}|}{\\sum_{i\\in\\mathbb{T}_{e}}\\theta_{e}^{i}}\\triangleright}\\end{array}$ Server selects $\\mathbb{T}_{e}$ clients for aggregation   \n17: end for ", "page_idx": 13}, {"type": "table", "img_path": "xeviQPXTMU/tmp/d14949ef300204fa123d66a62e2097eea2fd2ca2bbcc5fafe7dd666e129ba7cd.jpg", "table_caption": ["Table 6: Statistics of datasets. "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "xeviQPXTMU/tmp/7d3d13c1b11f4dc5a1436dfd0e70628d898a56be19ba0331d02e4be84f3888ed.jpg", "table_caption": ["Table 7: Detailed network architectures of CWG. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Table 8: Detailed network architectures for $S=\\{4,8,16\\}$ in Fed-GIN, Fed-GSAGE, and Fed-GCN models within our watermark. The convolutional (conv) layers in these three models are GINConv, GSAGEConv, and GCNConv, respectively. Here, $x$ represents the input dimension, while $y$ represents the output dimension. BN is short for BatchNorm. ", "page_idx": 14}, {"type": "table", "img_path": "xeviQPXTMU/tmp/84180b61ab6ba271755231f1c995abe5ae17352cf247c255227a56a51312f9c3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "C More Experimental Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table 6 shows the statistics of the used graph datasets. Table 7 shows the network architectures of the CWG component. Table 8 shows the detailed network architectures of Fed-GIN, Fed-GSAGE, and Fed-GCN models. ", "page_idx": 14}, {"type": "image", "img_path": "xeviQPXTMU/tmp/0c7dfaefe6bd3bf0630a5bf9edfbdb3fbf5f7eeabb12936f4c6c8ccd411ff661.jpg", "img_caption": ["MA WA Fed-GIN 1Fed-GSAGE 1Fed-GCN ", "Figure 4: Impact of $T_{w}$ on FedGMark against prior watermark removal and layer-perturbation attacks. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "xeviQPXTMU/tmp/2eca0295ac086b1061bee7ce76620d5655c3c8dfa5bafbd24b947789430ef68d.jpg", "img_caption": ["Figure 5: Impact of $n_{w}$ on FedGMark against prior watermark removal and layer-perturbation attacks. "], "img_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "xeviQPXTMU/tmp/9626493f9ab132e6afa537a9f7c5429da5a9a9e2425a10c5c23169854907d8d5.jpg", "table_caption": ["Table 9: Effect of learning rate $(l r)$ and #local epochs $(l e)$ . "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "D More Experimental Results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Impact of #watermarking clients $T_{w}$ and watermark size $n_{w}$ . See results in Figure 4 and Figure 5, respectively. We can see FedGMark achieves better watermarking performance with a larger number of clients generating watermarks; and a larger watermark size. ", "page_idx": 15}, {"type": "text", "text": "Impact of learning rate $(l r)$ and #local epochs $(l e)$ . The results with varying $l r$ and $l e$ are shown in Table 9 in Appendix. We can see that a large $l r$ may reduce WA, and WA increases slightly as $l e$ grows, indicating more thorough training makes our method perform better. ", "page_idx": 15}, {"type": "text", "text": "Scalability of FedGMark. Compared with graph-based or non-robust watermarking methods, the computation overhead of FedGMark is mainly from the introduced submodel models (fixing all the other parameters, such as #clients, #iterations, to be the same). Particularly, the overhead scales linearly with the number of submodels $S$ , and the runtime results are shown in Table 10. ", "page_idx": 15}, {"type": "text", "text": "FedGMark under watermark/backdoor detection attacks. Many existing works [Zhang et al., 2021] show the trigger-reverse based backdoor detection is ineffective to \u201cstealthy\u201d backdoor. This is because the effectiveness of trigger reverse attacks largely depends on the statistical differences between clean data and backdoored data. Since we do not notice any graph backdoor trigger-reverse attack, we instead propose to quantitatively test the structure similarity between the generated watermarked graphs and the clean graphs. Here we use the metrics NetSim and DeltaCon proposed in [Wills and Meyer, 2020], with the range $[0,1]$ and the higher value the larger similarity. As shown in Table 11, we observe the watermarked graphs and their clean counterparts are structurally very close. This implies that the proposed watermarks are hard to be detected. ", "page_idx": 15}, {"type": "text", "text": "Table 10: Number of submodels $S$ vs. the runtime of RML. We observe an almost linear relationship between the total training time $C_{t}$ and the number of submodels $S$ , expressed as $C_{t}\\approx S*C_{s}$ , where $C_{s}$ denotes the training time for an individual submodel. ", "page_idx": 16}, {"type": "table", "img_path": "xeviQPXTMU/tmp/6015ac4fc24df16c8a6e327117ff1691135ba81ca869ead059f2760323f9ce42.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "xeviQPXTMU/tmp/bc78764658634e86f44ef681f170fb75ac8f9518aa2f6279fa53bbe1288b41cd.jpg", "table_caption": ["Table 11: Structure similarity of generated watermarked graphs and clean graphs. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "xeviQPXTMU/tmp/1fbace9ab6520f7d292e6dd5509c623d54615de8eef208c6e565e76ea63a26f6.jpg", "table_caption": ["Table 12: Results of FedGMark on IID and non-IID/heterogeneous datasets. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "xeviQPXTMU/tmp/457ba7d5506e6f01618fcbb797ca1131852e0a5e581075cbe0f504d21a22b02e.jpg", "table_caption": ["Table 13: Results of FedGMark against $p\\%$ malicious clients whose watermarked data are mislabeled. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "FedGMark on non-IID/heterogeneous datasets. Recall that the local watermarks in FedGMark are learnt by considering the unique properties in each client. Such unique properties may include the heterogeneity across clients\u2019 data. To validate this, we also test FedGMark with non-IID graphs across clients and show results in Table 12, where each client holds a single label data. We observe that FedGMark demonstrates strong performance on non-IID datasets, indicating that the learnt customized watermarks effectively capture the heterogeneity of clients\u2019 graphs. ", "page_idx": 16}, {"type": "text", "text": "FedGMark against more knowledgeable/stronger adversaries. In our threat model, we assume all clients and the server are benign and assume the attacker does not know our CWG. Here, we test FedGMark against stronger adversaries, where some clients are malicious and the these malicious clients also has access to the CWG component to manipulate the FedGL training. First, we consider a passive attack where all malicious clients do not use CWG to generate customized local watermarks. Model (b) in Table 5 shows the maximum WA decrease is $9\\%$ , where all clients do not use CWG. ", "page_idx": 16}, {"type": "text", "text": "Second, we test an active attack where malicious clients modify their watermark data\u2019s label to obfuscate the training. Specifically, all malicious clients\u2019 watermark data are labeled (e.g., 2) differently from the target label (e.g., 1) and then follow the federated training. The results in Table 13 show MA/WA is marginally affected even with $20\\%$ malicious clients. ", "page_idx": 16}, {"type": "text", "text": "FedGMark with alternative aggregation methods. The current FedGMark\u2019s evaluation focuses on FedAvg for aggregating client models. Here, we evaluate FedGMark using Multi-Krum [Blanchard et al., 2017] and Trim-mean [Yin et al., 2018] aggregation methods that consider data quality (e.g., remove outlier clients). Specifically, Multi-Krum filters a set of $p$ clients whose gradients largely deviated from others, while Trim-mean trims off $q$ highest and lowest values for each parameter in clients\u2019 models. Table 14 shows the results with $p=10$ and $q=10$ . We can see these robust aggregators achieve a robustness-utility tradeoff, and MA and WA are not largely different. ", "page_idx": 16}, {"type": "text", "text": "Table 14: Results of our FedGMark against Multi-Krum [Blanchard et al., 2017] and Trim-mean [Yin et al., 2018] aggregation methods. ", "page_idx": 17}, {"type": "table", "img_path": "xeviQPXTMU/tmp/25e35db30d401d2d965e7a60582639d1bfad4cf64072e469057b51b965804aa6.jpg", "table_caption": ["Table 15: FedGMark with alternative triggers. "], "table_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "xeviQPXTMU/tmp/0f918650e0cedf8446a2d7e21c4edbf081d29d9bcc66481f39676242747b6959.jpg", "img_caption": ["Figure 6: Global watermark vs. local watermarks 1-4. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "FedGMark with alternative triggers such as feature-based triggers and hybrid triggers. The current FedGMark relies on graph structural information to learn the trigger used in watermark. Here, we adjust FedGMark to learn feature-based triggers and hybrid feature-structure triggers. ", "page_idx": 17}, {"type": "text", "text": "To learn feature-based triggers, we first select a set of nodes from a graph as the target nodes, and learn the watermarked features for the target nodes (we do not watermark structure). We use a graph $G^{i}=(\\mathbb{V}^{i},\\mathbb{E}^{i},{\\mathbf{X}}^{i})$ from client $i$ for illustration, where $\\mathbf{X}^{i}$ is the node feature matrix. We then define a feature-mask $\\mathbf{M}_{f}^{i}[v_{j}]=1$ if $v_{j}\\in\\mathbb{V}_{w}^{i}$ and 0 otherwise, where $\\mathbb{V}_{w}^{i}$ is the watermark node set described in the paper. Then, we introduce a feature network (FeaNet) that learns watermarked node features as $\\mathbf{X}_{w}^{i}=\\operatorname{FeaNet}(\\mathbf{X}^{i})\\odot\\mathbf{M}_{f}^{i}$ . The FeaNet takes input $\\mathbf{X}^{i}$ and outputs a matrix having the same size as $\\mathbf{X}^{i}$ , e.g., it has the same architecture as GatingNet but adjusts the input size. The corresponding watermarked graph is defined as $G_{w}^{i}=(\\mathbb{V}^{i},\\mathbb{E}^{i},{\\mathbf{\\tilde{X}}}_{w}^{i})$ . By generating a set of watermarked graphs $G_{w}^{i}$ for client $i$ , we minimize the loss on client $i$ \u2019s both clean graphs $\\Bar{G}_{c}^{i}$ and $G_{w}^{i}$ . ", "page_idx": 17}, {"type": "text", "text": "Further, to learn feature-structure triggers, we combine FeaNet (that gets $\\mathbf{X}_{w}^{i})$ with GatingNet/KeyNet (that gets $\\mathbb{E}_{w\\,.}^{i}$ ), and the watermarked graphs are $G_{w}^{i}\\,=\\,(\\mathbb{V}^{i},\\mathbb{E}_{w}^{i},{\\bar{\\mathbf{X}}}_{w}^{i})$ . We then minimize the loss on $G_{c}^{i}$ and $G_{w}^{i}$ . More details about training refer to Section 3.4. ", "page_idx": 17}, {"type": "text", "text": "We evaluate these triggers and the results are in Table 15. We observe that structure information alone is sufficient to enable designing effective triggers. ", "page_idx": 17}, {"type": "text", "text": "E Discussion ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Global watermark vs. local watermarks. In this paper, each client employs CWG to generate watermarks (i.e., local watermarks) to train the local model. During testing, we can use either local watermarks or a global watermark, i.e., fully connect local watermarks. Here, we further evaluate the impact of global watermark vs. local watermarks of the target watermarked model under default settings. For better clarity, we randomly select four local watermarks labeled as local watermarks 1 to 4. The experimental results depicted in Figure 6 demonstrate that, despite not being part of the training process, the global watermark performs slightly better than local watermarks. This implies that the distributed training of FedGL can aggregate the effects of local watermarks, providing a foundation for utilizing the global watermark for verification. ", "page_idx": 17}, {"type": "table", "img_path": "xeviQPXTMU/tmp/9faea771ba96e20db990be880fbba451652524a8a317b9ae40d63d780901e241.jpg", "table_caption": ["Table 16: FedGMark with confidence scores summation against layer-perturbation attack. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "xeviQPXTMU/tmp/f5fe346f6f6ebd0e9d96e773004728c4c4bf20ed3aa252ee30ae1f39a72eb113.jpg", "table_caption": ["Table 17: MA/WA on synthesized graphs for watermarking. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Ensemble classifier with majority voting vs. summation on submodels\u2019 outputs. In this paper, we propose an ensemble classifier $g$ to aggregate the predictions of each submodel $\\vartheta_{i}$ through a voting mechanism and classify a graph $G$ as $g(\\theta,G)=\\arg\\operatorname*{max}_{y\\in\\mathbb{Y}}N_{y}$ , where $\\begin{array}{r}{N_{y}=\\sum_{i=1}^{S}\\mathbb{I}(f(\\vartheta_{i},G)=}\\end{array}$ $y)$ . Another common strategy, as used in GL models such as GIN [Xu et al., 2019], is to sum the prediction confidence vector of each submodel $\\vartheta_{i}$ , i.e., $g(\\theta,G)=\\arg\\operatorname*{max}_{y\\in\\mathbb{Y}}\\mathbf{f}_{G}$ , where $\\mathbf{f}_{G}=$ $\\textstyle\\sum_{i=1}^{S}\\mathbf{f}\\big(\\vartheta_{i},G\\big)$ and f outputs a probabilistic confidence vector whose summation is 1. Here, we also test our FedGMark with this strategy against the 1-layer perturbation attack, and the results are shown in Table 16. We notice the results do not remain consistent across different numbers of submodels and are worse than those in Table 3, especially when $S$ is small. This suggests that the summing-based ensemble classifier is not robust to the layer-perturbation attack, emphasizing the necessity of our proposed majority-voting based ensemble classifier. ", "page_idx": 18}, {"type": "text", "text": "FedGMark against offilne clients that cannot provide watermark data or malicious clients that provide fake watermark samples during ownership verification. We deem that our ownership verification is still robust against offilne clients and malicious clients, if its number is less than $50\\%$ . During ownership verification, each client provides its own watermark data to the trusted judge. When some clients are offline, the trusted judge can simply neglect them and only use participating clients\u2019 watermark data for verification. ", "page_idx": 18}, {"type": "text", "text": "When facing malicious clients, their negative effect can be circumvented through a majority votingbased approach. Specifically, all clients provide their own watermark data to the judge and obtain the watermark accuracy per client. Though the watermark accuracy on malicious clients could be very low, the majority of benign clients can produce more number of high watermark accuracy, compared to the number of low accuracy. When the judge uses the majority-vote strategy, the final watermark accuracy is still high, ensuring the accurate ownership claim for benign clients. ", "page_idx": 18}, {"type": "text", "text": "Using clients\u2019 training graphs as the watermark samples and sending them to the trust judge may lead to privacy leakage. We clarify that the watermark data are not necessarily generated from the training/test samples. Remember the primary goal of backdoor-based watermarking is to force the model to memorize the relationship between the backdoor trigger (in the watermark samples) and the target label, while the samples to inject the trigger do not have constraints, i.e., they can be from training samples or artificially synthesized (which does not contain privacy information of any training/test data). For conveniences, existing methods inject backdoor triggers into the training/test samples. To validate this, we synthesize a set of random graphs using the popular Erd\u02ddos\u2013R\u00e9nyi model (via the NetworkX toolbox) and the watermark samples are generated by injecting the learnt watermark on the synthesized graphs. Under the default setting, we test on Fed-GIN and show results in Table 17, where we observe WAs are very close to those shown in the paper on the four datasets. ", "page_idx": 18}, {"type": "text", "text": "Furthermore, since all clients intend to verify model ownership, it is reasonable to believe that these clients are willing to provide their watermark data\u2014whether generated from private training/test data or non-private synthesized data\u2014exclusively to a trusted judge, with informed consent and in accordance with legal and ethical standards. From this perspective, the data is confidential between each client and the trusted judge. We acknowledge it is very interesting future work to design a provably private mechanism for model ownership verification that the verifier cannot access the watermark data but can guarantee the correctness of verification. ", "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We clearly state the research problem, motivation, and contributions of this paper in the Abstract and Introduction. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: We discuss the limitations of the approach in the Discussion section, e.g., using clients\u2019 training graphs as watermark samples and sending them to the trusted judge could lead to privacy leakage. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We provide complete theory background in the main text, and present proofs in the supplementary. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Our experimental results are reproducible. We provide detailed experimental settings in the main text. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We include links to the source code in the abstract. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We provide training and test details in the experimental part. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [No] ", "page_idx": 21}, {"type": "text", "text": "Justification: Our experimental results are stable. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: All experiments can be reproduced using one NVIDIA GeForce GTX 1080 Ti GPU. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: This research conforms to the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Positive societal impacts: Our proposed watermark i.e. FedGMark can protect the ownership of FedGL models. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This paper poses no the above risks. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We cite the reference of datasets employed in this paper following their requirements. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 23}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: There is no dataset contribution in this paper. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our work does not involve crowdsourcing and research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]