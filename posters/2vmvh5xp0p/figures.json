[{"figure_path": "2vMvh5XP0P/figures/figures_0_1.jpg", "caption": "Figure 1: SSS GS - We propose photorealistic real-time relighting and novel view synthesis of subsurface scattering objects. We learn to reconstruct the shape and translucent appearance of an object within the Gaussian Splatting framework. To do so we leverage our newly created multi-view multi-light dataset of synthetic and real-world objects acquired in a light-stage setup. The object is decomposed in a PBR fashion allowing for easy material editing and relighting. For a trailer visit our project page at https://sss.jdihlmann.com/.", "description": "This figure shows the main results and contributions of the paper.  It demonstrates the pipeline for reconstructing objects with subsurface scattering using 3D Gaussian Splatting. The process begins with a multi-view, multi-light dataset captured in a light stage. This data is then used to reconstruct a 3D model, which is decomposed into its physically based rendering (PBR) components (base color, normal, diffuse, specular, subsurface scattering residual, metalness, roughness, subsurfaceness).  The PBR model enables realistic material editing and relighting, while the Gaussian Splatting framework allows for real-time novel view synthesis. The figure visually presents the steps, from the initial dataset to the final rendered image showing the edited and relit object, highlighting the efficiency and visual fidelity of the method. ", "section": "1 Introduction"}, {"figure_path": "2vMvh5XP0P/figures/figures_3_1.jpg", "caption": "Figure 2: Subsurface Scattering Pipeline - Our method implicitly models the subsurface scattering appearance of an object and combines it with an explicit surface appearance model. The object is represented as a set of 3D Gaussians, consisting of geometry and appearence properties. We ultilize a small MLP to evaluate the subsurface scattering residual given the view and light direction and a subset of properties for each Gaussian. Further, we evaluate the incident light for each Gaussian as a joint task within the same MLP given the visibility supervised by ray-tracing. Based on the computed properties we accumulate and rasterize each property on the image plane in a deferred shading pipeline. We evaluate the diffuse and specular color with a BRDF model for every pixel in image space and combine it with the SSS residual to get the final color of the object.", "description": "The figure illustrates the pipeline of the proposed method.  It shows how the subsurface scattering (SSS) appearance of an object is modeled implicitly, combined with an explicit surface appearance model using 3D Gaussians.  A small Multilayer Perceptron (MLP) is utilized to predict the SSS residual and incident light, taking into account various properties of each Gaussian.  Ray tracing is used to determine visibility.  A deferred shading pipeline combines the results from the MLP with a Bidirectional Reflectance Distribution Function (BRDF) model in image space to produce the final pixel colors. ", "section": "3 Method"}, {"figure_path": "2vMvh5XP0P/figures/figures_5_1.jpg", "caption": "Figure 3: Results of Decomposition \u2013 showing two different views with different light directions. Further, the decomposition of PBR parameters is shown. The first two objects shown are synthetic while the lower two are scanned real-world world objects.", "description": "This figure shows the results of decomposing objects into their constituent components (base color, metalness, roughness, subsurface scattering, normals, SSS residual, incident light). The decomposition is shown for two different views and two different light directions.  The top two rows show results for synthetic objects, while the bottom two rows show results for real-world scanned objects.  The figure demonstrates the ability of the proposed method to accurately capture and represent the various physical properties of translucent objects, including their subsurface scattering behavior.", "section": "4 Experiments"}, {"figure_path": "2vMvh5XP0P/figures/figures_8_1.jpg", "caption": "Figure 4: Editing results, showcasing PBR based edits such as (roughness / metalness / base color) as well as method specific properties (subsurfaceness / residual color). The latter highlights editing only possible with this method. The rightmost column shows light positions not sampled from the light stage.", "description": "This figure demonstrates the editing capabilities of the proposed method.  It shows how different parameters like roughness, metalness, base color, subsurfaceness, and residual color can be adjusted individually to modify the appearance of the 3D object.  The rightmost column highlights the ability to edit with light sources not present during training.", "section": "4.4 Applications"}, {"figure_path": "2vMvh5XP0P/figures/figures_8_2.jpg", "caption": "Figure 5: Limits of Relightable 3D Gaussians (left) \u2013 While Relightable 3D Gaussians can reproduce view and illumination-dependent reflections they fail to properly relight subsurface scattering objects. Deferred Shading (right) \u2013 allows us to evaluate the surface reflectance for each rendered pixel instead of per Gaussian. This way, specular highlights are rendered with crisper detail.", "description": "This figure compares the results of Relightable 3D Gaussian Splatting (R3DGS) and the proposed method for rendering subsurface scattering objects.  The left side shows that R3DGS struggles to accurately relight objects with subsurface scattering because it does not explicitly model the subsurface scattering effect. The right side illustrates how deferred shading in the proposed method improves the rendering of specular highlights by evaluating the surface reflectance at each pixel instead of just at the Gaussian centers, resulting in crisper details.", "section": "3.2 Subsurface Scattering for 3D Gaussian Splatting"}, {"figure_path": "2vMvh5XP0P/figures/figures_8_3.jpg", "caption": "Figure 6: Comparison against the KiloOSF [46] method. The top two objects are real-world objects and the bottom two synthetic objects. Note the qualitative improvement in shape and appearance compared to KiloOSF.", "description": "This figure compares the results of the proposed method against the KiloOSF method on four different objects, two synthetic and two real-world. It showcases the superior reconstruction of object shape and appearance achieved by the proposed method compared to KiloOSF, particularly regarding fine details and surface smoothness.", "section": "4.2 Qualitative Results"}, {"figure_path": "2vMvh5XP0P/figures/figures_13_1.jpg", "caption": "Figure 7: Preprocessing and Light Stage showing our real-world and synthetic data acquisition pipeline. For the real-world data, we present a sample of the all lights on images used for COLMAP [34] reconstruction. Further we show our segement anything [21] based automatic masking of the data (with a failure case that we filter out for training). Note that all lights on images are not used during the training process. On the right, we show the synthetic data acquisition pipeline and the train and test split also applied to the real-world data.", "description": "This figure illustrates the data acquisition pipelines for both real-world and synthetic datasets.  The real-world pipeline shows the light stage setup, image denoising and demosaicing, mask generation using the Segment Anything Model (SAM), and structure-from-motion (SfM) processing using COLMAP. The synthetic pipeline depicts the Blender light stage setup and the train/test split used for training and evaluating the model.  It highlights the differences in data acquisition and preprocessing steps for both data sources.", "section": "4.1 Experimental setup"}, {"figure_path": "2vMvh5XP0P/figures/figures_13_2.jpg", "caption": "Figure 8: Translucent Captured Objects highlighting a subset of images from various camera and light poses with all 20 objects of our dataset. The top left 5 are synthetic and the remainder shows real-world captures.", "description": "This figure shows all 20 objects used in the real and synthetic datasets.  The top-left corner shows 5 synthetic objects, and the rest are real-world objects. Each object is shown under multiple lighting conditions captured from many camera views.  The image provides a visual overview of the diversity of translucent objects (e.g., varying shapes, colors, materials, and translucency) used to evaluate the proposed SSS (subsurface scattering) method.", "section": "B Translucent Object Dataset"}, {"figure_path": "2vMvh5XP0P/figures/figures_14_1.jpg", "caption": "Figure 9: Shading Ablation highlighting the importance of each component of our method and their difference compared against ground truth. First (left to right) \"w/o PBR\" (only residual and incident light field), second adding PBR in \"w/o Deferred\" and last showcasing full specular highlights and the least error by employing our full method \"PBR+Deffered\". We show results from individual training runs (not post-training toggling of components).", "description": "This figure shows an ablation study comparing different versions of the proposed method against the ground truth. The versions are:\n- Full Method: The complete method with PBR and deferred shading.\n- w/o PBR: Without the Physically Based Rendering (PBR) component. Only the SSS residual and incident light are considered. \n- w/o Deferred: Without deferred shading, the BRDF is evaluated in Gaussian space which results in less precise highlights.\n- PBR + Deferred: Without SSS effect, only PBR with deferred shading. \nThe differences between the versions show the contribution of each component to the final rendering quality, especially the impact of deferred shading for accurate specular highlights.", "section": "C Ablation"}, {"figure_path": "2vMvh5XP0P/figures/figures_15_1.jpg", "caption": "Figure 10: Image Based Lighting (IBL) results of our method, with an OLAT sample in the top row for comparison and three different environment maps used on two synthetic (bunny / soap) and two real-world (candle / car) objects.", "description": "This figure shows the results of applying image-based lighting (IBL) to the objects rendered by the proposed method. The top row shows rendering with a single light source for comparison. The subsequent rows show the same objects rendered under three different environment lighting conditions (Indoor, Woods, Snow).  Both synthetic (bunny, soap) and real-world (candle, car) objects are included to demonstrate the versatility of the approach.", "section": "4.2 Qualitative Results"}, {"figure_path": "2vMvh5XP0P/figures/figures_16_1.jpg", "caption": "Figure 3: Results of Decomposition \u2013 showing two different views with different light directions. Further, the decomposition of PBR parameters is shown. The first two objects shown are synthetic while the lower two are scanned real-world world objects.", "description": "This figure showcases the results of the proposed method's decomposition of objects into their constituent parts: base color, metalness, roughness, normals, SSS residual, specular, and diffuse components.  It demonstrates the method's ability to handle both synthetic and real-world objects, showing two different views of each object under varying light directions. The decomposition highlights the separation of surface properties (PBR) from subsurface scattering effects (SSS).", "section": "4 Experiments"}]