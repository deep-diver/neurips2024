[{"figure_path": "4t3ox9hj3z/tables/tables_4_1.jpg", "caption": "Table 1: True vs. learned Lorenz system: comparison of statistics. (W\u00b9: Wasserstein-1 Distance, A: set of LEs, \u00fb: empirical distribution of an orbit T. The subscript NN indicates quantities computed using NN models trained with different loss functions (MSE (1), JAC (3)).", "description": "This table compares the statistical properties of the true Lorenz system against those learned by neural networks trained using two different loss functions: Mean Squared Error (MSE) and Jacobian matching (JAC).  The comparison includes the Wasserstein-1 distance between the empirical distributions, the difference in Lyapunov exponents (LEs), and the difference in average states. The results show that the JAC loss leads to significantly better agreement with the true system's statistics than the MSE loss.", "section": "Jacobian-matching"}, {"figure_path": "4t3ox9hj3z/tables/tables_18_1.jpg", "caption": "Table 2: Hyperparameter choices", "description": "This table shows the hyperparameter choices used in the numerical experiments for various chaotic systems.  The hyperparameters include the number of epochs for training, the time step used in the simulation, the hidden layer width and number of layers in the neural network architecture, the training and testing set sizes, the type of neural network used (ResNet or MLP), and the lambda (\u03bb) value used in the Jacobian-matching loss function.", "section": "B Experimental Details"}, {"figure_path": "4t3ox9hj3z/tables/tables_22_1.jpg", "caption": "Table 3: Lyapunov Spectra learned by Neural ODE models trained on the MSE (1) for multi-step prediction.", "description": "This table presents the Lyapunov exponents learned by neural ordinary differential equation (NODE) models trained using the mean squared error (MSE) loss for multi-step prediction.  It compares the results for two different neural network architectures, MLP and ResNet, and for different numbers of timesteps (k) used in the unrolling of the dynamics during training. Lyapunov exponents are a measure of the rate of separation of nearby trajectories in a dynamical system, and thus this table provides insight into how accurately the NODE models are learning the underlying dynamics of the systems.", "section": "C.6 Learning Lyapunov exponents"}, {"figure_path": "4t3ox9hj3z/tables/tables_22_2.jpg", "caption": "Table 4: True vs. learned Lorenz '63 system: comparison of statistics. (W\u00b9: Wasserstein-1 Distance, A = [1, 2, 3], set of LEs, \u03bc\u0302T: empirical distribution of an orbit of length T. The subscript NN indicates quantities computed using NN models. The variable k refers to the sequence length used for training.)", "description": "This table compares the true statistical measures (Wasserstein-1 distance, Lyapunov exponents, and average state) of the Lorenz '63 system with those learned by neural ordinary differential equation (NODE) models trained using either mean-squared error (MSE) loss or Jacobian-matching loss.  It shows the differences between the true and learned quantities for different model types and training sequence lengths (k).", "section": "Jacobian-matching"}, {"figure_path": "4t3ox9hj3z/tables/tables_23_1.jpg", "caption": "Table 5: Chaotic systems and the true Lyapunov Spectra, the learned Lyapunov Spectra from Neural ODEs with MSE loss (1), and the learned Lyapunov Spectra from Neural ODEs with Jacobian-matching loss (3).", "description": "This table compares the true Lyapunov exponents for several chaotic systems (tent maps, Lorenz '63, R\u00f6ssler, hyperchaos, Kuramoto-Sivashinsky) with those obtained from neural ODE models trained using two different loss functions: mean squared error (MSE) and Jacobian-matching.  The comparison highlights the impact of including Jacobian information in the training process on the accuracy of learning the dynamical invariants of the systems.  The results indicate that Jacobian-matching leads to significantly more accurate estimates of the Lyapunov exponents, particularly for more complex systems.  Each system is represented by a vector of Lyapunov exponents reflecting the asymptotic exponential rate of separation or convergence of nearby trajectories in its phase space.", "section": "C.6 Learning Lyapunov exponents"}, {"figure_path": "4t3ox9hj3z/tables/tables_24_1.jpg", "caption": "Table 6: Lyapunov spectra computed from the learned latent SDE model evaluated on Euler-Maruyama and RK4 solvers. Both the learned drift and the near-zero diffusion vector fields are used in Euler-Maruyama, and only the drift vector field is used in RK4.", "description": "This table presents the Lyapunov spectra obtained from a latent stochastic differential equation (SDE) model trained on the Lorenz '63 system.  The spectra are computed using both the Euler-Maruyama and RK4 numerical integration methods. The comparison highlights the impact of including the diffusion term (stochasticity) in the model on the accuracy of the computed Lyapunov exponents.  The results suggest that the stochasticity affects the Lyapunov spectrum, indicating potential differences in the model's representation of the system dynamics.", "section": "C.8 Latent SDE: experimental details"}, {"figure_path": "4t3ox9hj3z/tables/tables_24_2.jpg", "caption": "Table 1: True vs. learned Lorenz system: comparison of statistics. (W\u00b9: Wasserstein-1 Distance, A: set of LEs, \u00fb: empirical distribution of an orbit T. The subscript NN indicates quantities computed using NN models trained with different loss functions (MSE (1), JAC (3)).", "description": "This table compares the statistical properties of the true Lorenz '63 system with those learned by neural networks trained using different loss functions.  Specifically, it contrasts the Wasserstein-1 distance (a measure of distribution similarity) between the true and learned systems, the differences in Lyapunov exponents (measures of chaos), and the differences in the average state values for orbits of length 500.  This provides a quantitative comparison of how accurately different training methods capture the statistical properties of a chaotic system.", "section": "Jacobian-matching"}, {"figure_path": "4t3ox9hj3z/tables/tables_26_1.jpg", "caption": "Table 1: True vs. learned Lorenz system: comparison of statistics. (W1: Wasserstein-1 Distance, A: set of LEs, \u00fb: empirical distribution of an orbit T. The subscript NN indicates quantities computed using NN models trained with different loss functions (MSE (1), JAC (3)).", "description": "This table compares the statistical properties of the true Lorenz '63 system with those learned by neural networks trained using different loss functions.  It shows the Wasserstein-1 distance between the true and learned probability distributions of orbits, the difference in the Lyapunov exponents, and the difference in the means of the state variables (x, y, z). The results highlight the impact of different loss functions on the statistical accuracy of the learned models.  MSE (mean squared error) loss leads to larger discrepancies compared to JAC (Jacobian matching) loss, indicating that incorporating Jacobian information during training is crucial for learning statistically accurate models.", "section": "Jacobian-matching"}, {"figure_path": "4t3ox9hj3z/tables/tables_27_1.jpg", "caption": "Table 7: Results of search over hyperparameters (batch size, weight decay, hidden layer depth and width) in training Neural ODEs with MLPs (with fully connected and convolution layers). We train with mean squared loss using the AdamW optimization algorithm, with two values of weight decay: 10<sup>-3</sup> and 10<sup>-4</sup>, and an adaptive learning rate with an initial value of 0.001. For each hyperparameter combination, we show the test loss and the relative error in the one-timestep predictions averaged over 8000 samples; we choose the hyperparameter combination that results in the least relative error. The time step of the maps (both true and NNs) are set at 0.01.", "description": "This table presents the results of a hyperparameter search for training Neural ODEs with MLPs using mean squared loss and the AdamW optimization algorithm.  The search explored batch size, weight decay, hidden layer depth and width. The table shows test loss and relative error for each combination, highlighting the optimal settings that minimize relative error.", "section": "B Experimental Details"}]