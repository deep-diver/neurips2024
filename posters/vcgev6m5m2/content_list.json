[{"type": "text", "text": "Template-free Articulated Gaussian Splatting for Real-time Reposable Dynamic View Synthesis ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diwen Wan1 Yuxiang Wang1 Ruijie Lu1 Gang Zeng1 ", "page_idx": 0}, {"type": "text", "text": "1National Key Laboratory of General Artificial Intelligence, School of IST, Peking University, China {wan,yuxiang123}@stu.pku.edu.cn {jason_lu,zeng}@pku.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "While novel view synthesis for dynamic scenes has made significant progress, capturing skeleton models of objects and re-posing them remains a challenging task. To tackle this problem, in this paper, we propose a novel approach to automatically discover the associated skeleton model for dynamic objects from videos without the need for object-specific templates. Our approach utilizes 3D Gaussian Splatting and superpoints to reconstruct dynamic objects. Treating superpoints as rigid parts, we can discover the underlying skeleton model through intuitive cues and optimize it using the kinematic model. Besides, an adaptive control strategy is applied to avoid the emergence of redundant superpoints. Extensive experiments demonstrate the effectiveness and efficiency of our method in obtaining re-posable 3D objects. Not only can our approach achieve excellent visual fidelity, but it also allows for the real-time rendering of high-resolution images. Please visit our project page for more results: https://dnvtmf.github.io/SK_GS/. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Novel view synthesis for 3D scenes is important for many domains including virtual/augmented/mixed reality, game or movie productions. In recent years, Neural Radiance Fields (NeRF) [1] have witnessed significant advances in both static and dynamic scenes. Among them, 3D Gaussian Splatting (3D-GS) [2] proposed a novel point-based representation, and is capable of real-time rendering while ensuring the quality of generated images, bringing new insights to more complex task scenarios. ", "page_idx": 0}, {"type": "text", "text": "Although visually compelling results and fast rendering speed have been achieved in reconstructing a dynamic scene, current methods mainly focus on replaying the motion in the video, which means it just renders novel view images within the given time range, making it hard to explicitly repose or control the movement of individual objects in the scene. For some specific categories such as the human body or human head, one main approach is to leverage the category-specific prior knowledge such as templates to support the manipulation of reconstructed objects. However, it is hard for these methods to generalize to large-scale in-the-wild scenes or human-made articulated objects. ", "page_idx": 0}, {"type": "text", "text": "Some template-free methods attempt to address these challenges by building reposable models from videos. Watch-It-Move (WIM) [3] leverages ellipsoids, an explicit representation, to coarsely model 3D objects, and then estimate the residual by a neural network. The underlying intuition is that one or more ellipsoids can represent a functional part. By observing the motion of parts from multi-view videos, WIM can learn both the appearance and structure of articulated objects. However, the reconstruction results of WIM are of low visual quality and the training and rendering speed is slow. Apart from WIM, Articulated Point NeRF (AP-NeRF) [4] samples feature point cloud from a pre-trained dynamic NeRF model (TiNeuVox [5]) and initializes the skeleton tree using the medial axis transform algorithm. By combining linear blend skinning (LBS) and point-based rendering [6], ", "page_idx": 0}, {"type": "text", "text": "AP-NeRF jointly optimizes dynamic NeRF and skeletal model from videos. Compared to WIM, AP-NeRF achieves higher visual fidelity while significantly reducing the training time. However, AP-NeRF cannot achieve real-time rendering, which is still far from practical application. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we target class-agnostic novel view synthesis of reposable models without the need for a template or pose annotations, while achieving real-time rendering. To enable fast rendering speed, we opt to represent the 3D object as 3D Gaussian Splatting. To be specific, we first reconstruct the 3D dynamic model using 3D Gaussians and superpoints, where each superpoint binds Gaussians with similar motions together. These superpoints will later be treated as the parts of an object. Afterward, a skeleton model is discovered leveraging some intuitive cues under the guidance of superpoint motions from the video. Finally, we jointly optimize the skeleton model and pose parameters to match the motions of the training videos. During the optimization process of object reconstruction, we will inevitably generate a lot of redundant superpoints to fti the complex motion. To simplify the skeleton model and avoid overfitting, we employ an adaptive control strategy and regularization losses to reduce the number of superpoints. Our contributions can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose a novel method based on 3D Gaussians and superpoints for modeling appearance, skeleton model, and motion of articulated dynamic objects from videos. Our approach can automatically discover the skeleton model without any category-specific prior knowledge.   \n\u2022 We effectively learn and control superpoints by employing an adaptive control strategy and regularization losses.   \n\u2022 We demonstrate excellent novel view synthesis quality while achieving real-time rendering on various datasets. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Static and Dynamic Neural Radiance Fields ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In recent years, we have witnessed significant progress in the field of novel view synthesis empowered by Neural Radiance Fields. While vanilla NeRF [1] manages to synthesize photorealistic images for any viewpoint using MLPs, subsequent works have explored various representations such as 4D tensors [7], hash encodings [8], or other well-designed data structures [9, 10] to improve rendering quality and speed. More recently, a novel framework 3D Gaussian Splatting [2] has received widespread attention for its ability to synthesize high-fidelity images for complex scenes in real-time. ", "page_idx": 1}, {"type": "text", "text": "Meanwhile, many research works challenge the hypothesis of a static scene in NeRFs and attempt to synthesize novel-view images of a dynamic scene at an arbitrary time from a 2D video, which is a more challenging task since the correspondence between different frames is non-trivial. One line of research works [11\u201313] directly represents the dynamic scene with an additional time dimension or a time-dependent interpolation in a latent space. Another line of work [14\u201318] represents the dynamic scene as a static canonical 3D scene along with its deformation fields. While one main bottleneck of synthesizing a dynamic scene is speed, some works [19\u201322] propose to extend 3D Gaussian Splatting into 4D to mitigate the problem. Though being able to recover a high-fidelity scene, this method cannot directly support editing and reposing objects within it. In this work, we leverage 3D Gaussian Splatting as the representation for faster rendering speed. ", "page_idx": 1}, {"type": "text", "text": "2.2 Object Reposing ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "It\u2019s impractical to directly repose the deformation fields of dynamic NeRFs due to the complexity of high-dimension. Therefore, utilizing parametric templates based on object priors to represent deformation is adopted in many research works. The classes of parametric templates range from human faces [23, 24], and bodies [25, 26] to non-human objects like animals [27]. With the help of skeleton-based LBS and 3D or 2D annotations, these parametric templates are capable of representing articulate human heads [28\u201333] and bodies [34\u201341]. Though these template-based reposing methods can synthesize high-fidelity images, they are restricted to certain object classes and mainly deal with rigid motions, not to mention the time-consuming process of annotations. ", "page_idx": 1}, {"type": "text", "text": "To alleviate the excessive reliance on domain-specific skeletal models, methods based on retrieval from database [42] or adaptation from a generic graph [43, 44] are adopted. However, these methods are still of relatively low flexibility and diversity. Another line of work attempts to learn a more general template-free object representation by 3D shape recovery [3, 4, 45]. WIM [3] proposes to jointly learn a surface representation and LBS model for articulation without any supervision or prior knowledge of the structure. However, the reposing images are of low visual quality and the required training time is considerably long. AP-NeRF [4] achieves a much faster training speed by leveraging a point-based NeRF representation, but cannot support real-time rendering as well. ", "page_idx": 1}, {"type": "image", "img_path": "vcGEV6m5m2/tmp/00844a650a08657629a871041f6906e171b11482c8294de21b8accdf699df6e5.jpg", "img_caption": ["Figure 1: The pipeline of proposed approach. Our approach follows a two-stage training strategy. In the first stage (i.e. dynamic stage), we learn the 3D Gaussians and superpoints to reconstruct the appearance. Each superpoint is associated with a rigid part, and the adaptive control strategy is used to control the count. After finishing the training of dynamic stage, we can discover the skeleton model based on superpoints. After we finish the second stage (i.e., kinematic stage), we can obtain an articulated model based on the kinematic model. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "3 Methods ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our goal is to reconstruct a reposable articulated object with real-time rendering speed from videos. The pipeline of proposed method is illustrated in Fig. 1. We represent the appearance of the articulated object as 3D Gaussians in the canonical space while aggregating 3D Gaussians with similar motion into superpoints, which can be treated as rigid parts. It is noteworthy that we apply a time-variant 6 DoF transformation matrix to model the motion of the object. Based on these superpoints, we leverage several intuitive observations to guide the discovery of the skeleton model, which includes both joints and skeletons. Since the observations hold for most objects, our method does not require a category-specific template or pose annotations. To reduce the redundant superpoint, we propose an adaptive control strategy to densify, prune, and merge superpoints during the training process. ", "page_idx": 2}, {"type": "text", "text": "3.1 Preliminaries: 3D Gaussian Splatting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3D Gaussian Splatting (3D-GS) [2] use a set of 3D Gaussians to represent a 3D scene. Each Gaussian $G_{i}$ : $\\{\\mu_{i},q_{i},s_{i},\\sigma_{i},h_{i}\\}$ is associated with a position $\\pmb{\\mu}_{i}$ , a rotation matrix ${\\bf{R}}_{i}$ which is parameterized by a quaternion $\\pmb q_{i}$ , a scaling matrix $\\mathbf{S}_{i}$ which is parameterized by a 3D vector $s_{i}$ , opacity $\\sigma_{i}$ and spherical harmonics (SH) coefficients $\\boldsymbol{h}_{i}$ . Therefore, the anisotropic 3D covariance matrix of Gaussian $G_{i}$ is defined as $\\pmb{\\Sigma}_{i}=\\mathbf{R}_{i}\\mathbf{S}_{i}\\mathbf{S}_{i}^{\\top}\\mathbf{R}_{i}^{\\top}$ , which is positive semi-definite matrix. ", "page_idx": 2}, {"type": "text", "text": "To render images, 3D-GS employs EWA Splatting algorithm [46] to project a 3D Gaussian with center $\\pmb{\\mu}_{i}$ and covariance $\\Sigma_{i}$ to 2D image space, and the projection can be approximated as a 2D Gaussian with center $\\pmb{\\mu}_{i}^{\\prime}$ and covariance $\\Sigma_{i}^{\\prime}$ . Let $\\mathbf{Q}$ , $\\mathbf{K}$ be the viewing transformation and projection matrix, $\\pmb{\\mu}_{i}^{\\prime}$ and $\\Sigma_{i}^{\\prime}$ are computed as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pmb{\\mu}_{i}^{\\prime}=\\mathbf{K}(\\mathbf{Q}\\pmb{\\mu}_{i})/(\\mathbf{Q}\\pmb{\\mu}_{i})_{z},\\quad\\pmb{\\Sigma}_{i}^{\\prime}=\\mathbf{J}\\mathbf{Q}\\pmb{\\Sigma}_{i}\\mathbf{Q}^{\\top}\\mathbf{J}^{\\top},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbf{J}$ is the Jacobian of the projective transformation. Therefore, the final opacity of a 3D Gaussian at pixel coordinate $\\textbf{\\em x}$ is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\alpha_{i}=\\sigma_{i}\\exp(-\\frac{1}{2}(\\mathbf{\\boldsymbol{x}}-\\boldsymbol{\\mu}_{i}^{\\prime})^{\\top}{\\boldsymbol{\\Sigma}}_{i}^{'-1}(\\mathbf{\\boldsymbol{x}}-\\boldsymbol{\\mu}_{i}^{\\prime}))\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "After sorting Gaussians by depth, the color at $\\textbf{\\em x}$ can be computed by volume rendering: ", "page_idx": 3}, {"type": "equation", "text": "$$\nI=\\sum_{i=1}^{N}(c_{i}\\alpha_{i}\\prod_{j=1}^{i-1}(1-\\alpha_{j}))\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where RGB color $c_{i}$ is evaluated by SH with coefficients $\\boldsymbol{h}_{i}$ and view direction. ", "page_idx": 3}, {"type": "text", "text": "Given multi-view images with known camera poses, 3D-GS optimizes a static 3D scene by minimizing the following loss function: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{r g b}=(1-\\lambda)\\mathcal{L}_{1}(I,I_{g t})+\\lambda\\mathcal{L}_{\\mathrm{SSIM}}(I,I_{g t})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\lambda=0.2$ , and $I_{g t}$ is the ground truth. Besides, 3D-GS is initialized from from random point cloud or SfM sparse point cloud, and an adaptive density adjustment strategy is applied to control the number of Gaussians. ", "page_idx": 3}, {"type": "text", "text": "3.2 Dynamic Stage ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To reconstruct an articulated object, we build the deformation based on superpoints and the linear blend skinning (LBS) [47], while the canonical model is represented by 3D-GS. ", "page_idx": 3}, {"type": "text", "text": "tToh ree spurepseernpto itnhtes $\\mathcal{P}=\\{p_{j}\\in\\mathbb{R}^{3}\\}_{j=1}^{M}$ F aorre t iamsseosctiaamtepd ,w iwthe  ad irseetc tolyf  3usDe  Gdeafuosrsimaanbsl, ea fniedl dc tboe  luesaerdn $t$ $\\Phi$ time-variant 6 DoF transformation $[\\mathbf{R}_{j}^{t},o_{j}^{t}]\\in\\mathbf{S}\\mathbf{E}(\\dot{\\mathbf{3}})$ of superpoint $\\pmb{p}_{j}$ as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Phi:(\\pmb{p}_{j},t)\\rightarrow(\\mathbf{R}_{j}^{t},\\pmb{o}_{j}^{t}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{R}_{j}^{t}\\in\\mathbf{SO}(\\mathbf{3})$ is the local rotation matrix and $o_{j}^{t}\\in\\mathbb{R}^{3}$ is the translation vector. Then, LBS is employed to derive the motion of each Gaussian by interpolating the transformations for their neighboring superpoints: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pmb{\\mu}_{i}^{t}=\\sum_{j\\in\\mathcal{N}_{i}}w_{i j}(\\mathbf{R}_{j}^{t}\\pmb{\\mu}_{i}+\\pmb{o}_{j}^{t}),\\quad\\pmb{q}_{i}^{t}=(\\sum_{j\\in\\mathcal{N}_{i}}w_{i j}\\pmb{r}_{j}^{t})\\otimes\\pmb{q}_{i},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $r_{j}^{t}\\in\\mathbb{R}^{4}$ is the quaternion representation for matrix $\\mathbf{R}_{j}^{t}$ , and $\\otimes$ is the production of quaternions. ${\\mathcal{N}}_{i}$ denotes the $K$ -nearest superpoints of Gaussian $G_{i}.\\ w_{i j}$ is the LBS weights between Gaussian $G_{i}$ and superpoint $\\pmb{p}_{j}$ , which can be computed as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nw_{i j}=\\frac{\\exp(\\mathbf{W}_{i j})}{\\sum_{k\\in\\mathcal{N}_{i}}\\exp(\\mathbf{W}_{i k})},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{W}\\in\\mathbb{R}^{N\\times M}$ is a learnable parameter. ", "page_idx": 3}, {"type": "text", "text": "While keeping other attributes (i.e., $s_{i},\\sigma_{i},h_{i})$ of Gaussians the same as canonical space, we can render the image at timestamp $t$ following Eq. 3. ", "page_idx": 3}, {"type": "text", "text": "3.3 Discovery of Skeleton Model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Treating each superpoint as a rigid part of the articulated object, we can discover the skeleton model (i.e., the 3D joints and the connection between joints) based on the motion of superpoints. Similar to WIM [3], there are some observations to help us discover the underlying skeleton. First, if there is a joint between two superpoints $\\scriptstyle{p_{a}}$ and $\\ensuremath{p_{b}}$ , the position of $\\scriptstyle{p_{a}}$ is more likely close to the position of $\\mathbf{\\mathit{p}}_{b}$ . Second, when the relative pose between two parts changes, the joint between the two parts is relatively unchanged. Lastly, two connected parts can be merged if they maintain the same relative pose throughout the whole sequence. ", "page_idx": 3}, {"type": "text", "text": "Let $\\boldsymbol{j}_{a b}\\in\\mathbb{R}^{3}$ be the position of underlying joint between superpoints $\\scriptstyle{p_{a}}$ and $\\mathbf{\\mathit{p}}_{b}$ , and $\\mathbf{R}_{a b}^{t}\\in\\mathbf{SO}(\\mathbf{3})$ is the relative rotation matrix between two superpoints at time $t$ . The relative transform between $\\scriptstyle{p_{a}}$ and $\\ensuremath{\\mathbf{{p}}}_{b}$ can be either represented by the global transform or the rotation of the joint, that is: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left[\\mathbf{R}_{r}^{t}\\quad t_{r}^{t}\\right]=\\left[\\mathbf{R}_{b}^{t}\\quad o_{b}^{t}\\right]^{-1}\\left[\\mathbf{R}_{a}^{t}\\quad o_{a}^{t}\\right]=\\left[\\mathbf{R}_{a b}^{t}\\quad j_{a b}-\\mathbf{R}_{a b}^{t}j_{a b}\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{R}_{r}^{t}=(\\mathbf{R}_{b}^{t})^{-1}\\mathbf{R}_{a}^{t}=\\mathbf{R}_{a b}^{t}\\in\\mathbf{SO}(3)$ and $\\pmb{t}_{r}^{t}\\in\\mathbb{R}^{3}$ are the relative rotation matrix and translation vector between two superpoints respectively. Considering two joints $_{j_{a b}}$ and $j_{b a}$ between $\\scriptstyle{p_{a}}$ and $\\mathbf{\\mathit{p}}_{b}$ should be the same, we compute following distance $d_{a b}$ for every superpoint pair $(a,b)$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nd_{a b}=\\sum_{t}\\|t_{r}-(j_{a b}-\\mathbf{R}_{a b}^{t}j_{a b})\\|_{2}^{2}+\\lambda_{d}\\|j_{a b}-j_{b a}\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\lambda_{d}=1$ is the hyper-parameter. To prevent the distance from changing too quickly, we smooth it across training iterations: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{d}_{a b}(\\tau+1)=(1-\\epsilon)\\cdot\\hat{d}_{a b}(\\tau)+\\epsilon\\cdot d_{a b}(\\tau),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\epsilon=0.1$ is the momentum, and $\\tau$ is the training iteration. ", "page_idx": 4}, {"type": "text", "text": "Similar to WIM [3], we discover the structure $\\Gamma$ of joints based on the distance $\\hat{d}_{a b}$ by the minimum spanning tree algorithm. We first select all pairs $(a,b)$ if superpoint $\\mathbf{\\mathit{p}}_{b}$ is $K^{\\prime}$ -nearest neighborhood for superpoint $\\scriptstyle{p_{a}}$ and sort the list of d\u02c6ab for those pairs in ascending order. We initialize $\\Gamma$ as an empty set. We pick pair $(a,b)$ from the lowest distance to the highest distance, and add this pair to $\\Gamma$ while there is no path between $a$ and $b$ . After finishing the procedure, we obtain the final object structure $\\Gamma$ , which is an acyclic graph, i.e., a tree. We choose the node whose length of the longest path from itself to any other node is the shortest as the root node. If there is more than one candidate node, we randomly choose one as the root. ", "page_idx": 4}, {"type": "text", "text": "3.4 Kinematic Stage ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "After discovering the skeleton model, we optimize the skeleton model and fine-tune 3D Gaussians by using the kinematic model. Specifically, we first predict time-variant rotations $\\hat{\\mathbf{R}}_{k}\\in\\mathbf{SO}(\\mathbf{3})$ for each joint $j_{k}$ by using an deformable field $\\Psi$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Psi:(j_{k},t)\\rightarrow\\hat{\\mathbf{R}}_{k}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then we forward-warp the superpoint $\\pmb{p}_{j}$ from the canonical space to the observation space of timestamp $t$ via the kinematic model. The local transformation matrix $\\hat{\\mathbf{T}}_{k}^{t}\\in\\mathbf{SE}(\\mathbf{3})$ of each joint $k$ is defined by a rotation $\\mathbf{R}_{k}^{t}$ around its parent joint $\\mathbf{j}_{k}$ . Consequently, the final transformation of each superpoint $\\pmb{p}_{j}$ can be computed as a linear combination of bone transformation: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{T}_{j}^{t}=\\left[\\mathbf{\\Delta}\\mathbf{0}_{j}^{t}\\quad\\mathbf{\\Delta}_{1}^{o_{j}^{t}}\\right]=\\mathbf{T}_{r o o t}^{t}\\prod_{k\\in\\mathbb{C}_{j}}\\hat{\\mathbf{T}}_{k}^{t},\\mathrm{where}\\;\\hat{\\mathbf{T}}_{k}^{t}=\\left[\\mathbf{\\Delta}\\mathbf{0}_{}^{t}\\;\\;\\;\\mathbf{\\Delta}_{1}^{j_{k}}-\\hat{\\mathbf{R}}_{k}^{t}j_{k}\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbb{C}_{j}$ is the list of ancestor of superpoint $j$ in skeleton model. $\\mathbf{T}_{r o o t}^{t}$ is global transformation of root. Same as Sec. 3.2, we use LBS to derive the motion of each Gaussian and render images. ", "page_idx": 4}, {"type": "text", "text": "3.5 Adaptive Control of Superpoints ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We use the farthest point sampling algorithm to sample $M$ Gaussians to initialize the superpoints. Simply making superpoints learnable is not enough to model complex motion patterns. More importantly, we wish to simplify the skeleton after training to ease pose editing by reducing the number of superpoints. Following 3D-GS [2] and SC-GS [48], we develop an adaptive control strategy to prune, densify, and merge superpoints. ", "page_idx": 4}, {"type": "text", "text": "Prune: To determine whether a superpoint $\\pmb{p}_{j}$ should be pruned, we calculate its overall impact $\\begin{array}{r}{W_{j}=\\sum_{i\\in\\tilde{\\mathcal{N}}_{j}}w_{i j}}\\end{array}$ , where $\\tilde{\\mathcal{N}}_{j}=\\{i\\mid j\\in\\mathcal{N}_{i}\\}$ is the set of Gaussians whose $K$ nearest neighbors include superpoints $\\pmb{p}_{j}$ . When $W_{j}<\\delta_{p r u n e}$ , we prune this superpoint as it is of little contribution to the motion of 3D Gaussians. ", "page_idx": 4}, {"type": "text", "text": "Densify: Two aspects determining whether a superpoint should be split into two superpoints. On one hand, we clone a superpoint when its impact $W_{j}$ is greater than a threshold $\\delta_{c l o n e}$ , indicating there is a great amount of Gaussians associated with this superpoint, and cloning such superpoints helps model fine motion. On the other hand, we calculate the weighted Gaussians gradient norm of superpoint $j$ as: ", "page_idx": 4}, {"type": "equation", "text": "$$\ng_{j}=\\sum_{i\\in\\tilde{\\mathcal{N}}_{j}}\\frac{w_{i j}}{\\sum_{k\\in\\tilde{\\mathcal{N}}_{j}}w_{k j}}\\|\\frac{\\partial\\mathcal{L}}{\\partial\\pmb{\\mu}_{i}}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "table", "img_path": "vcGEV6m5m2/tmp/5078b90fdbbd2ae274bb65c1f546b5212e611fe106741b6a8a94431e93de90c8.jpg", "table_caption": ["Table 1: Quality comparison of novel view synthesis for the D-NeRF dataset. "], "table_footnote": [], "page_idx": 5}, {"type": "table", "img_path": "vcGEV6m5m2/tmp/1150dfa733f49483ad5fe9702fbe2ecdd1595d6b32b42843e0e0662fa3b7d508.jpg", "table_caption": ["Table 2: Quantitative comparison of novel view synthesis on the Robots dataset. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "where $\\mathcal{L}$ is the loss function, which demonstrated in Appendix A. We clone the superpoint $\\pmb{p}_{j}$ if $g_{j}$ is greater than the threshold $\\delta_{g r a d}$ . ", "page_idx": 5}, {"type": "text", "text": "Merge: We merge the superpoints that should belong to the same rigid part. To determine which superpoints should be merged, we first calculate the transformations $\\bar{\\mathbf{T}}_{j}^{t}\\in\\mathfrak{s e}3$ for all superpoints at all training timestamps. Then, for each pair $(a,b)$ of superpoints, we calculate the average relative transformations: ", "page_idx": 5}, {"type": "equation", "text": "$$\nD_{a,b}=\\frac{1}{N_{t}}\\sum_{t}\\|\\log(\\mathbf{T}_{b}^{t\\,-1}\\mathbf{T}_{a}^{t})\\|,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $N_{t}$ is the number of train timestamps, $\\log()$ denotes the operation of converting a rigid transformation matrix to a Lie algebra. A small $D_{a,b}$ indicates two superpoints have similar motion patterns. Therefore, we merge two superpoints $\\scriptstyle{p_{a}}$ and $\\mathbf{\\mathit{p}}_{b}$ when $D_{a,b}~<~\\delta_{m e r g e}$ and $\\mathbf{\\mathit{p}}_{b}$ is the $K^{\\prime}$ -nearest superpoints of $\\scriptstyle{p_{a}}$ . ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we present the evaluation of our approach, which achieves excellent view-synthesis quality and real-time rendering speed. We also evaluate the contribution of each component through an ablation study. Additionally, we demonstrate the class-agnostic reposing capability. Please refer to our project https://dnvtmf.github.io/SK_GS/ for video visualization. ", "page_idx": 5}, {"type": "text", "text": "4.1 Datasets and Evaluation Metrics ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To ensure fair comparison with previous work, we choose the same datasets and configurations as AP-NeRF[4]. Specifically, we choose three multi-view video datasets. First, the D-NeRF [14] dataset is a sparse multi-view synthesis dataset, which includes 5 humanoids, 2 other articulated objects, and a multi-component scene. Each scene contains 50-200 frames. We choose 6 of 8 scenes 1 The second dataset, Robots [3],contains 7 topologically varied robots with multi-view synthetic video. We use 18 views for training and 2 views for evaluation. The third dataset, ZJU-MoCap [34], is commonly used for dynamic human reconstruction. Following WIM [3] and AP-NeRF [4], we evaluate 5 sequences with 6 training views for each sequence. We use three metrics to evaluate the image quality of the novel view, i.e., peak signal-to-noise ratio (PSNR), structural similarity (SSIM) [51], and learned perceptual image patch similarity (LPIPS) [52]. ", "page_idx": 5}, {"type": "image", "img_path": "vcGEV6m5m2/tmp/d61cbad55528a3b59233be043236ca03794a6094d6b856e7a074b9feadadf169.jpg", "img_caption": ["Figure 2: Qualitative comparison on $D$ -NeRF datasets. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4.2 Implementation Details ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We implement our framework using PyTorch. The number of superpoints is initialized as 512. For both deformable field $\\Phi$ and $\\Psi$ , we adopt the architecture of NeRF[1], i.e., 8-layers MLP where each layer employs 256-dimensional hidden fully connected layer and ReLU activation function. We also employ positional encoding for the input coordinates and time. For optimization, we employ the Adam optimizer and use the different learning rate decay schedules for each component: the learning rate about 3D Gaussians is the same as 3D-GS, while the learning rate of other components undergoes exponential decay, ranging from 1e-3 to 1e-5. We conducted all experiments on a single NVIDIA Tesla V100 (32GB). More implementation details are shown in Appendix A. ", "page_idx": 6}, {"type": "text", "text": "4.3 Baselines ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We mainly compare our method to state-of-the-art template-free articulated methods for view synthesis, i.e. WIM [3] and AP-NeRF [4]. Besides, we also compare our method with NeRF-based and 3D-GS-based non-articulated methods. D-NeRF [14] extends NeRF to dynamic scenes by warping a static NeRF. TiNeuVox [5] improves the visual quality and training speed by using voxel grids. ", "page_idx": 6}, {"type": "image", "img_path": "vcGEV6m5m2/tmp/5c997b84e4f9fb032d6c2c87bf48847bc47afa1dbe2e9fec583bfe9aeae11837.jpg", "img_caption": ["Figure 3: Qualitative comparison for the Robots[3] dataset. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "vcGEV6m5m2/tmp/9228b4f80d56778d86650d66ca4832768afc8c779703c0ba85838157d88428a9.jpg", "img_caption": ["Figure 4: Qualitative comparison for the ZJU-MoCap [34] dataset. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "HexPlane [49] and K-Planes [50] accelerate NeRF by decomposing the space-time volume into several planes. Similar to D-NeRF, Deformable-3D-GS [21] extends static 3D-GS to the temporal domain. 4D-GS [20] accelerate Deformable-3D-GS by decomposing neural voxel encoding algorithm inspired by HexPlane. Similar to ours, SP-GS [22] and SC-GS [48] employ the superpoints/control points to reconstruct dynamic scenes. However, SP-GS and SC-GS can not extract skeleton from reconstructed model. ", "page_idx": 7}, {"type": "text", "text": "4.4 Comparisons on Synthetic Dataset ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In our experiments, we benchmarked our method against several baselines using the $D$ -NeRF dataset and Robots datasets. The quantitative comparison results, presented in Tab. 1, demonstrate the superior performance of our approach in terms of both rendering speed and visual quality. Specifically, our method significantly outperforms WIM and AP-NeRF not only in visual quality but also in rendering speed. Compared to 3D-GS based dynamic scenes reconstruction models, our method not only have similar performance, but also discover the skeleton and can repose the object to generate a novel pose. Specifically, the rendering quality of ours is higher than 4D-GS [20] and SP-GS [22], and lower than D-3D-GS [21] and SC-GS [48]. Our method also can achieve real-time rendering $(>\\!100$ FPS), which is near to the rendering speed of SC-GS [48]. Fig. 2 provides the qualitative comparisons of $D$ -NeRF dataset, which demonstrates the advantages of our method over related methods. We also provide results for Robots datasets, quantitatively in Tab. 2 and qualitatively in Fig. 3. It is also clear that our approach is capable of producing high-fidelity novel views with real-time rendering speed. Per-scene results are shown in Appendix B. ", "page_idx": 7}, {"type": "text", "text": "4.5 Comparison on Real-world Dataset ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In Tab. 3 and Fig. 4, we compare our method to WIM and AP-NeRF in the ZJU-MoCap dataset. We observe that both methods can recover the 3D shape and skeleton models. However, imperfections in the camera calibrations (see Supplement F of [53]), lead to lower visual quality in our results compared to WIM and AP-NeRF. With respect to rendering speed, our approach achieves up to 198.23 FPS. In stark contrast, the rendering speed of WIM and AP-NeRF is extremely slow. ", "page_idx": 7}, {"type": "table", "img_path": "vcGEV6m5m2/tmp/5fb5caa18793a5ea5c3365389bca406df00f0d7af4951ce752bce1df7830f58f.jpg", "table_caption": ["Table 3: Quantitative comparison for the ZJU-MoCap [34] dataset. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "vcGEV6m5m2/tmp/de0819d78a4a902898e3b523c43bd639223d1c188ed84e1b6de43fcaa71adfc6.jpg", "img_caption": ["Figure 5: Reposing using skeleton. Interpolation from canonical to novel pose. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "vcGEV6m5m2/tmp/38304cba9c1dfab955bc1d90200500bcf0976b0743579ce7c91358ed2c353837.jpg", "img_caption": ["(a) Full (#sp: 97) (b) wo Control (#sp: 512) (c) wo Merge (#sp: 608) (d) wo $\\mathcal{L}_{a r a p}$ (#sp: 453) "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 6: We visualize the rendering results of (a) our full method, (b) our method without adaptive control, (c) our method without merge superpoints, (d) our method without $\\mathcal{L}_{a r a p}$ (see Appendix A). #sp denotes the number of superpoints. The blue points denotes superpoints. ", "page_idx": 8}, {"type": "text", "text": "4.6 Reposing ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In Fig. 5, we show our model allows free changes poses and generates animating video by smoothly interpolating the skeletons posing between user-defined poses. Video examples can be found on our project webpage. ", "page_idx": 8}, {"type": "text", "text": "4.7 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We use superpoints to model the parts and motion of the object. The adaptive control of superpoints is the key to reducing the number of superpoints. Fig. 6 (a), (b) and (c) intuitively illustrate the impact of this control strategy. Without the control strategy, the distribution of superpoints becomes uneven, with sparse representation in the arm region, which negatively impacts motion modeling. The merge process in the control strategy significantly reduces the number of superpoints (97 vs 608), while the superpoints are more distributed in the motion area. Besides, as illustrated in Fig. 6 (d), $\\mathcal{L}_{a r a p}$ (in Appendix A) also plays an important role in controlling the density of superpoints. See Appendix C for more ablation studies. ", "page_idx": 8}, {"type": "text", "text": "5 Discuss ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "5.1 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have demonstrated that our approach can achieve real-time rendering, state-of-the-art visual quality, and straightforward reposing capability by skeleton and kinematic models. However, there are some limitations to our approach. Firstly, similar to WIM and AP-NeRF, the learned skeleton model of our approach is restricted to the kinematic motion space exhibited in the input video. Therefore, the skeleton model may have significant differences from the actual one, and extrapolation to generate arbitrary unseen poses may cause errors. Secondly, our approach has similar limitations as other 3D-GS based methods for dynamic scenes. Specifically, the datasets with inaccurate camera poses will lead to reconstruction failures, and large motion or long-term sequences can also result in failures. Lastly, the paper focuses on building the kinematic model for one articulated object. Exporting build kinematic models for multi-component objects or complex scenes that contain multiple objects remains an opportunity for future research. Additionally, extending this approach to motion capture is an interesting research direction. ", "page_idx": 9}, {"type": "text", "text": "5.2 Broader Impacts ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Although our approach is universal, it is also suitable for rending novel views and poses for humans. Therefore, we acknowledge that our approach can potentially be used to generate fake images or videos. We firmly oppose the use of our research for disseminating false information or damaging reputations. ", "page_idx": 9}, {"type": "text", "text": "5.3 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have developed a new method for real-time rendering of articulated models for high-quality novel view synthesis. Without any template or annotations, our approach can reconstruct a kinematic model from multi-view videos. With state-of-the-art visual quality and real-time rendering speed, our work represents a significant step towards the development of low-cost animatable 3D objects for use in movies, games, and education. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported by the Sichuan Science and Technology Program (2023YFSY0008), China Tower-Peking University Joint Laboratory of Intelligent Society and Space Governance, National Natural Science Foundation of China (61632003, 61375022, 61403005), Grant SCITLAB-30001 of Intelligent Terminal Key Laboratory of SiChuan Province, Beijing Advanced Innovation Center for Intelligent Robots and Systems (2018IRS11), and PEKSenseTime Joint Laboratory of Machine Vision. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. $\\mathrm{Ng}$ , \u201cNerf: Representing scenes as neural radiance fields for view synthesis,\u201d in ECCV, 2020, pp. 405\u2013421.   \n[2] B. Kerbl, G. Kopanas, T. Leimk\u00fchler, and G. Drettakis, \u201c3d gaussian splatting for real-time radiance field rendering,\u201d ACM TOG, vol. 42, no. 4, Jul. 2023.   \n[3] A. Noguchi, U. Iqbal, J. Tremblay, T. Harada, and O. Gallo, \u201cWatch it move: Unsupervised discovery of 3d joints for re-posing of articulated objects,\u201d in CVPR, 2022, pp. 3667\u20133677.   \n[4] L. Uzolas, E. Eisemann, and P. Kellnhofer, \u201cTemplate-free articulated neural point clouds for reposable view synthesis,\u201d in NeurIPS, 2023.   \n[5] J. Fang et al., \u201cFast dynamic radiance fields with time-aware neural voxels,\u201d in SIGGRAPH Asia 2022 Conference Papers, 2022.   \n[6] Q. Xu et al., \u201cPoint-nerf: Point-based neural radiance fields,\u201d in CVPR, 2022, pp. 5428\u20135438.   \n[7] A. Chen, Z. Xu, A. Geiger, J. Yu, and H. Su, \u201cTensorf: Tensorial radiance fields,\u201d in ECCV, 2022, pp. 333\u2013350.   \n[8] T. M\u00fcller, A. Evans, C. Schied, and A. Keller, \u201cInstant neural graphics primitives with a multiresolution hash encoding,\u201d ACM TOG, vol. 41, no. 4, 102:1\u2013102:15, Jul. 2022. [9] T. Hu, S. Liu, Y. Chen, T. Shen, and J. Jia, \u201cEfficientnerf efficient neural radiance fields,\u201d in CVPR, 2022, pp. 12 902\u201312 911.   \n[10] C. Sun, M. Sun, and H.-T. Chen, \u201cDirect voxel grid optimization: Super-fast convergence for radiance fields reconstruction,\u201d in CVPR, 2022, pp. 5459\u20135469.   \n[11] W. Xian, J.-B. Huang, J. Kopf, and C. Kim, \u201cSpace-time neural irradiance fields for freeviewpoint video,\u201d in CVPR, 2021, pp. 9421\u20139431.   \n[12] C. Gao, A. Saraf, J. Kopf, and J.-B. Huang, \u201cDynamic view synthesis from dynamic monocular video,\u201d in ICCV, 2021, pp. 5712\u20135721.   \n[13] Z. Li, S. Niklaus, N. Snavely, and O. Wang, \u201cNeural scene flow fields for space-time view synthesis of dynamic scenes,\u201d in CVPR, 2021, pp. 6498\u20136508.   \n[14] A. Pumarola, E. Corona, G. Pons-Moll, and F. Moreno-Noguer, \u201cD-nerf: Neural radiance fields for dynamic scenes,\u201d in CVPR, 2021, pp. 10 313\u201310 322.   \n[15] K. Park et al., \u201cHypernerf: A higher-dimensional representation for topologically varying neural radiance fields,\u201d ACM TOG, vol. 40, no. 6, Dec. 2021.   \n[16] Y. Du, Y. Zhang, H.-X. Yu, J. B. Tenenbaum, and J. Wu, \u201cNeural radiance flow for 4d view synthesis and video processing,\u201d in ICCV, 2021, pp. 14 304\u201314 314.   \n[17] Z. Li, Q. Wang, F. Cole, R. Tucker, and N. Snavely, \u201cDynibar: Neural dynamic image-based rendering,\u201d in CVPR, 2023, pp. 4273\u20134284.   \n[18] E. Tretschk, A. Tewari, V. Golyanik, M. Zollh\u00f6fer, C. Lassner, and C. Theobalt, \u201cNon-rigid neural radiance fields: Reconstruction and novel view synthesis of a dynamic scene from monocular video,\u201d in ICCV, 2021, pp. 12 959\u201312 970.   \n[19] J. Luiten, G. Kopanas, B. Leibe, and D. Ramanan, \u201cDynamic 3d gaussians: Tracking by persistent dynamic view synthesis,\u201d in 3DV, 2024.   \n[20] G. Wu et al., \u201c4D gaussian splatting for real-time dynamic scene rendering,\u201d in CVPR, 2024, pp. 20 310\u201320 320.   \n[21] Z. Yang, X. Gao, W. Zhou, S. Jiao, Y. Zhang, and X. Jin, \u201cDeformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction,\u201d in CVPR, 2024, pp. 20 331\u201320 341.   \n[22] D. Wan, R. Lu, and G. Zeng, \u201cSuperpoint gaussian splatting for real-time high-fidelity dynamic scene reconstruction,\u201d in ICML, 2024, pp. 49 957\u201349 972.   \n[23] V. Blanz and T. Vetter, \u201cA morphable model for the synthesis of 3d faces,\u201d in ACM SIGGRAPH, 1999, pp. 187\u2013194.   \n[24] T. Li, T. Bolkart, M. J. Black, H. Li, and J. Romero, \u201cLearning a model of facial shape and expression from 4d scans.,\u201d ACM TOG, vol. 36, no. 6, pp. 194\u20131, 2017.   \n[25] M. Loper, N. Mahmood, J. Romero, G. Pons-Moll, and M. J. Black, \u201cSmpl: A skinned multi-person linear model,\u201d ACM TOG, vol. 34, no. 6, 248:1\u2013248:16, 2015.   \n[26] G. Pavlakos et al., \u201cExpressive body capture: 3d hands, face, and body from a single image,\u201d in CVPR, 2019, pp. 10 975\u201310 985.   \n[27] S. Zuff,i A. Kanazawa, D. W. Jacobs, and M. J. Black, \u201c3d menagerie: Modeling the 3d shape and pose of animals,\u201d in CVPR, 2017, pp. 6365\u20136373.   \n[28] Y. Zheng, W. Yifan, G. Wetzstein, M. J. Black, and O. Hilliges, \u201cPointavatar: Deformable point-based head avatars from videos,\u201d in CVPR, 2023, pp. 21 057\u201321 067.   \n[29] G. Gafni, J. Thies, M. Zollhofer, and M. Nie\u00dfner, \u201cDynamic neural radiance fields for monocular 4d facial avatar reconstruction,\u201d in CVPR, 2021, pp. 8649\u20138658.   \n[30] Z. Wang et al., \u201cLearning compositional radiance fields of dynamic human heads,\u201d in CVPR, 2021, pp. 5704\u20135713.   \n[31] Y. Guo, K. Chen, S. Liang, Y.-J. Liu, H. Bao, and J. Zhang, \u201cAd-nerf: Audio driven neural radiance fields for talking head synthesis,\u201d in ICCV, 2021, pp. 5784\u20135794.   \n[32] Y. Zheng, V. F. Abrevaya, M. C. B\u00fchler, X. Chen, M. J. Black, and O. Hilliges, \u201cIm avatar: Implicit morphable head avatars from videos,\u201d in CVPR, 2022, pp. 13 545\u201313 555.   \n[33] Y. Zhuang, H. Zhu, X. Sun, and X. Cao, \u201cMofanerf: Morphable facial neural radiance field,\u201d in ECCV, 2022, pp. 268\u2013285.   \n[34] S. Peng et al., \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in CVPR, 2021, pp. 9054\u20139063.   \n[35] S.-Y. Su, F. Yu, M. Zollh\u00f6fer, and H. Rhodin, \u201cA-nerf: Articulated neural radiance fields for learning human shape, appearance, and pose,\u201d in NeurIPS, 2021, pp. 12 278\u201312 291.   \n[36] S. Peng et al., \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in ICCV, 2021, pp. 14 314\u201314 323.   \n[37] H. Xu, T. Alldieck, and C. Sminchisescu, \u201cH-nerf: Neural radiance fields for rendering and temporal reconstruction of humans in motion,\u201d in NeurIPS, 2021, pp. 14 955\u201314 966.   \n[38] Y. Kwon, D. Kim, D. Ceylan, and H. Fuchs, \u201cNeural human performer: Learning generalizable radiance fields for human performance rendering,\u201d in NeurIPS, 2021, pp. 24 741\u201324 752.   \n[39] T. Hu, T. Yu, Z. Zheng, H. Zhang, Y. Liu, and M. Zwicker, \u201cHvtr: Hybrid volumetric-textural rendering for human avatars,\u201d in 3DV, 2022, pp. 197\u2013208.   \n[40] L. Liu, M. Habermann, V. Rudnev, K. Sarkar, J. Gu, and C. Theobalt, \u201cNeural actor: Neural free-view synthesis of human actors with pose control,\u201d ACM TOG, vol. 40, no. 6, pp. 1\u201316, 2021.   \n[41] T. Xu, Y. Fujita, and E. Matsumoto, \u201cSurface-aligned neural radiance fields for controllable 3d human synthesis,\u201d in CVPR, 2022, pp. 15 883\u201315 892.   \n[42] Y. Wu, Z. Chen, S. Liu, Z. Ren, and S. Wang, \u201cCasa: Category-agnostic skeletal animal reconstruction,\u201d in NeurIPS, 2022, pp. 28 559\u201328 574.   \n[43] C.-H. Yao, W.-C. Hung, Y. Li, M. Rubinstein, M.-H. Yang, and V. Jampani, \u201cLassie: Learning articulated shapes from sparse image ensemble via 3d part discovery,\u201d in NeurIPS, 2022, pp. 15 296\u201315 308.   \n[44] S. Wu, R. Li, T. Jakab, C. Rupprecht, and A. Vedaldi, \u201cMagicpony: Learning articulated 3d animals in the wild,\u201d in CVPR, 2023, pp. 8792\u20138802.   \n[45] C.-H. Yao, W.-C. Hung, Y. Li, M. Rubinstein, M.-H. Yang, and V. Jampani, \u201cHi-lassie: Highfidelity articulated shape and skeleton discovery from sparse image ensemble,\u201d in CVPR, 2023, pp. 4853\u20134862.   \n[46] M. Zwicker, H. Pfister, J. van Baar, and M. H. Gross, \u201cEwa volume splatting,\u201d Proceedings Visualization, 2001. VIS \u201901., pp. 29\u2013538, 2001.   \n[47] R. W. Sumner, J. Schmid, and M. Pauly, \u201cEmbedded deformation for shape manipulation,\u201d ACM TOG, vol. 26, no. 3, 80\u2013es, Jul. 2007.   \n[48] Y.-H. Huang, Y.-T. Sun, Z. Yang, X. Lyu, Y.-P. Cao, and X. Qi, \u201cSc-gs: Sparse-controlled gaussian splatting for editable dynamic scenes,\u201d in CVPR, 2024, pp. 4220\u20134230.   \n[49] A. Cao and J. Johnson, \u201cHexplane: A fast representation for dynamic scenes,\u201d in CVPR, 2023, pp. 130\u2013141.   \n[50] S. Fridovich-Keil, G. Meanti, F. R. Warburg, B. Recht, and A. Kanazawa, \u201cK-planes: Explicit radiance fields in space, time, and appearance,\u201d in CVPR, 2023, pp. 12 479\u201312 488.   \n[51] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, \u201cImage quality assessment: From error visibility to structural similarity,\u201d IEEE TIP, vol. 13, pp. 600\u2013612, 2004.   \n[52] Z. Li, S. Niklaus, N. Snavely, and O. Wang, \u201cNeural scene flow fields for space-time view synthesis of dynamic scenes,\u201d in CVPR, 2021, pp. 6494\u20136504.   \n[53] R. Li et al., \u201cTava: Template-free animatable volumetric actors,\u201d in ECCV, 2022, pp. 419\u2013436. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A More Implementation Details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The whole optimization progress can be divided into three stages: dynamic stage, joints discovery stage and kinematic stage. ", "page_idx": 12}, {"type": "text", "text": "A.1 Dynamic Stage ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this stage, we aim to reconstruct the appearance of the object, and find the position of underlying joints. Therefore, we employ the $\\mathcal{L}_{j o i n t}$ , whose formula is: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{L}_{j o i n t}=\\frac{1}{M^{2}}\\sum_{i=1}^{M}\\sum_{j=1}^{M}d_{i j}+\\frac{1}{M-1}\\sum_{(i,j)\\in\\Gamma}d_{i j}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Besides, to obtain a good segmentation of object parts by using superpoints, we employ some regularization losses. To reduce the total number of superpoints, we encourage nearby superpoints to possess same motion patterns via as-rigid-as-possible regularization $\\mathcal{L}_{a r a p}$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{L}_{a r a p}=\\sum_{j=1}^{M}\\sum_{k\\in\\mathcal{N}_{j}^{s p}}\\|\\log(\\mathbf{R}_{j}^{t}^{-1}\\mathbf{R}_{k}^{t})\\|_{2}^{2}+\\|\\pmb{o}_{j}^{t}-\\pmb{o}_{k}^{t}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\mathcal{N}_{j}^{s p}$ is the set of $K^{\\prime}$ -nearest neighbor superpoints of superpoint $j$ . For the blend skinning weight, we employ $\\mathcal{L}_{s m o o t h}$ to encourage smoothness by penalizing divergence of skinning weight: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{L}_{s m o o t h}=\\sum_{i=1}^{N}\\sum_{j\\in\\mathcal{N}_{i}}|w_{i}-w_{j}|.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Besides, we apply $\\mathcal{L}_{s p a r s e}$ to encourage sparsity so that one Gaussian is more likely associated with only one superpoint: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{L}_{s p a r s e}=-\\sum_{i}^{N}\\sum_{j}^{B}w_{i j}\\log(w_{i j})+(1-w_{i j})\\log(1-w_{i j})\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "In total, our training loss of dynamic stage is: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\lambda_{0}\\mathcal{L}_{r g b}+\\lambda_{1}\\mathcal{L}_{j o i n t}+\\lambda_{2}\\mathcal{L}_{a r a p}+\\lambda_{3}\\mathcal{L}_{s m o o t h}+\\lambda_{4}\\mathcal{L}_{s p a r s e}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\lambda=\\{1,1.,10^{-3},0.1,0.1\\}$ in our experiments. ", "page_idx": 12}, {"type": "text", "text": "In this stage, we conducted training for a total of $40\\mathrm{k}$ iterations. Similar to SC-GS [48], our training scheme is as follows: ", "page_idx": 12}, {"type": "text", "text": "1. In $_{0\\sim2\\mathrm{k}}$ iterations, we fix deformable field $\\Phi$ .   \n2. In $2\\mathrm{k}{\\sim}10\\mathrm{k}$ iterations, we train deformable field $\\Phi$ which using the position of Gaussians rather than superpoints as inputs. At 7500 iteration, we sample $M$ Gaussians by using futherest sampling algorithm. At 10k iteration, we initialize superpoints by traind $M$ Gaussians, and re-initialize the Gaussians in canonical space.   \n3. In $10\\sim13\\mathrm{k}$ iterations, we fix deformable field $\\Phi$ .   \n4. In $13\\mathbf{k}\\sim40\\mathbf{k}$ iterations, all parameters are optimized. ", "page_idx": 12}, {"type": "text", "text": "During the first 20k iterations, We do not discover joints and skeleton, i.e., $\\lambda_{1}=0$ . During $20\\mathbf{k}\\sim$ $40\\mathrm{k}$ iterations, we update the structure of skeleton every 100 iterations. Note that we stopped the gradient between joints and other parts so that the learning of joints has no effect on the learning of Gaussians, superpoints and deformation field. ", "page_idx": 12}, {"type": "text", "text": "During training, we adopt the same adaptive control strategy for Gaussians as 3D-GS[2] In details, in iterations $1\\!\\sim\\!7500$ and $10\\mathrm{k}{\\sim35\\mathrm{k}}$ , we densify and prune Gaussians every 100 iterations, while the densify grad threshold is set to 0.0002 and the opacity threshold for prune is set to 0.005. We reset the opacity of Gaussians every $3\\mathbf{k}$ steps. ", "page_idx": 12}, {"type": "text", "text": "We densify and prune superpoints every 1000 steps between iterations $20\\mathbf{k}$ and $30\\mathrm{k}$ , and the hyperparameter $\\delta_{g r a d}=0.0002$ and $\\delta_{p r u n e}=0.001$ . We merge superpoints every 1000 steps between iterations $30\\mathrm{k}$ and $40\\mathrm{k}$ , while threshold $\\delta_{m e r g e}=0.0005$ . ", "page_idx": 12}, {"type": "text", "text": "A.2 Skeleton Discovery Stage ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this stage, we aim to initialize deformable field $\\Psi$ the using the learned deformable field $\\Phi$ in dynamic stage. Firstly, we fix skeleton structure $\\Gamma$ , and initialize the joints $j_{k},k=1,\\dots,M-1$ by underlying joints $_{j_{a b}}$ according to $\\Gamma$ . We then cache the outputs of deformable field $\\Phi$ for all timestamps in the training dataset. Next, we use cached motions of superpoints to optimize the parameters of deformable field $\\Psi$ , the positon of the joints $j_{k}$ by employing Adam optimizer and following loss functions: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{L}_{d i s c o v e r y}=\\frac{1}{M}\\sum_{i=1}^{M}\\lambda_{5}(\\|\\pmb{\\mu}_{i}^{t}-\\hat{\\pmb{\\mu}}_{i}^{t}\\|_{2}^{2}+\\|\\log(\\mathbf{R}_{i}^{t}^{-1}\\hat{\\mathbf{R}}_{i}^{t}))+\\lambda_{6}\\|(\\mathbf{R}_{i}^{t}p+\\pmb{\\mu}^{t})-(\\hat{\\mathbf{R}}_{i}^{t}p+\\pmb{\\mu}^{\\hat{t}})\\|\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\hat{\\pmb{\\mu}}_{i}^{t}$ and $\\hat{\\mathbf{R}}_{i}^{t}$ is translations and rotations calculated by deformable field $\\Phi$ , while $\\pmb{\\mu}_{i}^{t}$ and $\\mathbf{R}_{i}^{t}$ is translations and rotations calculated by deformable field $\\Psi$ and kinematic model. $\\lambda_{5}=1$ controls the weights of the transform matrix difference, and $\\lambda_{6}=0.1$ adjusts the weights of relative offset of the superpoints position at timestamp $t$ . ", "page_idx": 13}, {"type": "text", "text": "For this stage, we train for 10k iterations with a fixed learning rate of $10^{-3}$ . ", "page_idx": 13}, {"type": "text", "text": "A.3 Kinematic Stage ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In the kinematic stage, we optimize the kinematic model, joints and Gaussians, while keeping the number of Gaussians, the number of superpoints and the structure of skeleton fixed. We also conduct training for a total of 40k iterations by only using $\\mathcal{L}_{r g b}$ . ", "page_idx": 13}, {"type": "text", "text": "A.4 Others ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "For the deformable field $\\Phi$ and $\\Psi$ , we adopt the following positional encoding for the input coordinates $\\pmb{x}\\in\\mathbb{R}^{N\\times3}$ and time $t\\in[0,1]$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\gamma(p)=(\\sin(2^{k}p),\\cos(2^{k}p))_{k=0}^{L},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $L=10$ for $\\textbf{\\em x}$ and $L=6$ for $t$ . ", "page_idx": 13}, {"type": "text", "text": "The initial number of superpoints is set to 512, while $K=K^{\\prime}=5$ . ", "page_idx": 13}, {"type": "text", "text": "B Per Scene Results ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Per-Scene Results for D-NeRF dataset ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Tab. 4 shows the per-scene quantitative results for the D-NeRF dataset. ", "page_idx": 13}, {"type": "text", "text": "B.2 Per-scene Results for Robots dataset ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Tab. 5 shows the per-scene quantitative results for Robots dataset. ", "page_idx": 13}, {"type": "text", "text": "C More Results ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "C.1 Required Resources ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "As shown in Tab. 6, we present detailed information on the optimization time and the required resources. ", "page_idx": 13}, {"type": "text", "text": "C.2 Ablation study for the number of initial superpoint ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The ablation study on the number of initial superpoint $M$ is shown in Tab. 7. ", "page_idx": 13}, {"type": "text", "text": "As shown in the Fig. 7, our method struggles to accurately reconstruct the endpoints of objects. ", "page_idx": 13}, {"type": "table", "img_path": "vcGEV6m5m2/tmp/b254f6fbcbbbabd0ca4b62270098ea6aa7fa99074533d3e4170cf06b9bc5515b.jpg", "table_caption": [], "table_footnote": ["Table 4: Quantitative Results Per-Scene in D-NeRF dataset. "], "page_idx": 14}, {"type": "table", "img_path": "vcGEV6m5m2/tmp/88d2bb64832ca35fcf5d0813c26656f1cf81b6b55ecdb370d6b81a15a8a3bfbb.jpg", "table_caption": [], "table_footnote": ["Table 5: Quantitative Results Per-Scene in Robots dataset. "], "page_idx": 15}, {"type": "table", "img_path": "vcGEV6m5m2/tmp/0f8bf61b8d1b234813dbc57ef5302fa76f690723b34c50c6d6368876b111c69f.jpg", "table_caption": ["Table 6: Optimization time and required resources for each scene in the D-NeRF dataset. "], "table_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "vcGEV6m5m2/tmp/fbd86f0c219f2fb2e48e1de3f1ba0b3f526e856a586226edaca1d3123ea730f6.jpg", "img_caption": ["Figure 8: Compare rendered images between canonical space and the warp space of timestamp 0. (a) canonical space in Dynamic stage, (b) at time 0 in Dynamic stage, (c) canonical space in Kinematic stage, (d) at time 0 in Kinematic stage, (e)LBS at time 0 in Kinematic stage (f)ground truth at time 0. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "vcGEV6m5m2/tmp/321bfc9c59bf0e9252d8ba12a0a1b4aee9ae23188a7a2d59d6dee52763e58cb4.jpg", "img_caption": ["(a) AP-NeRF (b) ours (c) ground truth (d) AP-NeRF (e) ours (f) ground truth Figure 7: Compare to AP-NeRF, ours method are more robust for complex motion. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table 7: Ablation study for the number of initial superpoint $M$ on the \u2018hellwarrior\u2019 scene of D-NeRF dataset. ", "page_idx": 16}, {"type": "table", "img_path": "vcGEV6m5m2/tmp/b14b51846c795b94bc15acf0c1224a78301e08f8feb3d564651af06764984269.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: The abstract and introduction clearly state the main contributions, which match experimental results. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: We discuss the limitations of this paper in Sec. 5.1 ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: This paper does not include theoretical results. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We provide disclose all information in Sec. 3 and Appendix A needed to reproduce the experimental results of paper. Besides, we will release the source code once the paper is accepted. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We released the source code on GitHub, i.e., https://github.com/dnvtmf/ SK_GS. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: We specify all the training and test details in Sec. 4,Sec. 3 and Appendix A. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [No] ", "page_idx": 19}, {"type": "text", "text": "Justification: We do not report error bars or other information about statistical significance as this is not a common procedure in the field and does not contribute to understanding our evaluation. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We provide sufficient information on the computer resources in the paper and code. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We conducted this research in every respect, with the NeurIPS Code of Ethics . Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We discuss both potential positive societal impacts and negative societal impacts of this paper in Sec. 5.2. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 20}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 21}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We cite the original paper that produced the code package or dataset. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}]