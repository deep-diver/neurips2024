[{"figure_path": "ZjgcYMkCmX/figures/figures_6_1.jpg", "caption": "Figure 2: The axis represents (estimated) (non)compatibility values. (a) Rewards r whose true (non)compatibility C(r) := \u03a4\u03c1,\u03c0\u03b5 (r) is far from threshold \u2206 by at least e, are correctly classified, while (b) in the opposite case, rewards can be mis-classified. (c) The red interval [\u0394 \u2013 \u20ac, \u0394 + \u20ac] exemplifies the set of rewards {r\u2208R||C(r) \u2013 \u2206| \u2264 \u20ac} that are (potentially) mis-classified. The length of the interval reduces with e.", "description": "The figure illustrates the concept of PAC framework for IRL classification problem. It shows how rewards are classified based on their (non)compatibility with the expert policy. The rewards whose true (non)compatibility is far from the threshold are correctly classified, while those near the threshold can be misclassified.  The uncertainty region shrinks as the accuracy increases.", "section": "4.3 A Learning Framework for Online IRL Classification"}, {"figure_path": "ZjgcYMkCmX/figures/figures_6_2.jpg", "caption": "Figure 1: Flow-chart of CATY-IRL.", "description": "The figure shows a flowchart of the CATY-IRL algorithm. The algorithm consists of two phases: exploration and classification. In the exploration phase, the algorithm collects a dataset D. In the classification phase, the algorithm estimates the expert's performance JE(r) using the expert dataset DE and estimates the optimal performance J*(r) via planning with dataset D. The algorithm then classifies the reward r based on the (non)compatibility C(r) = J*(r) - JE(r) and a given threshold \u0394. ", "section": "5 CATY-IRL: A Provably Efficient Algorithm for IRL"}, {"figure_path": "ZjgcYMkCmX/figures/figures_28_1.jpg", "caption": "Figure 3: In this figure, the point at the center represents the initial state s0 = d0 of the environment M, and each ray starting from it represents the occupancy measure dp,\u03c0 of some policy \u03c0. The figure aims to provide the intuition that policies with rays close to each other induce similar visit distributions (e.g., both point towards the same direction in some grid-world), and policies with rays far away from each other point toward very different directions (i.e., they have different occupancy measures). The red area in the right denotes the set of directions (occupancy measures dp,\u03c0 for some \u03c0) that are close in ||\u00b7||1 norm to the direction of the expert dp,\u03c0E.", "description": "The figure shows the occupancy measures of different policies as rays starting from the initial state.  Policies with similar occupancy measures have rays close together, while those with dissimilar measures have rays far apart. The red area highlights the policies close to the expert's policy in the L1 norm.", "section": "4.1 Compatible Rewards"}, {"figure_path": "ZjgcYMkCmX/figures/figures_38_1.jpg", "caption": "Figure 4: Hard instances.", "description": "This figure depicts the hard instances used to prove the lower bound for the sample complexity of RFE and IRL.  The key components are the initial state (s<sub>w</sub>), absorbing states (s<sub>g</sub>, s<sub>b</sub>, s<sub>E</sub>), and a full A-ary tree of depth d-1 with root s<sub>root</sub> and leaves L.  The expert policy (\u03c0<sup>E</sup>) leads to the absorbing state s<sub>E</sub>, while other actions lead to the tree. Transitions from leaves lead to s<sub>g</sub> (good state with reward 1) or s<sub>b</sub> (bad state with reward 0) with probabilities determined by parameter \u03b5'. This construction forces any algorithm to explore a significant portion of the state space to distinguish between MDP instances that are close in terms of the Hausdorff distance.", "section": "6.1 The Theoretical Limits of IRL (and RFE) in the Tabular Setting"}, {"figure_path": "ZjgcYMkCmX/figures/figures_40_1.jpg", "caption": "Figure 4: Hard instances.", "description": "The figure shows hard instances used in the lower bound proof. The instances are constructed using a binary tree structure, where the root node is connected to two subtrees. Each subtree consists of multiple absorbing states. The expert's policy leads to an absorbing state that gives a reward, and other actions lead to other absorbing states with either reward +1 or 0. The transitions to these absorbing states are probabilistic. It is used to demonstrate that a certain number of samples are needed to distinguish between two instances.", "section": "6.1 The Theoretical Limits of IRL (and RFE) in the Tabular Setting"}]