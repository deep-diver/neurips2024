[{"figure_path": "yCh1z6Dcto/figures/figures_1_1.jpg", "caption": "Figure 1: An overview of fixed-point forward gradient learning. The pipeline includes quantized weights perturbation, quantized forward gradient calculation through two forward calls with perturbed weights, and quantized weights update. Each process is explained in details in section 3.3.", "description": "This figure illustrates the process of fixed-point forward gradient learning. It starts with quantized weights which are perturbed using a random perturbation vector.  These perturbed weights are then used in two forward passes through the model, resulting in two loss values.  These loss values are used to calculate the quantized forward gradients. Finally, these gradients are used to update the quantized weights, completing one iteration of the training process.  The entire process is designed to be performed using fixed-point arithmetic, suitable for resource-constrained edge devices.", "section": "3 Quantized Forward Gradient Learning"}, {"figure_path": "yCh1z6Dcto/figures/figures_5_1.jpg", "caption": "Figure 1: An overview of fixed-point forward gradient learning. The pipeline includes quantized weights perturbation, quantized forward gradient calculation through two forward calls with perturbed weights, and quantized weights update. Each process is explained in details in section 3.3.", "description": "This figure illustrates the process of fixed-point forward gradient learning, which involves three main steps: 1. Perturbing the quantized weights using a random perturbation vector; 2. Calculating the quantized forward gradient using two forward passes with the perturbed weights; and 3. Updating the quantized weights using the calculated gradient.  The figure highlights the use of quantization throughout the process, emphasizing its suitability for resource-constrained edge devices.", "section": "3 Quantized Forward Gradient Learning"}, {"figure_path": "yCh1z6Dcto/figures/figures_8_1.jpg", "caption": "Figure 2: Ablation studies on cross-domain adaptation. The accuracy numbers (with standard deviation) are averaged over 5 runs.", "description": "This figure shows the results of ablation studies on cross-domain adaptation using ViT Tiny on the Visual Wake Word (VWW) dataset.  The left panel is a bar chart comparing the mean accuracy and standard deviation across different training methods: zero-shot, backpropagation (BP) with fp16 precision, forward gradient (FF) with fp16 precision (with different m values for gradient averaging), and quantized FF (16w8a and 8w8a with different m values) with both linear probing (LP) and visual-prompt tuning (D-VPT).  The right panel illustrates the sharpness-aware update method, a variation used in FF. The diagram shows how the weights are perturbed at a neighborhood position to avoid sharp minima in the loss landscape.", "section": "4.2 Cross-domain Adaptation"}, {"figure_path": "yCh1z6Dcto/figures/figures_13_1.jpg", "caption": "Figure 3: Comparison of Memory Usage during Training. BP: backpropagation, FF: forward gradient learning, fp16: 16-bit float point, Quant: 16w8a, FT: full fine-tuning, LP: linear probing.", "description": "This figure compares the total and scratch memory usage during training for different model architectures (ViT Tiny, ResNet18, FeatResNet12) and training methods (backpropagation (BP), forward gradient (FF), full fine-tuning (FT), linear probing (LP), and quantized (Quant)).  It visually demonstrates the memory savings achieved by using forward gradient learning, especially when combined with quantization and linear probing.", "section": "B Few-shot learning experiments"}, {"figure_path": "yCh1z6Dcto/figures/figures_13_2.jpg", "caption": "Figure 3: Comparison of Memory Usage during Training. BP: backpropagation, FF: forward gradient learning, fp16: 16-bit float point, Quant: 16w8a, FT: full fine-tuning, LP: linear probing.", "description": "This figure compares the memory usage of backpropagation (BP) and forward gradient learning (FF) during the training process for different model architectures (ViT Tiny, ResNet18, FeatResNet12).  It shows the total memory usage and the scratch memory usage separately.  The scratch memory is the memory needed for intermediate activations and gradients during the training process.  The figure shows that FF significantly reduces the scratch memory usage compared to BP, especially when using the quantized version of FF (16w8a).  This reduction is more pronounced in full fine-tuning (FT) compared to linear probing (LP).", "section": "B Few-shot learning experiments"}, {"figure_path": "yCh1z6Dcto/figures/figures_14_1.jpg", "caption": "Figure 3: Comparison of Memory Usage during Training. BP: backpropagation, FF: forward gradient learning, fp16: 16-bit float point, Quant: 16w8a, FT: full fine-tuning, LP: linear probing.", "description": "This figure compares the total and scratch memory usage during training for different model architectures (CRNN and ViT Base) and training methods (backpropagation (BP) and forward gradient learning (FF)). It shows that forward gradient learning significantly reduces memory usage compared to backpropagation, especially for the ViT Base model, achieving a reduction of up to 2.8x in scratch memory when using fp16 precision.", "section": "B.1 Vision Tasks"}, {"figure_path": "yCh1z6Dcto/figures/figures_14_2.jpg", "caption": "Figure 4: Comparison of Memory Usage during Training. BP: backpropagation, FF: forward gradient learning, fp16: 16-bit float point, Quant: 16w8a, FT: full fine-tuning, LP: linear probing.", "description": "The figure shows the memory usage comparison between backpropagation (BP) and forward gradient learning (FF) during training.  It breaks down the memory usage into total memory and scratch memory for different model architectures (CRNN and ViT Base) and training methods (full fine-tuning (FT) and linear probing (LP)).  The results highlight the significant memory reduction achieved by FF, particularly when using quantization (Quant).", "section": "B.2 Audio Tasks"}, {"figure_path": "yCh1z6Dcto/figures/figures_15_1.jpg", "caption": "Figure 2: Ablation studies on cross-domain adaptation. The accuracy numbers (with standard deviation) are averaged over 5 runs.", "description": "This figure presents ablation studies on cross-domain adaptation using ViT tiny backbone on the Visual Wake Word (VWW) dataset.  It shows a comparison of the classification accuracy with standard deviation obtained through different methods: Linear probing (LP), Visual prompt tuning with deep prompts (D-VPT), using floating point (fp16) and quantized (16w8a) precision with different numbers of forward gradient averaging (m=1, m=3).  The results demonstrate the impact of various methods and hyperparameters on the adaptation performance.", "section": "4.2 Cross-domain Adaptation"}, {"figure_path": "yCh1z6Dcto/figures/figures_17_1.jpg", "caption": "Figure 6: 2D visualization of loss landscape and loss trajectory during training under BP and QZO-FF. Both forward and backward learning shows a locally smooth loss contour, and the trajectory follows the gradient descent direction, with forward gradient learning taking a more conservative step after each epoch, resulting in slower convergence. We also observed that a good initialization (e.g., pre-trained model) is critical for forward gradient learning. Therefore, the convergence may not be guranteed if a model is trained from scratch. However, it is still promising that quantized forward gradients to be used for model adaptation on low resource devices, in which a general pre-trained model has been deployed.", "description": "This figure shows a 2D visualization of the loss landscape and the training trajectory for both backpropagation (BP) and quantized zero-order forward-forward gradient (QZO-FF) methods.  The loss landscape is relatively smooth for both methods. The QZO-FF trajectory shows a more gradual descent compared to BP, indicating slower convergence but potentially better generalization.", "section": "4.3 In-domain OOD Adaptation"}, {"figure_path": "yCh1z6Dcto/figures/figures_17_2.jpg", "caption": "Figure 6: 2D visualization of loss landscape and loss trajectory during training under BP and QZO-FF. Both forward and backward learning shows a locally smooth loss contour, and the trajectory follows the gradient descent direction, with forward gradient learning taking a more conservative step after each epoch, resulting in slower convergence. We also observed that a good initialization (e.g., pre-trained model) is critical for forward gradient learning. Therefore, the convergence may not be guranteed if a model is trained from scratch. However, it is still promising that quantized forward gradients to be used for model adaptation on low resource devices, in which a general pre-trained model has been deployed.", "description": "This figure visualizes the loss landscape and training trajectory for both backpropagation (BP) and quantized zero-order forward gradient (QZO-FF) methods. The 2D contour plots show the loss landscape, and the line plots illustrate the training trajectory within that landscape.  The results show that while both methods exhibit smooth loss contours, QZO-FF exhibits slower convergence.", "section": "E Empirical Studies, Discussions and Limitations"}, {"figure_path": "yCh1z6Dcto/figures/figures_17_3.jpg", "caption": "Figure 6: 2D visualization of loss landscape and loss trajectory during training under BP and QZO-FF. Both forward and backward learning shows a locally smooth 2D loss contour, and the trajectory follows the gradient descent direction, with forward gradient learning taking a more conservative step after each epoch. It is observed that 8-bit quantization of weights does not lead to model convergence. Therefore, 16-bit weights quantization is necessary for QZO-FF.", "description": "This figure visualizes the 2D loss landscape and training trajectory for both backpropagation (BP) and the proposed quantized zero-order forward gradient (QZO-FF) method.  The plots show that both methods navigate a relatively smooth loss landscape.  QZO-FF exhibits a more cautious step size compared to BP.  The results highlight that 8-bit quantization of weights is insufficient for QZO-FF to converge, requiring 16-bit quantization for effective training.", "section": "E Empirical Studies, Discussions and Limitations"}, {"figure_path": "yCh1z6Dcto/figures/figures_17_4.jpg", "caption": "Figure 6: 2D visualization of loss landscape and loss trajectory during training under BP and QZO-FF. Both forward and backward learning shows a locally smooth 2D loss contour, and the trajectory follows the gradient descent direction, with forward gradient learning taking a more conservative step after each epoch. It is observed that 8-bit quantization of weights does not lead to model convergence. Therefore, 16-bit weights quantization is necessary for QZO-FF.", "description": "This figure visualizes the loss landscape and training trajectory for both backpropagation (BP) and quantized zero-order forward gradient (QZO-FF) methods.  The 2D plots show the loss landscape, with contour lines representing different loss values. The trajectories show the path taken by the model's parameters during training. The results indicate that QZO-FF follows a smoother, more conservative path compared to BP, but still converges to a low-loss region.  It also highlights that using 8-bit quantization for the weights prevents the QZO-FF method from converging, whereas 16-bit quantization allows for successful convergence.", "section": "4.3 In-domain OOD Adaptation"}, {"figure_path": "yCh1z6Dcto/figures/figures_17_5.jpg", "caption": "Figure 6: 2D visualization of loss landscape and loss trajectory during training under BP and QZO-FF. Both forward and backward learning shows a locally smooth 2D loss contour, and the trajectory follows the gradient descent direction, with FF taking a more conservative step after each epoch. It is observed that 8-bit quantization of weights does not lead to model convergence. Therefore, 16-bit weights quantization is necessary for QZO-FF.", "description": "This figure visualizes the loss landscape and training trajectory for both backpropagation (BP) and the proposed quantized zero-order forward gradient (QZO-FF) method.  The 2D plots show the loss surface as contour lines, with the training trajectory overlaid as a sequence of points.  The plots demonstrate that both methods navigate a relatively smooth loss landscape.  However, QZO-FF shows a more cautious trajectory compared to BP. A key finding is that 8-bit weight quantization is insufficient for QZO-FF, highlighting the necessity of 16-bit quantization for successful training.", "section": "4.3 In-domain OOD Adaptation"}, {"figure_path": "yCh1z6Dcto/figures/figures_17_6.jpg", "caption": "Figure 6: 2D visualization of loss landscape and loss trajectory during training under BP and QZO-FF. Both forward and backward learning shows a locally smooth loss contour, and the trajectory follows the gradient descent direction, with forward gradient learning taking a more conservative step after each epoch, resulting in slower convergence. We also observed that a good initialization (e.g., pre-trained model) is critical for forward gradient learning. Therefore, the convergence may not be guranteed if a model is trained from scratch. However, it is still promising that quantized forward gradients to be used for model adaptation on low resource devices, in which a general pre-trained model has been deployed.", "description": "This figure visualizes the loss landscape and training trajectories using both backpropagation (BP) and quantized zero-order forward gradient learning (QZO-FF).  It highlights the smoother loss contour and more conservative step size of QZO-FF compared to BP, indicating that a good model initialization is key for QZO-FF's successful convergence. The figure also implicitly suggests that despite slower convergence, QZO-FF remains promising for low-resource device adaptation due to its reduced memory footprint.", "section": "4.3 In-domain OOD Adaptation"}, {"figure_path": "yCh1z6Dcto/figures/figures_17_7.jpg", "caption": "Figure 6: 2D visualization of loss landscape and loss trajectory during training under BP and QZO-FF. Both forward and backward learning shows a locally smooth loss contour, and the trajectory follows the gradient descent direction, with forward gradient learning taking a more conservative step after each epoch, resulting in slower convergence. We also observed that a good initialization (e.g., pre-trained model) is critical for forward gradient learning. Therefore, the convergence may not be guranteed if a model is trained from scratch. However, it is still promising that quantized forward gradients to be used for model adaptation on low resource devices, in which a general pre-trained model has been deployed.", "description": "This figure visualizes the loss landscape and training trajectories of both backpropagation (BP) and quantized zero-order forward gradient learning (QZO-FF) methods.  It shows that both methods exhibit locally smooth loss surfaces. However, QZO-FF demonstrates a more conservative trajectory with slower convergence compared to BP. The figure also highlights the importance of good model initialization for QZO-FF's success, as training from scratch may not guarantee convergence.  The use of 8-bit quantization for weights is shown to be insufficient for QZO-FF to converge, requiring at least 16-bit precision.", "section": "4.3 In-domain OOD Adaptation"}, {"figure_path": "yCh1z6Dcto/figures/figures_17_8.jpg", "caption": "Figure 6: 2D visualization of loss landscape and loss trajectory during training under BP and QZO-FF. Both forward and backward learning shows a locally smooth loss contour, and the trajectory follows the gradient descent direction, with forward gradient learning taking a more conservative step after each epoch, resulting in slower convergence. We also observed that a good initialization (e.g., pre-trained model) is critical for forward gradient learning. Therefore, the convergence may not be guranteed if a model is trained from scratch. However, it is still promising that quantized forward gradients to be used for model adaptation on low resource devices, in which a general pre-trained model has been deployed.", "description": "This figure visualizes the loss landscape and training trajectory for both backpropagation (BP) and quantized zero-order forward gradient learning (QZO-FF).  The 2D contour plots show the loss surface, with the trajectory indicating the path taken during training.  The results suggest that QZO-FF converges more slowly than BP, but still reaches a relatively good minimum.", "section": "4.3 In-domain OOD Adaptation"}]