[{"figure_path": "P3v3x7HnV0/tables/tables_7_1.jpg", "caption": "Table 6: Success rates after ablating the causality in QueST.", "description": "This table presents the results of ablating the causality in the QueST model.  It shows the success rates for LIBERO-90 and few-shot learning experiments, comparing the performance of QueST when different components of the network are not causal (i.e., lack the inductive bias of only attending to past information). The results demonstrate that maintaining causality is crucial for QueST's performance, particularly in few-shot scenarios.", "section": "5.6 Ablations"}, {"figure_path": "P3v3x7HnV0/tables/tables_14_1.jpg", "caption": "Table 3: Stage 1 Parameters", "description": "This table lists the hyperparameters used in Stage 1 of the Quantized Skill Transformer model, specifically for the self-supervised skill abstraction stage.  It details the dimensions of the encoder and decoder, sequence length, number of attention heads and layers in both encoder and decoder, attention dropout rate, FSQ quantization levels, number of convolutional layers, and the downsampling factor used in the encoder.", "section": "B.1 Hyperparameters"}, {"figure_path": "P3v3x7HnV0/tables/tables_14_2.jpg", "caption": "Table 4: Stage 2 Parameters", "description": "This table lists the hyperparameters used in Stage 2 of the Quantized Skill Transformer model.  These hyperparameters govern the autoregressive transformer's behavior, including aspects like vocabulary size, network architecture depth and width, dropout regularization, and inference-time settings such as temperature and beam search width. The 'decoder loss scale' and 'observation history' parameters influence the training and the way the model incorporates prior observations.", "section": "B Experiment Details"}, {"figure_path": "P3v3x7HnV0/tables/tables_16_1.jpg", "caption": "Table 5: Success rates after ablating design details of QueST.", "description": "This table presents the results of ablating different design choices of QueST on the LIBERO benchmark. It compares the performance of QueST with variations where (1) Vector Quantization (VQ) is used instead of Finite Scalar Quantization (FSQ), (2) The decoder is conditioned on observations, and (3) A mirrored decoder is used instead of the proposed decoder. The \"Ours\" column represents the performance of the original QueST model.", "section": "5.6 Ablations"}, {"figure_path": "P3v3x7HnV0/tables/tables_16_2.jpg", "caption": "Table 6: Success rates after ablating the causality in QueST.", "description": "This table shows the results of ablating the causality in the QueST model.  It compares the performance of the model with different levels of causality removed from the encoder, decoder, or both, against the fully causal model.  The results demonstrate the importance of causality for achieving good performance, particularly in the few-shot learning setting.", "section": "5.6 Ablations"}, {"figure_path": "P3v3x7HnV0/tables/tables_16_3.jpg", "caption": "Table 7: Success rates for decoder finetuning settings in few-shot IL.", "description": "This table presents the results of an ablation study on the impact of fine-tuning the decoder in the few-shot imitation learning setting. It compares the success rates achieved with a frozen decoder versus a fine-tuned decoder, using different loss scales (10 and 100). The results indicate the performance improvement obtained by fine-tuning the decoder for this specific setting.", "section": "4.3 Inference with Quantized Skill Transformer"}, {"figure_path": "P3v3x7HnV0/tables/tables_17_1.jpg", "caption": "Table 8: LIBERO 5-shot IL success rates across unseen 10 tasks. Results across 9 random seeds.", "description": "This table presents the success rates of different models on 10 unseen tasks from the LIBERO-LONG dataset.  Each model was fine-tuned using only 5 demonstrations per task.  The results represent the average success rate across 9 separate random trials, showing the performance variability.", "section": "5.3 Few-shot Transfer to Unseen Tasks"}, {"figure_path": "P3v3x7HnV0/tables/tables_17_2.jpg", "caption": "Table 9: MetaWorld 5-shot IL success rates across 5 unseen tasks. Results across 5 random seeds.", "description": "This table presents the results of a few-shot imitation learning experiment on the MetaWorld benchmark.  Five unseen tasks were used, and the model was fine-tuned on only five demonstrations from each task.  The table shows the average success rate across five random seeds for each of the tasks and for each of the compared methods. The methods compared are ResNet-T, ACT, Diffusion Policy, PRISE, VQ-BeT, and QueST.  The success rate is a measure of how often the robot successfully completed the task.", "section": "5.3 Few-shot Transfer to Unseen Tasks"}, {"figure_path": "P3v3x7HnV0/tables/tables_17_3.jpg", "caption": "Table 9: MetaWorld 5-shot IL success rates across 5 unseen tasks. Results across 5 random seeds.", "description": "This table presents the success rates achieved by different models (ResNet-T, ACT, Diffusion Policy, PRISE, VQ-BeT, and QueST) on five unseen tasks from the MetaWorld benchmark.  The results are based on a 5-shot learning setting, meaning that each model was finetuned using only five demonstrations from each task.  The table shows that QueST significantly outperforms the baselines in this setting.", "section": "5.3 Few-shot Transfer to Unseen Tasks"}, {"figure_path": "P3v3x7HnV0/tables/tables_18_1.jpg", "caption": "Table 9: MetaWorld 5-shot IL success rates across 5 unseen tasks. Results across 5 random seeds.", "description": "This table presents the success rates of different models on five unseen tasks from the MetaWorld benchmark using a 5-shot learning approach.  The results are averaged across five random seeds, providing a measure of performance variability and reliability.", "section": "5.3 Few-shot Transfer to Unseen Tasks"}, {"figure_path": "P3v3x7HnV0/tables/tables_19_1.jpg", "caption": "Table 11: MetaWorld multitask IL success rates across 45 tasks. Results across 5 random seeds.", "description": "This table presents the success rates of different models (ResNet-T, ACT, Diffusion Policy, PRISE, VQ-BeT, and QueST) on 45 multi-task imitation learning tasks from the MetaWorld benchmark.  The results are averaged across five random seeds, providing a measure of the model's performance and its robustness to variations due to random initialization.  Higher success rates indicate better performance on these manipulation tasks.", "section": "5.2 Performance on Multitask BC"}, {"figure_path": "P3v3x7HnV0/tables/tables_19_2.jpg", "caption": "Table 9: MetaWorld 5-shot IL success rates across 5 unseen tasks. Results across 5 random seeds.", "description": "This table presents the results of a few-shot imitation learning experiment on five unseen tasks from the MetaWorld benchmark.  The model was pre-trained and then fine-tuned on a small number of demonstrations (5) for each new task. The table shows the average success rate across five random seeds for each task, comparing the performance of QueST against several baseline methods (ResNet-T, ACT, Diffusion Policy, PRISE, VQ-BeT). The success rate is a measure of how well the model was able to successfully complete each task after few-shot learning.  The table helps to evaluate the generalization ability of different models in a low-data setting.", "section": "5.3 Few-shot Transfer to Unseen Tasks"}, {"figure_path": "P3v3x7HnV0/tables/tables_20_1.jpg", "caption": "Table 10: LIBERO-90 multitask IL success rates across 90 tasks. Results across 4 random seeds.", "description": "This table presents the success rates achieved by different models on 90 multitask imitation learning tasks from the LIBERO-90 benchmark.  The results are averages across four random seeds, providing a measure of the model's robustness and performance consistency across different runs.  Each row represents a single task, and the columns display the success rates for each of the compared models (ResNet-T, ACT, Diffusion Policy, PRISE, VQ-BeT, and QueST).  The table allows for a direct comparison of the models' performance on each task and overall multitask performance.", "section": "5.2 Performance on Multitask BC"}, {"figure_path": "P3v3x7HnV0/tables/tables_21_1.jpg", "caption": "Table 10: LIBERO-90 multitask IL success rates across 90 tasks. Results across 4 random seeds.", "description": "This table presents the results of a multitask imitation learning experiment on the LIBERO-90 benchmark.  It shows the success rates achieved by QueST and several baseline methods across 90 different manipulation tasks.  The results are averaged across four random seeds to assess the stability and reliability of the performance.", "section": "5.2 Performance on Multitask BC"}]