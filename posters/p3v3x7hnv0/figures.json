[{"figure_path": "P3v3x7HnV0/figures/figures_3_1.jpg", "caption": "Figure 1: Overview of Quantized Skill Transformer: we factorize the policy that outputs action based on task descriptions  and observations  encoding into two parts: \u03c0(A|o, e) = \u03c8\u03bf(\u0391|Z)\u03c0\u03c6(\u0396|\u03bf, e), where Z is a sequence of skill tokens for the action sequence A. In Stage I, we learn skill abstraction in a self-supervised way with a quantized autoencoder. In Stage II, we learn skill-based policy in the style of next-token prediction using a multi-modal transformer.", "description": "This figure provides a visual overview of the Quantized Skill Transformer (QueST) architecture. QueST is composed of two main stages: Stage I, which focuses on self-supervised skill abstraction, and Stage II, which utilizes the learned skills for decision-making in a multi-modal transformer setup. Stage I uses a causal convolutional 1D layer followed by a masked self-attention layer to process sequential action data.  A finite scalar quantization (FSQ) layer then transforms these into discrete latent codes (skill tokens).  Stage II employs an autoregressive transformer to predict the next token in a sequence of skill tokens, conditioned on the observations and task descriptions.", "section": "4 Method"}, {"figure_path": "P3v3x7HnV0/figures/figures_6_1.jpg", "caption": "Figure 1: Overview of Quantized Skill Transformer: we factorize the policy that outputs action based on task descriptions e and observations o encoding into two parts: \u03c0(A|o, e) = \u03c8\u03bf(\u0391|Z)\u03c0\u03c6(\u0396|\u03bf, e), where Z is a sequence of skill tokens for the action sequence A. In Stage I, we learn skill abstraction in a self-supervised way with a quantized autoencoder. In Stage II, we learn skill-based policy in the style of next-token prediction using a multi-modal transformer.", "description": "This figure shows the architecture of the Quantized Skill Transformer (QueST) model. QueST consists of two stages. Stage I is a self-supervised skill abstraction stage where a quantized autoencoder learns low-dimensional representations of action sequences.  These representations, called \"skill tokens\", capture temporal action abstractions. Stage II is a decision-making stage using a multi-modal transformer that takes task descriptions and observations as input along with the skill tokens to predict action sequences.", "section": "4 Method"}, {"figure_path": "P3v3x7HnV0/figures/figures_7_1.jpg", "caption": "Figure 1: Overview of Quantized Skill Transformer: we factorize the policy that outputs action based on task descriptions e and observations o encoding into two parts: \u03c0(A|o, e) = \u03c8\u03bf(\u0391|Z)\u03c0\u03c6(\u0396|\u03bf, e), where Z is a sequence of skill tokens for the action sequence A. In Stage I, we learn skill abstraction in a self-supervised way with a quantized autoencoder. In Stage II, we learn skill-based policy in the style of next-token prediction using a multi-modal transformer.", "description": "The figure illustrates the Quantized Skill Transformer (QueST) architecture, which factorizes the policy into two stages. Stage I involves self-supervised skill abstraction using a quantized autoencoder, learning a sequence of skill tokens from action sequences. Stage II uses a multi-modal transformer for decision making, predicting actions based on the skill tokens, task descriptions, and observations.", "section": "4 Method"}, {"figure_path": "P3v3x7HnV0/figures/figures_7_2.jpg", "caption": "Figure 1: Overview of Quantized Skill Transformer: we factorize the policy that outputs action based on task descriptions e and observations o encoding into two parts: \u03c0(A|o, e) = \u03c8\u03bf(\u0391|Z)\u03c0\u03c6(\u0396|\u03bf, e), where Z is a sequence of skill tokens for the action sequence A. In Stage I, we learn skill abstraction in a self-supervised way with a quantized autoencoder. In Stage II, we learn skill-based policy in the style of next-token prediction using a multi-modal transformer.", "description": "This figure shows the overall architecture of the Quantized Skill Transformer (QueST) model. It is composed of two stages. Stage I is a self-supervised skill abstraction stage that uses a quantized autoencoder to learn a representation of action sequences as a sequence of skill tokens. Stage II is a decision-making stage that uses a multi-modal transformer to predict actions based on the skill tokens and task descriptions. The figure also shows the flow of data through the model, including the input observations, task descriptions, and predicted actions.", "section": "4 Method"}, {"figure_path": "P3v3x7HnV0/figures/figures_8_1.jpg", "caption": "Figure 4: We conduct a sensitivity experiment across downsampling factors (a) and codebook sizes (b) on the LIBERO benchmark. For (a) we fix a sequence length of T = 32. Overall, we see that the few-shot version is more sensitive to hyperparameters and that F = 4 with 1024 codebook vectors are good choices.", "description": "The figure shows two plots that analyze the sensitivity of the model's performance to the downsampling factor and codebook size hyperparameters. The left plot shows that for LIBERO-90, the success rate generally increases as the downsampling factor increases, peaking at F=4 before declining. For the few-shot setting, the success rate peaks at F=4 but is much more sensitive to changes in this hyperparameter. The right plot demonstrates that for both LIBERO-90 and few-shot settings, the success rate generally increases with codebook size, though it begins to plateau beyond a codebook size of 1024. Overall, the plots suggest that a downsampling factor of 4 and a codebook size of 1024 are optimal choices for the model.", "section": "5.6 Ablations"}, {"figure_path": "P3v3x7HnV0/figures/figures_9_1.jpg", "caption": "Figure 1: Overview of Quantized Skill Transformer: we factorize the policy that outputs action based on task descriptions e and observations o encoding into two parts: \u03c0(A|o, e) = \u03c8\u03bf(\u0391|Z)\u03c0\u03c6(\u0396|\u03bf, e), where Z is a sequence of skill tokens for the action sequence A. In Stage I, we learn skill abstraction in a self-supervised way with a quantized autoencoder. In Stage II, we learn skill-based policy in the style of next-token prediction using a multi-modal transformer.", "description": "This figure shows the overall architecture of the Quantized Skill Transformer (QueST) model. It is composed of two stages: Stage I, which is a self-supervised skill abstraction module using a quantized autoencoder, and Stage II, which is a decision-making module that uses a multi-modal transformer to predict actions based on task descriptions and observations. The autoencoder learns a sequence of skill tokens from input actions and observation to generate low-dimensional representations of action sequences (skills), while the multi-modal transformer predicts sequences of actions by taking the skill tokens as input and performing next-token prediction. The model is designed to learn generalizable low-level skills from complex multitask demonstration data.", "section": "4 Method"}]