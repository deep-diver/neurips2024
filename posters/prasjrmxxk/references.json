{"references": [{"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2024", "reason": "This paper introduces Direct Preference Optimization (DPO), a core method upon which the proposed GRPO builds, offering a reward-free approach to RLHF."}, {"fullname_first_author": "Mohammad Gheshlaghi Azar", "paper_title": "A general theoretical paradigm to understand learning from human preferences", "publication_date": "2024", "reason": "This paper provides a theoretical framework for understanding learning from human preferences, which is crucial for analyzing the convergence and properties of the proposed GRPO."}, {"fullname_first_author": "Alec Radford", "paper_title": "Language models are unsupervised multitask learners", "publication_date": "2019-08-01", "reason": "This foundational paper establishes the capabilities of large language models (LLMs), which are central to the research presented on aligning them with human preferences."}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022", "reason": "This highly influential paper introduces a standard approach to aligning LLMs using reinforcement learning from human feedback (RLHF), providing a baseline for comparison with the proposed GRPO."}, {"fullname_first_author": "Daniel M Ziegler", "paper_title": "Fine-tuning language models from human preferences", "publication_date": "2019", "reason": "This foundational work on RLHF using human preference data provides an important earlier approach and context for the proposed GRPO method."}]}