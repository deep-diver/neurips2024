[{"type": "text", "text": "Group Robust Preference Optimization in Reward-free RLHF ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shyam Sundhar Ramesh1 Yifan Hu Iason Chaimalas University College London (UCL) ETH Zurich, EPFL University College London (UCL) ", "page_idx": 0}, {"type": "text", "text": "Viraj Mehta TensorZero ", "page_idx": 0}, {"type": "text", "text": "Pier Giuseppe Sessa ETH Zurich ", "page_idx": 0}, {"type": "text", "text": "Haitham Bou Ammar Ilija Bogunovic University College London (UCL) University College London (UCL) Huawei Noah\u2019s Ark Lab ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Adapting large language models (LLMs) for specific tasks usually involves finetuning through reinforcement learning with human feedback (RLHF) on preference data. While these data often come from diverse labelers\u2019 groups (e.g., different demographics, ethnicities, company teams, etc.), traditional RLHF approaches adopt a \"one-size-ftis-all\" approach, i.e., they indiscriminately assume and optimize a single preference model, thus not being robust to unique characteristics and needs of the various groups. To address this limitation, we propose a novel Group Robust Preference Optimization (GRPO) method to align LLMs to individual groups preferences robustly. Our approach builds upon reward-free direct preference optimization methods, but unlike previous approaches, it seeks a robust policy which maximizes the worst-case group performance. To achieve this, GRPO adaptively and sequentially weights the importance of different groups, prioritizing groups with worse cumulative loss. We theoretically study the feasibility of GRPO and analyze its convergence for the log-linear policy class. By fine-tuning LLMs with GRPO using diverse group-based global opinion data, we significantly improved performance for the worst-performing groups, reduced loss imbalances across groups, and improved probability accuracies compared to non-robust baselines. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "As the usage of large language models (LLMs) has grown in recent years, the question of their alignment has come to the forefront. Their remarkable capability to address a wide range of tasks (Radford et al. [36]) stems from pre-training on a self-supervised objective over internet-scale text. This vast internet-scale content, however, carries a higher risk of biases, inaccuracies, and controversial content than smaller, curated datasets. Thus, ensuring that the model\u2019s responses and behaviors correspond to human intentions and values is crucial. ", "page_idx": 0}, {"type": "text", "text": "Typical approaches to alignment [11, 34, 37] involve gathering preference feedback from human labelers to train models that reflect their desires. Such approaches often treat individual preferences as samples from a broader preference distribution. However, this perspective often oversimplifies the complex reality that human societies consist of numerous distinct groups (e.g., different demographics, ethnicities, company teams, etc.), each with their own set of preferences that can significantly diverge. Consequently, prevalent alignment strategies tend to adopt a \"one-size-fits-all\" model and disproportionately favor the preferences of the majority group, often at the expense of minority groups and their preferences, as illustrated in Figure 1. ", "page_idx": 0}, {"type": "image", "img_path": "PRAsjrmXXK/tmp/591ad50beeb0a3b2f2437a6afd460e4b163ea464ebc22f475a3b4aebc9c5be85.jpg", "img_caption": ["Figure 1: Current reward-free preference optimization methods typically optimize based on average human feedback. This often aligns predominantly with the preferences of the majority group (G1, R1 $>\\mathrm{R}2$ ) at the expense of minority groups (G2, $\\mathrm{R}2>\\mathrm{R}1$ ). In contrast, our GRPO algorithm introduces adaptive weighting for different user groups and prioritizes optimizing for the worst-case group performance, leading to better alignment for the most disadvantaged groups. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To improve alignment performance for even the most disadvantaged groups, we propose to robustly solve the problem of diverse group preferences by (i) including group information in the context of the LLM and (ii) optimizing against the worst-case alignment performance across all groups. We develop policies that guarantee equitable performance across all groups, ensuring that no group is disproportionately disadvantaged due to inherent biases or imbalances in the training data. ", "page_idx": 1}, {"type": "text", "text": "Related work. The established process for alignment of LLMs using Reinforcement Learning from Human Feedback (RLHF) is set out in [45, 60] and [34]. The RLHF fine-tuning process consists of learning a reward model from human comparisons between responses to a given prompt, using the Bradley-Terry model [5]. Then, one performs policy optimization using Proximal Policy Optimization [40] to learn a policy that maximizes the learned reward function. For a comprehensive overview and perspective of the RLHF topic, we refer the reader to [24, 7, 23]. ", "page_idx": 1}, {"type": "text", "text": "Due to the challenges of tuning PPO and the vulnerability of reward models ([50, 37, 17, 49]), alternative approaches to PPO-based RLHF have been proposed, including rejection sampling fine-tuning [14, 18, 49, 30] and conditional supervised fine-tuning [20, 54, 9]. In particular, Rafailov et al. [37] introduce Direct Preference Optimization (DPO), which optimizes policies directly based on human preferences, avoiding the need for a separate reward model. This approach simplifies training and reduces reward overftiting. Other studies, such as [1, 58, 47, 43, 16], propose novel reward-free RLHF methods, with some bypassing preference datasets altogether ([16, 6]). We utilize a reward-free framework similar to [37, 1], however, unlike previous works that assume a single preference distribution, we consider multiple preference distributions from diverse groups. Further, we aim to robustly fine-tune the LLM to ensure minimal disparity in performance across all groups. Other studies addressing robustness in preference optimization include [21] and [26]. However, these works primarily focus on different aspects of robustness, such as robustness to noise and resilience against out-of-preference data. ", "page_idx": 1}, {"type": "text", "text": "Robust language modeling techniques have been studied by [33, 52] to optimize performance of language models over a wide-range of topics. They consider robust pre-training of language models based on the group Distributionally Robust Optimization (DRO) approach. A concrete theoretical study of the group DRO approach was performed in [39] and applied to vision problems. These are designed by extending previous minimax algorithm for solving DRO from [31]. In the RLHF setup, [3] consider weighting of loss from different topics (harmless vs helpful) for robust reward learning. Also, Chakraborty et al. [8] consider robust policy optimization by learning multiple reward functions corresponding to sub-populations and learning a robust policy w.r.t. the learned rewards. Differing from these works, we embed group robustness directly into the reward-free tuning paradigm. We provide an concrete algorithm that adaptively weighs the loss for different groups and optimizes for a policy that minimizes the weighted loss. Further, our algorithm employs a novel gradient estimator tailored to the group robust DPO problem. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "In the non-robust setup, [57] also explore group-based preference learning with LLMs by including group information in prompts with a transformer module that is trained optimally to choose an example sequence of prompts, LLM responses, and group preferences for in-context learning. [49] consider alignment with user preferences assuming that each user has varied importance over the distinct metrics in their multi-objective reward model. In contrast, we are not modeling multi-reward objectives but consider a reward-free setting. And, our methodology directly models each group\u2019s preferences through a group-dependent latent reward model where the group dependency is injected through the prompt. Further, unlike the non-robust problem setups of [57, 49], we consider the robust alignment problem optimizing for the worst group performance. We detail other related works extensively in Appendix A. ", "page_idx": 2}, {"type": "text", "text": "Main Contributions. The following are the main contributions of this work: (i) We present GRPO, the group robust formulation of Direct Preference Optimization (DPO) [37], wherein we augment the context of the LLM with the group information, and pose the problem as a robust optimization problem to minimize the worst-case loss amongst the diverse groups. We also show that a naive application of group robustness to the LLM policy maximization objective does not offer robustness benefits. To the best of our knowledge, this is the first study that focuses on group robustness in RLHF preference optimization; (ii) We analyze the theoretical aspects of GRPO by examining the convergence and the feasibility of finding optimal solutions within the log-linear policy class. (iii) We present a tailored algorithm to tackle this robust optimization challenge, providing convergence guarantees for certain loss functions; (iv) We show the versatility of our approach by demonstrating how our algorithm can be utilized with other reward-free preference optimization methods such as Identity Preference Optimization (IPO) [1]. In particular, for the GR-IPO objective optimized over the log-linear policy class, we derive a closed-form weighted regression update for the policy parameters rather than a gradient update. To the best of our knowledge, this is a novel contribution towards efficient fine-tuning through preferential data; (v) Our empirical evaluations across synthetic datasets, real-world data, and publicly available LLMs show that the proposed GRPO significantly improves performance for the worst-performing groups, reduces loss imbalances across groups, and increases probability accuracies compared to non-robust baselines. ", "page_idx": 2}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We address the challenge of fine tuning a large language model (LLM) to align with user preferences. This process usually follows the Reinforcement Learning from Human Feedback (RLHF) protocol, using either an explicit reward model ([3, 34, 45, 60]) or an implicit reward model ([37]). RLHF typically comprises three key phases: (i) supervised fine-tuning of an initial (pre-trained) largelanguage model, (ii) reward learning, and (iii) RL fine-tuning. ", "page_idx": 2}, {"type": "text", "text": "In the supervised fine-tuning phase (SFT), the goal is to fine-tune a pre-trained LLM on a specific highquality dataset suited for the downstream task of interest. It results in a probabilistic model expressing the probability of the response $y$ given a prompt $x$ as $\\pi_{\\mathrm{ref}}(y|x)$ . Subsequently, in the reward learning phase, the goal is to learn a reward model from a dataset of prompts $x$ and responses $y_{w},y_{l}$ , with $y_{l}\\prec$ $y_{w}\\mid x$ meaning that human labellers preferred $y_{w}$ over $y_{l}$ . It is typically assumed that preferences follow some choice models with an unknown reward (utility) $r^{\\ast}(x,y)$ function. A popular model is the Bradley-Terry model [5] that assumes the preference distribution $p$ admits the following form : ", "page_idx": 2}, {"type": "equation", "text": "$$\np(y_{1}\\prec y_{2}\\mid x)=\\frac{\\exp(r^{*}(x,y_{2}))}{\\exp(r^{*}(x,y_{1}))+\\exp(r^{*}(x,y_{2}))}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Based on the above model, a maximum likelihood estimate of the reward function is obtained as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{r}\\{\\mathcal{L}_{R}(r;\\mathcal{D}):=-\\mathbb{E}_{(x,y_{w},y_{l})\\sim\\mathcal{D}}\\left[\\log\\left(\\sigma\\left(r(x,y_{w})-r(x,y_{l})\\right)\\right)\\right]\\},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\sigma(\\cdot)$ is the sigmoid function and $\\mathcal{D}$ represents the dataset consisting of $\\{(x,y_{w},y_{l})\\}$ ", "page_idx": 2}, {"type": "text", "text": "Then, in the RL fine-tuning phase the objective is to train a policy $\\pi$ that maximizes the learned reward function. Simultaneously, the policy should stay closely aligned with the reference, $\\pi_{\\mathrm{ref}}$ , as ", "page_idx": 2}, {"type": "text", "text": "quantified by the KL divergence, leading to the following KL-regularized optimization problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi}\\;\\mathbb{E}_{x\\sim\\mathcal{P}_{x}}\\Big[\\mathbb{E}_{y\\sim\\pi}\\left[r(x,y)\\right]-\\beta\\mathrm{KL}\\left[\\pi(y|x)\\|\\pi_{\\mathrm{ref}}(y|x)\\right]\\Big].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Direct Preference Optimization (DPO). The recent approach proposed by [37] exploits the closed-form solution of the problem in Equation (3) and sidesteps the explicit modelling of rewards to directly optimize the policy. Specifically, under the Bradley-Terry preference model, the reward function can be expressed directly in terms of the optimal policy $\\pi^{*}$ as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\nr(x,y)=\\beta\\log\\frac{\\pi^{*}(y\\mid x)}{\\pi_{\\mathrm{ref}}(y\\mid x)}+\\beta\\log Z(x),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for a partition function $\\begin{array}{r}{Z(x)=\\sum_{y}\\pi_{\\mathrm{ref}}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))}\\end{array}$ . Via a change of variable, finding the optimal reward function in Equation (2) is equivalent to finding the optimal policy $\\pi^{*}$ utilizing the given set of preference data $\\mathcal{D}$ . With a slight abuse of notation, we use $\\pi$ to denote $\\pi^{*}$ . Denote $\\begin{array}{r}{\\bar{h}_{\\pi}(x,y_{w},y_{l}):=\\log(\\frac{\\pi(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)})-\\log(\\frac{\\pi(y_{l}|\\bar{x})}{\\pi_{\\mathrm{ref}}(y_{l}|x)})}\\end{array}$ . Then, Equation (2) translates into the DPO loss: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{DPO}}(\\pi,\\mathcal{D})=-\\underset{(x,y_{w},y_{l})\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\log\\left(\\sigma(\\beta\\cdot h_{\\pi}(x,y_{w},y_{l}))\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "With a parameterized policy model $\\pi_{\\theta}$ , minimizing the DPO loss involves calculating the gradient over $\\theta$ using backpropagation and the log-probabilities of each completion, $y_{w}$ and $y_{l}$ , given the prompt $x$ for both the policy $\\pi_{\\theta}$ and the reference policy $\\pi_{\\mathrm{ref}}$ . ", "page_idx": 3}, {"type": "text", "text": "3 Group Robust Preference Optimization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we discuss Group Robust Preference Optimization (GRPO), i.e., instead of learning a reward function that maximizes the likelihood, we aim to derive (implicitly) a robust reward function and subsequently learn a robust DPO policy. ", "page_idx": 3}, {"type": "text", "text": "Group Preferences. Suppose that preferences come from an underlying latent reward $r^{*}(x,y,g)$ , with $\\bar{g}\\in\\mathcal{G}=\\{1,2,\\ldots,\\bar{K}\\}$ indexing the groups. When group information is available (e.g., as a text), we can represent the reward as $r^{\\ast}(x_{g},y)$ , where $x_{g}=x\\oplus g$ denotes merging1of the prompt with group information (e.g., string concatenation). We continue to apply a Bradley-Terry model as described in Equation (1), substituting $x$ with $x_{g}$ . Moreover, we assume access to a collective dataset $\\begin{array}{r}{\\mathcal{D}=\\bigcup_{g=1}^{K}\\mathcal{D}_{g}}\\end{array}$ where $\\mathcal{D}_{g}=\\{(x_{g}^{(i)},y_{w}^{(i)},y_{l}^{(i)})\\}_{i=1}^{\\bar{N}_{g}}$ with the available group information. Additionally, our dataset accommodates the exposure of different groups to identical prompts, meaning that the same $x$ can appear across various groups $g$ in our dataset, and these groups may favor different responses $y$ . ", "page_idx": 3}, {"type": "text", "text": "Given such $\\mathcal{D}$ , although one may obtain a common reward model using Equation (2), it could result in poor generalization for particular groups, especially with significant group-wise disparities in the data (see Figure 1). Such disparities might stem from imbalanced data across groups or difficulties associated with learning different groups. ", "page_idx": 3}, {"type": "text", "text": "GRPO Objective. Consequently, we propose to measure the alignment of the reward model on the worst-case group loss: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{g\\in G}\\;\\mathcal{L}_{R}(r;\\mathcal{D}_{g}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Incorporating the reward expression from (Equation (4)) into (Equation (6)), we establish the group robust preference optimization (GRPO) objective for a specified policy $\\pi$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{GR}}(\\pi):=\\operatorname*{max}_{g\\in\\mathcal{G}}\\mathcal{L}_{\\mathrm{DPO}}(\\pi,\\mathcal{D}_{g})=\\operatorname*{max}_{g\\in\\mathcal{G}}\\Big(-\\mathbb{E}_{(x_{g},y_{w},y_{l})\\sim\\mathcal{D}_{g}}\\Big[\\log\\Big(\\sigma\\big(\\beta h_{\\pi}(x_{g},y_{w},y_{l})\\big)\\Big)\\Big]\\Big).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Leveraging the equivalent formulation of maximizing over discrete set, the GRPO problem becomes ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pi}\\mathcal{L}_{\\mathrm{GR}}(\\pi)=\\operatorname*{min}_{\\pi}\\operatorname*{max}_{\\alpha\\in\\Delta_{K-1}}\\sum_{g=1}^{K}\\alpha_{g}\\Big(-\\mathbb{E}_{(x_{g},y_{w},y_{l})\\sim\\mathcal{D}_{g}}\\Big[\\log\\Big(\\sigma(\\beta h_{\\pi}(x_{g},y_{w},y_{l})\\Big)\\Big]\\Big),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\Delta_{K-1}$ represents the $(K-1)$ -dimensional simplex of probabilities.2 The inner maximization becomes a linear programming over simplex such that $\\alpha$ represents the weights of groups. In addition, it forms a two-player zero-sum game (see Section 3.1), where the policy $\\pi$ and $\\alpha$ act as opponents with inversely related payoffs. The DPO loss (logistic log loss) in Equation (7) can be replaced with alternatives like hinge or squared loss (see [47]). We label this objective GR-DPO when using DPO loss, and explore GRPO with squared loss in Section 4.1. ", "page_idx": 4}, {"type": "text", "text": "Applications. In this study, we do not assume any specific distribution for groups $\\mathcal{D}_{g}$ . The collection of prompts per group, $\\mathcal{P}_{x_{g}}$ , may have varying degrees of overlap. The GRPO framework accommodates both distinct and overlapping prompt scenarios across different groups. ", "page_idx": 4}, {"type": "text", "text": "Apart from human groups, GRPO can be useful in scenarios where groups $\\mathcal{D}_{g}$ represent distinct tasks sarpee cdiifsijco cinatt,e gaonrdi eGs R(eP.gO. , semeaktsh ,t op hoypstiicmsi, zceh peemrifsotrrym)a. nTcye peivcealnl ya, ctrhoessse t hper ommopstt  dcihstarlilbeuntgiionngs $\\{\\mathcal{P}_{x_{g}}\\}_{g=1}^{N}$ ", "page_idx": 4}, {"type": "text", "text": "GRPO is also applicable in scenarios where groups reflect diverse user preferences for a shared set of prompts, with the goal of achieving equitable performance across user groups. This contrasts with non-robust DPO, which aims to optimize preferences on average and might overlook minority groups. ", "page_idx": 4}, {"type": "text", "text": "Lastly, we acknowledge that the max-min objective of Equation (8) might be overly conservative, potentially degrading average performance. We explore a more balanced approach between worst-case and standard preference optimization objective in Appendix B.4. ", "page_idx": 4}, {"type": "text", "text": "3.1 Further Discussion and Insights ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "This section provides two insights regarding the GR-DPO loss in Equation (8). ", "page_idx": 4}, {"type": "text", "text": "Log-linear policy class. The zero-sum game perspective allows us to explore the presence of a Nash equilibrium, serving as a benchmark for convergence during the policy optimization process. Given that the domain of $\\alpha$ is a simplex $\\Delta_{K}$ (in Equation (8)), we further define a parameterized policy class $\\Pi_{\\theta}$ for the policy $\\pi_{\\theta}$ . We assume that the parameterized policy $\\pi_{\\theta}$ is of the form $\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}\\mid\\boldsymbol{x})=$ ye\u2208xYp  efx\u03b8p( xf,\u03b8y()x,y), where f\u03b8 is a linear function or a neural network, and \u03b8 belongs to a convex set \u0398. ", "page_idx": 4}, {"type": "text", "text": "In LLM fine-tuning, sometimes practitioners concentrate on modifying solely the final layer. It corresponds to a linear function, $\\bar{f_{\\theta}}(x,y)=\\theta^{T}\\phi(x,y)$ , with $\\phi(x,y)$ denoting the embedding derived from the language model removing its last layer, and $\\theta$ as the parameters of the last layer. When applying this linear parameterization, in conjunction with a uniform reference policy $\\pi_{\\mathrm{ref}}$ , the robust objective outlined in Equation (8) is as follows (details in Appendix B.1): ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta\\in\\Theta}\\operatorname*{max}_{\\alpha\\in\\Delta_{K-1}}\\sum_{g=1}^{K}\\alpha_{g}\\Big(-\\mathbb{E}_{(x_{g},y_{w},y_{l})\\sim\\mathcal{D}_{g}}\\Big[\\log\\Big(\\sigma\\big(\\beta\\langle\\phi(x,y_{w})-\\phi(x,y_{l}),\\theta\\rangle\\big)\\Big)\\Big]\\Big).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The objective defined in Equation (9) is concave with respect to $\\alpha$ and convex with respect to $\\theta$ . This structure allows the invocation of the minimax theorem for convex-concave functions ([42]) to assert the existence of a Nash equilibrium. ", "page_idx": 4}, {"type": "text", "text": "Proposition 3.1. Under log-linear parameterization of the policy class, there exists a Nash equilibrium for the group robust direct preference optimization problem in Equation (9). ", "page_idx": 4}, {"type": "text", "text": "Robust policy optimization. The earlier derivation for the GR-DPO objective ${\\mathcal L}_{\\mathrm{GR-DPO}}(\\pi)$ relies on incorporating robustness in the reward modeling step (in Equation (7)) while using the solution to the non-robust KL-regularized reward maximization objective in Equation (4). ", "page_idx": 4}, {"type": "text", "text": "Interestingly, we can obtain the identical expression for ${\\mathcal L}_{\\mathrm{GR-DPO}}(\\pi)$ if incorporating robustness in the KL-regularized reward maximization objective and using the reward function learnt in a non-robust way. Consider the robust KL-regularized reward maximization ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi}\\operatorname*{min}_{g\\in\\mathcal{G}}\\mathbb{E}_{x_{g}\\sim\\mathcal{P}_{x_{g}},y\\sim\\pi(\\cdot|x_{g})}\\Big[r(x_{g},y)-\\beta\\mathrm{KL}\\big[\\pi(y\\mid x_{g})||\\pi_{\\mathrm{ref}}(y\\mid x_{g})\\big]\\Big].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The following proposition characterizes such an invariant property. ", "page_idx": 4}, {"type": "text", "text": "2From a distributionally-robust viewpoint, this is equivalent to defining the uncertainty set as $\\begin{array}{r}{\\Big\\{\\sum_{g=1}^{K}\\alpha_{g}\\mathcal{D}_{g}:}\\end{array}$ $\\alpha\\in\\Delta_{K}\\}$ , and minimizing the worst-case expected loss across the uncertainty set. ", "page_idx": 4}, {"type": "text", "text": "1: Initialize: Step size $\\eta_{\\alpha}$ for group weights $\\alpha$ , step size $\\eta_{\\theta}$ for policy $\\pi$ with weights $\\theta$ , initial   \nweights $\\theta^{(0)}$ of the policy and weights over each group $\\alpha^{(0)}$ , Projection operator $\\mathrm{P}_{\\Theta}$   \n2: Input: Dataset $\\mathcal{D}$ with size $N=|\\mathcal{D}|$ , group size $N_{g}$ for $g=\\{1,2,\\cdots\\,,K\\}$ , loss $l(\\pi_{\\theta};\\cdot)$   \n3: for $t=1,\\dots,T$ do   \n4: \u03b1\u2032 \u2190\u03b1(t\u22121)   \n5: $\\begin{array}{r l}&{g\\sim\\mathrm{Categorical}(N_{1}/N,\\cdot\\cdot\\cdot\\cdot\\,,N_{K}/N),\\,(x_{g},y_{w},y_{l})\\sim\\mathcal{D}_{g}}\\\\ &{\\alpha_{g}^{\\prime}\\leftarrow\\alpha_{g}^{\\prime}\\exp\\eta_{\\alpha}\\Big(\\frac{N\\cdot l(\\pi_{\\theta}(t-1);(x_{g},y_{w},y_{l}))}{N_{g}}\\Big)\\quad//l\\,\\mathrm{Update~weights~for~group~}g}\\\\ &{\\alpha^{(t)}\\leftarrow\\alpha^{\\prime}/\\sum_{g^{\\prime}}\\alpha_{g^{\\prime}}^{\\prime}\\qquad/l\\,\\mathrm{Renormalize~}\\alpha}\\\\ &{\\theta^{(t)}\\leftarrow\\mathrm{P}_{\\Theta}\\Big(\\theta^{(t-1)}-\\eta_{\\theta}\\Big(\\frac{N\\alpha_{g}^{(t)}\\nabla_{\\theta}l(\\pi_{\\theta}(t-1);(x_{g},y_{w},y_{l}))}{N_{g}}\\Big)\\Big)\\,//\\,\\mathrm{Use}\\propto\\alpha\\,\\mathrm{u}\\,\\mathrm{update}\\,\\theta}\\\\ &{\\textit{s-}}\\end{array}$   \n6:   \n7:   \n8:   \n9: end for   \n10: Return: Output the robust policy $\\pi(\\theta^{(T)})$ ", "page_idx": 5}, {"type": "text", "text": "Proposition 3.2. Substituting the closed-form solution of the robust KL-regularized policy maximization problem (Equation (10)) into the robust reward maximization objective in Equation (6) leads to the same group robust DPO loss LGR\u2212DPO in Equation (8) . ", "page_idx": 5}, {"type": "text", "text": "The analysis leverages the fact that the optimal policy of Equation (10) is identical to the solution of the non-robust KL-regularized reward maximization in Equation (4) and is derived in Appendix B.2. ", "page_idx": 5}, {"type": "text", "text": "4 Algorithm ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we discuss the policy optimization algorithm for solving the group robust DPO problem in Equation (8). In particular, we aim to design an algorithm that performs updates in the parameterized space $\\Theta\\subset\\ensuremath{\\mathbb{R}}^{d}$ , i.e., updating $\\theta$ of the parameterized policy $\\pi_{\\theta}$ . Leveraging the perspective of the 2-player zero-sum game, we propose an alternating updating algorithm wherein one updates $\\alpha$ and $\\theta$ alternatively. We summarize the overall approach in Algorithm 1, which we discuss and analyze next. ", "page_idx": 5}, {"type": "text", "text": "We employ the DPO loss $l(\\pi_{\\theta};\\cdot)=\\log\\left(\\sigma(\\beta h_{\\pi_{\\theta}}(\\cdot))\\right)$ (Equation (5)) in Algorithm 1, however, our algorithm can support other preference optimization losses (see Section 4.1). The algorithm performs a gradient descent type update on $\\theta$ and a deterministic mirror ascent on $\\alpha$ using a Bregman divergence with the distance generating function as the KL divergence. Since the $\\alpha$ lies in a simplex and the objective is linear, the update of $\\alpha$ becomes multiplicative weights update with renormalization to a simplex via softmax (see Nemirovski et al. [32] for details). Further, the weights $\\alpha$ are determined by the cumulative losses $l(\\pi_{\\theta};\\cdot)$ accrued by each group, ensuring that groups with higher cumulative losses get higher weights. The size of the group $N_{g}$ appears as the empirical distribution $\\mathcal{D}_{g}$ involves $N_{g}$ . We call it alternating update as the updated $\\alpha^{t}$ is used in the update from $\\theta^{t-1}$ to $\\theta^{t}$ . In particular, the gradient descent type update on $\\theta$ is weighted by $\\alpha$ in order to orient the update towards groups with higher losses. The projection operator $\\mathrm{P}_{\\Theta}$ ensures the updated $\\theta^{t}$ lies within $\\Theta$ . ", "page_idx": 5}, {"type": "text", "text": "What does the weighted DPO update do? In Line 9 in Algorithm 1, the algorithm performs parameter updates based on the weighted gradients. By using the DPO loss, i.e., $l(\\pi_{\\theta};\\cdot)=\\log\\Big(\\sigma(\\beta h_{\\pi_{\\theta}}(\\cdot)\\Big)$ (see Equation (5)), we obtain the following gradient update expression ignoring the $N/N_{g}$ constant ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha_{g}^{(t)}\\nabla_{\\theta}l(\\pi_{\\theta^{(t-1)}};(x_{g},y_{w},y_{l}))=\\alpha_{g}^{(t)}\\nabla_{\\theta}\\log\\Big(\\sigma\\big(\\beta h_{\\pi_{\\theta^{(t-1)}}}(x_{g},y_{w},y_{l})\\big)\\Big)}\\\\ &{=\\alpha_{g}^{(t)}\\sigma\\big(r_{\\theta^{(t-1)}}(x_{g},y_{l})-r_{\\theta^{(t-1)}}(x_{g},y_{w})\\big)\\times[\\nabla_{\\theta}\\log\\pi_{\\theta^{(t-1)}}(y_{w}|x_{g})-\\nabla_{\\theta}\\log\\pi_{\\theta^{(t-1)}}(y_{w},y_{w})].}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The final term plays the critical role of enhancing the likelihood of the preferred response while simultaneously diminishing the likelihood of the rejected response. This adjustment is proportional to the disparity in rewards between the two responses. Moreover, the inclusion of $\\alpha_{g}$ is pivotal for ensuring group robustness. This coefficient scales the gradient w.r.t. $\\theta$ based on the cumulative loss previously received by all samples within a specific group. Such a mechanism ensures that the model\u2019s focus is increasingly directed towards groups that have historically suffered higher losses. Additionally, the scaling factor $N_{g}$ guarantees that groups with a smaller volume of data do not face a disadvantage.We defer further details in obtaining the gradient update expression to Appendix B.3. ", "page_idx": 5}, {"type": "text", "text": "We demonstrate the global convergence with the following proposition. ", "page_idx": 6}, {"type": "text", "text": "Proposition 4.1. Suppose that the loss $l(\\cdot;(x_{g},y,y^{\\prime}))$ is non-negative, convex, $B\\boldsymbol{\\nabla}$ \u2212Lipschitz continuous, and bounded by $B_{l}$ for all $(x_{g},y,y^{\\prime})\\in\\mathcal{X}\\oplus\\mathcal{G}\\times\\mathcal{Y}\\times\\mathcal{Y}$ and $\\|\\theta\\|_{2}\\leq B_{\\Theta}$ for all $\\theta\\in\\Theta$ with convex $\\Theta\\subset\\mathbb{R}^{d}$ . The error of the average iterate of Algorithm $^{\\,I}$ , i.e., $\\begin{array}{r}{\\pi_{\\bar{\\theta}^{(1:T)}}=\\frac{1}{T}\\sum_{t=1}^{T}\\theta^{t}}\\end{array}$ , satisfies ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathcal{L}_{\\mathrm{GR}}(\\pi_{\\bar{\\theta}^{(1:T)}})]-\\operatorname*{min}_{\\theta\\in\\Theta}\\mathcal{L}_{\\mathrm{GR}}(\\pi_{\\theta})=\\mathcal{O}\\big(T^{-1/2}\\big).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We defer the proof of this proposition to Appendix E. The analysis follows from an adaptation of the analysis in Nemirovski et al. [32] for the proposed sampling strategy in Algorithm $1^{3}$ . We note that when fine-tuning only the final layer of a LLM, the output policy exists within the log-linear policy class (see Section 3.1), and the corresponding loss function satisfies the assumptions in Proposition 4.1 (see Lemma E.1). ", "page_idx": 6}, {"type": "text", "text": "4.1 Group Robust Identity Preference Optimization ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The standard regularized reward maximization objective (Equation (3)) in DPO [37], tends to overlook the KL-regularization and learn deterministic policies. This learned policy assigns preference probability one to winning responses in the data which is often not realistic (see [1][Section 4.2] and Appendix C). Recently, Azar et al. [1] show that the standard regularized reward maximization objective (Equation (3)) in DPO [37] tends to overlook the KL-regularization and learn deterministic policies (see [1, Section 4.2] and Appendix C). They thus propose an alternative approach called Identity Preference Optimization (IPO) that is more likely to learn a randomized policy which assigns appropriate probability to the preferred response and prevents overfitting. Following a similar derivation as we did for group robust DPO with details given in Appendix C, we develop the corresponding group robust IPO (GR-IPO): ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pi}\\;\\mathcal{L}_{\\mathrm{GR}}(\\pi):=\\operatorname*{max}_{g\\in\\mathcal{G}}\\mathcal{L}_{\\mathrm{IPO}}(\\pi,\\mathcal{D}_{g})=\\operatorname*{max}_{\\alpha\\in\\Delta\\kappa-1}\\sum_{g=1}^{K}\\alpha_{g}\\Big(\\underset{(x_{g},y_{w},y_{t})\\sim\\mathcal{D}_{g}}{\\mathbb{E}}\\left[h_{\\pi}(x_{g},y_{w},y_{l})-\\frac{1}{2\\beta}\\right]^{2}\\Big).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "For the log-linear policy class (introduced in Section 3.1), the objective function simplifies to ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta\\in\\Theta}\\operatorname*{max}_{\\alpha\\in\\Delta_{K-1}}\\sum_{g=1}^{K}\\alpha_{g}\\Big(\\mathbb{E}_{(x_{g},y_{w},y_{l})\\sim\\mathcal{D}_{g}}\\Big[\\Big(\\langle\\phi(x_{g},y_{w})-\\phi(x_{g},y_{l}),\\theta\\rangle-\\frac{1}{2\\beta}\\Big)^{2}\\Big]\\Big).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "To solve the GR-IPO above, it suffices to use Algorithm 1 with slight modifications, see Algorithm 2 in Appendix C. In particular, the update of $\\theta$ is replaced by a weighted regression update: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{\\theta}\\leftarrow\\underset{\\theta\\in\\Theta}{\\arg\\operatorname*{min}}\\sum_{(x_{g},y_{w},y_{l})\\sim\\mathcal{D}}\\Big[\\frac{\\alpha_{g}}{N_{g}}\\big(\\langle\\phi(x_{g},y_{w})-\\phi(x_{g},y_{l}),\\theta\\rangle-\\frac{1}{2\\beta}\\big)^{2}\\Big].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "For fixed $\\alpha$ , we show (in Appendix C) that such an update admits a closed-form solution: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{\\theta}=\\frac{1}{2\\beta}(S^{T}W S)^{-1}S^{T}W\\mathbf{1}\\quad\\mathrm{~with~}\\quad W:=\\mathrm{Diag}\\left[\\frac{\\alpha_{g^{(1)}}}{N_{g^{(1)}}},\\cdot\\cdot\\cdot,\\frac{\\alpha_{g^{(N)}}}{N_{g^{(N)}}}\\right],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $g^{(i)}$ is the group of each sample $i.$ , $N_{g^{(i)}}$ is the number of samples in group $g^{(i)}$ and $\\mathbf{1}$ is a column vector of ones of dimension $N$ . Here $S$ is a matrix ${\\cal S}\\quad:=$ $\\left[(\\phi(x_{g}^{(1)},y_{w}^{(1)})-\\phi(x_{g}^{(1)},y_{l}^{(1)}))^{T},\\dots,(\\phi(x_{g}^{(N)},y_{w}^{(N)})-\\phi(x_{g}^{(N)},y_{l}^{(N)}))^{T}\\right]$ . Each row of $S$ represents the difference in feature mappings $\\phi$ of the preferred and less preferred response for each prompt. The group robust IPO (GR-IPO) algorithm is presented in Appendix $\\mathbf{C}$ , and its empirical results are shown in Section 5. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we study the empirical performance of our proposed Algorithm 1 on synthetic and real-world datasets4. First, we simulate multi-group data disparities by varying the size and preference distributions of two synthetic groups. In the real-world setup, we study the alignment of an LLM to the real preferences of people from various countries. We examine whether GRPO aligns the LLM in a more equitable manner to reduce discrepancies in alignment among various groups. Finally, we demonstrate that performance is improved by explicitly addressing the grouped nature of the data during alignment. ", "page_idx": 6}, {"type": "image", "img_path": "PRAsjrmXXK/tmp/ef74bae675bdb976e7bfece1de4a677190b5925c538a692402070933ce04accd.jpg", "img_caption": ["Figure 2: Synthetic experiments: Algorithm 1 (GR-DPO and GR-IPO) leads to a significantly lower worst-case validation loss and reward error compared to importance sampling (IS-DPO/IPO) and vanilla methods (DPO, IPO). Results refer to the scenario in which groups have different sizes and responses\u2019 distribution. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5.1 Synthetic Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We evaluate the performance of Algorithm 1 using synthetically generated group preference data for the loss function $l(\\pi_{\\theta;\\cdot})$ \u2013 either DPO loss or IPO loss and denote them as GR-DPO and GR-IPO, respectively. We compare them against vanilla DPO and IPO ([37, 1]), and the importance-sampling (IS) variants of DPO and IPO (where the loss of each datapoint is inversely weighted by its group data size). ", "page_idx": 7}, {"type": "text", "text": "Experimental Setup. Our experiments are designed to analyze settings where there exist multiple groups with distinct characteristics. We adapt the standard (non-group based) experimental setup proposed by [26] for the group preferences setting by incorporating group information into the reward function $r:\\mathcal{X}\\times\\mathcal{Y}\\times\\mathcal{G}\\to\\mathbb{R}$ . Here, $\\mathcal{X}$ represents a two-dimensional state space $[0,1]\\times$ $[0,1]$ , $\\boldsymbol{\\wp}$ denotes a discrete action space $\\{0,1,2,3,\\ldots,n\\}$ , and $\\mathcal{G}$ signifies a discrete group space $\\{0,1,2,\\ldots,K\\}$ . The reward function, defined by the group-dependent feature vector $\\phi(x,y,g)$ and parameter vector $\\theta_{g}$ , is given as $r(x,y,g):=\\,\\overbar{\\langle\\phi(x,\\bar{y},g),\\bar{\\theta}_{g}\\rangle}$ , while the feature vectors $\\phi(x,y,g)$ have a coordinate-flipped relationship and are defined in Appendix D.1. ", "page_idx": 7}, {"type": "text", "text": "We consider the following scenarios: (i) Groups are imbalanced in terms of size but have the same distribution over responses, (ii) Groups are balanced in terms of size but have different response distributions, and (iii) Groups are imbalanced in terms of size and also have different response distributions. Note that having different response distributions leads to a difference in the difficulty of learning, since groups with responses distant from each other (in terms of rewards or preference probabilities) are typically more distinguishable and easier to learn. We discuss in Appendix D.2 how we generate these three scenarios. ", "page_idx": 7}, {"type": "text", "text": "Implementation. Leveraging the linearity in the reward model, we utilize a log-linear policy class parameterized by \u03b8: \u03c0\u03b8(y|x) =  y\u2032e\u2208xYp  e\u27e8x\u03d5p( x\u27e8,\u03d5y(,xg,)y,\u2032\u03b8,\u27e9g),\u03b8\u27e9. We run Algorithm 1 for both DPO and IPO loss relative to the policy class detailed above with a dataset of 300 action pairs with preferences. ", "page_idx": 7}, {"type": "text", "text": "Evaluation Metrics. We use the following criteria to assess the performance of the algorithms: ", "page_idx": 7}, {"type": "text", "text": "Max Validation Loss. For each group $g$ , with preference data denoted as $(x^{i},y_{w}^{i},y_{l}^{i},g)_{i=1}^{N_{g}}$ , where $N_{g}$ is the number of data points in the group, we compute the DPO/IPO validation loss separately for each group and identify the maximum loss among them in each run. ", "page_idx": 7}, {"type": "text", "text": "Max Reward Error. This metric compares the true reward of the optimal action determined bwmyha $\\theta_{g}^{*}$ m  iuwnimctlh u etdrhreaost r  ooanfcl ryto hssets a ataeclstli  oagnnr dod uegpresom uienpds  ,e oawpctehi  mcraaullnc .ublya Itenes  trpieamwrtaaitcredu $\\hat{\\theta}$ rrf,ro orfr oser af ocdrha t eagv reiornuy p t,hg reao nfudop r $(x^{i},g)_{i=1}^{N_{g}}$ $g$ $\\mathbb{E}_{(x,g)\\sim(x^{i},g)_{i=1}^{N_{g}}}[\\operatorname*{max}_{y}\\langle\\phi(x,y,g)\\overset{*}{,}\\theta_{g}^{*}\\rangle-\\langle\\phi(x,\\arg\\operatorname*{max}_{y}\\langle\\phi(x,y,g),\\hat{\\theta}\\rangle,g),\\theta_{g}^{*}\\rangle].$ . ", "page_idx": 7}, {"type": "text", "text": "Results. We present the average performance of Algorithm 1 (error bars over 20 seeds) alongside baseline methods in Figure 2 for scenario (iii), while scenarios (i) and (ii) are Figures 4 and 5 in Appendix D.2 due to space constraints. Our findings indicate that the robust methods consistently surpass both vanilla and importance-sampling approaches. Notably, the robust methods demonstrate significant superiority in uneven group scenarios, where the importance-sampling technique falls short as it exclusively deals with data imbalance. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5.2 Global Opinion Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "For the real-data experiments, we consider the survey dataset GlobalOpinionQA ([15]) and the publicly available Gemma-2B model [48]. 5 The data contains multiple choice questions answered by participants from various countries, amounting to 2,554 questions covering various topics, including politics, media, technology, religion, race, and ethnicity. For each question, the dataset provides a probability vector over the choices, signifying the percentage of people from a particular country choosing each option. Note that this probability vector would be different for different countries. Hence, the goal is to align the LLM to the probability vector corresponding to each country in a robust manner. ", "page_idx": 8}, {"type": "text", "text": "We consider the following five countries in the dataset: Nigeria, Egypt, India, China and Japan, with data sizes 572, 570, 376, 309, and 712, respectively. We construct our training set as follows: For the SFT training, we choose the best option (the choice with the highest probability) as the target. For both IPO and GR-IPO training, we consider the best option as the winning response and another randomly chosen option as the losing response. We outline the exact prompt we use in Appendix D. ", "page_idx": 8}, {"type": "text", "text": "We run the SFT training for one epoch over the training data on the pre-trained Gemma-2B model. For both IPO/GR-IPO training we use the AdamW [27] optimizer with adaptive learning rates. For SFT/IPO/GR-IPO training, we apply the LoRA strategy to fine-tune all layers of the model. We then evaluate both the methods based on the worst group loss and accuracy. Here, the loss refers to the IPO loss for each group and the accuracy refers to the percentage of winning response and losing response pairs correctly ordered by the learned preference function (Equation (35)). We defer further training and hyperparameter details to Appendix D. ", "page_idx": 8}, {"type": "text", "text": "Results. We present the average performance of GR-IPO over five seeds alongside IPO in Figure 3 (top plots). Our findings indicate that GR-IPO outperforms IPO in terms of maximum group loss and minimum group reward accuracies. Moreover, GR-IPO effectively reduces the imbalance in loss values among different groups. Additionally, we observe an improvement in log-probability accuracies (which measure if the probability assigned by the fine-tuned model is higher for the winning response compared to the losing response) for both IPO and GR-IPO, with GR-IPO demonstrating better alignment for the worst-performing group compared to IPO. ", "page_idx": 8}, {"type": "text", "text": "Insights. We further note that the worst-performing groups are Groups-2,5, as shown in Figure 3. GR-IPO improves the loss for these groups by assigning more weight to them, as illustrated in Figure 3 (bottom middle plot). Additionally, we plot the initial log-probability accuracies for different groups in Figure 3 (bottom right plot), assessing how accurately the SFT model classifies the winning versus losing response for different groups. It is evident that Groups-2,5 are already underperforming. Given that SFT training converges within one epoch without any discrepancies between groups, this indicates that the base LLM inherently struggles with classifying responses for Groups-2,5. However, by employing GR-IPO, we have mitigated the imbalance in the performance of the fine-tuned model. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We formalize the problem of robustly aligning an LLM to preference distributions from diverse groups. To tackle the same, we introduced GRPO, a group robust formulation of reward-free RLHF, aiming to minimize worst-case loss among groups. We explored the theoretical aspects of GRPO and demonstrated its improved robust alignment performance through various experiments. We believe our approach will be highly valuable for future tailored LLM fine-tuning, specifically aimed at aligning with the needs of diverse teams and user groups. In a broader context, it holds promise for mitigating biases and discrepancies across various societal groups encountered in the task-specific adaptation of LLMs. ", "page_idx": 8}, {"type": "text", "text": "Limitations. When the dataset is balanced among groups and difficulty levels are comparable, our GRPO approach does not offer a significant advantage over standard reward-free RLHF algorithms. In addition, minimax methods often improve the worst group\u2019s performance at the cost of reducing the average or best group\u2019s performance. Our proposed GRPO formulation (see Equation (8)), ", "page_idx": 8}, {"type": "image", "img_path": "PRAsjrmXXK/tmp/f0d00dde4cabf5659ea3db10904f0db799313c769e63813499ccaae21246778e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 3: Global opinion data: Top plots: GR-IPO leads to better worst-case final test loss and reward accuracy compared to IPO. Moreover, it leads to more balanced losses across the different groups, reducing the gap between best and worst-group loss (Group-1 vs. Group-5). Bottom plots: Log-prob. accuracy (left plot) and group weights (middle plot) during GR-IPO training. GR-IPO increases the weight on worse-performing groups (Groups-2,5) and decreases it on high-performing ones (Groups-1,3,4), leading to better worst-case accuracy. Groups-2,5 are the ones with worse log-prob. accuracy at the beginning of training (right plot with a random subset of the training data). We show the corresponding end-of-training log-prob. accuracies for GR-IPO in Figure 13 of Appendix D. ", "page_idx": 9}, {"type": "text", "text": "stemming from a minimax framework, will comply with the same property. Hence, in scenarios where optimizing worst-case performance is less critical, we define a trade-off parameter to balance between the worst-case performance and the average performance. This modified objective and the necessary algorithmic changes are elaborated in Appendix B.4. The appropriate tuning of the trade-off parameter for the specific application remains a subject for future investigation. Further, we focus on settings with known groups, which are common in pluralistic alignment datasets and tasks (see [57, 44]). When groups are unknown, one can still undertake several approaches such as clustering, representation learning, feature analysis, expert consultations, etc., to help uncover group structures in the data. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "PGS was gratefully supported by ELSA (European Lighthouse on Secure and Safe AI) funded by the European Union under grant agreement No. 101070617. YH was supported as a part of NCCR Automation, a National Centre of Competence (or Excellence) in Research, funded by the Swiss National Science Foundation (grant number 51NF40_225155). IB was supported by the EPSRC New Investigator Award EP/X03917X/1; the Engineering and Physical Sciences Research Council EP/S021566/1; and Google Research Scholar award. SSR was supported by Department of Electronic and Electrical Engineering and the Institute of Communications and Connected Systems at UCL. The authors would like to thank William Bankes, Seongho Son, Matthieu Zimmer, Afroditi Papadaki, Eduardo Pignatelli, and Nagham Osman for the useful discussion. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Mohammad Gheshlaghi Azar et al. \u201cA general theoretical paradigm to understand learning from human preferences\u201d. In: International Conference on Artificial Intelligence and Statistics. 2024.   \n[2] Yuntao Bai et al. \u201cConstitutional ai: Harmlessness from ai feedback\u201d. In: arXiv preprint arXiv:2212.08073. 2022.   \n[3] Yuntao Bai et al. \u201cTraining a helpful and harmless assistant with reinforcement learning from human feedback\u201d. In: arXiv preprint arXiv:2204.05862. 2022.   \n[4] William Bankes et al. \u201cREDUCR: Robust Data Downsampling Using Class Priority Reweighting\u201d. In: Neural Information Processing Systems. 2024.   \n[5] Ralph Allan Bradley and Milton E Terry. \u201cRank analysis of incomplete block designs: I. The method of paired comparisons\u201d. In: Biometrika. JSTOR, 1952.   \n[6] Tianchi Cai et al. \u201cULMA: Unified Language Model Alignment with Demonstration and Point-wise Human Preference\u201d. In: arXiv preprint arXiv:2312.02554. 2023.   \n[7] Stephen Casper et al. \u201cOpen problems and fundamental limitations of reinforcement learning from human feedback\u201d. In: Transactions on Machine Learning Research. 2023.   \n[8] Souradip Chakraborty et al. \u201cMaxMin-RLHF: Alignment with Diverse Human Preferences\u201d. In: International Conference on Machine Learning. 2024.   \n[9] Lili Chen et al. \u201cDecision transformer: Reinforcement learning via sequence modeling\u201d. In: Neural Information Processing Systems. 2021.   \n[10] Zixiang Chen et al. \u201cSelf-play fine-tuning converts weak language models to strong language models\u201d. In: International Conference on Machine Learning. 2024.   \n[11] Paul F Christiano et al. \u201cDeep reinforcement learning from human preferences\u201d. In: Neural Information Processing Systems. 2017.   \n[12] Josef Dai et al. \u201cSafe rlhf: Safe reinforcement learning from human feedback\u201d. In: International Conference on Learning Representations. 2024.   \n[13] Nirjhar Das et al. \u201cProvably Sample Efficient RLHF via Active Preference Optimization\u201d. In: arXiv preprint arXiv:2402.10500. 2024.   \n[14] Hanze Dong et al. \u201cRaft: Reward ranked finetuning for generative foundation model alignment\u201d. In: Transactions on Machine Learning Research. 2023.   \n[15] Esin Durmus et al. \u201cTowards measuring the representation of subjective global opinions in language models\u201d. In: Conference on Language Modeling. 2024.   \n[16] Kawin Ethayarajh et al. \u201cKto: Model alignment as prospect theoretic optimization\u201d. In: International Conference on Machine Learning. 2024.   \n[17] Leo Gao, John Schulman, and Jacob Hilton. \u201cScaling laws for reward model overoptimization\u201d. In: International Conference on Machine Learning. 2023.   \n[18] Caglar Gulcehre et al. \u201cReinforced self-training (rest) for language modeling\u201d. In: arXiv preprint arXiv:2308.08998. 2023.   \n[19] Jiwoo Hong, Noah Lee, and James Thorne. \u201cReference-free monolithic preference optimization with odds ratio\u201d. In: arXiv preprint arXiv:2403.07691. 2024.   \n[20] Jian Hu et al. \u201cAligning language models with offline reinforcement learning from human feedback\u201d. In: arXiv preprint arXiv:2308.12050. 2023.   \n[21] Shawn Im and Yixuan Li. \u201cUnderstanding the Learning Dynamics of Alignment with Human Feedback\u201d. In: International Conference on Machine Learning. 2024.   \n[22] Kaixuan Ji, Jiafan He, and Quanquan Gu. \u201cReinforcement Learning from Human Feedback with Active Queries\u201d. In: arXiv preprint arXiv:2402.09401. 2024.   \n[23] Timo Kaufmann et al. \u201cA survey of reinforcement learning from human feedback\u201d. In: arXiv preprint arXiv:2312.14925. 2023.   \n[24] Nathan Lambert, Thomas Krendl Gilbert, and Tom Zick. \u201cEntangled preferences: The history and risks of reinforcement learning and human feedback\u201d. In: arXiv preprint arXiv:2310.13595. 2023.   \n[25] Harrison Lee et al. \u201cRlaif: Scaling reinforcement learning from human feedback with ai feedback\u201d. In: arXiv preprint arXiv:2309.00267. 2023.   \n[26] Ziniu Li, Tian Xu, and Yang Yu. \u201cPolicy Optimization in RLHF: The Impact of Out-ofpreference Data\u201d. In: arXiv preprint arXiv:2312.10584. 2023.   \n[27] Ilya Loshchilov and Frank Hutter. \u201cDecoupled weight decay regularization\u201d. In: International Conference on Learning Representations. 2019.   \n[28] Viraj Mehta et al. \u201cSample Efficient Reinforcement Learning from Human Feedback via Active Exploration\u201d. In: 2023.   \n[29] R\u00e9mi Munos et al. \u201cNash learning from human feedback\u201d. In: International Conference on Machine Learning. 2024.   \n[30] Reiichiro Nakano et al. \u201cWebgpt: Browser-assisted question-answering with human feedback\u201d. In: arXiv preprint arXiv:2112.09332. 2021.   \n[31] Hongseok Namkoong and John C Duchi. \u201cStochastic gradient methods for distributionally robust optimization with f-divergences\u201d. In: Neural Information Processing Systems. 2016.   \n[32] Arkadi Nemirovski et al. \u201cRobust stochastic approximation approach to stochastic programming\u201d. In: Society for Industrial and Applied Mathematics. 2009.   \n[33] Yonatan Oren et al. \u201cDistributionally robust language modeling\u201d. In: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.   \n[34] Long Ouyang et al. \u201cTraining language models to follow instructions with human feedback\u201d. In: Neural Information Processing Systems. 2022.   \n[35] Jing-Cheng Pang et al. \u201cLanguage model self-improvement by reinforcement learning contemplation\u201d. In: International Conference on Learning Representations. 2023.   \n[36] Alec Radford et al. \u201cLanguage models are unsupervised multitask learners\u201d. In: OpenAI blog. Vol. 1. 8. 2019, p. 9.   \n[37] Rafael Rafailov et al. \u201cDirect preference optimization: Your language model is secretly a reward model\u201d. In: Neural Information Processing Systems. 2024.   \n[38] Corby Rosset et al. \u201cDirect nash optimization: Teaching language models to self-improve with general preferences\u201d. In: arXiv preprint arXiv:2404.03715. 2024.   \n[39] Shiori Sagawa et al. \u201cDistributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization\u201d. In: International Conference on Learning Representations. 2019.   \n[40] John Schulman et al. \u201cProximal policy optimization algorithms\u201d. In: arXiv preprint arXiv:1707.06347. 2017.   \n[41] Avi Singh et al. \u201cBeyond human data: Scaling self-training for problem-solving with language models\u201d. In: Transactions on Machine Learning Research. 2023.   \n[42] Maurice Sion. \u201cOn general minimax theorems.\u201d In: 1958.   \n[43] Feifan Song et al. \u201cPreference ranking optimization for human alignment\u201d. In: AAAI Conference on Artificial Intelligence. 2024.   \n[44] Taylor Sorensen et al. \u201cA roadmap to pluralistic alignment\u201d. In: International Conference on Machine Learning. 2024.   \n[45] Nisan Stiennon et al. \u201cLearning to summarize with human feedback\u201d. In: Neural Information Processing Systems. 2020.   \n[46] Gokul Swamy et al. \u201cA minimaximalist approach to reinforcement learning from human feedback\u201d. In: International Conference on Machine Learning. 2024.   \n[47] Yunhao Tang et al. \u201cGeneralized Preference Optimization: A Unified Approach to Offline Alignment\u201d. In: International Conference on Machine Learning. 2024.   \n[48] Gemma Team et al. \u201cGemma: Open models based on gemini research and technology\u201d. In: arXiv preprint arXiv:2403.08295. 2024.   \n[49] Haoxiang Wang et al. \u201cArithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards\u201d. In: Association for Computational Linguistics. 2024.   \n[50] Tianhao Wu et al. \u201cPairwise proximal policy optimization: Harnessing relative feedback for llm alignment\u201d. In: arXiv preprint arXiv:2310.00212. 2023.   \n[51] Yue Wu et al. \u201cSelf-Play Preference Optimization for Language Model Alignment\u201d. In: arXiv preprint arXiv:2405.00675. 2024.   \n[52] Sang Michael Xie et al. \u201cDoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining\u201d. In: Neural Information Processing Systems. 2023.   \n[53] Wei Xiong et al. \u201cIterative preference learning from human feedback: Bridging theory and practice for RLHF under KL-constraint\u201d. In: International Conference on Machine Learning. 2023.   \n[54] Rui Yang et al. \u201cRewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment\u201d. In: International Conference on Machine Learning. 2024.   \n[55] Chenlu Ye et al. \u201cA theoretical analysis of nash learning from human feedback under general kl-regularized preference\u201d. In: arXiv preprint arXiv:2402.07314. 2024.   \n[56] Weizhe Yuan et al. \u201cSelf-rewarding language models\u201d. In: International Conference on Machine Learning. 2024.   \n[57] Siyan Zhao, John Dang, and Aditya Grover. \u201cGroup Preference Optimization: Few-Shot Alignment of Large Language Models\u201d. In: International Conference on Learning Representations. 2024.   \n[58] Yao Zhao et al. \u201cSlic-hf: Sequence likelihood calibration with human feedback\u201d. In: arXiv preprint arXiv:2305.10425. 2023.   \n[59] Han Zhong et al. \u201cDPO Meets PPO: Reinforced Token Optimization for RLHF\u201d. In: arXiv preprint arXiv:2404.18922. 2024.   \n[60] Daniel M Ziegler et al. \u201cFine-tuning language models from human preferences\u201d. In: arXiv preprint arXiv:1909.08593. 2019. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The supplementary section constitutes the following: An extended related work section in Appendix A, proofs for the theoretical insights and the weighted DPO gradient expression in Appendix B, an overview of IPO and the simplification for robust IPO in Appendix C, detailed experimental setup in Appendix D, and convergence proof for Algorithm 1 and other sampling strategies in Appendix E. ", "page_idx": 13}, {"type": "text", "text": "A Additional Related Work ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We detail a more extensive related work exposition in this section. ", "page_idx": 13}, {"type": "text", "text": "The Reinforcement Learning from Human Feedback (RLHF) is a concrete way to adapt a LLM towards specific tasks or objectives using human feedback. This framework for fine-tuning LLMs is inspired from [11] that uses human feedback to improve reinforcement learning policies. For LLMs, it was first established in [60] and further developed in [45, 34]. It involves learning a reward function using the pairwise human feedback data that provides comparisons between responses for a given prompt. This reward learning is performed through the Bradley-Terry model [5]. This learned reward model is then used as an objective for optimizing the LLM policy using Proximal Policy Optimization [40]. ", "page_idx": 13}, {"type": "text", "text": "However, obtaining high-quality human feedback data is expensive and several works such as [35, 2, 25] study the usage of AI feedback to reduce reliance on human input. Further, studies like [13, 28, 22] analyze the usage of active learning strategies to minimize the amount of human feedback data required to perform RLHF. For a comprehensive overview and perspective of the RLHF topic, with detailed discussions on various approaches and gaps, we refer the reader to [24, 7, 23]. ", "page_idx": 13}, {"type": "text", "text": "Due to the inherent issues of tuning the hyperparameters of a PPO ([50, 37]) and susceptible nature of reward models ([17, 49]), alternative approaches to the PPO-based RLHF have been proposed. Foremost of them is the rejection sampling fine-tuning ([14, 18, 49]) which is inspired by the best-of-n inference technique ([30]). The technique involves sampling $n$ responses per prompt and fine-tuning the model based on the highest scoring ones in terms of a learned reward function. Further works such as [20, 54] avoid reward learning and propose conditional Supervised Fine-Tuning (SFT) inspired by the reward-conditioned RL ([9]). ", "page_idx": 13}, {"type": "text", "text": "Another strategy proposed as an alternative to PPO-based RLPHF is DPO ( Rafailov et al. [37]), wherein one optimizes the policy directly based on human preferences, bypassing the need for learning a separate reward model. This method simplifies the training process and potentially reduces overftiting and model misalignment issues. Theoretical advancements in understanding the dynamics of DPO have been provided by [21, 53], who analyzed the convergence and stability of DPO policy learning. Some other works such as [1, 58, 47, 43, 16] also study a similar reward-free RLHF setup. In particular, [1] analyze the potential overfitting issues in DPO. Further, [16, 6] propose strategies that bypass the need for preference datasets. [38, 55, 29] adopt a game-theory perspective to learn Nash equilibrium policies. Typically, the reward-free RLHF approaches utilize a reference policy trained via supervised fine-tuning, which is then further optimized. However, [19] propose Monolithic Preference Optimization without reference model, bypassing supervised fine-tuning entirely. [59] address the RLHF problem from an MDP framework, introducing a DPO-type algorithm integrated with Proximal Policy Optimization (PPO). Additionally, works such as [51, 46, 10, 41, 56] investigate self-play preference optimization, where new data generated in each round by the current policy is used to train an improved policy. Our work utilizes a reward-free framework similar to [37, 1] but differs from all the above works in the following sense. Unlike previous works that assume a single preference distribution, we consider fine-tuning a LLM given data from multiple preference distributions across diverse groups. Further, we aim to fine-tune the LLM in a robust manner such that there is minimal imbalance in performance across all groups. ", "page_idx": 13}, {"type": "text", "text": "Robust language modeling technique in the context of language model was first studied in [33] to optimize performance over a wide-range of topics. It was further improved by [52] wherein they use smaller model to learn the weights/importance that needs to be assigned to different topics and train a larger model based on these weights. Both works aim to minimize the worst group loss based on the group-DRO approach and focus on the pre-training aspects of language models. Concrete theoretical study of this group-DRO approach was performed in [39] which studies the minimax formulation of this group-DRO problem based on [31]. In the context of online batch selection, [4] propose a robust data downsampling method that performs class-aware priority reweighting to reduce training costs while preserving worst-class performance. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "In the RLHF setup, recent research has increasingly focused on enhancing the robustness of policy learning to address certain inherent gaps in the RLHF framework as detailed in [7] such as reward hacking and model misspecification. In particular, [3] formulated a weighted group loss for reward model learning w.r.t. a parameter $\\lambda$ that determines the importance assigned to a group and apply it to the anthropic Harmless vs Helpful data $(^{6})$ . In a similar manner,[12] design a weighted objective for policy optimization w.r.t. parameter $\\lambda$ to trade-off the importance between task reward maximization and safety cost minimzation. Another work [8] consider alignment to different sub-populations by learning multiple reward functions corresponding to each sub-population and learning a robust policy (max-min) w.r.t. the reward functions. Distinct from previous works, our approach circumvents reward model learning and integrates group robustness directly into the reward-free tuning framework. This allows us to directly learn a robust policy without the need for learning reward models. To learn this robust policy, we present a concrete algorithm that adaptively assigns weights to the losses of different groups and optimizes the policy to minimize this weighted loss. Additionally, our algorithm features a novel gradient estimator specifically designed for the group robust DPO problem. ", "page_idx": 14}, {"type": "text", "text": "Other studies addressing robustness in preference optimization, such as [21] and [26], focus on different facets of robustness, including robustness to noise and resilience against out-of-preference data. In the non-robust group preference alignment setup, [57] explores learning group preferences with LLMs by incorporating group information into prompts. However, their approach does not involve fine-tuning a LLM but training a separate transformer module that optimally selects an example sequence of prompts, LLM responses, and group preferences for in-context learning using a LLM. Also, Wang et al. [49] consider alignment with user preferences assuming that each user/group has varied importance over the distinct metrics in their multi-objective reward model. The output policy is trained to output a response based on both the prompt and the importance/weights over the individual metrics. In contrast, the distinctive feature of our method is that we are not modeling multi-reward objectives but consider a reward-free setting. Specifically, we consider the robust alignment problem optimizing for the worst group performance. And, our methodology directly models each group\u2019s preferences through a group-dependent latent reward model where the group dependency is injected through the prompt. ", "page_idx": 14}, {"type": "text", "text": "B Theoretical Insights ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we detail the proofs for the theoretical insights elucidated in Section 3 and the weighted DPO gradient expression discussed in Section 4. ", "page_idx": 14}, {"type": "text", "text": "B.1 Robust Objective for the Log-linear Policy Class ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Here, we describe obtaining the robust objective for the log-linear policy class as in Equation (9). Starting from the robust objective for a general policy class in Equation (8), $\\mathcal{L}_{\\mathrm{GR}}$ can be specialized as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{GR}}(\\pi)=\\underset{\\alpha\\in\\Delta_{K-1}}{\\operatorname*{max}}\\sum_{g=1}^{K}\\alpha_{g}\\Big(-\\mathbb{E}_{(x_{g},y_{w},y_{l})\\sim\\mathcal{D}_{g}}\\Big[\\log\\Big(\\sigma(\\beta h_{\\pi}(x_{g},y_{w},y_{l})\\Big)\\Big]\\Big)}\\\\ &{\\qquad\\overset{(i)}{=}\\underset{\\alpha\\in\\Delta_{K-1}}{\\operatorname*{max}}\\sum_{g=1}^{K}\\alpha_{g}\\Big(-\\mathbb{E}_{(x_{g},y_{w},y_{l})\\sim\\mathcal{D}_{g}}\\Big[\\log\\Big(\\sigma(\\beta\\log\\big(\\frac{\\pi(y_{w}|x_{g})}{\\pi_{\\mathrm{ref}}(y_{w}|x_{g})}\\big)-\\beta\\log\\big(\\frac{\\pi(y_{l}|x_{g})}{\\pi_{\\mathrm{ref}}(y_{l}|x_{g})}\\big)\\Big)\\Big]\\Big)}\\\\ &{\\qquad\\overset{(i i)}{=}\\underset{\\alpha\\in\\Delta_{K-1}}{\\operatorname*{max}}\\sum_{g=1}^{K}\\alpha_{g}\\Big(-\\mathbb{E}_{(x_{g},y_{w},y_{l})\\sim\\mathcal{D}_{g}}\\Big[\\log\\Big(\\sigma(\\beta\\log(\\pi(y_{w}|x_{g}))-\\beta\\log(\\pi(y_{l}|x_{g}))\\Big)\\Big]\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{(i\\dot{\\omega})}{\\alpha\\in\\Delta\\kappa-1}\\displaystyle\\sum_{g=1}^{K}\\alpha_{g}\\Bigl(-\\mathbb{E}_{(x_{g},y_{w},y_{t})\\sim\\mathcal{D}_{g}}\\Bigl[\\log\\Big(\\sigma\\big(\\beta\\log(\\sum_{\\Sigma\\ y\\in\\mathcal{P}}^{\\mathrm{exp}\\,f_{g}(x_{g},y_{w})})-}\\\\ &{\\beta\\log\\bigl(\\sum_{y\\in\\mathcal{P}}^{\\mathrm{exp}\\,f_{g}(x_{g},y_{t})}\\bigr)\\Big)\\Bigr]\\Bigr)}\\\\ &{\\qquad\\beta\\log\\Bigl(\\sum_{y\\in\\mathcal{P}}^{\\mathrm{exp}\\,f_{g}(x_{g},y_{t})}\\Bigr)\\Bigr]}\\\\ &{\\frac{(i v)}{\\alpha\\in\\Delta\\kappa-1}\\displaystyle\\sum_{g=1}^{K}\\alpha_{g}\\Bigl(-\\mathbb{E}_{(x_{g},y_{w},y_{t})\\sim\\mathcal{D}_{g}}\\Bigl[\\log\\Big(\\sigma\\big(\\beta\\log(\\exp\\theta^{T}\\phi(x_{g},y_{w}))-}\\\\ &{\\beta\\log(\\exp\\theta^{T}\\phi(x_{g},y_{t}))\\Big)\\Bigr)\\Bigr]\\Bigr)}\\\\ &{\\qquad\\qquad\\times}\\\\ &{=\\displaystyle\\operatorname*{max}_{\\alpha\\in\\Delta\\kappa-1}\\sum_{g=1}^{K}\\alpha_{g}\\Bigl(-\\mathbb{E}_{(x_{g},y_{w},y_{t})\\sim\\mathcal{D}_{g}}\\Bigl[\\log\\Big(\\sigma\\big(\\beta\\langle\\phi(x_{g},y_{w})-\\phi(x_{g},y_{t}),\\theta\\rangle\\big)\\Big)\\Bigr]\\Bigr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "fHreorme,  t(hi)e  ffoallcot wths aftr ofomr  tah eu ndieffoinrimti orne foefr $\\begin{array}{r}{h_{\\pi}(x_{g},y_{w},y_{l})=\\log(\\frac{\\pi(y_{w}|x_{g})}{\\pi_{\\mathrm{ref}}(y_{w}|x_{g})})\\!-\\!\\log(\\frac{\\pi(y_{l}|x_{g})}{\\pi_{\\mathrm{ref}}(y_{l}|x_{g})})}\\end{array}$ , s( ifir) ofoml lsouwbs$\\pi_{\\mathrm{ref}}(y_{w}|x_{g})=\\pi_{\\mathrm{ref}}(y_{l}|x_{g})$ stituting $\\begin{array}{r}{\\pi_{\\theta}(y|x_{g})=\\frac{\\exp f_{\\theta}(x_{g},y)}{\\sum_{y\\in\\mathcal{Y}}\\exp f_{\\theta}(x_{g},y)}}\\end{array}$ , and (iv) follows from substituting $f_{\\theta}(x_{g},y)=\\theta^{T}\\phi(x_{g},y)$ . ", "page_idx": 15}, {"type": "text", "text": "B.2 Robust KL-Regularized Policy Maximization ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we consider the robust version of the classical KL-regularized reward maximization objective (Equation (3)) detailed in Equation (10) for a reward function $r(x_{g},y)$ , reference policy $\\pi_{\\mathrm{ref}}$ , and a general class of policies $\\Pi$ . ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi\\in\\Pi}\\operatorname*{min}_{g\\in\\mathcal{G}}\\mathbb{E}_{x_{g}\\sim\\mathcal{P}_{x_{g}},y\\sim\\pi(\\cdot|x_{g})}\\big[r(x_{g},y)\\big]-\\beta\\mathrm{KL}[\\pi(y\\mid x_{g})||\\pi_{\\mathrm{ref}}(y\\mid x_{g})].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Lemma B.1. The optimal policy $\\pi^{*}$ for the robust $K L$ -regularized reward maximization objective (Equation (12)) for a reward function $r(x_{g},y)$ , reference policy $\\pi_{r e f}$ is ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\pi^{*}(y|x_{g})=\\frac{1}{Z(x_{g})}\\pi_{r e f}(y|x_{g})\\exp\\Big(\\frac{r(x_{g},y)}{\\beta}\\Big),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\begin{array}{r}{Z(x_{g})=\\sum_{y}\\pi_{r e f}(y|x_{g})\\exp(\\frac{1}{\\beta}r(x_{g},y))}\\end{array}$ is a partition function. ", "page_idx": 15}, {"type": "text", "text": "Proof. We recast Equation (12) based on Equation (8) and perform the following analysis similar to [37][Appendix A.1]: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{\\operatorname*{max}_{\\pi\\in\\Delta_{K-1}}}{\\pi\\in\\Pi\\propto\\epsilon\\Delta_{K-1}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Further, by defining $\\begin{array}{r}{\\pi^{*}(y|x_{g})=\\frac{1}{Z(x_{g})}\\pi_{\\mathrm{ref}}(y|x_{g})\\exp(\\frac{r(x_{g},y)}{\\beta})}\\end{array}$ , we can rewrite Equation (17) as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta\\underset{\\pi\\in\\Pi}{\\operatorname*{min}}\\underset{\\alpha\\in\\Delta_{K-1}}{\\operatorname*{max}}\\sum_{g=1}^{K}\\alpha_{g}\\Big(\\underset{x_{g}\\sim\\mathcal{P}_{x_{g}}}{\\mathbb{E}}\\underset{y\\sim\\pi(\\cdot\\vert x_{g})}{\\mathbb{E}}\\left[\\log\\big(\\frac{\\pi(y\\vert x_{g})}{\\pi^{*}(y\\vert x_{g})}\\big)-\\log(Z(x_{g}))\\big]\\right)}\\\\ &{=\\!\\beta\\underset{\\pi\\in\\Pi}{\\operatorname*{min}}\\underset{\\alpha\\in\\Delta_{K-1}}{\\operatorname*{max}}\\!\\sum_{g=1}^{K}\\alpha_{g}\\Big(\\underset{x_{g}\\sim\\mathcal{P}_{x_{g}}}{\\mathbb{E}}\\left[D_{K L}\\big(\\pi(\\cdot\\vert x_{g})\\vert\\vert\\pi^{*}(\\cdot\\vert x_{g})\\big)-\\log(Z(x_{g}))\\right]\\Big)}\\\\ &{=\\!\\beta\\underset{\\pi\\in\\Pi}{\\operatorname*{min}}\\,r o L(\\pi),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we use ", "page_idx": 16}, {"type": "equation", "text": "$$\nr o L(\\pi):=\\operatorname*{max}_{\\alpha\\in\\Delta_{K-1}}\\sum_{g=1}^{K}\\alpha_{g}\\Big(\\underset{x_{g}\\sim\\mathcal{P}_{x_{g}}}{\\mathbb{E}}\\Big[D_{K L}\\big(\\pi(\\cdot|x_{g})||\\pi^{*}(\\cdot|x_{g})\\big)-\\log(Z(x_{g}))\\Big]\\Big).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "It is not hard to show that $r o L(\\pi)$ is minimized by $\\pi^{*}$ . Indeed, let us consider any other policy $\\pi\\neq\\pi^{*}$ . Then, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{r o L(\\pi)=\\beta\\underset{\\alpha\\in\\Delta_{K-1}}{\\operatorname*{max}}\\underset{g=1}{\\overset{K}{\\sum}}\\alpha_{g}\\Bigg(\\underset{x_{g}\\sim\\mathcal{P}_{x_{g}}}{\\mathbb{E}}\\left[D_{K L}\\big(\\pi(\\cdot|x_{g})||\\pi^{*}(\\cdot|x_{g})\\big)-\\log(Z(x_{g}))\\right]\\Bigg)}\\\\ &{\\qquad\\qquad\\geq\\beta\\underset{\\alpha\\in\\Delta_{K-1}}{\\operatorname*{max}}\\underset{g=1}{\\overset{K}{\\sum}}\\alpha_{g}\\Bigg(\\underset{x_{g}\\sim\\mathcal{P}_{x_{g}}}{\\mathbb{E}}\\left[-\\log(Z(x_{g}))\\right]\\Bigg)}\\\\ &{\\qquad=\\beta\\underset{\\alpha\\in\\Delta_{K-1}}{\\operatorname*{max}}\\underset{g=1}{\\overset{K}{\\sum}}\\alpha_{g}\\Bigg(\\underset{x_{g}\\sim\\mathcal{P}_{x_{g}}}{\\mathbb{E}}\\left[D_{K L}\\big(\\pi^{*}(\\cdot|x_{g})||\\pi^{*}(\\cdot|x_{g})\\big)-\\log(Z(x_{g}))\\right]\\Bigg)}\\\\ &{\\qquad=r o L(\\pi^{*}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the inequality follows since $D_{K L}\\big(\\pi(\\cdot|x)||\\pi^{*}(\\cdot|x)\\big)\\;\\geq\\;0$ . This implies $\\pi^{*}$ minimises Equation (20). \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Note that, we have proved that the optimal policy expression for Equation (12) aligns with the form of the optimal policy for standard (non-robust) KL-regularized reward maximization Equation (4). ", "page_idx": 16}, {"type": "text", "text": "Proposition 3.2. Substituting the closed-form solution of the robust $K L$ -regularized policy maximization problem (Equation (10)) into the robust reward maximization objective in Equation (6) leads to the same group robust DPO loss LGR\u2212DPO in Equation (8) . ", "page_idx": 16}, {"type": "text", "text": "Proof. In [37], the DPO loss (see Equation (5)) is obtained by using the following reward loss: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{L}_{R}(r;\\mathcal{D})=\\mathcal{L}_{R}(r;\\{\\mathcal{D}_{g}\\}_{g=1}^{K})=-\\sum_{g=1}^{K}(x_{g},y_{w},y_{l})\\sim\\mathcal{D}_{g}\\left[\\log\\left(\\sigma\\left(r(x_{g},y_{w})-r(x_{g},y_{l})\\right)\\right)\\right],\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and replacing $r(\\cdot,\\cdot)$ with the relation ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pi^{*}(y|x)=\\frac{1}{Z(x)}\\pi_{\\mathrm{ref}}(y|x)\\exp{\\left(\\frac{r(x,y)}{\\beta}\\right)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which is the solution to ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\pi^{*}=\\underset{\\pi\\in\\Pi}{\\operatorname*{max}}\\sum_{g=1}^{K}\\underset{(x_{g}\\sim\\mathcal{P}_{x_{g}},y\\sim\\pi(\\cdot\\vert x_{g}))}{\\mathbb{E}}\\Big[r_{\\phi}(x_{g},y)\\Big]-\\beta D_{K L}\\Big[\\pi(y\\vert x_{g})\\vert\\vert\\pi_{\\mathrm{ref}}(y\\vert x_{g})\\Big].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "However, in our group-robust formulation, we replace Equation (25) with ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{g\\in\\mathcal{G}}\\mathcal{L}_{R}(r;\\{\\mathcal{D}_{g}\\}_{g=1}^{K})=\\operatorname*{max}_{\\alpha\\in\\Delta_{K-1}}\\sum_{g=1}^{K}\\alpha_{g}\\Bigg(-\\underset{(x_{g},y_{w},y_{l})\\sim\\mathcal{D}_{g}}{\\mathbb{E}}\\left[\\log\\Big(\\sigma\\big(r(x_{g},y_{w})-r(x_{g},y_{l})\\big)\\Big)\\right]\\Bigg),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "while Equation (27) remains the same. Alternatively, we can replace Equation (27) with its robust version, i.e., ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\pi^{*}=\\operatorname*{max}_{\\pi\\in\\Pi}\\operatorname*{min}_{\\alpha\\in\\Delta_{K-1}}\\sum_{g=1}^{K}\\alpha_{g}\\left(\\underset{(x_{g}\\sim\\mathcal{P}_{x_{g}},y\\sim\\pi(\\cdot|x_{g}))}{\\mathbb{E}}\\left[r_{\\phi}(x_{g},y)\\right]-\\beta D_{K L}\\Big[\\pi(y|x_{g})\\vert\\vert\\pi_{\\mathrm{ref}}(y|x_{g})\\Big]\\right),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "however, we have already shown, in Lemma B.1, that this does not change the obtained policy-reward relation from Equation (26). Hence, this proves Proposition 3.2. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "B.3 Analysis of the Gradient ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We elucidate the steps to obtain the expression for the loss gradient in Equation (11). We can simplify the gradient as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\omega_{p}^{\\prime}\\nabla_{\\theta}\\big(\\vert\\nabla_{\\theta^{n-1}}(x_{p},y_{w},y_{t})\\big)}{n_{\\theta}}}\\\\ &{=\\frac{\\omega_{p}^{\\prime}\\nabla_{\\theta}\\log\\big(\\sigma(\\beta\\|x_{q-1}(x_{p},y_{w},y_{t}))\\big)}{n_{\\theta}}}\\\\ &{\\frac{(\\omega)}{n_{\\theta}}\\frac{\\sigma^{\\prime}\\big(\\beta\\|x_{q-1}(x_{p},y_{w},y_{t})\\big)}{\\sigma(\\beta\\|x_{q-1}(x_{p},y_{w},y_{t}))}\\times[\\beta\\hat{h}_{q_{q-1}}^{\\prime}(x_{p},y_{w},y_{t})]}\\\\ &{\\overset{(i i)}{=}\\frac{\\beta\\partial_{q}^{\\prime}}{n_{\\theta}}\\frac{\\big(1-\\sigma(\\beta\\|x_{q-1}(x_{p},y_{w},y_{t}))\\big)\\times[\\beta\\hat{h}_{q_{q-1}}^{\\prime}(x_{p},y_{w},y_{t})]}{\\sigma(\\beta\\|x_{q-1}(x_{p},y_{w},y_{t}))}}\\\\ &{\\overset{(i i i)}{=}\\frac{\\beta\\partial_{q}^{\\prime}}{n_{\\theta}}\\sigma\\big(-\\beta\\hbar_{w_{q-1}}(x_{p},y_{w},y_{t})\\big)\\times[\\hat{h}_{q_{\\theta+1}}^{\\prime}(x_{p},y_{w},y_{w},y_{t})]}\\\\ &{\\overset{(i i i i)}{=}\\frac{\\beta\\partial_{q}^{\\prime}}{n_{\\theta}}\\sigma\\big(\\beta\\big(\\log(\\frac{\\sigma(\\gamma\\hat{x}_{q}-1(\\theta)\\|x_{p},y_{w},y_{t})}{n_{\\theta}}\\big)-\\log(\\frac{\\sigma(\\gamma\\hat{x}_{q}-1(\\theta)\\|x_{q})}{n_{\\theta+1}(x_{p},y_{w},y_{w},y_{t})}\\big)\\big)\\times[\\hat{h}_{q_{\\theta-1}}^{\\prime}(x_{p},y_{w},y_{w})]}\\\\ &{\\overset{(i i i i)}{=}\\frac{\\beta\\partial_{q}^{\\prime}}{n_{\\theta}}\\sigma\\big(\\beta\\big(\\log(\\frac{\\sigma(\\gamma \n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Here, (i) follows from the derivative of $\\log(\\cdot)$ function and denoting the derivative of sigmoid function $\\sigma(\\cdot)$ as $\\sigma^{'}(\\cdot)$ . (ii) and (iii) follow from the fact that for any $s\\in\\mathbb{R}$ , $\\sigma^{'}(s)\\,=\\,1\\,-\\,\\sigma(s)\\,=\\,\\sigma(-s)$ . Finally, (iv) and (v) follow using the definition of $\\begin{array}{r}{h_{\\pi}(x,y_{w},y_{l})=\\log(\\frac{\\pi(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)})-\\log(\\frac{\\pi(y_{l}|x)}{\\pi_{\\mathrm{ref}}(y_{l}|x)})}\\end{array}$ and substituting $\\begin{array}{r}{r_{\\theta}(x_{g},y)=\\beta\\log(\\frac{\\pi_{\\theta}(y|x_{g})}{\\pi_{\\mathrm{ref}}(y|x_{g})})-\\beta\\log Z(x_{g})}\\end{array}$ from Equation (4). ", "page_idx": 17}, {"type": "text", "text": "B.4 Trading off worst-case vs. average performance ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In the robust objective of Equation (8), we intend to minimize the worst-case loss. This often might adversely impact the average loss leading to bad average performance across the different groups. To mitigate this, we propose the following robust trade-off direct preference optimization objective for a specified policy $\\pi$ and set of group weights $\\mu_{1},\\dots,\\mu_{K}$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{GR},\\chi}(\\pi):=\\!(1-\\chi)\\displaystyle\\sum_{g=1}^{K}\\mu_{g}\\mathcal{L}_{\\mathrm{DPO}}(\\pi,\\mathcal{D}_{g})+\\chi\\operatorname*{max}_{g\\in\\mathcal{G}}\\mathcal{L}_{\\mathrm{DPO}}(\\pi,\\mathcal{D}_{g})}\\\\ &{\\qquad\\qquad=\\!(1-\\chi)\\displaystyle\\sum_{g=1}^{K}\\mu_{g}\\Big(-\\mathbb{E}_{(x_{g},y_{w},y_{l})\\sim\\mathcal{D}_{g}}\\Big[\\log\\Big(\\sigma\\big(\\beta h_{\\pi}(x_{g},y_{w},y_{l})\\big)\\Big)\\Big]\\Big)+}\\\\ &{\\qquad\\qquad\\underset{g\\in\\mathcal{G}}{\\operatorname*{max}}\\Big(-\\mathbb{E}_{(x_{g},y_{w},y_{l})\\sim\\mathcal{D}_{g}}\\Big[\\log\\Big(\\sigma\\big(\\beta h_{\\pi}(x_{g},y_{w},y_{l})\\big)\\Big)\\Big]\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Here, note that the input weights $\\mu_{g}$ can be equal for all groups leading to a trade-off between the average and worst-case performance w.r.t. parameter $\\chi$ . Following Equation (8), this can be ", "page_idx": 17}, {"type": "text", "text": "equivalently recast into: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{GR},x}(\\pi)=\\!(1-\\chi)\\displaystyle\\sum_{g=1}^{K}\\mu_{g}\\Big(-\\mathbb{E}_{(x_{g},y_{w},y_{l})\\sim\\mathcal{D}_{g}}\\Big[\\log\\Big(\\sigma\\big(\\beta h_{\\pi}(x_{g},y_{w},y_{l})\\Big)\\Big]\\Big)}\\\\ &{\\qquad\\qquad\\quad+\\underbrace{\\!\\operatorname*{max}_{\\alpha\\in\\Delta_{K-1}}\\sum_{g=1}^{K}\\alpha_{g}\\Big(-\\mathbb{E}_{(x_{g},y_{w},y_{l})\\sim\\mathcal{D}_{g}}\\Big[\\log\\Big(\\sigma\\big(\\beta h_{\\pi}(x_{g},y_{w},y_{l})\\Big)\\Big]\\Big),}\\\\ &{\\qquad\\qquad\\quad=\\displaystyle\\operatorname*{max}_{\\alpha\\in\\Delta_{K-1}}\\sum_{g=1}^{K}\\!\\big((1-\\chi)\\mu_{g}+\\chi\\alpha_{g}\\big)\\Big(-\\mathbb{E}_{(x_{g},y_{w},y_{l})\\sim\\mathcal{D}_{g}}\\Big[\\log\\Big(\\sigma\\big(\\beta h_{\\pi}(x_{g},y_{w},y_{l})\\Big)\\Big]\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\Delta_{K-1}$ represents the $(K-1)$ -dimensional simplex of probabilities. Hence, minimizing the loss in Equation (31) implies that one can implicitly find an optimal reward function using human group preference data that is robust without losing average performance, and obtain a policy that works effectively in terms of both average and worst-case performance: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pi}\\;{\\mathcal{L}}_{\\mathrm{GR},\\chi}(\\pi).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "As Equation (8) involves maximizing with respect to $\\alpha$ and minimizing with respect to $\\pi$ , it forms a two-player game similar to the one considered in Section 3.1, where the policy $\\pi$ and $\\alpha$ act as opponents with inversely related payoffs. However, the contributions of the maximizing player $(\\alpha)$ on the final outcome are limited depending on parameters $\\chi$ and $\\mu$ . ", "page_idx": 18}, {"type": "text", "text": "C IPO Simplification ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Within the context of a finite data setting, it is common to record two responses $(y,y^{\\prime})$ for each prompt $x$ . Based on these observations, there might be a tendency to inaccurately deduce that the preference distribution fulfills $p(y\\succ y^{\\prime}|x)=1$ . Consequently, and assuming the Bradley-Terry model of preferences, the reward function $r$ has to satisfy $r(y,x)\\stackrel{.}{-}r(y^{\\prime},x)\\rightarrow\\infty$ . Further, given that DPO policy $\\pi$ is directly dependent on the reward function (Equation (4)), it holds that ${\\frac{\\pi(y^{\\prime}|x)}{\\pi(y|x)}}=0$ In such a case, the policy overfits irrespective of the KL regularization factor. To circumvent this, [1] consider reward-free RLHF problem. Specifically, they propose preference optimization in the policy objective instead of reward optimization as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi}\\ \\mathbb{E}_{x\\sim\\rho,y\\sim\\pi(\\cdot|x),y^{\\prime}\\sim\\mu(\\cdot|x)}\\big[\\Psi(p(y\\sim y^{\\prime})|x)\\big]-\\beta\\mathrm{KL}\\big[\\pi(y|x)||\\pi_{\\mathrm{ref}}(y|x)\\big].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Here, $\\Psi:[0,1]\\rightarrow\\mathbb{R}$ is a non-decreasing function, $\\rho$ refers to the distribution of prompts and, $\\mu$ refers to the competing behavior policy. Choosing $\\Psi$ as the identity function, the optimal policy $\\pi^{*}$ of Equation (34) has a similar expression to Equation (4) with the reward function $r(\\cdot)$ substituted by the following preference function $p(\\cdot)$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\np(y\\succ\\mu|x)=\\beta\\log\\frac{\\pi^{*}(y\\mid x)}{\\pi_{\\mathrm{ref}}(y\\mid x)}+\\beta\\log Z(x).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "fRreocma llEiqnug atthioe nd (e3f5in)i ttihoant $\\begin{array}{r}{h_{\\pi}(x,y,y^{\\prime})=\\log(\\frac{\\pi(y|x)}{\\pi_{\\mathrm{ref}}(y|x)})-\\log(\\frac{\\pi(y^{\\prime}|x)}{\\pi_{\\mathrm{ref}}(y^{\\prime}|x)})}\\end{array}$ efsr tohme  tSreucet iporne f2e,r ietn fcoel lwoiwths $\\pi^{*}$   \nthe policy\u2019s preference of $y$ over $y^{\\prime}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\nh_{\\pi^{*}}(x,y,y^{\\prime})=\\beta^{-1}(p(y\\succ\\mu|x)-p(y^{\\prime}\\succ\\mu|x)).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $p(y\\,\\succ\\,\\mu|x)\\,:=\\,\\mathbb{E}_{\\bar{y}\\sim\\mu(\\cdot|x)}\\bigl[p(y\\,\\succ\\,\\bar{y}|x)\\bigr]$ . The equation motivates us to find a policy $\\pi$ that minimizes the squared differences of two sides in Equation (36), ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\pi)=\\mathbb{E}_{y,y^{\\prime}\\sim\\mu}\\Big[h_{\\pi}(x,y,y^{\\prime})-\\beta^{-1}(p(y\\succ\\mu|x)-p(y^{\\prime}\\succ\\mu|x))\\Big]^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For an empirical dataset where one observes if $y_{w}$ is more preferable to $y_{l}$ , the policy minimization objective in Equation (37) becomes the IPO objective: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{IPO}}(\\pi,\\mathcal{D})=\\underset{(x,y_{w},y_{l})\\sim\\mathcal{D}}{\\mathbb{E}}\\Big[h_{\\pi}(x,y_{w},y_{l})-\\frac{1}{2\\beta}\\Big]^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We can simplify Equation (38) for the IPO Loss using Log-linear policy class similar to Equation (9) as follows ", "page_idx": 19}, {"type": "equation", "text": "$$\nL_{\\mathrm{IPO}}(\\pi,\\mathcal D)=\\underset{(x,y_{w},y_{l})\\sim\\mathcal D}{\\mathbb{E}}\\left[\\big(\\langle\\phi(x,y_{w})-\\phi(x,y_{l}),\\theta\\rangle-\\frac{1}{2\\beta}\\big)^{2}\\right]\\!.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Contrary to DPO, the IPO Loss results in a problem formulation equivalent to linear regression for the linear bandit setting. This leads us to obtain a closed form analytical solution for $\\theta$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\hat{\\theta}_{I P O}=\\frac{1}{2\\beta}(X^{T}X)^{-1}X^{T}{\\bf1},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where ${\\mathbf{1}}=\\{1\\}^{N}$ . Here, $X\\in\\mathbb{R}^{d\\times N}$ . In particular, each row of $X$ is the difference of the preferred and least preferred feature vectors $\\phi(x,y_{w})-\\phi(x,y_{l})$ for $(x,y_{w},y_{l})\\in\\mathcal{X}\\times\\mathcal{Y}\\times\\mathcal{Y}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\nX=\\left[\\begin{array}{c}{\\phi(x_{1},y_{1})-\\phi(s_{1},y_{1}^{\\prime})}\\\\ {\\phi(x_{2},y_{2})-\\phi(x_{2},y_{2}^{\\prime})}\\\\ {\\vdots}\\\\ {\\phi(x_{n},y_{n})-\\phi(x_{n},y_{n}^{\\prime})}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "When re-written in this form it is trivial to see that the stability of the IPO Loss depends upon the rank of the matrix $X$ . ", "page_idx": 19}, {"type": "text", "text": "Group IPO: We begin by conducting experiments with IPO considering the existence of closed form solution for this particular setting. In particular, given a set of preference data $\\boldsymbol{D}=\\{\\mathcal{D}_{1},\\mathcal{D}_{2}\\}$ from either groups with varying ratio, one aims to find an optimal $\\theta$ that minimizes the group IPO loss, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{\\calL}_{\\mathrm{IPO}}(\\pi,\\mathcal{D})=\\underset{(x,y_{w},y_{l})\\sim\\mathcal{D}_{1}}{\\mathbb{E}}\\left[\\big(\\langle\\phi(x,y_{w},0)-\\phi(x,y_{l},0),\\theta\\rangle-\\frac{1}{2\\beta}\\big)^{2}\\right]}&{}\\\\ {+\\underset{(x,y_{w},y_{l})\\sim\\mathcal{D}_{2}}{\\mathbb{E}}\\left[\\big(\\langle\\phi(x,y_{w},1)-\\phi(x,y_{l},1),\\theta\\rangle-\\frac{1}{2\\beta}\\big)^{2}\\right].}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Concisely, one can also write this as, ", "page_idx": 19}, {"type": "equation", "text": "$$\nL_{\\mathrm{IPO}}(\\pi,\\mathcal D)=\\underset{(x,y_{w},y_{l},g)\\sim\\mathcal D}{\\mathbb{E}}\\left[\\left(\\langle\\phi(x,y_{w},g)-\\phi(x,y_{l},g),\\theta\\rangle-\\frac{1}{2\\beta}\\right)^{2}\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We assume that the feature vectors for each group is known, but the reward parameter $\\theta$ is unknown. So, given the preference data with group information and the group dependent feature vectors, one aims to learn an optimal $\\theta$ that balances both groups by minimizing the above loss. The solution to this will still be the closed form solution expressed in Equation (40) with feature matrix $X$ consisting group dependent features. ", "page_idx": 19}, {"type": "text", "text": "Group Robust IPO: It is straightforward to see that in such a setting, the group with higher number of preference data will have an unfair advantage as they contribute more to the loss. Hence, a robust approach needs to be considered as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\nr o L_{\\mathrm{IPO}}(\\pi,\\mathcal{D})=\\operatorname*{max}_{\\sum_{g}\\alpha_{g}=1}\\underset{(x,y_{w},y_{l},g)\\sim\\mathcal{D}}{\\mathbb{E}}\\left[\\alpha_{g}\\big(\\langle\\phi(x,y_{w},g)-\\phi(x,y_{l},g),\\theta\\rangle-\\frac{1}{2\\beta}\\big)^{2}\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "One can use, Algorithm 1 to find the optimal $\\theta$ for Equation (44). But considering the weighted regression form of the loss, one can use the following simplified algorithm Algorithm 2. Note that the last step, inadvertently leads to a weighted regression whose solution can be obtained in closed form. ", "page_idx": 19}, {"type": "text", "text": "D Additional Experiments and Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we discuss additional experimental and training details. We use the following hyperparameters for the synthetic experiments. The importance sampling methods use the same hyperparameters as the corresponding vanilla ones. Further, we note that there is no learning rates for IPO and GR-IPO as we use the closed-form solution detailed in Section 4.1 for updates. ", "page_idx": 19}, {"type": "text", "text": "1: Initialize: Step size $\\eta_{\\alpha}$ for group weights $\\alpha$ , initial weights $\\theta^{(0)}$ of the policy and weights over   \neach group $\\alpha^{(\\bar{0})}$   \n2: Input: Dataset $\\mathcal{D}$ with size $N=|\\mathcal{D}|$ , group size $N_{g}$ for $g=\\{1,2,\\cdots\\,,K\\}$   \n3: for $t=1,\\dots,T$ do   \n4: Calculate group loss $l_{g}$ for each group $g$ on $\\mathcal{D}_{g}$   \n5: $\\alpha^{\\prime}\\gets\\alpha^{t-1}$ ; $\\alpha_{g}^{\\prime}\\gets\\alpha_{g}^{\\prime}\\exp\\left(\\eta_{G}l_{g}\\right)$ // Update weights for group g   \n6: $\\alpha^{(t)}\\gets\\alpha^{\\prime}/\\sum_{g^{\\prime}}\\alpha_{g^{\\prime}}^{\\prime}$ // Renormalize $\\alpha$   \n7: $\\begin{array}{r}{\\theta^{t}\\leftarrow\\arg\\operatorname*{min}_{\\theta}\\stackrel{=}\\mathbb{E}_{(x,y_{w},y_{l},g)\\sim\\mathcal{D}}^{-}\\left[\\alpha_{g}^{(t)}\\big(\\langle\\phi(x,y_{w},g)-\\phi(x,y_{l},g),\\theta\\rangle-\\frac{1}{2\\beta}\\big)^{2}\\right]}\\end{array}$   \n8: end for ", "page_idx": 20}, {"type": "text", "text": "9: Return: Output the robust policy $\\pi(\\theta^{t})$ ", "page_idx": 20}, {"type": "table", "img_path": "PRAsjrmXXK/tmp/98ba7ba35cecbf1f338d176d629cfbf19e741a47cae17092dc08c3ff55512d9e.jpg", "table_caption": [], "table_footnote": ["Table 1: Hyperparameters for synthetic experiments. "], "page_idx": 20}, {"type": "text", "text": "D.1 Synthetic Experiments - Experiments setup ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Here we provide a full definition of our synthetic experimental setup discussed in Section 5. ", "page_idx": 20}, {"type": "text", "text": "We adapt the standard (non-group based) experimental setup proposed by [26] for the group preferences setting by incorporating group information into the reward function $r:\\mathcal{X}\\times\\mathcal{Y}\\times\\mathcal{G}\\to\\mathbb{R}$ . Here, $\\mathcal{X}$ represents a two-dimensional state space $[0,1]\\times[0,1]$ , $\\boldsymbol{\\wp}$ denotes a discrete action space $\\{0,1,2,3,{\\overline{{\\ldots}}},n\\}$ , and $\\mathcal{G}$ signifies a discrete group space $\\{0,\\bar{1},2,\\ldots,K\\}$ . The reward function, defined by the group-dependent feature vector $\\phi(x,y,g)$ and parameter vector $\\theta_{g}$ , is given as: ", "page_idx": 20}, {"type": "equation", "text": "$$\nr(x,y,g):=\\langle\\phi(x,y,g),\\theta_{g}\\rangle.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Further, we consider $n=7$ , $K=2$ and denote $x:=(x_{0},x_{1})\\in\\mathcal{X}$ . The feature vector $\\phi(x,y,g):=$ $(\\phi_{0}(x,y,g),\\phi_{1}(x,y,g),\\phi_{2}(x,y,g),\\phi_{3}(x,y,g))$ and parameters $\\theta_{g}\\ \\forall g\\in\\mathcal{G}$ are defined as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathfrak{z}_{i}(x,y,g):=\\left\\{\\left(\\frac{y}{n+1}+1\\right)\\cdot\\cos(x_{\\lfloor i/2\\rfloor}\\cdot\\pi)\\quad\\mathrm{if~}i\\mathcal{Y}_{0}2=g}\\\\ {\\left(\\frac{1}{\\frac{y}{n+1}+1}\\right)\\cdot\\sin(x_{\\lfloor i/2\\rfloor}\\cdot\\pi)\\quad\\mathrm{~otherwise~}}\\end{array}\\right.,\\quad\\theta_{0}\\quad\\mathfrak{z}=(1,3,1,3),\\,\\theta_{1}:=(3,1,3,1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By design, this parameterization ensures a coordinate-filpped relationship between groups, effectively mirroring one group to the other. Also, we study two other feature parameterizations and include their experimental results in Appendix D.3. ", "page_idx": 20}, {"type": "text", "text": "D.2 Synthetic Experiments - Preferences data generation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In our synthetic experiments, we considered three scenarios: (i) Groups are imbalanced in size, (ii) preferences distribution, and (iii) both of the above. The size imbalance is generated by using ratios $0.2:0.8$ , respectively. Instead, to generate imbalance in terms of preferences we proceed as follows. In scenario (i), after randomly selecting a state $x$ within group $g$ and an action $y_{1}$ , a second action $y_{2}$ is randomly chosen (groups have the same distribution over responses). Instead, in scenarios (ii) and (iii), for one group $y_{2}$ is the action most distant from $y_{1}$ , while for the other, it is the closest (groups have different distributions over responses). In both methods, we calculate the rewards $r(x,y_{1},g)$ and $r(x,y_{2},g)$ , designating the action with the higher reward as preferred. ", "page_idx": 20}, {"type": "text", "text": "Here, we provide the resulting plots for scenarios (i) and (ii). ", "page_idx": 20}, {"type": "text", "text": "Trading off worst-case vs. Average performance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We perform ablation studies for various values of $\\chi$ trading-off between the average and worst-case performance. We observe in Figure 6, that the max validation loss decreases while moving from ", "page_idx": 20}, {"type": "image", "img_path": "PRAsjrmXXK/tmp/9d443c61613cee29a02ac953b0643c460d0ad809e0b1ac1627a77c53a3dd6461.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 4: Algorithm 1 (GR-DPO and GR-IPO) leads to a lower worst-case validation loss and reward error compared to importance sampling and vanilla methods. Results refer to the scenario in which groups have different sizes but same responses\u2019 distribution. Note that the gap between Algorithm 1 and importance sampling is smaller than in Figure 4. This is expected considering that the primary difference between groups arises from data imbalance, which is handled by importance sampling. ", "page_idx": 21}, {"type": "image", "img_path": "PRAsjrmXXK/tmp/5f7c2576e3579e1223a4d21c19288a7a5d6e23289cc83ff1a672a276cd4b1d59.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 5: Algorithm 1 (GR-DPO and GR-IPO) leads to a lower worst-case validation loss and reward error compared to the non-robust vanilla methods. Results refer to the scenario in which groups have same sizes but different responses\u2019 distribution. Unlike the setups of Figure 4 and Figure 2 importance sampling has no effect here (it coincides with vanilla DPO/IPO since groups have the same sizes). ", "page_idx": 21}, {"type": "image", "img_path": "PRAsjrmXXK/tmp/4341561111ec0f506a73aba253b58de8f9e047fb53f0d6f39a23975d493c3d96.jpg", "img_caption": ["Figure 6: Ablation study for the trade-off parameter $\\chi$ in the synthetic experimental setup. Results refer to the scenario in which groups have different sizes and different responses\u2019 distribution. Note that increasing $\\chi$ improves worst group performance at the expense of average performance. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "$\\chi=0$ to $\\chi=1$ , where $\\chi=0$ corresponds to importance sampling with group weights $\\mu_{1},\\cdot\\cdot\\cdot\\,,\\mu_{g}$ mapping to importance sampling weights (see Equation (30)) and $\\chi=1$ corresponds to GR-IPO. Further, we plot the average validation loss which increases while moving from $\\chi=0$ to $\\chi=1$ , demonstrating the trade-off between average and worst-case performance. Note that, GR-IPO aptly increases the average loss (as expected) in order to reduce the worst group loss. ", "page_idx": 21}, {"type": "text", "text": "D.3 Synthetic Experiments - Additional feature parametrizations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We present further experiments on synthetic preference data, using different configurations of the $\\bar{\\phi}(x,y,g)$ and $\\theta_{g}$ vectors characterising the group-specific reward distributions $\\bar{r}(x,y,g)\\,=$ $\\langle\\phi(x,y,g),\\theta_{g}\\rangle$ . ", "page_idx": 21}, {"type": "text", "text": "With the same action space $n=7$ and group space $K=2$ , we consider same and filpped configurations of $\\phi(x,y,g)=(\\bar{\\phi}_{0},\\phi_{1},\\phi_{2},\\phi_{3})$ , as follows: ", "page_idx": 21}, {"type": "text", "text": "Same: Here, the feature vectors are the same irrespective of group $g$ and reward vectors are coordinateflipped. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathfrak{z}_{i}(x,y,g):=\\left\\{\\left(\\frac{y}{n+1}+1\\right)\\cdot\\cos(x_{\\lfloor i/2\\rfloor}\\cdot\\pi)\\quad\\mathrm{if~}i\\mathcal{Y}_{0}2=0}\\\\ {\\left(\\frac{1}{\\frac{y}{n+1}+1}\\right)\\cdot\\sin(x_{\\lfloor i/2\\rfloor}\\cdot\\pi)\\quad\\mathrm{~otherwise~}}&{\\theta_{0}\\quad:=(1,3,1,3),\\,\\theta_{1}:=(3,1,3,1).}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Flipped: Here, the feature vectors include alternating sin, cos terms with swapped order for $g=$ $\\{0,1\\}$ ,with alternating coefficients. And the reward vectors are coordinate-flipped as before. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\phi_{i}(x,y,g):=\\left\\{\\begin{array}{l l}{\\left(\\frac{y}{n+1}+1\\right)^{-1\\cdot\\mathbb{I}\\,\\left[i\\mathcal{Y}_{0}2=1\\right]}\\cdot\\cos(x_{\\lfloor i/2\\rfloor}\\cdot\\pi)}&{\\mathrm{if~}i\\mathcal{Y}_{0}2=g}\\\\ {\\left(\\frac{y}{n+1}+1\\right)^{-1\\cdot\\mathbb{I}\\,\\left[i\\mathcal{Y}_{0}2=1\\right]}\\cdot\\sin(x_{\\lfloor i/2\\rfloor}\\cdot\\pi)}&{\\mathrm{otherwise}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The experimental results are shown in Figures 7, 8 and 9 for the same experiment, and Figures 10, 11 and 12 for the flipped experiment. ", "page_idx": 22}, {"type": "image", "img_path": "PRAsjrmXXK/tmp/96b5814e7a5d5d2cba026f2229d809f7fbe77272f52af1b6c6327226072fa49a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 7: In this experiment, we consider same feature vectors on even groups imbalanced in size. We observe similar performance as in Figure 4 where we consider the same setting with swapped feature vectors for groups. GR-DPO slightly improves over importance sampling DPO, however, GR-IPO exactly matches the performance of importance sampling. And this is expected considering that the groups have the same level of difficulty and differ only in terms of data size. ", "page_idx": 22}, {"type": "image", "img_path": "PRAsjrmXXK/tmp/c7624d7ac919e5a1b2204d13d98656e21b9ddd258414ce2f97be9dca7ecc24d7.jpg", "img_caption": ["Figure 8: In this experiment, we consider same feature vectors on uneven groups balanced in size. Here, GR-DPO/GR-IPO outperforms corresponding importance sampling methods in terms of worstcase validation loss, but, GR-IPO tends to overfit. This is reflected in the reward errors, where GR-IPO performs worse than IPO. However,overall, GR-DPO outperforms all other methods in terms of worst-case reward errors. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "PRAsjrmXXK/tmp/a6ad34b3d8bb08b3d79ae1c56e1de0b05f144b375fb148626b703740c548defc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 9: In this experiment, we consider same feature vectors on uneven groups imbalanced in size. We observe similar performance as in Figure 8 where we consider the same features for groups but with balanced data. Here, DPO is overfitting and GR-DPO outperforms both DPO and importance sampling DPO. IPO is stable, considering it has a closed form solution. And in terms of worst-case validation loss, GR-IPO tends to overfit, even though it outperforms IPO and importance sampling IPO. The overfitting is more evident in reward errors, where GR-IPO performs worse than IPO and importance sampling IPO. However,overall, GR-DPO outperforms all other methods in terms of worst-case reward errors. ", "page_idx": 23}, {"type": "image", "img_path": "PRAsjrmXXK/tmp/40016c53e9f590ecd670c95cf4610dcbf96f77ca25dab326a1339d1d5b6b4b7c.jpg", "img_caption": ["Figure 10: In this experiment, we consider filpped feature vectors on even groups imbalanced in size. We observe very similar performance as in Figure 4 where GR-DPO/GR-IPO slightly improves over importance sampling DPO/IPO. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "PRAsjrmXXK/tmp/89fdebef0dd26a6a33d523427b70f0df950062847c958916f7da120bcbbc9f61.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 11: In this experiment, we consider flipped feature vectors on uneven groups balanced in size. Here, GR-DPO/GR-IPO outperforms corresponding vanilla methods in terms of worst-case validation loss and GR-DPO outperforms all other methods in terms of worst-case reward errors. ", "page_idx": 23}, {"type": "image", "img_path": "PRAsjrmXXK/tmp/e4a95c1ceaa971b336efda112e587a08b54111e44eca744e0ec76785525d31f4.jpg", "img_caption": ["Figure 12: In this experiment, we consider flipped feature vectors on uneven groups imbalanced in size. Here, we observe a similar performance to that of Figure 2 where GR-DPO/GR-IPO outperforms corresponding vanilla and importance sampling methods in terms of worst-case validation loss. And, GR-DPO outperforms all other methods in terms of worst-case reward errors. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "D.4 Global Opinion Data Experiments - Setup and Additional Plots ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We use the following prompt type for both SFT and DPO training: \"Opinion of people in (countryname) on: (question). Please select the best response: A) Choice-1, B) Choice-2, ...\". The target response is just the letter $\\mathbf{\\Phi}_{\\mathbf{A}},$ , \u2019B\u2019, etc. The training data size from each country are as follows: Nigeria-572, Egypt- 570, India-376, China-309, Japan-712. Further, the data is split as $80\\%$ for training, $10\\%$ for validation and $10\\%$ for testing. We run the SFT training with learning rate $10^{-4}$ for one epoch over the training data on pre-trained Gemma-2B model. We use this SFT trained model as the reference model for training IPO and GR-IPO. For the IPO training, the optimal hyperparameters were learning rate $3*10^{-5}$ , and $\\beta=0.01$ . For the GR-IPO training, we use the same $\\beta$ but the optimal learning rate and the exponential step size were $6*10^{-5}$ and $5*10^{-7}$ . For both IPO/GR-IPO training we use AdamW [27] optimizer with adaptive learning rates decreasing by a factor of 10 if there is no improvement in terms of loss (average group loss for IPO/ worst group loss for GR-IPO) for $4k$ iterations on a validation set. Further, for GR-IPO, the exponential step size is decreased by a factor of 2 whenever learning rate is reduced. This adaptive learning rates and exponential step sizes ensures stability in training and leads to convergence. We then evaluate both the methods based on the worst group loss and accuracy. Here, the accuracy refer to the percentage of prompt, winning response and losing response correctly ordered by the learned preference function (Equation (35)). ", "page_idx": 24}, {"type": "text", "text": "All experiments were run on a single node of A100 SXM4 machine with 40GB GPU memory, 30 CPU cores, 200GB RAM, and 525GB SSD memory. Further, for each seed, the execution time of the experiments is approximately 3-5 hours until convergence. We also provide the exact hyperparameters used in the table below. ", "page_idx": 24}, {"type": "table", "img_path": "PRAsjrmXXK/tmp/95e817ca104b7f5910e31078c044eff6a8cee1ad14faa562eb269b171884f085.jpg", "table_caption": [], "table_footnote": ["Table 2: Hyperparameters for SFT, IPO, and GR-IPO training. "], "page_idx": 24}, {"type": "image", "img_path": "PRAsjrmXXK/tmp/c0c213a507370e1ec05c1e5e81a761b3d69cdd5f9f500640aa9347b25b25cd62.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 13: Global opinion data: Evolution of worst-case loss, reward accuracy during IPO and GR-IPO training. Notably, both IPO and GR-IPO improve their accuracy. However, GR-IPO achieves better worst-case alignment performance. In the right, we plot the end of training log-prob. accuracies for GR-IPO. ", "page_idx": 24}, {"type": "text", "text": "E Convergence Proofs for Different Sampling Strategies ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we study the convergence rates for different sampling strategies including the proposed Algorithm 1. ", "page_idx": 25}, {"type": "text", "text": "E.1 Convergence Proof for Algorithm 1 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we prove the convergence of the proposed Algorithm 1 in terms of the error of the average iterate $\\pi_{\\bar{\\theta}^{(1:T)}}$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\epsilon_{T}=\\mathcal{L}_{\\mathrm{GR}}\\big(\\pi_{\\bar{\\theta}^{(1:T)}}\\big)-\\operatorname*{min}_{\\theta\\in\\Theta}\\mathcal{L}_{\\mathrm{GR}}\\big(\\pi_{\\theta}\\big),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "as stated in Proposition 4.1. ", "page_idx": 25}, {"type": "text", "text": "Proposition 4.1. Suppose that the loss $l(\\cdot;(x_{g},y,y^{\\prime}))$ is non-negative, convex, $B\\boldsymbol{\\nabla}$ \u2212Lipschitz continuous, and bounded by $B_{l}$ for all $(x_{g},y,y^{\\prime})\\in\\mathcal{X}\\oplus\\mathcal{G}\\times\\mathcal{Y}\\times\\mathcal{Y}$ and $\\lvert\\lvert\\theta\\rvert\\rvert_{2}\\leq B_{\\Theta}$ for all $\\theta\\in\\Theta$ with convex $\\Theta\\subset\\mathbb{R}^{d}$ . The error of the average iterate of Algorithm $^{\\,I}$ , i.e., $\\begin{array}{r}{\\pi_{\\bar{\\theta}(1:T)}=\\frac{1}{T}\\sum_{t=1}^{T}\\theta^{t}}\\end{array}$ , satisfies ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathcal{L}_{\\mathrm{GR}}(\\pi_{\\bar{\\theta}^{(1:T)}})]-\\operatorname*{min}_{\\theta\\in\\Theta}\\mathcal{L}_{\\mathrm{GR}}(\\pi_{\\theta})=\\mathcal{O}\\big(T^{-1/2}\\big).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. We build our proof based upon the online mirror descent algorithm\u2019s regret bound in [32, Eq. 3.1]. [32]\u2019s theorem considers the following saddle-point stochastic optimization problem, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta\\in\\Theta}\\operatorname*{max}_{\\alpha\\in\\Delta_{K-1}}\\Big\\{\\phi(\\theta,\\alpha)=\\sum_{g=1}^{K}\\alpha_{g}\\mathbb{E}[F_{g}(\\theta,\\xi)]\\Big\\}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Denote $f_{g}(\\theta)=\\mathbb{E}[F_{g}(\\theta,\\xi)]$ . In our problem, we consider, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\xi:=(x_{g},y,y^{\\prime})\\sim\\mathcal{D},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which is equivalent to ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\xi:=(x_{g},y,y^{\\prime})\\sim\\sum_{g=1}^{K}\\frac{N_{g}}{N}\\mathcal{D}_{g},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and we define $\\begin{array}{r}{F_{g^{\\prime}}(\\theta,(x_{g},y,y^{\\prime})):=\\frac{N}{N_{g}}\\mathbf{1}_{[g=g^{\\prime}]}l(\\theta;(x_{g},y,y^{\\prime}))}\\end{array}$ . Then, the expectation ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}_{\\xi}[F_{g}(\\theta,\\xi)]=\\sum_{g^{\\prime}=1}^{K}\\frac{N_{g^{\\prime}}}{N}\\mathbb{E}_{\\mathcal{D}_{g^{\\prime}}}[F_{g}(\\theta,\\xi)|g=g^{\\prime}]}\\\\ {\\displaystyle\\qquad\\qquad=\\frac{N_{g}}{N}\\mathbb{E}_{\\mathcal{D}_{g}}[F_{g}(\\theta,\\xi)|g=g^{\\prime}]}\\\\ {\\displaystyle\\qquad\\qquad=\\frac{N_{g}}{N}\\mathbb{E}_{\\mathcal{D}_{g}}\\Big[\\frac{N}{N_{g}}l(\\theta;(x_{g},y,y^{\\prime}))|g=g^{\\prime}\\Big]}\\\\ {\\displaystyle\\qquad\\qquad=\\mathbb{E}_{\\mathcal{D}_{g}}[l(\\theta;(x_{g},y,y^{\\prime}))|g=g^{\\prime}]}\\\\ {\\displaystyle\\qquad\\qquad=f_{g}(\\theta)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Mirror Descent mapping. We begin by mapping Algorithm 1 to the general mirror descent framework of [32]. ", "page_idx": 25}, {"type": "text", "text": "Further, we denote the gradients of the objective $\\phi(\\theta,\\alpha)$ w.r.t. $\\theta$ and $\\alpha$ as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\partial\\phi(\\theta,\\alpha)=(\\partial_{\\theta}\\phi(\\theta,\\alpha),-\\partial_{\\alpha}\\phi(\\theta,\\alpha)).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Here, the negative sign for the gradient w.r.t. $\\alpha$ indicates that we perform maximization w.r.t. $\\alpha$ . Let $\\begin{array}{r}{\\Phi(\\theta,\\alpha,\\xi):=\\sum_{g=1}^{K}\\alpha_{g}[F_{g}(\\theta,\\xi)]}\\end{array}$ . Then the stochastic subgradients for $\\phi(\\theta,\\alpha)$ for a given $\\xi$ is as follows: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\partial\\Phi(\\theta,\\alpha,\\xi)=(\\partial_{\\theta}\\Phi(\\theta,\\alpha,\\xi),-\\partial_{\\alpha}\\Phi(\\theta,\\alpha,\\xi)).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Simplifying the gradients we obtain the following for $\\xi=(x_{g},y,y^{\\prime})$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\partial\\Phi(\\theta,\\alpha,\\xi)=\\left[\\phantom{-}\\partial_{\\theta}\\sum_{g^{\\prime}=1}^{K}\\alpha_{g^{\\prime}}[F_{g^{\\prime}}(\\theta,\\xi)]\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\alpha\\sum_{\\sigma_{\\alpha}}^{K}\\sum_{g^{\\prime}=1}^{K}\\alpha_{g^{\\prime}}[F_{g^{\\prime}}(\\theta,\\xi)]\\Bigr]}\\\\ &{\\qquad\\qquad\\qquad=\\left[\\phantom{-}\\partial_{\\theta}\\sum_{g^{\\prime}=1}^{K}\\alpha_{g^{\\prime}}[\\frac{N}{N_{g^{\\prime}}}{\\bf1}_{[g=g^{\\prime}]}l(\\theta;(x_{g},y,y^{\\prime}))]\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\left.-\\partial_{\\alpha}\\sum_{g^{\\prime}=1}^{K}\\alpha_{g^{\\prime}}[\\frac{N}{N_{g^{\\prime}}}{\\bf1}_{[g=g^{\\prime}]}l(\\theta;(x_{g},y,y^{\\prime}))]\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\left.\\frac{N\\alpha_{g}}{N_{g}}\\nabla_{\\theta}l(\\theta;(x_{g},y,y^{\\prime}))\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\left.-\\left(0,\\cdots\\,,\\frac{N}{N_{g}}l(\\theta;(x_{g},y,y^{\\prime})),\\cdots\\,,0\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Based on the above gradient expressions, we can map the updates of $\\theta^{t}$ and $\\alpha^{t}$ in Algorithm 1, to the general mirror descent update rule: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\zeta^{t+1}=(\\theta^{t+1},\\alpha^{t+1})=P_{(\\zeta^{t})}(\\gamma^{t}\\partial\\Phi(\\theta^{t},\\alpha^{t},\\xi)).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Here, we use the compact notation $\\zeta=(\\theta,\\alpha)$ . Further, $\\gamma^{t}$ denotes the step-size at time $t$ of the algorithm and $P_{\\zeta}(\\cdot)$ is the prox-mapping corresponding to the mirror descent algorithm defined as follows: ", "page_idx": 26}, {"type": "equation", "text": "$$\nP_{\\zeta}(\\nu)=\\operatorname*{arg\\,min}_{\\zeta^{\\prime}\\in(\\Theta\\times\\Delta_{K-1})}\\left\\{\\nu^{\\top}(\\zeta^{\\prime}-\\zeta)+V(\\zeta,\\zeta^{\\prime})\\right\\}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Here, $V(\\cdot,\\cdot)$ is the prox-function associated with the distance-generating function (Bregmann Divergence) $\\omega(\\cdot)$ . For our setup in Algorithm 1, we define the following combined distance-generating function over $\\zeta\\in\\left(\\Theta\\times\\Delta_{K-1}\\right)$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\omega(\\zeta)=\\frac{\\omega_{\\theta}(\\theta)}{2D_{\\omega_{\\theta},\\Theta}^{2}}+\\frac{\\omega_{\\alpha}(\\alpha)}{2D_{\\omega_{\\alpha},\\Delta_{K-1}}^{2}},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "whe $\\begin{array}{r l}&{\\mathrm{te}\\;\\omega_{\\theta}(\\theta)=\\frac{1}{2}\\|\\theta\\|^{2},\\omega_{\\alpha}(\\alpha)=\\sum_{g=1}^{K}\\alpha_{i}\\ln\\alpha_{i},D_{\\omega_{\\theta},\\Theta}:=\\left(\\operatorname*{max}_{\\theta\\in\\Theta}\\omega_{\\theta}(\\theta)-\\operatorname*{min}_{\\theta\\in\\Theta}\\omega_{\\theta}(\\theta)\\right)^{1/2},}\\\\ &{D_{\\omega_{\\alpha},\\Delta_{K-1}}:=\\left(\\operatorname*{max}_{\\alpha\\in\\Delta_{K-1}}\\omega_{\\alpha}(\\alpha)-\\operatorname*{min}_{\\alpha\\in\\Delta_{K-1}}\\omega_{\\alpha}(\\alpha)\\right)^{1/2}.\\mathrm{~Also,~the~corresponding~prox}.}\\end{array}$ and functions individually for $\\theta$ and $\\alpha$ would be $\\begin{array}{r}{V_{\\theta}(\\theta,\\theta^{\\prime})=\\frac{1}{2}\\|\\theta-\\theta^{\\prime}\\|_{2}^{2}}\\end{array}$ and $\\begin{array}{r}{V_{\\alpha}(\\alpha,\\alpha^{\\prime})=\\sum_{g=1}^{K}\\alpha_{i}^{\\prime}\\ln\\frac{\\alpha_{i}^{\\prime}}{\\alpha_{i}}}\\end{array}$ . It can be shown that the above prox-functions $V_{\\theta}$ and $V_{\\alpha}$ satisfy the following inequalities ([32]): ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\theta\\in\\Theta}V_{\\theta}(\\theta^{\\prime},\\theta)\\leq D_{\\omega_{\\theta},\\Theta}^{2},\\quad\\operatorname*{max}_{\\alpha\\in\\Delta_{K-1}}V_{\\alpha}(\\alpha^{\\prime},\\alpha)\\leq D_{\\omega_{\\alpha^{\\prime}},\\Delta_{K-1}}^{2}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Due to the above definitions, the corresponding prox-mapping $P(\\cdot)$ corresponds to gradient descent w.r.t. $\\theta$ and exponentiated gradient ascent w.r.t. $\\alpha$ , as defined in Algorithm 1. Moreover, due to the definition of $\\omega$ in Equation (52), the combined prox function $V(\\cdot,\\cdot)$ over $\\zeta$ would satisfy ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\zeta\\in\\Theta\\times\\Delta_{K-1}}V(\\zeta^{\\prime},\\zeta)\\leq1.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For further details regarding prox-function, kindly refer to its usage in [32]. ", "page_idx": 26}, {"type": "text", "text": "Bounding the error. According to the introduced notation, the approximation error $\\epsilon_{T}$ can be defined and bounded as: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\epsilon_{T}=\\underset{\\alpha\\in\\Delta_{K-1}}{\\operatorname*{max}}\\phi(\\tilde{\\theta}^{1:T},\\alpha)-\\underset{\\theta\\in\\Theta}{\\operatorname*{min}}\\underset{\\alpha\\in\\Delta_{K-1}}{\\operatorname*{max}}\\phi(\\theta,\\alpha)}\\\\ &{\\quad\\leq\\underset{\\alpha\\in\\Delta_{K-1}}{\\operatorname*{max}}\\phi(\\tilde{\\theta}^{1:T},\\alpha)-\\underset{\\theta\\in\\Theta}{\\operatorname*{min}}\\phi(\\theta,\\tilde{\\alpha}^{1:T})}\\\\ &{\\quad=\\epsilon_{\\phi}(\\tilde{\\zeta}^{1:T})}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "To bound $\\epsilon_{\\phi}\\big(\\tilde{\\zeta}^{1:T}\\big)$ , we invoke the result from [32, Equation 3.23] for mirror-descent convergence for max-min problems. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\epsilon_{\\phi}(\\tilde{\\zeta}^{1:T})\\leq2\\sqrt{10\\frac{R_{\\theta}^{2}M_{\\theta}^{2}+\\ln(K)M_{\\alpha}^{2}}{T}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Further, $M_{\\theta}$ and $M_{\\alpha}$ correspond to the following upper bounds to the maximum of the expected norms of $F_{g}(\\theta,\\xi)$ and $\\nabla_{\\boldsymbol{\\theta}}F_{g}\\bar{(\\boldsymbol{\\theta},\\boldsymbol{\\xi})}$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\mathbb{S}}{\\mathbf{x}}\\mathbb{E}\\|\\nabla F_{g}(\\theta,\\xi)\\|^{2}=\\underset{1\\leq g\\leq K}{\\operatorname*{max}}\\mathbb{E}\\bigg\\|\\frac{N}{N_{g}}\\mathbf{1}_{[g=g^{\\prime}]}\\nabla l(\\theta;(x_{g},y,y^{\\prime}))\\bigg\\|^{2}=\\underset{1\\leq g\\leq K}{\\operatorname*{max}}\\frac{N_{g}}{N}\\frac{N^{2}}{N_{g}^{2}}\\|\\nabla l(\\theta;(x_{g},y,y^{\\prime}))\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\underset{1\\leq g\\leq K}{\\operatorname*{max}}\\frac{N}{N_{g}}\\|\\nabla l(\\theta;(x_{g},y,y^{\\prime}))\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq B_{\\nabla}^{2}\\underset{g\\in\\mathcal{G}}{\\operatorname*{max}}\\frac{N}{N_{g}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=B_{\\nabla}^{2}\\underset{\\operatorname*{min}}{\\operatorname*{min}}\\L_{g\\in\\mathcal{G}}N_{g}}\\\\ &{=M_{\\theta}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Similarly, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{1\\leq g\\leq K}{\\mathrm{\\operatorname*{max}}}\\,\\|F_{g}(\\theta,\\xi)\\|^{2}=\\mathbb{E\\,\\underset{1\\leq g\\leq K}{\\operatorname*{max}}}\\,\\Big\\|\\frac{N}{N_{g}}\\mathbf{1}_{[g=g^{\\prime}]}l\\big(\\theta;(x_{g},y,y^{\\prime})\\big)\\Big\\|^{2}\\leq\\underset{g=1}{\\overset{K}{\\sum}}\\frac{N_{g}}{N}\\frac{N^{2}}{N_{g}^{2}}\\|l\\big(\\theta;(x_{g},y,y^{\\prime})\\big)\\|^{2}}\\\\ &{=\\underset{g=1}{\\overset{K}{\\sum}}\\frac{N}{N_{g}}\\|l(\\theta;(x_{g},y,y^{\\prime}))\\|^{2}}\\\\ &{\\leq K B_{l}^{2}\\underset{g\\in\\mathcal{G}}{\\operatorname*{max}}\\,\\frac{N}{N_{g}}}\\\\ &{=K B_{l}^{2}\\underset{\\operatorname*{min}}{\\overset{N}{\\sum}}\\frac{N}{\\|\\operatorname*{min}_{g\\in\\mathcal{G}}N_{g}}}\\\\ &{=M_{a}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Here, we have used $\\|\\nabla_{\\theta}l(\\theta;(x_{g},y))\\|\\le B_{\\nabla}$ and $\\|l(\\theta;(x_{g},y))\\|\\leq B_{l}$ . Here, $\\lambda_{\\theta},\\,\\lambda_{\\alpha}$ correspond to the strong convexity parameters of the distance generating functions $\\omega_{\\theta}(\\theta)$ and $\\omega_{\\alpha}(\\alpha)$ respectively. For our given functions, $\\omega_{\\theta}(\\theta)\\,=\\,\\textstyle{\\frac{1}{2}}\\|\\theta\\|^{2}$ and $\\begin{array}{r}{\\omega_{\\alpha}(\\alpha)=\\sum_{g=1}^{K}\\alpha_{i}\\ln\\alpha_{i}}\\end{array}$ , both $\\lambda_{\\theta}=\\lambda_{\\alpha}=1$ ([32]). Let, R\u03b82 = \u03bb\u03c9\u03b8\u03b8 $\\begin{array}{r}{R_{\\theta}^{2}=\\frac{D_{\\omega_{\\theta}}^{2}}{\\lambda_{\\theta}}=B_{\\Theta}^{2}=(\\operatorname*{max}_{\\theta}\\|\\theta\\|_{\\theta}-\\operatorname*{min}_{\\theta}\\|\\theta\\|_{\\theta}^{2})}\\end{array}$ , obtaining the overall error bound. ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\epsilon_{\\Phi}(\\tilde{\\zeta}^{1:T})\\leq2\\sqrt{10\\Bigl(\\frac{N}{\\operatorname*{min}_{g\\in\\mathcal{G}}N_{g}}\\Bigr)\\frac{B_{\\Theta}^{2}B_{\\nabla}^{2}+K B_{l}^{2}\\ln K}{T}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Lemma E.1. For the log-linear policy class parameterized with respect to $\\theta$ , the DPO loss function $l(\\pi_{\\theta};\\cdot)=\\log\\left(\\sigma(\\beta h_{\\pi_{\\theta}}(\\cdot))\\right)$ (see Equation (5)) is convex and Lipschitz continuous in $\\theta$ . Consequently, Proposition 4.1 applies to this case. ", "page_idx": 27}, {"type": "text", "text": "Proof. We want to show, $l(\\theta;(x,y_{w},y_{l}))$ is convex in $\\theta$ and $B_{\\phi}$ -Lipschitz in $\\theta$ for the log-linear policy class defined as follows: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pi_{\\theta}(y\\mid x)=\\frac{\\exp{\\theta^{T}\\phi(x,y)}}{\\sum_{y\\in\\mathcal{y}}\\exp{\\theta^{T}\\phi(x,y)}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Here $\\theta$ belongs to a convex set $\\Theta$ satisfying $\\lVert\\theta\\rVert\\leq B_{\\Theta}$ and $\\phi(x,y)$ denotes the feature vector such that $\\|\\phi(x_{g},\\bar{y_{w}})-\\phi(x_{g},y_{l})\\|\\leq B_{\\phi}$ . In conjunction with a uniform reference policy $\\pi_{\\mathrm{ref}}$ , the loss function $l(\\theta;(x_{g},y_{w},y_{l}))$ for the log-linear policy class is as follows: ", "page_idx": 27}, {"type": "equation", "text": "$$\nl(\\theta;(x_{g},y_{w},y_{l}))=-\\log\\Big(\\sigma\\big(\\beta\\langle\\phi(x_{g},y_{w})-\\phi(x_{g},y_{l}),\\theta\\rangle\\big)\\Big).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "1: Initialize: Step size $\\eta_{\\alpha}$ for group weights $\\alpha$ , step size $\\eta_{\\theta}$ for policy $\\pi$ with weights $\\theta$ , initial   \nweights $\\theta^{(0)}$ of the policy and weights over each group $\\alpha^{(0)}$ , Projection operator $\\mathrm{P}_{\\Theta}$   \n2: Input: Dataset $\\mathcal{D}$ with size $N=|\\mathcal{D}|$ , group size $N_{g}$ for $g=\\{1,2,\\cdots\\,,K\\}$ , loss $l(\\pi_{\\theta};\\cdot)$   \n3: for $t=1,\\dots,T$ do   \n4: \u03b1\u2032 \u2190\u03b1(t\u22121)   \n5: $\\begin{array}{r l}&{g\\sim\\mathrm{Uniform}(1,\\cdot\\cdot\\cdot\\cdot,K)}\\\\ &{(x_{g},y_{w},y_{l})\\sim\\mathcal{D}_{g}}\\\\ &{\\alpha_{g}^{\\prime}\\leftarrow\\alpha_{g}^{\\prime}\\exp\\eta_{\\alpha}(l(\\pi_{\\theta(t-1)};(x_{g},y_{w},y_{l})))\\quad//\\mathrm{~Update~weights~for~group~g~}}\\\\ &{\\alpha^{(t)}\\leftarrow\\alpha^{\\prime}/\\sum_{g^{\\prime}}\\alpha_{g^{\\prime}}^{\\prime}\\qquad//{l}\\mathrm{~Renormalize~}\\alpha}\\\\ &{\\theta^{(t)}\\leftarrow\\mathrm{P}_{\\Theta}\\Big(\\theta^{(t-1)}-\\eta_{\\theta}\\Big(\\alpha_{g}^{(t)}\\nabla_{\\theta}l(\\pi_{\\theta^{(t-1)}};(x_{g},y_{w},y_{l}))\\Big)\\Big)\\,/\\ l\\mathrm{~Use~}\\alpha\\mathrm{~u~pdate~}\\theta}\\\\ &{\\large\\texttt{\\footnotesize\\cite[\\footnotesize{\\texttt{S}}]{2}}}\\end{array}$   \n6:   \n7:   \n8:   \n9: ", "page_idx": 28}, {"type": "text", "text": "10: end for ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "11: Return: Output the robust policy $\\pi(\\theta^{(T)})$ ", "page_idx": 28}, {"type": "text", "text": "Next, we compute the derivative of $l(\\theta;(x,y_{w},y_{l}))$ w.r.t. $\\theta$ from Equation (58) to check the Lipschitz continuity. ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\theta}l(\\theta;(x_{g},y_{w},y_{l}))}\\\\ &{=\\frac{\\sigma\\left(\\beta\\left\\langle\\phi(x_{g},y_{w})-\\phi(x_{g},y_{l}),\\theta\\right\\rangle\\right)\\left(1-\\sigma\\left(\\beta\\left\\langle\\phi(x_{g},y_{w})-\\phi(x_{g},y_{l}),\\theta\\right\\rangle\\right)\\right)\\ast\\left(\\phi(x_{g},y_{w})-\\phi(x_{g},y_{l})\\right)}{\\sigma\\left(\\beta\\left\\langle\\phi(x_{g},y_{w})-\\phi(x_{g},y_{l}),\\theta\\right\\rangle\\right)}}\\\\ &{=\\left(1-\\sigma\\left(\\beta\\langle\\phi(x_{g},y_{w})-\\phi(x_{g},y_{l}),\\theta\\rangle\\right)\\right)\\ast(\\phi(x_{g},y_{w})-\\phi(x_{g},y_{l}))}\\\\ &{=\\sigma\\big(\\beta\\langle\\phi(x_{g},y_{w})-\\phi(x_{g},y_{l}),-\\theta\\rangle\\big)\\ast(\\phi(x_{g},y_{w})-\\phi(x_{g},y_{l})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Then, the gradient norm can be bounded as follows: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{\\theta}l(\\theta;(x_{g},y_{w},y_{l}))\\|\\le\\|\\phi(x_{g},y_{w})-\\phi(x_{g},y_{l})\\|}\\\\ &{\\qquad\\qquad\\qquad\\le B_{\\phi}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Hence, by the definition of Lipschitz functions for continuously differentiable functions, we have that $l(\\theta;(x,y_{w},y_{l}))$ is Lipschitz continuous with Lipschitz constant $B_{\\phi}$ . Further, we bound the loss function using the bounds for $\\theta$ and $\\phi(\\cdot,\\cdot)$ . ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\phi(x_{g},y_{w})-\\phi(x_{g},y_{l}),\\theta\\rangle\\geq-\\|\\phi(x_{g},y_{w})-\\phi(x_{g},y_{l})\\|\\,\\|\\theta\\|}\\\\ &{\\qquad\\qquad\\langle\\phi(x_{g},y_{w})-\\phi(x_{g},y_{l}),\\theta\\rangle\\geq-B_{\\phi}B_{\\Theta}}\\\\ &{\\quad\\log(\\sigma(\\langle\\phi(x_{g},y_{w})-\\phi(x_{g},y_{l}),\\theta\\rangle))\\geq\\log(\\sigma(-B_{\\phi}B_{\\Theta}))}\\\\ &{-\\log(\\sigma(\\langle\\phi(x_{g},y_{w})-\\phi(x_{g},y_{l}),\\theta\\rangle))\\leq-\\log(\\sigma(-B_{\\phi}B_{\\Theta}))}\\\\ &{\\quad\\leq\\log(1+\\exp{(B_{\\phi}B_{\\Theta})})}\\\\ &{\\quad\\leq\\log(\\exp{(B_{\\phi}B_{\\Theta})}+\\exp{(B_{\\phi}B_{\\Theta})})}\\\\ &{\\quad\\leq\\log(2)+(B_{\\phi}B_{\\Theta})=B_{l}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Hence, the loss function is bounded by $B_{l}$ for bounded $\\theta$ and $\\phi$ . ", "page_idx": 28}, {"type": "text", "text": "E.2 Alternate Sampling Strategy ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this section, we study the convergence of an alternate algorithm in Algorithm 3, wherein groups are sampled uniformly rather than a categorical distribution proportional to the group sizes, in terms of the error $\\epsilon_{T}$ as defined in Equation (46). ", "page_idx": 28}, {"type": "text", "text": "Proposition E.2. Suppose that the loss $l(\\cdot;(x_{g},y,y^{\\prime}))$ is non-negative, convex, $B\\boldsymbol{\\nabla}$ \u2212Lipschitz continuous, and bounded by $B_{l}$ for all $(x_{g},y,y^{\\prime})\\in\\mathcal{X}\\oplus\\mathcal{G}\\times\\mathcal{Y}\\times\\mathcal{Y}$ and $\\lvert\\lvert\\theta\\rvert\\rvert_{2}\\leq B_{\\Theta}$ for all $\\theta\\in\\Theta$ with convex $\\Theta\\subset\\mathbb{R}^{d}$ . Then, the average iterate of Algorithm 3 achieves an error at the rate ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\epsilon_{\\Phi}(\\tilde{\\zeta}^{1:T})\\leq2\\sqrt{10\\frac{B_{\\Theta}^{2}B_{\\nabla}^{2}+K B_{l}^{2}\\ln K}{T}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. We follow a similar proof structure as in the proof of Proposition 4.1. We recall the saddlepoint stochastic optimization problem from [32] stated earlier in Equation (47), ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta\\in\\Theta}\\operatorname*{max}_{\\alpha\\in\\Delta_{K-1}}\\Big\\{\\phi(\\theta,\\alpha)=\\sum_{g=1}^{K}\\alpha_{g}\\mathbb{E}[F_{g}(\\theta,\\xi)]\\Big\\}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "In lieu of the alternate sampling strategy in Algorithm 3 where groups are sampled uniformly, we consider, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\xi:=(x_{g},y,y^{\\prime})\\sim\\sum_{g=1}^{K}\\frac{1}{K}{\\mathcal{D}}_{g},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and we define $F_{g^{\\prime}}(\\theta,(x_{g},y,y^{\\prime})):=K\\mathbf{1}_{[g=g^{\\prime}]}l(\\theta;(x_{g},y,y^{\\prime}))$ . Then, the expectation ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}_{\\xi}[F_{g}(\\theta,\\xi)]=\\sum_{g^{\\prime}=1}^{K}\\frac{1}{K}\\mathbb{E}_{\\mathcal{D}_{g^{\\prime}}}[F_{g}(\\theta,\\xi)|g=g^{\\prime}]}\\\\ {\\displaystyle\\qquad\\qquad=\\frac{1}{K}\\mathbb{E}_{\\mathcal{D}_{g}}[F_{g}(\\theta,\\xi)|g=g^{\\prime}]}\\\\ {\\displaystyle\\qquad\\qquad=\\frac{1}{K}\\mathbb{E}_{\\mathcal{D}_{g}}[K l(\\theta;(x_{g},y,y^{\\prime}))|g=g^{\\prime}]}\\\\ {\\displaystyle\\qquad\\qquad=\\mathbb{E}_{\\mathcal{D}_{g}}[l(\\theta;(x_{g},y,y^{\\prime}))|g=g^{\\prime}]}\\\\ {\\displaystyle\\qquad\\qquad=f_{g}(\\theta)}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Mirror Descent mapping. We map Algorithm 3 to the general mirror descent framework of [32] as done in the proof of Proposition 4.1. We begin by recalculating the gradients of $\\phi(\\theta,\\alpha)$ w.r.t. $\\theta$ and $\\alpha$ for this alternate definition of $F_{g}(\\theta,\\xi)$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\partial\\Phi(\\theta,\\alpha,\\xi)=\\left[\\phantom{-}\\partial_{\\theta}\\sum_{g^{\\prime}=1}^{K}\\alpha_{g^{\\prime}}[F_{g^{\\prime}}(\\theta,\\xi)]\\phantom{\\big|}\\right]}\\\\ &{\\qquad\\qquad=\\partial_{\\alpha}\\sum_{g^{\\prime}=1}^{K}\\alpha_{g^{\\prime}}[F_{g^{\\prime}}(\\theta,\\xi)]\\bigg]}\\\\ &{\\qquad\\qquad=\\left[\\phantom{-}\\partial_{\\theta}\\sum_{g^{\\prime}=1}^{K}\\alpha_{g^{\\prime}}[K\\mathbf{1}_{[g=g^{\\prime}]}l(\\theta;(x_{g},y,y^{\\prime}))]\\phantom{\\big|}\\right]}\\\\ &{\\qquad\\qquad=\\left[\\phantom{-}\\partial_{\\alpha}\\sum_{g^{\\prime}=1}^{K}\\alpha_{g^{\\prime}}[K\\mathbf{1}_{[g=g^{\\prime}]}l(\\theta;(x_{g},y,y^{\\prime}))]\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad K\\alpha_{g}\\nabla_{\\theta}l(\\theta;(x_{g},y,y^{\\prime}))}\\\\ &{\\qquad\\qquad=\\left[\\phantom{-}\\!\\!-\\left(0,\\cdots,K l(\\theta;(x_{g},y,y^{\\prime})),\\cdots,0\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Based on the above gradient expressions, we can similarly map the updates of $\\theta^{t}$ and $\\alpha^{t}$ in Algorithm 3, to the general mirror descent update rule with corresponding prox-function as done for Proposition 4.1. We omit further details regarding the mirror descent related definitions as they follow from the proof of Proposition 4.1. We directly proceed to bounding the error $\\epsilon_{\\phi}\\big(\\tilde{\\zeta}^{1:T}\\big)$ using [32, Equation 3.23], ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\epsilon_{\\phi}(\\tilde{\\zeta}^{1:T})\\leq2\\sqrt{10\\frac{R_{\\theta}^{2}M_{\\theta}^{2}+\\ln(K)M_{\\alpha}^{2}}{T}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We recalculate $M_{\\theta}$ and $M_{\\alpha}$ for this alternate definition of $F_{g}(\\theta,\\xi)$ as they correspond to the upper bounds to the maximum of the expected norms of $F_{g}(\\theta,\\xi)$ and $\\nabla_{\\boldsymbol{\\theta}}F_{g}(\\boldsymbol{\\theta},\\boldsymbol{\\xi})$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\leq g\\leq K}{\\operatorname*{max}}\\mathbb{E}\\|\\nabla F_{g}(\\theta,\\xi)\\|^{2}=\\underset{1\\leq g\\leq K}{\\operatorname*{max}}\\mathbb{E}\\|K1_{[g=g^{\\prime}]}\\nabla l(\\theta;(x_{g},y,y^{\\prime}))\\|^{2}=\\underset{1\\leq g\\leq K}{\\operatorname*{max}}\\frac{1}{K}K^{2}\\|\\nabla l(\\theta;(x_{g},y,y^{\\prime}))\\|}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\underset{1\\leq g\\leq K}{\\operatorname*{max}}K\\|\\nabla l(\\theta;(x_{g},y,y^{\\prime}))\\|^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq B_{\\nabla}^{2}K}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=B_{\\nabla}^{2}K}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=M_{\\theta}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Similarly, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E\\operatorname*{max}_{1\\leq g\\leq K}\\|F_{g}(\\theta,\\xi)\\|^{2}=\\mathbb{E\\operatorname*{max}_{1\\leq g\\leq K}\\|K\\mathbf{1}_{[g=g^{\\prime}]}l(\\theta;(x_{g},y,y^{\\prime}))\\|^{2}\\leq\\displaystyle\\sum_{g=1}^{K}\\frac{1}{K}K^{2}\\|l(\\theta;(x_{g},y,y^{\\prime}))\\|^{2}}}}\\\\ &{}&{\\quad=\\displaystyle\\sum_{g=1}^{K}K\\|l(\\theta;(x_{g},y,y^{\\prime}))\\|^{2}}\\\\ &{}&{\\quad\\leq B_{l}^{2}K^{2}}\\\\ &{}&{\\quad=B_{l}^{2}K^{2}}\\\\ &{}&{\\quad=M_{g}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Here, we have used $\\|\\nabla_{\\theta}l(\\theta;(x_{g},y))\\|\\le B_{\\nabla}$ and $\\|l(\\theta;(x_{g},y))\\|\\leq B_{l}$ . ", "page_idx": 30}, {"type": "text", "text": "Using the alternate $M_{\\theta}$ and $M_{\\alpha}$ corresponding to this sampling rule, we obtain the overall error bound as follows: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\epsilon_{\\Phi}(\\tilde{\\zeta}^{1:T})\\leq2\\sqrt{10K\\frac{B_{\\Theta}^{2}B_{\\nabla}^{2}+B_{l}^{2}K\\ln K}{T}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: All the claims regarding the proposed methodology in terms of theory and experiments have been concretely illustrated in Sections 3 and 4(Theory) and Section 5 (Experiments). ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: Limitations are detailed along with conclusions in Section 6. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: All assumptions are mentioned in the theorem statements and the corresponding proofs are detailed in the appendix. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: All experimental details (models, datasets, configurations, training strategies, compute resources) are discussed in the experimental sections of the main text and appendix. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We provide links to Github repositories that contain all the codes required to reproduce the experiments with detailed instructions. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: All hyperparameters, optimizers, data splits, etc., are detailed in the experimental section in the appendix. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: Error bars are shown based on different seeds of the experiments. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Compute resources required and used to run the experiments are mentioned in the experimental sections in the main text and appendix ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We are using open-sourced data and language model. We do not foresee any potential harm caused by our work. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We discuss the broader positive societal impacts of our work in Section 6. We do not foresee any negative societal impacts of our work. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 34}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: We are using publicly available model and data. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: All the data and models used have been appropriately cited with URLs which includes the licenses. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Our code and algorithm implementation has been properly documented and links to the Github repositories have been provided. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: We do not conduct any such experiments for this work. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}]