[{"figure_path": "jIh4W7r0rn/figures/figures_2_1.jpg", "caption": "Figure 1: The interaction between the images in conditioning sequence occurs in the DiTBlock, which has a causal attention module to ensure In is conditioned on previous images X<n. During training, the net predicts the noise for each noisy image that is sampled from the target sequence given the conditioning sequence in parallel. During generation, the net iteratively refines the noisy input to produce a clean image, which is then appended to the conditioning sequence.", "description": "This figure illustrates the architecture of the autoregressive image diffusion (AID) model.  The model processes a conditioning sequence of images (X0, X1, X2, X3, X4, X5) to predict the noise for a target sequence of images (X1, X2, X3, X4, X5, X6).  The key component is the DiTBlock, which uses a causal attention mechanism to ensure that the prediction for each image in the target sequence only depends on the preceding images in both the target and conditioning sequences. This ensures the sequential coherence of generated images. During training, the network learns to predict the noise added to the target images, while during generation, it iteratively refines noisy images to produce clean samples, effectively reversing the diffusion process. The encoder and decoder blocks in the U-net-like architecture and TSC block handles temporal-spatial conditioning. The figure highlights the flow of information and the role of the causal attention mechanism in generating coherent image sequences.", "section": "2 Methods"}, {"figure_path": "jIh4W7r0rn/figures/figures_6_1.jpg", "caption": "Figure 2: (a): A sequence of images from dataset is shown in the first row and is used as conditioning to generate retrospective samples that are shown in the second row. (b): With the given sequence in (a) as a warm start, prospective samples extending it are shown.", "description": "This figure shows the results of two different sampling methods used to generate image sequences: retrospective sampling and prospective sampling.  Retrospective sampling starts with a given sequence of images and generates a new sequence based on this initial sequence.  Prospective sampling uses a sliding window that updates with each generated image; the earliest image is removed as a new one is added. The top row in each part shows the initial sequence used for generation (a) or the starting window (b). The bottom row shows the generated sequences.  The images demonstrate the model's ability to generate both coherent and consistent image sequences using both methods.", "section": "3.2 Generating sequence of images"}, {"figure_path": "jIh4W7r0rn/figures/figures_6_2.jpg", "caption": "Figure 2: (a): A sequence of images from dataset is shown in the first row and is used as conditioning to generate retrospective samples that are shown in the second row. (b): With the given sequence in (a) as a warm start, prospective samples extending it are shown.", "description": "This figure shows the results of two different sampling methods used to generate image sequences.  In (a), retrospective sampling is used, which generates a new sequence based on a given sequence. The first row shows the initial sequence, and the second row displays the generated sequence. In (b), prospective sampling is used, which generates a sequence by iteratively updating a sliding window of images. The initial sequence is shown, and the generated sequence is displayed extending this initial sequence. This demonstrates the model's ability to generate coherent image sequences using both methods, highlighting the differences between the two sampling approaches.", "section": "3.2 Generating sequence of images"}, {"figure_path": "jIh4W7r0rn/figures/figures_7_1.jpg", "caption": "Figure 4: (a): The folded single-coil image caused by two-times undersampling mask. (b): The comparison of unfolding ability by the autoregressive and the standard diffusion model, i.e., AID (top) and Guide (bottom). Reference image is reconstructed from k-space data without undersampling. The error is the difference between the mean, XMMSE, and the reference image. The \"Mean+std\" is the mean highlighted with confidence interval, which indicates the reconstruction by AID is more trustworthy in the region of folding artifacts.", "description": "This figure shows the results of an experiment designed to compare the performance of the autoregressive image diffusion (AID) model and a standard diffusion model (Guide) in reconstructing a single-coil image from undersampled k-space data.  Panel (a) displays the folded single-coil image resulting from two-times undersampling. Panel (b) presents a comparison of the reconstruction results. The top row shows the results for the AID model, and the bottom row shows the results for the Guide model. The \"Error\" column shows the difference between the reconstructed image and the reference image (reconstructed from fully sampled k-space data). The \"Mean+Std\" column displays the mean of the reconstructed images with the confidence interval based on the standard deviation, indicating the uncertainty in the reconstruction. The \"Mean\" column shows the average of the reconstructed images, and the \"Reference\" column shows the reference image. The comparison demonstrates that the AID model significantly reduces the errors in the folding artifact regions compared to the Guide model, leading to a more trustworthy reconstruction.", "section": "3.3 MRI reconstruction"}, {"figure_path": "jIh4W7r0rn/figures/figures_8_1.jpg", "caption": "Figure 5: E: equispaced, R: random. (a): PSNR and (b): NRMSE of the images reconstructed from the twelve-times undersampled k-space data using the autoregressive diffusion model (AID), the standard diffusion model (Guide), and the baseline method CSGM. PSNR higher is better, and NRMSE lower is better.", "description": "This figure compares the performance of three different models (AID, Guide, and CSGM) in reconstructing images from twelve times undersampled k-space data.  Two types of sampling masks were used: equispaced and random, each with and without autocalibration signals (ACS).  The results are presented in terms of Peak Signal-to-Noise Ratio (PSNR) and Normalized Root Mean Square Error (NRMSE).  Higher PSNR values indicate better reconstruction quality, while lower NRMSE values indicate less error.", "section": "3 Experiments and Results"}, {"figure_path": "jIh4W7r0rn/figures/figures_8_2.jpg", "caption": "Figure 5: E: equispaced, R: random. (a): PSNR and (b): NRMSE of the images reconstructed from the twelve-times undersampled k-space data using the autoregressive diffusion model (AID), the standard diffusion model (Guide), and the baseline method CSGM. PSNR higher is better, and NRMSE lower is better.", "description": "This figure compares the performance of three different models (AID, Guide, and CSGM) in reconstructing images from twelve-times undersampled k-space data.  Two types of sampling masks were used: equispaced and random, each with and without autocalibration signals (ACS).  The results are presented in terms of peak signal-to-noise ratio (PSNR) and normalized root-mean-square error (NRMSE). Higher PSNR values and lower NRMSE values indicate better reconstruction quality. The figure shows that the AID model generally outperforms the other two models across different sampling strategies.", "section": "3 Experiments and Results"}, {"figure_path": "jIh4W7r0rn/figures/figures_13_1.jpg", "caption": "Figure 1: The interaction between the images in conditioning sequence occurs in the DiTBlock, which has a causal attention module to ensure In is conditioned on previous images X<n. During training, the net predicts the noise for each noisy image that is sampled from the target sequence given the conditioning sequence in parallel. During generation, the net iteratively refines the noisy input to produce a clean image, which is then appended to the conditioning sequence.", "description": "This figure illustrates the architecture of the autoregressive image diffusion (AID) model.  The AID model processes a sequence of images.  It uses an encoder-decoder structure with a transformer-based block (DiTBlock) that incorporates causal attention to model the dependencies between consecutive images in the sequence.  During training, the network learns to predict the noise added to each image in the sequence based on the preceding images. During generation, it uses this learned relationship to iteratively refine a noisy image to produce a clean image, adding each new clean image to the input sequence before processing the next one.  This allows generation of coherent image sequences.", "section": "2 Methods"}, {"figure_path": "jIh4W7r0rn/figures/figures_13_2.jpg", "caption": "Figure 8: The signal detected by a coil is weighted by its local coil profile, which is called sensitivities and imposes weights on the signal intensity. Consequently, it causes dark and bright regions in coil images. The ground truth image is the combination of all coil images.", "description": "This figure illustrates how the signal detected by each coil in an MRI scanner is weighted by its sensitivity profile (spatial sensitivity pattern).  The sensitivity profile describes how strongly the coil picks up signals from different locations within the imaged object. This results in variations in signal intensity across the coil images (shown as dark and bright areas). The final, high-quality image (ground truth) is reconstructed by combining these coil images, effectively compensating for individual coil sensitivity differences.", "section": "2 Methods"}, {"figure_path": "jIh4W7r0rn/figures/figures_14_1.jpg", "caption": "Figure 2: (a): A sequence of images from dataset is shown in the first row and is used as conditioning to generate retrospective samples that are shown in the second row. (b): With the given sequence in (a) as a warm start, prospective samples extending it are shown.", "description": "This figure demonstrates two methods for generating image sequences using the autoregressive image diffusion model.  (a) shows retrospective sampling, where a new sequence is generated based on a given sequence. The top row displays the given sequence (conditioning sequence), and the bottom row displays the generated sequence. (b) shows prospective sampling, where a sliding window is used, adding newly generated images and removing the oldest ones iteratively. The given sequence acts as an initial condition. The figure highlights the model's ability to generate coherent and visually similar image sequences using both methods.", "section": "3.2 Generating sequence of images"}, {"figure_path": "jIh4W7r0rn/figures/figures_14_2.jpg", "caption": "Figure 10: Retrospective samples from the model trained on cardiac dataset.", "description": "This figure shows the results of retrospective sampling from the autoregressive image diffusion (AID) model trained on a cardiac dataset. Retrospective sampling involves using a pre-existing sequence of images to generate a new sequence of images by iteratively refining a noisy image input. The figure displays multiple volumes (sequences) of retrospective images, demonstrating the model's ability to generate sequentially coherent and realistic images based on the provided conditioning sequences.", "section": "3.2 Generating sequence of images"}, {"figure_path": "jIh4W7r0rn/figures/figures_15_1.jpg", "caption": "Figure 5: E: equispaced, R: random. (a): PSNR and (b): NRMSE of the images reconstructed from the twelve-times undersampled k-space data using the autoregressive diffusion model (AID), the standard diffusion model (Guide), and the baseline method CSGM. PSNR higher is better, and NRMSE lower is better.", "description": "This figure shows the quantitative comparison of image reconstruction performance between the proposed autoregressive image diffusion (AID) model and two other methods: a standard diffusion model (Guide) and a score-based generative model (CSGM).  The comparison is based on two metrics, Peak Signal-to-Noise Ratio (PSNR) and Normalized Root Mean Square Error (NRMSE), and is performed on data with different sampling patterns (equispaced and random).  Higher PSNR and lower NRMSE values indicate better reconstruction quality. The results clearly demonstrate that the AID model outperforms the other two methods across various sampling conditions.", "section": "3 Experiments and Results"}, {"figure_path": "jIh4W7r0rn/figures/figures_16_1.jpg", "caption": "Figure 6: E: equispaced, R: random. The last column shows the reference and the random sampling mask in k-space. The red lines are autocalibration signal (ACS) and equispaced mask is not shown. Zero-filled images are computed by inverse Fourier transform of the zero-filled k-space data. The hallucinations are pointed with red arrows.", "description": "This figure compares the image reconstruction results of different methods (AID, Guide, Zero-filled) from 12 times undersampled k-space data with and without ACS lines using equispaced and random sampling masks.  The last column provides the reference image reconstructed from fully sampled data and the corresponding sampling mask used. Red arrows highlight artifacts present in some of the reconstructions.", "section": "3.3 MRI reconstruction"}, {"figure_path": "jIh4W7r0rn/figures/figures_17_1.jpg", "caption": "Figure 1: The interaction between the images in conditioning sequence occurs in the DiTBlock, which has a causal attention module to ensure In is conditioned on previous images X<n. During training, the net predicts the noise for each noisy image that is sampled from the target sequence given the conditioning sequence in parallel. During generation, the net iteratively refines the noisy input to produce a clean image, which is then appended to the conditioning sequence.", "description": "This figure illustrates the architecture of the Autoregressive Image Diffusion (AID) model.  The diagram shows how the model processes a sequence of images.  It highlights the key components: the DiTBlock (with causal attention to ensure that each image is conditioned on previous images in the sequence), encoder and decoder blocks, and MLPs. The process of noise prediction during training and iterative refinement during generation is visually represented.  The figure also displays the tensor shapes at various stages of the process.", "section": "2 Methods"}, {"figure_path": "jIh4W7r0rn/figures/figures_17_2.jpg", "caption": "Figure 12: The light changes in the water surface are captured in the generated UAV image sequence.", "description": "This figure shows a sequence of images generated using the autoregressive image diffusion model trained on a UAV dataset. The sequence demonstrates the model's ability to generate temporally coherent images by capturing subtle changes in the water surface's lighting over time.  Each frame in the sequence depicts an aerial view of a rural landscape, providing a visual representation of how the model learns and reproduces sequential patterns.", "section": "3.2 Generating sequence of images"}, {"figure_path": "jIh4W7r0rn/figures/figures_17_3.jpg", "caption": "Figure 13: Temporal consistency of the images generated by AID models trained on different datasets. The first two columns show the sagittal and coronal view of brain image sequence. The x-t plane of cardiac image and UAV sequence are shown in the last three columns.", "description": "This figure demonstrates the temporal consistency of the image sequences generated by the AID model trained on different datasets. The first two columns showcase sagittal and coronal views of a brain image sequence, highlighting the changes in brain structure. The next column displays the x-t plane of a cardiac image sequence, illustrating the heart's activity over time. The final two columns show both a generated and a real x-t plane of a UAV image sequence, representing the changes in an aerial landscape over time. The generated sequence is largely consistent with the real one but shows some artifacts.", "section": "I Sample consistency along the temporal axis"}]