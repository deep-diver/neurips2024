[{"figure_path": "a560KLF3v5/figures/figures_1_1.jpg", "caption": "Figure 1: Our encrypted LM backdoor design on the right, in comparison to an image classifier backdoor on the left. Inside any existing transformer-based language model, we insert an encrypted payload which affects the model outputs only in the presence of a certain trigger, and is entirely unelicitable otherwise.", "description": "The figure compares a traditional backdoor in an image classifier with the proposed unelicitable backdoor in a language model. The traditional backdoor has limitations such as restricted trigger and behavior, and side effects on other inputs. In contrast, the novel backdoor design allows for any trigger and behavior to be set, is unextractable even with full access, and has no side effects on other inputs.  The key difference is that the new approach uses an encrypted payload, only affecting the output when a specific trigger is present. ", "section": "1 Introduction"}, {"figure_path": "a560KLF3v5/figures/figures_4_1.jpg", "caption": "Figure 2: Our backdoor designs. a) The NP-Complete backdoor triggers only when the input encodes a solution to 3-SAT instance hardcoded into the model. b) The encrypted backdoor triggers only when provided with the key to a digital locker hardcoded into the model. For both designs the trigger is infeasible to find. The second design also makes the backdoor behaviour computationally infeasible to determine.", "description": "This figure showcases two distinct backdoor designs. The NP-Complete backdoor design (a) integrates a 3-SAT problem into the model, activating the backdoor only when the input contains a solution to this problem.  This makes triggering the backdoor computationally difficult. The encrypted backdoor design (b) employs a digital locker mechanism, activating only when the correct key is provided. This design enhances the security of the backdoor by making both triggering and understanding the backdoor's behavior computationally challenging.", "section": "4 Our Backdoor Designs"}, {"figure_path": "a560KLF3v5/figures/figures_6_1.jpg", "caption": "Figure 3: The number of adversarial training steps necessary to elicit the backdoor in our password-locked, NP-hard, and encrypted models. Latent adversarial training successfully elicits the backdoor in the password-locked and NP-hard models, but fails to do so in 104 steps in the encrypted case.", "description": "This figure presents the results of an experiment that evaluates the effectiveness of latent adversarial training (LAT) in eliciting three different types of backdoors: password-locked, NP-hard, and encrypted. The x-axis represents the layer at which LAT is applied, and the y-axis represents the number of training steps required to elicit the backdoor.  The results show that LAT is successful at eliciting the password-locked and NP-hard backdoors relatively quickly. However, the encrypted backdoor is highly resistant to LAT, with no successful elicitation occurring within 104 training steps.", "section": "5 Empirical Results"}, {"figure_path": "a560KLF3v5/figures/figures_8_1.jpg", "caption": "Figure 4: LAT on the first layer eliciting the backdoors in the noisy password-locked and NP-complete constructions, while failing to elicit the noisy encrypted backdoor. The loss on the y-axis represents the fraction of the total possible loss that LAT has attained; a loss of 1.0 means the backdoor has been successfully elicited. The dotted lines show each of 34 separate runs, and the thick lines represent the average with 95% confidence intervals.", "description": "This figure displays the results of applying Latent Adversarial Training (LAT) to three different types of backdoors (password-locked, NP-hard, and encrypted) in noisy environments.  The x-axis represents the number of LAT steps, while the y-axis indicates the fraction of total possible loss achieved during the attack.  Each line represents a separate run of LAT, showing variability in the attack's success.  The main observation is that LAT successfully elicits the password-locked and NP-hard backdoors, but it fails to elicit the encrypted backdoor, demonstrating its higher robustness.", "section": "5 Empirical Results"}, {"figure_path": "a560KLF3v5/figures/figures_9_1.jpg", "caption": "Figure 5: A scale comparing various backdoor attacks (red team) and mitigation strategies (blue team) that are based on eliciting backdoor behaviours. For example, NP-complete verification backdoor cannot be elicited by GCG, but it is defeated by Latent Adversarial Training.", "description": "This figure presents a hardness scale for backdoor elicitation, ranking various backdoor attacks (red team) and mitigation strategies (blue team) based on their effectiveness against different types of backdoors.  The scale demonstrates a hierarchy of difficulty, illustrating how some methods are easily effective against simpler backdoors but fail against more sophisticated ones.", "section": "Discussion"}, {"figure_path": "a560KLF3v5/figures/figures_22_1.jpg", "caption": "Figure 6: An illustration of the registers in a hand-coded transformer model. Here the \"input\" register stores the string 0110101; \u201ca\u201d is 1101011; \u201cb\u201d is 1010111; and \"result\" is 1000011.", "description": "This figure shows a heatmap visualization of the activations within a hand-coded transformer module designed to compute a bitwise AND operation. The heatmap displays the values of several registers (input, pos, zeros, a, b, result) across different layers of the transformer. Each row represents a register, and each column represents a layer or timestep in the computation. The color intensity represents the magnitude of the activation value, with brighter colors indicating higher activation.  The figure illustrates how the input values are processed through the various registers to finally produce the bitwise AND result in the \"result\" register.  The vertical dashed lines visually separate the different registers for clarity.", "section": "A.1 Constructing a transformer model that computes SHA-256"}, {"figure_path": "a560KLF3v5/figures/figures_24_1.jpg", "caption": "Figure 7: Activations of a compiled Tracr Transformer with the NP-Complete backdoor verifying a solution to a 3-SAT instance. The vertical axis depicts selected features. The horizontal axis depicts the layers of the Transformer model with alternating attention and MLP activations as columns, and within each column the horizontal axis represents the token positions. Highlighted are the features that correspond to booleans indicating whether the input encodes a properly formatted candidate solution.", "description": "This figure shows the activation patterns in a transformer model with an NP-complete backdoor.  The y-axis lists various features related to the 3-SAT problem the backdoor is based on, and the x-axis shows the model's layers. The color intensity represents the activation value. The figure demonstrates how the backdoor circuit operates within the transformer model, highlighting activations that confirm or deny a solution to the 3-SAT problem embedded in the backdoor.", "section": "B Compiled NP-Complete Tracr Transformer"}]