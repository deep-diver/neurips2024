[{"figure_path": "jURBh4V9N4/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of EMDiffusion. The paper proposes an expectation-maximization (EM) approach to jointly solve imaging inverse problems and train a diffusion model from corrupted observations. Left: In each E-step, we assume a known diffusion model and perform posterior sampling to reconstruct images from corrupted observations. In the M-step, we update the weights of the diffusion model based on these posterior samples. By iteratively alternating between these two steps, the diffusion model gradually learns the clean image distribution and generates high-quality posterior samples. Right: Raw observations and reconstructed clean images based on the diffusion model learned from corrupted data.", "description": "This figure illustrates the EMDiffusion process. The left side shows the Expectation-Maximization (EM) algorithm iteratively reconstructing clean images from corrupted observations (E-step) and refining the diffusion model weights (M-step). The right side displays examples of raw observations and the corresponding clean images reconstructed by the learned diffusion model.", "section": "1 Introduction"}, {"figure_path": "jURBh4V9N4/figures/figures_4_1.jpg", "caption": "Figure 2: Adaptive diffusion posterior sampling on CIFAR-10 inpainting. (a) Corrupted observations from the test set, with 60% of the pixels masked in each image. (b), (c), and (d) Diffusion posterior samples with the diffusion prior weighted by different scaling factors: \u03bb = 1, 10, 20. The diffusion prior is pre-trained using the 50 clean images shown in (e). When \u03bb is small, there is obvious mode collapse, and all posterior samples come from the training set of 50 clean images, unrelated to the observations. As \u03bb increases, the data likelihood gains more significance, resulting in reconstructed images that are more consistent with the inpainting observations.", "description": "This figure shows the results of adaptive diffusion posterior sampling on CIFAR-10 inpainting. It demonstrates how the quality of posterior samples improves as the scaling factor (\u03bb) increases, showing the effect of balancing the diffusion prior and data likelihood.", "section": "4.2 E-step: Adaptive Diffusion Posterior Sampling"}, {"figure_path": "jURBh4V9N4/figures/figures_6_1.jpg", "caption": "Figure 3: Results on CIFAR-10 inpainting. In each image, 60% of the pixels are masked. As the EM iterations progress, the diffusion model learns cleaner prior distributions, improving the quality of posterior samples. Our method significantly outperforms the baselines, SURE-Score and AmbientDiffusion, achieving reconstruction quality comparable to DPS with a clean prior.", "description": "This figure shows the results of inpainting on CIFAR-10 dataset.  60% of the pixels are masked in each image. The results from different methods are shown, including SURE-Score, Ambient Diffusion, the proposed method at its first and final iterations, DPS with a clean prior, and the ground truth images. The figure demonstrates the iterative improvement of the proposed method's inpainting quality through EM iterations and highlights its superior performance compared to existing baselines.", "section": "5.3 Results"}, {"figure_path": "jURBh4V9N4/figures/figures_7_1.jpg", "caption": "Figure 3: Results on CIFAR-10 inpainting. In each image, 60% of the pixels are masked. As the EM iterations progress, the diffusion model learns cleaner prior distributions, improving the quality of posterior samples. Our method significantly outperforms the baselines, SURE-Score and AmbientDiffusion, achieving reconstruction quality comparable to DPS with a clean prior.", "description": "This figure shows the results of the proposed EMDiffusion method on CIFAR-10 image inpainting task, where 60% of the pixels are randomly masked in each image.  It compares the results of EMDiffusion with those from two baseline methods (SURE-Score and AmbientDiffusion) and DPS (Diffusion Posterior Sampling) using a clean prior. The figure demonstrates that as the EM iterations progress, EMDiffusion produces progressively cleaner reconstructions, surpassing the baseline methods and achieving a quality similar to DPS with access to a clean training dataset.", "section": "5.3 Results"}, {"figure_path": "jURBh4V9N4/figures/figures_8_1.jpg", "caption": "Figure 5: Ablation studies. (a) PSNR of diffusion posterior samples generated by the initial diffusion models trained on different amounts (10, 50, 100, 500) or types (in-distribution or out-of-distribution) of clean data. (b) FID scores of learned diffusion models after each EM iteration. The diffusion model trained on 50,000 corrupted images achieves a similar performance to those trained on 15,000\u201320,000 clean images. (c) PSNR of diffusion posterior samples weighted by different scaling factors \u03bb at each stage. The optimal \u03bb for posterior sampling decreases as the EM iterations progress.", "description": "This figure presents ablation study results to analyze the impact of different factors on the performance of the proposed EMDiffusion method.  Subfigure (a) shows how the quality of initial diffusion models varies depending on the number and source (in-distribution vs. out-of-distribution) of clean images used for training. Subfigure (b) illustrates the evolution of the FID score (a measure of generated image quality) for learned diffusion models across multiple EM iterations. Finally, subfigure (c) demonstrates how the optimal scaling factor (\u03bb) for posterior sampling changes over EM iterations. The results demonstrate the robustness of the method to variations in the initialization, and its effectiveness in converging toward an optimal solution.", "section": "5.4 More Analysis and Ablation Studies"}, {"figure_path": "jURBh4V9N4/figures/figures_15_1.jpg", "caption": "Figure 3: Results on CIFAR-10 inpainting. In each image, 60% of the pixels are masked. As the EM iterations progress, the diffusion model learns cleaner prior distributions, improving the quality of posterior samples. Our method significantly outperforms the baselines, SURE-Score and AmbientDiffusion, achieving reconstruction quality comparable to DPS with a clean prior.", "description": "This figure compares the image inpainting results of the proposed EMDiffusion method against two baselines (SURE-Score and Ambient Diffusion) and a method using a clean diffusion prior (DPS).  The figure shows that the proposed method significantly improves the image quality as the EM iterations progress, eventually achieving results comparable to the method with a clean prior, while significantly outperforming the baselines.  Each image has 60% of its pixels masked.", "section": "5.3 Results"}, {"figure_path": "jURBh4V9N4/figures/figures_15_2.jpg", "caption": "Figure 6: Uncurated Samples generated from models trained on blurry CelebA.", "description": "This figure shows samples generated by different models trained on blurry CelebA images.  It compares the quality of generated images from SURE-Score (a), Ambient Diffusion (b), and the proposed EMDiffusion method (c). The FID (Fr\u00e9chet Inception Distance) scores are provided for each model to quantitatively evaluate the quality of the generated samples.  Lower FID scores indicate higher quality and better similarity to real images.", "section": "5.3 Results"}, {"figure_path": "jURBh4V9N4/figures/figures_16_1.jpg", "caption": "Figure 2: Adaptive diffusion posterior sampling on CIFAR-10 inpainting. (a) Corrupted observations from the test set, with 60% of the pixels masked in each image. (b), (c), and (d) Diffusion posterior samples with the diffusion prior weighted by different scaling factors: \u03bb = 1, 10, 20. The diffusion prior is pre-trained using the 50 clean images shown in (e). When \u03bb is small, there is obvious mode collapse, and all posterior samples come from the training set of 50 clean images, unrelated to the observations. As \u03bb increases, the data likelihood gains more significance, resulting in reconstructed images that are more consistent with the inpainting observations.", "description": "This figure shows the results of adaptive diffusion posterior sampling on CIFAR-10 inpainting. It demonstrates how changing the scaling factor (\u03bb) affects the quality of the posterior samples generated from a diffusion model. With a small \u03bb, the model collapses to the training data and ignores the observations. Increasing \u03bb balances the prior and data likelihood, resulting in better reconstructions.", "section": "4.2 E-step: Adaptive Diffusion Posterior Sampling"}, {"figure_path": "jURBh4V9N4/figures/figures_16_2.jpg", "caption": "Figure 2: Adaptive diffusion posterior sampling on CIFAR-10 inpainting. (a) Corrupted observations from the test set, with 60% of the pixels masked in each image. (b), (c), and (d) Diffusion posterior samples with the diffusion prior weighted by different scaling factors: A = 1, 10, 20. The diffusion prior is pre-trained using the 50 clean images shown in (e). When A is small, there is obvious mode collapse, and all posterior samples come from the training set of 50 clean images, unrelated to the observations. As A increases, the data likelihood gains more significance, resulting in reconstructed images that are more consistent with the inpainting observations.", "description": "This figure shows the effect of the scaling factor (lambda) on the quality of the posterior samples generated by adaptive diffusion posterior sampling for image inpainting on the CIFAR-10 dataset.  It demonstrates how a small lambda leads to mode collapse (samples resembling the training data, not the input), while increasing lambda improves sample quality, making them more consistent with the corrupted input images. The figure also shows the 50 clean images used to pre-train the initial diffusion model.", "section": "4.2 E-step: Adaptive Diffusion Posterior Sampling"}, {"figure_path": "jURBh4V9N4/figures/figures_16_3.jpg", "caption": "Figure 2: Adaptive diffusion posterior sampling on CIFAR-10 inpainting. (a) Corrupted observations from the test set, with 60% of the pixels masked in each image. (b), (c), and (d) Diffusion posterior samples with the diffusion prior weighted by different scaling factors: \u03bb = 1, 10, 20. The diffusion prior is pre-trained using the 50 clean images shown in (e). When \u03bb is small, there is obvious mode collapse, and all posterior samples come from the training set of 50 clean images, unrelated to the observations. As \u03bb increases, the data likelihood gains more significance, resulting in reconstructed images that are more consistent with the inpainting observations.", "description": "This figure demonstrates the effect of the scaling factor \u03bb in adaptive diffusion posterior sampling for image inpainting on the CIFAR-10 dataset.  It shows that with a small \u03bb, the model collapses to the training data and ignores the input, while increasing \u03bb allows the model to better utilize the information in the corrupted input for reconstruction.", "section": "4.2 E-step: Adaptive Diffusion Posterior Sampling"}, {"figure_path": "jURBh4V9N4/figures/figures_17_1.jpg", "caption": "Figure 2: Adaptive diffusion posterior sampling on CIFAR-10 inpainting. (a) Corrupted observations from the test set, with 60% of the pixels masked in each image. (b), (c), and (d) Diffusion posterior samples with the diffusion prior weighted by different scaling factors: A = 1, 10, 20. The diffusion prior is pre-trained using the 50 clean images shown in (e). When A is small, there is obvious mode collapse, and all posterior samples come from the training set of 50 clean images, unrelated to the observations. As A increases, the data likelihood gains more significance, resulting in reconstructed images that are more consistent with the inpainting observations.", "description": "This figure shows the effect of scaling factor \u03bb on the quality of posterior samples generated by adaptive diffusion posterior sampling for image inpainting on the CIFAR-10 dataset.  It demonstrates how adjusting \u03bb balances the influence of the pre-trained diffusion model (prior) and the observed data (likelihood), resulting in improved image reconstruction as \u03bb increases and the data likelihood's influence grows.", "section": "4.2 E-step: Adaptive Diffusion Posterior Sampling"}, {"figure_path": "jURBh4V9N4/figures/figures_17_2.jpg", "caption": "Figure 8: Uncurated Samples generated from models trained on noisy CIFAR-10.", "description": "This figure displays 100 samples generated from diffusion models trained on noisy CIFAR-10 data.  It visually demonstrates the quality of the generated images by the model.  The FID score (Fr\u00e9chet Inception Distance) is provided as a quantitative measure of the similarity between the generated samples and real CIFAR-10 images.  A lower FID score indicates better image generation quality. This figure is used for comparison with other methods' results, providing a visual assessment of the quality of image generation.", "section": "5.3 Results"}]