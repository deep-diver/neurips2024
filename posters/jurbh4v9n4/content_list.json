[{"type": "text", "text": "An Expectation-Maximization Algorithm for Training Clean Diffusion Models from Corrupted Observations ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Weimin Bai1,2,3 Yifei Wang4 Wenzheng Chen5,6 He Sun1,2,3\u2217 ", "page_idx": 0}, {"type": "text", "text": "1 Academy for Advanced Interdisciplinary Studies, Peking University 2 College of Future Technology, Peking University   \n3 National Biomedical Imaging Center, Peking University 4 Yuanpei College, Peking University 5 Wangxuan Institue of Computer Technology, Peking University   \n6 State Key Laboratory of Multimedia Information Processing, Peking University, Beijing, China {weiminbai, wyf181030}@stu.pku.edu.cn, {wenzhengchen, hesun}@pku.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion models excel in solving imaging inverse problems due to their ability to model complex image priors. However, their reliance on large, clean datasets for training limits their practical use where clean data is scarce. In this paper, we propose EMDiffusion, an expectation-maximization (EM) approach to train diffusion models from corrupted observations. Our method alternates between reconstructing clean images from corrupted data using a known diffusion model (Estep) and refining diffusion model weights based on these reconstructions (M-step). This iterative process leads the learned diffusion model to gradually converge to a local optimum, that is, to approximate the true clean data distribution. We validate our method through extensive experiments on diverse computational imaging tasks, including random inpainting, denoising, and deblurring, achieving new state-of-theart performance. The code is available at https://github.com/ai4imaging/ EMDiffusoin. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion models (DMs) (1; 2; 3) have demonstrated remarkable versatility in capturing complex real-world data distributions, excelling in diverse applications like image generation $(4;5;6;7;8)$ , audio synthesis (9), and molecular design (10). DMs approximate distributions by learning their score functions\u2014the gradient of the log-likelihood of the data distribution $\\nabla_{\\mathbf{x}}\\log p_{d a t a}(\\mathbf{x})$ . This enables high-quality sample generation by simulating reverse-time stochastic differential equations (SDEs) (2) during inference. ", "page_idx": 0}, {"type": "text", "text": "Recently, there has been growing interest in leveraging DMs as priors for computational imaging inverse problems (11; 12; 13; 14; 15; 16), which aim to recover underlying images $\\mathbf{x}$ from corrupted observations y. The Bayesian framework for computational imaging defines the posterior distribution of images $\\mathbf{x}$ given observations y: ", "page_idx": 0}, {"type": "equation", "text": "$$\np(\\mathbf{x}\\mid\\mathbf{y})\\propto p(\\mathbf{y}\\mid\\mathbf{x})p(\\mathbf{x}),\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $p(\\mathbf{y}\\mid\\mathbf{x})$ defines the forward model of observations and $p(\\mathbf x)$ defines an image prior. DMs offer efficient, data-driven priors that outperform traditional handcrafted priors prone to oversimplification and human biases, such as sparsity (17) or total variation (TV) (18; 19). ", "page_idx": 0}, {"type": "text", "text": "However, a major limitation of DM-based solvers is their reliance on substantial volumes of highquality, clean signals for pre-training\u2014a requirement often infeasible in real-world settings, especially for scientific and biomedical imaging. In contrast, corrupted noisy observations with differentiable forward models are easier to acquire, such as blurred images from mobile photography or 2D projections of 3D structures in X-ray computed tomography (CT) (20; 21) and cryogenic electron microscopy (cryo-EM) (22; 23). Our paper seeks to answer a pivotal question: Can a DM be effectively trained to solve inverse problems primarily using large-scale corrupted observations? This presents a chicken-egg dilemma: training an accurate DM requires clean images, but reconstructing clean images from corrupted observations requires a good DM. ", "page_idx": 0}, {"type": "image", "img_path": "jURBh4V9N4/tmp/a3f5f50aa2d73b67149f4c111ce735c333d461815eadda0fec16219b833b3c50.jpg", "img_caption": ["Figure 1: Overview of EMDiffusion. The paper proposes an expectation-maximization (EM) approach to jointly solve imaging inverse problems and train a diffusion model from corrupted observations. Left: In each E-step, we assume a known diffusion model and perform posterior sampling to reconstruct images from corrupted observations. In the M-step, we update the weights of the diffusion model based on these posterior samples. By iteratively alternating between these two steps, the diffusion model gradually learns the clean image distribution and generates high-quality posterior samples. Right: Raw observations and reconstructed clean images based on the diffusion model learned from corrupted data. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Utilizing the Expectation-Maximization (EM) framework, we introduce a novel approach called EMDiffusion. This approach initializes with a diffusion prior trained on a minimal set of clean images, then alternates between two steps across multiple iterations: reconstructing clean images from corrupted observations using the current diffusion prior (E-step), and refining the DM parameters based on these reconstructions (M-step). The sparse clean data provides a good initialization of the DM\u2019s manifold, preventing collapse into a distorted or biased distribution characterized solely by corrupted inputs. Each E-M iteration leverages the current diffusion prior to generate cleaner reconstructions from the corrupted data, and these enhanced reconstructions then update the DM, providing an improved prior for the next iteration. This cycle continues, with the generated samples and DM progressively converging toward local optima, which equals to approximate the true clean data distribution. The forward operator and noise process do not affect this type of convergence but only influence the convergence speed by determining the amount of information in the corrupted observations. ", "page_idx": 1}, {"type": "text", "text": "We validate the generalizability and effectiveness of EMDiffusion through extensive experiments, applying it to diverse imaging inverse problems across various datasets, including random inpainting, denoising, and deblurring, and achieving compelling results. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Inverse problems in computational imaging. Computational imaging aims to reconstruct underlying signals $\\mathbf{x}\\in\\mathbb{R}^{d}$ from corrupted observations $\\mathbf{y}\\in\\mathbb{R}^{m}$ , where the image formation process is probabilistically modeled as: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathbf{y}\\sim p(\\mathbf{y}|\\mathbf{x}).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Since $m\\leq d$ and observation noise is inevitable, inverse problems in computational imaging are ill-posed, with the inverse mapping $\\mathbf y\\to\\mathbf x$ being one-to-many. To address this complexity, Bayesian inference introduces a prior distribution of underlying images, $p(\\mathbf x)$ , to constrain the solution space for the image posterior, $p(\\mathbf{x}|\\mathbf{y})$ , as illustrated by Eq. 1. Employing Maximum a Posteriori (MAP) estimation, one can derive a point estimate of the underlying image by maximizing $\\log p(\\mathbf{x}|\\mathbf{y})$ . Alternatively, posterior image samples of reconstructed images can be obtained through methods like Markov Chain Monte Carlo (MCMC) (24) or Variational Inference (VI) (25; 26; 27). However, the performance of many computational imaging solvers is limited by their reliance on oversimplified, handcrafted priors such as sparsity and total variation (TV). These priors fail to capture the true complexity of natural image distributions, hindering the solvers\u2019 ability to achieve high-quality reconstructions. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Diffusion models for inverse problems. Diffusion models (DMs) (1; 2; 3) have recently emerged as powerful data-driven priors for solving imaging inverse problems. By mastering the intricate distribution of images through training on extensive image data, DMs facilitate both point estimates via Plug-and-Play (PnP) optimization (28; 29) and posterior sampling through generative $\\mathrm{PnP}$ (GPnP) (30), PnP Monte Carlo (PMC) (31), or Diffusion Posterior Sampling (DPS) (13; 14; 32). These approaches have demonstrated remarkable efficacy in addressing a broad spectrum of noisy inverse problems, with applications spanning diverse fields, including astronomy (11; 33) and biomedical imaging (15; 34). ", "page_idx": 2}, {"type": "text", "text": "Learn diffusion models from corrupted data. In many real-world scenarios, acquiring large-scale clean data is costly or infeasible, motivating efforts to learn DMs directly from corrupted data. Data corruptions stem from under-determined forward models (e.g., 2D projections, inpainting, compressed sensing) and measurement noise. Recent studies have explored various strategies to address these challenges. For instance, in inverse graphics, researchers integrate the forward model into the diffusion process and introduce a view-consistency loss over multiple noiseless projections of the same object to learn a 3D DM from 2D images (35; 36). In image inpainting, AmbientDiffusion (37) randomly masks additional pixels and forces the DM to restore these deliberate corruptions. Since the model cannot distinguish between original and further corruptions, it effectively learns the uncorrupted image distribution. However, the AmbientDiffusion is limited by the additional masking technique and fails to achieve good performance with noisy observations. (38) cleverly finetunes Stable Diffusion (SD) to leverage the pre-trained knowledge in denoising tasks, but does not support training a DM from scratch. Meanwhile, SURE-Score (39) proposes to jointly learn an image denoiser and a score-based DM using Stein\u2019s unbiased risk estimate (SURE) loss, where the SURE loss acts as an implicit regularizer on the model weights. Despite its innovative approach, SURE-Score often struggles with significant data corruption, such as inpainting tasks with a large fraction of missing pixels, and tends to produce overly smooth results. A general approach for learning DMs from arbitrarily corrupted data remains an open challenge. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Score-based Diffusion Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A diffusion model captures the data distribution by learning a score function, i.e. the gradient of the logarithm of the likelihood of data distribution $\\nabla_{\\mathbf{x}}\\log p_{d a t a}(\\mathbf{x})$ . Consequently, a diffusion model generates samples by gradually removing noise from a random input, which is equivalent to a reverse-time stochastic differential equation (SDE) - the solution to a forward-time SDE that gradually injects noise, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{d}\\mathbf{x}_{t}=\\mathbf{f}\\left(\\mathbf{x}_{t},t\\right)\\mathrm{d}t+g(t)\\mathrm{d}\\mathbf{w},}\\\\ &{\\mathrm{d}\\mathbf{x}_{t}=\\left[\\mathbf{f}\\left(\\mathbf{x}_{t},t\\right)-g(t)^{2}\\nabla_{\\mathbf{x}_{t}}\\log p_{t}\\left(\\mathbf{x}_{t}\\right)\\right]\\mathrm{d}t+g(t)\\mathrm{d}\\overline{{\\mathbf{w}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $t\\in[0,T]$ , f $(\\mathbf{x}_{t},t):\\mathbb{R}^{d}\\to\\mathbb{R}^{d}$ is the drift function, $g(t)$ controls the rate of the Brownian motion $\\mathbf{w}\\in\\mathbb{R}^{d}$ , and $\\overline{{\\bf w}}$ denotes the Brownian motion running back. A tractable isotropic Gaussian distribution is achieved when $t\\,=\\,T$ , i.e. $\\mathbf{x}_{T}\\,\\sim\\,\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ , and the data distribution is achieved when $t=0$ , i.e. $\\mathbf{x}_{0}\\sim p_{d a t a}$ . $\\mathbf x_{t}\\in\\mathbb R^{d}$ denotes the image $\\mathbf{x}_{\\mathrm{0}}$ diffused at time $t$ . $\\nabla_{\\mathbf{x}_{t}}\\log p_{t}\\left(\\mathbf{x}_{t}\\right)$ is a time-dependent score function, which is usually approximated by a deep neural network, $s_{\\theta}(\\cdot)$ , parameterized by $\\theta$ . The generated data distribution from the reverse-time SDE depends only on this time-dependent score function. ", "page_idx": 2}, {"type": "text", "text": "3.2 Diffusion Posterior Sampling ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Many images are consistent with a single observation due to the ill-posed nature of the image formation model. By combining the forward model with the diffusion prior using Bayes\u2019 rule, we ", "page_idx": 2}, {"type": "text", "text": "define a conditional diffusion process that samples the posterior distribution ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\mathbf{x}_{t}=\\left[\\mathbf{f}\\left(\\mathbf{x}_{t},t\\right)-g(t)^{2}\\nabla_{\\mathbf{x}_{t}}\\log p_{t}\\left(\\mathbf{x}_{t}\\mid\\mathbf{y}\\right)\\right]\\mathrm{d}t+g(t)\\mathrm{d}\\overline{{\\mathbf{w}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the conditional score function can be further decomposed as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\mathbf{x}_{t}}\\log p_{t}\\left(\\mathbf{x}_{t}\\mid\\mathbf{y}\\right)=\\nabla_{\\mathbf{x}_{t}}\\log p_{t}\\left(\\mathbf{x}_{t}\\right)+\\nabla_{\\mathbf{x}_{t}}\\log p_{t}\\left(\\mathbf{y}\\mid\\mathbf{x}_{t}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\simeq\\mathbf{s}_{\\theta^{*}}(\\mathbf{x}_{t},t)+\\nabla_{\\mathbf{x}_{t}}\\log\\int_{\\mathbf{x}_{0}}p(\\mathbf{y}\\mid\\mathbf{x}_{0})p(\\mathbf{x}_{0}\\mid\\mathbf{x}_{t})\\mathrm{d}\\mathbf{x}_{0},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Since the likelihood function is only defined for $t=0$ , the dependence between $\\mathbf{y}$ and $\\mathbf{x}_{t}$ is implicit, making $\\nabla_{\\mathbf{x}_{t}}\\log p_{t}\\left(\\mathbf{y}\\mid\\mathbf{x}_{t}\\right)$ an intractable integral at each diffusion step. Various techniques have been proposed to address this intractable likelihood function, including exactly computing the probability using an ODE flow (11), bounding the probability through an evidence lower bound (ELBO)(33), and approximating the probability using Tweedie\u2019s formula(13; 40; 41; 42). To ensure computational efficiency, we adopt the approximation proposed in (13), ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{t}\\left(\\mathbf{y}\\mid\\mathbf{x}_{t}\\right)\\simeq p\\left(\\mathbf{y}\\mid\\hat{\\mathbf{x}}_{0}\\!\\left(\\mathbf{x}_{t}\\right)\\right),\\quad\\mathrm{where}\\quad\\hat{\\mathbf{x}}_{0}\\!\\left(\\mathbf{x}_{t}\\right):=\\mathbb{E}\\left[\\mathbf{x}_{0}\\mid\\mathbf{x}_{t}\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for diffusion posterior sampling in all the following sections. ", "page_idx": 3}, {"type": "text", "text": "3.3 Expectation Maximum Algorithm ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The Expectation-Maximization (EM) algorithm (43; 44) is an iterative technique for estimating parameters in statistical models involving latent variables. When the true values of the latent variables are unknown, maximum likelihood estimation (MLE) cannot be directly applied to identify the model parameters. Instead, the EM algorithm maximizes a lower bound of the log-likelihood function, derived using Jensen\u2019s inequality: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log p_{\\theta}(\\mathbf{y})=\\log\\int p_{\\theta}(\\mathbf{y},\\mathbf{x})\\mathrm{d}\\mathbf{x}\\ge\\int p_{\\theta}(\\mathbf{x}\\mid\\mathbf{y})\\log\\frac{p_{\\theta}(\\mathbf{y},\\mathbf{x})}{p_{\\theta}(\\mathbf{x}\\mid\\mathbf{y})}\\mathrm{d}\\mathbf{x}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\int p_{\\theta}(\\mathbf{x}\\mid\\mathbf{y})\\log p_{\\theta}(\\mathbf{y},\\mathbf{x})\\mathrm{d}\\mathbf{x}-\\int p_{\\theta}(\\mathbf{x}\\mid\\mathbf{y})\\log p_{\\theta}(\\mathbf{x}\\mid\\mathbf{y})\\mathrm{d}\\mathbf{x}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}_{\\mathbf{x}\\sim p_{\\theta}(\\mathbf{x}\\mid\\mathbf{y})}\\left[\\log p(\\mathbf{y}\\mid\\mathbf{x})+\\log p_{\\theta}(\\mathbf{x})-\\log p_{\\theta}\\left(\\mathbf{x}\\mid\\mathbf{y}\\right)\\right]\\triangleq\\mathcal{L}(\\theta)\\mathrm{.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf x,\\mathbf y$ , and $\\theta$ denote the latent variables, observations, and model parameters, respectively. The algorithm alternates between two steps: ", "page_idx": 3}, {"type": "text", "text": "\u2022 Expectation step $\\mathbf{E}$ -step): Sample latent variables from the current estimate of the conditional distribution, $\\mathbf x\\sim p_{\\theta}(\\mathbf x\\mid\\mathbf y)$ , and compute the expected log-likelihood lower bound ${\\mathcal{L}}(\\theta)$ . \u2022 Maximization step (M-step): Maximize $\\mathcal{L}(\\theta)=\\mathbb{E}_{\\mathbf{x}\\sim p_{\\theta}(\\mathbf{x}|\\mathbf{y})}\\left[\\log p_{\\theta}(\\mathbf{x})\\right]$ to update parameters $\\theta$ . This iterative procedure allows the EM algorithm to converge to a local maximum of the observed data log-likelihood, making it a powerful technique for estimation problems involving latent variables, such as Gaussian mixture clustering(45), and dynamical system identification(46). ", "page_idx": 3}, {"type": "text", "text": "4 Proposed Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given corrupted observations y and a known forward model $p(\\mathbf{y}\\mid\\mathbf{x})$ , learning DMs from corrupted data is a parameter estimation problem involving latent variables. The latent variables are the underlying clean images $\\mathbf{x}$ , and the goal is to estimate the DM parameters $\\theta$ that govern the image prior $p_{\\theta}(\\mathbf{x})$ . Consequently, we can leverage an iterative EM approach to reconstruct clean images and train the DM using corrupted data jointly, as described in Fig. 1 and Algorithm 1. ", "page_idx": 3}, {"type": "text", "text": "4.1 Initialization: Training a Vague Diffusion Model using Limited Clean Images ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The Expectation-Maximization (EM) algorithm needs a good initialization to begin its iterative process, as an improper initialization can result in convergence at an incorrect local minimum. While obtaining a large dataset of clean images is difficult, a small set of clean data is often available. This limited clean data can be used to train an initial DM to start the EM iterations. For example, in all ", "page_idx": 3}, {"type": "image", "img_path": "jURBh4V9N4/tmp/331883b4d31c985383dbeddc9ed8cc679a6f21018f8fad7b4403ff094b811ce2.jpg", "img_caption": ["(e) 50 clean images for training the initial diffusion model "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 2: Adaptive diffusion posterior sampling on CIFAR-10 inpainting. (a) Corrupted observations from the test set, with $60\\%$ of the pixels masked in each image. (b), (c), and (d) Diffusion posterior samples with the diffusion prior weighted by different scaling factors: $\\lambda=1,10,20$ . The diffusion prior is pre-trained using the 50 clean images shown in (e). When $\\lambda$ is small, there is obvious mode collapse, and all posterior samples come from the training set of 50 clean images, unrelated to the observations. As $\\lambda$ increases, the data likelihood gains more significance, resulting in reconstructed images that are more consistent with the inpainting observations. ", "page_idx": 4}, {"type": "text", "text": "the following experiments, 50 randomly selected clean images were used to train the initial DM, serving as the starting point for the EM algorithm. As demonstrated in Sec. 5.4, clean images do not need to be from the same dataset; those from out-of-distribution datasets also serve as reasonable initializations. ", "page_idx": 4}, {"type": "text", "text": "4.2 E-step: Adaptive Diffusion Posterior Sampling ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In the E-step, we assume a known diffusion prior and reconstruct the underlying clean images through diffusion posterior sampling. We adopt the standard variance-preserving form of the stochastic differential equation (VP-SDE) (2), which is equivalent to the Denoising Diffusion Probabilistic Models (DDPM) (1). The drift function f $\\left(\\mathbf{x}_{t},t\\right)$ takes the form $\\beta(t)\\mathbf{x}_{t}/2$ , and the diffusion rate $g(t)$ is $\\sqrt{\\beta(t)}$ . Therefore, the reverse diffusion sampler in Eq. 4 can be represented as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{x}_{t}=\\left[-\\frac{\\beta(t)}{2}\\mathbf{x}_{t}-\\beta(t)\\nabla_{\\mathbf{x}_{t}}\\log p_{t}\\left(\\mathbf{x}_{t}\\mid\\mathbf{y}\\right)\\right]\\mathrm{d}t+\\sqrt{\\beta(t)}\\mathrm{d}\\mathbf{\\overline{{w}}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Considering a known imaging forward model, $\\boldsymbol{\\mathcal{A}}$ , and additive Gaussian noise, $p(\\mathbf{y}\\mid\\mathbf{x})\\sim\\mathcal{N}(\\mathbf{y}\\mid\\mathbf{\\Sigma}$ ${\\mathcal{A}}(\\mathbf{x}),\\sigma^{2}\\mathbf{I})$ , the conditional score function can be represented as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\mathbf{x}_{t}}\\log p\\left(\\mathbf{x}_{t}\\mid\\mathbf{y}\\right)=\\nabla_{\\mathbf{x}_{t}}\\log p_{t}\\left(\\mathbf{x}_{t}\\right)-\\nabla_{\\mathbf{x}_{t}}\\log p_{t}\\left(\\mathbf{y}\\mid\\mathbf{x}_{t}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\simeq\\mathbf{s}_{\\theta}(\\mathbf{x}_{t},t)-\\frac{1}{2\\sigma^{2}}\\nabla_{\\mathbf{x}_{t}}\\left\\|\\mathbf{y}-\\mathcal{A}\\left(\\hat{\\mathbf{x}}_{0}\\left(\\mathbf{x}_{t}\\right)\\right)\\right\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{x}}_{0}(\\mathbf{x}_{t})=\\frac{1}{\\sqrt{\\bar{\\alpha}(t)}}\\left[\\mathbf{x}_{t}+\\left(1-\\bar{\\alpha}(t)\\right)\\mathbf{s}_{\\theta}(\\mathbf{x}_{t},t)\\right],\\quad\\bar{\\alpha}(t)=\\prod_{s=1}^{t}\\left(1-\\beta(s)\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "However, a naive diffusion posterior sampling approach using Eqs. 8, 9, and 10 often fails to produce high-quality reconstructions. This is because the learned DM is inaccurate during the early EM iterations. We demonstrate this issue with a toy experiment. We performed diffusion posterior sampling (DPS) on randomly masked observations, as shown in Fig. 2(a), using an initial DM trained on only 50 clean images. The resulting posterior samples, depicted in Fig. 2(b), show mode collapse due to the severely limited prior. All recovered samples come from the training set of 50 clean images and are unrelated to the observations. Similarly, if the DM is trained on blurry, noisy images with artifacts, naive DPS also performs poorly in image reconstruction. ", "page_idx": 4}, {"type": "text", "text": "Require: DM $\\mathbf{s}_{\\theta}$ , Observations $(\\mathbf{Y},A)$ , few clean data $\\mathbf{x}$ , Cycles $N$ , Timesteps $T$ , Epoches $M$   \nMeasurement noise $\\mathcal{N}(\\mathbf{0},\\sigma^{2}\\mathbf{I})$ , Diffusion rate $\\left\\{\\beta_{t}\\right\\}_{t=1}^{T}$   \n1: Initialize $\\mathbf{s}_{\\theta}$ on through denoising score matching (47)   \n2: for $i=1$ to $N$ do   \n3: $(\\mathbf{y},f)\\sim\\operatorname{Dataset}(\\mathbf{Y},{\\mathcal{A}})$   \n4: $\\mathbf{x}_{T}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$   \n5: for $t=T$ to 1 do   \n6: \u03b1\u00aft = ts=1(1 \u2212\u03b2t)   \n7: \u02c6x0 \u2190 t(i)+ (1 \u2212\u03b1\u00aft) s\u03b8(xt(i ), t)   \n8: z \u223cN(0, I)   \n9: Take reverse-time SDE step on {Sampling in Sec. 4.2}   \n$\\begin{array}{r}{\\mathbf{x}_{t-1}^{(i)}\\leftarrow\\mathbf{x}_{t}^{(i)}+\\beta(t)\\left[\\frac{\\mathbf{x}_{t}^{(i)}}{2}+\\left(\\mathbf{s}_{\\theta}(\\mathbf{x}_{t}^{(i)},t)-\\frac{\\lambda}{2\\sigma^{2}}\\nabla_{\\mathbf{x}_{t}^{(i)}}\\left|\\left|\\mathbf{y}-f\\left(\\hat{\\mathbf{x}}_{0}\\right)\\right|\\right|_{2}^{2}\\right)\\right]+\\sqrt{\\beta(t)}\\mathbf{z}}\\end{array}$   \n10: end for   \n11: for $m=0$ to $M$ do   \n12: xdata \u223cShuflfe(\u02c6x(0i)\u223cp\u03b8(x\u02c6(0i)| y(i)))   \n13: t \u223cUniform({1, . . . , T})   \n14: \u03b1\u00aft = ts=1(1 \u2212\u03b2t)   \n15: \u03f5 \u223cN (0, I)   \n16: Take gradient descent step on {Optimization in Sec. 4.3}   \n$\\underset{\\mathrm{~\\hphantom{~}{~0~}~}}{\\nabla}\\left\\|\\epsilon-\\epsilon_{\\theta}\\left(\\sqrt{\\bar{\\alpha}_{t}}\\mathbf{x}^{d a t a}+\\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon,t\\right)\\right\\|_{2}^{2}$   \n17: end for ", "page_idx": 5}, {"type": "text", "text": "18: end for ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "It does not mean that these low-quality DMs cannot provide any prior information. Although the prior is poor in the early training stages, it has learned common features and structures shared among natural images, such as the continuity and smoothness of natural images and profiles of specific object types. By introducing a hyper-parameter $\\lambda$ to rescale the likelihood term and avoid mode collapse, we find that the low-quality DM can also act as a weak prior for posterior sampling, where the reverse-time SDE can be written as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{d{\\bf x}=\\beta(t)\\left[-\\frac{\\bf x}{2}-(\\nabla_{\\bf x}_{t}\\log p_{t}\\left({\\bf x}_{t}\\right)+\\lambda\\nabla_{{\\bf x}_{t}}\\log p_{t}\\left({\\bf y}\\mid{\\bf x}_{t}\\right))\\right]d t+\\sqrt{\\beta(t)}d\\overline{{\\bf w}}}\\ ~}\\\\ {\\displaystyle{\\qquad\\simeq\\beta(t)\\left[-\\frac{\\bf x}{2}-\\left({\\bf s}_{\\theta}({\\bf x}_{t},t)-\\frac{\\lambda}{2\\sigma^{2}}\\nabla_{{\\bf x}_{t}}\\left\\|{\\bf y}-{\\cal A}\\left({\\bf\\hat{x}}_{0}\\left({\\bf x}_{t}\\right)\\right)\\right\\|_{2}^{2}\\right)\\right]d t+\\sqrt{\\beta(t)}d\\overline{{\\bf w}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The hyper-parameter $\\lambda$ efficiently balances the diffusion prior and the data likelihood, resulting in reliable reconstructed images even when the prior is poor. As demonstrated in Fig. 2 (b), (c), and (d), as $\\lambda$ increases from 1 to 20, the data likelihood term gains more emphasis, making the reconstructed images more consistent with the inpainting observations. The choice of the hyper-parameter $\\lambda$ is automated in each $\\mathrm{E}\\cdot$ -step by finding the value that minimizes the data loss, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\lambda^{*}=\\underset{\\lambda}{\\arg\\operatorname*{min}}\\mathbb{E}_{\\mathbf{y},\\hat{\\mathbf{x}}_{0,\\lambda}}\\left[\\left\\|\\mathbf{y}-\\mathcal{A}(\\hat{\\mathbf{x}}_{0,\\lambda})\\right\\|_{2}^{2}\\right],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\hat{\\mathbf{x}}_{0,\\lambda}$ represents the diffusion posterior samples of reconstructed images with $\\lambda$ scaling. ", "page_idx": 5}, {"type": "text", "text": "4.3 M-step: Optimizing Score-Based Priors ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "During the M-step, we update the weights of the score-based models using the posterior samples obtained in the $\\mathrm{E}$ -step. This resembles training a standard clean DM, $\\mathbf{s}_{\\theta}$ , to approximate the timedependent score function, $\\nabla_{\\mathbf{x}_{t}}\\log p(\\mathbf{x}_{t}\\mid\\hat{\\mathbf{x}}_{0})$ , through denoising score matching (47): ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\theta^{*}=\\underset{\\theta}{\\arg\\operatorname*{min}}\\mathbb{E}_{t,\\mathbf{x}_{t},\\hat{\\mathbf{x}}_{0}}\\left[\\left\\|\\mathbf{s}_{\\theta}(\\mathbf{x}_{t},t)-\\nabla_{\\mathbf{x}_{t}}\\log p(\\mathbf{x}_{t}\\mid\\hat{\\mathbf{x}}_{0})\\right\\|_{2}^{2}\\right],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $t\\,\\sim\\,\\mathrm{Uniform}(\\{1,...,T\\})$ , $\\hat{\\mathbf{x}}_{0}\\,=\\,\\hat{\\mathbf{x}}_{0,\\lambda^{*}}$ represents the posterior samples from the previous E-step, and $\\mathbf{x}_{t}\\sim p(\\mathbf{x}_{t}\\mid\\hat{\\mathbf{x}}_{0})$ are generated by the forward-time SDE in Eq. 3. ", "page_idx": 5}, {"type": "image", "img_path": "jURBh4V9N4/tmp/49b14165326eb1530e37cfccefdb4c1f3df4a1e503a6cd99049cff8556f15586.jpg", "img_caption": ["Figure 3: Results on CIFAR-10 inpainting. In each image, $60\\%$ of the pixels are masked. As the EM iterations progress, the diffusion model learns cleaner prior distributions, improving the quality of posterior samples. Our method significantly outperforms the baselines, SURE-Score and AmbientDiffusion, achieving reconstruction quality comparable to DPS with a clean prior. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "To accelerate training, especially during the early stages when the posterior samples are noisy, the M-step does not always train the score function $s_{\\theta}(\\cdot)$ from scratch. In the initial M-steps, we inherit the DM weights from the previous iteration and fine-tune them only using posterior samples from a subset of observations (e.g., randomly select $10\\%$ of total observations). However, once the quality of reconstructed images improves sufficiently, we reinitialize the DM weights and retrain the model with $100\\%$ data for a few more iterations. The training strategy transitions when the optimal balancing parameter, $\\lambda^{*}$ , falls below 1, or fails to decrease for more than three consecutive iterations. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we demonstrate the performance of our method in learning DMs from corrupted data and solving inverse problems using these models. We validate the method on three imaging tasks: random inpainting, denoising, and deblurring. Our main results are presented in Fig.3, Fig.4, and Table 1, with additional ablation studies in Fig. 5. Further details on neural network architectures, training settings, and additional reconstruction and generation samples are provided in the appendix. ", "page_idx": 6}, {"type": "text", "text": "5.1 Datasets and Evaluation Metrics ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The experiments are conducted on the CIFAR-10 (48) and CelebA (49) datasets at resolutions of $32\\times32$ and $64\\times64$ , respectively. CIFAR-10 consists of 50,000 images across 10 classes for training, while CelebA contains 30,000 images of human faces. At each iteration, 5,000 corrupted images are randomly chosen for posterior sampling and training, and 250 corrupted images from the test set are chosen for evaluation. ", "page_idx": 6}, {"type": "text", "text": "We evaluate the performance of our method using two groups of metrics. First, we compute the peak signal-to-noise-ratio (PSNR) and learned Perceptual Image Patch Similarity (LPIPS) scores between the reconstructed and ground-truth images, quantifying the accuracy of inverse imaging using learned DMs. Additionally, we compute the Fr\u00e9chet Inception Distance (FID) between the learned DMs and reserved test data to assess their image generation quality. ", "page_idx": 6}, {"type": "text", "text": "5.2 Baseline and Training Settings ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We compare our method with three related baselines: AmbientDiffusion (37), SURE-Score (39), and DPS with clean prior (13). AmbientDiffusion and SURE-Score have similar settings to our method, which do not require DMs pre-trained on large-scale clean signals. Considering AmbientDiffusion is well-designed for masked observations, we only use it as the baseline of the image inpainting task. On the other hand, DPS leverages a pre-trained clean diffusion prior for posterior sampling, so it defines the performance upper bound for our method. ", "page_idx": 6}, {"type": "text", "text": "In our experiments, we randomly select 50 clean images from each dataset to train the initial DMs for the EM iterations. AmbientDiffusion is trained with the standard setting in (37). The key hyperparameter of SURE-Score, $\\sigma_{\\omega}$ , is set to the observation noise\u2019s standard deviation (0.2 for denoising, ", "page_idx": 6}, {"type": "text", "text": "Table 1: Numerical Results of inverse imaging and learned priors. The average values of PSNR/LPIPS are from 250 samples randomly selected from the test set. FID is used to evaluate the quality of learned priors by comparing 50,000 generated samples to the train set. Optimal results are highlighted in bold and suboptimal results in underline. Note that we take DPS w/ clean prior as the upper bound. ", "page_idx": 7}, {"type": "table", "img_path": "jURBh4V9N4/tmp/e77695ee9d9bd05de81fe6c6ddd3727a5165ecb35cf6fa8a1f55876d88f9bb1b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "jURBh4V9N4/tmp/8a1fe2b1d4cf831a9e68d90d12c46a9f214b963e89707dd645a4cc41bb6cb7c4.jpg", "img_caption": ["Figure 4: Results on (a) CIFAR-10 denoising and (b) CelebA deblurring. Our method significantly outperforms the baseline, SURE-Score, and approximates DPS with clean prior. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "and 0.01 for inpainting and deblurring). To ensure a fair comparison, we also provide the same 50 clean images for training AmbientDiffusion and SURE-Score. Details are in Appendix A. ", "page_idx": 7}, {"type": "text", "text": "5.3 Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Image inpainting. We conduct random inpainting (with mask probability $p=0.6)$ ) on CIFAR-10. As shown in Fig. 3 and Table 1, our method significantly outperforms AmbientDiffusion and SUREScore, achieving reconstruction quality similar to DPS with a prior trained on the clean CIFAR-10 dataset. The iterative training process is also illustrated in Fig. 3. Initially, our method performs poorly with the DM trained on only 50 randomly selected clean samples. However, as the E-step and M-step alternate iteratively, the quality of posterior sampling improves. Large-scale posterior samples enrich the priors, leading to enhanced performance at each stage. ", "page_idx": 7}, {"type": "text", "text": "Image denoising. We perform image denoising on CIFAR-10 with Gaussian noise $\\mathbf{n}\\sim\\mathcal{N}(0,\\sigma^{2}\\mathbf{I})$ and $\\sigma=0.2$ . The results are shown in Fig. 4(a) and Table 1. Our method outperforms SURE-Score, and the self-supervised denoising benchmark, Noise2Self (50), though it slightly lags behind DPS with clean priors. However, while our method\u2019s reconstructions may appear noisier than DPS results, they sometimes reproduce more details, such as the car wheels in the second row and the cat face in the third row of Fig. 4(a), showcasing the better diversity of our learned DMs. ", "page_idx": 7}, {"type": "text", "text": "Image deblurring. We validate image deblurring on CelebA using a Gaussian blur kernel with a size of $9\\times9$ and a standard deviation of $\\sigma\\,=\\,2$ pixels. The results are shown in Fig. 4(b) and Table 1. As with the other tasks, our method significantly outperforms SURE-Score in solving imaging inverse problems, recovering fine details of human faces. However, the FID score of our learned diffusion models lags behind the original blurred observations. This is primarily because the FID score measures image similarity mainly through smooth features, making it a less effective metric for deblurring tasks. ", "page_idx": 7}, {"type": "image", "img_path": "jURBh4V9N4/tmp/56715dae8edd2b3de1ac6c119e0bee7d4861979481537a2aa907c7702dadefa7.jpg", "img_caption": ["Figure 5: Ablation studies. (a) PSNR of diffusion posterior samples generated by the initial diffusion models trained on different amounts (10, 50, 100, 500) or types (in-distribution or out-of-distribution) of clean data. (b) FID scores of learned diffusion models after each EM iteration. The diffusion model trained on 50,000 corrupted images achieves a similar performance to those trained on 15,000-20,000 clean images. (c) PSNR of diffusion posterior samples weighted by different scaling factors $\\lambda$ at each stage. The optimal $\\lambda$ for posterior sampling decreases as the EM iterations progress. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Comparing the results of all three tasks, we find that AmbientDiffusion only works well for inpainting because its additional masking technique is specifically designed for that purpose. SURE-Score consistently produces over-smoothed results because the SURE loss regularizes the gradient of generated images. As a comparison, our method does not make any special assumptions and provides a general framework applicable to all three tasks. The generation results are in Appendix D. ", "page_idx": 8}, {"type": "text", "text": "5.4 More Analysis and Ablation Studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Number of clean images for training initial DMs. Our EM approach starts with DMs trained on a small set of clean images. Fig. 5(a) shows the PSNR of posterior samples generated by these models in the first E-step, allowing us to evaluate the impact of the number of clean training images on the performance of the initial DMs. Remarkably, DMs trained on as few as 10 clean images $0.02\\%$ of the corrupted images) can still act as reasonable priors. For inpainting and denoising tasks, DMs trained on 10 clean images provide nearly the same reconstruction quality as those trained on 500 clean images, as these tasks primarily require priors for low-frequency features, and 10 images suffice for an initial guess. However, the deblurring task beneftis from DMs trained on more clean data since deblurring aims to recover high-frequency details where more data helps. ", "page_idx": 8}, {"type": "text", "text": "Surprisingly, we find that DMs trained on clean images from CelebA (downsampled to $32\\!\\times\\!32$ ) can also be used to initialize tasks on CIFAR-10. For inpainting and denoising tasks, DMs initialized with 50 clean images from an out-of-distribution (OOD) dataset (CelebA) achieve similar performance to those initialized with in-distribution (ID) data (CIFAR-10). For the deblurring task, DMs trained on OOD data perform better than those trained on a similar amount of ID data, suggesting that OOD data can sometimes serve as stronger priors for guessing high-frequency information. ", "page_idx": 8}, {"type": "text", "text": "Learned priors through iterative training. Fig. 5(b) shows the FID scores of the learned DMs in the inpainting task with 50,000 corrupted CIFAR-10 images after each EM stage. The generation ability of the DMs gradually improves as the EM iterations progress. As explained in Sec. 4.3, initially the DM inherits weights from previous steps for fast training and converges at the sixth iteration. After resetting the DM, the training resumes for three more rounds and finally converges to a FID score of approximately 21.08, significantly better than AmbientDiffusion\u2019s 28.88, setting a new state-of-the-art. Notably, our method achieves this performance using a DM architecture with far fewer parameters: our method employs a vanilla DDPM with 35.7 million parameters, while AmbientDiffusion uses an improved $\\mathsf{D D P M++}$ architecture with over 100 million parameters. Additionally, we compared our method with DMs trained on different amounts of clean data. Our model, learned from 50,000 corrupted images $60\\%$ pixels masked) using EM, performs better than a DM trained on around 15,000 clean images. ", "page_idx": 8}, {"type": "text", "text": "Scaling factor in adaptive diffusion posterior sampling. Fig. 5(c) shows the PSNR of reconstructed images at each EM stage with different scaling factors, using the inpainting task on CIFAR-10 as an example. We observe that the quality of posterior sampling initially improves and then deteriorates as the scaling factor increases, confirming the existence of an optimal scaling factor as suggested in Eq. 12. As the EM stages progress, the optimal scaling factor decreases, indicating that the learned priors progressively improve through the EM iterations. This observation justifies the need for adaptive scaling factors in our method. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we proposed EMDiffusion, a novel expectation-maximization (EM) framework for training diffusion models primarily from corrupted observations. The key assumption is that it is information-theoretically possible to learn the underlying distribution from measurements. Our method demonstrated state-of-the-art performance in image inpainting, denoising, and deblurring across various datasets. Additionally, an important finding is that a small amount of clean, indistribution data can act as an implicit regularizer, aiding the training of diffusion models from corrupted observations. Future work will aim to 1) extend initialization approaches, potentially by incorporating foundation models or traditional machine learning techniques, such as using preprocessed images from unsupervised inpainting, deblurring, or denoising for initialization, and 2) extend to various imaging inverse problems and learning unknown forward models or noise statistics. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the National Natural Science Foundation of China(62371007) and the National Key Research and Development Program of China (2022YFC3401100). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] J. Ho, A. Jain, and P. Abbeel, \u201cDenoising diffusion probabilistic models,\u201d Advances in neural information processing systems, vol. 33, pp. 6840\u20136851, 2020. [2] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole, \u201cScore-based generative modeling through stochastic differential equations,\u201d arXiv preprint arXiv:2011.13456, 2020.   \n[3] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli, \u201cDeep unsupervised learning using nonequilibrium thermodynamics,\u201d in International conference on machine learning. PMLR, 2015, pp. 2256\u20132265. [4] A. Q. Nichol and P. Dhariwal, \u201cImproved denoising diffusion probabilistic models,\u201d in International Conference on Machine Learning. PMLR, 2021, pp. 8162\u20138171. [5] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever, \u201cZeroshot text-to-image generation,\u201d in International Conference on Machine Learning. PMLR, 2021, pp. 8821\u20138831. [6] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, \u201cHierarchical text-conditional image generation with clip latents,\u201d arXiv preprint arXiv:2204.06125, 2022.   \n[7] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, \u201cHigh-resolution image synthesis with latent diffusion models,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 10 684\u201310 695. [8] C. Saharia, W. Chan, H. Chang, C. Lee, J. Ho, T. Salimans, D. Fleet, and M. Norouzi, \u201cPalette: Image-to-image diffusion models,\u201d in ACM SIGGRAPH 2022 Conference Proceedings, 2022, pp. 1\u201310. [9] Z. Kong, W. Ping, J. Huang, K. Zhao, and B. Catanzaro, \u201cDiffwave: A versatile diffusion model for audio synthesis,\u201d arXiv preprint arXiv:2009.09761, 2020.   \n[10] M. Xu, L. Yu, Y. Song, C. Shi, S. Ermon, and J. Tang, \u201cGeodiff: A geometric diffusion model for molecular conformation generation,\u201d arXiv preprint arXiv:2203.02923, 2022.   \n[11] B. T. Feng, J. Smith, M. Rubinstein, H. Chang, K. L. Bouman, and W. T. Freeman, \u201cScore-based diffusion models as principled priors for inverse imaging,\u201d arXiv preprint arXiv:2304.11751, 2023.   \n[12] G. Zhang, J. Ji, Y. Zhang, M. Yu, T. Jaakkola, and S. Chang, \u201cTowards coherent image inpainting using denoising diffusion implicit models,\u201d in International Conference on Machine Learning. PMLR, 2023, pp. 41 164\u201341 193.   \n[13] H. Chung, J. Kim, M. T. Mccann, M. L. Klasky, and J. C. Ye, \u201cDiffusion posterior sampling for general noisy inverse problems,\u201d arXiv preprint arXiv:2209.14687, 2022.   \n[14] H. Chung, B. Sim, D. Ryu, and J. C. Ye, \u201cImproving diffusion models for inverse problems using manifold constraints,\u201d Advances in Neural Information Processing Systems, vol. 35, pp. 25 683\u201325 696, 2022.   \n[15] Y. Song, L. Shen, L. Xing, and S. Ermon, \u201cSolving inverse problems in medical imaging with score-based generative models,\u201d arXiv preprint arXiv:2111.08005, 2021.   \n[16] B. Kawar, N. Elata, T. Michaeli, and M. Elad, \u201cGsure-based diffusion model training with corrupted data,\u201d arXiv preprint arXiv:2305.13128, 2023.   \n[17] W. Zhao, S. Zhao, L. Li, X. Huang, S. Xing, Y. Zhang, G. Qiu, Z. Han, Y. Shang, D.-e. Sun et al., \u201cSparse deconvolution improves the resolution of live-cell super-resolution fluorescence microscopy,\u201d Nature biotechnology, vol. 40, no. 4, pp. 606\u2013617, 2022.   \n[18] C. Bouman and K. Sauer, \u201cA generalized gaussian image model for edge-preserving map estimation,\u201d IEEE Transactions on image processing, vol. 2, no. 3, pp. 296\u2013310, 1993.   \n[19] K. Kuramochi, K. Akiyama, S. Ikeda, F. Tazaki, V. L. Fish, H.-Y. Pu, K. Asada, and M. Honma, \u201cSuperresolution interferometric imaging with sparse modeling using total squared variation: application to imaging the black hole shadow,\u201d The Astrophysical Journal, vol. 858, no. 1, p. 56, 2018.   \n[20] C. Wang, K. Shang, H. Zhang, Q. Li, Y. Hui, and S. K. Zhou, \u201cDudotrans: dual-domain transformer provides more attention for sinogram restoration in sparse-view ct reconstruction,\u201d arXiv preprint arXiv:2111.10790, 2021.   \n[21] A. W. Reed, H. Kim, R. Anirudh, K. A. Mohan, K. Champley, J. Kang, and S. Jayasuriya, \u201cDynamic ct reconstruction from limited views with implicit neural representations and parametric motion fields,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 2258\u20132268.   \n[22] E. D. Zhong, T. Bepler, B. Berger, and J. H. Davis, \u201cCryodrgn: reconstruction of heterogeneous cryo-em structures using neural networks,\u201d Nature methods, vol. 18, no. 2, pp. 176\u2013185, 2021.   \n[23] E. Ye, Y. Wang, H. Zhang, Y. Gao, H. Wang, and H. Sun, \u201cRecovering a molecule\u2019s 3d dynamics from liquid-phase electron microscopy movies,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 10 767\u201310 777.   \n[24] S. Brooks, A. Gelman, G. Jones, and X.-L. Meng, Handbook of markov chain monte carlo. CRC press, 2011.   \n[25] D. M. Blei, A. Kucukelbir, and J. D. McAuliffe, \u201cVariational inference: A review for statisticians,\u201d Journal of the American statistical Association, vol. 112, no. 518, pp. 859\u2013877, 2017.   \n[26] H. Sun and K. L. Bouman, \u201cDeep probabilistic imaging: Uncertainty quantification and multimodal solution characterization for computational imaging,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35, no. 3, 2021, pp. 2628\u20132637.   \n[27] H. Sun, K. L. Bouman, P. Tiede, J. J. Wang, S. Blunt, and D. Mawet, \u201c $\\mathrm{\\Delta}\\alpha$ -deep probabilistic inference ( $\\alpha$ -dpi): efficient uncertainty quantification from exoplanet astrometry to black hole feature extraction,\u201d The Astrophysical Journal, vol. 932, no. 2, p. 99, 2022.   \n[28] A. Graikos, N. Malkin, N. Jojic, and D. Samaras, \u201cDiffusion models as plug-and-play priors,\u201d Advances in Neural Information Processing Systems, vol. 35, pp. 14 715\u201314 728, 2022.   \n[29] Y. Zhu, K. Zhang, J. Liang, J. Cao, B. Wen, R. Timofte, and L. Van Gool, \u201cDenoising diffusion models for plug-and-play image restoration,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 1219\u20131229.   \n[30] C. A. Bouman and G. T. Buzzard, \u201cGenerative plug and play: Posterior sampling for inverse problems,\u201d arXiv preprint arXiv:2306.07233, 2023.   \n[31] Y. Sun, Z. Wu, Y. Chen, B. T. Feng, and K. L. Bouman, \u201cProvable probabilistic imaging using score-based generative priors,\u201d arXiv preprint arXiv:2310.10835, 2023.   \n[32] H. Chung, B. Sim, and J. C. Ye, \u201cCome-closer-diffuse-faster: Accelerating conditional diffusion models for inverse problems through stochastic contraction,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 12 413\u201312 422.   \n[33] B. T. Feng and K. L. Bouman, \u201cEfficient bayesian computational imaging with a surrogate score-based prior,\u201d arXiv preprint arXiv:2309.01949, 2023.   \n[34] H. Chung and J. C. Ye, \u201cScore-based diffusion models for accelerated mri,\u201d Medical image analysis, vol. 80, p. 102479, 2022.   \n[35] A. Tewari, T. Yin, G. Cazenavette, S. Rezchikov, J. B. Tenenbaum, F. Durand, W. T. Freeman, and V. Sitzmann, \u201cDiffusion with forward models: Solving stochastic inverse problems without direct supervision,\u201d arXiv preprint arXiv:2306.11719, 2023.   \n[36] T. Anciukevi\u02c7cius, Z. Xu, M. Fisher, P. Henderson, H. Bilen, N. J. Mitra, and P. Guerrero, \u201cRenderdiffusion: Image diffusion for 3d reconstruction, inpainting and generation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 12 608\u201312 618.   \n[37] G. Daras, K. Shah, Y. Dagan, A. Gollakota, A. G. Dimakis, and A. Klivans, \u201cAmbient diffusion: Learning clean distributions from corrupted data,\u201d 2023.   \n[38] G. Daras, A. G. Dimakis, and C. Daskalakis, \u201cConsistent diffusion meets tweedie: Training exact ambient diffusion models with noisy data,\u201d arXiv preprint arXiv:2404.10177, 2024.   \n[39] A. Aali, M. Arvinte, S. Kumar, and J. I. Tamir, \u201cSolving inverse problems with score-based generative priors learned from noisy data,\u201d arXiv preprint arXiv:2305.01166, 2023.   \n[40] B. Kawar, M. Elad, S. Ermon, and J. Song, \u201cDenoising diffusion restoration models,\u201d Advances in Neural Information Processing Systems, vol. 35, pp. 23 593\u201323 606, 2022.   \n[41] A. Jalal, M. Arvinte, G. Daras, E. Price, A. G. Dimakis, and J. Tamir, \u201cRobust compressed sensing mri with deep generative priors,\u201d Advances in Neural Information Processing Systems, vol. 34, pp. 14 938\u201314 954, 2021.   \n[42] J. Song, A. Vahdat, M. Mardani, and J. Kautz, \u201cPseudoinverse-guided diffusion models for inverse problems,\u201d in International Conference on Learning Representations, 2022.   \n[43] A. P. Dempster, N. M. Laird, and D. B. Rubin, \u201cMaximum likelihood from incomplete data via the em algorithm,\u201d Journal of the royal statistical society: series B (methodological), vol. 39, no. 1, pp. 1\u201322, 1977.   \n[44] A. Gao, J. Castellanos, Y. Yue, Z. Ross, and K. Bouman, \u201cDeepgem: Generalized expectationmaximization for blind inversion,\u201d Advances in Neural Information Processing Systems, vol. 34, pp. 11 592\u201311 603, 2021.   \n[45] D. A. Reynolds et al., \u201cGaussian mixture models.\u201d Encyclopedia of biometrics, vol. 741, no. 659-663, 2009.   \n[46] H. Sun, N. J. Kasdin, and R. Vanderbei, \u201cIdentification and adaptive control of a high-contrast focal plane wavefront correction system,\u201d Journal of Astronomical Telescopes, Instruments, and Systems, vol. 4, no. 4, pp. 049 006\u2013049 006, 2018.   \n[47] P. Vincent, \u201cA connection between score matching and denoising autoencoders,\u201d Neural computation, vol. 23, no. 7, pp. 1661\u20131674, 2011. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[48] A. Krizhevsky, G. Hinton et al., \u201cLearning multiple layers of features from tiny images,\u201d 2009. ", "page_idx": 12}, {"type": "text", "text": "[49] Z. Liu, P. Luo, X. Wang, and X. Tang, \u201cDeep learning face attributes in the wild,\u201d in Proceedings of International Conference on Computer Vision (ICCV), December 2015.   \n[50] J. Batson and L. Royer, \u201cNoise2self: Blind denoising by self-supervision,\u201d in International Conference on Machine Learning. PMLR, 2019, pp. 524\u2013533.   \n[51] O. Ronneberger, P. Fischer, and T. Brox, \u201cU-net: Convolutional networks for biomedical image segmentation,\u201d in Medical image computing and computer-assisted intervention\u2013MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18. Springer, 2015, pp. 234\u2013241.   \n[52] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image recognition,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770\u2013778.   \n[53] I. Loshchilov and F. Hutter, \u201cDecoupled weight decay regularization,\u201d arXiv preprint arXiv:1711.05101, 2017.   \n[54] D. Kim, S. Shin, K. Song, W. Kang, and I.-C. Moon, \u201cSoft truncation: A universal training technique of score-based diffusion model for high precision score estimation,\u201d arXiv preprint arXiv:2106.05527, 2021.   \n[55] T. Karras, M. Aittala, T. Aila, and S. Laine, \u201cElucidating the design space of diffusion-based generative models,\u201d Advances in Neural Information Processing Systems, vol. 35, pp. 26 565\u2013 26 577, 2022.   \n[56] J. Yu, Z. Lin, J. Yang, X. Shen, X. Lu, and T. S. Huang, \u201cFree-form image inpainting with gated convolution,\u201d in Proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 4471\u20134480.   \n[57] Y. Song and S. Ermon, \u201cImproved techniques for training score-based generative models,\u201d Advances in neural information processing systems, vol. 33, pp. 12 438\u201312 448, 2020.   \n[58] A. Danielyan, V. Katkovnik, and K. Egiazarian, \u201cBm3d frames and variational image deblurring,\u201d IEEE Transactions on image processing, vol. 21, no. 4, pp. 1715\u20131728, 2011.   \n[59] A. Beck and M. Teboulle, \u201cFast gradient-based algorithms for constrained total variation image denoising and deblurring problems,\u201d IEEE transactions on image processing, vol. 18, no. 11, pp. 2419\u20132434, 2009. ", "page_idx": 12}, {"type": "text", "text": "We provide the implementation details and more results in the appendix. We first describe our network architecture and training settings in Sec. A, then show initialization details in Sec. B. More results are also provided in Sec. C and Sec. D. ", "page_idx": 13}, {"type": "text", "text": "A Implementation Details. ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Our neural network architecture follows the vanilla denoising diffusion probabilistic model (DDPM) (1). For quick implementation, see https://huggingface.co/google/ ddpm-cifar10-32 and https://huggingface.co/google/ddpm-celebahq-256. ", "page_idx": 13}, {"type": "text", "text": "Model architecture. Our architecture is exactly aligned with DDPM (1), which is a U-Net (51) based on a Wide ResNet (52). Diffusion time $t$ is implemented by adding the Transformer sinusoidal position embedding into each residual block. For CIFAR-10, our $32\\times32$ models use four feature map resolutions $\\mathrm{32\\times32}$ to $4\\times4$ ) and convolutional residual blocks per resolution level. For CelebA, we increase the feature map number for our $64\\times64$ to six. We enable the dropout regularization to reduce overftiting. Our CIFAR-10 model has 35.7 million parameters and our CelebA model has 114 million parameters. ", "page_idx": 13}, {"type": "text", "text": "Noise schedule. We leverage the default settings on VP-SDE (2), which uses a linear schedule with timesteps $T=1000$ , $\\beta_{1}=1e-4$ , and $\\beta_{T}=0.02$ . ", "page_idx": 13}, {"type": "text", "text": "Exponential moving average. To stabilize the training process and reduce the color shift of samples generated by trained DMs, we adopt an exponential moving average (EMA) technique with a decay factor of 0.999 for all experiments. ", "page_idx": 13}, {"type": "text", "text": "Optimizer. We apply AdamW (53) and set the learning rate to $2e-4$ for CIFAR-10 and $2e-5$ for CelebA. ", "page_idx": 13}, {"type": "text", "text": "Hyperparameters for the training process. We set the batch size to 512 for CIFAR-10 and 64 for CelebA. We set the dropout rate to 0.1 for CIFAR-10 and 0 for CelebA. As for the learning rate, we adapt $1e-4$ for CIFAR-10 and $2e-5$ for CelebA, for a larger learning rate will result in unstable training. ", "page_idx": 13}, {"type": "text", "text": "Dataset argumentation. We only use random horizontal flips to CIFAR-10 during training to achieve better performance. We did not filp CelebA, as the distribution of human faces is quite simple. We provide a table of training hyperparameters in Table 2. ", "page_idx": 13}, {"type": "table", "img_path": "jURBh4V9N4/tmp/01a56beed992169ffef0bbd9320d3d82b8a79bb364f728b764f34a11f34f5361.jpg", "table_caption": ["Table 2: Training hyperparameters. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "Baselines. As for Ambient Diffusion, We use the official checkpoint, which adopts the improved $\\scriptstyle\\mathrm{DDPM++}$ architecture (54) and the EDM scheduler design (55). It also modifies the model architecture\u2019s convolutions to Gated Convolutions (56), as they are known to perform better for inpainting-type problems. Their $32\\times32$ model has 113 million parameters, which is larger than ours. SureScore, on the other hand, uses the deepest NCSNv2 (57) model architecture, which has 95 million parameters. As for DPS with clean priors, we adopt the pre-trained DDPMs provided by https://huggingface.co/google. The details of the architectures of all methods are shown in Table 3. ", "page_idx": 13}, {"type": "text", "text": "Training schedule. At each EM iteration, we randomly choose 5,000 corrupted observations for diffusion posterior sampling and then train DMs. We further divide the iterations into two phases: ", "page_idx": 13}, {"type": "text", "text": "\u2022 Phase 1 - Resume training DMs: at the early EM iterations, we inherit weights of DMs from the last iteration for quick convergence. For CIFAR-10, this phase lasts for about 6-8 EM iterations, while for CelebA deblurring, this phase increases to 10 EM iterations. ", "page_idx": 13}, {"type": "table", "img_path": "jURBh4V9N4/tmp/ba1de27d5dfcecd30bad5b91fb3f13df3bdfcf3f83891be40ed8f9716ee74c76.jpg", "table_caption": ["Table 3: Method architecture comparison. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "\u2022 Phase 2 - Reset training DMs: at the later EM iterations, we reset the weights of DMs at each M-step, that is, training DMs from scratch. The key insight is that DMs from Phase 1 always have a memory of bad posterior samples, which has a negative effect on the learned distribution. For CIFAR-10, this phase lasts for 3 EM iterations, we found it significantly improves the FID score of DMs. While for CelebA deblurring, this phase lasts for 2 EM iterations until we find the improvement is not obvious. ", "page_idx": 14}, {"type": "text", "text": "B Additional Strategies for Training Initial DMs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To verify the sensitivity of EMDiffusion\u2019s initial DM training data, we provide more quantitative and qualitative results in this section, as shown in Table 4. We draw the conclusion that EMDiffusion is not sensitive to the initial data. Apart from evaluating different numbers of in-distribution (ID) images and out-of-distribution (OOD) images for training the initial DMs, we also test the initialization on preprocessed observations, and find all of them converge similarly. ", "page_idx": 14}, {"type": "text", "text": "Table 4: Numerical results of different data for training initial DMs. We show the PSNR values of posterior sampling with diffusion initialized with different data. The results show that EMDiffusion is insensitive to initializations. ", "page_idx": 14}, {"type": "table", "img_path": "jURBh4V9N4/tmp/c8ab35d2a57910c05c4f524ff37b1cdebfe34ae4081a1b241ad8d789aabaf148.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Specifically, we preprocess the noisy observations with BM3D (58), the blurry observations with Fast TV Constraint (59), and leave the masked observations unchanged. DMs initialized on these preprocess samples also perform well on the following E-step. ", "page_idx": 14}, {"type": "text", "text": "C Additional Results on Random Inpainting ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table 5: Comparison of FID scores between the EM approach and Ambient Diffusion across different corruption levels (masking ratio $p=0.4,0.6,0.8,0.9)$ on CIFAR-10 and CelebA. ", "page_idx": 14}, {"type": "table", "img_path": "jURBh4V9N4/tmp/8820e6338054894372c36eb2a97ca78f9eb56ef281723507fa2e0b8a978af912.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "As shown in Table 5, we include additional comparisons to Ambient Diffusion (37) across various corruption conditions and datasets. The performance gap between the proposed method and Ambient Diffusion narrows under higher levels of corruption, likely due to the simpler DDPM architecture we employ. In contrast, Ambient Diffusion utilizes the improved $\\scriptstyle\\mathrm{DDPM++}$ architecture (54), which is specifically modified to perform better for high-corruption inpainting-type problems. Nonetheless, the overall performance demonstrates the effectiveness of the proposed method, as we do not focus on empirically optimized architecture details, but instead show the applicability of the new EM idea for training DMs from corrupted observations. ", "page_idx": 14}, {"type": "text", "text": "D Generative Samples ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "EMDiffusion is proposed to learn clean distributions from corrupted observations. In Sec. 5, we present detailed posterior sampling results and FID scores of learned DMs. ", "page_idx": 15}, {"type": "text", "text": "Our model outperforms baselines by a significant margin in three inverse imaging tasks on two datasets. Though the FID score of our model trained on blurry CelebA is slightly higher than Ambient Diffusion, we argue that FID scores are easily influenced by sharp artifacts introduced by DPS (13), which is adopted in our E-steps. However, the distributions EMDiffusion learned from various types of corrupted observations are obviously better than baselines, as shown in Fig. 7,6,8. ", "page_idx": 15}, {"type": "text", "text": "Future work. To achieve a better posterior distribution through the proposed EM framework, an accurate and efficient E-step plays a key role. We adopt the Diffusion Posterior Sampling (13) that could potentially introduce artifacts due to its approximation of the underlying data likelihood term. Therefore, a better FID score could be achieved by designing a principled posterior sampling method. ", "page_idx": 15}, {"type": "image", "img_path": "jURBh4V9N4/tmp/934980e5c0284ee9cc3cde47bbee8fb76a6d87ef14a581d1fc98133c1c353d9f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "jURBh4V9N4/tmp/b9400013f666d0c434ab12adecfa26adea8ea6fc1227bcf7f71b31d83bd247b4.jpg", "img_caption": ["Figure 6: Uncurated Samples generated from models trained on blurry CelebA. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "jURBh4V9N4/tmp/391b61abb9fff5584aa464e05de4fe43b70240d43db0ee8b03568256258016af.jpg", "img_caption": ["(a) SURE-Score, FID=220.01 "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "jURBh4V9N4/tmp/6e1b2ed343c5814cfa6ac5c6a2dce8471a481ae714b8121d7054ea0719ad27d4.jpg", "img_caption": ["(b) Ambient Diffusion, $\\scriptstyle\\mathrm{FID}=28.88$ "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "jURBh4V9N4/tmp/aceca4f2d42865b023228a32720ba1655f30321b34b3f9bd4dc8b3da35a6d3fc.jpg", "img_caption": ["Figure 7: Uncurated Samples generated from models trained on random masked CIFAR-10. ", "(c) Ours, FID=21.08 "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "jURBh4V9N4/tmp/6ff798a5cb9ae9ed227013a978b39669c7be40394ee70703200c015e22013d4e.jpg", "img_caption": ["(a) SURE-Score, FID $:=$ 132.61 "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "jURBh4V9N4/tmp/d61a50b51f2fe20ed4b8d1b39360850a46c816b602fb3526b2baa93b96f95ec4.jpg", "img_caption": ["Figure 8: Uncurated Samples generated from models trained on noisy CIFAR-10. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 18}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 18}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 18}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 18}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 18}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 18}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: We make clear claims that well reflect the paper\u2019s contributions and scope Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We discussed the limitation in the last. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We show ablation study for the proposed techniques. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We provide implementation details. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 19}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: We provide code in supp. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 20}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We provide all the experiment details. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [No] ", "page_idx": 21}, {"type": "text", "text": "Justification: Our experiments contain a large amount of examples that don\u2019t need the error bar. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We provide details of the computation resources. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our research explores computational imaging algorithms in which we didn\u2019t see any ethics issues. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification:Our research explores computational imaging algorithms in which we didn\u2019t see any negative social impacts. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: We study computational imaging algorithms and didn\u2019t see such risks. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We use public datasets that are licensed for research purposes. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: We did;t use new assets ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: We didn\u2019t do crowd sourcing with human subjects. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: We didn\u2019t do experiments related to human subjects. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]