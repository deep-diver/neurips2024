{"references": [{"fullname_first_author": "Masatoshi Uehara", "paper_title": "Future-dependent value-based off-policy evaluation in POMDPs", "publication_date": "2022-07-13", "reason": "This paper introduces the concept of future-dependent value functions, which is central to the methodology and analysis in the current paper."}, {"fullname_first_author": "Nan Jiang", "paper_title": "Doubly Robust Off-policy Value Evaluation for Reinforcement Learning", "publication_date": "2016-05-01", "reason": "This paper lays the groundwork for off-policy evaluation in MDPs, which is extended to the POMDP setting in the current paper."}, {"fullname_first_author": "Qiang Liu", "paper_title": "Breaking the curse of horizon: Infinite-horizon off-policy estimation", "publication_date": "2018-12-01", "reason": "This paper addresses the challenge of horizon dependence in off-policy evaluation, a problem also tackled in the current paper for POMDPs."}, {"fullname_first_author": "Andrea Zanette", "paper_title": "Provable benefits of actor-critic methods for offline reinforcement learning", "publication_date": "2021-01-01", "reason": "This paper provides theoretical insights into actor-critic methods in offline reinforcement learning, which are relevant to the current paper's context."}, {"fullname_first_author": "Leslie Pack Kaelbling", "paper_title": "Planning and acting in partially observable stochastic domains", "publication_date": "1998-01-01", "reason": "This foundational paper on POMDPs provides essential background and context for the current paper's work."}]}