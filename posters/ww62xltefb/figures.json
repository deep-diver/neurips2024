[{"figure_path": "ww62xltEfB/figures/figures_2_1.jpg", "caption": "Figure 1: Results of fitting y = 50x with a Lipschitz model (SN (left) or our model (right)), where the Lipschitz constant is constrained by an upper bound L. L = 50 (red line) is where an L-Lipschitz model with perfect tightness and expressive power should achieve a 0 loss for the first time. SN achieves this only from around L = 100 while ours at L = 50. See Appendix G.4 for further details.", "description": "This figure compares the performance of spectral normalization (SN) and the proposed model in fitting a linear function (y = 50x) while constraining the Lipschitz constant (L). The x-axis represents the upper bound of the Lipschitz constant (L), and the y-axis represents the loss. The red line indicates L=50, where a perfectly tight and expressive L-Lipschitz model should achieve zero loss.  The plot demonstrates that the proposed model achieves zero loss at L=50, showcasing its superior tightness and expressive power, while SN only reaches zero loss when L is significantly higher (around 100).", "section": "4.1 Bi-Lipschitz Control"}, {"figure_path": "ww62xltEfB/figures/figures_6_1.jpg", "caption": "Figure 2: Comparison of the time (left) and space (right) complexity for a single iteration between a traditional feedforward network and various BLNN variants.", "description": "This figure compares the computational complexity (FLOPS and memory usage) of a single iteration for different neural network architectures.  It compares a traditional feedforward network against several variations of the proposed Bi-Lipschitz Neural Network (BLNN).  Specifically, it shows the complexity for a BLNN using the full backpropagation method, a BLNN using Theorem 3.7 for optimized backpropagation, and a partially bi-Lipschitz BLNN (PBLNN). The results are displayed as graphs showing the FLOPS (floating point operations per second) and memory usage in MB (megabytes) as functions of batch size (number of data samples processed simultaneously). The BLNN variants show varying levels of complexity improvement over the traditional feedforward network, demonstrating the computational efficiency of the proposed methods, particularly when using the optimized backpropagation technique (Theorem 3.7).", "section": "3.8 Computational Complexity"}, {"figure_path": "ww62xltEfB/figures/figures_8_1.jpg", "caption": "Figure 16: Results of fitting the curve with SLL (first row), Sandwich (second row) and our method (third row) with a specified Lipschitzness of 50. The right column is a zoom of the right figure.", "description": "This figure compares the performance of three different bi-Lipschitz models (SLL, Sandwich, and the proposed model) on fitting a piecewise linear function with a specified Lipschitz constant of 50. The left column shows the fitted curve over the whole range of the input data. The right column zooms into the area around the discontinuity to highlight the differences in the accuracy of fitting the function, especially in regions where the Lipschitz constraint is binding.", "section": "4.1 Bi-Lipschitz Control"}, {"figure_path": "ww62xltEfB/figures/figures_8_2.jpg", "caption": "Figure 5: Uncertainty estimation with the two moons data set with several models. Blue indicates high uncertainty, and yellow low uncertainty. (d)-(f) are with DUQ+BLNN, where (\u03b1, \u03b2) are clarified.", "description": "This figure compares the uncertainty estimation results of several models on the two moons dataset.  The models compared include a deep ensemble, DUQ without regularization, DUQ, and DUQ combined with the proposed Bi-Lipschitz neural network (BLNN) with varying parameters \u03b1 and \u03b2. The color represents the level of uncertainty, with blue indicating high uncertainty and yellow indicating low uncertainty.  The figure visually demonstrates the improvement in uncertainty estimation that the BLNN provides compared to existing methods.", "section": "4.2 Uncertainty Estimation"}, {"figure_path": "ww62xltEfB/figures/figures_22_1.jpg", "caption": "Figure 6: Construction flow of bi-Lipschitz neural network through Legendre-Fenchel transformation and Brenier map.", "description": "This figure illustrates the process of constructing a bi-Lipschitz neural network using the Legendre-Fenchel transformation and the Brenier map.  It starts with a 1/\u03b2-strongly convex neural network, applies the Legendre transformation to obtain a \u03b2-smooth convex network, then adds a term to make it \u03b1 + \u03b2-smooth and \u03b1-strongly convex. Finally, taking the derivative results in an (\u03b1, \u03b1 + \u03b2)-bi-Lipschitz neural network.", "section": "3.2 Construction of Bi-Lipschitz Functions"}, {"figure_path": "ww62xltEfB/figures/figures_26_1.jpg", "caption": "Figure 7: A generalization of an architecture including our model.", "description": "This figure shows a generalized architecture that includes the proposed Bi-Lipschitz neural network (BLNN) model.  The input data (d) is first processed by a function h(d; \u03c6) parameterized by \u03c6. The output of h(d; \u03c6) is then fed into the BLNN, f(-; \u03b8), which is parameterized by \u03b8. The BLNN's output is further processed by a loss function L parameterized by \u03c8, which produces the final output. This architecture demonstrates the flexibility of integrating the BLNN into more complex models.", "section": "3. Bi-Lipschitz Neural Network"}, {"figure_path": "ww62xltEfB/figures/figures_28_1.jpg", "caption": "Figure 8: Evolution of bi-Lipschitzness (Lipschitz: left, inverse Lipschitz: right) through the iteration of several optimization algorithms: GD (top row), AGD (middle row) and Newton (bottom row).", "description": "This figure shows the evolution of the Lipschitz and inverse Lipschitz constants throughout the optimization process for three different algorithms: Gradient Descent (GD), Accelerated Gradient Descent (AGD), and the Newton method. Each row represents a different optimization algorithm, with the left column showing the Lipschitz constant and the right column showing the inverse Lipschitz constant. The x-axis represents the number of iterations, while the y-axis represents the value of the Lipschitz (or inverse Lipschitz) constant.  The plots illustrate how well each algorithm maintains the bi-Lipschitz property (i.e., the Lipschitz and inverse Lipschitz constants remain relatively stable and close to their theoretical bounds) during the optimization. The red line indicates the true value of the constants.", "section": "C.1 Influence of Approximate Optimization on Bi-Lipschitz Constants: Experiments"}, {"figure_path": "ww62xltEfB/figures/figures_29_1.jpg", "caption": "Figure 8: Evolution of bi-Lipschitzness (Lipschitz: left, inverse Lipschitz: right) through the iteration of several optimization algorithms: GD (top row), AGD (middle row) and Newton (bottom row).", "description": "The figure shows the evolution of Lipschitz and inverse Lipschitz constants during the training process using three different optimization algorithms: Gradient Descent (GD), Accelerated Gradient Descent (AGD), and Newton's method.  Each algorithm's performance is shown in a separate row of the figure with plots showing both the estimated and true values over the iterations.  This visualization helps in understanding the effectiveness of various optimization strategies in maintaining bi-Lipschitz properties throughout the training.", "section": "4.1 Bi-Lipschitz Control"}, {"figure_path": "ww62xltEfB/figures/figures_45_1.jpg", "caption": "Figure 11: Estimated Lipschitz and inverse Lipschitz constants of BLNN with gradient descent with softplus activation functions. The left figure is with 3 hidden layers and the right with 10. The x axis corresponds to j with \u03b2 = 0.05 + 99.95 \u00b7 j/100.", "description": "This figure shows the estimated Lipschitz and inverse Lipschitz constants obtained from a BLNN model using gradient descent with softplus activation functions.  Two separate plots are shown: one for a network with 3 hidden layers, and another for a network with 10 hidden layers.  The x-axis represents different values of \u03b2 (beta), calculated as \u03b2 = 0.05 + 99.95*j/100 where j is an index. The plots visually represent the relationship between the calculated Lipschitz and inverse Lipschitz constants and different values of beta, demonstrating how the model's sensitivity (as measured by these constants) changes with varying beta values. The 'Lower bound' and 'Upper bound' lines indicate the theoretically expected range of these constants based on the model parameters and design.", "section": "G.1 Simple Estimation of Bi-Lipschitz Constants at Initialization"}, {"figure_path": "ww62xltEfB/figures/figures_45_2.jpg", "caption": "Figure 11: Estimated Lipschitz and inverse Lipschitz constants of BLNN with gradient descent with softplus activation functions. The left figure is with 3 hidden layers and the right with 10. The x axis corresponds to j with \u03b2 = 0.05 + 99.95 \u00b7 j/100.", "description": "This figure shows the estimated Lipschitz and inverse Lipschitz constants obtained from a BLNN model using gradient descent with softplus activation functions.  The x-axis represents a parameter \u03b2, which varies from 0.05 to 100.  Two sets of results are displayed: one with a network architecture of 3 hidden layers and another with 10 hidden layers. The plots visualize the relationship between the parameter \u03b2 and the resulting Lipschitz and inverse Lipschitz constants, providing empirical evidence of how the model's sensitivity (as measured by the Lipschitz constants) is controlled through the parameter \u03b2. The lines show theoretical bounds, illustrating that the empirically obtained values stay within the theoretical expectations.", "section": "G.1 Simple Estimation of Bi-Lipschitz Constants at Initialization"}, {"figure_path": "ww62xltEfB/figures/figures_46_1.jpg", "caption": "Figure 1: Results of fitting y = 50x with a Lipschitz model (SN (left) or our model (right)), where the Lipschitz constant is constrained by an upper bound L. L = 50 (red line) is where an L-Lipschitz model with perfect tightness and expressive power should achieve a 0 loss for the first time. SN achieves this only from around L = 100 while ours at L = 50. See Appendix G.4 for further details.", "description": "This figure compares the performance of spectral normalization (SN) and the proposed bi-Lipschitz model in fitting a linear function (y = 50x) with a constrained Lipschitz constant (L).  The plot shows that the proposed model achieves zero loss at the theoretical minimum L=50, while SN only achieves zero loss at a much larger L=100, demonstrating the improved tightness and expressive power of the proposed model.", "section": "4.1 Bi-Lipschitz Control"}, {"figure_path": "ww62xltEfB/figures/figures_46_2.jpg", "caption": "Figure 11: Estimated Lipschitz and inverse Lipschitz constants of BLNN with gradient descent with softplus activation functions. The left figure is with 3 hidden layers and the right with 10. The x axis corresponds to j with \u03b2 = 0.05 + 99.95 \u00b7 j/100.", "description": "This figure shows the estimated Lipschitz and inverse Lipschitz constants obtained from 100 different (4, \u03b2)-BLNNs, where \u03b2 varies from 0.05 to 100. The experiments were conducted using gradient descent with softplus activation functions. The left panel displays results with 3 hidden layers, while the right panel shows results for 10 hidden layers. The x-axis represents the value of \u03b2, showcasing how the estimated Lipschitz and inverse Lipschitz constants change with different \u03b2 values.", "section": "G.1 Simple Estimation of Bi-Lipschitz Constants at Initialization"}, {"figure_path": "ww62xltEfB/figures/figures_46_3.jpg", "caption": "Figure 1: Results of fitting y = 50x with a Lipschitz model (SN (left) or our model (right)), where the Lipschitz constant is constrained by an upper bound L. L = 50 (red line) is where an L-Lipschitz model with perfect tightness and expressive power should achieve a 0 loss for the first time. SN achieves this only from around L = 100 while ours at L = 50. See Appendix G.4 for further details.", "description": "This figure compares the performance of spectral normalization (SN) and the proposed bi-Lipschitz model in fitting the linear function y = 50x.  The x-axis represents the upper bound (L) on the Lipschitz constant. The y-axis represents the loss. The red line indicates where a perfectly tight and expressive L-Lipschitz model should achieve zero loss. The results show that the proposed model achieves zero loss at L=50, while SN only achieves it at approximately L=100, demonstrating the superior tightness and expressiveness of the proposed model.", "section": "4.1 Bi-Lipschitz Control"}, {"figure_path": "ww62xltEfB/figures/figures_48_1.jpg", "caption": "Figure 16: Results of fitting the curve with SLL (first row), Sandwich (second row) and our method (third row) with a specified Lispchitzness of 50. The right column is a zoom of the right figure.", "description": "This figure compares the results of fitting a piecewise linear function using three different methods: SLL, Sandwich, and the proposed method. Each method is constrained to have a Lipschitz constant of 50. The plots show that the proposed method achieves a significantly better fit of the function, especially around the discontinuity at x = 0.  The zoomed-in plots on the right emphasize the superior accuracy of the proposed method in capturing the sharp transition near the discontinuity.", "section": "4.1 Bi-Lipschitz Control"}, {"figure_path": "ww62xltEfB/figures/figures_49_1.jpg", "caption": "Figure 1: Results of fitting y = 50x with a Lipschitz model (SN (left) or our model (right)), where the Lipschitz constant is constrained by an upper bound L. L = 50 (red line) is where an L-Lipschitz model with perfect tightness and expressive power should achieve a 0 loss for the first time. SN achieves this only from around L = 100 while ours at L \u2248 50. See Appendix G.4 for further details.", "description": "This figure compares the performance of spectral normalization (SN) and the proposed model in fitting a linear function (y = 50x) with a Lipschitz constraint. The x-axis represents the Lipschitz bound (L), and the y-axis represents the loss.  The red line indicates the point where a perfectly tight L-Lipschitz model should achieve zero loss. The proposed model achieves this at L \u2248 50, demonstrating better control and tightness of the Lipschitz constraint compared to SN which only achieves it at L \u2248 100.", "section": "4.1 Bi-Lipschitz Control"}, {"figure_path": "ww62xltEfB/figures/figures_50_1.jpg", "caption": "Figure 16: Results of fitting the curve with SLL (first row), Sandwich (second row) and our method (third row) with a specified Lipschitzness of 50. The right column is a zoom of the right figure.", "description": "This figure shows the results of fitting a piecewise linear function (y=x for x<0 and y=x+1 for x>=0) using three different bi-Lipschitz models: SLL, Sandwich, and the proposed model. Each model is trained with a Lipschitz constraint of 50. The left column plots the entire range of the fitted curve against the true function, while the right column zooms in on the region near the discontinuity (x=0) to highlight the differences in the models' behavior near the transition point. The proposed method demonstrates a much tighter fit to the true function, especially near the discontinuity. This indicates a superior ability to control and maintain the bi-Lipschitz property throughout the training process, compared to the layer-wise approaches of SLL and Sandwich.", "section": "4.1 Bi-Lipschitz Control"}, {"figure_path": "ww62xltEfB/figures/figures_51_1.jpg", "caption": "Figure 1: Results of fitting y = 50x with a Lipschitz model (SN (left) or our model (right)), where the Lipschitz constant is constrained by an upper bound L. L = 50 (red line) is where an L-Lipschitz model with perfect tightness and expressive power should achieve a 0 loss for the first time. SN achieves this only from around L = 100 while ours at L = 50. See Appendix G.4 for further details.", "description": "This figure compares the performance of spectral normalization (SN) and the proposed model in fitting a linear function y = 50x, with the Lipschitz constant constrained by an upper bound L.  The plot shows the loss as a function of L. The proposed model achieves a loss of 0 at L=50, demonstrating perfect tightness and expressive power, while the SN model only achieves this at a much larger L=100, indicating less tightness and expressive power.", "section": "4.1 Bi-Lipschitz Control"}, {"figure_path": "ww62xltEfB/figures/figures_52_1.jpg", "caption": "Figure 1: Results of fitting y = 50x with a Lipschitz model (SN (left) or our model (right)), where the Lipschitz constant is constrained by an upper bound L. L = 50 (red line) is where an L-Lipschitz model with perfect tightness and expressive power should achieve a 0 loss for the first time. SN achieves this only from around L = 100 while ours at L = 50. See Appendix G.4 for further details.", "description": "This figure compares the performance of spectral normalization (SN) and the proposed bi-Lipschitz model in fitting a linear function (y = 50x).  The x-axis represents the upper bound (L) on the Lipschitz constant, and the y-axis shows the loss.  The red line indicates the expected loss (0) when the model achieves perfect tightness and expressive power at L=50.  The proposed model achieves a loss near zero at L=50, demonstrating tight control of the Lipschitz constant. In contrast, the SN model only reaches a loss near zero at a significantly higher L value (around 100), indicating looser control.  This highlights the improved performance of the proposed model in precisely controlling bi-Lipschitzness.", "section": "4.1 Bi-Lipschitz Control"}, {"figure_path": "ww62xltEfB/figures/figures_53_1.jpg", "caption": "Figure 1: Results of fitting y = 50x with a Lipschitz model (SN (left) or our model (right)), where the Lipschitz constant is constrained by an upper bound L. L = 50 (red line) is where an L-Lipschitz model with perfect tightness and expressive power should achieve a 0 loss for the first time. SN achieves this only from around L = 100 while ours at L = 50. See Appendix G.4 for further details.", "description": "This figure shows the results of fitting the linear function y = 50x using two different Lipschitz models: a spectral normalized (SN) model and the proposed model in the paper.  The x-axis represents the upper bound L imposed on the Lipschitz constant, and the y-axis shows the loss achieved by the model.  The red line indicates the point where a perfectly tight and expressive L-Lipschitz model should achieve zero loss (L=50).  The plot highlights the superior performance of the proposed model, which achieves zero loss at L=50, while the SN model only reaches zero loss at around L=100. This demonstrates that the proposed model offers tighter control over the Lipschitz constant compared to the existing SN approach.", "section": "4.1 Bi-Lipschitz Control"}, {"figure_path": "ww62xltEfB/figures/figures_54_1.jpg", "caption": "Figure 16: Results of fitting the curve with SLL (first row), Sandwich (second row) and our method (third row) with a specified Lispchitzness of 50. The right column is a zoom of the right figure.", "description": "This figure compares the performance of three different bi-Lipschitz models in fitting a curve with a specified Lipschitz constant of 50.  The models compared are the Spectral Lipschitz Layer (SLL), the Sandwich layer, and the proposed model from the paper. The results are visualized by plotting the learned curve and showing the fit next to the true curve. The rightmost column of subplots shows a zoomed-in view of the rightmost columns, highlighting the details of how well each model fits the curve. The figure demonstrates that the proposed method outperforms the other models.", "section": "4.1 Bi-Lipschitz Control"}, {"figure_path": "ww62xltEfB/figures/figures_54_2.jpg", "caption": "Figure 26: ROC between downsampled FashionMNIST vs MNIST (left) and FashionMNIST vs NotMNIST (right).", "description": "This figure shows the Receiver Operating Characteristic (ROC) curves for out-of-distribution detection experiments using the DUQ and DUQ+BLNN models.  The left plot shows the results for distinguishing FashionMNIST from MNIST, while the right plot shows the results for distinguishing FashionMNIST from NotMNIST.  The ROC curves illustrate the trade-off between the true positive rate (TPR) and the false positive rate (FPR) for both models, indicating their performance in identifying out-of-distribution samples. The downsampled data used in this experiment consisted of images reduced in size from 28x28 to 14x14 via max pooling.", "section": "4.2 Uncertainty Estimation"}, {"figure_path": "ww62xltEfB/figures/figures_54_3.jpg", "caption": "Figure 26: ROC between downsampled FashionMNIST vs MNIST (left) and FashionMNIST vs NotMNIST (right).", "description": "This figure shows the Receiver Operating Characteristic (ROC) curves for the out-of-distribution detection task on the Fashion-MNIST dataset.  Two scenarios are presented: Fashion-MNIST vs. MNIST and Fashion-MNIST vs. NotMNIST.  A downsampled version of the Fashion-MNIST dataset (14x14) is used. The curves compare the performance of the DUQ model (blue) and the DUQ+BLNN model (orange).  The diagonal dashed line represents random chance; models above the line demonstrate better-than-chance performance in distinguishing between in-distribution and out-of-distribution samples. The area under the curve (AUC) is a common metric that quantifies performance.", "section": "4.2 Uncertainty Estimation"}, {"figure_path": "ww62xltEfB/figures/figures_55_1.jpg", "caption": "Figure 5: Uncertainty estimation with the two moons data set with several models. Blue indicates high uncertainty, and yellow low uncertainty. (d)-(f) are with DUQ+BLNN, where (\u03b1, \u03b2) are clarified.", "description": "This figure compares the uncertainty estimation performance of different models on the two moons dataset.  It shows uncertainty maps, where blue indicates high uncertainty and yellow indicates low uncertainty.  The models compared include Deep Ensembles, DUQ without regularization, DUQ (with regularization), and DUQ+BLNN with different (\u03b1, \u03b2) parameters. DUQ+BLNN is the proposed method of the paper, which incorporates the bi-Lipschitz Neural Network (BLNN). The comparison highlights how the proposed method, with its controlled bi-Lipschitzness, leads to better uncertainty estimates.", "section": "4.2 Uncertainty Estimation"}, {"figure_path": "ww62xltEfB/figures/figures_55_2.jpg", "caption": "Figure 24: Uncertainty quantification of two moons dataset with DUQ+BLNN with a high \u03b1. \u03b1 = 5.0 and \u03b2 = 30.0. We do not show the points so that the highly certain area is visible.", "description": "This figure shows the uncertainty quantification of the two moons dataset using the DUQ+BLNN model with a high alpha (\u03b1 = 5.0) and beta (\u03b2 = 30.0). The points are not shown to emphasize the area of high certainty (yellow).  The high alpha value causes the model to be highly uncertain almost everywhere except for a small region near the training data.  This highlights how the parameters \u03b1 and \u03b2 can be used to control the level of uncertainty in the model's predictions.", "section": "4.2 Uncertainty Estimation"}, {"figure_path": "ww62xltEfB/figures/figures_55_3.jpg", "caption": "Figure 26: ROC between downsampled FashionMNIST vs MNIST (left) and FashionMNIST vs NotMNIST (right).", "description": "This figure shows the Receiver Operating Characteristic (ROC) curves for the out-of-distribution detection task.  The left panel displays the ROC curve for distinguishing FashionMNIST from MNIST, while the right panel shows the ROC curve for distinguishing FashionMNIST from NotMNIST.  The ROC curves compare the performance of the standard DUQ model against the DUQ+BLNN model, illustrating the improvement in detection performance achieved by incorporating the bi-Lipschitz neural network.", "section": "4.2 Uncertainty Estimation"}, {"figure_path": "ww62xltEfB/figures/figures_55_4.jpg", "caption": "Figure 26: ROC between downsampled FashionMNIST vs MNIST (left) and FashionMNIST vs NotMNIST (right).", "description": "This figure shows the Receiver Operating Characteristic (ROC) curves for the out-of-distribution detection task using the DUQ and DUQ+BLNN models.  The left panel displays the ROC curve when distinguishing between FashionMNIST and MNIST datasets, while the right panel shows the ROC curve for distinguishing between FashionMNIST and NotMNIST datasets.  The ROC curves illustrate the trade-off between the true positive rate (TPR) and the false positive rate (FPR) for both models, allowing for a comparison of their performance in detecting out-of-distribution samples. The dashed line represents the performance of a random classifier.  The DUQ+BLNN model generally performs better, showing higher TPR for the same FPR.", "section": "Uncertainty Estimation"}, {"figure_path": "ww62xltEfB/figures/figures_56_1.jpg", "caption": "Figure 25: Uncertainty quantification of two moons dataset with DUQ+BLNN with different \u03b1.", "description": "Uncertainty quantification results for the two moons dataset using the DUQ+BLNN model with varying alpha values (\u03b1 = 0.0, 1.0, and 2.0). The plots show the uncertainty estimation, where blue indicates high uncertainty and yellow indicates low uncertainty. The figure visually demonstrates how different alpha values influence the uncertainty estimation.", "section": "4.2 Uncertainty Estimation"}]