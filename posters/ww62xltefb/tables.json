[{"figure_path": "ww62xltEfB/tables/tables_4_1.jpg", "caption": "Table 1: Tightness of Lipschitz bound when fitting f(x) = x (x < 0), x + 1 (x > 0). Mean over five trials. See Table 5 for further results.", "description": "This table compares the tightness of the Lipschitz bound achieved by different models when fitting a piecewise linear function with a discontinuity.  The models are tested with different upper bounds (L) on the Lipschitz constant. Tightness is measured as the percentage difference between the imposed Lipschitz constant and the actual Lipschitz constant observed after training.  The table shows that the proposed method (Ours) achieves much tighter bounds, particularly at higher values of L, indicating better control over the Lipschitzness.", "section": "4.1 Bi-Lipschitz Control"}, {"figure_path": "ww62xltEfB/tables/tables_7_1.jpg", "caption": "Table 1: Tightness of Lipschitz bound when fitting f(x) = x (x < 0), x + 1 (x > 0). Mean over five trials. See Table 5 for further results.", "description": "This table presents the results of an experiment designed to evaluate the tightness of the Lipschitz bound achieved by several Bi-Lipschitz models when fitting a piecewise linear function.  The models were trained with different upper bounds on the Lipschitz constant (L=5, 10, 50). The table shows the percentage of times each model's actual Lipschitz constant (after training) fell within a certain percentage of the imposed upper bound L.  A higher percentage indicates better tightness of the bound, meaning the model's actual Lipschitz constant remained closer to the constrained value. The results are averaged over five trials.", "section": "4.1 Bi-Lipschitz Control"}, {"figure_path": "ww62xltEfB/tables/tables_9_1.jpg", "caption": "Table 2: Out-of-distribution detection task of FashionMNIST vs MNIST and FashionMNIST vs NotMNIST with DUQ and DUQ+(0, 3)-BLNN. Means over five trials.", "description": "This table presents the results of an out-of-distribution detection experiment comparing the performance of DUQ (Deep Uncertainty Quantification) and DUQ+BLNN (DUQ enhanced with the Bi-Lipschitz Neural Network) on two tasks: distinguishing FashionMNIST from MNIST, and FashionMNIST from NotMNIST.  The results show accuracy, binary cross-entropy (BCE) loss, and Area Under the Receiver Operating Characteristic curve (AUROC) for each model and dataset.  The data demonstrates that the addition of the BLNN improves performance on the out-of-distribution detection task.", "section": "4.2 Uncertainty Estimation"}, {"figure_path": "ww62xltEfB/tables/tables_9_2.jpg", "caption": "Table 3: Comparison of our model with state-of-the-art monotone models in benchmark datasets. Means over three trials. Results of LMN and SMNN are from the original papers. BF = BlogFeedBack, LD = LoanDefaulter, HD = HeartDisease, Acc. = accuracy. See Table 9 for complete results.", "description": "This table compares the performance of the proposed Bi-Lipschitz neural network model against state-of-the-art monotone models on several benchmark datasets.  The metrics used for comparison include accuracy (Acc.), Root Mean Squared Error (RMSE), and Mean Squared Error (MSE), depending on the nature of each dataset.  The results show that the proposed model achieves competitive or better performance than the existing methods.", "section": "4.3 Partially Monotone Settings"}, {"figure_path": "ww62xltEfB/tables/tables_43_1.jpg", "caption": "Table 4: Out-of-distribution detection task of CIFAR10 vs SVHN with DUQ and BLNNconv.", "description": "This table presents the results of an out-of-distribution detection task comparing the performance of two models: DUQ and BLNNconv then DUQ.  The dataset used is CIFAR10 for in-distribution data and SVHN for out-of-distribution data. The table shows the accuracy, loss, and AUROC (Area Under the Receiver Operating Characteristic curve) for each model. The BLNNconv then DUQ model shows an improved AUROC on the SVHN dataset compared to DUQ, indicating better performance on out-of-distribution detection.", "section": "4.2 Uncertainty Estimation"}, {"figure_path": "ww62xltEfB/tables/tables_47_1.jpg", "caption": "Table 5: Tightness of Lipschitz bound with different methods. Each model was built with an upper-bound constraint on the Lipschitzness L, and the true Lipschitzness of the model after training was evaluated. The percentage between this value and L is reported in the table, with a mean and standard deviation over five different seeds.", "description": "This table compares the tightness of the Lipschitz bound for various methods in controlling Lipschitzness during neural network training. The methods are evaluated based on the percentage difference between the imposed Lipschitz constant (L) and the actual Lipschitz constant obtained after training, calculated over five different trials.  The table shows the performance for different values of L (2, 5, 10, and 50), demonstrating how well each method constrains the Lipschitz constant. The number of parameters in each model is also indicated.", "section": "4.1 Bi-Lipschitz Control"}, {"figure_path": "ww62xltEfB/tables/tables_52_1.jpg", "caption": "Table 6: Uncertainty quantification of two moons dataset with DUQ+BLNN with different \u03b1 and \u03b2. Mean and standard deviation over five different seeds.", "description": "This table presents the results of uncertainty quantification experiments using the DUQ+BLNN model on the two moons dataset.  The model's performance is evaluated using accuracy and BCE (Binary Cross Entropy) loss for different values of \u03b1 and \u03b2, which are hyperparameters controlling the bi-Lipschitzness of the model.  The table shows mean and standard deviation values across five independent trials, illustrating the impact of these hyperparameters on the model's uncertainty estimation capabilities. The high accuracy and low BCE loss for certain \u03b1/\u03b2 combinations highlight the potential benefits of DUQ+BLNN for uncertainty estimation.", "section": "4.2 Uncertainty Estimation"}, {"figure_path": "ww62xltEfB/tables/tables_53_1.jpg", "caption": "Table 7: Out-of-distribution detection task with down-sampled data. FashionMNIST vs MNIST and FashionMNIST vs NotMNIST dataset with DUQ and DUQ+BLNN. For the BLNN, \u03b1 = 0.2 and \u03b2 = 0.4. Mean and standard deviation over five trials.", "description": "This table shows the results of an out-of-distribution detection task using two different models: DUQ and DUQ+BLNN.  The task involves distinguishing FashionMNIST images from MNIST and NotMNIST images. The dataset was downsampled to 14x14 pixels.  The table presents the accuracy, BCE loss, and AUROC scores for MNIST and NotMNIST.  The DUQ+BLNN model uses a bi-Lipschitz neural network (BLNN) with parameters \u03b1 = 0.2 and \u03b2 = 0.4. The results are averaged over five trials, with standard deviations reported.", "section": "4.2 Uncertainty Estimation"}, {"figure_path": "ww62xltEfB/tables/tables_54_1.jpg", "caption": "Table 2: Out-of-distribution detection task of FashionMNIST vs MNIST and FashionMNIST vs NotMNIST with DUQ and DUQ+(0, 3)-BLNN. Means over five trials.", "description": "This table presents the results of an out-of-distribution detection task using two different models: DUQ and DUQ+BLNN.  The task involved distinguishing FashionMNIST images from MNIST and NotMNIST images. The table shows the accuracy, BCE (binary cross-entropy) loss, AUROC (area under the receiver operating characteristic curve) for MNIST, and AUROC for NotMNIST for both models, averaged over five trials.  The results demonstrate the performance of the models in identifying out-of-distribution data.", "section": "4.2 Uncertainty Estimation"}, {"figure_path": "ww62xltEfB/tables/tables_56_1.jpg", "caption": "Table 3: Comparison of our model with state-of-the-art monotone models in benchmark datasets. Means over three trials. Results of LMN and SMNN are from the original papers. BF = BlogFeedBack, LD = LoanDefaulter, HD = HeartDisease, Acc. = accuracy. See Table 9 for complete results.", "description": "This table compares the performance of the proposed Bi-Lipschitz Neural Network (BLNN) model against other state-of-the-art monotone models on several benchmark datasets.  The metrics used for comparison include accuracy (Acc.), Root Mean Squared Error (RMSE), and Mean Squared Error (MSE), depending on the specific dataset and task. The results suggest that the BLNN model demonstrates competitive performance compared to existing methods.", "section": "4.3 Partially Monotone Settings"}, {"figure_path": "ww62xltEfB/tables/tables_57_1.jpg", "caption": "Table 10: General details on the architectures of Figures 8 and 9.", "description": "This table shows the hyperparameter settings for the experiments shown in Figures 8 and 9 of the paper.  It lists the model used (in this case, \"Ours\"), the hidden dimension of the neural network layers, and the number of layers in the network. This information is crucial for reproducibility and allows readers to understand the specific network architecture used in those experiments.", "section": "H.2 Algorithms for LFT"}, {"figure_path": "ww62xltEfB/tables/tables_59_1.jpg", "caption": "Table 5: Tightness of Lipschitz bound with different methods. Each model was built with an upper-bound constraint on the Lipschitzness L, and the true Lipschitzness of the model after training was evaluated. The percentage between this value and L is reported in the table, with a mean and standard deviation over five different seeds.", "description": "This table compares the tightness of Lipschitz bounds for various bi-Lipschitz models (SN, AOL, Orthogonal, SLL, Sandwich, LMN, BiLipNet) and the proposed model.  The models were trained with an upper bound constraint (L) on the Lipschitz constant. The table shows the percentage of how close the actual Lipschitz constant achieved after training is compared to the imposed Lipschitz bound L for each model at different L values (L=2, 5, 10, 50). The results are averaged over five different random seeds.", "section": "4.1 Bi-Lipschitz Control"}, {"figure_path": "ww62xltEfB/tables/tables_60_1.jpg", "caption": "Table 12: General details on the architectures of Figure 23. \"reg.\" stands for regularization.", "description": "This table provides details on the architectures used for the uncertainty estimation experiments with the two moons dataset shown in Figure 23.  It compares four different models: DUQ, DUQ without regularization, DUQ combined with the proposed Bi-Lipschitz Neural Network (DUQ+BLNN), and Deep Ensembles. For each model, it lists the hidden dimension of the neural network, the number of layers, the output dimension, the centroid size, and the total number of parameters.", "section": "4.2 Uncertainty Estimation"}, {"figure_path": "ww62xltEfB/tables/tables_60_2.jpg", "caption": "Table 12: General details on the architectures of Figure 23. \"reg.\" stands for regularization.", "description": "This table presents the details of the neural network architectures used in the uncertainty quantification experiments shown in Figure 23 of the paper.  It breaks down the specifications for four models: DUQ, DUQ (no regularization), DUQ+BLNN, and Deep Ensembles. For each model, the table lists the hidden dimension, number of layers, output dimension, centroid size, and the total number of parameters.", "section": "H.4.1 Two Moons"}, {"figure_path": "ww62xltEfB/tables/tables_61_1.jpg", "caption": "Table 8: Out-of-distribution detection task with full size data. FashionMNIST vs MNIST and FashionMNIST vs NotMNIST dataset with DUQ and DUQ+BLNN. For the BLNN, \u03b1 = 0 and \u03b2 = 3.0. Mean and standard deviation over five trials.", "description": "This table presents the results of an out-of-distribution detection experiment using the FashionMNIST dataset.  Two scenarios are compared: FashionMNIST vs. MNIST and FashionMNIST vs. NotMNIST.  The performance of two models, DUQ and DUQ+BLNN (Deep Uncertainty Quantification with the proposed Bi-Lipschitz Neural Network), are evaluated using accuracy, binary cross-entropy loss, and AUROC (Area Under the Receiver Operating Characteristic curve) for both MNIST and NotMNIST datasets. The BLNN uses \u03b1=0 and \u03b2=3.0.  The results are averaged over five trials, with standard deviations reported.", "section": "4.2 Uncertainty Estimation"}, {"figure_path": "ww62xltEfB/tables/tables_62_1.jpg", "caption": "Table 15: General details on the architectures of PBLNN used for Table 3.", "description": "This table lists the hyperparameters used for the partially bi-Lipschitz neural network (PBLNN) in the partially monotone settings experiments.  It details the hidden dimensions, number of layers, Lipschitz constant, and inverse Lipschitz constant for each dataset (COMPAS, BlogFeedBack, LoanDefaulter, HeartDisease, AutoMPG, CIFAR101).  These hyperparameters reflect the architecture's inductive bias in each experiment.", "section": "4.3 Partially Monotone Settings"}]