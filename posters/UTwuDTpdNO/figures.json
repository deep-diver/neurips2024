[{"figure_path": "UTwuDTpdNO/figures/figures_2_1.jpg", "caption": "Figure 1: A graphical abstract of meta-Stackelberg defense. In the pertaining stage, a simulated environment is constructed using generated data and the attack domain. The defender utilizes meta-Stackelberg learning (Algorithm 1) to obtain the meta policy to be online adapted in the real FL.", "description": "This figure shows a graphical abstract of the meta-Stackelberg defense framework.  The framework consists of two stages: pre-training and online adaptation. The pre-training stage uses a simulated environment with generated data to train a meta-defense policy using meta-Stackelberg learning (Algorithm 1). This meta-policy is then adapted in the online adaptation stage when the real federated learning (FL) system encounters real attacks.", "section": "Meta Stackelberg Defense Framework"}, {"figure_path": "UTwuDTpdNO/figures/figures_8_1.jpg", "caption": "Figure 2: Comparisons of defenses against untargeted model poisoning attacks (i.e., LMP and RL) on MNIST and CIFAR-10. All parameters are set as default and random seeds are fixed.", "description": "The figure compares the performance of different defenses against untargeted model poisoning attacks, specifically LMP and RL attacks, on two datasets: MNIST and CIFAR-10.  The defenses being compared include ClipMed, meta-SG+, and meta-SG.  The x-axis represents the number of FL rounds, and the y-axis represents the model accuracy. The plot visually demonstrates how the accuracy changes over the course of the federated learning process under different attack scenarios. This provides a comparison of the effectiveness of each defense strategy against these attacks across different datasets.", "section": "4.2 Experiment Results"}, {"figure_path": "UTwuDTpdNO/figures/figures_16_1.jpg", "caption": "Figure 1: A graphical abstract of meta-Stackelberg defense. In the pertaining stage, a simulated environment is constructed using generated data and the attack domain. The defender utilizes meta-Stackelberg learning (Algorithm 1) to obtain the meta policy to be online adapted in the real FL.", "description": "This figure is a graphical abstract showing the two stages of the meta-Stackelberg defense framework.  The pre-training stage uses a simulated environment with generated data and known attack types to learn a meta-policy using meta-Stackelberg learning (Algorithm 1). The online adaptation stage then uses this meta-policy to adapt to real attacks in a real federated learning (FL) environment, using feedback from the environment to further refine the defense policy.", "section": "Meta Stackelberg Defense Framework"}, {"figure_path": "UTwuDTpdNO/figures/figures_16_2.jpg", "caption": "Figure 7: Examples of reconstructed images using inverting gradient (before and after denoising).", "description": "This figure shows examples of image reconstruction using the inverting gradient method. The top row displays noisy images, the middle row shows the images after denoising using a diffusion model and the bottom row shows the difference between the noisy and denoised images. This demonstrates the effectiveness of the inverting gradient method in reconstructing images from noisy data.", "section": "Simulated Environment"}, {"figure_path": "UTwuDTpdNO/figures/figures_16_3.jpg", "caption": "Figure 5: MNIST backdoor trigger patterns. The global trigger is considered the default poison pattern and is used for backdoor accuracy evaluation. The sub-triggers are used by pre-training and DBA only.", "description": "This figure shows different backdoor trigger patterns used in the MNIST dataset.  There is a global trigger, which is used as the standard backdoor, and several sub-triggers that are used during pre-training and in a specific attack method (DBA).  The image shows the original image and the variations created by adding the different triggers.", "section": "4.1 Experiment Settings"}, {"figure_path": "UTwuDTpdNO/figures/figures_16_4.jpg", "caption": "Figure 6: CIFAR-10 fixed backdoor trigger patterns. The global trigger is considered the default poison pattern and is used for online adaptation stage backdoor accuracy evaluation. The sub-triggers are used by pre-training and DBA only.", "description": "This figure shows different trigger patterns used for backdoor attacks in CIFAR-10 dataset.  It displays a set of images, each with a different trigger pattern superimposed on an original image.  The \"global trigger\" is the main trigger used to evaluate backdoor attack success during the online adaptation phase of the experiment. The \"sub-triggers\" are used during the pre-training phase of the experiment and by a specific backdoor attack method (DBA). The triggers are designed to manipulate the model's predictions, causing it to misclassify images even when the trigger is small and difficult to detect.", "section": "4.1 Experiment Settings"}, {"figure_path": "UTwuDTpdNO/figures/figures_18_1.jpg", "caption": "Figure 7: Examples of reconstructed images using inverting gradient (before and after denoising).", "description": "This figure shows examples of reconstructed images before and after applying denoising techniques using the inverting gradient method. The method is used to generate data to simulate the training process in a federated learning environment where the server only has access to a small amount of root data.", "section": "Experiment Setup"}, {"figure_path": "UTwuDTpdNO/figures/figures_18_2.jpg", "caption": "Figure 2: Comparisons of defenses against untargeted model poisoning attacks (i.e., LMP and RL) on MNIST and CIFAR-10. All parameters are set as default and random seeds are fixed.", "description": "This figure compares the performance of different defense mechanisms against untargeted model poisoning attacks (LMP and RL) on two datasets, MNIST and CIFAR-10. The defenses compared include Krum, meta-SG+, meta-SG, and meta-RL. The results show that meta-SG+ and meta-SG generally perform better than Krum and meta-RL, indicating their effectiveness in mitigating untargeted model poisoning attacks. The x-axis represents the number of federated learning rounds, and the y-axis represents the accuracy.", "section": "4.2 Experiment Results"}, {"figure_path": "UTwuDTpdNO/figures/figures_19_1.jpg", "caption": "Figure 10: Comparisons of baseline defenses, i.e., NeuroClip, Prun, ClipMed, FLTrust+NeuroClip (from left to right) and whitebox/blackbox meta-SG under RL-based backdoor attack (BRL) on CIFAR-10. The BRLs are trained before FL round 0 against the associate defenses (i.e., NeuroClip, Prun, ClipMed, FLTrust+NC and meta-policy of meta-SG). Other parameters are set as default and all random seeds are fixed.", "description": "This figure compares the performance of several baseline defenses (NeuroClip, Prun, ClipMed, FLTrust+NeuroClip) and the proposed meta-SG (both white-box and black-box versions) against RL-based backdoor attacks on the CIFAR-10 dataset.  The RL-based attacks are pre-trained against the corresponding baseline defenses.  The plots show model accuracy over training rounds, highlighting the resilience of the meta-SG approach, particularly the white-box version, against these attacks. The black-box meta-SG, lacking complete knowledge of the attack, still demonstrates improved robustness compared to the baselines.", "section": "4.2 Experiment Results"}, {"figure_path": "UTwuDTpdNO/figures/figures_20_1.jpg", "caption": "Figure 11: Ablation studies. (a)-(b): uncertain backdoor target and unknown backdoor triggers, where the meta-policies are trained by worst-case triggers generated from GAN-based models [12] or targeting multiple labels on CIFAR-10 during pre-training and utilizing inverting gradient [19] and reverse engineering [61] during online adaptation. (c)-(d): meta-RL tested by the number of malicious clients in [20%, 30%, 40%] and non-i.i.d. level in q = [0.5, 0.6, 0.7] on MNIST compared with Krum and ClipMed under LMP attack. Other parameters are set as default.", "description": "This figure presents ablation studies on the proposed meta-Stackelberg learning defense.  Subfigures (a) and (b) show the model accuracy over time when the backdoor attack has an uncertain target label and uses an unknown trigger pattern; the results highlight the advantage of the graybox approach. Subfigures (c) and (d) demonstrate the robustness of the meta-learning model against different numbers of malicious clients and varying levels of non-i.i.d. data.", "section": "4.2 Experiment Results"}]