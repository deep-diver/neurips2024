[{"figure_path": "UTwuDTpdNO/tables/tables_6_1.jpg", "caption": "Table 1: Comparisons of average global model accuracy (acc: higher the better) and backdoor accuracy (bac: lower the better) after 500 rounds under single/multiple type attacks on CIFAR-10. All parameters are set as default and random seeds are fixed.", "description": "This table compares the performance of various defense methods against different types of attacks (single or multiple) on the CIFAR-10 dataset.  It shows the average global model accuracy and backdoor accuracy after 500 rounds of federated learning. The results highlight the effectiveness of different defense strategies against various attack combinations.", "section": "4.2 Experiment Results"}, {"figure_path": "UTwuDTpdNO/tables/tables_7_1.jpg", "caption": "Table 1: Comparisons of average global model accuracy (acc: higher the better) and backdoor accuracy (bac: lower the better) after 500 rounds under single/multiple type attacks on CIFAR-10. All parameters are set as default and random seeds are fixed.", "description": "This table compares the performance of different defense methods against various attacks (untargeted poisoning and backdoor attacks, both single and mixed types).  The metrics used are global model accuracy (higher is better) and backdoor accuracy (lower is better). The results show how each defense method handles different attack scenarios, indicating its robustness.  The table highlights the effectiveness of the proposed meta-RL defense method.", "section": "4.2 Experiment Results"}, {"figure_path": "UTwuDTpdNO/tables/tables_14_1.jpg", "caption": "Table 2: A table showcasing all attacks in the experiments, with their corresponding categories and adaptivities.", "description": "This table lists the different attack types used in the experiments, categorized by whether they are untargeted model poisoning attacks or backdoor attacks, and whether they are adaptive or non-adaptive.  Adaptive attacks change their strategy over time, while non-adaptive attacks use a fixed strategy.", "section": "Experiment Settings"}, {"figure_path": "UTwuDTpdNO/tables/tables_15_1.jpg", "caption": "Table 1: Comparisons of average global model accuracy (acc: higher the better) and backdoor accuracy (bac: lower the better) after 500 rounds under single/multiple type attacks on CIFAR-10. All parameters are set as default and random seeds are fixed.", "description": "This table compares the performance of different defense methods against various single and mixed attacks (untargeted and backdoor) on the CIFAR-10 dataset.  The results are presented in terms of average global model accuracy and backdoor accuracy, evaluated after 500 rounds of federated learning.  The table helps to assess the effectiveness of each defense against different attack scenarios.", "section": "4.2 Experiment Results"}, {"figure_path": "UTwuDTpdNO/tables/tables_19_1.jpg", "caption": "Table 4: Comparisons of average backdoor accuracy (lower the better) after 250 FL rounds under backdoor attacks and defenses on MNIST. All parameters are set as default and all random seeds are fixed.", "description": "This table compares the backdoor accuracy of three different defense methods (Krum, CRFL, and Meta-SG) against three different backdoor attacks (BFL, DBA, and BRL) on the MNIST dataset.  Lower values indicate better defense performance.  The Meta-SG method, proposed in the paper, shows significantly lower backdoor accuracy than the other two methods, suggesting it is more effective at defending against these attacks.", "section": "4.2 Experiment Results"}, {"figure_path": "UTwuDTpdNO/tables/tables_19_2.jpg", "caption": "Table 1: Comparisons of average global model accuracy (acc: higher the better) and backdoor accuracy (bac: lower the better) after 500 rounds under single/multiple type attacks on CIFAR-10. All parameters are set as default and random seeds are fixed.", "description": "This table compares the performance of different defense methods against various attacks on the CIFAR-10 dataset.  The methods are evaluated based on their average global model accuracy and backdoor accuracy after 500 rounds of federated learning.  The attacks include single and multiple attack types, and the results highlight the effectiveness of different defense mechanisms.", "section": "4.2 Experiment Results"}, {"figure_path": "UTwuDTpdNO/tables/tables_21_1.jpg", "caption": "Table 1: Comparisons of average global model accuracy (acc: higher the better) and backdoor accuracy (bac: lower the better) after 500 rounds under single/multiple type attacks on CIFAR-10. All parameters are set as default and random seeds are fixed.", "description": "This table compares the performance of different defense methods against various single and mixed attacks on the CIFAR-10 dataset.  The methods include FedAvg (baseline), Trimmed Mean, FLTrust, ClipMed (a combination of Norm Bounding and Coordinate-wise Median), FLTrust+NC (FLTrust and NeuroClip), and the proposed Meta-RL method.  The attacks include untargeted model poisoning attacks (IPM, LMP) and backdoor attacks (BFL, DBA), as well as combinations of these attacks.  The table shows the average global model accuracy and backdoor accuracy after 500 rounds of federated learning. Random seeds were fixed to ensure fair comparison.", "section": "4.2 Experiment Results"}, {"figure_path": "UTwuDTpdNO/tables/tables_21_2.jpg", "caption": "Table 1: Comparisons of average global model accuracy (acc: higher the better) and backdoor accuracy (bac: lower the better) after 500 rounds under single/multiple type attacks on CIFAR-10. All parameters are set as default and random seeds are fixed.", "description": "This table compares the performance of different defense mechanisms against various single and mixed attacks on the CIFAR-10 dataset.  The results show global model accuracy and backdoor accuracy after 500 rounds of federated learning.  The table allows for a comparison of the effectiveness of various defenses in the presence of different attack types and combinations.", "section": "4.2 Experiment Results"}, {"figure_path": "UTwuDTpdNO/tables/tables_21_3.jpg", "caption": "Table 8: Comparisons of average model accuracy after 250 FL rounds under different adaptive attacks on MNIST. All parameters are set as default and all random seeds are fixed.", "description": "This table compares the model accuracy achieved after 250 rounds of federated learning (FL) under three different adaptive attack methods: 3-dimensional RL, Adaptive LMP, and 1-dimensional RL.  The results show the effectiveness of different attacks in reducing model accuracy.", "section": "4.2 Experiment Results"}]