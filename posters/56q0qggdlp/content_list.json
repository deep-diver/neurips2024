[{"type": "text", "text": "Molecule Generation with Fragment Retrieval Augmentation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Seul Lee1\u2217 Karsten Kreis2 Srimukh Prasad Veccham2 Meng Liu2 Danny Reidenbach2 Saee Paliwal2 Arash Vahdat2\u2020 Weili Nie2\u2020 1KAIST 2NVIDIA ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "seul.lee@kaist.ac.kr {kkreis,sveccham,menliu,dreidenbach,saeep,avahdat,wnie}@nvidia.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Fragment-based drug discovery, in which molecular fragments are assembled into new molecules with desirable biochemical properties, has achieved great success. However, many fragment-based molecule generation methods show limited exploration beyond the existing fragments in the database as they only reassemble or slightly modify the given ones. To tackle this problem, we propose a new fragmentbased molecule generation framework with retrieval augmentation, namely Fragment Retrieval-Augmented Generation $f$ -RAG). $f$ -RAG is based on a pre-trained molecular generative model that proposes additional fragments from input fragments to complete and generate a new molecule. Given a fragment vocabulary, $f$ -RAG retrieves two types of fragments: (1) hard fragments, which serve as building blocks that will be explicitly included in the newly generated molecule, and (2) soft fragments, which serve as reference to guide the generation of new fragments through a trainable fragment injection module. To extrapolate beyond the existing fragments, $f$ -RAG updates the fragment vocabulary with generated fragments via an iterative refinement process which is further enhanced with post-hoc genetic fragment modification. $f$ -RAG can achieve an improved exploration-exploitation trade-off by maintaining a pool of fragments and expanding it with novel and high-quality fragments through a strong generative prior. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The goal of small molecule drug discovery is to discover molecules with specific biochemical target properties, such as synthesizability [9], non-toxicity [40], solubility [31] and binding affinity, in the vast chemical space. Fragment-based drug discovery (FBDD) has been considered as an effective approach to explore the chemical space and has resulted in many successful marketed drugs [27]. Contrary to high-throughput screening (HTS) [41] that searches from a library of drug-like molecules, FBDD constructs a library of molecular fragments to synthesize new molecules beyond the existing molecule library, leading to a better chemical coverage [5]. ", "page_idx": 0}, {"type": "text", "text": "Recently, generative models have been adopted in the field of FBDD to accelerate the process of searching for drug candidates [18, 45, 46, 30, 20, 11, 25]. These methods reassemble or slightly modify the fragments to generate new molecules with the knowledge of the generative model. As the methods are allowed to exploit chemical knowledge in the form of fragments to narrow the search space, they have shown meaningful success in generating optimized molecules. However, their performance is largely limited by the existing library of molecular fragments, where discovering new fragments is either impossible or highly limited. Some of the methods [14, 38, 25] suggested making modifications to existing fragments, but the modifications are only applied to small and local substructures and still heavily dependent on the existing fragments. The limited exploration beyond known fragments greatly hinders the possibility of generating diverse and novel drug candidates that may exhibit better target properties. Therefore, it is a challenge to improve the ability of discovering novel high-quality fragments, while exploiting existing chemical space effectively. ", "page_idx": 0}, {"type": "image", "img_path": "56Q0qggDlp/tmp/159ad89d0203ac7a91c5b724e8c7c0fa688f54893eb58a465d2d3253a74f46ee.jpg", "img_caption": ["Figure 1: A radar plot of target properties. $f$ -RAG strikes better balance among optimization performance, diversity, novelty, and synthesizability than the state-of-the-art techniques on the PMO benchmark [10]. "], "img_footnote": [], "page_idx": 0}, {"type": "image", "img_path": "56Q0qggDlp/tmp/3af5ff321aecf4c9b1dfbdbb6d56a9ad659668de4d3a11ac5c5f0fbefaf40c03.jpg", "img_caption": ["Figure 2: The overall framework of $f$ -RAG. After an initial fragment vocabulary is constructed from an existing molecule library, two types of fragments are retrieved during generation. Hard fragments are explicitly included in the newly generated molecules, while soft fragments implicitly guide the generation of new fragments. SAFE-GPT generates a molecule using hard fragments as input, while the fragment injection module in the middle of the SAFE-GPT layers injects the embeddings of soft fragments into the input embedding. After the generation, the molecule population and fragment vocabulary are updated with the newly generated molecule and its fragments, respectively. The exploration is further enhanced with genetic fragment modification, which also updates the fragment vocabulary and molecule population. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To this end, we propose a fragment-based molecule generation framework leveraging retrievalaugmented generation (RAG) [26], namely Fragment Retrieval-Augmented Generation ( $f$ -RAG). As shown in Figure 2, $f$ -RAG augments the pre-trained molecular language model SAFE-GPT [34] with two types of retrieved fragments: hard fragments and soft fragments for a better explorationexploitation trade-off. Specifically, we first construct a fragment vocabulary by decomposing known molecules from the existing library into fragments and scoring the fragments by measuring their contribution to target properties. From the fragment vocabulary, $f$ -RAG first retrieves fragments that will be explicitly included in the new molecule (i.e., hard fragments). Hard fragments serve as the input context to the molecular language model that predicts the remaining fragments. In addition to retrieval of hard fragments, $f$ -RAG retrieves fragments that will not be part of the generated molecule but provide informative guidance on predicting novel, diverse molecules (i.e., soft fragments). Concretely, the embeddings of soft fragments are fused with the embeddings of hard fragments (or input embeddings) through a lightweight fragment injection module in the middle of SAFE-GPT. The fragment injection module allows SAFE-GPT to generate new fragments by referring to the information conveyed by soft fragments. ", "page_idx": 1}, {"type": "text", "text": "During training, we only update the fragment injection module while keeping SAFE-GPT frozen. Inspired by Wang et al. [42], we train $f$ -RAG to learn how to leverage the retrieved fragments for molecule generation, using a self-supervised loss that predicts the most similar one in the set of soft fragments. At inference, $f$ -RAG dynamically updates the fragment vocabulary with newly generated fragments via an iterative refinement process. To further enhance exploration, we propose to modify the generated fragments from SAFE-GPT with a post-hoc genetic fragment modification process. The proposed $f$ -RAG takes the advantages of both hard and soft fragment retrieval to achieve an improved exploration-exploitation trade-off. We verify $f$ -RAG on various molecular optimization tasks, by examining the optimization performance, along with diversity, novelty, and synthesizability. As shown in Figure 1, $f$ -RAG exhibits the best balance across these essential considerations, demonstrating its applicability as a promising tool for drug discovery. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "We summarize our contributions as follows: ", "page_idx": 2}, {"type": "text", "text": "\u2022 We introduce $f$ -RAG, a novel molecular generative framework that combines FBDD and RAG.   \n\u2022 We propose a retrieval augmentation strategy that operates at the fragment level with two types of fragments, allowing fine-grained guidance to achieve an improved exploration-exploitation trade-off and generate high-quality drug candidates.   \n\u2022 Through extensive experiments, we demonstrate the effectiveness of $f$ -RAG in various drug discovery tasks that simulate real-world scenarios. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Fragment-based molecule generation. Fragment-based molecular generative models refer to a class of methods that reassemble existing molecular substructures (i.e., fragments) to generate new molecules. Jin et al. [18] proposed to find big molecular substructures that satisfy the given chemical properties, and learn to complete the substructures into final molecules by adding small branches. Xie et al. [45] proposed to progressively add or delete fragments using Markov chain Monte Carlo (MCMC) sampling. Yang et al. [46] proposed to use reinforcement learning (RL) to assemble fragments, while Maziarz et al. [30], Kong et al. [20], and Geng et al. [11] used VAE-based techniques. Lee et al. [25] proposed to take the target chemical properties into account in the fragment vocabulary construction and used a combination of RL and a genetic algorithm (GA) to assemble and modify the fragments. On the other hand, graph-based GAs [14, 38] decompose parent molecules into fragments that are combined to generate an offspring molecule, and are also mutated with a small probability. Since fragment-based strategies are limited by generating molecules outside of the possible combinations of existing fragments, they suffer from limited exploration in the chemical space. Some of the methods [14, 38, 25] suggest making modifications to existing fragments to overcome this problem, but this is not a fundamental solution because the modifications are only local, still being based on the existing fragments. ", "page_idx": 2}, {"type": "text", "text": "Retrieval-augmented molecule generation. Retrieval-augmented generation (RAG) [26] refers to a technique that retrieves context from external data databases to guide the generation of a generative model. Recently, RAG has gained attention as a means to enhance accuracy and reliability of large language models (LLMs) [6, 36, 3]. In the field of molecule optimization, Liu et al. [28] developed a text-based drug editing framework that repurposes a conversational language model for solving molecular tasks, where the domain knowledge is injected by a retrieval and feedback module. More related to us, Wang et al. [42] proposed to use a pre-trained molecular language model to generate molecules, while augmenting the generation with retrieved molecules that have high target properties. Contrary to this method that retrieves molecules, our proposed $f$ -RAG employs a fine-grained retrieval augmentation scheme that operates at the fragment level to achieve an improved exploration-exploitation trade-off. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we introduce our Fragment Retrieval-Augmented Generation ( $f$ -RAG) framework. $f$ -RAG aims to generate optimized molecules by leveraging existing chemical knowledge through RAG, while exploring beyond the known chemical space under the fragment-based drug discovery (FBDD) paradigm. We first introduce the hard fragment retrieval in Sec. 3.1. Then, we present the soft fragment retrieval in Sec. 3.2. Lastly, we describe the genetic fragment modification in Sec. 3.3. ", "page_idx": 2}, {"type": "text", "text": "3.1 Hard Fragment Retrieval ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given a set of $N$ molecules $x_{i}$ and their corresponding properties $y_{i}\\;\\in\\;[0,1]$ , denoted as ${\\mathcal{D}}=$ $\\{(x_{i},y_{i})\\}_{i=1}^{N}$ , we first construct a fragment vocabulary. We adopt an arm-linker-arm slicing algorithm provided by Noutahi et al. [34] which decomposes a molecule $x$ into three fragments: two arms $F_{\\mathrm{arm}}$ (i.e., fragments that have one attachment point) and one linker $F_{\\mathrm{linker}}$ (i.e., a fragment that has two attachment points). Decomposing molecules into arms and linkers (or scaffolds) is a popular approach utilized in various drug discovery strategies, such as scaffold decoration or scaffold hopping [37]. We ignored the molecules in the training set that cannot be decomposed, and a set of arms ${\\mathcal{F}}_{\\mathrm{arm}}=$ $\\{F_{\\mathrm{arm},j}\\}_{j=1}^{2N}$ and a set of linkers $\\mathcal{F}_{\\mathrm{linker}}\\,\\,{\\bar{=}}\\,\\,\\{F_{\\mathrm{linker},j}\\}_{j=1}^{N}$ are obtained after the algorithm is applied to the molecules $\\{x_{i}\\}_{i=1}^{N}$ . Subsequently, we calculate the score of each fragment $F_{j}\\in{\\mathcal{F}}_{\\mathrm{arm}}\\cup{\\mathcal{F}}_{\\mathrm{linker}}$ using the average property of all molecules containing $F_{j}$ as their substructure as follows: ", "page_idx": 2}, {"type": "image", "img_path": "56Q0qggDlp/tmp/22072abbf7b4ea3bf486305a826985e65ced9de3679cf49704ebcea13ed2e512.jpg", "img_caption": ["Figure 3: Hard fragment retrieval of $f$ -RAG. With a probability of $50\\%$ , $f$ -RAG either retrieves two arms as hard fragments for linker design (top) or one arm and one linker as hard fragments for motif extension (bottom). "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname{score}(F_{j})={\\frac{1}{|S(F_{j})|}}\\sum_{(x,y)\\in S(F_{j})}y,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathrm{score}(F_{j})\\,\\in\\,[0,1]$ , and $S(F_{j})\\;=\\;\\{(x,y)\\;\\in\\;{\\mathcal D}\\;:\\;F_{j}$ is a fragment of $x\\}$ . Intuitively, the fragment score evaluates the contribution of a given fragment to the target property of the whole molecule of which it is a part. From ${\\mathcal{F}}_{\\mathrm{arm}}$ and ${\\mathcal{F}}_{\\mathrm{linker}}$ , we choose the top- $\\cdot N_{\\mathrm{frag}}$ fragments based on the score to construct the arm fragment vocabulary $\\mathcal{V}_{\\mathrm{arm}}\\subset\\mathcal{F}_{\\mathrm{arm}}$ and the linker fragment vocabulary $\\mathcal{V}_{\\mathrm{linker}}\\subset\\mathcal{F}_{\\mathrm{linker}}$ , respectively. ", "page_idx": 3}, {"type": "text", "text": "Given the fragment vocabularies $\\mathcal{V}_{\\mathrm{arm}}$ and $\\rangle_{\\mathrm{linker}}$ consisting of high-property fragments, two hard fragments are randomly retrieved from the vocabularies. The hard fragments together form a partial molecular sequence that serves as input to a pre-trained molecular language model. In this work, we employ Sequential Attachment-based Fragment Embedding (SAFE) [34] as the molecular representation and SAFE-GPT [34] as the backbone generative model of $f$ -RAG. SAFE is a noncanonical version of simplified molecular-input line-entry system (SMILES) [43] that represents molecules as a sequence of dot-connected fragments. Importantly, the order of fragments in a SAFE string does not affect the molecular identity. Using the SAFE representation, $f$ -RAG forces the hard fragments to be included in a newly generated molecule by providing them as an input sequence to the language model to complete the rest of the sequence. ", "page_idx": 3}, {"type": "text", "text": "During generation, with a probability of $50\\%$ , $f$ -RAG either (1) retrieves two hard fragments from $\\mathcal{V}_{\\mathrm{arm}}$ or (2) retrieves one from $\\mathcal{V}_{\\mathrm{arm}}$ and one from $\\rangle_{\\mathrm{linker}}$ . In the former case, $f$ -RAG performs linker design, which generates a new fragment that links the input fragments. In the latter case, $f$ -RAG first randomly selects an attachment point in the retrieved linker and combines it with the retrieved arm to form a single fragment, and then performs motif extension, which generates a new fragment that completes the molecule (Figure 3). ", "page_idx": 3}, {"type": "text", "text": "3.2 Soft Fragment Retrieval ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given two hard fragments as input, the molecular language model generates one new fragment to complete a molecule. Instead of relying solely on the model to generate the new fragment, we propose to augment the generation with the information of $K$ retrieved fragments, which we refer to as soft fragments, to guide the generation. Specifically, if the two hard fragments are all arms, $f$ -RAG randomly retrieves soft fragments from $\\rangle_{\\mathrm{linker}}$ . If one of the hard fragments is an arm and another is a linker, $f$ -RAG randomly retrieves soft fragments from $\\mathcal{V}_{\\mathrm{arm}}$ . Using up to the $L$ -th layer of the lauguage model $\\boldsymbol{\\mathrm{LM}}^{0:L}$ , the embeddings of the input sequence $x_{\\mathrm{{input}}}$ and the soft fragments $\\{F_{\\mathrm{soft},k}\\}_{k=1}^{K}$ are obtained as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\nh_{\\mathrm{input}}=\\mathrm{LM}^{0:L}(x_{\\mathrm{input}})\\;\\;\\mathrm{and}\\;\\;H_{\\mathrm{soft}}=\\mathrm{concatenate}([h_{\\mathrm{soft}}^{1},h_{\\mathrm{soft}}^{2},\\dots,h_{\\mathrm{soft}}^{K}]),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "56Q0qggDlp/tmp/72d6e19c47a42fff50630b65d315dc8fa36f7e9a9788dfcda7ee1b2b11b1618a.jpg", "img_caption": ["Figure 4: The self-supervised training process of the fragment injection module of $f$ -RAG. $F^{k\\mathrm{NN}}$ denotes the $k$ -th most similar fragment to $F$ . Using $F_{1}$ and $F_{2}$ as hard fragments, while using $F_{3}$ and its neighbors {F 3kNN}kK=2 as soft fragments, the training objective is to predict $\\check{F}_{3}^{1N N}$ "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Subsequently, $f$ -RAG injects the embeddings of soft fragments through a trainable fragment injection module. Following Wang et al. [42], the fragment injection module uses cross-attention to fuse the embeddings of the input sequence and soft fragments as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nh=\\mathrm{FI}(h_{\\mathrm{input}},H_{\\mathrm{soft}})=\\mathrm{softmax}\\left(\\frac{\\mathrm{Query}(h_{\\mathrm{input}})\\cdot\\mathrm{Key}(H_{\\mathrm{soft}})^{\\top}}{\\sqrt{d_{\\mathrm{Key}}}}\\right)\\cdot\\mathrm{Value}(H_{\\mathrm{soft}}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where FI is the fragment injection module, Query, Key, and Value are multi-layer perceptrons (MLPs), and $d_{\\mathrm{Key}}$ is the output dimension of Key. Next, a molecule is generated by decoding the augmented embedding $^h$ using the later layers of the language model as $x_{\\mathrm{new}}=\\mathrm{L}\\mathbf{M}^{L+1:L_{T}}(\\pmb{h})$ , where $L_{T}$ is the total number of layers of the model. With the fragment injection module, $f$ -RAG can utilize information of soft fragments to generate novel fragments which are also likely to contribute to the high target properties. During generation, the fragment vocabulary is dynamically updated through an iterative process that scores newly generated fragments based on Eq. (1) and replaces fragments in the fragment vocabulary to the top- $N_{\\mathrm{frag}}$ fragments. ", "page_idx": 4}, {"type": "text", "text": "Next, we need to train $f$ -RAG to learn how to augment the retrieved soft fragments into the molecule generation. To retain the high generation quality of SAFE-GPT and make the training process efficient, we keep the backbone language model frozen and only train the lightweight fragment injection module. Inspired by Wang et al. [42], we propose a new self-supervised objective that predicts the most similar fragment to the input fragments. Specifically, each molecular sequence $x$ in the training set is first decomposed into fragment sequences $(F_{1},F_{2},F_{3})$ with a random permutation between the fragments, using the same slicing algorithm used in the vocabulary construction. Importantly, using the SAFE representation, $x$ can be simply represented by connecting its fragments with dots as $F_{1}.F_{2}.F_{3}$ . We consider the first two fragments as hard fragments. Given the remaining fragment $F_{3}$ , we retrieve its $K$ most similar fragments $\\{F_{3}^{k N N}\\}_{k=1}^{K}$ from the training fragment pool. Here, we use the pairwise Tanimoto similarity using Morgan fingerprints of radius 2 and 1024 bits. Using the hard fragments as the input sequence as $F_{1}.F_{2}$ , the objective is to predict the most similar fragment $F_{3}^{1N N}$ utilizing the original fragment and the next $K-1$ most similar fragments $\\{F_{3}^{k N N}\\}_{k=2}^{K}$ }kK=2 as the soft fragments. The training process is illustrated in Figure 4, and the details are provided in Section D.1. ", "page_idx": 4}, {"type": "text", "text": "Note that the training of the fragment injection module is target property-agnostic, as the fragments used for training are independent of the target property. In contrast, the fragment vocabularies used for generation are target property-specific, as it is constructed using the scoring function in Eq. (1). This allows $f$ -RAG to effectively generate optimized molecules across different target properties without any retraining. ", "page_idx": 4}, {"type": "text", "text": "3.3 Genetic Fragment Modification ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To further enhance exploration in the chemical space, we propose to modify the generated fragments with a post-hoc genetic algorithm (GA). Specifically, we adopt the operations of Graph GA [14]. We first initialize the population $\\mathcal{P}$ with the top- $N_{\\mathrm{mol}}$ molecules generated by our fragment retrievalaugmented SAFE-GPT based on the target property $y$ . Parent molecules are then randomly selected from the population and offspring molecules are generated by the crossover and mutation operations (see Jensen [14] for more details). The offspring molecules can have new fragments not contained in the initial fragment vocabulary, and the fragment vocabularies $\\mathcal{V}_{\\mathrm{arm}}$ and $\\mathcal{V}_{\\mathrm{linker}}$ are again updated by the top- $N_{\\mathrm{frag}}$ fragments based on the scores of Eq. (1). In the subsequent generation, the population $\\mathcal{P}$ is updated with the generated molecules so far by both SAFE-GPT and GA. ", "page_idx": 4}, {"type": "text", "text": "As shown in Figure 2, $f$ -RAG generates desirible molecules through multiple cycles of (1) the SAFE-GPT generation augmented with the hard fragment retrieval (Section 3.1) and the soft fragment retrieval (Section 3.2), and (2) the GA generation (Section 3.3). Through this interplay of hard fragment retrieval, soft fragment retrieval, and the genetic fragment modification, $f$ -RAG can exploit existing chemical knowledge through the form of fragments both explicitly and implicitly, while exploring beyond initial fragments by the dynamic vocabulary update. We summarize the generation process of $f$ -RAG in Algorithm 1 in Section C. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We validate $f$ -RAG on molecule generation tasks that simulate various real-world drug discovery problems. We first conduct experiments on the practical molecular optimization (PMO) benchmark [10] in Section 4.1. We then conduct experiments to generate novel molecules that have high binding affinity, drug-likeness, and synthesizability in Section 4.2. We further perform analyses in Section 4.3. ", "page_idx": 5}, {"type": "text", "text": "4.1 Experiments on PMO Benchmark ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Setup. We demonstrate the efficacy of $f$ -RAG on the 23 tasks from the PMO benchmark. Following the standard setting of the benchmark, we set the maximum number of oracle calls to 10,000 and evaluate optimization performance with the area under the curve (AUC) of the average property scores of the top-10 molecules versus oracle calls. In addition, we evaluate diversity, novelty, and synthesizability of generated molecules, other essential considerations in drug discovery. We use the Therapeutics Data Commons (TDC) library [12] to calculate diversity. Following previous works [18, 45, 24, 25], novelty is defined as the fraction of molecules that have the maximum Tanimoto similarity less than 0.4 with the training molecules. We use the synthetic accessibility (SA) [8] score of the TDC library to measure synthesizability. These values are measured using the top-100 generated molecules, following the setting of the benchmark. Further explanation on the evaluation metrics and experimental setup are provided in Section D.3. ", "page_idx": 5}, {"type": "text", "text": "Baselines. We employ the top-7 methods reported by the PMO benchmark and two recent state-ofthe-art methods as our baselines. Graph GA [14] is a GA with a fragment-level crossover operation, and Mol GA [38] is a more hyperparameter-tuned version of Graph GA. Genetic GFN [19] is a method that uses Graph GA to guide generation of a GFlowNet. REINVENT [35] is a SMILESbased RL model and SELFIES-REINVENT is a modified REINVENT that uses the self-referencing embedded strings (SELFIES) [21] representation. GP BO [39] is a method that optimizes the Gaussian process acquisition function with Graph GA. STONED [33] is a GA-based model that operates on SELFIES strings. LSTM HC [7] is a SMILES-based LSTM model. SMILES GA [47] is a GA whose genetic operations are based on the SMILES context-free grammar. ", "page_idx": 5}, {"type": "text", "text": "Results. The results of optimization performance are shown in Table 1. $f$ -RAG outperforms the previous methods in terms of the sum of the AUC top-10 values and achieves the highest AUC top-10 values in 12 out of 23 tasks, demonstrating that the proposed combination of hard fragment retrieval, soft fragment retrieval, and genetic fragment modification is highly effective in discovering optimized drug candidates that have high target properties. On the other hand, the average scores of diversity, novelty, and synthesizability of the generated molecules are summarized in Table 2, and the full results are presented in Tables 4, 5, and 6. As shown in these tables, $f$ -RAG achieves the best diversity and synthesizability, and the second best novelty. Notably, $f$ -RAG shows the highest diversity in 12 out of 23 tasks, and the highest synthesizability in 19 out of 23 tasks. Note that the high novelty value of Mol GA comes at the cost of other important factors, i.e., optimization performance, diversity, and synthesizability. The essential considerations in drug discovery often conflict with each other, making the drug discovery problem challenging. These trade-offs are also visualized in Figure 1 and Figure 7, and $f$ -RAG effectively improves the trade-offs by utilizing existing fragments while dynamically updating the fragment vocabulary with newly proposed fragments. ", "page_idx": 5}, {"type": "text", "text": "4.2 Optimization of Docking Score under QED, SA, and Novelty Constraints ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Setup. Following Lee et al. [24] and Lee et al. [25], we validate $f$ -RAG in a set of tasks that aim to optimize the binding affinity against a target protein while also maintaining high drug-likeness, synthesizability, and novelty. Following the previous works, we use docking score calculated by QuickVina 2 [2] with five protein targets, parp1, fa7, 5ht1b, braf, and jak2, to measure binding affinity. We use quantitative estimates of drug-likeness (QED) [4] and SA [8] to measure drug-likeness and synthesizability, respectively. Following the previous works, we set the target property $y$ as follows: ", "page_idx": 5}, {"type": "table", "img_path": "56Q0qggDlp/tmp/927dc63e0166ccd7ebf85e88a3dabf4a0c28d5adc893083828b81637593845ee.jpg", "table_caption": ["Table 1: PMO AUC top-10 results. The results are the mean and standard deviation of 3 independent runs. The results for Genetic GFN [19] and Mol GA [38] are taken from the respective original papers and the results for other baselines are taken from Gao et al. [10]. The best results are highlighted in bold. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "56Q0qggDlp/tmp/b127aba25f5d74ad2d7d377925fa12521d100e3d63b98e55d312d35460211517.jpg", "table_caption": ["Table 2: Top-100 diversity, top-100 novelty, and top-100 SA score results. The results are the average values of all 23 tasks. The best results are highlighted in bold. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "56Q0qggDlp/tmp/d9e7cc513d5addd7f4d2a630de48d9c34378a3580f90d4260961fcdeb44337de.jpg", "table_caption": ["Table 3: Novel top ${\\bf5\\%}$ docking score (kcal/mol) results. The results are the means and standard deviations of 3 independent runs. The results for RationaleRL, PS-VAE, RetMol, and GEAM are taken from Lee et al. [25]. Other baseline results except for Genetic GFN are taken from Lee et al. [24]. Lower is better, and the best results are highlighted in bold. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\ny={\\widehat{\\mathrm{DS}}}\\times{\\mathrm{QED}}\\times{\\widehat{\\mathrm{SA}}}\\in[0,1],\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\widehat{\\sf D S}$ and $\\widehat{\\sf S A}$ are the normalized DS and SA according to Eq. (7) in Section D.4. We generate 3,000  molecule s and evaluate them using novel top ${\\pmb5}\\%$ DS (kcal/mol), which indicates the mean DS of the top $5\\%$ unique and novel hits. Here, novel hits are defined as the molecules that satisfy all the following criteria: (the maximum similarity with the training molecules) $<0.4$ , $\\mathrm{DS}<$ (the median DS of known active molecules), $\\mathrm{QED}>0.5$ , and $\\mathrm{SA}<5$ . Further details are provided in Section D.4. ", "page_idx": 7}, {"type": "text", "text": "Baselines. JT-VAE [16], HierVAE [17], MARS [45], RationaleRL [18], FREED [46], PSVAE [20], and GEAM [25] are the methods that first construct a fragment vocabulary using a molecular dataset, then learn to assemble those fragments to generate new molecules. On the other hand, Graph GA [14], GEGL [1], and Genetic GFN [19] are GA-based methods that utilize the information of fragments by adopting the fragment-based crossover operation. $\\mathbf{GA+D}$ [32] is a discriminator-enhanced GA method that operates on the SELFIES representation. RetMol [42] is a retrieval-based method that uses retrieved example molecules to augment the generation of a pre-trained molecular language model. REINVENT [35] and MORLD [15] are RL models that operate on SMILES and graph molecular representations, respectively. MOOD [24] is a diffusion model that employs an out-of-distribution control to improve novelty. ", "page_idx": 7}, {"type": "text", "text": "Results. The results are shown in Table 3. In all five tasks, $f$ -RAG outperforms all the baselines. Note that the evaluation metric, novel top $5\\%$ DS, is designed to reflect the nature of the drug discovery problem, which optimizes multiple target properties by finding a good balance between exploration and exploitation. The results demonstrate the superiority of $f$ -RAG in generating drug-like, synthesizable, and novel drug candidates that have high binding affinity to the target protein with the improved exploration-exploitation trade-off. We additionally visualize the generated molecules in Figure 8. ", "page_idx": 7}, {"type": "text", "text": "Additional comparison with GEAM. To further justify the advantage of $f$ -RAG over GEAM, we additionally include the results of seven multi-property optimization (MPO) tasks in the PMO benchmark (Section 4.1) in Table 7. As we can see, $f$ -RAG significantly outperforms GEAM in all the tasks, validating its applicability to a wide range of drug discovery problems. ", "page_idx": 7}, {"type": "image", "img_path": "56Q0qggDlp/tmp/8219dcee9f26a894cf165b5e6f840201fe9cd42ddc8103a3e0d8d7825dd91181.jpg", "img_caption": ["Figure 5: (a) The optimization curves in the deco_hop task of the PMO benchmark of the ablated $f$ -RAGs. Solid lines denote the mean and shaded areas denote the standard deviation of 3 independent runs. (b) Overall results of the ablated $f$ -RAGs. (c) Results with different values of $\\delta$ of the similarity-based fragment filter. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.3 Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Ablation study. To verify the effect of each component of $f$ -RAG, we conduct experiments with various combinations of the components, i.e., hard fragment retrieval (hard), soft fragment retrieval (soft), and genetic fragment modification (GA), in Figure 5(a) and Figure 5(b). The detailed results are shown in Table 8 and Table 9. For $f$ -RAG $\\mathrm{{soft}{+}G A)}$ that does not utilize the hard fragment retrieval, we randomly initialized the fragment vocabularies instead of selecting the top- $\\cdot N_{\\mathrm{frag}}$ fragments based on Eq. (1) and let the model use random hard fragments. Note that even though $f$ -RAG (hard+soft), $f$ -RAG that does not utilize the genetic fragment modification, shows very high diversity, the value is not meaningful as the generated molecules are not highly optimized, not novel, and not synthesizable. The same applies to the high novelty of $f$ -RAG $\\mathrm{{\\bf{so}}{\\bf{f t}}{+}{\\bf{G}}{\\bf{A}}})$ and $f$ -RAG (GA). On the contrary, the full $f$ -RAG achieves the best optimization performance and synthesizability while showing reasonably high diversity and novelty. Importantly, $f$ -RAG outperforms $f$ -RAG (hard+GA) in all the metrics, especially for diversity and novelty, indicating the soft fragment retrieval aids the model in generating more diverse and novel drug candidates while maintaining their high target properties. ", "page_idx": 8}, {"type": "text", "text": "Controlling optimization performance-diversity trade-off. It is well-known that molecular diversity is important in drug discovery, especially when the oracle has noise. However, optimization performance against the target property and molecular diversity are conflicting factors [10, 19]. To control this trade-off between optimization performance and diversity, we additionally introduce a similarity-based fragment filtering strategy, which excludes similar fragments from the fragment vocabulary. Specifically, when updating the vocabulary, new fragments that have a higher Tanimoto similarity than $\\delta$ to fragments in the vocabulary are flitered out. Morgan fingerprints of radius 2 and 1024 bits are used to calculate the Tanimoto similarity. The results of the similarity-based fragment filter with different values of $\\delta$ are shown in Figure 5(c), verifying $f$ -RAG can increase diversity at the expense of optimization performance by controlling $\\delta$ . ", "page_idx": 8}, {"type": "text", "text": "Effect of the fragment vocabulary update. To analyze the effect of the dynamic update of the fragment vocabulary during generation, we visualize the distribution of the docking scores of molecules generated by $f$ -RAG with or with the fragment vocabulary update in Figure 6. The dynamic update allows $f$ -RAG to generate more optimized molecules in terms of the target property. Notably, with the dynamic update, $f$ -RAG can discover molecules that have higher binding affinity to the target protein than the top molecule in the training set, and we visualize an example of such molecules in the figure. Note that the molecule also has low similarity (0.321) with the training molecules. This result demonstrates the explorability of $f$ -RAG to discover drug candidates that lie beyond the training distribution. ", "page_idx": 8}, {"type": "image", "img_path": "56Q0qggDlp/tmp/f2bdfb9d1c415bfb195f3498bafbf1af561a96bdc66b7b13073b3be27b92402f.jpg", "img_caption": ["Figure 6: DS distribution of molecules generated by $f$ -RAG with or without the fragment vocabulary update in the jak2 task. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We proposed $f$ -RAG, a new fragment-based molecular generative framework that utilizes hard fragment retrieval, soft fragment retrieval, and genetic fragment modification. With hard fragment retrieval, $f$ -RAG can explicitly exploit the information contained in existing fragments that contribute to the target properties. With soft fragment retrieval, $f$ -RAG can balance between exploitation of existing chemical knowledge and exploration beyond the existing fragment vocabulary. Soft fragment retrieval with SAFE-GPT allows $f$ -RAG to propose new fragments that are likely to contribute to the target chemical properties. The proposed novel fragments are then dynamically incorporated in the fragment vocabulary throughout generation. This exploration is further enhanced with genetic fragment modification, which modifies fragments with genetic operations and updates the vocabulary. Through extensive experiments, we demonstrated the efficacy of $f$ -RAG to improve the explorationexploitation trade-off. $f$ -RAG outperforms previous methods, achieving state-of-the-art performance to synthesize diverse, novel, and synthesizable drug candidates with high target properties. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We would like to thank the NVIDIA GenAIR team for their comments during the development of the framework. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Sungsoo Ahn, Junsu Kim, Hankook Lee, and Jinwoo Shin. Guiding deep molecular optimization with genetic exploration. Advances in neural information processing systems, 33:12008\u201312021, 2020.   \n[2] Amr Alhossary, Stephanus Daniel Handoko, Yuguang Mu, and Chee-Keong Kwoh. Fast, accurate, and reliable molecular docking with quickvina 2. Bioinformatics, 31(13):2214\u20132216, 2015.   \n[3] Akari Asai, Sewon Min, Zexuan Zhong, and Danqi Chen. Retrieval-based language models and applications. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 6: Tutorial Abstracts), pages 41\u201346, 2023.   \n[4] G Richard Bickerton, Gaia V Paolini, J\u00e9r\u00e9my Besnard, Sorel Muresan, and Andrew L Hopkins. Quantifying the chemical beauty of drugs. Nature chemistry, 4(2):90\u201398, 2012. [5] Marta Bon, Alan Bilsland, Justin Bower, and Kirsten McAulay. Fragment-based drug discovery\u2014the importance of high-quality molecule libraries. Molecular Oncology, 16(21): 3761\u20133777, 2022. [6] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. In International conference on machine learning, pages 2206\u20132240. PMLR, 2022. [7] Nathan Brown, Marco Fiscato, Marwin HS Segler, and Alain C Vaucher. Guacamol: benchmarking models for de novo molecular design. Journal of chemical information and modeling, 59(3):1096\u20131108, 2019. [8] Peter Ertl and Ansgar Schuffenhauer. Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions. Journal of cheminformatics, 1:1\u201311, 2009.   \n[9] Wenhao Gao and Connor W Coley. The synthesizability of molecules proposed by generative models. Journal of chemical information and modeling, 60(12):5714\u20135723, 2020.   \n[10] Wenhao Gao, Tianfan Fu, Jimeng Sun, and Connor Coley. Sample efficiency matters: a benchmark for practical molecular optimization. Advances in Neural Information Processing Systems, 35:21342\u201321357, 2022.   \n[11] Zijie Geng, Shufang Xie, Yingce Xia, Lijun Wu, Tao Qin, Jie Wang, Yongdong Zhang, Feng Wu, and Tie-Yan Liu. De novo molecular generation via connection-aware motif mining. In International Conference on Learning Representations, 2023.   \n[12] Kexin Huang, Tianfan Fu, Wenhao Gao, Yue Zhao, Yusuf H Roohani, Jure Leskovec, Connor W Coley, Cao Xiao, Jimeng Sun, and Marinka Zitnik. Therapeutics data commons: Machine learning datasets and tasks for drug discovery and development. In NeurIPS Track Datasets and Benchmarks, 2021.   \n[13] John J Irwin, Teague Sterling, Michael M Mysinger, Erin S Bolstad, and Ryan G Coleman. Zinc: a free tool to discover chemistry for biology. Journal of chemical information and modeling, 52 (7):1757\u20131768, 2012.   \n[14] Jan H Jensen. A graph-based genetic algorithm and generative model/monte carlo tree search for the exploration of chemical space. Chemical science, 10(12):3567\u20133572, 2019.   \n[15] Woosung Jeon and Dongsup Kim. Autonomous molecule generation using reinforcement learning and docking to develop potential novel inhibitors. Scientific Reports, 10, 12 2020.   \n[16] Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for molecular graph generation. In International Conference on Machine Learning, pages 2323\u20132332. PMLR, 2018.   \n[17] Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Hierarchical generation of molecular graphs using structural motifs. In International Conference on Machine Learning, pages 4839\u20134848. PMLR, 2020.   \n[18] Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Multi-objective molecule generation using interpretable substructures. In International conference on machine learning, pages 4849\u20134859. PMLR, 2020.   \n[19] Hyeonah Kim, Minsu Kim, Sanghyeok Choi, and Jinkyoo Park. Genetic-guided gflownets: Advancing in practical molecular optimization benchmark. arXiv preprint arXiv:2402.05961, 2024.   \n[20] Xiangzhe Kong, Wenbing Huang, Zhixing Tan, and Yang Liu. Molecule generation by principal subgraph mining and assembling. Advances in Neural Information Processing Systems, 35: 2550\u20132563, 2022.   \n[21] Mario Krenn, Florian H\u00e4se, AkshatKumar Nigam, Pascal Friederich, and Alan Aspuru-Guzik. Self-referencing embedded strings (selfies): A $100\\%$ robust molecular string representation. Machine Learning: Science and Technology, 1(4):045024, 2020.   \n[22] Matt J Kusner, Brooks Paige, and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. Grammar variational autoencoder. In International Conference on Machine Learning, pages 1945\u20131954. PMLR, 2017.   \n[23] Greg Landrum et al. RDKit: Open-source cheminformatics software, 2016. URL http://www. rdkit. org/, https://github. com/rdkit/rdkit, 2016.   \n[24] Seul Lee, Jaehyeong Jo, and Sung Ju Hwang. Exploring chemical space with score-based out-ofdistribution generation. In International Conference on Machine Learning, pages 18872\u201318892. PMLR, 2023.   \n[25] Seul Lee, Seanie Lee, Kenji Kawaguchi, and Sung Ju Hwang. Drug discovery with dynamic goal-aware fragments. International Conference on Machine Learning, 2024.   \n[26] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459\u20139474, 2020.   \n[27] Qingxin Li. Application of fragment-based drug discovery to versatile targets. Frontiers in molecular biosciences, 7:180, 2020.   \n[28] Shengchao Liu, Jiongxiao Wang, Yijin Yang, Chengpeng Wang, Ling Liu, Hongyu Guo, and Chaowei Xiao. Conversational drug editing using retrieval and domain feedback. In The Twelfth International Conference on Learning Representations, 2023.   \n[29] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. International Conference on Learning Representations, 2019.   \n[30] Krzysztof Maziarz, Henry Richard Jackson-Flux, Pashmina Cameron, Finton Sirockin, Nadine Schneider, Nikolaus Stief,l Marwin Segler, and Marc Brockschmidt. Learning to extend molecular scaffolds with structural motifs. In International Conference on Learning Representations, 2021.   \n[31] Nicholas A Meanwell. Improving drug candidates by design: a focus on physicochemical properties as a means of improving compound disposition and safety. Chemical research in toxicology, 24(9):1420\u20131456, 2011.   \n[32] AkshatKumar Nigam, Pascal Friederich, Mario Krenn, and Alan Aspuru-Guzik. Augmenting genetic algorithms with deep neural networks for exploring the chemical space. In International Conference on Learning Representations, 2020.   \n[33] AkshatKumar Nigam, Robert Pollice, Mario Krenn, Gabriel dos Passos Gomes, and Alan Aspuru-Guzik. Beyond generative models: superfast traversal, optimization, novelty, exploration and discovery (stoned) algorithm for molecules using selfies. Chemical science, 12(20):7079\u2013 7090, 2021.   \n[34] Emmanuel Noutahi, Cristian Gabellini, Michael Craig, Jonathan SC Lim, and Prudencio Tossou. Gotta be safe: a new framework for molecular design. Digital Discovery, 3(4):796\u2013804, 2024.   \n[35] Marcus Olivecrona, Thomas Blaschke, Ola Engkvist, and Hongming Chen. Molecular de-novo design through deep reinforcement learning. Journal of cheminformatics, 9(1):1\u201314, 2017.   \n[36] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. In-context retrieval-augmented language models. Transactions of the Association for Computational Linguistics, 11:1316\u20131331, 2023.   \n[37] Gisbert Schneider, Werner Neidhart, Thomas Giller, and Gerard Schmid. \u201cscaffold-hopping\u201d by topological pharmacophore search: a contribution to virtual screening. Angewandte Chemie International Edition, 38(19):2894\u20132896, 1999.   \n[38] Austin Tripp and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. Genetic algorithms are strong baselines for molecule generation. arXiv preprint arXiv:2310.09267, 2023.   \n[39] Austin Tripp, Gregor NC Simm, and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. A fresh look at de novo molecular design benchmarks. In NeurIPS 2021 AI for Science Workshop, 2021.   \n[40] Fabio Urbina, Filippa Lentzos, C\u00e9dric Invernizzi, and Sean Ekins. Dual use of artificialintelligence-powered drug discovery. Nature Machine Intelligence, 4(3):189\u2013191, 2022.   \n[41] W Patrick Walters, Matthew T Stahl, and Mark A Murcko. Virtual screening\u2014an overview. Drug discovery today, 3(4):160\u2013178, 1998.   \n[42] Zichao Wang, Weili Nie, Zhuoran Qiao, Chaowei Xiao, Richard Baraniuk, and Anima Anandkumar. Retrieval-based controllable molecule generation. In International Conference on Learning Representations, 2023.   \n[43] David Weininger. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. Journal of chemical information and computer sciences, 28 (1):31\u201336, 1988.   \n[44] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, et al. Transformers: Stateof-the-art natural language processing. In Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, pages 38\u201345, 2020.   \n[45] Yutong Xie, Chence Shi, Hao Zhou, Yuwei Yang, Weinan Zhang, Yong Yu, and Lei Li. Mars: Markov molecular sampling for multi-objective drug discovery. In International Conference on Learning Representations, 2020.   \n[46] Soojung Yang, Doyeong Hwang, Seul Lee, Seongok Ryu, and Sung Ju Hwang. Hit and lead discovery with explorative rl and fragment-based molecule generation. Advances in Neural Information Processing Systems, 34:7924\u20137936, 2021.   \n[47] Naruki Yoshikawa, Kei Terayama, Masato Sumita, Teruki Homma, Kenta Oono, and Koji Tsuda. Population-based de novo molecule generation, using grammatical evolution. Chemistry Letters, 47(11):1431\u20131434, 2018. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Since our proposed $f$ -RAG is built on a pre-trained backbone molecular language model, it relies heavily on the generation performance of the backbone model. However, this also means that our method delegates the difficult task of molecule generation to a large model and lets the lightweight fragment injection module take care of the relatively easy task of fragment retrieval augmentation. This strategy enables very efficient and fast training (Section D.5) and makes $f$ -RAG a simple but powerful method to solve various drug discovery tasks. ", "page_idx": 13}, {"type": "text", "text": "B Broader Impacts ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Through our paper, we demonstrated that $f$ -RAG can achieve improved trade-offs between various considerations in drug discovery, showing its strong applicability to real-world drug discovery tasks. However, given its effectiveness, $f$ -RAG has the possibility to be used maliciously to generate harmful molecules. This can be prevented by setting the target properties to comprehensively consider toxicity and other side effects, or flitering out toxic fragments from the fragment vocabulary during generation. ", "page_idx": 13}, {"type": "text", "text": "C Generation Process of $f$ -RAG ", "text_level": 1, "page_idx": 13}, {"type": "table", "img_path": "56Q0qggDlp/tmp/1a51d7b9fcfe7230c9a4f0469cfd521010a64801a60e0eb29d5fc94206c6be27.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "D Experimental Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "D.1 Training of Fragment Injection Module ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we describe the details for training the fragment injection module, the only part of the $f$ -RAG framework that is trained. $f$ -RAG has 2,362,368 trainable parameters, coming from the fragment injection module. These correspond to only $2.64\\%$ of the total parameters of 89,648,640, indicating that the fragment injection module is very lightweight compared to the backbone molecular language model. ", "page_idx": 14}, {"type": "text", "text": "Throughout the paper, we used the official codebase including the pre-trained $\\mathrm{SAFE-GPT}^{3}$ of Noutahi et al. [34]. Following the codebase, we used the HuggingFace Transformer library [44] (Apache-2.0 license) to train the fragment injection module. We set the number of retrieved soft fragments to $K=10$ . For the layer of the backbone language model that the fragment injection module will be inserted behind, we conducted experiments with the search space $L\\,\\in\\,\\{1,6\\}$ , and found the $L\\,=\\,1$ -st layer works well. The results showing this comparison are provided in Table 11. The fragment injection module was trained to 8 epochs with a learning rate of $\\dot{1}\\times10^{-4}$ using the AdamW optimizer [29]. We performed searches with the search spaces (epoch) $\\in\\{5,8,10\\}$ and (learning rate) $\\in\\{1\\times\\bar{10}^{-4},\\bar{5}\\times10^{-3}\\}$ , respectively. ", "page_idx": 14}, {"type": "text", "text": "D.2 Genetic Operations for Fragment Modification ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "As described in Section 3.3, we adopt the operations of Graph GA [14] to modify the generated fragments to further enhance exploration of $f$ -RAG. Specifically, in the crossover operation, parents are cut at random positions at ring or non-ring positions with a probability of $50\\%$ , and random fragments from the cut are combined to generate offspring. In the mutation operation, bond insertion/deletion, atom insertion/deletion, bond order swapping, or atom changes are performed on the offspring molecule with a predefined probability. ", "page_idx": 14}, {"type": "text", "text": "D.3 Experiments on PMO Benchmark ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we describe the experimental details used in the experiments of Section 4.1. ", "page_idx": 14}, {"type": "text", "text": "Implementation details. We used the official codebase4 of Gao et al. [10] for implementing the experiments. We used ZINC250k [13] with the same train/test split used by Kusner et al. [22] to train the fragment injection module and construct the initial fragment vocabulary. We set the size of the fragment vocabulary to $N_{\\mathrm{frag}}=50$ and the size of the molecule population to $N_{\\mathrm{mol}}=50$ . When constructing the fragment vocabulary, fragments were filtered according to the number of atoms. We searched the size range in the search space [(min. number of atoms), (max. number of atoms)] $\\in$ {[5, 12], [10, 16], $[10,30]\\}$ . As a result, we used the range [5, 12] for the albuterol_similarity, isomers_c7h8n2o2m, isomers $.\\mathrm{c}9\\mathrm{h}10\\mathrm{n}2\\mathrm{o}2\\mathrm{p}\\mathrm{f}2\\mathrm{c}\\mathrm{l}$ , median1, qed, sitagliptin_mpo, zaleplon_mpo tasks, [10, 30] for the gsk3b and jnk3 tasks, and [10, 16] for the rest of the tasks. We also confined the size of the generate molecules. We used the range [10, 30] for the albuterol_similarity, isomers $_\\mathrm{c7h8n2o2m}$ , isomers $.\\mathrm{c}9\\mathrm{h}10\\mathrm{n}2\\mathrm{o}2\\mathrm{p}\\mathrm{f}2\\mathrm{c}\\mathrm{l}$ , median1, qed, sitagliptin_mpo, zaleplon_mpo tasks, [30, 80] for the gsk3b and jnk3 tasks, and [20, 50] for the rest of the tasks. We set the mutation rate of the GA to 0.1. We set the number of SAFE-GPT generation and number of GA generation in one cycle to $G_{\\mathrm{SAFE-GPT}}=10$ and $G_{\\mathrm{GA}}=10$ , respectively. ", "page_idx": 14}, {"type": "text", "text": "Measuring novelty and synthesizability. Following previous works [18, 45, 24, 25], novelty of the generated molecules $\\mathcal{X}$ is measured by the fraction of molecules that have a similarity less than 0.4 compared to its nearest neighbor $x_{\\mathrm{SNN}}$ in the training set as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{Novelty}=\\frac{1}{N}\\sum_{x\\in\\mathcal{X}}\\mathbb{1}\\{\\sin(x,x_{\\mathrm{SNN}})<0.4\\},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $N\\,=\\,|\\mathcal{X}|$ and $\\sin(x,x^{\\prime})$ is the pairwise Tanimoto similarity of molecules $x$ and $x^{\\prime}$ . The similarity is calculated using Morgan fingerprints of radius 2 and 1024 bits obtained by the RDKit [23] ", "page_idx": 14}, {"type": "text", "text": "library. On the other hand, since synthetic accessibility (SA) [8] scores range from 1 to 10 with higher scores indicating more difficult synthesis, we measure synthesizability using the normalized SA score as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\mathrm{Synthesizability}}={\\frac{10-{\\mathrm{SA}}}{9}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Visualizing the Radar Plots. To draw Figure 1 and Figure 5, sum AUC top-10, average top-100 diversity, average top-100 novelty, and normalized average top-100 SA score on the PMO benchmark in Table 1 and Table 2 were used to indicate optimization performance, diversity, novelty, and synthesizability, respectively. After the SA score is normalized according to Eq. (6), all the values are min-max normalized to [0, 1]. ", "page_idx": 15}, {"type": "text", "text": "D.4 Optimization of Docking Score under QED, SA, and Novelty Constraints ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we describe the experimental details used in the experiments of Section 4.2. ", "page_idx": 15}, {"type": "text", "text": "Implementation details. We used the official codebase5 of Lee et al. [24] for implementing the experiments. Following Lee et al. [24] and Lee et al. [25], we used ZINC250k [13] with the same train/test split used by Kusner et al. [22] to train the fragment injection module and construct the initial fragment vocabulary. As in Section D.3, when constructing the fragment vocabulary, fragments were flitered based on their number of atoms. We set the range to [5, 12]. To confinee the size of the generate molecules, we used the range [20, 40]. The mutation rate of the GA was set to 0.1. We set the number of SAFE-GPT generation and number of GA generation in one cycle to $G_{\\mathrm{SAFE-GPT}}=1$ and $G_{\\mathrm{GA}}=3$ , respectively. ", "page_idx": 15}, {"type": "text", "text": "Measuring docking score, QED, SA, and novelty. We strictly followed the setting used in previous works [24, 25] to measure these properties. Specifically, we used QuickVina 2 [2] to calculate docking scores (DSs). We used the RDKit [23] library to calculate QED and SA. Following previous works, we compute the normalized DS (D S) and the normalized SA $\\widehat{(\\mathrm{SA})}$ as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\widehat{\\sf D S}=-\\frac{\\mathrm{clip}({\\bf D S})}{20}\\in[0,1],\\quad\\widehat{\\bf S A}=\\frac{10-{\\bf S A}}{9}\\in[0,1],\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where clip is the function that clips the value in the range $[-20,0]$ . As in Section D.3, novelty is determined as $\\mathbb{1}\\{\\mathrm{sim}(x,x_{\\mathrm{SNN}})<\\bar{0}.4\\}$ . ", "page_idx": 15}, {"type": "text", "text": "D.5 Computing resources ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We trained the fragment injection module using one GeForce RTX 3090 GPU. The training took less than 4 hours. We generated molecules using one Titan XP (12GB), GeForce RTX 2080 Ti (11GB), or GeForce RTX 3090 GPU (24GB). The experiments on each task in Section 4.1 took less than 2 hours and the experiments on each task in Section 4.2 took approximately 2 hours. ", "page_idx": 15}, {"type": "text", "text": "E Additional Experimental Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we provide additional experimental results. ", "page_idx": 16}, {"type": "text", "text": "Diversity, novelty, and synthesizability results on PMO benchmark. We provide the full results of Table 2 in Table 4, Table 5, and Table 6. As shown in the tables, $f$ -RAG achieves the best diversity and synthesizability, while showing the second best novelty. We also provide the pairwise trade-off plots in Figure 7. ", "page_idx": 16}, {"type": "text", "text": "Comparison with GEAM on PMO benchmark. To provide additional comparisons of $f$ -RAG to GEAM [25], we followed the setup of Lee et al. [25] to conduct experiments on the seven MPO tasks in the PMO benchmark. As shown in Table 7, $f$ -RAG largely outperforms the baselines including GEAM in all of the tasks. ", "page_idx": 16}, {"type": "text", "text": "Ablation study. We provide the full results of Figure 5(b) in Table 8 and Table 9. As shown in the tables, Figure 5(a), and Figure 5(b), the three components of $f$ -RAG, hard fragment retrieval, soft fragment retrieval, and genetic fragment modification, are essential to the superior performance of $f$ -RAG and improved balance between various drug discovery considerations. ", "page_idx": 16}, {"type": "text", "text": "Controlling optimization performance-diversity trade-off. We provide the full results of Figure 5(c) in Table 10. As shown in the table and Figure 5(c), $f$ -RAG can effectively control the performance-diversity trade-off with the similarity-based fragment filtering. ", "page_idx": 16}, {"type": "text", "text": "Effect of location of the fragment injection module. We provide the results of $f$ -RAG with $L=1$ and $L=6$ in Table 11. As shown in the table, $f$ -RAG with $L=1$ works better than $f$ -RAG with $L=6$ , probably because injecting information from soft fragments in an earlier layer can carry more information into the final augmented embedding. As a result of this experiment, $f$ -RAG in the main experiments used $L=1$ . ", "page_idx": 16}, {"type": "text", "text": "Visualization of generated molecules. We visualize examples of the generated molecules by $f$ -RAG in Figure 8. The examples are drawn randomly from the molecules used to calculate the novel top $5\\%$ DS values in Table 3. ", "page_idx": 16}, {"type": "image", "img_path": "56Q0qggDlp/tmp/15cbc013bdad8f37407a493c281e302ff5a98c567d3525ec81ccdbbb9c44000f.jpg", "img_caption": ["Figure 7: Various trade-offs between the basic considerations in drug discovery. Sum AUC top-10, average top-100 diversity, average top-100 novelty, and normalized average top-100 SA score on the PMO benchmark are used to measure optimization performance, diversity, novelty, and synthesizability, respectively. "], "img_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "56Q0qggDlp/tmp/78981812086ff2c210125decab25fc7093b30bfdcf85ac864b3e3e07c2ec0a93.jpg", "table_caption": ["Table 4: Top-100 diversity results. The results are the mean of 3 independent runs. The best results are highlighted in bold. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "56Q0qggDlp/tmp/a9f5d0bb2c5fa9851b833b81926f99bab5a0c67f58e69251a1afc8e0a4fc114c.jpg", "table_caption": ["Table 5: Top-100 novelty results. The results are the mean of 3 independent runs. The best results are highlighted in bold. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "56Q0qggDlp/tmp/d2f42b127f9555381a7a1e18f59bfbf31110ba939b57b3f608ec385fd007e8b3.jpg", "table_caption": ["Table 6: Top-100 SA score results. The results are the mean of 3 independent runs. Lower is better and the best results are highlighted in bold. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "56Q0qggDlp/tmp/7bb3bf88f02d3763536b8fe418ec3870aaba3249ab2aa6a85786d2d271827b99.jpg", "table_caption": ["Table 7: PMO MPO AUC top-100 results. The results are the means of 3 runs. The results for REINVENT, Graph GA, and SELFIES-REINVENT are taken from Gao et al. [10], and the results for GEAM are taken from Lee et al. [25]. The best results are highlighted in bold. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "56Q0qggDlp/tmp/0927b25f6aa9dfb7b9a9b17995a45146aa9ba72a96230d81336e556ffbf24357.jpg", "table_caption": ["Table 8: PMO AUC top-10 results of the ablated versions of $f$ -RAG. The results are the mean of 3 independent runs. The best results are highlighted in bold. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "56Q0qggDlp/tmp/43f1c2b2a8ae5265023b139dcc178bfdfb1545c9828ffeb0e55dfe5cf138e7ef.jpg", "table_caption": ["Table 9: Top-100 diversity, top-100 novelty, and top-100 SA score results of the ablated versions of $f$ -RAG. The results are the average values of all 23 tasks. The best results are highlighted in bold. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "56Q0qggDlp/tmp/3d3f99a180f4894605774674b3a977b4744f9b5e2206bb777a15bba4bb72ff1f.jpg", "table_caption": ["Table 10: Top-100 diversity, top-100 novelty, and top-100 SA score results with different values of $\\delta$ of the similarity-based fragment filter. The results are the average values of all 23 tasks. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "56Q0qggDlp/tmp/1204115ca3a913413fef13c882ea7d7489b1ab9aefd7fe9f0bcca00ef30ad116.jpg", "table_caption": ["Table 11: Effect of location of the fragment injection module $L$ . The results are the mean of 3 independent runs. $f$ -RAG in the main experiments used $L=1$ . The best results are highlighted in bold. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "56Q0qggDlp/tmp/29507e9551157b7c1d4423347cd13a09609845ea82fb473207229c23a79ddda4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 8: The generated molecules by $f$ -RAG. Random top $5\\%$ novel hits generated in the experiments of Section 4.2 are visualized. The docking score $\\mathrm{(kcal/mol)}$ , QED, SA, and the maximum similarity with the molecules in the training set are at the bottom of each molecule. ", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We discussed the limitations in Section A. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: This paper does not include theoretical results. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: The code to reproduce the results in our paper is provided as the supplementary material. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The code to reproduce the results in our paper is provided as the supplementary material. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provided the training details in Section D.1, and the evaluation details in Section D.3 and Section D.4. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We reported the standard deviation of the main results in Table 1 and Table 3.   \nWe provided the standard deviation of the ablation study in Figure 5(a). ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provided the details on computing resources in Section D.5. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We reviewed the NeurIPS Code of Ethics. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We discussed potential positive and negative societal impacts in Section B. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: For existing assets used in the paper, we cited their original paper, repository URL, and license name in Section D.1, Section D.3, and Section D.4. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 26}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The code of our work, including a description to run the code and the license, is provided as the supplementary material. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]