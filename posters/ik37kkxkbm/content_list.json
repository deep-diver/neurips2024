[{"type": "text", "text": "In-Context Learning with Representations: Contextual Generalization of Trained Transformers ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Tong Yang\u2217 Yu Huang\u2020 Yingbin Liang\u2021 Yuejie Chi\u00a7 CMU UPenn OSU CMU ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In-context learning (ICL) refers to a remarkable capability of pretrained large language models, which can learn a new task given a few examples during inference. However, theoretical understanding of ICL is largely under-explored, particularly whether transformers can be trained to generalize to unseen examples in a prompt, which will require the model to acquire contextual knowledge of the prompt for generalization. This paper investigates the training dynamics of transformers by gradient descent through the lens of non-linear regression tasks. The contextual generalization here can be attained via learning the template function for each task in-context, where all template functions lie in a linear space with $m$ basis functions. We analyze the training dynamics of one-layer multi-head transformers to in-contextly predict unlabeled inputs given partially labeled prompts, where the labels contain Gaussian noise and the number of examples in each prompt are not sufficient to determine the template. Under mild assumptions, we show that the training loss for a one-layer multi-head transformer converges linearly to a global minimum. Moreover, the transformer effectively learns to perform ridge regression over the basis functions. To our knowledge, this study is the first provable demonstration that transformers can learn contextual (i.e., template) information to generalize to both unseen examples and tasks when prompts contain only a small number of query-answer pairs. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Transformers [Vaswani et al., 2017] have achieved tremendous successes in machine learning, particularly in natural language processing, by introducing self-attention mechanisms that enable models to capture long-range dependencies and contextualized representations. In particular, these self-attention mechanisms endow transformers with remarkable in-context learning (ICL) capabilities, allowing them to adapt to new tasks or domains by simply being prompted with a few examples that demonstrate the desired behavior, without any explicit fine-tuning or updating of the model\u2019s parameters [Brown et al., 2020]. ", "page_idx": 0}, {"type": "text", "text": "A series of papers have empirically studied the underlying mechanisms behind in-context learning in transformer models [Garg et al., 2022, Von Oswald et al., 2023, Wei et al., 2023, Olsson et al., 2022, Xie et al., 2021, Chen and Zou, 2024, Agarwal et al., 2024], which have shown that transformers can predict unseen examples after being prompted on a few examples. The pioneering work of ", "page_idx": 0}, {"type": "text", "text": "Garg et al. [2022] showed empirically that transformers can be trained from scratch to perform in-context learning of simple function classes, providing a theoretically tractable in-context learning framework. Following this well-established framework, several works have investigated various properties of in-context learning in transformers. For instance, studies have explored generalization and stability [Li et al., 2023], expressive power [Bai et al., 2024, Aky\u00fcrek et al., 2022, Giannou et al., 2023], causal structures [Nichani et al., 2024, Edelman et al., 2024], statistical properties [Xie et al., 2021, Jeon et al., 2024], to name a few. ", "page_idx": 1}, {"type": "text", "text": "In particular, analysis from an optimization perspective can provide valuable insights into how these models acquire and apply knowledge that enable in-context learning. A few works [Huang et al., 2023, Chen et al., 2024, Li et al., 2024, Nichani et al., 2024] thus studied the training dynamics of shallow transformers with softmax attention in order to in-context learn simple tasks such as linear regression [Huang et al., 2023, Chen et al., 2024], binary classification tasks [Li et al., 2024], and causal graphs [Nichani et al., 2024]. Their theoretical analyses illuminated how transformers, given an arbitrary query token, learn to directly apply the answer corresponding to it from the query-answer pairs that appear in each prompt. Therefore, they all require the sequence length of each prompt to be large enough so that all query-answer pairs have been seen in each prompt with sufficiently high probability, whereas practical prompts are often too short to contain many query examples. This suggests that in-context learning can exploit inherent contextual information of the prompt to generalize to unseen examples, which further raise the following intriguing theoretical question: ", "page_idx": 1}, {"type": "text", "text": "How do transformers learn contextual information from more general function classes to predict unseen examples given prompts that contain only partial examples? ", "page_idx": 1}, {"type": "text", "text": "Since our paper studies ICL of non-linear function regression, the function mapping (which we also term as \u201ctemplate\u201d) naturally serves as the \u201ccontextual information\u201d that can be learned for generalization to unseen examples. When each prompt contains only a small number of (noisy) examples, the template that generates the labels may be underdetermined, i.e., multiple templates could generate the same labels in the prompt. Such an issue of underdetermination further raises a series of intriguing questions, such as: ", "page_idx": 1}, {"type": "text", "text": "When the template that generates a prompt is underdetermined, what is the transformer\u2019s preference for choosing the template and how good is such a choice? ", "page_idx": 1}, {"type": "text", "text": "1.1 Our contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this paper, we answer the above questions by analyzing the training dynamics of a one-layer transformer with multi-head softmax attention through the lens of non-linear regression tasks. In our setting, the template function for each task lies in the linear space formed by $m$ nearly-arbitrary basis functions that capture representation (i.e., features) of data. Our goal is to provide insights on how transformers trained by gradient descent (GD) acquire template information from more general function classes to generalize to unseen examples and tasks when each prompt contains only a small number of query-answer pairs. We summarize our contributions are as follows. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We first establish the convergence guarantee of a one-layer transformer with multi-head softmax attention trained with gradient descent on general non-linear regression in-context learning tasks. We assume each prompt contains only a few (i.e., partial) examples with their Gaussian noisy labels, which are not sufficient to determine the template. Under mild assumptions, we establish that the training loss of the transformer converges at a linear rate. Moreover, by analyzing the limit point of the transformer parameters, we are able to uncover what information about the basic tasks the transformer extracts and memorizes during training in order to perform in-context prediction. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We then analyze the transformer\u2019s behavior at inference time after training, and show that the transformer chooses its generating template by performing ridge regression over the basis functions. We also provide the iteration complexity for pretraining the transformer to reach $\\varepsilon$ -precision with respect to its choice of the template given an arbitrary prompt at inference time. We further compare the choice of the transformer and the best possible choice over the template class and characterize how the sequence length of each prompt influences the inference time performance of the model. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Under more realistic assumptions, our analysis framework allows us to overcome a handful of assumptions made in previous works such as large prompt length [Huang et al., 2023, Chen et al., 2024, Li et al., 2024, Nichani et al., 2024], orthogonality of data [Huang et al., 2023, Chen et al., 2024, Li et al., 2024, Nichani et al., 2024], restrictive initialization conditions [Chen et al., 2024], special structure of the transformer [Nichani et al., 2024], and mean-field models [Kim and Suzuki, 2024]. Further, the function classes we consider are a generalization of those considered in most theoretical works [Huang et al., 2023, Chen et al., 2024, Li et al., 2024, Wu et al., 2023, Zhang et al., 2023a]. We also highlight the importance of multi-head attention mechanism in this process. ", "page_idx": 2}, {"type": "table", "img_path": "ik37kKxKBm/tmp/30529e07368cf5c1798695d38b439fbf4ec396ef982560f5192d80cc42b605ed.jpg", "table_caption": ["To our best knowledge, this is the first work that analyzes how transformers learn contextual (i.e., template) information to generalize to unseen examples and tasks when prompts contain only a small number of query-answer pairs. Table 1 provides a detailed comparison with existing theoretical works in terms of settings, training analysis and generalization of in-context learning. "], "table_footnote": ["Table 1: Comparisons with existing theoretical works that study the learning dynamics of transformers in ICL. Here, the last column refers to the fact that the response in the regression task is generated by a linearly weighted unknown representation (feature) model. "], "page_idx": 2}, {"type": "text", "text": "1.2 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In-context learning. Recent research has investigated the theoretical underpinnings of transformers\u2019 ICL capabilities from diverse angles. For example, several works focus on explaining the in-context learning of transformers from a Bayesian perspective [Xie et al., 2021, Ahuja et al., 2023, Han et al., 2023, Jiang, 2023, Wang et al., 2023, Wies et al., 2024, Zhang et al., 2023b, Jeon et al., 2024, Hahn and Goyal, 2023]. Li et al. [2023] analyzed the generalization and stability of transformers\u2019 in-context learning. Focusing on the representation theory, Aky\u00fcrek et al. [2022], Bai et al. [2024] studied the expressive power of transformers on the linear regression task. Aky\u00fcrek et al. [2022] showed by construction that transformers can represent GD of ridge regression or the closed-form ridge regression solution. Bai et al. [2024] extended Aky\u00fcrek et al. [2022] and showed that transformers can implement a broad class of standard machine learning algorithms in-context. Dai et al. [2022], Von Oswald et al. [2023] showed transformers could in-context learn to perform GD. ", "page_idx": 2}, {"type": "text", "text": "More pertinent to our work, Guo et al. [2023] considered an ICL setting very similar to ours, where the label depends on the input through a basis of possibly complex but fixed template functions, composed with a linear function that differs in each prompt. By construction, the optimal ICL algorithm first transforms the inputs by the representation function, and then performs linear ICL on top of the transformed dataset. Guo et al. [2023] showed the existence of transformers that approximately implement such algorithms, whereas our work is from a different perspective, showing that (pre)training the transformer loss by GD will naturally yield a solution with the aforementioned desirable property characterized in Guo et al. [2023]. ", "page_idx": 2}, {"type": "text", "text": "Training dynamics of transformers performing ICL. A line of work initiated by Garg et al. [2022] aims to understand the ICL ability of transformers from an optimization perspective. [Zhang et al., 2023a, Kim and Suzuki, 2024] analyzed the training dynamics of transformers with linear attention. Huang et al. [2023], Chen et al. [2024], Li et al. [2024] studied the optimization dynamics of one-layer softmax attention transformers performing simple in-context learning tasks, such as linear regression [Huang et al., 2023, Chen et al., 2024] and binary classification [Li et al., 2024]. ", "page_idx": 2}, {"type": "text", "text": "Among them, Huang et al. [2023] was the first to study the training dynamics of softmax attention, where they gave the convergence results of a one-layer transformer with single-head attention on linear regression tasks, assuming context features come from an orthogonal dictionary and each token in the prompts is drawn from a multinomial distribution. In order to leverage the concentration property inherent to multinomial distributions, they require the sequence length to be much larger than the size of dictionary. Their analysis indicates that the prompt tokens that are the same as the query will have dominating attention weights, which allows the transformer to copy-paste the correct answer from those prompt tokens. ", "page_idx": 3}, {"type": "text", "text": "Li et al. [2024] studied the training of a one-layer single-head transformer in ICL on binary classification tasks. Same as Huang et al. [2023], they required the data to be pairwise orthogonal, and shared the same copy-paste mechanism as in Huang et al. [2023]. To be precise, a fraction of their context inputs needs to contain the same pattern as the query to guarantee that the total attention weights on contexts matching the query pattern outweigh those on other contexts. ", "page_idx": 3}, {"type": "text", "text": "Chen et al. [2024] studied the dynamics of gradient flow for training a one-layer multi-head softmax attention model for ICL of multi-task linear regression, where the coefficient matrix has certain spectral properties. They required the sequence length to be sufficiently large [Chen et al., 2024, Assumption 2.1], together with restrictive initialization conditions [Chen et al., 2024, Definition 3.1]. While using the copy-paste analysis framework as in Huang et al. [2023], Li et al. [2024], the attention probability vector in their work is delocalized, so that the attention is spread out to capture the information from similar tokens in regression tasks. Kim and Suzuki [2024] studied the dynamics of Wasserstein gradient flow for training a one-layer transformer with an infinite-dimensional fullyconnected layer followed by a linear attention layer for ICL of linear regression, assuming infinite prompt length. Nichani et al. [2024] analyzed the optimization dynamics of a simplified two-layer transformer with gradient descent on in-context learning a latent causal graph. ", "page_idx": 3}, {"type": "text", "text": "Notation. Boldface small and capital letters denote vectors and matrices, respectively. Sets are denoted with curly capital letters, e.g., $\\mathcal{W}$ . We let $(\\mathbb{R}^{d},\\|\\cdot\\|)$ denote the $d$ -dimensional real coordinate space equipped with norm $\\left\\Vert\\cdot\\right\\Vert$ . $\\pmb{I}_{d}$ is the identity matrix of dimension $d$ . The $\\,\\ell^{p}$ -norm of $\\pmb{v}$ is denoted by $\\left\\|\\pmb{v}\\right\\|_{p}$ , where $1\\leq p\\leq\\infty$ , and the spectral norm and the Frobenius norm of a matrix $_M$ are denoted by $\\|M\\|_{2}$ and $\\|M\\|_{F}$ , respectively. $M^{\\dagger}$ stands for the Moore-Penrose pseudoinverse of matrix $_M$ , and $M_{:,i}$ stands for its $i$ -th column vector. We let $[N]$ denote $\\{1,\\ldots,N\\}$ , and denote ${\\mathbf{1}}_{N}$ to represent the all-one vector of length $N$ , and by 0 a vector or a matrix consisting of all $\\mathrm{0\\,\\mathrm{\\Omega}}$ . We allow the application of functions such as $\\exp(\\cdot)$ to vectors or matrices, with the understanding that they are applied in an element-wise manner. We use $e_{i}$ to denote the one-hot vector whose $i$ -th entry is 1 and the other entries are all 0. ", "page_idx": 3}, {"type": "text", "text": "2 Problem Setup ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In-context learning with representation. We consider ICL of regression with unknown representation, similar to the setup introduced in Guo et al. [2023]. To begin, let $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{m}$ be a fixed representation map that $\\bar{f}(\\pmb{x})=(f_{1}(\\pmb{x}),\\cdots,f_{m}(\\pmb{x}))^{\\top}$ for any $\\pmb{x}\\in\\mathbb{R}^{d}$ . The map $f$ can be quite general, which can be regarded as a feature extractor that will be learned by the transformer. We assume that each ICL task corresponds to a map $\\lambda^{\\top}f(\\cdot)$ that lies in the linear span of those $m$ basis functions in $f(\\cdot)$ , where $\\lambda$ is generated according to the distribution $\\mathcal{D}_{\\lambda}$ . Thus, for each ICL instance, the (noisy) label of an input $\\pmb{v}_{k}$ $\\forall k\\in[K])$ is given as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{y_{k}=\\pmb{\\lambda}^{\\top}(f(\\pmb{v}_{k})+\\pmb{\\epsilon}_{k}),\\qquad\\pmb{\\lambda}\\sim\\mathcal{D}_{\\pmb{\\lambda}},\\quad\\pmb{\\epsilon}_{k}\\overset{i.i.d.}{\\sim}\\mathcal{N}(0,\\tau\\pmb{I}_{m})}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\tau>0$ is the noise level. ", "page_idx": 3}, {"type": "text", "text": "The goal of ICL is to form predictions on query $\\boldsymbol{x}_{\\mathsf{q u e r y}}$ given in-context labels of the form (1) on a few inputs, known as prompts. In this paper, we use $\\mathcal{V}$ to denote the dictionary set that contains all $K$ unit-norm distinct tokens, i.e., $\\mathcal{V}:=\\left\\{\\pmb{v}_{1},\\cdot\\cdot\\cdot,\\pmb{v}_{K}\\right\\}\\subset\\mathbb{R}^{d}$ with each token $\\|\\pmb{v}_{k}\\|_{2}=1$ . We assume that each prompt $P=P_{\\lambda}$ provides the first $N$ tokens (with $N\\ll K$ ) and their labels, and is embedded in the following matrix ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pmb{E}^{P}:=\\binom{v_{1}}{y_{1}}\\pmb{\\mathscr{v}}_{2}\\-\\cdots\\-\\pmb{\\mathscr{v}}_{N}\\biggr):=\\binom{V}{\\pmb{y}^{\\top}}\\in\\mathbb{R}^{(d+1)\\times N},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where ", "page_idx": 3}, {"type": "equation", "text": "$$\nV:=(\\pmb{v}_{1},\\cdot\\cdot\\cdot\\mathrm{~,~}\\pmb{v}_{N})\\in\\mathbb{R}^{d\\times N}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "ik37kKxKBm/tmp/0ef07b9a5d069ca57fb4028c19bded6a2a9d98fad135fce7a6c01d1b4c37e014.jpg", "img_caption": ["Figure 1: The structure of a one-layer transformer with multi-head softmax attention. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "is the collection of prompt tokens, and $\\pmb{y}:=\\left(y_{1},\\cdot\\cdot\\cdot\\,,y_{N}\\right)^{\\top}$ is the prompt label. Given the prompt as the input, the transformer predicts the labels for all the $K$ tokens $y_{1},\\cdot\\cdot\\cdot\\,,y_{K}$ in the dictionary set. ", "page_idx": 4}, {"type": "text", "text": "Transformer architecture. We adopt a one-layer transformer with multi-head softmax attention [Chen et al., 2024] \u2014 illustrated in Figure 1 \u2014 to predict the labels of all the tokens in the dictionary $\\nu$ , where $H$ is the number of heads. Denote the query embedding as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{E^{Q}:=\\left(\\!\\!\\begin{array}{c c c c}{\\!v_{N+1}}&{\\!v_{N+2}}&{\\cdot\\cdot\\cdot}&{v_{K}}\\\\ {0}&{0}&{\\cdot\\cdot\\cdot}&{0}\\end{array}\\!\\!\\right)\\in\\mathbb{R}^{(d+1)\\times(K-N)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and denote the embedding of both the prompt and the query as $\\pmb{E}:=(\\pmb{E}^{P},\\pmb{E}^{Q})\\in\\mathbb{R}^{(d+1)\\times K}$ . We define the output of each transformer head as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{head}_{h}(E):=W_{h}^{\\vee}E^{P}\\cdot\\mathsf{s o f t m a x}\\left((E^{P})^{\\top}(W_{h}^{\\mathsf{K}})^{\\top}W_{h}^{\\mathsf{Q}}E\\right),\\quad h\\in[H],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $W_{h}^{\\sf Q}\\in\\mathbb{R}^{d_{\\mathrm{c}}\\times(d+1)}$ , $W_{h}^{\\mathsf{K}}\\in\\mathbb{R}^{d_{\\mathrm{c}}\\times(d+1)}$ , and $W_{h}^{\\vee}\\in\\mathbb{R}^{K\\times(d+1)}$ are the query, key, and value matrices, respectively, and the softmax is applied column-wisely, i.e., given a vector input $\\textbf{\\em x}$ , the $i$ -th entry of softmax $(x)$ is given by $e^{x_{i}}/\\sum_{j}e^{\\bar{x}_{j}}$ . The attention map of the transformer $\\bar{\\mathcal{T}}\\bar{(}E)$ is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\nT(E):=W^{0}\\left(\\stackrel{\\mathrm{head}_{1}(E)}{\\underset{\\mathrm{head}_{H}(E)}{\\vdots}}\\right)\\in\\mathbb{R}^{K\\times K},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $W^{0}$ is the output matrix. Following recent theoretical literature to streamline analysis [Huang et al., 2023, Nichani et al., 2024, Deora et al., 2023, Chen et al., 2024], we assume that the embedding matrices take the following forms: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\pmb{W}^{0}:=(\\pmb{I}_{K},\\cdot\\cdot\\cdot,\\pmb{I}_{K})\\in\\mathbb{R}^{K\\times H K},\\quad\\pmb{W}_{h}^{\\vee}:=(\\pmb{0},\\pmb{w}_{h})\\in\\mathbb{R}^{K\\times(d+1)},}\\\\ &{}&{(\\pmb{W}_{h}^{\\mathsf{K}})^{\\top}\\pmb{W}_{h}^{\\mathsf{Q}}=\\left(\\pmb{Q}_{h}\\quad\\pmb{0}\\right)\\in\\mathbb{R}^{(d+1)\\times(d+1)},\\quad\\forall h\\in[H],}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\pmb{w}_{h}=(w_{h,1},\\cdot\\cdot\\cdot\\mathrm{~,~}w_{h,K})^{\\top}\\in\\mathbb{R}^{K}$ and $Q_{h}\\in R^{d\\times d}$ are trainable parameters for all $h\\in[H]$ . ", "page_idx": 4}, {"type": "text", "text": "The prediction of the labels is provided by the diagonal entries of $\\mathcal{T}(E)$ , which we denote by $\\widehat{\\pmb{y}}=\\left\\langle\\widehat{y}_{1},\\cdot\\cdot\\cdot\\,,\\widehat{y}_{K}\\right\\rangle\\in\\mathbb{R}^{K}$ . Note that $\\widehat{y}_{k}$ takes the following form under our parameter specification: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\forall k\\in[K]:\\qquad\\widehat{y}_{k}=\\Big\\langle y,\\sum_{h=1}^{H}w_{h,k}\\,\\mathsf{s o f t m a x}(V^{\\top}Q_{h}v_{k})\\Big\\rangle.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Training via GD. Let $\\pmb\\theta~=~\\{Q_{h},{\\pmb w}_{h}\\}_{h=1}^{H}$ denote all trainable parameters of $\\tau$ . Let $\\epsilon:=$ $\\left(\\epsilon_{1},\\cdot\\cdot\\cdot\\,,\\epsilon_{K}\\right)\\in\\mathbb{R}^{m\\times K}$ denote the noise matrix. Given training data over ICL instances, the goal of training is to predict labels $y_{k}$ for all $\\pmb{v}_{k}\\in\\mathcal{V}$ . Specifically, we train the transformer using gradient descent (GD) by optimizing the following mean-squared population loss: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\pmb{\\theta}):=\\frac{1}{2}\\mathbb{E}_{\\pmb{\\lambda},\\epsilon}\\left[\\frac{1}{K}\\sum_{k=1}^{K}\\left(\\widehat{y}_{k}-y_{k}\\right)^{2}\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We apply different learning rates $\\eta_{Q},\\eta_{w}>0$ for updating $\\{Q_{h}\\}_{h=1}^{H}$ and $\\{w_{h}\\}_{h=1}^{H}$ , respectively, i.e., at the $t$ -th $\\mathit{\\Omega}\\left(t\\geq1\\right)$ ) step, we have $\\forall h\\in[H]:\\quad Q_{h}^{(t)}=Q_{h}^{(t-1)}-\\eta_{Q}\\nabla_{Q_{h}}\\mathcal{L}(\\theta^{(t-1)}),\\quad w_{h}^{(t)}=w_{h}^{(t-1)}-\\eta_{w}\\nabla_{w_{h}}\\mathcal{L}(\\theta^{(t-1)}),$ (10) ", "page_idx": 5}, {"type": "text", "text": "where \u03b8(t) = {Q(ht ), w(ht )}hH=1 is the parameter at the t-th step. ", "page_idx": 5}, {"type": "text", "text": "Inference time. At inference time, given a prompt $P=P\\!\\!_{\\lambda}$ with $N$ examples, where $\\lambda$ may not be in the support of the generation distribution $\\mathcal{D}_{\\lambda}$ , the transformer applies the pretrained parameters and predicts the labels of all $K$ tokens without further parameter updating. ", "page_idx": 5}, {"type": "text", "text": "3 Theoretical Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "3.1 Training time convergence ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we show that the training loss $\\mathcal{L}$ converges to its minimum value at a linear rate during training, i.e., the function gap ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Delta^{(t)}:=\\mathcal{L}(\\pmb{\\theta}^{(t)})-\\operatorname*{inf}_{\\pmb{\\theta}}\\mathcal{L}\\rightarrow0,\\quad t\\rightarrow\\infty\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "at a linear rate, under some appropriate assumptions. ", "page_idx": 5}, {"type": "text", "text": "Key assumptions. We first state our technical assumptions. The first assumption is on the distribution $\\mathcal{D}_{\\lambda}$ for generating the coefficient vector $\\lambda$ of the representation maps. ", "page_idx": 5}, {"type": "text", "text": "Assumption 1 (Assumption on distribution $\\mathcal{D}_{\\lambda}$ ). We assume that in (1) each entry $\\lambda_{i}$ is drawn independently and satisfies $\\mathbb{E}[\\lambda_{i}]=0$ and $\\mathbb{E}[\\lambda_{i}^{2}]=1$ for all $i\\in[m]$ . ", "page_idx": 5}, {"type": "text", "text": "To proceed, we introduce the following notation: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pmb{\\Sigma}:=\\left(f(\\pmb{v}_{1})\\cdots f(\\pmb{v}_{N})\\right)\\in\\mathbb{R}^{m\\times N},\\quad\\bar{\\pmb{Z}}:=\\left(\\pmb{Z}^{\\top}\\pmb{Z}+m\\tau I_{N}\\right)^{1/2}\\in\\mathbb{R}^{N\\times N},\\,\\,\\,\\bar{f}_{\\operatorname*{max}}:=\\operatorname*{max}_{i\\in[N]}\\|\\bar{z}_{i}\\|_{2}\\,,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\bar{z}_{i}$ is the $i$ -th column vector of $\\bar{Z}$ for $i\\in[N]$ . We further define $C_{k}^{(t)}\\left(k\\in[K],t\\in\\mathbb{N}_{+}\\right)$ and $B_{k}^{(t)}$ as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\nC_{k}^{(t)}:=\\mathsf{s o f t m a x}(V^{\\top}Q_{1}^{(t)}v_{k},\\cdot\\cdot\\cdot,V^{\\top}Q_{H}^{(t)}v_{k})\\in\\mathbb{R}^{N\\times H},\\qquad B_{k}^{(t)}=\\bar{Z}C_{k}^{(t)}\\in\\mathbb{R}^{N\\times H}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "To guarantee the convergence, we require the initialization of the parameters ${\\pmb\\theta}^{(0)}$ satisfies the following condition. ", "page_idx": 5}, {"type": "text", "text": "Assumption 2 (Assumption on initialization). For all $k\\in[K],B_{k}^{(0)}$ has full row rank. ", "page_idx": 5}, {"type": "text", "text": "Before stating our main theorem, let us examine when the initialization condition in Assumption 2 is met. Fortunately, we only require the following mild assumption on $V$ to ensure our parameter initialization has good properties. ", "page_idx": 5}, {"type": "text", "text": "Assumption 3 (Assumption on $V$ ). There exists one row vector $\\pmb{x}=(x_{1},\\cdots\\,,x_{N})^{\\top}$ of the prompt token matrix $V$ (cf. (3)) such that $x_{i}\\neq x_{j}$ , $\\forall i\\ne j$ . ", "page_idx": 5}, {"type": "text", "text": "Assumption 3 implies that $\\nu$ has distinct tokens, i.e., $\\pmb{v}_{j}\\neq\\pmb{v}_{k}$ when $j\\neq k$ . It is worth noting that Assumption 3 is the only assumption we have on the dictionary $\\mathcal{V}$ . In comparison, all other theoretical works in Table 1 impose somewhat unrealistic assumptions on $\\nu$ . For example, Huang et al. [2023], Li et al. [2023], Nichani et al. [2024] assume that the tokens are pairwise orthogonal, which is restrictive since it implies that the dictionary size $K$ should be no larger than the token dimension $d$ , whereas in practice it is often the case that $K\\gg d$ [Reid et al., 2024, Touvron et al., 2023]. In addition, Chen et al. [2024], Zhang et al. [2023a], Wu et al. [2023] assume that each token is independently sampled from some Gaussian distribution, which also does not align with practical scenarios where tokens are from a fixed dictionary and there often exist (strong) correlations between different tokens. ", "page_idx": 5}, {"type": "text", "text": "The following proposition states that when the number of heads exceeds the number of prompts, i.e. $H\\geq N$ , we can guarantee that Assumption 2 holds with probability 1 by simply initializing $\\{Q_{h}\\}_{h=1}^{H}$ using Gaussian distribution. ", "page_idx": 5}, {"type": "text", "text": "Proposition 1 (Initialization of $\\{Q_{h}\\}_{h=1}^{H})$ . Suppose Assumptions 1, 3 hold and $H\\geq N$ . For any fixed $\\beta>0$ , let $\\pmb{Q}_{h}^{(0)}(i,j)\\stackrel{i.i.d.}{\\sim}\\mathcal{N}(0,\\beta^{2})$ , then Assumption 2 holds almost surely. ", "page_idx": 6}, {"type": "text", "text": "Proof. See Appendix E.1. ", "page_idx": 6}, {"type": "text", "text": "Choice of learning rates. Define ", "text_level": 1, "page_idx": 6}, {"type": "equation", "text": "$$\n\\zeta_{0}:=\\operatorname*{min}_{k\\in[K]}\\left\\{\\lambda_{\\operatorname*{min}}\\left(B_{k}^{(0)}B_{k}^{(0)\\top}\\right)\\right\\},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\Delta^{(0)}$ is the initial function gap (c.f. (11)). Assumption 2 indicates that $\\zeta_{0}>0$ . Let $\\gamma$ be any positive constant that satisfies ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\gamma\\geq\\zeta_{0}^{-5/4}\\left(\\frac{128\\sqrt{2}}{\\sqrt{2}-1}\\left\\Vert\\bar{Z}\\right\\Vert_{2}^{2}\\sqrt{H}\\bar{f}_{\\mathrm{max}}K^{3/2}\\Delta^{(0)}\\right)^{1/2}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We set the learning rates as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\eta_{Q}\\leq1/L\\quad\\mathrm{and}\\quad\\eta_{w}=\\gamma^{2}\\eta_{Q},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where5 ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L^{2}=\\left(8\\sqrt{2}H\\sqrt{K}\\frac{\\left\\Vert\\bar{Z}\\right\\Vert_{2}^{2}}{\\zeta_{0}}\\sqrt{\\Delta^{\\left(0\\right)}}+1+\\frac{\\left\\Vert Z^{\\top}\\widehat{Z}\\right\\Vert_{2}}{m\\tau}\\right)^{2}\\left\\Vert\\bar{Z}\\right\\Vert_{2}^{4}\\cdot\\left(\\frac{8}{K}\\gamma^{2}+\\frac{4096}{\\gamma\\zeta_{0}^{2}}K^{2}N\\Delta^{\\left(0\\right)}\\right)}\\\\ &{\\qquad\\quad+\\left.2H^{2}\\left\\Vert\\bar{Z}\\right\\Vert_{2}^{4}\\left(\\frac{\\gamma^{4}}{K^{2}}+\\frac{16384}{\\gamma\\zeta_{0}^{4}}K^{3}\\left\\Vert\\bar{Z}\\right\\Vert_{2}^{2}\\left(\\Delta^{\\left(0\\right)}\\right)^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Theoretical guarantee. Now we are ready to state our first main result, regarding the training dynamic of the transformer. ", "page_idx": 6}, {"type": "text", "text": "Theorem 1 (Training time convergence). Suppose Assumptions 1, 2 hold. We let (k0)= 0 and set the learning rates as in (16). Then we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\pmb{\\theta}^{(t)})-\\operatorname*{inf}_{\\pmb{\\theta}}\\mathcal{L}(\\pmb{\\theta})\\leq\\left(1-\\frac{\\eta_{w}\\zeta_{0}}{2K}\\right)^{t}\\left(\\mathcal{L}(\\pmb{\\theta}^{(0)})-\\operatorname*{inf}_{\\pmb{\\theta}}\\mathcal{L}(\\pmb{\\theta})\\right),\\quad\\forall t\\in\\mathbb{N}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Proof. See Appendix C. ", "page_idx": 6}, {"type": "text", "text": "Theorem 1, together with Proposition 1, shows that the training loss converges to its minimum value at a linear rate, under mild assumptions of the task coefficients and token dictionary. This gives the first convergence result for transformers with multi-head softmax attention trained using GD to perform ICL tasks (see Table 1). Our convergence guarantee (18) also indicates that the convergence speed decreases as the size $K$ of the dictionary or the number $H$ of attention heads increases, which is intuitive because training with a larger vocabulary size or number of parameters is more challenging. However, a small $H$ will limit the expressive power of the model (see Section 3.3 for detailed discussion), and we require $H\\geq N$ to guarantee Assumption 2 holds, as stated in Proposition 1. ", "page_idx": 6}, {"type": "text", "text": "3.2 Inference time performance ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now move to examine the inference time performance, where the coefficient vector $\\lambda$ corresponding to the inference task may not drawn from $\\mathcal{D}_{\\lambda}$ . In fact, we only assume that the coefficient vector $\\lambda$ at inference time is bounded as in the following assumption. ", "page_idx": 6}, {"type": "text", "text": "Assumption 4 (Boundedness of $\\lambda$ at inference time). We assume that at inference time $\\|\\pmb{\\lambda}\\|_{2}\\le B$ for some $B>0$ . ", "page_idx": 6}, {"type": "text", "text": "For notational simplicity, let $Z^{Q}\\in\\mathbb{R}^{m\\times(K-N)}$ denote ", "page_idx": 6}, {"type": "equation", "text": "$$\nZ^{Q}:=(f({\\boldsymbol{v}}_{N+1}),\\cdot\\cdot\\cdot\\mathbf{\\nabla},f({\\boldsymbol{v}}_{K}))\\in\\mathbb{R}^{m\\times(K-N)}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The following theorem characterizes the performance guarantee of the transformer\u2019s output $\\hat{\\pmb y}$ (after sufficient training) at the inference time. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2 (Inference time performance). Let\u03bb  be the solution to the following ridge regression problem: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\lambda}:=\\arg\\operatorname*{min}_{\\mathbf{\\lambda}}\\left\\{\\frac{1}{2N}\\sum_{i=1}^{N}(y_{i}-\\mathbf{\\lambda}^{\\mathsf{T}}f(\\pmb{v}_{i}))^{2}+\\frac{m\\tau}{2N}\\left\\|\\mathbf{\\lambda}\\right\\|_{2}^{2}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Under the assumptions in Theorem $^{\\,l}$ , for any $\\varepsilon>0$ and $\\delta\\in(0,1)$ , if the number of training iterates $T$ satisfies ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T\\ge\\frac{\\log\\bigg(B^{2}\\Delta^{(0)}\\Big(\\|Z\\|_{2}+\\sqrt{\\tau}\\big(2\\sqrt{N\\log(1/\\delta)}+2\\log(1/\\delta)+N\\big)^{1/2}\\Big)^{2}\\bigg/(m\\tau\\varepsilon)\\bigg)}{\\log\\big(1/\\big(1-\\frac{\\eta_{w}\\zeta_{0}}{2K}\\big)\\big)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "then given any prompt $P$ that satisfies Assumption $^{4}$ at the inference time, with probability at least $1-\\delta$ , the output of the transformer $\\hat{\\pmb y}$ satisfies ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{1}{2K}\\left\\|\\widehat{\\pmb{y}}-\\widehat{\\pmb{y}}^{\\star}\\right\\|_{2}^{2}\\leq\\varepsilon,\\qquad w i t h\\quad\\widehat{\\pmb{y}}^{\\star}:=\\left(\\underset{\\left(\\displaystyle{Z^{Q}}\\right)^{\\top}\\widehat{\\pmb{\\chi}}}{\\widehat{\\pmb{y}}}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Proof. See Appendix D. ", "page_idx": 7}, {"type": "text", "text": "In Theorem 2, (22) shows that after training, the transformer learns to output the given labels of the first $N$ tokens in each prompt, and more importantly, predicts the labels of the rest $K-N$ tokens by implementing the ridge regression given in (20). Note that Aky\u00fcrek et al. [2022] studied the expressive power of transformers on the linear regression task and showed by construction that transformers can represent the closed-form ridge regression solution. Interestingly, here we show from an optimization perspective that transformers can in fact be trained to do so. ", "page_idx": 7}, {"type": "text", "text": "Generalization capabilities of the pretrained transformer. Theorem 2 captures two generalization capabilities that the pretrained transformer can have. ", "page_idx": 7}, {"type": "text", "text": "i) Contextual generalization to unseen examples: Theorem 2 suggests that the transformer exploits the inherent contextual information (to be further discussed in Section 3.3) of the function template in the given prompt, and can further use such information to predict the unseen tokens.   \nii) Generalization to unseen tasks: Theorem 2 also suggests that the pretrained transformer can generalize to a function map corresponding to any $\\pmb{\\lambda}\\in\\mathbb{R}^{m}$ at the inference time (albeit satisfying Assumption 4), which is not necessarily sampled from the support of its training distribution $\\mathcal{D}_{\\lambda}$ . ", "page_idx": 7}, {"type": "text", "text": "We note that the contextual generalization that the transformer has here is different in nature from the prediction ability shown in previous works on ICL [Huang et al., 2023, Chen et al., 2024, Li et al., 2024, Nichani et al., 2024]. Those work focuses on a setting where each prompt contains a good portion of tokens similar to the query token, allowing the transformer to directly use the label of the corresponding answers from the prompt as the prediction. However, in practical scenarios, prompts often contain only partial information, and our analysis sheds lights on explaining how transformers generalize to unseen examples by leveraging ridge regression to infer the underlying template. ", "page_idx": 7}, {"type": "text", "text": "How does the representation dimension affect the performance? Beyond the above discovery, several questions are yet to be explored. For instance, while we demonstrate that transformers can be trained to implement ridge regression, how good is the performance of the ridge regression itself? What is the best choice of ridge regression we could expect? How close is the transformer\u2019s choice to the best possible choice? We address these questions as follows. ", "page_idx": 7}, {"type": "text", "text": "Given any prompt $P$ at inference time, since there is no label information about the rest $K-N$ tokens, the best prediction we could hope for from the transformer shall be ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\widehat{\\pmb{y}}^{\\mathrm{best}}:=\\left(\\mathbf{\\Xi}_{\\left(Z^{Q}\\right)}^{\\pmb{y}}\\!\\!\\!\\top\\!\\mathbf{\\widehat{\\lambda}}_{\\widehat{\\lambda}_{\\tau}}\\right),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $Z^{Q}$ is defined in (19), and $\\widehat{\\lambda}_{\\tau}$ satisfies: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\lambda}_{\\tau}:=\\arg\\operatorname*{min}_{\\mathbf{\\lambda}}\\mathbb{E}_{\\widetilde{\\epsilon}}\\left[\\frac{1}{2N}\\sum_{i=1}^{N}(y_{i}-\\mathbf{\\lambda}^{\\top}\\left(f(\\pmb{v}_{i})+\\pmb{\\epsilon}_{i}\\right))^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In other words, we hope the transformer outputs the given $N$ labels as they are. For the rest $K-N$ labels, the best we could hope for is that the transformer estimates the coefficient vector $\\lambda$ by solving the above regression problem to obtain $\\widehat{\\lambda}_{\\tau}$ , and predict the $k$ -th label by $\\widehat{\\lambda}_{\\tau}^{\\top}f(\\pmb{v}_{k})$ for $k=N+1,\\cdots\\,,K$ . Note that (24) is equivalent to the following ridge regression problem (see Lemma 4 in the appendix for its derivation): ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\lambda}_{\\tau}=\\arg\\operatorname*{min}_{\\mathbf{\\lambda}}\\left\\{\\frac{1}{2N}\\sum_{i=1}^{N}(y_{i}-\\mathbf{\\lambda}^{\\sf T}f(\\pmb{v}_{i}))^{2}+\\frac{\\tau}{2}\\left\\|\\mathbf{\\lambda}\\right\\|_{2}^{2}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The only difference between the two ridge regression problems (20) and (25) is the coefficient of the regularization term. This indicates that at the training time, the transformer learns to implement ridge regression to predict the labels of the rest $K-N$ tokens, assuming the noise level is given by ${\\frac{m}{N}}\\tau$ This observation also reflects how the sequence length $N$ affects the transformer\u2019s preference for choosing templates and its performance at inference time: ", "page_idx": 8}, {"type": "text", "text": "\u2022 The closer $m$ is to $N$ , the closer the transformer\u2019s choice of templates is to the best possible choice, and the better the transformer\u2019s prediction will be;   \n\u2022 When $N<m$ , the transformer tends to underfit by choosing a $\\lambda$ with small $\\ell_{2}$ -norm;   \n\u2022 When $N>m$ , the transformer tends to overfti since it underestimates the noise level and in turn captures noise in the prediction. ", "page_idx": 8}, {"type": "text", "text": "3.3 Further interpretation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We provide more interpretation on our results, which may lead to useful insights into the ICL ability of the transformer. ", "page_idx": 8}, {"type": "text", "text": "How does the transformer gain ICL ability with representations? Intuitively speaking, our pretrained transformer gains in-context ability by extracting and memorizing some \u201cinherent information\u201d of all basic function maps $f_{i}~(i\\in[m])$ ) during the training. Such information allows it to infer the coefficient vector $\\lambda$ from the provided labels in each prompt and calculate the inner product $\\langle\\lambda,f(\\pmb{v}_{k})\\rangle$ to compute $y_{k}$ given any token $\\pmb{v}_{k}\\in\\mathcal{V}$ at inference time. To be more specific, the \u201cinherent information\u201d of all basic tasks could be described by the $N$ -by- $K$ matrix $\\pmb{A}$ defined as follows (see also (34)): ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{A}:=\\left(\\pmb{Z}^{\\top}\\pmb{Z}+m\\tau\\pmb{I}_{N}\\right)^{-1}\\left(\\pmb{Z}^{\\top}\\widehat{\\pmb{Z}}+(m\\tau\\pmb{I}_{N},\\mathbf{0})\\right)\\in\\mathbb{R}^{N\\times K},}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\widehat{\\pmb{Z}}:=(f(\\pmb{v}_{1}),\\cdot\\cdot\\cdot\\cdot,f(\\pmb{v}_{K}))=(\\pmb{Z},\\pmb{Z}^{Q})\\in\\mathbb{R}^{m\\times K}$ . During training, the transformer learns to approximate $A_{:,k}$ by $\\begin{array}{r}{\\sum_{h=1}^{H}w_{h,k}\\mathsf{s o f t m a x}(V^{\\top}Q_{h}\\boldsymbol{v}_{k})}\\end{array}$ for each $k\\in[K]$ . ", "page_idx": 8}, {"type": "text", "text": "To further elaborate, we take a closer look at the special case when the labels do not contain any noise, i.e., $\\tau=0$ , and $N\\geq m$ . In this case, $\\pmb{A}$ becomes $Z^{\\dagger}\\widehat{Z}$ , and given any prompt $P=P_{\\lambda}$ , the coefficient vector $\\lambda$ could be uniquely determined from the provided token-label pairs in the prompt. It is straightforward to verify that the label of each token $\\pmb{v}_{k}$ could be represented by the inner product of the given label vector $\\textit{\\textbf{y}}$ and the $k$ -th column of $z^{\\dagger}\\hat{z}$ , i.e., ", "page_idx": 8}, {"type": "equation", "text": "$$\ny_{k}=\\left\\langle\\pmb{y},\\pmb{Z}^{\\dag}\\widehat{\\pmb{Z}}_{:,k}\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Comparing the above equation with (8), it can be seen that in order to gain the in-context ability, the transformer needs to learn an approximation of $Z^{\\dag}\\widehat{Z}_{:,k}$ by $\\begin{array}{r}{\\sum_{h=1}^{H}w_{h,k}\\mathsf{\\bar{s o f t m a x}}(V^{\\top}Q_{h}\\boldsymbol{v}_{k})}\\end{array}$ for each $k\\in[K]$ . ", "page_idx": 8}, {"type": "text", "text": "More generally, in the proof of Theorem 2, we show that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\pmb{y}}_{k}^{\\star}=\\langle\\pmb{y},\\pmb{A}_{:,k}\\rangle\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "comparing which with (8) suggests that a small training error implies that $\\begin{array}{r}{\\sum_{h=1}^{H}w_{h,k}\\mathsf{s o f t m a x}(V^{\\top}Q_{h}\\boldsymbol{v}_{k})}\\end{array}$ is close to $A_{:,k}$ . In fact, this is the necessary and sufficient condition for the training loss to be small. A rigorous argument is provided in Lemma 5. ", "page_idx": 8}, {"type": "text", "text": "The necessity and trade-offs of multi-head attention mechanism. Multi-head attention mechanism is essential in our setting. In fact, it is generally impossible to train a shallow transformer with only one attention head to succeed in the ICL task considered in our paper. This is because, as we have discussed above, the key for the transformer is to approximate $A_{:,k}$ by $\\begin{array}{r}{\\sum_{h=1}^{H}w_{h,k}\\mathsf{s o f t m a x}(V^{\\top}Q_{h}\\boldsymbol{v}_{k})}\\end{array}$ for each $k\\in[K]$ . If $H=1$ , the transformer could not approximate each $A_{:,k}$ by $w_{1,k}\\mathsf{s o f t m a x}(V^{\\top}Q_{1}v_{k})$ in general since the entries of the latter vector are either all positive or all negative. In addition, Proposition 1 indicates that when $H\\geq N$ , the weights of the transformer with a simple initialization method satisfy our desired property that is crucial to guarantee the fast linear convergence. However, (18) implies that we should not set $H$ to be too large, since larger $H$ yields slower convergence rate. ", "page_idx": 9}, {"type": "text", "text": "4 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We analyze the training dynamics of a one-layer transformer with multi-head softmax attention trained by gradient descent to solve complex non-linear regression tasks using partially labeled prompts. In this setting, the labels contain Gaussian noise, and each prompt may include only a few examples, which are insufficient to determine the underlying template. Our work overcomes several restrictive assumptions made in previous studies and proves that the training loss converges linearly to its minimum value. Furthermore, we analyze the transformer\u2019s strategy for addressing the issue of underdetermination during inference and evaluate its performance by comparing it with the best possible strategy. Our study provides the first analysis of how transformers can acquire contextual (template) information to generalize to unseen examples when prompts contain a limited number of query-answer pairs. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The work of T. Yang and Y. Chi is supported in part by the grants NSF CCF-2007911, DMS-2134080 and ONR N00014-19-1-2404. The work of Y. Liang was supported in part by the U.S. National Science Foundation under the grants ECCS-2113860, DMS-2134145 and CNS-2112471. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "R. Agarwal, A. Singh, L. M. Zhang, B. Bohnet, S. Chan, A. Anand, Z. Abbas, A. Nova, J. D. Co-Reyes, E. Chu, et al. Many-shot in-context learning. arXiv preprint arXiv:2404.11018, 2024.   \nK. Ahuja, M. Panwar, and N. Goyal. In-context learning through the bayesian prism. arXiv preprint arXiv:2306.04891, 2023.   \nE. Aky\u00fcrek, D. Schuurmans, J. Andreas, T. Ma, and D. Zhou. What learning algorithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661, 2022.   \nY. Bai, F. Chen, H. Wang, C. Xiong, and S. Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. Advances in neural information processing systems, 36, 2024.   \nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \nS. Chen, H. Sheen, T. Wang, and Z. Yang. Training dynamics of multi-head softmax attention for in-context learning: Emergence, convergence, and optimality. arXiv preprint arXiv:2402.19442, 2024.   \nX. Chen and D. Zou. What can transformer learn with varying depth? case studies on sequence learning tasks. arXiv preprint arXiv:2404.01601, 2024.   \nD. Dai, Y. Sun, L. Dong, Y. Hao, S. Ma, Z. Sui, and F. Wei. Why can gpt learn in-context? language models implicitly perform gradient descent as meta-optimizers. arXiv preprint arXiv:2212.10559, 2022.   \nP. Deora, R. Ghaderi, H. Taheri, and C. Thrampoulidis. On the optimization and generalization of multi-head attention. arXiv preprint arXiv:2310.12680, 2023.   \nB. L. Edelman, S. Goel, S. Kakade, and C. Zhang. Inductive biases and variable creation in selfattention mechanisms. In International Conference on Machine Learning, pages 5793\u20135831. PMLR, 2022.   \nB. L. Edelman, E. Edelman, S. Goel, E. Malach, and N. Tsilivis. The evolution of statistical induction heads: In-context learning markov chains. arXiv preprint arXiv:2402.11004, 2024.   \nS. Garg, D. Tsipras, P. S. Liang, and G. Valiant. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583\u201330598, 2022.   \nA. Giannou, S. Rajput, J.-y. Sohn, K. Lee, J. D. Lee, and D. Papailiopoulos. Looped transformers as programmable computers. In International Conference on Machine Learning, pages 11398\u201311442. PMLR, 2023.   \nT. Guo, W. Hu, S. Mei, H. Wang, C. Xiong, S. Savarese, and Y. Bai. How do transformers learn in-context beyond simple functions? a case study on learning with representations. arXiv preprint arXiv:2310.10616, 2023.   \nM. Hahn and N. Goyal. A theory of emergent in-context learning as implicit structure induction. arXiv preprint arXiv:2303.07971, 2023.   \nC. Han, Z. Wang, H. Zhao, and H. Ji. In-context learning of large language models explained as kernel regression. arXiv preprint arXiv:2305.12766, 2023.   \nY. Huang, Y. Cheng, and Y. Liang. In-context convergence of transformers. arXiv preprint arXiv:2310.05249, 2023.   \nH. J. Jeon, J. D. Lee, Q. Lei, and B. Van Roy. An information-theoretic analysis of in-context learning. arXiv preprint arXiv:2401.15530, 2024.   \nH. Jiang. A latent space theory for emergent abilities in large language models. arXiv preprint arXiv:2304.09960, 2023.   \nH. Karimi, J. Nutini, and M. Schmidt. Linear convergence of gradient and proximal-gradient methods under the Polyak-\u0141ojasiewicz condition. In European Conference on Machine Learning and Knowledge Discovery in Databases, pages 795\u2013811, 2016.   \nJ. Kim and T. Suzuki. Transformers learn nonlinear features in context. In ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2024.   \nB. Laurent and P. Massart. Adaptive estimation of a quadratic functional by model selection. Annals of statistics, pages 1302\u20131338, 2000.   \nH. Li, M. Wang, S. Lu, X. Cui, and P.-Y. Chen. Training nonlinear transformers for efficient in-context learning: A theoretical learning and generalization analysis. arXiv preprint arXiv:2402.15607, 2024.   \nY. Li, M. E. Ildiz, D. Papailiopoulos, and S. Oymak. Transformers as algorithms: Generalization and stability in in-context learning. In International Conference on Machine Learning, pages 19565\u201319594. PMLR, 2023.   \nQ. N. Nguyen and M. Mondelli. Global convergence of deep networks with one wide layer followed by pyramidal topology. Advances in Neural Information Processing Systems, 33:11961\u201311972, 2020.   \nE. Nichani, A. Damian, and J. D. Lee. How transformers learn causal structure with gradient descent. arXiv preprint arXiv:2402.14735, 2024.   \nC. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann, A. Askell, Y. Bai, A. Chen, et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022.   \nM. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b. Alayrac, R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.   \nH. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \nJ. Von Oswald, E. Niklasson, E. Randazzo, J. Sacramento, A. Mordvintsev, A. Zhmoginov, and M. Vladymyrov. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, pages 35151\u201335174. PMLR, 2023.   \nX. Wang, W. Zhu, M. Saxon, M. Steyvers, and W. Y. Wang. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning. In Workshop on Efficient Systems for Foundation Models@ ICML2023, 2023.   \nJ. Wei, J. Wei, Y. Tay, D. Tran, A. Webson, Y. Lu, X. Chen, H. Liu, D. Huang, D. Zhou, et al. Larger language models do in-context learning differently. arXiv preprint arXiv:2303.03846, 2023.   \nN. Wies, Y. Levine, and A. Shashua. The learnability of in-context learning. Advances in Neural Information Processing Systems, 36, 2024.   \nJ. Wu, D. Zou, Z. Chen, V. Braverman, Q. Gu, and P. L. Bartlett. How many pretraining tasks are needed for in-context learning of linear regression? arXiv preprint arXiv:2310.08391, 2023.   \nS. M. Xie, A. Raghunathan, P. Liang, and T. Ma. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021.   \nR. Zhang, S. Frei, and P. L. Bartlett. Trained transformers learn linear models in-context. arXiv preprint arXiv:2306.09927, 2023a.   \nY. Zhang, F. Zhang, Z. Yang, and Z. Wang. What and how does in-context learning learn? bayesian model averaging, parameterization, and generalization. arXiv preprint arXiv:2305.19420, 2023b. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "image", "img_path": "ik37kKxKBm/tmp/29ed6eb14bb7e18134a4c7f7d91c745270066e7769e3bdcb76c863641d7f1ff0.jpg", "img_caption": ["Figure 2: Training and inference losses of (a) 1-layer and (b) 4-layer transformers, which validate Theorem 2, as well as the transformer\u2019s contextual generalization to unseen examples and to unseen tasks. "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "A Experiments ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "This section aims to provide some empirical validation to our theoretical findings and verify that some of our results could be generalized to deeper transformers. ", "page_idx": 12}, {"type": "text", "text": "Setup. We conduct experiments on a synthetic dataset, where we randomly generate each token $\\pmb{v}_{k}$ and their representation $f(\\pmb{v}_{k})$ from standard Gaussian distribution. We employ both the 1-layer transformer described in Section 2 and a standard 4-layer transformer in Vaswani et al. [2017] with $d_{\\mathrm{model}}=256$ and $d_{\\mathrm{ff}}=512$ . We set the training loss to be the population loss defined in (9), and initialize $\\{Q_{h}^{(0)}\\}_{h\\in[H]}$ using standard Gaussian and set $\\{w_{h}^{(0)}\\}_{h\\in[H]}$ to be 0, identical to what is specified in Section 3. We generate $\\lambda$ from standard Gaussian distribution to create the training set with 10000 samples and in-domain test set with 200 samples; we also create an out-of-domain (ood) test set with 200 samples by sampling $\\lambda$ from $\\mathcal{N}(\\mathbf{1}_{m},4I_{m})$ . Given $\\lambda$ , we generate the label $y_{k}$ of token $\\pmb{v}_{k}$ using (1), for $k\\,\\in\\,[K]$ . We train with a batch size 256. All experiments use the Adam optimizer with a learning rate $1\\stackrel{.}{\\times}10^{-4}$ . ", "page_idx": 12}, {"type": "text", "text": "Training and inference performance. We set $N=30$ , $K=200$ , $d=100$ , $m=20$ , and set $H$ to be 64 and 8 for 1-layer and 4-layer transformers, respectively. Figure 2 shows the training and inference losses of both 1-layer and 4-layer transformers, where we measure the inference loss by $\\frac{1}{K}\\|\\widehat{\\pmb y}-\\widehat{\\pmb y}^{\\star}\\|_{2}^{2}$ to validate (22): after sufficient training, the output of the transformer $\\hat{\\pmb y}$ converges to $\\widehat{\\pmb{y}}^{\\star}$ .  From  Figure 2 we can see that for both 1-layer and 4-layer transformers, the thr e e curves have t he same descending trend, despite the inference loss on the ood dataset is higher than that on the in-domain dataset. This experiment also shows the transformer\u2019s contextual generalization to unseen examples and to unseen tasks, validating our claim in Section 3.2. ", "page_idx": 12}, {"type": "text", "text": "Figure 3 plots the performance gap $\\frac{1}{K}\\left\\|\\widehat{\\pmb{y}}^{\\star}-\\widehat{\\pmb{y}}^{\\mathrm{best}}\\right\\|_{2}^{2}$ of the one-layer transformer with respect to different $N$ ranging from 50 to 150, when we fix $m=100$ and $\\tau=0.01$ . This verifies that the ridge regression implemented by the pretrained transformer has a better performance when $m$ is close to $N$ , again verifying our claim at the end of Section 2. ", "page_idx": 12}, {"type": "text", "text": "Impact of the number of attention heads. We now turn to examine the impact of the number of attention heads. In this experiment, we use the population loss (9), and set the other configurations same as those in Figure 2. Figure 4 shows the training loss curves for different $H$ with respect the iteration number, which validates our claims. From Figure 4, we can see that we need to set $H$ large enough to guarantee the convergence of the training loss. However, setting $H$ too large $H=400)$ ) leads to instability and divergence of the loss. Recall that in Proposition 1, we require $H\\geq N$ to guarantee our convergence results hold. Although this condition may not be necessary, Figure 4 shows that when $H<N=30$ , the loss stopped descending even when it is far from the minimal value. On the other side, the loss keeps descending when $H=30$ (though slowly). ", "page_idx": 12}, {"type": "image", "img_path": "ik37kKxKBm/tmp/a70a0d2386355060cfcf54cbeb42f78f6e0fc5a767a631a04eb432bfd5e74df2.jpg", "img_caption": ["Figure 3: The performance gap $\\frac{1}{K}\\left\\|\\widehat{\\pmb{y}}^{\\star}-\\widehat{\\pmb{y}}^{\\mathrm{best}}\\right\\|_{2}^{2}$ with different $N$ when $m=100$ , which validates that the closer $N$ is to $m$ , the better the transformer\u2019s prediction is. "], "img_footnote": [], "page_idx": 13}, {"type": "image", "img_path": "ik37kKxKBm/tmp/c49576b04eea05a2b12cf822d4a99f2623c5fca187b7bb3a8c6d6287437c2202.jpg", "img_caption": ["Figure 4: Training losses of the 1-layer transformer with different number of attention heads $H$ , where $H$ should be large enough to guarantee the convergence of the training loss, but setting $H$ too large leads to instability and slower divergence. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "We also explore how $H$ affects the training of the 4-layer transformer, as displayed in Figure 5, where we set $K=200$ and the configurations other than $H$ are the same as in Figure 3. We fix the wall-clock time to be 100 seconds and plot the training loss curves with different $H$ . Figure 5 (a) shows the final training and inference losses with respect to $H$ . It reflects that the losses converge faster with smaller $H$ (here the final training loss is the smallest when $H=4$ ). The training curves in Figure 5 (b) corresponding to different $H$ within 100s may provide some explanation to this phenomenon: (i) transformers with larger $H$ could complete less iterations within a fixed amount of time (the curves corresponding to larger $H$ are shorter); (ii) the training loss curves corresponding to large $H$ ( $H=32$ , 64) descend more slowly. This suggests our claim that larger $H$ may yield slower convergence rate is still valid on deeper transformers. Note that unlike the 1-layer transformer, deeper transformers don\u2019t require a large $H$ to guarantee convergence. This is because deeper transformers have better expressive power even when $H$ is small. ", "page_idx": 13}, {"type": "text", "text": "B Proof Preparation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Summary of key notation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We summarize the frequently used notation in Table 2 for ease of reference. ", "page_idx": 13}, {"type": "text", "text": "B.2 Auxiliary lemmas ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We provide some useful facts that will be repeatedly used later on. Let ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{z_{k}:=f(\\pmb{v}_{k})=(f_{1}(\\pmb{v}_{k}),f_{2}(\\pmb{v}_{k}),\\cdot\\cdot\\cdot\\cdot,f_{m}(\\pmb{v}_{k}))^{\\top}\\in\\mathbb{R}^{m},\\qquad\\forall k\\in[K].}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Recalling (12), we can rewrite ", "page_idx": 13}, {"type": "equation", "text": "$$\nZ:=(z_{1},\\cdots,z_{N})\\in\\mathbb{R}^{m\\times N}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We further define $\\pmb{s}_{k}^{h}\\in\\mathbb{R}^{N}$ as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{s_{k}^{h}:=\\mathsf{s o f t m a x}(V^{\\top}Q_{h}v_{k})=(s_{1k}^{h},\\cdot\\cdot\\cdot\\cdot,s_{N k}^{h})^{\\top},\\quad\\forall k\\in[K],h\\in[H].}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "image", "img_path": "ik37kKxKBm/tmp/863381603fecdb7ec6ca92654744c5d78f7824eac77b7a090790ea8c67d085fa.jpg", "img_caption": ["(a) final losses vs H "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "ik37kKxKBm/tmp/d464c39ee78a12cc443024f46f6ac33ed29e674083831d136f6c0344fe8df354.jpg", "img_caption": ["(b) training loss curves for different $H$ "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 5: Training losses of a 4-layer transformer with different $H$ , fixing wall-clock time to be 100s. This experiment shows that unlike 1-layer transformers, deeper transformers don\u2019t require $H$ to be large to guarantee convergence of the loss. ", "page_idx": 14}, {"type": "table", "img_path": "ik37kKxKBm/tmp/493e7bd2fdf6ae75c05c5f8271c836601757203e42e4d4f6142024fcc480af21.jpg", "table_caption": [], "table_footnote": ["Table 2: Notation for key parameters. "], "page_idx": 14}, {"type": "text", "text": "Lemma 1 (Softmax gradient). For all $j\\in[N],k\\in[K]$ and $h\\in[H]$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{\\partial s_{j k}^{h}}{\\partial\\pmb{Q}_{h}}=s_{j k}^{h}\\sum_{i=1}^{N}s_{i k}^{h}\\big(\\pmb{v}_{j}-\\pmb{v}_{i}\\big)\\pmb{v}_{k}^{\\top},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where sjhk is defined in (28). ", "page_idx": 14}, {"type": "text", "text": "Proof. See the proof of Lemma A.1 in Huang et al. [2023]. ", "page_idx": 14}, {"type": "text", "text": "Lemma 2 (Smoothness of softmax). For vectors $\\pmb{\\xi}_{1},\\pmb{\\xi}_{2}\\in\\mathbb{R}^{l}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathsf{s o f t m a x}(\\xi_{1})-\\mathsf{s o f t m a x}(\\xi_{2})\\|_{1}\\leq2\\left\\|\\xi_{1}-\\xi_{2}\\right\\|_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. See Corollary A.7 in Edelman et al. [2022]. ", "page_idx": 14}, {"type": "text", "text": "We also need to make use of the following form of Young\u2019s inequality. ", "page_idx": 14}, {"type": "text", "text": "Lemma 3. For any $\\pmb{x}_{1},\\cdots,\\pmb{x}_{l}\\in\\mathbb{R}^{p}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\|\\sum_{i=1}^{l}\\pmb{x}_{i}\\right\\|_{2}^{2}\\leq l\\sum_{i=1}^{l}\\|\\pmb{x}_{i}\\|_{2}^{2}\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The following lemma shows the equivalence between (24) and (25). ", "page_idx": 14}, {"type": "text", "text": "Lemma 4 (Equivalence of the regression problems). Given any prompt $P_{\\lambda}:=(\\pmb{v}_{1},y_{1},\\cdots,\\pmb{v}_{N},y_{N})$ , we have the following equivalence: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\epsilon}\\left[\\frac{1}{2N}\\sum_{i=1}^{N}(y_{i}-\\lambda^{\\top}\\left(f(v_{i})+\\epsilon_{i}\\right))^{2}\\right]=\\frac{1}{2N}\\sum_{i=1}^{N}(y_{i}-\\lambda^{\\top}f(v_{i}))^{2}+\\frac{\\tau}{2}\\left\\|\\boldsymbol{\\lambda}\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "C Proof of Theorem 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We first outline the proof. To prove Theorem 1, we first remove the expectation in the expression of the loss function $\\mathcal{L}$ in (9) by reformulating it to a deterministic form (see Lemma 5). With this new form, we show by induction that the loss function $\\mathcal{L}$ is smooth (Lemma 10) and satisfies the Polyak-\u0141ojasiewicz (PL) condition (c.f. (49)). Provided with both smoothness and PL conditions, we are able to give the desired linear convergence rate [Karimi et al., 2016]. ", "page_idx": 15}, {"type": "text", "text": "We define ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\delta_{k}^{\\theta}:=\\left\\{\\underset{\\sum_{h=1}^{H}w_{h,k}s_{k}^{h}}{\\sum_{h=1}^{H}w_{h,k}s_{k}^{h}-\\left(\\boldsymbol Z^{\\top}\\boldsymbol Z+m\\tau I\\right)^{-1}\\left(z_{k}+m\\tau e_{k}\\right),}\\right.\\mathrm{~if~}k\\in[N],}\\\\ {\\left.\\sum_{h=1}^{H}w_{h,k}s_{k}^{h}-\\left(\\boldsymbol Z^{\\top}\\boldsymbol Z+m\\tau I\\right)^{-1}z_{k},\\right.\\mathrm{~if~}k\\in[K]\\setminus[N].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We also define the following matrices: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\pmb{A}:=\\left(\\pmb{Z}^{\\top}\\pmb{Z}+m\\tau\\pmb{I}_{N}\\right)^{-1}\\left(\\pmb{Z}^{\\top}\\widehat{\\pmb{Z}}+(m\\tau\\pmb{I}_{N},\\mathbf{0})\\right)\\in\\mathbb R^{N\\times K},}\\\\ &{\\displaystyle\\widehat{\\pmb{A}}(\\pmb{\\theta}):=\\left(\\sum_{h=1}^{H}w_{h,1}\\pmb{s}_{1}^{h},\\cdots,\\sum_{h=1}^{H}w_{h,K}\\pmb{s}_{K}^{h}\\right)\\in\\mathbb R^{N\\times K},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\widehat{\\pmb{Z}}:=(\\pmb{z}_{1},\\cdot\\cdot\\cdot\\mathrm{~,~}\\pmb{z}_{K})\\in\\mathbb{R}^{m\\times K}$ . ", "page_idx": 15}, {"type": "text", "text": "We first reformulate the loss function to remove the expectation in the population loss. ", "page_idx": 15}, {"type": "text", "text": "Lemma 5 (Reformulation of the loss function). Under Assumption $^{\\,l}$ , the loss function ${\\mathcal{L}}(\\theta)$ could be rewritten into the following equivalent form: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\boldsymbol{\\theta})=\\frac{1}{2K}\\sum_{k=1}^{K}\\left\\|\\left(\\boldsymbol{Z}^{\\top}\\boldsymbol{Z}+m\\tau\\boldsymbol{I}\\right)^{1/2}\\boldsymbol{\\delta}_{k}^{\\theta}\\right\\|_{2}^{2}+\\mathcal{L}^{\\star}=\\frac{1}{2K}\\sum_{k=1}^{K}\\left\\|\\bar{\\boldsymbol{Z}}\\boldsymbol{\\delta}_{k}^{\\theta}\\right\\|_{2}^{2}+\\mathcal{L}^{\\star},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle{\\mathcal E}^{\\star}=\\frac{1}{2K}\\sum_{k=1}^{N}\\left(-\\left(Z^{\\top}z_{k}+m\\tau e_{k}\\right)^{\\top}\\left(Z^{\\top}Z+m\\tau I\\right)^{-1}\\left(Z^{\\top}z_{k}+m\\tau e_{k}\\right)+\\left\\|z_{k}\\right\\|_{2}^{2}+m\\tau\\right)}}\\\\ {{\\displaystyle+\\left.\\frac{1}{2K}\\sum_{k=N+1}^{K}\\left(-\\left(Z^{\\top}z_{k}\\right)^{\\top}\\left(Z^{\\top}Z+m\\tau I\\right)^{-1}\\left(Z^{\\top}z_{k}\\right)+\\left\\|z_{k}\\right\\|_{2}^{2}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "is a constant that does not depend on $\\pmb{\\theta}$ , and $\\bar{Z}$ is defined in (12). ", "page_idx": 15}, {"type": "text", "text": "Proof. See Appendix E.3. ", "page_idx": 15}, {"type": "text", "text": "Lemma 5 indicates that $\\mathcal{L}^{\\star}$ is a lower bound of $\\mathcal{L}$ . We\u2019ll later show that $\\mathcal{L}^{\\star}$ is actually the infimum of $\\mathcal{L}$ , i.e., $\\mathcal{L}^{\\star}=\\operatorname*{inf}_{\\theta}\\mathcal{L}(\\theta)$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma 5 also indicates that, the necessary and sufficient condition for $\\mathcal{L}(\\pmb{\\theta}^{(t)})$ to converge to $\\mathcal{L}^{\\star}$ during training is ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\forall k\\in[K]:\\quad\\delta_{k}^{\\theta^{t}}\\to\\mathbf{0},\\quad t\\to\\infty,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which folllows immediately that (37) is equivalent to ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\widehat{\\pmb{A}}(\\pmb{\\theta}^{(t)})-\\pmb{A}\\rightarrow\\mathbf{0},\\quad t\\rightarrow\\infty.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "To simplify the analysis, we introduce the following reparameterization to unify the learning rates of all parameters, and we\u2019ll consider the losses after reparameterization in the subsequent proofs. ", "page_idx": 15}, {"type": "text", "text": "Lemma 6 (Reparameterization). Define ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\gamma:=\\sqrt{\\eta_{w}/\\eta_{Q}},\\quad\\alpha_{h}:={\\pmb w}_{h}/\\gamma,\\quad\\forall h\\in[H],\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\pmb{\\xi}:=\\{\\pmb{Q}_{h},\\pmb{\\alpha}_{h}\\}_{h=1}^{H},\\quad\\ell(\\pmb{\\xi}):=\\mathcal{L}(\\pmb{\\theta}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then (10) is equivalent to ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\pmb{\\xi}^{(t)}=\\pmb{\\xi}^{(t-1)}-\\eta_{Q}\\nabla_{\\pmb{\\xi}}\\ell(\\pmb{\\xi}^{(t-1)}),\\quad\\forall t\\in[T].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. See Appendix E.4. ", "page_idx": 16}, {"type": "text", "text": "We denote $_{\\alpha}$ as $\\pmb{\\alpha}:=(\\alpha_{h,k})_{h\\in[H],k\\in[K]}\\in\\mathbb{R}^{H\\times K}.$ ", "page_idx": 16}, {"type": "text", "text": "The following lemma bounds the gradient norms by the loss function, which is crucial to the proof of Theorem 1. ", "page_idx": 16}, {"type": "text", "text": "Lemma 7 (Upper bound of the gradient norms). Suppose Assumption $^{\\,l}$ holds and $|\\alpha_{h,k}^{(t)}|\\leq\\alpha$ . Then for all $h\\in[H]$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\|\\frac{\\partial\\ell(\\pmb{\\xi}^{(t)})}{\\partial\\pmb{Q}_{h}^{(t)}}\\right\\|_{F}\\leq2\\sqrt{2}\\gamma\\alpha\\bar{f}_{\\mathrm{max}}\\sqrt{\\ell(\\pmb{\\xi}^{(t)})-\\mathscr{L}^{\\star}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. See Appendix E.5. ", "page_idx": 16}, {"type": "text", "text": "Now we are ready to give the main proof. ", "page_idx": 16}, {"type": "text", "text": "Proof of Theorem 1. To prove Theorem 1, it suffices to prove that under our assumptions, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{(Upper~bound~of~the~parameters;~}}&{\\left\\|\\alpha_{h}^{(t)}\\right\\|_{2}\\leq\\alpha,}\\\\ {\\mathrm{(Lower~bound~of~eigenvalues;)}\\quad\\lambda_{\\operatorname*{min}}\\left(B_{k}^{(t)}B_{k}^{(t)\\top}\\right)\\geq\\frac{\\zeta_{0}}{2},}\\\\ {\\mathrm{(Linear~decay~of~the~loss;)}\\quad\\mathcal{L}(\\theta^{(t)})-\\mathcal{L}^{\\star}\\leq\\left(1-\\displaystyle\\frac{\\eta_{Q}\\sigma}{2}\\right)^{t}\\left(\\mathcal{L}(\\theta^{(0)})-\\mathcal{L}^{\\star}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sigma:=\\frac{\\zeta_{0}\\gamma^{2}}{K},\\quad\\alpha:=\\sqrt{2K}\\frac{4\\left\\lVert\\bar{\\pmb{Z}}\\right\\rVert_{2}}{\\gamma\\zeta_{0}}\\sqrt{\\mathcal{L}(\\pmb{\\theta}^{(0)})-\\mathcal{L}^{\\star}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and $\\gamma,\\alpha_{h}$ is defined in (39), $\\zeta_{0}$ is defined in (14). We shall prove (43a),(43b) and (43c) by induction. ", "page_idx": 16}, {"type": "text", "text": "Base case. It is apparent that (43a),(43b) and (43c) all hold when $t=0$ . ", "page_idx": 16}, {"type": "text", "text": "Induction. We make the following inductive hypothesis, i.e., when $s\\in[t-1]$ , (43a), (43b) and (43c) hold. Below we prove that (43a),(43b) and (43c) hold when $s=t$ by the following steps. ", "page_idx": 16}, {"type": "text", "text": "Step 1: verify (43b) and the Polyak-\u0141ojasiewicz condition. We first compute the gradient of the loss w.r.t. $_{\\alpha}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\forall k\\in\\left[K\\right]:\\quad\\frac{\\partial\\ell\\left(\\pmb{\\xi}\\right)}{\\partial\\alpha_{k}}=\\frac{1}{2K}\\frac{\\partial}{\\partial\\alpha_{k}}\\left\\Vert\\bar{Z}\\pmb{\\delta}_{k}^{\\theta}\\right\\Vert_{2}^{2}=\\frac{1}{2K}\\frac{\\partial}{\\partial\\alpha_{k}}\\left\\Vert\\bar{Z}\\left(\\gamma C_{k}\\alpha_{k}-\\boldsymbol{A}_{:k}\\right)\\right\\Vert_{2}^{2}}\\\\ {=\\frac{\\gamma}{K}\\left(\\bar{Z}C_{k}\\right)^{\\top}\\bar{Z}\\pmb{\\delta}_{k}^{\\theta}=\\frac{\\gamma}{K}\\pmb{B}_{k}^{\\top}\\bar{Z}\\pmb{\\delta}_{k}^{\\theta},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the first equality follows from Lemma 5, $C_{k},B_{k}$ is defined in (13). ", "page_idx": 16}, {"type": "text", "text": "Let $b_{k}^{h}$ denote the $h$ -th column vector of $B_{k}$ , $\\textit{h}\\in[H]$ , i.e., $B_{k}\\,:=\\,(\\pmb{b}_{k}^{1},\\cdot\\cdot\\cdot\\,,\\pmb{b}_{k}^{H})$ , then for any $k\\in[K]$ and $t\\in\\mathbb{N}_{+}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|(b_{k}^{h})^{(t)}-(b_{k}^{h})^{(0)}\\right\\|_{2}\\leq\\left\\|\\bar{Z}\\right\\|_{2}\\left\\|(s_{k}^{h})^{(t)}-(s_{k}^{h})^{(0)}\\right\\|_{2}}&{}\\\\ {\\leq\\left\\|\\bar{Z}\\right\\|_{2}\\left\\|(s_{k}^{h})^{(t)}-(s_{k}^{h})^{(0)}\\right\\|_{1}}&{}\\\\ {\\leq2\\left\\|\\bar{Z}\\right\\|_{2}\\left\\|V^{\\top}(Q_{h}^{(t)}-Q_{h}^{(0)})v_{k}\\right\\|_{\\infty}}&{}\\\\ {\\leq2\\left\\|\\bar{Z}\\right\\|_{2}\\underset{j\\in[N]}{\\operatorname*{max}}\\left|v_{j}^{\\top}(Q_{h}^{(t)}-Q_{h}^{(0)})v_{k}\\right|}&{}\\\\ {\\leq2\\left\\|\\bar{Z}\\right\\|_{2}\\left\\|Q_{h}^{(t)}-Q_{h}^{(0)}\\right\\|_{F},}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the third line uses Lemma 2, and that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\forall h\\in[H]:\\ }&{\\left\\lVert Q_{h}^{(t)}-Q_{h}^{(0)}\\right\\rVert_{F}\\leq\\displaystyle\\sum_{s=0}^{t-1}\\eta\\left\\lVert\\frac{\\partial\\ell(\\xi^{(s)})}{\\partial Q_{h}^{(s)}}\\right\\rVert_{F}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\sum_{s=0}^{t-1}2\\sqrt{2}\\eta\\gamma\\alpha\\bar{f}_{\\operatorname*{max}}\\sqrt{\\ell(\\xi^{(s)})-\\mathcal{L}^{\\star}}}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\sqrt{2}\\eta\\gamma\\alpha\\bar{f}_{\\operatorname*{max}}\\sqrt{\\mathcal{L}(\\theta^{(0)})-\\mathcal{L}^{\\star}}\\displaystyle\\sum_{s=0}^{t-1}\\left(\\sqrt{1-\\frac{\\eta\\sigma}{2}}\\right)^{s}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{8\\sqrt{2}\\gamma\\alpha\\bar{f}_{\\operatorname*{max}}}{\\sigma}\\sqrt{\\mathcal{L}(\\theta^{(0)})-\\mathcal{L}^{\\star}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the second inequality follows from Lemma 7 (cf. (42)) and the third inequality follows from the inductive hypothesis and the fact that $\\ell(\\pmb{\\xi}^{(s)})=\\mathcal{L}(\\pmb{\\theta}^{(s)}),\\forall s.$ Combining (47) with (46), we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|{B}_{k}^{(t)}-{B}_{k}^{(0)}\\right\\|_{F}\\leq2\\left\\|\\bar{Z}\\right\\|_{2}\\sqrt{\\displaystyle\\sum_{h=1}^{H}\\left\\|{Q}_{h}^{(t)}-{Q}_{h}^{(0)}\\right\\|_{F}^{2}}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left\\|\\bar{Z}\\right\\|_{2}\\sqrt{H}\\displaystyle\\frac{16\\sqrt{2}\\gamma\\alpha\\bar{f}_{\\operatorname*{max}}}{\\sigma}\\sqrt{\\mathcal{L}(\\theta^{(0)})-\\mathcal{L}^{\\star}}}\\\\ &{\\qquad\\qquad\\leq\\left(1-1/\\sqrt{2}\\right)\\sqrt{\\zeta_{0}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the last inequality follows from (15). The above inequality (48) indicates that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\forall x\\in\\mathbb{R}^{K}:\\quad\\left\\|x^{\\top}B_{k}^{(t)}\\right\\|_{2}\\geq\\left\\|x^{\\top}B_{k}^{(0)}\\right\\|_{2}-\\left\\|x^{\\top}(B_{k}^{(t)}-B_{k}^{(0)})\\right\\|_{2}\\geq\\sqrt{\\zeta_{0}/2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which gives (43b). ", "page_idx": 17}, {"type": "text", "text": "Therefore, we obtain the following PL condition: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\nabla_{\\theta}\\ell(\\pmb{\\xi}^{(t)})\\right\\|_{F}^{2}\\geq\\displaystyle\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\left(\\frac{\\partial\\ell(\\pmb{\\xi})}{\\partial\\alpha_{h,k}}\\right)^{2}=\\frac{\\gamma^{2}}{K^{2}}\\displaystyle\\sum_{k=1}^{K}\\left(\\bar{Z}\\delta_{k}^{(t)}\\right)^{\\top}\\pmb{B}_{k}^{(t)}\\pmb{B}_{k}^{(t)\\top}\\bar{Z}\\delta_{k}^{(t)}}\\\\ &{\\qquad\\qquad\\qquad\\geq\\displaystyle\\frac{\\zeta_{0}\\gamma^{2}}{2K^{2}}\\displaystyle\\sum_{k=1}^{K}\\left\\|\\bar{Z}\\delta_{k}^{(t)}\\right\\|_{2}^{2}=\\sigma\\left(\\ell(\\pmb{\\xi}^{(t)})-\\mathcal{L}^{\\star}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the equality comes from (45), and the last equality follows from (36). ", "page_idx": 17}, {"type": "text", "text": "Step 2: verify the smoothness of the loss function. We first give the following lemma that bounds the Lipschitzness of $b_{k}^{h}$ and $\\delta_{k}^{\\theta}$ , which will be used later on. For notation simplicity, we let $B,Q,\\alpha$ denote $B(\\theta),Q(\\theta),\\stackrel{..}{\\alpha}(\\theta)$ , respectively, and let $B^{\\prime},Q^{\\prime},\\alpha^{\\prime}$ denote $B(\\theta^{\\prime}),Q(\\bar{\\theta}^{\\prime}),\\bar{\\alpha}(\\theta^{\\prime})$ , respectively. ", "page_idx": 17}, {"type": "text", "text": "Lemma 8 (Lipschitzness of $b_{k}^{h}$ and $\\delta_{k\\,\\cdot}^{\\theta\\,\\cdot}$ ). For all $k\\in[K]$ and $h\\in[H]$ , and all transformer parameters $\\theta,\\theta^{\\prime},\\,i f\\operatorname*{max}\\bar{\\{}|\\alpha_{h,k}|,|\\alpha_{h,k}^{\\prime}|\\}\\leq\\alpha,$ , then we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|b_{k}^{h}(\\theta)-b_{k}^{h}(\\theta^{\\prime})\\right\\|_{2}\\le2\\left\\|\\bar{\\boldsymbol{Z}}\\right\\|_{2}\\left\\|{Q}_{h}-{Q}_{h}^{\\prime}\\right\\|_{F},}\\\\ &{\\qquad\\quad\\left\\|\\delta_{k}^{\\theta}-\\delta_{k}^{\\theta^{\\prime}}\\right\\|_{2}\\le2\\gamma\\sqrt{H}\\alpha\\sqrt{\\displaystyle\\sum_{h=1}^{H}\\left\\|{Q}_{h}-{Q}_{h}^{\\prime}\\right\\|_{F}^{2}}+\\gamma\\sqrt{H}\\left\\|\\alpha_{k}-\\alpha_{k}^{\\prime}\\right\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. (50) follows from a similar argument in (46). Regarding the Lipschitzness of $\\delta_{k}^{\\theta}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\left\\|\\delta_{k}^{\\theta}-\\delta_{k}^{\\theta^{\\prime}}\\right\\|_{2}=\\gamma\\left\\|\\displaystyle\\sum_{h=1}^{H}\\alpha_{h,k}\\big(\\pmb{s}_{k}^{h}(\\pmb{\\theta})-\\pmb{s}_{k}^{h}(\\pmb{\\theta}^{\\prime})\\big)+\\displaystyle\\sum_{h=1}^{H}(\\alpha_{h,k}-\\alpha_{h,k}^{\\prime})\\pmb{s}_{k}^{h}(\\pmb{\\theta}^{\\prime})\\right\\|_{2}}}\\\\ &{\\le\\gamma\\displaystyle\\sum_{h=1}^{H}|\\alpha_{h,k}|\\left\\|\\pmb{s}_{k}^{h}(\\pmb{\\theta})-\\pmb{s}_{k}^{h}(\\pmb{\\theta}^{\\prime})\\right\\|_{2}+\\gamma\\displaystyle\\sum_{h=1}^{H}|\\alpha_{h,k}-\\alpha_{h,k}^{\\prime}|\\left\\|\\pmb{s}_{k}^{h}(\\pmb{\\theta}^{\\prime})\\right\\|_{2}}\\\\ &{\\le2\\gamma\\sqrt{H}\\alpha\\sqrt{\\displaystyle\\sum_{h=1}^{H}\\|\\pmb{Q}_{h}-\\pmb{Q}_{h}^{\\prime}\\|_{F}^{2}}+\\gamma\\sqrt{H}\\left\\|\\pmb{\\alpha}_{k}-\\pmb{\\alpha}_{k}^{\\prime}\\right\\|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we use (46) again to bound the first term in the second line, and use the fact that  skh(\u03b8\u2032)  2 \u22641 and Cauchy-Schwarz inequality to bound the second term in the second line. ", "page_idx": 18}, {"type": "text", "text": "We also need the following lemma which bounds the norm of $B_{k}$ and $\\delta_{k}^{\\theta}$ . ", "page_idx": 18}, {"type": "text", "text": "Lemma 9 (Upper bounds of $b_{k}^{h}$ and $\\delta_{k\\,\\cdot}^{\\theta}$ ). For all $k\\in[K]$ and $h\\in[H],\\,i f\\operatorname*{max}\\{|\\alpha_{h,k}|,|\\alpha_{h,k}^{\\prime}|\\}\\leq\\alpha,$ then we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\pmb{b}_{k}^{h}\\right\\|_{2}\\leq\\left\\|\\bar{\\pmb{Z}}\\right\\|_{2},}\\\\ &{\\left\\|\\pmb{\\delta}_{k}^{\\theta}\\right\\|_{2}\\leq\\gamma H\\alpha+\\left\\|\\pmb{A}\\right\\|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where A is defined in (34). ", "page_idx": 18}, {"type": "text", "text": "Proof. (52) follows from ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\|\\pmb{{b}}_{k}^{h}\\right\\|_{2}\\leq\\left\\|\\bar{\\pmb{Z}}\\right\\|_{2}\\left\\|\\pmb{{s}}_{k}^{h}\\right\\|_{2}\\leq\\left\\|\\bar{\\pmb{Z}}\\right\\|_{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "(53) follows from ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\|{\\pmb\\delta}_{k}^{\\theta}\\right\\|_{2}\\leq\\gamma\\sum_{h=1}^{H}\\left|\\alpha_{h,k}\\right|\\left\\|{\\pmb s}_{k}^{h}\\right\\|_{2}+\\left\\|{\\pmb A}{\\pmb e}_{k}\\right\\|_{2}\\leq\\gamma H\\alpha+\\left\\|{\\pmb A}\\right\\|_{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "As a consequence of Lemma 8 and Lemma 9, For all $k\\in[K]$ , and all transformer parameters $\\theta,\\theta^{\\prime}$ , if $\\operatorname*{max}\\{|\\alpha_{h,k}|,|\\alpha_{h,k}^{\\prime}|\\}\\leq\\alpha$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\nabla_{\\alpha_{k}}\\ell(\\pmb{\\xi})-\\nabla_{\\alpha_{k}}\\ell(\\pmb{\\xi}^{\\prime})\\right\\|_{2}}\\\\ &{\\overset{(45)}{=}\\frac{\\gamma}{K}\\left\\|(\\pmb{B}_{k}-\\pmb{B}_{k}^{\\prime})^{\\top}\\bar{\\pmb{Z}}\\delta_{k}^{\\theta}+\\pmb{B}_{k}^{\\prime}^{\\top}\\bar{\\pmb{Z}}(\\delta_{k}^{\\theta}-\\delta_{k}^{\\theta^{\\prime}})\\right\\|_{2}}\\\\ &{\\le\\frac{\\gamma}{K}\\left\\|\\bar{\\pmb{Z}}\\right\\|_{2}\\left\\|\\pmb{B}_{k}-\\pmb{B}_{k}^{\\prime}\\right\\|_{F}\\left\\|\\delta_{k}^{\\theta}\\right\\|_{2}+\\frac{\\gamma}{K}\\left\\|\\bar{\\pmb{Z}}\\right\\|_{2}\\left\\|\\pmb{B}_{k}^{\\prime}\\right\\|_{F}\\left\\|\\delta_{k}^{\\theta}-\\delta_{k}^{\\theta^{\\prime}}\\right\\|_{2}}\\\\ &{\\le\\frac{\\gamma}{K}\\cdot2\\left\\|\\bar{\\pmb{Z}}\\right\\|_{2}^{2}\\left(2\\gamma H\\alpha+\\left\\|\\pmb{A}\\right\\|_{2}\\right)\\sqrt{\\displaystyle\\sum_{h=1}^{H}\\left\\|\\pmb{Q}_{h}-\\pmb{Q}_{h}^{\\prime}\\right\\|_{F}^{2}}+\\frac{\\gamma^{2}}{K}H\\left\\|\\bar{\\pmb{Z}}\\right\\|_{2}^{2}\\left\\|\\alpha_{k}-\\alpha_{k}^{\\prime}\\right\\|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "from which we obtain the smoothness of the $\\ell$ w.r.t. $_{\\alpha}$ as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\|\\nabla_{\\alpha}\\ell(\\xi)-\\nabla_{\\alpha}\\ell(\\xi^{\\prime})\\|_{F}^{2}}\\\\ &{\\displaystyle=\\sum_{k=1}^{K}\\|\\nabla_{\\alpha_{k}}\\ell(\\xi)-\\nabla_{\\alpha_{k}}\\ell(\\xi^{\\prime})\\|_{2}^{2}}\\\\ &{\\displaystyle\\le2K\\left(\\frac{\\gamma}{K}\\cdot2\\left\\|\\bar{Z}\\right\\|_{2}^{2}\\left(2\\gamma H\\alpha+\\|A\\|_{2}\\right)\\right)^{2}\\sum_{h=1}^{H}\\left\\|Q_{h}-Q_{h}^{\\prime}\\right\\|_{F}^{2}+2\\frac{\\gamma^{4}}{K^{2}}H^{2}\\left\\|\\bar{Z}\\right\\|_{2}^{4}\\left\\|\\alpha-\\alpha^{\\prime}\\right\\|_{F}^{2}}\\\\ &{\\displaystyle\\le2\\left(\\frac{1}{K}\\left(2\\gamma\\left\\|\\bar{Z}\\right\\|_{2}^{2}\\left(2\\gamma H\\alpha+\\|A\\|_{2}\\right)\\right)^{2}+\\frac{\\gamma^{4}}{K^{2}}H^{2}\\left\\|\\bar{Z}\\right\\|_{2}^{4}\\right)\\left\\|\\xi-\\xi^{\\prime}\\right\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the first inequality uses Young\u2019s inequality (c.f. Lemma 3). ", "page_idx": 18}, {"type": "text", "text": "To obtain the smoothness of the loss function w.r.t. $Q_{h}$ , we first note that by (82) we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\partial\\ell(\\pmb\\xi)}{\\partial\\pmb Q_{h}}=\\frac{\\gamma}{K}\\sum_{k=1}^{K}\\sum_{j=1}^{N}\\left(\\bar{Z}\\delta_{k}^{\\theta}\\right)^{\\top}\\boldsymbol{z}_{j}\\cdot\\alpha_{h,k}\\boldsymbol{s}_{j k}^{h}\\sum_{i=1}^{N}s_{i k}^{h}(\\boldsymbol{v}_{j}-\\boldsymbol{v}_{i})\\boldsymbol{v}_{k}^{\\top}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, if $\\operatorname*{max}\\{|\\alpha_{h,k}|,|\\alpha_{h,k}^{\\prime}|\\}\\leq\\alpha$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{d\\sigma(\\boldsymbol{\\xi})}{d\\eta_{1}}-\\frac{\\partial\\sigma(\\boldsymbol{\\xi})}{\\partial\\boldsymbol{\\xi}_{1}}\\bigg|_{r}\\leq\\frac{2\\sqrt{I_{\\mathrm{ons}}}}{K_{1}}\\sum_{\\stackrel{n}{=}}^{K}\\bigg\\{\\displaystyle\\sum_{s=1}^{K}\\Big|\\mathcal{Z}_{1}\\Big|_{2}\\Big\\lVert\\frac{\\boldsymbol{\\theta}_{1}}{K_{1}}-\\boldsymbol{\\xi}_{1}^{s}\\Big\\rVert_{2}^{2}\\cos_{s}^{2}\\theta_{1}\\Big\\rVert_{2}^{\\frac{1}{2}}e_{s}^{-s}\\sin_{t}^{2}(\\theta)\\sum_{s=1}^{K}\\theta_{1}}\\\\ &{\\quad+\\displaystyle\\sum_{s=1}^{K}\\Big|\\mathcal{Z}_{1}\\Big|_{2}\\Big|_{L}\\Big|_{2}\\Big|_{L}\\Big|_{2}\\Big|_{L^{\\infty}}\\Big|_{L^{\\infty}}\\alpha_{1}-\\alpha_{1}e_{s}^{s}\\Big|_{r}\\theta_{1}\\Big|_{2}\\sum_{s=1}^{K}e_{s}^{s}\\sin(\\theta)}\\\\ &{\\quad+\\displaystyle\\sum_{s=1}^{K}\\Big|\\mathcal{Z}_{1}\\Big|_{2}\\Big|_{L^{\\infty}}\\Big|_{L^{\\infty}}\\Big|_{2}\\Big|_{L^{\\infty}}\\Big|\\theta_{1}\\Big|_{2}-\\alpha_{1}^{s}\\Big|_{L^{\\infty}}(\\theta)\\sum_{s=1}^{K}e_{s}^{s}\\sin(\\theta)}\\\\ &{\\quad+\\displaystyle\\sum_{s=1}^{K}\\Big|\\mathcal{Z}_{1}\\Big|_{2}\\Big|_{L^{\\infty}}\\Big|_{L^{\\infty}}\\Big|_{2}\\alpha_{1}\\delta(\\theta)\\sum_{s=1}^{K}\\Big|_{L^{\\infty}}\\Big|_{L^{\\infty}}(\\theta)-\\lambda_{1}\\Big|_{2}^{2}}\\\\ &{\\leq\\displaystyle\\frac{2\\sqrt{I_{\\mathrm{ons}}}\\Big|\\mathcal{Z}_{1}\\Big|_{L^{\\infty}}}{K_{1}}\\sum_{\\stackrel{n}{=}}^{K}\\Big\\{\\left|\\mathcal{Z}_{1}^{s}-\\mathcal{Z}_{1}^{s}\\right|_{L^{\\infty}}+\\left|\\mathcal{Z}_{1}^ \n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the third inequality uses Cauchy-Schwarz inequality. Combining the above inequality (57) with Lemma 8 and Lemma 9, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\left\\vert\\frac{\\partial\\ell(\\pmb{\\xi})}{\\partial Q_{h}}-\\frac{\\partial\\ell(\\pmb{\\xi}^{\\prime})}{\\partial Q_{h}}\\right\\vert\\right\\vert_{F}\\le\\frac{2\\gamma\\bar{f}_{\\operatorname*{max}}\\,\\left\\|\\bar{\\pmb{Z}}\\right\\|_{2}}{K}\\Bigg\\{\\alpha\\gamma\\sqrt{H}\\left(2K\\alpha\\sqrt{\\displaystyle\\sum_{h=1}^{H}\\left\\|\\pmb{Q}_{h}-\\pmb{Q}_{h}^{\\prime}\\right\\|_{F}^{2}}+\\sqrt{K}\\left\\|\\alpha-\\pmb{\\alpha}^{\\prime}\\right\\|_{F}\\right)}}\\\\ &{}&{+\\left(\\gamma H\\alpha+\\left\\|\\pmb{A}\\right\\|_{2}\\right)\\sqrt{K}\\left\\|\\pmb{\\alpha}_{h,:}-\\pmb{\\alpha}_{h,:}^{\\prime}\\right\\|_{2}}\\\\ &{}&{+\\left(\\gamma H\\alpha+\\left\\|\\pmb{A}\\right\\|_{2}\\right)\\cdot2\\alpha\\sqrt{N}\\cdot2K\\left\\|\\pmb{Q}_{h}^{\\prime}-\\pmb{Q}_{h}\\right\\|_{F}\\Bigg\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the last line uses (46) to bound $\\left\\|\\pmb{s}_{k}^{h}(\\pmb{\\theta})-\\pmb{s}_{k}^{h}(\\pmb{\\theta}^{\\prime})\\right\\|_{2}$ . The above inequality (58) further gives ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{h=1}^{H}\\|\\nabla_{Q_{h}}\\ell(\\xi)-\\nabla_{Q_{h}}\\ell(\\xi^{\\prime})\\|_{F}^{2}}\\\\ &{\\le8\\cdot\\frac{\\gamma\\bar{f}_{\\operatorname*{max}}\\left\\|\\bar{Z}\\right\\|_{2}}{K}\\bigg\\{\\left(2K\\alpha\\right)^{2}\\left[\\left(\\alpha\\gamma H\\right)^{2}+4N\\left(\\alpha\\gamma H+\\left\\|A\\right\\|_{2}\\right)^{2}\\right]\\displaystyle\\sum_{h=1}^{H}\\|Q_{h}-Q_{h}^{\\prime}\\|_{F}^{2}}\\\\ &{\\qquad+K\\left[\\left(\\alpha\\gamma H\\right)^{2}+\\left(\\alpha\\gamma H+\\left\\|A\\right\\|_{2}\\right)^{2}\\right]\\left\\|\\alpha-\\alpha^{\\prime}\\right\\|_{F}^{2}\\bigg\\}}\\\\ &{\\le8\\gamma\\bar{f}_{\\operatorname*{max}}\\left\\|\\bar{Z}\\right\\|_{2}\\cdot\\operatorname*{max}\\left\\{1,\\left(2\\sqrt{K}\\alpha\\right)^{2}\\right\\}\\left[\\left(\\alpha\\gamma H\\right)^{2}+4N\\left(\\alpha\\gamma H+\\left\\|A\\right\\|_{2}\\right)^{2}\\right]\\left\\|\\xi^{\\prime}-\\xi\\right\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the first inequality makes use of Young\u2019s inequality (c.f. Lemma 3). ", "page_idx": 19}, {"type": "text", "text": "Combining the above two relations (55) and (59), we obtain the smoothness of $\\ell$ w.r.t. $\\xi$ as follows: ", "page_idx": 19}, {"type": "text", "text": "Lemma 10 (Smoothness of the loss function). Let $\\gamma:=\\sqrt{\\eta_{w}/\\eta_{Q}}$ . For all transformer parameters $\\xi,\\xi^{\\prime},\\,i f\\operatorname*{max}\\{|\\alpha_{h,k}|,|\\alpha_{h,k}^{\\prime}|\\}\\leq\\alpha$ , then we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla_{\\pmb{\\xi}}\\ell(\\pmb{\\xi})-\\nabla_{\\pmb{\\xi}}\\ell(\\pmb{\\xi}^{\\prime})\\|_{2}\\leq L\\,\\|\\pmb{\\xi}-\\pmb{\\xi}^{\\prime}\\|_{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal L}^{2}=2\\left(\\frac{1}{K}\\left(2\\gamma\\left\\|\\bar{\\cal Z}\\right\\|_{2}^{2}\\left(2\\gamma H\\alpha+\\left\\|{\\cal A}\\right\\|_{2}\\right)\\right)^{2}+\\frac{\\gamma^{4}}{K^{2}}H^{2}\\left\\|\\bar{\\cal Z}\\right\\|_{2}^{4}\\right)}\\ ~}\\\\ {{\\displaystyle~~~~~+8\\gamma\\bar{f}_{\\mathrm{max}}\\left\\|\\bar{\\cal Z}\\right\\|_{2}}\\cdot\\operatorname*{max}\\left\\{1,(2\\sqrt{K}\\alpha)^{2}\\right\\}\\left[(\\alpha\\gamma H)^{2}+4N\\left(\\alpha\\gamma H+\\left\\|{\\cal A}\\right\\|_{2})^{2}\\right].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Step 3: verify (43a). (45) implies ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\partial\\ell(\\pmb{\\xi})}{\\partial\\alpha_{h,k}}=\\frac{\\gamma}{K}(\\pmb{b}_{k}^{h})^{\\top}\\bar{\\pmb{Z}}\\delta_{k}^{\\theta},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which, combining with (52), gives ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\forall k\\in[K],h\\in[H]:\\quad\\left(\\frac{\\partial\\ell(\\pmb{\\xi})}{\\partial\\alpha_{h,k}}\\right)^{2}\\leq\\frac{\\gamma^{2}}{K^{2}}\\left\\|\\bar{\\pmb{Z}}\\right\\|_{2}^{2}\\left\\|\\bar{\\pmb{Z}}\\delta_{k}^{\\theta}\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Combining this with (36) we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\|\\frac{\\ell(\\pmb{\\xi})}{\\partial\\pmb{\\alpha}_{h}}\\right\\|_{2}^{2}\\leq\\left\\|\\bar{\\pmb{Z}}\\right\\|_{2}^{2}\\frac{2\\gamma^{2}}{K}\\left(\\ell(\\pmb{\\xi})-\\mathcal{L}^{\\star}\\right),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which indicates ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\|\\frac{\\partial\\ell(\\pmb{\\xi})}{\\partial\\pmb{\\alpha}_{h}}\\right\\|_{2}\\leq\\|\\bar{\\pmb{Z}}\\|_{2}\\,\\gamma\\sqrt{\\frac{2}{K}\\left(\\ell(\\pmb{\\xi})-\\mathcal{L}^{\\star}\\right)}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\left\\|\\alpha_{h}^{(t)}\\right\\|_{2}=\\left\\|\\alpha_{h}^{(0)}-\\eta_{Q}\\sum_{i=0}^{t-1}\\frac{\\partial\\ell(\\xi^{(i)})}{\\partial\\alpha_{h}}\\right\\|_{2}}\\qquad}&{}\\\\ &{\\leq\\left\\|\\alpha_{h}^{(0)}\\right\\|_{2}+\\eta_{Q}\\sum_{i=0}^{t-1}\\left\\|\\frac{\\partial\\ell(\\xi^{(i)})}{\\partial\\alpha_{h}}\\right\\|_{2}}\\\\ &{\\leq\\left\\|\\alpha_{h}^{(0)}\\right\\|_{2}+\\eta_{Q}\\left\\|\\bar{Z}\\right\\|_{2}\\sqrt{\\frac{2\\gamma^{2}}{K}}\\frac{t-1}{\\sum_{i=0}^{t-1}}\\sqrt{t(\\xi^{(i)})-\\mathcal{L}^{\\star}}}\\\\ &{\\leq\\left\\|\\alpha_{h}^{(0)}\\right\\|_{2}+\\eta_{Q}\\left\\|\\bar{Z}\\right\\|_{2}\\sqrt{\\frac{2\\gamma^{2}}{K}(Z(\\theta^{(0)})-\\mathcal{L}^{\\star})}\\underset{i=0}{\\overset{t-1}{\\sum}}\\left(\\sqrt{1-\\frac{\\eta_{Q}\\sigma}{2}}\\right)^{\\prime}}\\\\ &{\\leq\\left\\|\\alpha_{h}^{(0)}\\right\\|_{2}+\\eta_{Q}\\left\\|\\bar{Z}\\right\\|_{2}\\sqrt{\\frac{2\\gamma^{2}}{K}(Z(\\theta^{(0)})-\\mathcal{L}^{\\star})}\\cdot\\frac{4}{\\eta_{Q}\\sigma},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the second inequality follows from (62) and the third inequality follows from the induction hypothesis (43c). (43a) follows from plugging $\\sigma$ defined in (44) into the above inequality and using the initializtion condition that $\\begin{array}{r}{{\\pmb{\\alpha}}^{(0)}=\\frac{1}{\\gamma}\\pmb{\\dot{w^{(0)}}}=\\mathbf{0}}\\end{array}$ . ", "page_idx": 20}, {"type": "text", "text": "Step 4: verify the linear convergence rate (43c). Combining (43a), (60) and Lemma 4.3 in Nguyen and Mondelli [2020], we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\ell(\\pmb{\\xi}^{(t)})-\\mathcal{L}^{\\star}\\le\\ell(\\pmb{\\xi}^{(t-1)})-\\mathcal{L}^{\\star}+\\langle\\nabla_{\\pmb{\\xi}}\\ell(\\pmb{\\xi}^{(t-1)}),\\pmb{\\xi}^{(t)}-\\pmb{\\xi}^{(t-1)}\\rangle+\\frac{L}{2}\\left\\|\\pmb{\\xi}^{(t)}-\\pmb{\\xi}^{(t-1)}\\right\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which indicates when $\\eta_{Q}\\leq1/L$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\ell(\\pmb{\\xi}^{(t)})-\\mathcal{L}^{\\star}\\le\\ell(\\pmb{\\xi}^{(t-1)})-\\mathcal{L}^{\\star}-\\frac{\\eta_{Q}}{2}\\left\\|\\nabla_{\\pmb{\\xi}}\\ell(\\pmb{\\xi}^{(t-1)})\\right\\|_{F}^{2}\\overset{(49)}{\\le}\\left(1-\\frac{\\eta_{Q}\\sigma}{2}\\right)\\left(\\ell(\\pmb{\\xi}^{(t-1)})-\\mathcal{L}^{\\star}\\right),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which, combined with the fact that $\\mathcal{L}(\\pmb{\\theta}^{(s)})=\\ell(\\pmb{\\xi}^{(s)})$ for all $s$ (see Lemma 6), verifies (43c). ", "page_idx": 20}, {"type": "text", "text": "Note that (36) implies that $\\mathcal{L}^{\\star}\\le\\mathcal{L}(\\pmb{\\theta})$ holds for all $\\pmb{\\theta}$ . And from (43c) we know that $\\mathcal{L}(\\pmb{\\theta}^{(t)})\\rightarrow\\mathcal{L}^{\\star}$ as $t\\to\\infty$ . Therefore, it follows that ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\mathcal{L}}^{\\star}=\\operatorname*{inf}_{\\theta}{\\mathcal{L}}(\\theta).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Consequently, (43c) is equivalent to (18). ", "page_idx": 20}, {"type": "text", "text": "D Proof of Theorem 2 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "By (43c) we know that $\\mathcal{L}(\\pmb{\\theta}^{(t)})\\rightarrow\\mathcal{L}^{\\star}$ as $t\\to\\infty$ . Thus from (36) we know that (37) and (38) hold. By Sherman-Morrison-Woodbury formula, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left(m\\tau{\\cal I}_{N}+{\\cal Z}^{\\top}{\\cal Z}\\right)^{-1}=\\frac{1}{m\\tau}{\\cal I}_{N}-\\frac{1}{m\\tau}{\\cal Z}^{\\top}\\left(m\\tau{\\cal I}_{m}+{\\cal Z}{\\cal Z}^{\\top}\\right)^{-1}{\\cal Z}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A\\stackrel{\\lefteqn{(3\\mathcal{A})}}{\\stackrel{(\\because)}{=}}\\left(Z^{\\top}Z+m\\tau I_{N}\\right)^{-1}\\left(Z^{\\top}\\widehat{Z}+\\left(m\\tau I_{N},0\\right)\\right)}\\\\ &{\\stackrel{\\mathrm{(c)}}{=}\\frac{1}{m\\tau}\\left(I_{N}-Z^{\\top}\\left(m\\tau I_{m}+Z Z^{\\top}\\right)^{-1}Z\\right)\\left(Z^{\\top}\\widehat{Z}+\\left(m\\tau I_{N},0\\right)\\right)}\\\\ &{=\\frac{1}{m\\tau}\\bigg[Z^{\\top}\\widetilde{Z}+\\left(m\\tau I_{N},0\\right)-Z^{\\top}\\left(m\\tau I_{m}+Z Z^{\\top}\\right)^{-1}\\left(m\\tau I_{m}+Z Z^{\\top}\\right)\\widetilde{Z}}\\\\ &{\\quad+\\left.m\\tau Z^{\\top}\\left(m\\tau I_{m}+Z Z^{\\top}\\right)^{-1}\\widetilde{Z}-m\\tau Z^{\\top}\\left(m\\tau I_{m}+Z Z^{\\top}\\right)^{-1}(Z,0)\\right]}\\\\ &{=\\left(I_{N},0\\right)+Z^{\\top}\\left(m\\tau I_{m}+Z Z^{\\top}\\right)^{-1}\\left(0,Z^{\\top}\\right)}\\\\ &{=\\left(I_{N},Z^{\\top}\\left(m\\tau I_{m}+Z Z^{\\top}\\right)^{-1}Z^{Q}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $Z^{Q}$ is defined in (19). ", "page_idx": 21}, {"type": "text", "text": "On the other hand, it\u2019s straightforward to verify that $\\widehat{\\lambda}$ defined in (20) admits the following closed form: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\widehat{\\lambda}=\\left(m\\tau\\pmb{I}_{m}+\\pmb{Z}\\pmb{Z}^{\\top}\\right)^{-1}\\pmb{Z}\\pmb{y}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Combining the above two equations, we obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\pmb{A}^{\\top}\\pmb{y}=\\left(\\begin{array}{c}{\\pmb{y}}\\\\ {\\left(\\pmb{Z}^{Q}\\right)^{\\top}\\left(m\\tau\\pmb{I_{m}}+\\pmb{Z}\\pmb{Z}^{\\top}\\right)^{-1}\\pmb{Z_{y}}\\right)=\\left(\\begin{array}{c}{\\pmb{y}}\\\\ {\\left(\\pmb{Z}^{Q}\\right)^{\\top}\\hat{\\pmb{\\lambda}}\\right)=\\hat{\\pmb{y}}^{\\star},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the last equality follows from (22). ", "page_idx": 21}, {"type": "text", "text": "Now we give the iteration complexity for the mean-squared error between the prediction $\\hat{\\pmb y}$ and the limit point $\\widehat{\\pmb{y}}^{\\star}$ to be less than $\\varepsilon$ . Given any prompt $P=P_{\\lambda}$ , where $\\lambda$ satisfies Assumption 4 , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\ny_{i}=\\pmb{\\lambda}^{\\top}(\\pmb{z}_{i}+\\pmb{\\epsilon}_{i})\\sim\\mathcal{N}(\\pmb{\\lambda}^{\\top}\\pmb{z}_{i},\\|\\pmb{\\lambda}\\|_{2}^{2}\\,\\tau).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Letting xi = y\u2225i\u2212\u2225\u03bb\u22a4\u221azi , \u03c4 we have xi \u223cN(0, 1). Define ", "page_idx": 21}, {"type": "equation", "text": "$$\nZ=\\sum_{i=1}^{N}\\left\\|\\pmb{\\lambda}\\right\\|_{2}^{2}\\tau(x_{i}^{2}-1)=\\left\\|\\pmb{y}-\\pmb{Z}^{\\top}\\pmb{\\lambda}\\right\\|_{2}^{2}-N\\tau\\left\\|\\pmb{\\lambda}\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By Laurent and Massart [2000, Lemma 1], we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\forall s>0:\\quad\\mathbb{P}\\left(Z\\geq2\\sqrt{N}\\left\\|\\lambda\\right\\|_{2}^{2}\\tau\\sqrt{s}+2\\left\\|\\lambda\\right\\|_{2}^{2}\\tau s\\right)\\leq\\exp\\left(-s\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By letting $s=\\log(1/\\delta)$ and using the definition of $Z$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\left\\|y-Z^{\\top}\\lambda\\right\\|_{2}^{2}\\geq N\\tau\\left\\|\\lambda\\right\\|_{2}^{2}+2\\sqrt{N\\log(1/\\delta)}\\left\\|\\lambda\\right\\|_{2}^{2}\\tau+2\\left\\|\\lambda\\right\\|_{2}^{2}\\tau\\log(1/\\delta)\\right)\\leq\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus with probability at least $1-\\delta$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|y\\|_{2}\\leq\\left\\|Z^{\\top}\\lambda\\right\\|_{2}+\\left\\|y-Z^{\\top}\\lambda\\right\\|_{2}}\\\\ &{\\qquad\\leq\\left\\|Z^{\\top}\\lambda\\right\\|_{2}+\\left\\|\\lambda\\right\\|_{2}\\sqrt{\\tau}\\left(N+2\\sqrt{N\\log(1/\\delta)}+2\\log(1/\\delta)\\right)^{1/2}}\\\\ &{\\qquad\\leq B\\left(\\|Z\\|_{2}+\\sqrt{\\tau}\\left(N+2\\sqrt{N\\log(1/\\delta)}+2\\log(1/\\delta)\\right)^{1/2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where we use (68) in the second inequality, and the third inequality follows from Assumption 4. On the other hand, by (36) we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\pmb{\\theta}^{(t)})=\\frac{1}{2K}\\left\\|\\bar{Z}(\\widehat{\\pmb{A}}-\\pmb{A})\\right\\|_{2}^{2}+\\mathcal{L}^{\\star}\\geq\\frac{m\\tau}{2K}\\left\\|\\widehat{\\pmb{A}}-\\pmb{A}\\right\\|_{2}^{2}+\\mathcal{L}^{\\star},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which gives ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left\\|{\\hat{A}}-A\\right\\|_{2}\\leq{\\sqrt{{\\frac{2K}{m\\tau}}\\left({\\mathcal{L}}(\\theta^{(T)})-{\\mathcal{L}}^{\\star}\\right)}}\\leq{\\sqrt{{\\frac{2K}{m\\tau}}\\left({\\mathcal{L}}(\\theta^{(0)})-{\\mathcal{L}}^{\\star}\\right)}}\\left(1-{\\frac{\\gamma^{2}\\eta_{Q}\\zeta_{0}}{2K}}\\right)^{T/2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus we know that w.p. at least $1-\\delta$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{1}{2K}\\left\\|\\widehat{\\pmb{y}}-\\widehat{\\pmb{y}}^{\\star}\\right\\|_{2}^{2}=\\frac{1}{2K}\\left\\|\\left(\\widehat{\\pmb{A}}-\\pmb{A}\\right)^{\\top}\\pmb{y}\\right\\|_{2}^{2}\\leq\\frac{1}{2K}\\left\\|\\widehat{\\pmb{A}}-\\pmb{A}\\right\\|_{2}^{2}\\left\\|\\pmb{y}\\right\\|_{2}^{2}\\leq\\varepsilon,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the last relation follows from (69), (70) and (21). ", "page_idx": 22}, {"type": "text", "text": "E Proof of Key Lemmas ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "E.1 Proof of Proposition 1 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "For notation simplicity we drop the superscript (0) in the subsequent proof. ", "page_idx": 22}, {"type": "equation", "text": "$$\nD_{k}=V^{\\top}Q=V^{\\top}(q_{1},\\cdots,q_{H}),\\quad\\mathrm{where~}Q(i,j)\\overset{i.i.d.}\\sim\\mathcal{N}(0,\\beta^{2}\\left\\|v_{k}\\right\\|_{2}^{2}),\\quad\\forall i\\in[d],j\\in[H].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This suggests the column vectors of $D_{k}$ are i.i.d. and the density of each column vector is positive at any point $\\pmb{x}\\in\\mathcal{R}(\\pmb{V})$ , where $\\mathcal{R}(V)\\subset\\mathbb{R}^{N}$ is the row space of $V$ . ", "page_idx": 22}, {"type": "text", "text": "Since $\\bar{Z}$ has full rank, to prove $B_{k}$ has full rank a.s., we only need to argue that $C_{k}(:,1:N)$ has full rank w.p. 1. Below we prove this by contradiction (recall that by definition $C_{k}=\\mathsf{s o f t m a x}(D_{k})$ , and we assume $H\\geq N$ ). ", "page_idx": 22}, {"type": "text", "text": "Suppose w.p. larger than 0, there exists one of $C_{k}(:,1:N)$ \u2019s column vector that could be linearly represented by its other $N-1$ column vectors. Without loss of generality, we assume this colomn vector is $C_{k}(:,1)=\\mathsf{s o f t m a x}(D_{k}(:,1))$ . Let $\\pmb{x}=\\pmb{x}(\\pmb{q}_{1}):=\\exp(\\bar{\\pmb{D}}_{k}(:,1)\\bar{\\bf\\alpha})=\\exp(V^{\\top}\\pmb{q}_{1})$ . Then $\\textbf{\\em x}$ could be linearly represented by $\\exp(\\dot{D}_{k}(:,i))$ , $i=2,\\cdots\\,,N$ . ", "page_idx": 22}, {"type": "text", "text": "Let $\\tilde{\\cal A}:=\\exp(D_{k}(:,2:N))$ , then w.p. larger than 0, $\\pmb{x}\\in\\mathcal{C}(\\tilde{\\pmb{A}})$ , where $\\boldsymbol{\\mathcal{C}}(\\tilde{\\boldsymbol{A}})$ is the column vector space of $\\tilde{A}$ . i.e., we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\int_{\\mathbb{R}^{N\\times(m-1)}}\\mathbb{P}(\\pmb{x}\\in\\mathcal{C}(\\tilde{\\pmb{A}})|\\tilde{\\pmb{A}})d\\mu(\\tilde{\\pmb{A}})>0,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which further indicates that there exists $\\tilde{\\pmb{A}}\\in\\mathbb{R}^{N\\times(N-1)}$ such that $\\mathbb{P}(\\pmb{x}\\in\\mathcal{C}(\\tilde{\\pmb{A}}))>0$ . Since the dimension of $\\boldsymbol{\\mathcal{C}}(\\tilde{\\boldsymbol{A}})$ is at most $N-1$ , there exists $\\pmb{y}\\in\\mathbb{R}^{N}$ , ${\\bf{y}}\\ne{\\bf{0}}$ such that $y\\bot{\\mathcal{C}}({\\tilde{A}})$ . Therefore, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\pmb{y}^{\\top}\\pmb{x}=0)>0.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By Assumption 3, without loss of generality, we assume that $\\mathbf{\\widetilde{\\boldsymbol{u}}}_{1}=\\left(v_{11},v_{12},\\cdots\\,,v_{1N}\\right)^{\\top}$ has different entries. For any vector $\\pmb{w}=(w_{1},\\cdot\\cdot\\cdot\\,,w_{d})^{\\top}\\in\\mathbb{R}^{d}$ , we let $\\pmb{\\tilde{w}}=(w_{2},\\cdot\\cdot\\cdot\\mathrm{\\boldmath~,~}w_{d})^{\\top}\\in\\mathbb{R}^{d-1}$ denote the vector formed by deleting the first entry of $\\mathbf{\\nabla}w$ . Let $\\pmb q_{1}=(q,\\tilde{q}_{1}^{\\top})^{\\top}$ . For any fixed $\\tilde{q}_{1}\\in\\mathbb{R}^{d-1}$ , the function $g(\\cdot|\\tilde{\\pmb{q}}_{1}):\\mathbb{R}\\to\\mathbb{R}$ defined by ", "page_idx": 22}, {"type": "equation", "text": "$$\ng(q|\\tilde{q}_{1}):=\\sum_{i=1}^{N}y_{i}e^{q v_{1i}+\\tilde{q}_{1}^{\\top}\\tilde{v}_{i}}=\\sum_{i=1}^{N}y_{i}e^{\\tilde{q}_{1}^{\\top}\\tilde{v}_{i}}e^{q v_{1i}}=\\left\\langle y,\\exp(V^{\\top}q_{1})\\right\\rangle=\\left\\langle y,x(q_{1})\\right\\rangle\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "has finite zero points and thus $\\left\\{q\\in\\mathbb{R}|g(q|\\tilde{\\pmb{q}}_{1})=0\\right\\}$ is a zero-measure set. Therefore, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\langle\\pmb{y},\\pmb{x}\\rangle=0)=\\int_{\\mathbb{R}^{d-1}}\\mathbb{P}(g(q|\\tilde{\\pmb{q}}_{1})=0|\\tilde{\\pmb{q}}_{1})d\\mu(\\tilde{\\pmb{q}}_{1})=0,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which contradicts (72). Therefore, $C_{k}(:,1:N)$ has full rank with probability 1. ", "page_idx": 22}, {"type": "text", "text": "E.2 Proof of Lemma 4 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Lemma 4 can be verified by the following direct computation (recall that the noise in each label satisfies $\\epsilon_{i}\\overset{i.i.d}{\\sim}\\mathcal{N}(0,\\tau I_{m}),\\forall i\\in[N])$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\boldsymbol{\\epsilon}}\\left[\\displaystyle\\frac{1}{2N}\\sum_{i=1}^{N}(y_{i}-\\lambda^{\\top}\\left(f(\\boldsymbol{v}_{i})+\\boldsymbol{\\epsilon}_{i}\\right))^{2}\\right]}\\\\ &{=\\mathbb{E}_{\\boldsymbol{\\epsilon}}\\left[\\displaystyle\\frac{1}{2N}\\sum_{i=1}^{N}\\left((y_{i}-\\lambda^{\\top}f(\\boldsymbol{v}_{i}))^{2}-2\\lambda^{\\top}\\boldsymbol{\\epsilon}_{i}(y_{i}-\\lambda^{\\top}f(\\boldsymbol{v}_{i}))+\\lambda^{\\top}\\boldsymbol{\\epsilon}_{i}\\boldsymbol{\\epsilon}_{i}^{\\top}\\lambda\\right)\\right]}\\\\ &{=\\displaystyle\\frac{1}{2N}\\sum_{i=1}^{N}\\left((y_{i}-\\lambda^{\\top}f(\\boldsymbol{v}_{i}))^{2}+\\tau\\left\\|\\lambda\\right\\|_{2}^{2}\\right)}\\\\ &{=\\displaystyle\\frac{1}{2N}\\sum_{i=1}^{N}(y_{i}-\\lambda^{\\top}f(\\boldsymbol{v}_{i}))^{2}+\\frac{\\tau}{2}\\left\\|\\lambda\\right\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "E.3 Proof of Lemma 5 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We let $\\pmb{\\epsilon}^{P}\\ :=\\ (\\pmb{\\epsilon}_{1},\\cdot\\cdot\\cdot\\ ,\\pmb{\\epsilon}_{N})\\ \\in\\ \\mathbb{R}^{m\\times N}$ , $\\epsilon\\;:=\\;\\left(\\pmb{\\epsilon}_{1},\\cdot\\cdot\\cdot\\;,\\pmb{\\epsilon}_{K}\\right)\\;\\;\\in\\;\\;\\mathbb{R}^{m\\times K}$ . Recall that $\\textit{\\textbf{y}}=$ $(y_{1},\\cdot\\cdot\\cdot\\,,y_{N})^{\\top}\\in\\mathbb{R}^{N}$ . Then we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{y}=(\\pmb{Z}+\\pmb{\\epsilon}^{P})^{\\top}\\pmb{\\lambda},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle C(\\theta)=\\frac{1}{K}\\sum_{k=1}^{K}{\\mathcal{L}}_{k}(\\theta)=\\frac{1}{2}\\mathbb{E}_{\\lambda\\sim}\\left[\\frac{1}{K}\\sum_{i=1}^{K}(\\hat{y}_{k}-y_{k i})^{2}\\right]}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {=\\frac{1}{2K}\\sum_{k=1}^{K}\\mathbb{E}_{\\lambda\\sim}\\left[\\left\\|\\hat{\\cal Z}^{(k)}-\\hat{\\tau}^{*}(z_{k}+\\epsilon_{k})\\right\\|_{2}^{2}\\right.}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\end{=\\frac{1}{2K}\\sum_{k=1}^{K}\\mathbb{E}_{\\lambda\\sim}\\left[\\left(\\boldsymbol{\\cal Z}^{\\star}\\boldsymbol{e}^{\\prime}\\right)\\hat{\\boldsymbol{a}}_{k}-\\left(z_{k}+\\epsilon_{k}\\right)\\right]^{\\top}\\left[\\left(\\boldsymbol{\\cal Z}+\\boldsymbol{e}^{\\prime}\\right)\\hat{\\boldsymbol{a}}_{k}-\\left(z_{k}+\\epsilon_{k}\\right)\\right]}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle=\\frac{1}{2K}\\sum_{k=1}^{K}\\mathbb{E}_{\\kappa}\\left[\\left\\|\\boldsymbol{\\cal Z}^{\\star}\\boldsymbol{e}^{\\prime}\\right\\rangle\\hat{\\boldsymbol{a}}_{k}-\\left(z_{k}+\\epsilon_{k}\\right)\\right\\|^{\\top}\\left[\\left(\\boldsymbol{\\cal Z}+\\boldsymbol{e}^{\\prime}\\right)\\hat{\\boldsymbol{a}}_{k}-\\left(z_{k}+\\epsilon_{k}\\right)\\right]}\\\\ {\\displaystyle}\\\\ {\\displaystyle=\\frac{1}{2K}\\sum_{k=1}^{K}\\mathbb{E}_{\\kappa}\\left[\\left\\|\\boldsymbol{\\cal Z}\\hat{\\boldsymbol{a}}_{k}-\\boldsymbol{z}_{k}\\right\\|_{2}^{2}+2(\\boldsymbol{\\cal Z}\\hat{\\boldsymbol{a}}_{k}-\\boldsymbol{z}_{k})^{\\top}(\\boldsymbol{\\epsilon}^{\\prime}\\hat{\\boldsymbol{a}}_{k}-\\boldsymbol{\\epsilon}_{k})+\\left\\|\\boldsymbol{\\epsilon}^{\\prime}\\hat{\\boldsymbol{a}}_{k}-\\boldsymbol{z}_{k}\\right\\|_{2}^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\widehat{\\pmb{a}}_{k}$ denote the $k$ -th column vector of matrix $\\widehat{A}(\\pmb\\theta)$ defined in (35), and the fifth line uses Assumption 1. ", "page_idx": 23}, {"type": "text", "text": "Note that for all $k\\in[K]$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\epsilon}(Z\\widehat{\\pmb{a}}_{k}-\\pmb{z}_{k})^{\\top}(\\epsilon^{P}\\widehat{\\pmb{a}}_{k}-\\epsilon_{k})=0,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\epsilon}\\left\\|\\epsilon^{P}\\widehat{\\boldsymbol{a}}_{k}-\\epsilon_{k}\\right\\|_{2}^{2}=m\\tau\\left(\\left\\|\\widehat{\\boldsymbol{a}}_{k}\\right\\|_{2}^{2}+1\\right)-2m\\tau\\widehat{\\boldsymbol{a}}_{k k}\\mathbb{1}\\left\\{k\\in\\left[N\\right]\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\mathbb{1}\\left\\{k\\in[N]\\right\\}$ is the indicator function that equals 1 if $k\\in[N]$ and 0 otherwise, and we have made use of the assumption that $\\epsilon_{k}\\overset{i.i.d.}{\\sim}\\mathcal{N}(0,\\tau^{2}I_{m})$ . ", "page_idx": 23}, {"type": "text", "text": "Combining the above two equations with (75), we know that for $k\\in[N]$ , it holds that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{L}_{k}(\\pmb{\\theta})=\\frac{1}{2}\\left(\\|Z\\widehat{\\pmb{a}}_{k}-z_{k}\\|_{2}^{2}+m\\tau\\,\\|\\widehat{\\pmb{a}}_{k}-\\pmb{e}_{k}\\|_{2}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Reorganizing the terms in the RHS of the above equation, we obtain that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{L}_{k}(\\pmb{\\theta})=\\frac{1}{2}\\left\\|\\left(\\pmb{Z}^{\\top}\\pmb{Z}+m\\tau\\pmb{I}\\right)^{1/2}\\left(\\widehat{\\pmb{a}}_{k}-\\left(\\pmb{Z}^{\\top}\\pmb{Z}+m\\tau\\pmb{I}\\right)^{-1}\\left(\\pmb{Z}^{\\top}\\pmb{z}_{k}+m\\tau e_{k}\\right)\\right)\\right\\|_{2}^{2}+\\frac{1}{2}c_{k},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $c_{k}=-\\left(\\boldsymbol{Z}^{\\top}\\boldsymbol{z}_{k}+m\\tau\\boldsymbol{e}_{k}\\right)^{\\top}\\left(\\boldsymbol{Z}^{\\top}\\boldsymbol{Z}+m\\tau\\boldsymbol{I}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\top}\\boldsymbol{z}_{k}+m\\tau\\boldsymbol{e}_{k}\\right)+\\left\\|\\boldsymbol{z}_{k}\\right\\|_{2}^{2}+m\\tau.$ ", "page_idx": 24}, {"type": "text", "text": "By a similar argument, we can show that for $k\\in[K]\\backslash[N]$ , it holds thet ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{L}_{k}(\\pmb{\\theta})=\\frac{1}{2}\\left\\|\\left(\\pmb{Z}^{\\top}\\pmb{Z}+m\\tau\\pmb{I}\\right)^{1/2}\\left(\\widehat{\\pmb{a}}_{k}-\\left(\\pmb{Z}^{\\top}\\pmb{Z}+m\\tau\\pmb{I}\\right)^{-1}\\pmb{Z}^{\\top}\\pmb{z}_{k}\\right)\\right\\|_{2}^{2}+\\frac{1}{2}c_{k}^{\\prime},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$c_{k}^{\\prime}=-\\left(\\boldsymbol{Z}^{\\top}\\boldsymbol{z}_{k}\\right)^{\\top}\\left(\\boldsymbol{Z}^{\\top}\\boldsymbol{Z}+m\\tau\\boldsymbol{I}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\top}\\boldsymbol{z}_{k}\\right)+\\|\\boldsymbol{z}_{k}\\|_{2}^{2}.$ ", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "(78), (79) together with (33) and the definition of $\\mathcal{L}^{\\star}$ give (36). ", "page_idx": 24}, {"type": "text", "text": "E.4 Proof of Lemma 6 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "First, it holds that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boldsymbol{Q}_{h}^{(t)}=\\boldsymbol{Q}_{h}^{(t-1)}-\\eta_{Q}\\nabla_{\\boldsymbol{Q}_{h}}\\ell(\\boldsymbol{\\xi}^{(t-1)})=\\boldsymbol{Q}_{h}^{(t-1)}-\\eta_{Q}\\nabla_{\\boldsymbol{Q}_{h}}\\ell(\\boldsymbol{\\xi}^{(t-1)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Second, note that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\pmb w}_{h}^{(t)}={\\pmb w}_{h}^{(t-1)}-\\eta_{w}\\nabla_{{\\pmb w}_{h}}\\mathcal L({\\pmb\\theta}^{(t-1)})}\\\\ &{\\quad\\quad=\\gamma{\\pmb\\alpha}_{h}^{(t-1)}-\\gamma^{2}\\cdot\\frac{1}{\\gamma}\\eta_{Q}\\nabla_{{\\pmb\\alpha}_{h}}\\ell({\\pmb\\xi}^{(t-1)})}\\\\ &{\\quad\\quad=\\gamma\\left({\\pmb\\alpha}_{h}^{(t-1)}-\\eta_{Q}\\nabla_{{\\pmb\\alpha}_{h}}\\ell({\\pmb\\xi}^{(t-1)})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Dividing both sides of the above equality by $\\gamma$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{\\alpha}_{h}^{(t)}=\\pmb{\\alpha}_{h}^{(t-1)}-\\eta_{Q}\\nabla_{\\pmb{\\alpha}_{h}}\\ell(\\pmb{\\xi}^{(t-1)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Hence, (41) follows from combining (80) and (81). ", "page_idx": 24}, {"type": "text", "text": "E.5 Proof of Lemma 7 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Throughout this proof, we omit the superscript $(t)$ for simplicity. We first compute the gradient of $\\mathcal{L}$ w.r.t. $Q_{h}$ . By (36) we know that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\ell(\\pmb\\xi)=\\mathcal{L}(\\pmb\\theta)=\\frac{1}{2K}\\sum_{k=1}^{K}\\left\\|\\bar{\\pmb Z}\\pmb\\delta_{k}\\right\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and thus we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial\\ell(\\pmb{\\xi})}{\\partial\\pmb{Q}_{h}}=\\frac{1}{K}\\sum_{k=1}^{K}\\sum_{j=1}^{N}\\frac{\\partial}{\\partial\\delta_{j k}}\\left[\\frac{1}{2}\\left\\|\\sum_{i=1}^{N}\\delta_{i k}\\bar{z}_{i}\\right\\|_{2}^{2}\\right]\\frac{\\partial\\delta_{j k}}{\\partial\\pmb{Q}_{h}}}\\\\ &{\\displaystyle~~~~~~=\\frac{\\gamma}{K}\\sum_{k=1}^{K}\\sum_{j=1}^{N}\\left(\\bar{\\pmb{Z}}\\delta_{k}\\right)^{\\top}\\bar{z}_{j}\\cdot\\underbrace{\\alpha_{h,k}s_{j k}^{h}\\sum_{i=1}^{N}s_{i k}^{h}(\\boldsymbol{v}_{j}-\\boldsymbol{v}_{i})\\boldsymbol{v}_{k}^{\\top}}_{=:\\boldsymbol{G}_{h,j k}^{h,j k}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Note that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\|G^{h,j k}\\right\\|_{F}\\leq2\\alpha s_{j k}^{h},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we use the fact that $\\left\\|(\\pmb{v}_{j}-\\pmb{v}_{i})\\pmb{v}_{k}^{\\top}\\right\\|_{2}\\,\\leq\\,2$ (recall that we assume each $\\pmb{v}_{k}$ has unit norm, $k\\in[K]$ .) Combining (82) and (83), we have the desired result ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left\\|\\frac{\\partial\\ell(\\xi)}{\\partial Q_{h}}\\right\\|_{F}\\leq\\frac{\\gamma}{K}\\displaystyle\\sum_{k=1}^{K}\\displaystyle\\sum_{j=1}^{N}\\|\\bar{Z}\\delta_{k}\\|_{2}\\|\\bar{z}_{j}\\|_{2}\\left\\|G^{h,j k}\\right\\|_{F}}\\\\ {\\displaystyle\\leq\\frac{2\\gamma}{K}\\displaystyle\\sum_{k=1}^{K}\\displaystyle\\sum_{j=1}^{N}\\|\\bar{Z}\\delta_{k}\\|_{2}\\,\\bar{f}_{\\operatorname*{max}}\\alpha s_{j k}^{h}}\\\\ {\\displaystyle}&{\\leq\\frac{2\\gamma\\bar{f}_{\\operatorname*{max}}\\alpha}{K}\\sqrt{K}\\displaystyle\\sqrt{\\sum_{k=1}^{K}\\|\\bar{Z}\\delta_{k}\\|_{2}^{2}}}\\\\ {\\displaystyle}&{\\leq2\\sqrt{2}\\gamma\\bar{f}_{\\operatorname*{max}}\\alpha\\sqrt{\\ell(\\xi)-\\mathcal{L}^{\\star}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\bar{f}_{\\operatorname*{max}}$ is defined in (12) and the third line follows from Cauchy-Schwarz inequality. ", "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: we clearly state in the abstract and introduction the claims we made, including the contributions made in the paper and important assumptions and limitations. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: we clearly state our assumptions. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: we provide the full set of assumptions and a complete (and correct) proof. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: see Appendix A. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [No] ", "page_idx": 28}, {"type": "text", "text": "Justification: The experiments are very simple and can be easily reproduced by following the instructions in the paper. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Experiment details are included in Section A. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [No] ", "page_idx": 28}, {"type": "text", "text": "Justification: stochasticity is not critical in our experiments. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: the results are irrelevant to the compute resources. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: the research conducted in the paper conforms with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: this is a theoretical paper and it has no societal impact. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: the paper aims to provide a better understanding on existing algorithms and thus poses no such risks. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: the paper does not use existing assets. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 30}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: the paper does not release new assets. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]