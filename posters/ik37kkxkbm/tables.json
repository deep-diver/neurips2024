[{"figure_path": "ik37kKxKBm/tables/tables_2_1.jpg", "caption": "Table 1: Comparisons with existing theoretical works that study the learning dynamics of transformers in ICL. Here, the last column refers to the fact that the response in the regression task is generated by a linearly weighted unknown representation (feature) model.", "description": "This table compares the current work with other theoretical studies on the learning dynamics of transformers in in-context learning (ICL).  It contrasts various aspects of these studies, including whether they considered nonlinear attention mechanisms, multi-head attention, task shifting, gradient descent, noisy data, and whether the response variable was generated by a linearly weighted representation model. The table highlights the novel contributions of the current work by showing how it overcomes limitations of prior studies.", "section": "1.2 Related work"}, {"figure_path": "ik37kKxKBm/tables/tables_14_1.jpg", "caption": "Table 1: Comparisons with existing theoretical works that study the learning dynamics of transformers in ICL. Here, the last column refers to the fact that the response in the regression task is generated by a linearly weighted unknown representation (feature) model.", "description": "This table compares the current work with other theoretical studies on the learning dynamics of transformers in in-context learning.  It highlights key differences in settings (e.g., linearity of the function, multi-task setting, presence of noise, the use of multi-head attention), training analysis methods (gradient descent), and generalization capabilities. The final column notes whether the response in the regression task is modeled with a linearly weighted, unknown representation.", "section": "1.2 Related work"}]