{"references": [{"fullname_first_author": "A. Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, the foundation of the models studied in the current paper."}, {"fullname_first_author": "T. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper empirically demonstrated the remarkable in-context learning capabilities of large language models, which is the central phenomenon studied in the current paper."}, {"fullname_first_author": "S. Garg", "paper_title": "What can transformers learn in-context? a case study of simple function classes", "publication_date": "2022-12-01", "reason": "This paper provided a theoretically tractable framework for studying in-context learning in transformers, which is the foundation of many theoretical analyses of ICL, including the analysis in the current paper."}, {"fullname_first_author": "E. Aky\u00fcrek", "paper_title": "What learning algorithm is in-context learning? investigations with linear models", "publication_date": "2022-12-01", "reason": "This paper showed that transformers can represent gradient descent of ridge regression, a result that the current paper builds upon and extends."}, {"fullname_first_author": "Y. Huang", "paper_title": "In-context convergence of transformers", "publication_date": "2023-10-26", "reason": "This paper analyzed the training dynamics of shallow transformers with softmax attention for in-context learning, providing a foundation for the analysis in the current paper."}]}