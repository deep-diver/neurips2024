[{"type": "text", "text": "Learning the Expected Core of Strictly Convex Stochastic Cooperative Games ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Nam Phuong Tran ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The Anh Ta ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shuqing Shi ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "CSIRO\u2019s Data61 Marsfield, NSW, Australia theanh.ta@csiro.au ", "page_idx": 0}, {"type": "text", "text": "Department of Computer Science University of Warwick Coventry, United Kingdom   \nnam.p.tran@warwick.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Department of Informatics King\u2019s College London London, United Kingdom shuqing.shi@kcl.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Debmalya Mandal ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yali Du ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Department of Computer Science University of Warwick Coventry, United Kingdom debmalya.mandal@warwick.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Department of Informatics King\u2019s College London London, United Kingdom yali.du@kcl.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Long Tran-Thanh Department of Computer Science University of Warwick Coventry, United Kingdom long.tran-thanh@warwick.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reward allocation, also known as the credit assignment problem, has been an important topic in economics, engineering, and machine learning. An important concept in reward allocation is the core, which is the set of stable allocations where no agent has the motivation to deviate from the grand coalition. In previous works, computing the core requires either knowledge of the reward function in deterministic games or the reward distribution in stochastic games. However, this is unrealistic, as the reward function or distribution is often only partially known and may be subject to uncertainty. In this paper, we consider the core learning problem in stochastic cooperative games, where the reward distribution is unknown. Our goal is to learn the expected core, that is, the set of allocations that are stable in expectation, given an oracle that returns a stochastic reward for an enquired coalition each round. Within the class of strictly convex games, we present an algorithm named Common-Points-Picking that returns a point in the expected core given a polynomial number of samples, with high probability. To analyse the algorithm, we develop a new extension of the separation hyperplane theorem for multiple convex sets. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The reward allocation problem is a fundamental challenge in cooperative games that seeks reward allocation schemes to motivate agents to collaborate or satisfy certain constraints, and its solution concepts have recently gained popularity within the machine learning literature through its application in explainable AI (XAI) [17, 28, 13, 31] and cooperative Multi-Agent Reinforcement Learning (MARL) [29, 12, 30]. In the realm of XAI, designers often seek to understand which factors of the model contribute to the outputs. Solution concepts such as the Shapley value [17, 28] and the core [31, 13] provide frameworks for assessing the influence that each factor has on the model\u2019s behavior. In cooperative MARL, these solution concepts offer a framework for distributing team rewards to individual agents, promoting cooperation among players, and preventing the occurrence of lazy learner phenomena [29, 12, 30]. A crucial notion of reward allocation is stability, defined as an allocation scheme wherein no agent has the motivation to deviate from the grand coalition. The set of stable allocations is called the core of the game. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In the classical setting, the reward function is deterministic and commonly known among all agents, with no uncertainty within the game. However, assuming perfect knowledge of the game is often unrealistic, as the outcome of the game may contain uncertainty. This led to the study of stochastic cooperative games, dated back to the seminal works of [9, 27], where stability can be satisfied either with high probability, known as the robust core, or in expectation, known as the expected core. However, in these works, the distribution of stochastic rewards is given, allowing agents to calculate the reward before the game starts, which is not practical since the knowledge of the reward distribution may only be partially known to the players. When the distribution of the stochastic reward is unknown, the task of learning the stochastic core by sequentially interacting with the environment appears much more challenging. Early attempts [19, 20] studied robust core and expected core learning under full-information, where full information means that the random rewards of all coalitions are observed at each round. However, full-information feedback is a strong assumption, which does not hold in many real-world situations. Instead, agents typically observe the reward of their own coalition only (e.g., only know the value of their own action - joining a particular coalition in this case), which is typically known as bandit feedback in the online learning literature. ", "page_idx": 1}, {"type": "text", "text": "In our work, we focus on learning the expected core, which circumvents the potential emptiness of the robust core in many practical cases. Moreover, instead of full-information feedback, where the stochastic rewards of all coalitions are observed each round, we consider the bandit feedback setting, where only the stochastic reward of the inquired coalition is observed each round. Given the lack of knowledge about the probability distribution of the reward function, learning the expected core using data-driven approaches with bandit feedback is a challenging task. ", "page_idx": 1}, {"type": "text", "text": "Against this background, the contribution of this paper is three-fold: (1) We focus on expected core learning problem with unknown reward function, and propose a novel algorithm called the Common-Points-Picking algorithm, the first of its kind that is designed to learn the expected core with high probability. Notably, this algorithm is capable of returning a point in an unknown simplex, given access to the stochastic positions of the vertices, which can also be used in other domains, such as convex geometry. (2) We establish an analysis for finite sample performance of the Common-Points-Picking algorithm. The key component of the analysis revolves around a novel extension of the celebrated hyperplane separation theorem, accompanied by further results in convex geometry, which can also be of independent interest. (3) We show that our algorithm returns a point in expected core with at least $1-\\delta$ probability, using pol $\\mathrm{y}(n,\\log(\\delta^{-1}))$ number of samples. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Stochastic Cooperative Games. The study of stochastic cooperative games can be traced back to at least [9]. The main goal of the allocation scheme is to minimise the probability of objections arising after the realisation of the rewards. [27, 26] later extended and refined the notion of stability in stochastic settings. These seminal works require information about the reward distribution to compute a stable allocation scheme before the game starts. Stochastic cooperative games have also been studied in a Bayesian setting in a series of papers [5, 7, 8, 6]. These works develop a framework for Bayesian stochastic cooperative games, where the distribution of the reward is conditioned on a hidden parameter following a prior distribution. The prior distribution is common knowledge among agents and can be used to define a new concept of stability called Bayesian core. In contrast to previous works, our paper focuses on studying scenarios where the reward distribution or prior knowledge is not disclosed to the principal agent and computing a stable allocation requires a data-driven method. ", "page_idx": 1}, {"type": "text", "text": "Learning the Core. The literature on learning the core through sample-based methods can be categorised based on the type of core one seeks to evaluate. Two main concepts of the stochastic core are commonly considered, namely the robust core (i.e. core constraints are satisfied with high probability) [11, 22, 19] and the expected core (i.e. core constraints are satisfied in expectation) [10, 20]. The robust core is defined in a manner that allows the core inequalities to be satisfied with high probability when the reward is realised. However, this definition may lead to the practical issue of an empty robust core, as illustrated in [10]. To mitigate the potential emptiness of the robust core, we investigate the learnability of the expected core. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "The work most closely related to ours is [20], in which the authors introduce an algorithm designed to approximate the expected core using a robust optimization framework. In the context of full information feedback, where rewards for all allocations are revealed each round, the algorithm demonstrates asymptotic convergence to the expected core. Furthermore, the authors provide an finite-samples error bound for the distance to the expected core. However, when dealing with bandit feedback, where the reward of the enquired coalition is returned each round, naively applying the algorithm of [20] may result in an exponential number of samples in terms of the number of players. In the bandit feedback setting, as we later establish in Theorem 7 that, in general cooperative games, no algorithm can learn the expected core with a finite number of samples without additional assumptions. This highlights the key difference in dealing with bandit feedback compared to full-information feedback. Given this limitation, we narrow our focus to (strictly) convex games, an important class of cooperative games where the expected reward function is (strictly) supermodular. By leveraging strict convexity, we introduce the Common-Points-Picking algorithm, which efficiently returns a point in the expected core with high probability using only a polynomial number of samples. While [20] proposed a general algorithm applicable to strictly convex games, we argue that it lacks statistical and computational efficiency due to the absence of a mechanism to exploit the supermodular structure of the expected reward function. Further detailed comparison can be found in Appendix E.1. ", "page_idx": 2}, {"type": "text", "text": "Learning the Shapley Value. Another relevant line of research is the Shapley approximation, as the Shapley value is the barycenter of the core in convex games. Therefore, the approximation error of the Shapley value is an upper bound on the distance between the approximated Shapley value and the expected core [4, 18, 24]. It is worth noting that, in contrast to the Shapley approximation method, our algorithm ensures the return of a point in the core. In comparison, the Shapley approximation approach can only provide an allocation with a bounded distance from the core. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Description ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notations. For $k\\in\\mathbb{N}^{+}$ , denote $[k]$ as set $\\{1,2,\\ldots,k\\}$ . For $n\\in\\mathbb{N}^{+}$ , let ${\\bf E}^{n}$ be the $n$ -dimensional Euclidean space, and let us denote $\\mathcal{D}$ as the Euclidean distance in ${\\bf E}^{n}$ . Denote ${\\mathbf{1}}_{n}$ as the vector $[1,...,1]\\in\\bar{\\mathbb{R}}^{n}$ . Denote $\\langle\\cdot,\\cdot\\rangle$ as the dot product. For a set $C$ , we denote $C\\setminus x$ as the set resulting from eliminating an element $x$ in $C$ . For $C\\subset\\mathbf{E}^{n}$ , let $\\textstyle\\operatorname{diam}(C):=\\operatorname*{max}_{x,y\\in C}D(x,y)$ , and Conv $(C)$ denote the diameter and the convex hull of $C$ , respectively. ", "page_idx": 2}, {"type": "text", "text": "Denote $\\mathfrak{S}_{n}:=\\{\\omega:[n]\\rightarrow[n]\\ |\\ \\omega\\$ is a bijection $\\}$ as the permutation group of $[n]$ . For any collection of permutations ${\\mathcal{P}}\\subset{\\mathfrak{S}}_{n}$ , we denote $\\omega_{p}$ , $p\\in[|\\mathcal{P}|]$ , as $p^{\\mathrm{th}}$ permutation order in $\\mathcal{P}$ . Let $s_{i}:=(i,i\\!+\\!1)$ denote the adjacent transposition between $i$ and $i+1$ . Given a set $C$ , we denote by $\\mathcal{M}(C)$ the space of all probability distributions on $C$ . ", "page_idx": 2}, {"type": "text", "text": "Stochastic Cooperative Games. A stochastic cooperative game is defined as a tuple $(N,\\mathbb{P})$ , where $N$ is a set containing all agents with number of agents to be $|N|\\;=\\;n$ , and $\\mathbb{P}=$ $\\{\\mathbb{P}_{S}\\in\\mathcal{M}([0,1])~|~S\\subseteq N\\}$ is the collection of reward distributions with $\\mathbb{P}_{S}$ to be the reward distribution w.r.t. the coalition $S$ . For any coalition $S\\subseteq N$ , we denote $\\mu(S):=\\mathbb{E}_{r\\sim\\mathbb{P}_{S}}[r]$ as the expected reward of coalition $S$ . For a reward allocation scheme $x\\in\\mathbb{R}^{n}$ , let $\\textstyle{\\dot{x}}(S):=\\sum_{i\\in S}{\\dot{x}}_{i}$ as the total reward allocation for players in $S$ . A reward allocation $x$ is effective if $x(N)=\\mu(\\bar{N})$ . The hyperplane of all effective reward allocations, denoted by $H_{N}$ , is defined as $H_{N}=\\{x\\in\\mathbb{R}^{n}\\mid x(N)=\\mu(N)\\}$ . The convex stochastic cooperative game can be defined as follows: ", "page_idx": 2}, {"type": "text", "text": "Definition 1 (Convex stochastic cooperative game). A stochastic cooperative game is convex if the expected reward function is supermodular [23], that is, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mu\\left(S\\cup\\left\\{i\\right\\}\\right)-\\mu(S)\\geq\\mu\\left(C\\cup\\left\\{i\\right\\}\\right)-\\mu(C))\\,,\\quad\\forall i\\notin S\\cup C\\;{\\mathrm{and}}\\;\\forall C\\subseteq S\\subseteq N.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Next, we define the notion of strict convexity as follows: ", "page_idx": 2}, {"type": "text", "text": "Definition 2 $\\boldsymbol{.}$ -Strictly convex cooperative game). For some constant $\\varsigma>0$ , a cooperative game is $\\varsigma$ -strictly convex if the expected reward function is $\\varsigma$ -strictly supermodular [23], that is, $\\mu$ is supermodular and ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mu(S\\cup\\{i\\})-\\mu(S)\\geq\\mu(C\\cup\\{i\\})-\\mu(C)+\\varsigma,\\quad\\forall i\\notin S\\cup C\\mathrm{~and~}\\forall C\\subseteq S\\subseteq N.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Following [20], we define the expected core as follows: ", "page_idx": 3}, {"type": "text", "text": "Definition 3 (Expected core [20]). The core is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname{E-Core}:=\\{x\\in\\mathbb{R}^{n}\\mid x(N)=\\mu(N);\\ x(S)\\geq\\mu(S),\\ \\forall S\\subseteq N\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "That is, the E-Core is the collection of all effective reward allocation schemes $x$ (i.e., schemes where the total allocation adds up to the expected reward of the grand coalition $N$ - see the definition of effective reward allocation above), where if any agents deviate from the grand coalition $N$ to form a smaller team $S$ , regardless of how they allocate the reward, each individual will not receive more expected reward than if they had stayed in $N$ . Note that, as E-Core $\\subset H_{N}$ , its dimension is at most $(n-1)$ . We say that E-Core is full dimensional whenever its dimension is $n-1$ . ", "page_idx": 3}, {"type": "text", "text": "In convex games, each vertex of the core in the convex game is a marginal vector corresponding to a permutation order [23]. This is a special property of convex games, which plays a crucial role in our algorithm design. For any $\\omega\\in\\mathfrak{S}_{n}$ , define the marginal vector $\\phi^{\\omega}\\in\\mathbb{R}^{n}$ corresponding to $\\omega$ , that is, its $i^{\\mathrm{th}}$ entry is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\phi_{i}^{\\omega}:=\\mu(P^{\\omega}(i))-\\mu(P^{\\omega}(i)\\setminus i),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $P_{i}^{\\omega}=\\{j\\mid\\omega(j)\\leq\\omega(i)\\}$ . It is known from [23] that if the expected reward function is strictly supermodular, the $E$ -Core must be full dimensional. ", "page_idx": 3}, {"type": "text", "text": "3.2 Problem Setting ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In our setting we assume that there is a principal who does not know the reward distribution $\\mathbb{P}$ . In each round $t$ , the principal queries a coalition $S_{t}\\subset N$ . The environment returns a vector $r_{t}\\sim\\mathbb{P}_{S_{t}}$ independently of the past. For simplicity, we assume that the agent knows the expected reward of the grand coalition $\\mu(N)$ . Additionally, we assume that the convexity of the game, that is, $\\mu$ is supermodular. Our question is how many samples are needed so that with high probability $1-\\delta$ , the algorithm returns a point $x\\in\\mathrm{E}$ -Core. More particularly, we ask whether or not there is an algorithm that can output a point in the E-Core, with probability at least $1-\\delta$ and the number of queries ", "page_idx": 3}, {"type": "equation", "text": "$$\nT=\\mathrm{poly}(n,\\log(\\delta^{-1})).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "As well shall show in Theorem 7, if E-Core is not full-dimensional, no algorithm can output a point in E-Core with finite samples. As such, to guarantee the learnability of the E-Core. From now on in the rest of this paper, we assume that: ", "page_idx": 3}, {"type": "text", "text": "Assumption 4. The game is $\\varsigma$ -strictly convex. ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Note that strict convexity immediately implies full dimensionality [23], which is not the case with convexity (refer to Section 5). As we shall show in the next sections, strict convexity is a sufficient condition allowing the principal to learn a point in E-Core with polynomial number of samples. Practical examples of strictly convex games can be found in appendix D. ", "page_idx": 3}, {"type": "text", "text": "4 Learning the Expected Core ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we propose a general-purpose Common-Point-Picking algorithm that is able to return a point of unknown full-dimensional simplex, given an oracle that provides a noisy position of the simplex\u2019s verticies. Under the assumption that the game is strictly convex, we show that, when applying Common-Points-Picking algorithm to the $\\varsigma,$ -strictly convex game, it can return a point in E-Core provided the number of samples is poly $(n,\\varsigma)$ . ", "page_idx": 3}, {"type": "text", "text": "4.1 Geometric Intuition ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given E-Core polytope of dimension $(n-1)$ . In deterministic case when the knowledge of the game is perfect, to compute a point in the core, one can query a marginal vector corresponding to a permutation order $\\omega\\in\\mathfrak{S}_{n}$ [23]. Given that we have uncertainty in the estimation of E-Core, this approach is no longer applicable. The reason is that for each vertex $\\phi^{\\omega}$ , we do not precisely compute its position. Instead, we only have information on its confidence set $\\mathcal{C}(\\phi^{\\omega},\\delta)$ , a compact $(n-1)$ -dimensional set. The confidence set contains $\\phi^{\\omega}$ with probability at least $(1-\\delta)$ , as we will define shortly in this section. ", "page_idx": 3}, {"type": "text", "text": "One approach to overcome the effect of uncertainty is that we can estimate multiple vertices of the E-Core. Let ${\\mathcal{P}}\\subset{\\mathfrak{S}}_{n}$ be a collection of permutations, and $Q=\\{\\phi^{\\omega_{p}}\\mid\\omega_{p}\\in\\mathcal{P}\\}$ be the set of vertices corresponding to $\\mathcal{P}$ . For brevity, we denote $\\mathcal{C}_{p}:=\\mathcal{C}(\\phi^{\\omega_{p}},\\delta)$ , $\\omega_{p}\\in\\mathcal{P}$ . In this section, we assume that the confidence sets contain the true vertices, that is, $\\phi^{\\omega_{p}}\\in\\mathcal{C}_{p}$ , $\\forall p\\in[|\\mathcal{P}|]$ , which can be guaranteed with high probability. It is clear that Conv $(Q)\\subset\\mathbf{E}$ -Core, since $Q$ is a subset of vertices of E-Core. The challenge is that, as the ground truth of the position of the vertex can be any point in the confidence set, we need to ensure that the algorithm outputs a point in the convex hull of any collection of points in the confidence sets. A sufficient condition to achieve this is that, given $|\\mathcal P|$ confidence sets $\\{\\mathcal{C}_{p}\\}_{p\\in[|\\mathcal{P}|]}$ , for each $x^{p}\\in\\mathcal{C}_{p}$ , ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\bigcap_{x^{p}\\in C_{p}}\\mathrm{Conv}\\left(\\{x^{p}\\}_{p\\in[|\\mathcal{P}|]}\\right)\\neq\\emptyset.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This condition means that there exists a common point among all the convex hulls formed by choosing any point in confidence sets, $x^{p}\\in\\mathcal{C}_{p}$ . We call the above intersection a set of common points. The reason why it is a sufficient condition is that this set is a subset of a ground-truth simplex, implying that it is in the E-Core. Formally, we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\bigcap_{x^{p}\\in C_{p}}\\operatorname{Conv}\\left(\\{x^{p}\\}_{p\\in[|\\mathcal{P}|]}\\right)\\subset\\operatorname{Conv}\\left(Q\\right)\\subset\\operatorname{E-Core};\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which means that any common point must be in E-Core. Moreover, the set of common points is learnable. As we shall show in the next section, our algorithm can access the set of common points whenever it is nonempty. ", "page_idx": 4}, {"type": "text", "text": "We first state a necessary condition for the number of vertices of E-Core need to estimate for (5) can be satisfied:   \nProposition 5. Assume that all the confidence sets are full dimensional, that is, $\\dim({\\mathcal{C}}_{p})\\,=$ $n-1$ , $\\bar{\\forall}p\\in[|\\mathcal{P}|]$ , and suppose that $|{\\mathcal{P}}|<n$ , $\\bigcap_{x^{p}\\in\\mathcal{C}_{p}}\\mathrm{Conv}\\left(\\{x^{p}\\}_{p\\in[|\\mathcal{P}|]}\\right)=\\varnothing.$ (7) Proposition 5 implies that one needs to estimate at least $n$ vertices to guarantee the existence of a common point. ", "page_idx": 4}, {"type": "table", "img_path": "ZRYFftR4xn/tmp/0934681ab9f447ba9e9b3005f81c7c40ace1e0ea637afb414a144c17d0023475.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "4.2 Common-Points-Picking Algorithm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As Proposition 5 suggests, we need to estimate at least $n$ vertices. As such, from now on, we assume that $|{\\mathcal{P}}|=n$ . Based on the above intuition, we propose Common-Points-Picking, whose pseudo code is described in Algorithm 1. ", "page_idx": 4}, {"type": "text", "text": "The Common-Points-Picking Algorithm can be described as follows. First, let us explain the notation. In epoch ep, the variable $r_{\\mathrm{ep}}\\left(\\cdot\\right)$ in the algorithm represents the reward value of the enquired coalition; $\\hat{\\phi}^{\\omega_{p}}(\\mathrm{ep})$ represents the estimated marginal vector w.r.t. $\\omega_{p}$ . In each epoch ep, assuming that the stopping condition is not satisfied, the algorithm estimates the marginal vectors corresponding to the collection of given permutation orders $\\bar{Q}=\\left\\{\\hat{\\phi}^{\\omega_{p}}(\\mathrm{ep})\\right\\}_{p\\in[n]}$ (lines 6-10). For each $p\\in[n]$ , the estimation can be done by querying the value of the nested sequence $P_{1}^{\\omega_{p}}$ , $P_{2}^{\\omega_{p}}$ , . . . , $P_{n}^{\\omega_{p}}$ (line 6-7), then estimating the marginal contribution of each player with respect to the permutation order $\\omega_{p}$ (line 10). Next, it calculates the confidence bonus $b_{\\mathrm{ep}}$ of the confidence sets and checks the stopping condition for the next epoch. The algorithm continues until the stopping condition is satisfied, and then returns the average of the most recent values of the marginal vectors corresponding to $\\mathcal{P}$ . ", "page_idx": 4}, {"type": "text", "text": "The termination of the Common-Points-Picking algorithm is based on the Stopping-Condition algorithm (Algorithm 2), which can be described as follows. Consider the case where $a\\neq\\textit{}\\textbf{\\textit{0}}$ . For each point $x^{p}\\quad\\in\\quad Q$ , the algorithm attempts to calculate the separating hyperplane $H_{p}(Q)$ , that separates $x^{p}$ from the rest ${\\textit{Q}}\\setminus{\\textit{x}}^{p}$ (line 7). ", "page_idx": 4}, {"type": "text", "text": "The hyperplane $H_{p}(Q)$ is defined by two parameters $(v^{p}\\,\\in\\,\\mathbb{R}^{n},\\Bar{c}^{p}\\,\\in\\,\\mathbb{R})$ , where $v^{p}$ is one of its unit normal vectors, together with $c^{p}$ , satisfying Eq. (8). Specifically, the second and third equality in (8) implies that $H_{p}(Q)$ is parallel and at a distance of $\\epsilon_{\\mathrm{ep}}$ (toward $x^{p}$ ) from the hyperplane that passes through all the points in $Q\\setminus x^{p}$ . The fourth equality in (8) guarantees that $H_{p}(Q)$ is a subset of $H_{N}$ (as the normal vector of $H_{N}$ is ${\\mathbf{1}}_{n}$ ). After computing $H_{p}(Q)$ , the algorithm checks whether the distance from the confidence set $\\mathcal{C}_{p}$ to $H_{p}(Q)$ is large enough (lines 8-12). The stopping condition checks for all $\\textit{p}\\in\\ [n]$ ; if no condition is violated, then the algorithm returns TRUE. An example of the construction of separating hyperplanes in $H_{N}$ , where $n=3$ is depicted in Figure 1. ", "page_idx": 5}, {"type": "text", "text": "Note that the input of algorithm $\\mathcal{P}$ can be any collection of permutation orders such that $|{\\mathcal{P}}|=$ $n$ . In the next section, we will provide instances of the collection of permutation orders, in which, under Assumption 4, the algorithm can output a point in E-Core with high probability and a polynomial number of samples. ", "page_idx": 5}, {"type": "text", "text": "Remark 6. Most of the computational burden lies in computing the separating hyperplane $H_{p}(Q)$ for each $p$ (line 7), and calculating the ", "page_idx": 5}, {"type": "text", "text": "Algorithm 2 Stopping Condition ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "1: Input collection of $n$ points $Q$ , confidence bonus $b_{\\mathrm{ep}}$ .   \n2: Compute $\\epsilon_{\\mathrm{ep}}=2\\sqrt{n}b_{\\mathrm{ep}}$ .   \n3: if $Q=\\mathcal{O}$ then   \n4: Return FALSE.   \n5: end if   \n6: for $p\\in[n]$ do   \n7: Computing $H_{p}(Q)$ , i.e., compute $\\left(v^{p}\\in\\mathbb{R}^{n},c^{p}\\in\\mathbb{R}\\right)$ such that. $\\begin{array}{r}{\\left\\{\\begin{array}{l l}{\\|v^{p}\\|_{2}}&{=1;}\\\\ {\\langle v^{p},x\\rangle}&{=c^{p}+\\epsilon_{\\mathrm{ep}},\\forall x\\in Q\\setminus x^{p}.}\\\\ {\\langle v^{p},x^{p}\\rangle}&{<c^{p}+\\epsilon_{\\mathrm{ep}};}\\\\ {\\langle v^{p},\\mathbf{1}_{n}\\rangle}&{=0;}\\end{array}\\right.}\\end{array}$ (8) ", "page_idx": 5}, {"type": "text", "text": "8: Computing distance: ", "page_idx": 5}, {"type": "equation", "text": "$$\nh_{p}:=\\operatorname*{min}_{\\substack{x:\\|x-x^{p}\\|_{\\infty}<b_{\\mathrm{ep}}}}c^{p}-\\left<v^{p},x\\right>.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "9: if $h_{p}<n\\,\\epsilon_{\\mathrm{ep}}$ then   \n10: Return FALSE.   \n11: end if   \n12: end for   \n13: Return TRUE. ", "page_idx": 5}, {"type": "text", "text": "distance between the confidence set $\\mathcal{C}_{p}$ and $H_{p}(Q)$ (line 8) in Stopping Condition. Since all tasks can be completed within polynomial time w.r.t. $n$ , our algorithm is polynomial. ", "page_idx": 5}, {"type": "text", "text": "There are two challenges regarding the Common-Points-Picking algorithm. First, we need to design confidence sets such that they contain the vertices with high probability, which can be done easily using Hoeffding\u2019s inequality. Second, we need to define a stopping condition that can guarantee the non-emptiness of the common set and output a point in the common set with a polynomial number of samples. The second question is more involved and requires proving new results in convex geometry, including an extension of the hyperplane separation theorem, as we shall fully explain in Section 5.1. ", "page_idx": 5}, {"type": "text", "text": "Confidence Set To calculate this set we will use the probability concentration inequality to obtain a confidence set: First, let $r_{\\mathrm{ep}}\\left(\\varnothing\\right)\\;=\\;$ $0,\\;\\forall\\,\\mathrm{ep}>0$ , define the empirical marginal vector w.r.t. permutation $\\omega$ as $\\bar{\\hat{\\phi}}^{\\omega}\\in\\mathbb{R}^{n}$ at epoch ep as ", "page_idx": 5}, {"type": "image", "img_path": "ZRYFftR4xn/tmp/2db64837898e87f24a976b2bfecf8821f5f72516c9825701b0e17a5f31834234.jpg", "img_caption": ["Figure 1: Set of common points constructed by separating hyperplanes. The intersection of halfspaces defined by $H_{p}(Q)$ , $\\forall p\\:\\in\\:[n]$ , creates a subset of common points. The common points are in E-Core, provided that the confidence sets contain the ground-truth vertices. "], "img_footnote": [], "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{\\phi}_{i}^{\\omega}(\\mathrm{ep})=\\frac{1}{\\mathrm{ep}}\\sum_{s=1}^{\\mathrm{ep}}r_{s}\\left(P_{i}^{\\omega}\\right)-r_{s}\\left(P_{i}^{\\omega}\\setminus i\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "By the Hoeffding\u2019s inequality, one has that after ep epochs, $\\forall\\omega\\in\\mathcal{P}$ , for each $i\\in[n]$ , with probability at least $1-\\delta,\\phi^{\\omega}$ must belong to the following set: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{C}(\\phi^{\\omega},\\delta)=\\left\\{x\\in H_{N}\\bigg|\\left\\|x-\\hat{\\phi}^{\\omega}\\right\\|_{\\infty}\\leq b_{\\mathrm{ep}}\\right\\};\\quad\\mathrm{s.t.}\\quad b_{\\mathrm{ep}}:=\\sqrt{\\frac{2\\log(n\\exp\\delta^{-1})}{\\exp}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "5 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Before proceeding to the analysis of Algorithm 1, let us exclude the case where learning a stable allocation is not possible, thereby emphasizing the need of the strict convexity assumption. ", "page_idx": 6}, {"type": "text", "text": "Theorem 7. Suppose that $E$ -Core has dimension $k<n-1,$ , for any $0.2>\\delta>0$ and with finite samples, no algorithm can output a point in $E$ -Core with probability at least $1-\\delta$ . ", "page_idx": 6}, {"type": "text", "text": "The proof of Theorem 7 employs an information-theoretic argument. In particular, we construct two game instances with low-dimensional cores, utilising the concept of face games introduced in [14]. The construction of the two games is designed to ensure that, despite the KL distance between their reward distributions being arbitrarily small, their low-dimensional cores are parallel and maintain a positive distance from each other. Consequently, their cores have empty intersections. This implies that, given finite samples, no algorithm can reliably distinguish between the two games. Since these games lack a mutually stable allocation, if an algorithm fails to differentiate between them, it is likely to choose the wrong core with a certain probability. ", "page_idx": 6}, {"type": "text", "text": "It is worth noting that convex games may have a low-dimensional core, as demonstrated in the following example. ", "page_idx": 6}, {"type": "text", "text": "Example 8. Let $\\mu(S)=|S|$ for all $S\\subseteq N$ . It is easy to verify that $\\mu$ is indeed convex. The marginal contribution of any player $i$ to any set $S\\subseteq N$ is ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mu(S\\cup i)-\\mu(S)=1,\\;\\forall S\\subset N.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Therefore, the only stable allocation is ${\\mathbf{1}}_{n}$ , which coincides with the Shapley value. Hence, the core is one-point set. According to Theorem 7, since the core has a dimension of 0 in this case, it is impossible to learn a stable allocation with a finite number of samples. ", "page_idx": 6}, {"type": "text", "text": "Example 8 suggests that convexity alone does not ensure the problem\u2019s learnability, emphasizing the requirement for strict convexity. ", "page_idx": 6}, {"type": "text", "text": "5.1 On the Stopping Condition ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this subsection, we explain the construction of the stopping condition in Algorithm 2. We will show that the stopping condition can always be satisfied with the number of samples needed polynomially w.r.t the width of the ground truth simplex. Intuitively, the confidence sets need to shrink to be sufficiently small, relative to the width of the simplex, to guarantee the existence of a common point. ", "page_idx": 6}, {"type": "text", "text": "To simplify the presentation, we restrict our attention to $H_{N}$ and consider it as $\\mathbf{E}^{n-1}$ . First, we state a necessary condition for the existence of common points. ", "page_idx": 6}, {"type": "text", "text": "Proposition 9. Suppose there is a $(n-2)$ -dimensional hyperplane that intersect with all the interior of confidence sets $\\mathcal{C}_{p}$ , $\\forall p\\in[n]$ , then common points do not exist. ", "page_idx": 6}, {"type": "text", "text": "Proposition 9 suggests that if the ground truth simplex Conv $(Q)$ is not full-dimensional, then the common set is empty. In addition, even if Conv $(Q)$ is full-dimensional, but the confident sets are not sufficiently small, one can also create a hyperplane that intersects with all the confidence sets. For example, when the intersection of the confidence sets is not empty. ", "page_idx": 6}, {"type": "text", "text": "On the other hand, when the confidence sets are well-arranged and sufficiently small, that is, there does not exist a hyperplane that intersects with all of them, a nice separating property emerges, as stated in the next theorem. This new result can be considered as an extension of the classic separating hyperplane theorem [3]. First, let us recap the notion of separation as follows. ", "page_idx": 6}, {"type": "text", "text": "Definition 10 (Separating hyperplane). Let $C$ and $D$ be two compact and convex subsets of $\\mathbf{E}^{n-1}$ . Let $H$ be a hyperplane defined by the tuple $(v,c)$ , where $v$ is a unit normal vector and $c$ is a real number, such that $\\left\\langle x,v\\right\\rangle=c$ , $\\forall x\\in H$ . We say $H$ separates $C$ and $D$ if $\\langle x,v\\rangle>c$ , $\\forall x\\in$ $C$ ; and $\\langle y,v\\rangle<c$ , $\\forall y\\in D$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 11 (Hyperplane separation theorem for multiple convex sets). Assume that $\\{{\\mathcal{C}}_{p}\\}_{p\\in[n]}$ are mutually disjoint compact and convex subsets in $\\mathbf{E}^{n-1}$ . Suppose that there does not exist $a$ $(n-2)$ -dimensional hyperplane that intersects with confidence sets $\\mathcal{C}_{p}$ , $\\forall p\\in[n]$ , then for each $p\\in[n]$ , there exists a hyperplane that separates $\\mathcal{C}_{p}\\,f r o m\\bigcup_{q\\neq p}\\mathcal{C}_{q}$ . ", "page_idx": 6}, {"type": "text", "text": "Remark 12 (Non-triviality of Theorem 11). At a first glance, Theorem 11 may appear as a trivial extension of the classic hyperplane separation theorem due to the following reasoning: Consider the union of all hyperplanes that intersect $\\textstyle\\bigcup_{q\\neq p}{\\mathcal{C}}_{q}$ , which trivially contains $\\textstyle\\bigcup_{q\\neq p}{\\mathcal{C}}_{q}$ . Then, by assuming that these hyperplanes do not intersect $\\mathcal{C}_{p}$ , the separation between $\\mathcal{C}_{p}$ and $\\textstyle\\bigcup_{q\\neq p}{\\mathcal{C}}_{q}$ appears to follow from the classic separation hyperplane theorem. However, there is a flaw in the above reasoning: The union of these hyperplanes is not necessarily convex. Therefore, the classic separation hyperplane theorem cannot be applied directly. Instead, employing Carath\u00e9odory\u2019s theorem, we prove in Theorem 11 by contra-position that if the intersection between $\\mathcal{C}_{p}$ and $\\operatorname{Conv}(\\bigcup_{q\\neq p}{\\mathcal{C}}_{q})$ is non-empty, then we can construct a low-dimensional hyperplane that intersects with all the set. ", "page_idx": 7}, {"type": "text", "text": "When those confidence sets are well-separated, we can provide a sufficient condition for that the common points exist. Let $H_{p}$ be a separating hyperplane that separate $\\mathcal{C}_{p}$ from $\\textstyle\\bigcup_{q\\neq p}{\\mathcal{C}}_{q}$ . We define $H_{p}$ corresponding with tuple $(v^{p},c^{p})$ . Now, denote $E_{p}=\\left\\{x\\in\\mathbf{E}^{n-1}\\mid\\langle v^{p},x\\rangle\\,<\\,c^{p}\\right\\}$ as the half space containing $\\mathcal{C}_{p}$ . We have that: ", "page_idx": 7}, {"type": "text", "text": "Lemma 13. For any $\\boldsymbol{x}^{p}\\in\\mathcal{C}_{p}$ , $p\\in[n]$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\bigcap_{p\\in[n]}E_{p}\\subseteq{\\mathrm{Conv}}\\left(\\left\\{x^{p}\\right\\}_{p\\in[n]}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Consequently, $i f\\bigcap_{p\\in[n]}E_{p}$ is nonempty, it is the subset of common points. ", "page_idx": 7}, {"type": "text", "text": "From Lemma 13, as $\\bigcap_{p\\in[n]}E_{p}$ is the subset of any simplex defined by a set of points in the confidence sets, $\\bigcap_{p\\in[n]}E_{p}$ must be either empty or bounded subset of $\\mathbf{E}^{n-1}$ . The key implication here is that Lemma $^{l3}$ provides us a method to find a point in the common set. An example of Lemma 13 in $\\mathbf{E}^{2}$ is illustrated in Figure 1. ", "page_idx": 7}, {"type": "text", "text": "Now, the main question is under what conditions $\\bigcap_{p\\in[n]}E_{p}$ is nonempty. Next we show that the nonemptiness of $\\bigcap_{p\\in[n]}E_{p}$ can be guaranteed if the diameter of the confidence sets is sufficiently small. This establishes a condition for the number of samples required by the algorithm. ", "page_idx": 7}, {"type": "text", "text": "Theorem 14. Given a collection of confident set $\\{{\\mathcal{C}}_{p}\\}_{p\\in[n]}$ and let $Q=\\{x^{p}\\}_{p\\in[n]}$ , for any $x^{p}\\in\\mathcal{C}_{p}$ . For any $\\textit{p}\\in[n],$ , denote $H_{p}(Q)$ as the $(n-1)$ -dimensional hyperplane with constant $(v^{p},c^{p})$ , $\\|v^{p}\\|=1$ such that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\langle v^{p},x\\rangle=c^{p}+\\operatorname*{max}_{q\\in[n]\\setminus p}\\mathrm{diam}(\\mathcal{C}_{p}),\\quad\\forall x\\in Q\\setminus x^{p}.\\right.}\\\\ {\\left.\\left\\langle v^{p},x^{p}\\right\\rangle<c^{p}+\\operatorname*{max}_{q\\in[n]\\setminus p}\\mathrm{diam}(\\mathcal{C}_{p}).\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "For all $p\\in[n]$ , if the following holds ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{D}(\\mathcal{C}_{p},H_{p}(Q))>2n\\left(\\operatorname*{max}_{q\\in[n]\\backslash p}\\mathrm{diam}(\\mathcal{C}_{q})\\right);\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "then there exists a common point. In particular, the point $x^{\\star}={\\textstyle\\frac{1}{n}}\\sum_{p\\in[n]}x^{p}$ is a common point. ", "page_idx": 7}, {"type": "text", "text": "Intuitively, Theorem 14 states that if the distance from a confidence set $\\mathcal{C}_{p}$ to the hyperplane $H_{p}(Q)$ is relatively large compared to the sum of the diameters of all other confidence sets, then the average of any collection of points in the confidence set must be a common point. As such, Theorem 14 determines the stopping condition for Algorithm 1 and provide us a explicit way to find a common point, which validates the correctness of Algorithm 1. In particular, Algorithm 2 checks if conditions (14) are satisfied for the confidence sets in each round. If the conditions are satisfied, then Algorithm 1 stops sampling and returns $x^{\\star}$ as the common point. ", "page_idx": 7}, {"type": "text", "text": "Note that while the diameters of confidence sets can be controlled by the number of samples regarding the marginal vector, ${\\mathcal{D}}({\\mathcal{C}}_{p},H_{p}(Q))$ is a random variable and needs to be handled with care. We show that there exist choices of $n$ vertices such that the simplex formed by them has a sufficiently large width, resulting in the stopping condition being satisfied with high probability after poly $\\bar{(n,\\varsigma^{-1})}$ number of samples. ", "page_idx": 7}, {"type": "text", "text": "5.2 Sample Complexity Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Now, we show that, the conditions of Theorem 14 can be satisfied with high probability. The distance ${\\mathcal{D}}({\\mathcal{C}}_{p},H_{p}(Q))$ , $p\\in[n]$ can be lower bounded by the width of the ground-truth simplex, which is defined as follows: ", "page_idx": 8}, {"type": "text", "text": "Definition 15 (Width of simplex). Given $n$ points $\\{x^{1},...x^{n}\\}$ in $\\mathbb{R}^{n}$ , let matrix $\\boldsymbol{P}\\,=\\,[x^{i}]_{i\\in[n]}$ , we define the matrix of coordinates of the points in $P$ w.r.t. $x^{i}$ as $\\operatorname{coM}(P,i):=[(x^{j}-x^{i})]_{j\\neq i}\\,\\bar{\\in}$ $\\mathbb{R}^{n\\times(n-1)}$ . Denote $\\sigma_{k}(M)$ as the $k^{\\mathrm{th}}$ singular value of matrix $M$ (with descending order). We define the width of the simplex whose coordinate matrix is $P$ as follows ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\vartheta(P):=\\operatorname*{min}_{i\\in[n]}\\sigma_{n-1}\\left(\\operatorname{coM}(P,i)\\right).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Lemma 16. Given n points $\\{x^{1},...,x^{n}\\}$ in $\\mathbb{R}^{n}$ , let $M$ be the matrix corresponding to these points, assume that $0<M_{i j}<1$ and $\\vartheta(M)\\geq\\sigma,$ , for some constant $\\sigma>0$ . Let $R\\in\\mathbb{R}^{n\\times n}$ be a perturbation matrix, such that its entries $|R_{i j}|\\,<\\,\\epsilon/2$ , $\\forall(i,j)$ , and $0\\,<\\,\\epsilon\\,<\\,\\sigma^{2}/3n^{3}$ . Let $h_{\\mathrm{min}}$ be a smallest magnitude of the altitude of the simplex corresponding to the matrix $M+R$ . One has that ", "page_idx": 8}, {"type": "equation", "text": "$$\nh_{\\mathrm{min}}\\geq\\sqrt{\\sigma^{2}-6n^{3}\\epsilon}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Lemma 16 guarantees that if the width of the ground truth simplex is relatively large compared to the diameter of the confidence set, then the heights of the estimated simplex are also large. We now provide an example of a collection of permutation orders corresponding to a set of vertices as follows. ", "page_idx": 8}, {"type": "text", "text": "Proposition 17. Fix any $\\omega\\in\\mathfrak{S}_{n}$ , consider the collection of permutation $\\mathcal{P}=\\left\\{\\omega,\\omega s_{1},\\dots,\\omega s_{n-1}\\right\\}$ and matrix $M=[\\phi^{\\omega^{\\prime}}]_{\\omega^{\\prime}\\in\\mathcal{P}}$ . The width of the simplex that corresponds to $M$ , is upper bounded as $\\vartheta(M)\\ge0.5\\varsigma n^{-3/2}$ . ", "page_idx": 8}, {"type": "text", "text": "The vertex set in Proposition 17 comprises one vertex and its $(n-1)$ adjacent vertices. Combining Lemma 16, Proposition 17 with the stopping condition provided by Theorem 14, we now can guarantee the sample complexity of our algorithm: ", "page_idx": 8}, {"type": "text", "text": "Theorem 18. With the choice of collection of permutation order $\\mathcal{P}$ as in Proposition 17, and suppose that Assumption $^{4}$ holds. Then, for any $\\delta\\in[0,1]$ , if the number of samples is bounded by ", "page_idx": 8}, {"type": "equation", "text": "$$\nT=O\\left(\\frac{n^{15}\\log(n\\delta^{-1}\\varsigma^{-1})}{\\varsigma^{4}}\\right),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "the Common-Points-Picking algorithm returns a point in $E$ -Core with probability at least $1-\\delta$ . ", "page_idx": 8}, {"type": "text", "text": "While the choice of vertices in Proposition 17 achieves polynomial sample complexity, the width of the simplex decreases with dimension growth, hindering its sub-optimality. An alternative choice of vertices is those corresponding to cyclic permutation, denoted as $\\mathfrak{C}_{n}\\subset\\mathfrak{S}_{n}$ , which have a larger width in large subsets of strictly convex games (as observed in simulations) but can be difficult to verify in the worst case. We refer readers to Appendix A.4 for the detail simulation and discussion on the choice of set of $n$ vertices. Based on this observation, we achieve the sample complexity which better dependence on $n$ as follows. ", "page_idx": 8}, {"type": "text", "text": "Theorem 19. Suppose Assumption $^{4}$ holds. Let ${\\mathcal{P}}={\\mathfrak{S}}_{n}$ the collection of cyclic permutations, and denote the coordinate matrix of the corresponding vertices as $W$ . Assume that the width of the simplex $\\textstyle{\\mathcal{Y}}(W)\\geq{\\frac{n\\varsigma}{c_{W}}}$ for some $c_{W}>0$ . Then, for any $\\delta\\in[0,1]$ ,if number of samples is ", "page_idx": 8}, {"type": "equation", "text": "$$\nT=O\\left(\\frac{n^{5}c_{W}^{4}\\log(n c_{W}\\delta^{-1}\\varsigma^{-1})}{\\varsigma^{4}}\\right),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "the Common-Points-Picking algorithm returns a point in $E$ -Core with probability at least $1-\\delta$ . ", "page_idx": 8}, {"type": "text", "text": "It is worth noting that our algorithm does not require information about the constants of the game $\\varsigma,~c_{W}$ ; instead, the number of samples required automatically scales with these constants. This indicates that our algorithm is highly adaptive and requires fewer samples for benign game instances. ", "page_idx": 8}, {"type": "text", "text": "Remark 20 (Comment on sample complexity lower bounds). Deriving a lower bound is indeed important, but comes with several significant challenges. E.g., one possible direction is to extend the game instances in Theorem 7. However, there are two key technical issues with this idea: (1) Modifying the face game instance to ensure strict convexity is challenging; (2) It remains unclear how to generalize two face-game instances into $\\mathrm{poly}(n)$ game instances such that their cores do not intersect and the statistical distance of the reward can be upper bounded, which is crucial for showing poly $(n)$ dependencies in the lower bound (we refer the reader to appendix E.2 for further discussions). Given the unresolved challenges, deriving a lower bound remains an open question. ", "page_idx": 9}, {"type": "text", "text": "6 Experiment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "To illustrate the sample complexity of our algorithm in practice and compare it with our theoretical upper bound, we have conducted a simulation as described below. Code is available at: https: //github.com/NamTranKekL/ConstantStrictlyConvexGame.git. ", "page_idx": 9}, {"type": "text", "text": "Simulation setting: We generate convex game of $n$ players with the expected reward function $f$ defined recursively as follows: For each $S\\subset N$ s.t. $i\\not\\in S$ , ", "page_idx": 9}, {"type": "equation", "text": "$$\nf(S\\cup\\{i\\})=f(S)+|S|+1+0.9\\omega.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "for some $\\omega$ sampled i.i.d. from the uniform distribution $\\mathrm{Unif}([0,1])$ . We then normalize the value of the reward function within the range $[0,1]$ . It is straightforward to verify that the strict convexity constant is $\\varsigma\\approx0.1/n$ . From the simulation results in Figure 2 (LHS), we can see that the growth pattern nearly matches that of the theoretical bound given in Theorem 19, indicating that our theoretical bound is highly informative. ", "page_idx": 9}, {"type": "text", "text": "Moreover, to demonstrate that our algorithm is robust even when the strict convexity assumption is violated, we ran a simulation where the characteristic function is only convex, i.e., the strict convexity constant is arbitrarily small, as follows: ", "page_idx": 9}, {"type": "equation", "text": "$$\nf(S\\cup\\{i\\})=f(S)+|S|+1+\\omega.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "We use the cyclic permutations ${\\mathfrak{C}}_{n}$ as the input for the algorithm. In Figure 2 (RHS), one can see that the number of samples required as $n$ grows is sub-exponential, indicating that our algorithm is robust when the strict convexity assumption is violated. ", "page_idx": 9}, {"type": "image", "img_path": "ZRYFftR4xn/tmp/71de9f4dab6b0a6828604a465e405b7e6d26131c626d235f7c7fb3b2b6b6d74a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "", "img_caption": ["Figure 2: Simulation with game of $n\\in\\{2,...,10\\}$ players, where the strict convexity constant $\\varsigma$ is $0.1/n$ in the LHS and 0 in the RHS. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "7 Conclusion and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we address the challenge of learning the expected core of a strictly convex stochastic cooperative game. Under the assumptions of strict convexity and a large interior of the core, we introduce an algorithm named Common-Points-Picking to learn the expected core. Our algorithm guarantees termination after poly $\\left(n,\\log(\\delta^{-1}),\\varsigma^{-1}\\right)$ samples and returns a point in the expected core with probability $(1-\\delta)$ . For future work, we will investigate whether the sample complexity of our algorithm can be further improved by incorporating adaptive sampling techniques into the algorithm, along with developing a lower bound for the class of games. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] David Aadland and Van Kolpin. Shared irrigation costs: an empirical and axiomatic analysis. Mathematical Social Sciences, 35(2):203\u2013218, 1998. [2] Stefan Ambec and Lars Ehlers. Sharing a river among satiable agents. Games and Economic Behavior, 64(1):35\u201350, 2008.   \n[3] Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.   \n[4] Javier Castro, Daniel G\u00f3mez, and Juan Tejada. Polynomial calculation of the shapley value based on sampling. Computers and Operations Research, 36, 2009. ISSN 03050548. doi: 10.1016/j.cor.2008.04.004.   \n[5] Georgios Chalkiadakis and Craig Boutilier. Bayesian reinforcement learning for coalition formation under uncertainty. Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS 2004, 3, 2004. [6] Georgios Chalkiadakis and Craig Boutilier. Sequentially optimal repeated coalition formation under uncertainty. Autonomous Agents and Multi-Agent Systems, 24, 2012. ISSN 13872532. doi: 10.1007/s10458-010-9157-y. [7] Georgios Chalkiadakis, Evangelos Markakis, and Craig Boutilier. Coalition formation under uncertainty: Bargaining equilibria and the bayesian core stability concept. Proceedings of the International Conference on Autonomous Agents, 2007. doi: 10.1145/1329125.1329203. [8] Georgios Chalkiadakis, Edith Elkind, and Michael Wooldridge. Computational aspects of cooperative game theory. Synthesis Lectures on Artificial Intelligence and Machine Learning, 16, 2011. ISSN 19394608. doi: 10.2200/S00355ED1V01Y201107AIM016. [9] A. Charnes and Daniel Granot. Coalitional and chance-constrained solutions to n -person games, ii: Two-stage solutions. Operations Research, 25, 1977. ISSN 0030-364X. doi: 10.1287/opre.25.6.1013.   \n[10] Xin Chen and Jiawei Zhang. A stochastic programming duality approach to inventory centralization games. Operations Research, 57, 2009. ISSN 0030364X. doi: 10.1287/opre.1090.0699.   \n[11] Xuan Vinh Doan and Tri Dung Nguyen. Robust stable payoff distribution in stochastic cooperative games. Operations Research, 2014.   \n[12] Jakob N. Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Counterfactual multi-agent policy gradients. In 32nd AAAI Conference on Artificial Intelligence, AAAI 2018, 2018.   \n[13] Ian Gemp, Marc Lanctot, Luke Marris, Yiran Mao, Edgar Du\u00e9\u00f1ez Guzm\u00e1n, Sarah Perrin, Andras Gyorgy, Romuald Elie, Georgios Piliouras, Michael Kaisers, Daniel Hennes, Kalesha Bullard, Kate Larson, and Yoram Bachrach. Approximating the core via iterative coalition sampling. In Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems, 2024.   \n[14] Julio Gonz\u00e1lez-D\u00edaz and Estela S\u00e1nchez-Rodr\u00edguez. Cores of convex and strictly convex games. Games and Economic Behavior, 62, 2008. ISSN 08998256. doi: 10.1016/j.geb.2007.03.003.   \n[15] Ilse C.F. Ipsen and Rizwana Rehman. Perturbation bounds for determinants and characteristic polynomials. SIAM Journal on Matrix Analysis and Applications, 30, 2008. ISSN 08954798. doi: 10.1137/070704770.   \n[16] Robert Kleinberg, Aleksandrs Slivkins, and Eli Upfal. Bandits and experts in metric spaces. Journal of the ACM, 66, 2019. ISSN 1557735X. doi: 10.1145/3299873.   \n[17] Scott M. Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In Neural Information Processing Systems, 2017.   \n[18] Sasan Maleki, Long Tran-Thanh, Greg Hines, Talal Rahwan, and Alex Rogers. Bounding the estimation error of sampling-based shapley value approximation. arXiv preprint arXiv:1306.4265, 2013.   \n[19] George Pantazis, Filippo Fabiani, Filiberto Fele, and Kostas Margellos. Probabilistically robust stabilizing allocations in uncertain coalitional games. arXiv preprint arXiv:2203.11322, 3 2022.   \n[20] George Pantazis, Barbara Franci, Sergio Grammatico, and Kostas Margellos. Distributionally robust stability of payoff allocations in stochastic coalitional games. arXiv preprint arXiv:2304.01786, 2023.   \n[21] Alexander Postnikov. Permutohedra, associahedra, and beyond. arXiv preprint arXiv:math/0507163, 7 2005.   \n[22] Aitazaz Ali Raja and Sergio Grammatico. Payoff distribution in robust coalitional games on time-varying networks. IEEE Transactions on Control of Network Systems, 9 2020.   \n[23] Lloyd S. Shapley. Cores of convex games. International Journal of Game Theory, 1, 1971. ISSN 00207276. doi: 10.1007/BF01753431.   \n[24] Grah Simon and Thouvenot Vincent. A projected stochastic gradient algorithm for estimating shapley value applied in attribute importance. In Lecture Notes in Computer Science, volume 12279 LNCS, 2020. doi: 10.1007/978-3-030-57321-8_6.   \n[25] Milan Studen\u00fd and Tom\u00e1\u0161 Kroupa. Core-based criterion for extreme supermodular functions. Discrete Applied Mathematics, 206, 2016.   \n[26] Jeroen Suijs and Peter Borm. Stochastic cooperative games: superadditivity, convexity, and certainty equivalents. Games and Economic Behavior, 27, 1999. ISSN 08998256. doi: 10.1006/game.1998.0672.   \n[27] Jeroen Suijs, Peter Borm, Anja De Waegenaere, and Stef Tijs. Cooperative games with stochastic payoffs. European Journal of Operational Research, 113, 1999. ISSN 03772217. doi: 10.1016/S0377-2217(97)00421-9.   \n[28] Mukund Sundararajan and Amir Najmi. The many shapley values for model explanation. In Proceedings of the 37th International Conference on Machine Learning, 2020.   \n[29] Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z. Leibo, Karl Tuyls, and Thore Graepel. Value-decomposition networks for cooperative multi-agent learning. arXiv preprint arXiv:1706.05296, 6 2017.   \n[30] Jianhong Wang, Yuan Zhang, Yunjie Gu, and Tae-Kyun Kim. Shaq: Incorporating shapley value theory into multi-agent q-learning. Advances in Neural Information Processing Systems, 5 2022.   \n[31] Tom Yan and Ariel D. Procaccia. If you like shapley then you\u2019ll love the core. In AAAI Conference on Artificial Intelligence, 2021.   \n[32] Miguel \u00c1ngel Mir\u00e1s Calvo, Carmen Quinteiro Sandomingo, and Estela S\u00e1nchez Rodr\u00edguez. The boundary of the core of a balanced game: face games. International Journal of Game Theory, 49, 2020. ISSN 14321270. doi: 10.1007/s00182-019-00703-2. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Contents of Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Gane   \nA.1 Proof of Theorem 7 13   \nA.2 E-Core of convex games and Generalised Permutahedra 15   \nA.3 Proof of Proposition 17 . 16   \nA.4 Alternative choice of $n$ vertices of E-Core 17 ", "page_idx": 12}, {"type": "text", "text": "B On the Stopping Condition 20 ", "page_idx": 12}, {"type": "text", "text": "C Sample Complexity Analysis ", "page_idx": 12}, {"type": "text", "text": "D Examples of Strictly Convex Games ", "page_idx": 12}, {"type": "text", "text": "E Further Discussions 27   \nE.1 Comparison with Pantazis et al. [20] 27   \nE.2 Comment on Lower Bounds 28 ", "page_idx": 12}, {"type": "text", "text": "A Preliminary and Convex Game ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Proof of Theorem 7 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Here and onwards, we adopt the following notation convention: for real numbers $a,b\\,\\in\\,[0,1]$ , KL $(a,b)$ represents the KL-divergence KL $(p,q)$ where $p,q$ are probability distributions on $\\bar{\\{0,1\\}}$ such that $p(\\bar{1})=a,\\;q(1)=b$ . In other words, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{KL}\\left(a,b\\right)=a\\ln\\left(\\frac{a}{b}\\right)+\\left(1-a\\right)\\ln\\left(\\frac{1-a}{1-b}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Lemma 21 ([16]). For any $0<\\varepsilon<y\\leq1,\\,\\mathrm{KL}\\,(y-\\varepsilon,y)<\\varepsilon^{2}/y(1-y).$ ", "page_idx": 12}, {"type": "text", "text": "Before stating the proof of Theorem 7, let us introduce some extra notations. Given a game $G=$ $(N,\\mathbb{P})$ , with the expected reward function $\\mu$ , we define the following. ", "page_idx": 12}, {"type": "text", "text": "\u2022 $H_{C}(G):=\\{x\\in\\mathbb{R}^{n}\\mid x(C)=\\mu(C)\\}$ is the hyperplane corresponding to the effective allocation w.r.t coalition $C$ .   \n\u2022 E-Core $(G)$ is the expected core of the game $G$ .   \n\u2022 $F_{C}(G):=\\operatorname{E-Core}(G)\\cap H_{N\\setminus C}(G)$ is facet of the E-Core corresponding to the coalition $C$ . ", "page_idx": 12}, {"type": "text", "text": "We use the following definition of the face games in Theorem 7, introduced by [14]. ", "page_idx": 12}, {"type": "text", "text": "Definition 22 (Face Game). Given a game $G=(N,\\mathbb{P})$ with $\\mu(S)=\\mathbb{E}_{r\\sim\\mathbb{P}_{S}}[r]$ , $\\forall S\\subset N$ . For any $C\\subset N$ , define a face game $G(C)=(\\mathbf{\\bar{\\boldsymbol{N}}},\\mathbb{P}^{C})$ with $\\mu_{F_{C}}(S)=\\mathbb{E}_{r\\sim\\mathbb{P}_{S}^{C}}[r]$ such that, for any $S\\subset N$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mu_{F_{C}}(S)=\\mu((S\\cap C)\\cup(N\\setminus C))-\\mu(N\\setminus C)+\\mu(S\\cap(N\\setminus C)).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "[14] showed that the expected core of $G(C)$ is exactly the facet of $\\operatorname{E-Core}(G)$ corresponding $C$ , that is, $\\operatorname{E-Core}(G(C))=F_{C}(G)$ . As noted in [32], one can decompose the reward function of the face game as follows. For any $S\\subset N$ , we have that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mu_{F_{C}}(S)=\\mu_{F_{C}}(S\\cap C)+\\mu_{F_{C}}(S\\cap(N\\setminus C)).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We now proceed the proof of Theorem 7. ", "page_idx": 12}, {"type": "text", "text": "Proof of Theorem 7 . Denote the set convex games with Bernolli reward as GB, that is, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{G}\\mathbf{B}=\\{G=(N,\\mathbb{P})~|~\\mathbb{P}=\\{\\mathbb{P}_{S}\\}_{S\\subseteq N};~\\mathbb{P}_{S}\\in\\mathcal{M}(\\{0,1\\}),~\\forall S\\subseteq N\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Face-game instances and the distance between their E-Core. We first define two games, $G_{0}$ and $G_{1}$ , with a full-dimensional E-Core, such that $G_{1}$ is a slight perturbation of $G_{0}$ . Next, we define face games corresponding to $G_{0}$ and $G_{1}$ using the perturbed facet. We then show that the distance between the cores of these two face games is at least some positive number $\\varepsilon>0$ . ", "page_idx": 13}, {"type": "text", "text": "Define a strictly convex game $G_{0}:=(N,\\mathbb{P})\\in{\\bf G}{\\bf B}$ , such that $\\mu^{0}(S)\\,:=\\,\\mathbb{E}_{r\\sim\\mathbb{P}_{S}}[r]$ , and assume that $\\mu^{0}$ is $\\varsigma$ -strictly supermodular. Now, fix a subset $C\\subset N$ , let define a perturbed game instance $G_{1}:=(N,\\mathbb{Q})\\in{\\bf G}{\\bf B}$ , with $\\mu^{1}(S):=\\mathbb{E}_{r\\sim\\mathbb{Q}_{S}}[r]$ such that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\mu^{1}(C):=\\mu^{0}(C)-\\varepsilon;}\\\\ {\\mu^{1}(S):=\\mu^{0}(S);\\qquad\\forall S\\subset N,\\ S\\neq C;}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "for some $0<\\varepsilon<\\varsigma$ . It is straightforward that $G_{1}$ is $\\left(\\varsigma-\\varepsilon\\right)$ -strictly convex. Therefore, $\\mathtt{E-C o r e}(G_{0})$ and $\\mathtt{E-C o r e}(G_{1})$ are both full-dimensional. ", "page_idx": 13}, {"type": "text", "text": "Fixing a coalition $C\\subset N$ , we now construct the face games from $G_{0},\\ G_{1}$ as in Definition 22. Let $\\bar{G}_{0}(C):=(N,\\mathbb{P}^{C})$ , $G_{1}(C):=(N,\\mathbb{Q}^{C})\\in{\\bf G}{\\bf B}$ , whose expected rewards $\\mu_{F_{C}}^{0}$ and $\\mu_{F_{C}}^{1}$ are defined by applying (19) to $\\mu^{0}$ and $\\mu^{1}$ respectively. Now, we consider the difference between the expected reward function of these two games. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\mu_{F_{C}}^{1}(S)-\\mu_{F_{C}}^{0}(S)|=0}&{\\forall S\\subset N\\setminus C}\\\\ {|\\mu_{F_{C}}^{1}(S)-\\mu_{F_{C}}^{0}(S)|=\\varepsilon}&{\\forall S\\subseteq C}\\\\ {|\\mu_{F_{C}}^{1}(N\\setminus C)-\\mu_{F_{C}}^{0}(N\\setminus C)|=\\varepsilon.}&\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "As one can always decompose the set $S=(S\\cap C)\\cup(S\\cap N\\setminus C)$ , by the decomposibility of the face game (20), we has that ", "page_idx": 13}, {"type": "equation", "text": "$$\n|\\mu_{F_{C}}^{1}(S)-\\mu_{F_{C}}^{0}(S)|\\leq\\varepsilon,\\;\\forall S\\subset N.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "As the core of face game $G_{0}(C)$ and $G_{1}(C)$ lie on the hyperplane corresponding to the coalition $N\\setminus C$ , and the distance between the hyperplanes of $G_{0}$ and $G_{1}$ is $\\varepsilon$ , which lower bounds the distance between the expected core of $\\bar{G_{0}}(\\bar{C})$ and $G_{1}(C)$ . In particular, as $\\operatorname{E-Core}(G_{0}(C))\\,=$ $F_{C}(G_{0})$ and $\\operatorname{E-Core}(G_{1}(C))\\;=\\;F_{C}(G_{1})$ , and $|\\mu^{1}(N\\setminus\\bar{C})-\\bar{\\mu^{0}}(N\\setminus C)|\\,=\\,\\varepsilon$ , which leads to $\\begin{array}{r}{\\mathcal{D}(H_{N\\backslash C}(G_{0}),H_{N\\backslash C}(G_{1}))=\\varepsilon}\\end{array}$ , we have that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{D}\\left(\\mathrm{E-Core}(G_{0}(C)),\\mathrm{E-Core}(G_{1}(C))\\right)\\geq\\varepsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The KL distance and imposibility of learning low-dimensional E-Core. We show that, with probability $\\delta\\,\\in\\,(0,0.2)$ , any learner cannot distinguish between $G_{0}(C)$ and $G_{1}(C)$ given there are finite number of samples. We use the information-theoretic framework similar which is well developed within multi-armed bandit literature. ", "page_idx": 13}, {"type": "text", "text": "We first upper bound the KL-distance between $\\mathbb{P}_{S}^{C},\\mathbb{Q}_{S}^{C}$ , $\\forall S\\quad\\subset\\quad N$ . Denote $c_{1}\\quad:=$ $\\mathrm{min}_{S\\subset N}\\left(\\mu_{F_{C}}^{0}(S)(1-\\mu_{F_{C}}^{0}(S))\\right)>0$ , by Lemma 21, we have that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{KL}\\left(\\mathbb{P}_{S}^{C},\\mathbb{Q}_{S}^{C}\\right)=\\mathrm{KL}\\left(\\mu_{F_{C}}^{0}(S),\\mu_{F_{C}}^{1}(S)\\right)\\leq\\frac{\\varepsilon^{2}}{c_{1}},\\quad\\forall S\\subset N.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Define the probability space $\\Psi\\,=\\,2^{N}\\,\\times\\,\\{0,1\\}$ . Fix any algorithm (possibly randomised) $\\boldsymbol{\\mathcal{A}}$ . At round $t$ , denote $(S_{t},r_{t})\\in\\Psi$ as the coalition selected by the algorithm and the reward return by the environment. At round $s<t$ , denote $\\nu_{0}^{t},\\ \\nu_{1}^{t}$ as the probability distribution over $\\Psi^{t}$ determined by $\\boldsymbol{\\mathcal{A}}$ and $\\mathbb{P},\\mathbb{Q}$ accordingly. ", "page_idx": 13}, {"type": "text", "text": "We have the following, as stated in the appendix of [16]. For any $u<t$ , one has that, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}\\left(\\nu_{0}^{u},\\nu_{1}^{u}\\right)=\\underset{\\psi^{u-1}\\in\\Psi^{u-1}}{\\sum}\\nu_{0}^{u}(\\psi^{u})\\log\\left(\\frac{\\nu_{0}^{u}\\left(\\psi^{u}\\;\\vert\\;\\psi^{u-1}\\right)}{\\nu_{1}^{u}\\left(\\psi^{u}\\;\\vert\\;\\psi^{u-1}\\right)}\\right)}\\\\ &{\\qquad\\qquad=\\underset{\\psi^{u-1}\\in\\Psi^{u-1}}{\\sum}\\nu_{0}^{u}(\\psi^{u})\\log\\left(\\frac{\\nu_{0}^{u}\\left(S_{u}\\;\\vert\\;\\psi^{u-1}\\right)}{\\nu_{1}^{u}\\left(S_{u}\\;\\vert\\;\\psi^{u-1}\\right)}\\cdot\\frac{\\nu_{0}^{u}\\left(r_{u}\\;\\vert\\;S_{u},\\psi^{u-1}\\right)}{\\nu_{1}^{u}\\left(r_{u}\\;\\vert\\;S_{u},\\psi^{u-1}\\right)}\\right)}\\\\ &{\\qquad\\qquad=\\underset{\\psi^{u-1}\\in\\Psi^{u-1}}{\\sum}\\nu_{0}^{u}(\\psi^{u})\\log\\left(\\frac{\\nu_{0}^{u}\\left(r_{u}\\;\\vert\\;S_{u},\\psi^{u-1}\\right)}{\\nu_{1}^{u}\\left(r_{u}\\;\\vert\\;S_{u},\\psi^{u-1}\\right)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "[As the distribution of $S_{u}$ depends only on $\\boldsymbol{\\mathcal{A}}$ , not on the distribution $\\nu_{0}^{t},\\ \\nu_{1}^{t}.$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\displaystyle\\sum_{\\psi^{u-1}\\in\\Psi^{u-1}}\\displaystyle\\sum_{S_{u}\\in2^{N}}\\displaystyle\\sum_{r_{u}\\in\\{0,1\\}}\\nu_{0}^{u}(r_{u}\\mid S_{u},\\psi^{u-1})\\log\\left(\\frac{\\nu_{0}^{u}(r_{u}\\mid S_{u},\\psi^{u-1})}{\\nu_{1}^{u}(r_{u}\\mid S_{u},\\psi^{u-1})}\\right)\\nu_{0}^{u}(S_{u},\\psi^{u-1})}\\\\ &{=\\displaystyle\\sum_{\\psi^{u-1}\\in\\Psi^{u-1}}\\displaystyle\\sum_{S_{u}\\in2^{N}}\\mathrm{KL}\\left(\\mu_{F_{C}}^{0}(S_{u}),\\mu_{F_{C}}^{1}(S_{u})\\right)\\nu_{0}^{u}(S_{u},\\psi^{u-1})}\\\\ &{\\leq\\displaystyle\\frac{\\varepsilon^{2}}{c_{1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The last inequality hold because $\\begin{array}{r}{\\mathrm{KL}\\left(\\mu_{F_{C}}^{0}(S),\\mu_{F_{C}}^{1}(S)\\right)\\leq\\frac{\\varepsilon^{2}}{c_{1}},\\;\\forall S\\in2^{N}.}\\end{array}$ ", "page_idx": 14}, {"type": "text", "text": "We have that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{KL}\\left(\\nu_{0}^{t},\\nu_{1}^{t}\\right)=\\sum_{u=1}^{t}\\mathrm{KL}\\left(\\nu_{0}^{u},\\nu_{1}^{u}\\right)\\leq\\frac{t\\varepsilon^{2}}{c_{1}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "As we can choose $\\varepsilon$ to be arbitrarily small, we can choose $\\varepsilon$ such that $\\mathrm{KL}\\left(\\nu_{0}^{t},\\nu_{1}^{t}\\right)\\leq0.1$ . ", "page_idx": 14}, {"type": "text", "text": "Now, define the event $\\mathcal{E}$ as the event that $\\boldsymbol{\\mathcal{A}}$ outputs a point in $\\operatorname{E-Core}(G_{0}(C))$ , assume that $\\nu_{0}^{t}(\\mathcal{E})$ with probability at least 0.8. Note that, as E $-\\mathrm{Core}(G_{0}(C))\\cap\\mathrm{E-Core}(G_{1}(C))=\\varnothing$ , $\\mathcal{E}$ represents the event where the algorithm fails to output a stable allocation with the game instance $G_{1}(\\bar{C})$ . We have that from [16]\u2019s Lemma A.5, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\nu_{1}^{t}(\\mathcal{E})\\geq\\nu_{0}^{t}(\\mathcal{E})\\exp{\\left(-\\frac{\\mathrm{KL}\\left(\\nu_{0}^{t},\\nu_{1}^{t}\\right)+1/e}{\\nu_{0}^{t}(\\mathcal{E})}\\right)}>0.8\\exp{\\left(-\\frac{0.1+1/e}{0.8}\\right)}>0.3.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "As it holds for any $t>0$ , this means that for any finite number of samples, with probability at least 0.1, the algorithm will output the incorrect point. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Remark 23. Upon closely examining the face game instances in the proof of Theorem 7, we can see that even if the E-Core is full-dimensional, but the width of the interior of E-Core is arbitrarily small, it is still not possible to learn a point in E-Core with high probability and finite samples. To see this, let us create two perturbed game instances of the face game instances such that their E-Core are full-dimensional but have an arbitrarily narrow interior. The construction can be done by applying modification of equation (19) with an arbitrarily small constant $\\zeta\\,>\\,0$ on the game $G_{0},\\ G_{1}$ , as follows. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mu_{F c}\\left(S\\right)=\\mu(\\left(S\\cap C\\right)\\cup\\left(N\\setminus C\\right))-\\mu(N\\setminus C)+\\mu(S\\cap\\left(N\\setminus C\\right))+\\zeta.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Now, as long as $\\zeta<\\varepsilon/2$ , meaning that the width of the interior of their E-Core is less than half of the distance between the original face games, the distance between their E-Core remains positive. Therefore, as the KL distance between the reward distributions is arbitrarily small but the two games do not share any common stable allocation, no algorithm can output a stable allocation of the ground truth game with high probability and finite samples. ", "page_idx": 14}, {"type": "text", "text": "A.2 E-Core of convex games and Generalised Permutahedra ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Formulating the coordinates of the vertices of the core can be achieved using the connection between the core of a convex game and the generalised permutahedron. There is an equivalence between generalised permutahedra and polymatroids; it was also shown in [25] that the core of each convex game is a generalised permutahedron. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "For any $\\begin{array}{r l r}{\\omega}&{{}\\in}&{\\mathfrak{S}_{n}}\\end{array}$ , let $\\begin{array}{r l r}{{\\bf I}^{\\omega}}&{{}=}&{(\\omega(1),...,\\omega(n))}\\end{array}$ . The $n$ -permutahedron is defined as Conv $\\left\\langle\\left\\{\\mathbf{I}^{\\omega}\\mid\\omega\\in\\mathfrak{S}_{n}\\right\\}\\right\\rangle$ ). A generalised permutahedron can be defined as a deformation of the permutahedron, that is, a polytope obtained by moving the vertices of the usual permutohedron so that the directions of all edges are preserved [21]. Formally, the edge of the core corresponding to adjacent vertices $\\phi^{\\omega}$ , $\\phi^{\\omega s_{i}}$ can be written as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\phi^{\\omega}-\\phi^{\\omega s_{i}}=k_{\\omega,i}(e_{\\omega(i)}-e_{\\omega(i+1)}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Where, $k_{\\omega,i}\\geq0$ , and $e_{1},\\ldots e_{n}$ are the coordinate vectors in $\\mathbb{R}^{n}$ . If the game is $\\varsigma$ -strictly convex, $k_{\\omega,i}>\\varsigma$ . ", "page_idx": 15}, {"type": "text", "text": "A.3 Proof of Proposition 17 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We utilise the formulation of edges of the generalized permutahedron as described in Subsection A.2 to calculate the matrix of coordinates for the vertices of E-Core. Based on the matrix of coordinates, we now state the proof of Proposition 17. ", "page_idx": 15}, {"type": "text", "text": "Proof of Proposition 17. As the set of vertices is $\\phi^{\\omega}$ and its $n-1$ neighbors, there are only two cases to consider. First, we need to consider the matrix created by using $\\phi_{\\omega}$ as the reference, that is $\\mathrm{coM}(M,1)$ . As the neighbors have the same roles, bounding the width of the matrices using any neighbor as a reference point can be done identically. Therefore, we will prove the theorem for $\\mathrm{coM}(M,2)$ , and the proof for $\\operatorname{coM}(M,i),\\ i\\neq1$ can be done in the same manner. Let us denote ", "page_idx": 15}, {"type": "equation", "text": "$$\nV=\\operatorname{coM}(M,1)={\\left[\\begin{array}{l l l l l l}{c_{1}}&{0}&{0}&{\\cdots}&{0}&{0}\\\\ {-c_{1}}&{c_{2}}&{0}&{\\cdots}&{0}&{0}\\\\ {0}&{-c_{2}}&{c_{3}}&{\\cdots}&{0}&{0}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}&{\\vdots}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}&{\\vdots}\\\\ {0}&{0}&{0}&{\\cdots}&{-c_{n-2}}&{c_{n-1}}\\\\ {0}&{0}&{0}&{\\cdots}&{0}&{-c_{n-1}}\\end{array}\\right]}\\in\\mathbb{R}^{n\\times(n-1)},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\nU=\\operatorname{coM}(M,2)=\\left[\\begin{array}{c c c c c c c c}{-c_{1}}&{-c_{1}}&{-c_{1}}&{-c_{1}}&{\\cdots}&{-c_{1}}&{-c_{1}}\\\\ {c_{1}}&{c_{1}+c_{2}}&{c_{1}}&{c_{1}}&{\\cdots}&{c_{1}}&{c_{1}}\\\\ {0}&{-c_{2}}&{c_{3}}&{0}&{\\cdots}&{0}&{0}\\\\ {0}&{0}&{-c_{3}}&{c_{4}}&{\\cdots}&{0}&{0}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}&{\\vdots}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}&{\\vdots}\\\\ {0}&{0}&{0}&{0}&{\\cdots}&{-c_{n-2}}&{c_{n-1}}\\\\ {0}&{0}&{0}&{0}&{\\cdots}&{0}&{-c_{n-1}}\\end{array}\\right]\\in\\mathbb{R}^{n\\times(n-1)},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "in which each $c_{i}>\\varsigma$ . ", "page_idx": 15}, {"type": "text", "text": "We will exploit the following norm inequality in the proof. For any $A_{1},\\ldots,A_{n}\\in\\mathbb{R}$ , we use the following inequality $\\operatorname{norm}2$ vs. norm 1 of vectors) ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}A_{i}^{2}\\geq\\frac{(\\sum_{i=1}^{n}A_{i})^{2}}{n}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Consider V. Consider a unit vector $x=(x_{1},...,x_{n-1})$ . We have ", "page_idx": 15}, {"type": "equation", "text": "$$\nV x=\\left[\\begin{array}{c}{{c_{1}x_{1}}}\\\\ {{-c_{1}x_{1}+c_{2}x_{2}}}\\\\ {{-c_{2}x_{2}+c_{3}x_{3}}}\\\\ {{\\cdot\\cdot\\cdot}}\\\\ {{-c_{n-2}x_{n-2}+c_{n-1}x_{n-1}}}\\\\ {{-c_{n-1}x_{n-1}}}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Applying the Ineq. (31) for $A_{1}=c_{1}x_{1}$ , $A_{2}=-c_{1}x_{1}+c_{2}x_{2}$ , $A_{n-1}=-c_{n-2}x_{n-2}+c_{n-1}x_{n-1}$ , $A_{n}=-c_{n-1}x_{n-1}$ gives ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|V x\\|^{2}\\ge\\displaystyle\\frac{c_{1}^{2}x_{1}^{2}}{n}\\ge\\displaystyle\\frac{\\varsigma^{2}x_{1}^{2}}{n};}\\\\ &{\\|V x\\|^{2}\\ge c_{1}^{2}x_{1}^{2}+(-c_{1}x_{1}+c_{2}x_{2})^{2}\\ge\\displaystyle\\frac{c_{2}^{2}x_{2}^{2}}{n}\\ge\\displaystyle\\frac{\\varsigma^{2}x_{2}^{2}}{n};}\\\\ &{\\quad\\quad\\cdot\\cdot}\\\\ &{\\|V x\\|^{2}\\ge\\displaystyle\\frac{\\varsigma^{2}x_{n-1}^{2}}{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, ", "page_idx": 16}, {"type": "equation", "text": "$$\nn\\|V x\\|^{2}\\geq{\\frac{\\varsigma^{2}(x_{1}^{2}+\\cdot\\cdot\\cdot+x_{n-1}^{2})}{n}}={\\frac{\\varsigma^{2}}{n}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore $\\|V x\\|\\geq\\varsigma/n$ , hence $\\sigma_{n-1}(V)\\geq\\varsigma/n$ . ", "page_idx": 16}, {"type": "text", "text": "Consider U. Similarly, consider a unit vector $x=(x_{1},...,x_{n-1})$ . We have ", "page_idx": 16}, {"type": "equation", "text": "$$\nU x=\\left[\\begin{array}{c}{-c_{1}\\big(x_{1}+x_{2}+...+x_{n-1}\\big)}\\\\ {c_{1}\\big(x_{1}+x_{2}+...+x_{n-1}\\big)+c_{2}x_{2}}\\\\ {-c_{2}x_{2}+c_{3}x_{3}}\\\\ {-c_{3}x_{3}+c_{4}x_{4}}\\\\ {.\\cdot\\cdot}\\\\ {-c_{n-2}x_{n-2}+c_{n-1}x_{n-1}}\\\\ {-c_{n-1}x_{n-1}}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Applying the Ineq. (31) for $A_{1}=c_{1}(x_{1}+x_{2}+...+x_{n-1}),A_{2}=c_{1}(x_{1}+x_{2}+...+x_{n-1})+c_{2}x_{2},A_{3}=$ $-c_{2}x_{2}+c_{3}x_{3}$ , $A_{4}=-c_{3}x_{3}+c_{4}x_{4}$ , . . . , $A_{n-1}=-c_{n-2}x_{n-2}+c_{n-1}x_{n-1},$ , An = \u2212cn\u22121xn\u22121 gives ", "page_idx": 16}, {"type": "text", "text": "Note that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|U x\\|^{2}\\ge\\frac{\\varsigma^{2}\\left(x_{1}+x_{2}+\\ldots+x_{n-1}\\right)^{2}}{n};}\\\\ &{\\|U x\\|^{2}\\ge c_{1}^{2}(x_{1}+x_{2}+\\ldots+x_{n-1})^{2}+\\left(c_{1}(x_{1}+x_{2}+\\ldots+x_{n-1})+c_{2}x_{2}\\right)^{2}\\ge\\frac{c_{2}^{2}x_{2}^{2}}{n}\\ge\\frac{\\varsigma^{2}x_{2}^{2}}{n};}\\\\ &{\\|U x\\|^{2}\\ge c_{1}^{2}(x_{1}+x_{2}+\\ldots+x_{n-1})^{2}+\\left(c_{1}(x_{1}+x_{2}+\\ldots+x_{n-1})+c_{2}x_{2}\\right)^{2}+\\left(-c_{2}x_{2}+c_{3}x_{3}\\right)^{2}}\\\\ &{\\qquad\\cdots}\\\\ &{\\|U x\\|^{2}\\ge\\frac{\\varsigma^{2}x_{n-1}^{2}}{n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, we also have ", "page_idx": 16}, {"type": "equation", "text": "$$\nn\\|U x\\|^{2}\\ge\\frac{\\varsigma^{2}((x_{1}+x_{2}+...+x_{n-1})^{2}+x_{2}^{2}+...+x_{n-1}^{2})}{n}\\ge\\frac{\\varsigma^{2}x_{1}^{2}}{n^{2}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "From that, we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n2n\\|U x\\|^{2}\\ge\\varsigma^{2}\\frac{x_{1}^{2}}{n^{2}}+\\frac{x_{2}^{2}}{n}+\\cdot\\cdot\\cdot+\\frac{x_{n-1}^{2}}{n}\\ge\\frac{x_{1}^{2}+\\ldots+x_{n-1}^{2}}{n^{2}}=\\frac{\\varsigma^{2}}{n^{2}},\\;\\mathrm{{as}}\\;\\|x\\|=1\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "That is, $\\begin{array}{r}{\\|U x\\|\\geq\\frac{\\varsigma^{2}}{\\sqrt{2n^{3}}}}\\end{array}$ . Therefore, $\\begin{array}{r}{\\sigma_{n-1}(U)\\geq\\frac{\\varsigma^{2}}{\\sqrt{2n^{3}}}}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "Therefore, we have that $\\textstyle\\vartheta(M)>{\\frac{\\varsigma^{2}}{\\sqrt{2n^{3}}}}$ ", "page_idx": 16}, {"type": "text", "text": "A.4 Alternative choice of $n$ vertices of E-Core ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this subsection, we provide an alternative choice of vertices rather than that in Proposition 17. Recall that, with the choice of vertices in Proposition 17, the lower bound for the width of the simplex diminishes when the dimension increases. This leads to a large dependence of the sample complexity on $n$ . To mitigate this, we investigate other choices of $n$ vertices. To see this, we first recall the equivalence between E-Core and generalized permutahedra as explained in Subsection A.2. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "However, even in the case of a simple permutahedron, if the set of vertices is not carefully chosen, the width of their convex can be proportionally small w.r.t. $n$ , as demonstrated in the next proposition. In particular, the same choice of vertices as in 17 results in the simplex with diminishing width as follows. ", "page_idx": 17}, {"type": "text", "text": "Proposition 24. Consider a permutahedron, fix $\\begin{array}{r l r}{\\omega}&{{}\\in}&{\\mathfrak{S}_{n},}\\end{array}$ , consider the matrix $\\begin{array}{r l}{W}&{{}=}\\end{array}$ $[\\phi^{\\omega},{\\bf I}^{\\omega s_{1}},{\\bf I}^{\\omega s_{2}},\\ldots,{\\bf I}^{\\omega s_{n-1}}]$ . The width of the simplex that corresponds to $M$ , is upper bounded as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\vartheta(M)\\leq{\\frac{3}{n}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. The coordinate matrix w.r.t. $\\phi^{\\omega}$ , that is, $\\mathrm{coM}(M,1)$ can be written as follows. ", "page_idx": 17}, {"type": "equation", "text": "$$\nV={\\left[\\begin{array}{l l l l l l}{1}&{0}&{0}&{\\cdots}&{0}&{0}\\\\ {-1}&{1}&{0}&{\\cdots}&{0}&{0}\\\\ {0}&{-1}&{1}&{\\cdots}&{0}&{0}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}&{\\vdots}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}&{\\vdots}\\\\ {0}&{0}&{0}&{\\cdots}&{-1}&{1}\\\\ {0}&{0}&{0}&{\\cdots}&{0}&{-1}\\end{array}\\right]}\\in\\mathbb{R}^{n\\times(n-1)}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, the Gram matrix is ", "page_idx": 17}, {"type": "equation", "text": "$$\nG:=V^{\\top}V=\\left[\\begin{array}{l l l l l l l l l}{2}&{-1}&{0}&{0}&{0}&{\\cdots}&{0}&{0}&{0}\\\\ {-1}&{2}&{-1}&{0}&{0}&{\\cdots}&{0}&{0}&{0}\\\\ {0}&{-1}&{2}&{-1}&{0}&{\\cdots}&{0}&{0}&{0}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}&{\\vdots}&{\\vdots}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}&{\\vdots}&{\\vdots}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}&{\\vdots}&{\\vdots}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}&{\\vdots}&{\\vdots}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}&{\\vdots}&{\\vdots}\\\\ {0}&{0}&{0}&{\\cdots}&{0}&{0}&{-1}&{2}&{-1}\\\\ {0}&{0}&{0}&{\\cdots}&{0}&{0}&{0}&{-1}&{2}\\end{array}\\right]\\in\\mathbb{R}^{(n-1)\\times(n-1)}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that $G$ is a tridiagonal matrix and also Toeplitz matrix, therefore, its minimum eigenvalues has closed form as follows ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\lambda_{n-1}(G)=2+2\\cos\\left({\\frac{(n-1)\\pi}{n}}\\right)=2\\sin^{2}\\left({\\frac{\\pi}{2n}}\\right)\\leq{\\frac{5}{n^{2}}};\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "as $\\begin{array}{r}{|\\sin\\left(\\frac{\\pi}{2n}\\right)|\\leq\\frac{\\pi}{2n}}\\end{array}$ . Therefore, $\\begin{array}{r}{\\vartheta(M)\\leq\\sigma_{n-1}(V)=\\sqrt{\\lambda_{n-1}(G)}\\leq\\frac{3}{n}}\\end{array}$ . ", "page_idx": 17}, {"type": "text", "text": "Proposition 24 highlights the challenge of selecting a set of vertices such that the width does not contract with the increasing dimension, even in the case of a simple permutahedron. Denote $\\mathfrak{C}_{n}\\subset\\mathfrak{S}_{n}$ as the group of cyclic permutations of length $n$ . One potential candidate for such a set of vertices is the collection corresponding to cyclic permutations ${\\mathfrak{C}}_{n}$ , as described in the next proposition. ", "page_idx": 17}, {"type": "text", "text": "Proposition 25. Consider the matrix ${\\overline{{W}}}=[\\mathbf{I}^{\\omega}]_{\\omega\\in\\mathfrak{C}_{n}}$ . We have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\vartheta({\\overline{{W}}})\\geq{\\frac{n}{2}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. The form of matrix $\\overline{W}$ is as follows ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\overline{{W}}=\\left[\\begin{array}{c c c c c}{1}&{n}&{n-1}&{...}&{2}\\\\ {2}&{1}&{n}&{...}&{3}\\\\ {3}&{2}&{1}&{...}&{4}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}\\\\ {n-1}&{n-2}&{n-3}&{...}&{n-1}\\\\ {n}&{n-1}&{n-2}&{...}&{1}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The coordinate matrix w.r.t. the first column is as follows ", "page_idx": 18}, {"type": "equation", "text": "$$\nV=\\operatorname{coM}(\\overline{{W}},1)=\\left[\\begin{array}{c c c c}{n-1}&{n-2}&{\\cdot\\cdot\\cdot}&{1}\\\\ {-1}&{n-2}&{\\cdot\\cdot\\cdot}&{1}\\\\ {-1}&{-2}&{\\cdot\\cdot\\cdot}&{1}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {-1}&{-2}&{\\cdot\\cdot}&{1}\\\\ {-1}&{-2}&{\\cdot\\cdot}&{-(n-1)}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Let $u\\in\\mathbb{R}^{n-1}$ be any unit vector, and let $z=V u\\in\\mathbb{R}^{n}$ . We have that ", "page_idx": 18}, {"type": "equation", "text": "$$\nz_{i}-z_{i+1}=n u_{i}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Let us consider ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{4\\left\\|z\\right\\|^{2}=4z_{1}^{2}+4z_{2}^{2}+\\cdots+4z_{n}^{2}}\\\\ &{\\qquad=2z_{1}^{2}+[(z_{1}+z_{2})^{2}+(z_{1}-z_{2})^{2}]+[(z_{2}+z_{3})^{2}+(z_{2}-z_{3})^{2}]}\\\\ &{\\qquad\\quad+\\cdots+[(z_{n-1}+z_{n})^{2}+(z_{n-1}-z_{n})^{2}]+2z_{n}^{2}}\\\\ &{\\qquad\\geq(z_{1}-z_{2})^{2}+(z_{2}-z_{3})^{2}+\\cdots+(z_{n-1}-z_{n})^{2}}\\\\ &{\\qquad=n^{2}(u_{1}^{2}+u_{2}^{2}+\\cdots+u_{n-1}^{2})=n^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sigma_{n-1}(V)=\\operatorname*{min}_{u:\\|u\\|=1}{\\sqrt{\\frac{\\left\\|V u\\right\\|^{2}}{\\left\\|u\\right\\|^{2}}}}\\geq{\\frac{n}{2}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "It is straightforward that if one takes any column of $\\overline{W}$ as a reference column, the resulting coordinate matrices have identical singular values. In particular, for any $i,j\\in[n]$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname{coM}({\\overline{{W}}},i)=P\\cdot\\operatorname{coM}({\\overline{{W}}},j),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $P$ is a permutation matrix, thus, their singular values are identical. Therefore, we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\vartheta({\\overline{{W}}})\\geq{\\frac{n}{2}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "As a result, the set of vertices corresponding to cyclic permutations is a sensible choice. In case of a generalised permutahedron, let us define ", "page_idx": 18}, {"type": "equation", "text": "$$\nW:=[\\phi^{\\omega}]_{\\omega\\in{\\mathfrak{C}}_{n}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "As generalised permutahedra are deformations of the permutahedron, we expect that $\\vartheta(W)$ is reasonably large for a broad class of strictly convex games. In particular, we consider the class of strictly convex games in which the width $\\mathring{\\vartheta}\\big(W\\big)$ is lower bounded, as in the following assumption: ", "page_idx": 18}, {"type": "text", "text": "Assumption 26. The width of the simplex that corresponds to $W$ in (49) is bounded as follows ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\vartheta(W)\\geq{\\frac{n\\varsigma}{c_{W}}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for some constant $c_{W}>0$ . ", "page_idx": 18}, {"type": "image", "img_path": "ZRYFftR4xn/tmp/5fc6be601e542cfce67467bc096eb99aafd510b09c6a3504c95e6d2f53d9a899.jpg", "img_caption": ["Figure 3: $c_{W}$ with $n\\in\\{10$ , 50, 100, 150, 200, 300, 500, $1000\\}$ , $\\textstyle\\varsigma={\\frac{0.1}{n}}$ , and 20000 trials "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "These parameters will eventually play a crucial role in determining the number of samples required using this choice of $n$ permutation orders. Although proving an exact upper bound for $c_{W}$ in all strictly convex games is challenging, we conjecture that $c_{W}$ is relatively small in a large subset of the games. ", "page_idx": 19}, {"type": "text", "text": "To investigate Assumption 26, we conducted a simulation to compute the constant $c_{W}$ of the minimum singular value $\\sigma_{n-1}(M)$ . For each case where $n$ takes values of (10, 50, 100, 150, 200, 300, 500, 1000), the simulation consisted of 20000 game trials with $\\zeta\\;=\\;0.1/n$ . As depicted in Figure 3, the values of $c_{W}$ tend to be relatively small and highly concentrated within the interval $(0,30)$ . This observation suggests that for most cases of strictly convex games, $c_{W}$ remains reasonably small. Consequently, our algorithm exhibits relatively low sample complexity. Code of the experiment is available at: https://github.com/NamTranKekL/ ConstantStrictlyConvexGame.git. ", "page_idx": 19}, {"type": "text", "text": "For each case where $n$ takes values of (10, 50, 100, 150, 200, 300, 500, 1000), the simulation consisted of 20000 game trials with $\\varsigma=0.1/n$ . As depicted in Figure 3, the values of $c_{W}$ tend to be relatively small and highly concentrated within the interval $(0,30)$ . This observation suggests that for most cases of strictly convex games, $c_{W}$ remains reasonably small. The results indicate that $c_{W}$ tends to be relatively small with high probability, and does not depend on the value of $n$ . ", "page_idx": 19}, {"type": "text", "text": "B On the Stopping Condition ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof of Proposition 5. For each $\\mathcal{C}_{p}$ , choose a point in its interior, denote as $x^{p}$ . As there are at most $n-1$ points $\\{x^{p}\\}_{p\\in[|\\mathcal{P}|]}$ , there exists a $(n-2)$ -dimensional hyperplane $H$ that contains $\\{x^{p}\\}_{p\\in[|\\mathcal{P}|]}$ Let $\\tilde{H}$ be a hyperplane parallel to $H$ and let the distance $\\mathcal{D}(H,\\tilde{H})$ be arbitrary small. ", "page_idx": 19}, {"type": "text", "text": "As confidence sets are full-dimensional $(n-1)$ , $\\tilde{H}$ must also intersect with the interiors of all confidence sets. Since $H$ and $\\tilde{H}$ are parallel, any convex hull of points within $H$ and $\\tilde{H}$ cannot intersect. Therefore, there is no common point. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Proof of Proposition 9. The proof spirit is similar to that of Proposition 5. ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Let $H$ be the $(n-2)$ -dimensional hyperplane that intersects with the interiors of all confidence sets.   \nLet $\\tilde{H}$ be a hyperplane parallel to $H$ and let the distance $\\mathcal{D}(H,\\tilde{H})$ be arbitrary small. ", "page_idx": 20}, {"type": "text", "text": "As confidence sets are full-dimensional, $\\tilde{H}$ must also intersect with the interiors of all confidence sets. Since $H$ and $\\tilde{H}$ are parallel, any convex hull of points within $H$ and $\\tilde{H}$ cannot intersect. Therefore, there is no common point. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "The proof of Theorem 11 is a combination of the classic hyperplane separation theorem and the following lemma. ", "page_idx": 20}, {"type": "text", "text": "Lemma 27. Let $\\{{\\mathcal{C}}_{p}\\}_{p\\in[n]}$ be mutually disjoint compact and convex subsets in $\\mathbf{E}^{n-1}$ . Suppose there does not exist a $(n-2)$ -dimensional hyperplane that intersects with all confidence sets $\\mathcal{C}_{p}$ , $\\forall p\\in[n]$ , then for each $p\\in[n]$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\mathcal{C}}_{p}\\cap\\operatorname{Conv}\\left(\\bigcup_{q\\neq p}{\\mathcal{C}}_{q}\\right)=\\varnothing.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. We prove this lemma by contra-position, that is, if there is $\\mathcal{C}_{p}$ such that ", "page_idx": 20}, {"type": "equation", "text": "$$\nc_{p}\\cap\\mathrm{Conv}\\left(\\bigcup_{p\\neq q}\\mathcal{C}_{q}\\right)\\neq\\emptyset;\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "then there exist a hyperplane that intersects with all the $\\mathcal{C}_{p},\\;\\forall p\\in[n]$ . ", "page_idx": 20}, {"type": "text", "text": "First, assume there is a point $x=\\mathcal{C}_{p}\\cap\\mathrm{Conv}\\left(\\bigcup_{q\\neq p}\\mathcal{C}_{p}\\right)$ . By Carath\u00e9odory\u2019s theorem, there are at most $n$ points $x^{k}\\in\\bigcup_{q\\neq p}{\\mathcal{C}}_{q}$ such that ", "page_idx": 20}, {"type": "equation", "text": "$$\nx=\\sum_{k\\in[n]}\\alpha_{k}x^{k}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "As each $x^{k}\\in{\\mathcal{C}}_{q}$ for some $\\mathcal{C}_{q}$ , one can rewrite the equation above as ", "page_idx": 20}, {"type": "equation", "text": "$$\nx=\\sum_{q\\neq p}\\ \\sum_{k:\\ x^{k}\\in C_{q}}\\alpha_{k}x^{k}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Furthermore, we can write ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{x^{k}\\in{\\mathcal C}_{q}}\\alpha_{k}x^{k}=\\tilde{\\alpha}_{q}\\tilde{x}^{q},\\quad\\mathrm{in~which},\\quad\\tilde{x}^{q}:=\\frac{\\sum_{k:\\;x^{k}\\in{\\mathcal C}_{q}}\\alpha_{k}x^{k}}{\\sum_{k:\\;x^{k}\\in{\\mathcal C}_{q}}\\alpha_{k}},\\quad\\mathrm{and}\\quad\\tilde{\\alpha}_{q}:=\\sum_{k:\\;x^{k}\\in{\\mathcal C}_{q}}\\alpha_{k}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $\\mathcal{C}_{q}$ is convex, $\\tilde{x}^{q}\\in\\mathcal{C}_{q}$ . Substituting (54) into (52), one obtains ", "page_idx": 20}, {"type": "equation", "text": "$$\nx=\\sum_{q\\not=p}{\\tilde{\\alpha}}_{q}{\\tilde{x}}^{q}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Define $H$ as a hyperplane that passes through all $\\tilde{x}_{q}$ , we have that $x\\in H$ . ", "page_idx": 20}, {"type": "text", "text": "Second, we now show how to construct a hyperplane that intersects with all ${\\mathcal{C}}_{m}$ , $m\\in[n]$ . Let $I$ be the set of indices such that $\\mathcal{C}_{q}\\ni\\tilde{x}_{q}$ . We have two following cases. ", "page_idx": 20}, {"type": "text", "text": "(i) First, if $\\left|I\\right|=n-1$ , then $H$ is the $(n-2)$ -dimensional hyperplane that intersect with all ${\\mathcal{C}}_{m}$ , $m\\in[n]$ .   \n(ii) Second, if $|I|<n\\!-\\!1$ , for any $\\mathcal{C}_{q^{\\prime}}\\neq\\mathcal{C}_{p}$ that does not contain any ${\\tilde{x}}^{q}$ , we choose any arbitrary point $x^{q^{\\prime}}\\in\\mathcal{C}_{q^{\\prime}}$ . As there are $n-1$ points of $\\tilde{x}^{q}$ and $x^{q^{\\prime}}$ , there exists a hyperplane $\\overline{H}$ that contains all these points. Furthermore, $\\overline{H}$ must contain $x$ , so it is the $(n-2)$ -dimensional hyperplane that intersects with all sets ${\\mathcal{C}}_{m}$ , $\\forall m\\in[n]$ . ", "page_idx": 20}, {"type": "text", "text": "Now, we state the proof of Theorem 11. ", "page_idx": 21}, {"type": "text", "text": "Proof of Theorem $_{I I}$ . As a result of Lemma 27, we have that for all $\\mathcal{C}_{p},\\forall p\\in[n]$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\nc_{p}\\cap\\operatorname{Conv}\\left(\\bigcup_{q\\neq p}\\mathcal{C}_{q}\\right)=\\emptyset.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, by the hyperplane separation theorem, there must exist a hyperplane that separates $\\mathcal{C}_{p}$ and $\\mathrm{Conv}\\left(\\bigcup_{q\\neq p}\\mathcal{C}_{q}\\right).$ ", "page_idx": 21}, {"type": "text", "text": "Proof of Lemma 13. Let us denote $\\Delta_{n}$ as Conv $\\left(\\{x^{p}\\}_{p\\in[n]}\\right)$ . As there is no hyperplane of dimension $n-2$ go through all the set $\\mathcal{C}_{p}$ , the simplex $\\Delta_{n}$ is $(n-1)$ dimensional. We have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\bigcap_{p\\in[n]}E_{p}\\subseteq\\Delta_{n}\\iff\\Delta_{n}^{c}\\subseteq\\bigcup_{p\\in[n]}E_{p}^{c};\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $E_{p}^{c}$ is the complement of the set $E_{p}$ . ", "page_idx": 21}, {"type": "text", "text": "We will prove the RHS of the above. Consider $\\hat{x}\\in{\\Delta_{n}^{c}}$ , as $\\Delta_{n}$ is full dimensional, $\\hat{x}$ can be uniquely written as affine combination of the vertices, that is, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\hat{x}=\\sum_{p\\in[n]}\\lambda_{p}x^{p},\\quad\\sum_{p\\in[n]}\\lambda_{p}=1.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "As $\\hat{x}\\in\\Delta_{n}^{c}$ , there must exist some $\\lambda_{k}<0$ . ", "page_idx": 21}, {"type": "text", "text": "Now, we shall prove $\\hat{x}\\in E_{k}^{c}$ . Consider the following, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left<v^{k},\\hat{x}\\right>=\\left<v^{k},\\sum_{p\\in[n]}\\lambda_{p}x^{p}\\right>=\\lambda_{k}\\left<v^{k},x^{k}\\right>+\\displaystyle\\sum_{p\\neq k}\\lambda_{p}\\left<v^{k},x^{p}\\right>}\\\\ {\\displaystyle>\\lambda_{k}c^{k}+c^{k}\\displaystyle\\sum_{p\\neq k}\\lambda_{p}}\\\\ {\\displaystyle=c^{k}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The above inequality holds since $\\left\\langle v^{k},x^{k}\\right\\rangle<c_{k}$ and $\\lambda_{k}<0$ . Therefore, $\\hat{x}\\in E_{k}^{c}$ . This means that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Delta_{n}^{c}\\subseteq\\bigcup_{k\\in[n]}E_{k}^{c}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof of Theorem 14. Before proceeding the main proof, we show two simple consequences of the construction of $H_{p}(Q),p\\in[n]$ and the assumption (14). ", "page_idx": 21}, {"type": "text", "text": "Fact 1: Consider $p\\in[n]$ , $H_{p}(Q)$ acts as a separating hyperplane for $\\mathcal{C}_{p}$ . To see this, assume that $H_{p}(Q)$ is not a separate hyperplane for $\\mathcal{C}_{p}$ , then there exists $z^{p}\\in\\mathcal{C}_{p}$ such that $\\left\\langle v^{p},z^{p}\\right\\rangle\\geq c^{p}$ . From (13), we have $\\begin{array}{r}{\\langle v^{p},x^{p}\\rangle\\leq c^{p}+\\operatorname*{max}_{q\\in[n]\\backslash p}\\dim(\\mathcal{C}_{p})}\\end{array}$ . Then, there are two cases. First, assume that $\\langle v^{p},x^{p}\\rangle\\leq c^{p}$ . As $x^{p}$ , $z^{p}\\in\\mathcal{C}_{p}$ and $\\langle v^{p},z^{p}\\rangle\\geq c^{p}$ , there must exist a point $x$ in the line segment $[x^{p},z^{p}]$ such that $\\langle v^{p},x\\rangle=c^{p}$ . This means that $\\begin{array}{r}{\\mathcal{D}(\\mathcal{C}_{p},H_{p})=0}\\end{array}$ , which violates assumption (14). Second, assume that $c^{p}\\leq\\langle v^{p},x^{p}\\rangle\\leq c^{p}+\\operatorname*{max}_{q\\in[n]\\backslash p}\\dim({\\mathcal C}_{p})$ . Then, we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{D}(\\mathcal{C}_{p},H_{p})\\le\\mathcal{D}(x^{p},H_{p})=|\\left\\langle v^{p},x^{p}\\right\\rangle-c^{p}|\\le\\operatorname*{max}_{q\\in[n]\\backslash p}\\mathrm{diam}(\\mathcal{C}_{q}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This also violates assumption (14). This implies that if (14) is satisfied, $H_{p}(Q)$ must separate $\\mathcal{C}_{p}$ from \u222aq\u0338=pCq. ", "page_idx": 22}, {"type": "text", "text": "Fact 2: The distance from any point in $\\mathcal{C}_{q}$ from $H_{p}(Q)$ is bounded as follows. For $x\\in{\\mathcal{C}}_{q}$ , $q\\neq p$ , we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{D}(x,H_{p}(Q))\\leq\\mathcal{D}(x,x^{q})+\\mathcal{D}(x^{q},H_{p}(Q))\\leq2\\operatorname*{max}_{q^{\\prime}\\in[n]\\backslash p}\\mathrm{diam}(\\mathcal{C}_{q^{\\prime}}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Now, we proceed to the main proof. For the ease of notation, we simply write $H_{p}$ for $H_{p}(Q)$ . ", "page_idx": 22}, {"type": "text", "text": "First, from assumption (14), we has that for any $p\\in[n]$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{D}(\\mathcal{C}_{p},H_{p})=\\operatorname*{min}_{x\\in\\mathcal{C}_{p}}\\mathcal{D}(x,H_{p})=\\operatorname*{min}_{x\\in\\mathcal{C}_{p}}|c^{p}-\\langle v^{p},x\\rangle|.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{x\\in\\mathcal{C}_{p}}{\\operatorname*{min}}\\,\\mathcal{D}(x,H_{p})>2n\\underset{q\\neq p}{\\operatorname*{max}}\\,\\mathrm{diam}(\\mathcal{C}_{q})}\\\\ &{\\qquad\\qquad\\qquad\\geq\\displaystyle\\sum_{q\\in[n]\\backslash p}\\underset{x\\in\\mathcal{C}_{q}}{\\operatorname*{max}}\\,\\mathcal{D}(x,H_{p}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Second, we shows that how to pick a common point which exists when (61) is satisfied. Let us choose a collection of points $\\bar{x^{p}}\\,\\bar{\\in}\\,\\mathcal{C}_{p},\\l{p}\\in[n]$ , and define ", "page_idx": 22}, {"type": "equation", "text": "$$\nx^{\\star}={\\frac{1}{n}}\\sum_{p\\in[n]}x^{p}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Now, we show that $x^{\\star}\\in E_{p},\\;\\forall p\\in[n]$ . ", "page_idx": 22}, {"type": "text", "text": "For each $p\\in[n]$ , consider $H_{p}$ . We denote ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\zeta_{p p}:=c^{p}-\\langle v^{p},x^{p}\\rangle>0;}\\\\ {\\zeta_{p q}:=\\langle v^{p},x^{q}\\rangle-c^{p}>0,\\quad q\\neq p.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that $\\begin{array}{r}{\\mathcal{D}(\\boldsymbol{x},H_{p})=\\mid\\langle\\boldsymbol{v}^{p},\\boldsymbol{x}\\rangle-c^{p}\\rvert}\\end{array}$ . Follows (61), we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\zeta_{p p}\\geq\\operatorname*{min}_{x\\in\\mathcal{C}_{p}}\\mathcal{D}(x,H_{p})>\\sum_{q\\in[n]\\setminus p}\\operatorname*{max}_{x\\in\\mathcal{C}_{q}}\\mathcal{D}(x,H_{p})\\geq\\sum_{q\\in[n]\\setminus p}\\zeta_{p q}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Now, let consider ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle v^{p},x^{\\star}\\rangle=\\displaystyle\\frac{1}{n}\\sum_{q\\in[n]}\\langle v^{p},x^{q}\\rangle=\\displaystyle\\frac{1}{n}\\sum_{q\\in[n]\\backslash p}(c^{p}+\\zeta_{p q})+\\frac{1}{n}(c^{p}-\\zeta_{p p})}\\\\ {=c^{p}+\\displaystyle\\frac{1}{n}\\left(\\sum_{q\\in[n]\\backslash p}\\zeta_{p q}-\\zeta_{p p}\\right)<c^{p}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, $x^{\\star}\\in E_{p}$ . As it is true for all $E_{p}$ , one has that ", "page_idx": 22}, {"type": "equation", "text": "$$\nx^{\\star}\\in\\bigcap_{p\\in[n]}E_{p}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Finally, by Lemma 13, we can conclude that $x^{\\star}$ is a common point. ", "page_idx": 22}, {"type": "text", "text": "C Sample Complexity Analysis ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proof of Lemma 16. Denote $\\Delta$ as the simplex corresponding to $M=[x^{1},...,x^{n}]$ , $\\Delta_{i}$ as the facet opposite the vertex $x^{i}$ , and $h_{i}(\\Delta)$ is the height of simplex w.r.t. the vertex $x^{i}$ . Denote ${\\mathrm{Vol}}_{k}(C)$ as the $k$ -dimensional content of $C\\overset{\\cdot}{\\subset}\\frac{\\dot{\\mathbf{E}}^{n-1}}{\\mathbf{E}^{n}}$ , where $\\dim(C)=k$ . Using simple calculus, one has that ", "page_idx": 22}, {"type": "equation", "text": "$$\nh_{i}(\\Delta)=\\frac{1}{n-1}\\frac{\\mathrm{Vol}_{n-1}(\\Delta)}{\\mathrm{Vol}_{n-2}(\\Delta_{i})},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We also denote $\\hat{\\Delta}$ as the perturbed simplex corresponding to $M+R$ and $\\hat{\\Delta}_{i}$ as the facet opposite the to the perturbation of $x^{i}$ . ", "page_idx": 23}, {"type": "text", "text": "we bound the height $h_{i}(\\hat{\\Delta})$ for all $i\\in[n-1]$ . For the height $h_{n}(\\hat{\\Delta})$ , one can apply similar reasoning. Let define the coordinate matrix and the pertubation matrix w.r.t $x^{n}$ as follows ", "page_idx": 23}, {"type": "equation", "text": "$$\nV:=\\operatorname{coM}(M,n),\\quad U:=\\operatorname{coM}(R,n);\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We have that $|U_{i j}|<\\epsilon,\\ \\forall i,j$ . By the definition of width $\\vartheta(M)$ , we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sigma_{n-1}(V)\\geq\\vartheta(M)\\geq\\sigma.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Let define the Gram matrix and perturbed Gram matrix as follows ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{G:=V^{\\top}V}\\\\ &{\\hat{G}:=\\left(V+U\\right)^{\\top}(V+U).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "One has that, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{G}-G=V^{\\top}U+U^{\\top}V+U^{\\top}U:=\\overline{{U}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "One has that $\\overline{{U}}_{i j}\\leq\\bar{\\epsilon}:=3n\\epsilon$ , as $|V_{i j}|<1$ and $|U_{i j}|<\\epsilon<1$ . We also has that $\\|\\overline{{U}}\\|_{2}\\le\\|\\overline{{U}}\\|_{\\mathrm{F}}\\le n\\bar{\\epsilon}$ . ", "page_idx": 23}, {"type": "text", "text": "First step. we bound the quantity $\\textstyle{\\frac{|\\operatorname*{det}(G+{\\overline{{U}}})-\\operatorname*{det}(G)|}{|\\operatorname*{det}(G)|}}$ . By [15]\u2019s Corollary 2.14, one has that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{|\\mathrm{det}(G+\\overline{{U}})-\\mathrm{det}(G)|}{|\\mathrm{det}(G)|}\\leq\\left(1+\\frac{\\|\\overline{{U}}\\|_{2}}{\\sigma_{n-1}(G)}\\right)^{n-1}-1\\leq\\left(1+\\frac{n\\overline{{\\epsilon}}}{\\sigma^{2}}\\right)^{n}-1.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "As $\\begin{array}{r}{(1+z)^{n}\\leq\\frac{1}{1-n z}}\\end{array}$ when $z\\in(0,\\frac{1}{n})$ and $n>0$ . One has that ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\frac{|\\mathrm{det}(G+{\\overline{{U}}})-\\mathrm{det}(G)|}{|\\mathrm{det}(G)|}}\\leq{\\frac{n^{2}{\\bar{\\epsilon}}}{\\sigma^{2}-n^{2}{\\bar{\\epsilon}}}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "when $\\begin{array}{r}{\\bar{\\epsilon}\\le\\frac{\\sigma^{2}}{n^{2}}}\\end{array}$ , or $\\begin{array}{r}{\\epsilon\\leq\\frac{\\sigma^{2}}{3n^{3}}}\\end{array}$ 3\u03c3n23 . Let us define k := $\\begin{array}{r}{k:=\\frac{\\sigma^{2}-n^{2}\\bar{\\epsilon}}{n^{2}\\bar{\\epsilon}}}\\end{array}$ , one has that ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\frac{|\\operatorname*{det}(G+{\\overline{{U}}})-\\operatorname*{det}(G)|}{|\\operatorname*{det}(G)|}}\\leq{\\frac{1}{k}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "It means that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{det}(G+{\\overline{{U}}})\\geq\\left(1-{\\frac{1}{k}}\\right)\\operatorname*{det}(G)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Second step. we bound the change in content of the $i^{\\mathrm{th}}$ facets of the simplex, for $i\\,\\in\\,[n-1]$ . Consider the facet that is opposite to the vertex $x^{i}$ , and denote $V(i),\\;U_{i}$ as the sub-matrices of $V,\\,U$ by removing $i^{\\mathrm{th}}$ column. Denote the Gram matrix ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{G(i):=V(i)^{\\top}V(i)}\\\\ {\\hat{G}(i):=(V(i)+U(i))^{\\top}(V(i)+U(i))}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that one can obtain $G(i),\\;{\\hat{G}}(i)$ and by removing $i^{\\mathrm{th}}$ row and column of $G(i),\\;{\\hat{G}}(i)$ respectively.   \nDenote $\\overline{{U}}(i):=\\hat{G}(i)-G(i)$ , we has that all entries of $\\overline{{U}}(i)$ smaller than \u03f5\u00af. ", "page_idx": 23}, {"type": "text", "text": "Moreover, by Singular Value Interlacing Theorem, one has that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sigma_{1}(G)\\geq\\sigma_{1}(G(i))\\geq\\sigma_{2}(G)\\geq\\sigma_{2}(G(i))\\geq\\cdots\\geq\\sigma_{n-2}(G(i))\\geq\\sigma_{n-2}(G(i))\\geq\\sigma_{n-1}(G).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Similarly, one has that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{|\\mathrm{det}(G(i)+\\overline{{U}}(i))-\\mathrm{det}(G(i))|}{|\\mathrm{det}(G(i))|}\\leq\\left(1+\\frac{\\|\\overline{{U}}(i)\\|_{2}}{\\sigma_{n-2}(G(i))}\\right)^{n-2}-1\\leq\\left(1+\\frac{n\\bar{\\epsilon}}{\\sigma^{2}}\\right)^{n}-1\\leq\\frac{1}{k}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "It means that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{det}(G(i)+{\\overline{{U}}}(i))\\leq\\left(1+{\\frac{1}{k}}\\right)\\operatorname*{det}(G(i)).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Third step. We bound the height $h_{i}$ corresponding to the vertices $x^{i}$ in this step. For $i\\in[n-1]$ one has that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Vol}_{d}(\\hat{\\Delta})=\\frac{1}{(n-1)!}\\sqrt{\\operatorname*{det}(G+\\overline{{U}})}.}\\\\ &{\\mathrm{Vol}_{d-1}(\\hat{\\Delta}_{i})=\\frac{1}{(n-2)!}\\sqrt{\\operatorname*{det}(G(i)+\\overline{{U}}(i))}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Furthermore, by the Eigenvalue Interlacing Theorem, we have $\\operatorname*{det}(G)/\\operatorname*{det}(G_{i})\\geq\\sigma_{n-1}(G)\\geq\\sigma^{2}$ . Putting things together, one has that ", "page_idx": 24}, {"type": "equation", "text": "$$\nh_{i}(\\hat{\\Delta})=\\frac{1}{n-1}\\frac{\\mathrm{Vol}_{n-1}(\\hat{\\Delta})}{\\mathrm{Vol}_{n-2}(\\hat{\\Delta}_{i})}=\\sqrt{\\frac{\\operatorname*{det}(G+\\overline{{U}})}{\\operatorname*{det}(G(i)+\\overline{{U}}(i))}}\\ge\\sqrt{\\frac{k-1}{k+1}\\frac{\\operatorname*{det}(G)}{\\operatorname*{det}(G(i))}}\\ge\\sigma\\sqrt{\\frac{\\sigma^{2}-6n^{3}\\epsilon}{\\sigma^{2}}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We note that, the above holds true for $i\\in[n-1]$ . ", "page_idx": 24}, {"type": "text", "text": "Fourth Step. Now, we bound the height corresponding to the vertex $x^{n}$ . We can define the coordination matrix and pertubation matrix w.r.t $x^{1}$ as follows. ", "page_idx": 24}, {"type": "equation", "text": "$$\nV^{\\prime}=\\operatorname{coM}(M,1),\\quad U^{\\prime}=\\operatorname{coM}(R,1).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Note that, by the definition of the width, we have that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sigma_{n-1}(V^{\\prime})\\geq\\vartheta(M)\\geq\\sigma;\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and also, $|U_{i j}^{\\prime}|\\leq\\epsilon$ . Similarly, applying Steps 1-3, we also have that ", "page_idx": 24}, {"type": "equation", "text": "$$\nh_{n}(\\hat{\\Delta})\\geq\\sqrt{\\sigma^{2}-6n^{3}\\epsilon}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, $h_{i}(\\hat{\\Delta})\\geq\\sqrt{\\sigma^{2}-6n^{3}\\epsilon}$ holds true for all $i\\in[n]$ . We conclude that ", "page_idx": 24}, {"type": "equation", "text": "$$\nh_{\\mathrm{min}}\\geq\\sqrt{\\sigma^{2}-6n^{3}\\epsilon},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "whenever, $\\begin{array}{r}{\\epsilon\\leq\\frac{\\sigma^{2}}{3n^{3}}}\\end{array}$ ", "page_idx": 24}, {"type": "text", "text": "Proof of Theorem $_{18}$ . Note that $|{\\mathcal{P}}|=n$ . Denote $\\epsilon_{0}:=2\\operatorname*{max}_{p\\in[n]}\\dim(\\mathcal{C}_{p})$ . For any $\\omega_{p}\\in\\mathcal{P}$ , define the event ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{E}=\\{\\phi^{\\omega_{p}}\\in\\mathcal{C}_{p},\\forall p\\in[n]\\}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By the construction of the confidence set, we guarantee that $\\mathcal{E}$ happen with probability at least $1-n^{2}\\delta$ . ", "page_idx": 24}, {"type": "text", "text": "Consider $\\textit{p}\\in\\ [n]$ , for any $q~\\in~[n]~\\backslash~p$ , let $x^{q}$ be the projection of $\\phi^{\\omega_{q}}$ onto $H_{p}$ , and $x^{p}:=$ arg $\\mathrm{min}_{p\\in{\\mathcal{C}}_{p}}\\,{\\mathcal{D}}(x,H_{p})$ . We have that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{D}(x^{k},\\phi^{\\omega_{k}})\\leq\\epsilon_{0},\\;\\forall k\\in[n].}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We need to bound $\\mathcal{D}(x^{p},H_{p})$ by bounding the minimum height of simplex Conv $\\left(\\{x^{p}\\}_{p\\in[n]}\\right)$ , which is a pertubation of Conv $\\left(\\{\\phi^{\\omega_{p}}\\}_{p\\in[n]}\\right)$ . ", "page_idx": 24}, {"type": "text", "text": "Define matrix $\\boldsymbol{M}=[\\phi^{\\omega_{p}}]_{p\\in[n]}$ , and $\\hat{M}=[x^{p}]_{p\\in[n]}$ . Let $R:=M-\\hat{M}$ be the perturbation matrix, one has that $R_{i j}\\leq\\epsilon_{0}$ , $\\forall(i,j)$ . By Lemma 16, we have that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{D}(x^{p},H_{p})\\geq\\sqrt{\\sigma^{2}-12n^{3}\\epsilon_{0}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, for $D(x^{p},H_{p})\\geq n\\epsilon_{0}$ holds , it is sufficient to provide the condition for $\\sigma$ such that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sqrt{\\sigma^{2}-12n^{3}\\epsilon_{0}}\\ge n\\epsilon_{0}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Assuming that $\\epsilon_{0}<1$ , for the condition of Lemma 16 and the above inequality to hold, it is sufficient to choose ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\epsilon_{0}=\\frac{\\sigma^{2}}{13n^{3}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now, we calculate the upper bound for sample needed. At epoch $K$ , we have that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\epsilon_{0}=2\\mathrm{diam}(\\mathcal{C}_{p})\\geq4\\sqrt{\\frac{2n\\log(\\delta^{-1})}{K}}}\\\\ {\\displaystyle\\sigma=\\frac{n\\varsigma}{c_{W}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then we have $\\begin{array}{r}{K=O\\left(\\frac{n^{13}\\log(n\\delta^{-1}\\varsigma^{-1})}{\\varsigma^{4}}\\right)}\\end{array}$ . As each phase, there are at most $n^{2}$ queries, then the total number of sample needed is ", "page_idx": 25}, {"type": "equation", "text": "$$\nT=O\\left({\\frac{n^{15}\\log(n\\delta^{-1}\\varsigma^{-1})}{\\varsigma^{4}}}\\right)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for the algorithm to return a common point, with probability of at least $1-\\delta$ . ", "page_idx": 25}, {"type": "text", "text": "Proof of Theorem 19. The proof is identical to that of Theorem 18, with the width of the simplex bounded by $\\textstyle{\\mathcal{Y}}(W)\\geq{\\frac{n\\varsigma}{c_{W}}}$ . \u53e3 ", "page_idx": 25}, {"type": "text", "text": "D Examples of Strictly Convex Games ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Consider a facility-sharing game (a generalisation of cost-sharing games [2, 1]) where joining a coalition $S$ would provide each player of that coalition a utility value $v(k)$ where $|S|\\;=\\;k$ , and they have to pay an average maintenance cost $c(k)$ . The expected reward of $S$ is defined as $\\bar{\\mu(S)\\bar{)}}=v(k)\\bar{-c(k)}$ , representing the average utility of its coalitional members. This setting represents many real-world scenarios, such as: ", "page_idx": 25}, {"type": "text", "text": "\u2022 University departments together plan to set up and maintain a shared computing lab. The value of using the lab is the same $v(k)=v$ for each department (e.g., their students can have access computing facilities), but the maintenance cost $c(k)$ is monotone decreasing and strictly concave (e.g., the more participate the less the average maintenance cost becomes). An example to such maintenance cost function is, e.g., $c(\\bar{k})=C_{1}-C_{2}k^{\\alpha}$ , where $\\alpha>1$ and $C_{1},C_{2}$ are appropriately set constants such that the total maintenance cost $k c(k)=$ $C_{1}k-C_{2}k^{(\\alpha+1)}$ is non-negative and monotone increasing in the $[0,n]$ range ( $\\acute{n}$ is the total number of departments). A coalition $S$ here represents the cooperating departments.   \n\u2022 An international corporate is expanding its international markets via mutual collaborations with multiple local companies. The cost for each potential local member company to join the consortium is fixed $c$ (e.g., integration cost), while the benefti they can gain through this collaboration, $v(k)$ , is a strictly monotone convex function in the number of local markets $k$ (assuming that each local partner is in charge of their own local market), due to the potential synergies between different markets. An example benefit/utility function is, e.g., $\\bar{v}(k)=k^{\\alpha}$ with $\\alpha>1$ . The corporate\u2019s task is then to invite local companies to join their coalition/consortium $S$ .   \n\u2022 (An alternative version of airport games) Airlines decide whether they launch a filght from a particular airport. The more airlines decide to do so, the higher value $v(k)$ (e.g., more connection options), and the lower average buy-in cost $c(k)$ (e.g., runway maintenance, staff cost etc.) each airlines can have. It\u2019s reasonable to assume that $v(k)$ is strictly convex and monotone increasing (e.g., the number of connecting combinations grows exponentially) and $c(k)$ is monotone decreasing and strictly concave. Those airlines who decide to invest into that airport will form a coalition. ", "page_idx": 25}, {"type": "text", "text": "For each of the scenarios above, we can see that the expected reward function $\\mu(S)=v(|S|)-c(|S|)$ is indeed strictly supermodular. The reason is that $v(k)$ and $c(k)$ are discrete and finite on $[0,n]$ , and thus, we can easily find $\\varsigma>0$ for which these games also admit $\\varsigma,$ -strict convexity. ", "page_idx": 25}, {"type": "text", "text": "E Further Discussions ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "E.1 Comparison with Pantazis et al. [20] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "While the algorithm in [20] is proposed for general cooperative games and conceptually applicable to the class of strictly convex games, we argue that their algorithm is not statistically and computationally efficient when applied to strictly convex games, due to the absence of a specific mechanism to leverage the supermodular structure of the expected reward function. In particular, firstly, we argue that without any modification and with bandit feedback, their algorithm would require a minimum of $\\Omega(2^{n})$ samples. Secondly, although we believe the framework of [20] could be conceptually applied to strict convex games, significant non-trivial modifications may be necessary to leverage the supermodular structure of the mean reward function. ", "page_idx": 26}, {"type": "text", "text": "Appplying [20] to strictly convex games without any modifications. We first briefly outline their algorithmic framework. In this paper, the authors assume that each coalition $S\\subset N$ has access to a number of samples, denoted as $t_{S}>1$ . For each coalition $S$ , the empirical mean is denoted as $\\overline{{\\mu}}_{t_{S}}(S)$ , and a confidence set for the given mean reward is constructed, denoted as, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{C}(\\mu(S))=\\left\\{\\hat{\\mu}(S)\\in[0,\\,1]\\mid|\\hat{\\mu}(S)-\\overline{{\\mu}}_{t_{S}}(S)|\\leq\\varepsilon_{t_{S}}\\right\\}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We note that while the algorithm in [20] constructs the confidence set using Wasserstein distance, in the case of distributions with bounded support, we can simplify it by using the mean reward difference. After constructing the confidence set for the mean reward of each coalition, the algorithm solves the following robust optimization problem: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{x\\in\\mathbb{R}^{n}}{\\operatorname*{min}}\\,\\|x\\|_{2}^{2}}\\\\ &{\\quad\\mathrm{s.t.}\\;x(N)=\\mu(N)}\\\\ &{\\quad\\quad x(S)\\geq\\operatorname*{sup}(\\mathcal{C}(\\mu(S)),\\quad\\forall S\\subset N.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "That is, finding the stable allocation for the worst-case scenario within the confidence sets. It is clear that when directly applying this framework to the bandit setting, each coalition must be queried at least once, that is $t_{S}>1$ . This inevitably leads to a complexity of $\\Omega(2^{n})$ samples, regardless of the sampling scheme one employs. In term of computation, with $2^{n}-2$ confidence sets for all coalitions $S\\subset N$ , tabular representation of the confidence set incurs extreme computational cost. ", "page_idx": 26}, {"type": "text", "text": "Significant modifications required for [20]. As described above, the algorithm in [20] suffers from $2^{n}$ sample complexity, and the main reason is because it requires constructing confidence sets for the mean reward for all coalitions $S\\subset N$ . As such, if we want to apply their algorithm efficiently to the bandit setting, we need to address this limitation. ", "page_idx": 26}, {"type": "text", "text": "To do so, one may need to develop an approach to design a confidence set for a specific class of strictly convex games. For instance, we can consider the following approach: Given historical data, instead of writing a confidence set for each individual coalition, let us define a confidence set for the mean reward function as follows: ", "page_idx": 26}, {"type": "equation", "text": "$$\n{\\mathcal{C}}(\\mu)=\\left\\{{\\hat{\\mu}}:2^{N}\\rightarrow[0,\\ 1]\\ |\\ {\\hat{\\mu}}\\in[{\\mathcal{C}}(\\mu(S))]_{S\\subset N},\\ {\\hat{\\mu}}\\ i s\\,s t r i c t l y\\ s u p e r m o d u l a r\\right\\};\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the confidence set $\\mathcal{C}(\\mu(S))$ could potentially be $[0,1]$ for some coalition $S$ , as there is no data available for these coalitions. Let $\\operatorname{Core}(\\hat{\\mu})$ be the core with respect to the reward function $\\hat{\\mu}$ We propose a generalization of the framework from the robust optimization problem to adapt to the structure of the game as follows. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\min_{c\\in\\mathbb{R}^{n}}\\|x\\|_{2}^{2}}\\\\ {\\mathrm{\\s.t.~}x(N)=\\mu(N)}\\\\ {\\displaystyle\\qquad x\\in\\bigcap_{\\hat{\\mu}\\in\\mathcal{C}(\\mu)}\\mathrm{Core}(\\hat{\\mu}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "That is, we find a stable allocation $x$ for every possible supermodular function within the confidence set of the reward function. ", "page_idx": 26}, {"type": "text", "text": "However, implementing and analyzing this approach may pose significant challenges. The first challenge lies in constructing a tight confidence set $[\\mathcal{C}(\\mu(\\bar{S^{\\prime}}))]_{\\mathit{S}\\subset{N}}$ such that all functions within this collection are strictly supermodular. We are not aware of a method to explicitly construct $[\\mathcal{C}(\\mu(S))]_{S\\subset N}$ containing only strictly supermodular functions, and we believe this set could potentially be very complicated. To illustrate, consider the scenario where we have samples from two coalitions, $\\{1\\}$ and $\\{1,2\\}$ , with the following empirical means: ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "equation", "text": "$$\n\\overline{{\\mu}}(\\left\\{1\\right\\})=0.11;\\quad\\overline{{\\mu}}(\\left\\{1,2\\right\\})=0.1\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This situation might occurs when the number of samples is insufficient. In such cases, regardless of the value chosen for the remaining coalition rewards in the function $\\overline{{\\mu}}(S),\\overline{{\\mu}}(S)$ is not supermodular (as $\\{1\\}\\subset\\{1,2\\}$ , yet $\\overline{{\\mu}}(1)>\\overline{{\\mu}}(1,2))$ ). Consequently, either the confidence set $\\mathcal{C}(\\mu(1))$ or $\\mathcal{C}(\\mu(1,2))$ does not contain the empirical mean reward, indicating the highly complicated shape of the confidence set. ", "page_idx": 27}, {"type": "text", "text": "The second challenge is that while computing a stable allocation for a given supermodular reward function $\\hat{\\mu}$ is a straightforward task, computing a stable allocation for all supermodular reward functions in the confidence set $\\mathcal C(\\mu)$ in a computationally efficient way is an open problem, to the best of our knowledge. ", "page_idx": 27}, {"type": "text", "text": "The discussion above also highlights the key difference between our work and that of [20]: Instead of explicitly constructing the confidence set of the expected mean reward function to integrate the supermodular structure for computing a stable allocation, which might be a sophisticated task, we directly exploit the geometry of the core of strictly convex games. Specifically, in strictly convex games, each vertex of the core corresponds to a marginal vector with respect to some permutation orders. Given that one can construct the confidence set of marginal vectors easily, our method is conceptually and computationally simpler. However, we believe that adopting the more general framework of robust optimization as presented in [20] is a very interesting, but non-trivial, direction, and we leave it for future work. ", "page_idx": 27}, {"type": "text", "text": "E.2 Comment on Lower Bounds ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "It is also crucial to develop the lower bound for the class of strict convex game. One promising direction is to extend the game instances in Theorem 7. However, there could be several technical issues when it comes to deriving a meaningful lower bound for strictly convex games. The main problem is twofold: Firstly, not every small perturbation of the face game may result in a strictly convex game; therefore, careful tailoring to ensure strict convexity is required. Second, to show a polynomial dependence of sample complexity on $n$ , we need to generalise the two game instances in Theorem 7 into $\\mathrm{poly}(n)$ game instances for the information-theoretic argument. It is not clear how to construct them so that their core has no intersection, and the statistical distance of the reward can be upper bounded. This can be further detailed as follows: ", "page_idx": 27}, {"type": "text", "text": "Firstly, as proven in [14], a face game is only guaranteed to be a convex game, and not all perturbations of it can result in a strictly convex game. In fact, we believe great care is required to ensure that the perturbations of the face games are strictly convex, thereby allowing them to be used to derive lower bounds for strictly convex games. Moreover, even if one can guarantee that the perturbation of the two face-game instances in Theorem 7 is strictly convex, they can only result in a very loose lower bound. Particularly, The two game instances in Theorem 7 are originally constructed using two strictly convex games $G_{0}$ and $G_{1}$ , whose expected rewards differ in only one coalition, denoted as $C\\subset N$ . This setup simplifies the computation of the statistical distance of the face-games which are perturbations of $G_{0}$ and $G_{1}$ corresponding to the same coalition $C$ . However, since only two game instances are used to derive the result of Theorem 7, if we employ fully-dimensional-core perturbed game instances of them, the resulting lower bound will be independent of the dimension $n$ . In other words, the finite-sample lower bound can be very loose and does not show any dependence on the dimension $n$ . ", "page_idx": 27}, {"type": "text", "text": "Secondly, generalising the approach to $\\mathrm{poly}(n)$ game instances is not straightforward, as it requires choosing $\\mathrm{poly}(n)$ coalitions to perturb the rewards of the original game $G_{0}$ , not just one coalition. This results in significant differences in the expected reward functions of the corresponding face games, as each face game is a perturbation with respect to several different coalitions. Consequently, upper-bounding the pairwise KL distance between these games is highly nontrivial and would require sophisticated exploitation of the structure of strictly convex games. ", "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our claims made in the abstract and introduction reflect the paper\u2019s contributions and scope. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We discussed the limitations of the work in the main paper and the conclusion. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We stated the assumptions in the main paper and provide the proof in the appendix. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We described our simulations and provided a link to the code available online. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We provided a link to the code available online. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We described our simulations and provided a link to the code available online. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We described our simulations and statistical significance of the experiments. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We discussed the time complexity of our algorithm. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This paper is primarily theoretical and may have limited direct impact or implications. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: We does not use existing assets. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This paper does not release new assets ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]