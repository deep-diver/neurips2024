[{"type": "text", "text": "Robust Mixture Learning when Outliers Overwhelm Small Groups ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Daniil Dmitriev1\u2217 Rares-Darius Buhai1\u2217 Stefan Tiegel1 Alexander Wolters2 ", "page_idx": 0}, {"type": "text", "text": "Gleb Novikov3 Amartya Sanyal4 David Steurer1 Fanny Yang1 ", "page_idx": 0}, {"type": "text", "text": "1ETH Zurich 2TU Munich 3Lucerne School of Computer Science and Information Technology 4University of Copenhagen \u2217Equal contribution ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study the problem of estimating the means of well-separated mixtures when an adversary may add arbitrary outliers. While strong guarantees are available when the outlier fraction is significantly smaller than the minimum mixing weight, much less is known when outliers may crowd out low-weight clusters \u2013 a setting we refer to as list-decodable mixture learning (LD-ML). In this case, adversarial outliers can simulate additional spurious mixture components. Hence, if all means of the mixture must be recovered up to a small error in the output list, the list size needs to be larger than the number of (true) components. We propose an algorithm that obtains order-optimal error guarantees for each mixture mean with a minimal list-size overhead, significantly improving upon list-decodable mean estimation, the only existing method that is applicable for LD-ML. Although improvements are observed even when the mixture is non-separated, our algorithm achieves particularly strong guarantees when the mixture is separated: it can leverage the mixture structure to partially cluster the samples before carefully iterating a base learner for list-decodable mean estimation at different scales. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Estimating the mean of a distribution from empirical data is one of the most fundamental problems in statistics. The mean often serves as the primary summary statistic of the dataset or is the ultimate quantity of interest that is often not precisely measurable. In practical applications, data frequently originates from a mixture of multiple groups (also called subpopulations) and a natural goal is to estimate the distinct means of each group separately. For example, we might like to use representative individuals to study how a complex decision or procedure would impact different subpopulations. In other applications, such as genetics [1] or astronomy [2] research, finding the means themselves can be a crucial first step towards scientific discovery. In both scenarios, the algorithm should output a list of estimates that are close to the unobservable true means. ", "page_idx": 0}, {"type": "text", "text": "However, in practice, the data may also contain outliers, for example due to measurement errors or abnormal events. We would like to find good mean estimates for all inlier groups even when the proportion of such additive adversarial contaminations is larger than some smaller groups that we want to properly represent. The central open question that motivates our work is thus: ", "page_idx": 0}, {"type": "text", "text": "What is the cost of efficiently recovering small groups that may be outnumbered by outliers? ", "page_idx": 0}, {"type": "text", "text": "More specifically, consider a scenario where the practitioner would like to recover the means of small but significant enough inlier groups which constitute at least $w_{\\mathrm{low}}\\in(0,1)$ proportion of the (corrupted) data. If $k$ is the number of such inlier groups, for all $i\\in[k]$ , we then denote by $w_{i}\\geq{w_{\\mathrm{low}}}$ the unknown weight of the $i-$ th group with mean $\\mu_{i}$ . Further, we use $\\varepsilon$ to refer to the proportion of additive contamination \u2013 the data that comes from an unknown adversarial distribution. The goal is to estimate the unknown means $\\mu_{i}$ for all $i\\in[k]$ . ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Existing works on robust mixture learning such as [3, 4] consider the problem when the fraction of additive adversarial outliers is smaller than the weight of the smallest subgroup, i.e. $\\varepsilon<w_{\\mathrm{low}}$ . However, for large outlier proportions where $\\varepsilon\\ \\geqslant\\ w_{\\mathrm{low}}$ , these algorithms are not guaranteed to recover small clusters with $w_{i}\\leqslant\\varepsilon$ . In this case, outliers can form additional spurious clusters that are indistinguishable from small inlier groups. As a consequence, generating a list of size equal to the number of components would possibly lead to neglecting the means of small groups. In order to ensure that the output contains a precise estimate for each of the small group means, it is thus necessary the estimation algorithm to provide a list whose size is strictly larger than the number of components. We call this paradigm list-decodable mixture learning (LD-ML), following the footsteps of a long line of work on list-decodable learning (see Sections 2 and 5). ", "page_idx": 1}, {"type": "text", "text": "Specifically, the main challenge in LD-ML is to provide a short list that contains good mean estimates for all inlier groups. We first note that there is a minimum list size the algorithm necessarily has to output to guarantee that all groups are recovered. For example, consider an outlier distribution that includes several copies of the smallest inlier group distribution with means spread out throughout the domain. Since inlier groups are indisntinguishable from spurious outlier ones, the shortest list that includes means of all inlier groups must be of size at least $\\begin{array}{r}{|L|\\geqslant k+\\frac{\\varepsilon}{\\operatorname*{min}_{i}w_{i}}}\\end{array}$ n\u03b5w . Here,min\u03b5w can be interpreted as the minimal list-size overhead that is necessary due to \"caring\" about groups with weight smaller than $\\varepsilon$ . The key question is hence how good the error guarantees of an LD-ML algorithm can be when the list size overhead stays close to $\\frac{\\varepsilon}{\\operatorname*{min}{w_{i}}}$ , while being agnostic to $w_{i}$ aside from the knowledge of $w_{\\mathrm{low}}$ . Furthermore, we are interested in computationally efficient algorithms for LD-ML, especially when dealing with high-dimensional data. ", "page_idx": 1}, {"type": "text", "text": "To the best of our knowledge, the only existing efficient algorithms that are guaranteed to recover inlier groups with weights $w_{i}\\leqslant\\varepsilon$ are list-decodable mean estimation (LD-ME) algorithms. LD-ME algorithms model the data as a mixture of one inlier and outlier distribution with weights $\\alpha\\leqslant1/2$ and $1-\\alpha$ respectively. Provided with the weight parameter $\\alpha$ , they output a list that contains an estimate close to the inlier mean with high probability. However, for the LD-ML setting, the inlier weights $w_{i}$ are not known and we would have to use LD-ME algorithms with $w_{\\mathrm{low}}$ as weight estimates for each group. This leads to suboptimal error in particular for large groups, that hence (somewhat counter intuitively) would have to \"pay\" for the explicit constraint to recover small groups. Furthermore, even if LD-ME were provided with $w_{i}$ , by design it would treat inlier points from other components also as outliers, unnecessarily inflating the fraction of outliers to $1-w_{i}$ instead of $\\varepsilon$ . ", "page_idx": 1}, {"type": "text", "text": "Contributions In this paper, we propose an algorithm that (i) correctly estimates the weight of each component only given a lower bound and (ii) does not overestimate proportion of outliers when components are well-separated. In particular, we construct a meta-algorithm that uses mean estimation algorithms as base learners that are designed to deal with adversarial corruptions. This meta-algorithm inherits guarantees from the base learner and any improvement of the latter translates to better results for LD-ML. For example, if the base learner runs in polynomial time, so does our meta-algorithm. Our approach of using the output of weak base learners to achieve better performance is reminiscent of the boosting paradigm that is common in machine learning practice. ", "page_idx": 1}, {"type": "text", "text": "Our algorithm achieves significant improvements in error and list-size guarantees for multiple settings. For ease of comparison, we summarize error improvements for inlier Gaussian mixtures in Table 1 (see Theorem 3.3 for the general result regarding distributions with bounded moments). The main focus of our contributions is represented in the second row; that is the setting where outliers outnumber some inlier groups with weight $w_{j}\\leqslant\\varepsilon$ and the inlier components are well-separated, i.e., $\\begin{array}{r}{\\|\\mu_{i}-\\mu_{j}\\|\\gtrsim^{1}\\sqrt{\\log\\frac{1}{w_{\\mathrm{low}}}}}\\end{array}$ , where $\\mu_{i}$ \u2019s are the inlier component means. As we mentioned before, robust mixture learning algorithms, such as [4, 7], are not applicable here and the best error guarantees in prior work is achieved by an LD-ME algorithm, e.g. from [3]. While its error bounds are of order $O(\\sqrt{\\log{\\frac{1}{w_{\\mathrm{low}}}}})$ for a list size of $O(\\frac{1}{w_{\\mathrm{low}}})$ , our approach guarantees error $O(\\sqrt{\\log{\\frac{\\varepsilon}{w_{i}}}})$ for a list size 1We adopt the following standard notation: $f\\lesssim g,f=O(g)$ , and $g=\\Omega(f)$ mean that $f\\leqslant C g$ for some universal constant $C>0$ .O -notation hides polylogarithmic terms. ", "page_idx": 1}, {"type": "table", "img_path": "TrXV4dMDcG/tmp/e67517b6456a946ab955a592c3bf1fa876946d10b4bbbbaf3cf57c96d352307a.jpg", "table_caption": [], "table_footnote": ["Table 1: For a mixture of Gaussian components $\\overline{{\\mathbb{N}(\\mu_{i},I_{d})}}$ , we show upper and lower bounds for the error of the $i$ -component given a output list $L$ (of the respective algorithm) $\\mathrm{min}_{\\hat{\\mu}\\in L}\\|\\hat{\\mu}-\\mu_{i}\\|$ . When the error doesn\u2019t depend on $i$ , all means have the same error guarantee irrespective of their weight. Note that depending on the type of inlier mixture, different methods in [3] are used as the \u2019best prior work $\\ '_{\\cdot}$ robust mixture learning for the first row and list-decodable mean estimation for the rest. "], "page_idx": 2}, {"type": "text", "text": "of $\\begin{array}{r}{k+O(\\frac{\\varepsilon}{w_{\\mathrm{low}}})}\\end{array}$ . Remarkably, we obtain the same error guarantees as if an oracle would run LD-ME on each inlier group with the correct weight $w_{i}$ separately (with outliers). Hence, the only cost for recovering small groups is the increased list-size overhead of order $\\begin{array}{r}{O(\\frac{\\varepsilon}{w_{\\mathrm{low}}})}\\end{array}$ . Further, a sub-routine in our meta-algorithm also obtains novel guarantees under no separation assumption, as shown in the third row of Table 1. This algorithm achieves the same error guarantees for similar list size as a base learner that knows the correct weights of the inlier components. ", "page_idx": 2}, {"type": "text", "text": "Based on a reduction argument from LD-ME to LD-ML, we also provide information-theoretic (IT) lower bounds for LD-ML. If the LD-ME base learners achieve the IT lower bound (possible for inlier Gaussian mixtures), so does our LD-ML algorithm. In synthetic experiments, we implement our meta-algorithm with the LD-ME base learner from [8] and show clear improvements compared to the only prior method with guarantees, while being comparable or better than popular clustering methods such as $\\mathbf{k}$ -means and DBSCAN for various attack models. ", "page_idx": 2}, {"type": "text", "text": "2 Settings ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We now introduce the learning settings that appear in the paper. Let $d\\in\\mathbb{N}_{+}$ be the ambient dimension of the data and $k\\in\\mathbb{N}_{+}$ be the number of mixture components (inlier groups/clusters). ", "page_idx": 2}, {"type": "text", "text": "2.1 List-decodable mixture learning under adversarial corruptions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We focus on mixtures that consist of distributions that are sufficiently bounded in the following sense. Definition 2.1. Let $t\\,\\in\\,\\mathbb{N}_{+}$ be even and let $D(\\mu)$ be a distribution on $\\mathbb{R}^{d}$ with mean $\\mu$ . We say that $D(\\mu)$ has sub-Gaussian $t$ -th central moments if for all even $s\\leqslant t$ and for every $\\boldsymbol{v}\\in\\mathbb{R}^{d}$ with $\\|v\\|=1$ $,\\mathbb{E}_{x\\sim D}\\left\\langle x-\\mu,v\\right\\rangle^{s}\\leqslant(s-1)!!$ . ", "page_idx": 2}, {"type": "text", "text": "This class of distributions is closely related to commonly studied distributions in the literature (see, e.g., [5]) with bounded $t$ -th moment. Our requirement for the boundedness of all moments $s\\leqslant t$ stems from the fact that our algorithm should adapt to unknown and possibly non-uniform mixture weights. ", "page_idx": 2}, {"type": "text", "text": "We assume that we are given samples from a corrupted $d$ -dimensional mixture of $k$ inlier distributions $D_{i}(\\mu_{i})$ satisfying Definition 2.1, where the mixture is defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{X}=\\sum_{i=1}^{k}w_{i}D_{i}(\\mu_{i})+\\varepsilon Q,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "and ik= $\\begin{array}{r}{\\sum_{i=1}^{k}w_{i}+\\varepsilon\\,=\\,1}\\end{array}$ , where for all $i\\,=\\,1,\\ldots,k$ , it holds that $w_{i}\\;\\geq\\;w_{\\mathrm{low}}$ . Further, an $\\varepsilon\\,>\\,0$ proportion of the data comes from an outlier distribution $Q$ chosen by the adversary with full knowledge of our algorithm and inlier mixture. Samples drawn from $D_{i}(\\mu_{i})$ constitute the $i^{\\mathrm{th}}$ inlier cluster. The goal in mixture learning under corruptions as in Eq. (2.1), is to design an algorithm that takes in i.i.d. samples from $\\mathcal{X}$ and outputs a list $L$ , such that for each $i\\in[k]$ , there exists $\\hat{\\mu}\\in L$ with small estimation error $\\|\\mu_{i}-\\hat{\\mu}\\|$ . ", "page_idx": 2}, {"type": "text", "text": "To the best of our knowledge, we are the first to study the list-decodable mixture learning problem (LD-ML) that considers the case of large fractions of outliers $\\varepsilon\\geqslant\\operatorname*{min}_{i}w_{i}$ and the goal is to achieve small estimation errors while the list size $|L|$ remains small. While in robust estimation problems, the fractions of inliers and outliers are usually provided to the algorithm, in mixture learning, the mixture proportions are explicit quantities of interest. Throughout the paper, we hence assume that both the true weights $w_{i}$ of the mixture and the fraction of outliers $\\varepsilon$ are unknown. Instead, by definition in Eq. (2.1), we assume knowledge of a valid lower bound $w_{\\mathrm{low}}\\leqslant\\operatorname*{min}_{i}w_{i}$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Note that when $\\varepsilon\\lesssim\\operatorname*{min}_{i}w_{i}$ , the problem is known as robust mixture learning and can be solved with list size $|L|=k$ as discussed in [3, 4, 7]. However, algorithms for robust mixture learning fail when the fraction of outliers becomes comparable to the inlier group size. In the presence of \u201cspurious\u201d adversarial clusters, it is information-theoretically impossible to output a list $L$ , such that (i) ${\\big|}L{\\big|}=k$ and (ii) $L$ contains precise estimate for each true mean. ", "page_idx": 3}, {"type": "text", "text": "2.2 Mean estimation under adversarial corruptions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In order to solve LD-ML, we use mean estimation procedures that have provable guarantees under adversarial contamination. Mean estimation can be viewed as a particular case of the mixture learning problem in Eq. (2.1) with $k=1$ , the fraction of inliers $\\alpha=w_{1}$ and the fraction of outliers $\\varepsilon=1-\\alpha$ . The mean estimation algorithms we use to solve LD-ML with $w_{\\mathrm{low}}$ need to exhibit guarantees under a stronger adversarial model, where the adversary can also replace a small fraction (depending on $w_{\\mathrm{low.}}$ ) of the inlier points; see details in Definition B.1. This is a special case of the general contamination model as opposed to the slightly more benign additive contamination model in Eq. (2.1). For different regimes of $\\alpha$ we use black-box learners that solve corresponding regime when provided with $\\alpha$ . ", "page_idx": 3}, {"type": "text", "text": "Robust mean estimation When the majority of points are inliers, we are in the RME setting. Robust statistics has studied this setting with different corruption models and efficient algorithms are known to achieve information-theoretically optimal error guarantees (see Section 5). ", "page_idx": 3}, {"type": "text", "text": "List-decodable mean estimation When inliers form a minority, we are in the list-decodable setting and are required to return a list instead of a single estimate. We refer to this setting as cor-kLD (corrupted known list-decoding). For mixture learning, $\\alpha$ is usually unknown and we need to solve the cor-aLD (corrupted agnostic list-decoding) problem (i.e., $\\alpha$ is not provided, but instead a lower bound $\\alpha_{\\mathrm{low}}\\in[w_{\\mathrm{low}},\\alpha]$ is given to the algorithm). Finally, when only additive adversarial contamination is present, as in Eq. (2.1), we recover the standard list-decoding setting studied in prior works (see Section 5) that we call sLD (simple list-decoding). In Appendix G we show that two algorithms designed for sLD also exhibit guarantees for cor-kLD for any $w_{\\mathrm{low}}$ . ", "page_idx": 3}, {"type": "text", "text": "3 Main results ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now present our main results for list-decodable and robust mixture learning defined in Section 2. In Section 3.1, we provide algorithmic upper bounds and information-theoretic lower bounds. For the special case of spherical Gaussian mixtures, we show in Section 3.1 that we achieve optimality. Our results are constructive as we provide a meta-algorithm for which these bounds hold. ", "page_idx": 3}, {"type": "text", "text": "As depicted in Figure 1, our meta-algorithm (Algorithm 2) is a two-stage process. The outer stage (Algorithm 6) reduces the problem to mean estimation by leveraging the mixture structure and splitting the data into a small collection $\\upsigma$ of sets $T$ . Each set $T\\in\\mathcal{T}$ should (i) contain at most one inlier cluster (and few samples from other clusters) and (ii) the total number of outliers across all sets should be at most $O(\\varepsilon n)$ . We then run the inner stage (Algorithm 3) on sets $T$ , which outputs a mean estimate for the inlier cluster in $T$ . First, a cor-aLD algorithm identifies the weight of the inlier cluster and returns the result of a cor-kLD base learner with this weight. Then, if the weight is large, we improve the error via an RME base learner. A careful flitering procedure in both stages achieves the significantly reduced list size and better error guarantees. We require the base learners to satisfy the following set of assumptions. ", "page_idx": 3}, {"type": "text", "text": "Assumption 3.1 (Mean-estimation base learners for mixture learning). Let $t$ be an even integer and consider the corruption setting defined in Definition B.1. Further, let the inlier distribution $D(\\mu^{*})\\in\\mathcal{D}$ where $\\mathcal{D}$ is the the family of distributions satisfying Definition 2.1 for $t$ . We assume that (a) for $\\alpha\\;\\in\\;[w_{\\mathrm{low}},1/3]$ in the cor-kLD regime, there exists an algorithm $\\mathcal{A}_{\\mathrm{kLD}}$ that uses $N_{L D}(\\alpha)$ samples and $T_{L D}(\\alpha)$ time to output a list of size bounded by $1/\\alpha^{O(1)}$ that with probability at least $1/2$ contains some $\\hat{\\mu}$ with $\\|\\hat{\\mu}-\\mu^{*}\\|\\leqslant f(\\alpha)$ , where $f$ is non-increasing. ", "page_idx": 3}, {"type": "image", "img_path": "TrXV4dMDcG/tmp/47840c2e7e06a43008529f9d789a971b1bd691a2b843ce1a7482b9988e6e45e4.jpg", "img_caption": ["Figure 1: Schematic of the meta-algorithm (Algorithm 2) underlying Theorem 3.3 "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "(b) for $\\alpha\\in[1-\\varepsilon_{\\mathrm{RME}},1]$ , with $0\\leqslant\\varepsilon_{\\mathrm{RME}}\\leqslant1/2-2w_{\\mathrm{low}}^{2}$ in the RME regime, there exists an RME algorithm $\\boldsymbol{\\mathcal{A}}_{R}$ that uses $N_{R}(\\alpha)$ samples and $\\bar{T_{R}}\\bar{(}\\alpha)$ time to output with probability at least $1/2$ some $\\hat{\\mu}$ with $\\|\\hat{\\mu}-\\mu^{*}\\|\\leqslant g(\\alpha)$ , where $g$ is non-increasing. ", "page_idx": 4}, {"type": "text", "text": "Note that the sample and time-complexity functions such as $N_{L D}$ and $T_{L D}$ , might depend on $t$ , for example growing as $d^{t}$ . We emphasize that (i) the guarantees of our meta-algorithm depend on the guarantees of the base learners and (ii) we only require the base learners to work in the wellstudied setting with known fraction of inliers. Corollary 3.4 uses known base learners for Gaussian distributions achieving information-theoretically optimal error bounds. There also exists base learners for distributions beyond Gaussians, such as bounded covariance or log-concave distributions, see, e.g. [9]. ", "page_idx": 4}, {"type": "text", "text": "3.1 Upper bounds for list-decodable mixture learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Key quantities that appear in our error bounds are the relative proportion of inliers $\\tilde{w}_{i}$ and outliers $\\tilde{\\varepsilon}_{i}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{w}_{i}=\\frac{w_{i}}{w_{i}+\\varepsilon+w_{\\mathrm{low}}^{2}}\\quad\\mathrm{and}\\quad\\tilde{\\varepsilon}_{i}=1-\\tilde{w}_{i}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "These quantities reflect that each set $T$ in the inner stage contains at most one inlier cluster and a small (\u2272wl2ow) fraction of points from other inlier clusters. We now present a simplified version of our main result in Theorem 3.3 (see Theorem C.1 for the detailed result) that allows for a more streamlined presentation of the results using the following \u2018well-behavedness\u2019 of $f$ and $g$ . ", "page_idx": 4}, {"type": "text", "text": "Assumption 3.2. Let $f,g$ be as defined in Assumption 3.1. For some $C>0$ , we assume (i) $\\varepsilon_{\\mathrm{RME}}\\geqslant$ 0.01, (ii) $\\forall x\\in(0,1/3]$ , $f(x/2)\\leqslant C f(x)$ , and (iii) $\\forall x\\in[0.99,1]$ , $g(x-(1-x)^{2})\\leqslant C g(x)$ . ", "page_idx": 4}, {"type": "text", "text": "We are now ready to state the main result of the paper. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.3. Let $d,k\\in\\mathbb{N}_{+}$ , $w_{\\mathrm{low}}\\in(0,1/2]$ , and t be an even integer. Let $\\mathcal{X}$ be a $d$ -dimensional mixture distribution following Eq. (2.1). Let $\\mathcal{A}_{\\mathrm{kLD}}$ and $\\boldsymbol{\\mathcal{A}}_{R}$ satisfy Assumptions 3.1 and 3.2 for some even t. Further, suppose that $\\|\\mu_{i}-\\mu_{j}\\|\\gtrsim\\sqrt{t}(1/w_{\\mathrm{low}})^{4/t}+f(w_{\\mathrm{low}}).$ for all $i\\neq j\\in[k]$ . ", "page_idx": 4}, {"type": "text", "text": "Then there exists an algorithm that, given $\\mathrm{~poly}(d,1/w_{\\mathrm{low}})\\cdot(N_{L D}(w_{\\mathrm{low}})+N_{R}(w_{\\mathrm{low}}))$ i.i.d. samples from $\\mathcal{X}$ as well as $d,\\,k_{i}$ , $w_{\\mathrm{low}}$ , and $t$ , runs in time p $\\mathrm{oly}(d,1/w_{\\mathrm{low}})\\cdot(T_{L D}(w_{\\mathrm{low}})+T_{R}(w_{\\mathrm{low}}))$ and with probability at least 1 \u2212wlOo(w1) outputs a list $L$ of size $|L|\\leqslant k+O(\\varepsilon/w_{\\mathrm{low}})$ where, for each $i\\in[k]$ , there exists $\\hat{\\mu}\\in L$ such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\|\\hat{\\mu}-\\mu_{i}\\|=O\\left(\\operatorname*{min}_{1\\leqslant t^{\\prime}\\leqslant t}\\sqrt{t^{\\prime}}(1/\\tilde{w}_{i})^{1/t^{\\prime}}+f(\\operatorname*{min}(\\tilde{w}_{i},1/3))\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "If the relative weight of the $i$ -th cluster is large, i.e., $\\tilde{\\varepsilon}_{i}\\leqslant0.001$ , then the error is further bounded by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\|\\hat{\\mu}-\\mu_{i}\\|=O\\left(g(\\tilde{w}_{i})\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The proof together with a more general statement, Theorem C.1, can be found in Appendix C. ", "page_idx": 4}, {"type": "text", "text": "Note that for a mixture setting with $k\\geqslant2$ , the assumption $w_{\\mathrm{low}}\\,\\leqslant\\,1/k\\,\\leqslant\\,1/2$ is automatically fulfliled. Also, for large weights $\\tilde{w}_{i}$ such that $\\log(1/\\tilde{w}_{i})\\ll t$ , the $t^{\\prime}$ that minimizes $\\sqrt{t^{\\prime}}(1/\\tilde{w}_{i})^{1/t^{\\prime}}$ is smaller than $t$ , and for small weights the minimizer is $t^{\\prime}=t$ . ", "page_idx": 4}, {"type": "text", "text": "Gaussian case For Gaussian inlier distributions, LD-ME and RME base learners with guarantees for Assumption 3.1 have already been developed in prior work. We can thus readily use them in the meta-algorithm to arrive at the following statement with the relative proportions defined in Eq. (3.1). ", "page_idx": 5}, {"type": "text", "text": "Corollary 3.4 (Gaussian case). Let $d,k,w_{\\mathrm{low}}$ and t be as in Theorem 3.3. Let $\\mathcal{X}$ be as in Eq. (2.1) with $D_{i}(\\mu_{i})=\\mathbb{N}(\\mu_{i},I_{d})$ with $\\mu_{i}\\,{\\dot{s}}$ satisfying $\\|\\mu_{i}-\\mu_{j}\\|\\gtrsim\\sqrt{\\log{1/w_{\\mathrm{low}}}}$ for all $i\\neq j\\in[k]$ . There exists an algorithm that for $t=O(\\log1/w_{\\mathrm{low}}),$ , given $N=\\mathrm{poly}(d^{t},(1/w_{\\mathrm{low}})^{t})$ i.i.d. samples from $\\mathcal{X}$ and $w_{\\mathrm{low}}$ , runs in $\\mathrm{poly}(N)$ time and outputs a list $L$ such that with high probability $|L|=k+O(\\varepsilon/w_{\\mathrm{low}})$ and, for all $i\\in[k]$ , there exists $\\hat{\\mu}\\in L$ such that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\|\\hat{\\mu}-\\mu_{i}\\|=O\\left(\\sqrt{\\log{1/\\tilde{w}_{i}}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "If the relative weight of the $i$ -th cluster is large, i.e. $\\tilde{\\varepsilon}_{i}\\leqslant0.001$ , then the error is further bounded by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Vert{\\hat{\\mu}}-\\mu_{i}\\Vert=O\\left(\\tilde{\\varepsilon}_{i}\\sqrt{\\log{1/\\tilde{\\varepsilon}_{i}}}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proof. Theorem 6.12 from [5] provides an LD-ME algorithm $\\mathcal{A}_{\\mathrm{kLD}}$ achieving error $f(\\alpha)\\ \\leqslant$ $O(\\sqrt{t^{\\prime}}(1/\\alpha)^{1/t^{\\prime}})$ for all $t^{\\prime}\\leqslant t$ . The sample and time complexity scale as $\\mathrm{poly}(d^{t},(1/\\alpha)^{t})$ . Also, Theorem 5.1 from [10] provides a robust mean estimation algorithm $\\boldsymbol{\\mathcal{A}}_{R}$ such that for a small enough constant fraction of outliers $\\varepsilon=1-\\alpha$ it achieves error $g(\\alpha)=O((1-\\alpha)\\sqrt{\\log{1/(1-\\alpha)}})$ with sample complexity $\\tilde{\\Omega}(d/\\varepsilon^{2})$ . Using these $\\mathcal{A}_{\\mathrm{kLD}}$ and $\\boldsymbol{\\mathcal{A}}_{R}$ , we recover the desired bounds. \u53e3 ", "page_idx": 5}, {"type": "text", "text": "Comparison with prior work We now compare our result with the only previous method that can achieve guarantees in the LD-ML setting with unknown $w_{i}$ . As discussed in [3], algorithms for the simple list-decoding model with $\\alpha=w_{\\mathrm{low}}$ can be used for LD-ML by viewing a single mixture component as the \u201cground truth\u201d distribution and effectively treating all other inlier components and original outliers as outliers. Besides requiring a much larger list size of $O(1/w_{\\mathrm{low}})\\gg k\\Bar{+}O(\\varepsilon/w_{\\mathrm{low}})$ and error $O(\\sqrt{\\log{1/w_{\\mathrm{low}}}})$ , this approach has two drawbacks that manifest in the suboptimal guarantees: 1) For larger clusters $i$ with $w_{i}\\gg w_{\\mathrm{low}}$ , LD-ME only achieves an error $O\\left(\\sqrt{\\log{1/w_{\\mathrm{low}}}}\\right)$ . Our result, even without separation assumption, achieves a sharper error bound $O\\left({\\sqrt{\\log{1/w_{i}}}}\\right)$ . 2) When the mixture is separated, LD-ME cannot exploit the structure since it still models the data as $w_{\\mathrm{low}}\\mathcal{N}(\\mu_{i},I_{d})+(1\\stackrel{-}{-}w_{\\mathrm{low}})Q$ for each $i$ , so that the algorithm inevitably treats all other true components as outliers. This results in the error $O\\left({\\sqrt{\\log{1/w_{\\mathrm{low}}}}}\\right)\\gg O\\left({\\sqrt{\\log{1/\\tilde{w}_{i}}}}\\right)=O(1)$ (when $\\varepsilon\\sim w_{i}\\ll1)$ ). We refer to Appendix A for further illustrative examples. As a simple example, consider the uniform inlier mixtur\u221ae with $\\varepsilon=w_{i}=1/(k+1)$ , where $k$ is large. In this case, previous results have error guarantees $O({\\sqrt{\\log k}})$ , while we obtain error $O(1)$ . ", "page_idx": 5}, {"type": "text", "text": "Separation assumption For the problem of learning mixture models, a sep\u221aaration assumption is common in the literature [3, 9, 11, 12]. We require separation $\\|\\mu_{i}-\\mu_{j}\\|\\gtrsim\\sqrt{t}(1/w_{\\mathrm{low}})^{4/t}$ , which we believe to be sub-optimal for the case of finite $t$ . In cases when $\\varepsilon$ is small (namely $\\varepsilon\\lesssim w_{\\mathrm{low}})$ , there exist prior works on clustering allowing smaller separation. Specifically, when $t=2$ , a recent work [13] only requires $\\begin{array}{r}{\\|\\mu_{i}-\\mu_{j}\\|\\stackrel{\\cdot}{\\approx}1/\\sqrt{w_{\\mathrm{low}}}}\\end{array}$ . For a general $t\\geqslant4$ , [9] succeeds under separation $(1/w_{\\mathrm{low}})^{2/t}$ . We leave the possible relaxation of the separation requirement in the case of general $t$ and large $\\varepsilon$ for the future work. ", "page_idx": 5}, {"type": "text", "text": "In the Gaussian case we require separation $\\|\\mu_{i}-\\mu_{j}\\|\\gtrsim\\sqrt{\\log{1/w_{\\mathrm{low}}}}$ , which is optimal in the uniform $(w_{i}=1/k)$ case. Indeed, without the separation assumption, even in the noiseless uniform Gaussian case, [6] shows that no efficient algorithm can obtain error asymptotically better than $\\Omega(\\sqrt{\\log{1/w_{i}}})$ . In Corollary B.5, we prove that the inner stage (Algorithm 3) of our algorithm, without knowledge of $w_{i}$ and separation assumption, achieves with high probability matching error guarantees $O(\\sqrt{\\log{1/w_{i}}})$ with list size bounded by $O(1/w_{\\mathrm{low}})$ . ", "page_idx": 5}, {"type": "text", "text": "3.2 Information-theoretical lower bounds and optimality ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Next, we present information-theoretical lower bounds for list-decodable mixture learning on wellseparated distributions $\\mathcal{X}$ as defined in Eq. (2.1). We show that our error is optimal as long as the list size is required to be small. Our proof uses a simple reduction technique and leverages established lower bounds in [3] for the list-decodable mean estimation model (sLD in Section 2). ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 Outer stage, informal (see Algorithm 6) ", "page_idx": 6}, {"type": "text", "text": "Input: $X$ , $w_{\\mathrm{low}}$ , $\\Delta$ , and sLD algorithm $\\mathcal{A}_{\\mathrm{sLD}}$ .   \nOutput: Collection of sets $\\upsigma$ .   \n1: $L\\gets(\\hat{\\mu}_{1},\\dots,\\hat{\\mu}_{M}):=\\mathcal{A}_{\\mathrm{sLD}}(X)$ with $w_{\\mathrm{low}}$ ;   \n2: while $L\\neq\\emptyset$ do   \n3: for $\\hat{\\mu}\\in L$ do   \n4: compute for an appropriate distance function $d$   \n$S_{\\hat{\\mu}}^{(1)}=\\{x\\in X\\mid d(x,{\\hat{\\mu}})\\leqslant\\Delta\\}\\,,\\quad S_{\\hat{\\mu}}^{(2)}=\\{x\\in X\\mid d(x,{\\hat{\\mu}})\\leqslant3\\Delta\\}$ 5: if for all $\\hat{\\lambda},|S_{\\hat{\\mu}}^{(2)}|>2|S_{\\hat{\\mu}}^{(1)}|$ then add $X$ to $\\upsigma$ and update $L\\gets\\emptyset$   \n6: else   \n7: $\\begin{array}{r l}&{\\overset{\\cdot}{\\mu}\\leftarrow\\arg\\!\\operatorname*{max}_{|S_{\\hat{\\mu}}^{(2)}|\\leqslant2|S_{\\hat{\\mu}}^{(1)}|}|S_{\\hat{\\mu}}^{(1)}|}\\\\ &{\\mathrm{add}\\;S_{\\tilde{\\mu}}^{(2)}\\overset{\\quad}{\\mathrm{to}}\\mathcal{T}}\\\\ &{X\\leftarrow X\\setminus S_{\\tilde{\\mu}}^{(1)}}\\end{array}$   \n8:   \n9:   \n10: return $\\upsigma$ ", "page_idx": 6}, {"type": "text", "text": "Proposition 3.5 (Information-theoretic lower bounds). Let $\\mathcal{A}$ be an algorithm that, given access to $\\mathcal{X}$ , outputs a list $L$ that, with probability $\\geqslant1/2.$ , for each $i\\in[k]$ contains $\\hat{\\mu}\\in L$ with $\\|\\hat{\\mu}-\\mu_{i}\\|\\leqslant\\beta_{i}$ . (a) Consider the case with $\\|\\mu_{i}-\\mu_{j}\\|\\gtrsim(1/w_{\\mathrm{low}})^{4/t}$ for $i\\neq j\\in[k],\\,D_{i}(\\mu_{i})$ having $t$ -th bounded sub-Gaussian central moments and $\\beta_{i}\\leqslant C(1/w_{\\mathrm{low}})^{1/t}$ for each $i\\in[k]$ . If for some $s\\in[k]$ it holds that $w_{s}\\leqslant\\varepsilon$ , then algorithm $\\mathcal{A}$ must either have error bound $\\beta_{s}=\\Omega((1/\\tilde{w}_{i})^{1/t})\\,o r\\,|L|\\geqslant k+d-1$ . $(b)$ Consider the case with $\\|\\mu_{i}-\\mu_{j}\\|\\gtrsim\\sqrt{\\log{1/w_{\\mathrm{low}}}}$ for $i\\neq j\\in[k],$ , $D_{i}(\\mu_{i})=\\mathcal{N}(\\mu_{i},I_{d})$ and $\\beta_{i}\\leqslant C\\sqrt{\\log{1/w_{\\mathrm{low}}}}$ for each $i\\in[k]$ . If for some $s\\in[k]$ it holds that $w_{s}\\leqslant\\varepsilon$ , then algorithm $\\mathcal{A}$ must either have error bound $\\beta_{s}=\\Omega(\\sqrt{\\log{1/\\tilde{w}_{i}}})$ or $|L|\\geqslant k+\\operatorname*{min}\\{2^{\\Omega(d)},(1/\\tilde{w}_{i})^{\\omega(1)}\\}$ . ", "page_idx": 6}, {"type": "text", "text": "In the Gaussian inlier case, Corollary 3.4 together with Proposition 3.5 imply optimality of our meta-algorithm. Indeed, if one plugs in optimal base learners (as in the proof of Corollary 3.4), we obtain error guarantee that matches lower bound. In particular, \u201cexponentially\u201d larger list size is necessary for asymptotically smaller error. For inlier components with bounded sub-Gaussian moments, [3] obtains information-theoretically (nearly-)optimal LD-ME base learners. ", "page_idx": 6}, {"type": "text", "text": "Furthermore, in [3], formal evidence of computational hardness was obtained (see their Theorem 5.7, which gives a lower bound in the statistical query model introduced by [14]) that suggests obtaining error $\\Omega_{t}((1/\\tilde{w}_{s})^{1/t})$ requires running time at least $d^{\\Omega(t)}$ . This was proved for Gaussian inliers and the running time matches ours up to a constant in the exponent. ", "page_idx": 6}, {"type": "text", "text": "4 Algorithm sketch ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now sketch our meta-algorithm specialized to the case of separated Gaussian components $\\mathcal{N}(\\mu_{i},I_{d})$ and provide intuition for how it achieves the guarantees in Corollary 3.4. In this section, we only discuss how to obtain an error of $O(\\sqrt{\\log{1/\\tilde{w}_{i}}})$ for each mean when $\\varepsilon\\gtrsim\\operatorname*{min}_{i}w_{i}$ . We refer to Appendix D for how to achieve the refined error guarantee of $O(\\tilde{\\varepsilon}_{i}\\sqrt{\\log{1/\\tilde{\\varepsilon}_{i}}})$ when $\\tilde{\\varepsilon}_{i}$ is small. ", "page_idx": 6}, {"type": "text", "text": "As discussed in Section 3.1, running an out-of-the-box LD-ME algorithm for the sLD problem on our input with parameter $\\alpha=\\,w_{\\mathrm{low}}$ would give sub-optimal guarantees. In contrast, our twostage Algorithm 2, equipped with the appropriate cor-kLD and RME base learners as depicted in Figure 1, obtains for each component an error guarantee that is as good as if we had access to the samples only from this component and from the outliers. We now give more details about the outer stage, Algorithm 1, and inner stage, Algorithm 3, and describe on a high-level how they contribute to a short output list with optimal error bound in Corollary 3.4 for large outlier fractions. ", "page_idx": 6}, {"type": "text", "text": "4.1 Inner stage: list-decodable mean estimation with unknown inlier fraction ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now describe how to use a black-box cor-kLD algorithm to obtain a list-decoding algorithm $\\mathcal{A}_{\\mathrm{aLD}}$ for the cor-aLD mean-estimation setting with access only to $\\alpha_{\\mathrm{low}}~\\leqslant~\\alpha$ . $\\mathcal{A}_{\\mathrm{aLD}}$ is used in the proof of Corollary B.5 and plays a crucial role (see Figure 1) in our meta-algorithm. In particular, it deals with the unknown weight of the inlier distribution in each set returned by the outer stage. Note that estimating $\\alpha$ from the input samples is impossible by nature. Indeed, we cannot distinguish between potential outlier clusters of arbitrary proportion $\\leqslant1-\\alpha$ and the inlier component. Underestimating the size of a large component would inevitably lead to a suboptimal error guarantee. We now show how to overcome this challenge and achieve an error guarantee $O({\\sqrt{\\log{1/\\alpha}}})$ for a list size $1+O((1-\\alpha)/\\alpha_{\\mathrm{low}})$ for the cor-aLD setting. Here we only outline our algorithm and refer to Appendix D for the details. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Algorithm 3 first produces a large list of estimates corresponding to many potential values of $\\alpha$ and then prunes it while maintaining a good estimate in the list. In particular, for each ${\\hat{\\alpha}}\\in A:=$ $\\{\\alpha_{\\mathrm{low}},2\\bar{\\alpha_{\\mathrm{low}}},\\ldots,\\lfloor1/(3\\alpha_{\\mathrm{low}})\\rfloor\\alpha_{\\mathrm{low}}\\}$ , we run $\\mathcal{A}_{\\mathrm{kLD}}$ with parameter $\\hat{\\alpha}$ to obtain a list of means. We append $\\hat{\\alpha}$ to each mean in the list and obtain a list of pairs $(\\hat{\\mu},\\hat{\\alpha})$ . We concatenate these lists of pairs for all $\\hat{\\alpha}$ and obtain a list $L$ of size $O(1/\\alpha_{\\mathrm{low}}^{2})$ . By design, one element of $A$ is close to the true $\\alpha$ , so the list $L$ contains at least one $\\hat{\\mu}$ that is $O({\\sqrt{\\log{1/\\alpha}}})$ -close \u2014 the error guarantee that we aim for \u2014 and there is indeed at least an $\\alpha$ -fraction of samples near $\\hat{\\mu}$ . We call such a hypothesis \u201cnearby\". ", "page_idx": 7}, {"type": "text", "text": "Finally, we prune this concatenated list by verifying for each $\\hat{\\mu}$ whether there is indeed an $\\hat{\\alpha}$ -fraction of samples \u201cnot too far\" from it. This is similar to pruning procedures with known $\\alpha$ proposed in prior work (see Proposition B.1 in [3]). Our procedure (i) never discards a \u201cnearby\" hypothesis, and outputs a list where (ii) every hypothesis contains a sufficient number of points close to it and (iii) all hypotheses are separated. Property (i) implies that the final error is $O({\\sqrt{\\log{1/\\alpha}}})$ and properties (ii) and (iii) imply list size bound $1+O((1\\stackrel{-}{-}\\alpha)/\\alpha_{\\mathrm{low}})$ . Note that when $\\alpha<\\alpha_{\\mathrm{low}}$ , the list size can be simply upper bounded by ${\\cal O}(1/\\alpha_{\\mathrm{low}})$ , see Remark B.4. ", "page_idx": 7}, {"type": "text", "text": "4.2 Two-stage meta-algorithm ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Even though we could run $\\mathcal{A}_{\\mathrm{aLD}}$ on the entire dataset with $\\alpha_{\\mathrm{low}}=w_{\\mathrm{low}}$ , we would only achieve an error for the $i^{\\mathrm{th}}$ inlier cluster mean of $O(\\sqrt{\\log{1/w_{i}}})$ \u2013 which can be much larger than $O(\\sqrt{\\log{1/\\tilde{w}_{i}}})$ \u2013 for a list of size ${\\cal O}(1/w_{\\mathrm{low}})$ . While $\\mathcal{A}_{\\mathrm{aLD}}$ takes into account the unknown weight of the clusters, it still treats other inlier clusters as outliers. We now show that if the outer stage Algorithm 1 of our meta-algorithm Algorithm 2 separates the samples into a not-too-large collection $\\upsigma$ of sets with certain properties, running $\\mathcal{A}_{\\mathrm{aLD}}$ separately on each of the sets can lead to the desired guarantees. In particular, let us assume that $\\upsigma$ consists of potentially overlapping sets such that: ", "page_idx": 7}, {"type": "text", "text": "(1) For each inlier cluster $C^{*}$ , there exists one set $T\\in\\mathcal{T}$ such that $T$ contains (almost) all points from $C^{*}$ and at most $O(\\varepsilon n)$ other points,   \n(2) It holds that $\\begin{array}{r}{\\sum_{T\\in\\mathcal{T}}|T|\\leqslant n+O(\\varepsilon n)}\\end{array}$ . ", "page_idx": 7}, {"type": "text", "text": "By (1), for every inlier cluster $C^{*}$ with a corresponding true weight $w^{*}$ , there exists a set $T$ such that the points from $C^{*}$ constitute at least an $\\tilde{w}$ -fraction of $T$ with $\\tilde{w}:=\\Omega(w^{*}/(w^{*}+\\varepsilon))$ . By Section 4.1, applying $\\mathcal{A}_{\\mathrm{aLD}}$ with $\\alpha_{\\mathrm{low}}={w_{\\mathrm{low}}}\\cdot n/\\left|T\\right|$ on such a $T$ then yields a list of size $1+\\dot{O}((1-\\tilde{w})/w_{\\mathrm{low}})$ with an estimation error at most $O(\\sqrt{\\log{1/\\tilde{w}}})$ . If $T$ contains (almost) no inliers, that is, there is no inlier component that should recovered, then $\\mathcal{A}_{\\mathrm{kLD}}$ returns a list of size $O(|T|/(w_{\\mathrm{low}}n))$ . ", "page_idx": 7}, {"type": "text", "text": "Now, by the two properties, (almost) all inlier points lie in at most $k$ sets of $\\upsigma$ , and all other sets of $\\upsigma$ contain in total at most $O(\\varepsilon n)$ points. Hence, concatenating all lists outputted by $\\mathcal{A}_{\\mathrm{aLD}}$ applied to all $T\\in\\mathcal{T}$ leads to a final list size bounded by $k+O(\\varepsilon/w_{\\mathrm{low}})$ . ", "page_idx": 7}, {"type": "text", "text": "4.3 Outer stage: separating inlier clusters ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now informally describe the outer stage that produces the collection of sets $\\upsigma$ with the desiderata described in Section 4.2, leaving the details to Appendix E. The main steps are outlined in pseudocode in Algorithm 1. ", "page_idx": 7}, {"type": "text", "text": "Given a set $X$ of $N=\\mathrm{poly}(d^{t},1/w_{\\mathrm{low}})$ i.i.d. input samples from the distribution Eq. (2.1) with Gaussian inlier components, the first step of the meta-algorithm is to run Algorithm 1 on $X$ and $w_{\\mathrm{low}}$ with $\\Delta=O(\\sqrt{\\log{1/w_{\\mathrm{low}}}})$ . Algorithm 1 runs an sLD algorithm on the samples and produces a (large) list of estimates $L$ such that, for each mean, at least one estimate is $O(\\sqrt{\\log{1/w_{\\mathrm{low}}}})$ -close to it. It then add sets to $\\upsigma$ that correspond to these estimates via a dynamic \u201ctwo-scale\" process. ", "page_idx": 7}, {"type": "text", "text": "Specifically, for each $\\hat{\\mu}\\in L$ , we construct two sets $S_{\\hat{\\mu}}^{(1)}\\subseteq S_{\\hat{\\mu}}^{(2)}$ consisting of samples close to $\\hat{\\mu}$ . By construction, we guarantee that if S(\u00b5\u02c61) contains a non-negligible fraction of samples from any inlier cluster $C^{*}$ , then $S_{\\hat{\\mu}}^{(2)}$ contains (almost) all samples from $C^{*}$ (see Theorem B.7 (ii)). ", "page_idx": 8}, {"type": "text", "text": "Now we very briefly illustrate how this process could be helpful in proving properties (1) and (2). Observe that, as long as there exists some $\\hat{\\mu}$ with $|S_{\\hat{\\mu}}^{(2)}|\\leqslant2|S_{\\hat{\\mu}}^{(1)}|$ , we add $S_{\\hat{\\mu}}^{(2)}$ to $\\upsigma$ and remove the samples from $S_{\\hat{\\mu}}^{(1)}$ . Consider one such $\\hat{\\mu}$ . For property (1), we merely note that if $S_{\\hat{\\mu}}^{(1)}$ contains a part of an inlier cluster $C^{*}$ , then $S_{\\hat{\\mu}}^{(2)}$ contains (almost) all of $C^{*}$ , so we add to $\\upsigma$ a set that contains (almost) all of $C^{*}$ ; otherwise, when we remove $S_{\\hat{\\mu}}^{(1)}$ we remove (almost) no points from $C^{*}$ , so (almost) all the points from $C^{*}$ remain in play. For property (2), we merely note that whenever we $S_{\\hat{\\mu}}^{(2)}$ $\\upsigma$ ,  tihnec rneuasminbge rt ohfe  snaummpbleesr  bofy $|S_{\\hat{\\mu}}^{(2)}|$ h, e wper oaolfs oo fr ethmeo pvreo tpheer tiseasm upsleess  sforomme $S_{\\hat{\\mu}}^{(1)}$ $|S_{\\hat{\\mu}}^{(1)}|\\geqslant|S_{\\hat{\\mu}}^{(2)}|/2$   \nadditional arguments of a similar flavor, and we defer it to Appendix E. ", "page_idx": 8}, {"type": "text", "text": "5 Related work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "List-decodable mean estimation Inspired by the list-decoding paradigm that was first introduced for error-correcting codes for large error rates [15], list-decodable mean estimation has become a popular approach for robustly learning the mean of a distribution when the majority of the samples are outliers. A long line of work has proposed efficient algorithms with theoretical guarantees. These algorithms are either based on convex optimization [9, 16], a filtering approach [3, 17], or low-dimensional projections [18]. Near-linear time algorithms were obtained in [19] and [8]. The list-decoding paradigm is not only used for mean estimation but also other statistical inference problems. Examples include sparse mean estimation [20, 21], linear regression [22\u201324], subspace recovery [25, 26], clustering [27], stochastic block models and crowd sourcing [16, 28]. ", "page_idx": 8}, {"type": "text", "text": "Robust mean estimation and mixture learning When the outliers constitute a minority, algorithms typically achieve significantly better error guarantees than in the list-decodable setting. Robust mean estimation algorithms output a single vector close to the mean of the inliers. In a variety of corruption models, efficient algorithms are known to achieve (nearly) optimal error ", "page_idx": 8}, {"type": "text", "text": "Robust mixture learning tackles the model in Eq. (2.1) with $\\varepsilon\\ll\\operatorname*{min}_{i}w_{i}$ and aims to output exactly $k$ vectors with an accurate estimate for the population mean of each component [3, 4, 7, 9, 11, 13, 29, 30]. These algorithms do not enjoy error guarantees for clusters with weights $w_{i}<\\varepsilon$ . To the best of our knowledge, our algorithm is the first to achieve non-trivial guarantees in this larger noise regime. ", "page_idx": 8}, {"type": "text", "text": "Robust clustering Robust clustering [31] also addresses the presence of small fractions of outliers in a similar spirit to robust mixture learning, conceptually implemented in the celebrated DBScan algorithm [32]. Assuming the output list size is large enough to capture possible outlier clusters, these methods may also be used to tackle list-decodable mixture learning - however, they do not come with an inherent procedure to determine the right choice of hyperparameters that ultimately output a list size that adapts to the problem. ", "page_idx": 8}, {"type": "text", "text": "6 Discussion and future work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this work, we prove that even when small groups are outnumbered by adversarial data points, efficient list-decodable algorithms can provide an accurate estimation of all means with minimal list size. The proof for the upper bound is constructive and analyzes a plug-and-play meta-algorithm (cf. Figure 1) that inherits guarantees of the black-box cor-kLD algorithm $\\mathcal{A}_{\\mathrm{kLD}}$ and RME algorithm $\\boldsymbol{\\mathcal{A}}_{R}$ , which it uses as base learners. Notably, when the inlier mixture is a mixture of Gaussians with identity covariance, we achieve optimality. Furthermore, any new development for the base learners automatically translates to improvements in our bounds. ", "page_idx": 8}, {"type": "text", "text": "We would like to end by discussing the possible practical impact of this result. Since an extensive empirical study is out of the scope of this paper, besides the fact that ground-truth means for unsupervised real-world data are hard to come by, we provide preliminary experiments on synthetic data. Specifically, we generate data from a separated $k$ \u2212Gaussian mixture with additive contaminations as in Eq. (2.1) and different types of adversarial distributions (see detailed description in Appendix I). We focus on the regime $\\varepsilon\\sim w_{i}$ where our algorithms shows the largest theoretical improvements. ", "page_idx": 8}, {"type": "image", "img_path": "TrXV4dMDcG/tmp/7cabeae3fc3b15da1d200de54947be8ea691c58fe996396c90b39b30daaf7792.jpg", "img_caption": ["Figure 2: Comparison of five algorithms with two adversarial noise models. The attack distributions and further experimental details are given in Appendix I. On the left we show worst estimation error for constrained list size and on the right the smallest list size for constrained error guarantee. We plot the median of the metrics with the error bars showing 25th and 75th percentile. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "TrXV4dMDcG/tmp/346a5b287c9d5c41b92da9f84bbf189545594b3ad6bdff0935eba12a40627b26.jpg", "img_caption": ["Figure 3: Comparison of list size and estimation error for large inlier cluster for varying $w_{\\mathrm{low}}$ inputs. The experimental setup is illustrated in Appendix I. We plot the median values with error bars showing 25th and 75th quantiles. As $w_{\\mathrm{low}}$ decreases, we observe a roughly constant estimation error for our algorithm while the error for LD-ME increases. Further, the decrease in list size is much more severe for LD-ME than for our algorithm. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "We then compare the output of our algorithm with the vanilla LD-ME algorithm from [8] with $w_{\\mathrm{low}}=0.02$ and (suboptimal) LD-ML guarantees as well as well-known (robust) clustering heuristics without LD-ML guarantees, such as the $k$ -means [33], Robust $k$ -means [34], and DBSCAN [32]. Even though none of these heuristics have LD-ML guarantees, they are commonly used and known to also perform well in practice in noisy settings. In Figure 2 (left), we fix the list size to 10 and plot the errors for the worst inlier cluster, typically the smallest. We compare the performance of the algorithms by plotting the worst-case estimation errors for a given list size and list sizes that algorithms require to achieve a given worst-case estimation error. In Figure 2 (right), we fix the error and plot the minimal list size at which competing algorithms reach the same or smaller worst estimation error. Further details on the experiments are provided in Appendix I. In a different experiment (see Figure 3 and Appendix I.1 for details), we observe that our approach outperforms LD-ME when $w_{\\mathrm{low}}$ varies, both in achieving smaller list size and smaller estimation error. ", "page_idx": 9}, {"type": "text", "text": "Overall, in line with our theory, our method significantly outperforms the LD-ME algorithm, and performs better or on par with the heuristic approaches. Additional experimental comparison and implementation details can be found in Appendix I. Even though these experiments do not allow conclusive statements about the improvement of our algorithm for mixture learning for real-world data, they do provide encouraging evidence that effort could be well-spent on follow-up empirical and theoretical work building on our results. For example, it would be interesting to conduct a more extensive empirical study comparing our algorithm with a variety of robust clustering algorithms. Additionally, practical data often contains components with varying scales. An interesting direction for future work could be to extend our algorithm to handle differently scaled covariances in an agnostic manner. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "DD is supported by ETH AI Center doctoral fellowship and ETH Foundations of Data Science initiative. RB, ST, GN, and DS have received funding from the European Research Council (ERC) under the European Union\u2019s Horizon 2020 research and innovation programme (grant agreement No 815464). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] David R Bickel. Robust cluster analysis of microarray gene expression data with the number of clusters determined biologically. Bioinformatics, 19(7):818\u2013824, 2003.   \n[2] Eric D Feigelson and G Jogesh Babu. Statistical methods for astronomy. arXiv preprint arXiv:1205.2064, 2012.   \n[3] Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. List-decodable robust mean estimation and learning mixtures of spherical Gaussians. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pages 1047\u20131060, 2018.   \n[4] Ainesh Bakshi, Ilias Diakonikolas, He Jia, Daniel M Kane, Pravesh K Kothari, and Santosh S Vempala. Robustly learning mixtures of k arbitrary Gaussians. In Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing, pages 1234\u20131247, 2022.   \n[5] Ilias Diakonikolas and Daniel M Kane. Algorithmic high-dimensional robust statistics. Cambridge University Press, 2023.   \n[6] Oded Regev and Aravindan Vijayaraghavan. On learning mixtures of well-separated Gaussians. In 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS), pages 85\u201396. IEEE, 2017.   \n[7] Misha Ivkov and Pravesh K Kothari. List-decodable covariance estimation. In Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing, pages 1276\u20131283, 2022.   \n[8] Ilias Diakonikolas, Daniel M Kane, Daniel Kongsgaard, Jerry Li, and Kevin Tian. Clustering mixture models in almost-linear time via list-decodable mean estimation. In Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing, pages 1262\u20131275, 2022.   \n[9] Pravesh K Kothari, Jacob Steinhardt, and David Steurer. Robust moment estimation and improved clustering via sum of squares. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pages 1035\u20131046, 2018.   \n[10] Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. Robust estimators in high-dimensions without the computational intractability. SIAM Journal on Computing, 48(2):742\u2013864, 2019.   \n[11] Samuel B Hopkins and Jerry Li. Mixture models, robustness, and sum of squares proofs. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pages 1021\u20131034, 2018.   \n[12] Allen Liu and Jerry Li. Clustering mixtures with almost optimal separation in polynomial time. In Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing, pages 1248\u20131261, 2022.   \n[13] Ilias Diakonikolas, Daniel M Kane, Jasper CH Lee, and Thanasis Pittas. Clustering mixtures of bounded covariance distributions under optimal separation. arXiv preprint arXiv:2312.11769, 2023.   \n[14] Michael Kearns. Efficient noise-tolerant learning from statistical queries. Journal of the ACM (JACM), 45(6):983\u20131006, 1998.   \n[15] Peter Elias. List decoding for noisy channels. Technical report, Research Laboratory of Electronics, Massachusetts Institute of Technology, 1957.   \n[16] Moses Charikar, Jacob Steinhardt, and Gregory Valiant. Learning from untrusted data. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 47\u201360, 2017.   \n[17] Ilias Diakonikolas, Daniel Kane, and Daniel Kongsgaard. List-decodable mean estimation via iterative multi-filtering. Advances in Neural Information Processing Systems, 33:9312\u20139323, 2020.   \n[18] Ilias Diakonikolas, Daniel Kane, Daniel Kongsgaard, Jerry Li, and Kevin Tian. List-decodable mean estimation in nearly-PCA time. Advances in Neural Information Processing Systems, 34:10195\u201310208, 2021.   \n[19] Yeshwanth Cherapanamjeri, Sidhanth Mohanty, and Morris Yau. List decodable mean estimation in nearly linear time. In 2020 IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS), pages 141\u2013148. IEEE, 2020.   \n[20] Ilias Diakonikolas, Daniel Kane, Sushrut Karmalkar, Ankit Pensia, and Thanasis Pittas. Listdecodable sparse mean estimation via difference-of-pairs flitering. Advances in Neural Information Processing Systems, 35:13947\u201313960, 2022.   \n[21] Shiwei Zeng and Jie Shen. List-decodable sparse mean estimation. Advances in Neural Information Processing Systems, 35:24031\u201324045, 2022.   \n[22] Sushrut Karmalkar, Adam Klivans, and Pravesh Kothari. List-decodable linear regression. Advances in Neural Information Processing Systems, 32, 2019.   \n[23] Prasad Raghavendra and Morris Yau. List decodable learning via sum of squares. In Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 161\u2013180. SIAM, 2020.   \n[24] Ilias Diakonikolas, Daniel Kane, Ankit Pensia, Thanasis Pittas, and Alistair Stewart. Statistical query lower bounds for list-decodable linear regression. Advances in Neural Information Processing Systems, 34:3191\u20133204, 2021.   \n[25] Ainesh Bakshi and Pravesh K Kothari. List-decodable subspace recovery: Dimension independent error in polynomial time. In Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1279\u20131297. SIAM, 2021.   \n[26] Prasad Raghavendra and Morris Yau. List decodable subspace recovery. In Conference on Learning Theory, pages 3206\u20133226. PMLR, 2020.   \n[27] Maria-Florina Balcan, Avrim Blum, and Santosh Vempala. A discriminative framework for clustering via similarity functions. In Proceedings of the fortieth annual ACM symposium on Theory of computing, pages 671\u2013680, 2008.   \n[28] Michela Meister and Gregory Valiant. A data prism: Semi-verified learning in the small-alpha regime. In Conference On Learning Theory, pages 1530\u20131546. PMLR, 2018.   \n[29] Ainesh Bakshi, Ilias Diakonikolas, Samuel B Hopkins, Daniel Kane, Sushrut Karmalkar, and Pravesh K Kothari. Outlier-robust clustering of Gaussians and other non-spherical mixtures. In 2020 IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS), pages 149\u2013159. IEEE, 2020.   \n[30] Allen Liu and Ankur Moitra. Settling the robust learnability of mixtures of Gaussians. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, pages 518\u2013531, 2021.   \n[31] Luis Angel Garc\u00eda-Escudero, Alfonso Gordaliza, Carlos Matr\u00e1n, and Agust\u00edn Mayo-Iscar. A review of robust clustering methods. Advances in Data Analysis and Classification, 4:89\u2013109, 2010.   \n[32] Martin Ester, Hans-Peter Kriegel, J\u00f6rg Sander, Xiaowei Xu, et al. A density-based algorithm for discovering clusters in large spatial databases with noise. In kdd, pages 226\u2013231, 1996.   \n[33] Stuart Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, pages 129\u2013137, 1982.   \n[34] Christian Brownlees, Emilien Joly, and G\u00e1bor Lugosi. Empirical risk minimization for heavytailed losses. The Annals of Statistics, 43(6):2507\u20132536, 2015.   \n[35] Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. Robustly learning a Gaussian: Getting optimal error, efficiently. In Artur Czumaj, editor, Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2018, New Orleans, LA, USA, January 7-10, 2018, pages 2683\u20132702. SIAM, 2018. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Examples ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "$k$ inlier cluster, $c$ outlier clusters. One tricky adversarial distribution is the Gaussian mixture model itself. In particular, we consider ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{X}_{c}=\\frac{k}{k+c}\\sum_{i=1}^{k}\\frac{1}{k}\\mathcal{N}(\\mu_{i},I)+\\frac{c}{k+c}\\sum_{i=1}^{c}\\frac{1}{c}\\mathcal{N}(\\tilde{\\mu}_{i},I),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the first $k$ Gaussian components are inliers and $Q$ is a GMM with $c$ components, which we call fake clusters. Since all inlier cluster weights are identical, we denote $w:=w_{i}\\stackrel{\\cdot}{=}1/(k+c)$ . Assume that $1\\ll c\\ll k$ , which corresponds to $\\varepsilon\\gg w$ . Then, relative weights are $\\tilde{w}=1/(c+1)\\approx1/c$ . Due to large adversary, previous results on learning GMMs cannot be applied, leavin\u221ag vanilla listdecodable learning. However, the latter also cannot guarantee anything better that $\\Omega(\\Bar{\\sqrt{t}}k^{1/t})$ even with th\u221ae knowledge of $k$ , as long as list size is $O(k\\!+\\!c)$ , which can be much worse than our guarantees of $O(\\sqrt{t}c^{1/t})$ for the same list size. ", "page_idx": 13}, {"type": "text", "text": "Their drawback is that they do not utilize separation between true clusters, i.e., for each $i$ , they model the data as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{X}=\\frac{1}{k+c}\\mathcal{N}(\\mu_{i},I)+\\left(1-\\frac{1}{k+c}\\right)Q.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $Q$ can be \u201carbitratily adversarial\u201d for recovering $\\mu_{i}$ . ", "page_idx": 13}, {"type": "text", "text": "Big $^+$ small inlier clusters Consider the mixture ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{X}_{b}=\\left(1-w-\\varepsilon\\right)\\mathcal{N}(\\mu_{1},I_{d})+w\\mathcal{N}(\\mu_{2},I_{d})+\\varepsilon Q,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\|\\mu_{1}-\\mu_{2}\\|=\\Omega(\\sqrt{\\log{1/w}})$ , $w\\ll\\varepsilon\\ll1$ , and $Q$ is chosen adversarially. In this example we have two inlier clusters, one with large weight $\\approx1$ and another with small weight $w$ . Adversarial distribution $Q$ has large weight relative to the small cluster, but still negligible weight compared to the large one. ", "page_idx": 13}, {"type": "text", "text": "Previous methods would either (i) recover large cluster with optimal error $O(\\varepsilon)$ (see, e.g., [35]) but miss out small cluster or (ii) recover both clusters using list-decodable mean estimation with known $\\alpha=w$ , but with suboptimal errors $O(\\sqrt{\\log{1/w}})$ and list size ${\\cal O}(1/w)$ . In contrast, Corollary 3.4 guarantees list size at most $1+O(\\varepsilon/w)$ , error $O(\\sqrt{\\log{\\varepsilon/w}})$ for the small cluster, and error $O(\\varepsilon{\\sqrt{\\log{1/\\varepsilon}}})$ for the larger. In general, we achieve (i) optimal errors for both clusters and (ii) optimal (up to constants) list size. ", "page_idx": 13}, {"type": "text", "text": "B Inner and outer stage algorithms and guarantees ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Our meta-algorithm Algorithm 2 assumes black-box access to a list-decodable mean estimation algorithm and a robust mean estimation algorithm for sub-Gaussian (up to the $t^{\\mathrm{th}}$ moment) distributions. From these we obtain stronger mean estimation algorithms when the fraction of outliers is unknown, and finally stronger algorithms for learning separated mixtures when the fraction of outliers can be arbitrarily large. Our algorithm achieves guarantees with polynomial runtime and sample complexity if the black-box learners achieve the guarantees for their corresponding mean estimation setting. In this section we discuss the corruption model and inner and out stage of the meta-algorithm in detail and prove properties needed for the proof of the main Theorem 3.3. ", "page_idx": 13}, {"type": "text", "text": "B.1 Detailed setting ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In order to achieve these guarantees, our black-box algorithms need to work under a model in which an adversary is allowed to remove a small fraction of the inliers and to add arbitrarily many outliers. In our proofs, for simplicity of exposition, we require the algorithms to have mean estimation guarantees for a small adversarially removed fraction of $w_{\\mathrm{low}}^{2}$ . Formally, the corruption model as defined as follows. ", "page_idx": 13}, {"type": "text", "text": "Definition B.1 (Corruption model). Let $d\\,\\in\\,\\mathbb{N}_{+}$ , and $\\alpha\\in[w_{\\mathrm{low}},1]$ . Let $D$ be a $d$ -dimensional distribution. An input of size $n$ according to our corruption model is generated as follows: ", "page_idx": 13}, {"type": "text", "text": "Input: Samples $S=\\{x_{1},\\ldots,x_{n}\\}$ , $w_{\\mathrm{low}}$ , algorithms $\\mathcal{A}_{\\mathrm{kLD}}$ , and $\\boldsymbol{\\mathcal{A}}_{R}$ .   \nOutput: List $L$ .   \n1: Run OuterStage (Algorithm 6) on $S$ and let $\\upsigma$ be the returned list.   \n2: $L\\gets\\emptyset$ .   \n3: for $T\\in\\mathcal{T}$ do 4: Run InnerStage (Algorithm 3) on $T$ with $\\begin{array}{r}{\\alpha_{\\mathrm{low}}=w_{\\mathrm{low}}\\cdot\\frac{n}{|T|}}\\end{array}$ .   \n5: Add the elements of the returned list to $L$ .   \n6: return $L$ . Algorithm 3 InnerStage   \nInput: Samples $S=\\left\\{x_{1},\\dots,x_{n}\\right\\},\\alpha_{\\mathrm{low}}\\in[w_{\\mathrm{low}},1],\\mathcal{A}_{\\mathrm{kLD}},\\dot{\\boldsymbol{z}}$ and $\\boldsymbol{\\mathcal{A}}_{R}$ .   \nOutput: List $L$ .   \n1: $\\alpha_{\\mathrm{low}}\\leftarrow\\mathrm{min}(1/100,\\alpha_{\\mathrm{low}})$   \n2: $M\\gets\\emptyset$   \n3: for $\\hat{\\alpha}\\in\\{\\alpha_{\\mathrm{low}},2\\alpha_{\\mathrm{low}},\\ldots,\\lfloor1/(3\\alpha_{\\mathrm{low}})\\rfloor\\alpha_{\\mathrm{low}}\\}$ do   \n4: run $\\mathcal{A}_{\\mathrm{kLD}}$ on $S$ with fraction of inliers set to $\\hat{\\alpha}$   \n5: add the pair $(\\hat{\\mu},\\hat{\\alpha})$ to $M$ for each output $\\hat{\\mu}$   \n6: Let $L$ be the output of ListFilter (Algorithm 4) run on $S$ , $\\alpha_{\\mathrm{low}}$ , and $M$   \n7: for $(\\hat{\\mu},\\hat{\\alpha})\\in L$ do   \n8: replace $\\hat{\\mu}$ by the output of ImproveWithRME (Algorithm 5) run on $S,\\hat{\\mu},\\tau=40\\psi_{t}(\\hat{\\alpha})+$ $4f(\\hat{\\alpha})$ , and $\\mathcal{A}_{R}$   \n9: return $L$ ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "\u2022 Draw a set $C^{*}$ of $n_{1}=\\lceil\\alpha n\\rceil$ i.i.d. samples from the distribution $D$ . ", "page_idx": 14}, {"type": "text", "text": "\u2022 An adversary is allowed to arbitrarily remove $\\lfloor w_{\\mathrm{low}}^{2}n_{1}\\rfloor$ samples from $C^{*}$ . We refer to the resulting set as $S^{*}$ with size $n_{2}=|S^{*}|$ . ", "page_idx": 14}, {"type": "text", "text": "\u2022 An adversary is allowed to add $n-n_{2}$ arbitrary points to $S^{*}$ . We refer to the resulting set as $S_{\\mathrm{adv}}$ with size $n_{3}=|S_{\\mathrm{adv}}|$ . ", "page_idx": 14}, {"type": "text", "text": "\u2022 If $n_{3}<n$ , pad $S_{\\mathrm{adv}}$ with $n-n_{3}$ arbitrary points and call the resulting set $S$ .   \n\u2022 Return $S$ . ", "page_idx": 14}, {"type": "text", "text": "We call cor-kLD the model when $w_{\\mathrm{low}}$ and $\\alpha$ are given to the algorithm and cor-aLD the model when $w_{\\mathrm{low}}$ and lower bound $\\alpha_{\\mathrm{low}}\\geqslant w_{\\mathrm{low}}$ are given to the algorithm, such that $\\alpha\\geqslant\\alpha_{\\mathrm{low}}$ . Note that $\\alpha$ is not provided in cor-aLD model. ", "page_idx": 14}, {"type": "text", "text": "Note that in Definition B.1 $|S|=n$ and $S^{*}$ constitutes at least an $\\alpha(1-w_{\\mathrm{low}}^{2})$ -fraction of $S$ ", "page_idx": 14}, {"type": "text", "text": "B.2 Inner stage algorithm and guarantees ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The algorithm consists of three steps: (1) Constructing a list of hypotheses, (2) Filtering the hypotheses, and (3) Improving the hypotheses if $\\alpha\\geqslant1-\\varepsilon_{\\mathrm{RME}}$ . For convenience, we restate the InnerStage algorithm introduced in the main text. ", "page_idx": 14}, {"type": "text", "text": "Theorem B.2 (Inner stage guarantees). Let $d\\in\\mathbb{N}_{+}$ , $w_{\\mathrm{low}}\\in(0,10^{-4}]$ , $w_{\\mathrm{low}}\\leqslant\\alpha_{\\mathrm{low}}\\leqslant\\alpha\\leqslant1,$ , and $t$ be an even integer. Let $D(\\mu^{*})$ be a $d$ -dimensional distribution with mean $\\mu^{*}\\in\\mathbb{R}^{d}$ and sub-Gaussian $t$ -th central moments. ", "page_idx": 14}, {"type": "text", "text": "Consider the cor-aLD corruption model in Definition B.1 with parameters d, $w_{\\mathrm{low}}$ , $\\alpha$ and distribution $D\\,=\\,D(\\mu^{*})$ . Let $\\mathcal{A}_{\\mathrm{kLD}}$ and $\\boldsymbol{\\mathcal{A}}_{R}$ satisfy Assumption 3.1 with high success probability (see Remark B.3). ", "page_idx": 14}, {"type": "text", "text": "Then InnerStage (Algorithm 3), given an input of $\\mathrm{poly}(d,1/w_{\\mathrm{low}})\\cdot(N_{L D}(w_{\\mathrm{low}})+N_{R}(w_{\\mathrm{low}}))$ samples from the cor-aLD corruption model, and access to the parameters d, $w_{\\mathrm{low}}$ , $\\alpha_{\\mathrm{low}}$ , ", "page_idx": 14}, {"type": "text", "text": "Input: Samples $S=\\left\\{x_{1},\\dots,x_{n}\\right\\},\\alpha_{\\mathrm{low}}\\in[w_{\\mathrm{low}},1/100]$ , and $M=\\left\\{(\\hat{\\mu}_{1},\\hat{\\alpha}_{1}),\\dots,(\\hat{\\mu}_{m},\\hat{\\alpha}_{m})\\right\\}$   \nOutput: List $L$   \n1: define $\\beta(\\alpha)=10\\psi_{t}(\\alpha)+f(\\alpha)$   \n2: let $v_{i j}$ be a unit vector in the direction of $\\hat{\\mu}_{i}-\\hat{\\mu}_{j}$ for $\\hat{\\mu}_{i}\\neq\\hat{\\mu}_{j}\\in\\{\\hat{\\mu}$ , for $(\\hat{\\mu},\\hat{\\alpha})\\in M\\}$   \n3: $J\\gets\\bar{\\emptyset}$   \n4: for $(\\hat{\\mu}_{i},\\hat{\\alpha}_{i})\\in M$ in decreasing order of $\\hat{\\alpha}_{i}$ do   \n5: if exists $j\\in J$ , such that $\\|\\hat{\\mu}_{i}-\\hat{\\mu}_{j}\\|\\leqslant4\\beta(\\hat{\\alpha}_{i})$ then continue   \n6: $\\begin{array}{r}{T_{i}\\gets\\bigcap_{j\\in J}\\{x\\in S;}\\end{array}$ , s.t. $|v_{i j}^{\\top}(x-\\hat{\\mu}_{i})|\\leqslant\\beta(\\hat{\\alpha}_{i})\\big\\}$ .   \n7: if $|T_{i}|<0.9\\hat{\\alpha}_{i}n$ then remove $\\left(\\hat{\\mu}_{i},\\hat{\\alpha}_{i}\\right)$ from $M$ and continue   \n8: add $i$ to $J$   \n9: for $j\\in J\\backslash\\{i\\}$ do   \n10: $T_{j}\\gets T_{j}\\bigcap\\left\\{x\\in S\\right.$ , s.t. $|v_{i j}^{\\top}(x-\\hat{\\mu}_{j})|\\leqslant\\beta(\\hat{\\alpha}_{i})\\Big\\}$   \n11: if $|T_{j}|<0.9\\hat{\\alpha}_{j}n$ then:   \n12: remove $(\\hat{\\mu}_{j},\\hat{\\alpha}_{j})$ from $M$   \n13: rerun ListFilter (Algorithm 4) with the new $M$   \n14: return {(\u00b5\u02c6i, \u03b1\u02c6i), for i \u2208J} ", "page_idx": 15}, {"type": "text", "text": "Algorithm 5 ImproveWithRME ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Input: Samples $S=\\{x_{1},\\ldots,x_{n}\\}$ , vector $\\hat{\\mu}$ , threshold $\\tau$ , and $\\mathcal{A}_{R}$   \nOutput: A vector $\\tilde{\\mu}\\in\\mathbb{R}^{d}$   \n1: $\\bar{\\tilde{\\beta}}\\gets\\tau$   \n2: let $\\tilde{\\alpha}$ be the smallest value in $[1-\\varepsilon_{\\mathrm{RME}},1]$ that satisfies $g(\\tilde{\\alpha})\\leqslant\\tilde{\\beta}/2$ . If none exists, return $\\hat{\\mu}$   \n3: $\\tilde{\\mu}\\gets\\hat{\\mu}$ and let $\\mu_{\\mathrm{RME}}$ be the output of $\\boldsymbol{\\mathcal{A}}_{R}$ run on $S$ with inlier fraction set to $\\tilde{\\alpha}$ .   \n4: while $\\Vert\\tilde{\\mu}-\\mu_{\\mathrm{RME}}\\Vert\\leqslant3\\tilde{\\beta}/2$ do   \n5: $\\tilde{\\mu}\\leftarrow\\mu_{\\mathrm{RME}}$   \n6: $\\tilde{\\beta}\\gets g(\\tilde{\\alpha})$   \n7: let $\\widetilde{\\alpha}^{\\prime}$ be the smallest in $[\\tilde{\\alpha}+w_{\\mathrm{low}}^{2},1]$ such that $g(\\tilde{\\alpha}^{\\prime})\\leqslant\\tilde{\\beta}/2$ . If none exists, break   \n8: $\\tilde{\\alpha}\\gets\\tilde{\\alpha}^{\\prime}$   \n9: let $\\mu_{\\mathrm{RME}}$ be the output of $\\mathcal{A}_{R}$ on $S$ with inlier fraction set to $\\tilde{\\alpha}$   \n10: return $\\tilde{\\mu}$ ", "page_idx": 15}, {"type": "text", "text": "and $t$ , runs in time $\\mathrm{poly}(d,1/w_{\\mathrm{low}})\\cdot(T_{L D}(w_{\\mathrm{low}})+T_{R}(w_{\\mathrm{low}}))$ and outputs a list $L$ of size $|L|\\leqslant1+O((1-\\alpha)/\\alpha_{\\mathrm{low}})$ such that, with probability $1-w_{\\mathrm{low}}^{O(1)}$ , ", "page_idx": 15}, {"type": "text", "text": "1. There exists $\\hat{\\mu}\\in L$ such that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\hat{\\mu}-\\mu^{*}\\|\\leqslant O(\\psi_{t}(\\alpha/4)+f(\\alpha/4)).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "2. If $\\alpha\\geqslant1-\\varepsilon_{\\mathrm{RME}}$ , then there exists $\\hat{\\mu}\\in L$ such that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\hat{\\mu}-\\mu^{*}\\|\\leqslant O(g(\\alpha-w_{\\mathrm{low}}^{2})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof of Theorem B.2 can be found in Appendix D. ", "page_idx": 15}, {"type": "text", "text": "Remark B.3. For any $r\\in\\mathbb{N}$ , we can increase probabilities of success of $\\mathcal{A}_{\\mathrm{kLD}}$ and $\\mathcal{A}_{R}$ from $1/2$ to $1-2^{-r}$ in the following way: we increase number of samples by a factor of $r$ , randomly split $S$ into $r$ subsets of equal size, apply $\\mathcal{A}_{\\mathrm{kLD}}$ and $\\boldsymbol{\\mathcal{A}}_{R}$ to these subsets and concatenate their outputs. In the proofs we assume that the success probabilities are $1-w_{\\mathrm{low}}^{C}$ for large enough constant $C$ . This increases the size of the list returned by $\\mathcal{A}_{\\mathrm{kLD}}$ , the number of samples, and the running time by $a$ factor $O(\\log(1/w_{\\mathrm{low}}))$ . In particular, we assume that the size of the list returned by $\\mathcal{A}_{\\mathrm{kLD}}$ is much smaller than the inverse failure probability. ", "page_idx": 15}, {"type": "text", "text": "Remark B.4. In the execution of the meta-algorithm, it may happen that Algorithm 3 is run on set $T$ with almost no inliers, i.e., $\\alpha<\\alpha_{\\mathrm{low}}$ . We note that from the analysis (see Appendix $D_{\\mathrm{:}}$ , or $[3J_{:}$ Proposition $B.1$ ), we always have upper bound $|L|=O(1/\\alpha_{\\mathrm{low}})$ . ", "page_idx": 15}, {"type": "text", "text": "An immediate consequence of Theorem B.2 are the following guarantees of directly applying Algorithm 3 to the mixture learning case with no separation. Here we present upper bounds for Algorithm 3, when no separation assumptions are imposed. ", "page_idx": 16}, {"type": "text", "text": "Corollary B.5. Let $d,k\\in\\ensuremath{\\mathbb{N}}_{+}$ , $w_{\\mathrm{low}}\\in(0,10^{-4}]$ , and $t$ be an even integer. For all $i=1,\\ldots,k,$ , let $D_{i}(\\mu_{i})$ be a $d$ -dimensional distribution with mean $\\mu_{i}\\in\\mathbb{R}^{d}$ and sub-Gaussian $t$ -th central moments. Let $\\varepsilon>0$ and, for all $i=1,\\ldots,k,$ let $w_{i}\\in\\lceil w_{\\mathrm{low}},1\\rceil$ , such that $\\begin{array}{r}{\\sum_{i=1}^{k}w_{i}+\\varepsilon=1}\\end{array}$ . Let $\\mathcal{X}$ be the $d$ -dimensional mixture distribution ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{X}=\\sum_{i=1}^{k}w_{i}D_{i}(\\mu_{i})+\\varepsilon Q,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $Q$ is an unknown adversarial distribution that can depend on all the other parameters. Let $\\mathcal{A}_{\\mathrm{kLD}}$ and $\\boldsymbol{\\mathcal{A}}_{R}$ satisfy Assumption 3.1. ", "page_idx": 16}, {"type": "text", "text": "Then there exists an algorithm that, given $\\mathrm{poly}(d,1/w_{\\mathrm{low}})\\cdot(N_{L D}(w_{\\mathrm{low}})+N_{R}(w_{\\mathrm{low}}))$ i.i.d. samples from $\\mathcal{X}$ , and given also $d,\\ \\ k$ , $w_{\\mathrm{low}}$ , and $t$ , runs in time $\\mathrm{poly}(d,1/w_{\\mathrm{low}})\\cdot(T_{L D}(w_{\\mathrm{low}})+T_{R}(w_{\\mathrm{low}}))$ and outputs a list $L$ of size $\\vert L\\vert\\;\\;=\\;\\;O(1/w_{\\mathrm{low}})$ , such that, with probability at least 1 \u2212wlOo(w1 ) ", "page_idx": 16}, {"type": "text", "text": "1. For each $i\\in[k],$ , there exists $\\hat{\\mu}\\in L$ such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|\\hat{\\mu}-\\mu_{i}\\|\\leqslant O(\\psi_{t}(w_{i}/4)+f(w_{i}/4)).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "2. For each $i\\in[k],$ , if $w_{i}\\geqslant1-\\varepsilon_{\\mathrm{RME}}$ , then there exists $\\hat{\\mu}\\in L$ such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\hat{\\mu}-\\mu_{i}\\|\\leqslant O(g(w_{i}-w_{\\mathrm{low}}^{2})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Proof follows by applying Theorem B.2 to $\\mathcal{X}$ with $\\alpha_{\\mathrm{low}}=w_{\\mathrm{low}}$ and treating each component as a corresponding inlier distribution with $\\alpha\\,=\\,w_{i}$ . This gives error upper bound for all inlier components, furthermore, since $\\alpha_{\\mathrm{low}}~=~w_{\\mathrm{low}}$ , list size can be bounded as $|L|\\,\\leqslant\\,1+O((1\\,-$ $\\alpha)/\\dot{\\alpha}_{\\mathrm{low}})=O(1/w_{\\mathrm{low}})$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "B.3 Outer stage algorithm and guarantees ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In the outer stage, presented in Algorithm 6, we make use of the list-decodable mean estimation algorithm in Theorem B.2 in order to solve list-decodable mixture estimation with separated means. We now present results on the outer stage algorithm. For ease of notation, when it\u2019s clear from the context, we drop the indices and refer to elements $\\mu_{j}\\in M$ for some $j\\in[|M|]$ as $\\mu$ and their corresponding sets $S_{j}^{(1)},S_{j}^{(2)}$ , as defined in lines 6\u20137 in Algorithm 6, as $S^{(1)},S^{(2)}$ . Further, for $i\\in[k]$ , let $C_{i}^{*}$ denote the set of points corresponding to the $i$ -th inlier component, also called the $i$ -th inlier cluster. ", "page_idx": 16}, {"type": "text", "text": "Theorem B.6 (Outer stage guarantees, beginning of execution). Let $S$ consist of n i.i.d. samples from $\\mathcal{X}$ as in the statement of Theorem C.1. Run OuterStage (Algorithm $^{\\sc6}$ ) on $S$ and consider the first iteration of the while-loop and for each $\\mu\\in M$ , denote the corresponding sets as $S^{(1)},S^{(2)}$ . Then, with probability at least $1-w_{\\mathrm{low}}^{O(1)}$ , we have that ", "page_idx": 16}, {"type": "text", "text": "(i) the list $M$ that $\\mathcal{A}_{\\mathrm{sLD}}$ outputs has size $|M|\\leqslant2/w_{\\mathrm{low}}$ ,   \n(ii) for each $i\\in[k]$ , there exists $m_{i}\\in[|M|]$ such that $\\begin{array}{r}{\\left|S_{m_{i}}^{(1)}\\cap C_{i}^{*}\\right|\\geqslant\\left(1-\\frac{w_{\\mathrm{low}}^{2}}{2}\\right)|C_{i}^{*}|,}\\end{array}$ ,   \n(iii) for each $i\\in[k]$ and $\\mu\\in M$ , we have $\\left|S^{(1)}\\cap C_{i}^{*}\\right|\\,<\\,w_{\\mathrm{low}}^{4}\\left|C_{i}^{*}\\right|\\,o r\\left|S^{(2)}\\cap C_{i}^{*}\\right|\\,\\geqslant\\,\\left(1-\\right.$ wl22ow ) |Ci\u2217 |,   \n(iv) for each $\\begin{array}{r l r}{i}&{{}\\in}&{[k]}\\end{array}$ and $\\ensuremath{\\mu}\\mathrm{~\\ensuremath~{~\\in~}~}\\ensuremath{M}$ such that $\\begin{array}{r l r}{\\left|S^{(2)}\\cap C_{i}^{*}\\right|}&{{}\\geqslant}&{w_{\\mathrm{low}}^{4}\\left|C_{i}^{*}\\right|}\\end{array}$ , we have $\\begin{array}{r}{\\sum_{i^{\\prime}\\in[k]\\setminus\\{i\\}}\\left|S^{(2)}\\cap C_{i^{\\prime}}^{*}\\right|\\leqslant w_{\\mathrm{low}}^{4}n,}\\end{array}$   \n(v) for $\\cdot i\\neq i^{\\prime}\\in[k]\\;a n d f o r\\;j,j^{\\prime}\\in[|M|],i f|S_{j}^{(2)}\\cap C_{i}^{*}|\\geqslant w_{\\mathrm{low}}^{4}|C_{i}^{*}|\\;a n d\\,|S_{j^{\\prime}}^{(2)}\\cap C_{i^{\\prime}}^{*}|\\geqslant w_{\\mathrm{low}}^{4}|C_{i^{\\prime}}^{*}|,$ then $S_{j}^{(2)}\\cap S_{j^{\\prime}}^{(2)}=\\emptyset.$ . ", "page_idx": 16}, {"type": "text", "text": "Input: Samples $S=\\{x_{1},\\ldots,x_{n}\\}$ , $w_{\\mathrm{low}}$ , $\\mathcal{A}_{\\mathrm{sLD}}$   \nOutput: Collection of sets $\\upsigma$   \n1: run $\\mathcal{A}_{\\mathrm{sLD}}$ on $S$ with $\\alpha=w_{\\mathrm{low}}$ and let $M=\\{\\mu_{1},\\dots,\\mu_{|M|}\\}$ be the returned list   \n2: let $v_{i j}$ be a unit vector in the direction of $\\mu_{i}-\\mu_{j}$ for $i\\neq j\\in[|M|]$   \n3: $\\upgamma\\gets\\emptyset$ and $R\\leftarrow\\{1,\\ldots,|M|\\}$   \n4: while $R\\neq\\emptyset$ do   \n56:: 7: for $\\begin{array}{r l}&{S_{i}^{(1)}\\stackrel{=}-\\bigcap_{j\\in[|M|],j\\neq i}\\left\\{x\\in S,\\mathrm{s.t.~}|v_{i j}^{\\top}(x-\\mu_{i})|\\leqslant\\gamma+\\gamma^{\\prime}\\right\\}}\\\\ &{S_{i}^{(2)}\\gets\\bigcap_{j\\in[|M|],j\\neq i}\\left\\{x\\in S,\\mathrm{s.t.~}|v_{i j}^{\\top}(x-\\mu_{i})|\\leqslant3\\gamma+3\\gamma^{\\prime}\\right\\}}\\end{array}$ $i\\in R$   \n8: remove all $i\\in R$ for which $|S_{i}^{(1)}|\\leqslant100w_{\\mathrm{low}}^{4}n$   \n9: if $R=\\emptyset$ then break   \n10: if there exists $i\\in R$ such that $\\vert S_{i}^{(2)}\\vert\\leqslant2\\vert S_{i}^{(1)}\\vert$ then   \n11: select the $i\\in R$ with $|S_{i}^{(2)}|\\leqslant2|S_{i}^{(1)}|$ for which $|S_{i}^{(1)}|$ is largest   \n12: $\\begin{array}{r l}&{\\mathbb{T}\\leftarrow\\mathbb{T}\\cup\\left\\{S_{i}^{(2)}\\right\\}}\\\\ &{S\\leftarrow S\\setminus S_{i}^{(1)}}\\\\ &{R\\leftarrow R\\setminus\\{i\\}}\\end{array}$   \n13:   \n14:   \n15: else   \n16: T \u2190T \u222a{S}   \n17: break   \n18: return T ", "page_idx": 17}, {"type": "text", "text": "", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In words, Theorem B.6 (ii) states that at initialization, OuterStage represents each inlier cluster well, i.e., for each $i$ , the $i$ -th cluster is almost entirely contained in some set ${S}_{j}^{(1)}$ for some $j\\in[|M|]$ . Next, (iii) states that either $S_{j_{-}}^{(1)}$ intersects negligibly some true component, or ${S}_{j}^{(2)}$ contains almost entirely the same component. Further, (iv) and (v) state that sets that sufficiently intersect with some true component must be separated from other components and each other. ", "page_idx": 17}, {"type": "text", "text": "We now introduce some notation to present the next theorem that establishes further guarantees for the algorithm output during execution. For $\\upsigma$ , a collection of sets that is the output of Algorithm 6, we define ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left|S_{j}^{(1)}\\cap C_{i}^{*}\\right|\\geqslant w_{\\mathrm{low}}^{4}\\left|C_{i}^{*}\\right|,\\mathrm{for~some~}j\\right\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In words, it is the set of inlier components for which a corresponding set with \"sufficiently many\" points from the $i$ -th component was added to $\\upsigma$ . It may happen that for a given index $i\\in G$ , several $j\\in[|M|]$ satisfy $S_{j}^{(2)}\\in\\mathcal{T}$ and $\\left|S_{j}^{(1)}\\cap C_{i}^{*}\\right|\\geqslant w_{\\mathrm{low}}^{4}\\,|\\dot{C}_{i}^{*}|$ . We define $g_{i}\\in[|M|]$ to denote the index of the first such set $S_{g_{i}}^{(2)}$ added to $\\upsigma$ . ", "page_idx": 17}, {"type": "text", "text": "Further, we define Ui := (Ci\u2217 \u2229Sg(i2 )) \\ Sg(i1) to be the set of inlier points from the i-th component, which were not removed from at the iteration corresponding to $g_{i}$ . Let $U:=\\cup_{i\\in G}U_{i}$ denote the union of such \u2018left-over\u2019 inlier points. ", "page_idx": 17}, {"type": "text", "text": "Theorem B.7 (Outer stage guarantees, during execution). Let $S$ consist of n i.i.d. samples from $\\mathcal{X}$ as in the statement of Theorem C.1. Run OuterStage (Algorithm 6) on $S$ and consider the moment when the sets Si( $S_{i}^{(2)}$ $\\upsigma$ $1-w_{\\mathrm{low}}^{O(1)}$ are true: ", "page_idx": 17}, {"type": "text", "text": "$(i)\\;\\;|U|\\leqslant(2\\varepsilon+O(w_{\\mathrm{low}}^{2}))n,$   \n(ii) for $i\\in G$ , we have that $\\begin{array}{r}{\\left|S_{g_{i}}^{(2)}\\cap C_{i}^{*}\\right|\\geqslant\\left(1-\\frac{w_{\\mathrm{low}}^{2}}{2}-O(w_{\\mathrm{low}}^{3})\\right)|C_{i}^{*}|\\geqslant(1-w_{\\mathrm{low}}^{2})w_{i}n,}\\end{array}$   \n(iii) for $j\\in[|M|]\\setminus\\{g_{i}\\,|\\,i\\in G\\}$ , either $\\left|S_{j}^{(2)}\\right|\\leqslant O(w_{\\mathrm{low}}^{2})n$ , or at least half of the samples in $S_{j}^{(1)}$ are either adversarial samples or lie in $U$ , ", "page_idx": 17}, {"type": "text", "text": "(iv) if when the else statement is triggered, $|S|\\geqslant0.1w_{\\mathrm{low}}n_{\\mathrm{r}}$ , then at least a 0.4-fraction of the samples in $S$ are adversarial, or equivalently, $|S|\\leqslant2.5\\varepsilon n$ . ", "page_idx": 18}, {"type": "text", "text": "Note that the else statement of OuterStage can only be triggered once, at the end of the execution. In wwaosr drse, mTohveeodr,e cmo nBs.t7i t(uit) es taa tsems atlhl a(tc, ofomr $i\\in G$ e,  swaitmhp )s f frraoctmi $i$ n-.t hF culrutshteerr,  t(ihia)t  srteatmeasi tnheadt  itnh $S$ s aetfts ear $S_{g_{i}}^{(1)}$ $\\varepsilon$ to $\\upsigma$ , corresponding to $i\\in G$ , almost entirely contain $C_{i}^{*}$ . Finally, (iii) describes the sets that do not correspond to any $g_{i},i\\in G$ . These sets must either be small, or contain a significant amount of outlier points in the neighborhood. The proofs of Theorems B.6 and B.7 can be found in Appendix E. ", "page_idx": 18}, {"type": "text", "text": "C Proof of Theorem 3.3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we state and prove a refined version of our main result, Theorem C.1, from which the statement of Theorem 3.3 directly follows. ", "page_idx": 18}, {"type": "text", "text": "C.1 General theorem statement ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We define ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\psi_{t}(\\alpha)={\\binom{{\\sqrt{t}}(1/\\alpha)^{1/t}}{\\sqrt{2e\\log1/\\alpha}}}\\quad{\\mathrm{if}}\\,t\\leqslant2\\log1/\\alpha,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which captures a tail decay of a distribution with sub-Gaussian $t$ -th central moments: $\\begin{array}{r}{\\mathbb{P}_{x\\sim\\mathcal{D}}\\left(\\langle\\bar{x_{}}-\\mu,v\\rangle^{t}\\geqslant\\psi_{t}(\\alpha)\\right)\\stackrel{\\cdot}{\\sim}\\alpha.}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "We now state our main result for list-decodable mixture learning. Recall that $\\varepsilon_{\\mathrm{RME}}$ is defined in Assumption 3.1. ", "page_idx": 18}, {"type": "text", "text": "Theorem C.1 (Main mixture model result). Let $d,k\\in\\mathbb{N}_{+}$ , $w_{\\mathrm{low}}\\,\\in\\,(0,10^{-4}],$ , and $t$ be an even integer. For all $i=1,\\ldots,k_{\\!}$ , let $D_{i}(\\mu_{i})$ be a $d$ -dimensional distribution with mean $\\mu_{i}\\in\\mathbb{R}^{d}$ and sub-Gaussian $t$ -th central moments. Let $\\varepsilon>0$ and, for all $i=1,\\ldots,k,$ , let $w_{i}\\in[w_{\\mathrm{low}},1]$ , such that $\\begin{array}{r}{\\sum_{i=1}^{k}w_{i}+\\varepsilon=1.}\\end{array}$ . Let $\\mathcal{X}$ be the $d$ -dimensional mixture distribution ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{X}=\\sum_{i=1}^{k}w_{i}D_{i}(\\mu_{i})+\\varepsilon Q,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $Q$ is an unknown adversarial distribution that can depend on all the other parameters. Let $\\mathcal{A}_{\\mathrm{kLD}}$ and $\\boldsymbol{\\mathcal{A}}_{R}$ satisfy Assumption 3.1. Further, suppose that $\\|\\bar{\\mu}_{i}-\\mu_{j}\\|\\geqslant200\\psi_{t}(w_{\\mathrm{low}}^{4})\\!+\\!200f(w_{\\mathrm{low}})$ for all $i\\neq j\\in[k]$ . ", "page_idx": 18}, {"type": "text", "text": "Then there exists an algorithm (Algorithm 2) that, given 1 $\\mathrm{oly}(d,1/w_{\\mathrm{low}})\\cdot(N_{L D}(w_{\\mathrm{low}})+N_{R}(w_{\\mathrm{low}}))$ i.i.d. samples from $\\mathcal{X}$ , and given also $d,\\ k_{\\perp}$ , $w_{\\mathrm{low}}$ , and $t$ , runs in time po $\\mathrm{iy}(d,1/w_{\\mathrm{low}})\\cdot(T_{L D}(w_{\\mathrm{low}})+T_{R}(w_{\\mathrm{low}}))$ and with probability at least $1-w_{\\mathrm{low}}^{O(1)}$ outputs a list $L$ of size $|L|\\leqslant k+O(\\varepsilon/w_{\\mathrm{low}})$ such that, for each $i\\,\\in\\,[k]$ , there exists $\\hat{\\mu}\\in L$ such that: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|\\b{\\hat{\\mu}}-\\mu_{i}\\|\\leqslant O\\big(\\psi_{t}(\\b{\\tilde{w}}_{i}/10)+f\\big(\\b{\\tilde{w}}_{i}/10\\big)\\big),\\qquad w h e r e\\quad\\b{\\tilde{w}}_{i}=w_{i}\\big/\\big(w_{i}+\\varepsilon+w_{\\mathrm{low}}^{2}\\big).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "If the relative weight of the $i$ -th inlier cluster is large, i.e., $\\tilde{w}_{i}\\geqslant1-\\varepsilon_{\\mathrm{RME}}+2w_{\\mathrm{low}}^{2}$ , then there exists $\\hat{\\mu}\\in L$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\lVert\\hat{\\mu}-\\mu_{i}\\rVert\\leqslant O(g(\\tilde{w}_{i}-3w_{\\mathrm{low}}^{2})).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Further, we assume $w_{\\mathrm{low}}\\,\\in\\,(0,1/10000]$ , since this simplifies some of the proofs. We note that in a mixture with $k$ components we necessarily have $w_{\\mathrm{low}}~\\leqslant~1/k$ . Furthermore, when $w_{\\mathrm{low}}~\\in$ $(1/10000,1/2]$ , then we obtain the same result by replacing $w_{\\mathrm{low}}$ with $w_{\\mathrm{low}}/5000$ throughout the statements and the proof. This would only affect both list size and error guarantees by at most a multiplicative constant, which is absorbed in the Big-O notation. ", "page_idx": 18}, {"type": "text", "text": "Proof of Theorem 3.3. Proof follows directly from Theorem C.1, by noticing that Assumption 3.2 allows to replace $f(\\tilde{w}_{i}/10)$ by $C f(\\tilde{w}_{i})$ and $g(\\tilde{w}_{i}-3w_{\\mathrm{low}}^{2})$ by $C g(\\tilde{w}_{i})$ for some constant $C>0$ large enough. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "C.2 Proof of Theorem C.1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We now show how to use the results on the inner and outer stage, Theorem B.2 and Theorem B.7 respectively, to arrive at the guarantees for the full algorithm Algorithm 2 in Theorem C.1. For simplicity of the exposition, we split the proof of Theorem C.1 into two separate parts, proving that (i) the output list contains an estimate with small error and that (ii) the size of the output list is small. In what follows we condition on the event $E^{\\prime}$ from the proof of Theorem B.7. ", "page_idx": 19}, {"type": "text", "text": "(i) Proof of error statement We now prove that, conditioned on the event $E$ , the list $L$ output by Algorithm 2 for each $i\\in[k]$ contains an estimate $\\hat{\\mu}\\in L$ , such that, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\hat{\\mu}-\\mu_{i}\\|\\leqslant O(\\psi_{t}(\\tilde{w}_{i}/10)+f(\\tilde{w}_{i}/10)),}\\\\ &{\\mathrm{if~}\\tilde{w}_{i}\\geqslant1-\\varepsilon_{\\mathrm{RME}}+2w_{\\mathrm{low}}^{2},\\mathrm{then~}\\|\\hat{\\mu}-\\mu_{i}\\|\\leqslant O(g(\\tilde{w}_{i}-3w_{\\mathrm{low}}^{2})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We start by showing that list-decoding error guarantees as in (1) are achievable for all inlier clusters and proceed by improving the error to (2) with RME base learner. Recall that $G$ is as defined in Eq. (B.1). ", "page_idx": 19}, {"type": "text", "text": "Proof of $(I)$ We now show how the output of the base learner and flitering procedure lead to the error in (1). Fix $i\\in[k]$ . Recall that $C_{i}$ denotes the set of $w_{i}n$ points from $i$ -th inlier component with mean $\\mu_{i}$ . ", "page_idx": 19}, {"type": "text", "text": "If $\\textit{i}\\in\\textit{G}$ , then on event $E$ , we have $\\left|S_{g_{i}}^{(2)}\\cap C_{i}^{*}\\right|~\\geqslant~(1~-~w_{\\mathrm{low}}^{2})w_{i}n$ by Theorem B.7 (ii), $\\begin{array}{r}{\\sum_{j\\neq i}\\left|S_{g_{i}}^{(2)}\\cap C_{j}^{*}\\right|\\leqslant w_{\\mathrm{low}}^{4}n}\\end{array}$ by Theorem B.6 (iv), and that the total number of adversarial points is at most $(\\varepsilon+w_{\\mathrm{low}}^{4})n$ . ", "page_idx": 19}, {"type": "text", "text": "Therefore, the fraction of points from $C_{i}^{*}$ in $S_{g_{i}}^{(2)}$ is at least (w1\u2212+w\u03b5l2+oww)3wi, which implies \u03b1 \u2a7ew\u02dci as in Definition B.1. Then, by Theorem B.2, the InnerStage algorithm applied to $T$ leads to error $\\|\\hat{\\mu}-\\mu_{i}\\|\\leqslant O(\\psi_{t}(\\tilde{w}_{i}/4)+\\^{}f(\\tilde{w}_{i}/4))$ . Otherwise, if $i\\not\\in G$ , when the OuterStage algorithm reaches the else statement, $S$ contains at least $(1-O(w_{\\mathrm{low}}^{3}))\\ddot{|}C_{i}^{*}|$ samples from $C_{i}^{*}$ . Indeed, since $i\\not\\in G$ , each time we remove points from $S$ , we remove at most $w_{\\mathrm{low}}^{4}n$ points from $C_{i}^{*}$ . By Theorem B.6 (i), we do at most $O(1/w_{\\mathrm{low}})$ removals, so when the OuterStage algorithm reaches the else statement, $S$ contains at least $\\left(1-\\mathcal{O}(w_{\\mathrm{low}}^{3})\\right)|C_{i}^{*}|$ samples from $C_{i}^{*}$ . ", "page_idx": 19}, {"type": "text", "text": "We showed that samples from $C_{i}^{*}$ make up at least a $(1\\mathrm{~-~}w_{\\mathrm{low}}^{2})w_{i}n/|S|$ fraction of $S$ . Based on this fact we can then use Theorem B.7 (iv) and the assumption on the range of $w_{\\mathrm{low}}$ to conclude that |S| \u2a7d 2.5\u03b5n and that the fraction of inliers is at least (1 \u2212wl2ow)w $(1\\,-\\,\\bar{w_{\\mathrm{low}}^{2}})w_{i}/(2.5\\varepsilon)$ . Therefore, $S$ can be seen as containing samples from the corruption model cor-aLD with $\\alpha$ at least $w_{i}/(2.5\\varepsilon)\\geqslant w_{i}/(2.5(w_{i}+\\varepsilon))$ . Since $S$ is added to $\\upsigma$ in the else statement, applying InnerStage yields the error bound as in (1). ", "page_idx": 19}, {"type": "text", "text": "Proof of (2): Next, we prove that for all inlier components $i$ with large weight, i.e., such that $w_{i}/(w_{i}+\\varepsilon)\\geqslant1-\\varepsilon_{\\mathrm{RME}}$ , there exists a set $T\\in\\mathcal{T}$ that consists of samples from the corruption model cor-aLD with $\\alpha\\geqslant w_{i}/(w_{i}+\\varepsilon)-2w_{\\mathrm{low}}^{2}$ . Then, running InnerStage, in particular the RME base learner, results in the error bound as in (2) by Theorem B.2 (ii). If $i\\in G$ , in the previous paragraph we showed that there exists T \u2208T, such that the corresponding \u03b1 \u2a7ewi+\u03b5w+iw3 $\\begin{array}{r}{\\alpha\\geqslant\\frac{w_{i}}{w_{i}+\\varepsilon+w_{\\mathrm{low}}^{3}}\\stackrel{\\cdot}{\\geqslant}\\frac{w_{i}}{w_{i}+\\varepsilon}\\stackrel{\\cdot}{-}2\\tilde{w}_{\\mathrm{low}}^{2}}\\end{array}$ . We now prove by contradiction that the case $i\\not\\in G$ does not occur. Now assume $i\\not\\in G$ so that as we argued before when the else statement is triggered, $S$ contains at least $(1-O(w_{\\mathrm{low}}^{3}))|C_{i}^{*}|$ samples from $C_{i}^{*}$ . By Theorem B.6 (ii), for some $m_{i}\\in[|M|]$ , we have that $|S_{m_{i}}^{(1)}\\cap C_{i}^{*}|\\geqslant(1-w_{\\mathrm{low}}^{2}/2-O(w_{\\mathrm{low}}^{3}))|C_{i}^{*}|$ and by Theorem B.6 (iv), $S_{m_{i}}^{(2)}$ contains at most $w_{\\mathrm{low}}^{4}n$ samples from other true clusters. Then, since $|S_{m_{i}}^{(2)}|>2|S_{m_{i}}^{(1)}|$ , we have that $\\lvert S_{m_{i}}^{(2)}\\rvert$ contains at least ", "page_idx": 19}, {"type": "equation", "text": "$$\n(1-w_{\\mathrm{low}}^{2}-O(w_{\\mathrm{low}}^{3}))|C_{i}^{*}|-w_{\\mathrm{low}}^{4}n\\geqslant(1-1.5w_{\\mathrm{low}}^{2})|C_{i}^{*}|\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "adversarial samples. Therefore, $\\varepsilon\\geqslant(1-1.5w_{\\mathrm{low}}^{2})|C_{i}^{*}|/n$ , and using that $|C_{i}^{*}|\\geqslant w_{i}n-w_{\\mathrm{low}}^{10}n,$ we have $\\varepsilon\\geqslant(1-1.5w_{\\mathrm{low}}^{2})w_{i}-w_{\\mathrm{low}}^{10}$ . However, this contradicts $w_{i}/(w_{i}+\\varepsilon)\\geqslant1-\\varepsilon_{\\mathrm{RME}}$ unless $\\varepsilon_{\\mathrm{RME}}\\geqslant1/2-2w_{\\mathrm{low}}^{2}$ , which is prohibited by the assumptions in Theorem C.1. Therefore whenever $w_{i}/(w_{i}+\\varepsilon)\\geqslant1-\\varepsilon_{\\mathrm{RME}}$ we are in the case $i\\in G$ . ", "page_idx": 19}, {"type": "text", "text": "(ii) Proof of small list size We now prove that on the set $E$ , we have that $|L|\\leqslant k+O(\\varepsilon/w_{\\mathrm{low}})$ . Here, we need to carefully analyze iterations in the while loop where an inlier component is \"selected\" for the first time in order to obtain a tight list size bound. Recall that $g_{i}$ corresponds to the index in $R$ that is first selected for the $i$ -th inlier cluster. ", "page_idx": 20}, {"type": "text", "text": "First selection of a component: For any $i~\\in~[k]$ , if $\\textit{i}\\in\\textit{G}$ , then Theorem B.7 (ii) implies that running InnerStage on $S_{g_{i}}^{(2)}$ produces a list of size at most $1\\!+\\!O((|S_{g_{i}}^{(2)}\\setminus C_{i}^{*}|)/(w_{\\mathrm{low}}n))$ . Then, over all i \u2208G, these sets Sg(i2) contribute to the list size $|L|$ at most $\\begin{array}{r}{k+O\\left(\\sum_{i=1}^{k}|S_{g_{i}}^{(2)}\\setminus C_{i}^{*}|/(w_{\\mathrm{low}}n)\\right)}\\end{array}$ . Furthermore, by Theorem B.6 (v), all these sets $S_{g_{i}}^{(2)}$ are disjoint and each of them contains at most $w_{\\mathrm{low}}^{4}n$ samples from other true clusters. Thereforei $\\begin{array}{r}{\\sum_{i=1}^{k}|S_{g_{i}}^{(2)}\\setminus C_{i}^{*}|\\leqslant\\varepsilon n+O(w_{\\mathrm{low}}^{3})n}\\end{array}$ . Then the contribution to of all these \u2019s corresponding to true clusters is at most $k\\!+\\!O\\left((\\varepsilon+w_{\\mathrm{low}}^{3})/w_{\\mathrm{low}}\\right)$ . Note that if $\\varepsilon\\leqslant w_{\\mathrm{low}}^{3}$ and $w_{\\mathrm{low}}$ is small enough, Algorithm 3 actually produces a list of size 1 in each run considered above, so the contribution is exactly $k$ ; otherwise we can bound the contribution by $k+O(\\varepsilon/w_{\\mathrm{low}})$ . ", "page_idx": 20}, {"type": "text", "text": "Samples left over from a component: Next, all inlier samples that were not removed, i.e., constituting $U$ , can be considered outlier points for the future iterations, which, by Theorem B.7 (i), only increases the outlier fraction to $\\tilde{\\varepsilon}=3\\varepsilon+O(w_{\\mathrm{low}}^{2})$ . For the same reason as above, without loss of generality, we can consider $\\varepsilon>w_{\\mathrm{low}}^{2}$ since otherwise, the corresponding list size overhead (for small enough wl2ow) would again amount to zero. ", "page_idx": 20}, {"type": "text", "text": "Clusters of adversarial samples: For iterations where a set ${S}_{j}^{(2)}$ was added to the final list, which does not correspond to some $g_{i},i\\ \\in\\ G$ , Theorem B.7 (iii), states that either (i) at least half of the samples in Sj(2 were adversarial, or (ii) the cardinality of the set on which Algorithm 3 was executed is small. In both cases the set ${S}_{j}^{(2)}$ contributes at most $O(\\varepsilon/w_{\\mathrm{low}})$ to the final list size $|L|$ . ", "page_idx": 20}, {"type": "text", "text": "List size in the else statement: Finally, when the algorithm reaches the else statement, as argued in the first part, by Theorem B.7 (iv), at that iteration $|\\bar{S}|\\leqslant O(\\varepsilon)n$ . Since Algorithm 3 always produces a list of size bounded by $O(|S|/(w_{\\mathrm{low}}n))$ (see Remark B.4), the contribution to $|L|$ at this iteration is bounded by $O(\\varepsilon/w_{\\mathrm{low}})$ . ", "page_idx": 20}, {"type": "text", "text": "Overall, we obtain the desired bound on $|L|$ of $k+O(\\varepsilon/w_{\\mathrm{low}})$ . ", "page_idx": 20}, {"type": "text", "text": "D Proof of Theorem B.2 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "(i) Proof of error statement We now prove that, with probability $1-w_{\\mathrm{low}}^{O(1)}$ , for the output list $L$ of Algorithm 6, ", "page_idx": 20}, {"type": "text", "text": "1. there exists $\\hat{\\mu}\\in L$ such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|\\hat{\\mu}-\\mu^{*}\\|\\leqslant O(\\psi_{t}(\\alpha/4)+f(\\alpha/4)),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "2. if $\\alpha\\geqslant1-\\varepsilon_{\\mathrm{RME}}$ , then there exists $\\hat{\\mu}\\in L$ such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\hat{\\mu}-\\mu^{*}\\|\\leqslant O(g(\\alpha-w_{\\mathrm{low}}^{2})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Ds.u1c hw et hhata $|M|\\,\\leqslant\\,1/w_{\\mathrm{low}}^{O(1)}$ .a bTilhiteyn  aLt elemasmt $1-w_{\\mathrm{low}}^{O(1)}$ ,e st htehraet ,e xwiistths $(\\dot{\\mu},\\hat{\\alpha})\\,\\in\\,M$ $\\hat{\\alpha}~\\geqslant~\\alpha/4$ $\\|\\tilde{\\hat{\\mu}}-\\mu^{*}\\|\\,\\leqslant\\,\\bar{f}(\\hat{\\alpha})$ probability at least $1-|M|^{2}w_{\\mathrm{low}}^{O(1)}$ , $(\\hat{\\mu},\\hat{\\alpha})$ will not be removed from $M$ . Therefore, either $(\\hat{\\mu},\\hat{\\alpha})\\in L$ , or there exists $(\\tilde{\\mu},\\tilde{\\alpha})\\in L$ such that (i) $\\tilde{\\alpha}\\geqslant\\hat{\\alpha}$ and (ii) $\\|\\tilde{\\mu}-\\hat{\\mu}\\|\\leqslant4\\beta(\\hat{\\alpha})$ . The latter case implies that $\\|\\tilde{\\mu}-\\mu^{*}\\|\\leqslant40\\psi_{t}(\\alpha/4)+4f(\\alpha/4)$ . ", "page_idx": 20}, {"type": "text", "text": "For the second part, set first $\\tilde{\\mu}=\\hat{\\mu}$ . Then, in the $i^{\\mathrm{th}}$ iteration, $\\tilde{\\mu}$ moves away by at most $(3\\tau/2)/2^{i-1}$ . Since $\\textstyle\\sum_{i=1}^{\\infty}1/2^{i}\\leqslant1$ , the distance between $\\tilde{\\mu}$ and $\\hat{\\mu}$ is always bounded by $3\\tau$ . Now, assume that indee d $\\alpha\\geqslant1-\\varepsilon_{\\mathrm{RME}}$ and $\\|{\\hat{\\mu}}-\\mu^{*}\\|\\leqslant\\tau$ . Whenever $\\tilde{\\alpha}\\leqslant\\alpha$ , with high probability $\\boldsymbol{\\mathcal{A}}_{R}$ produces some $\\mu_{\\mathrm{RME}}$ such that $\\lVert\\mu_{\\mathrm{RME}}-\\mu^{*}\\rVert\\leqslant g(\\tilde{\\alpha})\\leqslant\\tilde{\\beta}/2$ . Furthermore, as long as $\\tilde{\\alpha}\\leqslant\\alpha$ , at the moment of the while statement check we have $\\|{\\tilde{\\mu}}-\\mu^{*}\\|\\leqslant{\\tilde{\\beta}}$ : in the first iteration this is by assumption, and in later iterations it follows because $\\tilde{\\mu}$ is the former $\\mu_{\\mathrm{RME}}$ . Therefore the while statement check passes as long as $\\tilde{\\alpha}\\leqslant\\alpha$ . ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "There exists the possibility that the algorithm returns or breaks even though $\\tilde{\\alpha}\\leqslant\\alpha$ . If the algorithm returns early, then the error $\\tau$ achieved by $\\hat{\\mu}$ is already within a factor of two of the optimal. If the algorithm breaks, either $\\tilde{\\alpha}+w_{\\mathrm{low}}^{2}>1$ , case in which $\\tilde{\\mu}$ already satisfies $\\|\\tilde{\\mu}-\\mu^{*}\\|\\leqslant\\^{\\!\\!*}g(1-w_{\\mathrm{low}}^{2})$ , or else $\\|\\tilde{\\mu}-\\mu^{*}\\|$ is already within a factor of two of the optimal. Therefore these cases do not affect the error negatively. ", "page_idx": 21}, {"type": "text", "text": "Finally, let us consider what happens when $\\tilde{\\alpha}>\\alpha$ and the while statement check continues to pass. The first time we reach some $\\tilde{\\alpha}>\\alpha$ , we must have $\\|\\tilde{\\mu}-\\mu^{*}\\|\\leqslant\\tilde{\\beta}\\leqslant2g(\\alpha-w_{\\mathrm{low}}^{2})$ . Then, in later iterations, $\\tilde{\\mu}$ can move from this estimate by a distance of most $3\\tilde{\\beta}\\leqslant6g(\\alpha-w_{\\mathrm{low}}^{2})$ , by the same argument as the argument that $\\|\\tilde{\\mu}-\\hat{\\mu}\\|\\leqslant3\\tau$ . Overall, at the end we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\tilde{\\mu}-\\hat{\\mu}\\|\\leqslant\\operatorname*{max}(2g(1),g(1-w_{\\mathrm{low}}^{2}),8g(\\alpha))\\leqslant8g(\\alpha-w_{\\mathrm{low}}^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The number of runs is at most $1/w_{\\mathrm{low}}^{2}$ , so with probability $1-w_{\\mathrm{low}}^{O(1)}$ wlOo(w1)all runs of AR succeed. ", "page_idx": 21}, {"type": "text", "text": "We showed that there exists $(\\hat{\\mu},\\hat{\\alpha})\\in L$ such that $\\hat{\\alpha}\\geqslant\\alpha/4$ and $\\|\\hat{\\mu}-\\mu^{*}\\|\\leqslant40\\psi_{t}(\\hat{\\alpha})+4f(\\hat{\\alpha})$ . This error can increase by running ImproveWithRME with $\\tau=40\\psi_{t}(\\hat{\\alpha})+4f(\\hat{\\alpha})$ to at most ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|\\hat{\\mu}-\\mu^{*}\\|\\leqslant160\\psi_{t}(\\hat{\\alpha})+16f(\\hat{\\alpha})=O(\\psi_{t}(\\alpha)+f(\\alpha)).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Furthermore, if $\\alpha\\geqslant1-\\tau_{\\operatorname*{min}}$ , this $(\\hat{\\mu},\\hat{\\alpha})\\in L$ satisfies the conditions of ImproveWithRME, so with high probability the error is reduced by running ImproveWithRME with $\\tau=40\\psi_{t}(\\hat{\\alpha})\\!+\\!4f(\\hat{\\alpha})$ to $\\|\\bar{\\mu}-\\mu^{*}\\|\\leqslant8g(\\bar{\\alpha}-w_{\\mathrm{low}}^{2})$ . ", "page_idx": 21}, {"type": "text", "text": "(ii) Proof of list size We now prove that $|L|\\leqslant1+O((1-\\alpha)/\\alpha_{\\mathrm{low}})$ . ", "page_idx": 21}, {"type": "text", "text": "First, assume that $\\alpha\\leqslant9/10$ . Since all $\\hat{\\alpha}_{s}\\geqslant\\alpha_{\\mathrm{low}}$ , we have that $|L|\\leqslant10/(9\\alpha_{\\mathrm{low}})\\leqslant12(1\\!-\\!\\alpha)/\\alpha_{\\mathrm{low}}$ . ", "page_idx": 21}, {"type": "text", "text": "For the rest of the proof we assume that $\\alpha\\,>\\,9/10$ . We analyze sets $J$ and $T_{i}$ for $i\\in J$ at the end of execution of ListFilter. In particular, recall that $L=\\dot{\\{}(\\hat{\\mu}_{i},\\hat{\\alpha}_{i}),\\,\\,i\\in J\\}$ . Also, at the end of Algorithm 4 we have the following expression for $T_{i}$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\nT_{i}=\\bigcap_{j\\in J\\backslash\\{i\\}}\\{x\\in S,\\mathrm{~s.t.~}|v_{i j}^{\\top}(x-\\hat{\\mu}_{i})|\\leqslant\\operatorname*{max}(\\beta(\\hat{\\alpha}_{i}),\\beta(\\hat{\\alpha}_{j}))\\},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $v_{i j}$ are unit vectors in direction $\\hat{\\mu}_{i}-\\hat{\\mu}_{j}$ . Select the $s\\in J$ for which $\\hat{\\alpha}_{s}$ is maximized. By (i), with probability at least $1-w_{\\mathrm{low}}^{60}$ , there exists a hypothesis in $J$ with $\\hat{\\alpha}\\geqslant\\alpha/4\\geqslant0.2.$ . Then we have that $\\hat{\\alpha}_{s}\\geqslant0.2$ . In addition, for all hypotheses, $\\hat{\\alpha}_{s}\\leqslant1/3.$ Let $j\\in J$ be such that $j\\neq s$ . We will show that at least half of the points in $T_{j}$ are adversarial, i.e., $|T_{j}|\\geqslant2\\,|T_{j}\\cap C^{*}|$ . If this is indeed the case, we can treat all inlier points in all $T_{j}$ as outliers, as it would at most double total number of outlier points in $S$ . ", "page_idx": 21}, {"type": "text", "text": "Now, assume that for some $j\\neq s$ , $|T_{j}|<2\\,|T_{j}\\cap C^{*}|$ . Note that, because $|T_{s}|\\geqslant0.9\\cdot0.2n=0.18n$ and $|C^{*}|\\geqslant0.9n,|T_{s}\\cap C^{*}|\\geqslant0.18{\\stackrel{\\cdot}{n}}-0.1n\\stackrel{\\cdot}{\\geqslant}0.08n$ . Therefore ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\left\\{x\\in C^{*},\\mathrm{s.t.}\\;|v_{s j}^{\\top}(x-\\hat{\\mu}_{s})|\\leqslant\\beta(\\hat{\\alpha}_{s})\\right\\}\\right|\\geqslant0.08\\left|C^{*}\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Also note that Lemma H.2 for radius $10\\psi_{t}(1/2)\\leqslant10\\psi_{t}(\\hat{\\alpha}_{s})$ implies that, with exponentially small failure probability, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\left\\{x\\in C^{*},\\mathrm{s.t.}\\;|v_{s j}^{\\top}(x-\\mu^{*})|\\leqslant\\beta(\\hat{\\alpha}_{s})\\right\\}\\right|\\geqslant0.99\\left|C^{*}\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since these two sets necessarily intersect, we can bound $|v_{s j}^{\\top}(\\hat{\\mu}_{s}-\\mu^{*})|\\leqslant2\\beta(\\hat{\\alpha}_{s})$ , implying that $|v_{s j}^{\\top}(\\hat{\\mu}_{j}\\,-\\,\\mu^{*})|\\;\\geqslant\\;2\\beta(\\hat{\\alpha}_{j})$ , since $\\|\\hat{\\mu}_{s}\\,-\\,\\hat{\\mu}_{j}\\|\\;\\geqslant\\;4\\beta(\\hat{\\alpha}_{j})$ . Thus, if $|v_{s j}^{\\top}(x-\\hat{\\mu}_{j})|\\,\\leqslant\\,\\beta(\\hat{\\alpha}_{j})$ , then $|v_{s j}^{\\top}(x-\\mu^{*})|>\\beta(\\hat{\\alpha}_{j})$ , implying that ", "page_idx": 21}, {"type": "equation", "text": "$$\n(T_{j}\\cap C^{*})\\subseteq\\left\\{x\\in C^{*},\\,\\mathrm{s.t.}\\;|v_{s j}^{\\top}(x-\\mu^{*})|>\\beta(\\hat{\\alpha}_{j})\\right\\}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "However, Lemma H.2 tells us that with high probability only a small fraction of points in $C^{*}$ satisfies $|v_{s j}^{\\top}(x-\\mu^{*})|>\\beta(\\hat{\\alpha}_{j})$ . In particular, applying the lemma with radius $10\\psi_{t}(\\hat{\\alpha}_{j})$ , we get that with exponentially small failure probability, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left|\\left\\{x\\in C^{*},\\mathrm{s.t.}\\;|v_{s j}^{\\top}(x-\\mu^{*})|\\leqslant\\beta(\\hat{\\alpha}_{j})\\right\\}\\right|\\geqslant\\left(1-\\frac{\\hat{\\alpha}_{j}}{50}\\right)|C^{*}|\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "From eqs. (D.1) and (D.2) it follows that $\\left\\vert T_{j}\\cap C^{*}\\right\\vert\\leqslant\\hat{\\alpha}_{j}\\left\\vert C^{*}\\right\\vert/50$ . Using that $|T_{j}|\\geqslant9\\hat{\\alpha}_{j}n/10\\geqslant$ $9\\hat{\\alpha}_{j}\\,|C^{*}|\\,/10$ by the properties of ListFilter, we obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n9\\hat{\\alpha}_{j}\\left|C^{*}\\right|/10\\leqslant\\left|T_{j}\\right|\\leqslant2\\left|T_{j}\\cap C^{*}\\right|\\leqslant\\hat{\\alpha}_{j}\\left|C^{*}\\right|/25,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which is a contradiction. ", "page_idx": 22}, {"type": "text", "text": "Therefore, for all $j\\in J$ such that $j\\neq s$ , we have that $|T_{j}|\\geqslant2\\,|T_{j}\\cap C^{*}|$ . As we said in the beginning, by treating all inlier points in those $T_{j}$ as outliers we at most double total number of outlier points. Since there are at most $(1-\\alpha+\\alpha w_{\\mathrm{low}}^{2})n$ outlier points and the sets $T_{j}$ are non-intersecting, we get $\\begin{array}{r}{\\sum_{j\\in J\\backslash\\{s\\}}|T_{j}|\\leqslant2(1-\\alpha+\\alpha w_{\\mathrm{low}}^{2})n}\\end{array}$ . The lower bound on the size $|T_{j}|\\geqslant9\\alpha_{\\mathrm{low}}n/10$ implies |J/ {s}| \u2a7d2(1\u2212\u03b1+\u03b1wl2ow)n\u00b710a nd thus $|L|=|J|\\leqslant1+3(1-\\alpha)/\\alpha_{\\mathrm{low}}$ . ", "page_idx": 22}, {"type": "text", "text": "Note that in InnerStage we set $\\alpha_{\\mathrm{low}}=\\operatorname*{min}(\\alpha_{\\mathrm{low}},1/100)$ . Therefore, for the original $\\alpha_{\\mathrm{low}}$ , the list size is bounded by $1+240(1-\\alpha)/\\alpha_{\\mathrm{low}}$ . ", "page_idx": 22}, {"type": "text", "text": "Conclusion Combining the probabilities of success of all steps, we get that the algorithm succeeds with probability at least 1 \u2212wlOo(w1) for some large constant in the exponent. Our algorithm, ignoring the calls to $\\mathcal{A}_{\\mathrm{kLD}}$ and $\\boldsymbol{\\mathcal{A}}_{R}$ , has sample complexity and time complexity bounded by $\\mathrm{poly}(d,1/w_{\\mathrm{low}})$ , which gives the desired sample and time complexity when taking $\\mathcal{A}_{\\mathrm{kLD}}$ and $\\boldsymbol{\\mathcal{A}}_{R}$ into consideration. This completes the proof of the theorem. ", "page_idx": 22}, {"type": "text", "text": "D.1 Auxiliary lemmas and proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Lemma D.1 (List initialization). Let $S$ , $\\alpha_{\\mathrm{low}}$ and $\\alpha$ be as in cor-aLD model. If InnerStage (Algorithm 3) is run with $S$ and $\\alpha_{\\mathrm{low}}$ , the size of $M$ is at most $1/w_{\\mathrm{low}}^{O(1)}$ , all $({\\hat{\\mu}},{\\hat{\\alpha}})\\in M$ satisfy $\\hat{\\alpha}\\leqslant1/3$ , and with probability at least $1-w_{\\mathrm{low}}^{O(1)}$ wlOo(w1) there exists (\u00b5\u02c6, \u03b1\u02c6) \u2208M such that \u03b1/4 \u2a7d\u03b1\u02c6 \u2a7dmin(\u03b1, 1/3) and $\\|\\hat{\\mu}-\\mu^{*}\\|\\leqslant f(\\hat{\\alpha})$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. There are at most $1/\\alpha_{\\mathrm{low}}$ choices for $\\hat{\\alpha}$ , and for each of them the output of $\\mathcal{A}_{\\mathrm{kLD}}$ has size at most $1/w_{\\mathrm{low}}^{O(1)}$ , so $|M|\\leqslant1/w_{\\mathrm{low}}^{O(1)}$ . With probability $1-w_{\\mathrm{low}}^{O(1)}$ wlOo(w1 ), AkLD succeeds in all up to 1/\u03b1low runs. Then we are guaranteed to produce one $\\hat{\\alpha}$ with $\\alpha/4\\stackrel{*}{\\leqslant}\\hat{\\alpha}\\leqslant\\operatorname*{min}(\\alpha,1/3)$ , and then $\\mathcal{A}_{\\mathrm{kLD}}$ is guaranteed to produce one corresponding $\\hat{\\mu}$ with $\\|\\hat{\\mu}-\\mu^{*}\\|\\leqslant f(\\hat{\\alpha})$ . \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Lemma D.2 (Good hypotheses are not removed). Let $S$ , $\\alpha_{\\mathrm{low}}$ and $\\alpha$ be as in cor-aLD model. Run ListFilter (Algorithm 4) on $S$ and $\\alpha_{\\mathrm{low}}$ with $M$ obtained from InnerStage (Algorithm 3) and call a hypothesis $(\\hat{\\mu},\\hat{\\alpha})\\in M$ good if $\\hat{\\alpha}\\geqslant\\alpha/4$ and $\\|\\hat{\\mu}-\\mu^{*}\\|\\leqslant f(\\hat{\\alpha})$ . Then, with probability at least $|M|^{2}w_{\\mathrm{low}}^{O(1)}$ , no good hypothesis is removed from $M$ (including in any of the reruns triggered by the algorithm). ", "page_idx": 22}, {"type": "text", "text": "Proof. Let $\\ell$ be an arbitrary iteration of the outer for loop. Then, at the beginning of the $\\ell^{\\mathrm{th}}$ iteration, ", "page_idx": 22}, {"type": "text", "text": "Indeed, the second property follows directly from the algorithm. ", "page_idx": 22}, {"type": "text", "text": "For the first property, assume that for $j<s\\in J$ , there exists $x\\in S$ , such that $x\\in T_{j}\\cap T_{s}$ . This would imply that $|v_{j s}^{\\top}(\\hat{\\mu}_{j}-\\hat{\\mu}_{s})|\\leqslant2\\beta(\\hat{\\alpha}_{s})$ , so $\\|\\hat{\\mu}_{j}-\\hat{\\mu}_{s}\\|\\leqslant2\\beta(\\hat{\\alpha}_{s})$ . However, in this case the first \u2019if\u2019 condition would pass and we would not add $s$ to $J$ . Thus, $T_{j}\\cap T_{s}=\\emptyset$ . ", "page_idx": 22}, {"type": "text", "text": "For the third property, note that ", "page_idx": 22}, {"type": "equation", "text": "$$\nn\\geqslant\\sum_{j\\in J}\\left|T_{j}\\right|\\geqslant\\sum_{j\\in J}0.9\\hat{\\alpha}_{j}n\\geqslant0.9\\left|J\\right|\\hat{\\alpha}_{\\ell}n,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which implies $\\lvert J\\rvert\\leqslant10/(9\\hat{\\alpha}\\ell)$ . ", "page_idx": 22}, {"type": "text", "text": "Now, let $s$ be the index of a hypothesis with $\\hat{\\alpha}_{s}\\geqslant\\alpha/4$ and $\\|{\\hat{\\mu}}_{s}-\\mu^{*}\\|\\leqslant f({\\hat{\\alpha}}_{s})$ . If $s$ was skipped in the $s^{\\mathrm{th}}$ iteration (i.e., there exists $j\\in J$ with $\\hat{\\mu}_{j}$ close to $\\displaystyle{\\hat{\\mu}}_{s.}$ ), then $\\left(\\hat{\\mu}_{s},\\hat{\\alpha}_{s}\\right)$ is trivially not removed from $M$ . For the rest of the proof we assume that $s$ is not skipped. ", "page_idx": 23}, {"type": "text", "text": "For the sake of the analysis, we introduce the analogue of the sets $T_{s}$ , which we call $\\tilde{T}_{s}$ , defined for points in the set $C^{*}$ (i.e., all inlier points before the adversarial removal), and show that (i) $\\left\\vert\\tilde{T}_{s}\\right\\vert$ is large and (ii) $\\left|\\tilde{T}_{s}\\setminus T_{s}\\right|$ is small. In particular, let ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\tilde{T}_{s}=\\bigcap_{j\\in J}\\left\\{x\\in C^{*},\\mathrm{~s.t.~}|v_{j s}^{\\top}(x-\\hat{\\mu}_{s})|\\leqslant\\beta(\\hat{\\alpha}_{s})\\right\\},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we recall $\\beta(\\hat{\\alpha}_{s})=10\\psi_{t}(\\hat{\\alpha}_{s})+f(\\hat{\\alpha}_{s})$ . Note that $|T_{s}|\\geqslant\\left|\\tilde{T}_{s}\\right|-|C^{*}\\setminus S^{*}|\\geqslant\\left|\\tilde{T}_{s}\\right|-w_{\\mathrm{low}}^{2}\\left|C^{*}\\right|$ Also, for any $\\alpha^{\\prime}\\leqslant\\hat{\\alpha}_{s}$ , applying Lemma H.2 with radius $10\\psi_{t}(\\dot{\\alpha}^{\\prime})$ , using that $\\lVert\\hat{\\mu}_{s}-\\mu^{*}\\rVert\\leqslant f(\\hat{\\alpha}_{s})\\leqslant$ $f(\\alpha^{\\prime})$ and $t\\geqslant2$ , we get that with exponentially small failure probability, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left|\\{x\\in C^{*},\\mathrm{s.t.}\\;|v^{\\top}(x-\\hat{\\mu}_{s})|>\\beta(\\alpha^{\\prime})\\}\\right|\\leqslant\\frac{\\alpha^{\\prime}}{50}\\left|C^{*}\\right|.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Consider the $s^{\\mathrm{th}}$ iteration. Using a union bound over $|J|\\ \\leqslant\\ 2/\\alpha_{\\mathrm{low}}$ directions, and since all $\\hat{\\alpha}_{s}\\geqslant\\alpha_{\\mathrm{low}}$ , we get that with exponentially small failure probability ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left|\\tilde{T}_{s}\\right|\\geqslant\\left|C^{*}\\right|-\\sum_{i\\in J}\\left|\\left\\{x\\in C^{*},\\,\\mathrm{s.t.}\\,|v_{i s}^{\\top}(x-\\mu_{s})|>\\beta(\\hat{\\alpha}_{s})\\right\\}\\right|\\geqslant\\left(1-\\frac{\\hat{\\alpha}_{s}}{50}\\left|J\\right|\\right)\\left|C^{*}\\right|\\geqslant0.95\\left|C^{*}\\right|,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we used that and $|J|\\leqslant10/(9\\hat{\\alpha}_{s})$ . This implies that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\vert T_{s}\\right\\vert\\geqslant\\left\\vert\\tilde{T}_{s}\\right\\vert-w_{\\mathrm{low}}^{2}\\left\\vert C^{*}\\right\\vert\\geqslant\\left(0.95-w_{\\mathrm{low}}^{2}\\right)\\left\\vert C^{*}\\right\\vert\\geqslant0.92\\left\\vert C^{*}\\right\\vert\\geqslant0.9\\alpha n\\geqslant0.9\\hat{\\alpha}_{s}n,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "i.e., $\\left(\\hat{\\mu}_{s},\\hat{\\alpha}_{s}\\right)$ is not removed from $M$ during $s^{\\mathrm{th}}$ iteration. ", "page_idx": 23}, {"type": "text", "text": "The pair $\\left(\\hat{\\mu}_{s},\\hat{\\alpha}_{s}\\right)$ could also be removed during later iterations, when we recalculate $T_{s}$ by removing points along new directions. However, following a similar argument, we show that still, with high probability, $|T_{s}|\\geqslant0.9\\hat{\\alpha}_{s}n$ . Assume that we are now in the $k^{\\mathrm{{\\bar{t}}h}}$ iteration of the outer cycle, where $k>s$ . We define again $\\tilde{T}_{s}$ and sets $A,B$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{T}_{s}:=\\displaystyle\\bigcap_{i\\in J\\backslash\\{s\\}}\\left\\{x\\in C^{*},\\mathrm{~s.t.~}|v_{i s}^{\\top}(x-\\hat{\\mu}_{s})|\\leqslant\\operatorname*{max}(\\beta(\\hat{\\alpha}_{s}),\\beta(\\hat{\\alpha}_{i}))\\right\\},}\\\\ &{A:=\\displaystyle\\bigcap_{i<s,i\\in J}A_{i},\\quad\\mathrm{~for~}\\quad A_{i}:=\\left\\{x\\in C^{*},\\mathrm{s.t.~}|v_{i s}^{\\top}(x-\\hat{\\mu}_{s})|\\leqslant\\beta(\\hat{\\alpha}_{s})\\right\\},}\\\\ &{B:=\\displaystyle\\bigcap_{i>s,i\\in J}B_{i},\\quad\\mathrm{~for~}\\quad B_{i}:=\\left\\{x\\in C^{*},\\mathrm{s.t.~}|v_{i s}^{\\top}(x-\\hat{\\mu}_{s})|\\leqslant\\beta(\\hat{\\alpha}_{i})\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "so that $\\tilde{T}_{s}=A\\cap B$ and again $\\left\\vert T_{s}\\right\\vert\\geqslant\\left\\vert\\tilde{T}_{s}\\right\\vert-w_{\\mathrm{low}}^{2}\\left\\vert C^{*}\\right\\vert$ . It is crucial that we have different right hand sides in the definitions of $A_{i}$ and $B_{i}$ (we wrote them in boldface to emphasize this). ", "page_idx": 23}, {"type": "text", "text": "Using a union bound again, we write ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left|\\tilde{T}_{s}\\right|\\geqslant\\left|C^{*}\\right|-\\sum_{i<s,i\\in J}\\left|C^{*}\\setminus A_{i}\\right|-\\sum_{i>s,i\\in J}\\left|C^{*}\\setminus B_{i}\\right|.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Using eq. (D.3), with exponentially small failure probability, for all $i\\in J$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|C^{*}\\setminus A_{i}|\\leqslant\\left(\\hat{\\alpha}_{s}/50\\right)|C^{*}|\\quad\\mathrm{(for~}i<s\\mathrm{)}\\quad\\mathrm{and}\\quad|C^{*}\\setminus B_{i}|\\leqslant\\left(\\hat{\\alpha}_{i}/50\\right)|C^{*}|\\quad\\mathrm{(for~}i>s\\mathrm{)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Next, note that before the last element was added, we had that (i) $T_{i}\\bigcap T_{j}=\\emptyset$ for any $i\\neq j\\in J$ and (ii) $|T_{i}|\\gg0.9\\hat{\\alpha}_{i}n$ for any $i\\in J$ . This implies that $\\begin{array}{r}{\\sum_{i\\in J}\\hat{\\alpha}_{i}\\,<\\,10/9+\\hat{\\alpha}_{\\mathrm{last}}\\,<\\,19/9}\\end{array}$ , where $\\hat{\\alpha}_{\\mathrm{last}}$ corresponds to the element which was added last (it might happen that after addition of the last element, we have $|T_{i}|<0.9\\hat{\\alpha}_{i}n$ for several $i\\in J$ ). Therefore, as before, we obtain that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\tilde{T}_{s}\\Big|\\geqslant\\left(1-\\sum_{i<s,i\\in J}(\\hat{\\alpha}_{s}/50)-\\sum_{i>s,i\\in J}(\\hat{\\alpha}_{i}/50)\\right)|C^{*}|\\geqslant\\left(1-10/(9\\cdot50)-19/(9\\cdot50)\\right)|C^{*}|\\geqslant0.93\\left|C^{*}\\right|\\geqslant0.93\\left|C^{*}\\right|\\,.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "therefore $\\left|T_{s}\\right|\\geqslant\\left|\\tilde{T}_{s}\\right|-w_{\\mathrm{low}}^{2}\\left|C^{*}\\right|\\geqslant0.92\\left|C^{*}\\right|\\geqslant0.9\\hat{\\alpha}_{s}n$ and $\\left(\\hat{\\mu}_{s},\\hat{\\alpha}_{s}\\right)$ will not be removed from $M$ . We established that in a single run of the algorithm a good hypothesis is removed with exponentially small probability. The number of good hypotheses is bounded by $|M|$ . Furthermore, the number of runs of the algorithm is also bounded by $|M|$ , since whenever the algorithm is rerun a hypothesis is removed from $M$ . Then, by a union bound, we can bound the probability that any good hypothesis is removed in any run of the algorithm by |M|2 wlOo(w1 ). \u53e3 ", "page_idx": 24}, {"type": "text", "text": "E Proof of outer stage algorithm guarantees in Appendix B.3 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Recall that $\\gamma=4\\psi_{t}(w_{\\mathrm{low}}^{4})$ and $\\gamma^{\\prime}=160\\psi_{t}(w_{\\mathrm{low}}/4)+16f(w_{\\mathrm{low}}/4).$ ", "page_idx": 24}, {"type": "text", "text": "E.1 Proof of Theorem B.6 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In what follows we condition on the event $E$ that the events under which the conclusions in Lemmas H.2 and H.3 hold and that AsLD succeeds. This event holds with probability 1 \u2212wlOo(w1) by Assumption 3.1, Remark B.3 and union bound (also see Appendix G). ", "page_idx": 24}, {"type": "text", "text": "Proof of Theorem B.6 (i) The list size bound follows from the standard results on $\\mathcal{A}_{\\mathrm{sLD}}$ (see [3], Proposition B.1). ", "page_idx": 24}, {"type": "text", "text": "Proof of Theorem B.6 (ii) Guarantees of $\\mathcal{A}_{\\mathrm{sLD}}$ imply that there exists $\\mu_{i}\\in M$ such that $\\parallel\\!\\mu_{i}-$ $\\mu^{*}\\Vert\\leqslant\\gamma^{\\prime}$ . By Lemma H.3, a $(1-w_{\\mathrm{low}}^{2}/2)$ -fraction of the samples in $C^{*}$ are $\\gamma$ -close to $\\mu^{*}$ along each direction $v_{i j}$ with $i\\neq j\\in[|M|]$ . Then, the same $(1-w_{\\mathrm{low}}^{2}/2)$ -fraction of samples are $(\\gamma+\\gamma^{\\prime})$ -close to $\\mu_{i}$ along each direction $v_{i j}$ , so they are included in ${S}_{i}^{(1)}$ ", "page_idx": 24}, {"type": "text", "text": "Proof of Theorem B.6 (iii) Suppose |Si(1)\u2229C\u2217| \u2a7ewl4ow|C\u2217|. Previous point implies that there exists $\\mu_{j}\\in M$ be such that $\\|\\boldsymbol{\\mu}_{j}-\\boldsymbol{\\mu}^{*}\\|\\leqslant\\gamma^{\\prime}$ . Then at least an $w_{\\mathrm{low}}^{4}$ -fraction of the samples in $C^{*}$ are $(\\gamma+\\gamma^{\\prime})$ -close to $\\mu_{i}$ in direction $\\mu_{i}-\\mu_{j}$ . By Lemma H.2, $\\mu^{*}$ is also $\\gamma$ -close in direction $\\mu_{i}-\\mu_{j}$ to more than a $(1-w_{\\mathrm{low}}^{4})$ -fraction of the samples in $C^{*}$ , so it is $\\gamma$ -close to at least one sample in any $w_{\\mathrm{low}}^{4}$ -fraction of samples in $C^{*}$ . Therefore $\\mu^{*}$ is also $(2\\gamma+\\gamma^{\\prime})$ -close to $\\mu_{i}$ in direction $\\mu_{i}-\\mu_{j}$ . Then $\\|\\ddot{\\mu}_{i}-\\mu_{j}\\|\\leqslant2\\gamma+2\\gamma^{\\prime}$ and $\\|\\mu_{i}-\\mu^{*}\\|\\leqslant2\\gamma+3\\gamma^{\\prime}$ . Again, using Lemma H.3 we obtain that there exists a $(1-w_{\\mathrm{low}}^{2}/2)$ -fraction of the samples in $C^{*}$ , which is included in $S_{i}^{(2)}$ . ", "page_idx": 24}, {"type": "text", "text": "Proof of Theorem B.6 (iv) Similarly, if $|S_{i}^{(2)}\\cap C^{*}|\\;\\geqslant\\;w_{\\mathrm{low}}^{4}|C^{*}|$ , then there exists $\\mu_{j}\\ \\in\\ M$ , such that at least an wl4ow -fraction of the samples in $C^{*}$ are $(3\\gamma+3\\gamma^{\\prime})$ -close to $\\mu_{i}$ in direction $\\mu_{i}-\\mu_{j}$ . By the same arguments as in previous paragraph, we obtain that $\\|\\mu_{i}-\\mu_{j}\\|\\leqslant4\\gamma+4\\gamma^{\\prime}$ and $\\|\\mu_{i}-\\stackrel{\\circ}{\\mu}^{*}\\|\\leqslant4\\gamma+5\\gamma^{\\prime}$ . ", "page_idx": 24}, {"type": "text", "text": "Then any other true cluster with mean $(\\mu^{*})^{\\prime}$ and set of samples $(C^{*})^{\\prime}$ satisfies $\\|\\mu^{*}-(\\mu^{*})^{\\prime}\\|\\geqslant$ $16\\gamma+16\\gamma^{\\prime}$ , so $\\lVert\\boldsymbol{\\mu}_{i}-(\\boldsymbol{\\mu}^{*})^{\\prime}\\rVert\\geqslant12\\gamma+11\\gamma^{\\prime}$ . From guarantees of $\\mathcal{A}_{\\mathrm{sLD}}$ , there exists $\\mu_{j}^{\\prime}\\in M$ such that $\\|\\mu_{j}^{\\prime}-(\\mu^{*})^{\\prime}\\|\\leqslant\\gamma^{\\prime}$ . Then $\\|\\mu_{i}-\\mu_{j}^{\\prime}\\|\\geqslant12\\gamma+10\\gamma^{\\prime}$ . By Lemma H.2, more than an $w_{\\mathrm{low}}^{4}$ -fraction of the samples from $(C^{*})^{\\prime}$ are $\\gamma$ -close to $(\\mu^{*})^{\\prime}$ in direction $\\mu_{i}-\\mu_{j}^{\\prime}$ , so also $(\\gamma+\\gamma^{\\prime})$ -close to $\\mu_{j}^{\\prime}$ in direction $\\mu_{i}-\\mu_{j}^{\\prime}$ , so also $(11\\gamma+9\\gamma^{\\prime})$ -far from $\\mu_{i}$ in direction $\\mu_{i}-\\mu_{j}^{\\prime}$ . Then ${S}_{i}^{(2)}$ selects at most a $w_{\\mathrm{low}}^{4}$ -fraction of the samples from $(C^{*})^{\\prime}$ . Overall, $S_{i}^{(2)}$ selects from all other true clusters at most wl4own samples. ", "page_idx": 24}, {"type": "text", "text": "Proof of Theorem B.6 (v) Note that by the same argument, $\\|\\mu_{i}-\\mu^{*}\\|\\leqslant4\\gamma+5\\gamma^{\\prime}$ and $\\parallel\\!\\mu_{i^{\\prime}}-$ $(\\mu^{*})^{\\prime}\\|\\leqslant4\\gamma+5\\gamma^{\\prime}$ . However, $\\lVert\\boldsymbol{\\mu}^{*}-(\\boldsymbol{\\mu}^{*})^{\\prime}\\rVert\\geqslant16\\gamma+16\\gamma^{\\prime}$ , so also $\\|\\mu_{i}-\\mu_{i^{\\prime}}\\|\\geqslant8\\gamma+6\\gamma^{\\prime}$ , so $S_{i}^{(2)}$ and S(\u20322) are disjoint by the condition that each selects only samples that are $(3\\gamma+3\\gamma^{\\prime})$ -close along direction $\\mu_{i}-\\mu_{i^{\\prime}}$ to the respective means $\\mu_{i}$ and $\\mu_{i^{\\prime}}$ . ", "page_idx": 24}, {"type": "text", "text": "E.2 Proof of Theorem B.7 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In the sequel, for any $i\\in G$ , let $m_{i}$ be the index in $R$ after initialization that satisfies Theorem B.6 (ii). We condition on the event $E^{\\prime}$ that event $E$ from the proof of Theorem B.6 holds and that both $||C_{i}^{*}|-w_{i}n|\\leqslant w_{\\mathrm{low}}^{10}n$ for all $i\\in[k]$ and the number of adversarial points lies in the range $\\varepsilon n{\\pm}w_{\\mathrm{low}}^{10}n$ . By Hoeffding\u2019s inequality and the union bound, the probability of $E^{\\prime}$ is at least $1-w_{\\mathrm{low}}^{O(1)}$ . ", "page_idx": 25}, {"type": "text", "text": "Proof of Theorem B.7 (i) Let $i\\,\\in\\,G$ , and consider the beginning of the iteration when $\\mu_{g_{i}}$ is selected. Then, using that all previous iterations could have removed at most $O(w_{\\mathrm{low}}^{3})|C_{i}^{*}|$ samples from $C_{i}^{*}$ , we have that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|S_{m_{i}}^{(1)}\\cap C_{i}^{*}|\\geqslant(1-w_{\\mathrm{low}}^{2}/2-O(w_{\\mathrm{low}}^{3}))|C_{i}^{*}|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Therefore at the iteration in which $\\mu_{g_{i}}$ is selected, we still have $m_{i}\\in R$ . We now discuss two cases: First, consider the case that $|S_{m_{i}}^{(2)}|\\leqslant2|S_{m_{i}}^{(1)}|$ . Then, because we selected $\\mu_{g_{i}}\\in M$ and not $\\mu_{m_{i}}\\in M$ it means that $|S_{g_{i}}^{(1)}|\\geqslant|S_{m_{i}}^{(1)}|\\geqslant(1-w_{\\mathrm{low}}^{2}/2-O(w_{\\mathrm{low}}^{3}))|C_{i}^{*}|$ . Note also by Theorem B.6 (iv), the number of samples from other true clusters in $S_{g_{i}}^{(2)}$ is at most $w_{\\mathrm{low}}^{4}n$ . Then the number of adversarial samples in $S_{g_{i}}^{(2)}$ is at least ", "page_idx": 25}, {"type": "text", "text": "$|S_{g_{i}}^{(2)}|-|C_{i}^{*}|-w_{\\mathrm{low}}^{4}n\\geqslant|S_{g_{i}}^{(2)}\\setminus S_{g_{i}}^{(1)}|-O(w_{\\mathrm{low}}^{2})|C_{i}^{*}|-w_{\\mathrm{low}}^{4}n\\geqslant|S_{g_{i}}^{(2)}\\setminus S_{g_{i}}^{(1)}|-O(w_{\\mathrm{low}}^{2})|C_{i}^{*}|.$ Then, either $\\begin{array}{c c}{\\displaystyle\\left\\vert\\boldsymbol{S}_{g_{i}}^{(2)}\\setminus\\boldsymbol{S}_{g_{i}}^{(1)}\\right\\vert~~=~~{\\cal O}(w_{\\mathrm{low}}^{2})\\vert{\\cal C}_{i}^{*}\\vert}\\end{array}$ and $\\begin{array}{c c c}{|U_{i}|}&{\\leqslant}&{\\left|S_{g_{i}}^{(2)}\\setminus S_{g_{i}}^{(1)}\\right|}&{=}&{O(w_{\\mathrm{low}}^{2})|C_{i}^{*}|}\\end{array}$ , or $\\left|S_{g_{i}}^{(2)}\\setminus S_{g_{i}}^{(1)}\\right|\\gg w_{\\mathrm{low}}^{2}|C_{i}^{*}|$ . In the latter case, even if $S_{g_{i}}^{(2)}\\setminus S_{g_{i}}^{(1)}$ consists of adversarial examples only, then, since $|S_{g_{i}}^{(2)}|\\leqslant2|S_{g_{i}}^{(1)}|$ , $U_{i}$ contains at most double the number of adversarial examples in $S_{g_{i}}^{(1)}$ , i.e. $|U_{i}|\\leqslant2V_{i}$ where $V_{i}$ denotes the number of adversarial examples in $S_{g_{i}}^{(1)}$ . gi ", "page_idx": 25}, {"type": "text", "text": "Now consider the case that $|S_{m_{i}}^{(2)}|>2|S_{m_{i}}^{(1)}|$ . By Theorem B.6 (iv), the number of samples from true clusters in $S_{m_{i}}^{(2)}$ is at most $|C_{i}^{*}|\\!+\\!w_{\\mathrm{low}}^{4}n\\leqslant1.02|S_{m_{i}}^{(1)}|$ , so the number $W_{i}$ of adversarial samples in $S_{m_{i}}^{(2)}$ is at least $W_{i}\\geqslant|S_{m_{i}}^{(2)}|-1.02|S_{m_{i}}^{(1)}|\\geqslant0.98|S_{m_{i}}^{(1)}|\\geqslant0.96|C_{i}^{*}|$ . Then, $\\begin{array}{r}{|U_{i}|=\\left|(C_{i}^{*}\\cap S_{g_{i}}^{(2)})\\setminus S_{g_{i}}^{(1)}\\right|\\leqslant}\\end{array}$ $|C_{i}^{*}|\\leqslant2W_{i}$ . ", "page_idx": 25}, {"type": "text", "text": "Finally note that by Theorem B.6 (v), the sets $S_{g_{i}}^{(2)}$ and $S_{m_{i}}^{(2)}$ are disjoint from any other sets $S_{g_{j}}^{(2)}$ and $S_{m_{j}}^{(2)}$ that correspond to another component $C_{j}^{*}$ . Therefore, the number of adversarial examples in the Smi in the second case and $S_{g_{i}}^{(2)}$ in the first case is smaller than the total number of adversarial examples, i.e. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{\\stackrel{i\\in G}{|S_{m_{i}}^{(2)}|\\leqslant2\\left|S_{m_{i}}^{(1)}\\right|}}V_{i}+\\sum_{\\stackrel{i\\in G}{|S_{m_{i}}^{(2)}|>2\\left|S_{m_{i}}^{(1)}\\right|}}W_{i}\\leqslant(\\varepsilon+w_{\\mathrm{low}}^{10})n.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Therefore, we directly obtain ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{U|\\leqslant\\sum_{i\\in G}\\left|\\big(C_{i}^{*}\\cap S_{g_{i}}^{(2)}\\big)\\setminus S_{g_{i}}^{(1)}\\right|=\\sum_{i\\in G}\\qquad\\left|\\big(C_{i}^{*}\\cap S_{g_{i}}^{(2)}\\big)\\setminus S_{g_{i}}^{(1)}\\right|+\\sum_{i\\in G}\\qquad\\left|\\big(C_{i}^{*}\\cap S_{g_{i}}^{(2)}\\big)\\setminus S_{g_{i}}^{(1)}\\right|}}\\\\ &{}&{\\big|S_{m_{i}}^{(2)}\\big|\\lesssim2\\big|S_{m_{i}}^{(1)}\\big|\\ \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left|S_{m_{i}}^{(2)}\\big|\\lesssim2\\big|S_{m_{i}}^{(1)}\\right|}\\\\ &{}&{\\leqslant\\sum_{i\\in G}2V_{i}+\\sum_{i\\in G}2W_{i}+O\\big(w_{\\mathrm{low}}^{2}\\big)n\\leqslant(2\\varepsilon+O\\big(w_{\\mathrm{low}}^{2}\\big)\\big)}\\\\ &{}&{\\big|S_{m_{i}}^{(2)}\\big|\\lesssim2\\big|S_{m_{i}}^{(1)}\\big|\\qquad\\quad\\big|S_{m_{i}}^{(2)}\\big|>2\\big|S_{m_{i}}^{(1)}\\big|\\qquad\\qquad\\qquad\\qquad\\qquad\\quad\\times(2\\varepsilon+O\\big(w_{\\mathrm{low}}^{2}\\big))\\big|}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof of Theorem B.7 (ii) Each iteration before $g_{i}$ was selected, removed at most $w_{\\mathrm{low}}^{4}|C_{i}^{*}|$ samples from $C_{i}^{*}$ , so all previous iterations removed at most $O(w_{\\mathrm{low}}^{3})|C_{i}^{*}|$ samples from $C_{i}^{*}$ . Then, by Lemma B.6 (iii), $S_{g_{i}}^{(2)}$ contains at least $(1-w_{\\mathrm{low}}^{2}/2-O(w_{\\mathrm{low}}^{3}))|C_{i}^{*}|$ samples from $C_{i}^{*}$ . The statement follows then since on the event $E^{\\prime}$ , we have $w^{\\ast}n-w_{\\mathrm{low}}^{10}n\\leqslant|C^{\\ast}|\\leqslant w^{\\ast}n+w_{\\mathrm{low}}^{10}n$ . ", "page_idx": 25}, {"type": "text", "text": "Proof of Theorem B.7 (iii) Here, either for all $i\\in[k]$ , $|S_{j}^{(1)}\\cap C_{i}^{*}|\\,<\\,w_{\\mathrm{low}}^{4}|C_{i}^{*}|$ or $i\\in G$ and the algorithm had already selected in a previous iteration $\\mu_{g_{i}}\\,\\in\\,M$ with $|S_{g_{i}}^{(1)}\\cap C_{i}^{*}|\\geqslant w_{\\mathrm{low}}^{4}|C_{i}^{*}|$ . Consider a first case, in which $|S_{j}^{(1)}\\cap C_{i}^{*}|\\,<\\,w_{\\mathrm{low}}^{4}|C_{i}^{*}|$ for all $i\\in[k]$ . Then the total number of samples from true clusters in ${S}_{j}^{(1)}$ is at most $w_{\\mathrm{low}}^{4}n$ . Using that $|S_{j}^{(1)}|>100w_{\\mathrm{low}}^{4}n$ , it follows that more than half of the samples in Sj(1)a re adversarial. ", "page_idx": 26}, {"type": "text", "text": "The second case is that $|S_{j}^{(1)}\\cap C_{i}^{*}|\\geqslant w_{\\mathrm{low}}^{4}|C_{i}^{*}|$ for some $i\\in G$ for which in a previous iteration $g_{i}$ we had that $|S_{g_{i}}^{(1)}\\cap C_{i}^{*}|\\geqslant w_{\\mathrm{low}}^{4}|C_{i}^{*}|$ . Note that at most $w_{\\mathrm{low}}^{2}|C_{i}^{*}|/2$ of the samples in $S\\cap C_{i}^{*}$ are not considered adversarial at this point (the ones that were outside $S_{g_{i}}^{(2)}$ ). Also, by Theorem B.6 (iv), ${S}_{j}^{(1)}$ contains at most $w_{\\mathrm{low}}^{4}n$ samples from other true clusters. Therefore either more than half of the samples in ${S}_{j}^{(1)}$ are considered adversarial or ", "page_idx": 26}, {"type": "equation", "text": "$$\n|S_{j}^{(1)}|\\leqslant w_{\\mathrm{low}}^{2}|C_{i}^{*}|+2w_{\\mathrm{low}}^{4}n\\leqslant O(w_{\\mathrm{low}}^{2})n.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof of Theorem B.7 (iv) Suppose that when the algorithm reaches the else statement we have for some $i\\in[k]$ that $i\\in R$ and $|S_{m_{i}}^{(1)}\\cap C_{i}^{*}|\\geqslant20w_{\\mathrm{low}}^{2}|C_{i}^{*}|$ . We have that $|S_{m_{i}}^{(2)}\\cap C_{i}^{*}|$ is at most $|S_{m_{i}}^{(1)}\\cap C_{i}^{*}|+w_{\\mathrm{low}}^{2}|C_{i}^{*}|/2$ , where we use that by Theorem B.6 (ii), at most $w_{\\mathrm{low}}^{2}|C_{i}^{*}|/2$ samples can fail to be captured by $S_{m_{i}}^{(1)}$ . By Theorem B.6 (iv), furthermore, the number of samples from other true clusters in $S_{m_{i}}^{(2)}$ is at most $w_{\\mathrm{low}}^{4}n$ . Therefore, using that $|S_{m_{i}}^{(2)}|>2|S_{m_{i}}^{(1)}|$ , the number of adversarial samples in $S_{m_{i}}^{(2)}$ is at least ", "page_idx": 26}, {"type": "equation", "text": "$$\n|S_{m_{i}}^{(2)}|-|S_{m_{i}}^{(1)}\\cap C_{i}^{*}|-w_{\\mathrm{low}}^{2}|C_{i}^{*}|/2-w_{\\mathrm{low}}^{4}n\\geqslant0.45|S_{m_{i}}^{(2)}|-w_{\\mathrm{low}}^{4}n\\geqslant0.44|S_{m_{i}}^{(2)}|\\,,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where in the last inequality we used that $|S_{m_{i}}^{(2)}|>100w_{\\mathrm{low}}^{4}n$ . Let $V$ be the union, over all $i\\in[k]$ , of etasr $S_{m_{i}}^{(2)}$ jsoiuncth.  tThhate $i\\in R$ aatn lde $|S_{m_{i}}^{(1)}\\cap C_{i}^{*}|\\geqslant20w_{\\mathrm{low}}^{2}|C_{i}^{*}|$ . mTphleeos rien B.a6r e( va)d vgievrseas rtihala.t all such sets $S_{m_{i}}^{(2)}$ $V$ ", "page_idx": 26}, {"type": "text", "text": "Consider now for some $i\\in[k]$ how many samples from $S\\cap C_{i}^{*}$ can be outside $V$ when the algorithm reaches the else statement. By Theorem B.6 (ii), $S_{m_{i}}^{(1)}$ can fail to capture at most $w_{\\mathrm{low}}^{2}|C_{i}^{*}|/2$ samples from $C_{i}^{*}$ , and we have no guarantee that these samples are in $V$ . Consider now the samples in ${\\cal S}_{m_{i}}^{(1)}\\cap{\\cal C}_{i}^{*}$ . If $i\\in R$ , we may miss up to $20w_{\\mathrm{low}}^{2}|C_{i}^{*}|$ of these samples if $|S_{m_{i}}^{(1)}\\cap C_{i}^{*}|<20w_{\\mathrm{low}}^{2}|C_{i}^{*}|$ because in this case we do not include $S_{m_{i}}^{(2)}$ in $V$ . On the other hand, if $i\\not\\in R$ , there are at most $100w_{\\mathrm{low}}^{4}n$ samples in $S_{m_{i}}^{(1)}\\cap C_{i}^{*}$ . Then the total number of samples from $S\\cap C_{i}^{*}$ outside $V$ is at most $w_{\\mathrm{low}}^{2}|C_{i}^{*}|/2+20w_{\\mathrm{low}}^{2}|C_{i}^{*}|+100w_{\\mathrm{low}}^{4}n$ . Summed across all $i\\in[k]$ , this makes up at most $21w_{\\mathrm{low}}^{2}n$ samples. ", "page_idx": 26}, {"type": "text", "text": "Overall, the number of adversarial samples in $S$ when the algorithm reaches the else statement is at least ", "page_idx": 26}, {"type": "text", "text": "$0.44|V|+(|S|-|V|-21w_{\\mathrm{low}}^{2}n)=|S|-0.56|V|-21w_{\\mathrm{low}}^{2}n\\geqslant0.44|S|-21w_{\\mathrm{low}}^{2}n\\geqslant0.4|S|$ where in the last inequality we also used that $|S|\\geqslant0.1w_{\\mathrm{low}}n$ . ", "page_idx": 26}, {"type": "text", "text": "F Proof of Proposition 3.5 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We now prove lower bounds for the case of Gaussian distributions and distributions with $t$ -th subGaussian moments. ", "page_idx": 26}, {"type": "text", "text": "F.1 Case b): For the Gaussian inliers ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We first focus on the case when $D_{i}(\\mu_{i})=\\mathbb{N}(\\mu_{i},I)$ . The proof goes through an efficient reduction from the problem considered by Proposition F.1 to the problem solved by algorithm $\\mathcal{A}$ . ", "page_idx": 26}, {"type": "text", "text": "Proposition F.1 ([5], Proposition 5.11). Let $\\mathcal{D}$ be the class of identity covariance Gaussians on $\\mathbb{R}^{d}$ and let $0<\\alpha\\leqslant1/2$ . Then any list-decoding algorithm that learns the mean of an element of $\\mathcal{D}$ with failure probability at most $1/2$ , given access to $(1-\\alpha)$ -additively corrupted samples, must either have error bound $\\beta=\\Omega(\\sqrt{\\log{1/\\alpha}})$ or return $\\operatorname*{min}(2^{\\Omega(d)},(1/\\alpha)^{w(1)})$ many hypotheses. ", "page_idx": 26}, {"type": "text", "text": "First, we describe the means of the components in the input distribution to algorithm $\\mathcal{A}$ . Let $\\bar{\\mu}_{1},\\ldots,\\bar{\\mu}_{k-1}\\in\\mathbb{R}^{d}$ be any set of $k-1$ points with pairwise separation larger than $\\bar{2}C\\sqrt{\\log{1/w_{\\mathrm{low}}}}$ . Then let $\\mu_{k}=(\\bar{\\mu},0)\\in\\mathbb{R}^{d+1}$ and $\\mu_{i}=(\\bar{\\mu}_{i},2C\\sqrt{\\log{1/w_{\\mathrm{low}}}}+1)\\in\\mathbb{R}^{d+1}$ for all $i\\in[k-1]$ . Then $\\mu_{1},\\dots,\\mu_{k}$ also have pairwise separation larger than $2C\\sqrt{\\log{1/w_{\\mathrm{low}}}}$ . ", "page_idx": 27}, {"type": "text", "text": "Then, given $n$ points $y_{1},\\ldots,y_{n}\\in\\mathbb{R}^{d}$ as in the input to the problem in Proposition F.1 (i.e. $(1-\\alpha)$ - additively corrupted samples), we generate $n$ points that we give as input to algorithm $\\mathcal{A}$ as follows: let $S=\\{1,\\ldots,n\\}$ , and then for each of the $n$ points, draw $i\\sim\\operatorname{Unif}\\{1,\\ldots,k\\}$ and generate the point as follows: ", "page_idx": 27}, {"type": "text", "text": "2. if $i=k$ , sample $j\\sim S$ uniformly at random, remove $j$ from $S$ , sample $g\\sim N(0,1)$ , and let the point be $(y_{j},g)\\in\\mathbb{R}^{d+1}$ . ", "page_idx": 27}, {"type": "text", "text": "We note that this construction simulates an input sampled i.i.d. according to the mixture $\\begin{array}{r}{\\frac{1}{k}N(\\mu_{1},I_{d+1})+...+\\frac{1}{k}N(\\mu_{k-1},I_{d+1})+\\frac{\\alpha}{k}N(\\mu_{k}^{\\quad\\cdot}I_{d+1})+\\frac{\\`}{k}\\frac{1-\\alpha}{k}Q^{\\prime}}\\end{array}$ for some $Q^{\\prime}$ . Then with success probability at least $1/2$ running $\\mathcal{A}$ on this input with $\\begin{array}{r}{w_{\\mathrm{low}}=\\frac{\\alpha}{k}}\\end{array}$ returns a list $L$ such that there exists $\\hat{\\mu}\\,\\in\\,L$ with $\\|{\\hat{\\mu}}-\\mu_{k}\\|\\leqslant\\beta_{k}$ . Note that this implies that $\\|(\\stackrel{\\prime\\prime}{\\hat{\\mu}})_{1:d}\\,-\\,\\bar{\\mu}\\|\\,\\leqslant\\,\\beta_{k}$ . Finally, we create a pruned list $L^{\\prime}$ as follows: initialize $L^{\\prime}=L$ and then for each $i\\in[k-1]$ remove all $\\hat{\\mu}\\,\\in\\,L^{\\prime}$ such that $\\|\\hat{\\mu}-\\mu_{i}\\|\\leqslant C\\sqrt{\\log{1/w_{\\mathrm{low}}}}$ . Then we return $L^{\\prime}$ as the output for the original problem in Proposition F.1. ", "page_idx": 27}, {"type": "text", "text": "Let us analyze now this output. The separation between the means ensures that any hypothesis $\\hat{\\mu}\\,\\in\\,L$ that is $C\\sqrt{\\log{1/w_{\\mathrm{low}}}}$ -close to $\\mu_{k}$ is not removed in the pruning. Therefore $L^{\\prime}$ continues to contain a hypothesis $\\hat{\\mu}$ such that $\\|(\\hat{\\mu})_{1:d}\\,-\\,\\bar{\\mu}\\|\\;\\leqslant\\;\\beta_{k}$ . Then, if $\\beta_{k}\\,\\ne\\,\\Omega(\\sqrt{\\log{1/\\alpha}})$ and $\\vert L^{\\prime}\\vert<\\operatorname*{min}\\{2^{\\Omega(d)},((w_{k}+\\varepsilon)/w_{k})^{\\omega(1)}\\}$ , this reduction violates the lower bound of Proposition F.1. Therefore we must have either $\\beta_{k}=\\Omega(\\sqrt{\\log{1/\\alpha}})$ or $|L^{\\prime}|\\geqslant\\operatorname*{min}\\{2^{\\Omega(d)},\\left(1/\\tilde{w}_{k}\\right)^{\\omega(1)}\\}$ . ", "page_idx": 27}, {"type": "text", "text": "Finally, we show that these lower bounds on $\\beta_{k}$ and $|L^{\\prime}|$ imply the desired lower bound for $\\mathcal{A}$ . Consider first the case: $\\beta_{k}=\\Omega(\\sqrt{\\log{1/\\alpha}})$ . Note that in the input to algorithm $\\mathcal{A}$ we have $\\tilde{w}_{k}=\\alpha$ Therefore $\\beta_{k}\\,=\\,\\Omega(\\sqrt{\\log{1/\\alpha}})$ corresponds to the desired lower bound in the lemma statement. Consider second the case: $|L^{\\prime}|\\geqslant\\operatorname*{min}\\{2^{\\Omega(d)},\\left(1/\\tilde{w}_{k}\\right)^{\\omega(1)}\\}$ . We note that, for each $i\\in[k-1]$ , the original list $L$ must contain some $\\hat{\\mu}\\in L$ such that $\\|\\hat{\\mu}-\\mu_{i}\\|\\leqslant C\\sqrt{\\log{1/w_{\\mathrm{low}}}}.$ . Furthermore, because the means $\\mu_{i}$ have pairwise separation larger than $2C\\sqrt{\\log{1/w_{\\mathrm{low}}}}$ , the original list $L$ must contain at least $k-1$ means of this kind. However, all of these means are removed in the pruning procedure, so $|L|\\geqslant k-1+|L^{\\prime}|$ , so $|L|\\geqslant k-1+\\operatorname*{min}\\{2^{\\Omega(d)},\\big(1/\\tilde{w}_{k}\\big)^{\\omega(1)}\\}$ . This matches the desired lower bound in the lemma statement. (The choice to make the hidden mean the $k$ -th mean was without loss of generality, as the distribution is invariant to permutations of the components.) ", "page_idx": 27}, {"type": "text", "text": "F.2 Case a): For distributions with $t$ -th sub-Gaussian moments ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "The proof for the case when $D_{i}(\\mu_{i})$ has sub-Gaussian $t$ -th central moments employs the same reduction scheme, but reduces from Proposition F.2. ", "page_idx": 27}, {"type": "text", "text": "Proposition F.2 ([5], Proposition 5.12). Let $\\mathcal{D}$ be the class of distributions on $\\mathbb{R}^{d}$ with bounded t-th central moments for some positive even integer $t_{\\perp}$ , and let $0^{\\stackrel{\\cdot}{<}}\\alpha<2^{-t-1}$ . Then any list-decoding algorithm that learns the mean of an element of $\\mathcal{D}$ with failure probability at most $1/2$ , given access to $(1-\\alpha)$ -additively corrupted samples, must either have error bound $\\beta=\\Omega(\\alpha^{-1/t})$ or return a list of at least d hypotheses. ", "page_idx": 27}, {"type": "text", "text": "Furthermore, in [3], formal evidence of computational hardness was obtained (see their Theorem 5.7, which gives a lower bound in the statistical query model introduced by [14]) that suggests obtaining error $\\Omega_{t}((1/\\tilde{w}_{s})^{1/t})$ requires running time at least $d^{\\Omega(t)}$ . This was proved for Gaussian inliers and the running time matches ours up to a constant in the exponent. ", "page_idx": 27}, {"type": "text", "text": "G Stability of list-decoding algorithms ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this section we discuss two of the existing list-decodable mean estimation algorithms for identitycovariance Gaussian distributions and show that they also work when a wl2ow- fraction of the inliers is adversarially removed. ", "page_idx": 28}, {"type": "text", "text": "First, we consider the algorithm in Theorem 3.1 in [3]. A central object in their an\u221aalysis is an \u201c $\\stackrel{!}{\\alpha}$ -good multiset\", which is a multiset of samples such that all are within distance $O({\\sqrt{d}})$ of each other and at least an $\\alpha$ -fraction of them come from a $(1-\\Omega(\\alpha))$ -fraction of an i.i.d. set of samples from a Gaussian distribution $N(\\mu,I_{d})$ . Then their algorithm essentially works as long as the input contains an $\\alpha$ -good multiset. For our case, after the removal of a $w_{\\mathrm{low}}^{2}$ -fraction of inliers, the input essentially continues to contain a $(1-w_{\\mathrm{low}}^{2})\\alpha$ -good multiset, so the algorithm continues to work in our corruption model. ", "page_idx": 28}, {"type": "text", "text": "Second, we consider the algorithm in Theorem 6.12 in [5]. The main distributional requirement of their algorithm is that $\\check{\\mathbb{E}}_{x,y\\sim S^{*}}[p^{2}(x\\,-\\,y)]\\;\\leqslant\\;2\\mathbb{E}_{g,h\\sim N(0,I_{d})}[p^{2}(g\\,-\\,h)]$ for all degree- $(t/2)$ polynomials $p$ , where $S^{*}$ is the set of inliers. Concentration arguments give with high probability that $\\bar{\\mathbb{E}}_{x,y\\sim C^{*}}[p^{2}(x-y)]\\leqslant1.5\\mathbb{E}_{g,h\\sim N(0,I_{d})}[p^{2}(g-h)]$ . Furthermore, the distribution over $x,y\\sim S^{*}$ can be seen as a $(1-w_{\\mathrm{low}}^{2})^{2}$ -fraction of the distribution over $x,y\\sim C^{*}$ . Then Fact G.1, which follows by standard probability calculations, also gives that any event under the former distribution can be bounded in terms of the second distribution: ", "page_idx": 28}, {"type": "text", "text": "Fact G.1. For any event $A$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{P}_{x,y\\sim S^{*}}(A)\\leqslant\\mathbb{P}_{x,y\\sim C^{*}}(A)/(1-w_{\\mathrm{low}}^{2})^{2},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where probabilities are taken over a uniform sample from $S^{*}$ and $C^{*}$ respectively. ", "page_idx": 28}, {"type": "text", "text": "Overall we obtain ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x,y\\sim S^{*}}[p^{2}(x-y)]\\leqslant1.5/(1-w_{\\mathrm{low}}^{2})^{2}\\mathbb{E}_{g,h\\sim N(0,I_{d})}[p^{2}(g-h)],\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "so for $w_{\\mathrm{low}}$ small enough we have $\\mathbb{E}_{x,y\\sim S^{*}}[p^{2}(x{-}y)]\\leqslant2\\mathbb{E}_{g,h\\sim N(0,I_{d})}[p^{2}(g{-}h)]$ and their algorithm continues to work in our corruption model. ", "page_idx": 28}, {"type": "text", "text": "H Concentration bounds ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this section we prove some concentration bounds essential to our analysis. ", "page_idx": 28}, {"type": "text", "text": "Lemma H.1. Let $D$ be a $d$ -dimensional distribution with mean $\\mu^{*}\\,\\in\\,\\mathbb{R}^{d}$ and sub-Gaussian $t$ -th central moments with parameter 1. Fix a unit vector $\\boldsymbol{v}\\in\\mathbb{R}^{d}$ . Then ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\underset{x\\sim D}{\\mathbb{P}}[|\\langle x-\\mu^{*},v\\rangle|\\leqslant R]\\geqslant1-\\left(\\frac{\\sqrt{t}}{R}\\right)^{t}\\,.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. We have that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname{\\mathbb{P}}_{x\\sim D}[|\\langle x-\\mu^{*},v\\rangle|>R]\\leqslant\\frac{\\operatorname{\\mathbb{E}}_{x\\sim D}\\langle x-\\mu^{*},v\\rangle^{t}}{R^{t}}\\leqslant\\frac{(t-1)!!}{R^{t}}\\leqslant\\left(\\frac{\\sqrt{t}}{R}\\right)^{t}\\,,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where we used that $(t-1)!!\\leqslant t^{t/2}=\\sqrt{t}^{t}$ . ", "page_idx": 28}, {"type": "text", "text": "Lemma H.2. Let $D$ be a $d$ -dimensional distribution with mean $\\mu^{*}\\,\\in\\,\\mathbb{R}^{d}$ and sub-Gaussian $t$ -th central moments with parameter 1. Let $C^{*}$ be a set of i.i.d. samples drawn from $D$ . Fix a unit vector $v\\in\\mathbb{R}^{d}$ . Then with probability at least $\\begin{array}{r}{1-\\exp{\\left(-2|C^{*}|\\left(\\frac{\\sqrt{t}}{R}\\right)^{2}\\right)},}\\end{array}$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left|\\{x\\in C^{*},\\,s.t.\\;|\\langle x-\\mu^{*},v\\rangle|\\leqslant R\\}\\right|\\geqslant\\left(1-2\\left(\\frac{\\sqrt{t}}{R}\\right)^{t}\\right)|C^{*}|\\,.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. The result follows by Lemma H.1 and a Binomial tail bound. ", "page_idx": 28}, {"type": "text", "text": "Lemma H.3. Let $D$ be a $d$ -dimensional distribution with mean $\\mu^{*}\\,\\in\\,\\mathbb{R}^{d}$ and sub-Gaussian $t$ -th central moments with parameter 1. Let $C^{*}$ be a set of i.i.d. samples drawn from $D$ . Fix m unit vectors $v_{1},\\ldots,v_{m}\\in\\mathbb{R}^{d}$ . Then with probability at least $\\begin{array}{r}{1-\\exp{\\left(-2|S^{*}|m^{2}\\left(\\frac{\\sqrt{t}}{R}\\right)^{2t}\\right)},}\\end{array}$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\Big|\\,\\bigcap_{i\\in[m]}\\left\\{x\\in S^{*},\\,s.t.\\;\\big|\\langle x-\\mu^{*},v_{i}\\rangle\\big|\\leqslant R\\right\\}\\Big|\\geqslant\\left(1-2m\\left(\\frac{\\sqrt{t}}{R}\\right)^{t}\\right)\\left|S^{*}\\right|.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. By Lemma H.1 and a union bound over the $m$ directions, we get ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname{\\mathbb{P}}_{x\\sim D}[|\\langle x-\\mu^{*},v_{i}\\rangle|\\leqslant R,\\forall i\\in[m]]\\geqslant1-m\\left(\\frac{\\sqrt{t}}{R}\\right)^{t}\\,.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then the result follows by a Binomial tail bound. ", "page_idx": 29}, {"type": "text", "text": "I Experimental details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Adversarial line and adversarial clusters The following figure illustrates the adversarial distributions used in Figure 2 and further in this section. ", "page_idx": 29}, {"type": "image", "img_path": "TrXV4dMDcG/tmp/43d3f24bd0a3b33f29d6ab0cf67fd0b66cd5d525977cb798d0b9d20cf5af20a4.jpg", "img_caption": ["Figure 4: Two variants of adversarial distribution: adversarial line (left) and adversarial clusters (right). "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Data Distribution We consider a mixture of $k=7$ well-separated $(\\|\\mu_{i}-\\mu_{j}\\|\\geqslant40)$ $d=100$ dimensional inlier clusters whose subgroup sizes range from 0.3 to 0.02. The experiments are conducted once using a Gaussian distribution and once using a heavy-tailed t-distribution with five degrees of freedom for both inlier and adversarial clusters. In Figure 6 the latter suggests that our algorithm works comparatively well even for mixture distributions which do not fulfill our assumptions. We set $w_{\\mathrm{low}}\\,=\\,0.02$ and $\\varepsilon=0.12$ so that it is larger than the smallest clusters but smaller than the largest ones and set the total number of data points to 10000. The Gaussian noise model simply computes the empirical mean and covariance matrix of the clean data and samples 1200 noisy samples from a Gaussian distribution with this mean and covariance. The adversarial cluster model and the adversarial model are as depicted in Figure 4. ", "page_idx": 29}, {"type": "text", "text": "Attack distributions We consider three distinct adversarial models (see Figure 4 for reference). ", "page_idx": 29}, {"type": "text", "text": "1. Adversarial clusters: After sampling the inlier cluster means, we choose the cluster with the smallest weight. Let $\\mu_{s}$ denote its mean. Then, we sample a random direction $v_{c}$ with $\\left\\|v_{c}\\right\\|=10$ . After that, we sample three directions $v_{1},v_{2}$ and $v_{3}$ with $\\left\\|v_{i}\\right\\|=10$ . Then we put three additional (outlier) clusters with means at $\\mu_{s}+v_{c}+v_{i}$ . This roughly corresponds to the right picture in Figure 4. The samples for each adversarial cluster are drawn from a distribution that matches the covariance of the inlier clusters, with the sample size being twice as large as of the affected inlier cluster.   \n2. Adversarial line: After sampling the inlier cluster means, we again choose the cluster with the smallest weight. Let $\\mu_{s}$ denote its mean. Then, we sample a random direction $v_{c}$ with $\\left\\|v_{c}\\right\\|=10$ . We put three additional (outlier) clusters with means at $\\mu_{s}+v_{c},\\mu_{s}+2v_{c}$ and $\\mu_{s}+3v_{c}$ , which form a line as shown in Figure 4. The samples are drawn similarly to the adversarial clusters, with the difference that the covariance is scaled by a factor of 5 in the direction of the line. ", "page_idx": 29}, {"type": "image", "img_path": "TrXV4dMDcG/tmp/b02d7b52c055b06be43875595da43de8b386ec22efcd487b973230641453adea.jpg", "img_caption": ["Figure 5: Comparison of five algorithms with three adversarial noise models. On the left we show worst estimation error of algorithms with constrained list size and on the right the smallest list size with constrained error guarantee. We plot the median of the metrics with the error bars showing 25th and 75th percentile. We observe that our method consistently outperforms prior works in terms of list size and worst estimation error, with the exception of DBSCAN, which performs at a similiar level. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "TrXV4dMDcG/tmp/2daa2204e02df501f3252a2e110dfe3065f9cc016224d9a51e0afbef53dcfc6f.jpg", "img_caption": ["Figure 6: Worst estimation error and list size comparison for the case where inlier distributions are heavy-tailed. We can observe numerical stability of our approach. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "3. Gaussian adversary: Here we simply introduce noise matching the empirical mean and covariance of all inlier data (i.e., as if all inlier clusters are generated from the same Gaussian distribution). ", "page_idx": 30}, {"type": "text", "text": "Note that in the first and second attack, the adversary creates clusters that do not respect the separation assumption of the true inlier clusters: either adversarial clusters are placed around the smallest inlier cluster (Adversarial Cluster), or the adversarial clusters form a line, pointing out in some fixed direction (Adversarial Line). ", "page_idx": 30}, {"type": "text", "text": "Implementation details We implement the list-decodable mean estimation base learner in our InnerStage algorithm (Algorithm 3) based on [8]. It leverages an iterative multi-flitering strategy and one-dimensional projections. In particular, we use the simplified gaussian version of the algorithm. It is designed for distributions sampled from a Gaussian but also shows promising results for the experiments involving a heavy-tailed t-distribution as depicted in Figure 6. The robust mean estimator used to improve the mean hypotheses for large clusters is omitted in our implementation. ", "page_idx": 30}, {"type": "text", "text": "Hyper-parameter search and experiment execution The hyper-parameters of our algorithm are tuned beforehand based on the experimental setup. For the comparative algorithms, hyper-parameter searches are conducted within each experiment after initial tuning. For our algorithm, key parameters include the pruning radius $\\gamma$ used in the OuterStage routine (Algorithm 6) and $\\beta$ used in the InnerStage (Algorithm 4). In addition, parameters for the LD-ME base learner, such as the cluster concentration threshold, also require careful selection, resulting in a total of 7 parameters. The tuning for these was performed using a grid search comparing about 250 different configurations. Similarly, we independently tune the vanilla LD-ME algorithm, which we run with $w_{\\mathrm{low}}$ as weight parameter. For ", "page_idx": 30}, {"type": "image", "img_path": "TrXV4dMDcG/tmp/f2d544946f01e68e871a23fbb872260c7f851ab720ec6f42994380bbdee2b9ff.jpg", "img_caption": ["Figure 7: Scatter plot of all results for one iteration of the experiment using three adversarial noise models. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "DBSCAN, we optimize the list size and error metrics by searching over a range of 100 values for $\\varepsilon$ , which controls the maximum distance between samples considered in the same neighbourhood. The minimum samples threshold, which validates the density based clusters, is pretuned beforehand and adjusted based on $w_{\\mathrm{low}}$ . For $k$ -means and its robust version, utilizing a median-of-means weighting scheme, we explore 21 values for $k$ , including the true number of clusters. Each parameter setting is executed 100 times to account for stochastic variations in the algorithmic procedures, such as $k$ -means initialization. The list size and worst estimation error for each list of clusters obtained is visualized exemplarily for one iteration of the experiment in Figure 7. The plot provides insight into how the different algorithms perform and vary with different list sizes. ", "page_idx": 31}, {"type": "text", "text": "Evaluation details Note that we have two sources of randomness: the data is random and also the algorithms themselves are random (except DBSCAN). For a clear comparison, we sample and fix one dataset for each attack model. we plot the performance of 100 runs of each algorithm for each parameter setting, each time recording the returned list size together with the worst estimation error $\\mathrm{max}_{i\\in[k]}\\,\\mathrm{min}_{\\hat{\\mu}\\in L}\\|\\mu_{i}-\\hat{\\mu}\\|$ . Then we either (i) report the worst estimation error for all runs with constrained list size (we pick the list size most frequently returned by our algorithm, specifically 7 or 10 in our experiments) (see Figure 5, left), or (ii) report the smallest list size required to achieve the same or smaller worst estimation error (we pick the 75th quantile of errors of our algorithm for a threshold) (see Figure 5, right). Under size constraint (i), the bar plots correspond to the median over the runs, with error bars indicating the 25th and 75th quantiles. Under error constraint (ii), the bar plots represent the minimum list size for which the median over the runs falls below the threshold, while the error bars show the minimum list size for which the 25th and 75th quantiles meet the constraint. Note that $\\cdot\\,\\mathsf{n}/\\mathsf{a}^{\\,\\prime}$ indicates that, within the scope of our parameter search, no list size achieves an error below the specified constraint. ", "page_idx": 31}, {"type": "text", "text": "In Figure 6 we study the numerical stability of our approach. In particular, whether the performance degrades when inlier distribution does not satisfy required assumptions. We observe that if one uses our meta-algorithm with base learner designed for Gaussian inliers, we still obtain stable results even in the case of heavy-tailed inlier distribution. ", "page_idx": 31}, {"type": "text", "text": "I.1 Variation of $w_{\\mathrm{low}}$ ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "To study the effect of varying $w_{\\mathrm{low}}$ input on the performance of our approach and LD-ME, we introduce a new noise model. As illustrated in Figure 8, we consider a mixture of $k\\,=\\,3$ wellseparated clusters: one small cluster with a weight of 0.045 and two large clusters, each with a weight of 0.2. We place two adversarial clusters (see paragraph on attack distributions for details): one near the small cluster and another near one of the large clusters. Furthermore, uniform noise is introduced, spanning the range of the data generated by the inlier and its nearby outlier cluster and accounting for $10\\%$ of the data in this region. Overall, $\\varepsilon=0.56$ and we draw 22650 samples from this mixture distribution. ", "page_idx": 31}, {"type": "image", "img_path": "TrXV4dMDcG/tmp/a12a26ff6846a2f2b974c8a648283588e3bb3793703d22499d0cfd106cc875c1.jpg", "img_caption": ["Figure 8: Setup for $w_{\\mathrm{low}}$ variation experiment with clusters contaminated by an adversarial cluster and uniform noise. Lower color intensities indicate smaller cluster weights. "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "TrXV4dMDcG/tmp/71f16eb92811846af578ce87ffffc950af991403177429fcabf5dcead1eb4b99.jpg", "img_caption": ["Figure 9: Comparison of list size and estimation error for small and large inlier clusters for varying $w_{\\mathrm{low}}$ inputs. The experimental setup is illustrated in Figure 8. The plot on the top left shows the estimation error for the small cluster and the plot on the top right shows the error for the large cluster. We plot the median values with error bars indicating $25\\mathrm{th}$ and $75\\mathrm{th}$ quantiles. As $w_{\\mathrm{low}}$ decreases, our algorithm maintains a roughly constant estimation error for the large cluster, while the error for LD-ME increases. "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "For both algorithms we run 100 seeds for each $w_{\\mathrm{low}}$ ranging from 0.02 to 0.2, which corresponds to the weight of the largest inlier cluster. In Figure 9, we plot the median estimation error with error bars showing the 25th and 75th quantiles for the small cluster (top left) and the large cluster near the outlier cluster (top right). As expected from our theoretical results, we observe that our algorithm performs roughly constant in estimating the mean of the large cluster, regardless of the initial $w_{\\mathrm{low}}$ . Meanwhile, the estimation error of LD-ME increases as $w_{\\mathrm{low}}$ decreases further below the true cluster weight. Furthermore, the plots show that our approach does consistently outperform LD-ME in terms of both worst estimation error and list size. Figure 10 also compares the performance of the clustering algorithms in this experimental setup with results similar to the ones obtained in the previous experimental settings. ", "page_idx": 32}, {"type": "text", "text": "I.2 Computational resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Our implementation of the algorithm and experiments leverages multi-threading. It utilizes CPU resources of an internal cluster with 128 cores, which results in a execution time of about 5 minutes for a single run of the experiment for one noise model with 10000 samples. We remark that classic ", "page_idx": 32}, {"type": "image", "img_path": "TrXV4dMDcG/tmp/1f1a1b5452b5ac4693b3f2b286ee30a466d4ed2b734378d85f65387372e8c1b3.jpg", "img_caption": ["Figure 10: Worst estimation error and list size comparison for the setup used in the $w_{\\mathrm{low}}$ variation experiment. "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "approaches like $k$ -means and DBSCAN perform fast and the most time-consuming part is the execution of the LD-ME base learner. Given our experimental setup with three noise models, it takes about 15 minutes to reproduce all our results for one data distribution. ", "page_idx": 33}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We summarized our contributions in Table 1 in the introduction, and all the claims made there (including more general results) appear in Section 3 and Appendix C. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 34}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: We discuss the limitations of our theoretical results in Section 6. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 34}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Our assumptions are stated in Sections 2 and 3, in particular Assumption 3.1.   \nThe complete proof is given in Appendix C. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 35}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We provide experimental details, including explanation of the adversarial noise distribution in Appendix I. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 35}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We uploaded zip archive containing the code together with instructions on how to reproduce the experiments. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 36}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We provide short version of experimental details in Section 6 and more detailed one in Appendix I. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 36}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: As described in Appendix I, we plot the median of the metric (error or list size) accompanied by the $25\\%$ and $75\\%$ percentiles, acting as error bars. The statistics was collected over 100 reruns of all algorithms under comparison. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 36}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 37}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We provide experimental details, including computational resources, in Appendix I. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 37}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We reviewed the NeurIPS Code of Ethics and confirm that our paper respects it. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 37}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: Main focus of our result is to deepen our theoretical understanding of fundamental problems in statistics, such as mixture learning. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 38}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: To the best of our judgement, our paper does not pose misuse risks, as it has theoretical focus. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 38}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: Our work does not use existing assets. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: Our work does not use release new assets. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 39}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: Our work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 39}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: Our work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 39}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 40}]