[{"figure_path": "xSziO6gQgG/figures/figures_8_1.jpg", "caption": "Figure 1: Vis. of NTP implicit optimization bias in a setting with m = 3 distinct contexts, embedding dimension d = 2, vocabulary of |V| = 5 words and support sets of length |Sj| = 3, j \u2208 [3]. Left: Vis. of context embeddings \u0127j in circle black markers (marked as A,B,C) and of their associated support sets Sj (colored text below each marker). Colored vectors (star markers) represent max-NTP-margin vectors w := eWmmT ev, v \u2208 [5] found by GD. Interpreting decoder vectors as word embeddings leads to intuitive findings on their geometry learned by NTP training. E.g., word embedding w3 (almost) aligns with context-embedding A and the normal hyperplane it defines separates A from B and C, since word 3 only appears after context A. The rest of the words follow two contexts each and their word-representation naturally belongs to the cone defined by the embeddings of those respective contexts. The wider the cone, the larger the magnitude of the word embedding to compensate for the large angle between context-representations that share the same next-word. Note that geometry of depicted word embeddings only depends on support sets, but the conditional probabilities define another set of word representations on an orthogonal (matrix) subspace; see text for details and vis. Right: Upper/lower graphs confirm the predictions of Lemma 2 and of Theorem 2, respectively.", "description": "This figure visualizes the implicit optimization bias of next-token prediction (NTP) in a simplified setting. The left panel shows the geometry of context embeddings and word embeddings learned by gradient descent (GD).  The word embeddings are visualized as vectors, with colors indicating which contexts they are most associated with. The right panel shows the convergence behavior of GD, confirming theoretical predictions about norm growth and directional alignment with the maximum-margin solution.", "section": "Gradient Descent"}, {"figure_path": "xSziO6gQgG/figures/figures_16_1.jpg", "caption": "Figure 1: Vis. of NTP implicit optimization bias in a setting with m = 3 distinct contexts, embedding dimension d = 2, vocabulary of |V| = 5 words and support sets of length |Sj| = 3, j \u2208 [3]. Left: Vis. of context embeddings \u0127; in circle black markers (marked as A,B,C) and of their associated support sets S; (colored text below each marker). Colored vectors (star markers) represent max-NTP-margin vectors w := eWmm , \u03c5\u03b5 [5] found by GD. Interpreting decoder vectors as word embeddings leads to intuitive findings on their geometry learned by NTP training. E.g., word embedding w3 (almost) aligns with context-embedding A and the normal hyperplane it defines separates A from B and C, since word 3 only appears after context A. The rest of the words follow two contexts each and their word-representation naturally belongs to the cone defined by the embeddings of those respective contexts. The wider the cone, the larger the magnitude of the word embedding to compensate for the large angle between context-representations that share the same next-word. Note that geometry of depicted word embeddings only depends on support sets, but the conditional probabilities define another set of word representations on an orthogonal (matrix) subspace; see text for details and vis. Right: Upper/lower graphs confirm the predictions of Lemma 2 and of Theorem 2, respectively.", "description": "This figure visualizes the implicit optimization bias of next-token prediction (NTP) in a simple setting. The left panel shows the context embeddings and word embeddings learned by gradient descent (GD), illustrating how the geometry reflects the support sets and conditional probabilities of the next tokens.  The right panel shows that gradient descent (GD) aligns with the maximal margin direction, and the finite component converges to a solution that equates logits differences of in-support tokens to their log-odds, demonstrating the implicit bias of GD in NTP.", "section": "Gradient Descent"}, {"figure_path": "xSziO6gQgG/figures/figures_17_1.jpg", "caption": "Figure 1: Vis. of NTP implicit optimization bias in a setting with m = 3 distinct contexts, embedding dimension d = 2, vocabulary of |V| = 5 words and support sets of length |Sj| = 3, j \u2208 [3]. Left: Vis. of context embeddings \u0127; in circle black markers (marked as A,B,C) and of their associated support sets S; (colored text below each marker). Colored vectors (star markers) represent max-NTP-margin vectors w := eWmm , \u03c5\u03b5 [5] found by GD. Interpreting decoder vectors as word embeddings leads to intuitive findings on their geometry learned by NTP training. E.g., word embedding w3 (almost) aligns with context-embedding A and the normal hyperplane it defines separates A from B and C, since word 3 only appears after context A. The rest of the words follow two contexts each and their word-representation naturally belongs to the cone defined by the embeddings of those respective contexts. The wider the cone, the larger the magnitude of the word embedding to compensate for the large angle between context-representations that share the same next-word. Note that geometry of depicted word embeddings only depends on support sets, but the conditional probabilities define another set of word representations on an orthogonal (matrix) subspace; see text for details and vis. Right: Upper/lower graphs confirm the predictions of Lemma 2 and of Theorem 2, respectively.", "description": "This figure illustrates the implicit optimization bias of next-token prediction (NTP) in a simple setting. The left panel shows the context embeddings and the corresponding word embeddings learned by gradient descent.  The right panel shows the convergence behavior of the gradient descent algorithm, demonstrating the alignment with the max-margin direction and convergence to a specific solution in a data subspace.", "section": "Gradient Descent"}, {"figure_path": "xSziO6gQgG/figures/figures_17_2.jpg", "caption": "Figure 1: Vis. of NTP implicit optimization bias in a setting with m = 3 distinct contexts, embedding dimension d = 2, vocabulary of |V| = 5 words and support sets of length |Sj| = 3, j \u2208 [3]. Left: Vis. of context embeddings \u0127j in circle black markers (marked as A,B,C) and of their associated support sets Sj (colored text below each marker). Colored vectors (star markers) represent max-NTP-margin vectors w := eWmmT ev, v \u2208 [5] found by GD. Interpreting decoder vectors as word embeddings leads to intuitive findings on their geometry learned by NTP training. E.g., word embedding w3 (almost) aligns with context-embedding A and the normal hyperplane it defines separates A from B and C, since word 3 only appears after context A. The rest of the words follow two contexts each and their word-representation naturally belongs to the cone defined by the embeddings of those respective contexts. The wider the cone, the larger the magnitude of the word embedding to compensate for the large angle between context-representations that share the same next-word. Note that geometry of depicted word embeddings only depends on support sets, but the conditional probabilities define another set of word representations on an orthogonal (matrix) subspace; see text for details and vis. Right: Upper/lower graphs confirm the predictions of Lemma 2 and of Theorem 2, respectively.", "description": "This figure visualizes the implicit optimization bias of next-token prediction (NTP) in a simple setting. The left panel shows the geometry of context embeddings and word embeddings learned by gradient descent. The right panel shows the training loss, norm growth of the weight matrix, alignment with the max-margin vector, and convergence of the weight matrix to a specific subspace.", "section": "Gradient Descent"}, {"figure_path": "xSziO6gQgG/figures/figures_18_1.jpg", "caption": "Figure 1: Vis. of NTP implicit optimization bias in a setting with m = 3 distinct contexts, embedding dimension d = 2, vocabulary of |V| = 5 words and support sets of length |Sj| = 3, j \u2208 [3]. Left: Vis. of context embeddings \u0127j in circle black markers (marked as A,B,C) and of their associated support sets Sj (colored text below each marker). Colored vectors (star markers) represent max-NTP-margin vectors w := eWmm, \u03c5\u03b5 [5] found by GD. Interpreting decoder vectors as word embeddings leads to intuitive findings on their geometry learned by NTP training. E.g., word embedding w3 (almost) aligns with context-embedding A and the normal hyperplane it defines separates A from B and C, since word 3 only appears after context A. The rest of the words follow two contexts each and their word-representation naturally belongs to the cone defined by the embeddings of those respective contexts. The wider the cone, the larger the magnitude of the word embedding to compensate for the large angle between context-representations that share the same next-word. Note that geometry of depicted word embeddings only depends on support sets, but the conditional probabilities define another set of word representations on an orthogonal (matrix) subspace; see text for details and vis. Right: Upper/lower graphs confirm the predictions of Lemma 2 and of Theorem 2, respectively.", "description": "The figure visualizes the implicit optimization bias of next-token prediction (NTP) using a simple setting with 3 distinct contexts, 2D embedding space, and a vocabulary of 5 words. The left panel shows the context embeddings and their associated support sets, illustrating the geometry of word embeddings learned by NTP training and how they relate to context embeddings. The right panel shows the results of gradient descent (GD) training in terms of loss, norm growth, alignment with the max-margin vector, and convergence to the subspace projection.", "section": "Gradient Descent"}]