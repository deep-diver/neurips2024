[{"type": "text", "text": "Implicit Optimization Bias of Next-token Prediction in Linear Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Christos Thrampoulidis Department of Electrical and Computer Engineering University of British Columbia Vancouver, Canada cthrampo@ece.ubc.ca ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We initiate an investigation into the optimization properties of next-token prediction (NTP), the dominant training paradigm for modern language models. Specifically, we study the structural properties of the solutions selected by gradient-based optimizers among the many possible minimizers of the NTP objective. By framing NTP as cross-entropy minimization across distinct contexts, each tied with a sparse conditional probability distribution across a finite vocabulary of tokens, we introduce \u201cNTP-separability conditions\u201d that enable reaching the data-entropy lower bound. With this setup, and focusing on linear models with fixed context embeddings, we characterize the optimization bias of gradient descent (GD): Within the data subspace defined by the sparsity patterns of distinct contexts, GD selects parameters that equate the logits\u2019 differences of in-support tokens to their logodds. In the orthogonal subspace, the GD parameters diverge in norm and select the direction that maximizes a margin specific to NTP. These findings extend previous research on implicit bias in one-hot classification to the NTP setting, highlighting key differences and prompting further research into the optimization and generalization properties of NTP, irrespective of the specific architecture used to generate the context embeddings. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Next-token prediction (NTP) has emerged as the go-to paradigm in training modern language models, revolutionizing various applications such as machine translation, text-summarization, and language generation [66]. In NTP, models are trained to predict the most probable token given a sequence of preceding tokens, commonly referred to as the context. Concretely, the objective is to learn a mapping from the input context to the probability distribution over the (finite) vocabulary of possible tokens, enabling the model to generate a token that is contextually appropriate [9, 8]. Recently, the NTP paradigm has witnessed remarkable empirical success through its utilization on large-scale deep-learning architectures trained on vast corpora of data [66, 67, 86], leading to unprecedented advances in the field, and the swift integration of these advanced language models into society [62]. Concurrently, researchers have raised critical concerns about robustness, interpretability, and fairness-bias issues arising from our limited understanding of the fundamental operational principles of these models [10, 6]. Despite progress, a comprehensive theory that elucidates the fundamentals of modern language models\u2014including key components like the NTP paradigm and transformer architecture, particularly in terms of optimization and generalization principles\u2014is still lacking. ", "page_idx": 0}, {"type": "text", "text": "We initiate an investigation when implicit optimization biases in training language models under the NTP paradigm, particularly in overparameterized regimes where the empirical-loss reaches its lower bound and there is many possible minimizers. To formalize the NTP paradigm, consider autoregressive model $q_{\\theta}$ parameterized by $\\pmb{\\theta}$ trained to predict the next-token on sequences of length $T$ using the cross-entropy (CE) loss: ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{\\pmb{\\theta}}{\\operatorname*{min}}\\ \\hat{\\mathbb{E}}_{z\\sim\\tau_{n}}\\Big[\\sum_{t\\in[T]}-\\log\\big(q_{\\pmb{\\theta}}\\big(z_{t}\\,\\big|\\,z_{1},\\dots,z_{t-1}\\big)\\big)\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Here, sequences $\\pmb{z}\\,=\\,\\left(z_{1},...\\,,z_{T}\\right)$ consist of tokens $z_{t}$ from a finite vocabulary $\\mathcal{V}\\,=\\,\\{1,\\dotsc,V\\}$ and $\\hat{\\mathbb{E}}$ is expectation over training set ${\\mathcal{T}}_{n}$ of $n$ such sequences sampled from some underlying true distribution over sequences. Typically, the model $q_{\\theta}$ outputs probability of the next token computed via softmax applied on output logits, which are computed by projecting $d$ -dimensional embeddings $h_{\\theta^{\\prime}}$ to the $V$ -dimensional space with a trainable linear decoder $\\check{W}\\in\\mathbb{R}^{\\tilde{V}\\times d}$ . Formally, 1 ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mu\\big(z_{t}\\,|\\,z_{1},\\dots,z_{t-1}\\big)=\\mathbb{S}_{z_{t}}\\big(W h_{\\theta^{\\prime}}(z_{1},\\dots,z_{t-1})\\big)\\,=\\frac{1}{1+\\sum_{z^{\\prime}\\in\\mathcal{Z}_{t}}\\exp\\big((e_{z^{\\prime}}-e_{z_{t}})^{\\top}W h_{\\theta^{\\prime}}(z_{1},\\dots,z_{t-1})\\big)}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "The CE loss is then minimized over $\\pmb{\\theta}=\\left(\\pmb{W},\\pmb{\\theta}^{\\prime}\\right)$ using gradient-based methods, e.g. (S)GD, Adam. ", "page_idx": 1}, {"type": "text", "text": "We pose the question: Given training set $\\mathcal{T}_{n}$ , what are the structural properties of the weights $\\pmb{\\theta}$ found by minimizing the NTP objective with gradient-based optimizers? As in prior research in one-hot supervised classification 2 (e.g. [97, 7, 76, 34]), we specifically target this question in an overparameterized setting, where the NTP objective (1) may have an infinite number of solutions, representing an infinite number of models $\\pmb{\\theta}$ that minimize the training loss. The central challenge is to discern the particular solution the optimizer is inherently biased towards. Since this \u2018bias\u2019 is not explicitly introduced through regularization but is instead ingrained in the training objective and algorithmic structure, it is termed \u2018implicit bias\u2019 [61]. The exploration of implicit bias has a long history in the traditional supervised one-hot classification (see Related Work in Sec. 6). In this traditional scenario, the training set comprises feature-label pairs $(\\boldsymbol{x},\\boldsymbol{y})$ , where $\\pmb{x}\\in\\mathbb{R}^{p}$ is a continuous feature, and $y$ represents its unique label. The optimization process minimizes the following training objective (over $W,\\pmb{\\theta}^{\\prime})$ : $\\hat{\\mathbb{E}}_{(\\pmb{x},y)}\\left[-\\log\\left(\\mathbb{S}_{y}(\\pmb{W}h_{\\pmb{\\theta}^{\\prime}}(\\pmb{x}))\\right)\\right]$ . ", "page_idx": 1}, {"type": "text", "text": "At first glance, excluding the sequential format of Eq. (1), the NTP training scenario might seem identical to traditional one-hot prediction: both aim to minimize the same CE loss across models that parameterize probabilities using the softmax of logits. Consider predicting the next token over fixed-length sequences, say sequences of length $t-1$ , via optimizing: $\\hat{\\mathbb{E}}_{z}\\left[-\\log\\left(\\mathbb{S}_{z_{t}}(W h_{\\theta}(z_{1},\\ldots,z_{t-1}))\\right)\\right]$ . The context here acts as the feature, and the next token as the label. Recent works [49, 52] draw on such apparent similarities to the traditional one-hot classification paradigm to extrapolate known results from the latter to the NTP setting. However, this comparison overlooks a fundamental, yet critical difference in the nature of the training data that distinguishes these two paradigms (even when the sequential format of Eq. (1) is disregarded): In the traditional setting, each feature (e.g., image) is assigned a single label (e.g., image category). In contrast, in the NTP setting, contexts $z_{1},\\dots,z_{t-1}$ of finite length sampled from finite vocabularies are naturally repeated in a (vast) training set, potentially multiple times, each time followed by different tokens $z_{t}$ [73]. Consequently, the NTP paradigm involves training over $m\\leq n$ distinct (non-repetitive) contexts, each followed by a multitude of possible next tokens, appearing at varying frequencies. For instance, the context \"She is excellent at her role as a\" may be followed by next tokens such as \"doctor,\" \"lawyer,\" \"reviewer,\" or \"mother,\" each with different frequencies. Importantly, certain vocabulary tokens may not appear after a given context; e.g., in the above example, tokens like \"run,\" \"and,\" etc., will not follow. ", "page_idx": 1}, {"type": "text", "text": "Model. We study NTP training over a finite vocabulary employing the following model. Given a large training set of $n$ total sequences, we identify $m\\leq n$ distinct contexts. Each distinct context $j\\in[m]$ is linked to a $V$ -dimensional empirical probability vector $\\hat{\\pmb{p}}_{j}$ , which encodes the frequency with which each vocabulary token follows the context throughout its occurrences in the training set. Crucially, the probability vectors $\\hat{\\pmb{p}}_{j}$ are sparse, i.e., the support set ${\\mathbf{}}S_{j}$ of $\\hat{\\pmb{p}}_{j}$ satisfies $\\left|S_{j}\\right|\\ll\\left|\\mathcal{V}\\right|=V$ . In an extreme where $\\left|S_{j}\\right|=1,\\breve{\\forall j}\\in[m]$ , the probability vector $\\hat{\\pmb{p}}_{j}$ becomes one-hot, leading to a scenario reminiscent of the traditional classification setting described earlier. However, such an extreme is essentially improbable in practical language modeling [73]. With this framing, the NTP paradigm is also related to supervised vision classification with soft labels, which advocates for training models on datasets where each example is associated with a vector of soft labels (rather than a one-hot vector), such as by averaging multiple annotators\u2019 hard labels [65], knowledge distillation [32] or label smoothing [79]. With this connection, our analysis can also be interpreted (more broadly) as investigating the implicit bias of sparse soft-label classification. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "1.1 Contributions and Organization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Formulation. Recognizing the differences between NTP and one-hot classification, we study the question of implicit optimization bias within the NTP setting. To facilitate this, we utilize the model outlined in the previous paragraph and detailed in Sec. 2. For concreteness, our analysis adopts a \u2019top-down\u2019 approach, training only the decoding (also referred to as word-embedding) matrix $W\\in\\stackrel{\\cdot}{\\mathbb{R}}^{V\\times d}$ while keeping context-embeddings fixed. This approach mirrors foundational studies on implicit optimization bias in one-hot classification [76, 34], which first focused on linear models. It allows exploring the complexities of the NTP training objective, distinct from the embedding architecture3, and while it renders the logits linear and the objective convex, it still poses a technical challenge in terms of determining parameter convergence [76, 34, 37, 60, 38]. ", "page_idx": 2}, {"type": "text", "text": "Conditions for reaching entropy. In Sec. 3, we identify the necessary and sufficient conditions for the logits of the trained model to enable the CE loss to approach its lower bound, the empirical conditional entropy. We introduce two conditions: ${\\mathrm{NTP}}_{\\mathcal{H}}$ -compatibility and NTP-separability, which impose constraints on mutually orthogonal subspaces that are determined by the sparsity patterns of distinct contexts within the dataset. These conditions determine the necessary and sufficient overparameterization a model needs to achieve the empirical entropy lower bound during training. ", "page_idx": 2}, {"type": "text", "text": "Margin in NTP setting. Motivated by the NTP-separability condition, we introduce a margin concept for NTP in Sec. 4, which extends the classical definition of margin used in one-hot supervised classification [88]. We further establish the relevance of this new margin notion for optimization by demonstrating that a decoder maximizing the NTP-margin, denoted as $W^{\\mathrm{mm}}$ , guides the directional convergence of the ridge-regularized CE minimizer, W\u0302\u03bb, as the regularization parameter $\\lambda\\to0$ . ", "page_idx": 2}, {"type": "text", "text": "Implicit bias of GD. We establish that $W^{\\mathrm{mm}}$ also determines the implicit bias of gradient descent (GD) iterates in Sec. 5. Specifically, in the limit of iterations $k\\ \\rightarrow\\ \\infty$ , the GD iterates grow undoubtedly in norm and converge to a finite $W^{\\star}$ within a data subspace $\\mathcal{F}$ , while simultaneously aligning with $W^{\\mathrm{mm}}$ in the complementary subspace $\\mathfrak{F}^{\\perp}$ . The finite component $W^{\\star}\\in\\mathcal{F}$ solves a system of linear equations associated with the $\\mathrm{NTP}_{\\mathcal{H}}$ -compatibility condition. ", "page_idx": 2}, {"type": "text", "text": "Finally, we numerically verify these findings and discuss related and future work in Secs. 6 and 7.   \nAdditional experiments, further related work and detailed proofs are in the appendix. ", "page_idx": 2}, {"type": "text", "text": "2 Setup", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Let vocabulary $\\mathcal{V}=[V]:=\\left\\{1,\\ldots,V\\right\\}$ represent a set of $V$ tokens (e.g. words) and $\\boldsymbol{z}_{1:t}=\\left(\\boldsymbol{z}_{1},\\ldots,\\boldsymbol{z}_{t}\\right)$ denote sequence of $t$ tokens $z_{t}\\in\\mathcal{V}$ . To simplify presentation, we focus on predicting the $T$ -th token $z_{T}$ given contexts $z_{<T}:=z_{1:T-1}$ of fixed length, and we further let $\\pmb{x}=z_{<t}$ denote the context and $z$ denote the last token. See App. C for straightforward extension to the sequential format of Eq. (1). ", "page_idx": 2}, {"type": "text", "text": "We assume access to a training set consisting of $n$ sequences $\\mathcal{T}_{n}:=\\{\\left(\\pmb{x}_{i},z_{i}\\right)\\}_{i\\in[n]}$ , with $\\pmb{x}_{i}\\in\\mathcal{X}:=$ $\\mathcal{V}^{T-1}$ and $z_{i}\\in\\mathcal{V}$ . Let $h:\\mathcal{X}\\to\\mathbb{R}^{d}$ an embedding map that maps contexts (i.e., sequences of $T-1$ tokens) to $d$ -dimensional embeddings. The map $h$ can be parameterized (e.g. by a transformer [90] or an LSTM [5]), but this paper assumes that it is fixed. The next-token is predicted via a linear model $f_{W}:\\mathcal{X}\\to\\bar{\\mathbb{R}^{V}}$ parameterized by decoding matrix $\\pmb{W}\\in\\mathbb{R}^{V\\times d}$ , such that $f_{W}({\\boldsymbol{\\mathbf{\\mathit{x}}}})=W h({\\boldsymbol{\\mathbf{\\mathit{x}}}})$ . When the model output passes through a softmax, it defines the model\u2019s probability mass function for the next-token prediction, given as $\\hat{q}_{W}(\\cdot|\\pmb{x})=\\mathbb{S}\\big(f_{W}(\\pmb{x})\\big)$ , where $\\mathbb{S}(\\cdot)\\,\\colon\\mathbb{R}^{V}\\to\\tilde{\\Delta}^{V-1}$ is the softmax and $\\Delta^{V-1}$ is the $V$ -dimensional simplex. The decoder is trained by minimizing the empirical CE loss $\\begin{array}{r}{\\mathrm{CE}(W):=\\frac{1}{n}\\sum_{i\\in[n]}-\\log\\left(\\hat{q}_{W}(\\hat{z}_{i}|\\pmb{x}_{i})\\right)}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "Distinct sequences and next-token distributions. Given dataset $\\mathcal{T}_{n}$ we denote $\\bar{\\pmb{x}}_{1},\\allowbreak\\cdot\\cdot\\cdot,\\bar{\\pmb{x}}_{m}$ the $m\\leq n$ distinct contexts among the (large number of) total $n$ contexts $\\pmb{x}_{1},\\dots,\\pmb{x}_{n}$ within ${\\mathcal{T}}_{n}$ . Let $\\hat{\\pi}_{j}$ be the empirical probability of distinct context $\\bar{\\pmb{x}}_{j}$ . That is, $1\\leq n\\cdot{\\hat{\\pi}}_{j}\\leq n$ is the number of contexts $\\pmb{x}_{i}$ that equal $\\bar{\\pmb{x}}_{j}$ . Furthermore, for each distinct context $\\bar{\\pmb{x}}_{j},j\\in[m]$ let $\\hat{p}_{j}\\in\\Delta^{V-1}$ denote the probability vector of conditional next-token distribution, i.e., $\\hat{p_{j,z}}:=\\hat{p}\\left(z|\\bar{\\pmb{x}}_{j}\\right),\\bar{z}\\in\\mathcal{V},j\\in[m]$ . In other words, $n\\cdot\\hat{\\pi}_{j}\\cdot\\hat{p}_{j,z}$ is the number of occurences of token $z$ as a follow-up to context $\\bar{\\pmb{x}}_{j}$ . Finally, we denote the support set and size of the support set of these conditional distributions as $\\bar{S_{j}}:=\\{z\\in\\mathcal{V}|\\hat{p}_{j,z}>0\\}$ and $S_{j}^{\\'}:=|S_{j}|$ . Tokens $z\\in S_{j}$ and $v\\not\\in{\\cal S}_{j}$ are referred to as \u2019in-support\u2019 and \u2019out-of-support\u2019 respectively. Onwards, we implicitly assume that \u201cnot all tokens are likely after every context,\u201d i.e. $\\exists j\\in[m]$ such that $S_{j}<V$ . This mild assumption is naturally satisfied in language modeling under rich enough vocabulary. With this notation, 4 we can express the NTP training loss as ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{CE}(W)=-\\sum_{j\\in[m]}\\hat{\\pi}_{j}\\sum_{z\\in\\mathcal{V}}\\hat{p}_{j,z}\\log\\left(\\mathbb{S}_{z}(W h(\\bar{x}_{j}))\\right)=-\\sum_{j\\in[m]}\\hat{\\pi}_{j}\\sum_{z\\in\\mathcal{S}_{j}}\\hat{p}_{j,z}\\log\\left(\\mathbb{S}_{z}(W\\bar{h}_{j})\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where, in the last line we defined the shorthand $\\bar{\\pmb{h}}_{j}=h(\\bar{\\pmb{x}}_{j})$ . Similarly, we let $\\pmb{h}_{i}=h(\\pmb{x}_{i}),i\\in[n]$ . With some abuse of notation, we then obtain the following equivalent descriptions of the training set ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\{(\\pmb{x}_{i},z_{i})\\}_{i\\in[n]}=:\\mathcal{T}_{n}\\equiv\\mathcal{T}_{m}:=\\{(\\bar{h}_{j},\\hat{\\pi}_{j},\\hat{p}_{j,z\\in\\mathcal{V}})\\}_{j\\in[m]}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "that emphasizes distinct contexts and their respsective sparse next-token probability distributions. ", "page_idx": 3}, {"type": "text", "text": "Entropy. The empirical $T$ -gram entropy (referred to hereafter as entropy for simplicity) of the dataset is [74, 73]: $\\begin{array}{r}{\\mathcal{H}_{T}:=\\mathcal{H}:=\\hat{\\mathbb{E}}_{(\\pmb{x},z)\\sim\\mathcal{T}_{n}}\\left[-\\log\\left(\\hat{p}(z|\\pmb{x})\\right)\\right]\\,=\\,-\\sum_{j\\in[m]}\\sum_{z\\in\\mathcal{S}_{j}}\\hat{\\pi}_{j}\\hat{p}_{j,z}\\log\\left(\\hat{p}_{j,z}\\right)}\\end{array}$ . It lower bounds the CE loss since $\\operatorname{CE}(W)=\\mathcal{H}+\\operatorname{KL}\\left(\\hat{p}\\left|\\right|\\hat{q}_{W}\\right)$ and the KL divergence is nonnegative. ", "page_idx": 3}, {"type": "text", "text": "3 When can the NTP-loss reach the entropy lower-bound? ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The first question we ask is: Under what conditions on the training data can the CE loss reach its entropy lower-bound? By the entropy lower-bound, $\\operatorname{CE}(W)=\\mathcal{\\bar{H}}\\Leftrightarrow\\operatorname{KL}\\left(\\hat{p}\\,\\big|\\big|\\,\\hat{q}_{W}\\right)=0$ iff for all $j\\in[\\bar{m}]$ and all $z\\in\\mathcal{V}$ : $\\hat{q}_{W}\\!\\left(z|\\bar{\\pmb x}_{j}\\right)=\\bar{\\hat{p_{j,z}}}$ . Equivalently, for all $j\\in[m]$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{S}_{z}\\big({\\pmb W}\\bar{\\pmb h}_{j}\\big)=\\hat{p}_{j,z},\\quad\\forall z\\in{\\b S}_{j}\\,,}\\\\ &{\\mathbb{S}_{v}\\big({\\pmb W}\\bar{\\pmb h}_{j}\\big)=0,\\quad\\forall v\\notin{\\b S}_{j}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Beginning with (3a), this requires5 the training data to satisfy the ${\\mathrm{NTP}}_{\\mathcal{H}}$ -compatibility condition defined below. ", "page_idx": 3}, {"type": "text", "text": "Definition 1 $\\mathrm{NTP}_{\\mathcal{H}}$ -compatible). Let $e_{v}$ denote the $v$ -th standard basis vector in $\\mathbb{R}^{V}$ . We say that training data $\\tau_{m}$ are NTP-entropy-compatible if there exists $V\\times d$ matrix $W^{\\mathrm{p}}$ satisfying: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\forall j\\in[m],z\\neq z^{\\prime}\\in S_{j}\\ :\\ (e_{z}-e_{z^{\\prime}})^{\\top}W^{\\mathrm{p}}\\bar{h}_{j}=\\log\\left(\\hat{p}_{j,z}\\Big/\\hat{p}_{j,z^{\\prime}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We comment on the independence of the constraints: Fix any $j\\in[m]$ . Then, the set of constraints (as expressed in Eq. (4)) for all $z\\neq z^{\\prime}\\in S_{j}$ (yielding $\\left(\\stackrel{S_{j}}{_{2}}\\right)$ constraints in total) is equivalent to the set of the same constraints for any anchor $z_{j}\\in S_{j}$ and $z^{\\prime}\\neq z_{j}\\in S_{j}$ , i.e., an effective total of $S_{j}-1$ linearly independent constraints for each $j\\in[m]$ . Additionally, note that the system of equations in Eq. (4) constrains $W^{\\mathrm{p}}$ with respect to a specific subspace of $V\\times d$ matrices: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{F}=\\mathrm{span}\\left(\\left\\{(e_{z}-e_{z^{\\prime}})\\bar{h}_{j}^{\\intercal}:z\\neq z^{\\prime}\\in S_{j},j\\in[m]\\right\\}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "that is defined in terms of context embeddings and their respective support sets. Assuming Eqs. (4) have a solution, we denote the unique solution within the subspace $\\mathcal{F}$ as $W^{\\star}\\in\\mathcal{F}$ for later reference 6. ", "page_idx": 3}, {"type": "text", "text": "Next, we examine Eq. (3b), which requires softmax outputs be zero for tokens that never occur following a fixed context throughout the dataset. Due to the strict positivity of softmax, the constraint is never satisfied for finite $W$ . Thus, for all finite $W$ , there exists a gap between the cross-entropy loss and its lower bound, i.e., $\\mathrm{CE}(W)>\\mathcal{H}$ . Yet, it is possible to approach entropy as the norm of the weights $W$ grows, provided that weights move in the appropriate direction formalized below. ", "page_idx": 3}, {"type": "text", "text": "Definition 2 (NTP-separable). We say that training data $\\tau_{m}$ are NTP-separable if there exists $V\\times d$ matrix $W^{\\mathrm{d}}$ satisfying the following: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\forall j\\in[m],z\\neq z^{\\prime}\\in S_{j}\\ :\\ (e_{z}-e_{z^{\\prime}})^{\\intercal}W^{\\mathrm{d}}\\bar{h}_{j}=0}\\\\ {\\forall j\\in[m],z\\in S_{j},v\\notin S_{j}\\ :\\ (e_{z}-e_{v})^{\\intercal}W^{\\mathrm{d}}\\bar{h}_{j}\\geq1\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "As before, it is easy to see that the constraints in (6) can be equivalently expressed by enforcing (6a) and (6b) for an anchor $z_{j}\\in S_{j}$ and all $z^{\\prime}\\in S_{j}\\setminus\\{z_{j}\\}$ and $v\\not\\in{\\cal S}_{j}$ , respectively. Consequently, there exist effectively $V-1$ linearly independent constraints per context $j\\in[m]$ . ", "page_idx": 4}, {"type": "text", "text": "We now discuss the interpretation of these constraints. The subspace constraints in Eq. (6a) project $W^{\\mathrm{d}}$ onto the subspace $\\mathfrak{F}^{\\perp}$ , which is the orthogonal complement of the subspace $\\mathcal{F}$ defined in (5). This leaves the softmax probabilities of possible next tokens (in set ${\\mathbf{}}S_{j}$ ) intact, and fully determined by $W^{\\mathrm{p}}$ as per the ${\\mathrm{NTP}}_{\\mathcal{H}}$ -compatibility condition. Formally, $W^{\\mathrm{p}}+W^{\\mathrm{d}}$ continues satisfying (4). Moving on the halfspace constraints in (6b), we can interpret these using Kesler\u2019s construction as enforcing linear separability in the space $\\mathbb{R}^{V\\times d}$ [30]: Each $d$ -dimensional context embedding $\\bar{h}_{j}$ is mapped to $S_{j}(V-S_{j})$ higher-dimensional points $(e_{z}-e_{v})\\bar{h}_{j}^{\\top},z\\in S_{j},v\\notin S_{j}$ . These points collectively for all $j\\in[m]$ must lie within the interior of the same halfspace induced by the hyperplane $\\langle W^{\\mathrm{d}},\\cdot\\rangle\\,=\\,0$ . Refer to Fig. 1(Left) and its caption for an alternative interpretation of the rows of $W^{\\mathrm{mm}}$ as word-embeddings in $\\mathbb{R}^{d}$ (illustration in $d=2$ ). ", "page_idx": 4}, {"type": "text", "text": "The impact of NTP-separability on the softmax probabilities can be understood algebraically by considering $W_{\\gamma}:=\\gamma\\bar{W}^{\\mathrm{d}}$ and $v\\not\\in{\\cal S}_{j}$ . We have: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{S}_{v}\\big(W^{\\gamma}\\bar{h}_{j}\\big)=\\Big(\\displaystyle\\sum_{z\\in\\mathcal{S}_{j}}e^{\\gamma(e_{z}-e_{v})^{\\top}W^{\\mathrm{d}}\\bar{h}_{j}}+\\displaystyle\\sum_{v^{\\prime}\\in\\mathcal{S}_{j}}e^{\\gamma\\left(e_{v^{\\prime}}-e_{v}\\right)^{\\top}W^{\\mathrm{d}}\\bar{h}_{j}}\\Big)^{-1}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\Big(\\displaystyle\\sum_{z\\in\\mathcal{S}_{j}}e^{\\gamma(e_{z}-e_{v})^{\\top}W^{\\mathrm{d}}\\bar{h}_{j}}\\Big)^{-1}}\\\\ &{\\qquad\\qquad\\qquad\\leq e^{-\\gamma},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the first inequality removes non-negative exponential terms and the second one follows from (6b). The upper bound above approaches 0 as $\\gamma\\to\\infty$ , thus (3b) holds asymptotically in $\\gamma$ . ", "page_idx": 4}, {"type": "text", "text": "Taking into account the observations made above, the satisfaction of both conditions guarantees convergence of the cross-entropy loss CE to $\\mathcal{H}$ . This is formalized in the proposition below. ", "page_idx": 4}, {"type": "text", "text": "Proposition 1. Assume training data $\\tau_{m}$ is $N T P_{\\mathcal{H}}$ -compatible and NTP-separable, with the respective matrices $W^{\\mathrm{p}}$ and $W^{\\mathrm{d}}$ satisfying conditions (4) and (6). While all finite $W$ satisfy $\\mathrm{CE}(W)>\\mathcal{H}$ , it holds for W \u03b3 = W p + \u03b3 \u22c5W d that CE(W \u03b3) \u03b3\u2192 + \u221e . ", "page_idx": 4}, {"type": "text", "text": "Hence, CE approaches its lower-bound in the limit of a direction $\\overline{{W^{\\mathrm{d}}}}:=W^{\\mathrm{d}}/\\|W^{\\mathrm{d}}\\|$ and offset $W^{\\mathrm{p}}$ satisfying the constraints of NTP-separability and NTP-compatibility, respectively. In other words, parameter weights $W$ that minimize the CE loss consist of two components: a finite projection $W_{\\mathcal{F}}:=\\mathcal{P}_{\\mathcal{F}}(W)=W^{\\star}$ onto the data subspace $\\mathcal{F}$ and an infinite-norm component onto the orthogonal complement $\\mathfrak{F}^{\\perp}$ in the direction of $W^{\\mathrm{d}}$ . ", "page_idx": 4}, {"type": "text", "text": "Finally, we note that while Defns. 1 and 2 are stated for linear models, they naturally extend to a more general formulation for nonlinear models. Specifically, consider NTP-separability (similar for NTP-compatibility): the general conditions require that both the decoder weights $W$ and model weights $\\pmb{\\theta}$ , which parameterize the embeddings $\\bar{h}_{j}^{\\ }=h_{\\theta}(\\bar{x}_{j})$ , must satisfy Eq. (6) simultaneously. ", "page_idx": 4}, {"type": "text", "text": "3.1 The role of overparameterization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We show that overparameterization provides a sufficient condition for the solvability of Eqs. (4) and (6). Start with the halfspace constraints in Eq. (4) for ${\\mathrm{NTP}}_{\\mathcal{H}}$ -compatibility. These can be compactly expressed as ${\\pmb E}_{j,z_{j}}\\,{\\pmb W}^{\\mathrm{p}}\\bar{\\pmb h}_{j}={\\pmb a}_{j,z}$ , where $\\pmb{E}_{j,z_{j}}\\in\\mathbb{R}^{(S_{j}-1)\\times V}$ has rows $e_{z_{j}}-e_{z^{\\prime}}$ and $\\pmb{a}_{j,z_{j}}\\in\\mathbb{R}^{(S_{j}-1)}$ has entries $\\log\\left(\\hat{p}_{j,z_{j}}/\\hat{p}_{j,z^{\\prime}}\\right)$ for some anchor $z_{j}\\;\\in\\;S_{j}$ . Now, since the rows of $E_{j,z_{j}}$ are linearly independent, the question becomes equivalently that of determining when $W^{\\mathrm{p}}\\big[\\bar{h}_{1},\\ldots,\\bar{h}_{m}\\big]\\;=$ $\\left[E_{1,z_{1}}^{\\dagger}a_{1,z_{1}},\\hdots,E_{m,z_{m}}^{\\dagger}a_{m,z_{m}}\\right]$ E\u2020m,zmam,zm] has a solution. This is always the case when d > m and the d \u00d7 m embedding matrix $\\bar{\\b{H}}=\\left[\\bar{\\b{h}}_{1},\\dots,\\bar{\\b{h}}_{m}\\right]$ is full rank $(m)$ . Then, there exists $W^{\\mathrm{p}}$ such that condition (4) holds. In fact, $\\bar{H}^{\\intercal}$ has a nullspace, implying the existence of an infinite number of solutions to (4). These solutions take the form $W^{\\mathrm{p}}=\\bar{W^{\\star}}+\\mathbf{\\bar{W}}_{\\perp}^{\\mathrm{p}}$ , where $W^{\\star}\\in\\mathcal{F}$ is the unique solution onto the subspace, and $W_{\\perp}^{\\mathrm{p}}\\in\\mathcal{F}^{\\perp}$ . ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "In contrast to (4), the constraints in (6) involve linear inequalities. However, a sufficient proxy for feasibility in this case is that the corresponding system of equations (instead of inequalities) has a solution. By following the exact same argument as before, we arrive at the same sufficient conditions for the existence of a solution $W^{\\mathrm{d}}$ . We summarize these findings. ", "page_idx": 5}, {"type": "text", "text": "Lemma 1 (Overparameterization implies NTP-separability). Assume overparameterization $d>m$ and full-rank embedding matrix $\\bar{H}\\in\\bar{\\mathbb{R}}^{d\\times m}$ . Then, there exists an infinite number of solutions $W^{\\mathrm{p}}$ and $\\mathbf{\\bar{W}}^{\\mathrm{d}}$ that satisfy conditions (4) and (6), respectively. ", "page_idx": 5}, {"type": "text", "text": "Thus, $d>m$ , 7 which also generically favors full-rankness of the embedding matrix [92], implies both ${\\mathrm{NTP}}_{\\mathcal{H}}$ -compatibility and NTP-separability. Combined with Prop. 1, it also implies that there are infinitely many possible directions $\\bar{\\b{W}}^{\\mathrm{d}}$ along which the NTP loss approaches $\\mathcal{H}$ , motivating the implicit-bias question: For a specific iterative algorithm aimed at minimizing the NTP loss, which direction does it prefer? We will address this question in the remainder of the paper. ", "page_idx": 5}, {"type": "text", "text": "Remark 1. In the trivial case where $S_{j}=1$ , $\\forall j\\in[m]$ (one-hot classification), the entropy lower bound is zero and is attained iff the data is linearly separable. Indeed, $\\mathcal{F}$ reduces to the empty set, and NTP-separability simplifies to traditional multiclass separability. For binary classification, [20] showed that $d/m>1/2$ is sufficient and necessary for data in general position to be linearly separable. More recently, several works have extended this analysis to structured (random) data, including [12, 71, 57, 54]. The exact threshold in corresponding mutliclass settings is more intricate, but [19, 81, 11] have made progress in this direction. An interesting question is determining exact thresholds for NTP-separability, which would improve upon the sufficient condition of Lemma 1. ", "page_idx": 5}, {"type": "text", "text": "4 Regularization path ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "This section investigates the implicit bias of NTP by examining the minimization of CE loss through iterates defined as follows for an increasing sequence of positive regularization parameters $B$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{W}_{B}:=\\arg\\operatorname*{min}_{\\|W\\|\\leq B}\\mathrm{CE}(W)\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This involves minimizing a strictly convex function in a bounded domain; thus, $\\widehat{W}_{B}$ is unique. This section\u2019s main result characterizes the limit of $\\widehat{W}_{B}$ as $B\\to\\infty$ under NTP-separability/compatibility. Before that, we first define the next-token prediction support-vector machines (SVM) problem. ", "page_idx": 5}, {"type": "text", "text": "Definition 3 (NTP-SVM). Given NTP-separable training set $\\tau_{m}$ , NTP-SVM solves the following: ", "page_idx": 5}, {"type": "equation", "text": "$$\nW^{\\mathrm{mm}}:=\\arg\\operatorname*{min}_{W}\\ \\|W\\|\\qquad s u b j.\\ t o\\ W\\in\\mathbb{R}^{V\\times d}\\,s a t i s f y i n g\\ (6\\mathrm{a})\\,a n d\\ (6\\mathrm{b}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This is a strongly convex quadratic program with $\\begin{array}{r}{m V{-}\\sum_{j\\in[m]}S_{j}}\\end{array}$ linear inequality and $\\textstyle\\sum_{j\\in[m]}S_{j}-m$ linear equality constraints. Its solution can be also defined as the classifier that maximizes margin between in and out-of -support tokens while being constrained on the orthogonal compelemnt ${\\mathcal{F}}^{\\perp}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{W^{\\mathrm{mm}}}}=\\arg\\operatorname*{max}_{\\|\\pmb{W}\\|=1,\\pmb{W}\\in\\mathcal{F}^{\\bot}}\\operatorname*{min}_{j\\in[m],z\\in S_{j},v\\notin S_{j}}\\big(\\pmb{e}_{z}-\\pmb{e}_{v}\\big)^{\\top}\\pmb{W}\\bar{h}_{j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "It turns out this direction determines the preferred limiting direction of the regularization path. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1 (Implicit bias of the regularization-path). Assume training data $\\tau_{m}$ is $N T P_{\\mathcal{H}}$ -compatible and NTP-separable. Let $\\widehat{W}_{B}$ be defined as in (8). Then, it holds that $\\begin{array}{r}{\\operatorname*{lim}_{B\\to\\infty}\\left\\langle\\frac{\\widehat{W}_{B}}{\\|\\widehat{W}_{B}\\|},\\frac{W^{\\mathrm{mm}}}{\\|W^{\\mathrm{mm}}\\|}\\right\\rangle=1}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "The proof sketch below illustrates how the NTP-separability/compatibility assumptions influence the outcome and why the regularization path induces an optimization bias toward the NTP-SVM direction. Complementing Thm. 1, we also show (see Lemma 4 in the appendix) that $\\begin{array}{r}{\\operatorname*{lim}_{B\\to\\infty}\\mathcal{P}_{\\mathcal{F}}(W_{B})=W^{\\star}}\\end{array}$ . These together provide a complete characterization of the implicit optimization bias of (8). ", "page_idx": 5}, {"type": "text", "text": "Proof sketch (App. E.2 for details). We first show $\\widehat{W}_{B}$ is on the boundary: $\\|\\widehat{\\pmb{W}}_{B}\\|=B$ . If not, then $\\langle\\nabla\\operatorname{CE}(\\widehat{\\pmb{W}}_{B}),\\pmb{W}^{\\mathrm{mm}}\\rangle=0$ . But, few algebraic manipulations show $\\langle-\\nabla\\,\\mathrm{CE}(\\widehat{\\pmb{W}}_{B}),\\pmb{W}^{\\mathrm{mm}}\\rangle$ equals ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sum_{j\\in[m]}\\hat{\\pi}_{j}\\sum_{z\\in S_{j}}\\hat{p}_{j,z}\\Big(\\sum_{z^{\\prime}\\in S_{j},z^{\\prime}\\neq z}s_{j,z^{\\prime}}\\left(e_{z}-e_{z^{\\prime}}\\right)^{\\top}W^{\\mathrm{mm}}\\bar{h}_{j}+\\sum_{v\\notin S_{j}}s_{j,v}\\left(e_{z}-e_{v}\\right)^{\\top}W^{\\mathrm{mm}}\\bar{h}_{j}\\Big),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where we denote $s_{j,v}:=\\mathfrak{S}_{v}\\big(\\widehat{W}_{B}\\bar{h}_{j}\\big)>0,v\\in\\mathcal{V},j\\in[m]$ . The first term in the parenthesis is zero by (6a), while the second term is strictly positive by (6b), leading to contradiction. ", "page_idx": 6}, {"type": "text", "text": "Now, consider a \u2018genie\u2019 point $W_{B}^{\\star}=W^{\\star}{+}R(B){\\cdot}W^{\\mathrm{mm}}$ , where $W^{\\star}\\in\\mathcal{F}$ satisfies (4), and $R=R(B)$ is chosen such that $\\|\\pmb{W}_{B}^{\\star}\\|\\,=\\,B$ . We will show that $W_{B}^{\\star}$ attains a small CE loss as $B$ (hence, $R$ ) grows. To do this, denote for convenience the logits ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\ell_{j,v}^{\\star}:=e_{v}^{\\top}W^{\\star}\\bar{h}_{j}\\quad\\mathrm{and}\\quad\\ell_{j,v}^{\\operatorname*{mm}}:=e_{v}^{\\top}W^{\\operatorname*{mm}}\\bar{h}_{j}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for all for $v\\in\\mathcal{V},j\\in[m]$ , and note that $e_{v}^{\\top}W_{B}^{\\star}\\bar{h}_{j}=\\ell_{j,v}^{\\star}+R\\ell_{j,v}^{\\mathrm{mm}}$ . By using (4) and (6a): ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sum_{z^{\\prime}\\in S_{j}}e^{-(\\ell_{j,z}^{\\star}+R\\ell_{j,z}^{\\operatorname*{mm}}-\\ell_{j,z^{\\prime}}^{\\star}-R\\ell_{j,z^{\\prime}}^{\\operatorname*{mm}})}=\\sum_{z^{\\prime}\\in S_{j}}e^{-(\\ell_{j,z}^{\\star}-\\ell_{j,z^{\\prime}}^{\\star})}=\\sum_{z^{\\prime}\\in S_{j}}\\frac{\\hat{p}_{j,z^{\\prime}}}{\\hat{p}_{j,z}}=\\frac{1}{\\hat{p}_{j,z}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Moreover, using (6b) and defining $C:=V e^{\\|\\pmb{W}^{\\star}\\|_{M}}$ for $M:=\\sqrt{2}\\cdot\\operatorname*{max}_{j\\in[m]}\\|\\bar{\\pmb{h}}_{j}\\|$ , gives: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sum_{v\\notin S_{j}}e^{-(\\ell_{j,z}^{\\star}+R\\ell_{j,z}^{\\operatorname*{mm}}-\\ell_{j,v}^{\\star}-R\\ell_{j,v}^{\\operatorname*{mm}})}\\leq e^{-R}\\sum_{v\\notin S_{j}}e^{-(\\ell_{j,z}^{\\star}-\\ell_{j,v}^{\\star})}\\leq C\\,e^{-R}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Combining the above within Eq. (2), using $\\log(1+x)\\,\\leq\\,x,x\\,>\\,0$ and the fact that $\\hat{\\pi}_{j},\\hat{p}_{j,z}$ are probabilities, yields: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{CE}(W_{B}^{\\star})\\leq\\sum_{j\\in[m]}\\hat{\\pi}_{j}\\sum_{z\\in S_{j}}\\hat{p}_{j,z}\\log\\Big(\\frac{1}{\\hat{p}_{j,z}}+C\\,e^{-R}\\Big)\\,\\leq\\mathcal{H}+C\\,e^{-R}\\,.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Next, towards contradiction, we will show that if $\\widehat{W}_{B}$ is not in the direction of $W^{\\mathrm{mm}}$ , then it incurs a loss that is larger than $\\mathrm{CE}(W_{B}^{\\star})$ . The trick here is to bound the KL divergence term: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{CE}(\\widehat{W}_{B})-\\mathcal{H}=\\sum_{j\\in[m]}\\hat{\\pi}_{j}\\sum_{z\\in{\\cal S}_{j}}\\hat{p}_{j,z}\\log\\Big(\\hat{p}_{j,z}\\Big(\\sum_{z^{\\prime}\\in{\\cal S}_{j}}e^{\\ell_{j,z^{\\prime}}-\\ell_{j,z}}+\\sum_{v\\notin{\\cal S}_{j}}e^{\\ell_{j,v}-\\ell_{j,z}}\\Big)\\Big),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where we denote logits $\\ell_{j,v}:=e_{v}^{\\top}\\widehat{\\pmb{W}}_{B}\\bar{h}_{j}$ . Assume there exists $\\epsilon>0$ and arbitrarily large $B$ satisfying: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\left\\|\\left(\\|W^{\\mathrm{mm}}\\|/B\\right)\\widehat{\\pmb{W}}_{B}-\\pmb{W}^{\\mathrm{mm}}\\right\\|>\\epsilon.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Define $\\widehat{\\pmb{W}}=(\\widehat{\\pmb{W}}_{B}-\\pmb{W}^{\\star})/R^{\\prime}(B)$ , where $R^{\\prime}=R^{\\prime}(B)>0$ can be chosen so that $\\|\\widehat{\\boldsymbol{W}}\\|=\\|\\boldsymbol{W}^{\\mathrm{mm}}\\|$ . Further choose $B$ large enough so that Eq. (11) guarantees $\\|\\widehat{\\pmb{W}}-\\pmb{W}^{\\mathrm{mm}}\\|\\,\\geq\\,\\epsilon^{\\prime}$ , for some $\\epsilon^{\\prime}>0$ . Since $W^{\\mathrm{mm}}$ is the unique minimizer of (NTP-SVM) and $\\|\\widehat{\\boldsymbol{W}}\\|\\,=\\,\\|\\boldsymbol{W}^{\\mathrm{mm}}\\|$ , there exists $\\delta\\in(0,1)$ and $j~\\in~[m]$ such that at least one of the following is true: $(i)~\\exists z$ and $z^{\\prime}~\\neq~z~\\in~{\\cal S}_{j}$ such that $|(e_{z}-e_{z^{\\prime}})^{\\intercal}\\widehat{\\pmb{W}}\\bar{h}_{j}|\\geq\\delta$ (ii) $\\exists z\\in S_{j},v\\notin S_{j}$ such that $(e_{z}-e_{v})^{\\top}\\widehat{W}\\bar{h}_{j}\\leq1-\\delta$ . ", "page_idx": 6}, {"type": "text", "text": "Case $(i)$ : Without loss of generality $(e_{z}-e_{z^{\\prime}})^{\\intercal}\\widehat{W}\\bar{h}_{j}\\leq-\\delta$ (otherwise, filp $z,z^{\\prime})$ . Thus, ignoring all but the $(j,z,z^{\\prime})$ -term in (10) and using $\\begin{array}{r}{\\ell_{j,z^{\\prime}}-\\ell_{j,z}\\geq R^{\\prime}\\delta+\\log\\left(\\frac{\\hat{p}_{j,z^{\\prime}}}{\\hat{p}_{j,z}}\\right)}\\end{array}$ gives ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{CE}(\\widehat{W}_{B})-\\mathcal{H}\\geq\\widehat{\\pi}_{j}\\widehat{p}_{j,z}\\log\\Big(\\widehat{p}_{j,z}e^{(\\ell_{j,z^{\\prime}}-\\ell_{j,z})}\\Big)\\geq\\frac{1}{n}\\log\\big(\\frac{e^{R^{\\prime}\\delta}}{n}\\big).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Comparing this to (9) for large enough $B$ gives that $\\mathrm{CE}(\\widehat{\\pmb{W}}_{B})>\\mathrm{CE}({\\pmb{W}}_{B}^{\\star})$ , a contradiction. ", "page_idx": 6}, {"type": "text", "text": "Case $(i i)$ : We can assume $\\widehat{W}\\in\\mathcal{F}^{\\perp}$ , since otherwise we are in Case (i). Now, again ignoring all but the $\\overline{{(j,z)}}$ term in the CE loss for which the assumption holds for some $v\\not\\in{\\cal S}_{j}$ , we find ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{CE}(\\widehat{W}_{B})-\\mathcal{H}\\geq\\hat{\\pi}_{j}\\hat{p}_{j,z}\\log\\Big(\\hat{p}_{j,z}\\big(\\sum_{z^{\\prime}\\in\\mathcal{S}_{j}}e^{(\\ell_{j,z^{\\prime}}-\\ell_{j,z})}+e^{(\\ell_{j,v}-\\ell_{j,z})}\\big)\\Big).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Using $\\mathcal{P}_{\\mathcal{F}}(\\widehat{W}_{B})=W^{\\star}$ and (4) yields $\\begin{array}{r}{\\sum_{z^{\\prime}\\in S_{j}}e^{(\\ell_{j,z^{\\prime}}-\\ell_{j,z})}=\\frac{1}{\\hat{p}_{j,z}}\\,.}\\end{array}$ . Moreover, by assumption of Case (ii): $e^{\\ell_{j,v}-\\ell_{j,z}}\\geq e^{-R^{\\prime}(1-\\delta)}\\,e^{\\ell_{j,v}^{\\star}-\\ell_{j,z}^{\\star}}\\geq c^{\\prime}e^{-R^{\\prime}(1-\\delta)}$ , for $c^{\\prime}:=e^{-\\|W^{\\star}\\|M}$ . Putting together yields: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{CE}(\\widehat{W}_{B})-\\mathcal{H}\\geq\\widehat{\\pi}_{j}\\widehat{p}_{j,z}\\log\\left(1+\\widehat{p}_{j,z}c^{\\prime}e^{-R^{\\prime}(1-\\delta)}\\right)\\geq c^{\\prime}e^{-R^{\\prime}(1-\\delta)}/2n^{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where the second inequality uses $\\textstyle\\log(1+x)\\geq{\\frac{x}{1+x}},x>0$ . Compare this with (9): For large enough $B$ , since $R,R^{\\prime}$ grow at the same rate, it holds $\\frac{c^{\\prime}}{2n^{2}}e^{-R^{\\prime}(1-\\delta)}>C e^{-R}$ . Thus, $\\mathrm{CE}(\\widehat{\\pmb{W}}_{B})>\\mathrm{CE}({\\pmb{W}}_{B}^{\\star})$ , a contradiction. In either case, we arrive at a contradiction, which completes the proof. \u53e3 ", "page_idx": 7}, {"type": "text", "text": "5 Gradient Descent ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "This section studies the implicit bias of GD. Denote the GD iterates at time $k$ by $W_{k}\\,=\\,W_{k-1}\\,-$ $\\eta\\nabla\\operatorname{CE}\\left(W_{k-1}\\right)$ for arbitrary initial point $W_{0}$ and constant step-size $\\eta>0$ small enough to guarantee descent. The first observation is that the norm of the GD iterates increases with iterations. ", "page_idx": 7}, {"type": "text", "text": "Lemma 2 (Norm growth). If training data are $N T P_{\\mathcal{H}}$ -compatible and NTP-separable, then $\\begin{array}{r}{\\operatorname*{lim}_{k\\rightarrow\\infty}\\mathrm{CE}(W_{k})\\stackrel{\\cdot}{=}\\mathcal{H}}\\end{array}$ and $\\operatorname*{lim}_{k\\to\\infty}\\left\\|W_{k}\\right\\|=\\infty$ . ", "page_idx": 7}, {"type": "text", "text": "This is intuitive because the CE loss is convex in $W$ (thus, GD approaches the objective\u2019s infimum $\\mathcal{H}_{\\;}$ ), and, in view of Proposition 1, the CE loss at all finite $W$ is bounded away from $\\mathcal{H}$ . The relevant question then becomes that of determining the limit of the direction of the GD iterates. ", "page_idx": 7}, {"type": "text", "text": "Theorem 2 (Implicit bias of GD). Assume $N T P_{\\mathcal{H}}$ -compatible and NTP-separable training data $\\tau_{m}$ .   \nThen, it holds that limk\u2192\u221e\u27e8\u2225WWkk\u2225,\u2225WW  mm\u2225 . Moreover, $\\begin{array}{r}{\\operatorname*{lim}_{k\\rightarrow\\infty}\\mathcal{P}_{\\mathcal{F}}(W_{k})=W^{\\star}}\\end{array}$ . ", "page_idx": 7}, {"type": "text", "text": "The theorem establishes 8 that in the limit of iterations: $W_{k}\\approx W^{\\star}+\\|\\mathcal{P}_{\\perp}(W_{k})\\|\\overline{{W^{\\mathrm{mm}}}}$ , which is analogous to the result we obtained previously for the regularization path. Although its proof is more involved compared to the proof of Thm. 1, the proof of its main ingredient (Lem. 5 in the appendix) is conceptually similar: It involves comparing the loss $\\mathrm{CE}(W_{k})$ for large iterations $k$ to the loss evaluated at a \u201cgenie\u201d point that is chosen so that: (i) On the subspace $\\mathcal{F}$ , it agrees with $W_{k}$ . This is because it is easy to show that $\\mathcal{P}_{\\mathcal{F}}(W_{k})$ converges to $W^{\\star}$ by standard gradient descent analysis for convex functions; (ii) On the orthogonal subspace $\\mathfrak{F}^{\\perp}$ , it follows the optimal (with respect to accelerating loss decrease) max-margin direction $\\overline{{W^{\\mathrm{mm}}}}\\in\\mathcal{F}^{\\perp}$ . To establish the loss comparison, the ideas is to compare the values of the adjusted loss $\\operatorname{CE}_{\\perp}(W):=\\operatorname{CE}(W)-\\operatorname{CE}\\left({\\mathcal{P}}_{\\mathcal{F}}(W)\\right)$ . ", "page_idx": 7}, {"type": "text", "text": "We validate our analysis with experiments on synthetic data in App. A. For illustration, Fig. 1 shows a 2D setting with $m=3$ distinct contexts, each followed by $S_{j}=3$ tokens/words out of total $V=5$ words in the vocabulary. The left subfigure illustrates: (i) In black markers, the context-embedding geometry along with the associated support sets for each context A, B, and C. (ii) In colored markers, the geometry of word-embeddings, that is the max-NTP-margin vectors $(W^{\\mathrm{mm}})^{\\top}e_{v},v\\,\\in\\,[5$ ], to which GD directionally converges. See caption for interpretation and Fig. 2 in the App. for vis. of the finite component of word-embeddings on the subspace $\\mathcal{F}$ . The right subfigure shows results of GD training with respect to training loss, norm growth, alignment with $W^{\\mathrm{mm}}$ , and convergence to $W^{\\star}$ on $\\mathcal{F}$ . See App. A for further implementation details and additional experiments. ", "page_idx": 7}, {"type": "text", "text": "6 Related work ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We build on the literature on implicit optimization bias of CE loss in one-hot supervised classification. [76] show that for linear models and linearly-separable data, GD converges in direction to the maxmargin classifier. This result strengthens [68] that showed the regularization path of CE minimization converges to the same limit. Closer to us, [34, 37] extend the analysis to encompass general binary data as follows: the data are linearly separable only on a certain subspace, and they show that GD converges, in direction, towards the max-margin classifier confined within that subspace. On the orthogonal subspace, it converges to a finite point. While operationally similar, Thms. 1, 2 cannot be directly derived from theirs since our setting is neither binary nor one-hot. Nevertheless, our proofs extend the foundational work of [68, 34, 37], akin to numerous other studies that explore extensions to nonlinear architectures[50, 35, 28, 29, 83, 89], and to stochastic and adaptive algorithms [60, 64, 21, 47, 77, 3, 14, 2]. The implicit bias viewpoint has also created opportunities to study generalization in overparameterized settings. [31, 4, 57, 22] build a two-stage approach initially leveraging implicit bias to simplify the complexities of optimization before addressing generalization. This narrows the generalization question to the properties of the corresponding max-margin classifier [58, 13, 43, 78, 23, 100, 72, 94]. The same strategy has also been adopted to study model robustness to adversarial perturbations [33, 80, 16], out-of-distribution data [87], and imbalances [69, 15, 42]. Our results motivate such extensions in the richer NTP setting. ", "page_idx": 7}, {"type": "image", "img_path": "xSziO6gQgG/tmp/71c3e8b6d29635970b00638d0d29e80b9d8df8f34b31cf264a495a94b778afb6.jpg", "img_caption": ["Figure 1: Vis. of NTP implicit optimization bias in a setting with $m=3$ distinct contexts, embedding dimension $d=2$ , vocabulary of $|\\gamma|=5$ words and support sets of length $\\left|S_{j}\\right|=3,j\\in[3$ 3]. Left: Vis. of context embeddings $\\bar{h}_{j}$ in circle black markers (marked as A,B,C) and of their associated support sets ${\\mathbf{}}S_{j}$ (colored text below each marker). Colored vectors (star markers) represent max-NTP-margin vectors $\\pmb{w}_{v}^{\\top}:=\\pmb{e}_{v}^{\\top}\\pmb{W}^{\\mathrm{mm}}$ , $v\\in[5]$ found by GD. Interpreting decoder vectors as word embeddings leads to intuitive findings on their geometry learned by NTP training. E.g., word embedding $\\pmb{w}_{3}$ (almost) aligns with context-embedding $A$ and the normal hyperplane it defines separates $A$ from $B$ and $C$ , since word 3 only appears after context $A$ . The rest of the words follow two contexts each and their word-representation naturally belongs to the cone defined by the embeddings of those respective contexts. The wider the cone, the larger the magnitude of the word embedding to compensate for the large angle between context-representations that share the same next-word. Note that geometry of depicted word embeddings only depends on support sets, but the conditional probabilities define another set of word representations on an orthogonal (matrix) subspace; see text for details and vis. Right: Upper/lower graphs confirm the predictions of Lemma 2 and of Theorem 2, respectively. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Recent work [49] also studies forms of implicit bias for language models trained to reach the risk lower bound. However, they assume training with population loss and analyze implicit bias through Hessian-trace minimization without providing explicit parameter characterizations as in Thm. 2. Crucially, their results do not apply to CE loss9 or to sparse support-sets. Another interesting work [52] studies learning abilities of autoregressive training and inference. However, their findings do not apply to NTP as they inherently assume each context is followed by a unique next token. ", "page_idx": 8}, {"type": "text", "text": "Finally, although stemming from different perspectives, the form of our convergence results echoes a recent conjecture by [82] regarding implicit optimization bias in transformers. Unlike their conjecture, which focuses on binary classification, our results are rigorously proven and apply to the NTP setting. Further detailed discussion on related follow-up work on implicit optimization bias in self-attention architectures, as initiated by [83], is deferred to Appendix B. In contrast to this line of work, we here focus on the optimization biases of the NTP training-paradigm itself, which is orthogonal to the intricacies of the specific architecture generating the context embeddings. ", "page_idx": 8}, {"type": "text", "text": "7 Conclusion, limitations and future work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Towards characterizing implicit regularization effects, we highlight two key aspects of NTP training: (i) Formulating it as CE optimization over distinct contexts; this is long recognized in language modeling (e.g., [44, 63]) since Shannon\u2019s initial work, yet seemingly overlooked in recent studies, such as [49, 52]. (ii) Accounting for sparsity in the matrix of next-token conditional probabilities. While traditional language modeling techniques often mitigate sparsity using smoothing heuristics that assign non-zero probabilities to unobserved next tokens [44, 63, 39], we recognize sparsity as a critical factor in NTP optimization that influences parameter divergence10. ", "page_idx": 9}, {"type": "text", "text": "As the first study of implicit biases in NTP training, our results are based on several assumptions essential for establishing an initial foundational understanding. The framework allows for various exciting promising research directions, some of which we outline below. ", "page_idx": 9}, {"type": "text", "text": "Even within the assumed linear setting and GD, interesting directions involve: ", "page_idx": 9}, {"type": "text", "text": "\u25cfNTP-separability thresholds: Identifying exact thresholds for NTP-separability under distributional assumptions, akin to previous work on one-hot separability (Remark 1). However, relaxing the overparameterization requirement that the embedding dimension $d$ be proportional to the number of distinct contexts $m$ would necessitate exploring non-convex architectures (see \u2019Memory capacity\u2019 below). ", "page_idx": 9}, {"type": "text", "text": "\u25cfGeneralization: Studying generalization in NTP settings by examining statistical properties of the NTP-SVM solution. Past research has successfully undertaken similar investigations for one-hot classification (see Sec. 6). While we acknowledge the importance of addressing specific challenges inherent to NTP \u2014such as determining an appropriate measure of generalization, or establishing suitable statistical models for context-embeddings that respect the discrete nature of the underlying token subsequences\u2014we believe this direction holds promise for further exploration. ", "page_idx": 9}, {"type": "text", "text": "In addition to these, essential extensions include relaxing the linearity assumption. ", "page_idx": 9}, {"type": "text", "text": "\u25cfArchitecture-specific embeddings: A bottom-up approach considering architecture-specific embeddings could begin by modeling the embeddings produced by, for instance, a shallow transformer and analyzing the effects of optimization biases on the training of both the transformer and the decoder weights. This complements the works of [83, 82], who investigate one-layer self-attention with a fixed decoder. A challenge in this approach is balancing the restriction to shallow transformers (for analytical tractability) with ensuring that the NTP loss reaches the entropy lower bound. This may require constraining the training data distribution, for example, to a Markov chain [51, 25]. ", "page_idx": 9}, {"type": "text", "text": "\u25cfMemory capacity in NTP settings: Without imposing further restrictions on the data beyond the discrete nature of tokens from a finite vocabulary, there is a strong case for investigating the memory capacity of sequence-to-sequence architectures, such as transformers, in the context of NTP. Recent studies on transformer memory capacity [40, 41] do not apply here. ", "page_idx": 9}, {"type": "text", "text": "\u25cfUnconstrained features: Extending the top-down approach, one could consider freely optimizing context embeddings together with decoder vectors (also known as word embeddings). The resulting log-bilinear model, reminiscent of wor2vec models [63, 55], extends the unconstrained features model, which has recently been employed to investigate neural collapse geometry in one-hot classification settings [56]. This idea offers a promising avenue for uncovering structures in the geometries of context and word embeddings when learned jointly, potentially revealing new insights into the capabilities of sufficiently expressive language models (see Fig. 1 for cases involving only the latter). ", "page_idx": 9}, {"type": "text", "text": "\u25cfOther optimizers: Exploring the NTP implicit bias of adaptive algorithms, such as Adam, potentially building on recent works in this area focused on one-hot classification [96, 95]. ", "page_idx": 9}, {"type": "text", "text": "We hope this work inspires further research in the discussed directions, contributing to a deeper understanding of the intricacies involved and potentially yielding improvements in NTP training. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Thank you to Tina Behnia, Yize Zhao, Vala Vakilian, and Puneesh Deora for inspiring discussions that contributed to this work and for their valuable suggestions on the manuscript. I am also grateful to Gautam Goel for his careful reading and for pointing out several typos. Thanks to the anonymous reviewers for their feedback. This work is supported by the NSERC Discovery Grant No. 2021-03677, the Alliance Grant ALLRP 581098-22, NFRFE-2023-00936, and a CIFAR AI Catalyst Grant. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Ekin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview. net/forum?id=0g0X4H8yN4I. [2] Maksym Andriushchenko and Nicolas Flammarion. Towards understanding sharpness-aware minimization. In International Conference on Machine Learning, pages 639\u2013668. PMLR, 2022. [3] Navid Azizan, Sahin Lale, and Babak Hassibi. Stochastic mirror descent on overparameterized nonlinear models. IEEE Transactions on Neural Networks and Learning Systems, 33(12): 7717\u20137727, 2021. [4] Peter L Bartlett, Philip M Long, G\u00e1bor Lugosi, and Alexander Tsigler. Benign overfitting in linear regression. arXiv preprint arXiv:1906.11300, 2019. [5] Maximilian Beck, Korbinian P\u00f6ppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, G\u00fcnter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. xlstm: Extended long short-term memory. arXiv preprint arXiv:2405.04517, 2024. [6] Mikhail Belkin. The necessity of machine learning theory in mitigating ai risk. ACM/JMS Journal of Data Science, 2024. [7] Mikhail Belkin, Alexander Rakhlin, and Alexandre B Tsybakov. Does data interpolation contradict statistical optimality? arXiv preprint arXiv:1806.09471, 2018. [8] Samy Bengio and Yoshua Bengio. Taking on the curse of dimensionality in joint distributions using neural networks. IEEE Transactions on Neural Networks, 11(3):550\u2013557, 2000. [9] Yoshua Bengio, R\u00e9jean Ducharme, and Pascal Vincent. A neural probabilistic language model. Advances in neural information processing systems, 13, 2000.   \n[10] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.   \n[11] Burak \u00c7akmak, Yue M Lu, and Manfred Opper. A convergence analysis of approximate message passing with non-separable functions and applications to multi-class classification. arXiv preprint arXiv:2402.08676, 2024.   \n[12] Emmanuel J Cand\u00e8s and Pragya Sur. The phase transition for the existence of the maximum likelihood estimate in high-dimensional logistic regression. arXiv preprint arXiv:1804.09753, 2018.   \n[13] Yuan Cao, Quanquan Gu, and Mikhail Belkin. Risk bounds for over-parameterized maximum margin classification on sub-gaussian mixtures. Advances in Neural Information Processing Systems, 34:8407\u20138418, 2021.   \n[14] Matias D Cattaneo, Jason M Klusowski, and Boris Shigida. On the implicit bias of adam. arXiv preprint arXiv:2309.00079, 2023.   \n[15] Niladri S Chatterji, Philip M Long, and Peter L Bartlett. When does gradient descent with logistic loss find interpolating two-layer networks? The Journal of Machine Learning Research, 22(1):7135\u20137182, 2021.   \n[16] Jinghui Chen, Yuan Cao, and Quanquan Gu. Benign overfitting in adversarially robust linear classification. In Uncertainty in Artificial Intelligence, pages 313\u2013323. PMLR, 2023.   \n[17] Sitan Chen and Yuanzhi Li. Provably learning a multi-head attention layer. arXiv preprint arXiv:2402.04084, 2024.   \n[18] Katherine M Collins, Umang Bhatt, and Adrian Weller. Eliciting and learning with soft labels from every annotator. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, volume 10, pages 40\u201352, 2022.   \n[19] Elisabetta Cornacchia, Francesca Mignacco, Rodrigo Veiga, C\u00e9dric Gerbelot, Bruno Loureiro, and Lenka Zdeborov\u00e1. Learning curves for the multi-class teacher\u2013student perceptron. Machine Learning: Science and Technology, 4(1):015019, 2023.   \n[20] Thomas M Cover. Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition. IEEE transactions on electronic computers, pages 326\u2013334, 1965.   \n[21] Alex Damian, Tengyu Ma, and Jason D Lee. Label noise sgd provably prefers flat global minimizers. Advances in Neural Information Processing Systems, 34:27449\u201327461, 2021.   \n[22] Zeyu Deng, Abla Kammoun, and Christos Thrampoulidis. A model of double descent for high-dimensional binary linear classification. Information and Inference: A Journal of the IMA, 11(2):435\u2013495, 2022.   \n[23] Konstantin Donhauser, Nicolo Ruggeri, Stefan Stojanovic, and Fanny Yang. Fast rates for noisy interpolation require rethinking the effect of inductive bias. In International Conference on Machine Learning, pages 5397\u20135428. PMLR, 2022.   \n[24] Benjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and variable creation in self-attention mechanisms. arXiv preprint arXiv:2110.10090, 2021.   \n[25] Benjamin L Edelman, Ezra Edelman, Surbhi Goel, Eran Malach, and Nikolaos Tsilivis. The evolution of statistical induction heads: In-context learning markov chains. arXiv e-prints, pages arXiv\u20132402, 2024.   \n[26] Daniel Y Fu, Tri Dao, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher R\u00e9. Hungry hungry hippos: Towards language modeling with state space models. arXiv preprint arXiv:2212.14052, 2022.   \n[27] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023.   \n[28] Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in terms of optimization geometry. In International Conference on Machine Learning, pages 1832\u20131841. PMLR, 2018.   \n[29] Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent on linear convolutional networks. Advances in Neural Information Processing Systems, 31:9461\u20139471, 2018.   \n[30] Peter E Hart, David G Stork, and Richard O Duda. Pattern classification. Wiley Hoboken, 2000.   \n[31] Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in high-dimensional ridgeless least squares interpolation. arXiv preprint arXiv:1903.08560, 2019.   \n[32] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.   \n[33] Adel Javanmard and Mahdi Soltanolkotabi. Precise statistical analysis of classification accuracies for adversarial training. The Annals of Statistics, 50(4):2127\u20132156, 2022.   \n[34] Ziwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic regression. arXiv preprint arXiv:1803.07300, 2018.   \n[35] Ziwei Ji and Matus Telgarsky. Directional convergence and alignment in deep learning. Advances in Neural Information Processing Systems, 33:17176\u201317186, 2020.   \n[36] Ziwei Ji and Matus Telgarsky. Characterizing the implicit bias via a primal-dual analysis. In Algorithmic Learning Theory, pages 772\u2013804. PMLR, 2021.   \n[37] Ziwei Ji, Miroslav Dud\u00edk, Robert E Schapire, and Matus Telgarsky. Gradient descent follows the regularization path for general losses. In Conference on Learning Theory, pages 2109\u20132136. PMLR, 2020.   \n[38] Ziwei Ji, Nathan Srebro, and Matus Telgarsky. Fast margin maximization via dual acceleration. In International Conference on Machine Learning, pages 4860\u20134869. PMLR, 2021.   \n[39] Daniel Jurafsky and James H. Martin. Speech and Language Processing. Draft, 3 edition, 2023. URL https://web.stanford.edu/\\~jurafsky/slp3/ed3book.pdf.   \n[40] Tokio Kajitsuka and Issei Sato. Are transformers with one layer self-attention using low-rank weight matrices universal approximators? 2024.   \n[41] Junghwan Kim, Michelle Kim, and Barzan Mozafari. Provable memorization capacity of transformers. 2023.   \n[42] Ganesh Ramachandra Kini, Orestis Paraskevas, Samet Oymak, and Christos Thrampoulidis. Label-imbalanced and group-sensitive classification under overparameterization. Advances in Neural Information Processing Systems, 34:18970\u201318983, 2021.   \n[43] Frederic Koehler, Lijia Zhou, Danica J Sutherland, and Nathan Srebro. Uniform convergence of interpolators: Gaussian width, norm bounds and benign overfitting. Advances in Neural Information Processing Systems, 34:20657\u201320668, 2021.   \n[44] Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. Advances in neural information processing systems, 27, 2014.   \n[45] Yingcong Li, M. Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms: Generalization and stability in in-context learning, 2023.   \n[46] Yingcong Li, Yixiao Huang, Muhammed E Ildiz, Ankit Singh Rawat, and Samet Oymak. Mechanics of next token prediction with self-attention. In International Conference on Artificial Intelligence and Statistics, pages 685\u2013693. PMLR, 2024.   \n[47] Zhiyuan Li, Tianhao Wang, and Sanjeev Arora. What happens after sgd reaches zero loss?\u2013a mathematical framework. arXiv preprint arXiv:2110.06914, 2021.   \n[48] Valerii Likhosherstov, Krzysztof Choromanski, and Adrian Weller. On the expressive power of self-attention matrices, 2021.   \n[49] Hong Liu, Sang Michael Xie, Zhiyuan Li, and Tengyu Ma. Same pre-training loss, better downstream: Implicit bias matters for language models. In International Conference on Machine Learning, pages 22188\u201322214. PMLR, 2023.   \n[50] Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks. In International Conference on Learning Representations, 2020.   \n[51] Ashok Vardhan Makkuva, Marco Bondaschi, Adway Girish, Alliot Nagle, Martin Jaggi, Hyeji Kim, and Michael Gastpar. Attention with markov: A framework for principled analysis of transformers via markov chains. arXiv preprint arXiv:2402.04161, 2024.   \n[52] Eran Malach. Auto-regressive next-token predictors are universal learners. arXiv preprint arXiv:2309.06979, 2023.   \n[53] William Merrill, Vivek Ramanujan, Yoav Goldberg, Roy Schwartz, and Noah A Smith. Effects of parameter norm growth during transformer training: Inductive bias from gradient descent. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1766\u20131781, 2021.   \n[54] Francesca Mignacco, Florent Krzakala, Yue M Lu, and Lenka Zdeborov\u00e1. The role of regularization in classification of high-dimensional noisy gaussian mixture. arXiv preprint arXiv:2002.11544, 2020.   \n[55] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.   \n[56] Dustin G Mixon, Hans Parshall, and Jianzong Pi. Neural collapse with unconstrained features. arXiv preprint arXiv:2011.11619, 2020.   \n[57] Andrea Montanari, Feng Ruan, Youngtak Sohn, and Jun Yan. The generalization error of max-margin linear classifiers: High-dimensional asymptotics in the overparametrized regime. arXiv preprint arXiv:1911.01544, 2019.   \n[58] Vidya Muthukumar, Adhyyan Narang, Vignesh Subramanian, Mikhail Belkin, Daniel Hsu, and Anant Sahai. Classification vs regression in overparameterized regimes: Does the loss function matter? arXiv preprint arXiv:2005.08054, 2020.   \n[59] Mor Shpigel Nacson, Nathan Srebro, and Daniel Soudry. Stochastic gradient descent on separable data: Exact convergence with a fixed learning rate. arXiv preprint arXiv:1806.01796, 2018.   \n[60] Mor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Pedro Henrique Pamplona Savarese, Nathan Srebro, and Daniel Soudry. Convergence of gradient descent on separable data. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 3420\u20133428. PMLR, 2019.   \n[61] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.   \n[62] OpenAI. Openai: Introducing chatgpt, 2022. URL https://openai.com/blog/chatgpt, 2022.   \n[63] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532\u20131543, 2014.   \n[64] Scott Pesme, Loucas Pillaud-Vivien, and Nicolas Flammarion. Implicit bias of sgd for diagonal linear networks: a provable benefti of stochasticity. Advances in Neural Information Processing Systems, 34:29218\u201329230, 2021.   \n[65] Joshua C Peterson, Ruairidh M Battleday, Thomas L Grifftihs, and Olga Russakovsky. Human uncertainty makes classification more robust. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9617\u20139626, 2019.   \n[66] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. OpenAI blog, 2018.   \n[67] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \n[68] Saharon Rosset, Ji Zhu, and Trevor J. Hastie. Margin maximizing loss functions. In NIPS, 2003.   \n[69] Shiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. An investigation of why overparameterization exacerbates spurious correlations. In International Conference on Machine Learning, pages 8346\u20138356. PMLR, 2020.   \n[70] Arda Sahiner, Tolga Ergen, Batu Ozturkler, John Pauly, Morteza Mardani, and Mert Pilanci. Unraveling attention via convex duality: Analysis and interpretations of vision transformers. International Conference on Machine Learning, 2022.   \n[71] Fariborz Salehi, Ehsan Abbasi, and Babak Hassibi. A precise analysis of phasemax in phase retrieval. In 2018 IEEE International Symposium on Information Theory (ISIT), pages 976\u2013980. IEEE, 2018.   \n[72] Ohad Shamir. The implicit bias of benign overfitting. In Conference on Learning Theory, pages 448\u2013478. PMLR, 2022.   \n[73] Claude E Shannon. Prediction and entropy of printed english. Bell system technical journal, 30(1):50\u201364, 1951.   \n[74] Claude Elwood Shannon. A mathematical theory of communication. The Bell system technical journal, 27(3):379\u2013423, 1948.   \n[75] Viktoriia Sharmanska, Daniel Hern\u00e1ndez-Lobato, Jose Miguel Hernandez-Lobato, and Novi Quadrianto. Ambiguity helps: Classification with disagreements in crowdsourced annotations. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2194\u20132202, 2016.   \n[76] Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):2822\u20132878, 2018.   \n[77] Haoyuan Sun, Kwangjun Ahn, Christos Thrampoulidis, and Navid Azizan. Mirror descent maximizes generalized margin and can be implemented efficiently. Advances in Neural Information Processing Systems, 35:31089\u201331101, 2022.   \n[78] Pragya Sur and Emmanuel J Cand\u00e8s. A modern maximum-likelihood theory for highdimensional logistic regression. Proceedings of the National Academy of Sciences, page 201810420, 2019.   \n[79] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2818\u20132826, 2016.   \n[80] Hossein Taheri, Ramtin Pedarsani, and Christos Thrampoulidis. Asymptotic behavior of adversarial training in binary linear classification. IEEE Transactions on Neural Networks and Learning Systems, 2023.   \n[81] Kai Tan and Pierre C Bellec. Multinomial logistic regression: Asymptotic normality on null covariates in high-dimensions. arXiv preprint arXiv:2305.17825, 2023.   \n[82] Davoud Ataee Tarzanagh, Yingcong Li, Christos Thrampoulidis, and Samet Oymak. Transformers as support vector machines, 2023.   \n[83] Davoud Ataee Tarzanagh, Yingcong Li, Xuechen Zhang, and Samet Oymak. Max-margin token selection in attention mechanism, 2023.   \n[84] Yuandong Tian, Yiping Wang, Beidi Chen, and Simon Du. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer, 2023.   \n[85] Yuandong Tian, Yiping Wang, Zhenyu Zhang, Beidi Chen, and Simon Du. Joma: Demystifying multilayer transformers via joint dynamics of mlp and attention. arXiv preprint arXiv:2310.00535, 2023.   \n[86] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[87] Nilesh Tripuraneni, Ben Adlam, and Jeffrey Pennington. Overparameterization improves robustness to covariate shift in high dimensions. Advances in Neural Information Processing Systems, 34:13883\u201313897, 2021.   \n[88] Vladimir N Vapnik and Alexey Ya Chervonenkis. A note on one class of perceptrons. Automation and Remote Control, 25:774\u2013780, 1964.   \n[89] Bhavya Vasudeva, Puneesh Deora, and Christos Thrampoulidis. Implicit bias and fast convergence rates for self-attention. arXiv preprint arXiv:2402.05738, 2024.   \n[90] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[91] Sharan Vaswani, Francis Bach, and Mark Schmidt. Fast and faster convergence of sgd for overparameterized models and an accelerated perceptron. In The 22nd international conference on artificial intelligence and statistics, pages 1195\u20131204. PMLR, 2019.   \n[92] R. Vershynin. Lectures in geometric functional analysis. Unpublished manuscript. Available at http://www-personal. umich. edu/romanv/papers/GFA-book/GFA-book. pdf, 2011.   \n[93] Johannes von Oswald, Eyvind Niklasson, E. Randazzo, Jo\u00e3o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. ArXiv, abs/2212.07677, 2022.   \n[94] David Wu and Anant Sahai. Precise asymptotic generalization for multiclass classification with overparameterized linear models. Advances in Neural Information Processing Systems, 36, 2024.   \n[95] Shuo Xie and Zhiyuan Li. Implicit bias of adamw: /ell_/infty norm constrained optimization. arXiv preprint arXiv:2404.04454, 2024.   \n[96] Chenyang Zhang, Difan Zou, and Yuan Cao. The implicit bias of adam on separable data. arXiv preprint arXiv:2406.10650, 2024.   \n[97] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization, 2017.   \n[98] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.   \n[99] Ruiqi Zhang, Spencer Frei, and Peter L. Bartlett. Trained transformers learn linear models in-context, 2023.   \n[100] Lijia Zhou, Danica J Sutherland, and Nati Srebro. On uniform convergence and low-norm interpolation learning. Advances in Neural Information Processing Systems, 33:6867\u20136877, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "image", "img_path": "xSziO6gQgG/tmp/4aaa78eb00b55467f38702125f2be8a145c54540d449c054cb68aae9a270f276.jpg", "img_caption": ["Figure 2: Same setup as Fig. 1. Left: Matrix $_{P}$ of conditional probabilities of words (cols.) per context (rows). Each row corresponds to the conditional probability vectors $\\mathbf{\\displaystyle{p}_{j},}j\\mathbf{\\displaystyle{\\epsilon}}\\left[m\\right]$ . Black entries correspond to off-support words. Middle: Shown as $\\begin{array}{r}{w_{z},z\\in[}\\end{array}$ [5], the rows of the NTP-SVM solution $W^{\\mathrm{mm}}$ to which GD directionally converges. Right: Shown as $w_{z},z\\in$ [5], the rows of the finite parameter $W^{\\star}$ to which GD iterates projected on $\\mathcal{F}$ converge to. The geometry of $W^{\\mathrm{mm}}$ depends only on the support-set of $_{P}$ . On the other hand, the geometry of $W^{\\star}$ depends on the entries of $_{P}$ for in-support tokens/words. As seen from visualization of $_{P}$ , the words 1 and 5 have the same support pattern (i.e., both follow the same contexts $A$ and $B$ ). Thus, ${\\pmb w}_{1}\\,=\\,{\\pmb w}_{5}$ in the Middle plot. However, on the subspace $\\mathcal{F}$ corresponding to the Right plot, $\\pmb{w}_{1}\\neq\\pmb{w}_{5}$ , which allows matching the different conditional probabilities with which each follows contexts $A$ and $B$ . "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "A Experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "All experiments were conducted on a MacBook Pro equipped with a $2.3\\:\\mathrm{GHz}$ Quad-Core Intel Core i7 processor and $32\\,\\mathrm{GB}$ of memory. The experiments are of relatively small scale and were implemented in Matlab. The code is straightforward to reproduce, following the detailed specifications provided in the subsequent sections. For completeness, the code will be made publicly available on Github in the final version of the paper. ", "page_idx": 16}, {"type": "text", "text": "A.1 Additional details on 2D example of Fig. 1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Figure 1 illustrates a toy 2d example where the embeddings and the hyperplanes defined by each row of $W^{\\mathrm{mm}}$ can be visualized. We used $d=2,m=3,V=5$ and $S_{1}=S_{2}=S_{3}=3$ . The support sets of each embedding are shown in the figure color-coded to match the respective decoder hyperplane. Probabilities are assigned randomly. The empirical conditional entropy evaluates to $\\mathcal{H}=0.8811$ and the matrix of conditional probabilities is visualized in Figure 2. In the same figure, we also visualize the rows of the directional component $W^{\\mathrm{mm}}$ (Middle) and of the finite component $W^{\\star}$ (Right). Interpreting the $V\\times d$ decoder matrix as the matrix of learned word embeddings, this provides a visualization of their geometry. As per our results, the two word-embedding matrices $W^{\\star}$ and $W^{\\mathrm{mm}}$ lie on orthogonal subspaces. The geometry of the first depends on the probabilities of in-support tokens, while that of the second depends only on the support set of these probabilities. See also caption of Fig. 2. ", "page_idx": 16}, {"type": "text", "text": "A.2 Overparameterized setting ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We examine the implicit bias of GD on NTP training with overparameterization on synthetic data generated as follows. We construct dataset with $n\\,=\\,5000$ sequences involving $m\\,=\\,50$ distinct contexts. Each distinct context gets mapped to a randomly generated embedding of dimension $d=60>m$ . We set vocabulary size $V=10$ and each context $j\\in[m]$ is followed by $\\bar{S_{j}}=6,\\forall j\\in[m]$ possible next-tokens. The support sets $S_{j}\\subset\\mathcal{V}$ and the probabilities $\\hat{p}_{j,z},z\\in S_{j}$ are chosen randomly; see Fig. 3 for representative examples from the training dataset. For a fixed realization of the dataset (for which $\\mathcal{H}\\approx1.445$ nats), we run GD, normalized GD (NGD), and Adam from random LeCun initialization. For GD, we use learning rate $\\eta=0.5$ and for NGD and Adam $\\eta=0.01$ . For Adam, we also set $\\beta_{1}=0.9,\\beta_{2}=0.99$ . We run all algorithms for $1e4$ iterations. For each case, we plot the following as a function of iterations: ", "page_idx": 16}, {"type": "text", "text": "1. Upper Left: CE loss versus entropy lower bound   \n2. Upper Right: parameter norm growth ", "page_idx": 16}, {"type": "text", "text": "3. Lower Left: correlation of $W^{\\mathrm{mm}}$ with iterates $W_{k}$ and of \u201ccorrected\u201d iterates $W_{k}-W^{\\star}$ after substracting the component on $\\mathcal{H}$ ", "page_idx": 17}, {"type": "text", "text": "Fig. 4 shows an instance of these. As predicted by our analysis, in this overparameterized setting: CE loss converges to its lower-bound, parameter norm increases, iterates align in direction with $W^{\\mathrm{mm}}$ , and the subspace component converges to $W^{\\star}$ . ", "page_idx": 17}, {"type": "image", "img_path": "xSziO6gQgG/tmp/01484b710058bfb699d75d8a406ea3b23b5e616905922bc3d5129a4e65997ee9.jpg", "img_caption": ["Figure 3: Eight randomly picked contexts with their associated next-token empirical conditional probabilities $\\hat{\\pmb{p}}_{j}$ . The indices shown on the $\\mathbf{X}$ -axis define the support set ${\\mathcal{S}}_{j}$ of each context. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 5 illustrates the same plots, but this time for training over the same dataset with NGD and Adam. We observe same implicit bias, but faster convergence. For NGD, this is consistent with analogous findings (rigorous in that case) for one-hot classification [60, 36]. ", "page_idx": 17}, {"type": "image", "img_path": "xSziO6gQgG/tmp/41c2acd5ee205f584c74827b95d8006f64dc6e577c84d01c55a997296cadcbdf.jpg", "img_caption": ["Figure 4: Experimental illustration of the implicit bias of GD in NTP over synthetic data with overparameterization. See App. A for detailed description of the experimental setting. The upper two graphs confirm the predictions of Lemma 2, while the lower two graphs adhere to the predictions of Theorem 2. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "xSziO6gQgG/tmp/59b311d82c6a56ebb56e444afb287afcaa2456b04fa14a11e3ca9e4d2c26f50d.jpg", "img_caption": ["Figure 5: Implicit bias of normalized GD (Left) and of Adam (Right) in NTP over synthetic data with overparameterization. Both exhibit the same implicit bias, but converge faster than GD, with Adam being slightly faster than NGD. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "B Additional related work ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Implicit bias in transformers. As already mentioned in Sec. 6, our work is closely related to [82], where the authors investigate the implicit bias of self-attention in transformers. The insight put forth in the prequel [83] is that softmax attention induces implicit-bias behaviors that bear similarities to vanilla implicit bias of one-hot prediction. Concretely, [82] studies GD optimization of one-layer self-attention with fixed decoder and one-hot binary classification. They show that, in the limit, GD finds attention weights that converge in direction to the solution of an SVM problem that separates optimal tokens from non-optimal ones. Their non-convex setting introduces locally optimal SVM directions to which GD may converge depending on initialization. Different to them, the NTP setting that we study involves predictions over multiple categories and is not one-hot. Also, while they fix the decoder, here, we fix the embeddings. In these respects their results are rather different. More similarities arise when [82] replace the linear decoder with a MLP, which they note can induce multiple optimal tokens per sequence. This leads them to formulate a more general token-separating SVM program, which similar to ours confines the separation on a certain data subspace. However, the operational nature of the programs remains different as theirs optimizes attention weights and separates tokens within a sequence, while ours optimizes decoder weights and separates context embeddings based on their respective support sets. More importantly, while [82] only conjectures the convergence of GD to their general SVM program, we leverage convexity in our setting to prove an analogous statement rigorously. Eventually, as we move lower in our top-down approach and consider architecture-specific embeddings generated by attention, we anticipate to see integration of our ideas with theirs. ", "page_idx": 18}, {"type": "text", "text": "Beyond [82], there is growing recent research investigating optimization and generalization principles of transformers, e.g., [70, 24, 48, 93, 99, 1, 45, 83, 82, 84, 17]. These efforts predominantly employ a \u2018bottom-up\u2019 approach that involves isolating shallow transformers, often with simplifications such as removing MLPs, utilizing single heads instead of multiple, and fixing certain parts while training only a subset of trainable parameters. Most of these studies have focused on classical one-hot supervised settings, and only a handful (e.g., [84, 85]) have seeked extending these \u2019bottom-up\u2019 analyses to NTP settings. Yet, their primary emphasis remains on uncovering the role of attention and how attention weights evolve during training. Instead, our approach uniquely emphasizes the NTP training paradigm itself, shifting the focus from the intricacies of specific transformer architectures. ", "page_idx": 18}, {"type": "text", "text": "Upon completing this paper, we became aware of independent contemporaneous research by Li et al. [46] that also examines the implicit bias of self-attention with a fixed linear decoder in next-token prediction scenarios. Unlike our study which utilizes the widely adopted CE loss, their approach is based on log-loss, which renders the training loss convex, a similarity shared with our model despite the inclusion of self-attention. Both our results and those of Li et al. substantiate the conjecture posited by Tarzanagh and colleagues [82], albeit in very distinct settings. Notably, contrary to both [83] and [46], we unveil the optimization intricacies of the NTP paradigm, even within the simplest linear settings. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "Classification with soft labels. Unlike one-hot classification, soft-label classification associates each example with a probability vector, where each entry represents the likelihood of a corresponding label characterizing the example. Although arguably less prevalent than one-hot (or hard-label) classification, soft-label classification arises in various contexts, including modeling human confusion during crowd-sourcing [65, 75, 18], knowledge distillation [32], label smoothing [79], and mixup [98]. Our model of last-token prediction also falls within this setting. Specifically, our approach is most closely related to soft-labels generated by averaging annotators\u2019 hard labels [65], rather than following the winner-takes-all rule to assign labels. [65] and follow-up work have provided empirical evidence that using probabilistic soft labels generated from crowd annotations for training leads to improved performance in terms of model generalization, calibration, and robustness to out-of-distribution data. To the best of our knowledge, no prior work has investigated the implicit bias of gradient descent in this or other soft-label classification settings; thus, our results are of direct relevance to these contexts as well. ", "page_idx": 19}, {"type": "text", "text": "C Autoregressive setting ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "For concreteness and simplified notation, in the paper\u2019s main body we focus on NTP over sequences of fixed length. We show here that this encompasses the autoregressive (i.e., sequential) setting with minimal changes. This also emphasizes the role played in our results by the sequence length. ", "page_idx": 19}, {"type": "text", "text": "As pointed in (1), the full autoregressive NTP objective averages $T$ individual losses (without loss of generality assume sequences of equal maximum length $T$ ). In order to make our analysis applicable, we first need to express (1) in terms of unique contexts. Mirroring the notations in Sec. 2, define the following for $t\\in[T-1]$ : ", "page_idx": 19}, {"type": "text", "text": "\u2022 $m_{t},t\\in[T-1]$ is the number of distinct contexts of size $t$ . Note that $m_{1}\\geq m_{2}\\geq\\cdots\\geq m_{T-1}$ . \u2022 $\\begin{array}{r}{m=\\sum_{t=1}^{T-1}m_{t}}\\end{array}$ is the total number of distinct contexts in the dataset   \n\u2022 $\\bar{h}_{t,j}:=h_{\\theta}\\big(\\bar{x}_{j,t}\\big),t\\in[T-1],j\\in[m_{t}]$ is the embedding of the $j$ -th (among all $t$ -long contexts) distinct context $\\bar{\\pmb{x}}_{j,t}$ .   \n\u2022 $\\hat{\\pi}_{j,t}$ is the empirical probability of $\\bar{\\pmb{x}}_{j,t}$ .   \n\u2022 $\\hat{p}_{j,t,z}$ is the empirical probability that context $\\bar{\\pmb{x}}_{j,t}$ is followed by token $z\\in\\mathcal{V}$ .   \n\u2022 $\\boldsymbol{S_{j,t}}$ is the support set of the next-token distribution of context $\\bar{\\pmb{x}}_{j,t}$ . ", "page_idx": 19}, {"type": "text", "text": "With this notation, the NTP objective becomes ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{CE}=-\\sum_{\\substack{t\\in[T-1]\\,j\\in[m_{t}]}}\\hat{\\pi}_{t,j}\\sum_{z\\in S_{j,t}}\\hat{p}_{t,j,z}\\log\\left(\\mathbb{S}_{z}(\\pmb{W}\\bar{h}_{t,j})\\right)\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "To continue enumerate the multi-set ${\\mathcal{T}}:=\\left\\lbrace i=(j,t)\\,{\\big|}\\,t\\in[T-1],j\\in[m_{t}]\\right\\rbrace$ . We may then rewrite the above as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{CE}=-\\sum_{i\\in\\mathbb{Z}}\\hat{\\pi}_{i}\\sum_{z\\in S_{i}}\\hat{p}_{i,z}\\log\\left(\\mathbb{S}_{z}(W\\bar{h}_{i})\\right)\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "At this point note that this is of identical form to (2). Consequently, the definitions (e.g., NTPseparability, NTP-margin) and results derived in the main body for sequences of fixed length are applicable to the AR setting, extending mutatis mutandis. ", "page_idx": 19}, {"type": "text", "text": "Remark 2 (The role of sequence length.). Despite the above reduction of the AR setting to the fixed-length setting, it is crucial to recognize that sequence length remains a significant factor in the AR model. Specifically, it influences the formulation through support sets and their associated probabilities. As sequences extend in length, their corresponding support sets generally become sparser, indicative of less ambiguity in predicting the next token. This dynamic is captured by Shannon\u2019s inequality, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{H}_{t}\\geq\\mathcal{H}_{t+1},\\;w h e r e\\;\\mathcal{H}_{t}=-\\sum_{\\substack{j\\in[m_{t}]\\,z\\in S_{t,j}^{\\ell}}}\\pi_{t,j}\\hat{p}_{t,j,z}\\log(\\hat{p}_{t,j,z}),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "reflecting the incremental reduction in entropy as sequence length increases. ", "page_idx": 19}, {"type": "text", "text": "D Notations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Throughout, lowercase and uppercase bold letters (e.g., $\\textbf{\\em a}$ and $\\pmb{A}$ ) represent vectors and matrices, respectively. $\\langle\\cdot,\\cdot\\rangle$ and $\\left\\Vert\\cdot\\right\\Vert$ denote Euclidean inner product and norm, respectively. For matrix $\\pmb{A}$ , we denote its pseudoinverse as $A^{\\dagger}$ . All logarithms are natural logarithms (base $e$ ). We denote $e_{v}$ the $v$ -th standard basis vector in $\\mathbb{R}^{V}$ . $\\Delta^{V-1}$ denotes the $V$ -dimensional unit simplex and $\\mathbb{S}(\\boldsymbol{\\mathbf{\\rho}}):\\mathbb{R}^{V}\\rightarrow\\Delta^{V-1}$ the softmax map: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{S}(\\pmb{a})=[\\mathbb{S}_{1}(\\pmb{a}),\\dots,\\mathbb{S}_{V}(\\pmb{a})]^{\\intercal},\\qquad\\mathrm{with~}\\mathbb{S}_{v}(\\pmb{a})=\\frac{e^{\\mathrm{e}_{v}^{\\intercal}\\pmb{a}}}{\\sum_{v^{\\prime}\\in[V]}e^{\\mathrm{e}_{v^{\\prime}}^{\\intercal}\\pmb{a}}}\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "As explained in Section 2 we represent a training set as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{T}_{m}:=\\big\\{(\\bar{h}_{j},\\hat{\\pi}_{j},\\hat{p}_{j,z\\in\\mathcal{V}})\\big\\}_{j\\in[m]}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We assume that embeddings are bounded and denote ", "page_idx": 20}, {"type": "equation", "text": "$$\nM:=\\sqrt{2}\\operatorname*{max}_{j\\in[m]}\\|\\bar{h}_{j}\\|\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Given $\\tau_{m}$ , let ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{F}=\\mathrm{span}\\left(\\left\\{(e_{z}-e_{z^{\\prime}})\\bar{h}_{j}^{\\intercal}:z\\neq z^{\\prime}\\in S_{j},j\\in[m]\\right\\}\\right)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "a subspace of $V\\times d$ matrices and $\\mathfrak{F}^{\\perp}$ its orthogonal complement. Denote $\\mathcal{P}_{\\mathcal{F}},\\mathcal{P}_{\\perp}$ the orthogonal projections onto $\\mathcal{F}$ and $\\mathfrak{F}^{\\perp}$ , respectively. For convenience, for $W\\in\\mathbb{R}^{V\\times d}$ , we denote ", "page_idx": 20}, {"type": "equation", "text": "$$\nW_{\\mathcal{F}}:=\\mathcal{P}_{\\mathcal{F}}(W)\\qquad\\mathrm{and}\\qquad W_{\\perp}=\\mathcal{P}_{\\perp}(W)\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Define ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{CE}_{\\mathcal{F}}(W)=\\sum_{j\\in[m]}\\hat{\\pi}_{j}\\sum_{z\\in S_{j}}\\hat{p}_{j,z}\\log\\left(1+\\sum_{z\\neq z}e^{-(e_{z}-e_{z^{\\prime}})^{\\top}W\\bar{h}_{j}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Clearly, for all $W\\in\\mathbb{R}^{V\\times d}$ , it holds $\\operatorname{CE}(\\boldsymbol{W})\\geq\\operatorname{CE}_{\\mathcal{F}}(\\boldsymbol{W})$ . Note also that for all $W\\in{\\mathcal{F}}$ and for all $W^{\\mathrm{d}}\\in\\mathcal{F}^{\\perp}$ that satisfy Eq. (6a), it holds $\\begin{array}{r}{\\mathrm{CE}_{\\mathcal{F}}(W)=\\operatorname*{lim}_{R\\to\\infty}\\mathrm{CE}(W+R W^{\\mathrm{d}})}\\end{array}$ . Thus, under NTP compatibility and NTP separability, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{W\\in\\mathcal{F}}\\mathrm{CE}_{\\mathcal{F}}(W)=\\operatorname*{inf}_{W}\\mathrm{CE}(W)=\\mathcal{H}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "E Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "E.1 Gradient Descent ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Throughout we assume GD is ran with step-size $\\eta\\leq1/(2L)$ where $L$ is the smoothness of CE loss.   \nThis condition is not explicitly mentioned thereafter. ", "page_idx": 20}, {"type": "text", "text": "E.1.1 Auxiliary Lemmata ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The following result follows from standard optimization analysis for smooth convex functions specialized to functions that do not attain their infimum. The version presented here is adopted from Lemma 2 in [37]. ", "page_idx": 20}, {"type": "text", "text": "Lemma 3. It holds ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{k\\to\\infty}\\operatorname{CE}(W_{k})=\\operatorname*{inf}_{W}\\operatorname{CE}(W)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and also $\\mathrm{lim}_{k\\rightarrow\\infty}\\left\\|\\pmb{W}_{k}\\right\\|=\\infty$ . ", "page_idx": 20}, {"type": "text", "text": "In the lemma below, we collect some useful and simple-to-show properties of the GD and regularization paths. These are adaptations of corresponding results for one-hot binary classification over general non-separable data established in [34]. ", "page_idx": 20}, {"type": "text", "text": "Lemma 4. Suppose conditions (6) hold for some $W^{\\mathrm{d}}$ . Also, that there exists $W^{\\mathrm{p}}\\;=\\;W^{\\star}\\;\\in\\;\\mathcal{F}$ satisfying condition (4). The following hold: ", "page_idx": 20}, {"type": "text", "text": "1. $\\operatorname{CE}_{\\mathcal{F}}(W^{\\star})=\\operatorname*{inf}_{W\\in\\mathcal{F}}\\operatorname{CE}_{\\mathcal{F}}(W)=\\mathcal{H},$   \n2. $W^{\\star}$ is the unique minimizer of $\\mathrm{CE_{\\mathcal{F}}}$ on the subspace $\\mathcal{F}$ ,   \n3. $\\begin{array}{r}{\\operatorname*{lim}_{k\\rightarrow\\infty}\\mathcal{P}_{\\mathcal{F}}(W_{k})=W^{\\star}}\\end{array}$ , where $W_{k}$ are $G D$ iterates,   \n4. $\\begin{array}{r}{\\operatorname*{lim}_{k\\rightarrow\\infty}\\left\\|\\mathcal{P}_{\\perp}(W_{k})\\right\\|=\\infty,}\\end{array}$ ,   \n5. $\\begin{array}{r}{\\operatorname*{lim}_{B\\to\\infty}\\mathcal{P}_{\\mathcal{F}}(\\widehat{\\pmb{W}}_{B})=\\pmb{W}^{\\star}}\\end{array}$ , where $\\widehat{W}_{B}$ is the reguarlized solution (8),   \n6 $\\begin{array}{r}{\\mathrm{:}\\ \\operatorname*{lim}_{B\\to\\infty}\\|\\mathcal{P}_{\\perp}(\\widehat{\\pmb{W}}_{B})\\|=\\infty.}\\end{array}$ ", "page_idx": 21}, {"type": "text", "text": "Proof. It is easy to check by direct substitution of $W^{\\star}$ in (12) and use of (4) that $\\mathrm{CE}_{\\mathcal{F}}(W^{\\star})=\\mathcal{H}$ .   \nThis and (13) show the first claim. ", "page_idx": 21}, {"type": "text", "text": "The first claim shows $W^{\\star}$ is a minimizer. Suppose for the sake of contradiction there is a different minimizer $W^{\\star}\\,\\neq\\,W_{1}\\,\\in\\,\\mathcal{F}$ . Then, since $\\mathrm{CE}_{\\mathcal{F}}(\\mathbf{\\bar{W}}_{1})=\\mathcal{H}$ , it also holds for $W_{R}:=W_{1}+R W^{\\mathrm{d}}$ that $\\begin{array}{r}{\\operatorname*{lim}_{R\\to\\infty}\\operatorname{CE}(W_{R})=\\mathcal{H}}\\end{array}$ . In turn, this implies for all $j\\in[m]$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{R\\to\\infty}\\mathbb{S}_{z}\\big(W_{R}\\bar{h}_{j}\\big)=\\hat{p}_{j,z},\\forall z\\in\\mathcal{S}_{j}\\,,\\qquad\\mathrm{and}\\qquad\\operatorname*{lim}_{R\\to\\infty}\\mathbb{S}_{v}\\big(W_{R}\\bar{h}_{j}\\big)=0,\\forall v\\notin\\mathcal{S}_{j}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The first condition gives then that $W_{1}$ must satisfy (4). Since $W^{\\star}$ also satisfies these equations, denoting $W_{\\Delta}=W^{\\star}-W_{1}\\neq0.$ , it holds: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle W_{\\Delta},(e_{z}-e_{z^{\\prime}})^{\\top}\\bar{h}_{j})\\rangle=0,\\,\\forall j\\in[m],z\\neq z^{\\prime}\\in S_{j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "But $W_{\\Delta}\\in\\mathcal{F}$ , so this forms a contradiction. Hence, $W^{\\star}$ is unique solution in $\\mathcal{F}$ of (4) and unique minimizer of $\\mathrm{CE_{\\mathcal{F}}}$ on the subspace $\\mathcal{F}$ . ", "page_idx": 21}, {"type": "text", "text": "The proof of the third claim follows the same way as the proof of part (1) of Thm. 15 of [37]. For completeness: It follows by the lemma\u2019s assumptions and Lemma 3 that $\\begin{array}{r}{\\operatorname*{lim}_{k\\to\\infty}\\operatorname{CE}(W_{k})=}\\end{array}$ $\\mathcal{H}$ . Combining with the first claim of the lemma yields $\\begin{array}{r}{\\operatorname*{lim}_{k\\to\\infty}\\operatorname{CE}(W_{k})\\,=\\,\\operatorname{CE}_{\\mathcal{F}}(W^{\\star})}\\end{array}$ . Since $\\mathrm{CE}_{\\mathcal{F}}(W_{k})\\leq\\bar{\\mathrm{CE}}(W_{k})$ , this finally gives ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{k\\rightarrow\\infty}\\mathrm{CE}_{\\mathcal{F}}(W_{k})=\\operatorname*{lim}_{k\\rightarrow\\infty}\\mathrm{CE}_{\\mathcal{F}}\\left(\\mathcal{P}_{\\mathcal{F}}(W_{k})\\right)=\\mathrm{CE}_{\\mathcal{F}}(W^{\\star}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since $W^{\\star}$ is unique by the second claim, the desired then follows. ", "page_idx": 21}, {"type": "text", "text": "For the fourth claim, recall from Lemma 3 that $\\mathrm{lim}_{k\\rightarrow\\infty}\\left\\|\\pmb{W}_{k}\\right\\|=\\infty$ . From the previous claim, we also have $\\begin{array}{r}{\\operatorname*{lim}_{k\\rightarrow\\infty}\\left\\|\\mathcal{P}_{\\mathcal{F}}({\\boldsymbol W}_{k})\\right\\|<C}\\end{array}$ for some constant $C>\\|W^{\\star}\\|$ . Thus, the desired follows by applying the fact that $\\left\\|\\dot{W}_{k}\\right\\|=\\left\\|\\mathcal{P}_{\\mathcal{F}}(W_{k})\\right\\|+\\left\\|\\mathcal{P}_{\\perp}(W_{k})\\right\\|$ . ", "page_idx": 21}, {"type": "text", "text": "The proof of the last two claim is exactly same as that of the third and fourth claim. Only now use the facts that $\\begin{array}{r}{\\operatorname*{lim}_{B\\to\\infty}\\operatorname{CE}(W_{B})=\\mathcal{H}}\\end{array}$ and $\\operatorname*{lim}_{B\\to\\infty}\\left\\|W_{B}\\right\\|=\\infty$ (see proof of Theorem 1). \u53e3 ", "page_idx": 21}, {"type": "text", "text": "E.1.2 Key Lemma ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Lemma 5. Let $W_{k}$ denote the GD iterate at iteration $k$ . Recall the decomposition $W_{k}=\\mathcal{P}_{\\mathcal{F}}(W_{k})+$ $\\mathcal{P}_{\\perp}(W_{k})=W_{k,\\mathcal{F}}+W_{k,\\perp}$ . Fix any $\\alpha\\in(0,1)$ . There exists large enough $R=R(\\alpha)$ and $k_{0}=k_{0}(R)$ such that for any $k\\geq k_{0}$ , it holds that $\\|W_{k,\\perp}\\|\\geq R$ and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{CE}\\left(\\,W_{k,\\mathcal{F}}+\\left(1+\\alpha\\right)\\|W_{k,\\bot}\\|\\overline{{W^{\\mathrm{mm}}}}\\,\\right)\\leq\\mathrm{CE}(W_{k})\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. We drop the subscript $k$ to lighten notation. ", "page_idx": 21}, {"type": "text", "text": "First, note by Lemma 4.D that, for arbitrary $R$ , we can pick $k_{1}\\,=\\,k_{1}(R)$ such that for all $k\\geq k_{1}$ : $\\|W_{\\bot}\\|\\geq R$ . ", "page_idx": 21}, {"type": "text", "text": "Thus next, we will prove the main claim, i.e. for large enough $\\|\\pmb{W}_{\\perp}\\|$ inequality (14) holds. Denote $\\begin{array}{r}{R^{\\prime}=\\frac{\\|W_{\\perp}\\|}{\\|W^{\\mathrm{mm}}\\|}}\\end{array}$ . Substituting in CE expression (2), and using the fact that $W^{\\mathrm{mm}}\\in\\mathcal{F}^{\\bot}$ by (6a) yield: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\SigmaE}\\left(\\mathbf{\\delta}W_{\\mathcal{F}}+(1+\\alpha)R^{\\prime}W^{\\mathrm{mm}}\\right)}\\\\ &{=\\underset{j\\in[m]}{\\sum}\\ \\underset{z\\in\\mathcal{S}_{j}}{\\hat{\\pi}}\\,\\hat{p}_{j,z}\\log\\left(\\underset{z^{\\prime}\\in\\mathcal{S}_{j}}{\\sum}e^{-(e_{z}-e_{z^{\\prime}})^{\\top}W_{\\mathcal{F}}\\bar{h}_{j}}+\\underset{v\\notin\\mathcal{S}_{j}}{\\sum}e^{-(e_{z}-e_{v})^{\\top}W_{\\mathcal{F}}\\bar{h}_{j}}+\\underset{v\\notin\\mathcal{S}_{j}}{\\sum}e^{-(1+\\alpha)R^{\\prime}(e_{z}-e_{v})^{\\top}W^{\\mathrm{mm}}\\bar{h}_{j}}\\right)}\\\\ &{=\\underset{j\\in[m]}{\\sum}\\ \\underset{z\\in\\mathcal{S}_{j}}{\\hat{\\pi}}\\,\\hat{p}_{j,z}\\log\\left(\\underset{v\\in\\mathcal{V}}{\\sum}e^{-(e_{z}-e_{v})^{\\top}W_{\\mathcal{F}}\\bar{h}_{j}}+\\underset{v\\notin\\mathcal{S}_{j}}{\\sum}e^{-(1+\\alpha)R^{\\prime}(e_{z}-e_{v})^{\\top}W^{\\mathrm{mm}}\\bar{h}_{j}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Moreover, decomposing $W=W_{\\mathcal{F}}+W_{\\bot}$ , and defining ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\widetilde W_{\\perp}:=\\frac{\\|W^{\\mathrm{mm}}\\|}{\\|W_{\\perp}\\|}W_{\\perp}\\ =\\frac{1}{R}W_{\\perp}\\,,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathrm{\\mathcal{X}}\\left(W\\right)=}&{\\underset{j\\in[m]}{\\sum}\\ \\hat{\\pi}_{j}\\underset{z\\in\\mathcal{S}_{j}}{\\sum}\\ \\hat{p}_{j,z}\\log\\left(\\underset{z^{\\prime}\\in\\mathcal{S}_{j}}{\\sum}e^{-(e_{z}-e_{z^{\\prime}})^{\\top}W_{\\mathcal{F}}\\bar{h}_{j}}+\\underset{v\\notin\\mathcal{S}_{j}}{\\sum}e^{-(e_{z}-e_{v})^{\\top}W_{\\mathcal{F}}\\bar{h}_{j}}+\\underset{v\\notin\\mathcal{S}_{j}}{\\sum}e^{-R^{\\prime}(e_{z}-e_{v})^{\\top}\\widetilde{W}_{\\mathcal{F}}}\\right)}&\\\\ &{=}&{\\underset{j\\in[m]}{\\sum}\\ \\hat{\\pi}_{j}\\underset{z\\in\\mathcal{S}_{j}}{\\sum}\\ \\hat{p}_{j,z}\\log\\left(\\underset{v\\in\\mathcal{V}}{\\sum}e^{-(e_{z}-e_{v})^{\\top}W_{\\mathcal{F}}\\bar{h}_{j}}+\\underset{v\\notin\\mathcal{S}_{j}}{\\sum}e^{-R^{\\prime}(e_{z}-e_{v})^{\\top}\\overline{{W}}_{\\mathcal{F}}\\bar{h}_{j}}\\right)\\,,\\qquad\\qquad\\qquad(16)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we used that, by definition, $W_{\\perp}\\in\\mathcal{F}^{\\perp}$ . Thus, our goal becomes showing $(15)\\leq(16)$ , for large enough $R$ . To do this, we consider two cases as follows below. ", "page_idx": 22}, {"type": "text", "text": "For the remaining of the proof recall $M:=\\operatorname*{max}_{j\\in[m]}\\sqrt{2}\\|\\bar{\\pmb{h}}_{j}\\|$ and use the logits shorthand: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\widetilde{\\ell}_{j,v}=e_{v}^{\\top}\\widetilde{W}_{\\perp}\\bar{h}_{j}\\qquad\\mathrm{and}\\qquad\\ell_{j,v}^{\\mathrm{mm}}=e_{v}^{\\top}{W}^{\\mathrm{mm}}\\bar{h}_{j}\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Case 1: $W_{\\perp}$ is well aligned with $W^{\\mathrm{mm}}$ . Suppose ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|W^{\\mathrm{mm}}-\\widetilde{W}_{\\perp}\\|\\leq\\epsilon:=\\frac{\\alpha}{M}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Using this, linearity of logits, and Cauchy-Schwartz, yields ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\widetilde{\\ell}_{j,z}-\\widetilde{\\ell}_{j,v}\\leq\\ell_{j,z}^{\\mathrm{mm}}-\\ell_{j,v}^{\\mathrm{mm}}+\\epsilon M,\\ \\ \\forall j\\in[m],z\\in S_{j},v\\notin S_{j}\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{v\\notin S_{j}}e^{-R^{\\prime}(e_{z}-e_{v})^{\\top}\\widetilde{W}_{\\perp}\\hat{h}_{j}}\\ge e^{-\\epsilon M R^{\\prime}}\\sum_{v\\notin S_{j}}e^{-R^{\\prime}(e_{z}-e_{v})^{\\top}W^{\\operatorname*{min}}\\bar{h}_{j}}=e^{-\\alpha R^{\\prime}}\\sum_{v\\notin S_{j}}e^{-R^{\\prime}(e_{z}-e_{v})^{\\top}W^{\\operatorname*{min}}\\bar{h}_{j}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Also recall by feasibility of $W^{\\mathrm{mm}}$ that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\ell_{j,z}^{\\mathrm{mm}}-\\ell_{j,v}^{\\mathrm{mm}}\\geq1,\\forall j\\in[m],z\\in S_{j},v\\notin S_{j}\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{v\\notin S_{j}}e^{-(1+\\alpha)R^{\\prime}(e_{z}-e_{v})^{\\top}\\widetilde W_{\\perp}\\bar{h}_{j}}\\le e^{-\\alpha R^{\\prime}}\\sum_{v\\notin S_{j}}e^{-R^{\\prime}(e_{z}-e_{v})^{\\top}W^{\\operatorname*{mm}}\\bar{h}_{j}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Comparing the above two displays yields ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{v\\notin S_{j}}e^{-(1+\\alpha)R^{\\prime}(e_{z}-e_{v})^{\\top}\\widetilde W_{\\bot}\\bar{h}_{j}}\\leq\\sum_{v\\notin S_{j}}e^{-R^{\\prime}(e_{z}-e_{v})^{\\top}\\widetilde W_{\\bot}\\bar{h}_{j}},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which implies the desired $(15){\\leq}(16)$ for any value of $R^{\\prime}$ (eqv. $\\|\\pmb{W}_{\\perp}\\|_{,}$ ). ", "page_idx": 22}, {"type": "text", "text": "Case 2: No alignment. Suppose now that (17) does not hold. Note that $\\|\\widetilde{\\mathbf{W}}_{\\perp}\\|=\\|W^{\\mathrm{mm}}\\|$ and since (NTP-SVM) has a unique solution it must be that $\\widetilde{W}_{\\perp}$ is not feasible. But $\\widetilde{W}_{\\perp}\\in\\mathcal{F}_{\\perp}$ , thus it satisfies the equality constraints. This then means that there exist $\\delta:=\\delta(\\epsilon)$ and $j_{\\star}\\in[m],v_{\\star}\\notin S_{j_{\\star}}$ such that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widetilde{\\ell}_{j_{\\star},z}-\\widetilde{\\ell}_{j_{\\star},v_{\\star}}\\leq1-\\delta\\,,\\ \\ \\forall z\\in S_{j_{\\star}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "(Note the above holds for all $z\\in S_{j_{\\star}}$ because $\\widetilde{\\ell}_{j_{\\star},z}=\\widetilde{\\ell}_{j_{\\star},z^{\\prime}}$ since $\\widetilde{W}_{\\perp}\\in\\mathcal{F}_{\\perp}$ .) To continue, we introduce the shorthand notation ", "page_idx": 23}, {"type": "equation", "text": "$$\nA_{j,z}:=A_{j,z}(W)=\\sum_{v\\in\\mathcal{V}}e^{-(e_{z}-e_{v})^{\\top}W_{\\mathcal{F}}\\bar{h}_{j}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "as well as ", "page_idx": 23}, {"type": "equation", "text": "$$\nA_{\\operatorname*{min}}:=\\operatorname*{min}_{j\\in[m],z\\in{\\cal S}_{j}}A_{j,z},\\qquad\\mathrm{and}\\qquad A_{\\operatorname*{max}}:=\\operatorname*{max}_{j\\in[m],z\\in{\\cal S}_{j}}A_{j,z}\\,.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Using (19) we may lower bound (16) as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathrm{\\mathcal{X}}(W)-\\displaystyle\\sum_{j\\in[m]}\\hat{\\pi}_{j}\\,\\sum_{z\\in\\mathcal{S}_{j}}\\hat{p}_{j,z}\\log\\left(\\displaystyle\\sum_{v\\in\\mathcal{V}}e^{-(e_{z}-e_{v})^{\\top}W_{\\mathcal{I}}\\bar{h}_{j}}\\right)\\geq\\hat{\\pi}_{j,\\star}\\displaystyle\\sum_{z\\in\\mathcal{S}_{j}}\\hat{p}_{j,z}\\log\\left(1+\\frac{e^{-R^{\\prime}(e_{z}-e_{v,\\star})^{\\top}\\widetilde{W}_{\\mathcal{I}}\\bar{h}_{j\\star}}}{A_{j\\star,z}}\\right)}&{}&\\\\ {\\geq\\hat{\\pi}_{j,\\star}\\displaystyle\\sum_{z\\in\\mathcal{S}_{j}}\\hat{p}_{j,z}\\log\\left(1+\\frac{e^{-R^{\\prime}(1-\\delta)}}{A_{\\operatorname*{max}}}\\right)}&{}&\\\\ {\\geq\\frac{e^{-R^{\\prime}(1-\\delta)}}{n(A_{\\operatorname*{max}}+1)}\\,,}&{}&{(20)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where in the last line we used $\\hat{\\pi}_{j}\\ge1/n,\\forall j\\in[m]$ as well as $\\textstyle\\log(1+x)\\geq{\\frac{x}{1+x}},x>0$ . ", "page_idx": 23}, {"type": "text", "text": "On the other hand, using property (18) for max-margin logits, we can upper bound (15) as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{CE}\\left(W_{\\mathcal{F}}+(1+\\alpha)R^{\\prime}W^{\\mathrm{mm}}\\right)-\\sum_{j\\in[m]}\\hat{\\pi}_{j}\\sum_{z\\in\\mathcal{S}_{j}}\\hat{p}_{j,z}\\log\\left(\\sum_{v\\in\\mathcal{V}}e^{-(e_{z}-e_{v})^{\\top}W_{\\mathcal{F}}\\hat{h}_{j}}\\right)\\leq\\log\\left(1+\\frac{V\\,e^{-R^{\\prime}(1+\\alpha)}}{A_{\\mathrm{min}}}\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where in the last line we used $\\log(1+x)\\leq x,x>0$ . ", "page_idx": 23}, {"type": "text", "text": "In view of the two last displays, it suffices that ", "page_idx": 23}, {"type": "equation", "text": "$$\nV\\,\\frac{e^{-R^{\\prime}(1+\\alpha)}}{A_{\\mathrm{min}}}\\le\\frac{e^{-R^{\\prime}(1-\\delta)}}{n(A_{\\mathrm{max}}+1)}\\iff R^{\\prime}\\ge\\frac{1}{\\delta+\\alpha}\\log\\left(\\frac{n V(A_{\\mathrm{max}}+1)}{A_{\\mathrm{min}}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "All it remains is obtaining bounds for $A_{\\operatorname*{min}},A_{\\operatorname*{max}}$ specifically showing that they do not depend on $R$ By Cauchy-Schwartz: ", "page_idx": 23}, {"type": "equation", "text": "$$\nV e^{-M\\|W_{\\mathcal{F}}\\|}\\le A_{\\operatorname*{min}}\\le A_{\\operatorname*{max}}\\le V e^{M\\|W_{\\mathcal{F}}\\|}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Further recall by Lemma 4.C that if $k$ is large enough then ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|W_{\\mathcal{F}}-W^{\\star}\\|\\leq\\|W^{\\star}\\|\\implies\\|W_{\\mathcal{F}}\\|\\leq2\\|W^{\\star}\\|.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Thus, there exists $k_{\\star}=k_{\\star}(\\|\\pmb{W}_{\\star}\\|)$ such that for all $k\\geq k_{\\star}$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\nV e^{-2M\\|\\pmb{W}_{\\star}\\|}\\leq\\pmb{A}_{\\operatorname*{min}}\\leq\\pmb{A}_{\\operatorname*{max}}\\leq V e^{2M\\|\\pmb{W}_{\\star}\\|}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Hence, the desired $(21){\\leq}(20)$ holds provided ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|\\pmb{W}_{\\perp}\\|\\geq\\frac{\\|\\pmb{W}^{\\mathrm{mm}}\\|}{\\alpha}\\log\\left(2n V e^{4\\|\\pmb{W}^{\\star}\\|}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Set $R=R(\\alpha)=\\left\\{{\\mathrm{RHS}}\\right.$ of $(23)\\}$ and $k_{0}(R):=\\operatorname*{max}\\{k_{1}(R),k_{\\star}\\}$ . We have shown this guarantees for all $k\\geq k_{0}\\colon\\|W_{\\perp}\\|\\geq R$ and by choice of $R$ also $(21)\u2010(20)$ . This in turn implies $(15){\\leq}(16)$ , as desired to complete the proof. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "E.1.3 Proof of Theorem 2 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "For the subspace component, see Lemma 4.C. For the directional convergence, the key ingredient of the proof is Lemma 5. After that, the proof follows identically to Thm. 15(2) in [37]. We include the details for completeness, but there are no novel aspects in the rest of this section. ", "page_idx": 23}, {"type": "text", "text": "Let any $\\epsilon\\in(0,1)$ and choose $\\alpha=\\epsilon/(1-\\epsilon)$ . By Lemma 5, there exists $k_{0}$ such that for any $k\\geq k_{0}$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|W_{k,\\bot}\\|\\ge\\operatorname*{max}\\{R(\\alpha),1/2\\}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\nabla\\,\\mathrm{CE}(W_{k}),W_{k,\\perp}-(1+\\alpha)\\|W_{k,\\perp}\\|W^{\\mathrm{mm}}\\rangle=\\left\\langle\\nabla\\,\\mathrm{CE}(W_{k}),W_{k}-\\left(W_{k,\\mathcal{I}}+(1+\\alpha)\\,\\|W_{k,\\perp}\\|\\overline{{W^{\\mathrm{mm}}}}\\right)\\right\\rangle}&{}\\\\ {\\supset\\mathrm{CE}(W_{k})-\\mathrm{CE}(W_{k,\\mathcal{I}}+(1+\\alpha)\\,\\|W_{k,\\perp}\\|\\overline{{W^{\\mathrm{mm}}}})\\ge0\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we also used convexity of the loss. ", "page_idx": 24}, {"type": "text", "text": "Consequently, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\langle W_{k+1}-W_{k},\\overline{{W^{\\mathrm{mm}}}}\\rangle=\\langle-\\eta\\nabla\\operatorname{CE}(W_{k}),\\overline{{W^{\\mathrm{mm}}}}\\rangle}&{}\\\\ {\\,\\,\\ge(1-\\epsilon)\\langle-\\eta\\nabla\\operatorname{CE}(W_{k}),\\overline{{W_{k,\\perp}}}\\rangle}&{}\\\\ {\\,\\,\\ge(1-\\epsilon)\\langle W_{k+1,\\perp}-W_{k,\\perp},\\overline{{W_{k,\\perp}}}\\rangle}&{}\\\\ {\\,\\,\\ge(1-\\epsilon)\\langle W_{k+1,\\perp}-W_{k,\\perp},\\overline{{W_{k,\\perp}}}\\rangle}&{}\\\\ {\\,\\,=\\frac{(1-\\epsilon)}{2\\left\\|W_{k,\\perp}\\right\\|}\\left(\\left\\|W_{k+1,\\perp}\\right\\|^{2}-\\left\\|W_{k,\\perp}\\right\\|^{2}-\\left\\|W_{k+1,\\perp}-W_{k,\\perp}\\right\\|^{2}\\right)}&{}\\\\ {\\,\\,\\ge(1-\\epsilon)\\left(\\left\\|W_{k+1,\\perp}\\right\\|-\\left\\|W_{k,\\perp}\\right\\|-2\\eta(\\operatorname{CE}(W_{k,\\perp})-\\operatorname{CE}(W_{k+1,\\perp})\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last step used $\\|W_{k,\\bot}\\|\\geq1/2$ , the fact that $x^{2}-y^{2}\\geq2y(x-y),\\forall x,y$ and smoothness of the CE loss. ", "page_idx": 24}, {"type": "text", "text": "Telescoping the above expression and rearranging yields ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\overline{{W}}_{k},\\overline{{W^{\\mathrm{mm}}}}\\rangle\\geq(1-\\epsilon)\\frac{\\|W_{k,\\perp}\\|}{\\|W_{k}\\|}-\\frac{\\langle W_{k_{0}},\\overline{{W^{\\mathrm{mm}}}}\\rangle-(1-\\epsilon)\\|w_{k_{0},\\perp}\\|-\\eta\\,\\mathrm{CE}(W_{k_{0}})}{\\|W_{k}\\|}}\\\\ &{\\qquad\\qquad\\geq(1-\\epsilon)-\\frac{\\|W_{k,\\mathcal{T}}\\|_{2}+\\langle W_{k_{0}},\\overline{{W^{\\mathrm{mm}}}}\\rangle-(1-\\epsilon)\\|w_{k_{0},\\perp}\\|-\\eta\\,\\mathrm{CE}(W_{k_{0}})}{\\|W_{k}\\|}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now recall from Lemma 4 that $\\operatorname*{lim}_{k\\to\\infty}\\left\\|W_{k}\\right\\|=\\infty$ and $\\begin{array}{r}{\\operatorname*{lim}_{k\\rightarrow\\infty}\\|\\boldsymbol{W}_{k,\\mathcal{F}}\\|=\\|\\boldsymbol{W}^{\\star}\\|}\\end{array}$ . Thus, $\\operatorname*{lim}\\operatorname*{inf}_{k\\to\\infty}\\langle\\overline{{W}}_{k},\\overline{{W^{\\mathrm{mm}}}}\\rangle\\geq1-\\epsilon$ . Since $\\epsilon$ is arbitrary, the desired follows. ", "page_idx": 24}, {"type": "text", "text": "E.2 Regularization Path ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We provide a detailed proof of Theorem 1 fliling in missing details from the proof sketch in the main paper. ", "page_idx": 24}, {"type": "text", "text": "E.2.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "First, we show that $\\widehat{W}_{B}$ is on the boundary, i.e. $\\|\\widehat{\\pmb{W}}_{B}\\|=B$ . Suppose not, then $\\langle\\nabla\\,\\mathrm{CE}(\\widehat{\\pmb{W}}_{B}),\\pmb{U}\\rangle=0$ for all $U\\in\\mathbb{R}^{V\\times d}$ . Using the CE expression in (2) and a few algebraic manipulations, yields ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\langle-\\nabla\\operatorname{CE}(\\widehat{W}_{B}),U\\rangle=\\sum_{j\\in[m]}\\hat{\\pi}_{j}\\sum_{z\\in S_{j}}\\hat{p}_{j,z}\\Big(\\sum_{z^{\\prime}\\in S_{j}}s_{j,z^{\\prime}}\\,\\big(e_{z}-e_{z^{\\prime}}\\big)^{\\top}U\\bar{h}_{j}+\\sum_{v\\notin S_{j}}s_{j,v}\\,(e_{z}-e_{v})^{\\top}U\\bar{h}_{j}\\Big),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we denote the output probabilities at $\\widehat{W}_{B}$ as $s_{j,v}\\,:=\\,\\mathbb{S}_{v}\\bigl(\\widehat{W}_{B}\\bar{h}_{j}\\bigr),v\\,\\in\\,\\mathcal{V},j\\,\\in\\,[m]$ . Choose $U=W^{\\mathrm{mm}}$ in (24). Then, the first term in the parenthesis in (24) is zero by (6a), while the second term is strictly positive by (6b) and strict positivity of softmax entries, leading to contradiction. ", "page_idx": 24}, {"type": "text", "text": "Now, consider point $W_{B}^{\\star}=W^{\\star}+R(B)\\cdot W^{\\mathrm{mm}}$ , where, $W^{\\star}\\in\\mathcal{F}$ satisfies (4), and $R=R(B)$ is chosen such that $\\|\\mathbf{\\boldsymbol{W}}_{B}^{\\star}\\|\\,{=}\\,B$ . Concretely, for $B>\\|\\pmb{W}^{\\star}\\|$ , set ", "page_idx": 24}, {"type": "equation", "text": "$$\nR=\\frac{1}{\\|W^{\\mathrm{mm}}\\|}\\sqrt{B^{2}-\\|W^{\\star}\\|^{2}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Note also that $R/B\\to1/\\|W^{\\mathrm{mm}}\\|$ as $B\\to\\infty$ . We will show that $W_{B}^{\\star}$ attains a small CE loss as $B$ (hence, $R$ ) grows. To do this, denote for convenience the logits for all $v\\in\\mathcal{V},j\\in[m]$ \u2236 ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\ell_{j,v}^{\\star}:=e_{v}^{\\top}W^{\\star}\\bar{h}_{j}\\quad\\mathrm{and}\\quad\\ell_{j,v}^{\\mathrm{mm}}:=e_{v}^{\\top}W^{\\mathrm{mm}}\\bar{h}_{j}\\,,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and note that $e_{v}^{\\top}W_{B}^{\\star}\\bar{h}_{j}=\\ell_{j,v}^{\\star}+R\\,\\ell_{j,v}^{\\mathrm{mm}}$ . By using (4) and (6a): ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{z^{\\prime}\\in S_{j}}e^{-(\\ell_{j,z}^{\\star}+R\\ell_{j,z}^{\\operatorname*{mm}}-\\ell_{j,z^{\\prime}}^{\\star}-R\\ell_{j,z^{\\prime}}^{\\operatorname*{mm}})}=\\frac{1}{\\hat{p}_{j}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Moreover, using (6b) ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{v\\notin S_{j}}e^{-(\\ell_{j,z}^{\\star}+R\\ell_{j,z}^{\\operatorname*{mm}}-\\ell_{j,v}^{\\star}-R\\ell_{j,v}^{\\operatorname*{mm}})}\\leq e^{-R}\\sum_{v\\notin S_{j}}e^{-(\\ell_{j,z}^{\\star}-\\ell_{j,v}^{\\star})}\\leq C\\,e^{-R},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where we define constant (independent of $R$ ) $C:=V e^{\\|\\pmb{W}^{\\star}\\|_{M}}$ , for $M:=\\sqrt{2}\\cdot\\operatorname*{max}_{j/\\in[m]}\\|\\bar{\\pmb{h}}_{j}\\|$ . Combining the above displays and using in Eq. (2), yields ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathrm{CE}(W_{B}^{\\star})\\!\\!\\!\\!\\!}&{\\le\\displaystyle\\sum_{j\\in[m]}\\,\\hat{\\pi}_{j}\\,\\sum_{z\\in\\mathcal{S}_{j}}\\hat{p}_{j,z}\\log\\Big(\\frac{1}{\\hat{p}_{j,z}}+C\\,e^{-R}\\Big)\\,\\le\\displaystyle\\sum_{j\\in[m]}\\,\\hat{\\pi}_{j}\\,\\sum_{z\\in\\mathcal{S}_{j}}\\hat{p}_{j,z}\\Big(\\log\\Big(\\frac{1}{\\hat{p}_{j,z}}\\Big)+\\hat{p}_{j,z}C\\,e^{-R}\\Big)}&\\\\ &{\\le\\mathcal{H}+C\\,e^{-R}\\,,}&{(2}&{)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where, the second line uses $\\log(1+x)\\leq x,x>0$ , and the third line uses $\\hat{\\pi}_{j},\\hat{p}_{j,z}$ are probabilities. ", "page_idx": 25}, {"type": "text", "text": "Next, towards arriving at a contradiction, we will show that if $\\widehat{W}_{B}$ is not in the direction of $W^{\\mathrm{mm}}$ , then it incurs a loss that is larger than $\\mathrm{CE}(W_{B}^{\\star})$ . Concretely, assuming the statement of the theorem is not true, we we will upper bound ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{CE}\\big(\\widehat{W}_{B}\\big)-\\mathcal{H}=\\sum_{j\\in[m]}\\hat{\\pi}_{j}\\sum_{z\\in S_{j}}\\hat{p}_{j,z}\\log\\Big(\\frac{\\hat{p}_{j,z}}{\\mathbb{S}_{z}\\big(\\widehat{W}_{B}\\bar{h}_{j}\\big)}\\Big).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By our assumption, there exists $\\epsilon>0$ , such that there exists arbitrarily large $B$ satisfying: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left\\|\\frac{\\|W^{\\mathrm{mm}}\\|}{B}\\widehat{W}_{B}-W^{\\mathrm{mm}}\\right\\|>\\epsilon.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Define ", "page_idx": 25}, {"type": "equation", "text": "$$\n{\\widehat{\\pmb{W}}}={\\frac{1}{R^{\\prime}(B)}}\\big({\\widehat{\\pmb{W}}}_{B}-\\pmb{W}^{\\star}\\big),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where, $R^{\\prime}=R^{\\prime}(B)>0$ is chosen so that $\\|\\widehat{\\boldsymbol{W}}\\|=\\|\\boldsymbol{W}^{\\mathrm{mm}}\\|$ . Concretely, for large enough $B\\geq2\\|\\pmb{W}^{\\star}\\|$ , set ", "page_idx": 25}, {"type": "equation", "text": "$$\nR^{\\prime}=\\frac{1}{\\|{\\cal W}^{\\mathrm{mm}}\\|}\\sqrt{B^{2}-2B\\langle\\overline{{{W_{B}}}},{W^{\\star}}\\rangle+\\|{W^{\\star}}\\|^{2}}\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Note that it holds $\\begin{array}{r}{\\operatorname*{lim}_{B\\to\\infty}R^{\\prime}/B=1/\\|W^{\\mathrm{mm}}\\|}\\end{array}$ . Thus, we can always choose $B$ large enough so that Eq. (27) guarantees $\\|\\widehat{\\boldsymbol{W}}-\\boldsymbol{W}^{\\mathrm{mm}}\\|\\geq\\epsilon^{\\prime}$ , for some $\\epsilon^{\\prime}>0$ . Since $W^{\\mathrm{mm}}$ is the unique minimizer of (NTP-SVM) and $\\|\\widehat{\\boldsymbol{W}}\\|=\\|\\boldsymbol{W}^{\\mathrm{mm}}\\|$ , it follows that there exists $\\delta\\in(0,1)$ and $j\\in[m]$ such that at least one of the following is true ", "page_idx": 25}, {"type": "text", "text": "(i) $\\exists z$ and $z^{\\prime}\\neq z\\in S_{j}$ such that ", "page_idx": 25}, {"type": "equation", "text": "$$\n|(e_{z}-e_{z^{\\prime}})^{\\intercal}\\widehat{W}\\bar{h}_{j}|\\geq\\delta\\,,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "(ii) $\\exists z\\in S_{j},v\\notin S_{j}$ such that ", "page_idx": 25}, {"type": "equation", "text": "$$\n(e_{z}-e_{v})^{\\top}\\widehat{W}\\bar{h}_{j}\\leq1-\\delta.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Case $(i)$ : Without loss of generality $(e_{z}-e_{z^{\\prime}})^{\\intercal}\\widehat{W}\\bar{h}_{j}\\leq-\\delta$ (otherwise, filp $z,z^{\\prime})$ . Thus, ignoring all but one term in (26) gives ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{CE}(\\widehat{W}_{B})-\\mathcal{H}\\geq\\widehat{\\pi}_{j}\\widehat{p}_{j,z}\\log\\Big(\\frac{\\widehat{p}_{j,z}}{\\mathbb{S}_{z}(\\widehat{W}_{B}\\bar{h}_{j})}\\Big)\\geq\\widehat{\\pi}_{j}\\widehat{p}_{j,z}\\log\\Big(\\widehat{p}_{j,z}e^{(\\ell_{j,z^{\\prime}}-\\ell_{j,z})}\\Big),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where we use $\\ell_{j,v}=e_{v}^{\\top}\\widehat{W}_{B}\\bar{h}_{j},v\\in\\mathcal{V}$ to denote logits of $\\widehat{W}_{B}$ . Using (4) and (28), yields ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\ell_{j,z^{\\prime}}-\\ell_{j,z}=(e_{z^{\\prime}}-e_{z})^{\\top}\\big(R^{\\prime}\\,\\widehat{W}+W^{\\star}\\big)\\,\\bar{h}_{j}\\geq R^{\\prime}\\delta+\\log\\left(\\frac{\\hat{p}_{j,z^{\\prime}}}{\\hat{p}_{j,z}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Put in (26) and using $\\hat{p}_{j,z}\\geq\\hat{\\pi}_{j}\\hat{p}_{j,z}\\geq1/n$ shows ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathrm{CE}(\\widehat{W}_{B})\\geq\\mathcal{H}+\\frac{1}{n}\\log\\Big(\\frac{e^{R^{\\prime}\\delta}}{n}\\Big)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Compare this with (25). For large enough $B$ , it is clear that $\\hat{\\pi}_{j}\\hat{p}_{j,z}\\log\\left(\\hat{p}_{j,z}\\,c\\,e^{R^{\\prime}\\delta}\\right)>C e^{-R}$ . Thus, $\\mathrm{CE}(\\widehat{\\pmb{W}}_{B})>\\mathrm{CE}({\\pmb{W}}_{B}^{\\star})$ , a contradiction. ", "page_idx": 26}, {"type": "text", "text": "Case $(i i)$ : We can assume $\\widehat{W}\\in\\mathcal{F}_{\\perp}$ , since otherwise we are in Case (i). Now, again ignoring all but the $\\overline{{(j,z)}}$ term in the CE loss for which (29) holds for some $v\\not\\in{\\cal S}_{j}$ , we find ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathrm{CE}(\\widehat{W}_{B})-\\mathcal{H}\\geq\\hat{\\pi}_{j}\\hat{p}_{j,z}\\log\\Big(\\hat{p}_{j,z}\\Big(\\sum_{z^{\\prime}\\in\\mathcal{S}_{j}}e^{(\\ell_{j,z^{\\prime}}-\\ell_{j,z})}+e^{(\\ell_{j,v}-\\ell_{j,z})}\\Big)\\Big).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Using $\\mathcal{P}_{\\mathcal{T}}(\\widehat{\\pmb{W}}_{B})=\\pmb{W}^{\\star}$ yields ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{z^{\\prime}\\in S_{j}}e^{(\\ell_{j,z^{\\prime}}-\\ell_{j,z})}=\\sum_{z^{\\prime}\\in S_{j}}\\frac{\\hat{p}_{j,z^{\\prime}}}{\\hat{p}_{j,z}}=\\frac{1}{\\hat{p}_{j,z}}\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Moreover, by (29): ", "page_idx": 26}, {"type": "equation", "text": "$$\ne^{\\ell_{j,v}-\\ell_{j,z}}\\geq e^{-R^{\\prime}(1-\\delta)}\\,e^{\\ell_{j,v}^{\\star}-\\ell_{j,z}^{\\star}}\\geq c^{\\prime}e^{-R^{\\prime}(1-\\delta)},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for constant (independent of $B$ ) $c^{\\prime}:=e^{-\\|W^{\\star}\\|M}$ . Putting the above together yield: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathrm{CE}(\\widehat{W}_{B})-\\mathcal{H}\\ge\\widehat{\\pi}_{j}\\widehat{p}_{j,z}\\log\\Big(1+\\widehat{p}_{j,z}c^{\\prime}e^{-R^{\\prime}(1-\\delta)}\\Big)\\ge\\frac{c^{\\prime}e^{-R^{\\prime}(1-\\delta)}}{2n^{2}}\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the second inequality uses $\\textstyle\\log(1+x)\\geq{\\frac{x}{1+x}},x>0$ . ", "page_idx": 26}, {"type": "text", "text": "Compare this with (25). For large enough $B$ , (recall $R,R^{\\prime}$ grow at the same rate) it holds $\\begin{array}{r}{\\frac{c^{\\prime}}{2n^{2}}e^{-R^{\\prime}(1-\\delta)}>C e^{-R}}\\end{array}$ . Thus, $\\mathrm{CE}(\\widehat{\\pmb{W}}_{B})>\\mathrm{CE}({\\pmb{W}}_{B}^{\\star})$ , a contradiction.   \nIn either case, we arrive at a contradiction, which completes the proof. ", "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Sec. 2-5. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: Sec. 7 ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Detailed proofs of all results provided in Sec. E. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: See Sec. A. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [No] ", "page_idx": 29}, {"type": "text", "text": "Justification: As mentioned in Sec. A: the code is straightforward to reproduce, following the detailed specifications provided in the same section. For completeness, the code will be made publicly available online in the final version of the paper. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: As mentioned in Sec. A ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Sec. A. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We use deterministic full-batch optimization in the experiments starting from zero initialization. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Sec. A ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper involves foundational research on the optimization properties of next-token prediction, that has the potential to enable better understanding of operating regimes of language models with respect to optimization, generalization and robustness. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]