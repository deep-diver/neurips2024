[{"figure_path": "3PqhU96Vvv/figures/figures_2_1.jpg", "caption": "Figure 1: Low-rank modulations to drive context-aware processing. We present a biologically motivated, end-to-end trainable network model of dynamics in the visual pathway. Layers in the model are parameterized by recurrent (tuned) Excitatory (E) and (weakly tuned) Inhibitory (I) neural populations that interact bidirectionally with a higher-order layer in a low-rank manner. The low-rank modulatory factors serve to extract abstract context cues from sensory responses for subsequently driving neural dynamics into context-appropriate dynamical regimes.", "description": "This figure illustrates the architecture of the Dynamical Cortical Network (DCnet) model.  The model consists of multiple layers representing different stages of visual processing (V1, V2, etc.), each containing excitatory (E) and inhibitory (I) neural populations.  These layers are interconnected via local, lateral, and feedforward connections.  Critically, a higher-order layer sends low-rank modulatory signals back down to the sensory layers to guide context-driven processing. The low-rank modulations act as multiplicative factors, shaping the input-driven activity in the sensory areas based on higher-level context cues extracted from earlier parts of the trial.", "section": "3 General methods"}, {"figure_path": "3PqhU96Vvv/figures/figures_3_1.jpg", "caption": "Figure 2: Explicit context-guided modulation of sensory dynamics is necessary for learning generalizable solutions. a. We introduce vis-count, a parametric, visually-cued, delayed search task. On each trial, models are cued with a visual attribute (either a color, shape, or a conjunction of the two), and after a delay, a scene is presented. The task is to count and report the number of cue-consistent objects in the scene. b. On each of the three types of trails, we find that our model consistently outperforms state-of-the-art standard DNNs (Section. 3.2; Baselines) on novel held-out scenes. Implicit models refer to the condition where cues and scenes are presented simultaneously. The red dashed line denotes chance performance. c. A harder test of generalization on novel scenes and cues reveal that our model is robust to such variations, unlike performant implicit models.", "description": "This figure demonstrates the performance of the Dynamical Cortical Network (DCnet) model on the vis-count task compared to several baseline models. The vis-count task involves a cue-delay-search paradigm where the model first receives a cue (color, shape, or conjunction) and then a scene, and it must count the number of objects in the scene matching the cue.  Panel (a) illustrates the task. Panel (b) shows the model's superior performance on various types of trials compared to different Deep Neural Networks (DNNs).  Panel (c) demonstrates the model's strong generalization ability to novel scenes and cues, exceeding the performance of implicit models.", "section": "4 vis-count: A parametric cued visual search task"}, {"figure_path": "3PqhU96Vvv/figures/figures_4_1.jpg", "caption": "Figure 3: Neural network dynamics reveal context-dependent behavior. We perform dimensionality reduction on late layer model activities and visualize neural trajectories (Gray for individual trials; Dark-colored for trial averages) for different experimental manipulations. The solid dot indicates the trial start. a. For a fixed cue (color trials visualized here), network dynamics reflect the extraction and preservation of task-relevant information (object counts) while being invariant to task-irrelevant bottom-up responses from the different scenes. b. Matched cued vs. uncued trials for the same scene (inset) reveal a divergence of the bottom-up responses driven by the low-rank modulations.", "description": "This figure visualizes the neural network dynamics of the DCnet model in a cue-delay-visual search task.  Panel (a) shows how the model's activity, for a fixed cue, tracks the relevant information (number of objects meeting the cue criteria) while ignoring irrelevant details in the scene. Panel (b) compares the model's response to cued and uncued trials with the same scene, illustrating how the top-down cue modulates the bottom-up responses, leading to distinct neural trajectories.", "section": "4 vis-count: A parametric cued visual search task"}, {"figure_path": "3PqhU96Vvv/figures/figures_5_1.jpg", "caption": "Figure 4: A selectivity gradient emerges across the cortical layers for the different visual cues. We sequentially lesion the modulatory synapses in a trained DCnet from the higher-order layer to each sensory area, starting from the top-most (D) to the bottom-most (A) sensory area, while documenting task performance on each trial type. (E) indicates the intact DCnet. While late lesions have a minimal effect on performance in color trials, the opposite is true for shape trials. Early areas in the DCnet exhibit color selectivity, while later areas exhibit shape selectivity.", "description": "This figure shows a selectivity gradient across cortical layers for different visual cues (color, conjunctions, shape). By sequentially lesioning modulatory synapses from higher-order layers to sensory areas, the authors demonstrate that early areas exhibit color selectivity, while later areas show shape selectivity. This suggests a hierarchical processing of visual information where early layers process basic features and later layers integrate more complex information.", "section": "4.2 Results"}, {"figure_path": "3PqhU96Vvv/figures/figures_6_1.jpg", "caption": "Figure 5: In silico electrophysiology sheds light on tuning properties and excitation-inhibition dynamics. We drive network activity in DCnet with uncorrelated, time-varying Gaussian noise inputs. (a) We depict a randomly chosen neuron in early and late sensory areas of DCnet. Each E/I neuron pair shown here has matched receptive fields. Learned time constants reflect fast/slow integration in early/late areas, revealing a macroscopic gradient. By contrasting DCnet trained with and without inhibition, we find that inhibition plays a crucial role in (b) imparting stability and (c) expanding the dynamic range of computations in the network. More quantitative details are presented in Appendix. A.4.", "description": "This figure demonstrates the results of in silico electrophysiology experiments on the DCnet model.  Panel (a) shows the activity of a single excitatory/inhibitory neuron pair in early and late sensory areas, highlighting the different time constants learned by the network. Panels (b) and (c) compare the autocorrelation and dynamic range of the network's activity when trained with and without inhibition, showing that inhibition is crucial for stability and increased computational expressiveness.", "section": "4.2 Results"}, {"figure_path": "3PqhU96Vvv/figures/figures_7_1.jpg", "caption": "Figure 6: Slower reaction times/larger output entropy with more heterogeneous distractors. a. Task description. b. We parametrically vary the heterogeneity in distractor orientation (similar to [5]). c. In both target-present and target-absent trials, we find that the DCnet's output entropy is significantly different between low distractor heterogeneity vs. high distractor heterogeneity trials. Error bars represent the standard error of the mean.", "description": "This figure demonstrates the impact of distractor heterogeneity on the model's performance in a visual search task.  Panel (a) describes the task: determining if a target shape (L or T) is present in a scene filled with distractors (also L or T shapes).  Panel (b) shows how the experiment manipulates distractor heterogeneity: the orientation variability of distractor shapes increases from left to right. Panel (c) presents the key results, showing that higher distractor heterogeneity leads to significantly higher model entropy (a proxy for reaction time) in both target-present and target-absent trials.  This aligns with human performance in similar tasks, showing that increased distractor similarity slows reaction times.", "section": "5.1 The role of distractor heterogeneity"}, {"figure_path": "3PqhU96Vvv/figures/figures_8_1.jpg", "caption": "Figure 7: Differential reaction times/output entropy predictions for varying levels of target-distractor differences. a. Task description. b. We parametrically vary both the number of distracting stimuli in the scene and the target-distractor color difference [5]). c. DCnet's output entropy is invariant (linear) to the number of distracting stimuli when the feature difference is high (low), capturing the psychophysical phenomenon of \"popout.\" d. Model entropy as a function of target-distractor feature difference when marginalized over the number of distracting stimuli. Error bars represent the standard error of the mean.", "description": "This figure shows the results of a psychophysics experiment using the DCnet model.  It demonstrates the model's ability to replicate human behavior in a feature search task, showing how reaction time (represented by entropy) changes based on the number of distractors and the difference in features between the target and distractors.  Panel (c) and (d) specifically illustrate the \"pop-out\" effect, where a large difference in features between target and distractors makes reaction time independent of the number of distractors.", "section": "5 Model psychophysics on parametric cued feature searches"}, {"figure_path": "3PqhU96Vvv/figures/figures_14_1.jpg", "caption": "Figure 8: An emergent macroscopic gradient in neuron time constants. Cell-type specific integration time constants in DCnet are learnable parameters. While we initialize the T's uniformly (blue) pre-training, we observe layer-dependence post-training for both the (a) excitatory, and (b) inhibitory cell types.", "description": "This figure shows the distribution of excitatory (a) and inhibitory (b) neuron time constants at the start and end of training across four cortical layers.  The initial distribution is uniform, but after training, a gradient emerges.  The time constants (integration times) become progressively shorter in higher layers, suggesting that the network learns a hierarchical processing structure where information is integrated at different time scales in different cortical areas.", "section": "A.4 Mechanistic interpretability analyses"}, {"figure_path": "3PqhU96Vvv/figures/figures_15_1.jpg", "caption": "Figure 9: Model outputs. Visualizing example DCnet predictions on out-of-distribution (held-out) scenes across trial types.", "description": "This figure shows example outputs from the DCnet model on held-out test sets (out-of-distribution).  The table shows examples for color, shape, and conjunction trials. For each trial type, the cue (a color, shape, or combination), the scene image, the correct answer (count of objects matching the cue), and the model's prediction are presented. This visualization helps illustrate the model's generalization performance on unseen data.", "section": "A.5 Sample model outputs"}, {"figure_path": "3PqhU96Vvv/figures/figures_16_1.jpg", "caption": "Figure 10: DCnet performance on visual object recognition. Confusion matrix of DCnet outputs on the CIFAR-10 test set. Overall model performance is 84.79%.", "description": "This figure shows a confusion matrix visualizing the performance of the DCnet model on the CIFAR-10 object recognition benchmark.  The matrix displays the counts of correctly and incorrectly classified images for each of the ten classes (plane, car, bird, cat, deer, dog, frog, horse, ship, truck).  The diagonal elements represent the number of correctly classified images for each class, while off-diagonal elements show misclassifications between classes. The overall accuracy of the model on this dataset is reported as 84.79%. This demonstrates that the model architecture developed for context-aware visual search can be adapted for other image classification tasks.", "section": "A.6 DCnet on visual object recognition"}, {"figure_path": "3PqhU96Vvv/figures/figures_17_1.jpg", "caption": "Figure 1: Low-rank modulations to drive context-aware processing. We present a biologically motivated, end-to-end trainable network model of dynamics in the visual pathway. Layers in the model are parameterized by recurrent (tuned) Excitatory (E) and (weakly tuned) Inhibitory (I) neural populations that interact bidirectionally with a higher-order layer in a low-rank manner. The low-rank modulatory factors serve to extract abstract context cues from sensory responses for subsequently driving neural dynamics into context-appropriate dynamical regimes.", "description": "This figure illustrates the architecture of the Dynamical Cortical network (DCnet) model.  The model is a biologically-inspired, trainable neural network designed to simulate the visual pathway. It consists of multiple layers, each containing excitatory (E) and inhibitory (I) neural populations.  Crucially, a higher-order layer uses low-rank modulations to send top-down signals that flexibly adjust the processing in lower layers. This allows the model to incorporate context and abstract cues to guide its processing, making it context-aware.", "section": "3 General methods"}]