{"references": [{"fullname_first_author": "Lili Chen", "paper_title": "Decision transformer: Reinforcement learning via sequence modeling", "publication_date": "2021-00-00", "reason": "This paper introduces the Decision Transformer (DT), a core method that the current research builds upon and improves."}, {"fullname_first_author": "Scott Emmons", "paper_title": "RVS: What is essential for offline RL via supervised learning?", "publication_date": "2021-12-00", "reason": "This paper analyzes the core principles of Reinforcement Learning via Supervised Learning (RvS), providing the theoretical foundation for the current research."}, {"fullname_first_author": "Aviral Kumar", "paper_title": "Conservative Q-learning for offline reinforcement learning", "publication_date": "2020-00-00", "reason": "This paper introduces Conservative Q-learning, a related method that addresses some of the challenges tackled in the current research."}, {"fullname_first_author": "Dennis J Aigner", "paper_title": "On the estimation of production frontiers: maximum likelihood estimation of the parameters of a discontinuous density function", "publication_date": "1976-00-00", "reason": "This paper introduces expectile regression, a statistical method used in the current research for minimax return prediction."}, {"fullname_first_author": "Chen Tessler", "paper_title": "Action robust reinforcement learning and applications in continuous control", "publication_date": "2019-00-00", "reason": "This paper introduces the Noisy Action Robust MDP, a benchmark task used to evaluate the robustness of the proposed algorithm."}]}