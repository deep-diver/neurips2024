[{"figure_path": "WEf2LT8NtY/figures/figures_1_1.jpg", "caption": "Figure 1: LHS presents the game where decision-maker P is confronted by adversary A. In the worst-case scenario, if P chooses action ao, A will respond with \u01012, and if P chooses a1, A will counter with \u01014. Consequently, the worst-case returns for actions ao and a1 are 0 and 1, respectively. Therefore, the robust choice of action for the decision-maker is a1. RHS displays tables of action probabilities and the worst-case returns for the Decision Transformer (DT), Expected Return-Conditioned DT (ERC-DT) methods and our algorithm, when conditioned on the largest return-to-go 6. After training using uniformly collected data that covered all possible trajectories, the results reveal that DT fails to select the robust action a1, whereas our algorithm manages to do so.", "description": "This figure demonstrates a simple sequential game to illustrate the robustness of the proposed Adversarially Robust Decision Transformer (ARDT) against an adversary. The left-hand side (LHS) shows the game tree, where the decision-maker (P) chooses an action, followed by the adversary (A), resulting in a final reward. The right-hand side (RHS) presents the action probabilities and worst-case returns obtained by three different algorithms (DT, ERC-DT, and ARDT) when trained on uniformly collected data with full trajectory coverage. The results highlight ARDT's superior ability to select the robust action (a1) that maximizes the worst-case return, unlike DT and ERC-DT.", "section": "1 Introduction"}, {"figure_path": "WEf2LT8NtY/figures/figures_4_1.jpg", "caption": "Figure 2: Training of Adversarially Robust Decision Transformer. We adopt Expectile Regression for estimator Q to approximate the in-sample minimax. In the subsequent protagonist DT training, we replace the original returns-to-go with the learned values Q* to train policy.", "description": "This figure illustrates the two-step training process of the Adversarially Robust Decision Transformer (ARDT).  The left side shows the minimax expectile regression used to estimate the worst-case returns-to-go. This involves iteratively updating two networks: one to estimate the minimum return given the adversary's actions, and the other to estimate the maximum return given the protagonist's actions. The result is Q*, which represents the minimax returns-to-go. The right side shows the standard Decision Transformer (DT) training but now using Q* as the target return instead of the actual observed returns. This conditioning on worst-case returns makes the policy robust against adversarial actions.", "section": "3 Adversarially Robust Decision Transformer"}, {"figure_path": "WEf2LT8NtY/figures/figures_6_1.jpg", "caption": "Figure 3: Worst-case return versus target return plot comparing the proposed ARDT algorithm against vanilla DT, on our Single-stage Game (left), Gambling (centre) and our Multi-stage Game (right), over 10 seeds.", "description": "This figure compares the worst-case return achieved by three different algorithms (ARDT, DT, and ESPER) across three different game scenarios (Single-stage Game, Gambling, and Multi-stage Game) under varying target returns.  The shaded regions represent the standard deviation across 10 different runs. The plot illustrates how the worst-case performance of each algorithm changes as the target return is adjusted.  The results demonstrate ARDT's superior robustness to test-time adversaries compared to DT and ESPER, particularly when the target return is high.  In the single-stage game, ARDT consistently achieves the Nash Equilibrium, while DT and ESPER exhibit suboptimal worst-case performance. The Gambling and Multi-stage game results also show ARDT's consistently better performance.", "section": "4.1 Full Data Coverage Setting"}, {"figure_path": "WEf2LT8NtY/figures/figures_6_2.jpg", "caption": "Figure 4: Average return of ARDT and vanilla DT on Connect Four when trained on suboptimal datasets collected with different levels of optimality for both the online protagonist's policy (30%, 40% and 50% optimal) and the adversary's policy (10%, 30%, 50% optimal), over 10 seeds. We test against a fixed adversary that acts optimally 50% of the time, and randomly otherwise.", "description": "This figure displays box plots comparing the average returns achieved by Adversarially Robust Decision Transformer (ARDT) and vanilla Decision Transformer (DT) in the Connect Four game under various conditions.  The training data used for both ARDT and DT was suboptimal in the sense that the data was generated by agents that were only partially optimal (30%, 40%, 50%). The testing was done against an adversary that acted optimally only 50% of the time, with the remaining actions chosen randomly. The figure shows three sets of box plots, each corresponding to a different level of optimality for the protagonist (30%, 40%, 50%), and within each set there are three box plots, one for each level of adversary optimality (10%, 30%, 50%).  The results show that ARDT consistently outperforms DT.", "section": "4.2 Discrete Game with Partial Data Coverage"}, {"figure_path": "WEf2LT8NtY/figures/figures_7_1.jpg", "caption": "Figure 5: From left to right, (1) the worst-case return under adversarial perturbations with different weights in Halfcheetah, (2) in Hopper, and (3) average returns of algorithms in environments with different relative mass. The initial target returns in the environments Halfcheetah and Hopper are 2000 and 500, respectively.", "description": "This figure displays the performance comparison of DT, ESPER, and ARDT algorithms in three continuous control tasks under adversarial noise.  The left and middle plots show the worst-case returns for Halfcheetah and Hopper respectively, as the weight of adversarial noise (\u03b4) increases. The right plot presents the average return for Hopper as the relative mass changes. The results demonstrate ARDT's superior robustness and higher worst-case returns compared to DT and ESPER in these adversarial environments.", "section": "4.3 Continuous Adversarial Environments"}, {"figure_path": "WEf2LT8NtY/figures/figures_9_1.jpg", "caption": "Figure 6: Ablation study on expectile level \u03b1 over 10 random seeds. In connect four, we set the target return to be 1, and the adversary to be \u03b5-greedy (\u03b5 = 0.5), i.e., taking uniform random policy with probability 0.5. On Connect Four, ARDT with \u03b1 = 0.01 acts too conservative.", "description": "This figure displays the results of an ablation study on the impact of the expectile level (\u03b1) in the Adversarially Robust Decision Transformer (ARDT) algorithm.  Three different game environments were tested: a single-stage game, a multi-stage game, and Connect Four. Each game was tested against a worst-case adversary. The results are presented as line graphs showing average return against target return for different values of \u03b1 and boxplots showing the average return against the expectile level in the Connect Four game.  The study reveals that a smaller \u03b1 leads to better robustness in most scenarios, but in Connect Four, a too small alpha results in the algorithm acting too conservatively.  Overall, the figure illustrates the importance of tuning the expectile level (\u03b1) to balance robustness and performance.", "section": "4.4 Ablation Study"}, {"figure_path": "WEf2LT8NtY/figures/figures_15_1.jpg", "caption": "Figure 7: Multi-stage Game. LHS is the tree representation of the problem, where P and A are decision maker (protagonist) and adversary, respectively. RHS is the same game tree where we attach the learned minimax return and the action probabilities of the well-trained ARDT of a single run when conditioning on the target return 7 to the adversarial node and the branches, respectively. For example, the highlighted node has minimax return-to-go 4.9. 0.99 on the branch lead to the highlited node indicates the action probability of taking ao at the initial state. Thick lines represent the optimal adversarial actions.", "description": "This figure illustrates a multi-stage game between a protagonist (P) and an adversary (A). The left-hand side (LHS) shows the game tree, representing the sequential decision-making process. The right-hand side (RHS) presents the same game tree, but with added information from the Adversarially Robust Decision Transformer (ARDT) model.  The minimax return (the worst-case outcome for the protagonist) and action probabilities are shown for each node, highlighting the optimal strategy learned by ARDT when aiming for a target return of 7. The thicker lines indicate the optimal actions for the adversary.", "section": "4.1 Full Data Coverage Setting"}, {"figure_path": "WEf2LT8NtY/figures/figures_15_2.jpg", "caption": "Figure 8: Connect Four game. Two players put pieces to specific columns in turns. The pieces will drop to the final empty row.", "description": "This figure illustrates a game of Connect Four.  It shows three different stages of the game, each with a different arrangement of red and yellow pieces.  Connect Four is a two-player game where players take turns dropping colored pieces into a grid. The goal is to be the first to get four of one's own pieces in a row\u2014horizontally, vertically, or diagonally.", "section": "4.2 Discrete Game with Partial Data Coverage"}, {"figure_path": "WEf2LT8NtY/figures/figures_15_3.jpg", "caption": "Figure 7: Multi-stage Game. LHS is the tree representation of the problem, where P and A are decision maker (protagonist) and adversary, respectively. RHS is the same game tree where we attach the learned minimax return and the action probabilities of the well-trained ARDT of a single run when conditioning on the target return 7 to the adversarial node and the branches, respectively. For example, the highlighted node has minimax return-to-go 4.9. 0.99 on the branch lead to the highlited node indicates the action probability of taking ao at the initial state. Thick lines represent the optimal adversarial actions.", "description": "This figure shows a multi-stage game tree illustrating the decision-making process of a protagonist (P) against an adversary (A). The left side displays the game tree structure, while the right side presents the same tree augmented with minimax returns-to-go calculated by Adversarially Robust Decision Transformer (ARDT) and the corresponding action probabilities.  The numbers on the leaf nodes represent the game outcomes. Highlighted nodes and thick lines emphasize the optimal adversarial actions and their probabilities based on the ARDT model.", "section": "4.1 Full Data Coverage Setting"}, {"figure_path": "WEf2LT8NtY/figures/figures_18_1.jpg", "caption": "Figure 3: Worst-case return versus target return plot comparing the proposed ARDT algorithm against vanilla DT, on our Single-stage Game (left), Gambling (centre) and our Multi-stage Game (right), over 10 seeds.", "description": "This figure compares the worst-case return achieved by three different algorithms (ARDT, DT, and ESPER) across three different sequential games with varying target returns.  The worst-case return represents the minimum return achieved when the adversary acts optimally. The x-axis shows the target return, and the y-axis displays the worst-case return obtained by each algorithm. The plot demonstrates ARDT's superior robustness against optimal adversaries compared to DT and ESPER, particularly when the target return is high, indicating its ability to choose actions that yield high returns even in worst-case adversarial scenarios.", "section": "4.1 Full Data Coverage Setting"}, {"figure_path": "WEf2LT8NtY/figures/figures_18_2.jpg", "caption": "Figure 7: Multi-stage Game. LHS is the tree representation of the problem, where P and A are decision maker (protagonist) and adversary, respectively. RHS is the same game tree where we attach the learned minimax return and the action probabilities of the well-trained ARDT of a single run when conditioning on the target return 7 to the adversarial node and the branches, respectively. For example, the highlighted node has minimax return-to-go 4.9. 0.99 on the branch lead to the highlited node indicates the action probability of taking ao at the initial state. Thick lines represent the optimal adversarial actions.", "description": "This figure shows a multi-stage game tree. The left-hand side (LHS) presents the game tree structure, illustrating the decision-making process of both the protagonist (P) and the adversary (A). The right-hand side (RHS) displays the same game tree augmented with information from the adversarially robust decision transformer (ARDT) model. Specifically, it shows the minimax return-to-go values at each decision node and the action probabilities calculated by ARDT when given a target return of 7. The thicker branches indicate the optimal adversarial actions according to the ARDT model.", "section": "4.1 Full Data Coverage Setting"}, {"figure_path": "WEf2LT8NtY/figures/figures_19_1.jpg", "caption": "Figure 12: Results against stochastic adversary (e-greedy policy). (a) Expected return computed based on the policy of single random seed in Table 1 in the paper. (b) Average return conducted on Multi-stage Game, where the adversary has e-greedy policy (\u03b5 = 0.2). ARDT with a tuned expectile level achieved the best average return.", "description": "This figure compares the performance of Decision Transformer (DT), Expected Return-Conditioned Decision Transformer (ESPER), and Adversarially Robust Decision Transformer (ARDT) in a stochastic environment against an \u03b5-greedy adversary.  Subfigure (a) shows the expected return for different values of \u03b5 in a single-stage game, while subfigure (b) displays the average return in a multi-stage game with \u03b5 = 0.2.  ARDT demonstrates superior performance, particularly when a well-tuned expectile level is used.", "section": "4.2 Discrete Game with Partial Data Coverage"}, {"figure_path": "WEf2LT8NtY/figures/figures_19_2.jpg", "caption": "Figure 3: Worst-case return versus target return plot comparing the proposed ARDT algorithm against vanilla DT, on our Single-stage Game (left), Gambling (centre) and our Multi-stage Game (right), over 10 seeds.", "description": "This figure presents a comparison of the worst-case return achieved by three different algorithms (ARDT, DT, and ESPER) across three different games (Single-stage Game, Gambling Game, Multi-stage Game).  The x-axis represents the target return used as a condition during training, and the y-axis represents the worst-case return the algorithm achieved in a test setting with an optimal adversary. The plot shows that ARDT consistently outperforms both DT and ESPER across all three games, demonstrating its superior robustness in adversarial environments.", "section": "4.1 Full Data Coverage Setting"}]