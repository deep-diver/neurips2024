[{"figure_path": "NC0Bjl4uTf/figures/figures_3_1.jpg", "caption": "Figure 1: Flowchart of Chinese inertial generative adversarial network. The Chinese character \u201c\u6570\u201d is input into the model, and its one-hot encoding is converted into glyph encoding (green cubes), which is then input into GAN together with random noise (blue cubes of different colors).", "description": "This figure illustrates the CI-GAN architecture.  A Chinese character (in this case, \u201c\u6570\u201d) is initially encoded using a one-hot encoding scheme. This one-hot encoding is then converted into a glyph encoding, which represents the character's shape and stroke features.  The glyph encoding, along with a random noise vector, is fed into a Generative Adversarial Network (GAN). The GAN generates an inertial signal (acceleration and gyroscope data) mimicking the writing motion of the character. The process incorporates a Forced Optimal Transport Loss to ensure the generated signal's authenticity and to prevent mode collapse and mixing, common issues in GANs.  The figure visually represents these steps and the various components of the CI-GAN.", "section": "3 Method"}, {"figure_path": "NC0Bjl4uTf/figures/figures_4_1.jpg", "caption": "Figure 2: Diagram of semantic relevance alignment.", "description": "This figure illustrates the Semantic Relevance Alignment (SRA) module used in the CI-GAN.  The SRA module aligns the semantic relationships between generated inertial signals and the corresponding Chinese character glyphs.  It does this by comparing the cosine similarity matrices of the glyph encodings and the extracted features from the generated signals. The goal is to ensure that the relationships between character glyphs are accurately reflected in the generated signals. A pre-trained variational autoencoder (VAE) extracts the features from the signals.", "section": "3.3 Semantic Relevance Alignment"}, {"figure_path": "NC0Bjl4uTf/figures/figures_5_1.jpg", "caption": "Figure 3: The visualization results of the 6-axis signals recorded by the inertial sensor for different Chinese character writing movements and the corresponding generated signals. The left side is the original inertial sensor signal, the middle is the corresponding generated signal, and the right side is the reconstructed writing trajectory.", "description": "This figure visualizes the 6-axis inertial signals (3-axis acceleration and 3-axis gyroscope) for two different Chinese characters, \"\u79d1\" and \"\u5b66\".  For each character, it shows the real signals recorded from an inertial sensor during handwriting, the corresponding signals generated by CI-GAN, and the reconstructed 3D writing trajectory for both real and generated signals. The close similarity between the real and generated signals demonstrates CI-GAN's ability to accurately capture the movement patterns during handwriting.", "section": "4.2 Signal Generation Visualization"}, {"figure_path": "NC0Bjl4uTf/figures/figures_6_1.jpg", "caption": "Figure 3: The visualization results of the 6-axis signals recorded by the inertial sensor for different Chinese character writing movements and the corresponding generated signals. The left side is the original inertial sensor signal, the middle is the corresponding generated signal, and the right side is the reconstructed writing trajectory.", "description": "This figure visualizes the 6-axis inertial signals (3-axis acceleration and 3-axis gyroscope) generated by CI-GAN for two different Chinese characters (\"\u79d1\" and \"\u5b66\").  It compares the real inertial signals recorded during handwriting with the corresponding signals generated by the model.  The rightmost column shows the reconstructed writing trajectories based on both real and generated signals. This visualization demonstrates CI-GAN's ability to generate signals that closely resemble the real handwriting movements in terms of overall trends and characteristics.", "section": "4.2 Signal Generation Visualization"}, {"figure_path": "NC0Bjl4uTf/figures/figures_6_2.jpg", "caption": "Figure 5: The recognition accuracy of 6 classifiers with varied training samples provided by CI-GAN.", "description": "This figure shows the performance of six different classifiers (1DCNN, LSTM, Transformer, RF, XGBoost, and SVM) on a Chinese handwriting recognition task.  The x-axis represents the number of training samples used, ranging from approximately 1500 to 16500. The y-axis shows the accuracy of each classifier. The graph demonstrates how the accuracy of all classifiers improves significantly as the number of training samples increases, with the deep learning models (1DCNN, LSTM, Transformer) exhibiting the most substantial gains. This illustrates the effectiveness of CI-GAN in generating a large amount of high-quality training data to boost the performance of handwriting recognition models.", "section": "4.3 Comparative Experiments"}, {"figure_path": "NC0Bjl4uTf/figures/figures_8_1.jpg", "caption": "Figure 6: The t-SNE visualization of Chinese glyph encodings.", "description": "This figure visualizes the results of t-SNE dimensionality reduction applied to Chinese glyph encodings. Each point represents a Chinese character, and the proximity of points reflects the similarity of their glyphs' structural and stroke features.  The visualization reveals clusters of characters sharing similar visual characteristics, highlighting the effectiveness of the Chinese Glyph Encoding (CGE) in capturing semantic information beyond mere character meanings.", "section": "4.5 Visualization Analysis of Chinese Glyph Encoding"}, {"figure_path": "NC0Bjl4uTf/figures/figures_12_1.jpg", "caption": "Figure 7: Confusion matrices of different classifiers for recognition results of Chinese characters with similar glyphs.", "description": "This figure presents confusion matrices for six different classifiers (1DCNN, LSTM, Transformer, RF, XGBoost, and SVM) when classifying Chinese characters with similar glyphs. Each matrix visually represents the classifier's performance by showing the number of times each character was correctly classified and the number of times it was misclassified as other characters.  Darker blue cells indicate more frequent correct classifications, while lighter blue cells represent misclassifications. The diagonal of each matrix displays the number of correctly classified instances for each character. By analyzing these matrices, one can understand the performance of each classifier in distinguishing between similar-looking Chinese characters.", "section": "A.1 Performance of Classifiers on Similar Characters"}]