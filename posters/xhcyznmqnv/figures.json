[{"figure_path": "XHCYZNmqnv/figures/figures_2_1.jpg", "caption": "Figure 1: Illustration of the input space margin, margin in the feature space and margin consistency. The model preserves the relative position of samples to the decision boundary in the input space to the feature space.", "description": "This figure visually represents the concept of margin consistency.  The left side shows the input space, where data points (x1, x2, x3) are plotted relative to a decision boundary.  Circles around points illustrate the robustness, or distance to the boundary; smaller circles indicate less robustness. The right side shows the corresponding feature space (after the feature extractor). The points (z1, z2, z3) maintain their relative order and distance to the decision boundary, demonstrating margin consistency.  If the model were not margin-consistent, the relative positions of the points in feature space would not accurately reflect their robustness in the input space.", "section": "2 Methodology"}, {"figure_path": "XHCYZNmqnv/figures/figures_4_1.jpg", "caption": "Figure 1: Illustration of the input space margin, margin in the feature space and margin consistency. The model preserves the relative position of samples to the decision boundary from the input space to the feature space.", "description": "This figure visually explains the concept of margin consistency. It shows two spaces: the input space and the feature space. In the input space, the distance from a sample to the decision boundary is called the input margin.  Samples closer to the boundary are less robust and more likely to be misclassified by adversarial attacks.  The feature space represents the output of a feature extractor (e.g., penultimate layer of a deep neural network) applied to the input samples. The distance from a sample's feature representation to the decision boundary in this space is the feature margin. Margin consistency means that the relative order of samples based on input margins is preserved in the feature space.  If a sample has a smaller input margin than another, it will also have a smaller feature margin. This property is crucial because it allows the use of the easily computable feature margin (logit margin) as a proxy for the computationally expensive input margin in order to efficiently detect vulnerable samples.", "section": "2 Methodology"}, {"figure_path": "XHCYZNmqnv/figures/figures_4_2.jpg", "caption": "Figure 2: Illustration of Theorem 1's proof.", "description": "This figure illustrates the proof of Theorem 1, which establishes the relationship between margin consistency and the ability to use the logit margin to separate robust and non-robust samples.  Panel (a) shows that with margin consistency (a monotonic relationship between input margin and logit margin), a threshold on the logit margin can perfectly separate robust and non-robust samples. Panel (b) demonstrates that without margin consistency, such a separation is not possible, highlighting the necessity of margin consistency for reliable non-robust sample detection using the logit margin.", "section": "2.3 Non-robust Samples Detection"}, {"figure_path": "XHCYZNmqnv/figures/figures_5_1.jpg", "caption": "Figure 3: Margin consistency of various models: there is a strong correlation between input space margin and logit margin for most l\u221e robust models tested, the exceptions being DIO and XU80 on CIFAR10. See Table 1 for the references on the models. The correlations are given with standard error for the y-axis values in each interval.", "description": "This figure shows the strong correlation between input space margin and logit margin for various robust models tested on CIFAR10 and CIFAR100 datasets.  The x-axis represents the input margin and the y-axis represents the logit margin.  Each line corresponds to a different model, and the Kendall rank correlation coefficient (\u03c4) is provided for each model.  Most models show a strong positive correlation, indicating margin consistency, with the exceptions of DIO and XU80 on CIFAR10, which have weaker correlations. This suggests that the logit margin can be used effectively as a proxy for the input margin in most robust models for identifying vulnerable samples.", "section": "3.2 Results and Analysis"}, {"figure_path": "XHCYZNmqnv/figures/figures_6_1.jpg", "caption": "Figure 3: Margin consistency of various models: there is a strong correlation between input space margin and logit margin for most l\u221e robust models tested, the exceptions being DIO and XU80 on CIFAR10. See Table 1 for the references on the models. The correlations are given with standard error for the y-axis values in each interval.", "description": "This figure displays the margin consistency of various robust models by plotting the logit margin against the input margin for each model on the CIFAR10 and CIFAR100 datasets.  The strong correlation (except for DIO and XU80 on CIFAR10) between input and logit margins demonstrates that the models preserve the relative positions of samples to the decision boundary between the input and feature spaces.  This is a key finding of the paper, supporting the use of logit margin as a proxy for input margin in non-robust sample detection.", "section": "3.2 Results and Analysis"}, {"figure_path": "XHCYZNmqnv/figures/figures_7_1.jpg", "caption": "Figure 3: Margin consistency of various models: there is a strong correlation between input space margin and logit margin for most l\u221e robust models tested, the exceptions being DIO and XU80 on CIFAR10. See Table 1 for the references on the models. The correlations are given with standard error for the y-axis values in each interval.", "description": "This figure shows the relationship between input space margin and logit margin for various robust models on CIFAR10 and CIFAR100 datasets.  The x-axis represents the input space margin, and the y-axis represents the logit margin. Each line represents a different model, and the Kendall rank correlation (\u03c4) is shown for each model.  Most models show a strong positive correlation (high margin consistency), indicating that the logit margin is a good proxy for the input margin.  However, some models (DIO and XU80 on CIFAR10) show weak correlations (low margin consistency), suggesting that the logit margin may not be a reliable estimate of the input margin for these models.", "section": "3.2 Results and Analysis"}, {"figure_path": "XHCYZNmqnv/figures/figures_7_2.jpg", "caption": "Figure 5: The correlations between the input margin, the distance between the feature representations of samples and their closest adversaries (feature distance \u2013 ||h\u03b8(x) \u2013 h\u03b8(x\u2032)||), and the logit margin may be due to the local isometry of the feature extractor. See Table 1 for the specific references on the model ID. The correlations are given with standard error for the y-axis values in each interval.", "description": "This figure shows the correlations between three different metrics: input margin, feature distance, and logit margin.  It visually represents the relationships between these metrics across various models, suggesting that the local isometry (preservation of distances) of the feature extractor plays a significant role in margin consistency.  The correlations (with standard errors) are displayed for each model, providing insight into how well the feature space reflects the input space regarding the distance to the decision boundary.  Table 1 is referenced to identify the specific models involved.", "section": "3.2 Results and Analysis"}, {"figure_path": "XHCYZNmqnv/figures/figures_8_1.jpg", "caption": "Figure 6: Correlation improvement of the learned pseudo-margin over the logit margin for DI0 (Ding et al., 2020) and XU80 (Xu et al., 2023).", "description": "This figure displays the correlation improvement achieved by learning a pseudo-margin over using only the logit margin for two models, DI0 and XU80, that exhibit weak margin consistency.  The x-axis represents the input margin, and the y-axis shows the margin (either logit margin or learned pseudo-margin). The plots demonstrate that learning a pseudo-margin significantly improves the correlation between the input and feature space margins for these models.  Kendall's tau and the false positive rate at 95% true positive rate (FPR@95) are provided for both the logit margin and the learned pseudo-margin to quantify the improvement.", "section": "3.3 Learning a Pseudo-Margin"}, {"figure_path": "XHCYZNmqnv/figures/figures_16_1.jpg", "caption": "Figure 7: Variation of AUROC score for different threshold values \n\u03b5.", "description": "This figure shows the performance of the non-robust sample detection for various values of robustness threshold (epsilon). The plots show AUROC and AUPR curves for different epsilon values.  The results demonstrate that even with varying thresholds, the high margin consistency of the models allows the logit margin to serve as a good proxy for the detection task.", "section": "3.2 Results and Analysis"}, {"figure_path": "XHCYZNmqnv/figures/figures_17_1.jpg", "caption": "Figure 8: Equidistance of classifiers for CIFAR10 models. The boxplot reports the distances' minimum value, lower quartile (Q1), median, upper quartile (Q3), and maximum value.", "description": "This boxplot shows the distribution of distances between pairs of linear classifiers for CIFAR10 models.  The distances are calculated as the norm of the weight difference between each pair of classifiers.  The plot displays the minimum, first quartile (Q1), median, third quartile (Q3), and maximum values for the distribution of distances across all model pairs. It helps visualize the level of equidistance among the classifiers, which is a factor related to margin consistency.", "section": "3.2 Results and Analysis"}, {"figure_path": "XHCYZNmqnv/figures/figures_17_2.jpg", "caption": "Figure 8: Equidistance of classifiers for CIFAR10 models. The boxplot reports the distances' minimum value, lower quartile (Q1), median, upper quartile (Q3), and maximum value.", "description": "This figure shows boxplots visualizing the distribution of distances between pairs of linear classifiers for various CIFAR10 models.  The boxplot for each model displays the minimum, first quartile (Q1), median, third quartile (Q3), and maximum distances. This visualization helps assess the assumption of equidistance between linear classifiers, which is relevant for the logit margin approximation of input space margin in margin-consistent models.", "section": "3.2 Results and Analysis"}, {"figure_path": "XHCYZNmqnv/figures/figures_20_1.jpg", "caption": "Figure 10: Estimations of the robust accuracy reported by Robustbench using logit margins with only 500 samples are quite accurate both on CIFAR10 and CIFAR100 for strongly margin-consistent models. The numbers indicate the absolute difference between the two values, averaged over ten subsets. See Table 1 for the specific references on the model ID.", "description": "This figure displays the results of estimating robust accuracy using only a subset of 500 samples instead of the full test set.  It compares the robust accuracy estimated using the logit margin against the robust accuracy obtained with the full AutoAttack.  The results show that for strongly margin-consistent models, the estimations are highly accurate, demonstrating the efficiency of using logit margins as a proxy for robust accuracy.", "section": "3.2 Results and Analysis"}, {"figure_path": "XHCYZNmqnv/figures/figures_20_2.jpg", "caption": "Figure 7: Variation of AUROC score for different threshold values  \u2208.", "description": "This figure shows the variation of the AUROC (Area Under the Receiver Operating Characteristic curve) and AUPR (Area Under the Precision-Recall curve) scores with different values of the robustness threshold (epsilon). It demonstrates how well the model can discriminate between robust and non-robust samples for various robustness thresholds. The AUROC and AUPR scores are metrics that evaluate the performance of the model in distinguishing between these two classes of samples. A higher value indicates better performance.", "section": "3.2 Results and Analysis"}]