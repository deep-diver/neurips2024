[{"figure_path": "93gz2lmFtm/figures/figures_0_1.jpg", "caption": "Figure 1: UNION discovers mobile objects (e.g., cars, pedestrians, cyclists) in an unsupervised manner by exploiting LiDAR, camera, and temporal information jointly. The key observation is that mobile objects can be distinguished from background objects (e.g., buildings, trees, poles) by grouping object proposals with similar visual appearance, i.e., clustering their appearance embeddings, and selecting appearance clusters that contain at least X% dynamic instances.", "description": "This figure illustrates the UNION method's pipeline for unsupervised 3D object detection.  It starts with LiDAR-based object proposals (1), followed by temporal motion estimation to separate static and dynamic objects (2). Camera visual appearance is then encoded using a DINOv2 embedding module (3). These visual and motion features are then fused to classify appearance clusters as mobile or not (4, 5). Finally, a standard 3D object detector is trained using the identified mobile clusters' pseudo-bounding boxes (6).", "section": "3 Method"}, {"figure_path": "93gz2lmFtm/figures/figures_1_1.jpg", "caption": "Figure 2: Comparison of the various designs for unsupervised 3D object discovery. (a) Most object discovery methods exploit LiDAR to generate pseudo-bounding boxes and use these to train a detector in a class-agnostic setting followed by self-training. (b) Wang et al. [27] generate pseudo-bounding boxes similar to (a) but alternate between training a LiDAR-based detector and a camera-based detector for self-training. (c) UNION: multi-modal multi-class 3D object discovery (ours)", "description": "This figure compares different approaches to unsupervised 3D object discovery.  (a) shows the common LiDAR-only approach using self-training. (b) shows a multi-modal approach (LiDAR and camera) also using self-training. (c) presents the proposed UNION method, which uses both LiDAR and camera data but without self-training, enabling multi-class object detection.", "section": "2 Related work"}, {"figure_path": "93gz2lmFtm/figures/figures_1_2.jpg", "caption": "Figure 2: Comparison of the various designs for unsupervised 3D object discovery. (a) Most object discovery methods exploit LiDAR to generate pseudo-bounding boxes and use these to train a detector in a class-agnostic setting followed by self-training. (b) Wang et al. [27] generate pseudo-bounding boxes similar to (a) but alternate between training a LiDAR-based detector and a camera-based detector for self-training. (c) UNION: multi-modal multi-class 3D object discovery (ours)", "description": "This figure compares different approaches for unsupervised 3D object discovery.  (a) shows the typical LiDAR-only approach using self-training. (b) illustrates a multi-modal approach also employing self-training, alternating between LiDAR and camera data. (c) presents the UNION method, a multi-modal approach that avoids self-training by directly using appearance-based pseudo-classes.", "section": "Related work"}, {"figure_path": "93gz2lmFtm/figures/figures_1_3.jpg", "caption": "Figure 2: Comparison of the various designs for unsupervised 3D object discovery. (a) Most object discovery methods exploit LiDAR to generate pseudo-bounding boxes and use these to train a detector in a class-agnostic setting followed by self-training. (b) Wang et al. [27] generate pseudo-bounding boxes similar to (a) but alternate between training a LiDAR-based detector and a camera-based detector for self-training. (c) UNION: multi-modal multi-class 3D object discovery (ours)", "description": "This figure compares different approaches for unsupervised 3D object discovery.  (a) shows the traditional LiDAR-only approach using self-training. (b) illustrates a multi-modal approach that still uses self-training.  (c) presents the UNION method, which uses multi-modal data and avoids self-training.", "section": "1 Introduction"}, {"figure_path": "93gz2lmFtm/figures/figures_6_1.jpg", "caption": "Figure 3: Qualitative results for the UNION pipeline compared to the ground truth annotations. (a) HDBSCAN (step 1 in Figure 1): object proposals (spatial clusters) in black. (b) Scene flow (step 2 in Figure 1): static and dynamic object proposals in black and red, respectively. (c) UNION: static and dynamic mobile objects in green and red, respectively. (d) Ground truth: mobile objects in blue.", "description": "This figure shows a qualitative comparison of object detection results from different stages of the UNION pipeline and the ground truth. (a) shows the object proposals generated by HDBSCAN, a spatial clustering algorithm. (b) shows the results of scene flow estimation, differentiating between static (black) and dynamic (red) objects. (c) shows the final results of the UNION pipeline, highlighting static (green) and dynamic (red) mobile objects. (d) shows the ground truth annotations for comparison.  The figure visually demonstrates how UNION effectively combines spatial, temporal, and visual information to improve object detection accuracy, particularly regarding the identification of static mobile objects.", "section": "4 Experiments"}, {"figure_path": "93gz2lmFtm/figures/figures_8_1.jpg", "caption": "Figure 4: The dynamic object proposal fractions of the visual appearance clusters. We use a threshold of 5% for selecting clusters.", "description": "This figure shows the percentage of dynamic object proposals within each of the 20 appearance clusters generated by the UNION method. The x-axis represents the cluster ID, and the y-axis shows the fraction of dynamic proposals in each cluster.  A horizontal red line indicates a threshold of 5%.  Clusters above this threshold are considered to contain enough dynamic instances and are classified as 'mobile clusters', suggesting the presence of mobile objects within them. Clusters below the line are considered 'non-mobile'. This visual representation helps to illustrate the effectiveness of the appearance-based clustering in identifying mobile objects.", "section": "4 Experiments"}]