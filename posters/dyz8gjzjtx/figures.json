[{"figure_path": "dyZ8GJZjtX/figures/figures_1_1.jpg", "caption": "Figure 1: (a) Expert activation distribution on XNLI [10] corpus, encompassing 6 parallel expert layers with 32 experts per layer. (b) MH-MoE showcases finer-grained understanding by distributing sub-tokens split from semantically-rich patches to more distinct experts to capture semantic information. Brighter regions indicate that sub-tokens split from this patch are distributed to a greater number of different experts.", "description": "This figure shows the expert activation distribution in both SMoE and MH-MoE. Subfigure (a) compares the expert activation distribution of SMoE and MH-MoE on the XNLI corpus, highlighting that MH-MoE achieves significantly higher expert activation. Subfigure (b) illustrates how MH-MoE achieves a more fine-grained understanding of semantically rich image patches by distributing sub-tokens to a wider range of experts.", "section": "1 Introduction"}, {"figure_path": "dyZ8GJZjtX/figures/figures_2_1.jpg", "caption": "Figure 2: Workflow of MH-MoE. For vision data, different heads routed to different experts try to capture different aspects of details within patches and relations between patches. For language data, different heads attend to capture the varying contexts of false cognates across different languages (e.g., Italian and English) or polysemous words within a single language.", "description": "This figure illustrates the workflow of the Multi-Head Mixture-of-Experts (MH-MoE) model.  It shows how input tokens (both vision and language data) are split into sub-tokens, which are then independently processed by different expert networks. The sub-tokens are assigned to experts based on their content (using a routing mechanism). This parallel processing improves the model's ability to handle nuanced linguistic and visual information. Finally, the processed sub-tokens are merged to produce the final output. The example uses images with captions in English and Italian. Note that the experts are attending to different aspects of the image and different meanings of the word \u201ccamera\u201d in the two languages.", "section": "2 Background"}, {"figure_path": "dyZ8GJZjtX/figures/figures_3_1.jpg", "caption": "Figure 3: Illustration of a typical SMoE layer and the proposed MH-MoE layer. (a) A SMOE layer consists of a router and expert networks, where the experts are sparsely activated according to dot-product token-expert routing scores. (b) MH-MoE introduces additional two MLP layers, namely the multi-head layer and merge layer to split and merge tokens, respectively.", "description": "This figure compares the architectures of a standard Sparse Mixture of Experts (SMoE) layer and the proposed Multi-Head Mixture of Experts (MH-MoE) layer.  The SMoE architecture shows a single token being routed to a subset of expert networks based on routing scores.  The MH-MoE architecture shows the input token being split into multiple sub-tokens, each independently routed to expert networks. A merging layer then combines the outputs of the experts into a single output vector for the next layer. The key difference is that MH-MoE uses a multi-head mechanism to improve expert utilization and capture information from different representation spaces.", "section": "3 Method"}, {"figure_path": "dyZ8GJZjtX/figures/figures_5_1.jpg", "caption": "Figure 4: Perplexity on validation dataset during the training phase reported for Dense, X-MoE and MH-MoE across three pre-training tasks.", "description": "The figure displays the perplexity curves during training for three pre-training tasks: English-focused language modeling, multi-lingual language modeling, and masked multi-modal modeling.  Four model variations are compared: Dense (a traditional dense model), X-MoE (a standard sparse mixture-of-experts model), and MH-MoE with both 8 and 32 experts. The plots illustrate the performance of each model across different training steps. The results show that MH-MoE consistently achieves lower perplexity than the other models, indicating improved learning efficiency and better language representation. The perplexity decrease is also more pronounced in MH-MoE as the number of experts increases.", "section": "4.2 Perplexity Evaluation"}, {"figure_path": "dyZ8GJZjtX/figures/figures_6_1.jpg", "caption": "Figure 4: Perplexity on validation dataset during the training phase reported for Dense, X-MoE and MH-MoE across three pre-training tasks.", "description": "The figure shows the perplexity curves for three different pre-training tasks (English-focused language modeling, multi-lingual language modeling, and masked multi-modal modeling) across different training steps for three different models (Dense, X-MoE, and MH-MoE).  It demonstrates the performance of MH-MoE compared to the baseline models across various pre-training tasks.  The lower perplexity indicates better performance, which is shown consistently by MH-MoE in all tasks. It also shows the impact of increasing the number of experts from 8 to 32.", "section": "4.2 Perplexity Evaluation"}, {"figure_path": "dyZ8GJZjtX/figures/figures_7_1.jpg", "caption": "Figure 6: Distribution of expert activation in X-MoE and MH-MoE on both Harness and XNLI corpus, where h denotes the number of heads.", "description": "This figure compares the distribution of expert activation in X-MoE and MH-MoE models across different numbers of heads (h).  It shows heatmaps representing the activation frequency of each expert across multiple layers for each model.  The heatmaps visually demonstrate that MH-MoE achieves a more even distribution of expert activation compared to X-MoE, indicating a more efficient utilization of experts. The impact of increasing the number of heads (h) on expert activation is also illustrated.", "section": "5 Analysis"}, {"figure_path": "dyZ8GJZjtX/figures/figures_7_2.jpg", "caption": "Figure 7: (a) Upstream training perplexity (\u2193) when scaling the number of experts for both X-MoE and MH-MoE. (b) Accuracy scores on the hellaswag task when scaling the number of experts for both X-MoE and MH-MoE. (c) Comparison for sub-tokens assign diversity (the number of different experts they are routed to) for P&F and Non P&F tokens.", "description": "This figure presents a comparative analysis of X-MoE and MH-MoE across three key aspects: upstream training perplexity, downstream accuracy on the hellaswag task, and sub-token assignment diversity.  Panel (a) shows that MH-MoE consistently achieves lower perplexity than X-MoE as the number of experts increases, demonstrating its superior training efficiency. Panel (b) illustrates that MH-MoE yields higher accuracy on the hellaswag task compared to X-MoE, highlighting its improved performance in downstream tasks. Lastly, panel (c) reveals that MH-MoE exhibits a greater diversity in sub-token assignment, distributing sub-tokens to a wider range of experts, especially for polysemous and false cognate tokens (P&F), indicating enhanced semantic understanding.", "section": "4.4 Ablation Studies"}, {"figure_path": "dyZ8GJZjtX/figures/figures_15_1.jpg", "caption": "Figure 1: (a) Expert activation distribution on XNLI [10] corpus, encompassing 6 parallel expert layers with 32 experts per layer. (b) MH-MoE showcases finer-grained understanding by distributing sub-tokens split from semantically-rich patches to more distinct experts to capture semantic information. Brighter regions indicate that sub-tokens split from this patch are distributed to a greater number of different experts.", "description": "This figure shows the expert activation distribution in the standard SMoE model and the proposed MH-MoE model.  Figure 1(a) compares the activation ratio of experts in each layer for both models on the XNLI corpus.  It demonstrates that MH-MoE achieves significantly higher expert activation. Figure 1(b) illustrates that MH-MoE achieves a finer-grained understanding of semantically rich image patches by distributing sub-tokens from these patches to a more diverse set of experts, indicated by brighter colors in the image. This suggests that MH-MoE can better capture subtle differences in complex data through the use of multiple representation spaces across the experts.", "section": "1 Introduction"}, {"figure_path": "dyZ8GJZjtX/figures/figures_15_2.jpg", "caption": "Figure 8: Comparison results for different splitting method. K denotes the size of kernel while S denotes the size of stride in Conv1D.", "description": "This figure compares the performance of different kernel sizes (K) and strides (S) used in a Conv1D layer for token splitting in the MH-MoE model.  It shows that varying the kernel size and stride affects the model's performance, measured by perplexity on a downstream task.  The best performing configuration indicates that finding optimal parameters for token splitting is crucial for achieving the best results with MH-MoE.", "section": "E Ablation Studies"}, {"figure_path": "dyZ8GJZjtX/figures/figures_16_1.jpg", "caption": "Figure 4: Perplexity on validation dataset during the training phase reported for Dense, X-MoE and MH-MoE across three pre-training tasks.", "description": "The figure showcases the validation perplexity curves for Dense, X-MoE, and MH-MoE across three pre-training tasks (English-focused language modeling, multi-lingual language modeling, and masked multi-modal modeling).  The curves show the perplexity over training steps for different model architectures and the number of experts used. MH-MoE consistently demonstrates lower perplexity than X-MoE and Dense models. The perplexity decreases as the number of experts increases for all models, highlighting improved model capability with more parameters.", "section": "4.2 Perplexity Evaluation"}, {"figure_path": "dyZ8GJZjtX/figures/figures_16_2.jpg", "caption": "Figure 10: Validation perplexity reported for both X-MoE and MH-MoE.", "description": "This figure shows the validation perplexity curves for both X-MoE and MH-MoE models with varying numbers of experts (8, 32, 64, 128, 256) during the training process.  It illustrates the performance of the models across different scales, allowing for a comparison of their learning efficiency and overall performance. The lower the perplexity, the better the model's performance. The curves clearly show how MH-MoE consistently outperforms X-MoE across all scales and expert counts, demonstrating its improved learning efficiency. The results also demonstrate that increasing the number of experts generally leads to lower perplexity, highlighting the scalability of both models.", "section": "4.2 Perplexity Evaluation"}, {"figure_path": "dyZ8GJZjtX/figures/figures_17_1.jpg", "caption": "Figure 1: (a) Expert activation distribution on XNLI [10] corpus, encompassing 6 parallel expert layers with 32 experts per layer. (b) MH-MoE showcases finer-grained understanding by distributing sub-tokens split from semantically-rich patches to more distinct experts to capture semantic information. Brighter regions indicate that sub-tokens split from this patch are distributed to a greater number of different experts.", "description": "This figure shows two key aspects of the proposed Multi-Head Mixture-of-Experts (MH-MoE) model. (a) compares the expert activation distribution in MH-MoE and the standard Sparse Mixture-of-Experts (SMoE) model on the XNLI dataset.  It highlights that MH-MoE achieves significantly higher expert activation (90.71% vs 8.33%), indicating better utilization of model capacity. (b) illustrates how MH-MoE processes semantically rich image patches by splitting tokens into sub-tokens and assigning them to multiple distinct experts. This allows for a more nuanced and granular understanding of semantic information within the image.", "section": "1 Introduction"}]