{"references": [{"fullname_first_author": "N. Shazeer", "paper_title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer", "publication_date": "2017-00-00", "reason": "This paper introduces the sparsely-gated mixture-of-experts layer, a foundational concept upon which the current work builds and improves."}, {"fullname_first_author": "D. Lepikhin", "paper_title": "Gshard: Scaling giant models with conditional computation and automatic sharding", "publication_date": "2020-00-00", "reason": "This paper presents GShard, a crucial method for training large models efficiently, directly relevant to the challenges addressed in the current research."}, {"fullname_first_author": "Z. Chi", "paper_title": "On the representation collapse of sparse mixture of experts", "publication_date": "2022-00-00", "reason": "This paper analyzes a key limitation of sparse Mixture-of-Experts models, providing context and motivation for the current work's proposed solution."}, {"fullname_first_author": "A. Clark", "paper_title": "Unified scaling laws for routed language models", "publication_date": "2022-00-00", "reason": "This paper investigates scaling laws for language models, offering insights into the efficiency trade-offs relevant to the current research."}, {"fullname_first_author": "W. Fedus", "paper_title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity", "publication_date": "2022-00-00", "reason": "This paper introduces Switch Transformers, an important advancement in efficient large model training, which informs the current work's approach."}]}