[{"figure_path": "dyZ8GJZjtX/tables/tables_5_1.jpg", "caption": "Table 2: Accuracy / accuracy-normalization scores for language understanding tasks using the LLM Evaluation Harness [14]. N denotes the number of experts.", "description": "This table presents the results of several language understanding tasks using the LLM Evaluation Harness.  The models compared are Dense, X-MoE (with 8 and 32 experts), and MH-MoE (with 8 and 32 experts).  The performance is measured using accuracy and accuracy-normalized scores across multiple benchmarks including ARC-Challenge, ARC-Easy, RTE, BookQA, Winogrande, PiQA, BoolQ, HellaSwag, and TruthfulQA.  The table shows the performance improvement achieved by MH-MoE over the baseline models (Dense and X-MoE).", "section": "4.3 Downstream Evaluation"}, {"figure_path": "dyZ8GJZjtX/tables/tables_5_2.jpg", "caption": "Table 1: Results of upstream perplexity evaluation. We report the validation perplexity cross two setting: 8 experts and 32 experts.", "description": "This table presents the results of the upstream perplexity evaluation for the English-focused language modeling, multi-lingual language modeling, and masked multi-modal modeling tasks.  The perplexity, a measure of how well the model predicts the next word in a sequence, is reported for two different expert settings: 8 experts and 32 experts. Lower perplexity values indicate better model performance.", "section": "4.2 Perplexity Evaluation"}, {"figure_path": "dyZ8GJZjtX/tables/tables_6_1.jpg", "caption": "Table 3: Accuracy / accuracy-normalization scores on multilingual understanding tasks using the LLM Evaluation Harness [14]. N denotes the number of experts.", "description": "This table presents the results of multilingual understanding tasks evaluated using the LLM Evaluation Harness.  It compares the performance of four different models: Dense, X-MoE (with 8 and 32 experts), and MH-MoE (with 8 and 32 experts). The performance is measured using accuracy and accuracy normalization scores across 14 different languages.  The table shows how the proposed MH-MoE model outperforms other models, particularly when a larger number of experts are used.", "section": "4.3 Downstream Evaluation"}, {"figure_path": "dyZ8GJZjtX/tables/tables_6_2.jpg", "caption": "Table 4: Results of visual question answering, visual reasoning, and image captioning tasks.", "description": "This table presents the performance of three different models (Dense, X-MoE, and MH-MoE) on three downstream tasks: Visual Question Answering (VQAv2), Numerical Visual Reasoning (NLVR2), and COCO Captioning.  The results show the performance metrics for each model on the respective datasets, demonstrating the improvement achieved by MH-MoE over the baseline models.", "section": "4.3 Downstream Evaluation"}, {"figure_path": "dyZ8GJZjtX/tables/tables_7_1.jpg", "caption": "Table 5: Ablation studies of MH-MoE components: MLP layers and the Token-Splitting-Merging (TSM, Eq. 3 and Eq. 6) operation.", "description": "This table presents the results of ablation experiments conducted to analyze the impact of different components of the MH-MoE model on its performance. Specifically, it investigates the contributions of MLP layers and the Token-Splitting-Merging (TSM) operation. The table shows the perplexity achieved by different model variations: Dense (baseline without any MoE components), Dense with MLP layers, X-MoE (another MoE baseline without MH-MoE components), X-MoE with MLP layers, MH-MoE without TSM, MH-MoE without MLP layers, and the full MH-MoE model.  The results highlight the individual and combined effects of the MLP layers and TSM on the model's performance, indicating the importance of both for optimal results.", "section": "4.4 Ablation Studies"}, {"figure_path": "dyZ8GJZjtX/tables/tables_8_1.jpg", "caption": "Table 6: Performance across multiple pure vision tasks: classification (CLS) on ImageNet-1k, object detection (OD) and instance segmentation (IS) on COCO. The number of expert is set to 8.", "description": "This table presents the performance comparison of three different models (Dense, SMoE, and MH-MoE) on three pure vision tasks: classification on ImageNet-1k, object detection, and instance segmentation on COCO.  The number of experts used in the SMoE and MH-MoE models is 8. The metrics used for evaluation are accuracy (ACC) for classification and Average Precision (AP), AP at 50% IoU (AP50), AP at 75% IoU (AP75), and Average Precision for instance segmentation (APmask) for the detection and segmentation tasks.", "section": "4.3 Downstream Evaluation"}, {"figure_path": "dyZ8GJZjtX/tables/tables_12_1.jpg", "caption": "Table 8: Pretraining data of Masked multi-modal modeling task. All the data are academically accessible.", "description": "This table details the datasets used for pre-training the Masked Multi-modal model.  It lists the type of data (Image-Text Pairs, Images, and Text), their respective sources (e.g., Conceptual Captions, ImageNet-21K, English Wikipedia), and their sizes (number of pairs, images, and total size in GB).  All data sources are publicly available for academic use.", "section": "4.1 Experimental Setup"}, {"figure_path": "dyZ8GJZjtX/tables/tables_12_2.jpg", "caption": "Table 9: Model hyperparameters of Dense, X-MoE and MH-MoE. The SMoE frequency refers to how many experts each token will be assigned to, i.e., the value of k in the Top-k expert selection.", "description": "This table shows the hyperparameters used for three different model architectures: Dense, X-MoE, and MH-MoE.  It details the settings for various aspects of the models, including the number of feed-forward networks (FFNs) within each layer, expert embedding dimensions, initialized gating temperature, number of transformer blocks, hidden size, FFN inner hidden size, number of attention heads, and the frequency with which sparse mixture-of-experts (SMoE) are activated.", "section": "4.1 Experimental Setup"}, {"figure_path": "dyZ8GJZjtX/tables/tables_13_1.jpg", "caption": "Table 10: Pre-training hyperparameters for Language modeling tasks (English-focused language modeling and Multi-lingual language modeling tasks) and Masked multi-modal modeling task tasks.", "description": "This table details the hyperparameters used during the pre-training phase for three distinct tasks: English-focused language modeling, multi-lingual language modeling, and masked multi-modality modeling.  For each task, it specifies settings such as batch size, optimizer, learning rate schedule, and other key training parameters. The differences in hyperparameters across tasks reflect the varying nature and requirements of each modeling approach.", "section": "4 Experiments"}, {"figure_path": "dyZ8GJZjtX/tables/tables_13_2.jpg", "caption": "Table 11: Parameter count setting of X-MoE and MH-MoE in our experiments for English-focused language modeling, Multi-lingual language modeling and Masked multi-modality modeling tasks. \u201cnon-expert param\u201d refers to the parameters that are not part of the expert networks, such as the attention layer, router, etc., while \u201cexpert params\u201d represents the total number of parameters in the parallel expert networks. For Dense models, since there are no expert network layers, we only list the total number of parameters. All models under the same task utilize the same architecture and hyperparameters, following identical training settings and steps.", "description": "This table details the parameter counts for the Dense, X-MoE, and MH-MoE models across three pre-training tasks with varying numbers of experts.  It breaks down the parameter counts into \"non-expert\" parameters (shared components) and \"expert\" parameters (those specific to each expert). The table highlights the impact of adding experts on the overall model size for each of the three tasks.", "section": "4.1 Experimental Setup"}, {"figure_path": "dyZ8GJZjtX/tables/tables_15_1.jpg", "caption": "Table 13: Comparison results for different numbers of MLP layers n. The results are averaged over five runs.", "description": "This table presents the results of experiments evaluating the impact of varying the number of MLP layers (n) on the model's performance.  The results are averaged over five runs to account for variations due to randomness in the training process.  The table shows the upstream perplexity (a measure of the model's performance on a language modeling task) and downstream performance on three different Natural Language Understanding (NLU) tasks: RTE, PIQA, and Winogrande.  The aim is to determine if adding more MLP layers improves the model's overall performance.", "section": "4.4 Ablation Studies"}]