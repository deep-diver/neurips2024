[{"type": "text", "text": "Multi-Head Mixture-of-Experts ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xun Wu, Shaohan Huang , Wenhui Wang, Shuming Ma, Li Dong, Furu Wei Microsoft Research Asia ", "page_idx": 0}, {"type": "text", "text": "xunwu@microsoft.com, shaohanh@microsoft.com, fuwei@microsoft.com https://aka.ms/GeneralAI ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sparse Mixtures of Experts (SMoE) scales model capacity without significant increases in computational costs. However, it exhibits the low expert activation issue, i.e., only a small subset of experts are activated for optimization, leading to suboptimal performance and limiting its effectiveness in learning a larger number of experts in complex tasks. In this paper, we propose Multi-Head Mixture-of-Experts (MH-MoE). MH-MoE split each input token into multiple sub-tokens, then these sub-tokens are assigned to and processed by a diverse set of experts in parallel, and seamlessly reintegrated into the original token form. The above operations enables MH-MoE to significantly enhance expert activation while collectively attend to information from various representation spaces within different experts to deepen context understanding. Besides, it\u2019s worth noting that our MH-MoE is straightforward to implement and decouples from other SMoE frameworks, making it easy to integrate with these frameworks for enhanced performance. Extensive experimental results across different parameter scales (300M to 7B) and three pre-training tasks\u2014English-focused language modeling, multi-lingual language modeling and masked multi-modality modeling\u2014along with multiple downstream validation tasks, demonstrate the effectiveness of MH-MoE. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large capacity models, such as Large Language Models (LLMs) [39, 28, 6, 25] and Large Multimodal Models (LMMs) [37, 27], have demonstrated their efficacy across various domains and tasks. To further enhance performance, a reliable approach involves scaling up these models by augmenting the parameter count [13]. But for most of these densely-activated large-capacity models (referred to as Dense models), which utilize all their parameters to process all inputs, the extremely large size of these models significantly reduces inference speed, further limiting their practicality. ", "page_idx": 0}, {"type": "text", "text": "A promising alternative, facilitating model scalability while mitigating the burdensome computational costs, resides in Sparse Mixtures of Experts (SMoE) [31, 12, 5, 7]. In contrast to Dense model, SMoE contains parallel feed-forward neural networks (referred to as experts) within each building block, and strategically activates distinct experts for specific input tokens via a router, thereby yielding noteworthy efficiency enhancements. For instance, GShard [21] scales a Dense model from 2B to 600B parameters with lower training costs than a 100B Dense model. And recently, Mixtral ${\\bf8}\\!\\times\\!7{\\bf B}$ [16], a SMoE model containing 8 experts is shown to outperform or matches LLaMA-2 70B [34] and GPT-3.5. ", "page_idx": 0}, {"type": "text", "text": "Despite its success, SMoE exhibits the low experts activation issue, which means that only a small subset of experts are activated during optimization and inference, e.g., $\\mathbf{8.33\\%}$ activation ratio1 shown in Figure 1 (a), while the majority of them are not used at all (see the dark area). As a result, SMoE fails to utilize the full expressive power of these experts, especially when the number of experts is large, which significantly limits effectiveness and scalability of SMoE. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Our aim in this paper is to achieve denser expert activation (i.e., better utilization of \u201cdead\u201d experts) without increase in computational cost. To achieve this, we propose Multi-Head Mixture-ofExperts (MH-MoE). The workflow of MH-MoE is illustrated in Figure 2. Inspired by the multi-head mechanism utilized in Multi-Head Self-Attention (MHSA) block, MH-MoE splits each input token into multiple sub-tokens and distribute them to different experts. After expert processing, subtokens are seamlessly reintegrated into the original token form, thereby achieving denser expert activation, e.g., $90.71\\%$ activation in Figure 1 (a), while also circumventing any additional computational burden in subsequent non-parallel layers, e.g., MHSA block. Specifically, as shown in Figure 2, when provided with a single input token, MH-MoE activates four experts by splitting it into four sub-tokens, whereas SMoE only activates a single expert. ", "page_idx": 1}, {"type": "text", "text": "Furthermore, we observe an interesting phenomenon: for tokens with richer semantic information, the sub-tokens split from these tokens are more likely to be allocated to distinct experts. For example, refer to the bright area in Figure 1 (b), where sub-tokens split from these patches are allocated to a greater number of different experts, facilitating the capture of semantically-rich information (e.g., the eagle in the figure). We therefore speculate that the allocation of sub-tokens to distinct experts enables MH-MoE to simultaneously focus on information from various representation spaces within different experts, ensuring a more granular understanding for subtle differences in both vision and language patterns, finally achieving better finer-grained understanding ability. See in Figure 2, sub-tokens assigned to Experts 3 and 2 capture a detailed understanding of each character\u2019s actions within an image patch, while those assigned to Experts 1 and 4 explicitly model the semantics of the false cognate \u2018camera\u2019. ", "page_idx": 1}, {"type": "image", "img_path": "dyZ8GJZjtX/tmp/aaec1b83b4ebcbdd22ac8869741a619e5abb5ae80ff6d1f7a4dd70f8c8d0d135.jpg", "img_caption": ["Figure 1: (a) Expert activation distribution on XNLI [10] corpus, encompassing 6 parallel expert layers with 32 experts per layer. (b) MH-MoE showcases finer-grained understanding by distributing sub-tokens split from semantically-rich patches to more distinct experts to capture semantic information. Brighter regions indicate that sub-tokens split from this patch are distributed to a greater number of different experts. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "MH-MoE maintains following strengths: (1) Higher experts activation & better scalability. MHMoE can alleviate lower expert activation problem and significantly enhance the usage of larger experts by enabling optimization of almost all of experts, allowing for more efficient scaling of model capacity. (2) Finer-grained understanding ability. By adaptively assigning sub-tokens to different experts based on the semantic richness of the token, MH-MoE enabling to jointly attend to information from different representation spaces at different experts, and finally achieving better finer-grained understanding ability. (3) Seamless integration. The implementation of MH-MoE is remarkably straightforward and decoupled from other SMoE optimization methods (e.g., GShard [21]), making it very easy to integrate them together to achieve better performance. ", "page_idx": 1}, {"type": "text", "text": "We evaluate the proposed MH-MoE on three model pre-training and fine-tuning setting: Englishfocused language modeling, multi-lingual language modeling and masked multi-modality modeling, across different parameter scales (300M to 7B). Extensive experimental among these three tasks demonstrate the effectiveness of MH-MoE. ", "page_idx": 1}, {"type": "image", "img_path": "dyZ8GJZjtX/tmp/c3c35a90b8e01e1881d8679a050f509a1d75de89339ce13cc4283c1c1158cc9e.jpg", "img_caption": ["Figure 2: Workflow of MH-MoE. For vision data, different heads routed to different experts try to capture different aspects of details within patches and relations between patches. For language data, different heads attend to capture the varying contexts of false cognates across different languages (e.g., Italian and English) or polysemous words within a single language. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Sparse Mixture-of-Experts (SMoE) [31, 12, 5, 7] enhances model capacity while maintaining a constant computational demand, thus achieving better performance than densely-activated models on various tasks [22, 19, 39, 28]. ", "page_idx": 2}, {"type": "text", "text": "Different from densely-activated models, each MoE layer consists of $N$ independent Feed-Forward Networks (FFN) $\\{f_{i}^{\\mathrm{FFN}}\\}_{i=0}^{N}$ as the experts, along with a gating function $g\\left(\\cdot\\right)$ to model a probability distribution indicating the weights over these experts\u2019 outputs. For the hidden representation $\\mathbf{h}\\in\\mathbb{R}^{d}$ of each input token, the gating value of routing $\\mathbf{h}$ to expert $f_{i}^{\\mathrm{FFN}}$ is denoted as: ", "page_idx": 2}, {"type": "equation", "text": "$$\ng\\left(f_{i}^{\\mathrm{FFN}}\\right)=\\exp\\left(\\mathbf{h}\\cdot\\mathbf{e}_{i}\\right)/\\sum_{j=0}^{N}\\exp\\left(\\mathbf{h}\\cdot\\mathbf{e}_{j}\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "cwohrerrees $\\mathbf{e}_{i}$ n ddienngo se xthpee rttrsa, iancacbolre deinmgb teod tdhien gt oopf- hgea $i$ e-tdh  veaxlupeesr,t  aarned a $\\begin{array}{r}{\\sum_{i=0}^{N}g\\left(f_{i}^{\\mathrm{FFN}}\\right)\\,=\\,1}\\end{array}$ .u tT no,f  tthhee $k$ $k$ $\\mathbf{O}$ MoE layer is $\\begin{array}{r}{\\mathbf{\\check{O}}=\\dot{\\mathbf{h}}+\\sum_{i\\in\\Phi}g\\left(f_{i}^{\\mathrm{\\check{F}}\\mathrm{FN}}\\right)\\cdot f_{i}^{\\mathrm{FF}\\dot{\\mathbf{N}}}\\left(\\mathbf{h}\\right)}\\end{array}$ , where $\\Phi$ denote activated experts set and $|\\Phi|=k$ ", "page_idx": 2}, {"type": "text", "text": "As described above, the most commonly used routing mechanism involves selecting the top- $k$ experts from $N$ experts, where $k\\ll N$ [32], e.g., $k=2$ and $N=2048$ in GShard [21]. Such a routing mechanism allows the combination of data parallelism and expert parallelism. Some works [38, 21] suggest that larger values of $k$ often contribute to better model performance. However, with the increase in the value of $k$ , training models with conventional top- $k$ routing implementation becomes much less efficient [21]. In this paper, we introduce MH-MoE, a simple but efficient manner to make denser expert activation without an increase in computational complexity. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Multi-Head Mixture-of-Experts ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Concretely, we denote a sequence of inputs tokens by $\\mathbf{X}\\in\\mathbb{R}^{l\\times d}$ , where $l$ is the number of tokens and $d$ represents the length of token dimension. In MH-MoE, each parallel layer contains a set of $N$ experts, each presented as $\\{f_{i}^{\\mathrm{FFN}}:\\mathbb{R}^{\\frac{d}{h}}\\rightarrow\\mathbb{R}^{\\frac{d}{h}}\\}_{i=0}^{N}$ , where $h$ denotes the number of heads (i.e., the number of sub-tokens a single token is split into), which is decoupled from the head in the multi-head self-attention layer. For clarity, we describe the operation of a single MH-MoE layer here only. ", "page_idx": 2}, {"type": "text", "text": "The full architecture of MH-MoE can be seen in Figure 3. First, $\\mathbf{X}$ is projected by a multi-head layer with parameter matrices $\\mathbf{W}_{\\mathrm{head}}\\in\\mathbb{R}^{d\\times d}$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{X}}=\\mathbf{X}\\cdot\\mathbf{W}_{\\mathrm{head}}^{\\top}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\hat{\\mathbf{X}}\\in\\mathbb{R}^{l\\times d}$ . After that, every token in $\\hat{\\textbf{X}}$ is split into $h$ sub-tokens along the token dimensions, and these sub-tokens are arranged in parallel according to the original token sequence, forming a new ", "page_idx": 2}, {"type": "image", "img_path": "dyZ8GJZjtX/tmp/fa10811e1892bd58335404dce27af2335a44b287917cebd2631fb8d643a1825f.jpg", "img_caption": ["Figure 3: Illustration of a typical SMoE layer and the proposed MH-MoE layer. (a) A SMoE layer consists of a router and expert networks, where the experts are sparsely activated according to dot-product token-expert routing scores. (b) MH-MoE introduces additional two MLP layers, namely the multi-head layer and merge layer to split and merge tokens, respectively. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "feature space $\\ddot{\\mathbf{X}}\\in\\mathbb{R}^{(l\\times h)\\times\\frac{d}{h}}\\;\\mathrm{as}^{2}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\ddot{\\mathbf{X}}}=F_{s}({\\hat{\\mathbf{X}}})=\\left[\\overbrace{\\underbrace{\\mathbf{x}_{0}^{0},\\dots,\\mathbf{x}_{h-1}^{0}}_{l\\times h}}^{h},\\dots,\\overbrace{\\underbrace{\\mathbf{x}_{0}^{i},\\dots,\\mathbf{x}_{h-1}^{i}}_{l\\times h}}^{h}\\}...\\,\\underbrace{\\sqrt{\\mathbf{x}_{0}^{l},\\dots,\\mathbf{x}_{h-1}^{l}}}_{\\left\\vert\\mathbf{x}_{0},\\dots,\\mathbf{x}_{h-1}^{l}\\right\\vert}\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where function $F_{\\mathrm{~s~}}$ denotes the token splitting operation: $\\mathbb{R}^{l\\times d}\\rightarrow\\mathbb{R}^{(l\\times h)\\times\\frac{d}{h}}$ , and each sub-token is presented as $\\mathbf{x}_{j}^{i}\\in\\mathbb{R}^{\\frac{d}{h}}$ , meaning it is the the $j^{t h}$ sub-token split from the $i^{t h}$ token. Then all these sub-tokens are fed into the gating function $g(\\cdot)$ . The gating value of routing a certain sub-token $\\mathbf{x}_{j}^{i}$ into the $p^{t h}$ expert is computed as ", "page_idx": 3}, {"type": "equation", "text": "$$\ng\\left(f_{p}^{\\mathrm{FFN}}\\right)=\\exp\\left(\\mathbf{x}_{j}^{i}\\cdot\\mathbf{e}_{p}\\right)/\\sum_{\\xi=0}^{N}\\exp\\left(\\mathbf{x}_{j}^{i}\\cdot\\mathbf{e}_{\\xi}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{e}_{p}\\in\\mathbb{R}^{\\frac{d}{h}}$ is the learnable embedding of the $p^{t h}$ expert. In this paper, we mainly focus on top- $k$ routing, i.e., only the experts with the largest top- $k$ routing score is activated. $\\Phi=\\operatorname{Top}_{k}\\left(g\\left(f^{\\operatorname{FFN}}\\right)\\right)$ denote the set of activated experts and $|\\Phi|=k$ . Then $\\mathbf{x}_{j}^{i}$ is processed by these activated experts as following, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{o}_{j}^{i}=\\mathbf{x}_{j}^{i}+\\sum_{p\\in\\Phi}g\\left(f_{p}^{\\mathrm{FFN}}\\right)\\cdot f_{p}^{\\mathrm{FFN}}\\left(\\mathbf{x}_{j}^{i}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "After that, all obtained $\\mathbf{o}_{j}^{i}$ are rearranged in the original order of sub-tokens and concatenated together $\\mathrm{as}^{2}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{O}=\\left[\\overbrace{\\underbrace{\\mathbf{\\uprho}_{0}^{0},\\dots\\mathbf{o}_{h-1}^{0}}_{\\textit{l}\\times\\mathbf{\\mu}}}^{h},\\dots,\\overbrace{\\underbrace{\\mathbf{o}_{0}^{i},\\dots,\\mathbf{o}_{h-1}^{i}}_{l\\times h}}^{h},\\dots,\\overbrace{\\mathbf{o}_{0}^{l},\\dots,\\mathbf{o}_{h-1}^{l}}^{h}\\right]\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{O}\\in\\mathbb{R}^{(l\\times h)\\times\\frac{d}{h}}$ . After that, $\\mathbf{O}$ is transformed back the into original token form by a token merging operation $\\begin{array}{r}{F_{\\mathrm{m}}\\colon\\mathbb{R}^{(l\\times h)\\times\\frac{d}{h}}\\rightarrow\\mathbb{R}^{l\\times d}}\\end{array}$ and then projected by a merge layer with parameter matrices $\\bar{\\mathbf{W}_{\\mathrm{merge}}}\\in\\mathbb{R}^{d\\times d}$ to effective integration of multiple features $\\mathbf{o}_{j}^{i}$ capturing detailed information from different expert representation spaces: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\breve{\\mathbf{X}}=\\boldsymbol{F}_{\\mathrm{m}}\\left(\\mathbf{O}\\right)^{\\top}\\cdot\\mathbf{W}_{\\mathrm{merge}}^{\\top}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Then we get the final output $\\breve{\\textbf{X}}$ of the single MH-MoE layer. ", "page_idx": 3}, {"type": "text", "text": "By implementing the aforementioned operations, we effectively increase the average volume of data routed to a specific expert by a factor of $h$ (as demonstrated in Eq. 3), thus achieving denser expert activation. Besides, the shapes of the input and output in the MH-MoE layer remain unchanged, thus no additional computational cost is introduced in the subsequent block. Specifically, we introduce a hyperparameter $\\beta$ to scale the inner dimensions of each expert, aiming to balance the parameters introduced by the multi-head layer and merge layer, aligning the model\u2019s parameters and computational complexity with the original SMoE. Furthermore, the allocation of sub-tokens to distinct experts within MH-MoE enables us to collectively capture semantic information from diverse feature spaces across these experts, thereby enhancing the model\u2019s ability to achieve a finer-grained understanding. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "As the Pytorch-like style pseudocode of MH-MoE shown in Appendix D, MH-MoE is characterized by its overall simplicity of implementation and decoupled from other SMoE optimization strategies [21, 5], making it easy to integrate with other optimized SMoE frameworks to enhance performance. ", "page_idx": 4}, {"type": "text", "text": "3.2 Training Objectives ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Load balancing loss. To mitigate the expert load imbalance issue, given the sub-token set $\\ddot{\\mathbf{X}}$ (depicted in Eq. 3) and the frequency $t_{p}$ of how many sub-tokens are routed to the $p^{t h}$ expert, we follow existing works [21, 13] to compute the load balancing loss $\\mathcal{L}_{\\mathrm{balance}}$ via: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{balance}}=\\frac{N}{\\vert\\dot{\\mathbf{X}}\\vert}\\sum_{p=1}^{N}\\sum_{\\mathbf{x}_{j}^{i}\\in\\ddot{\\mathbf{X}}}t_{p}\\cdot g\\left(f_{p}^{\\mathrm{FFN}}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $N$ denotes the number of experts, $|\\ddot{\\mathbf{X}}|$ is the number of sub-tokens contained in $\\ddot{\\mathbf{X}}$ . $g\\left(f_{p}^{\\mathrm{FFN}}\\right)$   \ndenotes the gating value of routing a certain sub-token $\\mathbf{x}_{j}^{i}$ into the $p^{t h}$ expert (see in Eq. 4). ", "page_idx": 4}, {"type": "text", "text": "Task specific loss. The term $\\mathcal{L}_{\\mathrm{task}}$ is dependent on the particular task that MH-MoE is designed to learn. For instance, during pre-training in the English-focused Language Modeling task, we utilize the language modeling loss [29], whereas the model predicts the next word in a sequence. ", "page_idx": 4}, {"type": "text", "text": "So, the overall training objective of MH-MoE is to minimize: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\mathcal{L}_{\\mathrm{task}}+\\alpha\\mathcal{L}_{\\mathrm{balance}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\alpha$ is a coefficient for load balancing. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Compared Baselines. We include two baseline models for comparison: (1) Dense, which represents a Transformer decoder without the incorporation of sparsely-activated parallel modules (i.e., SMoE layer). (2) X-MoE, which is our implementation based on the SMoE proposed by [5]. We build MH-MoE upon X-MoE and use identical settings. Note that the all models are pre-trained using the same training data and loss (Eq. 9) as MH-MoE, and we ensure that the parameter count of MH-MoE remains consistent with or lower than that of X-MoE, ensuring a fair and equitable comparison. A detailed comparison about parameter and computational complexity can be found in Table 11. ", "page_idx": 4}, {"type": "text", "text": "Pre-training Data. We detail the pre-training data of MH-MoE in three areas: (1) English-focused experiments use the RedPajama dataset [8], which is an open-source pre-training dataset. (2) multilingual tasks follow XLM [20] and use the multilingual Wikipedia as training data. (3) multimodal tasks use a masked multi-modality modeling task with a large dataset of images, documents, and image-text pairs. Further details are available in the Appendix A. ", "page_idx": 4}, {"type": "text", "text": "Model Architecture and Hyperparameters. For all experiments, we use the X-MoE [5] as our backbone architecture to build our MH-MoE, which has shown better performance than prior SMoE models such as Switch Transformers [13] on cross-lingual understanding benchmarks. For Englishfocused Language Modeling and Multi-lingual Language Modeling, we construct Dense, X-MoE and MH-MoE using the Transformer [36] decoder $\\mathrm{L}=12$ , $\\mathrm{H}=768$ , $\\mathrm{A}=12$ ) with the GPT- $.4^{3}$ vocabulary ", "page_idx": 4}, {"type": "image", "img_path": "dyZ8GJZjtX/tmp/dedc518eec9153f732abd64a1d80d6d4afd2f7eb4cead74e9674dcfcb1f8332c.jpg", "img_caption": ["Figure 4: Perplexity on validation dataset during the training phase reported for Dense, X-MoE and MH-MoE across three pre-training tasks. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Table 2: Accuracy / accuracy-normalization scores for language understanding tasks using the LLM Evaluation Harness [14]. $N$ denotes the number of experts. ", "page_idx": 5}, {"type": "table", "img_path": "dyZ8GJZjtX/tmp/648c7735bc0d8dea8917edd0d549525d8af212391b7ea40af96bcf55d878452f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "as the backbone architecture. The pre-training procedure takes 14 days on 2 NVIDIA DGX-2 Stations. For Masked Multi-modal Modeling, we build Dense, X-MoE and MH-MoE following the same Transformer encoder architecture as BEiT v3 [37]. The pre-training procedure takes 4 days on 2 NVIDIA DGX-2 Stations. For all three pre-training tasks, we set the head number $h=4$ . More details about architecture and training hyperparameters can be found in Appendix B and C. ", "page_idx": 5}, {"type": "text", "text": "4.2 Perplexity Evaluation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We examined the validation perplexity curves for all pretrained models and pre-training tasks under two expert settings (8 experts and 32 experts). The perplexity trends are depicted in Figure 4, with the final perplexity values listed in Table 1. We can observe that as training progresses: 1) the perplexity of our MH-MoE remained lower in comparison to the compared baselines, indicating more effective learning; 2) MH-MoE achieved the lowest perplexity across three distinct experimental setups; 3) an increase in the number of experts led to a corresponding decrease in the perplexity of MH-MoE, suggesting that the model benefits from enhanced representation learning capabilities as more experts are incorporated. These results collectively demonstrate the superiority of MH-MoE in terms of learning efficiency and language representation across multiple pre-training paradigms. ", "page_idx": 5}, {"type": "table", "img_path": "dyZ8GJZjtX/tmp/b558482ddadb58cdf4eae38fd2f9861dc73b711b21627e092cffbe588e1416ae.jpg", "table_caption": ["Table 1: Results of upstream perplexity evaluation. We report the validation perplexity cross two setting: 8 experts and 32 experts. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "4.3 Downstream Evaluation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "English-focused Language Modeling. We evaluate our models on a total of 9 different zeroshot benchmarks to assess their abilities across various natural language tasks like common sense reasoning, general language understanding and knowledge understanding using the LLM Evaluation Harness [14]. As shown in Table 2, comparing X-MoE with the Dense model, X-MoE show notable improvement, indicating that SMoE models (e.g., X-MoE) benefit from the large model capacity. Overall, for all benchmarks, our MH-MoE obtains the best performance, achieving an average performance gain of 1.1 for 8 experts setting and 1.5 for 32 experts setting compared to X-MoE, demonstrating the effectiveness of our proposed MH-MoE on modeling English-focused language. ", "page_idx": 5}, {"type": "text", "text": "Multi-lingual Language Modeling. We evaluate our multi-lingual language models on the crosslingual natural language inference (XNLI) corpus [10], which is the extension of the multi-genre NLI (MultiNLI) corpus to 14 languages. We follow the LLM Evaluation Harness pipeline and use the zeroshot setting to evaluate the multi-lingual ability. Table 3 presents the zero-shot evaluation results on XNLI task. Similarly, X-MoE benefti from the large model capacity and show notable improvement compared with Dense model. Overall, MH-MoE obtains the best performance, surpassing X-MoE by an average performance gain of 0.6 for 8 experts setting and 0.8 for 32 experts setting. Comparing MH-MoE with the X-MoE, it shows that MH-MoE models provide consistent gains on downstream tasks, demonstrating the effectiveness of our proposed MH-MoE on modeling cross-lingual natural language. ", "page_idx": 5}, {"type": "table", "img_path": "dyZ8GJZjtX/tmp/7a212c1b4f1d2d6e7b8a5432efd0dc27a81b55fa943a188781d818f51edfb73f.jpg", "table_caption": ["Table 3: Accuracy / accuracy-normalization scores on multilingual understanding tasks using the LLM Evaluation Harness [14]. $N$ denotes the number of experts. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "dyZ8GJZjtX/tmp/3d60170c967956cb957780da98721dc803765870f4911a5790e3f3b62cc649e0.jpg", "table_caption": ["Table 4: Results of visual question answering, visual reasoning, and image captioning tasks. "], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "dyZ8GJZjtX/tmp/471ea0e583eb26c248c17f9d6afcab7ed9781ab2730202fd79e30d89db51d87a.jpg", "img_caption": ["Figure 5: Comparison results for different head number $h$ . "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Masked Multi-modal Modeling. We evaluate on the widely used vision-language understanding and generation benchmarks, including visual question answering [15], visual reasoning [33] and image captioning [23]. We report vqa-score on VQAv2, accuracy for NLVR2. For COCO image captioning, we report BLEU $@4$ $(\\mathbf{B}\\,\\ @4)$ , METEOR (M), CIDEr (C), and SPICE (S). Table 4 presents the evaluation results. For VQA task, MH-MoE outperforms both Dense and X-MoE by a large margin, e.g., 4.24 and 1.69 points gain on test-dev split, respectively. For visual reasoning task, MH-MoE beats all these two baselines on both dev (1.5 points gain than X-MoE) and test-P split (1.7 points gain than X-MoE). For image captioning task, MH-MoE surpasses X-MoE by $4.2\\%$ , $10.2\\%$ , $9.4\\%$ in terms of $\\textstyle\\mathbf{B}\\circledcirc4$ , M and S, respectively. Above results show that X-MoE exhibits enhanced visual information comprehension, which also validates the effectiveness of our proposed MH-MoE in capturing diverse semantic information within text-image pair data. ", "page_idx": 6}, {"type": "text", "text": "4.4 Ablation Studies ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "This section presents experimental analysis to demonstrate the functionality of MH-MoE. In all comparative experiments, we ensure parameter equality across models by adjusting the internal dimensions of the experts. ", "page_idx": 6}, {"type": "text", "text": "Number of Heads $h$ . We conduct experiments by adjusting the number of heads $(h=2,4,6,8,$ , and 12) in MH-MoE. As shown in Figure 5, we find that across all settings of $h$ , our model consistently outperforms the X-MoE. Besides, as the value of $h$ increases, we observe an initial improvement followed by a decline in our model\u2019s performance. This leads us to hypothesize that as $h$ initially increases, the enhancement in performance benefits from MH-MoE by activating a greater number of experts, thereby enhancing the model\u2019s effectiveness and capturing a wider range of fine-grained token information. However, as $h$ becomes too large, the excessive subdivision of tokens may impair their original semantic content, leading to decreased model performance. ", "page_idx": 6}, {"type": "text", "text": "Does the Token-Splitting-Merging Really Matters? The key motivation of MH-MoE is to split each token into several sub-tokens and then merge the sub-tokens after processing by experts. We use two MLP layers for the splitting and merging processes. We conduct a detailed analysis to determine whether the performance improvement is due to the Token-Split-Merging (TSM) operation or the additional MLP layers. ", "page_idx": 6}, {"type": "table", "img_path": "dyZ8GJZjtX/tmp/04718088da15f8378e5c5619a8285f71861dfb73f78c9a86f0e1a32d9aeef68f.jpg", "table_caption": ["Table 5: Ablation studies of MH-MoE components: MLP layers and the TokenSplitting-Merging (TSM, Eq. 3 and Eq. 6) operation. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "dyZ8GJZjtX/tmp/30f0617724c0f4713bf51ab6be029ab2fba4c61471e7a5ac568d70ee56b43260.jpg", "img_caption": ["Figure 6: Distribution of expert activation in X-MoE and MH-MoE on both Harness and XNLI corpus, where h denotes the number of heads. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "dyZ8GJZjtX/tmp/239458bf562d6eee33272eebaa47bfa4fa3b86f9e834cdc8247deeb51b5876fc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 7: (a) Upstream training perplexity (\u2193) when scaling the number of experts for both X-MoE and MH-MoE. (b) Accuracy scores on the hellaswag task when scaling the number of experts for both X-MoE and MH-MoE. (c) Comparison for sub-tokens assign diversity (the number of different experts they are routed to) for P&F and Non P&F tokens. ", "page_idx": 7}, {"type": "text", "text": "The results are presented in Table 5. A comparative analysis between Dense v.s. $\\mathrm{Dense}_{w/\\mathrm{~MLP}}$ , as well as $\\boldsymbol{\\mathrm{X}}$ -MoE v.s. $\\mathbf{X}{\\mathrm{-}}\\mathbf{M}\\mathbf{o}\\mathbf{E}_{w}.$ / MLP, reveals that introduction of the MLP layer does not enhance the model\u2019s performance. Similarly, when comparing MH-MoE with MH- $\\mathrm{MoE}_{w/o}$ TSM, it becomes evident that the inclusion of only the MLP, in the absence of the TSM, also does not yield any improvement in the model\u2019s effectiveness. The parameter quantities of the models being compared pairwise are equal. ", "page_idx": 7}, {"type": "text", "text": "An intriguing observation is made when comparing MH-MoE with M $\\mathrm{{\\bf~{\\cdot}}{\\bf{M o}}{\\bf{E}}_{w/o\\mathrm{~r~}}}$ MLP. Introducing TSM alone, without MLP, results in a slight increase in model performance. In contrast, a significant enhancement in model performance is only achieved when both MLP and TSM are incorporated simultaneously. We hypothesize that introduction of TSM, without the integration of MLP, activates more experts, but the segmentation and merging of the model appears overly straightforward and abrupt in its execution. This limitation hinders the model\u2019s ability to meaningfully segment tokens into sub-tokens and effectively merge the diverse information gathered from different expert spaces. ", "page_idx": 7}, {"type": "text", "text": "We also conducted additional ablation studies to examine the impact of varying the number of MLP layers, different splitting-merging methods and different balance loss. The details of these experiments are provided in Appendix E. ", "page_idx": 7}, {"type": "text", "text": "5 Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Experts Activation. We visualize the activation of each expert varies across parallel expert layers for X-MoE and MH-MoE at Figure 6. It can be observed that: 1) X-MoE demonstrate a more skewed distribution, wherein a significant portion of experts remain inactivated all the time. 2) Our MH-MoE achieves a denser expert activation compared to X-MoE, effectively mitigating the issue of low expert utilization. 3) As the number of heads $h$ increases, the expert activation frequency in MH-MoE also rises. ", "page_idx": 7}, {"type": "text", "text": "Scalability. We explore the scalability for both X-MoE and MH-MoE by scaling up the number of experts from 8 to 256 (about 7B parameters). For upstream performance, as shown in Figure 7 (a), with the increase of experts, our MH-MoE could bring more gains. It is because MH-MoE could mitigate the low expert activation problem effectively. With this ability, the superiority of the large-scale SMoE model will be better exerted, thereby achieving the improvement of the upper bound ", "page_idx": 7}, {"type": "text", "text": "Table 6: Performance across multiple pure vision tasks: classification (CLS) on ImageNet-1k, object detection (OD) and instance segmentation (IS) on COCO. The number of expert is set to 8. ", "page_idx": 8}, {"type": "table", "img_path": "dyZ8GJZjtX/tmp/26ae4fc9c98dbd78cf195447cdf96251997cab3966063fad62ef4bdfd601484b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "of SMoE with more experts. Detailed validation perplexity curves for these scaling up experiments can be found in Figure 10 at Appendix F. For downstream performance shown in Figure 7 (b), for X-MoE, expert number $=64$ is the upper bound, meaning that continuing to increase the number of experts will not bring performance gain. Our MH-MoE not only has a performance advantage over the X-MoE with the same number of experts, but also improves the upper bound from 64 to 256, which demonstrates the effectiveness of the scalability of our MH-MoE on downstream tasks. ", "page_idx": 8}, {"type": "text", "text": "Experts Assign within Token. We delve into a more granular analysis to validate how the multihead mechanism aids MH-MoE in capturing diverse and intricate semantic information that is often challenging to comprehend, e.g., polysemous and false cognates words (denoted as PF tokens) in languages, and semantically-rich areas in images. ", "page_idx": 8}, {"type": "text", "text": "For languages data, we compute and compare the divergence levels (i.e., the number of different experts these sub-tokens are routed to) of sub-tokens split from PF tokens and Non-PF tokens. The results, presented in Figure 7 (c), clearly demonstrate that the distribution of divergence for PF tokens is significantly skewed towards the right when compared to that of Non-PF tokens. This indicates that during the MH-MoE\u2019s inference process, PF tokens route their sub-tokens to a greater number of different experts, thereby capturing diverse semantic information in contrast to Non-PF tokens for a better polysemous and false cognates word modeling. Note that we utilized the GPT-4 API [25] to extract polysemous words and false cognates from the XNLI [10] corpus, and the corresponding prompt can be found in Table 12. ", "page_idx": 8}, {"type": "text", "text": "For image data, we analyzed how the divergence levels of different patches evolve during the training process, as illustrated in Figure 7. Notably, as training progresses, divergence levels gradually increase in high-frequency texture regions (or regions with rich semantics), while they decrease in low-frequency texture regions. This suggests that MH-MoE tends to route tokens from complex texture areas to a wider variety of experts, thereby enhancing the finer-grained understanding of semantics in those regions. For more visualization examples, please refer to Figure 11 in Appendix G. ", "page_idx": 8}, {"type": "text", "text": "Experiments on Pure Vision Tasks. We conduct small-scale experiments on pure vision tasks to further validate the effectiveness of MH-MoE. We utilize ViT-B(ase) [11] as the Dense model and AdaMV-MoE [4] as the SMoE baseline. AdaMV-MoE is a multi-task vision SMoE model demonstrating a superior performance across various vision tasks. We build our MH-MoE upon AdaMV-MoE. The comparison results are shown in Table 6, which demonstrates that MH-MoE exhibits corresponding effectiveness and versatility in pure vision tasks. ", "page_idx": 8}, {"type": "text", "text": "Model Collapses. Concerns may arise that MH-MoE could revert to the original SMoE approach, routing all sub-tokens from the same token to a single expert. In MH-MoE\u2019s training framework, the load balancing loss $\\mathcal{L}_{\\mathrm{balance}}$ (Eq 8) serves as a weak constraint, treating all sub-tokens as independent entities and uniformly distributing them among experts, promoting a balanced allocation. Our observations indicate that most sub-tokens from the same token are distributed among 3-5 different experts (see Figures 7 and Figures 7 (c)). ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper, we study how we can to achieve a denser experts activation without introducing additional cost, while improving the fine-grained understanding ability. With the proposed MultiHead Mixture-of-Experts, we can easily implement the aforementioned functionality. Furthermore, the simplicity of MH-MoE allows it to integrate with other SMoE frameworks to enhance performance easily. Extensive empirical results across three tasks demonstrate the effectiveness of MH-MoE. ", "page_idx": 8}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] H. Bao, L. Dong, F. Wei, W. Wang, N. Yang, X. Liu, Y. Wang, J. Gao, S. Piao, M. Zhou, and H. Hon. UniLMv2: Pseudo-masked language models for unified language model pre-training. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 642\u2013652. PMLR, 2020.   \n[2] L. Caccia, E. Ponti, Z. Su, M. Pereira, N. L. Roux, and A. Sordoni. Multi-head adapter routing for cross-task generalization. arXiv preprint arXiv:2211.03831, 2022.   \n[3] S. Changpinyo, P. Sharma, N. Ding, and R. Soricut. Conceptual $12\\mathrm{m}$ : Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 3558\u20133568. Computer Vision Foundation / IEEE, 2021. [4] T. Chen, X. Chen, X. Du, A. Rashwan, F. Yang, H. Chen, Z. Wang, and Y. Li. Adamv-moe: Adaptive multi-task vision mixture-of-experts. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 17346\u201317357, 2023. [5] Z. Chi, L. Dong, S. Huang, D. Dai, S. Ma, B. Patra, S. Singhal, P. Bajaj, X. Song, X.-L. Mao, et al. On the representation collapse of sparse mixture of experts. Advances in Neural Information Processing Systems, 35:34600\u201334613, 2022.   \n[6] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, M. Dehghani, S. Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.   \n[7] A. Clark, D. d. l. Casas, A. Guy, A. Mensch, M. Paganini, J. Hoffmann, B. Damoc, B. Hechtman, T. Cai, S. Borgeaud, et al. Unified scaling laws for routed language models. arXiv preprint arXiv:2202.01169, 2022.   \n[8] T. Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023. [9] A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek, F. Guzma\u00b4n, E. Grave, M. Ott, L. Zettlemoyer, and V. Stoyanov. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, pages 8440\u20138451. Association for Computational Linguistics, 2020.   \n[10] A. Conneau, R. Rinott, G. Lample, A. Williams, S. Bowman, H. Schwenk, and V. Stoyanov. XNLI: Evaluating cross-lingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2475\u20132485, Brussels, Belgium, Oct.-Nov. 2018. Association for Computational Linguistics.   \n[11] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.   \n[12] N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. W. Yu, O. Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. arXiv preprint arXiv:2112.06905, 2021.   \n[13] W. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. The Journal of Machine Learning Research, 23(1):5232\u20135270, 2022.   \n[14] L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPof,i C. Foster, L. Golding, J. Hsu, A. Le Noac\u2019h, H. Li, K. McDonell, N. Muennighoff, C. Ociepa, J. Phang, L. Reynolds, H. Schoelkopf, A. Skowron, L. Sutawika, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou. A framework for few-shot language model evaluation, 12 2023.   \n[15] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh. Making the V in VQA matter: Elevating the role of image understanding in visual question answering. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 6325\u20136334. IEEE Computer Society, 2017.   \n[16] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.   \n[17] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen, Y. Kalantidis, L. Li, D. A. Shamma, M. S. Bernstein, and L. Fei-Fei. Visual genome: Connecting language and vision using crowdsourced dense image annotations. Int. J. Comput. Vis., 123(1):32\u201373, 2017.   \n[18] T. Kudo and J. Richardson. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 66\u201371, Brussels, Belgium, Nov. 2018. Association for Computational Linguistics.   \n[19] K. Kumatani, R. Gmyr, F. C. Salinas, L. Liu, W. Zuo, D. Patel, E. Sun, and Y. Shi. Building a great multi-lingual teacher with sparsely-gated mixture of experts for speech recognition. arXiv preprint arXiv:2112.05820, 2021.   \n[20] G. Lample and A. Conneau. Cross-lingual language model pretraining, 2019.   \n[21] D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.   \n[22] D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen. {GS}hard: Scaling giant models with conditional computation and automatic sharding. In International Conference on Learning Representations, 2021.   \n[23] T. Lin, M. Maire, S. J. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll\u00b4ar, and C. L. Zitnick. Microsoft COCO: common objects in context. In D. J. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars, editors, Computer Vision - ECCV 2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V, volume 8693 of Lecture Notes in Computer Science, pages 740\u2013755. Springer, 2014.   \n[24] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019.   \n[25] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.   \n[26] V. Ordonez, G. Kulkarni, and T. L. Berg. Im2text: Describing images using 1 million captioned photographs. In J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. C. N. Pereira, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems 2011. Proceedings of a meeting held 12-14 December 2011, Granada, Spain, pages 1143\u20131151, 2011.   \n[27] Z. Peng, W. Wang, L. Dong, Y. Hao, S. Huang, S. Ma, and F. Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023.   \n[28] H. Pham, Y. J. Kim, S. Mukherjee, D. P. Woodruff, B. Poczos, and H. H. Awadalla. Task-based moe for multitask multilingual machine translation. arXiv preprint arXiv:2308.15772, 2023.   \n[29] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Improving language understanding by generative pre-training. 2018.   \n[30] P. Sharma, N. Ding, S. Goodman, and R. Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In I. Gurevych and Y. Miyao, editors, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, pages 2556\u20132565. Association for Computational Linguistics, 2018.   \n[31] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In International Conference on Learning Representations, 2017.   \n[32] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.   \n[33] A. Suhr, S. Zhou, A. Zhang, I. Zhang, H. Bai, and Y. Artzi. A corpus for reasoning about natural language grounded in photographs. In A. Korhonen, D. R. Traum, and L. M\\`arquez, editors, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 6418\u20136428. Association for Computational Linguistics, 2019.   \n[34] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[35] T. H. Trinh and Q. V. Le. A simple method for commonsense reasoning. ArXiv, abs/1806.02847, 2018.   \n[36] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, pages 5998\u20136008, 2017.   \n[37] W. Wang, H. Bao, L. Dong, J. Bjorck, Z. Peng, Q. Liu, K. Aggarwal, O. K. Mohammed, S. Singhal, S. Som, et al. Image as a foreign language: Beit pretraining for all vision and vision-language tasks. arXiv preprint arXiv:2208.10442, 2022.   \n[38] A. Yang, J. Lin, R. Men, C. Zhou, L. Jiang, X. Jia, A. Wang, J. Zhang, J. Wang, Y. Li, et al. M6-t: Exploring sparse expert models and beyond. arXiv preprint arXiv:2105.15082, 2021.   \n[39] X. Zhao, X. Chen, Y. Cheng, and T. Chen. Sparse moe with language guided routing for multilingual machine translation. In Conference on Parsimony and Learning (Recent Spotlight Track), 2023.   \n[40] Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19\u201327, 2015. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Pre-training Data of Masked multi-modal modeling task ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Table 8 presents of pre-training data in Masked multi-modal modeling task. For multi-modal data, there are about 15M images and 21M image-text pairs collected from five public datasets: Conceptual 12M (CC12M) [3], Conceptual Captions (CC3M) [30], SBU Captions (SBU) [26], COCO [23] and Visual Genome (VG) [17]. For monomodal data, we use 14M images from ImageNet-21K and 160GB text corpora [1] from English Wikipedia, BookCorpus [40], OpenWebText4, CC-News [24], and Stories [35]. ", "page_idx": 12}, {"type": "table", "img_path": "dyZ8GJZjtX/tmp/c4d9414b4a38ffd04a5692b4fd908da5d467502906a8c25cd2906817d98251fd.jpg", "table_caption": ["Table 8: Pretraining data of Masked multi-modal modeling task. All the data are academically accessible. "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "B Model Hyperparameters of Language modeling tasks ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Table 9 presents the model hyperparameters of X-MoE and MH-MoE for both English-focused language modeling and Multi-lingual language modeling tasks. The gating temperature $\\tau_{0}$ is initialized as 0.3 and 0.07 for the softmax gating and sigmoid gating, respectively. We use the same vocabulary as XLM-R [9] with 250K subwords tokenized by SentencePiece [18]. ", "page_idx": 12}, {"type": "table", "img_path": "dyZ8GJZjtX/tmp/8ba59d1f84051903797247b7de3a30c90f8708ca695f687c17eab12f1b92bccc.jpg", "table_caption": ["Table 9: Model hyperparameters of Dense, X-MoE and MH-MoE. The SMoE frequency refers to how many experts each token will be assigned to, i.e., the value of k in the Top- expert selection. "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "C Hyperparameters for Pre-training ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Table 10 presents the hyperparameters for pre-training across three tasks: Language modeling tasks (English-focused language modeling and Multi-lingual language modeling tasks) and Masked multi-modal modeling task. ", "page_idx": 13}, {"type": "table", "img_path": "dyZ8GJZjtX/tmp/21b05cecff88afd1d11555c036e39ab083f3ca58e9078fe48ca85669fb66b6f8.jpg", "table_caption": ["Table 10: Pre-training hyperparameters for Language modeling tasks (English-focused language modeling and Multi-lingual language modeling tasks) and Masked multi-modal modeling task tasks. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "Table 11: Parameter count setting of X-MoE and MH-MoE in our experiments for English-focused language modeling, Multi-lingual language modeling and Masked multi-modality modeling tasks. \u201cnon-expert param\u201d refers to the parameters that are not part of the expert networks, such as the attention layer, router, etc., while \u201cexpert params\u201d represents the total number of parameters in the parallel expert networks. For Dense models, since there are no expert network layers, we only list the total number of parameters. All models under the same task utilize the same architecture and hyperparameters, following identical training settings and steps. ", "page_idx": 13}, {"type": "table", "img_path": "dyZ8GJZjtX/tmp/67b9ede0f848b7e00166005d4d4d3e7f6b0313d6eb269a42750c427fa55a5f79.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "D PyTorch-style Code ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We also provide the PyTorch-style code in Algorithm 1 to explain our MH-MoE, which including two main aspects: 1) Stage 1. The creation and initialization of multi-head layer and merge layer. 2) Stage 2. The segmentation of tokens, followed by processing through an expert network, and ultimately merging. ", "page_idx": 14}, {"type": "text", "text": "Algorithm 1 The Overall Procedures of MH-MoE in a PyTorch-like style. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "# Stage 1: Initial parameter of multi-head layer & merge layer   \nfor i in range(1, L): M[i].multi_head_layer $=$ nn.Linear(hidden_dim, hidden_dim) M[i].merge_layer $=$ nn.Linear(hidden_dim, hidden_dim) # Initialization nn.init.xavier_uniform_(M[i].multi_head_layer.weight, gain ${}^{=1}$ / math.sqrt(2)) nn.init.xavier_uniform_(M[i].merge_layer.weight) nn.init.constant_(M[i].merge_layer.bias, 0.0)   \n# Stage 2: The segmentation and merge of tokens for the i-th MH-MoE layer   \ndef MHMoE_Layer(x): \u2019\u2019\u2019 Input: x : Tensor shape: (batch_size, Length, hidden_dim) mask : Tensor shape: (batch_size, Length) Output: o : Tensor shape: (batch_size, Length, hidden_dim) heads: head number of multi_head layer \u2019\u2019\u2019 # Processed by multi-head layer x $=$ M[i].multi_head_layer(x) # Split token & rearrange sub-tokens in parallel $\\texttt{x}=\\texttt{x}$ .reshape(batch_size $^{\\ast}$ Length \\* heads, hidden_dim // heads).contiguous() mask $=$ mask.reshape(-1, 1).repeat(1, heads).reshape(batch_size $^{\\ast}$ Length $^\\ast$ heads) # Standrad SMoE routing block x, mask $=$ router(x, mask) # Merge back to the original token form $\\texttt{x}=\\texttt{x}$ .reshape(batch_size $^{\\ast}$ Length, heads, dim // heads).reshape(batch_size \\* Length, dim).contiguous() o $=$ M[i].merge_layer(x) return o ", "page_idx": 14}, {"type": "text", "text": "Table 12: Prompt template for identifying polysemous and false cognates in different languages. ", "page_idx": 15}, {"type": "image", "img_path": "dyZ8GJZjtX/tmp/c30e2e33b97a9ac6aed59b2ce17b98763afda87a65c19a8de179cebb88d206d3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "dyZ8GJZjtX/tmp/867c261f740dcaa702096b6c48d8cec6e2209654f5d7d5c56c418db15ba72a87.jpg", "table_caption": ["Table 13: Comparison results for different numbers of MLP layers $n$ . The results are averaged over five runs. "], "table_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "dyZ8GJZjtX/tmp/aacbd2d75c3e1be8b37ff7c100ff5fed53ee366949f0ff78d85c00039acda6f2.jpg", "img_caption": ["Figure 8: Comparison results for different splitting method. K denotes the size of kernel while S denotes the size of stride in Conv1D. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "E Ablation Studies ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Number of MLP layers. We explore the impact of varying the number of layers $\\mathit{\\Delta}n=0$ , 1, 2, 3) in MLP on MH-MoE performance. For configurations exceeding a single layer, ReLU activation functions were incorporated between MLP layers to ensure the non-linearity of transformations. The parameter quantities of the models being compared are equal. Upon analyzing the results in Table 13, we observe that increasing the number of MLP layers beyond one had a negligible impact on the model\u2019s performance. This indicates that a single-layer MLP is sufficient for accomplishing token segmentation and fusion. ", "page_idx": 15}, {"type": "text", "text": "What Makes a Good Splitting-Merging Method? We designed ablation experiments to investigate whether the tokenization method affects the MH-MoE\u2019s performance. We replace the multi-head layer and merge layer (both are FC layers) with Conv1d layers (denoted as MH-MoE\u266f). ", "page_idx": 15}, {"type": "text", "text": "We conducted experiments in two settings (English-focused language modeling and Multi-lingual language modeling tasks) and explored different levels of randomness by using different kernel sizes and strides for the Conv1d layers. We ensure that the parameters are kept as consistent as possible by adjusting the number of Conv1d layers and the dimensions of experts. Through experimental results presented at Figure 8, we observe that replacing splitting method with Conv1d (denoted as $\\mathbf{MH-MoE^{\\sharp}}$ ) resulted in significantly inferior performance compared to the original MH-MoE. This underscores the importance of tokenization methods, indicating that tokenization methods such as Conv1d, which disrupt the original input sequence features, are not suitable for this context. ", "page_idx": 15}, {"type": "text", "text": "Impact of different balance loss We conducted experiments with three groups to assess the impact of different constraints on sub-token allocation: (1) $\\mathbf{MH-MoE^{\\P}}$ : MH-MoE trained without the $\\mathcal{L}_{\\mathrm{balance}}$ , (2) $\\mathbf{M}\\mathbf{H}\\mathbf{-}\\mathbf{M}\\mathbf{o}\\mathbf{E}^{\\dagger}$ : MH-MoE trained with the $\\mathcal{L}_{\\mathrm{balance}}$ , and (3) $\\mathbf{M}\\mathbf{H}\\mathbf{-}\\mathbf{M}\\mathbf{o}\\mathbf{E}^{\\ddagger}$ : MH-MoE trained with both $\\mathcal{L}_{\\mathrm{balance}}$ and diversity loss $\\mathscr{L}_{\\mathrm{div}}$ . ${\\mathcal{L}}_{\\mathrm{div}}$ is designed to enhance sub-token allocation diversity and defined as: given a set of sub-tokens $\\Upsilon$ split from the same token, with $P_{i}$ representing the gating probability distribution of the $i^{t h}$ sub-token, $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{div}}\\,=\\,-\\sum_{i=1}^{|\\Upsilon|}\\sum_{j=i+1}^{|\\Upsilon|}\\mathcal{K}\\mathcal{L}(\\bar{P}_{i}||P_{j})}\\end{array}$ , where $\\kappa{\\mathcal{L}}(\\cdot)$ indicate Kullback\u2013Leibler divergence. This constraint increases the inconsistency in gating distributions among sub-tokens in $\\Upsilon$ , enhancing expert allocation diversity. ", "page_idx": 15}, {"type": "image", "img_path": "dyZ8GJZjtX/tmp/e2023556942c2a84792441329727c9be90b6577c5fbbd4322b0094184c718c3b.jpg", "img_caption": ["Figure 9: Experimental results for varying level constraints on sub-token allocation across two tasks: English-focused language modeling (left) and Multi-lingual language modeling (right). "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "dyZ8GJZjtX/tmp/1bae9178a5cb24669f9512c06d85071cc607f925595fc531867b8abba9b544ce.jpg", "img_caption": ["Figure 10: Validation perplexity reported for both X-MoE and MH-MoE. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "The experimental results (perplexity) are summarized in the Figure 9. Key findings include: (1) Without any constraints $(\\mathbf{\\dot{M}}\\mathbf{\\dot{H}}\\mathbf{-}\\mathbf{M}\\mathbf{o}\\dot{\\mathbf{E}}^{\\boldsymbol{\\P}})$ ), the model\u2019s performance degrades, likely due to severe imbalance, causing some experts to receive insufficient data, resulting in suboptimal performance. (2) The addition of $\\mathscr{L}_{\\mathrm{div}}$ did not significantly improve model performance compared to using only Lbalance. We hypothesize that $\\mathcal{L}_{\\mathrm{balance}}$ alone provides sufficient diversity in sub-token allocation. ", "page_idx": 16}, {"type": "text", "text": "F Visualization of training perplexity ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We provide the training perplexity curve for model training in the experimental setting of increasing the number of experts (from 8 to 256) in Figure 10. ", "page_idx": 16}, {"type": "text", "text": "G Visualization ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We provide more visualization of variation in assign diversity for sub-tokens split from different patches in vision data at Figure 11. ", "page_idx": 16}, {"type": "image", "img_path": "dyZ8GJZjtX/tmp/c24341c0060ff9b43922170a02f5e3cc7c228e7257e4ea887b69d2ae8a68d4a3.jpg", "img_caption": ["Figure 11: More visualization examples for assign diversity of sub-tokens split from different patches with respect to training steps. We analyze MH-MoE with 8 heads $(h{=}8)$ as the subject. Brighter regions indicate that sub-tokens from this patch are distributed to a greater number of diverse experts, while darker regions indicate that sub-tokens are assigned to more of the same experts. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer:[Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: Please refer to our abstract and introduction. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: If we discover limitations, we will include them in the latest version of the paper. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [No] ", "page_idx": 18}, {"type": "text", "text": "Justification: In our paper, we do not make any assumptions, and consequently, there are no associated proof processes. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We provide detailed explanations of the model training and data specifics in Sections 3 and Appendix A B and C. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We provide the code for model training and testing in the supplementary materials. The training data used are publicly available, and we detail the data specifics in the paper. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Please refer to Sections 3 and Appendix A, B and C. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: All results are reported as the average of five randomly initialized tests. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Please refer to Sections 3 and Appendix A B and C. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: Our model serves as the foundational architecture for large models rather than their socio-economic applications. As such, it operates on a different level, and we do not provide an analysis of societal impact. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 21}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [No] ", "page_idx": 22}, {"type": "text", "text": "Justification: We will conduct this before our model is publicly released. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: Please refer to Section 4. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Please refer to the README.md in our supplement code. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]