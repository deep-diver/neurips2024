[{"type": "text", "text": "Value-Based Deep Multi-Agent Reinforcement Learning with Dynamic Sparse Training ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Pihe Hu\u2217 Shaolong Li\u2217 Zhuoran Li Tsinghua University Central South University Tsinghua University Beijing, China Changsha, China Beijing, China hupihe@gmail.com shaolongli16@gmail.com lizr20@mails.tsinghua.edu.cn ", "page_idx": 0}, {"type": "text", "text": "", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ling Pan   \nHong Kong University of   \nScience and Technology   \nHong Kong, China   \nlingpan@ust.hk ", "page_idx": 0}, {"type": "text", "text": "Longbo Huang\u2020 Tsinghua University Beijing, China longbohuang@tsinghua.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep Multi-agent Reinforcement Learning (MARL) relies on neural networks with numerous parameters in multi-agent scenarios, often incurring substantial computational overhead. Consequently, there is an urgent need to expedite training and enable model compression in MARL. This paper proposes the utilization of dynamic sparse training (DST), a technique proven effective in deep supervised learning tasks, to alleviate the computational burdens in MARL training. However, a direct adoption of DST fails to yield satisfactory MARL agents, leading to breakdowns in value learning within deep sparse value-based MARL models. Motivated by this challenge, we introduce an innovative Multi-Agent Sparse Training (MAST) framework aimed at simultaneously enhancing the reliability of learning targets and the rationality of sample distribution to improve value learning in sparse models. Specifically, MAST incorporates the Soft Mellowmax Operator with a hybrid TD- $(\\lambda)$ schema to establish dependable learning targets. Additionally, it employs a dual replay buffer mechanism to enhance the distribution of training samples. Building upon these aspects, MAST utilizes gradient-based topology evolution to exclusively train multiple MARL agents using sparse networks. Our comprehensive experimental investigation across various value-based MARL algorithms on multiple benchmarks demonstrates, for the first time, significant reductions in redundancy of up to $20\\times$ in Floating Point Operations (FLOPs) for both training and inference, with less than $3\\%$ performance degradation. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Multi-agent reinforcement learning (MARL) [Shoham and Leyton-Brown, 2008], coupled with deep neural networks, has not only revolutionized artificial intelligence but also showcased remarkable success across a wide range of critical applications. From mastering multi-agent video games such as Quake III Arena [Jaderberg et al., 2019], StarCraft II [Mathieu et al., 2021], Dota 2 [Berner et al., 2019], and Hide and Seek [Baker et al., 2020] to guiding autonomous robots through intricate real-world environments [Shalev-Shwartz et al., 2016, Da Silva et al., 2017, Chen et al., 2020b], deep MARL has emerged as an indispensable and versatile tool for addressing complex, multifaceted challenges. Its unique capability to capture intricate interactions and dependencies among multiple agents has spurred novel solutions, solidifying its position as a transformative paradigm across various domains [Zhang et al., 2021, Albrecht et al., 2023]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "However, the exceptional success of deep MARL comes at a considerable computational cost. Training these agents involves the intricate task of adapting neural networks to accommodate an expanded parameter space, especially in scenarios with a substantial number of agents. For instance, the training regimen for AlphaStar [Mathieu et al., 2021], tailored for StarCraft II, extended over a grueling 14-day period, employing 16 TPUs per agent. Similarly, the OpenAI Five [Berner et al., 2019] model for Dota 2 underwent an extensive training cycle spanning 180 days and harnessing thousands of GPUs. This exponential increase in computational demands as the number of agents grows (and the corresponding joint action and state spaces) poses a significant challenge during MARL deployment. ", "page_idx": 1}, {"type": "text", "text": "To tackle these computational challenges, researchers have delved into dynamic sparse training (DST), a method that trains neural network models with dynamically sparse topology. For example, RigL [Evci et al., 2020] can train a $90\\%$ -sparse network from scratch in deep supervised learning without performance degradation. However, in deep reinforcement learning (DRL), the learning target evolves in a bootstrapping manner [Tesauro et al., 1995], and the distribution of training data is path-dependent [Desai et al., 2019], posing additional challenges to sparse training. Improper sparsification can result in irreversible damage to the learning path [Igl et al., 2020]. Initial attempts at DST in sparse single-agent DRL training have faced difficulties in achieving consistent model compression across diverse environments, as documented in [Sokar et al., 2022, Graesser et al., 2022]. This is mainly because sparse models may introduce significant bias, leading to unreliable learning targets and exacerbating training instability as agents learn through bootstrapping. Moreover, the partially observable nature of each agent makes training non-stationarity inherently more severe in multi-agent settings. Collectively, these factors pose significant challenges for value learning in each agent under sparse models. ", "page_idx": 1}, {"type": "text", "text": "We present a motivating experiment in Figure 1, where we evaluated various sparse training methods on the $\\mathtt{3s5z}$ task from SMAC [Samvelyan et al., 2019] using a neural network with only $10\\%$ of its original parameters. Classical DST methods, including SET [Mocanu et al., 2018] and RigL [Evci et al., 2020], demonstrate poor performance in MARL scenarios, along with using static sparse networks (SS). Additionally, RLx2 as proposed in [Tan et al., 2022] proves ineffective for multi-agent settings, despite enabling DST for single-agent settings with $\\bar{9}0\\%$ sparsity. In contrast, our MAST framework in this work achieves a win rate of over $90\\%$ . In addition, the only prior attempt to train sparse MARL agents, as described in [Yang et al., 2022], prunes agent networks during training with weight grouping [Wang et al., 2019]. However, this approach fails to maintain sparsity throughout training, and only achieves a final model sparsity of only $80\\%$ . Moreover, their experiment is confined to a two-user environment, PredatorPrey-v2, in MuJoCo [Todorov et al., 2012]. These observations highlight the fact that, despite its potential, the application of sparse networks in the context of MARL remains largely unexplored (A comprehensive literature review is deferred in Appendix A.1). Consequently, a critical and intriguing question arises: ", "page_idx": 1}, {"type": "image", "img_path": "Gug7wc0BSs/tmp/65feb9e29d7c66cf2700d0d4e0b90cb2898c4f26641ecbe6d6cd1ad270e66014.jpg", "img_caption": ["Figure 1: Comparison of different sparse training methods. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Can we train MARL agents effectively using ultra-sparse networks throughout? ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We affirmatively address the question by introducing a novel sparse training framework, Multi-Agent Sparse Training (MAST). Since improper sparsification results in network ftiting errors in the learning targets and incurs large policy inconsistency errors in the training samples, MAST ingeniously integrates the Soft Mellowmax Operator with a hybrid TD- $\\left(\\lambda\\right)$ schema to establish reliable learning targets. Additionally, it incorporates a novel dual replay buffer mechanism to enhance the distribution of training samples. Leveraging these components, MAST employs gradient-based topology evolution to exclusively train multiple MARL agents using sparse networks. Consequently, MAST facilitates the training of highly efficient MARL agents with minimal performance compromise, employing ultra-sparse networks throughout the training process. ", "page_idx": 1}, {"type": "text", "text": "Our extensive experimental investigation across various value-based MARL algorithms on multiple SMAC benchmarks reveals MAST\u2019s ability to achieve model compression ranging from $5\\times$ to $20\\times$ , while incurring minimal performance trade-offs (under $3\\%$ ). Moreover, MAST demonstrates an impressive capability to reduce the Floating Point Operations (FLOPs) required for both training and inference by up to $20\\times$ , showcasing a significant margin over other baselines (detailed in Section 4). ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Deep MARL We model the MARL problem as a decentralized partially observable Markov decision process (Dec-POMDP) [Oliehoek et al., 2016], represented by a tuple $\\langle N,S,\\mathcal{U},P,r,\\mathcal{Z},O,\\gamma\\rangle$ . Deep Multi-Agent $Q$ -learning extends the deep $Q$ learning method [Mnih et al., 2013] to multiagent scenarios [Sunehag et al., 2018, Rashid et al., 2020b, Son et al., 2019]. The agent-wise $\\mathrm{\\DeltaQ}$ function is defined over its history $\\tau_{i}$ as $Q_{i}$ for agent $i$ . Subsequently, the joint action-value function $Q_{\\mathrm{tot}}(\\tau,u)$ operates over the joint action-observation history $\\tau$ and joint action $\\textbf{\\em u}$ . The objective, given transitions $(\\boldsymbol{\\tau},\\boldsymbol{u},r,\\tau^{\\prime})$ sampled from the experience replay buffer $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ , is to minimize the mean squared error loss ${\\mathcal{L}}(\\theta)$ on the temporal-difference (TD) error $\\delta=y-Q_{\\mathrm{tot}}(\\tau,\\boldsymbol{u})$ . Here, the TD target $y=r+\\gamma\\operatorname*{max}_{\\pmb{u}^{\\prime}}\\bar{Q}_{\\mathrm{tot}}(\\pmb{\\tau}^{\\prime},\\pmb{u}^{\\prime})$ , where $\\bar{Q}_{\\mathrm{tot}}$ is the target network for the joint action $Q$ -function, periodically copied from $Q_{\\mathrm{tot}}$ . Parameters of $Q_{\\mathrm{tot}}$ are updated using $\\theta^{\\prime}=\\mathbf{\\bar{\\theta}}-\\alpha\\nabla_{\\theta}\\mathcal{L}(\\theta)$ , with $\\alpha$ representing the learning rate. ", "page_idx": 2}, {"type": "text", "text": "We focus on algorithms that adhere to the Centralized Training with Decentralized Execution (CTDE) paradigm [Kraemer and Banerjee, 2016], within which, agents undergo centralized training, where the complete action-observation history and global state are available. However, during execution, they are constrained to individual local action-observation histories. To efficiently implement CTDE, the Individual-Global-Maximum (IGM) property [Son et al., 2019] in Eq. (1), serves as a key mechanism: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\arg\\operatorname*{max}_{\\boldsymbol{u}}Q_{\\mathrm{tot}}(s,\\boldsymbol{u})=\\big(\\arg\\operatorname*{max}_{\\boldsymbol{u}_{1}}Q_{1}\\left(s,\\boldsymbol{u}_{1}\\right),\\cdots,\\arg\\operatorname*{max}_{\\boldsymbol{u}_{N}}Q_{N}\\left(s,\\boldsymbol{u}_{N}\\right)\\big).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Many deep MARL algorithms [Yu et al., 2022, Pan et al., 2022] adhere to the IGM criterion, such as the QMIX series algorithms [Rashid et al., 2020b,a, Pan et al., 2021]. These algorithms employ a mixing network $f_{s}$ with non-negative weights, enabling the joint Q-function to be expressed as $Q_{\\mathrm{tot}}(s,\\pmb{u})=f_{s}\\left(Q_{1}\\left(s,u_{1}\\right),\\cdot\\cdot\\cdot\\,,Q_{N}\\left(s,u_{N}\\right)\\right)$ . ", "page_idx": 2}, {"type": "text", "text": "Dynamic Sparse Training Dynamic sparse training (DST), initially proposed in deep supervised learning, can train a $90\\%$ sparse network without performance degradation from scratch, such as in ResNet-50 [He et al., 2016] and MobileNet [Howard et al., 2017]. In DST, the dense network is randomly sparsified at initialization, as shown in Figure 2, and its topology is dynamically changed during training by link dropping and growing. Specifically, the topology evolution mechanism in MAST follows the RigL method [Evci et al., 2020], which improves the optimization of sparse neural networks by leveraging weight magnitude and gradient information to jointly optimize model parameters and connectivity. ", "page_idx": 2}, {"type": "image", "img_path": "Gug7wc0BSs/tmp/0b266b7cd3a01b5b0efeb8217db8a833c27f83e99feae27a77faf1dec6033b3a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "RigL periodically and dynamically drops a subset of existing connections with the smallest absolute weight values and concurrently grows an equivalent number of empty connections with the largest gradients. The pseudo-code of RigL is given in Algorithm 1, where the symbol $\\odot$ denotes the element-wise multiplication operator, $M_{\\theta}$ symbolizes the binary mask that delineates the sparse topology for the network $\\theta$ , and $\\zeta_{t}$ is the update fraction in training step $t$ . This process maintains the network sparsity throughout the training with ", "page_idx": 2}, {"type": "table", "img_path": "Gug7wc0BSs/tmp/363ed4b233976b0d7d0e269a4c4a1fe66c853fa10de7cafec065cb651d2f30d7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "a strong evolutionary ability that saves training FLOPs and gives a sparse model after training. However, in DRL, the learning target evolves in a bootstrapping manner [Tesauro et al., 1995], such that the distribution of training data is path-dependent [Desai et al., 2019], posing additional challenges to sparse training. Moreover, the partially observable nature of each agent exacerbates training non-stationarity, particularly in multi-agent settings. These factors collectively present significant hurdles for value learning in each agent under sparse models. As illustrated in Figure 1, attempts to train ultra-sparse MARL models using simplistic topology evolution or the sparse training framework for single-agent RL have failed to achieve satisfactory performance. Therefore, MAST introduces innovative solutions to enhance value learning in ultra-sparse models by simultaneously improving the reliability of learning targets and the rationality of sample distribution. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3 Enhancing Value Learning in Sparse Models ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This section outlines the pivotal components of the MAST framework for training sparse MARL agents. MAST introduces innovative solutions to enhance the accuracy of value learning in ultrasparse models by concurrently refining training data targets and distributions. Consequently, the topology evolution in MAST effectively identifies appropriate ultra-sparse network topologies. This approach aligns with single-agent DRL, where sparse training necessitates co-design with the value learning method as described in [Tan et al., 2022]. However, the partially observable nature of each agent exacerbates training non-stationarity in multi-agent settings. As illustrated in Figure 3, MAST implements two key innovations to achieve accurate value learning in ultra-sparse models: i) hybrid $\\mathrm{TD}(\\lambda)$ targets combined with the Soft Mellowmax operator to mitigate estimation errors arising from network sparsity, and ii) dual replay buffers to reduce policy inconsistency errors due to sparsification. ", "page_idx": 3}, {"type": "image", "img_path": "Gug7wc0BSs/tmp/d073c01ff62e7e5944c7c196b6f75b6cae11812236cf597480de380c24851d39.jpg", "img_caption": ["Figure 3: An example of the MAST framework based on QMIX. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.1 Improving the Reliability of Training Targets ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Initially, we observe that the expected error is amplified under sparse models, motivating the introduction of multi-step TD targets. However, different environments may require varying values of step lengths to achieve optimal performance. Consequently, we focus on hybrid $\\mathbf{TD}(\\lambda)$ targets, which can achieve performance comparable to the best step length across different settings. Additionally, we find that the overestimation problem remains significant in sparse models. To address this, we propose the use of the Soft Mellowmax operator in constructing learning targets. This operator is effective in reducing overestimation bias without incurring additional computational costs. ", "page_idx": 3}, {"type": "text", "text": "Hybrid $\\mathbf{TD}(\\lambda)$ Targets. In deep multi-agent Q-learning, temporal difference (TD) learning is a fundamental method for finding an optimal policy, where the joint action-value network is iteratively updated by minimizing a squared loss driven by the TD target. Let $M_{\\theta}$ be a binary mask representing the network\u2019s sparse topology, and denote the sparse network as ${\\hat{\\theta}}=\\theta\\odot M_{\\theta}$ , where $\\odot$ signifies element-wise multiplication. Since sparse networks operate within a reduced hypothesis space with fewer parameters, the sparse network $\\hat{\\theta}$ may induce a large bias, making the learning targets unreliable, as evidenced in [Sokar et al., 2022]. ", "page_idx": 3}, {"type": "text", "text": "Moreover, we establish Theorem 3.1 to characterize the upper bound of the expected multi-step TD error under sparse models, where the multi-step return at $(s_{t},\\pmb{u}_{t})$ is $\\begin{array}{r}{T_{n}(s_{t},\\pmb{u}_{t}^{\\flat})=\\sum_{k=0}^{n-1}\\gamma^{k}\\bar{r_{t+k}}+}\\end{array}$ $\\gamma^{n}\\operatorname*{max}_{\\pmb{u}}Q_{\\mathrm{tot}}(s_{t+n},\\pmb{u};\\hat{\\theta})$ under sparse models. As Eq (2) shows, the expected multi-step TD error comes from two parts, intrinsical policy inconsistency error and network fitting error. Thus, Eq (2) implies that the expected TD error will be enlarged if the network is sparsified improperly with a larger network ftiting error. Indeed, the upper bound of the expected TD error will be enlarged if the network fitting error is increased. Subsequently, it is infeasible for the model to learn a good policy. Eq. (2) also shows that introducing a multi-step return target discounts the network ftiting error by a factor of \u03b3n in the upper bound of the expected TD error. Thus, employing a multi-step return T t(n) with a sufficiently large $n$ , or even Monte Carlo methods [Sutton and Barto, 2018], can effectively diminish the TD error caused by network sparsification for $\\gamma<1$ . ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.1. Denote $\\pi$ as the target policy at timestep $t_{\\perp}$ , and $\\rho$ as the behavior policy generating the transitions $\\left(s_{t},\\mathbf{{u}}_{t},\\ldots,s_{t+n},\\mathbf{{u}}_{t+n}\\right)$ . Denote the network fitting error as $\\epsilon(s,\\pmb{u})=|Q_{t o t}(s,\\pmb{u};\\hat{\\theta})-$ ", "page_idx": 3}, {"type": "text", "text": "$Q_{t o t}^{\\pi}(s,u)|$ . Then, the expected error between the multi-step $T D$ target ${\\mathcal{T}}_{n}$ conditioned on transitions from the behavior policy $\\rho$ and the true joint action-value function $Q_{t o t}^{\\pi}$ is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathbb{E}_{\\rho}[{\\mathcal T}_{n}(s_{t},u_{t})]-Q_{t o t}^{\\pi}(s_{t},u_{t})|\\leq\\gamma^{n}\\mathbb{E}_{\\rho}\\big[2\\epsilon\\big(s_{t+n},\\rho(s_{t+n})\\big)+\\epsilon\\big(s_{t+n},\\pi(s_{t+n})\\big)\\big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n+\\left|Q_{t o t}^{\\rho}(s_{t},u_{t})-Q_{t o t}^{\\pi}(s_{t},u_{t})\\right|+\\gamma^{n}\\mathbb{E}_{\\rho}\\big[|Q_{t o t}^{\\pi}(s_{t+n},\\pi(s_{t+n}))-Q_{t o t}^{\\rho}(s_{t+n},\\rho(s_{t+n}))|\\big].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Proof. Please refer to Appendix A.3. ", "page_idx": 4}, {"type": "text", "text": "However, the Monte Carlo method is prone to high variance, suggesting that an optimal TD target in sparse models should be a multi-step return with a judiciously chosen step length, balancing network fitting error due to sparsification and training variance. Figure 4 illustrates model performance across different step lengths and model sizes, revealing that an optimal step length exists for various model sizes. Moreover, the optimal step length increases as model size decreases, which aligns with Theorem 3.1 due to the increased network fitting error in models with higher sparsity. ", "page_idx": 4}, {"type": "text", "text": "The above facts suggest the need to increase the step length in the learning targets to maintain the performance of sparse models. However, the optimal step length varies across different settings. To address this, we introduce the $\\mathrm{TD}(\\lambda)$ target [Sutton and Barto, 2018] to achieve a good trade-off: $\\begin{array}{r}{\\mathcal{T}_{\\lambda}\\,=\\,\\left(1\\,-\\,\\lambda\\right)\\sum_{n=1}^{\\infty}\\lambda^{n-1}\\mathcal{T}_{n}}\\end{array}$ for $\\lambda\\in[0,1]$ . This target averages all possible multi-step returns $\\{\\tau_{n}\\}_{n=1}^{\\infty}$ into a single return using exponentially decaying weights, provid", "page_idx": 4}, {"type": "image", "img_path": "Gug7wc0BSs/tmp/9c46301f1493b49b5245fbc7bc486e68644521889a4fe7b9124116f4fb69ca6c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "ing a computationally efficient approach with Figure 4: Performances of different step lengths. episode-form data. In Figure 4, we also plot two representative performances of $\\mathrm{TD}(\\lambda)$ under $10\\%$ and $5\\%$ model sizes, which are both close to the optimal step length under different model sizes. ", "page_idx": 4}, {"type": "text", "text": "Previous studies [Fedus et al., 2020] have highlighted that an immediate shift to multi-step targets can exacerbate policy inconsistency error as shown in Eq. (2). Since the $\\mathrm{TD}(\\lambda)$ target $\\mathcal{T}_{\\lambda}$ averages all potential multi-step returns $\\{\\mathcal{T}_{n}\\}_{n=1}^{\\infty}$ , an immediate transition to this target may encounter similar issues. To address this challenge, we adopt a hybrid strategy inspired by the delayed mechanism proposed in [Tan et al., 2022]. Initially, when the training step is less than a threshold $T_{0}$ , we employ one-step TD targets $(\\mathcal{T}_{1})$ to minimize policy inconsistency errors. As training progresses and the policy stabilizes, we transit to $\\mathrm{TD}(\\lambda)$ targets to mitigate sparse network ftiting errors. This mechanism ensures consistent and reliable learning targets throughout the sparse training process. ", "page_idx": 4}, {"type": "text", "text": "Soft Mellowmax Operator. The max operator in the Bellman operator poses a well-known theoretical challenge, namely overestimation, which hinders the convergence of various linear and non-linear approximation schemes [Tsitsiklis and Van Roy, 1996]. Deep MARL algorithms, including QMIX [Rashid et al., 2020b], also grapple with the overestimation problem. Several works have addressed this issue in dense MARL algorithms, such as double critics [Ackermann et al., 2019], weighted critic updates [Sarkar and Kalita, 2021], the Softmax operator [Pan et al., 2021], and the Sub-Avg operator [Wu et al., 2022]. However, these methods introduce additional computational costs, sometimes even doubling the computational budget, which is infeasible for our sparse training framework. ", "page_idx": 4}, {"type": "text", "text": "We turn our attention to the Soft Mellowmax operator, which has been proven effective in reducing overestimation for dense MARL algorithms in [Gan et al., 2021]. For MARL algorithms satisfying the IGM property in Eq. (1), we replace the max operator in $Q_{i}$ with the Soft Mellowmax operator in Eq. (3) to mitigate overestimation bias in the joint-action Q function within sparse models: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{sm}_{\\omega}(Q_{i}(\\tau,\\cdot))=\\frac{1}{\\omega}\\log\\left[\\sum_{u\\in\\mathcal{U}}\\frac{\\exp\\left(\\alpha Q_{i}\\left(\\tau,u\\right)\\right)}{\\sum_{u^{\\prime}\\in\\mathcal{U}}\\exp\\left(\\alpha Q_{i}\\left(\\tau,u^{\\prime}\\right)\\right)}\\exp\\left(\\omega Q_{i}\\left(\\tau,u\\right)\\right)\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\omega>0$ , and $\\alpha\\in\\mathbb{R}$ . Let $\\tau$ be the value estimation operator that estimates the value of the next state $s^{\\prime}$ . Theorem 3.2 shows that the Soft Mellowmax operator can reduce the severe overestimation bias in sparse models. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.2. Let $\\begin{array}{r}{B(\\mathcal{T})\\;=\\;\\mathbb{E}\\left[\\mathcal{T}\\left(s^{\\prime}\\right)\\right]\\;-\\;\\operatorname*{max}_{\\pmb{u}^{\\prime}}\\underline{{Q}}_{t o t}^{*}\\;(s^{\\prime},\\pmb{u}^{\\prime})}\\end{array}$ be the bias of value estimates of $\\tau$ . For an arbitrary joint-action $Q$ -function $\\bar{Q}_{t o t}^{\\quad}$ , if there exists some $V_{t o t}^{*}\\,\\left(s^{\\prime}\\right)$ such that ", "page_idx": 4}, {"type": "text", "text": "$V_{t o t}^{*}\\;(s^{\\prime})\\;\\;=\\;\\;Q_{t o t}^{*}\\;(s^{\\prime},{\\pmb u}^{\\prime})$ for different joint actions, $\\begin{array}{r l r}{\\sum_{\\pmb{u}^{\\prime}}\\left(\\bar{Q}_{t o t}\\left(s^{\\prime},\\pmb{u}^{\\prime}\\right)-V_{t o t}^{*}\\left(s^{\\prime}\\right)\\right)}&{=}&{0,}\\end{array}$ and $\\begin{array}{r}{\\frac{1}{|U|}\\sum_{\\pmb{u}^{\\prime}}\\left(\\bar{Q}_{t o t}\\left(s^{\\prime},\\pmb{u}^{\\prime}\\right)-V_{t o t}^{*}\\left(s^{\\prime}\\right)\\right)^{2}=C}\\end{array}$ ( $C>0)$ , then $B\\left(\\mathcal{T}_{R i g L-Q M I X-S M}\\right)\\leq B\\left(\\mathcal{T}_{R i g L-Q M I X}\\right)\\!.$ ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Proof. Please refer to Appendix A.4. ", "page_idx": 5}, {"type": "text", "text": "Our empirical investigations reveal that overestimation remains a significant performance barrier in sparse models, resulting in substantial performance degradation. Figure 12 illustrates the win rates and estimated values of QMIX with and without our Soft Mellowmax operator on $\\mathtt{3s5z}$ in the SMAC. Figure 5(a) shows that the performance of RigL-QMIX-SM out", "page_idx": 5}, {"type": "image", "img_path": "Gug7wc0BSs/tmp/5060c9b39a41dde97447dd1a47d3169db61300ae3adaf6aec905cb3ab80251d8.jpg", "img_caption": ["Figure 5: Effects of Soft Mellowmax operator. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "performs RigL-QMIX, while Figure 5(b) demonstrates that the Soft Mellowmax operator effectively mitigates overestimation bias. These findings highlight that QMIX still faces overestimation issues in sparse models and underscore the efficacy of the Soft Mellowmax operator in addressing this problem. ", "page_idx": 5}, {"type": "text", "text": "Additionally, the Soft Mellowmax operator introduces negligible extra computational costs, as it averages the Q function over each agent\u2019s individual action spaces rather than the joint action spaces used in the Softmax operator [Pan et al., 2021], which grow exponentially with the number of agents. ", "page_idx": 5}, {"type": "text", "text": "3.2 Improving the Rationality of Sample Distribution ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Although we have improved the reliability of the learning target\u2019s confidence as discussed above, the learning process can still suffer from instability due to improper sparsification. This suggests the need to enhance the distribution of training samples to stabilize the training process. Improper sparsification can lead to irreversible damage to the learning path [Igl et al., 2020] and exacerbate training instability as agents learn through bootstrapping. ", "page_idx": 5}, {"type": "text", "text": "Generally, sparse models are more challenging to train compared to dense models due to the reduced hypothesis space [Evci et al., 2019]. Therefore, it is important to prioritize more recent training samples to estimate the true value function within the same training budget. Failing to do so can result in excessively large policy inconsistency errors, thereby damaging the learning process irreversibly. ", "page_idx": 5}, {"type": "text", "text": "MAST introduces a dual buffer mechanism utilizing two First-in-First-Out (FIFO) replay buffers: $\\boldsymbol{{\\beta}}_{1}$ (with large capacity) and $B_{2}$ (with small capacity), with some data overlap between them. While $B_{1}$ adopts an off-policy style, $B_{2}$ follows an on-policy approach. During each training step, MAST samples $b_{1}$ episodes from $\\boldsymbol{{\\beta}}_{1}$ and $b_{2}$ episodes from $\\boldsymbol{B}_{2}$ , conducting a gradient update using a combined batch size of $\\left(b_{1}+b_{2}\\right)$ . For instance, in our experiments, we set $|\\bar{B}_{1}|:|B_{2}|=50:1$ and $b_{1}:b_{2}=3:1$ . Generally, training with online data enhances learning stability, as the behavior policy closely matches the target policy. Conversely, training with offline data improves sample efficiency but can lead to instability. ", "page_idx": 5}, {"type": "text", "text": "Figure 6 illustrates the training dynamics for RigL-QMIX in the SMAC\u2019s $\\mathtt{3s5z}$ task. The green curve with large variances highlights the training instability of QMIX in sparse models. However, with the integration of dual buffers, QMIX\u2019s training stability and efficiency are significantly improved under sparse conditions, leading to consistent policy enhancements and higher rewards. Notably, the dual buffer mechanism does not enhance dense training, suggesting that this approach is particularly effective in sparse scenarios where network parameters are crucial for ensuring stable policy improvements. Although prior works [Schaul et al., 2015, Hou et al., 2017, Banerjee et al., 2022] have explored prioritized or dynamic-capacity buffers, their applicability in this context may be limited due to the data being in episode form in value-based deep MARL algorithms, making it difficult to determine the priority of each training episode. Similarly, the dynamic buffer approach in [Tan et al., 2022] is also inapplicable, as the policy distance measure cannot be established for episode-form data. This further emphasizes the unique effectiveness of the dual buffer approach in enhancing training stability for sparse MARL models. ", "page_idx": 5}, {"type": "image", "img_path": "Gug7wc0BSs/tmp/830106c72d16595099b88e3d507dc8fd5763ae17375920a3af981e35bcbd38aa.jpg", "img_caption": ["Figure 6: Different buffers. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "We highlight that the performance improvement observed with MAST is not due to providing more training data samples to agents. Instead, it results from a more balanced training data distribution enabled by the utilization of dual buffers. The primary objective of incorporating dual buffers is to shift the overall distribution of training data. Figure 7 illustrates the distribution of samples induced by different policies, where behavior policy and target policy are defined in Theorem 3.1. Specifically, samples in the original single buffer are subject to the distribution of the behavior policy (blue), which introduces a policy inconsistency error $d_{1}$ . However, by utilizing dual buffers, the distribution of training samples shifts towards the target policy, as there are more recent samples from the onpolicy buffer. This reduces the policy inconsistency in our dual buffers, thereby bolstering the stability and effectiveness of the learning process under sparse models. Additionally, the extra on-policy buffer does not significantly reduce sample efficiency, as its capacity is small. ", "page_idx": 6}, {"type": "image", "img_path": "Gug7wc0BSs/tmp/8e10c0f02b35547f9e6455836a1c8654d958130096fd13de834c84a6dc97646a.jpg", "img_caption": ["Figure 7: Distribution Shift: $d_{1}$ and $d_{2}$ are distribution distances. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we conduct a comprehensive performance evaluation of MAST across various tasks in the StarCraft Multi-Agent Challenge (SMAC) [Samvelyan et al., 2019] benchmark. Additional experiments on the multi-agent MuJoCo (MAMuJoCo) [Peng et al., 2021] benchmark are provided in Appendix B.9. MAST serves as a versatile sparse training framework specifically tailored for value decomposition-based MARL algorithms. In Section 4.1, we integrate MAST with state-of-the-art value-based deep MARL algorithms, including QMIX [Rashid et al., 2020b], WQMIX [Rashid et al., 2020a], and RES [Pan et al., 2021]. We also apply MAST to a hybrid value-based and policybased algorithm, FACMAC [Peng et al., 2021]. Subsequently, we assess the performance of sparse models generated by MAST in Section 4.2. Furthermore, a comprehensive ablation study of MAST components is detailed in Appendix B.7. Each reported result represents the average performance over eight independent runs, each utilizing distinct random seeds. ", "page_idx": 6}, {"type": "table", "img_path": "Gug7wc0BSs/tmp/34bb0d808d452b8ed62e019b84692f973b76e0b218f5a7c38c3cf203626c6462.jpg", "table_caption": ["Table 1: Comparisons of MAST with different sparse training baselines: \"Sp.\" stands for \"sparsity\", \"Total Size\" means total model parameters, and the data is all normalized w.r.t. the dense model. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.1 Comparative Evaluation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Table 1 presents a comprehensive summary of our comparative evaluation in the SMAC benchmark, where MAST is benchmarked against the following baseline methods: (i) Tiny: Utilizing tiny dense networks with a parameter count matching that of the sparse model during training. (ii) SS: Employing static sparse networks with random initialization. (iii) SET [Mocanu et al., 2018]: Pruning connections based on their magnitude and randomly expanding connections. (iv) RigL [Evci et al., 2020]: This approach leverages dynamic sparse training, akin to MAST, by removing and adding connections based on magnitude and gradient criteria, respectively. (v) $\\mathbf{RL}\\mathbf{x}2$ [Tan et al., 2022]: A specialized dynamic sparse training framework tailored for single-agent reinforcement learning. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "We set the same sparsity levels for both the joint Q function $Q_{\\mathrm{tot}}$ , and each individual agent\u2019s Q function $Q_{i}$ . For every algorithm and task, the sparsity level indicated in Table 1 corresponds to the highest admissible sparsity threshold of MAST. Within this range, MAST\u2019s performance consistently remains within a $3\\%$ margin compared to the dense counterpart, effectively representing the minimal sparse model size capable of achieving performance parity with the original dense model. All other baselines are evaluated under the same sparsity level as MAST. We assess the performance of each algorithm by computing the average win rate per episode over the final 20 policy evaluations conducted during training, with policy evaluations taking place at 10000-step intervals. Identical hyperparameters are employed across all scenarios. ", "page_idx": 7}, {"type": "text", "text": "Performance Table 1 unequivocally illustrates MAST\u2019s substantial performance superiority over all baseline methods in all four environments across the three algorithms. Notably, static sparse (SS) consistently exhibits the lowest performance on average, highlighting the difficulty of finding optimal sparse network topologies in the context of sparse MARL models. Dynamic sparse training methods, namely SET and RigL, slightly outperform SS, although their performance remains unsatisfactory. Sparse networks also, on average, underperform tiny dense networks. However, MAST significantly outpaces all other baselines, indicating the successful realization of accurate value estimation through our MAST method, which effectively guides gradient-based topology evolution. Notably, the single-agent method RLx2 consistently delivers subpar results in all experiments, potentially due to the sensitivity of the step length in the multi-step targets, and the failure of dynamic buffer for episode-form training samples. ", "page_idx": 7}, {"type": "text", "text": "To further substantiate the efficacy of MAST, we conduct performance comparisons across various sparsity levels in $3{\\tt s}5z$ , as depicted in Figure 8. This reveals an intriguing observation: the performance of sparse models experiences a sharp decline beyond a critical sparsity threshold. Compared to conventional DST techniques, MAST significantly extends this critical sparsity threshold, enabling higher levels of sparsity while maintaining performance. Moreover, MAST achieves a higher critical sparsity threshold than the other two algorithms with existing baselines, e.g., SET and RigL, achieving a sparsity level of over $80\\%$ on average. However, it is essential to note that the Softmax operator in RES averages the Q function over joint action spaces, which grow exponentially with the number of agents, resulting in significantly higher computational FLOPs and making it computationally incomparable to MAST. The detailed FLOPs calculation is deferred to Appendix B.4.2. ", "page_idx": 7}, {"type": "image", "img_path": "Gug7wc0BSs/tmp/bf009cbfaf7d5da3834591df20b98860e5eb57bcb71eeef0f22131b442d8a99f.jpg", "img_caption": ["Figure 8: Different sparsity. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "FLOP Reduction and Model Compression In contrast to knowledge distillation or behavior cloning methodologies, exemplified by works such as [Livne and Cohen, 2020, Vischer et al., 2022], MAST maintains a sparse network consistently throughout the entire training regimen. Consequently, MAST endows itself with a unique advantage, manifesting in a remarkable acceleration of training FLOPs. We observed an acceleration of up to $20\\times$ in training and inference FLOPs for MAST-QMIX in the ${}^{2}\\!\\mathtt{s}3\\!\\mathtt{z}$ task, with an average acceleration of $10\\times$ , $9\\times$ , and $8\\times$ for QMIX, WQMIX, and RESQMIX, respectively. Moreover, MAST showcases significant model compression ratios, achieving reductions in model size ranging from $5\\times$ to $20\\times$ for QMIX, WQMIX, and RES-QMIX, while incurring only minor performance degradation, all below $3\\%$ . ", "page_idx": 7}, {"type": "image", "img_path": "Gug7wc0BSs/tmp/3f6178f4c516eca02847a1550d7e033ac4f84023f709eef1bc28113877007f87.jpg", "img_caption": ["Figure 9: Mean episode win rates on different SMAC tasks with MAST-FACMAC. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Results on FACMAC In addition to pure value-based deep MARL algorithms, we also evaluate MAST with a hybrid value-based and policy-based algorithm, FACMAC [Peng et al., 2021], in SMAC. The results are presented in Figure 18. From the figure, we observe that MAST consistently achieves a performance comparable with that of the dense models, and outperforms other methods in three environments, demonstrating its applicability across different algorithms. ", "page_idx": 8}, {"type": "text", "text": "4.2 Sparse Models Obtained by MAST ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conduct a comparative analysis of diverse sparse network architectures. With identical sparsity levels, distinct sparse architectures lead to different hypothesis spaces. As emphasized in [Frankle and Carbin, 2019], specific architectures, such as the \"winning ticket,\" outperform randomly generated counterparts. We compare three architectures: the \"random ticket\" (randomly sampled topology held constant during training), the \"winning ticket\" (topology from a MAST or RigL run and kept unchanged during training), and the \"cheating ticket\" (trained by MAST). ", "page_idx": 8}, {"type": "text", "text": "Figure 10 illustrates that both the \"cheating ticket\" and \"winning ticket\" by MAST achieve the highest performance, closely approaching the original dense model\u2019s performance. Importantly, using a fixed random topology during training fails to fully exploit the beneftis of high sparsity, resulting in significant performance degradation. Furthermore, RigL\u2019s \"winning ticket\" fares poorly, akin to the \"random ticket.\" These results underscore the advantages of our MAST approach, which automatically discovers effective sparse architectures through gradient-based topology evolution, without the need for pretraining methods, e.g., knowledge distillation [Schmitt et al., 2018]. Crucially, our MAST method incorporates key elements: the hybrid $\\mathrm{TD}(\\lambda)$ mechanism, Soft Mellowmax operator, and dual buffers. Compared to RigL, these components significantly improve value estimation and training stability in sparse models facilitating efficient topology evolution. ", "page_idx": 8}, {"type": "image", "img_path": "Gug7wc0BSs/tmp/63ed5a705b6122c33ae8a6b4bf2be8cf12b317ef14025b441294058bab76d1e7.jpg", "img_caption": ["Figure 10: Comparison of different sparse masks. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Figure 11 showcases the evolving sparse mask of a hidden layer during MAST-QMIX training in $\\mathtt{3s5z}$ , capturing snapshots at 0, 5, 10, and 20 million steps. In Figure 11, the light pixel in row $i$ and column $j$ indicates the existence of the connection for input dimension $j$ and output dimension $i$ , while the dark pixel represents the empty connection. Notably, a pronounced shift in the mask is evident at the start of training, followed by a gradual convergence of connections within the layer onto a subset of input neurons. This convergence is discernible from the clustering of light pixels forming continuous rows in the lower segment of the final mask visualization, where several output dimensions exhibit minimal or no connections. This observation underscores the distinct roles played by various neurons in the representation process, showing the prevalent redundancy in dense models. ", "page_idx": 8}, {"type": "image", "img_path": "Gug7wc0BSs/tmp/878b2299cb794337456117a0c5e925ddf69fdbf310e0874800202600f279b1c2.jpg", "img_caption": ["Figure 11: Visualization of weight masks in the first hidden layer of agent 1 by MAST-QMIX. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "When sparsifying agent networks, MAST only requires setting the total sparsity. The sparsity of different agents is determined automatically by MAST, by concatenating all the agent networks together in our implementation based on PyMARL [Samvelyan et al., 2019] (detailed in Appendix B.3), and treating these networks as a single network during topology evolution with only a total sparsity requirement. We visualize the trained masks of different agents in $3{\\tt s}5{\\tt z}$ in Figure 12(a), including the masks of two stalkers and two zealots, respectively. ", "page_idx": 8}, {"type": "image", "img_path": "Gug7wc0BSs/tmp/bdb8e6f4c7b02d6174f1e77eaa00b21e84b8dcb2d21dda1d742ebe9715364cff.jpg", "img_caption": ["Figure 12: Agent roles. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Interestingly, we find that the network topology in the same type of agents looks very similar. However, stalkers have more connections than zealots, which aligns with the fact that stalkers play more critical roles due to their higher attack power and wider attack ranges. This observation highlights an advantage of the MAST framework, i.e., it can automatically discover the proper sparsity levels for different agents to meet the total sparsity budget. To further validate this point, we compare the adaptive allocation scheme with fixed manually-set patterns in Figure 12(b). The manual patterns include (stalker- $10\\%$ , zealot- $10\\%$ ), (stalker- $g\\%$ , zealot- $.12\\%$ ), and (stalker- $14\\%$ , zealot- $.6\\%$ ). The results also show that the adaptive sparsity allocation in MAST outperforms other manual sparsity patterns, demonstrating the superiority of MAST. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper introduces MAST, a novel sparse training framework for valued-based deep MARL. We identify and address value estimation errors and policy inconsistency caused by sparsification, two significant challenges in training sparse agents. MAST offers innovative solutions: a hybrid $\\mathrm{TD}(\\lambda)$ target mechanism combined with the Soft Mellowmax operator for precise value estimation under extreme sparsity, and a dual buffer mechanism to reduce policy inconsistency and enhance training stability. Extensive experiments validate MAST\u2019s effectiveness in sparse training, achieving model compression ratios of $5\\times$ to $20\\times$ with minimal performance degradation and up to a remarkable $20\\times$ reduction in FLOPs for both training and inference. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the Technology and Innovation Major Project of the Ministry of Science and Technology of China under Grant 2020AAA0108400 and 2020AAA0108403. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Johannes Ackermann, Volker Gabler, Takayuki Osa, and Masashi Sugiyama. Reducing overestimation bias in multi-agent domains using double centralized critics. arXiv preprint arXiv:1910.01465, 2019.   \nSV Albrecht, F Christianos, and L Sch\u00e4fer. Multi-agent reinforcement learning: Foundations and modern approaches. Massachusetts Institute of Technology: Cambridge, MA, USA, 2023.   \nBowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, and Igor Mordatch. Emergent tool use from multi-agent autocurricula. In International Conference on Learning Representations, 2020.   \nChayan Banerjee, Zhiyong Chen, and Nasimul Noman. Improved soft actor-critic: Mixing prioritized off-policy samples with on-policy experiences. IEEE Transactions on Neural Networks and Learning Systems, 2022.   \nGuillaume Bellec, David Kappel, Wolfgang Maass, and Robert Legenstein. Deep rewiring: Training very sparse deep networks. In International Conference on Learning Representations, 2018.   \nChristopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemys\u0142aw Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.   \nChristopher Brix, Parnia Bahar, and Hermann Ney. Successfully applying the stabilized lottery ticket hypothesis to the transformer architecture. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3909\u20133915, 2020.   \nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.   \nTianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, and Michael Carbin. The lottery ticket hypothesis for pre-trained bert networks. Advances in neural information processing systems, 33:15834\u201315846, 2020a.   \nYu-Jia Chen, Deng-Kai Chang, and Cheng Zhang. Autonomous tracking using a swarm of uavs: A constrained multi-agent reinforcement learning approach. IEEE Transactions on Vehicular Technology, 69(11):13702\u201313717, 2020b.   \nFilippos Christianos, Georgios Papoudakis, Muhammad A Rahman, and Stefano V Albrecht. Scaling multi-agent reinforcement learning with selective parameter sharing. In International Conference on Machine Learning, pages 1989\u20131998. PMLR, 2021.   \nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.   \nArnau Colom. Empirical analysis of exploration strategies in qmix. 2021.   \nFelipe Leno Da Silva, Ruben Glatt, and Anna Helena Reali Costa. Simultaneously learning and advising in multiagent reinforcement learning. In Proceedings of the 16th conference on autonomous agents and multiagent systems, pages 1100\u20131108, 2017.   \nShrey Desai, Hongyuan Zhan, and Ahmed Aly. Evaluating lottery tickets under distributional shifts. EMNLP-IJCNLP 2019, page 153, 2019.   \nTim Dettmers and Luke Zettlemoyer. Sparse networks from scratch: Faster training without losing performance. arXiv preprint arXiv:1907.04840, 2019.   \nXin Dong, Shangyu Chen, and Sinno Pan. Learning to prune deep neural networks via layer-wise optimal brain surgeon. Advances in Neural Information Processing Systems, 30, 2017.   \nUtku Evci, Fabian Pedregosa, Aidan Gomez, and Erich Elsen. The difficulty of training sparse neural networks. In ICML 2019 Workshop on Identifying and Understanding Deep Learning Phenomena, 2019.   \nUtku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery: Making all tickets winners. In International Conference on Machine Learning, pages 2943\u20132952. PMLR, 2020.   \nWilliam Fedus, Prajit Ramachandran, Rishabh Agarwal, Yoshua Bengio, Hugo Larochelle, Mark Rowland, and Will Dabney. Revisiting fundamentals of experience replay. In International Conference on Machine Learning, pages 3061\u20133071. PMLR, 2020.   \nJonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In International conference on learning representations, 2019.   \nYaozhong Gan, Zhe Zhang, and Xiaoyang Tan. Stabilizing q learning via soft mellowmax operator. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 7501\u20137509, 2021.   \nLaura Graesser, Utku Evci, Erich Elsen, and Pablo Samuel Castro. The state of sparse training in deep reinforcement learning. In International Conference on Machine Learning, pages 7766\u20137792. PMLR, 2022.   \nJayesh K Gupta, Maxim Egorov, and Mykel Kochenderfer. Cooperative multi-agent control using deep reinforcement learning. In Autonomous Agents and Multiagent Systems: AAMAS 2017 Workshops, Best Papers, S\u00e3o Paulo, Brazil, May 8-12, 2017, Revised Selected Papers 16, pages 66\u201383. Springer, 2017.   \nSong Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015.   \nSong Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International conference on learning representations, 2016.   \nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \nYuenan Hou, Lifeng Liu, Qing Wei, Xudong Xu, and Chunlin Chen. A novel ddpg method with prioritized experience replay. In 2017 IEEE international conference on systems, man, and cybernetics (SMC), pages 316\u2013321. IEEE, 2017.   \nAndrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.   \nMaximilian Igl, Gregory Farquhar, Jelena Luketina, Wendelin Boehmer, and Shimon Whiteson. Transient non-stationarity and generalisation in deep reinforcement learning. In International Conference on Learning Representations, 2020.   \nMax Jaderberg, Wojciech M Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia Castaneda, Charles Beattie, Neil C Rabinowitz, Ari S Morcos, Avraham Ruderman, et al. Humanlevel performance in 3d multiplayer games with population-based reinforcement learning. Science, 364(6443):859\u2013865, 2019.   \nWoojun Kim and Youngchul Sung. Parameter sharing with network pruning for scalable multi-agent deep reinforcement learning. In Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems, pages 1942\u20131950, 2023.   \nLandon Kraemer and Bikramjit Banerjee. Multi-agent reinforcement learning as a rehearsal for decentralized planning. Neurocomputing, 190:82\u201394, 2016.   \nNamhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-shot network pruning based on connection sensitivity. In International conference on learning representations, 2019.   \nChenghao Li, Tonghan Wang, Chengjie Wu, Qianchuan Zhao, Jun Yang, and Chongjie Zhang. Celebrating diversity in shared multi-agent reinforcement learning. Advances in Neural Information Processing Systems, 34:3991\u20134002, 2021.   \nDor Livne and Kobi Cohen. Pops: Policy pruning and shrinking for deep reinforcement learning. IEEE Journal of Selected Topics in Signal Processing, 14(4):789\u2013801, 2020.   \nChristos Louizos, Max Welling, and Diederik P Kingma. Learning sparse neural networks through l_0 regularization. In International conference on learning representations, 2018.   \nMichael Mathieu, Sherjil Ozair, Srivatsan Srinivasan, Caglar Gulcehre, Shangtong Zhang, Ray Jiang, Tom Le Paine, Konrad Zolna, Richard Powell, Julian Schrittwieser, et al. Starcraft ii unplugged: Large scale offline reinforcement learning. In Deep RL Workshop NeurIPS 2021, 2021.   \nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.   \nDecebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen, Madeleine Gibescu, and Antonio Liotta. Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science. Nature communications, 9(1):2383, 2018.   \nDmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov. Variational dropout sparsifies deep neural networks. In International Conference on Machine Learning, pages 2498\u20132507. PMLR, 2017.   \nPavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz. Importance estimation for neural network pruning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11264\u201311272, 2019.   \nHesham Mostafa and Xin Wang. Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization. In International Conference on Machine Learning, pages 4646\u20134655. PMLR, 2019.   \nFrans A Oliehoek, Christopher Amato, et al. A concise introduction to decentralized POMDPs, volume 1. Springer, 2016.   \nLing Pan, Tabish Rashid, Bei Peng, Longbo Huang, and Shimon Whiteson. Regularized softmax deep multi-agent q-learning. Advances in Neural Information Processing Systems, 34:1365\u20131377, 2021.   \nLing Pan, Longbo Huang, Tengyu Ma, and Huazhe Xu. Plan better amid conservatism: Offline multi-agent reinforcement learning with actor rectification. In International conference on machine learning, pages 17221\u201317237. PMLR, 2022.   \nAdam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.   \nBei Peng, Tabish Rashid, Christian Schroeder de Witt, Pierre-Alexandre Kamienny, Philip Torr, Wendelin B\u00f6hmer, and Shimon Whiteson. Facmac: Factored multi-agent centralised policy gradients. Advances in Neural Information Processing Systems, 34:12208\u201312221, 2021.   \nTabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson. Weighted qmix: Expanding monotonic value function factorisation for deep multi-agent reinforcement learning. Advances in neural information processing systems, 33:10199\u201310210, 2020a.   \nTabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Monotonic value function factorisation for deep multi-agent reinforcement learning. The Journal of Machine Learning Research, 21(1):7234\u20137284, 2020b.   \nMikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli, Tim G. J. Rudner, Chia-Man Hung, Philiph H. S. Torr, Jakob Foerster, and Shimon Whiteson. The StarCraft Multi-Agent Challenge. CoRR, abs/1902.04043, 2019.   \nTamal Sarkar and Shobhanjana Kalita. A weighted critic update approach to multi agent twin delayed deep deterministic algorithm. In 2021 IEEE 18th India Council International Conference (INDICON), pages 1\u20136. IEEE, 2021.   \nTom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv preprint arXiv:1511.05952, 2015.   \nSimon Schmitt, Jonathan J Hudson, Augustin Zidek, Simon Osindero, Carl Doersch, Wojciech M Czarnecki, Joel Z Leibo, Heinrich Kuttler, Andrew Zisserman, Karen Simonyan, et al. Kickstarting deep reinforcement learning. arXiv preprint arXiv:1803.03835, 2018.   \nJonathan Schwarz, Siddhant Jayakumar, Razvan Pascanu, Peter E Latham, and Yee Teh. Powerpropagation: A sparsity inducing weight reparameterisation. Advances in neural information processing systems, 34:28889\u201328903, 2021.   \nShai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement learning for autonomous driving. arXiv preprint arXiv:1610.03295, 2016.   \nYoav Shoham and Kevin Leyton-Brown. Multiagent systems: Algorithmic, game-theoretic, and logical foundations. Cambridge University Press, 2008.   \nGhada Sokar, Elena Mocanu, Decebal Constantin Mocanu, Mykola Pechenizkiy, and Peter Stone. Dynamic sparse training for deep reinforcement learning. 2022.   \nKyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning. In International conference on machine learning, pages 5887\u20135896. PMLR, 2019.   \nSuraj Srinivas, Akshayvarun Subramanya, and R Venkatesh Babu. Training sparse neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 138\u2013145, 2017.   \nChuangchuang Sun, Macheng Shen, and Jonathan P How. Scaling up multiagent reinforcement learning for robotic systems: Learn an adaptive sparse communication graph. In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 11755\u201311762. IEEE, 2020.   \nPeter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition networks for cooperative multi-agent learning based on team reward. In Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems, pages 2085\u20132087, 2018.   \nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.   \nYiqin Tan, Pihe Hu, Ling Pan, Jiatai Huang, and Longbo Huang. Rlx2: Training a sparse deep reinforcement learning model from scratch. In International Conference on Learning Representations, 2022.   \nGerald Tesauro et al. Temporal difference learning and td-gammon. Communications of the ACM, 38 (3):58\u201368, 1995.   \nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ international conference on intelligent robots and systems, pages 5026\u20135033. IEEE, 2012.   \nJN Tsitsiklis and B Van Roy. An analysis of temporal-difference learning with function approximationtechnical. Rep. LIDS-P-2322). Lab. Inf. Decis. Syst. Massachusetts Inst. Technol. Tech. Rep, 1996.   \nMarc Aurel Vischer, Robert Tjarko Lange, and Henning Sprekeler. On lottery tickets and minimal task representations in deep reinforcement learning. In International conference on learning representations, 2022.   \nChaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by preserving gradient flow. In International Conference on Learning Representations, 2020.   \nXijun Wang, Meina Kan, Shiguang Shan, and Xilin Chen. Fully learnable group convolution for acceleration of deep neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9049\u20139058, 2019.   \nHaolin Wu, Jianwei Zhang, Zhuang Wang, Yi Lin, and Hui Li. Sub-avg: Overestimation reduction for cooperative multi-agent reinforcement learning. Neurocomputing, 474:94\u2013106, 2022.   \nJe Yang, JaeUk Kim, and Joo-Young Kim. Learninggroup: A real-time sparse training on fpga via learnable weight grouping for multi-agent reinforcement learning. In 2022 International Conference on Field-Programmable Technology (ICFPT), pages 1\u20139. IEEE, 2022.   \nChao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising effectiveness of ppo in cooperative multi-agent games. Advances in Neural Information Processing Systems, 35:24611\u201324624, 2022.   \nHaonan Yu, Sergey Edunov, Yuandong Tian, and Ari S Morcos. Playing the lottery with rewards and multiple languages: lottery tickets in rl and nlp. In International conference on learning representations, 2020.   \nHongjie Zhang, Zhuocheng He, and Jing Li. Accelerating the deep reinforcement learning with neural network compression. In 2019 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138. IEEE, 2019.   \nKaiqing Zhang, Zhuoran Yang, and Tamer Ba\u00b8sar. Multi-agent reinforcement learning: A selective overview of theories and algorithms. Handbook of reinforcement learning and control, pages 321\u2013384, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Supplementary Materials ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A Additional Details for MAST Framework 16 ", "page_idx": 15}, {"type": "text", "text": "A.1 Comprehensive Related Work 16   \nA.2 Decentralized Partially Observable Markov Decision Process . 17   \nA.3 Proof of Theorem 3.1 . . 17   \nA.4 Proof of Theorem 3.2 . 18   \nA.5 MAST with Different Algorithms 18   \nA.6 Limitations of MAST . 21 ", "page_idx": 15}, {"type": "text", "text": "B Experimental Details 21 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Hardware Setup . 21   \nB.2 Environment . . . 21   \nB.3 Hyperparameter Settings . . 22   \nB.4 Calculation of Model Sizes and FLOPs . . 22   \nB.5 Training Curves of Comparative Evaluation in Section 4.1 26   \nB.6 Standard Deviations of Results in Table 1 . 27   \nB.7 Ablation Study . . . 28   \nB.8 Sensitivity Analysis for Hyperparameters . 29   \nB.9 Experiments on QMIX in Multi-Agent MuJoCo . . 30   \nB.10 Experiments on FACMAC in SMAC . 30   \nB.11 Visualization of Sparse Masks . . 30 ", "page_idx": 15}, {"type": "text", "text": "A Additional Details for MAST Framework ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Comprehensive Related Work ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Sparse networks, initially proposed in deep supervised learning can train a $90\\%$ -sparse network without performance degradation from scratch. However, for deep reinforcement learning, the learning target is not fixed but evolves in a bootstrap way [Tesauro et al., 1995], and the distribution of the training data can also be non-stationary [Desai et al., 2019], which makes the sparse training more difficult. In the following, we list some representative works for training sparse models from supervised learning to reinforcement learning. ", "page_idx": 15}, {"type": "text", "text": "Sparse Models in Supervised Learning Various techniques have been explored for creating sparse networks, ranging from pruning pre-trained dense networks [Han et al., 2015, 2016, Srinivas et al., 2017], to employing methods like derivatives [Dong et al., 2017, Molchanov et al., 2019], regularization [Louizos et al., 2018], dropout [Molchanov et al., 2017], and weight reparameterization [Schwarz et al., 2021]. Another avenue of research revolves around the Lottery Ticket Hypothesis (LTH) [Frankle and Carbin, 2019], which posits the feasibility of training sparse networks from scratch, provided a sparse \u201cwinning ticket\" initialization is identified. This hypothesis has garnered support in various deep learning models [Chen et al., 2020a, Brix et al., 2020]. Additionally, there is a body of work dedicated to training sparse neural networks from the outset, involving techniques that evolve the structures of sparse networks during training. Examples include Deep Rewiring (DeepR) [Bellec et al., 2018], Sparse Evolutionary Training (SET) [Mocanu et al., 2018], Dynamic Sparse Reparameterization (DSR) [Mostafa and Wang, 2019], Sparse Networks from Scratch (SNFS) [Dettmers and Zettlemoyer, 2019], and Rigged Lottery (RigL) [Evci et al., 2020]. Furthermore, methods like Single-Shot Network Pruning (SNIP) [Lee et al., 2019] and Gradient Signal Preservation (GraSP) [Wang et al., 2020] are geared towards identifying static sparse networks prior to training. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Sparse Models in Single-Agent RL Existing research [Schmitt et al., 2018, Zhang et al., 2019] has employed knowledge distillation with static data to ensure training stability and generate small dense agents. Policy Pruning and Shrinking (PoPs) [Livne and Cohen, 2020] generates sparse agents through iterative policy pruning, while the LTH in DRL is first indentified in [Yu et al., 2020]. Another line of investigation aims to train sparse DRL models from scratch, eliminating the necessity of pre-training a dense teacher. Specifically, [Sokar et al., 2022] introduces the Sparse Evolutionary Training (SET) approach, achieving a remarkable $50\\%$ sparsity level through topology evolution in DRL. Additionally, [Graesser et al., 2022] observes that pruning often yields superior results, with plain dynamic sparse training methods, including SET and RigL, significantly outperforming static sparse training approaches. More recently, RLx2 [Tan et al., 2022] has demonstrated the capacity to train DRL agents with highly sparse neural networks from scratch. Nevertheless, the application of RLx2 in MARL yields poor results, as demonstrated in Section 4.1. ", "page_idx": 16}, {"type": "text", "text": "Sparse Models in MARL Existing works have made attempts to train sparse MARL agents, such as [Yang et al., 2022], which prunes networks for multiple agents during training, employing weight grouping [Wang et al., 2019]. Another avenue of sparse MARL research seeks to enhance the scalability of MARL algorithms through sparse architectural modifications. For instance, [Sun et al., 2020] proposes the use of a sparse communication graph with graph neural networks to reduce problem scale. [Kim and Sung, 2023] adopts structured pruning for a deep neural network to extend the scalability. Yet another strand of sparse MARL focuses on parameter sharing between agents to reduce the number of trainable parameters, with representative works including [Gupta et al., 2017, Li et al., 2021, Christianos et al., 2021]. However, existing methods fail to maintain high sparsity throughout the training process, such that the FLOPs reduction during training is incomparable to the MAST framework outlined in our paper. ", "page_idx": 16}, {"type": "text", "text": "A.2 Decentralized Partially Observable Markov Decision Process ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We model the MARL problem as a decentralized partially observable Markov decision process (Dec-POMDP) [Oliehoek et al., 2016], represented by a tuple $\\langle N,S,\\mathcal{U},P,r,\\mathcal{Z},O,\\gamma\\rangle$ , where ${\\mathcal{N}}=$ $\\{1,\\ldots,N\\}$ denotes the finite set of agents, $\\boldsymbol{S}$ is the global state space, $\\boldsymbol{\\mathcal{U}}$ is the action space for an agent, $P$ is the transition probability, $r$ is the reward function, $\\mathcal{Z}$ is the observation space for an agent, $O$ is the observation function, and and $\\gamma\\in[0,1)$ is the discount factor. At each timestep $t$ , each agent $i\\in\\mathcal{N}$ receives an observation $z\\in{\\mathcal{Z}}$ from the observation function $O(s,i):S\\times\\Bar{N}\\mapsto\\mathcal{Z}$ due to partial observability, and chooses an action $u_{i}\\in\\mathcal{U}$ , which forms a joint action $\\pmb{u}\\in\\pmb{\\mathcal{U}}\\equiv\\mathcal{U}^{n}$ . The joint action $\\textbf{\\em u}$ taken by all agents leads to a transition to the next state $s^{\\prime}$ according to transition probability $P(s^{\\prime}\\mid s,\\pmb{u}):\\bar{\\mathcal{S}}\\times\\bar{\\mathcal{U}}\\times\\mathcal{S}\\mapsto[0,1]$ and a joint reward $r(s,\\pmb{u}):\\mathcal{S}\\times\\mathcal{U}\\mapsto\\bar{\\mathbb{R}}$ . As the time goes by, each agent $i\\in\\mathcal{N}$ has an action-observation history $\\tau_{i}\\in\\mathcal{T}\\equiv(\\mathcal{Z}\\times\\mathcal{U})^{*}$ , where $\\tau$ is the history space. Based on $\\tau_{i}$ , each agent $i$ outputs an action $u_{i}$ according to its constructed policy $\\pi_{i}(u_{i}\\mid$ $\\tau_{i}):\\boldsymbol{\\mathcal{T}}\\times\\mathcal{U}\\mapsto[0,1]$ . The goal of agents is to find an optimal joint policy $\\pi=\\langle\\pi_{1},...,\\pi_{N}\\rangle$ , which maximize the joint cumulative rewards $J(s_{0};\\pi)=\\mathbb{E}_{{u_{t}}\\sim\\pi(\\cdot|s_{t}),s_{t+1}\\sim P(\\cdot|s_{t},u_{t})}\\left[\\sum_{t=0}^{\\infty}\\gamma^{i}r(s_{t},u_{t})\\right]\\!,$ where is the initial state. The joint action-value function associated with policy $\\pi$ is defined as $Q^{\\pi}(s_{t},\\bar{u}_{t})=\\mathbb{E}_{{u}_{t+i}\\sim\\pi(\\cdot|s_{t+i}),s_{t+i+1}^{\\sim}\\sim P(\\cdot|s_{t+i},u_{t+i})}\\left[\\sum_{i=0}^{\\infty}\\gamma^{i}r(s_{t+i},u_{t+i})\\right]\\!.$ . ", "page_idx": 16}, {"type": "text", "text": "A.3 Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. By the definitions of multi-step targets, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{\\rho}[\\mathcal{T}_{n}(s_{t},u_{t})]}\\\\ &{\\quad\\quad\\quad\\prod_{n=1}^{n-1}\\gamma^{k}r_{t+k}+\\gamma^{n}\\operatorname*{max}_{u}Q_{\\mathrm{tot}}\\left(s_{t+n},u;\\theta\\right)]}\\\\ &{\\quad\\quad\\quad\\prod_{k=0}^{n-1}\\gamma^{k}r_{t+k}+\\gamma^{n}Q_{\\mathrm{tot}}\\left(s_{t+n},\\pi\\bigl(s_{t+n}\\bigr);\\theta\\right)\\Bigr]}\\\\ &{\\quad\\quad\\quad\\prod_{k=0}^{n-1}\\gamma^{k}r_{t+k}+\\gamma^{n}Q_{\\mathrm{tot}}\\left(s_{t+n},\\pi\\bigl(s_{t+n}\\bigr);\\theta\\right)\\Bigr]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-\\mathbb{E}_{\\rho}\\big[\\sum_{k=0}^{n-1}\\gamma^{k}r_{t+k}+\\gamma^{n}Q_{\\mathrm{tot}}\\left(s_{t+n},\\rho(s_{t+n});\\phi\\right)\\big]}\\\\ &{\\quad+\\gamma^{n}\\mathbb{E}_{\\rho}\\big[Q_{\\mathrm{tot}}\\left(s_{t+n},\\pi(s_{t+n});\\theta\\right)-Q_{\\mathrm{tot}}\\left(s_{t+n},\\rho(s_{t+n});\\phi\\right)\\big]}\\\\ &{\\quad=\\!\\!\\mathbb{E}_{\\rho}\\big[\\sum_{k=0}^{n-1}\\gamma^{k}r_{t+k}+\\gamma^{n}Q_{\\mathrm{tot}}^{\\rho}\\left(s_{t+n},\\rho(s_{t+n})\\right)\\big]}\\\\ &{\\quad\\quad+\\gamma^{n}\\mathbb{E}_{\\rho}\\big[Q_{\\mathrm{tot}}\\left(s_{t+n},\\rho(s_{t+n});\\phi\\right)-Q_{\\mathrm{tot}}^{\\rho}\\left(s_{t+n},\\rho(s_{t+n})\\right)\\big]}\\\\ &{\\quad\\quad+\\gamma^{n}\\mathbb{E}_{\\rho}\\big[Q_{\\mathrm{tot}}\\left(s_{t+n},\\pi(s_{t+n});\\theta\\right)-Q_{\\mathrm{tot}}\\left(s_{t+n},\\rho(s_{t+n});\\phi\\right)\\big]}\\\\ &{\\quad=\\!Q_{\\mathrm{tot}}^{\\rho}\\left(s_{t},u_{t}\\right)+\\gamma^{n}\\mathbb{E}_{\\rho}\\big[(s_{t+n},\\rho(s_{t+n}))\\big]+\\gamma^{n}\\mathbb{E}_{\\rho}\\big[Q_{\\mathrm{tot}}\\left(s_{t+n},\\pi(s_{t+n});\\theta\\right)-Q_{\\mathrm{tot}}\\left(s_{t+n},\\rho(s_{t+n});\\phi\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Besides, we also have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\rho}[Q_{\\mathrm{tot}}\\left(s_{t+n},\\pi(s_{t+n});\\theta\\right)-Q_{\\mathrm{tot}}\\left(s_{t+n},\\rho(s_{t+n});\\phi\\right)]}\\\\ &{=\\!\\mathbb{E}_{\\rho}[Q_{\\mathrm{tot}}\\left(s_{t+n},\\pi(s_{t+n});\\theta\\right)-Q_{\\mathrm{tot}}^{\\pi}\\left(s_{t+n},\\pi(s_{t+n})\\right)]}\\\\ &{\\quad+\\mathbb{E}_{\\rho}[Q_{\\mathrm{tot}}^{\\pi}\\left(s_{t+n},\\pi(s_{t+n})\\right)-Q_{\\mathrm{tot}}^{\\rho}\\left(s_{t+n},\\rho(s_{t+n})\\right)]}\\\\ &{\\quad+\\mathbb{E}_{\\rho}[Q_{\\mathrm{tot}}^{\\rho}\\left(s_{t+n},\\rho(s_{t+n})\\right)-Q_{\\mathrm{tot}}\\left(s_{t+n},\\rho(s_{t+n});\\phi\\right)]}\\\\ &{=\\!\\mathbb{E}_{\\rho}[\\epsilon(s_{t+n},\\pi(s_{t+n}))]+\\mathbb{E}_{\\rho}[Q_{\\mathrm{tot}}^{\\pi}\\left(s_{t+n},\\pi(s_{t+n})\\right)-Q_{\\mathrm{tot}}^{\\rho}\\left(s_{t+n},\\rho(s_{t+n})\\right)]+\\mathbb{E}_{\\rho}[\\epsilon(s_{t+n},\\rho(s_{t+n}))].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, combining Eq. (4) and Eq. (5) gives ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\rho}[T_{n}(s_{t},u_{t})]-Q_{\\mathrm{tot}}^{\\pi}\\left(s_{t},u_{t}\\right)=\\gamma^{n}\\mathbb{E}_{\\rho}[2\\epsilon(s_{t+n},\\rho(s_{t+n}))+\\epsilon(s_{t+n},\\pi(s_{t+n}))]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n+\\,Q_{\\mathrm{tot}}^{\\rho}\\left(s_{t},u_{t}\\right)-Q_{\\mathrm{tot}}^{\\pi}\\left(s_{t},u_{t}\\right)+\\gamma^{n}\\mathbb{E}_{\\rho}\\big[Q_{\\mathrm{tot}}^{\\pi}\\left(s_{t+n},\\pi\\big(s_{t+n}\\big)\\right)-Q_{\\mathrm{tot}}^{\\rho}\\left(s_{t+n},\\rho\\big(s_{t+n}\\big)\\right)\\big]\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "A.4 Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Relace the Softmax operator in the proof of Theorem 3 in [Pan et al., 2021] with Eq. (3) gives the result directly. ", "page_idx": 17}, {"type": "text", "text": "A.5 MAST with Different Algorithms ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we present the pseudocode implementations of MAST for QMIX [Rashid et al., 2020b] and WQMIX [Rashid et al., 2020a] in Algorithm 2 and Algorithm 3, respectively. It is noteworthy that RES [Pan et al., 2021] exclusively modifies the training target without any alterations to the learning protocol or network structure. Consequently, the implementation of MAST with RES mirrors that of QMIX. ", "page_idx": 17}, {"type": "text", "text": "Crucially, MAST stands as a versatile sparse training framework, applicable to a range of value decomposition-based MARL algorithms, extending well beyond QMIX, WQMIX3, and RES. Furthermore, MAST\u2019s three innovative components\u2014hybrid $\\mathrm{TD}(\\lambda)$ , Soft Mellowmax operator, and dual buffer\u2014can be employed independently, depending on the specific algorithm\u2019s requirements. This flexible framework empowers the training of sparse networks from the ground up, accommodating a wide array of MARL algorithms. ", "page_idx": 17}, {"type": "text", "text": "In the following, we delineate the essential steps of implementing MAST with QMIX (Algorithm 2). The steps for WQMIX are nearly identical, with the exception of unrestricted agent networks and the unrestricted mixing network\u2019s inclusion. Also, note that we follow the symbol definitions from [Colom, 2021] in Algorithm 2 and 3. ", "page_idx": 17}, {"type": "text", "text": "Gradient-based Topology Evolution: The process of topology evolution is executed within Lines 31-33 in Algorithm 2. Specifically, the topology evolution update occurs at intervals of $\\Delta_{m}$ timesteps. For a comprehensive understanding of additional hyperparameters pertaining to topology evolution, please refer to the definitions provided in Algorithm 1. ", "page_idx": 18}, {"type": "text", "text": "TD Targets: Hybrid $\\mathrm{TD}(\\lambda)$ with Soft Mellowmax operator is computed in the Line 25 in Algorithm 2, which modify the TD target $y$ as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{y_{\\mathrm{S}}=\\left\\{G_{t}^{(1)},\\ \\begin{array}{l l}{\\mathrm{if}\\ t<T_{0}.}\\\\ {(1-\\lambda)\\sum_{n=1}^{\\infty}\\lambda^{n-1}\\mathcal{T}_{t}^{(n)},}&{\\mathrm{Otherwise.}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Algorithm 2 MAST-QMIX   \n1: Initialize sparse agent networks, mixing network and hypernetwork with random parameters $\\overline{{\\theta}}$   \nand random masks $M_{\\theta}$ with determined sparsity $S$ .   \n2: $\\hat{\\theta}\\leftarrow\\theta\\odot M_{\\theta}$ // Start with a random sparse network   \n3: Initialize target networks ${\\hat{\\theta}}^{-}\\gets{\\hat{\\theta}}$   \n4: Set the learning rate to $\\alpha$   \n5: Initialize the replay buffer $B_{1}\\leftarrow\\{\\}$ with large capacity $C_{1}$ and $B_{2}\\leftarrow\\{\\}$ with small capacity $C_{2}$   \n6: Initialize training $\\mathrm{step}\\gets0$   \n7: while step $<T_{m a x}$ do   \n8: $t\\leftarrow0$   \n9: $s_{0}\\leftarrow$ initial state   \n10: while $s_{t}\\neq$ terminal and $t<$ episode limit do   \n11: for each agent a do   \n12: $\\begin{array}{r l}&{\\tau_{t}^{a}\\gets\\tau_{t-1}^{\\widecheck{a}}\\cup\\left\\{(o_{t},u_{t-1})\\right\\}}\\\\ &{\\epsilon\\gets\\mathrm{epsilon-schedule(step)}}\\\\ &{u_{t}^{a}\\gets\\left\\{\\begin{array}{l l}{\\mathrm{argmax}_{u_{t}^{a}}\\,Q\\left(\\tau_{t}^{a},u_{t}^{a}\\right)\\quad}&{\\mathrm{with~probability~1-}\\epsilon}\\\\ {\\mathrm{randint}(1,|U|)\\quad}&{\\mathrm{with~probability~}\\epsilon}\\end{array}\\right.\\,,\\,\\ell\\gets g r e e d y\\;e x p l o r a t i o n}\\end{array}$   \n13:   \n14:   \n15: end for   \n16: Get reward $r_{t}$ and next state $s_{t+1}$   \n17: $\\begin{array}{r l}&{\\mathcal{B}_{1}\\leftarrow\\mathcal{B}_{1}\\cup\\left\\{\\left(s_{t},\\mathbf{u}_{t},r_{t},s_{t+1}\\right)\\right\\}\\stackrel{*}{\\mathcal{M}}D a t a\\;i n\\;t h e\\;b u\\rlap{/}f e r\\;i s\\;o f e p i s o d e s\\;f o r m.}\\\\ &{\\mathcal{B}_{2}\\leftarrow\\mathcal{B}_{2}\\cup\\left\\{\\left(s_{t},\\mathbf{u}_{t},r_{t},s_{t+1}\\right)\\right\\}}\\\\ &{t\\leftarrow t+1.\\mathrm{step}\\leftarrow\\mathrm{step}+1}\\end{array}$   \n18:   \n19:   \n20: end while   \n21: if $|\\beta_{1}|>$ batch-size then   \n22: $b\\leftarrow$ random batch of episodes from $\\boldsymbol{{\\cal B}}_{1}$ and $B_{2}$ // Sample from dual buffers.   \n23: for each timestep $t$ in each episode in batch $b$ do   \n24:   \n$Q_{t o t}\\leftarrow\\mathrm{Mixing-network}\\left((Q_{1}(\\tau_{t}^{1},u_{t}^{1}),\\cdots,Q_{n}(\\tau_{t}^{n},u_{t}^{n}));\\mathrm{Hypernetwork}(s_{t};\\hat{\\theta})\\right)$   \n25: Compute TD target $y$ according to Eq. (6). // $T D(\\lambda)$ targets with Soft Mellowmax   \noperator.   \n26: end for   \n27: $\\begin{array}{l}{\\Delta Q_{t o t}\\leftarrow y-Q_{t o t}}\\\\ {\\Delta\\hat{\\theta}\\leftarrow\\nabla_{\\hat{\\theta}}\\frac{1}{b}\\sum_{\\hat{\\theta}}(\\Delta Q_{t o t})^{2}}\\\\ {\\hat{\\theta}\\leftarrow\\hat{\\theta}-\\alpha\\Delta\\hat{\\theta}}\\end{array}$   \n28:   \n29:   \n30: end if   \n31: if step mod $\\Delta_{m}=0$ then   \n32: Topology_Evolution(network $\\operatorname{\\romannumeral1}\\operatorname{\\romannumeral2}$ by Algorithm 1.   \n33: end if   \n34: if step mod $I=0$ , where is the target network update interval then   \n35: $\\hat{\\theta}^{-}\\gets\\hat{\\theta}$ // Update target network.   \n36: $\\hat{\\theta}^{-}\\leftarrow\\hat{\\theta}^{-}\\odot\\overset{\\cdot}{M}_{\\hat{\\theta}}$   \n37: end if   \n38: end while ", "page_idx": 18}, {"type": "text", "text": "", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Algorithm 3 MAST-(OW)QMIX   \n1: Initialize sparse agent networks, mixing network and hypernetwork with random parameters $\\theta$   \nand random masks $M_{\\theta}$ with determined sparsity $S$ .   \n2: Initialize unrestricted agent networks and unrestricted mixing network with random parameters   \n$\\phi$ and random masks $M_{\\phi}$ with determined sparsity $S$ .   \n3: $\\hat{\\theta}\\leftarrow\\theta\\odot M_{\\theta}$ , $\\hat{\\phi}\\leftarrow\\phi\\odot M_{\\phi}$ // Start with a random sparse network   \n4: Initialize target networks $\\hat{\\theta}^{\\dot{-}}\\gets\\hat{\\theta},\\hat{\\phi}^{-}\\gets\\hat{\\phi}$   \n5: Set the learning to rate $\\alpha$   \n6: Initialize the replay buffer $B_{1}\\leftarrow\\{\\}$ with large capacity $C_{1}$ and $B_{2}\\leftarrow\\{\\}$ with small capacity $C_{2}$   \n7: Initialize training $\\mathrm{step}\\gets0$   \n8: while step $<T_{m a x}$ do   \n9: $t\\leftarrow0$ ,   \n10: $s_{0}\\leftarrow$ initial state   \n11: while $s_{t}\\neq$ terminal and $t<$ episode limit do   \n12: for each agent a do   \n13: $\\begin{array}{r l}&{\\tau_{t}^{a}\\gets\\tau_{t-1}^{a}\\cup\\{(o_{t},u_{t-1})\\}}\\\\ &{\\epsilon\\gets\\mathrm{epsilon-schedule(step)}}\\\\ &{u_{t}^{a}\\gets\\left\\{\\mathrm{argmax}_{u_{t}^{a}}\\,Q(\\tau_{t}^{a},u_{t}^{a};\\hat{\\theta})\\right.\\quad\\mathrm{with~probability~}1-\\epsilon\\quad}\\\\ &{\\left.\\mathrm{randint}(1,|U|)\\right.}\\end{array}$   \n14:   \n15:   \n16: end for   \n17: Get reward $r_{t}$ and next state $s_{t+1}$   \n18: $B_{1}\\leftarrow B_{1}\\cup\\{(s_{t},{\\mathbf{u}}_{t},r_{t},s_{t+1})\\}$ // Data in the buffer is of episodes form.   \n19: $B_{2}\\gets B_{2}\\cup\\{(s_{t},\\mathbf{u}_{t},r_{t},s_{t+1})\\}$   \n20: $t\\gets t+1$ , $\\mathrm{step}\\gets\\mathrm{step}+1$   \n21: end while   \n22: if $|\\beta_{1}|>$ batch-size then   \n23: $b\\leftarrow$ random batch of episodes from $\\boldsymbol{{\\cal B}}_{1}$ and $B_{2}$ // Sample from dual buffers.   \n24: for each timestep $t$ in each episode in batch $b$ do   \n25:   \n$Q_{t o t}\\gets\\mathrm{Mixing-network}\\left((Q_{1}(\\tau_{t}^{1},u_{t}^{1};\\hat{\\theta}),...,Q_{n}(\\tau_{t}^{n},u_{t}^{n};\\hat{\\theta}));\\mathrm{Hypernetwork}(s_{t};\\hat{\\theta})\\right)$   \n26:   \n$\\hat{Q}^{*}\\gets\\mathrm{Unrestricted-Mixing-network}\\left(Q_{1}(\\tau_{t}^{1},u_{t}^{1};\\hat{\\phi}),...,Q_{n}(\\tau_{t}^{n},u_{t}^{n};\\hat{\\phi}),s_{t}\\right)$   \n27: Compute TD target $y$ with target Unrestricted-Mixing network according to Eq. (6).   \n$\\nearrow T D(\\lambda)$ targets with Soft Mellowmax operator.   \n28: $\\omega(s_{t},\\mathbf{u_{t}})\\gets\\left\\{1,\\quad Q_{t o t}<y\\right.$   \n29: end for   \n30: $\\begin{array}{r l}&{\\Delta Q_{t o t}\\leftarrow y-Q_{t o t}}\\\\ &{\\Delta\\hat{\\theta}\\leftarrow\\nabla_{\\hat{\\theta}}\\frac{1}{b}\\sum_{}\\omega(s,\\mathbf{u})(\\Delta Q_{t o t})^{2}}\\\\ &{\\hat{\\theta}\\leftarrow\\hat{\\theta}-\\alpha\\Delta\\hat{\\theta}}\\\\ &{\\Delta\\hat{Q}^{*}\\leftarrow y-\\hat{Q}^{*}}\\\\ &{\\Delta\\hat{\\phi}\\leftarrow\\nabla_{\\hat{\\phi}}\\frac{1}{b}\\sum_{}(\\Delta\\hat{Q}^{*})^{2}}\\\\ &{\\hat{\\phi}\\leftarrow\\hat{\\phi}-\\alpha\\Delta\\hat{\\phi}}\\end{array}$   \n31:   \n32:   \n33:   \n34:   \n35:   \n36: end if   \n37: if step mod $\\Delta_{m}=0$ then   \n38: Topology_Evolution(networks $\\hat{\\theta}$ ) and Topology_Evolution(networks $\\hat{\\phi}$ ) by Algorithm 1.   \n39: end if   \n40: if step mod $I=0$ , where is the target network update interval then   \n41: $\\begin{array}{r l}&{\\hat{\\theta}^{-}\\leftarrow\\hat{\\theta},\\hat{\\phi}^{-}\\leftarrow\\hat{\\phi}}\\\\ &{\\hat{\\theta}^{-}\\leftarrow\\hat{\\theta}^{-}\\odot M_{\\hat{\\theta}},\\hat{\\phi}^{-}\\leftarrow\\hat{\\phi}^{-}\\odot M_{\\hat{\\phi}}}\\end{array}$   \n42:   \n43: end if   \n44: end while ", "page_idx": 19}, {"type": "text", "text": "Here, $\\begin{array}{r l r l}{\\lambda}&{{}}&{\\in}&{{}\\quad[0,1]}\\end{array}$ is a hyperparameter, and $\\begin{array}{r l r}{\\mathcal{T}_{t}^{(n)}}&{{}=}&{\\sum_{i=t}^{t+n}\\gamma^{i-t}r_{i}\\quad+}\\end{array}$ $\\gamma^{n+1}f_{s}\\left(\\mathrm{sm}_{\\omega}({\\bar{Q}}_{1}(\\tau_{1},\\cdot),\\cdot\\cdot\\cdot,\\mathrm{sm}_{\\omega}({\\bar{Q}}_{N}(\\tau_{N},\\cdot)\\right)$ , where $f_{s}$ denotes the mixing network and $\\bar{Q}_{i}$ is the target network of $Q_{i}$ . The loss function of MAST, ${\\mathcal{L}}_{\\mathrm{S}}(\\theta)$ , is defined as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{S}}(\\boldsymbol{\\theta})=\\mathbb{E}_{(s,\\mathbf{u},r,s^{\\prime})\\sim\\mathcal{B}_{1}\\cup\\mathcal{B}_{2}}\\left[\\left(y_{\\mathrm{S}}-Q_{t o t}(s,\\mathbf{u})\\right)^{2}\\right]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Dual Buffers: With the creation of two buffers $\\boldsymbol{{\\cal B}}_{1}$ and $B_{2}$ , the gradient update with data sampled from dual buffers is performed in Lines 21-30 in Algorithm 2. ", "page_idx": 20}, {"type": "text", "text": "A.6 Limitations of MAST ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "This paper introduces MAST, a novel framework for sparse training in deep MARL, leveraging gradient-based topology evolution to explore network configurations efficiently. However, understanding its limitations is crucial for guiding future research efforts. ", "page_idx": 20}, {"type": "text", "text": "Hyperparameters: MAST relies on multiple hyperparameters for its key components: topology evolution, $\\mathrm{TD}(\\lambda)$ targets with Soft Mellowmax Operator, and dual buffers. Future work could explore methods to automatically determine these hyperparameters or streamline the sparse training process with fewer tunable settings. ", "page_idx": 20}, {"type": "text", "text": "Implementation: While MAST achieves efficient MARL agent training with minimal performance trade-offs using ultra-sparse networks surpassing $90\\%$ sparsity, its current use of unstructured sparsity poses challenges for running acceleration. The theoretical reduction in FLOPs might not directly translate to reduced running time. Future research should aim to implement MAST in a structured sparsity pattern to bridge this gap between theoretical efficiency and practical implementation. ", "page_idx": 20}, {"type": "text", "text": "B Experimental Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we offer comprehensive experimental insights, encompassing hardware configurations, environment specifications, hyperparameter settings, model size computations, FLOPs calculations, and supplementary experimental findings. ", "page_idx": 20}, {"type": "text", "text": "B.1 Hardware Setup ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Our experiments are implemented with PyTorch 2.0.0 [Paszke et al., 2017] and run on $4\\times$ NVIDIA GTX Titan X (Pascal) GPUs. Each run needs about $12\\sim24$ hours for QMIX or WQMIX, and about $24\\sim72$ hours for RES for two million steps. depends on the environment types. ", "page_idx": 20}, {"type": "text", "text": "B.2 Environment ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We assess the performance of our MAST framework using the SMAC benchmark [Samvelyan et al., 2019], a dedicated platform for collaborative multi-agent reinforcement learning research based on Blizzard\u2019s StarCraft II real-time strategy game, specifically version 4.10. It is important to note that performance may vary across different versions. Our experimental evaluation encompasses four distinct maps, each of which is described in detail below. ", "page_idx": 20}, {"type": "text", "text": "\u2022 3m: An easy map, where the agents are 3 Marines, and the enemiesa are 3 Marines.   \n\u2022 2s3z: An easy map, where the agents are 2 Stalkers and 3 Zealots, and the enemies are 2 Stalkers and 3 Zealots.   \n\u2022 3s5z: An easy map, where the agents are 3 Stalkers and 5 Zealots, and the enemies are 3 Stalkers and 5 Zealots.   \n\u2022 2c_vs_64zg: A hard map, where the agents are 2 Colossi, and the enemies are 64 Zerglings. ", "page_idx": 20}, {"type": "text", "text": "We also evaluate MAST on the Multi-Agent MuJoCo (MAMuJoCo) benchmark from [Peng et al., 2021], which is an environment designed for evaluating continuous MARL algorithms, focusing on cooperative robotic control tasks. It extends the single-agent MuJoCo framework included with OpenAI Gym [Brockman et al., 2016]. Inspired by modular robotics, MAMuJoCo includes scenarios with a large number of agents, aiming to stimulate progress in continuous MARL by providing diverse and challenging tasks for decentralized coordination. We test MAST-COMIX in the Humanoid, Humanoid Standup, and ManyAgent Swimmer scenarios in MAMuJoCo. The environments are tested using their default configurations, with other settings following FACMAC [Peng et al., 2021]. Specifically, we set the maximum observation distance to $k\\,=\\,0$ . In the ManyAgent Swimmer scenario, we configure 10 agents, each controlling a consecutive segment of length 2. The agent network architecture of MAST-COMIX uses an MLP with two hidden layers of 400 dimensions each, following the settings in FACMAC. All other hyperparameters are also based on FACMAC. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "B.3 Hyperparameter Settings ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Table 3 provides a comprehensive overview of the hyperparameters employed in our experiments for MAST-QMIX, MAST-WQMIX, and MAST-RES. It includes detailed specifications for network parameters, RL parameters, and topology evolution parameters, allowing for a thorough understanding of our configurations. Besides, MAST is implemented based on the PyMARL [Samvelyan et al., 2019] framework with the same network structures and hyperparameters as given in Table 3. We also provide a hyperparameter recommendation for three key components, i.e. gradient-based topology evolution, Soft Mellowmax enabled hybrid $\\mathrm{TD}(\\lambda)$ targets and dual buffers, in Table 2 for deployment MAST framework in other problems. ", "page_idx": 21}, {"type": "table", "img_path": "Gug7wc0BSs/tmp/2e9520e67963837a1b036673f95707da688bcea272125d69ee7e4b101b8c4d13.jpg", "table_caption": ["Table 2: Recommendation for Key Hyperparameters in MAST. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Besides, to extend the existing QMIX to continuous action spaces, we utilized COMIX from FACMAC [Peng et al., 2021] for our experiments, which employs the Cross-Entropy Method (CEM) for approximate greedy action selection. The hyperparameter configuration of CEM also follows FACMAC settings. ", "page_idx": 21}, {"type": "text", "text": "B.4 Calculation of Model Sizes and FLOPs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "B.4.1 Model Size ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "First, we delineate the calculation of model sizes, which refers to the total number of parameters within the model. ", "page_idx": 21}, {"type": "text", "text": "\u2022 For a sparse network with $L$ fully-connected layers, the model size, as expressed in prior works [Evci et al., 2020, Tan et al., 2022], can be computed using the equation: ", "page_idx": 21}, {"type": "equation", "text": "$$\nM_{\\mathrm{linear}}=\\sum_{l=1}^{L}(1-S_{l})I_{l}O_{l},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $S_{l}$ represents the sparsity, $I_{l}$ is the input dimensionality, and $O_{l}$ is the output dimensionality of the $l$ -th layer. ", "page_idx": 21}, {"type": "text", "text": "\u2022 For a sparse network with $L$ GRU layers, considering the presence of 3 gates in a single layer, the model size can be determined using the equation: ", "page_idx": 21}, {"type": "equation", "text": "$$\nM_{\\mathrm{{GRU}}}=\\sum_{l=1}^{L}(1-S_{l})\\times3\\times h_{l}\\times(h_{l}+I_{l}),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $h_{l}$ represents the hidden state dimensionality. ", "page_idx": 21}, {"type": "table", "img_path": "Gug7wc0BSs/tmp/3e2d305f144aeee431610e9976d40b58d13c47a58dad0d160018ff5a8eb13b82.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Specifically, the \"Total Size\" column in Table 1 within the manuscript encompasses the model size, including both agent and mixing networks during training. For QMIX, WQMIX, and RES, target networks are employed as target agent networks and target mixing networks. We denote the model sizes of the agent network, mixing network, unrestricted agent network, and unrestricted mixing network as $M_{\\mathrm{Agent}}$ , $M_{\\mathrm{Mix}}$ , $M_{\\mathrm{U}}$ nrestricted-Agent, and $M_{\\mathrm{Unrestricted-Mix}}$ , respectively. Detailed calculations of these model sizes are provided in the second column of Table 4. ", "page_idx": 23}, {"type": "table", "img_path": "Gug7wc0BSs/tmp/4aee34f646e9be20bc491752898babcbe18c3b864a68563cc0bd9dec94f73242.jpg", "table_caption": ["Table 4: FLOPs and model size for MAST-QMIX , MAST-WQMIX and MAST-RES. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "B.4.2 FLOPs Calculation ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Initially, for a sparse network with $L$ fully-connected layers, the required FLOPs for a forward pass are computed as follows (also adopted in [Evci et al., 2020] and [Tan et al., 2022]): ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{FLOPs}=\\sum_{l=1}^{L}(1-S_{l})(2I_{l}-1)O_{l},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $S_{l}$ is the sparsity, $I_{l}$ is the input dimensionality, and $O_{l}$ is the output dimensionality of the $l$ -th layer. Similarly, for a sparse network with $L$ GRU [Chung et al., 2014] layers, considering the presence of 3 gates in a single layer, the required FLOPs for a forward pass are: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{FLOPs}=\\sum_{l=1}^{L}(1-S_{l})\\times3\\times h_{l}\\times[2(h_{l}+I_{l})-1],\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $h_{l}$ is the hidden state dimensionality. ", "page_idx": 23}, {"type": "text", "text": "We denote $B$ as the batch size employed in the training process, and $\\mathtt{F L O P s}_{\\mathrm{Agent}}$ and $\\mathrm{FLOPs}_{\\mathrm{Mix}}$ as the FLOPs required for a forward pass in the agent and mixing networks, respectively. The inference FLOPs correspond exactly to $\\mathtt{F L O P s}_{\\mathrm{Agent}}$ , as detailed in the last column of Table 4. When it comes to training FLOPs, the calculation encompasses multiple forward and backward passes across various networks, which will be thoroughly elucidated later. Specifically, we compute the FLOPs necessary for each training iteration. Additionally, we omit the FLOPs associated with the following processes, as they exert minimal influence on the ultimate result: ", "page_idx": 23}, {"type": "text", "text": "\u2022 Interaction with the environment: This operation, where agents decide actions for interaction with the environment, incurs FLOPs equivalent to FLOPsAgent. Notably, this value is considerably smaller than the FLOPs required for network updates, as evident in Table 4, given that $B\\gg1$ .   \n\u2022 Updating target networks: Each parameter in the networks is updated as $\\theta^{\\prime}\\gets\\theta$ . Consequently, the number of FLOPs in this step mirrors the model size, and is thus negligible.   \n\u2022 Topology evolution: This element is executed every 200 gradient updates. To be precise, the average FLOPs involved in topology evolution are computed as $\\begin{array}{r}{B^{\\stackrel{\\textstyle\\bullet}{\\times}}\\frac{2\\mathrm{FLOPs}_{\\mathrm{Agent}}}{(1-S^{(a)})\\Delta_{m}}}\\end{array}$ 2FLOPsAgent for the agent, and B \u00d7 (12\u2212FSL(OmP)s)Mi\u2206xm for the mixer. Given that \u2206m = 200, the FLOPs incurred by topology evolution are negligible. ", "page_idx": 23}, {"type": "text", "text": "Therefore, our primary focus shifts to the FLOPs related to updating the agent and mixer. We will first delve into the details for QMIX, with similar considerations for WQMIX and RES. ", "page_idx": 23}, {"type": "text", "text": "B.4.3 Training FLOPs Calculation in QMIX ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Recall the way to update networks in QMIX is given by ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\theta\\leftarrow\\theta-\\alpha\\nabla_{\\theta}\\frac{1}{B}\\sum(y_{t}-Q_{t o t}(s_{i},a_{i};\\theta))^{2},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $B$ is the batch size. Subsequently, we can compute the FLOPs of training as: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{FLOPs}_{\\mathrm{train}}=\\mathrm{FLOPs}_{\\mathrm{TD\\_target}}+\\mathrm{FLOPs}_{\\mathrm{compute\\_loss}}+\\mathrm{FLOPs}_{\\mathrm{backward\\_pass}},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\mathrm{FLOPs}_{\\mathrm{TD}_{\\mathrm{s}}}$ _target, FLOPscompute_loss, and FLOPsbackward_pass refer to the numbers of FLOPs in computing the TD targets in forward pass, loss function in forward pass, and gradients in backward pass (backward-propagation), respectively. By Eq. (6) and (7), we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{FLOPs}_{\\mathrm{TD\\_target}}=B\\times(\\mathrm{FLOPs}_{\\mathrm{Agent}}+\\mathrm{FLOPs}_{\\mathrm{Mix}}),}\\\\ {\\mathrm{FLOPs}_{\\mathrm{compute\\_loss}}=B\\times(\\mathrm{FLOPs}_{\\mathrm{Agent}}+\\mathrm{FLOPs}_{\\mathrm{Mix}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For the FLOPs of gradients backward propagation, FLOPsbackward_pass, we compute it as two times the computational expense of the forward pass, which is adopted in existing literature [Evci et al., 2020], i.e., ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{FLOPs}_{\\mathrm{backward\\_pass}}=B\\times2\\times(\\mathrm{FLOPs}_{\\mathrm{Agent}}+\\mathrm{FLOPs}_{\\mathrm{Mix}}),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Combining Eq. (13), Eq. (14), and Eq. (15), the FLOPs of training in QMIX is: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{FLOPs}_{\\mathrm{train}}=B\\times4\\times(\\mathrm{FLOPs}_{\\mathrm{Agent}}+\\mathrm{FLOPs}_{\\mathrm{Mix}}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "B.4.4 Training FLOPs Calculation in WQMIX ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The way to update the networks in WQMIX is different from that in QMIX. Specifically, denote the parameters of the original network and unrestricted network as $\\theta$ and $\\phi$ , respectively, which are updated according to ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta\\longleftarrow\\theta-\\alpha\\nabla_{\\theta}\\displaystyle\\frac{1}{B}\\sum_{i}\\omega(s_{i},a_{i})(\\mathcal{T}_{\\lambda}-Q_{t o t}(s_{i},a_{i};\\theta))^{2}}\\\\ &{\\phi\\longleftarrow\\!\\phi-\\alpha\\nabla_{\\phi}\\displaystyle\\frac{1}{B}\\sum_{i}(\\mathcal{T}_{\\lambda}-\\hat{Q}^{*}(s_{i},a_{i};\\phi))^{2}}\\end{array},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $B$ is the batch size, $\\omega$ is the weighting function, $\\hat{Q}^{*}$ is the unrestricted joint action value function. As shown in Algorithm 3, the way to compute TD target in WQMIX is different from that in QMIX. Thus, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{FLOPs}_{\\mathrm{TD\\mathrm{=}\\mathrm{tr}{\\mathrm{pet}}}}=B\\times(\\mathrm{FLOPs}_{\\mathrm{Unrestricted\\mathrm{-}A g e n t}}+\\mathrm{FLOPs}_{\\mathrm{Unrestricted\\mathrm{-}M i x}}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "In this paper, we take an experiment on one of two instantiations of QMIX. i.e., OW-QMIX [Rashid et al., 2020a]. Thus, the number of FLOPs in computing loss is ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{\\mathbf{\\pi}}\\mathrm{{LOPs}}_{\\mathrm{compute\\_loss}}=B\\times\\left(\\mathbf{\\mathrm{FLOPs}}_{\\mathrm{Agent}}+\\mathbf{\\mathrm{FLOPs}}_{\\mathrm{Mix}}+\\mathbf{\\mathrm{FLOPs}}_{\\mathrm{Unresticted\\_Agent}}+\\mathbf{\\mathrm{FLOPs}}_{\\mathrm{Unrestricted\\_Mix}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "(19) where unrestricted-agent and unrestricted-mix have similar network architectures as $Q_{t o t}$ and $Q_{t o t}$ to, respevtively. The FLOPs of gradients backward propagation can be given as F $\\begin{array}{r}{\\mathbb{U}\\mathrm{De}_{\\mathrm{buckward\\_pass}}=B\\times2\\times\\left(\\mathbb{F}\\mathrm{LOPs}_{\\mathrm{Agent}}+\\mathbb{F}\\mathrm{LOPs}_{\\mathrm{Mix}}+\\mathbb{F}\\mathrm{LOPs}_{\\mathrm{Unrestriced\\_Agent}}+\\mathbb{F}\\mathrm{LOPs}_{\\mathrm{Unrestriced\\_Mix}}\\right).}\\end{array}$ (20) Thus, the FLOPs of training in WQMIX can be computed by ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{FLOPs}_{\\mathrm{train}}=B\\times(3\\mathrm{FLOPs}_{\\mathrm{Agent}}+3\\mathrm{FLOPs}_{\\mathrm{Mix}}+4\\mathrm{FLOPs}_{\\mathrm{Unestricted-Agent}}+4\\mathrm{FLOPs}_{\\mathrm{Unestricted-Mix}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "B.4.5 Training FLOPs Calculation in RES ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Calculations of FLOPs for RES are similar to those in QMIX. The way to update the network parameter in RES is: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\theta\\leftarrow\\theta-\\alpha\\nabla_{\\theta}\\frac{1}{B}\\sum_{i}(\\mathcal{T}_{\\lambda}-Q_{t o t}(s_{i},a_{i};\\theta))^{2},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $B$ is the batch size. Meanwhile, note that the way to compute TD target in RES [Pan et al., 2021] includes computing the approximate Softmax operator, we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{FLOPs}_{\\mathrm{TD\\_target}}=B\\times\\mathrm{(FLOPs}_{\\mathrm{Agent}}+n\\times m\\times\\mathrm{(2FLOPs}_{\\mathrm{Mix}})),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $n$ is the number of agents, $m$ is the maximum number of actions an agent can take in a scenario. Other terms for updating networks are the same as QMIX. Thus, the FLOPs of training in RES can be computed by ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{FLOPs}_{\\mathrm{train}}=B\\times(4\\mathrm{FLOPs}_{\\mathrm{Agent}}+(3+2\\times n\\times m)\\mathrm{FLOPs}_{\\mathrm{Mix}}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Figure 13, Figure 14, and Figure 15 show the training curves of different algorithms in four SMAC environments. The performance is calculated as the average win rate per episode over the last 20 evaluations of the training. MAST outperforms baseline algorithms on all four environments in all three algorithms. We smooth the training curve by a 1-D fliter by scipy.signal.savgol_filter in Python with window_length $^{1=21}$ and polyorder $^{\\prime=2}$ . ", "page_idx": 25}, {"type": "text", "text": "These figures unequivocally illustrate MAST\u2019s substantial performance superiority over all baseline methods in all four environments across the three algorithms. Notably, static sparse (SS) consistently exhibit the lowest performance on average, highlighting the difficulty of finding optimal sparse network topologies in the context of sparse MARL models. Dynamic sparse training methods, namely SET and RigL, slightly outperform (SS), although their performance remains unsatisfactory. Sparse networks also, on average, underperform tiny dense networks. However, MAST significantly outpaces all other baselines, indicating the successful realization of accurate value estimation through our MAST method, which effectively guides gradient-based topology evolution. Notably, the single-agent method $\\tt R L x2$ consistently delivers subpar results in all experiments, potentially due to the sensitivity of the step length in the multi-step targets, and the failure of dynamic buffer for episode-form training samples. ", "page_idx": 25}, {"type": "image", "img_path": "Gug7wc0BSs/tmp/00b945d740bc91e87c32f3806b7b9ff31e5a0e569afd35fbf3333e4dd9c89c0a.jpg", "img_caption": ["Figure 13: Training processes of MAST-QMIX on four SAMC benchmarks. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "Gug7wc0BSs/tmp/8b7d73da107ef1fb4b6f8f63a4d349f2caa7b0d8b3750d78bbf162fe80961d91.jpg", "img_caption": ["(c) MAST-WQMIX $90\\%$ on $\\mathtt{3s5z}$ (d) MAST-WQMIX $90\\%$ on $64z\\mathrm{g}$ Figure 14: Training processes of MAST-WQMIX on four SAMC benchmarks. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "Gug7wc0BSs/tmp/2361d9c567be80dfaf8bdd782c28cb4195758f1ac8c4d2ca334c3908710a884d.jpg", "img_caption": ["Figure 15: Training processes of MAST-RES on four SAMC benchmarks. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "B.6 Standard Deviations of Results in Table 1 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Table 5 showcases algorithm performance across four SMAC environments along with their corresponding standard deviations. It\u2019s important to note that the data in Table 5 is not normalized concerning the dense model. Notably, MAST\u2019s utilization of topology evolution doesn\u2019t yield increased variance in results, demonstrating consistent performance across multiple random seeds. ", "page_idx": 26}, {"type": "text", "text": "Table 5: Results in Table 1 with standard deviations ", "page_idx": 27}, {"type": "table", "img_path": "Gug7wc0BSs/tmp/df39b6ea34cfe2ce4f10cb1918cd225546131da55cef5739a8cdaf38ff4447e3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "B.7 Ablation Study ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We conduct a comprehensive ablation study on three critical elements of MAST: hybrid $\\mathrm{TD}(\\lambda)$ targets, the Soft Mellowmax operator, and dual buffers, specifically evaluating their effects on QMIX and WQMIX. Notably, since MAST-QMIX shares similarities with MAST-RES, our experiments focus on QMIX and WQMIX within the $\\mathtt{3s5z}$ task. This meticulous analysis seeks to elucidate the influence of each component on MAST and their robustness in the face of hyperparameter variations. The reported results are expressed as percentages and are normalized to dense models. ", "page_idx": 27}, {"type": "text", "text": "Hybrid $\\mathbf{TD}(\\lambda)$ We commence our analysis by evaluating different burn-in time $T_{0}$ in hybrid $\\mathrm{TD}(\\lambda)$ in Table 6. Additionally, we explore the impact of different $\\lambda$ values within hybrid $\\mathrm{TD}(\\lambda)$ in Table 7. These results reveal hybrid $\\mathrm{TD}(\\lambda)$ targets achieve optimal performance with a burn-in time of $T_{0}\\,=\\,0.75\\mathrm{M}$ and $\\lambda=0.6$ . It is noteworthy that hybrid $\\mathrm{TD}(\\lambda)$ targets lead to significant performance improvements in WQMIX, while their impact on QMIX is relatively modest. ", "page_idx": 27}, {"type": "table", "img_path": "Gug7wc0BSs/tmp/bd4a08c03858eeef09dadb98844749772d8b4d61d4b1bedc4a14c5c3593aab7f.jpg", "table_caption": ["Table 6: Ablation study on burn-in time $T_{0}$ in hybrid $\\mathrm{TD}(\\lambda)$ . "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "Gug7wc0BSs/tmp/04d08deec107d2ca0bf52826cabdf3048fa27d758f08ceb44b7a0f7819ebfc92.jpg", "table_caption": ["Soft Mellowmax Operator The Soft Mellowmax operator in Eq.(3) introduces two hyperparameters, $\\alpha$ and $\\omega$ . A comprehensive examination of various parameter configurations is presented ", "Table 7: Ablation study on $\\lambda$ in hybrid $\\mathrm{TD}(\\lambda)$ . "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "in Table 8, showing that the performance of MAST exhibits robustness to changes in the two hyperparameters associated with the Soft Mellowmax operator. ", "page_idx": 28}, {"type": "text", "text": "Additionally, it is worth noting that the Softmax operator is also employed in [Pan et al., 2021] to mitigate overestimation in multi-agent Q learning. To examine the effectiveness of various operators, including max, Softmax, Mellowmax, and Soft Mellowmax, we conduct a comparative analysis in Figure 16. Our findings indicate that the Soft Mellowmax operator surpasses all other baselines in alleviating overestimation. Although the Softmax operator demonstrates similar performance to the Soft Mellowmax operator, it should be noted that the Softmax operator entails higher computational costs. ", "page_idx": 28}, {"type": "image", "img_path": "Gug7wc0BSs/tmp/cb543e51fad319911bb050511e3e53e3ffa185c14ed07fadc313d2e07124059a.jpg", "img_caption": ["Figure 16: Comparison of different operators. "], "img_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "Gug7wc0BSs/tmp/fd108f9301fe1c016895ce07721e8b7e1090b978022289a8e7ab1fd25f11dbf2.jpg", "table_caption": ["Table 8: Ablation study on the Soft Mellowmax operator. "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "Dual Buffers In each training step, we concurrently sample two batches from the two buffers, $\\boldsymbol{{\\cal B}}_{1}$ and $\\boldsymbol{B}_{2}$ . We maintain a fixed total batch size of 32 while varying the sample partitions $b_{1}:b_{2}$ within MAST. The results, detailed in Table 9, reveal that employing two buffers with a partition ratio of $6:2$ yields the best performance. Additionally, we observed a significant degradation in MAST\u2019s performance when using data solely from a single buffer, whether it be the online or offline buffer. This underscores the vital role of dual buffers in sparse MARL. ", "page_idx": 28}, {"type": "table", "img_path": "Gug7wc0BSs/tmp/5bc6fe36dc79e9b7cb2200dde8535233363b558e3736f80f9eba226982873f14.jpg", "table_caption": ["Table 9: Ablation study on dual buffers. "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "B.8 Sensitivity Analysis for Hyperparameters ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Table 10 shows the performance with different mask update intervals (denoted as $\\Delta_{m}$ ) in different environments, which reveals several key observations: ", "page_idx": 28}, {"type": "text", "text": "\u2022 Findings indicate that a small $\\Delta_{m}$ negatively impacts performance, as frequent mask adjustments may prematurely drop critical connections before their weights are adequately updated by the optimizer. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Overall, A moderate $\\Delta_{m}=200$ episodes performs well in different algorithms. ", "page_idx": 28}, {"type": "table", "img_path": "Gug7wc0BSs/tmp/5dc199cc2a3077f8172dede86d6e9e913bbe1c30f38fc7c48dbcb8d03056dc30.jpg", "table_caption": ["Table 10: Sensitivity analysis on mask update interval. "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "B.9 Experiments on QMIX in Multi-Agent MuJoCo ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We compare MAST-QMIX with other baselines on the MAMuJoCo benchmark, and the results are presented in Figure 17. From the figure, we observe that MAST consistently achieves dense performance and outperforms other methods in three environments, except for ManyAgent Swimmer, where the static sparse network performs similarly to MAST. This similarity may be attributed to the simplicity of the ManyAgent Swimmer environment, which allows a random static sparse network to achieve good performance. This comparison demonstrates the applicability of MAST across different environments. ", "page_idx": 29}, {"type": "image", "img_path": "Gug7wc0BSs/tmp/a2781ac812b4be3a43a05ab8d3c1af3fa104d8a16b54df2eb06216638ab5c88d.jpg", "img_caption": ["Figure 17: Mean episode return on different MAMuJoCo tasks with MAST-QMIX. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "B.10 Experiments on FACMAC in SMAC ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In addition to pure value-based deep MARL algorithms, we also evaluate MAST with a hybrid valuebased and policy-based algorithm, FACMAC [Peng et al., 2021], in SMAC. The results are presented in Figure 18. From the figure, we observe that MAST consistently achieves dense performance and outperforms other methods in three environments, demonstrating its applicability across different algorithms. ", "page_idx": 29}, {"type": "image", "img_path": "Gug7wc0BSs/tmp/f66d5ed16a45be4165de869f9ca4a5ad362674e67c7052c04af85439d5c9315b.jpg", "img_caption": ["Figure 18: Mean episode win rates on different SMAC tasks with MAST-FACMAC. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "B.11 Visualization of Sparse Masks ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We present a series of visualizations capturing the evolution of masks within network layers during the MAST-QMIX training in the 3s5z scenario. These figures, specifically Figure 20 (a detailed view of Figure 11, Figure 21, and Figure 22, offer intriguing insights. Additionally, we provide connection counts in Figure 23, 24 and 25, for input and output dimensions in each sparse mask, highlighting pruned dimensions. To facilitate a clearer perspective on connection distributions, we sort dimensions based on the descending order of nonzero connections, focusing on the distribution rather than specific dimension ordering. The connection counts associated with Figure 11 in the main paper s given in Figure 19. ", "page_idx": 29}, {"type": "image", "img_path": "Gug7wc0BSs/tmp/844e527020485763ed0a59a844cb444cc9f45a652baca7eb606a79f8a3361d53.jpg", "img_caption": ["Figure 19: Number of nonzero connections for input and output dimensions in descending order of the sparse layer visualized in Figure 11. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "During the initial phases of training, a noticeable shift in the mask configuration becomes evident, signifying a dynamic restructuring process. As the training progresses, connections within the hidden layers gradually coalesce into a subset of neurons. This intriguing phenomenon underscores the distinct roles assumed by individual neurons in the representation process, thereby accentuating the significant redundancy prevalent in dense models. ", "page_idx": 30}, {"type": "text", "text": "\u2022 Figure 20 provides insights into the input layer, revealing that certain output dimensions can be omitted while preserving the necessity of each input dimension. \u2022 Figure 21 showcases analogous observations, reinforcing the idea that only a subset of output neurons is indispensable, even within the hidden layer of the GRU. \u2022 Figure 22 presents distinct findings, shedding light on the potential redundancy of certain input dimensions in learning the hyperparameters within the hypernetwork. ", "page_idx": 30}, {"type": "image", "img_path": "Gug7wc0BSs/tmp/b1f9cad2747176c350def1c01257bbceb16d7912c4d2919965a4de1b548a71fc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Figure 20: The learned mask of the input layer weight of $Q_{1}$ . Light pixels in row $i$ and column $j$ indicate the existence of the connection for input dimension $j$ and output dimension $i$ , while the dark pixel represents the empty connection. ", "page_idx": 31}, {"type": "image", "img_path": "Gug7wc0BSs/tmp/a6c7d51d47b94a4825c1b7084684d97bd73b239830e0bf149c14a74c0fc88be0.jpg", "img_caption": ["Figure 21: The learned mask of the GRU layer weight of $Q_{1}$ . Light pixels in row $i$ and column $j$ indicate the existence of the connection for input dimension $j$ and output dimension $i$ , while the dark pixel represents the empty connection. "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "Gug7wc0BSs/tmp/53ec7a40aa2f0064db8b5ac13225e4a36f3e7653fdb6b86505d0768419e004c6.jpg", "img_caption": ["Figure 22: The learned mask of the first layer weight of Hypernetwork. Light pixels in row $i$ and column $j$ indicate the existence of the connection for input dimension $j$ and output dimension $i$ , while the dark pixel represents the empty connection. "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "Gug7wc0BSs/tmp/abe2a03adfaa01d8d7b07ca7434e0b2e88eb270e74cbc221d88b8c92e4f5eb8f.jpg", "img_caption": ["Figure 23: Number of nonzero connections for input and output dimensions in descending order of the sparse layer visualized in Figure 20. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "Gug7wc0BSs/tmp/88ca788427f3724ffd932a7d0dc8d3558aa905d73e8c5147726c46050f05ce40.jpg", "img_caption": ["Figure 24: Number of nonzero connections for input and output dimensions in descending order of the sparse layer visualized in Figure 21. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "Gug7wc0BSs/tmp/ecdc6e5b00d626b02292f9f5fcda21068b771ec4366a8bb1c3f148040f2b7cef.jpg", "img_caption": ["Figure 25: Number of nonzero connections for input and output dimensions in descending order of the sparse layer visualized in Figure 22. "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: Our abstract and introduction clearly state the claims that training in multiagent reinforcement learning involves heavy computational load, and our proposed MAST framework can save up to 20 times FLOPs without performance degradation less than $3\\%$ for value-based deep MARL algorithms. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 34}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: Our paper explores some shortcomings of the MAST framework, such as the multiple hyperparameters. Please refer to Appendix A.6. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 34}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: All our theoretical results are accompanied by clear, complete and correct proofs. All the assumptions we present are clearly stated or referenced. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 35}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: We ensure that the configurations and details of each experimental result are clearly explained, and all experimental results are reproducible. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 35}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The code will be open-sourced upon publication of the paper. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 36}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We ensure that the parameter configurations and details of each experimental result are clearly explained, such as the network architecture and training hyperparameters. Please refer to B.3. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 36}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: For each experiment, we presented the results of eight random seeds and calculated their mean and standard deviation. Pleasr refer to B.6 ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 36}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 37}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We provided the necessary computational resources, hardware setup, and duration required to reproduce the experiments. Please refer to B.1. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 37}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We ensure compliance with all aspects of the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 37}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: There is no societal impact of the work performed ", "page_idx": 37}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 37}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 38}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 38}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We have properly credited assets used in our paper. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We have provided a README file alongside our code. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 39}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 39}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 40}]