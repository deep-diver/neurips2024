[{"figure_path": "nfC1OA6NeE/figures/figures_3_1.jpg", "caption": "Figure 1: Comparison of SignSGD and its SDE in terms of f(x): Our SDE successfully tracks the dynamics of SignSGD on several architectures: DNN on the Breast Cancer dataset (Left); CNN on MNIST (Center-Left); Transformer on MNIST (Center-Right); ResNet on CIFAR-10 (Right).", "description": "This figure compares the loss function f(x) of SignSGD and its corresponding SDE for four different neural network architectures: DNN, CNN, Transformer, and ResNet.  Each subplot shows the loss over iterations for both SignSGD and its SDE approximation. The close agreement between the SignSGD curves and their SDE counterparts demonstrates the accuracy of the proposed SDE in modeling the optimizer's behavior across various network types and datasets.", "section": "3 Formal statements & insights: the SDES"}, {"figure_path": "nfC1OA6NeE/figures/figures_4_1.jpg", "caption": "Figure 2: Phases of SignSGD: The ODE of Phase 1 and the SDE of Phase 3 overlap with the \"Full\" SDE as per Lemma 3.4 (Left); Phases of the Loss: The bounds derived in Lemma 3.5 for the loss during Phase 1 and Phase 3 correctly track the loss evolution (Center-Left); The dynamics of the moments of Xt predicted in Lemma 3.7 track the empirical ones (Center-Right); If the schedulers satisfy the condition in Lemma 3.9, the loss decays to 0 as prescribed. Otherwise, the loss does not converge to 0 (Right).", "description": "This figure displays four subfigures illustrating various aspects of SignSGD dynamics. The first demonstrates the agreement between the ODE of Phase 1, the SDE of Phase 3, and the full SDE. The second subfigure shows the loss in phases 1 and 3, aligning theoretical bounds with empirical data.  The third subfigure compares empirical and theoretical predictions of the moments of Xt. The final subfigure shows the convergence behavior of the loss under specific scheduler conditions.", "section": "3.1 SignSGD SDE"}, {"figure_path": "nfC1OA6NeE/figures/figures_6_1.jpg", "caption": "Figure 3: The first two images compare the SDEs of AdamW and RMSpropW with the respective optimizers in terms of trajectories and f(x) for a convex quadratic function while the other two figures provide a comparison for an embedded saddle. In all cases, we observe good agreements.", "description": "This figure compares the performance of AdamW and RMSpropW optimizers with their corresponding SDEs (stochastic differential equations).  Two simple landscapes are used for comparison: a convex quadratic function and an embedded saddle point. The plots show the trajectories of the optimizers and the loss function f(x) over iterations.  The close agreement between the optimizer's behavior and the SDE's simulation supports the validity of the SDE models in describing the optimizers' dynamics across different landscape types.", "section": "3 Formal statements & insights: the SDES"}, {"figure_path": "nfC1OA6NeE/figures/figures_7_1.jpg", "caption": "Figure 1: Comparison of SignSGD and its SDE in terms of f(x): Our SDE successfully tracks the dynamics of SignSGD on several architectures: DNN on the Breast Cancer dataset (Left); CNN on MNIST (Center-Left); Transformer on MNIST (Center-Right); ResNet on CIFAR-10 (Right).", "description": "This figure compares the loss function values of SignSGD optimizer and its corresponding SDE (Stochastic Differential Equation) model across four different neural network architectures: DNN, CNN, Transformer, and ResNet.  The results demonstrate that the SDE model effectively captures the training dynamics of SignSGD, particularly in its ability to track the loss function over iterations. Each subfigure shows the loss curves for both SignSGD and its SDE for one specific network architecture. The consistency observed across different network architectures supports the claim that the SDE provides a quantitatively accurate description of the SignSGD optimizer.", "section": "3 Formal statements & insights: the SDES"}, {"figure_path": "nfC1OA6NeE/figures/figures_7_2.jpg", "caption": "Figure 5: The loss predicted in Lemma 3.13 matches the experimental results on a convex quadratic function. AdamW is run with regularization parameter y = 1. AdamW R (AdamW Rescaled) is run as we apply the scaling rule with \u03ba = 2. AdamW NR (AdamW Not Rescaled) is run as we apply the scaling rule with \u03ba = 2 on all hyperparameters but y, which is left unchanged: Our scaling rule holds, and failing to rescale y leads the optimizer not to preserve the asymptotic loss level. The same happens for y = 4 (Left); The same for RMSpropW (Center-Left); For AdamW, \u03b2\u2081 and \u03b22 influence which basin will attract the dynamics and how fast this will converge, but not the asymptotic loss level inside the basin (Center-Right). For both AdamW and RMSpropW, the variance at convergence predicted in Lemma 3.14 matches the experimental results (Right).", "description": "This figure validates the theoretical findings (Lemma 3.13 and Lemma 3.14) about the asymptotic loss and variance at convergence for AdamW and RMSpropW on a convex quadratic function.  It demonstrates the effect of applying a scaling rule (\u03ba=2) to the hyperparameters, showing that correctly rescaling weight decay (\u03b3) is crucial for maintaining the asymptotic loss level. The figure also shows how \u03b2\u2081 and \u03b2\u2082 affect basin attraction and convergence speed but not the asymptotic loss value itself.  The rightmost plots compare empirical and theoretical variance results.", "section": "Contributions"}, {"figure_path": "nfC1OA6NeE/figures/figures_8_1.jpg", "caption": "Figure 6: For SGD (Left), SignSGD (Center-Left), Adam (Center-Right), and AdamW: For each optimizer, we plot the loss value on a convex quadratic and compare its asymptotic value with the limits predicted by our theory. As we take \u2211 = \u03c3\u00b2Id, we confirm that the loss of SGD scales quadratically in \u03c3 (Lemma 3.6), and linearly for SignSGD (Lemma 3.5) and Adam (Lemma 3.13 with \u03b3 = 0). For AdamW, the maximum asymptotic loss value is bounded in \u03c3 (Lemma 3.13 with \u03b3 > 0). In accordance with the experiments, our theory predicts that adaptive methods are more resilient to noise.", "description": "The figure compares the performance of four different optimizers (SGD, SignSGD, Adam, and AdamW) on a convex quadratic function under different noise levels. It shows that the loss for SGD scales quadratically with noise, while it scales linearly for SignSGD and Adam. For AdamW, the loss is bounded, demonstrating better resilience to noise.", "section": "5 Experiments: SDE validation"}, {"figure_path": "nfC1OA6NeE/figures/figures_25_1.jpg", "caption": "Figure 7: The first two subfigures on the left compare our SDE, that from Malladi et al. (2022), and RMSprop in terms of trajectories and f(x), respectively, for a convex quadratic function. The others subfigures do the same for an embedded saddle and one clearly observes that our derived SDE better matches RMSprop.", "description": "The figure compares the performance of three different methods: RMSprop (the original optimizer), the SDE proposed in the paper, and the SDE proposed by Malladi et al. in 2022.  It shows trajectories and loss values for both a simple convex quadratic function and a more complex function containing an embedded saddle point. The results demonstrate that the SDE proposed by the authors is a more accurate representation of RMSprop's dynamics than the model presented by Malladi et al. across both types of functions.", "section": "3.2 RMSprop SDE"}, {"figure_path": "nfC1OA6NeE/figures/figures_26_1.jpg", "caption": "Figure 8: We compare our SDE, that from Malladi et al. (2022), and RMSprop in terms of f(x): The first is an MLP on the Breast Cancer dataset, the second a CNN on MNIST, the third a Transformer on MNIST, and the last a ResNet on CIFAR-10: Ours match the algorithms better.", "description": "This figure compares the performance of three different methods for approximating the loss function of RMSprop on four different neural network architectures: a dense neural network (DNN), a convolutional neural network (CNN), a transformer network, and a residual network (ResNet).  The three methods are: RMSprop itself, the SDE derived in this paper, and the SDE derived by Malladi et al. (2022). The figure shows that the SDE derived in this paper provides a closer approximation to the actual RMSprop loss function across all four architectures.", "section": "Experiments: SDE validation"}, {"figure_path": "nfC1OA6NeE/figures/figures_28_1.jpg", "caption": "Figure 1: Comparison of SignSGD and its SDE in terms of f(x): Our SDE successfully tracks the dynamics of SignSGD on several architectures: DNN on the Breast Cancer dataset (Left); CNN on MNIST (Center-Left); Transformer on MNIST (Center-Right); ResNet on CIFAR-10 (Right).", "description": "This figure compares the loss function f(x) of SignSGD and its corresponding SDE across four different neural network architectures.  The SDE accurately models the behaviour of the optimizer in each case, indicating a strong match between the theoretical SDE model and the empirical optimizer performance. The four architectures are a Deep Neural Network (DNN), Convolutional Neural Network (CNN), Transformer Network and a Residual Network (ResNet), trained on distinct datasets.", "section": "3 Formal statements & insights: the SDES"}, {"figure_path": "nfC1OA6NeE/figures/figures_30_1.jpg", "caption": "Figure 1: Comparison of SignSGD and its SDE in terms of f(x): Our SDE successfully tracks the dynamics of SignSGD on several architectures: DNN on the Breast Cancer dataset (Left); CNN on MNIST (Center-Left); Transformer on MNIST (Center-Right); ResNet on CIFAR-10 (Right).", "description": "This figure demonstrates the accuracy of the novel stochastic differential equation (SDE) model for SignSGD developed in the paper.  The SDE's predictions for the loss function, f(x), are compared to the actual loss obtained using the SignSGD optimizer across four different neural network architectures and datasets.  The close agreement between the SDE and the optimizer's performance suggests that the SDE is a valid and accurate representation of SignSGD's dynamics.", "section": "3 Formal statements & insights: the SDES"}, {"figure_path": "nfC1OA6NeE/figures/figures_31_1.jpg", "caption": "Figure 1: Comparison of SignSGD and its SDE in terms of f(x): Our SDE successfully tracks the dynamics of SignSGD on several architectures: DNN on the Breast Cancer dataset (Left); CNN on MNIST (Center-Left); Transformer on MNIST (Center-Right); ResNet on CIFAR-10 (Right).", "description": "This figure compares the loss function f(x) of SignSGD and its corresponding SDE across four different neural network architectures: DNN, CNN, Transformer, and ResNet.  Each subfigure shows the loss curves for both SignSGD and the derived SDE, demonstrating that the SDE accurately represents the behavior of SignSGD across various model types and datasets. The results highlight the efficacy of the proposed SDE in modeling the dynamics of SignSGD.", "section": "3 Formal statements & insights: the SDEs"}]