[{"type": "text", "text": "SDEs for Adaptive Methods: The Role of Noise ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Despite the vast empirical evidence supporting the efficacy of adaptive optimization   \n2 methods in deep learning, their theoretical understanding is far from complete.   \n3 In this work, we introduce novel SDEs for commonly used adaptive optimizers:   \n4 SignSGD, RMSprop(W), and Adam(W). Our SDEs offer a quantitatively accurate   \n5 description of these optimizers and help bring to light an intricate relationship   \n6 between adaptivity, gradient noise, and curvature. Our novel analysis of SignSGD   \n7 highlights a noteworthy and precise contrast to SGD in terms of convergence speed,   \n8 stationary distribution, and robustness to heavy-tail noise. We extend this analysis   \n9 to AdamW and RMSpropW, for which we observe that the role of noise is much   \n10 more complex. Crucially, we support our theoretical analysis with experimental   \n1 evidence by verifying our insights: this includes numerically integrating our SDEs   \n12 using Euler-Maruyama discretization on various neural network architectures such   \n13 as MLPs, CNNs, ResNets, and Transformers. Our SDEs accurately track the   \n14 behavior of the respective optimizers, especially when compared to previous SDEs   \n15 derived for Adam and RMSprop. We believe our approach can provide valuable   \n16 insights into best training practices and novel scaling rules. ", "page_idx": 0}, {"type": "text", "text": "17 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "18 Adaptive optimizers lay the foundation for effectively training of modern deep learning models.   \n19 These methods are typically employed to optimize an objective function expressed as a sum across $N$   \n20 individual data points: $\\begin{array}{r}{\\operatorname*{min}_{x\\in\\mathbb{R}^{d}}[f(x):=\\bar{\\frac{1}{N}}\\sum_{i=1}^{N}f_{i}(x)]}\\end{array}$ , where $f,f_{i}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R},\\;\\;i=1,\\dots,N.$   \n21 Due to the practical difficulties of selecting the learning rate of stochastic gradient descent, adaptive   \n22 methods have grown in popularity over the past decade. At a high level, these optimizers adjust the   \n23 learning rate for each parameter based on the historical gradients. Popular optimizers that belong to   \n24 this family are RMSprop (Tieleman and Hinton, 2012), Adam (Kingma and Ba, 2015), SignSGD   \n25 (Bernstein et al., 2018), AdamW (Loshchilov and Hutter, 2019), and many other variants. SignSGD is   \n26 often used for compressing gradients in distributed machine learning (Karimireddy et al., 2019a), but   \n27 it also has gained popularity due to its connection to RMSprop and Adam (Balles and Hennig, 2018).   \n28 The latter algorithms have emerged as the standard methods for training modern large language   \n29 models, partly because of enhancements in signal propagation (Noci et al., 2022).   \n30 Although adaptive methods are widely favored in practice, their theoretical foundations remain enig  \n31 matic. Recent research has illuminated some of their advantages: Zhang et al. (2020b) demonstrated   \n32 how gradient clipping addresses heavy-tailed gradient noise, Pan and Li (2022) related the success of   \n33 Adam over SGD to sharpness, and Yang et al. (2024) showed that adaptive methods handle large gra  \n34 dients better than SGD. At the same time, many optimization studies focus on worst-case convergence   \n35 rates: These rates (e.g., D\u00e9fossez et al. (2022)) are valuable, yet they provide an incomplete depiction   \n36 of algorithm behavior, showing no quantifiable advantage over standard SGD. One particular aspect   \n37 still lacking clarity is the precise role of noise in the algorithm trajectory.   \n38 Our investigation aims to study how gradient noise influences the dynamics of adaptive optimizers   \n39 and how it impacts their asymptotic behaviors in terms of expected loss and stationary distribution. In   \n40 particular, we want to understand which algorithms are more resilient to high (possibly heavy-tailed)   \n41 gradient noise levels. To do this, we rely on stochastic differential equations (SDEs) which have   \n42 become popular in the literature to study the behavior of optimization algorithms (Li et al., 2017;   \n43 Jastrzebski et al., 2018). These continuous-time models unlock powerful tools from It\u00f4 calculus,   \n44 enabling us to establish convergence bounds, determine stationary distributions, unveil implicit   \n45 regularization, and elucidate the intricate interplay between landscape and noise. Notably, SDEs   \n46 facilitate direct comparisons between optimizers by explicitly illustrating how each hyperparameter   \n47 and certain landscape features influence their dynamics (Compagnoni et al., 2024).   \n48 We begin by analyzing SignSGD, showing how the signal-to-noise ratio affects its dynamics and   \n49 elucidating the impact of noise at convergence. After analyzing the case where the gradient noise   \n50 exhibits infinite variance, we extend our analysis to Adam and RMSprop with decoupled weight   \n51 decay (Loshchilov and Hutter, 2019) \u2013 i.e. AdamW and RMSpropW: for both, we refine batch size   \n52 scaling rules and compare the role of noise to SignSGD. Our analysis provides some theoretical   \n53 grounding for the resilience of these adaptive methods to high noise levels. Importantly, we highlight   \n54 that Adam and RMSprop are byproducts of our analysis and that our novel SDEs are derived under   \n55 much weaker and more realistic assumptions than those in the literature (Malladi et al., 2022). ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "56 Contributions We identify our key contributions as follows: ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "57 1. We derive the first SDE for SignSGD under very general assumptions: We show that SignSGD   \n58 exhibits three different phases of the dynamics and characterize the loss behavior in these phases,   \n59 including the stationary distribution and asymptotic loss value.   \n60 2. We demonstrate that for SignSGD, noise inversely affects the convergence rate of both the loss and   \n61 the iterates. Differently, it has a linear impact on the asymptotic expected loss and the asymptotic   \n62 variance of the iterates. This is in contrast to SGD, where noise does not influence the convergence   \n63 speed, but it has a quadratic effect on the loss and variance of the iterates. Finally, we show   \n64 that, even if the noise has infinite variance, SignSGD is very resilient: its performance is only   \n65 marginally impacted. In the same conditions, SGD would diverge.   \n66 3. We derive new, improved, SDEs for AdamW and RMSpropW and use them to (1) show a novel   \n67 batch size scaling rule and (2) inspect the stationary distribution and stationary loss value in   \n68 convex quadratics. In particular, we dive into the properties of weight decay: while for vanilla   \n69 Adam and RMSprop the effect of noise at convergence mimics SignSGD, something different   \n70 happens in AdamW and RMSpropW \u2014 Due to an intricate interaction between noise, curvature,   \n71 and regularization, weight decay plays a crucial stabilization role at high noise levels near the   \n72 minimizer.   \n73 4. We empirically verify every theoretical insight we derive. Importantly, we integrate our SDEs   \n74 with Euler-Maruyama to confirm that our SDEs faithfully track their respective optimizers. We do   \n75 so on an MLP, a CNN, a ResNet, and a Transformer. For RMSprop and Adam, our SDEs exhibit   \n76 superior modeling power than the SDEs already existing in the literature. ", "page_idx": 1}, {"type": "text", "text": "77 2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "78 SDE approximations and applications. (Li et al., 2017) introduced a formal theoretical framework   \n79 aimed at deriving SDEs that effectively model the inherent stochastic nature of optimizers. Ever since,   \n80 SDEs have found several applications in the field of machine learning, for instance in connection   \n81 with stochastic optimal control to select the stepsize (Li et al., 2017, 2019) and batch size (Zhao   \n82 et al., 2022), the derivation of convergence bounds and stationary distributions (Compagnoni et al.,   \n83 2023, 2024), implicit regularization (Smith et al., 2021), and scaling rules (Jastrzebski et al., 2018).   \n84 Previous work by Malladi et al. (2022) has already made strides in deriving SDE models for RMSprop   \n85 and Adam, albeit under certain restrictive assumptions. They establish a scaling rule which they   \n86 assert remains valid throughout the entirety of the dynamics. Unfortunately, their derivation is based   \n87 on the approach of Jastrzebski et al. (2018) which is problematic in the general case (See Appendix   \n88 E for a detailed discussion). Indeed, we demonstrate that the SDEs derived in Malladi et al. (2022)   \n89 are only accurate around minima, indicating that their scaling rule is not globally valid. (Zhou et al.,   \n90 2020a) also claimed to have derived a L\u00e9vy SDE for Adam. Unfortunately, the quality of their   \n91 SDE approximation does not come with theoretical guarantees. Additionally, their SDE has random   \n92 coefficients: an approach which is theoretically sound in very limited settings (Kohatsu-Higa et al.,   \n93 1997; Bishop and Del Moral, 2019). Zhou et al. (2024) informally presented an SDE for (only) the   \n94 parameters of AdamW: this is achieved under strong assumptions and various approximations, some   \n95 of which are hard to motivate formally.   \n96 Influence of noise on convergence. Several empirical papers demonstrate that adaptive algorithms   \n97 adjust better to the noise during training. Specifically, (Zhang et al., 2020b) noticed a consistent gap   \n98 in the performance of SGD and Adam on language models and connected that phenomenon with   \n99 heavy-tailed noise distributions. (Pascanu et al., 2013) suggests using gradient clipping to deal with   \n100 heavy tail noise, and consequently several follow-up works analyzed clipped SGD under heavy-tailed   \n101 noise (Zhang et al., 2020a; Mai and Johansson, 2021; Puchkin et al., 2024). Kunstner et al. (2024)   \n102 present thorough numerical experiments illustrating that a significant contributor to heavy-tailed noise   \n103 during language model training is class imbalance, where certain words occur much more frequently   \n104 than others. They demonstrate that adaptive optimization methods such as Adam and SignSGD can   \n105 better adapt to such class imbalances. However, the theoretical understanding of the influence of   \n106 noise in the context of adaptive algorithms is much more limited. The first convergence results on   \n107 Adam and RMSprop were derived under bounded stochastic gradients assumption (De et al., 2018;   \n108 Zaheer et al., 2018; Chen et al., 2019; D\u00e9fossez et al., 2022). Later, this noise model was relaxed   \n109 to weak growth condition (Zhang et al., 2022; Wang et al., 2022) and its coordinate-wise version   \n110 (Hong and Lin, 2023; Wang et al., 2024) and sub-gaussian noise (Li et al., 2023a). SignSGD and   \n111 its momentum version Signum were originally studied as a method for compressed communication   \n112 (Bernstein et al., 2018) under bounded variance assumption, but with a requirement of large batches.   \n113 Several works provided counterexamples where SignSGD fails to converge if stochastic and full   \n114 gradients are not correlated enough (Karimireddy et al., 2019b; Safaryan and Richtarik, 2021). In   \n115 the case of AdamW, (Zhou et al., 2022, 2024) provide convergence guarantees under restrictive   \n116 assumptions such as bounded gradient and bounded noise. All aforementioned results only show   \n117 that SignSGD, Adam, and RMSprop at least do not perform worse than vanilla SGD. None of them   \n118 studied how noise affects the dynamics of the algorithm: In this work, we attempt to close this gap. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "119 3 Formal statements & insights: the SDEs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "120 This section provides the general formulations of the SDEs of SignSGD (Theorem 3.2) and AdamW   \n121 (Theorem 3.12). Due to the technical nature of the analysis, we refer the reader to the appendix for   \n122 the complete formal statements and proofs.   \n123 Assumptions and notation. In this section, we assume that $\\nabla f_{\\gamma}(x)=\\nabla f(x)\\!+\\!Z(x),\\mathbb{E}[Z(x)]=0$   \n124 and, unless we study the cases where the gradient variance is unbounded, we write $C o v(Z(x))=$   \n125 $\\Sigma(x)$ where we omit the batch size unless relevant. To derive the stationary distribution around an   \n126 optimum, we will approximate the loss function with a quadratic convex function $f({\\underline{{x}}})={\\textstyle{\\frac{1}{2}}}x^{\\top}H x$   \n127 as commonly done in the literature (Ge et al., 2015; Levy, 2016; Jin et al., 2017; Poggio et al.,   \n128 2017; Mandt et al., 2017; Compagnoni et al., 2023). Regarding the notation, $\\eta\\,>\\,0$ is the step   \n129 size, the mini-batches $\\{\\gamma_{k}\\}$ are of size $B\\geq1$ and modeled as i.i.d. random variables uniformly   \n130 distributed on $\\{1,\\ldots,\\bar{N}\\}$ . The $\\beta$ parameters refer to momentum parameters, $\\gamma>0$ is the (decoupled)   \n131 $L^{2}$ -regularization parameter, and $\\epsilon>0$ is a small scalar used for numerical stability.   \n132 The following definition formalizes the idea that an SDE can be a \u201cgood model\u201d to describe an   \n133 optimizer. It is drawn from the field of numerical analysis of SDEs (see Mil\u2019shtein (1986)) and it   \n134 quantifies the disparity between the discrete and the continuous processes.   \n135 Definition 3.1 (Weak Approximation). A continuous-time stochastic process $\\{X_{t}\\}_{t\\in[0,T]}$ is an order   \n136 $\\alpha$ weak approximation (or $\\alpha$ -order SDE) of a discrete stochastic process k}\u230akT=/0\u03b7\u230bif for every   \n137 polynomial growth function $g$ , there exists a positive constant , independent of the stepsize $\\eta$ , such   \n138 that $\\begin{array}{r}{\\operatorname*{max}_{k=0,\\ldots,\\lfloor T/\\eta\\rfloor}|\\mathbb{E}g\\left(x_{k}\\right)-\\mathbb{E}g\\left(X_{k\\eta}\\right)|\\leq C\\eta^{\\alpha}.}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "139 3.1 SignSGD SDE ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "140 In this section, we derive an SDE model for SignSGD, which we believe to be a novel addition to   \n141 the existing literature. This derivation will reveal the unique manner in which noise influences the   \n142 dynamics of SignSGD. First, we recall the update equation of SignSGD: ", "page_idx": 2}, {"type": "equation", "text": "$$\nx_{k+1}=x_{k}-\\eta\\mathrm{sign}\\left(\\nabla f_{\\gamma_{k}}(x_{k})\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "image", "img_path": "nfC1OA6NeE/tmp/c585197722c3d230d2d50543dd158431103e5e50832da2c0b6098c465b48d81c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 1: Comparison of SignSGD and its SDE in terms of $f(x)$ : Our SDE successfully tracks the dynamics of SignSGD on several architectures: DNN on the Breast Cancer dataset (Left); CNN on MNIST (Center-Left); Transformer on MNIST (Center-Right); ResNet on CIFAR-10 (Right). ", "page_idx": 3}, {"type": "text", "text": "143 The following theorem derives a formal continuous-time model for SignSGD. ", "page_idx": 3}, {"type": "text", "text": "144 Theorem 3.2 (Informal Statement of Theorem C.5). Under sufficient regularity conditions, the   \n145 solution of the following SDE is an order 1 weak approximation of the discrete update of SignSGD: ", "page_idx": 3}, {"type": "equation", "text": "$$\nd X_{t}=-(1-2\\mathbb{P}(\\nabla f_{\\gamma}(X_{t})<0))d t+\\sqrt{\\eta}\\sqrt{\\bar{\\Sigma}(X_{t})}d W_{t},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "146 where $\\bar{\\Sigma}(x)$ is the noise covariance $\\bar{\\Sigma}(x)\\,=\\,\\mathbb{E}[\\xi_{\\gamma}(x)\\xi_{\\gamma}(x)^{\\top}]$ and $\\xi_{\\gamma}(x):=s i g n(\\nabla f_{\\gamma}(x))-1+$   \n147 $2\\mathbb{P}(\\nabla f_{\\gamma}(x)<0)$ the noise in the sample sign $(\\nabla f_{\\gamma}(x))$ .   \n148 For didactic reasons, we next present a corollary of Theorem 3.2 that provides a more interpretable   \n149 SDE. Figure 1 shows the empirical validation of this model for various neural network classes: All   \n150 details are presented in Appendix F.   \n151 Corollary 3.3 (Informal Statement of Corollary C.7). Under the assumptions of Theorem 3.2, and   \n152 that the stochastic gradient is $\\nabla f_{\\gamma}(x)=\\nabla f(x)\\dot{+}Z$ such that $Z\\sim{\\mathcal{N}}(0,\\Sigma)\\,$ , $\\dot{\\Sigma}=\\mathrm{diag}(\\sigma_{1}^{2},\\cdot\\cdot\\cdot\\cdot,\\sigma_{d}^{2}),$ ,   \n153 the following SDE provides a 1 weak approximation of the discrete update of SignSGD ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nd X_{t}=-E r f\\left(\\frac{\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})}{\\sqrt{2}}\\right)d t+\\sqrt{\\eta}\\sqrt{I_{d}-\\mathrm{diag}\\left(E r f\\left(\\frac{\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})}{\\sqrt{2}}\\right)\\right)^{2}}d W_{t},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "154 where the error function $E r f(x)$ and the square are applied component-wise. ", "page_idx": 3}, {"type": "text", "text": "155 While Eq. (3) may appear intricate at first glance, it becomes apparent upon closer inspection that   \n156 the properties of the $\\mathrm{Erf}(\\cdot)$ function enable a detailed exploration of the dynamics of SignSGD. In   \n157 particular, we demonstrate that the dynamics of SignSGD can be categorized into three distinct   \n158 phases. The left of Figure 2 empirically verifies this result on a convex quadratic function. ", "page_idx": 3}, {"type": "text", "text": "159 Lemma 3.4. Under the assumptions of Corollary 3.3 and signal-to-noise ratio Yt := \u03a3\u22122 \u221a\u22072f(Xt), ", "page_idx": 3}, {"type": "text", "text": "160 ", "page_idx": 3}, {"type": "text", "text": "1. Phase 1: If $\\left|Y_{t}\\right|>\\frac{3}{2}$ , the SDE coincides with the ODE of SignGD: ", "page_idx": 3}, {"type": "equation", "text": "$$\nd X_{t}=-s i g n(\\nabla f(X_{t}))d t;\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "161 ", "page_idx": 3}, {"type": "text", "text": "2. Phase 2: If $\\mathrm{~\\dot{~}{1}~}<|Y_{t}|<\\frac{3}{2}$ :1 ", "page_idx": 3}, {"type": "text", "text": "162 ", "page_idx": 3}, {"type": "text", "text": "163 ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(a)~m Y_{t}+\\mathbf{q}^{-}\\leq\\frac{d\\mathbb{E}\\left[X_{t}\\right]}{d t}\\leq m Y_{t}+\\mathbf{q}^{+};}\\\\ &{(b)~F o r\\,a n y\\,a>0,\\,\\mathbb{P}\\left[\\|X_{t}-\\mathbb{E}\\left[X_{t}\\right]\\|_{2}^{2}>a\\right]\\leq\\frac{\\eta}{a}\\left(d-\\|m Y_{t}+\\mathbf{q}^{-}\\|_{2}^{2}\\right);}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "164 ", "page_idx": 3}, {"type": "text", "text": "3. Phase 3: I $r|Y_{t}|<1$ , the SDE is ", "page_idx": 3}, {"type": "equation", "text": "$$\nd X_{t}=-\\sqrt{\\frac{2}{\\pi}}\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})d t+\\sqrt{\\eta}\\sqrt{I_{d}-\\frac{2}{\\pi}}\\operatorname{diag}\\Big(\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})\\Big)^{2}d W_{t}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "1Let $m$ and $q_{1}$ are the slope and intercept of the line secant to the graph of $\\operatorname{Erf}(x)$ between the points $(1,\\mathrm{Erf}(1))$ and $\\textstyle\\left({\\frac{3}{2}},\\operatorname{Erf}\\left({\\frac{3}{2}}\\right)\\right)$ , while $q_{2}$ is the intercept of the line tangent to the graph of $\\operatorname{Erf}(x)$ and slope $m$ , (q+)i := q2 if \u2202if(x) > 0 \u2212q1 if \u2202if(x) < 0 , (q )i : = \u2212q2 if \u2202if(x) < 0 , and q\u02c6 := max(q1, q2). q1 if \u2202if(x) > 0 ", "page_idx": 3}, {"type": "image", "img_path": "nfC1OA6NeE/tmp/4e7bf23244353f78c347d4803a4e9c289e12c769ee2b98218a3ba743cec53038.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 2: Phases of SignSGD: The ODE of Phase 1 and the SDE of Phase 3 overlap with the \u201cFull\u201d SDE as per Lemma 3.4 (Left); Phases of the Loss: The bounds derived in Lemma 3.5 for the loss during Phase 1 and Phase 3 correctly track the loss evolution (Center-Left); The dynamics of the moments of $X_{t}$ predicted in Lemma 3.7 track the empirical ones (Center-Right); If the schedulers satisfy the condition in Lemma 3.9, the loss decays to 0 as prescribed. Otherwise, the loss does not converge to 0 (Right). ", "page_idx": 4}, {"type": "text", "text": "165 Remark: The behavior of SignSGD depends on the size of the signal-to-noise ratio. In particular, the   \n166 SDE itself shows that in Phase 3, the inverse of the scale of the noise $\\sum-{\\frac{1}{2}}$ premultiplies the gradient,   \n167 thus affecting the rate of descent. This is not the case for SGD where $\\Sigma$ only influences the diffusion   \n168 term.2 To better understand the role of the noise, we need to study how it affects the dynamics of the   \n169 loss and compare it with SGD.   \n170 Lemma 3.5. Let $f$ be $\\mu$ -strongly convex, $T r(\\nabla^{2}f(x))\\,\\leq\\,\\mathcal{L}_{\\tau}$ , and $S_{t}:=f(X_{t})-f(X_{*})$ . Then,   \n171 during ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "172 ", "page_idx": 4}, {"type": "text", "text": "173 ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\iota\\Delta:=\\left(\\frac{m}{\\sqrt{2}\\sigma_{m a x}}+\\frac{\\eta\\mu m^{2}}{4\\sigma_{m a x}^{2}}\\right)\\colon\\mathbb{E}[S_{t}]\\leq S_{0}e^{-2\\mu\\Delta t}+\\frac{\\eta}{2}\\frac{\\left(\\mathcal{L}_{\\tau}-\\mu d\\hat{q}^{2}\\right)}{2\\mu\\Delta}\\left(1-e^{-2\\mu\\Delta t}\\right);}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "174 ", "page_idx": 4}, {"type": "equation", "text": "$\\begin{array}{r}{\\Delta:=\\Big(\\sqrt{\\frac{2}{\\pi}}\\frac{1}{\\sigma_{m a x}}+\\frac{\\eta}{\\pi}\\frac{\\mu}{\\sigma_{m a x}^{2}}\\Big)\\colon{\\mathbb{E}}[S_{t}]\\leq S_{0}e^{-2\\mu\\Delta t}+\\frac{\\eta}{2}\\frac{\\mathcal{L}_{\\tau}}{2\\mu\\Delta}\\,\\big(1-e^{-2\\mu\\Delta t}\\big).}\\end{array}$ ", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "175 In Phase 1, the signal-to-noise ratio is large, meaning that SignSGD behaves like SignGD: Consistently   \n176 with the analysis of SignGD in (Ma et al., 2022), this explains the fast initial convergence of the   \n177 optimizer as well as of RMSprop and Adam. In this phase, the loss undergoes a steady decrease   \n178 which ensures the emergence of Phase 2 which in turn triggers that of Phase 3 which is characterized   \n179 by an exponential decay to an asymptotic loss level: As a practical example, we verify the dynamics   \n180 of the expected loss around a minimum in the center-left of Figure 2. ", "page_idx": 4}, {"type": "text", "text": "181 Lemma 3.6. For SGD, the expected loss satisfies: $\\begin{array}{r}{\\mathbb{E}[S_{t}]\\leq S_{0}e^{-2\\mu t}+\\frac{\\eta}{2}\\frac{\\mathcal{L}_{\\tau}\\sigma_{\\operatorname*{max}}^{2}}{2\\mu}\\,\\big(1-e^{-2\\mu t}\\big).}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "182 Remark: The two key observations are that: ", "page_idx": 4}, {"type": "text", "text": "1. Both in Phase 2 and Phase 3, the noise level $\\sigma_{\\mathrm{max}}$ inversely affects the exponential convergence speed, while this trend is not observed with SGD; 2. The asymptotic loss of SignSGD is (almost) linear in $\\sigma_{\\mathrm{max}}$ while that of SGD is quadratic. ", "page_idx": 4}, {"type": "text", "text": "186 Additionally, we characterize the stationary distribution of SignSGD around a minimum: Empirical   \n187 validation is provided in the center-right of Figure 2. ", "page_idx": 4}, {"type": "text", "text": "188 Lemma 3.7. Let $H=\\operatorname{diag}(\\lambda_{1},\\dots,\\lambda_{d})$ and $M_{t}:=e^{-2\\left(\\sqrt{\\frac{2}{\\pi}}\\Sigma^{-\\frac{1}{2}}H+\\frac{\\eta}{\\pi}\\Sigma^{-\\frac{1}{2}}H^{2}\\right)t}.$ . Then, ", "page_idx": 4}, {"type": "equation", "text": "$$\nI.\\,\\,\\,\\mathbb{E}\\left[X_{t}\\right]=e^{-\\sqrt{\\frac{2}{\\pi}}\\Sigma^{-\\frac{1}{2}}H t}X_{0}\\,\\overset{t\\rightarrow\\infty}{\\rightarrow}0;\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\cdot\\;C o v\\left[X_{t}\\right]=\\left(M_{t}-e^{-2\\sqrt{\\frac{2}{\\pi}}\\Sigma^{-\\frac{1}{2}}H t}\\right)X_{0}^{2}+\\frac{\\eta}{2}\\left(\\sqrt{\\frac{2}{\\pi}}I_{d}+\\frac{\\eta}{\\pi}H\\right)^{-1}H^{-1}\\Sigma^{\\frac{1}{2}}\\left(I_{d}-M_{t}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "192 Lemma 3.8. Under the same assumptions as Lemma 3.7, the stationary distribution for SGD is: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[X_{t}\\right]=e^{-H t}X_{0}\\stackrel{t\\rightarrow\\infty}{\\rightarrow}0\\quad a n d\\quad C o v\\left[X_{t}\\right]=\\frac{\\eta}{2}H^{-1}\\Sigma\\left(I_{d}-e^{-2H t}\\right)\\stackrel{t\\rightarrow\\infty}{\\rightarrow}\\frac{\\eta}{2}H^{-1}\\Sigma.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "194 As we observed above, the noise inversely affects the convergence rate of the iterates of SignSGD   \n195 while it does not impact that of SGD. Additionally, while both covariance matrices essentially scale   \n196 inversely to the hessian, that of SignSGD scales with $\\textstyle\\sum{\\frac{1}{2}}$ while that of SGD scales with $\\Sigma$ .   \n197 We conclude this section by presenting a condition on the step size scheduler that ensures the   \n198 asymptotic convergence of the expected loss to 0 in Phase 3. For general schedulers, we characterize   \n199 precisely the speed of convergence and the factors influencing it. Empirical validation is provided in   \n200 the right of Figure 2 for a convex quadratic. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "201 Lemma 3.9. Under the assumptions of Lemma 3.5, any step size scheduler $\\eta_{t}$ such that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\int_{0}^{\\infty}\\eta_{s}d s=\\infty\\,a n d\\operatorname*{lim}_{t\\to\\infty}\\eta_{t}=0\\implies\\mathbb{E}[f(X_{t})-f(X_{*})]\\stackrel{t\\to\\infty}{\\longrightarrow}\\times\\frac{\\mathcal{L}_{\\tau}\\sigma_{m a x}}{4\\mu}\\sqrt{\\frac{\\pi}{2}}\\eta_{t}\\stackrel{t\\to\\infty}{\\longrightarrow}0.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "202 Remark: Under the same conditions, SGD satisfies $\\begin{array}{r}{\\mathbb{E}[f(X_{t})-f(X_{*})]\\stackrel{t\\rightarrow\\infty}{\\rightarrow}\\stackrel{\\mathcal{L}_{\\tau}\\sigma_{\\operatorname*{max}}^{2}}{\\sim}\\eta_{t}\\stackrel{t\\rightarrow\\infty}{\\rightarrow}0.}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "Conclusion: As noted in Bernstein et al. (2018), the signal-to-noise ratio is key in determining the dynamics of SignSGD. Our SDEs help clarify the mechanisms underlying the dynamics of SignSGD: we show that the effect of noise is radically different from SGD: 1) It affects the rate of convergence of the iterates, of the covariance of the iterates, and of the expected loss; 2) The asymptotic loss value and covariance of the iterates scale in $\\textstyle\\sum{\\frac{1}{2}}$ while for SGD it does so in $\\Sigma$ . On the one hand, low levels of noise will ensure a faster and steadier loss decrease close to minima for SignSGD than for SGD. On the other, SGD will converge to much lower loss values. A symmetric argument holds for high levels of noise, which suggests that SignSGD is more resilient to high levels of noise. ", "page_idx": 5}, {"type": "text", "text": "204 3.1.1 Heavy-tailed noise ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "205 Interestingly, we can replicate the efforts above also in case the noise structure is heavy-tailed as it is   \n206 distributed according to a Student\u2019s t distribution. Notably, we derive the SDE for the case where the   \n207 noise has infinite variance and show how little marginal effect this has on the dynamics of SignSGD.   \n208 Lemma 3.10. Under the assumptions of Corollary 3.3 but the noise on the gradients $U\\sim t_{\\nu}(0,I_{d})$   \n209 where $\\nu\\in\\mathbb{Z}^{+}$ : The following SDE is a 1 weak approximation of the discrete update of SignSGD ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\nd X_{t}=-2\\Xi\\left(\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})\\right)d t+\\sqrt{\\eta}\\sqrt{I_{d}-4\\operatorname{diag}\\left(\\Xi\\left(\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})\\right)\\right)^{2}}d W_{t},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "210 where $\\Xi(x)$ is defined as $\\begin{array}{r}{\\Xi(x):=x\\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\sqrt{\\pi\\nu}\\Gamma\\left(\\frac{\\nu}{2}\\right)}{_2F_{1}}\\left(\\frac{1}{2},\\frac{\\nu+1}{2};\\frac{3}{2};-\\frac{x^{2}}{\\nu}\\right)}\\end{array}$ and ${_2F_{1}}\\left(a,b;c;x\\right)$ is the hyper  \n211 geometric function. Above, the function $\\Xi(x)$ and the square are applied component-wise.   \n212 We now characterize the dynamics of SignSGD when the noise on the gradient has infinite variance.   \n213 Corollary 3.11. Under the assumptions of Lemma 3.10 and $\\nu=2$ , the dynamics in Phase 3 is: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\nd X_{t}=-\\sqrt{\\frac{1}{2}}\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})d t+\\sqrt{\\eta}\\sqrt{I_{d}-\\frac{1}{2}\\operatorname{diag}\\left(\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})\\right)^{2}}d W_{t}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Conclusion: We observe that the dynamics of SignSGD when the noise is Gaussian (Eq. (5)) and when the noise is heavy-tailed with unbounded variance (Eq. (8)) are very similar: By comparing the constants in front of the drift terms $\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})$ , they are only $\\sim10\\%$ apart, and the diffusion coefficients are comparable. Not only do we once more showcase the resilience of SignSGD to high levels of noise, but in alignment with (Zhang et al., 2020b), we provide theoretical support to the success of Adam in such a scenario where SGD would diverge. ", "page_idx": 5}, {"type": "text", "text": "215 All the results derived above can be extended to this setting: this is left as an exercise for the reader. ", "page_idx": 5}, {"type": "text", "text": "216 3.2 AdamW SDE ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "217 In the last subsection, we showcased how SDEs can serve as powerful tools to understand the   \n218 dynamics of the simplest among coordinate-wise adaptive methods: SignSGD. Here, we extend the ", "page_idx": 5}, {"type": "image", "img_path": "nfC1OA6NeE/tmp/70dfc7a760a7899a2c33de72c5fde2ecdc062e6f5b7942174fe89ab1b3880ade.jpg", "img_caption": ["Figure 3: The first two images compare the SDEs of AdamW and RMSpropW with the respective optimizers in terms of trajectories and $f(x)$ for a convex quadratic function while the other two figures provide a comparison for an embedded saddle. In all cases, we observe good agreements. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "219 discussion to Adam with decoupled weight decay, i.e. AdamW: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{v_{k+1}}=\\beta_{2}v_{k}+\\left(1-\\beta_{2}\\right)\\left(\\nabla f_{\\gamma_{k}}(x_{k})\\right)^{2},\\quad{m_{k+1}}=\\beta_{1}{m_{k}}+(1-\\beta_{1})\\nabla f_{\\gamma_{k}}(x_{k}),}\\\\ {{x_{k+1}}=x_{k}-\\eta\\displaystyle\\frac{\\hat{m}_{k+1}}{\\sqrt{\\hat{v}_{k+1}}+\\epsilon}-\\eta\\gamma x_{k},\\quad\\hat{m}_{k}=\\displaystyle\\frac{m_{k}}{1-\\beta_{1}^{k}},\\quad\\hat{v}_{k}=\\displaystyle\\frac{v_{k}}{1-\\beta_{2}^{k}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "220 which, of course, covers Adam, RMSprop, and RMSpropW depending on the values of $\\gamma$ and $\\beta_{1}$ . ", "page_idx": 6}, {"type": "text", "text": "221 The following result proves the SDE of AdamW which we validate in Figure 3 for two simple   \n222 landscapes and in Figure 4 for a Transformer and a ResNet.   \n223 Theorem 3.12 (Informal Statement of Theorem C.31). Under sufficient regularity conditions, $\\rho_{1}=$   \n224 $\\mathcal{O}(\\eta^{-\\zeta})$ s.t. $\\zeta\\in(0,1)$ , and $\\rho_{2}=\\mathcal{O}(1)$ , the order 1 weak approximation of AdamW is: ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d X_{t}=-\\frac{\\sqrt{\\gamma_{2}\\left(t\\right)}}{\\gamma_{1}\\left(t\\right)}P_{t}^{-1}(M_{t}+\\eta\\rho_{1}\\left(\\nabla f\\left(X_{t}\\right)-M_{t}\\right))d t-\\gamma X_{t}d t}\\\\ &{d M_{t}=\\rho_{1}\\left(\\nabla f\\left(X_{t}\\right)-M_{t}\\right)d t+\\sqrt{\\eta}\\rho_{1}\\Sigma^{1/2}\\left(X_{t}\\right)d W_{t}}\\\\ &{\\;d V_{t}=\\rho_{2}\\left((\\nabla f(X_{t}))^{2}+\\mathrm{diag}\\left(\\Sigma\\left(X_{t}\\right)\\right)-V_{t}\\right)d t,}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "225 where $\\beta_{i}=1-\\eta\\rho_{i}\\sim1$ , $\\gamma_{i}(t)=1-e^{-\\rho_{i}t}$ , and $P_{t}=\\mathrm{diag}\\,\\sqrt{V_{t}}+\\epsilon\\sqrt{\\gamma_{2}(t)}I_{d}$ . ", "page_idx": 6}, {"type": "text", "text": "226 In contrast to Remark 4.3 of Malladi et al. (2022), which suggests that an SDE for RMSprop and   \n227 Adam is only viable if $\\sigma\\gg\\|\\nabla f(x)\\|$ and $\\begin{array}{r}{\\sigma\\sim\\frac{1}{\\eta}}\\end{array}$ , our derivation that does not need these assumptions:   \n228 See Remark C.25 for a deeper discussion, the implications, and the experimental comparison.   \n229 The following result demonstrates how the asymptotic expected loss of AdamW scales with the noise   \n230 level. Notably, it introduces the first scaling rule for AdamW, extending the one proposed for Adam   \n231 in (Malladi et al., 2022) to include weight decay scaling. It is crucial to understand that, unlike the   \n232 typical approach in the literature (see (Jastrzebski et al., 2018; Malladi et al., 2022)), our objective in   \n233 deriving these rules is not to maintain the dynamics of the optimizers or the SDE unchanged. Instead,   \n234 our goal is to offer a practical strategy for adjusting hyperparameters (e.g., from $\\eta$ to $\\widetilde{\\eta}$ ) to retain   \n235 certain performance metrics or optimizer properties as the batch size increases (e.g., from $B$ to $\\tilde{B}$ ).   \n236 Therefore, in our upcoming analysis, we aim to derive scaling rules that preserve specific relevant   \n237 aspects of the dynamics, such as the convergence bound on the loss or the speed. For a more detailed   \n238 discussion motivating our approach, see Appendix E.   \n239 Lemma 3.13. If $f$ is $\\mu$ -strongly convex and $L$ -smooth, ${\\mathcal{L}}_{\\tau}:=T r(\\nabla^{2}f(x)),$ , and $(\\nabla f(x))^{2}=\\mathcal{O}(\\eta),$ ,   \n240 $\\widetilde{\\eta}=\\kappa\\eta$ , $\\tilde{B}=B\\delta_{i}$ , and $\\tilde{\\rho}_{i}=\\alpha_{i}\\rho_{i}$ , and $\\tilde{\\gamma}=\\xi\\gamma,$ , AdamW satisfies ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}[f(X_{t})-f(X_{*})]\\stackrel{t\\rightarrow\\infty}{\\leq}\\frac{\\eta\\mathcal{L}_{\\tau}\\sigma L}{2}\\frac{\\kappa}{2\\mu\\sqrt{B\\delta}L+\\sigma\\xi\\gamma(L+\\mu)}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "241 We derive the novel scaling rule by $^{\\,l}$ ) Preserving the upper bound, which requires that $\\kappa=\\sqrt\\delta$ and   \n242 $\\xi=\\kappa_{!}$ ; 2) Preserving the relative speed of $M_{t}$ , $V_{t}$ and $X_{t}$ , which requires that $\\tilde{\\beta}_{i}=1-\\kappa^{2}(1-\\beta_{i})$ .   \n243 The left of Figure 5 shows the empirical verification of the predicted loss value and scaling rule on   \n244 a convex quadratic function.3Interestingly, and consistently with Lemma 3.13, such a value is not ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "nfC1OA6NeE/tmp/793782913581c92bf142a304a1ec104666246a1344dfd994f338e80f12d23332.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 4: The first two represent the comparison between AdamW and its SDE in terms of $f(x)$ . The other two do the same for RMSpropW. In both cases, the first is a Transformer on MNIST and the second a ResNet on CIFAR-10: Our SDEs match the respective optimizers. ", "page_idx": 7}, {"type": "image", "img_path": "nfC1OA6NeE/tmp/3150c7b6ce6de1f319e8f5a40ee46e56d4a3e1a41a73f22af1fc6a680b74a798.jpg", "img_caption": ["Figure 5: The loss predicted in Lemma 3.13 matches the experimental results on a convex quadratic function. AdamW is run with regularization parameter $\\gamma=1$ . AdamW $R$ (AdamW Rescaled) is run as we apply the scaling rule with $\\kappa=2$ . AdamW NR (AdamW Not Rescaled) is run as we apply the scaling rule with $\\kappa=2$ on all hyperparameters but $\\gamma$ , which is left unchanged: Our scaling rule holds, and failing to rescale $\\gamma$ leads the optimizer not to preserve the asymptotic loss level. The same happens for $\\gamma=4$ (Left); The same for RMSpropW (Center-Left); For AdamW, $\\beta_{1}$ and $\\beta_{2}$ influence which basin will attract the dynamics and how fast this will converge, but not the asymptotic loss level inside the basin (Center-Right). For both AdamW and RMSpropW, the variance at convergence predicted in Lemma 3.14 matches the experimental results (Right). "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "245 influenced by the choice of $\\beta_{i}$ : We argue that $\\beta_{i}$ do not impact the asymptotic level of the loss, but   \n246 rather drive the selection of the basin and speed at which AdamW converges to it \u2014 The center-right   \n247 of Figure 5 exemplifies this on a simple nonconvex landscape.   \n248 We conclude this section with the stationary distribution of AdamW around a minimum which we   \n249 empirically validate on the right of Figure 5. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "250 Lemma 3.14. The stationary distribution of AdamW is ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left(\\mathbb{E}[X_{\\infty}],C o v[X_{\\infty}]\\right)=\\left(0,\\frac{\\eta}{2}\\left(I_{d}+\\gamma H^{-1}\\Sigma^{\\frac{1}{2}}\\right)^{-1}H^{-1}\\Sigma^{\\frac{1}{2}}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "251 RMSpropW We derived the same results for RMSprop(W) and we reported them in Appendix   \n252 C.4: importantly, we validate the SDE in Figure 3 for two simple landscapes and in Figure 4 for a   \n253 Transformer and a ResNet. The results regarding the asymptotic loss level and stationary distributions   \n254 are validated in the center-left and right of Figure 5 for a convex quadratic function. ", "page_idx": 7}, {"type": "text", "text": "Conclusion: While for both SignSGD and Adam the asymptotic loss value and the covariance of the iterates scale linearly with $\\textstyle\\sum{\\frac{1}{2}}$ , we observe for AdamW this is more intricate: The interaction between curvature, noise, and regularization implies that these two quantities are upper-bounded in $\\Sigma^{\\frac{1}{2}}$ and increasing $\\Sigma$ to infinity does not lead to their explosion: Weight decay plays a crucial stabilization role at high noise levels near the minimizer \u2014 See Figure 6 for a comparison across optimizers. Finally, we argue that $\\beta_{i}$ play a key role in selecting the basin and the convergence speed to the asymptotic loss value rather than impacting the loss value itself. ", "page_idx": 7}, {"type": "text", "text": "255 ", "page_idx": 7}, {"type": "text", "text": "256 4 Experiments: SDE validation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "257 The point of our experiments is to validate the theoretical results derived from the SDEs. Therefore,   \n258 we first show that our SDEs faithfully represent the dynamics of their respective optimizers. To do   \n259 so, we integrate the SDEs with Euler-Maruyama (Algorithm 1): This is particularly challenging and   \n260 expensive as one needs to calculate the full gradients of the DNNs at each iteration.4 We present the   \n261 first set of validation experiments on a variety of architectures and datasets: An MLP on the Breast   \n262 Cancer dataset, a CNN and a Transformer on MNIST, and a ResNet on CIFAR-10. All details are in   \n263 Appendix F. ", "page_idx": 7}, {"type": "image", "img_path": "nfC1OA6NeE/tmp/157117c3e2f83c5f74c14f42a6343d523b0db939c225330c032e5453fb5c8d4c.jpg", "img_caption": ["Figure 6: For SGD (Left), SignSGD (Center-Left), Adam (Center-Right), and AdamW: For each optimizer, we plot the loss value on a convex quadratic and compare its asymptotic value with the limits predicted by our theory. As we take $\\bar{\\Sigma}\\,=\\,\\sigma^{2}I_{d}$ , we confirm that the loss of SGD scales quadratically in $\\sigma$ (Lemma 3.6), and linearly for SignSGD (Lemma 3.5) and Adam (Lemma 3.13 with $\\gamma=0$ ). For AdamW, the maximum asymptotic loss value is bounded in $\\sigma$ (Lemma 3.13 with $\\gamma>0$ ). In accordance with the experiments, our theory predicts that adaptive methods are more resilient to noise. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "264 5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "265 We derived the first formal SDE for SignSGD, enabling us to demonstrate its dynamics traversing   \n266 three discernible phases. We characterize how the signal-to-noise ratio drives the dynamics of the   \n267 loss in each of these phases, and we derive the asymptotic value of the loss function, as well as the   \n268 stationary distribution. Regarding the role of noise, we draw a straightforward comparison with   \n269 SGD. For SignSGD, the noise level $\\sqrt{\\Sigma}$ has an inverse linear effect on the convergence speed of the   \n270 loss and the iterates. However, it linearly affects the asymptotic expected loss and the asymptotic   \n271 variance of the iterates. In contrast, for SGD, noise does not influence the convergence speed but   \n272 has a quadratic impact on the loss level and variance. We also examine the scenario where the noise   \n273 has infinite variance and demonstrate the resilience of SignSGD, showing that its performance is   \n274 only marginally affected. Finally, we generalize the analysis to include AdamW and RMSpropW.   \n275 Specifically, we leverage our novel SDEs to derive the asymptotic value of the loss function, their   \n276 stationary distribution on a convex quadratic, and a novel scaling rule. The key insight is that, similarly   \n277 to SignSGD, the loss level and covariance matrix of the iterates of Adam and RMSprop scale linearly   \n278 in the noise level $\\textstyle\\sum{\\frac{1}{2}}$ . For AdamW and RMSpropW, the complex interaction of noise, curvature, and   \n279 regularization implies that these two quantities are bounded in terms of $\\textstyle\\sum{\\frac{1}{2}}$ , showing that weight   \n280 decay plays a crucial stabilization role at high noise levels near the minimizer. Interestingly, the   \n281 SDEs for Adam and RMSprop are straightforward corollary of our general results and were derived   \n282 under much less restrictive and more realistic assumptions than those in the literature. Finally, we   \n283 thoroughly validate all our theoretical results: We compare the dynamics of the various optimizers   \n284 with the respective SDEs and find good agreement on simple landscapes and deep neural networks.   \n285 For Adam and RMSprop, our SDEs track them better than those derived in (Malladi et al., 2022).   \n286 Future work We believe that our results can be extended to other optimizers commonly used in   \n287 practice such as Signum, AdaGrad, AdaMax, and Nadam. Additionally, inspired by the insights   \n288 from our SDE analysis, there is potential for designing new optimization algorithms that combine the   \n289 strengths of existing methods while mitigating their weaknesses. For example, developing hybrid   \n290 optimizers that adaptively switch between different strategies based on the training phase or current   \n291 state of the optimization process could offer superior performance. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "292 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "293 An, J., Lu, J., and Ying, L. (2020). Stochastic modified equations for the asynchronous stochastic   \n294 gradient descent. Information and Inference: A Journal of the IMA, 9(4):851\u2013873.   \n295 Ankirchner, S. and Perko, S. (2024). A comparison of continuous-time approximations to stochastic   \n296 gradient descent. Journal of Machine Learning Research, 25(13):1\u201355.   \n297 Ayadi, I. and Turinici, G. (2021). Stochastic runge-kutta methods and adaptive sgd- $\\mathrm{\\textmu}$ stochastic   \n298 gradient descent. In 2020 25th International Conference on Pattern Recognition (ICPR), pages   \n299 8220\u20138227. IEEE.   \n300 Balles, L. and Hennig, P. (2018). Dissecting adam: The sign, magnitude and variance of stochastic   \n301 gradients. In International Conference on Machine Learning, pages 404\u2013413. PMLR.   \n302 Barakat, A. and Bianchi, P. (2021). Convergence and dynamical behavior of the adam algorithm for   \n303 nonconvex stochastic optimization. SIAM Journal on Optimization, 31(1):244\u2013274.   \n304 Bardi, M. and Kouhkouh, H. (2022). Deep relaxation of controlled stochastic gradient descent via   \n305 singular perturbations. arXiv preprint arXiv:2209.05564.   \n306 Bercher, A., Gonon, L., Jentzen, A., and Salimova, D. (2020). Weak error analysis for stochastic   \n307 gradient descent optimization algorithms. arXiv preprint arXiv:2007.02723.   \n308 Bernstein, J., Wang, Y.-X., Azizzadenesheli, K., and Anandkumar, A. (2018). signSGD: Compressed   \n309 optimisation for non-convex problems. In Proceedings of the 35th International Conference on   \n310 Machine Learning.   \n311 Bishop, A. N. and Del Moral, P. (2019). Stability properties of systems of linear stochastic differential   \n312 equations with random coefficients. SIAM Journal on Control and Optimization, 57(2):1023\u20131042.   \n313 Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke,   \n314 A., VanderPlas, J., Wanderman-Milne, S., and Zhang, Q. (2018). JAX: composable transformations   \n315 of Python+NumPy programs.   \n316 Chen, P., Lu, J., and Xu, L. (2022). Approximation to stochastic variance reduced gradient langevin dy  \n317 namics by stochastic delay differential equations. Applied Mathematics & Optimization, 85(2):15.   \n318 Chen, X., Liu, S., Sun, R., and Hong, M. (2019). On the convergence of a class of adam-type   \n319 algorithms for non-convex optimization. In International Conference on Learning Representations.   \n320 Compagnoni, E. M., Biggio, L., Orvieto, A., Proske, F. N., Kersting, H., and Lucchi, A. (2023). An   \n321 sde for modeling sam: Theory and insights. In International Conference on Machine Learning,   \n322 pages 25209\u201325253. PMLR.   \n323 Compagnoni, E. M., Orvieto, A., Kersting, H., Proske, F., and Lucchi, A. (2024). Sdes for minimax   \n324 optimization. In International Conference on Artificial Intelligence and Statistics, pages 4834\u20134842.   \n325 PMLR.   \n326 Cui, Z.-X., Fan, Q., and Jia, C. (2020). Momentum methods for stochastic optimization over   \n327 time-varying directed networks. Signal Processing, 174:107614.   \n328 Dambrine, M., Dossal, C., Puig, B., and Rondepierre, A. (2024). Stochastic differential equations for   \n329 modeling first order optimization methods. SIAM Journal on Optimization, 34(2):1402\u20131426.   \n330 De, S., Mukherjee, A., and Ullah, E. (2018). Convergence guarantees for rmsprop and adam in   \n331 non-convex optimization and an empirical comparison to nesterov acceleration. arXiv preprint   \n332 arXiv:1807.06766.   \n333 D\u00e9fossez, A., Bottou, L., Bach, F., and Usunier, N. (2022). A simple convergence proof of adam and   \n334 adagrad. Transactions on Machine Learning Research.   \n335 Deng, L. (2012). The mnist database of handwritten digit images for machine learning research.   \n336 IEEE Signal Processing Magazine, 29(6):141\u2013142.   \n337 Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani,   \n338 M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. (2021). An image is   \n339 worth 16x16 words: Transformers for image recognition at scale. In International Conference on   \n340 Learning Representations.   \n341 Dua, D. and Graff, C. (2017). UCI machine learning repository.   \n342 Fontaine, X., De Bortoli, V., and Durmus, A. (2021). Convergence rates and approximation results   \n343 for sgd and its continuous-time counterpart. In Conference on Learning Theory, pages 1965\u20132058.   \n344 PMLR.   \n345 Ge, R., Huang, F., Jin, C., and Yuan, Y. (2015). Escaping from saddle points\u2014online stochastic   \n346 gradient for tensor decomposition. In Conference on Learning Theory, pages 797\u2013842.   \n347 Gess, B., Kassing, S., and Konarovskyi, V. (2024). Stochastic modified flows, mean-field limits and   \n348 dynamics of stochastic gradient descent. Journal of Machine Learning Research, 25(30):1\u201327.   \n349 Gu, H., Guo, X., and Li, X. (2021). Adversarial training for gradient descent: Analysis through its   \n350 continuous-time approximation. arXiv preprint arXiv:2105.08037.   \n351 Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., Wieser,   \n352 E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van Kerkwijk, M. H., Brett, M.,   \n353 Haldane, A., del R\u00edo, J. F., Wiebe, M., Peterson, P., G\u00e9rard-Marchant, P., Sheppard, K., Reddy,   \n354 T., Weckesser, W., Abbasi, H., Gohlke, C., and Oliphant, T. E. (2020). Array programming with   \n355 NumPy. Nature, 585(7825):357\u2013362.   \n356 Higham, D. J. (2001). An algorithmic introduction to numerical simulation of stochastic differential   \n357 equations. SIAM review, 43(3):525\u2013546.   \n358 Hong, Y. and Lin, J. (2023). High probability convergence of adam under unbounded gradients and   \n359 affine variance noise. arXiv preprint arXiv:2311.02000.   \n360 Hu, W., Li, C. J., and Zhou, X. (2019). On the global convergence of continuous\u2013time stochastic   \n361 heavy\u2013ball method for nonconvex optimization. In 2019 IEEE International Conference on Big   \n362 Data (Big Data), pages 94\u2013104. IEEE.   \n363 Jastrzebski, S., Kenton, Z., Arpit, D., Ballas, N., Fischer, A., Bengio, Y., and Storkey, A. (2018).   \n364 Three factors influencing minima in sgd. ICANN 2018.   \n365 Jin, C., Ge, R., Netrapalli, P., Kakade, S. M., and Jordan, M. I. (2017). How to escape saddle points   \n366 efficiently. In International Conference on Machine Learning, pages 1724\u20131732. PMLR.   \n367 Karatzas, I. and Shreve, S. (2014). Brownian motion and stochastic calculus, volume 113. springer.   \n368 Karimireddy, S. P., Rebjock, Q., Stich, S., and Jaggi, M. (2019a). Error feedback fixes signsgd and   \n369 other gradient compression schemes. In International Conference on Machine Learning, pages   \n370 3252\u20133261. PMLR.   \n371 Karimireddy, S. P., Rebjock, Q., Stich, S., and Jaggi, M. (2019b). Error feedback fixes SignSGD   \n372 and other gradient compression schemes. In Proceedings of the 36th International Conference on   \n373 Machine Learning.   \n374 Kingma, D. P. and Ba, J. (2015). Adam: A method for stochastic optimization. In International   \n375 Conference on Learning Representations.   \n376 Kohatsu-Higa, A., Le\u00f3n, J. A., and Nualart, D. (1997). Stochastic differential equations with random   \n377 coefficients. Bernoulli, pages 233\u2013245.   \n378 Krizhevsky, A., Hinton, G., et al. (2009). Learning multiple layers of features from tiny images.   \n379 Toronto, ON, Canada.   \n380 Kunin, D., Sagastuy-Brena, J., Gillespie, L., Margalit, E., Tanaka, H., Ganguli, S., and Yamins, D. L.   \n381 (2023). The limiting dynamics of sgd: Modified loss, phase-space oscillations, and anomalous   \n382 diffusion. Neural Computation, 36(1):151\u2013174.   \n383 Kunstner, F., Yadav, R., Milligan, A., Schmidt, M., and Bietti, A. (2024). Heavy-tailed class   \n384 imbalance and why adam outperforms gradient descent on language models. arXiv preprint   \n385 arXiv:2402.19449.   \n386 Lanconelli, A. and Lauria, C. S. (2022). A note on diffusion limits for stochastic gradient descent.   \n387 arXiv preprint arXiv:2210.11257.   \n388 Levy, K. Y. (2016). The power of normalization: Faster evasion of saddle points. arXiv preprint   \n389 arXiv:1611.04831.   \n390 Li, H., Rakhlin, A., and Jadbabaie, A. (2023a). Convergence of adam under relaxed assumptions. In   \n391 Thirty-seventh Conference on Neural Information Processing Systems.   \n392 Li, L. and Wang, Y. (2022). On uniform-in-time diffusion approximation for stochastic gradient   \n393 descent. arXiv preprint arXiv:2207.04922.   \n394 Li, Q., Tai, C., and Weinan, E. (2017). Stochastic modified equations and adaptive stochastic gradient   \n395 algorithms. In International Conference on Machine Learning, pages 2101\u20132110. PMLR.   \n396 Li, Q., Tai, C., and Weinan, E. (2019). Stochastic modified equations and dynamics of stochastic   \n397 gradient algorithms i: Mathematical foundations. The Journal of Machine Learning Research,   \n398 20(1):1474\u20131520.   \n399 Li, Z., Malladi, S., and Arora, S. (2021). On the validity of modeling SGD with stochastic differential   \n400 equations (SDEs). In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W., editors,   \n401 Advances in Neural Information Processing Systems.   \n402 Li, Z., Wang, Y., and Wang, Z. (2023b). Fast equilibrium of sgd in generic situations. In The Twelfth   \n403 International Conference on Learning Representations.   \n404 Liu, T., Chen, Z., Zhou, E., and Zhao, T. (2021). A diffusion approximation theory of momentum   \n405 stochastic gradient descent in nonconvex optimization. Stochastic Systems.   \n406 Loshchilov, I. and Hutter, F. (2019). Decoupled weight decay regularization. In International   \n407 Conference on Learning Representations.   \n408 Ma, C., Wu, L., and Weinan, E. (2022). A qualitative study of the dynamic behavior for adaptive   \n409 gradient algorithms. In Mathematical and Scientific Machine Learning, pages 671\u2013692. PMLR.   \n410 Mai, V. V. and Johansson, M. (2021). Stability and convergence of stochastic gradient clipping:   \n411 Beyond lipschitz continuity and smoothness. In International Conference on Machine Learning.   \n412 Malladi, S., Lyu, K., Panigrahi, A., and Arora, S. (2022). On the SDEs and scaling rules for adaptive   \n413 gradient algorithms. In Advances in Neural Information Processing Systems.   \n414 Mandt, S., Hoffman, M. D., and Blei, D. M. (2017). Stochastic gradient descent as approximate   \n415 bayesian inference. JMLR 2017.   \n416 Mao, X. (2007). Stochastic differential equations and applications. Elsevier.   \n417 Maulen-Soto, R., Fadili, J., Attouch, H., and Ochs, P. (2024). Stochastic inertial dynamics via time   \n418 scaling and averaging. arXiv preprint arXiv:2403.16775.   \n419 Maul\u00e9n Soto, R. I. (2021). A continuous-time model of stochastic gradient descent: convergence   \n420 rates and complexities under lojasiewicz inequality. Universidad de Chile.   \n421 Milstein, G. N. (2013). Numerical integration of stochastic differential equations, volume 313.   \n422 Springer Science & Business Media.   \n423 Mil\u2019shtein, G. (1986). Weak approximation of solutions of systems of stochastic differential equations.   \n424 Theory of Probability & Its Applications, 30(4):750\u2013766.   \n425 Noci, L., Anagnostidis, S., Biggio, L., Orvieto, A., Singh, S. P., and Lucchi, A. (2022). Signal   \n426 propagation in transformers: Theoretical perspectives and the role of rank collapse. Advances in   \n427 Neural Information Processing Systems, 35:27198\u201327211.   \n428 \u00d8ksendal, B. (1990). When is a stochastic integral a time change of a diffusion? Journal of theoretical   \n429 probability, 3(2):207\u2013226.   \n430 Pan, Y. and Li, Y. (2022). Toward understanding why adam converges faster than SGD for transform  \n431 ers. In OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop).   \n432 Paquette, C., Lee, K., Pedregosa, F., and Paquette, E. (2021). Sgd in the large: Average-case analysis,   \n433 asymptotics, and stepsize criticality. In Conference on Learning Theory, pages 3548\u20133626. PMLR.   \n434 Pascanu, R., Mikolov, T., and Bengio, Y. (2013). On the difficulty of training recurrent neural   \n435 networks. In International conference on machine learning.   \n436 Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M.,   \n437 Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher,   \n438 M., Perrot, M., and Duchesnay, E. (2011). Scikit-learn: Machine learning in Python. Journal of   \n439 Machine Learning Research, 12:2825\u20132830.   \n440 Poggio, T., Kawaguchi, K., Liao, Q., Miranda, B., Rosasco, L., Boix, X., Hidary, J., and Mhaskar,   \n441 H. (2017). Theory of deep learning iii: explaining the non-overfitting puzzle. arXiv preprint   \n442 arXiv:1801.00173.   \n443 Puchkin, N., Gorbunov, E., Kutuzov, N., and Gasnikov, A. (2024). Breaking the heavy-tailed noise   \n444 barrier in stochastic optimization problems. In International Conference on Artificial Intelligence   \n445 and Statistics.   \n446 Safaryan, M. and Richtarik, P. (2021). Stochastic sign descent methods: New algorithms and better   \n447 theory. In Proceedings of the 38th International Conference on Machine Learning.   \n448 Smith, S. L., Dherin, B., Barrett, D. G. T., and De, S. (2021). On the origin of implicit regularization   \n449 in stochastic gradient descent. ArXiv, abs/2101.12176.   \n450 Soto, R. M., Fadili, J., and Attouch, H. (2022). An sde perspective on stochastic convex optimization.   \n451 arXiv preprint arXiv:2207.02750.   \n452 Su, L. and Lau, V. K. (2023). Accelerated federated learning over wireless fading channels with   \n453 adaptive stochastic momentum. IEEE Internet of Things Journal.   \n454 Sun, J., Yang, Y., Xun, G., and Zhang, A. (2023). Scheduling hyperparameters to improve generaliza  \n455 tion: From centralized sgd to asynchronous sgd. ACM Transactions on Knowledge Discovery from   \n456 Data, 17(2):1\u201337.   \n457 Tieleman, T. and Hinton, G. (2012). Lecture 6.5-rmsprop: Divide the gradient by a running average   \n458 of its recent magnitude.   \n459 Van Rossum, G. and Drake, F. L. (2009). Python 3 Reference Manual. CreateSpace, Scotts Valley,   \n460 CA.   \n461 Wang, B., Fu, J., Zhang, H., Zheng, N., and Chen, W. (2024). Closing the gap between the upper   \n462 bound and lower bound of adam\u2019s iteration complexity. Advances in Neural Information Processing   \n463 Systems, 36.   \n464 Wang, B., Zhang, Y., Zhang, H., Meng, Q., Ma, Z.-M., Liu, T.-Y., and Chen, W. (2022). Provable   \n465 adaptivity in adam. arXiv preprint arXiv:2208.09900.   \n466 Wang, Y. and Wu, S. (2020). Asymptotic analysis via stochastic differential equations of gradient   \n467 descent algorithms in statistical and computational paradigms. Journal of machine learning   \n468 research, 21(199):1\u2013103.   \n469 Wang, Z. and Mao, Y. (2022). Two facets of sde under an information-theoretic lens: Generalization   \n470 of sgd via training trajectories and via terminal states. arXiv preprint arXiv:2211.10691.   \n471 Yang, J., Li, X., Fatkhullin, I., and He, N. (2024). Two sides of one coin: the limits of untuned sgd   \n472 and the power of adaptive methods. Advances in Neural Information Processing Systems, 36.   \n473 Zaheer, M., Reddi, S., Sachan, D., Kale, S., and Kumar, S. (2018). Adaptive methods for nonconvex   \n474 optimization. Advances in neural information processing systems, 31.   \n475 Zhang, J., He, T., Sra, S., and Jadbabaie, A. (2020a). Why gradient clipping accelerates training: A   \n476 theoretical justification for adaptivity. In International Conference on Learning Representations.   \n477 Zhang, J., Karimireddy, S. P., Veit, A., Kim, S., Reddi, S., Kumar, S., and Sra, S. (2020b). Why are   \n478 adaptive methods good for attention models? Advances in Neural Information Processing Systems.   \n479 Zhang, Y., Chen, C., Shi, N., Sun, R., and Luo, Z.-Q. (2022). Adam can converge without any   \n480 modification on update rules. Advances in neural information processing systems.   \n481 Zhang, Z., Li, Y., Luo, T., and Xu, Z.-Q. J. (2023). Stochastic modified equations and dynamics of   \n482 dropout algorithm. arXiv preprint arXiv:2305.15850.   \n483 Zhao, J., Lucchi, A., Proske, F. N., Orvieto, A., and Kersting, H. (2022). Batch size selection by   \n484 stochastic optimal control. In Has it Trained Yet? NeurIPS 2022 Workshop.   \n485 Zhou, P., Feng, J., Ma, C., Xiong, C., Hoi, S. C. H., et al. (2020a). Towards theoretically understanding   \n486 why sgd generalizes better than adam in deep learning. Advances in Neural Information Processing   \n487 Systems, 33:21285\u201321296.   \n488 Zhou, P., Xie, X., Lin, Z., and Yan, S. (2024). Towards understanding convergence and generalization   \n489 of adamw. IEEE Transactions on Pattern Analysis and Machine Intelligence.   \n490 Zhou, P., Xie, X., and Shuicheng, Y. (2022). Win: Weight-decay-integrated nesterov accelera  \n491 tion for adaptive gradient algorithms. In The Eleventh International Conference on Learning   \n492 Representations.   \n493 Zhou, X., Yuan, H., Li, C. J., and Sun, Q. (2020b). Stochastic modified equations for continuous   \n494 limit of stochastic admm. arXiv preprint arXiv:2003.03532.   \n495 Zhu, Y. and Ying, L. (2021). A sharp convergence rate for a model equation of the asynchronous   \n496 stochastic gradient descent. Communications in Mathematical Sciences. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "497 A Additional related works ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "498 In this section, we list some papers that derived or used SDEs to model optimizers. In particular, we   \n499 focus on the aspect of empirically verifying the validity of such SDEs in the sense that they indeed   \n500 track the respective optimizers. We divide these into three categories: Those that did not carry out   \n501 any type of validation, those that did it on simple landscapes (quadratic functions et similia), and   \n502 those that did small experiments or neural networks.   \n503 None of the following papers carried out any experimental validation of the approximating power of   \n504 the SDEs they derived. Many of them did not even validate the insights derived from the SDEs: (Liu   \n505 et al., 2021; Hu et al., 2019; Bercher et al., 2020; Zhu and Ying, 2021; Cui et al., 2020; Maul\u00e9n Soto,   \n506 2021; Wang and Wu, 2020; Lanconelli and Lauria, 2022; Ayadi and Turinici, 2021; Soto et al., 2022;   \n507 Li and Wang, 2022; Wang and Mao, 2022; Bardi and Kouhkouh, 2022; Chen et al., 2022; Kunin   \n508 et al., 2023; Zhang et al., 2023; Sun et al., 2023; Li et al., 2023b; Gess et al., 2024; Dambrine et al.,   \n509 2024; Maulen-Soto et al., 2024).   \n510 The following ones carried out validation experiments on artificial landscapes, e.g. quadratic or   \n511 quartic function, or easy regression tasks: (Li et al., 2017, 2019; Zhou et al., 2020b; An et al., 2020;   \n512 Fontaine et al., 2021; Gu et al., 2021; Su and Lau, 2023; Ankirchner and Perko, 2024).   \n513 The following papers carried out some experiments which include neural networks: (Paquette et al.,   \n514 2021; Compagnoni et al., 2023). In particular, they both simulate the SDEs with a numerical integrator   \n515 and compare them with the respective optimizers: The first validates the SDE on a shallow MLP   \n516 while the second does so on a shallow and a deep MLP. Regarding (Li et al., 2021; Malladi et al.,   \n517 2022), they do not validate their SDEs: Rather, their approach conceptually proceeds as follows: ", "page_idx": 13}, {"type": "text", "text": "518 1. Derive an SDE for an optimizer which we now dub \u201cA\u201d; ", "page_idx": 13}, {"type": "text", "text": "519 2. Notice that simulating the SDE is too expensive;   \n520 3. Define another discrete-time algorithm called SVAG which also has the same SDE as \u201cA\u201d   \n521 but does not numerically integrate the SDE as it does not even require access to it: It does   \n522 not need access neither to the drift nor to the diffusion term;   \n523 4. Simulate SVAG and show that it tracks \u201cA\u201d successfully;   \n524 5. Conclude that the SDE is a good approximation for \u201cA\u201d.   \n525 However, they never validated that the SDE is a good approximation for $^{\\bullet}A^{\\bullet}$ or for SVAG either.   \n526 With the same logic, they could have done the following:   \n527 1. Derive an SDE for $^{\\bullet}A^{\\,\\bullet}$ ;   \n528 2. Notice that simulating the SDE is too expensive;   \n529 3. Define another discrete-time algorithm called \u201cB\u201d which coincides with $^{\\bullet}A^{\\bullet}$ and thus of   \n530 course shares the same SDE;   \n531 4. Simulate \u201cB\u201d and show that it tracks \u201cA\u201d perfectly;   \n532 5. Conclude that the SDE is a good approximation for $^{\\bullet}A^{\\,\\bullet}$ .   \n533 In particular, the only fact they prove is that SVAG is a discrete-time optimizer that shares the same   \n534 SDE as \u201cA\u201d because it describes a discrete trajectory that is a 1st-order approximation of the SDE of   \n535 \u201cA\u201d. Technically speaking, \u201cA\u201d also does the same. One cannot conclude that the SDE derived for \u201cA\u201d   \n536 is a good model for \u201cA\u201d by simply comparing two algorithms $^{\\bullet}A^{\\,\\bullet}$ and $\\mathbf{\\nabla}B^{\\circ}$ that share the same SDE.   \n537 Otherwise, simply comparing an optimizer $^{\\bullet}A^{\\bullet}$ with itself would do the trick. An SDE\u2019s empirical   \n538 validation can only occur if the SDE is simulated with a numerical integrator that requires access to   \n539 the drift and diffusion terms (Higham, 2001; Milstein, 2013). ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "540 B Stochastic calculus ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "541 In this section, we summarize some important results in the analysis of Stochastic Differential   \n542 Equations Mao (2007); \u00d8ksendal (1990). The notation and the results in this section will be used   \n543 extensively in all proofs in this paper. We assume the reader to have some familiarity with Brownian   \n544 motion and with the definition of stochastic integral (Ch. 1.4 and 1.5 in Mao (2007)). ", "page_idx": 14}, {"type": "text", "text": "545 B.1 It\u00f4\u2019s Lemma ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We start with some notation: Let $(\\Omega,\\mathcal{F},\\{\\mathcal{F}_{t}\\}_{t\\geq0},\\mathbb{P})$ be a filtered probability space. We say that an event $E\\in{\\mathcal{F}}$ holds almost surely (a.s.) in this space if $\\mathbb{P}(E)=1$ . We call $\\mathcal{L}^{p}([a,b],\\mathbb{R}^{d})$ , with $p>0$ , the family of $\\mathbb{R}^{d}$ -valued $\\mathcal{F}_{t}$ -adapted processes $\\{f_{t}\\}_{a\\leq t\\leq b}$ such that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\int_{a}^{b}\\|f_{t}\\|^{p}d t\\leq\\infty.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "546 Moreover, we denote by $\\mathcal{M}^{p}([a,b],\\mathbb{R}^{d})$ , with $p>0$ , the family of $\\mathbb{R}^{d}$ -valued processes $\\{f_{t}\\}_{a\\leq t\\leq b}$   \n547 in $\\mathcal{L}([a,b],\\ensuremath{\\mathbb{R}}^{d})$ such that $\\mathbb{E}\\left[\\int_{a}^{b}\\|f_{t}\\|^{p}d t\\right]\\leq\\infty$ . We will write $h\\,\\in\\,\\mathcal{L}^{p}\\left(\\mathbb{R}_{+},\\mathbb{R}^{d}\\right)$ , with $p>0$ , if   \n548 $h\\in\\mathcal{L}^{p}\\left([0,T],\\mathbb{R}^{d}\\right)$ for every $T>0$ . Similar definitions hold for matrix-valued functions using the   \n549 Frobenius norm $\\begin{array}{r}{\\|\\dot{A}\\|:=\\sqrt{\\sum_{i j}|A_{i j}|^{2}}}\\end{array}$ .   \n550 Let $W=\\{W_{t}\\}_{t\\geq0}$ be a one-dimensional Brownian motion defined on our probability space and let   \n551 $X=\\{X_{t}\\}_{t\\geq0}$ be an $\\mathcal{F}_{t}$ -adapted process taking values on $\\mathbb{R}^{d}$ .   \n552 Definition B.1. Let the drift be $b\\in\\mathcal{L}^{1}\\left(\\mathbb{R}_{+},\\mathbb{R}^{d}\\right)$ and the diffusion term be $\\sigma\\in\\mathcal{L}^{2}\\left(\\mathbb{R}_{+},\\mathbb{R}^{d\\times m}\\right)$ .   \n553 $X_{t}$ is an It\u00f4 process if it takes the form ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\nX_{t}=x_{0}+\\int_{0}^{t}b_{s}d s+\\int_{0}^{t}\\sigma_{s}d W_{s}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "554 We shall say that $X_{t}$ has the stochastic differential ", "page_idx": 15}, {"type": "equation", "text": "$$\nd X_{t}=b_{t}d t+\\sigma_{t}d W_{t}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "555 ", "page_idx": 15}, {"type": "text", "text": "556 Theorem B.2 (It\u00f4\u2019s Lemma). Let $X_{t}$ be an It\u00f4 process with stochastic differential $d X_{t}=b_{t}d t+$   \n557 ${\\sigma}_{t}d W_{t}$ . Let $f\\left(x,t\\right)$ be twice continuously differentiable in x and continuously differentiable in $t_{\\perp}$ ,   \n558 taking values in $\\mathbb{R}$ . Then $f(X_{t},t)$ is again an It\u00f4 process with stochastic differential ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\boldsymbol{\\mathcal{H}}(X_{t},t)=\\partial_{t}f(X_{t},t))d t+\\langle\\nabla f(X_{t},t),b_{t}\\rangle d t+\\frac{1}{2}T r\\left(\\sigma_{t}\\sigma_{t}^{\\top}\\nabla^{2}f(X_{t},t)\\right)d t+\\langle\\nabla f(X_{t},t),\\sigma_{t}\\rangle d W_{t}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "559 B.2 Stochastic Differential Equations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "560 Stochastic Differential Equations (SDEs) are equations of the form ", "page_idx": 15}, {"type": "equation", "text": "$$\nd X_{t}=b(X_{t},t)d t+\\sigma(X_{t},t)d W_{t}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "561 First of all, we need to define what it means for a stochastic process $X=\\{X_{t}\\}_{t\\geq0}$ with values in $\\mathbb{R}^{d}$   \n562 to solve an SDE.   \n563 Definition B.3. Let $X_{t}$ be as above with deterministic initial condition $X_{0}\\,=\\,x_{0}$ . Assume $b:$   \n564 $\\mathbb R^{d}\\times[0,T]\\rightarrow\\mathbb R^{d}$ and $\\sigma:\\mathbb{R}^{d}\\times[0,T]\\rightarrow\\mathbb{R}^{d\\times m}$ are Borel measurable; $X_{t}$ is called a solution to the   \n565 corresponding SDE if ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "566 1. $X_{t}$ is continuous and $\\mathcal{F}_{t}$ -adapted; ", "page_idx": 15}, {"type": "equation", "text": "$$\nX_{t}=x_{0}+\\int_{0}^{t}b(X_{s},s)d s+\\int_{0}^{t}\\sigma(X_{s},s)d W(s)\\textrm{\\ }a.s.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Moreover, the solution $X_{t}$ is said to be unique if any other solution $X_{t}^{\\star}$ is such that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left\\{X_{t}=X_{t}^{\\star},\\;\\mathrm{for}\\;\\mathrm{all}\\;0\\leq t\\leq T\\right\\}=1.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "569   \n570 Notice that since the solution to an SDE is an It\u00f4 process, we can use It\u00f4\u2019s Lemma. The following   \n571 theorem gives a sufficient condition on $b$ and $\\sigma$ for the existence of a solution to the corresponding   \n572 SDE. ", "page_idx": 15}, {"type": "text", "text": "573 Theorem B.4. Assume that there exist two positive constants $\\bar{K}$ and $K$ such that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{max}\\{||b(x,t)-b(y,t)||^{2},\\,\\,||\\sigma(x,t)-\\sigma(y,t)||^{2}\\}\\leq\\bar{K}\\|x-y\\|^{2};\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "2. (Linear growth condition) for all $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ and $t\\in[0,T]$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\{\\|b(x,t)\\|^{2},\\;\\|\\sigma(x,t)\\|^{2}\\}\\leq K(1+\\|x\\|^{2}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "574 Then, there exists a unique solution $X_{t}$ to the corresponding SDE, and $X_{t}\\in\\mathcal{M}^{2}([0,T],\\mathbb{R}^{d})$ . ", "page_idx": 15}, {"type": "text", "text": "575 Numerical approximation. Often, SDEs are solved numerically. The simplest algorithm to provide   \n576 a sample path $(\\hat{x}_{k})_{k\\geq0}$ for $X_{t}$ , so that $X_{k\\Delta t}\\approx\\hat{x}_{k}$ for some small $\\Delta t$ and for all $k\\Delta t\\leq M$ is called   \n577 Euler-Maruyama (Algorithm 1). For more details on this integration method and its approximation   \n578 properties, the reader can check Mao (2007). ", "page_idx": 15}, {"type": "text", "text": "input The drift $b$ , the volatility $\\sigma$ , and the initial condition $x_{0}$ . Fix a stepsize $\\Delta t$ ; Initialize x\u02c60 = x0; $k=0$ ; while $\\begin{array}{r}{k\\leq\\left\\lfloor\\frac{T}{\\Delta t}\\right\\rfloor}\\end{array}$ do Sample some $d$ -dimensional Gaussian noise $Z_{k}\\sim\\mathcal{N}(0,I_{d})$ ; Compute $\\hat{x}_{k+1}=\\hat{x}_{k}+\\Delta t\\,b(\\hat{x}_{k},k\\Delta t)+\\sqrt{\\Delta t}\\,\\sigma(\\hat{x}_{k},k\\Delta t)Z_{k}$ ; $k=k+1$ ; end while   \noutput The approximated sample path (x\u02c6k)0\u2264k\u2264\u230a\u2206Tt\u230b. ", "page_idx": 16}, {"type": "text", "text": "579 C Theoretical framework - Weak Approximation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "580 In this section, we introduce the theoretical framework used in the paper, together with its assumptions   \n581 and notations. ", "page_idx": 16}, {"type": "text", "text": "First of all, many proofs will use Taylor expansions in powers of $\\eta$ . For ease of notation, we introduce the shorthand that whenever we write $\\mathcal{O}\\left(\\eta^{\\alpha}\\right)$ , we mean that there exists a function $K(x)\\in G$ such that the error terms are bounded by $K(x)\\eta^{\\alpha}$ . For example, we write ", "page_idx": 16}, {"type": "equation", "text": "$$\nb(x+\\eta)=b_{0}(x)+\\eta b_{1}(x)+\\mathcal{O}\\left(\\eta^{2}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "to mean: there exists $K\\in G$ such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n|b(x+\\eta)-b_{0}(x)-\\eta b_{1}(x)|\\leq K(x)\\eta^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "582 Additionally, we introduce the following shorthand: ", "page_idx": 16}, {"type": "text", "text": "583 ", "page_idx": 16}, {"type": "text", "text": "\u2022 A multi-index is $\\alpha=(\\alpha_{1},\\alpha_{2},\\ldots,\\alpha_{n})$ such that $\\alpha_{j}\\in\\{0,1,2,\\ldots\\}$ ; ", "page_idx": 16}, {"type": "text", "text": "584 ", "page_idx": 16}, {"type": "text", "text": "585 ", "page_idx": 16}, {"type": "text", "text": "586 ", "page_idx": 16}, {"type": "text", "text": "\u2022 $|\\alpha|:=\\alpha_{1}+\\alpha_{2}+\\cdot\\cdot\\cdot+\\alpha_{n}$ ;   \n\u2022 $\\alpha!:=\\alpha_{1}!\\alpha_{2}!\\cdot\\cdot\\cdot\\alpha_{n}!$ ;   \n\u2022 For $x=(x_{1},x_{2},...,x_{n})\\in\\mathbb{R}^{n}$ , we define $x^{\\alpha}:=x_{1}^{\\alpha_{1}}x_{2}^{\\alpha_{2}}\\cdot\\cdot\\cdot x_{n}^{\\alpha_{n}}$ 1 \u2202|\u03b2| ", "page_idx": 16}, {"type": "text", "text": "587 ", "page_idx": 16}, {"type": "text", "text": "588 ", "page_idx": 16}, {"type": "text", "text": "\u2022 We also denote the partial derivative with respect to $x_{i}$ by $\\partial_{e_{i}}$ ", "page_idx": 16}, {"type": "text", "text": "589 ", "page_idx": 16}, {"type": "text", "text": "590 Definition C.1 (G Set). Let $G$ denote the set of continuous functions $\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ of at most polynomial   \n591 growth, i.e. $g\\in G$ if there exists positive integers $\\nu_{1},\\nu_{2}>0$ such that $|g(x)|\\leq\\nu_{1}\\left(1+\\hat{|x|}^{\\bar{2}\\nu_{2}}\\right)$ , for   \n592 all $z\\in\\mathbb{R}^{d}$ .   \n593 The next results are inspired by Theorem 1 of Li et al. (2017) and are derived under some regularity   \n594 assumption on the function $f$ . ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Assumption C.2. Assume that the following conditions on $f,f_{i}$ , and their gradients are satisfied: ", "page_idx": 16}, {"type": "text", "text": "\u2022 $\\nabla f,\\nabla f_{i}$ satisfy a Lipschitz condition: there exists $L>0$ such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n|\\nabla f(u)-\\nabla f(v)|+\\sum_{i=1}^{n}|\\nabla f_{i}(u)-\\nabla f_{i}(v)|\\leq L|u-v|;\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "\u2022 $f,f_{i}$ and its partial derivatives up to order 7 belong to $G$ ; ", "page_idx": 16}, {"type": "text", "text": "\u2022 $\\nabla f,\\nabla f_{i}$ satisfy a growth condition: there exists $M>0$ such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n|\\nabla f(x)|+\\sum_{i=1}^{n}|\\nabla f_{i}(x)|\\leq M(1+|x|).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Lemma C.3 (Lemma 1 Li et al. (2017)). Let $0<\\eta<1$ . Consider a stochastic process $X_{t},t\\ge0$ satisfying the SDE ", "page_idx": 17}, {"type": "equation", "text": "$$\nd X_{t}=b\\left(X_{t}\\right)d t+\\sqrt{\\eta}\\sigma\\left(X_{t}\\right)d W_{t}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with $X_{0}=x\\in\\mathbb{R}^{d}$ and $b,\\sigma$ together with their derivatives belong to $G$ . Define the one-step difference $\\Delta=X_{\\eta}-x$ , and indicate the $i$ -th component of $\\Delta$ with $\\Delta_{i}$ . Then we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{l.~\\mathbb{E}\\Delta_{i}=b_{i}\\eta+\\frac{1}{2}\\left[\\sum_{j=1}^{d}b_{j}\\partial_{e_{j}}b_{i}\\right]\\eta^{2}+{\\mathcal O}\\left(\\eta^{3}\\right)\\quad\\forall i=1,\\ldots,d;}\\\\ &{2.~\\mathbb{E}\\Delta_{i}\\Delta_{j}=\\left[b_{i}b_{j}+\\sigma\\sigma_{\\left(i j\\right)}^{T}\\right]\\eta^{2}+{\\mathcal O}\\left(\\eta^{3}\\right)\\quad\\forall i,j=1,\\ldots,d;}\\\\ &{3.~\\mathbb{E}\\prod_{j=1}^{s}\\Delta_{\\left(i_{j}\\right)}={\\mathcal O}\\left(\\eta^{3}\\right)f o r\\,a l l\\,s\\ge3,i_{j}=1,\\ldots,d.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "All functions above are evaluated at $x$ . ", "page_idx": 17}, {"type": "text", "text": "596 ", "page_idx": 17}, {"type": "text", "text": "Theorem C.4 (Theorem 2 and Lemma 5, Mil\u2019shtein (1986)). Let Assumption C.2 hold and let us define $\\bar{\\Delta}=x_{1}-x$ to be the increment in the discrete-time algorithm, and indicate the $i$ -th component of $\\bar{\\Delta}$ with $\\bar{\\Delta}_{i}$ . If in addition there exists $K_{1},K_{2},K_{3},K_{4}\\in G$ so that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathbb E\\Delta_{i}-\\mathbb E\\bar{\\Delta}_{i}\\right|\\leq K_{1}(x)\\eta^{2},\\quad\\forall i=1,\\ldots,d;}\\\\ &{\\left|\\mathbb E\\Delta_{i}\\Delta_{j}-\\mathbb E\\bar{\\Delta}_{i}\\bar{\\Delta}_{j}\\right|\\leq K_{2}(x)\\eta^{2},\\quad\\forall i,j=1,\\ldots,d;}\\\\ &{\\left|\\mathbb E\\prod_{j=1}^{s}\\Delta_{i j}-\\mathbb E\\prod_{j=1}^{s}\\bar{\\Delta}_{i j}\\right|\\leq K_{3}(x)\\eta^{2},\\quad\\forall s\\geq3,\\quad\\forall i_{j}\\in\\{1,\\ldots,d\\};}\\\\ &{\\mathbb E\\prod_{j=1}^{3}\\left|\\bar{\\Delta}_{i j}\\right|\\leq K_{4}(x)\\eta^{2},\\quad\\forall i_{j}\\in\\{1,\\ldots,d\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, there exists a constant $C$ so that for all $k=0,1,\\ldots,N$ we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathbb{E}g\\left(X_{k\\eta}\\right)-\\mathbb{E}g\\left(x_{k}\\right)|\\leq C\\eta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "597 ", "page_idx": 17}, {"type": "text", "text": "598 C.1 Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "599 Modeling of discrete-time algorithms using SDEs relies on Assumption C.2. As noted by Li et al.   \n600 (2021), the approximation can fail when the stepsize $\\eta$ is large or if certain conditions on $\\nabla f$ and the   \n601 noise covariance matrix are not met. Although these issues can be addressed by increasing the order   \n602 of the weak approximation, we believe that the primary purpose of SDEs is to serve as simplification   \n603 tools that enhance our intuition: We would not benefit significantly from added complexity. ", "page_idx": 17}, {"type": "text", "text": "604 C.2 Formal derivation - SignSGD ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "605 In this subsection, we provide the first formal derivation of an SDE model for SignSGD. Let us   \n606 consider the stochastic process $X_{t}\\in\\mathbb{R}^{d}$ defined as the solution of ", "page_idx": 17}, {"type": "equation", "text": "$$\nd X_{t}=-(1-2\\mathbb{P}(\\nabla f_{\\gamma}(X_{t})<0))d t+\\sqrt{\\eta}\\sqrt{\\bar{\\Sigma}(X_{t})}d W_{t},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "607 where ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\bar{\\Sigma}(x)=\\mathbb{E}[\\xi_{\\gamma}(x)\\xi_{\\gamma}(x)^{\\top}],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "608 and $\\xi_{\\gamma}(x):=\\mathrm{sign}(\\nabla f_{\\gamma}(x))-1+2\\mathbb{P}(\\nabla f_{\\gamma}(x)<0)$ the noise in the sample sign $(\\nabla f_{\\gamma}(x))$ . The   \n609 following theorem guarantees that such a process is a 1-order SDE of the discrete-time algorithm of   \n610 SignSGD ", "page_idx": 17}, {"type": "equation", "text": "$$\nx_{k+1}=x_{k}-\\eta\\mathrm{sign}\\left(f_{\\gamma_{k}}(x_{k})\\right),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "611 with $x_{0}\\in\\mathbb{R}^{d},\\eta\\in\\mathbb{R}^{>0}$ is the step size, the mini-batches $\\{\\gamma_{k}\\}$ are modelled as i.i.d. random variables   \n612 uniformly distributed on $\\{1,\\cdot\\cdot\\cdot,N\\}$ , and of size $B\\geq1$ . ", "page_idx": 17}, {"type": "text", "text": "Theorem C.5 (Stochastic modified equations). Let $0<\\eta<1,T>0$ and set $N=\\lfloor T/\\eta\\rfloor$ . Let $x_{k}\\,\\in\\,\\mathbb{R}^{d},0\\,\\le\\,k\\,\\le\\,N$ denote a sequence of SignSGD iterations defined by Eq. (18). Consider the stochastic process $X_{t}$ defined in Eq. (16) and fix some test function $g\\in G$ and suppose that $g$ and its partial derivatives up to order 6 belong to $G$ . ", "page_idx": 18}, {"type": "text", "text": "Then, under Assumption C.2, there exists a constant $C>0$ independent of $\\eta$ such that for all $k=0,1,\\ldots,N$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathbb{E}g\\left(X_{k\\eta}\\right)-\\mathbb{E}g\\left(x_{k}\\right)|\\leq C\\eta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "613 ", "page_idx": 18}, {"type": "text", "text": "That is, the SDE (16) is an order 1 weak approximation of the SignSGD iterations (18). ", "page_idx": 18}, {"type": "text", "text": "Lemma C.6. Under the assumptions of Theorem C.5, let $0<\\eta<1$ and consider $x_{k},k\\ge0$ satisfying the SignSGD iterations ", "page_idx": 18}, {"type": "equation", "text": "$$\nx_{k+1}=x_{k}-\\eta s i g n\\left(\\nabla f_{\\gamma_{k}}(x_{k})\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "with $x_{0}\\in\\mathbb{R}^{d}$ . From the definition the one-step difference $\\bar{\\Delta}=x_{1}-x_{2}$ , then we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{l.}&{\\mathbb{E}\\bar{\\Delta}_{i}=-\\left(1-2\\mathbb{P}\\left(\\partial_{i}f_{\\gamma}<0\\right)\\right)\\eta\\quad\\forall i=1,\\ldots,d;}\\\\ &{2.}&{\\mathbb{E}\\bar{\\Delta}_{i}\\bar{\\Delta}_{j}\\quad=\\quad\\left(\\left(1-2\\mathbb{P}\\left(\\partial_{i}f_{\\gamma}<0\\right)\\right)\\left(1-2\\mathbb{P}\\left(\\partial_{j}f_{\\gamma}<0\\right)\\right)+\\bar{\\Sigma}_{(i j)}\\right)\\eta^{2}\\quad\\forall i,j}\\\\ &{1,\\ldots,d;}\\\\ &{3.}&{\\mathbb{E}\\prod_{j=1}^{s}\\bar{\\Delta}_{i j}=\\mathcal{O}\\left(\\eta^{3}\\right)\\quad\\forall s\\geq3,\\quad i_{j}\\in\\{1,\\ldots,d\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "614 ", "page_idx": 18}, {"type": "text", "text": "All the functions above are evaluated at $x$ . ", "page_idx": 18}, {"type": "text", "text": "615 Proof of Lemma C.6. First of all, we have that by definition ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[x_{1}^{i}-x^{i}\\right]=-\\eta\\mathbb{E}\\left[\\mathrm{sign}\\left(\\partial_{i}f_{\\gamma}(x)<0\\right)\\right],\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "616 which implies ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\bar{\\Delta}_{i}=-\\left(1-2\\mathbb{P}\\left(\\partial_{i}f_{\\gamma}(x)<0\\right)\\right)\\eta\\quad\\forall i=1,\\ldots,d.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "617 Second, we have that by definition ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left(x_{1}-x\\right)\\left(x_{1}-x\\right)^{\\top}\\right]=\\mathbb{E}\\Big[\\left(\\mathrm{sign}\\left(\\partial_{i}f_{\\gamma}(x)<0\\right)-1+2\\mathbb{P}\\left(\\partial_{i}f_{\\gamma}(x)<0\\right)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad\\left(\\mathrm{sign}\\left(\\partial_{i}f_{\\gamma}(x)<0\\right)-1+2\\mathbb{P}\\left(\\partial_{i}f_{\\gamma}(x)<0\\right)\\right)^{\\top}\\Big]\\eta^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "618 which implies that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\bar{\\Delta}_{i}\\bar{\\Delta}_{j}=\\left(1-2\\mathbb{P}\\left(\\partial_{i}f_{\\gamma}<0\\right)\\right)\\left(1-2\\mathbb{P}\\left(\\partial_{j}f_{\\gamma}<0\\right)\\right)\\eta^{2}+\\bar{\\Sigma}_{(i j)}\\eta^{2}\\quad\\forall i,j=1,\\ldots,d.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "619 Finally, by definition ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\prod_{j=1}^{s}\\bar{\\Delta}_{i_{j}}=\\mathcal{O}\\left(\\eta^{3}\\right)\\quad\\forall s\\ge3,\\quad i_{j}\\in\\{1,\\ldots,d\\},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "620 which concludes our proof. ", "page_idx": 18}, {"type": "text", "text": "621 Proof of Theorem C.5. To prove this result, all we need to do is check the conditions in Theorem C.4.   \n622 As we apply Lemma C.3, we make the following choices: ", "page_idx": 18}, {"type": "text", "text": "623 ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\bullet\\ b(x)=-(1-2\\mathbb{P}\\left(\\nabla f_{\\gamma}(x)<0\\right));\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "624 ", "page_idx": 18}, {"type": "text", "text": "625 First of all, we notice that $\\forall i=1,\\ldots,d.$ , it holds that ", "page_idx": 19}, {"type": "text", "text": "626 ", "page_idx": 19}, {"type": "text", "text": "627 ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\;\\mathbb{E}\\bar{\\Delta}_{i}\\overset{1\\mathrm{.\\,Lemma\\,C.6}}{=}-\\left(1-2\\mathbb{P}\\left(\\partial_{i}f_{\\gamma}(x)<0\\right)\\right)\\eta;}\\\\ &{\\bullet\\;\\mathbb{E}\\Delta_{i}\\overset{1\\mathrm{.\\,Lemma\\,C.3}}{=}-\\left(1-2\\mathbb{P}\\left(\\partial_{i}f_{\\gamma}(x)<0\\right)\\right)\\eta+\\mathcal{O}\\left(\\eta^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "628 Therefore, we have that for some $K_{1}(x)\\in G$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}\\Delta_{i}-\\mathbb{E}\\bar{\\Delta}_{i}\\right|\\leq K_{1}(x)\\eta^{2},\\quad\\forall i=1,\\ldots,d.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "629 Additionally, we notice that $\\forall i,j=1,\\ldots,d_{!}$ , it holds that ", "page_idx": 19}, {"type": "text", "text": "630 ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\;\\mathbb{E}\\bar{\\Delta}_{i}\\bar{\\Delta}_{j}\\!\\stackrel{2\\cdot\\operatorname{Lemma}\\,\\mathrm{C}.6}{=}(1-2\\mathbb{P}\\left(\\partial_{i}f_{\\gamma}(x)<0\\right))\\left(1-2\\mathbb{P}\\left(\\partial_{j}f_{\\gamma}(x)<0\\right)\\right)\\eta^{2}+\\bar{\\Sigma}_{(i j)}(x)\\eta^{2};}\\\\ &{\\bullet\\;\\mathbb{E}\\Delta_{i}\\Delta_{j}\\quad^{2,\\mathrm{\\Lambda}\\mathrm{Lemma}\\,\\mathrm{C}.3}\\quad\\left((1-2\\mathbb{P}\\left(\\partial_{i}f_{\\gamma}(x)<0\\right))\\left(1-2\\mathbb{P}\\left(\\partial_{j}f_{\\gamma}(x)<0\\right)\\right)+\\bar{\\Sigma}_{(i j)}(x)\\right)\\eta^{2}\\;+}\\\\ &{\\mathcal{O}\\left(\\eta^{3}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "631 ", "page_idx": 19}, {"type": "text", "text": "632 ", "page_idx": 19}, {"type": "text", "text": "633 Therefore, we have that for some $K_{2}(x)\\in G$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\mathbb{E}\\Delta_{i}\\Delta_{j}-\\mathbb{E}\\bar{\\Delta}_{i}\\bar{\\Delta}_{j}\\right|\\leq K_{2}(x)\\eta^{2},\\quad\\forall i,j=1,\\ldots,d.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "634 Additionally, we notice that $\\forall s\\geq3,\\forall i_{j}\\in\\{1,\\ldots,d\\}$ , it holds that ", "page_idx": 19}, {"type": "text", "text": "635 ", "page_idx": 19}, {"type": "text", "text": "636 ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\ \\mathbb{E}\\prod_{j=1}^{s}\\bar{\\Delta}_{i_{j}}\\stackrel{3\\cdot\\mathrm{Lemma}\\,\\mathrm{C.6}}{=}\\mathcal{O}\\left(\\eta^{3}\\right);}\\\\ &{\\bullet\\ \\mathbb{E}\\prod_{j=1}^{s}\\Delta_{i_{j}}\\stackrel{3\\cdot\\mathrm{Lemma}\\,\\mathrm{C.3}}{=}\\mathcal{O}\\left(\\eta^{3}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "637 Therefore, we have that for some $K_{3}(x)\\in G$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}\\prod_{j=1}^{s}\\Delta_{i_{j}}-\\mathbb{E}\\prod_{j=1}^{s}\\bar{\\Delta}_{i_{j}}\\right|\\leq K_{3}(x)\\eta^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "638 Additionally, for some $K_{4}(x)\\in G,\\forall i_{j}\\in\\{1,\\ldots,d\\}.$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\prod_{j=1}^{3}\\left|\\bar{\\Delta}_{(i_{j})}\\right|^{3.\\operatorname{Lemma}\\mathbb{C}.6}K_{4}(x)\\eta^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "639 To conclude, Eq. (25), Eq. (26), Eq. (27), and Eq. (28) allow us to conclude the proof. ", "page_idx": 19}, {"type": "text", "text": "Corollary C.7. Let us take the same assumptions of Theorem C.5, and that the stochastic gradient is $\\nabla f_{\\gamma}(x)=\\nabla f(x)+U$ such that $\\dot{U}\\sim\\mathcal{N}(0,\\Sigma)$ that does not depend on x. Then, the following $S D E$ provides a 1 weak approximation of the discrete update of SignSGD ", "page_idx": 19}, {"type": "equation", "text": "$$\nd X_{t}=-E r f\\left(\\frac{\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})}{\\sqrt{2}}\\right)d t+\\sqrt{\\eta}\\sqrt{I_{d}-\\mathrm{diag}\\left(E r f\\left(\\frac{\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})}{\\sqrt{2}}\\right)\\right)^{2}}d W_{t},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the error function $E r f(x)$ and the square are applied component-wise, and $\\Sigma\\,=$ diag $\\left(\\sigma_{1}^{2},\\cdot\\cdot\\cdot,\\sigma_{d}^{2}\\right)$ . ", "page_idx": 19}, {"type": "text", "text": "640 ", "page_idx": 19}, {"type": "text", "text": "641 Proof of Corollary C.7. First of all, we observe that ", "page_idx": 19}, {"type": "equation", "text": "$$\n1-2\\mathbb{P}\\left(\\nabla f_{\\gamma}(x)<0\\right)=1-2\\mathbb{P}\\left(\\nabla f(x)+\\Sigma^{\\frac{1}{2}}U<0\\right)=1-2\\Phi\\left(-\\Sigma^{-\\frac{1}{2}}\\nabla f(x)\\right),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "642 where $\\Phi$ is the cumulative distribution function of the standardized normal distribution. Remembering   \n643 that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Phi(x)={\\frac{1}{2}}\\left(1+\\mathrm{Erf}\\left({\\frac{x}{\\sqrt{2}}}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "644 we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n1-2\\mathbb{P}\\left(\\nabla f_{\\gamma}(x)<0\\right)=1-2\\frac{1}{2}\\left(1+\\mathrm{Erf}\\left(-\\frac{\\Sigma^{-\\frac{1}{2}}\\nabla f(x)}{\\sqrt{2}}\\right)\\right)=\\mathrm{Erf}\\left(\\frac{\\Sigma^{-\\frac{1}{2}}\\nabla f(x)}{\\sqrt{2}}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "645 Similarly, one can prove that $\\bar{\\Sigma}$ defined in (17) becomes ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\bar{\\Sigma}=I_{d}-\\mathrm{diag}\\left(\\mathrm{Erf}\\left(\\frac{\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})}{\\sqrt{2}}\\right)\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "646 ", "page_idx": 20}, {"type": "text", "text": "Corollary C.8. Let us take the sa\u221ame assumptions of Theorem C.5, and that the stochastic gradient is $\\nabla f_{\\gamma}(x)=\\nabla f(x)+\\sqrt{\\Sigma}U$ such that $U\\sim t_{\\nu}(0,I_{d})$ that does not depend on $x$ and $\\nu$ is a positive integer number. Then, the following SDE provides a 1 weak approximation of the discrete update of SignSGD ", "page_idx": 20}, {"type": "equation", "text": "$$\nd X_{t}=-2\\Xi\\left(\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})\\right)d t+\\sqrt{\\eta}\\sqrt{I_{d}-4\\operatorname{diag}\\left(\\Xi\\left(\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})\\right)\\right)^{2}}d W_{t},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\Xi(x)$ is defined as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Xi(x):=x{\\frac{{\\Gamma}\\left({\\frac{\\nu+1}{2}}\\right)}{{\\sqrt{\\pi\\nu}}\\Gamma\\left({\\frac{\\nu}{2}}\\right)}}{_2F_{1}}\\left({\\frac{1}{2}},{\\frac{\\nu+1}{2}};{\\frac{3}{2}};-{\\frac{x^{2}}{\\nu}}\\right),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and ${_2F_{1}}\\left(a,b;c;x\\right)$ is the hypergeometric function. Above, function $\\Xi(x)$ and the square are applied component-wise, and $\\Sigma=\\operatorname{diag}\\left(\\sigma_{1}^{2},\\cdot\\cdot\\cdot\\,,\\sigma_{d}^{2}\\right)$ . ", "page_idx": 20}, {"type": "text", "text": "647 ", "page_idx": 20}, {"type": "text", "text": "648 Proof of Corollary C.8. First of all, we observe that ", "page_idx": 20}, {"type": "equation", "text": "$$\n1-2\\mathbb{P}\\left(\\nabla f_{\\gamma}(x)<0\\right)=1-2\\mathbb{P}\\left(\\nabla f(x)+\\Sigma^{\\frac{1}{2}}U<0\\right)=1-2F_{\\nu}\\left(-\\Sigma^{-\\frac{1}{2}}\\nabla f(x)\\right),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "649 where $F_{\\nu}\\left(x\\right)$ is the cumulative function of a $t$ distribution with $\\nu$ degrees of freedom. Remembering   \n650 that ", "page_idx": 20}, {"type": "equation", "text": "$$\nF_{\\nu}\\left(x\\right)=\\frac{1}{2}+\\Xi(x),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "651 we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n1-2\\mathbb{P}\\left(\\nabla f_{\\gamma}(x)<0\\right)=1-2\\left(\\frac{1}{2}+\\Xi(x)\\right)=-2\\Xi(x).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "652 Similarly, one can prove that $\\bar{\\Sigma}$ defined in (17) becomes ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\bar{\\Sigma}=I_{d}-4\\,\\mathrm{diag}\\,\\Big(\\Xi\\,\\Big(\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})\\Big)\\Big)^{2}\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "653 ", "page_idx": 20}, {"type": "text", "text": "654 Lemma C.9. Under the assumptions of Corollary C.7 and signal-to-noise ratio Yt := \u03a3\u22122 \u221a\u22072f(Xt), ", "page_idx": 20}, {"type": "text", "text": "1. Phase 1: If $|Y_{t}|>\\frac{3}{2}$ , the SDE coincides with the ODE of SignGD: ", "page_idx": 20}, {"type": "equation", "text": "$$\nd X_{t}=-s i g n(\\nabla f(X_{t}))d t;\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "656 ", "page_idx": 20}, {"type": "text", "text": "2. Phase 2: If $\\begin{array}{r}{1<|Y_{t}|<\\frac{3}{2}}\\end{array}$ : ", "page_idx": 20}, {"type": "text", "text": "657 ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{1)\\ m Y_{t}+\\mathbf{q}^{-}\\leq\\frac{d\\mathbb{E}\\left[X_{t}\\right]}{d t}\\leq m Y_{t}+\\mathbf{q}^{+};}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(b)\\ \\mathbb{P}\\left[\\|X_{t}-\\mathbb{E}\\left[X_{t}\\right]\\|_{2}^{2}>a\\right]\\le\\frac{\\eta}{a}\\left(d-\\|m Y_{t}+\\mathbf{q}^{-}\\|_{2}^{2}\\right);}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "659 ", "page_idx": 21}, {"type": "text", "text": "3. Phase 3: If $\\ '|Y_{t}|<1$ , the SDE is ", "page_idx": 21}, {"type": "equation", "text": "$$\nd X_{t}=-\\sqrt{\\frac{2}{\\pi}}\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})d t+\\sqrt{\\eta}\\sqrt{I_{d}-\\frac{2}{\\pi}}\\operatorname{diag}\\Big(\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})\\Big)^{2}d W_{t}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "660 Proof of Lemma C.9. Exploiting the regularity of the Erf function, we approximate the SDE in (29)   \n661 in three different regions: ", "page_idx": 21}, {"type": "text", "text": "662 ", "page_idx": 21}, {"type": "text", "text": "1. Phase 1: If $|x|>\\frac{3}{2}$ , $\\mathrm{Erf}(x)\\sim\\mathrm{sign}(x)$ . Therefore, if $\\left|\\,\\frac{\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})}{\\sqrt{2}}\\,\\right|\\,>\\,\\frac{3}{2}$ ", "page_idx": 21}, {"type": "text", "text": "663 ", "page_idx": 21}, {"type": "text", "text": "664 ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\operatorname{Erf}\\left({\\frac{\\Sigma^{-{\\frac{1}{2}}}\\nabla f(X_{t})}{\\sqrt{2}}}\\right)\\sim\\operatorname{sign}\\left({\\frac{\\Sigma^{-{\\frac{1}{2}}}\\nabla f(X_{t})}{\\sqrt{2}}}\\right)=\\operatorname{sign}\\left(\\nabla f(X_{t})\\right);}\\\\ &{\\operatorname{Erf}\\left({\\frac{\\Sigma^{-{\\frac{1}{2}}}\\nabla f(X_{t})}{\\sqrt{2}}}\\right)^{2}\\sim\\operatorname{sign}\\left({\\frac{\\Sigma^{-{\\frac{1}{2}}}\\nabla f(X_{t})}{\\sqrt{2}}}\\right)^{2}=(1,\\dots,1).}\\end{array}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "665 ", "page_idx": 21}, {"type": "text", "text": "Therefore, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d X_{t}=-\\mathrm{Erf}\\left(\\frac{\\sum^{-\\frac{1}{2}}\\nabla f(X_{t})}{\\sqrt{2}}\\right)d t+\\sqrt{\\eta}\\sqrt{I_{d}-\\mathrm{diag}\\left(\\mathrm{Erf}\\left(\\frac{\\sum^{-\\frac{1}{2}}\\nabla f(X_{t})}{\\sqrt{2}}\\right)\\right)^{2}}d W_{t}}\\\\ &{\\qquad\\sim-\\mathrm{sign}(\\nabla f(X_{t}));}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "666   \n667   \n668 ", "page_idx": 21}, {"type": "text", "text": "2. Phase 2: Let $m$ and $q_{1}$ are the slope and intercept of the line secant to the graph of $\\operatorname{Erf}(x)$ between the points $(1,\\mathrm{Erf}(1))$ and $\\left({\\frac{3}{2}},\\operatorname{Erf}\\left({\\frac{3}{2}}\\right)\\right)$ , while $q_{2}$ is the intercept of the line tangent to the graph of $\\mathrm{Erf}(x)$ and slope $m$ . If $\\begin{array}{r}{1<x<\\frac{3}{2}}\\end{array}$ , we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\nm x+q_{1}<\\operatorname{Erf}(x)<m x+q_{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "669 ", "page_idx": 21}, {"type": "text", "text": "Analogously, if $-{\\frac{3}{2}}<x<-1$ ", "page_idx": 21}, {"type": "equation", "text": "$$\nm x-q_{2}<\\operatorname{Erf}(x)<m x-q_{1}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "670 ", "page_idx": 21}, {"type": "text", "text": "Therefore, we have that if $\\begin{array}{r}{1<\\left|\\frac{\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})}{\\sqrt{2}}\\right|<\\frac{3}{2}}\\end{array}$ , then ", "page_idx": 21}, {"type": "text", "text": "(a) ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{m}{\\sqrt{2}}\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})+\\mathbf{q}^{-}<\\operatorname{Erf}\\left(\\frac{\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})}{\\sqrt{2}}\\right)<\\frac{m}{\\sqrt{2}}\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})+\\mathbf{q}^{+},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "671 ", "page_idx": 21}, {"type": "text", "text": "where ", "page_idx": 21}, {"type": "equation", "text": "$$\n(\\mathbf{q}^{+})_{i}:={\\binom{q_{2}}{-q_{1}}}\\quad\\mathrm{if}\\;\\partial_{i}f(x)>0\\quad\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "672 ", "page_idx": 21}, {"type": "text", "text": "and ", "page_idx": 21}, {"type": "equation", "text": "$$\n(\\mathbf{q}^{-})_{i}:=\\left\\{\\!\\!\\begin{array}{l l}{q_{1}}&{\\mathrm{if}\\;\\partial_{i}f(x)>0}\\\\ {-q_{2}}&{\\mathrm{if}\\;\\partial_{i}f(x)<0\\;,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "673 ", "page_idx": 21}, {"type": "text", "text": "Therefore, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{m}{\\sqrt{2}}\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})+\\mathbf{q}^{-}\\leq\\frac{d\\mathbb{E}\\left[X_{t}\\right]}{d t}\\leq\\frac{m}{\\sqrt{2}}\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})+\\mathbf{q}^{+};\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "674 ", "page_idx": 21}, {"type": "text", "text": "(b) Similar to the above, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left({\\frac{m}{\\sqrt{2}}}\\Sigma^{-{\\frac{1}{2}}}\\nabla f(X_{t})+\\mathbf{q}^{-}\\right)^{2}\\leq\\operatorname{Erf}\\left({\\frac{\\Sigma^{-{\\frac{1}{2}}}\\nabla f(X_{t})}{\\sqrt{2}}}\\right)^{2}\\leq\\left({\\frac{m}{\\sqrt{2}}}\\Sigma^{-{\\frac{1}{2}}}\\nabla f(X_{t})+\\mathbf{q}^{+}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}\\left[\\|X_{t}-\\mathbb{E}\\left[X_{t}\\right]\\|_{2}^{2}>a\\right]\\le\\mathbb{P}\\left[\\exists i\\;\\mathrm{s.t.}\\;|X_{t}^{i}-\\mathbb{E}\\left[X_{t}^{i}\\right]|^{2}>a\\right]}&{}\\\\ {\\le\\displaystyle\\sum_{i}\\mathbb{P}\\left[|X_{t}^{i}-\\mathbb{E}\\left[X_{t}^{i}\\right]|>\\sqrt{a}\\right]}&{}\\\\ {\\le\\displaystyle\\frac{\\eta}{a}\\sum_{i}\\left(1-\\mathrm{Erf}\\left(\\frac{\\sum_{i}^{-\\frac12}\\partial_{i}f\\left(X_{t}\\right)}{\\sqrt{2}}\\right)^{2}\\right)}&{}\\\\ {<\\displaystyle\\frac{\\eta}{a}\\left(d-\\|\\frac{m}{\\sqrt{2}}\\Sigma^{-\\frac12}\\nabla f(X_{t})+\\mathbf{q}^{-}\\|_{2}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "676 ", "page_idx": 22}, {"type": "text", "text": "3. Phase 3: If $\\begin{array}{r}{|x|<1,\\operatorname{Erf}(x)\\sim\\frac{2}{\\sqrt{\\pi}}}\\end{array}$ . Therefore, if $\\begin{array}{r}{\\left|\\frac{\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})}{\\sqrt{2}}\\right|<1}\\end{array}$ , ", "page_idx": 22}, {"type": "text", "text": "677 ", "page_idx": 22}, {"type": "text", "text": "678 ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Erf}\\left(\\frac{\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})}{\\sqrt{2}}\\right)\\sim\\sqrt{\\frac{2}{\\pi}}\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t});}\\\\ &{\\bigg(\\mathrm{Erf}\\left(\\frac{\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})}{\\sqrt{2}}\\right)\\bigg)^{2}\\sim\\frac{2}{\\pi}\\left(\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "679 ", "page_idx": 22}, {"type": "text", "text": "Therefore, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d X_{t}=-\\mathrm{Erf}\\left(\\frac{\\sum^{-\\frac{1}{2}}\\nabla f(X_{t})}{\\sqrt{2}}\\right)d t+\\sqrt{\\eta}\\sqrt{I_{d}-\\mathrm{diag}\\left(\\mathrm{Erf}\\left(\\frac{\\sum^{-\\frac{1}{2}}\\nabla f(X_{t})}{\\sqrt{2}}\\right)\\right)^{2}}d W_{t}}\\\\ &{\\qquad\\sim-\\sqrt{\\frac{2}{\\pi}}\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})d t+\\sqrt{\\eta}\\sqrt{I_{d}-\\frac{2}{\\pi}}\\operatorname{diag}\\left(\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})\\right)^{2}d W_{t}.\\qquad\\qquad(2)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "680 ", "page_idx": 22}, {"type": "text", "text": "681 Lemma C.10 (Dynamics of Expected Loss). Let $f$ be $\\mu$ -strongly convex, $T r(\\nabla^{2}f(x))\\leq\\mathcal{L}_{\\tau}$ , and   \n682 $S_{t}:=f(X_{t})-f(X_{*})$ . Then, during ", "page_idx": 22}, {"type": "text", "text": "683 ", "page_idx": 22}, {"type": "text", "text": "684 ", "page_idx": 22}, {"type": "text", "text": "685 ", "page_idx": 22}, {"type": "text", "text": "2. Phase 2 wit $\\begin{array}{r l}&{\\iota\\,\\Delta:=\\left(\\frac{m}{\\sqrt{2}\\sigma_{m a x}}+\\frac{\\eta\\mu m^{2}}{4\\sigma_{m a x}^{2}}\\right)\\!\\colon\\!\\mathbb{E}[S_{t}]\\le S_{0}e^{-2\\mu\\Delta t}+\\frac{\\eta}{2}\\frac{\\left(\\mathcal{L}_{\\tau}-\\mu d\\hat{q}^{2}\\right)}{2\\mu\\Delta}\\left(1-e^{-2\\mu\\Delta t}\\right);}\\\\ &{\\iota\\,\\Delta:=\\left(\\sqrt{\\frac{2}{\\pi}}\\frac{1}{\\sigma_{m a x}}+\\frac{\\eta}{\\pi}\\frac{\\mu}{\\sigma_{m a x}^{2}}\\right)\\!\\colon\\!\\mathbb{E}[S_{t}]\\le S_{0}e^{-2\\mu\\Delta t}+\\frac{\\eta}{2}\\frac{\\mathcal{L}_{\\tau}}{2\\mu\\Delta}\\left(1-e^{-2\\mu\\Delta t}\\right)\\!.}\\end{array}$   \n3. Phase 3 wit   \n686 Proof of Lemma C.10. We prove each point by leveraging the shape of the law of $X_{t}$ derived in   \n687 Lemma C.9: ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "Phase 1: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "690 ", "page_idx": 22}, {"type": "text", "text": "$d(f(X_{t})-f(X_{*}))=-\\nabla f(X_{t})\\mathrm{sign}(\\nabla f(X_{t}))=-\\|\\nabla f(X_{t})\\|_{1}\\leq-\\|\\nabla f(X_{t})\\|_{2}$ (53) Since $f$ is $\\mu-P L$ , we have that $-\\|\\nabla f(X_{t})\\|_{2}^{2}<-2\\mu(f(X_{t})-f(X_{*}))$ , which implies that ", "page_idx": 22}, {"type": "equation", "text": "$$\nf(X_{t})-f(X_{*})\\leq\\frac{1}{4}\\left(\\sqrt{\\mu}t-2\\sqrt{f(X_{0})-f(X_{*})}\\right)^{2},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "691 ", "page_idx": 22}, {"type": "text", "text": "meaning that the dynamics will stop before t\u2217= 2 f(X0)\u2212\u00b5f(X\u2217); ", "page_idx": 22}, {"type": "text", "text": "692 ", "page_idx": 22}, {"type": "text", "text": "2. Phase 2: By applying the It\u00f4 Lemma to $f(X_{t})-f(X_{*})$ and that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{m}{\\sqrt{2}}\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})+\\mathbf{q}^{-}<\\operatorname{Erf}\\left(\\frac{\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})}{\\sqrt{2}}\\right)<\\frac{m}{\\sqrt{2}}\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})+\\mathbf{q}^{+},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d(f(X_{t})-f(X_{*}))\\leq-\\left(\\frac{m}{\\sqrt{2}}\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})+\\mathbf{q}^{-}\\right)^{\\top}\\nabla f(X_{t})d t+{\\mathcal{O}}(\\mathrm{Noise})\\qquad\\mathrm{~(~)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,\\frac{\\eta}{2}\\mathrm{Tr}\\left[\\nabla^{2}f(X_{t})\\left(I_{d}-\\mathrm{diag}\\left(\\frac{m}{\\sqrt{2}}\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})+\\mathbf{q}^{-}\\right)^{2}\\right)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq-\\displaystyle\\frac{m}{\\sqrt{2}}\\displaystyle\\frac{1}{\\sigma_{\\operatorname*{max}}}\\|\\nabla f(X_{t})\\|_{2}^{2}d t-\\hat{q}\\|\\nabla f(X_{t})\\|_{1}d t+\\frac{\\eta\\mathcal{L}_{\\tau}}{2}d t}\\\\ &{\\quad-\\displaystyle\\frac{\\eta\\mu}{2}\\|\\frac{m}{\\sqrt{2}}\\Sigma^{-\\frac{1}{3}}\\nabla f(X_{t})+\\mathbf{q}^{-\\parallel}\\|_{2}^{2}d t+\\mathcal{O}(\\mathrm{Noise})}\\\\ &{\\le-\\displaystyle\\frac{m}{\\sqrt{2}}\\displaystyle\\frac{1}{\\sigma_{\\operatorname*{max}}}\\|\\nabla f(X_{t})\\|_{2}^{2}d t-\\hat{q}\\|\\nabla f(X_{t})\\|_{1}d t+\\frac{\\eta\\mathcal{L}_{\\tau}}{2}d t}\\\\ &{\\quad-\\displaystyle\\frac{\\eta\\mu m^{2}}{4\\sigma_{\\operatorname*{max}}^{2}}\\|\\nabla f(X_{t})\\|_{2}^{2}d t-\\frac{\\eta\\mu d\\hat{q}^{2}}{2}d t-\\frac{\\sqrt{2m}\\hat{q}}{\\sigma_{\\operatorname*{max}}}\\|\\nabla f(X_{t})\\|_{1}d t}\\\\ &{\\quad+\\mathcal{O}(\\mathrm{Noise})}\\\\ &{\\le-2\\mu\\left(\\displaystyle\\frac{m}{\\sqrt{2}\\sigma_{\\operatorname*{max}}}+\\frac{\\eta\\mu m^{2}}{4\\sigma_{\\operatorname*{max}}^{2}}\\right)(f(X_{t})-f(X_{*}))d t}\\\\ &{\\quad+\\frac{\\eta}{2}\\left(C_{7}-\\mu d\\hat{q}^{2}\\right)d t+\\mathcal{O}(\\mathrm{Noise}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "694 ", "page_idx": 23}, {"type": "text", "text": "which implies that if $\\begin{array}{r}{k:=2\\mu\\left(\\frac{m}{\\sqrt{2}\\sigma_{\\mathrm{max}}}+\\frac{\\eta\\mu m^{2}}{4\\sigma_{\\mathrm{max}}^{2}}\\right)}\\end{array}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}[f(X_{t})-f(X_{*})]\\le(f(X_{0})-f(X_{*})))e^{-k t}+\\frac{\\eta\\left(\\mathcal{L}_{\\tau}-\\mu d\\widehat{q}^{2}\\right)}{2k}\\left(1-e^{-k t}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "3. Phase 3: By applying the It\u00f4 Lemma to $f(X_{t})-f(X_{*})$ , we have that: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d(f(X_{t})-f(X_{*}))=-\\sqrt{\\frac{2}{\\pi}}\\nabla f(X_{t})^{\\top}\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})d t+\\mathcal{O}(\\mathrm{Noise})}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\frac{\\eta}{2}\\mathrm{Tr}\\left(\\left(I_{d}-\\frac{2}{\\pi}\\dim\\left(\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})\\right)^{2}\\right)\\nabla^{2}f(X_{t})\\right)d t}\\\\ &{\\qquad\\qquad\\qquad\\leq-\\sqrt{\\frac{2}{\\pi}}\\frac{1}{\\sigma_{\\operatorname*{max}}}\\|\\nabla f(X_{t})\\|_{2}^{2}d t+\\mathcal{O}(\\mathrm{Noise})}\\\\ &{\\qquad\\qquad\\qquad+\\frac{\\eta}{2}\\mathrm{Tr}\\left(\\nabla^{2}f(X_{t})\\right)d t-\\frac{\\eta}{\\pi}\\frac{\\mu}{\\sigma_{\\operatorname*{max}}^{2}}\\|\\nabla f(X_{t})\\|_{2}^{2}d t}\\\\ &{\\qquad\\qquad\\qquad\\leq-\\left(\\sqrt{\\frac{2}{\\pi}}\\frac{1}{\\sigma_{\\operatorname*{max}}}+\\frac{\\eta}{\\pi}\\frac{\\mu}{\\sigma_{\\operatorname*{max}}^{2}}\\right)\\|\\nabla f(X_{t})\\|_{2}^{2}d t}\\\\ &{\\qquad\\qquad\\qquad+\\frac{\\eta}{2}T r(\\nabla^{2}f(X_{t}))d t+\\mathcal{O}(\\mathrm{Noise})}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "696 ", "page_idx": 23}, {"type": "text", "text": "Since $f$ is $\\mu$ -Strongly Convex, $f$ is also $\\mu$ -PL. Therefore, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d(f(X_{t})-f(X_{*}))\\leq-\\,2\\mu\\left(\\sqrt{\\frac{2}{\\pi}}\\frac{1}{\\sigma_{\\operatorname*{max}}}+\\frac{\\eta}{\\pi}\\frac{\\mu}{\\sigma_{\\operatorname*{max}}^{2}}\\right)(f(X_{t})-f(X_{*}))d t}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,\\frac{\\eta}{2}T r(\\nabla^{2}f(X_{t}))d t+{\\mathcal{O}}(\\mathrm{Noise}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "697 ", "page_idx": 23}, {"type": "text", "text": "Therefore, ", "page_idx": 23}, {"type": "equation", "text": "$$\nd\\mathbb{E}[f(X_{t})-f(X_{*})]\\leq-2\\mu\\left(\\sqrt{\\frac{2}{\\pi}}\\frac{1}{\\sigma_{\\operatorname*{max}}}+\\frac{\\eta}{\\pi}\\frac{\\mu}{\\sigma_{\\operatorname*{max}}^{2}}\\right)(\\mathbb{E}[f(X_{t})-f(X_{*})])d t+\\frac{\\eta}{2}\\mathcal{L}_{\\tau}d t,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which implies that if $\\begin{array}{r}{k:=2\\mu\\left(\\sqrt{\\frac{2}{\\pi}}\\frac{1}{\\sigma_{\\operatorname*{max}}}+\\frac{\\eta}{\\pi}\\frac{\\mu}{\\sigma_{\\operatorname*{max}}^{2}}\\right)}\\end{array}$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}[f(X_{t})-f(X_{*})]\\le(f(X_{0})-f(X_{*})))e^{-k t}+\\frac{\\eta\\mathcal{L}_{\\tau}}{2k}\\left(1-e^{-k t}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "699 ", "page_idx": 24}, {"type": "text", "text": "700 Lemma C.11. Under the assumptions of Lemma 3.5, for any step size scheduler $\\eta_{t}$ such that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\int_{0}^{\\infty}\\eta_{s}d s=\\infty\\,a n d\\,\\operatorname*{lim}_{t\\to\\infty}\\eta_{t}=0\\implies\\mathbb{E}[f(X_{t})-f(X_{*})]\\stackrel{t\\to\\infty}{\\longrightarrow}0.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "701 Proof of Lemma C.11. For any scheduler $\\eta_{k}$ used in ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{x_{k+1}=x_{k}-\\eta\\eta_{k}\\mathrm{sign}\\left(f_{\\gamma_{k}}(x_{k})\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "702 the SDE of Phase 3 is ", "page_idx": 24}, {"type": "equation", "text": "$$\nd X_{t}=-\\sqrt{\\frac{2}{\\pi}}\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})\\eta_{t}d t+\\sqrt{\\eta}\\eta_{t}\\sqrt{I_{d}-\\frac{2}{\\pi}\\operatorname{diag}\\left(\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})\\right)^{2}}d W_{t}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "703 Therefore, analogously to the calculations in Lemma C.10, we have that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}[f(X_{t})-f(X_{*})]\\leq\\frac{f(X_{0})-f(X_{*})+\\frac{\\eta\\mathcal{L}_{\\tau}}{2}\\int_{0}^{t}e^{2\\mu\\int_{0}^{s}\\left(\\sqrt{\\frac{2}{\\pi}}\\frac{1}{\\sigma_{\\operatorname*{max}}}\\eta_{l}+\\frac{\\eta}{\\pi}\\frac{\\mu}{\\sigma_{\\operatorname*{max}}^{2}}\\eta_{l}^{2}\\right)d l}\\eta_{s}^{2}d s}{e^{2\\mu\\int_{0}^{t}\\left(\\sqrt{\\frac{2}{\\pi}}\\frac{1}{\\sigma_{\\operatorname*{max}}}\\eta_{s}+\\frac{\\eta}{\\pi}\\frac{\\mu}{\\sigma_{\\operatorname*{max}}^{2}}\\eta_{s}^{2}\\right)d s}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "704 Therefore, using l\u2019H\u00f4pital\u2019s rule we have that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\int_{0}^{\\infty}\\eta_{s}d s=\\infty\\,{\\mathrm{and~}}\\operatorname*{lim}_{t\\to\\infty}\\eta_{t}=0\\implies\\mathbb{E}[f(X_{t})-f(X_{*})]\\stackrel{t\\to\\infty}{\\longrightarrow}0.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "705 ", "page_idx": 24}, {"type": "text", "text": "706 Lemma C.12. Let $H=\\operatorname{diag}(\\lambda_{1},\\dots,\\lambda_{d})$ and $M_{t}:=e^{-2\\left(\\sqrt{\\frac{2}{\\pi}}\\Sigma^{-\\frac{1}{2}}H+\\frac{\\eta}{\\pi}\\Sigma^{-\\frac{1}{2}}H^{2}\\right)t}.$ . Then, ", "page_idx": 24}, {"type": "text", "text": "707 ", "page_idx": 24}, {"type": "text", "text": "708 ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{2.\\ \\ V a r\\left[X_{t}\\right]=\\left(M_{t}-e^{-2\\sqrt{\\frac{2}{\\pi}}\\Sigma^{-\\frac{1}{2}}H t}\\right)X_{0}^{2}+\\frac{\\eta}{2}\\left(\\sqrt{\\frac{2}{\\pi}}I_{d}+\\frac{\\eta}{\\pi}H\\right)^{-1}H^{-1}\\Sigma^{\\frac{1}{2}}\\left(I_{d}-M_{t}\\right)\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "709 Proof of Lemma C.12. The proof is banal: The expected value derivation leverages the martingale   \n710 property of the Brownian motion while that of the variance uses the Ito Isomerty. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "711 Lemma C.13. Let $H=\\operatorname{diag}(\\lambda_{1},\\dots,\\lambda_{d})$ . Then, $\\mathbb{E}\\left[\\frac{X_{t}^{\\top}H X_{t}}{2}\\right]$ is equal to ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{d}\\frac{\\lambda_{i}(X_{0}^{i})^{2}}{2}e^{-2\\lambda_{i}\\left(\\sqrt{\\frac{2}{\\pi}}\\frac{1}{\\sigma_{i}}+\\frac{\\lambda_{i}\\eta}{\\pi\\sigma_{i}^{2}}\\right)t}+\\frac{\\eta}{4\\left(\\sqrt{\\frac{2}{\\pi}}\\frac{1}{\\sigma_{i}}+\\frac{\\lambda_{i}\\eta}{\\pi\\sigma_{i}^{2}}\\right)}\\left(1-e^{-2\\lambda_{i}\\left(\\sqrt{\\frac{2}{\\pi}}\\frac{1}{\\sigma_{i}}+\\frac{\\lambda_{i}\\eta}{\\pi\\sigma_{i}^{2}}\\right)t}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "712 Proof of Lemma C.13. Since the matrix $H$ is diagonal, we focus on a single component. We apply   \n713 the Ito Lemma to $\\frac{\\lambda_{i}(X_{t}^{i})^{2}}{2}$ ", "page_idx": 24}, {"type": "equation", "text": "$$\nd\\left(\\frac{\\lambda_{i}(X_{t}^{i})^{2}}{2}\\right)=-2\\sqrt{\\frac{2}{\\pi}}\\frac{\\lambda_{i}}{\\sigma_{i}}\\frac{\\lambda_{i}(X_{t}^{i})^{2}}{2}d t+\\frac{\\eta\\lambda_{i}}{2}d t-\\frac{2\\lambda_{i}^{2}\\eta}{\\pi\\sigma_{i}^{2}}\\frac{\\lambda_{i}(X_{t}^{i})^{2}}{2}+\\mathcal{O}(\\mathrm{Noise}),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "714 which implies that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\frac{\\lambda_{i}(X_{t}^{i})^{2}}{2}\\right]=\\frac{\\lambda_{i}(X_{0}^{i})^{2}}{2}e^{-2\\left(\\sqrt{\\frac{2}{\\pi}}\\frac{\\lambda_{i}}{\\sigma_{i}}+\\frac{\\lambda_{i}^{2}\\eta}{\\pi\\sigma_{i}^{2}}\\right)t}+\\frac{\\eta}{4\\left(\\sqrt{\\frac{2}{\\pi}}\\frac{1}{\\sigma_{i}}+\\frac{\\lambda_{i}\\eta}{\\pi\\sigma_{i}^{2}}\\right)}\\left(1-e^{-2\\left(\\sqrt{\\frac{2}{\\pi}}\\frac{\\lambda_{i}}{\\sigma_{i}}+\\frac{\\lambda_{i}^{2}\\eta}{\\pi\\sigma_{i}^{2}}\\right)t}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "715 Therefore, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\natural\\left[\\frac{X_{t}^{\\top}H X_{t}}{2}\\right]=\\sum_{i=1}^{d}\\frac{\\lambda_{i}(X_{0}^{i})^{2}}{2}e^{-2\\lambda_{i}\\left(\\sqrt{\\frac{2}{\\pi}}\\frac{1}{\\sigma_{i}}+\\frac{\\lambda_{i}\\eta}{\\pi_{i}^{2}}\\right)t}+\\frac{\\eta}{4\\left(\\sqrt{\\frac{2}{\\pi}}\\frac{1}{\\sigma_{i}}+\\frac{\\lambda_{i}\\eta}{\\pi\\sigma_{i}^{2}}\\right)}\\left(1-e^{-2\\lambda_{i}\\left(\\sqrt{\\frac{2}{\\pi}}\\frac{1}{\\sigma_{i}}+\\frac{\\lambda_{i}\\eta}{\\pi\\sigma_{i}^{2}}\\right)t}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "717 Lemma C.14. Under the assumptions of Corollary C.8, where $\\nabla f_{\\gamma}(x)=\\nabla f(x)+\\sqrt{\\Sigma}U$ , we have   \n718 that the dynamics of SignSGD in Phase 3 is: ", "page_idx": 25}, {"type": "equation", "text": "$$\nd X_{t}=-\\sqrt{\\frac{1}{2}}\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})d t+\\sqrt{\\eta}\\sqrt{I_{d}-\\frac{1}{2}\\operatorname{diag}\\left(\\Sigma^{-\\frac{1}{2}}\\nabla f(X_{t})\\right)^{2}}d W_{t}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "719 Proof of lemma C.14. We apply Eq. (34) with $\\nu=2$ and linearly approximate $\\Xi(x)$ as $|x|<1$ ,   \n720 where 2\u039e(x) \u223c\u221ax2. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "721 C.3 Formal derivation - RMSprop ", "text_level": 1, "page_idx": 25}, {"type": "image", "img_path": "nfC1OA6NeE/tmp/3c7c50e79c1a9444240750c732bdb9579b4691c9ee9e865590592aea653d6852.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 7: The first two subfigures on the left compare our SDE, that from Malladi et al. (2022), and RMSprop in terms of trajectories and $f(x)$ , respectively, for a convex quadratic function. The others subfigures do the same for an embedded saddle and one clearly observes that our derived SDE better matches RMSprop. ", "page_idx": 25}, {"type": "text", "text": "722 In this subsection, we provide our formal derivation of an SDE model for RMSprop. Let us consider   \n723 the stochastic process $\\stackrel{\\cdot}{L}_{t}:=(X_{t},V_{t})\\in\\mathbb R^{d}\\times\\mathbb R^{d}$ defined as the solution of ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d X_{t}=-P_{t}^{-1}(\\nabla f(X_{t})d t+\\sqrt{\\eta}\\Sigma(X_{t})^{\\frac{1}{2}}d W_{t})}\\\\ &{d V_{t}=\\rho((\\nabla f(X_{t}))^{2}+\\mathrm{diag}(\\Sigma(X_{t}))-V_{t}))d t,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "724 where $\\beta=1-\\eta\\rho,\\rho=\\mathcal{O}(1)$ , and $P_{t}:=\\mathrm{diag}\\left(V_{t}\\right)^{\\frac{1}{2}}+\\epsilon I_{d}$ . ", "page_idx": 25}, {"type": "text", "text": "725 Remark C.15. We observe that the term in blue is the only difference w.r.t. the SDE derived in   \n726 (Malladi et al., 2022) (see Theorem D.2): This is extremely relevant when the gradient size is not   \n727 negligible. Figure 7 shows the comparison between our SDE, the one derived in (Malladi et al., 2022),   \n728 and RMSprop itself: It is clear that even on simple landscapes, our SDE matches the algorithm much   \n729 better. Importantly, one can observe that the SDE derived in (Malladi et al., 2022) is only slightly   \n730 worse than ours at the end of the dynamics: As we show in Lemma C.17, Theorem D.2 is a corollary   \n731 of Theorem C.16 when $\\nabla f(x)\\,=\\,\\mathcal{O}(\\sqrt{\\eta})$ : It only describes the dynamics where the gradient is   \n732 vanishing. In Figure 8, we compare the two SDEs in question with RMSprop on an MLP, a CNN, a   \n733 ResNet, and a Transformer: Our SDE exhibits a superior description of the dynamics.   \n734 The following theorem guarantees that such a process is a 1-order SDE of the discrete-time algorithm   \n735 of RMSprop ", "page_idx": 25}, {"type": "image", "img_path": "nfC1OA6NeE/tmp/8c1114a8b01c0ad8ac61651bb78f1719aa1d2587ddf25e91ce8739ddc6d178f4.jpg", "img_caption": ["Figure 8: We compare our SDE, that from Malladi et al. (2022), and RMSprop in terms of $f(x)$ : The first is an MLP on the Breast Cancer dataset, the second a CNN on MNIST, the third a Transformer on MNIST, and the last a ResNet on CIFAR-10: Ours match the algorithms better. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{x_{k+1}}={x_{k}}-\\eta\\frac{\\nabla f_{\\gamma_{k}}(x_{k})}{\\sqrt{v_{k+1}}+\\epsilon I_{d}}}\\\\ {{v_{k+1}}=\\beta v_{k}+(1-\\beta)\\left(\\nabla f_{\\gamma_{k}}(x_{k})\\right)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "736 with $(x_{0},v_{0})\\in\\mathbb R^{d}\\times\\mathbb R^{d}$ , $\\eta\\in\\mathbb{R}^{>0}$ is the step size, $\\beta=1-\\rho\\eta$ for $\\rho=\\mathcal{O}(1)$ , the mini-batches $\\{\\gamma_{k}\\}$   \n737 are modelled as i.i.d. random variables uniformly distributed on $\\{1,\\cdot\\cdot\\cdot,N\\}$ , and of size $B\\geq1$ . ", "page_idx": 26}, {"type": "text", "text": "Theorem C.16 (Stochastic modified equations). Let $0<\\eta<1,T>0$ and set $N=\\lfloor T/\\eta\\rfloor$ . Let $l_{k}:=(x_{k},v_{k})\\in\\mathbb{R}^{d}\\times\\mathbb{R}^{d},0\\le k\\le N$ denote a sequence of RMSprop iterations defined by Eq. (88). Consider the stochastic process $L_{t}$ defined in $E q$ . (86) and fix some test function $g\\in G$ and suppose that $g$ and its partial derivatives up to order 6 belong to $G$ . Then, under Assumption C.2 and $\\rho=\\mathcal{O}(1)$ there exists a constant $C>0$ independent of $\\eta$ such that for all $k=0,1,\\ldots,N$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathbb{E}g\\left(L_{k\\eta}\\right)-\\mathbb{E}g\\left(l_{k}\\right)|\\leq C\\eta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "That is, the SDE (86) is an order 1 weak approximation of the RMSprop iterations (88). ", "page_idx": 26}, {"type": "text", "text": "738 ", "page_idx": 26}, {"type": "text", "text": "739 Proof. The proof is virtually identical to that of Theorem C.5. Therefore, we only report the key   \n740 steps necessary to conclude the thesis. First of all, we observe that since $\\beta=1-\\eta\\rho$ ", "page_idx": 26}, {"type": "equation", "text": "$$\nv_{k+1}-v_{k}=-\\eta\\rho\\left(v_{k}-\\left(\\nabla f_{\\gamma_{k}}(x_{k})\\right)^{2}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "741 Then, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{v_{k+1}}}=\\sqrt{\\frac{v_{k}}{v_{k+1}}\\frac{1}{v_{k}}}=\\sqrt{\\frac{v_{k+1}+\\mathcal{O}(\\eta)}{v_{k+1}}\\frac{1}{v_{k}}}=\\sqrt{1+\\frac{\\mathcal{O}(\\eta)}{v_{k+1}}}\\sqrt{\\frac{1}{v_{k}}}\\sim\\sqrt{\\frac{1}{v_{k}}}(1+\\mathcal{O}(\\eta)).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "742 Therefore, we work with the following algorithm as all the approximations below only carry an   \n743 additional error of order ${\\mathcal{O}}(\\eta^{2})$ , which we can ignore. Therefore, we have that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{k+1}-x_{k}=-\\eta\\frac{\\nabla f_{\\gamma_{k}}\\left(x_{k}\\right)}{\\sqrt{v_{k}}+\\epsilon I_{d}}}\\\\ &{v_{k}-v_{k-1}=-\\eta\\rho\\left(v_{k-1}-\\left(\\nabla f_{\\gamma_{k-1}}\\left(x_{k-1}\\right)\\right)^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "744 Therefore, if $\\nabla f_{\\gamma_{j}}(x_{j})=\\nabla f(x_{j})+Z_{j}(x_{j}),\\mathbb{E}[Z_{j}(x_{j})]=0$ , and $C o v(Z_{j}(x_{j}))=\\Sigma(x_{j})$ ", "page_idx": 26}, {"type": "text", "text": "745 ", "page_idx": 26}, {"type": "text", "text": "746 ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1.~\\mathbb{E}[x_{k+1}-x_{k}]=-\\eta\\,\\mathrm{diag}(v_{k}+\\epsilon I_{d})^{-\\frac{1}{2}}\\nabla f(x_{k})\\,;}\\\\ &{2.~\\mathbb{E}[v_{k}-v_{k-1}]=\\eta\\rho\\left[(\\nabla f(x_{k-1}))^{2}+\\mathrm{diag}(\\Sigma(x_{k}))-v_{k-1}\\right]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "747 Then, we have that if $\\begin{array}{r}{\\Phi_{k}:=\\frac{\\nabla f(x_{k})}{\\sqrt{v_{k}}+\\epsilon I_{d}}-\\frac{\\nabla f_{\\gamma_{k}}(x_{k})}{\\sqrt{v_{k}}+\\epsilon I_{d}}}\\end{array}$ ", "page_idx": 27}, {"type": "text", "text": "1. ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[(x_{k+1}-x_{k})(x_{k+1}-x_{k})^{\\top}]=\\mathbb{E}[(x_{k+1}-x_{k})]\\mathbb{E}[(x_{k+1}-x_{k})]^{\\top}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\eta^{2}\\mathbb{E}\\left[\\left(\\Phi_{k}\\right)(\\Phi_{k})^{\\top}\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\mathbb{E}[(x_{k+1}-x_{k})]\\mathbb{E}[(x_{k+1}-x_{k})]^{\\top}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\eta^{2}(\\mathrm{diag}(v_{k})+\\epsilon I_{d})^{-1}\\Sigma(x_{k});}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "748 ", "page_idx": 27}, {"type": "equation", "text": "$\\begin{array}{r}{\\mathbb{E}[(v_{k}-v_{k-1})(v_{k}-v_{k-1})^{\\top}]=\\mathbb{E}[(v_{k}-v_{k-1})]\\mathbb{E}[(v_{k}-v_{k-1})]^{\\top}+\\mathcal{O}(\\rho\\eta^{2});}\\end{array}$ ", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "749 ", "page_idx": 27}, {"type": "equation", "text": "$$\n3.\\ \\mathbb{E}[(x_{k+1}-x_{k})(v_{k}-v_{k-1})^{\\top}]=\\mathbb{E}[(x_{k+1}-x_{k})]\\mathbb{E}[(v_{k}-v_{k-1})^{\\top}]+0.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "750 Therefore ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d X_{t}=-P_{t}^{-1}(\\nabla f(X_{t})d t+\\sqrt{\\eta}\\Sigma(X_{t})^{\\frac{1}{2}}d W_{t})}\\\\ &{d V_{t}=\\rho(((\\nabla f(X_{t}))^{2}+\\mathrm{diag}(\\Sigma(X_{t}))-V_{t}))d t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "751 ", "page_idx": 27}, {"type": "text", "text": "752 Lemma C.17. If $(\\nabla f(x))^{2}=\\mathcal{O}(\\eta),$ , Theorem $D.2$ is a Corollary of Theorem C.16. ", "page_idx": 27}, {"type": "text", "text": "753 Proof. In the proof of Theorem C.16, one drops the term $\\eta(\\nabla f(x))^{2}$ as it is of order $\\eta^{2}$ . ", "page_idx": 27}, {"type": "text", "text": "754 Corollary C.18. Under the assumptions of Theorem C.16 with $\\Sigma(x)=\\sigma^{2}I_{d},\\,\\tilde{\\eta}=\\kappa\\eta,\\,\\tilde{B}=B\\delta,$ , and   \n755 $\\tilde{\\rho}=\\alpha\\rho$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d X_{t}=\\kappa\\operatorname{diag}(V_{t})^{-\\frac{1}{2}}\\left(-\\nabla f(X_{t})d t+\\frac{1}{\\sqrt{\\delta}}\\sqrt{\\frac{\\eta}{B}}\\sigma I_{d}d W_{t}\\right)}\\\\ &{d V_{t}=\\frac{\\alpha}{\\kappa}\\rho\\left((\\nabla f(X_{t}))^{2}+\\frac{\\sigma^{2}}{B\\delta}{\\bf1}-V_{t}\\right)d t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "756 Lemma C.19 (Scaling Rule at Convergence). Under the assumptions of Corollary C.18, $f$ is $\\mu$ -   \n757 strongly convex, ${\\mathcal{L}}_{\\tau}:=T r(\\nabla^{2}f(x))$ , and $(\\nabla f(x))^{2}=O(\\eta)$ , the asymptotic dynamics of the iterates   \n758 of RMSprop satisfies the classic scaling rule $\\kappa=\\sqrt\\delta$ because ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}[f(X_{t})-f(X_{*})]\\overset{t\\rightarrow\\infty}{\\leq}\\frac{\\eta\\sigma\\mathcal{L}_{\\tau}}{4\\mu\\sqrt{B}}\\frac{\\kappa}{\\sqrt{\\delta}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "759 By enforcing that the speed of $V_{t}$ matches that of $X_{t}$ , one needs $\\tilde{\\rho}\\,=\\,\\kappa^{2}\\rho,$ , which implies $\\tilde{\\beta}\\,=$   \n760 $\\stackrel{.}{1}-\\kappa^{\\stackrel{.}{2}}(1-\\stackrel{.}{\\beta})$ .   \n761 Proof of Lemma C.19. In order to recover the scaling of $\\beta$ , we enforce that the rate at which $V_{t}$   \n762 converges to its limit matches the speed of $X_{t}$ : We need $\\tilde{\\rho}=\\kappa^{2}\\rho$ , which recovers the classic scaling   \n763 $\\tilde{\\beta}=1-\\kappa^{2}(1-\\beta)$ . Additionally, since $(\\nabla f(x))^{2}=\\mathcal{O}(\\eta)$ we have that ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d X_{t}=\\kappa\\operatorname{diag}(V_{t})^{-\\frac{1}{2}}\\left(-\\nabla f(X_{t})d t+\\frac{1}{\\sqrt{\\delta}}\\sqrt{\\frac{\\eta}{B}}\\sigma I_{d}d W_{t}\\right)}\\\\ &{d V_{t}=\\kappa\\rho\\left(\\frac{\\sigma^{2}}{B\\delta}{\\bf1}-V_{t}\\right)d t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "764 Therefore, $\\begin{array}{r}{V_{t}\\stackrel{t\\rightarrow\\infty}{\\rightarrow}\\frac{\\sigma^{2}}{B\\delta}{\\bf1}}\\end{array}$ , meaning that under these conditions: ", "page_idx": 27}, {"type": "equation", "text": "$$\nd X_{t}=-\\frac{\\sqrt{B\\delta}\\kappa}{\\sigma}\\nabla f(X_{t})d t+\\kappa\\sqrt{\\eta}I_{d}d W_{t},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "765 which satisfies the following for $\\mu$ -strongly convex functions ", "page_idx": 27}, {"type": "equation", "text": "$$\nd\\mathbb{E}[f(X_{t})-f(X_{*})]\\leq-2\\kappa\\mu\\frac{\\sqrt{B\\delta}}{\\sigma}\\mathbb{E}[f(X_{t})-f(X_{*})]d t+\\frac{\\kappa^{2}\\eta\\mathcal{L}_{\\tau}}{2}d t,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "766 meaning that $\\begin{array}{r}{\\mathbb{E}[f(X_{t})-f(X_{*})]\\stackrel{t\\rightarrow\\infty}{\\leq}\\frac{\\eta\\sigma\\mathcal{L}_{\\tau}}{4\\mu\\sqrt{B}}\\frac{\\kappa}{\\sqrt{\\delta}}.}\\end{array}$ ", "page_idx": 28}, {"type": "text", "text": "767 Since the asymptotic the loss is $\\frac{\\eta}{2}\\,\\frac{\\mathcal{L}_{\\tau}\\sigma}{2\\mu\\sqrt{B}}\\,\\frac{\\kappa}{\\sqrt{\\delta}}$ does not depend on $\\kappa$ and $\\delta$ if $\\begin{array}{r}{\\frac{\\kappa}{\\sqrt{\\delta}}=1}\\end{array}$ , we recover the   \n768 classic scaling rule. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "769 Remark: Under the same conditions, SGD satisfies ", "page_idx": 28}, {"type": "equation", "text": "$$\nd X_{t}=-\\kappa\\nabla f(X_{t})d t+\\kappa\\frac{1}{\\sqrt{\\delta}}\\sqrt{\\frac{\\eta}{B}}\\sigma I_{d}d W_{t}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "770 and therefore ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}[f(X_{t})-f(X_{*})]\\leq(f(X_{0})-f(X_{*}))e^{-2\\mu\\kappa t}+\\frac{\\eta}{2}\\frac{\\mathcal{L}_{\\tau}\\sigma^{2}}{2\\mu B}\\frac{\\kappa}{\\delta}\\left(1-e^{-2\\mu\\kappa t}\\right),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "771 meaning that asymptotically the loss is \u03b72L2\u03c4\u00b5 \u03c3B which does not depend on $\\kappa$ and $\\delta$ if $\\begin{array}{r}{\\frac{\\kappa}{\\delta}=1}\\end{array}$ . ", "page_idx": 28}, {"type": "text", "text": "772 Lemma C.20. For $\\begin{array}{r}{f(x):=\\frac{x^{\\top}H x}{2}}\\end{array}$ , the stationary distribution of RMSprop is $(\\mathbb{E}[X_{\\infty}]],C o v(X_{\\infty}))=$   \n773 0, \u03b72\u03a312 H\u22121 . ", "page_idx": 28}, {"type": "text", "text": "774 Proof. As $(\\nabla f(x))^{2}=O(\\eta)$ and $t\\to\\infty$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\nd X_{t}=-\\Sigma^{-\\frac{1}{2}}H X_{t}d t+\\sqrt{\\eta}I_{d}d W_{t}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "775 which implies that ", "page_idx": 28}, {"type": "equation", "text": "$$\nX_{t}=e^{-\\Sigma^{-\\frac{1}{2}}H t}\\left(X_{0}+\\sqrt{\\eta}\\int_{0}^{t}e^{\\Sigma^{-\\frac{1}{2}}H s}d W_{s}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "776 The thesis follows from the martingale property of Brownian motion and the It\u00f4 isometry. ", "page_idx": 28}, {"type": "text", "text": "777 C.4 RMSpropW ", "text_level": 1, "page_idx": 28}, {"type": "image", "img_path": "nfC1OA6NeE/tmp/17a0b4031984017ad8fa6eed480d1dad2863b630d06a18eef84cfd01f814dc11.jpg", "img_caption": ["Figure 9: The first two represent the comparison between AdamW and its SDE in terms of $f(x)$ . The other two do the same for RMSpropW. In both cases, the first is an MLP on the Breast Cancer Dataset and the second a CNN on MNIST: Our SDEs match the respective optimizers. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "778 In this subsection, we derive the SDE of RMSpropW defined as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{x_{k+1}=x_{k}-\\eta\\frac{\\nabla f_{\\gamma_{k}}(x_{k})}{\\sqrt{v_{k+1}}+\\epsilon I_{d}}-\\eta\\gamma x_{k}}}\\\\ {{v_{k+1}=\\beta v_{k}+(1-\\beta)\\left(\\nabla f_{\\gamma_{k}}(x_{k})\\right)^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "779 with $(x_{0},v_{0})\\in\\mathbb{R}^{d}\\times\\mathbb{R}^{d}$ , $\\eta\\in\\mathbb{R}^{>0}$ is the step size, $\\beta=1\\!-\\!\\rho\\eta$ for $\\rho=\\mathcal{O}(1),\\gamma>0$ , the mini-batches   \n780 $\\{\\gamma_{k}\\}$ are modelled as i.i.d. random variables uniformly distributed on $\\{1,\\cdot\\cdot\\cdot,N\\}$ , and of size $B\\geq1$ .   \n781 Theorem C.21. Under the same assumptions as Theorem C.16, the SDE of RMSpropW is ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d X_{t}=-P_{t}^{-1}(\\nabla f(X_{t})d t+\\sqrt{\\eta}\\Sigma(X_{t})^{\\frac{1}{2}}d W_{t})-\\gamma X_{t}d t}\\\\ &{d V_{t}=\\rho((\\nabla f(X_{t}))^{2}+\\mathrm{diag}(\\Sigma(X_{t}))-V_{t}))d t,}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "782 where $\\beta=1-\\eta\\rho,\\,\\rho=\\mathcal{O}(1),\\,\\gamma>0,$ , and $P_{t}:=\\mathrm{diag}\\left(V_{t}\\right)^{\\frac{1}{2}}+\\epsilon I_{d}$ . ", "page_idx": 28}, {"type": "text", "text": "783 Proof. The proof is the same as the of Theorem C.16 and the only difference is that $\\eta\\gamma x_{k}$ is   \n784 approximated with $\\gamma X_{t}d t$ . \u53e3 ", "page_idx": 29}, {"type": "text", "text": "785 Figure 4 and Figure 9 validate this result on a variety of architectures and datasets. ", "page_idx": 29}, {"type": "text", "text": "786 Corollary C.22. Under the assumptions of Theorem $C.2l$ with $\\Sigma(x)=\\sigma^{2}I_{d},\\,\\tilde{\\eta}=\\kappa\\eta,\\,\\tilde{B}=B\\delta,$ , and   \n787 $\\tilde{\\rho}=\\alpha\\rho$ , and $\\tilde{\\gamma}=\\xi\\gamma$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{d X_{t}=\\kappa\\operatorname{diag}(V_{t})^{-\\frac{1}{2}}\\left(-\\nabla f(X_{t})d t+\\displaystyle\\frac{1}{\\sqrt{\\delta}}\\sqrt{\\frac{\\eta}{B}}\\sigma I_{d}d W_{t}\\right)-\\xi\\gamma\\kappa X_{t}d t}\\\\ {d V_{t}=\\displaystyle\\frac{\\alpha}{\\kappa}\\rho\\left((\\nabla f(X_{t}))^{2}+\\frac{\\sigma^{2}}{B\\delta}{\\bf1}-V_{t}\\right)d t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "788 Lemma C.23 (Scaling Rule at Convergence). Under the assumptions of Corollary C.22, $f$ is $\\mu$ -   \n789 strongly convex and $L$ -smooth, ${\\mathcal{L}}_{\\tau}:=T{\\check{r}}(\\nabla^{2}f(x))$ , and $(\\nabla f(x))^{2}\\,{\\overset{}{=}}\\,{\\mathcal{O}}(\\eta)$ , the asymptotic dynamics   \n790 of the iterates of RMSpropW satisfies the novel scaling rule if $\\kappa=\\sqrt\\delta$ and $\\xi=\\kappa$ because ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}[f(X_{t})-f(X_{*})]\\stackrel{t\\rightarrow\\infty}{\\leq}\\frac{\\eta\\mathcal{L}_{\\tau}\\sigma L}{2}\\frac{\\kappa}{2\\mu\\sqrt{B\\delta}L+\\sigma\\xi\\gamma(L+\\mu)}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "791 By enforcing that the speed of $V_{t}$ matches that of $X_{t}$ , one needs $\\tilde{\\rho}\\,=\\,\\kappa^{2}\\rho,$ , which implies $\\tilde{\\beta}\\,=$   \n792 $1-\\kappa^{2}(1-\\Bar{\\beta})$ .   \n793 Proof of Lemma C.23. In order to recover the scaling of $\\beta$ , we enforce that the rate at which $V_{t}$   \n794 converges to its limit matches the speed of $X_{t}$ : We need $\\tilde{\\rho}=\\kappa^{2}\\rho$ , which recovers the classic scaling   \n795 $\\tilde{\\beta}=1-\\kappa^{2}(1-\\beta)$ . Additionally, since $(\\nabla f(x))^{2}=\\mathcal{O}(\\eta)$ we have that ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d X_{t}=\\kappa\\operatorname{diag}(V_{t})^{-\\frac{1}{2}}\\left(-\\nabla f(X_{t})d t+\\frac{1}{\\sqrt{\\delta}}\\sqrt{\\frac{\\eta}{B}}\\sigma I_{d}d W_{t}\\right)-\\kappa\\xi\\gamma X_{t}d t}\\\\ &{d V_{t}=\\kappa\\rho\\left(\\frac{\\sigma^{2}}{B\\delta}{\\bf1}-V_{t}\\right)d t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "796 Therefore, $\\begin{array}{r}{V_{t}\\stackrel{t\\rightarrow\\infty}{\\rightarrow}\\frac{\\sigma^{2}}{B\\delta}{\\bf1}}\\end{array}$ , meaning that under these conditions: ", "page_idx": 29}, {"type": "equation", "text": "$$\nd X_{t}=-\\frac{\\sqrt{B\\delta}\\kappa}{\\sigma}\\nabla f(X_{t})d t+\\kappa\\sqrt{\\eta}I_{d}d W_{t}-\\kappa\\xi\\gamma X_{t}d t,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "797 which satisfies the following for $\\mu$ -strongly convex and $L$ -smooth functions ", "page_idx": 29}, {"type": "equation", "text": "$$\nd\\mathbb{E}[f(X_{t})-f(X_{*})]\\leq\\kappa\\left(2\\mu\\frac{\\sqrt{B\\delta}}{\\sigma}+\\xi\\gamma\\left(1+\\frac{\\mu}{L}\\right)\\right)\\mathbb{E}[f(X_{t})-f(X_{*})]d t+\\frac{\\kappa^{2}\\eta\\mathcal{L}_{\\tau}}{2}d t,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "798 meaning that $\\begin{array}{r}{\\mathbb{E}[f(X_{t})-f(X_{*})]\\stackrel{t\\rightarrow\\infty}{\\leq}\\frac{\\eta\\mathcal{L}_{\\tau}\\sigma L}{2}\\frac{\\kappa}{2\\mu\\sqrt{B\\delta}L+\\sigma\\xi\\gamma(L+\\mu)}.}\\end{array}$ ", "page_idx": 29}, {"type": "text", "text": "799 Since the asymptotic the loss \u03b7L\u03c42 \u03c3L $\\frac{\\eta\\mathcal{L}_{\\tau}\\sigma L}{2}\\frac{\\kappa}{2\\mu\\sqrt{B\\delta}L\\!+\\!\\sigma\\xi\\gamma(L\\!+\\!\\mu)}$ does not depend on $\\kappa$ and $\\delta$ and $\\xi$ if $\\kappa=\\xi=$   \n800 $\\sqrt{\\delta}$ , we recover the novel scaling rule.   \n801 Lemma C.24. For $\\begin{array}{r l r}{f(x)}&{{}:=}&{\\frac{x^{\\top}H x}{2}}\\end{array}$ , the stationary distribution of RMSpropW is   \n802 $\\begin{array}{r}{(\\mathbb{E}[X_{\\infty}]],C o v(X_{\\infty}))=\\Big(0,\\frac{\\eta}{2}(H\\Sigma^{-\\frac{1}{2}}+\\gamma I_{d})^{-1}\\Big).}\\end{array}$ ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "803 Proof. As $(\\nabla f(x))^{2}=O(\\eta)$ and $t\\to\\infty$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\nd X_{t}=-\\Sigma^{-\\frac{1}{2}}H X_{t}d t+\\sqrt{\\eta}I_{d}d W_{t}-\\gamma X_{t}d t\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "804 which implies that ", "page_idx": 29}, {"type": "equation", "text": "$$\nX_{t}=e^{-(\\Sigma^{-\\frac{1}{2}}H+\\gamma I_{d})t}\\left(X_{0}+\\sqrt{\\eta}\\int_{0}^{t}e^{(\\Sigma^{-\\frac{1}{2}}H+\\gamma I_{d})s}d W_{s}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "805 The thesis follows from the martingale property of Brownian motion and the It\u00f4 isometry. ", "page_idx": 29}, {"type": "image", "img_path": "nfC1OA6NeE/tmp/3584803f202032ade6dbb83e3a18d171db2742120749c31f52ccef0dc5cb5a66.jpg", "img_caption": ["Figure 10: The first two on the left compare our SDE, that from Malladi et al. (2022), and Adam in terms of trajectories and $f(x)$ , respectively, for a convex quadratic function. The others do the same for an embedded saddle: Ours clearly matches Adam better. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "806 C.5 Formal derivation - Adam ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "807 In this subsection, we provide our formal derivation of an SDE model for Adam. Let us consider the   \n808 stochastic process $L_{t}:=(X_{t},M_{t},V_{t})\\in\\mathbb R^{d}\\times\\mathbb R^{d}\\times\\mathbb R^{d}$ defined as the solution of ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d X_{t}=-\\frac{\\sqrt{\\gamma_{2}(t)}}{\\gamma_{1}(t)}P_{t}^{-1}(M_{t}+\\eta\\rho_{1}\\left(\\nabla f\\left(X_{t}\\right)-M_{t}\\right))d t}\\\\ &{d M_{t}=\\rho_{1}\\left(\\nabla f\\left(X_{t}\\right)-M_{t}\\right)d t+\\sqrt{\\eta}\\rho_{1}\\Sigma^{1/2}\\left(X_{t}\\right)d W_{t}}\\\\ &{\\;d V_{t}=\\rho_{2}\\left((\\nabla f(X_{t}))^{2}+\\mathrm{diag}\\left(\\Sigma\\left(X_{t}\\right)\\right)-V_{t}\\right)d t,}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "809 where\u221a $\\beta_{i}\\,=\\,1\\,-\\,\\eta\\rho_{i}$ , $\\gamma_{i}(t)\\,=\\,1\\,-\\,e^{-\\rho_{i}t}$ , $\\rho_{1}\\,=\\,\\mathcal{O}(\\eta^{-\\zeta})$ s.t. $\\zeta\\,\\in\\,(0,1)$ , $\\rho_{2}\\,=\\,\\mathcal{O}(1)$ , and $P_{t}\\,=$   \n810 diag $\\sqrt{V_{t}}+\\epsilon\\sqrt{\\gamma_{2}(t)}I_{d}$ .   \n811 Remark C.25. The terms in purple and in blue are the two differences w.r.t. that of (Malladi et al.,   \n812 2022) which is reported in Theorem D.5. The first appears because we assume realistic values of $\\beta_{1}$   \n813 while the second appears because we allow the gradient size to be non-negligible. For two simple   \n814 landscapes, Figure 10 compares our SDE and that of Malladi et al. (2022) with Adam: In both   \n815 cases, the first part of the dynamics is perfectly represented only by our SDE. While the discrepancy   \n816 between the SDE of (Malladi et al., 2022) and Adam is asymptotically negligible in the convex   \n817 setting, we observe that in the nonconvex case, it converges to a different local minimum than ours   \n818 and of Adam. Finally, Theorem D.5 is a corollary of ours when $(\\nabla f(x))^{2}=O(\\eta)$ and $\\rho_{1}=\\mathcal{O}(1)$ :   \n819 It only describes the dynamics where the gradient to noise ratio is vanishing and only for unrealistic   \n820 values of $\\beta_{1}=1-\\eta\\rho_{1}$ . In Figure 11, we compare the dynamics of our SDE, that of Malladi et al.   \n821 (2022), and Adam on an MLP, a CNN, a ResNet, and a Transformer. One can clearly see that our   \n822 SDE more accurately captures the dynamics. Details on these experiments are available in Appendix   \n823 F.   \n824 The following theorem guarantees that such a process is a 1-order SDE of the discrete-time algorithm   \n825 of Adam ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v_{k+1}=\\beta_{2}v_{k}+(1-\\beta_{2})\\left(\\nabla f_{\\gamma_{k}}(x_{k})\\right)^{2}}\\\\ &{m_{k+1}=\\beta_{1}m_{k}+(1-\\beta_{1})\\nabla f_{\\gamma_{k}}(x_{k})}\\\\ &{\\hat{m}_{k}=m_{k}\\left(1-\\beta_{1}^{k}\\right)^{-1}}\\\\ &{\\hat{v}_{k}=v_{k}\\left(1-\\beta_{2}^{k}\\right)^{-1}}\\\\ &{x_{k+1}=x_{k}-\\eta\\frac{\\hat{m}_{k+1}}{\\sqrt{\\hat{v}_{k+1}}+\\epsilon I_{d}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "826 with $(x_{0},m_{0},v_{0})\\in\\mathbb{R}^{d}\\times\\mathbb{R}^{d}\\times\\mathbb{R}^{d}$ , $\\eta\\in\\mathbb{R}^{>0}$ is the step size, $\\beta_{i}=1-\\rho_{i}\\eta$ for $\\rho_{1}=\\mathcal{O}(\\eta^{-\\zeta})$ s.t.   \n827 $\\zeta\\,\\in\\,(0,1)$ , $\\rho_{2}=\\mathcal{O}(1)$ , the mini-batches $\\{\\gamma_{k}\\}$ are modelled as i.i.d. random variables uniformly   \n828 distributed on $\\{1,\\cdot\\cdot\\cdot,N\\}$ , and of size $B\\geq1$ . ", "page_idx": 30}, {"type": "image", "img_path": "nfC1OA6NeE/tmp/9db8b2b40d889471a9afdb93d51b96d943c54f242c8e3c39effd01e642b43ce4.jpg", "img_caption": ["Figure 11: We compare our SDE, that from Malladi et al. (2022), and Adam in terms of $f(x)$ : The first is an MLP on the Breast Cancer dataset, the second a CNN on MNIST, the third a Transformer on MNIST, and the last a ResNet on CIFAR-10: Ours match the algorithms better. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Theorem C.26 (Stochastic modified equations). Let $0<\\eta<1,T>0$ and set $N=\\lfloor T/\\eta\\rfloor$ . Let $l_{k}:=(x_{k},m_{k},v_{k})\\in\\mathbb{R}^{d}\\times\\mathbb{R}^{d}\\times\\mathbb{R}^{d}$ , $0\\leq k\\leq N$ denote a sequence of Adam iterations defined by Eq. (127). Consider the stochastic process $L_{t}$ defined in Eq. (124) and fix some test function $g\\in G$ and suppose that $g$ and its partial derivatives up to order $^{6}$ belong to $G$ . Then, under Assumption $\\bar{C}.2\\;\\rho_{1}=\\bar{O(\\eta^{-\\zeta})}$ s.t. $\\zeta\\in(0,1)$ , while $\\rho_{2}=\\mathcal{O}(1)$ , there exists $a$ constant $C>0$ independent of \u03b7 such that for all $k=0,1,\\ldots,N$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathbb{E}g\\left(L_{k\\eta}\\right)-\\mathbb{E}g\\left(l_{k}\\right)|\\leq C\\eta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "That is, the SDE (124) is an order 1 weak approximation of the Adam iterations (127). ", "page_idx": 31}, {"type": "text", "text": "829 ", "page_idx": 31}, {"type": "text", "text": "830 Proof. The proof is virtually identical to that of Theorem C.5. Therefore, we only report the key   \n831 steps necessary to conclude the thesis. First of all, we observe that since $\\beta_{1}=1-\\eta\\rho_{1}$ ", "page_idx": 31}, {"type": "equation", "text": "$$\nv_{k+1}-v_{k}=-\\eta\\rho_{1}\\left(v_{k}-\\left(\\nabla f_{\\gamma_{k}}(x_{k})\\right)^{2}\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "832 Then, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{v_{k+1}}}=\\sqrt{\\frac{v_{k}}{v_{k+1}}\\frac{1}{v_{k}}}=\\sqrt{\\frac{v_{k+1}+\\mathcal{O}(\\eta)}{v_{k+1}}\\frac{1}{v_{k}}}=\\sqrt{1+\\frac{\\mathcal{O}(\\eta)}{v_{k+1}}}\\sqrt{\\frac{1}{v_{k}}}\\sim\\sqrt{\\frac{1}{v_{k}}}(1+\\mathcal{O}(\\eta)).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "833 Therefore, we work with the following algorithm as all approximations only carry an additional error   \n834 of order ${\\mathcal{O}}(\\eta^{2})$ , which we can ignore. Therefore, we have that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v_{k}-v_{k-1}=-\\eta\\rho_{2}\\left(v_{k-1}-\\left(\\nabla f_{\\gamma_{k-1}}(x_{k-1})\\right)^{2}\\right)}\\\\ &{m_{k+1}-m_{k}=-\\eta\\rho_{1}\\left(m_{k}-\\nabla f_{\\gamma_{k}}(x_{k})\\right)}\\\\ &{\\hat{m}_{k}=m_{k}\\left(1-\\beta_{1}^{k}\\right)^{-1}}\\\\ &{\\hat{v}_{k}=v_{k}\\left(1-\\beta_{1}^{k}\\right)^{-1}}\\\\ &{x_{k+1}-x_{k}=-\\frac{\\eta}{\\sqrt{v_{k}}+\\epsilon I_{d}}\\frac{\\sqrt{1-\\left(1-\\eta\\rho_{2}\\right)^{k}}}{1-\\left(1-\\eta\\rho_{1}\\right)^{k+1}}(m_{k}+\\eta\\rho_{1}\\big(\\nabla f_{\\gamma_{k}}(x_{k})-m_{k}\\big)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "835 Therefore, if $\\nabla f_{\\gamma_{j}}(x_{j})=\\nabla f(x_{j})+Z_{j}(x_{j})$ and $\\mathbb{E}[Z_{j}(x_{j})]=0$ , and $C o v(Z_{j}(x_{j}))=\\Sigma(x_{j})$ , we   \n836 have that ", "page_idx": 31}, {"type": "text", "text": "837 ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1.~\\mathbb{E}[v_{k}-v_{k-1}]=\\eta\\rho_{2}\\left[(\\nabla f(x_{k-1}))^{2}+\\mathrm{diag}(\\Sigma(x_{k}))-v_{k-1}\\right];}\\\\ &{2.~\\mathbb{E}[m_{k+1}-m_{k}]=\\eta\\rho_{1}\\left[\\nabla f(x_{k})-m_{k}\\right];}\\\\ &{3.~\\mathbb{E}[x_{k+1}-x_{k}]=-\\frac{\\eta}{\\sqrt{v_{k}}+\\epsilon I_{d}}\\frac{\\sqrt{1-(1-\\eta\\rho_{2})^{k}}}{1-(1-\\eta\\rho_{1})^{k+1}}(m_{k}+\\eta\\rho_{1}(\\nabla f(x_{k})-m_{k}))\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "838 ", "page_idx": 31}, {"type": "text", "text": "839 ", "page_idx": 31}, {"type": "text", "text": "840 Then, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}[(x_{k+1}-x_{k})(x_{k+1}-x_{k})^{\\top}]=\\mathbb{E}[(x_{k+1}-x_{k})]\\mathbb{E}[(x_{k+1}-x_{k})]^{\\top}+\\mathcal{O}(\\eta^{4}\\rho_{1}^{2});\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "842 ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}[(x_{k+1}-x_{k})(m_{k}-m_{k-1})^{\\top}]=\\mathbb{E}[(x_{k+1}-x_{k})]\\mathbb{E}[(m_{k}-m_{k-1})]^{\\top}+0;\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "843 ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}[(x_{k+1}-x_{k})(v_{k}-v_{k-1})^{\\top}]=\\mathbb{E}[(x_{k+1}-x_{k})]\\mathbb{E}[(v_{k}-v_{k-1})]^{\\top}+0;\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "844 ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\cdot\\ \\mathbb{E}[(v_{k}-v_{k-1})(v_{k}-v_{k-1})^{\\top}]=\\mathbb{E}[(v_{k}-v_{k-1})]\\mathbb{E}[(v_{k}-v_{k-1})]^{\\top}+{\\mathcal O}(\\eta^{2}\\rho_{2}^{2});\n$$", "text_format": "latex", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{S},\\ \\mathbb{E}[(m_{k}-m_{k-1})(m_{k}-m_{k-1})^{\\top}]=\\mathbb{E}[(m_{k}-m_{k-1})]\\mathbb{E}[(m_{k}-m_{k-1})]^{\\top}+\\eta^{2}\\rho_{1}^{2}\\Sigma(x_{k-1});}\\\\ &{\\mathbb{E},\\ \\mathbb{E}[(v_{k}-v_{k-1})(m_{k}-m_{k-1})^{\\top}]=\\mathbb{E}[(v_{k}-v_{k-1})]\\mathbb{E}[(m_{k}-m_{k-1})]^{\\top}+\\mathcal{O}(\\eta^{2}\\rho_{1}\\rho_{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "847 Since in real-world applications, $\\rho_{1}=\\mathcal{O}(\\eta^{-\\zeta})$ s.t. $\\zeta\\in(0,1)$ , while $\\rho_{2}=\\mathcal{O}(1)$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d X_{t}=-\\frac{\\sqrt{\\gamma_{2}(t)}}{\\gamma_{1}(t)}P_{t}^{-1}(M_{t}+\\eta\\rho_{1}\\left(\\nabla f\\left(X_{t}\\right)-M_{t}\\right))d t}\\\\ &{d M_{t}=\\rho_{1}\\left(\\nabla f\\left(X_{t}\\right)-M_{t}\\right)d t+\\sqrt{\\eta}\\rho_{1}\\Sigma^{1/2}\\left(X_{t}\\right)d W_{t}}\\\\ &{\\;d V_{t}=\\rho_{2}\\left((\\nabla f(X_{t}))^{2}+\\mathrm{diag}\\left(\\Sigma\\left(X_{t}\\right)\\right)-V_{t}\\right)d t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "848 where $\\beta_{i}=1-\\eta\\rho_{i}$ , $\\gamma_{i}(t)=1-e^{-\\rho_{i}t}$ , and $P_{t}=\\mathrm{diag}\\,\\sqrt{V_{t}}+\\epsilon\\sqrt{\\gamma_{2}(t)}I_{d}$ . ", "page_idx": 32}, {"type": "text", "text": "849 Corollary C.27. Under the assumptions of Theorem C.26 with $\\Sigma(x)=\\sigma^{2}I_{d},$ , $\\tilde{\\eta}=\\kappa\\eta$ , $\\tilde{B}=B\\delta$ ,   \n850 $\\tilde{\\rho}_{1}=\\alpha_{1}\\rho_{1}$ , and ${\\tilde{\\rho}}_{2}=\\alpha_{2}\\rho_{2}$ ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d X_{t}=-\\kappa\\frac{\\sqrt{\\gamma_{2}\\left(t\\right)}}{\\gamma_{1}\\left(t\\right)}P_{t}^{-1}(M_{t}+\\eta\\alpha_{1}\\rho_{1}\\left(\\nabla f\\left(X_{t}\\right)-M_{t}\\right))d t}\\\\ &{d M_{t}=\\frac{\\alpha_{1}\\rho_{1}}{\\kappa}\\left(\\nabla f\\left(X_{t}\\right)-M_{t}\\right)d t+\\sqrt{\\eta}\\frac{\\alpha_{1}\\rho_{1}}{\\kappa}\\frac{\\sigma}{\\sqrt{B\\delta}}I_{d}d W_{t}}\\\\ &{\\quad d V_{t}=\\frac{\\alpha_{2}\\rho_{2}}{\\kappa}\\left((\\nabla f(X_{t}))^{2}+\\frac{\\sigma^{2}}{B\\delta}I_{d}-V_{t}\\right)d t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "851 Lemma C.28. Under the assumptions of Corollary C.27, $f$ is $\\mu$ -strongly convex, ${\\mathcal{L}}_{\\tau}:=T r(\\nabla^{2}f(x))$ , 853 $(\\nabla f(x))^{2}=O(\\eta)$ $\\kappa=\\sqrt\\delta$ $\\begin{array}{r}{\\mathbb{E}[f(X_{t})]\\stackrel{t\\rightarrow\\infty}{\\leq}\\frac{\\eta\\sigma\\mathcal{L}_{\\tau}}{4\\sqrt{B}}\\frac{\\kappa}{\\sqrt{\\delta}}}\\end{array}$ .t the speed of $M_{t}$ and $V_{t}$ match that of $X_{t}$ $\\tilde{\\rho}_{i}=\\kappa^{2}\\rho_{i}$ $\\tilde{\\beta}_{i}=1-\\kappa^{2}(1-\\beta_{i})$ ", "page_idx": 32}, {"type": "text", "text": "855 Proof. First of all, we need to ensure that the relative speeds of $X_{t},M_{t}$ , and $V_{t}$ match. Therefore,   \n856 we select $\\alpha_{i}=\\kappa^{2}$ , which recovers the scaling rules for $\\tilde{\\beta}_{i}=1-\\kappa^{2}(1-\\beta_{i})$ . Then, recalling that   \n857 $(\\nabla f(x))^{2}=\\mathcal{O}(\\eta)$ , we have that as $t\\to\\infty$ , $\\begin{array}{r}{V_{t}\\rightarrow\\frac{\\sigma^{2}}{B\\delta}}\\end{array}$ , and $M_{t}\\to\\nabla f(X_{t})$ with high probability.   \n858 Therefore, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d X_{t}=-\\kappa\\frac{\\sqrt{B\\delta}}{\\sigma}\\nabla f(X_{t})d t}\\\\ &{d M_{t}=\\kappa\\sqrt{\\eta}\\rho_{1}\\frac{\\sigma}{\\sqrt{B\\delta}}d W_{t}}\\\\ &{d V_{t}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "859 Therefore, if $\\begin{array}{r}{H(X_{t},V_{t}):=f(X_{t})+\\frac{\\mathcal{L}_{\\tau}\\delta B}{\\rho^{2}\\sigma^{2}}\\frac{\\|M_{t}\\|_{2}^{2}}{2}}\\end{array}$ and $\\xi\\in(0,1)$ we have that by It\u00f4\u2019s lemma, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{d H(X_{t},V_{t})=-(\\nabla f(X_{t}))^{\\top}\\left(\\kappa\\frac{\\sqrt{B\\delta}}{\\sigma}\\nabla f(X_{t})\\right)d t+\\left(\\frac{C_{\\tau}\\delta B}{\\rho^{2}\\sigma^{2}}M_{t}\\right)\\kappa\\sqrt{\\eta}\\rho_{1}\\frac{\\sigma}{\\sqrt{B\\delta}}d W_{t}}&{}\\\\ {+\\frac{1}{2}\\left(\\frac{C_{\\tau}\\delta B}{\\rho^{2}\\sigma^{2}}\\right)\\kappa^{2}\\eta\\rho_{2}^{2}\\frac{\\sigma^{2}}{B\\delta}d t}&{}\\\\ {=-\\left(\\kappa\\frac{\\sqrt{B\\delta}}{\\sigma}\\right)\\|\\nabla f(X_{t})\\|_{2}^{2}d t+\\mathrm{Noise}+\\frac{\\kappa^{2}\\eta\\lambda}{2}d t}\\\\ {=-\\left(\\kappa\\frac{\\sqrt{B\\delta}}{\\sigma}\\right)\\left(\\xi\\|\\nabla f(X_{t})\\|_{2}^{2}+(1-\\xi)\\|\\nabla f(X_{t})\\|_{2}^{2}\\right)d t+\\mathrm{Noise}+\\frac{\\kappa^{2}\\eta\\lambda}{2}d t}\\\\ {\\leq-2\\kappa\\mu\\frac{\\sqrt{B\\delta}}{\\sigma}\\xi\\left(f(X_{t})+\\frac{1-\\xi}{\\mu\\xi}\\frac{\\|\\nabla f(X_{t})\\|_{2}^{2}}{2}\\right)d t+\\mathrm{Noise}+\\frac{\\kappa^{2}\\eta\\lambda}{2}d t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "860 Let us now select \u03be such that 1\u00b5\u2212\u03be\u03be = 861 . Since $\\begin{array}{r}{\\frac{1-\\xi}{\\mu\\xi}\\,=\\,\\frac{{\\mathcal L}_{\\tau}\\delta B}{\\rho^{2}\\sigma^{2}}\\,}\\end{array}$ L\u03c1\u03c42 \u03b4\u03c32B , this means that \u03be = that $\\begin{array}{r}{\\xi\\,=\\,\\frac{\\sigma^{2}\\rho^{2}}{\\sigma^{2}\\rho^{2}+\\mu\\mathcal{L}_{\\tau}\\sigma B}\\,\\in\\,(0,1)}\\end{array}$ and $\\textstyle{\\frac{1}{\\xi}}\\,=$ $1+\\mu\\frac{\\mathcal{L}_{\\tau}\\delta B}{\\rho^{2}\\sigma^{2}}$ $M_{t}\\to\\nabla f(X_{t})$ ", "page_idx": 33}, {"type": "equation", "text": "$$\nd H(X_{t},V_{t})\\leq-2\\kappa\\mu\\frac{\\sqrt{B\\delta}}{\\sigma}\\xi H(X_{t},V_{t})d t+\\frac{\\kappa^{2}\\eta\\lambda}{2}d t+\\mathrm{Noise}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "862 Therefore, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\frac{\\mathbb{E}[f(X_{t})]}{\\xi}=\\left(1+\\mu\\frac{\\mathcal{L}_{\\tau}\\delta B}{\\rho^{2}\\sigma^{2}}\\right)\\mathbb{E}[f(X_{t})]\\leq\\mathbb{E}[H(X_{t},V_{t})]\\overset{t\\rightarrow\\infty}{\\leq}\\frac{1}{\\xi}\\frac{\\eta\\sigma\\mathcal{L}_{\\tau}}{4\\mu\\sqrt{B}}\\frac{\\kappa}{\\sqrt{\\delta}},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "863 which implies that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{E}[f(X_{t})]\\stackrel{t\\rightarrow\\infty}{\\leq}\\frac{\\eta\\sigma\\mathcal{L}_{\\tau}}{4\\mu\\sqrt{B}}\\frac{\\kappa}{\\sqrt{\\delta}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "864 Analogously, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{E}[f(X_{t})-f(X_{*})]\\overset{t\\rightarrow\\infty}{\\leq}\\frac{\\eta\\sigma\\mathcal{L}_{\\tau}}{4\\mu\\sqrt{B}}\\frac{\\kappa}{\\sqrt{\\delta}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "865 which gives the square root scaling rule. ", "page_idx": 33}, {"type": "text", "text": "866 Lemma C.29. Under the assumptions of Corollary C.27, $\\begin{array}{r}{f(x)=\\frac{x^{\\top}H x}{2}}\\end{array}$ s.t. $H=\\mathrm{diag}(\\lambda_{1},\\cdot\\cdot\\cdot\\,,\\lambda_{d})$   \n867 and (\u2207f(x))2 = O(\u03b7), the dynamics of Adam implies that f(Xt) \u21924\u03b7\u221a\u03c3dB\u221a\u03ba\u03b4.   \n868 Proof. Recalling that $(\\nabla f(x))^{2}=\\mathcal{O}(\\eta)$ , we have that as $t\\to\\infty$ , $\\begin{array}{r}{V_{t}\\rightarrow\\frac{\\sigma^{2}}{B\\delta}}\\end{array}$ , and $M_{t}\\to\\lambda X_{t}$ with   \n869 high probability. Therefore, in the one-dimensional case ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle d X_{t}=-\\kappa\\frac{\\sqrt{B\\delta}}{\\sigma}\\lambda X_{t}d t}\\\\ {\\displaystyle d M_{t}=\\kappa\\sqrt{\\eta}\\rho_{1}\\frac{\\sigma}{\\sqrt{B\\delta}}d W_{t}}\\\\ {\\displaystyle d V_{t}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "870 Therefore, if $\\begin{array}{r}{H(X_{t},V_{t}):=\\frac{\\lambda X_{t}^{2}}{2}+\\frac{\\lambda\\delta B}{\\rho^{2}\\sigma^{2}}\\frac{M_{t}^{2}}{2}}\\end{array}$ , 5 we have that by It\u00f4\u2019s lemma, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d H(X_{t},V_{t})=-(\\lambda X_{t})\\left(\\kappa\\frac{\\sqrt{B\\delta}}{\\sigma}\\lambda X_{t}\\right)d t+\\left(\\frac{\\lambda\\delta B}{\\rho^{2}\\sigma^{2}}M_{t}\\right)\\kappa\\sqrt{\\eta}\\rho_{1}\\frac{\\sigma}{\\sqrt{B\\delta}}d W_{t}}\\\\ &{\\qquad\\qquad+\\frac{1}{2}\\left(\\frac{\\lambda\\delta B}{\\rho^{2}\\sigma^{2}}\\right)\\kappa^{2}\\eta\\rho^{2}\\frac{\\sigma^{2}}{B\\delta}d t}\\\\ &{\\qquad\\qquad=-2\\kappa\\lambda\\frac{\\sqrt{B\\delta}}{\\sigma}f(X_{t})d t+\\frac{\\kappa^{2}\\eta\\rho^{2}\\sigma^{2}}{2B\\delta}\\frac{\\lambda\\delta B}{\\rho^{2}\\sigma^{2}}d t+\\mathrm{Noise}.}\\\\ &{\\qquad\\qquad=-2\\kappa\\lambda\\frac{\\sqrt{B\\delta}}{\\sigma}f(X_{t})d t+\\frac{\\kappa^{2}\\eta\\lambda}{2}d t+\\mathrm{Noise}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "871 Once again, since $M_{t}\\to\\lambda X_{t}$ , we have that ", "page_idx": 34}, {"type": "equation", "text": "$$\nH(X_{t},V_{t})=\\frac{\\lambda X_{t}^{2}}{2}+\\frac{\\lambda\\delta B}{\\rho^{2}\\sigma^{2}}\\frac{M_{t}^{2}}{2}\\to\\frac{\\lambda X_{t}^{2}}{2}+\\lambda\\frac{\\lambda\\delta B}{\\rho^{2}\\sigma^{2}}\\frac{\\lambda X_{t}^{2}}{2}=\\left(1+\\lambda\\frac{\\lambda\\delta B}{\\rho^{2}\\sigma^{2}}\\right)\\frac{\\lambda X_{t}^{2}}{2}=:K f(X_{t}).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "872 Therefore, ", "page_idx": 34}, {"type": "equation", "text": "$$\nK d\\mathbb{E}[f(X_{t})]=-2\\kappa\\lambda\\frac{\\sqrt{B\\delta}}{\\sigma}\\mathbb{E}[f(X_{t})]d t+\\frac{\\kappa^{2}\\eta\\lambda}{2}d t,\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "887734 iwzhatiicohn i tmo edsi mtheant $\\begin{array}{r}{\\mathbb{E}[f(X_{t})]\\to\\frac{\\eta\\sigma}{4\\sqrt{B}}\\frac{\\kappa}{\\sqrt{\\delta}}}\\end{array}$ ,o nweh incehe dasl stoo  gsiuvmes  atchreo sssq uaallr et hreo odti smceanlisniog nrsu.le. The general$d$ ", "page_idx": 34}, {"type": "text", "text": "875 Lemma C.30. Let $\\textstyle f(x):={\\frac{x^{\\intercal}H x}{2}}$ where $H=\\operatorname{diag}(\\lambda_{1},\\dots,\\lambda_{d})$ . The stationary distribution of   \n876 Adam is $\\begin{array}{r}{(\\mathbb{E}[X_{\\infty}]],C o v(X_{\\infty}))=\\left(0,\\frac{\\eta}{2}\\Sigma^{\\frac{1}{2}}H^{-1}\\right)}\\end{array}$ . ", "page_idx": 34}, {"type": "text", "text": "877 Proof. The expected value follows immediately from the fact that ", "page_idx": 34}, {"type": "equation", "text": "$$\nd X_{t}=-\\Sigma^{-\\frac{1}{2}}X_{t}d t\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "878 For the covariance, we focus on the one-dimensional case. We define $\\begin{array}{r}{H(X_{t},V_{t}):=\\frac{X_{t}^{2}}{2}+\\frac{\\lambda^{2}}{2\\sigma^{2}\\rho^{2}}\\frac{M_{t}^{2}}{2}}\\end{array}$ .   \n879 With the same arguments as Lemma C.29, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\nd(X_{t})^{2}=-{\\frac{\\lambda}{\\sigma}}X_{t}^{2}d t+{\\frac{\\eta}{2}}d t+{\\mathrm{Noise,}}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "880 which implies that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}[X_{t}^{2}]\\stackrel{t\\rightarrow0}{\\rightarrow}\\frac{\\eta}{2}\\frac{\\sigma}{\\lambda}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "881 The thesis follows by applying the same logic to multiple dimensions. ", "page_idx": 34}, {"type": "text", "text": "882 C.6 AdamW ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "883 In this subsection, we derive the SDE of AdamW defined as defined as ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v_{k+1}=\\beta_{2}v_{k}+(1-\\beta_{2})\\left(\\nabla f_{\\gamma_{k}}(x_{k})\\right)^{2}}\\\\ &{m_{k+1}=\\beta_{1}m_{k}+(1-\\beta_{1})\\nabla f_{\\gamma_{k}}(x_{k})}\\\\ &{\\hat{m}_{k}=m_{k}\\left(1-\\beta_{1}^{k}\\right)^{-1}}\\\\ &{\\hat{v}_{k}=v_{k}\\left(1-\\beta_{2}^{k}\\right)^{-1}}\\\\ &{x_{k+1}=x_{k}-\\eta\\frac{\\hat{m}_{k+1}}{\\sqrt{\\hat{v}_{k+1}}+\\epsilon I_{d}}-\\eta\\gamma x_{k}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "884 with $(x_{0},m_{0},v_{0})\\,\\in\\,\\mathbb{R}^{d}\\,\\times\\,\\mathbb{R}^{d}\\,\\times\\,\\mathbb{R}^{d}$ , $\\eta\\,\\in\\,\\mathbb{R}^{>0}$ is the step size, $\\beta_{i}\\,=\\,1\\,-\\,\\rho_{i}\\eta$ for $\\rho_{1}\\,=\\,\\mathcal{O}(\\eta^{-\\zeta})$   \n885 s.t. $\\zeta\\in(0,1),\\,\\rho_{2}=\\mathcal{O}(1),\\,\\gamma>0,$ the mini-batches $\\{\\gamma_{k}\\}$ are modelled as i.i.d. random variables   \n886 uniformly distributed on $\\{1,\\cdot\\cdot\\cdot,N\\}$ , and of size $B\\geq1$ . ", "page_idx": 34}, {"type": "text", "text": "887 Theorem C.31. Under the same assumptions as Theorem C.26, the SDE of AdamW is ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d X_{t}=-\\frac{\\sqrt{\\gamma_{2}(t)}}{\\gamma_{1}(t)}P_{t}^{-1}(M_{t}+\\eta\\rho_{1}\\left(\\nabla f\\left(X_{t}\\right)-M_{t}\\right))d t-\\gamma X_{t}d t}\\\\ &{d M_{t}=\\rho_{1}\\left(\\nabla f\\left(X_{t}\\right)-M_{t}\\right)d t+\\sqrt{\\eta}\\rho_{1}\\Sigma^{1/2}\\left(X_{t}\\right)d W_{t}}\\\\ &{\\quad d V_{t}=\\rho_{2}\\left((\\nabla f(X_{t}))^{2}+\\mathrm{diag}\\left(\\Sigma\\left(X_{t}\\right)\\right)-V_{t}\\right)d t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "888 where $\\beta_{i}=1-\\eta\\rho_{i},\\,\\gamma>0,\\,\\gamma_{i}(t)=1-e^{-\\rho_{i}t}$ , and $P_{t}=\\mathrm{diag}\\,\\sqrt{V_{t}}+\\epsilon\\sqrt{\\gamma_{2}(t)}I_{d}.$ ", "page_idx": 35}, {"type": "text", "text": "889 Proof. The proof is the same as the of Theorem C.26 and the only difference is that $\\eta\\gamma x_{k}$ is   \n890 approximated with $\\gamma X_{t}d t$ . \u53e3 ", "page_idx": 35}, {"type": "text", "text": "891 Figure 4 and Figure 9 validate this result on a variety of architectures and datasets. ", "page_idx": 35}, {"type": "text", "text": "892 Corollary C.32. Under the assumptions of Theorem C.31 with $\\Sigma(x)=\\sigma^{2}I_{d},\\,\\tilde{\\eta}=\\kappa\\eta,\\,\\tilde{B}=B\\delta,$ ,   \n893 ${\\tilde{\\rho}}_{1}=\\alpha_{1}\\rho_{1}.$ , $\\widetilde{\\gamma}:\\xi\\gamma,$ , and ${\\tilde{\\rho}}_{2}=\\alpha_{2}\\rho_{2}$ ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d X_{t}=-\\kappa\\frac{\\sqrt{\\gamma_{2}(t)}}{\\gamma_{1}(t)}P_{t}^{-1}(M_{t}+\\eta\\alpha_{1}\\rho_{1}\\left(\\nabla f\\left(X_{t}\\right)-M_{t}\\right))d t-\\kappa\\xi\\gamma X_{t}d t}\\\\ &{d M_{t}=\\frac{\\alpha_{1}\\rho_{1}}{\\kappa}\\left(\\nabla f\\left(X_{t}\\right)-M_{t}\\right)d t+\\sqrt{\\eta}\\frac{\\alpha_{1}\\rho_{1}}{\\kappa}\\frac{\\sigma}{\\sqrt{B\\delta}}I_{d}d W_{t}}\\\\ &{d V_{t}=\\frac{\\alpha_{2}\\rho_{2}}{\\kappa}\\left((\\nabla f(X_{t}))^{2}+\\frac{\\sigma^{2}}{B\\delta}I_{d}-V_{t}\\right)d t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "894 Lemma C.33 (Scaling Rule at Convergence). Under the assumptions of Corollary C.32, $f$ is $\\mu$ -   \n895 strongly convex and $L$ -smooth, ${\\mathcal{L}}_{\\tau}:=T{\\check{r}}(\\nabla^{2}f(x))$ , and $(\\nabla f(x))^{2}\\,{\\overset{}{=}}\\,{\\mathcal{O}}(\\eta)$ , the asymptotic dynamics   \n896 of the iterates of AdamW satisfies the novel scaling rule $i f\\kappa=\\sqrt{\\delta}$ and $\\xi=\\kappa$ because ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{E}[f(X_{t})-f(X_{*})]\\stackrel{t\\rightarrow\\infty}{\\leq}\\frac{\\eta\\mathcal{L}_{\\tau}\\sigma L}{2}\\frac{\\kappa}{2\\mu\\sqrt{B\\delta}L+\\sigma\\xi\\gamma(L+\\mu)}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "897 By enforcing that the speed of $V_{t}$ matches that of $X_{t}$ , one needs $\\tilde{\\rho}\\,=\\,\\kappa^{2}\\rho,$ , which implies $\\tilde{\\beta}_{i}\\mathrm{~=~}$   \n898 $1^{\\dot{\\mathrm{~\\,~}}}\\dot{\\kappa^{2}}(1-\\check{\\beta}_{i})$ . ", "page_idx": 35}, {"type": "text", "text": "899 Proof. The proof is the same as Lemma C.28 where we also use $L$ -smoothness as in Lemma C.23. ", "page_idx": 35}, {"type": "text", "text": "900 Lemma C.34. For $\\begin{array}{r}{f(x):=\\frac{x^{\\top}H x}{2}}\\end{array}$ , the stationary distribution of AdamW is $(\\mathbb{E}[X_{\\infty}]],C o v(X_{\\infty}))=$   \n901 $\\left(0,\\frac{\\eta}{2}(H\\Sigma^{-\\frac{1}{2}}+\\gamma I_{d})^{-1}\\right)$ . ", "page_idx": 35}, {"type": "text", "text": "902 Proof. The proof is the same as Lemma C.30. ", "page_idx": 35}, {"type": "text", "text": "903 D SDEs from the literature ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "904 Theorem D.1 (Original Malladi\u2019s Statement). Let $\\sigma_{0}:=\\sigma\\eta,\\epsilon_{0}:=\\epsilon\\eta,$ , and $\\begin{array}{r}{c_{2}:=\\frac{1-\\beta}{\\eta^{2}}}\\end{array}$ . Define the   \n905 state of the SDE as $L_{t}=\\left(X_{t},u_{t}\\right)$ and the dynamics as ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{d X_{t}=-P_{t}^{-1}\\left(\\nabla f\\left(X_{t}\\right)d t+\\sigma_{0}\\Sigma^{1/2}\\left(X_{t}\\right)d W_{t}\\right)}\\\\ {d u_{t}=c_{2}\\left(\\operatorname{diag}\\left(\\Sigma\\left(X_{t}\\right)\\right)-u_{t}\\right)d t}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "906 where $P_{t}:=\\sigma_{0}\\,\\mathrm{diag}\\left(u_{t}\\right)^{1/2}+\\epsilon_{0}I_{d}.$ . ", "page_idx": 35}, {"type": "text", "text": "907 Theorem D.2 (Informal State\u221ament of Theorem C.2 Malladi et al. (2022)). Under sufficient regularity   \n908 conditions and $\\nabla f(x)=\\mathcal{O}(\\sqrt{\\eta})$ , the following $S D E$ is an order 1 weak approximation of RMSprop: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d X_{t}=-P_{t}^{-1}(\\nabla f(X_{t})d t+\\sqrt{\\eta}\\Sigma(X_{t})^{\\frac{1}{2}}d W_{t})}\\\\ &{d V_{t}=\\rho(\\mathrm{diag}(\\Sigma(X_{t}))-V_{t}))d t,}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "909 where $\\beta=1-\\eta\\rho,\\,\\rho=\\mathcal{O}(1).$ , and $P_{t}:=\\mathrm{diag}\\left(V_{t}\\right)^{\\frac{1}{2}}+\\epsilon I_{d}$ . ", "page_idx": 35}, {"type": "text", "text": "910 Lemma D.3. Theorem D.1 and Theorem $D.2$ are equivalent. ", "page_idx": 36}, {"type": "text", "text": "911 Proof. It follows applying time rescaling $t:=\\eta\\xi$ and observing that $W_{t}=W_{\\eta\\xi}=\\sqrt{\\eta}W_{\\xi}$ . ", "page_idx": 36}, {"type": "text", "text": "912 Theorem D.4 (Original Malladi\u2019s Statement). Let $c_{1}:=\\left(1-\\beta_{1}\\right)/\\eta^{2},c_{2}:=\\left(1-\\beta_{2}\\right)/\\eta^{2}$ and define   \n913 $\\sigma_{0},\\epsilon_{0}$ in Theorem D.1. Let $\\gamma_{1}(t):=1-\\exp\\left(-c_{1}t\\right)$ and $\\gamma_{2}(t):=1-\\exp\\left(-c_{2}t\\right)$ . Define the state   \n914 of the SDE as $L_{t}=(X_{t},m_{t},i_{t})$ and the dynamics as ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d X_{t}=-\\frac{\\sqrt{\\gamma_{2}(t)}}{\\gamma_{1}(t)}P_{t}^{-1}m_{t}d t}\\\\ &{d m_{t}=c_{1}\\left(\\nabla f\\left(X_{t}\\right)-m_{t}\\right)d t+\\sigma_{0}c_{1}\\Sigma^{1/2}\\left(X_{t}\\right)d W_{t},}\\\\ &{d u_{t}=c_{2}\\left(\\mathrm{diag}\\left(\\Sigma\\left(X_{t}\\right)\\right)-u_{t}\\right)d t,}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "915 where $P_{t}:=\\sigma_{0}\\,\\mathrm{diag}\\left(u_{t}\\right)^{1/2}+\\epsilon_{0}\\sqrt{\\gamma_{2}(t)}I_{d}.$ ", "page_idx": 36}, {"type": "text", "text": "916 Theorem D.5 (Informal State\u221ament of Theorem D.2 Malladi et al. (2022)). Under sufficient regularity   \n917 conditions and $\\nabla f(x)={\\mathcal{O}}({\\sqrt{\\eta}})$ , the following SDE is an order 1 weak approximation of Adam: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{l}{d X_{t}=-\\frac{\\sqrt{\\gamma_{2}\\left(t\\right)}}{\\gamma_{1}\\left(t\\right)}P_{t}^{-1}M_{t}d t\\,}\\\\ {d M_{t}=\\rho_{1}\\left(\\nabla f\\left(X_{t}\\right)-M_{t}\\right)d t+\\sqrt{\\eta}\\rho_{1}\\Sigma^{1/2}\\left(X_{t}\\right)d W_{t}}\\\\ {d V_{t}=\\rho_{2}\\left(\\mathrm{diag}\\left(\\Sigma\\left(X_{t}\\right)\\right)-V_{t}\\right)d t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "918 where $\\beta_{i}=1-\\eta\\rho_{i}$ , $\\gamma_{i}(t)=1-e^{-\\rho_{i}t}$ , $\\rho_{i}=\\mathcal{O}(1)$ , and $P_{t}=\\mathrm{diag}\\,\\sqrt{V_{t}}+\\epsilon\\sqrt{\\gamma_{2}(t)}I_{d}.$ . ", "page_idx": 36}, {"type": "text", "text": "919 Lemma D.6. Theorem D.4 and Theorem D.5 are equivalent. ", "page_idx": 36}, {"type": "text", "text": "920 Proof. It follows applying time rescaling $t:=\\eta\\xi$ and observing that $W_{t}=W_{\\eta\\xi}=\\sqrt{\\eta}W_{\\xi}$ . ", "page_idx": 36}, {"type": "text", "text": "921 E SDE cannot be derived nor used naively ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "922 In this section, we provide a gentle introduction to the meaning of deriving an SDE model for an   \n923 optimizer and discuss how SDEs have been used to derive scaling rules. To aid the intuition of the   \n924 reader, we informally derive an SDE for SGD with learning rate $\\eta$ , mini-batches $\\gamma_{B}$ of size $B$ , and   \n925 starting point $x_{0}=x$ , which we dub $\\mathrm{SGD}^{(\\eta,B)}$ . The iterates are given by: ", "page_idx": 36}, {"type": "equation", "text": "$$\nx_{k+1}=x_{k}-\\eta\\nabla f_{\\gamma_{k}^{B}}(x_{k})\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "926 which for $U_{k}:=\\sqrt{\\eta}(\\nabla f(x_{k})-\\nabla f_{\\gamma_{k}^{B}}(x_{k}))$ , we rewrite as ", "page_idx": 36}, {"type": "equation", "text": "$$\nx_{k}-\\eta\\nabla f(x_{k})+\\sqrt{\\eta}U_{k},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "927 where $\\mathbb{E}[U_{k}]=0$ and $\\begin{array}{r}{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! $   \n928 If we now consider the SDE ", "page_idx": 36}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 36}, {"type": "equation", "text": "$$\nd X_{t}=-\\nabla f(X_{t})d t+{\\sqrt{\\frac{\\eta}{B}}}\\Sigma(X_{t})^{\\frac{1}{2}}d W_{t},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "929 its Euler-Maruyama discretization with pace $\\Delta t=\\eta$ and $Z_{k}\\sim\\mathcal{N}(0,I_{d})$ is ", "page_idx": 36}, {"type": "equation", "text": "$$\nX_{k+1}=X_{k}-\\eta\\nabla f(X_{k})+\\sqrt\\eta\\sqrt{\\frac{\\eta}{B}}\\Sigma(X_{t})^{\\frac{1}{2}}Z_{k}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "930 Since the Eq. (191) and Eq. (194) share the first two moments, it is reasonable that by identifying   \n931 $t=k\\eta$ , the SDE in Eq. (193) is a good model to describe the iterates of SGD in Eq. (191).   \n932 Informally, we need a \u201cgood model\u201d, which is an SDE that is close to the real optimizer. This is   \n933 formalized in the following definition which comes from the field of numerical analysis of SDEs (see   \n934 Mil\u2019shtein (1986)) and bounds the disparity between the the discrete and the continuous process.   \n935 Definition E.1 (Weak Approximation). A continuous-time stochastic process $\\{X_{t}\\}_{t\\in[0,T]}$ is an order   \n936 $\\alpha$ weak approximation (or $\\alpha$ -order SDE) of a discrete stochastic process k}\u230aT/\u03b7\u230bif for every   \n937 polynomial growth function $g$ , there exists a positive constant , independent of the stepsize $\\eta$ , such   \n938 that $\\begin{array}{r}{\\operatorname*{max}_{k=0,\\ldots,\\lfloor T/\\eta\\rfloor}|\\mathbb{E}g\\left(x_{k}\\right)-\\mathbb{E}g\\left(X_{k\\eta}\\right)|\\leq C\\eta^{\\alpha}.}\\end{array}$ . ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "939 To see if an SDE satisfies such a definition, one has to check that for $\\bar{\\Delta}=x_{1}-x$ and $\\Delta=X_{\\eta}-x$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1.~\\left|\\mathbb{E}\\Delta_{i}-\\mathbb{E}\\bar{\\Delta}_{i}\\right|=\\mathcal{O}(\\eta^{2}),~~~\\forall i=1,\\ldots,d;}\\\\ &{2.~\\left|\\mathbb{E}\\Delta_{i}\\Delta_{j}-\\mathbb{E}\\bar{\\Delta}_{i}\\bar{\\Delta}_{j}\\right|=\\mathcal{O}(\\eta^{2}),~~~\\forall i,j=1,\\ldots,d.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "942 Example: Let us prove that the SDE in Eq. (193) is a valid approximation of $\\mathrm{SGD}^{(\\eta,B)}$ : The first   \n943 condition is easily verified. Coming to the second condition we have that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1.~\\mathbb{E}\\Delta_{i}\\Delta_{j}=\\eta^{2}\\partial_{i}f(x)\\partial_{j}f(x)+\\frac{\\eta^{2}}{B}\\Sigma(x);}\\\\ &{2.~\\mathbb{E}\\bar{\\Delta}_{i}\\bar{\\Delta}_{j}=\\eta^{2}\\partial_{i}f(x)\\partial_{j}f(x)+\\frac{\\eta^{2}}{B}\\Sigma(x)+\\mathcal{O}(\\eta^{3});}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "946 whose difference is of order $\\eta^{3}$ and thus satisfies the condition. However, we observe that if the   \n947 scale of the noise is too small w.r.t $\\eta$ , i.e. $\\Sigma(x)={\\mathcal{O}}(\\eta^{\\alpha})$ for $\\alpha\\geq0$ , then the simplest SDE model   \n948 describing $\\mathrm{SGD}^{(\\eta,B)}$ is the ODE $d X_{t}=-\\nabla f(X_{t})d t$ as in that case ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1.\\;\\,\\mathbb{E}\\Delta_{i}\\Delta_{j}=\\eta^{2}\\partial_{i}f(x)\\partial_{j}f(x)+\\mathcal{O}(\\eta^{2+\\alpha});}\\\\ &{2.\\;\\,\\mathbb{E}\\bar{\\Delta}_{i}\\bar{\\Delta}_{j}=\\eta^{2}\\partial_{i}f(x)\\partial_{j}f(x)+\\mathcal{O}(\\eta^{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "951 whose difference is also of order $\\eta^{2}$ . Much differently, if $\\Sigma(x)={\\mathcal{O}}(\\eta^{-\\alpha})$ for $\\alpha>0$ , the simplest   \n952 model is the SDE in Eq. (193). We highlight that simplest does not mean best: The SDE is more   \n953 accurate than the ODE even in a regime with low noise, but this observation serves as a provocation.   \n954 One has to pay attention when deriving SDEs: Some models are more realistic than others.   \n955 Let us dig deeper into this thought as we derive two SDEs for SGD with learning rate $\\tilde{\\eta}:=\\kappa\\eta$ and   \n956 batch size $\\tilde{B}:=\\delta B$ for $\\kappa>1$ and $\\delta>1$ , which we dub $\\mathrm{SGD}^{(\\tilde{\\eta},\\tilde{B})}$ . The first is derived considering   \n957 that the learning rate is \u03b7\u02dc and carries an error of order O(\u03b7\u02dc) w.r.t. SGD(\u03b7\u02dc, B\u02dc) ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 37}, {"type": "equation", "text": "$$\nd X_{t}=-\\nabla f(X_{t})d t+\\sqrt{\\frac{\\tilde{\\eta}}{\\tilde{B}}}\\Sigma(X_{t})^{\\frac{1}{2}}d W_{t}=-\\nabla f(X_{t})d t+\\sqrt{\\frac{\\eta\\kappa}{B\\delta}}\\Sigma(X_{t})^{\\frac{1}{2}}d W_{t}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "958 The second one instead is derived considering $\\eta$ as the learning rate and $\\kappa$ as a constant \u201cscheduler\u201d.   \n959 Consistently with (Li et al., 2017), the SDE which carries an error of order $\\mathcal{O}(\\eta)$ w.r.t $\\mathrm{SGD}^{(\\tilde{\\eta},\\tilde{B})}$ is ", "page_idx": 37}, {"type": "equation", "text": "$$\nd X_{t}=-\\kappa\\nabla f(X_{t})d t+\\kappa\\sqrt{\\frac{\\eta}{B\\delta}}\\Sigma(X_{t})^{\\frac{1}{2}}d W_{t}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "960 While they both are valid models, there are three reasons why one should prefer the latter: ", "page_idx": 37}, {"type": "text", "text": "1. It fully reflects the fact that a larger learning rate results in a faster and noisier dynamics   \n2. It has intrinsically less error than the other;   \n3. It is consistent with the optimizer in that there is no combination of $\\kappa$ and $\\delta$ that can ever leave the dynamics unchanged. ", "page_idx": 37}, {"type": "text", "text": "965 E.1 Deriving scaling rules ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "966 Jastrzebski et al. (2018) observed that only the ratio between $\\eta$ and $B$ matters in determining the   \n967 dynamics of Eq. (194). Therefore, they argue that for $\\kappa=\\delta$ the SDE for $\\mathrm{SGD}^{(\\kappa\\eta,\\delta B)}$ coincides with   \n968 that of $\\mathrm{SGD}^{(\\eta,\\bar{B})}$ and that this implies that the path properties of the optimizers are the same. On the   \n969 contrary, the path of $\\mathrm{SGD}^{(\\eta,B)}$ strongly depends on the hyperparameters: The speed and volatility of   \n970 the dynamics are driven by $\\eta$ , and no choice of $B$ can undo this. We remind the reader that the goal of   \n971 these rules is not to keep the dynamics of the optimizers unaltered, but rather to give a practical way   \n972 to change a hyperparameter, e.g. $\\eta$ , and have a principled way to adjust the others, e.g. $B$ , such that   \n973 the performance of the optimizer is preserved. Therefore, we propose deriving scaling rules as we   \n974 preserve certain relevant quantities of the dynamics such as the convergence bound on the expected   \n975 loss or the speed. To show this quantitative, we use this rationale to derive the scaling rule of SGD as   \n976 we aim at preserving the asymptotic loss level. ", "page_idx": 37}, {"type": "text", "text": "977 Lemma E.2. If $f$ is a \u00b5 strongly convex function, ${\\mathcal{L}}_{\\tau}\\leq T r(\\nabla^{2}f(x))$ and $\\Sigma(x)=\\sigma^{2}I_{d},$ , then: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{E}[f(X_{t})-f(X_{*})]\\leq(f(X_{0})-f(X_{*}))e^{-2\\mu t}+\\frac{\\eta}{2}\\frac{\\mathcal{L}_{\\tau}\\sigma^{2}}{2\\mu B}\\left(1-e^{-2\\mu t}\\right);\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "979 2. Under the dynamics of Eq. (195) we have: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{E}[f(X_{t})-f(X_{*})]\\leq(f(X_{0})-f(X_{*}))e^{-2\\mu t}+\\frac{\\eta}{2}\\frac{\\mathcal{L}_{\\tau}\\sigma^{2}}{2\\mu B}\\frac{\\kappa}{\\delta}\\left(1-e^{-2\\mu t}\\right);\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "980 3. Under the dynamics of Eq. (196) we have: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{E}[f(X_{t})-f(X_{*})]\\leq(f(X_{0})-f(X_{*}))e^{-2\\mu\\kappa t}+\\frac{\\eta}{2}\\frac{\\mathcal{L}_{\\tau}\\sigma^{2}}{2\\mu B}\\frac{\\kappa}{\\delta}\\left(1-e^{-2\\mu\\kappa t}\\right).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "981 The first bound implies that the asymptotic limit of the expected loss for SGD(\u03b7,B) is \u03b72L2\u03c4\u00b5 \u03c3B2 . The   \n982 last two bounds predict that the asymptotic loss level for $\\mathrm{SGD}^{(\\tilde{\\eta},\\tilde{B})}$ is $\\frac{\\eta}{2}\\,\\frac{\\mathcal{L}_{\\tau}\\sigma^{2}}{2\\mu B}\\,\\frac{\\kappa}{\\delta}$ . Since the objective   \n983 of the scaling rule is to find $\\kappa$ and $\\delta$ such that $\\mathrm{SGD}^{(\\tilde{\\eta},\\tilde{B})}$ achieves the same loss level as $\\mathrm{SGD}^{(\\eta,B)}$ ,   \n984 we recover the linear scaling rule setting $\\kappa=\\delta$ . However, only the last bound can correctly capture   \n985 the fact that the dynamics of $\\mathrm{SGD}^{(\\tilde{\\eta},\\tilde{B})}$ is $\\kappa$ times faster than that of $\\mathrm{SGD}^{(\\eta,B)}$ .   \n986 We conclude the discussion with a simple sample of how deriving a scaling rule from the SDE itself   \n987 inevitably leads to the wrong conclusion. We define the following algorithm which is inspired by   \n988 AdamW and which we dub SGDW: ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 38}, {"type": "equation", "text": "$$\nx_{k+1}=x_{k}-\\eta\\nabla f_{\\gamma_{k}}(x_{k})-\\eta\\gamma x_{k}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "989 Lemma E.3. The SDE of SGDW is ", "page_idx": 38}, {"type": "equation", "text": "$$\nd X_{t}=-\\nabla f(X_{t})d t+{\\sqrt{\\frac{\\eta}{B}}}\\Sigma(X_{t})^{\\frac{1}{2}}d W_{t}-\\gamma X_{t}d t.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "990 Therefore, one would naively deduce that to keep the SDE unchanged, one can simply use the linear   \n991 scaling rule of SGD and leave $\\gamma$ unaltered. However, one can easily derive the upper bound on the   \n992 expected loss for a convex quadratic function and observe that to preserve that, it is imperative to   \n993 scale $\\gamma$ by $\\kappa$ as well. ", "page_idx": 38}, {"type": "text", "text": "994 We thus conclude that: ", "page_idx": 38}, {"type": "text", "text": "1. Eq. (196) is a better model for $\\mathrm{SGD}^{(\\tilde{\\eta},\\tilde{B})}$ as it represents the dynamics more accurately; 2. Maintaining the shape of the SDE does not preserve the path properties of the optimizer; 3. Deriving a scaling rule uniquely from the SDE might lead to the wrong conclusions in the general case. ", "page_idx": 38}, {"type": "text", "text": "999 Remark E.4. We highlight that Theorem 5.3 of Malladi et al. (2022) claimed to have formally derived   \n1000 one for RMSprop: In line with (Jastrzebski et al., 2018), they argue that if they were to find a scaling   \n1001 rule that would leave their SDE unchanged, this would imply that even the dynamics of the iterates of   \n1002 RMSprop itself would be unchanged. First, we remind the reader that an SDE is formally defined   \n1003 as an equation that drives the dynamics plus an initial condition (See (Karatzas and Shreve, 2014),   \n1004 Section 5). While their scaling rule does leave the equation unchanged, it alters the initial condition,   \n1005 thus changing the SDE itself: This invalidates their claim and proof. Second, contrary to their claim,   \n1006 the rule is only valid near convergence as their SDE is only valid there. Third, Lemma E.2 offers a   \n1007 shred of concrete evidence that keeping the SDE unchanged does not imply that the path properties   \n1008 of the optimizers are preserved. Fourth, Lemma E.3 is a piece of concrete evidence that deriving   \n1009 scaling rules directly and naively from the SDE might lead to the wrong conclusions. ", "page_idx": 38}, {"type": "text", "text": "101 0 F Experiments ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "1011 In this section, we provide the modeling choices and instructions to replicate our experiments. All   \n1012 experiments we run on one NVIDIA GeForce RTX 3090 GPU. The code is implemented in Python 3   \n1013 (Van Rossum and Drake, 2009) mainly using Numpy (Harris et al., 2020), scikit-learn (Pedregosa   \n1014 et al., 2011), and JAX (Bradbury et al., 2018). ", "page_idx": 38}, {"type": "text", "text": "1015 F.1 SignSGD: SDE validation (Figure 1) ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "1016 In this subsection, we describe the experiments we run to produce Figure 1: The loss dynamics of   \n1017 SignSGD and that of our SDE match on average.   \n1018 DNN on Breast Cancer Dataset (Dua and Graff, 2017) This paragraph refers to the left of Figure   \n1019 1. The DNN has 10 dense layers with 20 neurons each activated with a ReLu. We minimize the binary   \n1020 cross-entropy loss. We run SignSGD for 50000 epochs as we calculate the full gradient and inject it   \n1021 with Gaussian noise $Z\\sim\\mathcal{N}(0,\\sigma^{2}I_{d})$ where $\\sigma=1$ . The learning rate is $\\eta=0.001$ . Similarly, we   \n1022 integrate the SignSGD SDE (Eq. (7)) with Euler-Maruyama (Algorithm 1) with $\\Delta t=\\eta$ . Results are   \n1023 averaged over 3 runs and the shaded areas are the average $\\pm$ the standard deviation.   \n1024 CNN on MNIST (Deng, 2012) This paragraph refers to the center-left of Figure 1. The CNN   \n1025 has a $(3,3,32)$ convolutional layer with stride 1, followed by a ReLu activation, a $(2,2)$ max pool   \n1026 layer with stride $(2,2)$ , a $(3,3,32)$ convolutional layer with stride 1, a ReLu activation, a $(2,2)$ max   \n1027 pool layer with stride $(2,2)$ . Then the activations are flattened and passed through a dense layer that   \n1028 compresses them into 128 dimensions, a final ReLu activation, and a final dense layer into the output   \n1029 dimension 10. The output finally goes through a softmax as we minimize the cross-entropy loss. We   \n1030 run SignSGD for 40000 epochs as we calculate the full gradient and inject it with Gaussian noise   \n1031 $Z\\sim\\bar{\\mathcal{N}}(0,\\sigma^{2}I_{d})$ where $\\sigma=1$ . The learning rate is $\\eta=0.001$ . Similarly, we integrate the SignSGD   \n1032 SDE (Eq. (7)) with Euler-Maruyama (Algorithm 1) with $\\Delta t=\\eta$ . Results are averaged over 3 run   \n1033 and the shaded areas are the average $\\pm$ the standard deviation.   \n1034 Transformer on MNIST This paragraph refers to the center-right of Figure 1. The Architecture is   \n1035 a scaled-down version of (Dosovitskiy et al., 2021), where the hyperparameters are patch size $=\\!28$ ,   \n1036 out feature $s{=}10$ , width $\\scriptstyle=48$ , depth $=\\!3$ , num head $\\mathrm{s}{=}6$ , and dim ff $\\scriptstyle{\\mathrm{192}}$ . We minimize the cross-entropy   \n1037 loss as we run SignSGD for 5000 epochs as we calculate the full gradient and inject it with Gaussian   \n1038 noise $Z\\sim\\mathcal{N}(0,\\bar{\\sigma}^{2}I_{d})$ where $\\sigma=1$ . The learning rate is $\\eta=0.001$ . Similarly, we integrate the   \n1039 SignSGD SDE (Eq. (7)) with Euler-Maruyama (Algorithm 1) with $\\Delta t=\\eta$ . Results are averaged   \n1040 over 3 runs and the shaded areas are the average $\\pm$ the standard deviation.   \n1041 ResNet on CIFAR-10 (Krizhevsky et al., 2009) This paragraph refers to the right of Figure 1.   \n1042 The ResNet has a $(3,3,128)$ convolutional layer with stride 1, followed by a ReLu activation, a   \n1043 second $(3,3,64)$ convolutional layer with stride 1, followed by a residual connection from the first   \n1044 convolutional layer, then a $(2,2)$ max pool layer with stride $(2,2)$ . Then the activations are flattened   \n1045 and passed through a dense layer that compresses them into 128 dimensions, a final ReLu activation,   \n1046 and a final dense layer into the output dimension 10. The output finally goes through a softmax as we   \n1047 minimize the cross-entropy loss. We run SignSGD for 5000 epochs as we calculate the full gradient   \n1048 and inject it with Gaussian noise $Z\\sim\\bar{\\mathcal{N}(0,\\sigma^{2}I_{d})}$ where $\\sigma=1$ . The learning rate is $\\eta=0.001$ .   \n1049 Similarly, we integrate the SignSGD SDE (Eq. (7)) with Euler-Maruyama (Algorithm 1) with $\\Delta t=\\eta$ .   \n1050 Results are averaged over 3 runs and the shaded areas are the average $\\pm$ the standard deviation. ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "1051 F.2 SignSGD: insights validation (Figure 2) ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "1052 In this subsection, we describe the experiments we run to produce Figure 2: We successfully validate   \n1053 them all.   \n1054 Phases: Lemma 3.4 and Lemma 3.5 In this paragraph, we describe how we validated the existence   \n055 of the phases of SignSGD as predicted in Lemma 3.4 and Lemma 3.5. To produce the left of Figure   \n056 2), we simulated the full SDE (Eq. (16)) and the one describing Phase 3 (Eq. (5)). The optimized   \n057 function is $\\begin{array}{r}{f(x)\\,=\\,\\frac{x^{\\top}H x}{2}}\\end{array}$ for $H\\,=\\,\\mathrm{diag}(1,2)$ , $x_{0}$ drawn (and fixed for all runs) from a normal   \n058 distribution $\\mathcal{N}(0,0.01)$ , $\\eta=0.001$ , and $\\Sigma\\,=\\,\\sigma^{2}I_{d}$ where $\\sigma\\,=\\,0.1$ . We integrate the SDEs with   \n1059 Euler-Maruyama (Algorithm 1) with $\\Delta t=\\eta$ and for 3000 iterations. Results are averaged over 500   \n1060 runs and the shaded areas are the average $\\pm$ the standard deviation. Clearly, the two SDEs share the   \n1061 same dynamics.   \n1062 To produce the center-left of Figure 2, we repeat the above as $x_{0}$ drawn (and fixed for all runs) from   \n1063 a normal distribution ${\\mathcal{N}}(0,1)$ . Then, we plot the average loss values together with the theoretical   \n1064 prediction of Phase 1 and Phase 3: They perfectly overlap.   \n1065 Stationary distribution: Lemma 3.7 In this paragraph, we describe how we validated the conver  \n1066 gence behavior predicted in Lemma 3.7. To produce the center-right of Figure 2), we run SignSGD on   \n1067 $\\begin{array}{r}{\\bar{f}(x)=\\frac{x^{\\top}H x}{2}}\\end{array}$ for $H=\\mathrm{diag}(1,2)$ , $x_{0}=(0.001,0.001)$ , $\\eta=0.001$ and $\\Sigma=\\sigma^{2}I_{d}$ where $\\sigma=0.1$ .   \n1068 We run this for 5000 times and report the evolution of the moments. Then, we add lines representing   \n1069 the theoretical predictions derived in Lemma 3.7: They match.   \n1070 Schedulers: Lemma 3.9 In this paragraph, we describe how we validated the convergence behavior   \n1071 predicted in Lemma 3.9. To produce the right of Figure 2, we run SignSGD on $\\begin{array}{r}{f(x)=\\frac{x^{\\top}H x}{2}}\\end{array}$ for   \n1072 $H=\\mathrm{diag}(1,2)$ , $x_{0}=(0.01,0.01)$ , $\\eta=0.01$ and $\\Sigma=\\sigma^{2}I_{d}$ where $\\sigma=0.1$ . We used the scheduler   \n1073 $\\begin{array}{r}{\\eta_{t}^{\\gamma}\\,=\\,\\frac{1}{(t+1)^{\\gamma}}}\\end{array}$ for $\\gamma\\,\\in\\,\\{0.1,0.5,1.5\\}$ . For the first two choices of $\\gamma$ , $\\eta_{t}^{\\gamma}$ satisfies our sufficient   \n1074 condition for the convergence of SignSGD: In the figure, we observe that indeed SignSGD converges   \n1075 to 0 with the same speed as the one predicted in the Lemma. For $\\gamma=1.5$ , we observe that SignSGD   \n1076 does not converge following the theoretical curve because it does not satisfy our sufficient condition.   \n1077 Results are averaged over 500 runs. ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "1078 F.3 RMSprop: SDE validation (Figure 7 and Figure 8) ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "1079 In this subsection, we describe the experiments we run to produce Figure 7 and Figure 8: The   \n1080 dynamics of our SDE matches that of RMSprop better than the SDE derived in (Malladi et al., 2022).   \n1081 Quadratic convex function This paragraph refers to the left and center-left of Figure 7. We   \n1082 optimize the function $\\begin{array}{r}{f(x)=\\frac{x^{\\top}H x}{2}}\\end{array}$ where $H=\\mathrm{diag}(10,2)$ . We run RMSprop for 2000 epochs as   \n1083 we calculate the full gradient and inject it with Gaussian noise $Z\\sim\\mathcal{N}(0,\\sigma^{2}I_{d})$ where $\\sigma=0.1$ . The   \n1084 learning rate is $\\eta=0.01$ , $\\beta=0.99$ . Similarly, we integrate our RMSprop SDE (Eq. (86)) and that of   \n1085 Malladi (Eq. (183)) with Euler-Maruyama (Algorithm 1) with $\\Delta t=\\eta$ . Results are averaged over   \n1086 500 runs and the shaded areas are the average $\\pm$ the standard deviation: Our SDE matches RMSprop   \n1087 much better.   \n1088 Embedded saddle This paragraph refers to the center-right and right of Figure 7. We optimize the   \n1089 function $\\begin{array}{r}{f(x)=\\frac{x^{\\top}H x}{2}+\\frac{1}{4}\\lambda\\sum_{i=1}^{2}x_{i}^{4}-\\frac{\\xi}{3}\\sum_{i=1}^{2}x_{i}^{3}}\\end{array}$ where $H=\\mathrm{diag(-1,2)}$ , $\\lambda=1$ , and $\\xi=0.1$   \n1090 We run RMSprop for 1600 epochs as we calculate the full gradient and inject it with Gaussian noise   \n1091 $Z\\sim\\mathcal{N}(0,\\sigma^{\\dot{2}}I_{d})$ where $\\sigma=0.01$ . The learning rate is $\\eta=0.01$ , $\\beta=0.99$ . Similarly, we integrate   \n1092 our RMSprop SDE (Eq. (86)) and that of Malladi (Eq. (183)) with Euler-Maruyama (Algorithm 1)   \n1093 with $\\Delta t=\\eta$ . Results are averaged over 500 runs and the shaded areas are the average $\\pm$ the standard   \n1094 deviation: Our SDE matches RMSprop much better.   \n1095 DNN on Breast Cancer Dataset This paragraph refers to the left of Figure 8. The architecture and   \n1096 loss are the same as used above for SignSGD. We run RMSprop for 2000 epochs as we calculate the   \n1097 full gradient and inject it with Gaussian noise $Z\\sim\\mathcal{N}(0,\\sigma^{\\hat{2}}I_{d})$ where $\\sigma=10^{-2}$ . The learning rate   \n1098 is $\\eta=10^{-4}$ , $\\beta=0.9995$ . Similarly, we integrate our RMSprop SDE (Eq. (86)) and that of Malladi   \n1099 (Eq. (183)) with Euler-Maruyama (Algorithm 1) with $\\Delta t=\\eta$ . Results are averaged over 3 runs and   \n1100 the shaded areas are the average $\\pm$ the standard deviation: Our SDE matches RMSprop much better.   \n1101 CNN on MNIST This paragraph refers to the center-left of Figure 8. The architecture and loss   \n1102 are the same as used above for SignSGD. We run RMSprop for 2000 epochs as we calculate the full   \n1103 gradient and inject it with Gaussian noise $Z\\sim\\mathcal{N}(0,\\bar{\\sigma}^{2}\\bar{I_{d}})$ where $\\sigma=10^{-2}$ . The learning rate is   \n1104 $\\bar{\\eta}=10^{-3}$ , $\\beta=0.995$ . Similarly, we integrate our RMSprop SDE (Eq. (86)) and that of Malladi (Eq.   \n1105 (183)) with Euler-Maruyama (Algorithm 1) with $\\Delta t=\\eta$ . Results are averaged over 3 run and the   \n1106 shaded areas are the average $\\pm$ the standard deviation: Our SDE matches RMSprop much better.   \n1107 Transformer on MNIST This paragraph refers to the center-right of Figure 8. The architecture   \n1108 and loss are the same as used above for SignSGD. We run RMSprop for 2000 epochs as we calculate   \n1109 the full gradient and inject it with Gaussian noise $Z\\sim\\mathcal{N}(0,\\bar{\\sigma^{2}}I_{d})$ where $\\sigma=10^{-2}$ . The learning   \n1110 rate is $\\bar{\\eta}=10^{-3}$ , $\\beta\\doteq0.995$ . Similarly, we integrate our RMSprop SDE (Eq. (86)) and that of   \n1111 Malladi (Eq. (183)) with Euler-Maruyama (Algorithm 1) with $\\Delta t=\\eta$ . Results are averaged over   \n1112 3 runs and the shaded areas are the average $\\pm$ the standard deviation: Our SDE matches RMSprop   \n1113 much better.   \n1114 ResNet on CIFAR-10 This paragraph refers to the right of Figure 8. The architecture and loss   \n1115 are the same as used above for SignSGD. We run RMSprop for 500 epochs as we calculate the full   \n1116 gradient and inject it with Gaussian noise $Z\\sim\\mathcal{N}(0,\\bar{\\sigma^{2}}I_{d})$ where $\\sigma=10^{-4}$ . The learning rate is   \n1117 $\\bar{\\eta}=10^{-4}$ , $\\beta=0.9999$ . Similarly, we integrate our RMSprop SDE (Eq. (86)) and that of Malladi   \n1118 (Eq. (183)) with Euler-Maruyama (Algorithm 1) with $\\Delta t=\\eta$ . Results are averaged over 3 runs and   \n1119 the shaded areas are the average $\\pm$ the standard deviation: Our SDE matches RMSprop much better. ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "1120 F.4 Adam: SDE validation (Figure 10 and Figure 11) ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "1121 In this subsection, we describe the experiments we run to produce Figure 11 and Figure 10: The   \n1122 dynamics of our SDE matches that of Adam better than that derived in (Malladi et al., 2022).   \n1123 Quadratic convex function This paragraph refers to the left and center-left of Figure 10. We   \n1124 optimize the function $\\textstyle f(x)={\\frac{x^{\\top}H x}{2}}$ where $H=\\mathrm{diag}(10,2)$ . We run Adam for 50000 epochs as we   \n1125 calculate the full gradient and inject it with Gaussian noise $Z\\sim\\mathcal{N}(0,\\sigma^{2}I_{d})$ where $\\sigma=0.01$ . The   \n1126 learning rate is $\\eta=0.001$ , $\\beta_{1}=0.9$ , and $\\beta_{2}=0.999$ . Similarly, we integrate our Adam SDE (Eq.   \n1127 (124)) and that of Malladi (Eq. (188)) with Euler-Maruyama (Algorithm 1) with $\\Delta t=\\eta$ . Results   \n1128 are averaged over 500 runs and the shaded areas are the average $\\pm$ the standard deviation: Our SDE   \n1129 matches Adam much better.   \n1130 Embedded saddle This paragraph refers to the center-right and right of Figure 10. We optimize the   \n1131 function $\\begin{array}{r}{f(x)=\\frac{x^{\\top}H x}{2}+\\frac{1}{4}\\lambda\\sum_{i=1}^{2}x_{i}^{4}-\\frac{\\xi}{3}\\sum_{i=1}^{2}x_{i}^{3}}\\end{array}$ where $H=\\mathrm{diag(-1,2)}$ , $\\lambda=1$ , and $\\xi=0.1$   \n1132 We run Adam as we calculate the full gradient and inject it with Gaussian noise $Z\\sim\\mathcal{N}(0,\\sigma^{2}I_{d})$   \n1133 where $\\sigma=0.1$ . The learning rate is $\\eta=0.001$ , $\\beta_{1}=0.9$ , and $\\beta_{2}=0.999$ . Similarly, we integrate   \n1134 our Adam SDE (Eq. (124)) and that of Malladi (Eq. (188)) with Euler-Maruyama (Algorithm 1) with   \n1135 $\\Delta t=\\eta$ . Results are averaged over 500 runs and the shaded areas are the average $\\pm$ the standard   \n1136 deviation: Our SDE matches Adam much better.   \n1137 DNN on Breast Cancer Dataset This paragraph refers to the left of Figure 11. The architecture   \n1138 and loss are the same as used above for SignSGD. We run Adam for 2000 epochs as we calculate the   \n1139 full gradient and inject it with Gaussian noise $Z\\sim\\mathcal{N}(0,\\sigma^{2}I_{d})$ where $\\sigma=10^{-2}$ . The learning rate   \n1140 is $\\eta=10^{-4}$ , $\\beta_{1}=\\overline{{0.99}}$ , and $\\beta_{2}=0.999$ . Similarly, we integrate our Adam SDE (Eq. (124)) and   \n1141 that of Malladi (Eq. (188)) with Euler-Maruyama (Algorithm 1) with $\\Delta t=\\eta$ . Results are averaged   \n1142 over 3 runs and the shaded areas are the average $\\pm$ the standard deviation: Our SDE matches Adam   \n1143 much better.   \n1144 CNN on MNIST This paragraph refers to the center-left of Figure 11. The architecture and loss are   \n1145 the same as used above for SignSGD. We run Adam for 2000 epochs as we calculate the full gradient   \n1146 and inject it with Gaussian noise $Z\\sim\\mathcal{N}(0,\\sigma^{2}I_{d})$ where $\\sigma=\\mathrm{\\dot{1}0^{-2}}$ . The learning rate is $\\eta=10^{-2}$ ,   \n1147 $\\beta_{1}=0.9$ , and $\\beta_{2}=0.99$ . Similarly, we integrate our Adam SDE (Eq. (124)) and that of Malladi (Eq.   \n1148 (188)) with Euler-Maruyama (Algorithm 1) with $\\Delta t=\\eta$ . Results are averaged over 3 runs and the   \n1149 shaded areas are the average $\\pm$ the standard deviation: Our SDE matches Adam much better.   \n1150 Transformer on MNIST This paragraph refers to the center-right of Figure 11. The architecture   \n1151 and loss are the same as used above for SignSGD. We run Adam for 2000 epochs as we calculate the   \n1152 full gradient and inject it with Gaussian noise $Z\\sim\\mathcal{N}(0,\\sigma^{2}I_{d})$ where $\\sigma=10^{-2}$ . The learning rate   \n1153 is $\\eta=10^{-2}$ , $\\beta_{1}=0.9$ , and $\\beta_{2}=0.99$ . Similarly, we integrate our Adam SDE (Eq. (124)) and that   \n1154 of Malladi (Eq. (188)) with Euler-Maruyama (Algorithm 1) with $\\Delta t=\\eta$ . Results are averaged over   \n1155 3 runs and the shaded areas are the average $\\pm$ the standard deviation: Our SDE matches Adam much   \n1156 better.   \n1157 ResNet on CIFAR-10 This paragraph refers to the right of Figure 11. The architecture and loss are   \n1158 the same as used above for SignSGD. We run Adam for 2000 epochs as we calculate the full gradient   \n1159 and inject it with Gaussian noise $Z\\sim\\mathcal{N}(0,\\sigma^{2}I_{d})$ where $\\sigma=\\mathrm{{}^{10^{-5}}}$ . The learning rate is $\\eta=10^{-5}$ ,   \n1160 $\\beta_{1}=0.99$ , and $\\beta_{2}=0.9999$ . Similarly, we integrate our Adam SDE (Eq. (124)) and that of Malladi   \n1161 (Eq. (188)) with Euler-Maruyama (Algorithm 1) with $\\Delta t=\\eta$ . Results are averaged over 3 runs and   \n1162 the shaded areas are the average $\\pm$ the standard deviation: Our SDE matches Adam much better. ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "1163 F.5 RMSpropW & AdamW: SDE validation (Figure 3, Figure 4) ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "1164 The settings are exactly the same as those for RMSprop and Adam. The regularization parameter   \n1165 used is always $\\gamma=0.01$ . We observe that our SDEs match the respective algorithm with a good   \n1166 agreement. ", "page_idx": 42}, {"type": "text", "text": "1167 F.6 RMSpropW & AdamW: insights validation (Figure 5) ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "1168 In this subsection, we describe the experiments we run to produce Figure 5: The theoretically   \n1169 predicted asymptotic loss value and moments of RMSpropW and AdamW match those empirically   \n1170 found.   \n1171 Asymptotic loss $\\&$ scaling rule of AdamW This paragraph refers to the left of Figure 5. We   \n1172 optimize the function $\\begin{array}{r}{f(x)=\\frac{x^{\\top}H x}{2}}\\end{array}$ where $H=\\mathrm{diag}(1,3)$ . We run AdamW for 20000 epochs as   \n1173 we calculate the full gradient and inject it with Gaussian noise $Z\\sim\\mathcal{N}(0,\\sigma^{2}I_{d})$ where $\\sigma=1$ . The   \n1174 learning rate is $\\eta=0.001$ , $\\beta_{1}=0.9$ , and $\\beta_{2}\\,=\\,0.999$ . Experiments are run for both $\\gamma=1$ and   \n1175 $\\gamma=4$ . The rescaled versions of the algorithms AdamW $R$ follow the novel scaling rule with $\\kappa=2$ .   \n1176 AdamW NR follows the scaling rule but not for $\\gamma$ which is left unchanged. We plot the evolution of   \n1177 the loss values with the theoretical predictions of Lemma C.28: Results are averaged over 500 runs.   \n1178 Asymptotic loss & scaling rule of RMSpropW This paragraph refers to the center-left of Figure   \n1179 5: The only difference with the previous paragraph is that we use RMSpropW with $\\beta=0.999$ .   \n11118801 tAhde afumnWct:i otnh $\\begin{array}{r}{f(x)\\,=\\,\\frac{x^{\\top}H x}{2}\\,+\\,\\frac{1}{4}\\lambda\\sum_{i=1}^{2}x_{i}^{4}\\,-\\,\\frac{\\xi}{3}\\sum_{i=1}^{2}x_{i}^{3}}\\end{array}$ $\\beta\\mathbf{s}$ ewhere $H\\,=\\,\\mathrm{diag}(-1,2)$ , $\\lambda\\,=\\,1$ , mainzde   \n. We run AdamW as we  calculate the full gradient and inject it with Gaussian noise   \n1183 $\\overline{{Z}}\\sim\\mathcal{N}(0,\\sigma^{2}I_{d})$ where $\\sigma\\,=\\,0.1$ . The learning rate is $\\eta=0.001$ , $\\gamma\\,=\\,0.1$ , $\\beta_{1}\\,\\in\\,\\{0.99,0.999\\}$ ,   \n1184 and $\\beta_{2}\\in\\{0.992,0.996,0.998\\}$ : Clearly, three combinations go into a minimum and three go into   \n1185 the other. For each minimum, the three optimizers converge to the same asymptotic loss value   \n1186 independently on the values of $\\beta_{1}$ and $\\beta_{2}$ . We argue that $\\beta_{1}$ , and $\\beta_{2}$ select the basin and the speed of   \n1187 convergence, not the asymptotic loss value: This is consistent with Lemma 3.13.   \n1188 Stationary distribution This paragraph refers to the right of Figure 5. We optimize the function   \n1189 $\\begin{array}{r}{f(x)=\\frac{x^{\\top}H x}{2}}\\end{array}$ x\u22a42Hxwhere H = diag(1, 3). We run Adam for 20000 epochs as we calculate the full   \n1190 gradient and inject it with Gaussian noise $Z\\sim\\mathcal{N}(0,\\sigma^{2}I_{d})$ where $\\sigma=0.01$ . The learning rate is   \n1191 $\\eta=0.001$ , $\\gamma=4$ , $\\beta=0.999$ , $\\beta_{1}\\,=\\,0.9$ , and $\\beta_{2}\\,=\\,0.999$ . We plot the evolution of the average   \n1192 variances with the theoretical predictions of Lemma C.24 and Lemma 3.14: Results are averaged   \n1193 over 100 runs. ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "1194 F.7 Effect of noise - validation (Figure 6) ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "1195 In this subsection, we describe the experiments run to produce Figure 6: All bounds on the asymptotic   \n1196 expected loss value for SGD, SignSGD, Adam, and AdamW are perfectly verified.   \n1197 We optimize the loss $\\textstyle f(x)={\\frac{x^{\\top}H x}{2}}$ where $H=\\mathrm{diag}(1,1)$ as we run each optimizer for 100000   \n1198 iterations with $\\eta=0.01$ . We repeat this procedure five times, one for each $\\sigma\\in\\{0.01,0.1,1,10,100\\}$ .   \n1199 As we train, we inject noise on the gradient as distributed as $\\mathcal{N}(0,\\sigma^{2}I_{d})$ . We plot the average loss   \n1200 together with the respective limits predicted by our Lemmas. For each optimizer and each $\\sigma$ , the   \n1201 average asymptotic loss matches the predicted limit. Therefore, we verify that the loss of SGD scales   \n1202 quadratically in $\\sigma$ , that of Adam and SignSGD scales linearly, and that of AdamW is limited in $\\sigma$ . ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "1203 F.8 Increasing weight decay with the batch size ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "1204 The analysis of Malladi et al. (2022) suggests \u221athat, when scaling batch size $B$ by a factor $\\kappa$ one has   \n1205 to scale up $(\\uparrow)$ the learning rate $\\eta$ by a factor $\\sqrt{\\kappa}$ and scale down (\u2193) $\\beta_{2}$ to the value $1-\\kappa(1-\\beta_{2})$ .   \n1206 Our SDE analysis confirms similar rules (Lemma \u221a3.13) but additionally suggests scaling up the   \n1207 decoupled weight decay parameter $\\gamma$ by a factor $\\sqrt{\\kappa}$ . We test this in two settings: VGG11 and   \n1208 ResNet34 (convolutional networks) on CIFAR-10 classification. We select a base batch size of 256,   \n1209 and run AdamW with $\\eta=0.001$ , $\\beta_{2}=0.99$ , and $\\gamma=0.1$ . We consider scaling the batch by a factor   \n1210 4: In Table 1, we show the effect of updating each hyperparameter with the proposed rule and we   \n1211 denote by a \u201c\u00b7\u201d the model parameters of the base run with $B=256$ . We train for 150 epochs the   \n1212 model with $B=256$ , and $150\\times4$ the model with $B=4\\times256$ . Experiments are repeated 3 times.   \n1213 We find that, while improvements are marginal, they are consistent with our theoretical results. ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 43}, {"type": "table", "img_path": "nfC1OA6NeE/tmp/ef86e59b4c14d6d947c933aa53d58916031789e55da977b5ef8a4a345d891dff.jpg", "table_caption": ["Table 1: Scaling with the batch size: Effect of adapting AdamW hyperparameters. "], "table_footnote": [], "page_idx": 43}, {"type": "text", "text": "1215 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "1216 1. Claims   \n1217 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n1218 paper\u2019s contributions and scope?   \n1219 Answer: [Yes]   \n1220 Justification: The abstract is a high-level description of what we achieve. The results are   \n1221 clearly presented in Section 3 and validated in the figures. Details are in the appendix.   \n1222 Guidelines:   \n1223 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n1224 made in the paper.   \n1225 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n1226 contributions made in the paper and important assumptions and limitations. A No or   \n1227 NA answer to this question will not be perceived well by the reviewers.   \n1228 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n1229 much the results can be expected to generalize to other settings.   \n1230 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n1231 are not attained by the paper.   \n1232 2. Limitations   \n1233 Question: Does the paper discuss the limitations of the work performed by the authors?   \n1234 Answer: [Yes]   \n1235 Justification: See Section C.1.   \n1236 Guidelines:   \n1237 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n1238 the paper has limitations, but those are not discussed in the paper.   \n1239 \u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n1240 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n1241 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n1242 model well-specification, asymptotic approximations only holding locally). The authors   \n1243 should reflect on how these assumptions might be violated in practice and what the   \n1244 implications would be.   \n1245 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n1246 only tested on a few datasets or with a few runs. In general, empirical results often   \n1247 depend on implicit assumptions, which should be articulated.   \n1248 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n1249 For example, a facial recognition algorithm may perform poorly when image resolution   \n1250 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n1251 used reliably to provide closed captions for online lectures because it fails to handle   \n1252 technical jargon.   \n1253 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n1254 and how they scale with dataset size.   \n1255 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n1256 address problems of privacy and fairness.   \n1257 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n1258 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n1259 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n1260 judgment and recognize that individual actions in favor of transparency play an impor  \n1261 tant role in developing norms that preserve the integrity of the community. Reviewers   \n1262 will be specifically instructed to not penalize honesty concerning limitations.   \n1263 3. Theory Assumptions and Proofs   \n1264 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n1265 a complete (and correct) proof? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: In the main paper, Theorems, Lemmas, and Corollaries state the assumptions and theses. Sometimes, these are simplified for the sake of clarity: Complete and formal statements including proofs are in the Appendices. ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 45}, {"type": "text", "text": "1281 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "82 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n83 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n84 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: We provide all the hyperparameters necessary to replicate our experiments.   \nDatasets are all publicly available: Breast Cancer, MNIST, and CIFAR-10. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of   \n2 whether the code and data are provided or not. \u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. \u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may   \n8 be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case   \n2 of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. \u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how   \n8 to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in   \n8 some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 45}, {"type": "text", "text": "1320 5. Open access to data and code ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "1321   \n1322   \n1323   \n1324   \n1325   \n1326   \n1327   \n1328   \n1329   \n1330   \n1331   \n1332   \n1333   \n1334   \n1335   \n1336   \n1337   \n1338   \n1339   \n1340   \n1341   \n1342   \n1343   \n1344   \n1345   \n1346   \n1347   \n1348   \n1349   \n1350   \n1351   \n1352   \n1353   \n1354   \n1355   \n1356   \n1357   \n1358   \n1359   \n1360   \n1361   \n1362   \n1363   \n1364   \n1365   \n1366   \n1367   \n1368   \n1369   \n1370 ", "page_idx": 46}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Most of the codes have been released in the supplementary material. The missing ones are simply the implementations of the numerical integration of the SDEs, which consist of applying Euler-Maruyama: All code will be released in an appropriate GitHub repository upon publication. Guidelines: \u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. \u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). \u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. \u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. \u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. \u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.   \n6. Experimental Setting/Details Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We describe all the experimental settings in Section F. Guidelines: \u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material.   \n7. Experiment Statistical Significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Our figures report error bars when relevant. Guidelines: \u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 46}, {"type": "text", "text": "1371 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n1372 example, train/test split, initialization, random drawing of some parameter, or overall   \n1373 run with given experimental conditions).   \n1374 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n1375 call to a library function, bootstrap, etc.)   \n1376 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n1377 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n1378 of the mean.   \n1379 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n1380 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n1381 of Normality of errors is not verified.   \n1382 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n1383 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n1384 error rates).   \n1385 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n1386 they were calculated and reference the corresponding figures or tables in the text.   \n1387 8. Experiments Compute Resources   \n1388 Question: For each experiment, does the paper provide sufficient information on the com  \n1389 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n1390 the experiments?   \n1391 Answer: [Yes]   \n1392 Justification: As we state in Section F, we run our experiments on an NVIDIA GeForce   \n1393 RTX 3090.   \n1394 Guidelines:   \n1395 \u2022 The answer NA means that the paper does not include experiments.   \n1396 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n1397 or cloud provider, including relevant memory and storage.   \n1398 \u2022 The paper should provide the amount of compute required for each of the individual   \n1399 experimental runs as well as estimate the total compute.   \n1400 \u2022 The paper should disclose whether the full research project required more compute   \n1401 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n1402 didn\u2019t make it into the paper).   \n1403 9. Code Of Ethics   \n1404 Question: Does the research conducted in the paper conform, in every respect, with the   \n1405 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n1406 Answer: [Yes]   \n1407 Justification: All we do is derive some convergence bounds and similar results.   \n1408 Guidelines:   \n1409 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n1410 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n1411 deviation from the Code of Ethics.   \n1412 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n1413 eration due to laws or regulations in their jurisdiction).   \n1414 10. Broader Impacts   \n1415 Question: Does the paper discuss both potential positive societal impacts and negative   \n1416 societal impacts of the work performed?   \n1417 Answer: [Yes]   \n1418 Justification: It can have a positive impact as it helps understand adaptive optimizers better.   \n1419 Possibly, it might help reduce the cost of fine-tuning thanks to our novel scaling law.   \n1420 Guidelines:   \n1421 \u2022 The answer NA means that there is no societal impact of the work performed.   \n1422 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n1423 impact or why the paper does not address societal impact.   \n1424 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n1425 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n1426 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n1427 groups), privacy considerations, and security considerations.   \n1428 \u2022 The conference expects that many papers will be foundational research and not tied   \n1429 to particular applications, let alone deployments. However, if there is a direct path to   \n1430 any negative applications, the authors should point it out. For example, it is legitimate   \n1431 to point out that an improvement in the quality of generative models could be used to   \n1432 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n1433 that a generic algorithm for optimizing neural networks could enable people to train   \n1434 models that generate Deepfakes faster.   \n1435 \u2022 The authors should consider possible harms that could arise when the technology is   \n1436 being used as intended and functioning correctly, harms that could arise when the   \n1437 technology is being used as intended but gives incorrect results, and harms following   \n1438 from (intentional or unintentional) misuse of the technology.   \n1439 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n1440 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n1441 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n1442 feedback over time, improving the efficiency and accessibility of ML).   \n1443 11. Safeguards   \n1444 Question: Does the paper describe safeguards that have been put in place for responsible   \n1445 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n1446 image generators, or scraped datasets)?   \n1447 Answer: [NA]   \n1448 Justification: The paper poses no such risks.   \n1449 Guidelines:   \n1450 \u2022 The answer NA means that the paper poses no such risks.   \n1451 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n1452 necessary safeguards to allow for controlled use of the model, for example by requiring   \n1453 that users adhere to usage guidelines or restrictions to access the model or implementing   \n1454 safety filters.   \n1455 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n1456 should describe how they avoided releasing unsafe images.   \n1457 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n1458 not require this, but we encourage authors to take this into account and make a best   \n1459 faith effort.   \n1460 12. Licenses for existing assets   \n1461 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n1462 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n1463 properly respected?   \n1464 Answer: [Yes]   \n1465 Justification: We cite the used datasets. The rest is all our code and we cite the most relevant   \n1466 libraries used.   \n1467 Guidelines:   \n1468 \u2022 The answer NA means that the paper does not use existing assets.   \n1469 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n1470 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n1471 URL.   \n1472 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n1473 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n1474 service of that source should be provided. ", "page_idx": 47}, {"type": "text", "text": "", "page_idx": 48}, {"type": "text", "text": "5 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n6 package should be provided. For popular datasets, paperswithcode.com/datasets   \n7 has curated licenses for some datasets. Their licensing guide can help determine the   \n8 license of a dataset.   \n9 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n0 the derived asset (if it has changed) should be provided. \u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 49}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 49}, {"type": "text", "text": "486 Answer: [NA]   \n487 Justification: The paper does not release new assets   \n488 Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 49}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 49}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 49}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 49}, {"type": "text", "text": "4 Question: Does the paper describe potential risks incurred by study participants, whether 5 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 6 approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 49}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 49}, {"type": "text", "text": "", "page_idx": 50}]