{"references": [{"fullname_first_author": "Li", "paper_title": "Stochastic modified equations and adaptive stochastic gradient algorithms", "publication_date": "2017-07-01", "reason": "This paper introduced the formal theoretical framework for deriving SDEs to effectively model the inherent stochastic nature of optimizers, which is the foundation of this paper's methodology."}, {"fullname_first_author": "Malladi", "paper_title": "On the SDEs and scaling rules for adaptive gradient algorithms", "publication_date": "2022-07-01", "reason": "This paper is the closest related work to the present paper, making its comparison essential to highlight the novel contributions of the current study."}, {"fullname_first_author": "Zhang", "paper_title": "Why are adaptive methods good for attention models?", "publication_date": "2020-07-01", "reason": "This paper demonstrated the advantage of adaptive optimization methods, such as Adam, over SGD in handling heavy-tailed gradient noise, specifically in the context of attention models, which is an important motivation for this paper."}, {"fullname_first_author": "Kingma", "paper_title": "Adam: A method for stochastic optimization", "publication_date": "2015-07-01", "reason": "Adam is one of the most commonly used adaptive optimizers, and its properties are directly relevant to the analysis and understanding of adaptive optimization methods in this work."}, {"fullname_first_author": "Bernstein", "paper_title": "signSGD: Compressed optimisation for non-convex problems", "publication_date": "2018-07-01", "reason": "SignSGD is another commonly used adaptive optimizer; this paper analyzes its properties and provides insights into its behavior, which are directly relevant to the current work."}]}