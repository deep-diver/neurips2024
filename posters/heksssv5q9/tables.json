[{"figure_path": "hEKSSsv5Q9/tables/tables_5_1.jpg", "caption": "Table 1: Detection accuracy comparison on three source models ChatGPT (GPT-3.5-Turbo-0301), GPT-4 (GPT-4-0613) and Claude-3 (claude-3-opus-20240229). Our method surpasses previous methods on all passages generated from different source models.", "description": "This table presents the performance comparison of different detection methods on three different Large Language Models (LLMs): ChatGPT, GPT-4, and Claude-3. The models are evaluated on three datasets: PubMed, XSum, and Writing, each representing a different text genre.  The table shows the Area Under the Receiver Operating Characteristic (AUROC) for each method on each dataset and LLM.  The results demonstrate that the proposed DALD method outperforms existing methods across all datasets and LLMs.", "section": "4.2 Main Results"}, {"figure_path": "hEKSSsv5Q9/tables/tables_7_1.jpg", "caption": "Table 2: The results comparison of our method trained with the combination of different data sources. Our method achieves comparable results with more data sources, demonstrating the generalizability of our method and potentially leading to training a universal surrogate model for all closed-source models. \u2020: Train the surrogate model separately to each test set. *: Train one surrogate model for all test sets.", "description": "This table presents the results of the DALD method trained on different combinations of data sources. It compares the performance of the model trained on a single dataset, multiple datasets, and one universal model trained on all datasets.  The goal is to demonstrate the generalizability and scalability of the DALD approach by showing that training on multiple datasets doesn't harm performance and may even improve it, suggesting the potential to train a single model that can effectively detect various LLMs.", "section": "4 Experiments"}, {"figure_path": "hEKSSsv5Q9/tables/tables_7_2.jpg", "caption": "Table 3: Results comparison of our method with different surrogate models on Claude-3. The performance improvement with our method on different surrogate models shows that our method can be adapted to any open-source surrogate model.", "description": "This table compares the performance of the proposed DALD method using different surrogate models (Llama2-7B, Llama3-8B, and GPT-Neo-2.7B) against their respective baselines on three datasets (PubMed, XSum, and Writing).  It demonstrates that DALD improves the performance regardless of the selected surrogate model, highlighting its adaptability and robustness.", "section": "4 Experiments"}, {"figure_path": "hEKSSsv5Q9/tables/tables_8_1.jpg", "caption": "Table 4: Ablation study. We report the results comparison of the baseline method and the method with our DALD. The improvement upon all baselines shows the effectiveness of our DALD.", "description": "This table presents the ablation study results, comparing the performance of baseline methods (Detect-GPT, DNA-GPT, Fast-DetectGPT) against their corresponding versions enhanced by the proposed DALD framework.  The improvement in performance across all baselines highlights the effectiveness of DALD in boosting the accuracy of existing logits-based LLM detection methods.", "section": "4.3 Experimental Analysis"}, {"figure_path": "hEKSSsv5Q9/tables/tables_8_2.jpg", "caption": "Table 5: Results comparison on Non-English texts.", "description": "This table presents the performance comparison of three different methods (DNA-GPT, Fast-DetectGPT, and DALD) on detecting German texts generated by GPT-4.  The AUROC scores are reported for each method, demonstrating the superior performance of DALD in identifying German language model-generated content compared to the baseline methods.", "section": "4.3 Experimental Analysis"}, {"figure_path": "hEKSSsv5Q9/tables/tables_9_1.jpg", "caption": "Table 6: Results on open-source models Llama-3, Llama-3.1, and Mistral across three datasets: PubMed (PM), XSum, and Writing. We compare DALD with Fast-DetectGPT(Fast).", "description": "This table presents the performance comparison of the proposed DALD method against the Fast-DetectGPT method on three open-source large language models (LLMs): Llama-3, Llama-3.1, and Mistral.  The evaluation is performed across three different datasets: PubMed, XSum, and Writing.  The results are shown in terms of AUROC scores for each LLM and dataset combination, highlighting the relative performance of DALD compared to the baseline.", "section": "4 Experiments"}, {"figure_path": "hEKSSsv5Q9/tables/tables_16_1.jpg", "caption": "Table 1: Detection accuracy comparison on three source models ChatGPT (GPT-3.5-Turbo-0301), GPT-4 (GPT-4-0613) and Claude-3 (claude-3-opus-20240229). Our method surpasses previous methods on all passages generated from different source models.", "description": "This table presents the results of a comparative study evaluating the performance of various methods in detecting text generated by three different large language models (LLMs): ChatGPT, GPT-4, and Claude-3.  The models were tested on various datasets (PubMed, XSum, Writing), and the table compares the Area Under the ROC Curve (AUROC) achieved by different detection methods, including several baseline methods and the proposed method, DALD.  The results demonstrate the superior performance of the proposed DALD method across all tested LLMs and datasets.", "section": "4. Main Results"}, {"figure_path": "hEKSSsv5Q9/tables/tables_17_1.jpg", "caption": "Table 8: Full results of ChatGPT(GPT-3.5-Turbo) on PubMed, XSum and Writing as the complementary results to Table 1.", "description": "This table presents the complete detection accuracy results for ChatGPT (GPT-3.5-Turbo) across three datasets: PubMed, XSum, and Writing.  It complements the results shown in Table 1 by providing a more detailed breakdown of the performance of different detection methods (DNA-GPT, Fast-DetectGPT, and DALD) on this specific LLM model. The table showcases the Area Under the Receiver Operating Characteristic (AUROC) scores, indicating the effectiveness of each method in distinguishing between human-written and machine-generated text.", "section": "4.2 Main Results"}, {"figure_path": "hEKSSsv5Q9/tables/tables_17_2.jpg", "caption": "Table 1: Detection accuracy comparison on three source models ChatGPT (GPT-3.5-Turbo-0301), GPT-4 (GPT-4-0613) and Claude-3 (claude-3-opus-20240229). Our method surpasses previous methods on all passages generated from different source models.", "description": "This table presents the AUROC scores achieved by various methods (including the proposed DALD) on three different LLMs (ChatGPT, GPT-4, and Claude-3) across three text datasets (PubMed, XSum, and Writing).  The results demonstrate the superior performance of the DALD method compared to existing state-of-the-art (SOTA) approaches for detecting AI-generated text, even when the specific LLM used to generate the text is unknown (black-box setting).", "section": "4.2 Main Results"}, {"figure_path": "hEKSSsv5Q9/tables/tables_18_1.jpg", "caption": "Table 10: Results of code detection on the APPS dataset.", "description": "This table presents the results of code detection experiments using the APPS dataset. It compares the performance of two methods, Fast-DetectGPT and the proposed DALD method, in detecting AI-generated code.  The metric used is likely AUROC (Area Under the Receiver Operating Characteristic curve), a common measure of classification performance, although this is not explicitly stated in the provided text snippet.  The table shows a significant improvement in performance using the DALD method compared to Fast-DetectGPT.", "section": "4.2 Main Results"}, {"figure_path": "hEKSSsv5Q9/tables/tables_18_2.jpg", "caption": "Table 11: Results across different text genres using the RAID dataset.", "description": "This table compares the performance of Fast-DetectGPT and DALD on various text genres using the RAID dataset.  The RAID dataset is a domain-specific dataset that consists of different types of text. The table shows that DALD outperforms Fast-DetectGPT in all text genres, demonstrating its robustness and generalizability.", "section": "4.3 Experimental Analysis"}, {"figure_path": "hEKSSsv5Q9/tables/tables_18_3.jpg", "caption": "Table 1: Detection accuracy comparison on three source models ChatGPT (GPT-3.5-Turbo-0301), GPT-4 (GPT-4-0613) and Claude-3 (claude-3-opus-20240229). Our method surpasses previous methods on all passages generated from different source models.", "description": "This table presents the AUROC scores achieved by different methods for detecting AI-generated text from three different models: ChatGPT (GPT-3.5-Turbo-0301), GPT-4 (GPT-4-0613), and Claude-3 (claude-3-opus-20240229).  The results are broken down by dataset (PubMed, XSum, Writing) and method.  The table highlights that the proposed DALD method significantly outperforms existing methods in all cases, demonstrating its superior performance across various models and datasets.", "section": "4.2 Main Results"}, {"figure_path": "hEKSSsv5Q9/tables/tables_19_1.jpg", "caption": "Table 1: Detection accuracy comparison on three source models ChatGPT (GPT-3.5-Turbo-0301), GPT-4 (GPT-4-0613) and Claude-3 (claude-3-opus-20240229). Our method surpasses previous methods on all passages generated from different source models.", "description": "This table presents the performance comparison of different LLM detection methods on three different large language models (ChatGPT, GPT-4, and Claude-3).  The performance is measured across three different datasets (PubMed, XSum, and Writing).  The table highlights that the proposed method, DALD, outperforms existing methods in detecting AI-generated text from each of these models.", "section": "4.2 Main Results"}, {"figure_path": "hEKSSsv5Q9/tables/tables_20_1.jpg", "caption": "Table 1: Detection accuracy comparison on three source models ChatGPT (GPT-3.5-Turbo-0301), GPT-4 (GPT-4-0613) and Claude-3 (claude-3-opus-20240229). Our method surpasses previous methods on all passages generated from different source models.", "description": "This table presents a comparison of the detection accuracy of several methods for identifying AI-generated text from three different large language models (LLMs): ChatGPT, GPT-4, and Claude-3. The accuracy is evaluated across three different datasets (PubMed, XSum, and Writing) for both human-written and machine-generated text.  The table shows that the proposed DALD method outperforms existing state-of-the-art methods in all cases.", "section": "4.2 Main Results"}]