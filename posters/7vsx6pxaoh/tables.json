[{"figure_path": "7vsx6PxAOH/tables/tables_3_1.jpg", "caption": "Table 1: Effects of path complexity", "description": "This table presents the results of an ablation study on the complexity of different MLP paths used in the Multi-Path Aggregation (MPA) model.  It shows how different MLP path types (Bottleneck, Inverted Bottleneck) affect the performance of different tasks (MSE, Classification, Segmentation) as measured by BD-Rate, Accuracy, and mIoU, respectively. This helps to understand the impact of different path complexities on overall model performance for various downstream tasks.", "section": "5 Experiments"}, {"figure_path": "7vsx6PxAOH/tables/tables_8_1.jpg", "caption": "Table 1: Effects of path complexity", "description": "This table presents the results of an ablation study conducted to evaluate the impact of different Multi-Layer Perceptron (MLP) architectures on the overall performance of the proposed Multi-Path Aggregation (MPA) method. Specifically, it compares the performance of using bottleneck MLPs versus inverted bottleneck MLPs for the MSE, classification (Cls), and segmentation (Seg) tasks. The metrics used for evaluation include BD-Rate (lower is better), accuracy (higher is better), and mIoU (higher is better).", "section": "5 Experiments"}, {"figure_path": "7vsx6PxAOH/tables/tables_8_2.jpg", "caption": "Table 2: Cross-validations on path choices", "description": "This table presents the results of cross-validation experiments conducted to evaluate the performance of different path choices in the Multi-Path Aggregation (MPA) model.  The table shows the performance metrics (BD-Rate, Accuracy, and mIoU) for the MSE, Classification, and Segmentation tasks when using different combinations of paths, such as the perceptual path (perc), MSE path (MSE), classification path (cls), and segmentation path (seg).  It demonstrates how the selection of paths affects the overall performance of the model for each task. The lowest BD-Rate values are in bold.", "section": "5 Experiments"}, {"figure_path": "7vsx6PxAOH/tables/tables_8_3.jpg", "caption": "Table 4: Comparison of complexity", "description": "This table compares the model complexity (number of parameters, KFLOPs per pixel, and latency) of the proposed Multi-Path Aggregation (MPA) method with other methods such as MRIC and TinyLIC.  It shows the impact of adding different components to the MPA architecture on its computational cost.", "section": "5 Experiments"}, {"figure_path": "7vsx6PxAOH/tables/tables_16_1.jpg", "caption": "Table 5: Comparison between MPA and MoE", "description": "This table compares the proposed Multi-Path Aggregation (MPA) method with the Mixture-of-Experts (MoE) approach in terms of architecture, optimization strategy, feature routing, efficiency, and usage.  MPA uses a unified all-in-one model with task-specific paths, employing a two-stage optimization process that fine-tunes only a small portion of the parameters.  It leverages feature importance and correlations to efficiently allocate resources across different tasks, making it suitable for resource-constrained multi-task scenarios. In contrast, MoE uses multiple expert models selected by a gating network, requiring joint optimization and potentially leading to a large number of parameters.  Expert allocation is fully determined by the gating network, unlike MPA's user-defined task path selection.", "section": "C Comparison to other related work"}, {"figure_path": "7vsx6PxAOH/tables/tables_16_2.jpg", "caption": "Table 6: R-D performance comparison between MPA and LoRA. The ranks r of LoRA are set to 64, 16, and 4, respectively, and the lora_alpha (NOT the a in MPA) is fixed to 1. We optimize MSE paths for low distortion. Other experimental settings are consistent with those in Sec. 4.", "description": "This table compares the rate-distortion performance of the proposed Multi-Path Aggregation (MPA) method with the Low-Rank Adaptation (LoRA) method.  It shows the BD-rate reduction achieved by each method against the Versatile Video Coding (VTM) baseline on the Kodak and CLIC datasets.  Different rank values for LoRA are tested to explore the impact of model complexity on performance.  The MSE path is optimized for low distortion in both methods.", "section": "5.2 Results of multi-task performance"}, {"figure_path": "7vsx6PxAOH/tables/tables_17_1.jpg", "caption": "Table 7: Ablations on loss terms in Ltask. We use the same experimental settings as in Sec. 4, and re-train the classification path for each case. Metrics are evaluated at 0.1521bpp on ImageNet-1K [14]. The top-3 results are underlined. LMSE, Lperc and Lce denote MSE loss, perceptual loss and cross-entropy loss respectively.", "description": "This table presents the ablation study on the loss terms used for training the classification path in the proposed MPA model.  It shows the impact of each loss term (MSE, perceptual, and cross-entropy) on various metrics, including PSNR, LPIPS, and top-1 accuracy on two different datasets (ConvNeXt-T and SwinV2-T). The results are evaluated at a specific bitrate (0.1521bpp) and the top-3 performing combinations are highlighted.", "section": "D Loss functions"}, {"figure_path": "7vsx6PxAOH/tables/tables_17_2.jpg", "caption": "Table 5: Comparison between MPA and MoE", "description": "This table compares Multi-Path Aggregation (MPA) and Mixture-of-Experts (MoE) approaches across several key aspects.  It highlights the differences in architecture (unified vs. multiple expert models), optimization strategies (two-stage partial vs. joint optimization), routing mechanisms (leveraging feature importance and correlations vs. gating networks), efficiency (designed for storage-sensitive scenarios vs. large parameter models), and usage (user-defined vs. automatic allocation).  The table clarifies how MPA, designed for storage-efficient multi-task coding, contrasts with the more resource-intensive MoE approach.", "section": "C Comparison to other related work"}]