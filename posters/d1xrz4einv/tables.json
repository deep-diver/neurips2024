[{"figure_path": "d1XrZ4EINV/tables/tables_2_1.jpg", "caption": "Table 1: Number of unique, correct, wrong solutions sampled from pre-trained LLMs, as well as the number of correct refinement generated by GPT-3.5-Turbo and its refinement rate on each dataset.", "description": "This table presents the results of data collection using pre-trained LLMs.  For each dataset (MBPP Training, APPS Training, and CodeContest), it shows the number of unique problems, the number of correct solutions generated by the LLMs, the number of incorrect solutions, the number of successfully refined incorrect solutions by GPT-3.5-Turbo, and the success rate of refinement for each dataset.", "section": "2.1 Data collection and verification"}, {"figure_path": "d1XrZ4EINV/tables/tables_5_1.jpg", "caption": "Table 2: Pass@k of initial and refined solutions on four benchmarks. Each backbone's best performance on every benchmark is bolded.", "description": "This table presents the performance of different LLMs (StarCoder-15B, CodeLlama-7B, and CodeLlama-13B) on four code generation benchmarks (MBPP, HumanEval, MBPP+, and HumanEval+).  The results are shown for initial solutions, solutions refined directly, and solutions refined with explanations.  The \"pass@k\" metric indicates the percentage of tasks for which the model generates a correct solution within the top k attempts.  The bolded numbers highlight the best performance for each LLM on each benchmark.", "section": "4.1 RQ1: Code generation and refinement capability"}, {"figure_path": "d1XrZ4EINV/tables/tables_5_2.jpg", "caption": "Table 2: Pass@k of initial and refined solutions on four benchmarks. Each backbone's best performance on every benchmark is bolded.", "description": "This table presents the results of the Pass@k metric for both initial and refined solutions across four different benchmarks (MBPP, HumanEval, MBPP+, and HumanEval+).  The table is broken down by model architecture (StarCoder-15B, CodeLlama-7B, CodeLlama-13B), approach (initial solution, refinement only, explanation and refinement), and metric (pass@1, pass@10). The best performance achieved by each model on each benchmark is highlighted in bold.  This allows for a direct comparison of the effectiveness of different models and approaches at generating accurate code solutions, both initially and through iterative refinement.", "section": "4.1 RQ1: Code generation and refinement capability"}, {"figure_path": "d1XrZ4EINV/tables/tables_6_1.jpg", "caption": "Table 4: Success refinement rates over four benchmarks. Blue numbers show the improvement.", "description": "This table presents the success rates of code refinement across four benchmarks (MBPP, HumanEval, MBPP+, HumanEval+) for three different LLMs (StarCoder-15B, CodeLlama-7B, CodeLlama-13B).  The success rate is calculated as the percentage of incorrect initial solutions that were successfully refined to a correct solution.  Three approaches are compared: prompting, supervised fine-tuning (SFT) with LEDEX, and reinforcement learning (RL) with LEDEX. The blue numbers highlight the percentage point improvement gained by each technique over the prompting approach.", "section": "4.1 RQ1: Code generation and refinement capability"}, {"figure_path": "d1XrZ4EINV/tables/tables_7_1.jpg", "caption": "Table 2: Pass@k of initial and refined solutions on four benchmarks. Each backbone's best performance on every benchmark is bolded.", "description": "This table presents the performance of different LLMs on four code generation benchmarks (MBPP, HumanEval, MBPP+, HumanEval+) using three different approaches: initial solutions (without refinement), refinement using a single attempt, and refinement with explanation.  The Pass@k metric shows the percentage of times the correct code was generated within the top k attempts. The table showcases the performance improvements achieved after employing the self-debugging method, specifically after applying supervised fine-tuning (SFT) and reinforcement learning (RL). The bolded values indicate the best performance achieved by each model architecture on each benchmark.", "section": "4.1 RQ1: Code generation and refinement capability"}, {"figure_path": "d1XrZ4EINV/tables/tables_7_2.jpg", "caption": "Table 3: Overall pass@k on MBPP & HumanEval and MBPP+ & HumanEval+. Blue or red numbers show the improvement or deterioration: SFT is compared to prompting, and RL is compared to SFT.", "description": "This table presents the overall performance of different LLMs across four benchmarks (MBPP, HumanEval, MBPP+, HumanEval+) after applying supervised fine-tuning (SFT) and reinforcement learning (RL).  It shows the improvement in pass@1 and pass@10 metrics, comparing the results of using prompting alone, SFT, and RL.  The color-coding (blue for improvement, red for deterioration) highlights the performance gains or losses at each stage of training.", "section": "4 Result"}, {"figure_path": "d1XrZ4EINV/tables/tables_8_1.jpg", "caption": "Table 2: Pass@k of initial and refined solutions on four benchmarks. Each backbone's best performance on every benchmark is bolded.", "description": "This table presents the performance of three different large language models (LLMs) on four code generation benchmarks.  The models were tested with three different approaches: initial solution generation, refinement using only refinement prompts, and refinement using explanation and refinement prompts.  The results are given in terms of Pass@1 and Pass@10 metrics, showing the percentage of times the model produced a correct solution within the top 1 and top 10 solutions generated, respectively. The table highlights the best performance achieved by each LLM on each benchmark using each method. The bolded numbers represent the best performance among all methods for each model and benchmark.", "section": "4.1 RQ1: Code generation and refinement capability"}, {"figure_path": "d1XrZ4EINV/tables/tables_8_2.jpg", "caption": "Table 8: Overall pass@k on MBPP & HumanEval and MBPP+ & HumanEval+, trained with self-bootstrapped data. Blue or red numbers show the improvement or deterioration.", "description": "This table presents the overall performance of CodeLlama-7B model trained using self-bootstrapped data (data generated by the model itself). It compares the performance of the model using three different approaches: prompting, supervised fine-tuning (SFT), and reinforcement learning (RL). The performance is measured using the pass@1 and pass@10 metrics across four benchmarks: MBPP, HumanEval, MBPP+, and HumanEval+.  Blue numbers indicate improvements over the previous method, while red numbers indicate performance degradation.", "section": "4.1 RQ1: Code generation and refinement capability"}, {"figure_path": "d1XrZ4EINV/tables/tables_9_1.jpg", "caption": "Table 9: Average scores of code explanations rated by GPT-4 and developers. SC for StarCoder and CL for CodeLlama. \"-\" refers to not applied.", "description": "This table presents the average scores given by GPT-4 and human developers to the code explanations generated by different LLMs (StarCoder-15B, CodeLlama-7B, CodeLlama-13B, and GPT-3.5-Turbo) using three different approaches: prompting, supervised fine-tuning (SFT), and reinforcement learning (RL).  The scores range from 1 to 5, where 1 indicates a completely incorrect or misleading explanation and 5 denotes a correct explanation that also provides helpful hints.  The table allows for a comparison of explanation quality across different models and training methods.", "section": "4.4 RQ4: Quality of generated explanation"}, {"figure_path": "d1XrZ4EINV/tables/tables_13_1.jpg", "caption": "Table 10: The success rate of self-refinement using greedy decoding.", "description": "This table presents the success rate of self-refinement for four different LLMs (StarCoder-15B, CodeLlama-7B, CodeLlama-13B, and GPT-3.5) using two different prompting strategies:  one where the LLM directly generates the refinement and another where it first explains why the code is incorrect before generating the refinement.  The results are shown for two benchmarks, MBPP and HumanEval.  The table highlights the limited self-refinement capabilities of open-source LLMs compared to GPT-3.5.", "section": "A.1 Limited refinement ability of open-sourced LLMs"}, {"figure_path": "d1XrZ4EINV/tables/tables_16_1.jpg", "caption": "Table 2: Pass@k of initial and refined solutions on four benchmarks. Each backbone's best performance on every benchmark is bolded.", "description": "This table presents the performance of different LLMs (StarCoder-15B, CodeLlama-7B, and CodeLlama-13B) on four code generation benchmarks (MBPP, HumanEval, MBPP+, and HumanEval+).  It shows the pass@1 and pass@10 scores for both the initial solutions generated by the LLMs and the refined solutions obtained after applying the LEDEX framework.  The \"best\" performance for each LLM on each benchmark is highlighted in bold.  The table also shows results for different prompting approaches (\"Init.\", \"Refine\", \"Expl.+Refine\") and training methods (SFT, RL).", "section": "4.1 RQ1: Code generation and refinement capability"}, {"figure_path": "d1XrZ4EINV/tables/tables_17_1.jpg", "caption": "Table 2: Pass@k of initial and refined solutions on four benchmarks. Each backbone's best performance on every benchmark is bolded.", "description": "This table presents the performance of different LLMs (StarCoder-15B, CodeLlama-7B, and CodeLlama-13B) on four code generation benchmarks (MBPP, HumanEval, MBPP+, and HumanEval+).  It shows the pass@1 and pass@10 scores for both the initial solutions generated by the LLMs and the refined solutions obtained after applying different self-debugging techniques (prompting, supervised fine-tuning (SFT), and reinforcement learning (RL)).  The best performance for each LLM on each benchmark is highlighted in bold, allowing for easy comparison of the effectiveness of different approaches.", "section": "4.1 RQ1: Code generation and refinement capability"}, {"figure_path": "d1XrZ4EINV/tables/tables_17_2.jpg", "caption": "Table 3: Overall pass@k on MBPP & HumanEval and MBPP+ & HumanEval+. Blue or red numbers show the improvement or deterioration: SFT is compared to prompting, and RL is compared to SFT.", "description": "This table presents the overall performance of different LLMs (StarCoder-15B, CodeLlama-7B, and CodeLlama-13B) across four benchmarks (MBPP, HumanEval, MBPP+, and HumanEval+) using three different approaches (Prompting, Supervised Fine-Tuning (SFT), and Reinforcement Learning (RL)).  The pass@1 and pass@10 metrics are shown, indicating the percentage of tasks where the model generated a correct solution within the top 1 and top 10 attempts, respectively.  Blue numbers highlight the improvements observed in each approach, comparing SFT against prompting and RL against SFT.  The table helps in understanding the effectiveness of the LEDEX framework in improving LLMs\u2019 code generation and refinement capabilities.", "section": "4 Result"}, {"figure_path": "d1XrZ4EINV/tables/tables_17_3.jpg", "caption": "Table 4: Success refinement rates over four benchmarks. Blue numbers show the improvement.", "description": "This table shows the success rates of code refinement for three different LLMs (StarCoder-15B, CodeLlama-7B, and CodeLlama-13B) across four benchmark datasets.  It compares three different approaches: prompting, supervised fine-tuning (SFT) with LEDEX, and reinforcement learning (RL) with LEDEX.  The \"Refine\" column shows results where the model is directly prompted to refine code, while the \"Explain + Refine\" column shows results where the model first explains the bug and then refines the code. The blue numbers highlight the improvement in success rate achieved by SFT and RL compared to the baseline prompting method.", "section": "4.1 RQ1: Code generation and refinement capability"}, {"figure_path": "d1XrZ4EINV/tables/tables_18_1.jpg", "caption": "Table 9: Average scores of code explanations rated by GPT-4 and developers. SC for StarCoder and CL for CodeLlama. \"-\" refers to not applied.", "description": "This table presents the average scores given by GPT-4 and human developers to code explanations generated by different LLMs.  The scores range from 1 to 5, with higher scores indicating better correctness and helpfulness.  The LLMs evaluated include StarCoder-15B, CodeLlama-7B, and CodeLlama-13B, across different training approaches (Prompting, SFT, and RL).  The table shows that the LLMs trained with the proposed LEDEX framework generally receive higher scores than those trained only with prompting.", "section": "4.4 RQ4: Quality of generated explanation"}]