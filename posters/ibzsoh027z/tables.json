[{"figure_path": "iBZSOh027z/tables/tables_6_1.jpg", "caption": "Table 1: Results of Coverage, Size and SH on different datasets. For SNAPS we use the APS score as the basic score. We report the average calculated from 10 GCN runs with each run of 100 conformal splits at a significance level a = 0.05. Bold numbers indicate optimal performance.", "description": "This table presents the results of coverage, average prediction set size, and singleton hit ratio for various node classification methods (APS, RAPS, DAPS, and SNAPS) across ten datasets.  The results are averaged over ten runs, each with 100 conformal splits at a significance level of 0.05, demonstrating the performance of SNAPS against other methods.", "section": "4.1 Experimental Settings"}, {"figure_path": "iBZSOh027z/tables/tables_7_1.jpg", "caption": "Table 1: Results of Coverage, Size and SH on different datasets. For SNAPS we use the APS score as the basic score. We report the average calculated from 10 GCN runs with each run of 100 conformal splits at a significance level \u03b1 = 0.05. Bold numbers indicate optimal performance.", "description": "This table presents the results of three metrics (Coverage, Size, and Singleton Hit Ratio (SH)) for different node classification datasets using various methods, including APS, RAPS, DAPS, and the proposed SNAPS.  The table compares the performance of these methods in terms of prediction set efficiency (Size) and the accuracy of singleton predictions (SH), while ensuring the validity of the marginal coverage.", "section": "4.1 Experimental Settings"}, {"figure_path": "iBZSOh027z/tables/tables_8_1.jpg", "caption": "Table 1: Results of Coverage, Size and SH on different datasets. For SNAPS we use the APS score as the basic score. We report the average calculated from 10 GCN runs with each run of 100 conformal splits at a significance level \u03b1 = 0.05. Bold numbers indicate optimal performance.", "description": "This table presents the results of three metrics (Coverage, Size, and Singleton Hit Ratio or SH) for evaluating the efficiency and accuracy of different conformal prediction methods (APS, RAPS, DAPS, and SNAPS) on ten different datasets.  The experiment used Graph Convolutional Networks (GCNs) and a significance level of 0.05.  SNAPS consistently outperforms the other methods in terms of Size and SH, indicating improved efficiency and accuracy, while maintaining valid Coverage.  Bold numbers highlight the best performance for each metric on each dataset.", "section": "4.1 Experimental Settings"}, {"figure_path": "iBZSOh027z/tables/tables_8_2.jpg", "caption": "Table 1: Results of Coverage, Size and SH on different datasets. For SNAPS we use the APS score as the basic score. We report the average calculated from 10 GCN runs with each run of 100 conformal splits at a significance level \u03b1 = 0.05. Bold numbers indicate optimal performance.", "description": "This table presents the results of three evaluation metrics (Coverage, Size, and Singleton Hit Ratio or SH) for different graph datasets using various methods including Adaptive Prediction Sets (APS), RAPS, DAPS, and the proposed SNAPS algorithm.  The results highlight the performance of SNAPS in terms of efficiency (smaller prediction set size) and accuracy (higher singleton hit ratio) while maintaining a valid coverage level.", "section": "4.2 Experimental results"}, {"figure_path": "iBZSOh027z/tables/tables_15_1.jpg", "caption": "Table 1: Results of Coverage, Size and SH on different datasets. For SNAPS we use the APS score as the basic score. We report the average calculated from 10 GCN runs with each run of 100 conformal splits at a significance level \u03b1 = 0.05. Bold numbers indicate optimal performance.", "description": "This table presents the results of three evaluation metrics (Coverage, Size, and Singleton Hit Ratio (SH)) for various node classification datasets using different conformal prediction methods, including APS, RAPS, DAPS, and the proposed SNAPS method.  The table compares the performance of these methods on 10 datasets, showing the average values of the metrics across 10 separate runs of a Graph Convolutional Network (GCN) model, each with 100 conformal prediction splits performed at a significance level (alpha) of 0.05.  The results demonstrate SNAPS's ability to achieve high singleton hit ratios while maintaining a valid coverage rate and smaller prediction set sizes compared to the other methods.", "section": "4.1 Experimental Settings"}, {"figure_path": "iBZSOh027z/tables/tables_16_1.jpg", "caption": "Table 5: Results of Coverage, Size on two heterophilous graph datasets. We report the average calculated from FSGNN with 100 conformal splits at different significance levels (\u03b1 = 0.05, 0.1). Bold numbers indicate optimal performance.", "description": "This table presents the results of coverage and size for two heterophilous graph datasets (Chameleon and Squirrel).  The results are obtained using the FSGNN model and the SNAPS algorithm, with comparisons made against APS and DAPS.  The table shows the average values calculated from 100 conformal splits at significance levels \u03b1 = 0.05 and \u03b1 = 0.1. Bold numbers highlight the best performance achieved by SNAPS for each metric and significance level.", "section": "C.1 Experiments on heterophilous graph datasets"}, {"figure_path": "iBZSOh027z/tables/tables_16_2.jpg", "caption": "Table 6: Results of Size, Time on three graph datasets for CF-GNN and SNAPS. We report the results at different significance levels (\u03b1 = 0.05, 0.1). Bold numbers indicate optimal performance.", "description": "This table compares the performance of SNAPS and CF-GNN on three graph datasets, in terms of prediction set size and computation time.  Results are shown for two different significance levels (\u03b1 = 0.05 and \u03b1 = 0.1).  SNAPS consistently shows smaller prediction set sizes and faster computation times, indicating improved efficiency.", "section": "C.2 Comparison with CF-GNN"}, {"figure_path": "iBZSOh027z/tables/tables_18_1.jpg", "caption": "Table 1: Results of Coverage, Size and SH on different datasets. For SNAPS we use the APS score as the basic score. We report the average calculated from 10 GCN runs with each run of 100 conformal splits at a significance level \u03b1 = 0.05. Bold numbers indicate optimal performance.", "description": "This table presents the results of applying different conformal prediction methods (APS, RAPS, DAPS, and SNAPS) to several datasets using Graph Convolutional Networks (GCNs).  It compares the methods across three metrics: Coverage (the percentage of times the prediction set contains the true label), Size (the average size of the prediction sets), and Singleton Hit Ratio (SH, the percentage of times the prediction set contains only the true label and is size one). The results show the effectiveness of SNAPS in generating smaller and more accurate prediction sets compared to other methods. The table highlights the best performance for each metric using bold numbers.", "section": "4.2 Experimental results"}, {"figure_path": "iBZSOh027z/tables/tables_18_2.jpg", "caption": "Table 1: Results of Coverage, Size and SH on different datasets. For SNAPS we use the APS score as the basic score. We report the average calculated from 10 GCN runs with each run of 100 conformal splits at a significance level \u03b1 = 0.05. Bold numbers indicate optimal performance.", "description": "This table presents the results of experiments evaluating the performance of different methods (APS, RAPS, DAPS, and SNAPS) for semi-supervised node classification on ten different datasets.  The metrics reported are Coverage (empirical marginal coverage), Size (average size of prediction sets), and SH (singleton hit ratio).  The results demonstrate SNAPS' superiority in terms of efficiency (smaller prediction set size) and singleton hit ratio while maintaining valid coverage compared to other methods.", "section": "4.2 Experimental results"}, {"figure_path": "iBZSOh027z/tables/tables_19_1.jpg", "caption": "Table 1: Results of Coverage, Size and SH on different datasets. For SNAPS we use the APS score as the basic score. We report the average calculated from 10 GCN runs with each run of 100 conformal splits at a significance level \u03b1 = 0.05. Bold numbers indicate optimal performance.", "description": "This table presents the results of experiments comparing the performance of different conformal prediction methods (APS, RAPS, DAPS, and SNAPS) on various datasets. The metrics used are Coverage (the percentage of prediction sets that contain the true label), Size (the average size of the prediction sets), and SH (Singleton Hit ratio, the percentage of prediction sets of size one containing the true label). The results show that SNAPS consistently outperforms other methods in terms of efficiency while maintaining a valid coverage.", "section": "4.1 Experimental Settings"}, {"figure_path": "iBZSOh027z/tables/tables_19_2.jpg", "caption": "Table 1: Results of Coverage, Size and SH on different datasets. For SNAPS we use the APS score as the basic score. We report the average calculated from 10 GCN runs with each run of 100 conformal splits at a significance level \u03b1 = 0.05. Bold numbers indicate optimal performance.", "description": "This table presents the results of evaluating several methods on 10 datasets using three metrics: Coverage, Size, and Singleton Hit Ratio (SH).  The methods compared include APS, RAPS, DAPS, and SNAPS, with SNAPS using the Adaptive Prediction Sets (APS) score as its base.  Each method is run 10 times with 100 conformal splits at a significance level (\u03b1) of 0.05.  The table shows that SNAPS significantly improves efficiency while maintaining coverage compared to other methods. Bold numbers highlight the best performance for each metric on each dataset.", "section": "4.1 Experimental Settings"}, {"figure_path": "iBZSOh027z/tables/tables_19_3.jpg", "caption": "Table 1: Results of Coverage, Size and SH on different datasets. For SNAPS we use the APS score as the basic score. We report the average calculated from 10 GCN runs with each run of 100 conformal splits at a significance level  = 0.05. Bold numbers indicate optimal performance.", "description": "This table presents the results of three metrics (Coverage, Size, and Singleton Hit Ratio (SH)) for different node classification datasets using various methods, including APS, RAPS, DAPS, and SNAPS.  The table compares the performance of these methods in terms of the average size of the prediction sets they generate, while ensuring that the ground truth label is included at a specified confidence level (95%).  Lower size values indicate better efficiency, while a higher SH indicates a greater proportion of perfectly accurate predictions.", "section": "4.1 Experimental Settings"}, {"figure_path": "iBZSOh027z/tables/tables_20_1.jpg", "caption": "Table 1: Results of Coverage, Size and SH on different datasets. For SNAPS we use the APS score as the basic score. We report the average calculated from 10 GCN runs with each run of 100 conformal splits at a significance level \u03b1 = 0.05. Bold numbers indicate optimal performance.", "description": "This table presents the performance of different conformal prediction methods (APS, RAPS, DAPS, and SNAPS) on ten different datasets using Graph Convolutional Networks (GCNs).  The metrics reported include the empirical marginal coverage (Coverage), the average size of the prediction sets (Size), and the singleton hit ratio (SH).  The table highlights the superior performance of SNAPS in terms of efficiency (smaller prediction set size) while maintaining valid coverage.", "section": "4.1 Experimental Settings"}]