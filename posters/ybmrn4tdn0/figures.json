[{"figure_path": "ybMrn4tdn0/figures/figures_1_1.jpg", "caption": "Figure 1: Local explanations (see Section 2.2 for notation): In both panels, a set of training points x and their classifications f(x) (red/blue, decision boundary in green) are shown. For three training points (one centered at each ball), a local linear explanation (gx, Rx) is illustrated where gx is a local linear classifier (black decision boundary) and Rx is a local ball centered at x. Panel (a) depicts a regime where there is insufficient data for verifying how accurate the local explanations approximate the classifier f \u2013 none of the provided regions contain enough points to assess the accuracy of the linear explanations. Panel (b) depicts a regime with more training points allowing us to validate the accuracy of the linear explanations based on how closely they align with the points in their corresponding regions.", "description": "This figure compares two scenarios of local explanations. Panel (a) shows insufficient data points to verify the accuracy of local linear explanations against the true classifier.  Panel (b) shows a sufficient number of data points within the local regions, enabling a more accurate assessment of the explanations' fidelity.", "section": "2 Local Explanations"}, {"figure_path": "ybMrn4tdn0/figures/figures_1_2.jpg", "caption": "Figure 1: Local explanations (see Section 2.2 for notation): In both panels, a set of training points x and their classifications f(x) (red/blue, decision boundary in green) are shown. For three training points (one centered at each ball), a local linear explanation (gx, Rx) is illustrated where gx is a local linear classifier (black decision boundary) and Rx is a local ball centered at x. Panel (a) depicts a regime where there is insufficient data for verifying how accurate the local explanations approximate the classifier f \u2013 none of the provided regions contain enough points to assess the accuracy of the linear explanations. Panel (b) depicts a regime with more training points allowing us to validate the accuracy of the linear explanations based on how closely they align with the points in their corresponding regions.", "description": "This figure shows two scenarios for auditing local explanations. The left panel (a) shows insufficient data points within each local region to assess whether the local linear classifier accurately approximates the true classifier. In contrast, the right panel (b) shows sufficient data points to enable accurate assessment. This illustrates the relationship between data quantity, local region size, and the ability to audit the fidelity of local explanations.", "section": "2 Local Explanations"}, {"figure_path": "ybMrn4tdn0/figures/figures_8_1.jpg", "caption": "Figure 2: An illustration of Theorem 5.1, with the concentric blue and red circles depicting the data distribution \u03bc classified by f, and with local explanations being depicted at points A and B. Explanations are forced to either have large local loss (point A) or a low local mass (point B).", "description": "This figure illustrates Theorem 5.1, which shows that for high dimensional data, local linear explanations either have a large loss (meaning that they do not accurately represent the function in the local region), or they have small local mass (meaning that the regions they apply to are very small). The figure shows concentric circles representing datapoints classified into two classes (red and blue). Point A represents an explanation with large local loss because its local linear approximation does not correctly classify many points in the region. Point B represents a small local mass because it only accurately classifies points in a small region.  The key takeaway is that high-dimensional settings make verification of explanations based solely on pointwise predictions and explanations very challenging.", "section": "5 The locality of practical explainability methods can be extremely small"}]