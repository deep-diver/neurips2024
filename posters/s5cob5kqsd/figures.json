[{"figure_path": "S5coB5kqSD/figures/figures_1_1.jpg", "caption": "Figure 1: (Left) Visualization of BEVFusion feature maps before and after fusion, indicating minimal information gain. (Right) Significant performance degradation in the LiDAR-missing scenario, highlighting the over-reliance on LiDAR features.", "description": "This figure shows a visualization of BEVFusion's feature maps before and after fusion, illustrating minimal information gain from the fusion process.  The bar chart highlights a significant drop in performance (NDS score) when LiDAR data is missing, emphasizing the model's heavy reliance on LiDAR-specific information rather than general multi-modal features.", "section": "Abstract"}, {"figure_path": "S5coB5kqSD/figures/figures_3_1.jpg", "caption": "Figure 2: Overall architecture of VeXKD: Building upon the common BEV fusion pipeline, we tailor a Modality-General Fusion Module and design a masked feature distillation method with learned masks assisted by the byproduct of the fusion module, applied across both low-level and high-level BEV features. Our feature distillation framework circumvents variations in different model architectures, making it modality- and task-agnostic.", "description": "This figure illustrates the overall architecture of the VeXKD framework, which integrates cross-modal fusion and knowledge distillation.  It shows how a modality-general fusion module combines LiDAR and camera inputs to create a BEV (Bird's Eye View) feature map.  This BEV feature map then undergoes masked feature distillation, where learned masks guide the knowledge transfer to single-modal student models for different perception tasks (e.g., 3D object detection, BEV map segmentation). The design is intended to be versatile and work across different model architectures.", "section": "3.1 Overall architecture of Cross-Modal Fusion and Knowledge Distillation Framework"}, {"figure_path": "S5coB5kqSD/figures/figures_3_2.jpg", "caption": "Figure 3: Illustration of BEV query guided mask generation and masked feature distillation. (a) Overview of the transformer-based block for mask generation, which adopts the byproducts from the fusion module as the BEV query. (b) In the deformable cross attention operation, the BEV query interacts with the teacher feature maps to identify crucial spatial locations. (c) In the masked feature distillation stage, learned spatial masks are applied to both teacher and student feature maps before calculating the distillation loss.", "description": "This figure illustrates the process of BEV query guided mask generation and masked feature distillation.  The (a) part shows the transformer-based mask generation block utilizing fusion module byproducts as a query.  (b) illustrates the deformable cross-attention mechanism where the query identifies key spatial locations in teacher feature maps.  Finally, (c) depicts the masked feature distillation stage, where learned masks are applied to both teacher and student features before calculating the distillation loss.", "section": "3.3 BEV Query Guided Mask Generation"}, {"figure_path": "S5coB5kqSD/figures/figures_4_1.jpg", "caption": "Figure 4: The architecture of the Modality-General Fusion Module: (a) Overview of the transformer-based block. (b) Deformable cross-modal attention operation: the BEV query symmetrically interacts with features sampled from both LiDAR and camera. (c) Deformable query self-attention operation: the BEV query interacts with itself to integrate correlational relationships.", "description": "This figure illustrates the architecture of the Modality-General Fusion Module (MGFM), a key component of the VeXKD framework. The MGFM uses a transformer-based architecture with deformable attention mechanisms to fuse features from LiDAR and camera modalities in the BEV (Bird's Eye View) space.  Panel (a) shows the overall structure. Panel (b) details the deformable cross-modal attention, highlighting how the learnable BEV query interacts symmetrically with LiDAR and camera features to extract modality-general information. Panel (c) illustrates the deformable query self-attention mechanism, which allows the BEV query to interact with itself, integrating correlational relationships among features and improving the accuracy and semantic richness of the fused BEV features.", "section": "3.2 Modality-General Fusion Module (MGFM)"}, {"figure_path": "S5coB5kqSD/figures/figures_8_1.jpg", "caption": "Figure 5: Ablation study of the blocks number in the mask generation network on nuScenes val.", "description": "This ablation study examines the impact of varying the number of transformer blocks within the mask generation network on the model's performance.  The graph shows that increasing the number of blocks beyond two does not significantly improve the final mAP (mean Average Precision), while the number of epochs needed to achieve convergence decreases.  This suggests that a balance must be struck, and three blocks are chosen as the optimal configuration in the paper. The experiment is conducted on the nuScenes validation set.", "section": "4.3 Ablation Studies"}, {"figure_path": "S5coB5kqSD/figures/figures_9_1.jpg", "caption": "Figure 6: Visualization of learned spatial mask. Our mask generation network can produce spatial masks for features at different levels, tailored for various 3D perception tasks.", "description": "This figure visualizes the spatial masks generated by the BEV Query Guided Mask Generation module for different feature levels (low and high) and tasks (object detection and BEV segmentation).  The masks are learned in a data-driven manner and highlight specific regions of the feature maps deemed most relevant for downstream tasks.  For object detection, the masks emphasize areas around ground truth object locations, especially for low-level features. For BEV segmentation, the masks also highlight background features. This adaptive mask generation is key to the effectiveness of the knowledge distillation process.", "section": "3.3 BEV Query Guided Mask Generation"}, {"figure_path": "S5coB5kqSD/figures/figures_9_2.jpg", "caption": "Figure 8: Comparison of feature maps \u2014 without vs. with distillation. KD improves camera feature, offering more deterministic view projection accuracy and accentuating important LiDAR features.", "description": "This figure compares feature maps from a camera and LiDAR before and after knowledge distillation (KD). The left shows the original feature maps, while the right shows the feature maps after applying KD. The results indicate that KD improves the camera feature maps by making the depth projection more deterministic. Additionally, KD enhances the LiDAR feature maps by highlighting crucial spatial features, which improves the overall accuracy of 3D perception.", "section": "4.4 Qualitative Results"}, {"figure_path": "S5coB5kqSD/figures/figures_14_1.jpg", "caption": "Figure 2: Overall architecture of VeXKD: Building upon the common BEV fusion pipeline, we tailor a Modality-General Fusion Module and design a masked feature distillation method with learned masks assisted by the byproduct of the fusion module, applied across both low-level and high-level BEV features. Our feature distillation framework circumvents variations in different model architectures, making it modality- and task-agnostic.", "description": "This figure illustrates the overall architecture of the VeXKD framework.  It shows how a modality-general fusion module combines LiDAR and camera data into a unified BEV representation.  This fused representation is then used by a masked feature distillation method to transfer knowledge to single-modal student models.  The key innovation is the use of learned masks, generated from the fusion process, that focus the distillation on the most informative parts of the teacher model's feature maps. The framework is designed to be versatile and adaptable to various student modalities and downstream tasks.", "section": "3.1 Overall architecture of Cross-Modal Fusion and Knowledge Distillation Framework"}, {"figure_path": "S5coB5kqSD/figures/figures_16_1.jpg", "caption": "Figure 8: Comparison of feature maps - without vs. with distillation. KD improves camera feature, offering more deterministic view projection accuracy and accentuating important LiDAR features.", "description": "This figure shows a comparison of feature maps before and after knowledge distillation (KD). The left two images show camera feature maps, where the leftmost image is before KD and the other is after KD. Similarly, the rightmost two images display LiDAR feature maps before and after KD. The results demonstrate that KD enhances the quality of camera features by improving the accuracy of the depth projection and enhances the quality of LiDAR features by highlighting their importance and correlating them with textual information from the images. ", "section": "Qualitative Results"}, {"figure_path": "S5coB5kqSD/figures/figures_16_2.jpg", "caption": "Figure C.1: BEV feature map comparison between KD and no-KD ones", "description": "This figure visualizes the BEV feature maps for both camera and LiDAR modalities before and after applying knowledge distillation (KD).  For camera features, KD improves depth estimation, leading to a more organized and accurate feature map compared to the chaotic distribution seen without KD. For LiDAR features, KD enhances the correlation between point cloud distribution and image texture, improving visualization of crucial areas and partially mitigating the sparsity issue at longer distances.", "section": "C More Qualitative Results"}, {"figure_path": "S5coB5kqSD/figures/figures_16_3.jpg", "caption": "Figure C.1: BEV feature map comparison between KD and no-KD ones", "description": "This figure visualizes the BEV feature maps for both camera and LiDAR modalities, comparing the results before and after applying knowledge distillation (KD).  The top row shows camera features, demonstrating how KD improves the depth information by creating a more deterministic and less chaotic feature distribution. The bottom row illustrates LiDAR features, showing that KD enhances the key features by highlighting important information that was less clear in the original LiDAR data.  The enhanced features are more spatially localized and easier to interpret for perception tasks.", "section": "C More Qualitative Results"}, {"figure_path": "S5coB5kqSD/figures/figures_16_4.jpg", "caption": "Figure C.1: BEV feature map comparison between KD and no-KD ones", "description": "This figure visualizes the BEV (Bird's Eye View) feature maps for both camera and LiDAR modalities before and after knowledge distillation (KD).  For camera-based features, the differences highlight the improved depth estimation accuracy and more deterministic depth projection after KD, resulting in less chaotic depth distribution. For LiDAR-based features, the improved correlation between point cloud distribution and camera textural information is shown by the enhanced visibility of key features and reduction in sparsity, especially in areas further away.", "section": "C More Qualitative Results"}, {"figure_path": "S5coB5kqSD/figures/figures_17_1.jpg", "caption": "Figure C.2: Detection result before and after knowledge distillation for camera and LiDAR students.", "description": "This figure shows a comparison of 3D object detection results before and after applying knowledge distillation. The left side displays the results from a camera-based model, and the right side shows the results from a LiDAR-based model. In both cases, the use of knowledge distillation leads to improvements in the accuracy and reliability of the object detection results. Specifically, the camera-based model benefits from increased confidence in depth estimations, while the LiDAR-based model achieves better recognition and classification, especially for objects at longer distances.", "section": "C More Qualitative Results"}, {"figure_path": "S5coB5kqSD/figures/figures_17_2.jpg", "caption": "Figure C.2: Detection result before and after knowledge distillation for camera and LiDAR students.", "description": "This figure shows a comparison of the detection results before and after applying knowledge distillation to both camera and LiDAR-based student models. The left side demonstrates the results obtained using camera images, while the right side displays the results using LiDAR point clouds. In both cases, the improved accuracy and robustness of the models after applying knowledge distillation are apparent, as indicated by the tighter bounding boxes and more accurate positioning of detected objects.", "section": "C More Qualitative Results"}, {"figure_path": "S5coB5kqSD/figures/figures_17_3.jpg", "caption": "Figure C.2: Detection result before and after knowledge distillation for camera and LiDAR students.", "description": "This figure shows a comparison of the 3D object detection results before and after applying knowledge distillation to both camera and LiDAR-based student models. The top two images show a camera view, and the bottom two show a LiDAR view. In each pair, the left image displays the detection result before KD and the right image shows the result after KD.  The improvements in detection accuracy and localization are clearly visible after the application of KD. Specifically, the camera-based detection shows improved depth estimation accuracy, and the LiDAR-based detection shows enhanced localization, especially for objects at greater distances.", "section": "C More Qualitative Results"}, {"figure_path": "S5coB5kqSD/figures/figures_17_4.jpg", "caption": "Figure C.2: Detection result before and after knowledge distillation for camera and LiDAR students.", "description": "This figure shows a comparison of 3D object detection results before and after applying knowledge distillation. The top row shows the results for a camera-based model, and the bottom row shows the results for a LiDAR-based model.  In both cases, the knowledge distillation improves the accuracy of the detection results, particularly in terms of reducing false positives and enhancing the accuracy of bounding boxes, especially for objects at a distance or partially occluded.", "section": "C More Qualitative Results"}, {"figure_path": "S5coB5kqSD/figures/figures_18_1.jpg", "caption": "Figure C.3: Segmentation result before or after Knowledge Distillation", "description": "This figure visually compares the segmentation results obtained from camera and LiDAR modalities before and after applying knowledge distillation. The top row shows the ground truth segmentation labels for both camera and LiDAR. The second row displays the segmentation results from the camera modality before applying knowledge distillation, showing inaccuracies and boundary blurring. The third row presents the improved segmentation results from the camera modality after applying knowledge distillation, demonstrating enhanced boundary definition and more precise segmentation. The bottom row shows the segmentation results from the LiDAR modality before and after applying knowledge distillation. The results indicate that knowledge distillation enhances the accuracy of both camera and LiDAR-based segmentation, especially in regions with sparse data points.", "section": "C More Qualitative Results"}, {"figure_path": "S5coB5kqSD/figures/figures_18_2.jpg", "caption": "Figure C.3: Segmentation result before or after knowledge distillation", "description": "This figure visualizes the segmentation results for both camera and LiDAR modalities, comparing the results before and after applying knowledge distillation.  The top row shows the ground truth segmentation labels. The middle row shows the camera segmentation results before applying knowledge distillation, illustrating inaccuracies, particularly in boundary delineation. The bottom row demonstrates the improved accuracy and boundary definition in the camera segmentation after knowledge distillation.  The LiDAR segmentation results show similar improvements from knowledge distillation, particularly in reducing hallucinations and improving the definition of less prominent features and background areas.", "section": "C More Qualitative Results"}, {"figure_path": "S5coB5kqSD/figures/figures_18_3.jpg", "caption": "Figure C.3: Segmentation result before or after knowledge distillation", "description": "This figure visualizes the segmentation results for both camera and LiDAR modalities before and after applying knowledge distillation. It highlights how knowledge distillation improves the accuracy of depth estimation in camera-based segmentation and enhances background details in LiDAR-based segmentation by correlating camera textural information and point cloud distribution.", "section": "C More Qualitative Results"}, {"figure_path": "S5coB5kqSD/figures/figures_18_4.jpg", "caption": "Figure C.3: Segmentation result before or after Knowledge Distillation", "description": "This figure visualizes the segmentation results for both camera and LiDAR modalities, comparing the results before and after applying knowledge distillation (KD).  It shows the ground truth segmentation labels alongside the model's predictions before and after KD. The comparison highlights how KD improves the accuracy of segmentation, especially for camera data (reducing boundary blur) and LiDAR data (improving background accuracy and mitigating sparsity issues).", "section": "C More Qualitative Results"}, {"figure_path": "S5coB5kqSD/figures/figures_18_5.jpg", "caption": "Figure C.3: Segmentation result before or after knowledge distillation", "description": "This figure visually compares the segmentation results for both camera and LiDAR modalities, before and after applying knowledge distillation.  It highlights the improvements in boundary delineation and background representation achieved through the knowledge distillation process. The camera modality benefits from improved depth estimation, while the LiDAR modality shows enhanced background representation and reduced misclassifications. The comparison showcases the effectiveness of the proposed approach in improving the accuracy and quality of BEV map segmentation for both modalities.", "section": "C More Qualitative Results"}, {"figure_path": "S5coB5kqSD/figures/figures_18_6.jpg", "caption": "Figure C.3: Segmentation result before or after knowledge distillation", "description": "This figure visualizes the segmentation results for both camera and LiDAR modalities, comparing the results before and after applying the proposed knowledge distillation (KD) method.  The top row shows the ground truth segmentation labels, providing a basis for comparison. The middle row displays the camera-based segmentation before KD, highlighting areas where the model struggles with boundary delineation and accuracy. The bottom row showcases the LiDAR-based segmentation before KD, demonstrating challenges with sparsity and accurate background representation. The red circles highlight specific regions of interest where the improvements made by the KD method are most evident. The results demonstrate that the KD method enhances both the camera and LiDAR-based segmentation, improving boundary delineation and accuracy, as well as reducing errors in areas with sparse point clouds or a lack of features.", "section": "C More Qualitative Results"}, {"figure_path": "S5coB5kqSD/figures/figures_18_7.jpg", "caption": "Figure C.3: Segmentation result before or after knowledge distillation", "description": "This figure visualizes the results of BEV map segmentation for both camera and LiDAR modalities, comparing the results before and after applying knowledge distillation (KD).  The top row shows the ground truth segmentation labels. The middle rows demonstrate the camera-based segmentation results: the first showing the results before applying KD, and the second showing the results after. Similarly, the bottom rows show the LiDAR-based segmentation results, also before and after KD. The red circles highlight areas where the KD significantly improves the segmentation accuracy by enhancing the details and reducing inaccuracies, particularly in areas with sparse data or ambiguous boundaries. The comparison helps illustrate how KD improves the quality and precision of segmentation in various scenarios.", "section": "C More Qualitative Results"}, {"figure_path": "S5coB5kqSD/figures/figures_18_8.jpg", "caption": "Figure C.3: Segmentation result before or after knowledge distillation", "description": "This figure shows a comparison of segmentation results for camera and LiDAR modalities before and after applying knowledge distillation. The top row displays ground truth segmentation labels, followed by the results from camera-only segmentation (before and after KD), and LiDAR-only segmentation (before and after KD). The red circles highlight areas where the knowledge distillation process significantly improves the accuracy of segmentation results.", "section": "C More Qualitative Results"}]