[{"type": "text", "text": "Outlier-Robust Phase Retrieval in Nearly-Linear Time ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Phase retrieval is a fundamental problem in signal processing, where the goal is   \n2 to recover a (complex-valued) signal from phaseless intensity measurements. It is   \n3 known that natural non-convex formulations of phase retrieval do not have spurious   \n4 local optima. However, the theoretical analyses of such landscape results often rely   \n5 on strong assumptions, such as the sampling vectors being Gaussian distributed.   \n6 In this paper, we propose and study the problem of outlier robust phase retrieval.   \n7 We seek to recover a vector $\\boldsymbol{x}\\in\\mathbb{R}^{\\dot{d}}$ from $n$ intensity measurements $y_{i}=(a_{i}^{\\top}x)^{2}$ ,   \n8 where the sampling vectors $a_{i}$ \u2019s are initially i.i.d. Gaussian, but a small fraction of   \n9 the $\\left(a_{i},y_{i}\\right)$ pairs are adversarially corrupted.   \n10 Our main result is a near-sample-optimal and nearly-linear-time algorithm that   \n11 provably recovers the ground-truth $x$ in the presence of adversarial corruptions.   \n12 We first solve a lightweight convex program to find a vector close to the ground   \n13 truth. We then run robust gradient descent starting from this initial solution,   \n14 leveraging recent advances in high-dimensional robust statistics. Our approach is   \n15 conceptually simple and provides a framework for developing robust algorithms   \n16 for other tractable non-convex problems. ", "page_idx": 0}, {"type": "text", "text": "17 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "18 Phase retrieval is a fundamental problem in signal processing with applications in various fields, in  \n19 cluding electron microscopy [32], crystallography [33, 36], astronomy [11], and optical imaging [37].   \n20 In these applications, one often has access to only the magnitudes of the Fourier transforms of a   \n21 complex signal. This is because measuring magnitude (e.g., by aggregating energy over time) is   \n22 much easier than measuring phase (which requires detecting rapid changes). We refer the reader to   \n23 the survey articles [37, 26] for more details about the theory and applications of phase retrieval.   \n24 In this paper, we focus on the real-valued generalized phase retrieval problem, where the Fourier   \n25 transform is replaced by a general linear operator. We first give a formal definition of this problem.   \n26 Definition 1.1 (Phase Retrieval). Let $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ be the ground-truth vector. Let $\\boldsymbol{a}_{1}\\boldsymbol{\\cdot}\\boldsymbol{\\cdot}\\boldsymbol{a}_{n}\\in\\mathbb{R}^{d}$ be $n$   \n27 sampling vectors and let $y_{i}=\\langle a_{i},x\\rangle^{2}\\in\\mathbb{R}$ be the corresponding intensity measurements. Given   \n28 $(a_{i},y_{i})_{i=1}^{n}$ as input, the task is to recover $x$ .   \n29 Note that it is impossible to distinguish between $x$ and $-x$ , so it is sufficient to recover either one.   \n30 Under certain assumptions (e.g., when the $a_{i}$ \u2019s are Gaussian distributed), the phase retrieval problem   \n31 in Definition 1.1 can be solved in polynomial time with provable recovery guarantees. This was first   \n32 achieved via approaches based on semidefinite programming (SDP) relaxations (see, e.g., Cand\u00e8s   \n33 et al. [5]). In practice, the problem is often solved using first-order optimization algorithms such as   \n34 gradient descent. It is well-established that, although many natural formulations of phase retrieval   \n35 have nonconvex objectives, all local optima are globally optimal under certain assumptions [34, 3, 40].   \n36 An example of such objective function is the following: ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{minimize}}&{{}f(z)=\\sum_{i=1}^{n}(y_{i}-\\langle a_{i},z\\rangle^{2})^{2}\\quad\\mathrm{subject\\;to}\\quad z\\in\\mathbb{R}^{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "38 However, existing analyses of such landscape results often rely on strong assumptions, such as the   \n39 sampling vectors $a_{i}$ \u2019s are i.i.d. Gaussian. Our work is motivated by the following questions: Can we   \n40 relax the assumptions used in proving landscape results in many tractable nonconvex problems? In the   \n41 context of phase retrieval, what happens if a small fraction of the $(a_{i},y_{i})$ \u2019s are changed adversarially?   \n42 We focus on the following strong contamination model (see, e.g., [13]).   \n43 Definition 1.2 $\\mathbf{\\boldsymbol{\\epsilon}}$ -Corruption). An algorithm first specifies the number of samples $n$ , and $n$ samples   \n44 are drawn independently from some unknown distribution $D$ . The adversary is allowed to replace up   \n45 to $\\epsilon n$ samples with arbitrary points. The modified set of $n$ samples is then given to the algorithm as   \n46 input. We say that a set of samples is $\\epsilon$ -corrupted if it is generated by the above process.   \n47 Under the $\\epsilon$ -corruption model for high-dimensional data, a common goal is to design efficient   \n48 algorithms that can achieve dimension-independent error guarantees. Early work in robust statis  \n49 tics [42, 23, 25] provided sample-efficient estimators for various tasks, but with runtimes exponential   \n50 in the dimension. A recent line of work, initiated by [13, 28], has developed computationally efficient   \n51 robust algorithms for many fundamental high-dimensional tasks. There has been significant progress   \n52 in the algorithmic aspects of robust high-dimensional statistics (see, e.g., [12]). ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "53 We now formally define the main problem that we pose and study in this paper. ", "page_idx": 1}, {"type": "text", "text": "54 Problem 1.3 (Outlier-Robust Phase Retrieval). Let $\\epsilon>0$ . Let $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ be the ground-truth vector with   \n55 $\\|x\\|_{2}=1$ . First, $n$ sampling vectors $(a_{i})_{i=1}^{n}$ are drawn i.i.d. from $\\mathcal{N}(0,I)\\stackrel{=}{\\in}\\mathbb{R}^{d}$ . Let $y_{i}=\\langle a_{i},x\\rangle^{2}$   \n56 be the corresponding intensity measurements. Then, an adversary arbitrarily corrupts an $\\epsilon$ -fraction of   \n57 the $(a_{i},y_{i})$ \u2019s. Finally, the corrupted $(a_{i},y_{i})$ \u2019s are given to the algorithm as input. The task is to find a   \n58 vector $z\\in\\mathbb{R}^{d}$ such that $\\operatorname*{min}\\{\\|z-x\\|_{2}\\,,\\|z+x\\|_{2}\\}\\leq\\Delta$ for some precision parameter $\\Delta>0$ .   \n59 Note that we allow corruption in both the sampling vectors $a_{i}\\in\\mathbb{R}^{d}$ and the intensity measurements   \n60 $y_{i}\\in\\mathbb{R}$ . We would like to answer the following algorithmic question:   \n61 Can we design a provably robust and near sample-optimal algorithm for the   \n62 \u03f5-corrupted phase retrieval problem (Problem 1.3) that runs in nearly-linear time? ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "63 1.1 Our Results and Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "64 In this paper, we answer the above question affirmatively. We first state the main result of our paper.   \n65 Theorem 1.4 (Main, Informal). Consider the outlier-robust phase retrieval problem (Problem 1.3).   \n66 Let $\\Delta>0$ . Given an $\\epsilon$ -corrupted set of $n=\\widetilde\\Omega(d\\log^{2}(1/\\Delta))$ samples, we can compute $z\\in\\mathbb{R}^{d}$ in   \n67 time $\\widetilde{O}(n d)$ such that $\\operatorname*{min}(\\|z-x\\|_{2}\\,,\\|z+x\\|_{2})\\leq\\Delta$ with probability at least 0.8.   \n68 Our algorithm has near-optimal sample complexity, because even without corruption, recovering the   \n69 ground-truth vector $x$ in general requires $\\Omega(d)$ samples because there are $d$ degrees of freedom in $x$ .   \n70 Moreover, our algorithm runs in time nearly-linear in the size of the input, and provably recovers   \n71 the ground-truth vector $x$ with arbitrary precision $\\Delta$ . The formal version of Theorem 1.4 is stated as   \n72 Theorem 3.1 in Section 3.   \n73 We remark that the success probability of Theorem 1.4 can be boosted to $1-\\delta$ for any $\\delta>0$ by   \n74 incurring an additional factor of $T=O(\\log(1/\\delta))$ in the sample complexity and runtime. We can   \n75 randomly partition the input into $T$ equal-sized disjoint sets and run our algorithm on each set to   \n76 obtain $T$ solutions $Z=\\{z_{1},\\dots,z_{T}\\}$ . If we output a solution $z^{\\star}$ that has the maximum number of   \n77 points in $Z$ within distance $2\\Delta$ , we can show that $r(z^{\\star})\\leq3\\Delta$ with probability at least $1-\\delta$ .   \n78 Our main conceptual contribution is to propose and study the outlier-robust phase retrieval problem,   \n79 where a small fraction of the input data is adversarially corrupted. Note that we allow arbitrary   \n80 corruption in both the sampling vectors $a_{i}\\ \\in\\ \\mathbb{R}^{d}$ and the intensity measurements $y_{i}\\ \\in\\ \\mathbb{R}$ . The   \n81 nonconvex optimization landscape of phase retrieval is well understood when the $a_{i}$ \u2019s are Gaussian   \n82 distributed, but the adversarial robustness of such landscape results is largely unexplored.   \n83 Our main technical contributions include the design and analysis of a near sample-optimal and nearly  \n84 linear time algorithm that provably solves the phase retrieval problem in the presence of outliers.   \n85 Our approach provides a conceptually simple two-step framework for developing outlier-robust   \n86 algorithms for tractable nonconvex problems that combines the robustness of spectral initialization   \n87 and the efficiency of the subsequent robust gradient descent. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "88 1.2 Our Approach and Techniques ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "89 When there are infinite samples and no corruption, the objective function $f(z)$ can be simplified as ", "page_idx": 2}, {"type": "equation", "text": "$$\nf(z)=\\sum_{a\\sim\\mathcal{N}(0,I_{d})}\\left[(\\langle a_{i},z\\rangle^{2}-y_{i})^{2}\\right]=3\\left\\Vert x\\right\\Vert_{2}^{4}+3\\left\\Vert z\\right\\Vert_{2}^{4}-2\\left\\Vert x\\right\\Vert_{2}^{2}\\left\\Vert z\\right\\Vert_{2}^{2}-4\\left\\langle x,z\\right\\rangle^{2}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "90 Even though $f(z)$ is nonconvex, we know that it has no spurious local optima [34, 3, 40]. ", "page_idx": 2}, {"type": "text", "text": "91 Our approach follows the general structure of Cand\u00e8s et al. [3], which uses a two-step procedure.   \n92 The first step uses spectral techniques to find an initial guess that is close enough to the ground truth.   \n93 The second step applies gradient descent to converge to the final solution. However, both steps are   \n94 susceptible to adversarial corruption. We develop nearly-linear time and provably robust algorithms   \n95 for both steps and combine them to get our main result.   \n96 Step 1: Robust Spectral Initialization. When there is no adversarial corruption, the empirical   \n97 second-moment matrix $\\begin{array}{r}{Y\\,=\\,(1/n)\\sum_{i=1}^{n}y_{i}a_{i}a_{i}^{\\top}}\\end{array}$ has expectation $\\mathbf{E}\\left[Y\\right]=\\bar{I^{\\prime}}+2x x^{\\top}$ , so its top   \n98 eigenvector is close to $x$ . However, the adversary can arbitrarily change the top eigenvector.   \n19090 Twoe icgihrtceudm ivnetennt stihtiys- bisassueed,  swece oansdsi-gmno am (ennot nmneatgraitxi $\\begin{array}{r}{Y_{w}=\\dot{\\sum}_{i=1}^{n}w_{i}y_{i}a_{i}a_{i}^{\\top}}\\end{array}$ $w_{i}$ .a Imdpelael,l ya, nidf  tlhete $Y_{w}$ idgehtnso $w$ ta   \nuniformly distributed on the remaining clean samples , the top eigenvector of will align with $x$ .   \n102 We propose a novel optimization problem that can be used to find a weighting $w$ such that $Y_{w}$ must   \n103 be close to the unknown unbiased expectation $I+2x x^{\\top}$ . Moreover, we show that such a weight $w$   \n104 can be computed in nearly-linear time.   \n105 Step 2: Approximate Gradient Descent. Starting with the initial guess $z_{1}\\in\\mathbb{R}^{d}$ produced by the   \n106 robust spectral initialization, we want to apply gradient descent to recover the ground truth $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ .   \n107 Without corruption, if the initialization is close enough to $x$ , each iteration will bring $z$ closer to $x$ by   \n108 a constant factor. This convergence guarantee can be compromised by the corrupted samples.   \n109 At a high level, approximating the gradient at a specific point amounts to a robust mean estimation   \n110 problem (for the underlying distribution of the gradients). When the input data is $\\epsilon_{}$ -corrupted, the   \n111 gradients of the $n$ samples can be viewed as an $\\epsilon$ -corrupted set of vectors. We can approximate the   \n112 true gradient using this $\\epsilon$ -corrupted set of $n$ gradients using robust mean estimation algorithms. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "113 1.3 Related and Prior Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "114 Phase Retrieval. The problem of phase retrieval arises in many areas of science and engineering   \n115 [11, 33]. Early research on this problem proposes error-reduction algorithms [22, 17, 18]. Convex   \n116 and nonconvex optimization with various objective functions were later proposed and achieved exact   \n117 recovery [43, 3\u20135, 38]. Follow-up works generalize to robust phase retrieval where the observations   \n118 are subject to perturbations [45, 27, 7, 6, 31].   \n119 Nonconvex Optimization. Even though optimizing a nonconvex function is NP-Hard in general,   \n120 recent works showed that many nonconvex functions are locally optimizable due to discrete or   \n121 rotational symmetry. Besides phase retrieval, it is known that all local optima are globally optimal   \n122 for natural nonconvex formulations of a wide range of machine learning problems, such as matrix   \n123 completion [21], matrix sensing [2], phase synchronization [1], dictionary learning [39], and tensor   \n124 decomposition [20] (see also Chapter 7 of [44]). Closely related to our work, a recent line of work   \n125 explored the robustness of these landscape results: [30] studied matrix sensing in the $\\epsilon$ -corrupted   \n126 model and [8, 19] studied matrix completion and matrix sensing in semi-random models.   \n127 High-Dimensional Robust Statistics. Recent works in high-dimensional robust statistics developed   \n128 nearly-linear time algorithms for the problem of robust mean estimation [9, 16, 29]. Prior works [35,   \n129 14] developed meta-algorithms for finding first-order stationary points with dimension-independent   \n130 accuracy guarantees, which is closely related to the robust gradient descent procedure that we use. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "131 1.4 Roadmap ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "132 We first introduce notations and background in Section 2. Then we give an overview of our approach   \n133 in Section 3. Next, we focus on how to get an initialization that is close enough to the ground truth $x$   \n134 in Section 4. After the initialization, we use robust mean algorithms to estimate gradients to converge   \n135 to the desired accuracy in Section 5. Finally, we conclude in Section 6 and discuss open problems.   \n137 Notation. We write $[n]$ for the set of integers $\\{1,\\ldots,n\\}$ . We use $\\{e_{1},\\ldots,e_{d}\\}$ for the standard unit   \n138 vector basis in $\\mathbb{R}^{d}$ and $I$ for the identity matrix. For a vector $x$ , we denote its $\\ell_{1},\\ell_{2}$ and $\\ell_{\\infty}$ norm as   \n139 $\\Vert{\\boldsymbol{x}}\\Vert_{1},\\Vert{\\boldsymbol{x}}\\Vert_{2}$ and $\\left\\|{\\boldsymbol{x}}\\right\\|_{\\infty}$ , respectively, and write the $i^{\\mathrm{th}}$ coordinate in $x$ as $x_{i}$ . For vectors $\\boldsymbol{x},\\boldsymbol{y}\\in\\mathbb{R}^{d}$ ,   \n140 we denote its inner product as $\\langle x,y\\rangle=x^{\\top}y$ . For a matrix $A$ , we use $\\left\\|A\\right\\|_{2},\\left\\|A\\right\\|_{*}$ , and $\\|A\\|_{F}$ as its   \n141 operator norm, nuclear norm, and Frobenius norm, respectively. We write $\\lambda_{k}(A)$ as the $k$ th-largest   \n142 eigenvalues of $A$ , and ${\\overline{{\\lambda}}}_{k}(A)$ as the sum of the $k$ largest eigenvalues. A symmetric $n\\times n$ matrix $A$   \n143 is said to be positive semidefinite (PSD) if for all vectors $x\\in\\mathbb{R}^{n}$ , $x^{\\top}A x\\geq0$ . For two symmetric   \n144 matrices $A$ and $B$ , we write $A\\preceq B$ when $B-A$ is positive semidefinite. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "145 Packing SDP. We will use nearly-linear time solvers for the following packing SDP. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{w}\\ \\|w\\|_{1}\\qquad\\mathrm{subject}\\,\\mathrm{to}\\quad\\sum_{i=1}^{n}w_{i}A_{i}\\preceq I,\\quad{\\overline{{\\lambda}}}_{k}\\left(\\sum_{i=1}^{n}w_{i}B_{i}\\right)\\leq k,\\quad w_{i}\\geq0,\\forall i.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "146 Lemma 2.1 ([10]). Given an instance of optimization $(^{*})$ with semi-positive definite matrices   \n147 $A_{i}\\,\\in\\,\\mathbb{R}^{d_{1}\\times d_{1}}$ and $\\boldsymbol{B}_{i}\\in\\mathbb{R}^{d_{2}\\times d_{2}}$ with $A_{i}=\\dot{C}_{i}C_{i}^{\\top}$ , $B_{i}=\\dot{D_{i}}D_{i}^{\\top}$ for all $i=1,2,\\cdots,m,$ , together   \n148 with integer $k>0$ , error tolerance $\\epsilon_{0}\\geq1/m^{2}$ , and failure probability $\\delta_{0}$ , there is an algorithm that   \n149 runs in time $\\widetilde{O}((t_{C}+t_{D}+d_{1}+d_{2})\\,\\mathrm{poly}(1/\\epsilon_{0},\\log1/\\delta_{0}))$ , where $t_{C_{i}}$ and $t_{D_{i}}$ are the time take to   \n150 perform a m atrix product with $C_{i}$ and $D_{i}$ respectively and $\\begin{array}{r}{\\dot{t}_{C}=\\sum_{i=1}^{n}\\dot{t}_{C_{i}}}\\end{array}$ and $\\begin{array}{r}{\\dot{t}_{D}=\\sum_{i=1}^{n}t_{D_{i}}}\\end{array}$ , and   \n151 outputs $w^{\\prime}$ with $\\|w^{\\prime}\\|_{1}\\geq(1-\\epsilon_{0})mathsf{O P T}$ where OPT is optimal value, with probability at least $1-\\delta_{0}$ . ", "page_idx": 3}, {"type": "text", "text": "152 Computing the Top Eigenvector. We use power method to compute the top eigenvector of a matrix. ", "page_idx": 3}, {"type": "text", "text": "153 Lemma 2.2 (Power Method for Top Eigenvector, e.g., [41]). Let $A\\in\\mathbb{R}^{d\\times d}$ and let $\\lambda_{1}$ be its largest   \n154 eigenvalue. For any ${\\overline{{\\delta}}}\\,\\in\\,(0,1)$ , there exists an algorithm that takes $A$ and outputs a unit vector   \n155 $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ in time $O(t\\log(d)/\\overline{{\\delta}})$ such that $x^{T}A x\\geq(1-\\overline{{\\delta}})\\lambda_{1}$ with probability at least 0.99, where $t$ is   \n156 the time required to compute $A v$ for an arbitrary $\\boldsymbol{v}\\in\\mathbb{R}^{d}$ .   \n157 Robust Mean Estimation. Another tool we use is robust mean estimation in the $\\epsilon_{}$ -corruption model   \n158 for distributions with bounded covariance. We use robust mean estimation algorithms to approximate   \n159 the true gradient under adversarial corruption.   \n160 Lemma 2.3 (Robust Mean Estimation, e.g., [15]). Let $\\mathcal{D}$ be a distribution on $\\mathbb{R}^{d}$ with unknown   \n161 mean $\\mu$ and unknown covariance matrix $\\Sigma$ where $\\Sigma\\preceq\\sigma^{2}I$ . Let $\\epsilon_{0}$ be a sufficiently small universal   \n162 constant. Let $0<\\epsilon<\\epsilon_{0}$ and $\\delta>0$ . Given an $\\epsilon$ -corrupted set of $n$ samples drawn from $\\mathcal{D}$ , we can   \n163 output a vector $\\widehat{\\mu}\\in\\mathbb{R}^{d}$ in time ${\\widetilde O}(n d\\log(1/\\delta))$ ) such that, with probability at least $1-\\delta-\\exp(-n\\epsilon),$ ,   \n164 we have $\\begin{array}{r}{\\|\\widehat{\\mu}-\\mu\\|_{2}=O\\left(\\sqrt{\\epsilon}+\\sqrt{\\frac{d}{n\\delta}}+\\sqrt{\\frac{d(\\log d+\\log1/\\delta)}{n}}\\right)\\sigma.}\\end{array}$ ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "165 3 Overview ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "166 We first state a formal version of our main result. ", "page_idx": 3}, {"type": "text", "text": "167 Theorem 3.1 (Main). Consider the setting of Problem 1.3. Let $0\\,<\\,\\epsilon\\,<\\,\\epsilon^{\\prime}$ for some universal   \n168 constant $\\epsilon^{\\prime}$ and let $\\Delta\\,>\\,0$ . Given an $\\epsilon$ -corrupted set of $n\\,=\\,\\widetilde\\Omega(d\\log^{2}(1/\\Delta))$ samples, we can   \n169 compute a vector $z\\in\\mathbb{R}^{d}$ in time ${\\widetilde O}(n d\\log(1/\\Delta))$ such that $r(z)=\\operatorname*{min}\\{\\left\\|z-x\\right\\|_{2},\\left\\|z+x\\right\\|_{2}\\}\\leq\\Delta$   \n170 with probability at least 0.8.   \n171 Theorem 3.1 requires two key technical lemmas: the robust spectral initialization (Lemma 3.2) and   \n172 the approximate gradient descent (Lemma 3.3).   \n173 We first show that the spectral initialization can be done in nearly linear time with high probability,   \n174 the proof of which can be found in Section 4.   \n175 Lemma 3.2 (Robust Spectral Initialization). Under the setting of Problem 1.3, for any $0<\\epsilon<\\epsilon^{\\prime}$ for   \n176 some universal constant $\\epsilon^{\\prime}>0$ , given an \u03f5-corrupted set of $n=\\widetilde\\Omega(d)$ samples, we can compute a vec  \n177 tor $z_{0}\\in\\mathbb{R}^{d}$ of the ground truth $x$ in time ${\\widetilde{O}}(n d)$ such that $\\begin{array}{r}{r(z_{1})=\\operatorname*{min}\\{\\left\\|z_{1}-x\\right\\|_{2},\\left\\|z_{1}+x\\right\\|_{2}\\}\\leq\\frac{1}{8}}\\end{array}$   \n178 with probability at least 0.95.   \n179 Then, with such initialization results, we can proceed to show that an approximate gradient descent   \n180 algorithm can be used to find an arbitrary approximation of the ground truth in Section 5.   \n181 Lemma 3.3 (Robust Gradient Descent). Consider the setting of Problem 1.3. Let $\\Delta\\ >\\ 0$ be   \n182 the desired precision. Let $0\\,<\\,\\epsilon\\,<\\,\\epsilon_{0}$ for some sufficiently small universal constant $\\epsilon_{0}$ . Given   \n183 an $\\epsilon$ -corrupted set of $n\\,=\\,\\widetilde\\Omega(d\\log^{2}(1/\\Delta))$ samples and an initial guess $z_{1}$ such that $r(z_{1})\\,=$   \n184 $\\operatorname*{min}(\\|z_{1}-x\\|_{2}\\,,\\|z_{1}+x\\|_{2})\\,\\leq\\,1/8$ , we can compute a vector $z~\\in~\\mathbb{R}^{d}$ in time ${\\widetilde{O}}(n d)$ such that   \n185 $r(z)\\leq\\Delta$ with probability at least 0.95.   \n186 For technical reasons, we cannot use the same set of samples for both the robust spectral initialization   \n187 and the approximate gradient descent. Therefore, we partition the $\\epsilon$ -corrupted set of $2n$ samples into   \n188 two equally sized disjoint sets, using one set for each algorithm.   \n189 Proof of Theorem 3.1. Let $2n=\\widetilde\\Omega(d\\log^{2}(1/\\Delta))$ be a set of $\\epsilon/2$ -corrupted samples. We partition   \n190 the input into two disjoint sets of $n$ samples. Both sets are $\\epsilon_{}$ -corrupted. By Lemmas 3.2 and 3.3, for   \n191 any $\\epsilon\\in[0,\\epsilon^{\\prime}]$ and $\\Delta>0$ , our algorithm takes the first set of samples and output a vector $z^{\\prime}$ in time   \n192 ${\\widetilde{O}}(n d)$ such that $r(z^{\\prime})\\leq1/8$ with probability at least 0.95. Then, using $z^{\\prime}$ and the second set of   \n193 samples, our algorithm can output $z\\in\\mathbb{R}^{d}$ in time $\\widetilde{O}(n d)$ such that $r(z)\\leq\\Delta$ with probability at least   \n194 0.95. The overall success probability is at least 0.8, and the combined running time is ${\\widetilde{O}}(n d)$ \uff1a\u53e3 ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "195 4 Robust Spectral Initialization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "196 We dedicate this section to proving Lemma 3.2: Given an $\\epsilon$ -corrupted set of $(a_{i},y_{i})$ \u2019s, we can compute   \n197 an initial guess $z_{1}\\in\\mathbb{R}^{d}$ that is close to the ground truth $x$ , where $\\operatorname*{min}(\\|z_{1}-x\\|_{2}\\,,\\|z_{1}+x\\|_{2})\\leq$   \n198 $1/8$ . To build some intuition, consider the following intensity-based covariance matrix $Y=$   \n199 $\\begin{array}{r}{\\frac{1}{n}\\sum_{i=1}^{n}y_{i}a_{i}a_{i}^{\\top}}\\end{array}$ , where each $a_{i}$ is drawn independently from ${\\mathcal{N}}(0,I)$ and $y_{i}~=~\\langle a_{i},x\\rangle^{2}$ . The   \n200 expectation of this matrix is $\\mathbb{E}[Y]=I+2x x^{\\top}$ . In other words, when there are enough samples and   \n201 no adversarial corruption, we can obtain a good guess of the ground truth $x$ $:(\\alpha\\mathbf{r}-\\boldsymbol{x})$ by computing   \n202 the top eigenvector of $Y$ . However, we cannot rely on this approach in adversarial settings.   \n203 To tackle this issue, we propose a nearly-linear time preprocessing step (Algorithm 1) that can recover   \n204 the true expectation of $Y$ under adversarial corruptions. Algorithm 1 assigns a non-negative weight to   \n205 each sample. For a weight vector $w\\in\\mathbb{R}^{n}$ and a set of indices $S\\subseteq[n]$ , the weighted intensity-based   \n206 covariance matrix is defined as $\\begin{array}{r}{Y_{S,w}\\doteq\\sum_{i\\in S}w_{i}y_{i}a_{i}a_{i}^{\\top}}\\end{array}$ , and we omit $S$ when $S=[n]$ . The feasible   \n207 region for the weight vector is: $\\Delta_{n,\\epsilon}:=\\Big\\{w\\in\\mathbb{R}^{n}:\\|w\\|_{1}=1$ and $\\begin{array}{r}{\\forall i\\in[n],0\\leq w_{i}\\leq\\frac{1}{(1-\\epsilon)n}\\Biggr\\}\\,.}\\end{array}$   \n208 A weight $w$ defines an empirical distributions over the samples $(a_{i},y_{i})_{i=1}^{n}$ , where the largest prob  \n209 ability assigned to any point is $\\frac{1}{(1\\!-\\!\\epsilon)n}$ . Ideally, we would like to find a weight vector $w^{\\ast}\\in\\Delta_{n,\\epsilon}$   \n210 that assigns its weight uniformly to all the uncorrupted samples, i.e., $\\begin{array}{r}{w_{i}^{*}=\\frac{1}{(1-\\epsilon)n}\\cdot\\mathbb{1}_{i\\in G}}\\end{array}$ . To find a   \n211 suitable weighting $w$ , we use the following optimization problem $(^{**})$ in which $\\overline{{\\lambda}}_{2}$ returns the sum of   \n212 the top two eigenvalues (commonly known as the Ky Fan $k$ norm for $k=2$ ). ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{w}\\quad\\overline{{\\lambda}}_{2}\\left(\\sum_{i=1}^{n}w_{i}y_{i}a_{i}a_{i}^{\\top}\\right)\\qquad\\mathrm{subject}~\\mathrm{to}\\quad0\\leq w_{i}\\leq\\frac{1}{(1-\\epsilon)n},\\forall i\\in[n],\\quad\\sum_{i=1}^{n}w_{i}=1~.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "213 At a high level, our main observation is that $y_{i}a_{i}a_{i}^{\\top}$ is always a positive semidefinite matrix as $y_{i}\\geq0$ .   \n214 Consequently, the adversary can only add extra directions with large eigenvalues, but will not be able   \n215 to remove the eigendirection of $x$ . By minimizing the Ky Fan 2 norm, we can remove any directions   \n216 added by the adversary and make sure that the only remaining large eigendirection is close to $x$ .   \n217 Let $\\delta\\geq0$ be some constant to be determined. We show that we can obtain a robust spectral initial  \n218 ization by solving the packing SDP problem $(^{*})$ , which can be solved efficiently using Lemma 2.1.   \n219 In particular, to fit the reweighting problem of $(^{**})$ into the framework of the generalized packing   \n220 problem $({^*})$ , we define the following constraint matrices for all $i\\in[n]$ : ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{A_{i}:=(1-\\epsilon)n\\cdot e_{i}e_{i}^{\\top},\\;\\;\\;\\;\\;\\;B_{i}:=\\frac{1}{2}(1-\\delta)y_{i}a_{i}a_{i}^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "221 The matrices $(A_{i})_{i=1}^{n}$ are used to implement the constraint that $w\\in\\Delta_{n,\\epsilon}$ . The matrices $(B_{i})_{i=1}^{n}$ help   \n222 make sure the sum of the top two eigenvalues of $Y_{w}$ must be at most roughly 4, because $\\overline{{\\lambda}}_{2}(Y_{w^{*}})\\approx4$ ", "page_idx": 4}, {"type": "text", "text": "Input: $\\epsilon$ -corrupted samples $(a_{i},y_{i})_{i\\in[n]}$ Output: The initial guess $z^{\\prime}\\in\\mathbb R^{d}$ 1: function ROBUSTINIT $\\langle\\{(a_{i},y_{i})\\}_{i\\in[n]})$ 2: $\\left\\{A_{i},B_{i}\\right\\}\\gets\\mathsf{C}$ onstraint matrices as defined in Equation (2) 3: $w^{\\prime\\prime}\\gets$ Solution to Optimization $(^{*})$ with constraints $\\{A_{i},B_{i}\\}$ , $k=2$ , and precision $\\epsilon_{0}=0.9$ as in Lemma 2.1 4: $w^{\\prime}\\leftarrow w^{\\prime\\prime}/\\Vert w^{\\prime\\prime}\\Vert_{1}$ 5: $z_{1}\\gets\\mathrm{TopEIGENVECTOR}(Y_{w^{\\prime}})$ as in Lemma 2.2 with sufficiently small constant $\\overline{{\\delta}}$ . 6: return $z_{1}$ 7: end function ", "page_idx": 5}, {"type": "text", "text": "223 First, we show that the weight $w^{\\prime}$ computed by Algorithm 1 can ensure the weighted intensity-based   \n224 covariance matrix $Y_{w^{\\prime}}$ is close enough to the unbiased expectation $I+2x x^{\\top}$ . ", "page_idx": 5}, {"type": "text", "text": "225 Lemma 4.1. With probability at least 0.98, the $w^{\\prime}$ outputted by Algorithm $^{\\,l}$ satisfies: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\left\\|Y_{w^{\\prime}}-(I+2x x^{\\top})\\right\\|_{2}=O(\\delta)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "226 In order to show Lemma 4.1, we need the following auxiliary Lemma 4.2, the proof of which can be   \n227 found in Section A. Intuitively, Lemma 4.2 suggests that any weight $w$ in the feasible region $\\Delta_{n,2\\epsilon}$   \n228 will not have a huge impact on the properties of uncorrupted measurements.   \n229 Lemma 4.2. For any $\\delta_{0}>0$ , and sufficiently small $\\epsilon\\geq0$ , given a set of n $\\epsilon$ -corrupted samples with   \n230 $n>{\\widetilde\\Omega}(d)$ , with probability at least 0.98, we have $\\left\\|Y_{G,w}-\\left(I+2x x^{\\top}\\right)\\right\\|_{2}\\leq\\delta_{0}$ for all $w\\in\\Delta_{n,2\\epsilon}$ . ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "231 Using Lemma 4.2, we provide a proof sketch for Lemma 4.1, and defer the details to Section A. ", "page_idx": 5}, {"type": "text", "text": "232 Proof. We condition on the fact that the event of Lemma 4.2 holds (with probability at least 0.98)   \n233 for $\\delta_{0}=\\delta$ . Thus, for the remaining of the proof, we assume that for all $w\\in\\Delta_{n,2\\epsilon}$ , it holds that   \n234 $\\left\\|Y_{G,w}-(I+2x x^{\\top})\\right\\|_{2}\\leq\\delta$ .   \n235 Let $\\lambda_{1}$ and $\\lambda_{2}$ be the top two eigenvalues of $Y_{w^{\\prime}}$ , with $v_{1}$ and $v_{2}$ to be their corresponding eigenvectors.   \n236 Note that the largest eigenvalue of $I+2x x^{\\top}$ is 3, and the rest of the eigenvalues are all 1. In the   \n237 proof, we show that the eigenvalues of $Y_{w^{\\prime}}$ are also close to the ones of $I+2x x^{\\top}$ . Our proof consists   \n238 of two parts. We first establish lower bounds for $\\lambda_{1}$ and $\\lambda_{2}$ , and then find an upper bound for $\\lambda_{1}+\\lambda_{2}$ .   \n239 Lower Bound. Since $y_{i}a_{i}a_{i}^{\\top}\\succeq0$ for any $i\\in[n]$ , for any positive weight vector $w\\in\\Delta_{n,\\epsilon}$ , we   \n240 have $Y_{G,w}\\preceq Y_{w}$ . Thus a lower bound on eigenvalues of $Y_{G,w^{\\prime}}$ will also be a lower bound on $Y_{w^{\\prime}}$ . ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "241 For the top eigenvalue $\\lambda_{1}$ of $Y_{w^{\\prime}}$ , it holds ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\lambda_{1}=v_{1}^{\\top}Y_{w^{\\prime}}v_{1}\\geq x^{\\top}Y_{w^{\\prime}}x\\geq x^{\\top}Y_{G,w^{\\prime}}x\\geq x^{\\top}(I+2x x^{\\top})x-\\delta=3-\\delta.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "242 Similarly, for the second largest eigenvalue $\\lambda_{2}$ of $Y_{w^{\\prime}}$ , we have: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\lambda_{2}=v_{2}^{\\top}Y_{w^{\\prime}}v_{2}\\geq v_{2}^{\\top}Y_{G,w^{\\prime}}v_{2}\\geq v_{2}^{\\top}(I+2x x^{\\top})v_{2}-\\delta=1+2\\left<v_{2},x\\right>^{2}-\\delta\\geq1-\\delta.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "243 Upper Bound. Through the optimization problem $({^*})$ , a weight $w^{\\prime\\prime}$ is calculated such that $Y_{w^{\\prime\\prime}}$   \n244 are operator-norm upper-bounded by the constraint parameters. Let OPT be the value of the   \n245 optimal solution of the optimization problem $({^*})$ . The desired uniform weight vector over the good   \n246 samples $w^{*}\\in\\Delta_{n,2\\epsilon}$ is also a feasible solution to this optimization problem because $Y_{w^{*}}$ satisfy the   \n247 optimization constraints due to Lemma 4.2. Since $w^{\\prime\\prime}$ is an $\\epsilon_{0}$ -approximation to the problem, we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|w^{\\prime\\prime}\\|_{1}\\geq(1-\\epsilon_{0})0\\mathsf{P T}\\geq(1-\\epsilon_{0})\\,\\|w^{*}\\|_{1}=1-\\epsilon_{0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "248 By optimization constraints, the Ky Fan 2-norm of $\\sum_{i}w_{i}^{\\prime\\prime}B_{i}=\\textstyle\\frac12(1-\\delta)Y_{w^{\\prime\\prime}}\\leq2$ , and consequently, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lambda_{1}+\\lambda_{2}=\\frac{1}{\\Vert w^{\\prime\\prime}\\Vert_{1}}\\overline{{\\lambda}}_{2}(Y_{w^{\\prime\\prime}})\\leq\\frac{4}{(1-\\epsilon_{0})(1-\\delta)}\\ \\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "249 By combining inequalities (4), (5), and (6), we have shown that the top two eigenvalues of $Y_{w^{\\prime}}$ are   \n250 close to 3 and 1. Since the rest of the eigenvalues of $Y_{w^{\\prime}}$ can also be bounded, we can conclude that   \n251 $\\left\\|Y_{w^{\\prime}}-(I+2x x^{\\top})\\right\\|_{2}=O(\\delta)$ . \u53e3 ", "page_idx": 5}, {"type": "text", "text": "252 We can now show the closeness between the top eigenvector of $Y_{w^{\\prime}}$ and the ground truth. ", "page_idx": 6}, {"type": "text", "text": "253 Lemma 4.3. There exists an universal constant $\\epsilon^{\\prime}$ such that i $f0\\leq\\epsilon\\leq\\epsilon^{\\prime}$ , and Algorithm 1 receives   \n254 in input an $\\epsilon$ -corrupted set of samples, then it outputs $z_{1}\\in\\mathbb{R}^{d}$ such that with probability at least 0.95   \n255 it holds $\\begin{array}{r}{r(z_{1})\\leq\\frac{1}{8}}\\end{array}$ .   \n256 Proof. We condition on the fact that the event of Lemma 4.1 holds (with probability at least 0.98).   \n257 Let the eigendecomposition of $Y_{w^{\\prime}}$ be $\\begin{array}{r}{Y_{w^{\\prime}}\\;=\\;\\sum_{i\\in[d]}\\lambda_{i}v_{i}v_{i}^{\\top}}\\end{array}$ , where $\\lambda_{1}\\;\\geq\\;.\\;.\\;\\geq\\;\\lambda_{d}$ . Under   \n258 the basis $\\{v_{1},\\cdot\\cdot\\cdot,v_{d}\\}$ , the ground truth $x$ can be represented as $\\textstyle x\\;=\\;\\sum_{i\\in[d]}\\alpha_{i}v_{i}$ . Note that   \n259 $\\begin{array}{r}{\\|\\boldsymbol{x}\\|_{2}^{2}=\\sum_{i\\in[d]}\\alpha_{i}^{2}=1}\\end{array}$ . By Lemma 4.1, we have $\\left\\|Y_{w^{\\prime}}-(I+2x x^{\\top})\\right\\|_{2}=O(\\delta)$ . Thus, we have   \n$\\begin{array}{r l}&{x^{\\top}Y_{w^{\\prime}}x\\geq3-O(\\delta)\\qquad\\mathrm{and}}\\\\ &{x^{\\top}Y_{w^{\\prime}}x\\leq\\lambda_{1}\\alpha_{1}^{2}+\\lambda_{2}(1-\\alpha_{1}^{2})\\leq(3+O(\\delta))\\alpha_{1}^{2}+(1+O(\\delta))(1-\\alpha_{1}^{2})\\leq1+2\\alpha^{2}+O(\\delta).}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "260 This implies $\\alpha_{1}^{2}\\geq1-O(\\delta)$ . As a result, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{r^{2}(v_{1})=\\displaystyle\\Big(\\!\\operatorname*{min}\\{\\|v_{1}-x\\|_{2}^{2},\\|v_{1}+x\\|_{2}^{2}\\}\\Big)=\\operatorname*{min}\\{(\\alpha_{1}-1)^{2},(\\alpha_{1}+1)^{2}\\}+\\sum_{i=2}^{d}\\alpha_{i}^{2}}}\\\\ {{\\displaystyle~~~~~~~~~=\\operatorname*{min}\\{2-2\\alpha_{1},2+2\\alpha_{1}\\}=O(\\delta).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "261 The last inequality holds as long as $\\delta$ is sufficiently small. Let $\\begin{array}{r}{z_{1}=\\sum_{i\\in[d]}\\beta_{i}v_{i}}\\end{array}$ be the unit vector   \n262 approximating $v_{1}$ returned by the algorithm. By Lemma 2.2, we have that $z_{1}^{\\top}Y_{w^{\\prime}}z_{1}\\geq(1-\\overline{{{\\delta}}})\\lambda_{1}$   \n263 with probability at least 0.99. Thus, we have: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{z_{1}^{\\top}Y_{w^{\\prime}}z_{1}\\geq(1-\\overline{{\\delta}})\\lambda_{1}\\geq(1-\\overline{{\\delta}})(3-O(\\delta))\\geq3-O(\\delta+\\overline{{\\delta}})\\qquad\\mathrm{and}}\\\\ &{z_{1}^{\\top}Y_{w^{\\prime}}z_{1}\\leq\\lambda_{1}\\alpha_{1}^{2}+\\lambda_{2}(1-\\alpha_{1}^{2})\\leq1+2\\beta_{1}^{2}+O(\\delta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "264 Again, this implies that $\\beta_{1}^{2}\\geq1-O(\\delta+\\overline{{\\delta}})$ . We can show that $\\begin{array}{r l}{\\operatorname*{min}\\lbrace\\left\\|v_{1}-z_{1}\\right\\|_{2}^{2},\\|v_{1}+z_{1}\\|_{2}^{2}\\rbrace=}\\end{array}$   \n265 $O(\\overline{{\\delta}}+\\delta)$ . By the triangle inequality, we can conclude that $r^{2}(z_{1})=O(\\overline{{\\delta}}+\\delta)\\leq1/64$ , where the   \n266 last inequality is obtained by choosing sufficiently small $\\delta$ and $\\overline{{\\delta}}$ . Therefore, there exists an universal   \n267 constant $\\epsilon^{\\prime}\\geq0$ such that for all $0\\le\\epsilon\\le\\epsilon^{\\prime}$ , Algorithm 1 takes $n=\\widetilde\\Omega(d)$ samples and outputs $z_{1}$   \n268 such that $r(z_{1})\\leq1/8$ with probability at least 0.95. $\\sqsupset$ ", "page_idx": 6}, {"type": "text", "text": "269 Lemma 4.4. Algorithm 1 runs in time ${\\widetilde{O}}(n d)$ . ", "page_idx": 6}, {"type": "text", "text": "270 Proof of Lemma 4.4. Since we have the factorization of the rank-two matrices $A_{i}$ and rank-one   \n271 matrices $B_{i}$ for all $i=1,2,\\dots,n$ , and the time to perform a matrix-vector product with $C_{i}$ and $D_{i}$ is   \n272 $O(d)$ . Therefore, by Lemma 2.1, with $t_{C}$ and $t_{D}$ to be ${\\widetilde{O}}(n d)$ , Line 3 runs in ${\\widetilde{O}}(n d)$ time. In Line 5,   \n273 by Lemma 2.2, the top eigenvector of $Y_{w^{\\prime}}$ can be computed in ${\\widetilde{O}}(n\\log d)$ time using power method.   \n274 Scaling in Line 4 runs in $O(n)$ time. As a result, Algorithm 1 runs in ${\\widetilde{O}}(n d)$ time. \u53e3 ", "page_idx": 6}, {"type": "text", "text": "275 We can directly combine Lemma 4.3 and Lemma 4.4 to finish the proof of Lemma 3.2. ", "page_idx": 6}, {"type": "text", "text": "276 5 Robust Gradient Descent ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "277 After the robust spectral initialization in Section 4, we have an initial guess $z_{1}\\in\\mathbb{R}^{d}$ that is close to   \n278 the ground truth $x$ or $-x$ . Without loss of generality, we can assume that $z_{1}$ is closer to $x$ than to $-x$ .   \n279 In this section, we prove Lemma 3.3: Given an initial guess $z_{1}$ with $\\|z_{1}-x\\|_{2}\\leq1/8$ , we can use   \n280 a robust gradient descent algorithm (Algorithm 2) to recover $x$ to any desire precision $\\Delta>0$ . It is   \n281 well-known that gradient descent can achieve geometric convergence rates in non-adversarial settings.   \n282 We show that Algorithm 2 achieves a similar convergence rate even when the input is $\\epsilon$ -corrupted.   \n283 Consider the natural nonconvex formulation: $\\begin{array}{r}{\\operatorname*{min}_{z\\in\\mathbb{R}^{d}}\\;\\sum_{i=1}^{n}f_{i}(z)}\\end{array}$ where $f_{i}(z)\\doteq\\left(\\langle a_{i},z\\rangle^{2}-y_{i}\\right)^{2}$ .   \n284 Let $g_{i}$ denote the gradient of $f_{i}$ with respect to $z$ . Let $\\mathcal{D}_{z}$ denote the distribution of $g_{i}(z)\\in\\mathbb{R}^{d}$ when   \n285 there is no adversarial corruption. Formally, $g(z)\\sim\\mathcal{D}_{z}$ is distributed as ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\ng(z)={\\frac{\\partial}{\\partial z}}\\left[\\left(\\langle a,z\\rangle^{2}-\\langle a,x\\rangle^{2}\\right)^{2}\\right]=-4\\left(\\langle a,z\\rangle^{2}-\\langle a,x\\rangle^{2}\\right)\\langle a,z\\rangle a\\;\\;{\\mathrm{where}}\\;\\;a\\sim{\\mathcal{N}}(0,I)\\ \\;.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "286 To run gradient descent, we want to estimate the expected true gradient $\\mu_{z}\\doteq\\mathbb{E}\\,g(z)$ using samples.   \n287 The challenge is that the input samples $\\{(a_{i},y_{i})\\}_{i\\in[n]}$ are $\\epsilon$ -corrupted, and consequently the gradients   \n288 $\\{g_{i}(z)\\}_{i\\in[n]}$ is an $\\epsilon$ -corrupted set of vectors drawn from $\\mathcal{D}_{z}$ . To address this, we use robust mean   \n289 estimation algorithms (e.g., [16]) to approximate $\\mu_{z}$ , the true mean of $\\mathcal{D}_{z}$ . ", "page_idx": 7}, {"type": "text", "text": "Algorithm 2 Robust Gradient Descent   \nInput: $\\epsilon\\,>\\,0$ , an $\\epsilon$ -corrupted set of $n$ samples $\\{(a_{i},y_{i})\\}_{i\\in[n]}$ , initial guess $z_{1}\\,\\in\\,\\mathbb{R}^{d}$ with $\\|z_{1}-x\\|\\leq1/8$ , and desired precision $\\Delta>0$ .   \nOutput: $z\\in\\mathbb{R}^{d}$ such that $\\|z-x\\|_{2}\\leq\\Delta$ where $x$ is the ground truth.   \n1: procedure ROBUSTGD $(\\epsilon,\\{(a_{i},y_{i})\\}_{i\\in[n]},z_{1},\\Delta)$   \n2: $T\\leftarrow O(\\log(1/\\Delta))$ , $\\eta\\leftarrow1/300$   \n3: $\\{N_{1},N_{2},\\cdot\\cdot\\cdot,N_{T}\\}\\gets\\mathrm{a}$ random disjoint partition of $[n]$ such that $|N_{t}|=n/T$ for all $t\\in[T]$ 4: for $t=1,2,\\ldots,T$ do   \n5: $\\widehat{\\mu}_{z_{t}}\\gets$ Robust mean estimation on input $\\{g_{i}(z_{t})\\}_{i\\in N_{t}}$ using Lemma 5.2   \n6: $z_{t+1}\\leftarrow z_{t}-\\eta\\,\\widehat{\\mu}_{z_{t}}$   \n7: end for   \n8: return zT +1   \n9: end procedure ", "page_idx": 7}, {"type": "text", "text": "290 The error guarantee of robust mean estimation algorithms depends on the covariance matrix $\\Sigma_{z}$ of   \n291 the distribution $\\mathcal{D}_{z}$ . The next lemma upper bounds the spectral norm of $\\Sigma_{z}$ .   \n292 Lemma 5.1. Let $\\mathcal{D}_{z}$ be the distribution of gradients at $z$ as defined in Equation (7). For any $z\\in\\mathbb{R}^{d}$   \n293 with $\\left\\|z-x\\right\\|_{2}\\leq1$ , the covariance matrix $\\Sigma_{z}$ of $\\mathcal{D}_{z}$ satisfies $\\Sigma_{z}\\preceq O(\\left\\|z-x\\right\\|_{2}^{2})I.$ .   \n294 The proof of Lemma 5.1 is deferred to Appendix B. Given Lemma 5.1, we can show that robust   \n295 mean estimation algorithms can approximate $\\mu_{z}$ with small error. For technical reasons, we randomly   \n296 partition the input samples $(a_{i},y_{i})$ into $T$ subsets, and use one subset in each iteration. With high   \n297 probability, each partition has at most $(2\\epsilon)$ -fraction of corrupted samples   \n298 Lemma 5.2. Consider any $z\\,\\in\\,\\mathbb{R}^{d}$ with $\\|z-x\\|_{2}\\leq1$ . Let $\\mu_{z}$ be the mean of $\\mathcal{D}_{z}$ as defined in   \n299 Equation (7). Fix universal constants $c\\,>\\,0$ and $\\epsilon_{0}\\,=\\,\\Theta(c^{2})$ . Let $2\\epsilon\\,<\\,\\epsilon_{0}$ and $\\delta\\,>\\,0$ . Given   \n300 $a$ (2\u03f5)-corrupted set of $m\\,=\\,\\Omega(d\\log d/\\delta)$ samples drawn from $\\mathcal{D}_{z}$ , we can compute $\\widehat{\\mu}_{z}$ in time   \n301 $\\widetilde{O}(m d\\log(1/\\delta))$ such that $\\|\\widehat{\\mu}_{z}-\\mu_{z}\\|_{2}\\leq c\\,\\|z-x\\|_{2}$ with probability at least $1-O(\\delta)$ .   \n302 Proof of Lemma 5.2. Since $2\\epsilon<\\epsilon_{0}$ , we can view the $(2\\epsilon)$ -corrupted set of $m$ samples as $\\epsilon_{0}$ -corrupted.   \n303 We need to replace $2\\epsilon$ with $\\epsilon_{0}$ to reduce the failure probability of Lemma 2.3. This weakens the error   \n304 guarantee of Lemma 2.3, but the resulting $\\widehat{\\mu}_{z}$ is still accurate enough for our algorithm.   \n305 We apply Lemma 2.3 to the $\\epsilon_{0}$ -corrupted set of $m$ vectors drawn from $\\mathcal{D}_{z}$ . By Lemma 5.1,   \n306 the covariance matrix of $\\mathcal{D}_{z}$ satisfies $\\Sigma_{z}\\,\\leq\\,O(\\|z-x\\|_{2}^{2})I$ . Consequently, for sufficiently large   \n307 $m\\ =\\ \\Theta(d\\log d/\\delta)$ and sufficiently small $\\epsilon_{0}~=~O(c^{2})$ , the error guarantee of Lemma 2.3 is   \n308 $\\begin{array}{r}{O\\left(\\sqrt{\\epsilon_{0}}+\\sqrt{\\frac{d}{m\\delta}}+\\sqrt{\\frac{d(\\log d+\\log(1/\\delta))}{m}}\\right)\\|z-x\\|_{2}\\le c\\,\\|z-x\\|_{2}.}\\end{array}$ . The success probability is at least   \n309 $\\begin{array}{r}{1-\\delta-\\exp(-\\epsilon_{0}m)=1-O(\\delta).}\\end{array}$ . \u53e3   \n310 Lemma 5.2 shows that even with a $(2\\epsilon)$ -corrupted set of gradients, the true gradient $\\mu_{z}$ can be   \n311 estimated up to an additive error proportional to the distance between $z$ and $x$ . The next lemma shows   \n312 that such an approximate gradient is sufficient for gradient descent to converge, reducing the distance   \n313 to the ground truth $x$ by a constant factor in each iteration.   \n314 Lemma 5.3. Suppose in iteration t of Algorithm 2, the current solution $z_{t}$ satisfies $\\|z_{t}-x\\|_{2}\\leq1/8,$ ,   \n315 and the estimated gradient $\\widehat{\\mu}_{z_{t}}\\in\\mathbb{R}^{d}$ satisfies $\\|\\widehat{\\mu}_{z_{t}}-\\mu_{z_{t}}\\|_{2}\\leq c\\,\\|z_{t}-x\\|_{2}$ for $c=4$ . Then, we have   \n316 $\\left\\|\\boldsymbol{z}_{t+1}-\\boldsymbol{x}\\right\\|_{2}^{2}\\leq0.99\\left\\|\\boldsymbol{z}_{t}-\\boldsymbol{x}\\right\\|_{2}^{2},$ .   \n317 Proof Sketch of Lemma 5.3. We provide a proof sketch and defer the full proof to Appendix B. Our   \n318 objective function is nonconvex (even with infinitely many samples and no corruption). However,   \n319 when the starting point $z_{1}$ is close to a global optimum, it is well-known that gradient descent is   \n320 well-behaved. More specifically, for any $z$ close to the ground truth $x$ , we can show that the (expected)   \n321 true gradient $\\mu_{z}$ aligns with the direction toward $x$ : ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "equation", "text": "$$\n\\left\\langle\\mu_{z},z-x\\right\\rangle\\geq7.5\\left\\|z-x\\right\\|_{2}^{2}\\ \\mathrm{{and}}\\ \\ \\left\\|\\mu_{z}\\right\\|_{2}\\leq29\\left\\|z-x\\right\\|_{2}\\ ,\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "322 which is sufficient for proving geometric convergence. We can immediately see that this argument is   \n323 robust to additive error in $\\mu_{z}$ that is proportional to $\\|z-x\\|_{2}$ . When $\\|\\widehat{\\mu}_{z}-\\mu_{z}\\|_{2}\\leq c\\,\\|z-x\\|_{2}$ , ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\left\\langle\\widehat{\\mu}_{z},z-x\\right\\rangle\\geq\\left(7.5-c\\right)\\left\\Vert z_{t}-x\\right\\Vert_{2}^{2}\\mathrm{~}\\mathrm{~and~}\\mathrm{~}\\left\\Vert\\widehat{\\mu}_{z_{t}}\\right\\Vert_{2}\\leq\\left(29+c\\right)\\left\\Vert z_{t}-x\\right\\Vert_{2}\\ .\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "324 When $c<7.5$ , we can choose an appropriate step size $\\eta$ such that the distance between $z_{t}$ and $x$   \n325 decreases by a constant factor in each iteration. \u53e3   \n326 We are now ready to prove Lemma 3.3, which states the performance guarantee, sample complexity,   \n327 runtime, and success probability of Algorithm 2. We restate Lemma 3.3 before proving it.   \n328 Lemma 3.3 (Robust Gradient Descent). Consider the setting of Problem 1.3. Let $\\Delta\\ >\\ 0$ be   \n329 the desired precision. Let $0\\,<\\,\\epsilon\\,<\\,\\epsilon_{0}$ for some sufficiently small universal constant $\\epsilon_{0}$ . Given   \n330 an $\\epsilon$ -corrupted set of $n\\,=\\,\\widetilde\\Omega(d\\log^{2}(1/\\Delta))$ samples and an initial guess $z_{1}$ such that $r(z_{1})\\,=$   \n331 $\\operatorname*{min}(\\|z_{1}-x\\|_{2}\\,,\\|z_{1}+x\\|_{2})\\,\\leq\\,1/8$ , we can compute a vector $z~\\in~\\mathbb{R}^{d}$ in time ${\\widetilde{O}}(n d)$ such that   \n332 $r(z)\\leq\\Delta$ with probability at least 0.95.   \n333 Proof of Lemma 3.3. First, we prove the success probability of Algorithm 2. Algorithm 2 can fail in   \n334 two ways: $(i)$ if some $N_{t}$ has more than $(2\\epsilon)$ -fraction of corrupted samples, or $(i i)$ if Lemma 5.2 fails   \n335 in some iteration $t$ . The probability of event $(i)$ is at most 0.01 for our choice of $n$ , which follows   \n336 from a standard application of Hoeffding\u2019s inequality and a union bound over $T$ iterations. For event   \n337 $(i i)$ , we choose a sufficiently small $\\delta=\\bar{O}(1/\\bar{T})$ in Lemma 5.2, so each robust gradient estimation   \n338 fails with probability at most $O(\\delta)=0.01/T$ , and overall the probability of event $(i i)$ is at most 0.01.   \n339 For the rest of the proof, we assume these bad events do not happen.   \n340 Next, we show the correctness of Algorithm 2. Without loss of generality, we can assume that $z_{1}$ is   \n341 closer to the ground truth $x$ than to $-x$ , which implies $\\|z_{1}-\\bar{x}\\|_{2}\\leq1/8$ . By Lemma 5.2, we can   \n342 obtain an approximation $\\widehat{\\mu}_{z_{1}}$ of the true gradient $\\mu_{z_{1}}$ at $z_{1}$ such that $\\begin{array}{r}{\\|\\dot{\\boldsymbol{\\mu}}_{z_{1}}-\\boldsymbol{\\mu}_{z_{1}}\\|_{2}\\leq c\\,\\|z_{1}-x\\|_{2}}\\end{array}$ .   \n343 Then by Lemma 5.3, we know that $\\left\\|z_{2}-x\\right\\|_{2}\\leq0.99\\left\\|z_{1}-x\\right\\|_{2}$ after one step of gradient descent.   \n344 We can iteratively apply these two lemmas to show that, after $T\\,=\\,O(\\log(\\bar{1}/\\Delta))$ iterations, we   \n345 have $\\left\\Vert z_{T+1}-x\\right\\Vert_{2}\\leq\\Delta$ . One technical issue is that in iteration $t$ , we need to use a fresh subset of   \n346 samples $N_{t}$ . By the principle of deferred decisions, we can view $(a_{i},y_{i})_{i\\in N_{t}}$ as being generated (and   \n347 corrupted) after $z_{t}$ is chosen, which forms a $(2\\epsilon)$ -corrupted set of gradients at $z_{t}$ .   \n348 Finally, we analyze the sample complexity and runtime of Algorithm 2. Algorithm 2 requires in total   \n349 $n=m T=\\Omega(d\\log d\\log^{2}(1/\\Delta))$ samples. A random partition can be computed in $O(n)$ time by   \n350 shuffling the input. In each iteration, the $m$ gradients in $N_{t}$ can be computed using Equation (7) in   \n351 time $O(m d)$ , and $z_{t}$ can be updated in time $O(d)$ . By Lemma 5.2, the true gradient can be robustly   \n352 estimated in time $\\widetilde{O}(m d\\log T)=\\widetilde{O}(m d\\log\\log(1/\\Delta))$ . The overall runtime of the algorithm is   \n353 $\\widetilde{O}(n+(m d\\log\\log(1/\\Delta))T)=\\widetilde{O}(n d\\log\\log(1/\\Delta))=\\widetilde{O}(n d).$ . \u53e3 ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "354 6 Conclusions and Future Directions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "355 In this paper, our main conceptual contribution is to propose and study the outlier-robust phase   \n356 retrieval problem, where a constant fraction of the input data is corrupted. Notably, we allow   \n357 corruption in both the sampled frequencies $a_{i}\\in\\mathbb{R}^{d}$ and the intensity measurements $y_{i}\\in\\mathbb{R}$ . Our   \n358 main technical contribution is the design and analysis of a near-sample-optimal and nearly-linear-time   \n359 algorithm that solves this problem with provably guarantees.   \n360 An immediate technical question is whether our sample complexity can be tightened by removing   \n361 some $\\log(1/\\Delta)$ factors. One potential approach is to open robust mean estimation algorithms instead   \n362 of using them in a black-box manner. One could examine the stability conditions that these algorithms   \n363 require, and see if these stability conditions can be proved without partitioning the samples and using   \n364 fresh samples in each iteration. More broadly, we believe our framework can be used to develop   \n365 outlier-robust algorithms for other tractable nonconvex problems, by first finding an initial solution in   \n366 a saddle-free region near a global optimum and then converging to this global optimum using robust   \n367 gradient descent. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "368 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "369 [1] A. S. Bandeira, N. Boumal, and V. Voroninski. On the low-rank approach for semidefinite   \n370 programs arising in synchronization and community detection. In Conference on learning   \n371 theory, pages 361\u2013382. PMLR, 2016.   \n372 [2] S. Bhojanapalli, B. Neyshabur, and N. Srebro. Global optimality of local search for low rank   \n373 matrix recovery. Advances in Neural Information Processing Systems, 29, 2016.   \n374 [3] E. J. Cand\u00e8s, X. Li, and M. Soltanolkotabi. Phase retrieval via wirtinger flow: Theory and   \n375 algorithms. IEEE Trans. Inf. Theory, 61(4):1985\u20132007, 2015a.   \n376 [4] E. J. Cand\u00e8s, X. Li, and M. Soltanolkotabi. Phase retrieval from coded diffraction patterns.   \n377 Applied and Computational Harmonic Analysis, 39(2):277\u2013299, Sept. 2015b. ISSN 1063-5203.   \n378 [5] E. J. Cand\u00e8s, Y. C. Eldar, T. Strohmer, and V. Voroninski. Phase retrieval via matrix completion.   \n379 SIAM Rev., 57(2):225\u2013251, 2015c.   \n380 [6] J. Chen, L. Wang, X. Zhang, and Q. Gu. Robust Wirtinger Flow for Phase Retrieval with   \n381 Arbitrary Corruption, Jan. 2018.   \n382 [7] Y. Chen and E. Candes. Solving Random Quadratic Systems of Equations Is Nearly as Easy as   \n383 Solving Linear Systems. In Advances in Neural Information Processing Systems, volume 28.   \n384 Curran Associates, Inc., 2015.   \n385 [8] Y. Cheng and R. Ge. Non-convex matrix completion against a semi-random adversary. In   \n386 Conference On Learning Theory, pages 1362\u20131394. PMLR, 2018.   \n387 [9] Y. Cheng, I. Diakonikolas, and R. Ge. High-dimensional robust mean estimation in nearly-linear   \n388 time. In Proceedings of the 30th ACM-SIAM Symposium on Discrete Algorithms (SODA), pages   \n389 2755\u20132771. SIAM, 2019.   \n390 [10] Y. Cherapanamjeri, S. Mohanty, and M. Yau. List decodable mean estimation in nearly linear   \n391 time. In S. Irani, editor, 61st IEEE Annual Symposium on Foundations of Computer Science,   \n392 FOCS, pages 141\u2013148. IEEE, 2020.   \n393 [11] C. Dainty and J. R. Fienup. Phase retrieval and image reconstruction for astronomy. Image   \n394 recovery: theory and application, 231:275, 1987.   \n395 [12] I. Diakonikolas and D. M. Kane. Algorithmic High-Dimensional Robust Statistics. Cambridge   \n396 University Press, 2023.   \n397 [13] I. Diakonikolas, G. Kamath, D. M. Kane, J. Li, A. Moitra, and A. Stewart. Robust estimators in   \n398 high dimensions without the computational intractability. In 57th Annual IEEE Symposium on   \n399 Foundations of Computer Science\u2014FOCS 2016, pages 655\u2013664. IEEE Computer Soc., Los   \n400 Alamitos, CA, 2016.   \n401 [14] I. Diakonikolas, G. Kamath, D. Kane, J. Li, J. Steinhardt, and A. Stewart. Sever: A robust   \n402 meta-algorithm for stochastic optimization. In K. Chaudhuri and R. Salakhutdinov, editors, Pro  \n403 ceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings   \n404 of Machine Learning Research, pages 1596\u20131606. PMLR, 09\u201315 Jun 2019.   \n405 [15] Y. Dong, S. Hopkins, and J. Li. Quantum Entropy Scoring for Fast Robust Mean Estimation and   \n406 Improved Outlier Detection. In Advances in Neural Information Processing Systems, volume 32.   \n407 Curran Associates, Inc., 2019.   \n408 [16] Y. Dong, S. B. Hopkins, and J. Li. Quantum entropy scoring for fast robust mean estimation and   \n409 improved outlier detection. In Proc. 33rd Advances in Neural Information Processing Systems   \n410 (NeurIPS), 2019.   \n411 [17] J. R. Fienup. Reconstruction of an object from the modulus of its Fourier transform. Optics   \n412 Letters, 3(1):27\u201329, July 1978. ISSN 1539-4794.   \n413 [18] J. R. Fienup. Phase retrieval algorithms: A comparison. Applied Optics, 21(15):2758\u20132769,   \n414 Aug. 1982. ISSN 2155-3165.   \n415 [19] X. Gao and Y. Cheng. Robust matrix sensing in the semi-random model. Proceedings of the   \n416 37th Conference on Neural Information Processing Systems (NeurIPS), 2023.   \n417 [20] R. Ge, F. Huang, C. Jin, and Y. Yuan. Escaping from saddle points\u2014online stochastic gradient   \n418 for tensor decomposition. In Conference on Learning Theory, pages 797\u2013842, 2015.   \n419 [21] R. Ge, J. D. Lee, and T. Ma. Matrix completion has no spurious local minimum. In Advances in   \n420 Neural Information Processing Systems, pages 2973\u20132981, 2016.   \n421 [22] R. W. Gerchberg. A practical algorithm for the determination of phase from image and   \n422 diffraction plane pictures. Optik, Jan. 1972.   \n423 [23] F. R. Hampel, E. M. Ronchetti, P. J. Rousseeuw, and W. A. Stahel. Robust statistics. The   \n424 approach based on influence functions. Wiley New York, 1986.   \n425 [24] P. Hand and V. Voroninski. Corruption robust phase retrieval via linear programming. CoRR,   \n426 abs/1612.03547, 2016.   \n427 [25] P. J. Huber and E. M. Ronchetti. Robust statistics. Wiley New York, 2009.   \n428 [26] K. Jaganathan, Y. C. Eldar, and B. Hassibi. Phase retrieval: An overview of recent developments.   \n429 Optical Compressive Imaging, pages 279\u2013312, 2016.   \n430 [27] R. Kolte and A. \u00d6zg\u00fcr. Phase Retrieval via Incremental Truncated Wirtinger Flow, June 2016.   \n431 [28] K. A. Lai, A. B. Rao, and S. Vempala. Agnostic estimation of mean and covariance. In focs2016,   \n432 pages 665\u2013674, 2016.   \n433 [29] G. Lecu\u00e9, M. Lerasle, and T. Mathieu. Robust classification via MOM minimization. Mach.   \n434 Learn., 109(8):1635\u20131665, 2020.   \n435 [30] S. Li, Y. Cheng, I. Diakonikolas, J. Diakonikolas, R. Ge, and S. Wright. Robust second-order   \n436 nonconvex optimization and its application to low rank matrix sensing. In Proc. 37th Advances   \n437 in Neural Information Processing Systems (NeurIPS), 2023.   \n438 [31] J.-W. Liu, Z.-J. Cao, J. Liu, X.-L. Luo, W.-M. Li, N. Ito, and L.-C. Guo. Phase Retrieval via   \n439 Wirtinger Flow Algorithm and Its Variants. In 2019 International Conference on Machine   \n440 Learning and Cybernetics (ICMLC), pages 1\u20139, July 2019.   \n441 [32] J. Miao, T. Ishikawa, B. Johnson, E. H. Anderson, B. Lai, and K. O. Hodgson. High resolution   \n442 3D X-ray diffraction microscopy. Physical review letters, 89(8):088303, 2002.   \n443 [33] R. P. Millane. Phase retrieval in crystallography and optics. JOSA A, 7(3):394\u2013411, 1990.   \n444 [34] P. Netrapalli, P. Jain, and S. Sanghavi. Phase retrieval using alternating minimization. In Proc.   \n445 27th Advances in Neural Information Processing Systems (NeurIPS), pages 2796\u20132804, 2013.   \n446 [35] A. Prasad, A. S. Suggala, S. Balakrishnan, and P. Ravikumar. Robust estimation via robust   \n447 gradient estimation. Journal of the Royal Statistical Society. Series B. Statistical Methodology,   \n448 82(3):601\u2013627, 2020.   \n449 [36] W. H. Robert. Phase problem in crystallography. JOSA a, 10(5):1046\u20131055, 1993.   \n450 [37] Y. Shechtman, Y. C. Eldar, O. Cohen, H. N. Chapman, J. Miao, and M. Segev. Phase retrieval   \n451 with application to optical imaging: a contemporary overview. IEEE signal processing magazine,   \n452 32(3):87\u2013109, 2015.   \n453 [38] M. Soltanolkotabi. Structured signal recovery from quadratic measurements: Breaking sample   \n454 complexity barriers via nonconvex optimization. IEEE Trans. Inf. Theory, 65(4):2374\u20132400,   \n455 2019.   \n456 [39] J. Sun, Q. Qu, and J. Wright. Complete dictionary recovery over the sphere i: Overview and the   \n457 geometric picture. IEEE Trans. Inf. Theor., 63(2):853\u2013884, 2 2017.   \n458 [40] J. Sun, Q. Qu, and J. Wright. A geometric analysis of phase retrieval. Found. Comput. Math.,   \n459 18(5):1131\u20131198, 2018.   \n460 [41] L. Trevisan. Lecture notes on graph partitioning, expanders and spectral methods. University of   \n461 California, Berkeley, https://people. eecs. berkeley. edu/luca/books/expanders-2016. pdf, 2017.   \n462 [42] J. Tukey. Mathematics and picturing of data. In Proceedings of ICM, volume 6, pages 523\u2013531,   \n463 1975.   \n464 [43] V. Voroninski. PhaseLift: A Novel Methodology for Phase Retrieval. PhD thesis, UC Berkeley,   \n465 2013.   \n466 [44] J. Wright and Y. Ma. High-dimensional data analysis with low-dimensional models: Principles,   \n467 computation, and applications. Cambridge University Press, 2022.   \n468 [45] H. Zhang, Y. Chi, and Y. Liang. Provable non-convex phase retrieval with outliers: Median   \n469 truncated wirtinger flow. In International conference on machine learning, pages 1022\u20131031.   \n470 PMLR, 2016. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "471 A Omitted Proofs in Section 4 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "472 Lemma A.1. [Lemma 4.2, Formal]. For any $\\delta_{0}>0$ , there exists constants $\\epsilon_{0},c>0$ such that   \n473 when $n>c\\cdot d\\log d$ and we are given a set of n \u03f5-corrupted samples, where $0\\leq\\epsilon\\leq\\epsilon_{0}$ , then with   \n474 probability at least 0.98, it holds ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\forall w\\in\\Delta_{n,2\\epsilon},\\left\\|Y_{G,w}-(I+2x x^{\\top})\\right\\|_{2}\\leq\\delta_{0}\\;.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "475 Proof of Lemma A.1. We recall the definition of $\\begin{array}{r}{Y_{G,w}\\,=\\,\\sum_{i\\in G}w_{i}y_{i}a_{i}a_{i}^{\\top}}\\end{array}$ . Let $\\ell=\\epsilon\\cdot n$ and let   \n476 $\\{(a_{n+i},y_{n+i})\\}_{i=1}^{\\ell}$ be the set of samples that were removed by the $\\epsilon$ -corruption adversary. Let   \n477 $G^{\\prime}=G\\cup\\{n+1,\\ldots,n+\\ell\\}$ , $n^{\\prime}=n+\\ell,$ and $\\epsilon^{\\prime}=\\epsilon/(1+\\epsilon)$ . Note that without loss of generality,   \n478 we can assume that $|G|=(1-\\epsilon)n$ and $|G^{\\prime}|=(1-\\epsilon^{j})\\dot{n}^{\\prime}=\\dot{n}$ . ", "page_idx": 12}, {"type": "text", "text": "479 We define a mapping $\\sigma:\\Delta_{n,2\\epsilon}\\rightarrow\\Delta_{n^{\\prime},3\\epsilon^{\\prime}}$ such that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\sigma(w)_{i}=\\left\\{w_{i}\\quad\\quad i\\in[n]\\right.}\\\\ {0\\quad\\quad\\mathrm{~otherwise}}\\end{array}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "480 In other words, all the weights are the same for the samples with index in the set $[n]$ , and are equal to   \n481 0 for the samples removed by the adversary. We can verify that $\\sigma(w)\\in\\Delta_{n^{\\prime},3\\epsilon^{\\prime}}$ for all $w\\in\\Delta_{n,2\\epsilon}$   \n482 since $\\sigma(w)_{i}\\,\\stackrel{.}{\\leq}w_{i}\\,\\leq\\,1/(1\\stackrel{.}{-}2\\epsilon)n\\,=\\,1/(\\mathrm{i}-3\\epsilon^{\\prime})n^{\\prime}$ for all $i\\,\\in\\,[n^{\\prime}]$ , and $\\|\\dot{\\boldsymbol{\\sigma}}(\\boldsymbol{w})\\|_{1}\\,=\\,\\|\\boldsymbol{w}\\|_{1}\\,=\\,1$ .   \n483 Furthermore, we have $Y_{G,w}=Y_{G^{\\prime},\\sigma(w)}$ for all $w\\in\\Delta_{n,2\\epsilon}$ . We denote with $w^{\\ast}\\in\\Delta_{n^{\\prime},3\\epsilon^{\\prime}}$ the desired   \n484 uniform weighting of the samples with index in G\u2032, i.e., wi\u2217 = (1\u2212\u03f51\u2032)n\u2032 1i\u2208G\u2032. ", "page_idx": 12}, {"type": "text", "text": "485 It suffices to show both $\\left\\|Y_{G^{\\prime},w^{*}}-(I+2x x^{\\top})\\right\\|_{2}\\leq\\delta_{0}/2$ and $\\left\\|Y_{G^{\\prime},\\sigma(w)-w^{*}}\\right\\|_{2}\\leq\\delta_{0}/2$ . ", "page_idx": 12}, {"type": "text", "text": "486 By triangle inequality, for any $w\\in\\Delta_{n,2\\epsilon}$ , it holds ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|Y_{G^{\\prime},\\sigma(w)}-(I+2x x^{\\top})\\right\\|_{2}\\leq\\left\\|Y_{G^{\\prime},w^{*}}-(I+2x x^{\\top})\\right\\|_{2}+\\left\\|Y_{G^{\\prime},\\sigma(w)-w^{*}}\\right\\|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "487 Thus, it suffices to show both $\\left\\|Y_{G^{\\prime},w^{*}}-(I+2x x^{\\top})\\right\\|_{2}\\leq\\delta_{0}/2$ and $\\left\\|Y_{G^{\\prime},\\sigma(w)-w^{*}}\\right\\|_{2}\\leq\\delta_{0}/2.$   \n488 We upper bound the first term. By using the definition of $w^{*}$ , note that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\Vert Y_{G^{\\prime},\\sigma(w)}-(I+2x x^{\\top})\\right\\Vert_{2}=\\left\\Vert\\displaystyle\\sum_{i\\in G^{\\prime}}w_{i}^{*}y_{i}a_{i}a_{i}^{\\top}-(I+2x x^{\\top})\\right\\Vert_{2}}\\\\ {=\\left\\Vert\\displaystyle\\sum_{i\\in G^{\\prime}}\\frac{1}{|G^{\\prime}|}y_{i}a_{i}a_{i}^{\\top}-(I+2x x^{\\top})\\right\\Vert_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "489 Since $\\mathbb{E}(y_{i}a_{i}a_{i}^{\\top})=I+2x x^{\\top}$ for any $i\\in G^{\\prime}$ , we can use a concentration inequality to upper bound   \n490 this term. By Lemma A.2, as long as $\\dot{n}\\geq c_{1}(\\delta_{0}/2)\\cdot d\\log d_{!}$ , with probability at least 0.995, we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\left\\|Y_{G^{\\prime},w^{*}}-(I+2x x^{\\top})\\right\\|_{2}\\leq\\delta_{0}/2\\;\\;.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "491 It remains to show a high-probability upper bound to the second term $\\left\\|Y_{G^{\\prime},w^{*}-\\sigma(w)}\\right\\|_{2}\\,\\leq\\,\\delta_{0}/2$ .   \n492 The first observation is that the weighting $w^{*}$ and $\\sigma(w)$ for any $w\\in\\Delta_{n,2\\epsilon}$ cannot too different. In   \n493 particular, we can show the following upper bound: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\sum_{i\\in G^{\\prime}}|w_{i}^{*}-\\sigma(w)_{i}|\\leq\\sum_{i=1}^{n^{\\prime}}|w_{i}^{*}-\\sigma(w)_{i}|\\leq\\operatorname*{sup}_{w,w^{\\prime}\\in\\Delta_{n^{\\prime},3\\epsilon^{\\prime}}}\\sum_{i=1}^{n^{\\prime}}|w_{i}-w_{i}^{\\prime}|\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "494 We observe that $\\Delta_{n^{\\prime},3\\epsilon^{\\prime}}$ can be seen as the convex combination of all possible uniform weighting   \n495 over subsets of $n^{\\prime}(1-3\\epsilon^{\\prime})$ samples. Thus, the maximum distance will be between two points of the   \n496 convex hull, and we can upper bound (11) as: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\sum_{i\\in G^{\\prime}}|w_{i}^{*}-\\sigma(w)_{i}|\\leq\\operatorname*{sup}_{w,w^{\\prime}\\in\\Delta_{n^{\\prime},3\\epsilon^{\\prime}}}\\sum_{i=1}^{n^{\\prime}}|w_{i}-w_{i}^{\\prime}|\\leq\\frac{6\\epsilon^{\\prime}n}{n^{\\prime}(1-3\\epsilon^{\\prime})}\\leq6\\epsilon\\,\\,.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "497 For a fixed unit vector $z\\in\\mathbb{S}^{d-1}$ with $z=p x+q u$ where $u\\in\\mathbb{S}^{d-1}$ and $\\langle u,x\\rangle=0$ , we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{max}_{w\\in\\Delta_{n,2}}\\left\\vert z^{\\top}Y_{G^{\\prime},w^{*}-\\sigma(w)}z\\right\\vert=\\operatorname*{max}_{w}\\left\\vert\\sum_{\\{\\epsilon\\in G^{\\prime}}}(w_{i}^{*}-\\sigma(w)_{i})\\left\\langle a_{i},x\\right\\rangle^{2}\\left\\langle a_{i},z\\right\\rangle^{2}\\right\\vert}&{}\\\\ {\\displaystyle=\\operatorname*{max}_{w}\\left\\vert\\sum_{i\\in G^{\\prime}}(w_{i}^{*}-\\sigma(w)_{i})\\left\\langle a_{i},x\\right\\rangle^{2}\\left(p\\left\\langle a_{i},x\\right\\rangle+q\\left\\langle a_{i},w\\right\\rangle\\right)^{2}}\\\\ &{\\leq2\\operatorname*{max}_{w}\\displaystyle\\left\\vert\\sum_{i\\in G^{\\prime}}(w_{i}^{*}-\\sigma(w)_{i})(\\left\\langle a_{i},x\\right\\rangle^{4}+\\left\\langle a_{i},x\\right\\rangle^{2}\\left\\langle a_{i},u\\right\\rangle^{2})\\right\\vert}\\\\ &{\\leq2\\operatorname*{max}_{w}\\displaystyle\\sum_{i\\in G^{\\prime}}\\vert w_{i}^{*}-\\sigma(w)_{i}\\vert\\left\\langle a_{i},x\\right\\rangle^{4}}\\\\ &{\\quad+\\left.2\\operatorname*{max}_{w}\\displaystyle\\sum_{i\\in G^{\\prime}}\\vert w_{i}^{*}-\\sigma(w)_{i}\\vert\\left\\langle a_{i},x\\right\\rangle^{2}\\left\\langle a_{i},u\\right\\rangle^{2}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "498 For ease of notation, let $\\beta_{i}\\doteq|w_{i}^{*}-\\sigma(w)_{i}|$ . Observe that $\\begin{array}{r}{0\\leq\\beta_{i}\\leq\\frac{1}{(1-2\\epsilon)n}}\\end{array}$ for all $i$ , and $\\sum_{i}\\beta_{i}\\le6\\epsilon$   \n499 due to (11). We have: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{w\\in\\Delta_{n,2\\epsilon}}{\\operatorname*{max}}\\left|z^{\\top}Y_{G^{\\prime},w^{*}-\\sigma(w)}z\\right|\\leq2\\,\\underset{\\substack{\\beta:\\sum_{i\\in G^{\\prime}}\\beta_{i}\\leq6\\epsilon\\operatorname*{max}0\\leq\\beta_{i}\\leq\\frac{1}{(1-2\\epsilon)n}}}{\\operatorname*{max}}\\sum_{i\\in G^{\\prime}}\\beta_{i}\\left\\langle a_{i},x\\right\\rangle^{4}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\underset{\\substack{\\beta:\\sum_{i\\in G^{\\prime}}\\beta_{i}\\leq6\\epsilon\\operatorname*{max}0\\leq\\beta_{i}\\leq\\frac{1}{(1-2\\epsilon)n}}}{\\operatorname*{max}}\\sum_{i\\in G^{\\prime}}\\beta_{i}\\left\\langle a_{i},x\\right\\rangle^{2}\\left\\langle a_{i},u\\right\\rangle^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\cfrac{2}{(1-2\\epsilon)n}\\underset{L\\subseteq G^{\\prime},|L|=6\\epsilon n}{\\operatorname*{max}}\\sum_{i\\in L}\\left\\langle a_{i},x\\right\\rangle^{4}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\cfrac{2}{(1-2\\epsilon)n}\\ L\\underset{L\\subseteq G^{\\prime},|L|=6\\epsilon n}{\\operatorname*{max}}\\underset{i\\in L}{\\sum}\\left\\langle a_{i},x\\right\\rangle^{2}\\left\\langle a_{i},u\\right\\rangle^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "500 Inequality (13) follows by assigning the maximum possible $\\beta_{i}$ to the largest entries of the sum until   \n501 we hit the budget $6\\epsilon$ due to (11).   \n502 We bound $\\begin{array}{r}{\\operatorname*{max}_{L}\\sum_{i\\in L}\\left\\langle a_{i},x\\right\\rangle^{4}}\\end{array}$ first. Let $X_{i}\\;=\\;\\langle a_{i},x\\rangle\\;\\sim\\mathcal{N}(0,1)$ for $i\\ \\in\\ G^{\\prime}$ , and define the   \n503 threshold function $h_{r}(z)=\\left\\{0,\\quad z\\leq r\\right.$ 0, z \u2264rfor r = C2 \u00b7ln2(1/\u03f5) with constant C > 0 to be determined.   \n504 Note that $z\\le r+h_{r}(z)$ for all $z>0$ . Therefore, ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{max}_{\\substack{L\\subseteq G^{\\prime},|L|=6\\epsilon n}}\\frac{1}{n}\\sum_{i\\in L}X_{i}^{4}\\leq\\displaystyle\\operatorname*{max}_{L}\\frac{1}{n}\\sum_{i\\in L}r+\\operatorname*{max}_{L}\\frac{1}{n}\\sum_{i\\in L}h_{r}(X_{i}^{4})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq6\\epsilon r+\\displaystyle\\frac{1}{n}\\sum_{i\\in G^{\\prime}}h_{r}(X_{i}^{4}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "505 Then, we consider to bound $\\begin{array}{r}{\\exp\\left(\\sum_{i\\in G^{\\prime}}c\\cdot h_{r}(X_{i}^{4})\\right)}\\end{array}$ for some $c>0$ to be determined. For any   \n506 $i\\in G^{\\prime}$ and $z\\geq1$ , with $C=6$ , for all $\\epsilon>0$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{Pr}\\left[\\exp\\left(c\\cdot h_{r}(X_{i}^{4})\\right)\\geq z\\right]\\leq\\mathbf{Pr}\\left[h_{r}(X_{i}^{4})\\geq0\\right]=\\mathbf{Pr}\\left[X_{i}\\geq r^{1/4}\\right]\\leq\\exp(-\\sqrt{r}/2)}\\\\ &{\\phantom{=}\\leq\\exp(\\ln\\epsilon\\cdot C/2)}\\\\ &{\\phantom{=}\\leq\\exp(\\ln\\epsilon\\cdot C/2)}\\\\ &{\\phantom{=}\\leq\\epsilon^{3}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "507 At the same time, with $c<1/200$ , for all $z\\geq1$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbf{Pr}\\left[\\exp\\left(c\\cdot h_{r}(X_{i}^{4})\\right)\\geq z\\right]\\leq\\mathbf{Pr}\\left[\\exp(c\\cdot X_{i}^{4})\\geq z\\right]\\leq\\mathbf{Pr}\\left[X_{i}^{4}\\geq\\displaystyle\\frac{\\ln z}{c}\\right]}\\\\ {\\qquad\\qquad\\qquad\\leq\\exp\\left(-\\sqrt{\\displaystyle\\frac{\\ln z}{c}}/2\\right)}\\\\ {\\qquad\\qquad\\qquad\\leq z^{-3}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "508 Therefore, $\\mathbf{Pr}\\left[\\exp\\left(c\\cdot h_{r}(X_{i}^{4})\\right)\\geq z\\right]\\leq\\operatorname*{min}\\{z^{-3},\\epsilon^{3}\\}$ , and for all $\\epsilon<1/2$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbf{E}\\left[\\exp\\left(c\\cdot h_{r}(X_{i}^{4})\\right)\\right]=\\int_{0}^{\\infty}\\mathbf{Pr}\\left[\\exp\\left(c\\cdot h_{r}(X_{i}^{4})\\right)\\geq z\\right]\\ensuremath{\\mathrm{d}}z}\\\\ {\\displaystyle\\leq\\int_{0}^{1}1\\ensuremath{\\mathrm{d}}z+\\int_{1}^{1/\\epsilon}\\epsilon^{3}\\ensuremath{\\mathrm{d}}z+\\int_{1/\\epsilon}^{\\infty}z^{-3}\\ensuremath{\\mathrm{d}}z}\\\\ {\\displaystyle=1+\\epsilon^{2}-\\epsilon^{3}+\\frac{1}{2}\\epsilon^{2}}\\\\ {\\displaystyle\\leq1+\\epsilon^{2}}\\\\ {\\displaystyle\\leq\\exp(\\epsilon^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "509 Since $\\{X_{i}\\}_{i\\in G^{\\prime}}$ are independent, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{E}\\left[\\exp\\left(\\sum_{i\\in G^{\\prime}}c\\cdot h_{r}(X_{i}^{4})\\right)\\right]=\\mathbf{E}\\left[\\prod_{i\\in G^{\\prime}}\\exp\\left(c\\cdot h_{r}(X_{i}^{4})\\right)\\right]\\leq\\exp(\\epsilon^{2}n).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "510 Finally, by Markov\u2019s inequality, for any constant $\\delta_{1}>0$ , as long as $\\epsilon\\leq\\sqrt{\\delta_{1}c}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbf{Pr}\\left[\\sum_{i\\in G^{\\prime}}h_{r}(X_{i}^{4})\\geq2\\delta_{1}n\\right]=\\mathbf{Pr}\\left[\\exp\\left(\\sum_{i\\in G^{\\prime}}c\\cdot h_{r}(X_{i}^{4})\\right)\\geq\\exp(2\\delta_{1}c n)\\right]}}\\\\ &{}&{\\leq\\exp(\\epsilon^{2}n-2\\delta_{1}c n)\\leq\\exp(-\\delta_{1}c n).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "511 As a result, with sufficiently large $n\\geq c_{2}(\\delta_{1})\\cdot d\\log d$ and sufficiently small $\\epsilon$ such that $6\\epsilon r\\le\\delta_{1}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{Pr}\\left[\\epsilon r+\\frac{1}{n}\\sum_{i\\in G^{\\prime}}h_{r}(X_{i}^{4})\\geq3\\delta_{1}\\right]\\leq\\exp(-\\delta_{1}c n)\\leq0.995\\cdot9^{-d}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "512 Also, $\\begin{array}{r}{\\operatorname*{max}_{L}\\sum_{i\\in L}\\left\\langle a_{i},x\\right\\rangle^{2}\\left\\langle a_{i},u\\right\\rangle^{2}}\\end{array}$ has a similar tail bound and therefore can be bounded in the same   \n513 way. We can then bound the operator norm via an epsilon-net argument. Set $\\delta_{1}=\\delta_{0}/24$ . By an   \n514 $1/4$ -net on $S^{d-1}$ with $|{\\mathcal{N}}|\\leq9^{d}$ , we have that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{Pr}\\left[\\operatorname*{max}_{z\\in\\mathbb{S}^{d-1}}\\left|z^{\\top}Y_{G^{\\prime},w^{*}-\\sigma(w)}z\\right|\\geq\\delta_{0}/2\\right]\\leq9^{d}\\cdot0.99\\cdot9^{-d}\\leq0.99.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "515 By combining the above inequality with Equation (10), we can conclude that, for any $\\delta_{0}>0$ , there   \n516 exists $\\epsilon_{0}>0$ , such that when $n\\geq\\operatorname*{max}\\{c_{1}(\\delta_{0}),c_{2}(\\delta_{0})\\}\\cdot d\\log d$ and $0\\leq\\epsilon\\leq\\epsilon_{0}$ , with probability at   \n517 least 0.98, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\forall w\\in\\Delta_{n,2\\epsilon},\\left\\|Y_{G,w}-(I+2x x^{\\top})\\right\\|_{2}=\\left\\|Y_{G^{\\prime},\\sigma(w)}-(I+2x x^{\\top})\\right\\|_{2}\\leq\\delta_{0}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "518 Therefore, with probability at least 0.98, we have  YG,w \u2212(I + 2xx\u22a4)  2 \u2264\u03b40 for all w \u2208\u2206n,2\u03f5. ", "page_idx": 14}, {"type": "text", "text": "520 A.1 Concentration Inequalities ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "521 For the undisturbed samples, we have the following concentration result. ", "page_idx": 14}, {"type": "text", "text": "522 Lemma A.2 ([3] Section A.4.2). Let $x\\,\\in\\,\\mathbb{R}^{d}$ . For any $\\delta>0$ , there exists a constant $C(\\delta)>0$   \n523 such that when $n>C\\cdot d\\log d$ and we are given a set of $n$ samples $\\{(a_{i},y_{i})\\}_{i=1}^{n}$ with $a_{i}\\sim\\mathcal{N}(0,I)$   \n524 independently and $y_{i}=\\left\\langle a_{i},x\\right\\rangle^{2}$ for all $i\\in[n]$ , then with probability at least 0.99, it holds ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}y_{i}a_{i}a_{i}^{\\top}-\\left(I+2x x^{\\top}\\right)\\right\\|_{2}\\leq\\delta.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "525 B Omitted Proofs in Section 5 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "526 Lemma 5.1. Let $\\mathcal{D}_{z}$ be the distribution of gradients at $z$ as defined in Equation (7). For any $z\\in\\mathbb{R}^{d}$   \n527 with $\\left\\|z-x\\right\\|_{2}\\leq1$ , the covariance matrix $\\Sigma_{z}$ of $\\mathcal{D}_{z}$ satisfies $\\Sigma_{z}\\preceq O(\\left\\|z-x\\right\\|_{2}^{2})I.$ . ", "page_idx": 15}, {"type": "text", "text": "528 Proof of Lemma 5.1. Recall that $g\\sim\\mathcal{D}_{z}$ is distributed as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{g=\\displaystyle\\frac{\\partial}{\\partial z}\\left[\\left(\\langle a,z\\rangle^{2}-\\langle a,x\\rangle^{2}\\right)^{2}\\right]}}\\\\ {{\\quad=-4\\left(\\langle a,z\\rangle^{2}-\\langle a,x\\rangle^{2}\\right)\\langle a,z\\rangle a\\quad\\mathrm{where~}\\,a\\sim\\mathcal{N}(0,1)\\;.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "529 Because ${\\bf E}_{g\\sim\\mathcal{D}_{z}}\\left[g\\right]=\\mu_{z}$ , the spectral norm of $\\Sigma_{z}$ can be upper bounded as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\|\\Sigma_{z}\\right\\|_{2}=\\left\\|\\underset{g\\sim\\mathcal{D}_{z}}{\\mathbf{E}}\\left[g g^{\\top}\\right]-\\mu_{z}\\mu_{z}^{\\top}\\right\\|_{2}\\leq\\left\\|\\underset{g\\sim\\mathcal{D}_{z}}{\\mathbf{E}}\\left[g g^{\\top}\\right]\\right\\|_{2}\\ .\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "530 Consequently, it suffices to upper bound $\\left|\\left|\\mathbf{E}_{g\\sim\\mathcal{D}_{z}}\\left[g g^{\\top}\\right]\\right|\\right|_{2}$ . Let $h=z-x$ . ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\bigg\\|\\underset{y\\sim N_{z}}{\\mathbf{E}}\\left[y g^{\\top}\\right]\\bigg\\|_{2}=\\underset{1\\parallel=1}{\\operatorname*{max}}\\ v^{\\top}\\underset{y\\sim N_{z}}{\\mathbf{E}}\\left[g\\vartheta^{\\top}\\right]v=\\underset{1\\parallel=1}{\\operatorname*{max}}\\ \\mathbf{E}\\left[\\langle g,v\\rangle^{2}\\right]}\\\\ {=16\\underset{1\\parallel=1}{\\operatorname*{max}}\\ \\underset{u\\sim N(0,1)}{\\operatorname*{max}}\\left[\\langle(\\langle a,z\\rangle^{2}-\\langle a,x\\rangle^{2})^{2}\\langle a,z\\rangle^{2}\\langle a,v\\rangle^{2}\\right]}\\\\ {=16\\underset{1\\parallel=1}{\\operatorname*{max}}\\ \\underset{u\\sim N(0,1)}{\\operatorname*{max}}\\left[\\langle a,h\\rangle^{2}\\left\\langle a,2x+h\\rangle^{2}\\left\\langle a,x+h\\right\\rangle^{2}\\langle a,v\\rangle^{2}\\right]}\\\\ {\\leq16\\underset{1\\parallel=1}{\\operatorname*{max}}\\ \\bigg(\\mathbf{E}\\left[\\langle a,h\\rangle^{8}\\right]\\ \\underset{u}{\\leq}\\left[\\langle a,2x+h\\rangle^{8}\\right]\\underset{a}{\\leq}\\left[\\langle a,x+h\\rangle^{8}\\right]\\underset{a}{\\leq}\\left[\\langle a,v\\rangle^{8}\\right]\\bigg)^{1/4}}\\\\ {=16\\underset{1\\parallel=1}{\\operatorname*{max}}\\ \\left[105^{4}\\left\\Vert h\\right\\Vert_{2}^{8}\\left\\Vert2x+h\\right\\Vert_{2}^{8}\\left\\Vert1z+h\\right\\Vert_{2}^{8}\\left\\Vert v\\right\\Vert_{2}^{8}\\right]^{1/4}}\\\\ {=(16\\cdot105)\\left\\Vert h\\right\\Vert_{2}^{2}\\left\\Vert2x+h\\right\\Vert_{2}^{2}\\left\\Vert x+h\\right\\Vert_{2}^{2}=O(\\Vert h\\Vert_{2}^{2})\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "531 The first inequality is due to Cauchy-Schwarz inequality. The last step uses the fact that $\\|{\\boldsymbol{x}}\\|_{2}=1$   \n532 and $\\|h\\|_{2}\\leq1$ . \u53e3   \n533 Lemma 5.3. Suppose in iteration t of Algorithm 2, the current solution $z_{t}$ satisfies $\\|z_{t}-x\\|_{2}\\leq1/8,$ ,   \n534 and the estimated gradient $\\widehat{\\mu}_{z_{t}}\\in\\mathbb{R}^{d}$ satisfies $\\left\\|\\widehat{\\boldsymbol{\\mu}}_{z_{t}}-\\boldsymbol{\\mu}_{z_{t}}\\right\\|_{2}\\leq c\\left\\|\\boldsymbol{z}_{t}-\\boldsymbol{x}\\right\\|_{2}$ for $c=4$ . Then, we have   \n535 $\\left\\|\\boldsymbol{z}_{t+1}-\\boldsymbol{x}\\right\\|_{2}^{2}\\leq0.99\\left\\|\\boldsymbol{z}_{t}-\\boldsymbol{x}\\right\\|_{2}^{2},$ . ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "536 Proof of Lemma 5.3. Recall that $g\\sim\\mathcal{D}_{z}$ is distributed as ", "page_idx": 15}, {"type": "equation", "text": "$$\ng=\\frac{\\partial}{\\partial z}\\left[\\left(\\langle a,z\\rangle^{2}-\\langle a,x\\rangle^{2}\\right)^{2}\\right]\\;\\;\\mathrm{where}\\;\\;a\\sim{\\mathcal N}(0,I)\\;\\;.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "537 We can compute the mean of $\\mathcal{D}_{z}$ using moments of Gaussian: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu_{z}=\\underset{g\\sim\\mathcal{D}_{z}}{\\mathbb{E}}g=\\left(12\\left\\|z\\right\\|_{2}^{2}-4\\left\\|x\\right\\|_{2}^{2}\\right)z-8\\langle x,z\\rangle x\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "538 Consider one step of gradient descent in Algorithm 2: $z_{t+1}=z_{t}-\\eta\\widehat{\\mu}_{z_{t}}$ , where $\\widehat{\\mu}_{z_{t}}$ is an approximate   \n539 gradient. We have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Vert z_{t+1}-x\\Vert_{2}^{2}=\\Vert z_{t}-\\eta\\widehat{\\mu}_{z_{t}}-x\\Vert_{2}^{2}=\\Vert z_{t}-x\\Vert_{2}^{2}-2\\eta\\langle\\widehat{\\mu}_{z_{t}},z_{t}-x\\rangle+\\eta^{2}\\langle\\widehat{\\mu}_{z_{t}},\\widehat{\\mu}_{z_{t}}\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "540 To prove convergence, we need to lower bound $\\langle\\widehat{\\mu}_{z_{t}},z_{t}-x\\rangle$ and upper bound $\\langle\\widehat{\\mu}_{z_{t}},\\widehat{\\mu}_{z_{t}}\\rangle$ . ", "page_idx": 15}, {"type": "text", "text": "541 We write $z=z_{t}$ and $h=z-x$ to simplify notation. We can substitute $z=x+h$ in Equation (14): ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mu_{z}=\\left(16\\langle x,h\\rangle+12\\left\\|h\\right\\|_{2}^{2}\\right)x+\\left(8\\left\\|x\\right\\|_{2}^{2}+24\\langle x,h\\rangle+12\\left\\|h\\right\\|_{2}^{2}\\right)h\\;.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "542 Recall the assumptions of this lemma: $\\|{\\boldsymbol x}\\|_{2}=1$ , $\\|h\\|_{2}\\leq1/8$ , and $\\left\\|{\\widehat{\\mu}}_{z}-\\mu_{z}\\right\\|_{2}\\leq c\\left\\|h\\right\\|_{2}$ .   \n543 First we lower bound $\\langle\\widehat{\\mu}_{z},h\\rangle$ . ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle\\widehat{\\mu}_{z},h\\right\\rangle=\\left\\langle\\mu_{z},h\\right\\rangle+\\left\\langle\\widehat{\\mu}_{z}-\\mu_{z},h\\right\\rangle}\\\\ &{\\qquad\\quad=16\\langle x,h\\rangle^{2}+36\\langle x,h\\rangle\\left\\|h\\right\\|_{2}^{2}+8\\left\\|x\\right\\|_{2}^{2}\\left\\|h\\right\\|_{2}^{2}+12\\left\\|h\\right\\|_{2}^{4}+\\left\\langle\\widehat{\\mu}_{z}-\\mu_{z},h\\right\\rangle}\\\\ &{\\qquad\\quad\\geq-\\frac{81}{4}\\left\\|h\\right\\|_{2}^{4}+8\\left\\|x\\right\\|_{2}^{2}\\left\\|h\\right\\|_{2}^{2}+12\\left\\|h\\right\\|_{2}^{4}-\\left\\langle\\widehat{\\mu}_{z}-\\mu_{z},h\\right\\rangle}\\\\ &{\\qquad\\quad\\geq\\left(8-\\frac{33}{256}-c\\right)\\left\\|h\\right\\|_{2}^{2}}\\\\ &{\\qquad\\quad\\geq\\left(7.5-c\\right)\\left\\|h\\right\\|_{2}^{2}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "544 The first inequality uses the fact that $16\\langle x,h\\rangle^{2}+36\\langle x,h\\rangle\\left\\|h\\right\\|_{2}^{2}$ is a second-order polynomial of   \n545 $\\langle x,h\\rangle$ , which has minimum value $-\\frac{81}{4}\\left\\|h\\right\\|_{2}^{4}$ for all $\\langle x,h\\rangle\\in\\mathbb{R}$ . ", "page_idx": 16}, {"type": "text", "text": "546 Next we upper bound $\\langle\\widehat{\\mu}_{z},\\widehat{\\mu}_{z}\\rangle$ using the triangle inequality. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\widehat{\\mu}_{z}\\right\\|_{2}\\leq\\left\\|\\mu_{z}\\right\\|_{2}+\\left\\|\\widehat{\\mu}_{z}-\\mu_{z}\\right\\|_{2}}\\\\ &{\\qquad\\leq\\left(16\\langle x,h\\rangle+12\\left\\|h\\right\\|_{2}^{2}\\right)\\left\\|x\\right\\|_{2}+\\left(8\\left\\|x\\right\\|_{2}^{2}+24\\langle x,h\\rangle+12\\left\\|h\\right\\|_{2}^{2}\\right)\\left\\|h\\right\\|_{2}+c\\left\\|h\\right\\|_{2}}\\\\ &{\\qquad\\leq\\left(16+\\frac{12}{8}+8+\\frac{24}{8}+\\frac{12}{64}+c\\right)\\left\\|h\\right\\|_{2}}\\\\ &{\\qquad\\leq(29+c)\\left\\|h\\right\\|_{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "547 Putting everything together, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|z_{t+1}-x\\right\\|_{2}^{2}=\\big\\|z_{t}-x\\big\\|_{2}^{2}-2\\eta\\langle\\widehat{\\mu}_{z_{t}},z_{t}-x\\rangle+\\eta^{2}\\langle\\widehat{\\mu}_{z_{t}},\\widehat{\\mu}_{z_{t}}\\rangle}\\\\ &{\\qquad\\qquad\\leq\\left[1-2(7.5-c)\\eta+(29+c)^{2}\\eta^{2}\\right]\\big\\|z_{t}-x\\big\\|_{2}^{2}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "548 Choosing $c=4$ and $\\eta=1/300$ gives that $\\left\\|z_{t+1}-x\\right\\|_{2}^{2}\\leq0.99\\left\\|z_{t}-x\\right\\|_{2}^{2}.$ ", "page_idx": 16}, {"type": "text", "text": "549 C Counter-examples ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "550 Prior robust phase retrieval algorithms [24, 45] focus on the setting where the observations $y_{i}$ \u2019s are sub  \n551 ject to adversarial perturbation while the measuring vectors $a_{i}$ \u2019s are independently sampled from the   \n552 Gaussian distribution. The Median Truncated Wirtinger Flow Algorithm [45] first initialize $z^{(0)}$ by the   \n553 spectral method, calculating $z^{(0)}$ as the top eigenvector of $\\begin{array}{r}{Y:=\\frac{1}{m}\\sum_{i=1}^{m}y_{i}a_{i}a_{i}^{\\top}\\mathbb{1}_{|y_{i}|\\leq\\alpha^{2}\\operatorname{med}(\\{y_{i}\\}_{i=1}^{m})}}\\end{array}$   \n554 using a truncated set of samples, where the threshold is determined by $\\mathrm{med}(\\{y_{i}\\}_{i=1}^{m})$ , the median   \n555 over all $y_{i}$ \u2019s. As long as the fraction of of outliers is not too large and the sample complexity is large   \n556 enough, the initialization is guaranteed to be within a small neighborhood of the ground truth.   \n557 In this section, we present a counter-example where robust phase retrieval algorithms [24, 45] can be   \n558 insufficient when directly applied to the $\\epsilon$ -corruption phase retrieval problem.   \n559 Let $x\\in\\mathbb{S}^{d-1}$ be the ground truth unit vector. Here we construct an $\\epsilon$ -corruption adversary that can   \n560 manipulate the top eigenvector of the empirical covariance matrix $\\begin{array}{r}{Y=\\sum_{i=1}^{n^{\\star}}y_{i}a_{i}a_{i}^{\\top}}\\end{array}$ , even when $y_{i}$   \n561 are accurately calculated as $y_{i}=(a_{i}^{\\top}x)^{2}$ .   \n562 Let $u\\in\\mathbb{S}^{d-1}$ be a unit vector such that $x^{\\top}u=0$ . Suppose the adversary changes $1\\%$ of the $a_{i}$ \u2019s   \n563 to $a_{i}=\\sqrt{d-1/25}\\cdot u+1/5x$ , and suppose all the $y_{i}$ \u2019s are accurate. In particular, the length of   \n564 the corrupted $a_{i}$ \u2019s is comparable to Gaussian vectors, and the corresponding $y_{i}=(a_{i}^{\\top}x)^{2}=1/25$ .   \n565 Consequently, the median-truncated initialization in [45] will not be abl\u221ae to fliter out such $y_{i}$ . However,   \n566 the top eigenvector of $\\begin{array}{r}{\\mathbf{E}\\left[Y\\right]=\\mathbf{E}\\left[\\sum_{i=1}^{n}y_{i}a_{i}a_{i}^{\\top}\\right]=O(d)u u^{\\top}+O(\\sqrt{d})(u x^{\\top}+x u^{\\top})+O(1)(I+\\sqrt{d}).}\\end{array}$   \n567 $2x x^{\\top}$ ) will be manipulated to $u$ , which is far from the ground truth $x$ . ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "568 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "569 1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Justification: We provide a formal discussion of our theoretical claim in the first section of the paper, and include a formal statement of our resul in Section 3. ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We point out to all the assumptions used in our paper. There are a discussion of the limitations in the conclusion. Our algorithm can only handle a corruption $\\epsilon<\\epsilon_{0}$ for some universal constant $\\epsilon_{0}$ . Additionally, it is possible that the dependency on $\\Delta$ for the sample complexity can be removed. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "619 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "620 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n621 a complete (and correct) proof?   \n22 Answer: [Yes]   \n623 Justification: We formally describe our problem setting and assumption used. All the proofs   \n624 are in the paper, either in the main content pages or in the appendix.   \n625 Guidelines:   \n626 \u2022 The answer NA means that the paper does not include theoretical results.   \n627 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n628 referenced.   \n629 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n630 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n631 they appear in the supplemental material, the authors are encouraged to provide a short   \n32 proof sketch to provide intuition.   \n633 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n634 by formal proofs provided in appendix or supplemental material.   \n635 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n636 4. Experimental Result Reproducibility   \n637 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n638 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n639 of the paper (regardless of whether the code and data are provided or not)?   \n640 Answer: [NA]   \n641 Justification: The paper is a theory paper, and it has no experiments.   \n642 Guidelines:   \n643 \u2022 The answer NA means that the paper does not include experiments.   \n644 \u2022 If the paper includes experiments, a No answer to this question will not be perceived   \n645 well by the reviewers: Making the paper reproducible is important, regardless of   \n646 whether the code and data are provided or not.   \n647 \u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken   \n648 to make their results reproducible or verifiable.   \n649 \u2022 Depending on the contribution, reproducibility can be accomplished in various ways.   \n650 For example, if the contribution is a novel architecture, describing the architecture fully   \n651 might suffice, or if the contribution is a specific model and empirical evaluation, it may   \n652 be necessary to either make it possible for others to replicate the model with the same   \n53 dataset, or provide access to the model. In general. releasing code and data is often   \n654 one good way to accomplish this, but reproducibility can also be provided via detailed   \n655 instructions for how to replicate the results, access to a hosted model (e.g., in the case   \n656 of a large language model), releasing of a model checkpoint, or other means that are   \n657 appropriate to the research performed.   \n658 \u2022 While NeurIPS does not require releasing code, the conference does require all submis  \n659 sions to provide some reasonable avenue for reproducibility, which may depend on the   \n660 nature of the contribution. For example   \n661 (a) If the contribution is primarily a new algorithm, the paper should make it clear how   \n662 to reproduce that algorithm.   \n663 (b) If the contribution is primarily a new model architecture, the paper should describe   \n64 the architecture clearly and fully.   \n665 (c) If the contribution is a new model (e.g., a large language model), then there should   \n666 either be a way to access this model for reproducing the results or a way to reproduce   \n667 the model (e.g., with an open-source dataset or instructions for how to construct   \n668 the dataset).   \n669 (d) We recognize that reproducibility may be tricky in some cases, in which case   \n670 authors are welcome to describe the particular way they provide for reproducibility.   \n671 In the case of closed-source models, it may be that access to the model is limited in   \n672 some way (e.g., to registered users), but it should be possible for other researchers   \n673 to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "674 5. Open access to data and code   \n675 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n676 tions to faithfully reproduce the main experimental results, as described in supplemental   \n677 material?   \n678 Answer: [NA]   \n679 Justification: The paper is a theory paper, and it has no experiments.   \n680 Guidelines:   \n681 \u2022 The answer NA means that paper does not include experiments requiring code.   \n682 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n683 public/guides/CodeSubmissionPolicy) for more details.   \n684 \u2022 While we encourage the release of code and data, we understand that this might not be   \n685 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n686 including code, unless this is central to the contribution (e.g., for a new open-source   \n687 benchmark).   \n688 \u2022 The instructions should contain the exact command and environment needed to run to   \n689 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n690 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n691 \u2022 The authors should provide instructions on data access and preparation, including how   \n692 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n693 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n694 proposed method and baselines. If only a subset of experiments are reproducible, they   \n695 should state which ones are omitted from the script and why.   \n696 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n697 versions (if applicable).   \n698 \u2022 Providing as much information as possible in supplemental material (appended to the   \n699 paper) is recommended, but including URLs to data and code is permitted.   \n700 6. Experimental Setting/Details   \n701 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n702 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n703 results?   \n704 Answer: [NA]   \n705 Justification: The paper is a theory paper, and it has no experiments.   \n706 Guidelines:   \n707 \u2022 The answer NA means that the paper does not include experiments.   \n708 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n709 that is necessary to appreciate the results and make sense of them.   \n710 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n711 material.   \n712 7. Experiment Statistical Significance   \n713 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n714 information about the statistical significance of the experiments?   \n715 Answer: [NA]   \n716 Justification: The paper is a theory paper, and it has no experiments.   \n717 Guidelines:   \n718 \u2022 The answer NA means that the paper does not include experiments.   \n719 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n720 dence intervals, or statistical significance tests, at least for the experiments that support   \n721 the main claims of the paper.   \n722 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n723 example, train/test split, initialization, random drawing of some parameter, or overall   \n724 run with given experimental conditions).   \n725 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n726 call to a library function, bootstrap, etc.)   \n727 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n728 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n729 of the mean.   \n730 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n731 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n732 of Normality of errors is not verified.   \n733 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n734 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n735 error rates).   \n736 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n737 they were calculated and reference the corresponding figures or tables in the text.   \n738 8. Experiments Compute Resources   \n739 Question: For each experiment, does the paper provide sufficient information on the com  \n740 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n741 the experiments?   \n742 Answer: [NA]   \n743 Justification: The paper is a theory paper, and it has no experiments.   \n744 Guidelines:   \n745 \u2022 The answer NA means that the paper does not include experiments.   \n746 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n747 or cloud provider, including relevant memory and storage.   \n748 \u2022 The paper should provide the amount of compute required for each of the individual   \n749 experimental runs as well as estimate the total compute.   \n750 \u2022 The paper should disclose whether the full research project required more compute   \n751 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n752 didn\u2019t make it into the paper).   \n753 9. Code Of Ethics   \n754 Question: Does the research conducted in the paper conform, in every respect, with the   \n755 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n756 Answer: [Yes]   \n757 Justification: The paper conforms with the NeurIPS Code of Ethics.   \n758 Guidelines:   \n759 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n760 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n761 deviation from the Code of Ethics.   \n762 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n763 eration due to laws or regulations in their jurisdiction).   \n764 10. Broader Impacts   \n765 Question: Does the paper discuss both potential positive societal impacts and negative   \n766 societal impacts of the work performed?   \n767 Answer: [NA]   \n768 Justification: The paper provides a theoretical result. We do not believe there are potential   \n769 societal consequences of our work, aside from advancing the field of Machine Learning.   \n770 Guidelines:   \n771 \u2022 The answer NA means that there is no societal impact of the work performed.   \n772 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n773 impact or why the paper does not address societal impact.   \n774 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n775 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n776 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n777 groups), privacy considerations, and security considerations.   \n778 \u2022 The conference expects that many papers will be foundational research and not tied   \n779 to particular applications, let alone deployments. However, if there is a direct path to   \n780 any negative applications, the authors should point it out. For example, it is legitimate   \n781 to point out that an improvement in the quality of generative models could be used to   \n782 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n783 that a generic algorithm for optimizing neural networks could enable people to train   \n784 models that generate Deepfakes faster.   \n785 \u2022 The authors should consider possible harms that could arise when the technology is   \n786 being used as intended and functioning correctly, harms that could arise when the   \n787 technology is being used as intended but gives incorrect results, and harms following   \n788 from (intentional or unintentional) misuse of the technology.   \n789 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n790 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n791 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n792 feedback over time, improving the efficiency and accessibility of ML).   \n793 11. Safeguards   \n794 Question: Does the paper describe safeguards that have been put in place for responsible   \n795 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n796 image generators, or scraped datasets)?   \n797 Answer: [NA]   \n798 Justification: The paper only contains theoretical results.   \n799 Guidelines:   \n800 \u2022 The answer NA means that the paper poses no such risks.   \n801 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n802 necessary safeguards to allow for controlled use of the model, for example by requiring   \n803 that users adhere to usage guidelines or restrictions to access the model or implementing   \n804 safety filters.   \n805 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n806 should describe how they avoided releasing unsafe images.   \n807 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n808 not require this, but we encourage authors to take this into account and make a best   \n809 faith effort.   \n810 12. Licenses for existing assets   \n811 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n812 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n813 properly respected?   \n814 Answer: [NA]   \n815 Justification: The paper only contains theoretical results. Previous work is properly cited.   \n816 Guidelines:   \n817 \u2022 The answer NA means that the paper does not use existing assets.   \n818 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n819 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n820 URL.   \n821 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n822 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n823 service of that source should be provided.   \n824 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n825 package should be provided. For popular datasets, paperswithcode.com/datasets   \n826 has curated licenses for some datasets. Their licensing guide can help determine the   \n827 license of a dataset. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 22}, {"type": "text", "text": "830 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n831 the asset\u2019s creators.   \n832 13. New Assets   \n833 Question: Are new assets introduced in the paper well documented and is the documentation   \n834 provided alongside the assets?   \n835 Answer: [NA]   \n836 Justification: The paper does not introduce any new asset.   \n837 Guidelines:   \n838 \u2022 The answer NA means that the paper does not release new assets.   \n839 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n840 submissions via structured templates. This includes details about training, license,   \n841 limitations, etc.   \n842 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n843 asset is used.   \n844 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n845 create an anonymized URL or include an anonymized zip file.   \n846 14. Crowdsourcing and Research with Human Subjects   \n847 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n848 include the full text of instructions given to participants and screenshots, if applicable, as   \n849 well as details about compensation (if any)?   \n850 Answer: [NA]   \n851 Justification: This point does not apply to our paper.   \n852 Guidelines:   \n853 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n854 human subjects.   \n855 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n856 tion of the paper involves human subjects, then as much detail as possible should be   \n857 included in the main paper.   \n858 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n859 or other labor should be paid at least the minimum wage in the country of the data   \n860 collector. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 22}]