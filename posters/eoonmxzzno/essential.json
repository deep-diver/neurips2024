{"importance": "This paper is important because it addresses a critical limitation in existing Temporal Sentence Grounding (TSG) methods by introducing a new task, TSG-RF, which considers the uncertainty of relevant segments' existence in videos. This is highly relevant to the current research trend of building more robust and realistic multi-modal models.  **The proposed RaTSG network offers a novel solution to a challenging problem**, paving the way for further research on improving TSG methods' accuracy and applicability to real-world scenarios.", "summary": "RaTSG network tackles Temporal Sentence Grounding with Relevance Feedback (TSG-RF) by discerning query relevance at multiple granularities before selectively grounding segments.", "takeaways": ["The paper introduces a novel task: Temporal Sentence Grounding with Relevance Feedback (TSG-RF).", "A novel Relation-aware Temporal Sentence Grounding (RaTSG) network is proposed for TSG-RF.", "Experiments on reconstructed datasets demonstrate RaTSG's effectiveness in handling both relevant and irrelevant video segments."], "tldr": "Traditional Temporal Sentence Grounding (TSG) in videos unrealistically assumes relevant segments always exist. This paper introduces Temporal Sentence Grounding with Relevance Feedback (TSG-RF), addressing this limitation by incorporating relevance feedback. TSG-RF localizes segments when relevant content is present and provides feedback on the content's absence otherwise.\nThe proposed Relation-aware Temporal Sentence Grounding (RaTSG) network tackles TSG-RF as a foreground-background detection problem. **It uses a multi-granularity relevance discriminator to assess video-query relevance at both frame and video levels**, and a relation-aware segment grounding module dynamically adapts to the presence or absence of query-related segments.  **Experimental results on reconstructed TSG datasets show RaTSG's effectiveness**.", "affiliation": "Peking University", "categories": {"main_category": "Natural Language Processing", "sub_category": "Vision-Language Models"}, "podcast_path": "eOonmxzzno/podcast.wav"}