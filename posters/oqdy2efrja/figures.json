[{"figure_path": "oqdy2EFrja/figures/figures_3_1.jpg", "caption": "Figure 1: Illustration of our method. It consists of three stages: 1) The input textual prompt is decomposed into individual concepts by the LLM. Then we generate each concept in the form of 3D with the corresponding pre-trained expert model (left & Sec. 4.1). 2) We leverage knowledge in multi-modal LLM to estimate the 2D trajectory of objects step-by-step (middle & Sec. 4.2). 3) After lifting the estimated 2D trajectory into 3D as initialization, we refine the scales, locations, and rotations of objects within the 3D scene using 2D diffusion priors (right & Sec. 4.3).", "description": "This figure illustrates the three-stage process of the proposed method (C3V).  Stage 1 involves decomposing a complex textual prompt into individual concepts (scene, objects, motion) using an LLM and generating their 3D representations using pre-trained expert models. Stage 2 utilizes a multi-modal LLM to estimate the 2D trajectories of objects step-by-step. Finally, Stage 3 refines the 3D object scales, locations, and rotations by lifting the 2D trajectories into 3D and using 2D diffusion priors with score distillation sampling.", "section": "4 Method"}, {"figure_path": "oqdy2EFrja/figures/figures_4_1.jpg", "caption": "Figure 2: Illustration of coarse-grained trajectory generation with LLM. Instead of querying multi-modal LLM to estimate dynamic trajectory directly, we generate trajectory in a step-by-step manner: estimating the locations of starting and ending points first, then reasoning the path between them.", "description": "This figure illustrates the process of generating a trajectory using a Large Language Model (LLM).  Instead of directly asking the LLM for a complete trajectory, which can lead to unrealistic results, the authors employ a step-by-step approach. First, they ask the LLM to identify the starting and ending points of the trajectory using bounding boxes. Then, they provide this information to the LLM, along with the image, and request a series of intermediate points that create a smooth path between the start and end. This approach makes it easier for the LLM to generate a reasonable and natural-looking trajectory.", "section": "4.2 Coarse-grained Trajectory Generation with LLM"}, {"figure_path": "oqdy2EFrja/figures/figures_6_1.jpg", "caption": "Figure 3: Qualitative comparisons with baselines. When prompting complex queries, the baseline methods fail to follow the queries in terms of the number of objects and the corresponding motion. In contrast, our method excels in yielding both diverse motion and high visual quality.", "description": "This figure compares the results of the proposed method (C3V) with three baseline methods (4D-FY, Comp4D, and VideoCrafter2) on two different text prompts. The results show that C3V outperforms baselines in generating videos with diverse motions and high visual quality, especially when dealing with complex queries containing multiple objects and actions.", "section": "5.1 Comparison with Competitors"}, {"figure_path": "oqdy2EFrja/figures/figures_6_2.jpg", "caption": "Figure 3: Qualitative comparisons with baselines. When prompting complex queries, the baseline methods fail to follow the queries in terms of the number of objects and the corresponding motion. In contrast, our method excels in yielding both diverse motion and high visual quality.", "description": "This figure compares the video generation results of the proposed C3V model with three baseline models (4D-FY, Comp4D, and VideoCrafter2) on two complex text prompts. The results demonstrate that the baseline methods struggle to accurately generate videos with multiple objects and the corresponding motions, while the proposed C3V model is able to generate high-quality videos with diverse motions and high visual quality, fulfilling the requirements of the complex prompts.", "section": "5.1 Comparison with Competitors"}, {"figure_path": "oqdy2EFrja/figures/figures_7_1.jpg", "caption": "Figure 4: Ablation studies on framework design. Each ablation is prompted with the same text.", "description": "This figure presents ablation studies on the proposed C3V framework.  The top row (a) shows the impact of different trajectory estimation methods using a multi-modal LLM: direct estimation, estimation with bounding boxes, step-by-step estimation, and the final C3V method.  The bottom row (b) illustrates the effect of refining composition using 2D diffusion priors, showing results without refinement and with refinements to scale, trajectory, and rotation.  Each ablation uses the same text prompt to highlight the impact of each design choice.", "section": "5.2 Ablation Studies"}, {"figure_path": "oqdy2EFrja/figures/figures_8_1.jpg", "caption": "Figure 5: Our method offers flexible control of individual concepts. We demonstrate this by editing different concepts: the appearance and motion of the actors, and the scenes.", "description": "This figure shows the flexible control offered by the proposed method over individual concepts in the generated videos.  It demonstrates this capability by presenting three editing scenarios:\n\n(a) **Object Editing:** Changing the appearance of an actor by replacing it with different objects. \n(b) **Motion Editing:** Modifying the motion of an actor by replacing its motion with different actions. \n(c) **Scene Editing:** Altering the background scene completely, showcasing the ability to seamlessly integrate the actors into new and varied environments.", "section": "5.3 Applications on Controllable Generation"}, {"figure_path": "oqdy2EFrja/figures/figures_15_1.jpg", "caption": "Figure 6: Results of different stages given the textual prompt: \"In a Magician's magical cabin alone in a serene forest, an alien walking on the floor, starting from the cabin's door to the mow near the bottom right corner of this image.\"", "description": "This figure illustrates the three stages of the C3V model. The first stage is task decomposition using an LLM, which breaks down the complex prompt into sub-prompts describing the scene, the object (alien), and the motion (walking). The second stage is trajectory generation using an LLM, which estimates the trajectory of the alien by using bounding boxes and stepwise reasoning. The third stage is the rendering of the final video using 3D Gaussian splatting and 2D diffusion priors.", "section": "4.1 Task Decomposition with LLM"}, {"figure_path": "oqdy2EFrja/figures/figures_16_1.jpg", "caption": "Figure 7: Results of different stages given the textual prompt: \"Inside a cozy livingroom in Christmas, a astrologer performing ballet on the floor, starting from wooden floor behind the red armchair near the bottom left of this image to the sofa in the bottom right corner of this image.\"", "description": "This figure shows a breakdown of the process for generating a video based on the given text prompt.  It illustrates the three stages of the C3V method: Task Decomposition with LLM (breaking the complex prompt into sub-prompts), Trajectory Generation with LLM (estimating the trajectory based on the sub-prompts using a bounding box approach and refining through a step-by-step method), and finally the Rendered Video (output of the process).  Each stage is visually represented, allowing one to see how the initial prompt is translated into a coherent video.", "section": "4. Method"}, {"figure_path": "oqdy2EFrja/figures/figures_17_1.jpg", "caption": "Figure 8: Results of different stages given the textual prompt: \"On a simple stage, a man with a black fedora and a denim jacket and a woman wearing ski clothes are performing Kungfu and dancing respectively, on the left side and right side of this stage.\"", "description": "This figure shows a breakdown of the process of generating a video using the C3V method.  It starts with task decomposition using an LLM to break down the input prompt into individual concepts (a simple stage, a man in a specific outfit performing kung fu, a woman in ski clothes dancing).  Then, trajectory generation uses the LLM to estimate the path each character will take on the stage, leveraging bounding boxes to help define starting and ending points. Finally, the rendered video output from the three stages is shown.", "section": "4 Method"}, {"figure_path": "oqdy2EFrja/figures/figures_18_1.jpg", "caption": "Figure 9: Results of actor's appearance editing.", "description": "This figure shows the results of editing the appearance of an actor in a generated video.  Four different prompts are tested: (a) A black man wearing a green t-shirt playing Kungfu on the stage; (b) Turn the character into a fairy; (c) Turn the character into a warlock.  Each prompt demonstrates the system's ability to change the actor's visual characteristics in the generated video sequence.", "section": "5.3 Applications on Controllable Generation"}, {"figure_path": "oqdy2EFrja/figures/figures_18_2.jpg", "caption": "Figure 9: Results of actor's appearance editing.", "description": "This figure shows the results of editing the appearance of the actor in a generated video.  The top row shows a video of a black man in a green shirt performing Kung Fu. The next two rows show the same video but with the actor's appearance changed, first into a fairy and then into a warlock. This demonstrates the model's ability to alter the visual characteristics of individual elements within a generated scene.", "section": "5.3 Applications on Controllable Generation"}, {"figure_path": "oqdy2EFrja/figures/figures_18_3.jpg", "caption": "Figure 9: Results of actor's appearance editing.", "description": "This figure shows the results of the actor's appearance editing experiment.  The top row shows a baseline video of a black man in a green shirt performing kung fu on a stage. The subsequent rows demonstrate edits of that character's appearance.  The second row shows the character transformed into a fairy; the third row, into a warlock.  This demonstrates the system's ability to alter the appearance of a character based on textual prompts.", "section": "5.3 Applications on Controllable Generation"}, {"figure_path": "oqdy2EFrja/figures/figures_18_4.jpg", "caption": "Figure 9: Results of actor\u2019s appearance editing.", "description": "This figure shows the results of editing the appearance of the actor in a generated video.  Subfigure (a) shows the original generated video of a black man in a green t-shirt doing Kung Fu.  Subfigure (b) shows the same video, but the character has been changed to a fairy.  Subfigure (c) shows the same video again, but with the character changed to a warlock.  This demonstrates the ability of the model to edit the appearance of the character by simply changing a textual prompt.", "section": "5.3 Applications on Controllable Generation"}, {"figure_path": "oqdy2EFrja/figures/figures_18_5.jpg", "caption": "Figure 9: Results of actor's appearance editing.", "description": "This figure shows the results of editing the appearance of the actor in a generated video.  Four subfigures show the results using different prompts. Subfigure (a) shows the base result, with a black man wearing a green t-shirt.  Subfigure (b) changes the character's appearance to that of a fairy, Subfigure (c) shows the result of changing the character into a warlock.", "section": "5.3 Applications on Controllable Generation"}, {"figure_path": "oqdy2EFrja/figures/figures_18_6.jpg", "caption": "Figure 9: Results of actor's appearance editing.", "description": "This figure shows the results of editing the appearance of an actor in a generated video.  Four image sequences are presented, each corresponding to a different prompt. (a) shows a base video with a black man in a green shirt. (b) modifies the prompt to change the character to a fairy. (c) changes the prompt to turn the character into a warlock. Each row displays the resulting video frames demonstrating that the model successfully alters the character's appearance based on the textual instructions given.", "section": "5.3 Applications on Controllable Generation"}, {"figure_path": "oqdy2EFrja/figures/figures_19_1.jpg", "caption": "Figure 11: Results of scene editing.", "description": "This figure shows the results of scene editing in the C3V model.  Three sub-figures demonstrate how easily the scene can be changed while retaining the other elements (character and action). (a) shows a scene with a modern mega villa by the sea and a swimming pool,  (b) transforms the scene into an anime-style road, and (c) changes the setting to a post-apocalyptic desert city. This illustrates the model's capacity for flexible and precise control over individual scene components during video generation.", "section": "5.3 Applications on Controllable Generation"}, {"figure_path": "oqdy2EFrja/figures/figures_19_2.jpg", "caption": "Figure 1: Illustration of our method. It consists of three stages: 1) The input textual prompt is decomposed into individual concepts by the LLM. Then we generate each concept in the form of 3D with the corresponding pre-trained expert model (left & Sec. 4.1). 2) We leverage knowledge in multi-modal LLM to estimate the 2D trajectory of objects step-by-step (middle & Sec. 4.2). 3) After lifting the estimated 2D trajectory into 3D as initialization, we refine the scales, locations, and rotations of objects within the 3D scene using 2D diffusion priors (right & Sec. 4.3).", "description": "This figure illustrates the three-stage process of the proposed C3V method. Stage 1 uses an LLM to decompose the input text prompt into individual concepts (scene, objects, motion). Stage 2 estimates the 2D trajectory of the objects using a multi-modal LLM. Stage 3 refines the 3D representation of the objects using 2D diffusion priors.", "section": "4 Method"}, {"figure_path": "oqdy2EFrja/figures/figures_19_3.jpg", "caption": "Figure 11: Results of scene editing.", "description": "This figure shows three examples of scene editing using the proposed method.  In (a), the original scene is a modern mega-villa by the sea, and the prompt is changed to show the character dancing in the scene. In (b), the scene is changed to a long anime-style road, and in (c), the scene is transformed into a post-apocalyptic city in the desert. This demonstrates the flexibility of the method to edit the scene while keeping the character consistent.", "section": "5.3 Applications on Controllable Generation"}]