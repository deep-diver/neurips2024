[{"figure_path": "5FATPIlWUJ/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of RRP to a standard GP and a variational GP with a Student-t likelihood on a regression example. While the other models are led astray by the corrupted observations, RRP successfully identifies the corruptions (red) and thus achieves a much better fit to the ground truth.", "description": "This figure compares the performance of three different Gaussian Process (GP) models on a regression task with label corruptions.  The standard GP and a variational GP with a Student-t likelihood are shown to be negatively affected by the corrupted data points, while the proposed Robust Gaussian Processes via Relevance Pursuit (RRP) method accurately identifies and accounts for these corruptions, resulting in a significantly improved fit to the true underlying function.", "section": "Introduction"}, {"figure_path": "5FATPIlWUJ/figures/figures_3_1.jpg", "caption": "Figure 2: Left: Evolution of model posterior during Relevance Pursuit, as the number of data-point-specific variances |S| increases (from light colors to dark). Red points indicate corruptions that were generated by uniformly sampling from the function's range. Right: Comparison of posterior marginal likelihoods as a function of a model's |S|. The maximizer \u2013 boxed in black \u2013 is the preferred model.", "description": "The left panel shows how the model's posterior distribution evolves as more data points are considered as potential outliers by the Relevance Pursuit algorithm.  Each line represents the posterior distribution for different numbers of potential outliers, starting with one and increasing to 30. The red points highlight the locations of artificially introduced corruptions in the data. The right panel displays a bar chart showing the marginal likelihood for models with varying numbers of potential outliers.  The model with the highest marginal likelihood, which is the preferred model, is highlighted in black.", "section": "4 Robust Gaussian Process Regression via Relevance Pursuit"}, {"figure_path": "5FATPIlWUJ/figures/figures_6_1.jpg", "caption": "Figure 2: Left: Evolution of model posterior during Relevance Pursuit, as the number of data-point-specific variances |S| increases (from light colors to dark). Red points indicate corruptions that were generated by uniformly sampling from the function's range. Right: Comparison of posterior marginal likelihoods as a function of a model's |S|. The maximizer \u2013 boxed in black \u2013 is the preferred model.", "description": "The left panel of the figure shows how the model posterior evolves as more data points are included in the model. The right panel shows the posterior marginal likelihood as a function of the number of data points included in the model.", "section": "4 Robust Gaussian Process Regression via Relevance Pursuit"}, {"figure_path": "5FATPIlWUJ/figures/figures_7_1.jpg", "caption": "Figure 4: Left: Distribution of predictive test-set log likelihood for various methods. Methods omitted are those that performed substantially worse. Right: Predictive log likelihood as a function of the corruption probability for Student-t-distributed corruptions with two degrees of freedom. The GP model with the Student-t likelihood only starts outperforming RRP as the corruption probability increases beyond 40%, and exhibits a large variance in outcomes, which shrinks as the proportion of corruptions increases. All methods not shown were inferior to either RRP or Student-t.", "description": "The figure compares the performance of different robust Gaussian process regression methods on synthetic datasets. The left panel shows the distribution of predictive log-likelihood for each method, highlighting RRP's superior performance, especially with high corruption levels. The right panel shows how predictive log-likelihood changes with increasing corruption probability, emphasizing RRP's robustness to label corruptions.", "section": "6.1 Regression Problems"}, {"figure_path": "5FATPIlWUJ/figures/figures_8_1.jpg", "caption": "Figure 2: Left: Evolution of model posterior during Relevance Pursuit, as the number of data-point-specific variances |S| increases (from light colors to dark). Red points indicate corruptions that were generated by uniformly sampling from the function's range. Right: Comparison of posterior marginal likelihoods as a function of a model's |S|. The maximizer \u2013 boxed in black \u2013 is the preferred model.", "description": "This figure shows two plots. The left plot visualizes the evolution of the model posterior during the Relevance Pursuit algorithm.  The algorithm iteratively adds data points as potential outliers, indicated by increasing data-point-specific noise variance. The lines represent different stages of the algorithm. Red points show corrupted data points that were generated randomly within the range of the function. The right plot illustrates the marginal likelihood as a function of the number of outliers (S). The model with the highest marginal likelihood is the preferred model, as indicated by the black box.", "section": "4 Robust Gaussian Process Regression via Relevance Pursuit"}, {"figure_path": "5FATPIlWUJ/figures/figures_9_1.jpg", "caption": "Figure 6: BO results for Hartmann6: Left: Relevance pursuit performs well in the case of constant outliers of value 100, almost as well as the oracle. Middle: Relevance pursuit performs the best followed by the Student-t likelihood in the case of U[\u22123, 3]. Right: Similar to the middle plot, this setting hides the corruptions within the range of the function, making it a challenging task.", "description": "This figure shows the results of Bayesian Optimization (BO) experiments on the Hartmann6 test function with three different types of label corruptions: constant outliers, uniform outliers, and uniform input outliers.  The x-axis represents the number of evaluations and the y-axis is the value of the best inferred point found so far. The results for various BO algorithms are plotted, including Relevance Pursuit (RRP), an oracle that always knows the true uncorrupted value, as well as standard baselines like a standard GP, and a Student-t likelihood model. The results show that Relevance Pursuit performs competitively in most cases, even when the corruptions are harder to detect (like uniform input outliers).", "section": "6.2 Robust Bayesian Optimization"}, {"figure_path": "5FATPIlWUJ/figures/figures_9_2.jpg", "caption": "Figure 2: Left: Evolution of model posterior during Relevance Pursuit, as the number of data-point-specific variances |S| increases (from light colors to dark). Red points indicate corruptions that were generated by uniformly sampling from the function's range. Right: Comparison of posterior marginal likelihoods as a function of a model's |S|. The maximizer \u2013 boxed in black \u2013 is the preferred model.", "description": "The left panel shows how the model posterior evolves during the Relevance Pursuit algorithm.  As the number of data points with adjusted noise variances increases, the model's fit improves, adapting to the corrupted data points (shown in red). The right panel shows how the posterior marginal likelihood varies with the number of data points with adjusted noise variances. The algorithm selects the model with the highest marginal likelihood (boxed in black).", "section": "4 Robust Gaussian Process Regression via Relevance Pursuit"}, {"figure_path": "5FATPIlWUJ/figures/figures_22_1.jpg", "caption": "Figure 2: Left: Evolution of model posterior during Relevance Pursuit, as the number of data-point-specific variances |S| increases (from light colors to dark). Red points indicate corruptions that were generated by uniformly sampling from the function's range. Right: Comparison of posterior marginal likelihoods as a function of a model's |S|. The maximizer \u2013 boxed in black \u2013 is the preferred model.", "description": "The figure shows two plots. The left plot displays the evolution of the model posterior during the Relevance Pursuit algorithm. As the number of data-point-specific noise variances increases, the model's fit improves. Red points in the plot highlight corruptions that were randomly generated within the function's range. The right plot compares the posterior marginal likelihoods for different model sizes. The model with the highest marginal likelihood is chosen as the preferred model.", "section": "4 Robust Gaussian Process Regression via Relevance Pursuit"}, {"figure_path": "5FATPIlWUJ/figures/figures_23_1.jpg", "caption": "Figure 2: Left: Evolution of model posterior during Relevance Pursuit, as the number of data-point-specific variances |S| increases (from light colors to dark). Red points indicate corruptions that were generated by uniformly sampling from the function's range. Right: Comparison of posterior marginal likelihoods as a function of a model's |S|. The maximizer \u2013 boxed in black \u2013 is the preferred model.", "description": "The left panel of the figure shows how the model's posterior changes as more data-point-specific noise variances are added using the Relevance Pursuit algorithm.  The algorithm iteratively adds data points whose individual noise variance maximizes the marginal likelihood. The red points are corrupted data points. The right panel shows how the posterior marginal likelihood changes as a function of the model's support size (the number of data points with non-zero noise variances).  The algorithm selects the model with the maximum marginal likelihood, indicated by the black box.", "section": "4 Robust Gaussian Processes Regression via Relevance Pursuit"}, {"figure_path": "5FATPIlWUJ/figures/figures_23_2.jpg", "caption": "Figure 6: BO results for Hartmann6: Left: Relevance pursuit performs well in the case of constant outliers of value 100, almost as well as the oracle. Middle: Relevance pursuit performs the best followed by the Student-t likelihood in the case of U[\u22123, 3]. Right: Similar to the middle plot, this setting hides the corruptions within the range of the function, making it a challenging task.", "description": "This figure shows the results of Bayesian optimization (BO) experiments on the Hartmann6 test function with three different types of label corruption: constant outliers, outliers uniformly sampled from [-3, 3], and outliers uniformly sampled from the function's input domain.  The results compare the performance of several BO methods, including Relevance Pursuit (RRP), showing that RRP performs favorably across various corruption settings, often approaching or matching the performance of an 'oracle' that has access to uncorrupted labels.", "section": "Robust Bayesian Optimization"}]