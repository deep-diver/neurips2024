[{"type": "text", "text": "Robust Gaussian Processes via Relevance Pursuit ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sebastian Ament Elizabeth Santorella David Eriksson Meta Meta Meta   \nament@meta.com santorella@meta.com deriksson@meta.com Ben Letham Maximilian Balandat Eytan Bakshy Meta Meta Meta   \nbletham@meta.com balandat@meta.com ebakshy@meta.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Gaussian processes (GPs) are non-parametric probabilistic regression models that are popular due to their flexibility, data efficiency, and well-calibrated uncertainty estimates. However, standard GP models assume homoskedastic Gaussian noise, while many real-world applications are subject to non-Gaussian corruptions. Variants of GPs that are more robust to alternative noise models have been proposed, and entail significant trade-offs between accuracy and robustness, and between computational requirements and theoretical guarantees. In this work, we propose and study a GP model that achieves robustness against sparse outliers by inferring data-point-specific noise levels with a sequential selection procedure maximizing the log marginal likelihood that we refer to as relevance pursuit. We show, surprisingly, that the model can be parameterized such that the associated log marginal likelihood is strongly concave in the data-point-specific noise variances, a property rarely found in either robust regression objectives or GP marginal likelihoods. This in turn implies the weak submodularity of the corresponding subset selection problem, and thereby proves approximation guarantees for the proposed algorithm. We compare the model\u2019s performance relative to other approaches on diverse regression and Bayesian optimization tasks, including the challenging but common setting of sparse corruptions of the labels within or close to the function range. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Probabilistic models have long been a central part of machine learning, and Gaussian process (GP) models are a key workhorse for many important tasks [49], especially in the small-data regime. GPs are flexible, non-parametric predictive models known for their high data efficiency and well-calibrated uncertainty estimates, making them a popular choice for regression, uncertainty quantification, and downstream applications such as Bayesian optimization (BO) [22, 24] and active learning [6, 50]. ", "page_idx": 0}, {"type": "text", "text": "GPs flexibly model a distribution over functions, but assume a particular observation model. The standard formulation assumes i.i.d Gaussian observation noise, i.e., $y(\\mathbf{x})=f(\\mathbf{x})+\\epsilon$ , where $f(\\mathbf{x})$ is the true (latent) function value at a point $\\mathbf{x}$ and $\\epsilon\\sim\\mathcal{N}(0,\\sigma^{2})$ , implying a homoskedastic Gaussian likelihood. While mathematically convenient, this assumption can be a limitation in practice, since noise distributions are often heavy-tailed or observations may be corrupted due to issues such as sensor failures, data processing errors, or software bugs. Using a standard GP model in such settings can result in poor predictive performance. ", "page_idx": 0}, {"type": "text", "text": "A number of robust GP modeling approaches have been proposed to remedy this shortcoming, most of which fall into the following broad categories: data pre-processing (e.g., Winsorizing), modified likelihood functions (e.g., Student- $\\cdot t$ ), and model-based data selection and down-weighting procedures. ", "page_idx": 0}, {"type": "text", "text": "These approaches offer different trade-offs between model accuracy, degree of robustness, broad applicability, computational requirements, and theoretical guarantees. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose a simple yet effective implicit data-weighting approach that endows GPs with a high degree of robustness to challenging label corruptions. Our approach is flexible and can be used with arbitrary kernels, is efficient to compute, and yields provable approximation guarantees. Our main contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "1. We propose a modification to the standard GP model that introduces learnable data-point-specific noise variances.   \n2. We introduce a novel greedy sequential selection procedure for maximizing the model\u2019s marginal log-likelihood (MLL) that we refer to as relevance pursuit.   \n3. We prove that, under a particular parameterization, the MLL is strongly concave in the datapoint-specific noise variances, and derive approximation guarantees for our algorithm.   \n4. We demonstrate that our approach, Robust Gaussian Processes via Relevance Pursuit (RRP), performs favorably compared to alternative methods across various benchmarks, including challenging settings of sparse label corruptions within the function\u2019s range, see e.g. Figure 1. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We aim to model a function $f:\\mathbb{X}\\to\\mathbb{R}$ over some domain $\\mathbb{X}\\subset\\mathbb{R}^{d}$ . With a standard Gaussian noise model, for $\\mathbf{x}_{i}\\in\\mathbb{X}$ we obtain observations $y_{i}=f(\\mathbf{x}_{i})+\\epsilon_{i}$ , where $\\epsilon_{i}\\sim\\mathcal{N}(0,\\sigma^{2})$ are i.i.d. draws from a Gaussian random variable. $\\lVert\\cdot\\rVert$ denotes the Euclidean norm unless indicated otherwise. ", "page_idx": 1}, {"type": "text", "text": "2.1 Gaussian Processes ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "A GP $f\\sim\\mathcal{G P}(\\mu(\\cdot),k_{\\theta}(\\cdot,\\cdot))$ is fully defined by its mean function $\\mu:\\mathbb{X}\\to\\mathbb{R}$ and covariance or kernel function $k_{\\theta}:\\mathbb{X}\\times\\mathbb{X}\\rightarrow\\mathbb{R}$ , which is parameterized by $\\pmb{\\theta}$ . Without loss of generality, we will assume that $\\mu\\equiv0$ . Suppose we have collected data $\\mathbf{\\mathcal{D}}\\,=\\,\\{(\\mathbf{x}_{i},y_{i})\\}_{i=1}^{n}$ where $\\mathbf{X}:=\\{\\mathbf{x}_{i}\\}_{i=1}^{n}$ , $\\mathbf{y}:=\\{y_{i}\\}_{i=1}^{n}$ . Let $\\Sigma_{\\theta}\\in\\mathcal{S}_{++}^{n}$ denote the covariance matrix of the data set, i.e., $[\\Sigma_{\\theta}]_{i j}\\,=\\,k_{\\theta}({\\bf x}_{i},{\\bf x}_{j})+\\delta_{i j}\\sigma^{2}$ , where $\\delta_{i j}$ is the Kronecker delta. The negative marginal loglikelihood (NMLL) $\\mathcal{L}$ is given by ", "page_idx": 1}, {"type": "image", "img_path": "5FATPIlWUJ/tmp/7931303b519951218bcc4bb2ba2a8cdbbbdb6badd32099525c2963fac3a3fea8.jpg", "img_caption": ["Posterior Predictive Distributions ", "Figure 1: Comparison of RRP to a standard GP and a variational GP with a Student- $\\cdot t$ likelihood on a regression example. While the other models are led astray by the corrupted observations, RRP successfully identifies the corruptions (red) and thus achieves a much better fti to the ground truth. "], "img_footnote": [], "page_idx": 1}, {"type": "equation", "text": "$$\n-2\\mathcal{L}(\\pmb{\\theta}):=-2\\log p(\\mathbf{y}|\\mathbf{X},\\pmb{\\theta})=\\mathbf{y}^{\\top}\\pmb{\\Sigma}_{\\pmb{\\theta}}^{-1}\\mathbf{y}+\\log\\operatorname*{det}\\pmb{\\Sigma}_{\\pmb{\\theta}}+n\\log2\\pi.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "In the following, we will suppress the explicit dependence of the kernel matrix on $\\pmb{\\theta}$ for brevity of notation. For a comprehensive background on GPs, we refer to Rasmussen et al. [49]. ", "page_idx": 1}, {"type": "text", "text": "2.2 Noise Models ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Additive, heavy-tailed noise Instead of assuming the noise term $\\epsilon_{i}$ in the observation model to be Gaussian, other noise models consider zero-mean perturbations drawn from distributions with heavier tails, such as the Student- $\\cdot t$ [30], Laplace [35], or $\\alpha$ -Stable [5] distributions. These types of errors are common in applications such as finance, geophysics, and epidemiology [16]. Robust regression models utilizing Student- $\\boldsymbol{\\cdot}$ errors are commonly used to combat heavy-tailed noise and outliers. ", "page_idx": 1}, {"type": "text", "text": "Sparse corruptions In practice, often a small number of labels are corrupted. We will refer to these as \u201coutliers,\" though emphasize that the corrupted values may fall within the range of normal outputs. Sparse corruptions are captured by a model of the form $y_{i}\\bar{=}Z_{i}f(\\mathbf{x}_{i})+(1-\\bar{Z_{i}})W_{i}$ , where ", "page_idx": 1}, {"type": "text", "text": "$Z_{i}\\in\\{0,1\\}$ and $W_{i}\\in\\mathbb{R}$ is a random variable. Note that $W_{i}$ need not have (and rarely has) $f(\\mathbf{x}_{i})$ as its mean. For instance, consider a faulty sensor that with some probability $p$ reports a random value within the sensor range $[y_{l},y_{h}]$ . In this case $Z_{i}\\sim\\operatorname{Ber}(p)$ and $W_{i}\\sim\\mathrm{U}[y_{l},y_{h}]$ . Software bugs, such as those found in ML training procedures, or errors in logging data can result in sparse corruptions. ", "page_idx": 2}, {"type": "text", "text": "3 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Data pre-processing Data pre-processing can be an effective technique for handling simple forms of data corruption, such as values that fall outside a valid range of outputs. With such pre-processing, outliers are handled upstream of the regression model. Common techniques include the power transformations [13], trimming, and winsorization. These methods can add substantial bias if not used carefully, and generally do not handle data corruptions that occur within the normal range of the process to be modeled. See [15] for a review on data cleaning. ", "page_idx": 2}, {"type": "text", "text": "Heavy-tailed likelihoods One class of robust methods uses additive heavy-tailed noise likelihoods for GPs, particularly Student- $\\cdot t$ [30], Laplace [48], and Huber [1], and could be extended with $\\alpha$ -Stable distributions, which follow a generalized central limit theorem [5]. These models are less sensitive to outliers, but they lose efficiency when the outliers are a sparse subset of the observations, as opposed to global heavy-tailed noise. Furthermore, model inference is no longer analytic, necessitating the use of approximate inference approaches such as MCMC [44], Laplace approximation [59], expectation propagation (EP) [30], Expectation Maximization [48], or variational inference [56]. Shah et al. [51] take a related approach using a Student- $\\cdot t$ process prior in the place of the GP prior. Unfortunately, the Student- $\\cdot t$ process is not closed under addition and lacks the tractability that makes GPs so versatile. Alternative noise specifications include a hierarchical mixture of Gaussians [17] and a \u201ctwinned\u201d GP model [43] that uses a two-component noise model to allow outlier behavior to depend on the inputs. This method is suited for settings where outliers are not totally stochastic, but generally is not able to differentiate \u201cinliers\u201d from outliers when they can occur with similar inputs. ", "page_idx": 2}, {"type": "text", "text": "Outlier classification Awasthi et al. [10] introduces the Trimmed MLE approach, which identifies the subset of data points (of pre-specified size) under which the marginal likelihood is maximized. Andrade and Takeda [9] fti GPs using the trimmed MLE by applying a projected gradient method to an approximation of the marginal likelihood. The associated theory only guarantees convergence to a stationary point, with no guarantee on quality. When no outliers are present, this method can be worse than a standard GP. Li et al. [36] propose a heuristic iterative procedure of removing those data points with the largest residuals after ftiting a standard GP, with subsequent reweighting. The method shows favorable empirical performance but has no theoretical guarantees, and fails if the largest residual is not associated with an outlier. Park et al. [47] consider a model of the form $y_{i}=\\bar{\\delta_{i}}+f(\\mathbf{x}_{i})+\\epsilon_{i}$ , where outliers are regarded as data with a large bias $\\delta_{i}$ . Their random bias model is related to our model in that it also introduces learnable, data-point-specific variances. However, inference is done in one step by optimizing the NMLL with an inverse-gamma prior on the $\\delta_{i}$ \u2019s, which \u2013 in contrast to the method proposed herein \u2013 generally does not lead to exactly sparse $\\delta_{i}$ \u2019s . ", "page_idx": 2}, {"type": "text", "text": "Sample re-weighting Altamirano et al. [2] propose robust and conjugate GPs (RCGP) based on a modification to the Gaussian likelihood function that is equivalent to standard GP inference, where the covariance of the noise ${\\boldsymbol{\\sigma}}^{2}\\mathbf{I}$ is replaced by $\\sigma^{2}\\,\\mathrm{diag}(\\mathbf{w}^{\\star})$ and the prior mean $\\mathbf{m}$ is replaced by $\\mathbf{m}_{\\mathbf{w}}=\\mathbf{m}+\\sigma^{2}\\nabla_{y}\\log(\\mathbf{w}^{2})$ . The authors advocate for the use of the inverse multi-quadratic weight function $w(\\mathbf{x},y)\\stackrel{.}{=}\\beta(1+(y-m(\\mathbf{x}))^{2}/c^{2})^{-1/2}$ , which introduces two additional hyper-parameters: the soft threshold $c$ , and the \u201clearning rate\u201d $\\beta$ . Importantly, the weights w are defined $a$ -priori as a function of the prior mean $m(\\mathbf{x})$ and the targets $y$ , thereby necessitating the weights to identify the correct outliers without access to a model. This is generally only realistic if the outlier data points are clearly separated in the input or output spaces rather than randomly interspersed. ", "page_idx": 2}, {"type": "text", "text": "4 Robust Gaussian Process Regression via Relevance Pursuit ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our method adaptively identifies a sparse set of outlying data points that are corrupted by a mechanism that is not captured by the other components of the model. This is in contrast to many other approaches to robust regression that non-adaptively apply a heavy-tailed likelihood to all observations, which can be suboptimal if many observations are of high quality. ", "page_idx": 2}, {"type": "image", "img_path": "5FATPIlWUJ/tmp/6dc4728b84c890318b8f9b7adbbeb234662806d19406a02878a3153b7143dfb1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: Left: Evolution of model posterior during Relevance Pursuit, as the number of data-pointspecific variances $|S|$ increases (from light colors to dark). Red points indicate corruptions that were generated by uniformly sampling from the function\u2019s range. Right: Comparison of posterior marginal likelihoods as a function of a model\u2019s $|S|$ . The maximizer \u2013 boxed in black \u2013 is the preferred model. ", "page_idx": 3}, {"type": "text", "text": "4.1 The Extended Likelihood Model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We extend the standard GP observation noise variance $\\sigma^{2}$ with data-point-specific noise variances $\\pmb{\\rho}=\\{\\rho_{i}\\}_{i=1}^{n}$ , so that the $i$ -th data point is distributed as ", "page_idx": 3}, {"type": "equation", "text": "$$\ny_{i}\\mid\\mathbf{x}_{i}\\sim\\mathcal{N}\\left(f(\\mathbf{x}_{i}),\\sigma^{2}+\\rho_{i}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This is similar to Sparse Bayesian Learning [55] in which weight-specific prior variances control a feature\u2019s degree of influence on a model\u2019s predictions. The likelihood in (2) is similar in that the marginal likelihood optimization of the $\\rho_{i}$ gives rise to an automatic mechanism for outlier detection and down-weighting. Intuitively, the effect of $y_{i}$ on the estimate of $f$ vanishes as $\\rho_{i}\\to\\infty$ . This model can be viewed as a type of GP with heteroskedastic noise model. However, while many heteroskedastic models model noise as an input-dependent process [26, 33], our formulation does not make such assumptions, and is thus suitable for corruptions that are not spatially correlated. ", "page_idx": 3}, {"type": "text", "text": "An elegant consequence of our modeling assumption is that we can compute individual marginallikelihood maximizing $\\rho_{i}$ \u2019s in closed form when keeping all $\\rho_{j}$ for $j\\neq i$ fixed. In particular, ", "page_idx": 3}, {"type": "text", "text": "Lemma 1. [Optimal Robust Variances] Let $\\mathcal{D}_{\\backslash i}\\,=\\,\\{(\\mathbf{x}_{j},y_{j})\\,:\\,j\\,\\neq\\,i\\}$ , $\\pmb{\\rho}=\\pmb{\\rho}_{\\backslash i}+\\rho_{i}\\mathbf{e}_{i}$ , where $\\rho,\\pmb{\\rho}_{\\setminus i}\\in\\mathbb{R}_{+}^{n}$ , $[\\rho_{\\backslash i}]_{i}=0$ , and $\\mathbf{e}_{i}$ is the ith canonical basis vector. Then keeping $\\rho_{\\setminus i}$ fixed, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\rho_{i}^{*}=\\arg\\operatorname*{max}_{\\rho_{i}}\\mathcal{L}\\big(\\rho_{\\setminus i}+\\rho_{i}\\mathbf{e}_{i}\\big)=\\left[(y_{i}-\\mathbb{E}[y(\\mathbf{x}_{i})|\\mathcal{D}_{\\setminus i}])^{2}-\\mathbb{V}[y(\\mathbf{x}_{i})|\\mathcal{D}_{\\setminus i}]\\right]_{+},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $y(\\mathbf{x}_{i})=f(\\mathbf{x}_{i})\\!+\\!\\epsilon_{i}$ . These quantities can be expressed as functions of $\\Sigma^{-1}=(\\mathbf{K}{+}\\mathbf{D}_{\\sigma^{2}{+}\\rho})^{-1}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{E}[y(\\mathbf{x}_{i})\\vert\\mathcal{D}_{\\backslash i}]^{2}=y_{i}-\\left[\\mathbf{\\Sigma}\\mathbf{\\Sigma}^{-1}\\mathbf{y}\\right]_{i}/\\left[\\mathbf{\\Sigma}\\mathbf{\\Sigma}^{-1}\\right]_{i i},\\qquad a n d\\qquad\\mathbb{V}[y(\\mathbf{x}_{i})\\vert\\mathcal{D}_{\\backslash i}]=1/\\left[\\mathbf{\\Sigma}\\mathbf{\\Sigma}^{-1}\\right]_{i i},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{D}_{\\sigma^{2}+\\rho}$ is a diagonal matrix whose entries are ${\\sigma}^{2}+{\\rho}$ . ", "page_idx": 3}, {"type": "text", "text": "The first component $\\mathbb{E}[f(\\mathbf{x}_{i})+\\epsilon_{i}|\\mathcal{D}_{\\backslash i}]^{2}$ of (3) is the empirical error to $y_{i}$ of the model trained without the $i$ -th data point, i.e., the leave-one-out (LOO) cross-validation error [49]. The second component $\\mathbb{V}[f(\\mathbf{x}_{i})+\\epsilon_{i}|D_{\\backslash i}]$ is the LOO predictive variance. The optimal solution to $\\rho_{i}$ is only non-zero for those observations whose squared LOO error is larger than the LOO predictive variance at that point. ", "page_idx": 3}, {"type": "text", "text": "4.2 Optimization with a Maximum Number of Outliers ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Without additional structure, inference of the noise variances $\\rho_{i}$ does not yield desirable models, as the marginal likelihood can be improved by increasing the prior variance $\\rho_{i}$ of any data point where Eq. (3) is greater than zero, even if that is due to regular (non-outlier) measurement noise. To avoid this, we constrain the number of non-zero $\\rho_{i}$ , that is, $\\|\\pmb{\\rho}\\|_{\\infty}=|\\{0<\\rho_{i}\\}|\\,\\le\\,k\\,<\\,n$ . While this sparsity constraint mitigates over-flexibility, it gives rise to a formidably challenging optimization problem, as there are a combinatorial number of sparse outlier sets to consider. Even if the number of outliers $n_{o}$ were known, exhaustive search would still require considering $n$ -choose- $n_{o}$ possibilities. ", "page_idx": 3}, {"type": "text", "text": "For tractability, we iteratively add data points to a set of potential \u201coutliers\u201d by setting their associated $\\rho_{i}$ to be nonzero, using the closed-form expression for the optimal individual $\\rho_{i}$ variances in Lemma 1. As the algorithm seeks to identify the most \u201crelevant\u201d data points (as measured by $\\mathcal{L}$ ) upon completion, we refer to it as Relevance Pursuit. This is Algorithm 1 with useBayesianModelSelection as false. Specifically, this is the \u201cforward\u201d variant; Algorithm 2 in the Appendix presents an alternative \u201cbackward\u201d variant that we found to work well if the number of corrupted data points is large. ", "page_idx": 4}, {"type": "text", "text": "Crucial to the performance of the optimizer, it never removes data from consideration completely; a data point is only down-weighted if it is apparently an outlier. This allows the down-weighting to be reversed if a data point appears \u201cinlying\u201d after having down-weighted other data points, improving the method\u2019s robustness and performance. This is in contrast to Andrade and Takeda [9]\u2019s greedy algorithm, in which the exclusion of a data point can both increase or decrease the associated marginal likelihood. This means that their objective is not monotonic, a necessary condition to provide constant-factor submodular approximation guarantees for greedy algorithms, see Section 5. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Relevance Pursuit (Forward Algorithm) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Require: $\\mathbf{X}$ , y, schedule ${\\cal K}=(k_{1},k_{2},\\ldots,k_{K})$ , useBayesianModelSelection (boolean) Initialize ${\\cal S}_{0}\\subseteq\\{1,\\ldots,n\\}$ (typically ${\\cal S}_{0}=\\emptyset$ ) for $i$ in $(1,\\ldots,|\\kappa|)$ do Optimize MLL: $\\begin{array}{r}{\\rho_{S_{i}}\\leftarrow\\arg\\operatorname*{max}_{\\rho_{S_{i}}}\\mathcal{L}\\left(\\rho_{S_{i}}\\right),\\ \\ \\mathrm{where}\\ \\rho_{S_{i}}=\\{\\rho:\\rho_{j}=0,\\ \\forall{\\,j\\notin S_{i}}\\}}\\end{array}$ . Expand Support: Compute $\\begin{array}{r}{\\Delta_{i}(j)\\gets\\operatorname*{max}_{\\rho_{j}}\\mathcal{L}(\\rho_{S_{i}}+\\rho_{j}\\mathbf{e}_{j})-\\mathcal{L}(\\rho_{S_{i}})}\\end{array}$ for all $j\\not\\in S_{i}$ via Lemma 1 . $\\begin{array}{r l}&{A_{i}\\leftarrow\\{j_{1},\\dots,j_{k_{i}}\\}}\\\\ &{S_{i+1}\\leftarrow S_{i}\\cup\\mathcal{A}_{i}}\\end{array}$ such that $\\Delta_{i}(j)\\geq\\Delta_{i}(j^{\\prime})$ for all $j\\in A_{i}$ and $j^{\\prime}\\not\\in\\left(A_{i}\\cup S_{i}\\right)$ . if useBayesianModelSelection then Compute the marginal likelihood $p({\\mathcal{D}}|S_{i})\\approx p({\\mathcal{D}}|S_{i},\\pmb{\\rho}_{S_{i}})$ $S^{*}\\gets\\arg\\operatorname*{max}_{S_{i}}\\bar{p}(\\mathcal{D}|S_{i})p(S_{i})$ . else $S^{*}=S_{\\cal K}$ . Return S\u2217, \u03c1S\u2217. ", "page_idx": 4}, {"type": "text", "text": "4.3 Automatic Outlier Detection via Bayesian Model Selection ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In practice, it is often impossible to set a hard threshold on the number of outliers for a particular data set. For example, a sensor might have a known failure rate, but how many outliers it produces will depend on the specific application of the sensor. Thus, is often more natural to specify a prior distribution $p(S)$ over the number of outliers, rather than fix the number a priori. We leverage the Bayesian model selection framework [61, 40] to determine the most probable number of outliers in a data- and model-dependent way, aiming to maximize $p(S|D)$ . This gives rise to Algorithm 1, with useBayesianModelSelction as true. ", "page_idx": 4}, {"type": "text", "text": "Computationally, we start by iteratively adding outliers up to the maximal support of the prior, similar to the procedure described in Section 4.2. We store a trace of models generated at each iteration, then approximate the model posterior $p(S_{i}|D)\\propto p(\\mathcal{D}|S_{i})p(S_{i})$ at each point in the trace. As the exact posterior is intractable, we approximate it with $\\begin{array}{r}{\\dot{p}(\\bar{\\mathcal{D}}|\\dot{S_{i}})\\,{=}\\,\\int p(\\bar{\\mathcal{D}}|S_{i},\\rho_{S_{i}})\\mathrm{d}\\rho_{S_{i}}\\approx\\,p(\\mathcal{D}|S_{i},\\rho_{S_{i}}^{*})}\\end{array}$ Finally, we select the model from the model trace $\\{S_{i}\\}_{i}$ that attains the highest model posterior likelihood. Imposing a prior on the number of outliers differs notably from most sparsity-inducing priors, which are instead defined on the parameter values, like $l_{1}$ -norm regularization. In practice, $p(S)$ can be informed by empirical distributions of outliers. For our experiments, we use an exponential prior on $|{\\cal S}|$ to encourage the selection of models that fit as much of the data as tightly as possible. ", "page_idx": 4}, {"type": "text", "text": "Regarding the schedule $\\kappa$ in Algorithm 1, the most natural choice is simply to add one data point at a time, i.e. $\\mathcal{K}=(1,1,...)$ , but this can be slow for large $n$ . In practice, we recommend schedules that test a fixed set of outlier fractions, e.g. $\\mathcal{K}=(0.05n,0.05n,\\dots)$ . ", "page_idx": 4}, {"type": "text", "text": "5 Theoretical Analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We now provide a theoretical analysis of our approach. We first propose a re-parameterization of the $\\rho_{i}$ that maps the optimization problem to a compact domain. Surprisingly, the re-parameterized problem exhibits strong convexity and smoothness when the base covariance matrix (excluding the $\\rho_{i}$ ) is well-conditioned. We connect the convexity and smoothness with existing results that yield approximation guarantees for sequential greedy algorithms, implying a constant-factor approximation guarantee to the optimal achievable NMLL value for generalized orthogonal matching pursuit (OMP), a greedy algorithm that is closely related to Algorithm 1. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "5.1 Preliminaries for Sparse Optimization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The optimization of linear models with respect to least-squares objectives in the presence of sparsity constraints has been richly studied in statistics [54], compressed sensing [3, 57], and machine learning [8, 62]. Of central importance to the theoretical study of this problem class are the eigenvalues of sub-matrices of the feature matrix, corresponding to sparse feature selections and so often referred to as sparse eigenvalues. The restricted isometry property (RIP) formalizes this. ", "page_idx": 5}, {"type": "text", "text": "Definition 2 (Restricted Isometry Property). $A n\\;(n\\times m)$ -matrix A satisfies the $r$ -restricted isometry property $\\left(R I P\\right)$ with constant $\\delta_{r}\\in(0,1)$ if for every submatrix ${\\bf A}_{\\mathcal{S}}$ with $|S|=r\\leq m$ columns, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(1-\\delta_{r})\\|\\mathbf{x}\\|\\leq\\|\\mathbf{A}_{S}\\mathbf{x}_{S}\\|\\leq(1+\\delta_{r})\\|\\mathbf{x}\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{x}_{S}\\in\\mathbb{R}^{r}$ . This is equivalent to $(1-\\delta_{r})\\mathbf{I}\\preceq(\\mathbf{A}_{S}^{*}\\mathbf{A}_{S})\\preceq(1-\\delta_{r})\\mathbf{I}.$ ", "page_idx": 5}, {"type": "text", "text": "The RIP has been proven to lead to exact recovery guarantees [14], as well as approximation guarantees [18]. Elenberg et al. [21] generalized the RIP to non-linear models and other data likelihoods, using the notion of restricted strong convexity (RSC) and restricted smoothness. ", "page_idx": 5}, {"type": "text", "text": "Definition 3 (Restricted Strong Convexity and Smoothness). A function $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ is $m_{r}$ -restricted strong convex and $M_{r}$ -restricted smooth if for all $(\\mathbf{x},\\mathbf{x}^{\\prime})$ in the domain $\\overset{D_{r}}{D_{r}}\\subset(\\mathbb{R}^{d}\\times\\mathbb{R}^{d}),$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\nm_{r}\\|\\mathbf{x}^{\\prime}-\\mathbf{x}\\|^{2}/2\\;\\leq\\;f(\\mathbf{x}^{\\prime})-f(\\mathbf{x})-\\nabla[f](\\mathbf{x})^{\\top}(\\mathbf{x}^{\\prime}-\\mathbf{x})\\;\\leq\\;M_{r}\\|\\mathbf{x}^{\\prime}-\\mathbf{x}\\|^{2}/2.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In the context of sparse optimization, we let $D_{r}$ be the set of tuples of $r$ -sparse vectors whose difference is also at most $r$ -sparse. In particular, $D_{r}=\\{({\\bf x},{\\bf x}^{\\prime})$ s.t. $||\\mathbf{x}||_{0};||\\mathbf{x}^{\\prime}||_{0},||\\mathbf{x}^{\\prime}-\\mathbf{x}||_{0}\\leq r\\}$ . ", "page_idx": 5}, {"type": "text", "text": "Generalized orthogonal matching pursuit (OMP) [4, 39] is a greedy algorithm that keeps track of a support set $\\boldsymbol{S}$ of non-zero coefficients, and expands the support based on the largest gradient magnitudes, applied to the marginal liklihood optimization problem, $S_{i+1}=S_{i}\\cup\\arg\\operatorname*{max}_{j\\notin S}|\\nabla_{\\rho}\\mathcal{L}(\\rho)|_{j}$ . Algorithm 1 generalizes OMP [57] by allowing more general support expansion schedules $\\kappa$ , and specializes the support expansion criterion using the special problem structure exposed by Lemma 1. ", "page_idx": 5}, {"type": "text", "text": "5.2 The Convex Parameterization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The NMLL $\\mathcal{L}$ of a GP (1) is the sum of a convex function $(\\cdot)^{-1}$ and a concave function $\\log\\operatorname*{det}(\\cdot)$ of $\\mathbf{K}$ , and is therefore not generally convex as a function of the hyper-parameters $\\pmb{\\theta}$ , including the robust variances $\\rho$ . Here, we propose a re-parameterization that allows us to prove strong convexity guarantees of the associated NMLL. In particular, we let $\\pmb{\\rho}(\\mathbf{s})=\\mathrm{diag}(\\mathbf{K}_{0})\\odot((1-\\mathbf{s})^{-1}-1)$ , where $\\mathbf{\\check{K}}_{0}:=k(\\mathbf{X},\\mathbf{X})+\\sigma^{2}\\mathbf{I}$ and the inverse is element-wise. Note that $\\rho(\\mathbf{s})$ is a diffeomorphism that maps s from the compact domain $\\mathbf{s}\\in[0,1]^{n}$ to the entire range of $\\rho\\in[0,\\infty]^{n}$ . ", "page_idx": 5}, {"type": "text", "text": "Henceforth, we refer to the original $\\rho$ as the canonical or $\\rho$ -parameterization and the newly proposed $\\rho(\\mathbf{s})$ as the convex or s-parameterization. Lemma 4 shows the Hessian of the s-parameterization. ", "page_idx": 5}, {"type": "text", "text": "Lemma 4. [Reparameterized Hessian] Let $\\begin{array}{r l r}{{\\bf K}_{\\bf s}}&{{}=}&{k({\\bf X},{\\bf X})\\;+\\;\\sigma^{2}{\\bf I}\\;+\\;{\\bf D}_{\\rho({\\bf s})}}\\end{array}$ , $\\begin{array}{r l}{\\hat{\\mathbf{K}}_{\\mathbf{s}}}&{{}=}\\end{array}$ $\\mathrm{diag}(\\mathbf{K}_{\\mathbf{s}})^{-1/2}\\mathbf{K}_{\\mathbf{s}}\\,\\mathrm{diag}(\\mathbf{K}_{\\mathbf{s}})^{-1/2}$ , and $\\hat{\\pmb{\\alpha}}=\\hat{\\mathbf{K}}_{\\mathbf{s}}^{-1}\\,\\mathrm{diag}(\\mathbf{K}_{\\mathbf{s}})^{-1/2}\\mathbf{y}$ . Then ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{H}_{\\mathrm{s}}[-2\\mathcal{L}(\\rho(\\mathbf{s})]=\\mathbf{D}_{1-\\mathrm{s}}^{-1}\\left[2\\left(\\hat{\\alpha}\\hat{\\alpha}^{\\top}\\odot(\\hat{\\mathbf{K}}^{-1}-\\mathbf{I})\\right)+2\\operatorname{diag}(\\hat{\\mathbf{K}}^{-1})-(\\hat{\\mathbf{K}}^{-1}\\odot\\hat{\\mathbf{K}}^{-1})\\right]\\mathbf{D}_{1-\\mathrm{s}}^{-1}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Based on this representation, we now derive conditions on the eigenvalues of $\\hat{\\bf K}$ that imply the $m$ -strong convexity and $M$ -smoothness of the NMLL. ", "page_idx": 5}, {"type": "text", "text": "Lemma 5. [Strong Convexity via Eigenvalue Condition] Let $\\hat{\\mathbf{K}}_{\\mathbf{s}}$ as in Lemma 4. Then $\\mathbf{H_{s}}\\succ m\\:i f$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{min}}\\hat{\\lambda}_{\\operatorname*{min}}^{2}\\frac{(2\\hat{\\lambda}_{\\operatorname*{max}}^{-1}-\\hat{\\lambda}_{\\operatorname*{min}}^{-2}-m)}{2(1-\\lambda_{\\operatorname*{min}}/\\lambda_{\\operatorname*{max}})}>\\|\\mathbf{y}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\lambda_{\\mathrm{min,max}}$ (resp. $\\hat{\\lambda}_{\\mathrm{min,max}})$ are the smallest and largest eigenvalues of $\\bf{K_{s}}$ , respectively $\\hat{\\mathbf{K}}_{\\mathbf{s}}$ . ", "page_idx": 5}, {"type": "image", "img_path": "5FATPIlWUJ/tmp/9feca439f3e35816693b02b57ebd1a0877838cfc2ce6fe6ed42748ace11682cf.jpg", "img_caption": ["Figure 3: Top: The behavior of the $-\\log{\\mathcal{L}}(\\rho)$ with respect to the canonical parameterization of $\\rho$ . Bottom: The behavior of $-\\log{\\mathcal{L}}(\\rho(\\mathbf{s}))$ , highlighting the convexity property. Left: The value, and first two derivatives of $-\\log{\\mathcal{L}}$ for a 1d example. Center: The second derivatives of a $\\mathrm{{ld}}-\\log{\\mathcal{L}}$ as a function of $|y|$ . The s-parameterization is everwhere convex for all considered $|y|$ , while the canonical $\\rho$ -parameterization is only convex around the origin and only for $|y|>0.5$ . Right: The heatmaps highlight that the original parameterization is non-convex (red) for larger values of $\\rho$ , and quickly becomes ill-conditioned, whereas the parameterization $\\rho(\\mathbf{s})$ is convex and much better conditioned. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "The behavior Lemma 5 predicts is surprising and validated in Fig. 3. Notably, the denominator \u201cblows up\u201d as $\\mathbf{K}$ becomes close to unitary, making the inequality more likely to be satisfied, an indication that the convexity property of the NMLL is intimately linked to the RIP (Def. 2). Note that Lemma 5 is a condition for non-support-restricted convexity, which is stronger than is necessary for the approximation guarantees that rely on restricted convexity (Def. 3). However, sparse eigenvalues are generally difficult to compute exactly. Fortunately, covariance matrices of GPs naturally tend to exhibit a property that facilitates a different sufficient condition for convexity for all $\\mathbf{s}\\in[0,1]^{n}$ . ", "page_idx": 6}, {"type": "text", "text": "Definition 6 (Diagonal Dominance). A matrix A is said to be $\\delta$ -diagonally dominant if the elements $a_{i j}$ satisfy $\\begin{array}{r}{\\sum_{i\\neq j}|a_{i j}|<\\delta|a_{i i}|}\\end{array}$ for all $i$ . ", "page_idx": 6}, {"type": "text", "text": "Intuitively, the $\\rho_{i}(s)$ that are selected to be non-zero by the greedy algorithm take on large values, further encouraging the diagonal dominance of the sub-matrix of $\\mathbf{K}$ associated with the support of $\\rho$ . For this reason, the following condition on $\\mathbf{K}_{0}$ is sufficient to guarantee convexity for all $\\mathbf{s}\\in[0,1]^{n}$ . ", "page_idx": 6}, {"type": "text", "text": "Lemma 7. [Strong Con\u221avexity via Diagonal Dominanc\u221ae] Let $m>0$ and $\\mathbf{K}_{0}$ be $\\delta$ -diagonally dominant with $\\delta<\\left((5-m)-\\sqrt{25-9m+17}\\right)/4\\leq(5-\\sqrt{17})/4\\approx0.44$ and ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{min}}(\\mathbf{K}_{0})(1-\\delta)^{2}\\frac{2(1+\\delta)^{-1}-(1-\\delta)^{-2}-m}{2(1-(1-\\delta)/(1+\\delta))}\\geq\\|\\mathbf{y}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Then the NMLL is $m$ -strongly convex for all $\\mathbf{s}\\in[0,1]^{n}$ , i.e. $\\rho(\\mathbf{s})\\in[0,\\infty]^{n}$ . ", "page_idx": 6}, {"type": "text", "text": "We attain similar results for $M$ -smoothness, see Lemma 13 and Lemma 14 in the Appendix. Having proven $m$ -convexity and $M$ -smoothness conditions, we appeal to the results of Elenberg et al. [21]. ", "page_idx": 6}, {"type": "text", "text": "Theorem 8. [Approximation Guarantee] Let ${\\bf K}_{0}\\;=\\;k({\\bf X},{\\bf X})+\\sigma^{2}{\\bf I}$ be $\\delta$ -diagonally dominant, $s_{\\mathrm{max}}>0$ be an upper bound on $\\|\\mathbf{s}\\|_{\\infty}$ , and suppose $\\|\\mathbf{y}\\|$ , $\\delta$ satisfy the bounds of Lemmas 7 and $^{l4}$ , guaranteeing $m$ -convexity and $M$ -smoothness of the NMLL for some $m>0$ , $\\dot{M}>1/(1-s_{\\mathrm{max}})^{2}$ . Let $\\mathbf{s}_{\\mathrm{OMP}}(r)$ be the $r$ -sparse vector attained by OMP on the NMLL objective for $r$ steps, and let $\\begin{array}{r}{\\mathbf{s}_{\\mathrm{OPT}}(r)=\\arg\\operatorname*{max}_{\\|\\mathbf{s}\\|_{0}=r,\\|\\mathbf{s}\\|_{\\infty}\\leq s_{\\operatorname*{max}}}\\mathcal{L}(\\rho(\\mathbf{s}))}\\end{array}$ be the optimal $r$ -sparse vector. Then for any $2r\\leq n_{!}$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{L}}\\left(\\rho({\\mathbf{s}}_{\\mathrm{OMP}}(r))\\right)\\;\\geq\\;\\left(1-e^{-m/M}\\right)\\,\\tilde{\\mathcal{L}}\\left(\\rho({\\mathbf{s}}_{\\mathrm{OPT}}(r))\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\tilde{\\mathcal{L}}(\\cdot)=\\mathcal{L}(\\cdot)-\\mathcal{L}(\\mathbf{0})$ is normalized so that $\\operatorname*{max}_{\\mathbf{s}_{S}}\\tilde{\\mathcal{L}}(\\mathbf{s}_{S})\\geq0$ for any support $\\boldsymbol{S}$ . ", "page_idx": 6}, {"type": "text", "text": "A limitation of the theory is that it assumes the other hyper-parameters of the GP model to be constant, as doing otherwise would introduce the non-convexity that is common to most marginal likelihood optimization problems. In practice, we typically optimize $\\rho$ jointly with the other hyper-parameters of the model in each iteration of RRP, as this yields improved performance, see App. D.5 for details. ", "page_idx": 7}, {"type": "text", "text": "6 Empirical Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We evaluate the empirical performance of RRP against various baselines on a number of regression and Bayesian Optimization problems. Specifically, we compare against a standard GP with a Matern5/2 kernel (\u201cStandard GP\u201d), data pre-processing through Ax\u2019s adaptive winsorization procedure (\u201cAdapt. Wins.\u201d) [11], and a power transformation (\u201cPower Transf.\u201d) [13]. Further, we also consider a Student- $\\cdot t$ likelihood model from Jyl\u00e4nki et al. [30] (\u201cStudent- $\\cdot t^{\\bullet}$ ), the trimmed marginal likelihood model from Andrade and Takeda [9] (\u201cTrimmed MLL\u201d), and the RCGP model from Altamirano et al. [2]. Unless stated otherwise, all models are implemented in GPyTorch [23] and all experiments in this section use 32 replications. See Appendix D for additional details. ", "page_idx": 7}, {"type": "text", "text": "6.1 Regression Problems ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Synthetic We first consider the popular Friedman10 and Hartmann6 [19] test functions from the literature. We use two data generating processes: uniform noise, extreme outliers at some fixed value, and heavy-tailed (Student-t) noise at true function values. In these experiments, we compare the performance predictive log-likelihood. The results are shown in Fig. 4. ", "page_idx": 7}, {"type": "image", "img_path": "5FATPIlWUJ/tmp/b6fa52555769309637f58e178041d8c063a12c2609fa4593033c62e45392a6ed.jpg", "img_caption": ["Figure 4: Left: Distribution of predictive test-set log likelihood for various methods. Methods ommitted are those that performed substantially worse. Right: Predictive log likelihood as a function of the corruption probability for Student- $t$ -distributed corruptions with two degrees of freedom. The GP model with the Student- $\\cdot t$ likelihood only starts outperforming RRP as the corruption probability increases beyond $40\\%$ , and exhibits a large variance in outcomes, which shrinks as the proportion of corruptions increases. All methods not shown were inferior to either RRP or Student- $\\boldsymbol{\\cdot}$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Twitter Flash Crash In Fig. 5, we report a comparison to Altamirano et al. [2]\u2019s RCGP on data from the Dow Jones Industrial Average (DJIA) index on April 22-23 2013, which includes a sharp drop at 13:10 on the 23rd. The top panels shows that RCGP exhibits higher robustness than the standard GP, but is still affected by the outliers, when trained on data from the 23rd. RRP is virtually unaffected. Notably, RCGP relies on an a-priori weighting of data points based on the target values\u2019 proximity to their median, which can be counter-productive when the outliers are not a-priori separated in the range. To highlight this, we included the previous trading day into the training data for the bottom panels, leading RCGP to assign the highest weight to the outlying data points due to their proximity to the target values\u2019 median, thereby leading RCGP to \u201ctrust\u201d the outliers more than any inlier, resulting in it being less robust than a standard GP in this scenario. See Appendix D.6 for additional comparisons to RCGP, on data sets from the UCI machine learning repository [32]. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "5FATPIlWUJ/tmp/84a7e18ceb149b463c647f793711940b80bb824b42b60da186712d04e92e61f0.jpg", "img_caption": ["Figure 5: Results on the intra-day data from the Dow Jones Industrial Average (DJIA) index on April 22-23 2013, which includes a sharp drop at 13:10 on the 23rd, see (b) for a detailed view. The accompanying panels labeled $w_{\\mathrm{imq}}$ show the function that Altamirano et al. [2]\u2019s RCGP uses to down-weight data points. Top: RCGP, exhibits higher robustness than the standard GP, but is still affected by the outliers. The RRP model is virtually unaffected. Bottom: Including the previous trading day into the training data in (c), leads RCGP to assign the highest weight $w_{\\mathrm{imq}}$ to the outlying data points due to their proximity to the target values\u2019 median, thereby leading RCGP to be even more affected than a standard GP, see (d) for a detailed view of the results on the data of April 23. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "6.2 Robust Bayesian Optimization ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "GPs are commonly used for Bayesian optimization (BO), which is a popular approach to sampleefficient black-box optimization [22]. However, many of the GP models used for BO are sensitive to outliers and may not perform well in settings where such outliers occur. While Martinez-Cantin et al. [41] consider the use of a Student- $t$ likelihood for BO with outliers, the use of other robust GP models has not been thoroughly studied in the literature. ", "page_idx": 8}, {"type": "text", "text": "Experimental setup We use Ament et al. [7]\u2019s qLogNoisyExpectedImprovement (qLogNEI), a variant of the LogEI family of acquisition functions, 32 replications, and initialize all methods with the same quasi-random Sobol batch for each replication. We follow Hvarfner et al. [29] and plot the true value of the best in-sample point according to the GP model posterior at each iteration. We also include Sobol and an \u201cOracle\u201d, which is a Standard GP that always observes the uncorrupted value, and consider the backward canonical version of relevance pursuit, denoted by RRP, for these experiments. The plots show the mean performance with a bootstrapped $90\\%$ confidence interval. ", "page_idx": 8}, {"type": "text", "text": "Synthetic problems We consider the popular 6-dimensional Hartmann test function with three different corruption settings: (1) a constant value of 100, (2) a $U[-3,3]$ distributed value, (3) the objective value for a randomly chosen point in the domain. The results for a $10\\%$ corruption probability are shown in Fig. 6. We also include results for a $20\\%$ corruption probability in Appendix D.3. ", "page_idx": 8}, {"type": "image", "img_path": "5FATPIlWUJ/tmp/c1636a51e2d0b05f08a6aa3f542fcee28f8269ffa8fb662bcb094ab4510d7ad9.jpg", "img_caption": ["Figure 6: BO results for Hartmann6: Left: Relevance pursuit performs well in the case of constant outliers of value 100, almost as well as the oracle. Middle: Relevance pursuit performs the best followed by the Student- $\\cdot t$ likelihood in the case of $U[-3,3]$ . Right: Similar to the middle plot, this setting hides the corruptions within the range of the function, making it a challenging task. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Real-world problems We include three real-world problems: A 3D SVM problem, a 5D CNN problem, and a 20D rover trajectory planning problem, see the App. D.2 for details. For SVM and CNN, we simulate random corruptions corresponding to an I/O error, which causes the corresponding ML model to be trained using only a small subset of the training data. For the rover planning problem we follow the setup in [42] with the main difference that we consider a 20D trajectory, and the corruptions are generated randomly, causing the rover to break down at an arbitrary point along its trajectory. In most cases, this results in a smaller reward than the reward of the full trajectory. ", "page_idx": 9}, {"type": "image", "img_path": "5FATPIlWUJ/tmp/1bac2466bddb08142ba1cff0704556456798e4a60c47ef5b607bf8bcd362a633.jpg", "img_caption": ["Figure 7: BO results for three real-world problems: Left: RRP is competitive with the oracle on the 3D SVM problem. Middle: The power transform performs best on the 5D CNN problem, outperforming RRP as well as the Oracle. Right: RRP performs well on the 20D Rover problem. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "7 Conclusion and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Contributions Robust Gaussian Processes via Relevance Pursuit (RRP) provides a novel and principled way to perform robust GP regression. It permits efficient and robust inference, performs well across a variety of label corruption settings, retains good performance in the absence of corruptions, and is flexible, e.g., can be used with any mean or kernel function. Our method can be readily applied to both robust regression problems as well as applications such as Bayesian optimization and is available through BoTorch [12]. Importantly, it also provides theoretical approximation guarantees. ", "page_idx": 9}, {"type": "text", "text": "Limitations As our approach does not explicitly consider the locations of the data points in the outlier identification, it may be outperformed by other methods if the underlying noise is heteroskedastic and location-dependent. On the other hand, those methods generally do not perform well in the presence of sparse, location-independent data corruptions. ", "page_idx": 9}, {"type": "text", "text": "Extensions Promising extensions of this work include performing Bayesian model averaging, i.e., average the predictions of the different possible sparsity models according to their likelihoods instead of using a MAP estimate, applying RRP to specialized models such as Lin et al. [37]\u2019s scalable learning-curve model for AutoML applications, and leveraging forward-backward greedy algorithms. On a higher level, the approach of combining greedy optimization algorithms with Bayesian model selection and leveraging a convex parameterization to achieve approximation guarantees might apply to other parameters that are optimized using the MLL objective: length-scales of stationary kernels, coefficients of additive kernels, inducing inputs, and even related model classes like Tipping [55]\u2019s Sparse Bayesian Learning (SBL), which seeks to identify sparse linear models and is intimately linked to greedy matching pursuits [8]. Overall, the approach has the potential to lead to theoretical guarantees, new insights, and performance improvements to widely-adopted Bayesian models. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] P. Algikar and L. Mili. Robust gaussian process regression with huber likelihood, 2023. URL https://arxiv.org/abs/2301.07858.   \n[2] M. Altamirano, F.-X. Briol, and J. Knoblauch. Robust and conjugate gaussian process regression. arXiv preprint arXiv:2311.00463, 2023.   \n[3] S. Ament and C. Gomes. On the optimality of backward regression: Sparse recovery and subset selection. In ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5599\u20135603, 2021. doi: 10.1109/ICASSP39728.2021. 9415082.   \n[4] S. Ament and C. Gomes. Generalized matching pursuits for the sparse optimization of separable objectives. In ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5553\u20135557, 2022. doi: 10.1109/ICASSP43922.2022. 9747510.   \n[5] S. Ament and M. O\u2019Neil. Accurate and efficient numerical calculation of stable densities via optimized quadrature and asymptotics. Statistics and Computing, 28:171\u2013185, 2018.   \n[6] S. Ament, M. Amsler, D. R. Sutherland, M.-C. Chang, D. Guevarra, A. B. Connolly, J. M. Gregoire, M. O. Thompson, C. P. Gomes, and R. B. van Dover. Autonomous materials synthesis via hierarchical active learning of nonequilibrium phase diagrams. Science Advances, 7(51): eabg4930, 2021. doi: 10.1126/sciadv.abg4930. URL https://www.science.org/doi/abs/ 10.1126/sciadv.abg4930.   \n[7] S. Ament, S. Daulton, D. Eriksson, M. Balandat, and E. Bakshy. Unexpected improvements to expected improvement for bayesian optimization. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 20577\u201320612. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ 419f72cbd568ad62183f8132a3605a2a-Paper-Conference.pdf.   \n[8] S. E. Ament and C. P. Gomes. Sparse bayesian learning via stepwise regression. In M. Meila and T. Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 264\u2013274. PMLR, 18\u201324 Jul 2021. URL https://proceedings.mlr.press/v139/ament21a.html.   \n[9] D. Andrade and A. Takeda. Robust Gaussian process regression with the trimmed marginal likelihood. In R. J. Evans and I. Shpitser, editors, Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence, volume 216 of Proceedings of Machine Learning Research, pages 67\u201376. PMLR, 31 Jul\u201304 Aug 2023. URL https://proceedings.mlr. press/v216/andrade23a.html.   \n[10] P. Awasthi, A. Das, W. Kong, and R. Sen. Trimmed maximum likelihood estimation for robust generalized linear model. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum? id $\\cdot^{=}$ VHmdFPy4U_u.   \n[11] E. Bakshy, L. Dworkin, B. Karrer, K. Kashin, B. Letham, A. Murthy, and S. Singh. Ae: A domain-agnostic platform for adaptive experimentation. In NeurIPS Workshop on Systems for Machine Learning, 2018.   \n[12] M. Balandat, B. Karrer, D. R. Jiang, S. Daulton, B. Letham, A. G. Wilson, and E. Bakshy. BoTorch: A Framework for Efficient Monte-Carlo Bayesian Optimization. In Advances in Neural Information Processing Systems 33, 2020.   \n[13] G. E. Box and D. R. Cox. An analysis of transformations. Journal of the Royal Statistical Society Series B: Statistical Methodology, 26(2):211\u2013243, 1964.   \n[14] E. J. Candes. The restricted isometry property and its implications for compressed sensing. Comptes rendus. Mathematique, 346(9-10):589\u2013592, 2008.   \n[15] X. Chu, I. F. Ilyas, S. Krishnan, and J. Wang. Data cleaning: Overview and emerging challenges. In Proceedings of the 2016 international conference on management of data, pages 2201\u20132206, 2016.   \n[16] P. Congdon. Representing spatial dependence and spatial discontinuity in ecological epidemiology: a scale mixture approach. Stochastic Environmental Research and Risk Assessment, 31(2): 291\u2013304, 2017.   \n[17] A. Daemi, H. Kodamana, and B. Huang. Gaussian process modelling with gaussian mixture likelihood. Journal of Process Control, 81:209\u2013220, 2019.   \n[18] A. Das and D. Kempe. Approximate submodularity and its applications: Subset selection, sparse approximation and dictionary selection. Journal of Machine Learning Research, 19(3): 1\u201334, 2018.   \n[19] L. C. W. Dixon. The global optimization problem: an introduction. Towards Global Optimiation 2, pages 1\u201315, 1978.   \n[20] K. Dong, D. Eriksson, H. Nickisch, D. Bindel, and A. G. Wilson. Scalable log determinants for gaussian process kernel learning. Advances in Neural Information Processing Systems, 30, 2017.   \n[21] E. R. Elenberg, R. Khanna, A. G. Dimakis, and S. Negahban. Restricted strong convexity implies weak submodularity. The Annals of Statistics, 46(6B):3539\u20133568, 2018.   \n[22] P. I. Frazier. A tutorial on bayesian optimization. arXiv preprint arXiv:1807.02811, 2018.   \n[23] J. R. Gardner, G. Pleiss, D. Bindel, K. Q. Weinberger, and A. G. Wilson. GPyTorch: Blackbox Matrix-matrix Gaussian Process Inference with GPU Acceleration. In Proceedings of the 32Nd International Conference on Neural Information Processing Systems, NIPS\u201918, pages 7587\u20137597, USA, 2018. Curran Associates Inc.   \n[24] R. Garnett. Bayesian Optimization. Cambridge University Press, 2023. to appear.   \n[25] J. Gerritsma, R. Onnink, and A. Versluis. Yacht hydrodynamics. UCI Machine Learning Repository, 1981. DOI: https://doi.org/10.24432/C5XG7R.   \n[26] P. W. Goldberg, C. K. I. Williams, and C. Bishop. Regression with input-dependent noise: A gaussian process treatment. In Advances in Neural Information Processing Systems, volume 10, January 1998.   \n[27] J. Harrison, David and D. L. Rubinfeld. Hedonic prices and the demand for clean air. Journal of Environmental Economics and Management, 5:81\u2013102, 1978.   \n[28] R. A. Horn and C. R. Johnson. Matrix analysis. Cambridge university press, 2012.   \n[29] C. Hvarfner, E. Hellsten, F. Hutter, and L. Nardi. Self-correcting bayesian optimization through bayesian active learning. Advances in Neural Information Processing Systems, 36, 2024.   \n[30] P. Jyl\u00e4nki, J. Vanhatalo, and A. Vehtari. Robust gaussian process regression with a student-t likelihood. Journal of Machine Learning Research, 12(11), 2011.   \n[31] O. Karaca, D. Tihanyi, and M. Kamgarpour. Performance guarantees of forward and reverse greedy algorithms for minimizing nonsupermodular nonsubmodular functions on a matroid. Operations Research Letters, 49(6):855\u2013861, 2021.   \n[32] M. Kelly, R. Longjohn, and K. Nottingham. The uci machine learning repository. URL https://archive. ics. uci. edu, 2023.   \n[33] K. Kersting, C. Plagemann, P. Pfaff, and W. Burgard. Most likely heteroscedastic gaussian process regression. In Proceedings of the 24th International Conference on Machine Learning, ICML \u201907, pages 393\u2013400, 2007.   \n[34] A. Krause, A. Singh, and C. Guestrin. Near-optimal sensor placements in gaussian processes: Theory, efficient algorithms and empirical studies. Journal of Machine Learning Research, 9(2), 2008.   \n[35] M. Kuss. Gaussian process models for robust regressi\u00cfon, classification, and reinforcement learning. PhD thesis, Technische Universit\u00e4t Darmstadt, 2006.   \n[36] Z.-Z. Li, L. Li, and Z. Shao. Robust gaussian process regression based on iterative trimming. Astronomy and Computing, 36:100483, 2021. ISSN 2213-1337. doi: https://doi.org/10.1016/ j.ascom.2021.100483. URL https://www.sciencedirect.com/science/article/pii/ S2213133721000378.   \n[37] J. A. Lin, S. Ament, M. Balandat, and E. Bakshy. Scaling gaussian processes for learning curve prediction via latent kronecker structure, 2024. URL https://arxiv.org/abs/2410.09239.   \n[38] D. C. Liu and J. Nocedal. On the limited memory bfgs method for large scale optimization. Math. Program., 45(1\u20133):503\u2013528, Aug. 1989. ISSN 0025-5610.   \n[39] F. Locatello, R. Khanna, M. Tschannen, and M. Jaggi. A Unified Optimization View on Generalized Matching Pursuit and Frank-Wolfe. In A. Singh and J. Zhu, editors, Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, volume 54 of Proceedings of Machine Learning Research, pages 860\u2013868. PMLR, 20\u201322 Apr 2017. URL https://proceedings.mlr.press/v54/locatello17a.html.   \n[40] S. Lotf,i P. Izmailov, G. Benton, M. Goldblum, and A. G. Wilson. Bayesian model selection, the marginal likelihood, and generalization, 2023. URL https://arxiv.org/abs/2202.11678.   \n[41] R. Martinez-Cantin, M. McCourt, and K. Tee. Robust bayesian optimization with student-t likelihood, 2017.   \n[42] N. Maus, K. Wu, D. Eriksson, and J. Gardner. Discovering many diverse solutions with Bayesian optimization. In Proceedings of the International Conference on Artificial Intelligence and Statistics, volume 206, pages 1779\u20131798. PMLR, Apr. 2023.   \n[43] A. Naish-Guzman and S. Holden. Robust regression with twinned gaussian processes. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems, volume 20. Curran Associates, Inc., 2007. URL https://proceedings.neurips.cc/ paper_files/paper/2007/file/2ab56412b1163ee131e1246da0955bd1-Paper.pdf.   \n[44] R. M. Neal. Monte Carlo Implementation of Gaussian Process Models for Bayesian Regression and Classification. Technical Report 9702, Department of Statistics, University of Toronto, 1997.   \n[45] G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An analysis of approximations for maximizing submodular set functions\u2014i. Mathematical programming, 14:265\u2013294, 1978.   \n[46] R. K. Pace and R. Barry. Sparse spatial autoregressions. Statistics & Probability Letters, 33(3): 291\u2013297, 1997.   \n[47] C. Park, D. J. Borth, N. S. Wilson, C. N. Hunter, and F. J. Friedersdorf. Robust gaussian process regression with a bias model. Pattern Recognition, 124:108444, 2022.   \n[48] R. Ranjan, B. Huang, and A. Fatehi. Robust gaussian process modeling using em algorithm. Journal of Process Control, 42:125\u2013136, 2016.   \n[49] C. E. Rasmussen, C. K. Williams, et al. Gaussian processes for machine learning, volume 1. Springer, 2006.   \n[50] C. Riis, F. Antunes, F. H\u00fcttel, C. Lima Azevedo, and F. Pereira. Bayesian active learning with fully bayesian gaussian processes. Advances in Neural Information Processing Systems, 35: 12141\u201312153, 2022.   \n[51] A. Shah, A. Wilson, and Z. Ghahramani. Student-t processes as alternatives to gaussian processes. In Artificial intelligence and statistics, pages 877\u2013885. PMLR, 2014.   \n[52] N. Srinivas, A. Krause, S. Kakade, and M. Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In Proceedings of the 27th International Conference on International Conference on Machine Learning, ICML\u201910, page 1015\u20131022, Madison, WI, USA, 2010. Omnipress. ISBN 9781605589077.   \n[53] S. Sundararajan and S. Keerthi. Predictive approaches for choosing hyperparameters in gaussian processes. In S. Solla, T. Leen, and K. M\u00fcller, editors, Advances in Neural Information Processing Systems, volume 12. MIT Press, 1999. URL https://proceedings.neurips.cc/ paper_files/paper/1999/file/e8fd4a8a5bab2b3785d794ab51fef55c-Paper.pdf.   \n[54] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society Series B: Statistical Methodology, 58(1):267\u2013288, 1996.   \n[55] M. E. Tipping. Sparse bayesian learning and the relevance vector machine. Journal of machine learning research, 1(Jun):211\u2013244, 2001.   \n[56] M. E. Tipping and N. D. Lawrence. Variational inference for student-t models: Robust bayesian interpolation and generalised component analysis. Neurocomputing, 69(1):123\u2013141, 2005.   \n[57] J. A. Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Transactions on Information theory, 50(10):2231\u20132242, 2004.   \n[58] A. Tsanas and A. Xifara. Energy Efficiency. UCI Machine Learning Repository, 2012. DOI: https://doi.org/10.24432/C51307.   \n[59] J. Vanhatalo, P. Jyl\u00e4nki, and A. Vehtari. Gaussian process regression with student-t likelihood. In Advances in Neural Information Processing Systems, volume 22, 2009.   \n[60] Z. Wang, C. Gehring, P. Kohli, and S. Jegelka. Batched large-scale bayesian optimization in high-dimensional spaces. In Proceedings of the International Conference on Artificial Intelligence and Statistics, volume 84 of PMLR, pages 745\u2013754. JMLR, Mar. 2018.   \n[61] L. Wasserman. Bayesian model selection and model averaging. Journal of mathematical psychology, 44(1):92\u2013107, 2000.   \n[62] D. Wipf and S. Nagarajan. A new view of automatic relevance determination. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems, volume 20. Curran Associates, Inc., 2007. URL https://proceedings.neurips.cc/ paper_files/paper/2007/file/9c01802ddb981e6bcfbec0f0516b8e35-Paper.pdf.   \n[63] I.-C. Yeh. Concrete Compressive Strength. UCI Machine Learning Repository, 1998. DOI: https://doi.org/10.24432/C5PK67. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Additional Details on the Model ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Algorithm 2 below is the \u201cbackward\u201d variant of Algorithm 1 from Sec 4. As its name suggests, the main difference compared to the \u201cforward\u201d variant is that rather than building up a set of \u201coutliers\u201d, it starts from a (typically large) set of \u201coutliers\u201d and iteratively removes those data points from the set that have the smallest inferred data-point-dependent noise variance $\\rho_{i}$ . ", "page_idx": 14}, {"type": "text", "text": "While we have not derived theoretical guarantees for this \u201cbackward\u201d version, we have found it to generally behave similarly to the \u201cforward\u201d version in terms of performance and robustness. One empirical observation from our studies is that while the \u201cforward\u201d version tends to perform slightly better than the \u201cbackward\u201d version if there are only few outliers, the opposite is true if the outlier frequency is very high. This behavior is rather intuitive and illustrates that relevance pursuit is particularly well-suited to identify sparse, low-cardinality subsets (note that in the \u201cbackward\u201d variant under large corruptions, the uncorrupted data points can be viewed as the sparse subset that needs to be identified). ", "page_idx": 14}, {"type": "text", "text": "Algorithm 2 Relevance Pursuit (Backward Algorithm)   \nRequire: $\\mathbf{X}$ , y, schedule $\\cal{K}=(k_{1},k_{2},\\ldots)$ Initialize ${\\cal S}_{0}^{c}\\subseteq\\{1,\\ldots,n\\}$ (typically $S_{0}^{c}=\\{1,\\dots,n\\}\\rangle$ ) for $k_{i}$ in $\\kappa$ do Optimize ML: $\\pmb{\\rho}_{S_{i}^{c}}^{*}\\leftarrow\\arg\\operatorname*{max}_{\\pmb{\\rho}_{S_{i}^{c}}}\\mathcal{L}(\\pmb{\\rho}_{S_{i}^{c}})$ , where $\\pmb{\\rho}_{S_{i}^{c}}=\\{\\pmb{\\rho}:\\rho_{j}=0,\\,\\forall\\,j\\notin\\mathcal{S}_{i}^{c}\\}$ Compute the set $\\mathcal{R}_{i}$ containing the $k_{i}$ elements of $S_{i}^{c}$ with smallest inferred variance: $\\mathcal{R}_{i}\\leftarrow\\{j_{i}^{1},\\ldots,j_{i}^{k_{i}}\\}$ where $j_{i}^{l}\\in S_{i}^{c}$ such that $\\rho_{S_{i}^{c}}^{*}(j_{i}^{l})\\leq\\rho_{S_{i}^{c}}^{*}(j_{i}^{l^{\\prime}})$ for $l<l^{\\prime}$ $S_{i+1}^{c}\\leftarrow S_{i}^{c}\\setminus\\mathcal{R}_{i}$ $S_{i}\\leftarrow\\{1,\\ldots,n\\}\\setminus S_{i}^{c}$ for each $k_{i}$ Compute the marginal likelihood $p(S_{i}|\\mathbf{X},\\mathbf{y})\\approx p(S_{i},\\pmb{\\rho}_{S_{i}}^{*},|\\mathbf{X},\\mathbf{y})$ $S^{*}\\gets\\arg\\operatorname*{max}_{S_{i}}p(S_{i}|\\mathbf{X},\\mathbf{y})p(S_{i})$ . Return $S^{*}$ , $\\rho_{S^{*}}^{*}$ . ", "page_idx": 14}, {"type": "text", "text": "B Additional Background on the Theory ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Submodular Functions ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Das and Kempe [18] showed that the subset selection problem of regression features with an $R^{2}$ objective satisfies a weak submodularity property, which can be invoked to prove approximation guarantees for the greedy maximization of the objective. Elenberg et al. [21] generalized this work by proving that any log likelihood function exhibiting restricted strong concavity gives rise to the weak submodularity of the associated subset selection problem, which can be invoked to prove approximation guarantees for the greedy algorithm. Karaca et al. [31] contains a guarantee for the backward algorithm applied to the maximization of submodular set functions. ", "page_idx": 14}, {"type": "text", "text": "Submodularity has also found application to Gaussian process models. For a sensor placement problem, Krause et al. [34] proved that the mutual information (MI) criterion, capturing the reduction in uncertainty in the entire search space, can be a submodular function. In this case, MI is not monotonic everywhere, but monotonic for small sets $(2k)$ of sensors, which is sufficient to apply Nemhauser\u2019s guarantee for sparse sets of sensors up to size $k$ [45]. Srinivas et al. [52] used the submodularity of the joint entropy in order to prove regret bounds for the convergence of a GP-based BO algorithm using the upper-confidence bound acquisition function. ", "page_idx": 14}, {"type": "text", "text": "Elenberg et al. [21] proved that any log likelihood function exhibiting restricted strong concavity and smoothness implies the weak submodularity of the associated subset selection problem. ", "page_idx": 14}, {"type": "text", "text": "Definition 9 (Submodularity Ratios [21]). Let ${\\mathcal{A}},{\\mathcal{B}}\\subset[n]$ be two disjoint sets, and $f:2^{[n]}\\to\\mathbb{R}$ . The submodularity ratio of $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ with respect to $\\boldsymbol{\\mathcal{A}}$ is defined by ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\gamma_{\\mathcal{B},A}=\\sum_{i\\in\\mathcal{A}}\\left(f(\\mathcal{B}\\cup\\left\\{i\\right\\})-f(\\mathcal{B})\\right)\\left/\\left(f(\\mathcal{B}\\cup\\mathcal{A})-f(\\mathcal{B})\\right)\\right.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The submodularity ratio of a set $\\mathcal{C}$ with respect to an integer $k$ is defined by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\gamma_{\\mathcal{C},k}=\\operatorname*{min}_{B,A}\\gamma_{B,A}\\qquad s u c h\\;t h a t\\qquad A\\cap B=\\emptyset,\\qquad B\\subseteq\\mathcal{C},\\qquad a n d\\qquad|A|\\leq k.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then given $\\gamma>0$ , a function is $\\gamma$ -weakly submodular at a set $\\mathcal{C}$ with respect to $k$ $\\stackrel{\\cdot}{\\mathrm{~\\boldmath~\\cdot~}}i f\\gamma_{\\mathscr{C},k}\\geq\\gamma$ . ", "page_idx": 15}, {"type": "text", "text": "Theorem 10 (Weak Submodularity via RSC [21]). The submodularity ratio $\\gamma_{\\ensuremath{\\mathcal{S}},k}$ can be bound below using the restricted convexity and smoothness parameters $m_{|S|+k}$ and $M_{|S|+k}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\gamma_{S,k}\\geq m_{|S|+k}\\;/\\;M_{|S|+k}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Theorem 11 (OMP Approximation Guarantee [21]). Let $\\mathbf{x}_{\\mathrm{OMP}}(r)$ be the $r$ -sparse vector selected by OMP, and $\\begin{array}{r}{\\mathbf{x}_{\\mathrm{OPT}}(r)=\\arg\\operatorname*{min}_{\\|\\mathbf{x}\\|_{0}=r}f(\\mathbf{x})}\\end{array}$ be the optimal $r$ -sparse vector. Then ", "page_idx": 15}, {"type": "equation", "text": "$$\nf\\left({\\bf x}_{\\mathrm{OMP}}(r)\\right)\\ge\\left(1-e^{-m_{2r}/M_{2r}}\\right)f\\left({\\bf x}_{\\mathrm{OPT}}(r)\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $m_{2r},M_{2r}$ are the restricted strong convexity and smoothness parameters of $f$ , respectively. ", "page_idx": 15}, {"type": "text", "text": "C Theoretical Results and Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Lemma 1. [Optimal Robust Variances] Let $\\mathcal{D}_{\\backslash i}\\,=\\,\\{(\\mathbf{x}_{j},y_{j})\\,:\\,j\\,\\neq\\,i\\}$ , $\\pmb{\\rho}=\\pmb{\\rho}_{\\backslash i}+\\rho_{i}\\mathbf{e}_{i}$ , where $\\rho,\\pmb{\\rho}_{\\setminus i}\\in\\mathbb{R}_{+}^{n}$ , $[\\rho_{\\backslash i}]_{i}=0$ , and $\\mathbf{e}_{i}$ is the ith canonical basis vector. Then keeping $\\rho_{\\setminus i}\\,f\\!\\!\\!/x e d,$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\rho_{i}^{*}=\\arg\\operatorname*{max}_{\\rho_{i}}\\mathcal{L}\\big(\\rho_{\\setminus i}+\\rho_{i}\\mathbf{e}_{i}\\big)=\\left[(y_{i}-\\mathbb{E}[y(\\mathbf{x}_{i})|\\mathcal{D}_{\\setminus i}])^{2}-\\mathbb{V}[y(\\mathbf{x}_{i})|\\mathcal{D}_{\\setminus i}]\\right]_{+},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $y(\\mathbf{x}_{i})=f(\\mathbf{x}_{i})\\!+\\!\\epsilon_{i}$ . These quantities can be expressed as functions of $\\Sigma^{-1}=(\\mathbf{K}{+}\\mathbf{D}_{\\sigma^{2}{+}\\rho})^{-1}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}[y(\\mathbf{x}_{i})\\vert\\mathcal{D}_{\\backslash i}]^{2}=y_{i}-\\left[\\Sigma^{-1}\\mathbf{y}\\right]_{i}/\\left[\\Sigma^{-1}\\right]_{i i},\\qquad a n d\\qquad\\mathbb{V}[y(\\mathbf{x}_{i})\\vert\\mathcal{D}_{\\backslash i}]=1/\\left[\\Sigma^{-1}\\right]_{i i},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\mathbf{D}_{\\sigma^{2}+\\rho}$ is a diagonal matrix whose entries are ${\\sigma}^{2}+{\\rho}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. First, we partition the covariance matrix ${\\bf K}+\\sigma^{2}{\\bf I}+{\\bf D}_{\\rho}$ to separate the effect of $\\rho_{i}$ and use the block matrix inverse ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\pmb{\\Sigma}^{-1}=\\left(\\mathbf{K}+\\mathbf{D}_{\\rho+\\sigma^{2}}\\right)^{-1}=\\left[\\mathbf{\\Sigma}_{-\\mathbf{u}}^{\\pmb{\\Sigma}_{+}^{-1}}+\\mathbf{u}\\beta_{i}\\mathbf{u}^{\\top}\\right.\\quad\\left.-\\mathbf{u}\\beta_{i}\\right],\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Sigma_{\\backslash i}=k(\\mathbf{X}_{\\backslash i},\\mathbf{X}_{\\backslash i})+\\mathbf{D}_{\\rho_{\\backslash i}+\\sigma^{2}},}\\\\ &{\\quad\\mathbf{u}=\\Sigma_{\\backslash i}^{-1}k(\\mathbf{X}_{\\backslash i},\\mathbf{x}_{i}),\\qquad\\mathrm{and}}\\\\ &{\\beta_{i}=\\left([k(\\mathbf{x}_{i},\\mathbf{x}_{i})+\\sigma^{2}+\\rho_{i}]-k(\\mathbf{x}_{i},\\mathbf{X}_{\\backslash i})\\Sigma_{\\backslash i}^{-1}k(\\mathbf{X}_{\\backslash i},\\mathbf{x}_{i})\\right)^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Quadratic Term With the expression for the inverse of $\\Sigma$ above, we can write the quadratic term of the log likelihood as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{y}^{\\top}(\\mathbf{K}+\\mathbf{D}_{\\rho+\\sigma^{2}})^{-1}\\mathbf{y}=\\mathbf{y}_{\\setminus i}^{\\top}(\\Sigma_{\\setminus i}^{-1}+\\mathbf{u}\\beta_{i}\\mathbf{u}^{\\top})\\mathbf{y}_{\\setminus i}-2\\mathbf{y}_{\\setminus i}^{\\top}\\mathbf{u}\\beta_{i}y_{i}+y_{i}^{2}\\beta_{i}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathbf{y}_{\\setminus i}^{\\top}\\Sigma_{\\setminus i}^{-1}\\mathbf{y}_{\\setminus i}+\\beta_{i}(\\mathbf{y}_{\\setminus i}^{\\top}\\mathbf{u}-y_{i})^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Determinant Term The determinant of a block matrix is given by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left|\\left[\\mathbf{A}\\mathbf{\\Lambda}\\mathbf{B}\\right]\\right|=|\\mathbf{A}||\\mathbf{D}-\\mathbf{CA}^{-1}\\mathbf{B}|.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Applying this identity to $\\Sigma=(\\mathbf{K}+\\mathbf{D}_{\\rho+\\sigma^{2}})$ , we get ", "page_idx": 15}, {"type": "equation", "text": "$$\n|\\mathbf{K}+\\mathbf{D}_{\\rho+\\sigma^{2}}|=|\\Sigma_{\\backslash i}|\\beta_{i}^{-1}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Log Marginal Likelihood ", "text_level": 1, "page_idx": 16}, {"type": "equation", "text": "$$\n2(\\mathcal{L}(\\rho)-\\mathcal{L}(\\rho_{\\backslash i}))=-\\beta_{i}(\\mathbf{y}_{\\backslash i}^{\\top}\\mathbf{u}-y_{i})^{2}-\\log(\\beta_{i}^{-1}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Noting that $\\partial_{\\rho_{i}}\\beta_{i}=-\\beta_{i}^{2}$ , the derivative of the difference in log marginal likelihood w.r.t. $\\rho_{i}$ , is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\partial_{\\rho_{i}}2(\\mathcal{L}(\\pmb{\\rho})-\\mathcal{L}(\\pmb{\\rho}_{\\backslash i}))=(\\mathbf{y}_{\\backslash i}^{\\top}\\mathbf{u}-y_{i})^{2}\\beta_{i}^{2}-\\beta_{i}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "While $\\beta_{i}=0$ is a root of the derivative, we ignore this solution since $\\beta_{i}$ is never zero when $\\sigma^{2}>0$ Therefore, the remaining stationary point is $\\mathbf{\\beta}_{i}^{-1}=(\\mathbf{y}_{\\backslash i}^{\\top}\\mathbf{u}-y_{i})^{2}$ . Since we constrain $\\rho\\geq0$ , this point might not always be attainable. However, because there is only a single stationary point with respect to $\\beta_{i}$ when $\\sigma^{\\frac{\\xi}{2}}>0$ , and $\\beta_{i}$ is a strictly decreasing function of $\\rho_{i}$ , it follows that the marginal likelihood is monotonic as a function of $\\rho_{i}$ to both the left and the right of the stationary point. Therefore, the optimal constraint $\\rho_{i}$ is simply the optimal unconstrained value, projected into the feasible space. In particular, solving $\\beta_{i}^{-1}=(\\mathbf{y}_{\\backslash i}^{\\top}\\mathbf{u}-\\bar{y}_{i})^{2}$ for $\\rho_{i}$ and projecting to the non-negative half-line, we get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\rho_{i}=\\bigg[\\Big(\\mathbf{y}_{\\backslash i}^{\\top}\\mathbf{u}-y_{i}\\Big)^{2}-\\Big(k(\\mathbf{x}_{i},\\mathbf{x}_{i})+\\sigma^{2}-k(\\mathbf{x}_{i},\\mathbf{X}_{\\backslash i})\\Sigma_{\\backslash i}^{-1}k(\\mathbf{X}_{\\backslash i},\\mathbf{x}_{i})\\Big)\\bigg]_{+}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Lastly, note ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{y}_{\\setminus i}^{\\top}\\mathbf{u}-y_{i}=\\mathbf{y}_{\\setminus i}^{\\top}(k(\\mathbf{X}_{\\setminus i},\\mathbf{X}_{\\setminus i})+\\mathbf{D}_{\\sigma^{2}+\\rho_{\\setminus i}})^{-1}k(\\mathbf{X}_{\\setminus i},\\mathbf{x}_{i})-y_{i}=\\mathbb{E}[y(\\mathbf{x}_{i})|\\mathcal{D}_{\\setminus i}]-y_{i},}\\\\ &{\\qquad\\qquad\\qquad k(\\mathbf{x}_{i},\\mathbf{x}_{i})-k(\\mathbf{x}_{i},\\mathbf{X}_{\\setminus i})\\mathbf{\\Sigma}_{\\setminus i}^{-1}k(\\mathbf{X}_{\\setminus i},\\mathbf{x}_{i})+\\sigma^{2}=\\mathbb{V}[y(\\mathbf{x}_{i})|\\mathcal{D}_{\\setminus i}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "As stated by Rasmussen et al. [49] (P. 117, Eq. 5.12), originally shown by Sundararajan and Keerthi [53], these quantities can be expressed as simple functions of $\\Sigma^{-1}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[y(\\mathbf{x}_{i})|\\mathcal{D}_{\\backslash i}]^{2}=y_{i}-\\left[\\boldsymbol{\\Sigma}^{-1}\\mathbf{y}\\right]_{i}/\\left[\\boldsymbol{\\Sigma}^{-1}\\right]_{i i}}\\\\ &{\\ \\mathbb{V}[y(\\mathbf{x}_{i})|\\mathcal{D}_{\\backslash i}]=1/\\left[\\boldsymbol{\\Sigma}^{-1}\\right]_{i i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, all LOO predictive values can be computed in ${\\mathcal{O}}(n^{3})$ or faster, if an inducing point method is used for $\\mathbf{K}$ . ", "page_idx": 16}, {"type": "text", "text": "The following is a preliminary result for our analysis of the log marginal likelihood w.r.t. $\\rho$ . ", "page_idx": 16}, {"type": "text", "text": "Lemma 12. The gradient and Hessian of the log marginal likelihood $\\mathcal{L}$ with respect to $\\rho$ are given by ", "page_idx": 16}, {"type": "equation", "text": "$$\n-2\\nabla_{\\rho}[\\mathcal{L}]=\\mathrm{diag}(\\mathbf{K}^{-1}-\\alpha\\pmb{\\alpha}^{\\top}),\\qquad a n d\\qquad-\\ 2\\mathbf{H}_{\\rho}[\\mathcal{L}]=(2\\alpha\\pmb{\\alpha}^{\\top}-\\mathbf{K}^{-1})\\odot\\mathbf{K}^{-1}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\mathbf{K}=\\mathbf{K}_{0}+\\mathbf{D}_{\\rho}$ for some base covariance matrix $\\mathbf{K}_{\\mathrm{0}}$ and $\\alpha=\\mathbf{K}^{-1}\\mathbf{y}$ ", "page_idx": 16}, {"type": "text", "text": "Proof. Let $\\pmb{\\alpha}=\\mathbf{K}^{-1}\\mathbf{y}$ . Regarding the gradient, note that $\\partial_{\\rho_{i}}\\mathbf{K}=\\mathbf{e}_{i}\\mathbf{e}_{i}^{\\top}$ , where $\\mathbf{e}_{i}$ is the canonical basis vector with a one as the $i$ th element, and based on Equation 5.9 of Rasmussen et al. [49], ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-2\\partial_{\\rho_{i}}[\\mathcal{L}]=\\mathrm{tr}\\left(\\left(\\mathbf{K}^{-1}-\\alpha\\boldsymbol{\\alpha}^{\\top}\\right)\\partial_{\\rho_{i}}\\mathbf{K}\\right)}\\\\ &{\\quad\\quad\\quad\\quad=\\mathrm{tr}(\\left(\\mathbf{K}^{-1}-\\alpha\\boldsymbol{\\alpha}^{\\top}\\right)\\mathbf{e}_{i}\\mathbf{e}_{i}^{\\top})}\\\\ &{\\quad\\quad\\quad\\quad=\\mathbf{e}_{i}^{\\top}\\left(\\mathbf{K}^{-1}-\\alpha\\boldsymbol{\\alpha}^{\\top}\\right)\\mathbf{e}_{i}}\\\\ &{\\quad\\quad\\quad\\quad=\\left(\\mathbf{K}^{-1}-\\alpha\\boldsymbol{\\alpha}^{\\top}\\right)_{i i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Regarding the second derivatives, according to Dong et al. [20], ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\partial_{\\theta_{i}}\\partial_{\\theta_{j}}[\\mathrm{log}\\,|{\\bf K}|]=\\mathrm{tr}({\\bf K}^{-1}[\\partial_{\\theta_{i}}\\partial_{\\theta_{j}}{\\bf K}]-{\\bf K}^{-1}[\\partial_{\\theta_{i}}{\\bf K}]{\\bf K}^{-1}[\\partial_{\\theta_{j}}{\\bf K}])}\\\\ &{}&{\\partial_{\\theta_{i}}\\partial_{\\theta_{j}}[{\\bf y}^{\\top}{\\bf K}^{-1}{\\bf y}]=2\\alpha^{T}[\\partial_{\\theta_{i}}{\\bf K}]{\\bf K}^{-1}[\\partial_{\\theta_{j}}{\\bf K}]\\alpha-\\alpha^{\\top}[\\partial_{\\theta_{i}}\\partial_{\\theta_{j}}{\\bf K}]\\alpha.\\ \\ \\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-2\\partial_{\\rho_{i}}\\partial_{\\rho_{j}}\\mathcal{L}=\\mathrm{tr}\\left((\\mathbf{K}^{-1}-\\alpha\\boldsymbol{\\alpha}^{T})[\\partial_{\\rho_{i}}\\partial_{\\rho_{j}}\\mathbf{K}]-(\\mathbf{K}^{-1}-2\\alpha\\boldsymbol{\\alpha}^{\\top})([\\partial_{\\rho_{i}}\\mathbf{K}]\\mathbf{K}^{-1}[\\partial_{\\rho_{j}}\\mathbf{K}])\\right)}\\\\ &{\\quad\\quad\\quad\\quad=\\mathrm{tr}\\left((2\\alpha\\boldsymbol{\\alpha}^{\\top}-\\mathbf{K}^{-1})([\\mathbf{K}^{-1}]_{i j}\\mathbf{e}_{i}\\mathbf{e}_{j}^{\\top})\\right)}\\\\ &{\\quad\\quad\\quad\\quad=\\mathrm{tr}\\left(\\mathbf{e}_{j}^{\\top}(2\\alpha\\boldsymbol{\\alpha}^{\\top}-\\mathbf{K}^{-1})\\mathbf{e}_{i}\\right)[\\mathbf{K}^{-1}]_{i j}}\\\\ &{\\quad\\quad\\quad\\quad=[2\\alpha\\boldsymbol{\\alpha}^{\\top}-\\mathbf{K}^{-1}]_{i j}[\\mathbf{K}^{-1}]_{i j}}\\\\ &{\\quad\\quad\\quad\\quad=[(2\\alpha\\boldsymbol{\\alpha}^{\\top}-\\mathbf{K}^{-1})\\odot\\mathbf{K}^{-1}]_{i j}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "since $[\\partial_{\\rho_{i}}\\partial_{\\rho_{j}}{\\bf K}]={\\bf0}$ . The third equality is due to the invariance of the trace to circular shifts of its argument. The forth equality is due to the symmetry of the matrix in brackets. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "C.1 Strong Convexity and Smoothness of the Reparameterized Robust Marginal Likelihood ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Here, we re-parameterize $\\pmb{\\rho}(\\mathbf{s})=\\mathrm{diag}(\\mathbf{K}_{0})\\odot((1-\\mathbf{s})^{-1}-1)$ , and attain strong convexity for all inputs s, if conditions on the eigenvalues of the covariance matrix and the norm of the data vector $\\|\\mathbf{y}\\|$ are met. The convexity result is surprising in two ways: the negative log marginal likelihood of GPs is generally a non-convex function, and in addition, the negative log likelihoods of many alternative robust regression methods like the Student- $\\boldsymbol{\\cdot}$ likelihood or $\\alpha$ -stable likelihoods are non-convex, and even Huber\u2019s proposal is non-strongly-convex. ", "page_idx": 17}, {"type": "text", "text": "Lemma 4. [Reparameterized Hessian] Let $\\begin{array}{r l r}{{\\bf K}_{\\bf s}}&{{}=}&{k({\\bf X},{\\bf X})\\;+\\;\\sigma^{2}{\\bf I}\\;+\\;{\\bf D}_{\\rho({\\bf s})}}\\end{array}$ , $\\begin{array}{r l}{\\hat{\\mathbf{K}}_{\\mathbf{s}}}&{{}=}\\end{array}$ $\\mathrm{diag}(\\mathbf{K_{s}})^{-1/2}\\mathbf{K_{s}}\\,\\mathrm{diag}(\\mathbf{K_{s}})^{-1/2}$ , and $\\hat{\\pmb{\\alpha}}=\\hat{\\mathbf{K}}_{\\mathbf{s}}^{-1}\\,\\mathrm{diag}(\\mathbf{K}_{\\mathbf{s}})^{-1/2}\\mathbf{y}$ . Then ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{H}_{\\mathrm{s}}[-2\\mathcal{L}(\\rho(\\mathbf{s})]=\\mathbf{D}_{1-\\mathrm{s}}^{-1}\\left[2\\left(\\hat{\\alpha}\\hat{\\alpha}^{\\top}\\odot(\\hat{\\mathbf{K}}^{-1}-\\mathbf{I})\\right)+2\\operatorname{diag}(\\hat{\\mathbf{K}}^{-1})-(\\hat{\\mathbf{K}}^{-1}\\odot\\hat{\\mathbf{K}}^{-1})\\right]\\mathbf{D}_{1-\\mathrm{s}}^{-1}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. Using the chain-rule, the Hessian $\\mathbf{H}_{\\mathbf{s}}[\\mathcal{L}]$ can be expressed as a function of the Jacobian $\\mathbf{J_{s}}[\\rho(\\mathbf{s})]\\;=\\;\\mathbf{D}_{\\partial\\rho(\\mathbf{s})}$ , which is diagonal since $\\rho(\\mathbf{s})$ is an element-wise function, and the second derivatives $\\partial_{s}^{2}\\rho(s_{i})$ . Then ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-2\\mathbf{H}_{\\mathbf{s}}[\\mathcal{L}]=-\\mathbf{J}_{\\mathbf{s}}[\\rho]^{\\top}\\mathbf{H}_{\\rho}[2\\log\\mathcal{L}]\\mathbf{J}_{\\mathbf{s}}[\\rho]+\\mathbf{D}_{\\nabla_{\\rho}[-2\\mathcal{L}]}\\mathbf{D}_{\\partial_{s}^{2}\\rho}}\\\\ &{\\qquad\\qquad=\\mathbf{D}_{\\rho^{\\prime}(\\mathbf{s})}[(2\\alpha\\alpha^{\\top}-\\mathbf{K}^{-1})\\odot\\mathbf{K}^{-1}]\\mathbf{D}_{\\rho^{\\prime}(\\mathbf{s})}+\\mathrm{diag}(\\mathbf{K}^{-1})\\mathbf{D}_{\\rho^{\\prime\\prime}}-\\mathbf{D}_{\\rho^{\\prime}}(\\mathbf{K}^{-1}\\circ\\mathbf{K}^{-1})\\mathbf{D}_{\\rho^{\\prime}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we substituted the relevant expressions from Lemma 12. Further substituting $\\rho(\\mathbf{s})_{i}\\ =$ $[\\mathbf{K}_{0}]_{i i}[(1\\,-\\,s_{i})^{-1}\\,-\\,1]$ , $\\pmb{\\rho}^{\\prime}(\\mathbf{s})_{i}\\ =\\ [\\dot{\\mathbf{K}_{0}}]_{i i}(1-s_{i})^{-2}$ , and $\\pmb{\\rho}^{\\prime\\prime}(\\mathbf{s})_{i}\\ =\\ 2[\\mathbf{K}_{0}]_{i i}(1\\,-\\,s_{i})^{-3}$ , noting that $\\mathbf{K}=\\mathbf{K}_{0}+\\operatorname{diag}(\\mathbf{K}_{0})[(1-s)^{-1}-1]=(\\mathbf{K}_{0}-\\operatorname{diag}(\\mathbf{K}_{0}))+\\operatorname{diag}(\\mathbf{K}_{0})(1-s)^{-1}$ , and algebraic manipulation finish the proof. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Lemma 5. [Strong Convexity via Eigenvalue Condition] Let $\\hat{\\mathbf{K}}_{\\mathbf{s}}$ as in Lemma 4. Then $\\mathbf{H_{s}}\\succ m\\:i f$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{min}}\\hat{\\lambda}_{\\operatorname*{min}}^{2}\\frac{(2\\hat{\\lambda}_{\\operatorname*{max}}^{-1}-\\hat{\\lambda}_{\\operatorname*{min}}^{-2}-m)}{2(1-\\lambda_{\\operatorname*{min}}/\\lambda_{\\operatorname*{max}})}>\\|\\mathbf{y}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\lambda_{\\mathrm{min,max}}$ (resp. $\\hat{\\lambda}_{\\mathrm{min,max}})$ are the smallest and largest eigenvalues of $\\bf{K_{s}}$ , respectively $\\hat{\\mathbf{K}}_{\\mathbf{s}}$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. We seek to lower-bound the smallest eigenvalue of the Hessian matrix, which\u2014for twicecontinuously-differentiable problems\u2014is equivalent to lower and upper bounds of the problem\u2019s Hessian matrix. Starting with the result of Lemma 4, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{H}_{\\mathrm{s}}[-2\\mathcal{L}(\\rho(\\mathbf{s})]=\\mathbf{D}_{1-\\mathrm{s}}^{-1}\\left[2\\left(\\hat{\\alpha}\\hat{\\alpha}^{\\top}\\odot(\\hat{\\mathbf{K}}^{-1}-\\mathbf{I})\\right)+2\\operatorname{diag}(\\hat{\\mathbf{K}}^{-1})-(\\hat{\\mathbf{K}}^{-1}\\odot\\hat{\\mathbf{K}}^{-1})\\right]\\mathbf{D}_{1-\\mathrm{s}}^{-1}}\\\\ &{\\qquad\\qquad\\qquad\\succeq2\\left(\\hat{\\alpha}\\hat{\\pmb{\\alpha}}^{\\top}\\odot(\\hat{\\mathbf{K}}^{-1}-\\mathbf{I})\\right)+2\\operatorname{diag}(\\hat{\\mathbf{K}}^{-1})-(\\hat{\\mathbf{K}}^{-1}\\odot\\hat{\\mathbf{K}}^{-1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now, we bound each of the three additive terms independently from below. ", "page_idx": 17}, {"type": "text", "text": "Term 1: ", "page_idx": 17}, {"type": "equation", "text": "$$\n2\\operatorname{diag}(\\hat{\\mathbf{K}}^{-1})\\succeq2\\lambda_{\\operatorname*{min}}(\\hat{\\mathbf{K}}^{-1})\\mathbf{I}=2\\lambda_{\\operatorname*{max}}(\\hat{\\mathbf{K}})^{-1}\\mathbf{I}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The first inequality comes from $\\mathbf{K}$ being positive definite, and the absolute value of the diagonal of a matrix, which is already positive for positive definite matrices, being lower bounded by the minimum eigenvalue of the matrix. The last steps is a basic consequence of the eigenvalues of inverses matrices. Note that the eigenvalues of $\\hat{\\bf K}$ can be further bound by the eigenvalues of the original matrix $\\mathbf{K}$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lambda_{\\operatorname*{min}}(\\hat{\\mathbf{K}})=\\lambda_{\\operatorname*{min}}(\\mathrm{diag}(\\mathbf{K})^{-1/2}\\mathbf{K}\\,\\mathrm{diag}(\\mathbf{K})^{-1/2})}&{}\\\\ {\\geq\\lambda_{\\operatorname*{min}}(\\mathrm{diag}(\\mathbf{K})^{-1/2})^{2}\\lambda_{\\operatorname*{min}}(\\mathbf{K})}&{}\\\\ {=\\lambda_{\\operatorname*{min}}(\\mathrm{diag}(\\mathbf{K})^{-1})\\lambda_{\\operatorname*{min}}(\\mathbf{K})}&{}\\\\ {\\geq\\lambda_{\\operatorname*{min}}(\\mathbf{K}^{-1})\\lambda_{\\operatorname*{min}}(\\mathbf{K})}&{}\\\\ {=\\lambda_{\\operatorname*{min}}(\\mathbf{K})/\\lambda_{\\operatorname*{max}}(\\mathbf{K}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In a similar way, we can show that $\\lambda_{\\operatorname*{max}}(\\hat{\\mathbf{K}})\\leq\\lambda_{\\operatorname*{max}}(\\mathbf{K})/\\lambda_{\\operatorname*{min}}(\\mathbf{K})$ , which implies $\\lambda_{\\operatorname*{max}}(\\hat{\\mathbf{K}})^{-1}\\geq$ $\\lambda_{\\operatorname*{min}}(\\mathbf{K})/\\lambda_{\\operatorname*{max}}\\dot{(\\mathbf{K})}$ . ", "page_idx": 17}, {"type": "text", "text": "Term 2: Next, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n-(\\hat{\\mathbf{K}}^{-1}\\odot\\hat{\\mathbf{K}}^{-1})\\succeq-\\lambda_{\\operatorname*{max}}(\\hat{\\mathbf{K}}^{-1})^{2}\\mathbf{I}=-\\lambda_{\\operatorname*{min}}(\\hat{\\mathbf{K}})^{-2}\\mathbf{I},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which is due to the Hadamard product being a sub-matrix of the Kronecker product of the same matrices, the largest eigenvalue of the former are bounded by the largest eigenvalue of the latter, which is the product of the largest eigenvalues of the constituent matrices. ", "page_idx": 18}, {"type": "text", "text": "Term 3: Lastly, note that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\left(\\hat{\\alpha}\\hat{\\alpha}^{\\top}\\odot(\\hat{\\mathbf{K}}^{-1}-\\mathbf{I})\\right)=2\\mathbf{D}_{\\hat{\\alpha}}(\\hat{\\mathbf{K}}^{-1}-\\mathbf{I})\\mathbf{D}_{\\hat{\\alpha}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\succeq2\\mathbf{D}_{\\hat{\\alpha}}(\\lambda_{\\mathrm{min}}(\\hat{\\mathbf{K}}^{-1})\\mathbf{I}-\\mathbf{I})\\mathbf{D}_{\\hat{\\alpha}}}\\\\ &{\\qquad\\qquad\\quad=2(\\lambda_{\\mathrm{min}}(\\hat{\\mathbf{K}}^{-1})-1)\\mathbf{D}_{\\hat{\\alpha}}^{2}}\\\\ &{\\qquad\\qquad\\quad=2(\\lambda_{\\mathrm{max}}(\\hat{\\mathbf{K}})^{-1}-1)\\mathbf{D}_{\\hat{\\alpha}}^{2}}\\\\ &{\\qquad\\qquad\\quad\\succeq2(\\lambda_{\\mathrm{min}}(\\mathbf{K})/\\lambda_{\\mathrm{max}}(\\mathbf{K})-1)\\mathbf{D}_{\\hat{\\alpha}}^{2}}\\\\ &{\\qquad\\qquad\\quad\\succeq2(\\lambda_{\\mathrm{min}}(\\mathbf{K})/\\lambda_{\\mathrm{max}}(\\mathbf{K})-1)\\|\\hat{\\alpha}\\|_{\\infty}^{2}\\mathbf{I},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the last inequality comes from the second to last lower bound of Term 3 being non-positive, and therefore, being able to lower bound it with the largest magnitude entry of $\\hat{\\pmb{\\alpha}}$ . ", "page_idx": 18}, {"type": "text", "text": "Term $\\mathbf{1}+2+3$ : Putting together the inequalities for all terms, we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lambda_{\\operatorname*{min}}(\\mathbf{H_{s}})\\geq2\\lambda_{\\operatorname*{max}}(\\hat{\\mathbf{K}})^{-1}-\\lambda_{\\operatorname*{min}}(\\hat{\\mathbf{K}})^{-2}+2(\\lambda_{\\operatorname*{min}}(\\mathbf{K})/\\lambda_{\\operatorname*{max}}(\\mathbf{K})-1)\\|\\hat{\\alpha}\\|_{\\infty}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where in slight abuse of notation, we let $\\mathbf{H_{s}}=\\mathbf{H_{s}}[-2\\mathcal{L}]$ be the Hessian of the negative log likelihood. Using the bound $\\|\\hat{\\alpha}\\|_{\\infty}\\;\\leq\\;\\|\\hat{\\alpha}\\|_{2}\\;\\leq\\;\\lambda_{\\operatorname*{min}}(\\hat{\\mathbf{K}})^{-1}\\|\\hat{\\mathbf{y}}\\|$ , where $\\hat{\\bf y}\\;=\\;\\mathrm{diag}({\\bf K})^{-1/2}{\\bf y}$ . Therefore, $\\|\\hat{\\alpha}\\|_{\\infty}\\leq\\lambda_{\\operatorname*{min}}(\\hat{\\mathbf{K}})^{-1}\\lambda_{\\operatorname*{min}}(\\mathbf{K})^{-1/2}\\|\\mathbf{y}\\|_{2}\\leq\\lambda_{\\operatorname*{max}}(\\mathbf{K})\\lambda_{\\operatorname*{min}}(\\mathbf{K})^{-3/2}\\|\\mathbf{y}\\|_{2}.$ ", "page_idx": 18}, {"type": "text", "text": "Finally, lower bounding the current lower bound by $m\\,>\\,0$ yields a sufficient condition for the convexity at s. ", "page_idx": 18}, {"type": "equation", "text": "$$\n2\\lambda_{\\operatorname*{max}}(\\hat{\\mathbf{K}})^{-1}-\\lambda_{\\operatorname*{min}}(\\hat{\\mathbf{K}})^{-2}+2(\\lambda_{\\operatorname*{min}}(\\mathbf{K})/\\lambda_{\\operatorname*{max}}(\\mathbf{K})-1)\\lambda_{\\operatorname*{min}}(\\hat{\\mathbf{K}})^{-2}\\lambda_{\\operatorname*{min}}(\\mathbf{K})^{-1}\\|\\mathbf{y}\\|_{2}^{2}>m.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Re-arranging, we attain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{min}}\\hat{\\lambda}_{\\operatorname*{min}}^{2}\\frac{(2\\hat{\\lambda}_{\\operatorname*{max}}^{-1}-\\hat{\\lambda}_{\\operatorname*{min}}^{-2}-m)}{2(1-\\lambda_{\\operatorname*{min}}/\\lambda_{\\operatorname*{max}})}>\\|\\mathbf{y}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which is a non-trivial guarantee when $2\\hat{\\lambda}_{\\operatorname*{max}}^{-1}-\\hat{\\lambda}_{\\operatorname*{min}}^{-2}-m>0$ . ", "page_idx": 18}, {"type": "text", "text": "Lemma 13. [Smoothness via Eigenvalue Condition] Let $\\hat{\\bf K}$ as in Lemma 4. Suppose that $\\|\\mathbf{s}\\|_{\\infty}\\leq$ $s_{\\mathrm{max}}$ and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{min}}\\hat{\\lambda}_{\\operatorname*{min}}^{2}\\frac{(M(1-s_{\\operatorname*{max}})^{2}+\\hat{\\lambda}_{\\operatorname*{min}}^{-2}-2\\hat{\\lambda}_{\\operatorname*{max}}^{-1})}{2(\\lambda_{\\operatorname*{max}}/\\lambda_{\\operatorname*{min}}-1)}>\\|\\mathbf{y}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\lambda_{\\mathrm{min,max}}$ (resp. $\\hat{\\lambda}_{\\mathrm{min,max}})$ are the smallest and largest eigenvalues of $\\mathbf{K}\\,(r e s p.\\ \\hat{\\mathbf{K}})$ respectively.   \nThen $\\mathbf{H}_{\\mathbf{s}}\\prec M$ . This is a non-trivial guarantee when $M(1-s_{\\mathrm{max}})^{2}+\\hat{\\lambda}_{\\mathrm{min}}^{-2}-2\\hat{\\lambda}_{\\mathrm{max}}^{-1}>0$ . ", "page_idx": 18}, {"type": "text", "text": "Proof. We now derive an equivalent upper bound for the largest eigenvalue of the Hessian. Starting with the result of Lemma 4, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{H}_{\\mathbf{s}}[-2\\mathcal{L}(\\boldsymbol{\\rho}(\\mathbf{s})]=\\mathbf{D}_{1-\\mathbf{s}}^{-1}\\left[2\\left(\\hat{\\alpha}\\hat{\\alpha}^{\\top}\\odot(\\hat{\\mathbf{K}}^{-1}-\\mathbf{I})\\right)+2\\operatorname{diag}(\\hat{\\mathbf{K}}^{-1})-(\\hat{\\mathbf{K}}^{-1}\\odot\\hat{\\mathbf{K}}^{-1})\\right]\\mathbf{D}_{1-\\mathbf{s}}^{-1}}\\\\ &{\\qquad\\qquad\\qquad\\preceq\\left\\lVert(1-\\mathbf{s})^{-1}\\right\\rVert_{\\infty}^{2}\\left[2\\left(\\hat{\\alpha}\\hat{\\alpha}^{\\top}\\odot(\\hat{\\mathbf{K}}^{-1}-\\mathbf{I})\\right)+2\\operatorname{diag}(\\hat{\\mathbf{K}}^{-1})-(\\hat{\\mathbf{K}}^{-1}\\odot\\hat{\\mathbf{K}}^{-1})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, it becomes immediately apparent that we will need to introduce an upper bound on $\\|(1-\\mathbf{s})^{-1}\\|_{\\infty}^{2}$ , which is a restriction on the domain that $\\rho$ can take. Proceeding in a similar way as above, we bound the three terms in square brackets, now from above. ", "page_idx": 18}, {"type": "equation", "text": "$$\n2\\operatorname{diag}(\\hat{\\mathbf{K}}^{-1})\\preceq2\\lambda_{\\operatorname*{max}}(\\hat{\\mathbf{K}}^{-1})\\mathbf{I}=2\\lambda_{\\operatorname*{min}}(\\hat{\\mathbf{K}})^{-1}\\mathbf{I}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Term 2: ", "page_idx": 19}, {"type": "equation", "text": "$$\n-(\\hat{\\mathbf{K}}^{-1}\\odot\\hat{\\mathbf{K}}^{-1})\\preceq-\\lambda_{\\operatorname*{min}}(\\hat{\\mathbf{K}}^{-1})^{2}\\mathbf{I}=-\\lambda_{\\operatorname*{max}}(\\hat{\\mathbf{K}})^{-2}\\mathbf{I}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Term 3: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\left(\\hat{\\alpha}\\hat{\\alpha}^{\\top}\\odot(\\hat{\\mathbf{K}}^{-1}-\\mathbf{I})\\right)=2\\mathbf{D}_{\\hat{\\alpha}}(\\hat{\\mathbf{K}}^{-1}-\\mathbf{I})\\mathbf{D}_{\\hat{\\alpha}}}\\\\ &{\\phantom{2p c}\\preceq2\\mathbf{D}_{\\hat{\\alpha}}(\\lambda_{\\mathrm{max}}(\\hat{\\mathbf{K}}^{-1})\\mathbf{I}-\\mathbf{I})\\mathbf{D}_{\\hat{\\alpha}}}\\\\ &{\\phantom{2p c}=2(\\lambda_{\\mathrm{max}}(\\hat{\\mathbf{K}}^{-1})-1)\\mathbf{D}_{\\hat{\\alpha}}^{2}}\\\\ &{\\phantom{2p c}=2(\\lambda_{\\mathrm{min}}(\\hat{\\mathbf{K}})^{-1}-1)\\mathbf{D}_{\\hat{\\alpha}}^{2}}\\\\ &{\\preceq2(\\lambda_{\\mathrm{max}}(\\mathbf{K})/\\lambda_{\\mathrm{min}}(\\mathbf{K})-1)\\mathbf{D}_{\\hat{\\alpha}}^{2}}\\\\ &{\\preceq2(\\lambda_{\\mathrm{max}}(\\mathbf{K})/\\lambda_{\\mathrm{min}}(\\mathbf{K})-1)\\mathbf{D}_{\\hat{\\alpha}}^{2}}\\\\ &{\\preceq2(\\lambda_{\\mathrm{max}}(\\mathbf{K})/\\lambda_{\\mathrm{min}}(\\mathbf{K})-1)\\|\\hat{\\alpha}\\|_{\\infty}^{2}\\mathbf{I},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where now the last inequality follows because the second to last expression is always non-negative. Term $\\mathbf{1}+2+3$ : Putting together the inequalities for all terms, we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lambda_{\\operatorname*{max}}(\\mathbf{H}_{\\mathrm{s}})\\leq2\\lambda_{\\operatorname*{min}}(\\hat{\\mathbf{K}})^{-1}-\\lambda_{\\operatorname*{max}}(\\hat{\\mathbf{K}})^{-2}+2(\\lambda_{\\operatorname*{max}}(\\mathbf{K})/\\lambda_{\\operatorname*{min}}(\\mathbf{K})-1)\\|\\hat{\\boldsymbol{\\alpha}}\\|_{\\infty}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Finally, upper bounding the current upper bound by $M/\\|(1-\\mathbf{s})^{-1}\\|_{\\infty}^{2}\\,>\\,0$ yields a sufficient condition for the $M$ -smoothness at s. Using the same bound for $\\|\\hat{\\alpha}\\|_{\\infty}$ derived for the convexity result, ", "page_idx": 19}, {"type": "text", "text": "$2\\lambda_{\\operatorname*{min}}(\\hat{\\mathbf{K}})^{-1}-\\lambda_{\\operatorname*{max}}(\\hat{\\mathbf{K}})^{-2}+2\\big(\\big(\\lambda_{\\operatorname*{max}}(\\mathbf{K})/\\lambda_{\\operatorname*{min}}(\\mathbf{K})-1\\big)\\lambda_{\\operatorname*{min}}(\\hat{\\mathbf{K}})^{-2}\\lambda_{\\operatorname*{min}}(\\mathbf{K})^{-1}\\|\\mathbf{y}\\|_{2}^{2}<M.$ Re-arranging, we attain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{min}}\\hat{\\lambda}_{\\operatorname*{min}}^{2}\\frac{(M(1-s_{\\operatorname*{max}})^{2}+\\hat{\\lambda}_{\\operatorname*{min}}^{-2}-2\\hat{\\lambda}_{\\operatorname*{max}}^{-1})}{2(\\lambda_{\\operatorname*{max}}/\\lambda_{\\operatorname*{min}}-1)}>\\|\\mathbf{y}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which is a non-trivial guarantee when $M(1-s_{\\operatorname*{max}})^{2}+\\hat{\\lambda}_{\\operatorname*{min}}^{-2}-2\\hat{\\lambda}_{\\operatorname*{max}}^{-1}>0.$ ", "page_idx": 19}, {"type": "text", "text": "Lemma 7. [Strong Convexity v\u221aia Diagonal Dominance] L\u221aet $m\\ >\\ 0$ and $\\mathbf{K}_{\\mathrm{0}}$ be $\\delta$ -diagonally dominant with $\\delta<\\left((5-m)-\\sqrt{25-9m+17}\\right)/4\\leq(5-\\sqrt{17})/4\\approx0.44$ and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{min}}(\\mathbf{K}_{0})(1-\\delta)^{2}\\frac{2(1+\\delta)^{-1}-(1-\\delta)^{-2}-m}{2(1-(1-\\delta)/(1+\\delta))}\\geq\\|\\mathbf{y}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then the NMLL is $m$ -strongly convex for all $\\mathbf{s}\\in[0,1]^{n}$ , i.e. $\\rho(\\mathbf{s})\\in[0,\\infty]^{n}$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. Fist, Gershgorin\u2019s Disk Theorem implies that the eigenvalues of $\\mathrm{diag}(\\mathbf{A})^{-1/2}\\mathbf{A}\\,\\mathrm{diag}(\\mathbf{A})^{-1/2}$ lie in $(1-\\delta,1+\\bar{\\delta})$ for a $\\delta$ -diagonally dominant matrix A. Further, the condition number of $\\mathbf{A}$ is bounded above by $\\kappa(\\mathbf{A})=\\lambda_{\\operatorname*{max}}(\\mathbf{A})/\\lambda_{\\operatorname*{min}}(\\mathbf{A})\\leq(1+\\delta)/(1-\\delta)$ . See Horn and Johnson [28] for more background on matrix analysis. Plugging these bounds into the results of Lemma 5 yields ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{min}}(\\mathbf{K})\\hat{\\lambda}_{\\operatorname*{min}}^{2}\\frac{(2\\hat{\\lambda}_{\\operatorname*{max}}^{-1}-\\hat{\\lambda}_{\\operatorname*{min}}^{-2}-m)}{2(1-\\lambda_{\\operatorname*{min}}/\\lambda_{\\operatorname*{max}})}\\geq\\lambda_{\\operatorname*{min}}(\\mathbf{K})(1-\\delta)^{2}\\frac{(2(1+\\delta)^{-1}-(1-\\delta)^{-2}-m)}{2(1-(1-\\delta)/(1+\\delta))}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Lower bounding the last expression by $\\|\\mathbf{y}\\|_{2}^{2}$ implies $m$ -strong convexity. This gives rise to a non-trivial guarantee whenever the numerator is larger than zero. In particular, ", "page_idx": 19}, {"type": "equation", "text": "$$\n2(1+\\delta)^{-1}-(1-\\delta)^{-2}-m>0.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Expanding, noting that $0\\,<\\,\\delta\\,<\\,1$ implies $\\delta^{3}\\,<\\,\\delta^{2}$ in order to reduce a power in the resulting expression, and collecting terms with like powers, we attain the following sufficient condition ", "page_idx": 19}, {"type": "equation", "text": "$$\n2\\delta^{2}+(m-5)\\delta+(1-m)>0.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that for this condition to hold if $\\delta=0$ , we need to have $m<1$ . Fortunately, this is a quadratic in $\\delta$ whose smallest positive root is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\delta_{-}=\\frac{1}{4}\\left((5-m)-\\sqrt{(5-m)^{2}-8(1-m)}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In particular, for $m=0$ , this reduces to ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\delta_{-}=\\frac{1}{4}\\left(5-\\sqrt{17}\\right)\\approx0.4384471871911697.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Lastly, note that if $\\mathbf{K}_{0}$ is $\\delta$ -diagonally dominant, then so is $\\mathbf{K_{s}}=\\mathbf{K_{0}}+\\mathbf{D}_{\\rho}(\\mathbf{s})$ , since the robust variances add to the diagonal, making it more dominant. Therefore, the convexity guarantee holds for all $\\rho(\\mathbf{s})$ , if it holds for the base covariance matrix $\\mathbf{K}_{0}$ . Note that $\\lambda_{\\operatorname*{min}}(\\mathbf{K})\\geq\\sigma^{\\bar{2}}$ . \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Similarly, we can prove a similar statement relating diagonal dominance with $M$ -smoothness. ", "page_idx": 20}, {"type": "text", "text": "Lemma 14. [Smoothness via Diagonal Dominance] Suppose $\\mathbf{K}_{\\mathrm{0}}$ is a $\\delta$ -diagonally dominant covariance matrix and suppose we constrain $\\|\\mathbf{s}\\|_{\\infty}\\leq s_{\\operatorname*{max}}\\leq1-\\sqrt{1/M}$ . Then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{min}}(\\mathbf{K}_{0})(1-\\delta)^{2}\\frac{M(1-s_{\\operatorname*{max}})^{2}-1}{2((1+\\delta)/(1-\\delta)-1)}\\geq\\|\\mathbf{y}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "implies that the NLML is $m$ -strongly convex for all $\\mathbf{s}\\in[0,1]^{n}$ , i.e. $\\rho(\\mathbf{s})\\in[0,\\infty]^{n}$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. We proceed in a similar way as for Lemma 7, but with Lemma 13 as the starting point. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{min}}(\\mathbf{K})(\\hat{\\lambda}_{\\operatorname*{min}})^{2}\\frac{(M(1-s_{\\operatorname*{max}})^{2}+\\hat{\\lambda}_{\\operatorname*{min}}^{-2}-2\\hat{\\lambda}_{\\operatorname*{max}}^{-1})}{2(\\lambda_{\\operatorname*{max}}/\\lambda_{\\operatorname*{min}}-1)}\\geq\\lambda_{\\operatorname*{min}}(\\mathbf{K})(1-\\delta)^{2}\\frac{M(1-s_{\\operatorname*{max}})^{2}-1}{2((1+\\delta)/(1-\\delta)-1)},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we used $(\\hat{\\lambda}_{\\operatorname*{min}}^{-2}-2\\hat{\\lambda}_{\\operatorname*{max}}^{-1})\\geq-1$ , which is tight when $\\hat{\\bf K}$ is unitary. Lower bounding the last expression by $\\|\\mathbf{y}\\|_{2}^{2}$ implies $M$ -smoothness. This gives rise to a non-trivial guarantee whenever the numerator is larger than zero. In particular, $M(1\\!-\\!s_{\\operatorname*{max}})^{2}\\!-\\!1>0$ , which implies $s_{\\operatorname*{max}}\\leq1\\!-\\!\\sqrt{1/M}$ or equivalently, $M>1/(1-s_{\\mathrm{max}})^{2}$ . Lastly, note that if $\\mathbf{K}_{0}$ is $\\delta$ -diagonally dominant, then so is $\\mathbf{K_{s}}=\\mathbf{K_{0}}+\\mathbf{D}_{\\rho}(\\mathbf{s})$ , since the robust variances only add to the diagonal. Therefore, if the inequality holds for the base covariance matrix $\\mathbf{K}_{\\mathrm{0}}$ , the smoothness guarantee holds for all $\\rho(\\mathbf{s})$ such that $\\mathbf s\\leq s_{\\operatorname*{max}}\\leq1-\\sqrt{1/M}$ . Note also that $\\lambda_{\\operatorname*{min}}(\\mathbf{K})\\geq\\sigma^{2}$ . \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Theorem 8. [Approximation Guarantee] Let ${\\bf K}_{0}\\;=\\;k({\\bf X},{\\bf X})+\\sigma^{2}{\\bf I}$ be $\\delta$ -diagonally dominant, $s_{\\mathrm{max}}>0$ be an upper bound on $\\|\\mathbf{s}\\|_{\\infty}$ , and suppose $\\|\\mathbf{y}\\|$ , $\\delta$ satisfy the bounds of Lemmas 7 and $^{l4}$ , guaranteeing $m$ -convexity and $M$ -smoothness of the NMLL for some $m>0$ , $M>1/(1-s_{\\mathrm{max}})^{2}$ . Let $\\mathbf{s}_{\\mathrm{OMP}}(r)$ be the $r$ -sparse vector attained by OMP on the NMLL objective for $r$ steps, and let $\\begin{array}{r}{\\mathbf{s}_{\\mathrm{OPT}}(r)=\\arg\\operatorname*{max}_{\\|\\mathbf{s}\\|_{0}=r,\\|\\mathbf{s}\\|_{\\infty}\\leq s_{\\operatorname*{max}}}\\mathcal{L}(\\rho(\\mathbf{s}))}\\end{array}$ be the optimal $r$ -sparse vector. Then for any $2r\\leq n_{!}$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{L}}\\left(\\rho({\\mathbf{s}}_{\\mathrm{OMP}}(r))\\right)\\;\\geq\\;\\left(1-e^{-m/M}\\right)\\,\\tilde{\\mathcal{L}}\\left(\\rho({\\mathbf{s}}_{\\mathrm{OPT}}(r))\\right),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\tilde{\\mathcal{L}}(\\cdot)=\\mathcal{L}(\\cdot)-\\mathcal{L}(\\mathbf{0})$ is normalized so that $\\operatorname*{max}_{\\mathbf{s}_{S}}\\tilde{\\mathcal{L}}(\\mathbf{s}_{S})\\geq0$ for any support $\\boldsymbol{S}$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. The result is a direct consequence of meeting the $m$ -convexity and $M$ -smoothness conditions of Lemmas 7, 14 above, and the OMP approximation guarantee of Theorem 11 due to Elenberg et al. [21]. Note that the condition on $2r\\leq n$ comes from the RSC condition in Theorem 11 being required for subsets of size $2r$ . As we proved bounds for the $m$ -convexity of the full Hessian of size $n,r$ has to be smaller than $n/2$ for the assumptions of the theorem to hold. Regarding the upper bound $s_{\\mathrm{max}}$ on s, we note that the constraint is convex and therefore doesn\u2019t change the convexity property of the optimization problem. ", "page_idx": 20}, {"type": "text", "text": "Further, note that $\\begin{array}{r}{\\operatorname*{max}_{\\rho_{S}}\\mathcal{L}(\\rho_{S})\\leq\\operatorname*{max}_{\\rho_{S\\cup i}}\\mathcal{L}(\\rho_{S\\cup i})}\\end{array}$ , since the additional non-zero element could stay at 0, if the marginal likelihood does not improve with $\\rho_{i}$ increasing. That is, the subset selection problem is monotonic. As a consequence, we can normalize the MLL by $\\tilde{\\mathcal{L}}(\\cdot)=\\mathcal{L}(\\cdot)-\\mathcal{L}(\\mathbf{0})$ , which then only takes positive values for any $\\begin{array}{r}{\\mathbf{s}_{S}^{*}=\\arg\\operatorname*{max}_{\\mathbf{s}_{\\setminus S}=0}\\mathcal{L}(\\pmb{\\rho}(\\mathbf{s}_{S}))}\\end{array}$ , i.e. $\\operatorname*{max}_{\\mathbf{s}_{S}}\\tilde{\\mathcal{L}}(\\mathbf{s}_{S})\\geq0$ . This normalization is required for the constant factor approximation guarantee to apply, similar to the original work of Nemhauser. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "This theoretical approach could lead to approximation guarantees for Tipping [55]\u2019s Sparse Bayesian Learning (SBL) model, for which Ament and Gomes [8] show that greedily optimizing the associated NMLL is equivalent to stepwise regression in the limit of $\\sigma\\rightarrow0$ , proving exact recovery guarantees. ", "page_idx": 20}, {"type": "text", "text": "D Additional Detail on the Experiments ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Our benchmark uses a modified version of the code from Andrade and Takeda [9], available at https://github.com/andrade-stats/TrimmedMarginalLikelihoodGP under the GNU GPLv2 license. ", "page_idx": 21}, {"type": "text", "text": "D.1 Synthetic Regression Problems ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Model fitting runtimes Fig. 8 summarizes the ftiting times for the different models on the different scenarios from Section 6.1. We observe that the outlier type and the fraction of outliers both have relatively limited effect on the fitting times for all of the models. Fig. 9 therefore provides a more compact view of the same data (aggregated across outlier types and outlier fractions), giving a better sense of the distribution of the ftiting times. Unsurprisingly, the baselines that simply fti a single GP model (\u201cvanilla\u201d, \u201cwinsorize\u201d, \u201cpower_transform\u201d) are substantially faster than any of the robust approaches. While all of the robust models show similar ftiting times on the Hartmann problem, ftiting our RRP is significantly faster (note the logarithmic scale of the y-axis) than ftiting the Student- $\\cdot t$ and trimmed MLE models on the 5-dim and 10-dim Friedman problems. The trimmed MLE model in particular ends up being quite slow, especially on the 5d Friedman function. ", "page_idx": 21}, {"type": "text", "text": "D.2 BO experiments, additional details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "For Hartmann6, we consider the standard domain of $[0,1]^{6}$ . ", "page_idx": 21}, {"type": "text", "text": "SVM The SVM problem, the goal is to optimize the test RMSE of an SVM regression model trained on 100 features from the CT slice UCI dataset. We tune the following three parameters: $C\\in$ [1e-2, 1e2], $\\epsilon\\in[1\\mathrm{e}{-2},1]$ , and $\\gamma\\in[1\\mathrm{e}{-3}$ , 1e-1]. All parameters are tuned in log-scale. Corruptions simulate I/O failures in which case we only train on $U[100,1000]$ training points out of the available 50, 000 training observations. ", "page_idx": 21}, {"type": "text", "text": "CNN For the 5D CNN problem the goal is to optimize the test accuracy of a CNN classifier trained on the MNIST dataset. We tune the following 5 parameters: learning rate in the interval [1e-4, 1e-1], momentum in the interval [0, 1], weight decay in the interval [0, 1], step size in the interval [1, 100], and $\\gamma\\in[0,1]$ . Similarly to the SVM problem, we only train on $U[100,1000]$ of the available training batches when an $_\\mathrm{I/O}$ failure occurs. ", "page_idx": 21}, {"type": "text", "text": "Rover The rover trajectory planning problem was originally proposed in Wang et al. [60]. The goal is to tune the trajectory of a rover in order to maximize the reward of its final trajectory. We use the same obstacle locations and trajectory parameterization as in Maus et al. [42], with the main difference being that we parameterize the trajectory using 10 points, resulting in 20 tunable parameters. When a corruption occurs, the rover will stop at a uniformly random point along its trajectory, generally resulting in lower reward than the original trajectory. ", "page_idx": 21}, {"type": "text", "text": "D.3 Additional Synthetic BO Results ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In addition to the results in Fig. 6, we also include results when the corruption probability is $20\\%$ in Fig. 10. We observe that the results are similar as with $10\\%$ corruption probability, but that the performance of several baselines regresses significantly. ", "page_idx": 21}, {"type": "text", "text": "D.4 Computational Setup and Requirements ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Robust GP regression is very data-efficient, focuses on the small-data regime, and runs fast (faster than competing baselines studied in this paper). Therefore, each individual run required very limited compute resources (this includes the baseline methods). To produce statistically meaningful results, however, we ran a large number of replications for both our regression and Bayesian optimization benchmarks on a proprietary cluster. We estimate the amount of compute spent on these experiments to be around 2 CPU years in total, using standard (Intel Xeon) CPU hardware. The amount of compute spent on exploratory investigations as part of this work was negligible (this was ad-hoc exploratory and development work on a single CPU machine). ", "page_idx": 21}, {"type": "image", "img_path": "5FATPIlWUJ/tmp/c3c6f93a16006e33f36c02d2ca560e55760089b09c49cbfc7ae915d69379088c.jpg", "img_caption": ["Figure 8: Fitting times of the different robust GP modeling approaches. The plots show means and one standard deviation. Here \u201cbkwd\u201d indicates the backward variant of RRP (Algorithm 2), and \u201ccan\u201d indicates the canonical (non-convex) parameterization. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "D.5 Impact of Convex Parameterization on Joint Optimization of Hyper-Parameters ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "A limitation of Lemma 4 is that it only guarantees convexity for the sub-problem of optimizing the $\\rho_{i}$ \u2019s. In practice, we jointly optimize all GP hyper-parameters, including length scales. In this case, the theory guarantees the positive-definiteness of the submatrix of the Hessian corresponding to $\\rho(\\mathbf{s})$ , and we expect this to improve the quality of the results of the numerical optimization routines. ", "page_idx": 22}, {"type": "text", "text": "Indeed, positive-definiteness is beneficial for quasi-Newton optimization algorithms like L-BFGS [38], which restarts its approximation to the Hessian whenever it encounters non-convex regions, because the associated updates to the Hessian approximation are not positive-definite. This leads the algorithm to momentarily revert back to gradient descent, with an associated slower convergence rate. ", "page_idx": 22}, {"type": "text", "text": "To quantify the impact of this, we ran convergence analyses using the data from Fig. 1, allowing all $\\rho$ to be optimized jointly with other hyper-parameters (length scale, kernel variance and noise variance), recording the achieved negative log marginal likelihood (NLML) as a function of the tolerance parameter ftol of the L-BFGS optimizer. The results are reported in Table D.5, and indicate that the optimizer terminates with a much better NLML using the convex parameterization with the same convergence tolerance. ", "page_idx": 22}, {"type": "image", "img_path": "5FATPIlWUJ/tmp/226a7bee083ae33ebfc047acda9bfc88290e5acdd8b757b1d70fa6dcc9aa46e2.jpg", "img_caption": ["friedman5", "Figure 9: Fitting times of the different robust GP modeling approaches on the regression tasks from Section 6.1. Results are aggregated across outlier types and outlier fractions as those do not affect fitting times much (see Fig. 8). "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "5FATPIlWUJ/tmp/e571f1597d100a5fd5b0affefe377e16b91819a276de8baa8faa469ab10ab452.jpg", "img_caption": ["Figure 10: BO results for Hartmann6: Left: We see that Relevance pursuit performs well in the case of constant outliers of value 100 and almost performs just as well as the oracle. Middle: Relevance pursuit performs the best followed by the Student- $\\cdot t$ likelihood from [41] in the case of $U[-3,3]$ . No method performs as well as the oracle when the outlier probability is $20\\%$ . Right: Similarly to the middle column, this setting hides the outliers within the range of the outliers making it difficult to match the performance of the oracle. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "There are also settings in which we do not actually jointly optimize the hyper-parameters, particularly when we have access to data from the same data generating process that has been manually labeled ", "page_idx": 23}, {"type": "table", "img_path": "5FATPIlWUJ/tmp/9004a35ea42ca4a49ef33b21ab980576182524b7fac3b448e12da431f1667c30.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Table 1: Comparison of the negative log marginal likelihood achieved after numerical optimization of the canonical and convex parameterization of $\\rho$ with L-BFGS. Notably, the convex parameterization leads to improved marginal likelihood values for for a given convergence tolerance ftol. ", "page_idx": 24}, {"type": "text", "text": "by domain experts as outlier-free. Then we can estimate the model hyper-parameters on that data, and fix them for the RRP on new data sets that we do not know to be outlier-free. ", "page_idx": 24}, {"type": "text", "text": "D.6 Comparison to Robust and Conjugate Gaussian Processes (RCGP) ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We report extended empirical comparisons with Altamirano et al. [2]\u2019s RCGP method, using their experimental setup and method implementation in GPFLow. Including GPFlow in our own benchmarking setup and compute resources proved difficult. To circumvent this, we wrote wrappers for both BoTorch\u2019s standard GP and RRP, which also accounts for any orthogonal implementation differences between the two frameworks, and ran the benchmarks locally on an M-series MacBook. ", "page_idx": 24}, {"type": "text", "text": "See Tables 2 and 3 for the mean absolute error and negative log predictive density, respectively. The tables include the empirical mean and standard deviation over 20 replications on corrupted version of the following base data sets: 1) Synthetic, which is generated as a draw of a GP with a exponentiated quadratic kernel, and four data sets available on the UCI machine learning repository[32], in particular, 2) Boston [27], 3) Concrete [63], 4) Energy [58], and 5) Yacht [25]. The benchmark considers no corruptions (\u201cNo Outliers\u201d), \u201cAsymmetric Outliers\u201d, which are uniform in $x$ are shifted negatively in $y$ , \u201cUniform Outliers\u201d, which shift $y$ in both directions (positively and negatively), and \u201cFocused Outliers\u201d, which form concentrated clusters in both $x$ and $y$ . Any bold entry in the table signifies a results that is within one standard-error of the best result\u2019s one standard-error confidence bound. ", "page_idx": 24}, {"type": "table", "img_path": "5FATPIlWUJ/tmp/26d9d1726c9071f6073ca4ce41674dc341ea8790a6dbdc8c12d444ab71711fe1.jpg", "table_caption": ["Table 2: Mean absolute error (MAE) using Altamirano et al. [2]\u2019s experimental setup in GPFlow. RRP is always competitive with the other methods, and outperforms them significantly for uniform and asymmetric outliers. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "D.7 Comparison to Robust Gaussian Process with Huber Likelihood ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In the following, we compare our method with additional variational GP baselines with Laplace and Huber likelihoods, and translated the Matlab code of the \"projection statistics\" of Algikar and Mili [1] to PyTorch. We then combined the projection-statistics-based weighting of the Huber loss with a variational (referred to as Huber-Projection) to get as close as possible to a direct comparison to Algikar and Mili [1] without access to a Matlab license. ", "page_idx": 24}, {"type": "table", "img_path": "5FATPIlWUJ/tmp/940aba9f841e13651816ded7fb16674e7ae58a2ff6d7522c7833b3e0cd4c4db3.jpg", "table_caption": ["Table 3: Negative log predictive density (NLPD) using Altamirano et al. [2]\u2019s experimental setup in GPFlow. RRP is generally competitive, and outperforms other methods significantly for uniform and asymmetric outliers. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "Tables 4 and 5 shows the root mean square error and negative log predictive density on the Neal, Friedman 5 and Friedman 10 test functions, as well as the Yacht Hydrodynamics [25] and California Housing [46] datasets from the UCI database [32], where $15\\%$ of the training data sets of the models were corrupted. Tables 6 and 7 below were generated in a similar way, but $100\\%$ of the data were subject to heavier-tailed noise, either Student- $\\cdot t$ or Laplace. ", "page_idx": 25}, {"type": "text", "text": "In summary, the Relevance Pursuit model generally outperforms the variational GPs with heavy-tailed likelihoods when the corruptions are a sparse subset of all observations. Unsurprisingly, the GPs with heavy-tailed likelihoods perform best when all observations are subject to heavy-tailed noise. While such uniformly heavy-tailed noise does exist in practice, we stress that this is a distinct setting to the common setting where datasets contain a subset of a-priori unknown outliers, while a dominant fraction of the data can be considered inliers that, once they are identified, can be used to train a model without additional treatment. ", "page_idx": 25}, {"type": "table", "img_path": "5FATPIlWUJ/tmp/a50f9a88686ca5b4932a072183177218d91beebfa2a00f500ff2d167b3b0488f.jpg", "table_caption": ["Table 4: Comparison with Huber GP: Root mean square error with $15\\%$ Corruptions "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "5FATPIlWUJ/tmp/674ebc4d2e68cc6a5b11aa0635de35da037f6c23134e6cd488acc119384e3836.jpg", "table_caption": ["Table 5: Comparison with Huber GP: Negative log predictive density with $15\\%$ Corruptions "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "5FATPIlWUJ/tmp/205017cfbd22cf5fb2553e7f8213451e99fd3a69e3abfdf09991d52baf580459.jpg", "table_caption": ["Table 6: Comparison with Huber GP: Root mean squared error for $100\\%$ Laplace noise "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "5FATPIlWUJ/tmp/aa9693b82fef2176431477cd5d690c0ecb4d18442623628796477a8687cc8c74.jpg", "table_caption": ["Table 7: Comparison with Huber GP: Negative log predictive density with $100\\%$ Laplace noise "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: Everything that is claimed in the abstract and introduction is discussed in detail in the rest of the paper. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: Limitations of the work are mentioned throughout the paper as well as summarized in the conclusion. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: All theoretical results clearly state the required assumptions; proofs are provided in Appendix C. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Code for our method as well as for the benchmarking setup is provided in the supplementary material, and core models and algorithms will be open-sourced upon publication. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Code for reproducing the results in the paper is included in the supplementary material. All data sets are publicly available and referenced in the work and the code. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Details on the experiments are provided in the experimental section as well as the Appendix. Exact implementation details are contained in the included code submission. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Variance in the results is provided throughout in the form of confidence intervals and/or violin plots. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 30}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Total compute resources spent are described in Section D.4 and are estimated at around 2 CPU years in total. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The research fully conforms to the NeurIPS Code of ethics. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper makes foundational methodological contributions to robust probabilistic regression, and we do not see any direct societal impacts of the work that would require specific discussion. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper poses no such risks. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: All creators or original owners of the code and data sets that were used in this paper are properly cited and credited. Licenses for these assets are mentioned in Sec. D and our use complies with those licenses. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The only new asset introduced in this paper is the code for method and benchmarks; it does not introduce any other assets (data sets or models). The code is provided as an anonymized zip flie for the reviewers. Upon publication, the code released as open source and documented according to community standards. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]