[{"figure_path": "V2e0A2XIPF/tables/tables_6_1.jpg", "caption": "Table 1: Image classification results on ImageNet-1k dataset. QT-ViTs are compared with SOTA baselines. Methods are grouped based on FLOPs.", "description": "This table presents a comparison of the proposed QT-ViT models against state-of-the-art vision transformers and CNNs on the ImageNet-1k image classification benchmark.  The models are categorized into groups based on their FLOPs (floating-point operations), and their performance is evaluated using Top-1 and Top-5 accuracy metrics.  The table also shows the number of parameters for each model.", "section": "4.1 Image Classification"}, {"figure_path": "V2e0A2XIPF/tables/tables_7_1.jpg", "caption": "Table 2: Results of using different kernels. The baseline method uses the original self-attention operation with O(N2d) computational complexity and is used as the strong baseline. Other methods use different linear attentions.", "description": "This table compares the performance of different kernel functions used in linear attention mechanisms within the context of vision transformers.  It shows the top-1 accuracy achieved by various methods, each using a different kernel function to approximate the softmax attention.  The baseline represents the standard softmax attention with quadratic complexity, while the others use linear approximations with varying kernels (ReLU, cosine similarity, mean, sigmoid & softmax, angular, 1st order Taylor expansion). The table highlights the improvement in accuracy achieved by the proposed QT-ViT method using a 2nd order Taylor expansion kernel.", "section": "4.2 Ablation Study"}, {"figure_path": "V2e0A2XIPF/tables/tables_7_2.jpg", "caption": "Table 3: Ablation on reducing the time complexity of the Kronecker product. The experiments are conducted using the QT-ViT-1 model on the ImageNet-1k dataset.", "description": "This table presents the ablation study results focusing on the impact of different methods to reduce the time complexity of the Kronecker product within the QT-ViT model. It compares different approaches, including pooling the input vector, dividing it into chunks, randomly preserving elements, and using the proposed compact version (ours), assessing their effects on computational complexity and top-1 accuracy on ImageNet-1k.", "section": "4.2 Ablation Study"}, {"figure_path": "V2e0A2XIPF/tables/tables_12_1.jpg", "caption": "Table 4: Experimental results on COCO 2017 dataset using different backbones.", "description": "This table presents a comparison of object detection results on the COCO 2017 dataset using different backbone models.  It shows the mean Average Precision (AP), AP at IoU threshold of 0.5 (AP50), AP at IoU threshold of 0.75 (AP75), and the number of parameters (in millions) for EfficientViT and QT-ViT models of varying sizes (B1, B2, B3). The results highlight the performance improvement achieved by QT-ViT compared to EfficientViT.", "section": "A Object Detection on COCO 2017 Dataset"}, {"figure_path": "V2e0A2XIPF/tables/tables_12_2.jpg", "caption": "Table 5: Experimental results on COCO 2017 dataset using different backbones.", "description": "This table presents the experimental results of object detection on the COCO 2017 dataset using different backbones. It compares the performance of EfficientViT and QT-ViT models with and without absolute positional embedding (APE). The metrics used for comparison are AP (Average Precision), AP50 (Average Precision at IoU=0.5), and AP75 (Average Precision at IoU=0.75).", "section": "A Object Detection on COCO 2017 Dataset"}, {"figure_path": "V2e0A2XIPF/tables/tables_13_1.jpg", "caption": "Table 6: The effectiveness of APE.", "description": "This table presents the results of adding absolute positional embedding (APE) to the QT-ViT models for the semantic segmentation task on the ADE20K dataset.  It compares the performance (mIoU and mAcc) and the number of parameters of QT-ViT models with and without APE, showing that using APE improves performance.", "section": "B Semantic Segmentation on ADE20K dataset"}, {"figure_path": "V2e0A2XIPF/tables/tables_13_2.jpg", "caption": "Table 7: The impact of using original softmax attention during training.", "description": "This table shows the impact on GPU memory usage and top-1 accuracy when using the original softmax attention during training versus not using it.  The results indicate a significant increase in memory usage (13.7%) with minimal improvement in accuracy.", "section": "4.2 Ablation Study"}]