{"references": [{"fullname_first_author": "Francesca Babiloni", "paper_title": "Poly-nl: Linear complexity non-local layers with 3rd order polynomials", "publication_date": "2021-00-00", "reason": "This paper is highly relevant due to its focus on linear complexity non-local layers, a key aspect in improving the efficiency of vision transformers."}, {"fullname_first_author": "Daniel Bolya", "paper_title": "Hydra attention: Efficient attention with many heads", "publication_date": "2022-00-00", "reason": "This work directly addresses the efficiency challenges of self-attention mechanisms in vision transformers, proposing the 'Hydra' approach."}, {"fullname_first_author": "Han Cai", "paper_title": "Efficientvit: Enhanced linear attention for high-resolution low-computation visual recognition", "publication_date": "2022-00-00", "reason": "This paper's introduction of 'EfficientViT' provides a strong baseline for comparison and context for the proposed method."}, {"fullname_first_author": "Krzysztof Choromanski", "paper_title": "Rethinking attention with performers", "publication_date": "2020-00-00", "reason": "The 'Performer' method, introduced here, offers an alternative to traditional self-attention mechanisms, and its performance is benchmarked in the paper."}, {"fullname_first_author": "Zihang Dai", "paper_title": "Coatnet: Marrying convolution and attention for all data sizes", "publication_date": "2021-00-00", "reason": "This paper presents 'CoAtNet', a hybrid CNN-transformer architecture, which is used for comparison in the paper, highlighting the state-of-the-art in vision transformer design."}]}