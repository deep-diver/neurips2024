[{"figure_path": "V2e0A2XIPF/figures/figures_1_1.jpg", "caption": "Figure 1: The accuracy-speed trade-offs of the proposed QT-ViTs and other state-of-the-art transformer models on the ImageNet dataset. Latencies are evaluated on the AMD Instinct MI250 GPU.", "description": "This figure compares the accuracy and speed of various vision transformer models, including the proposed QT-ViT models, on the ImageNet dataset.  The x-axis represents the latency (in milliseconds) of each model, indicating the inference speed, while the y-axis represents the ImageNet top-1 accuracy.  The plot shows the accuracy-speed trade-off for each model, illustrating how different models balance accuracy and computational efficiency.  QT-ViTs are demonstrated to achieve state-of-the-art results by achieving a new Pareto front, meaning they significantly outperform the compared models in terms of accuracy for a given speed or provide faster inference for a given accuracy level.", "section": "Experiments"}, {"figure_path": "V2e0A2XIPF/figures/figures_8_1.jpg", "caption": "Figure 2: Attention maps from different linear attention methods including the first-order Taylor expansion, ReLU non-linearity function and the second-order Taylor expansion (ours).", "description": "This figure visualizes attention maps generated by different linear attention methods, including the first-order Taylor expansion, ReLU non-linearity function and the proposed second-order Taylor expansion, focusing on four different image categories: Ball, Bird, Dog, and Taxi.  For each category, a specific query region (marked in red) is selected, and the resulting attention maps highlight which areas of the image the model focuses on in relation to the query. The visualizations show how the different methods produce varying degrees of focus and sharpness in their attention maps, with the proposed method showing a more focused and sharper response.", "section": "4.3 Visualization"}]