[{"type": "text", "text": "MADIFF: Offline Multi-agent Learning with Diffusion Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhengbang Zhu1 Minghuan Liu1 Liyuan Mao1 Bingyi Kang2 Minkai $\\mathbf{X}\\mathbf{u}^{3}$ Yong $\\mathbf{Y}\\mathbf{u}^{1}$ Stefano Ermon3 Weinan Zhang1\u2020 ", "page_idx": 0}, {"type": "text", "text": "1 Shanghai Jiao Tong University, 2 ByteDance, 3 Stanford University {zhengbangzhu, minghuanliu, maoliyuan, yyu, wnzhang}@sjtu.edu.cn, bingykang@gmail.com, {minkai, ermon}@cs.stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Offline reinforcement learning (RL) aims to learn policies from pre-existing datasets without further interactions, making it a challenging task. Q-learning algorithms struggle with extrapolation errors in offline settings, while supervised learning methods are constrained by model expressiveness. Recently, diffusion models (DMs) have shown promise in overcoming these limitations in single-agent learning, but their application in multi-agent scenarios remains unclear. Generating trajectories for each agent with independent DMs may impede coordination, while concatenating all agents\u2019 information can lead to low sample efficiency. Accordingly, we propose MADIFF, which is realized with an attention-based diffusion model to model the complex coordination among behaviors of multiple agents. To our knowledge, MADIFF is the first diffusion-based multi-agent learning framework, functioning as both a decentralized policy and a centralized controller. During decentralized executions, MADIFF simultaneously performs teammate modeling, and the centralized controller can also be applied in multi-agent trajectory predictions. Our experiments demonstrate that MADIFF outperforms baseline algorithms across various multi-agent learning tasks, highlighting its effectiveness in modeling complex multi-agent interactions. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Offline reinforcement learning (RL) [Fujimoto et al., 2019, Kumar et al., 2020] learns exclusively from static datasets without online interactions, enabling the effective use of pre-collected large-scale data. However, applying temporal difference (TD) learning in offline settings causes extrapolation errors [Fujimoto et al., 2019], where target value functions are evaluated on out-of-distribution actions. Sequence modeling algorithms bypass TD-learning by directly ftiting the dataset distribution [Chen et al., 2021, Janner et al., 2021]. Nevertheless, these methods are limited by the model\u2019s expressiveness, making it difficult to handle diverse datasets. They also suffer from compounding errors [Xiao et al., 2019] due to autoregressive generation. Recently, diffusion models (DMs) have achieved remarkable success in various generative modeling tasks [Song and Ermon, 2019, Ho et al., 2020, Xu et al., 2022], owing to their exceptional abilities at capturing complex, high-dimensional data distributions. Their successes have also been introduced into offilne RL, offering a superior modeling choice for sequence modeling algorithms [Janner et al., 2022, Ajay et al., 2023]. ", "page_idx": 0}, {"type": "text", "text": "Compared to single-agent learning, offline multi-agent learning (MAL) has been less studied and is more challenging. Since the behaviors of all agents are interrelated, each agent is required to model interactions and coordination among agents, while making decisions in a decentralized manner to achieve the goal. Current MAL approaches typically train a centralized value function to update individual agents\u2019 policies [Rashid et al., 2020] or use an autoregressive transformer to determine each agent\u2019s actions [Meng et al., 2021, Wen et al., 2022]. However, without online interactions, an incorrect centralized value can lead to significant extrapolation errors, and the transformer can only serve as an independent model for each agent. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this paper, we aim to study the potential of employing DMs to solve the above challenges in offilne MAL problems. Merely adopting existing diffusion RL methods by using independent DMs to model each agent can result in serious inconsistencies due to a lack of proper credit assignment among agents. Another possible solution is to concatenate all agents\u2019 information as the input and output of the DM. However, treating the agents as a single unified agent neglects the important nature of multi-agent systems. One agent may have strong correlations with only a few other agents, which makes a full feature interaction redundant. In many multi-agent systems, agents exhibit certain symmetry and can share model parameters for efficient learning [Arel et al., 2010]. However, concatenating them in a fixed order breaks this symmetry, forcing the model to treat each agent differently. ", "page_idx": 1}, {"type": "text", "text": "To address the aforementioned coordination challenges, we propose the first centralized-trainingdecentralized-execution (CTDE) diffusion framework for MA problems, named MADIFF. MADIFF adopts a novel attention-based DM to learn a return-conditional trajectory generation model on a reward-labeled multi-agent interaction dataset. In particular, the designed attention is computed in several latent layers of the model of each agent to fully interchange the information and integrate the global information of all agents. To model the coordination among agents, MADIFF applies the attention mechanism on latent embedding for information interaction across agents. The attention mechanism enables the dynamic modeling of agent interactions through learned weights, while also enabling the use of a shared backbone to model each agent\u2019s trajectory, significantly reducing the number of parameters. During training, MADIFF performs centralized training on the joint trajectory distributions of all agents from offline datasets, including different levels of expected returns. During inference, MADIFF adopts classifier-free guidance with low-temperature sampling to generate behaviors given the conditioned high expected returns, allowing for decentralized execution by predicting the behavior of other agents and generating its own behavior. Therefore, MADIFF can be regarded as a principled offline MAL solution that not only serves as a decentralized policy for each agent or a centralized controller for all agents, but also includes teammate modeling without additional cost. Comprehensive experiments demonstrated superior performances of MADIFF on various multi-agent learning tasks, including offline MARL and trajectory prediction. ", "page_idx": 1}, {"type": "text", "text": "In summary, our contributions are (1) the first diffusion-based multi-agent learning framework that unifies decentralized policy, centralized controller, teammate modeling, and trajectory prediction; (2) a novel attention-based DM structure that is designed explicitly for MAL and enables coordination among agents in each denoising step; (3) achieving superior performances for various offilne multiagent problems. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Multi-agent Offline Reinforcement Learning ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We consider a partially observable and fully cooperative multi-agent learning (MAL) problem, where agents with local observations cooperate to finish the task. Formally, it is defined as a DecPOMDP [Oliehoek and Amato, 2016]: $\\boldsymbol{\\bar{G}}\\,=\\,\\langle\\boldsymbol{S},\\boldsymbol{\\mathcal{A}},P,r,\\Omega,O,N,U,\\gamma\\rangle$ , where $\\boldsymbol{S}$ and $\\boldsymbol{\\mathcal{A}}$ denote state and action space separately, and $\\gamma$ is the discounted factor. The system includes $N$ agents $\\{1,2,\\ldots,N\\}$ act in discrete time steps, and starts with an initial global state $s_{0}\\in\\mathcal{S}$ sampled from the distribution $U$ . At each time step $t$ , every agent $i$ only observes a local observation $o^{i}\\,\\in\\,\\Omega$ produced by the function $O(s,a):\\mathcal{S}\\times\\mathcal{A}\\to\\Omega$ and decides $a\\in A$ , which forms the joint action $\\mathbf{\\dot{a}}\\in\\mathcal{A}\\equiv\\mathbf{\\dot{\\mathcal{A}}}^{N}$ , leading the system transits to the next state $s^{\\prime}$ according to the dynamics function $P(s^{\\prime}|s,\\mathbf{a})\\,:\\,S\\times\\mathbf{\\mathcal{A}}\\to\\mathcal{S}$ . Normally, agents receive a shared reward $r(s,\\mathbf{a})$ at each step, and the optimization objective is to learn a policy $\\pi^{i}$ for each agent that maximizes the discounted cumulative reward $\\begin{array}{r}{\\underline{{\\bar{\\mathbb{E}}}}_{s_{t},\\mathbf{a}_{t}}\\bigl[\\sum_{t}\\gamma^{t}r\\bigl(s_{t},\\mathbf{a}_{t}\\bigr)\\bigr]}\\end{array}$ . In offline settings, instead of collecting online data in environments, we only have access to a static dataset $\\mathcal{D}$ to learn the policies. The dataset $\\mathcal{D}$ is generally composed of trajectories $\\tau$ , i.e., observation-action sequences $\\left[\\pmb{o}_{0},\\pmb{a}_{0},\\pmb{o}_{1},\\pmb{a}_{1},\\cdot\\cdot\\cdot\\mathbf{\\nabla},\\pmb{o}_{T},\\pmb{a}_{T}\\right]$ or observation sequences $[o_{0},o_{1},\\cdot\\cdot\\cdot\\,,o_{T}]$ . We use bold symbols to denote the joint vectors of all agents. ", "page_idx": 1}, {"type": "image", "img_path": "PvoxbjcRPT/tmp/5f56ba4bf08dfb1afce0952bacdac95e9e36e8df96041481f06c7429914884e8.jpg", "img_caption": ["Figure 1: The architecture of MADIFF, which is an attention-based diffusion network framework that performs attention across all agents at every decoder layer of each agent. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "2.2 Diffusion Probabilistic Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Diffusion models (DMs) [Sohl-Dickstein et al., 2015, Song and Ermon, 2019, Ho et al., 2020], as a powerful class of generative models, implement the data generation process as reversing a forward noising process (denoising process). For each data point $x_{0}\\sim p_{\\mathrm{data}}(x)$ from the\u221a dataset $\\mathcal{D}$ , the noising process is a discrete Markov chain $x_{0:K}$ such that $p(x_{k}|x_{k-1})\\,=\\,\\mathcal{N}(x_{k}|\\sqrt{\\alpha_{k}}x_{k-1},(1-\\alpha_{k})I),$ where $\\mathcal{N}(\\mu,\\Sigma)$ denotes a Gaussian distribution with mean $\\mu$ and variance $\\Sigma$ , and $\\alpha_{0:K}\\in\\mathbb{R}$ are hyperparameters which control the variance schedule. The variational reverse Markov chain is parameterized with $q_{\\theta}(x_{k-1}|x_{k})\\,=\\,\\mathcal{N}(x_{k-1}|\\mu_{\\theta}(x_{k},k),(1-\\alpha_{k})I)$ . The data sampling process begins by sampling an initial noise $x_{K}\\sim\\mathcal{N}(0,I)$ , and follows the reverse process until $x_{0}$ . The reverse process can be estimated by optimizing a simplified surrogate loss as in Ho et al. [2020]: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta)=\\mathbb{E}_{k\\sim[1,K],x_{0}\\sim q,\\epsilon\\sim\\mathcal{N}(0,I)}\\left[\\left\\|\\epsilon-\\epsilon_{\\theta}\\left(x_{k},k\\right)\\right\\|^{2}\\right]\\;.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The estimated Gaussian mean can be written as $\\begin{array}{r}{\\mu_{\\theta}(x_{k},k)=\\frac{1}{\\sqrt{\\alpha_{k}}}\\left(x_{k}-\\frac{1-\\alpha_{k}}{\\sqrt{1-\\bar{\\alpha}_{k}}}\\epsilon_{\\theta}(x_{k},k)\\right)}\\end{array}$ , where $\\bar{\\alpha}_{k}=\\Pi_{s=1}^{k}\\alpha_{s}$ . ", "page_idx": 2}, {"type": "text", "text": "2.3 Diffusing Decision Making ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Diffusing over state trajectories and acting with inverse dynamics model. Among existing works in single-agent learning, Janner et al. [2022] chose to diffuse over state-action sequences, so that the generated actions for the current step can be directly used for executing. Another choice is diffusing over state trajectories only [Ajay et al., 2023], which is claimed to be easier to model and can obtain better performance due to the less smooth nature of action sequences: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{\\tau}:=\\big[s_{t},\\hat{s}_{t+1},\\cdot\\cdot\\cdot\\,,\\hat{s}_{t+H-1}\\big],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $t$ is the sampled time step and $H$ denotes the trajectory length (horizon) modeled by DMs. But the generated state sequences can not provide actions to be executed during online evaluation. Therefore, an inverse dynamics model is trained to predict the action $\\hat{a}_{t}$ that makes the state transit from $s_{t}$ to the generated next state $\\hat{s}_{t+1}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{a}_{t}=I_{\\phi}\\big(s_{t},\\hat{s}_{t+1}\\big)\\;.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Therefore, at every environment step $t$ , the agent first plans the state trajectories using an offlinetrained DM, and infers the action with the inverse dynamics model. ", "page_idx": 2}, {"type": "text", "text": "Classifier-free guided generation. For targeted behavior synthesis, DMs should be able to generate future trajectories by conditioning the diffusion process on an observed state $s_{t}$ and information $y$ . We use classifier-free guidance [Ho and Salimans, 2022] which requires taking $y(\\tau)$ as additional inputs for the diffusion model. Formally, the sampling procedure starts with Gaussian noise $\\hat{\\tau}_{K}\\sim\\mathcal{N}(0,\\alpha I)$ , and diffuse $\\hat{\\tau}_{k}$ into $\\hat{\\tau}_{k-1}$ at each diffusion step $k$ . Here $\\alpha\\,\\in\\,[0,1)$ is the scaling factor used in low-temperature sampling to scale down the variance of initial samples [Ajay et al., 2023]. We use $\\tilde{x}_{k,t}$ to denote the denoised state $s_{t}$ at $k$ \u2019s diffusion step. $\\hat{\\tau}_{k}$ denotes the denoised trajectory at $k$ \u2019s diffusion step for a single agent: $\\hat{\\tau}_{k}:=\\left[s_{t},\\tilde{x}_{k,t+1},\\cdot\\cdot\\cdot\\,,\\tilde{x}_{k,t+H-1}\\right]$ . Note that for sampling during evaluations, the first state of the trajectory is always set to the current observed state at all diffusion steps for conditioning, and every diffusion step proceeds with the perturbed noise: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\epsilon}:=\\epsilon_{\\theta}(\\hat{\\tau}_{k},\\emptyset,k)+\\omega\\big(\\epsilon_{\\theta}(\\hat{\\tau}_{k},y(\\tau),k)-\\epsilon_{\\theta}(\\hat{\\tau}_{k},\\emptyset,k)\\big)\\;,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\omega$ is a scalar for extracting the distinct portions of data with characteristic $y(\\tau)$ . By iterative diffusing the noisy samples, we can obtain a clean state trajectory: $\\hat{\\tau}_{0}(\\tau):=[s_{t},\\hat{s}_{t+1},\\cdot\\cdot\\cdot\\cdot,\\bar{s}_{t+H-1}]$ . ", "page_idx": 3}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We formulate the problem of MAL as conditional generative modeling: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\theta}\\mathbb{E}_{\\tau\\sim\\mathcal{D}}[\\log p_{\\theta}(\\tau|\\pmb{y}(\\cdot))]~,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $p_{\\theta}$ is learned for estimating the conditional data distribution of joint trajectory $\\tau$ , given information $y(\\cdot)$ , such as observations, rewards, and constraints. When all agents are managed by a centralized controller, $i.e.$ ., the decisions of all agents are made jointly, we can learn the generative model by conditioning the global information aggregated from all agents $\\pmb{y}(\\pmb{\\tau})$ ; otherwise, if we consider each agent $i$ separately and require each agent to make decisions in a decentralized manner, we can only utilize the local information $y^{i}(\\tau^{i})$ of each agent $i$ , including the private information and the common information shared by all (e.g., team rewards). ", "page_idx": 3}, {"type": "text", "text": "3.1 Multi-Agent Diffusion with Attention ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In order to handle MAL problems, agents must learn to coordinate. To solve the challenge of modeling the complex inter-agent coordination in the dataset, we propose a novel attention-based diffusion architecture designed to interchange information among agents. ", "page_idx": 3}, {"type": "text", "text": "In Figure 1, we illustrate the architecture of MADIFF model. In detail, we adopt U-Net as the base structure for modeling agents\u2019 individual trajectories, which consists of repeated one-dimensional convolutional residual blocks. The convolution is performed over the time step dimension, and the observation feature dimension is treated as the channel dimension. To encourage information interchange and improve coordination ability, a critical change is made by adopting attention [Vaswani et al., 2017] layers before all decoder blocks in the U-Nets of all agents. Since embedding vectors from different agents are aggregated by the attention operation rather than concatenations, MADIFF is index-free such that the input order of agents can be arbitrary and does not affect the results. ", "page_idx": 3}, {"type": "text", "text": "Formally, the input to $l$ -th decoder layer in the U-Net of each agent $i$ is composed of two components: the skip-connected feature $c_{l}^{i}$ from the symmetric $l$ -th encoder layer and the embedding $e_{l}^{i}$ from the previous decoder layer. The computation of attention in MADIFF is conducted on $c_{l}^{i}$ rather than $e_{l}^{i}$ since in the U-Net structure the encoder layers are supposed to extract informative features from the input data. We use $\\boldsymbol{c^{\\prime}}_{l}^{i}$ to denote the skip-connected feature after attention operations which aggregate information across agents. We adopt the multi-head attention mechanism to fuse the encoded feature $\\boldsymbol{c^{\\prime}}_{l}^{i}$ with other agents\u2019 information, which is important in effective multi-agent coordination. ", "page_idx": 3}, {"type": "text", "text": "3.2 Centralized Training Objectives ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given a multi-agent offilne dataset $\\mathcal{D}$ , we train MADIFF which is parameterized through the unified noise model $\\epsilon_{\\theta}$ for all agents and the inverse dynamics model $I_{\\phi}^{i}$ of each agent $i$ with the reverse diffusion loss and the inverse dynamics loss: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}(\\boldsymbol{\\theta},\\boldsymbol{\\phi}):=\\displaystyle\\sum_{i}\\mathbb{E}_{(o^{i},a^{i},o^{\\prime i})\\in\\mathcal{D}}[\\|a^{i}-I_{\\phi}^{i}(o^{i},o^{\\prime i})\\|^{2}]}\\\\ &{\\quad\\quad+\\mathbb{E}_{k,\\tau_{0}\\in\\mathcal{D},\\beta}[\\|\\boldsymbol{\\epsilon}-\\boldsymbol{\\epsilon}_{\\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\tau}}_{k},(1-\\beta)\\pmb{y}(\\tau_{0})+\\beta\\boldsymbol{\\phi},\\boldsymbol{k})\\|^{2}]\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\beta$ is sampled from a Bernoulli distribution to balance the training effort on unconditioned and conditioned models. For training the DM, we sample noise $\\epsilon\\,\\sim\\,\\bar{\\mathcal{N}}(\\mathbf{0},I)$ and a time step $k\\sim\\mathcal{U}\\{1,\\cdots,K\\}$ , construct a noise corrupted joint state sequence $\\tau_{k}$ from $\\tau$ and predict the noise $\\hat{\\epsilon}_{\\theta}:=\\epsilon_{\\theta}(\\hat{\\tau}_{k},{\\pmb y}(\\tau_{0}),k)$ . Note that the noisy array $\\hat{\\tau}_{k}$ is applied with the same condition required by the sampling process, as we will discuss in Section 3.3 in detail. As for the inverse dynamics training, we sample the observation transitions of each agent to predict the action. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "It is worth noting that the choice of whether agents should share their parameters of $\\epsilon_{\\theta}^{i}$ and $I_{\\phi^{i}}$ depends on the homogeneous nature and requirements of tasks. If agents choose to share their parameters, only one shared DM and inverse dynamics model are used for generating all agents\u2019 trajectories; otherwise, each agent $i$ has extra parameters (i.e., the U-Net and inverse dynamic models) to generate their states and predict their actions. The attention modules are always shared to incorporate global information into generating each agent\u2019s trajectory. ", "page_idx": 4}, {"type": "text", "text": "3.3 Centralized Control or Decentralized Execution ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Centralized control. A direct and straightforward way to utilize MADIFF in online decision-making tasks is to have a centralized controller for all agents. The centralized controller has access to all agents\u2019 current local observations and generates all agents\u2019 trajectories along with predicting their actions, which are sent to every single agent for acting in the environment. This is applicable for multi-agent trajectory prediction problems and when interactive agents are permitted to be centralized controlled, such as in team games. During the generation process, we sample an initial noise trajectory $\\hat{\\tau}_{K}$ , condition the current joint states of all agents and the global information to utilize ${\\pmb y}({\\pmb\\tau}_{0})$ ; following the diffusion step described in Equation (4) with $\\epsilon_{\\theta}$ , we finally sample the joint observation sequence $\\hat{\\tau}_{0}$ as below: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\underbrace{\\left[\\pmb{o}_{t},\\cdot\\cdot\\cdot,\\tilde{\\pmb{x}}_{K,t+H-1}\\right]}_{\\hat{\\tau}_{K}}\\xrightarrow{K\\mathrm{{steps}}}\\underbrace{\\left[\\pmb{o}_{t},\\cdot\\cdot\\cdot,\\hat{\\pmb{o}}_{t+H-1}\\right]}_{\\hat{\\tau}_{0}}\\,\\,,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where every $\\tilde{\\pmb{x}}_{K,t}\\sim\\mathcal{N}(\\mathbf{0},\\pmb{I})$ is a noise vector sampled from the normal Gaussian. After generation, each agent obtains the action through its own inverse dynamics model following Equation (3) using the current observation $o_{t}^{i}$ and the predicted next observation $\\hat{o}_{t+1}^{i}$ , and takes a step in the environment. We highlight that MADIFF provides an efficient way to generate joint actions and the attention module guarantees sufficient feature interactions and information interchange among agents. ", "page_idx": 4}, {"type": "text", "text": "Decentralized execution with teammate modeling. Compared with centralized control, a more popular and widely-adopted setting is that each agent only makes its own decision without any communication with other agents, which is what most current works [Lowe et al., 2017, Rashid et al., 2020, Wang et al., 2023] dealt with. In this case, we can only utilize the current local observation of each agent $i$ to plan its own trajectory. To this end, the initial noisy trajectory is conditioned on the current observation of the agent $i$ . Similar to the centralized case, by iterative diffusion steps, we finally sample the joint state sequence based on the local observation of agent $i$ as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left[\\tilde{x}_{K,t}^{0},\\cdots,\\tilde{x}_{K,t+H-1}^{0}\\right]}\\\\ {\\underbracket{o_{t}^{i},\\cdots,\\tilde{x}_{K,t+H-1}^{i}}_{\\qquad\\cdot\\cdot\\cdot\\ ,\\ }\\xrightarrow{K\\mathrm{\\tiny~steps}}\\left[\\begin{array}{l}{\\hat{o}_{t}^{0},\\cdots,\\hat{o}_{t+H-1}^{0}}\\\\ {\\cdots\\cdot\\cdot\\cdot,}\\\\ {o_{t}^{i},\\cdots\\cdot\\cdot,\\hat{o}_{t+H-1}^{i}}\\\\ {\\quad\\cdots\\cdot\\cdot,}\\\\ {\\hat{o}_{t}^{N},\\cdots\\cdot,\\hat{o}_{t+H-1}^{N}}\\end{array}\\right]}\\end{array},\\;\\;\\times\\;\\underbrace{\\mathrm{\\tiny~weps}}_{\\begin{array}{c}{\\hat{r}_{t}^{i},\\cdots,\\hat{o}_{t+H-1}^{i}}\\\\ {\\hat{\\tau}_{t}^{i},\\cdots\\cdot,\\hat{o}_{t+H-1}^{i}}\\\\ {\\quad\\cdots\\cdot\\cdot,}\\\\ {\\hat{o}_{t}^{N},\\cdots,\\hat{o}_{t+H-1}^{N}}\\end{array}}\\,\\;,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and we can also obtain the action through the agent $i$ \u2019s inverse dynamics model as mentioned above. An important observation is that, the decentralized execution of MADIFF includes teammate modeling such that the agent $i$ infers all others\u2019 observation sequences based on its own local observation. We show in experiments that this achieves great performances in various tasks, indicating the effectiveness of teammate modeling and the great ability in coordination. ", "page_idx": 4}, {"type": "text", "text": "History-based generation. We find DMs are good at modeling the long-term joint distributions, and as a result MADIFF perform better in some cases when we choose to condition on the trajectory of the past history instead of only the current observation. This implies that we replace the joint observation $\\scriptstyle\\mathbf{0}_{t}$ in Equation (7) as the $C$ -length joint history sequence $\\bar{\\b{h}}_{t}:=[\\pmb{o}_{t-C},\\cdot\\cdot\\cdot\\mathrm{~,~}\\pmb{o}_{t-1},\\cdot\\cdot\\pmb{o}_{t}]$ , and replace the independent observation $o_{t}^{i}$ in Equation (8) as the history sequence $h_{t}^{i}:=[o_{t-C}^{i},\\cdot\\cdot\\cdot\\;,o_{t-1}^{i},\\bar{o}_{t}^{i}]$ of each agent $i$ . Appendix Section D illustrates how agents\u2019 history and future trajectories are modeled by MADIFF in both centralized control and decentralized execution. ", "page_idx": 4}, {"type": "text", "text": "4 Related Work ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Multi-agent Offline RL. While offline RL has become an active research topic, only a limited number of works studied offline MARL due to the challenge of offline coordination. Jiang and Lu [2021] extended BCQ [Fujimoto et al., 2019], a single-agent offline RL algorithm with policy regularization to multi-agent; Yang et al. [2021] developed an implicit constraint approach for offilne Q learning, which was found to perform particularly well in MAL tasks; Pan et al. [2022] argued the actor update tends to be trapped in local minima when the number of agents increases, and correspondingly proposed an actor regularization method named OMAR. All of these Q-learningbased methods naturally have extrapolation error problem [Fujimoto et al., 2019] in offline settings, and their solution cannot get rid of it but only mitigate some. As an alternative, MADT [Meng et al., 2021] formulated offilne MARL as return-conditioned supervised learning, and use a similar structure to a previous transformer-based offilne RL work [Chen et al., 2021]. However, offilne MADT learns an independent model for each agent without modeling agent interactions; it relies on the gradient from centralized critics during online fine-tuning to integrate global information into each agent\u2019s decentralized policy. MADIFF not only avoids the problem of extrapolation error, but also achieves the modeling of collaborative information while allowing CTDE in a completely offline training manner. ", "page_idx": 5}, {"type": "text", "text": "Diffusion Models for Decision-Making. There is a recent line of work applying diffusion models (DMs) to decision-making problems such as RL and imitation learning. Janner et al. [2022] design a diffusion-based trajectory generation model and train a value function to sample high-rewarded trajectories. A consequent work [Ajay et al., 2023] takes conditions as inputs to the DM, thus bringing more flexibility that generates behaviors that satisfy combinations of diverse conditions. Another line of work [Wang et al., 2022, Hansen-Estruch et al., 2023, Kang et al., 2024] uses the DM as a form of policy, i.e., generating actions conditioned on states, and the training objective behaves as a regularization under the framework of TD-based offline RL algorithms. Different from the above, SynthER [Lu et al., 2024] adopts the DM to upsample the rollout data to facilitate learning of any RL algorithms. All of these existing methods focus on solving single-agent tasks. The proposed MADIFF is structurally similar to Ajay et al. [2023], but includes effective modules to model agent coordination in MAL tasks. ", "page_idx": 5}, {"type": "text", "text": "Opponent Modeling in MARL. Our modeling of teammates can be placed under the larger framework of opponent modeling, which refers to the process by which an agent tries to infer the behaviors or intentions of other agents using its local information. There is a rich literature on utilizing opponent modeling in online MARL. Rabinowitz et al. [2018] used meta-learning to build three models, and can adapt to new agents after observing their behavior. SOM [Raileanu et al., 2018] uses the agent\u2019s own goal-conditioned policy to infer other agents\u2019 goals from a maximum likelihood perspective. LIAM [Papoudakis et al., 2021] extracts representations of other agents with variational auto-encoders conditioned on the controlled agent\u2019s local observations. Considering the impact of the ego agent\u2019s policy on other agents\u2019 policies, LOLA [Foerster et al., 2017] and following works [Willi et al., 2022, Zhao et al., 2022] instead model the parameter update of the opponents. Different from these methods, MADIFF can use the same generative model to jointly output plans of its own trajectory and predictions of other agents\u2019 trajectories and is shown to be effective in offline settings. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In experiments, we are aiming at excavating the ability of MADIFF in modeling the complex interactions among cooperative agents, particularly, whether MADIFF is able to (i) generate highquality multi-agent trajectories; (ii) appropriately infer teammates\u2019 behavior; (iii) learn effective, coordinated policies from offline data. ", "page_idx": 5}, {"type": "text", "text": "5.1 Task Descriptions ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We conduct experiments on multiple commonly used multi-agent testbeds. ", "page_idx": 5}, {"type": "text", "text": "\u2022 Multi-agent particle environments (MPE) [Lowe et al., 2017]: multiple 2D particles cooperate to achieve a common goal. Spread, three agents start at some random locations and have to cover three landmarks without collisions; Tag, three predators try to catch a pre-trained prey opponent that moves faster and needs cooperative containment; World, also requires three predators to catch a pre-trained prey, whose goal is to eat the food on the map while not getting caught, and the map has forests that agents can hide and invisible from the outside. ", "page_idx": 5}, {"type": "table", "img_path": "PvoxbjcRPT/tmp/f0b8196056a267679c3577d8e42f6aa89b96514f0f27bf58aacec07c39c23e99.jpg", "table_caption": ["Table 1: The average score on offline MARL tasks. Shaded columns represent our methods. The mean and standard error are computed over 5 different seeds. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "\u2013 Datasets: we use the offline datasets constructed by Pan et al. [2022], including four datasets collected by policies of different qualities trained by MATD3 [Ackermann et al., 2019], namely, Expert, Medium-Replay (Md-Replay), Medium and Random. ", "page_idx": 6}, {"type": "text", "text": "\u2022 Multi-Agent Mujoco (MA Mujoco) [Peng et al., 2021]: independent agents control different subsets of a robot\u2019s joints to run forward as fast as possible. We use three configurations: 2-agent halfcheetah (2halfcheetah), 2-agent ant (2ant), and 4-agent ant (4ant). ", "page_idx": 6}, {"type": "text", "text": "\u2013 Datasets: we use the off-the-grid offline dataset [Formanek et al., 2023], including three datasets with different qualities for each robot control task, e.g., Good, Medium, and Poor. ", "page_idx": 6}, {"type": "text", "text": "\u2022 StarCraft Multi-Agent Challenge (SMAC) [Samvelyan et al., 2019]: a team of either homogeneous or heterogeneous units collaborates to fight against the enemy team that is controlled by the hand-coded built-in StarCraft II AI. We cover four maps: $3m$ , both teams control three Marines; $2s3z$ , both teams control two Stalkers and 3 Zealots; 5m_vs_6m (5m6m), requires controlling five Marines and the enemy team has six Marines; 8m, both teams control eight Marines. ", "page_idx": 6}, {"type": "text", "text": "\u2013 Datasets: we use the off-the-grid offline dataset [Formanek et al., 2023], including three datasets with different qualities for each map, e.g., Good, Medium, and Poor. ", "page_idx": 6}, {"type": "text", "text": "\u2022 Multi-Agent Trajectory Prediction (MATP): different from the former offilne MARL challenges which should learn the policy for each agent, the MATP problem only requires predicting the future behaviors of all agents, and no decentralized model is needed. ", "page_idx": 6}, {"type": "text", "text": "\u2013 NBA dataset: the dataset consists of various basketball players\u2019 recorded trajectories from 631 games in the 2015-16 season. Following Alcorn and Nguyen [2021], we split 569/30/32 training/validation/test games, with downsampling from $25\\:\\mathrm{Hz}$ to $5\\mathrm{Hz}$ . Different from MARL tasks, other information apart from agents\u2019 historical trajectories is available for making predictions, including the ball\u2019s historical trajectories, player ids, and a binary variable indicating the side of each player\u2019s frontcourt. Each term is encoded and concatenated with diffusion time embeddings as side inputs to each U-Net block. ", "page_idx": 6}, {"type": "text", "text": "For offline MARL experiments, we use the episodic return obtained in online rollout as the performance measure. We include MA-ICQ [Yang et al., 2021] and MA-CQL [Kumar et al., 2020] as baselines on all offilne RL tasks. On MPE, we also include OMAR and MA-TD $3+\\mathrm{BC}$ [Fujimoto and Gu, 2021] in baseline algorithms and use the results reported by Pan et al. [2022]. On MA Mujoco, baseline results are adopted from Formanek et al. [2023]. On SMAC, we include MADT [Meng et al., 2021] as a sequence modeling baseline, while other baseline results are reported by Formanek et al. [2023]. We implement independent behavior cloning (BC) as a naive supervised learning baseline. ", "page_idx": 7}, {"type": "text", "text": "We use distance-based metrics including average displacement error (ADE) $\\begin{array}{r}{\\frac{1}{L\\cdot N}\\sum_{t=1}^{L}\\sum_{i=1}^{N}\\|\\hat{o}_{t}^{i}-\\|}\\end{array}$ $o_{t}^{i}\\big|\\big|$ and final displacement error (FDE) $\\begin{array}{r}{\\frac{1}{N}\\sum_{i=1}^{N}\\|\\hat{o}_{L}^{i}-o_{L}^{i}\\|}\\end{array}$ , where $L$ is the prediction length [Li et al., 2020]. We also report $\\mathrm{minADE_{20}}$ and $\\mathrm{minFDE_{20}}$ as additional metrics to balance the stochasticity in sampling, which are the minimum ADE and FDE among 20 predicted trajectories, respectively. We compare MADIFF with Balle $\\cdot2{\\mathrm{Vec}}++$ [Alcorn and Nguyen, 2021], an autoregressive MATP algorithm based on the transformer structure and specifically designed for the NBA dataset. ", "page_idx": 7}, {"type": "text", "text": "5.3 Numerical Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We reported the numerical results both for the CTDE version of MADIFF (denoted as MADIFFD) and the centralized version MADIFF (MADIFF-C). For offline MARL, since baselines are tested in a decentralized style, i.e., all agents independently decide their actions with only local observations, MADIFF-C is not meant to be a fair comparison but to show if MADIFF-D fills the gap for coordination without global information. For MATP, due to its centralized prediction nature, MADIFF-C is the only variant involved. ", "page_idx": 7}, {"type": "text", "text": "Offline MARL. As listed in Table 1, MADIFF-D achieves the best result on most of the datasets. Similar to the single-agent case, direct supervised learning (BC) on the dataset behaves poorly when datasets are mixed quality. Offilne RL algorithms such as MA-CQL that compute conservative values have a relatively large drop in performance when the dataset quality is low. Part of the reason may come from the fact that those algorithms are more likely to fall into local optima in multi-agent scenarios [Pan et al., 2022]. Thanks to the distributional modeling ability of the DM, MADIFF-D generally obtains better or competitive performance compared with OMAR [Pan et al., 2022] without any design for avoiding bad local optima similar to Pan et al. [2022]. On SMAC tasks, MADIFF-D achieves comparable performances, although it is slightly degraded compared with MADIFF-C. ", "page_idx": 7}, {"type": "text", "text": "MATP on the NBA dataset. In Table 2, when comparing ADE and FDE, MADIFF-C significantly outperforms the baseline; however, our algorithm only slightly beats baseline for $\\mathrm{\\minADE_{20}}$ , and has higher $\\mathrm{minFDE_{20}}$ . We suspect the reason is that Baller $\\scriptstyle2\\mathrm{Vec}++$ has a large prediction variance. When Baller $2{\\sf V e c}++$ only predicts one trajectory, a few players\u2019 trajectories deviate from the truth so far that deteriorate the overall ADE and FDE. When allowing to sample 20 times and calculating the minimum ADE/FDE according to the ground truth, Baller $2{\\mathrm{Vec}}++$ can choose the best trajectory for every single agent, which makes minADE20 and $\\mathrm{minFDE_{20}}$ significantly smaller than one-shot metrics. However, considering it may be not practical to select the best trajectories without access to the ground truth, MADIFF-C is much more stable than Baller2Vec $^{++}$ . Predicted trajectories of MADIFF-C and Baller2Vec++ are provided in the Appendix Section H.4. ", "page_idx": 7}, {"type": "table", "img_path": "PvoxbjcRPT/tmp/029307d59e82b0469bcbd9a1c569532422cd4541d94bc74ed607e3d997903deb.jpg", "table_caption": ["Table 2: Multi-agent trajectory prediction results on NBA dataset across 3 seeds, given the first step of all agents\u2019 positions. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "PvoxbjcRPT/tmp/99484e7ee496ac541e6189b195829989ad217f6fcc12e644cfc500a974b9bcff.jpg", "img_caption": ["Figure 2: Visualization of an episode in the Spread task. Solid lines are real rollouts, and dashed lines are DM-planned trajectories. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.4 Qualitative Analysis on Teammate modeling ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We discuss the quality of teammate modeling as mentioned in Section 3.3 and how it is related to the decentralized execution scenario. In Figure 2 left, we visualize an episode generated by MADIFF-D trained on the Expert dataset of Spread task. The top and bottom rows are snapshots of entities\u2019 positions on the initial and intermediate time steps. The three rows from left to right in each column represent the perspectives of the three agents, red, purple, and green, respectively. Dashed lines are the planned trajectories for the controlled agent and other agents output by DMs, and solid lines are the real rollout trajectories. We observe that at the start, the red agent and the purple agent generate inconsistent plans, where both agents decide to move towards the middle landmark and assume the other agent is going to the upper landmark. At the intermediate time step, when the red agent is close to the middle landmark while far from the uppermost ones, the purple agent altered the planned trajectories of both itself and the red teammate, which makes all agents\u2019 plans consistent with each other. This particular case indicates that MADIFF is able to correct the prediction of teammates\u2019 behaviors during rollout and modify each agent\u2019s own desired goal correspondingly. ", "page_idx": 8}, {"type": "text", "text": "In Figure 2 right, we demonstrate that such corrections of teammate modeling are common and can help agents make globally coherent behaviors. We sample 100 episodes with different initial states and define Consistent Ratio at some time step $t$ as the proportion of episodes in which the three agents make consistent planned trajectories. We plot the curve up to step $t=9$ , which is approximately halfway through the episode length limit in MPE. The horizontal red line represents how many portions of the real rollout trajectories are consistent at step $t=9$ . The interesting part is that the increasing curve reaches the red line before $t=9$ , and ends up even higher. This indicates that the planned teammates\u2019 trajectories are guiding the multi-agent interactions beforehand, which is a strong exemplar of the benefits of MADIFF\u2019s teammate modeling abilities. We also include visualizations of imagined teammate observation sequences in SMAC $3m$ task in the Appendix Section H.3. ", "page_idx": 8}, {"type": "text", "text": "5.5 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our key argument is that the great coordination ability of MADIFF is brought by the attention modules among individual agents\u2019 diffusion networks. We validate this insight through a set of ablation experiments on MPE. We compare MADIFF-D with independent DMs, i.e., each agent learns from corresponding offline data using independent U-Nets without attention. We denote this variant as MADIFF-D-Ind. In addition, we also ablate the choice of whether each agent should share parameters of their basic U-Net, noted as Share or NoShare. Without causing ambiguity, we omit the name of MADIFF, and notate the different variants as $D$ -Share, $D$ -NoShare, Ind-Share, Ind-NoShare. ", "page_idx": 8}, {"type": "image", "img_path": "PvoxbjcRPT/tmp/84009ffa7969a3c54b72d279223cbff44794e4fb5a005de012b28fc13d0edf67.jpg", "img_caption": ["Figure 3: The average normalized score of MADIFF ablation variants in MPE tasks. The mean and standard error are computed over 5 different seeds. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "As is obviously observed in Figure 3, with attention modules, MADIFF-D significantly exceeds that of the independent version on most tasks, justifying the importance of inter-agent attentions. The advantage of MADIFF-D is more evident when the task becomes more challenging and the data becomes more confounded, e.g., results on World, where the gap between centralized and independent models is larger, indicating the difficulty of solving offline coordination with independently trained models. As for the parameter sharing choice, the performance of MADIFF-D-Share and MADIFFD-NoShare is similar overall. Since MADIFF-D-Share has fewer parameters, we prefer MADIFFD-Share, and use it as the default variant to be reported in Table 1. Another advantage of sharing U-Net parameters is that the trajectories of various agents can be batched together and fed through the network. This not only decreases sampling time but also renders it insensitive to an increasing number of agents. We provide a specific example in Appendix Section G.4. ", "page_idx": 8}, {"type": "text", "text": "5.6 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Scalability to many agents. MADIFF-D requires each agent to infer all teammates\u2019 future trajectories, which is difficult and unnecessary in environments with a large number of agents. Although we have done experiments on a maximum number of 8 agents (SMAC $\\it{8m}$ ), MADIFF-D is in general not suitable for scenarios with tens or hundreds of agents. A potential solution is to infer a latent representation of teammates\u2019 trajectories. ", "page_idx": 9}, {"type": "text", "text": "Applicability in highly stochastic environments. Several theoretical and empirical studies [Paster et al., 2022, Brandfonbrener et al., 2022, Chen et al., 2021] have demonstrated that in offline RL, sequence modeling algorithms tend to underperform Q-learning-based algorithms in environments with high stochasticity. This is primarily because sequence modeling algorithms are more susceptible to high-reward offilne trajectories that are achieved by chance. Since MADIFF is a sequence modeling algorithm, it shares this weakness. To assess how much MADIFF is affected by environmental stochasticity, we conducted experiments on the terran_5_vs_5 map in SMACv2 [Ellis et al., 2022]. The design principle of SMACv2 is to add stochasticity to the original SMAC environment, including randomized initial positions and unit types. We conducted experiments under four settings: the original version, without position randomness, without unit type randomness, and without both kinds of randomness. MADIFF performs worse than the Q-learning-based method only when both kinds of stochasticity are present. In all settings, MADIFF outperforms the sequence modeling baseline. Detailed experimental settings and results can be found in Appendix Section H.1. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose MADIFF, a novel generative multi-agent learning framework, which is realized with an attention-based diffusion model designed to model the complex coordination among multiple agents. To our knowledge, MADIFF is the first diffusion-based offilne multi-agent learning algorithm, which behaves as both a decentralized policy and a centralized controller including teammate modeling, and can be used for multi-agent trajectory prediction. Our experiments indicate strong performance compared with a set of recent offline MARL baselines on a variety of tasks. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The SJTU team is partially supported by National Key R&D Program of China (2022ZD0114804), Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102) and National Natural Science Foundation of China (62322603, 62076161). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Johannes Ackermann, Volker Gabler, Takayuki Osa, and Masashi Sugiyama. Reducing overestimation bias in multi-agent domains using double centralized critics. arXiv preprint arXiv:1910.01465, 2019.   \nAnurag Ajay, Yilun Du, Abhi Gupta, Joshua Tenenbaum, Tommi Jaakkola, and Pulkit Agrawal. Is conditional generative modeling all you need for decision-making? International Conference on Learning Representations, 2023.   \nMichael A Alcorn and Anh Nguyen. baller2vec++: A look-ahead multi-entity transformer for modeling coordinated agents. arXiv preprint arXiv:2104.11980, 2021.   \nItamar Arel, Cong Liu, Tom Urbanik, and Airton G Kohls. Reinforcement learning-based multi-agent system for network traffic signal control. IET Intelligent Transport Systems, 4(2):128\u2013135, 2010.   \nDavid Brandfonbrener, Alberto Bietti, Jacob Buckman, Romain Laroche, and Joan Bruna. When does return-conditioned supervised learning work for offline reinforcement learning? arXiv preprint arXiv:2206.01079, 2022.   \nLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084\u201315097, 2021.   \nBenjamin Ellis, Skander Moalla, Mikayel Samvelyan, Mingfei Sun, Anuj Mahajan, Jakob N Foerster, and Shimon Whiteson. Smacv2: An improved benchmark for cooperative multi-agent reinforcement learning. arXiv preprint arXiv:2212.07489, 2022.   \nJakob N Foerster, Richard Y Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, and Igor Mordatch. Learning with opponent-learning awareness. arXiv preprint arXiv:1709.04326, 2017.   \nClaude Formanek, Asad Jeewa, Jonathan Shock, and Arnu Pretorius. Off-the-grid marl: a framework for dataset generation with baselines for cooperative offline multi-agent reinforcement learning. arXiv preprint arXiv:2302.00521, 2023.   \nJustin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.   \nScott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning. Advances in neural information processing systems, 34:20132\u201320145, 2021.   \nScott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In International conference on machine learning, pages 2052\u20132062. PMLR, 2019.   \nPhilippe Hansen-Estruch, Ilya Kostrikov, Michael Janner, Jakub Grudzien Kuba, and Sergey Levine. Idql: Implicit q-learning as an actor-critic method with diffusion policies, 2023.   \nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.   \nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840\u20136851, 2020.   \nMichael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence modeling problem. Advances in neural information processing systems, 34:1273\u20131286, 2021.   \nMichael Janner, Yilun Du, Joshua Tenenbaum, and Sergey Levine. Planning with diffusion for flexible behavior synthesis. In International Conference on Machine Learning, pages 9902\u20139915. PMLR, 2022.   \nJiechuan Jiang and Zongqing Lu. Offline decentralized multi-agent reinforcement learning. arXiv preprint arXiv:2108.01832, 2021.   \nBingyi Kang, Xiao Ma, Chao Du, Tianyu Pang, and Shuicheng Yan. Efficient diffusion policies for offline reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024.   \nAviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offilne reinforcement learning. Advances in Neural Information Processing Systems, 33:1179\u20131191, 2020.   \nJiachen Li, Fan Yang, Masayoshi Tomizuka, and Chiho Choi. Evolvegraph: Multi-agent trajectory prediction with dynamic relational reasoning. Advances in neural information processing systems, 33:19783\u201319794, 2020.   \nRyan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actorcritic for mixed cooperative-competitive environments. Neural Information Processing Systems (NIPS), 2017.   \nCong Lu, Philip Ball, Yee Whye Teh, and Jack Parker-Holder. Synthetic experience replay. Advances in Neural Information Processing Systems, 36, 2024.   \nLinghui Meng, Muning Wen, Yaodong Yang, Chenyang Le, Xiyun Li, Weinan Zhang, Ying Wen, Haifeng Zhang, Jun Wang, and Bo Xu. Offilne pre-trained multi-agent decision transformer: One big sequence model tackles all smac tasks. arXiv e-prints, pages arXiv\u20132112, 2021.   \nFrans A Oliehoek and Christopher Amato. A concise introduction to decentralized POMDPs. Springer, 2016.   \nLing Pan, Longbo Huang, Tengyu Ma, and Huazhe Xu. Plan better amid conservatism: Offilne multiagent reinforcement learning with actor rectification. In International Conference on Machine Learning, pages 17221\u201317237. PMLR, 2022.   \nGeorgios Papoudakis, Filippos Christianos, and Stefano Albrecht. Agent modelling under partial observability for deep reinforcement learning. Advances in Neural Information Processing Systems, 34:19210\u201319222, 2021.   \nKeiran Paster, Sheila McIlraith, and Jimmy Ba. You can\u2019t count on luck: Why decision transformers and rvs fail in stochastic environments. Advances in neural information processing systems, 35: 38966\u201338979, 2022.   \nBei Peng, Tabish Rashid, Christian Schroeder de Witt, Pierre-Alexandre Kamienny, Philip Torr, Wendelin B\u00f6hmer, and Shimon Whiteson. Facmac: Factored multi-agent centralised policy gradients. Advances in Neural Information Processing Systems, 34:12208\u201312221, 2021.   \nNeil Rabinowitz, Frank Perbet, Francis Song, Chiyuan Zhang, SM Ali Eslami, and Matthew Botvinick. Machine theory of mind. In International conference on machine learning, pages 4218\u20134227. PMLR, 2018.   \nRoberta Raileanu, Emily Denton, Arthur Szlam, and Rob Fergus. Modeling others using oneself in multi-agent reinforcement learning. In International conference on machine learning, pages 4257\u20134266. PMLR, 2018.   \nTabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Monotonic value function factorisation for deep multi-agent reinforcement learning. The Journal of Machine Learning Research, 21(1):7234\u20137284, 2020.   \nMikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nantas Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019.   \nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256\u20132265. PMLR, 2015.   \nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019.   \nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \nXihuai Wang, Zheng Tian, Ziyu Wan, Ying Wen, Jun Wang, and Weinan Zhang. Order matters: Agent-by-agent policy optimization. In The Eleventh International Conference on Learning Representations, 2023.   \nZhendong Wang, Jonathan J Hunt, and Mingyuan Zhou. Diffusion policies as an expressive policy class for offline reinforcement learning. arXiv preprint arXiv:2208.06193, 2022.   \nMuning Wen, Jakub Kuba, Runji Lin, Weinan Zhang, Ying Wen, Jun Wang, and Yaodong Yang. Multiagent reinforcement learning is a sequence modeling problem. Advances in Neural Information Processing Systems, 35:16509\u201316521, 2022.   \nTimon Willi, Alistair Hp Letcher, Johannes Treutlein, and Jakob Foerster. Cola: consistent learning with opponent-learning awareness. In International Conference on Machine Learning, pages 23804\u201323831. PMLR, 2022.   \nChenjun Xiao, Yifan Wu, Chen Ma, Dale Schuurmans, and Martin M\u00fcller. Learning to combat compounding-error in model-based reinforcement learning. arXiv preprint arXiv:1912.11206, 2019.   \nMinkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. Geodiff: A geometric diffusion model for molecular conformation generation. arXiv preprint arXiv:2203.02923, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Yiqin Yang, Xiaoteng Ma, Chenghao Li, Zewu Zheng, Qiyuan Zhang, Gao Huang, Jun Yang, and Qianchuan Zhao. Believe what you see: Implicit constraint approach for offline multi-agent reinforcement learning. Advances in Neural Information Processing Systems, 34:10299\u201310312, 2021. ", "page_idx": 12}, {"type": "text", "text": "Stephen Zhao, Chris Lu, Roger B Grosse, and Jakob Foerster. Proximal learning with opponentlearning awareness. Advances in Neural Information Processing Systems, 35:26324\u201326336, 2022. ", "page_idx": 12}, {"type": "text", "text": "A Outline ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this appendix, we provide a table to explain the main notations we used in Section B. In Section C, we give the pseudocode of multi-agent planning and multi-agent trajectory prediction with MADIFF model. In Section D, we demonstrate how multiple agents\u2019 trajectories are modeled by MADIFF during centralized control and decentralized execution in an example three-agent environment. In Section E, we give additional information on offline datasets, including how they are collected, violin plots of return distributions, and a minor issue of MPE dataset. In Section F, we briefly describe the implementation of baseline algorithms and links to related resources. In Section G, we provide details of the experiments, including the normalization used to compute the average score, the detailed network illustration unrolling each agent\u2019s U-Net, crucial hyperparameters, and examples of wall-clock time and resources required for training and sampling from MADIFF. In Section H, we demonstrate and analyze additional experimental results. Specifically, we provide experiment results on SMACv2 to demonstrate how much MADIFF is affected by environmental stochasticity. We also provide ablation results to support the effectiveness of teammate modeling in MADIFF-D, show the quality of teammate modeling by MADIFF-D on SMAC tasks, and visualize predicted multi-player trajectories by MADIFF and the baseline algorithm on the NBA dataset. ", "page_idx": 13}, {"type": "text", "text": "B Notations ", "text_level": 1, "page_idx": 13}, {"type": "table", "img_path": "PvoxbjcRPT/tmp/aa98c472343556e55387224d7e09877a33e365df98f67f9c9edc36b886ae6d28.jpg", "table_caption": ["Table 3: List of main notations used in the paper. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "C Algorithm ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "D Illustration of Multi-agent Trajectory Modeling ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "To provide a better understanding of how multiple agents\u2019 observations are modeled by MADIFF in centralized control and decentralized execution scenarios, we show illustrative examples in a typical three-agent environment in Figure 4. If the environment allows for centralized control, we can condition MADIFF on all agents\u2019 historical and current observations, and let the model sample all agents\u2019 future trajectories as a single sample, as shown in Figure 4a. Then the current and next observations are sent to the inverse dynamics model for action prediction. If only decentralized execution is permitted, as shown in Figure 4b, agent 1 can only condition the model on its own information. The historical and current observations of other agents are masked when performing conditioning. MADIFF now not only generates agent 1\u2019s own future trajectories but also predicts the ", "page_idx": 13}, {"type": "text", "text": "1: Input: Noise model $\\epsilon_{\\theta}$ , inverse dynamics $I_{\\phi}$ , guidance scale $\\omega$ , history length $C$ , condition $\\textit{\\textbf{y}}$   \n2: Initialize $h\\leftarrow\\upmu\\upmu\\upmu\\upmu\\upmu\\upmu\\upmu=C)$ ; $t\\gets0$ // Maintain a history of length C   \n3: while not done do   \n4: Observe joint observation $^o$ ; h.insert $(o)$ ; Initialize $\\pmb{\\tau}_{K}\\sim\\mathcal{N}(\\mathbf{0},\\alpha\\pmb{I})$   \n5: for $k=K\\ldots1$ do   \n6: $\\tau_{k}[:{\\tt l e n g t h}(h)]\\leftarrow h$ $//$ Constrain plan to be consistent with history   \n7: if Centralized control then   \n8: $\\begin{array}{r l}&{\\hat{\\boldsymbol{\\epsilon}}\\gets\\epsilon_{\\boldsymbol{\\theta}}(\\tau_{k},{k})+\\omega(\\epsilon_{\\boldsymbol{\\theta}}(\\tau_{k},{y},{k})-\\epsilon_{\\boldsymbol{\\theta}}(\\tau_{k},{k}))}\\\\ &{(\\mu_{k-1},\\Sigma_{k-1})\\gets\\mathsf{D e n o i s e}(\\tau_{k},\\hat{\\epsilon})}\\end{array}$ // Classifier-free guidance   \n9:   \n10: else if Decentralized execution then   \n11: for agent $i\\in\\{1,2,\\dots,N\\}$ do   \n12: $\\hat{\\epsilon}^{i}\\leftarrow\\epsilon_{\\theta}^{i}(\\tau_{k}^{i},k)+\\omega(\\epsilon_{\\theta}^{i}(\\tau_{k}^{i},y^{i},k)-\\epsilon_{\\theta}^{i}(\\tau_{k}^{i},k))$ // Classifier-free guidance   \n13: (\u00b5ik\u22121, \u03a3ik\u22121) \u2190Denoise $(\\tau_{k}^{i},\\hat{\\epsilon}^{i})$   \n14: end for   \n15: end if   \n16: $\\pmb{\\tau}_{k-1}\\sim\\mathcal{N}(\\pmb{\\mu}_{k-1},\\alpha\\pmb{\\Sigma}_{k-1})$   \n17: end for   \n18: Extract $\\left(\\boldsymbol{o}_{t},\\boldsymbol{o}_{t+1}\\right)$ from $\\tau_{0}$   \n19: for agent $i\\in\\{1,2,\\dots,N\\}$ do   \n20: $a_{t}^{i}\\leftarrow f_{\\phi^{i}}\\big(o_{t}^{i},o_{t+1}^{i}\\big)$   \n21: end for   \n22: Execute $\\pmb{a}_{t}$ in the environment; $t\\gets t+1$   \n23: end while ", "page_idx": 14}, {"type": "text", "text": "Algorithm 2 Multi-Agent Trajectory Prediction with MADIFF ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "1: Input: Noise model $\\epsilon_{\\theta}$ , guidance scale $\\omega$ , condition $\\textit{\\textbf{y}}$ , historical joint observations $h$ with length   \n$C$ , predict horizon $H$   \n2: Initialize $\\pmb{\\tau}_{K}\\sim\\mathcal{N}(\\mathbf{0},\\alpha\\pmb{I})$   \n3: for $k=K\\ldots1$ do   \n4: $\\tau_{k}[:C]\\leftarrow h$ // Constrain prediction to be consistent with history   \n5: $\\hat{\\epsilon}\\stackrel{.}{\\leftarrow}\\epsilon_{\\theta}(\\tau_{k},k)+\\omega(\\epsilon_{\\theta}(\\tau_{k},y,k)-\\epsilon_{\\theta}(\\tau_{k},k))$ // Classifier-free guidance   \n6: $(\\mu_{k-1},\\Sigma_{k-1})\\leftarrow1$ Denoise $(\\tau_{k},\\hat{\\epsilon})$   \n7: $\\pmb{\\tau}_{k-1}\\sim\\mathcal{N}(\\pmb{\\mu}_{k-1},\\alpha\\pmb{\\Sigma}_{k-1})$   \n8: end for   \n9: Extract prediction $(o_{C},o_{C+1},\\ldots,o_{C+H-1})$ from \u03c40 ", "page_idx": 14}, {"type": "text", "text": "current and future observations of the other two agents. Due to the joint modeling of all agents during training, such predictions are also reasonable and can be considered as a form of teammate modeling from agent 1\u2019s perspective. Although teammate modeling is not directly used in generating agent 1\u2019s ego actions, it can help agent 1 refine its planned trajectories to be consistent with the predictions of others. ", "page_idx": 14}, {"type": "text", "text": "E Additional Information on offline datasets ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "E.1 MPE Datasets ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For MPE experiments, we use datasets and a fork of environment2provided by OMAR [Pan et al., 2022]. They seem to be using an earlier version of MPE where agents can receive different rewards. For example, in the Spread task, team reward is defined using the distance of each landmark to its closest agent, which is the same for all agents. But when an agent collides with others, it will receive the team reward minus a penalty term. The collision reward has been brought into the team reward in the official repository since this commit3. However, the fork provided by OMAR still uses a ", "page_idx": 14}, {"type": "image", "img_path": "PvoxbjcRPT/tmp/5828b9ec772681875dd78c5f02c0687e989e33389256d290ab76fe9e612be5b0.jpg", "img_caption": ["(a) MADIFF in centralized control. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "PvoxbjcRPT/tmp/b75e5484df5320f61026904f3b04336b536c00419b87785323af785c3e764f6e.jpg", "img_caption": ["(b) MADIFF in decentralized execution. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 4: Illustration of how agents\u2019 observations are modelled by MADIFF in a three-agent environment. Note that figure (b) shows the situation when Agent 1 is taking action during decentralized execution. ", "page_idx": 15}, {"type": "text", "text": "legacy version. For fair and proper comparisons, we use OMAR\u2019s dataset and environment where all baseline models are trained and evaluated. ", "page_idx": 15}, {"type": "text", "text": "We have to note that different rewards for agents only happen at very few steps, which might not contradict the fully cooperative setting much. For example, OMAR\u2019s expert split of the Spread dataset consists of 1M steps, and different rewards are recorded only at less than $1.5\\%$ (14929) steps. ", "page_idx": 15}, {"type": "text", "text": "E.2 MA Mujoco Datasets ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For MA Mujoco experiments, we adopt the off-the-grid dataset Formanek et al. [2023] and use Good, Medium and Poor datasets for each task. Each dataset is collected by three independently trained MA-TD3 policies, and a small amount of exploration noise is added to the policies for enhanced behavioral diversity. ", "page_idx": 15}, {"type": "text", "text": "For visualizations of the distribution of episode returns in each dataset, we provide violin plots of all datasets we used in Figure 5. ", "page_idx": 15}, {"type": "text", "text": "E.3 SMAC Datasets ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For SMAC experiments, we adopt the off-the-grid dataset [Formanek et al., 2023] and use Good, Medium and Poor datasets for each map. Each dataset is collected by three independently trained QMIX policies, and a small amount of exploration noise is added to the policies for enhanced behavioral diversity. ", "page_idx": 15}, {"type": "text", "text": "For visualizations of the distribution of episode returns in each dataset, we provide violin plots of all datasets we used in Figure 6. ", "page_idx": 15}, {"type": "text", "text": "F Baseline Implementations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Here we briefly describe how the baseline algorithms are implemented. For MATP experiments, we use the implementation from the official repository of Baller $2{\\mathsf{V e c}}++^{4}$ . Baseline results on MPE datasets are borrowed from Pan et al. [2022]. According to their paper, they build all algorithms upon a modified version of $\\mathbf{MADDPG^{5}}$ , which uses decentralized critics for all methods. Baselines on SMAC datasets are implemented by Formanek et al. [2023], and the performances are adopted from their reported benchmark results. The open-sourced implementation and hyperparameter settings can be found in the official repository6. ", "page_idx": 15}, {"type": "image", "img_path": "PvoxbjcRPT/tmp/8fa07dee1d338dd755c320eb98f29b808f1cf7facbbf70c46300f539797d6da6.jpg", "img_caption": ["Figure 5: Violin plots of returns in MA Mujoco datasets. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "G Implementation Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "G.1 Score Normalization ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The average scores of MPE tasks in Table 1 are normalized by the expert and random scores on each task. Denote the original episodic return as $S$ , then the normalized score $S_{\\mathrm{norm}}$ is computed as ", "page_idx": 16}, {"type": "equation", "text": "$$\nS_{\\mathrm{norm}}=100\\times(S-S_{\\mathrm{random}})/(S_{\\mathrm{expert}}-S_{\\mathrm{random}})\\;,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which follows Pan et al. [2022] and Fu et al. [2020]. The expert and random scores on Spread, Tag, and World are {516.8, 159.8}, {185.6, -4.1}, and $\\{79.5,\\,-6.8\\}$ , respectively. ", "page_idx": 16}, {"type": "text", "text": "G.2 Detailed Network Architecture ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In Figure 7, we unroll the U-Net structure of different agents. ", "page_idx": 16}, {"type": "text", "text": "We describe the computation steps of attention among agents in formal. Each agent\u2019s local embedding $c^{i}$ is passed through the key, value, and query network to form $\\boldsymbol{q}^{i},\\,\\boldsymbol{k}^{i}$ , and $\\bar{v}^{i}$ , respectively. Then the dot product with scaling is performed between all agents\u2019 $q^{i}$ and $k^{i}$ , which is followed by a Softmax operation to obtain the attention weight $\\alpha^{i j}$ . Each $\\alpha^{i j}$ can be viewed as the importance of $j$ -th agent to the $i$ -th agent at the current time step. The second dot product is carried out between the weight matrix and the value embedding $v^{i}$ to get ${\\hat{c}}^{i}$ after multi-agent feature interactions. Then ${\\hat{c}}^{i}$ is skip-connected to the corresponding decoder block. The step-by-step computation of multi-agent ", "page_idx": 16}, {"type": "image", "img_path": "PvoxbjcRPT/tmp/3172267ae0b6a47418fb81e10e2d9ca1c38872fc9007493e1a979c2cacb7a8b2.jpg", "img_caption": ["Figure 6: Violin plots of returns in SMAC datasets. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "attention in MADIFF can be written as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{q^{i}=f_{\\mathrm{query}}(c^{i}),\\,k^{i}=f_{\\mathrm{key}}(c^{i}),\\,v^{i}=f_{\\mathrm{value}}(c^{i})\\,;\\,}}\\\\ {{\\alpha^{i j}=\\displaystyle\\frac{\\exp(q^{i}k^{j}/\\sqrt{d_{k}})}{\\sum_{p=1}^{N}\\exp(q^{i}k^{p}/\\sqrt{d_{k}})}\\,;\\,}}\\\\ {{\\hat{c}^{i}=\\displaystyle\\sum_{j=1}^{N}\\alpha^{i j}v^{j}\\;,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $d_{k}$ is the dimension of $k^{i}$ . ", "page_idx": 17}, {"type": "text", "text": "G.3 Hyperparameters ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We list the key hyperparameters of MADIFF we used in Table 4, Table 5, and Table 6. In all of our experiments, we use a scaling factor of 0.5 and $\\beta$ of 0.25. Return scale is the normalization factor used to divide the conditioned return before input to the diffusion model. The rough range of the return scale can be determined by the return distributions of the training dataset. We only tune the guidance weight $\\omega$ , return scale, planning horizon $H$ , and history horizon. We tried the guidance weight of {1.0, 1.2, 1.4, 1.6}, and found that different choices do not significantly affect final performances, we chose 1.2 for all experiments. For MPE tasks, we find it unnecessary to condition on history observation sequence; thus, we set all history horizons to zero. ", "page_idx": 17}, {"type": "text", "text": "G.4 Computing Resources and Wall Time ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The training of MADIFF does not involve an iterative process, and thus, the training time is not related to the total number of diffusion steps. Thanks to the property that the sum of two independent ", "page_idx": 17}, {"type": "image", "img_path": "PvoxbjcRPT/tmp/1a7e27f25b60ea4ff052966e6067ffe85464f33c632cb79606001a82313afd60.jpg", "img_caption": ["Figure 7: The detailed architecture of MADIFF. Each agent\u2019s U-Net is unrolled and lined up in the horizontal direction. "], "img_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "PvoxbjcRPT/tmp/6b9aada9a19179ef00e02a28b870ceb7758874be11ca44ad38a31230cc850ebb.jpg", "table_caption": ["Table 4: Hyperparameters of MADIFF on MPE datasets. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "PvoxbjcRPT/tmp/124696e05e11317a7d9022c0d41858403731dfa147dd02415e44517f744136eb.jpg", "table_caption": ["Table 5: Hyperparameters of MADIFF on MA Mujoco datasets. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Gaussian random variables remains a Gaussian, the multistep forward process can be written in a closed form [Ho et al., 2020]: ", "page_idx": 18}, {"type": "equation", "text": "$$\nq(\\hat{\\tau}_{k}|\\tau_{0})=\\mathcal{N}(\\hat{\\tau}_{k};\\sqrt{\\bar{\\alpha}_{t}}\\tau_{0},(1-\\bar{\\alpha}_{t})I)\\;.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "table", "img_path": "PvoxbjcRPT/tmp/471b7a451db9d255321e5b1551c0010edcfe669046eab054267895504e3ea6ee.jpg", "table_caption": ["Table 6: Hyperparameters of MADIFF on SMAC datasets. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Therefore, the $k$ -th step noisy trajectory in Equation (6) can be easily sampled from the Gaussian distribution above without an iterative process. We provide a concrete example to illustrate the time and resources required for training MADIFF. On a server equipped with an AMD Ryzen 9 5900X (12 cores) CPU and an RTX 3090 GPU, we trained the MADIFF-C model on the Expert dataset from the MPE Spread task, achieving convergence in approximately one hour. The curve depicting Wall-clock time spent on training and the corresponding model performance is shown in Figure 8. ", "page_idx": 19}, {"type": "image", "img_path": "PvoxbjcRPT/tmp/ade219e21be49eca424e1f60bfee2453c717735de3aa4138b92f9aa90ee19f0d.jpg", "img_caption": ["Figure 8: Wall-clock time and corresponding average episode return (average over 10 episodes) during training MADIFF-C for MPE Spread task. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Table 7: The wall-clock time spent when generating multi-agent trajectories. We fix the dimension of observation space to 88 and use DDIM of 15 steps during sampling. The history horizon is set to 20, and the planning horizon is 8. The results are obtained on a server with an AMD Ryzen 9 5900X (12 cores) CPU and an RTX 3090 GPU, and are averaged over 1000 trials. The computation time does not increase much with the number of agents thanks to GPU-accelerated computing. ", "page_idx": 19}, {"type": "table", "img_path": "PvoxbjcRPT/tmp/3f772103ab88f395876b45073aff07982de40c43d888184abb28a66b33980906.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "In Table 7, we showcase the time required for sampling multi-agent trajectories with MADIFF as the number of agents increases. We can see that the sampling time does not differ much when generating different number of trajectories. Since we use shared U-Net models for all agents in our experiments, different agents\u2019 trajectories can be batched together and passed through the network. Therefore, using GPU-accelerated computing, the second part does not cost much more time than predicting each agent\u2019s trajectory during inference. ", "page_idx": 19}, {"type": "text", "text": "H Additional Experimental Results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "H.1 SMACv2 Experiments ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "To understand how much MADIFF is affected by environmental stochasticity, we conducted experiments on the terran_5_vs_5 map in SMACv2 [Ellis et al., 2022]. SMACv2 is built upon SMAC with a focus on higher stochasticity. Specifically, in SMACv2, the unit types and agent start positions are randomized at the beginning of each episode. As each agent can only observe a nearby area, such randomness results in increased stochasticity in environment transitions. There are two different types of starting positions, reflect and surround. In reflect settings, the map is splitted into two sides. Allied units and enemy units are randomly and uniformly spawned on different sides. In surround settings, allied units are spawned at the center of the map, and enemy units are randomly stationed along the four diagonals. In terran $\\angles{\\xi}\\!\\!\\!\\!\\!_{-}\\!\\!\\nu s\\!\\!\\!\\!\\!\\!_{-}\\!\\!\\!\\!\\!\\!5$ , there are three different unit types: marine, marauder, and medivac. The default sampling probabilities of these three types are 0.45, 0.45 and 0.1. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "We design four settings with different degree of stochasicity: the original version, without position randomness (w/o PR), without unit type randomness (w/o TR), and without both kinds of randomness (w/o PR&TR). To reduce position randomness, we only use surrounding settings. Note that this does not mean the staring positions of all units are fixed, since enemy units are still randomized along the four diagonals. To remove unit type randomness, we set all units to be marines. The dataset for the original version is the terran_5_vs_5 Replay dataset from Formanek et al. [2023]. Datasets for other three stochasicity settings were collected by ourselves. We partially trained three MAPPO7 models in each setting. Each model was then used to collect 500 episodes, resulting in a dataset comprising 1500 episodes for each setting. ", "page_idx": 20}, {"type": "text", "text": "Three algorithms are benchmarked under these four settings: MAICQ, which represents the stateof-the-art in Q-learning-based algorithms; MADT, a representative multi-agent sequence modeling baseline; and MADIFF-D. Results are presented in Table 8. We can see that MADIFF-D performs worse than MAICQ only when both kinds of stochasticity are present. As the environmental randomness diminishes, MADIFF-D\u2019s performance gradually catches up with and surpasses MAICQ. In all settings, MADIFF-D outperforms MADT. ", "page_idx": 20}, {"type": "table", "img_path": "PvoxbjcRPT/tmp/7f331ef1c81f713a8e17768905e2b4479413d92f6832df822b831bcd8fdd98a0.jpg", "table_caption": ["Table 8: The average score on different settings of SMACv2 terran_5_vs_5. Shaded columns represent our method. The mean and standard error are computed over 3 different seeds. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "H.2 Effectiveness of Teammate Modeling ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "To investigate whether teammate modeling can lead to performance improvements during decentralized execution, we conduct ablation experiments on MPE Spread datasets. We compare MADIFF-D with its variant that adopts the same network architecture but masks the diffusion loss on other agents\u2019 trajectories during training. We denote the variant as MADIFF-D w/o TM. The results are presented in Table 9, which show that teammate modeling results in notable performance improvements on all four levels of datasets. ", "page_idx": 20}, {"type": "table", "img_path": "PvoxbjcRPT/tmp/52685e59de23a94cfdc2c76977fe0e00cba526489733e2b41c29cb7b59f57a3c.jpg", "table_caption": ["Table 9: Ablation results of teammate modeling on MPE Spread datasets across 3 seeds. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "H.3 Teammate Modeling on SMAC Tasks ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We show and analyze the quality of teammate modeling by MADIFF-D on SMAC. Specifically, we choose two time steps from an episode on $3\\mathrm{m}$ map to analyze predictions on allies\u2019 attack targets and health points (HP), respectively. ", "page_idx": 21}, {"type": "text", "text": "On top of Figure 9a is attacked enemy agent ID (0, 1, 2 stands for E0, E1, E2) of ally agents A0, A1, and A2. The first row is the ground-truth ID, and the second and the third rows are the predictions made by MADIFF-D from the other two allies\u2019 views. We can see that the predictions are in general consistent with the ground-truth ID. As can be seen from the true values of the attack enemy ID, agents tend to focus their firepower on the same enemies at the same time. And the accurate prediction of allies\u2019 attack enemy IDs intuitively can help to execute such a strategy. ", "page_idx": 21}, {"type": "text", "text": "In Figure 9b, we visualize the HP change curve of ally agents starting from another time step. From the environment state visualization below, agent A2 is the closest to enemies, so its HP drops the fastest. Such a pattern is successfully predicted by the other two agents. ", "page_idx": 21}, {"type": "image", "img_path": "PvoxbjcRPT/tmp/30467bba73566e135f1b0175bcdb9128da1d063edaee787057a234e2a99c647a.jpg", "img_caption": ["(a) Ground-truth and predicted enemy\u2019s ID to attack by each ally agent. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "PvoxbjcRPT/tmp/cef3cffe9fb0785c81fded1be0d6b0c2f4ab1cff4e80dcf11de6467b36b0df9d.jpg", "img_caption": ["(b) Ground-truth and predicted health points (HP) of each ally agent. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 9: The ground-truth and predicted information of different MADIFF agents at two-time step. On the top of each figure, each column describes a different agent. The first row shows the change curve of the real value, and the last two rows below are the information predicted by other agents. ", "page_idx": 21}, {"type": "text", "text": "H.4 Predicted Trajectory Visualization on NBA Dataset ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We visualize the players\u2019 moving trajectories predicted by MADIFF-C and Baller $2{\\mathrm{Vec}}++$ on the NBA dataset in Figure 10. In each image, the solid lines are real trajectories and the dashed lines are trajectories predicted by the model. The trajectories predicted by MADiff-C are closer to the real trajectories and are overall smoother compared to the Baller2Vec $^{++}$ predictions. ", "page_idx": 21}, {"type": "image", "img_path": "PvoxbjcRPT/tmp/b65f053bf1c584cd525f7451b8e88c5dac3b4a1ac338e15f5ebe2e36b193a9db.jpg", "img_caption": ["Figure 10: Real and Predicted multi-player trajectories by MADIFF-C and Baller2Vec $^{++}$ . "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The claims made in the abstract and introduction accurately reflect our contributions. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We point out the limitations of our method in Section 5.6. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our paper does not include theoretical results. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We illustrate our model architecture in Figure 1 and list important hyperparameters in Appendix Section G.3. We also provide the source code in supplementary materials. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide code, anonymous data download link, and necessary instructions in supplementary materials. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We provide the experimental details in Section 5.1, Section 5.2, and Appendix Section E, Section F, Section G. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide error bars in Table 1, Table 2, and Figure 3. Reported error bar is standard deviation calculated over trials with different random seeds. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 25}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We provide concrete instances of both training and sampling wall time and resources of our algorithm in Appendix Section G.4. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our paper conform with the NeurIPS Code of Ethics. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The proposed algorithm is a general solution for a wide range of offilne multiagent learning problems. In our opinion, there is no specific societal impact that should be stated explicitly. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our paper poses no such risks. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We cited and mentioned open-sourced implementation we used in Appendix Section F. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our paper does not publicly release new assets. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our paper does not involve human subjects. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our paper does not involve human subjects. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]