[{"type": "text", "text": "A Primal-Dual-Assisted Penalty Approach to Bilevel Optimization with Coupled Constraints ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Liuyuan Jiang\u2020, Quan Xiao\u2020, Victor M. Tenorio\u22c6, Fernando Real-Rojas Antonio G. Marques\u22c6, Tianyi Chen\u2020 ", "page_idx": 0}, {"type": "text", "text": "\u2020Rensselaer Polytechnic Institute, Troy, NY, United States \u22c6King Juan Carlos University, Madrid, Spain {jiangl7, xiaoq5, chent18}@rpi.edu {victor.tenorio, antonio.garcia.marques}@urjc.es; f.real.2018@alumnos.urjc.es ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Interest in bilevel optimization has grown in recent years, partially due to its applications to tackle challenging machine-learning problems. Several exciting recent works have been centered around developing efficient gradient-based algorithms that can solve bilevel optimization problems with provable guarantees. However, the existing literature mainly focuses on bilevel problems either without constraints, or featuring only simple constraints that do not couple variables across the upper and lower-levels, excluding a range of complex applications. Our paper studies this challenging but less explored scenario and develops a (fully) first-order algorithm, which we term BLOCC, to tackle BiLevel Optimization problems with Coupled Constraints. We establish rigorous convergence theory for the proposed algorithm and demonstrate its effectiveness on two well-known real-world applications - hyperparameter selection in support vector machine (SVM) and infrastructure planning in transportation networks using the real data from the city of Seville. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Bilevel optimization (BLO) approaches are pertinent in various machine learning problems, including hyperparameter optimization [49, 24], meta-learning [22], and reinforcement learning [67, 63]. Moreover, the ability to handle BLO with constraints is particularly important, as these constraints appear in applications such as pricing [15], transportation [50, 1], and kernelized SVM [30]. Although there is extensive research on BLO problems without constraints or with uncoupled constraints [31, 12, 39, 42, 62], solutions for BLO problems with coupled constraints (CCs) remain limited; see details in Table 1. However, it is of particular interest to investigate BLO with lower-level CCs. Taking infrastructure planning in a transportation network as an example, the lower-level seeks to optimize a utility constrained by the upper-level parameter, network configuration. ", "page_idx": 0}, {"type": "text", "text": "Motivated by this, we consider the coupled-constrained BLO problem in the following form ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{min}_{x\\in\\mathcal{X}}f(x,y_{g}^{*}(x))}\\\\ &{\\mathrm{s.t.}\\quad y_{g}^{*}(x):=\\arg\\operatorname*{min}_{y\\in\\mathcal{Y}(x)}g(x,y)\\quad\\mathrm{with}\\quad\\mathcal{Y}(x):=\\left\\{y\\in\\mathcal{Y}:g^{c}(x,y)\\leq0\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "objective which is strongly convex, $g^{c}(x,y):\\mathbb{R}^{d_{x}}\\times\\mathbb{R}^{d_{y}}\\rightarrow\\mathbb{R}^{d_{c}}$ defines the lower-level CCs, and $\\mathcal{X}\\subseteq\\mathbb{R}^{d_{x}},\\mathcal{Y}\\subseteq\\mathbb{R}^{d_{y}}$ are the domain of $x$ and $y$ that are easy to project, such as the Euclidean ball. ", "page_idx": 1}, {"type": "text", "text": "The challenge in solving (1) arises from the coupling of the upper and lower-level problems. Prior work addressed this by starting with the unconstrained BLO problem, using implicit gradient descent (IGD) methods [27, 31, 34, 12, 13, 40, 61, 43, 66] and penalty-based methods [45, 44, 61, 41, 42]. To solve BLO with CCs, AiPOD [71] and GAM [72] investigated the IGD method under different constraint settings. However, AiPOD only considered equality constraints, and GAM lacked finitetime convergence guarantees. Leveraging a penalty reformulation, [75] developed a Hessian-free method with finite-time convergence. However, a key algorithm step in [75] is a joint projection of the current iterate $(x,y)$ onto the coupled constraint set. This projection, required at each iteration, becomes particularly challenging when $g^{c}(x,y)$ is not jointly convex and can be computationally expensive for large-scale problems with a high number of variables $(d_{x},d_{y})$ or constraints $(d_{c})$ . ", "page_idx": 1}, {"type": "text", "text": "To this end, this paper aims to address the following question ", "page_idx": 1}, {"type": "text", "text": "Can we develop an efficient algorithm that bypasses joint projections on $g^{c}(x,y)$ and quickly solves the BLO problem with coupled inequality constraints in (1)? ", "page_idx": 1}, {"type": "text", "text": "We address this question affirmatively, focusing on the setting where the lower-level objective, $g(x,y)$ , is strongly convex in $y$ , and the constraints $g^{c}(x,y)$ are convex in $y$ . To avoid implementing a joint projection, we put forth a novel single-level primal-dual-assisted penalty reformulation that decouples $x$ and $y$ . Specifically, with $\\mu\\in\\mathbb{R}^{\\tilde{d_{c}}}$ denoting the Lagrange multiplier of (1b), we propose solving where the penalty constant $\\gamma$ controls the distance between $y$ and $y_{g}^{*}(x)$ by penalizing $g(x,y)$ to its value function $v(x)$ , and the Lagrangian term penalizes the constraint violation of $g^{c}(x,y)$ . ", "page_idx": 1}, {"type": "image", "img_path": "uZi7H5Ac0X/tmp/d13bc5453f2e05b6239d34e6df561ff708b5afa48fdbd4a980a1176048850b86.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "However, recognizing the max-min subproblems involved in (2a), it becomes computationally costly to evaluate the penalty function $F_{\\gamma}(x)$ and its gradient. To this end, we pose the following question ", "page_idx": 1}, {"type": "text", "text": "Can we develop efficient algorithms to solve the max-min subproblem and evaluate $\\nabla F_{\\gamma}(x)$ ? ", "page_idx": 1}, {"type": "text", "text": "We answer this question by proving that this reformulation exhibits several favorable properties, including smoothness. These properties are critical for designing gradient-based algorithms and characterizing their performance. However, the presence of the CCs renders the calculation of the gradient $\\nabla F_{\\gamma}(x)$ more challenging than for its unconstrained counterpart [62]. Building upon this, we design a primal-dual gradient method with rigorous convergence guarantees for the BLO with general inequality CCs, and provide an improved result for the case of $g^{c}$ being affine in $y$ . ", "page_idx": 1}, {"type": "text", "text": "1.1 Main contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In a nutshell, our main contributions are outlined below. ", "page_idx": 1}, {"type": "text", "text": "C1) In Section 2, leveraging the Lagrangian duality theorem, we introduce the function $F_{\\gamma}(x)$ in (2a) as a penalty-based reformulation of (1), establish the continuity and smoothness of $F_{\\gamma}(\\boldsymbol{x})$ , and develop a novel way to compute its gradient. ", "page_idx": 1}, {"type": "text", "text": "C2) In Section 3, we develop BLOCC, a fully first-order algorithm to tackle BLO problems with CCs. With $\\epsilon$ being the target error for the generalized gradient norm square of $F_{\\gamma}(x)$ , we establish that the iteration complexity under the generic constraint $g^{c}(x,y)$ in (1b) is $\\tilde{\\mathcal{O}}(\\epsilon^{-2.5})$ . We establish, for the first time, the linear convergence of a strongly convexconcave max-min problem with linear interaction and a constrained maximization parameter, reducing BLOCC\u2019s complexity to $\\tilde{\\mathcal{O}}(\\epsilon^{-1.5})$ when the constraint $g^{c}(x,y)$ is affine in $y$ .   \nC3) In Section 4, we apply our BLOCC algorithm to two real-world applications: SVM model training and transportation network planning. By comparison with LV-HBA [75] and GAM [72], we demonstrate the algorithm\u2019s effectiveness and its robustness to large-scale problems. ", "page_idx": 1}, {"type": "table", "img_path": "uZi7H5Ac0X/tmp/73a665953682a9ffe0fe772b7471ff8d0935b487cbb56d0c9ee6c0bf60a08d42.jpg", "table_caption": [], "table_footnote": ["Table 1: Comparison of our work with LV-HBA [75], BVFSM [46], AiPOD [71], and Gradient Approximation method (GAM) [72]. LL convergence is on metric the squared distance of $y_{t}$ to its optimal solution, and UL convergence is on squared (generalized) gradient norm. "], "page_idx": 2}, {"type": "text", "text": "1.2 Related works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "BLO has a long history, dating back to the seminal work of [8]. It has inspired a rich body of literature, e.g., [76, 68, 14, 65]. The recent focus on BLO is centered on developing efficient gradient-based approaches with provable finite-time guarantees. ", "page_idx": 2}, {"type": "text", "text": "Methods for BLO without constraints. A cluster of BLO gradient-based approaches gravitates around the implicit gradient descent (IGD) method [55], where the key idea is to approximate the hypergradient by the implicit function theorem. The finite-time convergence of IGD was first established in [27] for unconstrained strongly-convex lower-level problems. Subsequent works improved the convergence rates and/or relaxed the assumptions under various settings; see [31, 34, 12, 13, 40, 61, 43, 66, 73, 35, 70]. Another cluster of works is based on iterative differentiation (ITD) [49, 23, 53, 60], which estimates the hypergradient by differentiating the entire iterative algorithm used to solve the lower-level problem with respect to the upper-level variables. The finite-time guarantee was first established in [28, 47, 33, 6]. Viewing the lower-level problem as a constraint such as in [64], penalty-based methods have also emerged as a promising approach for BLO. Dated back to [77], this line of works [45, 51, 44, 41, 62, 25, 48] reformulated the original BLO as the single-level problems with various penalty terms and leveraged first-order methods to solve them. ", "page_idx": 2}, {"type": "text", "text": "BLO with constraints. While substantial progress has been made for unconstrained BLO, the analysis for constrained BLO is more limited. Upper-level constraints of the form $x\\in\\mathscr{X}$ were considered in [31, 12]. For the lower-level uncoupled constraint, SIGD [39] considered the uncoupled constraint $A y\\le b$ and achieved asymptotic convergence, [42, 62] employed penalty reformulation and considered both upper and lower uncoupled constraints. However, the literature on BLO with CCs is scarce. BVFSM [46] conducted a penalty-based method to avoid the calculation of the Hessian, as IGD methods do. However, only asymptotic convergence was achieved. GAM [72] investigated the IGD method under inequality constraints while failing to provide finite-time convergence results as well. AiPOD [71] also applied IGD and successfully achieved finite-time convergence, but it only considered equality constraints. LV-HBA [75] considered inequality constraints and constructed a penalty-based reformulation. However, it employed a joint projection of $(x,y)$ onto $\\{\\mathcal{X}\\times\\mathcal{Y}:$ $\\bar{g^{c}}(x,y)\\bar{\\leq}\\,0\\}$ which is computationally inefficient when there are many constraints or when $g^{c}(x,y)$ is not jointly convex. After our initial submission, we found a concurrent work [74] posted on ArXiv, which used Lagrange duality theory differently from ours, applying it to construct a new smoothed penalty term. However, it does not quantify the relationship between the relaxation of this penalty and the relaxation of lower-level optimality, and it does not guarantee lower-level feasibility. We summarized prior works on BLO with lower-level CCs in Table 1. ", "page_idx": 2}, {"type": "text", "text": "2 Primal-dual Penalty-based Reformulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, our goal is to construct a primal-dual-assisted penalty reformulation for our BLOCC problem. The technical challenge comes from finding a suitable penalty function for BLO with CCs. ", "page_idx": 2}, {"type": "text", "text": "2.1 The challenges in BLO with coupled constraints ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Here, we will elaborate on the two technical challenges of BLO with CCs. ", "page_idx": 2}, {"type": "text", "text": "The first challenge associated with the presence of CCs is the difficulty to find the descent direction for $x$ , which involves finding the closed-form expression of the gradient $\\nabla v(x)$ . The expression without CCs, $\\nabla v(x)\\;=\\;\\nabla_{x}g(x,y_{g}^{*}(x))$ provided in [62, 42, 41, 44], is not applicable. ", "page_idx": 2}, {"type": "text", "text": "For example, when $g(x,y)\\,=\\,(y\\,-\\,2x)^{2}$ and $g^{c}(x,y)\\,=\\,3x\\,-\\,y,$ the optimal lower-level solution is $y_{g}^{*}(x)=3x$ and thus, $v(x)=x^{2}$ with $\\nabla v(x)\\,=\\,2x$ . However, $\\nabla_{x}\\bar{g}(x,y_{g}^{*}(x))\\;=\\;-4x\\;\\neq\\;\\nabla v(x)$ . The closed form gradient for BLO with CCs should be (8), which considers a Lagrange term that will be illustrated later in this paper. In Figure 1, we present the gradient $\\nabla v(x)$ without the Lagrange multiplier in [62, 42, 41, 44] and ours with the Lagrange term. In this example, the gradient without the Lagrange term leads to the opposite direction to the true gradient. ", "page_idx": 3}, {"type": "text", "text": "The second challenge associated with the presence of CCs is the difficulty of performing the joint projection. If we directly extend the penalty reformulation in [62, 41], we could treat the coupling constraint set $y(x)$ as a joint constraint $\\{(x,y)\\,:\\,g^{c}(x,y)\\,\\\\leq\\,0\\}$ , and employ a joint projection of $(x,y)$ to ensure the feasibility. However, this can be computationally inefficient when the problem is large-scale, i.e. the number of variables or constraints is large. The detailed analysis of computational cost can be seen in Appendix H. Moreover, when $g^{c}(x,y)$ is complex, the projection may not have a closed form and may not even be well-defined if $g^{c}(x,y)$ is not jointly convex. ", "page_idx": 3}, {"type": "image", "img_path": "uZi7H5Ac0X/tmp/acef61a1fa194ae1466ced8ec1bbff5c95bddd54e10c5cfae1323059122b5f57.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 1: Calculation of $\\nabla v(x)$ . The blue line is $v(x)$ , the the yellow dashed line is calculated by the formulation given in [62, 41], while red dashed line is derived by our BLOCC. It can be seen that $\\nabla v(x)$ without the Lagrange multiplier is very biased from the true gradient. ", "page_idx": 3}, {"type": "text", "text": "2.2 The Lagrangian duality-based penalty reformulation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Before proceeding, we summarize the assumptions considered as follows. ", "page_idx": 3}, {"type": "text", "text": "Assumption 1 (Lipschitz Continuity). Assume that $f,\\,\\nabla f,\\,\\nabla g,\\,g^{c}$ and $\\nabla g^{c}$ , and are respectively $l_{f,0}$ , $l_{f,1},\\,l_{g,1},\\,l_{g^{c},0},$ and $l_{g^{c},1}$ -Lipschitz jointly over $(x,y)\\in\\mathcal{X}\\times\\mathcal{Y}$ , and $g$ is $l_{g_{x},0}$ -Lipschitz in $x\\in\\mathscr{X}$ . ", "page_idx": 3}, {"type": "text", "text": "Assumption 2 (Convexity in $y$ ). For any given $x\\in\\mathscr{X}$ , $g(x,y)$ and $g^{c}(x,y)$ are $\\alpha_{g}$ -strongly convex and convex in $y\\in\\mathcal{V}$ , respectively. ", "page_idx": 3}, {"type": "text", "text": "Assumption 3 (Domain Feasibility). Domain $\\mathcal{X}\\subseteq\\mathbb{R}^{d_{\\boldsymbol{x}}}$ and $\\mathcal{V}\\subseteq\\mathbb{R}^{d_{\\mathcal{Y}}}$ are non-empty, closed, and convex. For any given $x\\in\\mathscr{X}$ , $\\mathcal{V}(x):=\\{y\\in\\mathcal{y}:g^{c}(x,y)\\leq0\\}$ is non-empty. ", "page_idx": 3}, {"type": "text", "text": "Assumption 4 (Constraint Qualification). For any given $x\\in\\mathscr{X}$ , $g^{c}$ satisfies the Linear Independence Constraint Qualification (LICQ) condition for every $y\\mathcal{V}$ in a neighborhood of $y_{g}^{*}(x)$ . ", "page_idx": 3}, {"type": "text", "text": "The Lipschitz continuity in Assumption 1 and the strong convexity of $g$ over $y$ in Assumption 2 are conventional [27, 31, 41, 71, 35, 13]. Moreover, we only require $g^{c}(x,y)$ to be convex in $y$ rather than: i) jointly convex in $(x,y)$ as in [75], or ii) linear as in [39, 71]. Assumption 3 pertains to the convexity and closeness of the domain, which is also conventional, and Assumption 4 is a standard constraint qualification condition. ", "page_idx": 3}, {"type": "text", "text": "To build a penalty reformulation for BLO with lower-level CCs, the first challenge is to find a penalty that regulates $\\|y-y_{g}^{*}(x)\\|$ . In the following lemma, we show that $g(x,y)-v(x)$ is a good choice. ", "page_idx": 3}, {"type": "text", "text": "Lemma 1. Suppose that Assumptions 2-4 hold and $v(x)$ is defined as in (2b). Then, it holds that ", "page_idx": 3}, {"type": "text", "text": "The technical challenge of proving this lemma lies in showing the quadratic growth property as in c1) of Lemma 1. For BLO without the CCs, this naturally holds as the lower-level objective $g(x,y)$ is strongly convex in $y$ . For BLO with CCs, one needs to use the Lagrangian duality theorem. The full proof of Lemma 1 is given in Appendix B.1, where the key challenge is to show the invariance of the modulus of strong convexity in the Lagrangian reformulated lower-level objective. ", "page_idx": 3}, {"type": "text", "text": "With $\\gamma$ denoting a penalty constant, we consider the following penalty reformulation: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{(x,y)\\in\\{x\\times y:g^{c}(x,y)\\leq0\\}}f(x,y)+\\gamma(g(x,y)-v(x))\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the penalty term confines the squared Euclidean distance from $y$ to $y_{g}^{*}(x)$ . Furthermore, to avoid projecting $(x,y)$ onto the $\\{\\mathcal{X}\\times\\mathcal{Y}:g^{c}(x,y)\\leq0\\}$ , we propose the primal-dual-assisted penalty ", "page_idx": 3}, {"type": "text", "text": "reformulation, which was defined in (2). The approximate equivalence of the reformulation (2) to the original BLO problem (1) is established in the following theorem. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1 (Equivalence). Suppose that $f$ is Lipschitz in y and $\\nabla f$ is $l_{f,1}$ -Lipschitz in $(x,y)$ in Assumption 1 hold, and Assumptions 2-4 hold. Then, solving the $\\epsilon$ -approximation problem of (1): ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{(x,y)\\in\\{x\\times y:g^{c}(x,y)\\leq0\\}}f(x,y)\\quad s.t.\\quad\\|y-y_{g}^{*}(x)\\|^{2}\\leq\\epsilon,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The detailed proof is provided in Appendix B.2. By setting $\\gamma=\\mathcal{O}(\\epsilon^{-0.5})$ , we effectively state the equivalence between the penalty reformulation in (3) and the approximated original problem (4). We can then decouple the joint minimization on $(x,y)$ to a min-min problem on $x$ and $g^{c}$ constrained $y$ . For the inner minimization problem in $y$ , we choose $\\gamma$ in a way that $\\gamma\\alpha_{g}-l_{f,1}>0$ holds. This ensures the objective in (2a) being strongly convex in $y$ , as $l_{f,1}$ -smoothness ensures a lower bound for negative curvature of $f(x,y)$ . Furthermore, the convexity of $g^{c}(x,y)$ in $y$ validates the strong duality theorem in [59, 32], thereby enabling the max-min primal-dual reformulation in (2). ", "page_idx": 4}, {"type": "text", "text": "2.3 Smoothness of the penalty reformulation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To evaluate $F_{\\gamma}(x)$ defined in (2a), we can find the solution to the inner max-min problem: ", "page_idx": 4}, {"type": "equation", "text": "$$\n(\\mu_{F}^{*}(x),y_{F}^{*}(x)):=\\arg\\operatorname*{max}_{\\mu\\in\\mathbb{R}_{+}^{d x}}\\operatorname*{min}_{y\\in\\mathcal{Y}}\\underbrace{f(x,y)+\\gamma(g(x,y)-v(x))+\\langle\\mu,g^{c}(x,y)\\rangle}_{=:L_{F}(\\mu,y;x)}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The uniqueness of $y_{F}^{*}(x)$ and $\\mu_{F}^{*}(x)$ is guaranteed under Assumptions 2 and 4 (see Lemma 5 in Appendix A). Therefore, $F_{\\gamma}(x)$ in (2a) can be evaluated using the unique optimal solutions by ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\cal F}_{\\gamma}(x)={\\cal L}_{F}(\\mu_{F}^{*}(x),y_{F}^{*}(x);x).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Similarly, we can evaluate $v(x)=L_{g}(\\mu_{g}^{*}(x),y_{g}^{*}(x);x)$ , where ", "page_idx": 4}, {"type": "equation", "text": "$$\n(\\mu_{g}^{*}(x),y_{g}^{*}(x)):=\\arg\\operatorname*{max}_{\\mu\\in\\mathbb{R}_{+}^{d_{x}}}\\operatorname*{min}_{y\\in\\mathcal{Y}}\\underbrace{g(x,y)+\\langle\\mu,g^{c}(x,y)\\rangle}_{=:L_{g}(\\mu,y;x)}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The penalty reformulation $F_{\\gamma}(x)$ can hardly be convex, as $-v(x)$ may be concave, even when $g(x,y)=g(y)$ and $g^{c}(x,y)=A^{\\top}y-x$ ; see Lemma 4.24 in [59]. Instead, in the subsequent lemma, we not only show that $v(x)$ is differentiable, but also provide a closed-form expression of $\\nabla v(x)$ . ", "page_idx": 4}, {"type": "text", "text": "Lemma 2 (Danskin-like theorem for $v(x))$ . Suppose that Assumptions 1\u20134 hold, and let $B_{g}<\\infty\\,b e$ a constant such that $\\|\\mu_{g}^{*}(x)\\|<B_{g}$ for all $x\\in\\mathscr{X}$ . Then, it holds that ", "page_idx": 4}, {"type": "text", "text": "1. $y_{g}^{*}(x)$ and $\\mu_{g}^{*}(x)$ defined in (7) are $L_{g}$ -Lipschitz for some finite constant $L_{g}\\ge0$ . ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nabla v(x)=\\nabla_{x}g(x,y_{g}^{*}(x))+\\langle\\mu_{g}^{*}(x),\\nabla_{x}g^{c}(x,y_{g}^{*}(x))\\rangle.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The assumption of the existence of the upper bound for the Lagrange multiplier is a consequence of the LICQ condition [69, Theorem 1]. This assumption is mild and traditional [75]. Finding $\\nabla v(x)$ is crucial for the design of a gradient-based method to solve $\\mathrm{min}_{x\\in\\mathcal{X}}\\,F_{\\gamma}(x)$ . Leveraging Lemma 2, the gradient $\\nabla F_{\\gamma}(x)$ can be obtained by the next lemma. ", "page_idx": 4}, {"type": "text", "text": "Lemma 3 (Danskin-like theorem for $F_{\\gamma}(x)$ ). Suppose that the conditions in Lemma 2 hold. Moreover, assume that $\\begin{array}{r}{\\gamma>\\frac{l_{f,1}}{\\alpha_{g}}}\\end{array}$ , and there exist $B_{F}\\!<\\!\\infty$ such that $\\|\\mu_{F}^{*}(x)\\|<B_{F}$ , $\\forall x\\in\\mathcal{X}$ . Then, it holds that 1. $y_{F}^{*}(x)$ and $\\mu_{F}^{*}(x)$ defined in (5) are $L_{F}$ -Lipschitz for some constant $L_{F}\\geq0$ . ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nabla F_{\\gamma}(x)=\\nabla_{x}f(x,y_{F}^{*}(x))+\\gamma\\left(\\nabla_{x}g(x,y_{F}^{*}(x))-\\nabla v(x)\\right)+\\langle\\mu_{F}^{*}(x),\\nabla_{x}g^{c}(x,y_{F}^{*}(x))\\rangle.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The proof of Lemmas 2 and 3 is provided in Appendix C. Similar to [62], the Danskin-like theorems in Lemmas 2 and 3 rely on the Lipschitzness of the solutions in (5) and (7). Different from BLO without CCs [62], the results here also hinge on the Lagrange multipliers $\\mu_{g}^{*}(x)$ and $\\mu_{F}^{*}(x)$ . ", "page_idx": 4}, {"type": "text", "text": "3 Main Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We will first introduce an algorithm tailored for BiLevel Optimization problems with inequality Coupled Constraints, and present its convergence analysis. In Section 3.2, we will propose a primal-dual solver for the inner max-min problems and characterize the overall convergence. ", "page_idx": 5}, {"type": "text", "text": "3.1 BLOCC algorithm ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "As $F_{\\gamma}(x)$ features differentiability and smoothness, we can apply a projected gradient descent (PGD)- based method to solve $\\mathrm{min}_{x\\in\\mathcal{X}}\\,F_{\\gamma}(x)$ . At iteration $t$ , update ", "page_idx": 5}, {"type": "equation", "text": "$$\nx_{t+1}=\\operatorname{Proj}_{\\mathcal{X}}\\!\\left(x_{t}-\\eta g_{F,t}\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with stepsize $\\eta$ and $g_{F,t}$ as an estimate of $\\nabla F_{\\gamma}(x_{t})$ . ", "page_idx": 5}, {"type": "text", "text": "In the previous section, we have obtained the closedform expressions of $\\nabla v(x)$ in (8) and $\\nabla F_{\\gamma}(x)$ in (9). Evaluating the closed-form expression requires finding $(y_{g}^{*}(x_{t}),\\mu_{g}^{*}(x_{t}))$ and $(y_{F}^{*}(\\bar{x}_{t}),\\mu_{F}^{*}(x_{t})\\bar{)}$ , the solutions to the max-min problem $L_{g}(\\mu,y;x)$ in (7) and $L_{F}(\\mu,y;x)$ in (5) respectively. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 Meta algorithm: BLOCC   \n1: inputs: initial points x0, yg,0, \u00b5g,0, yF,0, $\\mu_{F,0}$ ; stepsize $\\eta,\\;\\eta_{g},\\;\\eta_{F}$ ; counters $T_{g}$ , $T_{F}$ .   \n2: for $t=0,1,\\ldots,T$ do   \n3: $(y_{g,t}^{T_{g}},\\mu_{g,t}^{T_{g}})=\\mathsf{M a x M i n}(T_{g},T_{y}^{g})$   \n4: $(y_{F,t}^{T_{F}},\\mu_{F,t}^{T_{F}})=\\mathsf{M a x M i n}(T_{F},T_{y}^{F})$   \n5: update $x_{t+1}$ via (10) $\\triangleright$ with $g_{F,t}$ in (13)   \n6: end for   \n7: outputs: $(x_{T},y_{F,T}^{T_{F}})$ ", "page_idx": 5}, {"type": "text", "text": "Given $x_{t}\\,\\in\\,\\mathcal{X}$ , we can use some minmax optimization solvers with $T_{g}$ iterations on (7) and $T_{F}$ iterations on (5) to find an $\\epsilon_{g}$ -solution $(y_{g,t}^{T_{g}},\\mu_{g,t}^{T_{g}})$ and an $\\epsilon_{F}$ -solution $(y_{F,t}^{T_{F}},\\mu_{F,t}^{T_{F}})$ satisfying ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\|(y_{g,t}^{T_{g}},\\mu_{g,t}^{T_{g}})-(y_{g}^{*}(x_{t}),\\mu_{g}^{*}(x_{t}))\\|^{2}=\\mathcal{O}(\\epsilon_{g});\\ \\ \\|(y_{F,t}^{T_{F}},\\mu_{F,t}^{T_{F}})-(y_{F}^{*}(x_{t}),\\mu_{F}^{*}(x_{t}))\\|^{2}=\\mathcal{O}(\\epsilon_{F})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for target estimation accuracy $\\epsilon_{g},\\epsilon_{F}>0$ . Such an effective minmax optimization solver will be introduced in the following Section 3.2. In this way, $\\nabla v(x_{t})$ in (9) can be estimated as ", "page_idx": 5}, {"type": "equation", "text": "$$\ng_{v,t}=\\nabla_{x}g(x_{t},y_{g,t}^{T_{g}})+\\langle\\mu_{g,t}^{T_{g}},\\nabla_{x}g^{c}(x_{t},y_{g,t}^{T_{g}})\\rangle.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Leveraging $g_{v,t}$ , the gradient $\\nabla F_{\\gamma}(x)$ can be estimated via (8) as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{g_{F,t}=\\nabla_{x}f(x_{t},y_{F,t}^{T_{F}})+\\gamma\\left(\\nabla_{x}g(x_{t},y_{F,t}^{T_{F}})-g_{v,t}\\right)+\\langle\\mu_{F}^{T_{F}},\\nabla_{x}g^{c}(x,y_{F,t}^{T_{F}})\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We summarize the oracle for finding $\\operatorname*{min}F_{\\gamma}(x)$ as in Algorithm 1, which we term BLOCC, an algorithm designed for BiLevel Optimization with Coupled Constraints. Notably, our BLOCC algorithm can be seamlessly integrated with any MaxMin Solver (or min-max solver) that converges to the optimal solutions of the max-min subproblems by achieving (11). In the following, we present the convergence result of it allowing estimation error $\\epsilon_{g},\\epsilon_{F}>0$ from the MaxMin Solver. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. Suppose that the assumptions in Lemma 3 hold. Run Algorithm $^{\\,l}$ with some effective inner MaxMin solver to find (ygT,gt, \u00b5gT,gt) and $(y_{F,t}^{T_{F}},\\mu_{F,t}^{T_{F}})$ respectively $\\mathcal{O}(\\epsilon_{g})$ and $\\mathcal{O}(\\epsilon_{F})$ -optimal in squared distance as in (11). Set \u03b7 \u2264 lF1,1 with some $l_{F,1}$ defined in Lemma 3. It then holds that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\|G_{\\eta}(x_{t})\\|^{2}:=\\frac{1}{T\\eta^{2}}\\sum_{t=0}^{T-1}\\|(x_{t+1}-x_{t})\\|^{2}=\\mathcal{O}(\\gamma T^{-1}+\\gamma^{2}\\epsilon_{F}+\\gamma^{2}\\epsilon_{g}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The proof is available in Appendix D.1. Using the projected gradient $G_{\\eta}=\\eta^{-1}(x_{t+1}-x_{t})$ as the convergence metric for constrained optimization problems is standard [26]. The term $\\mathcal{O}(\\gamma^{2}\\epsilon_{F}\\!+\\!\\gamma^{2}\\epsilon_{g})$ arises from estimation errors using specific MaxMin Solver. Although the inner oracle can be any one that achieves (11), we value the computational effectiveness and therefore present a particular efficient solver Algorithm 2 in the following section. ", "page_idx": 5}, {"type": "text", "text": "3.2 MaxMin Solver for the BLO with inequality CCs ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we specify the MaxMin solver in Algorithm 1. By viewing $L_{g}(\\mu,y;x)$ in (7) and $L_{F}(\\mu,y;x)$ in (5) as $L(\\mu,y)$ for fixed given $x\\in\\mathscr{X}$ , we consider the following max-min problem ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mu\\in\\mathbb R_{+}^{d_{c}}}\\operatorname*{min}_{y\\in{\\mathcal y}}L(\\mu,y)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "which is concave (linear) in $\\mu$ and strongly convex in $y$ . We can evaluate the dual function of $L(\\mu,y)$ defined below by finding $y_{\\mu}^{*}(\\mu)$ . ", "page_idx": 6}, {"type": "equation", "text": "$$\nD(\\mu):=\\operatorname*{min}_{y\\in\\mathcal{Y}}L(\\mu,y)=L(\\mu,y_{\\mu}^{*}(\\mu))\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$$\ny_{\\mu}^{*}(\\mu):=\\arg\\operatorname*{min}_{y\\in\\mathcal{Y}}L(\\mu,y).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "According to Danskin\u2019s theorem, we have $\\nabla D(\\mu)\\;\\stackrel{!}{=}\\;\\nabla_{\\mu}L(\\mu,y_{\\mu}^{*}(\\mu))$ . Taking either $L_{g}(\\mu,y;x)$ or $L_{F}(\\mu,y;x)$ as $L(\\mu,y)$ for given $x\\in\\mathscr{X}$ , $D(\\mu)$ defined as (16) exhibits favorable properties namely smoothness and concavity, with details illustrated in Lemma 10 in Appendix D.2. We can, therefore, apply accelerated gradient methods designed for smooth and convex functions such as [52, 4]. ", "page_idx": 6}, {"type": "text", "text": "Algorithm 2 Subroutine on MaxMin $(T,T_{y})$ ", "page_idx": 6}, {"type": "text", "text": "1: inputs: initial points $y_{0},\\;\\mu_{0}$ ; stepsizes $\\eta_{1},\\eta_{2}$ ;   \ncounters $T,T_{y}$ . Blue part is the version with   \nacceleration; and red part is without.   \n2: for $t=0,\\dots,T-1$ do   \n3: update $\\mu_{t+\\frac{1}{2}}$ via (17) or $\\mu_{t+\\frac{1}{2}}=\\mu_{t}$   \n4: for $t_{y}=0,\\ldots,T_{y}-1$ do   \n5: update $y_{t,t_{y}+1}$ via (18) $\\triangleright$ set $y_{t,0}=y_{t}$   \n6: end for   \n7: update $\\mu_{t+1}$ via (19) \u25b7set yt+1 = yt,Ty   \n8: end for   \n9: outputs: $(y_{T},\\mu_{T})$ ", "page_idx": 6}, {"type": "text", "text": "At each iteration $t$ , we first perform a momentum-based update step to update $\\mu_{t+\\frac{1}{2}}$ as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mu_{t+\\frac{1}{2}}=\\mu_{t}+\\frac{t-1}{t+2}(\\mu_{t}-\\mu_{t-1}),\\quad\\mathrm{with}\\,\\mu_{-1}=\\mu_{0}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "To evaluate $\\nabla D(\\mu_{t+\\frac{1}{2}})=\\nabla_{\\mu}L(\\mu_{t+\\frac{1}{2}},y_{\\mu}^{*}(\\mu_{t+\\frac{1}{2}})$ , with an arbitrary small target accuracy $\\epsilon>0$ , we can run $T_{y}=\\mathcal{O}(\\ln(\\epsilon^{-1}))$ ) PGD steps on $L(\\mu_{t+\\frac{1}{2}},y)$ in $y$ via ", "page_idx": 6}, {"type": "equation", "text": "$$\ny_{t,t_{y}+1}=\\mathrm{Proj}_{\\mathcal{Y}}\\left(y_{t,t_{y}}-\\eta_{1}\\nabla_{y}L(\\mu_{t+\\frac{1}{2}},y_{t,t_{y}})\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Defining the output after $T_{y}$ iterations as $y_{t+1}$ , since strongly convexity of ${\\cal L}(\\mu,\\cdot)$ ensures that PGD converges linearly [9, Theorem 3.10], it implies that $\\|y_{t+1}-y_{\\mu}^{*}(\\mu_{t+\\frac{1}{2}})\\|=\\mathcal{O}(\\epsilon)$ . We can conduct ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mu_{t+1}=\\mathrm{Proj}_{\\mathbb{R}_{+}^{d_{c}}}\\bigl(\\mu_{t+\\frac{1}{2}}+\\eta_{2}\\nabla_{\\mu}L\\bigl(\\mu_{t+\\frac{1}{2}},y_{t+1}\\bigr)\\bigr).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We summarize this oracle in Algorithm 2 based on an accelerated method [52]. When we skip the momentum update, i.e. setting $\\mu_{t+\\frac{1}{2}}=\\mu_{t}$ , it is a simple PGD method on $D(\\mu)$ . ", "page_idx": 6}, {"type": "text", "text": "However, for a convex function $-D(\\mu)$ , the standard results only provide convergence of the function value, i.e. $\\begin{array}{r}{\\operatorname*{max}_{\\mu\\in\\mathbb{R}_{+}^{d_{c}}}D(\\mu)-D(\\mu_{t})\\overset{t\\rightarrow\\infty}{\\longrightarrow}0}\\end{array}$ . To establish the convergence of $\\|\\mu_{g,t}^{T_{g}}-\\mu_{g}^{*}(x_{t})\\|$ and $\\|\\mu_{F,t}^{T_{F}}-\\mu_{F}^{*}(x_{t})\\|$ , we define the dual functions associated with inner problems (7) and (5) as ", "page_idx": 6}, {"type": "equation", "text": "$$\nD_{g}(\\mu)=\\operatorname*{min}_{y\\in\\mathcal{Y}}L_{g}(\\mu,y;x)\\quad{\\mathrm{~and~}}\\quad D_{F}(\\mu)=\\operatorname*{min}_{y\\in\\mathcal{Y}}L_{F}(\\mu,y;x)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and make the following curvature assumption near the optimum. ", "page_idx": 6}, {"type": "text", "text": "Assumption 5. There exist $\\delta_{g},\\delta_{F}>0$ and $C_{\\delta_{g}},C_{\\delta_{F}}>0$ such that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\langle-\\nabla D_{g}(\\mu)+\\nabla D_{g}(\\mu_{g}^{*}(x)),\\mu-\\mu_{g}^{*}(x)\\rangle\\geq C_{\\delta_{g}}\\|\\mu-\\mu_{g}^{*}(x)\\|^{2},\\quad\\forall\\mu\\in B(\\mu_{g}^{*}(x);\\delta_{g}),}\\\\ &{\\langle-\\nabla D_{F}(\\mu)+\\nabla D_{F}(\\mu_{F}^{*}(x)),\\mu-\\mu_{F}^{*}(x)\\rangle\\geq C_{\\delta_{F}}\\|\\mu-\\mu_{F}^{*}(x)\\|^{2},\\quad\\forall\\mu\\in B(\\mu_{F}^{*}(x);\\delta_{F}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "It is worth noting that $\\langle-\\nabla D_{g}(\\mu)+\\nabla D_{g}(\\mu_{g}^{*}(x)),\\mu-\\mu_{g}^{*}(x)\\rangle\\geq0$ holds for all $\\mu$ due to concavity and in a neighborhood of the optimal, the equality only happens at the optimal due to the uniqueness of $\\mu_{g}^{*}(x)$ . The same argument applies to $D_{F}$ due to the concavity of the dual functions. Therefore, Assumption 5 essentially asserts a positive lower bound on the curvature of the left-hand side term, which is mild as it only applies to the neighborhood of the optima $\\mu_{g}^{*}(x)$ and $\\mu_{F}^{*}(x)$ . It is also weaker than the local strong concavity or global restricted secant inequality (RSI) conditions. ", "page_idx": 6}, {"type": "text", "text": "By choosing Algorithm 2 with acceleration as the MaxMin solver, we provide the convergence analysis of Algorithm 1 next, the proof of which can be found in Appendix D.2. ", "page_idx": 6}, {"type": "text", "text": ".  aSnud s.u Imf pwtieo cnhs $_{I-5}$ aAnlgd otrhiteh cmo n2d iwtiitohn sa cinc eTlehreaotiroenm  a2s  hthole di.n nLeert $\\begin{array}{r}{\\gamma>\\frac{l_{f,1}}{\\alpha_{g}}}\\end{array}$ , $\\begin{array}{r}{\\epsilon_{g}\\le\\frac{C\\delta_{g}}{2}\\delta_{g}}\\end{array}$ $\\epsilon_{F}\\,\\leq\\,\\frac{C\\delta_{F}}{2}\\delta_{F}$ C\u03b42F \u03b4F  oose  loop and input $T_{g}\\,=\\,\\mathcal{O}\\bigl(\\epsilon_{g}^{-0.5}\\bigr),T_{F}\\,=\\,\\mathcal{O}\\bigl(\\epsilon_{F}^{-0.5}\\bigr)$ , and $T_{y}^{g}\\,=\\,\\mathcal{O}(\\ln(\\epsilon_{g}^{-1})),T_{y}^{F}\\,=\\,\\mathcal{O}(\\ln(\\epsilon_{F}^{-1}))$ with proper constant stepsizes in Remark $^3$ , then the iterates generated by the Algorithm $^{\\,l}$ satisfy (11): ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\|(y_{g,t}^{T_{g}},\\mu_{g,t}^{T_{g}})-(y_{g}^{*}(x_{t}),\\mu_{g}^{*}(x_{t}))\\|^{2}=\\mathcal{O}(\\epsilon_{g});\\ \\ \\|(y_{F,t}^{T_{F}},\\mu_{F,t}^{T_{F}})-(y_{F}^{*}(x_{t}),\\mu_{F}^{*}(x_{t}))\\|^{2}=\\mathcal{O}(\\epsilon_{F}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Theorem 3 concludes the $\\mathcal{O}(\\epsilon_{g}^{-0.5})$ complexity for achieving $\\epsilon_{g}$ -optimal solutions for the constrained concave-strongly-convex problem $\\begin{array}{r}{\\operatorname*{max}_{\\mu\\in\\mathbb{R}_{+}^{d_{x}}}\\operatorname*{min}_{y\\in\\mathcal{Y}}L(\\mu,y)}\\end{array}$ , and so as for $\\epsilon_{F}$ . For solving an $\\epsilon$ - approximation problem of BLO defined in (4), we solve $\\mathrm{min}_{x\\in\\mathcal{X}}\\,F_{\\gamma}(x)$ with $\\gamma=\\mathcal{O}(\\epsilon^{-0.5})$ according to Theorem 1. In this way, to achieve $\\begin{array}{r}{\\frac{1}{T}\\sum_{t=0}^{T-1}\\|G_{\\eta}(x_{t})\\|^{2}=\\mathcal{O}(\\gamma T^{-1}+\\gamma^{2}\\epsilon_{F}+\\gamma^{2}\\epsilon_{g})\\,\\leq\\,\\epsilon}\\end{array}$ by (14), we need $\\epsilon_{g},\\epsilon_{F}=\\mathcal{O}(\\epsilon^{2})$ , i.e. complexity $\\tilde{\\mathcal{O}}(\\epsilon^{-1})$ for the MaxMin solver in Algorithm 2, and $T=\\mathcal{O}(\\gamma\\epsilon^{-1})\\stackrel{\\cdot}{=}\\mathcal{O}(\\epsilon^{-1.5})$ for the number of iteration in BLOCC (Algorithm 1). Therefore, the overall complexity is $\\tilde{\\mathcal{O}}(\\epsilon^{-2.5})$ , where $\\tilde{\\mathcal{O}}$ omits the ln terms. ", "page_idx": 7}, {"type": "text", "text": "3.3 Special case of the MaxMin Solver: $g^{c}(x,y)$ being affine in $y$ and $\\mathcal{V}=\\mathbb{R}^{d_{\\mathcal{Y}}}$ ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we investigate a special case of BLO with CCs where Assumption 5 automatically holds. Specifically, we focus on the case where $g^{c}$ is affine in $y$ and $\\mathcal{V}=\\mathbb{R}^{d_{\\boldsymbol{y}}}$ , i.e. ", "page_idx": 7}, {"type": "equation", "text": "$$\ng^{c}(x,y)=g_{1}^{c}(x)^{\\top}y-g_{2}^{c}(x).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In this case, fixing $x$ , taking $L_{g}(\\mu,y;x)$ in (7) and $L_{F}(\\mu,y;x)$ as $L(\\mu,y)$ , (16) gives ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{g}(\\mu)=\\displaystyle\\operatorname*{min}_{y\\in\\mathbb{R}^{d_{y}}}-\\langle g_{2}^{c}(x),\\mu\\rangle+\\langle y,g_{1}^{c}(x)\\mu\\rangle+g(x,y)\\quad\\mathrm{and}}\\\\ &{D_{F}(\\mu)=\\displaystyle\\operatorname*{min}_{y\\in\\mathbb{R}^{d_{y}}}-\\langle g_{2}^{c}(x),\\mu\\rangle+\\langle y,g_{1}^{c}(x)\\mu\\rangle+f(x,y)+\\gamma(g(x,y)-v(x)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "When $g_{1}^{c}(x)$ is of full column rank, both $D_{g}(\\mu)$ and $D_{F}(\\mu)$ are strongly concave according to Lemma 13 in Appendix so that Assumption 5 holds globally. Moreover, when applying PGD on $-D_{g}(\\mu)$ and $-D_{F}(\\mu)$ , strongly convexity guarantees linear convergence (Theorem 3.10 in [9]). Therefore, even without acceleration, Algorithm 2 with $T_{y}$ sufficiently large performs PGD on $-D(\\mu)$ and it converges linearly up to inner loop accuracy. Moreover, PGD on $L(\\mu,y)$ in $y$ also converges linearly. This motivates us to implement a single-loop version $\\begin{array}{r}{T_{y}=1}\\end{array}$ ) of Algorithm 2. ", "page_idx": 7}, {"type": "text", "text": "When both $y$ and $\\mu$ are unconstrained, the analysis has been established in [19]. However, as $\\mu$ is constrained to $\\mathbb{R}_{+}^{d_{c}}$ in the Lagrangian formulation for inequality constraints, the convergence analysis in [19] is not applicable, and the extension is nontrivial due to the non-differentiability of the projection. We address this technical challenge by treating $\\mathbb{R}_{+}^{d_{c}}$ as an inequality constraint and reformulating it as an unconstrained problem using Lagrange duality theory. This approach demonstrates that a single-loop $T_{y}=1$ update in Algorithm 2, without acceleration, achieves linear convergence for the max-min problem (15). The detailed proof is provided in Appendix D.3. ", "page_idx": 7}, {"type": "text", "text": "Thus, by selecting the single-loop version $\\begin{array}{r}{T_{y}\\,=\\,1;}\\end{array}$ ) of Algorithm 2 without acceleration as the MaxMin solver in Algorithm 1, we establish the following theorem with proof available in Appendix D.3. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4 (Inner linear convergence). Consider BLO with $\\mathcal{V}=\\mathbb{R}^{d_{\\boldsymbol{y}}}$ and $g^{c}(x,y)$ defined in (22). Suppose Assumptions $^{l-4}$ and the conditions in Theorem 2 hold. Suppose for any $x\\in\\mathscr{X}$ , there exist constants $s_{\\mathrm{min}}$ and $s_{\\mathrm{max}}$ such that $0\\,<\\,s_{\\mathrm{min}}\\,\\leq\\,\\sigma_{\\mathrm{min}}\\bigl(g_{1}^{c}(x)\\bigr)\\,\\leq\\,\\sigma_{\\mathrm{max}}\\bigl(g_{1}^{c}(x)\\bigr)\\,\\leq\\,s_{\\mathrm{max}}\\,<\\,\\infty.$ $\\begin{array}{r}{L e t\\triangledown\\gamma\\ y>\\frac{l_{f,1}}{\\alpha_{g}}}\\end{array}$ . If we choose Algorithm 2 without acceleration as the inner loop and input $T_{g}\\,=$ $\\mathcal{O}(\\ln(\\epsilon_{g}^{-1})),T_{F}=\\mathcal{O}(\\ln(\\epsilon_{F}^{-1}))$ , and $T_{y}^{g}=T_{y}^{F}=1$ with proper constant stepsizes in Remark 4, then the iterates generated by Algorithm $^{\\,l}$ satisfy (11): ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\|(y_{g,t}^{T_{g}},\\mu_{g,t}^{T_{g}})-(y_{g}^{*}(x_{t}),\\mu_{g}^{*}(x_{t}))\\|^{2}=\\mathcal{O}(\\epsilon_{g});\\ \\ \\|(y_{F,t}^{T_{F}},\\mu_{F,t}^{T_{F}})-(y_{F}^{*}(x_{t}),\\mu_{F}^{*}(x_{t}))\\|^{2}=\\mathcal{O}(\\epsilon_{F}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Theorem 4 establishes, for the first time, the linear convergence of a strongly convex-concave maxmin problem with a constrained maximization parameter. Similar to the previous analysis, we choose $\\gamma={\\mathcal{O}}(\\epsilon^{-0.5})$ to solve the equivalent (2a) to $\\epsilon$ -approximation problem of BLO (4). To achieve $\\begin{array}{r}{\\frac{1}{T}\\sum_{t=0}^{T-1}\\|G_{\\eta}(x_{t})\\|^{2}\\leq\\epsilon}\\end{array}$ , the number of iteration in BLOCC $T=\\mathcal{O}(\\gamma\\epsilon^{-1})=\\mathcal{O}(\\epsilon^{-1.5})$ by (14). As the inner MaxMin solver convergences linearly, the overall complexity is $\\tilde{\\mathcal{O}}(\\epsilon^{-1.5})$ where $\\tilde{\\mathcal{O}}$ omits the ln terms. We summarized the overall iteration complexity of our BLOCC algorithm in different settings in Table 1. ", "page_idx": 7}, {"type": "text", "text": "4 Numerical Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "This section reports the results of numerical experiments for three different problems: a toy example used to validate our method, an SVM training application, and a network design problem in both synthetic and real-world transportation scenarios. We provide sensitivity analysis and insights for hyper-parameter choices in Appendix G.1. In the two real-world experiments, we compare the proposed algorithm with two baselines, LV-HBA [75] and GAM [72]. The code is available at https://github.com/Liuyuan999/Penalty Based Lagrangian Bilevel. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.1 Toy example ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Consider the BLO problem with an inequality coupled constraint $\\mathcal{V}(x):=\\{y\\in\\mathcal{y}:y-x\\leq0\\}$ , given by ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\operatorname*{min}_{x\\in[0,3]}\\ f(x,y_{g}^{*}(x))=\\frac{e^{-y_{g}^{*}(x)+2}}{2+\\cos(6x)}+\\frac{1}{2}\\ln\\bigl((4x-2)^{2}+1\\bigr)}\\\\ {\\mathrm{with}\\ \\,y_{g}^{*}(x)\\in\\arg\\underset{y\\in\\mathcal{Y}(x)}{\\operatorname*{min}}\\ \\,g(x,y)=(y-2x)^{2}.\\qquad\\qquad(24)}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Problem (24) satisfies all assumptions for Theorem 2 and Theorem 4. The lower-level problem is strongly convex and $y_{g}^{*}(x)=x$ . Therefore, the BLO problem with inequality constraint in (24) reduces to $\\operatorname*{min}_{x\\in[0}$ ,3] $f(x,y)|_{y=x}$ . In Figure 2, we plot the dashed line as the intersected line of the surface $f(x,y)$ and the plane $f(x,y_{g}^{*}(x))$ , and the red points as the converged points by running BLOCC with $\\gamma\\,=\\,5$ and with 200 different initialization values. It can be seen that BLOCC consistently finds the local minima, verifying the effectiveness. ", "page_idx": 8}, {"type": "image", "img_path": "uZi7H5Ac0X/tmp/b0dbd962aab9f882073878a5a5370f835c1f1097444180d4cd7427a08f7c3fd1.jpg", "img_caption": ["Figure 2: 3-D plot of the upperlevel objective $\\bar{f}(x,y)$ of the toy example, with the line $f(x,y)|_{y=x}$ shown in dashed red and the convergence points marked as red dots. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.2 Hyperparameter optimization for SVM ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We test the performance of our algorithm BLOCC with $\\gamma=12$ when training a linear SVM model on the diabetes [20] and fourclass datasets [29]. The model is trained via the BLO formulation (1) with inequality CCs; see the details in Appendix E. We compared performance with two baselines, LV-HBA [75] and GAM [72], as they are the only existing algorithms addressing coupled constraints and experimentally evaluated on SVM problems in their respective papers. ", "page_idx": 8}, {"type": "text", "text": "Table 2 shows that the model trained by our BLOCC algorithm outperforms that of ", "page_idx": 8}, {"type": "table", "img_path": "uZi7H5Ac0X/tmp/31ec1e92af8c2c6340db3838a17ba1763784680f956d7c25db8481a4e5d37672.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 2: Numerical results on the training outcome of our BLOCC in comparison with LV-HBA [75] and GAM [72]. The first row represents accuracy mean $\\pm$ standard deviation, and the second row between brackets represents the running time until the upper-level objective\u2019s update is smaller than $1\\mathrm{e}^{-5}$ . ", "page_idx": 8}, {"type": "text", "text": "GAM [72] significantly and is of a similar level as that of LV-HBA [75]. We present some of the performance plots for the diabetes dataset as in Figure 3. Looking into the test accuracy (left), our algorithm achieves more than 0.76 accuracy in the first 2 iterations, which is significantly better than the other ones. For the upper-level objective (middle), in the first few iterations, the loss decreases significantly under all algorithms, in which our BLOCC achieves the lowest results. Moreover, in Figure 3 (right), the lower-level optimum for LV-HBA was not attained until the very end. This indicates that the decrease of upper loss between 0-40 iterations in the middle figure may be due to the suboptimality of lower-level variables. For our BLOCC and GAM, the lower-level minimum is attained as lower-level objective $g\\geq0$ in this case. ", "page_idx": 8}, {"type": "text", "text": "4.3 Transportation network design problem ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "BLO is particularly relevant in transportation, where network planning must consider different time horizons and actors. Those problems are large-scale with a large number of upper- and lower-level variables and CCs, challenging the use of traditional BLO techniques, which involve expensive calculations of second order information. As a result, our final experiment considers a network design problem, where we act as an operator whose profit is modeled as the upper-level objective that is determined by the passengers\u2019 behavior, modeled in the lower-level. We considered three networks: ", "page_idx": 8}, {"type": "image", "img_path": "uZi7H5Ac0X/tmp/6d22e80f0985431d12c2593a3e2f8d5cdab93987196abe271d07b61945fbe9fe.jpg", "img_caption": ["Figure 3: Test accuracy (left), upper loss $f(x,y)$ (middle), and lower loss $g(x,y)$ (right) for the SVM on the diabetes dataset. The experiments are executed for 50 different random train-validation-test splits, with the bold line representing the mean, and the shaded regions being the standard deviation. "], "img_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "uZi7H5Ac0X/tmp/156b1c474bdf984b7dae3e223524de25eec8603c42ff41fc77c00cd380afe954.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Table 3: Results of the transportation experiment, both in terms of running time (Runtime) and convergenced upper-level objective value (UL utility, larger there better), with stepsize $\\eta=1.6e{-4}$ . We use $\\,^{\\circ}\\!/\\,^{\\circ}$ for algorithms that cannot converge within 24 hours of execution. NN (Number of Nodes) is the number of stations in the network. Analogously, NV (Number of Variables), NC (Number of Constraints), and NNZ (Number of non-zero elements) are the number of optimization variables, constraints, and non-zero elements of the constraints matrix, respectively. ", "page_idx": 9}, {"type": "text", "text": "two synthetic networks of 3 and 9 nodes, respectively, and a real-world network of 26 nodes in the city of Seville, Spain. Further details about the formulation and the experiment can be found in Appendix F. ", "page_idx": 9}, {"type": "text", "text": "In this experiment, we only compare our BLOCC with LV-HBA [75] as the sole baseline, which is the only existing algorithm that addresses both coupled inequality $g^{c}(x,y)\\leq0$ and domain constraints $\\boldsymbol{\\wp}$ . GAM [72] and BVFSM [46] cannot handle lower-level domain constraints as they rely on the hypergradient of lower-level and require LL stationarity in an unconstrained space. From Table 3, we can see that LV-HBA failed to work efficiently, especially for large networks. This is mainly because the increased constraints render the projection step impracticable. Our BLOCC, in contrast, is much faster, and it successfully converges even with large real-world networks. We provided computational complexity analysis in Appendix H and we can conclude that our BLOCC is robust to large-scale problems. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusions and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper proposed a novel primal-dual-assisted penalty reformulation for BLO problems with coupled lower-level constraints, and developed a new first-order method BLOCC to solve the resultant problem. The non-asymptotic convergence rate of our algorithm is $\\tilde{\\mathcal{O}}(\\epsilon^{-2.5})$ , tightening to $\\tilde{\\mathcal{O}}(\\epsilon^{-1.5})$ when the lower-level constraints are affine in $y$ without any other constraints. Our method achieves the best-known convergence rate and is projection-free, making it more favorable for large-scale, highdimensional, constrained BLO problems. Experiments on SVM model training and transportation network planning showcased the effectiveness of our algorithm. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Seyed Mehdi Alizadeh, Patrice Marcotte, and Gilles Savard. Two-stage stochastic bilevel programming over a transportation network. Transportation Research Part B: Methodological, 58:92\u2013105, 2013.   \n[2] Aram V Arutyunov, Evgeniy R Avakov, and Alexey F Izmailov. Directional regularity and metric regularity. SIAM Journal on Optimization, 18(3):810\u2013833, 2007.   \n[3] Dominique Aze\u00b4 and Jean-Paul Penot. Uniformly convex and uniformly smooth convex functions. In Annales de la Facult\u00b4e des sciences de Toulouse: Math\u00b4ematiques, 1995.   \n[4] Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM journal on imaging sciences, 2(1):183\u2013202, 2009.   \n[5] Moshe E Ben-Akiva and Steven R Lerman. Discrete choice analysis: theory and application to travel demand, volume 9. MIT press, 1985.   \n[6] J. Bolte, E. Pauwels, and S. Vaiter. Automatic differentiation of nonsmooth iterative algorithms. In Advances in Neural Information Processing Systems, 2022.   \n[7] J Fr\u00b4ed\u00b4eric Bonnans and Alexander Shapiro. Perturbation analysis of optimization problems. Springer Science & Business Media, 2013.   \n[8] Jerome Bracken and James T McGill. Mathematical programs with optimization problems in the constraints. Operations Research, 21(1):37\u201344, 1973.   \n[9] Se\u00b4bastien Bubeck. Convex Optimization: Algorithms and Complexity. Foundations and Trends\u00ae in Machine Learning, 2015.   \n[10] Luis Cadarso and Angel Marin. Combining robustness and recovery in rapid transit network design. Transportmetrica A: Transport Science, 12:1\u201326, 11 2015.   \n[11] Ennio Cascetta. Transportation systems analysis: models and applications, volume 29. Springer Science & Business Media, 2009.   \n[12] Tianyi Chen, Yuejiao Sun, Quan Xiao, and Wotao Yin. A single-timescale method for stochastic bilevel optimization. In International Conference on Artificial Intelligence and Statistics, virtual, 2022.   \n[13] Tianyi Chen, Yuejiao Sun, and Wotao Yin. Closing the gap: Tighter analysis of alternating stochastic gradient methods for bilevel problems. In Advances in Neural Information Processing Systems, Virtual, 2021.   \n[14] Beno\u02c6\u0131t Colson, Patrice Marcotte, and Gilles Savard. An overview of bilevel optimization. Annals of operations research, 153(1):235\u2013256, 2007.   \n[15] Jean-Philippe Co\u02c6te\u00b4, Patrice Marcotte, and Gilles Savard. A bilevel modelling approach to pricing and fare optimisation in the airline industry. Journal of Revenue and Pricing Management, 2:23\u201336, 2003.   \n[16] Olivier Devolder, Franc\u00b8ois Glineur, and Yurii Nesterov. First-order methods of smooth convex optimization with inexact oracle. Mathematical Programming, 146:37\u201375, 2014.   \n[17] Asen L Dontchev and R Tyrrell Rockafellar. Implicit functions and solution mappings, volume 543. Springer, 2009.   \n[18] Dmitriy Drusvyatskiy and Adrian S Lewis. Error bounds, quadratic growth, and linear convergence of proximal methods. Mathematics of Operations Research, 43(3):919\u2013948, 2018.   \n[19] Simon S Du and Wei Hu. Linear convergence of the primal-dual gradient method for convexconcave saddle point problems without strong convexity. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 196\u2013205, 2019.   \n[20] Dheeru Dua and Casey Graff. Uci machine learning repository, 2017. Accessed: 2024-05-21.   \n[21] Laureano Escudero and Susana Mun\u02dcoz. An approach for solving a modification of the extended rapid transit network design problem. Top, 17:320\u2013334, 12 2009.   \n[22] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning, pages 1126\u20131135, Sydney, Australia, 2017.   \n[23] Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and reverse gradient-based hyperparameter optimization. In International Conference on Machine Learning, pages 1165\u20131173, 2017.   \n[24] Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil. Bilevel programming for hyperparameter optimization and meta-learning. In International Conference on Machine Learning, Stockholm, Sweden, 2018.   \n[25] Lucy L Gao, Jane Ye, Haian Yin, Shangzhi Zeng, and Jin Zhang. Value function based difference-of-convex algorithm for bilevel hyperparameter selection problems. In International Conference on Machine Learning, pages 7164\u20137182, Baltimore, MD, 2022.   \n[26] Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang. Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization. Mathematical Programming, 155(1):267\u2013305, 2016.   \n[27] Saeed Ghadimi and Mengdi Wang. Approximation methods for bilevel programming. arXiv preprint arXiv:1802.02246, 2018.   \n[28] Riccardo Grazzi, Luca Franceschi, Massimiliano Pontil, and Saverio Salzo. On the iteration complexity of hypergradient computation. In International Conference on Machine Learning, pages 3748\u20133758, virtual, 2020.   \n[29] Tin Kam Ho and Eugene M Kleinberg. Building projectable classifiers of arbitrary complexity. In Proceedings of 13th International Conference on Pattern Recognition, volume 2, pages 880\u2013885, 1996.   \n[30] Thomas Hofmann, Bernhard Sch\u00a8olkopf, and Alexander J Smola. Kernel methods in machine learning. 2008.   \n[31] Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A two-timescale framework for bilevel optimization: Complexity analysis and application to actor-critic. SIAM Journal on Optimization, 33(1), 2023.   \n[32] Kazufumi Ito and Karl Kunisch. Lagrange Multiplier Approach to Variational Problems and Applications, volume 15 of Advances in Design and Control. Not specified in the provided information, 2008. Includes bibliographical references and index.   \n[33] Kaiyi Ji, Mingrui Liu, Yingbin Liang, and Lei Ying. Will bilevel optimizers benefti from loops. arXiv preprint arXiv:2205.14224, 2022.   \n[34] Kaiyi Ji, Junjie Yang, and Yingbin Liang. Provably faster algorithms for bilevel optimization and applications to meta-learning. In Advances in Neural Information Processing Systems, 2020.   \n[35] Kaiyi Ji, Junjie Yang, and Yingbin Liang. Bilevel optimization: Convergence analysis and enhanced design. In International Conference on Machine Learning, virtual, 2021.   \n[36] Sham Kakade, Shai Shalev-Shwartz, Ambuj Tewari, et al. On the duality of strong convexity and strong smoothness: Learning applications and matrix regularization. Unpublished Manuscript, http://ttic. uchicago. edu/shai/papers/KakadeShalevTewari09. pdf, 2(1):35, 2009.   \n[37] Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximalgradient methods under the polyak-\u0142ojasiewicz condition. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2016, Riva del Garda, Italy, September 19-23, 2016, Proceedings, Part I 16, pages 795\u2013811, 2016.   \n[38] Narendra Karmarkar. A new polynomial-time algorithm for linear programming. In Proceedings of the sixteenth annual ACM symposium on Theory of computing, pages 302\u2013311, 1984.   \n[39] Prashant Khanduri, Ioannis Tsaknakis, Yihua Zhang, Jia Liu, Sijia Liu, Jiawei Zhang, and Mingyi Hong. Linearly constrained bilevel optimization: A smoothed implicit gradient approach. In International Conference on Machine Learning, pages 16291\u201316325, 2023.   \n[40] Prashant Khanduri, Siliang Zeng, Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A near-optimal algorithm for stochastic bilevel optimization via double-momentum. In Advances in Neural Information Processing Systems, Virtual, 2021.   \n[41] Jeongyeol Kwon, Dohyun Kwon, Stephen Wright, and Robert D Nowak. A fully first-order method for stochastic bilevel optimization. In International Conference on Machine Learning, pages 18083\u201318113, 2023.   \n[42] Jeongyeol Kwon, Dohyun Kwon, Steve Wright, and Robert Nowak. On penalty methods for nonconvex bilevel optimization and first-order stochastic approximation. In International Conference on Learning Representations, Vienna, Austria, 2024.   \n[43] Junyi Li, Bin Gu, and Heng Huang. A fully single loop algorithm for bilevel optimization without hessian inverse. In Association for the Advancement of Artificial Intelligence, pages 7426\u20137434, virtual, 2022.   \n[44] Bo Liu, Mao Ye, Stephen Wright, Peter Stone, and Qiang Liu. Bome! bilevel optimization made easy: A simple first-order approach. Advances in neural information processing systems, 35:17248\u201317262, 2022.   \n[45] Risheng Liu, Xuan Liu, Xiaoming Yuan, Shangzhi Zeng, and Jin Zhang. A value-function-based interior-point method for non-convex bi-level optimization. In International conference on machine learning, pages 6882\u20136892, 2021.   \n[46] Risheng Liu, Xuan Liu, Shangzhi Zeng, Jin Zhang, and Yixuan Zhang. Value-function-based sequential minimization for bi-level optimization. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.   \n[47] Risheng Liu, Yaohua Liu, Shangzhi Zeng, and Jin Zhang. Towards gradient-based bilevel optimization with non-convex followers and beyond. In Advances in Neural Information Processing Systems, volume 34, pages 8662\u20138675, 2021.   \n[48] Songtao Lu. Slm: A smoothed first-order lagrangian method for structured constrained nonconvex optimization. Advances in Neural Information Processing Systems, 36, 2023.   \n[49] Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimization through reversible learning. In International Conference on Machine Learning, pages 2113\u20132122, Lille, France, 2015.   \n[50] Patrice Marcotte. Network design problem with congestion effects: A case of bilevel programming. Mathematical programming, 34(2):142\u2013162, 1986.   \n[51] Akshay Mehra and Jihun Hamm. Penalty method for inversion-free deep bilevel optimization. In Asian conference on machine learning, pages 347\u2013362, 2021.   \n[52] Yu Nesterov. Smooth minimization of non-smooth functions. Mathematical programming, 103:127\u2013152, 2005.   \n[53] Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. arXiv preprint arXiv:1803.02999, 2018.   \n[54] Norbert Oppenheim. Equilibrium trip distribution/assignment with variable destination costs. Transportation Research Part B: Methodological, 27(3):207\u2013217, 1993.   \n[55] Fabian Pedregosa. Hyperparameter optimization with approximate gradient. In International conference on machine learning, pages 737\u2013746, 2016.   \n[56] Fernando Real-Rojas. Dise\u02dcno de redes de transporte: integrando la competencia modal con te\u00b4cnicas de optimizacio\u00b4n convexa, BSc Thesis, King Juan Carlos University. June 2024.   \n[57] Fernando Real-Rojas, Victor M. Tenorio, and Antonio G. Marques. A sparse nonlinear approach for designing hub-and-spoke air transportation networks. In Asilomar Conference on Signals, Systems, and Computers, Pacific Grove, CA, 2024.   \n[58] R. Tyrrell Rockafellar. Convex Analysis. Princeton University Press, Princeton, NJ, USA, 1970s.   \n[59] Andrzej Ruszczyn\u00b4ski. Nonlinear Optimization. Princeton University Press, 2006.   \n[60] Amirreza Shaban, Ching-An Cheng, Nathan Hatch, and Byron Boots. Truncated backpropagation for bilevel optimization. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 1723\u20131732, 2019.   \n[61] Han Shen and Tianyi Chen. A single-timescale analysis for stochastic approximation with multiple coupled sequences. Advances in Neural Information Processing Systems, 35:17415\u2013 17429, 2022.   \n[62] Han Shen, Quan Xiao, and Tianyi Chen. On penalty-based bilevel gradient descent method. In International Conference on Machine Learning, Honolulu, HI, 2023.   \n[63] Han Shen, Zhuoran Yang, and Tianyi Chen. Principled penalty-based methods for bilevel reinforcement learning and RLHF. In International Conference on Machine Learning, Vienna, Austria, 2024.   \n[64] Ankur Sinha, Samish Bedi, and Kalyanmoy Deb. Bilevel optimization based on kriging approximations of lower level optimal value function. In 2018 IEEE congress on evolutionary computation (CEC), pages 1\u20138. IEEE, 2018.   \n[65] Ankur Sinha, Pekka Malo, and Kalyanmoy Deb. A review on bilevel optimization: from classical to evolutionary approaches and applications. IEEE Transactions on Evolutionary Computation, 22(2):276\u2013295, 2017.   \n[66] Daouda Sow, Kaiyi Ji, and Yingbin Liang. On the convergence theory for hessian-free bilevel algorithms. volume 35, pages 4136\u20134149, 2022.   \n[67] Bradly Stadie, Lunjun Zhang, and Jimmy Ba. Learning intrinsic rewards as a bi-level optimization problem. In Conference on Uncertainty in Artificial Intelligence, virtual, 2020.   \n[68] Luis N Vicente and Paul H Calamai. Bilevel and multilevel programming: A bibliography review. Journal of Global optimization, 5(3):291\u2013306, 1994.   \n[69] Gerd Wachsmuth. On licq and the uniqueness of lagrange multipliers. Operations Research Letters, 41(1):78\u201380, 2013.   \n[70] Quan Xiao, Songtao Lu, and Tianyi Chen. A generalized alternating method for bilevel learning under the polyak-\u0142ojasiewicz condition. In Advances in Neural Information Processing Systems, 2023.   \n[71] Quan Xiao, Han Shen, Wotao Yin, and Tianyi Chen. Alternating implicit projected sgd and its efficient variants for equality-constrained bilevel optimization. In International Conference on Artificial Intelligence and Statistics, 2023.   \n[72] Siyuan Xu and Minghui Zhu. Efficient gradient approximation method for constrained bilevel optimization. In Proceedings of the AAAI Conference on Artificial Intelligence, 2023.   \n[73] Haikuo Yang, Luo Luo, Chris Junchi Li, Michael Jordan, and Maryam Fazel. Accelerating inexact hypergradient descent for bilevel optimization. In OPT 2023: Optimization for Machine Learning, 2023.   \n[74] Wei Yao, Haian Yin, Shangzhi Zeng, and Jin Zhang. Overcoming lower-level constraints in bilevel optimization: A novel approach with regularized gap functions. arXiv preprint arXiv:2406.01992, 2024.   \n[75] Wei Yao, Chengming Yu, Shangzhi Zeng, and Jin Zhang. Constrained bi-level optimization: Proximal lagrangian value function approach and hessian-free algorithm. arXiv preprint arXiv:2401.16164, 2024.   \n[76] Jane J Ye and Daoli Zhu. Optimality conditions for bilevel programming problems. Optimization, 33(1):9\u201327, 1995.   \n[77] Jane J Ye, Daoli Zhu, and Qiji Jim Zhu. Exact penalization and necessary optimality conditions for generalized bilevel programming problems. SIAM Journal on optimization, 7(2):481\u2013507, 1997. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Supplementary Material for \u201cA Primal-Dual-Assisted Penalty Approach to Bilevel Optimization with Coupled Constraints\u201d ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A Preliminaries 16 ", "page_idx": 15}, {"type": "text", "text": "B Analysis of the Penalty-Based Lagrangian Reformulation 17 ", "page_idx": 15}, {"type": "text", "text": "B.1 Proof of Lemma 1 17   \nB.2 Proof of Theorem 1 17 ", "page_idx": 15}, {"type": "text", "text": "C Analysis of the Differentiability of Value Functions 18 ", "page_idx": 15}, {"type": "text", "text": "C.1 Proof of Lemma 2 19   \nC.2 Proof of Lemma 3 19 ", "page_idx": 15}, {"type": "text", "text": "D Convergence Analysis of the Main Result 20 ", "page_idx": 15}, {"type": "text", "text": "D.1 Proof of Theorem 2 20   \nD.2 Proof of Theorem 3 21   \nD.3 Proof of Theorem 4 24 ", "page_idx": 15}, {"type": "text", "text": "E Applications to Hyperparameter Optimization for SVM 29 ", "page_idx": 15}, {"type": "text", "text": "E.1 Problem formulation . 29   \nE.2 Experiment details . 30 ", "page_idx": 15}, {"type": "text", "text": "F Applications to Transportation Network Planning 31 ", "page_idx": 15}, {"type": "text", "text": "F.1 Problem formulation . . 31   \nF.2 Numerical results for the 3-node network . 34   \nF.3 Numerical results for the 9-node network . 35   \nF.4 Numerical results for the Seville network 36   \nG Sensitivity Analysis 37   \nG.1 Sensitivity analysis for the toy example . . . 37   \nG.2 Sensitivity Analysis for the 3-node network . . . 37 ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "H Analysis of the Computational Complexity 38 ", "page_idx": 15}, {"type": "text", "text": "H.1 Complexity comparison . . . 38   \nH.2 Complexity analysis of BLOCC . . 39 ", "page_idx": 15}, {"type": "text", "text": "A Preliminaries ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "This section will provide some preliminaries for our subsequent analysis. ", "page_idx": 15}, {"type": "text", "text": "Definition 5. For a convex function $h:\\mathbb{R}^{d_{q}}\\rightarrow\\mathbb{R}$ whose domain is $\\mathcal{Q}\\subseteq\\mathbb{R}^{d_{q}}$ , the Legendre conjugate of $h^{\\ast}:\\mathcal{Q}^{\\ast}\\to\\mathbb{R}$ is defined as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h^{*}(q):=\\displaystyle\\operatorname*{sup}_{q^{\\prime}\\in\\mathcal{Q}}\\{\\langle q^{\\prime},q\\rangle-h(q^{\\prime})\\}=-\\displaystyle\\operatorname*{inf}_{q^{\\prime}\\in\\mathcal{Q}}\\{-\\langle q^{\\prime},q\\rangle+h(q^{\\prime})\\},}\\\\ {\\forall q\\in\\mathcal{Q}^{*}:=\\{q\\in\\mathbb{R}^{d_{q}}:\\displaystyle\\operatorname*{sup}_{q^{\\prime}\\in\\mathcal{Q}}\\{\\langle q^{\\prime},q\\rangle-h(q^{\\prime})\\}<\\infty\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Remark 1. When $h$ is strongly convex in $\\mathbb{R}^{d_{q}}$ , it is lower bounded and therefore $\\mathcal{Q}^{\\ast}=\\mathbb{R}^{d_{y}}$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma 4. Suppose $h:\\mathbb{R}^{d_{y}}\\rightarrow\\mathbb{R}$ is $l_{h,1}$ -smooth and $\\alpha_{h}$ -strongly convex and its domain $\\mathcal{Q}\\subseteq\\mathbb{R}^{d_{q}}$ is convex, closed and non-empty. ", "page_idx": 15}, {"type": "text", "text": "1. If $\\mathcal{Q}=\\mathbb{R}^{d_{q}}$ , the gradient mappings $\\nabla h$ and $\\nabla h^{*}$ are inverse of each other ([58]); and $h^{\\ast}:\\mathbb{R}^{d_{q}}\\rightarrow\\mathbb{R}$ is $\\frac{1}{\\alpha_{h}}$ -smooth and $\\frac{\\bar{1}}{l_{h,1}}$ -strongly convex (Proposition $2.6\\,\\langle3J,$ ). ", "page_idx": 15}, {"type": "text", "text": "2. If $\\mathcal{Q}\\subset\\mathbb{R}^{d_{q}}$ , $h^{*}$ is $\\frac{1}{\\alpha_{h}}$ -smooth ([36]) and and convex (Theorem 4.43 [32]). ", "page_idx": 16}, {"type": "text", "text": "Lemma 5. Suppose $\\mathcal{Q}\\subseteq\\mathbb{R}^{d_{q}}$ is convex, closed and non-empty, $h:\\mathbb{R}^{d_{q}}\\rightarrow\\mathbb{R}$ is strongly convex on $\\mathcal{Q},\\;h^{c}:\\mathbb{R}^{d_{q}}\\;\\middle\\rightarrow\\;\\mathbf{\\bar{R}}^{d_{c}}$ is convex on $\\mathcal{Q}$ and $d_{c}$ is finite, and $\\{q\\in\\mathcal{Q}:h^{c}(q)\\leq0\\}$ is non-empty. ", "page_idx": 16}, {"type": "text", "text": "1. The problem $\\operatorname*{min}_{q\\in\\{q\\in\\mathcal{Q}:h^{c}(q)\\leq0\\}}h(q)$ has a unique feasible solution. 2. When linear independence constraint qualification $(L I C Q)$ condition holds for $h^{c}(q)$ , the Lagrange multiplier for the problem $\\operatorname*{min}_{q\\in\\{q\\in\\mathcal{Q}:h^{c}(q)\\leq0\\}}h(q)$ , i.e. solution to the problem $\\begin{array}{r}{\\operatorname*{max}_{\\mu\\in\\mathbb{R}_{+}^{d_{c}}}\\operatorname*{min}_{q\\in\\mathcal{Q}}h(q)+\\langle\\mu,h^{c}(q)\\rangle}\\end{array}$ is unique [69]. ", "page_idx": 16}, {"type": "text", "text": "Lemma 6 (Lemma 3.1 in [9]). Suppose $\\mathcal{Q}\\subseteq\\mathbb{R}^{d_{q}}$ is convex, closed, and nonempty. For any $q_{1}\\in\\mathbb{R}^{d_{q}}$ and any $q_{2}\\in{\\mathcal{Q}},$ it follows that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\langle\\mathrm{Proj}_{\\mathcal{Q}}(q_{1})-q_{2},\\mathrm{Proj}_{\\mathcal{Q}}(q_{1})-q_{1}\\rangle\\leq0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In this way, take $q_{1}=q_{3}-\\eta g$ for any $q_{3}\\in{\\mathcal{Q}},$ and denote $q_{3}^{\\eta,g}=\\mathrm{Proj}_{\\mathcal{Q}}(q_{3}-\\eta g)$ as a projected gradient update in direction $g$ with stepsize $\\eta,$ , we have, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\langle g,q_{3}^{\\eta,g}-q_{2}\\rangle\\leq-\\frac{1}{\\eta}\\langle q_{3}^{\\eta,g}-q_{2},q_{3}^{\\eta,g}-q_{3}\\rangle,\\quad\\forall q_{2},q_{3}\\in\\mathcal{Q}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Lemma 7 (Theorem 3.10 [9]). Suppose a differentiable function $h$ is $l_{h,1}$ -smooth and $\\alpha_{h_{2}}$ -strongly convex. Consider the constrained problem $\\operatorname*{min}_{q\\in\\mathcal{Q}}h(q)$ where $\\mathcal{Q}$ is non-empty, closed and convex. Projected Gradient Descent with \u03b7 \u2264lh1,1 converges linearly to the unique $q^{*}=\\arg\\operatorname*{min}_{q\\in\\mathcal{Q}}h(q)$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\operatorname{Proj}_{Q}(q-\\eta\\nabla h(q))-q^{*}\\|\\leq(1-\\alpha\\eta)^{1/2}\\|q-q^{*}\\|\\leq(1-\\alpha\\eta/2)\\|q-q^{*}\\|,\\quad\\forall q\\in\\mathcal{Q}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B Analysis of the Penalty-Based Lagrangian Reformulation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Proof of Lemma 1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "According to Lemma 5, for any fixed $x$ , there exists a unique $\\mu_{g}^{*}(x)$ . Therefore, according to Lagrange duality theorem, for any fixed $x$ , the primal problem ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{y\\in\\mathcal{Y}}g(x,y)\\quad\\mathrm{s.t.}\\quad g^{c}(x,y)\\leq0\\quad\\Leftrightarrow\\quad\\operatorname*{min}_{y\\in\\mathcal{Y}}g(x,y)+\\langle\\mu_{g}^{*}(x),g^{c}(x,y)\\rangle.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "As $g(x,y)$ is $\\alpha_{g}$ -strongly convex in $y$ and $g^{c}(x,y)$ is convex in $y$ , we know $g(x,y)\\!+\\!\\langle\\mu_{g}^{*}(x),g^{c}(x,y)\\rangle$ is $\\alpha_{g}$ -strongly convex in $y$ , where the modulus $\\alpha_{g}$ is independent of $x$ . Therefore, according to Appendix $\\boldsymbol{\\mathrm F}$ and $\\mathrm{G}$ in [37], and Theorem 3.3 in [18], the quadratic growth in statement 1 can be concluded. ", "page_idx": 16}, {"type": "text", "text": "As $g(x,y)$ is strongly convex in $y$ , and $y(x)$ is a non-empty, closed and convex set under assumption 3, there exists a unique solution $y_{g}^{*}(x)$ such that $g(x,y_{g}^{*}(x))=v(x)$ by Lemma 5. In this way, if $y\\ne y_{g}^{*}(x)$ and $y\\in\\mathcal{Y}(x)$ , we have $\\overleftarrow{g}(x,y)>v(x)$ . This completes the proof of statement 2. ", "page_idx": 16}, {"type": "text", "text": "B.2 Proof of Theorem 1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We know from Lemma 1 that $\\begin{array}{r}{g(x,y)-v(x)\\ge\\frac{\\alpha_{g}}{2}\\|y-y_{g}^{*}(x)\\|^{2}}\\end{array}$ and $g(x,y)=v(x)$ if and only if $y=y_{g}^{*}(x)$ . This is squared-distance bound follows Definition 1 in [62]. Under Lipschitzness of $f(x,y)$ with respect to $y$ , finding the solutions to the $\\epsilon$ -approximate problem in (4) is equivalent to finding the solutions to its penalty reformulation ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{(x,y)\\in\\{x\\times y:g^{c}(x,y)\\leq0\\}}f(x,y)+\\gamma(g(x,y)-v(x))\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with $\\gamma=\\mathcal{O}(\\epsilon^{-0.5})$ following Theorems 1 and 2 in [62]. ", "page_idx": 16}, {"type": "text", "text": "Moreover, jointly finding solution for $(x,y)$ in (28) is in equivalence to finding solutions in ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in{\\mathcal{X}}}\\operatorname*{min}_{y\\in{\\mathcal{Y}}(x)}f(x,y)+\\gamma(g(x,y)-v(x)).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The proof of this equivalence is as follows. Suppose $(x_{0},y_{0})\\in\\{\\mathcal{X}\\times\\mathcal{Y}:g^{c}(x,y)\\leq0\\}$ being a solution to (28). Suppose for any $x\\in\\mathscr{X}$ , $\\begin{array}{r}{y_{F}^{*}(x)\\in\\arg\\operatorname*{min}_{y\\in\\mathcal{Y}(x)}f(x,y)+\\gamma(g(x,y)-v(x)}\\end{array}$ ). We know that for any $x\\in\\mathscr{X}$ , $y\\in\\mathcal{Y}(x)$ , it follows that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f(x_{0},y_{0})+\\gamma(g(x_{0},y_{0})-v(x_{0}))\\leq\\!f(x,y_{\\cal F}^{*}(x))+\\gamma(g(x,y_{\\cal F}^{*}(x))-v(x))}\\\\ {\\leq\\!f(x,y)+\\gamma(g(x,y)-v(x)).\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This means any solution to (28) is a solution to (29). On the other hand, suppose $x_{0}\\in\\mathcal{X},y_{F}^{*}(x_{0})\\in$ $\\mathcal{V}(x_{0})$ is a solution to (29). We know that for any $(x,y)\\in\\{\\mathcal{X}\\times\\mathcal{Y}:g^{c}(x,\\overbar{y})\\leq0\\}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(x_{0},y_{F}^{*}(x_{0}))+\\gamma(g(x_{0},y_{F}^{*}(x_{0}))-v(x_{0}))\\leq f(x,y_{F}^{*}(x))+\\gamma(g(x,y_{F}^{*}(x))-v(x))}\\\\ &{\\qquad\\qquad\\leq f(x,y)+\\gamma(g(x,y)-v(x)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This means any solution to (29) is a solution to (28). ", "page_idx": 17}, {"type": "text", "text": "Besides, we know $f(x,y)$ is $l_{f,1}$ -smooth, $g(x,y)$ is $\\alpha_{g}$ -strongly convex in $y$ . By the definitions of strongly convexity and smoothness, we know for fixed $x$ , for any $y_{1},y_{2}\\in\\mathcal{Y}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad f(x,y_{1})+\\gamma(g(x,y_{1})-v(x))-f(x,y_{2})+\\gamma(g(x,y_{2})-v(x))}\\\\ &{=f(x,y_{1})-f(x,y_{2})+\\gamma(g(x,y_{1})-g(x,y_{2}))}\\\\ &{\\geq\\langle\\nabla_{y}f(x,y_{2}),y_{1}-y_{2}\\rangle-\\frac{l_{f,1}}{2}\\Vert y_{1}-y_{2}\\Vert^{2}+\\gamma\\langle\\nabla_{y}g(x,y_{2}),y_{1}-y_{2}\\rangle+\\gamma\\frac{\\alpha_{g}}{2}\\Vert y_{1}-y_{2}\\Vert^{2}}\\\\ &{=\\langle\\nabla_{y}f(x,y_{2})+\\gamma\\nabla_{y}g(x,y_{2}),y_{1}-y_{2}\\rangle+\\frac{\\gamma\\alpha_{g}-l_{f,1}}{2}\\Vert y_{1}-y_{2}\\Vert^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This proves that $f(x,y)\\!+\\!\\gamma(g(x,y)\\!-\\!v(x))$ is $(\\gamma\\alpha_{g}\\!-\\!l_{f,1})$ -strongly convex in $y$ . Moreover, according to Assumption 2, the constraint $g^{c}(x,y)$ is convex in $y$ , and $\\mathrm{min}_{y\\in\\mathcal{Y}(x)}\\,f(x,\\bar{y})+\\gamma(g(x,y)-v(x)\\bar{)}$ is equivalent to its equivalent $L$ agrangian Dual Form [59] ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mu\\in\\mathbb{R}_{+}^{d_{c}}}\\operatorname*{min}_{y\\in\\mathcal{Y}}f(x,y)+\\gamma(g(x,y)-v(x))+\\langle\\mu,g^{c}(x,y)\\rangle.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, (28) can be recovered to (2a) and this completes the proof. ", "page_idx": 17}, {"type": "text", "text": "C Analysis of the Differentiability of Value Functions ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Lemma 8 (Theorem 2.16 in [32]). Suppose $h(x,y)$ is strongly convex in $y\\in\\mathcal{V}$ and is Lipschitz with respect to $x\\in\\mathscr{X}$ , $h^{c}(x,y)$ is convex in y and is Lipschitz with respect to $x$ , and both $\\boldsymbol{\\wp}$ and $\\{y\\in\\mathcal{y}:$ $h^{c}(x,y)\\leq0\\}$ are non-empty, closed, and convex. For the problem $\\begin{array}{r}{\\operatorname*{min}_{y\\in\\{y\\in\\mathcal{Y}:h^{c}(x,y)\\leq0\\}}h(x,y)}\\end{array}$ , the unique solution $y_{h}^{*}(x)$ and unique Lagrange multiplier $\\mu_{h}^{*}(x)$ , defined as ", "page_idx": 17}, {"type": "equation", "text": "$$\n(y_{h}^{*}(x),\\mu_{h}^{*}(x)):=\\arg\\operatorname*{max}_{\\mu\\in\\mathbb{R}_{+}^{d_{x}}}\\operatorname*{min}_{y\\in\\mathcal{Y}}h(x,y)+\\langle\\mu,h^{c}(x,y)\\rangle,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "is Lipschitz in $x$ . In other words, there exist $L_{h}\\geq0$ that, for all $x_{1},x_{2}\\in\\mathcal{X}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|(y_{h}^{*}(x_{1});\\mu_{h}^{*}(x_{1}))-(y_{h}^{*}(x_{2});\\mu_{h}^{*}(x_{2}))\\|\\leq L_{h}\\|x_{1}-x_{2}\\|.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Before proving Lemmas 2 and 3, we would like to introduce a more general form. ", "page_idx": 17}, {"type": "text", "text": "Lemma 9. Suppose $\\boldsymbol{\\wp}$ and $\\{y\\in\\mathcal{y}:h^{c}(x,y)\\leq0\\}$ are both non-empty, closed and convex, $h(x,y)$ is jointly smooth in $(x,y)$ and is strongly convex in $y$ , $h^{c}(x,y)$ is convex in $y$ , and both $h(x,y)$ and $h^{c}(x,y)$ are Lipschitz with respect to $x$ . Then we have ", "page_idx": 17}, {"type": "equation", "text": "$$\nv_{h}(x)=\\operatorname*{min}_{y\\in\\mathcal{Y}}h(x,y)\\quad s.t.\\quad h^{c}(x,y)\\leq0\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "is differentiable with its gradient as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\nabla v_{h}(x)=\\nabla_{x}h(x,y_{h}^{*}(x))+\\langle\\mu_{h}^{*}(x),h^{c}(x,y_{h}^{*}(x))\\rangle,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $(y_{h}^{*}(x),\\mu_{h}^{*}(x))$ defined in (32) are unique. ", "page_idx": 17}, {"type": "text", "text": "Proof. We prove this using Theorem 4.24 in [7]. ", "page_idx": 18}, {"type": "text", "text": "i) As $h(x,y)$ being strongly convex in $y$ , Condition 1 in Theorem 4.24 in [7] is satisfied and the solution sets are of singleton value $(y_{h}^{*}(x),\\mu_{h}^{*}(x))$ according to Lemma 5. ", "page_idx": 18}, {"type": "text", "text": "ii) Moreover, the smoothness of $h(x,y)$ guarantees Robinson\u2019s constraint qualification [2], which implies the directional regularity condition (Definition 4.8 in [7]) for any direction $d$ (Theorem 4.9. (ii) in [7]). This guarantees Condition 2 in Theorem 4.24 in [7] can be satisfied for all directions $d$ . ", "page_idx": 18}, {"type": "text", "text": "iii) Additionally, under the Lipschitzness of $h(x,y)$ and $h^{c}(x,y)$ with respect to $x$ , $y_{h}^{*}(x),\\mu_{h}^{*}(x)$ are Lipschitz according to Lemma 8. This implies condition 3 in Theorem 4.24 in [7] holds. ", "page_idx": 18}, {"type": "text", "text": "In this way, all conditions in Theorem 4.24 in [7] hold and it gives the gradient as in (33) for unique $(y_{h}^{*}(x),\\mu_{h}^{*}(x))$ . This completes the proof. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "C.1 Proof of Lemma 2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. The problem $\\operatorname*{min}_{y\\in\\mathcal{Y}}g(x,y)$ s.t. $g^{c}(x,y)\\,\\leq\\,0$ fits in the setting of Lemma 9 by taking $h(x,y)=g(x,y)$ and $h^{c}(\\bar{x},y)=g^{c}(x,y)$ . Therefore the derivative (8) can be obtained accordingly. Moreover, for any $x_{1},x_{2}\\in\\mathcal{X}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|\\nabla v(x_{1})-\\nabla v(x_{2})\\|}\\\\ &{=\\|\\nabla v\\ g(x_{1},y_{g}^{*}(x_{1}))+\\langle\\mu_{g}^{*}(x_{1}),\\nabla_{x}g^{c}(x_{1},y_{g}^{*}(x_{1}))\\rangle-\\nabla_{x}g(x_{2},y_{g}^{*}(x_{2}))}\\\\ &{\\quad-\\langle\\mu_{g}^{*}(x_{2}),\\nabla_{x}g^{c}(x_{2},y_{g}^{*}(x_{2}))\\rangle\\|}\\\\ &{\\overset{(a)}{\\leq}\\|\\nabla_{x}g(x_{1},y_{g}^{*}(x_{1}))-\\nabla_{x}g(x_{2},y_{g}^{*}(x_{2}))\\|}\\\\ &{\\quad+\\|\\langle\\mu_{g}^{*}(x_{1}),\\nabla_{x}g^{c}(x_{1},y_{g}^{*}(x_{1}))\\rangle-\\langle\\mu_{g}^{*}(x_{1}),\\nabla_{x}g^{c}(x_{2},y_{g}^{*}(x_{2}))\\rangle\\|}\\\\ &{\\quad+\\|\\langle\\mu_{g}^{*}(x_{1}),\\nabla_{x}g^{c}(x_{2},y_{g}^{*}(x_{2}))\\rangle-\\langle\\mu_{g}^{*}(x_{2}),\\nabla_{x}g^{c}(x_{2},y_{g}^{*}(x_{2}))\\rangle\\|}\\\\ &{\\overset{(b)}{\\leq}(l_{g,1}+B_{g}l_{g^{*},1})(\\|x_{1}-x_{2}\\|+\\|y_{g}^{*}(x_{1})-y_{g}^{*}(x_{2})\\|)+l_{g^{*},0}\\|\\mu_{g}^{*}(x_{1})-\\mu_{g}^{*}(x_{2})\\|}\\\\ &{\\overset{(c)}{\\leq}((l_{g,1}+B_{g}l_{g^{*},1})(1+L_{g},+l_{g,0}L_{g})\\|x_{1}-x_{2}\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $(a)$ follows triangle inequality; $(b)$ leverage on the Lipschitzness of $\\nabla g,\\,g^{c}$ and $\\nabla g^{c}$ in $x$ , and the upper bound for $\\|\\mu_{g}^{*}(x)\\|$ ; and $(c)$ uses the Lipschitzness of $y_{g}^{*}(x)$ and $\\mu_{g}^{*}(x)$ from Lemma 8. As the bound is loose due to the use of triangle inequality, we can conclude that $v(x)$ is $l_{v,1}$ -smooth where $l_{v,1}\\leq((1+B_{g})(1+L_{g})l_{g^{c},1}+l_{g^{c},0}L_{g})$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "C.2 Proof of Lemma 3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "By assumption, $f(x,y)$ is $l_{f,1}$ -smooth and $g(x,y)$ is $\\alpha_{g}$ -strongly convex in $y$ . We know $f(x,y)+$ $\\gamma(g(x,y)-v(x))$ is $(\\gamma\\alpha_{g}-l_{f,1})$ -strongly convex when $\\begin{array}{r}{\\gamma>\\frac{l_{f,1}}{\\alpha_{g}}}\\end{array}$ l\u03b1f,1 as discussed in (30). Moreover, constraint $g^{c}(x,y)$ is convex in $y$ by Assumption 2. In this way, the problem ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{y\\in\\mathcal{Y}}\\;f(x,y)+\\gamma(g(x,y)-v(x))\\qquad{\\mathrm{s.t.}}\\;g^{c}(x,y)\\leq0\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "features strong convexity according to Chapter 4 in [59] and it equals to ", "page_idx": 18}, {"type": "equation", "text": "$$\nF_{\\gamma}(x)=\\operatorname*{max}_{\\mu\\in\\mathbb{R}_{+}^{d_{c}}}\\operatorname*{min}_{y\\in\\mathcal{Y}}f(x,y)+\\gamma(g(x,y)-v(x))+\\langle\\mu,g^{c}(x,y)\\rangle.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Considering the smoothness of $v(x)$ as presented in Lemma 2, all assumptions in Lemma 9 are satisfied. Therefore the derivative (9) can be obtained. In addition, for any $x_{1},x_{2}\\in\\mathcal{X}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|\\nabla F(x_{1})-\\nabla F(x_{2})\\|}\\\\ &{=\\|\\nabla_{x}f(x_{1},y_{F}^{*}(x_{1}))+\\gamma\\left(\\nabla_{x}g(x_{1},y_{F}^{*}(x_{1}))-\\nabla v(x_{1})\\right)+\\langle\\mu_{F}^{*}(x_{1}),\\nabla_{x}g^{c}(x_{1},y_{F}^{*}(x_{1}))\\rangle}\\\\ &{\\quad-\\nabla_{x}f(x_{2},y_{F}^{*}(x_{2}))-\\gamma\\left(\\nabla_{x}g(x_{2},y_{F}^{*}(x_{2}))-\\nabla v(x_{2})\\right)-\\langle\\mu_{F}^{*}(x_{2}),\\nabla_{x}g^{c}(x_{2},y_{F}^{*}(x_{2}))\\rangle\\|}\\\\ &{\\stackrel{(a)}{\\leq}\\|\\nabla_{x}f(x_{1},y_{F}^{*}(x_{1}))-\\nabla_{x}f(x_{2},y_{F}^{*}(x_{2}))\\|+\\gamma\\|\\nabla_{x}g(x_{1},y_{F}^{*}(x_{1}))-\\nabla_{x}g(x_{2},y_{F}^{*}(x_{2}))\\|}\\\\ &{\\quad+\\gamma\\|\\nabla v(x_{1})-\\nabla v(x_{2})\\|+\\|\\langle\\mu_{F}^{*}(x_{1}),\\nabla_{x}g^{c}(x_{1},y_{F}^{*}(x_{1}))\\rangle-\\langle\\mu_{F}^{*}(x_{1}),\\nabla_{x}g^{c}(x_{2},y_{F}^{*}(x_{2}))\\rangle\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+\\left\\|\\langle\\mu_{F}^{*}(x_{1}),\\nabla_{x}g^{c}(x_{2},y_{F}^{*}(x_{2}))\\rangle-\\langle\\mu_{F}^{*}(x_{2}),\\nabla_{x}g^{c}(x_{2},y_{F}^{*}(x_{2}))\\rangle\\right\\|}\\\\ &{\\overset{(b)}{\\leq}(l_{f,1}+\\gamma l_{g,1}+B_{F}l_{g^{c},1})(\\|x_{1}-x_{2}\\|+\\|y_{F}^{*}(x_{1})-y_{F}^{*}(x_{2})\\|)+\\gamma l_{v,1}\\|x_{1}-x_{2}\\|}\\\\ &{\\quad+l_{g^{c},0}\\|\\mu_{F}^{*}(x_{1})-\\mu_{F}^{*}(x_{2})\\|}\\\\ &{\\overset{(c)}{\\leq}((l_{f,1}+\\gamma l_{g,1}+B_{F}l_{g^{c},1})(1+L_{F})+\\gamma l_{v,1}+l_{f^{c},0}L_{F})\\|x_{1}-x_{2}\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $(a)$ follows triangle inequality; $(b)$ leverage on the Lipschitzness of $\\nabla f,\\nabla g,g^{c}$ and $\\nabla g^{c}$ in $x$ , and the upper bound for $\\|\\mu_{F}^{*}(\\bar{x})\\|$ ; and $(c)$ uses the Lipschitzness of $y_{F}^{*}(x)$ and $\\mu_{F}^{*}(x)$ from Lemma 8. As the bound is loose due to the use of triangle equality, we can conclude that $F(x)$ is $l_{F,1}$ -smooth where $l_{F,1}\\leq(l_{f,1}+\\gamma l_{g,1}+B_{F}l_{g^{c},1})(1+L_{F}^{'})+\\bar{\\gamma}l_{v,1}^{'}+l_{f^{c},0}L_{F}.$ ", "page_idx": 19}, {"type": "text", "text": "D Convergence Analysis of the Main Result ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "D.1 Proof of Theorem 2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Define the bias term $b(x_{t})$ of the gradient $\\nabla F_{\\gamma}(x_{t})$ as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle x_{t}\\rangle:=\\!\\nabla F_{\\gamma}(x_{t})-g F,t}\\\\ &{\\quad=\\!\\Bigg(\\nabla_{x}f(x_{t},y_{F}^{*}(x_{t}))+\\gamma\\big(\\nabla_{x}g(x,y_{F}^{*}(x_{t}))-\\big(\\nabla_{x}g(x_{t},y_{g}^{*}(x_{t}))+\\big\\langle\\mu_{g}^{*}(x_{t}),\\nabla_{x}g^{c}(x_{t},y_{g}^{*}(x_{t}))\\big\\rangle\\big)}\\\\ &{\\quad\\quad\\quad+\\,\\langle\\mu_{F}^{*}(x_{t}),\\nabla_{x}g^{c}(x_{t},y_{F}^{*}(x_{t}))\\rangle\\Bigg)}\\\\ &{\\quad\\quad\\quad-\\Bigg(\\nabla_{x}f(x_{t},y_{F,t}^{T_{F}})+\\gamma\\Big(\\nabla_{x}g(x_{t},y_{F,t}^{T_{F}})-\\Big(\\nabla_{x}g(x_{t},y_{g,t}^{T_{g}})+\\Big\\langle\\mu_{g,t}^{T_{g}},\\nabla_{x}g^{c}(x_{t},y_{g,t}^{T_{g}})\\Big\\rangle\\Big)\\Bigg)}\\\\ &{\\quad\\quad\\quad+\\,\\langle\\mu_{F}^{T_{F}},\\nabla_{x}g^{c}(x_{t},y_{F,t}^{T_{F}})\\rangle\\Bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In this way, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|\\{\\boldsymbol{\\Lambda}(\\tau)\\}\\prod_{i=1}^{6}\\biggl(\\prod_{j=1}^{6}\\biggl(z_{i}\\eta_{j}\\biggr)-\\nabla_{j}\\{(x_{i},y_{j}^{*}(x_{i}))\\}\\biggr)}\\qquad}&{}\\\\ &{+\\gamma\\left(\\left\\|\\nabla_{j}g(x_{i},y_{j}^{*})-\\nabla_{j}g(x_{i},y_{j}^{*}(x_{i}))\\right\\|+\\left\\|\\nabla_{z}g(x_{i},y_{i}^{*})-\\nabla_{\\sigma}g(x_{i},y_{j}^{*}(x_{i}))\\right\\|\\right.}\\\\ &{\\qquad\\qquad+\\left\\|\\left\\langle\\hat{\\sigma}_{j}g(x_{i},y_{i}^{*})\\nabla_{j}g(x_{i},y_{i}^{*}(x_{i}))\\right\\rangle-\\left\\langle\\hat{\\sigma}_{j}^{*}(x_{i}^{*})\\nabla_{\\sigma}g^{\\prime}(x_{i},y_{j}^{*})\\right\\|}\\right.}\\\\ &{\\qquad\\qquad\\left.+\\left\\|\\left\\langle\\hat{\\sigma}_{j}^{*}(x_{i})\\nabla_{j}g^{\\prime}(x_{i},y_{i}^{*}(x_{i}))-\\hat{\\sigma}_{j}^{*}(x_{i}^{*},y_{i}^{*}(x_{i},y_{j}^{*}))\\right\\|\\right)}\\\\ &{+\\left\\|\\left\\langle\\hat{\\sigma}_{j}^{*}(x_{i})\\nabla_{j}g(x_{i},y_{i}^{*})-\\hat{\\sigma}_{j}^{*}(x_{i}^{*})\\nabla_{j}g(x_{i},y_{i}^{*}(x_{i}))\\right\\|\\right.}\\\\ &{\\qquad\\qquad+\\left.\\|\\hat{\\sigma}_{j}^{*}(x_{i})\\nabla_{i}g(x_{i},y_{j}^{*})-\\hat{\\sigma}_{j}^{*}(x_{i}^{*})\\nabla_{i}g(x_{i},y_{i}^{*}(x_{i}))\\right\\|}\\\\ &{+\\left\\|\\hat{\\sigma}_{j}^{*}(x_{i})\\nabla_{i}g(x_{i},y_{j}^{*})-\\hat{\\sigma}_{j}^{*}(x_{i},y_{i}^{*})\\right\\|-\\left\\langle\\hat{\\sigma}_{j}^{*}(x_{i},y_{\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $(a)$ uses triangle inequality, $(b)$ relies on the Lipschitzness of $\\nabla f,\\,\\nabla g,\\,g^{c}$ , and $\\nabla g^{c}$ in $x$ , the upper bounds for $\\Vert\\mu_{F}^{*}(x)\\Vert$ and $\\|\\mu_{g}^{*}(x)\\|$ , and Cauchy-Schwartz inequality, and $(c)$ is by rearrangement. ", "page_idx": 19}, {"type": "text", "text": "Furthermore, according to Young\u2019s inequality, it follows that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|b(x_{t})\\|^{2}\\leq2\\left((l_{f,1}+\\gamma l_{g,1}+B_{F}l_{g^{c},0})\\|y_{F,t}^{T_{F}}-y_{F,t}^{*}\\|+l_{g^{c},0}\\|\\mu_{F,t}^{T_{F}}-\\mu_{F,t}^{*}\\|\\right)^{2}}\\\\ &{\\qquad\\qquad+\\,2\\gamma^{2}\\left((l_{g,1}+B_{g}l_{g^{c},1})\\|y_{g,t}^{T_{g}}-y_{g}^{*}(x_{t})\\|+l_{g^{c},0}\\|\\mu_{g,t}^{T_{g}}-\\mu_{g}^{*}(x_{t})\\|\\right)^{2}}\\\\ &{\\qquad\\qquad=\\!\\mathcal{O}(\\gamma^{2}\\epsilon_{F}+\\gamma^{2}\\epsilon_{g}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "According to Lemma 3, $F_{\\gamma}(x)$ is $l_{F,1}$ -smooth in $\\mathcal{X}$ . In this way, by the smoothness, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{F(x_{t+1})\\le F(x_{t})+\\langle\\nabla F(x_{t}),x_{t+1}-x_{t}\\rangle+\\displaystyle\\frac{l_{F,1}}{2}\\|x_{t+1}-x_{t}\\|^{2}}\\\\ {\\le F(x_{t})+\\langle g_{F,t},x_{t+1}-x_{t}\\rangle+\\displaystyle\\frac{1}{2\\eta}\\|x_{t+1}-x_{t}\\|^{2}+\\langle b(x_{t}),x_{t+1}-x_{t}\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the second inequality is by $\\begin{array}{r}{\\eta\\leq\\frac{1}{l_{F,1}}}\\end{array}$ and $\\nabla F(x_{t})=g_{F_{t}}+b(x_{t})$ ", "page_idx": 20}, {"type": "text", "text": "The projection guarantees that $x_{t+1}$ and $x_{t}$ are in $\\mathcal{X}$ . Following Lemma 6, we know that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\langle g_{F,t},x_{t+1}-x_{t}\\rangle\\leq-\\frac{1}{\\eta}\\|x_{t+1}-x_{t}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Plugging this back to (34), it follows ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal F}(x_{t+1})\\leq}{\\cal F}(x_{t})-\\frac{1}{2\\eta}\\|x_{t+1}-x_{t}\\|^{2}+\\langle b(x_{t}),x_{t+1}-x_{t}\\rangle}}\\\\ {~~}\\\\ {{\\displaystyle~\\leq{\\cal F}(x_{t})-\\frac{1}{2\\eta}\\|x_{t+1}-x_{t}\\|^{2}+\\eta\\|b(x_{t})\\|^{2}+\\frac{1}{4\\eta}\\|x_{t+1}-x_{t}\\|^{2}}}\\\\ {{\\displaystyle~={\\cal F}(x_{t})-\\frac{1}{4\\eta}\\|x_{t+1}-x_{t}\\|^{2}+\\eta\\|b(x_{t})\\|^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the second inequality is from Young\u2019s inequality. Telescoping therefore gives ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{1}{T}\\sum_{t=0}^{T-1}\\|G_{\\eta}(x_{t})\\|^{2}\\le\\!\\frac{4}{\\eta T}(F(x_{0})-F(x_{T}))+\\frac{4}{T}\\sum_{t=0}^{T-1}\\|b(x_{t})\\|^{2}}}\\\\ &{=\\!\\mathcal{O}(\\eta^{-1}T^{-1})+{O}(\\gamma^{2}\\epsilon_{F}+\\gamma^{2}\\epsilon_{g})}\\\\ &{=\\!\\mathcal{O}(\\gamma T^{-1}+\\gamma^{2}\\epsilon_{F}+\\gamma^{2}\\epsilon_{g})}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where last equality comes from $\\eta=\\mathcal{O}(\\gamma^{-1})$ as $\\begin{array}{r}{\\eta\\leq\\frac{1}{l_{F,1}}}\\end{array}$ and $l_{F,1}\\leq(l_{f,1}+\\gamma l_{g,1}+B_{F}l_{g^{c},1})(1+$ $L_{F})+\\gamma l_{v,1}+l_{f^{c},0}L_{F}=\\mathcal{O}(\\gamma)$ . This completes the proof. ", "page_idx": 20}, {"type": "text", "text": "D.2 Proof of Theorem 3 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "To restate, we are viewing $L_{g}(\\mu,y;x)$ in (7) and $L_{F}(\\mu,y;x)$ in (5) respectively as $L(\\mu,y)$ and considering the following max-min problem ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mu\\in\\mathbb{R}_{+}^{d_{c}}}\\operatorname*{min}_{y\\in\\mathcal{Y}}L(\\mu,y).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In (16), we defined the minimization part as ", "page_idx": 20}, {"type": "equation", "text": "$$\nD(\\mu):=\\operatorname*{min}_{y\\in\\mathcal{Y}}L(\\mu,y)=L(\\mu,y_{\\mu}^{*}(\\mu))\\quad\\mathrm{where}\\quad y_{\\mu}^{*}(\\mu):=\\arg\\operatorname*{min}_{y\\in\\mathcal{Y}}L(\\mu,y).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "To evaluate $D(\\mu)$ for $L_{g}(\\mu,y;x)$ in (7) and $L_{F}(\\mu,y;x)$ in (5) as $L(\\mu,y)$ , we define the following mappings for fixed $x\\in\\mathscr{X}$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{y_{\\mu,g}^{*}(\\mu):=\\arg\\underset{y\\in\\mathcal{Y}}{\\operatorname*{min}}\\,L_{g}(\\mu,y;x),}\\\\ &{y_{\\mu,F}^{*}(\\mu):=\\arg\\underset{y\\in\\mathcal{Y}}{\\operatorname*{min}}\\,L_{F}(\\mu,y;x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In this way, for $L_{g}(\\mu,y;x)$ in (7) and $L_{F}(\\mu,y;x)$ in (5) respectively as $L(\\mu,y),D(\\mu)$ defined in (16) equals to $D_{g}(\\mu)$ and $\\dot{D}_{F}(\\mu)$ respectively, where ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal D}_{g}(\\mu)=\\operatorname*{min}_{y\\in\\mathcal{Y}}L_{g}(\\mu,y;x)=L_{g}(\\mu,y_{\\mu,g}^{\\ast}(\\mu);x),}}\\\\ {{\\displaystyle{\\cal D}_{F}(\\mu)=\\operatorname*{min}_{y\\in\\mathcal{Y}}L_{F}(\\mu,y;x)=L_{F}(\\mu,y_{\\mu,F}^{\\ast}(\\mu);x).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In the following lemma, we show that $D(\\mu)$ exhibits concavity and smoothness, which are favorable properties for conducting gradient-based algorithm. ", "page_idx": 21}, {"type": "text", "text": "Lemma 10 (Smoothness and Concavity of $D(\\mu)$ ). Suppose all the assumptions in Theorem $^3$ hold. For fixed $x\\in\\mathscr{X}$ , the following holds ", "page_idx": 21}, {"type": "text", "text": "2. $D_{g}(\\mu)$ in (37) is concave an d lg\u03b1c,0 -smooth, and DF (\u00b5) in (38) is concave and $\\frac{l_{g}c_{,0}}{\\gamma\\alpha_{g}\\!-\\!l_{f,1}}$ smooth. ", "page_idx": 21}, {"type": "text", "text": "Proof. To restate, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{g}(\\mu,y;x)=\\!g(x,y)+\\langle\\mu,g^{c}(x,y)\\rangle,}\\\\ &{L_{F}(\\mu,y;x)=\\!f(x,y)+\\gamma(g(x,y)-v(x))+\\langle\\mu,g^{c}(x,y)\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Under Assumption 1 and 2, for fixed $x$ and given $\\mu$ , $L_{g}(\\mu,y;x)$ is $\\alpha_{g}$ -strongly convex and $(l_{g,1}+$ $\\|\\mu\\|l_{g^{c},1})$ -smooth in $y$ , and $L_{F}(\\mu,y;x)$ is $(\\gamma\\alpha_{g}-l_{f,1})$ -strongly convex and $(l_{f,1}+\\gamma l_{g,1}+\\Vert\\mu\\Vert\\bar{l}_{g^{c},1}^{'}).$ - smooth in $y$ . Therefore, we know $y_{\\mu,g}^{*}(\\mu)$ in (35) and $y_{\\mu,F}^{*}(\\mu)$ in (36) are respectively $\\textstyle{\\frac{1}{\\alpha_{g}}}$ and $\\frac{1}{\\gamma\\alpha_{g}\\!-\\!l_{f,1}}$ -Lipschitz to $\\mu$ by directly quoting Theorem F.10 in [17] or Theorem 4.47 in [32]. This proves the first part of the Lemma. ", "page_idx": 21}, {"type": "text", "text": "For the second part, the concavity of $D_{g}(\\mu)$ and $D_{F}(\\mu)$ can be directly obtained by Lemma 2.58 in [59] as $L_{g}(\\mu,y;x)$ and $L_{g}(\\mu,y;x)$ are both convex in $y$ . ", "page_idx": 21}, {"type": "text", "text": "Moreover, following Theorem 4.24 in [7], we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla D_{g}(\\mu)=\\nabla_{\\mu}L_{g}(\\mu,y_{\\mu,g}^{*}(\\mu))=g^{c}(x,y_{\\mu,g}^{*}(\\mu)),}\\\\ &{\\nabla D_{F}(\\mu)=\\nabla_{\\mu}L_{F}(\\mu,y_{\\mu,F}^{*}(\\mu))=g^{c}(x,y_{\\mu,F}^{*}(\\mu)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "As $g^{c}(x,y)$ is $l_{g^{c},0}$ -Lipschitz by Assumption 1, for any $\\mu_{1},\\mu_{2}\\in\\mathbb{R}_{+}^{d_{c}}$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla D_{g}(\\mu_{1})-\\nabla D_{g}(\\mu_{2})\\|=\\|g^{c}(x,y_{\\mu,g}^{*}(\\mu_{1}))-g^{c}(x,y_{\\mu,g}^{*}(\\mu_{2}))\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\boldsymbol{l}_{g^{c},0}\\|y_{\\mu,g}^{*}(\\mu_{1})-y_{\\mu,g}^{*}(\\mu_{2})\\|\\leq\\displaystyle\\frac{l_{g^{c},0}}{\\alpha_{g}}\\|\\mu_{1}-\\mu_{2}\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and similarly, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla D_{F}(\\mu_{1})-\\nabla D_{F}(\\mu_{2})\\|=\\|g^{c}(x,y_{\\mu,F}^{*}(\\mu_{1}))-g^{c}(x,y_{\\mu,F}^{*}(\\mu_{2}))\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\boldsymbol{l}_{g^{c},0}\\|y_{\\mu,F}^{*}(\\mu_{1})-y_{\\mu,F}^{*}(\\mu_{2})\\|\\leq\\displaystyle\\frac{\\boldsymbol{l}_{g^{c},0}}{\\gamma\\alpha_{g}-\\boldsymbol{l}_{f,1}}\\|\\mu_{1}-\\mu_{2}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We can conclude that $D_{g}(\\mu)$ and $D_{F}(\\mu)$ are respectively $\\frac{l_{g^{c},0}}{\\alpha_{g}}$ and $\\frac{l_{g^{c},0}}{\\gamma\\alpha_{g}\\!-\\!l_{f,1}}$ -smooth. ", "page_idx": 21}, {"type": "text", "text": "In Algorithm 2, we are implementing an accelerated projected gradient descent on $-D(\\mu)$ where the gradient bias is controlled by $T_{y}$ . Following [16], the following lemma presents the convergence analysis of the accelerated method on smooth and convex functions. ", "page_idx": 21}, {"type": "text", "text": "Lemma 11 (Section 5 and 6 in [16]). Suppose $D(\\mu)$ is concave and $l_{D,1}$ -smooth. Consider the constrained problem $\\operatorname*{max}_{\\mu\\in\\mathbb{R}_{+}^{d_{c}}}D(\\mu)$ . At iteration $t=0,\\dots,T-1$ , perform accelerated projected gradient update with stepsize $\\begin{array}{r}{\\eta\\leq\\frac{1}{l_{D,1}}}\\end{array}$ and initial value $\\mu_{0}=\\mu_{-1}$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mu_{t+\\frac{1}{2}}=\\mu_{t}+\\frac{t-1}{t+2}(\\mu_{t}-\\mu_{t-1})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mu_{t+1}=\\mathrm{Proj}_{\\mathbb{R}_{+}^{d_{c}}}(\\mu_{t+\\frac{1}{2}}+\\eta g_{t+\\frac{1}{2}})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $g_{t+\\frac12}$ is an $\\epsilon^{1.5}$ -approximate to $\\nabla D(\\mu_{t+\\frac{1}{2}})$ satisfying $\\begin{array}{r}{\\|g_{t+\\frac{1}{2}}\\ -\\nabla D(\\mu_{t+\\frac{1}{2}})\\|=\\mathcal{O}(\\epsilon^{1.5}),}\\end{array}$ , for $a$ given accuracy $\\epsilon>0$ . Denote $D^{*}=\\operatorname*{max}_{\\mu\\in\\mathbb{R}_{+}^{d_{c}}}D(\\mu)$ , performing $T=\\mathcal{O}(\\epsilon^{-0.5})$ iterations leads to ", "page_idx": 22}, {"type": "equation", "text": "$$\nD^{*}-D(\\mu_{T})=\\mathcal{O}(\\epsilon).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Remark 2. The domain for $\\mu\\in\\mathbb{R}_{+}^{d_{c}}$ can be replaced by any closed, convex, non-empty domain. ", "page_idx": 22}, {"type": "text", "text": "In this way, we are ready to proceed to the proof of Theorem 3. ", "page_idx": 22}, {"type": "text", "text": "proof of Theorem $^3$ . Algorithm 2 solves (7) and (5) by taking $L_{g}(\\mu,y;x)$ and $L_{F}(\\mu,y;x)$ respectively as $L(\\mu,y)$ and run respectively iterations $T$ equals $T_{g}$ and $T_{F}$ ", "page_idx": 22}, {"type": "text", "text": "We begin our proof with analysis on (7). The accelerated version of Algorithm 2 solves this by taking $L_{g}(\\mu,y;x)$ in (7) with fixed $x\\in\\mathscr{X}$ as $L(\\mu,y)$ . ", "page_idx": 22}, {"type": "text", "text": "Fixing $\\mu_{t+\\frac{1}{2}}$ , steps 4-6 are $T_{y}$ -step projected gradient descent in $y$ . As $L_{g}(\\mu,y;x)$ is $(l_{g,1}+l_{g^{c},1})$ - smooth and \u03b1g-strongly convex in y, taking the inner loop stepsize \u03b71 as \u03b7g,1 \u2264 lg,1+1lgc,1 ensures linear convergence according to Lemma 7. Choosing $T_{y}=\\mathcal{O}(\\ln((\\epsilon^{1.5})^{-1}))=\\mathcal{O}(\\ln(\\epsilon_{g}^{-1}))$ leads to ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\lVert y_{t+1}-y_{\\mu,g}^{*}(\\mu_{t+\\frac{1}{2}})\\rVert=\\mathcal{O}(\\epsilon_{g}^{1.5})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for target accuracy $\\epsilon_{g}>0$ where $y_{\\mu,g}^{*}(\\mu)$ is defined in (35). ", "page_idx": 22}, {"type": "text", "text": "In step 7, we update $\\mu$ with a projected gradient descent step which takes $\\nabla_{\\mu}L(\\mu_{t+\\frac{1}{2}},y_{t+1})\\,=$ $g^{c}(x,y_{t+1})$ as an estimate of $\\nabla D_{g}(\\mu_{t+\\frac{1}{2}})=g^{c}(x,y_{\\mu,g}^{*})$ . The estimation bias is bounded by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\nabla_{\\mu}L(\\mu_{t+\\frac{1}{2}},y_{t+1})-\\nabla D_{g}(\\mu_{t+\\frac{1}{2}})\\|=\\|g^{c}(x,y_{t+1})-g^{c}(x,y_{\\mu,g}^{*}(\\mu_{t+\\frac{1}{2}}))\\|}&{}\\\\ {\\leq\\!l_{g^{c},0}\\|y_{t+1}-y_{\\mu,g}^{*}(\\mu_{t+\\frac{1}{2}})\\|=\\mathcal{O}(\\epsilon_{g}^{1.5}).}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By Lemma 11, we can conclude the complexity is $\\tilde{\\mathcal{O}}(\\epsilon_{g}^{-0.5})$ for conducting the accelerated version of Algorithm 2 on $L_{g}(\\mu,y;x)$ in (7) as $L(\\mu,y)$ to achieve ", "page_idx": 22}, {"type": "equation", "text": "$$\nD_{g}(\\mu_{g}^{*}(x))-D_{g}(\\mu_{T_{g}})=\\mathcal{O}(\\epsilon_{g}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Similarly, the complexity of the accelerated version of Algorithm 2 for (5) is $\\tilde{\\mathcal{O}}(\\epsilon_{F}^{-0.5})$ , i.e., ", "page_idx": 22}, {"type": "equation", "text": "$$\nD_{F}(\\mu_{F}^{*}(x))-D_{F}(\\mu_{T_{F}})=\\mathcal{O}(\\epsilon_{F}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In the following, we are going to show that (42) and (43) and respectively sufficient to bound $\\|\\mu_{T_{g}}-\\mu_{g}^{*}(x)\\|$ and $\\|\\mu_{T_{F}}-\\mu_{F}^{*}(x)\\|$ considering Assumption 5 is satisfied. ", "page_idx": 22}, {"type": "text", "text": "As $D_{g}(\\mu)$ and $D_{F}(\\mu)$ are both concave in $\\mu$ and $\\mu\\in\\mathbb{R}_{+}^{d_{c}}$ is equivalent to $\\mu\\geq0$ , the problems ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mu\\in\\mathbb{R}_{+}^{d_{c}}}D_{g}(\\mu)\\quad\\mathrm{and}\\quad\\operatorname*{max}_{\\mu\\in\\mathbb{R}_{+}^{d_{c}}}D_{F}(\\mu)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "are respectively equivalent to the unconstrained problems ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mu\\in\\mathbb{R}^{d_{c}}}\\tilde{D}_{g}(\\mu):=D_{g}(\\mu)+\\lambda_{g}^{\\top}\\mu\\quad\\mathrm{and}\\quad\\operatorname*{max}_{\\mu\\in\\mathbb{R}^{d_{c}}}\\tilde{D}_{F}(\\mu):=D_{F}(\\mu)+\\lambda_{F}^{\\top}\\mu\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "with the Lagrange multipliers $\\lambda_{g},\\lambda_{F}$ being non-negative and finite in all dimension, i.e. $0\\le\\lambda_{g}<\\infty$ , $0\\leq\\lambda_{F}<\\infty$ according to Lagrange duality Theorem. Moreover, it is well known (Chapter 4 in [59]) that the Largrangian terms respectively equal zero when the problems attain the optimals. i.e. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\lambda_{g}^{\\top}\\mu_{g}^{*}(x)=0\\quad\\mathrm{and}\\quad\\lambda_{F}^{\\top}\\mu_{F}^{*}(x)=0.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Moreoever, the first-order stationary condition requires $\\nabla\\tilde{D}_{g}(\\mu_{g}^{*}(x))=\\nabla D_{g}(\\mu_{g}^{*}(x))+\\lambda_{g}=0$ and $\\nabla\\tilde{D}_{F}(\\mu_{F}^{*}(x))=\\nabla D_{F}(\\mu_{F}^{*}(x))+\\lambda_{F}=0$ and therefore ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\nabla D_{g}(\\mu_{g}^{*}(x))=-\\lambda_{g}\\quad\\mathrm{and}\\quad\\nabla D_{F}(\\mu_{F}^{*}(x))=-\\lambda_{F}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In this way, for all $\\mu\\in\\mathcal{B}(\\mu_{g}^{*}(x);\\delta_{g})\\cap\\mathbb{R}_{+}^{d_{c}}$ . ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{g}(\\mu_{g}^{*}(x))-D_{g}(\\mu)=\\int_{\\tau=0}^{1}\\langle\\nabla D_{g}(\\mu+\\tau(\\mu_{g}^{*}(x)-\\mu)),\\mu_{g}^{*}(x)-\\mu\\rangle d\\tau}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\int_{\\tau=0}^{1}\\frac{1}{\\tau}\\langle\\nabla D_{g}(\\mu_{g}^{*}(x))-D_{g}(\\mu+\\tau(\\mu_{g}^{*}(x)-\\mu)),\\tau(\\mu-\\mu_{g}^{*}(x))\\rangle d\\tau}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad-\\langle\\nabla D_{g}(\\mu_{g}^{*}(x)),\\mu-\\mu_{g}^{*}(x)\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\stackrel{(a)}{\\geq}\\int_{0}^{1}C_{\\delta_{g}}\\Vert\\mu-\\mu_{g}^{*}(x)\\Vert^{2}\\tau d\\tau-\\langle\\nabla D_{g}(\\mu_{g}^{*}(x)),\\mu-\\mu_{g}^{*}(x)\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\frac{(b)C_{\\delta_{g}}}{2}\\Vert\\mu-\\mu_{g}^{*}(x)\\Vert^{2}+\\langle\\lambda_{g},\\mu-\\mu_{g}^{*}(x)\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\stackrel{(c)C_{\\delta_{g}}}{\\geq}\\Vert\\mu-\\mu_{g}^{*}(x)\\Vert^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $(a)$ uses (21a) in Assumption 5 and the fact that the $\\mu,\\mu_{g}^{*}(x)\\in\\mathcal{B}(\\mu_{g}^{*}(x);\\delta_{g})\\cap\\mathbb{R}_{+}^{d_{c}}$ implies $\\mu+\\tau(\\mu_{g}^{\\ast}(x)-\\mu)\\in\\mathcal{B}(\\mu_{g}^{\\ast}(x);\\delta_{g})\\cap\\mathbb{R}_{+}^{d_{c}}$ ; $(b)$ solves the integral and uses $\\lambda_{g}=-\\nabla D_{g}(\\mu_{g}^{*}(x))$ in (45); and $(c)$ follows from the fact that $\\langle\\lambda,\\mu_{g}^{*}(x)\\rangle=0$ in (44) and $\\mu,\\lambda_{g}\\geq0$ . ", "page_idx": 23}, {"type": "text", "text": "Analogously, for all $\\mu\\in\\mathcal{B}(\\mu_{F}^{*}(x);\\delta_{F})\\cap\\mathbb{R}_{+}^{d_{c}}$ , it follows that ", "page_idx": 23}, {"type": "equation", "text": "$$\nD_{F}(\\mu_{F}^{*}(x))-D_{F}(\\mu)\\geq\\frac{C_{\\delta_{F}}}{2}\\|\\mu-\\mu_{F}^{*}(x)\\|^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In this way, for arbitrary $\\begin{array}{r}{\\epsilon_{g}<\\frac{C\\delta_{g}}{2}\\delta_{g}}\\end{array}$ , the complexity of Algorithm 2 to solve (7) is $\\tilde{\\mathcal{O}}(\\epsilon_{F}^{-0.5})$ , i.e., ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|\\mu_{T_{g}}-\\mu_{g}^{*}(x)\\|^{2}={\\mathcal O}(\\epsilon_{g}),}\\\\ &{\\mathrm{and}\\;\\|y_{T_{g}}-y_{g}^{*}(x)\\|^{2}\\leq\\|y_{T_{g}}-y_{g}^{*}(\\mu_{T_{g}};x)\\|^{2}+\\|\\mu_{T_{g}}-\\mu_{g}^{*}(x)\\|^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\leq(1/\\alpha_{g}+1)\\|\\mu_{T_{g}}-\\mu_{g}^{*}(x)\\|^{2}={\\mathcal O}(\\epsilon_{g}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "At each iteration $t$ in Algorithm 1, $x=x_{t}$ , and the output $(y_{T_{g}},\\mu_{T_{g}})$ is chosen as $(y_{g,t}^{T_{g}},\\mu_{g,t}^{T_{g}})$ . ", "page_idx": 23}, {"type": "text", "text": "aScihmiielvarel (y,4 t3o)  scoalnv ea c(h5i)e, vfeor arbitrary $\\begin{array}{r}{\\epsilon_{g}<\\frac{C\\delta_{g}}{2}\\delta_{g}}\\end{array}$ , applying Algorithm 2 with complexity $\\tilde{\\mathcal{O}}(\\epsilon_{F}^{-0.5})$ to ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|\\mu_{T_{F}}-\\mu_{F}^{*}(x)\\|^{2}=\\mathcal{O}(\\epsilon_{F}),}\\\\ &{\\mathrm{~nd~}\\|y_{T_{F}}-y_{F}^{*}(x)\\|^{2}\\leq\\|y_{T_{F}}-y_{F}^{*}(\\mu_{T_{F}};x)\\|^{2}+\\|\\mu_{T_{F}}-\\mu_{F}^{*}(x)\\|^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\leq(1/\\alpha_{F}+1)\\|\\mu_{T_{F}}-\\mu_{F}^{*}(x)\\|^{2}=\\mathcal{O}(\\epsilon_{F}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "At each iteration $t$ in Algorithm 1, $x=x_{t}$ , and the output $\\left(y_{T_{F}},\\mu_{T_{F}}\\right)$ is chosen as $(y_{F,t}^{T_{F}},\\mu_{F,t}^{T_{F}})$ . ", "page_idx": 23}, {"type": "text", "text": "This completes the proof. ", "page_idx": 23}, {"type": "text", "text": "Remark 3. Under the same assumptions as in Theorem 3, we cam choose $\\eta_{g,1}\\leq(l_{g,1}+l_{g^{c},1})^{-1}$ , $\\begin{array}{r l r}{\\eta_{g,2}\\!\\!}&{{}\\le}&{\\!\\!\\frac{\\alpha_{g}}{l_{g^{c},0}}}\\end{array}$ as stepsizes for running the accelerated version of Algorithm 2 to solve (7), and $\\eta_{F,1}\\leq(l_{f,1}+\\gamma l_{g,1}+l_{g^{c},1})^{-1}$ , $\\begin{array}{r}{\\eta_{F,2}\\le\\frac{\\gamma\\alpha_{g}-l_{f,1}}{l_{g^{c},0}}}\\end{array}$ as the ones for (5). ", "page_idx": 23}, {"type": "text", "text": "D.3 Proof of Theorem 4 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we consider ", "page_idx": 23}, {"type": "equation", "text": "$$\ng^{c}(x,y)=g_{1}^{c}(x)^{\\top}y-g_{2}^{c}(x)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "being affine in $y$ , and $\\mathcal{V}=\\mathbb{R}^{d_{\\boldsymbol{y}}}$ . ", "page_idx": 23}, {"type": "text", "text": "Therefore, for a fixed $x$ , taking either $L_{g}(\\mu,y;x)$ in (7) or $L_{F}(\\mu,y;x)$ in (5) as $L(\\mu,y)$ fits into a special case of strongly-convex-concave saddle point problems in the following form: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mu\\in\\mathbb{R}_{+}^{d_{c}}}\\operatorname*{min}_{y\\in\\mathbb{R}^{d_{y}}}L(\\mu,y)=-h_{1}(\\mu)+y^{\\top}A\\mu+h_{2}(y)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $h_{1}(\\mu)$ is smooth and linear (concave) in $\\mu$ and $h_{2}(y)$ is smooth and strongly convex in $y$ . Specifically, for $L_{g}(\\mu,y;x)$ as $L(\\mu,y)$ , $h_{1}(\\mu)\\,=\\,\\langle g_{2}^{c}(x),\\mu\\rangle$ is 0-smooth and linear (concave), $A=g_{1}^{c}(x)$ , and $h_{2}(y)=g(x,y)$ is $l_{g,1}$ -smooth and $\\alpha_{g}$ -strongly convex. ", "page_idx": 24}, {"type": "text", "text": "In this context, performing PGD on $L(\\mu,y)$ on $y$ is equivalent to a gradient descent step since $\\mathcal{V}=\\mathbb{R}^{d_{y}}$ . The following lemma summarizes the error of $\\|y_{t}-y_{\\mu}^{*}(\\mu_{t})\\|$ where $y_{\\mu}^{*}(\\mu)$ is defined in (16) and the update of $\\mu_{t+1}$ and $y_{t+1}$ is the non-accelerated version of Algorithm 2 with $T_{y}=1$ . ", "page_idx": 24}, {"type": "text", "text": "Lemma 12 (Update of $\\|y_{t}-\\nabla h_{2}^{*}(-A\\mu_{t})\\|)$ . Consider the problem (47) where $A$ is of full column rank and $h_{2}(y)$ is $\\alpha_{h_{2}}$ -strongly convex and $l_{f,1}$ -smooth. $y_{\\mu}^{*}(\\mu)$ defined in (16) satisfies ", "page_idx": 24}, {"type": "equation", "text": "$$\ny_{\\mu}^{*}(\\mu)=\\nabla h_{2}^{*}(-A\\mu)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $h_{2}^{*}(y)$ is the conjugate function of $h_{2}(y)$ by Definition 5. At iteration $t_{\\mathrm{:}}$ , $\\mu_{t},y_{t}$ are known, and conduct $\\boldsymbol{y}_{t+1}=\\boldsymbol{y}_{t}-\\eta_{1}\\nabla_{y}L(\\mu_{t},y_{t})$ , a gradient descent step for $L(\\mu_{t},y)$ in $y$ . This gives ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|y_{t+1}-\\nabla h_{2}^{*}(-A\\mu_{t})\\|\\leq(1-\\eta_{1}\\alpha_{h_{2}}/2)\\|y_{t}-\\nabla h_{2}^{*}(-A\\mu_{t})\\|\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "when \u03b71 \u2264lh1,1 . Additionally, given another $\\mu_{t+1}$ , we know ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|y_{t+1}-\\nabla h_{2}^{*}(-A\\mu_{t+1})\\|\\leq(1-\\eta_{1}\\alpha_{h_{2}}/2)\\|y_{t}-\\nabla h_{2}^{*}(-A\\mu_{t})\\|+\\frac{\\sigma_{\\operatorname*{max}}(A)}{\\alpha_{h_{2}}}\\|\\mu_{t+1}-\\mu_{t}\\|.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. Recall the definition $y_{\\mu}^{*}(\\mu)=\\arg\\operatorname*{min}_{y}L(\\mu,y)$ following (16). The first-order stationary optimality condition requires that for any given $\\mu$ , it holds ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\nabla_{y}L(\\mu,y_{\\mu}^{*})=A\\mu+\\nabla h_{2}(y_{\\mu}^{*})=0\\quad\\Leftrightarrow\\quad\\nabla h_{2}(y_{\\mu}^{*})=-A\\mu.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "As the mapping $\\nabla h_{2}$ and $\\nabla h_{2}^{*}$ are the inverse of each other according to Lemma 4, for any $\\mu$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\ny_{\\mu}^{*}=\\nabla h_{2}^{*}(-A\\mu).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "At iteration $t$ , conducting a gradient descent step on $L(\\mu_{t},y)$ gives $y_{t+1}=y_{t}-\\eta_{1}\\nabla_{y}L(\\mu_{t},y_{t})$ . As $L(\\mu_{t},y)$ is $\\alpha_{h_{2}}$ -strongly convex and $l_{h_{2},1}$ -smooth in $y$ , following Lemma 7, take $\\begin{array}{r}{\\eta_{1}\\leq\\frac{1}{l_{h_{2},1}}}\\end{array}$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|y_{t+1}-\\nabla h_{2}^{*}(-A\\mu_{t})\\|\\leq(1-\\eta_{1}\\alpha_{h_{2}}/2)\\|y_{t}-\\nabla h_{2}^{*}(-A\\mu_{t})\\|.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Following triangle inequality, we also have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|y_{t+1}-\\nabla h_{2}^{*}(-A\\mu_{t+1})\\|}\\\\ &{\\leq\\|y_{t+1}-\\nabla h_{2}^{*}(-A\\mu_{t})\\|+\\|\\nabla h_{2}^{*}(-A\\mu_{t})-\\nabla h_{2}^{*}(-A\\mu_{t+1})\\|}\\\\ &{\\leq(1-\\eta_{1}\\alpha_{h_{2}}/2)\\|y_{t}-\\nabla h_{2}^{*}(-A\\mu_{t})\\|+\\frac{\\sigma_{\\operatorname*{max}}(A)}{\\alpha_{h_{2}}}\\|\\mu_{t+1}-\\mu_{t}\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the second term comes from the smoothness of the conjugate function (see Lemma 4). ", "page_idx": 24}, {"type": "text", "text": "In (50), the update behavior of $\\|y_{t}-\\nabla h_{2}^{*}(-A\\mu_{t})\\|$ depends on $\\|\\mu_{t+1}-\\mu_{t}\\|$ . Therefore, we are interested in the the update behavior of $\\|\\mu_{t+1}-\\mu_{t}\\|$ where $\\mu_{t+1}=\\operatorname{Proj}_{\\mathbb{R}^{d_{c}}}(\\mu_{t}+\\eta\\nabla_{\\mu}L(\\mu_{t},y_{t+1}))$ as in the non-accelerated version of Algorithm 2 with $T_{y}=1$ . Before proceeding, we would like to look into the properties of $D(\\mu)$ defined in (16) under the setting (47): ", "page_idx": 24}, {"type": "equation", "text": "$$\nD(\\mu)=\\operatorname*{min}_{y}-h_{1}(\\mu)+\\langle y,A\\mu\\rangle+h_{2}(y)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $h_{1}(\\mu)$ is smooth and linear (concave) in $\\mu,h_{2}(y)$ is smooth and strongly convex in $y$ , and $A$ is of full rank in column. The next lemma shows that $D(\\mu)$ features strong concavity and smoothness. ", "page_idx": 24}, {"type": "text", "text": "Lemma 13 (Smoothness and strongly concavity of $D(\\mu),$ ). Suppose $h_{1}$ is concave and $l_{h_{1},1}$ -smooth, $h_{2}$ is $\\alpha_{h_{2}}$ -strongly convex and $l_{h_{2},1}$ -smooth, and $A$ is full column rank. Then $D(\\mu)$ in (52) satisfies ", "page_idx": 24}, {"type": "equation", "text": "$$\nD(\\mu)=-h_{1}(\\mu)-h_{2}^{*}(-A\\mu),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and is $\\frac{\\sigma_{\\mathrm{min}}^{2}(A)}{l_{h_{2},1}}$ -strongly concave and $\\begin{array}{r}{(l_{h_{1},1}+\\frac{\\sigma_{\\operatorname*{max}}^{2}(A)}{\\alpha_{h_{2}}})}\\end{array}$ \u03c3m\u03b1ax(A))-smooth with respect to \u00b5. ", "page_idx": 24}, {"type": "text", "text": "Proof. Following Definition 5, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\nD(\\mu)=-h_{1}(\\mu)-h_{2}^{*}(-A\\mu)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $h_{2}^{*}(y)$ is $\\frac{1}{l_{h_{2},1}}$ -strongly convex and $\\frac{1}{\\alpha_{h_{2}}}$ -smooth according to Lemma 4. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-D(\\mu_{1})-(-D(\\mu_{2}))=h_{2}^{*}(-A\\mu_{1})-h_{2}^{*}(-A\\mu_{2})+h_{1}(\\mu_{1})-h_{1}(\\mu_{2})}\\\\ &{\\qquad\\qquad\\qquad\\geq\\langle\\frac{\\partial h_{2}^{*}}{\\partial-A\\mu_{2}},-A\\mu_{1}+A\\mu_{2}\\rangle+\\frac{1/l_{h_{2,1}}}{2}\\|A\\mu_{1}-A\\mu_{2}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\quad+\\left\\langle\\nabla h_{1}(\\mu_{2}),\\mu_{1}-\\mu_{2}\\right\\rangle\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\geq\\langle\\nabla D(\\mu_{2}),\\mu_{1}-\\mu_{2}\\rangle+\\frac{\\sigma_{\\operatorname*{min}}^{2}(A)/l_{h_{2,1}}}{2}\\|\\mu_{1}-\\mu_{2}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the first inequality follows the strong convexity of $h_{2}^{*}(y)$ and the fact that $-h_{1}(\\mu)$ is convex as $h_{1}(y)$ is concave. and the second inequality follows the chain rule to formulate $\\nabla D(\\mu_{2})$ . Therefore, $-D(\\mu)$ is $\\frac{\\sigma_{\\mathrm{min}}^{2}(A)}{l_{h_{2},1}}$ -strongly convex, and $D(\\mu)$ is $\\frac{\\sigma_{\\mathrm{min}}^{2}(A)}{l_{h_{2},1}}$ -strongly concave. ", "page_idx": 25}, {"type": "text", "text": "Moreover $D(\\mu)$ is $\\bigl(l_{h_{1},1}+\\frac{\\sigma_{\\operatorname*{max}}^{2}(A)}{\\alpha_{h_{2}}}\\bigr)$ \u03c32max(A))-smooth as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D(\\mu_{1})-D(\\mu_{2})=-\\;h_{2}^{\\ast}(-A\\mu_{1})-\\big(-h_{2}^{\\ast}(-A\\mu_{2})\\big)-h_{1}(\\mu_{1})+h_{1}(\\mu_{2})}\\\\ &{\\qquad\\qquad\\leq\\langle\\displaystyle\\frac{\\partial-h_{2}^{\\ast}(-A\\mu_{2})}{\\partial-A\\mu_{2}},-A\\mu_{1}-(-A\\mu_{2})\\rangle+\\displaystyle\\frac{1/\\alpha_{h_{2}}}{2}\\|-A\\mu_{1}-(-A\\mu_{2})\\|^{2}}\\\\ &{\\qquad\\qquad\\quad+\\left\\langle-\\nabla h_{1}(\\mu_{2}),\\mu_{1}-\\mu_{2}\\right\\rangle\\rangle+\\displaystyle\\frac{l_{h_{1},1}}{2}\\|\\mu_{1}-\\mu_{2}\\|^{2}}\\\\ &{\\qquad\\qquad\\leq\\langle\\nabla D(\\mu_{2}),\\mu_{1}-\\mu_{2}\\rangle+\\displaystyle\\frac{l_{h_{1},1}+\\frac{\\sigma_{\\operatorname*{max}}^{2}(A)}{\\alpha_{h_{2}}}}{2}\\|\\mu_{1}-\\mu_{2}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The first inequality holds as $h_{2}^{*}(y)$ and $h_{1}(\\mu)$ are smooth. The second follows the chain rule. ", "page_idx": 25}, {"type": "text", "text": "Note $\\sigma_{\\operatorname*{max}}(A)\\geq\\sigma_{\\operatorname*{min}}(A)>0$ as $A$ is full column rank. This completes the proof. ", "page_idx": 25}, {"type": "text", "text": "Knowing $D(\\mu)$ has such favorable properties, we next analyze the update of $\\|\\mu_{t+1}-\\mu_{t}\\|$ , where $\\left\\{\\mu_{t}\\right\\}$ is the sequence generated in the non-accelerated version of Algorithm 2 with $T_{y}=1$ . ", "page_idx": 25}, {"type": "text", "text": "Lemma 14 (Update of $\\|\\mu_{t+1}-\\mu_{t}\\|)$ . Consider the problem in (47) where $h_{1}$ is concave and $l_{h_{1},1}$ -smooth, $h_{2}$ is $\\alpha_{h_{2}}$ -strongly convex and $l_{h_{2},1}$ -smooth, and $A$ is full column rank. Running the non-accelerated version of Algorithm 2 with $T_{y}=1$ and $\\eta_{1}\\leq l_{h_{2},1}{}^{-1}$ gives ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{\\eta_{2}}\\|\\mu_{t+1}-\\mu_{t}\\|\\leq\\left(l_{h_{1},1}+\\frac{\\sigma_{\\operatorname*{max}}^{2}(A)}{\\alpha_{h_{2}}}\\right)\\|\\mu_{t}-\\mu^{*}\\|}\\\\ {\\displaystyle\\ +\\,\\sigma_{\\operatorname*{max}}(A)(1-\\eta_{1}\\alpha_{h_{2}}/2)\\|y_{t}-\\nabla h_{2}^{*}(-A\\mu_{t})\\|+\\|\\lambda\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the constant $\\lambda$ satisfies $0\\leq\\lambda<\\infty$ and $\\mu^{*}=\\arg\\operatorname*{max}_{\\mu\\in\\mathbb{R}_{+}^{d_{c}}}D(\\mu)$ with $D(\\mu)$ defined in (52). ", "page_idx": 25}, {"type": "text", "text": "Proof. According to Lemma 13, ", "page_idx": 25}, {"type": "equation", "text": "$$\nD(\\mu)=\\operatorname*{min}_{y\\in\\mathbb{R}^{d_{y}}}-h_{1}(\\mu)+y^{\\top}A\\mu+h_{2}(y)=-h_{1}(\\mu)-h_{2}^{*}(-A\\mu)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "is $\\frac{\\sigma_{\\mathrm{min}}^{2}(A)}{l_{h_{2},1}}$ -strongly concave and $\\begin{array}{r}{(l_{h_{1},1}+\\frac{\\sigma_{\\operatorname*{max}}^{2}(A)}{\\alpha_{h_{2}}})}\\end{array}$ -smooth with respect to $\\mu$ . Moreover, the problem $\\operatorname*{max}_{\\mu\\in\\mathbb{R}_{+}^{d_{c}}}D(\\mu)$ is equivalent to the unconstrained problem with the Lagrange multiplier ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mu\\in\\mathbb{R}^{d_{c}}}\\tilde{D}(\\mu):=D(\\mu)+\\lambda^{\\top}\\mu\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where unique $\\lambda$ is non-negative and finite in all dimension, i.e. $0\\leq\\lambda<\\infty$ , as $D(\\mu)$ is strongly convex and $\\mu\\in\\mathbb{R}_{+}^{d_{c}}$ is equivalent to $\\mu\\geq0$ satisfying the LICQ condition (Lemma 5). In this way, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\nabla\\tilde{D}(\\mu)=\\nabla D(\\mu)+\\lambda=-\\nabla h_{1}(\\mu)+A^{\\top}\\nabla h_{2}^{\\ast}(-A\\mu)+\\lambda.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We can see that $\\tilde{D}(\\mu)$ is smooth and strongly concave with the same modulus as $D(\\mu)$ . The first-order stationary condition requires ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\nabla\\tilde{D}(\\mu^{*})=-\\nabla h_{1}(\\mu^{*})+A^{\\top}\\nabla h_{2}^{*}(-A\\mu^{*})+\\lambda=0.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "In this way, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac{1}{\\eta_{2}}\\Vert\\mu_{t+1}-\\mu_{t}\\Vert=\\frac{1}{\\eta_{2}}\\Vert\\operatorname*{Proj}_{\\mathbb{R}_{+}^{d_{\\varepsilon}}}\\big(\\mu_{t}+\\eta_{2}(-\\nabla h_{1}(\\mu_{t})+A^{\\top}y_{t+1})\\big)-\\mu_{t}\\Vert}\\\\ &{\\stackrel{(a)}\\le\\Vert-\\nabla h_{1}(\\mu_{t})+A^{\\top}y_{t+1}\\Vert}\\\\ &{=\\Vert-\\nabla h_{1}(\\mu_{t})+A^{\\top}\\nabla h_{2}^{*}(-A\\mu_{t})+\\lambda+A^{\\top}y_{t+1}-A^{\\top}\\nabla h_{2}^{*}(-A\\mu_{t})-\\lambda\\Vert}\\\\ &{\\stackrel{(b)}\\le\\Vert\\nabla\\tilde{D}(\\mu_{t})\\Vert+\\sigma_{\\operatorname*{max}}(A)\\Vert y_{t+1}-\\nabla h_{2}^{*}(-A\\mu_{t})\\Vert+\\Vert\\lambda\\Vert}\\\\ &{\\stackrel{(c)}\\le\\Vert\\nabla\\tilde{D}(\\mu_{t})-\\nabla\\tilde{D}(\\mu^{*})\\Vert+\\sigma_{\\operatorname*{max}}(A)(1-\\eta_{1}\\alpha_{h_{2}}/2)\\Vert y_{t}-\\nabla h_{2}^{*}(-A\\mu_{t})\\Vert+\\Vert\\lambda\\Vert}\\\\ &{\\stackrel{(d)}\\le\\Big(I_{h_{1},1}+\\frac{\\sigma_{\\operatorname*{max}}^{2}(A)}{\\alpha_{h_{2}}}\\Big)\\Vert\\mu_{t}-\\mu^{*}\\Vert+\\sigma_{\\operatorname*{max}}(A)(1-\\eta_{1}\\alpha_{h_{2}}/2)\\Vert y_{t}-\\nabla h_{2}^{*}(-A\\mu_{t})\\Vert+\\Vert\\lambda\\Vert}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Inequality $(a)$ comes from the non-expansiveness (1-Lipschitzness) of the projection operation, $(b)$ follows triangle inequality and uses (54), $(c)$ uses (55) and (49) in Lemma 12, and $(d)$ comes from the smoothness of $\\tilde{D}(\\mu)$ , which is of the same modulus as $D(\\mu)$ . This completes the proof. \u518f\u53e3 ", "page_idx": 26}, {"type": "text", "text": "In (53), the update behavior of $\\|\\mu_{t+1}-\\mu_{t}\\|$ depends on $\\|\\mu_{t}-\\mu^{*}\\|$ . We further look into the update of $\\|\\mu_{t}-\\mu^{*}\\|$ and summarize in the following lemma the bound of the update of $\\|\\mu_{t}-\\mu^{*}\\|$ . ", "page_idx": 26}, {"type": "text", "text": "Lemma 15 (Update of $\\|\\mu_{t}-\\mu^{*}\\|)$ . Consider the problem in (47) where $h_{1}$ is concave and $l_{h_{1},1}$ - smooth, $h_{2}$ is $\\alpha_{h_{2}}$ -strongly convex and $l_{h_{2},1}$ -smooth, and $A$ is full column rank. Conduct the non-accelerated version of Algorithm 2 with $T_{y}=1$ , gives ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|\\mu_{t+1}-\\mu^{*}\\|\\leq\\left(1-\\eta_{2}\\frac{\\sigma_{\\operatorname*{min}}^{2}(A)}{2l_{h_{2},1}}\\right)\\|\\mu_{t}-\\mu^{*}\\|+\\eta_{2}\\sigma_{\\operatorname*{max}}(A)(1-\\eta_{1}\\alpha_{h_{2}}/2)\\|y_{t}-\\nabla h_{2}^{*}(-A\\mu_{t})\\|.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "when $\\eta_{1}\\leq l_{h_{2},1}{}^{-1}$ and $\\begin{array}{r}{\\eta_{2}\\leq\\left(l_{h_{1},1}+\\frac{\\sigma_{\\operatorname*{max}}^{2}(A)}{\\alpha_{h_{2}}}\\right)^{-1}}\\end{array}$ . Here \u00b5\u2217= arg mi $\\mu^{*}=\\arg\\operatorname*{min}_{\\mu\\in\\mathbb{R}_{+}^{d_{c}}}D(\\mu)$ n\u00b5\u2208Rd+c D(\u00b5) where D(\u00b5) is defined in (52). ", "page_idx": 26}, {"type": "text", "text": "Proof. Define an auxiliary update as ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\tilde{\\mu}_{t+1}:=\\mathrm{Proj}_{\\mathbb{R}_{+}^{d_{c}}}\\left(\\mu_{t}+\\eta_{2}\\nabla D(\\mu_{t})\\right)=\\mathrm{Proj}_{\\mathbb{R}_{+}^{d_{c}}}\\left(\\mu_{t}+\\eta_{2}(-\\nabla h_{1}(\\mu_{t})+A^{\\top}\\nabla h_{2}^{*}(-A\\mu_{t}))\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "This is a projected gradient descent on strongly convex $-D(\\mu)$ . As $\\mathbb{R}_{+}^{d_{c}}$ is closed and convex, following Lemma 7, for $\\begin{array}{r}{\\eta_{2}\\,\\le\\,\\left(l_{h_{1},1}+\\frac{\\sigma_{\\operatorname*{max}}^{2}(A)}{\\alpha_{h_{2}}}\\right)^{-1}}\\end{array}$ , where $\\left(l_{h_{1},1}+\\frac{\\sigma_{\\operatorname*{max}}^{2}(A)}{\\alpha_{h_{2}}}\\right)$ is the modulus for smoothness of $D(\\mu)$ by Lemma 13, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|\\tilde{\\mu}_{t+1}-\\mu^{*}\\|\\leq\\left(1-\\eta_{2}\\frac{\\sigma_{\\operatorname*{min}}^{2}(A)}{2l_{h_{2},1}}\\right)\\|\\mu_{t}-\\mu^{*}\\|.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "As the real update is $\\mu_{t+1}=\\operatorname{Proj}_{\\mathbb{R}_{+}^{d_{c}}}\\left((\\mu_{t}+\\eta_{2}(-\\nabla h_{1}(\\mu_{t})+A^{\\top}y_{t})\\right)$ , by the non-expansiveness (1-Lipschitzness) of projection operation, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|\\tilde{\\mu}_{t+1}-\\mu_{t+1}\\|\\leq\\|\\eta_{2}A^{\\top}(y_{t+1}-\\nabla h_{2}^{*}(-A\\mu_{t}))\\|\\leq\\eta_{2}\\sigma_{\\operatorname*{max}}(A)\\|y_{t+1}-\\nabla h_{2}^{*}(-A\\mu_{t})\\|\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By triangle inequality and (49), we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|\\mu_{t+1}-\\mu^{*}\\|\\leq\\left(1-\\eta_{2}\\frac{\\sigma_{\\operatorname*{min}}^{2}(A)}{2l_{h_{2},1}}\\right)\\|\\mu_{t}-\\mu^{*}\\|+\\eta_{2}\\sigma_{\\operatorname*{max}}(A)\\|y_{t+1}-\\nabla h_{2}^{*}(-A\\mu_{t})\\|}\\\\ &{\\leq\\left(1-\\eta_{2}\\frac{\\sigma_{\\operatorname*{min}}^{2}(A)}{2l_{h_{2},1}}\\right)\\|\\mu_{t}-\\mu^{*}\\|+\\eta_{2}\\sigma_{\\operatorname*{max}}(A)(1-\\eta_{1}\\alpha_{h_{2}}/2)\\|y_{t}-\\nabla h_{2}^{*}(-A\\mu_{t})\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "This completes the proof. ", "page_idx": 26}, {"type": "text", "text": "We are ready to proceed with the convergence analysis for the single-loop algorithm (Algorithm 2) without acceleration and $T_{y}=1$ , on the problems (47), which is a general form to (7) and (5). In this way, Theorem 4 follows directly from the following theorem. ", "page_idx": 27}, {"type": "text", "text": "Theorem 6. Suppose $L(\\mu,y)$ is in the form of (47) where $A$ is full column rank, $h_{1}$ is concave and $l_{h_{1},1}$ -smooth, $h_{2}$ is $\\alpha_{h_{2}}$ -strongly convex and $l_{h_{2},1}$ -smooth satisfying $l_{h_{1},1}=\\mathcal{O}(1)$ , $l_{h_{2},1},l_{\\alpha_{2}}\\geq$ $\\mathcal{O}(1)$ , and $\\begin{array}{r}{\\frac{l_{h_{2},1}}{\\alpha_{h_{2}}}\\,=\\mathcal{O}(1)}\\end{array}$ . Conduct the non-accelerated version of Algorithm 2 with $T_{y}=1$ . For arbitrary small positive $\\begin{array}{r}{\\epsilon\\leq\\left(\\frac{4l_{h_{2},1}\\sigma_{\\operatorname*{max}}(A)}{\\alpha_{h_{2}}\\sigma_{\\operatorname*{min}}^{2}(A)}(l_{h_{1},1}+\\frac{\\sigma_{\\operatorname*{max}}^{2}(A)}{\\alpha_{h_{2}}})\\right)^{-1}}\\end{array}$ \u03c32m\u03b1axh(A))  , when \u03b71 = O(lh1,1 ) \u2264lh and $\\begin{array}{r}{\\eta_{2}=\\mathcal{O}(\\epsilon)\\leq\\frac{1}{l_{h_{1},1}+\\sigma_{\\operatorname*{max}}^{2}(A)/\\alpha_{h_{2}}}}\\end{array}$ , the algorithm yields output $(\\mu_{T},y_{T})$ such that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|\\mu_{T}-\\mu^{*}\\|^{2}<\\epsilon,\\quad a n d\\quad\\|y_{T}-y^{*}\\|^{2}<\\epsilon\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "with complexity $T=\\mathcal{O}(\\ln(\\epsilon^{-1}))$ . Here, $\\begin{array}{r}{(\\mu^{*},y^{*})=\\arg\\operatorname*{max}_{\\mu\\in\\mathbb{R}^{d_{c}}}\\operatorname*{min}_{y\\in\\mathbb{R}^{d_{y}}}L(\\mu,y).}\\end{array}$ ", "page_idx": 27}, {"type": "text", "text": "Proof. For some positive constant $\\rho>0$ , denote ", "page_idx": 27}, {"type": "equation", "text": "$$\nP_{t}:=\\rho\\|\\mu_{t}-\\mu^{*}\\|+\\|y_{t}-\\nabla h_{2}^{*}(-A\\mu_{t})\\|.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Plugging (50) in Lemma 12 , (53) in Lemma 14, and (56) in Lemma 15 to (59), we know ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad P_{t+1}=\\rho\\|\\mu_{t+1}-\\mu^{*}\\|+\\|y_{t+1}-\\nabla h_{2}^{*}(-A\\mu_{t+1})\\|}\\\\ &{\\leq\\rho\\bigg(\\bigg(1-\\eta_{2}\\frac{\\sigma_{2}^{*}\\|\\tilde{A}_{1}}{2\\tilde{l}_{2,1}}\\bigg)\\,\\|\\mu_{t}-\\mu^{*}\\|+\\eta_{2}\\sigma_{\\operatorname*{max}}(A)(1-\\eta_{1}\\alpha_{h_{2}}/2)\\|y_{t}-\\nabla h_{2}^{*}(-A\\mu_{t})\\|\\bigg)}\\\\ &{\\quad+\\left(1-\\eta_{1}\\alpha_{h_{2}}/2\\right)\\|y_{t}-\\nabla h_{2}^{*}(-A\\mu_{t})\\|+\\frac{\\sigma_{\\operatorname*{max}}(A)}{\\alpha_{h_{2}}}\\eta_{2}}\\\\ &{\\quad\\times\\,\\bigg((l_{h_{1},1}+\\frac{\\sigma_{2}^{*}\\|\\tilde{A}_{1}}{\\alpha_{h_{2}}}(A))\\|\\mu_{t}-\\mu^{*}\\|+\\sigma_{\\operatorname*{max}}(A)(1-\\eta_{1}\\alpha_{h_{2}}/2)\\|y_{t}-\\nabla h_{2}^{*}(-A\\mu_{t})\\|+\\|\\boldsymbol{\\lambda}\\|\\bigg)}\\\\ &{=\\bigg(1-\\eta_{2}\\frac{\\sigma_{2}^{*}\\|\\tilde{A}_{1}}{2\\tilde{l}_{2,1}}+\\frac{1}{\\rho}\\frac{\\sigma_{\\operatorname*{max}}(A)}{\\alpha_{h_{2}}}\\eta_{2}(l_{h_{1},1}+\\frac{\\sigma_{\\operatorname*{max}}^{2}(A)}{\\alpha_{h_{2}}})\\bigg)\\,\\rho\\|\\mu_{t}-\\mu^{*}\\|}\\\\ &{\\quad+\\left(1-\\eta_{1}\\alpha_{h_{2}}/2\\right)\\bigg(1+\\rho\\eta_{2}\\sigma_{\\operatorname*{max}}(A)+\\frac{\\sigma_{\\operatorname*{max}}^{2}(A)}{\\alpha_{h_{2}}}\\eta_{2}\\bigg)\\,\\|y_{t}-\\nabla h_{2}^{*}(-A\\mu_{t})\\|+\\frac{\\sigma_{\\operatorname*{max}}(A)}{\\alpha_{h_{2}}}\\eta_{2}\\|\\boldsymbol{\\lambda}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "To construct $\\begin{array}{r}{P_{t+1}\\leq(1-c)P_{t}+\\frac{\\sigma_{\\operatorname*{max}}(A)}{\\alpha_{h_{2}}}\\eta_{2}\\|\\lambda\\|}\\end{array}$ for some constant $0<c<1$ , it is sufficient to find $\\begin{array}{r}{\\eta_{1}\\le\\frac{1}{l_{h_{2},1}},\\eta_{2}\\le\\frac{1}{(l_{h_{1},1}+\\frac{\\sigma_{\\operatorname*{max}}^{2}(A)}{\\alpha_{h_{2}}})}}\\end{array}$ , and $\\rho>0$ such that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\{0<\\left(1-\\eta_{2}\\frac{\\sigma_{\\operatorname*{min}}^{2}(A)}{2l_{h_{2},1}}+\\frac{1}{\\rho}\\frac{\\sigma_{\\operatorname*{max}}(A)}{\\alpha_{h_{2}}}\\eta_{2}(l_{h_{1},1}+\\frac{\\sigma_{\\operatorname*{max}}^{2}(A)}{\\alpha_{h_{2}}})\\right)\\leq1-\\eta_{2}\\frac{\\sigma_{\\operatorname*{min}}^{2}(A)}{4l_{h_{2},1}}<1\\right.}\\\\ &{\\left.\\left.\\left(0<(1-\\eta_{1}\\alpha_{h_{2}}/2)\\left(1+\\rho\\eta_{2}\\sigma_{\\operatorname*{max}}(A)+\\frac{\\sigma_{\\operatorname*{max}}^{2}(A)}{\\alpha_{h_{2}}}\\eta_{2}\\right)\\leq(1-\\eta_{1}\\alpha_{h_{2}}/2)(1+\\eta_{1}\\alpha_{h_{2}}/2)<1\\right)\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This can be obtained when ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\rho\\geq\\frac{4l_{h_{2},1}\\sigma_{\\operatorname*{max}}(A)}{\\alpha_{h_{2}}\\sigma_{\\operatorname*{min}}^{2}(A)}(l_{h_{1},1}+\\frac{\\sigma_{\\operatorname*{max}}^{2}(A)}{\\alpha_{h_{2}}})\\right.}\\\\ {\\left.\\eta_{2}\\leq\\frac{\\eta_{1}\\alpha_{h_{2}}}{2\\Big(\\rho\\sigma_{\\operatorname*{max}}(A)+\\frac{\\sigma_{\\operatorname*{max}}^{2}(A)}{\\alpha_{h_{2}}}\\Big)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Conditions in (60) can be satisfied when $\\epsilon\\mathrm{~>~0~}$ is sufficiently small such that $\\rho\\ =\\ \\epsilon^{-1}\\ \\geq$ $\\begin{array}{r}{\\frac{4l_{h_{2},1}\\sigma_{\\operatorname*{max}}(A)}{\\alpha_{h_{2}}\\sigma_{\\operatorname*{min}}^{2}(A)}(l_{h_{1},1}+\\frac{\\sigma_{\\operatorname*{max}}^{2}(A)}{\\alpha_{h_{2}}})}\\end{array}$ , $\\begin{array}{r}{\\eta_{1}=\\mathcal{O}(\\frac{1}{l_{h_{2},1}})}\\end{array}$ and $\\begin{array}{r}{\\eta_{2}=\\mathcal{O}(\\frac{\\alpha_{h_{2}}}{l_{h_{2},1}}\\rho^{-1})=\\mathcal{O}(\\epsilon^{-1})}\\end{array}$ . In this way, ", "page_idx": 27}, {"type": "equation", "text": "$$\nP_{t+1}\\leq(1-c)P_{t}+\\mathcal{O}(\\alpha_{h_{2}}^{-1}\\epsilon)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $c>0$ is of the order $O(\\epsilon)$ . Iteration gives ", "page_idx": 27}, {"type": "equation", "text": "$$\nP_{t}\\leq(1-c)^{t}P_{0}+\\mathcal{O}(\\alpha_{h_{2}}^{-1}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Notice $\\mathcal{O}(\\alpha_{h_{2}}^{-1})<\\mathcal{O}(1)$ and $P_{0}=\\mathcal{O}(\\epsilon^{-1})$ as $\\rho=\\epsilon^{-1}$ . In this way, there exist $T_{1}=\\mathcal{O}(\\ln(\\epsilon^{-1}))$ such that for all $t>T_{1}$ , $(1-c)^{t}P_{0}=\\mathcal{O}(1)$ and $\\mathcal{O}(\\alpha_{h_{2}}^{-1})\\leq\\mathcal{O}(1)$ . Accordingly, we can achieve ", "page_idx": 28}, {"type": "equation", "text": "$$\nP_{t}=\\mathcal{O}(1),\\quad\\forall t>T_{1}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Moreover, as $P_{t}=\\epsilon^{-1}\\|\\mu_{t}-\\mu^{*}\\|+\\|y_{t}-\\nabla h_{2}^{*}(-A\\mu_{t})\\|,$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|\\mu_{t}-\\mu^{*}\\|\\leq\\epsilon P_{t}=\\mathcal{O}(\\epsilon),\\quad\\forall t>T_{1}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Furthermore, choose $\\begin{array}{r}{\\eta_{1}=\\mathcal{O}(\\frac{1}{l_{h_{2},1}})}\\end{array}$ satisfying $\\begin{array}{r}{\\eta_{1}\\leq\\frac{1}{l_{h_{2},1}}}\\end{array}$ , for $t>T_{1}$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|y_{t}-\\nabla h_{2}^{*}(-A\\mu_{t})\\|\\leq(1-\\eta_{1}\\alpha_{h_{2}}/2)^{t-T_{1}}\\|y_{T_{1}}-\\nabla h_{2}^{*}(-A\\mu_{T_{1}})\\|+\\mathcal{O}(\\epsilon).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "This is an iteration outcome using (50) in Lemma 12, (62), and the fact that \u03b71\u03b1h2/2 = O(l\u03b1h2h,21 ) = $\\mathcal{O}(1)$ . In this way, for another $T_{2}=\\mathcal{O}(\\ln(\\epsilon^{-1}))$ steps, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|y_{t}-\\nabla h_{2}^{*}(-A\\mu_{t})\\|=\\mathcal{O}(\\epsilon),}\\\\ {\\mathrm{and}}&{\\|y_{t}-y^{*}\\|\\leq\\|y_{t}-\\nabla h_{2}^{*}(-A\\mu_{t})\\|+\\|\\nabla h_{2}^{*}(-A\\mu_{t})-\\nabla h_{2}^{*}(-A\\mu^{*})\\|}\\\\ &{\\qquad\\qquad\\leq\\|y_{t}-\\nabla h_{2}^{*}(-A\\mu_{t})\\|+\\displaystyle\\frac{\\sigma_{\\operatorname*{max}}^{2}(A)}{\\alpha_{h_{2}}}\\|\\mu_{t}-\\mu^{*}\\|}\\\\ &{\\qquad\\qquad=\\mathcal{O}\\left(\\epsilon+\\alpha_{h_{2}}^{-1}\\epsilon\\right)=\\mathcal{O}\\left(\\epsilon\\right),\\quad\\forall t>T_{1}+T_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We can see that the algorithm converges linearly with complexity $\\mathcal{O}(T_{1}+T_{2})=\\mathcal{O}(\\ln(\\epsilon^{-1}))$ . In this way, obtaining ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|y_{T}-y^{*}\\|^{2}={\\mathcal{O}}(\\epsilon)\\quad{\\mathrm{and}}\\quad\\|\\mu_{T}-\\mu^{*}\\|^{2}={\\mathcal{O}}(\\epsilon),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "requires complexity $T=\\mathcal{O}(\\ln\\bigl((\\sqrt{\\epsilon})^{-1}\\bigr))=\\mathcal{O}(\\ln\\bigl(\\epsilon^{-1}\\bigr))$ . This completes the proof. ", "page_idx": 28}, {"type": "text", "text": "Remark 4. Under the same assumptions as in Theorem $^{4}$ , we cam choose $\\eta_{g,2}\\lesssim(l_{g,1})^{-1}$ , $\\eta_{g,1}\\leq$ $\\alpha_{g}^{2}\\epsilon_{g}\\big(2s_{\\mathrm{max}}\\alpha_{g}+s_{\\mathrm{max}}^{2}\\epsilon^{-1}\\big)^{-1}\\eta_{g,2}$ as stepsizes for running the single-loop version of Algorithm $2\\;t o$ solve (7), and $(l_{f,1}+\\gamma l_{g,1})^{-1},\\,\\eta_{F,2}\\leq(\\gamma\\alpha_{g}-l_{f,1})^{2}\\epsilon_{F}(2s_{\\operatorname*{max}}(\\gamma\\alpha_{g}-l_{f,1})+s_{\\operatorname*{max}}^{2}\\epsilon^{-1})^{-1}\\eta_{F,2}$ as the ones for (5). ", "page_idx": 28}, {"type": "text", "text": "E Applications to Hyperparameter Optimization for SVM ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this section, we provide additional details about the SVM model training experiment for the linear SVM model, including the problem formulation and analysis of the results. ", "page_idx": 28}, {"type": "text", "text": "E.1 Problem formulation ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "SVMs train a machine learning model by finding the optimal hyperplane that separates data points of different classes with the maximum margin $w$ . Misclassification is not tolerated for hard-margin SVMs. In contrast, some samples are allowed to be misclassified for soft-margin SVMs. Specifically, one first introduces variables $\\xi_{i}$ , which measure the violation associated with the classification of sample $i$ , and then augments the original SVM objective with the norm of $\\xi$ , which is a vector collecting the values of $\\xi_{i}$ for all the samples in the training set. ", "page_idx": 28}, {"type": "text", "text": "BLO can be applied to the hyperparameter selection task of SVM. For example, it can be used to choose the value of the regularization parameters during soft-margin linear SVM training. Let us consider a classification problem and define $\\mathcal{D}_{\\mathrm{tr}}:=\\{(z_{\\mathrm{tr},i},l_{\\mathrm{tr},i})\\}_{i=1}^{|\\bar{\\mathcal{D}}_{\\mathrm{tr}}|}$ as the training set, with $z_{\\mathrm{tr},i}$ being the input feature vector for sample $i$ and $l_{\\mathrm{tr},i}$ being its associated binary label. Similarly, let $\\mathcal{D}_{\\mathrm{val}}:=\\{(z_{\\mathrm{val},},l_{\\mathrm{val},})\\}_{i=1}^{|\\mathcal{D}_{\\mathrm{val}}|}$ )}|iD=v1al|be the validation set. For a linear classification problem, the parameters of the SVM are $w$ , a vector of coefficients with the same size as $z_{\\mathrm{tr},i}$ , and the intercept $b$ . ", "page_idx": 28}, {"type": "text", "text": "In short, we are interested in the following constrained BLO problem ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{c}{\\mathrm{min}}}&{\\mathcal{L}_{\\mathcal{D}_{\\mathrm{val}}}(w^{*},b^{*})=\\underset{(z_{\\mathrm{val}},l_{\\mathrm{val}})\\in\\mathcal{D}_{\\mathrm{val}}}{\\sum}\\exp\\left(1-l_{\\mathrm{val}}\\left(z_{\\mathrm{val}}^{\\top}w^{*}+b^{*}\\right)\\right)+\\frac{1}{2}\\|c\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{with}\\ w^{*},b^{*},\\xi^{*}=\\underset{w,b,\\xi}{\\mathrm{arg\\min}}\\ \\frac{1}{2}\\|w\\|^{2}}&{}\\\\ {\\mathrm{s.t.}\\ \\ l_{\\mathrm{tr},i}(z_{\\mathrm{tr},i}^{\\top}w+b)\\geq1-\\xi_{i}}&{\\forall\\ i\\in\\{1,\\ldots,|\\mathcal{D}_{\\mathrm{tr}}|\\}}\\\\ {\\xi_{i}\\leq c_{i}}&{\\forall\\ i\\in\\{1,\\ldots,|\\mathcal{D}_{\\mathrm{tr}}|\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The first term of upper-level objective (63a) is a validation loss, evaluated on the validation set $\\mathcal{D}_{\\mathrm{val}}$ , the second is a regulation term on the upper-level variable $c$ ; the lower-level problem is designed to train the parameters of the hyperplane on the training set $\\mathcal{D}_{\\mathrm{tr}}$ , with the soft margin violation $\\xi_{i}$ being upper bounded by the hyperparameter $c_{i}$ [cf. (63d), which are the CCs]. The lower-level objective (63b) focuses on maximizing the margin by minimizing $\\|w\\|^{2}$ while allowing for violations $\\xi$ to the separating hyperplane, which are regulated by the hyperparameter $c$ . The BLO formulation aims to adjust the hyperparameter $c$ using the validation loss (upper-level objective), ensuring that the model parameters (lower-level variables) are optimal for the training dataset. ", "page_idx": 29}, {"type": "text", "text": "E.2 Experiment details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this section, we present detailed experimental results for the training of the SVM model in (63) using our BLOCC algorithm. We compare our algorithm to two baselines, LV-HBA [75] and GAM [72], both designed for BLO problems with inequality CCs. We use $\\gamma=12$ and $\\eta=0.01$ for running our BLOCC and we apply $\\alpha\\,=\\,0.01$ , $\\gamma_{1}\\,=\\,0.1$ , $\\gamma_{2}\\,=\\,0.1$ , $\\eta\\,=\\,0.001$ for LV-HBA and $\\alpha\\,=\\,0.05$ , $\\epsilon\\,=\\,0.005$ for GAM respectively. The hyper-parameters for LV-HBA and GAM are the ones used in the SVM experiments in their paper. The GAM algorithm often encounters issues with matrix inversion, as the matrix required for the GAM algorithm to solve the problem in (63) can be singular, preventing it from finding a solution. Consequently, only BLOCC and LV-HBA solve the problem in (63), while GAM uses a different formulation as introduced in [72]. We compare the algorithms based on validation loss (the first term in the upper-level objective) and classification accuracy. These metrics are computed for both validation and test datasets. We evaluate the algorithms on two datasets: diabetes [20] and fourclass [29]. ", "page_idx": 29}, {"type": "text", "text": "The detailed results are shown in Figure 4 and 5. Ten realizations of the problem are run, each with different training and validation sets. The plots show both the mean (line) and standard deviation (shaded region). The results reveal that our algorithm converges faster in terms of accuracy and loss, achieving a lower loss value than the alternatives for both datasets, in validation and test sets. Furthermore, in terms of accuracy, we observe that for the diabetes dataset, our algorithm reaches a higher accuracy value faster than the alternatives in both validation and test sets. ", "page_idx": 29}, {"type": "image", "img_path": "uZi7H5Ac0X/tmp/0f2b7c58b9aa99763aa79260397069291b2de9e06012fa20c8d3a00723a9f8ad.jpg", "img_caption": ["Figure 5: Plots for fourclass dataset "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "F Applications to Transportation Network Planning ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "This section applies the proposed BLOCC algorithm to a transportation network design problem, comparing it with the baselines from [72] and [75]. ", "page_idx": 30}, {"type": "text", "text": "F.1 Problem formulation ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In transportation network planning, the planning operator is to construct a new transportation network (upper level) connecting a collection of stations $\\boldsymbol{S}$ , and the passengers will decide whether to use the new network (lower-level), considering the options given by the new network and existing alternatives (constraints). The goal is to design a network that maximizes the operator\u2019s benefit, knowing the passengers will make rational choices depending on the given design. ", "page_idx": 30}, {"type": "text", "text": "The operator considers building a new network on a collection of links $A\\subseteq S\\times S$ . For any link $(i,j)\\in A$ connecting stations $i\\in S$ to $j\\in S$ , the operator needs to design the capacity $x_{i j}$ . If the link\u2019s capacity is set to zero, then the link is not constructed. The larger the capacity, the larger the number of travelers, thus the larger the revenue while the higher the construction cost $c_{i j}$ . ", "page_idx": 30}, {"type": "text", "text": "The passengers are in demand to travel in the network $\\mathcal{K}\\subseteq\\mathcal{S}\\times\\mathcal{S}$ . For every origin-destination pair $(o,d)$ , the traffic demand $w^{o d}$ and the traffic time on the existing route $t_{\\mathrm{ext}}^{o d}$ is known. We assume that there is only one existing network. The proportion of passengers choosing the new network $y^{o d}$ and $y_{i j}^{o d}$ the fraction of passengers using link $(i,j)$ to travel from $o$ to $d$ will be determined following passengers\u2019 rational choice, modeled by logit choice model [5, 11] that will be explained later. If $\\bar{y}_{i j}^{o d}=\\bar{0}$ , then the link does not belong to the route that passengers follow to travel from $o$ to $d$ . ", "page_idx": 30}, {"type": "text", "text": "In a) of Figure 6, the station $\\boldsymbol{S}$ are presented as the dots, and links $\\boldsymbol{\\mathcal{A}}$ are as the dashed lines (input topology); b) of Figure 6 shows a heatmap for the demand matrix for each $(o,d)\\in K$ (input to the design); c) is an example of the constructed network connecting some of the stations (output of the design); and d) illustrates the number of passengers using the constructed network (design output). ", "page_idx": 30}, {"type": "text", "text": "To summarize, in this experiment, we assume that ", "page_idx": 30}, {"type": "text", "text": "A1) Only one existing alternative network exists; ", "page_idx": 30}, {"type": "text", "text": "A2) Passengers decide whether to use our network rationally, which is modeled by the logit choice model considering a utility depends on travel attributes (travel time) [5, 11]; and, ", "page_idx": 30}, {"type": "text", "text": "A3) The demand per market, the trip prices, and the travel times per link are known to operators. ", "page_idx": 30}, {"type": "text", "text": "We summarize the optimization variables as follows ", "page_idx": 30}, {"type": "text", "text": "\u2022 $x_{i j}\\in\\mathbb{R}_{+}$ , the capacity constructed for the link $(i,j)\\in A$ . The number of capacity variables is $|{\\mathcal{A}}|$ . The link is not constructed if $x_{i j}=0$ .   \n\u2022 $y^{o d}\\in[0,1]$ , the fraction (proportion) of passengers from market $(o,d)\\in K$ choosing the new network for their travel. The number of flow variables is $|\\kappa|$ . Since we consider only one competitor, $1-y^{o d}$ represents the fraction of incumbent network passengers. If $y^{o d}=0$ , then the passengers of market $(o,d)$ do not use the new network.   \n\u2022 $y_{i j}^{o d}\\in[0,1]$ , the fraction of passengers from market $(o,d)\\in K$ that, when choosing the new network to travel from $o$ to $d$ , use the link $(i,j)\\in A$ in their route to the destination. The number of link flow variables is $|{\\mathcal{A}}||K|$ . With this definition, it holds that $y_{i j}^{o d}\\leq y^{o d}$ ; and, if $y_{i j}^{o d}=0$ , then link $(i,j)$ does not belong to the route when traveling from $o$ to $d$ . ", "page_idx": 30}, {"type": "text", "text": "To simplify the notation and to make it consistent with the notations used in the paper, we denote ", "page_idx": 30}, {"type": "text", "text": "\u2022 $x=\\{x_{i j}\\}_{\\forall(i,j)\\in A}$ represents the upper-level variables to be optimized.   \n\u2022 $\\mathcal{X}=\\mathbb{R}_{+}^{|\\mathcal{A}|}$ represents the domain of $x$ .   \n\u2022 $\\boldsymbol{y}=\\{y^{o d},\\{y_{i j}^{o d}\\}_{\\forall(i,j)\\in A}\\}_{\\forall(o,d)\\in K}$ represents the lower-level variables to be optimized   \n\u2022 $\\mathcal{V}=[\\varepsilon,1-\\varepsilon]^{|\\mathcal{K}|}\\times[\\varepsilon,1-\\varepsilon]^{|\\mathcal{A}||\\mathcal{K}|}$ , where $\\varepsilon$ is a small positive number set by the designer of the network, represents the domain of $y$ . ", "page_idx": 30}, {"type": "image", "img_path": "uZi7H5Ac0X/tmp/9d78eca0eee18004b25861081c5ee2acce2396acbd0c5714eb7c1ced070c4a8e.jpg", "img_caption": ["Figure 6: Example of a transportation network design. (a) represents the set of stations ${\\boldsymbol{S}}\\,=$ $\\{\\bar{1},2,3,4,5,6,7,8,9\\}$ and the set of links $A\\subseteq S\\times S$ , where the number of links is $\\lvert A\\rvert=30$ (15 segments with two orientations each) and the value on each edge represents the travel time. (b) represents the demand $\\{w^{o d}\\}$ between all $(o,d)$ pairs, where $|K|=9\\stackrel{\\_}{\\times}8=72$ values are provided (those in the off-diagonal elements of the heatmap). (c) represents the constructed network, where, to facilitate visualization, we have assumed that the capacity is symmetric and used the width of the edge to represent the capacity of every link. (d) represents $\\{w^{\\stackrel{\\bullet}{o}d}y^{o d}\\}_{\\{\\forall(o,d)\\in K\\}}$ , the number of passengers served by the constructed network. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Besides the optimization variables, our objective and constraints include the following parameters ", "page_idx": 31}, {"type": "text", "text": "\u2022 $w^{o d}$ , the total estimated demand (number of passengers) for the market $(o,d)\\in K$ .   \n\u2022 $m^{o d}$ , the revenue obtained by the operator from a passenger in the market $(o,d)\\in K$ .   \n\u2022 $c_{i j}$ , the construction cost per passenger associated with link $(i,j)\\in A$ .   \n\u2022 $t_{i j}$ , the travel time for link $(i,j)\\in A$ .   \n\u2022 $t_{\\mathrm{ext}}^{o d}$ , travel time on the alternative network for passengers in the market $(o,d)\\in K$ .   \n\u2022 $\\omega_{t}<0$ , the coefficient associated with the travel time in passengers\u2019 utility function. ", "page_idx": 31}, {"type": "text", "text": "In transportation network design, a bilevel formulation is essential due to the interaction between two players with different levels of influence: the operator, who constructs the network, and the passengers, who choose their routes based on the network\u2019s characteristics. The operator\u2019s goal is to maximize their benefit by minimizing construction costs and maximizing attracted demand, with link capacity as the optimization variable. Conversely, passengers aim to maximize their trip utility, determining the proportion of demand using each link. This dual optimization requires that passenger choices comply with link capacity constraints set by the operator, coupling the variables at both levels. This necessitates a bilevel optimization approach, specifically using BLOCC, to address the interdependent decisions and constraints effectively. ", "page_idx": 31}, {"type": "text", "text": "Now, we are ready to introduce the objective formulations of our BLO problem. For the upper level, the network operator aims to maximize profits and minimize costs, and therefore, its interest is ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathcal{X}}f(x,y_{g}^{*}(x)):=-\\Biggl(\\underbrace{\\sum_{\\forall(o,d)\\in\\mathcal{K}}m^{o d}y^{\\mathrm{od}*}(x)}_{\\mathrm{profit}}-\\underbrace{\\sum_{\\forall(i,j)\\in\\mathcal{A}}c_{i j}x_{i j}}_{c o s t}\\Biggr),\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $y^{\\mathrm{od}*}(x)$ are optimal lower-level passenger flows associated with the network design $x$ . ", "page_idx": 32}, {"type": "text", "text": "For the lower-level, we model the passenger\u2019s behavior by finding the flow variables that maximize utility and minimizes flow entropy cost. ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{y\\in\\mathcal{Y}}g(x,y):=-\\left(\\sum_{(o,d)\\in\\mathcal{K}}\\sum_{(i,j)\\in\\mathcal{A}}w^{o d}\\omega_{t}t_{i j}y_{i j}^{o d}+\\sum_{(o,d)\\in\\mathcal{K}}w^{o d}\\omega_{t}t_{\\mathrm{ext}}^{o d}(1-y^{o d})\\right)\n$$", "text_format": "latex", "page_idx": 32}, {"type": "equation", "text": "$$\n+\\sum_{(o,d)\\in K\\atop\\tiny\\{o,d\\}\\in K}w^{o d}y^{o d}(\\ln\\bigl(y^{o d}\\bigr)-1)+\\sum_{(o,d)\\in K\\atop\\tiny\\{o,d\\}\\in K}w^{o d}(1-y^{o d})(\\ln\\bigl(1-y^{o d}\\bigr)-1)\\Biggr).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The passengers\u2019 utility considers the time cost of choosing the new network and the existing network. The users set the flow variables $y$ so that the transportation network yielding a higher utility is preferred. Here, the probability of chosing new network is modeled by a logistic (softmax) model. However, setting the objective as a simple linear utility maximization would lead to an all-or-nothing policy, which is not the behavior observed in practice. Hence, the second term is brought to consider. The approach considered here is to formulate an objective given by the Legendre transform (cf. Definition 5) of the softmax sharing (see [54, 57, 56] for additional details). Intuitively, this means that rather than imposing the softmax sharing a fortiori, we formulate a convex problem whose KKT conditions lead to the softmax sharing. Using the fact that the Legendre transform of an exponential $e^{y}$ is the negative entropy function $\\bar{y(\\ln(y)-1)}$ , the lower-level objective is traditional as in [57]. ", "page_idx": 32}, {"type": "text", "text": "Having introduced the optimization variables, parameters, and objective functions, we next formulate our BLO problem, where we also incorporate the network constraints: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{c c l}{\\displaystyle\\operatorname*{min}_{x\\in\\mathcal{X}}}&{\\displaystyle(6c)\\epsilon^{\\mathrm{out}}y^{\\mathrm{ode}}+\\sum_{\\forall(i,j)\\in\\mathcal{A}}c_{i j}x_{i j}}&{\\mathrm{(6c)}}\\\\ {\\displaystyle\\cdot(y^{\\mathrm{ode}},y_{i j}^{\\mathrm{ode}})=\\arg\\operatorname*{min}_{y\\in\\mathcal{Y}}}&{\\displaystyle(6c,\\mathrm{for})\\epsilon^{\\mathrm{od}}\\omega_{i}t_{i j}y_{i j}^{\\mathrm{ode}}-\\sum_{(o,i)\\in\\mathcal{A}}w^{\\mathrm{od}}\\omega_{i}t_{e x i}^{\\mathrm{od}}(1-y^{\\mathrm{od}})}&{\\mathrm{(6c)}}\\\\ {\\displaystyle+\\sum_{(o,i)\\in\\mathcal{E}}}&{\\displaystyle w^{\\mathrm{ode}}y^{\\mathrm{ode}}(\\ln(y^{\\mathrm{ode}})-1)+\\sum_{(o,i)\\in\\mathcal{E}}w^{\\mathrm{od}}(1-y^{\\mathrm{od}})(\\ln(1-y^{\\mathrm{od}})-1)}&{}\\\\ {\\displaystyle\\operatorname*{s.t}}&{\\displaystyle\\sqrt{s_{i j}}\\sum_{\\forall j\\mid(i,j)\\in\\mathcal{A}}^{j\\mathrm{od}}}&{\\displaystyle y_{j i}^{\\mathrm{od}}=\\left\\{\\begin{array}{l l}{y^{\\mathrm{od}}}&{\\mathrm{if}\\;i=o}\\\\ {-y^{\\mathrm{od}}}&{\\mathrm{if}\\;i=d}\\\\ {0}&{\\mathrm{otherwise}}\\end{array}\\right.\\mathrm{~(6c)}}&{\\mathrm{(6c)}}\\\\ {\\displaystyle\\sum_{(o,d)\\in\\mathcal{E}}w^{\\mathrm{od}}y_{i j}^{\\mathrm{od}}\\leq x_{i j}}&{\\displaystyle(i,j)\\in\\mathcal{A}}&{\\mathrm{(6c)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where (66c) are the flow-conservation constraints, (66d) are the capacity constraints that involve both upper and lower-level variables, coupling the optimization and motivating the use of BLOCC. Note that, for networks with $n$ stations (nodes): i) the number of CCs is approximately $n^{2}$ ; and ii) each of the constraints involves approximately $n^{2}$ variables. Hence, even for a moderate-size network (say 30-50 nodes), we may have thousands of CCs involving millions of variables. ", "page_idx": 32}, {"type": "text", "text": "Experiment roadmap. To provide numerical results illustrating the behavior of our algorithm, we solve the optimization in (66) for three scenarios: ", "page_idx": 32}, {"type": "text", "text": "S1) The design of a 3-node simple synthetic network; ", "page_idx": 32}, {"type": "image", "img_path": "uZi7H5Ac0X/tmp/f1b2e2e9dd2d478483f7d19a59d9fd5df2ed7a3d669191711cc0cf16d00d1059.jpg", "img_caption": ["Figure 7: Upper-level objective $f(x_{t},y_{t})$ for a Figure 8: Optimality gap of the lower-leve 3-node network design problem; Solid lines show problem for a 3-node network design prob mean value of f(xt, ygT,gt) with the shaded region lem $g(x_{t},y_{t})\\;-\\;g(x_{t},y_{t}^{*})$ . Solid lines repreas a standard deviation; Dashed lines show the sent the mean value of the 10 realizations o mean value of f(xt, yFT,Ft) with the shaded region the upper-level variables, dashed lines represen as a standard deviation; Three different $\\gamma$ values $g(x_{t},\\Bar{y}_{F,t}^{T_{F}})-g(x_{t},y_{t}^{*})$ , and the shaded region is (red, purple, blue) and fixed stepsize $\\eta=1.6\\times$ the standard deviation. Three different $\\gamma$ values $10^{-4}$ ; The orange color represents the result of are represented in our algorithm, and fixed step the LV-HBA algorithm. size $\\dot{\\eta^{*}}\\mathrm{=1.6\\times10^{-4}}$ . "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "S2) The design of a 9-node synthetic network from the prior transportation literature; and S3) The design of a (real-world) subway network for the city of Seville, Spain, with 24 nodes where S1) involves 6 CCs and 48 variables, and S3) involves around $100\\,\\mathrm{CCs}$ and 50,000 variables. For the 3-node network scenario, we will conduct a comparative analysis against other algorithms to evaluate the efficacy of our approach. In the other two scenarios, the baseline algorithms cannot find a solution; hence, for the 9-node and Seville networks, we will focus on providing insights into the performance and behavior of our algorithm under varying parameters, shedding light on the versatility and adaptability of our approach to real-world transportation networks. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "Before delving into the presentation and analysis of the results, two additional remarks are in order: ", "page_idx": 33}, {"type": "text", "text": "R1) While one of the goals of these experiments was to compare our BLOCC algorithm against LV-HBA [75] and GAM [72], for the scenario at hand, the GAM algorithm cannot be implemented, since the inverse of a matrix at each iteration for the problem in (66) is not tractable. In this way, we only conducted the experiments using our BLOCC and LV-HBA. ", "page_idx": 33}, {"type": "text", "text": "R2) The BLOCC algorithm produces two lower-level variables: $y_{F,T}^{T_{F}}$ and $y_{g,T}^{T_{g}}$ . While Theorems 1 and 2 guarantee that $(x_{T},y_{F,T}^{T_{F}})$ is an $\\epsilon$ -approximate solution, the pair $(x_{T},y_{g,T}^{T_{g}})$ is strictly feasible, meaning that ygT,gT $y_{g,T}^{T_{g}}=y_{g}^{*}(x_{T})$ . Since strict feasibility is important in the transportation network, this section will report the results using both $y_{F,T}^{T_{F}}$ and yg,T . ", "page_idx": 33}, {"type": "text", "text": "F.2 Numerical results for the 3-node network ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In this section, we address the problem defined in equation (66) for a network comprising 3 nodes (stations). We assume that the graph of potential links $\\boldsymbol{\\mathcal{A}}$ is complete, leading to a total of 6 link capacities that need to be determined in the upper level. Additionally, as in the rest of the manuscript, we assume that there is demand for all markets, meaning that the set $\\kappa$ is complete and includes all 6 possible origin-destination pairs. The specific values of the key parameters can be found in Table 4, with further details about the simulated scenario available in the online code repository. ", "page_idx": 33}, {"type": "text", "text": "For BLOCC, we set the stepsize to $\\eta=1.6\\times10^{-4}$ and analyze the algorithm\u2019s convergence for three different values of $\\gamma$ : $\\gamma=2$ , $\\gamma=3$ , and $\\gamma=4$ . The upper-level objective values are computed using $f(x_{t},y_{g,t}^{T_{g}})$ and $f(x_{t},y_{F,t}^{T_{F}})$ . The results are presented in Figures 7 and 8. ", "page_idx": 33}, {"type": "table", "img_path": "uZi7H5Ac0X/tmp/2b332566404eff9cca63646393a2eef9300febf5c5a1289eb48df5437a9defa8.jpg", "table_caption": ["Table 4: Value of the parameters for scenario 1 (3-node network). The value of $\\omega_{t}$ is set to 0.1. "], "table_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "uZi7H5Ac0X/tmp/899c8b41c158f8bf43be09d9ffdec50627442a616c8b0beb0625f9ebc602ab3e.jpg", "img_caption": ["Figure 9: Upper-level objective $f(x_{t},y_{t})$ for a 9-node network design problem; Solid lines show the mean value of f(xt, ygT,gt) with the shaded region as a standard deviation; Dashed lines show the mean value of f(xt, yFT,Ft) with the shaded region as a standard deviation; Three different $\\gamma$ values (red, purple, blue) and fixed stepsize $\\eta=1.6\\times10^{-4}$ ; The orange color represents the result of the LV-HBA algorithm. "], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "uZi7H5Ac0X/tmp/96631af8f73aa3f14618b880471c96f876e2f633302f7dc8c49963f0eda9c41d.jpg", "img_caption": ["Figure 10: Upper-level objective $f(x_{t},y_{t})$ for a metro network design problem in Seville, Spain, for 2 random initializations of the upper-level variables. Solid lines represent the mean of $f(x_{t},y_{g,t}^{T_{g}})$ , and the shaded region is the standard deviation. The dashed lines represent the mean of $f(x_{t},y_{F,t}^{T_{F}})$ , and the shaded region is the standard deviation. Three different $\\gamma$ values are tested with a fixed stepsize $\\eta=1.6\\times10^{-4}$ . "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Figure 7 illustrates the performance of our BLOCC algorithm over time. The orange line represents the evolution of $f(x_{t},y_{t})$ for the LV-HBA algorithm, while the other six lines represent different implementations of BLOCC. Each color represents a different value of $\\gamma$ ; solid lines represent $f(\\bar{x}_{t},y_{g,t}^{T_{g}})$ and dashed lines represent $f(x_{t},y_{F,t}^{T_{F}})$ . Each simulation is conducted 10 times with 10 different random initializations of the upper-level variables, and both the mean and the standard deviation values are displayed. The results indicate that all versions of our BLOCC algorithm converge in less than 10 seconds, while LV-HBA shows slight fluctuations even after running for more than 50 seconds. This may be attributed to the fact that the LV-HBA algorithm requires a joint projection into $\\{\\chi\\times\\mathcal{Y}:g^{c}(x,\\dot{y})\\leq0\\}$ at each iteration, involving 42 variables and $6\\,\\mathrm{CCs}$ . ", "page_idx": 34}, {"type": "text", "text": "Figure 8 shows the lower-level optimality gap, namely $g(x_{t},y_{t})-g(x_{t},y^{*}(x_{t}))$ for LV-HBA and BLOCC with 3 different values of $\\gamma$ . In the case of LV-HBA, lower-level optimality is not attained within 70 seconds of running time. Conversely, for BLOCC, lower-level optimality is achieved by construction when $y_{t}$ is set to $y_{g,T}^{T_{g}}$ , resulting in a zero gap. Additionally, when $y_{t}$ is set to $y_{F,T}^{T_{F}}$ , we observe that: i) lower-level optimality is accomplished for $\\gamma\\geq3$ , and ii) when $\\gamma=2$ , an optimality gap exists, but it is one order of magnitude smaller than that for LV-HBA. ", "page_idx": 34}, {"type": "text", "text": "F.3 Numerical results for the 9-node network ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "In this case, we consider the network in [21], see also Figure 6, which has $|{\\cal S}|\\,=\\,9$ nodes and $\\lvert A\\rvert=30$ potential links. As before, we consider that all markets exist, so that $|K|=9\\cdot8=72$ . The remaining parameters are described in Figure 6 and the code repository. ", "page_idx": 34}, {"type": "text", "text": "Figure 9 is the counterpart of Figure 7 for the 9-node scenario, showing the behavior of our BLOCC algorithm for $\\eta=1.6^{\\circ}\\times10^{-4}$ and $\\gamma\\in\\{2,3,4\\}$ . Each simulation is repeated 10 times (using 10 different random initializations of the upper-level variables), and both the mean and the standard deviation values are shown. Since the number of variables and constraints is almost one order of magnitude larger, the algorithm requires more time to converge. However, convergence takes place in a reasonable amount of time (20-40 times longer than in the previous 3-node test case). Regarding the optimal value, we observe that: i) the sensitivity of the (steady-state) optimal value with respect to $\\gamma$ is not too large; ii) the solutions based on $y_{F,T}^{T_{F}}$ yield better upper-level values than those based on $y_{g,T}^{T_{g}}$ ; and iii) the gap between $f(x_{T},y_{g,T}^{T_{g}})$ and $f(x_{T},y_{F,T}^{T_{F}})$ yFT,FT )decreases as \u03b3 increases. Observation ii) is due to the fact that $y_{F,T}^{T_{F}}$ is not feasible (meaning that it violates the optimality of the lower-level); hence, it iws itahb lteh teo  daiscchiuesvsieo an  bine ttSeer cutipopne r2-.l2e,v ewl itohb jtehcet ivvael.u Ien  oaf $\\gamma$ ithiaovni,n tgh ea nb eihmapvaicotr  oonb stehrev seud bion pitiii)m iasl ictoy nosfi $y_{F,t}^{T_{F}}$ at the lower-level. Specifically, higher values of $\\gamma$ push $y_{F,t}^{T_{F}}$ closer to $y_{g,t}^{T_{g}}$ and, hence, decrease the lower-level optimality gap. Finally, we must note that for the solid lines (associated with $y_{g,t}^{T_{g}},$ ), it holds that f(xt, ygT,gt) = f(xt, y\u2217(xt)). This implies that if we need a solution that is feasible at the lower-level, then better objective values are associated with higher values of $\\gamma$ . ", "page_idx": 35}, {"type": "text", "text": "F.4 Numerical results for the Seville network ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "In this section, we demonstrate the practical use of BLOCC in a real transportation network design problem. Specifically, we address the design of a metro network in the city of Seville, which has approximately one million inhabitants and is located in the south of Spain, with the bus system as its competitor. The data for the demand, number of stations, and locations have been taken from [21]. Information about the construction costs, capacity, and travel time has been obtained from Spanish rapid transit operators (see references in [10, 56] for full details). The city authorities considered $|{\\cal S}|=24$ potential station locations. Regarding the links, the following assumptions are made: ", "page_idx": 35}, {"type": "image", "img_path": "uZi7H5Ac0X/tmp/a4eabd124a37d24f2fa54aa10fe0f4c7fceba298ebce152f18186a93a2022087.jpg", "img_caption": ["Figure 11: Topology of the Seville network. "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "A1) The link between nodes $(i,j)\\in\\mathcal{S}\\times\\mathcal{S}$ only exists if node $j$ is one of the three closest neighbors to $i$ , or vice versa. The distance here is measured in terms of travel time. This assumption limits the number of lines in any station to be at most 3, which is a very mild assumption for a network with 24 stations. ", "page_idx": 35}, {"type": "text", "text": "A2) The link between nodes $(i,j)\\in S\\times S$ only exists if the travel time $t_{i j}$ is less than 7 minutes. This is also a mild assumption, since it enables all the stations to be connected, including those that are further away from the city center (the airport and the university campus [21]). ", "page_idx": 35}, {"type": "text", "text": "Under these two conditions, the set $\\boldsymbol{\\mathcal{A}}$ of potential links contains $|{\\mathcal{A}}|=88$ links. The sets of links and stations, along with their actual locations, are shown in Figure 11. Finally, we consider all possible markets between nodes, so $|K|=24\\times23=552$ . ", "page_idx": 35}, {"type": "text", "text": "Following the narrative in Sections F.2 and F.3, Figure 10 presents the evolution of the upper-level objective function with time for three values of the parameter $\\gamma\\in\\{2,3,4\\}$ . The stepsize value has been set to $\\eta=1.6\\times10^{-4}$ , and two different initializations have been considered. Regarding the behavior of the algorithm with respect to the value of \u03b3 and the particular output chosen (yTT,gg vs. $y_{T,F}^{T_{F}})$ , the findings are very similar to those in Figure 9. Namely, smaller gaps are found for larger values of $\\gamma$ , and if feasibility at the lower-level (i.e., consistency with the user preference level) must be preserved, better objective values are achieved for larger values of $\\gamma$ . ", "page_idx": 35}, {"type": "text", "text": "The most important observation, however, is related to the running time. Specifically, Figure 10 reveals that, for this real-world scenario, our BLOCC algorithm converges in 10-20 hours. While this is more than 1,000 times larger than the convergence interval for the 3-node network, the number of variables and constraints here is 100 times larger. More importantly, in the context of network transportation design, optimization times of 100 hours are widely accepted even for single-level formulations. Overall, we believe that the numerical results demonstrate that the BLOCC algorithm proposed in this work can solve problems with a large number of variables and CCs, which can have practical value in real-world applications, such as the one studied in this section. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "G Sensitivity Analysis ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Regarding the selection and impact of hyper-parameters on the performance of BLOCC, we conducted an ablation study on various values of the two critical parameters, $\\gamma$ and $\\eta$ , and measured their effects on the optimal value and computational time. In this section, we present the sensitivity analysis on the toy example that we introduced in Section 4.1 and the 3-node network example for the transportation network planning problem in Section 4.3. ", "page_idx": 36}, {"type": "text", "text": "G.1 Sensitivity analysis for the toy example ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "We present in Table 5 the results of lower-level optimality $\\|y_{g}^{*}(x_{T})-y_{F,T}\\|$ using different $\\gamma$ and $\\eta$ to conduct BLOCC (Algorithm 1) on the toy example in Section 4.1. ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\frac{\\left\\|y_{g}^{*}\\left(x_{T}\\right)-y_{F}\\right\\|}{\\left\\|\\begin{array}{l l}{\\phantom{-}\\frac{\\eta}{0.001}}&{\\left\\|\\phantom{-}\\frac{\\eta}{0.00}}\\\\ {\\phantom{-}\\frac{\\eta}{0.00}\\ \\right\\|\\phantom{-}\\left(0.0.042\\right)}&{0.035\\pm0.076}\\\\ {\\phantom{-}\\frac{0.1}{0.01}\\ \\left[\\phantom{-}\\frac{0.00}{(2.02)}\\phantom{-}\\frac{0.1}{0.02}\\phantom{-}\\frac{\\eta}{0.041}\\right]}&{0.027\\pm0.064}\\end{array}\\right\\|\\phantom{-}\\frac{\\gamma=1.0}{(1.96)\\pm0.00}}\\\\ &{}&{\\frac{\\left\\|\\begin{array}{l l}{0.00}&{\\left\\|\\phantom{-}\\frac{\\eta}{0.1}\\right\\|\\phantom{-}{0.00}}\\\\ {\\phantom{-}\\frac{0.1}{0.1}\\left[\\phantom{-}\\frac{0.00}{(2.02)}\\pm0.01\\right]}&{\\left(1.96\\pm0.01\\right)}\\\\ {\\phantom{-}\\frac{0.1}{0.1}\\left[\\phantom{-}\\frac{0.00}{(2.02)}\\pm0.01\\right]}&{\\left(1.075\\pm0.027\\right)\\phantom{-}(1.211\\pm0.03)}\\\\ {\\phantom{-}\\frac{0.1}{0.1}\\left[\\phantom{-}\\frac{0.00}{(2.02)}\\phantom{-}\\frac{0.1}{0.02}\\phantom{-}\\frac{0.1}{0.02}\\right]}&{(1.275\\pm0.027)\\phantom{-}\\frac{0.03}{(1.275\\pm0.03)}}\\\\ {\\phantom{-}\\frac{1.00}{1.00}\\left[\\phantom{-}\\frac{0.00}{(2.118\\pm0.019)}\\phantom{-}\\frac{0.02}{(1.475\\pm0.02)}\\phantom{-}\\frac{0.20}{(1.277\\pm0.03)}\\phantom{-}\\frac{0.00}{(1.845\\pm0.00)}\\right.}\\\\ &{}&{\\left.\\frac{00}{(1.18\\pm0.019)}\\left[\\phantom{-}\\frac{0.00}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Table 5: Sensitivity analysis for the hyperparameters in Section 4.1. Top line in each cell represents the optimality gap $\\|y_{g}^{*}(x_{T})-y_{F,T}\\|$ , while the bottom line represents the time required for the algorithm to converge. Both the mean and the standard deviation are for 40 simulations. ", "page_idx": 36}, {"type": "text", "text": "From the table, we can draw the following empirical observations: ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "O1) Larger values of $\\gamma$ bring the upper-level objectives closer to optimal. This is consistent with Theorem 1 which illustrated that larger $\\gamma$ improves the accuracy of the lower-level optimality. Since the obtained solution $y_{F,T}$ is closer to $y_{g}^{*}(x_{T})$ for larger $\\gamma$ values, the distance between the $f(x_{T},y_{F,T})$ and $f(x_{T},y_{g}^{*}(x_{T}))$ will be closer as well. ", "page_idx": 36}, {"type": "text", "text": "O2) For a sufficiently small fixed $\\eta,$ larger $\\gamma$ lead to faster convergence. This implies that smaller \u03b7 \u2264 lF1,1 choice due to larger \u03b3 will not significantly dampen the convergence time. This is because a large value of $\\gamma$ increases $l_{F,1}$ , sharpening the function $F_{\\gamma}(x)$ and its gradient will be larger for most points. Thus, the gradient update $\\eta\\nabla F_{\\gamma}(x_{t})$ will not be very small and thus won\u2019t make the convergence slower. ", "page_idx": 36}, {"type": "text", "text": "G.2 Sensitivity Analysis for the 3-node network ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "We present in Table 6 the results of the upper-level objective value $f(x,y)$ in network planning problem for a 3-node network, whose detailed framework is introduced in Section F. ", "page_idx": 36}, {"type": "table", "img_path": "uZi7H5Ac0X/tmp/3479e957e857e7b2c4b1d3c147cbd88f43b5ae1ab96a01a0eb1f806bde7d36bf.jpg", "table_caption": [], "table_footnote": ["Table 6: Sensitivity analysis for hyperparameters $\\eta$ and $\\gamma$ in the experiment of the 3-node network transportation design described in Section 4.3 and Appendix F. Top line in each cell represents the upper-level objective value $f(x_{T},y_{g,T})$ , while the bottom line represents convergence time. Both the mean and the standard deviation of 10 simulations are provided. "], "page_idx": 37}, {"type": "text", "text": "We know from Section F that the goal is to minimize $f(x,y)$ , and negative values of $f(x,y)$ are expected. Therefore, $f(x_{T},y_{T})=0$ in the table indicates a failure to converge and we can see that ", "page_idx": 37}, {"type": "text", "text": "O3) Smaller \u03b7 is needed to satisfy \u03b7 \u2264 lF1,1 to ensure convergence if $\\gamma$ is chosen larger. The algorithm tends to converge for different $\\gamma$ values when $\\eta$ is small enough. This is because the Lipschitz smoothness constant $l_{F,1}$ of $F_{\\gamma}(x)$ increases with $\\gamma$ (Lemma 3), and the BLOCC algorithm is guaranteed to converge if $\\begin{array}{r}{\\eta\\leq\\frac{1}{l_{F,1}}}\\end{array}$ (Theorem 2). ", "page_idx": 37}, {"type": "text", "text": "In summary, the experimental results align with the theoretical analysis, demonstrating that the penalty constant $\\gamma$ is robust and can be effectively set around 10, and the stepsize should be adjusted to ensure smooth and monotonic convergence. ", "page_idx": 37}, {"type": "text", "text": "H Analysis of the Computational Complexity ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "H.1 Complexity comparison ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "We present in the following the computational complexity of our BLOCC algorithm in comparison with LV-HBA [75] and GAM [72] as baselines. ", "page_idx": 37}, {"type": "table", "img_path": "uZi7H5Ac0X/tmp/d340b856389680be0052bd8e97eae8d84075403524caa870c74099d63fd963cb.jpg", "table_caption": [], "table_footnote": ["Table 7: Complexity comparison of our work with LV-HBA [75] and GAM [72]. "], "page_idx": 37}, {"type": "text", "text": "The iteration costs are detailed in the earlier sections, and in [75] and [72]. Hence, we discuss next the per-iteration cost. In the following discussion, we assume ", "page_idx": 37}, {"type": "text", "text": "A1) the complexity of calculating a function is proportional to the dimension of the inputs. For example, The complexity of finding $\\bar{\\nabla_{x}}\\bar{g}(x,\\bar{y}),\\bar{\\nabla_{x}}f(x,y)$ are $\\mathcal{O}(d_{x})$ , and the one for $g^{c}(x,y)\\rangle$ is $\\mathcal{O}(d_{x}d_{y})$ .   \nA2) Projection cost on $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ is respectively no more than ${\\mathcal O}(d_{x}^{2})$ and $O(d_{y}^{2})$ . As discussed in the introduction, $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ are assumed to be easy-to-project domains, i.e. projection can be done using a projection matrix or a simple formula, and the projection costs are no more than ${\\mathcal O}(d_{x}^{2})$ and $\\bar{O}(\\bar{d}_{y}^{2})$ , respectively. ", "page_idx": 37}, {"type": "text", "text": "In this way, BLOCC is a first-order method with gradient calculation costs of $\\mathcal{O}(d_{x}d_{c})$ and $\\mathcal{O}(d_{y}d_{c})$ , and the projection cost of ${\\mathcal O}(d_{x}^{2})$ and $O(d_{y}^{2})$ . We present the detailed analysis of achieving the computational cost in the table in the following Section H.2. ", "page_idx": 37}, {"type": "text", "text": "In the SVM model training and network planning experiments in Section 4, projections are simpler (e.g., truncation), resulting in $\\mathcal{O}(d_{x})$ and $\\mathcal{O}(d_{y})$ costs. In this scenario, for the general setting (Theorem 3), the complexity is $\\tilde{\\mathcal{O}}(\\epsilon^{-1.5}d_{c}d_{x}\\!+\\!\\epsilon^{-2.5}d_{c}d_{y})$ ; and for the $g^{c}$ affine in $y$ setting (Theorem 4), it is $\\tilde{\\mathcal{O}}(\\epsilon^{-1.5}d_{c}(d_{x}+d_{y}))$ . Therefore, our BLOCC is especially robust to large-scale problems. ", "page_idx": 37}, {"type": "text", "text": "LV-HBA [75], also a first-order method, has similar gradient calculation costs. However, its projection onto $\\{\\mathcal{X}\\times\\mathcal{Y}:g^{c}(x,y)\\leq0\\}$ is expensive, with a complexity of $\\mathcal{O}((d_{x}+d_{y})^{3.5})$ of using interior point method to find the projected point [38], and evaluating $g^{c}(x,y)\\,\\leq\\,0$ adds $\\bar{\\mathcal{O}}(d_{x}d_{y}\\bar{d}_{c})$ as it requires $d_{c}$ inequality judgment on functions taking input dimension $d_{x},d_{y}$ . ", "page_idx": 38}, {"type": "text", "text": "GAM [72] lacks an explicit algorithm for lower-level optimality and Lagrange multipliers. Even if we omit this, calculating $\\nabla_{y y}^{2}g$ and its inverse incurs a cost of $\\mathcal{O}(d_{y}^{3})$ . Additionally, it has cost $\\mathcal{O}(d_{x}^{2}+d_{c}(d_{x}+d_{y}))$ for projection onto $\\mathcal{X}$ and calculating gradients. ", "page_idx": 38}, {"type": "text", "text": "We can see that BLOCC stands out with the lowest computational cost in both iterational and overall complexity thanks to its first-order and joint-projection-free nature. Consequently, BLOCC is particularly well-suited for large-scale applications with high-dimensional parameters. ", "page_idx": 38}, {"type": "text", "text": "H.2 Complexity analysis of BLOCC ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "At each iteration $t$ , our proposed BLOCC algorithm in Algorithm 1 involves: ", "page_idx": 38}, {"type": "text", "text": "Step 1: Solve two max-min problems. ", "page_idx": 38}, {"type": "text", "text": "Step 2: Calculate $g_{F,t}$ in (13) and update $x_{t+1}=\\operatorname{Proj}_{\\mathcal{X}}\\left(x_{t}-\\eta g_{F,t}\\right)$ . ", "page_idx": 38}, {"type": "text", "text": "where Step 1 can be achieved by our proposed max-min solver in Algorithm 2. In the following, we provide analysis for using the accelerated version of the Algorithm 2 in the general case (Theorem 3) and the single-loop version for the special case (Theorem 4) as discussed in Section 3.3. ", "page_idx": 38}, {"type": "text", "text": "In the general case, we use the accelerated version of Algorithm 2 to solve both (7) and (5). For solving (7), at each inner loop iteration $t$ , line 3 of the algorithm is of computational cost $\\mathcal{O}(d_{c})$ . The update of $y$ in line 4-6 involves calculating $\\nabla_{y}g(x,y)$ with a cost of $\\bar{O}(\\bar{d_{y}})$ , finding $\\langle\\mu,\\nabla_{y}g^{c}(x,y)\\rangle$ for fixed $x$ of $\\mathcal{O}(d_{y}d_{c})$ following assumption A1), and the projection $\\mathrm{Proj}_{\\mathcal{Y}}$ of complexity $O(d_{y}^{2})$ according to A2). Moreover, $y$ converges linearly as $L(\\mu,y)$ is strongly convex in $y$ (Lemma 7), this inner update for $y$ gives an iteration complexity of $O(\\ln(\\epsilon^{-1}))$ . Therefore, the cost of the update for $y$ totals up to $\\mathcal{O}(\\ln(\\epsilon^{-1})(d_{y}d_{c}+d_{y}^{2}))$ . The update of $\\mu$ in line 7 involves $\\mathcal{O}(d_{y}d_{c})$ for calculating $\\nabla_{\\mu}L(\\mu_{t+1/2},y_{t+1})=g^{c}(x,y_{t+1})$ with $x$ being fixed, and $\\mathcal{O}(d_{c})$ for projection onto $\\mathbb{R}_{+}^{d_{c}}$ . ", "page_idx": 38}, {"type": "text", "text": "Moreover, to achieve target accuracy $\\epsilon$ on the metric $\\begin{array}{r}{\\frac{1}{T}\\sum_{t=0}^{T}\\|G_{\\eta}(x_{t})\\|^{2}\\,\\le\\,\\epsilon}\\end{array}$ , with $\\gamma=\\mathcal{O}(\\epsilon^{-1})$ using the BLOCC algorithm, the iteration complexity of the max-min solver is $O(\\epsilon^{-1})$ according to Theorem 3. Therefore, the complexity for solving (7) in the general case using the accelerated version of Algorithm 2 (Theorem 3) is ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathcal{O}(\\epsilon^{-1}(\\ln(\\epsilon^{-1})(d_{y}d_{c}+d_{y}^{2})+d_{c}d_{y}+d_{c}))=\\tilde{\\mathcal{O}}(\\epsilon^{-1}(d_{y}d_{c}+d_{y}^{2})).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Solving (5) is of the same order as the only difference is in calculating $\\nabla_{y}f(x,y)$ . In this way, we can conclude that Step 1 is at a cost of $\\tilde{\\mathcal{O}}(\\epsilon^{-1}(d_{y}d_{c}+d_{y}^{2}))$ in the general case. ", "page_idx": 38}, {"type": "text", "text": "In the special case (Theorem 4), the non-accelerated (single-loop) version of Algorithm 2 involves $y_{t+1}\\,=\\,\\mathrm{Proj}_{\\mathcal{V}}(y_{t}\\,-\\,\\eta_{1}\\nabla_{y}L(\\mu_{t},y_{t}))$ with complexity $\\bar{\\mathcal{O}}(d_{y}^{2}+\\bar{d}_{y}d_{c})$ , and $\\mu_{t+1}\\,=\\,\\mathrm{Proj}_{\\mathbb{R}_{+}^{d_{c}}}(\\mu_{t}\\,+$ $\\eta_{2}\\nabla_{\\mu}L(\\mu_{t},y_{t+1}))$ with complexity $\\mathcal{O}(d_{c}+d_{y}d_{c})$ following a similar analysis as in the general case. Moreover, the algorithm converges linearly in the special case of $g^{c}$ being affine in $y$ and the computational complexity is $\\mathcal{O}(\\ln\\!\\left(\\epsilon^{-1}\\right))$ , according to Theorem 4. Therefore, the complexity for the max-min Step 1 in the special case is ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathcal{O}(\\ln(\\epsilon^{-1})(d_{y}d_{c}+d_{y}^{2}+d_{c}))=\\tilde{\\mathcal{O}}(d_{y}d_{c}+d_{y}^{2}+d_{c}).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Step 2 involves calculating $\\nabla_{x}g(x,y)$ with with a fixed $y$ , and $\\langle\\mu,\\nabla_{x}g^{c}(x,y)\\rangle$ for fixed $\\mu,y$ , which are of complexity $\\mathcal{O}(d_{x})$ and $\\mathcal{O}(d_{x}d_{c})$ respectively, and conducting a projection on $\\mathcal{X}$ of cost ${\\mathcal O}(d_{x}^{2})$ according to assumption A2). ", "page_idx": 38}, {"type": "text", "text": "In this way, the computational complexity of Step 2 is ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{O}(d_{x}d_{c}+d_{x}^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We can therefore conclude the complexity of BLOCC in Table 7. ", "page_idx": 38}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "1. Claims ", "page_idx": 39}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: The abstract and introduction clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. ", "page_idx": 39}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: yes, we have discussed in the conclusions. ", "page_idx": 39}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: All have been clearly listed. ", "page_idx": 39}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: The code used to run the experiments is published on GitHub. ", "page_idx": 39}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: The data flies considered and the code used to run the experiments are published on GitHub. ", "page_idx": 39}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: All the necessary details to run the experiments are available in the code. ", "page_idx": 39}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: The figures in the paper include confidence intervals representing the standard deviation for different realizations of the experiments. Also, the tables include the standard deviation of the simulations. ", "page_idx": 39}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: The paper is theory paper, and does not include much computation-heavy experiments. ", "page_idx": 40}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \nAnswer: [Yes]   \nJustification: We follow the NeurIPS Code of Ethics. ", "page_idx": 40}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: The work is foundational research, and there is no societal impact of the work performed. ", "page_idx": 40}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] Justification: The paper poses no such risks. ", "page_idx": 40}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 40}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: the paper does not release new assets. ", "page_idx": 40}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 40}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] .   \nJustification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 40}]