[{"figure_path": "bCL9U2X9Jg/tables/tables_6_1.jpg", "caption": "Table 1: Retrieval performances (full evaluation) on Fashion-Gen. (*) denotes results from our implementation. As the code and pre-trained model of FILIP are not released yet, we implemented the model and initialize it with pre-trained CLIP weights.", "description": "This table presents the results of cross-modal retrieval experiments on the Fashion-Gen dataset.  It compares the performance of the proposed model, E2, against several existing state-of-the-art methods. The evaluation metrics used are Rank@1, Rank@5, Rank@10, and SumR (sum of Rank@1, Rank@5, and Rank@10).  The table indicates that E2 shows significant improvements in retrieval performance compared to existing models, particularly in retrieving relevant items.", "section": "4.1 Comparison with State-of-the-Art"}, {"figure_path": "bCL9U2X9Jg/tables/tables_7_1.jpg", "caption": "Table 7: Retrieval performances (sample-100 evaluation) on AmazonFashion. (*) denotes results from our implementation.", "description": "This table presents the results of a sample-100 evaluation on the AmazonFashion dataset for cross-modal retrieval.  The evaluation metrics used are R@1, R@5, R@10 for both Image-to-Text and Text-to-Image retrieval tasks.  The table compares the performance of three different models: FILIP-FT, CLIP-FT, and the authors' proposed model, E2. The asterisk (*) indicates that results for FILIP-FT were obtained through the authors' implementation of the model since the original code and pre-trained model were not publicly released. The SumR metric represents the sum of Rank@1, Rank@5, and Rank@10 for both tasks.  The table highlights the superior performance of the E2 model compared to the other two models.", "section": "4.1 Comparison with State-of-the-Art"}, {"figure_path": "bCL9U2X9Jg/tables/tables_7_2.jpg", "caption": "Table 4: Zero-shot image captioning results on FashionGen with BLEU@4 (B), CIDEr (C), METEOR (M), ROUGE (R).", "description": "This table presents the results of zero-shot image captioning experiments conducted on the FashionGen dataset.  The model used is CapDec, with and without the enhancement of E2. DeCap is also evaluated in the same way.  The metrics used to evaluate the performance are BLEU@4, CIDEr, METEOR, and ROUGE. The results show that integrating E2 significantly improves the performance of both CapDec and DeCap on zero-shot image captioning, indicating the effectiveness of E2 in generating more accurate and detailed captions.", "section": "4.1 Comparison with State-of-the-Art"}, {"figure_path": "bCL9U2X9Jg/tables/tables_8_1.jpg", "caption": "Table 8: Ablation study (selection tokens) on FashionGen.", "description": "This table presents the results of an ablation study conducted on the FashionGen dataset to evaluate the impact of different groups of selection tokens on the model's performance.  The study removes one group of selection tokens at a time (corresponding to one of the tag entities: Composition, Sub-category, Brand, or Season), and measures the impact on retrieval performance using R@1, R@5, R@10, and SumR metrics for both image-to-text and text-to-image retrieval tasks.  The results reveal which tag entities contribute the most and the least to the model's overall performance. ", "section": "F Ablation"}, {"figure_path": "bCL9U2X9Jg/tables/tables_8_2.jpg", "caption": "Table 9: Retrieval performances (full-candidate) on FashionGen.", "description": "This table presents the results of a full evaluation of the proposed Easy Regional Contrastive Learning of Expressive Fashion Representations (E2) model and several ablation studies on the FashionGen dataset.  The metrics used are Recall@1, Recall@5, and Recall@10 for both Image-to-Text and Text-to-Image retrieval tasks.  The ablation studies remove different components of the E2 model (fusion blocks, selection tokens, regional contrastive loss) to understand their individual contributions. Additionally, a comparison with the model using the FILIP backbone is shown.", "section": "4.2 Ablation and Further Analysis"}, {"figure_path": "bCL9U2X9Jg/tables/tables_13_1.jpg", "caption": "Table 5: Influence of batch size. We evaluate the performance of E\u00b2 and CLIP-FT under different batch size settings on FashionGen.", "description": "This table presents the results of experiments conducted to evaluate the impact of different batch sizes on the performance of both the proposed E2 model and the fine-tuned CLIP-FT model. The evaluation is performed on the FashionGen dataset, and the metrics used are R@1, R@5, R@10 for both image-to-text and text-to-image retrieval tasks, along with the SumR metric, which is the sum of the three rank-based metrics. The table shows that E2 consistently outperforms CLIP-FT across various batch sizes, with the performance difference becoming more pronounced at smaller batch sizes.", "section": "C Parameter Sensitivity"}, {"figure_path": "bCL9U2X9Jg/tables/tables_15_1.jpg", "caption": "Table 6: Data statistics of AmazonFashion and FashionGen. We show the number of image-text pairs, brand count, product count and the average length of the image descriptions.", "description": "This table presents a comparison of two datasets used in the paper: FashionGen and AmazonFashion.  For each dataset, it shows the total number of image-text pairs, the number of unique brands represented, the number of unique products, and the average length of the product descriptions.  This allows the reader to understand the scale and characteristics of each dataset and how they differ, impacting the model's training and evaluation.", "section": "D Datasets"}, {"figure_path": "bCL9U2X9Jg/tables/tables_17_1.jpg", "caption": "Table 7: Retrieval performances (sample-100 evaluation) on AmazonFashion. (*) denotes results from our implementation.", "description": "This table presents the performance of three different models on the AmazonFashion dataset using a sample-100 evaluation method.  The models compared are FILIP-FT, CLIP-FT, and the authors' proposed model, E2. The evaluation metrics used are Recall@1 (R@1), Recall@5 (R@5), Recall@10 (R@10), and the sum of these three metrics (SumR).  The asterisk (*) indicates that the FILIP-FT results were obtained by the authors' implementation of the model because the original code and pre-trained model were not publicly released.", "section": "4.1 Comparison with State-of-the-Art"}, {"figure_path": "bCL9U2X9Jg/tables/tables_18_1.jpg", "caption": "Table 8: Ablation study (selection tokens) on FashionGen.", "description": "This table presents the results of an ablation study conducted on the FashionGen dataset.  The study examines the impact of removing each group of selection tokens (associated with a specific tag entity: Composition, Sub-category, Brand, Season) from the model.  It shows the performance (R@1, R@5, R@10 for Image-to-Text and Text-to-Image retrieval, as well as the SumR) of the model with and without each tag entity's selection tokens. The results highlight the different contributions of each tag entity's selection tokens to the model's overall performance. ", "section": "F Ablation"}, {"figure_path": "bCL9U2X9Jg/tables/tables_19_1.jpg", "caption": "Table 9: Retrieval performances (full-candidate) on FashionGen.", "description": "This table presents the results of a full-candidate evaluation on the FashionGen dataset for cross-modal retrieval. It compares the performance of three models: EI-CLIP [27], EI-CLIP [27] without E (Easy Regional Contrastive Learning), and E2 (the proposed model).  The evaluation metrics include Recall@1 (R@1), Recall@5 (R@5), Recall@10 (R@10), and the sum of these three metrics (SumR). The table shows that E2 outperforms both versions of EI-CLIP, demonstrating its effectiveness in the task.", "section": "H Discussion"}, {"figure_path": "bCL9U2X9Jg/tables/tables_20_1.jpg", "caption": "Table 10: Results on FashionGen with full-candidate evaluation. C: Composition, S: Season, B: Brand, SC: Sub-category. Numbers are the quantity of corresponding selection tokens. Total number of learnable parameters in vision encoder: Params (Total). We also list parameters of selection tokens (part of the total Params): Params (Token).", "description": "This table presents ablation study results on the FashionGen dataset, focusing on the impact of varying numbers of selection tokens on the model's performance. It compares different configurations where the number of selection tokens for each category (Composition, Season, Brand, Sub-category) is adjusted.  The table shows the total number of parameters in the model, the number of parameters specifically related to the selection tokens, and the final SumR performance metric (a combination of R@1, R@5, and R@10).  This helps analyze the impact of the selection tokens on model parameter efficiency and performance.", "section": "I Ablation"}]