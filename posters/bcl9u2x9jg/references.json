{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces CLIP, a foundational vision-language model that the current work builds upon and improves for the fashion domain."}, {"fullname_first_author": "Robin Strudel", "paper_title": "Segmenter: Transformer for semantic segmentation", "publication_date": "2021-10-01", "reason": "This paper introduces the Segmenter architecture, which is related to the architecture used in the current work and provides relevant background on image segmentation techniques."}, {"fullname_first_author": "Haoyu Ma", "paper_title": "EI-CLIP: Entity-aware interventional contrastive learning for e-commerce cross-modal retrieval", "publication_date": "2022-06-01", "reason": "This paper is highly relevant as it also focuses on improving CLIP for fashion-related tasks, allowing for a direct comparison of approaches."}, {"fullname_first_author": "Wei Li", "paper_title": "DeCap: Decoding CLIP latents for zero-shot captioning via text-only training", "publication_date": "2023-05-01", "reason": "This paper is a relevant work that uses CLIP for zero-shot image captioning, a task also addressed in the current work, enabling comparison and potential improvements."}, {"fullname_first_author": "Shyamgopal Karthik", "paper_title": "Vision-by-language for training-free compositional image retrieval", "publication_date": "2023-10-01", "reason": "This paper addresses zero-shot text-guided image retrieval, a related task to the main focus, providing context and a benchmark for comparison."}]}