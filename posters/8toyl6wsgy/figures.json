[{"figure_path": "8tOYl6WsGY/figures/figures_1_1.jpg", "caption": "Figure 1: (a) Existing training-required TTA methods utilize self-supervised objective like entropy minimization for better generalization. (b) Existing training-free TTA methods perform feature retrieval on the historical samples to adjust the model prediction. (c) Performance comparison on the Out-of-Distribution benchmark and Cross-Datasets benchmark.", "description": "This figure shows a comparison of training-required and training-free test-time adaptation (TTA) methods.  Panel (a) illustrates the typical workflow of training-required TTA, where a self-supervised objective (entropy minimization) is used to improve model generalization on downstream tasks. Panel (b) shows the typical workflow of training-free TTA, where feature retrieval from historical samples is used to adjust model predictions.  Panel (c) provides a comparison of the performance of various TTA methods, including BoostAdapter (the proposed method), on two benchmark datasets: Out-of-Distribution and Cross-Datasets.", "section": "1 Introduction"}, {"figure_path": "8tOYl6WsGY/figures/figures_3_1.jpg", "caption": "Figure 2: Connection between cross-entropy optimization and cache classifier over well-clustered samples with a frozen feature encoder. With optimization of cross-entropy, samples will pull the classifier weights closer of the same class while pushing them away from different class weights. Since the feature space is well-clustered, the classifier weights will ultimately converge near the feature center of the samples. Finally, the optimal classifier achieved through cross-entropy minimization will exhibit similar behavior with the cache classifier.", "description": "This figure illustrates the connection between cross-entropy optimization and cache classifier methods when dealing with well-clustered data points.  It shows how, during cross-entropy optimization, the classifier weights are adjusted to pull closer samples of the same class and push apart those of different classes. Because the features are clustered, these weights converge towards the centroid of each feature cluster.  The cache classifier, by contrast, directly utilizes the centroid as a basis for classification, leading to similar results.", "section": "3.2 A Closer Look at Entropy-based and Cache-based Methods"}, {"figure_path": "8tOYl6WsGY/figures/figures_4_1.jpg", "caption": "Figure 3: Overall architecture of BoostAdapter. BoostAdapter leverages knowledge from the target domain and employs self-bootstrapping with historical and boosting samples in the boosting cache, respectively.", "description": "This figure illustrates the architecture of the BoostAdapter model.  The model takes an image as input and augments it to create multiple versions of the image. These augmented images are passed through an image encoder to produce image embeddings.  The image embeddings are used for feature retrieval from two caches: a historical cache (containing instance-agnostic historical samples from the test data stream), and a boosting cache (containing instance-aware boosting samples created using a self-bootstrapping method from the test sample itself).  The outputs from both caches and the original CLIP logits are combined to produce the final logits, which represent the model's prediction.  A key component is the filtering step that uses entropy to select high-quality boosting samples from the augmented image versions.", "section": "3.3 Boosting your Training-free Adapters"}, {"figure_path": "8tOYl6WsGY/figures/figures_7_1.jpg", "caption": "Figure 4: Ablation studies of (a) number of augmented views to generate boosting samples (b) different adaptation methods and (c) total shot capacity of the cache.", "description": "This figure presents the results of ablation studies conducted to analyze the impact of different factors on the BoostAdapter model.  Panel (a) shows how the number of augmented views used to create boosting samples affects the model's performance. Panel (b) compares the performance of BoostAdapter using only historical samples, only boosting samples, and both historical and boosting samples, illustrating the contribution of each component. Panel (c) demonstrates the effect of varying the total shot capacity (the number of samples stored in the cache) on the model's accuracy. Each panel provides insights into the optimal configuration of the BoostAdapter for improved performance.", "section": "4.4 Ablation Study"}, {"figure_path": "8tOYl6WsGY/figures/figures_9_1.jpg", "caption": "Figure 5: Qualitative results. The model predictions are provided below the images. Boosting samples with low entropy improves information extraction from the test sample and helps the model to distinguish better.", "description": "This figure shows four examples of qualitative results using the BoostAdapter method. Each example displays a test image and its corresponding boosting sample. The boosting samples were obtained through regional bootstrapping and filtering based on entropy. In each pair of images, the text below indicates the model's predictions for both the test image and the boosting sample.  The goal of the figure is to visually demonstrate how BoostAdapter uses boosting samples to improve prediction accuracy, especially on finer details or distinguishing features.", "section": "5 Conclusions"}, {"figure_path": "8tOYl6WsGY/figures/figures_24_1.jpg", "caption": "Figure 3: Overall architecture of BoostAdapter. BoostAdapter leverages knowledge from the target domain and employs self-bootstrapping with historical and boosting samples in the boosting cache, respectively.", "description": "BoostAdapter's architecture is shown, highlighting its use of two types of samples: historical samples and boosting samples.  Historical samples are filtered from the test data stream and are used for feature retrieval. Boosting samples are created via regional bootstrapping from the test sample itself using augmentation and entropy filtering to select high-quality samples. These samples are stored in a boosting cache and utilized along with historical samples in a key-value memory for improved feature retrieval and prediction.", "section": "3.3 Boosting your Training-free Adapters"}]