[{"figure_path": "8tOYl6WsGY/tables/tables_6_1.jpg", "caption": "Table 1: Full results on the OOD benchmark with ViT-B/16 backbone. We report top-1 accuracy and \"Average\" is calculated by taking the mean accuracy across all four OOD datasets.", "description": "This table presents the results of the proposed BoostAdapter method and several baseline methods on the Out-of-Distribution (OOD) benchmark using the ViT-B/16 backbone.  The OOD benchmark consists of four datasets: ImageNet-V2, ImageNet-Sketch, ImageNet-A, and ImageNet-R. For each dataset and each method, the top-1 accuracy is reported, representing the percentage of correctly classified images.  An \"Average\" column provides the mean accuracy across all four datasets, offering a summarized performance comparison across different OOD settings.", "section": "4.2 Out-of-Distribution Generalization"}, {"figure_path": "8tOYl6WsGY/tables/tables_6_2.jpg", "caption": "Table 2: Full results on the Cross-Domain Benchmark with ViT-B/16 backbone. We report top-1 accuracy and \"Average\" is calculated by taking the mean accuracy across all ten datasets. The error bound is \u00b10.17.", "description": "This table presents the performance comparison of different vision-language models on ten cross-domain datasets using the ViT-B/16 backbone.  The models compared include CLIP, CLIP+TPT, CoOp, CoCoOp, Maple, Maple+TPT, DiffTPT, PromptAlign, TDA and BoostAdapter.  The \"Average\" column shows the mean accuracy across all ten datasets.  The error bound of \u00b10.17 indicates the uncertainty in the reported results.", "section": "4.2 Out-of-Distribution Generalization"}, {"figure_path": "8tOYl6WsGY/tables/tables_7_1.jpg", "caption": "Table 3: Ablation study on historical samples and boosting samples on the OOD benchmark with ViT-B/16 backbone. We report top-1 accuracy and the error bound is \u00b10.12.", "description": "This table presents the results of an ablation study conducted on the Out-of-Distribution (OOD) benchmark using the Vision Transformer (ViT-B/16) backbone.  The study compares the performance of three different approaches: using only historical samples, using only boosting samples, and using both (BoostAdapter). The top-1 accuracy and error bounds (\u00b10.12) are reported for each method across four OOD datasets: ImageNet-V2, ImageNet-Sketch, ImageNet-A, and ImageNet-R.  This table helps to demonstrate the individual and combined contributions of historical and boosting samples to the overall performance of the BoostAdapter model.", "section": "4.2 Out-of-Distribution Generalization"}, {"figure_path": "8tOYl6WsGY/tables/tables_7_2.jpg", "caption": "Table 4: Full results on the OOD benchmark with RN-50 backbone. We report top-1 accuracy and the error bound is \u00b10.06.", "description": "This table presents the results of the out-of-distribution (OOD) benchmark using a ResNet-50 backbone.  It compares the top-1 accuracy of various vision-language models on four ImageNet variants (ImageNet-V2, ImageNet-Sketch, ImageNet-A, ImageNet-R).  The error bound of \u00b10.06 indicates the uncertainty in the reported accuracies.  The table showcases the performance of the BoostAdapter in comparison to other state-of-the-art methods.", "section": "4.2 Out-of-Distribution Generalization"}, {"figure_path": "8tOYl6WsGY/tables/tables_8_1.jpg", "caption": "Table 2: Full results on the Cross-Domain Benchmark with ViT-B/16 backbone. We report top-1 accuracy and \"Average\" is calculated by taking the mean accuracy across all ten datasets. The error bound is \u00b10.17.", "description": "This table presents the results of the proposed BoostAdapter method and several baseline methods on a cross-domain benchmark using the ViT-B/16 backbone.  The benchmark consists of ten different datasets, evaluating the model's ability to generalize across various domains. Top-1 accuracy is reported for each dataset, along with the average accuracy across all datasets.  The error bound provides a measure of uncertainty in the reported results.", "section": "4.2 Out-of-Distribution Generalization"}, {"figure_path": "8tOYl6WsGY/tables/tables_8_2.jpg", "caption": "Table 6: Comparisons with baselines on ImageNet-C at severity level 5 regarding accuracy (%).", "description": "This table compares the performance of BoostAdapter with other baseline methods on ImageNet-C dataset at severity level 5.  ImageNet-C is a dataset that evaluates the robustness of image classifiers to various corruptions. The results show BoostAdapter's performance in handling different types of corruptions, indicating its superior generalization capability compared to existing methods.", "section": "4.5 Discussions"}, {"figure_path": "8tOYl6WsGY/tables/tables_9_1.jpg", "caption": "Table 7: Efficiency analysis. We evaluate different methods on a single NVIDIA 3090 24GB GPU and report the frames per second (fps) and memory cost (GB).", "description": "This table presents the efficiency comparison of different test-time adaptation methods.  It shows the inference speed in frames per second (fps) and the memory consumption in gigabytes (GB) for each method on a single NVIDIA 3090 24GB GPU.  The methods compared include CLIP, TPT, DiffTPT, TDA, and BoostAdapter.  The table also includes the augmentation strategy and number of views used for each method.  Finally, the table provides the OOD (Out-of-Distribution) and cross-domain results for context.", "section": "4 Experiments"}, {"figure_path": "8tOYl6WsGY/tables/tables_9_2.jpg", "caption": "Table 1: Full results on the OOD benchmark with ViT-B/16 backbone. We report top-1 accuracy and \"Average\" is calculated by taking the mean accuracy across all four OOD datasets.", "description": "This table presents the top-1 accuracy results for different vision-language models on four out-of-distribution (OOD) benchmark datasets: ImageNet-V2, ImageNet-Sketch, ImageNet-A, and ImageNet-R.  The models evaluated include CLIP, CLIP with Test-Time Prompt Tuning (TPT), CoOp, CoOp+TPT, CoCoOp, CoCoOp+TPT, Maple, Maple+TPT, PromptAlign, DiffTPT, TDA, and BoostAdapter.  The \"Average\" column represents the mean accuracy across the four datasets.  This table demonstrates the performance of BoostAdapter compared to state-of-the-art methods in handling OOD generalization.", "section": "4.2 Out-of-Distribution Generalization"}, {"figure_path": "8tOYl6WsGY/tables/tables_14_1.jpg", "caption": "Table 1: Full results on the OOD benchmark with ViT-B/16 backbone. We report top-1 accuracy and \"Average\" is calculated by taking the mean accuracy across all four OOD datasets.", "description": "This table presents the results of the out-of-distribution (OOD) benchmark experiments using the Vision Transformer (ViT-B/16) backbone.  The benchmark evaluates the robustness of several vision-language models to distribution shifts.  The table shows the top-1 accuracy achieved by various models on four ImageNet variants (ImageNet-V2, ImageNet-Sketch, ImageNet-A, and ImageNet-R). The \"Average\" column represents the average top-1 accuracy across these four datasets.  The table provides a quantitative comparison of the models' performance in handling out-of-distribution data.", "section": "4.2 Out-of-Distribution Generalization"}, {"figure_path": "8tOYl6WsGY/tables/tables_21_1.jpg", "caption": "Table 10: Independent cache for boosting samples on the OOD benchmark.", "description": "This table presents the ablation study on using independent cache for boosting samples in the OOD (Out-of-Distribution) benchmark. It compares the performance of BoostAdapter using an independent cache for boosting samples against the original BoostAdapter which uses a joint cache for both historical and boosting samples. The results are presented in terms of top-1 accuracy for four different datasets: Imagenet-V2, Imagenet-Sketch, Imagenet-A, and Imagenet-R, along with their average accuracy.", "section": "4.4 Ablation Study"}, {"figure_path": "8tOYl6WsGY/tables/tables_22_1.jpg", "caption": "Table 11: Independent cache for boosting sample on the Cross-Domain Benchmark.", "description": "This table presents the results of the Cross-Domain benchmark using an independent cache for boosting samples. It compares the performance of BoostAdapter using a joint cache (historical and boosting samples) with a setup using independent caches for historical and boosting samples.  The results are shown for various image classification tasks, and the 'Average' column shows the mean accuracy across all ten datasets.", "section": "4.4 Ablation Study"}, {"figure_path": "8tOYl6WsGY/tables/tables_22_2.jpg", "caption": "Table 1: Full results on the OOD benchmark with ViT-B/16 backbone. We report top-1 accuracy and \"Average\" is calculated by taking the mean accuracy across all four OOD datasets.", "description": "This table presents the top-1 accuracy results of different vision-language models on four Out-of-Distribution (OOD) benchmark datasets: ImageNet-V2, ImageNet-Sketch, ImageNet-A, and ImageNet-R.  The models evaluated include CLIP, CLIP+TPT, CoOp, CoOp+TPT, Co-CoOp, Co-CoOp+TPT, Maple, Maple+TPT, PromptAlign, DiffTPT, TDA, and the proposed BoostAdapter. The \"Average\" column shows the mean accuracy across all four datasets, providing a comparative overview of the models' performance in handling OOD scenarios.", "section": "4.2 Out-of-Distribution Generalization"}, {"figure_path": "8tOYl6WsGY/tables/tables_22_3.jpg", "caption": "Table 13: Comparison of different augmentations on the Cross-Domain Benchmark. Default settings are marked in gray", "description": "This table presents a comparison of different data augmentation techniques used for generating boosting samples in the BoostAdapter model.  The augmentations tested are Random Brightness, Random Auto Contrast, Random Rotate, Random Vertical Flip, and Random Horizontal Flip. The table shows the top-1 accuracy achieved by BoostAdapter on various datasets in the Cross-Domain benchmark using each augmentation strategy. The default augmentation setting (Random Horizontal Flip) is highlighted in gray for easy identification.", "section": "4.4 Ablation Study"}, {"figure_path": "8tOYl6WsGY/tables/tables_23_1.jpg", "caption": "Table 3: Ablation study on historical samples and boosting samples on the OOD benchmark with ViT-B/16 backbone. We report top-1 accuracy and the error bound is \u00b10.12.", "description": "This table presents the ablation study results on the Out-of-Distribution (OOD) benchmark using the Vision Transformer (ViT-B/16) backbone.  It compares the performance of using only historical samples, only boosting samples, and both in the BoostAdapter method. The top-1 accuracy and an error bound of \u00b10.12 are reported for each configuration. The results show how each component contributes to the overall performance of the BoostAdapter.", "section": "4.4 Ablation Study"}, {"figure_path": "8tOYl6WsGY/tables/tables_23_2.jpg", "caption": "Table 15: Results of different views on the OOD benchmark. Default settings are marked in gray", "description": "This table presents the results of an ablation study on the number of augmented views used to generate boosting samples in the BoostAdapter method.  The study was conducted on the Out-of-Distribution (OOD) benchmark, which consists of four ImageNet variants: ImageNet-V2, ImageNet-Sketch, ImageNet-A, and ImageNet-R.  The table shows that increasing the number of augmented views generally improves performance, with the best results obtained using 64 augmented views. The default settings are highlighted in gray.", "section": "4.2 Out-of-Distribution Generalization"}, {"figure_path": "8tOYl6WsGY/tables/tables_23_3.jpg", "caption": "Table 16: Results of different views on the Cross-Domain Benchmark. Default settings are marked in gray.", "description": "This table presents the results of an ablation study on the number of augmented views used to generate boosting samples within the BoostAdapter method.  The study evaluates the impact on the model's performance across ten different datasets in a cross-domain benchmark.  The \"Default settings\" refers to the configuration used in the main experiments of the paper.  The table shows how the top-1 accuracy varies as the number of views increases, providing insight into the tradeoff between computational cost and improved performance.", "section": "4.4 Ablation Study"}, {"figure_path": "8tOYl6WsGY/tables/tables_24_1.jpg", "caption": "Table 1: Full results on the OOD benchmark with ViT-B/16 backbone. We report top-1 accuracy and \"Average\" is calculated by taking the mean accuracy across all four OOD datasets.", "description": "This table presents the results of the out-of-distribution (OOD) benchmark experiments using the ViT-B/16 backbone. The benchmark evaluates the model's robustness to distribution shifts across four ImageNet variants: ImageNet-V2, ImageNet-Sketch, ImageNet-A, and ImageNet-R.  The table shows the top-1 accuracy for each dataset and the average accuracy across all four datasets for various vision-language models: CLIP, CLIP+TPT, CoOp, CoOp+TPT, Co-CoOp, Co-CoOp+TPT, Maple, Maple+TPT, PromptAlign, DiffTPT, TDA, and BoostAdapter.  This allows comparison of the performance of different models on challenging OOD data.", "section": "4.2 Out-of-Distribution Generalization"}, {"figure_path": "8tOYl6WsGY/tables/tables_24_2.jpg", "caption": "Table 2: Full results on the Cross-Domain Benchmark with ViT-B/16 backbone. We report top-1 accuracy and \"Average\" is calculated by taking the mean accuracy across all ten datasets. The error bound is \u00b10.17.", "description": "This table presents the results of the proposed BoostAdapter and other comparative methods on a cross-domain benchmark using the ViT-B/16 backbone.  Top-1 accuracy is reported for ten different datasets, along with an average accuracy across all datasets.  The error bound of \u00b10.17 indicates the uncertainty in the reported accuracy values.", "section": "4.2 Out-of-Distribution Generalization"}]