[{"type": "text", "text": "BoostAdapter: Improving Vision-Language Test-Time Adaptation via Regional Bootstrapping ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Taolin Zhang1 Jinpeng Wang 1 Hang Guo 1 Tao Dai\u22172 Bin Chen 3 Shu-tao Xia 1,4 ", "page_idx": 0}, {"type": "text", "text": "1 Tsinghua University 2 Shenzhen University 3 Harbin Institute of Technology 4 PengCheng Laboratory https://github.com/taolinzhang/BoostAdapter ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Adaptation of pretrained vision-language models such as CLIP to various downstream tasks have raised great interest in recent researches. Previous works have proposed a variety of test-time adaptation (TTA) methods to achieve strong generalization without any knowledge of the target domain. However, existing trainingrequired TTA approaches like TPT necessitate entropy minimization that involves large computational overhead, while training-free methods like TDA overlook the potential for information mining from the test samples themselves. In this paper, we break down the design of existing popular training-required and training-free TTA methods and bridge the gap between them within our framework. Specifically, we maintain a light-weight key-value memory for feature retrieval from instance-agnostic historical samples and instance-aware boosting samples. The historical samples are flitered from the testing data stream and serve to extract useful information from the target distribution, while the boosting samples are drawn from regional bootstrapping and capture the knowledge of the test sample itself. We theoretically justify the rationality behind our method and empirically verify its effectiveness on both the out-of-distribution and the cross-domain datasets, showcasing its applicability in real-world situations. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Vision Language models [49, 16, 23\u201325, 7] have shown incredible performance in downstream vision tasks [1], such as classification [29, 55, 54, 8], generation [20, 38, 9] and recognition [46, 47]. Among these models, CLIP [36] has been trained with large-scale noisy image-text pairs and can generalize well in zero-shot recognition tasks. The key idea behind CLIP is modality alignment during training and similarity comparison during testing for classification. However, CLIP suffers from domain shift problems during test-time inference. In the presence of out-of-distribution issues [27, 43, 12] that commonly appear in real-world scenarios, CLIP may fail to effectively align the feature across modalities, leading to performance degradation. ", "page_idx": 0}, {"type": "text", "text": "Test-time adaptation (TTA) has been widely explored in recent approaches [43, 15, 41, 17] to mitigate misalignment issues and improve performance in downstream tasks. Current mainstream TTA methods can be divided into training-required methods and training-free methods, as depicted in Figure. 1a and Figure. 1b. Training-required approaches [43, 41, 39] adjust model parameters or learnable prompts based on self-supervised objectives like entropy and increase the prediction confidence of model for distribution adaptation. TPT [41] applies entropy minimization to the vision-language model first. Furthermore, inspired by consistency regularization, TPT performs information mining from the test sample itself by random regional cropping in a self-bootstrapping style. However, training-required methods require gradient descent that is time-consuming with large training overhead, which prevents them from being applied in computationally limited situations. Training-free approaches [15, 52, 17] utilize memory networks, cache, or prototypes to store information regarding target samples and distributions, which is then used to adaptively modify the model\u2019s prediction. For example, TDA [17] leverages historical samples from the test data steam to build a dynamic key-value cache. It updates the prior knowledge encoded in CLIP through feature retrieval and output prediction based on the similarity between the test sample and the high-quality data stored in the memory bank. However, existing training-free approaches only consider interaction with other historical samples in the cache and do not effectively exploit the information within the test sample itself. This limitation prevents them from performing well especially in tasks that require fine-grained information. ", "page_idx": 0}, {"type": "image", "img_path": "8tOYl6WsGY/tmp/f9201802aba0aec5e0f658b5db7805f4885ddc83ad95e1bc59105dd6f32e2011.jpg", "img_caption": ["Figure 1: (a) Existing training-required TTA methods utilize self-supervised objective like entropy minimization for better generalization. (b) Existing training-free TTA methods perform feature retrieval on the historical samples to adjust the model prediction. (c) Performance comparison on the Out-of-Distribution benchmark and Cross-Datasets benchmark. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Both of these approaches demonstrate excellent performance in enhancing the robustness of visionlanguage models to unknown distributions. However, the connection between them remains unclear. In this paper, we aim to answer three questions: (1) How are training-required methods like TPT and training-free methods like TDA connected? (2) How can we combine these two methods based on their shared nature? (3) Does vision-language models benefti from the combination of these methods? ", "page_idx": 1}, {"type": "text", "text": "In order to answer these questions, we first consider that the augmented images of test samples form a regional bootstrapping distribution of the original data. By flitering out the noisy augmentations based on mutual information with the predefined CLIP text embedding clusters, we can obtain a boosting distribution from which high-quality samples close to the target clusters can be drawn. Based on this, we delve into the connection between the target operations over the boosting distribution, i.e., crossentropy optimizations and cache classifier, which reveals the shared nature between entropy-based and cache-based methods. Specifically, we pinpoint that with the samples derived from the bootstrapping distribution, entropy minimization over them performs equivalently to feature retrieval from the cache consisting of them. Motivated by this analysis, we propose a brand-new adaptation strategy, dubbed BoostAdapter, to improve training-free adapters by incorporating the samples derived from the boosting distribution to the memory bank. Particularly, the cache in BoostAdapter consists of instance-agnostic historical samples filtered from the test data stream, along with instance-aware boosting samples generated through regional bootstrapping from the sample itself. The interactions between intra-sample and cross-sample operations make BoostAdapter effective and efficient by incorporating the idea of information mining from training-required methods while maintaining the efficiency of training-free methods. Theoretical analyses and empirical results are also provided to validate the effectiveness of BoostAdapter. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "To summarize, we make the following contributions in this paper. ", "page_idx": 2}, {"type": "text", "text": "\u2022 We first discuss the relationship between training-required and training-free methods in test-time adaptation and establish connections between them.   \n\u2022 We propose BoostAdapter, a brand new adaptation strategy in test-time adaptation of visionlanguage models, which improves training-free adapters by introducing high-quality samples from regional bootstrapping into the memory.   \n\u2022 We theoretically derive target domain error bound of BoostAdapter and shows that BoostAdapter benefit from incorporating self-bootstrapping data.   \n\u2022 Extensive experiments conducted over two benchmark demonstrate the superior performance of BoostAdapter under test-time adaptation settings. ", "page_idx": 2}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Vision-Language Models have shown remarkable potential in generalization by contrastive pretraining over amounts of text-image pairs [16, 36, 24, 25] . One typical work is CLIP [36], which benefits from the alignment of 400 million curated image-text pairs and predicts the most relevant text description for a given image based on cosine similarity. Adapting CLIP to the downstream applications has attracted much attention and has been widely explored in recent approaches [55, 54, 52, 26, 56, 30]. CoOp [55] introduces learnable prompts [22, 51, 50, 28] and CoCoOp [54] conditions the text prompts on image embedding for better generalization. Maple [18] performs prompting for both vision and language branches and improves the alignment of the embedding between modalities. These approaches have demonstrated significant performance enhancements, but they still require few training data from the target domain. In contrast, we focus on test-time adaptation where there is no information about the target distribution and aim to generalize the model to any unknown scenarios. ", "page_idx": 2}, {"type": "text", "text": "Training-required Test-time Adaptation updates partial wights of the model like prompts [41, 39] or BN layer [43] with self-supervised objectives that benefit the downstream tasks without requiring additional training data. Tent [43] reduces generalization error on shifted data by testtime entropy minimization. For vision-language models, Test-time prompt tuning (TPT) [41] is a method that dynamically optimizes prompts during the testing phase, enhancing the model\u2019s zero-shot generalization ability. Specifically, TPT generates multiple augmented views of the test sample and then minimizes the entropy of the model\u2019s output logits across them to ensure consistent prediction. Recently, many works built upon TPT have been proposed to further enhance the performance of vision-language models. Particularly, DiffTPT [6] leverages the power of diffusion models to generate semantically consistent augmented images for entropy minimization. PromptAlign [39] bridges the gap between the test sample and source distribution by aligning token statistics, including mean and variance. Nevertheless, these approaches require gradient descent over the augmented images, which is computationally expensive and time-consuming. ", "page_idx": 2}, {"type": "text", "text": "Training-free Test-time Adaptation applies cache model or prototypes to make prediction of test samples in a non-parametric manner [15, 17, 53]. T3A [15] utilizes prototypes as downstream classifiers and dynamically adjusts the weights. AdaNPC [53] leverages the data from the source domain to address the issues of computation overhead and domain forgetting. For vision-language models, TDA [17] introduces both positive cache and negative cache to obtain high-quality test samples from the target domain. However, these methods only consider inter-sample interactions and may fail to generalize well when the downstream tasks require fine-grained knowledge or there is insufficient similarity across samples. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Problem setting. We begin by introducing the basic notations in test-time adaptation. We consider binary classification for simplicity and the theory can be easily extended to multi-classifications settings. Let $p_{t}(x,y)$ denotes the joint distribution of image and labels in the target distribution, and ", "page_idx": 2}, {"type": "image", "img_path": "8tOYl6WsGY/tmp/e564bb10acb5a2e38efa6d5c3fd8954fa00d704c4c018b364a9c8b483ed68ab5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: Connection between cross-entropy optimization and cache classifier over well-clustered samples with a frozen feature encoder. With optimization of cross-entropy, samples will pull the classifier weights closer of the same class while pushing them away from different class weights. Since the feature space is well-clustered, the classifier weights will ultimately converge near the feature center of the samples. Finally, the optimal classifier achieved through cross-entropy minimization will exhibit similar behavior with the cache classifier. ", "page_idx": 3}, {"type": "text", "text": "we simply assume that samples $\\{(x_{i},y_{i})\\}_{i=1}^{n}$ are drawn i.i.d. from the distribution with $y_{i}$ represents the one-hot label. ", "page_idx": 3}, {"type": "text", "text": "Definition 1. (Classification error.) Given $f$ as a binary classification function. The error incurred by hypothesis $\\tilde{f}\\in\\mathcal{H}:\\mathcal{X}\\rightarrow\\{0,1\\}$ under the distribution $p_{t}(x,y)$ can be defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\epsilon(f)=\\mathbb{E}_{p_{t}(x,y)}[f(x)\\neq y]=\\mathbb{E}_{p_{t}(x,y)}[|f(x)-y|],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "the last equality holds in a binary classification setting. ", "page_idx": 3}, {"type": "text", "text": "Definition 2. (Excess error.) Given the Bayes classifier under distribution $p_{t}(\\boldsymbol{x})$ : $f^{*}(x)=\\mathbb{I}\\{f(x)\\geq$ $1/2\\}$ and the optimal classfier $f^{*}$ , the excess error of $f$ is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{E}(f)=\\epsilon(f)-\\epsilon(f^{*})=2\\mathbb{E}_{x\\sim p_{t}(x)}\\left[\\left|f(x)-\\frac{1}{2}\\right|\\mathbb{I}\\{f(x)\\neq f^{*}(x)\\}\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "CLIP classifier Let $g$ be the image encoder of CLIP, $C$ be the feature dimension, $N$ denotes the number of categories, $w_{i}\\in R^{C}$ represents the $i_{t h}$ text embedding cluster. Considering normalized embedding $w$ and $g(x)$ , we can derive a simplified version of the output of CLIP for class $i$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nZ_{i}=w_{i}^{T}g(x).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "And we denote the output logits as $\\pmb{p}(x)=[Z_{1},Z_{2},...,Z_{N}]\\in\\cal R^{N}.$ ", "page_idx": 3}, {"type": "text", "text": "Cache classifier Given an unseen sample $x$ , encoder $g$ with dimensional $C$ , cache size $K$ and number of categories $N$ , the cache classfier conduct feature retrieval based on the similarity with the data $\\{(x_{i},y_{i})\\}_{i=1}^{\\overline{{k}}}$ in the cache. The predictions based on Tip-Adapter [52] are as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{c a c h e}(x)=A\\left(g(x)G_{c a c h e}^{T}\\right)Y,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $A(z)=\\alpha\\mathbf{exp}(-\\beta(1\\!-\\!z))$ denotes a scaling function with a weighting factor $\\alpha$ and a smoothing scalar \u03b2, Gcache \u2208RK \u00d7C represents the feature of K samples {xi}iK=1 in the cache and Y \u2208RK\u00d7N is the corresponding labels $\\{y_{i}\\}_{i=1}^{K}$ . Considering the number of samples in class $y_{i}$ , We can also derive a simplified version of Eq.(4) as follows, by ignoring the scaling function and adopting an instance-wise computation style: ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{c a c h e}(x)=\\sum_{i=1}^{k}\\alpha_{i}\\left[g(x_{i})^{T}g(x)\\right]y_{i},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where \u03b1i =n1yi for class balance or \u03b1i = jk=1[g(1xj)T g(x)] for normalization across all the samples. ", "page_idx": 3}, {"type": "text", "text": "3.2 A Closer Look at Entropy-based and Cache-based Methods ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We start with analyzing the flitering operation of augmentated images in TPT. Pseudo-labels tends to be noisy in the test time, and entropy can serve as a confidence metric to identify trustworthy samples ", "page_idx": 3}, {"type": "image", "img_path": "8tOYl6WsGY/tmp/19f6bd05927f6342a6e5944d6613cc2198295ccfd3c0f8ef9f664bd329f86feb.jpg", "img_caption": ["Figure 3: Overall architecture of BoostAdapter. BoostAdapter leverages knowledge from the target domain and employs self-bootstrapping with historical and boosting samples in the boosting cache, respectively. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "among augmented views [43, 41, 33]. These high-quality samples can be considered drawn i.i.d.   \nfrom the so-called boosting distribution as defined below. ", "page_idx": 4}, {"type": "text", "text": "Definition 3. (Boosting Distribution.) Given a test sample from target distribution $x\\sim p_{t}(x)$ , let $H(\\cdot)$ be the entropy measuring function and $A u g(\\cdot)$ be the regional augmentation. By flitering noisy samples based on thresthould $\\tau$ , we have the following property of boosting distribution $p_{b}(x)$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{x}\\sim p_{b}(x)\\rightarrow\\{\\hat{x}=A u g(x)\\wedge H(p(x))\\leq\\tau\\}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We also terms the samples from the boosting distribution as boosting samples. Then we can connect entropy-based methods and cache classfier by the following proposition: ", "page_idx": 4}, {"type": "text", "text": "Proposition 1. (Informal) Given n samples $\\{(x_{i},y_{i})\\}_{i=1}^{n}$ with a freeze encoder g that effectively performing feature clustering with respect to labels, the gradient descent optimization direction of the classifier\u2019s weights based on cross-entropy generally tends towards making predictions using the cache classifier with class balance weights defined in $^{5}$ on these samples. ", "page_idx": 4}, {"type": "text", "text": "An intuitive illustration of Proposition 1 is depicted in Figure 2, where the weights of optimal classfier behave like the feature centers across different classes with of the well-clusterd samples. Revisiting the entropy-based method TPT, when provided with high-quality boosting samples with low entropy drawn from the boosting distribution, the objective function of entropy minimization optimizes in a manner similar to conducting cross-entropy optimization over the pseudo-labels. According to Proposition 1, TPT performs similarly to the cache-based methods with a cache comprising the same boosting samples from the boosting distribution. ", "page_idx": 4}, {"type": "text", "text": "3.3 Boosting your Training-free Adapters ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Existing cache-based methods store historical test samples only as useful information for prediction. In light of the analysis above, we can integrate the idea behind TPT into these training-free adapters by incorporating boosting samples into the memory bank. In particular, each sample can participate in both inter-sample and intra-sample interactions with the instance-agnostic historical samples and the instance-aware boosting samples in the cache, respectively. ", "page_idx": 4}, {"type": "text", "text": "Specifically, with $k_{t}$ selected historical samples and $k_{b}$ selected boosting samples to comprise the cache, we extend the classifier defined in Eq.(4) and formulate our BoostAdapter as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\np_{b o o s t}(x)=A\\left(g(x)\\tilde{G}_{c a c h e}^{T}\\right)\\tilde{Y},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $A$ is the same scaling function defined in Eq.(4), $\\tilde{G}_{c a c h e}\\in R^{(k_{t}+k_{b})\\times C}$ denotes the features of the combination of both the historical and boosting samples, and $\\tilde{Y}\\in R^{(k_{t}+k_{b})\\times N}$ is the label. ", "page_idx": 4}, {"type": "text", "text": "Since we do not have access to the labels of the test samples, we generate one-hot pseudo-labels for them using argmax operations. However, these pseudo-labels tend to be noisy in the target domain. Therefore, we apply filtering based on entropy thresholds on the test data stream following [41] to obtain trustworthy historical samples. We employ a similar operation to select boosting samples from multiple augmented views of the current sample. In practice, we dynamically adapt the entropy thresholds $\\tau$ for each test sample, with a fixed percentile $p$ . The cache continuously updates with lower entropy historical samples from the test data stream, while the current test sample augments the cache with self-boosting samples and forms an independent cache that only affects its own prediction. Additionally, to maintain diversity while considering the relevance to each test sample, we set a maximum shot capacity for each class $k$ in the cache. This means that samples in the cache will be replaced by a lower-entropy historical sample or boosting sample when necessary. ", "page_idx": 5}, {"type": "text", "text": "An important issue is whether introducing boosting samples brings improvements to the trainingfree adapters. We will first make some necessary assumptions and then theoretically verify the effectiveness in reducing target error by incorporating samples from the boosting distribution. ", "page_idx": 5}, {"type": "text", "text": "Assumption 1. (Strong Density Condition) For any test sample $x_{0}$ in the target distribution $x_{0}\\sim$ $p_{t}(\\boldsymbol{x})$ and the boosting distribution $p_{b}(x_{0})$ , given positive lower bound m and upper bound $M$ , positive scaling constant $c_{t}$ and $c_{b}$ , the radius bound $R>0$ , and ${\\mathcal{B}}(x,r)=\\{x^{\\prime}:\\parallel{\\bar{x^{\\prime}}}-x\\parallel\\leq r\\}$ is the ball centered on $x$ with radius $r$ . We assume $p_{t}(\\boldsymbol{x})$ and $p_{b}(x_{0})$ are absolutely continuous with respect to the Lebesgue measure in $\\mathbb{R}^{d}$ . For $r\\in(0,R]$ , we assume ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\begin{array}{l l}{\\displaystyle\\lambda[p_{t}(x)\\cap\\mathcal{B}(x_{0},r)]\\geq c_{t}\\lambda[\\mathcal{B}(x_{0},r)]}\\\\ {\\displaystyle\\lambda[p_{b}(x_{0})\\cap\\mathcal{B}(x_{0},r)]\\geq c_{b}\\lambda[\\mathcal{B}(x_{0},r)]}\\\\ {m<\\displaystyle\\frac{d p_{t}(x)}{d\\lambda}<M;m<\\displaystyle\\frac{d p_{b}(x)}{d\\lambda}<M,}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\lambda$ is the Lebesgue measure in Euclidean space. ", "page_idx": 5}, {"type": "text", "text": "Assumption 2. $L$ -Lipschitz Condition) Let $f$ be the classification function and $L$ be a positive constant. For all feasible $x,x^{\\prime}$ we have $|f(x)-f(x^{\\prime})|\\leq L\\ \\|\\ x-x^{\\prime}\\|$ . ", "page_idx": 5}, {"type": "text", "text": "Assumption 3. (Low Noise Condition). Let $\\beta,C_{\\beta}$ be positive constants and we assume $p_{t}(\\boldsymbol{x})$ satisfies $\\begin{array}{r}{P_{x\\sim p_{t}(x)}\\left(\\left|f(x)-\\frac{1}{2}\\right|<t\\right)\\leq C_{\\beta}t^{\\beta}}\\end{array}$ for all $t>0$ . ", "page_idx": 5}, {"type": "text", "text": "Remark Assumption 1 intuitively ensures that for any test sample, there is a surrounding neighborhood with a significant presence of samples from the target domain and the boosting distribution. More importantly, for a specific sample $x_{0}$ , boosting samples $x\\sim p_{b}(x_{0})$ should be closer to $x_{0}$ than other samples $x\\sim p_{t}(x)$ from the target domain, i.e., generally, we have $c_{t}\\leq c_{b}$ . Assumption 2 and 3 describe the smoothness of functions and imply a high level of confidence in predictions around the threshold, respectively. ", "page_idx": 5}, {"type": "text", "text": "Proposition 2. (Historical Cache reduce Emperical Risk) Given $f$ as the training-free classfier consisting of historical samples only defined by Eq.(4). Let $n_{t}$ to be the number of confident previously predicted samples in the target domain and $k_{t}$ as the number of historical samples in the cache, with assumptions 1-3, the following results hold with high-probability for large enough $k_{t}$ and $n_{t}$ . ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{E}(f)\\leq\\mathcal{O}\\left(\\left(\\frac{1}{k_{t}}\\right)^{1/4}+\\left(\\frac{k_{t}}{c_{t}n_{t}}\\right)^{1/d}\\right)^{1+\\beta}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proposition 3. (Historical Cache benefits from Boosting Samples) Let $n_{t}$ to be all confident previously predicted samples in the target domain and $n_{b}$ be the number of boosting samples that are drawn from the boosting distribution. Given $k_{t}$ and $k_{b}$ to be the number of historical samples and the number of boosting samples to be selected as the nearest neighbors stored in the cache, respectively. Let $w_{t i}$ and $w_{b i}$ be the weights defined in Eq.(5) of the historical samples and boosting samples. We have the following bound for the empirical risk of the cache classfier defined in 7. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{E}(f)\\leq\\mathcal{O}\\left(\\left(\\frac{1}{k_{t}+k_{b}}\\right)^{1/4}+\\sum_{i=1}^{k_{t}}w_{t i}\\left(\\frac{k_{t}}{c_{t}n_{t}}\\right)^{1/d}+\\sum_{i=1}^{k_{b}}w_{b i}\\left(\\frac{k_{b}}{c_{b}n_{b}}\\right)^{1/d}\\right)^{1+\\beta}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "table", "img_path": "8tOYl6WsGY/tmp/fcd3dab6892032f47eb4707d669064f2fdd9464240774bb0bf93a352817e226a.jpg", "table_caption": ["Table 1: Full results on the OOD benchmark with ViT-B/16 backbone. We report top-1 accuracy and \u201cAverage\" is calculated by taking the mean accuracy across all four OOD datasets. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "8tOYl6WsGY/tmp/19aaa7097ffe4f5add2a25f25d2e299a1572ac0476c0fa486bf0ee4691e11be9.jpg", "table_caption": ["Table 2: Full results on the Cross-Domain Benchmark with ViT-B/16 backbone. We report top-1 accuracy and \u201cAverage\" is calculated by taking the mean accuracy across all ten datasets. The error bound is $\\pm0.17.$ . "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Remark Proposition 2 provides a guarantee of the effectiveness of selecting $k_{t}$ out of $n_{t}$ historical samples to comprise the cache. The empirical risk is quite small when $n_{t}\\,\\rightarrow\\,\\infty$ since the cache captures the full information of the target domain. Proposition 3 demonstrates that the historical cache can further reduce empirical risk by incorporating $k_{b}$ boosting samples. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets Following the setting in TPT [41], we conduct experiments on both Out-of-Distribution (OOD) benchmark and Cross-Domain benchmark. The OOD benchmark evaluates the model\u2019s robustness to natural distribution shifts on 4 ImageNet [4] Variants, including ImageNetV2 [37], ImageNet-Sketch [44], ImageNet-A [14] and ImageNet-R [13]. We evaluate the transferring performance on 11 datasets in the Cross-Domain benchmark: Aircraft [31], Caltech101 [5], Cars [19], DTD [3], EuroSAT [11], Flower102 [32], Food101 [2], Pets [34], SUN397 [48],and UCF101 [42]. We follow the split in [55] and report the top-1 accuracy. The error bound are also provided. ", "page_idx": 6}, {"type": "text", "text": "Implementation details We utilize a pre-trained ViT-B/16 of CLIP as the foundation model. In test-time adaptation, the batch size is set to be 1. We search for the optimal shot capacity to balance diversity and relevance of samples. For boosting samples, we utilize random crop and then random horizontal flip as augmentations. Moreover, we empirically set the entropy threshold percentile to $p=0.1$ and fliter 64 augmented views based on random cropping to obtain the boosting samples. and fliter 64 augmented views to obtain the boosting samples. The top-1 accuracy and the error bound is reported on the test sets. All our experiments are conducted with a Nvidia 3090 24GB GPU. ", "page_idx": 6}, {"type": "image", "img_path": "8tOYl6WsGY/tmp/0c8ca1f4f1dd0dc1351e305307941cb32124543422adcc40efa671baaf5e141d.jpg", "img_caption": ["Figure 4: Ablation studies of (a) number of augmented views to generate boosting samples (b) different adaptation methods and (c) total shot capacity of the cache. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "8tOYl6WsGY/tmp/cedf4805f3bdab06815840a0f5d3bea4585de8e0ac104df02df569a31f85bbb4.jpg", "table_caption": ["Table 3: Ablation study on historical samples and boosting samples on the OOD benchmark with ViT-B/16 backbone. We report top-1 accuracy and the error bound is $\\pm0.12$ . "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "8tOYl6WsGY/tmp/61e078e30c10f62cd9ccb4bee40e31c99331a26f7a0434649d1a89802598a910.jpg", "table_caption": ["Table 4: Full results on the OOD benchmark with RN-50 backbone. We report top-1 accuracy and the error bound is $\\pm0.06$ . "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.2 Out-of-Distribution Generalization ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To verify the robustness of BoostAdapter, we evaluate our method on the OOD benchmark, in comparison with existing training-require methods including CoOp [55], CoCoOp [54], TPT [41], DiffTPT [6], Maple [18] and PromptAlign [39], as well as training-free method TDA [17]. As can be seen from Table 8, the most striking observation emerging from the comparison is that BoostAdapter significantly outperforms other baselines on average and improves the generalization ability of the model. For training-free methods such as TPT, DiffTPT and PromptAlign, BoostAdapter achieves superior performance while saving on optimization computation overhead. For training-free methods like TDA, BoostAdapter gains consistent performance improvements with the introduction of the boosting samples. Notably, BoostAdapter surpasses TDA by $4.42\\%$ on ImageNet-A and $0.84\\%$ on ImageNet-V2, respectively. This enhancement indicates the effectiveness of self-bootstrapping when historical samples may not provide sufficient useful information. ", "page_idx": 7}, {"type": "text", "text": "4.3 Cross-Domain Transfer ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We further highlight our improvements in the transfer ability of CLIP on the Cross-Domain benchmark and present the results in Table 2. Compared with existing training-required and training-free methods, BoostAdapter achieves state-of-the-art performance on 7 out of 10 tasks, surpassing the strongest baselines by an average of $1.15\\%$ . With diverse classes at test time, regional boosting enables BoostAdapter to adaptively extract knowledge that makes classes distinct from each other in a multi-scale manner. Notably, for datasets requiring fine-grained information for classification such as Aircraft, the improvement of BoostAdapter is most significant. ", "page_idx": 7}, {"type": "text", "text": "4.4 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Historical Samples and Boosting Samples. To demonstrate the effect of historical and boosting samples, we introduce two variants of BoostAdapter that utilize only historical samples or only boosting samples, respectively. Additionally, we provide the zero-shot results of CLIP for comparison. As shown in Table 3, CLIP significantly beneftis from both historical samples and boosting samples, resulting in notable improvements in performance. The consistent improvement of BoostAdapter compared to the variant that utilizes only historical samples further confirms the effectiveness of incorporating boosting samples into the training-free adapters. See Section E in the Appendix for more results. ", "page_idx": 7}, {"type": "table", "img_path": "8tOYl6WsGY/tmp/4c91df1bb0b88ed100446a358990b16a803a4cbc8a342f70a0e1fe2aa23d3f3e.jpg", "table_caption": ["Table 5: Full results on the Cross-Domain Benchmark with RN-50 backbone. We report top-1 accuracy and \u201cAverage\" is calculated by taking the mean accuracy across all ten datasets. The error bound is $\\pm0.05$ . "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "8tOYl6WsGY/tmp/c43a4effb92a63526766089c72e8f5443f18513a1ca70c0f7746bd51598808bf.jpg", "table_caption": ["Table 6: Comparisons with baselines on ImageNet-C at severity level 5 regarding accuracy $(\\%)$ "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Number of Augmented Views for Boosting Samples. We augment the testing samples and filter them by mutual information with the CLIP text embedding to obtain the boosting samples. We vary the number of augmented views and investigate the performance of BoostAdapter on UCF101 in Figure 4a. With a larger number of augmented views, the performance improves due to more bootstrapping information of the test sample, which is consistent with the conclusions of TPT [41] and PromptAlign [39]. However, the computational overhead also increases with more augmented views, and selecting 64 augmented views is a fair trade-off between boosting performance and efficiency. ", "page_idx": 8}, {"type": "text", "text": "Adaptation Methods. Training-required methods use entropy as a self-supervised objective, whereas training-free methods classify samples based on feature retrieval. We compare the performance of these two adaptation methods under the constraints of historical samples only, boosting samples only, or both, and present the results on Flower102 in Fig. 4b. Entropy minimization requires gradient descent and model optimization, resulting in high training costs and relatively lower performance across all three settings. In contrast, the training-free methods based on feature retrieval offer significant performance improvements with lower computational overhead. Additionally, both adaptation methods benefti from combining historical samples and boosting samples, consistent with the conclusions in Table 3. ", "page_idx": 8}, {"type": "text", "text": "Total shot capacity. BoostAdapter maintains low-entropy samples per class in the cache, and Figure $_\\mathrm{4c}$ studies the influence of different total shot capacities containing historical samples and boosting samples of each class on Aircraft. As can be observed from the results, when the cache capacity is small, the low-entropy samples maintained by BoostAdapter do not necessarily provide a benefti for classification compared to CLIP. As the shot capacity increases, BoostAdapter will achieve the best balance of diversity and relevance, and a larger capacity does not guarantee better performance. ", "page_idx": 8}, {"type": "text", "text": "Versatility. To demonstrate the versatility of BoostAdapter, we apply it to the RN-50 backbone and present the results in Tables 4 and 5. The improvement is consistent and on average, BoostAdapter outperforms TDA by $1.57\\%$ on the OOD benchmark and $0.49\\%$ on the Cross-Domain benchmark. ", "page_idx": 8}, {"type": "text", "text": "4.5 Discussions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Generalization on Corruption Datasets To further evaluate the generalization ability of BoostAdapter in new test-time scenarios, we compare BoostAdapter with baseline methods on the ImagenetC dataset at the highest severity level 5. The key observation from Table 6 is that BoostAdapter consistently outperforms TDA across all 15 corruption types, highlighting its practical applicability in real-world situations. The superior performance of BoostAdapter stems from its capability to capture the knowledge of the test sample even under severe corruption. This is achieved with the help of the boosting samples, which effectively filter out noisy parts while retaining useful information. ", "page_idx": 8}, {"type": "table", "img_path": "8tOYl6WsGY/tmp/4bb182b45daf7fe3c2e49ff2f0c2f5dbd1b741427ec364eca9a743afad4b54c5.jpg", "table_caption": ["Table 7: Efficiency analysis. We evaluate different methods on a single NVIDIA 3090 24GB GPU and report the frames per second (fps) and memory cost (GB). "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "8tOYl6WsGY/tmp/f192608d06af0618ea9b3e5c7560ddc318bf7ec2a9e07ea9f9de8645cdea3e85.jpg", "table_caption": ["Table 8: Unification of more training-required methods. BoostAdapter benefits from different training-required methods. "], "table_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "8tOYl6WsGY/tmp/d8665586f30610a0836ec241c481c8a342c5cdadf6767f074e6d4de7e4700199.jpg", "img_caption": ["Figure 5: Qualitative results. The model predictions are provided below the images. Boosting samples with low entropy improves information extraction from the test sample and helps the model to distinguish better. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Efficiency Analysis BoostAdapter requires augmentation over the test samples, which may slightly affect the inference speed during testing. We conduct an efficiency analysis of BoostAdapter in comparison with existing Test Time Augmentation (TTA) methods and provide the results in Table 7. BoostAdapter is slightly slower than the cache-based method TDA, yet still significantly faster than training-required methods. The memory cost of BoostAdapter is also comparable to other baselines. ", "page_idx": 9}, {"type": "text", "text": "Unification of Training-required and Training-free Methods. From the unified perspective, we can also enhance training-free adapters with additional training-required methods. Here we take TSD [45] and DEYO [21] as the showcase. Specifically, in the BoostAdapter+DEYO variant, we filter out augmented views with a PLPD lower than 0.2. For the BoostAdapter TSD variant, we discard augmented views that have different cache predictions and CLIP predictions to ensure consistency of the boosting samples. When equipping BoostAdapter with the technique of TSD and DEYO, we observe further improvement and find that training-free adapters can benefit from various boosting techniques of training-required methods. ", "page_idx": 9}, {"type": "text", "text": "Qualitative Results The qualitative results are provided in Figure. 5. By incorporating samples with low entropy from regional bootstrapping, the model is enhanced to more effectively capture the fine-grained information of the test samples, thereby improving the overall performance. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we present an insightful analysis of existing training-required and training-free TTA methods to bridge the gap between them. In particular, we improve training-free adapters by incorporating self-boosting samples into the memory bank inspired by the idea of regional bootstrapping from entropy-based methods. The cache in our method, containing instance-agnostic historical samples and instance-aware boosting samples, is capable of performing knowledge mining on both the target domain and the testing sample itself. We also derive error bounds in the test-time adaptation setting and show that this cache benefits from both historical samples and boosting samples. Extensive experiments on the two benchmarks demonstrate the effectiveness of our method. ", "page_idx": 9}, {"type": "text", "text": "Despite the promising performance of our method, it also has some limitations. It requires slightly more computation overhead than existing training-free adapters due to the multiple augmentation of the test samples, as discussed in Appendix. One future direction is to develop a more efficient augmentation method to obtain boosting samples, rather than merely randomly cropping and then filtering over the test samples. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work is supported in part by the National Natural Science Foundation of China, under Grant(62302309,62171248), Shenzhen Science and Technology Program (JCYJ20220818101014030, JCYJ20220818101012025), and the PCNL KEY project (PCL2023AS6-1). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.   \n[2] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101\u2013mining discriminative components with random forests. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13, pages 446\u2013461. Springer, 2014.   \n[3] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3606\u20133613, 2014.   \n[4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A largescale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248\u2013255. IEEE, 2009.   \n[5] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In 2004 Conference on Computer Vision and Pattern Recognition Workshop, pages 178\u2013178. IEEE, 2004.   \n[6] Chun-Mei Feng, Kai Yu, Yong Liu, Salman Khan, and Wangmeng Zuo. Diverse data augmentation with diffusions for effective test-time prompt tuning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2704\u20132714, 2023.   \n[7] Kuofeng Gao, Jindong Gu, Yang Bai, Shu-Tao Xia, Philip Torr, Wei Liu, and Zhifeng Li. Energy-latency manipulation of multi-modal large language models via verbose samples. arXiv preprint arXiv:2404.16557, 2024.   \n[8] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. Clip-adapter: Better vision-language models with feature adapters. International Journal of Computer Vision, 132(2):581\u2013595, 2024.   \n[9] Hang Guo, Tao Dai, Zhihao Ouyang, Taolin Zhang, Yaohua Zha, Bin Chen, and Shu-tao Xia. Refir: Grounding large restoration models with retrieval augmentation. arXiv preprint arXiv:2410.05601, 2024.   \n[10] Ziyu Guo, Renrui Zhang, Longtian Qiu, Xianzheng Ma, Xupeng Miao, Xuming He, and Bin Cui. Calip: Zero-shot enhancement of clip with parameter-free attention. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 746\u2013754, 2023.   \n[11] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 12(7):2217\u20132226, 2019.   \n[12] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019.   \n[13] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8340\u20138349, 2021.   \n[14] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15262\u201315271, 2021.   \n[15] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier adjustment module for model-agnostic domain generalization. Advances in Neural Information Processing Systems, 34:2427\u20132440, 2021.   \n[16] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In ICML, pages 4904\u20134916. PMLR, 2021.   \n[17] Adilbek Karmanov, Dayan Guan, Shijian Lu, Abdulmotaleb El Saddik, and Eric Xing. Efficient test-time adaptation of vision-language models. arXiv preprint arXiv:2403.18293, 2024.   \n[18] Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad Shahbaz Khan. Maple: Multi-modal prompt learning. In The IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023.   \n[19] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for finegrained categorization. In Proceedings of the IEEE International Conference on Computer Vision Workshops, pages 554\u2013561, 2013.   \n[20] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multiconcept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1931\u20131941, 2023.   \n[21] Jonghyun Lee, Dahuin Jung, Saehyung Lee, Junsung Park, Juhyeon Shin, Uiwon Hwang, and Sungroh Yoon. Entropy is not enough for test-time adaptation: From the perspective of disentangled factors. arXiv preprint arXiv:2403.07366, 2024.   \n[22] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021.   \n[23] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language representation learning with momentum distillation. Advances in neural information processing systems, 34:9694\u20139705, 2021.   \n[24] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping languageimage pre-training for unified vision-language understanding and generation. In International conference on machine learning, pages 12888\u201312900. PMLR, 2022.   \n[25] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 19730\u201319742. PMLR, 2023.   \n[26] Xin Li, Dongze Lian, Zhihe Lu, Jiawang Bai, Zhibo Chen, and Xinchao Wang. Graphadapter: Tuning vision-language models with dual knowledge graph. Advances in Neural Information Processing Systems, 36, 2024.   \n[27] Shiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhancing the reliability of out-ofdistribution image detection in neural networks. arXiv preprint arXiv:1706.02690, 2017.   \n[28] Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. arXiv preprint arXiv:2110.07602, 2021.   \n[29] Yuning Lu, Jianzhuang Liu, Yonggang Zhang, Yajing Liu, and Xinmei Tian. Prompt distribution learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5206\u20135215, 2022.   \n[30] Zhihe Lu, Jiawang Bai, Xin Li, Zeyu Xiao, and Xinchao Wang. Beyond sole strength: Customized ensembles for generalized vision-language models. arXiv preprint arXiv:2311.17091, 2023.   \n[31] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Finegrained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013.   \n[32] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing, pages 722\u2013729. IEEE, 2008.   \n[33] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, and Mingkui Tan. Towards stable test-time adaptation in dynamic wild world. arXiv preprint arXiv:2302.12400, 2023.   \n[34] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pages 3498\u20133505. IEEE, 2012.   \n[35] Sarah Pratt, Ian Covert, Rosanne Liu, and Ali Farhadi. What does a platypus look like? generating customized prompts for zero-shot image classification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15691\u201315701, 2023.   \n[36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748\u20138763. PMLR, 2021.   \n[37] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In International Conference on Machine Learning, pages 5389\u20135400. PMLR, 2019.   \n[38] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.   \n[39] Jameel Hassan Abdul Samadh, Hanan Gani, Noor Hazim Hussein, Muhammad Uzair Khattak, Muzammal Naseer, Fahad Khan, and Salman Khan. Align your prompts: Test-time prompting with distribution alignment for zero-shot generalization. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[40] Jian Shen, Yanru Qu, Weinan Zhang, and Yong Yu. Wasserstein distance guided representation learning for domain adaptation. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.   \n[41] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Test-time prompt tuning for zero-shot generalization in vision-language models. Advances in Neural Information Processing Systems, 35:14274\u201314289, 2022.   \n[42] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. A dataset of 101 human action classes from videos in the wild. Center for Research in Computer Vision, 2(11), 2012.   \n[43] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. arXiv preprint arXiv:2006.10726, 2020.   \n[44] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. Advances in Neural Information Processing Systems, 32, 2019.   \n[45] Shuai Wang, Daoan Zhang, Zipei Yan, Jianguo Zhang, and Rui Li. Feature alignment and uniformity for test time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20050\u201320060, 2023.   \n[46] Xiang Wang, Shiwei Zhang, Jun Cen, Changxin Gao, Yingya Zhang, Deli Zhao, and Nong Sang. Clip-guided prototype modulating for few-shot action recognition. International Journal of Computer Vision, pages 1\u201314, 2023.   \n[47] Syed Talal Wasim, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan, and Mubarak Shah. Vita-clip: Video and text adaptive clip via multimodal prompting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23034\u201323044, 2023.   \n[48] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 3485\u20133492. IEEE, 2010.   \n[49] Jinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath Chanda, Liqun Chen, Belinda Zeng, Trishul Chilimbi, and Junzhou Huang. Vision-language pre-training with triple contrastive learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15671\u201315680, 2022.   \n[50] Sheng Yang, Jiawang Bai, Kuofeng Gao, Yong Yang, Yiming Li, and Shu-Tao Xia. Not all prompts are secure: A switchable backdoor attack against pre-trained vision transfomers. In CVPR, 2024.   \n[51] Yaohua Zha, Jinpeng Wang, Tao Dai, Bin Chen, Zhi Wang, and Shu-Tao Xia. Instance-aware dynamic prompt tuning for pre-trained point cloud models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14161\u201314170, 2023.   \n[52] Renrui Zhang, Wei Zhang, Rongyao Fang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip-adapter: Training-free adaption of clip for few-shot classification. In European conference on computer vision, pages 493\u2013510. Springer, 2022.   \n[53] Yifan Zhang, Xue Wang, Kexin Jin, Kun Yuan, Zhang Zhang, Liang Wang, Rong Jin, and Tieniu Tan. Adanpc: Exploring non-parametric classifier for test-time adaptation. In International Conference on Machine Learning, pages 41647\u201341676. PMLR, 2023.   \n[54] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.   \n[55] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. International Journal of Computer Vision (IJCV), 2022.   \n[56] Xiangyang Zhu, Renrui Zhang, Bowei He, Aojun Zhou, Dong Wang, Bin Zhao, and Peng Gao. Not all features matter: Enhancing few-shot clip with adaptive prior refinement. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2605\u20132615, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Dataset and Licenses ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table 9 presents the statistics and details of datasets used in the paper. We also provide the corresponding license information of the datasets and source code. ", "page_idx": 14}, {"type": "text", "text": "Datasets. Below are the datasets used in this paper that have known license information: The following datasets used in this paper are under the MIT License: ImageNet-A [14], ImageNet-V2 [37], ImageNet-R [13], ImageNet-Sketch [44], EuroSAT [11], Food101 [2]. ", "page_idx": 14}, {"type": "text", "text": "The following datasets used in this paper are under the CC BY-SA 4.0 License: Oxford-Pets [34], Caltech101 [5].   \nThe following datasets used in this paper are for research purposes only: DTD [3], StanfordCars [19], SUN397 [48], FGVC-Aircraft [31], Flower102 [34], UCF101 [42]. ", "page_idx": 14}, {"type": "text", "text": "Source code. We use the implementation of existing baseline methods for reporting their results in this paper. Below are their license information: Source code used in this paper that are under the MIT License: CLIP [36], PromptAlign [39] and TDA [17]. ", "page_idx": 14}, {"type": "table", "img_path": "8tOYl6WsGY/tmp/0922cab11fbb798adea9bb54f21bb5b5c5d3e1f7e843fac3fdffe879e9cc62c5.jpg", "table_caption": ["Table 9: Datasets statistics. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "B Broader Impacts ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this paper, we focus on bridging the gap between training-required and training-free methods to improve the generalization ability of vision-language models. We also theoretically derive the error bound of incorporating boosting samples into the historical cache. We hope that our work will inspire the community to explore test-time adaptation in an effective and efficient way. ", "page_idx": 14}, {"type": "text", "text": "C Theoretical Proof ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 Cross-entropy Optimization behaves like Cache Classifier over well-clustered Samples (Proof of Proposition 1) ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Given well-clustered samples in the feature space and the classifier defined in Eq.(3), we first derive the distance between the weights of the classifier and the optimal weights and then establish the connection between the optimal weights with the features center of the samples. ", "page_idx": 14}, {"type": "text", "text": "Suppose the classifier function $f$ over samples is convex and differentiable, and also $L$ -smooth. Let the distance between initial weight $w^{(0)}$ and optimal weight $w^{*}$ to be $D=||\\boldsymbol{w}^{(0)}-\\boldsymbol{w}^{*}||$ . GD updates by $w^{(t+1)}=w^{t}-f_{t}^{*}\\nabla f(w^{t})$ with step size $\\begin{array}{r}{f_{t}^{*}=\\frac{1}{L}}\\end{array}$ , and then GD enjoys the following convergence guarantee: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\lvert|w-w^{*}\\rvert|\\leq\\frac{2L\\left\\lVert w^{(0)}-w^{\\star}\\right\\rVert^{2}}{T-1}=\\mathcal{O}\\left(\\frac{L D^{2}}{T}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We then showcase the relationship between $w_{*}$ and the features center $\\mu_{i}$ of class $i,i=1,2,...,N$ . Since we optimize on well-clustered samples, we consider the scenarios of perfect clusters, where samples in the class $i$ will be encoded into the same point $\\mu_{i}$ by the encoder $g$ , and these points should be farthest enough between each other. Given $n$ samples $\\{(x_{k},y_{k})\\}_{k=1}^{n}$ , with the number of samples in class $i$ to be $n_{i}$ , the cross-entropy loss function $L$ can be written as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\cal L}=-\\sum_{i=1}^{n}\\log P(y=y_{k}|x_{k})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Substitute the sample $g(x_{k})=\\mu_{i}$ from class $i$ , we derive the probability $P(y=i|x_{k})$ using the softmax function from Eq.(3) is: ", "page_idx": 15}, {"type": "equation", "text": "$$\nP(y=i|x_{k})=\\frac{e x p(w_{i}^{T}\\mu_{i})}{\\sum_{j=1}^{N}e x p(w_{j}^{T}\\mu_{i})}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, the cross-entropy loss for a sample $(x_{k},y_{k}=i)$ is: ", "page_idx": 15}, {"type": "equation", "text": "$$\nL_{k}=-\\log\\left(\\frac{e x p(w_{i}^{T}\\mu_{i})}{\\sum_{j=1}^{N}e x p(w_{j}^{T}\\mu_{i})}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For all samples, the total loss is: ", "page_idx": 15}, {"type": "equation", "text": "$$\nL=-\\sum_{i=1}^{N}n_{i}\\log\\left(\\frac{e x p(w_{i}^{T}\\mu_{i})}{\\sum_{j=1}^{N}e x p(w_{j}^{T}\\mu_{i})}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The gradient of the loss with respect to $w_{i}$ can be simplified as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\partial L}{\\partial w_{i}}=-\\mu_{i}n_{i}+\\mu_{i}\\sum_{k=1}^{N}n_{k}\\frac{\\exp(w_{i}^{T}\\mu_{k})}{\\sum_{j=1}^{N}\\exp(w_{j}^{T}\\mu_{k})}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "When converges to the optimal weight, we have the condition of fixed point $\\begin{array}{r}{\\frac{\\partial L}{\\partial w_{i}^{*}}=0}\\end{array}$ . And we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n-\\mu_{i}n_{i}+\\mu_{i}\\sum_{k=1}^{N}n_{k}\\frac{\\mathrm{exp}((w_{i}^{*})^{T}\\mu_{k})}{\\sum_{j=1}^{N}\\mathrm{exp}((w_{j}^{*})^{T}\\mu_{k})}=0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{N}n_{k}\\frac{\\exp((w_{i}^{*})^{T}\\mu_{k})}{\\sum_{j=1}^{N}\\exp((w_{j}^{*})^{T}\\mu_{k})}=n_{i}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Given a well-clustered samples, we could have $e x p((w_{i}^{*})^{T}\\mu_{k})\\,\\gg\\,e x p((w_{j}^{*})^{T}\\mu_{k})$ for a specific $i$ when $\\boldsymbol{w}_{i}^{*}$ is near $\\mu_{k}$ . Then since the equality in Eq.(18) will hold for each class and for class $i=1,2,...,N$ we have ", "page_idx": 15}, {"type": "equation", "text": "$$\nw_{i}^{*}\\to\\mu_{i}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Combining Eq.(11) and Eq.(18), with iteration steps $T$ , we show that the weight of classfier will finally converge to the feature center of each class: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\lvert|w-\\mu\\rvert|\\leq\\lvert|w-w^{*}\\rvert|+\\lvert|w^{*}-\\mu\\rvert|\\leq\\mathcal{O}\\left(\\frac{L D^{2}}{T}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "And we have the output logits of the optimal weights with the encoder $g$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\np_{c r o s s}(x)=[\\mu_{1}^{T}g(x),\\mu_{2}^{T}g(x),...,\\mu_{N}^{T}g(x)]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Next we discess the behavior of the cache classifier over these samples. Given the number of wellclustered samples in class $i$ to be $n_{i}$ , the output logits of the cache classifier defined in Eq.(5) using samples $\\{(x_{k},y_{k})\\}_{k=1}^{n}$ can be described as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle p_{c a c h e}(x)=\\sum_{k=1}^{n}\\frac{1}{n_{y_{i}}}[g(x_{k})^{T}g(x)]y_{k}}}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~=\\sum_{i=1}^{N}\\frac{n_{i}}{n_{i}}[\\mu_{i}^{T}g(x)]y_{i}}}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~~=[\\mu_{1}^{T}g(x),\\mu_{2}^{T}g(x),...,\\mu_{N}^{T}g(x)]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Combining Eq.(21) and Eq.(22), we draw the conclusion that cross-entropy optimization behaves like cache classifier over well-clustered samples. ", "page_idx": 16}, {"type": "text", "text": "C.2 Historical Cache reduce Empirical Risk (Proof of Proposition 2) ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We follow the proofs in [53] and extend the conclusion to boosting samples. ", "page_idx": 16}, {"type": "text", "text": "C.2.1 Additional Definitions and Assumptions ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Definition 4. (Wasserstein-distance and the dual form). Wasserstein distance measures the distance between two probability distributions on a given metric space. It is defined using the concept of optimal transport. For two distributions $\\mathbb{P},\\mathbb{Q},$ The $\\rho$ -th Wasserstein distance is defined as ", "page_idx": 16}, {"type": "equation", "text": "$$\nW_{p}(\\mathbb{P},\\mathbb{Q})=\\left(\\operatorname*{inf}_{\\gamma\\in\\Pi(\\mathbb{P},\\mathbb{Q})}\\int_{X\\times X}d(x,y)^{p}d\\gamma(x,y)\\right)^{1/p}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Here, $\\Pi(\\mathbb{P},\\mathbb{Q})$ denotes the set of all couplings (or transport plans) $\\gamma$ of $\\mathbb{P}$ and $\\mathbb{Q}_{:}$ , i.e., joint distributions on $X\\times X$ with marginals $\\mathbb{P}$ and $\\mathbb{Q}$ .The idea is to find the optimal way to transport the mass from one distribution to the other with the minimal cost, where the cost is given by the $p$ -th power of the distance. ", "page_idx": 16}, {"type": "text", "text": "The first Wasserstein distance, $W_{1}({\\mathbb{P}},{\\mathbb{Q}})$ ,often referred to as the Earth-Mover Distance(EMD), has a particularly elegant dual representation. The dual form of $W_{1}$ leverages the Kantorovich-Rubinstein duality and can be expressed as: ", "page_idx": 16}, {"type": "equation", "text": "$$\nW_{1}({\\mathbb P},{\\mathbb Q})=\\operatorname*{sup}_{\\|f\\|_{\\mathrm{Lip}}\\leq1}\\left(\\int_{X}f\\ d{\\mathbb P}-\\int_{X}f\\ d{\\mathbb Q}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Here, the supremum is taken over all $^{\\,l}$ -Lipschitz functions $f$ ,which are functions satisfying $\\left|f(x)-\\right.$ $f(y)\\vert\\leq d(\\bar{x},y)$ for all $x,y\\in X$ .This representation shows that $W_{1}$ can be seen as the maximum difference in expected values of a $^{\\,l}$ -Lipschitz function over the two distributions. In the following part, Wasserstein distance represents the first Wasserstein distance for simplicity and we utilize $W(\\cdot,\\cdot)$ instead of $W_{1}(\\cdot,\\cdot)$ . ", "page_idx": 16}, {"type": "text", "text": "Given the definition of the Wasserstein distance, we have the following proposition that derive the empirical risk on the target domain according to Theorem 1 from [40]. ", "page_idx": 16}, {"type": "text", "text": "Proposition 4. Given two distributions $\\mathbb{P},\\mathbb{Q},$ , denote $\\begin{array}{r}{f^{\\ast}=\\arg\\operatorname*{min}_{f\\in\\mathcal{H}}(\\epsilon_{P}(f)+\\epsilon_{Q}(f))}\\end{array}$ and $\\xi=$ $\\epsilon_{P}(f^{*})+\\epsilon_{Q}(f^{*})$ . Assume all hypotheses $h$ are $L$ -Lipschitz continuous, the risk of hypothesis $\\hat{f}$ is then bounded by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\epsilon_{Q}(\\hat{f})\\leq\\xi+\\epsilon_{P}(\\hat{f})+2L\\mathcal{W}(\\mathbb{P},\\mathbb{Q}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "C.2.2 Distance between the Ball Distribution with the Target Distribution ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "When using the cache classifier with historical samples, a large number of samples that are not similar enough from the target domain will be flitered and the selected samples with high weight are all close to the target data. Thus we extend the conclusion in [53] to the distance between the ball distribution with the target distribution. Considering a test sample from the target distribution $x_{t}\\in p_{t}(x)$ and a distribution consisting of ball center of all the test samples $\\textstyle\\Omega:=\\bigcup_{x_{t}\\in p_{t}(x)}B(x_{t},r)$ , informally, according to Eq.(23), we have the distance between the ball distribution with the target distribution as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{W}(\\Omega,p_{t}(x))=\\operatorname*{inf}_{\\gamma\\in\\Pi[\\Omega,p_{t}(x)]}\\iint\\parallel x_{t}-x_{b a l l}\\parallel d\\gamma(x_{t},x_{b a l l}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where for each $x_{b a l l}\\in\\Omega$ , we can find at least one $x_{t}\\,\\in\\,p_{t}(x)$ such that $\\parallel\\boldsymbol{x}_{b a l l}\\,-\\,\\boldsymbol{x}_{t}\\,\\parallel\\,\\leq\\,r$ , the overall distance will then be bounded by $r$ . Specifically, we can choose a density function $\\gamma^{*}$ where $\\gamma^{*}(x_{b a l l},x_{t})>0$ only if $\\|\\ x_{b a l l}-x_{t}\\ \\|\\leq r$ otherwise 0, then we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathcal{W}(\\Omega,p_{t}(x))=\\operatorname*{inf}_{\\gamma\\in\\Pi[\\Omega,p_{t}(x)]}\\iint\\parallel x_{b a l l}-x_{t}\\parallel d\\gamma(x_{b a l l},x_{t})}}\\\\ &{}&{\\leq\\iint\\parallel x_{b a l l}-x_{t}\\parallel\\gamma^{*}(x_{b a l l},x_{t})d x_{b a l l}x_{t}\\leq r.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "However, there is no guarantee that each data $x_{t}~\\in~p_{t}(x)$ can find a neighbor $B(x_{t},r)$ with $|B(x_{t},r)|\\;>\\;0$ with all the small $r$ . We then provide the probability that the set of neighbors $B(x_{t},r)$ of each $x_{t}\\in p_{t}(x)$ is not measuring zero with respect to the radius $r$ . ", "page_idx": 17}, {"type": "text", "text": "As defined in the cache classfier Eq.(5), we denote $k_{t}$ is the number of historical samples we select in the cache and $n_{t}$ is the total number of data from the historical stream. With the strong density assumption, given the coefficient bound $m$ and $M$ , for any $x_{t}\\in p_{t}(x),r<R$ , according to Assumption 1, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{|\\hat{x}_{t}\\in p_{t}(x)\\wedge\\hat{x}_{t}\\in\\mathcal{B}(x_{t},r)|=\\int_{\\mathcal{B}(x_{t},r)\\cap p_{t}(x)}\\frac{d p_{t}(x)}{d\\lambda}(\\hat{x}_{t})d\\hat{x}_{t}}}\\\\ &{}&{\\geq m\\lambda(\\mathcal{B}(x_{t},r)\\cap p_{t}(x))}\\\\ &{}&{\\geq m c_{t}\\pi_{d}r^{d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ${\\pi}_{d}=\\lambda(B(0,1))$ is the volume of the $d$ dimension unit ball and $\\lambda$ is the Lebesgue measure of a set in a Euclidean space. Set r0 = (mct2\u03c0kdnt ) 1/d, with a additional assumption that we utilize a small $k_{t}$ compared to $n_{t}$ so that <ctm2\u03c0drd\u00b5, we have r0 < R. Then for any xt \u2208pt(x), according to Eq.(28), we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n|\\hat{x_{t}}\\in p_{t}(x)\\wedge\\hat{x_{t}}\\in\\mathcal{B}(x_{t},r_{0})|\\geq m c_{t}\\pi_{d}r_{0}^{d}>\\frac{2k_{t}}{n_{t}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $\\hat{x}_{t}\\in p_{t}(x)$ are independently drawn from the target distribution, let $\\mathbb{I}(\\cdot)$ to be the Indicator funciton and $\\begin{array}{r}{\\dot{S_{n_{t}}(x_{t})}=\\dot{\\sum_{i=1}^{n_{t}}}\\mathbb{I}(\\hat{x_{t}}\\overleftarrow{\\in}\\mathcal{B}(x_{t},r_{0}))}\\end{array}$ denote the number of data $\\hat{x_{t}}\\in p_{t}(x)$ that fall into $B(x_{t},r_{0})$ , then $S_{n_{t}}(x_{t})$ follows the Binomial distribution. Let $\\begin{array}{r}{W\\sim B i n o m i a l(n_{t},\\frac{2k}{n_{t}})}\\end{array}$ , according to the Chernoff inequality, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P(S_{n_{t}}(x_{t})<k_{t})\\le P(W<k_{t})}\\\\ &{\\qquad\\qquad=P(W-\\mathbb{E}[W]<-k_{t})}\\\\ &{\\qquad\\qquad\\qquad\\le\\exp(-k_{t}^{2}/2\\mathbb{E}[W])}\\\\ &{\\qquad\\qquad\\qquad=\\exp(-k_{t}/4),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the second inequality holds since $S_{n}(x)$ has a larger mean than $W$ . With a large $k_{t}$ , the probability that $S_{n}(x)<k_{t}$ is small for any $x_{t}\\in p_{t}(x)$ . Denoting $\\hat{x_{t}}^{(i)}$ as the $i_{t h}$ nearest sample to $x_{t}$ among $B(x_{t},r_{0})$ in the cache, we have for any $x_{t}\\in p_{t}(x)$ ", "page_idx": 17}, {"type": "equation", "text": "$$\nP(\\|\\;\\hat{x_{t}}^{(k_{t})}-x_{t}\\;\\|\\le r_{0})=P(S_{n}(x_{t})\\ge k_{t})\\ge1-\\exp(-k_{t}/4)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Combine Eq.(31) with the assumption that the distribution $p_{t}(\\boldsymbol{x})$ is finite with cardinality $\\aleph_{p_{t}}$ and the desired probability part is shown by union bound. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\bigcap_{x_{t}\\in p_{t}(x)}P(\\|\\,\\hat{x_{t}}^{(k_{t})}-x_{t}\\,\\|\\le r_{0}))=\\bigcap_{x_{t}\\in p_{t}(x)}P(S_{n}(x)\\ge k_{t})}}\\\\ &{}&{=1-\\bigcup_{x_{t}\\in p_{t}(x)}P(S_{n}(x)<k_{t})}\\\\ &{}&{\\ge1-\\mathbb{N}_{p_{t}}\\exp\\left(-\\frac{k_{t}}{4}\\right)}\\\\ &{}&{=1-\\exp\\left(-\\frac{k_{t}}{4}+\\log\\mathbb{N}_{p_{t}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "And then we have the following proposition. ", "page_idx": 18}, {"type": "text", "text": "Proposition 5. Given the target domain distributions $p_{t}(\\boldsymbol{x})$ that is finite with cardinality $\\aleph_{p_{t}}$ , and $\\Omega:=\\cup_{x\\in p_{t}(x)}\\,\\textstyle\\mathcal{B}(x,r)$ , where $\\mathcal{B}(x,r)\\,=\\,\\{x^{\\prime}:\\,\\mid\\,x^{\\prime}-x\\,\\,\\Vert\\leq\\,r\\}$ denotes a ball centered on $x$ with radius $r$ . Denote $\\begin{array}{r}{f^{*}=\\arg\\operatorname*{min}_{f\\in\\mathcal{H}}(\\epsilon_{t}(f)\\!+\\!\\epsilon_{\\Omega}(f))}\\end{array}$ and $\\xi=\\epsilon_{t}(f^{\\ast})\\!+\\!\\epsilon_{\\Omega}(f^{\\ast})$ . Assume all hypotheses $h$ are $L$ -Lipschitz continuous, the risk of hypothesis $\\hat{f}$ on the unseen target domain is then bounded by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\epsilon_{t}(\\hat{f})\\leq\\kappa+\\epsilon_{\\Omega}(\\hat{f})+2L\\left(\\frac{2k_{t}}{m c_{t}\\pi_{d}n_{t}}\\right)^{1/d}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "with probability $\\begin{array}{r}{1-\\exp(-\\frac{k_{t}}{4}+\\log\\aleph_{p_{t}})}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "C.2.3 Excess Error Bound of Cache Classifier ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Let $s_{i}$ to be the softmax probability $s o f t m a x(p_{c a c h e})$ for class $i$ in the the cache classifier from Eq.(5), we can simplify the classifier as $\\hat{f}_{c a c h e}=\\mathbb{I}\\{s_{1}\\geq\\frac{1}{2}\\}$ on the binary classification setting. Then $\\hat{f}_{c a c h e}(x_{t})\\neq f^{*}(x_{t})$ implies that $\\begin{array}{r}{\\left|\\hat{f}_{c a c h e}(x_{t})-f^{*}(x_{t})\\right|\\ge\\left|f^{*}(x_{t})-\\frac12\\right|}\\end{array}$ . We then bridge the gap between the excess error and the classify error as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{E}_{t}(\\hat{f})=2\\mathbb{E}_{x_{t}\\sim p_{t}(x)}\\left[\\left|f^{*}(x_{t})-\\frac{1}{2}\\right|\\mathbb{I}\\left\\{\\left|\\hat{f}_{c a c h e}(x_{t})-f^{*}(x_{t})\\right|\\geq\\left|f^{*}(x_{t})-\\frac{1}{2}\\right|\\right\\}\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We want to bound $\\begin{array}{r}{\\operatorname*{sup}_{x_{t}}\\left|\\hat{f}_{c a c h e}(x_{t})-f^{*}(x_{t})\\right|\\le t,}\\end{array}$ combining with the marginal assumption in Assumption 3 and the fact that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[Z\\cdot\\mathbb{I}\\{Z\\leq t\\}\\right]\\leq t P(Z\\leq t),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $Z=\\left|f^{*}(x_{t})-{\\textstyle{\\frac{1}{2}}}\\right|$ , so we have $\\mathcal{E}_{t}(\\hat{f})\\leq C_{\\beta}t^{\\beta+1}$ . To bound $\\Big|\\hat{f}_{c a c h e}(x_{t})-f^{*}(x_{t})\\Big|,$ , we denote $(\\hat{x_{t}}^{(i)},\\hat{y_{t}}^{(i)})$ as the $i_{t h}$ nearest data and the corresponding labels to $x_{t}$ in $B(x_{t},r_{0})$ . The result of the cache classfier with normalized weight will be ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\hat{f}_{c a c h e}(x_{t})=\\displaystyle\\sum_{i=1}^{k_{t}}\\frac{1}{\\sum_{j=1}^{k_{t}}\\left[g\\left(\\hat{x_{t}}^{(j)}\\right)^{T}g(x)\\right]}\\left[g\\left(\\hat{x_{t}}^{(i)}\\right)^{T}g(x)\\right]\\hat{y_{t}}^{(i)}}\\\\ {\\displaystyle=\\sum_{i=1}^{k_{t}}w_{i}\\hat{y_{t}}^{(i)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where wi = $\\begin{array}{r}{w_{i}=\\frac{g\\left(\\hat{x_{t}}^{(i)}\\right)^{T}g\\left(x\\right)}{\\sum_{j=1}^{k_{t}}\\left[g\\left(\\hat{x_{t}}^{(j)}\\right)^{T}g\\left(x\\right)\\right]}}\\end{array}$ is the normalized weight and $\\textstyle\\sum_{i=1}^{k_{t}}w_{i}=1$ . Based on the assumptions and notions above, we have for any $x_{t}\\in p_{t}(x)$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\displaystyle\\hat{f}_{c a c h e}(x_{t})-f^{*}(x_{t})\\right|=\\left|\\displaystyle\\sum_{i=1}^{k_{t}}w_{i}\\hat{y}_{t}^{(i)}-f^{*}(x_{t})\\right|}\\\\ &{\\phantom{\\leq\\leq\\left|\\displaystyle\\sum_{i=1}^{k_{t}}w_{i}\\hat{y}_{t}^{(i)}-\\sum_{i=1}^{k_{t}}w_{i}f^{*}\\left(\\hat{x}_{t}^{(i)}\\right)\\right|+\\left|\\displaystyle\\sum_{i=1}^{k_{t}}w_{i}f^{*}\\left(\\hat{x}_{t}^{(i)}\\right)-f^{*}(x_{t})\\right|}\\\\ &{\\phantom{\\leq\\left|\\displaystyle\\sum_{i=1}^{k_{t}}w_{i}\\hat{y}_{t}^{(i)}-\\sum_{i=1}^{k_{t}}f^{*}\\left(\\hat{x}_{t}^{(i)}\\right)\\right|+\\displaystyle\\sum_{i=1}^{k_{t}}w_{i}\\left|f^{*}\\left(\\hat{x}_{t}^{(i)}\\right)-f^{*}(x_{t})\\right|,}\\\\ &{\\phantom{\\leq\\left|\\displaystyle\\sum_{i=1}^{k_{t}}w_{i}\\hat{y}_{t}^{(i)}-\\sum_{i=1}^{k_{t}}f^{*}\\left(\\hat{x}_{t}^{(i)}\\right)\\right|+\\underbrace{\\sum_{i=1}^{k_{t}}w_{i}\\left|f^{*}\\left(\\hat{x}_{t}^{(i)}\\right)-f^{*}(x_{t})\\right|}_{\\textcircled{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\textcircled{2}$ is easy to bound. According to the assumption that $f^{*}$ is $C$ -Smoothness, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{k_{t}}w_{i}\\left|f^{*}\\left(\\hat{x_{t}}^{(i)}\\right)-f^{*}(x_{t})\\right|\\leq\\sum_{i=1}^{k_{t}}w_{i}C\\cdot\\Vert\\mathbf{\\Psi}\\hat{x_{t}}^{(i)}-x_{t}\\Vert\\leq C\\cdot\\Vert\\mathbf{\\Psi}\\hat{x_{t}}^{(k_{t})}-x_{t}\\Vert\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "According to Eq.(31), with probability at least $1-\\exp(-k_{t}/4)$ , $\\begin{array}{r}{\\textcircled{2}\\ \\leq\\ C\\left(\\frac{2k_{t}}{m c_{t}\\pi_{d}n_{t}}\\right)^{1/d}}\\end{array}$ . Note that We store the target sample into the cache only when its prediction confidence is large enough. Therefore, it is natural to assume that: ", "page_idx": 19}, {"type": "equation", "text": "$$\nE_{Y|X}\\left[{\\hat{y_{t}}}^{(i)}\\right]=f^{*}(x_{t}^{(i)}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then we use the Hoeffding inequality to obtain the upper bound of $\\textcircled{1}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle P_{X,Y}\\left(\\left|\\sum_{i=1}^{k_{t}}w_{i}\\hat{y}_{t}^{(i)}-\\sum_{i=1}^{k_{t}}f^{*}(\\hat{x_{t}}^{(i)})\\right|>\\epsilon\\right)}\\\\ {\\displaystyle}&{\\qquad=\\mathbb{E}_{X}\\left[P_{Y|X}\\left(\\left|\\sum_{i=1}^{k_{t}}w_{i}\\hat{y}_{t}^{(i)}-\\sum_{i=1}^{k_{t}}f^{*}(\\hat{x_{t}}^{(i)})\\right|>\\epsilon\\right)\\right]}\\\\ {\\displaystyle}&{\\qquad\\le2\\exp(-\\frac{2\\epsilon^{2}}{\\sum_{i=1}^{k_{t}}w_{i}^{2}})}\\\\ {\\displaystyle}&{\\qquad\\approx2\\exp(-2\\eta k_{t}\\epsilon^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We simplify the bound by assuming that the weights in the target domain are evenly distributed in the subset of all samples with respect to a specific class controlled by coefficient $\\eta$ , according to Assumption 1 and Proposition 4. That is, we have $\\begin{array}{r}{\\sum_{i=1}^{k_{t}}w_{i}^{2}\\approx\\sum_{i=1}^{\\eta k_{t}}\\big(\\frac{1}{\\eta k_{t}}\\big)^{2}=\\frac{1}{\\eta k_{t}},}\\end{array}$ . ", "page_idx": 19}, {"type": "text", "text": "Set $\\epsilon=(1/k_{t})^{1/4}$ , we have, with probability, at least $1-3\\exp(-2\\eta\\sqrt{k_{t}})$ , $\\textcircled{1}\\leq(1/k_{t})^{1/4}$ , $\\circled{2}\\leq$ $\\begin{array}{r}{C\\left({\\frac{2k_{t}}{m c_{t}\\pi_{d}n_{t}}}\\right)^{1/d}}\\end{array}$ , and then  f\u02c6cache(xt) \u2212f \u2217(xt)  \u2264(1/kt)1/4 + C mc2k\u03c0tn 1/d . According to Eq.(31) and Eq.(35), the excess error is bounded by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathcal E_{t}(\\hat{f})\\leq2C_{\\beta}\\left(\\left(\\frac{1}{k_{t}}\\right)^{1/4}+C\\left(\\frac{2k_{t}}{m c_{t}\\pi_{d}n_{t}}\\right)^{1/d}\\right)^{1+\\beta}}}\\\\ {{\\displaystyle\\approx\\left(\\left(\\frac{1}{k_{t}}\\right)^{1/4}+C_{1}\\left(\\frac{k_{t}}{c_{t}n_{t}}\\right)^{1/d}\\right)^{1+\\beta},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with constant $C_{1}$ . When appropriately choosing $k_{t}=\\mathcal{O}(\\log n_{t})$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{min}\\{1-2\\exp(-2\\eta\\sqrt{k_{t}}),1-\\exp(-k_{t}/4)\\}}\\\\ &{\\geq1-2\\exp(-2\\eta\\sqrt{k_{t}})-\\exp(-k_{t}/4)}\\\\ &{\\geq1-3\\exp(-2\\eta\\sqrt{k_{t}})}\\\\ &{=1-3\\exp(-\\mathcal{O}(1)\\sqrt{\\log n_{t}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the thir\u221ad line is because $k_{t}/4>2\\eta\\sqrt{k_{t}}$ for large enough $k_{t}$ . Namely, with probability at least $1-3\\exp(-\\sqrt{\\log n_{t}})^{\\mathcal{O}(1)}$ , the following bound holds true. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{E}_{t}(\\hat{f})\\leq\\mathcal{O}\\left(\\left(\\frac{1}{\\log n_{t}}\\right)^{1/4}+\\left(\\frac{\\log n_{t}}{c_{t}n_{t}}\\right)^{1/d}\\right)^{1+\\beta},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "C.3 Historical Cache benefits from Boosting Samples (Proof of Proposition 3) ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "To study the effect of the boosting samples, we consider the cache classfier containing both $k_{t}$ historical samples $\\{\\hat{x_{t}}^{(i)},\\hat{y_{t}}^{(i)}\\}_{i=1}^{k_{t}}$ and $k_{b}$ boosting samples $\\{\\hat{x_{b}}^{(i)},\\hat{y_{b}}^{(i)}\\}_{i=1}^{k_{b}}$ as the nearest data to $x_{t}$ in $B(x_{t},r_{0})$ . With the normalized weights wti = $\\begin{array}{r}{w_{t i}=\\frac{\\mathbf{\\boldsymbol{\\omega}}}{\\sum_{j=1}^{k_{t}}\\left[g\\left(\\hat{x_{t}}^{(j)}\\right)^{T}g\\left(x\\right)\\right]+\\sum_{j=1}^{k_{b}}\\left[g\\left(x_{b}^{(j)}\\right)^{T}g\\left(x\\right)\\right]}}\\end{array}$ $g\\Big(\\hat{x_{t}}^{(i)}\\Big)^{T}g(x)$ and $\\begin{array}{r}{w_{b i}=\\frac{s\\mid\\cdots\\,s\\mid\\cdots\\,\\mid\\cdots\\,s\\mid\\cdots\\,}{\\sum_{j=1}^{k_{t}}\\left[g\\left(\\hat{x_{t}}^{(j)}\\right)^{T}g\\left(x\\right)\\right]+\\sum_{j=1}^{k_{b}}\\left[g\\left(\\hat{x_{b}}^{(j)}\\right)^{T}g\\left(x\\right)\\right]}}\\end{array}$ $g\\Big(\\hat{x_{b}}^{(i)}\\Big)^{T}g(x)$ , the prediction result of the cache classifier will be $\\begin{array}{r}{\\hat{f}_{c a c h e}(x_{t})=\\sum_{i=1}^{k_{t}}w_{t i}\\hat{y_{t}}^{(i)}+\\sum_{i=1}^{k_{b}}w_{b i}y_{b}^{(i)}}\\end{array}$ yb(i ). Then we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\int_{\\omega\\times\\mathbf R}(x)-f^{*}(x)\\right|}\\\\ &{=\\left|\\displaystyle\\sum_{i=1}^{k}w_{i i}f^{*}(x)-\\frac{k}{\\omega_{i}}w_{j}f^{*}(x_{i})+\\sum_{i=1}^{k}w_{i i}g_{i}^{\\prime}-\\frac{k}{\\omega_{i}}w_{j}f^{*}(x_{i})\\right|}\\\\ &{\\le\\left|\\left[\\displaystyle\\sum_{i=1}^{k}w_{i i}f^{*}(x_{i})^{(i)}-\\frac{k}{\\omega_{i}}w_{i}f^{*}(x_{i})^{(i)}\\right]+\\left[\\displaystyle\\sum_{i=1}^{k}w_{i i}f^{*}(x_{i}^{(i)})-\\frac{k}{\\omega_{i}}w_{i}f^{*}(x_{i})\\right]\\right|}\\\\ &{\\phantom{=\\;}+\\left[\\displaystyle\\sum_{i=1}^{k}w_{i i}g_{i}^{\\prime}(x)-\\frac{k}{\\omega_{i}}w_{i}f^{*}\\left(x_{i}^{(i)}\\right)\\right]+\\left[\\displaystyle\\sum_{i=1}^{k}w_{i i}f^{*}\\left(x_{i}^{(i)}\\right)-\\frac{k}{\\omega_{i}}w_{i}f^{*}(x_{i})\\right]}\\\\ &{\\le\\left|\\displaystyle\\sum_{i=1}^{k}w_{i i}f^{*}(x_{i}^{(i)})-\\frac{k}{\\omega_{i}}w_{i}f^{*}\\left(x_{i}^{(i)}\\right)\\right|+\\left[\\displaystyle\\sum_{i=1}^{k}w_{i i}f^{*}\\left(x_{i}^{(i)}\\right)-\\displaystyle\\sum_{i=1}^{k}w_{i i}f^{*}(x_{i})\\right]}\\\\ &{\\le\\left|\\displaystyle\\sum_{i=1}^{k}w_{i i}\\hat{w}_{i}^{(i)}+\\sum_{i=1}^{k}w_{i i}w_{i}^{(i)}-\\frac{k}{\\omega_{i}}w_{i}f^{*}(x_{i}^{(i)})-\\sum_{i=1}^{k}w_{i i}f^{*}\\left(x_{i}^{(i)}\\right)\\right|}\\\\ &{\\phantom{=\\;}+\\displaystyle\\sum_{i=1}^{k}w_{i i}\\left|f^{*}(x_{i}^{(i)})-f^{*}(x_{i})\\right|+\\sum_{i=1}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Similar to Eq.(40), we have the following assumption on the boosting distribution: ", "page_idx": 20}, {"type": "equation", "text": "$$\nE_{Y|X}\\left[\\hat{y_{b}}^{(i)}\\right]=f^{*}(x_{b}^{(i)}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "According to Eq.(41), we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{X,Y}\\left(\\left|\\displaystyle\\sum_{i=1}^{k_{t}}w_{t i}\\hat{y}_{t}^{(i)}+\\sum_{i=1}^{k_{b}}w_{b i}y_{b}^{(i)}-\\sum_{i=1}^{k_{t}}w_{t i}f^{*}(\\hat{x}_{t}^{(i)})-\\displaystyle\\sum_{i=1}^{k_{b}}w_{b i}f^{*}\\left(x_{b}^{(i)}\\right)\\right|\\right)}\\\\ &{=\\mathbb{E}_{X}\\left[P_{Y|X}\\left(\\left|\\displaystyle\\sum_{i=1}^{k_{t}}w_{t i}\\hat{y}_{t}^{(i)}+\\sum_{i=1}^{k_{b}}w_{b i}y_{b}^{(i)}-\\sum_{i=1}^{k_{t}}w_{t i}f^{*}(\\hat{x}_{t}^{(i)})-\\sum_{i=1}^{k_{b}}w_{b i}f^{*}\\left(x_{b}^{(i)}\\right)\\right|\\right)\\right]}\\\\ &{\\leq2\\exp(-2\\eta(k_{t}+k_{b})\\epsilon^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Set $\\epsilon=(1/(k_{t}+k_{b}))^{1/4}$ , we have, with probability, at least $1-3\\exp(-2\\eta\\sqrt{(k_{t}+k_{b})})$ , $\\textcircled{1}\\leq$ $(1/(k_{t}+k_{b}))^{1/4}$ . Then, according to Eq.(39), we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{k_{t}}w_{t i}\\left|f^{*}\\left(\\hat{x_{t}}^{(i)}\\right)-f^{*}(x_{t})\\right|\\leq\\sum_{i=1}^{k_{t}}w_{t i}C\\cdot\\Vert\\mathbf{\\mathcal{\\hat{x}}}_{t}^{(i)}-x_{t}\\Vert\\leq S_{t}C\\cdot\\Vert\\mathbf{\\mathcal{\\hat{x}}}_{t}^{(k_{t})}-x_{t}\\Vert\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{k_{b}}w_{b i}\\left|f^{*}\\left(\\hat{x_{b}}^{(i)}\\right)-f^{*}(x_{t})\\right|\\leq\\sum_{i=1}^{k_{b}}w_{b i}C\\cdot\\left\\Vert\\mathbf{\\Omega}\\hat{x_{b}}^{(i)}-x_{t}\\right\\Vert\\leq S_{b}C\\cdot\\left\\Vert\\mathbf{\\Omega}\\hat{x_{b}}^{(k_{b})}-x_{t}\\right\\Vert.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where samples, respectively, and we have $\\textstyle S_{t}=\\sum_{i=1}^{k_{t}}w_{t i}$ , $\\begin{array}{r}{S_{b}=\\sum_{i=1}^{k_{b}}w_{b i}}\\end{array}$ ${\\bf\\dot{\\cal S}}_{t}+{\\cal S}_{b}=1$ are the sum of weights of historical samples and boosting . ", "page_idx": 21}, {"type": "text", "text": "Then we have the following results in similar: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\textcircled{2}\\,\\leq\\,S_{t}C\\left(\\frac{2k_{t}}{m c_{t}\\pi_{d}n_{t}}\\right)^{1/d};\\,\\textcircled{3}\\,\\leq\\,S_{b}C\\left(\\frac{2k_{b}}{m c_{b}\\pi_{d}n_{b}}\\right)^{1/d}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Finally, the excess error under the covariate shift setting can be bounded by ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathcal{E}_{t}(\\hat{f})\\leq2C_{\\beta}\\left((\\frac{1}{k_{t}+k_{b}})^{1/4}+S_{t}C\\left(\\frac{2k_{t}}{m c_{t}\\pi_{d}n_{t}}\\right)^{1/d}+S_{b}C\\left(\\frac{2k_{b}}{m c_{b}\\pi_{d}n_{b}}\\right)^{1/d}\\right)^{1+\\beta}}\\\\ {\\displaystyle\\approx\\left(\\left(\\frac{1}{k_{t}+k_{b}}\\right)^{1/4}+C_{1}S_{t}\\left(\\frac{k_{t}}{c_{t}n_{t}}\\right)^{1/d}+C_{1}S_{b}\\left(\\frac{k_{b}}{c_{b}n_{b}}\\right)^{1/d}\\right)^{1+\\beta}}\\\\ {\\displaystyle=\\left(\\left(\\frac{1}{k_{t}+k_{b}}\\right)^{1/4}+C_{1}\\sum_{i=1}^{k_{t}}w_{t i}\\left(\\frac{k_{t}}{c_{t}n_{t}}\\right)^{1/d}+C_{1}\\sum_{i=1}^{k_{b}}w_{b i}\\left(\\frac{k_{b}}{c_{b}n_{b}}\\right)^{1/d}\\right)^{1+\\beta}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Compared Eq.(50) to Eq.(42) and $S_{t}+S_{b}=1$ , it is easy to verify that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(S_{t}+S_{b})C\\left(\\frac{2(k_{t}+k_{b})}{m c_{t}\\pi_{d}n_{t}}\\right)^{1/d}-S_{t}C\\left(\\frac{2k_{t}}{m c_{t}\\pi_{d}n_{t}}\\right)^{1/d}-S_{b}C\\left(\\frac{2k_{b}}{m c_{b}\\pi_{d}n_{b}}\\right)^{1/d}}\\\\ &{\\geq S_{b}C\\left(\\frac{2k_{t}}{m c_{t}\\pi_{d}n_{t}}\\right)^{1/d}-S_{b}C\\left(\\frac{2k_{b}}{m c_{b}\\pi_{d}n_{b}}\\right)^{1/d}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In general, the boosting distribution is more close to the test sample than the target distribution and we have $c_{b}>c_{t}$ . Thus the difference in Eq.(51) is then larger than 0, namely incorporating boosting samples into the memory bank, the excess error can be further reduced. ", "page_idx": 21}, {"type": "text", "text": "D More Experiments ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Independent Cache for Boosting Samples. In BoostAdater, due to the cost of augmentation, the number of boosting samples is relatively smaller than the number of historical samples. Therefore, we use a joint cache for storing both historical and boosting samples to facilitate intra-sample and inter-sample interactions. Table 10 and Table 11 study the influence of using an independent cache for the boosting samples. As can be observed from the results, BoostAdapter suffers from slight performance degradation due to the independent cache. ", "page_idx": 21}, {"type": "table", "img_path": "8tOYl6WsGY/tmp/f99ce31b85898ae64074d53324cab2ea7182bee2cf9de0cbaceb61ccaa8342bb.jpg", "table_caption": ["Table 10: Independent cache for boosting samples on the OOD benchmark. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "8tOYl6WsGY/tmp/43c52c4450da039a39224955fd3d15641b3911f9ea07ad4b73f4ace0838a35b8.jpg", "table_caption": ["Table 11: Independent cache for boosting sample on the Cross-Domain Benchmark. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Different Augmentation for Boosting Samples. We make use of random crop followed by random horizontal flip as augmentations for generating boosting samples. Additionally, we further explore the influences of different augmentations applied to the randomly cropped images. The comparison methods include: (i) Random Brighness: Randomly set the brighness of image from $50\\%$ to $150\\%$ . (ii) Random Auto Contrast: Apply auto contrast over image with probability $p=0.5$ . (iii) Random Rotate: Randomly rotate the image from -45 degree to 45 degree. (iv) Random Vertical Flip: Apply vertical filp over image with probability $p=0.5$ . (v) Random Horizontal Flip (BoostAdapter): Apply horizontal flip over image with probability $p=0.5$ . The results are presented in Table 12 and Table 13. The results indicate that random horizontal flipping outperforms other augmentation methods, primarily because the images generated from horizontal flips are closer to the original distribution when training CLIP. ", "page_idx": 22}, {"type": "table", "img_path": "8tOYl6WsGY/tmp/7adecf281db6c9c21489623fba50d7081321de00e4928a775c76f749e29dbf9f.jpg", "table_caption": ["Table 12: Comparison of different augmentations on the OOD benchmark . Default settings are marked in gray . "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "8tOYl6WsGY/tmp/3b48a72f94f921a1c01b3e9b2d5c1ae0aa8d46175dc034b593ed1b57788e0ece.jpg", "table_caption": ["Table 13: Comparison of different augmentations on the Cross-Domain Benchmark. Default settings are marked in gray . "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "E Additional Ablation Results ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Historical Samples and Boosting Samples. We provide more ablation results of the historical and boosting samples on the Cross-Dataset benchmark in Table 14. The observation is consistent with the results in Table 3, showing that CLIP gains improvements from both historical and boosting samples. Furthermore, when applied to various downstream tasks, the importance of regional bootstrapping becomes more significant, as indicated by the gap between BoostAdapter and the variant that uses boosting samples only. ", "page_idx": 22}, {"type": "text", "text": "Number of Augmented Views for Boosting Samples. The complete results on the number of augmented views are presented in Table 15 and Table 16. With more augmented views, BoostAdapter is able to better extract the fine-grained information from the original test sample, achieving improved performance. ", "page_idx": 22}, {"type": "table", "img_path": "8tOYl6WsGY/tmp/c75833028eb61164a36ada513acb462cbc70d6d157256c47b117014ac116ddbf.jpg", "table_caption": ["Table 14: Ablation study on historical samples and boosting sample on the Cross-Domain Benchmark. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "8tOYl6WsGY/tmp/d2602264e89c6e8df3e7c624d7e506deef554a1735517e317ff420c3837fd6a0.jpg", "table_caption": ["Table 15: Results of different views on the OOD benchmark. Default settings are marked in gray "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "8tOYl6WsGY/tmp/83f9396949c66f5f43a27ce93b5d499c0ec6c80dfdd47126b314e692b40f1504.jpg", "table_caption": ["Table 16: Results of different views on the Cross-Domain Benchmark. Default settings are marked in gray . "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Fixed shot capacity. We search the optimal total shot capicity in BoostAdapter. We also find that fixing the cache size to be 3 can generalize well in different task settings, as shown in Table 17 and Table 18. ", "page_idx": 23}, {"type": "text", "text": "F More Qualitative Results ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "More qualitative results are provided in Fig. 6. ", "page_idx": 23}, {"type": "table", "img_path": "8tOYl6WsGY/tmp/5461dba1a369c283abe28fb65b69c7d82b7b8d43f0ba3ddbd3f0c1cf35c1ef64.jpg", "table_caption": ["Table 17: Results of fixed shot capacity on the OOD benchmark. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "8tOYl6WsGY/tmp/43a343faea8e4ef39fea5c091bf39c3e14b8d67ef8ca31ab2a0c6d8996d651ba.jpg", "table_caption": ["Table 18: Results of fixed shot capacity on the Cross-Domain Benchmark. "], "table_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "8tOYl6WsGY/tmp/2cd39fc805d59d292f28a3f806ecaecee647b9e5ce1e755b7c9d1d8b90f8485e.jpg", "img_caption": ["Figure 6: More qualitative results on ImagNet-A, Aircraft and EuroSAT. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction reflect the main idea described in Section 3. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The limitation and the discussion about computational overhead can be found in the Section 5. These assumptions are reasonable in domain adaptation and parameter analysis is conduct in the ablation studies in Section 4.4. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and   \na complete (and correct) proof?   \nAnswer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The theoretical proof of the proposition used in the paper can be found in Section C. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include theoretical results. ", "page_idx": 25}, {"type": "text", "text": "\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We provide the implementation details in Section 4.1 for reproduction. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We will open source the code once accepted. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. \u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not ", "page_idx": 26}, {"type": "text", "text": "including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We provide the implementation details in Section 4.1. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate   \ninformation about the statistical significance of the experiments?   \nAnswer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification:We provide the error bound along with the main results. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Justification: We provide the information about compute resources in the implementation details in Section 4.1. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The research conducted in the paper conform with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide discussions of broader impacts in Section B in Appendix Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible   \nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,   \nimage generators, or scraped datasets)?   \nAnswer: [NA]   \nJustification: [NA]   \nGuidelines: \u2022 The answer NA means that the paper poses no such risks. \u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Justification: We mention the licenses of existing assets in the Section A in Appendix. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 30}, {"type": "text", "text": "uidelines:   \n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]