[{"type": "text", "text": "Theoretical guarantees in KL for Diffusion Flow Matching ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Marta Gentiloni Silveri Ecole polytechnique Route de Saclay, 91120 Palaiseau, France marta.gentiloni-silveri@polytechnique.edu ", "page_idx": 0}, {"type": "text", "text": "Giovanni Conforti ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Alain Durmus ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Universita degli Studi di Padova Via Trieste, 63, 35131 Padova, Italia giovanni.conforti@math.unipd.it ", "page_idx": 0}, {"type": "text", "text": "Ecole polytechnique Route de Saclay, 91120 Palaiseau, France alain.durmus@polytechnique.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Flow Matching (FM) (also referred to as stochastic interpolants or rectified flows) stands out as a class of generative models that aims to bridge in finite time the target distribution $\\nu^{\\star}$ with an auxiliary distribution $\\mu$ , leveraging a fixed coupling $\\pi$ and a bridge which can either be deterministic or stochastic. These two ingredients define a path measure which can then be approximated by learning the drift of its Markovian projection. The main contribution of this paper is to provide relatively mild assumptions on $\\nu^{\\star}$ \uff0c $\\mu$ and $\\pi$ to obtain non-asymptotics guarantees for Diffusion Flow Matching (DFM) models using as bridge the conditional distribution associated with the Brownian motion. More precisely, we establish bounds on the Kullback-Leibler divergence between the target distribution and the one generated by such DFM models under moment conditions on the score of $\\nu^{\\star}$ \uff0c $\\mu$ and $\\pi$ , and a standard $\\mathrm{L^{2}}$ -drift-approximation error assumption. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A significant task in statistics and machine learning currently revolves around generating samples from a target distribution that is only accessible via a dataset. To tackle this challenge, generative models have become prominent as effective computational tools for learning to simulate new data. Essentially, these models involve learning a generator capable of mapping a source distribution into new approximate samples from the target distribution. ", "page_idx": 0}, {"type": "text", "text": "One of the most productive approaches to generative modeling is based on deterministic and stochastic transport dynamics, that connect a target distribution with a base distribution. Typically, the target distribution represents the data set from which we want to generate new samples, while the base distribution is one that can be easily simulated or sampled. Regarding the dynamics, they correspond to SDEs Stochastic Differential Equations (SDEs) or Ordinary Differential Equations (ODEs), where the drift (for SDEs) or velocity field (for ODEs) is determined by solving a regression problem. This regression problem is usually addressed with appropriate neural networks and related training techniques [SDWMG15a, OFLR21,FJNO20,DBMP19, CLT22,DBTHD21, SE19, $\\mathrm{LYB}^{+}23$ ,GCBD19]. ", "page_idx": 0}, {"type": "text", "text": "Among these methods, score-base generative models (SGMs) and in particular diffusion models based on score matching [SDWMG15b, HJA20, SE20, SE19] was an important milestone. In a nutshell, these models involve transforming an arbitrary density into a standard Gaussian model and consists in learning the drift of the corresponding reversal process. More precisely, the idea is to first consider an Ornstein-Uhlenbeck (OU) process $(X_{t}^{\\mathrm{OU}})_{t\\in[0,T]}$ , over a time interval $[0,T]$ ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathrm{d}X_{t}^{\\mathrm{OU}}=-(1/2)X_{t}^{\\mathrm{OU}}\\mathrm{d}t+\\mathrm{d}B_{t}\\;,\\quad X_{0}^{\\mathrm{OU}}\\sim\\nu^{\\star}\\;,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Where $(B_{t})_{t\\geqslant0}$ is a $d_{\\cdot}$ -dimensional Brownian motion. Then, the reversal process $(\\overleftarrow{X}_{t}^{\\mathrm{OU}})_{t\\in[0,T]}$ which is defined from the non-homogeneous SDE [And82]: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathrm{d}\\overleftarrow{X}_{t}^{\\mathrm{oU}}=\\{(1/2)\\overleftarrow{X}_{t}^{\\mathrm{oU}}+\\nabla\\log p_{T-t}^{\\mathrm{oU}}(\\overleftarrow{X}_{t}^{\\mathrm{oU}})\\}\\mathrm{d}t+\\mathrm{d}B_{t}\\;,\\quad t\\in[0,T]\\quad\\;,\\;\\mathrm{with}\\;\\overleftarrow{X}_{0}^{\\mathrm{oU}}\\sim\\nu^{\\star}P_{T}^{\\mathrm{OU}}\\;,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "allows the law of $X_{T}^{\\mathrm{OU}}$ to be transported to $\\nu^{\\star}$ :[And82,Equations 3312] shw that $\\overleftarrow{X}_{T}^{\\mathrm{OU}}$ has distribution $\\nu^{\\star}$ The initialization $\\nu^{\\star}P_{T}^{\\mathrm{OU}}$ in (2) is the distribution of $X_{T}^{\\mathrm{OU}}$ defined in (1). The drift in (2) can be decomposed as the sum of a linear function and the score associated with the densityof $X_{T-t}^{\\mathrm{OU}}$ Wwithrspeeto the Lebesguemeasure denotedby pT. From the Tweedie identity, this score is the solution to a regression problem that can be solved efficiently by score matching [HD05, Hyv05, Vin11]. Learning this score at different times can also be formulated as a sequence of denoising problems. Once the drift of the reversal process is learned or equivalently the scores $(\\nabla\\log p_{t}^{\\mathrm{OU}}\\big)_{t\\in[0,T]}$ score-based generative models consist in following the reversal dynamics over $[0,T]$ or, more commonly, a discretization of it, starting with a sample from ${\\mathrm{N}}(0,{\\mathrm{Id}})$ . The final sample at time $T$ is then approximatively distributed according to $\\nu^{\\star}$ . Note that an approximation is made even if the reversal dynamics were simulated exactly, because for full accuracy, the model would need to start from a sample of $\\nu^{\\star}P_{T}^{\\mathrm{OU}}$ . However, it is wel known that for sufficiently large $T$ \uff0c $\\nu^{\\star}P_{T}^{\\mathrm{OU}}$ is (exponentially) lose to ${\\mathrm{N}}(0,{\\mathrm{Id}})$ ", "page_idx": 1}, {"type": "text", "text": "When exploring diffusion models, it has been realized that the generation of approximate data samples could also be achieved using an ODE instead of the reversal diffusion: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathrm{d}\\overleftarrow{\\mathbf{x}}_{t}^{\\mathrm{OU}}/\\mathrm{d}t=(1/2)(\\overleftarrow{\\mathbf{x}}_{t}^{\\mathrm{OU}}+\\nabla\\log p_{T-t}^{\\mathrm{OU}}(\\overleftarrow{\\mathbf{x}}_{t}^{\\mathrm{OU}}))\\ .\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Similarly to the drift of the reversal diffusion, the velocity fields at time $t\\in[0,T]$ associated with this ODE is the sum of alinear function and the score of the density of $X_{T-t}^{\\mathrm{{OU}}}$ This observation has prompted the introduction of the Probability Flow ODE implementation of diffusion models $[\\mathrm{CCL}^{+}2\\bar{3}\\mathrm{a}]$ . SGMs in their standard and probability fow ODE implementations have achieved notable success in a range of applications; see e.g., $[\\mathrm{RBL}^{+}22$ $\\mathrm{RDN}^{+}22$ $\\mathrm{PVG}^{+}21]$ ", "page_idx": 1}, {"type": "text", "text": "While diffusion-based methods have now become popular generative models, they can suffer from two limitations. First, there is a trade-off in selecting the time horizon $T$ and second, they rely solely on Gaussian distributions as base distributions, in general. Therefore, there remains considerable interest in developing methods that consider a more general base distribution $\\mu$ and that accomplish the transport between $\\nu^{\\star}$ and $\\mu$ relying on dynamics defined on a fixed finite time interval. Defining a generative process in finite time by means of a coupling and an interpolating process, [Pel22], laid the foundation for Flow Matching (FM) models [AVE22, ABVE23, LCBH $^{+}\\bar{23}$ ,Liu22, LGL23], finally addressing this problem. ", "page_idx": 1}, {"type": "text", "text": "In its simplest form, the main strategy employed by FMs to bridge two distributions, involves a fixed coupling $\\pi$ between $\\nu^{\\star}$ and $\\mu$ and the use of a bridge, i.e., a conditional distribution on the path space $\\mathrm{C}([\\bar{0},1],\\mathbb{R}^{d})$ of a reference process $(R_{t})_{t\\in[0,1]}$ given its starting point $R_{0}$ and end point $R_{1}$ . In case $R_{t}$ is a deterministic function of $R_{0}$ and $R_{1}$ , we say that the bridge is deterministic and stochastic otherwise. As $(R_{t})_{t\\in[0,1]}$ corresponds to the solution of a stochastic differential equation, we coin the term Diffusion Flow Matching (DFM) to distinguish the latter case from the former one and focus on it. Then, this bridge and the coupling $\\pi$ between $\\nu^{\\star}$ and $\\mu$ define an interpolated process $(X_{t}^{\\mathrm{I}})_{t\\in[0,1]}$ , referred to as an interpolant, defined as $(X_{0}^{\\mathrm{I}},X_{1}^{\\mathrm{I}})\\sim\\pi$ and $(X_{t}^{\\mathrm{I}})_{t\\in[0,1]}$ given $(\\bar{X}_{0}^{\\mathrm{I}},X_{1}^{\\mathrm{I}})$ has the same conditional distribution as $(R_{t})_{t\\in[0,1]}$ given $(R_{0},R_{1})$ . However, $(X_{t}^{\\mathrm{I}})_{t\\in[0,1]}$ does not correspond in general to the distribution of a diffusion or even to the one of a Markov process. This characteristic poses a challenge when dealing with potential stochastic sampling procedures: indeed, similarly to SGMs, FMs and DFMs aim to design a Markov process that approximatively transport $\\mu$ to $\\nu^{\\star}$ . To address this issue, most works proposing FM and DFM models [SBCD23, LWYql23] rely on mimicking the marginal flow of the interpolated process $(X_{t}^{\\mathrm{I}})_{t\\in[0,1]}$ through a diffusion process known as the Markovian projection. A remarkable feature of this diffusion lies in the fact that its drift is also a solution of a regression problem that can be approximatively solved using only samples from the interpolant $(X_{t}^{\\mathrm{I}})_{t\\in[0,1]}$ Then, an approximate samples from $\\nu^{\\star}$ is obtained by following a discretization of the dynamics associated with the considered Markovian projection, starting with a sample of $\\mu$ ", "page_idx": 1}, {"type": "text", "text": "While there exists now an important literature on theoretical guarantees for SGMs [CDS23, CLL23, $\\mathrm{CCL}^{+}23\\mathrm{b}$ , PMM23, LLT23, Bor22a], only a few works have been considering FMs. In addition, up to our knowledge, these works on FMs only consider deterministic interpolants [AVE22, BDD23, GHJZ24]. The main objective of this paper is to fill this gap and to analyze DFMs using the $d$ -dimensional Brownian motion as reference process, in which case the bridge is simply the $d\\!.$ dimensional standard Brownian bridge. We provide theoretical guarantees, upper bounding the Kullback Leibler divergence between the target distribution and the one resulting from the DFM. Our results consider the two sources of error coming from the DFM model, namely drift-estimation and time-discretization. This pursuit underscores the significance of comprehending and quantifying the factors influencing the performance of DFMs, paving the way for further advancements in generative modeling techniques. ", "page_idx": 2}, {"type": "text", "text": "Our contribution. _ In this work, we analyze a DFM model using as bridge the $d$ -dimensional Brownian bridge and examine how it performs in two distinct scenarios: one without early-stopping and another with early-stopping. In our first main contribution Theorem 2, we establish an explicit and simple bound on the KL divergence between the data distribution and the distribution at the final time of the DFM model. We achieve our bound without early stopping, by assuming only (1) moment conditions on the target $\\nu^{\\star}$ and the base $\\mu$ $(\\mathbf{H}1)$ ; (2) integrability conditions on the scores associated with the data distribution $\\nu^{\\star}$ , the base distribution $\\mu$ and the coupling $\\pi$ $(\\mathbf{H}2)$ ; (3) a $\\mathrm{L^{2}}$ -drift-approximation error $(\\mathbf{H}3)$ (an assumption commonly considered in previous works). Note that condition (2) implies that $\\nu^{\\star}$ necessarily admits a density. We relax this condition in our second contribution. In Theorem 3, we establish an explicit bound on the KL divergence between a smoothed version of the target distribution and an early stopped version of the DFM model, assuming (1) and (3), but replacing the condition (2) by assuming (4) $\\pi=\\mu\\otimes\\nu^{\\star}$ and integrability conditions only on the score associated with $\\mu$ ", "page_idx": 2}, {"type": "text", "text": "To the best of our knowledge, our paper provides the first convergence analysis for diffusion-type FMs, that tackles all the sources of error, i.e., the drift-approximation-error and the time-discretization error. In addition, previous studies concerning FMs and Probability Flow ODEs, with deterministic or mixed sampling procedure, either rely on at least some Lipschitz regularity of the flow velocity field or its estimator and/or do not take the time-discretization error into consideration. Also, in the context of SGMs with constant step-size, most of existing works without early-stopping are obtained assuming either the score (or its estimator) to be Lipschitz or the data distribution to satisfy some additional conditions (e.g., manifold hypothesis, bounded support, etc.); the unique exception being [CDS23]. We refer to Section 3.2 for a more in depth literature comparison. ", "page_idx": 2}, {"type": "text", "text": "Notation.  Given a measurable space $(\\mathsf E,{\\mathcal E})$ , we denote by $\\mathcal{P}(\\mathsf{E})$ the set of probability measures of E. Also, given a topological space $(\\mathsf E,\\tau)$ , we use $\\beta(\\mathsf{E})$ to denote the Borel $\\sigma$ -algebra on E. Given two random variables $Y,{\\tilde{Y}}$ , we write $Y\\perp\\!\\!\\!\\!\\perp\\tilde{Y}$ to say that $Y$ and $\\tilde{Y}$ are independent. Denote by $(B_{t})_{t\\in[0,1]}$ a $d_{\\cdot}$ -dimensional Brownian motion. We denote by $\\mathrm{Leb}^{d}$ the Lebesgue measure on $\\mathbb{R}^{d}$ . Given two real numbers $u,v\\in\\mathbb{R}$ , we write $u\\lesssim v$ (resp. $u\\gtrsim v)$ to mean $u\\leq C v$ (resp. $u\\geq C v)$ for a universal constant $C>0$ . Also, we denote by $\\|x\\|$ the Euclidean norm of $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , by $\\langle x,y\\rangle$ the scalar product between $x,y\\in\\mathbb{R}^{d}$ and by $x^{\\mathrm{{T}}}$ the ranspose of $x$ . Given a matrix $\\mathbf{A}\\in\\mathbb{R}^{d\\times s}$ ,we denote by $\\|\\mathbf{A}\\|_{\\mathrm{op}}$ the operator norm of A. For $f:[0,1]\\times\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ regular enough, we denote by $\\nabla_{x}f(t,x),\\nabla_{x}^{2}f(t,x)$ and $\\Delta_{x}f(t,x)$ respectively the gradient, hessian and laplacian of $f$ defined for $t,x\\in[0,1]\\times\\ensuremath{\\mathbb{R}}^{d}$ by $\\begin{array}{r}{\\nabla_{x}f(t,x):=(\\partial_{x_{i}}f(t,x))_{i},\\nabla_{x}^{2}f(t,x):=(\\partial_{x_{i}}\\partial_{x_{j}}f(t,x))_{i,j},\\Delta f(t,x):=\\sum_{i=1}^{d}\\partial_{x_{i}}^{2}f(t,x)}\\end{array}$ where $\\partial_{x_{j}}$ denotes the partial derivative with respect to the $j$ -th variable. For $F:[0,1]\\times\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d}$ regular enough, we denote by $D_{x}F$ $\\operatorname{div}_{x}F$ and $\\Delta_{x}F$ respectively, the Jacobian matrix, the divergence and the vectorial laplacian of $F$ , defined for $t,x\\in[0,1]\\stackrel{.}{\\times}\\mathbb{R}^{d}$ by $D_{x}F(t,x)\\,=\\,(\\partial_{x_{j}}F_{i}(t,x))_{i,j}$ $\\begin{array}{r}{\\mathrm{div}_{x}\\,F(t,x):=\\sum_{j=1}^{d}\\partial_{x_{j}}F_{j}(t,x),\\,\\Delta_{x}F(t,x)=\\big(\\Delta_{x}F_{1}(t,x),...,\\Delta_{x}F_{d}(t,x)\\big).}\\end{array}$ ", "page_idx": 2}, {"type": "text", "text": "2   Diffusion Flow Matching. ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given a target distribution $\\nu^{\\star}\\in\\mathcal{P}(\\mathbb{R}^{d})$ and a base distribution $\\mu\\,\\in\\,\\mathcal{P}(\\mathbb{R}^{d})$ , the idea at the core of FM models is intuitively to construct a path between these two by considering two ingredients (1) a coupling $\\pi$ and (2) a bridge (or an interpolant following [AVE22]) between $\\mu$ and $\\nu^{\\star}$ (or more precisely a bridge with foundations $\\pi$ ). More formally, here we say that $\\pi$ is a coupling between $\\mu$ and $\\nu^{\\star}$ if for any. $\\mathsf{A}\\,\\in\\,B(\\mathbb{R}^{d})$ \uff0c $\\pi(\\mathsf{A}\\times\\mathbb{R}^{d})\\:=\\:\\mu(\\mathsf{A})$ and $\\pi(\\mathbb{R}^{d}\\,\\times\\,\\mathsf{A})\\;=\\;\\nu^{\\star}(\\mathsf{A})$ , and denote by $\\Pi(\\mu,\\nu^{\\star})$ the set of coupling between $\\mu$ and $\\nu^{\\star}$ . Then, based on a probability measure on $\\mathbb{W}\\,=\\,\\mathrm{C}([0,1]\\,,\\mathbb{R}^{d})$ the set of continuous functions from $[0,1]$ to $\\mathbb{R}^{d}$ , we define the bridge $\\mathrm{b}\\mathbb{Q}$ associated with $\\mathbb{Q}$ as the Markov kernel $\\mathrm{b}\\mathbb{Q}$ on $\\mathbb{R}^{2d}\\,\\times\\,\\bar{\\mathbb{W}}$ , such that, for any $\\textsf{A}\\in\\ B(\\mathbb{W})$ \uff0c $\\begin{array}{r}{\\mathbb{Q}({\\boldsymbol{\\mathsf{A}}})=\\int\\mathbb{Q}_{0,1}\\mathrm{d}(x_{0},x_{1})\\mathrm{b}\\mathbb{Q}((x_{0},x_{1}),\\mathsf{A})}\\end{array}$ (see e.g., [Kle13, Theorem 8.37] for the existence of this kernel), where for any $1=\\{t_{1},\\dots,t_{n}\\}\\subset[0,1]$ \uff0c $t_{1}<\\cdot\\cdot\\cdot<t_{n},\\mathbb{Q}$ is the I-marginal distribution of $\\mathbb{Q}$ , i.e., the pushforward measure of $\\mathbb{Q}$ by $(x_{t})_{t\\in[0,1]}\\,\\mapsto\\,\\left(x_{t_{1}},\\ldots,x_{t_{n}}\\right)$ . From a probabilistic perspectiv,mls t $(W_{t})_{t\\in[0,1]}\\sim\\mathbb{Q}$ then $\\mathrm{b}\\mathbb{Q}$ is a conditional distribution of $(W_{t})_{t\\in[0,1]}\\sim$ $\\mathbb{Q}$ given $(W_{0},W_{1})$ : for any bounded and measurable function on W, $\\mathbb{E}[f((W_{t})_{t\\in[0,1]})|X_{0},\\dot{X}_{1}]=$ $\\begin{array}{r}{\\int f((w_{t})_{t\\in[0,1]})\\mathrm{b}\\mathbb{Q}((W_{0},W_{1}),\\mathrm{d}(w_{t})_{t\\in[0,1]})}\\end{array}$ ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "2.1  Definition of the interpolated process ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Consider now a coupling $\\pi$ and a bridge $\\mathrm{b}\\mathbb{Q}^{\\beta}$ associated to $\\mathbb{Q}^{\\beta}\\in\\mathcal{P}(\\mathbb{W})$ . We suppose here that $\\mathbb{Q}^{\\beta}$ is the distribution of $(Y_{t})_{t\\in[0,1]}$ solution of the stochastic differential equation ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}Y_{t}=\\beta(Y_{t})\\mathrm{d}t+\\sqrt{2}\\mathrm{d}B_{t}\\;,\\quad t\\in[0,1]\\;,\\quad Y_{0}\\sim\\mu\\in\\mathcal{P}(\\mathbb{R}^{d})\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $(B_{t})_{t\\in\\mathbb{R}_{+}}$ is a standard $d$ -dimensional Brownian motion. In addition, we suppose $\\beta\\ \\in$ $\\mathrm{C}^{\\infty}(\\mathbb{R}^{d},\\mathbb{R}^{d})$ for simplicity and that (3) admits a unique strong solution. Consider now, the interpolated measure $\\mathbb{I}(\\pi,\\mathbb{Q}^{\\beta})^{1}$ corresponding to the distribution of the process defined by $(X_{0}^{\\mathrm{I}},X_{1}^{\\mathrm{I}})\\sim\\pi$ and $(X_{t}^{\\mathrm{I}})_{t\\in[0,1]}|(X_{0}^{\\mathrm{I}},X_{1}^{\\mathrm{I}})\\sim\\mathrm{b}\\mathbb{Q}^{\\beta}((X_{0}^{\\mathrm{I}},X_{1}^{\\mathrm{I}}),\\cdot)$ . In [ABVE23], $(X_{t}^{\\mathrm{I}})_{t\\in[0,1]}$ is referred to as a stochastic interpolant. Denote by $(s,t,x,y)\\mapsto p_{t|s}^{Y}(y|x)$ the conditional density of $Y_{t}$ given $Y_{s}$ with respect to the Lebesgue measure and by $(p_{t}^{\\mathrm{I}})_{t\\in[0,1]}$ the time marginal densities of $(X_{t}^{\\mathrm{I}})_{t\\in[0,1]}$ with respect to the Lebesgue measure, that is $\\begin{array}{r}{\\mathbb{P}(Y_{t}\\in\\dot{{\\mathsf{A}}}|Y_{s})=\\int_{\\mathsf{A}}p_{t|s}(x|Y_{s})\\mathrm{d}x}\\end{array}$ and $\\begin{array}{r}{\\mathbb{P}(X_{t}^{\\mathrm{I}}\\in\\dot{\\mathsf{A}})\\overset{.}{=}\\int_{\\mathsf{A}}p_{t}^{\\mathrm{I}}(x)\\mathrm{d}x}\\end{array}$ for $\\mathsf{A}\\in\\mathcal{B}(\\mathbb{R}^{d})$ and $s,t\\in[0,1]$ $s\\neq t$ . Then, note that, as a straightforward consequence of the definition of $(X_{t}^{\\mathrm{I}})_{t\\in[0,1]}$ , it holds ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{t}^{\\mathrm{I}}(x)=\\int_{\\mathbb{R}^{2d}}p_{t|0}^{Y}(x|x_{0})p_{1|t}^{Y}(x_{1}|x)\\tilde{\\pi}(\\mathrm{d}x_{0},\\mathrm{d}x_{1})\\;,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\tilde{\\pi}(\\mathrm{d}x_{0},\\mathrm{d}x_{1})=\\frac{\\pi(\\mathrm{d}x_{0},\\mathrm{d}x_{1})}{p_{1|0}^{Y}(x_{1}|x_{0})}\\ .\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "An example that we will focus on in this paper is $\\mathbb{Q}^{\\beta}=\\mathbb{B}$ the distribution of the Brownian motion $({\\sqrt{2}}B_{t})_{t\\in\\mathbb{R}}$ solution of (3) with $\\beta\\equiv0$ : Then, it is well known that $\\mathrm{b}\\mathbb{B}$ is then the Markov kernel associated with the Brownian bridge and the resulting stochastic interpolant satisfies for any $t\\in[0,1]$ ", "page_idx": 3}, {"type": "equation", "text": "$$\nX_{t}^{\\mathrm{I}}\\stackrel{\\mathrm{dist}}{=}(1-t)X_{0}^{\\mathrm{I}}+t X_{1}^{\\mathrm{I}}+\\sqrt{2t(1-t)}\\mathrm{Z}\\;,\\quad\\mathrm{Z}\\sim\\mathrm{N}(0,\\mathrm{Id})\\;,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where \" denotes the equality in distribution. ", "page_idx": 3}, {"type": "text", "text": "It is well-known that for any $x_{0},x_{1}\\in\\mathbb{R}^{d}$ , the distribution $\\mathrm{b}\\mathbb{Q}^{\\beta}((x_{0},x_{1}),\\cdot)$ is diffusion-like under appropriate conditions. More precisely, $\\mathrm{b}\\mathbb{Q}^{\\beta}((x_{0},x_{1}),\\cdot)$ is the distribution of $(\\mathbf{Y}_{t})_{t\\in[0,1]}$ solution to ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{Y}_{t}=\\{\\beta(\\mathbf{Y}_{t})+2\\nabla\\Phi_{t}^{x_{1}}(\\mathbf{Y}_{t})\\}\\mathrm{d}t+\\sqrt{2}\\mathrm{d}B_{t}\\;,\\quad t\\in[0,1]\\;,\\quad\\mathbf{Y}_{0}=x_{0}\\;,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\Phi_{t}^{x_{1}}(y)=\\log p_{1|t}^{Y}(x_{1}|y)$ . For instance, for $x_{0},x_{1}\\in\\mathbb{R}^{d}$ , the bridge $\\mathrm{b}\\mathbb{B}((x_{0},x_{1}),\\cdot)$ associated 10 $\\mathbb{B}$ is the distribution of a Brownianbridge $(\\bar{B}_{t}^{x_{0},x_{1}})_{t\\in[0,1]}$ solution to the SDE ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{d}\\bar{B}_{t}^{x_{0},x_{1}}=\\frac{x_{1}-\\bar{B}_{t}^{x_{0},x_{1}}}{1-t}\\mathrm{d}t+\\sqrt{2}\\mathrm{d}B_{t}\\;,\\quad t\\in[0,1]\\;,\\quad\\bar{B}_{0}^{x_{0},x_{1}}=x_{0}\\;.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "From (6), it turns out that $(X_{t}^{\\mathrm{I}})_{t\\in[0,1]}$ given $(X_{0}^{\\mathrm{I}},X_{1}^{\\mathrm{I}})$ is therefore solution to ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{d}X_{t}^{\\mathrm{I}}=\\big\\{\\beta(X_{t}^{\\mathrm{I}})+2\\nabla\\Phi_{t}^{X_{1}^{\\mathrm{I}}}(X_{t}^{\\mathrm{I}})\\big\\}\\mathrm{d}t+\\sqrt{2}\\mathrm{d}B_{t}\\;,\\quad t\\in[0,1]\\;,\\quad X_{0}^{\\mathrm{I}}\\sim\\mu\\;.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note that the drift coefficient in (7) depends on $X_{1}^{\\mathrm{I}}$ , and therefore, $(X_{t}^{\\mathrm{I}})_{t\\in[0,1]}$ is not Markov, which is a natural property if we are interested in constructing a generative process. To circumvent this issue, based on $\\bar{\\mathbb{I}}(\\bar{\\pi},\\bar{\\mathbb{Q}}^{\\beta})$ , we aim to define a distribution $\\breve{\\mathbb{M}}(\\pi,\\mathbb{Q}^{\\beta})$ such that it has the same onedimensional time marginals as $\\mathbb{I}(\\pi,\\mathbb{Q}^{\\beta})$ and corresponds to a diffusion, i.e., if $(X_{t}^{\\mathrm{I}})_{t\\in[0,1]}\\sim\\mathbb{I}(\\pi,\\mathbb{Q}^{\\beta})$ and $(X_{t}^{\\mathrm{M}})_{t\\in[0,1]}\\sim\\mathbb{M}(\\pi,\\mathbb{Q}^{\\beta})$ , for any $t\\in[0,1]$ $X_{t}^{\\mathrm{I}}\\,\\,{\\stackrel{\\mathrm{dist}}{=}}\\,\\,X_{t}^{\\mathrm{M}}$ and $(X_{t}^{\\mathrm{M}})_{\\in\\mathbb{N}}$ is solution of diffusion with Markov coefficient. This can be done trough the Markovian projection. ", "page_idx": 4}, {"type": "text", "text": "2.2  Markovian projection and Diffusion Flow Matching ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Markovian projection.  The idea of Markovian projection originally dates back to [Gyo86] and [Kry]. Its main idea is in essence to define a diffusion Markov process which \u201cmimics\" the timemarginal of an Ito process: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{d}{\\bf X}_{t}={\\bf b}_{t}\\mathrm{d}t+\\sqrt{2}\\mathrm{d}B_{t}\\;,\\quad t\\in[0,1]\\;,\\quad{\\bf X}_{0}\\sim\\mu\\;,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for some adapted process $(\\mathbf{b}_{t})_{t\\in[0,1]}$ and initial distribution $\\mu$ . Under appropriate conditions (see [S13Clla,isdiioe $(\\mathbf{X}_{t}^{\\mathrm{M}})_{t\\in[0,1]}$ , exists and is solution to an SDE with a (relatively) simple modification of the drift ${\\bf b}_{t}$ , namely ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{d}{\\bf X}_{t}^{\\mathrm{M}}=\\tilde{{\\bf b}}_{t}({\\bf X}_{t}^{\\mathrm{M}})\\mathrm{d}t+\\sqrt{2}\\mathrm{d}B_{t}\\;,\\quad t\\in[0,1]\\;,\\quad{\\bf X}_{0}^{\\mathrm{M}}\\sim\\mu\\;,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Where $\\widetilde{\\mathbf{b}}_{t}(\\mathbf{X}_{t}^{\\mathrm{M}})\\,=\\,\\mathbb{E}[\\mathbf{b}_{t}|\\mathbf{X}_{t}^{\\mathrm{I}}]$ . This result can be applied to the non-Markov process $(X_{t}^{\\mathrm{I}})_{t\\in[0,1]}$ solution of (7): its Markovian projection is solution of ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{d}X_{t}^{\\mathrm{M}}=\\tilde{\\beta}_{t}(X_{t}^{\\mathrm{M}})\\mathrm{d}t+\\sqrt{2}\\mathrm{d}B_{t}\\;,\\quad t\\in[0,1]\\;,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for some function $\\tilde{\\beta}:[0,1]\\times\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d}$ .It turns out that we can identify $\\tilde{\\beta}$ , relying on the family of conditional densities $(p_{t|s}^{Y})_{0\\leq s\\leq t\\leq1}$ and marginal densities $(p_{t}^{\\mathrm{I}})_{0\\leq t\\leq1}$ This s the content of the following result. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Consider a $\\pi\\in\\Pi(\\mu,\\nu^{\\star})$ and $\\mathbb{Q}^{\\beta}$ associated with (3). Consider the drift field ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{\\beta}_{t}^{Y}(x)=\\beta(x)+2\\frac{\\int\\nabla_{x}\\log p_{1|t}^{Y}(x_{1}|x)p_{t|0}^{Y}(x|x_{0})p_{1|t}^{Y}(x_{1}|x)\\tilde{\\pi}(\\mathrm{d}x_{0},\\mathrm{d}x_{1})}{p_{t}^{\\mathrm{I}}(x)}\\;.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Under appropriate conditions (see Appendix A.1), the Markov process $(X_{t}^{\\mathrm{M}})_{t\\in[0,1]}$ solutionof ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{d}X_{t}^{\\mathrm{M}}=\\tilde{\\beta}_{t}^{Y}(X_{t}^{\\mathrm{M}})\\mathrm{d}t+\\sqrt{2}\\mathrm{d}B_{t}\\;,\\quad t\\in[0,1)\\;,\\quad X_{0}^{\\mathrm{M}}\\sim\\mu\\;,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "mimics the one-dimensional time marginals of $\\mathbb{I}(\\pi,\\mathbb{Q}^{\\beta})$ , i.e., for any $t\\in[0,1)$ $X_{t}^{\\mathrm{I}}\\,{\\overset{\\underset{d i s t}{=}}{=}}\\;X_{t}^{\\mathrm{M}}$ ", "page_idx": 4}, {"type": "text", "text": "This result is well-know, but, for sake of completeness, we provide the proof in Appendix A.1. The process (9) is known as the Markovian projection of $\\mathbb{I}(\\pi,\\mathbb{Q}^{\\beta})$ , and its drift (8) as mimicking drift. In what follows, we denote by $\\mathbb{M}(\\pi,\\mathbb{Q}^{\\beta})$ the distribution of $(X_{t}^{\\mathrm{M}})_{t\\in[0,1]}$ on $\\mathbb{W}$ ", "page_idx": 4}, {"type": "text", "text": "Remark 1. It can be easily shown by continuity that $X_{t}^{\\mathrm{{M}}}=X_{t}^{\\mathrm{{I}}}\\Rightarrow X_{1}^{\\mathrm{{I}}}$ for $t\\rightarrow1$ ,where $\\Rightarrow{}$ denotes the convergence in distribution. As I $\\overset{\\cdot}{\\mathbf{\\alpha}}\\mathrm{aw}(X_{1}^{\\mathrm{I}})=\\nu^{\\star}$ , the Markovian projection therefore gives a an ideal generative model which would consist in following the SDE (9) with initial distribution $\\mu$ Remark 2. Note that, because of (4), the mimicking drift (8) rewrites as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widetilde{\\beta}_{t}^{Y}(X_{t}^{\\mathrm{I}})=\\mathbb{E}\\Big[2\\nabla_{x}\\log p_{1|t}^{Y}(X_{1}^{\\mathrm{I}}|X_{t}^{\\mathrm{I}})+\\beta(X_{t}^{\\mathrm{I}})|X_{t}^{\\mathrm{I}}\\Big]\\ .\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Diffusion Flow Matching. Eventually, as pointed out in Remark 1, the Markovian projection gives a an ideal generative model. However, a) the mimicking drift (8) is intractable and b) the continuous-time SDE (9) can not be numerically simulated. Thus, in order to implement the proposed theoretical idea, we first need to address and overcome the aforementioned computational challenges. To circumvent a), observe that, because of Remark 2 and [Kle13, Corollary 8.17], we can approximate the mimicking drift via solving ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta\\in\\Theta}\\mathbb{E}\\Big[\\left\\|s_{\\theta}^{Y}(t,X_{t}^{\\mathrm{I}})-\\tilde{\\beta}_{t}^{Y}(X_{t}^{\\mathrm{I}})\\right\\|^{2}\\Big]\\;,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for a properly chosen class of neural networks $\\{(t,x)\\,\\mapsto\\,s_{\\theta}^{Y}(t,x)\\}_{\\theta\\in\\Theta}$ , and replace $\\tilde{\\beta}^{Y}$ in (9) with $s_{\\theta^{\\star}}^{Y}$ , where $\\theta^{\\star}\\,\\in\\,\\Theta$ denotes a minimizer of (10). To deal with b), we simply make use of the Euler-Maruyama scheme, i.e., for a choice of sequence of step sizes $\\{h_{k}\\}_{k=1}^{N}$ \uff0c $N\\geqslant1$ , and the corresponding time discretization $\\textstyle t_{k}=\\sum_{i=1}^{k}h_{i}$ , such that $t_{0}=0$ and $t_{N}=1$ , we define the continuous process $(X_{t}^{\\theta^{\\star}})_{t\\in[0,1]}$ recursively on the intervals $[t_{k},t_{k+1}]$ by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{d}X_{t}^{\\theta^{\\star}}=s_{\\theta^{\\star}}^{Y}(t_{k},X_{t_{k}}^{\\theta^{\\star}})\\mathrm{d}t+\\sqrt{2}\\mathrm{d}B_{t}\\;,\\quad t\\in[t_{k},t_{k+1}]\\;,\\quad\\mathrm{with}\\quad X_{0}^{\\theta^{\\star}}\\sim\\mu\\;.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "(11) is the DFM generative model we are going to analyze. ", "page_idx": 5}, {"type": "text", "text": "3 Main results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we provide convergence guarantees in Kullback-Leibler divergence for the Diffusion Flow Matching model (11), under mild assumptions on the $\\mu,\\nu^{\\star},\\pi$ and $s_{\\theta^{\\star}}$ , either within a nonearly-stopping regime or within a early-stopping regime. ", "page_idx": 5}, {"type": "text", "text": "From now on, we consider the case $\\beta\\equiv0$ , i.e., $\\mathbb{Q}^{\\beta}=\\mathbb{B}$ . We show in Appendix A.2, Remark 9, that, under out set of assumptions, the conditions of Theorem 1 hold for this setup. Moreover, in this case, for any $s,t\\in[0,1]$ $s<t$ and $x,y\\in\\mathbb{R}^{d}$ , the conditional density $p_{t|s}^{Y}(y|x)\\stackrel{\\cdot}{\\equiv}p_{t-s}(y|x)$ where $(t,x,y)\\mapsto p_{t}(y|x)$ is the heat kernel: ", "page_idx": 5}, {"type": "equation", "text": "$$\np_{t}(y|x)=\\frac{1}{(4\\pi t)^{d/2}}\\exp\\Big(-\\frac{\\|y-x\\|^{2}}{4t}\\Big)\\;,\\quad t\\in(0,1]\\;.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In the following, we set $\\widetilde{\\beta}^{Y}\\equiv\\widetilde{\\beta}$ and $s_{\\theta^{\\star}}^{Y}\\equiv s_{\\theta^{\\star}}$ ", "page_idx": 5}, {"type": "text", "text": "3.1  Convergence Bounds ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We assume moment conditions on the probability measures $\\mu$ and $\\nu^{\\star}$ , and mild integrability conditions on the probability distributions $\\mu,\\nu^{\\star}$ and on the coupling $\\pi$ ", "page_idx": 5}, {"type": "text", "text": "For $p\\geqslant1$ , we denote for $\\zeta\\in\\mathcal{P}(\\mathbb{R}^{d})$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{m}_{p}[\\zeta]=\\int\\|x\\|^{p}\\,\\mathrm{d}\\zeta(x)\\;.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "H1.The probability distributions $\\mu,\\nu^{\\star}$ Ssatisfy $\\mathbf{m}_{\\mathrm{8}}[{\\boldsymbol{\\mu}}]+\\mathbf{m}_{8}[{\\boldsymbol{\\nu}}^{\\star}]<+\\infty$ ", "page_idx": 5}, {"type": "text", "text": "H2. The probability distributions $\\nu^{\\star},\\mu$ and $\\pi$ are absolutely continuous with respect to the Lebesgue measureon $\\mathbb{R}^{d}$ and $\\mathbb{R}^{2d}$ respectively, and satisfy ", "page_idx": 5}, {"type": "text", "text": "(i) The functions $\\log\\mathrm{d}\\mu/\\mathrm{dLeb}^{d}$ and $\\log\\mathrm{d}\\nu^{\\star}/\\mathrm{d}\\mathrm{Leb}^{d}$ are continuously diferentiable and satisfy $\\begin{array}{r}{\\left\\lVert\\nabla\\log\\nu^{\\star}\\right\\rVert_{\\mathrm{L}^{8}(\\nu^{\\star})}^{8}+\\left\\lVert\\nabla\\log\\mu\\right\\rVert_{\\mathrm{L}^{8}(\\mu)}^{8}<+\\infty}\\end{array}$ where for $\\zeta\\in\\{\\nu^{\\star},\\mu\\}$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\left\\|\\nabla\\log\\zeta\\right\\|_{\\mathrm{L}^{8}(\\zeta)}^{8}=\\int\\left\\|\\nabla\\log\\Big(\\frac{\\mathrm{d}\\zeta}{\\mathrm{dLeb}^{d}}\\Big)(x_{0})\\right\\|^{8}\\mathrm{d}\\zeta(x_{0})\\;.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "(i) The function $\\log\\mathrm{d}\\pi/\\mathrm{dLeb}^{2d}$ iscontinuuslydiffrentiableand satisfe $\\left\\lVert\\nabla\\log\\tilde{\\pi}\\right\\rVert_{\\mathrm{L}^{8}(\\pi)}^{8}<+\\infty$ where ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\nabla\\log\\tilde{\\pi}\\Vert_{\\mathrm{L}^{8}(\\pi)}^{8}=\\int\\Vert\\nabla\\log\\left(\\tilde{\\pi}\\right)(x_{0},x_{1})\\Vert^{8}\\,\\mathrm{d}\\pi(x_{0},x_{1})\\,,\\quad\\tilde{\\pi}(x_{0},x_{1})=\\frac{1}{p_{1}(x_{1}|x_{0})}\\frac{\\mathrm{d}\\pi}{\\mathrm{d}\\mathrm{L}\\mathrm{e}\\mathrm{b}^{2d}}(x_{0},x_{1})\\ ,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and $p_{1}$ is defined in (12). ", "page_idx": 5}, {"type": "text", "text": "Remark 3. Under $\\mathbf{H}1$ , note that $\\left\\lVert\\nabla\\log\\tilde{\\pi}\\right\\rVert_{\\mathrm{L}^{8}(\\pi)}^{8}<+\\infty$ is equivalent by (12 and $\\pi\\in\\Pi(\\mu,\\nu^{\\star})$ 10 ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\int\\|\\nabla\\log(\\mathrm{d}\\pi/\\mathrm{d}\\mathrm{Leb}^{2d})(x_{0},x_{1})\\|^{8}\\mathrm{d}\\pi(x_{0},x_{1})<+\\infty\\;.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Remark 4. We can relax the condition that $\\log\\mathrm{d}\\mu/\\mathrm{dLeb}^{d}$ $\\log\\mathrm{d}\\nu^{\\star}/\\mathrm{dLeb}^{d}$ and $\\log\\mathrm{d}\\pi/\\mathrm{dLeb}^{2d}$ are continuously differentiable assuming that $\\sqrt[8]{\\mathrm{d}\\mu/\\mathrm{dLeb}^{d}}$ \uff0c $\\sqrt[8]{\\mathrm{d}\\nu^{\\star}/\\mathrm{dLeb}^{d}}$ and $\\sqrt[8]{\\mathrm{d}\\pi/\\mathrm{dLeb}^{2d}}$ belongs to some Sobolev space, but, for ease of presentation, we prefer not to delve into these technical details. ", "page_idx": 6}, {"type": "text", "text": "Moreover, we assume to have estimated the mimicking drift with an $\\varepsilon^{2}$ -precision, for some $\\varepsilon^{2}>0$ sufficiently small. ", "page_idx": 6}, {"type": "text", "text": "H3. There exist $\\theta^{\\star}\\in\\Theta$ and $\\varepsilon^{2}>0$ suchthat ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sum_{k=0}^{N-1}h_{k+1}\\mathbb{E}\\Big[\\left\\|s_{\\theta^{\\star}}(t_{k},\\boldsymbol{X}_{t_{k}}^{\\mathrm{M}})-\\tilde{\\beta}_{t_{k}}(\\boldsymbol{X}_{t_{k}}^{\\mathrm{M}})\\right\\|^{2}\\Big]\\leq\\varepsilon^{2}\\;.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Remark 5. To be coherent with the previous section, observe that, as a consequence of Theorem 1, forany $k=0,\\cdots\\,,N$ ,it holds ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\Big[\\left\\|s_{\\theta^{\\star}}(t_{k},X_{t_{k}}^{\\mathrm{M}})-\\tilde{\\beta}_{t_{k}}(X_{t_{k}}^{\\mathrm{M}})\\right\\|^{2}\\Big]=\\mathbb{E}\\Big[\\left\\|s_{\\theta^{\\star}}(t_{k},X_{t_{k}}^{\\mathrm{I}})-\\tilde{\\beta}_{t_{k}}(X_{t_{k}}^{\\mathrm{I}})\\right\\|^{2}\\Big]\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Under such assumptions, we derive an upper bound on the KL divergence between the data distribution $\\nu^{\\star}$ and the output of the DFM (11): ", "page_idx": 6}, {"type": "text", "text": "Theorem 2. Consider a uniform partition of $[0,1]$ with a constant stepsize $h_{k}\\equiv h$ $h=1/N_{h}>0$ for $N_{h}\\in\\mathbb{N}^{*}$ and consider the corresponding process $(X_{t}^{\\theta^{\\star}})_{t\\in[0,1]}$ defined in (11). Assume $H I$ 10 $3$ Denoting by $\\nu_{1}^{\\theta^{\\star}}$ thedistributionof $X_{1}^{\\theta^{\\star}}$ ,we have that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}(\\nu^{\\star}|\\nu_{1}^{\\theta^{\\star}})\\lesssim\\varepsilon^{2}+h(h^{1/8}+1)\\Big(d^{4}+\\mathbf{m}_{8}[\\mu]+\\mathbf{m}_{8}[\\nu^{\\star}]+\\|\\nabla\\log\\tilde{\\pi}\\|_{\\mathrm{L}^{s}(\\pi)}^{8}}\\\\ &{\\phantom{=\\;\\;}\\;\\;\\phantom{\\nabla^{4}(\\nu^{\\star}|\\nu_{1}^{\\theta^{\\star}})\\lesssim\\varepsilon^{2}+h(h^{1/8}+1)}+\\|\\nabla\\log\\mu\\|_{\\mathrm{L}^{s}(\\mu)}^{8}+\\|\\nabla\\log\\nu^{\\star}\\|_{\\mathrm{L}^{s}(\\nu^{\\star})}^{8}\\Big)\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Remark 6. Under almost the same conditions as Theorem 2, i.e., H1 H2, H3, replacing $\\tilde{\\pi}$ in (13) by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\tilde{\\pi}_{T}(x_{0},x_{1})=\\frac{1}{p_{T}(x_{1}|x_{0})}\\frac{\\mathrm{d}\\pi}{\\mathrm{d}\\mathrm{Leb}^{2d}}(x_{0},x_{1})\\;,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "our proofs apply also to DFM using a time horizon $T>0$ and the Brownian bridge on $[0,T]$ .In particular, we would have obtained similar bounds as the ones derived in Theorem 2 but with a factor $\\operatorname*{max}(1,T^{8})$ in front of the second addend.   \nRemark 7. Choosing, in Theorem 2, ", "page_idx": 6}, {"type": "equation", "text": "$$\nN_{h}=\\frac{d^{4}+\\mathbf{m}_{8}[\\mu]+\\mathbf{m}_{8}[\\nu^{\\star}]+\\|\\nabla\\log\\tilde{\\pi}\\|_{\\mathrm{L}^{s}(\\pi)}^{8}+\\|\\nabla\\log\\mu\\|_{\\mathrm{L}^{s}(\\mu)}^{8}+\\|\\nabla\\log\\nu^{\\star}\\|_{\\mathrm{L}^{s}(\\nu^{\\star})}^{8}}{\\varepsilon^{2}}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "makes the approximation error of order ${\\mathcal{O}}(\\varepsilon^{2})$ and the complexity of order $\\mathcal{O}(\\varepsilon^{-2})$ ", "page_idx": 6}, {"type": "text", "text": "Using an early-stopping procedure, we obtain in the case $\\pi$ is the independent coupling: ", "page_idx": 6}, {"type": "text", "text": "Theorem 3. Fix $0\\,<\\,\\delta\\,<\\,1/2$ Consider a uniform partition of $[0,1]$ with a constant stepsize $h_{k}\\equiv h_{k}$ $h=1/N_{h}>0,$ for $N_{h}\\in\\mathbb{N}^{*}$ andconsiderthecorrespondingprocess $(X_{t}^{\\theta^{\\star}})_{t\\in[0,1]}$ defined in (11). Assume $H I$ \uff0c $H3$ and $\\pi=\\mu\\otimes\\nu^{\\star}$ tobe the independent coupling.Suppose in addition that $\\mu$ is absolutely continuously withrespect to the Lebesguemeasure, $\\log\\mathrm{d}\\mu/\\mathrm{d}\\mathrm{Leb}^{d}$ is continuously diferentiable and stisjies $\\begin{array}{r}{\\left\\lVert\\nabla\\log\\mu\\right\\rVert_{\\mathrm{L}^{8}(\\mu)}^{8}<+\\infty.}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "Then, denoting by $\\nu_{1-\\delta}^{\\star}$ and $\\nu_{1-\\delta}^{\\theta^{\\star}}$ the distribution of $X_{1-\\delta}^{\\mathrm{M}}$ and $X_{1-\\delta}^{\\theta^{\\star}}$ respectively, we have that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{KL}(\\nu_{1-\\delta}^{\\star}|\\nu_{1-\\delta}^{\\theta^{\\star}})\\lesssim\\varepsilon^{2}+h(h^{1/8}+1)\\Big(\\frac{d^{4}}{\\delta^{4}}+\\mathbf{m}_{8}[\\mu]+\\mathbf{m}_{8}[\\nu^{\\star}]\\frac{1}{\\delta^{8}}+\\|\\nabla\\log\\mu\\|_{\\mathrm{L}^{8}(\\mu)}^{8}\\Big)\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Corollary 1. Fix $\\delta\\;=\\;{\\mathcal O}(\\sqrt{\\varepsilon})$ \uff1aConsider a uniform partition of $[0,1]$ with a constant stepsize $h\\;=\\;{\\mathcal O}(\\operatorname*{min}(\\varepsilon^{4}/d^{4},\\varepsilon^{6}))$ , and consider the corresponding process $(X_{t}^{\\theta^{\\star}})_{t\\in[0,1]}$ defined in (11). Assume $H I$ \uff0c $H3$ and $\\pi\\,=\\,\\mu\\otimes\\nu^{\\star}$ to be the independent coupling. Suppose in addition that $\\mu$ ", "page_idx": 6}, {"type": "text", "text": "is absolutely continuously with respect to the Lebesgue measure, $\\log\\mathrm{d}\\mu/\\mathrm{dLeb}^{d}$ iscontinuously difereniable and atisies $\\|\\nabla\\log\\mu\\|_{\\mathrm{L}^{8}(\\mu)}^{8}<+\\infty$ ", "page_idx": 7}, {"type": "text", "text": "Then, denoting by $\\nu_{1-\\delta}^{\\star}$ and $\\nu_{1-\\delta}^{\\theta^{\\star}}$ the distribution of $X_{1-\\delta}^{\\mathrm{M}}$ and $X_{1-\\delta}^{\\theta^{\\star}}$ respectively, we have that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{W}_{2,F M}^{2}(\\nu_{1-\\delta}^{\\star}|\\nu_{1-\\delta}^{\\theta^{\\star}})\\lesssim\\varepsilon^{2}\\ ,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\mathcal{W}_{2,F M}$ denotes the Fortet- Mourier distance of order 2, i.e. ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{W}_{2,F M}^{2}(\\mu,\\nu)=\\operatorname*{inf}_{\\pi\\in\\Pi(\\mu,\\nu)}\\int\\operatorname*{min}\\{\\|x-y\\|^{2},1\\}\\mathrm{d}\\pi(x,y)\\;.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "3.2  Related works and comparison with existing literature. ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "FMs stand at the forefront of innovation in generative modeling, offering a practical solution to the longstanding challenge of bridging two arbitrary distributions within a finite time interval. Their consequent immense potential has prompted substantial research efforts aimed at providing a theoretical explanation for their effectiveness and has put SGMs and Probability Flow ODEs all in perspective. In this section we report and discuss previous researches on FMs, SGMs and Probability Flow ODEs with the purpose of highlighting the links and differences with our work and contextualizing our contribution. ", "page_idx": 7}, {"type": "text", "text": "Non-early-stopping setting. In the context of FMs, [AVE22] and [BDD23] seek convergence guarantees in 2-Wasserstein distance. Both consider more general designs for the stochastic interpolant and a deterministic sampling procedure, rather than a stochastic one. However, both works rely on some regularity condition on the approximated fow velocity filed, i.e., that it is Lipschitz. Namely, [AVE22] works under a $K$ -Lipschitz (uniform in time and space) assumption on the estimator of the exact fow velocity field. How such assumption pertains to the flow matching framework for generative modeling is not articulated and remains unclear. On the other hand, [BDD23] assumes the estimator of the exact flow velocity field to be $L_{t}$ -Lipschtz for any $t\\in[0,1]$ and discuss in [BDD23, Theorem 2] how such assumption relate to the setting : under the additional assumption [BDD23, Assumption 4], the true flow velocity field is proven to be $L_{t}$ -Lipschitz in space for any $t\\in[0,1]$ Therefore, [BDD23] enhances the findings of [AVE22]. However, [BDD23, Assumption 4] is not an usual conditions considered in papers about convergence guarantees for SGM and it is unclear which type of distributions satisfy [BDD23, Assumption 4]. Moreover, both works [BDD23, AVE22] do not take into account the discretization error in their analysis. ", "page_idx": 7}, {"type": "text", "text": "In the context of Probability Flow ODEs, [LWCC24] and [GZ24] investigate the performance of such models in Total Vartiation distance and 2-Wasserstein distance. In contrast to [BDD23] and [AVE22], [LWCC24] and [GZ24] examine the error coming from the (prerequisite when implementing an algorithm) introduction of a time-discretization scheme. However, once again, the provided bounds work under smoothness assumptions either on the score or on its estimator. More precisely, the result reported in [LWCC24] depends on a small $\\mathrm{L^{2}}$ -Jacobian-estimation error assumption, besides a classical small $\\mathrm{L^{2}}$ -score-estimation error assumption. As for [GZ24], they assume the score to be Lipschitz in time and the data to be smooth and log-concave. In contrast, our result do not make such assumptions. Finally, to the best of our knowledge, the recent work [CDS23] represents the state of art in the context of SGMs without early-stopping procedure: [CDS23, Theorem 2.1] provides a sharp bound in KL divergence between the data distribution and the law of the SGM both in the overdamped and kinetic setting under the sole assumptions of an $\\mathrm{L^{2}}$ -score-approximation error and that the data distribution has finite Fisher information with respect to the standard Gaussian distribution. All previous results are obtained assuming either some Lipschitz condition on the score and/or its estimator ( $[\\mathrm{CCL}^{+}23\\mathrm{b}]$ , [CLL23]) or a manifold hypothesis on the data distribution ([Bor22b]). However, we underline that the FM framework enables to consider a significantly wider range of interpolating paths compared to SGMs and to avoid the trade-off concerning the time horizon $T$ which is inevitable when dealing with SGMs. ", "page_idx": 7}, {"type": "text", "text": "Early-stopping procedure. The recent work [GHJZ24] establishes convergence guarantees in 2-Wasserstein distance for FMs based on a deterministic sampling procedure. However, the results of [GHJZ24] requires to interpolate with a Gaussian distribution and applies only to data distributions which either have a bounded support, are strongly log-concave, or are the convolution between a Gaussian and an other probability distribution supported on an Euclidean Ball. In contrast, for our bound to hold true we only need the data distribution $\\nu^{\\star}$ and its score to have finite eight-order moment. Furthermore, even if [GHJZ24] goes into depth when dealing with the statistical analysis of the estimator and the $\\mathrm{L^{2}}$ -estimation error, the entire investigation therein pursued depends on the choice of ReLUnetworks with Lipschitz regularity control to approximate the velocity field. They motivate such choice by proving (see [GHJZ24, Theorem 5.1]) Lipschitz properties in time and space of the true velocity field under the aforementioned assumptions on the data. On the contrary, we do not assume any regularity on the estimator of the mimicking drift. In the context of Probability FlowODEs, $[\\mathrm{CCL}^{+}\\bar{2}3\\mathrm{a}]$ provides bound in Total Variation distance, but assuming both the score and its estimator to be Lipschitz in space. So, also in the early-stopping regime, our bound improves previously obtained one. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "To conclude, in the context of SGMs, [CLL23, CDS23, BDBDD23] are able to cover any data distribution with bounded second moment at the cost of using exponentially decreasing step-sizes. However, [CDS23, Corollary 2.4] and [BDBDD23] improves upon [CLL23, Theorem 2.2]: the term that takes track of the time-discretization error is linearly dependent on the dimension $d$ in the former works, whereas quadratically dependent on $d$ in the latter. ", "page_idx": 8}, {"type": "text", "text": "3.3  The proposed methodology. ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In what follows, we provide a sketch of the proofs of Theorem 2 and Theorem 3 in order to outline and delineate our methodology. ", "page_idx": 8}, {"type": "text", "text": "The starting point of our proof of Theorem 2 is the following (by now) standard $[\\mathrm{CCL}^{+}23\\mathrm{b}$ ,CLL23, CDS23] decomposition of the KL divergence which is derived from Girsanov theorem: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathrm{KL}(\\nu^{\\star}|\\nu_{1}^{\\theta^{\\star}})\\le\\mathrm{KL}(\\mathbb{M}(\\pi,\\mathbb{B})|\\mathrm{Law}(X_{[0,1]}^{\\theta^{\\star}}))\\lesssim\\sum_{k=0}^{N-1}\\int_{t_{k}}^{t_{k+1}}\\mathbb{E}\\Big[\\Big\\|s_{\\theta^{\\star}}(t_{k},X_{t_{k}}^{\\mathrm{M}})-\\tilde{\\beta}_{t}(X_{t}^{\\mathrm{M}})\\Big\\|^{2}\\Big]\\mathrm{d}t\n$$", "text_format": "latex", "page_idx": 8}, {"type": "equation", "text": "$$\n\\lesssim\\varepsilon^{2}+\\sum_{k=0}^{N-1}\\int_{t_{k}}^{t_{k+1}}\\mathbb{E}\\Big[\\left\\|\\tilde{\\beta}_{t_{k}}(X_{t_{k}}^{\\mathrm{M}})-\\tilde{\\beta}_{t}(X_{t}^{\\mathrm{M}})\\right\\|^{2}\\Big]\\mathrm{d}t\\ ,\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where, for the first inequality, we used the data processing inequality [Nut21, Lemma 1.6] and the last inequality follows from the triangle inequality and the assumption $\\mathbf{H}3$ . In order to conclude, weshould bound the $\\mathrm{L^{2}}$ norm of the adjoint process in the Pontryagin system associated with the Markovian projection of the interpolant. We do so by introducing a novel quantity in the generative model literature (see [Kre97]), namely the so-called reciprocal characteristic of the mimicking drift, i.e., ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\bigl(\\partial_{t}+\\mathcal{L}_{t}^{\\mathrm{M}}\\bigr)\\tilde{\\beta}_{t}\\ ,\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Where $\\mathcal{L}^{\\mathrm{M}}$ denotesthegeneratorof $(X_{t}^{\\mathrm{M}})_{t\\in[0,1]}$ This quantity may be viewed as some sort of mean acceleration field and guides the time evolution of the mimicking drift, as ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathrm{d}\\tilde{\\beta}_{t}(X_{t}^{\\mathrm{M}})=(\\partial_{t}+\\mathcal{L}_{t}^{\\mathrm{M}})\\tilde{\\beta}_{t}(X_{t}^{\\mathrm{M}})\\mathrm{d}t+\\sqrt{2}D_{x}\\tilde{\\beta}_{t}(X_{t}^{\\mathrm{M}})\\mathrm{d}B_{t}\\;,\\quad t\\in[0,1]\\;.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The main efforts in our proof are directed towards bounding the $\\mathrm{L^{2}}$ norm of the reciprocal characteristic whose representation in terms of either conditional moments or higher-order logarithmic derivatives of conditional densities is quite intricate, see (39). Trying to bound each of these terms separately requires strong assumptions on the initial distributions and couplings leading to suboptimal results. However, using integration by parts both in time and space and a double change of measure argument, and profiting from symmetry properties of the heat kernel, we managed to bound these terms under assumptions comparable to the minimal ones required in the analysis of SGMs. Note that the analysis of the reciprocal characteristic is not required for SGMs (it is always O) and that controlling it also requires new tricks and ideas, since its representation contains up to three logarithmic derivatives of conditional distributions, whereas the analysis of SGMs requires at most two such derivatives to be analyzed. ", "page_idx": 8}, {"type": "text", "text": "Regarding the proof of Theorem 3, we consider the interpolated process $(X_{t}^{\\mathrm{I}})_{t\\in[0,\\delta]}$ restricted to $[0,1-\\delta]$ . Denoting by $\\pi_{1-\\delta}$ , the coupling between $\\mu$ and $\\nu_{1-\\delta}^{\\star}$ corresponding to the distribution of the couple $(X_{0}^{\\mathrm{I}},X_{1-\\delta}^{\\mathrm{I}})$ . By the property of the Brownian bridge, $(X_{t}^{\\mathrm{I}})_{t\\in[0,1-\\delta]}$ is a stochastic interpolant resulting from $\\pi_{1-\\delta}$ and the Brownian bridge on $[0,1-\\delta]$ . Therefore based on Remark 6, we only have to show $\\nu_{1-\\delta}^{\\star}$ and $\\pi_{1-\\delta}$ satisfy $\\mathbf{H}1$ and $\\mathbf{H}2$ , replacing $\\tilde{\\pi}$ in $\\mathbf{H}2$ by $\\tilde{\\pi}_{1-\\delta}$ defined in (15). More precisely, we show that they hold and that ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Vert\\nabla\\log\\tilde{\\pi}_{1-\\delta}\\Vert_{\\mathrm{L}^{8}(\\pi_{1-\\delta})}^{8}\\leqslant\\Vert\\nabla\\log\\mu\\Vert_{\\mathrm{L}^{8}(\\mu)}^{8}+\\mathbf{m}_{8}[\\mu]\\frac{1}{(1-\\delta)^{8}}+\\mathbf{m}_{8}[\\nu^{\\star}]\\frac{1}{\\delta^{8}}+d^{4}\\frac{1}{\\delta^{4}(1-\\delta)^{4}}}\\\\ &{\\Vert\\nabla\\log\\nu_{1-\\delta}^{\\star}\\Vert_{\\mathrm{L}^{8}(\\nu_{1-\\delta}^{\\star})}^{8}\\leqslant\\mathbf{m}_{8}[\\mu]\\frac{1}{(1-\\delta)^{8}}+\\mathbf{m}_{8}[\\nu^{\\star}]\\frac{1}{\\delta^{8}}+d^{4}\\frac{1}{\\delta^{4}(1-\\delta)^{4}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "4 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we have investigated a Diffusion Flow Matching model built around the Markovian projection of the $d$ -dimensional Brownian bridge between the data distribution $\\nu^{\\star}$ and the base distribution $\\mu$ . In particular, we have derived convergence guarantees in Kullback-Leibler divergence, which take account of all the sources of error - time-discretization error and drift-estimation error - that arise when implementing the model, and which hold under mild moments conditions on $\\mu,\\nu^{\\star}$ the scores of $\\mu$ $\\nu^{\\star}$ and the score of the coupling $\\pi$ between $\\mu$ and $\\nu^{\\star}$ . However, there are several questions remaining open. First, it would be worthy to understand if we could lower the order of integrability of the score associated with $\\mu,\\nu$ and $\\pi$ . Second, it would be interesting to complement our analysis by a statistical analysis of DFM (11), similarly to what have been achieved in [GHJZ24] for a particular deterministic FM model. Finally, it would be valuable to obtain better dimension dependence with respect to the space dimension $d$ when applying early-stopping procedure. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The work of Marta Gentiloni-Silveri has been supported by the Paris Ile-de-France R\u00e9gion in the framework of DIM AI4IDF. The work by Alain Durmus is partially funded by the European Union (ERC-2022-SYG-OCEAN-101071601). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[ABVE23] Michael S Albergo, Nicholas M Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: A unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797,2023. [And82] Brian DO Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12(3):313-326, 1982. [AVE22] Michael S Albergo and Eric Vanden-Eijnden. Building normalizing fows with stochastic interpolants. arXiv preprint arXiv:2209.15571, 2022. [BB17] Paolo Baldi and Paolo Baldi. Stochastic calculus. Springer, 2017.   \nBDBDD23] Joe Benton, Valentin De Bortoli, Arnaud Doucet, and George Deligiannidis. Linear convergence bounds for diffusion models via stochastic localization. arXiv preprint arXiv:2308.03686, 2023. [BDD23] Joe Benton, George Deligiannidis, and Arnaud Doucet. Error bounds for flow matching methods. arXiv preprint arXiv:2305.16860, 2023.   \n[BKRS15] Vladimir 1. Bogachev, Nicolai V. Krylov, Michael Rockner, and Stanislav V. Shaposhnikov. Fokker-planck-kolmogorov equations. 2015. [Bor22a]  Valentin De Bortoli. Convergence of denoising diffusion models under the manifold hypothesis. Transactions on Machine Learning Research, 2022. Expert Certification. [Bor22b]  Valentin De Bortoli. Convergence of denoising diffusion models under the manifold hypothesis. Transactions on Machine Learning Research, 2022. Expert Certification. [BS13] Gerard Brunick and Steven Shreve. Mimicking an Ito process by a solution of a stochastic differential equation. The Annals of Applied Probability, 23(4):1584 - 1628, 2013.   \n$[\\mathrm{CCL}^{+}23\\mathrm{a}]$ Sitan Chen, Sinho Chewi, Holden Lee, Yuanzhi Li, Jianfeng Lu, and Adil Salim. The probability How ODE is provably fast. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n$[\\mathsf{C C L}^{+}23\\mathsf{b}]$ Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R. Zhang. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. International Conference on Learning Representations, 2023. [CDS23] Giovanni Conforti, Alain Durmus, and Marta Gentiloni Silveri. Score diffusion models without early stopping: finite fisher information is all you need. arXiv preprint arXiv:2308.12240, 2023. [CLL23] Hongrui Chen, Holden Lee, and Jianfeng Lu. Improved analysis of score-based generative modeling: User-friendly bounds under minimal smoothness assumptions. In International Conference on Machine Learning, pages 4735-4763. PMLR, 2023. [CLT22] Tianrong Chen, Guan-Horng Liu, and Evangelos Theodorou. Likelihood training of schrodinger bridge using forward-backward SDEs theory. In International Conference on Learning Representations, 2022.   \n[DBMP19] Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. Neural spline fows. Advances in neural information processing systems, 32, 2019.   \n)BTHD21] Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Difusion schrodinger bridge with applications to score-based generative modeling. Advances in Neural Information Processing Systems, 34:17695-17709, 2021.   \n[FJNO20] Chris Finlay, Jorn-Henrik Jacobsen, Levon Nurbekyan, and Adam Oberman. How to train your neural ode: the world of jacobian and kinetic regularization. In International conference on machine learning, pages 3154-3164. PMLR, 2020.   \n[GCBD19] Will Grathwohl, Ricky TQ Chen, Jesse Bettencourt, and David Duvenaud. Scalable reversible generative models with free-form continuous dynamics. In International Conference on Learning Representations, page 7, 2019.   \n[GHJZ24] Yuan Gao, Jian Huang, Yuling Jiao, and Shurong Zheng. Convergence of continuous normalizing flows for learning probability distributions. arXiv preprint arXiv:2404.00551, 2024. [Gyo86] Istvan Gyongy. Mimicking the one-dimensional marginal distributions of processes having an ito differential. Probability theory and related fields, 71(4):501-516, 1986. [GZ24] Xuefeng Gao and Lingjiong Zhu. Convergence analysis for general probability fow odes of diffusion models in wasserstein distances. arXiv preprint arXiv:2401.17958, 2024. [HD05]  Aapo Hyvarinen and Peter Dayan. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6(4), 2005. [HJA20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020. [Hyv05] Aapo Hyvarinen. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6(24):695-709, 2005. [Kle13] Achim Klenke. Probability theory: a comprehensive course. Springer Science & Business Media, 2013. [Kre97]  Arthur J Krener. Reciprocal diffusions in flat space. Probability theory and related fields, 107(2):243-281, 1997. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[IIy] 1v. v. INi yiuv. Uu ue ieiauun vetwcen umtitnuai opeiatois vi sttonu viuti anu ut solutions of stochastic differential equations. Steklov Seminar.   \n$[\\mathrm{LCBH}^{+}23]$ Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. [LGL23] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified fow. International Conference on Learning Representations (ICLR), 2023. [Liu22] Qiang Liu. Rectified fow: A marginal preserving aproach to optimal transport. arXiv preprint arXiv:2209.14577, 2022. [LLT23] Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence of score-based generative modeling for general data distributions. In International Conference on Algorithmic Learning Theory, pages 946-985. PMLR, 2023.   \n[LWCC24] Gen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi. Towards non-asymptotic convergence for diffusion-based generative models. In The Twelfth International Conference onLearningRepresentations,2024.   \n[LWYql23] Xingchao Liu, Lemeng Wu, Mao Ye, and qiang liu. Learning diffusion bridges on constrained domains. In The Eleventh International Conference on Learning Representations, 2023. $[\\mathrm{LYB}^{+}23]$ Sungbin Lim, EUN BI YOON, Taehyun Byun, Taewon Kang, Seungwoo Kim, Kyungjae Lee, and Sungjoon Choi. Score-based generative modeling through stochastic evolution equations in hilbert spaces. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 37799-37812. Curran Associates, Inc., 2023. [Nut21] Marcel Nutz. Introduction to entropic optimal transport. Lecture notes, Columbia University, 2021. [OFLR21] Derek Onken, Samy Wu Fung, Xingjian Li, and Lars Ruthotto. Ot-flow: Fast and accurate continuous normalizing fows via optimal transport. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 9223-9232, 2021. [Pel22] Stefano Peluchetti. Non-denoising forward-time diffusions, 2022. [PMM23] Francesco Pedrotti, Jan Maas, and Marco Mondelli. Improved convergence of scorebased diffusion models via prediction-correction. arXiv preprint arXiv:2305.14164, 2023.   \n$[\\mathbf{PVG}^{+}21]$ Vadim Ppo IanVvk,VadimiGogoryan,Tanima adekova and MiailK nov. Grad-tts: A diffusion probabilistic model for text-to-speech. In International Conference on Machine Learning, pages 8599-8608. PMLR, 2021.   \n[RBL $^{+}22\\$ Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684-10695, 2022.   \n$[\\mathrm{RDN}^{+}22]$ Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. [RY13] Daniel Revuz and Marc Yor. Continuous martingales and Brownian motion, volume 293. Springer Science & Business Media, 2013.   \n[SBCD23]  Yuyang Shi, Valentin De Bortoli, Andrew Campbell, and Arnaud Doucet. Diffusion schrdinger bridge matching. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[SDWMG15a] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 2256- 2265, Lille, France, 07-09 Jul 2015. PMLR.   \n[SDWMG15b] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics, 2015. [SE19] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. [SE20]  Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution, 2020. [VEH14] Tim Van Erven and Peter Harremos. R\u00e9nyi divergence and kullback-leibler divergence. IEEE Transactions on Information Theory, 60(7):3797-3820, 2014. [Vin11] Pascal Vincent. A connection between score matching and denoising autoencoders. Neural Computation, 23:1661-1674, 2011. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Postponed proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 The Markovian Projection ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "First, we introduce the set of assumptions under which Theorem 1 holds true. ", "page_idx": 13}, {"type": "text", "text": "H4. For $t>s$ $(s,t,x,y)\\mapsto p_{t\\mid s}^{Y}(y|x)$ is continuously differentiable in the t and s variables and twice continuously differentiable in the $x$ and y variables. Furthermore, $\\partial_{t}p_{t|s}^{Y}(y|x)$ $\\partial_{s}p_{t|s}^{Y}(y|x)$ $\\nabla_{x}p_{t|s}^{Y}(y|x),\\,\\nabla_{y}p_{t|s}^{Y}(y|x),\\,\\nabla_{x}^{2}p_{t|s}^{Y}(y|x),\\,\\nabla_{y}^{2}p_{t|s}^{Y}(y|x)$ are bounded. ", "page_idx": 13}, {"type": "text", "text": "H4 ensures that $(s,t,x,y)\\mapsto p_{t\\mid s}^{Y}(y|x)$ is enough regular to allow a series of algebraic manipulations. ", "page_idx": 13}, {"type": "text", "text": "H5. $\\tilde{\\beta}^{Y}$ is a locally bounded Borel vector field on $\\mathbb{R}^{d}\\times(0,1)$ such that, for at least one probability solution $\\mu_{t}$ to the Fokker-Planck equation ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\partial_{t}\\mu_{t}+\\mathrm{div}(\\tilde{\\beta}_{t}^{Y}\\mu_{t})-\\Delta\\mu_{t}=0\\;,\\quad t\\in(0,1)\\;,\\quad\\mu_{0}=\\mu\\;,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "it holds $\\begin{array}{r}{\\int\\|\\tilde{\\beta}_{t}^{Y}(x)\\|\\mu_{t}(\\mathrm{d}x)\\mathrm{d}t<+\\infty.}\\end{array}$ ", "page_idx": 13}, {"type": "text", "text": "H5 provides uniqueness of the solution to the Fokker Planck equation with drift field $\\tilde{\\beta}^{Y}$ ,see [BKRS15, Theorem 9.4.3]. ", "page_idx": 13}, {"type": "text", "text": "We are now ready to rigorously state and prove Theorem 1: ", "page_idx": 13}, {"type": "text", "text": "Theorem 4. Consider a $\\pi\\,\\in\\,\\Pi(\\mu,\\nu^{\\star}),\\,\\mathbb{Q}_{}$ associated with (3) and the drift field defined in (8). Under $\\mathit{\\textbf{H4}}$ and 5, the Markov process $(X_{t}^{\\mathrm{M}})_{t\\in[0,1]}$ solution of (9) is such that, for any $t\\,\\in\\,[0,1)$ \uff0c $X_{t}^{\\mathrm{I}}\\overset{d i s t}{=}X_{t}^{\\mathrm{M}}$ ", "page_idx": 13}, {"type": "text", "text": "Proof of Theorem 4: We start by reminding that $(s,t,x,y)\\mapsto p_{t\\mid s}^{Y}(y|x)$ satisfies for any $x,y\\in\\mathbb{R}^{d}$ and $s,t\\in[0,1]$ With $s<t$ both the Fokker-Planck equation ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\partial_{t}p_{t|s}^{Y}(y|x)+\\mathrm{div}_{y}(p_{t|s}^{Y}(y|x)\\beta(x))-\\Delta_{y}p_{t|s}^{Y}(y|x)=0\\;,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and the Kolmogorov backward equation ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\partial_{s}p_{t|s}^{Y}(y|x)+\\langle\\beta(x),\\nabla_{x}p_{t|s}^{Y}(y|x)\\rangle+\\Delta_{x}p_{t|s}^{Y}(y|x)=0\\;.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "If we exploit these well-known results, (4) and $\\mathbf{H}4$ , we get that for $t\\in(0,1)$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\left(\\boldsymbol{x}\\right)=\\left(\\int_{\\partial\\Omega}\\left(x_{p_{i}}^{{\\mathsf{A}}}\\gamma_{i}^{\\mathsf{A}}(x_{i})\\gamma_{i}^{\\mathsf{A}}(x_{i})\\gamma_{i}^{\\mathsf{A}}(x_{i})\\gamma_{i}^{\\mathsf{A}}(\\Omega_{\\mathsf{M}}),\\partial_{x_{i}}\\gamma_{i}^{\\mathsf{B}}(x_{i})\\right)\\boldsymbol{\\hat{x}}(\\Delta_{\\mathsf{M}})\\,\\mathrm{d}x_{i}\\right)}\\quad}&{}\\\\ &{=\\int_{\\partial\\Omega}\\left(\\Delta_{p_{i}}\\gamma_{i}^{\\mathsf{A}}(x_{i})\\gamma_{i}^{\\mathsf{A}}(x_{i})\\gamma_{i}^{\\mathsf{B}}(x_{i})\\gamma_{i}^{\\mathsf{B}}(x_{i})-\\gamma_{p_{i}^{\\mathsf{A}}}^{\\mathsf{B}}(x_{i})\\gamma_{i}^{\\mathsf{B}}(x_{i})\\gamma_{i}^{\\mathsf{B}}(x_{i})\\right)\\boldsymbol{\\hat{x}}(\\Delta_{\\mathsf{M}})\\,\\mathrm{d}x_{i}}\\\\ &{-\\int_{\\partial\\Omega}\\mathrm{d}x_{i}\\,(\\gamma_{p_{i}^{\\mathsf{A}}}\\gamma_{i}^{\\mathsf{B}}(x_{i})\\gamma_{i}^{\\mathsf{A}}(x_{i}))\\gamma_{i}^{\\mathsf{B}}(x_{i})\\gamma_{i}^{\\mathsf{A}}(x_{i})\\gamma_{i}^{\\mathsf{B}}(x_{i})\\,\\mathrm{d}x_{i}\\,\\mathrm{d}x}\\\\ &{-\\int_{\\partial\\Omega}\\left(\\delta(x),\\gamma_{p_{i}^{\\mathsf{A}}}\\gamma_{i}^{\\mathsf{B}}(x_{i})\\gamma_{i}^{\\mathsf{A}}(x_{i})\\gamma_{p_{i}^{\\mathsf{B}}}(x_{i})\\gamma_{i}^{\\mathsf{B}}(x_{i})\\delta(x_{i},\\mathrm{d}x_{i})\\right)}\\\\ &{=\\int_{\\partial\\Omega}\\left(\\Delta_{p_{i}}\\gamma_{p_{i}^{\\mathsf{A}}(x)}\\gamma_{i}^{\\mathsf{A}}(x_{i})\\gamma_{p_{i}^{\\mathsf{B}}}(x_{i})\\gamma_{i}^{\\mathsf{B}}(x_{i})\\gamma_{p_{i}^{\\mathsf{B}}} \n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Therefore, if we set ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{v_{t}^{Y}(x)=\\frac{\\int_{\\mathbb{R}^{2d}}\\left\\{\\nabla_{x}\\log p_{1[t}^{Y}(x_{1}\\vert x)-\\nabla_{x}\\log p_{t[0}^{Y}(x\\vert x_{0})\\right\\}p_{t[0}^{Y}(x\\vert x_{0})p_{1[t}^{Y}(x_{1}\\vert x)\\tilde{\\pi}(\\mathrm{d}x_{0},\\mathrm{d}x_{1})}{p_{t}^{\\mathrm{I}}(x)}+\\beta(x)\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "we have proven that $p_{t}^{\\mathrm{I}}(x)$ satisfies the following continuity equation ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\partial_{t}p_{t}^{\\mathrm{I}}(x)+\\mathrm{div}_{x}(v_{t}^{Y}(x)p_{t}^{\\mathrm{I}}(x))=0\\;,\\quad t\\in(0,1)\\;,\\;x\\in\\mathbb{R}^{d}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Consequently, if we define ", "page_idx": 14}, {"type": "equation", "text": "$$\nb_{t}^{Y}(x)=v_{t}^{Y}(x)+\\nabla_{x}\\log p_{t}^{\\mathrm{I}}(x)\\;,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "we have that, under $\\mathbf{H}4$ $p_{t}^{\\mathrm{I}}(x)$ satisfies the following Fokker-Planck equation ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\partial_{t}p_{t}^{\\mathrm{I}}(x)+\\mathrm{div}_{x}(b_{t}^{Y}(x)p_{t}^{\\mathrm{I}}(x))-\\Delta_{x}p_{t}^{\\mathrm{I}}(x)=0\\;,\\quad t\\in(0,1)\\;,\\ x\\in\\mathbb{R}^{d}\\;.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "So, $b_{t}(x)$ is a mimicking drift. It remains to show that $b^{Y}\\equiv\\tilde{\\beta}^{Y}$ : the thesis will then follows from the uniqueness of the solution to the Fokker Planck equation (17) under $\\mathbf{H}5$ , see [BKRS15, Theorem 9.4.3]. To this aim, note that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{x}\\log p_{t}^{\\mathrm{I}}(x)=\\frac{\\int_{\\mathbb{R}^{2d}}\\left\\{\\nabla_{x}p_{t\\mid0}^{Y}(x|x_{0})p_{1\\mid t}^{Y}(x_{1}|x)+p_{t\\mid0}^{Y}(x|x_{0})\\nabla_{x}p_{1\\mid t}^{Y}(x_{1}|x)\\right\\}\\tilde{\\pi}(\\mathrm{d}x_{0},\\mathrm{d}x_{1})}{p_{t}^{\\mathrm{I}}(x)}\\,}\\\\ &{\\qquad\\qquad=\\frac{\\int_{\\mathbb{R}^{2d}}\\left\\{\\nabla_{x}\\log p_{t\\mid0}^{Y}(x|x_{0})+\\nabla_{x}\\log p_{1\\mid t}^{Y}(x_{1}|x)\\right\\}p_{t\\mid0}^{Y}(x|x_{0})p_{1\\mid t}^{Y}(x_{1}|x)\\tilde{\\pi}(\\mathrm{d}x_{0},\\mathrm{d}x_{1})}{p_{t}^{\\mathrm{I}}(x)}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, for any $t\\in[0,1)$ and $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , we have that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{b_{t}^{Y}(x)=2\\frac{\\int_{\\mathbb{R}^{2d}}\\nabla_{x}\\log p_{1|t}^{Y}(x_{1}|x)p_{t|0}^{Y}(x|x_{0})p_{1|t}^{Y}(x_{1}|x)\\tilde{\\pi}(\\mathrm{d}x_{0},\\mathrm{d}x_{1})}{p_{t}^{\\mathrm{I}}(x)}+\\beta(x)=\\tilde{\\beta}_{t}^{Y}(x)\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A.2 Preliminary results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We begin this section with two remarks aiming at highlighting some of the properties of the heat kernels (12) and some of their important consequences. ", "page_idx": 14}, {"type": "text", "text": "Remark 8. It is well-known that $(s,x,y)\\mapsto p_{s}(y|x)$ defined in (12) is twice continuously differentiable in the space variables $x$ and $y$ and satisfies for $x,y\\in\\mathbb{R}^{d},s\\in(0,1]$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\nabla_{x}p_{s}(y|x)=-\\frac{x-y}{2s}p_{s}(y|x)=-\\nabla_{y}p_{s}(y|x)\\;,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\nabla_{x}^{2}p_{s}(y|x)=-\\frac{1}{2s}p_{s}(y|x)\\,\\mathrm{Id}+\\frac{(x-y)(x-y)^{\\mathrm{T}}}{4s^{2}}p_{s}(y|x)=\\nabla_{y}^{2}p_{s}(y|x)\\;,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Delta_{x}p_{s}(y|x)=-\\frac{d}{2s}p_{s}(y|x)+\\left\\|\\frac{x-y}{2s}\\right\\|^{2}p_{s}(y|x)=\\Delta_{y}p_{s}(y|x)\\;.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Moreover (12) satisfies the heat equation, i.e., ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\partial_{s}p_{s}(y|x)=\\Delta_{x}p_{s}(y|x)\\;,\\quad s\\in(0,1]\\;,\\;x,y\\in\\mathbb{R}^{d}\\;.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, in particular, $(s,x,y)\\mapsto p_{s}(y|x)$ is continuously differentiable in the time variable $s$ ", "page_idx": 14}, {"type": "text", "text": "Remark 9. In the case $\\beta\\equiv0$ , i.e., $\\mathbb{Q}^{\\beta}=\\mathbb{B}$ , under $\\mathbf{H}1$ , the conditions of Theorem 1 hold. Indeed, as highlighted in Remark 8, $(s,x,y)\\mapsto p_{s}(y|x)$ defined in (12) is continuously differentiable in the time variable $s$ and twice continuously differentiable in the space variables $x$ and $y$ .Also, using Equation (18), Equation (19 and (12), t's straightforward to verify that $\\nabla_{x}p_{t|s}^{Y}(y|x),\\nabla_{y}p_{t|s}^{Y}(y|x)$ $\\nabla_{x}^{2}p_{t|s}^{Y}(y|x),\\nabla_{y}^{2}p_{t|s}^{Y}(y|x)$ are bounded. Additionally, using (20) and (21),it's easy to argue that also $\\partial_{t}p_{t|s}^{Y}(y|x)$ and $\\partial_{s}p_{t|s}^{Y}(y|x)$ are bounded. So $\\mathbf{H}4$ is verified and $p_{t}^{\\mathrm{I}}$ solves the Fokker-Planck equation (16). Moreover $\\tilde{\\beta}$ is clearly locally bounded on $\\mathbb{R}^{d}\\times(0,1)$ and, as a consequence of (18), (5), and Jensen inequality, it satisfies uniformly in time ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\lceil\\int_{\\mathbb{R}^{d}}\\left\\|\\tilde{\\beta}_{t}(x)\\right\\|^{2}p_{t}^{1}(x)\\mathrm{d}x=\\int_{\\mathbb{R}^{d}}\\left\\|\\frac{1}{p_{t}^{1}(x)}\\int_{\\mathbb{R}^{2d}}\\frac{x_{1}-x}{1-t}p_{t}(x|x_{0})p_{1-t}(x_{1}|x)\\tilde{\\pi}(\\mathrm{d}x_{0},\\mathrm{d}x_{1})\\right\\|^{2}p_{t}^{1}(x)\\mathrm{d}x}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}\\left[\\left\\|\\mathbb{E}\\left[\\frac{X_{1}-X_{t}^{1}}{1-t}\\bigg|X_{t}^{1}\\right]\\right\\|^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}\\left[\\left\\|\\mathbb{E}\\left[X_{1}^{1}-X_{0}^{1}-\\frac{\\sqrt{2t}}{\\sqrt{1-t}}Z\\middle|X_{t}^{1}\\right]\\right\\|^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\lesssim\\mathbb{E}\\left[\\left\\|X_{1}^{1}-X_{0}^{1}-\\sqrt{2t}Z\\right\\|^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\lesssim\\mathbf{m}_{2}[\\mu]+\\mathbf{m}_{2}[\\nu^{*}]+d\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which is finite under $\\mathbf{H}1$ . It follows that $\\begin{array}{r}{\\int\\|\\tilde{\\beta}_{t}^{Y}(x)\\|p_{t}^{\\mathrm{I}}(x)\\mathrm{d}x\\mathrm{d}t<+\\infty}\\end{array}$ So, $\\mathbf{H}5$ is verified. ", "page_idx": 15}, {"type": "text", "text": "Additionally, hereunder, we state three lemmas that will be crucial in the derivation of the bounds provided in Theorems 2 and 3. ", "page_idx": 15}, {"type": "text", "text": "Lemma 1. For any $p\\geq1$ they hold ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\left\\|X_{s}^{\\mathrm{I}}-X_{0}^{\\mathrm{I}}\\right\\|^{2p}]\\lesssim s^{2p}\\mathbf{m}_{2p}[\\mu]+s^{2p}\\mathbf{m}_{2p}[\\nu^{\\star}]+d^{p}s^{p}(1-s)^{p}\\ ,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\left\\|X_{1}^{\\mathrm{I}}-X_{s}^{\\mathrm{I}}\\right\\|^{2p}]\\mathrm{d}s\\lesssim(1-s)^{2p}\\mathbf{m}_{2p}[\\mu]+(1-s)^{2p}\\mathbf{m}_{2p}[\\nu^{\\star}]+d^{p}s^{p}(1-s)^{p}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma $^{l}$ : As a direct consequence of (5) and Young inequality, it holds ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\left\\|X_{s}^{\\mathrm{I}}-X_{0}^{\\mathrm{I}}\\right\\|^{2p}]=\\mathbb{E}\\Big[\\left\\|s(X_{1}^{\\mathrm{I}}-X_{0}^{\\mathrm{I}})+\\sqrt{2s(1-s)}\\mathbb{Z}\\right\\|^{2p}\\Big]}\\\\ &{\\qquad\\qquad\\qquad\\lesssim s^{2p}\\mathbb{E}[\\left\\|X_{0}^{\\mathrm{I}}-X_{1}^{\\mathrm{I}}\\right\\|^{2p}]+(2s(1-s))^{p}\\mathbb{E}[\\left\\|\\mathbf{Z}\\right\\|^{2p}]}\\\\ &{\\qquad\\qquad\\lesssim s^{2p}\\mathbf{m}_{2p}[\\mu]+s^{2p}\\mathbf{m}_{2p}[\\nu^{\\star}]+d^{p}s^{p}(1-s)^{p}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "A similar argument holds for $\\mathbb{E}[\\left\\lVert X_{1}^{\\mathrm{I}}-X_{s}^{\\mathrm{I}}\\right\\rVert^{2p}]$ ", "page_idx": 15}, {"type": "text", "text": "We preface the next lemma with a definition. ", "page_idx": 15}, {"type": "text", "text": "Definition 1. Consider $\\mathbb{Q}\\in\\mathcal{P}(\\mathbb{W})$ . The reverse time measure $\\mathbb{Q}^{\\mathrm{R}}\\in\\mathcal{P}(\\mathbb{W})$ of $\\mathbb{Q}$ is defined as follows: for any $\\mathsf{A}\\in B(\\mathbb{W})$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{Q}^{\\mathrm{R}}(\\mathsf{A})=\\mathbb{Q}(\\mathsf{A}^{\\mathrm{R}}),\\;,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\mathsf{A}^{\\mathrm{R}}:=\\{t\\mapsto\\omega(1-t)\\ :\\ \\omega\\in\\mathsf{A}\\}$ ", "page_idx": 15}, {"type": "text", "text": "Lemma 2. Assume $\\pi\\ll{\\mathrm{Leb}}^{2d}$ and $\\mu,\\nu^{\\star}\\ll\\mathrm{Leb}^{d}$ Then $(X_{t}^{\\mathrm{I}})_{t\\in[0,1]}$ solves weakly ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\overrightarrow{X}_{t}=2\\overrightarrow{\\textit{b}}_{t}\\big(\\overrightarrow{X}_{0},\\overrightarrow{X}_{t}\\big)\\mathrm{d}t+\\sqrt{2}\\mathrm{d}\\overrightarrow{B}_{t}\\mathrm{~,~}\\quad t\\in[0,1]\\mathrm{~,~}\\quad\\overrightarrow{X}_{0}\\sim\\mu\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with $(\\overrightarrow{B}_{t})_{t\\in[0,1]}\\,d$ dimensionalBrownian motion independent of $\\scriptstyle{{\\vec{X}}_{0}}$ and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\overrightarrow{b}_{\\ t}(x_{0},x)=\\nabla_{x}\\psi_{t}^{x_{0}}(x)\\;,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\psi_{t}^{x_{0}}(x)$ solves ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\partial_{t}\\psi_{t}^{x_{0}}+\\Delta_{x}\\psi_{t}^{x_{0}}+\\|\\nabla_{x}\\psi_{t}^{x_{0}}\\|^{2}=0\\;,\\quad t\\in[0,1)\\;,\\quad\\psi_{1}^{x_{0}}=\\log\\tilde{\\pi}_{0}^{x_{0}}\\;,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\tilde{\\pi}_{0}^{x_{0}}(x):=\\left.\\frac{\\mathrm{d}\\pi(x_{0},x)}{\\mathrm{dLeb}^{2d}}\\frac{1}{p_{1}(x|x_{0})}\\middle/\\frac{\\mathrm{d}\\mu(x_{0})}{\\mathrm{dLeb}^{d}}\\right.\\ ,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and $p_{1}(x|x_{0})$ defined in (12). Similarly, $((\\stackrel{\\mathrm{\\scriptsize~\\it{X}}_{t}^{\\mathrm{I}}}{x_{t}})_{t\\in[0,1]})^{\\mathrm{R}}$ solves weakly ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\overleftarrow{X}_{t}=2\\overleftarrow{b}_{t}(\\overleftarrow{X}_{0},\\overleftarrow{X}_{t})\\mathrm{d}t+\\sqrt{2}\\mathrm{d}\\overleftarrow{B}_{t}\\;,\\quad t\\in[0,1]\\;,\\quad\\overleftarrow{X}_{0}\\sim\\nu^{\\star}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with $(\\overleftarrow{B}_{t})_{t\\in[0,1]}\\,d$ dimensional Brownian motion independent of ${\\overleftarrow{X}}_{0}$ and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\overleftarrow{b}_{t}(x_{1},x)=\\nabla_{x}\\varphi_{1-t}^{x_{1}}(x)\\ ,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\varphi_{t}^{x_{1}}(x)$ solves ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{-\\partial_{t}\\varphi_{t}^{x_{1}}+\\Delta_{x}\\varphi_{t}^{x_{1}}+\\|\\nabla_{x}\\varphi_{t}^{x_{1}}\\|^{2}=0\\;,\\quad t\\in[0,1)\\;,\\quad\\varphi_{0}^{x_{1}}=\\log\\tilde{\\pi}_{1}^{x_{1}}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\tilde{\\pi}_{1}^{x_{1}}(x):=\\left.\\frac{\\mathrm{d}\\pi(x,x_{1})}{\\mathrm{dLeb}^{2d}}\\frac{1}{p_{1}(x_{1}|x)}\\middle/\\frac{\\mathrm{d}\\nu^{\\star}(x_{1})}{\\mathrm{dLeb}^{d}}\\right.\\,,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and $p_{1}(x_{1}|x)$ defined in (12). ", "page_idx": 16}, {"type": "text", "text": "Proof of Lemma 2: We just show that $(X_{t}^{\\mathrm{I}})_{t\\in[0,1]}$ solves weakly (22). The argument for $((X_{t}^{\\mathrm{I}})_{t\\in[0,1]})^{\\mathrm{R}}$ is similar and therefore omitted. ", "page_idx": 16}, {"type": "text", "text": "For a fixed $\\bar{x}_{0}\\in\\mathbb{R}^{d}$ , we denote by $(x_{0},\\mathsf{A})\\mapsto\\mathbb{I}_{\\pi,\\mathbb{B}}(x_{0},\\mathsf{A})$ the conditional distribution of $(X_{t}^{\\mathrm{I}})_{t\\in[0,1]}$ given $X_{0}^{\\mathrm{I}}$ (see e.g. [Kle13, Theorem 8.37] for the existence of this conditional distribution) and by $(B_{t}^{x_{0}})_{t\\in[0,1]}$ the solution to ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{d}B_{t}^{x_{0}}=\\sqrt{2}\\mathrm{d}B_{t}\\;,\\quad t\\in[0,1],\\quad B_{0}^{x_{0}}=x_{0}\\;.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Also, we denote by $(W_{t})_{t\\in[0,1]}$ the canonical process on the Wiener space W and by $(\\mathcal{F}_{t})_{t\\in[0,1]}$ the corresponding natural fltration. Note that, as a consequence of the very defnition f $(X_{t}^{\\mathrm{I}})_{t\\in[0,1]}$ (23 and Ito's formula applied to $\\big(\\psi_{t}^{x_{0}}(B_{t}^{x_{0}})\\big)_{t\\in[0,1]}.$ it holds ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathrm{d}\\mathrm{I}_{\\pi,\\mathbf{{b}}}(x_{0},\\cdot)}{\\mathrm{d}\\mathrm{I}_{\\mathrm{L}}\\mathrm{ax}((B_{1}^{x_{0}})_{t\\in[0,1]})}((B_{t}^{x_{0}})_{t\\in[0,1]})}\\\\ &{=\\tilde{\\pi}_{0}^{x_{0}}(B_{1}^{x_{0}})}\\\\ &{=\\exp\\left(\\psi_{1}^{x_{0}}(B_{1}^{x_{0}})\\right)}\\\\ &{=\\exp\\left(\\psi_{1}^{x_{0}}(B_{1}^{x_{0}})-\\psi_{0}^{x_{0}}(x_{0})\\right)}\\\\ &{=\\exp\\left(\\psi_{1}^{x_{0}}(B_{1}^{x_{0}})-\\psi_{0}^{x_{0}}(B_{0}^{x_{0}})-\\displaystyle\\int_{0}^{1}\\left\\{\\partial_{t}\\psi_{t}^{x_{0}}+\\Delta_{x}\\psi_{t}^{x_{0}}+\\|\\nabla_{x}\\psi_{t}^{x_{0}}\\|^{2}\\right\\}(B_{t}^{x_{0}})\\mathrm{d}t\\right)}\\\\ &{=\\exp\\left(\\int_{0}^{1}\\nabla_{x}\\psi_{t}^{x_{0}}(B_{t}^{x_{0}})\\mathrm{d}B_{t}^{x_{0}}-\\displaystyle\\int_{0}^{1}\\|\\nabla_{x}\\psi_{t}^{x_{0}}(B_{t}^{x_{0}})\\|^{2}\\,\\mathrm{d}t\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Hence, for any $t\\in[0,1]$ we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left.\\frac{\\mathrm{d}\\mathbb{I}_{\\pi,\\mathbb{B}}(x_{0},\\cdot)}{\\mathrm{d}\\mathrm{Law}((B_{t}^{x_{0}})_{t\\in[0,1]})}\\right|_{\\mathcal{F}_{t}}=D_{t}\\;,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where ", "page_idx": 16}, {"type": "equation", "text": "$$\nD_{t}=\\exp\\Big(\\int_{0}^{t}\\nabla_{x}\\psi_{t}^{x_{0}}(W_{s})\\mathrm{d}W_{s}-\\int_{0}^{1}\\|\\nabla_{x}\\psi_{t}^{x_{0}}(W_{s})\\|^{2}\\,\\mathrm{d}s\\Big)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "is continuous $\\mathrm{Law}\\big(\\big(B_{t}^{x_{0}}\\big)_{t\\in[0,1]}\\big).$ - almost surely and is such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{d}D_{t}=D_{t}\\nabla_{x}\\psi_{s}^{x_{0}}(W_{s})\\mathrm{d}W_{t}\\ .\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore,being $(W_{t})_{t\\in[0,1]}$ a martingale under $\\mathrm{Law}\\big(\\big(B_{t}^{x_{0}}\\big)_{t\\in[0,1]}\\big)$ , as a consequence of Girsanov theorem (see [RY13, Theorem 1.4]), we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left(W_{t}-\\left(D^{-1}\\langle W,D\\rangle\\right)_{t}\\right)_{t\\in[0,1]}=\\left(W_{t}-2\\int_{0}^{t}\\nabla_{x}\\psi_{s}^{x_{0}}(W_{s})\\mathrm{d}s\\right)_{t\\in[0,1]}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "isa $((\\mathcal{F}_{t})_{t},\\mathbb{I}_{\\pi,\\mathbb{B}}(x_{0},\\cdot))\\cdot$ martingale with bracket ${\\sqrt{2}}t$ . It fllows that, under $\\mathbb{I}_{\\pi,\\mathbb{B}}(x_{0},\\cdot),(W_{t})_{t\\in[0,1]}$ solves ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}W_{t}=2\\nabla_{x}\\psi_{t}^{x_{0}}(W_{s})\\mathrm{d}t+\\sqrt{2}\\mathrm{d}B_{t}\\;,\\quad t\\in[0,1]\\;,\\quad W_{0}=x_{0}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The thesis is now a direct consequence of the fact that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{I}(\\pi,\\mathbb{B})(\\mathsf{A})=\\int_{\\mathbb{R}^{d}}\\mathbb{P}(X^{\\mathrm{I}}\\in\\mathsf{A}|X_{0}^{\\mathrm{I}}=x_{0})\\mu(\\mathrm{d}x_{0})=\\int_{\\mathbb{R}^{d}}\\mathbb{I}_{\\pi,\\mathbb{B}}(x_{0},\\mathsf{A})\\mu(\\mathrm{d}x_{0})\\;,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for any measurable $\\mathsf{A}\\in\\mathbb{W}$ ", "page_idx": 17}, {"type": "text", "text": "Lemma 3. Assume $_{H2}$ .Then, almost surely, it holds ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\vec{b_{\\ t}}(X_{0}^{\\mathrm{I}},X_{t}^{\\mathrm{I}})=\\mathbb{E}\\left[\\frac{\\nabla\\tilde{\\pi}_{0}^{X_{0}^{\\mathrm{I}}}(X_{1}^{\\mathrm{I}})}{\\tilde{\\pi}_{0}^{X_{0}^{\\mathrm{I}}}(X_{1}^{\\mathrm{I}})}\\bigg|(X_{0}^{\\mathrm{I}},X_{t}^{\\mathrm{I}})\\right]\\,,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\tilde{\\pi}_{0}^{x_{0}}$ is defined as in (24). Moreover, for any $u\\in[0,1-s]$ \uff0c $s\\in[0,1]$ and $p\\in\\{2,4,8\\}$ it holds ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\!\\left[\\left\\|\\int_{u}^{u+s}\\vec{\\,b}_{\\,r}(\\vec{X}_{0},\\vec{X}_{r})\\mathrm{d}r\\right\\|^{p}\\right]\\lesssim s^{p}\\Big(\\left\\|\\nabla\\log\\tilde{\\pi}\\right\\|_{\\mathrm{L}^{p}(\\pi)}^{p}+\\left\\|\\nabla\\log\\mu\\right\\|_{\\mathrm{L}^{p}(\\mu)}^{p}\\Big)\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Similarly,almost surely itholds ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\overleftarrow{b}_{t}(X_{0}^{\\mathrm{I}},X_{t}^{\\mathrm{I}})=\\mathbb{E}\\left[\\frac{\\nabla\\tilde{\\pi}_{1}^{X_{1}^{\\mathrm{I}}}(X_{0}^{\\mathrm{I}})}{\\tilde{\\pi}_{1}^{X_{1}^{\\mathrm{I}}}(X_{0}^{\\mathrm{I}})}\\bigg|(X_{0}^{\\mathrm{I}},X_{t}^{\\mathrm{I}})\\right]\\,,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\tilde{\\pi}_{1}^{x_{1}}$ is defined as in (25). Moreover, for any $u\\in[0,1-s]$ \uff0c $s\\in[0,1]$ and $p\\in\\{2,4,8\\}$ itholds ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\!\\left[\\left\\|\\int_{u}^{u+s}\\overleftarrow{b}_{r}(\\overleftarrow{X}_{0},\\overleftarrow{X}_{r})\\mathrm{d}r\\right\\|^{p}\\right]\\lesssim s^{p}\\Big(\\left\\|\\nabla\\log\\tilde{\\pi}\\right\\|_{\\mathrm{L}^{p}(\\pi)}^{p}+\\left\\|\\nabla\\log\\nu^{\\star}\\right\\|_{\\mathrm{L}^{p}(\\nu^{\\star})}^{p}\\Big)\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma 3: We just show (26) and (27). The proof of (28) and (29) is similar and therefore omitted. First of all, note that (26) is trivial for $t=1$ .We thereforefocus on $t\\in[0,1)$ .It's well known that the solution $\\psi_{t}^{x_{0}}(x)$ to the Hamilton-Jacobi-Bellman equation (23) is given by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\psi_{t}^{x_{0}}(x)=\\log\\Big(\\int\\widetilde{\\pi}_{0}^{x_{0}}(x_{1})p_{1-t}(x_{1}|x)\\mathrm{d}x_{1}\\Big)\\ ,\\quad t\\in[0,1)\\ .\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, we have that for $t\\in[0,1)$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\overrightarrow{b}_{\\iota}(x_{0},x)=\\nabla_{x}\\psi_{t}^{x_{0}}(x)=\\frac{\\int\\tilde{\\pi}_{0}^{x_{0}}(x_{1})\\nabla_{x}p_{1-t}(x_{1}|x)\\mathrm{d}x_{1}}{\\int\\tilde{\\pi}_{0}^{x_{0}}(\\tilde{x}_{1})p_{1-t}(\\tilde{x}_{1}|x)\\mathrm{d}\\tilde{x}_{1}}\\:.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Using (18) and integrating by parts, we get for $t\\in[0,1)$ that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overrightarrow{b}_{\\;t}(x_{0},x)=-\\frac{\\int\\widetilde{\\pi}_{0}^{x_{0}}(x_{1})\\nabla_{x_{1}}p_{1-t}(x_{1}|x)\\mathrm{d}x_{1}}{\\int\\widetilde{\\pi}_{0}^{x_{0}}(\\widetilde{x}_{1})p_{1-t}(\\widetilde{x}_{1}|x)\\mathrm{d}\\widetilde{x}_{1}}=\\frac{\\int(\\nabla\\widetilde{\\pi}_{0}^{x_{0}}(x_{1})/\\widetilde{\\pi}_{0}^{x_{0}}(x_{1}))\\widetilde{\\pi}_{0}^{x_{0}}(x_{1})p_{1-t}(x_{1}|x)\\mathrm{d}x_{1}}{\\int\\widetilde{\\pi}_{0}^{x_{0}}(\\widetilde{x}_{1})p_{1-t}(\\widetilde{x}_{1}|x)\\mathrm{d}\\widetilde{x}_{1}}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "But then, it suffices to prove that for $t\\in[0,1)$ it holds ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\tilde{\\pi}_{0}^{x_{0}}(x_{1})p_{1-t}(x_{1}|x)}{\\int\\tilde{\\pi}_{0}^{x_{0}}(\\tilde{x}_{1})p_{1-t}(\\tilde{x}_{1}|x)\\mathrm{d}\\tilde{x}_{1}}=p_{1|0,t}^{\\mathrm{I}}(x_{1}|x_{0},x)\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "to conclude. To do so, we simply use (4). ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{1|0,t}^{\\mathrm{I}}(x_{1}|x_{0},x)=\\frac{p_{0,1,t}^{\\mathrm{I}}(x_{0},x_{1},x)}{\\int p_{0,1,t}^{\\mathrm{I}}(x_{0},\\tilde{x}_{1},x)\\mathrm{d}\\tilde{x}_{1}}}\\\\ &{\\phantom{m m m m m m m m m}=\\frac{\\pi(x_{0},x_{1})p_{t}(x|x_{0})p_{1-t}(x_{1}|x)}{p_{1}(x_{1}|x_{0})}\\Biggl/\\int\\frac{\\pi(x_{0},\\tilde{x}_{1})p_{t}(x|x_{0})p_{1-t}(\\tilde{x}_{1}|x)}{p_{1}(\\tilde{x}_{1}|x_{0})}\\mathrm{d}\\tilde{x}_{1}}\\\\ &{\\phantom{m m m m m m m m m m m m m m m}=\\frac{\\pi(x_{0},x_{1})p_{1-t}(x_{1}|x_{0})}{p_{1}(x_{1}|x_{0})}\\Biggl/\\int\\frac{\\pi(x_{0},\\tilde{x}_{1})p_{1-t}(\\tilde{x}_{1}|x)}{p_{1}(\\tilde{x}_{1}|x_{0})}\\mathrm{d}\\tilde{x}_{1}}\\\\ &{\\phantom{m m m m m m m m m m}=\\frac{\\pi(x_{0},x_{1})p_{1-t}(x_{1}|x)}{\\mu(x_{0})p_{1}(x_{1}|x_{0})}\\Biggl/\\int\\frac{\\pi(x_{0},\\tilde{x}_{1})p_{1-t}(\\tilde{x}_{1}|x)}{\\mu(x_{0})p_{1}(\\tilde{x}_{1}|x_{0})}\\mathrm{d}\\tilde{x}_{1}}\\\\ &{\\phantom{m m m m m m m m}=\\frac{\\pi_{0}^{\\mathrm{I}}(x_{1})p_{1-t}(x_{1}|x)}{\\int\\tilde{\\pi}_{0}^{\\mathrm{I}}(x_{1})p_{1-t}(\\tilde{x}_{1}|x)}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We are left with the bounds (27). ", "page_idx": 18}, {"type": "text", "text": "We prove (27) only for $p=1$ . The other two cases are analogous. To this aim, we simply make use of (22), Holder and Jensen inequalities and the properties of the conditional expectation. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\bigg[\\bigg\\|\\int_{u}^{u+s}\\widehat{v}_{r}(\\vec{X}_{0},\\vec{X}_{r})\\mathrm{d}u\\bigg\\|^{2}\\bigg]=\\mathbb{E}\\bigg[\\bigg\\|\\int_{u}^{u+s}\\mathbb{E}\\bigg[\\frac{\\nabla\\bar{v}_{u}^{(1)}(X_{0}^{1})}{\\bar{w}_{u}^{(1)}(X_{0}^{1})}\\bigg|(X_{0}^{1},X_{r}^{1})\\bigg]\\mathrm{d}u\\bigg\\|^{2}\\bigg]}\\\\ &{\\lesssim s\\mathbb{E}\\bigg[\\int_{u}^{u+s}\\bigg\\|\\mathbb{E}\\bigg[\\frac{\\nabla\\bar{v}_{u}^{(1)}(X_{0}^{1})}{\\bar{w}_{u}^{(1)}(X_{0}^{1})}\\bigg|(X_{0}^{1},X_{r}^{1})\\bigg]\\bigg\\|^{2}\\,\\mathrm{d}r\\bigg]}\\\\ &{\\lesssim s\\mathbb{E}\\bigg[\\int_{u}^{u+s}\\mathbb{E}\\bigg[\\bigg\\|\\frac{\\nabla\\bar{v}_{u}^{(3)}(X_{0}^{1})}{\\bar{w}_{u}^{(1)}(X_{0}^{1})}\\bigg\\|(X_{0}^{1},X_{r}^{1})\\bigg\\|\\mathrm{d}r\\bigg]}\\\\ &{=s\\mathbb{E}\\bigg[\\int_{u}^{u+s}\\bigg\\|\\frac{\\nabla\\bar{v}_{u}^{(1)}(X_{0}^{1})}{\\bar{w}_{u}^{(1)}(X_{0}^{1})}\\bigg\\|\\mathrm{d}r\\bigg]}\\\\ &{\\lesssim s^{2}\\int\\|\\nabla\\log\\bar{u}_{0}^{(2)}(X_{0}^{1)}\\|^{2}\\,\\mathrm{d}\\tau(x_{0},x_{1})}\\\\ &{\\lesssim s^{2}\\left(\\|\\nabla\\log\\bar{u}_{0}^{(1)}(X_{0}^{1})+\\|\\nabla\\log\\bar{u}_{1}\\|_{2}^{2}(u)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where, in the last inequality, we have used the very definition of $\\tilde{\\pi}_{0}^{x_{0}}$ given in (24). ", "page_idx": 18}, {"type": "text", "text": "A.3 Main results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We preface the proofs of our main results with some extra notation. For sake of brevity, we denote by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\vec{f}_{u}^{\\,t}=2\\int_{u}^{t+u}\\vec{\\,b}_{\\,r}(\\vec{X}_{0},\\vec{X}_{r})\\mathrm{d}r\\ ,\\quad\\vec{g}_{\\,u}^{\\,t}=\\vec{B}_{t+u}-\\vec{B}_{\\,u}\\ ,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and similarly ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\overleftarrow{f}_{u}^{t}=2\\int_{u}^{t+u}\\overleftarrow{b}_{r}(\\overleftarrow{X}_{0},\\overleftarrow{X}_{r})\\mathrm{d}r\\ ,\\quad\\overleftarrow{g}_{u}^{t}=\\overleftarrow{B}_{t+u}-\\overleftarrow{B}_{u}\\ .\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Remark 10. With this new notation, according to Lemma 2, we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\n(X_{t}^{\\mathrm{I}})_{t\\in[0,1]}\\stackrel{\\mathrm{dist}}{=}(\\overrightarrow{X}_{t})_{t\\in[0,1]}\\ ,\\quad(X_{t}^{\\mathrm{I}})_{t\\in[0,1]}\\stackrel{\\mathrm{dist}}{=}((\\overleftarrow{X}_{t})_{t\\in[0,1]})^{\\mathrm{R}}\\ ,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and that for any $u\\in[0,1]$ and $t\\in[0,1-u]$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{X_{t+u}^{\\mathrm{I}}-X_{u}^{\\mathrm{I}}\\stackrel{\\mathrm{dist}}{=}\\vec{f}_{u}^{\\,t}+\\vec{g}_{\\,u}^{\\,t}\\,,\\quad X_{1-(t+u)}^{\\mathrm{I}}-X_{1-u}^{\\mathrm{I}}\\stackrel{\\mathrm{dist}}{=}\\overleftarrow{f}_{\\,u}^{\\,t}+\\overleftarrow{g}_{\\,u}^{\\,t}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Furthermore, according to Lemma 3, under $\\mathbf{H}2$ , for any $u\\in[0,1],t\\in[0,1-u],p\\in\\{2,4,8\\}.$ we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\Big[\\left\\lVert\\overrightarrow{f}_{u}^{t}\\right\\rVert^{p}\\Big]\\lesssim t^{p}(\\Vert\\nabla\\log\\mu\\Vert_{\\mathrm{L}^{p}(\\mu)}^{p}+\\Vert\\nabla\\log\\tilde{\\pi}\\Vert_{\\mathrm{L}^{p}(\\pi)}^{p})\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\Big[\\left\\Vert\\overleftarrow{f}_{u}^{t}\\right\\Vert^{p}\\Big]\\lesssim t^{p}(\\Vert\\nabla\\log\\nu^{\\star}\\Vert_{\\mathrm{L}^{p}(\\nu^{\\star})}^{p}+\\Vert\\nabla\\log\\tilde{\\pi}\\Vert_{\\mathrm{L}^{p}(\\pi)}^{p})\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Remark 11. Note that for any $u\\in[0,1]$ and $t\\in[0,1-u]$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\overrightarrow{g}_{\\textit{u}}^{t}\\perp(\\overrightarrow{X}_{r})_{r\\leq u}\\ ,\\quad\\overleftarrow{g}_{\\textit{u}}^{t}\\perp(\\overleftarrow{X}_{r})_{r\\leq u}\\ .\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This fact is an almost immediate consequence of the Markov property of $(\\overrightarrow{B}_{t})_{t\\in[0,1]}$ , see [BB17, Theorem 3.3]: consider the filtration $(\\mathcal{F}_{t})_{t\\in[0,1]}$ defined by $\\mathcal{F}_{t}=\\sigma(\\vec{X}_{0},(\\vec{B}_{u})_{u\\leq t})$ . Then, being $(\\overrightarrow{B}_{t})_{t\\in[0,1]}\\perp\\overrightarrow{X}_{0},(\\overrightarrow{B}_{t})_{t\\in[0,1]}$ .is $(\\mathcal{F}_{t})_{t\\in[0,1]}$ adatedand,a a consequeneo theMarkoproperty of $(\\overrightarrow{B}_{t})_{t\\in[0,1]}$ \uff0c\u4e2a $(\\overrightarrow{g}_{u}^{t})_{t\\in[0,1-u]}\\ =\\ (\\overrightarrow{B}_{t+u}\\ -\\ \\overrightarrow{B}_{u})_{t\\in[0,1-u]}\\ \\perp\\ \\mathcal{F}_{u}$ .On the other hand, it's well known that $(\\overrightarrow{X}_{t})_{t\\in[0,1]}$ .s. $(\\mathcal{F}_{t})_{t\\in[0,1]}$ -adapted. It therefore folows that $\\overrightarrow{g}_{\\textit{u}}^{t}\\perp(\\overrightarrow{X}_{\\textit{r}})_{\\boldsymbol{r}\\leq u}$ A similar arguments holds for $(\\overleftarrow{X}_{t})_{t\\in[0,1]}$ ", "page_idx": 18}, {"type": "text", "text": "A.4Proof of Theorem 2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We fix $0\\,<\\,\\epsilon\\,<\\,\\operatorname*{min}\\{h,1/2\\}$ and, for any $t\\,\\in\\,[0,1\\,-\\,\\epsilon]$ , we denote by $\\nu_{t}^{\\star}\\,=\\,\\mathrm{Law}(X_{t}^{\\mathrm{M}})$ and $\\nu_{t}^{\\theta^{\\star}}=\\mathrm{Law}(X_{t}^{\\theta^{\\star}})$ ", "page_idx": 19}, {"type": "text", "text": "First, using the data processing inequality [Nut21, Lemma 1.6], the standard decomposition of the KL divergence $[{\\mathrm{CCL}}^{+}23\\mathrm{b}$ , CLL23, CDS23] based on Girsanov theorem, triangle inequality and $\\mathbf{H}3$ we bound the KL divergence between $\\nu_{1-\\epsilon}^{\\star}$ and $\\nu_{1-\\epsilon}^{\\theta^{\\star}}$ asfollows ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\dot{\\boldmath~\\Xi~}}\\displaystyle\\mathrm{KL}(\\nu_{1-\\epsilon}^{*}|\\nu_{1-\\epsilon}^{\\theta^{*}})}\\\\ &{\\le\\mathrm{KL}(\\mathrm{Law}((X_{t}^{\\mathrm{M}})_{t\\in[0,1-\\epsilon]})|\\mathrm{Law}((X_{t}^{\\theta^{*}})_{t\\in[0,1-\\epsilon]}))}\\\\ &{\\lesssim\\displaystyle\\sum_{k=0}^{N-2}\\int_{t_{k}}^{t_{k+1}}\\mathbb{E}\\left[\\left\\|s_{\\theta^{*}}(t_{k},X_{t_{k}}^{\\mathrm{M}})-\\tilde{\\beta}_{t}(X_{t}^{\\mathrm{M}})\\right\\|^{2}\\right]\\mathrm{d}t+\\int_{1-h}^{1-\\epsilon}\\mathbb{E}\\left[\\left\\|s_{\\theta^{*}}(1-h,X_{1-h}^{\\mathrm{M}})-\\tilde{\\beta}_{t}(X_{t}^{\\mathrm{M}})\\right\\|^{2}\\right]\\mathrm{d}t}\\\\ &{\\lesssim\\displaystyle\\varepsilon^{2}+\\sum_{k=0}^{N-2}\\int_{t_{k}}^{t_{k+1}}\\mathbb{E}\\left[\\left\\|\\tilde{\\beta}_{t_{k}}(X_{t_{k}}^{\\mathrm{M}})-\\tilde{\\beta}_{t}(X_{t}^{\\mathrm{M}})\\right\\|^{2}\\right]\\mathrm{d}t+\\int_{1-h}^{1-\\epsilon}\\mathbb{E}\\left[\\left\\|\\tilde{\\beta}_{1-h}(X_{1-h}^{\\mathrm{M}})-\\tilde{\\beta}_{t}(X_{t}^{\\mathrm{M}})\\right\\|^{2}\\right]\\mathrm{d}t\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Second, we aim at bounding the RHS of (30) uniformly in $\\epsilon$ . Indeed, if we assume to be able to boundit with a constant $A$ independent of $\\epsilon$ thnusing weanr $X_{1-\\epsilon}^{\\mathrm{M}}$ 0 $X_{1}^{\\mathrm{I}}$ (whose law is given by $\\nu^{\\star}$ )as $\\epsilon\\rightarrow0$ , the continuity of $(X_{t}^{\\theta^{\\star}})_{t\\in[0,1]}$ , hence the weak convergence of $X_{1-\\epsilon}^{\\theta^{\\star}}$ to $X_{1}^{\\theta^{\\star}}$ (whose law is given by $\\nu_{1}^{\\theta^{\\star}}$ )as $\\epsilon\\to0$ , and the lower semi-continuity of the KL-divergence with respect to the weak convergence [VEH14, Theorem 19], we will get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname{KL}(\\nu^{\\star}|\\nu_{1}^{\\theta^{\\star}})\\leq\\operatorname*{lim}_{\\epsilon\\to0}\\operatorname{inf}_{\\epsilon\\to0}\\operatorname{KL}(\\nu_{1-\\epsilon}^{\\star}|\\nu_{1-\\epsilon}^{\\theta^{\\star}})\\lesssim\\operatorname*{lim}_{\\epsilon\\to0}A=A\\;.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let us therefore bound the RHS of (30). We will do so by using stochastic calculus tools, and, more precisely, Ito's formula. To this aim let us introduce the generator of $(X_{t}^{\\mathrm{M}})_{t\\in[0,1-\\epsilon]}$ which is defined for any $t\\in[0,1-\\epsilon]$ and $\\rho\\in C^{2}(\\ensuremath{\\mathbb{R}}^{d})$ as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{L}_{t}^{\\mathrm{M}}\\rho:=\\langle\\nabla_{x}\\rho,\\tilde{\\beta}_{t}\\rangle+\\Delta_{x}\\rho\\;.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Using Ito's formula, we get that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{d}\\tilde{\\beta}_{t}(X_{t}^{\\mathrm{M}})=(\\partial_{t}+\\mathcal{L}_{t}^{\\mathrm{M}})\\tilde{\\beta}_{t}(X_{t}^{\\mathrm{M}})\\mathrm{d}t+\\sqrt{2}D_{x}\\tilde{\\beta}_{t}(X_{t}^{\\mathrm{M}})\\mathrm{d}B_{t}\\;,\\quad t\\in[0,1-\\epsilon]\\;.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "So, applying Young inequality and Ito's isometry, we have that, for any $k=0,\\cdot\\cdot\\cdot\\,,N-1$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\bigg[\\bigg\\|\\tilde{\\beta}_{t_{k}}(X_{t_{k}}^{\\mathrm{M}})-\\tilde{\\beta}_{t}(X_{t}^{\\mathrm{M}})\\bigg\\|^{2}\\bigg]}\\\\ &{~~=\\mathbb{E}\\bigg[\\bigg\\|\\int_{t_{k}}^{t}(\\partial_{s}+\\mathcal{L}_{s}^{\\mathrm{M}})\\tilde{\\beta}_{s}(X_{s}^{\\mathrm{M}})\\mathrm{d}s+\\sqrt{2}\\int_{t_{k}}^{t}D_{x}\\tilde{\\beta}_{s}(X_{s}^{\\mathrm{M}})\\mathrm{d}B_{s}\\bigg\\|^{2}\\bigg]}\\\\ &{~~\\lesssim\\mathbb{E}\\bigg[\\bigg\\|\\int_{t_{k}}^{t}(\\partial_{s}+\\mathcal{L}_{s}^{\\mathrm{M}})\\tilde{\\beta}_{s}(X_{s}^{\\mathrm{M}})\\mathrm{d}s\\bigg\\|^{2}\\bigg]+2\\int_{t_{k}}^{t}\\mathbb{E}\\bigg[\\bigg\\|D_{x}\\tilde{\\beta}_{s}(X_{s}^{\\mathrm{M}})\\bigg\\|^{2}\\bigg]\\mathrm{d}s\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We now bound separately the two upper addends. To do so, we introduce the auxiliary measures $\\lambda_{k}^{h}(\\mathrm{d}s)\\in\\mathcal{P}([t_{k},t_{k+1}^{\\bar{\\mathbf{\\alpha}}}])$ for $k=0,...,N-2$ and $\\lambda_{N-1}^{h}(\\mathrm{d}s)\\in\\mathcal{P}([1-h,1-\\epsilon])$ which will help us, via a double change of measure argument, to mitigate the bad behaviour at $t=0$ and $t=1$ of the reciprocal characteristic of the mimicking drift $\\bar{\\mu}.e.,\\bar{\\partial}_{s}+\\mathcal{L}_{s}^{\\mathrm{M}})$ , which is the trickiest addend. Namely, for $k=0,...,N-2$ we consider the measures $\\lambda_{k}^{h}(\\mathrm{d}s)\\in\\mathcal{P}([t_{k},t_{k+1}])$ defined as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\lambda_{k}^{h}(\\mathrm{d}s)=\\frac{\\,\\rho(s)^{-1}}{Z_{k}}\\mathbb{1}_{[t_{k},t_{k+1}]}\\mathrm{d}s\\;,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\varrho(s)^{-1}=s^{-7/8}\\mathbb{1}_{\\{s\\leq1/2\\}}+(1-s)^{-7/8}\\mathbb{1}_{\\{s>1/2\\}}\\;,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and ", "page_idx": 19}, {"type": "equation", "text": "$$\nZ_{k}=\\int_{\\operatorname*{min}\\{t_{k},1/2\\}}^{\\operatorname*{min}\\{t_{k+1},1/2\\}}r^{-7/8}\\mathrm{d}r+\\int_{\\operatorname*{max}\\{t_{k},1/2\\}}^{\\operatorname*{max}\\{t_{k+1},1/2\\}}(1-r)^{-7/8}\\mathrm{d}r\\ .\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Whereas, for $k=N-1$ , we consider the measures $\\lambda_{N-1}^{h}(\\mathrm{d}s)\\in\\mathcal{P}([1-h,1-\\epsilon])$ defined as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\lambda_{N-1}^{h}(\\mathrm{d}s)=\\frac{\\mathsf{0}(s)^{-1}}{Z_{N-1}}\\mathbb{1}_{[1-h,1-\\epsilon]}\\mathrm{d}s\\;,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "with $\\rho(s)^{-1}$ as above and ", "page_idx": 20}, {"type": "equation", "text": "$$\nZ_{N-1}=\\int_{\\operatorname*{min}\\{1-h,1/2\\}}^{\\operatorname*{min}\\{1-\\epsilon,1/2\\}}r^{-7/8}\\mathrm{d}r+\\int_{\\operatorname*{max}\\{1-h,1/2\\}}^{\\operatorname*{max}\\{1-\\epsilon,1/2\\}}(1-r)^{-7/8}\\mathrm{d}r\\;.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that, for any $s\\in[0,1-\\epsilon]$ and for any $k=0,...,N-1$ , they hold ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathfrak{o}(s)\\lesssim1\\,,\\quad Z_{k}\\lesssim h^{1/8}\\,,\\quad\\int_{0}^{1-\\epsilon}\\mathfrak{o}^{-1}(s)\\mathrm{d}s=16\\left(\\frac{1}{2}\\right)^{1/8}-8\\epsilon^{1/8}\\lesssim1\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We start by bounding the first addend, that is the one that involves the reciprocal characteristic of the mimicking drift. With a first change of measure argument, we get for any $k=0,...,N-1$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\!\\left[\\left\\Vert\\int_{t_{k}}^{t}(\\partial_{s}+\\mathcal{L}_{s}^{\\mathrm{M}})\\tilde{\\beta}_{s}(X_{s}^{\\mathrm{M}})\\mathrm{d}s\\right\\Vert^{2}\\right]=Z_{k}^{2}\\mathbb{E}\\!\\left[\\left\\Vert\\int_{t_{k}}^{t}(\\partial_{s}+\\mathcal{L}_{s}^{\\mathrm{M}})\\tilde{\\beta}_{s}(X_{s}^{\\mathrm{M}})\\uprho(s)\\lambda_{k}^{h}(\\mathrm{d}s)\\right\\Vert^{2}\\right]\\,,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where, in the last inequality, we used (32). But then, if we apply Jensen inequality and use an other change of measure argument, we get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}\\bigg[\\bigg\\|\\int_{t_{k}}^{t}(\\partial_{s}+\\mathcal{L}_{s}^{\\mathrm{M}})\\tilde{\\beta}_{s}(X_{s}^{\\mathrm{M}})\\mathrm{d}s\\bigg\\|^{2}\\bigg]\\leq Z_{k}^{2}\\mathbb{E}\\bigg[\\int_{t_{k}}^{t}\\bigg\\|(\\partial_{s}+\\mathcal{L}_{s}^{\\mathrm{M}})\\tilde{\\beta}_{s}(X_{s}^{\\mathrm{M}})\\bigg\\|^{2}\\,\\mathsf{\\rho}(s)^{2}\\lambda_{k}^{h}(\\mathrm{d}s)\\bigg]}}\\\\ &{}&{\\leq Z_{k}\\int_{t_{k}}^{t}\\mathbb{E}\\bigg[\\bigg\\|(\\partial_{s}+\\mathcal{L}_{s}^{\\mathrm{M}})\\tilde{\\beta}_{s}(X_{s}^{\\mathrm{M}})\\bigg\\|^{2}\\bigg]\\,\\mathsf{\\rho}(s)\\mathrm{d}s}\\\\ &{}&{\\lesssim h^{1/8}\\int_{t_{k}}^{t}\\mathbb{E}\\bigg[\\left\\|(\\partial_{s}+\\mathcal{L}_{s}^{\\mathrm{M}})\\tilde{\\beta}_{s}(X_{s}^{\\mathrm{M}})\\right\\|^{2}\\bigg]\\,\\mathsf{\\rho}(s)\\mathrm{d}s\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let us now focus on the second addend. Remarkably, this addend can be bounded via the reciprocal characteristic of $\\tilde{\\beta}$ : because of Ito's formula, for $t\\in[0,1-\\epsilon]$ , it holds true ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{d}\\left\\|\\tilde{\\beta}_{t}(X_{t}^{\\mathrm{M}})\\right\\|^{2}=\\left\\{2\\langle\\tilde{\\beta}_{t},(\\partial_{t}+\\mathcal{L}_{t}^{\\mathrm{M}})\\tilde{\\beta}_{t}\\rangle+2\\left\\|D_{x}\\tilde{\\beta}_{t}\\right\\|^{2}\\right\\}(X_{t}^{\\mathrm{M}})\\mathrm{d}t+2\\sqrt{2}\\langle\\tilde{\\beta}_{t},D_{x}\\tilde{\\beta}_{t}\\rangle(X_{t}^{\\mathrm{M}})\\mathrm{d}B_{t}\\;.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Consequently, if we assume that the process $(\\int_{0}^{s}\\langle\\tilde{\\beta}_{t},D_{x}\\tilde{\\beta}_{t}\\rangle(X_{t}^{\\mathrm{M}})\\mathrm{d}B_{t})_{s\\in[0,1-\\epsilon]}$ is a true martingale (see Lemma 4 below), we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\displaystyle\\int_{t_{k}}^{t}\\mathbb{E}\\left[\\left\\|D_{x}\\tilde{\\beta}_{s}(X_{s}^{\\mathrm{M}})\\right\\|^{2}\\right]\\mathrm{d}s}\\\\ &{\\leq\\mathbb{E}\\left[\\left\\|\\tilde{\\beta}_{t}(X_{t}^{\\mathrm{M}})\\right\\|^{2}\\right]-\\mathbb{E}\\left[\\left\\|\\tilde{\\beta}_{t_{k}}(X_{t_{k}}^{\\mathrm{M}})\\right\\|^{2}\\right]+2\\Big|\\displaystyle\\int_{t_{k}}^{t}\\mathbb{E}[\\langle\\tilde{\\beta}_{s}(X_{s}^{\\mathrm{M}}),(\\partial_{s}+\\mathcal{L}_{s}^{\\mathrm{M}})\\tilde{\\beta}_{s}(X_{s}^{\\mathrm{M}})\\rangle]\\mathrm{d}s\\Big|\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "But then, using, as before, a double change of measure argument and applying Cauchy-Schwartz inequality, we can bound the above expression as follows ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\int_{t_{k}}^{t}\\mathbb{E}\\bigg[\\bigg\\|\\mathcal{D}_{x}\\mathcal{\\tilde{B}}_{x}(X_{s}^{\\lambda})\\bigg\\|^{2}\\bigg]\\,\\mathrm{d}s}\\\\ &{\\leq\\mathbb{E}\\bigg[\\bigg\\|\\tilde{\\mathcal{B}}_{t}(X_{s}^{\\lambda})\\bigg\\|^{2}\\bigg]-\\mathbb{E}\\bigg[\\bigg\\|\\tilde{\\mathcal{B}}_{t_{k}}(X_{s}^{\\lambda})\\bigg\\|^{2}\\bigg]}\\\\ &{\\quad+2\\varepsilon\\bigg[\\bigg\\|\\int_{t_{k}}^{t}\\mathbb{E}\\bigg[\\tilde{\\mathcal{B}}_{s}(X_{s}^{\\lambda}),(\\beta_{s}+\\mathcal{L}_{s}^{\\lambda})\\tilde{\\mathcal{B}}_{s}(X_{s}^{\\lambda}))\\bigg\\|\\rho(s)\\lambda_{k}^{\\lambda}(\\Delta\\Delta)\\bigg]}\\\\ &{\\leq\\mathbb{E}\\bigg[\\bigg\\|\\tilde{\\mathcal{B}}_{t}(X_{s}^{\\lambda})\\bigg\\|^{2}\\bigg]-\\mathbb{E}\\bigg[\\bigg\\|\\tilde{\\mathcal{B}}_{t_{k}}(X_{s}^{\\lambda})\\bigg\\|^{2}\\bigg]+\\mathbb{E}\\int_{t_{k}}\\bigg[\\bigg\\|\\tilde{\\mathcal{B}}_{s}(X_{s}^{\\lambda})\\bigg\\|^{2}\\bigg]\\lambda_{k}^{\\lambda}(\\Delta)}\\\\ &{\\quad+\\,\\mathbb{E}\\int_{t_{k}}\\bigg[\\bigg\\|(\\beta_{s}+\\mathcal{L}_{s}^{\\lambda})\\tilde{\\mathcal{B}}_{s}(X_{s}^{\\lambda})\\bigg\\|^{2}\\bigg]\\rho(s)^{2}\\lambda_{k}^{\\lambda}(\\Delta)}\\\\ &{=\\mathbb{E}\\bigg[\\bigg\\|\\tilde{\\mathcal{B}}_{t}(X_{s}^{\\lambda})\\bigg\\|^{2}\\bigg]-\\mathbb{E}\\bigg[\\bigg\\|\\tilde{\\mathcal{B}}_{t_{k}}(X_{s}^{\\lambda})\\bigg\\|^{2}\\bigg]+\\int_{t_{k}}^{t}\\mathbb{E}\\bigg[\\bigg\\|\\tilde{\\mathcal{B}}_{s}(X_{s}^{\\lambda})\\bigg\\|^{2}\\bigg]\\rho(s)^{-1}\\mathrm{d}s}\\\\ &{\\quad+\\int_{t_{k}}^{t}\\mathbb{E}\\bigg[\\bigg\\|(\\beta_{s}+\\mathcal{L}_{s}^{\\lambda})\\tilde{\\mathcal{B}}_{s}(X_{s}^{\\lambda}) \n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Plugging this bound and (33) in (30), we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}(\\nu_{1-\\epsilon}^{\\star}|\\nu_{1-\\epsilon}^{\\theta^{\\star}})\\lesssim\\varepsilon^{2}+h\\mathbb{E}\\Big[\\Big\\|\\tilde{\\beta}_{1-\\epsilon}(X_{1-\\epsilon}^{\\mathrm{M}})\\Big\\|^{2}\\Big]+h\\int_{0}^{1-\\epsilon}\\mathbb{E}\\Big[\\Big\\|\\tilde{\\beta}_{s}(X_{s}^{\\mathrm{M}})\\Big\\|^{2}\\Big]\\mathsf{\\rho}(s)^{-1}\\mathrm{d}s}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\ h(h^{1/8}+1)\\int_{0}^{1-\\epsilon}\\mathbb{E}\\Big[\\Big\\|(\\partial_{s}+\\mathcal{L}_{s}^{\\mathrm{M}})\\tilde{\\beta}_{s}(X_{s}^{\\mathrm{M}})\\Big\\|^{2}\\Big]\\mathsf{\\rho}(s)\\mathrm{d}s\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We now compute explicitly and upper bound each term appearing in the RHS of (34), recalling that, because of Theorem 1, for any $s\\in[0,1-\\epsilon]$ $\\nu_{s}^{\\star}=\\mathrm{Law}(\\dot{X}_{s}^{\\mathrm{I}})$ .We start with ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Big[\\left\\|\\tilde{\\beta}_{1-\\epsilon}(X_{1-\\epsilon}^{\\mathrm{M}})\\right\\|^{2}\\Big]\\lesssim\\int_{\\mathbb{R}^{d}}\\Big\\|\\frac{\\int_{\\mathbb{R}^{2d}}p_{1-\\epsilon}(x|x_{0})\\nabla_{x}p_{\\epsilon}(x_{1}|x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{1}\\mathrm{d}x_{0}}{p_{1-\\epsilon}^{\\mathrm{I}}(x)}\\Big\\|^{2}\\,p_{1-\\epsilon}^{\\mathrm{I}}(x)\\mathrm{d}x\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "First we use (18), second we integrate by part, third we apply Jensen inequality and last we rely on $\\mathbf{H}$ 2. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\bigg[\\bigg\\|\\tilde{\\beta}_{1-\\epsilon}(X_{1-\\epsilon}^{\\mathrm{M}})\\bigg\\|^{2}\\bigg]}\\\\ &{\\lesssim\\int_{\\mathbb{R}^{d}}\\bigg\\|\\frac{\\int_{\\mathbb{R}^{2d}}p_{1-\\epsilon}(x|x_{0})p_{\\epsilon}(x_{1}|x)(\\nabla_{x_{1}}\\tilde{\\pi}(x_{0},x_{1})/\\tilde{\\pi}(x_{0},x_{1}))\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}{p_{1-\\epsilon}^{1}(x)}\\bigg\\|^{2}p_{1-\\epsilon}^{\\mathrm{I}}(x)\\mathrm{d}x}\\\\ &{=\\mathbb{E}\\bigg[\\bigg\\|\\mathbb{E}\\bigg[\\frac{\\nabla_{x_{1}}\\tilde{\\pi}}{\\tilde{\\pi}}(X_{0}^{\\mathrm{I}},X_{1}^{\\mathrm{I}})\\bigg\\|X_{1-\\epsilon}^{\\mathrm{I}}\\bigg\\|^{2}\\bigg]\\leq\\mathbb{E}\\bigg[\\bigg\\|\\frac{\\nabla_{x_{1}}\\tilde{\\pi}}{\\tilde{\\pi}}(X_{0}^{\\mathrm{I}},X_{1}^{\\mathrm{I}})\\bigg\\|^{2}\\bigg|X_{1-\\epsilon}^{\\mathrm{I}}\\bigg]\\bigg]}\\\\ &{=\\mathbb{E}\\bigg[\\bigg\\|\\frac{\\nabla_{x_{1}}\\tilde{\\pi}}{\\tilde{\\pi}}(X_{0}^{\\mathrm{I}},X_{1}^{\\mathrm{I}})\\bigg\\|^{2}\\bigg]=\\bigg\\|\\frac{\\nabla_{x_{1}}\\tilde{\\pi}}{\\tilde{\\pi}}\\bigg\\|_{\\mathrm{L}^{2}(\\pi)}^{2}\\leq\\|\\nabla\\log\\tilde{\\pi}\\|_{\\mathrm{L}^{2}(\\pi)}^{2}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In the very same way we deal with the third term of the RHS of (34). ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int_{0}^{1-\\varepsilon}\\mathbb{E}\\bigg[\\bigg\\|\\bar{\\mathcal{B}}_{s}(X_{s}^{\\parallel})\\bigg\\|^{2}\\bigg]\\theta(s)^{-1}\\ensuremath{\\mathrm{d}}s}\\\\ &{\\lesssim\\int_{0}^{1-\\varepsilon}\\theta(s)^{-1}\\int_{\\mathbb{R}^{d}}\\bigg\\|\\frac{\\|\\bar{\\mathcal{B}}_{s}(X_{s}^{\\parallel})\\nabla_{\\theta}\\cdot\\mathcal{B}(\\cdot)\\|_{\\mathcal{B}}(\\cdot)\\|_{\\mathcal{B}}(\\cdot)}{\\theta(s)}\\frac{\\nabla_{\\theta}\\bar{\\mathcal{B}}(\\cdot)\\|_{\\mathcal{B}}(\\cdot)}{\\theta(s)}\\|_{\\mathcal{B}}\\bigg\\|_{\\mathcal{B}}^{2}\\mathbb{r}_{s}(s)\\ensuremath{\\mathrm{d}}s}\\\\ &{=\\int_{0}^{1-\\varepsilon}\\theta(s)^{-1}\\int_{\\mathbb{R}^{d}}\\bigg\\|\\frac{1}{\\|\\mathcal{B}_{s}^{\\parallel}\\|}\\int_{\\mathbb{R}^{d}}P_{\\theta}(s)\\|_{\\mathcal{B}}\\|_{\\mathcal{B}}\\|_{\\mathcal{B}}\\bigg\\|_{\\mathcal{B}}\\bigg\\|_{\\mathcal{B}}\\bigg\\|_{\\mathcal{B}}\\bigg\\|_{\\mathcal{B}}\\bigg\\|_{\\mathcal{B}}\\bigg\\|_{\\mathcal{B}}\\bigg\\|_{\\mathcal{B}}\\bigg\\|_{\\mathcal{B}}\\bigg\\|_{\\mathcal{B}}\\bigg\\|_{\\mathcal{B}}^{2}\\mathbb{r}_{s}(s)}\\\\ &{=\\int_{0}^{1-\\varepsilon}\\theta(s)^{-1}\\mathbb{E}\\bigg[\\bigg\\|\\frac{1}{\\|\\mathcal{B}_{s}^{\\parallel}\\|}\\bigg\\|\\frac{\\nabla_{\\theta}\\cdot\\mathbb{E}(\\cdot)}{\\theta(s)}\\|_{\\mathcal{B}}^{1}\\bigg\\|_{\\mathcal{B}}^{2}\\bigg\\|\\bigg]^{2}\\bigg\\|\\ensuremath{\\mathrm{d}}s}\\\\ &{\\lesssim\\int_{0}^{1-\\varepsilon}\\theta(s)^{-1}\\mathbb{E}\\bigg[\\bigg\\|\\frac{1}{\\|\\nabla_{\\theta}\\|}\\bigg\\|\\frac{\\nabla_{\\theta}\\cdot\\mathbb{E}(\\cdot)}{\\theta(s)}\\|_{\\mathcal{B}}^{1}\\bigg\\|_{\\mathcal{B}}^{2}\\bigg\\|\\times_{1}^{2}\\bigg]}\\\\ &{=\\int_{0}^{1-\\varepsilon}\\theta(s)^{- \n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where, in the last inequality, we used (32). We now turn to the last term, $i.e.$ ,to ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\int_{0}^{1-\\epsilon}\\mathbb{E}\\Big[\\left\\|(\\partial_{s}+\\mathcal{L}_{s}^{\\mathrm{M}})\\tilde{\\beta}_{s}(X_{s}^{\\mathrm{M}})\\right\\|^{2}\\Big]\\d{\\rho}(s)\\mathrm{d}s=\\int_{0}^{1-\\epsilon}\\int_{\\mathbb{R}^{d}}\\left\\|(\\partial_{s}+\\mathcal{L}_{s}^{\\mathrm{M}})\\tilde{\\beta}_{s}(x)\\right\\|^{2}\\d{\\rho}(s)p_{s}^{\\mathrm{I}}(x)\\mathrm{d}x\\,\\mathrm{d}s\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Some computations and (21) lead to ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\rangle_{\\mathcal{S}}\\!\\!\\tilde{\\beta}_{s}(x)}\\\\ &{=2\\frac{\\int_{\\mathbb{R}^{2d}}\\Delta_{x}p_{s}(x|x_{0})\\nabla_{x}p_{1-s}(x_{1}|x\\rangle\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}{p_{s}^{\\mathrm{\\Delta}}(x)}}\\\\ &{-\\,2\\frac{\\int_{\\mathbb{R}^{2d}}p_{s}(x|x_{0})\\nabla_{x}\\Delta_{x}p_{1-s}(x_{1}|x\\rangle\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}{p_{s}^{\\mathrm{\\Delta}}(x)}}\\\\ &{-\\,2\\frac{\\int_{\\mathbb{R}^{2d}}p_{s}(x|x_{0})\\nabla_{x}p_{1-s}(x_{1}|x\\rangle\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}\\int_{\\mathbb{R}^{2d}}\\Delta_{x}p_{s}(x|x_{0})p_{1-s}(x_{1}|x\\rangle\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}{(p_{s}^{\\mathrm{\\Delta}}(x))^{2}}}\\\\ &{+\\,2\\frac{\\int_{\\mathbb{R}^{2d}}p_{s}(x|x_{0})\\nabla_{x}p_{1-s}(x_{1}|x\\rangle\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}\\int_{\\mathbb{R}^{2d}}p_{s}(x|x_{0})\\Delta_{x}p_{1-s}(x_{1}|x\\rangle\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}{(p_{s}^{\\mathrm{\\Delta}}(x))^{2}}\\,;}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{x}\\Tilde{\\beta}_{s}(x)}\\\\ &{=2\\frac{\\int_{\\mathbb{R}^{2d}}\\nabla_{x}p_{1-s}(x_{1}|x)(\\nabla_{x}p_{s}(x|x_{0}))^{\\mathrm{T}}\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}{p_{s}^{\\mathrm{A}}(x)}}\\\\ &{+\\,2\\frac{\\int_{\\mathbb{R}^{2d}}p_{s}(x|x_{0})\\nabla_{x}^{2}p_{1-s}(x_{1}|x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}{p_{s}^{\\mathrm{A}}(x)}}\\\\ &{-\\,2\\biggl(\\frac{\\int_{\\mathbb{R}^{2d}}p_{s}(x|x_{0})\\nabla_{x}p_{1-s}(x_{1}|x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}{p_{s}^{\\mathrm{A}}(x)}\\biggr)\\biggl(\\frac{\\int_{\\mathbb{R}^{2d}}\\nabla_{x}p_{s}(x|x_{0})p_{1-s}(x_{1}|x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}{p_{s}^{\\mathrm{B}}(x)}}\\\\ &{-\\,2\\biggl(\\frac{\\int_{\\mathbb{R}^{2d}}p_{s}(x|x_{0})\\nabla_{x}p_{1-s}(x_{1}|x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}{p_{s}^{\\mathrm{B}}(x)}\\biggr)\\biggl(\\frac{\\int_{\\mathbb{R}^{2d}}p_{s}(x|x_{0})\\nabla_{x}p_{1-s}(x_{1}|x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}{p_{s}^{\\mathrm{B}}(x)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "hence ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2_{\\mathcal{X}}\\tilde{\\beta}_{s}(x)\\tilde{\\beta}_{s}(x)}\\\\ &{=4\\frac{\\int_{\\mathbb{R}^{2d}}\\nabla x\\cdot\\rho(x_{1}\\vert x)\\left(\\nabla_{x}\\rho_{s}(x)\\right)\\nabla\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}\\int_{\\mathbb{R}^{2d}}p_{s}(x|x_{0})\\nabla_{x}p_{1-s}(x_{1}\\vert x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}{p_{s}^{\\lambda}(x)}}\\\\ &{+\\frac{\\int_{\\mathbb{R}^{2d}}p_{s}(x)\\left(x_{0}\\vert x\\right)\\nabla_{x}^{2}p_{1-s}(x_{1}\\vert x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}\\int_{\\mathbb{R}^{2d}}p_{s}(x|x_{0})\\nabla_{x}p_{1-s}(x_{1}\\vert x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}{p_{s}^{\\lambda}(x)}}\\\\ &{-4\\left(\\frac{\\int_{\\mathbb{R}^{2d}}p_{s}(x)\\left(x_{0}\\vert x\\right)\\nabla_{x}p_{1-s}(x_{1}\\vert x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}{p_{s}^{\\lambda}(x)}\\right)\\left(\\frac{\\int_{\\mathbb{R}^{2d}}\\nabla_{x}p_{s}(x)\\left(x_{0}\\vert x\\right)\\nabla_{x}\\mathrm{d}x_{1}\\sigma}{p_{s}^{\\lambda}(x)}\\sigma_{(0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left.\\int_{\\mathbb{R}^{2d}}p_{s}(x)\\frac{\\int_{\\mathbb{R}^{2}}\\rho_{s}(x)\\left(x_{1}\\right)x_{1}\\left(x_{0}\\right)\\tilde{\\pi}(x_{0},x_{1})}{p_{s}^{\\lambda}(x)}}\\\\ &{-4\\frac{\\int_{\\mathbb{R}^{2d}}p_{s}(x)\\left(x_{0\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "+4(x)(a)(, $$\n\\begin{array}{r l}&{\\mathbb{E}\\{Q(t)>\\sum_{l}(\\alpha,\\cdot)\\}}\\\\ &{=\\underset{\\mathcal{C}_{u}=\\mathcal{C}_{v}}{\\int}\\underset{\\mathcal{C}_{u}=\\mathcal{C}_{v}}{\\int}(\\alpha,\\cdot)\\alpha(\\cdot)\\frac{1}{\\|\\alpha(\\cdot)(\\cdot)\\|_{\\mathcal{H}_{l}}(\\cdot)\\|_{\\mathcal{H}_{l}}(\\cdot)\\|_{\\mathcal{H}_{l}}(\\cdot)\\|_{\\mathcal{H}_{l}}(\\cdot)\\}}\\\\ &{\\quad\\quad\\quad\\quad\\cdot\\int_{\\Omega}\\mathbb{E}\\{Q(t)\\|\\alpha_{\\mathcal{H}_{l}}(\\cdot)\\|_{\\mathcal{H}_{l}}(\\cdot)\\|_{\\mathcal{H}_{l}}(\\cdot)\\|_{\\mathcal{H}_{l}}(\\cdot)\\|_{\\mathcal{H}_{l}}(\\cdot)\\|_{\\mathcal{H}_{l}}(\\cdot)\\|_{\\mathcal{H}_{l}}(\\cdot)\\|_{\\mathcal{H}_{l}}(\\cdot)\\|_{\\mathcal{H}_{l}}(\\cdot)\\|_{\\mathcal{H}_{l}}(\\cdot)\\|_{\\mathcal{H}_{l}}(\\cdot)\\|_{\\mathcal{H}_{l}}(\\cdot)\\|_{\\mathcal{H}_{l}}(\\cdot)\\|_{\\mathcal{H}_{l}}(\\cdot)\\|_{\\mathcal{H}_{l}}(\\cdot)\\|_{\\mathcal{H}_{l}}(\\cdot)\\|_{\\mathcal{H}_{l}}(\\cdot)\\|_{\\mathcal{H}_{l}}(\\cdot)\\|_{\\mathcal{H}_{l}}(\\cdot)\\|_{\\mathcal{H}_{l}}(\\cdot)\\|_{\\mathcal{H}_{l}}(\\cdot)\\|_{\\mathcal{H}_{l}}(\\cdot)\\|_{\\mathcal{H}_{l}}(\\cdot)\\|_{\\mathcal{H}_{l}}(\\cdot)\\|_{\\mathcal{H}_{l}}(\\cdot)\\|_{\\mathcal{H}_{l}}(\\cdot)\\|_{\\mathcal{H}_{l}}(\\cdot)\\|_{\\mathcal{H}_{l}}(\\cdot)\\|_{\\mathcal{H}_{l}}(\\cdot)\\|_{\\mathcal{H}_{l}}(\\cdot)\\|_{\\mathcal{H}_{l}}(\\cdot)\\|_{\\mathcal{H}_{l}}(\\cdot)\\|_{\\mathcal{H}_{l}}(\\cdot)\\|_{\\mathcal{H}_{l}}(\\cdot)\\|_{\\mathcal{H}_{l}}(\\cdot)\\|_{\\mathcal{H}_{l}}(\\cdot)\\|_{\\mathcal{H}_{l}\n$$$$\n\\begin{array}{r l}&{\\begin{array}{r l}&{-i\\beta\\cos^{(2)}\\phi_{i-1}(i)!\\dot{\\phi}_{i-1}(i)!\\dot{\\phi}_{i-1}(i)!\\dot{\\phi}_{i-1}(i)!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{i-1}(i)\\!\\dot{\\phi}_{\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "So, we get ", "page_idx": 23}, {"type": "equation", "text": "$$\n(\\partial_{s}+\\mathcal{L}_{s}^{\\mathrm{M}})\\tilde{\\beta}_{s}(x)=\\sum_{k=1}^{6}A_{s}^{k}(x)\\;,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we have defined ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{s}^{1}(x)=-4\\frac{\\int_{\\mathbb{R}^{2d}}\\nabla_{x}p_{1-s}(x_{1}|x)(\\nabla_{x}p_{s}(x|x_{0}))^{\\mathrm{T}}\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}{p_{s}^{\\mathrm{I}}(x)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\cdot\\underbrace{\\int_{\\mathbb{R}^{2d}}\\nabla_{x}p_{s}(x|x_{0})p_{1-s}(x_{1}|x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}_{p_{s}^{\\mathrm{I}}(x)}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{s}^{2}(x)=-4\\frac{\\int_{\\mathbb{R}^{2d}}\\langle\\nabla_{x}p_{1-s}(x_{1}\\vert x),\\nabla_{x}p_{s}(x\\vert x_{0})\\rangle\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}{p_{s}^{\\mathrm{I}}(x)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\cdot\\underbrace{\\int_{\\mathbb{R}^{2d}}p_{s}(x\\vert x_{0})\\nabla_{x}p_{1-s}(x_{1}\\vert x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}_{p_{s}^{\\mathrm{I}}(x)}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{s}^{3}(x)=4\\frac{\\left\\|\\int_{\\mathbb{R}^{2d}}\\nabla_{x}p_{s}(x|x_{0})p_{1-s}(x_{1}|x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}\\right\\|^{2}}{(p_{s}^{\\mathrm{I}}(x))^{2}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\cdot\\underbrace{\\int_{\\mathbb{R}^{2d}}p_{s}(x|x_{0})\\nabla_{x}p_{1-s}(x_{1}|x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}_{p_{s}^{\\mathrm{I}}(x)}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{s}^{4}(x)=4\\frac{\\int_{\\mathbb{R}^{2d}}p_{s}\\left(x|x_{0}\\right)\\nabla_{x}p_{1-s}\\left(x_{1}|x\\right)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}{p_{s}^{\\mathrm{I}}(x)}}\\\\ &{\\left(\\frac{\\int_{\\mathbb{R}^{2d}}\\nabla_{x}p_{s}\\left(x\\right|x_{0}\\right)p_{1-s}\\left(x_{1}\\left|x\\right)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}{p_{s}^{\\mathrm{I}}(x)}\\right)^{\\top}\\frac{\\int_{\\mathbb{R}^{2d}}p_{s}\\left(x\\left|x_{0}\\right|\\nabla_{x}p_{1-s}\\left(x_{1}\\right|x\\right)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}{p_{s}^{\\mathrm{I}}(x)}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad A_{s}^{5}(x)=4\\frac{\\int_{\\mathbb{R}^{2d}}\\Delta_{x}p_{s}(x|x_{0})\\nabla_{x}p_{1-s}(x_{1}|x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}{p_{s}^{\\mathrm{I}}(x)}}\\\\ &{-4\\frac{\\int_{\\mathbb{R}^{2d}}\\Delta_{x}p_{s}(x|x_{0})p_{1-s}(x_{1}|x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}\\int_{\\mathbb{R}^{2d}}p_{s}(x|x_{0})\\nabla_{x}p_{1-s}(x_{1}|x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}{(p_{s}^{\\mathrm{I}}(x))^{2}}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad A_{s}^{6}(x)=4\\frac{\\int_{\\mathbb{R}^{2d}}\\nabla_{x}^{2}p_{1-s}(x_{1}\\vert x)\\nabla_{x}p_{s}(x\\vert x_{0})\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}{p_{s}^{1}(x)}}\\\\ &{\\quad-4\\frac{\\int_{\\mathbb{R}^{2d}}p_{s}(x\\vert x_{0})\\nabla_{x}^{2}p_{1-s}(x_{1}\\vert x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}\\int_{\\mathbb{R}^{2d}}\\nabla_{x}p_{s}(x\\vert x_{0})p_{1-s}(x_{1}\\vert x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}{(p_{s}^{1}(x))^{2}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\int_{0}^{1-\\epsilon}\\mathbb{E}\\Big[\\left\\|(\\partial_{s}+\\mathscr{L}_{s}^{\\mathrm{M}})\\widetilde{\\beta}_{s}(X_{s}^{\\mathrm{M}})\\right\\|^{2}\\Big]\\d{\\rho}(s)\\mathrm{d}s\\lesssim\\sum_{k=1}^{6}\\int_{0}^{1-\\epsilon}\\int_{\\mathbb{R}^{d}}\\left\\|A_{s}^{k}(x)\\right\\|^{2}\\d{\\rho}(s)p_{s}^{\\mathrm{I}}(x)\\mathrm{d}x\\,\\mathrm{d}s\\;.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We now bound each term $A_{s}^{k}$ in the sum. Using (32) and Young inequality, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int_{0}^{1-\\epsilon}\\int_{\\mathbb{R}^{d}}\\left\\|A_{s}^{1}(x)\\right\\|^{2}\\mathsf{\\rho}(s)p_{s}^{\\mathsf{I}}(x)\\mathrm{d}x\\,\\mathrm{d}s}\\\\ &{\\lesssim\\int_{0}^{1-\\epsilon}\\int_{\\mathbb{R}^{d}}\\left\\|\\frac{\\int_{\\mathbb{R}^{2d}}\\nabla_{x}p_{s}(x|x_{0})p_{1-s}(x_{1}|x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}{p_{s}^{\\mathsf{I}}(x)}\\right\\|^{4}p_{s}^{\\mathsf{I}}(x)\\mathrm{d}x\\mathrm{d}s}\\\\ &{\\quad+\\int_{0}^{1-\\epsilon}\\int_{\\mathbb{R}^{d}}\\left\\|\\frac{\\int_{\\mathbb{R}^{2d}}\\nabla_{x}p_{s}(x|x_{0})(\\nabla_{x}p_{1-s}(x_{1}|x))^{\\mathsf{T}}\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}{p_{s}^{\\mathsf{I}}(x)}\\right\\|_{\\mathrm{op}}^{4}p_{s}^{\\mathsf{I}}(x)\\mathrm{d}x\\mathrm{d}s\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "To bound the first term we proceed as in (36), that is, we first exploit (18) and the integration by part formula and we then use Jensen inequality and the properties of the conditional expectation. ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\int_{0}^{1-\\epsilon}\\int_{\\mathbb{R}^{d}}\\left\\|\\frac{\\int_{\\mathbb{R}^{2d}}\\nabla_{x}p_{s}(x|x_{0})p_{1-s}(x_{1}|x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}{p_{s}^{\\mathrm{I}}(x)}\\right\\|^{4}p_{s}^{\\mathrm{I}}(x)\\mathrm{d}x\\mathrm{d}s\\leq\\|\\nabla\\log\\tilde{\\pi}\\|_{\\mathrm{L}^{4}(\\pi)}^{4}\\ .\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "To bound the second term we first split the time interval $[0,1-\\epsilon]$ in two, $[0,1/2]$ and $[1/2,1-\\epsilon]$ we second make either $\\nabla_{x}p_{s}(x|x_{0})$ or $\\nabla_{x}p_{1-s}(x_{1}|x)$ explicit and we last proceed as before, i.e., we exploit (18), we integrate by parts and we use Young and Jensen inequalities. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int_{\\mathbb{R}_{+}}^{+\\infty}\\int_{\\mathbb{R}_{+}}\\left[\\frac{\\int_{\\mathbb{R}_{+}}\\mathbf{v}_{\\alpha}\\exp_{t}(\\mathbf{r},\\mathbf{f}(\\mathbf{r}))\\cdot\\mathbf{f}(\\mathbf{r}_{-},\\mathbf{f}(\\mathbf{r})))^{\\mathbb{T}}(\\mathbb{R}_{+})\\cdot\\mathrm{d}\\mathbf{f}(\\mathbf{r}_{-})\\,\\mathrm{d}\\mathbf{f}\\right]=}\\\\ &{=\\int_{0}^{t/2}\\int_{\\mathbb{R}_{+}}\\left[\\frac{\\int_{\\mathbb{R}_{+}}\\mathbf{v}_{\\alpha}\\exp_{t}(\\mathbf{r},\\mathbf{f}(\\mathbf{r}))\\cdot\\mathbf{f}(\\mathbf{r}_{-},\\mathbf{f}(\\mathbf{r}))\\cdot\\mathbf{f}(\\mathbf{r}_{-},\\mathbf{f})\\cdot\\mathrm{d}\\mathbf{f}(\\mathbf{r}_{-})\\,\\mathrm{d}\\mathbf{f}\\right]=}\\\\ &{\\quad+\\int_{\\mathbb{R}_{+}}^{t/2}\\int_{\\mathbb{R}_{+}}\\left[\\frac{\\int_{\\mathbb{R}_{+}}\\mathbf{v}_{\\alpha}\\exp_{t}(\\mathbf{r},\\mathbf{f}(\\mathbf{r}_{-}))\\cdot\\mathbf{f}(\\mathbf{r}_{-},\\mathbf{f}(\\mathbf{r}))\\cdot\\mathbf{f}(\\mathbf{r}_{-},\\mathbf{f})\\cdot\\mathrm{d}\\mathbf{f}(\\mathbf{r}_{+})\\right]=}\\\\ &{\\le\\int_{0}^{t/2}\\int_{\\mathbb{R}_{+}}\\left[\\frac{\\int_{\\mathbb{R}_{+}}\\mathbf{v}_{\\alpha}\\exp_{t}(\\mathbf{r},\\mathbf{f}(\\mathbf{r}_{-}))\\cdot\\mathbf{f}(\\mathbf{r}_{-},\\mathbf{f}(\\mathbf{r}_{-}))\\cdot\\mathbf{f}(\\mathbf{r}_{+})\\cdot\\mathbf{f}(\\mathbf{r}_{+})\\right]\\,\\mathrm{d}\\mathbf{f}\\right]=}\\\\ &{\\le\\int_{0}^{t/2}\\int_{\\mathbb{R}_{+}}^{t}\\int_{\\mathbb{R}_{+}}^{\\infty}\\int_{\\mathbb{R}_{+}}\\frac{\\sum_{t}\\mathbf{v}_{\\alpha}\\cdot\\mathbf{f}(\\mathbf{r}_{-})}{\\mu}\\frac{(\\mathbf{r}-\\mathbf{f})}{1-\\mathbf{v}}\\exp_{t}(\\mathbf{r}|\\mathbf{f}\n$$$$\n\\begin{array}{r l}&{\\lesssim\\int_{0}^{1/2}\\mathbb{E}\\bigg[\\bigg\\|\\Big(\\frac{\\nabla_{x_{0}}\\tilde{\\pi}}{\\tilde{\\pi}}(X_{0}^{\\mathrm{I}},X_{1}^{\\mathrm{I}})\\Big)(X_{1}^{\\mathrm{I}}-X_{s}^{\\mathrm{I}})^{\\mathrm{T}}\\bigg\\|_{\\mathrm{op}}^{4}\\bigg]\\,\\mathrm{d}s}\\\\ &{~~+\\int_{1/2}^{1}\\mathbb{E}\\bigg[\\bigg\\|\\Big(\\frac{\\nabla_{x_{1}}\\tilde{\\pi}}{\\tilde{\\pi}}(X_{0}^{\\mathrm{I}},X_{1}^{\\mathrm{I}})\\Big)(X_{s}^{\\mathrm{I}}-X_{0}^{\\mathrm{I}})^{\\mathrm{T}}\\bigg\\|_{\\mathrm{op}}^{4}\\bigg]\\,\\mathrm{d}s}\\\\ &{\\lesssim\\int_{0}^{1/2}\\mathbb{E}\\Big[\\left\\|X_{1}^{\\mathrm{I}}-X_{s}^{\\mathrm{I}}\\right\\|^{8}\\Big]\\mathrm{d}s+\\int_{1/2}^{1}\\mathbb{E}\\Big[\\left\\|X_{s}^{\\mathrm{I}}-X_{0}^{\\mathrm{I}}\\right\\|^{8}\\Big]\\mathrm{d}s+\\left\\|\\nabla\\log\\tilde{\\pi}\\right\\|_{\\mathrm{L}^{8}(\\pi)}^{8}}\\\\ &{\\lesssim d^{4}+\\mathbf{m}_{8}[\\mu]+\\mathbf{m}_{8}[\\nu^{\\mathrm{I}}]+\\|\\nabla\\log\\tilde{\\pi}\\|_{\\mathrm{L}^{8}(\\pi)}^{8}\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where, in the last inequality, we have used Lemma 1. In a similar way we get ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\int_{0}^{1-\\epsilon}\\int_{\\ensuremath{{\\mathbb R}}^{d}}\\left\\|A_{s}^{2}(x)\\right\\|^{2}\\rho(s)p_{s}^{\\mathrm{I}}(x)\\mathrm{d}x\\,\\mathrm{d}s\\lesssim d^{4}+\\mathbf{m}_{8}[\\mu]+\\mathbf{m}_{8}[\\nu^{\\star}]+\\|\\nabla\\log\\tilde{\\pi}\\|_{\\mathrm{L}^{8}(\\pi)}^{8}\\ .\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The argument to bound $A_{s}^{2}$ resembles the one used to bound $A_{s}^{1}$ and we therefore omit it. ", "page_idx": 25}, {"type": "text", "text": "Wenowfocus on $A_{s}^{3}$ . To bound such term, we first use (32) and Young inequality and we second proceed as in (40) and (36). ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int_{0}^{1-\\epsilon}\\int_{\\mathbb{R}^{d}}\\left\\|A_{s}^{3}(x)\\right\\|^{2}\\rho(s)p_{s}^{\\mathrm{I}}(x)\\mathrm{d}x\\mathrm{~d}s}\\\\ &{\\lesssim\\int_{0}^{1-\\epsilon}\\int_{\\mathbb{R}^{d}}\\left\\|\\frac{\\int_{\\mathbb{R}^{2d}}\\nabla_{x}p_{s}(x|x_{0})p_{1-s}(x_{1}|x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}{p_{s}^{\\mathrm{I}}(x)}\\right\\|^{s}p_{s}^{\\mathrm{I}}(x)\\mathrm{d}x\\mathrm{d}s}\\\\ &{\\quad+\\int_{0}^{1-\\epsilon}\\int_{\\mathbb{R}^{d}}\\left\\|\\frac{\\int_{\\mathbb{R}^{2d}}p_{s}(x|x_{0})\\nabla_{x}p_{1-s}(x_{1}|x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}{p_{s}^{\\mathrm{I}}(x)}\\right\\|^{4}p_{s}^{\\mathrm{I}}(x)\\mathrm{d}x\\mathrm{d}s}\\\\ &{\\lesssim\\mathbb{E}\\left[\\left\\|\\frac{\\nabla_{x_{0}}\\tilde{\\pi}}{\\tilde{\\pi}}(X_{0}^{1},X_{1}^{1})\\right\\|^{s}\\right]+\\mathbb{E}\\left[\\left\\|\\frac{\\nabla_{x_{1}}\\tilde{\\pi}}{\\tilde{\\pi}}(X_{0}^{1},X_{1}^{1})\\right\\|^{4}\\right]}\\\\ &{\\lesssim\\|\\nabla\\log\\tilde{\\pi}\\|_{\\mathrm{L}^{s}(\\pi)}^{s}+\\|\\nabla\\log\\tilde{\\pi}\\|_{\\mathrm{L}^{4}(\\pi)}^{4}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proceeding in a similar way (we omit the argument, as it is almost a duplication of the previous one), weget ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\int_{0}^{1-\\epsilon}\\int_{\\mathbb{R}^{d}}\\left\\|A_{s}^{4}(x)\\right\\|^{2}\\rho(s)p_{s}^{\\mathrm{I}}(x)\\mathrm{d}x\\,\\mathrm{d}s\\lesssim\\|\\nabla\\log\\tilde{\\pi}\\|_{\\mathrm{L}^{8}(\\pi)}^{8}+\\|\\nabla\\log\\tilde{\\pi}\\|_{\\mathrm{L}^{4}(\\pi)}^{4}\\ .\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We now turn to $A_{s}^{5}$ . Because of (20), $A_{s}^{5}$ rewrites as ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{A_{s}^{5}(x)=\\cfrac{1}{p_{s}^{1}(x)}\\int_{\\mathbb{R}^{2d}}\\bigg\\{\\frac{-d}{2s}+\\cfrac{\\|x-x_{0}\\|^{2}}{4s^{2}}\\bigg\\}\\frac{x_{1}-x}{2(1-s)}p_{s}(x|x_{0})p_{1-s}(x_{1}|x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}\\\\ &{\\qquad-\\bigg(\\frac{1}{p_{s}^{1}(x)}\\int_{\\mathbb{R}^{2d}}\\bigg\\{\\frac{-d}{2s}+\\cfrac{\\|x-x_{0}\\|^{2}}{4s^{2}}\\bigg\\}p_{s}(x|x_{0})p_{1-s}(x_{1}|x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}\\bigg)}\\\\ &{\\qquad\\cdot\\bigg(\\frac{1}{p_{s}^{1}(x)}\\int_{\\mathbb{R}^{2d}}\\frac{x_{1}-x}{2(1-s)}p_{s}(x|x_{0})p_{1-s}(x_{1}|x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}\\bigg)}\\\\ &{\\lesssim\\cfrac{1}{p_{s}^{1}(x)}\\int_{\\mathbb{R}^{2d}}\\cfrac{\\|x-x_{0}\\|^{2}\\tau_{1}-x}{s^{2}}\\frac{p_{s}}{1-s}p_{s}(x|x_{0})p_{1-s}(x_{1}|x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}\\\\ &{\\qquad-\\bigg(\\frac{1}{p_{s}^{1}(x)}\\int_{\\mathbb{R}^{2d}}\\cfrac{\\|x-x_{0}\\|^{2}}{s^{2}}p_{s}(x|x_{0})p_{1-s}(x_{1}|x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}\\bigg)}\\\\ &{\\qquad\\cdot\\bigg(\\frac{1}{p_{s}^{1}(x)}\\int_{\\mathbb{R}^{2d}}\\cfrac{x_{1}-x}{1-s}p_{s}(x|x_{0})p_{1-s}(x_{1}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Therefore, we have that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int_{0}^{1-\\epsilon}\\int_{\\mathbb{R}^{d}}\\left\\|A_{s}^{5}(x)\\right\\|^{2}\\,\\rho(s)p_{s}^{\\mathrm{I}}(x)\\mathrm{d}x\\;\\mathrm{d}s\\lesssim\\int_{0}^{1-\\epsilon}\\mathbb{E}\\Bigg[\\Bigg\\|\\mathbb{E}\\Bigg[\\frac{\\left\\|X_{s}^{\\mathrm{I}}-X_{0}^{\\mathrm{I}}\\right\\|^{2}}{s^{2}}\\frac{X_{1}^{\\mathrm{I}}-X_{s}^{\\mathrm{I}}}{1-s}\\bigg|X_{s}^{\\mathrm{I}}\\Bigg]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad-\\mathbb{E}\\Bigg[\\frac{\\left\\|X_{s}^{\\mathrm{I}}-X_{0}^{\\mathrm{I}}\\right\\|^{2}}{s^{2}}\\bigg|X_{s}^{\\mathrm{I}}\\bigg]\\mathbb{E}\\Bigg[\\frac{X_{1}^{\\mathrm{I}}-X_{s}^{\\mathrm{I}}}{1-s}\\bigg|X_{s}^{\\mathrm{I}}\\Bigg]\\Bigg\\|^{2}\\Bigg]\\,\\rho(s)\\mathrm{d}s\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We now split the time interval $[0,1-\\epsilon]$ in two, $[0,1/2]$ $[1/2,1-\\epsilon]$ and we focus on the first one. So we look at $s\\in[0,1/2]$ and we try to bound the integrand. Using Lemma 1, Lemma 2, Lemma 3 and standard and well-known inequalities (Cauchy-Schwarz, Young and Jensen inequalities), we get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{E}\\left[\\left|{\\left|{\\frac{{\\left|{\\bar{X}}_{t}}\\right|{\\left|{\\bar{X}}_{t}-{\\bar{X}}_{t}^{\\perp}\\right|}}{\\alpha_{t}^{2}}}\\right|}^{2}{\\bar{X}}_{t}-{\\bar{X}}_{t}^{\\perp}\\right|}\\right|\\times\\right]-\\alpha\\left[{\\frac{{\\left|{\\bar{X}}_{t}-{\\bar{X}}_{t}\\right|}}{\\alpha_{t}^{2}}}{\\bar{\\alpha}}\\right]^{2}{\\left|{\\bar{X}}_{t}\\right|}{\\left|{\\bar{X}}_{t}^{\\perp}\\right|}-\\alpha\\left|\\right]^{2}}\\\\ &{=\\mathbf{E}\\left[\\left|{\\left|{\\frac{{\\left|{\\bar{X}}_{t}-{\\bar{X}}_{t}-{\\bar{X}}_{t}}\\right|}{\\alpha_{t}}}\\right|}^{2}{\\frac{\\sum_{\\alpha_{t}={\\bar{X}}_{t}-{\\bar{X}}_{t}}{\\left|{\\bar{X}}_{t}-{\\bar{X}}_{t}}\\right|}{1-\\alpha_{t}}}\\right|\\right]}\\\\ &{\\quad-\\alpha{\\left|{\\frac{{\\left|{\\bar{X}}_{t}-{\\bar{X}}_{t}}\\right|}{\\alpha_{t}}}\\right|}^{2}{\\left|{\\bar{X}}_{t}\\right|}-\\left|{\\frac{{\\left|{\\bar{X}}_{t}-{\\bar{X}}_{t}-{\\bar{X}}_{t}\\right|}}{\\alpha_{t}}}{\\left|{\\bar{X}}_{t}-{\\bar{X}}_{t}\\right|}\\right|}^{2}{\\left|{\\bar{Y}}_{t}\\right|}^{2}}\\\\ &{=\\mathbf{E}\\left[\\left|{\\left|{\\frac{{\\left|{\\bar{Y}}_{t}-{\\bar{Y}}_{t}}\\right|}{\\alpha_{t}}}\\right|}^{2}{\\bar{X}}_{t}\\right]-\\alpha{\\left|{\\bar{Y}}_{t}-{\\bar{Y}}_{t}\\right|}^{2}{\\left|{\\bar{X}}_{t}\\right|}-{\\frac{1}{\\alpha_{t}}}{\\left|{\\bar{X}}_{t}-{\\bar{X}}_{t}\\right|}{\\left|{\\bar{X}}_{t}-{\\bar{Y}}_{t}\\right|}}\\right]}\\\\ &{\\quad-\\alpha{\\left|{\\frac{{\\left|{\\bar{Y}}_{t\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{1}\\Big[\\prod_{j=1}^{1}\\textstyle\\frac{a_{j}^{2}}{\\alpha_{j}}\\Big.\\Bigg.}\\\\ &{\\left.\\!\\!-\\mathbb{E}\\Bigg[\\Bigg[\\frac{\\prod_{j=1}^{1}\\prod_{i=1}^{j=1}\\alpha_{j}\\Big(P_{j}\\Big)-\\alpha_{j}\\Big(P_{j-1,i}\\Big)+\\prod_{i=1}^{1}\\Bigg[X_{i-1}\\Bigg]\\Big[X_{i-1}\\Bigg]\\Bigg[X_{i-1}\\Bigg]\\Bigg[X_{i-1}\\Bigg]\\right]^{\\!2}\\Bigg],}\\\\ &{\\le\\mathbb{E}\\Bigg[\\Bigg[\\Bigg|\\frac{\\prod_{j=1}^{1}\\alpha_{j}\\Big(\\frac{a_{j}^{2}}{\\alpha_{j}}\\Big(X_{i-1}\\cdot\\nabla_{i-1,i-1}\\Big)\\Big)}{\\alpha_{j}}(X_{i-1}-X_{i-1})\\Big|\\Bigg]\\Bigg]\\Bigg[\\alpha_{j}\\Bigg]^{\\!2}\\Bigg[\\alpha_{1}\\Bigg]^{\\!2}}\\\\ &{\\Bigg.\\!\\!-\\mathbb{E}\\Bigg[\\Bigg|\\frac{\\prod_{j=1}^{1}\\alpha_{j}\\Big(\\frac{a_{j}^{2}}{\\alpha_{j}}\\Big(X_{i-1}\\cdot\\nabla_{i-1,i-1}\\Big)\\Big)}{\\alpha_{j}}\\Big|X_{i-1}\\Bigg]\\Bigg[\\nabla_{i}\\Bigg]\\!\\!-\\!\\!-\\!\\!\\!-\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$$$\n\\begin{array}{r l}&{\\quad-\\kappa\\Big[\\displaystyle\\prod_{i=1}^{n}\\|\\displaystyle\\sum_{s=0}^{t-1}(2\\gamma_{1}^{t}-\\mathbb{R}_{+}^{n-1},\\frac{\\gamma_{1}}{\\gamma}[\\sigma_{1},\\sigma_{1}^{t}])\\|_{\\infty}\\Big]\\le\\Big[\\sigma_{1}-\\sum_{1=1}^{n}\\Big(\\displaystyle\\sum_{s=0}^{\\infty}\\Big(\\displaystyle\\sum_{u=1}^{n}\\Big(\\displaystyle\\sum_{s=0}^{t-1}\\Big(\\displaystyle\\sum_{s=0}^{t-1}\\Big(\\displaystyle\\sum_{s=0}^{t-1}\\Big(\\displaystyle\\sum_{s=0}^{t-1}\\Big)\\Big(\\displaystyle\\sum_{s=0}^{t-1}\\Big(\\displaystyle\\sum_{s=0}^{t-1}\\Big(\\displaystyle\\sum_{s=0}^{t-1}\\Big)\\Big(\\displaystyle\\sum_{s=0}^{t-1}\\Big)\\Big(\\displaystyle\\sum_{s=0}^{t-1}\\Big(\\displaystyle\\sum_{s=0}^{t-1}\\Big(\\displaystyle\\sum_{s=0}^{t-1}\\Big)\\Big(\\displaystyle\\sum_{s=0}^{t-1}\\Big(\\displaystyle\\sum_{s=0}^{t-1}\\Big)\\Big(\\Big\\lvert\\nabla_{\\lambda}\\Big)\\Big(\\displaystyle\\sum_{s=0}^{t-1}\\Big(\\displaystyle\\sum_{s=1}^{t-1}\\Big(\\displaystyle\\sum_{s=0}^{t-1}\\Big(\\displaystyle\\sum_{s=0}^{t-1}\\Big(\\Big)\\Big(\\displaystyle\\sum_{s=0}^{t-1}\\Big\\Big(\\bigg)\\Big)\\Big)\\Big)\\Big)\\Big)^{\\lambda}))^{\\lambda})^{\\lambda}}\\\\ &{\\quad\\le\\kappa\\Big[\\displaystyle\\Bigg\\|\\displaystyle\\sum_{s=1}^{t-1}\\displaystyle\\sum_{s=0}^{t-1}\\Big(\\displaystyle\\sum_{s=0}^{t-1}\\Big(\\displaystyle\\sum_{s=0}^{t-1}\\Big(\\displaystyle\\sum_{s=0}^{t-1}\\Big(\\displaystyle\\sum_{s=0}^{t-1}\\Big(\\displaystyle\\sum_{s=0}^{t-1}\\Big(\\displaystyle\\sum_{s=0}^{t-1}\\Big(\\displaystyle\\sum_{s=0}^{t-1}\\Big(\\Big)\\Big(\\displaystyle\\sum_{s=0}^{t-1}\\Big(\\Big\\bigg)\\Big(\\sum_{s=0}^{t-1}\\Big(\\bigg)\\Big(\\sum_{s=0}^{t-1}\\Big(\\bigg)\\Big(\\sum_{s=0}^{t-1}\\Big \n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lesssim\\mathbb{E}\\Bigg[\\frac{\\left\\|\\overleftarrow{\\mathcal{f}}_{1-s}^{s}\\right\\|^{8}}{s^{8}}\\Bigg]+\\mathbb{E}\\Bigg[\\frac{\\left\\|\\overleftarrow{\\mathcal{G}}_{1-s}^{s}\\right\\|^{8}}{s^{8}}\\Bigg]s^{7/2}+\\mathbb{E}\\bigg[\\left\\|\\overleftarrow{X}_{0}-\\overleftarrow{X}_{1-s}\\right\\|^{4}\\bigg]}\\\\ &{\\lesssim\\mathbb{E}\\Bigg[\\frac{\\left\\|\\overleftarrow{\\mathcal{f}}_{1-s}^{s}\\right\\|^{8}}{s^{8}}\\Bigg]+d^{4}s^{-1/2}+\\mathbb{E}\\bigg[\\left\\|X_{1}^{\\mathrm{I}}-X_{s}^{\\mathrm{I}}\\right\\|^{4}\\bigg]}\\\\ &{\\lesssim\\|\\nabla\\log\\tilde{\\pi}\\|_{\\mathrm{L}^{8}(\\pi)}^{8}+\\|\\nabla\\log\\nu^{\\star}\\|_{\\mathrm{L}^{8}(\\nu^{\\star})}^{8}+d^{4}s^{-1/2}+d^{2}+\\mathbf{m}_{4}[\\mu]+\\mathbf{m}_{4}[\\nu^{\\star}]\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "But then, we obtain that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\int_{0}^{1/2}\\int_{\\mathbb{R}^{d}}\\left\\|A_{s}^{5}(x)\\right\\|^{2}\\varrho(s)p_{s}^{\\mathrm{I}}(x)\\mathrm{d}x\\,\\mathrm{d}s\\lesssim d^{4}+\\mathbf{m}_{4}[\\mu]+\\mathbf{m}_{4}[\\nu^{\\star}]+\\|\\nabla\\log\\tilde{\\pi}\\|_{\\mathrm{L}^{s}(\\pi)}^{8}}\\\\ {+\\left\\|\\nabla\\log\\nu^{\\star}\\right\\|_{\\mathrm{L}^{s}(\\nu^{\\star})}^{8}\\ .\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We now focus on the second time interval, that is we look at $s\\in[1/2,1-\\epsilon]$ and try to bound the integrand. Using (32) and proceeding in a similar way, we get that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\bigg[\\bigg\\|\\mathbb{E}\\bigg[\\frac{\\|X_{s}^{\\frac{1}{\\alpha}}-X_{0}^{\\frac{1}{\\alpha}}\\|^{2}}{s^{2}}\\frac{X_{1}^{\\alpha}-X_{s}^{\\frac{1}{\\alpha}}}{1-s}\\bigg|X_{s}^{\\frac{1}{\\alpha}}\\bigg]-\\mathbb{E}\\bigg[\\bigg\\|\\frac{X_{s}^{1}-X_{0}^{\\frac{1}{\\alpha}}}{s^{2}}\\bigg\\|X_{s}^{\\frac{1}{\\alpha}}\\bigg\\|\\bigg[\\frac{X_{1}^{1}-X_{s}^{\\frac{1}{\\alpha}}}{1-s}\\bigg|X_{s}^{\\frac{1}{\\alpha}}\\bigg]\\bigg\\|^{2}\\bigg]\\bigg]\\bigg|\\sigma(s)}\\\\ &{\\lesssim\\mathbb{E}\\bigg[\\bigg\\|\\mathbb{E}\\bigg[\\bigg\\|\\frac{\\hat{X}_{s}}{s}-\\vec{X}_{0}\\bigg\\|^{2}\\bigg\\|\\frac{\\hat{Y}_{s}^{\\frac{1}{\\alpha}}-s}{1-s}\\frac{\\vec{y}_{1}^{1-s}}{s}\\bigg|\\vec{X}_{s}\\bigg]}\\\\ &{\\quad-\\mathbb{E}\\bigg[\\bigg\\|\\vec{X}_{s}-\\vec{X}_{0}\\bigg\\|^{2}\\bigg\\|\\vec{X}_{s}\\bigg\\|\\bigg[\\frac{\\sqrt{\\frac{1}{\\alpha}}\\nu^{\\frac{1}{\\alpha}}+\\frac{\\vec{y}_{1}^{1-s}}{s}\\bigg|\\vec{X}_{s}^{\\frac{1}{\\alpha}}\\bigg\\|^{2}\\bigg]}\\\\ &{=\\mathbb{E}\\bigg[\\bigg\\|\\frac{\\vec{X}_{s}}{s}-\\vec{X}_{0}\\bigg\\|^{2}\\frac{\\vec{Y}_{s}^{\\frac{1}{\\alpha}}-s}{1-s}\\bigg|\\vec{X}_{s}\\bigg\\|-\\mathbb{E}\\bigg[\\bigg\\|\\vec{X}_{s}-\\vec{X}_{0}\\bigg\\|^{2}\\bigg\\|\\vec{X}_{s}\\bigg\\|\\bigg[\\frac{\\vec{Y}_{s}^{\\frac{1}{\\alpha},\\alpha}}{1-s}\\bigg|\\vec{X}_{s}^{\\frac{1}{\\alpha}}\\bigg]\\bigg\\|^{2}\\bigg]}\\\\ &{\\lesssim\\mathbb{E}\\bigg[\\bigg\\|\\vec{X}_{s}-\\vec{X}_{0}^{\\frac{1}{\\alpha}}\\bigg\\|^{s}\\bigg]+\\mathbb{E\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "But then, we have that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\int_{1/2}^{1-\\epsilon}\\int_{\\ensuremath{\\mathbb{R}}^{d}}\\left\\|A_{s}^{5}(x)\\right\\|^{2}\\mathsf{o}(s)p_{s}^{\\mathrm{I}}(x)\\mathrm{d}x\\;\\mathrm{d}s\\lesssim d^{4}+\\mathbf{m}_{8}[\\mu]+\\mathbf{m}_{8}[\\nu^{\\star}]+\\|\\nabla\\log\\tilde{\\pi}\\|_{\\mathbf{L}^{8}(\\pi)}^{8}}\\\\ {+\\;\\|\\nabla\\log\\mu\\|_{\\mathbf{L}^{8}(\\mu)}^{8}\\;.}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "To conclude, we can bound $A_{s}^{5}$ as follows ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\int_{0}^{1-\\epsilon}\\int_{\\mathbb{R}^{d}}\\left\\|A_{s}^{5}(x)\\right\\|^{2}\\,\\mathsf{0}(s)p_{s}^{\\mathrm{I}}(x)\\mathrm{d}x\\,\\mathrm{d}s\\lesssim d^{4}+\\mathbf{m}_{8}[\\mu]+\\mathbf{m}_{8}[\\nu^{\\star}]+\\|\\nabla\\log\\tilde{\\pi}\\|_{\\mathrm{L}^{8}(\\pi)}^{8}}}\\\\ &{\\hphantom{\\sum_{\\theta^{d}}\\big(\\big\\|_{0}^{8}(s)\\big\\|_{\\mathrm{R}^{d}}^{8}(x)\\big)^{2}\\,\\mathsf{\\Omega}}+\\|\\nabla\\log\\mu\\|_{\\mathrm{L}^{8}(\\mu)}^{8}+\\|\\nabla\\log\\nu^{\\star}\\|_{\\mathrm{L}^{8}(\\nu^{\\star})}^{8}~.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We are left with $A_{s}^{6}$ . Using (19), we can rewrite $A_{s}^{6}$ as follows ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{s}^{(s)}(x)}\\\\ &{=\\frac{1}{p_{s}^{3}(x)}\\int_{\\mathbb{R}^{2d}}\\Big\\{\\frac{-1}{2(1-s)}\\mathrm{Id}+\\frac{(x_{1}-x)(x_{1}-x)^{\\mathrm{T}}}{4(1-s)^{2}}\\Big\\}\\frac{x-x_{0}}{2s}p_{s}(x|x_{0})p_{1-s}(x_{1}|x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}\\\\ &{\\quad-\\left(\\frac{1}{p_{s}^{3}(x)}\\int_{\\mathbb{R}^{2d}}\\Big\\{\\frac{-1}{2(1-s)}\\mathrm{Id}+\\frac{(x_{1}-x)(x_{1}-x)^{\\mathrm{T}}}{4(1-s)^{2}}\\Big\\}p_{s}(x|x_{0})p_{1-s}(x_{1}|x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}\\right)}\\\\ &{\\quad\\cdot\\left(\\frac{1}{p_{s}^{1}(x)}\\int_{\\mathbb{R}^{2d}}\\frac{x-x_{0}}{2s}p_{s}(x|x_{0})p_{1-s}(x_{1}|x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}\\right)}\\\\ &{\\lesssim(\\frac{1}{p_{s}^{3}(x)}\\int_{\\mathbb{R}^{2d}}\\frac{(x_{1}-x)^{\\mathrm{T}}}{(1-s)^{2}}\\frac{x-x_{0}}{s}p_{s}(x|x_{0})p_{1-s}(x_{1}|x_{0})p_{1-s}(x_{1}|x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}\\\\ &{\\quad-\\left(\\frac{1}{p_{s}^{1}(x)}\\int_{\\mathbb{R}^{2d}}\\frac{(x_{1}-x)(x_{1}-x)^{\\mathrm{T}}}{(1-s)^{2}}p_{s}(x|x_{0})p_{1-s}(x_{1}|x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}\\right)}\\\\ &{\\quad\\cdot \n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "It follows that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\int_{0}^{1-\\epsilon}\\int_{\\mathbb{R}^{d}}\\left\\|A_{s}^{6}(x)\\right\\|^{2}\\varrho(s)p_{s}^{\\mathrm{I}}(x)\\mathrm{d}x\\;\\mathrm{d}s}\\\\ &{\\lesssim\\displaystyle\\int_{0}^{1-\\epsilon}\\mathbb{E}\\Bigg[\\left\\|\\mathbb{E}\\Bigg[\\frac{(X_{1}^{\\mathrm{I}}-X_{s}^{\\mathrm{I}})(X_{1}^{\\mathrm{I}}-X_{s}^{\\mathrm{I}})^{\\mathrm{T}}}{(1-s)^{2}}\\frac{X_{s}^{\\mathrm{I}}-X_{0}^{\\mathrm{I}}}{s}\\Bigg|X_{s}^{\\mathrm{I}}\\right]}\\\\ &{\\qquad-\\displaystyle\\mathbb{E}\\Bigg[\\frac{(X_{1}^{\\mathrm{I}}-X_{s}^{\\mathrm{I}})(X_{1}^{\\mathrm{I}}-X_{s}^{\\mathrm{I}})^{\\mathrm{T}}}{(1-s)^{2}}\\Bigg|X_{s}^{\\mathrm{I}}\\Bigg]\\mathbb{E}\\Bigg[\\frac{X_{s}^{\\mathrm{I}}-X_{0}^{\\mathrm{I}}}{s}\\Bigg|X_{s}^{\\mathrm{I}}\\Bigg]\\Bigg\\|^{2}\\Bigg]\\varrho(s)\\mathrm{d}s}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "At this point, we proceed as for $A_{s}^{5}$ , that is we split the time interval in two and we use Lemma 1, Lemma 2 and Lemma 3. By doing so and by using (32), we get that for $s\\in[0,1/2]$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\bigg[\\bigg\\|\\bigg\\{\\frac{\\big[X_{t}^{\\top}(X_{t})-\\bar{X}_{t}\\big](X_{t})-\\bar{X}_{t}^{\\top}X_{t}-\\frac{1}{r}\\big\\}\\bigg|X_{t}\\bigg\\}\\bigg]\\bigg|^{2}\\bigg]}\\\\ &{\\quad-\\mathbb{E}\\bigg[\\bigg\\|\\frac{X_{t}^{\\top}(X_{t})-\\bar{X}_{t}^{\\top}X_{t}}{(1-\\bar{X}_{t})-\\bar{X}_{t}^{\\top}X_{t}}\\bigg\\|_{\\mathcal{X}_{t}}^{2}\\bigg]\\bigg|^{2}\\bigg]\\bigg|\\Bigg|\\Bigg|\\Bigg|\\rho(\\rho)}\\\\ &{\\leq\\mathbb{E}\\bigg[\\bigg\\|\\bigg\\|\\bigg\\{\\bar{X}_{t}-\\bar{X}_{t-1}\\bigg\\}\\bigg\\|\\bar{X}_{t}-\\bar{X}_{t}\\bigg\\|_{\\mathcal{X}_{t}}^{\\top}\\bigg\\|_{\\mathcal{X}_{t}}\\bigg\\|_{\\mathcal{X}_{t}}\\bigg\\|_{\\mathcal{X}_{t}}\\bigg]\\bigg|^{2}}\\\\ &{\\quad-\\mathbb{E}\\bigg[\\bigg\\|\\bar{X}_{t}-\\bar{X}_{t-1}\\bigg\\|\\bar{X}_{t}-\\bar{X}_{t-1}\\bigg\\|\\bar{Y}_{t}\\bigg\\|_{\\mathcal{X}_{t}}\\bigg\\|_{\\mathcal{X}_{t}}\\bigg\\|_{\\mathcal{X}_{t}}\\bigg\\|_{\\mathcal{X}_{t}}\\bigg\\|_{\\mathcal{X}_{t}}\\bigg\\|_{\\mathcal{X}_{t}}^{2}\\bigg]\\bigg|^{2}}\\\\ &{\\quad\\times\\bigg[\\bigg\\|\\bigg\\{\\bar{X}_{t}-\\bar{X}_{t-1}\\bigg\\}\\bigg\\|\\bar{X}_{t}-\\bar{X}_{t-1}\\bigg\\|\\bar{Y}_{t}\\bigg\\|_{\\mathcal{X}_{t}}\\bigg\\|_{\\mathcal{X}_{t}}\\bigg\\|_{\\mathcal{X}_{t}}\\bigg\\|_{\\mathcal{X}_{t}}\\bigg\\|_{\\mathcal{Y}_{t}}\\bigg\\|_{\\mathcal{Y}_{t}}\\bigg\\|_{\\mathcal{X}_{t}}\\bigg\\|_{\\mathcal{Y}_{t}}\\bigg\\|_{\\mathcal{Y}_{t}}}\\\\ &{\\quad-\\mathbb{E}\\bigg[\\bigg\\|\\bar{X}_{t}-\\bar{X}_{t-1}\\bigg\\|\\bar{X}_{t}-\\bar{X}_{t-1}\\bigg\\| \n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Whereas, for $s\\in[1/2,1-\\epsilon]$ , we get ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\bigg[\\bigg|\\mathbb{E}\\bigg[\\frac{\\big(X_{t}-X_{t}\\big)\\big(X_{t}-X_{t}\\big)^{*}}{(1-\\eta)^{3}}\\bigg|\\mathcal{X}_{t}\\bigg]\\bigg|_{\\infty}^{\\infty}\\bigg]}\\\\ &{\\quad-\\mathbb{E}\\bigg[\\bigg|\\frac{\\big(X_{t}-X_{t}\\big)\\big(X_{t}-X_{t}\\big)^{*}}{(1-\\eta)^{3}}\\bigg|\\mathcal{X}_{t}\\bigg|\\bigg|_{\\infty}^{\\infty}\\bigg]}\\\\ &{\\leq\\mathbb{E}\\bigg[\\bigg|\\frac{\\big(X_{t}-X_{t}\\big)\\big(X_{t}-X_{t}\\big)}{(1-\\eta)^{3}}\\bigg|\\mathcal{X}_{t}\\bigg|\\bigg|_{\\infty}^{\\infty}\\bigg]}\\\\ &{\\qquad-\\mathbb{E}\\bigg[\\bigg|\\frac{\\big(X_{t}-X_{t}\\big)\\big(X_{t}-X_{t}\\big)}{(1-\\eta)^{3}}\\bigg|\\mathcal{X}_{t}\\bigg|\\bigg|_{\\infty}^{\\infty}\\bigg]}\\\\ &{\\qquad-\\mathbb{E}\\bigg[\\bigg|\\frac{\\big(X_{t}-X_{t}\\big)\\big(X_{t}-X_{t}\\big)^{*}}{(1-\\eta)^{3}}\\bigg|\\mathcal{X}_{t}\\bigg|\\bigg|_{\\infty}^{\\infty}\\bigg]\\bigg|[1-\\eta^{3}\\bigg]^{2}\\bigg|}\\\\ &{\\qquad-\\mathbb{E}\\bigg[\\bigg|\\frac{\\big(X_{t}-X_{t}\\big)\\big(X_{t}-X_{t}\\big)}{(1-\\eta)^{3}}\\bigg|\\mathcal{X}_{t}\\bigg|\\bigg|_{\\infty}^{\\infty}\\bigg]}\\\\ &{\\qquad-\\mathbb{E}\\bigg[\\bigg|\\frac{\\big(X_{t}-X_{t}\\big)\\big(X_{t}-X_{t}\\big)}{(1-\\eta)^{3}}\\bigg|\\mathcal{X}_{t}\\bigg|\\bigg|_{\\infty}^{\\infty}\\bigg]}\\\\ &{\\qquad-\\mathbb{E}\\bigg[\\bigg|\\frac{\\big(X_{t}-X_{t}\\big)\\big(X_{t}-X_{t}\\big)}{(1-\\eta)^{3}}+\\frac{\\eta^{3}-(\\frac{1}{2}-\\eta)^{2}}{(1-\\eta)^{3}}\\bigg|\\mathcal{X}_{t}\\bigg|\\bigg|_{\\infty}^{\\infty}\\bigg]}\\\\ &{\\leq\\mathbb{E} \n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Consequently, we have that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\int_{0}^{1-\\epsilon}\\int_{\\mathbb{R}^{d}}\\left\\|A_{s}^{6}(x)\\right\\|^{2}\\,\\mathsf{0}(s)p_{s}^{\\mathrm{I}}(x)\\mathrm{d}x\\,\\mathrm{d}s\\lesssim d^{4}+\\mathbf{m}_{8}[\\mu]+\\mathbf{m}_{8}[\\nu^{\\star}]+\\|\\nabla\\log\\tilde{\\pi}\\|_{\\mathrm{L}^{8}(\\pi)}^{8}}}\\\\ &{\\hphantom{\\sum_{\\theta^{d}}\\big(\\big\\|_{0}^{8}(x)\\big\\|_{\\mathrm{R}^{d}}^{8}(x)\\big)^{2}\\,\\mathsf{\\Omega}}+\\|\\nabla\\log\\mu\\|_{\\mathrm{L}^{8}(\\mu)}^{8}+\\|\\nabla\\log\\nu^{\\star}\\|_{\\mathrm{L}^{8}(\\nu^{\\star})}^{8}~.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Putting together the bounds on the $\\{A_{s}^{k}\\}_{k=1}^{6}$ derived so far, we eventually obtain ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\int_{0}^{1-\\epsilon}\\mathbb{E}\\Big[\\Big\\|(\\partial_{s}+\\mathcal{L}_{s}^{\\mathrm{M}})\\tilde{\\beta}_{s}(\\vec{X}_{s})\\Big\\|^{2}\\Big]\\mathsf{0}(s)\\mathrm{d}s}\\\\ &{\\displaystyle\\lesssim d^{4}+\\mathbf{m}_{8}[\\mu]+\\mathbf{m}_{8}[\\nu^{\\star}]+\\|\\nabla\\log\\tilde{\\pi}\\|_{\\mathrm{L}^{8}(\\pi)}^{8}+\\|\\nabla\\log\\mu\\|_{\\mathrm{L}^{8}(\\mu)}^{8}+\\|\\nabla\\log\\nu^{\\star}\\|_{\\mathrm{L}^{8}(\\nu^{\\star})}^{8}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Plugging (41), (35) and (36) into (34), we get ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}(\\nu_{1-\\epsilon}^{\\star}|\\nu_{1-\\epsilon}^{\\theta^{\\star}})\\lesssim\\varepsilon^{2}+h(h^{1/8}+1)\\Big(d^{4}+\\mathbf{m}_{8}[\\mu]+\\mathbf{m}_{8}[\\nu^{\\star}]+\\|\\nabla\\log\\tilde{\\pi}\\|_{\\mathrm{L}^{s}(\\pi)}^{8}}\\\\ &{\\phantom{=\\quad}\\qquad\\qquad\\qquad\\qquad+\\|\\nabla\\log\\mu\\|_{\\mathrm{L}^{s}(\\mu)}^{8}+\\|\\nabla\\log\\nu^{\\star}\\|_{\\mathrm{L}^{s}(\\nu^{\\star})}^{8}\\Big)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "The estimate (14) then follows from the above estimate and (31) However, for (14) to hold true, we still need to prove that ", "page_idx": 30}, {"type": "text", "text": "Lemma 4. $(\\int_{0}^{s}\\langle\\tilde{\\beta}_{t},D_{x}\\tilde{\\beta}_{t}\\rangle(X_{t}^{\\mathrm{M}})\\mathrm{d}B_{t})_{s\\in[0,1-\\epsilon]}$ is a martingale. ", "page_idx": 30}, {"type": "text", "text": "Proof of Lemma 4. If we show that for any $s\\in[0,1-\\epsilon]$ ,it holds $\\mathbb{E}[\\|\\langle\\tilde{\\beta}_{s},D_{x}\\tilde{\\beta}_{s}\\rangle(X_{t}^{\\mathrm{M}})\\|^{2}]<C$ for some $C>0$ independent of time, then by Fubini's theorem and [BB17, Theorem 7.3] we are done. To do so, by the Cauchy-Schwarz inequality, we just need to show that $\\mathbb{E}[\\|\\tilde{\\beta}_{s}(X_{s}^{\\mathrm{M}})\\|^{4}]$ and ", "page_idx": 30}, {"type": "text", "text": "$\\mathbb{E}[\\|D_{x}\\tilde{\\beta}_{s}(X_{s}^{\\mathrm{M}})\\|_{\\mathrm{op}}^{4}]$ are bounded from above by constants which are independent of time. To this aim, note that, as a direct consequence of (8), Theorem 1, (18), Jensen inequality and $\\mathbf{H}2$ , it holds ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\bigg[\\bigg\\|\\tilde{\\mathcal{B}}_{s}(X_{s}^{\\mathrm{M}})\\bigg\\|^{4}\\bigg]}\\\\ &{\\lesssim\\int_{\\mathbb{R}^{d}}\\bigg\\|\\frac{\\int_{\\mathbb{R}^{2d}}p_{s}(x|x_{0})\\nabla_{x}p_{1-s}(x_{1}|x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}{p_{s}^{1}(x)}\\bigg\\|^{4}p_{s}^{1}(x)\\mathrm{d}x}\\\\ &{=\\int_{\\mathbb{R}^{d}}\\bigg\\|\\frac{1}{p_{s}^{1}(x)}\\int_{\\mathbb{R}^{2d}}p_{s}(x|x_{0})p_{1-s}(x_{1}|x)\\frac{\\nabla_{x_{1}}\\tilde{\\pi}}{\\tilde{\\pi}}(x_{0},x_{1})\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0},\\mathrm{d}x_{1}\\bigg\\|^{4}p_{s}^{1}(x)\\mathrm{d}x}\\\\ &{=\\mathbb{E}\\bigg[\\bigg\\|\\mathbb{E}\\bigg[\\frac{\\nabla_{x_{1}}\\tilde{\\pi}}{\\tilde{\\pi}}(X_{0}^{1},X_{1}^{1})\\bigg\\|X_{s}^{1}\\bigg]\\bigg\\|^{4}\\bigg]\\leq\\mathbb{E}\\bigg[\\mathbb{E}\\bigg[\\bigg\\|\\frac{\\nabla_{x_{1}}\\tilde{\\pi}}{\\tilde{\\pi}}(X_{0}^{1},X_{1}^{1})\\bigg\\|^{4}\\bigg|X_{s}^{1}\\bigg]\\bigg]}\\\\ &{=\\mathbb{E}\\bigg[\\bigg\\|\\frac{\\nabla_{x_{1}}\\tilde{\\pi}}{\\tilde{\\pi}}(X_{0}^{1},X_{1}^{1})\\bigg\\|^{4}\\bigg]=\\|\\nabla\\log\\tilde{\\pi}\\|_{\\mathrm{L}^{4}(\\pi)}^{4}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Similarly, recalling (38), we have that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\Big[\\Big\\|D_{x}\\tilde{\\beta}_{s}(X_{s}^{\\mathrm{M}})\\Big\\|_{\\mathrm{op}}^{4}\\Big]}\\\\ &{\\lesssim\\int_{\\mathbb{R}^{d}}\\Bigg|\\frac{\\int_{\\mathbb{R}^{2d}}\\nabla_{x}p_{1-s}(x_{1}|x)(\\nabla_{x}p_{s}(x|x_{0}))^{\\mathrm{T}}\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}{p_{s}^{\\mathrm{A}}(x)}\\Bigg|_{\\mathrm{op}}^{4}\\Bigg\\|_{s}^{4}\\eta_{s}^{\\mathrm{I}}(x)\\mathrm{d}x}\\\\ &{+\\int_{\\mathbb{R}^{d}}\\Bigg\\|\\frac{\\int_{\\mathbb{R}^{2d}}p_{s}(x|x_{0})\\nabla_{x}^{2}p_{1-s}(x_{1}|x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}{p_{s}^{\\mathrm{A}}(x)}\\Bigg\\|_{\\mathrm{op}}^{4}\\eta_{s}^{\\mathrm{I}}(x)\\mathrm{d}x}\\\\ &{+\\int_{\\mathbb{R}^{d}}\\Bigg\\|\\frac{\\int_{\\mathbb{R}^{2d}}p_{s}(x|x_{0})\\nabla_{x}p_{1-s}(x_{1}|x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}{p_{s}^{\\mathrm{B}}(x)}\\Bigg\\|^{8}p_{s}^{\\mathrm{I}}(x)\\mathrm{d}x}\\\\ &{+\\int_{\\mathbb{R}^{d}}\\Bigg\\|\\frac{\\int_{\\mathbb{R}^{2d}}\\nabla_{x}p_{s}(x|x_{0})p_{1-s}(x_{1}|x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}{p_{s}^{\\mathrm{B}}(x)}\\Bigg\\|^{8}p_{s}^{\\mathrm{I}}(x)\\mathrm{d}x}\\\\ &{+\\int_{\\mathbb{R}^{d}}\\Bigg\\|\\frac{\\int_{\\mathbb{R}^{2d}}\\nabla_{x}p_{s}(x|x_{0})p_{1-s}(x_{1}|x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "To bound the first term of the RHS of the above expression, we integrate by parts and use Lemma 1 and $\\mathbf{H}2$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\int_{\\mathbb{R}^{d}}\\left\\|\\frac{\\int_{\\mathbb{R}^{2d}}\\nabla_{x}p_{1-s}(x_{1}|x)(\\nabla_{x}p_{s}(x|x_{0}))^{\\top}\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}{p_{s}^{*}(x)}\\right\\|_{\\varphi_{0}}^{4}}\\\\ &{\\displaystyle=\\int_{\\mathbb{R}^{d}}\\left\\|\\frac{\\int_{\\mathbb{R}^{2d}}\\nabla_{x}p_{5}(x|x_{0})(\\nabla_{x}p_{1-s}(x_{1}|x))^{\\top}\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}{p_{s}^{*}(x)}\\right\\|_{\\varphi_{0}}^{4}}\\\\ &{\\displaystyle=\\int_{\\mathbb{R}^{d}}\\left\\|\\frac{1}{p_{s}^{*}(x)}\\int_{\\mathbb{R}^{2d}}\\frac{\\nabla_{x}\\tilde{\\pi}}{\\tilde{\\pi}}(x_{0},x_{1})\\frac{(x_{1}-x)^{\\top}}{1-s}p_{s}(x|x_{0})p_{1-s}(x_{1}|x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}\\right\\|_{\\varphi_{0}}^{4}p_{s}^{1}(x)\\mathrm{d}x}\\\\ &{\\lesssim\\mathbb{E}\\left[\\left\\|\\frac{\\nabla_{x_{0}}\\tilde{\\pi}}{\\tilde{\\pi}}(X_{0}^{1},X_{1}^{1})\\frac{(X_{1}^{1}-X_{s}^{1})^{\\top}}{\\epsilon}\\right\\|_{\\varphi_{0}}^{4}\\right]}\\\\ &{\\lesssim\\frac{1}{\\epsilon}(d^{4}+\\mathbf{m}_{8}|\\mu|+\\mathbf{m}_{8}|\\nu^{*})+\\|\\nabla\\log\\tilde{\\pi}\\|_{\\mathbf{R}^{8}(\\pi)}^{8}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "To bound the second term, we integrate by parts and proceed as before. ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int_{\\mathbb{R}^{d}}\\bigg|\\displaystyle\\int_{\\mathbb{R}^{d}}\\mu_{s}(x|x_{0})\\nabla_{x}^{2}\\rho_{1-s}(x_{1}|x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}\\bigg|\\displaystyle\\int_{\\mathbb{R}^{p}}^{4}p_{s}^{\\intercal}(x)\\mathrm{d}x}\\\\ &{=\\int_{\\mathbb{R}^{d}}\\bigg|\\displaystyle\\int_{\\mathbb{R}^{d}}\\mu_{s}(x|x_{0})(\\nabla_{x_{1}}\\tilde{\\pi}(x_{0},x_{1})/\\tilde{\\pi}(x_{0},x_{1}))(\\nabla_{x}p_{1-s}(x_{1}|x))^{\\mathrm{T}}\\,\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}\\bigg|\\displaystyle\\int_{\\mathbb{R}^{p}}^{4}}\\\\ &{=\\int_{\\mathbb{R}^{d}}\\bigg|\\displaystyle\\bigg|\\frac{1}{p_{s}^{\\intercal}(x)}\\int_{\\mathbb{R}^{2d}}p_{s}(x|x_{0})p_{1-s}(x_{1}|x)\\frac{\\nabla_{x_{1}}\\tilde{\\pi}}{\\tilde{\\pi}}(x_{0},x_{1})\\frac{(x-x_{1})^{\\mathrm{T}}}{1-s}\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}\\bigg|\\displaystyle\\int_{\\mathbb{R}^{p}}^{4}}\\\\ &{\\lesssim\\frac{1}{e}(d^{4}+\\mathbf{m}_{8}|\\mu|+\\mathbf{m}_{8}|\\nu^{*}|)+\\|\\nabla\\log\\tilde{\\pi}\\|_{\\mathbb{R}^{s}(\\pi)}^{8}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "To bound the third and last term, we proceed as in (42), getting ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\int_{\\mathbb{R}^{d}}\\left\\|\\frac{\\int_{\\mathbb{R}^{2d}}p_{s}(x|x_{0})\\nabla_{x}p_{1-s}(x_{1}|x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}{p_{s}^{\\mathrm{I}}(x)}\\right\\|^{8}p_{s}^{\\mathrm{I}}(x)\\mathrm{d}x\\lesssim\\|\\nabla\\log\\tilde{\\pi}\\|_{\\mathrm{L}^{8}(\\pi)}^{8}\\ ,\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\int_{\\mathbb{R}^{d}}\\left\\|\\frac{\\int_{\\mathbb{R}^{2d}}\\nabla_{x}p_{s}(x|x_{0})p_{1-s}(x_{1}|x)\\tilde{\\pi}(x_{0},x_{1})\\mathrm{d}x_{0}\\mathrm{d}x_{1}}{p_{s}^{\\mathrm{I}}(x)}\\right\\|^{8}p_{s}^{\\mathrm{I}}(x)\\mathrm{d}x\\lesssim\\|\\nabla\\log\\tilde{\\pi}\\|_{\\mathrm{L}^{8}(\\pi)}^{8}\\ .\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "A.5Proof of Theorem 3 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Fix $\\delta>0$ . Then, because of Theorem 1, (5) and $\\mathbf{H}1$ , it holds ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbf{m}_{\\mathsf{B}}[\\nu_{1-\\delta}^{\\star}]=\\mathbb{E}[\\left\\|X_{1-\\delta}^{\\mathrm{I}}\\right\\|^{8}]\\lesssim\\delta^{8}\\mathbf{m}_{\\mathsf{B}}[\\mu]+(1-\\delta)^{8}\\mathbf{m}_{\\mathsf{B}}[\\nu^{\\star}]+d^{4}\\delta^{4}(1-\\delta)^{4}<+\\infty\\;.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Moreover $\\nu_{1-\\delta}^{\\star}\\ll\\mathrm{Leb}^{d}$ with density $p_{1-\\delta}^{\\mathrm{I}}$ and ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\nabla\\log\\Big(\\frac{\\mathrm{d}\\nu_{1-\\delta}^{\\star}}{\\mathrm{dLeb}^{d}}\\Big)\\in\\mathrm{L}^{8}(\\nu_{1-\\delta}^{\\star})\\ .\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Indeed because of (18), it holds ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla\\log\\frac{\\mathrm{d}v_{1-\\delta}^{\\star}}{\\mathrm{d}L\\log\\delta}(x_{1-\\delta})}\\\\ &{=\\frac{\\nabla\\left(\\int_{\\mathbb R^{2d}}p_{1-\\delta}(x_{1-\\delta}|x_{0})p_{\\delta}(x_{1}|x_{1-\\delta})\\tilde{\\pi}(\\mathrm{d}x_{0},\\mathrm{d}x_{1})\\right)}{p_{1-\\delta}^{\\star}(x_{1-\\delta})}}\\\\ &{=\\frac{1}{p_{1-\\delta}^{\\star}(x_{1-\\delta})}\\int_{\\mathbb R^{2d}}\\left\\{\\frac{x_{1-\\delta}-x_{0}}{1-\\delta}+\\frac{x_{1}-x_{1-\\delta}}{\\delta}\\right\\}p_{1-\\delta}(x_{1-\\delta}|x_{0})p_{\\delta}(x_{1}|x_{1-\\delta})\\tilde{\\pi}(\\mathrm{d}x_{0},\\mathrm{d}x_{1})}\\\\ &{=\\frac{1}{p_{1-\\delta}^{\\star}(x_{1-\\delta})}\\int_{\\mathbb R^{2d}}\\frac{(2\\delta-1)x_{1-\\delta}-\\delta x_{0}+(1-\\delta)x_{1}}{\\delta(1-\\delta)}p_{1-\\delta}(x_{1-\\delta}|x_{0})p_{\\delta}(x_{1}|x_{1-\\delta})\\tilde{\\pi}(\\mathrm{d}x_{0},\\mathrm{d}x_{1})}\\\\ &{\\lesssim\\mathbb E\\left[\\frac{X_{1-\\delta}-\\delta X_{0}^{1}+(1-\\delta)X_{1}^{1}}{\\delta(1-\\delta)}\\Big|X_{1-\\delta}^{1}=x_{1-\\delta}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "But then, using Jensen inequality we obtain that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\vert\\displaystyle\\int_{\\mathbb R^{d}}\\left\\|\\nabla_{x_{1-\\delta}}\\log\\frac{\\mathrm{d}\\nu_{1-\\delta}^{\\star}}{\\mathrm{d}\\mathrm{Leb}^{d}}\\right\\|^{8}\\mathrm{d}\\nu_{1-\\delta}^{\\star}=\\mathbb{E}\\left[\\left\\|\\mathbb{E}\\left[\\frac{X_{1-\\delta}^{\\mathrm{I}}-\\delta X_{0}^{\\mathrm{I}}+(1-\\delta)X_{1}^{\\mathrm{I}}}{\\delta(1-\\delta)}\\bigg|X_{1-\\delta}^{\\mathrm{I}}\\right]\\right\\|^{8}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\times\\mathbb{E}\\left[\\left\\|\\frac{X_{1-\\delta}^{\\mathrm{I}}-\\delta X_{0}^{\\mathrm{I}}+(1-\\delta)X_{1}^{\\mathrm{I}}}{\\delta(1-\\delta)}\\right\\|^{8}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\lesssim\\operatorname{ms}[\\nu_{1-\\delta}^{\\star}]\\frac{1}{\\delta^{8}(1-\\delta)^{8}}+\\mathbf{m}_{8}[\\mu]\\frac{1}{(1-\\delta)^{8}}+\\mathbf{m}_{8}[\\nu^{\\star}]\\frac{1}{\\delta^{8}}}\\\\ &{\\qquad\\qquad\\qquad\\lesssim\\operatorname{ms}[\\mu]\\frac{1}{(1-\\delta)^{8}}+\\mathbf{m}_{8}[\\nu^{\\star}]\\frac{1}{\\delta^{8}}+d^{4}\\frac{1}{\\delta^{4}(1-\\delta)^{4}}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Also, consider ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\pi_{1-\\delta}(x_{0},x_{1-\\delta})=\\mu(x_{0})\\int_{\\mathbb{R}^{d}}p_{1-\\delta|0,1}^{\\mathrm{I}}(x_{1-\\delta}|x_{0},x_{1})\\nu^{\\star}(\\mathrm{d}x_{1})\\;,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $(x_{0},x_{1},x_{1-\\delta})\\mapsto p_{1-\\delta|0,1}^{\\mathrm{I}}(x_{1-\\delta}|x_{0},x_{1})$ denotes the density of $X_{1-\\delta}^{\\mathrm{I}}$ given $(X_{0}^{\\mathrm{I}},X_{1}^{\\mathrm{I}})$ with respect to the Lebesgue measure. Then $\\pi_{1-\\delta}\\in\\Pi(\\mu,\\nu_{1-\\delta}^{\\star})$ and $\\pi_{1-\\delta}\\ll\\mathrm{Leb}^{2d}$ . Moreover ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\nabla\\log\\Big(\\frac{1}{p_{1-\\delta}}\\frac{\\mathrm{d}\\pi_{1-\\delta}}{\\mathrm{dLeb}^{2d}}\\Big)\\in\\mathrm{L}^{8}(\\pi_{1-\\delta})\\ .\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Indeed, because of (5), ", "page_idx": 33}, {"type": "equation", "text": "$$\np_{1-\\delta|0,1}^{\\mathrm{I}}(x_{1-\\delta}|x_{0},x_{1})=\\frac{1}{(4\\pi\\delta(1-\\delta))^{d/2}}\\exp\\left(-\\;\\frac{\\|x_{1-\\delta}-\\delta x_{0}-(1-\\delta)x_{1}\\|^{2}}{4\\delta(1-\\delta)}\\right).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Therefore ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\nabla_{x_{0}}p_{1-\\delta|0,1}^{\\mathrm{I}}(x_{1-\\delta}|x_{0},x_{1})=\\frac{x_{1-\\delta}-\\delta x_{0}-(1-\\delta)x_{1}}{2(1-\\delta)}p_{1-\\delta|0,1}^{\\mathrm{I}}(x_{1-\\delta}|x_{0},x_{1})\\;,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\nabla_{x_{1-\\delta}}p_{1-\\delta|0,1}^{\\mathrm{I}}(x_{1-\\delta}|x_{0},x_{1})=-\\frac{x_{1-\\delta}-\\delta x_{0}-(1-\\delta)x_{1}}{2\\delta(1-\\delta)}p_{1-\\delta|0,1}^{\\mathrm{I}}(x_{1-\\delta}|x_{0},x_{1})\\;.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Furthermore ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{p_{1-\\delta|0,1}^{\\mathrm{I}}(x_{1-\\delta}|x_{0},x_{1})\\nu^{\\star}(\\mathrm{d}x_{1})}{\\int_{\\mathbb{R}^{d}}p_{1-\\delta|0,1}^{\\mathrm{I}}(x_{1-\\delta}|x_{0},\\tilde{x}_{1})\\nu^{\\star}(\\mathrm{d}\\tilde{x}_{1})}=\\frac{p_{1-\\delta|0,1}^{\\mathrm{I}}(x_{1-\\delta}|x_{0},x_{1})\\mu(x_{0})\\nu^{\\star}(\\mathrm{d}x_{1})}{\\int_{\\mathbb{R}^{d}}p_{1-\\delta|0,1}^{\\mathrm{I}}(x_{1-\\delta}|x_{0},\\tilde{x}_{1})\\mu(x_{0})\\nu^{\\star}(\\mathrm{d}\\tilde{x}_{1})}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=p_{1|0,1-\\delta}^{\\mathrm{I}}(x_{1}|x_{0},x_{1-\\delta})\\mathrm{d}x_{1}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Consequently, we have that ", "text_level": 1, "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\nabla_{x_{0}}\\pi_{1-\\delta}}{\\pi_{1-\\delta}}(x_{0},x_{1-\\delta})}\\\\ &{=\\frac{\\nabla\\mu(x_{0})\\int_{\\mathbb{R}^{d}}p_{1-\\delta|0,1}^{\\mathrm{I}}(x_{1-\\delta}|x_{0},x_{1})\\nu^{\\star}(\\mathrm{d}x_{1})+\\mu(x_{0})\\int_{\\mathbb{R}^{d}}\\nabla_{x_{0}}p_{1-\\delta|0,1}^{\\mathrm{I}}(x_{1-\\delta}|x_{0},x_{1})\\nu^{\\star}(\\mathrm{d}x_{1})}{\\mu(x_{0})\\int_{\\mathbb{R}^{d}}p_{1-\\delta|0,1}^{\\mathrm{I}}(x_{1-\\delta}|x_{0},\\tilde{x}_{1})\\nu^{\\star}(\\mathrm{d}\\tilde{x}_{1})}}\\\\ &{=\\frac{\\nabla\\mu(x_{0})}{\\mu(x_{0})}+\\int_{\\mathbb{R}^{d}}\\frac{x_{1-\\delta}-\\delta x_{0}-\\left(1-\\delta\\right)x_{1}}{2(1-\\delta)}p_{1/0,1-\\delta}^{\\mathrm{I}}(x_{1}|x_{0},x_{1-\\delta})\\mathrm{d}x_{1}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\nabla_{x_{1-\\delta}}\\pi_{1-\\delta}}{\\pi_{1-\\delta}}(x_{0},x_{1-\\delta})=\\frac{\\mu(x_{0})\\int_{\\mathbb{R}^{d}}\\nabla_{x_{1-\\delta}}p_{1-\\delta|0,1}^{\\mathrm{I}}(x_{1-\\delta}|x_{0},x_{1})\\nu^{\\star}(\\mathrm{d}x_{1})}{\\mu(x_{0})\\int_{\\mathbb{R}^{d}}p_{1-\\delta|0,1}^{\\mathrm{I}}(x_{1-\\delta}|x_{0},\\tilde{x}_{1})\\nu^{\\star}(\\mathrm{d}\\tilde{x}_{1})}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=-\\int_{\\mathbb{R}^{d}}\\frac{x_{1-\\delta}-\\delta x_{0}-(1-\\delta)x_{1}}{2\\delta(1-\\delta)}p_{1|0,1-\\delta}^{\\mathrm{I}}(x_{1}|x_{0},x_{1-\\delta})\\mathrm{d}x_{1}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "But then, if we use Jensen inequality and (43), we get ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\int_{\\mathbb{R}^{2d}}\\left\\|\\nabla_{x}\\log\\frac{\\mathrm{d}\\pi_{1-\\delta}}{\\mathrm{d}\\mathrm{Lep}^{2d}}\\right\\|^{s}\\mathrm{d}\\pi_{1-\\delta}\\leq\\displaystyle\\int_{\\mathbb{R}^{d}}\\left\\|\\nabla_{x}\\log\\frac{\\mathrm{d}\\mu}{\\mathrm{d}\\mathrm{Lep}^{4d}}\\right\\|^{s}\\mathrm{d}\\mu}&{}\\\\ &{\\qquad\\qquad+\\mathbb{E}\\left[\\left\\|\\mathbb{E}\\left[\\frac{X_{1-\\delta}-\\delta X_{0}^{\\epsilon}-(1-\\delta)X_{1}^{\\epsilon}}{1-\\delta}\\right|(X_{0}^{1},X_{1-\\delta}^{1})\\right]\\right\\|^{s}\\right]}\\\\ &{\\leq\\|\\nabla\\log\\mu\\|_{\\mathrm{L}^{s}(\\mu)}+\\mathbb{E}\\left[\\left\\|\\frac{X_{1-\\delta}-\\delta X_{0}^{\\epsilon}-(1-\\delta)X_{1}^{\\epsilon}}{1-\\delta}\\|^{s}\\right\\|^{s}\\right]}\\\\ &{\\lesssim\\|\\nabla\\log\\mu\\|_{\\mathrm{L}^{s}(\\mu)}+\\mathbf{m}_{5}|\\nu_{-\\delta}^{\\star}\\big)\\frac{1}{(1-\\delta)^{s}}+\\mathbf{m}_{8}|\\mu|_{(1-\\delta)^{s}}^{\\frac{\\delta}{s}}+\\mathbf{m}_{8}|\\nu^{\\star}\\rangle}\\\\ &{\\lesssim\\|\\nabla\\log\\mu\\|_{\\mathrm{L}^{s}(\\mu)}^{s}+\\mathbf{m}_{8}|\\mu|_{(1-\\delta)}^{\\frac{\\delta}{s}}+\\mathbf{m}_{8}|\\nu^{\\star}|+d^{4}\\frac{\\delta^{4}}{(1-\\delta)^{s}}}\\\\ &{\\lesssim\\|\\nabla\\log\\mu\\|_{\\mathrm{L}^{s}(\\mu)}^{s}+\\mathbf{m}_{8}\\|\\mu\\|_{(1-\\delta)^{s}}^{\\frac{1}{s}}+\\mathbf{m}_{8}|\\nu^{\\star}|\\frac{1}{\\delta}+d^{4}\\frac{1}{(1-\\delta)^{s}}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and (similarly) ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int_{\\mathbb{R}^{2d}}\\left\\|\\nabla_{x_{1-\\delta}}\\log\\frac{\\mathrm{d}\\pi_{1-\\delta}}{\\mathrm{d}\\mathrm{Le}\\mathrm{b}^{2d}}\\right\\|^{8}\\mathrm{d}\\pi_{1-\\delta}\\lesssim\\mathbb{E}\\Bigg[\\left\\|\\mathbb{E}\\Bigg[\\frac{X_{1-\\delta}^{\\mathrm{I}}-\\delta X_{0}^{\\mathrm{I}}-(1-\\delta)X_{1}^{\\mathrm{I}}}{\\delta(1-\\delta)}\\bigg|(X_{0}^{\\mathrm{I}},X_{1-\\delta}^{\\mathrm{I}})\\right]\\Bigg\\|^{8}\\Bigg]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\lesssim\\mathbf{m}_{8}[\\mu]\\frac{1}{(1-\\delta)^{8}}+\\mathbf{m}_{8}[\\nu^{\\star}]\\frac{1}{\\delta^{8}}+d^{4}\\frac{1}{\\delta^{4}(1-\\delta)^{4}}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Additionally, because of Remark 8 and (43), they hold ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int_{\\mathbb{R}^{2d}}\\left\\|\\nabla_{x_{0}}\\log p_{1-\\delta}(x_{1-\\delta}|x_{0})\\right\\|^{8}\\mathrm{d}\\pi_{1-\\delta}(x_{0},x_{1-\\delta})}\\\\ &{=\\int_{\\mathbb{R}^{2d}}\\left\\|\\nabla_{x_{1-\\delta}}\\log p_{1-\\delta}(x_{1-\\delta}|x_{0})\\right\\|^{8}\\mathrm{d}\\pi_{1-\\delta}(x_{0},x_{1-\\delta})}\\\\ &{=\\mathbb{E}\\bigg[\\bigg\\|\\frac{X_{1-\\delta}^{\\top}-X_{0}^{\\top}}{1-\\delta}\\bigg\\|^{8}\\bigg]\\lesssim\\mathbf{m}_{8}[\\nu_{1-\\delta}^{\\star}]\\frac{1}{(1-\\delta)^{8}}+\\mathbf{m}_{8}[\\mu]\\frac{1}{(1-\\delta)^{8}}}\\\\ &{\\lesssim\\mathbf{m}_{8}[\\mu]\\frac{\\delta^{8}}{(1-\\delta)^{8}}+\\mathbf{m}_{8}[\\nu^{\\star}]+d^{4}\\frac{\\delta^{4}}{(1-\\delta)^{4}}+\\mathbf{m}_{8}[\\mu]\\frac{1}{(1-\\delta)^{8}}}\\\\ &{\\lesssim\\mathbf{m}_{8}[\\mu]\\frac{1}{(1-\\delta)^{8}}+\\mathbf{m}_{8}[\\nu^{\\star}]\\frac{1}{\\delta^{8}}+d^{4}\\frac{1}{\\delta^{4(1-\\delta)^{4}}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "It follows from $\\mathbf{H}1$ \uff0c $\\mathbf{H}2(\\mathbf{i})$ (43), (44) and (45) that the probability distributions $\\mu,\\nu_{1-\\delta}^{\\star}$ and the coupling $\\pi_{1-\\delta}\\in\\Pi(\\mu,\\nu_{1-\\delta}^{\\star})$ satisfy $\\mathbf{H}1$ . The bound in Theorem 3 is now a straightforward consequence of Theorem 2 and the bounds on the scores derived so far. ", "page_idx": 34}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The claims made in the abstract and introduction make clear the goals attained by the paper and match the results therein provided. Additionally, a discussion on the contributions made by the paper and the comparison with the literature is held. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 35}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: The paper discusses the limitations of the work performed in Section 4 Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should refect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 35}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper provides the full set of assumptions for either Theorem 2 and Theorem 3, see Section 3.1. Moreover, it includes the detailed proofs of all the stated theorems in Appendix A and a sketch of the proof of our main contributions Theorem 2, Theorem 3 in Section 3.3. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 36}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: Since this work is of theoretical nature, we do not provide experimental data.   \nhence our answer. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 36}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: Since this work is of theoretical nature, we do not provide experimental data, hence our answer. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so ^No\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 37}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: Since this work is of theoretical nature, we do not provide experimental data, hence our answer. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 37}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: Since this work is of theoretical nature, we do not provide experimental data, hence our answer. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 38}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: Since this work is of theoretical nature, we do not provide experimental data, hence our answer. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 38}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We confirm that the research conducted our paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 38}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: Since this work is of theoretical nature, it does not present societal impacts up to our knowledge. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 39}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: Since this work is of theoretical nature, it does not present such risks. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 39}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: In the present work, we do not use any assets to be credited. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: In the present work, we do not introduce new assets. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 40}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: The present work does not include any experiment involving human subjects hence our answer. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 40}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)wereobtained? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: The present work does not include any experiment involving human subjects, hence our answer. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 40}, {"type": "text", "text": "\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 41}]