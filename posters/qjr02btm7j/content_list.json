[{"type": "text", "text": "Understanding the Expressivity and Trainability of Fourier Neural Operator: A Mean-Field Perspective ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Takeshi Koshizuka Masahiro Fujisawa Department of Computer Science RIKEN AIP The University of Tokyo masahiro.fujisawa@riken.jp koshizuka-takeshi938444@g.ecc.u-tokyo.ac.jp ", "page_idx": 0}, {"type": "text", "text": "Yusuke TanakaNTT Communication Science Laboratoriesysk.tanaka@ntt.com", "page_idx": 0}, {"type": "text", "text": "Issei Sato   \nDepartment of Computer Science The University of Tokyo   \nsato@g.ecc.u-tokyo.ac.jp ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this paper, we explores the expressivity and trainability of the Fourier Neural Operator (FNO). We establish a mean-field theory for the FNO, analyzing the behavior of the random FNO from an edge of chaos perspective. Our investigation into the expressivity of a random FNO involves examining the ordered-chaos phase transition of the network based on the weight distribution. This phase transition demonstrates characteristics unique to the FNO, induced by mode truncation, while also showcasing similarities to those of densely connected networks. Furthermore, we identify a connection between expressivity and trainability: the ordered and chaotic phases correspond to regions of vanishing and exploding gradients, respectively. This finding provides a practical prerequisite for the stable training of the FNO. Our experimental results corroborate our theoretical findings. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The recent surge in interest in solving partial differential equations (PDEs) has led to the use of neural network (NN)-based surrogate models. One promising line of work is the neural operator (NO), which learns the solution operator of PDEs, thereby bypassing the need for mesh dependency. Among the variants of NO, the Fourier neural operator (FNO) (Li et al., 2020c) has gained popularity because of its advantageous cost/accuracy trade-off. The FNO can capture long-distance spatial interactions using the Fourier transform, whereas convolutional neural networks (CNNs) (Wen et al., 2019; Jiang et al., 2021b) and message-passing graph neural networks (GNNs) (Li et al., 2020a,b) are limited to operating solely on local variables. From a computational cost perspective, the Fourier transform is performed in quasi-linear time by the fast Fourier transform (FFT), making it significantly faster than the Transformer (Li et al., 2023). ", "page_idx": 0}, {"type": "text", "text": "Despite the widespread use of FNO as an architecture, there is a lack of comprehensive theoretical analysis on its expressivity and trainability. The universal approximation property (Kovachki et al., 2021), recognized as the basic expressivity of the FNO, is well-known; however, the exponential expressivity depending on the weight distribution, which are known for the densely connected network (DCN) (Schoenholz et al., 2016), a.k.a. fully connected network, and CNN (Xiao et al., 2018), remains unexplored. Regarding the trainability of FNO, the training instability in deep FNO has been experimentally reported by Tran et al. (2022), but the causes and conditions of the difficulty have not been clarified either theoretically or experimentally. ", "page_idx": 0}, {"type": "image", "img_path": "QJr02BTM7J/tmp/b18f0fef6f0caf53ac93499b51a82c7bd1e98e391281455f63e8c95f5ceb9e7d.jpg", "img_caption": ["Figure 1: Mllustration of ordered-chaos phase transition for the weight initialization parameter $\\sigma^{2}$ .In the ordered phase, the spatial hidden representations $\\mathbf{H}^{(\\ell)}$ on the grid converge to a uniform state during forward propagation and the gradient vanishes during backpropagation. In the chaotic phase, the representations either converge to a distinct state or diverge and the gradient explodes. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "We analyze the exponential expressivity (how far can two different input vectors be pulled apart) and trainability (how much gradient explosion on average) of the random FNO from the perspective of whether the network is ordered or chaotic. This viewpoint is grounded in mean-field theory, an analytical framework for NN established by Poole et al. (2016); Schoenholz et al. (2016); Yang & Schoenholz (2017); Xiao et al. (2018). A network is considered ordered when it brings all representations of two different spatial positions closer together, and chaotic when it drives them apart during forward propagation. Furthermore, a network can only be stably trained when initialized close to the edge of chaos, which is the transition point between the ordered phase and the chaotic phase. In fact, He initialization (He et al., 2015) is an example of a commonly used edge of chaos initialization for the DCN with ReLU activation (Burkholz & Dubatovka, 2019). ", "page_idx": 1}, {"type": "text", "text": "In this study, we establish a mean-field theory to analyze the expressivity and trainability of the FNO. Our investigation reveals the expressivity of random FNO at initialization by examining the transition point between ordered and chaos phases. The phase transition exhibits FNO-specific characteristics induced by mode truncation, as well as similarities with the characteristics of DCN and CNN. We also find a link between expressivity and trainability: the ordered and chaotic phases correspond to regions of vanishing and exploding gradient, respectively. This discovery offers a practical initialization prerequisite for the stable training of the FNO. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1  Fourier Neural Operators ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The FNO (Li et al., 2020c) is one of the well-established methods for solving PDEs across many scientific problems (Yang et al., 2021; Wen et al., 2022b; Hwang et al., 2022; Jiang et al., 2021a; Pathak et al., 2022). An $M$ -dimensional FNO with the number of hidden features $D$ and the spatial size $N$ for learning the operators between scalar-valued functions is defined as follows. ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathbf{X}^{\\left(\\ell+1\\right)}=\\phi\\left(\\mathcal{D}^{\\left(\\ell\\right)}\\left(\\mathbf{X}^{\\left(\\ell\\right)}\\right)+\\mathcal{K}^{\\left(\\ell\\right)}\\left(\\mathbf{X}^{\\left(\\ell\\right)}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\mathbf{X}^{(\\ell)}\\in\\mathbb{R}^{\\widetilde{N}\\times\\cdots\\times N}\\times D$ is the $\\ell$ th hidden representation, $M$ is the number of spatial dimensions, and $\\phi$ is activation. The hidden representations $\\mathbf{\\bar{X}}^{(0)}$ and $\\mathbf{X}^{(L)}$ are the output of the lifting operator and the input of the projection operator, respectively. The architecture of these operators does not affect our analysis as long as the network stays shallow, as implemented in (Li et al., 2020c). The $\\ell$ -th densely connected (DC) module $\\mathcal{D}^{(\\ell)}$ is an affine point-wise map in the physical space and the $\\ell$ -th ", "page_idx": 1}, {"type": "text", "text": "Fourier convolution module $K^{(\\ell)}$ is a parameterized kernel integral operator using Fourier transform. The bias term is considered to be included in either or both modules $\\bar{\\kappa}^{(\\ell)}$ and $\\bar{\\mathcal{D}^{(\\ell)}}$ ", "page_idx": 2}, {"type": "text", "text": "Several FNO variants have been developed to address specific challenges, such as geo-FNO (Li et al., 2022) for irregular regions and group equivariant FNO (G-FNO) (Helwig et al., 2023), which maintains equivariance to rotation, reflection, and translation. U-NO (Rahman et al., 2022) and U-FNO (Wen et al., 2022a) integrate FNO with U-Net for multiscale modeling. Additionally, WNO (Tripura & Chakraborty, 2022) utilizes wavelet bases, while CFNO (Brandstetter et al., 2023) enhances the use of geometric relations between different fields and field components through Clifford algebras. Adaptive FNO (Zhao et al., 2022; Guibas et al., 2021) and F-FNO (Tran et al., 2022) have improved computational and memory efficiency through incremental learning and architectural modifications. Other approaches for improving performance include methods with increasing physical inductive bias (Li et al., 2024), data augmentation (Brandstetter et al., 2022), and a variance-preserving weight initialization scheme (Poli et al., 2022). ", "page_idx": 2}, {"type": "text", "text": "While numerous new models and learning methods have been proposed, relatively little research has been conducted to understand the intrinsic nature of these methods. Issues such as spectral bias (Zhao et al., 2022) and training instability (Tran et al., 2022) have been reported. Tran et al. (2022) observed that training did not converge even at 24 layers. They successfully addressed the stability and accuracy degradation issues associated with an increase in the number of layers by implementing skip connections behind activation and introducing various training techniques. However, it is still unknown that the theoretical basis for why the original architecture of the FNO has problems with training instability and accuracy degradation. ", "page_idx": 2}, {"type": "text", "text": "2.2 Mean-field Theory for Neural Networks ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The mean-field theory has been used to provide a mathematical framework for understanding the expressivity and trainability of neural networks (Poole et al., 2016; Schoenholz et al., 2016; Yang & Schoenholz, 2017; Hayou et al., 2018; Xiao et al., 2018). A series of papers (Poole et al., 2016; Schoenholz et al., 2016) delved into the average behavior of infinite-width random deep DCN, with weights and biases initialized by a zero-mean Gaussian distribution. The formulation is given below. ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\pmb x}^{(\\ell)}=\\phi\\left(h^{(\\ell)}\\right),\\;{\\pmb h}^{(\\ell)}={\\pmb W}^{(\\ell)}{\\pmb x}^{(\\ell-1)}+{\\pmb b}^{(\\ell)}},}\\\\ {{\\displaystyle W_{i,j}^{(\\ell)}\\;\\tilde{\\mathrm{\\Delta}}\\sim^{i.i.d.}\\mathcal{N}\\left(0,\\frac{\\sigma^{2}}{D}\\right),\\;b_{i}^{(\\ell)}\\;\\tilde{\\mathrm{\\Delta}}\\sim^{i.i.d.}\\mathcal{N}\\left(0,\\sigma_{b}^{2}\\right),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\pmb{x}^{(\\ell)}\\in\\mathbb{R}^{D}$ is the $\\ell_{}$ th hidden representation, $\\mathbf{\\deltaW}^{(\\ell)}\\in\\mathbb{R}^{D\\times D},\\,\\mathbf{\\delta\\phi}^{(\\ell)}\\in\\mathbb{R}^{D}$ are the $\\ell$ -th learnable parameters, and the width is assumed to be sufficiently large $D\\gg1$ ", "page_idx": 2}, {"type": "text", "text": "Poole et al. (2016) and Schoenholz et al. (2016) explored the exponential expressivity of random DCN determined by two phases depending on the initial variance parameters $\\bar{\\sigma}^{2}$ and $\\bar{\\sigma}_{b}^{2}$ , as shown in Fig. 5a. Poole et al. (2016) first examined the forward propagation of a random DCN with Tanh activation. They demonstrated that the covariance $\\Sigma^{(\\ell)}$ of the $\\ell_{}$ -th pre-activation representations $\\pmb{h}^{(\\ell)}$ and $\\tilde{h}^{(\\ell)}$ corresponding to two different inputs $\\pmb{x}^{(0)}$ and $\\tilde{\\pmb{x}}^{(0)}$ are obtained by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\forall d\\in[D],\\ \\mathbf{\\Sigma}\\mathbf{\\Sigma}^{(\\ell)}=\\sigma^{2}\\mathbb{E}\\left[\\phi\\left(h_{d}^{(\\ell-1)}\\right)\\phi\\left(\\tilde{h}_{d}^{(\\ell-1)}\\right)\\right]+\\sigma_{b}^{2},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the expectation is taken over the pre-ativtions $\\big[h_{d}^{(\\ell-1)},\\widetilde{h}_{d}^{(\\ell-1)}\\big]\\;\\sim\\;{\\mathcal N}(\\mathbf{0},\\boldsymbol{\\Sigma}^{(\\ell-1)})$ .The covariance converges exponentially to a fixed point $\\pmb{\\Sigma}^{*}$ determined by parameters $\\sigma^{2}$ and $\\sigma_{b}^{2}$ ", "page_idx": 2}, {"type": "text", "text": "A network is considered ordered when it brings two distinct representations closer together, which implies a state of small expressivity. Conversely, a network is chaotic when it drives them apart during forward propagation, implying a state of large expressivity. Networks with either excessively small or large expressivity can disrupt the structure of the input: the difference between two distinct inputs quickly becomes indistinguishable in networks with small expressivity, while similarities between inputs are no longer recognized in networks with large expressivity. The network is ordered if the initial variance of the weights is small. For larger values, and beyond a certain threshold, the phase shifts, and the network behaves chaotically. This phase shift point is termed the edge of chaos. ", "page_idx": 2}, {"type": "text", "text": "Subsequently, Schoenholz et al. (2016) discovered the connection between expressivity and trainability of DCN through analysis of the backpropagation behavior. In an ordered network, the expected value of the gradient norm becomes exponentially small during backpropagation, while it becomes exponentially large in a chaotic network. This implies that the gradient vanishes/explodes in ordered or chaotic networks, respectively. These findings suggest that deep DCN can be stably trained only near the edge of chaos. Schoenholz et al. (2016) also provided an estimate of the maximum depth at which a network can be trained when initialized away from the edge of chaos. These insights are not limited to DCN and similar results have been observed for residual networks (Yang & Schoenholz, 2017) and CNN (Xia0 et al., 2018). ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3  A mean-field theory for FNO ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we establish a mean-field theory of FNO. We demonstrate the exponential expressivity of random FNO by examining the ordered-chaos phase transition during the forward propagation. Furthermore, we identify the connection between expressivity and trainability by concentrating on backward propagation behaviors. Our analysis is an advanced version of the approach developed by Poole et al. (2016); Schoenholz et al. (2016); Yang & Schoenholz (2017); Xiao et al. (2018). In Section 3.1, we outline the problem setup. In Section 3.2, we analyze the forward and backward propagation behavior of random FNO at initialization. In Section 3.3, we discuss the practical prerequisites for initialization to stabilize the training of FNO, leveraging the similarities between FNO and DCN. The proofs for all the lemmas and theorems are provided in Appendices A and B. ", "page_idx": 3}, {"type": "text", "text": "3.1  Problem setting ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Here, we consider a simplified one-dimensional (1D) FNO. Note that our theory is extensively applicable to the original FNO, as discussed in Section 3.3. The simplified 1D FNO, with a depth of $L$ , is defined by the number of hidden features $D$ , a spatial size $N\\;=\\;2^{m}$ (where $m$ is an integer), the number of Fourier modes $\\begin{array}{r}{K\\,\\leq\\,\\frac{N}{2}+1}\\end{array}$ , two learnable weights $\\Theta^{(\\ell,k)}\\,\\in\\,\\mathbb{R}^{D\\times D}$ and $\\Xi^{(\\ell,k)}\\in\\mathbb{R}^{D\\times D}$ and a bias $\\mathbf{b}^{(\\ell)}\\in\\mathbb{R}^{D}$ .Denote $\\phi\\colon\\ensuremath{\\mathbb{R}}\\to\\ensuremath{\\mathbb{R}}$ by the non-decreasing activation function. Let $\\mathbf{X}^{(\\ell)}\\in\\mathbb{R}^{N\\times D}$ and ${\\bf H}^{(\\ell)}\\in\\mathbb{R}^{N\\times D}$ be the post and pre-activation representations defined by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\bf X}^{(\\ell)}=\\phi\\left({\\bf H}^{(\\ell)}\\right),\\;{\\bf H}^{(\\ell)}=\\displaystyle\\sum_{k=0}^{K-1}\\sqrt{\\frac{c_{k}}{2}}\\left({\\bf H}^{(\\ell,k)}+\\overline{{{\\bf H}}}^{(\\ell,k)}\\right)+{\\bf b}^{(\\ell)}{\\bf1}_{N}^{\\top},}\\\\ &{\\quad\\quad{\\bf H}^{(\\ell,k)}:={\\cal F}^{\\dagger}{\\cal D}^{(k)}{\\cal F}{\\bf X}^{(\\ell-1)}\\left(\\Theta^{(\\ell,k)}+\\sqrt{-1}\\Xi^{(\\ell,k)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\delta_{a,b}$ is the Kronecker-delta, $c_{k}\\,=\\,2\\,-\\,\\delta_{k,0}\\,-\\,\\delta_{k,N/2}$ is a constant, ${\\mathbf{1}}_{N}$ is all-ones column vector with thesise $N,\\overline{{\\mathbf{H}}}^{(\\ell,k)}$ istheconjugat of $\\mathbf{H}^{(\\ell,k)}$ correspondin tothe $(N-k)$ components, $\\dagger$ is the transpose conjugate, $\\pmb{F}\\,\\in\\,\\mathbb{C}^{N\\times N}$ is the Discrete Fourier Transform (DFT) matrix denedbyF,n= exp(-n),and D() isadiagonalmatrix wiha at position D ", "page_idx": 3}, {"type": "text", "text": "There are two differences from the original FNO proposed by Li et al. (2020a): (1) the DC module is dropped for the simplicity, and (2) $\\mathbf{H}^{(\\ell,k)}$ is multiplied by $\\sqrt{2}$ with respect to $\\begin{array}{r}{k\\,=\\,0,\\frac{N}{2}}\\end{array}$ for appropriate normalization. We assume that the weights of FNO are initialized by i.i.d. samples from $\\begin{array}{r}{\\Theta_{i,j}^{(\\ell,k)}\\stackrel{i.i.d.}{\\sim}\\mathcal{N}(0,\\frac{\\sigma^{2}}{2D})}\\end{array}$ $\\begin{array}{r}{\\Xi_{i,j}^{(\\ell,k)}\\stackrel{i.i.d.}{\\sim}\\mathcal{N}(0,\\frac{\\sigma^{2}}{2D})}\\end{array}$ \uff0c $b_{i}^{(\\ell)}\\overset{i.i.d.}{\\sim}\\mathcal{N}(0,\\sigma_{b}^{2})$ For $\\begin{array}{r}{k=0,\\frac{N}{2}}\\end{array}$ , the parameter $\\Xi^{(\\ell,k)}$ is set to zero exceptionally. For all $d\\in[D]=\\{0,\\,\\,.\\,.\\,,\\,D-1\\}$ \uff0c the pre-activations H() $\\mathbf{H}_{:,d}^{(\\ell)}\\,\\in\\,\\mathbb{R}^{N}$ are i.i.d. random variables. When $D\\gg1$ , by the central limit theorem, the variables $\\mathbf{H}_{:,d}^{(\\ell)}$ follow Gaussian distribtion with mean O and covariance matrix $\\Sigma_{\\alpha,\\alpha^{\\prime}}^{(\\ell)}:=$ $\\mathbb{E}_{\\Theta^{1:\\ell},\\Xi^{1:\\ell}}\\left[H_{\\alpha,d}^{(\\ell)}H_{\\alpha^{\\prime},d}^{(\\ell)}\\right]$ , where the expectation is taken over ll random variables $\\begin{array}{r l}{\\lefteqn{[\\Theta^{1:\\ell},\\Xi^{1:\\ell}]:=}}\\end{array}$ $\\{\\Theta^{(\\ell^{\\prime},k^{\\prime})},\\Xi^{(\\ell^{\\prime},k^{\\prime})}\\}_{\\ell^{\\prime}\\in[\\ell],k^{\\prime}\\in[K]}$ .Ourteory canbe easilyexteded to2Dand3DFOs. ", "page_idx": 3}, {"type": "text", "text": "3.2  Expressivity and trainability of FNO ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Firstly, the forward propagation of a single input signal with spatial features is described as follows. ", "page_idx": 3}, {"type": "text", "text": "Lemma 3.1 (Iterated map). For all $d\\in[D]$ the covariance $\\begin{array}{r}{\\Sigma^{(\\ell)}:=\\mathbb{E}_{\\Theta^{1:\\ell},\\Xi^{1:\\ell}}\\left[\\mathbf{H}_{:,d}^{(\\ell)}\\mathbf{\\Lambda}\\mathbf{H}_{:,d}^{(\\ell)}\\right]}\\end{array}$ obtained recursively by the iterated map $\\mathcal{C}$ definedby ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Sigma_{\\alpha,\\alpha^{\\prime}}^{(\\ell)}=\\underbrace{\\sigma^{2}\\displaystyle\\sum_{k=0}^{K-1}c_{k}\\mathbb{E}\\left[\\left|\\left[F\\phi\\left(\\mathbf{H}_{:,d}\\right)\\right]_{k}\\right|^{2}\\right]\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k)}\\right)+\\sigma_{b}^{2}}_{=:\\;\\mathcal{C}\\left(\\Sigma^{(\\ell-1)}\\right)_{\\alpha,\\alpha^{\\prime}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where thexpectationistaken overthe pre-activations $\\mathbf{H}_{:,d}\\sim\\mathcal{N}(0,\\pmb{\\Sigma}^{(\\ell-1)})$ $\\begin{array}{r}{\\theta_{\\alpha,\\alpha^{\\prime}}^{(k)}:=\\frac{2\\pi k}{N}(\\alpha-\\alpha^{\\prime})}\\end{array}$ representsthescaledpositionaldifference. ", "page_idx": 4}, {"type": "text", "text": "The indices $\\alpha$ and $\\alpha^{\\prime}$ correspond to different spatial locations as with the mean-field theory for CNN (Xia0 et al., 2018). Note that $\\left[F\\phi\\left(\\mathbf{H};_{,d}\\right)\\right]_{k}$ is the $k$ -th Fourier modes of the post-activation representation. When applying DCN (Poole et al., 2016; Schoenholz et al., 2016) or CNN (Xiao et al., 2018) to the spatial signal, the iterated map depends only on local spatial locations, while in the case of FNO, the iterated map depends on all spatial locations because of the global Fourier convolution. In addition, only periodic spatial correlations with shift-invariant are propagated, and high-frequency components exceeding mode $K$ are eliminated. ", "page_idx": 4}, {"type": "text", "text": "Next, we explore the fixed point $\\Sigma^{*}$ of the iterated map $\\mathcal{C}$ satisfying $\\Sigma^{*}=\\mathcal{C}(\\Sigma^{*})$ .By linearizing the dynamics of signal propagation around this fixed point and analyzing the stability and rate of convergence to the fixed point, we can determine the depth to which each component of the input can propagate. Schoenholz et al. (2016) showed that the iterated map of DCN defined in Eq. (2) has a fixed point of the form: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{\\Sigma^{*}}=q^{*}\\pmb{I}_{N}+q^{*}c^{*}(\\mathbf{1}_{N}\\mathbf{1}_{N}^{\\top}-\\pmb{I}_{N}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $q^{*},c^{*}$ are the fixed points of variance and correlation, and ${\\cal I}_{N}$ is the identity matrix. Meanwhile, Xiao et al. (2018) showed that any fixed point for the iterated map of the DCN is also a fixed point for that of CNN. We show that random FNO has the same fixed points of the form of Eq. (5) with $c^{*}=1$ in the following lemma. ", "page_idx": 4}, {"type": "text", "text": "Lemma 3.2 (Exsistance of fixed points). When a random DCN defined by Eq. (2) has the fixed point $(q^{*},c^{*}=1)$ for the initial parameters $(\\sigma^{2},\\sigma_{b}^{2})$ , then a random simplified FNO defined by $E q.$ (3) has afixedpoint $\\Sigma^{*}$ oftheform ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Sigma^{*}=q^{*}I_{N}+q^{*}c^{*}(\\mathbf{1}_{N}\\mathbf{1}_{N}^{\\top}-I_{N})=q^{*}\\mathbf{1}_{N}\\mathbf{1}_{N}^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Lemma 3.2 indicates that the fixed point for the iterated map $\\Sigma^{*}$ of the DCN serves as a fixed point for the iterated map of the simplified FNO (as well as CNN). To analyze the stability and convergence rate, we linearly approximate the C-map around the fixed point $\\Sigma^{*}$ ,i.e., $\\mathcal{C}(\\Sigma)\\approx\\Sigma^{*}+J_{\\Sigma^{*}}(\\Sigma-\\Sigma^{*})$ where $J_{\\pmb{\\Sigma}^{*}}$ is the Jacobian linear map of the iterated map defined in Eq. (15). We then derive the eigenvalues and eigenvectors for the Jacobian linear map $J_{\\pmb{\\Sigma}^{*}}$ as follows. ", "page_idx": 4}, {"type": "text", "text": "Definition 3.3. ", "text_level": 1, "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle{\\chi_{q^{*}}:=\\sigma^{2}\\mathbb{E}\\left[\\phi^{\\prime2}(H_{\\alpha,d})+\\phi^{\\prime\\prime}(H_{\\alpha,d})\\phi(H_{\\alpha,d})\\right],}}}\\\\ {{\\displaystyle{\\chi_{c^{*}}:=\\sigma^{2}\\mathbb{E}[\\phi^{\\prime}(H_{\\alpha,d})\\phi^{\\prime}(H_{\\alpha^{\\prime},d})],}}}\\\\ {{\\displaystyle{\\chi_{\\kappa}:=\\frac{\\sigma^{2}}{2}\\mathbb{E}\\left[\\phi^{\\prime\\prime}(H_{\\alpha,d})\\phi(H_{\\alpha^{\\prime},d})+\\phi(H_{\\alpha,d})\\phi^{\\prime\\prime}(H_{\\alpha^{\\prime},d})\\right]+\\sigma^{2}\\mathbb{E}\\left[c^{*}\\phi^{\\prime}(H_{\\alpha,d})\\phi^{\\prime}(H_{\\alpha^{\\prime},d})\\right],}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the expectation is taken over the pre-activations $\\mathbf{H}_{:,d}\\sim\\mathcal{N}(0,\\pmb{\\Sigma}^{*})$ ,and $\\phi^{\\prime},\\ \\phi^{\\prime\\prime}$ are the firstand second-order derivatives of the activation $\\phi$ ", "page_idx": 4}, {"type": "text", "text": "The bases $\\psi$ \uff0c $\\psi^{(1)}$ $\\pmb{\\psi}^{(K-1)}\\in\\mathbb{R}^{N\\times N}$ using the above quantities are defned below. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\psi_{\\beta,\\beta^{\\prime}}:=1-\\displaystyle\\frac{1}{N}\\left(\\frac{\\chi_{\\kappa}+\\chi_{c^{*}}-\\chi_{q^{*}}}{\\chi_{\\kappa}}\\right)\\sum_{s=0}^{K-1}c_{s}\\cos\\left(\\theta_{\\beta,\\beta^{\\prime}}^{(s)}\\right),}\\\\ &{\\psi_{\\beta,\\beta^{\\prime}}^{(k)}:=\\cos\\left(\\theta_{\\beta,\\beta^{\\prime}}^{(k)}\\right)-\\displaystyle\\frac{1}{\\sum_{s=0}^{K-1}c_{s}}\\sum_{s=0}^{K-1}c_{s}\\cos\\left(\\theta_{\\beta,\\beta^{\\prime}}^{(s)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "From Lemma A.4, $K\\,-\\,1$ matrices in $\\{\\psi^{(k)}\\}_{k\\in[K]\\backslash\\{0\\}}$ are eigenbases with the eigenvalue $\\chi_{c^{*}}$ of the Jacobian linear map. From Lemma A.5, the matrix $\\psi$ is the eigenbases with the eigenvalue $\\chi$ of the Jacobian linear map. Since the rank of the Jacobian linear map is at most K (Lemma A.3), the deviation from the fixed point $\\boldsymbol{\\Sigma}^{(\\ell)}-\\boldsymbol{\\Sigma}^{*}$ is spanned by K-dimensional eigenspace span $\\left(\\{\\psi^{(k)}\\}_{k\\in[K]\\setminus\\{0\\}}\\cup\\{\\psi\\}\\right)$ . Then, the fxed point stability and the convergence rate are shown in the following theorem. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.4 (Exponential expressivity). Let $\\boldsymbol{E}^{(\\ell)}:=\\boldsymbol{\\Sigma}^{(\\ell)}-\\boldsymbol{\\Sigma}^{*}$ be the deviation from the fixed point at the $\\ell$ -th layer Suppose that the deviation at the first layer is decomposed as ${\\pmb{{\\cal E}}}^{(0)}=\\epsilon{\\pmb{\\psi}}+$ $\\sum_{k=1}^{K-1}\\epsilon_{k}\\psi^{(k)}+e$ The scalars $\\epsilon$ \uff0c $\\epsilon_{k}$ representthscalefthprubationforacheigencmonet of the linearly approximated map $E^{(\\ell)}\\,\\mapsto\\,E^{(\\ell+1)}$ The component $\\pmb{e}\\,\\in\\,\\mathbb{R}^{N\\times N}$ belongs to the orthogonal complements of the space span ${\\big(}\\{\\psi,\\psi^{(1)},\\,.\\,.\\,.\\,,\\,\\psi^{({\\hat{K}}-1)}\\}{\\big)}$ ", "page_idx": 5}, {"type": "text", "text": "Then, the deviation at the $\\ell$ -th layer is obtained by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle{\\cal E}^{(\\ell)}=\\chi^{\\ell}\\epsilon\\psi+\\sum_{k=1}^{K-1}\\chi_{c^{*}}^{\\ell}\\epsilon_{k}\\psi^{(k)},}}\\\\ {{\\displaystyle{\\chi:=\\frac{1}{N}\\sum_{s=0}^{K-1}c_{s}\\chi_{q^{*}}+\\left(1-\\frac{1}{N}\\sum_{s=0}^{K-1}c_{s}\\right)(\\chi_{\\kappa}+\\chi_{c^{*}})}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In particular when the Fourier mode is $\\begin{array}{r}{K=\\frac{N}{2}+1}\\end{array}$ Eqs. (9) and (10) reduce to the following. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\pmb{E}^{(\\ell)}=\\chi_{q^{*}}^{\\ell}\\epsilon\\psi+\\sum_{k=1}^{K-1}\\chi_{c^{*}}^{\\ell}\\epsilon_{k}\\psi^{(k)},}\\\\ {\\forall\\beta,\\beta^{\\prime}\\in[N],\\ \\psi_{\\beta,\\beta^{\\prime}}=1,\\ \\psi_{\\beta,\\beta^{\\prime}}^{(k)}=\\cos\\left(\\theta_{\\beta,\\beta^{\\prime}}^{(k)}\\right)-\\delta_{\\beta,\\beta^{\\prime}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Theorem 3.4 shows the expressivity of the FNO, which is characterized by the ordered-chaos phases and varies exponentially with respect to the number of layers. Theorem 3.4 indicates that the asymptotic behavior of the zero-frequency deviation is mostly determined by $\\chi$ and the periodic deviation is determined by $\\chi_{c^{*}}$ . If $\\chi<1$ and $\\chi_{c^{*}}<1$ , the fixed point is stable as the deviation from the fixed point converges exponentially to zero. When the fixed point remains stable at $c^{*}=1$ ,a random network exists in an ordered phase, where all spatial representations are correlated in an asymptotic manner. Conversely, when the fixed point with $c^{*}=1$ becomes unstable, the network transitions into a chaotic phase, exhibiting behavior dependent on the activation function $\\phi$ . The boundary between these two phases is referred to as the edge of chaos. ", "page_idx": 5}, {"type": "text", "text": "The convergence rates $\\chi_{q^{*}}$ and $\\chi_{c^{*}}$ are the same as the convergence rates of the variance and   \ncorrelation to the fixed point for DCN (Schoenholz et al., 2016) and CNN (Xiao et al., 2018).   \nHowever, only periodic spatial correlations are propagated in the FNO, resulting in a different   \neigenspace of the map $E^{(\\ell)}\\mapsto E^{(\\ell+1)}$ compared to the DCN and CNN. In DCN, the deviation $\\frac{N(\\bar{N^{-}}1)}{2}$ $K$ $\\textstyle{\\frac{N}{2}}+1$ $\\chi_{q^{*}}$ $\\chi_{c^{*}}$   \na similarity, possessing eigenspaces $\\chi_{q^{*}}$ for zero-frequency and eigenspaces $\\chi_{c^{*}}$ for $\\mathbf{k}$ -frequencies   \nwith diagonal components removed. Furthermore, mode truncation increases the convergence rate of   \nzero-frequency deviation from $\\chi_{q^{*}}$ to $\\chi$ and affects all eigenbases as well. For further discussions on   \nthe similarities between CNN and FNO, please refer to Appendix C. A visualization of the covariance   \nof the FNO with Tanh and ReLU activations is shown in Appendix F. ", "page_idx": 5}, {"type": "text", "text": "Finally, we demonstrate the connection between expressivity and trainability. By examining the covariance of the gradient in each layer during backpropagation, we investigate the conditions under which training is stable without gradient vanishing or exploding. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.5 (Trainability). Let $\\tilde{\\Sigma}^{(\\ell)}\\in\\mathbb{R}^{N\\times N}$ be the gradient covariance with respect to some loss $\\mathcal{L}_{g}$ .e.g. mean squared error, at the $\\ell$ -th layer. Suppose that the gradient covariance at the L-th layer is decomposed as $\\begin{array}{r}{\\tilde{\\Sigma}_{\\alpha,\\alpha^{\\prime}}^{(L)}=\\sum_{k=0}^{K-1}\\tilde{\\epsilon}_{k}\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k)}\\right)+\\tilde{e}}\\end{array}$ where $\\tilde{\\epsilon}_{k}$ is the coefcient of each basis and $\\tilde{e}$ ", "page_idx": 5}, {"type": "image", "img_path": "QJr02BTM7J/tmp/fee17d24406fd1939e9ee543c0e199c949fb441dcfb53de1925fd96ea66ab446.jpg", "img_caption": ["(a) Simplified FNO with Tanh (b) Simplified FNO with ReLU (c) Original FNO with ReLU "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 2: Average gradient norm $\\mathrm{Tr}(\\tilde{\\Sigma}^{(\\ell)})/D$ during the backpropagation of several FNOs plotted as a function of layer $\\ell$ . Each line corresponds to the result of different initial values of $\\sigma^{2}$ from 0.5 to 4.0 in increments of 0.5. The $\\mathbf{X}$ -axis is the layer and the y-axis is the log-scale of the gradient norm. Depending on the value of $\\sigma^{2}$ , the gradient norm increases or decreases consistently as the gradient propagates to shallower layers. ", "page_idx": 6}, {"type": "text", "text": "belongs o the orthogonal complemensof $\\operatorname{span}(\\{\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k)}\\right)\\}_{k=0}^{K-1})$ Then,the gradien covariane at the l-th layer is obtained by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\tilde{\\Sigma}_{\\alpha,\\alpha^{\\prime}}^{(\\ell)}=\\sum_{k=0}^{K-1}\\chi_{c^{*}}^{L-\\ell}\\tilde{\\epsilon}_{k}\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k)}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Theorem 3.5 shows that gradient vanishing occurs when $\\chi_{c^{*}}<1$ (ordered phase) and gradient explosion occurs when $\\chi_{c^{*}}>1$ (chaos phase). Thus, stable training of the FNO can be achieved close to the edge of chaos by setting the initial parameter $\\sigma^{2}$ tosatisfy $\\chi_{c^{*}}\\approx1$ We specifically present the initial parameter choices that achieve the edge of chaos for several FNOs in Section 3.3. ", "page_idx": 6}, {"type": "text", "text": "When $c^{*}=1$ , there is no change in the dynamics during backpropagation due to mode truncation. When usingthe full mode $\\begin{array}{r}{K\\stackrel{{}}{=}\\frac{N}{2}+1}\\end{array}$ thecondion $\\chi_{c^{*}}=1$ always achieves the edge of chaos, which is consistent with the results for the DCN and CNN. Despite the architectural and iterative map differences among FNO, DCN, and CNN, Theorem 3.4 and Theorem 3.5 demonstrate the similarities in the random behavior of FNO, DCN, and CNN. This allows existing results based on mean-field theory to be applied to the FNO. ", "page_idx": 6}, {"type": "text", "text": "3.3 Initialization requirements for stable training ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "For stable training, Theorem 3.5 suggests the necessity of initializing FNO near the edge of chaos, i.e., initializing FNO so that $\\chi_{c^{*}}\\approx1$ . In this section, we present the initial parameter choices that achieve the edge of chaos for several FNOs, each with slightly different architectures such as activation functions. Furthermore, the behavior of the gradient norm $\\mathrm{Tr}(\\tilde{\\Sigma}^{(\\ell)})/D$ as a function of layer $\\ell$ are visualized in Fig. 2 for several variants of random FNO with different initialization parameters $\\sigma^{2}$ We used FNO with a width of $D\\,=\\,32$ and a number of layers $L\\,=\\,64$ , and for simplicity, we computed the absolute value of the output as the loss with respect to the input sampled from the standard normal distribution. ", "page_idx": 6}, {"type": "text", "text": "Simplified FNO with Tanh activation. ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The behavior of $\\chi_{c^{*}}$ for the parameters $\\sigma^{2}$ and $\\sigma_{b}^{2}$ of the Tanh network has been extensively studied by Poole et al. (2016); Schoenholz et al. (2016). The phase diagram drawn by Pennington et al. (2017) is shown in Fig. 5a. By using parameters $(\\sigma^{2},\\sigma_{b}^{2})$ around the two phase boundaries of ordered and chaotic that achieve $\\chi_{c^{*}}=1$ , the training of the simplified FNO with Tanh activation can be stabilized. Figure 2a depicts the behavior of the gradient backpropagation in the simplified FNO with Tanh activation and the bias parameter being $\\bar{\\sigma}_{b}^{2}=0.1$ . Figure 2a shows that when $\\sigma^{2}\\lessapprox2$ , the gradient diminishes exponentially; otherwise, it explodes exponentially. ", "page_idx": 6}, {"type": "text", "text": "Simplified FNO with ReLU activation. ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The iterated map $\\mathcal{C}$ of the DCN with ReLU activation is given by Cho & Saul (2009) as follows. ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{q^{(\\ell+1)}=\\displaystyle\\frac{1}{2}\\sigma^{2}q^{(\\ell)}+\\sigma_{b}^{2},}}\\\\ {{c^{(\\ell+1)}q^{(\\ell+1)}=\\displaystyle\\frac{1}{2}\\sigma^{2}q^{(\\ell)}\\mathbb{J}_{1}\\left(\\frac{c^{(\\ell)}q^{(\\ell)}}{q^{(\\ell)}}\\right)+\\sigma_{b}^{2},}}\\\\ {{\\mathbb{J}_{1}(c)=\\displaystyle\\frac{1}{\\pi}\\left(\\sqrt{1-c^{2}}+(\\pi-\\operatorname{arccos}{(c)})c\\right),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The edge of chaos initialization for the DCN with ReLU activation is known as He initialization (He et al., 2015), which sets the initial variance parameter as $\\sigma^{2}=2$ and initial bias as $b_{i}^{(\\ell)}=0$ for Eq. (2). From the similarity of the DCN and the FNO, we can derive the FNO version of the He initialization that achieves $\\chi_{c^{*}}=1$ by setting $\\sigma^{2}=2$ $b_{i}^{(\\ell)}=0$ for Eq. (3). The He initialization scheme for the simplified FNO with the activation $\\phi=\\operatorname{ReLU}$ is derived as follows. ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\Theta_{i,j}^{(\\ell,k)\\ i\\cdot\\ i\\cdot d\\cdot}\\mathcal{N}(0,D^{-1}),\\ \\Xi_{i,j}^{(\\ell,k)\\ i\\cdot i\\cdot d\\cdot}\\mathcal{N}(0,D^{-1}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Figure 2b demonstrates that the choice of $\\sigma^{2}\\,=\\,2$ preserves the magnitude of the gradient norm during backpropagation of deep simplified FNO with ReLU activation. ", "page_idx": 7}, {"type": "text", "text": "Original FNO. ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In the original architecture of the FNO proposed by Li et al. (2020c), the DC module is used together with the Fourier convolution module as shown in Eq. (1). We initialize the weights of both layers consistently as follows. For all $\\ell\\in[L]$ $k\\in[K]$ ,and $i,j\\in[D]$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\Theta_{i,j}^{(\\ell,k)\\ i\\stackrel{i.i.d.}{\\sim}}\\mathcal{N}\\left(0,\\frac{\\sigma^{2}}{4D}\\right),\\ \\Xi_{i,j}^{(\\ell,k)\\ i\\stackrel{i.i.d.}{\\sim}}\\mathcal{N}\\left(0,\\frac{\\sigma^{2}}{4D}\\right),}}\\\\ {{\\hphantom{\\left(0,1\\right)}W_{i,j}^{(\\ell)\\ i\\stackrel{i.i.d.}{\\sim}}\\mathcal{N}\\left(0,\\frac{\\sigma^{2}}{2D}\\right),\\ b_{i}^{(\\ell)}\\sim\\mathcal{N}\\left(0,\\sigma_{b}^{2}\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "From the similarity of the initial network behavior of the FNO and the DCN, the fixed point with $c^{*}=1$ of the simplified FNO is also a fixed point of the original FNO. In the neighborhood of the fixed point $\\pmb{\\Sigma}^{*}$ theeigencomponents spannedby $\\{\\psi^{(k)}\\}_{k=1}^{K}$ Wwilldecayorincreaseatertof $\\chi_{c^{*}}$ Following the derivation of Theorem 3.5, the eigenvalues of the cos function eigencomponents of the gradient covariance are also $\\chi_{c^{*}}$ . These results show that the edge of chaos initialization scheme can be used for the original FNO with each activation function. Figure 2c shows that the original FNO With ReLU activation and theprameter fixed as $b_{i}^{(\\ell)}=0$ exhibits simlaackpagationha as the simplified FNO, i.e., $\\sigma^{2}\\approx2$ is an appropriate choice. ", "page_idx": 7}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we experimentally demonstrate that deep FNO training requires appropriate initialization settings on a variety of datasets, consistent with the theory discussed in Section 3. ", "page_idx": 7}, {"type": "text", "text": "4.1 Datasets ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We evaluated three models on commonly used PDEs: the advection equation, Burgers\u2019 equation, Darcy Flow equation, and incompressible Navier-Stokes (NS) equation. All datasets were generated by numerical simulations used in (Takamoto et al., 2022; Li et al., 2020c) and are publicly available. A summary of the dataset is provided in Table 1, and more details are given in Appendix D. ", "page_idx": 7}, {"type": "text", "text": "Advection equation and Burgers? equation. The linear advection equation for the function $u(x,t)$ is givenby ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\partial_{t}u(x,t)+\\beta\\partial_{x}\\left(u(x,t)/2\\right)=0,\\;u(x,0)=u_{0}(x),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $u_{0}$ is the initial condition and $\\beta=2.0$ is an advection speed. The non-linear Burgers\u2032 equation for the function $u(x,t)$ is given by ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\partial_{t}u(x,t)+\\partial_{x}\\left(u^{2}(x,t)/2\\right)=\\nu\\partial_{x x}u(x,t),\\;u(x,0)=u_{0}(x),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "table", "img_path": "QJr02BTM7J/tmp/bdf08a0e81e8b28a3069a6fbc8d2049942ef96f0adc3a88c7fb785368743c835.jpg", "table_caption": ["Table 1: Overview of dataset with number of spatial dimensions $M$ , time dependence, spatial resolution $N_{s}=N\\times\\cdots\\times N$ , temporal resolution $N_{t}$ , and number of samples for training, validation, and testing. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "where $u_{0}$ is the initial condition and $\\nu=4.0$ is the diffusion coefficient. ", "page_idx": 8}, {"type": "text", "text": "Darcy Flow equation. The Darcy Flow equation for the function $u(x)$ with a Dirichlet boundary is givenby ", "page_idx": 8}, {"type": "equation", "text": "$$\n-\\nabla\\cdot(a(x)\\nabla u(x))=f(x)\\quad(x\\in(0,1)^{2}),\\;u(x)=0\\quad(x\\in\\partial(0,1)^{2}),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $a$ is the diffusion coefficient and $f(x)=1$ is the forcing function. ", "page_idx": 8}, {"type": "text", "text": "Incompressible Navier-Stokes equation. The 2D NS equation on the unit torus is defined by ", "page_idx": 8}, {"type": "equation", "text": "$\\partial_{t}\\omega(x,t)+u(x,t)\\cdot\\nabla\\omega(x,t)=\\nu\\nabla^{2}\\omega(x,t)+f(x),\\,\\nabla\\cdot u(x,t)=0,\\,\\omega(x,0)=\\omega(x,t)+\\nu\\nabla^{2}\\omega(x,t)$ ", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\omega(x,t)$ is the vorticity, $\\omega_{0}$ is  the initial  vorticity, $u(x,t)$ is  the  velocityfield  for any $r~\\,>\\,\\,0$ \uff0c $\\nu\\;\\in\\;\\mathbb{R}_{+}$ is the viscosity, and $f$ is the external forcing function defined by $f(x)\\;=\\;0.1\\left(\\sin(2\\pi(x_{1}+x_{2})+\\cos(2\\pi(x_{1}+x_{2})\\right)$ . We experimented with the viscosities $\\nu\\,=$ $1e{-3,1e{-4,}1e{-5}}$ . For the data with $\\nu=1e{-4}$ , the time resolution was also downsampled by half. ", "page_idx": 8}, {"type": "text", "text": "4.2  Experimental Settings ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In the experiments on the 1D advection and Burgers\u2019 equation, we compared the results of simplified FNOs defined in Eq. (3) with Tanh and ReLU activations for varying number of layer $L$ and initial parameters $\\sigma^{2}$ . In the experiments on the 2D Darcy Flow and NS equations, we compared the results of the original FNO (Li et al., 2020c) as shown in Eq. (1) for varying number of layer $L$ and initial parameters $\\sigma^{2}$ All models have a width of $D=32$ and the Fourier mode of $K=12$ When using Tanh as the activation, we fixed the initial bias parameter $\\sigma_{b}^{2}=0.1$ , and when using ReLU activation, we fixed the initial bias $b_{i}^{(\\ell)}=0$ We used the Adam W optimizer and cosine annealing scheduler. Training was stopped early at the epoch of minimal normalized mean squared error (nMSE) on the validation data. Details of the experimental setup are given in Appendix D. In the experiments on the NS equation, we trained the original FNO with the autoregressive scheme using a teacher-forcing technique, input normalization, and regularization that adds small-Gaussian noise to the input (Tran et al., 2022). During the evaluation phase, only the initial 10 steps are provided as input, and the rollout results of all subsequent steps are evaluated. For all tasks, the mean squared error (MSE) is used as the training loss and nMSE as the validation and testing (Li et al., 2020a; Tran et al., 2022). ", "page_idx": 8}, {"type": "text", "text": "4.3 Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Heatmaps of the training loss measured by MSE at the last epoch and the test performance measured by nMSE on six different datasets, for different depths $L$ and initial parameters $\\sigma^{2}$ ,areshown in Figs. 3 and 4, respectively. Despite the differences in architectures and datasets, Figure 3 shows the same trend supporting the theory in all experiments. As the number of layers $L$ increases, the range of acceptable initial $\\sigma^{2}$ value settings becomes narrower, and initialization near the edge of chaos $\\quad\\sigma^{2}\\approx{\\dot{2}},$ is required for stable training of deep FNO. Detailed analyses on training loss and test performance are presented in Appendix E.1 and Appendix E.2, respectively. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this study, we developed a mean-field theory for FNO. We showed the expressivity and trainability of the FNO, which is characterized by the ordered-chaos phases. Furthermore, we observed both ", "page_idx": 8}, {"type": "image", "img_path": "QJr02BTM7J/tmp/494d815870b55ff3190158478f71d597d98d7a9ed4fc5e8f520076733daa59ab.jpg", "img_caption": ["(a) Tanh on advection eq. (b) ReLU on advection eq.(c) Tanh on Burgers? eq. (d) ReLU on Burgers eq. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "QJr02BTM7J/tmp/889de843376c38e121de98f7e36459356fc78ca6a0590fa4fff549c14b988c4b.jpg", "img_caption": ["(e) ReLU on Darcy Flow (f) ReLU on NS eq. (le-3)(g) ReLU on NS eq. (le-4) (h) ReLU on NS eq. (le-5) "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 3: Training loss of FNOs at last epoch for four distinct PDEs. (a, b): the advection equation, $(\\mathbf{c},\\mathbf{d})$ : the Burgers? equation, (e): Darcy Flow, (f-h): the NS equation. The heatmaps represents the training loss values for varying depth $L\\,\\in\\,\\{4,8,16,32\\}$ and initial weight parameter $\\sigma^{2}\\ \\in$ $\\{0.1,0.5,\\bar{1.0},2.0,3.0,4.0\\}$ , with lighter colors signifying lower training loss. The presented results are the mean training loss at the last epoch over three different seeds. ", "page_idx": 9}, {"type": "image", "img_path": "QJr02BTM7J/tmp/b27d96588b1f8bc63a6bcb583a0c3a7551c7a5edbe0ad60bdaebaaef0c13b4a8.jpg", "img_caption": ["(a) Tanh on advection eq. (b) ReLU on advection eq. (c) Tanh on Burgers eq.  (d) ReLU on Burgers? eq. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "QJr02BTM7J/tmp/dc35de39d501a0e778d5ac7b2d2447ac6912ea536598d7075dbef8c2aa5113ca.jpg", "img_caption": ["(e) ReLU on Darcy Flow (f) ReLU on NS eq. (1e-3) (g) ReLU on NS eq. (le-4) (h) ReLU on NS eq. (le-5) "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 4: nMSE of FNOs on test datasets for four distinct PDEs. (a, b): the advection equation (c, d): the Burgers\u2019 equation, (e): Darcy Flow, $(\\mathbf{f}\\mathbf{-}\\mathbf{h})$ : the NS equation. The heatmaps for each nMSE correspond to the results of each heatmap of training loss in Fig. 3. The lighter colors, the better the test performance. The presented results are the mean nMSE calculated over three different seeds. ", "page_idx": 9}, {"type": "text", "text": "unique FNO-specific behaviors caused by mode truncation, as well as common behaviors akin to those of DCN. With our analysis as a basis, we identified the necessity of initializing FNO near the edge of chaos for stable training of the FNO. Experimental results supported our theoretical results. ", "page_idx": 9}, {"type": "text", "text": "A limitation of our analysis is that it is limited to the network at initialization and does not address the stability of the entire optimization process. While we do not provide sufficient conditions for stable training, we do offer one necessary condition for achieving stable training. Future work may consider a mean-field analysis of the FNO when using skip-connection (Tran et al., 2022), Dropout and batch normalization, as well as initialization methods that ensure the input-output Jacobian of the FNO satisfies dynamical isometry (Pennington et al., 2017, 2018). ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported by JSPS KAKENHI Grant Number 24HO0709 Japan, JST ASPIRE Grant Number JPMJAP2329, RIKEN Special Postdoctoral Researcher Program, JST ACT-X Grant Number JPMJAX210K, and JST ACT-X Grant Number JPMJAX210D. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Brandstetter, J., Welling, M., and Worrall, D. E. Lie point symmetry data augmentation for neural pde solvers. In International Conference on Machine Learning, pp. 2241-2256. PMLR, 2022. ", "page_idx": 10}, {"type": "text", "text": "Brandstetter, J., van den Berg, R., Welling, M., and Gupta, J. K. Clifford neural layers for pde modeling. In The Eleventh International Conference on Learning Representations, 2023.   \nBurkholz, R. and Dubatovka, A. Initialization of relus for dynamical isometry. Advances in Neural Information Processing Systems, 32, 2019.   \nCho, Y. and Saul, L. Kernel methods for deep learning. Advances in neural information processing systems,22,2009.   \nGuibas, J., Mardani, M., Li, Z., Tao, A., Anandkumar, A., and Catanzaro, B. Adaptive fourier neural operators: Effcient token mixers for transformers. arXiv preprint arXiv:2111.13587, 2021.   \nHayou, S., Doucet, A., and Rousseau, J. On the selection of initialization and activation function for deep neural networks. arXiv preprint arXiv: 1805.08266, 2018.   \nHe, K., Zhang, X., Ren, S., and Sun, J. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pp. 1026-1034, 2015.   \nHelwig, J., Zhang, X., Fu, C., Kurtin, J., Wojtowytsch, S., and Ji, S. Group equivariant fourier neural operators for partial differential equations. arXiv preprint arXiv:2306.05697, 2023.   \nHwang, R., Lee, J. Y., Shin, J. Y, and Hwang, H. J. Solving pde-constrained control problems using operator learning. In Proceedings of the AAAl Conference on Artificial Intelligence, volume 36, No. 4, Pp. 4504-4512, 2022.   \nJiang, P., Meinert, N., Jordao, H., Weisser, C., Holgate, S., Lavin, A., Lutjens, B., Newman, D., Wainright, H., Walker, C., et al. Digital twin earth-coasts: Developing a fast and physics-informed surrogate model for coastal floods via neural operators. In Fourth Workshop on Machine Learning and the Physical Sciences (NeurIPS 2021), 2021a.   \nJiang, Z., Tahmasebi, P., and Mao, Z. Deep residual u-net convolution neural networks with autoregressive strategy for fuid flow predictions in large-scale geosystems. Advances in Water Resources, 150:103878, 2021b.   \nKovachki, N., Lanthaler, S., and Mishra, S. On universal approximation and error bounds for fourier neural operators. Journal of Machine Learning Research, 22(290):1-76, 2021.   \nLi, H., Xu, Z., Taylor, G., Studer, C., and Goldstein, T. Visualizing the loss landscape of neural nets. Advances in neural information processing systems, 31, 2018.   \nLi, Z., Kovachki, N., Azizzadenesheli, K., Liu, B., Bhattacharya, K., Stuart, A., and Anandkumar, A. Neural operator: Graph kernel network for partial differential equations. arXiv preprint arXiv:2003.03485, 2020a.   \nLi, Z., Kovachki, N., Azizzadenesheli, K., Liu, B., Stuart, A., Bhattacharya, K., and Anandkumar, A. Multipole graph neural operator for parametric partial differential equations. Advances in Neural Information Processing Systems, 33:6755-6766, 2020b.   \nLi, Z., Kovachki, N. B., Azizzadenesheli, K., Bhattacharya, K., Stuart, A., Anandkumar, A., et al. Fourier neural operator for parametric partial differential equations. In International Conference on Learning Representations, 2020c.   \nLi, Z., Huang, D. Z., Liu, B., and Anandkumar, A. Fourier neural operator with learned deformations for pdes on general geometries. arXiv preprint arXiv:2207.05209, 2022.   \nLi, Z., Meidani, K., and Farimani, A. B. Transformer for partial differential equations\u2019 operator learning. Transactions on Machine Learning Research, 2023.   \nLi,Z., Zheng, H, Kovachki, N., Jin, D., Chen, H., Liu, B.,Azizzadenesheli, K., and Anandkumar, A. Physics-informed neural operator for learning partial differential equations. ACM/JMS Journal of Data Science, 1(3):1-27, 2024.   \nPathak, J, Suramanian, S., Harrington, P, Raja, S., Chattpadhyay, A., Mardani, M, Kuth, T, Hall D., Li, Z., Azizzadenesheli, K., et al. Fourcastnet: A global data-driven high-resolution weather model using adaptive fourier neural operators. arXiv preprint arXiv:2202.11214, 2022.   \nPenningthlaGagl.Rn iidnt dynamical isometry: theory and practice. Advances in neural information procesing systems, 30, 2017.   \nPennington, J, Schoenholz, S., and Ganguli, S. The emergence of spectral universality in deep networks. In International Conference on Artificial Intelligence and Statistics, pp. 1924-1932. PMLR, 2018.   \nPoli, M, Massarol, S., Berto, F, Park, J., Dao, T, Re, C., and Ermon, S. Transform once: Effint operator learning in frequency domain. Advances in Neural Information Processing Systems, 35: 7947-7959, 2022.   \nPoole, B., Lahiri, S., Raghu, M., Sohl-Dickstein, J., and Ganguli, S. Exponential expressivity in deep neural networks through transient chaos. Advances in neural information processing systems, 29, 2016.   \nRahman, M. A., Ross, Z. E., and Azizzadenesheli, K. U-no: U-shaped neural operators. arXiv preprint arXiv:2204.11127, 2022.   \nSchoenholz, S. S., Gilmer, J., Ganguli, S., and Sohl-Dickstein, J. Deep information propagation. In International Conference on Learning Representations, 2016.   \nTakamoto, M., Praditia, T., Leiteritz, R., MacKinlay, D., Alesiani, F, Pfuger, D., and Niepert, M. Pdebench: An extensive benchmark for scientific machine learning. Advances in Neural Information Processing Systems, 35:1596-1611, 2022.   \nTran, A., Mathews, A., Xie, L., and Ong, C. S. Factorized fourier neural operators. In The Eleventh International Conference on Learning Representations, 2022.   \nTripura, T. and Chakraborty, S. Wavelet neural operator: a neural operator for parametric partial differential equations. arXiv preprint arXiv:2205.02191, 2022.   \nWen, G., Tang, M., and Benson, S. M. Multiphase flow prediction with deep neural networks. arXiv preprint arXiv:1910.09657, 2019.   \nWen, G., Li, Z., Azizzadenesheli, K., Anandkumar, A., and Benson, S. M. U-fno\u2014-an enhanced fourier neural operator-based deep-learning model for multiphase flow. Advances in Water Resources, 163:104180, 2022a.   \nWen, G., Li, Z., Long, Q., Azizzadenesheli, K., Anandkumar, A., and Benson, S. M. Accelerating carbon capture and storage modeling using fourier neural operators. arXiv preprint arXiv:2210.17051, 3,2022b.   \nXiao, L., Bahri, Y., Sohl-Dickstein, J., Schoenholz, S., and Pennington, J. Dynamical isometry and a mean field theory of cnns: How to train 10,0o0-layer vanilla convolutional neural networks. In International Conference on Machine Learning, Pp. 5393-5402. PMLR, 2018.   \nYang, G. and Schoenholz, S. Mean feld residual networks: On the edge of chaos. Advances in neural information processing systems, 30, 2017. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Yang, Y., Gao, A. F., Castellanos, J. C., Ross, Z. E., Azizzadenesheli, K., and Clayton, R. W. Seismic wave propagation and inversion with neural operators. The Seismic Record, 1(3):126-134, 2021. Zhao, J., George, R. J., Zhang, Y., Li, Z., and Anandkumar, A. Incremental fourier neural operator. arXiv preprint arXiv:2211.15188, 2022. ", "page_idx": 12}, {"type": "image", "img_path": "QJr02BTM7J/tmp/c39fb6e5d2d9b2a5357c7a1bcaf6b13c07312d82e9f3a39470dbf449fd98f2e3.jpg", "img_caption": ["Figure 5: Ordered-chaos phase transition diagram for the DCN "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "AProof of Theorem 3.4 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Theorem 3.4 (Exponential expressivity). Let $\\boldsymbol{E}^{(\\ell)}:=\\boldsymbol{\\Sigma}^{(\\ell)}-\\boldsymbol{\\Sigma}^{*}$ be the deviation from the fixed point at the l-th layer. Suppose that the deviation at the first layer is decomposed as $\\pmb{{\\cal E}}^{(0)}=\\epsilon\\psi+$ $\\sum_{k=1}^{K-1}\\epsilon_{k}\\psi^{(k)}+e$ The scalars $\\epsilon$ \uff0c $\\epsilon_{k}$ represent the scale of the perturbation foreach eigencomponent of thelinearly approximatedmap $E^{(\\ell)}\\,\\mapsto\\,E^{(\\ell+1)}$ The component $\\pmb{e}\\,\\in\\,\\mathbb{R}^{N\\times N}$ belongs to the orthogonal complements of the space span ${\\bigl(}\\{\\psi,\\psi^{(1)},\\ .\\ .\\ .\\ ,\\ \\psi^{({\\hat{K}}-1)}\\bigr\\}{\\bigr)}$ ", "page_idx": 13}, {"type": "text", "text": "Then, the deviation at the $\\ell$ -th layer is obtained by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle{\\pmb E}^{(\\ell)}=\\chi^{\\ell}\\epsilon\\psi+\\sum_{k=1}^{K-1}\\chi_{c^{*}}^{\\ell}\\epsilon_{k}\\psi^{(k)},}}\\\\ {{\\displaystyle{\\chi:=\\frac{1}{N}\\sum_{s=0}^{K-1}c_{s}\\chi_{q^{*}}+\\left(1-\\frac{1}{N}\\sum_{s=0}^{K-1}c_{s}\\right)(\\chi_{\\kappa}+\\chi_{c^{*}})}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "In particular, when the Fourier mode is $\\begin{array}{r}{K=\\frac{N}{2}+1,}\\end{array}$ . Eqs. (9) and (10) reduce to the following. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\pmb{E}^{(\\ell)}=\\chi_{q^{*}}^{\\ell}\\epsilon\\psi+\\sum_{k=1}^{K-1}\\chi_{c^{*}}^{\\ell}\\epsilon_{k}\\psi^{(k)},}\\\\ {\\forall\\beta,\\beta^{\\prime}\\in[N],\\ \\psi_{\\beta,\\beta^{\\prime}}=1,\\ \\psi_{\\beta,\\beta^{\\prime}}^{(k)}=\\cos\\left(\\theta_{\\beta,\\beta^{\\prime}}^{(k)}\\right)-\\delta_{\\beta,\\beta^{\\prime}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. The theorem is obtained by the eigenvalue analysis on the the first-order Taylor approximation of the iterated map $\\mathcal{C}$ at the fixed point $\\Sigma^{*}$ .The Jacobian matrix $J(\\Sigma^{*})\\in\\mathbb{R}^{N^{2}\\,\\stackrel{*}{\\times}\\,N^{2}}$ of the iterated map at the fixed point and the Jacobian linear map $J_{\\Sigma^{*}}(\\cdot)\\colon\\mathbb{R}^{N\\times N}\\stackrel{\\cdot}{\\to}\\mathbb{R}^{N\\times N}$ are defined as follows. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~\\left[J(\\Sigma^{*})\\right]_{(\\alpha,\\alpha^{\\prime}),(\\beta,\\beta^{\\prime})}:=\\left.\\frac{\\partial\\left[\\boldsymbol{\\mathcal{C}}\\left(\\Sigma\\right)\\right]_{\\alpha,\\alpha^{\\prime}}}{\\partial\\Sigma_{\\beta,\\beta^{\\prime}}}\\right|_{\\Sigma=\\Sigma^{*}},}\\\\ &{\\forall\\Sigma\\in\\mathbb{R}^{N\\times N},\\ J_{\\Sigma^{*}}(\\Sigma):=\\mathrm{mat}\\left(J(\\Sigma^{*})\\,\\mathrm{vec}(\\Sigma)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where vec performs the vectorization, i.e. transforming an $N\\times N$ matrix into a vector of size $N^{2}$ and mat performs the inverse operation of vec. ", "page_idx": 13}, {"type": "text", "text": "From Lemma A.3, $K-1$ matrices in $\\{\\mathrm{vec}(\\psi^{(k)})\\}_{k\\in[K]\\backslash\\{0\\}}$ are eigenbases with the eigenvalue $\\chi_{c^{*}}$ of the Jacobian linear map. From Lemma A.4, the matrix $\\psi$ is the eigenbases with the eigenvalue $\\chi$ of the Jacobian linear map. Since the sets of $K$ matrices in $\\{\\psi^{(k)}\\}_{k\\in[K]\\setminus\\{0\\}}\\cup\\{\\psi\\}$ are linearly independent (yet non-orthogonal) in $\\mathbb{R}^{N\\times N}$ , the subspace span $\\left(\\{\\psi^{(k)}\\}_{k\\in[K]\\setminus\\{0\\}}\\cup\\{\\psi\\}\\right)$ is the $K$ dimensional eigenspace of the Jacobian linear map. From Lemma A.2, the rank of the Jacobian matrix $J(\\pmb{\\Sigma}^{*})$ at the fixed point $\\pmb{\\Sigma}^{*}$ is at most $K$ , thereby the rank of the Jacobian linear map in Eq. (15) is at most $K$ .Therefore,wehave ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\quad~}\\forall e\\in\\mathrm{span}\\left(\\left\\{\\psi^{(k)}\\right\\}_{k\\in[K]\\backslash\\{0\\}}\\cup\\left\\{\\psi\\right\\}\\right)^{\\perp},\\quad J_{\\mathbf{Z}^{*}}(e)=O.}\\\\ &{\\mathcal{C}(\\mathbf{Z}^{*}+\\mathbf{E}^{(\\ell-1)})\\approx\\mathbf{Z}^{*}+J_{\\mathbf{Z}^{*}}(\\mathbf{E}^{(\\ell-1)})}\\\\ &{\\qquad\\qquad\\qquad=\\mathbf{Z}^{*}+\\left(\\chi^{\\ell-1}\\epsilon J_{\\mathbf{Z}^{*}}(\\psi)+\\displaystyle\\sum_{k=1}^{K-1}\\chi_{c^{*}}^{\\ell-1}\\epsilon_{k}J_{\\mathbf{Z}^{*}}(\\psi^{(k)})\\right)}\\\\ &{\\qquad\\qquad\\qquad=\\mathbf{Z}^{*}+\\underbrace{\\chi^{\\ell}\\epsilon\\psi+\\sum_{k=1}^{K-1}\\chi_{c^{*}}^{\\ell}\\epsilon_{k}\\psi^{(k)}}_{=K^{(\\ell)}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Lemma 3.1 (Iterated map). For all $d\\in[D]$ the covariance $\\mathbf{\\Sigma}^{\\left(\\ell\\right)}:=\\mathbb{E}_{\\Theta^{1:\\ell},\\Xi^{1:\\ell}}\\left[\\mathbf{H}_{:,d}^{\\left(\\ell\\right)}\\mathbf{\\Lambda}\\mathbf{H}_{:,d}^{\\left(\\ell\\right)}\\right]i$ obtained recursively by the iterated map $\\mathcal{C}$ definedby ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Sigma_{\\alpha,\\alpha^{\\prime}}^{(\\ell)}=\\underbrace{\\sigma^{2}\\displaystyle\\sum_{k=0}^{K-1}c_{k}\\mathbb{E}\\left[\\left|\\left[F\\phi\\left(\\mathbf{H}_{:,d}\\right)\\right]_{k}\\right|^{2}\\right]\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k)}\\right)+\\sigma_{b}^{2}}_{=:\\;\\mathcal{C}\\left(\\Sigma^{(\\ell-1)}\\right)_{\\alpha,\\alpha^{\\prime}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the expectation is taken over the pre-activations $\\mathbf{H}_{:,d}\\sim\\mathcal{N}(0,\\pmb{\\Sigma}^{(\\ell-1)})$ $\\begin{array}{r}{\\theta_{\\alpha,\\alpha^{\\prime}}^{(k)}:=\\frac{2\\pi k}{N}(\\alpha-\\alpha^{\\prime})}\\end{array}$ represents the scaled positional difference. ", "page_idx": 14}, {"type": "text", "text": "Proof. For simplicity, we introduce $Y_{\\alpha,k,\\beta,i}$ as follows. ", "page_idx": 14}, {"type": "equation", "text": "$$\nH_{\\alpha,d}^{(\\ell,k)}=\\sum_{i=1}^{D}\\sum_{\\beta=0}^{N-1}\\underset{=:\\,Y_{\\alpha,k,\\beta,i}}{\\overset{\\mathrm{F}^{\\dagger}}{\\sum}}F_{\\alpha,k}F_{k,\\beta}X_{\\beta,i}^{(\\ell-1)}\\left(\\Theta_{i,d}^{(\\ell,k)}+\\left(1-\\left(\\delta_{k,0}+\\delta_{k,N/2}\\right)\\right)\\sqrt{-1}\\Xi_{i,d}^{(\\ell,k)}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since the weights are sampled independently, for different $k\\neq k^{\\prime}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[H_{\\alpha,d}^{(\\ell,k)}H_{\\alpha,d}^{(\\ell,k^{\\prime})}\\right]=\\mathbb{E}\\left[H_{\\alpha,d}^{(\\ell,k)}\\overline{{H}}_{\\alpha,d}^{(\\ell,k^{\\prime})}\\right]=\\mathbb{E}\\left[\\overline{{H}}_{\\alpha,d}^{(\\ell,k)}\\overline{{H}}_{\\alpha,d}^{(\\ell,k^{\\prime})}\\right]=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "From Eq. (3), we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\Theta^{(\\varepsilon)},\\Xi^{(\\varepsilon)}}[H_{\\alpha,d}^{(\\ell)}H_{\\alpha^{\\prime},d}^{(\\ell)}]=\\sum_{k=0}^{K-1}\\frac{c_{k}}{2}\\mathbb{E}\\left[\\left(H_{\\alpha,d}^{(\\ell,k)}+\\overline{{H}}_{\\alpha,d}^{(\\ell,k)}\\right)\\left(H_{\\alpha^{\\prime},d}^{(\\ell,k)}+\\overline{{H}}_{\\alpha^{\\prime},d}^{(\\ell,k)}\\right)\\right]+\\mathbb{E}\\left[(b_{d}^{(\\ell)})^{2}\\right].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "First, we calculate the term $k\\neq0,\\frac{N}{2}$ , where $c_{k}=2$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left(H_{t}^{(\\alpha,\\beta)}+H_{t}^{(\\alpha,\\beta)}\\right)\\left(H_{t}^{(\\alpha,\\beta)}+H_{t}^{(\\alpha,\\beta)}\\right)\\right]}\\\\ &{=\\underset{\\mathcal{L}_{u=1}}{\\overset{\\mathcal{L}_{u=1}}{\\sum}}\\underset{\\mathcal{L}_{u=1}}{\\overset{\\mathcal{L}_{u=1}}{\\sum}}\\underset{\\mathcal{L}_{u=1}}{\\sum}\\chi_{u,u,v}\\chi_{v,u,v}(\\mu^{\\alpha}-\\sigma^{\\alpha})/2\\mathbb{D}}\\\\ &{\\quad+\\underset{\\mathcal{L}_{u=1}}{\\overset{\\mathcal{L}_{u=1}}{\\sum}}\\underset{\\mathcal{L}_{u=1}}{\\overset{\\mathcal{L}_{u=1}}{\\sum}}\\underset{\\mathcal{L}_{u=1}}{\\sum}\\underset{\\mathcal{L}_{u=1}}{\\overset{\\mathcal{L}_{u=1}}{\\sum}}\\underset{\\mathcal{L}_{u=1}}{\\sum}\\left(\\mu_{u,v}^{(\\alpha,\\beta)}+H_{t}^{(\\alpha,\\beta)}\\right)\\mathbb{D}}\\\\ &{\\quad+\\underset{\\mathcal{L}_{u=1}}{\\overset{\\mathcal{L}_{u=1}}{\\sum}}\\underset{\\mathcal{L}_{u=1}}{\\overset{\\mathcal{L}_{u=1}}{\\sum}}\\underset{\\mathcal{L}_{u=1}}{\\sum}\\underset{\\mathcal{L}_{u=1}}{\\sum}\\sum_{u=1}^{\\mathcal{L}_{u}}\\mathbb{D}\\varphi_{u,v}(\\mu^{\\alpha}+\\sigma^{\\alpha})/2\\mathbb{D}}\\\\ &{\\quad+\\underset{\\mathcal{L}_{u=1}}{\\overset{\\mathcal{L}_{u=1}}{\\sum}}\\underset{\\mathcal{L}_{u=1}}{\\overset{\\mathcal{L}_{u=1}}{\\sum}}\\underset{\\mathcal{L}_{u=1}}{\\sum}\\underset{\\mathcal{L}_{u=1}}{\\sum}\\sum_{u=1}^{\\mathcal{L}_{u}}\\mathbb{D}\\varphi_{u,v}(\\mu^{\\alpha}+\\sigma^{\\alpha})/2\\mathbb{D}}\\\\ &{=\\frac{\\alpha^{2}}{\\mathcal{L}_{u=1}^{2}}\\underset{\\mathcal{L}_{u=1}}{\\overset{\\mathcal{L}_{u=1}}{\\sum}}\\underset{\\mathcal{L}_{u=1}}{\\overset{\\mathcal{L}_{u=1}}{\\sum}}\\\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "$\\hat{X}_{k,i}^{(\\ell-1)}$ isthe $k$ $\\mathbf{X}_{:,i}^{(\\ell-1)}$ ", "page_idx": 15}, {"type": "text", "text": "Second, we calculae te terms k = 0, , where ck = 1 and H(.b) = H(.a). ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\frac{c_{k}}{2}\\left(H_{\\alpha,d}^{(\\ell,k)}+\\overline{{H}}_{\\alpha,d}^{(\\ell,k)}\\right)\\left(H_{\\alpha^{\\prime},d}^{(\\ell,k)}+\\overline{{H}}_{\\alpha^{\\prime},d}^{(\\ell,k)}\\right)\\right]=2\\displaystyle\\sum_{i,i^{\\prime}=1}^{D}\\displaystyle\\sum_{\\beta,\\beta^{\\prime}=0}^{N-1}\\left(Y_{\\alpha,k,\\beta,i}Y_{\\alpha^{\\prime},k,\\beta^{\\prime},i^{\\prime}}\\right)\\delta_{i,i^{\\prime}}\\frac{\\sigma^{2}}{2D}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\frac{\\sigma^{2}}{D}\\displaystyle\\sum_{i=1}^{D}\\left|\\hat{X}_{k,i}^{(\\ell-1)}\\right|^{2}\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since the Fourier modes $|\\hat{X}_{k,i}^{(\\ell-1)}|^{2}$ are i.i.d. for each hidden dimension $i\\in[D]$ ,we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathfrak{L}_{\\Theta^{0;\\varepsilon-1},\\Xi^{0;\\varepsilon-1},b^{0;\\varepsilon-1}}\\mathbb{E}_{\\Theta^{(\\varepsilon)},\\Xi^{(\\varepsilon)},b^{(\\varepsilon)}}\\left[H_{\\alpha,d}^{(\\ell)}H_{\\alpha^{\\prime},d}^{(\\ell)}\\right]=\\sigma^{2}\\displaystyle\\sum_{k=0}^{K-1}c_{k}\\mathbb{E}_{\\Theta^{0;\\varepsilon-1},\\Xi^{0;\\varepsilon-1}}\\left[\\left|\\hat{X}_{k,i}^{(\\ell-1)}\\right|^{2}\\right]\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k)}\\right)}\\\\ {=\\sigma^{2}\\displaystyle\\sum_{k=0}^{K-1}c_{k}\\mathbb{E}_{\\mathbf{H}_{:,d}\\sim N\\left(0,\\Sigma^{(\\varepsilon-1)}\\right)}\\left[\\left|\\hat{X}_{k,i}^{(\\ell-1)}\\right|^{2}\\right]\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "To obtain a more tractable expression in the subsequent proofs of theorems, we express the iterated map without using Fourier modes as follows. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbf{Q}^{0;\\varepsilon},\\Xi^{0;\\varepsilon},b^{0;\\varepsilon}}\\left[H_{\\alpha,d}^{(\\ell)}H_{\\alpha^{\\prime},d}^{(\\ell)}\\right]}\\\\ &{=\\frac{\\sigma^{2}}{N^{2}}\\displaystyle\\sum_{k=0}^{K-1}c_{k}\\displaystyle\\sum_{\\beta,\\beta^{\\prime}=0}^{N-1}\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k)}\\right)\\cos\\left(\\theta_{\\beta,\\beta^{\\prime}}^{(k)}\\right)\\mathbb{E}_{\\mathbf{H}_{\\bot,d}\\sim N\\left(0,\\Sigma^{(\\ell-1)}\\right)}\\left[\\phi(H_{\\beta,d})\\phi(H_{\\beta^{\\prime},d})\\right]+\\sigma_{b}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Lemma 3.2 (Exsistance of fixed points). When a random DCN defined by Eq. (2) has the fixed point $(q^{*},c^{*}=1)$ for theinitialparameters $(\\sigma^{2},\\sigma_{b}^{2})$ , then a random simplified FNO defined by Eq. (3) has afixedpoint $\\Sigma^{*}$ oftheform ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Sigma^{*}=q^{*}I_{N}+q^{*}c^{*}(\\mathbf{1}_{N}\\mathbf{1}_{N}^{\\top}-I_{N})=q^{*}\\mathbf{1}_{N}\\mathbf{1}_{N}^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Using the properties of cosine functions, the following holds ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sigma_{b}^{2}=\\frac{1}{N^{2}}\\sum_{k=0}^{K-1}c_{k}\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k)}\\right)\\underbrace{\\sum_{\\beta,\\beta^{\\prime}=0}^{N-1}\\cos\\left(\\theta_{\\beta,\\beta^{\\prime}}^{(k)}\\right)\\sigma_{b}^{2}}_{=N^{2}\\delta_{k,0}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, the following holds for all $\\alpha,\\alpha^{\\prime}\\in[N]$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{C}(\\Sigma^{*})]_{\\alpha,\\alpha^{\\prime}}=\\frac{1}{N^{2}}\\sum_{k=0}^{K-1}c_{k}\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k)}\\right)\\sum_{\\beta,\\beta^{\\prime}=0}^{N-1}\\cos\\left(\\theta_{\\beta,\\beta^{\\prime}}^{(k)}\\right)\\left(\\sigma^{2}\\mathbb{E}_{{\\mathbf{H}}_{\\bot,d}\\sim N(0,\\Sigma^{*})}\\left[\\phi(H_{\\beta,d})\\phi(H_{\\beta^{\\prime},d})\\right]+\\sigma_{b}^{2}\\sigma_{d}^{2}\\sigma_{d}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "When the DCN is applied pointwise to each spatial representation $H_{\\beta,3}$ $\\beta\\in[N]$ , the iterated map of the random DCN (Poole et al., 2016) is given by $\\mathcal{C}_{\\mathrm{DCN}}(\\Sigma):=\\sigma^{2}\\mathbb{E}_{\\mathbf{H}_{:,d}\\sim\\mathcal{N}(0,\\Sigma)}\\left[\\phi(H_{\\beta,d})\\phi(H_{\\beta^{\\prime},d})\\right]+$ $\\sigma_{b}^{2}$ .Since the covariance $\\Sigma^{*}$ is a fixed point with respect to the iterative map of the DCN, i.e. $\\bar{\\mathcal{C}}_{\\mathrm{DCN}}(\\Sigma^{*})=\\Sigma^{*}$ , the following holds. ", "page_idx": 16}, {"type": "equation", "text": "$$\n[\\mathcal{C}(\\Sigma^{*})]_{\\alpha,\\alpha^{\\prime}}=\\frac{1}{N^{2}}\\sum_{k=0}^{K-1}c_{k}\\cos\\Big(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k)}\\Big)\\underbrace{\\sum_{\\beta,\\beta^{\\prime}=0}^{N-1}\\cos\\Big(\\theta_{\\beta,\\beta^{\\prime}}^{(k)}\\Big)}_{=N^{2}\\delta_{k,0}}\\underbrace{q^{*}(\\delta_{\\beta,\\beta^{\\prime}}+(1-\\delta_{\\beta,\\beta^{\\prime}})c^{*})}_{=q^{*}}=q^{*}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, we confirm that the covariance $\\Sigma^{*}$ satisfies the definition of a fixed point in the map $\\mathcal{C}$ ,i.e. $\\Sigma^{*}=\\mathcal{C}(\\Sigma^{*})$ . This means that the fixed point for the iterated map $\\Sigma^{*}$ of the DCN also serves as a fixed point for the iterated map of the simplified FNO. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Lemma A.1. Let $\\Sigma^{*}$ be the fixed point of the form in Eq. (5). Suppose that the symmetric perturbation $\\pmb{E}\\in\\mathbb{R}^{N\\times N}$ where $E_{\\beta,\\beta}=E_{\\beta^{\\prime},\\beta^{\\prime}}$ and $E_{\\beta,\\beta^{\\prime}}$ are non-zero for some $\\beta,\\beta^{\\prime}\\in[N]$ \uff0c $\\beta\\neq\\beta$ and all other elements arezero.Then,wehave ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sigma^{2}\\mathbb{E}_{\\mathbf{H}_{:},d\\sim N(0,\\pmb{\\Sigma}^{*}+\\pmb{E})}\\left[\\phi(H_{\\beta,d})^{2}\\right]+\\sigma_{b}^{2}=q^{*}+E_{\\beta,\\beta\\chi q^{*}}+\\mathcal{O}\\left(|E_{\\beta,\\beta}|^{2}\\right),\\qquad\\qquad(1.67)}\\\\ {\\sigma^{2}\\mathbb{E}_{\\mathbf{H}_{:},d\\sim N(0,\\pmb{\\Sigma}^{*}+\\pmb{E})}\\left[\\phi(H_{\\beta,d})\\phi(H_{\\beta^{\\prime},d})\\right]+\\sigma_{b}^{2}=q^{*}c^{*}+E_{\\beta,\\beta\\chi_{\\kappa}}+E_{\\beta,\\beta^{\\prime}\\chi c^{*}}+\\mathcal{O}\\left(|E_{\\beta,\\beta^{\\prime}}|^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\chi_{q^{*}},\\:\\chi_{c^{*}}$ , and $\\chi_{\\kappa}$ are the constants defined by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\qquad\\chi_{q^{*}}:=\\sigma^{2}\\mathbb{E}_{\\mathbf{H}_{:,d}\\sim\\mathcal{N}(0,\\Sigma^{*})}\\left[\\phi^{\\prime2}(H_{\\beta,d})+\\phi^{\\prime\\prime}(H_{\\beta,d})\\phi(H_{\\beta,d})\\right],}\\\\ &{\\qquad\\qquad\\qquad\\chi_{c^{*}}:=\\sigma^{2}\\mathbb{E}_{\\mathbf{H}_{:,d}\\sim\\mathcal{N}(0,\\Sigma^{*})}\\left[\\phi^{\\prime}(H_{\\beta,d})\\phi^{\\prime}(H_{\\beta^{\\prime},d})\\right],}\\\\ &{\\qquad\\qquad\\qquad\\chi_{\\kappa}:=\\frac{\\sigma^{2}}{2}\\mathbb{E}\\left[\\phi^{\\prime\\prime}(H_{\\alpha,d})\\phi(H_{\\alpha^{\\prime},d})+\\phi(H_{\\alpha,d})\\phi^{\\prime\\prime}(H_{\\alpha^{\\prime},d})+2c^{*}\\phi^{\\prime}(H_{\\alpha,d})\\phi^{\\prime}(H_{\\alpha^{\\prime},d})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Equation (19) is obviously shown by the result of Section 2 in (Poole et al., 2016) and Section 3 in (Schoenholz et al., 2016). We prove Eq. (20) with reference to the results of (Schoenholz et al., 2016). ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sigma^{2}\\mathbb{E}_{\\mathbf{H}_{:},d\\sim\\mathcal{N}(0,\\Sigma^{*}+E)}\\left[\\phi(H_{\\beta,d})\\phi(H_{\\beta^{\\prime},d})\\right]+\\sigma_{b}^{2}=\\sigma^{2}\\int\\mathcal{D}z_{1}\\mathcal{D}z_{2}\\phi(u_{1})\\phi(u_{2})+\\sigma_{b}^{2},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\begin{array}{r}{\\int\\mathcal{D}\\,\\mathrm{d}z=\\frac{1}{\\sqrt{2\\pi}}\\int\\ \\mathrm{d}z e^{-\\frac{1}{2}z^{2}}}\\end{array}$ is the measure for a standard Gaussian distribution, $u_{1}=\\sqrt{q}z_{1}$ \uff0c $u_{2}=\\sqrt{q}\\left(c_{\\beta,\\beta^{\\prime}}z_{1}+\\sqrt{1-c_{\\beta,\\beta^{\\prime}}^{2}}z_{2}\\right)$ \uff0c $q=q^{*}+E_{\\beta,\\beta}$ and $\\begin{array}{r}{c_{\\beta,\\beta^{\\prime}}=c^{*}+\\frac{E_{\\beta,\\beta^{\\prime}}}{q}}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "We consider the case where $c^{*}<1$ and $c^{*}=1$ separately. Later, we will show that the two results agree with each other. First, we consider the case where $c^{*}<1$ . Using Taylor expansion, we can approximate $\\phi(u_{1})$ and $\\phi(u_{2})$ as follows. For the simplicity, we assume $\\mathcal{O}(\\vert E_{\\beta,\\beta}\\vert)^{\\prime}{=}\\mathcal{O}(\\vert E_{\\beta,\\beta^{\\prime}}\\vert)$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\phi(u_{1})=\\phi(u_{1}^{*})+\\frac{1}{2}\\frac{E_{\\beta,\\beta}}{\\sqrt{q^{*}}}z_{1}\\phi^{\\prime}(u_{1}^{*})+\\mathcal{O}(|E_{\\beta,\\beta}|^{2}),}\\\\ {\\displaystyle\\widehat{\\upsilon}(u_{2})=\\phi\\left(u_{2}^{*}\\right)+\\frac{E_{\\beta,\\beta^{\\prime}}}{\\sqrt{q^{*}}}\\left(z_{1}-\\frac{c^{*}}{\\sqrt{1-\\left(c^{*}\\right)^{2}}}z_{2}\\right)\\phi^{\\prime}(u_{2}^{*})+\\frac{E_{\\beta,\\beta}}{2\\sqrt{q^{*}}}(c^{*}z_{1}+\\sqrt{1-\\left(c^{*}\\right)^{2}}z_{2})\\phi^{\\prime}(u_{2}^{*})+\\mathcal{O}(\\phi^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $u_{1}^{*}=\\sqrt{q^{*}}z_{1}$ and $u_{2}^{*}=\\sqrt{q^{*}}(c^{*}z_{1}+\\sqrt{1-(c^{*})^{2}}z_{2})$ ", "page_idx": 17}, {"type": "text", "text": "Thus, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{r^{2}\\displaystyle\\int\\mathcal{D}z_{1}\\mathcal{D}z_{2}\\phi(u_{1})\\phi(u_{2})+\\sigma_{b}^{2}}\\\\ &{=\\underbrace{\\sigma^{2}\\displaystyle\\int\\mathcal{D}z_{1}\\mathcal{D}z_{2}\\phi(u_{1}^{*})\\phi(u_{2}^{*})+\\sigma_{b}^{2}+\\sigma^{2}}_{=q^{*}c^{*}}+\\sigma^{2}\\displaystyle\\int\\mathcal{D}z_{1}\\mathcal{D}z_{2}\\frac{E_{\\beta,\\beta^{\\prime}}}{\\sqrt{q^{*}}}\\left(z_{1}-\\frac{c^{*}}{\\sqrt{1-(c^{*})^{2}}}z_{2}\\right)\\phi(u_{1}^{*})\\phi^{\\prime}(u_{2}^{*})}\\\\ &{\\quad+\\sigma^{2}\\displaystyle\\int\\mathcal{D}z_{1}\\mathcal{D}z_{2}\\frac{1}{2}\\frac{E_{\\beta,\\beta}}{\\sqrt{q^{*}}}(c^{*}z_{1}+\\sqrt{1-(c^{*})^{2}}z_{2})\\phi(u_{1}^{*})\\phi^{\\prime}(u_{2}^{*})+\\sigma^{2}\\displaystyle\\int\\mathcal{D}z_{1}\\mathcal{D}z_{2}\\frac{1}{2}\\frac{E_{\\beta,\\beta}}{\\sqrt{q^{*}}}z_{1}\\phi^{\\prime}(u_{1}^{*})\\phi(u_{2}^{*}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The results of the second term are obtained from the transformation of equations 36 to 39 in Appendix 7.2 of (Schoenholz et al., 2016). ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sigma^{2}\\frac{E_{\\beta,\\beta^{\\prime}}}{\\sqrt{q^{*}}}\\int\\mathcal{D}z_{1}\\mathcal{D}z_{2}\\left(z_{1}-\\frac{c^{*}}{\\sqrt{1-(c^{*})^{2}}}z_{2}\\right)\\phi(u_{1}^{*})\\phi^{\\prime}(u_{2}^{*})=E_{\\beta,\\beta^{\\prime}}\\underbrace{\\sigma^{2}\\int\\mathcal{D}z_{1}\\mathcal{D}z_{2}\\phi^{\\prime}(u_{1}^{*})\\phi^{\\prime}(u_{2}^{*})}_{=\\chi_{c}.}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Utilizing the identity, $\\begin{array}{r}{\\int\\mathcal{D}z z f(z)=\\int\\mathcal{D}z f^{\\prime}(z)}\\end{array}$ , we obtain the third term as follows. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma^{2}\\displaystyle\\int D z_{1}D z_{2}\\frac{1}{2}\\frac{E_{\\beta,\\beta}}{\\sqrt{q^{*}}}(c^{*}z_{1}+\\sqrt{1-(c^{*})^{2}}z_{2})\\phi(u_{1}^{*})\\phi^{\\prime}(u_{2}^{*})}\\\\ &{\\displaystyle=\\sigma^{2}\\frac{1}{2}E_{\\beta,\\beta}\\displaystyle\\int\\mathcal{D}z_{1}\\mathcal{D}z_{2}\\left(c^{*}\\left(\\phi^{\\prime}(u_{1}^{*})\\phi^{\\prime}(u_{2}^{*})+c^{*}\\phi(u_{1}^{*})\\phi^{\\prime\\prime}(u_{2}^{*})\\right)+(1-(c^{*})^{2})\\phi(u_{1}^{*})\\phi^{\\prime\\prime}(u_{2}^{*})\\right)}\\\\ &{\\displaystyle=\\sigma^{2}\\frac{1}{2}E_{\\beta,\\beta}\\displaystyle\\int\\mathcal{D}z_{1}\\mathcal{D}z_{2}\\left(c^{*}\\phi^{\\prime}(u_{1}^{*})\\phi^{\\prime}(u_{2}^{*})+\\phi(u_{1}^{*})\\phi^{\\prime\\prime}(u_{2}^{*})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The last term is calculated as follows. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sigma^{2}\\int\\mathcal{D}z_{1}\\mathcal{D}z_{2}\\frac{1}{2}\\frac{E_{\\beta,\\beta}}{\\sqrt{q^{*}}}z_{1}\\phi^{\\prime}(u_{1}^{*})\\phi(u_{2}^{*})=\\sigma^{2}\\frac{1}{2}E_{\\beta,\\beta}\\int\\mathcal{D}z_{1}\\mathcal{D}z_{2}(\\phi^{\\prime\\prime}(u_{1}^{*})\\phi(u_{2}^{*})+c^{*}\\phi^{\\prime}(u_{1}^{*})\\phi^{\\prime}(u_{2}^{*})).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Summing the last two terms, we obtain the term $E_{\\beta,\\beta}\\chi_{\\kappa}$ ", "page_idx": 17}, {"type": "text", "text": "Next, we consider the case where $c^{*}=1$ . As with the discussion of (Schoenholz et al., 2016), the perturbd corelationisdefined by $\\begin{array}{r}{c_{\\beta,\\beta^{\\prime}}=c^{*}-\\frac{E_{\\beta,\\beta^{\\prime}}}{q}}\\end{array}$ where $E_{\\beta,\\beta^{\\prime}}>0$ and then $\\phi(u_{2})$ is expanded as follows. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{\\Sigma}^{\\mathrm{5}}(u_{2})=\\phi(u_{2}^{*})+\\left(\\sqrt{2\\frac{E_{\\beta,\\beta^{\\prime}}}{q^{*}}}z_{2}-\\frac{E_{\\beta,\\beta^{\\prime}}}{\\sqrt{q^{*}}}z_{1}\\right)\\phi^{\\prime}(u_{2}^{*})+E_{\\beta,\\beta^{\\prime}}z_{2}^{2}\\phi^{\\prime\\prime}(u_{2}^{*})+\\frac{E_{\\beta,\\beta}}{2\\sqrt{q^{*}}}z_{1}\\phi^{\\prime}(u_{2}^{*})+\\mathcal{O}(|E_{\\beta,\\beta}|^{2}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma^{2}\\displaystyle\\int\\mathcal{D}z_{1}\\mathcal{D}z_{2}\\phi(u_{1})\\phi(u_{2})+\\sigma_{b}^{2}}\\\\ &{=\\underbrace{\\sigma^{2}\\displaystyle\\int\\mathcal{D}z_{1}\\mathcal{D}z_{2}\\phi(u_{1}^{*})\\phi(u_{2}^{*})+\\sigma_{b}^{2}}_{=q^{*}c^{*}}+\\sigma^{2}\\displaystyle\\int\\mathcal{D}z_{1}\\mathcal{D}z_{2}\\sqrt{2\\frac{E_{\\beta,\\beta^{\\prime}}}{q^{*}}}z_{2}\\phi(u_{1}^{*})\\phi^{\\prime}(u_{2}^{*})}\\\\ &{\\quad-\\sigma^{2}\\displaystyle\\int\\mathcal{D}z_{1}\\mathcal{D}z_{2}\\frac{E_{\\beta,\\beta^{\\prime}}}{\\sqrt{q^{*}}}z_{1}\\phi(u_{1}^{*})\\phi^{\\prime}(u_{2}^{*})+\\sigma^{2}\\displaystyle\\int\\mathcal{D}z_{1}\\mathcal{D}z_{2}E_{\\beta,\\beta^{\\prime}}z_{2}^{2}\\phi(u_{1}^{*})\\phi^{\\prime\\prime}(u_{2}^{*})}\\\\ &{\\quad+\\sigma^{2}\\displaystyle\\int\\mathcal{D}z_{1}\\mathcal{D}z_{2}\\frac{E_{\\beta,\\beta^{\\prime}}}{2\\sqrt{q^{*}}}z_{1}\\phi(u_{1}^{*})\\phi^{\\prime}(u_{2}^{*})+\\sigma^{2}\\displaystyle\\int\\mathcal{D}z_{1}\\mathcal{D}z_{2}\\frac{1}{2}\\frac{E_{\\beta,\\beta}}{\\sqrt{q^{*}}}z_{1}\\phi^{\\prime}(u_{1}^{*})\\phi(u_{2}^{*})+\\mathcal{O}(|E_{\\beta,\\beta}^{3/2}|).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Using the fact that $u_{2}^{*}=u_{1}^{*}$ and $u_{2}^{*}$ is independent of $z_{2}$ \uff0c ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int D z_{1}D_{2}\\bigg[\\frac{E_{3,B^{\\prime}}}{2}z_{0}^{2}\\Big(u_{1}^{3}\\Big)\\bar{\\phi}^{\\prime}(u_{2}^{1})=\\sigma^{2}\\int D z_{1}\\bigg[\\frac{E_{3,B^{\\prime}}}{2}\\bar{\\phi}^{\\prime}(u_{1}^{1})\\bar{\\phi}^{\\prime}(u_{2}^{1})\\bigg(\\underbrace{\\int\\mathcal{D}z_{2,2}}_{\\infty}\\bigg)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad\\sigma^{2}\\int D z_{1}\\frac{E_{3,B^{\\prime}}}{2}z_{0}^{2}\\big(u_{1}^{3}\\big)\\bar{\\phi}^{\\prime}(u_{1}^{2})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad-\\sigma^{2}E_{3,B^{\\prime}}\\int D z_{1}(\\bar{\\phi}^{\\prime}(u_{1}^{2})^{2}+\\bar{\\phi}(u_{1}^{3})\\bar{\\phi}^{\\prime}(u_{1}^{4}))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\sigma^{2}\\int D z_{1}(\\bar{\\phi}^{\\prime}(u_{1}^{2})^{2}+\\bar{\\phi}(u_{1}^{3})\\bar{\\phi}^{\\prime}(u_{1}^{4}))\\,.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\sigma^{2}\\int D z_{1}D\\bar{\\phi}(u_{1}^{3})\\bar{\\phi}^{\\prime}(u_{1}^{2})\\underbrace{\\int\\mathcal{D}z_{2,2}\\bar{\\phi}(u_{1}^{2})}_{=\\bar{1}}\\,.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\sigma^{2}\\int D z_{1}\\frac{E_{3,B^{\\prime}}}{2}z_{0}^{2}\\big(u_{1}^{3}\\big)\\bar{\\phi}^{\\prime}(u_{1}^{4})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\sigma^{2}\\frac{E_{3,B^{\\prime}}}{2}\\int D z_{1}(\\bar{\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Substituting these facts into Eq. (21), we obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma^{2}\\displaystyle\\int\\mathcal{D}z_{1}\\mathcal{D}z_{2}\\phi(u_{1})\\phi(u_{2})+\\sigma_{b}^{2}}\\\\ &{\\approx q^{*}c^{*}-\\sigma^{2}E_{\\beta,\\beta^{\\prime}}\\displaystyle\\int\\mathcal{D}z_{1}\\phi^{\\prime}(u_{1}^{*})^{2}+\\sigma^{2}E_{\\beta,\\beta}\\displaystyle\\int\\mathcal{D}z_{1}(\\phi^{\\prime}(u_{1}^{*})^{2}+\\phi(u_{1}^{*})\\phi^{\\prime\\prime}(u_{1}^{*}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The above result agrees with that obtained by substituting $c^{*}=1$ for the result obtained when the case $c^{*}<1$ ", "page_idx": 18}, {"type": "text", "text": "Lemma A.2. The Jacobian matrix $J(\\pmb{\\Sigma}^{*})$ of the iterated map $\\mathcal{C}$ defined in Eq. (14) is obtained as follows. ", "page_idx": 18}, {"type": "equation", "text": "$$\nJ(\\Sigma^{*})\\big]_{(\\alpha,\\alpha^{\\prime}),(\\beta,\\beta^{\\prime})}=\\frac{1}{N^{2}}\\sum_{k^{\\prime}=0}^{K-1}c_{k^{\\prime}}\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k^{\\prime})}\\right)\\cos\\left(\\theta_{\\beta,\\beta^{\\prime}}^{(k^{\\prime})}\\right)(\\delta_{\\beta,\\beta^{\\prime}}(\\chi_{q^{*}}-\\chi_{\\kappa}+\\delta_{k^{\\prime},0}N\\chi_{\\kappa})+(1-\\delta_{\\beta,0}N\\chi_{\\kappa})^{2})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Furthermore, the rank of the Jacobian matrix $J(\\Sigma^{*})\\in\\mathbb{R}^{N^{2}\\times N^{2}}$ is at most $K$ ", "page_idx": 18}, {"type": "text", "text": "Proof. Let some semi-positive definite matrix ${\\pmb E}\\in\\mathbb{R}^{N\\times N}$ be a deviation from the fixed point $\\Sigma^{*}$ From Lemma 3.1 and Lemma A.1, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\mathcal{C}(\\Sigma^{*}+E)]_{\\alpha,\\alpha^{\\prime}}}}\\\\ {{\\displaystyle=\\frac{1}{N^{2}}\\sum_{k^{\\prime}=0}^{K-1}c_{k^{\\prime}}\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k^{\\prime})}\\right)\\sum_{\\beta,\\beta^{\\prime}=0}^{N-1}\\cos\\left(\\theta_{\\beta,\\beta^{\\prime}}^{(k^{\\prime})}\\right)\\left(\\sigma^{2}\\mathbb{E}_{{\\bf H}_{\\cdot},d\\sim N(0,\\Sigma^{*}+E)}\\left[\\phi(H_{\\gamma,d})\\phi(H_{\\gamma^{\\prime},d})\\right]+\\sigma_{b}^{2}\\right)}}\\\\ {{\\displaystyle\\approx\\frac{1}{N^{2}}\\sum_{k^{\\prime}=0}^{K-1}c_{k^{\\prime}}\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k^{\\prime})}\\right)\\sum_{\\beta,\\beta^{\\prime}=0}^{N-1}\\cos\\left(\\theta_{\\beta,\\beta^{\\prime}}^{(k^{\\prime})}\\right)\\left(\\delta_{\\beta,\\beta^{\\prime}}(q^{*}+E_{\\beta,\\beta\\chi_{q^{\\prime}}})+(1-\\delta_{\\beta,\\beta^{\\prime}})(q^{*}c^{*}+E_{\\beta,\\beta\\chi_{\\kappa}}+1-\\delta_{\\beta,\\beta^{\\prime}})\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that Eq. (22) is obtained by neglecting higher order terms in Eqs. (19) and (20) ", "page_idx": 19}, {"type": "text", "text": "By the definition of the fixed point ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Sigma_{\\alpha,\\alpha^{\\prime}}^{*}=\\frac{1}{N^{2}}\\sum_{k^{\\prime}=0}^{K-1}c_{k^{\\prime}}\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k^{\\prime})}\\right)\\sum_{\\beta,\\beta^{\\prime}=0}^{N-1}\\cos\\left(\\theta_{\\beta,\\beta^{\\prime}}^{(k^{\\prime})}\\right)q^{*}(\\delta_{\\beta,\\beta^{\\prime}}+(1-\\delta_{\\beta,\\beta^{\\prime}})c^{*}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By substituting the fact into Eq. (22), we obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{C}(\\boldsymbol{\\Sigma}^{*}+\\boldsymbol{E})-\\boldsymbol{\\Sigma}^{*}\\big]_{\\alpha,\\alpha^{\\prime}}}\\\\ &{\\approx\\displaystyle\\frac{1}{N^{2}}\\sum_{k^{\\prime}=0}^{K-1}c_{k^{\\prime}}\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k^{\\prime})}\\right)\\sum_{\\beta,\\beta^{\\prime}=0}^{N-1}\\cos\\left(\\theta_{\\beta,\\beta^{\\prime}}^{(k^{\\prime})}\\right)(\\delta_{\\beta,\\beta^{\\prime}}E_{\\beta,\\beta}\\chi_{\\alpha^{*}})+(1-\\delta_{\\beta,\\beta^{\\prime}})(E_{\\beta,\\beta}\\chi_{\\kappa}+E_{\\beta,\\beta^{\\prime}}\\chi_{\\ c_{*}}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n=\\sum_{\\beta,\\beta^{\\prime}=0}^{N-1}\\underbrace{\\left[\\frac{1}{N^{2}}\\sum_{k^{\\prime}=0}^{K-1}c_{k^{\\prime}}\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k^{\\prime})}\\right)\\cos\\left(\\theta_{\\beta,\\beta^{\\prime}}^{(k^{\\prime})}\\right)\\left(\\delta_{\\beta,\\beta^{\\prime}}\\left(\\chi_{q^{*}}-\\chi_{\\kappa}+\\delta_{k^{\\prime},0}N\\chi_{\\kappa}\\right)+(1-\\delta_{\\beta,\\beta^{\\prime}})\\chi_{\\ c_{*}}\\right)\\right)}_{=\\left[J(\\Sigma^{*})\\right]_{\\left(\\alpha,\\alpha^{\\prime}\\right),(\\beta,\\beta^{\\prime})}}E_{\\L}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The last equation can be rewritten using the matrix calculation as follows. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathscr{C}(\\Sigma^{*}+E)-\\Sigma^{*}\\approx\\mathrm{mat}(J(\\Sigma^{*})\\,\\mathrm{vec}(E)).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since $J(\\pmb{\\Sigma}^{*})$ is the first-order coefficient to the deviaction $\\pmb{E}$ $J(\\pmb{\\Sigma}^{\\ast})$ is exactly the Jacobian matrix of the iterated map $\\mathcal{C}$ ", "page_idx": 19}, {"type": "text", "text": "Furthermore, the Jacobian matrix can be decomposed to two matricies $\\pmb{A}\\in\\mathbb{R}^{N^{2}\\times K}$ and $\\boldsymbol{B}\\in\\mathbb{R}^{K\\times N^{2}}$ as follows. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\displaystyle[J(\\Sigma^{*})]_{(\\alpha,\\alpha^{\\prime}),(\\beta,\\beta^{\\prime})}=\\sum_{k^{\\prime}=0}^{K-1}A_{(\\alpha,\\alpha^{\\prime}),k^{\\prime}}B_{k^{\\prime},(\\beta,\\beta^{\\prime})},\\ A_{(\\alpha,\\alpha^{\\prime}),k^{\\prime}}:=\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k^{\\prime})}\\right),}\\\\ &{}&{\\displaystyle B_{k^{\\prime},(\\beta,\\beta^{\\prime})}:=\\frac{1}{N^{2}}c_{k^{\\prime}}\\cos\\left(\\theta_{\\beta,\\beta^{\\prime}}^{(k^{\\prime})}\\right)\\left(\\delta_{\\beta,\\beta^{\\prime}}(\\chi_{q^{*}}-\\chi_{\\kappa}+\\delta_{k^{\\prime},0}N\\chi_{\\kappa})+(1-\\delta_{\\beta,\\beta^{\\prime}})\\chi_{c_{*}}\\right)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, the rank of the Jacobian matrix is at most $K$ ", "page_idx": 19}, {"type": "text", "text": "Lemma A.3. Let $\\pmb{\\Sigma}^{*}$ be the fixed point of the form in Eq. (5) and $E^{(k)}$ be the perturbation expressed as ", "page_idx": 19}, {"type": "equation", "text": "$$\nE_{\\beta,\\beta^{\\prime}}^{(k)}=\\epsilon_{k}\\psi_{\\beta,\\beta^{\\prime}}^{(k)},\\quad\\psi_{\\beta,\\beta^{\\prime}}^{(k)}=\\left[\\cos\\left(\\theta_{\\beta,\\beta^{\\prime}}^{(k)}\\right)-\\left(\\sum_{s=0}^{K-1}c_{s}\\right)^{-1}\\sum_{s=0}^{K-1}c_{s}\\cos\\left(\\theta_{\\beta,\\beta^{\\prime}}^{(s)}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\epsilon_{k}$ denotes the scale of the perturbation, assumed to be sufficiently small. ", "page_idx": 19}, {"type": "text", "text": "Then, we have, for all $k\\in[K]\\backslash\\{0\\}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\mathcal{C}}(\\Sigma^{*}+E^{(k)})\\approx\\Sigma^{*}+\\chi_{c^{*}}E^{(k)}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. From Eq. (23), we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathcal{C}(\\Sigma^{*}+E^{(k)})\\big]_{\\alpha,\\alpha^{\\prime}}\\approx\\frac{1}{N^{2}}\\sum_{k^{\\prime}=0}^{K-1}c_{k^{\\prime}}\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k^{\\prime})}\\right)\\sum_{\\beta=0}^{N-1}(q^{*}+E_{\\beta,\\beta}^{(k)}\\chi_{q^{*}})}\\\\ {\\displaystyle\\qquad\\qquad\\qquad+\\,\\frac{1}{N^{2}}\\sum_{k^{\\prime}=0}^{K-1}c_{k^{\\prime}}\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k^{\\prime})}\\right)\\sum_{\\beta=0}^{N-1}\\sum_{\\beta\\neq\\beta^{\\prime}}\\cos\\left(\\theta_{\\beta,\\beta^{\\prime}}^{(k^{\\prime})}\\right)(q^{*}c^{*}+E_{\\beta,\\beta}^{(k)}\\chi_{\\kappa}+E_{\\beta,\\beta^{\\prime}}^{(k)}\\chi_{\\kappa})}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "From the definition of the fixed point, we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Sigma_{\\alpha,\\alpha^{\\prime}}^{*}=\\frac{1}{N^{2}}\\sum_{k^{\\prime}=0}^{K-1}c_{k^{\\prime}}\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k^{\\prime})}\\right)\\sum_{\\beta,\\beta^{\\prime}=0}^{N-1}\\cos\\left(\\theta_{\\beta,\\beta^{\\prime}}^{(k^{\\prime})}\\right)q^{*}(\\delta_{\\beta,\\beta^{\\prime}}+(1-\\delta_{\\beta,\\beta^{\\prime}})c^{*}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By combining the above two results, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{[\\mathcal{C}(\\Sigma^{*}+E^{(k)})-\\Sigma^{*}]_{\\alpha,\\alpha^{\\prime}}\\approx\\displaystyle\\frac{1}{N^{2}}\\sum_{k^{\\prime}=0}^{K-1}c_{k^{\\prime}}\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k^{\\prime})}\\right)\\sum_{\\beta=0}^{N-1}E_{\\beta,\\beta}^{(k)}\\chi_{q^{*}}}\\\\ &{\\quad\\quad+\\displaystyle\\frac{1}{N^{2}}\\sum_{k^{\\prime}=0}^{K-1}c_{k^{\\prime}}\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k^{\\prime})}\\right)\\sum_{\\beta=0}^{N-1}\\sum_{\\beta\\neq\\beta^{\\prime}}\\cos\\left(\\theta_{\\beta,\\beta^{\\prime}}^{(k^{\\prime})}\\right)(E_{\\beta,\\beta}^{(k)}\\chi_{\\kappa}+E_{\\beta,\\beta^{\\prime}}^{(k)}\\chi_{c^{*}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since cos(0) = 1 for ll k [K], we have .3a $\\psi_{\\beta,\\beta}^{(k)}=0$ and $E_{\\beta,\\beta}^{(k)}=0$ . Thus, only the term with $E_{\\beta,\\beta^{\\prime}}^{(k)}$ remains. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{[\\mathcal{C}(\\Sigma^{*}+E^{(k)})-\\Sigma^{*}]_{\\alpha,\\alpha^{\\prime}}\\approx\\epsilon_{k}\\chi_{c^{*}}\\frac{1}{N^{2}}\\left(\\displaystyle\\sum_{k^{\\prime}=0}^{K-1}c_{k^{\\prime}}\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k^{\\prime})}\\right)\\sum_{\\beta\\neq\\beta^{\\prime}}\\cos\\left(\\theta_{\\beta,\\beta^{\\prime}}^{(k^{\\prime})}\\right)\\cos\\left(\\theta_{\\beta,\\beta^{\\prime}}^{(k)}\\right)\\right.\\right.\\left.(24)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left.-\\left(\\displaystyle\\sum_{s=0}^{K-1}c_{s}\\right)^{-1}\\displaystyle\\sum_{k^{\\prime}=0}^{K-1}c_{k^{\\prime}}\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k^{\\prime})}\\right)\\sum_{\\beta\\neq\\beta^{\\prime}}\\cos\\left(\\theta_{\\beta,\\beta^{\\prime}}^{(k^{\\prime})}\\right)\\sum_{s=0}^{K-1}c_{s}\\cos\\left(\\theta_{\\beta,\\beta^{\\prime}}^{(k^{\\prime})}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Given the orthogonality of the cosine functions, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\sum_{\\beta,\\beta^{\\prime}=0}^{N-1}c_{k^{\\prime}}\\cos\\Big(\\theta_{\\beta,\\beta^{\\prime}}^{(k^{\\prime})}\\Big)\\cos\\Big(\\theta_{\\beta,\\beta^{\\prime}}^{(k)}\\Big)=\\left\\{\\begin{array}{c c}{N^{2}-c_{k^{\\prime}}N}&{(k^{\\prime}=k)}\\\\ {-c_{k^{\\prime}}N}&{(\\mathrm{otherwise}).}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Utilizing Eq. (26) leads to the following two facts: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\sum_{k^{\\prime}=0}^{K-1}c_{k^{\\prime}}\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k^{\\prime})}\\right)\\sum_{\\beta\\neq\\beta^{\\prime}}\\cos\\left(\\theta_{\\beta,\\beta^{\\prime}}^{(k^{\\prime})}\\right)\\cos\\left(\\theta_{\\beta,\\beta^{\\prime}}^{(k)}\\right)=N^{2}\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k)}\\right)-N\\sum_{k^{\\prime}=0}^{K-1}c_{k^{\\prime}}\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k^{\\prime})}\\right)}}\\\\ {{\\displaystyle\\sum_{\\gamma=0}^{c-1}c_{k^{\\prime}}\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k^{\\prime})}\\right)\\sum_{\\beta\\neq\\beta^{\\prime}}\\cos\\left(\\theta_{\\beta,\\beta^{\\prime}}^{(k^{\\prime})}\\right)\\sum_{s=0}^{K-1}c_{s}\\cos\\left(\\theta_{\\beta,\\beta^{\\prime}}^{(s)}\\right)=N^{2}\\sum_{k^{\\prime}=0}^{K-1}\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k^{\\prime})}\\right)-N\\left(\\sum_{s=0}^{K-1}c_{s}\\right)\\sum_{k^{\\prime}=0}^{K-1}c_{k^{\\prime}}\\sin\\left(\\theta_{\\beta,\\beta^{\\prime}}^{(k^{\\prime})}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By substituting these facts into Eq. (24), we obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{[\\mathcal{C}(\\Sigma^{*}+E^{(k)})-\\Sigma^{*}]_{\\alpha,\\alpha^{\\prime}}}\\\\ &{\\approx\\chi_{c^{*}}\\epsilon_{k}\\frac{1}{N^{2}}\\left(N^{2}\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k)}\\right)-N\\displaystyle\\sum_{-k^{\\prime}=0}^{K-1}c_{k^{\\prime}\\exp}\\widetilde{\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k^{\\prime})}\\right)}\\right.}\\\\ &{\\qquad\\left.-\\left(\\displaystyle\\sum_{s=0}^{K-1}c_{s}\\right)^{-1}N^{2}\\displaystyle\\sum_{k^{\\prime}=0}^{K-1}\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k^{\\prime})}\\right)+N\\displaystyle\\sum_{-k^{\\prime}=0}^{K-1}c_{k^{\\prime}}\\mathrm{exes}\\widetilde{\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k^{\\prime})}\\right)}\\right)}\\\\ &{=\\chi_{c^{*}}\\epsilon_{k}\\left[\\displaystyle\\sum_{s=0}^{}\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k)}\\right)-\\left(\\displaystyle\\sum_{s=0}^{K-1}c_{s}\\right)^{-1}\\displaystyle\\sum_{k^{\\prime}=0}^{K-1}\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k^{\\prime})}\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This completes the proof. ", "page_idx": 21}, {"type": "text", "text": "Lemma A.4. Let $\\pmb{\\Sigma}^{*}$ be the fixed point of the form in Eq. (5) and $\\pmb{E}$ be the perturbation expressed as ", "page_idx": 21}, {"type": "equation", "text": "$$\nE_{\\beta,\\beta^{\\prime}}=\\epsilon\\psi_{\\beta,\\beta^{\\prime}},\\;\\psi_{\\beta,\\beta^{\\prime}}:=\\left[1-\\frac{1}{N}\\left(\\frac{\\chi_{\\kappa}+\\chi_{c^{*}}-\\chi_{q^{*}}}{\\chi_{\\kappa}}\\right)\\sum_{s=0}^{K-1}c_{s}\\cos\\left(\\theta_{\\beta,\\beta^{\\prime}}^{(s)}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where e denotes the scale of the perturbation, assumed to be suffciently small, $\\mathbf{1}_{N}\\mathbf{1}_{N}^{\\top}$ is all-one matrixwithsize $N\\times N$ ", "page_idx": 21}, {"type": "text", "text": "Then, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{C}(\\Sigma^{*}+E)\\approx\\Sigma^{*}+\\underbrace{\\left(\\frac{\\sum_{s=0}^{K-1}c_{s}}{N}\\chi_{q^{*}}+\\left(1-\\frac{\\sum_{s=0}^{K-1}c_{s}}{N}\\right)\\left(\\chi_{\\kappa}+\\chi_{c^{*}}\\right)\\right)}_{:=\\chi}E.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "ProfFr smliy we rode $\\begin{array}{r}{x:=\\frac{1}{N}\\left(\\frac{\\chi_{\\kappa}+\\chi_{c^{*}}-\\chi_{q^{*}}}{\\chi_{\\kappa}}\\right)\\left(\\sum_{s=0}^{K-1}c_{s}\\right)}\\end{array}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{C}(\\Sigma^{*}+E)\\approx\\Sigma^{*}+\\left(\\frac{\\sum_{s=0}^{K-1}c_{s}}{N}\\chi_{q^{*}}+\\left(1-\\frac{\\sum_{s=0}^{K-1}c_{s}}{N}\\right)\\left(\\chi_{\\kappa}+\\chi_{c^{*}}\\right)\\right)E}\\\\ &{\\quad\\quad=\\Sigma^{*}+\\left[\\left(1-\\underbrace{\\frac{1}{N}\\left(\\frac{\\chi_{\\kappa}+\\chi_{c^{*}}-\\chi_{q^{*}}}{\\chi_{\\kappa}}\\right)\\left(\\displaystyle\\sum_{s=0}^{K-1}c_{s}\\right)}_{=:x}\\right)\\chi_{\\kappa}+\\chi_{c^{*}}\\right]E.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "From Eq. (23), we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{[\\mathcal{C}(\\Sigma^{*}+E)-\\Sigma^{*}]_{\\alpha,\\alpha^{\\prime}}}\\\\ &{\\approx\\displaystyle\\frac{1}{N^{2}}\\sum_{k^{\\prime}=0}^{K-1}c_{k^{\\prime}}\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k^{\\prime})}\\right)\\sum_{\\beta=0}^{N-1}\\left(E_{\\beta,\\beta\\chi_{q^{*}}}+\\sum_{\\beta\\neq\\beta^{\\prime}}\\cos\\left(\\theta_{\\beta,\\beta^{\\prime}}^{(k^{\\prime})}\\right)(E_{\\beta,\\beta\\chi_{\\kappa}}+E_{\\beta,\\beta^{\\prime}}\\chi_{c^{*}})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Our goal is to derive Eq. (27) from Eq. (28). The following results are useful for the computation of each term of Eq. (28). ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\forall\\beta,\\beta^{\\prime}\\in[N],\\ \\beta\\neq\\beta^{\\prime},\\quad E_{\\beta,\\beta}=\\epsilon(1-x),\\quad E_{\\beta,\\beta^{\\prime}}=\\epsilon\\left(1-x\\left(\\displaystyle\\sum_{s=0}^{K-1}c_{s}\\right)^{-1}\\displaystyle\\sum_{s=0}^{K-1}c_{s}\\cos\\left(\\theta_{\\beta,\\beta^{\\prime}}^{(s)}\\right)\\right)}}\\\\ {{\\displaystyle\\sum_{\\beta=0}^{N-1}\\sum_{\\beta\\neq\\beta^{\\prime}}\\cos\\left(\\theta_{\\beta,\\beta^{\\prime}}^{(k^{\\prime})}\\right)=\\left\\{\\begin{array}{c c}{{N^{2}-N}}&{{(k^{\\prime}=0)}}\\\\ {{-N}}&{{(\\mathrm{otherwise})}}\\end{array}\\right..}}\\\\ {{\\displaystyle\\sum_{\\beta=0}^{N-1}c_{k^{\\prime}}\\cos\\left(\\theta_{\\beta,\\beta^{\\prime}}^{(k^{\\prime})}\\right)\\cos\\left(\\theta_{\\beta,\\beta^{\\prime}}^{(k)}\\right)=\\left\\{\\begin{array}{c c}{{N^{2}-c_{k^{\\prime}}N}}&{{(k^{\\prime}=k)}}\\\\ {{-c_{k^{\\prime}}N}}&{{(\\mathrm{otherwise})}}\\end{array}\\right..}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Using the above results, the first and second terms of Eq. (28) are calculated as follows. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\frac{1}{N^{2}}\\sum_{k^{\\prime}=0}^{K-1}c_{k^{\\prime}}\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k^{\\prime})}\\right)\\sum_{\\beta=0}^{N-1}E_{\\beta,\\beta}\\chi_{q^{*}}=\\epsilon(1-x)\\chi_{q^{*}}\\frac{1}{N}\\sum_{k^{\\prime}=0}^{K-1}c_{k^{\\prime}}\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k^{\\prime})}\\right).}}\\\\ {\\displaystyle\\frac1{N^{2}}\\sum_{k^{\\prime}=0}^{K-1}c_{k^{\\prime}}\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k^{\\prime})}\\right)\\sum_{\\beta=0}^{N-1}\\sum_{\\beta\\neq\\beta^{\\prime}}\\cos\\left(\\theta_{\\beta,\\beta^{\\prime}}^{(k^{\\prime})}\\right)E_{\\beta,\\beta}\\chi_{\\kappa}=\\epsilon(1-x)\\chi_{\\kappa}-\\epsilon(1-x)\\chi_{\\kappa}\\frac{1}{N}\\sum_{k^{\\prime}=0}^{K-1}c_{k^{\\prime}}\\cos\\left(\\theta_{\\alpha,\\beta^{\\prime}}^{(k^{\\prime})}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The third term of Eq. (28) is obtained as follows. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{N^{2}}\\sum_{k=0}^{N-1}\\alpha_{k}\\cos\\left(\\theta_{s,k}^{(k)}\\right)_{s,0}^{N-1}\\sum_{s=0}^{N-1}\\cos\\left(\\theta_{s,s}^{(k)}\\right)E_{s,s}\\gamma_{k},}\\\\ {\\displaystyle=\\frac{1}{N^{2}}\\sum_{k=0}^{N-1}\\alpha_{k}\\cos\\left(\\theta_{s,k}^{(k)}\\right)_{s,0}^{N-1}\\sum_{s=0}^{N-1}\\cos\\left(\\theta_{s,s}^{(k)}\\right)_{s,s}^{\\prime}\\gamma_{k},}\\\\ {\\displaystyle\\quad-\\left(\\sum_{s=0}^{N-1}\\epsilon_{s,s}\\right)^{-1}\\frac{1}{N^{2}}\\sum_{k=0}^{N-1}\\epsilon_{s,0}\\cos\\left(\\theta_{s,k}^{(k)}\\right)_{s,0}^{N-1}\\sum_{s=0}^{N-1}\\sum_{s=0}^{\\Gamma}\\cos\\left(\\theta_{s,s}^{(k)}\\right)\\cos\\left(\\theta_{s,s}^{(k)}\\right)\\alpha_{k}-}\\\\ {\\displaystyle=c_{k}\\epsilon_{s}-c_{k}\\epsilon_{s}^{-1}\\frac{1}{N}\\sum_{k=0}^{N-1}c_{k}\\cos\\left(\\theta_{s,s}^{(k)}\\right)}\\\\ {\\displaystyle\\quad-c_{k}\\epsilon_{s}\\sum_{s=0}^{N-1}\\sum_{s=0}^{N-1}\\epsilon_{s,0}\\cos\\left(\\theta_{s,s}^{(k)}\\right)+c_{k}\\epsilon_{s}\\frac{1}{N}\\sum_{k=0}^{N-1}c_{k}\\cos\\left(\\theta_{s,s}^{(k)}\\right)}\\\\ {\\displaystyle=c_{k}\\epsilon_{s}-c_{k}\\epsilon_{s}\\left(\\sum_{s=0}^{N-1}\\epsilon_{s,s}\\right)^{-1}\\sum_{s=0}^{N-1}c_{k}\\cos\\left(\\theta_{s,s}^{(k)}\\right)-c_{k}(1-x)\\epsilon_{s,0}^{-1}\\sum_{s=0}^{N-1}c_{k}\\cos\\left(\\theta_{s,s}^{(k)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By substituting these facts into Eq. (28), we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{C}(\\Sigma^{*}+E)-\\Sigma^{*}\\Big|_{\\alpha,\\alpha^{\\prime}}}\\\\ &{\\approx\\epsilon((1-x)\\chi_{\\kappa}+\\chi_{c^{*}})-\\epsilon\\chi_{c^{*}}x\\left(\\displaystyle\\sum_{s=0}^{K-1}c_{s}\\right)^{-1}\\displaystyle\\sum_{k^{\\prime}=0}^{K-1}c_{k^{\\prime}}\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k^{\\prime})}\\right)}\\\\ &{\\quad-\\epsilon(1-x)\\displaystyle\\frac{1}{\\Omega}\\left(\\chi_{\\kappa}+\\chi_{c^{*}}-\\chi_{\\kappa^{\\prime}}\\right)\\displaystyle\\sum_{k^{\\prime}=0}^{K-1}c_{k^{\\prime}}\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k^{\\prime})}\\right)}\\\\ &{=((1-x)\\chi_{\\kappa}+\\chi_{c^{*}})\\underbrace{\\epsilon\\left(1-x\\left(\\displaystyle\\sum_{s=0}^{K-1}c_{s}\\right)^{-1}\\displaystyle\\sum_{k^{\\prime}=0}^{K-1}c_{k^{\\prime}}\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k^{\\prime})}\\right)\\right)}_{=E_{0,\\alpha^{\\prime}}}}\\end{array}\n$$$\\scriptstyle=E_{\\alpha,\\alpha^{\\prime}}$ ", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Finally, we obtain Eq. (27). ", "page_idx": 22}, {"type": "text", "text": "BProof of Theorem 3.5 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Theorem 3.5 (Trainability). Let $\\tilde{\\Sigma}^{(\\ell)}\\in\\mathbb{R}^{N\\times N}$ be the gradient covariance with respect to some loss $\\mathcal{L}_{g}$ .e.g. mean squared error, at the $\\ell$ -th layer. Suppose that the gradient covariance at the L-th layer is decomposed as \u2265) $\\begin{array}{r}{\\tilde{\\Sigma}_{\\alpha,\\alpha^{\\prime}}^{(L)}=\\sum_{k=0}^{K-1}\\tilde{\\epsilon}_{k}\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k)}\\right)+\\tilde{e}}\\end{array}$ (oQo\u03b1s ) + e, where Ek is the coefcient o each basis and e $\\{\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k)}\\right)\\}_{k=0}^{K-1})$ Then, thg radien covarance ", "page_idx": 22}, {"type": "text", "text": "at the -th layer is obtained by ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\tilde{\\Sigma}_{\\alpha,\\alpha^{\\prime}}^{(\\ell)}=\\sum_{k=0}^{K-1}\\chi_{c^{*}}^{L-\\ell}\\tilde{\\epsilon}_{k}\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k)}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Recall the definition of the gradient covariance. We first demonstrate the iterated map of the gradient covariance, starting from this definition. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\tilde{\\Sigma}_{\\alpha,\\alpha^{\\prime}}^{(\\ell)}:=\\mathbb{E}_{\\Theta^{\\ell;L},\\Xi^{\\ell;L}}\\left[g_{\\alpha,j}^{(\\ell)}g_{\\alpha^{\\prime},j}^{(\\ell)}\\right],\\;g_{\\alpha,j}^{(\\ell)}:=\\frac{\\partial\\mathcal{L}}{\\partial H_{\\alpha,j}^{(\\ell)}}=\\sum_{i=1}^{D}\\sum_{\\beta=0}^{N-1}\\frac{\\partial\\mathcal{L}}{\\partial H_{\\beta,i}^{(\\ell+1)}}\\frac{\\partial H_{\\beta,i}^{(\\ell+1)}}{\\partial H_{\\alpha,j}^{(\\ell)}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The Jacobian $\\frac{\\partial H_{\\beta,i}^{(\\ell+1)}}{\\partial H_{\\alpha,j}^{(\\ell)}}$ is calculated as follows. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial H_{\\beta,i}^{(\\ell+1)}}{\\partial H_{\\alpha,j}^{(\\ell)}}}\\\\ &{\\displaystyle=\\sum_{k=0}^{K-1}\\sqrt{\\frac{c_{k}}{2}}\\left(F_{\\beta,k}^{\\dag}F_{k,\\alpha}\\left(\\Theta_{j,i}^{(\\ell+1,k)}+\\sqrt{-1}\\Xi_{j,i}^{(\\ell+1,k)}\\right)+\\overline{{F_{\\beta,k}^{\\dag}F_{k,\\alpha}}}\\left(\\Theta_{j,i}^{(\\ell+1,k)}-\\sqrt{-1}\\Xi_{j,i}^{(\\ell+1,k)}\\right)\\right)\\phi^{\\prime}\\left(H_{\\alpha,j}^{(\\ell)}\\right)}\\\\ &{\\displaystyle=\\frac{1}{N}\\sum_{k=1}^{K-1}2\\sqrt{\\frac{c_{k}}{2}}\\left(\\Theta_{j,i}^{(\\ell+1,k)}\\cos\\left(\\theta_{\\beta,\\alpha}^{(k)}\\right)+\\Xi_{j,i}^{(\\ell+1,k)}\\sin\\left(\\theta_{\\beta,\\alpha}^{(k)}\\right)\\right)\\phi^{\\prime}(H_{\\alpha,j}^{(\\ell)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The covariance of Jacobian $\\frac{\\partial H_{\\beta,i}^{(\\ell+1)}}{\\partial H_{\\alpha,j}^{(\\ell)}}$ is as follows. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbb{Q}^{\\ell,L},\\Xi^{\\ell,L}}\\left[\\displaystyle\\frac{\\partial H_{\\beta,i}^{(\\ell+1)}}{\\partial H_{\\alpha,j}^{(\\ell)}}\\frac{\\partial H_{\\beta^{\\prime},i}^{(\\ell+1)}}{\\partial H_{\\alpha^{\\prime},j}^{(\\ell)}}\\right]}\\\\ &{\\,=\\frac{\\sigma^{2}}{N^{2}D}\\mathbb{E}_{\\mathbb{H}_{\\mathfrak{t},\\mathfrak{d}}\\sim N(0,\\Xi^{(\\ell)})}\\left[\\phi^{\\prime}(H_{\\alpha,j}^{(\\ell)})\\phi^{\\prime}(H_{\\alpha,j}^{(\\ell)})\\right]}\\\\ &{\\,\\,\\,\\,\\,\\,\\times\\left(\\displaystyle\\sum_{k=0}^{K-1}c_{k}\\left(\\cos\\left({\\theta}_{\\beta,\\alpha}^{(k)}\\right)\\cos\\left({\\theta}_{\\beta^{\\prime},\\alpha^{\\prime}}^{(k)}\\right)+\\sin\\left({\\theta}_{\\beta,\\alpha}^{(k)}\\right)\\sin\\left({\\theta}_{\\beta^{\\prime},\\alpha^{\\prime}}^{(k)}\\right)\\right)\\right)}\\\\ &{\\,\\,\\,\\,=\\frac{\\sigma^{2}}{N^{2}D}\\mathbb{E}_{\\mathbb{H}_{\\mathfrak{t},\\mathfrak{d}}\\sim N(0,\\Xi^{(\\ell)})}\\left[\\phi^{\\prime}(H_{\\alpha,j}^{(\\ell)})\\phi^{\\prime}(H_{\\alpha,j}^{(\\ell)})\\right]\\left(\\displaystyle\\sum_{k=0}^{K-1}c_{k}\\cos\\left({\\theta}_{\\beta-\\beta^{\\prime},\\alpha-\\alpha^{\\prime}}^{(k)}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then, the covariance of the gradient is given by the following recurrence relation for all $\\alpha,\\alpha^{\\prime}\\in[N]$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\Sigma}_{\\alpha,\\alpha^{\\prime}}^{(\\ell)}:=\\mathbb{E}_{\\Theta^{\\ell}:L}\\underline{{\\varepsilon}}_{\\alpha^{\\ell-1}}\\left[g_{\\alpha,i}^{(\\ell)}g_{\\alpha^{\\prime},j}^{(\\ell)}\\right]}\\\\ &{\\qquad=\\displaystyle\\sum_{i,i^{\\prime}=1}^{D}\\sum_{\\beta,\\beta^{\\prime}=0}^{N-1}\\mathbb{E}_{\\Theta^{\\ell+L},\\Xi^{\\ell-L}}\\left[g_{\\beta,i}^{(\\ell+1)}g_{\\beta^{\\prime},i^{\\prime}}^{(\\ell+1)}\\frac{\\partial H_{\\beta,i}^{(\\ell+1)}}{\\partial H_{\\alpha,j}^{(\\ell)}}\\frac{\\partial H_{\\beta^{\\prime},i^{\\prime}}^{(\\ell+1)}}{\\partial H_{\\alpha^{\\prime},j}^{(\\ell)}}\\right]}\\\\ &{\\qquad=\\displaystyle\\sum_{\\beta,\\beta^{\\prime}=0}^{N-1}\\sum_{i,i^{\\prime}=1}^{D}\\mathbb{E}_{\\Theta^{\\ell+1:L},\\Xi^{\\ell+1:L}}\\left[g_{\\beta,i}^{(\\ell+1)}g_{\\beta^{\\prime},i^{\\prime}}^{(\\ell+1)}\\right]\\mathbb{E}_{\\Theta^{\\ell+L},\\Xi^{\\ell:L}}\\left[\\frac{\\partial H_{\\beta,i}^{(\\ell+1)}}{\\partial H_{\\alpha,j}^{(\\ell)}}\\frac{\\partial H_{\\beta^{\\prime},i^{\\prime}}^{(\\ell+1)}}{\\partial H_{\\alpha^{\\prime},j}^{(\\ell)}}\\right]\\delta_{i,i^{\\prime}}}\\\\ &{\\qquad=\\displaystyle\\frac{\\sigma^{2}}{N^{2}}\\sum_{\\beta,\\beta^{\\prime}=0}^{N-1}\\tilde{\\Sigma}_{\\beta,\\beta^{\\prime}}^{(\\ell+1)}\\mathbb{E}_{\\mathbf{H}_{\\cdot},d\\sim N}(0,\\Omega^{(\\ell)})\\left[\\phi^{\\prime}(H_{\\alpha,j}^{(\\ell)})\\phi^{\\prime}(H_{\\alpha^{\\prime},j}^{(\\ell)})\\right]\\left(\\displaystyle\\sum_{k=0}^{K-1}c_{k}\\cos\\left(\\theta_{\\beta-\\beta^{\\prime},\\alpha-\\alpha^{\\prime}}^{(k)}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "As with (Schoenholz et al., 2016), we approximate $\\boldsymbol{\\Sigma}^{(\\ell+1)}\\approx\\boldsymbol{\\Sigma}^{*}$ since the number of layer $\\ell$ is assumed to be sufficiently large. Then, the linear iterated map of the gradient covariance $\\Sigma^{(\\stackrel{\\cdot}{\\ell}+1)}\\mapsto$ $\\Sigma^{(\\ell)}$ is given as follows. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\tilde{\\Sigma}_{\\alpha,\\alpha^{\\prime}}^{(\\ell)}=\\frac{1}{N^{2}}\\sum_{\\beta,\\beta^{\\prime}=0}^{N-1}\\tilde{\\Sigma}_{\\beta,\\beta^{\\prime}}^{(\\ell+1)}\\chi_{c^{*}}\\left(\\sum_{k=0}^{K-1}c_{k}\\cos\\left(\\theta_{\\beta-\\beta^{\\prime},\\alpha-\\alpha^{\\prime}}^{(k)}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The rank of the linear iterated map $\\Sigma^{(\\ell+1)}\\mapsto\\Sigma^{(\\ell)}$ is less than $K$ since the matrix representation of the linear map can be decomposed into two matrices A E RN2xK and B E RKxN2 as follows. ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\stackrel{\\zeta(\\ell)}{\\sum}_{\\alpha,\\alpha^{\\prime}}^{(\\ell)}=\\frac{1}{N^{2}}\\sum_{k=0}^{K-1}c_{k}\\exp\\left(-\\sqrt{-1}\\theta_{\\beta,\\beta^{\\prime}}^{(k)}\\right)\\exp\\left(\\sqrt{-1}\\theta_{\\alpha^{\\prime},\\alpha}^{(k)}\\right)\\chi_{c^{*}}\\tilde{\\Sigma}_{\\beta,\\beta^{\\prime}}^{(\\ell+1)}=\\left(\\sum_{k=0}^{K-1}\\tilde{A}_{(\\alpha,\\alpha^{\\prime}),k}\\tilde{B}_{k,(\\beta,\\beta^{\\prime})}\\right)\\tilde{\\Sigma}_{\\alpha^{\\prime},\\beta^{\\prime}}^{(\\ell)}}\\\\ {\\tilde{A}_{(\\alpha,\\alpha^{\\prime}),k}:=\\frac{1}{N^{2}}c_{k}\\exp\\left(\\sqrt{-1}\\theta_{\\alpha^{\\prime},\\alpha}^{(k)}\\right)\\chi_{c^{*}},\\ \\tilde{B}_{k,(\\beta,\\beta^{\\prime})}:=\\exp\\left(-\\sqrt{-1}\\theta_{\\beta,\\beta^{\\prime}}^{(k)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Next, we show that the subspae $\\operatorname{span}\\left(\\{\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k)}\\right)\\}_{k=0}^{K-1}\\right)\\ \\subset\\ \\mathbb{R}^{N\\times N}$ is the $K$ dimensonal eigenspace with eigenvalue $\\chi_{c^{*}}$ of the linear iterated map $\\Sigma^{(\\ell+1)}\\mapsto\\Sigma^{(\\ell)}$ ", "page_idx": 24}, {"type": "text", "text": "By substituting $\\begin{array}{r}{\\tilde{\\Sigma}_{\\beta,\\beta^{\\prime}}^{(\\ell+1)}=\\sum_{k=0}^{K-1}\\chi_{c^{*}}^{L-(\\ell+1)}\\tilde{\\epsilon}_{k}\\cos\\left(\\theta_{\\beta,\\beta^{\\prime}}^{(k)}\\right)}\\end{array}$ into Eq. (29), we obtain ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\tilde{\\Sigma}_{\\alpha,\\alpha^{\\prime}}^{(\\ell)}=\\frac{1}{N^{2}}\\sum_{\\beta,\\beta^{\\prime}=0}^{N-1}\\chi_{c^{*}}\\left(\\sum_{k=0}^{K-1}\\tilde{\\epsilon}_{k}\\cos\\left(\\theta_{\\beta,\\beta^{\\prime}}^{(k)}\\right)\\right)\\sum_{k^{\\prime}=0}^{K-1}c_{k^{\\prime}}\\cos\\left(\\theta_{\\beta-\\beta^{\\prime},\\alpha-\\alpha^{\\prime}}^{(k^{\\prime})}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "From the orthogonality of the cosine and sine function, we obtain ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{N^{2}}\\displaystyle\\sum_{\\beta,\\beta^{\\prime}=0}^{N-1}\\sum_{k^{\\prime}=0}^{K-1}c_{k^{\\prime}}\\cos\\Big(\\theta_{\\beta-\\beta^{\\prime},\\alpha-\\alpha^{\\prime}}^{(k^{\\prime})}\\Big)\\cos\\Big(\\theta_{\\beta,\\beta^{\\prime}}^{(k)}\\Big)}\\\\ &{=\\displaystyle\\frac{1}{N^{2}}\\sum_{\\beta,\\beta^{\\prime}=0}^{N-1}\\sum_{k^{\\prime}=0}^{K-1}c_{k^{\\prime}}\\left(\\cos\\Big(\\theta_{\\beta,\\beta^{\\prime}}^{(k^{\\prime})}\\Big)\\cos\\Big(\\theta_{\\alpha^{\\prime},\\alpha}^{(k^{\\prime})}\\Big)\\cos\\Big(\\theta_{\\beta,\\beta^{\\prime}}^{(k)}\\Big)\\right.}\\\\ &{\\qquad\\left.+\\sin\\Big(\\theta_{\\beta,\\beta^{\\prime}}^{(k^{\\prime})}\\Big)\\sin\\Big(\\theta_{\\alpha^{\\prime},\\alpha}^{(k^{\\prime})}\\Big)\\cos\\Big(\\theta_{\\beta,\\beta^{\\prime}}^{(k)}\\Big)\\right)}\\\\ &{=\\cos\\Big(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k)}\\Big)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Hence, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\tilde{\\Sigma}_{\\alpha,\\alpha^{\\prime}}^{(\\ell)}=\\sum_{k=0}^{K-1}\\chi_{c^{*}}^{L-\\ell}\\tilde{\\epsilon}_{k}\\cos\\left(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k)}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This completes the proof. ", "page_idx": 24}, {"type": "text", "text": "C  Discussions on the similarity of DCN and CNN ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "While CNNs perform local convolutions in the spatial domain, FNOs execute convolutions in the frequency domain, thereby achieving global convolutions in the spatial domain. Our theory for the FNO and the mean-field theory for CNNs (Xiao et al., 2018) share a common focus on the correlation dynamics $\\Sigma^{(0)},\\Sigma^{(1)},\\dots,\\Sigma^{(L)}$ of the spatial representations $\\mathbf{H}^{(0)},\\mathbf{H}^{(1)},\\dots,\\mathbf{H}^{(L)}\\in\\mathbb{R}^{N\\times D}$ The iterated maps for CNNs and FNOs are obtained as follows. ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\Sigma_{\\alpha,\\alpha^{\\prime}}^{(\\ell+1)}=\\frac{\\sigma^{2}}{2r+1}\\sum_{\\beta\\in\\mathrm{ker}}\\mathbb{E}_{\\mathbf{H}_{\\bot,d}\\sim\\mathcal{N}(\\mathbf{0},\\Sigma^{(\\ell)})}\\left[\\phi(H_{\\alpha+\\beta,d})\\phi(H_{\\alpha^{\\prime}+\\beta,d})\\right]+\\sigma_{b}^{2}=:\\mathcal{C}_{\\mathrm{CNN}}(\\Sigma^{(\\ell)}),}\\\\ {\\displaystyle\\Sigma_{\\alpha,\\alpha^{\\prime}}^{(\\ell)}=\\sigma^{2}\\sum_{k=0}^{K-1}c_{k}\\mathbb{E}_{\\mathbf{H}_{\\bot,d}\\sim\\mathcal{N}(\\mathbf{0},\\Sigma^{(\\ell)})}\\left[\\left|\\left[F\\phi\\left(\\mathbf{H}_{\\bot,d}\\right)\\right]_{k}\\right|^{2}\\right]\\cos\\Big(\\theta_{\\alpha,\\alpha^{\\prime}}^{(k)}\\Big)+\\sigma_{b}^{2}=:\\mathcal{C}_{\\mathrm{FNO}}(\\Sigma^{(\\ell)}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $2r+1$ is the number of filter width and $\\ker=\\{\\beta\\in\\mathbb{Z}||\\beta|\\leq r\\}$ is set of indices referring to the elements of the filter. ", "page_idx": 24}, {"type": "text", "text": "When we consider the FNO without mode truncation $\\mathit{\\Omega}^{K}=N/2+1)$ , the propagation of the diagonal components $(\\alpha,\\alpha)$ for any $\\alpha\\in[N]$ is equivalent to a CNN with filter size $N$ performing global ", "page_idx": 24}, {"type": "text", "text": "convolution. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\Sigma_{\\alpha,\\alpha}^{(\\ell)}=\\sigma^{2}\\sum_{k=0}^{\\frac{N}{2}}c_{k}\\mathbb{E}_{{\\mathbf{H}}_{\\perp,d}\\sim N({\\mathbf{0}},{\\mathbf{\\Sigma}}\\mathbb{\\mathbf{Z}}^{(\\ell)})}\\left[\\left|\\left[F\\phi\\left({\\mathbf{H}}_{:,d}\\right)\\right]_{k}\\right|^{2}\\right]\\underbrace{\\cos\\Big(\\theta_{\\alpha,\\alpha}^{(k)}\\Big)}_{=1}+\\sigma_{b}^{2}}\\\\ {\\displaystyle=\\frac{\\sigma^{2}}{N}\\sum_{\\beta=0}^{N-1}\\mathbb{E}_{{\\mathbf{H}}_{\\perp,d}\\sim N({\\mathbf{0}},{\\mathbf{\\Sigma}}\\mathbb{Z}^{(\\ell)})}\\left[\\left|\\phi(H_{\\beta,d})\\right|^{2}\\right]+\\sigma_{b}^{2}}\\\\ {\\displaystyle=\\frac{\\sigma^{2}}{N}\\sum_{\\beta=0}^{N-1}\\mathbb{E}_{{\\mathbf{H}}_{\\perp,d}\\sim N({\\mathbf{0}},{\\mathbf{\\Sigma}}\\mathbb{Z}^{(\\ell)})}\\left[\\phi(H_{\\alpha+\\beta,d})\\phi(H_{\\alpha+\\beta,d})\\right]+\\sigma_{b}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The first equality follows from Perseval's equality and the second from the periodic boundary condition. In contrast, the propagation of the off-diagonal components differs between CNN and FNO, and the iterated map is different even in the presence of mode truncation. ", "page_idx": 25}, {"type": "text", "text": "Regarding the fixed point, Xiao et al. (2018) demonstrated that any fixed point for the iterated map of the DCN is also a fixed point for that of the CNN. Consequently, Lemma A.1 indicates that the fixed points for CNN and FNO are consistent. ", "page_idx": 25}, {"type": "text", "text": "The behaviour of the iterated map around fixed points reflects the nature of each architecture. CNN possess diagonal eigenspaces associated with eigenvalues $\\chi_{q^{*}}$ and non-diagonal eigenspaces associated with eigenvalues $\\chi_{c^{*}}$ . FNOs without mode truncation exhibit a similarity, possessing eigenspaces $\\chi_{q^{*}}$ for zero-frequency and eigenspaces $\\chi_{c^{*}}$ for $\\boldsymbol{\\mathrm{k}}$ -frequencies with diagonal components removed. ", "page_idx": 25}, {"type": "text", "text": "Finally, the iterated map during the backpropagation for CNN and FNO are given by ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\Sigma}_{\\alpha,\\alpha^{\\prime}}^{(\\ell)}=\\sigma^{2}\\displaystyle\\sum_{\\beta\\in\\mathrm{ker}}\\nu_{\\beta}\\tilde{\\Sigma}_{\\alpha-\\beta,\\alpha^{\\prime}-\\beta}^{(\\ell+1)}\\mathbb{E}_{\\mathbf{H}_{\\because,d}\\sim\\mathcal{N}(0,\\Sigma^{(\\ell)})}\\left[\\phi^{\\prime}(H_{\\alpha,j}^{(\\ell)})\\phi^{\\prime}(H_{\\alpha^{\\prime},j}^{(\\ell)})\\right]}\\\\ &{\\tilde{\\Sigma}_{\\alpha,\\alpha^{\\prime}}^{(\\ell)}=\\frac{\\sigma^{2}}{N^{2}}\\displaystyle\\sum_{\\beta,\\beta^{\\prime}=0}^{N-1}\\tilde{\\Sigma}_{\\beta,\\beta^{\\prime}}^{(\\ell+1)}\\mathbb{E}_{\\mathbf{H}_{\\therefore,d}\\sim\\mathcal{N}(0,\\Sigma^{(\\ell)})}\\left[\\phi^{\\prime}(H_{\\alpha,j}^{(\\ell)})\\phi^{\\prime}(H_{\\alpha^{\\prime},j}^{(\\ell)})\\right]\\left(\\displaystyle\\sum_{k=0}^{K-1}c_{k}\\cos\\left(\\theta_{\\beta-\\beta^{\\prime},\\alpha-\\alpha^{\\prime}}^{(k)}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $v_{\\beta}$ is the variance weight parameter dependent of the filter position $\\beta$ i.e., $w_{i,j}(\\beta)\\;\\sim$ ${\\mathcal{N}}(0,\\sigma^{2}v_{\\beta}/D)$ $\\begin{array}{r}{\\sum_{\\beta\\in\\ker}v_{\\beta}=1}\\end{array}$ ", "page_idx": 25}, {"type": "text", "text": "Using the approximation $\\Sigma^{(\\ell+1)}\\approx\\Sigma^{*}$ as with (Schoenholz et al., 2016), the backpropagation of the diagonal components $(\\alpha,\\alpha)$ of the FNO without mode truncation is equivalent to that of the global CNN (see Eq. 2.16) in Xiao et al., 2018) with $\\begin{array}{r}{v_{\\beta}=\\frac{1}{N}}\\end{array}$ asshownbelow: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\tilde{\\Sigma}_{\\alpha,\\alpha}^{(\\ell)}\\approx\\frac{1}{N^{2}}\\sum_{\\beta,\\beta^{\\prime}=0}^{N-1}\\tilde{\\Sigma}_{\\beta,\\beta^{\\prime}}^{(\\ell+1)}\\chi_{c^{*}}\\left(\\underbrace{\\sum_{k=0}^{N/2}c_{k}\\cos\\Big(\\theta_{\\beta-\\beta^{\\prime},\\alpha-\\alpha}^{(k)}\\Big)}_{=N\\delta_{\\beta,\\beta^{\\prime}}}\\right)=\\chi_{c^{*}}\\sum_{\\beta=0}^{N-1}\\frac{1}{N}\\tilde{\\Sigma}_{\\beta,\\beta}^{(\\ell+1)}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This equivalence suggests that the edge of chaos initialization (e.g., He initialization) is also valid for the FNO since the problems of gradient vanishing and explosion are determined by the diagonal componentsof $\\tilde{\\Sigma}$ ", "page_idx": 25}, {"type": "text", "text": "D   Details of Experimental Setup ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we summarize the detailed setup of the all experiments, including the experiments in Section 4. ", "page_idx": 25}, {"type": "text", "text": "D.1 Datasets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "D.1.1 Advection equation ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We used the advection equation data published by Takamoto et al. (2022). The advection equation for the function $u(x,t)\\in L^{\\dot{2}}((0,1)\\times(\\dot{0},2];\\mathbb{R})$ is given by ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\partial_{t}u(x,t)+\\beta\\partial_{x}\\left(u(x,t)/2\\right)=0,\\;u(x,0)=u_{0}(x),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $u_{0}\\in L^{2}((0,1);\\mathbb{R})$ is the initial condition and $\\beta\\in\\mathbb R$ is an advection speed set to 2.0. The exact solution is given as $u(x,t)=u_{0}(x-\\beta t)$ for any initial condition $u_{0}$ ", "page_idx": 26}, {"type": "text", "text": "Only periodic boundary conditions were used in this dataset. The initial conditions are the superposition of sinusoidal wave given by ", "page_idx": 26}, {"type": "equation", "text": "$$\nu_{0}(x)=\\sum_{i=1}^{k_{\\mathrm{max}}}A_{i}\\sin{(k_{i}x+\\phi_{i})},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Where $\\begin{array}{r}{k_{i}=2\\pi\\sum_{j=1}^{N}n_{i,j}/L_{x}}\\end{array}$ arewavembers hose $n_{i,j}$   \nin $[1,k_{\\mathrm{max}}]$ \uff0c $L_{x}\\,=1$ is the calculation domain size, $N\\,=\\,2$ is the number of wave to be added, and $k_{\\operatorname*{max}{}}\\,=\\,8$ is the maximum wave number. The amplitude $A_{i}$ is uniformly chosen in $[0,1]$ and the phase $\\phi_{i}$ is the randomly chosen in $(0,2\\pi)$ . The 2nd-order temporal and spatial upwind finite difference scheme was used for generating the data. Settings are described in Appendix D of (Takamoto et al., 2022). ", "page_idx": 26}, {"type": "text", "text": "D.1.2 Burgers' equation ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We used the Burgers? equation data published by Takamoto et al. (2022). The Burgers? equation for the function $u(x,t)\\in\\dot{L^{2}}((0,1)\\times(\\dot{0},2];\\mathbb{R})$ is givenby ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\partial_{t}u(x,t)+\\partial_{x}\\left(u^{2}(x,t)/2\\right)=\\nu\\partial_{x x}u(x,t),}\\\\ {u(x,0)=u_{0}(x),\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $u_{0}\\,\\in\\,L^{2}((0,1);\\mathbb{R})$ is the initial condition and $\\nu$ is the diffusion coefficient set to 4.0. The periodic boundary conditions and Equation (31) are used as the initial conditions. The 2nd-order temporal and spatial upwind finite difference scheme is used for generating the data. Settings are described in Appendix $\\mathrm{D}$ of (Takamoto et al., 2022). ", "page_idx": 26}, {"type": "text", "text": "D.1.3 Darcy Flow equation ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We used the data of 2D Darcy Flow equation on a regular grid published by Li et al. (2020a). The Darcy Flow equation for the function $\\bar{u}\\in H_{0}^{1}((0,1)^{2};\\mathbb{R}_{+})$ with a Dirichlet boundary is given by ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{-\\nabla\\cdot(a(x)\\nabla u(x))=f(x),\\qquad\\qquad\\qquad}&&{x\\in(0,1)^{2},}\\\\ &{\\qquad\\qquad\\qquad u(x)=0,}&&{x\\in\\partial(0,1)^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $a\\in L^{\\infty}((0,1)^{2};\\mathbb R_{+})$ is the diffusion coefficient and $f\\in L^{2}((0,1)^{2};\\mathbb{R})$ is the forcing function. The coefficients $a$ was generated by measure $\\mu=\\psi_{\\sharp}\\mathcal{N}(0,(-\\Delta+9I)^{-2})$ using the Laplacian with zero Neumann boundary and the binary point-wise mapping $\\psi(x)\\,=\\,12\\,\\left(x\\,\\geq\\,0\\right)$ ,3 $(x\\,<\\,0)$ The forcing function is fixed $f(x)=1$ . Our aim is to predict the operator mapping the diffusion coefficient to the solution $a\\to u$ . The solution function $u$ was generated by using the second-order finite difference scheme on a $421\\times421$ grid. Settings are described in Appendix A.3.2 of (Li et al., 2020c). ", "page_idx": 26}, {"type": "text", "text": "D.1.4  Incompressible Navier-Stokes equation ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We used the 2D NS equation on the unit torus defined by ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\partial_{t}\\omega(x,t)+u(x,t)\\cdot\\nabla\\omega(x,t)=\\nu\\nabla^{2}\\omega(x,t)+f(x),}\\\\ &{}&{\\nabla\\cdot u(x,t)=0,\\;\\omega(x,0)=\\omega_{0}(x),}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\omega(x,t)\\;\\in\\;C([0,T];H^{r}((0,1)^{2};\\mathbb{R}^{2}))$ is the vorticity, $\\omega_{0}\\;\\in\\;H^{r}((0,1)^{2};\\mathbb{R}^{2})$ is the initial vorticity, $u(x,t)~\\in~C([0,T];H^{r}((0,1)^{2};\\mathbb{R}^{2}))$ is the velocity field for any. $r~>~0$ $\\nu\\ \\in\\ \\mathbb{R}_{+}$ is the viscosity, and $f^{\\mathrm{~\\,~}}\\in\\;\\bar{L}^{2}((0,1)^{2};\\mathbb{R})$ is the external forcing function defined by $f(x)\\ =$ $0.1\\left(\\sin(2\\pi(x_{1}+x_{2})+\\cos(2\\pi(x_{1}+x_{2})\\right)$ . The initial vorticity $\\omega_{0}$ was generated by $\\omega_{0}\\sim\\mu$ where $\\mu=\\mathcal{N}(0,7^{\\frac{3}{2}}(-\\Delta+49I)^{-2.5})$ with periodic boundary conditions. The viscosity was set to $1e\\!-\\!3$ $1e\\!-\\!4$ ,or $1e{-5}$ . Our aim is to predict the operator that maps a solution $u$ up to time 10 to a solution up to some later time $T\\,>\\,10$ . The data was generated by the pseudo-spectral Crak-Nicholson second-order method on $64\\times64$ grid. For the data with $\\nu=1e{-4}$ , the time resolution was also downsampled by half. Settings are described in Section 5.3 of (Li et al., 2020c). ", "page_idx": 26}, {"type": "table", "img_path": "QJr02BTM7J/tmp/6a5613a96a3e56b3c825710ecc4e936335c6306d75ee97c2588c7f9a50d20bc3.jpg", "table_caption": ["Table 2: Training settings "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "D.2  Training settings ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Our detailed training settings of the experiments in Section 4 are provided in Table 2. Our experimental environment consists of an Intel Xeon Plantinum 8360Y (36-core) CPU and a single NVIDIA A100 GPU. Most of our code for experiments are based on the code of PDEBench (https : //github. com/pdebench/PDEBench) (Takamoto et al., 2022). The only modifications to the model are to multiply the outputs (variable out_ft in code of class FNO1d and FNO2d) corresponding to mode $k=0,N/2$ by $\\sqrt{2}$ and to initialize the weights by Gaussian distribution, as described in Section 3.1. ", "page_idx": 27}, {"type": "text", "text": "E Detailed Experimental Analysis ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "E.1  Analysis of Training Loss ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Figure 6a shows the training loss for each epoch of the 32-layer FNOs with parameters $\\sigma^{2}~\\in$ $\\{0.1,0.5,1.0,2.0,3.0,4.0\\}$ on the NS dataset with $\\nu=1e{-3}$ . When the initial parameter $\\sigma^{2}$ istoo small, the training loss is not well reduced due to gradient vanishing. On the other hand, when the initial parameter $\\bar{\\sigma}^{2}$ is too large, the initial training loss blows up due to gradient exploding. The proposed edge of chaos initialization smoothly reduces the training loss in the initial epoch and enables stable training. ", "page_idx": 27}, {"type": "text", "text": "E.2  Analysis of Test Performance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "The nMSE of the FNOs on test datasets for six distinct PDEs is presented in Tables 3 and 4. Results are shown only for the FNOs with initial parameters where training was successful in many cases. For the NS equation with viscosity values of $\\nu=1e{-3,1}e{-4}$ , where sufficient data is available, Table 4 shows that best performance is achieved with 8 or 16 layers. This suggests that while shallow FNOs are currently prevalent, deep FNOs could be advantageous in certain tasks, underscoring the significance of our analysis of the bias in deep FNOs. Conversely, for other equations, the 4-layer FNO performs best and deeper FNOs result in a drop in performance even with the edge of chaos initialization. We will discuss this test performance deterioration in detail. ", "page_idx": 27}, {"type": "text", "text": "The over-fitting phenomenon is observed in the Darcy Flow and NS equation datasets with $\\nu=1e{-5}$ where only limited training data is available. The training loss for each epoch on the NS equation is depicted in Fig. 6b. As demonstrated in Fig. 6b, the 16 and 32-layer FNOs yield a lower training loss than the 4-layer FNO, but exhibit poorer performance on the test dataset as shown in Table 4. These results suggest over-fitting to the training data, necessitating either abundant training data or appropriate regularization. ", "page_idx": 27}, {"type": "text", "text": "Conversely, the under-fitting phenomenon is apparent in the 1D advection and Burgers\u2019 equation datasets in Table 3. The training loss of the FNO with ReLU activation for each epoch on the Burgers equation is presented in Fig. 6c. Figure 6c indicates that the larger the number of layers, the higher the training loss in the final epoch, and the worse the test performance. This under-fitting to the training data could be attributed to the escalating complexity of the loss landscape as the layer count increases, a known issue for DCN and CNN (Li et al., 2018). This may be due to the emergence of local minima corresponding to operators that generate too complex functions, preventing the ", "page_idx": 27}, {"type": "image", "img_path": "QJr02BTM7J/tmp/eb78059fa819e2e75739aacb903cdd06ce05d9ca4cd69ae4d14dd9483fc2ec7f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Figure 6: Training Loss Curve. (a): training loss curve of the 32-layer original FNOs with varying initial parameters $\\bar{\\sigma}^{2}\\in\\{0.1,0.5,1.0,2.0,3.0,4.0\\}$ , on the NS equation with $\\nu=1e{-3}$ (b): training loss curve of the original FNOs with an initial parameter $\\sigma^{2}=\\dot{2}.0$ with a varying number of layers $L\\,\\in\\,\\{4,8,16,32\\}$ on the NS equation with $\\nu=1e\\mathrm{-}5$ (c): training loss curve of the simplified FNOs with ReLU activation and the initial parameter $\\sigma^{2}\\,=\\,2.0$ with varying number of layers $L\\in\\{4,\\underline{{8}},16,32\\}$ on the Burgers\u2019 equation. ", "page_idx": 28}, {"type": "text", "text": "Table 3: Test performance measured by nMSE of 1D simplified FNO on 1D PDEs attainment of parameters that achieve global minima. This issue could be mitigated by introducing more suitable regularization, an appropriate optimizer, or a skip connection (Tran et al., 2022). ", "page_idx": 28}, {"type": "table", "img_path": "QJr02BTM7J/tmp/90fb7e5133aefd16d04d63a8244e8df61cf5da205ee8e936ca77fb5e674de953.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "Our theory and experiments suggest that the training of deep FNOs has suffered from problems including gradient vanishing and exploding due to improper initialization, over-fitting caused by insufficient training data, and under-fitting caused by loss landscapes with strong non-convexity. While our edge of chaos initialization prevents the gradient vanishing and exploding, techniques to solve over-fitting and under-fitting problems are still needed in practice. ", "page_idx": 28}, {"type": "text", "text": "F  Visualization of Forward Propagation ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We visualized the behavior of the simplified FNO's covariance matrix $\\Sigma^{(\\ell)}$ with varying initialization parameters $\\sigma^{2}\\in\\{0.1,1.0,2.0,4.0\\}$ and activation functions. The FNO, with a width of $D=1024$ was used and the input was sampled from the standard normal distribution with a spatial size of $N=32$ . The results of the FNO with Tanh activation, both with and without mode truncation, are shown in Figs. 7 and 8 and Figs. 9 and 10 respectively. Similarly, the results of the FNO with ReLU activation, both with and without mode truncation, are displayed in Figs. 11 and 12 and Figs. 13 and 14 respectively. In the ordered phase, all figures illustrate convergence to the fixed point $\\pmb{\\Sigma}^{*}$ where $c^{*}=1$ , with the rate of convergence increasing as the parameter $\\sigma^{\\bar{2}}$ decreases. In the chaotic phase, the activation function dictates the covariance behavior. Without mode truncation, the covariance behavior of the FNO is identical to those of the DCN; otherwise non-uniform, FNO-specific periodic covariance is exhibited. ", "page_idx": 28}, {"type": "table", "img_path": "QJr02BTM7J/tmp/5510eb62d6ef24be9cb817140ddbae530bb84e3769ea78fc98046ce723353c9e.jpg", "table_caption": ["Table 4: Test performance measured by nMSE of 2D original FNO with ReLU activation on Darcy Flow and NS equation. "], "table_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "QJr02BTM7J/tmp/6f4a54c7fe2aafd959965f22febef132c124ff1e61674ad40ea3291b74088a63.jpg", "img_caption": ["$\\Sigma_{\\beta,\\beta^{\\prime}}^{(\\ell)}/\\sqrt{\\Sigma_{\\beta,\\beta}^{(\\ell)}\\Sigma_{\\beta^{\\prime},\\beta^{\\prime}}^{(\\ell)}}$ ,B,s for the simplifed FNO with Tanh activation and no mode truncation. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "QJr02BTM7J/tmp/52e7eae61381b002014899f814eefabc55f6d1d2e211aebd3de9d887bea3e920.jpg", "img_caption": ["Figure 8: Visualization of the covariance $\\Sigma^{(\\ell)}$ for the simplified FNO with Tanh activation and no mode truncation. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "QJr02BTM7J/tmp/c8981014b9a1cb703d9a654c02ef8433b526b8b3cac4e191d0d6d8f890f5806a.jpg", "img_caption": ["Figue isuaizato of he corlato $\\Sigma_{\\beta,\\beta^{\\prime}}^{(\\ell)}/\\sqrt{\\Sigma_{\\beta,\\beta}^{(\\ell)}\\Sigma_{\\beta^{\\prime},\\beta^{\\prime}}^{(\\ell)}}$ activation and the Fourier mode $K=5$ "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "QJr02BTM7J/tmp/fea6b7157086d731d5bcac907e85ae76a13b75176921913642fb98c03a05bffb.jpg", "img_caption": ["Figure 10: Visualization of the covariance $\\Sigma^{(\\ell)}$ for the simplified FNO with Tanh activation and the Fourier mode $K=5$ "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "QJr02BTM7J/tmp/bf6a4e38b9ba6abde4dc78396219dfbdb72e655a9356ac79ba4919943d912c34.jpg", "img_caption": ["Figure 11: Visualization of the correlation $\\Sigma_{\\beta,\\beta^{\\prime}}^{(\\ell)}/\\sqrt{\\Sigma_{\\beta,\\beta}^{(\\ell)}\\Sigma_{\\beta^{\\prime},\\beta^{\\prime}}^{(\\ell)}}$ for the simplified FNO with ReLU activation and no mode truncation. "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "QJr02BTM7J/tmp/b2c4d63cecb209b395dd0ec5470fdc28ff383500cb0fc9e77ca9001eb3fa5a64.jpg", "img_caption": [], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "QJr02BTM7J/tmp/4a2e8a49d3a5a80c1cf70a46c6ed8ea21661d3b3a194df987e2f244be9729806.jpg", "img_caption": ["Figure 12: Visualization of the covariance $\\Sigma^{(\\ell)}$ for the simplified FNO with ReLU activation and no mode truncation. ", "(b) Initial parameter $\\sigma^{2}=4.0$ "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "QJr02BTM7J/tmp/112b709ddc0f2e32b0a1cb9627ed294f2d57c840e60b531288467bb859ae9779.jpg", "img_caption": ["Figure 13: Visualization of the coelation .0 $\\Sigma_{\\beta,\\beta^{\\prime}}^{(\\ell)}/\\sqrt{\\Sigma_{\\beta,\\beta}^{(\\ell)}\\Sigma_{\\beta^{\\prime},\\beta^{\\prime}}^{(\\ell)}}$ ,B,s for the simplied FNO with ReLU activation and the Fourier mode $K=5$ "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "QJr02BTM7J/tmp/07e1a79c0f3c3a381dfa66d390565d16da7c9f4a65657b201131e64ee3f911b7.jpg", "img_caption": ["Figure 14: Visualization of the covariance $\\Sigma^{(\\ell)}$ for the simplified FNO with ReLU activation and the Fourier mode $K=5$ ", ""], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The contribution and scope of the paper are described in the abstract and introduction. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 34}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: We mention the limitation of this paper in Section 5. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational effciency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 34}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The full set of assumptions and a complete proof are provided in Section 3 and Appendices A and B. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 35}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: All the information needed to reproduce the main experimental results are provided in Section 4 and Appendix D. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 35}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We use existing public datasets and cite them appropriately. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 36}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: All training and test details are provided in Section 4 and Appendix D Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 36}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [No] ", "page_idx": 36}, {"type": "text", "text": "Justification: In the experimental results depicted in Figs. 3 and 4, we determined that the variation in the reported values has minimal impact. In fact, the inclusion of error bars seems to obstruct intuitive understanding through visualization. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: All sufficient information on the computer resources are provided in Section 4 and Appendix D. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 37}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We follow the NeurIPS Code of Ethics. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 37}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Our paper is foundational research and not tied to particular applications. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed. \u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 37}, {"type": "text", "text": "\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 38}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 38}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: We have stated the license and copyright of the data used. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: The content of our public code is well described. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset isused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 39}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 39}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 39}]