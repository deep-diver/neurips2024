[{"figure_path": "QJr02BTM7J/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of ordered-chaos phase transition for the weight initialization parameter \u03c32. In the ordered phase, the spatial hidden representations H(l) on the grid converge to a uniform state during forward propagation and the gradient vanishes during backpropagation. In the chaotic phase, the representations either converge to a distinct state or diverge and the gradient explodes.", "description": "This figure illustrates the ordered-chaos phase transition in a Fourier Neural Operator (FNO) based on the weight initialization parameter (\u03c32).  The left side shows the ordered phase where, during forward propagation, spatial hidden representations (H(l)) converge to a uniform state, leading to vanishing gradients during backpropagation. Conversely, the right side depicts the chaotic phase, where representations either converge to distinct states or diverge, resulting in exploding gradients. The transition point between these phases is identified as 'the edge of chaos'.", "section": "3 A mean-field theory for FNO"}, {"figure_path": "QJr02BTM7J/figures/figures_6_1.jpg", "caption": "Figure 2: Average gradient norm Tr(\u2211(\u2113))/D during the backpropagation of several FNOs plotted as a function of layer \u2113. Each line corresponds to the result of different initial values of \u03c3\u00b2 from 0.5 to 4.0 in increments of 0.5. The x-axis is the layer and the y-axis is the log-scale of the gradient norm. Depending on the value of \u03c3\u00b2, the gradient norm increases or decreases consistently as the gradient propagates to shallower layers.", "description": "This figure shows the average gradient norm during backpropagation for several Fourier Neural Operators (FNOs) with different initial weight variance parameters (\u03c3\u00b2).  The plots illustrate how the gradient norm changes across layers (depth) of the network.  The behavior is shown to depend on the initial \u03c3\u00b2 value, with some showing consistent increase or decrease as the gradient propagates through the network.", "section": "3 A mean-field theory for FNO"}, {"figure_path": "QJr02BTM7J/figures/figures_9_1.jpg", "caption": "Figure 2: Average gradient norm Tr(\u2211(\u2113))/D during the backpropagation of several FNOs plotted as a function of layer \u2113. Each line corresponds to the result of different initial values of \u03c3\u00b2 from 0.5 to 4.0 in increments of 0.5. The x-axis is the layer and the y-axis is the log-scale of the gradient norm. Depending on the value of \u03c3\u00b2, the gradient norm increases or decreases consistently as the gradient propagates to shallower layers.", "description": "This figure shows how the average gradient norm changes during backpropagation in different FNO models with varying initial weight variance (\u03c3\u00b2).  Each line represents a different initial \u03c3\u00b2, ranging from 0.5 to 4.0. The x-axis represents the layer number, while the y-axis shows the logarithm of the average gradient norm. The plot demonstrates that the gradient either consistently increases or decreases depending on the initial \u03c3\u00b2, highlighting the impact of initialization on the training stability of FNOs.", "section": "3 A mean-field theory for FNO"}, {"figure_path": "QJr02BTM7J/figures/figures_9_2.jpg", "caption": "Figure 2: Average gradient norm Tr(\u2211(e))/D during the backpropagation of several FNOs plotted as a function of layer l. Each line corresponds to the result of different initial values of \u03c3\u00b2 from 0.5 to 4.0 in increments of 0.5. The x-axis is the layer and the y-axis is the log-scale of the gradient norm. Depending on the value of \u03c3\u00b2, the gradient norm increases or decreases consistently as the gradient propagates to shallower layers.", "description": "This figure shows the average gradient norm across different layers during backpropagation for various FNO architectures. The initial weight variance (\u03c3\u00b2) is varied, and its impact on gradient behavior is observed. The plots illustrate how gradient norms change consistently (either increasing or decreasing) as backpropagation progresses through the layers depending upon the initial weight variance.", "section": "3 A mean-field theory for FNO"}, {"figure_path": "QJr02BTM7J/figures/figures_9_3.jpg", "caption": "Figure 1: Illustration of ordered-chaos phase transition for the weight initialization parameter \u03c3\u00b2. In the ordered phase, the spatial hidden representations H(l) on the grid converge to a uniform state during forward propagation and the gradient vanishes during backpropagation. In the chaotic phase, the representations either converge to a distinct state or diverge and the gradient explodes.", "description": "This figure illustrates the ordered-chaos phase transition in a Fourier Neural Operator (FNO) based on the weight initialization parameter (\u03c3\u00b2).  The left side shows the ordered phase where the hidden representations converge uniformly during forward propagation, resulting in vanishing gradients during backpropagation.  Conversely, the right side depicts the chaotic phase where representations diverge or converge to distinct states, leading to exploding gradients. This transition highlights the impact of weight initialization on FNO behavior and its implications for training stability.", "section": "3 A mean-field theory for FNO"}, {"figure_path": "QJr02BTM7J/figures/figures_9_4.jpg", "caption": "Figure 5: Ordered-chaos phase transition diagram for the DCN", "description": "This figure shows the phase transition diagram for Deep Convolutional Networks (DCNs) with different activation functions (Tanh and ReLU).  The diagrams illustrate how the network's behavior transitions between an 'ordered' phase (where representations of different inputs converge) and a 'chaotic' phase (where representations diverge). This transition is controlled by the variance of the weight initialization (\u03c3\u00b2) and the variance of the bias initialization (\u03c3\u03c4). The edge of chaos, representing the optimal initialization for training stability, is highlighted.  The vanishing and exploding gradients regions are also identified, associated with the ordered and chaotic phases, respectively.", "section": "3 A mean-field theory for FNO"}, {"figure_path": "QJr02BTM7J/figures/figures_13_1.jpg", "caption": "Figure 5: Ordered-chaos phase transition diagram for the DCN", "description": "This figure shows the phase transition diagram for Deep Convolutional Neural Networks (DCNNs). The diagrams illustrate the relationship between the initial variance of weights (\u03c3\u00b2) and biases (\u03c3b\u00b2), and the behavior of the network during forward and backward propagation. The left diagram (a) uses the Tanh activation function, while the right diagram (b) uses the ReLU activation function.  The x-axis represents \u03c3\u00b2, and the y-axis represents \u03c3b\u00b2. The color scale indicates the average gradient magnitude.  The ordered phase is characterized by vanishing gradients, while the chaotic phase is characterized by exploding gradients. The network is stably trained only at the edge of chaos, the transition point between the ordered and chaotic phases.  The key difference between the Tanh and ReLU activations is how the edge of chaos is defined: For Tanh, there is a continuous transition, while for ReLU, the transition is sharp and occurs at a specific value of \u03c3\u00b2.", "section": "3 A mean-field theory for FNO"}, {"figure_path": "QJr02BTM7J/figures/figures_28_1.jpg", "caption": "Figure 2: Average gradient norm Tr(\u2211(l))/D during the backpropagation of several FNOs plotted as a function of layer l. Each line corresponds to the result of different initial values of \u03c3\u00b2 from 0.5 to 4.0 in increments of 0.5. The x-axis is the layer and the y-axis is the log-scale of the gradient norm. Depending on the value of \u03c3\u00b2, the gradient norm increases or decreases consistently as the gradient propagates to shallower layers.", "description": "This figure shows the average gradient norm across different layers of several Fourier Neural Operators (FNOs). The initial weight variance (\u03c3\u00b2) is varied across several runs, and its impact on the gradient norm over the layers is shown in a log scale.  The results show that the gradient norm either consistently increases or decreases, depending on the initial \u03c3\u00b2 value.", "section": "3.3 Initialization requirements for stable training"}, {"figure_path": "QJr02BTM7J/figures/figures_29_1.jpg", "caption": "Figure 2: Average gradient norm Tr(\u2211(l))/D during the backpropagation of several FNOs plotted as a function of layer l. Each line corresponds to the result of different initial values of \u03c3\u00b2 from 0.5 to 4.0 in increments of 0.5. The x-axis is the layer and the y-axis is the log-scale of the gradient norm. Depending on the value of \u03c3\u00b2, the gradient norm increases or decreases consistently as the gradient propagates to shallower layers.", "description": "This figure shows the average gradient norm across different layers during backpropagation for various initial weight variances (\u03c3\u00b2).  It demonstrates how the gradient norm changes as the backpropagation progresses through the network.  The plot shows that the gradient either consistently grows (exploding gradients) or shrinks (vanishing gradients) depending on the initial value of \u03c3\u00b2. The behavior observed provides insights into the stability of FNO training related to initialization.", "section": "3 A mean-field theory for FNO"}, {"figure_path": "QJr02BTM7J/figures/figures_30_1.jpg", "caption": "Figure 2: Average gradient norm Tr(\u2211(\u2113))/D during the backpropagation of several FNOs plotted as a function of layer \u2113. Each line corresponds to the result of different initial values of \u03c3\u00b2 from 0.5 to 4.0 in increments of 0.5. The x-axis is the layer and the y-axis is the log-scale of the gradient norm. Depending on the value of \u03c3\u00b2, the gradient norm increases or decreases consistently as the gradient propagates to shallower layers.", "description": "This figure shows how the average gradient norm changes during backpropagation for different initial values of the weight variance (\u03c3\u00b2).  The plot demonstrates that the gradient either consistently increases (exploding gradients) or decreases (vanishing gradients) as it propagates through the network's layers, depending on the initial \u03c3\u00b2.  The behavior is consistent across layers, revealing a relationship between the initial weight variance and gradient stability during training.", "section": "3 A mean-field theory for FNO"}, {"figure_path": "QJr02BTM7J/figures/figures_30_2.jpg", "caption": "Figure 1: Illustration of ordered-chaos phase transition for the weight initialization parameter \u03c3\u00b2. In the ordered phase, the spatial hidden representations H(l) on the grid converge to a uniform state during forward propagation and the gradient vanishes during backpropagation. In the chaotic phase, the representations either converge to a distinct state or diverge and the gradient explodes.", "description": "This figure illustrates the ordered-chaos phase transition in a Fourier Neural Operator (FNO) as a function of the weight initialization parameter (\u03c3\u00b2).  The left side shows the ordered phase where spatial representations converge uniformly, leading to vanishing gradients during backpropagation. The right shows the chaotic phase where representations either converge to different states or diverge, resulting in exploding gradients. The transition between these phases highlights a crucial aspect of FNO training stability.", "section": "3 A mean-field theory for FNO"}, {"figure_path": "QJr02BTM7J/figures/figures_31_1.jpg", "caption": "Figure 2: Average gradient norm Tr(\u2211(l))/D during the backpropagation of several FNOs plotted as a function of layer l. Each line corresponds to the result of different initial values of \u03c3\u00b2 from 0.5 to 4.0 in increments of 0.5. The x-axis is the layer and the y-axis is the log-scale of the gradient norm. Depending on the value of \u03c3\u00b2, the gradient norm increases or decreases consistently as the gradient propagates to shallower layers.", "description": "This figure shows how the average gradient norm changes across different layers of several Fourier Neural Operators (FNOs). Each line represents a different initial weight variance (\u03c3\u00b2). The x-axis shows the layer number, and the y-axis shows the gradient norm on a logarithmic scale. The plot illustrates that depending on the initial variance, the gradient norm either increases or decreases consistently as backpropagation progresses.", "section": "3 A mean-field theory for FNO"}, {"figure_path": "QJr02BTM7J/figures/figures_31_2.jpg", "caption": "Figure 1: Illustration of ordered-chaos phase transition for the weight initialization parameter \u03c3\u00b2. In the ordered phase, the spatial hidden representations H(l) on the grid converge to a uniform state during forward propagation and the gradient vanishes during backpropagation. In the chaotic phase, the representations either converge to a distinct state or diverge and the gradient explodes.", "description": "This figure illustrates the ordered-chaos phase transition in a Fourier Neural Operator (FNO) based on the weight initialization parameter (\u03c3\u00b2).  The left side shows the 'ordered phase', where the spatial representations converge uniformly, leading to vanishing gradients during backpropagation. The right side shows the 'chaotic phase', resulting in either distinct states or divergence, causing exploding gradients.  The transition between these phases is crucial for stable FNO training.", "section": "3 A mean-field theory for FNO"}, {"figure_path": "QJr02BTM7J/figures/figures_32_1.jpg", "caption": "Figure 2: Average gradient norm Tr(\u2211(\u2113))/D during the backpropagation of several FNOs plotted as a function of layer \u2113. Each line corresponds to the result of different initial values of \u03c3\u00b2 from 0.5 to 4.0 in increments of 0.5. The x-axis is the layer and the y-axis is the log-scale of the gradient norm. Depending on the value of \u03c3\u00b2, the gradient norm increases or decreases consistently as the gradient propagates to shallower layers.", "description": "The figure shows how the average gradient norm changes during backpropagation for different FNOs.  The initial variance parameter (\u03c3\u00b2) is varied, demonstrating its significant impact on gradient behavior.  As gradients propagate towards earlier layers, they consistently increase or decrease depending on the initial \u03c3\u00b2 value, highlighting the importance of appropriate initialization for stable training.", "section": "3 A mean-field theory for FNO"}, {"figure_path": "QJr02BTM7J/figures/figures_32_2.jpg", "caption": "Figure 7: Visualization of the correlation \u03a3(l)\u03b2,\u03b2'/\u221a\u03a3(l)\u03b2,\u03b2\u03a3(l)\u03b2',\u03b2' for the simplified FNO with Tanh activation and no mode truncation.", "description": "The figure visualizes the correlation of the covariance matrix \u03a3(l) at different layers (l=0,1,4,8,16,32) of a simplified Fourier Neural Operator (FNO) using the Tanh activation function and without mode truncation.  Each subplot represents a layer, showing how the correlation between spatial positions changes during forward propagation. The color scale indicates the strength of the correlation.  The pattern shows how the model's understanding of spatial relationships evolves as it processes the input.", "section": "F Visualization of Forward Propagation"}, {"figure_path": "QJr02BTM7J/figures/figures_32_3.jpg", "caption": "Figure 2: Average gradient norm Tr(\u2211(l))/D during the backpropagation of several FNOs plotted as a function of layer l. Each line corresponds to the result of different initial values of \u03c3\u00b2 from 0.5 to 4.0 in increments of 0.5. The x-axis is the layer and the y-axis is the log-scale of the gradient norm. Depending on the value of \u03c3\u00b2, the gradient norm increases or decreases consistently as the gradient propagates to shallower layers.", "description": "This figure shows the average gradient norm across different layers of several Fourier Neural Operators (FNOs) during backpropagation. The initial weight variance (\u03c3\u00b2) is varied, and each line in the plot represents a different \u03c3\u00b2.  The y-axis is a logarithmic scale of the gradient norm, and the x-axis represents the layer number. The plot demonstrates how the gradient norm changes consistently depending on the chosen value of \u03c3\u00b2 as backpropagation progresses.  A consistent increase or decrease in gradient norm is observed depending on \u03c3\u00b2.", "section": "3 A mean-field theory for FNO"}, {"figure_path": "QJr02BTM7J/figures/figures_33_1.jpg", "caption": "Figure 2: Average gradient norm Tr(\u2211(l))/D during the backpropagation of several FNOs plotted as a function of layer l. Each line corresponds to the result of different initial values of \u03c3\u00b2 from 0.5 to 4.0 in increments of 0.5. The x-axis is the layer and the y-axis is the log-scale of the gradient norm. Depending on the value of \u03c3\u00b2, the gradient norm increases or decreases consistently as the gradient propagates to shallower layers.", "description": "This figure shows the average gradient norm over the depth of several FNOs with different initial weight variances (\u03c3\u00b2). The plot shows how the gradient norm changes as backpropagation proceeds through the network layers.  The gradient norm either consistently increases (exploding gradients) or decreases (vanishing gradients) depending on the initial \u03c3\u00b2 value, illustrating the impact of initialization on training stability.", "section": "3 A mean-field theory for FNO"}]