[{"figure_path": "m4NI2yIwJA/tables/tables_1_1.jpg", "caption": "Table 1: Comparison of various model reuse tasks in the non-Euclidean domain, tailored for GNNs.", "description": "This table compares different graph neural network (GNN) model reuse techniques in the non-Euclidean domain.  It contrasts Knowledge Distillation, Knowledge Amalgamation, and the novel Deep Graph Mating (GRAMA) approach introduced in the paper. The columns indicate whether each method supports multi-model reuse, operates without annotations, and is training-free/fine-tuning-free.", "section": "1 Introduction"}, {"figure_path": "m4NI2yIwJA/tables/tables_7_1.jpg", "caption": "Table 2: Multi-class molecule property prediction results for parent GNNs, each pre-trained on disjoint partitions of the ogbn-arxiv and ogbn-products datasets [18].", "description": "This table presents the results of a multi-class node classification task using two parent GNNs pre-trained on disjoint partitions of the ogbn-arxiv and ogbn-products datasets.  It compares the performance of several methods, including Knowledge Amalgamation (KA), Vanilla Parameter Interpolation (VPI), Vanilla Alignment Prior to Interpolation (VAPI), and the proposed Dual-Message Coordination and Calibration (DuMCC) approach (with and without Child Message Calibration). The table shows the accuracy achieved by each method on the test sets for both datasets.  The \"Re-train?\" column indicates whether the method required retraining.", "section": "6 Experiments"}, {"figure_path": "m4NI2yIwJA/tables/tables_8_1.jpg", "caption": "Table 4: Results of the point cloud classification task on ModelNet40 [55] using DGCNN, with two parent models trained on disjoint partitions.", "description": "This table presents the results of a point cloud classification task using the DGCNN architecture on the ModelNet40 dataset.  Two parent DGCNN models were pre-trained on disjoint partitions of the dataset. The table compares the performance of the Knowledge Amalgamation (KA) method, two vanilla methods (VPI and VAPI), and the proposed DuMCC approach (with and without Child Message Calibration) on this task.  The results show the accuracy achieved by each method on two different dataset partitions (Dataset I and Dataset J).", "section": "6 Experiments"}, {"figure_path": "m4NI2yIwJA/tables/tables_8_2.jpg", "caption": "Table 3: Results for multi-label node classification and graph classification, indicating KA's vulnerability to misclassification errors from pre-trained models.", "description": "This table presents the results of multi-label node classification and graph classification tasks using GIN and GAT architectures respectively.  It compares the performance of the proposed DuMCC method with existing methods (KA, VPI, and VAPI) and pre-trained parent models. The results show that DuMCC exhibits comparable performance to KA while avoiding the drawbacks of relying on soft labels generated by pre-trained teacher models, which are susceptible to misclassification errors.", "section": "6 Experiments"}, {"figure_path": "m4NI2yIwJA/tables/tables_9_1.jpg", "caption": "Table 2: Multi-class molecule property prediction results for parent GNNs, each pre-trained on disjoint partitions of the ogbn-arxiv and ogbn-products datasets [18].", "description": "This table presents the results of a multi-class molecule property prediction task.  It compares the performance of several methods, including the proposed DuMCC approach, on two datasets (ogbn-arxiv and ogbn-products).  The methods compared include training-free methods (VPI, VAPI, DuMCC with and without CMC), and a training-based method (KA).  The table shows the accuracy achieved by each method on each dataset, highlighting the effectiveness of the proposed DuMCC approach, particularly when CMC is included, in achieving competitive results without requiring any retraining.", "section": "6 Experiments"}]