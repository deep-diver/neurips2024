[{"figure_path": "WSu1PPi2UP/tables/tables_2_1.jpg", "caption": "Table 1: Glossary", "description": "This table provides a glossary of terms and symbols used in the paper.  It includes definitions for key concepts such as the ambient space (e.g., descriptions of movies), latent embedding space, action space (e.g., changes to a movie plot), latent dimension, ambient metric, latent metric, utility function, latent encoder, reference policy, and reference policy distribution.  These terms are fundamental to understanding the EAGLE model and its application.", "section": "2 Preliminaries and Related Work"}, {"figure_path": "WSu1PPi2UP/tables/tables_5_1.jpg", "caption": "Table 2: Comparison of ELM, supervised training, and EAGLE. We test three distribution of reference policies: uniform (i.e., random), optimistic (i.e., next-step best action), and G-optimal design (Definition 1).", "description": "This table compares the performance of three different methods for generating novel entities that align with a latent embedding space: ELM, supervised training using a reference policy, and the proposed EAGLE agent.  Three different reference policy distributions are tested with each method: uniform (random), optimistic (choosing the action with the highest expected immediate reward), and G-optimal design (a method for choosing actions to maximize exploration and minimize variance). The results are evaluated based on utility scores (measuring how well the generated entities meet the predefined criteria), user utility (ratings from human evaluators about how well the generated entities satisfy user needs), rater utility (ratings from human evaluators reflecting their personal preference for the generated content), and distance (measuring how different the generated content is from the existing content).", "section": "4 Surfacing Optimal Entities Using LLMs"}, {"figure_path": "WSu1PPi2UP/tables/tables_7_1.jpg", "caption": "Table 2: Comparison of ELM, supervised training, and EAGLE. We test three distribution of reference policies: uniform (i.e., random), optimistic (i.e., next-step best action), and G-optimal design (Definition 1).", "description": "The table compares the performance of three different methods for generating novel entities that adhere to an underlying embedding space: ELM (Embedding Language Models), supervised training using different reference policies (uniform, optimistic, and G-optimal design), and EAGLE (Embedding-Aligned Guided Language).  The results show the utility of the generated entities, as well as human rater evaluations of user utility, rater utility, and distance from the original entity.  EAGLE generally outperforms the other methods, particularly when using the G-optimal design policy.", "section": "6 Experiments"}, {"figure_path": "WSu1PPi2UP/tables/tables_7_2.jpg", "caption": "Table 3: For EnvTrain \u2208 {Gemini Pro, Gemini Ultra} and EnvTest \u2208 {Gemini Ultra, GPT-4}, the experiment EnvTrain EnvTest shows results for training EAGLE on EnvTrain and testing it on EnvTest. All experiments use an EAGLE based on Gemini-Nano and a G-optimal design baseline.", "description": "This table presents the results of experiments designed to test the transferability of EAGLE across different LLM environments.  It shows how well EAGLE, trained on either Gemini Pro or Gemini Ultra, performs when tested on either Gemini Ultra or GPT-4.  All experiments used a Gemini-Nano based EAGLE agent and a G-optimal design baseline. The metrics reported include the utility score (U(z)), user utility, rater utility, and distance score. This evaluation assessed the transferability of the trained model to different LLMs.", "section": "Experiments"}, {"figure_path": "WSu1PPi2UP/tables/tables_8_1.jpg", "caption": "Table 4: The effect of changing the action space on EAGLE, showing (1) the default action space as described in Sec. 5, (2) the default action space with personalized actions removed, (3) only macro actions (i.e., taking three actions at once), and (4) a combined action space, with both the default actions as well as macro actions. All experiments use a G-optimal design strategy as reference policy.", "description": "This table presents the results of experiments evaluating the impact of different action space designs on the performance of the EAGLE agent.  It compares four different action space configurations: the default action space, the default action space without personalized actions, a macro-action space (three actions at once), and a combined action space (default + macro actions). All configurations utilize a G-optimal design as the reference policy. The metrics reported include utility scores (U(z)) and human rater evaluation scores across different aspects (User Utility, Rater Utility, Distance).", "section": "Action Space"}, {"figure_path": "WSu1PPi2UP/tables/tables_15_1.jpg", "caption": "Table 2: Comparison of ELM, supervised training, and EAGLE. We test three distribution of reference policies: uniform (i.e., random), optimistic (i.e., next-step best action), and G-optimal design (Definition 1).", "description": "This table compares the performance of three different methods for generating novel entities that adhere to a latent embedding space: ELM (Embedding Language Model), supervised training with three different reference policies (uniform, optimistic, and G-optimal design), and the proposed EAGLE (Embedding-Aligned Guided Language) method.  The results are evaluated using utility scores and a human rater evaluation. The table shows the utility, user utility, rater utility, and distance scores for each method and reference policy. The G-optimal design generally outperforms other methods.", "section": "4.2 A Reinforcement Learning Perspective"}, {"figure_path": "WSu1PPi2UP/tables/tables_20_1.jpg", "caption": "Table 6: Training hyperparameters for reference policy.", "description": "This table shows the hyperparameters used for training the reference policy in the EAGLE agent.  The hyperparameters include the number of training steps, batch size, learning rate, and dropout probability. These parameters are important for controlling the training process and achieving good performance in reinforcement learning.", "section": "Implementation Details"}, {"figure_path": "WSu1PPi2UP/tables/tables_20_2.jpg", "caption": "Table 7: Training hyperparameters for EAGLE.", "description": "This table lists the hyperparameters used for training the EAGLE agent.  It shows the values used for training steps, KL regularization, learning rates for the policy and value functions, temperature for the agent and environment, the horizon, and the discount factor.", "section": "E.2 EAGLE Training"}, {"figure_path": "WSu1PPi2UP/tables/tables_43_1.jpg", "caption": "Table 8: Evaluation of EAGLE on Amazon Review Dataset.", "description": "This table presents the results of the EAGLE agent on a subset of the Amazon review dataset focused on clothing, shoes, and jewelry. It compares the performance of EAGLE against a reference policy using three metrics: Utility (measuring the overall value of the generated items), User Utility (measuring the value to a specific user), and Distance Score (measuring how different the generated item is from the original). The results demonstrate that EAGLE significantly outperforms the reference policy in terms of Utility and User Utility, suggesting its effectiveness in generating high-quality and user-relevant content.", "section": "Additional Experiments on Amazon Reviews"}]