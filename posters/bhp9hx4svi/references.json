{"references": [{"fullname_first_author": "Alekh Agarwal", "paper_title": "Distributed delayed stochastic optimization", "publication_date": "2011", "reason": "This paper is foundational for the study of asynchronous stochastic gradient descent (ASGD), a key topic of the current research."}, {"fullname_first_author": "Yossi Arjevani", "paper_title": "A tight convergence analysis for stochastic gradient descent with delayed updates", "publication_date": "2020", "reason": "This paper provides a rigorous convergence analysis of ASGD, which is crucial for understanding its theoretical properties."}, {"fullname_first_author": "Moritz Hardt", "paper_title": "Train faster, generalize better: Stability of stochastic gradient descent", "publication_date": "2016", "reason": "This paper introduces the concept of algorithmic stability for analyzing generalization performance of SGD, a fundamental concept extended in this work to ASGD."}, {"fullname_first_author": "Yunwen Lei", "paper_title": "Fine-grained analysis of stability and generalization for stochastic gradient descent", "publication_date": "2020", "reason": "This paper provides a refined analysis of SGD's stability and generalization, which serves as a basis for the current work's sharper bounds."}, {"fullname_first_author": "Xiangru Lian", "paper_title": "Asynchronous parallel stochastic gradient for nonconvex optimization", "publication_date": "2015", "reason": "This paper studies the convergence of ASGD in non-convex settings, which is relevant to the current work's broader scope."}]}