[{"figure_path": "bHP9hX4SvI/figures/figures_8_1.jpg", "caption": "Figure 1: The generalization errors of three categories of machine learning models trained using ASGD with learning rate \u03b7k = 0.1/\u03c4. The horizontal axis denotes the number of asynchronous training iterations, and the legend represents the average delay. A degradation in generalization performance is observed as the number of training iterations increases, and the generalization performance can be improved by appropriately increasing the asynchronous delay.", "description": "This figure shows the generalization error for three different machine learning tasks (convex optimization, computer vision, and natural language processing) as a function of the number of asynchronous training iterations.  Different lines represent different average delays (\u03c4).  The results show a general trend of increasing generalization error with more iterations, but also demonstrate that increasing the average delay can mitigate this effect and improve generalization performance. ", "section": "6 Experimental Validation"}, {"figure_path": "bHP9hX4SvI/figures/figures_29_1.jpg", "caption": "Figure 2: The on-average model stability in training various machine learning tasks using ASGD with learning rate \u03b7k = 0.1/\u03c4. The horizontal axis denotes the number of asynchronous training iterations, and the legend represents the average delay. A degradation in algorithm stability is observed as the number of training iterations increases.", "description": "This figure shows how the on-average model stability changes with the number of training iterations and the average delay (\u03c4) in three different machine learning tasks: convex optimization on RCV1, computer vision (CV) task on CIFAR100, and natural language processing (NLP) task on SST-2.  As the number of iterations increases, the stability decreases for all delay values.  The stability is better for larger delay values (\u03c4).", "section": "4.1 Algorithmic Stability of ASGD"}, {"figure_path": "bHP9hX4SvI/figures/figures_29_2.jpg", "caption": "Figure 2: The on-average model stability in training various machine learning tasks using ASGD with learning rate \u03b7k = 0.1/\u03c4. The horizontal axis denotes the number of asynchronous training iterations, and the legend represents the average delay. A degradation in algorithm stability is observed as the number of training iterations increases.", "description": "This figure shows how the on-average model stability changes with respect to the number of training iterations and the average asynchronous delay (\u03c4).  Across three different machine learning tasks (convex optimization, computer vision, and natural language processing), the stability generally decreases as the number of iterations increases.  However, increasing the average delay tends to improve the stability, suggesting that a larger delay mitigates some negative effects of asynchronous updates.", "section": "4 Stability and Generalization Bounds"}, {"figure_path": "bHP9hX4SvI/figures/figures_30_1.jpg", "caption": "Figure 1: The generalization errors of three categories of machine learning models trained using ASGD with learning rate \u03b7k = 0.1/\u03c4. The horizontal axis denotes the number of asynchronous training iterations, and the legend represents the average delay. A degradation in generalization performance is observed as the number of training iterations increases, and the generalization performance can be improved by appropriately increasing the asynchronous delay.", "description": "This figure shows the generalization error for three different machine learning tasks (convex optimization, computer vision, and natural language processing) when trained using the Asynchronous Stochastic Gradient Descent (ASGD) algorithm.  The x-axis represents the number of iterations during training, and the different colored lines represent different average delays (\u03c4). The figure demonstrates that increasing the average delay can mitigate the negative impact of asynchronous updates on generalization performance, especially as the number of training iterations increases.", "section": "6 Experimental Validation"}, {"figure_path": "bHP9hX4SvI/figures/figures_30_2.jpg", "caption": "Figure 1: The generalization errors of three categories of machine learning models trained using ASGD with learning rate \u03b7k = 0.1/\u03c4. The horizontal axis denotes the number of asynchronous training iterations, and the legend represents the average delay. A degradation in generalization performance is observed as the number of training iterations increases, and the generalization performance can be improved by appropriately increasing the asynchronous delay.", "description": "This figure shows the generalization error curves for three different machine learning tasks (convex optimization, computer vision, and natural language processing) trained using the Asynchronous Stochastic Gradient Descent (ASGD) algorithm.  The x-axis represents the number of training iterations, and different colored lines represent different average delays (\u03c4) in the ASGD algorithm.  The results demonstrate that increasing the asynchronous delay (\u03c4) can improve generalization performance, especially in the later stages of training.  However, as the training iterations increase, the generalization error tends to worsen, regardless of the delay.", "section": "6 Experimental Validation"}]