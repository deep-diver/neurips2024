[{"figure_path": "79eWvkLjib/tables/tables_7_1.jpg", "caption": "Table 1: Aggregate performance on full ExORL datasets. IQM scores aggregated over domains and tasks for all datasets, averaged across three seeds. Both VC-FB and MC-FB maintain the performance of FB; the largest relative performance improvement is on RANDOM.", "description": "This table presents the average IQM (interquartile mean) scores across all domains and tasks in the ExORL benchmark for three different datasets (RND, DIAYN, and RANDOM) using three zero-shot reinforcement learning methods: FB (Forward-Backward), VC-FB (Value-Conservative Forward-Backward), and MC-FB (Measure-Conservative Forward-Backward).  The results show the performance of VC-FB and MC-FB compared to the baseline FB method.  The table also includes aggregate results across all datasets. The results indicate that VC-FB and MC-FB generally maintain or slightly improve upon FB's performance, with the largest relative improvement seen on the RANDOM dataset.", "section": "4 Results"}, {"figure_path": "79eWvkLjib/tables/tables_8_1.jpg", "caption": "Table 2: Aggregated performance of conservative variants employing differing z sampling procedures on EXORL. DVC-FB derives all zs from the backward model; VC-FB derives all zs from Z; and MC-FB combines both. Performance correlates with the degree to which z ~ Z.", "description": "This table presents the aggregated performance results from experiments conducted on the EXORL benchmark.  It specifically compares three variants of conservative forward-backward (FB) representations, varying how the task vectors (z) are sampled.  DVC-FB samples z exclusively from the backward model, VC-FB samples z exclusively from a distribution over tasks, and MC-FB combines these two approaches. The results show a clear correlation between the sampling method and the overall performance, with VC-FB, which uses samples from the task distribution, achieving the best results.", "section": "Extended Results"}, {"figure_path": "79eWvkLjib/tables/tables_17_1.jpg", "caption": "Table 6: 100k dataset experimental results on ExORL. For each dataset-domain pair, we report the score at the step for which the all-task IQM is maximised when averaging across 5 seeds, and the constituent task scores at that step. Bracketed numbers represent the 95% confidence interval obtained by a stratified bootstrap.", "description": "This table presents the results of experiments conducted using 100,000 data points from three different datasets (RND, DIAYN, and RANDOM) on four different domains (Walker, Quadruped, Maze, and Jaco) within the ExORL benchmark. The table shows the performance of various algorithms (including the proposed VC-FB and MC-FB) on each task within each domain-dataset pair. The scores reported are the interquartile mean (IQM) values obtained at the learning step where the overall performance is best, averaged across 5 seeds. 95% confidence intervals are provided for better statistical reliability.", "section": "C Extended Results"}, {"figure_path": "79eWvkLjib/tables/tables_20_1.jpg", "caption": "Table 4: Hyperparameters for zero-shot RL methods. The additional hyperparameters for Conservative FB representations are highlighted in blue", "description": "This table lists the hyperparameters used in the zero-shot RL methods discussed in the paper.  It shows the values used for parameters like latent dimension, network layer dimensions, learning rate, discount factor etc.  The hyperparameters specific to the conservative variants of the FB method are highlighted in blue.", "section": "B Implementation Details"}, {"figure_path": "79eWvkLjib/tables/tables_24_1.jpg", "caption": "Table 5: Hyperparameters for Non-zero-shot RL.", "description": "This table lists the hyperparameters used for the non-zero-shot reinforcement learning methods (CQL, Offline TD3, and GC-IQL) in the paper's experiments.  It shows the architecture details (critic and actor dimensions), training parameters (learning steps, batch size, optimizer, learning rate, discount factor), and activation functions.  It also specifies hyperparameters specific to CQL (alpha, Lagrange, sampled actions number) and GC-IQL (IQL temperature, IQL Expectile) and Offline TD3 (standard deviation for policy smoothing, truncation level for policy smoothing).", "section": "B Implementation Details"}, {"figure_path": "79eWvkLjib/tables/tables_25_1.jpg", "caption": "Table 6: 100k dataset experimental results on ExORL. For each dataset-domain pair, we report the score at the step for which the all-task IQM is maximised when averaging across 5 seeds, and the constituent task scores at that step. Bracketed numbers represent the 95% confidence interval obtained by a stratified bootstrap.", "description": "This table presents the results of experiments conducted on the ExORL benchmark using datasets with 100,000 samples.  For each combination of dataset and domain, the table shows the cumulative reward (score) achieved at the learning step where the average performance across all tasks is maximized, along with the individual task scores. The scores are averages across five separate experimental runs. Confidence intervals (95%) are included to show the variability in the results.", "section": "C Extended Results"}, {"figure_path": "79eWvkLjib/tables/tables_26_1.jpg", "caption": "Table 6: 100k dataset experimental results on ExORL. For each dataset-domain pair, we report the score at the step for which the all-task IQM is maximised when averaging across 5 seeds, and the constituent task scores at that step. Bracketed numbers represent the 95% confidence interval obtained by a stratified bootstrap.", "description": "This table presents the results of experiments performed using datasets with 100,000 transitions.  The table shows the performance of different zero-shot RL methods across various tasks and domains in the ExORL benchmark.  The reported scores represent the interquartile mean (IQM) of cumulative rewards, averaged across five different random seeds.  Confidence intervals are also given.", "section": "C Extended Results"}, {"figure_path": "79eWvkLjib/tables/tables_27_1.jpg", "caption": "Table 8: Aggregate zero-shot performance on ExORL for all evaluation statistics recommended by [1]. VC-FB outperforms all methods across all evaluation statistics. \u2191 means a higher score is better; \u2193 means a lower score is better. Note that the optimality gap is large because we set y = 1000 and for many dataset-domain-tasks the maximum achievable score is far from 1000.", "description": "This table shows the performance of different zero-shot reinforcement learning methods (SF-LAP, GC-IQL, FB, CQL, MC-FB, and VC-FB) on the ExORL benchmark.  The results are evaluated using multiple metrics (IQM, Mean, Median, and Optimality Gap) recommended by Agarwal et al. [1]. The table highlights that the proposed VC-FB method outperforms all other methods across all evaluation metrics.  The large optimality gap is noted to be due to the setting of the discount factor (\u03b3) to 1000, which is not representative of the actual maximum achievable scores in many cases.", "section": "4.3 Results"}, {"figure_path": "79eWvkLjib/tables/tables_27_2.jpg", "caption": "Table 9: D4RL experimental results. For each dataset-domain pair, we report the score at the step for which the IQM is maximised when averaging across 3 seeds. Bracketed numbers represent the 95% confidence interval obtained by a stratified bootstrap.", "description": "This table presents the results of experiments conducted using the D4RL benchmark.  It shows the interquartile mean (IQM) scores achieved by several reinforcement learning algorithms across different tasks and datasets. The scores are the maximum IQM obtained during training, averaged across three different random seeds, and confidence intervals are provided to indicate the variability of the results.", "section": "4.3 Results"}]