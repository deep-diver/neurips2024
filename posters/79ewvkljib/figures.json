[{"figure_path": "79eWvkLjib/figures/figures_1_1.jpg", "caption": "Figure 1: Conservative zero-shot RL. (Left) Zero-shot RL methods must train on a dataset collected by a behaviour policy optimising against task zcollect, yet generalise to new tasks zeval. Both tasks have associated optimal value functions Qcollect and Q*eval for a given marginal state. (Middle) Existing methods, in this case forward-backward representations (FB), overestimate the value of actions not in the dataset for all tasks. (Right) Value-conservative forward-backward representations (VC-FB) suppress the value of actions not in the dataset for all tasks. Black dots () represent state-action samples present in the dataset.", "description": "This figure illustrates the core concept of the paper. The left panel shows the standard zero-shot RL problem where a model trained on data from one task (zcollect) needs to generalize to a new task (zeval). The middle panel illustrates the overestimation problem of existing methods, such as FB, which overestimate the value of out-of-distribution (OOD) actions. The right panel introduces the solution of the paper, VC-FB, which mitigates this problem by suppressing the values of OOD actions.", "section": "3 Zero-Shot RL from Low Quality Data"}, {"figure_path": "79eWvkLjib/figures/figures_3_1.jpg", "caption": "Figure 2: FB value overestimation with respect to dataset size n and quality. Log Q values and IQM of rollout performance on all Maze tasks for datasets RND and RANDOM. Q values predicted during training increase as both the size and \"quality\" of the dataset decrease. This contradicts the low return of all resultant policies (note: a return of 1000 is the maximum achievable for this task). Informally, we say the RND dataset is \"high\" quality, and the RANDOM dataset is \"low\" quality-see Appendix A.2 for more details.", "description": "This figure shows how the performance of the Forward-Backward (FB) algorithm degrades when trained on small, low-quality datasets. The top panels show that the predicted Q-values during training increase as both the dataset size and quality decrease. This is unexpected, as it contradicts the low returns observed in the resultant policies. The bottom panels confirm this by visualizing the actual rollout returns obtained on various tasks across different dataset sizes. The plot clearly shows that FB's performance drops significantly when trained on smaller and lower-quality datasets.", "section": "3.1 Failure Mode of Existing Methods"}, {"figure_path": "79eWvkLjib/figures/figures_4_1.jpg", "caption": "Figure 3: Ignoring out-of-distribution actions. The agents are tasked with learning separate policies for reaching  and . (a) RND dataset with all \"left\" actions removed; quivers represent the mean action direction in each state bin. (b) Best FB rollout after 1 million learning steps. (c) Best VC-FB performance after 1 million learning steps. FB overestimates the value of OOD actions and cannot complete either task; VC-FB synthesises the requisite information from the dataset and completes both tasks.", "description": "This figure shows how a conservative zero-shot RL method (VC-FB) outperforms a non-conservative method (FB) when trained on a dataset with missing actions.  Panel (a) shows the dataset used for training, highlighting the removal of all left actions.  Panel (b) shows the trajectory resulting from FB, demonstrating its failure to reach the goal because of overestimation of the value of out-of-distribution (OOD) actions. Panel (c) shows the trajectory from VC-FB, highlighting its success in reaching the goal due to its ability to synthesize information from the dataset and avoid overestimating OOD action values.", "section": "3 Zero-Shot RL from Low Quality Data"}, {"figure_path": "79eWvkLjib/figures/figures_5_1.jpg", "caption": "Figure 4: Aggregate zero-shot performance on ExORL. (Left) IQM of task scores across datasets and domains, normalised against the performance of CQL, our baseline. (Right) Performance profiles showing the distribution of scores across all tasks and domains. Both conservative FB variants stochastically dominate vanilla FB-see [1] for performance profile exposition. The black dashed line represents the IQM of CQL performance across all datasets, domains, tasks and seeds.", "description": "The left panel shows the IQM (interquartile mean) of zero-shot performance across different datasets and domains. The performance is normalized against that of CQL (Conservative Q-Learning), a strong baseline single-task offline RL algorithm. The right panel shows performance profiles, illustrating the cumulative distribution function of the task scores obtained by the various methods.  The figure demonstrates the improved performance of the proposed conservative zero-shot RL methods (MC-FB and VC-FB) compared to the baseline methods and vanilla FB (Forward-Backward) on the ExORL benchmark.", "section": "4 Results"}, {"figure_path": "79eWvkLjib/figures/figures_6_1.jpg", "caption": "Figure 5: Performance by dataset/domain on ExORL. IQM scores across tasks/seeds with 95% conf. intervals.", "description": "The figure shows the performance of different zero-shot RL methods (SF-LAP, GC-IQL, FB, MC-FB, VC-FB, and CQL) across various datasets (RND, DIAYN, RANDOM) and domains (Walker, Maze, Quadruped, Jaco) in the ExORL benchmark.  The y-axis represents the Interquartile Mean (IQM) of zero-shot performance, while the x-axis is not explicitly labeled but represents the range of scores for each method and domain/dataset combination. Error bars show the 95% confidence intervals. The results highlight the varying performance of each method across different datasets and domains, demonstrating the impact of data quality and diversity on zero-shot RL performance.", "section": "4.3 Results"}, {"figure_path": "79eWvkLjib/figures/figures_6_2.jpg", "caption": "Figure 6: Performance by dataset size. Aggregate IQM scores across all domains and tasks as RND size is varied. The performance delta between vanilla FB and the conservative variants increases as dataset size decreases.", "description": "The figure shows how the performance of three different zero-shot RL algorithms (FB, VC-FB, and MC-FB) changes with the size of the training dataset. The training dataset is the RND dataset, and its size is varied from 100k to 10M. The y-axis represents the aggregate IQM score across all tasks and domains. The figure shows that the performance gap between vanilla FB and the conservative variants (VC-FB and MC-FB) increases as the dataset size decreases. This suggests that the conservative variants are more robust to the distribution shift that occurs when the training dataset is small. The figure supports the claim that conservative zero-shot RL algorithms are superior on low-quality datasets.", "section": "4 Results"}, {"figure_path": "79eWvkLjib/figures/figures_7_1.jpg", "caption": "Figure 4: Aggregate zero-shot performance on ExORL. (Left) IQM of task scores across datasets and domains, normalised against the performance of CQL, our baseline. (Right) Performance profiles showing the distribution of scores across all tasks and domains. Both conservative FB variants stochastically dominate vanilla FB-see [1] for performance profile exposition. The black dashed line represents the IQM of CQL performance across all datasets, domains, tasks and seeds.", "description": "This figure presents a comparison of zero-shot performance across different reinforcement learning methods on the ExORL benchmark.  The left panel shows the interquartile mean (IQM) of task scores for various methods, normalized against the performance of Conservative Q-learning (CQL), highlighting the improvement achieved by conservative forward-backward (FB) methods. The right panel shows performance profiles, illustrating the cumulative distribution function of task scores, visually demonstrating the stochastic dominance of conservative FB methods over the standard FB method.", "section": "4 Results"}, {"figure_path": "79eWvkLjib/figures/figures_18_1.jpg", "caption": "Figure 8: Maze state coverage by dataset. (left) RANDOM; (middle) DIAYN; (right) RND.", "description": "This figure shows the state coverage of three different datasets used in the paper: RANDOM, DIAYN, and RND. Each dataset was collected using a different unsupervised exploration method. The figure visualizes the state coverage in a 2D maze environment.  The color intensity represents the density of state visits, darker colors indicating more frequent visits.  The image demonstrates that the RND dataset (right) provides significantly better coverage than the other datasets, while the RANDOM dataset (left) shows very limited coverage. This highlights a key point of the paper about the differing quality of data used to train the reinforcement learning model.", "section": "A.2 ExORL Datasets"}, {"figure_path": "79eWvkLjib/figures/figures_29_1.jpg", "caption": "Figure 6: Performance by dataset size. Aggregate IQM scores across all domains and tasks as RND size is varied. The performance delta between vanilla FB and the conservative variants increases as dataset size decreases.", "description": "This figure shows how the performance of different zero-shot reinforcement learning (RL) methods, specifically Forward-Backward (FB) and its conservative variants, changes with respect to dataset size.  The x-axis represents the dataset size (number of transitions), while the y-axis indicates the aggregate IQM (Interquartile Mean) return across multiple domains and tasks. It demonstrates that the performance gap between the vanilla FB method and its conservative counterparts increases as the dataset size decreases, highlighting the effectiveness of conservative approaches when dealing with smaller datasets.", "section": "3 Zero-Shot RL from Low Quality Data"}, {"figure_path": "79eWvkLjib/figures/figures_30_1.jpg", "caption": "Figure 1: Conservative zero-shot RL. (Left) Zero-shot RL methods must train on a dataset collected by a behaviour policy optimising against task zcollect, yet generalise to new tasks zeval. Both tasks have associated optimal value functions Qcollect and Q*eval for a given marginal state. (Middle) Existing methods, in this case forward-backward representations (FB), overestimate the value of actions not in the dataset for all tasks. (Right) Value-conservative forward-backward representations (VC-FB) suppress the value of actions not in the dataset for all tasks. Black dots () represent state-action samples present in the dataset.", "description": "This figure illustrates the core idea of the paper.  Zero-shot reinforcement learning aims to train an agent on a dataset of reward-free transitions, so it can perform any downstream task without further training.  The left panel shows the training and testing setting.  The middle panel shows how existing methods overestimate the value of out-of-distribution actions that were not seen in the dataset.  The right panel presents the paper's proposed solution: to suppress the overestimation of unseen actions, which improves the agent's performance on downstream tasks when trained on low-quality data.", "section": "3 Zero-Shot RL from Low Quality Data"}, {"figure_path": "79eWvkLjib/figures/figures_31_1.jpg", "caption": "Figure 2: FB value overestimation with respect to dataset size n and quality. Log Q values and IQM of rollout performance on all Maze tasks for datasets RND and RANDOM. Q values predicted during training increase as both the size and \"quality\" of the dataset decrease. This contradicts the low return of all resultant policies (note: a return of 1000 is the maximum achievable for this task). Informally, we say the RND dataset is \"high\" quality, and the RANDOM dataset is \"low\" quality-see Appendix A.2 for more details.", "description": "This figure shows how the performance of the Forward-Backward (FB) algorithm in zero-shot reinforcement learning is affected by the size and quality of the training dataset.  It demonstrates that as the dataset size decreases and quality reduces (fewer and less diverse state-action pairs), the predicted Q-values during training become overly optimistic, even though the resulting policies yield poor returns.  This highlights a significant challenge in zero-shot RL when dealing with limited data.", "section": "3.1 Failure Mode of Existing Methods"}, {"figure_path": "79eWvkLjib/figures/figures_32_1.jpg", "caption": "Figure 1: Conservative zero-shot RL. (Left) Zero-shot RL methods must train on a dataset collected by a behaviour policy optimising against task zcollect, yet generalise to new tasks zeval. Both tasks have associated optimal value functions Qcollect and Q*eval for a given marginal state. (Middle) Existing methods, in this case forward-backward representations (FB), overestimate the value of actions not in the dataset for all tasks. (Right) Value-conservative forward-backward representations (VC-FB) suppress the value of actions not in the dataset for all tasks. Black dots () represent state-action samples present in the dataset.", "description": "This figure illustrates the core concept of the paper.  Zero-shot reinforcement learning aims to train an agent on a dataset of reward-free transitions to perform any downstream task without further learning.  Panel (a) shows the general problem: training on zcollect data to predict Q*eval values for unseen tasks. Panel (b) illustrates existing methods (Forward-Backward representations) that tend to overestimate the value of unseen actions, while (c) demonstrates the proposed conservative approach (Value-Conservative Forward-Backward representations) which suppresses the estimated value of unseen actions. This correction improves performance, particularly on low-quality data.", "section": "3 Zero-Shot RL from Low Quality Data"}, {"figure_path": "79eWvkLjib/figures/figures_33_1.jpg", "caption": "Figure 2: FB value overestimation with respect to dataset size n and quality. Log Q values and IQM of rollout performance on all Maze tasks for datasets RND and RANDOM. Q values predicted during training increase as both the size and \"quality\" of the dataset decrease. This contradicts the low return of all resultant policies (note: a return of 1000 is the maximum achievable for this task). Informally, we say the RND dataset is \"high\" quality, and the RANDOM dataset is \"low\" quality-see Appendix A.2 for more details.", "description": "This figure shows how the performance of Forward-Backward (FB) zero-shot reinforcement learning degrades with smaller and lower-quality datasets.  It plots the predicted Q-values during training and the actual rollout performance on Maze tasks using two datasets: RND (high-quality) and RANDOM (low-quality).  The key takeaway is that FB overestimates the value of actions not seen during training, leading to poor performance, especially with smaller, lower-quality datasets.", "section": "3.1 Failure Mode of Existing Methods"}, {"figure_path": "79eWvkLjib/figures/figures_34_1.jpg", "caption": "Figure 2: FB value overestimation with respect to dataset size n and quality. Log Q values and IQM of rollout performance on all Maze tasks for datasets RND and RANDOM. Q values predicted during training increase as both the size and \"quality\" of the dataset decrease. This contradicts the low return of all resultant policies (note: a return of 1000 is the maximum achievable for this task). Informally, we say the RND dataset is \"high\" quality, and the RANDOM dataset is \"low\" quality-see Appendix A.2 for more details.", "description": "This figure shows how the performance of the Forward-Backward (FB) algorithm in zero-shot reinforcement learning is affected by the size and quality of the training dataset.  It demonstrates that as the dataset size decreases and quality diminishes (fewer and less diverse state-action pairs), the predicted Q-values during training become overestimated.  This overestimation is paradoxical because the resulting policies achieve far lower actual returns than expected, highlighting a critical failure mode of FB methods when trained on low-quality data.", "section": "3.1 Failure Mode of Existing Methods"}, {"figure_path": "79eWvkLjib/figures/figures_34_2.jpg", "caption": "Figure 14: VC-FB sensitivity to conservative budget \u03c4 on Walker and Maze. Top: RND dataset; bottom: RANDOM dataset. Maximum IQM return across the training run averaged over 3 random seeds", "description": "The figure shows the sensitivity analysis of the Value-Conservative Forward-Backward (VC-FB) method to the hyperparameter \u03c4 (conservative budget).  It presents the maximum Interquartile Mean (IQM) return achieved during training on two different environments, Walker and Maze, using two datasets: RND (high quality) and RANDOM (low quality).  The results show the impact of various values of \u03c4 on the algorithm's performance in these different experimental setups.  The top half illustrates results using the RND dataset and the bottom half shows the results using the RANDOM dataset.", "section": "F Learning Curves & Hyperparameter Sensitivity"}, {"figure_path": "79eWvkLjib/figures/figures_35_1.jpg", "caption": "Figure 2: FB value overestimation with respect to dataset size n and quality. Log Q values and IQM of rollout performance on all Maze tasks for datasets RND and RANDOM. Q values predicted during training increase as both the size and \"quality\" of the dataset decrease. This contradicts the low return of all resultant policies (note: a return of 1000 is the maximum achievable for this task). Informally, we say the RND dataset is \"high\" quality, and the RANDOM dataset is \"low\" quality-see Appendix A.2 for more details.", "description": "The figure shows how the performance of forward-backward (FB) representations in zero-shot reinforcement learning degrades as the size and quality of the training dataset decrease.  It highlights the issue of out-of-distribution (OOD) state-action value overestimation, where the model predicts high values for actions not seen during training, leading to poor actual performance.", "section": "3.1 Failure Mode of Existing Methods"}, {"figure_path": "79eWvkLjib/figures/figures_35_2.jpg", "caption": "Figure 1: Conservative zero-shot RL. (Left) Zero-shot RL methods must train on a dataset collected by a behaviour policy optimising against task zcollect, yet generalise to new tasks zeval. Both tasks have associated optimal value functions Qcollect and Q*eval for a given marginal state. (Middle) Existing methods, in this case forward-backward representations (FB), overestimate the value of actions not in the dataset for all tasks. (Right) Value-conservative forward-backward representations (VC-FB) suppress the value of actions not in the dataset for all tasks. Black dots () represent state-action samples present in the dataset.", "description": "This figure illustrates the core concept of the paper.  Zero-shot RL methods are trained on a dataset from one task (zcollect), and then they must generalize to new tasks (zeval) without additional training.  Existing methods (FB) tend to overestimate the value of actions not seen during training. The paper proposes a solution (VC-FB) that addresses this overestimation by suppressing the value of out-of-distribution actions. The figure shows the effect of this conservative approach.", "section": "3 Zero-Shot RL from Low Quality Data"}]