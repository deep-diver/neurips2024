[{"figure_path": "hF6vatntqc/figures/figures_35_1.jpg", "caption": "Figure 1: Architecture of the compared models. Each model contains two MLP components, all attention layers are single-head and LayerNorm is not included. (a),(b) implement the simplified reparametrization for attention, while all layers in (c) utilize the full embeddings. The input dimension is 8 and all hidden layer and DNN output widths are 32. The query prediction is read off the last entry of the output at the query position.", "description": "The figure shows the architecture of three different transformer models used in the paper's experiments.  Model (a) is a simplified linear model, (b) uses softmax attention, and (c) is a full transformer with two encoder layers. All models have two MLP components, single-head attention, and no layer normalization.  The input dimension is 8, hidden layer widths and DNN output widths are 32. The query prediction is extracted from the last element of the model output.", "section": "Numerical Experiments"}, {"figure_path": "hF6vatntqc/figures/figures_35_2.jpg", "caption": "Figure 2: Training and test curves for the ICL pretraining objective. We use the Adam optimizer with a learning rate of 0.02 for all layers. For the task class we take a = 1, p = q = \u221e, T = n = 512 and generate samples from random combinations of order 2 wavelets.", "description": "This figure shows the training and testing loss curves for three different transformer models during the pretraining phase of in-context learning. The models are: a linear model, a softmax model, and a full transformer model. The Adam optimizer was used with a learning rate of 0.02 for all layers. The task class parameters were set to a=1, p=q=\u221e, T=n=512. Samples were generated from random combinations of order 2 wavelets. The plot shows that all three models converge to a low training loss, demonstrating the effectiveness of the pretraining procedure.", "section": "E Numerical Experiments"}, {"figure_path": "hF6vatntqc/figures/figures_35_3.jpg", "caption": "Figure 3: Training and test losses of the three models after 50 epochs while varying (a) DNN width N; (b) number of in-context samples n; (c) number of tasks T. For (a), the widths of all hidden layers also vary with N. We take the median over 5 runs for robustness.", "description": "This figure shows the training and test losses of three different transformer models after 50 training epochs.  The models are compared under varying conditions: (a) changes in the width of the deep neural network (DNN) layer, (b) changes in the number of in-context examples, and (c) changes in the number of training tasks used for pretraining.  The results are shown as median values over 5 runs to highlight robustness.  The figure demonstrates how model performance is affected by these various parameters.", "section": "E Numerical Experiments"}]