[{"figure_path": "bTHFrqhASY/tables/tables_5_1.jpg", "caption": "Table 1: The results of InfLLM and baseline models on \u221e-Bench. The 95% quantile for text lengths in \u221e-Bench is 214K. The context window size for sliding window models refers to the local window size, and for InfLLM refers to \"local window size + selected memory size\".", "description": "This table presents the performance comparison of InfLLM against several baseline models on the \u221e-Bench benchmark.  It shows various metrics (R.PK, R.Num, R.KV, Choice, QA, Sum, Math.F, Avg.) for different models with varying context window sizes, highlighting InfLLM's ability to handle longer sequences effectively without additional training.  The \u221e-Bench benchmark is specifically designed to test long-context understanding capabilities, with the 95th percentile length being 214K tokens.", "section": "4 Experiments"}, {"figure_path": "bTHFrqhASY/tables/tables_6_1.jpg", "caption": "Table 2: The comparison between InfLLM and models with continual pre-training, Llama-3-8B-Instruct-Gradient-1048k (Llama-1M). InfLLM can achieve comparable performance with Llama-1M with less computation consumption and memory usage.", "description": "This table compares the performance of InfLLM and Llama-1M (a model with continual pre-training) on the \u221e-Bench benchmark.  It shows that InfLLM achieves comparable performance to Llama-1M while using significantly less VRAM and computation time.  The results highlight InfLLM's efficiency in achieving strong results without additional training.", "section": "4.4 Comparing to Models with Continual Training"}, {"figure_path": "bTHFrqhASY/tables/tables_7_1.jpg", "caption": "Table 3: The comparison between InfLLM and RAG.", "description": "This table compares the performance of InfLLM and RAG (Retrieval Augmented Generation) on three context retrieval tasks.  It shows that InfLLM, even without additional training or data, outperforms RAG across all three tasks, highlighting its superior generalization capabilities.  The tasks are represented by R.PK, R.Num, and R.KV.", "section": "4.5 Comparing to Retrieval-Augmented Generation"}, {"figure_path": "bTHFrqhASY/tables/tables_8_1.jpg", "caption": "Table 4: The results for ablation study.", "description": "This table presents the ablation study results for the InfLLM model. It compares the performance of the full InfLLM model against variations where either only the decoding step uses memory lookup or where memory lookup is completely omitted.  It also includes a comparison with a version using average representations instead of representative tokens. The results highlight the importance of both encoding and decoding memory lookups and the effectiveness of the chosen representative token method.", "section": "4.7 Ablation Study"}, {"figure_path": "bTHFrqhASY/tables/tables_15_1.jpg", "caption": "Table 1: The results of InfLLM and baseline models on \u221e-Bench. The 95% quantile for text lengths in \u221e-Bench is 214K. The context window size for sliding window models refers to the local window size, and for InfLLM refers to \"local window size + selected memory size\".", "description": "This table presents the performance comparison of InfLLM against various baseline models on the \u221e-Bench benchmark.  It shows the results for different models across various metrics, including the context window size used.  The 95th percentile length of text in the benchmark is 214K tokens, highlighting the challenge of processing long sequences.  The table breaks down results by model type (Mistral-based, Llama-3-based) and shows metrics relevant to question answering, summarization, and mathematical reasoning.  InfLLM's context window size includes both the local window and the size of the selected memory units.", "section": "4 Experiments"}, {"figure_path": "bTHFrqhASY/tables/tables_16_1.jpg", "caption": "Table 6: The results of Vicuna-based models.", "description": "This table presents the performance comparison between the original Vicuna model and InfLLM on four different tasks: Recall@1 (R.PK), Recall@N (R.Num), Recall@KV (R.KV), and Math.F.  The results show that InfLLM significantly improves performance over the original Vicuna model, especially on R.PK and R.Num.  This highlights InfLLM's ability to effectively extend the context length of the model, even for models with smaller original context windows.", "section": "C.3 Experiments on Vicuna"}, {"figure_path": "bTHFrqhASY/tables/tables_17_1.jpg", "caption": "Table 7: The combination of InfLLM and models with continual pre-training, Yi-9B-200K (Yi-200K).", "description": "This table shows the results of combining InfLLM with pre-trained models (Yi-9B-200K).  It compares the performance of the Yi-200K model alone to the performance when InfLLM is added. The metrics used for comparison include Recall@P (R.PK), Recall@Num (R.Num), Recall@KV (R.KV), Choice, QA, Sum, and Math.F, all of which assess different aspects of the model's ability to process long sequences. This demonstrates how InfLLM can improve the performance of models already trained on extensive datasets.", "section": "C.4 Combination of InfLLM with Yi-200K"}]