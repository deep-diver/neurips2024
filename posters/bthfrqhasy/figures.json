[{"figure_path": "bTHFrqhASY/figures/figures_3_1.jpg", "caption": "Figure 1: The illustration of InfLLM. Here, the current tokens refer to tokens that need to be encoded in the current computation step. The past key-value vectors can be divided into the initial tokens, evicted tokens, and local tokens, arranged the furthest to the nearest relative to the current tokens. For each computation step, the context window consists of the initial tokens, relevant memory units, and local tokens.", "description": "This figure illustrates the InfLLM architecture, showing how it combines a sliding window attention mechanism with an external context memory.  The input sequence is processed chunk-by-chunk, with past key-value vectors stored in the context memory.  Relevant information from the memory is retrieved (lookup) and combined with the current tokens to form the context window for attention computation.  This allows InfLLM to process long sequences efficiently.", "section": "3 Methodology"}, {"figure_path": "bTHFrqhASY/figures/figures_7_1.jpg", "caption": "Figure 2: Extra studies about InfLLM. Here, (a), (b), and (c) investigate the impact of the context memory under different numbers of representative tokens, different numbers of selected units, and memory unit sizes, respectively.", "description": "This figure shows three sub-figures that analyze the impact of different hyperparameters on InfLLM's performance.  Subfigure (a) shows performance variation with different numbers of representative tokens used for context memory. Subfigure (b) illustrates how performance changes with different numbers of selected units from the context memory. Finally, subfigure (c) displays the performance change according to the context memory unit size.", "section": "4.6 The Impact of Memory Settings"}, {"figure_path": "bTHFrqhASY/figures/figures_8_1.jpg", "caption": "Figure 3: The results on sequences with different lengths.", "description": "The figure shows the performance of InfLLM and LM-Infinite on the Retrieve.PassKey task with varying sequence lengths.  InfLLM maintains high accuracy (around 100%) even as the sequence length increases to 1024K tokens, demonstrating its ability to capture long-distance dependencies. In contrast, LM-Infinite's accuracy drastically decreases as the sequence length grows, highlighting the limitations of discarding distant contexts.", "section": "4.8 Scaling to 1,024K Context"}, {"figure_path": "bTHFrqhASY/figures/figures_14_1.jpg", "caption": "Figure 4: Missing rates of different cache management strategies.", "description": "This figure shows the missing rates of different cache management strategies (LRU, random, FIFO) as a function of score decay.  The LRU (Least Recently Used) strategy consistently demonstrates the lowest missing rate across various score decay values, highlighting its effectiveness in managing memory units for efficient long-context processing.  The random and FIFO (First-In, First-Out) strategies exhibit significantly higher missing rates, indicating their inferiority compared to LRU.", "section": "A Cache Management Strategy"}]