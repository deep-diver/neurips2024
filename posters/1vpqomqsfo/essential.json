{"importance": "This paper is crucial for researchers working on **uncertainty quantification** in deep learning, particularly those dealing with **high-dimensional models**. It offers a novel, memory-efficient solution for computing uncertainty scores, which is a significant challenge in the field. The proposed method, based on **sketched Lanczos algorithms**, opens new avenues for improving the scalability and reliability of uncertainty estimation in various applications, enabling researchers to tackle larger, more complex models.  The approach's **provable error bounds** and **empirical improvements** over existing methods make it a valuable contribution with practical implications.", "summary": "SLU: a novel, low-memory uncertainty score for neural networks, achieves logarithmic memory scaling with model parameters, providing well-calibrated uncertainties and outperforming existing methods.", "takeaways": ["SLU offers a memory-efficient uncertainty score for neural networks.", "SLU's memory usage scales logarithmically with the number of model parameters.", "SLU outperforms existing uncertainty quantification methods in low-memory regimes."], "tldr": "Current uncertainty quantification methods are computationally expensive and memory-intensive, particularly for deep neural networks with high parameter counts. This prevents their practical use in many applications. Existing methods either train multiple models or approximate the Fisher information matrix, both of which are limited in scalability. The high memory cost is a major bottleneck.  This paper addresses these issues by introducing a novel uncertainty score, called SLU (Sketched Lanczos Uncertainty). \nSLU combines Lanczos' algorithm with dimensionality reduction techniques. It computes a sketch of the leading eigenvectors of the Fisher information matrix using a small memory footprint. This approach only requires logarithmic memory, in contrast to existing techniques requiring linear memory. Empirically, SLU demonstrates well-calibrated uncertainties, reliably identifies out-of-distribution samples, and significantly outperforms existing methods in terms of accuracy while consuming considerably less memory. The algorithm's error is provably bounded and the improved performance is shown in various experiments.", "affiliation": "Technical University of Denmark", "categories": {"main_category": "Machine Learning", "sub_category": "Deep Learning"}, "podcast_path": "1vPqOmqSfO/podcast.wav"}