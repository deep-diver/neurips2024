[{"type": "text", "text": "Network Lasso Bandits ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 We consider a multi-task contextual bandit setting, where the learner is given a   \n2 graph encoding relations between the bandit tasks. The tasks\u2032 preference vectors   \n3 are assumed to be piecewise constant over the graph, forming clusters. At every   \n4 round, we estimate the preference vectors by solving an online network lasso   \n5 problem with a suitably chosen, time-dependent regularization parameter. We   \n6 establish a novel oracle inequality relying on a convenient restricted eigenvalue   \n7 assumption. Our theoretical findings highlight the importance of dense intra-cluster   \n8 connections and sparse inter-cluster ones. That results in a sublinear regret bound   \n9 significantly lower than its counterpart in the independent task learning setting.   \n0 Finally, we support our theoretical findings by experimental evaluation against   \n1 graph bandit multi-task learning and online clustering of bandits algorithms. ", "page_idx": 0}, {"type": "text", "text": "121 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "13 Online commercial websites aim to properly recommend their products to their customers, and the   \n14 performance of these recommendations depends on the knowledge of users\u2019 preferences. Unlike   \n15 traditional collaborative-filtering-based methods [Su and Khoshgoftaar, 2009], such knowledge is   \n16  initially unavailable. Therefore, the online recommender systems need to recommend various items   \n17 to the users and observe their ratings to explore their preferences. At the same time, the recommender   \n18 system should be able to recommend items that attract users\u2019 attention and receive high ratings by   \n19 exploiting the learned knowledge. The contextual bandits frameworks [Li et al., 2010] have been   \n20 popularly used to formalize and address this exploration-exploitation trade-off.   \n21 However, the classical form of contextual bandits [Li et al., 2010, Chu et al., 2011, Abbasi-Yadkori   \n22 et al., 2011] ignores the availability of social networks amongst users and solves the problem for   \n23 each user separately. Consequently, such algorithms have some drawbacks when applied to problems   \n24 with a large number of users. First, such a large number hinders the computational efficiency of   \n25 such algorithms. Second, the partial feedback of the bandit settings exposes the algorithms to have   \n26 weak estimations and impair their decision-making ability [Yang et al., 2020]. Consequently, to   \n27 improve bandit algorithms' performance for large-scale applications, structural assumptions that link   \n28  the different users are usually integrated within bandit algorithms [Cesa-Bianchi et al., 2013, Gentile   \n29 et al., 2014, Li et al., 2019, Herbster et al., 2021].   \n30  The papers of Cesa-Bianchi et al. [2013], Yang et al. [2020] attempt to integrate the prior knowledge of   \n31 social networks into their contextual bandit algorithms. Both papers proposed UCB-style algorithms   \n32  and exhibited the importance of using the social network graph to achieve lower regrets using   \n33 Laplacian regularization. Consequently, both methods promote smoothness among the preference   \n34 vectors of users in order to transfer the collected information between them. However, the Laplacian   \n35  regularization does not account for the smoothness heterogeneity introduced by a piecewise constant   \n36  behavior over the graph [Wang et al., 2016]. On the other hand, algorithms of online clustering of   \n37 bandits [Gentile et al., 2014, Li et al., 2019] start from a graph and gradually add or remove edges to   \n38  form clusters as connected components. However, their clustering can cause overconfidence in the   \n39 constructed clusters, potentially leading to error accumulation.   \n40 In this paper, we assume access to a graph encoding relations between bandit tasks, and that the task   \n41 parameter vectors are piecewise constant over the graph. That means that tasks form clusters. We   \n42 propose an algorithm that integrates the prior knowledge of the piecewise constant structure to update   \n43 tasks rather than finding the clusters explicitly. That way, we mitigate the limitations mentioned   \n44 above: the piecewise constant smoothness is naturally integrated into our regularizer, and we do not   \n45  estimate the clusters so our algorithm does not suffer from overconfidence drawbacks. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "46  More precisely, we provide the following contributions ", "page_idx": 1}, {"type": "text", "text": "47\u00b7 We analyze an instance of the Network Lasso problem [Hallac et al., 2015], where every vertex's   \n48 preference vector is estimated using data generated during the interaction between users and the   \n49 bandit. We provide the first oracle inequality in this setting and link it to fundamental quantities   \n50 characterizing the relation between the graph and the true preference vectors of the users. Our   \n51 result relies on our novel restricted eigenvalue (RE) condition, which we assume for our setting.   \n52 This result is of independent interest and can be applied to independently generated data as a   \n53 special case.   \n54 \u00b7 We prove how the empirical multi-task Gram matrix of the data inherits the RE condition from   \n55 its true counterpart. Both this result and the previous one depend on the sparsity of inter-cluster   \n56 connections and the density of intra-cluster ones.   \n57 \u00b7 We provide a regret upper bound for our setting. Our bound highlights the advantage of our   \n58 algorithm in high dimensional settings, and for large graphs.   \n59 \u00b7 We support our theoretical findings by extensive numerical experiments on simulated data that   \n60 prove the advantage of our algorithm compared to other approaches used for online clustering of   \n61 bandits.   \n62 The rest of the paper is organized as follows. Section 2 discusses the relation of our work to the   \n63 literature. We formulate our problem and state some of our assumptions in Section 3, then we present   \n64  our bandit algorithm in Section 4. We analyze the problem theoretically in Section 5, and finally, we   \n65 demonstrate its practical interest via numerical experiments in Section 6. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "662Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "67 Lasso contextual bandits  To address the high dimensional setting for linear bandits, several multi  \n68 armed bandit papers solve a LASSO [Tibshirani, 1996] problem under different assumptions [Bastani   \n69 and Bayati, 2019, Kim and Paik, 2019, Oh et al., 2021, Ariu et al., 2022]. They all rely on a previously   \n70 established compatibility or RE condition [Buhlmann and van de Geer, 2011], that they adapt to the   \n71 non-i.i.d case. Such assumptions were also used in the multi-task setting by Cella and Pontil [2021]   \n72 with a Group Lasso regularization [Yuan and Lin, 2006], and to impose a low rank structure on the   \n73 task preference vectors in Cella et al. [2023]. In our case, we provide a novel oracle inequality, rather   \n74  than just generalize an existing one to the non-i.i.d setting, with a newly introduced RE assumption.   \n75  Clustering of bandits Sequentially clustering bandit tasks was introduced in Gentile et al. [2014]   \n76  with CLUB algorithm. In CLUB, starting with a fully connected graph, an iterative graph learning pro  \n77 cess is performed, where edges between users are deleted if their preference vectors are significantly   \n78 different. As a result, any connected component is seen as a cluster and only one recommendation per   \n79 cluster is developed. In another work, Li et al. [2019] generalize the setting of Gentile et al. [2014]   \n80 and address its limitations via including merging operations in addition to splitting. In contrast to   \n81 these approaches, the algorithm in Nguyen and Lauw [2014] groups users via K-means clustering,   \n82 and the algorithm in Cheng et al. [2023] relies on hedonic games for online clustering of bandits.   \n83 Furthermore, Yang and Toni [2018] make use of community detection techniques on graphs to find   \n84 user clusters. Gentile et al. [2017] study the clustering of the contextual bandit problem where their   \n85 proposed algorithm, named CAB, adaptively matches user preferences in the face of constantly   \n86 evolving items. Our work fundamentally differs from the previous ones on two aspects. First, we   \n87 assume access to a graph encoding relations between users, which is more informative than a complete   \n88 graph. Second, we do not keep track of a model for each cluster, but rather we integrate a prior over   \n89 the graph via a graph total variation regularizer that enforces a piecewise constant behaviour for the   \n90 estimated preference vectors.   \n91 Multi-task learning  Several contributions assume some underlying structure that links the bandit   \n92 tasks. In Cella and Pontil [2021], task preference vectors are assumed to be sparse and to share their   \n93 sparsity support, implying that they lie in a low-dimensional subspace with dimensions aligning with   \n94 the canonical basis vectors. This idea is further generalized in Cella et al. [2023], where the tasks   \n95 are assumed to be confined to an arbitrary unknown low-dimensional subspace. That work improves   \n96 upon Hu et al. [2021] by not requiring the knowledge of the small dimenson of the task space. The   \n97 underlying structure linking tasks can also be a graph encoding relations between them [Cesa-Bianchi   \n98  et al., 2013, Yang and Toni, 2018], which is our case. However, while they assume smoothness as a   \n99prior, we assume piecewise constant behavior. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "100 3Problem setting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "101 We consider a linear bandit setting, with a finite number of tasks representing users in a recommenda  \n102 tion system for example. For each task the agent has to choose among $K$ arms, each associated to a   \n103 $d$ -dimensional context vector. All interactions over a horizon of $T$ time steps. We further assume   \n104that we have access to an undirected graph $\\mathcal{G}=(\\mathcal{V},\\mathcal{E})$ , with vertex set $\\mathcal{V}$ representing the tasks   \n105and edge set $\\mathcal{E}$ encoding the relationships between them. We identify the vertex set $\\mathcal{V}$ with the set   \n106 of vertex indices $[|\\protect\\gamma|]$ . Thus, we consider $\\mathcal{E}$ to be a subset of $\\mathcal{V}^{2}$ , where every edge $(m,n)\\,\\in\\,\\mathcal{E}$   \n107 has weight $w_{m n}>0$ , with $m<n$ . The tasks\u2019 preference vectors are denoted by $\\{\\pmb{\\theta}_{m}\\}_{m\\in\\mathcal{V}}\\subset\\mathbb{R}^{d}$   \n108 verifying $\\|\\pmb{\\theta}_{m}\\|\\leq1\\,\\forall m\\in\\mathcal{V}$ , which we concatenate as row vectors into matrix $\\Theta\\in\\mathbb{R}^{|\\mathcal{V}|\\times d}$ . The   \n109 latter represents a graph vector signal, assumed to be piecewise constant over $\\mathcal{G}$   \n110 At a round $t\\in\\mathbb{N}^{\\star}$ , a user $m(t)\\in\\mathcal{V}$ is selected uniformly at random and served an arm with context   \n111 vector ${\\bf x}(t)$ from a finite action set $\\mathcal{A}(t)\\subset\\mathbb{R}^{d}$ with size $K$ , depending on their estimated preference   \n112  vector $\\widehat{\\pmb{\\theta}}_{m(t)}(t)\\in\\mathbb{R}^{d}$ . We assume the expected reward to be linear, with an additive, $\\sigma$ -sub-Gaussian   \n113 noise conditionally on the past. Formally, denoting by ${\\mathcal{F}}_{0}$ the trivial sigma-algebra, and for all $t\\geq1$   \n114 by $\\mathcal{F}_{t}$ the sigma-algebra generated by history set $\\{\\stackrel{.}{m}(1),\\mathbf{x}(1),y(1),\\cdots,m(t),\\mathbf{x}(t),y(t),m(t\\!+\\!1)\\}$   \n115 the received reward $y(t)$ is given by $y(t)=\\big\\langle\\pmb{\\theta}_{m(t)}(t),\\mathbf{x}(t)\\big\\rangle+\\eta(t)$ , where $\\eta(t)$ is $\\mathcal{F}_{t}$ -measurable   \n116and ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\eta(t)|\\mathcal{F}_{t-1}\\right]=0,\\quad\\quad\\mathbb{E}\\left[\\exp(s\\eta(t))|\\mathcal{F}_{t-1}\\right]\\le\\exp\\left(\\frac{1}{2}\\sigma^{2}s^{2}\\right)\\quad\\forall t\\ge1,\\forall s\\in\\mathbb{R}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "117 At the end of a round $t$ all preference vectors are updated into a new estimation $\\hat{\\Theta}(t)$ while leveraging   \n118 the structure of graph $\\mathcal{G}$ , formally by solving the following optimization problem: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{\\Theta}(t)=\\underset{\\tilde{\\Theta}\\in\\mathbb{R}^{|\\nu|\\times d}}{\\arg\\operatorname*{min}}\\,\\frac{1}{2t}\\sum_{\\tau=1}^{t}\\Big(\\Big\\langle\\tilde{\\theta}_{m(\\tau)},\\mathbf{x}(\\tau)\\Big\\rangle-y(\\tau)\\Big)^{2}+\\alpha(t)\\sum_{(m,n)\\in\\mathcal{E}}w_{m n}\\Big\\|\\tilde{\\theta}_{m}-\\tilde{\\theta}_{n}\\Big\\|,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "119where $\\left\\Vert\\cdot\\right\\Vert$ denotes the Euclidean norm for vectors. The performance of our policy is assessed by the   \n120 expected regret over the $T$ interaction rounds for all tasks: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{R}(T)=\\mathbb{E}\\left[\\sum_{t=1}^{T}\\left\\langle\\pmb{\\theta}_{m(t)},\\mathbf{x}^{\\star}(t)-\\mathbf{x}(t)\\right\\rangle\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "121 where $\\begin{array}{r}{\\mathbf{x}^{\\star}(t)\\in\\arg\\operatorname*{max}_{\\tilde{\\mathbf{x}}\\in\\mathcal{A}(t)}\\big\\langle\\pmb{\\theta}_{m(t)},\\tilde{\\mathbf{x}}\\big\\rangle.}\\end{array}$ ", "page_idx": 2}, {"type": "text", "text": "122 The Optimization problem in (2) is an instance of the Network Lasso [Hallac et al., 2015]. Other   \n123 instances of the same type were studied by Jung et al. [2018], Jung and Vesselinova [2019], Jung   \n124 [2020]. The objective is characterized by its second term that, while being just the Laplacian   \n125 regularization without squaring the norms, promotes a piecewise constant behavior rather than   \n126 smoothness. For real-valued signals $(d=1)$ ), this regularization has been extensively studied for   \n127 image and graph signal denoising, for the problem of trend filtering on graphs [Wang et al., 2016].   \n128 According to Wang et al. [2016], that regularization better adapts to the heterogeneity of smoothness   \n129  of the signal and induces a cluster structure in the data: similar users will not only have similar   \n130 models but the same model, which offers a compression of the overall model over the graph. Note   \n131 that our setting is cluster agnostic; our algorithm does not aim to learn the cluster structure explicitly   \n132 but to exploit it implicitly using the total variation semi-norm as regularization. The latter's strength   \n133 is controlled via a time-dependent regularization coefficient $\\alpha(t)$ , which we will express later in the   \n134 analysis. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "135 We formalize our assumption on the context generation as follows. ", "page_idx": 3}, {"type": "text", "text": "136 Assumption 1 (i.i.d action sets). Context sets $\\{\\mathcal{A}(t)\\}_{t=1}^{T}$ are generated i.i.d. from a distribution $p$   \n137over $\\mathbb{R}^{K\\times d}$ suchthat $\\|\\mathbf{x}\\|\\leq1\\forall\\,\\mathbf{x}\\in A(t)\\,\\forall t\\geq1$ ", "page_idx": 3}, {"type": "text", "text": "138 In addition to the i.i.d assumption, we assume more regularity. ", "page_idx": 3}, {"type": "text", "text": "139  Assumption 2 (Relaxed symmetry and balanced covariance). There exists a constant $\\nu\\geq1$ such   \n140that for all $\\mathbf{X}\\,\\in\\,\\mathbb{R}^{K\\times d}$ \uff0c $p(-\\mathbf{X})\\,\\leq\\,\\nu p(\\mathbf{X})$ Furthermore,thereexists $\\omega\\,>\\,0$ suchthatforany   \n141permutation $\\left(a_{1},\\cdot\\cdot\\cdot,a_{K}\\right)$ of $[K],$ for any $i\\in\\{2,\\cdots\\,,K-1\\}$ , and for any $\\mathbf{w}\\in\\mathbb{R}^{d}$ , we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\mathbf{x}_{a_{i}}\\mathbf{x}_{a_{i}}^{\\top}[\\mathbf{w}^{\\top}\\mathbf{x}_{a_{1}}<\\cdots<\\mathbf{w}^{\\top}\\mathbf{x}_{a_{K}}]\\right]\\prec\\omega\\mathbb{E}\\left[(\\mathbf{x}_{a_{1}}\\mathbf{x}_{a_{1}}^{\\top}+\\mathbf{x}_{a_{K}}\\mathbf{x}_{a_{K}}^{\\top})[\\mathbf{w}^{\\top}\\mathbf{x}_{a_{1}}<\\cdot\\cdot<\\mathbf{w}^{\\top}\\mathbf{x}_{a_{K}}]\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "142where $\\mathbf{M}\\preccurlyeq\\mathbf{N}$ means that $\\mathbf{N}-\\mathbf{M}$ is a PSD matrix. ", "page_idx": 3}, {"type": "text", "text": "143 This assumption was introduced in Oh et al. [2021], and has already been used in a multi-task setting   \n144 by Cella et al. [2023]. Parameter $\\nu$ controls the skewness, as $\\nu=1$ corresponds to a symmetric   \n145 distribution. $\\omega$ decreases with increasing positive correlation between arms. It verifies $\\dot{\\omega}=O(1)$   \n146 for multi-variate Gaussians and uniform distributions over the unit sphere [Oh et al., 2021]. The   \n147 piecewise constant behaviour of the graph signal $\\Theta$ is formalized in the next assumption.   \n148Assumption 3 (Piecewise constant signal). There exists a partition $\\mathcal{P}$ of $\\nu$ such that for any cluster   \n149 $\\mathcal{C}\\in\\mathcal{P}$ ,signal $\\Theta$ is constant on $\\mathcal{C}$ and the graph obtained by taking the vertices in $\\mathcal{C}$ and the edges   \n150linking them is connected.   \n151 Assumption 3 basically states that the true preference vectors are clustered and that the given graph   \n152 induces the cluster structure. It is required for our approach to be beneficial, as we will detail in the   \n153 analysis section. For the sake of clarity, we defer the statement of other technical assumptions to   \n154Section5. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "1554Algorithm ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "156  Our policy in Algorithm 1 follows a greedy arm selection rule in a multi-task setting, in the same   \n157 vein as those presented in Oh et al. [2021], Cella et al. [2023]. Indeed, as pointed out in Oh et al.   \n158  [2021], exploration is implicitly incorporated into regularization parameter $\\alpha(t)$ 's time dependence.   \n159 It has the following expression ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\alpha(t):=\\frac{\\alpha_{0}\\sigma}{t}\\sqrt{t+\\sqrt{2\\sum_{m\\in\\mathcal{V}}\\left|T_{m}(t)\\right|^{2}\\log\\frac{1}{\\delta(t)}}+2\\operatorname*{max}_{m\\in\\mathcal{V}}\\left|T_{m}(t)\\right|\\log\\frac{1}{\\delta(t)}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "160 where the set of time steps a task $m$ has been selected up to time $t$ is denoted by $\\mathcal{T}_{m}(t)$ ", "page_idx": 3}, {"type": "text", "text": "161 5 Analysis ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "162 This section provides the main steps of the analysis. One of the paper's contribution lies in finding an   \n163 oracle inequality of the network lasso problem given a restricted eigenvalue condition holding for the   \n164 true multi-task Gram matrix. In this regard, the next major challenge and contribution is to show that   \n165 the empirical multi-task Gram matrix, estimated in the algorithm, satisfies the restricted eigenvalue   \n166 condition. We start by proving an oracle inequality for the estimation error of $\\Theta$ , assuming that the   \n167 condition given by Definition 2 is verified by the empirical data Gram matrix. Then, we prove that the   \n168 latter assumption actually holds with high probability given that true multi-task Gram matrix satisfies   \n169it. Our final contribution in this work is the establishment of a regret bound for our algorithm. ", "page_idx": 3}, {"type": "text", "text": "170  5.1  Notation and technical assumptions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "171 We provide additional notations required for the analysis. We denote by $\\partial\\mathcal{P}$ the set of all edges in   \n172 $\\mathcal{E}$ connecting vertices from different clusters from partition $\\mathcal{P}$ (Assumption 3), and we call it the ", "page_idx": 3}, {"type": "text", "text": "Algorithm 1: Network Lasso Policy ", "text_level": 1, "page_idx": 4}, {"type": "image", "img_path": "WxW4nZMD3D/tmp/6a300620dfa9f60b433ec80bef4266eb5a70343b494b60dd3a859b514787c23c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "173  boundary of $\\mathcal{P}$ . Thus, $\\partial\\mathcal{P}^{c}$ , the complementary set of $\\partial\\mathcal{P}$ , is formed by edges connecting vertices of   \n174  the same cluster. The total weight of the boundary, i.e.the sum of its edges\u2032 weights, is referred to as   \n175 $w(\\partial{\\mathcal{P}})$ . Given a signal $\\mathbf{Z}\\in\\mathbb{R}^{|\\bar{\\mathcal{V}}|\\times d}$ we denote by $\\mathbf{Z}_{\\mathcal{P}}$ the signal obtainedby setting row vectors of $\\mathbf{Z}$   \n176 to their mean-per-cluster value w.r.t. $\\mathcal{P}$ . For any edge subset $I\\in{\\mathcal{E}}$ , we denote the following norms:   \n177 $\\left\\|\\cdot\\right\\|_{F}$ as the Frobenius norm, $\\|\\mathbf{z}\\|_{\\mathbf{M}}=\\sqrt{\\mathbf{z}^{\\top}\\mathbf{M}\\mathbf{z}}$ as the weighted norm of vector $\\textbf{z}\\in\\mathbb{R}^{d}$ induced   \n178 by matrix $\\mathbf{M}\\in\\mathbb{R}^{d\\times d}$ and $\\begin{array}{r}{\\|\\Theta\\|_{I}:=\\sum_{(m,n)\\in I}w_{m n}\\|\\pmb{\\theta}_{m}-\\pmb{\\theta}_{n}\\|}\\end{array}$ as the total variatio semi-norm   \n179 of $\\Theta\\in\\mathbb{R}^{|\\mathcal{V}|\\times d}$ over $I$ . Thus, the regularization term of Problem (2) is equal to $\\|\\Theta\\|_{\\mathcal{E}}$ . Also, we   \n180 define the incidence matrix $\\mathbf{B}_{I}\\subset\\mathbb{R}^{|\\mathcal{E}|\\times|\\mathcal{V}|}$ restricted to $I\\subseteq\\mathcal{E}$ to be null except at rows with index   \n181 $i\\in I$ corresponding to edge $(m,n)$ , where it equals $w_{m n}(\\mathbf{e}_{m}-\\mathbf{e}_{n})$ , where $\\mathbf{e}_{m}$ is the $m^{\\mathrm{th}}$ canonical   \n182 basis vector of $\\mathbb{R}^{|\\mathcal{V}|}$ We define $\\mathbf{A}\\nu(t):=\\operatorname{diag}\\left(\\mathbf{X}_{1}(t)^{\\top}\\mathbf{X}_{1}(t),\\ldots,\\mathbf{X}_{|\\mathcal{V}|}(t)^{\\top}\\mathbf{X}_{|\\mathcal{V}|}(t)\\right)\\in\\mathbb{R}^{d|\\mathcal{V}|\\times d|\\mathcal{V}|}$ \uff0c   \n183 and subsequently the empirical multi-task Gram matrix up to time step $t$ is given by $\\begin{array}{r}{\\frac{1}{t}\\mathbf{A}_{\\mathcal{V}}(t)}\\end{array}$ . The   \n184  following definition introduces quantities related to the clusters defined by partition $\\mathcal{P}$ , with crucial   \n185 roles that we will elucidate throughout the analysis. ", "page_idx": 4}, {"type": "text", "text": "186Definition 1 (Cluster content constants). Let $\\mathcal{C}\\in\\mathcal{P}$ beacluster. ", "page_idx": 4}, {"type": "text", "text": "187\u00b7 We denote by $\\partial_{v}{\\mathcal{C}}$ the inner boundary of $\\mathcal{C}$ i.e.the vertices of $\\mathcal{C}$ that are connected to its comple  \n188 mentary. We define the inner isoperimetric ratio of $\\mathcal{C}$ as $\\begin{array}{r}{\\iota_{\\mathcal{G}}(\\mathcal{C}):=\\frac{|\\partial_{v}\\mathcal{C}|}{|\\mathcal{C}|}}\\end{array}$   \n189 \u00b7 By abuse of notation, we denote as $\\mathbf{B}_{\\mathcal{C}}$ the incidence matrix restricted to edges linking vertices   \n190 of C, its associated Laplacian matrix by $\\mathbf{L}_{\\mathcal{C}}\\,:=\\,\\mathbf{B}_{\\mathcal{C}}^{\\top}\\mathbf{B}_{\\mathcal{C}}$ ,and its pseudo-inverse by $\\mathbf{L}_{\\mathcal{C}}^{\\dagger}$ .The   \n110 $\\mathcal{C}$ $\\begin{array}{r}{c_{\\mathcal{G}}(\\mathcal{C}):=\\operatorname*{min}_{m\\in\\mathcal{C}}(\\mathbf{L}_{\\mathcal{C}}^{\\dagger})_{m m}^{-1}}\\end{array}$ $m\\in\\mathcal{C}_{\\mathit{w.r.t}}\\mathcal{C}$ $(\\mathbf{L}_{\\mathcal{C}}^{\\dagger})_{m m}^{-1}$   \n193 The inner isoperimetric ratio of a cluster measures how many \u201cinterior\u2019 nodes a cluster contains, in   \n194  the sense that they are not connected to its complementary. It is at most equal to the isoperimetric ratio   \n195 for weightless graphs as the size of the inner boundary is at most equal to that of the edge boundary,   \n196 the latter being connected to the algebraic connectivity via the Cheeger inequality [Cheeger, 1970].   \n197 The topological centrality index measures the overall connectedness of a vertex in a network and   \n198  indicates how robust a node is to edge failures [Ranjan and Zhang, 2013]. Also, it can be tied to   \n199 electricity spreading in a network according to Van Mieghem et al. [2017]. We refer the interested   \n200 reader to the two previously mentioned works for a detailed account of the properties of the topological   \n201 centrality index. In the appendix, we show that for binary weights graphs the minimum topological   \n202 centrality index is at least equal to the algebraic connectivity theoretically and experimentally, where   \n203 we showcase that the difference between the two can be significant.   \n204  To proceed, we will need the following definition that introduces several notations to reduce the   \n205clutter.   \n206 Defnition 2 Resticted Eigenvalue RE) condiion and nom). Lt $\\{\\mathbf{M}_{i}\\}_{i=1}^{|\\mathcal{V}|}\\subset\\mathbb{R}^{d\\times d}$ be a stof   \n207 positive semi-definite matrices. We say that the matrix $\\mathbf{M}_{\\mathcal{V}}:=\\mathrm{diag}(\\mathbf{M}_{1},\\cdot\\cdot\\cdot\\,,\\mathbf{M}_{|\\mathcal{V}|})$ verifies the   \n208 restricted eigenvalue condition with constants $\\kappa\\geq0$ and $\\phi>0$ f ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\phi^{2}\\|\\mathbf{Z}\\|_{\\mathrm{RE}}^{2}\\leq\\sum_{i\\in\\mathcal{V}}\\|\\mathbf{z}_{i}\\|_{\\mathbf{M}_{i}}^{2}\\quad\\forall\\mathbf{Z}\\in S\\,w i t h\\,r o w s\\,\\{\\mathbf{z}_{i}\\}_{i\\in\\mathcal{V}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "209 where $\\boldsymbol{S}$ is the cone defined by: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{S}:=\\{\\mathbf{Z}\\in\\mathbb{R}^{|\\hat{\\mathcal{V}}|\\times d};a_{1}(\\mathcal{G},\\Theta)\\|\\mathbf{Z}\\|_{\\partial\\mathcal{P}^{c}}\\leq a_{2}(\\mathcal{G},\\Theta)\\left\\|\\overline{{\\mathbf{Z}}}_{\\mathcal{P}}\\right\\|_{F}+(1-\\kappa)^{+}\\|\\mathbf{Z}\\|_{\\partial\\mathcal{P}}\\},}\\\\ &{a_{1}(\\mathcal{G},\\Theta):=1-\\displaystyle\\frac{\\frac{1}{\\alpha_{0}}+2\\kappa w(\\partial\\mathcal{P})}{\\underset{c\\in\\mathcal{P}}{\\operatorname*{min}}\\sqrt{c_{\\mathcal{G}}(\\mathcal{C})}},\\quad a_{2}(\\mathcal{G},\\Theta):=\\displaystyle\\frac{1}{\\alpha_{0}}+\\sqrt{2}\\kappa w(\\partial\\mathcal{P})\\operatorname*{max}_{c\\in\\mathcal{P}}\\sqrt{\\iota_{\\mathcal{G}}(\\mathcal{C})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "210 and the RE semi-norm is defined by $\\left\\|\\mathbf{Z}\\right\\|_{\\mathrm{RE}}:=\\left\\|\\overline{{\\mathbf{Z}}}_{\\mathcal{P}}\\right\\|_{F}\\vee(1-\\kappa)^{+}\\biggl\\|\\mathbf{B}_{\\partial\\mathcal{P}}^{\\dagger}\\mathbf{B}_{\\partial\\mathcal{P}}\\mathbf{Z}\\biggr\\|.$ ", "page_idx": 5}, {"type": "text", "text": "211 To interpret the previous definition, we point out that the sum on the right-hand side of Definition 2   \n212  an be writen as $\\left\\|\\operatorname{vec}(\\mathbf{Z}^{\\top})\\right\\|_{\\mathbf{M}_{\\mathcal{V}}}$ where ve denotes the opertion of stacking amatri'scolumns   \n213 vertically. As a result, the condition is analogous to requiring that ${{\\bf{M}}_{\\nu}}$ is invertible with minimum   \n214  eigenvalue $\\phi^{2}$ , but weaker since it holds only for signals $\\mathbf{Z}\\,\\in\\,S$ and for the $\\lVert\\cdot\\rVert_{\\mathrm{RE}}$ norm. This   \n215  requirement has the same form as the compatibility assumption for the Lasso [Buihlmann and van de   \n216  Geer, 2011, Oh et al., 2021] or the restricted strong convexity assumption [Cella et al., 2023]. ", "page_idx": 5}, {"type": "text", "text": "217  We further make the following assumption on the true multi-task Gram matrix: ", "page_idx": 5}, {"type": "text", "text": "218 Assumption 4 (RE condition for the true multi-task Gram matrix). For $k\\in[K]$ let $\\pmb{\\Sigma}_{k}:=\\mathbb{E}\\left[\\mathbf{x}_{k}\\mathbf{x}_{k}^{\\top}\\right]$   \n219be the Gram matrix of the $k^{t h}$ context vector's marginal distribution, let $\\Sigma_{\\nu}$ be thetruemulti-task   \n220  Gram matrix of the context vector generating distribution, given by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pmb{\\Sigma}_{\\mathcal{V}}:=\\mathbf{I}_{|\\mathcal{V}|}\\otimes\\mathbf{\\overline{{\\Sigma}}},\\quad w h e r e\\quad\\overline{{\\Sigma}}=\\frac{1}{K}\\sum_{k=1}^{K}\\pmb{\\Sigma}_{k}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "221Weassumethat $\\Sigma_{\\nu}$ verifies RE condition (Definition 2\uff09 with some problem dependent constants   \n222 $\\begin{array}{r}{\\kappa\\in\\left[0,\\frac{1}{2w(\\partial\\mathcal{P})}\\underset{\\mathcal{C}\\in\\mathcal{P}}{\\operatorname*{min}}\\,\\sqrt{c_{\\mathcal{G}}(\\mathcal{C})}\\right)}\\end{array}$ and $\\phi>0$   \n223 This assumption is common to make for Lasso-like bandit problems [Oh et al., 2021, Ariu et al., 2022,   \n224 Cella et al., 2023]. We will later show that it can be transferred to empirical multi-task Gram matrix. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "2255.2 Oracle inequality ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "226 This section is dedicated to provide a bound on the estimation error of the Network Lasso problem   \n227 given in Equation (2) at a particular step $t$ of Algorithm 1. We assume fixed design, meaning that   \n228 the context vectors are given and fixed, and we are not concerned by their randomness (due to the   \n229 context generating distribution), nor by the randomness of their number for each user (due to random   \n230selection at each time step).   \n231 For a time step $t$ , we deliver the oracle inequality controlling the deviation between the estimated   \n232preference vectors $\\hat{\\Theta}(t)$ and the true ones $\\Theta$ . For the sake of simplicity, we provisionally assume   \n233 that the RE condition holds for the empirical multi-task Gram matrix $\\mathbf{A}_{\\mathcal{V}}(t)$   \n234  Theorem 1 (Oracle inequality). Assume that the $R E$ assumption holds for theempiricalmulti  \n235task Gram matrix with constants $\\begin{array}{r l r}{\\kappa}&{\\in}&{\\left[0,\\frac{1}{2w(\\partial\\mathcal{P})}\\underset{\\mathcal{C}\\in\\mathcal{P}}{\\operatorname*{min}}\\,\\sqrt{c_{\\mathcal{G}}(\\mathcal{C})}\\right)}\\end{array}$ and $\\phi~>~0$ \uff1aSuppose that   \n236 $\\mathrm{max}_{m\\in\\mathcal{V}}\\left|\\mathcal{T}_{m}(t)\\right|\\leq b t$ for some $b>0$ Then, with a probability at least $1-\\delta(t)$ we have ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\lVert\\Theta-\\hat{\\Theta}(t)\\right\\rVert_{F}\\le2\\frac{\\sigma}{\\phi^{2}\\sqrt{t}}f(\\mathcal{G},\\Theta)\\sqrt{1+2b\\sqrt{|\\mathcal{V}|\\log\\frac{1}{\\delta(t)}}+2b\\log\\frac{1}{\\delta(t)}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "237 where ", "page_idx": 5}, {"type": "equation", "text": "$$\nf(\\mathcal{G},\\Theta):=\\alpha_{0}\\left(a_{2}(\\mathcal{G},\\Theta)+\\sqrt{2}\\mathbb{1}_{\\leq1}(\\kappa)w(\\partial\\mathcal{P})\\right)\\left(\\frac{a_{2}(\\mathcal{G},\\Theta)+\\sqrt{2}\\mathbb{1}_{\\leq1}(\\kappa)w(\\partial\\mathcal{P})}{a_{1}(\\mathcal{G},\\Theta)\\operatorname*{min}_{c\\in\\mathcal{P}}\\sqrt{c_{\\mathcal{G}}(\\mathcal{C})}}+1\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "238 The proof of the previous theorem mainly relies on a decomposition of the estimation error signal   \n239  into two parts: one is the projection of the error onto its mean per cluster value, that is, every node   \n240 within the same cluster is mapped to the mean estimation error of its cluster. The second part of the   \n241 decomposition is simply the residual part i.e. the deviation from the mean per cluster value, which   \n242 is related to the incidence matrices of each cluster. The probabilistic statement comes from a high   \n243 probability bound on the Euclidean norm of an empirical vector process associated with our problem,   \n244 using a generalization of the Hanson-Wright inequality to the subgaussian case [Hsu et al., 2012,   \n245 Theorem 2.1]. Compared to the bound of Jung [2020, Theorem 1], we bound a norm of the estimation   \n246 error rather than just the total variation semi-norm. Additionally, the bound exhibits different behavior   \n247 depending on whether $\\kappa\\,>\\,1$ . Indeed, due to the expressions of $a_{1}(\\Theta,\\mathcal{G})$ and $a_{2}(\\Theta,\\mathcal{G})$ , in the   \n248 case where $\\kappa>1$ , the bound significantly decreases with the products $w(\\partial\\mathcal{P})\\operatorname*{min}_{\\mathcal{C}\\in\\mathcal{P}}\\sqrt{\\iota(\\mathcal{C})}$ and   \n249 $w(\\partial\\mathcal{P})\\operatorname*{max}_{\\mathcal{C}\\in\\mathcal{P}}c_{\\mathcal{G}}(\\mathcal{C})^{-\\frac{1}{2}}$ , which are both small enough for dense intra-cluster edge links and sparse   \n250 inter-cluster ones. However, when $\\kappa<1$ , the $w(\\partial{\\mathcal{P}})$ term might dominate if it is moderately large,   \n251 and its effect can only be mitigated via a smali subgaussianity constant $\\sigma$ or a large enough RE   \n252condition constant $\\phi$ ", "page_idx": 6}, {"type": "text", "text": "253  5.3  RE condition for the empirical multi-task Gram matrix ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "254To establish the oracle inequality, we assumed that the RE condition holds for the empirical multi-task   \n255Gram matrix. The goal of this section is to prove this holds with high probability. To this end, we use   \n256 the same strategy as in Oh et al. [2021], Cella et al. [2023]. We prove that on the one hand, given   \n257 the empirical multi-task Gram matrix inherits the RE condition from its adapted counterpart since it   \n258  concentrates around it. On the other hand, we prove that the adapted Gram matrix verifies the RE   \n259  condition due to Assumption 1, 2 and 4 made on the context generation distribution.   \n260 Theorem 2 (RE condition holding for the empirical multi-task Gram matrix). Under assumptions 2   \n261 and 4, let $t\\geq1$ andlet $\\kappa,\\phi$ betheconstantsfromAssumption4.Assume that $\\mathrm{max}_{m\\in\\mathcal{V}}\\left|\\mathcal{T}_{m}(t)\\right|\\leq b t$   \n262  Then, for any $\\begin{array}{r}{\\gamma\\in\\bigg(0,\\Big(1+\\frac{a_{2}(\\mathcal{G},\\Theta)+(1-\\kappa)^{+}\\sqrt{2}w(\\partial\\mathcal{P})}{a_{1}(\\mathcal{G},\\Theta)}\\Big)^{-2}\\bigg)}\\end{array}$ , the empirical multi-taskGram matrix   \n263verifies the RE conditionwith constants $\\kappa$ and $\\hat{\\phi}$ with ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\phi}=\\tilde{\\phi}\\sqrt{1-\\gamma\\left(1+\\frac{a_{2}(\\mathcal{G},\\Theta)+(1-\\kappa)^{+}\\sqrt{2}w(\\partial\\mathcal{P})}{a_{1}(\\mathcal{G},\\Theta)}\\right)^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{l}{w i t h\\;a\\;p r o b a b i l i t y\\;a t\\;l e a s t\\;e q u a l\\;t o\\;1-6d|\\mathcal{V}|\\exp\\left(\\frac{-\\;3\\gamma^{2}\\widetilde{\\phi}^{4}\\left(\\operatorname*{min}_{C\\in\\mathcal{P}}\\left(\\widetilde{c}_{\\mathcal{G}}(\\mathcal{C})\\wedge\\widetilde{c}_{\\mathcal{G}}(\\mathcal{C})^{2}\\right)t\\right)}{6b+2\\sqrt{2}\\gamma\\widetilde{\\phi}^{2}}\\right),}\\\\ {\\widetilde{\\phi}:=\\frac{\\phi}{\\sqrt{2\\nu\\omega}}\\,a n d\\;\\widetilde{c}_{\\mathcal{G}}(\\mathcal{C}):=c_{\\mathcal{G}}(\\mathcal{C})\\wedge|\\mathcal{C}|\\quad\\forall\\mathcal{C}\\in\\mathcal{P}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "266The proof follows the same approach as in Oh et al. [2021], Cella et al. [2023]; we prove that the RE   \n267 condition transfers from the true multi-task Gram matrix to its adapted counterpart $\\bar{\\mathbf{V}}_{\\mathcal{V}}(t)$ ,defined as   \n268follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbf{V}_{\\mathcal{V}}(t)=\\mathrm{diag}\\left(\\mathbf{V}_{1}(t),\\cdot\\cdot\\cdot\\mathbf{\\Omega},\\mathbf{V}_{|\\mathcal{V}|}(t)\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "269 where ", "page_idx": 6}, {"type": "equation", "text": "$$\n{\\bf V}_{m}(t)=\\frac{1}{t}\\sum_{\\tau\\in\\mathcal{T}_{m}(t)}\\mathbb{E}\\left[{\\bf x}(\\tau){\\bf x}(\\tau)^{\\top}|\\mathcal{F}_{\\tau-1}\\right].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "270 This transfer relies on the work of Oh et al. [2021, lemma 10]. The other step of the proof is showing   \n271 that the empirical multi-task Gram matrix and $\\mathbf{V}_{\\mathcal{V}}(t)$ become close to each other with high probability   \n272 after sufficiently many time steps, the respective distance between the two is measured with a matrix   \n273 norm induced by the RE semi-norm and the restriction to set $\\boldsymbol{S}$ (Definition 2). The bound showcases   \n274 a dependence on $\\mathrm{min}_{{\\mathcal{C}}\\in{\\mathcal{P}}}\\,c_{\\mathcal{G}}({\\mathcal{C}})\\wedge|{\\mathcal{C}}|$ , which is of the same order as $|{\\mathcal{C}}|$ for a fully connected cluster   \n275 with vertices $\\mathcal{C}$ . It is also clear that with a higher minimum centrality of a cluster, the probability of   \n276  satisfying the RE condition increases. ", "page_idx": 6}, {"type": "text", "text": "277 5.4  Regret bound ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "278 To bound the regret, we bound the expected instantaneous regret for each round $t\\geq1$ . This bound   \n279  relies on the oracle inequality holding and on the RE condition being satisfied for the empirical Gram   \n280 matrix, both with high probability. These two conditions are ensured and Theorem 1 and Theorem 2.   \n281 Theorem 3 (Regret bound). Let the mean horizon per node be $\\begin{array}{r}{\\overline{{T}}\\ =\\ \\frac{T}{|\\mathcal{V}|}}\\end{array}$ $L e t\\,\\operatorname*{min}_{\\mathcal{C}\\in\\mathcal{P}}\\sqrt{c_{\\mathcal{G}}(\\mathcal{C})}$   \n282 going asymptotically to infinity and $\\operatorname*{max}_{\\mathcal{C}\\in\\mathcal{P}}\\sqrt{\\iota_{\\mathcal{G}}(\\mathcal{C})}$ going asymptotically to zero as well as   \n283 maxceP \u221ag(C)w(P) and iw(OP) $\\frac{w(\\partial{\\mathcal P})}{\\operatorname*{min}_{{\\mathcal C}\\in{\\mathcal P}}\\sqrt{c_{\\mathcal G}({\\mathcal C})}}$ going asymptotically to zero. Under assumptions1 to $^{4}$   \n284  and $\\kappa<1$ the expected regret of the Network Lasso Bandit algorithm is upper bounded as follows:   \n$\\mathcal{R}(|\\nu|\\overline{{T}})=\\mathcal{O}\\left(\\sqrt{\\frac{\\overline{{T}}}{\\operatorname*{min}_{c\\in\\mathcal{P}}c_{\\mathcal{O}}(\\mathcal{C})}}\\left(\\sqrt{|\\mathcal{V}|}+\\sqrt{\\log(\\overline{{T}}|\\mathcal{V}|)}+\\sqrt[4]{|\\mathcal{V}\\log(\\overline{{T}}|\\mathcal{V}|)|}\\right)+\\frac{1}{A}\\log(d|\\mathcal{V}|)\\right),$   \n285with A = $A=\\frac{3\\gamma^{2}\\operatorname*{min}_{\\mathcal{C}\\in\\mathcal{P}}(\\tilde{c}_{\\mathcal{G}}(\\mathcal{C})\\wedge\\tilde{c}_{\\mathcal{G}}^{2}(\\mathcal{C}))}{6\\frac{\\log(|\\mathcal{V}|)}{\\sqrt{|\\mathcal{V}|}}+2\\sqrt{2}\\gamma}.$   \n286  Our regret is mainly formed of two parts. The first one is the sublinear time-dependent term and   \n287 represents the bulk of horizon dependence. Interestingly, it does not depend on the dimension,   \n288 which is a consequence of using the concentration inequality from Hsu et al. [2012]. Interestingly, it   \n289 decreases as the topological centrality index grows with the graph size, which proves the importance   \n290 of intra-cluster high connectivity.   \n291 The second significant term comes from ensuring the RE condition for the empirical multi-task Gram   \n292  matrix, and can be interpreted as the number of time steps necessary for it to hold, as pointed out by   \n293  Oh et al. [2021]. It has a logarithmic dependence in the graph size and in the dimension, which is   \n294  a characteristic of regret bound of the \"lasso type\". Also noteworthy is that the regret grows with   \n295 $\\log(d)$ only in the time-independent term, making our policy useful in high-dimensional settings. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "296 6 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "297 We provide experiments to showcase the effect on the problem's parameters on our algorithm's   \n298 performance as well as highlighting its advantageous performance compared to other algorithms. At   \n299 each time step, the algorithm solves the network lasso problem (2) via a primal-dual algorithm used   \n300 in Jung [2020].   \n301 We compare our algorithm to several baselines of the literature. On the one hand, baselines relying   \n302  on a given graph, GOBLin [Cesa-Bianchi et al., 2013] and GraphUCB [Yang et al., 2020] that use   \n303 the Laplacian to smooth the preference vectors. On the other hand, we consider online clustering   \n304 of bandits baselines, namely CLUB [Gentile et al., 2014] and SCLUB [Li et al., 2019]. Since these   \n305  latter approaches start with a fully connected graph, we provide them the known graph for a fair   \n306 comparison. As a sanity check, we also compare the independent task learning case with LinUCB   \n307 (LinUcbITL) where each task is solved independently, and to the case of a LinUCB agent for each   \n308 cluster (LinUcbOracle). The graph used is generated using stochastic block models in order to ensure   \n309 that the generated graph induces a cluster structure, where an edge is constructed with probability $p$   \n310within clusters and $q$ betweenclusters.   \n311 Experimentally, we found that normalizing the adjacency matrix, that is we utilize the following   \n312  normalized edges: $w_{m n}={\\frac{1}{\\sqrt{\\deg(m)\\deg(n)}}},$ where $\\deg(m)$ denotes the degree of node $m$ ,yields   \n313 significantly better results. Indeed, such a normalization makes the algorithm focus more on edges   \n314  between low-degree nodes, which improves the propagation of the collected information within the   \n315  graph. In all experiments we have set $\\alpha_{0}=0.1$   \n316  Our results clearly showcase an improvement compared to the other baselines. Apart from the oracle   \n317 that has complete knowledge of all clusters from the beginning, our policy performs significantly   \n318 better than the rest beyond the error margins, covering one standard deviation at ten repetitions. We   \n319 provide results for up to $|\\nu|=500$ nodes showing the effective transfer of knowledge within the   \n320graph. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "WxW4nZMD3D/tmp/4fb8e925edb80650a908c3438f3d588d6b3a3ad8fd95bc3ca383e805b960c617.jpg", "img_caption": ["Figure 1: Synthetic data experiment showing the cumulative regret of Network Lasso Policy as a function of time-steps compared to other baselines, for different choices of $|\\gamma|,d,p$ and $q$ "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "321  7  Conclusion and future perspectives ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "322 In this work, we proposed a multi-task bandit framework that solves the case where the task preference   \n323 vectors are piecewise constant over a graph. To this end, we used the Network Lasso policy to estimate   \n324 the task parameters, which bypasses explicit clustering procedures. We showed a sublinear regret   \n325 bound and as a byproduct, we proved a novel oracle inequality that relies on the small size of the   \n326 boundary as well as on the high value of the topological centrality index of each node within its   \n327 cluster. Our experimental evaluations highlight the advantage of our method, especially when either   \n328  the number of dimensions or nodes increases.   \n329 Due to the technical similarity of our problem with the Lasso, a natural extension would be to extend   \n330 it to a thresholded approach, in the same vein as [Ariu et al., 2022]. Another possible extension would   \n331 be to use regularization with higher order total variation terms that impose a piecewise polynomial   \n332 signal on a graph, as explained for scalar signals in Wang et al. [2016], Ortelli and van de Geer   \n333[2019]. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "334 References ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "335Y. Abbasi-Yadkori, D. Pal, and C. Szepesvari. Improved algorithms for linear stochastic bandits.   \n336 Advances in neural information processing systems, 24, 2011.   \n337 K. Ariu, K. Abe, and A. Proutiere. Thresholded Lasso Bandit. In Proceedings of the 39th International   \n338 Conference on Machine Learning, pages 878-928. PMLR, 2022.   \n339 H. Bastani and M. Bayati. Online Decision Making with High-Dimensional Covariates. Operations   \n340 Research, 2019. doi: 10.1287/opre.2019.1902.   \n341 S. Basu, B. Kveton, M. Zaheer, and C. Szepesvari. No Regrets for Learning the Prior in Bandits. In   \n342  Advances in Neural Information Processing Systems, 2021.   \n343 S. Bilaj, S. Dhouib, and S. Maghsudi. Meta learning in bandits within shared affine subspaces. In   \n344 Proceedings of The 27th International Conference on Artificial Intelligence and Statistics. PMLR,   \n345 2024.   \n346  J. Borge-Holthoefer, A. Rivero, I. Garcia, E. Cauhe, A. Ferrer, D. Ferrer, D. Francos, D. Iniguez, M. P.   \n347 Perez, G. Ruiz, et al. Structural and dynamical pattrns on online social networks: the spanish may   \n348 15th movement as a case study. PloS one, 6(8), 2011.   \n349P. Buhlmann and S. van de Geer. Statistics for high-dimensional data. Springer Series in Statistics.   \n350 Springer, Heidelberg, 2011. ISBN 978-3-642-20191-2.   \n351 L. Cella and M. Pontil. Multi-task and meta-learning with sparse linear bandits. In Uncertainty in   \n352 Artificial Intelligence. PMLR, 2021.   \n353  L. Cella, A. Lazaric, and M. Pontil. Meta-learning with stochastic linear bandits. In Proceedings of   \n354 the 37th International Conference on Machine Learning. PMLR, 2020.   \n355  L. Cella, K. Lounici, G. Pacreau, and M. Pontil. Multi-task representation learning with stochastic   \n356 linear bandits. In International Conference on Artificial Intelligence and Statistics, 2023.   \n357 N. Cesa-Bianchi, C. Gentile, and G. Zappella. A gang of bandits. Advances in neural information   \n358 processing systems, 26, 2013.   \n359 J. Cheeger. A lower bound for the smallest eigenvalue of the laplacian. Problems in analysis, 1970.   \n360X. Cheng, C. Pan, and S. Maghsudi. Parallel online clustering of bandits via hedonic game. In   \n361 International Conference on Machine Learning, pages 5485-5503. PMLR, 2023.   \n362W. Chu, L. Li, L. Reyzin, and R. Schapire. Contextual bandits with linear payoff functions. In   \n363 Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics.   \n364 JMLR Workshop and Conference Proceedings, 2011.   \n365 X.Dong,D.Thanou, MRabbat, and P Frossard. Leaming graps from data: A sigal reresentatn   \n366 perspective. IEEE Signal Processing Magazine, 2019.   \n367 D. Easley, J. Kleinberg, et al. Networks, crowds, and markets: Reasoning about a highly connected   \n368 world, volume 1. Cambridge university press Cambridge, 2010.   \n369 A. Fontan and C. Altafini. On the properties of laplacian pseudoinverses. In 2021 60th IEEE   \n370 Conference on Decision and Control (CDC). IEEE, 2021.   \n371 C. Gentile, S. Li, and G. Zappella. Online clustering of bandits. In International Conference on   \n372 Machine Learning, pages 757-765. PMLR, 2014.   \n373  C. Gentile, S. Li, P. Kar, A. Karatzoglou, G. Zappella, and E. Etrue. On context-dependent clustering   \n374 of bandits. In International Conference on machine learning, pages 1253-1262. PMLR, 2017.   \n375  D. Hallac, J. Leskovec, and S. Boyd. Network lasso: Clustering and optimization in large graphs. In   \n376Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data   \n377 mining, pages 387-396, 2015.   \n378M. Herbster, S. Pasteris, F. Vitale, and M. Pontil. A gang of adversarial bandits. Advances in Neural   \n379 Information Processing Systems, 34, 2021.   \n380 D. Hsu, S. Kakade, and T. Zhang. A tail inequality for quadratic forms of subgaussian random vectors.   \n381 Electronic Communications in Probability, 17, 2012.   \n382 J. Hu, X. Chen, C. Jin, L. Li, and L. Wang. Near-optimal representation learning for linear bandits   \n383 and linear rl. In International Conference on Machine Learning. PMLR, 2021.   \n384A. Jung. Networked Exponential Families for Big Data Over Networks. IEEE Access, 8, 2020. ISSN   \n385 2169-3536.   \n386 A. Jung and N. Vesselinova. Analysis of network lasso for semi-supervised regression. In The 22nd   \n387 International Conference on Artificial Intelligence and Statistics, pages 380-387. PMLR, 2019.   \n388 A. Jung, N. Tran, and A. Mara. When Is Network Lasso Accurate? Frontiers in Applied Mathematics   \n389 and Statistics, 3, 2018. ISSN 2297-4687.   \n390 G.-S. Kim and M. C. Paik. Doubly-robust lasso bandit. Advances in Neural Information Processing   \n391 Systems, 32, 2019.   \n392 B. Kveton, M. Konobeey, M. Zaheer, C.-w. Hsu, M. Mladenov, C. Boutilier, and C. Szepesvari.   \n393 Meta-thompson sampling. In International Conference on Machine Learning. PMLR, 2021.   \n394  L. Li, W. Chu, J. Langford, and R. E. Schapire. A contextual-bandit approach to personalized news   \n395 article recommendation. In Proceedings of the 19th international conference on World wide web,   \n396 pages 661-670, 2010.   \n397S. Li, W. Chen,and K-S. Leung. Improved algorithm on online clustering of bandits. arXiv preprint   \n398 arXiv: 1902.09162, 2019.   \n39 M. McPherson, L. Smith-Lovin, and J. M. Cook. Birds of a feather: Homophily in social networks.   \n400 Annual review of sociology, 27(1):415-444, 2001.   \n401 M. E. Newman. Modularity and community structure in networks. Proceedings of the national   \n402 academy of sciences, 103(23):8577-8582, 2006.   \n403  T. T. Nguyen and H. W. Lauw. Dynamic clustering of contextual multi-armed bandits. In Pro  \n404 ceedings of the 23rd ACM international conference on conference on information and knowledge   \n405 management, pages 1959-1962, 2014.   \n406B. Nourani-Koliji, S. Bilaj, A. R. Balef, and S. Maghsudi. Piecewise-stationary combinatorial   \n407 semi-bandit with causally related rewards. arXiv preprint arXiv:2307.14138, 2023.   \n408M.-H. Oh, G. Iyengar, and A. Zeevi. Sparsity-Agnostic Lasso Bandit. In Proceedings of the 38th   \n409 International Conference on Machine Learning, pages 8271-8280. PMLR, 2021.   \n410 F. Ortelli and S. van de Geer. Synthesis and analysis in total variation regularization. arXiv preprint   \n411 arXiv:1901.06418, 2019.   \n412  A. Peleg, N. Pearl, and R. Meir. Metalearning linear bandits by prior update. In Proceedings of The   \n413 25th International Conference on Artificial Intelligence and Statistics. PMLR, 2022.   \n414  G. Ranjan and Z.-L. Zhang. Geometry of complex networks and topological centrality. Physica A:   \n415 Statistical Mechanics and its Applications, 2013.   \n416 X. Su and T. M. Khoshgoftar. A survey of collaborative filtering techniques. Advances in artificial   \n417 intelligence, 2009, 2009.   \n418 R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical   \n419 Society Series B: Statistical Methodology, 1996.   \n420 J. Tropp. Freedman's inequality for matrix martingales. Electronic Communications in Probability,   \n421 16:262 - 270, 2011.   \n422 P. Van Mieghem, K. Devriendt, and H. Cetinay. Pseudoinverse of the laplacian and best spreader   \n423 node in a network. Physical Review E, 2017.   \n424 Y-X. Wang, J. Sharpnack, A. J. Smola, and R. J. Tibshirani. Trend filtering on graphs. Joual   \n425 of Machine Learning Research, 17(105):1-41, 2016. URL http: //jmlr.org/papers/v17/   \n426 15-147.html.   \n427 K. Yang and L. Toni. Graph-based recommendation system. In 2018 IEEE Global Conference on   \n428 Signal and Information Processing (GlobalSIP), pages 798-802. IEEE, 2018.   \n429  K. Yang, L. Toni, and X. Dong. Laplacian-regularized graph bandits: Algorithms and theoretical   \n430 analysis. In International Conference on Artificial Intelligence and Statistics, pages 3133-3143.   \n431 PMLR,2020.   \n432 M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of   \n433 the Royal Statistical Society Series B: Statistical Methodology, 2006. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "434 A Some helper results ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "435  Proposition 1 (Bounds on norms of matrix products). Let $\\mathbf{M}\\in\\mathbb{R}^{m\\times n}$ and $\\mathbf{N}\\in\\mathbb{R}^{n\\times p}$ .Then ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{M}\\mathbf{N}\\|_{q,1}\\leq\\|\\mathbf{M}\\|_{\\infty,1}\\|\\mathbf{N}\\|_{q,1}\\quad\\forall q\\in[1,\\infty]}\\\\ &{\\|\\mathbf{M}\\mathbf{N}\\|_{F}\\leq\\|\\mathbf{M}\\|\\|\\mathbf{N}\\|_{F}}\\\\ &{\\|\\mathbf{M}\\mathbf{N}\\|_{F}\\leq\\sqrt{\\|\\mathbf{M}^{\\top}\\mathbf{M}\\|_{\\infty,\\infty}}\\|\\mathbf{N}\\|_{2,1}}\\\\ &{\\|\\mathbf{M}\\mathbf{N}\\|_{2,1}\\leq\\|\\mathbf{M}\\|_{2,1}\\|\\mathbf{N}\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "436 Proof. ", "page_idx": 11}, {"type": "text", "text": "437  First inequality  For any $q\\in[1,\\infty]$ , we have: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{e}_{i}^{\\top}\\mathbf{M}\\mathbf{N}\\right\\|_{q}=\\left\\|\\mathbf{e}_{i}^{\\top}\\mathbf{M}\\sum_{j=1}^{n}\\mathbf{e}_{j}\\mathbf{e}_{j}^{\\top}\\mathbf{N}\\right\\|_{q}\\leq\\operatorname*{max}_{1\\leq j\\leq n}\\left|\\mathbf{e}_{i}^{\\top}\\mathbf{M}\\mathbf{e}_{j}\\right|\\sum_{j=1}^{n}\\left\\|\\mathbf{e}_{j}^{\\top}\\mathbf{N}\\right\\|_{q}=\\operatorname*{max}_{1\\leq j\\leq n}|(\\mathbf{M})_{i j}|||\\mathbf{N}\\right\\|_{q,1}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "438 ", "page_idx": 11}, {"type": "text", "text": "439 Second inequality  We have ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{M}\\mathbf{N}\\right\\|_{F}^{2}=\\sum_{j=1}^{p}\\left\\|\\mathbf{M}\\mathbf{N}\\mathbf{e}_{j}\\right\\|^{2}\\leq\\sum_{j=1}^{p}\\left\\|\\mathbf{M}\\right\\|\\left\\|\\mathbf{N}\\mathbf{e}_{j}\\right\\|^{2}=\\left\\|\\mathbf{M}\\right\\|\\left\\|\\mathbf{N}\\right\\|_{F}^{2}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "440 ", "page_idx": 11}, {"type": "text", "text": "441 Third inequality  We have ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\|\\mathbf{M}\\mathbf{N}\\|_{F}^{2}=\\operatorname{Tr}(\\mathbf{M}\\mathbf{N}\\mathbf{N}^{\\top}\\mathbf{M}^{\\top})\\leq\\left\\|\\mathbf{M}^{\\top}\\mathbf{M}\\right\\|_{\\infty,\\infty}\\left\\|\\mathbf{N}\\mathbf{N}^{\\top}\\right\\|_{1,1}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "442  Elements of $(i,j)$ entry of matrix $\\mathbf{NN^{\\top}}$ is the inner product $\\langle\\mathbf{e}_{i}^{\\top}\\mathbf{N},\\mathbf{e}_{j}^{\\top}\\mathbf{N}\\rangle$ . Hence, we have ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\|\\mathbf{NN}^{\\top}\\|_{1,1}=\\sum_{i,j}\\left|\\langle\\mathbf{e}_{i}^{\\top}\\mathbf{N},\\mathbf{e}_{j}^{\\top}\\mathbf{N}\\rangle\\right|\\leq\\sum_{i,j}\\|\\mathbf{e}_{i}^{\\top}\\mathbf{N}\\|\\|\\mathbf{e}_{j}^{\\top}\\mathbf{N}\\|=\\|\\mathbf{N}\\|_{2,1}^{2}.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "443 ", "page_idx": 11}, {"type": "text", "text": "444 Fourth inequality We have ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\|\\mathbf{M}\\mathbf{N}\\|_{2,1}=\\sum_{i=1}^{m}\\|\\mathbf{e}_{i}\\mathbf{M}\\mathbf{N}\\|\\leq\\sum_{i=1}^{m}\\|\\mathbf{e}_{i}\\mathbf{M}\\|\\|\\mathbf{N}\\|=\\|\\mathbf{M}\\|_{2,1}\\|\\mathbf{N}\\|\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "445 ", "page_idx": 11}, {"type": "text", "text": "446 Proposition 2 (Decomposition of a signal over a graph). For any $\\mathcal{C}\\in\\mathcal{P}$ ", "page_idx": 11}, {"type": "text", "text": "447   \n448   \n449   \n450   \n451 ", "page_idx": 12}, {"type": "text", "text": "\u00b7 Let $\\mathbf{Z}\\,\\in\\,\\mathbb{R}^{|\\mathcal{V}|\\times d}$ be a graph signal. Let us denote by $\\mathbf{Z}_{\\mathcal{C}}$ the signal obtained from $\\mathbf{Z}$ by setting rows of vertices outside of $\\mathcal{C}$ to zeros, and let $\\mathbf{Z}_{\\vert\\mathcal{C}}\\in\\mathbb{R}^{\\vert\\mathcal{C}\\vert\\sqrt{\\times}d}$ be the signal obtained from $\\mathbf{Z}_{\\mathcal{C}}$ by removing the rows of vertices outside of $\\mathcal{C}$ . Also, let $\\mathbf{B}_{|\\mathcal{C}}\\in\\mathbb{R}^{|\\mathcal{E}_{c}|\\times|\\mathcal{C}|}$ be the matrix obtained by taking $\\mathbf{B}_{\\mathcal{C}}$ , and removing rows of edges that link $\\mathcal{C}$ to its outside, and the resulting null columns. It is clear that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbf{B}_{\\mathit{c}}\\mathbf{Z}=\\mathbf{B}_{\\mathit{c}}\\mathbf{Z}_{\\mathit{c}}=\\mathbf{B}_{|\\mathit{c}}\\mathbf{Z}_{|\\mathit{c}}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "452 ", "page_idx": 12}, {"type": "text", "text": "\u00b7 Let $\\mathbf{Q}_{\\mathcal{C}}:=\\mathbf{B}_{\\mathcal{C}}^{\\dagger}\\mathbf{B}_{\\mathcal{C}}$ Then ", "page_idx": 12}, {"type": "equation", "text": "$$\n{\\bf I}_{|\\mathcal{V}|}=\\sum_{\\mathcal{C}\\in\\mathcal{P}}{\\bf J}_{\\mathcal{C}}+{\\bf Q}_{\\mathcal{C}}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbf{Q}_{\\partial\\mathcal{P}^{c}}:==\\mathbf{B}_{\\partial\\mathcal{P}^{c}}^{\\dagger}\\mathbf{B}_{\\partial\\mathcal{P}^{c}}=\\sum_{\\mathcal{C}\\in\\mathcal{P}}\\mathbf{Q}_{\\mathcal{C}}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "453 ", "page_idx": 12}, {"type": "equation", "text": "$\\begin{array}{r l r}&{}&{=\\frac{{\\bf1}_{C}{\\bf1}_{C}^{\\top}}{|\\mathcal{C}|},\\,{\\bf Q}_{\\mathcal{C}}={\\bf B}_{\\mathcal{C}}^{\\dagger}{\\bf B}_{\\mathcal{C}}\\qquad\\forall\\mathcal{C}\\in\\mathcal{P}\\;a n d\\,{\\bf Q}_{\\partial\\mathcal{P}^{c}}:={\\bf B}_{\\partial\\mathcal{P}^{c}}^{\\dagger}{\\bf B}_{\\partial\\mathcal{P}^{c}}.}\\end{array}$ ", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "454   \n455   \n456 ", "page_idx": 12}, {"type": "text", "text": "While $\\sum_{\\mathcal{C}\\in\\mathcal{P}}\\mathbf{J}_{\\mathcal{C}}$ projects each entry ofa graph signal onto the mean vector value of its respective cluster, its residual $\\mathbf{Q}_{\\partial\\mathcal{P}^{c}}$ can be interpreted as the projection onto the respective entries deviationfrom its cluster meanvalue. ", "page_idx": 12}, {"type": "text", "text": "457 Proof. Since the proof of the frst point is trivial, we directly treat the second point. Denoting $\\mathbf{B}_{|c}^{\\dagger}$ the   \n458 pseudo-inverse of $\\mathbf{B}_{\\mathit{|c}}$ it is a well-known linear algebra result that the matrix $Q_{|\\mathcal{C}}:=\\mathbf{B}_{|\\mathcal{C}}^{\\dagger}\\mathbf{B}_{|\\mathcal{C}}\\mathbf{B}_{|\\mathcal{C}}$ is the   \n459 projector onto the null space of $\\mathbf{B}_{\\mathit{|c}}$ . Since $\\mathcal{C}$ is connected, the null space of $\\mathbf{B}_{\\mathit{|c}}$ is unidimensional,   \n460 and is generated by vector $\\mathbf{1}_{|c|}\\in\\mathbb{R}^{|c|}$ having only ones as coordinates. Since the projector into that   \n461 nullspace is $\\begin{array}{r}{\\mathbf{J}_{|\\mathcal{C}|}:=\\frac{\\mathbf{1}_{|\\mathcal{C}|}\\mathbf{1}_{|\\mathcal{C}|}}{|\\mathcal{C}|}}\\end{array}$ l, we deduce that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{Z}_{|c}=\\mathbf{J}_{|c|}\\mathbf{Z}_{|c}+\\mathbf{Q}_{|c}\\mathbf{Z}_{|c}}\\\\ {\\implies\\mathbf{Z}_{c}=\\mathbf{J}_{c}\\mathbf{Z}_{\\mathcal{C}}+\\mathbf{Q}_{\\mathcal{C}}\\mathbf{Z}_{\\mathcal{C}}}\\\\ {=\\mathbf{J}_{\\mathcal{C}}\\mathbf{Z}+\\mathbf{Q}_{\\mathcal{C}}\\mathbf{Z}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "462  where in the last line, $Q_{\\mathcal{C}}:=\\mathbf{B}_{\\mathcal{C}}^{\\dagger}\\mathbf{B}_{\\mathcal{C}}$ . Consequently, we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\bf Z}=\\sum_{\\mathcal{C}\\in\\mathcal{P}}{\\bf Z}_{\\mathcal{C}}}\\ ~}\\\\ {{\\displaystyle~~~~~=\\sum_{\\mathcal{C}\\in\\mathcal{P}}{\\bf J}_{\\mathcal{C}}{\\bf Z}+{\\bf Q}_{\\mathcal{C}}{\\bf Z}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "463  To prove the second point, we recall that $\\mathbf{B}_{\\partial\\mathcal{P}^{c}}$ is the incidence matrix obtained by setting rows   \n464 corresponding to edges in $\\partial\\mathcal{P}$ to zero. In other words, $\\mathbf{B}_{\\partial\\mathcal{P}^{c}}$ is the incidence matrix of the graph   \n465 after removing the boundary edges, and having exactly $|\\mathcal P|$ connected components. Hence, $\\mathbf{B}_{\\partial\\mathcal{P}^{c}}$   \n466 has a null space spanned by the set $\\{\\mathbf{1}_{\\mathcal{C}}\\}_{\\mathcal{C}\\in\\mathcal{P}}$ , and the orthogonal projector onto this null space is   \n467 $\\sum_{\\mathcal{C}\\in\\mathcal{P}}\\mathbf{J}_{\\mathcal{C}}$ . Combining this fact with the fact that $\\mathbf{Q}_{\\partial\\mathcal{P}^{c}}$ is the projector onto the orthogonal of the   \n468 null space of $\\mathbf{B}_{\\partial\\mathcal{P}^{c}}$ , we arrive at the second point. \u53e3   \n469 Proposition 3 (On the minimum topological centrality index of a graph vertex). Let $\\mathcal{G}$ be a connected   \n470graphwithincidencematrix $\\mathbf{B}$ and vertex set size $N$ .and let $\\mathbf{\\dot{L}}:=\\mathbf{B}^{\\top}\\mathbf{B}$ Let $c(\\mathcal{G})$ denote the   \n471minimumvalue of inverses of diagonal element of $\\mathbf{L}^{\\dagger}$ called its minimum topological centrality index.   \n472Also let $a(\\mathcal{G})$ be its algebraic connectivity, defined as the minimum non null eigenvalue of L.Then ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "473 ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{\\nabla}\\cdot\\left.c(\\mathcal{G})=\\|\\mathbf{L}\\|_{\\infty,\\infty}^{-1}.\\right.}\\\\ {\\left.\\cdot\\mathbf{\\nabla}c(\\mathcal{G})\\geq a(\\mathcal{G}).\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "\u00b7If $\\mathcal{G}$ isweightless,then $c(\\mathcal{G})\\leq\\frac{N^{2}}{N-1}$ ", "page_idx": 12}, {"type": "text", "text": "476 Proof. Since $\\mathbf{L}$ is PSD, $\\mathbf{L}^{\\dagger}$ is PSD and hence $\\|\\mathbf{L}^{\\dag}\\|_{\\infty,\\infty}$ is equal to the maximum diagonal entry of   \n477 $\\mathbf{L}^{\\dagger}$ . Taking the inverse proves the first point. Also, this implies that ", "page_idx": 13}, {"type": "equation", "text": "$$\nc(\\mathcal{G})=\\left\\|\\mathbf{L}^{\\dagger}\\right\\|_{\\infty,\\infty}^{-1}\\geq\\left\\|\\mathbf{L}^{\\dagger}\\right\\|^{-1}=a(\\mathcal{G}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "478  where we used the fact that $\\|\\cdot\\|_{\\infty,\\infty}\\ \\leq\\ \\|\\cdot\\|$ for matrices. This proves the second point of the   \n479 proposition.   \n480  For the last point, assume $\\mathcal{G}$ is weightless, let $\\mathbf{L}_{\\mathrm{comp}}$ be the Laplaciane of complete graph built on the   \n481  vertices o $\\mathcal{G}$ . Then we have $\\mathbf{L}_{\\mathrm{comp}}=N(\\mathbf{I}_{N}-\\mathbf{J}_{N}^{\\star})$ where $J$ is the square matrix of dimension $N$   \n482having $1/N$ as entries. From Fontan and Altafini [2021, Lemma 4], we have ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{L}_{\\mathrm{comp}}^{\\dagger}=(\\mathbf{L}_{\\mathrm{comp}}+N\\mathbf{J}_{N})^{-1}-\\frac{1}{N}\\mathbf{J}_{N}=\\frac{\\mathbf{I}_{N}}{N}-\\frac{1}{N}\\mathbf{J}_{N}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "483 which has diagonal elements $\\begin{array}{r}{\\frac{1}{N}-\\frac{1}{N^{2}}}\\end{array}$ ", "page_idx": 13}, {"type": "text", "text": "484  On the other hand, $\\mathbf{L}\\preccurlyeq\\mathbf{L}_{\\mathrm{comp}}$ Hence, by Fontan and Altafini [2021, lemma 4] we have for any   \n485 $u\\ne0$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{L}^{\\dagger}=(\\mathbf{L}+a\\mathbf{J}_{N})^{-1}-\\mathbf{J}_{N}/a\\succcurlyeq(\\mathbf{L}_{\\mathrm{comp}}+a\\mathbf{J}_{N})^{-1}-\\mathbf{J}_{N}/a=\\mathbf{L}_{\\mathrm{comp}}^{\\dagger}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "486 Thisimpliesthat the maximum diagonal enry of $\\mathbf{L}^{\\dagger}$ is t least equal to ha of $\\mathbf{L}_{\\mathrm{comp}}^{\\dagger}$ ie.to $\\begin{array}{r}{\\frac{1}{N}-\\frac{1}{N^{2}}}\\end{array}$   \n487 Taking the inverse of that entry finishes the proof. ", "page_idx": 13}, {"type": "text", "text": "488 ", "page_idx": 13}, {"type": "text", "text": "489 B  Proofs of the different claims ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "490 B.1  Additional notation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "491 The regularization term can be written more compactly using the incidence matrix of the graph   \n492 $\\mathbf{B}\\in\\mathbb{R}^{|\\mathcal{E}|\\times|\\mathcal{V}|}$ corresponding to an arbitrary orientation under the following form ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sum_{1\\leq m<n\\leq|\\mathcal{V}|}w_{m n}\\|\\pmb{\\theta}_{m}-\\pmb{\\theta}_{n}\\|=\\|\\mathbf{B}\\pmb{\\Theta}\\|_{2,1}=\\|\\pmb{\\Theta}\\|_{\\varepsilon}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "493 where the $\\lVert\\cdot\\rVert_{2,1}$ norm denotes the sum of the $L_{2}$ norms o the rows of a matrix.1 We provide notations 494 that we use in the proofs of the different statements, in order to reduce the clutter. We define $\\mathbf{E}:=\\hat{\\Theta}-\\Theta$ $\\{\\epsilon_{m}\\}_{m=1}^{\\mid\\mathcal{V}\\mid}$ ", "page_idx": 13}, {"type": "text", "text": "496While $\\scriptstyle\\sum_{k=1}^{C}\\mathbf{J}_{{\\mathcal{C}}}$ projeets each entry of agraphsignal onto the mean vector value of itsrespective   \n497 cluster, its residual $\\mathbf{Q}_{\\partial\\mathcal{P}^{c}}$ can be interpreted as the projection onto the respective entries deviation   \n498from its cluster mean value.   \n499 Let $\\eta_{m}$ be a vector, vertically concatenated by noise terms of rewards received by node $m$ ,thenwe   \n500define $\\mathbf{K}\\in\\mathbb{R}^{|\\mathcal{V}|\\times d}$ as the matrix of verticllyconcatenated row vectors $\\eta_{m}^{\\top}\\mathbf{X}_{m}$ ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "501B.2 Oracle inequality ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "502 In this section, we present all intermediary theoretical results leading to Theorem 1 stating the oracle   \n503 inequality. To reduce the clutter, we omit the dependence on $t$ of several quantities. For instance, we   \n504 write $\\alpha$ and $\\hat{\\Theta}$ instead of $\\alpha(t)$ and $\\hat{\\Theta}(t)$ ", "page_idx": 13}, {"type": "text", "text": "505 Lemma 1 (A first deterministic inequality). Let t be a time step. We have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{1}{2t\\alpha}\\sum_{m\\in\\mathcal{V}}\\left\\|\\mathbf{X}_{m}\\pmb{\\epsilon}_{m}\\right\\|^{2}+\\left\\|\\mathbf{E}\\right\\|_{\\partial\\mathcal{P}^{c}}\\leq\\frac{1}{t\\alpha}\\left\\langle\\mathbf{K},\\mathbf{E}\\right\\rangle+\\left\\|\\mathbf{E}\\right\\|_{\\partial\\mathcal{P}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "table", "img_path": "WxW4nZMD3D/tmp/992568efda0588a169f77568a56441f86addb502b4906fba34a7dfd68b896a06.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{1}{2t}\\sum_{m\\in\\mathcal{V}}\\left\\|\\mathbf{X}_{m}\\hat{\\boldsymbol{\\theta}}_{m}-\\mathbf{y}_{m}\\right\\|^{2}+\\alpha\\|\\boldsymbol{\\Theta}\\|_{\\mathcal{E}}\\leq\\frac{1}{2t}\\sum_{m\\in\\mathcal{V}}\\|\\mathbf{X}_{m}\\boldsymbol{\\theta}_{m}-\\mathbf{y}_{m}\\|^{2}+\\alpha\\|\\boldsymbol{\\Theta}\\|_{\\mathcal{E}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "507 where the second line holds by definition of the observed rewards. ", "page_idx": 14}, {"type": "text", "text": "508  On the one hand, given a user index $m\\in\\mathcal{V}$ , and since by definition of the observed rewards we have   \n509  we have for the least squared terms ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\mathbf{X}_{m}\\hat{\\boldsymbol{\\theta}}_{m}-\\mathbf{y}_{m}\\right\\|^{2}=\\left\\|\\mathbf{X}_{m}\\hat{\\boldsymbol{\\theta}}_{m}-\\mathbf{X}_{m}\\boldsymbol{\\theta}_{m}-\\boldsymbol{\\eta}_{m}\\right\\|^{2}}\\\\ &{=\\left\\|\\mathbf{X}_{m}\\boldsymbol{\\epsilon}_{m}-\\boldsymbol{\\eta}_{m}\\right\\|^{2}}\\\\ &{=\\left\\|\\mathbf{X}_{m}\\boldsymbol{\\epsilon}_{m}\\right\\|^{2}+\\left\\|\\mathbf{X}_{m}\\boldsymbol{\\theta}_{m}-\\mathbf{y}_{m}\\right\\|^{2}-\\eta_{m}^{\\top}\\mathbf{X}_{m}\\boldsymbol{\\epsilon}_{m}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "510 where we used the fact that $\\mathbf{y}_{m}=\\mathbf{X}_{m}\\pmb{\\theta}_{m}+\\pmb{\\eta}_{m}$ , which holds by definition of the observed rewards.   \n511 Summing over the users, and using the definition of $\\mathbf{K}$ ,wehave ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{1}{2t}\\sum_{m\\in\\mathcal{V}}\\left\\|\\mathbf{X}_{m}\\hat{\\boldsymbol{\\theta}}_{m}-\\mathbf{y}_{m}\\right\\|^{2}-\\frac{1}{2t}\\sum_{m\\in\\mathcal{V}}\\left\\|\\mathbf{X}_{m}\\boldsymbol{\\theta}_{m}-\\mathbf{y}_{m}\\right\\|^{2}=\\frac{1}{2t}\\sum_{m\\in\\mathcal{V}}\\left\\|\\mathbf{X}_{m}\\boldsymbol{\\epsilon}_{m}\\right\\|^{2}-\\frac{1}{t}\\left\\langle\\mathbf{K},\\mathbf{E}\\right\\rangle\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "512 On the other hand, we have for the estimated preference vectors ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|\\Theta\\|_{\\varepsilon}=\\sum_{(m,n)\\in\\mathcal{E}}w_{m n}\\Big\\|\\hat{\\theta}_{m}-\\hat{\\theta}_{n}\\Big\\|}\\\\ {\\displaystyle\\quad=\\sum_{(m,n)\\in\\partial\\mathcal{P}}w_{m n}\\Big\\|\\hat{\\theta}_{m}-\\hat{\\theta}_{n}\\Big\\|+\\sum_{(m,n)\\in\\partial\\mathcal{P}^{c}}w_{m n}\\Big\\|\\hat{\\theta}_{m}-\\hat{\\theta}_{n}\\Big\\|}\\\\ {\\displaystyle\\quad=\\Big\\|\\hat{\\Theta}\\Big\\|_{\\partial\\mathcal{P}}+\\Big\\|\\hat{\\Theta}\\Big\\|_{\\partial\\mathcal{P}^{c}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "513  For the true ones, and for any $\\mathcal{C}\\in\\mathcal{P}$ ,let $\\mathcal{E}_{\\mathcal{C}}$ denote the edges linking the nodes of set of nodes $\\mathcal{C}$ . It is   \n514  Clear that $\\partial\\mathcal{P}^{c}=\\bigcup_{\\mathcal{C}\\in\\mathcal{P}}\\mathcal{E}_{\\mathcal{C}}$ as a disjoint union, hence ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|\\Theta\\|_{\\mathcal E}=\\sum_{(m,n)\\in\\mathcal E}w_{m n}\\|\\theta_{m}-\\theta_{n}\\|}}\\\\ &{=\\displaystyle\\sum_{(m,n)\\in\\partial\\mathcal P}w_{m n}\\|\\theta_{m}-\\theta_{n}\\|+\\sum_{(m,n)\\in\\partial\\mathcal P^{c}}w_{m n}\\|\\theta_{m}-\\theta_{n}\\|}\\\\ &{=\\|\\Theta\\|_{\\partial\\mathcal P}+\\displaystyle\\sum_{c\\in\\mathcal P}\\sum_{(m,n)\\in\\mathcal E_{c}}w_{m n}\\|\\theta_{m}-\\theta_{n}\\|}\\\\ &{=\\|\\Theta\\|_{\\partial\\mathcal P}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "515 where the last equality holds due to the cluster assumption. ", "page_idx": 15}, {"type": "text", "text": "516 Hence, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\Theta\\|_{\\mathcal{E}}-\\|\\Theta\\|_{\\mathcal{E}}=\\|\\Theta\\|_{\\partial\\mathcal{P}}-\\left\\|\\hat{\\Theta}\\right\\|_{\\partial\\mathcal{P}}-\\left\\|\\hat{\\Theta}\\right\\|_{\\partial\\mathcal{P}}}\\\\ {\\le\\left\\|\\mathbf{E}\\right\\|_{\\partial\\mathcal{P}}-\\left\\|\\hat{\\Theta}\\right\\|_{\\partial\\mathcal{P}^{c}},\\quad\\quad\\quad\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "517  where the first inequality holds due to the triangle inequality, and the last one since $\\|\\Theta\\|_{\\partial\\mathcal{P}^{c}}=0$   \n518   Combining Equations (16) to (18), we obtain the result of the statement. \u53e3   \n519 In the proof for the oracle inequality, we utilize projection operators on the graph signal, that we   \n520define as followed:   \n521While $\\scriptstyle\\sum_{k=1}^{C}\\mathbf{J}_{\\mathcal{C}}$ projeets each etry of a graph signal ontothe mean vector value of its rspective   \n522  cluster, its residual $\\mathbf{Q}_{\\partial\\mathcal{P}^{c}}$ can be interpreted as the projection onto the respective entries deviation   \n523 from its cluster mean value.   \n524  Lemma 2 (Bounding the error restricted to the boundary). The total variation of $\\mathbf{E}$ restrictedtothe   \n525boundaryverifies ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\mathbf{E}\\|_{\\partial\\mathcal{P}}\\leq w(\\partial\\mathcal{P})\\left(\\sqrt{2}\\operatorname*{max}_{\\mathcal{C}\\in\\mathcal{P}}\\sqrt{\\iota_{\\mathcal{G}}(\\mathcal{C})}\\big\\|\\overline{{\\mathbf{E}}}_{\\mathcal{P}}\\big\\|_{F}+2\\frac{\\|\\mathbf{E}\\|_{\\partial\\mathcal{P}^{c}}}{\\underset{\\mathcal{C}\\in\\mathcal{P}}{\\operatorname*{min}}\\sqrt{c_{\\mathcal{G}}(\\mathcal{C})}}\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "526Proof. The proof relies on a decomposition of the $\\|\\mathbf{E}\\|_{\\partial\\mathcal{P}}$ term from Proposition 2. We have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\mathbf{E}\\|_{\\partial\\mathcal{P}}\\!}&{=\\left\\|\\displaystyle\\sum_{c\\in\\mathcal{P}}\\mathbf{J}_{c}\\mathbf{E}+\\mathbf{Q}_{c}\\mathbf{E}\\right\\|_{\\partial\\mathcal{P}}}\\\\ &{\\quad\\quad=\\left\\|\\overline{{\\mathbf{E}}}_{\\mathcal{P}}+\\mathbf{B}_{\\partial\\mathcal{P}^{c}}^{\\dagger}\\mathbf{B}_{\\partial\\mathcal{P}^{c}}\\mathbf{E}\\right\\|_{\\partial\\mathcal{P}}}\\\\ &{\\quad\\quad\\leq\\left\\|\\overline{{\\mathbf{E}}}_{\\mathcal{P}}\\right\\|_{\\partial\\mathcal{P}}+\\left\\|\\mathbf{B}_{\\partial\\mathcal{P}^{c}}^{\\dagger}\\mathbf{B}_{\\partial\\mathcal{P}^{c}}\\mathbf{E}\\right\\|_{\\partial\\mathcal{P}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "527where $\\overline{{\\mathbf{E}}}_{\\mathcal{P}}$ is obtained by setting the error signal on every cluster to its mean. ", "page_idx": 16}, {"type": "text", "text": "528  For the first term on the right-hand side, let us denote by $\\epsilon_{\\mathcal{C}}$ the value of any row of $\\overline{{\\mathbf{E}}}_{\\mathcal{P}}$ belonging to   \n529   cluster $\\mathcal{C}$ , which is equal to the mean of errors $\\mathbf{E}$ over that cluster. Also, we denote by $(\\overline{{\\mathbf{E}}}_{\\mathcal{P}})_{\\partial\\mathcal{P}}$ the   \n530 signal obtained from $\\overline{{\\mathbf{E}}}_{\\mathcal{P}}$ by setting its rows corresponding to nodes that are not adjacent to any edge   \n531 in the boundary $\\partial\\mathcal{P}$ to zeros. Also, let $\\partial_{v}\\mathcal{C}$ denote the inner boundary of set of nodes $\\mathcal{C}$ ,i.e. nodes of   \n532 $\\mathcal{C}$ that connect it to its complementary. Then it holds that: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|{\\widetilde{\\mathbf{E}}_{P}}\\right|\\!\\!\\!}&{_{P}\\!\\!\\!}\\\\ &{=\\left\\|{\\mathbf{B}_{Q\\mathcal{P}}}\\!\\!\\left\\mathbb{E}_{P}\\right\\|_{\\mathcal{D}}\\right\\|_{\\mathcal{A}}}\\\\ &{\\le\\left\\|{\\mathbf{B}_{\\mathcal{P}}}\\right\\|_{\\mathcal{D}_{1}}\\left|\\left({\\mathbf{E}}_{P}\\right)\\!\\!\\log p\\right\\|_{\\mathcal{D}_{1}}}\\\\ &{\\le\\left\\|{\\mathbf{B}_{\\mathcal{P}}}\\right\\|_{\\mathcal{D}_{2}}\\left|\\left({\\widehat{\\mathbf{E}}}_{P}\\right)\\!\\!\\log p\\right\\|_{\\mathcal{F}}}\\\\ &{=\\left\\|{\\mathbf{B}_{\\mathcal{P}}}\\right\\|_{\\mathcal{D}_{1}}\\!\\!\\left\\|\\left({\\widehat{\\mathbf{E}}}_{P}\\right)\\!\\!\\log p\\right\\|_{\\mathcal{F}}}\\\\ &{=\\left\\|{\\mathbf{B}_{\\mathcal{P}}}\\right\\|_{\\mathcal{D}_{1}}\\!\\!\\left\\|\\sum_{\\ell\\in\\mathcal{P}}\\!\\!\\left\\|\\phi_{0}{C}\\right\\|_{\\mathcal{C}}\\!\\!\\right\\|_{\\mathcal{C}}^{2}}\\\\ &{=\\left\\|{\\mathbf{B}_{\\mathcal{P}}}\\right\\|_{\\mathcal{D}_{1}}\\!\\!\\int_{\\mathcal{C}\\mathcal{C}}\\!\\!\\!\\!\\!\\!\\int\\!\\!\\frac{|\\phi_{0}{C}|}{|\\mathcal{C}|}\\!\\!\\left|\\mathcal{C}\\right|\\!\\!\\left\\|\\epsilon_{\\ell}\\right\\|_{\\mathcal{C}}^{2}}\\\\ &{\\le\\left\\|{\\mathbf{B}_{\\mathcal{P}}}\\right\\|_{\\mathcal{D}_{1}}\\!\\!\\!\\frac{\\operatorname*{max}{\\zeta}|\\!\\!\\zeta|\\!}{\\operatorname*{cup}}\\sqrt{\\epsilon{\\zeta\\zeta}}\\sqrt{\\epsilon{\\zeta\\zeta}}\\!\\!\\left\\|\\epsilon_{\\ell}\\right\\|_{\\mathcal{C}}^{2}}\\\\ &{=\\sqrt{2}w(\\mathcal{D})\\!\\!\\!\\operatorname*{max}\\sqrt{\\epsilon{\\zeta\\zeta}}\\!\\!\\left\\|{\\mathbf{E}}_{P}\\right\\|_{\\mathcal{C}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "533  For the second term, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Big\\|\\mathbf{B}_{\\partial p}^{\\dagger},\\mathbf{B}_{\\partial p^{*}}\\mathbf{E}\\Big\\|_{\\partial p^{*}}\\=\\Big\\|\\mathbf{B}_{\\partial p}\\mathbf{B}_{\\partial p^{*}}\\mathbf{B}_{\\partial p^{*}}\\mathbf{E}\\Big\\|_{2,1}}&{}\\\\ &{\\qquad\\qquad\\leq\\Big\\|\\mathbf{B}_{\\partial p}\\mathbf{B}_{\\partial p^{*}}\\mathbf{B}_{\\partial p^{*}}\\Big\\|_{\\partial_{1}}\\mathbf{E}\\Big\\|_{\\partial p^{*}}}\\\\ &{\\qquad\\qquad\\leq\\Big\\|\\mathbf{B}_{\\partial p^{*}}\\mathbf{B}_{\\partial p^{*}}\\Big\\|_{\\mathbf{F}}\\Big\\|\\mathbf{E}\\Big\\|_{\\partial p^{*}}}\\\\ &{\\qquad\\leq\\Big\\|(\\mathbf{B}_{\\partial p^{*}}\\mathbf{\\hat{\\xi}}^{\\mathsf{T}}\\mathbf{B}_{\\partial p}^{\\top}\\mathbf{\\hat{\\xi}}^{\\mathsf{T}})\\mathbf{B}_{\\partial p}^{\\top}\\Big\\|_{\\mathbf{F}}\\Big\\|\\mathbf{E}\\Big\\|_{\\partial p^{*}}}\\\\ &{\\qquad\\leq\\|\\mathbf{B}_{\\partial p^{*}}^{\\top}\\|_{2,1}\\sqrt{\\left\\|\\mathbf{B}_{\\partial p^{*}}(\\mathbf{B}_{\\partial p^{*}}^{\\top})^{\\top}\\right\\|_{\\infty,\\infty}}\\|\\mathbf{E}\\|_{\\partial p^{*}}\\ \\ \\mathrm{(by~Propsition~1)}}\\\\ &{\\qquad=\\frac{\\left\\|\\mathbf{B}_{\\partial p}^{\\top}\\mathbf{B}_{\\partial p}^{\\top}\\mathbf{\\hat{\\xi}}\\right\\|_{1,1}}{\\operatorname*{min}}\\|\\mathbf{E}\\|_{\\partial p^{*}}}\\\\ &{\\qquad\\overset{\\mathrm{consin~}}{\\underset{\\mathrm{min}}{\\leq}}\\zeta_{\\sigma}(C)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "534  The result is obtained by combining Equations (20) to (22) ", "page_idx": 16}, {"type": "text", "text": "535 Theorem 4 (Theorem 2.1 of Hsu et al. [2012]). At time step $t_{;}$ let $\\mathbf{A}\\in\\mathbb{R}^{b\\times t}$ where $b\\in\\mathbb{N}^{*}$ andlet   \n536 ${\\bf v}\\in\\mathbb{R}^{t}$ be a random vector such that for some $\\sigma\\geq0$ wehave ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\exp(\\langle\\mathbf{u},\\mathbf{v}\\rangle)\\right]\\leq\\exp\\!\\left(\\left\\|\\mathbf{u}\\right\\|^{2}\\!\\frac{\\sigma^{2}}{2}\\right)\\quad\\forall\\mathbf{u}\\in\\mathbb{R}^{t}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "537 Then for any $\\delta\\in(0,1)$ , we have with a probability at least $1-\\delta$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\Vert\\mathbf{Av}\\right\\Vert^{2}\\leq\\sigma^{2}\\left(\\left\\Vert\\mathbf{A}\\right\\Vert_{F}^{2}+2\\left\\Vert\\mathbf{A}^{\\top}\\mathbf{A}\\right\\Vert_{F}\\sqrt{\\log\\frac{1}{\\delta}}+2\\left\\Vert\\mathbf{A}\\right\\Vert^{2}\\log\\frac{1}{\\delta}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "538 Lemma 3 (Empirical process bound). Let $\\mathbf{X}_{m}\\in\\mathbb{R}^{|\\mathcal{T}_{m}|\\times d}$ denotes the matrix of collected context   \n539  vectors for task $m\\in\\mathcal{V}$ then, given collected context matrices $\\{\\mathbf{X}_{m}\\}_{m\\in\\mathcal{V}}$ for any $\\delta\\,\\in\\,(0,1)$ we   \n540 have with probability of at least $1-\\delta$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\big\\|\\mathbf{K}\\big\\|_{F}\\leq\\frac{\\alpha_{\\delta}(t)}{\\alpha_{0}}t,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "541 where ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\alpha_{\\delta}(t):=\\frac{\\alpha_{0}\\sigma}{t}\\sqrt{t+2\\sqrt{\\sum_{m\\in\\mathcal{V}}\\left|\\mathcal{T}_{m}(t)\\right|^{2}\\log\\frac{1}{\\delta}}+2\\operatorname*{max}_{m\\in\\mathcal{V}}\\left|\\mathcal{T}_{m}(t)\\right|\\log\\frac{1}{\\delta}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "542  Proof. We recall that $\\mathbf{K}\\in\\mathbb{R}^{t\\times d}$ is the matrix obtained by stacking the row vectors $\\eta_{m}^{\\top}\\mathbf{X}_{m}$ vertically.   \n543  On the one hand, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\big\\|\\mathbf{K}\\big\\|_{F}^{2}=\\sum_{m\\in\\mathcal{V}}\\big\\|\\mathbf{X}_{m}^{\\top}\\pmb{\\eta}_{m}\\big\\|^{2}=\\big\\|\\mathbf{X}_{\\mathcal{V}}^{\\top}\\pmb{\\eta}\\big\\|^{2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "544  where $\\begin{array}{r}{\\mathbf{X}_{\\mathcal{V}}:=\\operatorname{diag}(\\mathbf{X}_{1},\\cdot\\cdot\\cdot\\mathbf{\\mu},\\mathbf{X}_{|\\mathcal{V}|})\\in\\mathbb{R}^{t\\times d|\\mathcal{V}|}}\\end{array}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[P(t)\\right]=\\mathbb{E}\\left[\\mathbb{E}\\left[\\exp\\{u_{t}\\eta_{t}\\}P(t-1)|\\mathcal{F}_{t-1}\\right]\\right]\\quad\\mathrm{(by~the~law~of~total~spectation)}}\\\\ &{\\qquad\\qquad=\\mathbb{E}\\left[P(t-1)\\mathbb{E}\\left[\\exp(u_{t}\\eta_{t})|\\mathcal{F}_{t-1}\\right]\\quad\\mathrm{(because~}\\{\\eta_{s}\\}_{s=1}^{t-1}\\mathrm{~are~}\\mathcal{F}_{t-1}\\mathrm{~measurable.)}}\\\\ &{\\qquad\\qquad\\leq\\exp\\biggl(\\frac{1}{2}\\sigma^{2}u_{t}^{2}\\biggr)\\mathbb{E}\\left[P(t-1)\\right]\\quad\\mathrm{(by~the~conditional~subgaussianity~assumption)}}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\prod_{s=1}^{t}\\exp\\biggl(\\frac{1}{2}\\sigma^{2}u_{s}^{2}\\biggr)\\quad\\mathrm{(by~induction)}}\\\\ &{\\qquad\\qquad=\\exp\\biggl(\\frac{1}{2}\\sigma^{2}\\|\\mathbf{u}\\|^{2}\\biggr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "546 From Equations (24) and (25), we can apply Theorem 4 to matrix $\\mathbf{X}_{\\nu}$ and random vector $\\eta$ ,which   \n547 implies that with a probability at least $1-\\delta$ ,wehave ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\mathbf{X}_{\\mathcal{V}}\\eta\\|\\leq\\sigma\\sqrt{\\operatorname{Tr}\\biggl(\\sum_{m\\in\\mathcal{V}}\\mathbf{A}_{m}\\biggr)+2\\sqrt{\\sum_{m\\in\\mathcal{V}}\\|\\mathbf{A}_{m}\\|_{F}^{2}\\log\\frac{1}{\\delta}}+2\\operatorname*{max}_{m\\in\\mathcal{V}}\\|\\mathbf{A}_{m}\\|\\log\\frac{1}{\\delta}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "548  where we used the qualies $\\begin{array}{r}{\\|\\mathbf{X}_{\\nu}\\|_{F}=\\sum_{m\\in\\nu}\\mathrm{Tr}(\\mathbf A_{m}),\\|\\mathbf{X}_{\\nu}\\|^{2}=\\underset{m\\in\\nu}{\\mathrm{max}}\\,\\|\\mathbf A_{m}\\|}\\end{array}$ and $\\left\\Vert\\mathbf{X}_{\\nu}\\mathbf{X}_{\\nu}^{\\top}\\right\\Vert_{F}^{2}=$   \n549 $\\begin{array}{r}{\\left\\|\\mathbf{X}_{\\mathcal{V}}^{\\top}\\mathbf{X}_{\\mathcal{V}}\\right\\|_{F}^{2}=\\sum_{m\\in\\underline{{\\mathcal{V}}}}\\left\\|\\mathbf{A}_{m}\\right\\|_{F}^{2}.}\\end{array}$ To ariv thethe statementof the theorem, we usethefct thathe   \n550 context vectors have Euclidean norms of at most 1. ", "page_idx": 17}, {"type": "text", "text": "551 ", "page_idx": 17}, {"type": "text", "text": "552 Proposition 4 (Probabilistic inequality). With a probabability at least $1-\\delta_{i}$ wehave ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{1}{2t\\alpha}\\sum_{m\\in\\mathcal{V}}\\left\\|\\mathbf{X}_{m}\\boldsymbol{\\epsilon}_{m}\\right\\|^{2}+a_{1}(\\mathcal{G},\\Theta)\\|\\mathbf{E}\\|_{\\partial\\mathcal{P}^{c}}\\leq a_{2}(\\mathcal{G},\\Theta)\\big\\|\\overline{{\\mathbf{E}}}_{\\mathcal{P}}\\big\\|_{F}+(1-\\kappa)\\|\\mathbf{E}\\|_{\\partial\\mathcal{P}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "553 where $\\begin{array}{r}{0\\le\\kappa<\\frac{\\displaystyle\\operatorname*{min}_{\\mathcal{C}\\in\\mathcal{P}}{\\sqrt{c_{\\mathcal{G}}(\\mathcal{C})}}}{2w(\\partial\\mathcal{P})},\\,\\frac{1}{\\alpha_{0}}<\\displaystyle\\operatorname*{min}_{\\mathcal{C}\\in\\mathcal{P}}{\\sqrt{c_{\\mathcal{G}}(\\mathcal{C})}}-2\\kappa w(\\partial\\mathcal{P})\\,a n}\\end{array}$ d ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle a_{1}(\\mathcal{G},\\Theta)=1-\\frac{\\frac{1}{\\alpha_{0}}+2\\kappa w(\\partial\\mathcal{P})}{\\operatorname*{min}_{\\mathcal{C}\\in\\mathcal{P}}\\sqrt{c_{\\mathcal{G}}(\\mathcal{C})}}}\\\\ {\\displaystyle a_{2}(\\mathcal{G},\\Theta)=\\frac{1}{\\alpha_{0}}+\\sqrt{2}\\kappa w(\\partial\\mathcal{P})\\operatorname*{max}_{\\mathcal{C}\\in\\mathcal{P}}\\sqrt{\\iota_{\\mathcal{G}}(\\mathcal{C})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "554 Proof. The proof is a combination of the results of Lemmas 1 to 3. We have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{2t\\alpha_{\\delta}}\\sum_{m\\in\\mathcal{V}}\\|\\mathbf{X}_{m}\\boldsymbol{\\epsilon}_{m}\\|^{2}+\\|\\mathbf{E}\\|_{\\partial P^{c}}}\\\\ {\\displaystyle\\leq\\frac{1}{t\\alpha_{\\delta}}\\left\\langle\\mathbf{K},\\mathbf{E}\\right\\rangle+\\|\\mathbf{E}\\|_{\\partial P}\\quad\\mathrm{(by~Lemma~1)}}\\\\ {\\displaystyle\\leq\\frac{1}{\\alpha_{0}}\\|\\mathbf{E}\\|_{F}+\\kappa\\|\\mathbf{E}\\|_{\\partial P}+(1-\\kappa)\\|\\mathbf{E}\\|_{\\partial P}\\quad\\mathrm{(by~Lemma~3)}}\\\\ {\\displaystyle\\leq\\frac{\\|\\overline{{\\mathbf{E}}}\\mathcal{P}\\|_{F}}{\\alpha_{0}}+\\frac{\\|\\mathbf{E}\\|_{\\partial P^{c}}}{\\alpha_{0}\\frac{\\operatorname*{min}}{c\\epsilon P}\\sqrt{c_{\\mathcal{G}}(\\mathcal{C})}}+\\kappa w(\\partial P)\\left(\\sqrt{2}\\underset{c\\in\\mathcal{P}}{\\operatorname*{max}}\\sqrt{\\iota_{\\mathcal{G}}(\\mathcal{C})}\\|\\overline{{\\mathbf{E}}}_{P}\\|_{F}+2\\frac{\\|\\mathbf{E}\\|_{\\partial P^{c}}}{\\underset{c\\in\\mathcal{P}}{\\operatorname*{min}}\\sqrt{c_{\\mathcal{G}}(\\mathcal{C})}}\\right)+(1-\\kappa)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "555 where the last line is an application of Lemma 2. Grouping the terms by the type of norm applied to   \n556 $\\mathbf{E}$ finishes the proof. \u53e3   \n557 Theorem 1 (Oracle inequality). Assume that the $R E$ assumption holds for the empirical multi  \n558task Grammarixwihconstants $\\begin{array}{r l r}{\\kappa}&{\\in}&{\\left[0,\\frac{1}{2w\\left(\\partial\\mathcal{P}\\right)}\\underset{\\mathcal{C}\\in\\mathcal{P}}{\\operatorname*{min}}\\,\\sqrt{c_{\\mathcal{G}}(\\mathcal{C})}\\right)}\\end{array}$ and $\\phi~>~0$ \uff1aSuppose that   \n559 $\\mathrm{max}_{m\\in\\mathcal{V}}\\left|\\mathcal{T}_{m}(t)\\right|\\leq b t$ for some $b>0$ Then,with a probability at least $1-\\delta(t)$ we have ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\lVert\\Theta-\\hat{\\Theta}(t)\\right\\rVert_{F}\\le2\\frac{\\sigma}{\\phi^{2}\\sqrt{t}}f(\\mathcal{G},\\Theta)\\sqrt{1+2b\\sqrt{|\\mathcal{V}|\\log\\frac{1}{\\delta(t)}}}+2b\\log\\frac{1}{\\delta(t)},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "560 where ", "page_idx": 18}, {"type": "equation", "text": "$$\nf(\\mathcal{G},\\Theta):=\\alpha_{0}\\left(a_{2}(\\mathcal{G},\\Theta)+\\sqrt{2}\\mathbb{1}_{\\leq1}(\\kappa)w(\\partial\\mathcal{P})\\right)\\left(\\frac{a_{2}(\\mathcal{G},\\Theta)+\\sqrt{2}\\mathbb{1}_{\\leq1}(\\kappa)w(\\partial\\mathcal{P})}{a_{1}(\\mathcal{G},\\Theta)\\operatorname*{min}_{c\\in\\mathcal{P}}\\sqrt{c_{\\mathcal{G}}(\\mathcal{C})}}+1\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "561 Proof. Using the previously established results, we obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\frac{1}{2t}\\sum_{n\\in\\mathcal{N}}|\\mathbf{X}_{n}\\varepsilon_{n}|^{2}+\\alpha||\\mathbf{E}||_{\\mathcal{D}^{n}}}\\\\ &{\\le\\alpha_{\\delta}\\alpha_{\\ell}(\\Theta,\\mathcal{G})||\\mathbf{E}_{P}||_{F}+\\alpha_{\\delta}(1-\\kappa)^{+}|\\mathbf{E}||_{\\partial\\mathcal{P}}\\quad(\\mathrm{by~Propsition~4})}\\\\ &{-\\alpha_{\\delta}\\alpha_{\\ell}(\\Theta,\\mathcal{G})||\\mathbf{E}_{P}||_{F}+\\alpha_{\\delta}(1-\\kappa)^{+}\\left|\\mathbf{B}_{\\partial\\mathcal{P}}\\mathbf{B}_{\\partial\\mathcal{P}}^{\\dagger}\\mathbf{B}_{\\partial\\mathcal{P}}\\mathbf{E}\\right|\\Big|_{2,1}\\quad(\\mathrm{by~propertics~of~the~pseudo-invers})}\\\\ &{\\le\\alpha_{\\delta}\\alpha_{\\alpha}(\\Theta,\\mathcal{G})||\\mathbf{E}_{P}||_{F}+\\alpha_{\\delta}||\\mathbf{B}_{\\partial\\mathcal{P}}||_{2,1}\\le_{1}\\mathrm{(b)}(1-\\kappa)^{+}\\left|\\mathbf{B}_{\\partial\\mathcal{P}}^{\\dagger}\\mathbf{B}_{\\partial\\mathcal{P}}\\mathbf{E}\\right|\\quad(\\mathrm{by~Proposition~1})}\\\\ &{\\le\\alpha_{\\delta}(\\alpha_{\\ell}(\\Theta,\\mathcal{G})+1_{\\le1}(\\kappa)\\sqrt{2\\upsilon(\\partial P)})|\\mathbf{E}||_{\\mathrm{RE}}\\quad(\\mathrm{by~definition~of~the~\\ensuremath{\\mathbb{I}}||\\partial_{K}\\mathbf{B}=\\partial\\Omega~})}\\\\ &{\\le\\alpha_{\\delta}^{2}(\\Theta,\\mathcal{G})+\\mathbf{1}_{\\le1}(\\kappa)\\sqrt{2\\upsilon(\\partial P)}\\sqrt{\\sum_{n\\in\\mathcal{V}}\\left|\\mathbf{\\varepsilon}\\right|_{\\Delta_{n}}^{2}}\\quad(\\mathrm{using~the~RE~assumption})}\\\\ &{\\le\\frac{\\beta\\alpha_{\\delta}^{2}(\\alpha_{\\delta}(\\Theta,\\mathcal{G})+1_{\\le1}(\\kappa)\\|\\mathbf{B}_{\\partial\\mathcal{P}}\\|_{2,1})^{2}}{2\\phi^{2}}+\\frac{1}{2\\beta\\mu\\_{\\tau}\\kappa}\\sum_{i=\\mathcal{V}}\\| \n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "whell $\\beta>0$ andisaqu $u v\\leq\\frac{u^{2}+v^{2}}{2}$ 563 for any $u,v\\in\\mathbb{R}$ ", "page_idx": 19}, {"type": "text", "text": "564  As a result, we can bound the norm of $\\mathbf{Q}_{\\partial\\mathcal{P}^{c}}\\mathbf{E}$ as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{Q}_{\\partial\\mathcal{P}^{c}}\\mathbf{E}\\|_{F}=\\left\\|\\mathbf{B}_{\\partial\\mathcal{P}^{c}}^{\\dagger}\\mathbf{B}_{\\partial\\mathcal{P}^{c}}\\mathbf{E}\\right\\|_{F}}\\\\ &{\\qquad\\qquad\\leq\\sqrt{\\left\\|\\mathbf{L}_{\\partial\\mathcal{P}^{c}}^{\\dagger}\\right\\|_{\\infty,\\infty}}\\|\\mathbf{E}\\|_{\\partial\\mathcal{P}^{c}}}\\\\ &{\\qquad\\qquad\\leq\\frac{2\\alpha_{\\delta}\\left(a_{2}\\left(\\Theta,\\mathcal{G}\\right)+\\;\\mathbb{1}_{\\leq1}\\left(\\kappa\\right)\\left\\|\\mathbf{B}_{\\partial\\mathcal{P}}\\right\\|_{2,1}\\right)^{2}}{\\phi^{2}a_{1}\\left(\\Theta,\\mathcal{G}\\right)\\operatorname*{min}\\sqrt{c_{\\mathcal{G}}\\left(\\mathcal{C}\\right)}}\\quad\\mathrm{(Equation~(29)~with~}\\beta=1\\mathrm{)}\\mathrm{.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "565  We can also bound the norm of $\\overline{{\\mathbf{E}}}_{\\mathcal{P}}$ as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|\\overline{{\\mathbf{E}}}_{\\mathcal{P}}\\right\\|_{F}^{2}}&{\\leq\\frac{1}{t\\phi^{2}}\\displaystyle\\sum_{m\\in\\mathcal{V}}\\left\\|\\mathbf{X}_{m}\\boldsymbol{\\epsilon}_{m}\\right\\|^{2}\\quad(\\mathrm{by~RE~assumption~on~empirical~multi\\cdottask~Gram~matrix})}\\\\ &{\\qquad\\qquad\\leq\\frac{4\\alpha_{\\delta}^{2}\\left(a_{2}(\\Theta,\\mathcal{G})+\\mathbb{1}_{\\leq1}(\\kappa)\\|\\mathbf{B}_{\\partial\\mathcal{P}}\\|_{2,1}\\right)^{2}}{\\phi^{4}}\\quad(\\mathrm{by~Equation~}(29)\\mathrm{~with~}\\beta=2).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "566 The result is then obtained by combining Equations (30) and (31) along with using the fact that   \n567 $\\mathbf{E}=\\overline{{\\mathbf{E}}}_{\\mathcal{P}}+\\mathbf{Q}_{\\partial\\mathcal{P}^{c}}\\mathbf{E}$ and the expressions of $a_{1}(\\Theta,\\mathcal{G})$ and $a_{2}(\\Theta,\\mathcal{G})$ ,and bounding $\\alpha_{\\delta}(t)$ as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\alpha_{\\delta}(t)^{2}}{\\alpha_{0}^{2}}=\\frac{\\sigma^{2}}{t^{2}}\\left(\\displaystyle\\sum_{m\\in V}\\|\\mathbf{X}_{m}\\|_{F}^{2}+2\\sqrt{\\sum_{m\\in V}\\|\\mathbf{X}_{m}\\mathbf{X}_{m}^{\\top}\\|_{F}^{2}\\log\\frac{1}{\\delta}}+2\\operatorname*{max}_{m\\in V}\\|\\mathbf{X}_{m}\\|^{2}\\log\\frac{1}{\\delta}\\right)}\\\\ &{\\qquad\\le\\frac{\\sigma^{2}}{t^{2}}\\left(t+2\\sqrt{\\sum_{m\\in V}|T_{m}(t)|^{2}\\log\\frac{1}{\\delta}}+2\\operatorname*{max}_{m\\in V}|T_{m}(t)|\\log\\frac{1}{\\delta}\\right)}\\\\ &{\\qquad\\le\\frac{\\sigma^{2}}{t^{2}}\\left(t+2t\\sqrt{\\log\\frac{1}{\\delta}}+2t\\log\\frac{1}{\\delta}\\right)}\\\\ &{\\qquad\\le2\\frac{\\sigma^{2}}{t}\\left(1+\\sqrt{\\log\\frac{1}{\\delta}}\\right)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "568 ", "page_idx": 19}, {"type": "text", "text": "569 B.3  Inheriting the RE condition from the true to the empirical data Gram matrix ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "570 B.3.1 From the adapted to the empirical multi-task Gram matrix ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "571  Lemma 4 (Bounding a quadratic form using projections). Let $\\mathbf{M}_{1},\\,\\cdot\\,\\cdot\\,,\\mathbf{M}_{p}\\in\\mathbb{R}^{d\\times d}$ be symmetric   \n572 matrices, and let $\\begin{array}{r}{\\mathbf{J}:=\\frac{1}{p}\\mathbf{1}\\mathbf{1}^{\\top}}\\end{array}$ , and $\\mathbf Q=\\mathbf I-\\mathbf J$ Then, for any $\\mathbf{Z}\\in\\mathbb{R}^{p\\times d}$ with rows $\\{{\\bf z}_{i}\\}_{i=1}^{p}$ , we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left|\\sum_{i=1}^{p}\\mathbf{z}_{i}^{\\top}\\mathbf{M}_{i}\\mathbf{z}_{i}\\right|\\leq\\frac{1}{p}\\left\\|\\sum_{i=1}^{p}\\mathbf{M}_{i}\\right\\|\\|\\mathbf{Z}\\|_{\\mathbf{J}}^{2}+2\\sqrt{\\left\\|\\frac{1}{p}\\sum_{i=1}^{p}\\mathbf{M}_{i}^{2}\\right\\|}\\|\\mathbf{Z}\\|_{\\mathbf{Q}}\\|\\mathbf{Z}\\|_{\\mathbf{J}}+\\operatorname*{max}_{1\\leq i\\leq p}\\|\\mathbf{M}_{i}\\|\\|\\mathbf{Z}\\|_{\\mathbf{Q}}^{2}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "573 Proof. We have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\displaystyle\\sum_{i=1}^{p}\\mathbf z_{i}^{\\top}\\mathbf M_{i}\\mathbf z_{i}\\right|=\\left|\\displaystyle\\sum_{i=1}^{p}\\bar{\\mathbf z}^{\\top}\\mathbf M_{i}\\bar{\\mathbf z}+2\\displaystyle\\sum_{i=1}^{p}(\\mathbf z_{i}-\\bar{\\mathbf z})^{\\top}\\mathbf M_{i}\\bar{\\mathbf z}+\\displaystyle\\sum_{i=1}^{p}(\\mathbf z_{i}-\\bar{\\mathbf z})^{\\top}\\mathbf M_{i}(\\mathbf z_{i}-\\bar{\\mathbf z})\\right|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left|\\displaystyle\\bar{\\mathbf z}^{\\top}\\displaystyle\\sum_{i=1}^{p}\\mathbf M_{i}\\bar{\\mathbf z}\\right|+2\\left|\\displaystyle\\sum_{i=1}^{p}\\mathbf e_{i}^{\\top}\\mathbf Q\\mathbf Z\\mathbf M_{i}\\bar{\\mathbf z}\\right|+\\left|\\displaystyle\\sum_{i=1}^{p}\\mathbf e_{i}^{\\top}\\mathbf Q\\mathbf Z\\mathbf M_{i}\\mathbf Z^{\\top}\\mathbf Q\\mathbf e_{i}\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "574  where we used the fact that $\\mathbf{z}_{i}-\\bar{\\mathbf{z}}=\\mathbf{Z}^{\\top}\\mathbf{e}_{i}-\\mathbf{Z}^{\\top}\\mathbf{J}\\mathbf{e}_{i}=\\mathbf{Z}^{\\top}\\mathbf{Q}\\mathbf{e}_{i}.$ ", "page_idx": 20}, {"type": "text", "text": "575 Let us now examine every term on the right-hand side of Equation (32). For the first term, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left|\\bar{\\mathbf{z}}^{\\top}\\sum_{i=1}^{p}\\mathbf{M}_{i}\\bar{\\mathbf{z}}\\right|\\leq\\left\\|\\sum_{i=1}^{p}\\mathbf{M}_{i}\\right\\|\\left\\|\\bar{\\mathbf{z}}\\right\\|^{2}=\\left\\|\\frac{1}{p}\\sum_{i=1}^{p}\\mathbf{M}_{i}\\right\\|\\left\\|\\mathbf{Z}\\right\\|_{\\mathbf{J}}^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "576  For the second term, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{\\sqrt{\\lambda}}{\\alpha_{1}}e^{\\frac{1}{(2\\beta2\\lambda_{2})}}|\\lambda|_{\\infty}^{2}\\left\\{\\frac{\\sqrt{\\lambda}}{\\alpha_{1}},\\alpha_{2};\\sqrt{\\alpha_{3}},\\right\\}|\\alpha_{1}|}&{{}}\\\\ {=}&{\\left\\{\\frac{\\sqrt{\\lambda}}{\\alpha_{1}}e^{\\frac{1}{(\\alpha_{1}-\\alpha_{2})}}|\\alpha_{1}|,\\alpha_{2}|,\\alpha_{3}|,\\alpha_{4}|,\\alpha_{5}\\right\\}|\\alpha_{1}|}\\\\ {=}&{{}\\left\\{\\frac{\\sqrt{\\lambda}}{\\alpha_{1}}e^{\\frac{1}{(\\alpha_{1}-\\alpha_{2})}}|\\alpha_{1}|,\\alpha_{4}|,\\alpha_{5}\\right\\}|\\alpha_{1}|}\\\\ {=}&{{}\\left\\{\\frac{\\sqrt{\\lambda}}{\\alpha_{1}}e^{\\frac{1}{(\\alpha_{1}-\\alpha_{3})}}\\left|\\alpha_{2}\\right|,\\alpha_{3}|,\\alpha_{4}\\right\\}}\\\\ {=}&{{}\\sqrt{\\left|\\frac{\\sqrt{\\lambda}}{\\alpha_{1}}e^{\\frac{1}{(\\alpha_{1}-\\alpha_{2})}}\\sum_{i=1}^{\\infty}\\alpha_{3}\\right|}\\left|\\alpha_{2}\\right\\rangle_{i,1}|}\\\\ {=}&{{}\\sqrt{\\left|\\frac{\\sqrt{\\lambda}}{\\alpha_{1}}\\sum_{i=1}^{\\infty}\\left(\\alpha_{1}\\right)\\alpha_{1}|,\\alpha_{3}|,\\alpha_{4}|,\\alpha_{1}|,\\alpha_{2}|,\\alpha_{1}|,\\alpha_{2}|,\\alpha_{3}|,\\alpha_{4}|,\\alpha_{4}\\rangle}{\\left|\\alpha_{1}\\right|,\\alpha_{2}\\right\\rangle}}\\\\ {=}&{{}\\sqrt{\\left|\\frac{\\sqrt{\\lambda}}{\\alpha_{1}}\\sum_{i=1}^{\\infty}\\left(\\alpha_{1}\\right)\\alpha_{1}|,\\alpha_{3}|,\\alpha_{4}|,\\alpha_{4}|,\\alpha_{5}\\right\\}}|\\alpha_{2}|,\\alpha_{1}|,\\alpha_{1}|,\\alpha_{6}\\rangle}\\\\ {=}&{{}\\sqrt{\\left|\\frac{\\sqrt{\\lambda}}{\\alpha_{1}}\\sum_{i=1}^{\\infty}\\left|\\alpha_{2\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "577 Finally, for the last term, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\left\\lvert\\displaystyle\\sum_{i=1}^{p}\\mathbf{e}_{i}^{\\top}\\mathbf{Q}\\mathbf{Z}\\mathbf{M}_{i}\\mathbf{Z}^{\\top}\\mathbf{Q}\\mathbf{e}_{i}\\right\\rvert\\leq\\displaystyle\\sum_{i=1}^{p}\\left\\lVert\\mathbf{M}_{i}\\right\\rVert\\left\\lVert\\mathbf{Z}^{\\top}\\mathbf{Q}\\mathbf{e}_{i}\\right\\rVert^{2}}&{}\\\\ {\\displaystyle\\leq\\displaystyle\\operatorname*{max}_{1\\leq i\\leq p}\\left\\lVert\\mathbf{M}_{i}\\right\\rVert\\displaystyle\\sum_{i=1}^{p}\\left\\lVert\\mathbf{Z}^{\\top}\\mathbf{Q}\\mathbf{e}_{i}\\right\\rVert^{2}}&{}\\\\ {\\displaystyle}&{=\\displaystyle\\operatorname*{max}_{1\\leq i\\leq p}\\left\\lVert\\mathbf{M}_{i}\\right\\rVert\\left\\lVert\\mathbf{Q}\\mathbf{Z}\\right\\rVert_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "578 Combining Equations (33) to (35) yields the result. ", "page_idx": 20}, {"type": "text", "text": "579 We also define an operator norm that is induced by the $\\|\\|_{\\mathrm{RE}}$ introduced in Definition 2. ", "page_idx": 20}, {"type": "text", "text": "580 Definition 3 ( $(\\boldsymbol{\\mathrm{RE}},\\boldsymbol{S}),$ -induced operator norm). Let $\\{\\mathbf{M}_{m}\\}_{m\\in\\mathcal{V}}\\,\\subseteq\\,\\mathbb{R}^{d\\times d}$ be symmetric matrices   \n581 associated to the graph nodes $\\nu$ and let $\\mathbf{M}_{\\mathcal{V}}:=\\operatorname{diag}\\left(\\mathbf{M}_{1},\\cdot\\cdot\\cdot\\cdot,\\mathbf{M}_{|\\mathcal{V}|}\\right)\\,\\in\\,\\mathbb{R}^{d|\\mathcal{V}|\\times d|\\mathcal{V}|}$ .For any   \n582 cluster $\\mathcal{C}\\in\\mathcal{P}$ , let the cluster mean and mean of squares associated to those matrices be given by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\overline{{\\mathbf{M}}}_{\\mathcal{C}}:=\\frac{1}{|\\mathcal{C}|}\\sum_{m\\in\\mathcal{C}}\\mathbf{M}_{m},\\qquad\\overline{{\\mathbf{M}^{2}}}_{\\mathcal{C}}:=\\frac{1}{|\\mathcal{C}|}\\sum_{m\\in\\mathcal{C}}\\mathbf{M}_{m}^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "583 The RE-induced operator norm of ${{\\bf{M}}_{\\nu}}$ is defined as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|\\mathbf{M}\\|_{\\mathrm{RE},S}:=\\operatorname*{max}_{\\boldsymbol{c}\\in\\mathcal{P}}\\|\\overline{{\\mathbf{M}}}_{\\boldsymbol{c}}\\|\\vee\\sqrt{\\operatorname*{min}_{\\boldsymbol{c}\\in\\mathcal{P}}c(\\boldsymbol{c})^{-1}\\operatorname*{max}_{\\boldsymbol{c}\\in\\mathcal{P}}\\left\\|\\overline{{\\mathbf{M}^{2}}}_{\\boldsymbol{c}}\\right\\|}\\vee\\operatorname*{min}_{\\boldsymbol{c}\\in\\mathcal{P}}c_{\\boldsymbol{c}}(\\boldsymbol{c})^{-1}\\operatorname*{max}_{m\\in\\mathcal{V}}\\|\\mathbf{M}_{m}\\|.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "584  B.3.2 Linking the adapted to the empirical Gram ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "585  We first start by establishing that given the closeness of two PSD matrices in a certain sense, the RE   \n586condition can be transferred between them. ", "page_idx": 21}, {"type": "text", "text": "587  Proposition 5 (Restricted spectral norm). Let $\\mathbf{Z}\\in\\mathbb{R}^{|\\mathcal{V}|\\times d}$ verifying ", "page_idx": 21}, {"type": "equation", "text": "$$\na_{1}(\\mathcal{G},\\Theta)\\|\\mathbf{Z}\\|_{\\partial\\mathcal{P}^{c}}\\leq a_{2}(\\mathcal{G},\\Theta)\\big\\|\\overline{{\\mathbf{Z}}}_{\\mathcal{P}}\\big\\|_{F}+(1-\\kappa)^{+}\\|\\mathbf{Z}\\|_{\\partial\\mathcal{P}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "588 Let $\\{\\mathbf{M}_{m}\\}_{m\\in\\mathcal{V}}\\subseteq\\mathbb{R}^{d\\times d}$ be symmetric matrices associated to the graph nodes $\\nu$ and let $\\mathbf{M}_{\\mathcal{V}}:=$   \n589 $\\mathrm{diag}(\\mathbf{M}_{1},\\cdot\\cdot\\cdot\\,,\\mathbf{M}_{|\\mathcal{V}|})\\in\\mathbb{R}^{d|\\mathcal{V}|\\times d|\\mathcal{V}|}$ . Then we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left|\\sum_{m\\in\\mathcal{V}}\\mathbf{z}_{m}^{\\top}\\mathbf{M}_{m}\\mathbf{z}_{m}\\right|\\leq\\|\\mathbf{M}\\|_{\\mathrm{RE},S}^{2}\\left(1+\\frac{a_{2}(\\mathcal{G},\\Theta)+(1-\\kappa)^{+}\\|\\mathbf{B}_{\\partial\\mathcal{P}}\\|_{2,1}}{a_{1}(\\mathcal{G},\\Theta)}\\right)^{2}\\|\\mathbf{Z}\\|_{\\mathrm{RE}}^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "590Proof. For any cluster $\\mathcal{C}$ , we denote by $\\mathbf{B}_{\\mathcal{C}}$ the incidence matrix obtained by setting the rows of $\\mathbf{B}$   \n591 outside the edges linking nodes in $\\mathcal{C}$ to null vectors. The latter's nullspace is the span of the vector $\\mathbf{1}_{\\mathcal{C}}$   \n592 having coordinates 1 at nodes in $\\mathcal{C}$ and zeros elsewhere. Hence, the projector onto the orthogonal of   \n593 $\\mathbf{1}_{\\mathcal{C}}$ is $\\mathbf{\\bar{Q}}_{\\mathcal{C}}:=\\mathbf{B}_{\\mathcal{C}}^{\\dagger}\\mathbf{B}_{\\mathcal{C}}$ ", "page_idx": 21}, {"type": "text", "text": "594 On the one hand, for any signal $\\mathbf{Z}\\in\\mathbb{R}^{|\\mathcal{V}|\\times d}$ wehave ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|{\\bf Z}\\|_{\\partial\\mathcal{P}^{c}}=\\displaystyle\\sum_{c\\in\\mathcal{P}}\\|{\\bf B}c{\\bf Z}\\|_{2,1}}\\\\ &{\\qquad\\ge\\displaystyle\\sum_{c\\in\\mathcal{P}}\\frac{\\left\\|{\\bf B}_{c}^{\\dagger}{\\bf B}c{\\bf Z}\\right\\|_{F}}{\\sqrt{\\left\\|{\\bf A}_{c}^{\\dagger}\\right\\|_{\\infty,\\infty}}}}\\\\ &{\\qquad\\ge\\displaystyle\\operatorname*{min}_{c\\in\\mathcal{P}}\\sqrt{c_{\\mathcal{Q}}(\\mathcal{C})}\\sum_{c\\in\\mathcal{P}}\\|{\\bf Z}\\|_{\\bf Q}_{c}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "595 Hence, by the proposition's assumptions, $\\mathbf{Z}$ verifies ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{min}_{c\\in\\mathcal{P}}\\sqrt{c_{\\mathcal{G}}(\\mathcal{C})}a_{1}(\\mathcal{G},\\Theta)\\sum_{c\\in\\mathcal{P}}\\|\\mathbf{Z}\\|_{\\mathbf{Q}_{c}}\\leq(a_{2}(\\mathcal{G},\\Theta)\\|\\overline{{\\mathbf{Z}}}_{\\mathcal{P}}\\|_{F}+(1-\\kappa)\\|\\mathbf{Z}\\|_{\\partial\\mathcal{P}})}&{}\\\\ {\\leq a_{2}(\\mathcal{G},\\Theta)\\|\\overline{{\\mathbf{Z}}}_{\\mathcal{P}}\\|_{F}+(1-\\kappa)^{+}\\|\\mathbf{B}_{\\partial\\mathcal{P}}\\|_{2,1}\\Big\\|\\mathbf{B}_{\\partial\\mathcal{P}}^{\\dagger}\\mathbf{B}_{\\partial\\mathcal{P}}\\mathbf{Z}\\Big\\|}&{}\\\\ {\\leq(a_{2}(\\mathcal{G},\\Theta)+(1-\\kappa)^{+}\\|\\mathbf{B}\\|_{2,1})\\|\\mathbf{Z}\\|_{\\mathrm{RE}}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "596  From Lemma 4, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\left\\lvert\\sum_{m\\in\\mathcal{V}}\\mathbf{z}_{m}^{\\top}\\mathbf{M}_{m}\\mathbf{z}_{m}\\right\\rvert}\\\\ &{\\displaystyle\\leq\\sum_{c\\in\\mathcal{P}}\\left\\lvert\\sum_{m\\in\\mathcal{C}}\\mathbf{z}_{m}^{\\top}\\mathbf{M}_{m}\\mathbf{z}_{m}\\right\\rvert}\\\\ &{\\displaystyle\\leq\\sum_{c\\in\\mathcal{P}}\\left\\lVert\\overline{{\\mathbf{M}_{c}}}\\right\\rVert\\mathbf{\\overline{{\\mathbf{J}}}}_{c}+2\\sum_{c\\in\\mathcal{P}}\\sqrt{\\left\\lVert\\mathbf{M}^{2}c\\right\\rVert}\\mathbf{\\|Z}\\mathbf{\\rVert}_{\\mathbf{Q}_{c}}\\left\\lVert\\mathbf{Z}\\right\\rVert_{\\mathbf{J}_{c}}+\\sum_{c\\in\\mathcal{P}}\\operatorname*{max}_{m\\in\\mathcal{C}}\\left\\lVert\\mathbf{M}_{m}\\right\\rVert\\mathbf{\\left\\lVert}\\mathbf{Z}\\right\\rVert_{\\mathbf{Q}_{c}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "597 where we used Equation (9). ", "page_idx": 21}, {"type": "text", "text": "598 This allows us to bound every term in Equation (38). For the second term on the right-hand side, we   \n599have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{c\\in\\mathcal{P}}\\sqrt{\\big\\|\\overline{{\\mathbf{M}^{2}}}c\\big\\|}\\,\\|\\mathbf{Z}\\|_{\\mathbf{Q}_{c}}\\|\\mathbf{Z}\\|_{\\mathbf{J}_{c}}}\\\\ &{\\leq\\displaystyle\\operatorname*{max}_{c\\in\\mathcal{P}}\\sqrt{\\big\\|\\overline{{\\mathbf{M}^{2}}}c\\big\\|}\\,\\|\\overline{{\\mathbf{Z}}}_{\\mathcal{P}}\\|_{F}\\sqrt{\\displaystyle\\sum_{c\\in\\mathcal{P}}\\|\\mathbf{Z}\\|_{\\mathbf{Q}_{c}}^{2}}}\\\\ &{\\displaystyle\\qquad\\operatorname*{min}_{a_{1}\\in\\mathcal{C}}(\\mathcal{C})^{-\\frac{1}{2}}\\displaystyle\\operatorname*{max}_{c\\in\\mathcal{P}}\\sqrt{\\big\\|\\overline{{\\mathbf{M}^{2}}}c\\big\\|}(a_{2}(\\mathcal{G},\\Theta)+(1-\\kappa)^{+}\\|\\mathbf{B}\\|_{2,1})\\|\\mathbf{Z}\\|_{\\mathrm{RE}}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "600 As for the third term, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{C\\in\\mathcal{P}}\\operatorname*{max}_{m\\in\\mathcal{C}}\\|\\mathbf{M}_{m}\\|\\|\\mathbf{Z}\\|_{\\mathbf{Q}_{c}}^{2}\\leq\\operatorname*{max}_{m\\in\\mathcal{V}}\\|\\mathbf{M}_{m}\\|\\left(\\displaystyle\\sum_{C\\in\\mathcal{P}}\\|\\mathbf{Z}\\|_{\\mathbf{Q}_{c}}\\right)^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\displaystyle\\operatorname*{min}_{m\\in\\mathcal{C}}\\boldsymbol{c}(\\mathcal{C})^{-1}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\displaystyle\\operatorname*{max}_{m\\in\\mathcal{V}}\\|\\mathbf{M}_{m}\\|\\frac{C\\mathcal{C}\\mathcal{P}}{a_{1}(\\mathcal{G},\\Theta)^{2}}(a_{2}(\\mathcal{G},\\Theta)+(1-\\kappa)^{+}\\|\\mathbf{B}\\|_{2,1})^{2}\\|\\mathbf{Z}\\|_{\\mathrm{RE}}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "601 Consequently, denoting $v=\\frac{a_{2}(\\mathcal{G},\\Theta)+(1-\\kappa)^{+}\\|\\mathbf{B}\\|_{2,1}}{a_{1}(\\mathcal{G},\\Theta)},$   \n602 we obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~\\left|\\displaystyle\\sum_{m\\in\\mathcal{V}}\\mathbf{z}_{m}^{\\top}\\mathbf{M}_{m}\\mathbf{z}_{m}\\right|}\\\\ &{~~\\left(\\displaystyle\\operatorname*{max}_{c\\in\\mathcal{P}}\\left\\|\\overline{{\\mathbf{M}}}_{c}\\right\\|+2v\\displaystyle\\operatorname*{max}_{c\\in\\mathcal{P}}\\sqrt{\\left\\|\\overline{{\\mathbf{M}^{2}}}c\\right\\|}+v^{2}\\displaystyle\\operatorname*{max}_{i\\in\\mathcal{V}}\\left\\|\\mathbf{M}_{i}\\right\\|\\right)\\left\\|\\mathbf{Z}\\right\\|_{\\mathrm{RE}}^{2}}\\\\ &{\\leq\\left(\\displaystyle\\operatorname*{max}_{c\\in\\mathcal{P}}\\left\\|\\overline{{\\mathbf{M}}}_{c}\\right\\|\\right)\\vee\\sqrt{\\displaystyle\\operatorname*{min}_{c\\in\\mathcal{P}}c_{\\mathcal{C}}(\\mathcal{C})^{-1}\\displaystyle\\operatorname*{max}_{c\\in\\mathcal{P}}\\left\\|\\overline{{\\mathbf{M}^{2}}}c\\right\\|}\\vee\\displaystyle\\operatorname*{min}_{c\\in\\mathcal{P}}c_{\\mathcal{C}}(\\mathcal{C})^{-1}\\displaystyle\\operatorname*{max}_{i\\in\\mathcal{V}}\\left\\|\\mathbf{M}_{i}\\right\\|\\right)(1+v)^{2}\\left\\|\\mathbf{Z}\\right\\|_{\\mathrm{RE}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "603 which finishes the proof. ", "page_idx": 22}, {"type": "text", "text": "604 Proposition 6 (Inheritance of a RE condition from a close matrix). Assume that the matrix $\\mathbf{V}_{\\mathcal{V}}$   \n605verifies theREconditionwith constant $\\phi~>~0~$ .and that $\\left\\lVert\\frac{\\mathbf{A}_{\\mathcal{V}}}{t}-\\mathbf{V}_{\\mathcal{V}}\\right\\rVert_{\\mathrm{op,RE}}\\,\\le\\,\\gamma\\phi^{2}$ for some   \n606 $\\gamma\\in\\Bigg(0,\\bigg(1+\\frac{a_{2}(\\mathcal{G},\\Theta)+(1-\\kappa)^{+}\\sqrt{2}w(\\partial\\mathcal{P})}{a_{1}(\\mathcal{G},\\Theta)}\\bigg)^{-2}\\Bigg).\\;T h e n\\;\\frac{\\mathbf{A}_{\\mathcal{V}}}{t}\\,{\\nu}e r i f i e s\\;t h e\\;R E\\;c o n d i t i o n\\;w i t h\\;c o n s t a n t$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\hat{\\phi}=\\phi\\sqrt{1-\\gamma\\left(1+\\frac{a_{2}(\\mathcal{G},\\Theta)+(1-\\kappa)^{+}\\sqrt{2}w(\\partial\\mathcal{P})}{a_{1}(\\mathcal{G},\\Theta)}\\right)^{2}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "607 Proof. From Proposition 4, we know that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac1t\\epsilon_{\\nu}^{\\top}\\mathbf{A}_{\\nu\\epsilon\\nu}=\\frac{1}{|\\gamma|}\\epsilon_{\\nu}^{\\top}\\mathbf{V}_{\\nu\\epsilon\\nu}+\\epsilon_{\\nu}^{\\top}\\mathbf{A}_{\\nu\\epsilon\\nu}}\\\\ &{\\quad\\quad\\quad\\quad\\geq\\frac{1}{|\\gamma|}\\epsilon_{\\nu}^{\\top}\\mathbf{V}_{\\nu\\epsilon\\nu}-\\left|\\epsilon_{\\nu}^{\\top}\\mathbf{A}_{\\nu}\\epsilon_{\\nu}\\right|}\\\\ &{\\quad\\quad\\quad\\geq\\left(\\phi^{2}-\\displaystyle\\operatorname*{max}_{m\\in\\gamma}\\left\\|\\Delta_{\\nu}\\right\\|_{\\mathrm{op,\\ensuremath{R^{E}}}}\\left(1+\\displaystyle\\frac{a_{2}(\\mathcal{G},\\mathbf{\\Theta}\\mathbf{e})+(1-\\kappa)^{+}\\left\\|\\mathbf{B}_{\\partial\\mathcal{P}}\\right\\|_{2,1}}{a_{1}(\\mathcal{G},\\mathbf{e})}\\right)^{2}\\right)\\left\\|\\mathbf{E}\\right\\|_{\\mathrm{RE}}^{2}}\\\\ &{\\quad\\quad\\quad\\geq\\left(\\phi^{2}-\\gamma\\phi^{2}\\left(1+\\displaystyle\\frac{a_{2}(\\mathcal{G},\\mathbf{e})+(1-\\kappa)^{+}\\left\\|\\mathbf{B}_{\\partial\\mathcal{P}}\\right\\|_{2,1}}{a_{1}(\\mathcal{G},\\mathbf{e})}\\right)^{2}\\right)\\left\\|\\mathbf{E}\\right\\|_{\\mathrm{RE}}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "609Theorem 5 (Matrix Freedman Inequality, Tropp [2011]). Consider a matrix martingale $\\{\\mathbf{M}(t)\\}_{t\\geq1}$   \n610  with dimension $d_{1}\\times d_{2}$ Let $\\{\\mathbf{N}(t)\\}_{t\\geq1}$ be the associated difference sequence. Assume that for some   \n611 $A>0$ we have $\\|\\mathbf{N}(t)\\|\\leq A\\;\\;\\;\\forall t\\geq\\overline{{1}}$ almost surely.Definefor any $t\\geq1$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\bf W}_{c o l}(t):=\\sum_{\\tau=1}^{t}{\\mathbb E}\\left[{\\bf N}(\\tau){\\bf N}(\\tau)^{\\top}|\\mathcal{F}_{\\tau-1}\\right]}}\\\\ {{\\displaystyle{\\bf W}_{r o w}(t):=\\sum_{\\tau=1}^{t}{\\mathbb E}\\left[{\\bf N}(\\tau)^{\\top}{\\bf N}(\\tau)|\\mathcal{F}_{\\tau-1}\\right].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "612 Then, for any $u,v>0$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\exists t\\geq1;\\lVert\\mathbf{M}(t)\\rVert\\geq u\\,a n d\\,\\lVert\\mathbf{W}_{c o l}\\rVert(t)\\vee\\lVert\\mathbf{W}_{r o w}(t)\\rVert\\leq v\\right]\\leq(d_{1}+d_{2})\\exp\\left(-\\frac{3u^{2}}{6v+2A u}\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "613  Corollary 1. Let $\\{\\mathbf{N}(\\tau)\\}_{\\tau=1}^{t}$ by a sequence of matrices of dimension $d_{1}\\times d_{2}$ adapted tofiltration   \n614 $\\{\\mathcal{F}_{\\tau}\\}_{\\tau=1}^{t}$ Let $\\{t_{i}\\}_{i=1}^{N}$ an increasing sequence with elements in $[t]$ for some $N\\leq t$ . Consider the   \n615   equence $\\{\\mathbf{M}(n)\\}_{\\tau=1}^{N}$ of random matrices defined by ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{M}(n)=\\sum_{i=1}^{n}\\mathbf{N}(t_{i})-\\mathbb{E}\\left[\\mathbf{N}(t_{i})\\vert\\mathcal{F}_{t_{i}-1}\\right]\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "616 Then $\\{\\mathbf{M}(n)\\}_{n=1}^{N}$ is a martingale adapted to the filtration $\\{\\mathcal{F}_{t_{n}}\\}_{n=1}^{N}$ ", "page_idx": 23}, {"type": "text", "text": "617 Moreover,if $\\|\\mathbf{N}(\\tau)\\|\\leq b$ $\\forall\\tau\\in[t]$ for some $b>0$ then we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\Vert\\mathbf{M}(N)\\Vert\\ge u\\right]\\le(d_{1}+d_{2})\\exp\\left(-\\frac{3u^{2}}{6N b^{2}+2\\sqrt{2}b u}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "618 Proof.  We denote $\\mathbb{E}\\left[\\cdot|\\mathcal{F}_{s}\\right]$ as $\\mathbb{E}_{s}\\left[\\cdot\\right]$ for any $s~\\in~\\mathbb{N}$ Also, let $\\mathbf{C}(s)\\;:=\\;\\mathbb{E}_{s-1}\\left[\\mathbf{N}(s)\\right]$ , which is   \n619 $\\mathcal{F}_{s-1}$ -measurable by construction. We have for any $n\\in[N]$ \uff0c ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad~\\mathbb{E}_{t_{n-1}}\\left[\\mathbf{C}(t_{n})\\right]=\\mathbb{E}_{t_{n-1}}\\left[\\mathbb{E}_{t_{n}-1}\\left[\\mathbf{N}(t_{n})\\right]\\right]=\\mathbb{E}_{t_{n-1}}\\left[\\mathbf{N}(t_{n})\\right]}\\\\ &{\\Longrightarrow\\mathbb{E}_{t_{n-1}}\\left[\\mathbf{N}(t_{n})-\\mathbf{C}(t_{n})\\right]=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "620   where the first equality is due to the tower rule since $\\mathcal{F}_{t_{n-1}}\\subset\\mathcal{F}_{t_{n}-1}$ .Also,we have for any $\\tau\\geq1$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{N}(\\tau)-\\mathbf{C}(\\tau)\\|^{2}=\\big\\|(\\mathbf{N}(\\tau)-\\mathbf{C}(\\tau))^{2}\\big\\|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathrm{Tr}\\big((\\mathbf{N}(\\tau)-\\mathbf{C}(\\tau))^{2}\\big)}\\\\ &{\\qquad\\qquad\\qquad=\\mathrm{Tr}\\big((\\mathbf{N}(\\tau)-\\mathbf{C}(\\tau))^{2}\\big)}\\\\ &{\\qquad\\qquad=\\|\\mathbf{N}(\\tau)\\|_{F}^{2}-2\\,\\mathrm{Tr}(\\mathbf{C}(\\tau)\\mathbf{N}(\\tau))+\\mathrm{Tr}\\big(\\mathbf{C}(\\tau)^{2}\\big)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\|\\mathbf{N}(\\tau)\\|_{F}^{2}+\\mathrm{Tr}\\big(\\mathbf{C}(\\tau)^{2}\\big)\\leq2b^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "621 Hence $\\mathbf{N}(\\tau)-\\mathbf{C}(\\tau)$ is integrable for any $\\tau\\geq1$ .This shows that $\\mathbf{M}(n)$ is a sequence of partial sums   \n622 of matrix martingale differences, hence it is a matrix martingale.   \n623  The second part of the corollary statement is a consequence of Theorem 5. The boundedness of   \n624  the sequence of martingale differences has already been established above. To verify the second   \n625 requirement of the theorem, let us compute bounds on the norms of $\\mathbf{W}_{\\mathrm{col}}$ and $\\mathbf{W}_{\\mathrm{row}}$ from Theorem 5.   \n626 Notice that the two matrices are equal since the difference sequence matrices ${\\bf N}(t_{s})$ are symmetric. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "627 Hence, for any $n\\in[N]$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\|\\mathbf{W}_{\\mathrm{coll}}(N)\\|\\vee\\|\\mathbf{W}_{\\mathrm{vow}}(N)\\|\\le\\mathrm{Tr}(\\mathbf{W}_{\\mathrm{coll}}(N))\\vee\\mathrm{Tr}(\\mathbf{W}_{\\mathrm{row}}(N))}}&{\\quad{\\scriptstyle(\\le1)}}\\\\ &{}&{\\quad=\\mathrm{Tr}\\left(\\displaystyle\\sum_{n=1}^{N}\\mathbb{E}_{t_{n-1}}\\left[(\\mathbf{N}(t_{n})-\\mathbf{C}(t_{n}))^{2}\\right]\\right)}\\\\ &{}&{\\quad=\\displaystyle\\sum_{n=1}^{N}\\mathbb{E}_{t_{n-1}}\\left[\\|\\mathbf{N}(t_{n})\\|_{F}^{2}\\right]-\\mathbb{E}_{t_{n-1}}\\left[2\\mathrm{Tr}(\\mathbf{C}(t_{n})\\mathbf{N}(t_{n}))\\right]+\\mathrm{Tr}(\\mathbf{C}(t_{n})^{2}}\\\\ &{}&{\\quad\\stackrel{\\mathrm{()}}{\\longrightarrow}\\displaystyle\\sum_{n=1}^{N}\\mathbb{E}_{t_{n-1}}\\left[\\|\\mathbf{N}(t_{n})\\|_{F}^{2}\\right]-\\mathrm{Tr}\\big(\\mathbf{C}(t_{n})^{2}\\big)}\\\\ &{}&{\\quad\\stackrel{\\mathrm{()}}{\\le}\\displaystyle\\sum_{n=1}^{N}\\mathbb{E}_{t_{n}-1}\\left[\\|\\mathbf{N}(t_{n})\\|_{F}^{2}\\right]-\\mathrm{Tr}\\big(\\mathbf{C}(t_{n})^{2}\\big)}\\\\ &{}&{\\quad\\le\\displaystyle\\sum_{n=1}^{N}\\mathbb{E}_{t_{n-1}}\\left[\\|\\mathbf{N}(t_{n})\\|_{F}^{2}\\right]\\le N b^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "628By Theorem 5, we have for any $u>0$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2d\\exp\\left(-\\frac{3u^{2}}{6N b^{2}+2\\sqrt{2}b u}\\right)\\geq\\mathbb{P}\\left[\\exists n\\geq1;\\|\\mathbf{M}(n)\\|\\geq u\\mathrm{~and~}\\|\\mathbf{W}_{\\mathrm{col}}(n)\\|\\leq N b^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq\\mathbb{P}\\left[\\|\\mathbf{M}(N)\\|\\geq u\\mathrm{~and~}\\|\\mathbf{W}_{\\mathrm{col}}(N)\\|\\leq N b^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathbb{P}\\left[\\|\\mathbf{M}(N)\\|\\geq u\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "629  where the last line holds because we showed that the inequality $\\|\\mathbf{W}_{\\mathrm{col}}(N)\\|\\le N b^{2}$ holds almost   \n630surely. \u53e3   \n631 Proposition 7 (Concentration of the empirical multi-task Gram matrix around the adapted one). Let   \n632 $t\\geq1,\\,b>0$ Thenwehave: ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{s}\\left[\\left\\lVert\\frac{\\mathbf{A}_{\\mathcal{V}}(t)}{t}-\\mathbf{V}_{\\mathcal{V}}\\right\\rVert_{\\mathrm{op,RE}}>\\gamma\\left|\\operatorname*{max}_{m\\in\\mathcal{V}}|T_{m}(t)|\\leq b t\\right]\\leq d(2|\\mathcal{P}|e^{-A_{1}t}+(|\\mathcal{V}|+|\\mathcal{P}|)e^{-A_{2}t}+2|\\mathcal{V}|e^{-A_{3}t}),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "633 where ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{1}:=\\frac{3\\gamma^{2}\\operatorname*{min}_{1}|\\mathcal{C}|t}{66+2\\sqrt{2}\\gamma}}\\\\ &{A_{2}:=\\frac{3\\gamma^{2}\\operatorname*{min}_{1}c_{\\mathcal{C}}(\\mathcal{C})t}{c6\\gamma}}\\\\ &{\\qquad\\qquad6b+2\\sqrt{2}\\gamma\\sqrt{\\frac{\\operatorname*{min}_{1}\\left|\\sigma_{\\mathcal{C}}(\\mathcal{C})\\right.}{\\operatorname*{min}_{1}\\left|\\sigma_{\\mathcal{C}}\\right.\\kern-\\nulldelimiterspace}\\ C\\right|}}\\\\ &{A_{3}:=\\frac{3\\gamma^{2}\\operatorname*{min}_{1}c_{\\mathcal{C}}(\\mathcal{C})^{2}t}{6b+2\\sqrt{2}\\gamma\\operatorname*{min}_{1}c_{\\mathcal{C}}(\\mathcal{C})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "634 Proof. For $\\gamma>0$ , let us define ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\Delta_{m}:=\\frac{\\mathbf{A}\\nu}{t}-\\mathbf{V}\\nu\\quad\\mathrm{~and~}G_{\\mathrm{Gram},\\gamma}:=\\left\\{\\frac{1}{t}\\|\\Delta\\nu\\|_{\\mathrm{RE},S}\\leq\\gamma\\right\\},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "635where $\\Delta_{\\nu}$ is block diagonal matrix formed by $\\{\\Delta_{m}\\}_{m\\in\\mathcal{V}}$ We alsodefine $\\overline{{\\Delta_{c}}}$ and $\\overline{{\\Delta^{2}}}c$ in the same   \n636 pattern of Definition 3. We can express the complementary of this event as the disjunction of a finite ", "page_idx": 24}, {"type": "text", "text": "637 number of events as follows: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad G_{\\mathrm{Gram},\\gamma}^{c}}\\\\ &{=\\displaystyle\\left\\{\\operatorname*{max}_{c\\in\\mathcal{P}}\\|\\overline{{\\Delta}}c\\|\\vee\\sqrt{\\underset{c\\in\\mathcal{P}}{\\operatorname*{min}}c_{\\mathcal{G}}(\\mathcal{C})^{-1}\\underset{c\\in\\mathcal{P}}{\\operatorname*{max}}}\\left\\|\\overline{{\\Delta^{2}}}c\\right\\|\\vee\\underset{c\\in\\mathcal{P}}{\\operatorname*{min}}c_{\\mathcal{G}}(\\mathcal{C})^{-1}\\underset{m\\in\\mathcal{V}}{\\operatorname*{max}}\\|\\Delta_{m}\\|>t\\gamma\\right\\}\\qquad(6)}\\\\ &{=\\displaystyle\\bigcup_{c\\in\\mathcal{P}}\\left\\{\\|\\overline{{\\Delta}}c\\|>t\\gamma\\right\\}\\cup\\bigcup_{c\\in\\mathcal{P}}\\left\\{\\left\\|\\overline{{\\Delta^{2}}}c\\right\\|>t^{2}\\gamma^{2}\\underset{c\\in\\mathcal{P}}{\\operatorname*{min}}c_{\\mathcal{G}}(\\mathcal{C})\\right\\}\\cup\\bigcup_{m\\in\\mathcal{V}}\\left\\{\\|\\Delta_{m}\\|>t\\gamma\\underset{c\\in\\mathcal{P}}{\\operatorname*{min}}c_{\\mathcal{G}}(\\mathcal{C})\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "638 The frst and third event can be bounded by considering the sequence $\\mathbf{x}\\mathbf{x}^{\\top}(\\tau)$ adapted to the filtration   \n639 $\\{\\mathcal{F}_{\\tau}\\}$ , verifying $\\left\\|\\mathbf{x}\\mathbf{x}^{\\top}(\\tau)\\right\\|\\leq$ ", "page_idx": 25}, {"type": "text", "text": "640 Bounding the probability of the first event  Let $\\mathcal{C}\\in\\mathcal{P}$ be a cluster. By definition, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathcal{C}|\\overline{{\\Delta}}_{\\mathcal{C}}(t)=\\displaystyle\\sum_{m\\in\\mathcal{C}}\\displaystyle\\sum_{\\tau\\in\\mathcal{T}_{m}(t)}\\mathbf{xx}(\\tau)-\\mathbb{E}\\left[\\mathbf{xx}(\\tau)|\\mathcal{F}_{\\tau-1}\\right]}\\\\ {=\\displaystyle\\sum_{\\tau\\in\\bigcup_{m\\in\\mathcal{C}}\\mathcal{T}_{m}(t)}\\mathbf{xx}(\\tau)-\\mathbb{E}\\left[\\mathbf{xx}(\\tau)|\\mathcal{F}_{\\tau-1}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "641 We will apply Corollary 1 for the sequence of time indices in $\\mathcal{C}$ i.e. $\\textstyle\\bigcup_{m\\in\\gamma}{\\mathcal{T}}_{m}(t)$ .Hence $|c|\\overline{{\\Delta c}}$ is a   \n642  martingale sequence, and we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left[\\Vert\\overline{{\\Delta}}_{C}(t)\\Vert>\\gamma t|\\underset{m\\in\\mathcal{V}}{\\operatorname*{max}}\\,|T_{m}(t)|\\leq b t\\right]\\leq2d\\exp\\left(\\frac{-3\\gamma^{2}|C|^{2}t^{2}}{6\\sum_{m\\in\\mathcal{C}}|T_{m}(t)|+2\\sqrt{2}\\gamma|C|t|}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq2d\\exp\\left(\\frac{-3\\gamma^{2}|C|^{2}t^{2}}{6|{\\mathcal{C}}|b t+2\\sqrt{2}\\gamma|C|t|}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=2d\\exp\\left(\\frac{-3\\gamma^{2}|C|t}{6b+2\\sqrt{2}\\gamma}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq2d\\exp\\left(\\frac{-3\\gamma^{2}\\underset{c\\in\\mathcal{C}}{\\operatorname*{min}}\\,|C|t|}{6b+2\\sqrt{2}\\gamma}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "643  Bounding the probability of the third event  Let $m\\in\\mathcal{V}$ be a task index. We apply Corollary 1 for   \n644  the sequence of time steps in $\\mathcal{T}_{m}(t)$ .Wehave ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\Delta_{m}(t)=\\sum_{\\tau\\in\\mathcal{T}_{m}(t)}\\mathbf{xx}(\\tau)-\\mathbb{E}\\left[\\mathbf{xx}(\\tau)|\\mathcal{F}_{\\tau-1}\\right]\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "645  is a martingale sequence, hence ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}\\left[\\|\\Delta_{m}(t)\\|>\\gamma\\operatorname*{min}_{\\mathcal{C}\\in\\mathcal{P}}c_{\\mathcal{G}}(\\mathcal{C})t\\big|\\operatorname*{max}_{m\\in\\mathcal{V}}|\\mathcal{T}_{m}(t)|\\leq b t\\right]\\leq2d\\exp\\left(\\frac{-3\\gamma^{2}\\operatorname*{min}_{\\mathcal{C}\\in\\mathcal{P}}c_{\\mathcal{G}}(\\mathcal{C})^{2}t^{2}}{c\\in\\mathcal{P}}\\right)}&{}\\\\ {\\leq2d\\exp\\left(\\frac{-3\\gamma^{2}\\operatorname*{min}_{\\mathcal{C}\\in\\mathcal{P}}c_{\\mathcal{C}}(\\mathcal{C})^{2}t^{2}}{c\\in\\mathcal{P}}\\right)}&{}\\\\ {=2d\\exp\\left(\\frac{-3\\gamma^{2}\\operatorname*{min}_{\\mathcal{C}\\in\\mathcal{P}}c_{\\mathcal{C}}(\\mathcal{C})}{6b+2\\sqrt{2}\\gamma\\operatorname*{min}_{\\mathcal{C}\\in\\mathcal{C}}(\\mathcal{C})t}\\right)}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "646  Bounding the probability of the second event  Let $\\mathcal{C}\\in\\mathcal{P}$ be a cluster, and let us denote $\\mathbf{e}_{m}$ the   \n647 $m^{\\mathrm{th}}$ canonical vector of $\\mathbb{R}^{[c]}$ Wehave ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left|\\overline{{\\Delta^{2}}}c(t)\\right|\\right|=\\frac{1}{|C|}\\left|\\displaystyle\\sum_{m\\in\\mathcal{C}}\\left(\\sum_{\\tau\\in\\mathcal{T}_{m}(t)}\\mathbf{xxx}(\\tau)-\\mathbb{E}\\left[\\mathbf{xx}(\\tau)|\\mathcal{F}_{\\tau-1}\\right]\\right)^{2}\\right|}\\\\ &{\\qquad\\qquad=\\frac{1}{|C|}\\left|\\displaystyle\\sum_{m\\in\\mathcal{C}}\\mathbf{e}_{m}^{\\top}\\otimes\\left(\\sum_{\\tau\\in\\mathcal{T}_{m}(t)}\\mathbf{xxx}(\\tau)-\\mathbb{E}\\left[\\mathbf{xx}(\\tau)|\\mathcal{F}_{\\tau-1}\\right]\\right)\\right|^{2}}\\\\ &{\\qquad\\qquad=\\frac{1}{|C|}\\displaystyle\\left|\\sum_{\\tau\\in\\mathcal{C}}\\sum_{m\\in\\mathcal{C}^{\\prime}}\\mathbf{e}_{m}^{\\top}(\\tau)\\otimes(\\mathbf{xx}(\\tau)-\\mathbb{E}\\left[\\mathbf{xx}(\\tau)|\\mathcal{F}_{\\tau-1}\\right])\\right|^{2}}\\\\ &{\\qquad=\\frac{1}{|C|}\\displaystyle\\left|\\sum_{\\tau\\in\\mathcal{C}}\\sum_{m\\in\\mathcal{C}^{\\prime}}\\mathbf{e}_{m}^{\\top}(\\tau)\\otimes\\mathbf{xx}(\\tau)-\\mathbb{E}\\left[\\mathbf{e}_{m(\\tau)}\\otimes\\mathbf{xx}(\\tau)|\\mathcal{F}_{\\tau-1}\\right]\\right|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "648   where the last equality holds since $m(\\tau)$ is measurable w.r.t. $\\mathcal{F}_{\\tau-1}$ . We will apply the Corollary 1 to   \n649   the set of time steps $\\textstyle\\bigcup_{m\\in\\mathcal{C}}\\mathcal{T}_{m}(t)$ and the adapted sequence $\\mathbf{e}_{m(\\tau)}^{\\top}\\otimes\\mathbf{x}\\mathbf{x}(\\tau)$ ) \u03b2 xx(T) of matrices in Rdxd|Cl.   \n650  Hence we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P\\left[\\sqrt{\\left|\\overline{{\\Delta}}^{\\nu}(z^{\\mu})\\right|}\\;>\\gamma\\frac{\\alpha}{\\xi\\xi\\eta_{0}^{\\alpha}}\\sqrt{\\eta_{0}(\\xi^{\\nu})}\\mathrm{m}^{\\nu}|\\mathcal{F}_{n}(z^{\\nu})|\\right]\\leq\\kappa\\Bigg]}\\\\ &{\\leq q(1+\\xi)\\cot\\left(\\frac{-3\\,^{\\eta}\\sqrt{\\eta_{0}\\xi}\\mathrm{m}^{\\nu}\\varphi^{\\alpha}}{\\phi\\xi\\eta_{0}(1+\\xi)}\\sqrt{\\eta_{0}\\xi^{\\alpha}\\mathrm{m}^{\\nu}\\varphi^{\\alpha}}(\\xi^{\\nu})\\right)}\\\\ &{\\leq q(1+\\xi)\\cot\\left(\\frac{-3\\,^{\\eta}\\sqrt{\\eta_{0}\\xi}\\mathrm{m}^{\\nu}\\varphi^{\\alpha}}{\\phi\\xi\\left(\\xi^{\\nu}+2\\sqrt{\\eta_{0}\\xi}\\right)\\sqrt{\\xi^{\\eta}\\mathrm{m}^{\\eta}\\xi\\mathrm{m}^{\\eta}\\left(\\xi^{\\nu}\\right)}}\\right)}\\\\ &{=q(1+\\xi)\\cot\\left(\\frac{-3\\,^{\\eta}\\eta_{0}\\mathrm{m}\\phi(\\xi)}{\\phi\\xi^{\\alpha}}\\right)}\\\\ &{\\leq q(1+\\xi)\\cot\\left(\\frac{-3\\,^{\\eta}\\eta_{0}\\mathrm{m}\\phi(\\xi)}{\\phi\\xi^{\\alpha}}\\right)}\\\\ &{\\leq q(1+\\xi)\\cot\\left(\\frac{-3\\,^{\\eta}\\eta_{0}\\mathrm{m}\\phi(\\xi)}{\\phi\\xi^{\\alpha}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "651 Union bound  We conclude the result of the statement via a union bound using Equation (61). ", "page_idx": 26}, {"type": "text", "text": "652 Proposition 8 (Concentration of the empirical multi-task Gram matrix around the adapted one,   \n653simplified).propEmpCovConcentrationSimplified Let $t\\geq1,\\,b>0.$ Assumethat $\\mathrm{max}_{m\\in\\mathcal{V}}\\left|\\bar{\\mathcal{T}}_{m}(t)\\right|\\leq$   \n654bt.Thenwe have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left\\lVert\\frac{\\mathbf{A}_{\\mathcal{V}}}{t}-\\mathbf{V}_{\\mathcal{V}}\\right\\rVert_{\\mathrm{op,RE}}>\\gamma\\right]\\le6d|\\mathcal{V}|\\exp\\bigg(\\frac{-3\\gamma^{2}(\\operatorname*{min}_{\\mathcal{C}\\in\\mathcal{P}}(\\tilde{c}_{\\mathcal{G}}(\\mathcal{C})\\wedge\\tilde{c}_{\\mathcal{G}}(\\mathcal{C})^{2})t}{6b+2\\sqrt{2}\\gamma}\\bigg),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "655  where $\\tilde{c}_{\\mathcal{G}}(\\mathcal{C}):=c_{\\mathcal{G}}(\\mathcal{C})\\wedge|\\mathcal{C}|$ $\\forall{\\mathcal{C}}\\in{\\mathcal{P}}$ ", "page_idx": 26}, {"type": "text", "text": "656 Proof. The proof will rely on simple calculus inequalities. Hence, let $\\begin{array}{r}{u\\,=\\,\\operatorname*{min}_{\\substack{c\\in{\\mathcal P}\\,c_{\\mathcal G}({\\mathcal C}),\\,v\\,=}}}\\end{array}$   \n657 $\\begin{array}{r}{\\operatorname*{min}_{\\mathcal{C}\\in\\mathcal{P}}|\\mathcal{C}|,f=3\\gamma^{2},g=6b,h=2\\sqrt{2}\\gamma}\\end{array}$ , which are all positive. Then, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{A_{1}=\\displaystyle\\frac{f u}{f+g}\\geq\\frac{(u\\wedge v)f}{f+g}\\geq(u\\wedge v)\\frac{(1\\wedge u\\wedge v)f}{f+g(1\\wedge u\\wedge v)}}}\\\\ {{A_{2}=\\displaystyle\\frac{f v}{f+g\\frac{v}{u}}\\geq\\frac{(v\\wedge u)f}{f+g\\frac{v\\wedge u}{u}}\\geq\\frac{(v\\wedge u)f}{f+g}\\geq(u\\wedge v)\\frac{(1\\wedge u\\wedge v)f}{f+(1\\wedge u\\wedge v)g}}}\\\\ {{A_{3}=\\displaystyle\\frac{f v^{2}}{f+g v}\\geq\\frac{(v\\wedge u)^{2}}{f+(v\\wedge u)g}\\geq(u\\wedge v)\\frac{(1\\wedge u\\wedge v)f}{f+(1\\wedge u\\wedge v)g}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "65   where we used the fact that functions of the form $\\begin{array}{r}{x\\mapsto\\frac{x}{\\beta_{1}x+\\beta_{2}}}\\end{array}$ for positive $\\beta_{1},\\beta_{2}$ are increasing on   \n659 $\\mathbb{R}_{+}$ ", "page_idx": 27}, {"type": "text", "text": "As asweutq ${\\frac{(1\\wedge x)f}{f+(1\\wedge x)g}}\\geq{\\frac{x\\wedge1}{f+g}}$ taken for $x=u\\wedge v$ we applythe 661 $\\exp(-\\cdot\\,t)$ function and we use the result of Proposition 7, we deduce the result. \u53e3 ", "page_idx": 27}, {"type": "text", "text": "662 B.3.3From the true to the adapted Gram matrix ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "663For all of the proofs in this subsection, we follow an approach similar to that of Oh et al. [2021]. In   \n664  particular, we use their Lemma 10.   \n665Theorem 6 (Lemma 10 of Oh et al. [2021]). Under Assumption 2 on the context generating distribu  \n666tion,let $t\\geq1$ Wehaveforany $\\pmb{\\theta}\\in\\mathbb{R}^{d}$ ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left.\\sum_{\\mathbf{x}\\in\\mathcal{A}(t)}\\mathbb{E}\\left[\\mathbf{x}\\mathbf{x}^{\\top}\\mathbb{1}\\left\\{\\mathbf{x}\\in\\arg\\operatorname*{max}_{\\mathbf{\\tilde{x}}\\in\\mathcal{A}(t)}\\left\\langle\\pmb{\\theta},\\tilde{\\mathbf{x}}\\right\\rangle\\right\\}\\right]\\succcurlyeq\\frac{1}{2\\nu\\omega}\\overline{{\\Sigma}}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "667 Proposition 9 (RE condition from the true to the adapted Gram matrix). Under Assumption 2, for   \n668any $t\\geq1$ , the adapted Gram matrix $\\mathbf{V}_{\\mathcal{V}}(t)$ verifies thecompatibility conditionwith constants $\\kappa$ and   \n669 $\\frac{\\phi}{\\sqrt{2\\nu\\omega}}.$ ", "page_idx": 27}, {"type": "text", "text": "670 Proof. For $t\\geq1$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\mathbf{x}(t)\\mathbf{x}(t)^{\\top}|\\mathcal{F}_{t-1}\\right]=\\mathbb{E}\\left[\\sum_{\\mathbf{x}\\in\\mathcal{A}(t)}\\mathbf{x}(t)\\mathbf{x}(t)^{\\top}|\\mathcal{F}_{t-1}\\right]\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "671 Let $m\\in\\mathcal{V}$ .Wehave ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbf{V}_{m}(t)=\\frac{1}{t}\\sum_{\\tau\\in\\mathcal{T}_{m}(t)}\\mathbb{E}\\left[\\mathbf{x}(\\tau)\\mathbf{x}(\\tau)^{\\top}|\\mathcal{F}_{\\tau-1}\\right]}}\\\\ &{=\\frac{1}{t}\\sum_{\\tau\\in\\mathcal{T}_{m}(t)}\\mathbb{E}\\left[\\mathbb{E}\\left[\\mathbf{x}(\\tau)\\mathbf{x}(\\tau)^{\\top}|\\theta_{m}(\\tau-1),\\mathcal{F}_{\\tau-1}\\right]|\\mathcal{F}_{\\tau-1}\\right]\\quad\\mathrm{(Iaw~of~total~epectation)}}\\\\ &{=\\frac{1}{t}\\sum_{\\tau\\in\\mathcal{T}_{m}(t)}\\mathbb{E}\\left[\\mathbf{x}(\\tau)\\mathbf{x}(\\tau)^{\\top}|\\theta_{m}(\\tau-1)\\right]\\quad\\mathrm{(}\\mathbf{x}(\\tau)\\mathrm{~is~fully~determincol~by~}\\theta_{m}(\\tau-1)\\mathrm{)}}\\\\ &{=\\frac{1}{t}\\sum_{\\tau\\in\\mathcal{T}_{m}(t)}\\mathbb{E}\\left[\\sum_{\\mathbf{x}\\in\\mathcal{A}(\\tau)}\\mathbf{x}^{\\top}\\mathbf{l}\\left\\{\\mathbf{x}\\in\\arg\\operatorname*{max}\\left\\{\\theta,\\bar{\\mathbf{x}}\\right\\}\\right\\}|\\theta_{m}(\\tau-1)\\right]}\\\\ &{\\approx\\frac{1}{2t}\\sum_{\\tau\\in\\mathcal{T}_{m}(t)}\\quad\\mathrm{(by~Teorem~6)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "672Now, let $\\mathbf{Z}\\in S$ , where $\\boldsymbol{S}$ is defined with constant $\\kappa$ of Assumption 4. Then ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{m\\in\\mathcal{V}}\\left\\|\\mathbf{z}\\right\\|_{\\mathbf{V}_{m}(t)}\\geq\\displaystyle\\frac{1}{2\\nu\\omega}\\displaystyle\\sum_{m\\in\\mathcal{V}}\\left\\|\\mathbf{z}_{m}\\right\\|_{\\overline{{\\Sigma}}}}&{\\mathrm{by~Equation~}(67)}\\\\ {\\displaystyle\\geq\\frac{\\phi^{2}}{2\\nu\\omega}\\|\\mathbf{Z}\\|_{\\mathrm{RE}}^{2}}&{(\\mathrm{by~Assumption~}4),}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "673 which finishes the proof. ", "page_idx": 27}, {"type": "text", "text": "674  Theorem 2 (RE condition holding for the empirical multi-task Gram matrix). Under assumptions 2   \n675 and 4, let $t\\geq1$ , and let $\\kappa,\\phi$ betheconstantsfromAssumption $^{4}$ Assume that $\\mathrm{max}_{m\\in\\mathcal{V}}\\left|\\mathcal{T}_{m}(t)\\right|\\leq b t$   \n676   Then, for any $\\begin{array}{r}{\\gamma\\in\\bigg(0,\\Big(1+\\frac{a_{2}(\\mathcal{G},\\Theta)+(1-\\kappa)^{+}\\sqrt{2}w(\\partial\\mathcal{P})}{a_{1}(\\mathcal{G},\\Theta)}\\Big)^{-2}\\bigg)}\\end{array}$ ,the empirical multi-taskGrammatrix   \n677 verifies the RE condition with constants $\\kappa$ and $\\hat{\\phi}$ with ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\phi}=\\tilde{\\phi}\\sqrt{1-\\gamma\\left(1+\\frac{a_{2}(\\mathcal{G},\\Theta)+(1-\\kappa)^{+}\\sqrt{2}w(\\partial\\mathcal{P})}{a_{1}(\\mathcal{G},\\Theta)}\\right)^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{w i t h\\;a\\;p r o b a b i l i t y\\;a t\\;l e a s t\\;e q u a l\\;t o\\;1-6d|\\mathcal{V}|\\exp\\left(\\frac{-\\;3\\gamma^{2}\\tilde{\\phi}^{4}\\left(\\operatorname*{min}_{C\\in\\mathcal{P}}\\left(\\tilde{c}_{\\mathcal{G}}(\\mathcal{C})\\wedge\\tilde{c}_{\\mathcal{G}}(\\mathcal{C})^{2}\\right)t\\right)}{6b+2\\sqrt{2}\\gamma\\tilde{\\phi}^{2}}\\right),}\\\\ {\\tilde{\\phi}:=\\frac{\\phi}{\\sqrt{2\\nu\\omega}}\\,a n d\\;\\tilde{c}_{\\mathcal{G}}(\\mathcal{C}):=c_{\\mathcal{G}}(\\mathcal{C})\\wedge|\\mathcal{C}|\\quad\\forall\\mathcal{C}\\in\\mathcal{P}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "680 Proof. For the sake of readability, let $\\begin{array}{r}{\\tilde{\\phi}=\\frac{\\phi}{\\sqrt{2\\nu\\omega}}}\\end{array}$ the compatibility constant of the adapted Gram   \n681 matrix, according to Proposition 9. Then: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~1-6d|\\mathcal{V}|\\exp\\left(\\frac{-3\\gamma^{2}\\tilde{\\phi}^{4}\\left(\\operatorname*{min}_{\\mathcal{C}\\in\\mathcal{P}}\\left(\\tilde{c}_{\\mathcal{G}}(\\mathcal{C})\\wedge\\tilde{c}_{\\mathcal{G}}(\\mathcal{C})^{2}\\right)t\\right)}{6b+2\\sqrt{2}\\gamma\\tilde{\\phi}^{2}}\\right)}\\\\ &{\\leq\\!\\mathbb{P}\\left[\\left\\|\\frac{\\mathbf{A}_{\\mathcal{V}}}{t}-\\mathbf{V}_{\\mathcal{V}}\\right\\|_{\\mathrm{op,RE}}\\leq\\gamma\\tilde{\\phi}^{2}\\right]\\quad\\mathrm{(by~Proposition~8)}}\\\\ &{\\leq\\!\\mathbb{P}\\left[\\frac{\\mathbf{A}_{\\mathcal{V}}}{t}\\operatorname{satisfies~the~RE~condition~with~constant~\\hat{\\phi}~}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "683 B.4  Regret bound ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "684 Lemma 5 (Concentration of the fraction of observations per task). lemma Assume that $|\\mathcal{V}|\\ge2$ Then   \n685for $\\delta\\in(0,1)$ ,wehavewith aprobability at least $1-\\delta$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{m\\in\\mathcal{V}}\\frac{|\\mathcal{T}_{m}(t)|}{t}\\leq\\frac{1}{|\\mathcal{V}|}+2\\sqrt{\\frac{1}{t|\\mathcal{V}|}\\log\\frac{|\\mathcal{V}|}{\\delta}}+\\frac{4}{3t}\\log\\frac{|\\mathcal{V}|}{\\delta}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. We have $\\begin{array}{r}{|\\mathcal{T}_{m}(t)|:=\\sum_{\\tau=1}^{t}[m(\\tau)=m]}\\end{array}$ where $\\begin{array}{r}{\\forall t,\\forall m\\in\\mathcal{V},\\mathbb{P}\\left[m(t)=m\\right]=\\frac{1}{|\\mathcal{V}|}}\\end{array}$ , meaning that the binary variable $[m(t)=m]$ follows a Bernoulli distribution $\\textstyle B({\\frac{1}{\\nu}})$ . Then, the random variable $\\begin{array}{r}{X_{t}:=\\left[m(t)\\,=\\,m\\right]-\\frac{1}{|\\mathcal{V}|}}\\end{array}$ has mean O, variance $\\begin{array}{r}{\\frac{1}{|\\mathcal{V}|}(1-\\frac{1}{|\\mathcal{V}|})}\\end{array}$ , and verifies $\\begin{array}{r}{|X_{t}|\\,\\le\\,1\\,-\\,\\frac{1}{|\\mathcal{V}|}}\\end{array}$ since $|\\nu|\\geq2$ . As a result, via the Bernstein inequality, we have for any $m\\in\\mathcal{V}$ , and for any $w\\geq0$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb P\\left[\\frac{|\\mathcal T_{m}(t)|}t\\geq\\frac1{|\\mathcal V|}+w\\right]\\le\\exp\\left(-\\frac{t w^{2}}{2(1-\\frac1{|\\mathcal V|})(\\frac1{|\\mathcal V|}+\\frac w3)}\\right)\\le\\exp\\left(-\\frac{t w^{2}}{2(\\frac1{|\\mathcal V|}+\\frac w3)}\\right)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "686 For the right-hand side to hold with a probability at most $\\delta\\in(0,1)$ , it is sufficient to have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad t\\frac{w^{2}}{2\\left(\\frac{1}{|V|}+\\frac{w}{3}\\right)}\\geq\\log\\frac{1}{\\delta}}\\\\ &{\\Longleftarrow\\frac{w^{2}}{2}\\geq\\frac{2\\frac{1}{|V|}\\log\\frac{1}{\\delta}}{t}\\mathrm{~and~}\\frac{w^{2}}{2}\\geq\\frac{2w\\log\\frac{1}{\\delta}}{3t}}\\\\ &{\\Longleftarrow w=2\\sqrt{\\frac{\\frac{1}{|V|}\\log\\frac{1}{\\delta}}{t}}+\\frac{4\\log\\frac{1}{\\delta}}{3t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "687 Hence, and via a union bound, we get ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left[\\frac{|{\\mathcal{T}}_{m}(t)|}{t}\\geq\\frac{1}{|{\\mathcal{V}}|}+2\\sqrt{\\frac{1}{|{\\mathcal{V}}|}\\log\\frac{1}{\\delta}}+\\frac{4}{3t}\\log\\frac{1}{\\delta}\\right]\\leq\\delta}\\\\ &{\\Longrightarrow\\mathbb{P}\\left[\\underset{m\\in\\mathcal{V}}{\\operatorname*{max}}\\frac{|{\\mathcal{T}}_{m}(t)|}{t}\\geq\\frac{1}{|{\\mathcal{V}}|}+2\\sqrt{\\frac{\\frac{1}{|\\mathcal{V}|}\\log\\frac{1}{\\delta}}{t}}+\\frac{4\\log\\frac{1}{\\delta}}{3t}\\right]\\leq|\\mathcal{V}|\\delta}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "688 The result is obtained by adjusting the value of $\\delta$ ", "page_idx": 29}, {"type": "text", "text": "6es Theorem 3 (Regret bound). Let the mean horizon per node be $\\begin{array}{r}{\\overline{{T}}\\ =\\ \\frac{T}{|\\mathcal{V}|}}\\end{array}$ $L e t\\,\\operatorname*{min}_{\\mathcal{C}\\in\\mathcal{P}}\\sqrt{c_{\\mathcal{G}}(\\mathcal{C})}$   \n690 going asymptotically to infinity and $\\operatorname*{max}_{\\mathcal{C}\\in\\mathcal{P}}\\sqrt{\\iota_{\\mathcal{G}}(\\mathcal{C})}$ going asymptotically to zero as well as   \n691  maxceP \u221ag(C)w(P) and mi w(8P) $\\frac{w(\\partial{\\mathcal P})}{\\operatorname*{min}_{{\\mathcal C}\\in{\\mathcal P}}\\sqrt{c_{\\mathcal G}({\\mathcal C})}}$ going asymptotically to zero. Under assumptions1 to $^{4}$   \n692 and $\\kappa<1$ the expected regret of the Network Lasso Bandit algorithm is upper bounded as follows:   \n$\\mathcal{R}(|\\nu|\\overline{{T}})=\\mathcal{O}\\left(\\sqrt{\\frac{\\overline{{T}}}{\\operatorname*{min}_{c\\in\\mathcal{P}}c_{\\mathcal{O}}(\\mathcal{C})}}\\left(\\sqrt{|\\mathcal{V}|}+\\sqrt{\\log(\\overline{{T}}|\\mathcal{V}|)}+\\sqrt[4]{|\\mathcal{V}\\log(\\overline{{T}}|\\mathcal{V}|)|}\\right)+\\frac{1}{A}\\log(d|\\mathcal{V}|)\\right),$   \n693with A = $A=\\frac{3\\gamma^{2}\\operatorname*{min}_{\\mathcal{C}\\in\\mathcal{P}}(\\tilde{c}_{\\mathcal{G}}(\\mathcal{C})\\wedge\\tilde{c}_{\\mathcal{G}}^{2}(\\mathcal{C}))}{6\\frac{\\log(|\\mathcal{V}|)}{\\sqrt{|\\mathcal{V}|}}+2\\sqrt{2}\\gamma}.$   \n694 Proof. For any time step $t$ , we will define a list of good events under which the Oracle inequality and   \n695 the RE condition for the empirical multi-task Gram matrix both hold with high probability. Then, we   \n696  will use those bounds to sum up over time steps until horizon $T$   \n697 Good events We formalize these requirements as three families of time-depending \"good\" events.   \n698 $G_{\\mathrm{pro}}(t)$ is the event that the mean of the empirical process bounded by $\\alpha(t)$ up to a constant $c$   \n699 which is equivalent to saying that it converges: ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "equation", "text": "$$\nG_{\\mathrm{pro}}(t):=\\left\\{\\frac{1}{t}\\|\\mathbf{K}\\|_{F}\\leq\\frac{\\alpha(t)}{\\alpha_{0}}\\right\\}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "700\u3002 $G_{\\mathrm{sel}}(t)$ is the event that the number of selections of all tasks is bounded by its expected value up   \n701 to a small constant $\\rho(t)$ ", "page_idx": 29}, {"type": "equation", "text": "$$\nG_{\\mathrm{sel}}(t):=\\left\\{\\operatorname*{max}_{m\\in\\mathcal{V}}\\frac{|\\mathcal{T}_{m}(t)|}{t}\\leq\\frac{1}{|\\mathcal{V}|}+\\frac{\\rho(t)}{t}\\right\\}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "702\u00b7 $G_{\\mathrm{RE}}(t)$ is the event that the empirical multi-task Gram matrix $\\begin{array}{r}{{\\frac{1}{t}}\\mathbf{A}_{\\mathcal{V}}(t)}\\end{array}$ satisfies the RE condition. ", "page_idx": 29}, {"type": "equation", "text": "$$\nG_{\\mathrm{RE}}(t):=\\left\\{{\\frac{1}{t}}\\mathbf{A}_{\\mathcal{V}}(t){\\mathrm{~verifies~the~RE~condition~with~constants~}}\\kappa,{\\hat{\\phi}}\\right\\}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "703Event $G_{\\mathrm{pro}}(t)$ is the most straightforward to cover since our bound on the empirical process given in   \n704  Lemma 3 holds with a probability of at least $1-\\delta(t)$ ,thus: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left[G_{\\mathrm{pro}}(t)^{c}|G_{\\mathrm{sel}}(t)\\right]\\leq\\delta(t),}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "705 where we included the time dependency on $\\delta(t)$ in contrast to the previous section. This way we   \n706 emphasize to adjust $\\delta(t)$ after each round, to guarantee a sub linear regret bound. The probability of   \n707event $G_{\\mathrm{sel}}(t)$ can be determined using Bernstein's inequality: ", "page_idx": 29}, {"type": "text", "text": "708gFrom Lema 5 we can selet $\\begin{array}{r}{\\rho(t)=2\\sqrt{\\frac{t}{|\\mathcal{V}|}\\log\\frac{|\\mathcal{V}|}{\\delta_{\\mathrm{sel}}(t)}}+\\frac{4}{3}\\log\\frac{|\\mathcal{V}|}{\\delta_{\\mathrm{sel}}(t)}}\\end{array}$ as welasls $\\mathbb{P}\\left[G_{\\mathrm{sel}}(t)^{c}\\right]\\leq\\delta_{\\mathrm{sel}}(t)$ ", "page_idx": 29}, {"type": "text", "text": "709 B.4.1  Instantaneous regret decomposition ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "710 Now, given the event probabilities, we condition the instantaneous regret ${\\boldsymbol{r}}(t)$ on the good events at a   \n711time $t>t_{0}$ . We have for its expectation: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[r(t)\\right]\\leq\\mathbb{E}\\left[r(t)|G_{\\mathrm{sel}}(t)\\right]+2\\mathbb{P}\\left[G_{\\mathrm{sel}}(t)^{c}\\right]}\\\\ &{\\qquad\\qquad\\leq\\mathbb{E}\\left[r(t)|G_{\\mathrm{pro}}(t)\\cap G_{\\mathrm{RE}}(t)\\cap G_{\\mathrm{sel}}(t)\\right]}\\\\ &{\\qquad\\qquad+\\left.2\\left(\\mathbb{P}\\left[G_{\\mathrm{pro}}(t)^{c}|G_{\\mathrm{sel}}(t)\\right]+\\mathbb{P}\\left[G_{\\mathrm{RE}}(t)^{c}|G_{\\mathrm{sel}}(t)\\right]+\\mathbb{P}\\left[G_{\\mathrm{sel}}(t)^{c}\\right]\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "712  where we used the worst case bound $r(t)\\leq2$ if any one of the good events does not hold. ", "page_idx": 30}, {"type": "text", "text": "713 Bounding the regret  Inserting our results of the event probabilities, the oracle inequality and the   \n714 decomposition of the expected instantaneous regret in Equation (76) and bounding the sum over   \n715  rounds, yields the final result. Thus, we start by bounding the sum over the first term i.e. the expected   \n716 regret in case all good events hold: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}\\left[r(t)|G_{\\mathrm{pro}}(t)\\cap G_{\\mathrm{RE}}(t)\\cap G_{\\mathrm{sel}}(t)\\right]\\leq\\sum_{t=1}^{T}\\Big\\|\\Theta-\\hat{\\Theta}(t)\\Big\\|_{F}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "717 Taking the result of our oracle inequality in Theorem 1, we point out that only $\\alpha(t)$ is time dependent   \n718 such that the rest of the terms can be pulled outside the sum: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{T}{c_{1}}\\Big\\lVert\\Theta-\\dot{\\Theta}(t)\\Big\\rVert_{F}\\le\\sum_{i=1}^{T}\\frac{2}{\\bar{\\mathcal{G}}^{2}\\sqrt{c_{i}}}f(\\mathcal{G},\\Theta)\\sqrt{1+2b}\\sqrt{|\\mathcal{V}|\\log\\frac{1}{\\delta(t)}}+2b\\log\\frac{1}{\\delta(t)}}\\\\ &{\\quad=\\frac{2\\sigma}{\\bar{\\mathcal{G}}^{2}}f(\\Theta,\\Theta)\\sum_{i=1}^{T}\\sqrt{\\frac{1}{t}+\\frac{2b}{\\mathcal{N}}\\sqrt{2|\\mathcal{V}|\\log(t)}+\\frac{4b}{\\mathcal{G}}\\log(t)}}\\\\ &{\\quad\\le\\frac{2\\sigma}{\\bar{\\mathcal{G}}^{2}}f(\\Theta,\\Theta)\\int_{0}^{T}\\frac{1}{\\sqrt{t}}+\\sqrt{\\frac{\\mathcal{D}}{t}\\left(\\sqrt{2|\\mathcal{V}|\\log(T)}+2\\log(T)\\right)}\\,d t}\\\\ &{\\quad\\le\\frac{2\\sigma}{\\bar{\\mathcal{G}}^{2}}f(\\Theta,\\Theta)\\left(2\\sqrt{T}+\\left(\\frac{\\sqrt{\\mathcal{N}T}}{|\\mathcal{V}|}+4\\sqrt{\\frac{32\\log(|\\mathcal{V}|T)T}{|\\mathcal{V}|}}+\\sqrt{\\frac{16}{3}\\log(|\\mathcal{V}|T)}\\log(T)\\right)}\\\\ &{\\quad\\Big(\\sqrt{2|\\mathcal{V}|\\log(T)}+\\sqrt{2\\log(T)}\\Big)}\\\\ &{\\quad=\\mathcal{O}\\left(\\frac{f(\\mathcal{G},\\Theta)\\sqrt{T}}{\\bar{\\mathcal{G}}^{2}}\\left(\\sqrt{|\\mathcal{V}|}+\\sqrt{\\log(\\bar{T}|\\mathcal{V}|)}+\\sqrt{|\\mathcal{V}\\log(\\bar{T}|\\mathcal{V}|)}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "719 where ", "page_idx": 30}, {"type": "equation", "text": "$$\nf(\\mathcal{G},\\Theta):=\\left(a_{2}(\\mathcal{G},\\Theta)+\\sqrt{2}\\mathbb{1}_{\\leq1}(\\kappa)w(\\partial\\mathcal{P})\\right)\\left(\\frac{a_{2}(\\mathcal{G},\\Theta)+\\sqrt{2}\\mathbb{1}_{\\leq1}(\\kappa)w(\\partial\\mathcal{P})}{a_{1}(\\mathcal{G},\\Theta)\\displaystyle\\operatorname*{min}_{c\\in\\mathcal{P}}\\sqrt{c_{\\mathcal{G}}(\\mathcal{C})}}+1\\right).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "720 We upper bounded the sum with an integrali.e. $\\begin{array}{r}{\\sum_{t=1}^{T}f(t)\\le\\int_{0}^{T}f(t)d t}\\end{array}$ for monotonically decreasing   \n721functions $f(t)$ in the last inequality. Also $b$ is the bound on the concentration of the fraction of   \n722 observation per task provided by Lemma 5. For $t_{0}=\\sqrt{|\\nu|}$ we find by inserting the result to Lemma 5   \n723 for all $t>t_{0}$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{|\\mathcal{V}|}+2\\sqrt{\\frac{1}{t|\\mathcal{V}|}\\log\\frac{|\\mathcal{V}|}{\\delta}}+\\frac{4}{3t}\\log\\frac{|\\mathcal{V}|}{\\delta}\\leq\\frac{1}{|\\mathcal{V}|}+2\\sqrt{\\frac{2\\log\\left(|\\mathcal{V}|\\sqrt{|\\mathcal{V}|}\\right)}{\\sqrt{|\\mathcal{V}|}|\\mathcal{V}|}}+\\frac{8\\log\\left(|\\mathcal{V}|\\sqrt{|\\mathcal{V}|}\\right)}{3\\sqrt{|\\mathcal{V}|}}}\\\\ &{\\displaystyle=\\frac{1}{|\\mathcal{V}|}+\\frac{2}{\\sqrt{|\\mathcal{V}|}}\\left[\\sqrt{\\frac{3}{\\sqrt{|\\mathcal{V}|}}\\log(|\\mathcal{V}|)}+2\\log(|\\mathcal{V}|)\\right]}\\\\ &{\\displaystyle=\\mathcal{O}\\left(\\frac{\\log(|\\mathcal{V}|)}{\\sqrt{|\\mathcal{V}|}}\\right)=b.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "724 Finally we bound the sum over the instantaneous regret term for the bad events: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}2\\left(\\mathbb{P}\\left[G_{\\mathrm{pro}}(t)^{c}|G_{\\mathrm{sel}}(t)\\right]+\\mathbb{P}\\left[G_{\\mathrm{RE}}(t)^{c}|G_{\\mathrm{sel}}(t)\\right]+\\mathbb{P}\\left[G_{\\mathrm{sel}}(t)^{c}\\right]\\right)\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "725   By construction, we have $\\begin{array}{r}{\\operatorname*{max}(\\mathbb{P}\\left[G_{\\mathrm{pro}}(t)^{c}|G_{\\mathrm{sel}}(t)\\right],\\mathbb{P}\\left[G_{\\mathrm{sel}}(t)^{c}\\right])\\leq\\delta(t)=\\frac{1}{t^{2}}.}\\end{array}$ Hence, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{P}\\left[G_{\\mathrm{pro}}(t)^{c}|G_{\\mathrm{sel}}(t)\\right]+\\mathbb{P}\\left[G_{\\mathrm{sel}}(t)^{c}\\right]\\leq2\\sum_{t=1}^{T}\\frac{1}{t^{2}}\\leq2\\left(1+\\int_{1}^{T}\\frac{d t}{t^{2}}\\right)\\leq4\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "726 As for the RE condition event, letting $A:=\\frac{3\\gamma^{2}\\operatorname*{min}_{\\mathcal{C}\\in\\mathcal{P}}(\\tilde{c}_{\\mathcal{G}}(\\mathcal{C})\\wedge\\tilde{c}_{\\mathcal{G}}^{2}(\\mathcal{C}))}{6b+2\\sqrt{2}\\gamma};$ we have for any $t_{0}\\geq1$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{t=t_{0}}^{T}\\mathbb{P}\\left[G_{\\mathrm{RE}}(t)^{c}|G_{\\mathrm{sel}}(t)\\right]\\,\\le\\,6d|\\mathcal{V}|\\displaystyle\\sum_{t=t_{0}}^{T}\\exp(-A t)\\quad(\\mathrm{by~Theorem~2})}\\\\ {\\displaystyle\\le\\,6d|\\mathcal{V}|\\displaystyle\\frac{e^{-A t_{0}}}{1-e^{-A}}\\le6d|\\mathcal{V}|e^{-A t_{0}}\\left(1+\\displaystyle\\frac{1}{A}\\right)}\\\\ {\\displaystyle\\le\\,6d|\\mathcal{V}|e^{-A t_{0}}\\left(1+\\displaystyle\\frac{1}{A}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "727 where in the last line, we used the inequality $\\exp(A)\\geq A+1$ .Hence, for any $u>0$ choosing ", "page_idx": 31}, {"type": "equation", "text": "$$\nt_{0}=\\left\\lceil\\sqrt{|\\mathcal{V}|}\\right\\rceil\\vee\\left\\lceil\\frac{1}{A}\\log\\left(\\frac{6d|\\mathcal{V}|(1+\\frac{1}{A})}{u}\\right)\\right\\rceil\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "72  impliesthat $\\begin{array}{r}{\\sum_{t=t_{0}}^{T}\\mathbb{P}\\left[G_{\\mathrm{RE}}(t)^{c}|G_{\\mathrm{sel}}(t)\\right]\\leq u.}\\end{array}$   \n729  fnd an approprate bound on $\\frac{f(\\mathcal{G},\\Theta)}{\\hat{\\phi}^{2}}$ Giveourrst Thrmlndauing $\\kappa>1$ we   \n730  get: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{\\alpha}=\\frac{\\alpha_{0}\\alpha_{2}\\left(Q,\\Theta\\right)}{\\delta^{2}}\\left(\\frac{\\alpha_{2}\\left(Q,\\Theta\\right)}{\\alpha_{1}\\left(U,\\Theta\\right)\\sin\\frac{\\Theta\\left(C\\Theta\\right)}{c\\Theta}}+1\\right)}\\\\ &{=\\frac{\\left(\\sqrt{2\\alpha\\kappa\\left(\\partial P\\right)}\\operatorname*{max}_{C\\in\\mathcal{C}}\\sqrt{\\mathrm{v}_{\\phi}(G|G|)}\\alpha_{1}+1\\right)\\left(\\frac{\\sqrt{2}\\kappa\\alpha\\left(\\partial P\\right)\\operatorname*{max}_{C\\in\\mathcal{C}}\\sqrt{\\mathrm{v}_{\\phi}(G|\\mathcal{D}_{\\phi}\\circ\\mathbb{t}_{1})}}{\\alpha_{1}\\left(\\frac{\\sqrt{c}\\Theta}{c\\Theta}\\right)\\sin\\frac{\\Theta\\left(C\\Theta\\right)}{c\\Theta}-2\\exp\\left(\\mathcal{D}\\mathcal{D}_{1}\\right)-1}+1\\right)}{1-\\left(1-\\frac{\\sqrt{2}\\kappa\\alpha\\left(\\partial P\\right)\\operatorname*{max}_{C\\in\\mathcal{C}}\\sqrt{\\mathrm{v}_{\\phi}(G|\\mathcal{D}_{\\phi}\\circ\\mathbb{t}_{1})}}{\\alpha_{1}\\left(1-\\frac{\\lambda_{2}\\kappa\\alpha_{1}}{\\frac{\\sqrt{c}\\Theta}{c\\Theta}}\\right)^{2}\\frac{1}{c\\Theta}\\sqrt{c}\\mathcal{C}(\\mathcal{C})}\\right)^{\\frac{1}{2}}}}\\\\ &{=\\sigma\\left(\\frac{\\operatorname*{max}_{C\\in\\mathcal{C}}\\nu_{\\phi}\\left(Q\\right)+\\operatorname*{max}_{C\\in\\mathcal{C}}\\nu_{\\phi}\\left(\\mathcal{C}\\right)+1}{\\frac{\\alpha_{1}\\alpha_{1}\\sqrt{c}\\phi}{\\alpha_{1}\\phi}\\sqrt{c_{\\phi}(C)}}+\\frac{1}{\\frac{\\alpha_{2}\\kappa\\alpha_{1}}{c\\Theta}c\\left(C\\right)+1}\\right)}\\\\ &{=\\sigma\\left(\\frac{1}{\\operatorname*{min}\\sqrt{c}_{\\phi}\\left(\\mathcal{C}\\right)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "731 The first big $\\scriptscriptstyle\\mathcal{O}$ notation is obtained due to the fact that for for large $\\operatorname*{min}_{\\mathcal{C}\\in\\mathcal{P}}\\sqrt{c_{\\mathcal{G}}(\\mathcal{C})}$ and small   \n732 $\\operatorname*{max}_{\\mathcal{C}\\in\\mathcal{P}}\\sqrt{\\iota_{\\mathcal{G}}(\\mathcal{C})}$ the denominator term i.e. $\\hat{\\phi}^{2}$ behaves like $1-\\gamma$ whichleaves the numerator   \n733 dominating the rest of the term. Now, we simply have to insert all our results into the sum of   \n734 instantaneous regrets: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{R}(\\widetilde{T})\\leq\\ell_{1}+2n+8\\cdot S\\left(\\int_{0}^{\\ell(\\ell,\\frac{\\delta}{\\delta})\\sqrt{T}}\\left(\\sqrt{|\\nabla^{\\perp}(\\star)|\\nabla^{\\perp}(\\widetilde{T}^{\\mathbb{V}})|}+\\sqrt{|\\nabla\\uptheta_{\\mathrm{off}}(\\widetilde{T}^{\\mathbb{V}})|}\\right)\\right)}\\\\ &{\\quad\\leq\\sqrt{|\\nabla^{\\perp}|}\\left(\\frac{1}{n}\\sum_{k=1}^{n}\\widetilde{\\left(\\epsilon\\right)|\\nabla^{\\perp}(\\frac{1}{n}+\\lambda)}\\right)+2n+S}\\\\ &{\\quad+o\\left(\\frac{f(\\ell,\\Theta)\\sqrt{T}}{\\delta}\\right)}\\\\ &{\\quad\\leq\\sqrt{|\\nabla^{\\perp}|}+\\left[\\frac{1}{n}\\log(12\\widetilde{\\nu}|)+\\sqrt{\\log(17\\mathbb{V})}+\\sqrt{|\\nabla\\uptheta_{\\mathrm{off}}(\\widetilde{T}^{\\mathbb{V}})|}\\right)\\right]}\\\\ &{\\quad\\leq\\sqrt{\\|\\nabla^{\\perp}|}+\\left[\\frac{1}{n}\\log(12\\widetilde{\\nu}|)(1+A)\\right]+\\frac{1}{n}+6}\\\\ &{\\quad+O\\left(\\frac{f(\\ell,\\Theta)\\sqrt{T}}{\\delta}\\left(\\sqrt{|\\nabla^{\\perp}(\\star)|\\nabla\\mathbb{V}|}+\\sqrt{\\log(17\\mathbb{V})}+\\sqrt{|\\nabla\\uptheta_{\\mathrm{off}}(\\widetilde{T}^{\\mathbb{V}})|}\\right)\\right)}\\\\ &{\\quad\\leq\\sqrt{|\\nabla^{\\perp}|}+\\left[\\frac{1}{n}\\log(12\\widetilde{\\nu}|)(1+A)\\right]+\\frac{1}{n}+8}\\\\ &{\\quad+O\\left(\\frac{f(\\ell,\\Theta)\\sqrt{T}}{2}\\right)}\\\\ &{\\quad=O\\left(\\frac{1}{n}\\log(\\tau)+\\sqrt{\\left(\\sqrt{\\|\\nabla^{\\perp}+\\sqrt{\\log(17\\mathbb{V})}}+\\sqrt{\\left|\\nabla\\uptheta_{\\mathrm{off}}(\\widetilde{T}^{\\mathbb{V}})\\|}\\right)}\\right)}\\\\ &{\\quad=O\\left(\\frac{1}{n}\\log(\\tau)+\\sqrt{\\underbrace{T_{\\mathrm{min}}^{\\ T}}_{\\mathrm{papc}(\\mathcal{T}^{\\mathbb{V}})}\\left(\\sqrt{|\\nabla^{\\perp}+\\sqrt{\\log(T)\\mathbb{V}}|}+\\sqrt{|\\nabla\\uptheta_{\\mathrm{off}}(\\widetilde{T}^{\\mathbb{V}}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "735  where we set u = 2A in the third inequality. ", "page_idx": 32}, {"type": "text", "text": "738 Homophily and modularity in social networks   Given the large number of users on social networks,   \n739 one may be able to learn their preferences more quickly by leveraging the similarities between them.   \n740 This idea relies on the notion of homophily in social networks McPherson et al. [2o01], Easley et al.   \n741 [2010]. In modelling social networks, users\u2019 preferences relationships are encoded in a graph, where   \n742 neighboring nodes are users with similar preferences. This graph can be known a priori or it can   \n743 be inferred from previously collected feedback Dong et al. [2019]. Exploiting this information and   \n744 integrating them into bandit algorithms can lead to a significant increase in performance Yang et al.   \n745 [2020]. Indeed, the knowledge of user relations allows the algorithm to tackle the data sparsity issue   \n746  that is inherent to bandit settings.   \n747Another fundamental point that can be used for integration of information from social networks is   \n748 that, social networks show large modularity measures Newman [2006] Borge-Holthoefer et al. [2011].   \n749 This implies that we have high density of edges within clusters and low density of edges between   \n750 clusters. As a result, users can be clustered based on the graph topology and a preference vector   \n751 can be learned for each cluster, substantially reducing the dimensionality of the problem. In other   \n752 words, discovering the clustering structure of users can reduce the computational burden of large   \n753 social networks. Consequently, there have been attempts in exploiting the clustered structures of   \n754  social networks in bandit algorithms Gentile et al. [2014], Nguyen and Lauw [2014], Yang and Toni   \n755 [2018], Li et al. [2019], Nourani-Koliji et al. [2023], Cheng et al. [2023].   \n756 Bandit meta-learning  In contrast to the multi-task setting, meta learning deals with sequentially   \n757 arriving tasks that have to be learnt and generalizing the gained information to improve performance   \n758 for future tasks. Here, as in the multi-task setting, it is assumed that the tasks share some common   \n759 structure that is ought to be learnt and exploited. In the work of Bilaj et al. [2024] it is assumed that   \n760 the tasks were sampled from a common distribution such that they are concentrated around an affine   \n761 subspace, which is learnt through PCA algorithm. The resulting projection matrices could then be   \n762 exploited to improve learning for new tasks in an adapted UCB and Thompson sampling approach.   \n763 Other lines of work are Cella et al. [2020], Kveton et al. [2021], Basu et al. [2021], which learns the   \n764 mean of the distribution under the assumption that the covariance of the prior is known or Peleg et al.   \n765 [2022] which generalizes this assumption and attempts to learn the covariance as well. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "766  D Additional experimental details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "767 D.1  About experiments of the main paper ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "768The experiments have been conducted with an intel i7 CPU with $12\\ 2.6\\ \\mathrm{GHz}$ coresand $32\\;\\mathrm{GB}$ Oof   \n769 RAM. The two experiments with the highest number of tasks (200) and dimension (80) take about 8   \n770 hours, parallelized over the 12 cores.   \n771 To generate clusters, we generate $|\\mathcal P|$ variables $v_{i i\\in\\mathcal{P}}$ from the uniform distribution, then we use   \n772 them to construct a categorical distribution with probabilities proportional to $e^{v_{i}}$ . These probabilities   \n773 defines the cluster proportions. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "774 D.2 Solving the Network Lasso problem ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "775  We implement the Primal-Dual algorithm proposed in Jung [2020] to solve the Network Lasso   \n776 problem but we do not vectorize the matrices (in the sense of stacking their columns into a vector),   \n777 whichspeedsupcomputation. ", "page_idx": 33}, {"type": "text", "text": "778  D.3  Algebraic connectivity vs topological centrality index ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "779 Given two fully connected graphs weightless $\\mathcal{G}_{1}$ and $\\mathcal{G}_{2}$ with size 100 each, we progressively link   \n780 them by edges, we construct the Laplcian $\\mathbf{L}$ of the resulting graph $\\mathcal{G}$ . We measure the minimum   \n781  topological centrality index $\\operatorname*{min}_{1\\leq i\\in200}(L_{\\mathcal{C}}^{\\dag})_{i i}^{-1}$ , and the algebraic connectivity, i.e. the minimum   \n782  non-null eigenvalue of $L$   \n783 Clearly, the minimum topological centrality index grows faster than the algebraic connectivity in   \n784  this case, and seems to saturate at some level that is reached in a linear progress by the algebraic   \n785connectivity. ", "page_idx": 33}, {"type": "image", "img_path": "WxW4nZMD3D/tmp/1293fc5933be78f216d8a39e86e247919b85ddbfc65385366dd53b45f19d3fd6.jpg", "img_caption": ["Figure 2: Minimum Topological centrality index vs Algebraic Connectivity, for a graph formed by connecting two fully connected initial graphs $\\mathcal{G}_{1},\\mathcal{G}_{2}$ with size 100 each. "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "786 D.4 Limitations ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "787  The first limitation of the paper is the restriction to the setting of i.i.d generated action sets. This   \n788 restriction is common to all papers relying on Lasso-type optimization objectives [Bastani and Bayati,   \n789  2019, Oh et al., 2021, Cella and Pontil, 2021, Ariu et al., 2022, Cella et al., 2023]. Also, we do not   \n790 provide a lower bound for the regret, a challenge that we let for future work. Besides, our optimization   \n791 problem is not strongly convex, which can be mitigated by adding a squared $L^{2}$ norm regularization.   \n792 However, such an addition would probably drastically change the theoretical analysis. ", "page_idx": 34}, {"type": "text", "text": "793 D.5 Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "794  As our method can be applied to transfer knowledge between users of a recommender system, it has   \n795 the potential to improve their overall experience by learning their preferences quickly. However, one   \n796  must be careful with the strength of the integrated prior knowledge as it can lead to an adverse effect   \n797 of slowing down the learning process. ", "page_idx": 34}, {"type": "text", "text": "798  NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "99 1. Claims   \n300 Question: Do the main claims made in the abstract and introduction accurately refect the   \n301 paper's contributions and scope?   \n302 Answer: [Yes]   \n303 Justification: The piecewise stationarity on a graph assumption is mentioned in Section 3   \n304 and formalized in Assumption 3. Theorem 1 states the oracle inequality, and Theorem 3   \n305 provides the regret bound after using the result of Theorem 2. Experiments are carried out   \n306 at Section 6.   \n307 Guidelines:   \n308 \u00b7 The answer NA means that the abstract and introduction do not include the claims made   \n309 in the paper.   \n310 \u00b7 The abstract and/or introduction should clearly state the claims made, including the   \n311 contributions made in the paper and important assumptions and limitations. A No or NA   \n312 answer to this question will not be perceived well by the reviewers.   \n313 \u00b7 The claims made should match theoretical and experimental results, and refect how   \n314 much the results can be expected to generalize to other settings.   \n315 \u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n316 are not attained by the paper. ", "page_idx": 35}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Guidelines: \u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. \u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper. \u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. \u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. \u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. \u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. \u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. \u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. Theory Assumptions and Proofs ", "page_idx": 35}, {"type": "text", "text": "9 Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 35}, {"type": "text", "text": "53 are available in the supplementary material and some proof ideas in the main material.   \n54 Guidelines:   \n55 \u00b7 The answer NA means that the paper does not include theoretical results.   \n56 \u00b7 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n57 referenced.   \n58 \u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n59 \u00b7 The proofs can either appear in the main paper or the supplemental material, but if they   \n60 appear in the supplemental material, the authors are encouraged to provide a short proof   \n61 sketch to provide intuition.   \n62 \u00b7 Inversely, any informal proof provided in the core of the paper should be complemented   \n63 by formal proofs provided in appendix or supplemental material.   \n64 \u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n66 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n67 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n368 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "Justification: The code is included as a zip-file and all results are reproducible. ", "page_idx": 36}, {"type": "text", "text": "2 \u00b7 The answer NA means that the paper does not include experiments.   \n3 \u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. \u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken   \n7 to make their results reproducible or verifiable.   \n3 \u00b7 Depending on the contribution, reproducibility can be accomplished in various ways.   \n3 For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may   \n1 be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often   \n3 one good way to accomplish this, but reproducibility can also be provided via detailed   \n+ instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. \u00b7 While NeurIPS does not require releasing code, the conference does require all submis  \n3 sions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example   \n\uff09 (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n2 (b) If the contribution is primarily a new model architecture, the paper should describe   \n3 the architecture clearly and fully. () If the contribution is a new model (e.g., a large language model), then there should   \n5 either be a way to access this model for reproducing the results or a way to reproduce   \n3 the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n3 (d)  We recognize that reproducibility may be tricky in some cases, in which case authors   \n3 are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 36}, {"type": "text", "text": "903 5. Open access to data and code ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "04 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n05 tions to faithfully reproduce the main experimental results, as described in supplemental   \n06 material?   \n07 Answer: [Yes]   \n08 Justification: We use only simulated data.   \n09 Guidelines:   \n10 \u00b7 The answer NA means that paper does not include experiments requiring code.   \n11 \u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/   \n12 public/guides/CodeSubmissionPolicy) for more details.   \n13 \u00b7 While we encourage the release of code and data, we understand that this might not be   \n14 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n15 including code, unless this is central to the contribution (e.g., for a new open-source   \n16 benchmark).   \n17 \u00b7 The instructions should contain the exact command and environment needed to run to   \n18 reproduce the results. See the NeurIPS code and data submission guidelines (https :   \n19 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n20 \u00b7 The authors should provide instructions on data access and preparation, including how   \n21 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n22 \u00b7 The authors should provide scripts to reproduce all experimental results for the new   \n23 proposed method and baselines. If only a subset of experiments are reproducible, they   \n24 should state which ones are omitted from the script and why.   \n25 \u00b7 At submisson time, to preserve anonymity, the authors should release anonymized   \n26 versions (if applicable).   \n27 \u00b7 Providing as much information as possible in supplemental material (appended to the   \n28 paper) is recommended, but including URLs to data and code is permitted.   \n29 6. Experimental Setting/Details   \n30 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n31 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n32 results?   \n33 Answer: [Yes]   \n34 Justification: While training and testing sets are not part of our paper as it is about bandit   \n35 algorithms, we provide the source of the optimization algorithm we use to solve Equation (2).   \n36 Guidelines:   \n37 \u00b7 The answer NA means that the paper does not include experiments.   \n38 \u00b7 The experimental setting should be presented in the core of the paper to a level of detail   \n39 that is necessary to appreciate the results and make sense of them.   \n40 \u00b7 The full details can be provided either with the code, in appendix, or as supplemental   \n41 material.   \n42 7. Experiment Statistical Significance   \n43 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n44 information about the statistical significance of the exneriments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). ", "page_idx": 37}, {"type": "text", "text": "955 \u00b7 The method for calculating the error bars should be explained (closed form formula, call   \n956 to a library function, bootstrap, etc.)   \n957 \u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n958 \u00b7 It should be clear whether the error bar is the standard deviation or the standard error of   \n959 the mean.   \n960 \u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n961 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n962 of Normality of errors is not verified.   \n963 \u00b7 For asymmetric distributions, the authors should be careful not to show in tables or   \n964 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n965 error rates).   \n966 \u00b7If errorbars are reported in tablesorlots, he authrs should explain in the texthow   \n967 they were calculated and reference the corresponding fgures or tables in the text.   \n968 8. Experiments Compute Resources   \n969 Question: For each experiment, does the paper provide suffcient information on the com  \n970 puter resources (type of compute workers, memory,time of execution) needed to reproduce   \n971 the experiments?   \n972 Answer: [Yes]   \n973 Justification: Computation resources used are mentioned in Appendix D.1.   \n974 Guidelines:   \n975 \u00b7 The answer NA means that the paper does not include experiments.   \n976 \u00b7The paper should indicate the tye of compute workers CPU or GPU, intenal cluster, or   \n977 cloud provider, including relevant memory and storage.   \n978 \u00b7 The paper should provide the amount of compute required for each of the individual   \n979 experimental runs as well as estimate the total compute.   \n980 \u00b0The paper should disclose whether the fullresearch project required more compute than   \n981 the experiments reported in the paper (e.g., preliminary or failed experiments that didn't   \n982 make it into the paper).   \n983 9. Code Of Ethics   \n984 Question: Does the research conducted in the paper conform, in every respect, with the   \n985 NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines?   \n986 Answer: [Yes]   \n987 Justifcation: Our results are theoretical, and their potential ham largely depends on their   \n988 application.   \n989 Guidelines:   \n990 \u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n991 \u00b7 If the authors answer No, they should explain the special circumstances that require a   \n992 deviation from the Code of Ethics.   \n993 \u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special considera  \n994 tion due to laws or regulations in their jurisdiction).   \n995 10. Broader Impacts   \n996 Question: Does the paper discuss both potential positive societal impacts and negative   \n997 societal impacts of the work performed?   \n998 Answer: [Yes]   \n999 Justification: Itis provided in Appendix D.5 in the appendix.   \n1000 Guidelines:   \n1001 \u00b7 The answer NA means that there is no societal impact of the work performed.   \n1002 \u00b7 If the authors answer NA or No, they should explain why their work has no societal   \n1003 impact or why the paper does not address societal impact.   \n1004 \u00b7 Examples of negative societal impacts include potential malicious or unintended uses   \n1005 (e.g., disinformation, generating fake profles, surveillance), fairnes considerations   \n1006 (e.g., deployment of technologies that could make decisions that unfairly impact specifc   \n1007 groups), privacy considerations, and security considerations.   \n1008 \u00b7The conference expects that many papers willbe foundational research and not tied   \n1009 to particular applications, lt alone deployments. However, if there is a direct path t   \n1010 any negative applications, the authors should point it out. For example, it is legitimate   \n1011 to point out that an improvement in the quality of generative models could be used to   \n1012 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n1013 that a generic algorithm for optimizing neural networks could enable people to train   \n1014 models that generate Deepfakes faster.   \n1015 \u00b7The authors should consider possible harms that could arise when the technology is being   \n1016 used as intended and functioning correctly, harms that could arise when the technology is   \n1017 being used as intended but gives incorrect results, and harms following from (intentional   \n1018 or unintentional) misuse of the technology.   \n1019 \u00b7If there are negative societal impacts, the authors could also discuss possible mitigation   \n1020 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n1021 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n1022 feedback over time, improving the efficiency and accessibility of ML).   \n1023 11. Safeguards   \n1024 Question: Does the paper describe safeguards that have been put in place for responsible   \n1025 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n1026 image generators, or scraped datasets)?   \n1027 Answer: [NA]   \n1028 Justification: Only simulated data are used.   \n1029 Guidelines:   \n1030 \u00b7 The answer NA means that the paper poses no such risks.   \n1031 \u00b7 Released models that have a high risk for misuse or dual-use should be released with   \n1032 necessary safeguards to allow for controlled use of the model, for example by requiring   \n1033 that users adhere to usage guidelines orrestrictions to access the model or implementing   \n1034 safety filters.   \n1035 \u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n1036 should describe how they avoided releasing unsafe images.   \n1037 \u00b7We recognize that providing effective safeguards is challenging, and many papers do not   \n1038 require this, but we encourage authors to take this into account and make a best faith   \n1039 effort.   \n1040 12. Licenses for existing assets   \n1041 Question: Are the creators or original owners of asets (e.g, code, data, models), used in   \n1042 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n1043 properly respected?   \n1044 Answer: [Yes]   \n1045 Justification: All relevant work is properly cited and code is provided to reproduce the   \n1046 results.   \n1047 Guidelines:   \n1048 \u00b7 The answer NA means that the paper does not use existing assets.   \n1049 \u00b7 The authors should cite the original paper that produced the code package or dataset.   \n1050 \u00b7 The authors should state which version of the asset is used and, if possible, include a   \n1051 URL.   \n1052 \u00b7The name of the license (e.g., CC-BY 4.0) should be included for each asst.   \n1053 \u00b7 For scraped data from a particular source (e.g, website), the copyright and tems of   \n1054 service of that source should be provided.   \n055 \u00b7 If assets are released, the license, copyright information, and terms of use in the pack  \n056 age should be provided. For popular datasets, paperswithcode . com/datasets has   \n057 curated licenses for some datasets. Their licensing guide can help determine the license   \n058 of a dataset.   \n059 \u00b7 For existing datasets that are re-packaged, both the original license and the license of the   \n060 derived asset (if it has changed) should be provided.   \n061 \u00b7 If this information is not available online, the authors are encouraged to reach out to the   \n062 asset's creators.   \n063 13. New Assets   \n064 Question: Are new assets introduced in the paper well documented and is the documentation   \n065 provided alongside the assets?   \n066 Answer: [No]   \n067 Justification: [NA]   \n068 Guidelines:   \n069 \u00b7 The answer NA means that the paper does not release new assets.   \n070 \u00b7 Researchers should communicate the details of the dataset/code/model as part of their   \n071 submissions via structured templates. This includes details about training, license,   \n72 limitations, etc.   \n073 \u00b7 The paper should discuss whether and how consent was obtained from people whose   \n074 asset is used.   \n75 \u00b7 At submission time, remember to anonymize your assets (if applicable). You can either   \n076 create an anonymized URL or include an anonymized zip file.   \n077 14. Crowdsourcing and Research with Human Subjects   \n078 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n079 include the full text of instructions given to participants and screenshots, if applicable, as   \n080 well as details about compensation (if any)?   \n081 Answer: [NA]   \n082 Justification: [NA]   \n083 Guidelines:   \n084 \u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with   \n085 human subjects.   \n086 \u00b7 Including this information in the supplemental material is fine, but if the main contri  \n087 bution of the paper involves human subjects, then as much detail as possible should be   \n088 included in the main paper.   \n089 \u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation,   \n090 or other labor should be paid at least the minimum wage in the country of the data   \n091 collector.   \n15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "Subjects ", "page_idx": 40}, {"type": "text", "text": "34 Question: Does the paper describe potential risks incurred by study participants, whether   \n35 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n36 approvals (or an equivalent approval/review based on the requirements of your country or   \n97 institution) were obtained? ", "page_idx": 40}, {"type": "text", "text": "3 Justification: [NA] Guidelines: \u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with   \n2 human subjects.   \n3 \u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n4 may be required for any human subjects research. If you obtained IRB approval, you   \n5 should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 41}]