[{"figure_path": "WxW4nZMD3D/figures/figures_4_1.jpg", "caption": "Figure 1: Synthetic data experiment showing the cumulative regret of Network Lasso Policy as a function of time-steps compared to other baselines, for different choices of |V|, d, p and q.", "description": "This figure presents the results of a synthetic data experiment comparing the cumulative regret of the proposed Network Lasso Bandit algorithm against several baselines.  The cumulative regret is plotted against the number of time steps. The experiment is repeated for different graph sizes (|V|), dimensions (d), and parameters (p, q) controlling the probability of edges within and between clusters respectively in a stochastic block model for graph generation. The baselines include GOBLin, GraphUCB, CLUB, SCLUB, independent task learning (LinUCB ITL), and an oracle LinUCB. The results show the performance of the Network Lasso Bandit algorithm in various settings, highlighting its advantage, particularly in larger and higher dimensional scenarios.", "section": "6 Experiments"}, {"figure_path": "WxW4nZMD3D/figures/figures_8_1.jpg", "caption": "Figure 1: Synthetic data experiment showing the cumulative regret of Network Lasso Policy as a function of time-steps compared to other baselines, for different choices of |V|, d, p and q.", "description": "The figure displays the cumulative regret of the proposed Network Lasso Policy against several baselines (CLUB, GOBLin, GraphUCB, LinUCB (independent task learning setting), LinUCB (oracle, i.e., knowing the cluster structure), SCLUB) over time.  Four different experimental settings are shown, each varying the number of tasks |V|, the context dimension d, and the probabilities p and q used to generate the graph structure using stochastic block models. The settings are chosen to illustrate the performance of the algorithm under different conditions of graph density and task heterogeneity. Higher curves represent higher regret, indicating worse performance.", "section": "6 Experiments"}, {"figure_path": "WxW4nZMD3D/figures/figures_34_1.jpg", "caption": "Figure 1: Synthetic data experiment showing the cumulative regret of Network Lasso Policy as a function of time-steps compared to other baselines, for different choices of |V|, d, p and q.", "description": "This figure shows the cumulative regret over time for the Network Lasso Policy and several baseline algorithms across different experimental settings.  The settings vary the number of tasks |V|, the dimensionality of the context vectors d, and the parameters p and q that control the probability of edges within and between clusters in the graph, respectively. The plot demonstrates how the Network Lasso Policy outperforms the baselines in terms of cumulative regret, highlighting its ability to leverage the graph structure for improved performance.", "section": "6 Experiments"}]