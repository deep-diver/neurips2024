[{"figure_path": "HcqnhqoXS3/figures/figures_3_1.jpg", "caption": "Figure 1: The architecture diagram of DPDT. For simplicity and clarity, the figure only displays the decomposition and integration process during training on a single task. Snowflake icons represent the frozen parts of the model that are not subject to training updates, while flame icons indicate components of the model that remain trainable. Left: Decomposed Prompt Tuning. The prompt decomposition is trained on the entire dataset with the assistance of the teacher prompt pteacher.p is then combined with the training samples that include K steps, inputted into DPDT, and outputs the corresponding action a. Right: Test Time Adaptation. When test samples are fed into the model, we calculate the mean and variance of all samples at each layer and compute the loss by comparing them with the mean and variance of the corresponding layer from the training samples. The losses from all layers are summed to obtain the alignment loss, denoted as Lalign.", "description": "This figure illustrates the architecture of the Decomposed Prompt Decision Transformer (DPDT).  The left side shows the \"Decomposed Prompt Tuning\" stage, highlighting the decomposition of the task prompt into cross-task and task-specific components, leveraging a teacher prompt for distillation. The right side depicts the \"Test Time Adaptation\" stage, focusing on how the model dynamically adjusts the cross-task prompt based on the distribution of unseen test samples, using mean and variance calculations for alignment.", "section": "4 Decomposed Prompt Decision Transformer"}, {"figure_path": "HcqnhqoXS3/figures/figures_7_1.jpg", "caption": "Figure 1: The architecture diagram of DPDT. For simplicity and clarity, the figure only displays the decomposition and integration process during training on a single task. Snowflake icons represent the frozen parts of the model that are not subject to training updates, while flame icons indicate components of the model that remain trainable. Left: Decomposed Prompt Tuning. The prompt decomposition is trained on the entire dataset with the assistance of the teacher prompt pteacher.p is then combined with the training samples that include K steps, inputted into DPDT, and outputs the corresponding action a. Right: Test Time Adaptation. When test samples are fed into the model, we calculate the mean and variance of all samples at each layer and compute the loss by comparing them with the mean and variance of the corresponding layer from the training samples. The losses from all layers are summed to obtain the alignment loss, denoted as Lalign.", "description": "This figure illustrates the architecture of the Decomposed Prompt Decision Transformer (DPDT) model.  It shows the two main stages: Decomposed Prompt Tuning and Test Time Adaptation. The left side depicts the prompt decomposition process during training, where a task prompt is decomposed into cross-task and task-specific prompts to avoid gradient conflicts.  The right side shows the test-time adaptation phase, where the cross-task prompt is further optimized on unseen tasks using test-time adaptation techniques by comparing the statistics of training and testing samples across all layers of the model.", "section": "4 Decomposed Prompt Decision Transformer"}, {"figure_path": "HcqnhqoXS3/figures/figures_8_1.jpg", "caption": "Figure 3: Ablation: The effect of prompt length on DPDT's zero-shot generalization ability.", "description": "This figure shows the ablation study on the effect of prompt length on the performance of DPDT in zero-shot generalization scenarios.  The x-axis represents different prompt lengths (3, 9, 30, 60, 90), and the y-axis shows the average scores achieved by DPDT on three different tasks: Cheetah-vel, Ant-dir, and MW ML45.  The plot demonstrates the impact of prompt length on model performance, allowing one to determine the optimal length that balances model convergence and generalization performance.", "section": "5.3 Further Analysis"}, {"figure_path": "HcqnhqoXS3/figures/figures_15_1.jpg", "caption": "Figure 1: The architecture diagram of DPDT. For simplicity and clarity, the figure only displays the decomposition and integration process during training on a single task. Snowflake icons represent the frozen parts of the model that are not subject to training updates, while flame icons indicate components of the model that remain trainable. Left: Decomposed Prompt Tuning. The prompt decomposition is trained on the entire dataset with the assistance of the teacher prompt pteacher. P* is then combined with the training samples that include K steps, inputted into DPDT, and outputs the corresponding action a. Right: Test Time Adaptation. When test samples are fed into the model, we calculate the mean and variance of all samples at each layer and compute the loss by comparing them with the mean and variance of the corresponding layer from the training samples. The losses from all layers are summed to obtain the alignment loss, denoted as Lalign.", "description": "This figure illustrates the architecture of the Decomposed Prompt Decision Transformer (DPDT).  The left side shows the \"Decomposed Prompt Tuning\" phase, where the task prompt is decomposed into cross-task and task-specific components using a pre-trained GPT model.  The right side illustrates the \"Test Time Adaptation\" phase, which involves dynamically adjusting the cross-task prompt on unseen tasks using the training data mean and variance for comparison.", "section": "4 Decomposed Prompt Decision Transformer"}]