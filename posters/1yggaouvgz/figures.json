[{"figure_path": "1YGgaouVgZ/figures/figures_1_1.jpg", "caption": "Figure 1: Counterintuitive generalization of perturbation learning.\u00b9 A classifier g is trained solely on mislabeled adversarial examples Dadv := {(xadv, yadv)}_1. These examples \u00e6adv are generated to mislead a classifier f, which is trained on correctly labeled clean samples D := {(xn, Yn)}n=1, into predicting yadv (\u2260 yn). Surprisingly, despite being trained only on mislabeled data, the classifier g generalizes well to clean test samples. This counterintuitive result suggests that adversarial perturbations contain label-aligned class features, enabling the classifier g to generalize from them.", "description": "The figure illustrates the counterintuitive result of perturbation learning.  A classifier (g) trained only on mislabeled adversarial examples generalizes well to correctly labeled clean data. This is unexpected, because the training data is entirely incorrect according to human perception.  The success suggests that adversarial perturbations contain class-specific features that are useful for classification.", "section": "1 Introduction"}, {"figure_path": "1YGgaouVgZ/figures/figures_5_1.jpg", "caption": "Figure 1: Counterintuitive generalization of perturbation learning.\u00b9 A classifier g is trained solely on mislabeled adversarial examples Dadv := {(xadv, yadv)}N1. These examples xadv are generated to mislead a classifier f, which is trained on correctly labeled clean samples D := {(xn, yn)}N=1, into predicting yadv (\u2260 yn). Surprisingly, despite being trained only on mislabeled data, the classifier g generalizes well to clean test samples. This counterintuitive result suggests that adversarial perturbations contain label-aligned class features, enabling the classifier g to generalize from them.", "description": "The figure illustrates the surprising result of perturbation learning, where a classifier trained only on mislabeled adversarial examples still generalizes well to correctly labeled data.  This challenges conventional wisdom and suggests adversarial perturbations contain class-specific features.", "section": "Introduction"}, {"figure_path": "1YGgaouVgZ/figures/figures_9_1.jpg", "caption": "Figure 3: Accuracy on the mean-shifted Gaussian dataset in Scenario (a). The blue lines represent accuracy of the classifier f on D, i.e., training accuracy. The orange lines represent accuracy of the classifier g on D.", "description": "This figure shows the accuracy results of two classifiers, f and g, trained on different datasets, on a mean-shifted Gaussian dataset.  The classifier f is trained on correctly labeled clean samples, while classifier g is trained on mislabeled adversarial examples. The blue lines show the training accuracy of f, which is high and plateaus as the input dimension increases. The orange lines show the accuracy of g, which surprisingly also achieves high accuracy despite only being trained on mislabeled adversarial examples.  The trend highlights the effectiveness of perturbation learning.", "section": "Experiments"}, {"figure_path": "1YGgaouVgZ/figures/figures_16_1.jpg", "caption": "Figure 3: Accuracy on the mean-shifted Gaussian dataset in Scenario (a). The blue lines represent accuracy of the classifier f on D, i.e., training accuracy. The orange lines represent accuracy of the classifier g on D.", "description": "This figure shows the accuracy of two classifiers, f and g, trained on different datasets, on a mean-shifted Gaussian dataset for scenario (a). Classifier f is trained on correctly labeled data (clean samples), while classifier g is trained on mislabeled adversarial examples.  The blue lines show f's accuracy (training accuracy), and the orange lines show g's accuracy. The results indicate that despite being trained on mislabeled data, g can generalize reasonably well to clean test samples. The figure demonstrates the counter-intuitive effectiveness of perturbation learning, where a classifier trained solely on adversarial examples generalizes well to unseen clean data.", "section": "Experiments"}, {"figure_path": "1YGgaouVgZ/figures/figures_17_1.jpg", "caption": "Figure 3: Accuracy on the mean-shifted Gaussian dataset in Scenario (a). The blue lines represent accuracy of the classifier f on D, i.e., training accuracy. The orange lines represent accuracy of the classifier g on D.", "description": "This figure shows the accuracy of two classifiers, f and g, trained on different datasets, on a mean-shifted Gaussian dataset. Classifier f is trained on correctly labeled clean samples, while classifier g is trained solely on mislabeled adversarial examples.  The plot illustrates how input dimension, hidden dimension, perturbation size, activation function slope, sample size, and training time impact the accuracy of the classifiers, demonstrating the counterintuitive ability of the mislabeled-data-trained classifier (g) to generalize to the clean data.", "section": "Experiments"}, {"figure_path": "1YGgaouVgZ/figures/figures_18_1.jpg", "caption": "Figure 3: Accuracy on the mean-shifted Gaussian dataset in Scenario (a). The blue lines represent accuracy of the classifier f on D, i.e., training accuracy. The orange lines represent accuracy of the classifier g on D.", "description": "This figure shows the accuracy of two classifiers, f and g, trained on different datasets, on a mean-shifted Gaussian dataset. Classifier f is trained on correctly labeled clean samples (training accuracy), while classifier g is trained only on mislabeled adversarial examples.  The results demonstrate the counterintuitive success of perturbation learning, where the classifier trained only on mislabeled data still achieves a relatively high accuracy, supporting the hypothesis that adversarial perturbations contain sufficient class-specific features.", "section": "Experiments"}, {"figure_path": "1YGgaouVgZ/figures/figures_19_1.jpg", "caption": "Figure 3: Accuracy on the mean-shifted Gaussian dataset in Scenario (a). The blue lines represent accuracy of the classifier f on D, i.e., training accuracy. The orange lines represent accuracy of the classifier g on D.", "description": "This figure shows the results of an experiment using a mean-shifted Gaussian dataset and focusing on Scenario (a) of the perturbation learning process described in the paper. The blue lines represent the accuracy of a classifier (f) trained on correctly labeled clean samples (D). The orange lines represent the accuracy of another classifier (g) that was trained only on mislabeled adversarial examples generated from classifier f.  The results suggest that despite being trained on entirely mislabeled data, classifier g still generalizes well to the clean test data. The plot demonstrates the impact of input dimension (d) and hidden dimension (m) on the accuracy of both classifiers.", "section": "Experiments"}, {"figure_path": "1YGgaouVgZ/figures/figures_20_1.jpg", "caption": "Figure 3: Accuracy on the mean-shifted Gaussian dataset in Scenario (a). The blue lines represent accuracy of the classifier f on D, i.e., training accuracy. The orange lines represent accuracy of the classifier g on D.", "description": "This figure shows the accuracy of two classifiers, f and g, on a mean-shifted Gaussian dataset. Classifier f is trained on correctly labeled data, while classifier g is trained solely on mislabeled adversarial examples.  The plot demonstrates that the accuracy of classifier g approaches that of classifier f as input dimensions increase, which supports the hypothesis that adversarial perturbations contain class-specific information.", "section": "4 Experiments"}, {"figure_path": "1YGgaouVgZ/figures/figures_21_1.jpg", "caption": "Figure 3: Accuracy on the mean-shifted Gaussian dataset in Scenario (a). The blue lines represent accuracy of the classifier f on D, i.e., training accuracy. The orange lines represent accuracy of the classifier g on D.", "description": "The figure shows the accuracy of two classifiers, f and g, on a mean-shifted Gaussian dataset. Classifier f is trained on correctly labeled clean samples, while classifier g is trained only on mislabeled adversarial examples. The results demonstrate that even when trained on entirely mislabeled data, the classifier g trained with perturbation learning can still generalize well to clean test data. This supports the idea that adversarial perturbations contain sufficient class-specific features.", "section": "4 Experiments"}, {"figure_path": "1YGgaouVgZ/figures/figures_22_1.jpg", "caption": "Figure 3: Accuracy on the mean-shifted Gaussian dataset in Scenario (a). The blue lines represent accuracy of the classifier f on D, i.e., training accuracy. The orange lines represent accuracy of the classifier g on D.", "description": "This figure shows the results of experiments on a mean-shifted Gaussian dataset using Scenario (a) of perturbation learning.  The blue lines display the accuracy of a classifier (f) trained on correctly labeled clean data. The orange lines illustrate the accuracy of another classifier (g) trained solely on mislabeled adversarial examples generated from f. The x-axis represents different parameters such as input dimension, hidden dimension, perturbation size, etc,  demonstrating how these factors affect the accuracy of both classifiers. This figure supports the study's conclusion that adversarial perturbations contain sufficient class-specific information for classifiers to generalize, even when trained on mislabeled data.", "section": "Experiments"}, {"figure_path": "1YGgaouVgZ/figures/figures_23_1.jpg", "caption": "Figure 3: Accuracy on the mean-shifted Gaussian dataset in Scenario (a). The blue lines represent accuracy of the classifier f on D, i.e., training accuracy. The orange lines represent accuracy of the classifier g on D.", "description": "This figure shows the accuracy of two classifiers, f and g, on a mean-shifted Gaussian dataset for Scenario (a) of the perturbation learning experiment. Classifier f is trained on correctly labeled clean data, while classifier g is trained only on mislabeled adversarial examples. The blue lines show the accuracy of f on the training data (training accuracy), while the orange lines show the accuracy of g on the training data. The results demonstrate that even when trained solely on mislabeled adversarial examples, classifier g achieves a high level of accuracy.", "section": "Experiments"}, {"figure_path": "1YGgaouVgZ/figures/figures_24_1.jpg", "caption": "Figure 3: Accuracy on the mean-shifted Gaussian dataset in Scenario (a). The blue lines represent accuracy of the classifier f on D, i.e., training accuracy. The orange lines represent accuracy of the classifier g on D.", "description": "This figure shows the accuracy of two classifiers, f and g, trained on different datasets, on a mean-shifted Gaussian dataset for scenario (a) in the paper. The classifier f was trained on correctly labelled clean samples and the classifier g was trained solely on mislabeled adversarial examples.  The blue lines represent f's training accuracy (how well f performs on the training data), and the orange lines show g's accuracy on the same training dataset (a surprising result, given that g was trained only on mislabeled data). The x-axis shows the input dimension (d), while the y-axis shows accuracy.", "section": "Experiments"}, {"figure_path": "1YGgaouVgZ/figures/figures_25_1.jpg", "caption": "Figure 1: Counterintuitive generalization of perturbation learning.\u00b9 A classifier g is trained solely on mislabeled adversarial examples Dadv := {(xadv, yadv)}N_1. These examples xadv are generated to mislead a classifier f, which is trained on correctly labeled clean samples D := {(xn, Yn)}N_1, into predicting yadv (\u2260 yn). Surprisingly, despite being trained only on mislabeled data, the classifier g generalizes well to clean test samples. This counterintuitive result suggests that adversarial perturbations contain label-aligned class features, enabling the classifier g to generalize from them.", "description": "The figure illustrates the counterintuitive generalization ability of perturbation learning.  A classifier (g) trained only on mislabeled adversarial examples, surprisingly, generalizes well to correctly labeled clean data. This suggests that adversarial perturbations contain class-specific features that the classifier can learn and generalize from, despite the incorrect labels.", "section": "1 Introduction"}]