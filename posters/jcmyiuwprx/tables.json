[{"figure_path": "jCMYIUwprx/tables/tables_2_1.jpg", "caption": "Table 1: We compared INDICT and related methods from 3 directions: self-refine/self-critic, multi-agent, and finetuning. Compared to these methods, INDICT is a more well-rounded generation framework with the following contributions: (1) integrates code execution-based feedback and enhances them with external knowledge, (2) targets both helpfulness and safety of output code, and (3) facilitates an interactive and supervision-free multi-agent collaboration framework. Our experiment results showcase the efficacy of INDICT.", "description": "This table compares INDICT with other code generation methods, categorized into self-refine, multi-agent, and finetuning approaches. It highlights INDICT's advantages: integrating code execution feedback with external knowledge, focusing on both helpfulness and safety, and utilizing a collaborative, supervision-free multi-agent framework.", "section": "Related Work"}, {"figure_path": "jCMYIUwprx/tables/tables_4_1.jpg", "caption": "Table 1: We compared INDICT and related methods from 3 directions: self-refine/self-critic, multi-agent, and finetuning. Compared to these methods, INDICT is a more well-rounded generation framework with the following contributions: (1) integrates code execution-based feedback and enhances them with external knowledge, (2) targets both helpfulness and safety of output code, and (3) facilitates an interactive and supervision-free multi-agent collaboration framework. Our experiment results showcase the efficacy of INDICT. See Appendix D for cited references.", "description": "This table compares INDICT with other relevant methods categorized into three groups: self-refine/self-critic, multi-agent, and finetuning approaches.  It highlights INDICT's advantages, including its integrated code execution feedback, use of external knowledge, focus on both helpfulness and safety, and interactive multi-agent collaboration.  The table uses checkmarks to show which methods include specific features and notes the supervision-free aspect of INDICT.", "section": "Related Work"}, {"figure_path": "jCMYIUwprx/tables/tables_7_1.jpg", "caption": "Table 2: We evaluated INDICT with HarmBench against 6 different types of red-teaming optimization methods. We reported the safety measure as the percentage of outputs classified as benign by the given AI evaluator from HarmBench.", "description": "This table presents the results of evaluating the INDICT framework against six different red-teaming optimization methods using the HarmBench benchmark.  The safety measure, expressed as a percentage, indicates how often the AI evaluator classified the model's output as benign (non-harmful).  The table compares the performance of the base CommandR and Llama3 models (with and without INDICT) across these different optimization methods.  It shows the impact of INDICT on improving the safety of generated outputs, particularly notable with Llama3-8b-instruct and Llama3-70b-instruct.", "section": "4.3 Open-ended generation tasks"}, {"figure_path": "jCMYIUwprx/tables/tables_8_1.jpg", "caption": "Table 1: We compared INDICT and related methods from 3 directions: self-refine/self-critic, multi-agent, and finetuning. Compared to these methods, INDICT is a more well-rounded generation framework with the following contributions: (1) integrates code execution-based feedback and enhances them with external knowledge, (2) targets both helpfulness and safety of output code, and (3) facilitates an interactive and supervision-free multi-agent collaboration framework. Our experiment results showcase the efficacy of INDICT. See Appendix D for cited references.", "description": "This table compares INDICT with other code generation methods across three categories: self-refine/self-critic, multi-agent, and finetuning.  It highlights INDICT's advantages, including its use of code execution feedback, external knowledge integration, focus on both safety and helpfulness, and its unique interactive multi-agent collaboration.  The table uses checkmarks to show which methods incorporate each feature.  Appendix D provides references for the cited methods.", "section": "Related Work"}, {"figure_path": "jCMYIUwprx/tables/tables_9_1.jpg", "caption": "Table 3: We conducted an ablation analysis of INDICT when removing the proposed dual critic system and/or external tool enhancement. We conducted our experiments on Codellama(CL) models from 7B to 34B parameters and the CommandR model.", "description": "This table presents the results of an ablation study conducted to evaluate the impact of removing different components of the INDICT framework.  Specifically, it assesses the effects of removing the dual critic system and/or the external tool enhancement. The experiments were performed on CodeLlama models with varying parameter counts (7B to 34B) and the CommandR model.  The table shows the resulting safety and helpfulness scores for each configuration.", "section": "4.5 Ablation analysis"}, {"figure_path": "jCMYIUwprx/tables/tables_9_2.jpg", "caption": "Table 4: We conducted an ablation analysis of INDICT with different combinations of our critics, during either preemptive or posthoc feedback stage or both. To fairly compare these variants, we excluded any access to external tools, and used CommandR as the base model in all experiments.", "description": "This table presents the results of an ablation study on the INDICT framework.  The study explores the impact of different combinations of safety and helpfulness critics, and the timing of their feedback (preemptive, post-hoc, or both) on the performance of the model. The experiment uses CommandR as the base model and excludes external tools to isolate the effect of the critic configurations. The results are evaluated based on the average performance across safety and helpfulness metrics.", "section": "4.5 Ablation analysis"}, {"figure_path": "jCMYIUwprx/tables/tables_17_1.jpg", "caption": "Table 1: We compared INDICT and related methods from 3 directions: self-refine/self-critic, multi-agent, and finetuning. Compared to these methods, INDICT is a more well-rounded generation framework with the following contributions: (1) integrates code execution-based feedback and enhances them with external knowledge, (2) targets both helpfulness and safety of output code, and (3) facilitates an interactive and supervision-free multi-agent collaboration framework. Our experiment results showcase the efficacy of INDICT. See Appendix D for cited references.", "description": "This table compares INDICT with other code generation methods categorized into three approaches: self-refine, multi-agent, and finetuning.  It highlights INDICT's advantages, including its use of code execution feedback and external knowledge, its focus on both helpfulness and safety, and its unique interactive multi-agent collaboration framework.  Appendix D provides the references for the compared methods.", "section": "Related Work"}, {"figure_path": "jCMYIUwprx/tables/tables_19_1.jpg", "caption": "Table 1: We compared INDICT and related methods from 3 directions: self-refine/self-critic, multi-agent, and finetuning. Compared to these methods, INDICT is a more well-rounded generation framework with the following contributions: (1) integrates code execution-based feedback and enhances them with external knowledge, (2) targets both helpfulness and safety of output code, and (3) facilitates an interactive and supervision-free multi-agent collaboration framework. Our experiment results showcase the efficacy of INDICT. See Appendix D for cited references.", "description": "This table compares INDICT with other code generation methods across three categories: self-refine/self-critic, multi-agent, and finetuning.  It highlights INDICT's key advantages: incorporating code execution feedback and external knowledge, focusing on both safety and helpfulness, and utilizing a collaborative, supervision-free multi-agent framework. The table includes checkmarks indicating the presence or absence of certain features in each method, demonstrating INDICT's comprehensive approach.", "section": "Related Work"}, {"figure_path": "jCMYIUwprx/tables/tables_20_1.jpg", "caption": "Table 7: With CodeLlama-7b-instruct as the base model, we compared INDICT with CodeUltraFeedback [Weyssow et al., 2024], a finetuning approach using supervised-finetuning (SFT) or preference-based finetuning (DPO).", "description": "This table compares the performance of INDICT against several finetuning methods using CodeLlama-7b-instruct as the base language model.  It shows the safety and helpfulness scores for each method, indicating that INDICT, even when combined with finetuning, achieves superior results.", "section": "4.4 Comparison to baselines"}, {"figure_path": "jCMYIUwprx/tables/tables_21_1.jpg", "caption": "Table 8: With GPT40-mini as the base model, we compared INDICT with 4 different variants of the critic framework. We found that our proposed INDICT can lead to more well-rounded performance, with high results in both safety and helpfulness of the generated code.", "description": "This table presents the ablation study results of INDICT with different variants of critic frameworks, using GPT40-mini as the base language model.  It compares the full INDICT model against four variants: one with a single critic for both safety and helpfulness; one without a critic summarizer; one using RAG-based critics; and one using tool-based critics.  The results show that the full INDICT model achieves the highest performance in both safety and helpfulness, highlighting the benefits of using dual critics and external tools.", "section": "Ablation analysis"}, {"figure_path": "jCMYIUwprx/tables/tables_22_1.jpg", "caption": "Table 1: We compared INDICT and related methods from 3 directions: self-refine/self-critic, multi-agent, and finetuning. Compared to these methods, INDICT is a more well-rounded generation framework with the following contributions: (1) integrates code execution-based feedback and enhances them with external knowledge, (2) targets both helpfulness and safety of output code, and (3) facilitates an interactive and supervision-free multi-agent collaboration framework. Our experiment results showcase the efficacy of INDICT. See Appendix D for cited references.", "description": "This table compares INDICT with other code generation methods categorized into self-refine/self-critic, multi-agent, and finetuning approaches.  It highlights INDICT's advantages in integrating code execution feedback, incorporating external knowledge, focusing on both helpfulness and safety, and employing a supervision-free multi-agent collaboration framework.  Checkmarks indicate the features included in each method.", "section": "Related Work"}, {"figure_path": "jCMYIUwprx/tables/tables_22_2.jpg", "caption": "Table 1: We compared INDICT and related methods from 3 directions: self-refine/self-critic, multi-agent, and finetuning. Compared to these methods, INDICT is a more well-rounded generation framework with the following contributions: (1) integrates code execution-based feedback and enhances them with external knowledge, (2) targets both helpfulness and safety of output code, and (3) facilitates an interactive and supervision-free multi-agent collaboration framework. Our experiment results showcase the efficacy of INDICT. See Appendix D for cited references.", "description": "This table compares INDICT with other related methods in three categories: self-refine/self-critic, multi-agent, and finetuning. It highlights INDICT's advantages, such as integrating code execution feedback, utilizing external knowledge, targeting both helpfulness and safety, and employing a supervision-free multi-agent framework.  The table includes a checklist indicating which features each method incorporates.", "section": "Related Work"}, {"figure_path": "jCMYIUwprx/tables/tables_22_3.jpg", "caption": "Table 1: We compared INDICT and related methods from 3 directions: self-refine/self-critic, multi-agent, and finetuning. Compared to these methods, INDICT is a more well-rounded generation framework with the following contributions: (1) integrates code execution-based feedback and enhances them with external knowledge, (2) targets both helpfulness and safety of output code, and (3) facilitates an interactive and supervision-free multi-agent collaboration framework. Our experiment results showcase the efficacy of INDICT. See Appendix D for cited references.", "description": "This table compares INDICT with other existing code generation methods.  It categorizes methods into three groups: self-refine/self-critic, multi-agent, and fine-tuning.  The table highlights INDICT's advantages, such as incorporating code execution feedback, using external knowledge, addressing both helpfulness and safety, and utilizing a multi-agent framework.", "section": "Related Work"}, {"figure_path": "jCMYIUwprx/tables/tables_23_1.jpg", "caption": "Table 12: Test results of CVS: we reported the % output codes that are considered secure (determined by a rule-based detector). We applied INDICT with 3 base LLMs: CommandR, Llama3-8b-instruct and Llama3-70b-instruct. We observed that with INDICT, all 3 models are consistently improved by safety measure, even better than the given ground-truth secure code solutions.", "description": "This table presents the results of evaluating the security of code generated by three different large language models (LLMs): CommandR, Llama3-8b-instruct, and Llama3-70b-instruct, both with and without the INDICT framework.  The security of the generated code is assessed using a rule-based detector.  The table shows the percentage of generated code samples deemed secure by the detector for each model and language (C++, C#, Java, Javascript, PHP).  The results demonstrate a significant improvement in code security when using INDICT.", "section": "4.1 Insecure coding practice tasks"}, {"figure_path": "jCMYIUwprx/tables/tables_23_2.jpg", "caption": "Table 10: Test results of CyberSecEval-1 - Insecure Coding Practice (Instruction): we reported the % output codes that are considered secure (determined by a rule-based detector). Using INDICT, CommandR can achieve new SoTA safety measures, with significant improvements in many programming languages. The results of the baseline models are from Bhatt et al. [2023].", "description": "This table presents the results of the Insecure Coding Practice (Instruction) task from the CyberSecEval-1 benchmark. It shows the percentage of secure code generated by different LLMs, both with and without the INDICT framework.  The results highlight the improvement in code security achieved by INDICT, particularly with the CommandR model, exceeding the state-of-the-art (SoTA) in many programming languages.", "section": "4.1 Insecure coding practice tasks"}, {"figure_path": "jCMYIUwprx/tables/tables_23_3.jpg", "caption": "Table 1: We compared INDICT and related methods from 3 directions: self-refine/self-critic, multi-agent, and finetuning. Compared to these methods, INDICT is a more well-rounded generation framework with the following contributions: (1) integrates code execution-based feedback and enhances them with external knowledge, (2) targets both helpfulness and safety of output code, and (3) facilitates an interactive and supervision-free multi-agent collaboration framework. Our experiment results showcase the efficacy of INDICT. See Appendix D for cited references.", "description": "This table compares INDICT with other existing methods categorized into three groups: self-refine/self-critic methods, multi-agent methods, and finetuning methods.  The comparison highlights INDICT's advantages, including its integration of code execution feedback, enhanced external knowledge, focus on both helpfulness and safety, and a supervision-free, interactive multi-agent collaboration framework.  The table uses checkmarks to indicate the presence or absence of key features in each method, ultimately demonstrating INDICT's superior capabilities.", "section": "Related Work"}, {"figure_path": "jCMYIUwprx/tables/tables_24_1.jpg", "caption": "Table 1: We compared INDICT and related methods from 3 directions: self-refine/self-critic, multi-agent, and finetuning. Compared to these methods, INDICT is a more well-rounded generation framework with the following contributions: (1) integrates code execution-based feedback and enhances them with external knowledge, (2) targets both helpfulness and safety of output code, and (3) facilitates an interactive and supervision-free multi-agent collaboration framework. Our experiment results showcase the efficacy of INDICT. See Appendix D for cited references.", "description": "This table compares INDICT with other code generation methods categorized into self-refine, multi-agent, and fine-tuning approaches.  It highlights INDICT's advantages in incorporating code execution feedback, utilizing external knowledge, and focusing on both code helpfulness and safety through a unique multi-agent, supervision-free framework.", "section": "Related Work"}, {"figure_path": "jCMYIUwprx/tables/tables_24_2.jpg", "caption": "Table 1: We compared INDICT and related methods from 3 directions: self-refine/self-critic, multi-agent, and finetuning. Compared to these methods, INDICT is a more well-rounded generation framework with the following contributions: (1) integrates code execution-based feedback and enhances them with external knowledge, (2) targets both helpfulness and safety of output code, and (3) facilitates an interactive and supervision-free multi-agent collaboration framework. Our experiment results showcase the efficacy of INDICT. See Appendix D for cited references.", "description": "This table compares INDICT with other related methods across three categories: self-refine/self-critic, multi-agent, and finetuning.  It highlights INDICT's advantages, namely its comprehensive approach that integrates code execution feedback, leverages external knowledge, and focuses on both helpfulness and safety.  The table uses checkmarks to indicate the presence or absence of key features in each method, demonstrating INDICT's superior capabilities.", "section": "Related Work"}]