[{"figure_path": "YO6GVPUrKN/figures/figures_1_1.jpg", "caption": "Figure 1: Adversarial initialization is a failure mode for PH dimension-based generalization measures. Training models from an adversarial initialization leads to higher accuracy gap than for models trained from random initialization. Both PH dimensions fail to correctly attribute higher values to the poorly generalizing models on FCN-5 MNIST and CNN CIFAR-10.", "description": "This figure shows the impact of adversarial initialization on the performance of persistent homology (PH) dimension-based generalization measures.  It compares models trained with standard random initialization versus those initialized adversarially.  The results reveal that adversarially initialized models exhibit a larger accuracy gap (the difference between training and test accuracy), indicating poorer generalization.  Furthermore, the PH dimensions (both Euclidean and loss-based) fail to accurately reflect this poor generalization, assigning them similar or even lower dimensions compared to the randomly initialized models. This highlights a limitation of using PH dimensions as a measure of generalization in scenarios with adversarial initializations.", "section": "Adversarial Initialization"}, {"figure_path": "YO6GVPUrKN/figures/figures_4_1.jpg", "caption": "Figure 1: Adversarial initialization is a failure mode for PH dimension-based generalization measures. Training models from an adversarial initialization leads to higher accuracy gap than for models trained from random initialization. Both PH dimensions fail to correctly attribute higher values to the poorly generalizing models on FCN-5 MNIST and CNN CIFAR-10.", "description": "This figure shows the results of an experiment comparing the performance of models trained with adversarial initialization versus random initialization. The x-axis represents the accuracy gap (difference between training and testing accuracy), and the y-axis represents the persistent homology (PH) dimension. The results demonstrate that adversarial initialization leads to a higher accuracy gap, indicating poorer generalization.  Furthermore, the PH dimension fails to correctly capture this difference in generalization, highlighting a limitation of using PH dimension as a measure of generalization.", "section": "Adversarial Initialization"}, {"figure_path": "YO6GVPUrKN/figures/figures_7_1.jpg", "caption": "Figure 3: Diagram of causal relationships under investigation in the conditional independence test. In H0 the PH dimension is conditionally independent of PH dimension given learning rate and there is no direct causal relationship between these variables. In H\u2081 generalization gap is conditionally dependent of the PH dimension indicating a causal relationship may exist.", "description": "This figure shows two causal diagrams representing two different hypotheses about the relationship between learning rate, PH dimension, and generalization gap.  H0 represents the null hypothesis, where learning rate influences PH dimension, but there's no direct causal link between PH dimension and generalization.  H1 is the alternative hypothesis; learning rate influences PH dimension, which in turn directly influences the generalization gap. This figure helps visually explain the conditional independence test used in the paper to determine which hypothesis better fits the data.", "section": "4.3 Conditional Independence"}, {"figure_path": "YO6GVPUrKN/figures/figures_8_1.jpg", "caption": "Figure 4: Model-wise double descent manifests in Euclidean PH dimension, whilst neither PH dimension correlates with generalization gap in this setting. Test accuracy, generalization gap, and PH dimensions for range of CNN widths. The double descent behavior is clearly visible in test accuracy and Euclidean PH dimension, but the generalization gap is monotonic in this critical region. Mean of three seeds with standard deviation shaded.", "description": "This figure displays the results of an experiment on model-wise double descent using a CNN trained on CIFAR-100 with varying widths.  The top panel shows the test accuracy, the middle panel shows the generalization gap, and the bottom panel shows the PH dimensions (Euclidean and loss-based).  The key observation is the double descent behavior in test accuracy and Euclidean PH dimension, while the generalization gap remains monotonic.  Loss-based PH dimension shows less clear correlation with the other trends.", "section": "Model-Wise Double Descent"}, {"figure_path": "YO6GVPUrKN/figures/figures_13_1.jpg", "caption": "Figure 5: Vietoris\u2013Rips filtration over two noisy circles (with 30 and 15 points each) at 4 different filtration values; and corresponding persistence barcode and diagram (0-dimensional PH in red, 1-dimensional PH in blue). Images produced using GUDHI [The GUDHI Project, 2020].", "description": "This figure shows an example of Vietoris-Rips filtration applied to a point cloud representing two noisy circles.  The filtration is shown at four different scales, progressing from isolated points (0.28) to connected components (0.76) that eventually merge into a single connected component. The corresponding persistence barcode and persistence diagram visualize the topological features (connected components, holes) that emerge and disappear as the filtration progresses, providing a concise summary of the point cloud's topology. The barcode's bars represent topological features, and their lengths indicate persistence.", "section": "A Persistent Homology and Vietoris\u2013Rips Filtrations"}, {"figure_path": "YO6GVPUrKN/figures/figures_18_1.jpg", "caption": "Figure 1: Adversarial initialization is a failure mode for PH dimension-based generalization measures. Training models from an adversarial initialization leads to higher accuracy gap than for models trained from random initialization. Both PH dimensions fail to correctly attribute higher values to the poorly generalizing models on FCN-5 MNIST and CNN CIFAR-10.", "description": "This figure shows the impact of adversarial initialization on the performance of persistent homology (PH) dimension-based generalization measures.  It compares models trained with adversarial initialization against models with random initialization across three different network architectures and datasets (FCN-5 MNIST, CNN CIFAR-10, and AlexNet CIFAR-10). The results indicate that adversarial initialization leads to a larger accuracy gap (difference between training and test accuracy), signifying poorer generalization.  Importantly, the PH dimensions fail to accurately reflect this poorer generalization; they don't assign higher PH dimension values to the poorly generalizing adversarial models, contradicting the expected correlation between PH dimension and generalization.", "section": "Adversarial Initialization"}, {"figure_path": "YO6GVPUrKN/figures/figures_19_1.jpg", "caption": "Figure 1: Adversarial initialization is a failure mode for PH dimension-based generalization measures. Training models from an adversarial initialization leads to higher accuracy gap than for models trained from random initialization. Both PH dimensions fail to correctly attribute higher values to the poorly generalizing models on FCN-5 MNIST and CNN CIFAR-10.", "description": "This figure shows the accuracy gap for three different model architectures (FCN-5 MNIST, CNN CIFAR-10, and Alexnet CIFAR-10) trained with both standard (random) and adversarial initializations.  The accuracy gap is plotted against the persistent homology (PH) dimension, a measure of the fractal dimension of the model's optimization trajectory. The results indicate that adversarial initialization leads to larger accuracy gaps compared to random initialization. Importantly, the PH dimension does not consistently reflect the generalization performance, failing to assign higher dimensions to models with larger accuracy gaps when trained with adversarial initialization.", "section": "Adversarial Initialization"}]