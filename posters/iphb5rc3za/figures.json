[{"figure_path": "IpHB5RC3za/figures/figures_1_1.jpg", "caption": "Figure 1: In the context of online streaming perception, the environment changes during inference.", "description": "The figure illustrates the challenges of real-time online streaming perception.  The left shows stereo images at time t. These are fed to a processing unit (CPU), which takes a time \u0394t to compute. During this computation time \u0394t, the real-world scene changes (right side).  The predictions made at time t based on the input at time t (bottom left) are misaligned with the actual scene at time t+\u0394t because of the computation delay. The misalignment is clearly shown for vehicles and pedestrians.", "section": "1 Introduction"}, {"figure_path": "IpHB5RC3za/figures/figures_1_2.jpg", "caption": "Figure 2: Illustration of the challenges.", "description": "This figure illustrates the challenges in streaming perception, specifically the misalignment between future supervisory signals and current features, implicit supervision when only using the ground truth of a single future frame, and the effective utilization of context information embedded in the combined features.  The left panel (a) shows a scenario with low relative velocity, where the misalignment between prediction and ground truth is relatively small. The right panel (b) shows a scenario with high relative velocity, where the misalignment is significantly larger, exacerbating the challenge for the model to accurately predict the future state of moving objects.", "section": "Challenges"}, {"figure_path": "IpHB5RC3za/figures/figures_3_1.jpg", "caption": "Figure 3: The architecture of StreamDSGN pipeline. (a) The feature extractor retrieves features from streaming stereo image pairs {(If, If)|t = 1, ..., n \u2212 1} and flattens them into BEV features. (b) The depth regression component utilizes Gdepth as the supervision. (c) The BEV detector predicts the object state of the next moment by merging features from the current and previous frames. (d) The Feature-Flow Fusion generates a pseudo-next feature Fpseudo by extrapolating from past features and then concatenates it with the existing historical feature set {Ft, Ft-1}.", "description": "This figure illustrates the architecture of the StreamDSGN pipeline, which is a real-time stereo-based 3D object detection framework designed for streaming perception. It shows four key components: (a) feature extraction, (b) depth regression, (c) BEV detection, and (d) feature-flow fusion. The pipeline leverages historical information to predict the 3D properties of objects in the next moment, addressing challenges associated with streaming perception such as misalignment between features and ground truth.", "section": "3 Our Approaches"}, {"figure_path": "IpHB5RC3za/figures/figures_4_1.jpg", "caption": "Figure 4: A toy example of pseudo-next feature generation.", "description": "This figure illustrates the process of generating pseudo-next features using Feature-Flow Fusion (FFF).  It starts with two feature maps, Ft and Ft-1, representing features from the current and previous frames, respectively.  The FFF method computes a similarity volume by shifting Ft-1 relative to Ft and calculating the similarity at each shift.  The flow map Ft\u2192t\u22121 is then obtained by finding the maximum similarity for each pixel in Ft. This flow map indicates the displacement of features between frames. Using this flow map, the current frame's features (Ft) are warped to align with the ground truth of the next frame (t+1) using an inverse warping operation. The resulting warped features are referred to as pseudo-next features (Fpseudo t+1) which are then used in conjunction with historical features.", "section": "3.2 Feature-Flow Fusion (FFF)"}, {"figure_path": "IpHB5RC3za/figures/figures_5_1.jpg", "caption": "Figure 5: Illustration of MCL.", "description": "This figure illustrates the Motion Consistency Loss (MCL) which is a supplementary regression supervision method for improving the accuracy of object detection in streaming perception.  It leverages historical motion trajectories to guide the prediction of the next frame. The figure shows how the velocity and acceleration loss are calculated based on the correspondence between the ground truth and predicted bounding boxes across different time steps.  Specifically, it shows how the displacement vector and the sine difference of the rotation angles are used to calculate the velocity loss and how the velocity change is used to calculate the acceleration loss.", "section": "3.3 Motion Consistency Loss (MCL)"}, {"figure_path": "IpHB5RC3za/figures/figures_6_1.jpg", "caption": "Figure 6: Illustration of LKBB.", "description": "This figure shows the architecture of the Large Kernel BEV Backbone (LKBB) used in the StreamDSGN model.  (a) depicts the architecture of a single VAN (Visual Attention Network) block, highlighting its components: large kernel attention, feed-forward network, residual connections, and element-wise multiplication.  (b) illustrates how multiple VAN blocks are stacked to form the LKBB, along with the inclusion of transpose convolutions and upsampling to generate multi-scale feature fusion. The LKBB is designed to increase the receptive field and improve the model's capacity to capture long-range dependencies in the bird's-eye view (BEV) representation.", "section": "3.4 Large Kernel BEV Backbone (LKBB)"}, {"figure_path": "IpHB5RC3za/figures/figures_8_1.jpg", "caption": "Figure 7: Qualitative analysis of different relative velocity scenarios. We visualize the predictions in point clouds for a clearer comparison. The first row describes DSGN++ [7] without any modifications. The second row integrates real-time optimization and fusion of historical frames to predict the next frame. The third row showcases our three enhancement strategies. Ground truth and prediction instances are respectively delineated by red and green bounding boxes, with lines inside the boxes indicating the orientation of objects.", "description": "This figure compares the 3D object detection results of three different methods in various scenarios with different relative velocities between the ego vehicle and other objects.  The top row shows the baseline DSGN++ method, which struggles with accuracy when objects are moving relative to the camera. The middle row shows the improved pipeline, which utilizes real-time optimization and fusion of historical frames for prediction. The bottom row displays the results from the proposed method that incorporates three additional enhancement strategies (Feature-Flow Fusion, Motion Consistency Loss, and Large Kernel BEV Backbone) leading to improved accuracy and alignment, particularly in high relative velocity scenarios.", "section": "4.3 Ablation Studies"}, {"figure_path": "IpHB5RC3za/figures/figures_9_1.jpg", "caption": "Figure 8: Qualitative analysis of the pseudo-next feature. The first row displays complete feature maps from different time steps, while the second row shows corresponding local regions of the feature maps. The solid red box and dashed box respectively represent the ground truth of the next frame used for supervision and the locally zoomed-in area.", "description": "This figure provides a qualitative analysis of the pseudo-next feature generation process in StreamDSGN. The top row shows complete feature maps from previous, current, next, and pseudo-next frames.  The bottom row zooms into specific regions of interest within those maps. Red boxes highlight the ground truth location for the next frame (used as supervision during training), while dashed boxes indicate the zoomed region. The comparison helps visualize how well the pseudo-next feature aligns with the actual next frame's ground truth, demonstrating the effectiveness of the Feature-Flow Fusion (FFF) method in aligning features across time steps.", "section": "Qualitative Analysis"}]