[{"figure_path": "Pa8jsrdOnU/tables/tables_6_1.jpg", "caption": "Table 1: The quantitative comparisons of fine-tuning methods with three evaluation metrics. The number of parameters are the ones held in GPU memory during fine-tuning stage. The results are obtained by averaging over four runs with different seeds (standard deviation is added in a small-sized text).", "description": "This table presents a quantitative comparison of different fine-tuning methods for text-to-image diffusion models, focusing on three evaluation metrics: DINO, CLIP-I, and CLIP-T.  It shows the number of parameters used (both base model and LoRA parameters), the peak GPU memory usage during training, and the performance improvement compared to inference-only memory usage.  Results are averaged over four runs with different random seeds to demonstrate robustness.  The methods compared are Full Fine-Tuning (Full FT), LoRA Fine-Tuning with ranks of 128 and 1, and the proposed Hollowed Net method.", "section": "5.2 Results"}, {"figure_path": "Pa8jsrdOnU/tables/tables_6_2.jpg", "caption": "Table 2: Human evaluation results", "description": "This table presents the results of a human evaluation comparing the subject and text fidelity of images generated using Hollowed Net and LoRA FT.  The evaluation was conducted using a pairwise comparison task where human participants judged the quality of images generated by each method. The results show that Hollowed Net performs comparably to LoRA FT, achieving a higher subject fidelity but a lower text fidelity. The \"Tie\" row indicates results when participants couldn't distinguish between the two methods.", "section": "5.2 Results"}, {"figure_path": "Pa8jsrdOnU/tables/tables_6_3.jpg", "caption": "Table 1: The quantitative comparisons of fine-tuning methods with three evaluation metrics. The number of parameters are the ones held in GPU memory during fine-tuning stage. The results are obtained by averaging over four runs with different seeds (standard deviation is added in a small-sized text).", "description": "This table presents a quantitative comparison of different fine-tuning methods for text-to-image diffusion models.  It compares full fine-tuning (Full FT), LoRA fine-tuning with rank 128 and rank 1 (LORA FT), and the proposed Hollowed Net method.  For each method, the table shows the number of parameters, the peak GPU memory usage during training, and the percentage increase in memory usage compared to inference. It also includes three evaluation metrics (DINO, CLIP-I, CLIP-T) and their standard deviation across four runs with different random seeds.", "section": "5. Experiments"}, {"figure_path": "Pa8jsrdOnU/tables/tables_12_1.jpg", "caption": "Table 4: Quantitative results of BK-SDM on the DreamBooth dataset [5].", "description": "This table presents quantitative results obtained using BK-SDM (a layer-pruned Stable Diffusion model) on the DreamBooth dataset. It compares the performance metrics of two different BK-SDM models: BK-SDM-Base and BK-SDM-Small, showing the number of parameters, training memory usage, and the scores for DINO (subject fidelity), CLIP-I (subject fidelity), and CLIP-T (text fidelity) metrics.  The results highlight a tradeoff between model size, memory consumption, and performance.", "section": "A Experiments with Layer-Pruned Diffusion Models"}, {"figure_path": "Pa8jsrdOnU/tables/tables_13_1.jpg", "caption": "Table 5: Quantitative results of LORA FT and Hollowed Net with different ranks", "description": "This table presents a quantitative comparison of the performance of LoRA fine-tuning (LORA FT) and Hollowed Net with different ranks (4 and 16) using the DreamBooth dataset.  The table shows the number of parameters, training memory used, and evaluation metrics (DINO, CLIP-I, CLIP-T).  The results demonstrate the impact of the rank on performance and memory efficiency of both methods, highlighting Hollowed Net's ability to achieve comparable performance with significantly less memory.", "section": "5.3 Results"}, {"figure_path": "Pa8jsrdOnU/tables/tables_13_2.jpg", "caption": "Table 1: The quantitative comparisons of fine-tuning methods with three evaluation metrics. The number of parameters are the ones held in GPU memory during fine-tuning stage. The results are obtained by averaging over four runs with different seeds (standard deviation is added in a small-sized text).", "description": "This table presents a quantitative comparison of different fine-tuning methods for text-to-image diffusion models.  It shows the number of parameters, GPU memory usage during training, and three evaluation metrics (DINO, CLIP-I, CLIP-T) for each method.  The methods compared include full fine-tuning, LoRA fine-tuning with different ranks (r=128 and r=1), and the proposed Hollowed Net method.  The results are averaged over four runs with different random seeds, with standard deviations included.", "section": "5.2 Results"}]