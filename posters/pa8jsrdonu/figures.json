[{"figure_path": "Pa8jsrdOnU/figures/figures_2_1.jpg", "caption": "Figure 1: The LoRA personalization with Hollowed Net for resource-constrained environments. The input image is from the DreamBooth dataset [5].", "description": "This figure illustrates the LoRA personalization process using Hollowed Net, designed for resource-constrained environments.  It is divided into two stages: pre-computing and fine-tuning. In the pre-computing stage, a forward pass is performed through the original diffusion U-Net with an input image and text prompt. The intermediate activations (\u03b81-\u03b87) are saved for later use. The fine-tuning stage uses a modified version of the U-Net called the 'Hollowed Net', where the middle layers are temporarily removed. This significantly reduces the memory footprint during training. The pre-computed activations are loaded into the Hollowed Net, and LoRA parameters are fine-tuned.  After fine-tuning, these LoRA parameters are seamlessly transferred back to the original U-Net for inference without any additional memory overhead.", "section": "4 Methodology"}, {"figure_path": "Pa8jsrdOnU/figures/figures_4_1.jpg", "caption": "Figure 1: The LoRA personalization with Hollowed Net for resource-constrained environments. The input image is from the DreamBooth dataset [5].", "description": "This figure illustrates the LoRA personalization process using Hollowed Net, designed for resource-constrained environments.  It shows a two-stage process: pre-computing and fine-tuning.  In the pre-computing stage, intermediate activations from a pre-trained diffusion U-Net are saved. Then, in the fine-tuning stage, a modified U-Net (Hollowed Net), with some layers temporarily removed, is fine-tuned using the pre-computed activations.  The LoRA (Low-Rank Adaptation) parameters are learned only for the trainable parts of Hollowed Net.  Finally, these learned parameters are transferred back to the original U-Net, enabling inference with minimal additional memory overhead. The example input image is from the DreamBooth dataset.", "section": "4 Methodology"}, {"figure_path": "Pa8jsrdOnU/figures/figures_5_1.jpg", "caption": "Figure 1: The LoRA personalization with Hollowed Net for resource-constrained environments. The input image is from the DreamBooth dataset [5].", "description": "This figure illustrates the LoRA personalization process using Hollowed Net, which is designed for resource-constrained environments.  The process is broken down into two stages: pre-computing and fine-tuning. During pre-computing, the original U-Net processes the input image to generate intermediate activations, which are saved to data storage. Then, fine-tuning uses these pre-computed activations to train the Hollowed Net, a modified U-Net with some central layers temporarily removed (the \"hollowed\" structure). Only the LoRA parameters (small, trainable parameters for efficient fine-tuning) are updated in the Hollowed Net. Finally, the personalized LoRA parameters from the Hollowed Net are transferred back to the original U-Net for inference. This approach drastically reduces the need for large memory during training and maintains the high performance of the original U-Net during inference.", "section": "4 Methodology"}, {"figure_path": "Pa8jsrdOnU/figures/figures_7_1.jpg", "caption": "Figure 1: The LoRA personalization with Hollowed Net for resource-constrained environments. The input image is from the DreamBooth dataset [5].", "description": "This figure illustrates the LoRA personalization process using Hollowed Net.  It shows a two-stage process: pre-computing intermediate activations from the original U-Net, and then fine-tuning LoRA parameters using a \"hollowed\" version of the U-Net (with some middle layers temporarily removed to save memory).  The fine-tuned LoRA parameters are then transferred back to the original U-Net for inference without additional memory overhead. The example image is from the DreamBooth dataset.", "section": "4 Methodology"}, {"figure_path": "Pa8jsrdOnU/figures/figures_8_1.jpg", "caption": "Figure 5: Analysis of different fractions of hollowed layers. For all figures, the x-axis represents the fractions of layers removed from the pre-trained diffusion U-Net. The y-axis corresponds to the metric used for each figure.", "description": "The figure shows the effect of varying the fraction of hollowed layers in Hollowed Net on peak GPU memory usage, DINO score, CLIP-I score, and CLIP-T score.  The x-axis represents the percentage of layers removed, while the y-axis shows the performance metric.  The results indicate a trade-off between memory efficiency and performance.  As the fraction of hollowed layers increases, memory usage decreases, but performance metrics may also decrease, particularly subject fidelity.  However, the model maintains relatively good performance even with a significant number of layers removed.", "section": "5.3 Ablation Study on Fractions of Hollowed Layers"}, {"figure_path": "Pa8jsrdOnU/figures/figures_9_1.jpg", "caption": "Figure 6: Qualitative results with different fractions of hollowed layers given three subjects from the DreamBooth dataset [5].", "description": "This figure shows the qualitative results of applying Hollowed Net with varying fractions of hollowed layers (from small to large). Three example subjects from the DreamBooth dataset are used. For each subject, the original image and several generated images are shown, each corresponding to a different fraction of hollowed layers.  The goal is to demonstrate how the removal of layers (creating the 'hollow' in Hollowed Net) impacts the quality of the generated images, showing that too much removal significantly degrades the results but a moderate removal yields good results with reduced memory usage.", "section": "5.3 Ablation Study on Fractions of Hollowed Layers"}, {"figure_path": "Pa8jsrdOnU/figures/figures_12_1.jpg", "caption": "Figure 1: The LoRA personalization with Hollowed Net for resource-constrained environments. The input image is from the DreamBooth dataset [5].", "description": "This figure illustrates the LoRA personalization method using Hollowed Net for resource-constrained environments.  The process is divided into two stages: pre-computing and fine-tuning.  In the pre-computing stage, intermediate activations from a pre-trained diffusion U-Net are saved. Then, the fine-tuning stage uses a modified U-Net (Hollowed Net), where some layers are temporarily removed, to train LoRA parameters using the pre-computed activations. Finally, the trained LoRA parameters are transferred back to the original U-Net for inference, significantly reducing memory requirements.  An example input image from the DreamBooth dataset is shown to demonstrate the personalization process.", "section": "4 Methodology"}, {"figure_path": "Pa8jsrdOnU/figures/figures_13_1.jpg", "caption": "Figure 5: Analysis of different fractions of hollowed layers. For all figures, the x-axis represents the fractions of layers removed from the pre-trained diffusion U-Net. The y-axis corresponds to the metric used for each figure.", "description": "This figure analyzes the impact of varying the fraction of hollowed layers in the Hollowed Net model.  It shows the relationship between the percentage of layers removed from the U-Net and four different metrics: peak GPU memory usage (GB), DINO score, CLIP-I score, and CLIP-T score.  Each metric assesses a different aspect of model performance after personalization. The results demonstrate the trade-off between memory efficiency and model performance as more layers are removed.  The findings help determine an optimal balance, where sufficient layers are maintained for acceptable performance while maximizing memory savings.", "section": "5.3 Ablation Study on Fractions of Hollowed Layers"}, {"figure_path": "Pa8jsrdOnU/figures/figures_14_1.jpg", "caption": "Figure 1: The LoRA personalization with Hollowed Net for resource-constrained environments. The input image is from the DreamBooth dataset [5].", "description": "This figure illustrates the LoRA personalization process using Hollowed Net, designed for resource-constrained environments.  It shows a two-stage process: (1) pre-computing, where intermediate activations from a pre-trained diffusion U-Net are saved; and (2) fine-tuning, where a layer-pruned version of the U-Net (Hollowed Net) is trained using the pre-computed activations to learn LoRA parameters. The LoRA parameters are then transferred to the original U-Net for inference, requiring minimal additional memory. The input image example is from the DreamBooth dataset.", "section": "4 Methodology"}, {"figure_path": "Pa8jsrdOnU/figures/figures_14_2.jpg", "caption": "Figure 1: The LoRA personalization with Hollowed Net for resource-constrained environments. The input image is from the DreamBooth dataset [5].", "description": "This figure illustrates the LoRA personalization process using Hollowed Net, a method designed for resource-constrained environments. It shows a two-stage process: pre-computing and fine-tuning. In pre-computing, intermediate activations from the original diffusion U-Net are saved for later use in fine-tuning. During fine-tuning, a modified U-Net (Hollowed Net), where some middle layers are temporarily removed, is trained using these pre-computed activations. This reduces memory consumption during training. Finally, the learned LoRA (Low-Rank Adaptation) parameters are transferred back to the original U-Net, allowing inference without additional memory overhead. The input image is an example from the DreamBooth dataset, a popular dataset for subject-driven image generation.", "section": "4 Methodology"}]