[{"type": "text", "text": "Absorb & Escape: Overcoming Single Model Limitations in Generating Genomic Sequences ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zehui Li1,\\*, Yuhao $\\mathbf{Ni^{1}}$ , Guoxuan Xia1, William Beardall1, Akashaditya Das1, Guy-Bart Stan1,\\*, Yiren Zhao1,\\* ", "page_idx": 0}, {"type": "text", "text": "1Imperial College London, {zehui.li22, harry.ni21, g.xia21, william.beardall15, akashaditya.das13, g.stan, a.zhao}@imperial.ac.uk \\*Correspondence: {zehui.li22, g.stan, a.zhao}@imperial.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent advances in immunology and synthetic biology have accelerated the development of deep generative methods for DNA sequence design. Two dominant approaches in this field are AutoRegressive (AR) models and Diffusion Models (DMs). However, genomic sequences are functionally heterogeneous, consisting of multiple connected regions (e.g., Promoter Regions, Exons, and Introns) where elements within each region come from the same probability distribution, but the overall sequence is non-homogeneous. This heterogeneous nature presents challenges for a single model to accurately generate genomic sequences. In this paper, we analyze the properties of AR models and DMs in heterogeneous genomic sequence generation, pointing out crucial limitations in both methods: (i) AR models capture the underlying distribution of data by factorizing and learning the transition probability but fail to capture the global property of DNA sequences. (ii) DMs learn to recover the global distribution but tend to produce errors at the base pair level. To overcome the limitations of both approaches, we propose a post-training sampling method, termed Absorb & Escape (A&E) to perform compositional generation from AR models and DMs. This approach starts with samples generated by DMs and refines the sample quality using an AR model through the alternation of the Absorb and Escape steps. To assess the quality of generated sequences, we conduct extensive experiments on 15 species for conditional and unconditional DNA generation. The experiment results from motif distribution, diversity checks, and genome integration tests unequivocally show that $\\mathsf{A}\\mathsf{\\&E}$ outperforms state-of-the-art AR models and DMs in genomic sequence generation. A&E does not suffer from the slowness of traditional MCMC to sample from composed distributions with Energy-Based Models whilst it obtains higher quality samples than single models. Our research sheds light on the limitations of current single-model approaches in DNA generation and provides a simple but effective solution for heterogeneous sequence generation. Code is available at the Github Repo1. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "DNA sequences, as the blueprint of life, encode proteins and RNAs and directly interact with these molecules to regulate biological activities within cells. The success of deep generative models in image [28], text [27], and protein design [35] has drawn the attention of deep learning researchers to the problem of DNA design, i.e. applying these models to genomic sequence generation [4, 34, 38]. However, one rarely explored issue is how well existing methods can handle the unique property of DNA sequences: heterogeneity. DNA sequences are highly heterogeneous, consisting of multiple ", "page_idx": 0}, {"type": "image", "img_path": "XHTl2k1LYk/tmp/eb938f3cb8129626e9c0737c293107e2936e252c16cd8dd6f2ad015acaacc3c1.jpg", "img_caption": ["(a) A DNA generated by A&E, interacting with TATA-binding protein. The DNA sequences highlighted in magenta are the TATA-box motif. The confirmation is predicted by AlphaFold 3 [1]: the DNA bends at the TATA-box position. "], "img_footnote": [], "page_idx": 1}, {"type": "image", "img_path": "XHTl2k1LYk/tmp/809f3ebd20f8c6e326fb0c4140c4ffe728ac776359697b0e5bbd16a9c83c4d3a.jpg", "img_caption": ["(b) The proposed framework Fast Absorb & Escape (Fast A&E): The DM and AR models jointly optimize a given sequence by alternating between the A-step and the E-step. See Section 4 for a detailed explanation. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: (a) Generated DNA interacting with TATA-binding protein. (b) Proposed A&E framework. ", "page_idx": 1}, {"type": "text", "text": "connected functional regions (e.g. Promoter Regions, Exons, and Introns) in sequential order. While elements within each functional region might be homogeneous (coming from the same distribution), the overall sequence is non-homogeneous. This heterogeneity, along with the discrete nature of genomic sequences, poses challenges to popular deep generative methods. ", "page_idx": 1}, {"type": "text", "text": "Limitations of Existing Single-Model Approaches in Generating Genomic Sequences AutoRegressive Models (AR) [8, 17, 24] are one of the most dominant approaches for discrete sequence generation. To model the data distribution of a sequence $x$ of length $T$ , the probability of $x$ is factorized as: ", "page_idx": 1}, {"type": "equation", "text": "$$\np^{A R}(\\mathbf{x})=\\prod_{i=1}^{T}p_{\\boldsymbol{\\theta}}(x_{i}|x_{1},x_{2},...\\,,x_{i-1}).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "An issue arises when modeling heterogeneous data, where the value of $\\theta$ may vary significantly from one segment to another. Additionally, AR models assume a dependency between the current element and previous elements; this assumption may not hold true for heterogeneous sequences, potentially hindering the learning process (see Section 3 for details). ", "page_idx": 1}, {"type": "text", "text": "On the other hand, Diffusion Models (DMs), initially proposed by [30], have been dominant in image generation. In the probabilistic denoising view [16], DMs gradually add noise to the input data $x_{0}$ , and a reverse diffusion (generative) process is trained to gradually remove the noise from the perturbed data $x_{t}$ . DMs directly model the data distribution without AutoRegressive factorization, thereby avoiding the issues associated with AR models. However, it has been shown that DMs are less competent than AR models for discrete data generation [21, 36]. When it comes to modeling heterogeneous genomic sequences, it remains unclear how the performance of DMs compares to AR models within each homogeneous segment. ", "page_idx": 1}, {"type": "text", "text": "Model Composition As a Solution Balancing the ability of generative algorithms to capture both local and global properties of the data distribution is central to the problem. An obvious solution could be to combine these two types of models and perform generation using the composed models. However, this typically requires converting these two models into an Energy-Based Model and then sampling using Markov Chain Monte Carlo (MCMC), which can be inherently slow due to the sampling nature of the algorithm [10], and the potential long inference time of individual models. With the goal of accurate and efficient DNA generation, we aim to investigate two key questions in this work: (i) How well does a single AR model or DM perform in DNA generation, given the heterogeneous nature of genomic sequences? (ii) Is there an efficient algorithm to combine the beneftis of AR models and DMs, outperforming a single model? In answering these two questions, our contribution is three-fold: ", "page_idx": 1}, {"type": "text", "text": "(a) We study the properties of AR models and DMs in heterogeneous sequence generation through theoretical and empirical analysis (Section 3). ", "page_idx": 1}, {"type": "text", "text": "(b) We design the theoretical framework Absorb & Escape (A&E) to sample from the compositional distribution of an AR model and a DM (Section 4.1). Furthermore, as shown in Figure 1b, we propose an efficient post-training sampling algorithm termed Fast A&E to sample from the composed model, requiring at most one forward pass through the pretrained DM and AR model (Section 4.2). ", "page_idx": 2}, {"type": "text", "text": "(c) We design a comprehensive evaluation workflow for DNA generation, assessing the sequence composition, diversity, and functional properties of generated genomic sequences. Extensive experiments (15 species, 6 recent DMs, 1 AR model, and 3 types of evaluations) reveal: 1) the limitations of existing models in DNA generation (Section 5.3), and 2) that the proposed algorithm Fast A&E consistently outperforms state-of-the-art models as measured by motif distribution and functional property similarity to natural DNA sequences (Sections 4.2 and 5.3). ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries and Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Problem Formulation: DNA Generation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "DNA generation aims to produce synthetic sequences that functionally approximate real DNA sequences. Formally, let $\\mathbb{N}_{4}=\\{1,2,3,4\\}$ , where each element represents one of the four nucleotides: adenine (A), thymine (T), guanine $(\\mathbf{G})$ , and cytosine (C). A DNA sequence of length $L$ can be represented as $\\dot{\\mathbf{x}}\\in\\mathbb{N}_{4}^{L}$ , with each element/nucleotide denoted by $\\mathbf{x}_{1},\\mathbf{x}_{2},\\cdots,\\mathbf{x}_{L}$ . ", "page_idx": 2}, {"type": "text", "text": "Unconditional Generation: Given a dataset of real-world DNA sequences $\\boldsymbol{\\mathcal{X}}~=~\\{\\mathbf{x}^{(n)}\\}_{n=1}^{N}$ collected from some distribution $p(\\mathbf{x})$ , where each sequence $\\mathbf{x}^{(n)}\\,\\in\\,\\mathbb{N}_{4}^{L}$ represents a chain of nucleotides, the objective is to develop a generative model $p_{\\theta}(\\mathbf{x})$ of the data distribution $p(\\mathbf{x})$ from which we can sample novel sequences $\\tilde{\\mathbf{x}}\\sim\\,p_{\\theta}(\\mathbf{x})$ . These sequences should be structured arrangements of A, T, G, and C, reflecting the complex patterns found in actual DNA. Earlier works applying Generative Adversarial Networks (GANs) [13] to generate protein-encoding sequences [14] and functional elements [33, 34, 38] fall into this category. ", "page_idx": 2}, {"type": "text", "text": "Conditional Generation: In this task, the dataset of DNA sequences $\\boldsymbol{\\mathcal{X}}=\\{\\mathbf{x}^{(n)},\\boldsymbol{c}^{(n)}\\}_{n=1}^{N}$ is sampled from the joint distribution $p(\\mathbf{x},c)$ , where $c$ represents the condition associated with each sequence. The objective is to develop a model $p_{\\theta}(\\mathbf{x}|c)$ that generates new DNA sequences $\\tilde{\\bf x}$ given condition $c$ . Recently, a discrete diffusion model DDSM [4] and an AutoRegressive model $\\mathbf{RegML}$ [20] have used expression level as the condition, while DNADiffusion [26] has used cell type as the condition. ", "page_idx": 2}, {"type": "text", "text": "2.2 Homogeneous vs. Heterogeneous Sequences ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Homogeneous Generation Process In the context of sequence generation, a homogeneous Markov Chain is characterized by constant probabilistic rules for generating the sequence at each time step $t$ More generally, a process is defined as homogeneous if the transition probabilities are independent of time $t$ . This means there exists a constant $P_{\\mathbf{c},j}$ such that: ", "page_idx": 2}, {"type": "equation", "text": "$$\nP_{\\mathbf{c},j}=\\operatorname*{Pr}[\\mathbf{x}_{t}=j\\mid\\mathbf{x}_{1:t-1}=\\mathbf{c}]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "holds for all times $t$ , where $P_{\\mathbf{c},j}$ is a constant, and c represents a specific sequence of past values, i.e., $\\mathbf{c}=(c_{1},c_{2},\\ldots,c_{t-1})$ . ", "page_idx": 2}, {"type": "text", "text": "Heterogeneous Generation Process Assuming homogeneous properties simplifies modeling but can be overly restrictive for certain modalities, leading to inaccuracies. For example, DNA sequences consist of various functionally distinct regions, such as promoters, enhancers, regulatory regions, and protein-coding regions scattered across the genome [11]. Each region may be assumed to be homogeneous, but the overall sequence is non-homogeneous due to the differing properties of these elements. ", "page_idx": 2}, {"type": "text", "text": "For sequences like DNA, which consist of locally homogeneous segments, we define them as heterogeneous sequences. Formally, a heterogeneous sequence is defined as follows: Suppose a sequence $\\mathbf{x}$ is divided into segments $S_{1},S_{2},\\ldots,S_{m}$ , where each segment $\\boldsymbol{S}_{i}$ is homogeneous. For each segment ${\\cal S}_{i}=({\\bf x}_{t},{\\bf x}_{t-1},{\\bf x}_{t-2},.\\mathrm{~.~.~},{\\bf x}_{t-k})$ , there exists a constant $P_{\\mathbf{c}_{i},j}$ such that: ", "page_idx": 2}, {"type": "equation", "text": "$$\nP_{\\mathbf{c}_{i},j}=\\mathrm{Pr}[\\mathbf{x}_{t}=j\\mid\\mathbf{x}_{t-k:t-1}=\\mathbf{c}_{i}]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "holds for all times $t$ within segment ${\\mathbf{}}S_{i}$ , where $P_{\\mathbf{c}_{i},j}$ is a constant, and $\\mathbf{c}_{i}=(c_{i,t-1},c_{i,t-2},\\ldots,c_{i,t-k})$ represents values characterizing the history within that segment. While segment $\\mathcal{S}_{i}$ is homogeneous, the entire sequence is non-homogeneous due to the varying properties across different segments. ", "page_idx": 2}, {"type": "image", "img_path": "XHTl2k1LYk/tmp/9d1ba36b02de728e2bf3dba0a80189aa2f82bf86b1238043017f7629a006c3ab.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: A toy example with heterogeneous sequences: (a) The overall training set consists of $N=50,000$ heterogeneous sequences, where each sequence further consists of 16 homogeneous segments. We apply an autoregressive and a diffusion model to learn the underlying distribution. (b) Within each segment, the sequences are generated with a simple Hidden Markov Model (HMM), with deterministic transition probability and emission probability. ", "page_idx": 3}, {"type": "text", "text": "3 Single Model Limitations in Heterogeneous Sequence Generation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "How powerful are AR models and DMs in modelling heterogeneous sequences? We first provide a theoretical analysis, and then perform experiments on synthetic sequences to validate our assumption. ", "page_idx": 3}, {"type": "text", "text": "AutoRegressive (AR) Models Suppose a heterogeneous sequence x consist of two homogeneous segments of length $\\boldsymbol{\\mathrm{k}}$ , then $\\mathbf{x}=\\{\\{\\bar{x_{1}},x_{2},\\cdots,x_{k}\\},\\{x_{k+1},\\bar{x_{k+2}},\\cdots,x_{2k}\\}\\}$ . AR models factorize $p(\\mathbf{x})$ into conditional probability in eq. (4); consider the case where the true factorisation of $p(x)$ follows eq. (5). ", "page_idx": 3}, {"type": "equation", "text": "$$\n^{A R}(\\mathbf{x})=p\\theta(x_{1})p\\theta(x_{2}|x_{1})\\cdot\\cdot\\cdot p_{\\theta}(x_{k}|\\mathbf{x}_{1:k-1})\\cdot p_{\\theta}(x_{k+1}|\\mathbf{x}_{1:k})p_{\\theta}(x_{k+2}|\\mathbf{x}_{1:k+1})\\cdot\\cdot\\cdot p_{\\theta}(x_{2k}|\\mathbf{x}_{1:2k-1})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\np^{d a t a}(\\mathbf{x})=\\underbrace{p_{1}(x_{1})p_{1}(x_{2}|x_{1})\\cdots p_{1}(x_{k}|\\mathbf{x}_{1:k-1})}_{\\mathrm{Segment}\\,1}\\cdot\\underbrace{p_{2}(x_{k+1})p_{2}(x_{k+2}|\\mathbf{x}_{k+1})\\cdots p_{2}(x_{2k}|\\mathbf{x}_{k+1:2k-1})}_{\\mathrm{Segment}\\,2}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "AR factorisation allows the accurate modelling of the first homogeneous segment; however, it may struggle to disassociate the elements of the second segment from the first segment. More precisely, sufficient data is needed for AR model to learn that $p_{\\theta}(x_{k+1}),p_{\\theta}(x_{k+2}),\\cdot\\cdot\\cdot\\,,p_{\\theta}(x_{2k})$ should be independent to the elements $\\{x_{1},x_{2},\\cdot\\cdot\\cdot,x_{k}\\}$ in the first segments. Secondly, when the context length of the AR model is shorter than the sequence length $2k$ , it could struggle to capture the difference between $p_{1}$ and $p_{2}$ with a single set of parameters $\\theta$ . ", "page_idx": 3}, {"type": "text", "text": "Diffusion Models (DMs) On the other hand, DMs estimate the overall probability distribution $p(\\mathbf{x})$ without factorization. The elements of $\\mathbf{x}$ are usually generated in parallel. Thus, they do not suffer from the conditional dependence assumption. However, the removal of the conditional dependence assumption may also decrease the accuracy of generation within each homogeneous segment compared to AR models, as DMs do not explicitly consider previous elements. ", "page_idx": 3}, {"type": "text", "text": "3.1 A Toy Example ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To evaluate the performance of Autoregressive (AR) models and Diffusion Models (DMs) in generating heterogeneous sequences, we consider a toy example with 50,000 heterogeneous sequences X = {x(n)}5n0=010 0. Each sequence contains 16 segments, as illustrated in Figure 2(a), and each segment comprises 16 elements, resulting in a total sequence length of 256 $\\mathbf{\\tilde{(x}\\in\\mathbb{N}_{4}^{256})}$ ). A simple Hidden Markov Model (HMM) is used to generate each segment, as shown in Figure 2(b), with deterministic transition and emission probabilities that ensure homogeneity within each segment. The emitted tokens differ from one segment to another, mimicking the properties of real DNA sequences. Whilst it is possible to use more complex distributions for each segment, doing so could complicate the evaluation of the generated sequences. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Evaluation Under our toy HMM setup, a generative model could make two types of mistakes within each segment: 1) Illegal Start Token: The generated sequence starts with a token which has zero emission probability. E.g. in Figure 2(b), the starting token could only be $\\{A,T\\}$ . $\\{G,C\\}$ at the beginning of the sequence are classified as illegal start tokens. 2) Incorrect Transition: The generated sequence contains tokens with zero transition probability. E.g. in Figure 2(b) given the start of the sequence is $(A,T,A,T)$ , the next token must be $A$ , any other tokens such as $\\{T,G,C\\}$ are classified as incorrect transitions. We use the number of incorrect tokens as the metric for evaluation. ", "page_idx": 4}, {"type": "text", "text": "Experiment We use Hye- Table 1: Number of Incorret Tokens on Synthetic Dataset. The naDNA [20, 24] as the repre- performance metrics used are the number of Illegal Start (IS) sentative autoregressive (AR) Tokens and Incorrect Transition (IT) Tokens. Note that there are a model. For the diffusion model, total of $4,000\\times256=1024,000$ tokens. ", "page_idx": 4}, {"type": "text", "text": "we develop a simple latent Discrete Diffusion model termed DiscDiff. It resembles the design of StableDiffusion [28], a latent diffusion model for image generation. DiscDiff consists of ", "page_idx": 4}, {"type": "table", "img_path": "XHTl2k1LYk/tmp/b398375a60e31d1587058328af8a433ee29a90c453e06ca445e41ee81cecc434.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "a CNN-based Variational Encoder-decoder, trained with cross entropy, to map the discrete DNA data into a latent space, and a standard 2-D UNet as the denoising network (detailed in appendix A). The training dataset consists of $\\mathcal{X}=\\{\\mathbf{x}^{(n)}\\}_{n=1}^{50,000}$ . For detailed training procedures see Appendix B. We generate 4,000 sequences from each model and present the evaluation results in Table 1. As expected, the diffusion model DiscDiff makes fewer errors regarding Illegal Start (IS) tokens but tends to generate more Incorrect Transition (IT) tokens. Conversely, while the AR model HyenaDNA generates some IS token errors, it produces significantly fewer IT token errors. This motivates the question: can we combine the strengths of both algorithms to achieve better sequence generation? ", "page_idx": 4}, {"type": "text", "text": "4 Method ", "text_level": 1, "page_idx": 4}, {"type": "image", "img_path": "XHTl2k1LYk/tmp/82f55e5bc7fc05cfeae43170f6e324d81484e7e4eea289df9ee6e16d21cddb86.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "image", "img_path": "XHTl2k1LYk/tmp/b38d6af4de1aaf7bff1d10a27a7c6f52244243f3e20403c7ca8c7d8207a9fa5c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "4.1 The Absorb & Escape Framework ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Given a pretrained AutoRegressive model $p_{\\theta}^{A R}(\\mathbf{x})$ and a Diffusion Model $p_{\\beta}^{D M}(\\mathbf{x})$ , we aim to generate a higher quality example $\\tilde{\\bf x}$ from the composed distribution $p_{\\theta,\\beta}^{C}(\\mathbf{x})=p_{\\theta}^{A R}(\\mathbf{x})\\circ p_{\\beta}^{D M}(\\mathbf{x})$ . However, directly computing $p_{\\theta,\\beta}^{C}(\\mathbf{x})$ is generally intractable, as both the autoregressive factorizations from $p^{A R}$ and score functions from $p^{D M}$ are not directly composable [9, 10]. We propose the Absorb $\\pmb{\\&}$ Escape (A&E) framework, as shown in Algorithm 1, to efficiently sample from $p_{\\theta,\\beta}^{\\mathbf{\\hat{C}}}(\\mathbf{x})$ . ", "page_idx": 5}, {"type": "text", "text": "Absorb-Step Inspired by Gibbs sampling [12], which iteratively refines each dimension of a single sample and moves to higher density areas, our algorithm starts with a sequence $\\mathbf{x}^{0}\\sim p^{D M}(\\mathbf{\\check{x}})$ , generated by the diffusion model and then refines the samples through the Absorb step and Escape step. By exploiting the heterogeneous nature of the sequence, we assume that $\\mathbf{x}^{0}$ can be factorized into multiple segments $\\{\\mathbf{s}_{1},\\mathbf{s}_{2},\\ldots,\\mathbf{s}_{n}\\}$ . For each segment $\\mathbf{s}_{k}$ , we set $i$ and $j$ as the start and end indices, respectively. During the Absorb step, we sample a subset of segments ${\\mathcal{S}}\\subseteq\\{\\mathbf{s}_{1},\\mathbf{s}_{2},\\ldots,\\mathbf{s}_{n}\\}$ and refine each segment $\\mathbf{s}_{k}$ by sampling $\\tilde{\\mathbf{x}}_{i:j}^{\\bar{\\prime}}\\sim p(\\mathbf{x}_{i:j}|\\mathbf{x}_{0:i-1},\\mathbf{x}_{j+1:L}\\bar{\\mathbf{\\xi}})\\approx p_{\\theta}^{A R}(\\mathbf{x}_{i:j}|\\mathbf{x}_{0:i-1})$ , using the autoregressive model to approximate the conditional probability. ", "page_idx": 5}, {"type": "text", "text": "Escape-Step After refining the segment in the Absorb step, we proceed with the Escape step where we update the refined segment $\\tilde{\\mathbf{x}}_{i:j}^{t}$ to $\\tilde{\\mathbf{x}}_{i:j}^{\\prime}$ . This iterative process continues for each selected segment $\\mathbf{s}_{k}$ , with $t$ incrementing after each update. By leveraging the ability of the diffusion model to capture the overall data distribution and the autoregressive model to refine homogeneous sequences within each segment, our algorithm efficiently improves the quality of the generated samples. The final output $\\tilde{\\mathbf{x}}^{t}$ is hereby closer to the true data distribution $p(\\mathbf x)$ compared to the initial sample $\\tilde{\\mathbf{x}}^{0}$ . A proof for the convergence in Proposition 1 is provided in Appendix C. ", "page_idx": 5}, {"type": "text", "text": "Proposition 1. The Absorb & Escape (A&E) algorithm converges to the target distribution $p_{\\theta,\\beta}^{C}(\\mathbf{x})=$ $p_{\\theta}^{A R}(\\mathbf{x})\\circ p_{\\beta}^{D M}(\\mathbf{x})$ , under the assumptions that both models are properly trained, the segments of x are homogeneous, the subset of segments is chosen randomly, and the conditional distribution $p(\\mathbf{x}_{i:j}|\\mathbf{x}_{0:i-1},\\mathbf{x}_{j+1:L})$ is accurately approximated by $p_{\\theta}^{A R}(\\mathbf{x}_{i:j}|\\dot{\\mathbf{x}}_{0:i-1})$ . ", "page_idx": 5}, {"type": "text", "text": "4.2 Practical Implementation: Fast A&E ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "While $\\mathsf{A}\\mathsf{\\&E}$ offers a method to sample from a compositional distribution, two practical issues remain unresolved. Firstly, the algorithm may take a considerable amount of time to converge. Secondly, a crucial step in Line 3 of Algorithm 1 involves splitting $\\mathbf{x}$ into homogeneous segments $\\{\\mathbf{s_{1}},\\mathbf{s_{2}},\\cdots,\\mathbf{s_{n}}\\}$ and then sampling a subset of these segments. Segmentation is straightforward when the boundaries of functional regions of the DNA sequence are known, such as protein-coding regions, exons, or introns, where each region naturally forms a homogeneous segment. However, this information is often unavailable in practice. ", "page_idx": 5}, {"type": "text", "text": "To address these challenges, we propose a practical implementation termed Fast A&E. For generating a sequence $\\textbf{x}\\in\\mathbb{N}_{4}^{L}$ , it requires at most $L$ forward passes through the AR model. As shown in Algorithm 2 and Figure 1b, Fast A&E adopts a heuristic-based approach to select segments for refinement. It scans the sequence from left to right, identifying low-quality segments through a thresholding mechanism. Tokens with predicted probabilities smaller than the $T_{a b s o r b}$ threshold trigger the absorb action, while the autoregressive process terminates once the probability of a token generated by the AR model $p^{D M}(\\tilde{\\mathbf{x}}_{j}^{\\prime})$ is smaller than that of the diffusion model $p^{A R}(\\bar{\\bf x}_{j})$ . In this manner, Fast A&E corrects errors made by the diffusion model with a maximum running time of $O(T_{D M}+T_{A R})$ , where $T_{D M}$ and $T_{A R}$ are the times required for generating a single sequence from the diffusion model and autoregressive model, respectively. ", "page_idx": 5}, {"type": "text", "text": "5 Experiment ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "5.1 Transcription Profile (TP) conditioned Promoter Design ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We first evaluate Fast $\\mathsf{A}\\mathsf{\\&E}$ in the task of TP-conditioned promoter design, following the same evaluation procedures and metrics as used by DDSM [4] and Dirichlet Flow Matching (DFM) [32]. ", "page_idx": 5}, {"type": "text", "text": "Data Format & Evaluation Metric: Each data point in this task is represented as a (DNA, signal) pair, where signal corresponds to the CAGE values for a given DNA sequence, providing a quantitative measure of gene expression around the transcription start site (TSS). Both the DNA sequence and the signal have a length of 1024. The goal of this task is to generate DNA sequences conditioned on specified signals. During evaluation, for a given test set data point $(x,c)$ , the generated sequence $\\tilde{\\bf x}$ and the ground truth sequence are processed through the genomic neural network Sei [37]. The performance metric is the mean squared error (MSE) between $S e i(x)$ and $S e i(\\tilde{\\mathbf{x}})$ . ", "page_idx": 6}, {"type": "table", "img_path": "XHTl2k1LYk/tmp/47ce616d29fdf370b27d4c87445ab507c445e8e5681d91da02cf0cf14d43c8ce.jpg", "table_caption": ["Table 2: Evaluation of transcription proflie conditioned promoter sequence design. A&E achieves the smallest MSE with Language Model and DFM distilled being the AR and DM components. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Results: As shown in Table 2, we ran Fast A&E on this task with a default threshold of $T_{\\mathrm{absorb}}=0.85$ , using the pretrained model as both the autoregressive (AR) model and the denoising model (DM) component. The evaluation followed the same procedure as described in the DFM repository. The Fast $\\mathsf{A}\\mathsf{\\&E}$ model, comprising AR and DM components (i.e., the language model and the distilled DFM checkpoints provided in the DFM repository), achieved state-of-the-art results with an MSE of 0.0262 on the test split. ", "page_idx": 6}, {"type": "text", "text": "5.2 Multi-species Promoter Generation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.2.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Dataset Construction Prior efforts in DNA generation have been constrained by small, single-species datasets [19, 34]. To better evaluate the capability of various generative algorithms in DNA generation, we construct a dataset with 15 species from the Eukaryotic Promoter Database (EPDnew)[23]. Table 3 compares our EPD dataset with those used in previous studies, including DDSM[4], ExpGAN [38], and EnhancerDesign [33]. The key advantage of EPD is its diversity in both species types and DNA sequence types. Additionally, although the number of sequences in EPD is on a similar scale to that of DDSM, EPD offers greater uniqueness: each sequence corresponds to a unique promoter-gene combination, a guarantee not provided by the other datasets. ", "page_idx": 6}, {"type": "text", "text": "Baseline Model We evaluate the stateof-the-art diffusion models for DNA sequence generation: DDSM [4], DNADiffusion [26], DDPM [2, 3], and a AR model Hyena [20, 24]. In addition, we implement a VAE with a CNN-based encoder-decoder architecture. Adding UNet as the denoising network to VAE results in another baseline latent diffusion model termed DiscDiff. For a fair evaluation, we maximally scale up the denoising networks of each diffusion model to fit ", "page_idx": 6}, {"type": "table", "img_path": "XHTl2k1LYk/tmp/bcbac29822b68445832033bff18875960449b2692df60abec4033f6b34556e36.jpg", "table_caption": ["Table 3: A comparison of DNA generation datasets. EPD used in this work is significantly larger in size and contains fifteen species. Reg. represents the regulatory regions, and Prot. represents the protein encoding region. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "into an 40GB NVIDIA A100. Additionally, we adapt four pretrained Hyena models from HuggingFace for comprehensive fine-tuning. The additional details of the network architectures are shown in Appendix D. ", "page_idx": 6}, {"type": "text", "text": "Model Training All the models are implemented in Pytorch and trained on a NVIDIA A100-PCIE40GB with a maximum wall time of 48 GPU hours per model; most of the models converged within the given time. Adam optimizer [7] is used together with the CosineAnnealingLR [22] scheduler. The learning rate of each model are detailed in Appendix D. For the evaluation of various diffusion models in unconditional generation (see Section 5.2.2), we sample 50,000 sequences from each model. For the conditional generation across 15 species (see Section 5.3), we generate 4,000 sequences. In both cases, we use the DDPM sampler [31] with 1,000 sequential denoising steps. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5.2.2 Evaluating Diffusion Models on Mammalian Model Organisms ", "text_level": 1, "page_idx": 7}, {"type": "table", "img_path": "XHTl2k1LYk/tmp/00849c1c58860a8e32d30414f96aa8329229f19e9bf3ce88f2e653160702aab0.jpg", "table_caption": ["Table 4: Comparison of diffusion models on unconditional generation evaluated on EPD (256 bp) and EPD $(2048\\;\\mathrm{bp})$ ). Metrics include S-FID, $\\mathrm{Cor_{TATA}}$ , and $\\mathrm{MSE_{TATA}}$ . The best and second-best scores are highlighted in bold and underlined, respectively. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "One prerequisite of Fast A&E (Algorithm 2) is that the diffusion model $P_{\\beta}^{D M}$ should be properly trained and provide accurate approximations of underlying data distribution. We first evaluate existing Diffusion Models on a subset of EPD datasets. This subset includes sequences from four mammalians H. Sapiens (human), Rattus Norvegicus (rat), Macaca mulatta, and Mus musculus (mouse), which collectively represent $50\\%$ of the total dataset. Training on this subset allows for a more precise assessment of the generative algorithm\u2019s accuracy in a unconditional generation setting. ", "page_idx": 7}, {"type": "text", "text": "Metrics ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Motif Distribution Correlation $(\\mathbf{Cor}_{\\mathbf{M}})$ and Mean Square Error $(\\mathbf{MSE_{M}})$ : $\\mathrm{Cor}_{\\mathrm{M}}$ is the Pearson correlation between the motif distributions of generated and natural DNA sequences for motifs like TATA-box, GC-box, Initiator, and CCAAT-box. $\\mathrm{MSE_{M}}$ is the average squared differences between these motif distributions. ", "page_idx": 7}, {"type": "text", "text": "2. S-FID (Sei Fr\u00e9chet Inception Distance): Measures the distance between distributions of generated and natural DNA sequences in latent space similar to the FID metric [15] for images, replacing the encoder with the pre-trained genomic neural network, Sei [37]. ", "page_idx": 7}, {"type": "text", "text": "The results are presented in Table 4, indicating that most existing models perform worse than the simple baseline DiscDiff proposed here, as measured by S-FID, CorTATA, and $\\mathrm{MSE_{TATA}}$ . TATA is one of the most fundamental motifs for gene transcription \u2013 a special type of protein called transcription factors binds to TATA tokens on the DNA as shown in Figure 1a. It changes the shape of DNA and then enables the gene transcription. The failure of existing diffusion models to capture the TATA-box distribution indicates a potential gap in existing research. In the following, we hereby use DiscDiff as the p\u03b2 $p_{\\beta}^{D M}$ to initialize the A&E algorithm. ", "page_idx": 7}, {"type": "text", "text": "5.3 Multi-species DNA Sequences Generation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We compare our model Fast A&E with Hyena [24] and the best-performing diffusion model from Section 5.2.2, DiscDiff, on the task of generating species-specific DNA sequences. ", "page_idx": 7}, {"type": "text", "text": "Motif-centric Evaluation We consider four types of motifs closely related to promoter activities {TATA-box, GC-box, Initiator, CCAAT-box}. We calculate 4 types of motif distributions for 15 species across 3 models, resulting in 180 frequency distributions. ", "page_idx": 7}, {"type": "text", "text": "Figure 3 shows the average MSE and Correlation between generated and natural DNA distributions for each model and motif type across 15 species. Fast A&E improves upon Hyena and DiscDiff, generating the most realistic sequences across almost all species and motif types. It achieves the lowest MSE and highest Correlation across all four motifs. This pattern is consistent across all 15 species. The motif plots for all 15 species are provided in Appendix F. As an example, Figure 4 shows the motif distributions of sequences generated by the three models versus real DNA sequences for macaque. Fast A&E closely resembles the natural motif distribution, especially for the TATA and GC box motifs, while Hyena and DiscDiff fail to capture the values or trends accurately. ", "page_idx": 7}, {"type": "image", "img_path": "XHTl2k1LYk/tmp/f6ec86adf6c953eddecdd3cb81a2be5153922b6749c6b9ea22e7516bd1a8bec3.jpg", "img_caption": ["Figure 3: The average MSE and Correlation between generated and natural DNA distributions for each model and motif type across 15 species. Fast A&E outperforms Hyena and DiscDiff, generating the most realistic sequences with the lowest MSE and highest Correlation across four motif types. This pattern is consistent across all 15 species. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Sequence Diversity To assess the diversity of the generated sequences, we applied BLASTN [18] to check (1) the similarity between the training set (Natural DNA) and generated sequences from three models, and (2) the similarity within the generated sequences. BLASTN takes a query DNA sequence and compares it with a database of sequences, returning all the aligned sequences in the database that are similar to the query. For each alignment, an alignment score, the alignment length (AlignLen), and statistical significance (eValue) are provided to indicate the quality of the alignment, where a larger alignment score, a smaller statistical significance (eValue), and a longer alignment sequence (AlignLen) indicate a higher similarity between the query sequence and the database ", "page_idx": 8}, {"type": "image", "img_path": "XHTl2k1LYk/tmp/f08adbb469fd6a5f60ee07d6f49fc05ff32a0e6e7d102ee90ee703e9db565a5b.jpg", "img_caption": ["Figure 4: Motif distributions in macaque DNA compared across natural DNA, FAST A&E, DiscDiff, and Hyena. FAST A&E closely aligns with natural DNA, especially for the TATA and GC motifs. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "sequences. Ideally, when using generated sequences to query the training dataset, a good generative sequence should align better than a random sequence, but not replicate the sequences in the training set. ", "page_idx": 8}, {"type": "text", "text": "Table 5 shows the results of the BLASTN algorithm. From the table, DiscDiff, Hyena, and A&E all satisfied the mentioned criteria. In terms of the diversity within the groups, none of the algorithms generated repetitive sequences. Furthermore, A&E tends to lie between Hyena and DiscDiff in terms of the diversity of the generated sequences, implying that A&E may combine the properties of AR and DM models. One notable fact is that the alignment scores are very high within the natural sequences, potentially indicating that natural sequences ", "page_idx": 8}, {"type": "table", "img_path": "XHTl2k1LYk/tmp/a5411a3d0f5be8e8c24d1aebe38827de13454f43cc292f11ac752a0a63ea071b.jpg", "table_caption": ["Table 5: BLASTN Results "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "naturally have repetitive properties (conservative motifs), while the generative sequences do not have this characteristic. ", "page_idx": 8}, {"type": "text", "text": "Genome Integration with Promoter Sequences As shown in Figure 5, to evaluate the functional properties of sequences generated by Hyena, DiscDiff, and A&E, we inserted the generated promoter sequences of length 128 bp upstream (5\u2019) of three commonly studied genes in oncology: TP53, ", "page_idx": 8}, {"type": "table", "img_path": "XHTl2k1LYk/tmp/bb4ee5f167b9f0014eb1cff2f27a839c5984432f0373a3832c4cd43fe386a156.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "We use Enformer [5] to predict transcription proflies. Enformer takes a DNA sequence of $200\\mathrm{k}$ bps as input and outputs a matrix $P\\in\\mathbb{R}^{896\\times638}$ , representing a multi-cell type transcription proflie. We sampled 300 promoter sequences from each source: Natural DNA promoters, Hyena, DiscDiff, and A&E. For each set, we calculated the average transcription proflie across the sequences. The Sum of Squared Errors (SSE) between these average transcription profiles of the generated sequences and those of natural promoters are shown in Table 6. The results indicate that A&E produces the smallest SSE, suggesting it best captures the properties of natural DNA. This finding highlights the potential of generative algorithms to create promoter sequences that effectively regulate gene expression, with applications in bioproduct manufacturing and gene therapy. ", "page_idx": 9}, {"type": "table", "img_path": "XHTl2k1LYk/tmp/bce69a0d22ff65174fb8e36f085e2e4b0954cbf48ef4ee5300a1c0dae9976f96.jpg", "table_caption": ["Table 6: Sum of Squared Errors (SSE) of Transcription Profiles between Real and Generated Sequences "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5.3.1 Sensitivity Analysis of TAbsorb ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We perform a sensitivity analysis of A&E algorithm over hyperparameter $T_{\\mathrm{absorb}}$ . As shown in Figure 6, with a small $T_{\\mathrm{absorb}}$ , the sequences generated by A&E are dominated by the diffusion model. As $T_{\\mathrm{absorb}}$ increases, the AR helps to correct the errors made by the DM. Finally, when $T_{\\mathrm{absorb}}$ is larger than 0.7, the correlation flattens and fluctuates. In conclusion, A&E is robust under different values of $T_{\\mathrm{absorb}}$ , and it is best to use the validation dataset to choose the optimal value. However, a wide range of $T_{\\mathrm{absorb}}$ can still be used with improved performance. ", "page_idx": 9}, {"type": "image", "img_path": "XHTl2k1LYk/tmp/c38ed0c28e8728e684e75379656a3921b12303763be8a5bb3c8f3d8e50e30b46.jpg", "img_caption": ["Figure 5: Evaluation of Generated Promoters for gene regulation through Genome Integration EGFR, and AKT1 [6, 25, 29], which are closely related to tumor activities. Our goal was to determine which model generates promoter sequences that produce gene expression levels closest to those of natural promoters when reinserted into the human genome. ", "Figure 6: Sensitivity of A&E under various $T_{\\mathbf{absorb}}$ . For each value of $T_{\\mathrm{absorb}}$ , 3000 sequences are generated and compared with natural DNA. The correlation between the generated sequences and natural DNA increases initially as $T_{\\mathrm{absorb}}$ increases, and then it flattens. An optimal $T_{\\mathrm{absorb}}$ can be selected based on the validation set, or a default value of 0.85 can be used. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper demonstrates that (i) both the AutoRegressive (AR) model and Diffusion Models (DMs) fail to accurately model DNA sequences due to the heterogeneous nature of DNA sequences when used separately, and (ii) this limitation can be overcome by introducing A&E, a novel sampling algorithm that combines AR models and DMs. Additionally, we developed a fast implementation of the proposed algorithm, Fast A&E, which enables efficient generation of realistic DNA sequences without the repetitive function evaluations required by conventional sampling algorithms. Experimental results across 15 species show that Fast A&E consistently outperforms single models in generating DNA sequences with functional and structural similarities to natural DNA, as evidenced by metrics such as Motif Distribution, Sequence Diversity, and Genome Integration. Regarding the future work, the generated DNA sequences still require validation through wet-lab experiments before they can be directly used in clinical settings. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This research project has benefitted from the Microsoft Accelerate Foundation Models Research (AFMR) grant program. Zehui Li is grateful to his family\u2014Jinghai Li, Huiqing Wang, and Yiqiu Sun for their support. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Josh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, Lindsay Willmore, Andrew J Ballard, Joshua Bambrick, et al. Accurate structure prediction of biomolecular interactions with alphafold 3. Nature, pages 1\u20133, 2024.   \n[2] Sarah Alamdari, Nitya Thakkar, Rianne van den Berg, Alex Xijie Lu, Nicolo Fusi, Ava Pardis Amini, and Kevin K Yang. Protein generation with evolutionary diffusion: sequence is all you need. bioRxiv, pages 2023\u201309, 2023.   \n[3] Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34:17981\u201317993, 2021.   \n[4] Pavel Avdeyev, Chenlai Shi, Yuhao Tan, Kseniia Dudnyk, and Jian Zhou. Dirichlet diffusion score model for biological sequence generation. arXiv preprint arXiv:2305.10699, 2023.   \n[5] \u017diga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka GrabskaBarwinska, Kyle R Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David R Kelley. Effective gene expression prediction from sequence by integrating long-range interactions. Nature methods, 18(10):1196\u20131203, 2021.   \n[6] Gillian Bethune, Drew Bethune, Neale Ridgway, and Zhaolin Xu. Epidermal growth factor receptor (egfr) in lung cancer: an overview and update. Journal of thoracic disease, 2(1):48, 2010.   \n[7] Kingma Da. A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[8] Hugo Dalla-Torre, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, Evan Trop, Bernardo P de Almeida, Hassan Sirelkhatim, et al. The nucleotide transformer: Building and evaluating robust foundation models for human genomics. bioRxiv, pages 2023\u201301, 2023.   \n[9] Yilun Du, Conor Durkan, Robin Strudel, Joshua B Tenenbaum, Sander Dieleman, Rob Fergus, Jascha Sohl-Dickstein, Arnaud Doucet, and Will Sussman Grathwohl. Reduce, reuse, recycle: Compositional generation with energy-based diffusion models and mcmc. In International conference on machine learning, pages 8489\u20138510. PMLR, 2023.   \n[10] Yilun Du and Leslie Kaelbling. Compositional generative modeling: A single model is not all you need. arXiv preprint arXiv:2402.01103, 2024.   \n[11] G\u00f6kcen Eraslan, \u017diga Avsec, Julien Gagneur, and Fabian J Theis. Deep learning: new computational modelling techniques for genomics. Nature Reviews Genetics, 20(7):389\u2013403, 2019.   \n[12] Alan E Gelfand. Gibbs sampling. Journal of the American statistical Association, 95(452):1300\u2013 1304, 2000.   \n[13] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014.   \n[14] Anvita Gupta and James Zou. Feedback gan for dna optimizes protein functions. Nature Machine Intelligence, 1(2):105\u2013111, 2019.   \n[15] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.   \n[16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[17] Yanrong Ji, Zhihan Zhou, Han Liu, and Ramana V Davuluri. Dnabert: pre-trained bidirectional encoder representations from transformers model for dna-language in genome. Bioinformatics, 37(15):2112\u20132120, 2021.   \n[18] W James Kent. Blat\u2014the blast-like alignment tool. Genome research, 12(4):656\u2013664, 2002.   \n[19] Nathan Killoran, Leo J Lee, Andrew Delong, David Duvenaud, and Brendan J Frey. Generating and designing dna with deep generative models. arXiv preprint arXiv:1712.06148, 2017.   \n[20] Avantika Lal, David Garfield, Tommaso Biancalani, and Gokcen Eraslan. reglm: Designing realistic regulatory dna with autoregressive language models. bioRxiv, pages 2024\u201302, 2024.   \n[21] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang, and Tatsunori B Hashimoto. Diffusion-lm improves controllable text generation. Advances in Neural Information Processing Systems, 35:4328\u20134343, 2022.   \n[22] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016.   \n[23] Patrick Meylan, Ren\u00e9 Dreos, Giovanna Ambrosini, Romain Groux, and Philipp Bucher. Epd in 2020: enhanced data visualization and extension to ncrna promoters. Nucleic Acids Research, 48(D1):D65\u2013D69, 2020.   \n[24] Eric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas, Michael Wornow, Callum BirchSykes, Stefano Massaroli, Aman Patel, Clayton Rabideau, Yoshua Bengio, et al. Hyenadna: Long-range genomic sequence modeling at single nucleotide resolution. Advances in neural information processing systems, 36, 2024.   \n[25] Magali Olivier, Monica Hollstein, and Pierre Hainaut. Tp53 mutations in human cancers: origins, consequences, and clinical use. Cold Spring Harbor perspectives in biology, 2(1):a001008, 2010.   \n[26] Luca Pinello. Dna-diffusion: Leveraging generative models for controlling chromatin accessibility and gene expression via synthetic regulatory elements. In ICLR 2024 Workshop on Machine Learning for Genomics Explorations.   \n[27] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \n[28] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.   \n[29] Marion Rudolph, Tobias Anzeneder, Anke Schulz, Georg Beckmann, Annette T Byrne, Michael Jeffers, Carol Pena, Oliver Politz, Karl K\u00f6chert, Richardus Vonk, et al. Akt1 e17k mutation profiling in breast cancer: prevalence, concurrent oncogenic alterations, and blood-based detection. BMC cancer, 16:1\u201312, 2016.   \n[30] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 2256\u20132265. PMLR, 2015.   \n[31] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020.   \n[32] Hannes Stark, Bowen Jing, Chenyu Wang, Gabriele Corso, Bonnie Berger, Regina Barzilay, and Tommi Jaakkola. Dirichlet flow matching with applications to dna sequence design. arXiv preprint arXiv:2402.05841, 2024.   \n[33] Ibrahim I Taskiran, Katina I Spanier, Hannah Dickm\u00e4nken, Niklas Kempynck, Alexandra Panc\u02c7\u00edkov\u00e1, Eren Can Eks\u00b8i, Gert Hulselmans, Joy N Ismail, Koen Theunis, Roel Vandepoel, et al. Cell type directed design of synthetic enhancers. Nature, pages 1\u20133, 2023.   \n[34] Ye Wang, Haochen Wang, Lei Wei, Shuailin Li, Liyang Liu, and Xiaowo Wang. Synthetic promoter design in escherichia coli based on a deep generative network. Nucleic Acids Research, 48(12):6403\u20136412, 2020.   \n[35] Joseph L Watson, David Juergens, Nathaniel R Bennett, Brian L Trippe, Jason Yim, Helen E Eisenach, Woody Ahern, Andrew J Borst, Robert J Ragotte, Lukas F Milles, et al. De novo design of protein structure and function with rfdiffusion. Nature, pages 1\u20133, 2023.   \n[36] Yizhe Zhang, Jiatao Gu, Zhuofeng Wu, Shuangfei Zhai, Joshua Susskind, and Navdeep Jaitly. Planner: Generating diversified paragraph via latent language diffusion model. Advances in Neural Information Processing Systems, 36, 2024.   \n[37] Jian Zhou and Olga G Troyanskaya. Predicting effects of noncoding variants with deep learning\u2013 based sequence model. Nature methods, 12(10):931\u2013934, 2015.   \n[38] Jan Zrimec, Xiaozhi Fu, Azam Sheikh Muhammad, Christos Skrekas, Vykintas Jauniskis, Nora K Speicher, Christoph S B\u00f6rlin, Vilhelm Verendel, Morteza Haghir Chehreghani, Devdatt Dubhashi, et al. Controlling gene expression with deep generative design of regulatory dna. Nature communications, 13(1):5099, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "image", "img_path": "XHTl2k1LYk/tmp/4b2d354611dd30d0e031e26da23e4fb7bcac6194be114cbdade1a1fba174229d.jpg", "img_caption": ["Figure 7: DiscDiff Model: A two-step process for DNA sequence generation. Step 1: VAE Training: A sequence $s\\,\\in\\,\\{A,T,G,C\\}^{2048/25\\bar{6}}$ is encoded via a 1D-Encoder to a 2D-Encoder. The latent space representation $Z$ with parameters $\\mu,\\epsilon,\\sigma$ is then decoded back to $\\tilde{s}$ through a 2D-Decoder and 1D-Decoder. Step 2: Denoising Network Training: The latent representation $Z$ is processed through a denoising network comprising a ResNet Block, optional Self-Attention, and Cross Attention, with species and time information. The network outputs a Gaussian distribution $N(z;\\mu,\\Sigma)$ . A U-Net architecture takes this distribution to produce various $z_{0}$ representations, which a Locked Decoder (fronzen parameters) used to generate the final DNA sequences. "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "A A simple baseline latent diffusion model for discrete data: DiscDiff ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 The DiscDiff Model ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We design DiscDiff, a Latent Discrete Diffusion model for DNA generation tasks. As shown in Figure 7, this model is structured into two main components: a Variational-Auto-Encoder (VAE) and a denoising model. The VAE consists of an encoder $\\mathbf{E}:\\underline{{\\mathbf{s}}}\\mapsto\\underline{{\\mathbf{z}}}$ , which maps discrete input sequence $\\underline{{\\mathbf{S}}}$ to a continuous latent variable $\\underline{{\\boldsymbol{Z}}}$ , and a decoder $\\mathbf{D}:\\mathbf{Z}\\mapsto\\tilde{\\mathbf{S}}$ , w\u00afhich reverts $\\underline{{\\boldsymbol{Z}}}$ back to s\u02dc in the discrete\u00af space. The denoising model $\\varepsilon_{\\theta}(\\mathbf{z}_{t},t)$ is trained to pre\u00afdict \u00afthe added noise $\\varepsilon$ in the latent space. ", "page_idx": 12}, {"type": "text", "text": "A.1.1 VAE Architecture ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The choice of VAE architecture in LDMs is critical and often domain-specific. We find that mapping the input data to a higher dimension space can help to learn a better denoising network, generating more realistic DNA sequences. We hereby propose to use a two-stage VAE architecture as shown in Figure 7. ", "page_idx": 12}, {"type": "text", "text": "The first stage encoder $\\mathbf{E}_{\\phi_{1}}:\\mathbb{N}_{4}^{L}\\,\\rightarrow\\,\\mathbb{R}^{K\\times M}$ maps $\\underline{{\\mathrm{s}}}\\,\\in\\,\\mathbb{N}_{4}^{L}$ to a 2D latent space $\\mathbf{z}_{1}\\,\\in\\,\\mathbb{R}^{K\\times M}$ , where $K$ is the number of channels and $M$ is the len\u00afgth of the latent representation\u00af. The second stage encoder $\\mathbf{E}_{\\phi_{2}}\\,:\\,\\mathbb{R}^{1\\times K\\times M}\\ \\to\\ \\mathbb{R}^{C\\times K^{\\prime}\\times M^{\\prime}}$ first adds a dummy dimension to $\\underline{{\\boldsymbol{Z}}}_{1}$ such that $\\mathbf{z}_{1}\\,\\in\\,\\mathbb{R}^{1\\times K\\times M}$ and then maps it to 3d latent space $\\mathbf{z}\\,\\in\\,\\mathbb{R}^{C\\times K^{\\prime}\\times M^{\\prime}}$ , where $C$ is th\u00afe number of channels, $K^{\\prime}$ and $M^{\\prime}$ are the reduced dimensions of $K$ and $M$ respectively. The decoder in the first and second stage are $\\mathbf{D}_{\\theta1}$ and $\\mathbf{D}_{\\theta2}$ respectively. Which are symmetric to the encoders. Overall, we have $\\mathbf{z}=\\mathbf{E}_{\\phi}(\\mathbf{s})=\\mathbf{E}_{\\phi_{2}}(\\mathbf{E}_{\\phi_{1}}(\\mathbf{s}))$ , and the reconstruction is $\\tilde{\\mathbf{s}}^{-}\\mathbf{\\delta}=\\mathbf{D}_{\\theta}(\\mathbf{z})=\\mathbf{D}_{\\theta1}(\\mathbf{D}_{\\theta2}(\\mathbf{z}))$ . ", "page_idx": 12}, {"type": "text", "text": "A.1.2 VAE Loss ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "When training the VAE, we propose to use Cross Entropy (CE) as reconstruction loss. The loss function is given by: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{L}_{\\theta,\\phi}=\\underbrace{\\mathbb{E}_{p(\\mathbf{s})}\\left[\\mathbb{E}_{q_{\\phi}(\\mathbf{z}|\\mathbf{s})}\\left[-\\displaystyle\\sum_{l=1}^{L}\\sum_{i=1}^{4}\\delta_{i s_{l}}\\log p_{\\theta}(\\mathbf{s}_{l}|\\mathbf{z})\\right]\\right]}_{\\mathrm{Reconstruction\\:Loss}}+}_{\\beta\\cdot\\underbrace{\\mathbb{E}_{p(\\mathbf{s})}\\left[\\mathrm{KL}\\left(q_{\\phi}(\\mathbf{z}|\\mathbf{s})\\,|\\,|\\mathcal{N}(\\mathbf{z};\\mu,\\mathbf{D})\\right)\\right]}_{\\mathrm{KL\\:Divergence}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\delta_{i j}$ is the Kronecker delta, $p_{\\theta}(\\mathbf{s}|\\mathbf{z})$ is the probabilistic decoder output from $\\mathbf{D}_{\\theta}$ ; $q_{\\phi}(\\mathbf{z}|\\mathbf{s})$ is the probabilistic output from encoder $\\mathbf{E}_{\\phi}$ that represents the approximate posterior of the latent variable ${\\bf z}$ given the input s; $\\mathcal{N}(\\mathbf{z};\\mu,\\Sigma)$ is the prior on ${\\bf z}$ . Here we use a simple isotropic. $\\beta$ is a mixing hyperparameter. $\\beta$ is set to $10^{-5}$ in the experiments used in this paper. ", "page_idx": 13}, {"type": "text", "text": "A.1.3 Denoising Network Training ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Once $\\mathbf{D}_{\\theta}$ and $\\mathbf{E}_{\\phi}$ are trained in the first step, we train a noise prediction $\\varepsilon_{\\theta}$ in the latent space $\\mathbf{z}=\\mathbf{E}_{\\phi}(\\mathbf{s})$ with Equation (6). ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{z},t\\sim U[1,T],\\varepsilon\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})}\\left[\\left\\|\\varepsilon-\\varepsilon_{\\theta}(\\mathbf{z}_{t},t)\\right\\|_{2}^{2}\\right]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "B Toy Experiment Training Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We train both HyenaDNA and DiscDiff on an NVIDIA A100-PCIE-40GB using the Adam optimizer. For HyenaDNA, the learning rate is set to 0.0001, and we use the model heyenadna-large-1m-seqlen for this task. The maximum number of epochs is set to 100, with early stopping enabled to facilitate early convergence. ", "page_idx": 13}, {"type": "text", "text": "For DiscDiff, the VAE is trained with a learning rate of 0.0001, while the UNet is trained with a learning rate of 0.00005. DiscDiff is trained for 600 epoches; during the inference time, we use DDPM [16] sampler with 1000 denoising steps. ", "page_idx": 13}, {"type": "text", "text": "C Convergence Proof of the Absorb & Escape Algorithm ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we provide a proof for the convergence of the Absorb & Escape (A&E) algorithm under certain assumptions. The convergence proof will demonstrate that the sequence generated by the A&E algorithm converges to a sample from the target distribution $p_{\\theta,\\beta}^{C}(\\mathbf{x})$ . ", "page_idx": 13}, {"type": "text", "text": "C.1 Assumptions ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We make the following assumptions for the convergence proof: ", "page_idx": 13}, {"type": "text", "text": "1. The autoregressive model $p_{\\theta}^{A R}(\\mathbf{x})$ and the diffusion model $p_{\\beta}^{D M}(\\mathbf{x})$ are both properly trained and provide accurate approximations of the underlying data distribution.   \n2. The initial sample $\\mathbf{x}^{0}\\sim p_{\\beta}^{D M}(\\mathbf{x})$ is a valid sample from the diffusion model.   \n3. The segments $\\{\\mathbf{s}_{1},\\mathbf{s}_{2},\\ldots,\\mathbf{s}_{n}\\}$ of the sequence $\\mathbf{x}$ are chosen such that each segment is homogeneous.   \n4. The subset of segments $\\boldsymbol{S}$ is chosen randomly and includes all segments over multiple iterations.   \n5. The conditional distribution $p(\\mathbf{x}_{i:j}|\\mathbf{x}_{0:i-1},\\mathbf{x}_{j+1:L})$ approximated by $p_{\\theta}^{A R}(\\mathbf{x}_{i:j}|\\mathbf{x}_{0:i-1})$ is accurate. ", "page_idx": 13}, {"type": "text", "text": "C.2 Proof ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We aim to show that the $\\mathsf{A}\\mathsf{\\&E}$ algorithm produces samples from the target distribution $p_{\\theta,\\beta}^{C}(\\mathbf{x})$ . We do this by showing that the Markov chain defined by the A&E algorithm has $p_{\\theta,\\beta}^{C}(\\mathbf{x})$ as its stationary distribution. ", "page_idx": 13}, {"type": "text", "text": "Step 1: Initialization The initial sample $\\mathbf{x}^{0}\\sim p_{\\beta}^{D M}(\\mathbf{x})$ is drawn from the diffusion model. This ensures that $\\mathbf{x}^{0}$ is a valid sample from $p_{\\beta}^{D M}(\\mathbf{x})$ . ", "page_idx": 14}, {"type": "text", "text": "Step 2: Absorb Step For each segment $\\mathbf{s}_{k}$ in the subset $\\boldsymbol{S}$ , the Absorb step samples $\\tilde{\\mathbf{x}}_{i:j}^{\\prime}$ from the conditional distribution $p(\\mathbf{x}_{i:j}|\\mathbf{x}_{0:i-1},\\mathbf{x}_{j+1:L})$ . Under the assumption that $p_{\\theta}^{A R}(\\mathbf{x}_{i:j}|\\bar{\\mathbf{x}}_{0:i-1})$ accurately approximates this conditional distribution, the refined segment $\\tilde{\\mathbf{x}}_{i:j}^{\\prime}$ is a valid sample from the target conditional distribution. ", "page_idx": 14}, {"type": "text", "text": "Step 3: Escape Step The Escape step updates the segment $\\tilde{\\mathbf{x}}_{i:j}^{t}$ to $\\tilde{\\mathbf{x}}_{i:j}^{\\prime}$ . This ensures that the updated sequence $\\tilde{\\mathbf{x}}^{t}$ incorporates the refinement from the autoregressive model. ", "page_idx": 14}, {"type": "text", "text": "Step 4: Stationary Distribution To show that the Markov chain defined by the A&E algorithm converges to $p_{\\theta,\\beta}^{C}(\\mathbf{\\dot{x}})$ , we need to show that $p_{\\theta,\\beta}^{C}(\\mathbf{x})$ is the stationary distribution of this Markov chain. The transition probability for the A&E algorithm is given by the product of the probabilities of the Absorb and Escape steps: ", "page_idx": 14}, {"type": "equation", "text": "$$\nP(\\mathbf{x}\\rightarrow\\mathbf{x}^{\\prime})=P_{\\mathrm{Absorb}}(\\mathbf{x}\\rightarrow\\mathbf{x}^{\\prime})P_{\\mathrm{Escape}}(\\mathbf{x}^{\\prime}\\rightarrow\\mathbf{x}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Given that $p_{\\beta}^{D M}(\\mathbf{x})$ captures the overall structure and $p_{\\theta}^{A R}(\\mathbf{x})$ refines the segments, the composed distribution $p_{\\theta,\\beta}^{C}(\\mathbf{x})$ is achieved by iteratively applying the Absorb and Escape steps. We need to show that the stationary distribution satisfies: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\int p_{\\theta,\\beta}^{C}(\\mathbf{x})P(\\mathbf{x}\\rightarrow\\mathbf{x}^{\\prime})d\\mathbf{x}=p_{\\theta,\\beta}^{C}(\\mathbf{x}^{\\prime}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Using the detailed balance condition for the Markov chain: ", "page_idx": 14}, {"type": "equation", "text": "$$\np_{\\theta,\\beta}^{C}(\\mathbf{x})P(\\mathbf{x}\\rightarrow\\mathbf{x}^{\\prime})=p_{\\theta,\\beta}^{C}(\\mathbf{x}^{\\prime})P(\\mathbf{x}^{\\prime}\\rightarrow\\mathbf{x}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Given that the Markov chain is ergodic, the detailed balance condition implies that $p_{\\theta,\\beta}^{C}(\\mathbf{x})$ is the stationary distribution. ", "page_idx": 14}, {"type": "text", "text": "Step 5: Ergodicity Ergodicity ensures that the Markov chain will visit all possible states given sufficient iterations. The random selection of segments $\\boldsymbol{S}$ and the iterative updates in the A&E algorithm guarantee that all parts of the sequence are refined over time. ", "page_idx": 14}, {"type": "text", "text": "Conclusion By satisfying the detailed balance condition and ergodicity, we have shown that the Markov chain defined by the A&E algorithm converges to the target distribution $p_{\\theta,\\beta}^{C}(\\mathbf{x})$ . Therefore, the $\\mathsf{A}\\mathsf{\\&E}$ algorithm produces samples from the composed distribution $p_{\\theta,\\beta}^{C}(\\mathbf{x})$ as the number of iterations $t$ approaches infinity. ", "page_idx": 14}, {"type": "text", "text": "D Experiment Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Baselines The details about the architecture and implementation of the baseline models are as below: ", "page_idx": 14}, {"type": "text", "text": "\u2022 DNADiffusion [26]: We enhance the current DNADiffusion implementation for DNA synthesis, originally from the DNA-Diffusion project2, by expanding the models to encompass 380 million parameters. This network is composed of Convolutional Neural Networks (CNNs), interspersed with layers of cross-attention and self-attention. The learning rate is set to 0.0001.   \n\u2022 DDSM [4]: We scale up the original implementation of the denoising network used for promoter design in $\\mathrm{DDSM}^{3}$ to what is the corresponding size of the network given 470 million parameters. It is a convolution-based architecture with dilated convolution layers. The learning rate is set to 0.00001.   \n\u2022 D3PM [3]: We take the implementation of D3PM for biological sequence generation from EvoDiff [2]4, adopting the algorithm for DNA generation. We use the original implementation of the denoising network, which has two versions: with sizes of 38M and 640M. We hereby have D3PM (small) and D3PM (big), respectively. The learning rate for both D3PM (small) and D3PM (large) are set to 0.0001.   \n\u2022 Hyena [24]: We modify the RegLM $[20]^{5}$ , a existing work uses hyena for DNA generation. Four pretrained Hyena models of different sizes (hyenadna-large-1m-seqlen, hyenadnamedium-160k0seqlen, heynadna-small- $.32\\mathrm{k}$ -seqlen, and hyenaana-tiny-16k-seqlen-d128) are downloaded from HuggingFace6and used for full-size fine-tuning, we apply the fine-tuned models for generations on EPD-GenDNA. The learning rate for fine-tuning is set to 0.0001.   \n\u2022 DiscDiff: A 2D-UNet of 500 Million parameters are used as the denoising network. See Appendix A for the implementation details. The learning rate is set to 0.00005 for UNet training, 0.0001 for VAE training. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "For the Fast A&B algorithm, we set the $T_{a b s o r b}$ to 0.80. ", "page_idx": 15}, {"type": "text", "text": "E Content List of Supplementary Code and Data ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Our code folder includes the following sub-folders, within each folder there is a readme flie, detailing the steps to run the code. The below is the list of sub-folders. ", "page_idx": 15}, {"type": "text", "text": "E.1 Toy Experiment ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Include the source code to reproduce Section 3. No external package is required to run the code except for python ", "page_idx": 15}, {"type": "text", "text": "E.2 DiscDiff", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A implementation of the discdiff baseline in Appendix A. Please follow the readme for running the code. ", "page_idx": 15}, {"type": "text", "text": "E.3 AbsorbEscape ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "This includes an implementation of the proposed algorithm in Section 4.2; however, it depends on the external AR models, please adopt it accordingly to AR models which you want to try. ", "page_idx": 15}, {"type": "text", "text": "E.4 EPD Data ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We include the training dataset used for producing the main results, which includes 160K DNA sequences from EPD, each sequence has a length of 256 bp. ", "page_idx": 15}, {"type": "text", "text": "F Motif Distributions for 15 species ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We plot TATA-box, GC content, Initiator, and CCAAT-box for 15 speces as below. ", "page_idx": 15}, {"type": "image", "img_path": "XHTl2k1LYk/tmp/066051f383fb9e664a91927c162aa5d470dab488908874dd724b0e7378ee1104.jpg", "img_caption": ["Figure 8: Baking Yeast "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "XHTl2k1LYk/tmp/2c22235d3e92843f4be4a7956525070877b7e38913ffee7db6e79633bd4e173a.jpg", "img_caption": ["Figure 9: Chicken "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "XHTl2k1LYk/tmp/ec02a992f707f86eaa5775f12e8baa4161333c6d538c941fa6be1942e5032432.jpg", "img_caption": ["Figure 10: Chicken "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "XHTl2k1LYk/tmp/dd8cae4690c9d761dedef49f52343ce732370bf0e8c09bb12882e3f7f8b6b931.jpg", "img_caption": ["Figure 11: Corn "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "XHTl2k1LYk/tmp/3da101bfb75fa66aef079f546dfba6d2fd0ef51a308fcd9d0ca6dd6229bf6479.jpg", "img_caption": ["Figure 12: dog "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "XHTl2k1LYk/tmp/8aa04173d5ae031aeb35e9edbc7ae4a033bbc50f41b8a56633444f699d51ff22.jpg", "img_caption": ["Figure 13: elegans "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "XHTl2k1LYk/tmp/05b7bb60312ce33684bd3374b1a7332e2d70090c98ed781446e8c1e540f9fa07.jpg", "img_caption": ["Figure 14: fission yeast "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "XHTl2k1LYk/tmp/43e8a77dcc993bc03237dd282fdd518f4eb202d2643eda4ab3323a198c58babb.jpg", "img_caption": ["Figure 15: fruit fly "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "XHTl2k1LYk/tmp/fd01455a712b20872f84366d738710477b02ab46104d6d2f33f3405abc44225f.jpg", "img_caption": ["Figure 16: honey bee "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "XHTl2k1LYk/tmp/9275aef66343b3adea9e8d5d9428abdfa1d7627f168b4a27adb05e60316fd1b2.jpg", "img_caption": ["Figure 17: human "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "XHTl2k1LYk/tmp/9f7662c83959fc1bff5b1cf3e3a92b0554e7afd581f3fdb651a9a9c2efce7fd3.jpg", "img_caption": ["Figure 18: plasmodium "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "XHTl2k1LYk/tmp/16a06b547c889b946373867b3fe7ba449463a883e9ae6229e0be6fee8401d695.jpg", "img_caption": ["Figure 19: rat "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "XHTl2k1LYk/tmp/a7e60828df39217e2fd0d810a70b0af6e0a6e16c1c639deef86c93811d050881.jpg", "img_caption": ["Figure 20: thale cress "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "XHTl2k1LYk/tmp/b7880eba4001a8c0b1be54766ba2afa209eb69695cfafa9dc292045ee458ffaf.jpg", "img_caption": ["Figure 21: macaque "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "XHTl2k1LYk/tmp/ca36022f805212f40459e2a75306bb3acc4f8da3b17f31f87954b2d8956060c3.jpg", "img_caption": ["Figure 22: zebrafish "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The sections related to the contributions are listed at the end of the introduction. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We talked about the assumption required by the algorithm in the Appendix C, we also talked about other limitations through the paper. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Can be found in Appendix C ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: For the Toy Example experiment details see Appendix B, for the baseline and main result set up see Appendix D ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: See Appendix E for the instructions to access the code. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: See Section 5.3 and Appendix D. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: For the main result which compares the motif distributions across 15 species, we compute the error bar. For some of the other tasks requiring heavy compute, we didn\u2019t repeat the experiment. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: See Appendix D and Section 5.3 ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Mainly in the introduction part ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: All the code repos - The MIT License (MIT) eukaryotic promoter database - CC-BY 4.0 ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 28}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: See Appendix E Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: No Crowdsourcing Involved ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: No Crowdsourcing Involved ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]