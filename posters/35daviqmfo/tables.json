[{"figure_path": "35DAviqMFo/tables/tables_1_1.jpg", "caption": "Table 1: English and Chinese datasets evaluated in the experiment, and their task types, prompting types, answer forms and metrics. For prompting type, we refer to the chain-of-thought prompting [59] as few-shot CoT and the original in-context learning prompting [6] as few-shot.", "description": "This table lists twelve English and six Chinese datasets used in the paper's experiments.  For each dataset, it specifies the task type (e.g., question answering, natural language inference), the prompting type used (zero-shot, few-shot, or few-shot chain-of-thought), the answer format (e.g., open-ended, multiple choice), and the evaluation metric used (e.g., Exact Match, Accuracy).  The table provides a comprehensive overview of the diverse tasks and evaluation methods employed in the study.", "section": "2 Does Pre-training Loss Predict Task Performance?"}, {"figure_path": "35DAviqMFo/tables/tables_3_1.jpg", "caption": "Table 2: Statistical measures of the correlation between task performance and pre-training loss in Figure 1. The spearman correlation coefficient [50] measures the monotonicity of the relationship between the two variables, and the pearson correlation coefficient measures the linearity of the relationship. Both vary between -1 and +1 with 0 implying no correlation. Correlations of -1 or +1 imply an exact monotonic/linear relationship.", "description": "This table presents the statistical correlation (Spearman and Pearson) between pre-training loss and the performance of 12 downstream tasks.  It shows the strength and type of relationship between the pre-training loss and performance across diverse tasks.", "section": "2 Does Pre-training Loss Predict Task Performance?"}, {"figure_path": "35DAviqMFo/tables/tables_18_1.jpg", "caption": "Table 1: English and Chinese datasets evaluated in the experiment, and their task types, prompting types, answer forms and metrics. For prompting type, we refer to the chain-of-thought prompting [59] as few-shot CoT and the original in-context learning prompting [6] as few-shot.", "description": "This table lists twelve English and six Chinese datasets used in the paper's experiments.  For each dataset, it specifies the task type (e.g., question answering, natural language inference), the prompting type used (zero-shot, few-shot, few-shot chain-of-thought), the answer format (open-formed, multiple-choice), and the evaluation metric (exact match, accuracy).  The table shows the diversity of tasks and languages used to evaluate the relationship between pre-training loss and model performance.", "section": "2 Does Pre-training Loss Predict Task Performance?"}, {"figure_path": "35DAviqMFo/tables/tables_18_2.jpg", "caption": "Table 1: English and Chinese datasets evaluated in the experiment, and their task types, prompting types, answer forms and metrics. For prompting type, we refer to the chain-of-thought prompting [59] as few-shot CoT and the original in-context learning prompting [6] as few-shot.", "description": "This table lists twelve English and Chinese datasets used in the paper's experiments.  For each dataset, it specifies the type of task (e.g., question answering, natural language inference), the prompting type used (zero-shot, few-shot, few-shot chain-of-thought), the answer form (open-ended, multiple-choice), and the evaluation metric (exact match, accuracy).  The table provides a comprehensive overview of the diverse tasks and evaluation methods employed in the study.", "section": "2 Does Pre-training Loss Predict Task Performance?"}, {"figure_path": "35DAviqMFo/tables/tables_19_1.jpg", "caption": "Table 1: English and Chinese datasets evaluated in the experiment, and their task types, prompting types, answer forms and metrics. For prompting type, we refer to the chain-of-thought prompting [59] as few-shot CoT and the original in-context learning prompting [6] as few-shot.", "description": "This table lists twelve English and Chinese datasets used in the paper's experiments to evaluate the performance of language models.  For each dataset, it provides the task type (e.g., question answering, natural language inference), the prompting type used (zero-shot, few-shot, few-shot chain-of-thought), the answer format (e.g., open-ended, multiple-choice), and the evaluation metric (e.g., exact match, accuracy).  The table helps to illustrate the diversity of tasks and evaluation methods used in the study.", "section": "2 Does Pre-training Loss Predict Task Performance?"}, {"figure_path": "35DAviqMFo/tables/tables_19_2.jpg", "caption": "Table 1: English and Chinese datasets evaluated in the experiment, and their task types, prompting types, answer forms and metrics. For prompting type, we refer to the chain-of-thought prompting [59] as few-shot CoT and the original in-context learning prompting [6] as few-shot.", "description": "This table lists twelve English and Chinese datasets used in the paper's experiments.  For each dataset, it specifies the type of task (e.g., question answering, natural language inference), the prompting type used (zero-shot, few-shot, few-shot chain-of-thought), the format of the answers (open-formed, multiple-choice), and the evaluation metric used (Exact Match, Accuracy).  The table provides a comprehensive overview of the diverse range of tasks and evaluation methods employed in the study.", "section": "2 Does Pre-training Loss Predict Task Performance?"}]