[{"figure_path": "35DAviqMFo/figures/figures_3_1.jpg", "caption": "Figure 1: The performance-vs-loss curves of 1.5B, 6B, and 32B models. Each data point is the loss (x-axis) and performance (y-axis) of the intermediate checkpoint of one of the three models. We mark the results of random guess in black dashed lines.", "description": "This figure displays the relationship between the pre-training loss and the performance on 12 downstream tasks for three different sized models (1.5B, 6B, and 32B parameters).  Each point represents a model checkpoint during training, showing performance on each task as pre-training loss decreases. The dashed lines indicate random performance levels for each task.", "section": "2 Does Pre-training Loss Predict Task Performance?"}, {"figure_path": "35DAviqMFo/figures/figures_4_1.jpg", "caption": "Figure 1: The performance-vs-loss curves of 1.5B, 6B, and 32B models. Each data point is the loss (x-axis) and performance (y-axis) of the intermediate checkpoint of one of the three models. We mark the results of random guess in black dashed lines.", "description": "This figure displays the relationship between pre-training loss and performance across three different model sizes (1.5B, 6B, and 32B parameters) on twelve downstream tasks.  Each point represents a model checkpoint, showing performance on the y-axis and corresponding pre-training loss on the x-axis.  The dashed lines indicate the performance expected from random guessing. The figure demonstrates the strong correlation between lower pre-training loss and improved performance across all model sizes.", "section": "2 Does Pre-training Loss Predict Task Performance?"}, {"figure_path": "35DAviqMFo/figures/figures_5_1.jpg", "caption": "Figure 3: The performance-vs-loss curves of LLaMA. The values of performance and training loss are extracted from the figures in the original LLaMA paper [55]. Note that the LLaMA2 paper [56] does not cover such figures with related information.", "description": "This figure displays the performance versus training loss curves for various LLaMA models.  The data points are taken directly from the original LLaMA paper and showcase the relationship between pre-training loss and performance on multiple downstream tasks.  The figure helps to validate the authors' claim that pre-training loss is a good predictor of performance, regardless of model size.", "section": "2.5 LLAMA's Loss vs. Performance"}, {"figure_path": "35DAviqMFo/figures/figures_6_1.jpg", "caption": "Figure 4: The performance-vs-loss curves of different metrics on MMLU and C-Eval. Accuracy: discontinuous; CorrectChoiceProb and BrierScore: continuous. We mark the result of random guess in black dashed lines.", "description": "This figure shows the performance versus training loss curves for MMLU and C-Eval using three different metrics: Accuracy, CorrectChoiceProb, and BrierScore.  The purpose is to demonstrate that the relationship between pre-training loss and performance persists even when using continuous metrics (CorrectChoiceProb and BrierScore),  addressing concerns that the observed emergent abilities are simply an artifact of using discontinuous metrics (Accuracy).  The dashed lines indicate the performance level expected from random guessing.", "section": "3.2 Influence of Different Metrics"}, {"figure_path": "35DAviqMFo/figures/figures_20_1.jpg", "caption": "Figure 1: The performance-vs-loss curves of 1.5B, 6B, and 32B models. Each data point is the loss (x-axis) and performance (y-axis) of the intermediate checkpoint of one of the three models. We mark the results of random guess in black dashed lines.", "description": "This figure shows the relationship between the pre-training loss and the performance on 12 downstream tasks for three different sized language models (1.5B, 6B, and 32B parameters). Each point represents a checkpoint during training.  The plot demonstrates that performance improves as pre-training loss decreases, and that models of different sizes exhibit similar performance trends at the same loss level.", "section": "2 Does Pre-training Loss Predict Task Performance?"}, {"figure_path": "35DAviqMFo/figures/figures_21_1.jpg", "caption": "Figure 6: The performance-vs-compute curves of 1.5B, 6B, and 32B models.", "description": "This figure shows the performance (y-axis) versus training compute (x-axis) for language models with 1.5B, 6B, and 32B parameters on 12 downstream tasks.  It complements Figure 1, which showed performance against pre-training loss.  The purpose is to compare the predictability of model performance using training compute versus pre-training loss, showing pre-training loss is a better predictor.  Each point represents a model checkpoint during training.", "section": "2 Does Pre-training Loss Predict Task Performance?"}, {"figure_path": "35DAviqMFo/figures/figures_22_1.jpg", "caption": "Figure 7: The performance-vs-loss curves of Pythia. The performance of Pythia is from the official repository and the loss is evaluated with the released checkpoints.", "description": "This figure displays the performance-versus-loss curves for different sizes of Pythia language models across six downstream tasks. Each point represents a model checkpoint. The x-axis shows the training loss, and the y-axis represents the model's performance. This figure supports the paper's claim that pre-training loss is a good predictor of model performance, irrespective of model size.", "section": "2.5 LLAMA's Loss vs. Performance"}, {"figure_path": "35DAviqMFo/figures/figures_22_2.jpg", "caption": "Figure 8: The performance-vs-loss curves of 1.5B, 6B, and 32B models on 3 tasks in BIG-bench. Each data point is the loss (x-axis) and performance (y-axis) of the intermediate checkpoint of one of the three models. We mark the results of random guess in black dashed lines.", "description": "This figure displays the performance against pre-training loss curves for three different sized models (1.5B, 6B, and 32B parameters) across three BIG-bench tasks: word unscramble, modular arithmetic, and IPA transliteration.  Each point represents an intermediate checkpoint during training. The black dashed line indicates the performance level of random guessing. The figure demonstrates the relationship between pre-training loss and model performance on these tasks, highlighting the point at which performance moves beyond random chance.", "section": "3.1 Performance Trends of Different Tasks"}]