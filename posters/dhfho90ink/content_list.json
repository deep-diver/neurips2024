[{"type": "text", "text": "Implicitly Guided Design with PropEn: Match your Data to Follow the Gradient ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Nata\u0161a Tagasovska \u2217 Vladimir Gligorijevic\u00b4 \u2217 Kyunghyun Cho \u2020\u2217 Andreas Loukas \u2217 ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Across scientific domains, generating new models or optimizing existing ones while meeting specific criteria is crucial. Traditional machine learning frameworks for guided design use a generative model and a surrogate model (discriminator), requiring large datasets. However, real-world scientific applications often have limited data and complex landscapes, making data-hungry models inefficient or impractical. We propose a new framework, PropEn, inspired by \u201cmatching\u201d, which enables implicit guidance without training a discriminator. By matching each sample with a similar one that has a better property value, we create a larger training dataset that inherently indicates the direction of improvement. Matching, combined with an encoder-decoder architecture, forms a domain-agnostic generative framework for property enhancement. We show that training with a matched dataset approximates the gradient of the property of interest while remaining within the data distribution, allowing efficient design optimization. Extensive evaluations in toy problems and scientific applications, such as therapeutic protein design and airfoil optimization, demonstrate PropEn\u2019s advantages over common baselines. Notably, the protein design results are validated with wet lab experiments, confirming the competitiveness and effectiveness of our approach. Our code is available at https://github.com/prescient-design/propen. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Navigating the complex world of design is a challenge in many fields, from engineering [35] to material science [33, 40] and life sciences [6]. In life sciences, the goal may be to refine molecular structures for drug discovery [6], focusing on properties like binding affinity or stability. In engineering, optimizing the shapes of aircraft wings or windmill blades to achieve desired aerodynamic traits like lift and drag forces is crucial [35]. The common thread in these fields is the design cycle: experts start with an initial design and aim to improve a specific property. Guided by intuition and expertise, they make adjustments and evaluate the changes. If the property improves, they continue this iterative optimization process. This cycle is repeated multiple times, making it time-consuming and resource-intensive. ML holds promise to reduce these costs, speed up design cycles, and create better-performing designs [3, 31, 44, 22]. ", "page_idx": 0}, {"type": "text", "text": "Yet, progress in ML methods for design is hindered by practical challenges. The first challenge is limited data availability. Since gathering label measurements is resource intensive [46, 28, 50, 19], designers are more often than not constrained to very small-scale datasets. In addition, there are often non-smooth functional dependencies between the features and outcome, complicating approximation, even for deep neural networks [30, 45]. Traditional methods use two part frameworks, requiring discriminators to guide the property enhancement for examples produced by a generative model. Such discriminators should be able to reliably predict the property of interest given some training data or its latent representations. Because of the dependency on training a discriminator for guidance, ", "page_idx": 0}, {"type": "image", "img_path": "dhFHO90INk/tmp/6489dfa14e9c99424b2d786201fa2366f6fdccfcb18c02230978a261c924717b.jpg", "img_caption": ["Step 1: Train a generative model Step 2: Train a property predictor Step 3: Guide optimization in latent space "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Conceptual summary of implicit and explicit guidance. The task is increasing the size of the objects. Top - in implicit guidance, first we match the training dataset, by pairing each sample with the closest one w.r.t. its shape, which has a better property (size). Then, we train a encoder-decoder framework which due to the construction of the dataset learns a lower dimensional manifold where the embeddings are ordered by the property value. Bottom - in explicit guidance, we train two separate models: a generator and a discriminator that guides the optimization in latent space. ", "page_idx": 1}, {"type": "text", "text": "we denote these methods as explicit guidance. While Genetic Algorithms were once prevalent [14], contemporary models like auto-encoders [47], GANs [49], and diffusion models now dominate both research and practice [31] the role of generative models. Despite their flexibility, such models face challenges typical to deep learning: they are \u201cdata-hungry\u201d and unreliable when encountering out-of-distribution examples [29, 21, 7, 37]. ", "page_idx": 1}, {"type": "text", "text": "Motivated by these challenges, we propose a new approach inspired by the concept of \u201cmatching\u201d. Matching techniques in econometrics are used to address the challenge of selection bias and confounding when estimating causal effects in observational studies [2, 38, 39, 43]. These techniques aim to create comparable groups of units by matching treated and control observations based on observable characteristics. The basic idea behind matching is to identify untreated units that are similar to treated units in terms of observed covariates, effectively creating a counterfactual comparison group for each treated unit. Matching techniques in ML as in econometrics have only been used to provide more robust, reliable causal-effect estimation [24, 48, 9]. ", "page_idx": 1}, {"type": "text", "text": "This work argues that, in lack of large datasets, matching allows for implicit guidance, completely sidestepping the need for training a discriminator (differentiable surrogate model). We match each sample with a similar one that has a superior value for the property of interest. By doing so, we obtain a much larger training dataset, inherently embedding the direction of property enhancement. We name our method PropEn and we illustrate it in Figure 1. By leveraging this expanded dataset within a standard encoder-decoder framework, we circumvent the need for a separate discriminator model. We show that PropEn is domain agnostic, can be applied to any data modality continuous or discrete. Additionally, PropEn alleviates some common problems with explicit guidance such as falling off the data manifold or requiring complex engineering. ", "page_idx": 1}, {"type": "text", "text": "Overall, our contributions are as follows: ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "\u2022 We propose \u201cmatching\u201d (inspired by causal effect estimation) to expand small training datasets (subsection 2.1); \u2022 We provide a theoretical analysis on how training on a matched dataset implicitly learns an approximation of the gradient for a property of interest (subsection 2.2); ", "page_idx": 1}, {"type": "text", "text": "\u2022 We provide guarantees that the proposed designs are as likely as the training distribution, avoiding common pitfalls where unreliable discriminators lead to unrealistic, pathological designs (subsection 2.3);   \n\u2022 We demonstrate the effectiveness and advantages of implicit guidance through extensive experiments in both toy and real-world scientific problems, using both numerical and wet lab validation (section 3). ", "page_idx": 2}, {"type": "text", "text": "2 Property Enhancer (PropEn) ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given a set of initial examples, our objective is to propose a new design which is similar to the initial set, but, exceeds it in some property value of interest. ", "page_idx": 2}, {"type": "text", "text": "Problem setup. Concretely, we start with a dataset $D\\,=\\,\\{x_{i},y_{i}\\}_{i=1}^{n}$ consisting of $n$ observed examples $x_{i}\\in\\mathbb{R}^{m}$ drawn from a distribution $p$ together with their corresponding properties $y_{i}=$ $g(x_{i})\\overset{.}{\\in}\\mathbb{R}$ . Our objective is to determine ways to improve the property of a test example. Concretely, at test time, we are given a point $x_{0}$ and aim to identify some new point $x^{n e w}\\sim p$ close to $x_{0}$ such that $g(x^{n e w})>g(x_{0})$ . In effect, our problem combines constrained optimization (maximize $g(x^{n e w})$ while staying close to $x_{0}$ ) with sampling from a distribution (point $x^{n e w}$ should be likely according to $p$ ). ", "page_idx": 2}, {"type": "text", "text": "Hereafter, we will refer to the initial example we wish to optimize as seed design and the model\u2019s proposal as candidate design. Our method, PropEn, entails three steps: (i) matching a dataset, (ii) approximating the gradient by training a model with a matched reconstruction objective and (iii) sampling with implicit guidance. ", "page_idx": 2}, {"type": "text", "text": "2.1 Match the dataset ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We view the group of samples with superior property values as the treated group and their lower-value counter part as the control group. This motivates us to construct a \u201cmatched dataset\u201d for every $(x,y)$ within $\\mathcal{D}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{M}=\\left\\{(x,x^{\\prime})\\;\\middle|\\;\\begin{array}{l}{x,x^{\\prime}\\in\\mathcal{D}}\\\\ {\\|x^{\\prime}-x\\|^{2}\\leq\\Delta_{x},\\;g(x^{\\prime})-g(x)\\in(0,\\Delta_{y}]}\\end{array}\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\Delta_{x}$ and $\\Delta_{y}$ are predefined positive thresholds. ", "page_idx": 2}, {"type": "text", "text": "The matched dataset gives us a new and extended collection $\\mathcal{M}$ whose size $N=O(n^{2})\\gg n$ can significantly exceed that of the training set, depending on the choice of matching thresholds. ", "page_idx": 2}, {"type": "text", "text": "2.2 Approximate the gradient ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "After matching the dataset, we train a deep encoder-decoder network $f_{\\theta}$ over $\\mathcal{M}$ by minimizing the matched reconstruction objective: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\ell(f_{\\theta};\\mathcal{M})=\\frac{1}{|\\mathcal{M}|}\\sum_{(x,x^{\\prime})\\in\\mathcal{M}}\\ell(f_{\\theta}(x),x^{\\prime}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "(matched reconstruction objective) ", "page_idx": 2}, {"type": "text", "text": "where $\\ell$ is an appropriate loss for the data in question, such as an mean-squared error (MSE) or cross-entropy loss. ", "page_idx": 2}, {"type": "text", "text": "Before illustrating the properties of our method empirically, we perform a theoretical analysis. We show that minimizing the matched reconstruction objective yields a model that approximates the direction of the gradient of $g(\\cdot)$ , even if no property predictor has been explicitly trained: ", "page_idx": 2}, {"type": "text", "text": "Theorem 1. Let $f^{*}$ be the optimal solution of the matched reconstruction objective with a sufficiently small $\\Delta_{x}$ . For any point $x$ in the matched dataset for which $p$ is uniform within a ball of radius $\\Delta_{x}$ , we have $f^{*}(x)\\rightarrow c\\nabla g(x)$ for some positive constant $c$ . ", "page_idx": 2}, {"type": "text", "text": "The detailed proof is provided in subsection A.1. ", "page_idx": 2}, {"type": "text", "text": "Remark 1. The proof of 1 is founded on the assumption that distribution is uniformly distributed within a ball of radius $\\Delta_{x}$ around point $x$ . This assumption is made to maintain the generality of the theorem without specific information about the sampling distribution, assuming uniformity avoids ", "page_idx": 2}, {"type": "image", "img_path": "dhFHO90INk/tmp/de749cf5795c383cbdd5500b9a05058e522cdde07450c1fa742303cd66f3a875.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: Illustration of PropEn on the pinwheel toy example with only 72 training examples. The training data are circles in grey, colored by the value of the property. With pink we mark the initial hold out test points and in orange $\\mathbf{\\dot{\\bar{\\rho}}}\\times\\mathbf{\\bar{\\rho}}$ the PropEn trajectories. The color of the candidates intensifies with each iteration step. On the right-hand-side, we depict the sum of negative log likelihoods of the seeds and optimized designs across optimization steps. ", "page_idx": 3}, {"type": "text", "text": "introducing any biases that could arise from other distributional assumptions, such as symmetry, finite variance etc. ", "page_idx": 3}, {"type": "text", "text": "Remark 2. It can be shown that with with a matched reconstruction objective we learn a direction that is $a$ -colinear with the gradient of $g$ , avoiding the isotropy assumption. This leads to additional analysis on understanding the implications of the choices while matching, all included in Appendix A. ", "page_idx": 3}, {"type": "text", "text": "2.3 Optimize designs with implicit guidance ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Training on a matched dataset allows for auto-regressive sampling. Starting with a design seed $x_{0}$ , for $t=1,2,\\ldots$ , we can generate $x_{t}=f_{\\theta}(x_{t-1})$ until convergence, $f_{\\theta}(x_{t})=x_{t}$ , s.t. $g(\\bar{x}_{t})>g(x_{t-1})$ At test time, we feed a seed design $x_{0}$ to PropEn, and read out an optimized design $x_{1}$ from its output. We then proceed to iteratively re-feed the current design to PropEn until $f_{\\theta}(x_{t})=x_{t}$ , which is analogous to arriving at a stationary point with $\\nabla g(x_{t})=0$ and we have exhausted the direction of property enhancement given the training data. Exploiting the implicit guidance from matching results in a trajectory of multiple optimized candidate designs. ", "page_idx": 3}, {"type": "text", "text": "We next show that optimized samples are almost as likely as our training set according to the data distribution $p$ . This serves as a guarantee that the generated designs lie within distribution, as desired: Theorem 2. Consider a model $f^{*}$ trained to minimize the matched reconstruction objective. The probability of $f^{*}(x)$ is at least ", "page_idx": 3}, {"type": "equation", "text": "$$\np(f^{*}(x))\\geq\\mathbb{E}_{x^{\\prime}\\sim\\hat{\\mu}_{x}}[p(x^{\\prime})]-\\frac{\\|H_{p}(f(x))\\|_{2}\\,\\sigma^{2}(\\mathcal{M}_{x})}{2},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\hat{\\mu}_{x}$ is the empirical measure on the dataset, $H_{p}(x)$ is the Hessian of $p$ at $x$ and $\\sigma^{2}(\\mathcal{M}_{x})=$ $\\mathbb{E}_{x^{\\prime}\\sim\\hat{\\mu}_{x}}[\\|x^{\\prime}-\\mathbb{E}_{x^{\\prime\\prime}\\sim\\hat{\\mu}_{x}}[x^{\\prime\\prime}]\\|_{2}^{2}]$ is the variance induced by the matching process. ", "page_idx": 3}, {"type": "text", "text": "The detailed proof is provided in subsection A.3. ", "page_idx": 3}, {"type": "text", "text": "We use a synthetic example to illustrate optimizing designs with PropEn. We choose a 2d pinwheel dataset. As a property to optimize, we choose the log-likelihood of the data as estimated by a KDE with Gaussian kernel with $\\sigma\\,=\\,0.01$ . Figure 2 depicts in gray the training points, with the color intensity representing the value of the property\u2014hence a higher/darker value is better in this example. After training PropEn, we take held out points (pink squares) and use them as seed designs. With orange x-markers, we illustrate PropEn candidates, with the color intensity increasing at each step $t$ We notice that PropEn moves towards the regions of the training data with highest property value, consistently improving at each step (right-most panel). Additionally, we also use out-of-distribution seeds, and we demonstrate in the middle panel that PropEn chooses to optimize them by proposing designs from the closest regions in the training data. ", "page_idx": 3}, {"type": "table", "img_path": "dhFHO90INk/tmp/7747f4fed316b5563996f4e5cc78bff5ac2746958344b30eb7be1a51b65ddcc7.jpg", "table_caption": ["Table 1: Overview of the datasets in experiments. "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "3 Experimental Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We empirically evaluate PropEn on synthetic and real-world experiments to answer the following main questions: (i) Can PropEn be applied across various domains and datasets? (ii) Does PropEn provide reliable guidance, especially in situations with limited data and when dealing with out-ofdistribution examples? (iii) How effective is PropEn in recommending optimal designs? Can it suggest candidates with property values exceeding those in the training set? (iv) How does PropEn\u2019s performance vary with different data characteristics (e.g., dimensionality, sample size, heterogeneity) and hyperparameters (such as $\\Delta_{x}$ , $\\Delta_{y}$ , and regularization terms)? Our code is available at https: //github.com/prescient-design/propenhttps://github.com/prescient-design/propen. ", "page_idx": 4}, {"type": "text", "text": "Datasets. We consider three different data types: synthetic 2d toy datasets and their higher dimension transformations, NACA airfoil samples, and therapeutic antibody proteins. An overview of the data is given in Table 1. We present our results in two settings, in silico where we rely on experimental validation using computer simulations and solvers, and in vitro experiments where candidate designs were tested in a wet lab. Each of the experiments is evaluated under the baselines and metrics suitable for the domain. ", "page_idx": 4}, {"type": "text", "text": "PropEn variants. We investigate the utilization of matching and reconstruction within the PropEn framework. Two key considerations emerge: first, whether to reconstruct solely the input features $(\\mathbf{x}2\\mathbf{x})$ or both the input features and the property $(\\mathrm{xy2xy})$ ; second, the proximity to the initial sample, regulated by incorporating a straightforward reconstruction regularizer into the training loss $\\bar{\\ell}(f_{\\theta}\\bar{(}x),x\\bar{)}$ . This regularized variant will be referred to as mixup/mix. ", "page_idx": 4}, {"type": "text", "text": "3.1 In silico experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "3.1.1 Toy data ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We choose two well-known multi-modal densities: pinwheel and 8-Gaussians. These are 2d datasets, but, in order to make the task more challenging, we expand the dimensionality to $d\\in\\{10,50,100\\}$ by randomly isometrically embedding the data within a higher dimensional space. Our findings are summarized in Figure 3 and we include the tabular results in item B.7. We empirically validate the four variants of PropEn and we compare against explicit guidance method: for consistency, we chose an auto-encoder of the same architecture as PropEn augmented with a discriminator for guidance in the latent space. We denote this baseline Explicit. We compare the methods by ratio of improvement , the proportion of holdout samples for which PropEn or baselines demonstrate enhanced property values. To assess the quality of the generated samples we report uniqueness and novelty in tables. We use a likelihood model derived from a Kernel Density Estimation (KDE) fti on the training data. The negative log-likelihood scores under this model serve as an indicator of in-distribution performance. Higher values for all metrics indicate better performance. ", "page_idx": 4}, {"type": "text", "text": "Results. Several insights can be gleaned from these experiments. When analyzing the results based on the number of samples, a clear trend emerges: as the number of training samples increases, PropEn consistently outperforms explicit guidance across all metrics, except for average improvement, where all methods exhibit similar behavior. The choice of the preferred metric may vary depending on the specific application; however, it is noteworthy that while explicit AE guidance improves approximately $50\\%$ of the designs for all datasets, PropEn demonstrates the potential to enhance up to $85\\%$ of the designs. Importantly, this improvement trend remains consistent regardless of the dimensionality of the data. Furthermore, an intriguing observation pertains to the performance of different variations of PropEn. It is noted that as the sample size increases, PropEn xy2xy does not exhibit an advantage over PropEn $\\mathbf{x}2\\mathbf{x}$ . Moreover, the impact of iterative sampling with PropEn is notable. With each step, the property improves until it reaches a plateau after multiple iterations, albeit with a simultaneous drop in the uniqueness of the solution to around $80\\%$ . Nevertheless, iterative optimization can be continued until convergence, with all designs saved along the trajectory for later filtering according to user needs. ", "page_idx": 4}, {"type": "image", "img_path": "dhFHO90INk/tmp/17fc178b63e961264092da00580321f941d6c887efc762a5d9a6430ad4e33daf.jpg", "img_caption": ["Figure 3: PropEn in toy examples in $d\\in\\{50,100\\}$ , left side: 8-Gaussians, right side: pinwheel. Distribution of evaluation metrics from 10 repetitions of each experiment. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "3.1.2 Engineering: airfoil optimization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In an engineering context, shape optimization entails altering the shape of an object to enhance its efficiency. NACA airfoils (National Advisory Committee for Aeronautics) [4], rooted in aerodynamics and parameterized by numerical values, serve as a well-documented benchmark due to their versatility. These airfoils span diverse aerodynamic characteristics, from high lift to low drag, making them ideal for exploring different optimization objectives. Given their integral role in aircraft wings, optimizing airfoil shapes can significantly impact aerodynamic performance by improving lift, drag, and other properties essential for aerospace engineering. ", "page_idx": 5}, {"type": "text", "text": "Data $\\&$ experimental design. We generate NACA 4-digit series airfoil coordinates by choosing these parameters: $M$ (maximum camber percentage), $P$ (location of maximum camber percentage), and $T$ (maximum thickness percentage). Each airfoil is represented by 200 coordinates, resulting in a 400 vector representation when flattened. Our objective is to optimize the lift-to-drag ratio $(C_{l}/C_{d}$ ratio) for each shape. We calculate lift and drag using NeuralFoil [41], a precise deep-learning emulator of XFoil [11]. ", "page_idx": 5}, {"type": "text", "text": "Note that the lift-to-drag ratio is pivotal in aircraft design, reflecting the wing\u2019s lift generation efficiency relative to drag production. A high value signifies superior lift production with minimal drag, translating to enhanced fuel efficiency, extended filght ranges, and overall improved performance. This ratio is paramount in aerodynamic design and optimization, facilitating aircraft to travel farther and more efficiently through the air. Traditionally, engineers have relied on genetic algorithms guided by Gaussian Process models (kriging) [22], however, in recent years the community has moved towards ML-based methods which consist of a generative model that can be GAN-based [51, 49] or a variation of a VAE [26, 52, 31, 47]. Similarly, for the surrogate, guidance model, GPs and numerical solvers have been replaced by deep models [5, 41]. For our experiments we follow this standard setup: we choose a VAE-like baseline as it is the most similar architectural choice to PropEn, and for guidance we use a MLP. All networks (encoder, decoder and surrogate) are fully connected 3 layer MLPs with 50 units, ReLU activations per layer and a latent space of dimension 50. We randomly select $0.1\\%$ as holdout dataset for seeds, and use the rest for training. ", "page_idx": 5}, {"type": "text", "text": "Results. Our numerical findings are summarized in ??. Similar to the toy dataset, the designs produced by PropEn variants demonstrate enhanced properties compared to those guided explicitly. Delving into further analysis with ablation studies, depicted in Figure 4(b) and (c), we observe that increasing the matching thresholds in PropEn correlates with higher rates of improvement. Remarkably, all designs within a PropEn trajectory are deemed plausible, as depicted in the accompanying figure. Moreover, a consistent enhancement in the lift-to-drag ratio $C_{l}/C_{d}$ is noted along the optimization trajectory until convergence. This consistent trend underscores the effectiveness of PropEn in progressively refining airfoil designs to bolster their aerodynamic performance. ", "page_idx": 5}, {"type": "text", "text": "Interestingly, we find that in larger training datasets the threshold for property improvement $(\\Delta_{y})$ may not be necessary for optimization, as the PropEn $\\mathbf{x}2\\mathbf{x}$ demonstrates satisfactory performance. ", "page_idx": 5}, {"type": "image", "img_path": "dhFHO90INk/tmp/99c0b265d619fc414e6cc71481e4fbea7e6f27d17e328606b7c84822e060b0a9.jpg", "img_caption": ["Figure 4: Ablation studies for PropEn on Airfoils. (a) PropEn improves $\\mathrm{Cl/Cd}$ ratio along its trajectory and produces realistic/valid airfoil shapes. (b and c) the impact of choice of threshold $\\Delta_{x}$ and $\\Delta_{y}$ in the matching phase. When varying $\\Delta_{x}$ , $\\Delta_{y}$ is set to 1, and vice versa. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "We also notice that the mix variant of PropEn may require longer training. This observation is consistent with the notion that mix, by introducing a regularization term in the training loss, may improve at a slower rate compared to other PropEn variants. This slower improvement can be attributed to the regularization term\u2019s tendency to pull generated designs closer to the initial seed, thereby limiting the extent of exploration in the design space. ", "page_idx": 6}, {"type": "text", "text": "3.2 In vitro experiment: therapeutic protein optimization ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The design of antibodies good functional and developability properties is essential for developing effective treatments for diseases ranging from cancer to autoimmune disorders. We here focus on the task of optimizing the binding affinity of a starting antibody (the seed) while staying close to it in terms of edit distance. The task is referred to as affinity maturation in the drug design literature and constitutes an essential and challenging step in any antibody design campaign. ", "page_idx": 6}, {"type": "text", "text": "Antibody binding affinity refers to the strength of the interaction between an antibody molecule and its target antigen. High binding affinity is crucial in antibody-based therapeutics as it determines the antibody\u2019s ability to recognize and bind to its target with high specificity and efficiency. We follow the standard practice of quantifying binding affinity by the negative log ratio of the association and dissociation constants (pKD), which represents the concentration of antigen required to dissociate half of the bound antibody molecules. Higher pKD indicates a tighter and more stable interaction, leading to improved therapeutic outcomes such as enhanced neutralization of pathogens or targeted delivery of drugs to specific cells. ", "page_idx": 6}, {"type": "text", "text": "Data & experimental design. The data collection process involved conducting low-throughput Surface Plasmon Resonance (SPR) experiments aimed at measuring with high accuracy the binding affinity of antibodies targeting three different target antigens: the human epidermal growth factor receptor 2 (HER2) and two additional targets that we denote as T1 and T2. For each of those targets, one or more seed designs were selected by domain experts. In the case of HER2, we used the cancer drug Herceptin as seed. We ensured the correctness of the SPR measurements by validating the fti of the SPR kinetic curves according to standard practices. ", "page_idx": 6}, {"type": "text", "text": "As the targets differ in the properties of their binding sites, we trained a PropEn model per each target (but for all seeds for that target jointly). For this application, we opted for the PropEn $\\mathbf{x}2\\mathbf{x}$ mix variant. The reconstruction of the original sequence (mix) complies with antibody engineering wisdom that a candidate design should not deviate from a seed by more than small number of mutations. Similar to [34], we used a one-hot encoded representation of antibodies aligned according to the AHo numbering scheme [20] determined by ANARCI [12]. The encoder-decoder architecture is based on a ResNet [18] with 3 blocks each. We compare PropEn with four strong baselines: two state-of-the-art methods for guided and unguided antibody design namely walk-jump sampler [13] and lambo [17]; as well as two variants of a diffusion model trained on AHo antibody sequences differing on their use of guidance. The first one (labeled as diffusion) is based on a variational diffusion model [25] trained on a latent space obtained by projecting AHo 1-hot representation using an encoder-decoder type of architecture similar to PropEn\u2019s architecture; encoder-decoder model is trained simultaneously with the diffusion model. The second one (labeled as diffusion(guided)) is a variant of the first one with added guidance based on the iterative latent variable refinement idea described in the paper by Choi et al. [8], which ensures generating samples that are close to the initial seed. ", "page_idx": 6}, {"type": "image", "img_path": "dhFHO90INk/tmp/e42a974b4ddb39e24070327c7415d07e22a998601f44677ebf99789564c688f8.jpg", "img_caption": ["Figure 5: Therapeutic protein optimization results: (a) The left figure contrasts the binding rate with the 90-th percentile of the binding affinity improvement for each method and seed. Points on the top-right are on the Pareto front. (b) The right figure focuses on binders and reports the histograms of binding affinity improvement across all designs and seeds. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "dhFHO90INk/tmp/4f04aaedd9d1694b3382f112a889746b530cd1268e78de63d801db868269eb51.jpg", "table_caption": ["Table 2: Binding rate (and number of designs submitted). Higher is better. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "We evaluate the set of designs in terms of their binding rate (fraction of designs tested that were binders), the percentage of designs than improve the seed, and their binding affinity improvement (pKD design - pKD of seed). ", "page_idx": 7}, {"type": "table", "img_path": "dhFHO90INk/tmp/a7589528ab02dac6c46b0266f37132403737c28e1ce299f71ee0bf4d9b206ba9.jpg", "table_caption": ["Table 3: Fraction of designs improving the seed and total designs tested. Higher is better. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Results. As seen in Tables 2 and 3, PropEn excelled in generating functional antibodies with consistently high binding rates $(94.6\\%)$ and $34.5\\%$ of the tested designs showed improve binding than the seed, outpacing other models in overall performance. To account for the trade-off between binding rate and affinity improvement (larger affinity improvement requires making risky mutations that might end-up killing binding), we visualize the Pareto front in Figure 5(a). In the plot, we mark the performance of each method for a specific seed design by placing a marker based on the achieved binding rate $\\bf\\chi$ -axis) and maximum affinity improvement (y-axis). Compared to baseline methods, PropEn struck a beneficial trade-off, on average achieving a larger affinity improvement than methods with a high binding rate. ", "page_idx": 7}, {"type": "text", "text": "Figure 5(b) takes a closer look at the affinity improvement on the subset of designs that bound. As observed, all models produced some binders that were better than the seed, speaking for the strength of all considered models. Interestingly, none the top three models in terms of binding affinity improvement relied on explicit guidance, which aligns with our argument about the brittleness of explicit guidance in low-data regimes. Out of the those three models, PropEn generated two designs that improved the seed by at least one pKD unit (10 times better binder) followed by the walk-jump and the unguided diffusion model, that generated one such design each. ", "page_idx": 7}, {"type": "text", "text": "4 Related work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "As design optimization has been ubiquitous across science domains, naturally our approach relates to a variety of methods and applications. In the molecular design domain, data are bound to discrete representation which can be challenging for ML methods. A natural way to circumvent that is by optimization in a latent continuous space. Bombarelli et al. [15] presented such an approach in their seminal work on optimizing molecules for chemical properties, and it has since spawn across different domains [6]. Recently, one of the common way for obtaining embeddings for explicit guidance relies on language models [27, 32]. One of the challenges of using a latent space is the issue for blindly guiding the optimization into ambiguous regions of the manifold where no training data was available [21, 29]. Follow up works attempt to address this problem [7, 16, 37] by incorporating uncertainty estimates into black-box optimization. ", "page_idx": 8}, {"type": "text", "text": "Another line of work perhaps more close to our approach is the notion of neural translation, where the goal is to go from one language to another by training on aligned datasets. [23, 10] have build on this idea to improve properties for small molecules translating one graph or sequence to another one with better properties. These works propose tailored approaches for domain specific applications. With PropEn we take a step further and propose a domain-agnostic framework that is empirically validated across different domains (uniquely including wetlab experiments). We also derive novel theoretical guarantees that illustrate the relation of the generated samples with the property gradient, as well as provide guarantees that our designs fall within distribution. ", "page_idx": 8}, {"type": "text", "text": "Previous works have also considered learning an optimizer for some function based on observed samples [42, 1, 36]. This is usually achieved by either (i) rendering the optimizer differentiable and training some of its hyperparameters; or (ii) by unfolding the iterative optimizer and treating each iteration as a trainable layer. Our approach is different, thanks to the matched reconstruction objective that lets us implicitly approximate the gradient of a function of interest. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Strengths. We introduced PropEn, a new method for implicit guidance in design optimization that approximates the gradient for a property of interest. We achieve this by leveraging matched datasets, which increase the size of the training data and inherently include the direction of property enhancement. Our findings highlight the versatility and effectiveness of PropEn in optimizing designs in engineering and drug discovery domains. We include wet lab in-vitro results for comparison with state-of-the-art baselines in therapeutic protein design. By utilizing thresholds for shape dissimilarity and property improvement, PropEn efficiently navigates the design space, generating diverse and high-performance configurations. We believe our method offers a simple yet effective recipe for design optimization that can be applied across various scientific domains. ", "page_idx": 8}, {"type": "text", "text": "Limitations. The matching step adds some computation overhead with complexity depending on the choice of distance metric. Since PropEn is targeting low data-regime applications, scalability is out of scope for the current work. However, we are considering on-the-fly distance evaluation or parallelisation across multiple nodes. The choice of distance metric for matching to a certain extent, can be considered a limitation because it requires understanding of the context for the application. However, this choice is also what allows for incorporating domain knowledge and constraints, which can be meaningful and necessary in the domain of interest (edit distance for antibodies, deviations only in the camber of the airfoil etc). ", "page_idx": 8}, {"type": "text", "text": "Future work. Immediate extensions of PropEn are applications to other molecular modalities, such as small molecules, material discovery, and optimization of adeno-associated virus vectors for gene-therapy. Additionally, we are keen to explore how different similarity metrics incorporate various inductive biases that can be leveraged for property optimization. Ongoing worthwhile efforts include developing a multi-property PropEn framework to address the optimization of properties simultaneously, offering a more comprehensive approach to the design process. ", "page_idx": 8}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "[1] Akshay Agrawal, Shane Barratt, Stephen Boyd, and Bartolomeo Stellato. Learning convex optimization control policies. In Learning for Dynamics and Control, pages 361\u2013373. PMLR, ", "page_idx": 8}, {"type": "text", "text": "2020. [2] Robert P Althauser and Donald Rubin. The computerized construction of a matched sample. American Journal of Sociology, 76(2):325\u2013346, 1970. [3] Prasanna V Balachandran, Benjamin Kowalski, Alp Sehirlioglu, and Turab Lookman. Experimental search for high-temperature ferroelectric perovskites guided by two-step machine learning. Nature communications, 9(1):1668, 2018.   \n[4] Roger E Bilstein. Orders of Magnitude: A History of the NACA and NASA, 1915-1990, volume 4406. National Aeronautics and Space Administration, Office of Management ..., 1989. [5] Mohamed Amine Bouhlel, Sicheng He, and Joaquim RRA Martins. Scalable gradient\u2013enhanced artificial neural networks for airfoil shape design in the subsonic and transonic regimes. Structural and Multidisciplinary Optimization, 61(4):1363\u20131376, 2020. [6] John Bradshaw, Brooks Paige, Matt J Kusner, Marwin Segler, and Jos\u00e9 Miguel Hern\u00e1ndezLobato. A model to search for synthesizable molecules. Advances in Neural Information Processing Systems, 32, 2019.   \n[7] David Brookes, Hahnbeom Park, and Jennifer Listgarten. Conditioning by adaptive sampling for robust design. In International conference on machine learning, pages 773\u2013782. PMLR, 2019.   \n[8] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. Ilvr: Conditioning method for denoising diffusion probabilistic models. arXiv preprint arXiv:2108.02938, 2021. [9] Oscar Clivio, Fabian Falck, Brieuc Lehmann, George Deligiannidis, and Chris Holmes. Neural score matching for high-dimensional causal inference. In International Conference on Artificial Intelligence and Statistics, pages 7076\u20137110. PMLR, 2022.   \n[10] Farhan Damani, Vishnu Sresht, and Stephen Ra. Black box recursive translations for molecular optimization. arXiv preprint arXiv:1912.10156, 2019.   \n[11] Mark Drela. Xfoil: An analysis and design system for low reynolds number airfoils. In Low Reynolds Number Aerodynamics: Proceedings of the Conference Notre Dame, Indiana, USA, 5\u20137 June 1989, pages 1\u201312. Springer, 1989.   \n[12] James Dunbar and Charlotte M. Deane. ANARCI: antigen receptor numbering and receptor classification. Bioinformatics, 32(2):298\u2013300, 09 2015.   \n[13] Nathan C. Frey, Dan Berenberg, Karina Zadorozhny, Joseph Kleinhenz, Julien LafranceVanasse, Isidro Hotzel, Yan Wu, Stephen Ra, Richard Bonneau, Kyunghyun Cho, Andreas Loukas, Vladimir Gligorijevic, and Saeed Saremi. Protein discovery with discrete walk-jump sampling. In The Twelfth International Conference on Learning Representations, 2024.   \n[14] Mitsuo Gen and Runwei Cheng. Genetic algorithms and engineering optimization, volume 7. John Wiley & Sons, 1999.   \n[15] Rafael G\u00f3mez-Bombarelli, Jennifer N Wei, David Duvenaud, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Benjam\u00edn S\u00e1nchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, and Al\u00e1n Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. ACS central science, 4(2):268\u2013276, 2018.   \n[16] Ryan-Rhys Grifftihs and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. Constrained bayesian optimization for automatic chemical design using variational autoencoders. Chemical science, 11(2):577\u2013586, 2020.   \n[17] Nate Gruver, Samuel Don Stanton, Nathan C. Frey, Tim G. J. Rudner, Isidro Hotzel, Julien Lafrance-Vanasse, Arvind Rajpal, Kyunghyun Cho, and Andrew Gordon Wilson. Protein design with guided discrete diffusion. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[19] Jordan Hoffmann, Yohai Bar-Sinai, Lisa M Lee, Jovana Andrejevic, Shruti Mishra, Shmuel M Rubinstein, and Chris H Rycroft. Machine learning in a data-limited regime: Augmenting experiments with synthetic data uncovers order in crumpled sheets. Science advances, 5(4):eaau6792, 2019.   \n[20] Annemarie Honegger and Andreas Plu\u00c8ckthun. Yet another numbering scheme for immunoglobulin variable domains: an automatic modeling and analysis tool. Journal of molecular biology, 309(3):657\u2013670, 2001.   \n[21] David Janz, Jos van der Westhuizen, and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. Actively learning what makes a discrete sequence valid. arXiv preprint arXiv:1708.04465, 2017.   \n[22] Jingchao Jiang. A survey of machine learning in additive manufacturing technologies. International Journal of Computer Integrated Manufacturing, 36(9):1258\u20131280, 2023.   \n[23] Wengong Jin, Kevin Yang, Regina Barzilay, and Tommi Jaakkola. Learning multimodal graph-to-graph translation for molecular optimization. arXiv preprint arXiv:1812.01070, 2018.   \n[24] Nathan Kallus. Deepmatch: Balancing deep covariate representations for causal inference using adversarial training. In International Conference on Machine Learning, pages 5067\u20135077. PMLR, 2020.   \n[25] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. Advances in neural information processing systems, 34:21696\u201321707, 2021.   \n[26] Jiaqing Kou, Laura Botero-Bol\u00edvar, Rom\u00e1n Ballano, Oscar Marino, Leandro De Santana, Eusebio Valero, and Esteban Ferrer. Aeroacoustic airfoil shape optimization enhanced by autoencoders. Expert Systems with Applications, 217:119513, 2023.   \n[27] Ben Krause, Subu Subramanian, Tom Yuan, Marisa Yang, Aaron Sato, and Nikhil Naik. Improving antibody affinity using laboratory data with language model guided design. bioRxiv, pages 2023\u201309, 2023.   \n[28] Rafa\u0142 Kurczab, Sabina Smusz, and Andrzej J Bojarski. The influence of negative training set size on machine learning-based virtual screening. Journal of cheminformatics, 6:1\u20139, 2014.   \n[29] Matt J Kusner, Brooks Paige, and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. Grammar variational autoencoder. In International conference on machine learning, pages 1945\u20131954. PMLR, 2017.   \n[30] Karolina Kwapien, Eva Nittinger, Jiazhen He, Christian Margreitter, Alexey Voronov, and Christian Tyrchan. Implications of additivity and nonadditivity for machine learning and deep learning models in drug design. ACS omega, 7(30):26573\u201326581, 2022.   \n[31] Jichao Li, Xiaosong Du, and Joaquim RRA Martins. Machine learning in aerodynamic shape optimization. Progress in Aerospace Sciences, 134:100849, 2022.   \n[32] Lin Li, Esther Gupta, John Spaeth, Leslie Shing, Rafael Jaimes, Emily Engelhart, Randolph Lopez, Rajmonda S Caceres, Tristan Bepler, and Matthew E Walsh. Machine learning optimization of candidate antibody yields highly diverse sub-nanomolar affinity antibody libraries. Nature Communications, 14(1):3454, 2023.   \n[33] Turab Lookman, Stephan Eidenbenz, Frank Alexander, and Cris Barnes. Materials discovery and design: By means of data science and optimal learning, volume 280. Springer, 2018.   \n[34] Karolis Martinkus, Jan Ludwiczak, WEI-CHING LIANG, Julien Lafrance-Vanasse, Isidro Hotzel, Arvind Rajpal, Yan Wu, Kyunghyun Cho, Richard Bonneau, Vladimir Gligorijevic, and Andreas Loukas. Abdiffuser: full-atom generation of in-vitro functioning antibodies. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[35] Joaquim RRA Martins. Aerodynamic design optimization: Challenges and perspectives. Computers & Fluids, 239:105391, 2022.   \n[36] Vishal Monga, Yuelong Li, and Yonina C Eldar. Algorithm unrolling: Interpretable, efficient deep learning for signal and image processing. IEEE Signal Processing Magazine, 38(2):18\u201344, 2021.   \n[37] Pascal Notin, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, and Yarin Gal. Improving black-box optimization in vae latent space using decoder uncertainty. Advances in Neural Information Processing Systems, 34:802\u2013814, 2021.   \n[38] Donald B Rubin. Matching to remove bias in observational studies. Biometrics, pages 159\u2013183, 1973.   \n[39] Donald B Rubin. The design versus the analysis of observational studies for causal effects: parallels with the design of randomized trials. Statistics in medicine, 26(1):20\u201336, 2007.   \n[40] Jonathan Schmidt, M\u00e1rio RG Marques, Silvana Botti, and Miguel AL Marques. Recent advances and applications of machine learning in solid-state materials science. npj computational materials, 5(1):83, 2019.   \n[41] Peter Sharpe. NeuralFoil: An airfoil aerodynamics analysis tool using physics-informed machine learning. https://github.com/peterdsharpe/NeuralFoil, 2023.   \n[42] Nir Shlezinger, Yonina C Eldar, and Stephen P Boyd. Model-based deep learning: On the intersection of deep learning and optimization. IEEE Access, 10:115384\u2013115398, 2022.   \n[43] Elizabeth A Stuart. Matching methods for causal inference: A review and a look forward. Statistical science: a review journal of the Institute of Mathematical Statistics, 25(1):1, 2010.   \n[44] Jessica Vamathevan, Dominic Clark, Paul Czodrowski, Ian Dunham, Edgardo Ferran, George Lee, Bin Li, Anant Madabhushi, Parantu Shah, Michaela Spitzer, et al. Applications of machine learning in drug discovery and development. Nature reviews Drug discovery, 18(6):463\u2013477, 2019.   \n[45] Derek van Tilborg, Alisa Alenicheva, and Francesca Grisoni. Exposing the limitations of molecular machine learning with activity cliffs. Journal of chemical information and modeling, 62(23):5938\u20135951, 2022.   \n[46] Derek van Tilborg, Helena Brinkmann, Emanuele Criscuolo, Luke Rossen, R\u0131za \u00d6z\u00e7elik, and Francesca Grisoni. Deep learning for low-data drug discovery: hurdles and opportunities. Current Opinion in Structural Biology, 86:102818, 2024.   \n[47] Jing Wang, Cheng He, Runze Li, Haixin Chen, Chen Zhai, and Miao Zhang. Flow field prediction of supercritical airfoils via variational autoencoder based deep learning framework. Physics of Fluids, 33(8), 2021.   \n[48] Tianyu Wang, Marco Morucci, M Usaid Awan, Yameng Liu, Sudeepa Roy, Cynthia Rudin, and Alexander Volfovsky. Flame: A fast large-scale almost matching exactly approach to causal inference. Journal of Machine Learning Research, 22(31):1\u201341, 2021.   \n[49] Yuyang Wang, Kenji Shimada, and Amir Barati Farimani. Airfoil gan: encoding and synthesizing airfoils for aerodynamic shape optimization. Journal of Computational Design and Engineering, 10(4):1350\u20131362, 2023.   \n[50] Zhenpeng Yao, Yanwei Lum, Andrew Johnston, Luis Martin Mejia-Mendoza, Xin Zhou, Yonggang Wen, Al\u00e1n Aspuru-Guzik, Edward H Sargent, and Zhi Wei Seh. Machine learning for a sustainable energy future. Nature Reviews Materials, 8(3):202\u2013215, 2023.   \n[51] Jianmiao Yi and Feng Deng. Cooperation of thin-airfoil theory and deep learning for a compact airfoil shape parameterization. Aerospace, 10(7):650, 2023.   \n[52] Kazuo Yonekura, Kazunari Wada, and Katsuyuki Suzuki. Generating various airfoil shapes with required lift coefficient using conditional variational autoencoders. arXiv preprint arXiv:2106.09901, 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Appendix / supplemental material ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A Deferred proofs ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Proof. The matched reconstruction objective for the MSE loss can be expressed as ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathop{\\mathrm{arg\\,min}}_{\\theta}\\sum_{x\\sim\\mathcal{X}}\\sum_{\\delta_{x}\\sim U(0,\\Delta_{x})^{m}}\\|f_{\\theta}(x)-(x+\\delta_{x})\\|^{2}\\,\\mathbb{1}(g(x)<g(x+\\delta_{x})).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Assuming a model that is sufficient overparametrized, we can w.l.o.g. suppose that there is some $\\theta_{x}$ which minimizes every $x$ and thus swap the sum and argmin. We are left with the objective of minimizing the inner sum: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\underset{\\theta_{x}}{\\arg\\operatorname*{min}}\\sum_{\\delta_{x}\\sim U(0,\\Delta_{x})^{m}}\\|f_{\\theta_{x}}(x)-(x+\\delta_{x})\\|^{2}\\,\\mathbb{1}(g(x)<g(x+\\delta_{x}))\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Noting that the the squared loss is minimized by the expected value and taking the data limit we get: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f_{\\theta_{x}}(x)=\\displaystyle\\frac{1}{C}\\sum_{\\delta_{x}\\sim\\mathcal{U}(0,\\Delta_{x})^{m}}\\delta_{x}\\,\\mathbb{1}(g(x)<g(x+\\delta_{x}))}\\\\ {\\underset{n\\to\\infty}{\\longrightarrow}\\mathbb{E}_{\\delta_{x}\\sim\\mathcal{U}(0,\\Delta_{x})^{m}}(\\delta_{x}\\mid g(x)<g(x+\\delta_{x}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where constant $C$ is needed because the expected value should only be computed on the non-zero terms. To proceed, we consider a rotation matrix $R_{x}$ for which the following holds: ", "page_idx": 12}, {"type": "equation", "text": "$$\nR_{x}\\nabla g(x)=\\left(\\begin{array}{c}{{\\|\\nabla g(x)\\|_{2}}}\\\\ {{0}}\\\\ {{\\vdots}}\\\\ {{0}}\\end{array}\\right)\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and introduce the reparametrization $z=R_{x}\\delta_{x}$ . The last vector is also uniformly distributed ", "page_idx": 12}, {"type": "equation", "text": "$$\nz\\sim R_{x}\\mathcal{U}(0,\\Delta_{x})^{m}\\sim\\mathcal{U}(0,\\Delta_{x})^{m}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "due to the uniform distribution on a ball being rotation invariant. Note that above we write $R_{x}\\mathcal{U}(0,\\Delta_{x})^{m}$ to mean the push-forward of the distribution through the inverse rotation. ", "page_idx": 12}, {"type": "text", "text": "We can now rewrite Equation 4 as follows: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}_{\\delta_{x}\\sim\\mathcal{U}(0,\\Delta_{x})^{m}}(\\delta_{x})|g(x)<g(x+\\delta_{x})=R_{x}^{-1}\\mathbb{E}_{\\delta_{x}\\sim\\mathcal{U}(0,\\Delta_{x})^{m}}(R_{x}\\delta_{x})\\ |\\ g(x)<g(x+\\delta_{x})\\qquad}}&{{}}&{(1,...,N)}\\\\ &{}&{=R_{x}^{-1}\\mathbb{E}_{t\\sim\\mathcal{U}(0,\\Delta_{x})^{m}}\\,t\\ |\\ g(x)<g(x+R_{x}^{-1}z)\\qquad}&{{}(1,...,N)}\\\\ &{}&{=R_{x}^{-1}\\mathbb{E}_{z\\sim\\mathcal{U}(0,\\Delta_{x})^{m}}\\,z\\ |\\ 0<\\nabla g^{\\top}(x)\\,R_{x}^{-1}z+\\epsilon(g,x,\\Delta_{x}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Above, to go from line 7 to line 8 we Taylor expand $g$ around $x$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\ng(x+R_{x}^{-1}z)=g(x)+\\nabla g(x)^{\\top}R_{x}^{-1}z+\\epsilon(g,x,\\Delta_{x})\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "with $|\\epsilon(g,x,\\Delta_{x})|=O(\\Delta_{x}^{2})L$ and $L$ being the Lipschitz constant of $\\nabla g$ . ", "page_idx": 12}, {"type": "text", "text": "The conditional in Equation 8 is equal to ", "page_idx": 12}, {"type": "equation", "text": "$$\n-\\epsilon(g,x,\\Delta_{x})<\\nabla g(x)^{\\top}R_{x}^{-1}z=z^{\\top}R_{x}\\nabla g^{\\top}(x)=z^{\\top}\\left(\\!\\!\\begin{array}{c}{{\\|\\nabla g(x)\\|_{2}}}\\\\ {{0}}\\\\ {{\\vdots}}\\\\ {{0}}\\end{array}\\!\\!\\right)\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "or equivalently ", "page_idx": 12}, {"type": "equation", "text": "$$\nz_{1}>\\frac{-\\epsilon(g,x,\\Delta_{x})}{||\\nabla g(x)||_{2}},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where we have denoted the first coordinate of $z$ as $z_{1}$ . ", "page_idx": 13}, {"type": "text", "text": "Thus, setting \u2212\u2225\u03f5\u2207(gg,(xx,)\u2206\u2225x) , Equation 8 can be re-written as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{R_{x}^{-1}\\mathbb{E}_{\\delta_{x}\\sim\\mathcal{M}(0,\\Delta_{x})^{m}}(z|z_{1}>c)=R_{x}^{-1}\\left(\\underbrace{\\mathbb{E}(z_{1}|z_{1}>c)}_{\\vdots}\\right)}}\\\\ &{}&\\\\ &{}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ &{}&{\\qquad\\qquad\\qquad=\\mathbb{E}(z_{1}|z_{1}>c)R_{x}^{-1}\\left(\\underbrace{\\overset{1}{0}}_{\\vdots}\\right)}\\\\ &{}&\\\\ &{}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathbb{E}(z_{1}|z_{1}>c)}\\\\ &{}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We further note that E(z1|z1 > c) = (\u2206x \u2212\u2212\u2225\u03f5\u2207(gg,(xx,)\u2206\u2225x2) due to $z_{1}$ being distributed uniformly in $U(0,\\Delta_{x})$ . We have thus shown that ", "page_idx": 13}, {"type": "equation", "text": "$$\nf_{\\theta_{x}}(x)\\underset{n\\rightarrow\\infty}{\\longrightarrow}\\frac{(\\Delta_{x}+\\frac{\\epsilon(g,x,\\Delta_{x})}{\\|\\nabla g(x)\\|_{2}})}{2\\|\\nabla g(x)\\|_{2}}\\,\\nabla g(x),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and the constant in front of the gradient is positive for $\\begin{array}{r}{\\Delta_{x}>\\frac{|\\epsilon(g,x,\\Delta_{x})|}{||\\nabla g(x)||_{2}}}\\end{array}$ |\u03f5\u2225(\u2207g,gx(,x\u2206)\u2225x)|. The latter condition is met whenever $\\begin{array}{r}{\\Delta_{x}=O(\\frac{\\|\\nabla g(x)\\|_{2}}{L})}\\end{array}$ . \u53e3 ", "page_idx": 13}, {"type": "text", "text": "A.2 Understanding the relation between the learned direction and the property gradient ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Our approach entails constructing a conditional \u2018matching distribution\u2019: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mu_{x}(x^{\\prime})\\propto\\binom{p(x^{\\prime})}{0}\\begin{array}{l r}{\\mathrm{if}\\quad\\|x^{\\prime}-x\\|_{2}^{2}\\leq\\Delta_{x},\\ g(x^{\\prime})-g(x)\\in(\\delta_{y},\\Delta_{y}]}\\\\ {\\mathrm{otherwise},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and then training a model to optimize the following regularized matched reconstruction objective: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\ell(f,\\hat{p})=\\mathbb{E}_{x\\sim\\hat{p}}[\\mathbb{E}_{x^{\\prime}\\sim\\hat{\\mu}_{x}}[\\ell(x^{\\prime},f(x))+\\beta\\,\\ell(x,f(x))]].\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Note that by ${\\hat{p}}(x)$ we denote the empirical density supported on the training set $\\begin{array}{r}{\\hat{p}(x)=\\frac{1}{n}\\sum_{i=1}^{n}\\delta(x_{i}-}\\end{array}$ $x)$ (and analogously for $\\hat{\\mu}_{x}$ ). ", "page_idx": 13}, {"type": "text", "text": "Theorem 3. Let $f^{*}$ be the optimal solution of the matched reconstruction objective matched reconstruction objective. For any point $x$ , the global minimizer is given by ", "page_idx": 13}, {"type": "equation", "text": "$$\nf^{*}(x)=\\frac{\\mathbb{E}_{x^{\\prime}\\sim\\hat{\\mu}_{x}}[x^{\\prime}]+\\beta x}{1+\\beta}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Further, for a $\\lambda_{1}$ -Lipschitz and $\\lambda_{2}$ -smooth function $g$ , the vector $f^{*}(x)-x$ is $a$ -colinear with the gradient of $g$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\na\\leq\\frac{\\nabla g(x)^{\\top}(f^{*}(x)-x)}{\\|\\nabla g(x)\\|_{2}\\|f^{*}(x)-x\\|_{2}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "whenever \u2206  < 2\u03b4y\u2212\u03b1\u03bb1\u2225Ex\u2032\u223c\u00b5\u02c6x[x\u2032]\u2212(1\u2212\u03b2)x\u22252. ", "page_idx": 13}, {"type": "text", "text": "Proof. The global minimizer of the matched reconstruction objective for the MSE loss is given by ", "page_idx": 13}, {"type": "equation", "text": "$$\nf^{*}(x)=\\frac{\\mathbb{E}_{x^{\\prime}\\sim\\hat{\\mu}_{x}}[x^{\\prime}]+\\beta x}{1+\\beta}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which directly follows by taking the gradient of the mean-squared error loss and setting it to zero. We next consider a $C^{2}$ function $g$ and Taylor expand it around $x$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{g(x^{\\prime})=g(x)+\\nabla g(x)^{\\top}(x^{\\prime}-x)+\\epsilon(x,x^{\\prime}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the norm of the approximation error is at most $\\begin{array}{r}{|\\epsilon(x,x^{\\prime})|\\leq\\lambda_{2}\\|x-x^{\\prime}\\|_{2}^{2}/2\\leq\\lambda_{2}\\Delta_{x}/2.}\\end{array}$ Taking the expectation w.r.t. $\\hat{\\mu}_{x}$ yields ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\qquad\\mathbb{E}_{x^{\\prime}\\sim\\hat{\\mu}_{x}}[g(x^{\\prime})]-g(x)=\\nabla g(x)^{\\top}(\\mathbb{E}_{x^{\\prime}\\sim\\hat{\\mu}_{x}}[x^{\\prime}]-x)+\\mathbb{E}_{x^{\\prime}\\sim\\hat{\\mu}_{x}}[\\epsilon(x,x^{\\prime})]}\\\\ &{\\Leftrightarrow\\mathbb{E}_{x^{\\prime}\\sim\\hat{\\mu}_{x}}[g(x^{\\prime})-\\epsilon(x,x^{\\prime})]-g(x)=\\nabla g(x)^{\\top}(f^{*}(x)-x)(1+\\beta),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where in the last step we substituted the expectation by $f^{\\prime}(x)$ . We notice that, as long as $\\mathbb{E}_{x^{\\prime}\\sim\\hat{\\mu}_{x}}[g(x^{\\prime})\\,-\\,\\epsilon(x,\\bar{x^{\\prime}})]\\,-\\,g(x)\\,>\\,0$ , the learned direction is pointing towards a similar direction as the gradient: ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\mathcal{L}}(\\nabla g(x),f^{*}(x)-x)=\\operatorname{arc}\\cos\\left({\\frac{\\nabla g(x)^{\\top}(f^{*}(x)-x)}{\\|\\nabla g(x)\\|_{2}\\|f^{*}(x)-x\\|_{2}}}\\right)\\leq90^{\\circ}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We expand the term within the condition in the following: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{x^{\\prime}\\sim\\hat{\\mu}_{x}}[g(x^{\\prime})-\\epsilon(x,x^{\\prime})]-g(x)\\ge\\operatorname*{inf}_{x^{\\prime}\\sim\\hat{\\mu}_{x}}[g(x^{\\prime})-|\\epsilon(x,x^{\\prime})|]-g(x)}\\\\ &{\\qquad\\qquad\\qquad\\ge\\delta_{y}-\\displaystyle\\frac{\\lambda_{2}\\Delta_{x}}{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "to determine that the following sufficient condition for $\\angle(\\nabla g(x)^{\\top},f^{*}(x)\\!-\\!x)$ to be below 90 degrees: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Delta_{x}<\\frac{2\\delta_{y}}{\\lambda_{2}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "More generally, since ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{\\nabla g(x)^{\\top}(f^{*}(x)-x)}{\\|\\nabla g(x)^{\\top}\\|\\|f^{*}(x)-x\\|}\\ge\\frac{\\delta_{y}-\\frac{\\lambda_{2}\\Delta_{x}}{2}}{(1+\\beta)\\lambda_{1}\\|f^{*}(x)-x\\|_{2}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "a sufficient condition for the normalized inner product to be above $\\alpha$ is ", "page_idx": 14}, {"type": "equation", "text": "$$\na\\leq\\frac{\\nabla g(x)^{\\top}(f^{*}(x)-x)}{\\|\\nabla g(x)^{\\top}\\|\\|f^{*}(x)-x\\|}\\Leftarrow\\Delta_{x}<2\\,\\frac{\\delta_{y}-\\alpha(1+\\beta)\\lambda_{1}\\|f^{*}(x)-x\\|_{2}}{\\lambda_{2}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "One may also set $\\begin{array}{r}{\\|f^{*}(x)-x\\|_{2}=\\frac{\\mathbb{E}_{x^{\\prime}\\sim\\hat{\\mu}_{x}}[x^{\\prime}]-(1-\\beta)x}{1+\\beta}}\\end{array}$ in the equation above to obtain the condition ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Delta_{x}<2\\,\\frac{\\delta_{y}-\\alpha\\lambda_{1}\\|\\mathbb{E}_{x^{\\prime}\\sim\\hat{\\mu}_{x}}[x^{\\prime}]-(1-\\beta)x\\|_{2}}{\\lambda_{2}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "as claimed. ", "page_idx": 14}, {"type": "text", "text": "A.3 Proof of Theorem 2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. Set $x^{\\prime}=f^{*}(x)$ . Taking a Taylor expansion of $p$ around $x^{\\prime}$ , we deduce that for every $x^{\\prime\\prime}\\in D$ the following holds: ", "page_idx": 14}, {"type": "equation", "text": "$$\np(\\boldsymbol{x}_{i})\\leq p(\\boldsymbol{x}^{\\prime})+\\nabla p(\\boldsymbol{x}^{\\prime})^{\\top}(\\boldsymbol{x}^{\\prime\\prime}-\\boldsymbol{x})+\\frac{\\|H_{p}(\\boldsymbol{x}^{\\prime})\\|_{2}\\,\\|\\boldsymbol{x}^{\\prime\\prime}-\\boldsymbol{x}\\|_{2}^{2}}{2},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "with $\\|\\boldsymbol{H_{p}}(\\boldsymbol{x}^{\\prime})\\|_{2}$ being the Hessian of $p$ at $x^{\\prime}$ . Taking the expectation w.r.t. $\\hat{\\mu}_{x}$ yields ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x^{\\prime\\prime}\\sim\\hat{\\mu}_{x}}[p(x^{\\prime\\prime})]\\leq p(x^{\\prime})+\\nabla p(x^{\\prime})^{\\top}(\\mathbb{E}_{x^{\\prime\\prime}\\sim\\hat{\\mu}_{x}}[x^{\\prime\\prime}]-x^{\\prime})+\\frac{\\|H_{p}(x^{\\prime})\\|_{2}\\mathbb{E}_{x^{\\prime\\prime}\\sim\\hat{\\mu}_{x}}[\\|x^{\\prime\\prime}-x^{\\prime}\\|_{2}^{2}]}{2}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "or equivalently ", "page_idx": 14}, {"type": "equation", "text": "$$\np(x^{\\prime})\\geq\\mathbb{E}_{x^{\\prime\\prime}\\sim\\hat{\\mu}_{x}}[p(x^{\\prime\\prime})]-\\nabla p(x^{\\prime})^{\\top}(\\mathbb{E}_{x^{\\prime\\prime}\\sim\\hat{\\mu}_{x}}[x^{\\prime\\prime}]-x^{\\prime})-\\frac{\\|H_{p}(x^{\\prime})\\|_{2}\\mathbb{E}_{x^{\\prime\\prime}\\sim\\hat{\\mu}_{x}}[\\|x^{\\prime\\prime}-x^{\\prime}\\|_{2}^{2}]}{2}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "If we further assume that the model is trained to minimize the matched reconstruction objective with an MSE loss, we obtain the claimed result: ", "page_idx": 14}, {"type": "equation", "text": "$$\np(x^{\\prime})\\geq\\mathbb{E}_{x^{\\prime\\prime}\\sim\\hat{\\mu}_{x}}[p(x^{\\prime\\prime})]-\\frac{\\|H_{p}(x^{\\prime})\\|_{2}\\,\\sigma^{2}(\\mathcal{M}_{x})}{2},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\sigma^{2}(\\mathcal{M}_{x})=\\mathbb{E}_{x^{\\prime}\\sim\\hat{\\mu}_{x}}[\\|x^{\\prime}\\!-\\!\\mathbb{E}_{x^{\\prime\\prime}\\sim\\hat{\\mu}_{x}}[x^{\\prime\\prime}]\\|_{2}^{2}]$ is the variance induced by the matching process. ", "page_idx": 14}, {"type": "text", "text": "Observe that, since the variance is upper bounded by $\\sigma^{2}(\\mathcal{M}_{x})\\,\\leq\\,4\\Delta_{x}$ , one may provide higher likelihood samples by restricting the matching process to consider closer pairs. Further, as perhaps expected, the likelihood is higher for smoother densities. ", "page_idx": 14}, {"type": "text", "text": "A.4 Understanding the matched reconstruction objective ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The following analysis analyses the key characteristics of the proposed method. Though our analysis does not take into account the training process with stochastic gradient descent and the potentially useful inductive biases of the neural network $f_{\\theta}$ , it provides useful insights that qualitatively align with the practical model behavior. ", "page_idx": 15}, {"type": "text", "text": "Property 1. The learned direction follows the property gradient. We first show that by optimizing the matched reconstruction objective, the model will learn to adjust an input point by approximately following the gradient $\\nabla g(x)$ of the property function. ", "page_idx": 15}, {"type": "text", "text": "For ease of notation, we denote by $\\mathcal{M}_{x}$ the subset of our paired dataset with all pairs that start from point $x$ . ", "page_idx": 15}, {"type": "text", "text": "Theorem 4. Let $f_{\\beta}^{*}$ be the optimal solution of the matched reconstruction objective. For a $\\lambda_{1}$ - Lipschitz and $\\lambda_{2}$ -smooth function $g$ , the vector $v_{x}=f_{\\beta}^{*}(x)-x$ is $a$ -colinear with the gradient of $g$ : ", "page_idx": 15}, {"type": "text", "text": "The theorem reveals the role of the introduced thresholds in controlling how close we follow the property gradient: the larger the required property change $\\delta_{y}$ and the smaller the allowed input deviation $\\Delta_{x}$ the closer $v_{x}$ tracks the gradient of $g$ . We thus observe a trade-off between approximation and dataset size: selecting less conservative thresholds will lead to a larger $|{\\mathcal{M}}|$ at the expense of a rougher approximation. ", "page_idx": 15}, {"type": "text", "text": "Property 2. Regularization controls the step size. Hyperparameter $\\beta$ enables us to control the magnitude of the allowed input change (the step size in optimization parlance), with larger $\\beta$ encouraging smaller changes: ", "page_idx": 15}, {"type": "text", "text": "Corollary 1. In the setting of Theorem 4, we have: $\\begin{array}{r}{\\|f_{\\beta}^{*}(x)-x\\|_{2}=\\frac{\\|f_{0}^{*}(x)-x\\|_{2}}{1+\\beta}}\\end{array}$ . ", "page_idx": 15}, {"type": "text", "text": "Therefore, setting $\\beta$ to a larger value can be handy for conservative design problems that prioritize distance constraints or when the property function is known to be complex. ", "page_idx": 15}, {"type": "text", "text": "B Additional results for experiments ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Details on Matched datasets ", "text_level": 1, "page_idx": 15}, {"type": "table", "img_path": "dhFHO90INk/tmp/ee03204742175e4c9de46919e9219b16efb8bb8ef6b9268eb5e5db29c6c5748d.jpg", "table_caption": ["Table 4: Overview of matched dataset and choice of $\\Delta_{x}$ and $\\Delta_{y}$ "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "B.2 Toy experiment - 8 Gaussians with anti-clockwise increasing property value ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We include a toy example where the property is disentangled from the likelihood of the data. The results in Figure B.2 and Figure B.2 are consistent with the discussion in the main text. ", "page_idx": 15}, {"type": "text", "text": "B.3 Setting $\\Delta_{x}$ and $\\Delta_{y}$ ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The choice of parameters $\\Delta_{x}$ and $\\Delta_{y}$ should be informed by the specific application. For example, in antibody design, domain experts recommend not using thresholds above a Levenshtein distance of 8, as such differences are considered biologically irrelevant. Similarly, for $\\Delta_{y}$ , knowing that noise in binding measurements can be up to 0.3, we chose 0.5 to ensure proper matching. When faced ", "page_idx": 15}, {"type": "image", "img_path": "dhFHO90INk/tmp/7c3acd8a7c9f315c2942c42506b63b8a4b69d99ace2d2fcad39a272ab2e9299d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 6: New toy example $d=50.$ . Explicit guidance suffers in higher dimensions (shown here for $d=50$ ) as there are increasingly more directions around the data in which the surrogate model is erroneous. Implicit guidance is robust to the higher data dimensionality because generated samples lie within distribution (as proven). ", "page_idx": 16}, {"type": "image", "img_path": "dhFHO90INk/tmp/5c3fb8197f504c2efbad32d68668e8163f01cd676f4e42f99ce25ac044c94de1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 7: New toy example in $d\\in\\{2,10,50\\}$ , for 8-Gaussians with property increasing in anticlockwise direction. Distribution of evaluation metrics from 10 repetitions of each experiment. ratio of improvement, higher is better, NLL lower is better. ", "page_idx": 16}, {"type": "text", "text": "with a new dataset of $n$ unique data points, a good initial approach is to use the standard deviation of pairwise distances for $x$ , and the mean or median for property $y$ : ", "page_idx": 16}, {"type": "image", "img_path": "dhFHO90INk/tmp/3508586c0db22f0a2b08e5be09ca72f344ee21f1af472630db3d41dc84266112.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "How $g(\\cdot)$ influences the calibration/selection of $\\Delta_{y}$ When the $g(.)$ is non-smooth, or we have a very few datapoints, this certainly influences the choice of $\\Delta_{y}$ , in this case we opt for as small steps as possible since this will create many pairs which should give sense of the direction of the gradient around the sparse regions. ", "page_idx": 16}, {"type": "text", "text": "In the Figure 8, we provide an additional ablation study on how threshold choices affect the number of training pairs, showing that sufficiently large thresholds include all unique training points. For performance impact, please refer to the ablation study in Figure 4. ", "page_idx": 16}, {"type": "image", "img_path": "dhFHO90INk/tmp/b9f68076393d4f83f32ed3eea20bf4063e8918042cd5fa0b5683f6efd5b5c40c.jpg", "img_caption": ["Figure 8: Effect of thresholds $\\Delta_{x}$ and $\\Delta_{y}$ on the number of pairs. Though the number of pairs increases with larger thresholds, the benefti saturates (especially when increasing $\\Delta_{y}$ ). "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 9: Airfoils optimized with PropEn from multiple seeds. Red: seed designs, blue: PropEn candidates. To reduce the lift-drag ratio, PropEn tends to flatten the bottom of the airfoil to reduce the drag, while extend the front upwards to increase lift. ", "page_idx": 17}, {"type": "table", "img_path": "dhFHO90INk/tmp/c0ed5877074ddcb5f02669f07eaea3a73de9f14f40738475b3df277575c6ad5b.jpg", "table_caption": ["Table 5: Diffusion models do improve almost all hold out designs (RI - ratio of improvement), however, only by a very small value (AI-average improvement.) "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "B.4 Airfoils ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.4.1 Additional baseline for airfoils ", "page_idx": 17}, {"type": "text", "text": "B.5 Example of PropEn designs for antibodies ", "page_idx": 17}, {"type": "image", "img_path": "dhFHO90INk/tmp/d862df0bfa2315e08e4e7d306aca9ba85d83d85e49b022ebb79620ee9c50b3ec.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 10: HER2 binders from PropEn validated in wet lab experiments. Up: sequence alignment of the heavy chains wrt the seed which is the top sequence. The positions marked in black correspond to mutational differences from the seed. Bottom: folded structures of the corresponding designs with the mutations to seed marked in green. ", "page_idx": 17}, {"type": "text", "text": "B.6 Details on experimental setups ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Toy datasets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "\u2022 ablation studies: $N\\in\\{100,200\\},d\\in\\{2,10,50,100\\}$ \u2022 matching thresholds: $\\Delta_{x}=\\Delta_{y}=1$ , number of epochs: 500, batch size: 64 ", "page_idx": 17}, {"type": "text", "text": "Criterion: MSELoss() ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Encoder: ", "page_idx": 17}, {"type": "text", "text": "Sequential( (0): Linear(in_features $^{=2}$ , out_features $=\\!30$ , bias $=$ True) (1): ReLU() (2): Linear(in_features $=\\!30$ , out_features $=\\!30$ , bias $=$ True) (3): ReLU() (4): Linear(in_features $=\\!30$ , out_features $=\\!30$ , bias $=$ True) (5): ReLU() (6): Linear(in_features $=\\!30$ , out_features ${\\tt=}15$ , bias $=$ True) ", "page_idx": 18}, {"type": "text", "text": "Decoder: Analogous to the encoder, starting with the compressed representation (15 features) and progressively reconstructing the original input size (2 features). ", "page_idx": 18}, {"type": "text", "text": "Toy Example Discriminator: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Sequential( (0): Linear(in_features $^{=2}$ , out_feature $\\mathtt{i=}30$ , bias $\\equiv$ True) (1): ReLU() (2): Linear(in_features $=\\!30$ , out_features $\\scriptstyle{=}30$ , bias $=$ True) (3): ReLU() (4): Linear(in_features $=\\!30$ , out_features $\\scriptstyle{=}30$ , bias $=$ True) (5): ReLU() (6): Linear(in_features $=\\!30$ , out_features $=\\!1$ , bias $\\equiv$ True) ", "page_idx": 18}, {"type": "text", "text": "Airfoil ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 ablation studies: $N\\in\\{200,500,1000\\}$ , \u2022 matching thresholds: $\\Delta_{x}=\\Delta_{y}\\in\\{0.3,0.5,0.7,1\\}$ \u2022 number of epochs: 1000, batch size: 100 ", "page_idx": 18}, {"type": "text", "text": "Criterion: MSELoss() ", "page_idx": 18}, {"type": "text", "text": "Encoder: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Sequential( (0): Linear(in_features $^{=2}$ , out_features $\\scriptstyle=100$ , bias $=$ True) (1): ReLU() (2): Linear(in_features $=\\!100$ , out_features $\\scriptstyle=100$ , bias $=$ True) (3): ReLU() (4): Linear(in_features $=\\!100$ , out_features $=\\!100$ , bias $=$ True) (5): ReLU() (6): Linear(in_features $_{-100}$ , out_features $\\mathtt{=}50$ , bias $,=$ True) ", "page_idx": 18}, {"type": "text", "text": "Decoder: Analogous to the encoder, starting with the compressed representation (50 features) and progressively reconstructing the original input size (400 features). ", "page_idx": 18}, {"type": "text", "text": "AirFoil Discriminator: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Sequential( (0): Linear(in_features $=\\!100$ , out_features $=\\!100$ , bias $=$ True) (1): ReLU() (2): Linear(in_features $=\\!100$ , out_features $=\\!100$ , bias $=$ True) (3): ReLU() (4): Linear(in_features $=\\!100$ , out_features $=\\!100$ , bias $=$ True) (5): ReLU() (6): Linear(in_features $=\\!100$ , out_features $^{=1}$ , bias $=$ True) ", "page_idx": 18}, {"type": "text", "text": "Hyper-parameter choice. For optimizing the parameters of the baselines in the toy and engineering experiments, we conducted a grid search over the learning rate ([1e-2, 1e-5]), weight decay ([1e-2, 1e-5]), number of epochs ([300, 1000, 5000]), batch size (32, 64, 128), and number of neurons per layer ([30, 50, 100]). ", "page_idx": 19}, {"type": "text", "text": "Therapeutic Proteins ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 batch size: 32, training epochs: 300   \n\u2022 matching thresholds: $\\Delta_{x}=\\in\\{10,15\\}$ mutational, edit distance, $\\Delta_{y}=0.5$   \n\u2022 baselines: For WJS, we used a 1D Conv architecture (please see A.1 in [38]), and for Lambo, a bert-small transformer (please see C.1 in [39]). ", "page_idx": 19}, {"type": "text", "text": "Encoder( (mlp): Linear(in_features $=\\mathtt{6556}$ , out_features $=\\!128$ , bias $\\equiv$ True) (blocks): ModuleList( 3 x ResNetBlock( (ln1): LayerNorm((128,), $\\mathtt{e p s=1e-05}$ , elementwise_affine $\\equiv$ True) (mlp1): Linear(in_features $_{:=128}$ , out_features $=\\mathtt{256}$ , bias $\\AA,=$ True) (ln2): LayerNorm((256,), $\\mathtt{e p s=1e-05}$ , elementwise_affine $:=$ True) (mlp2): Linear(in_features $=\\mathtt{256}$ , out_features $_{=128}$ , bias $\\AA,=$ True) (act): GELU(approximate $\\qquad=.$ \u2019none\u2019) ) ", "page_idx": 19}, {"type": "text", "text": "Decoder: Analogous to the encoder, starting with the compressed representation (256 features) and progressively reconstructing the original input size (6556). ", "page_idx": 19}, {"type": "text", "text": "Comparison to Bayesian Optimization. While both BO and PropEn aim at optimization in small sample sizes, the two frameworks solve different problems. The goal of PropEn is to generate designs, whereas in BO/AL the goal is to choose the most promising designs that should be labeled in order to imporve a predictors performance or find the best candidate, i.e. the focus is selection. In the context of optimizing designs, one would have a suite of (1) generative models, (2) property predictors and (3) BO/AL module that will do the final selection across pool of candidates. PropEn falls in (1), the category of generative models section that will contribute to the library of potential candidates. As a side note, Lambo, a method we compared to, uses a BO inspired acquisition function to guide the search for better designs, and we do compare to it (favorably), but, we must highlight the difference, \\*\\*PropEn and Lambo are generative models not BO/AL methods. We hope this clarifies the differences between the two frameworks and highlights their complementarity. ", "page_idx": 19}, {"type": "text", "text": "B.7 Ablation study for toy experiments ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Additional metrics included: ", "page_idx": 19}, {"type": "text", "text": "1. Uniqueness: number of unique designs divided by number of generated designs. 2. Novelty: number of designs proposed by the method that don\u2019t appear in the training data, divided by the number of generated designs. ", "page_idx": 19}, {"type": "table", "img_path": "dhFHO90INk/tmp/83386e6cf1210783c172fe0cd9b19dde1457829880037e20f234a6a996f3a4ed.jpg", "table_caption": ["Table 6: PropEn vs Explicit guidance results on toy datasets, end of optimization trajectory. $N$ is the number of samples, $d$ is the number of dimensions for projection $\\mathrm{2D}\\rightarrow d{\\bf D}$ datasets). Mean (std) over 10 repetitions of the experiment. For all metrics, higher is better. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "dhFHO90INk/tmp/8d20cecd7bd22707dcbfb8311906b969eb8493b3d46539ad8135d072849545ff.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 22}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 22}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 22}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 22}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \u201d[Yes] \u201d is generally preferable to \u201d[No] \u201d, it is perfectly acceptable to answer \u201d[No] \u201d provided a proper justification is given (e.g., \u201derror bars are not reported because it would be too computationally expensive\u201d or \u201dwe were unable to find the license for the dataset we used\u201d). In general, answering \u201d[No] \u201d or \u201d[NA] \u201d is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 22}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 22}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\u201d, \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: For all claims made in the introduction, we provide theoretical analysis in section 2 and experimental validation in section 4. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] , ", "page_idx": 22}, {"type": "text", "text": "Justification: We include a paragraph on limitations of the current method at the end of section 4. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: All proofs are included in the supplementary material and corresponding references can be found in the main text. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Yes, we provide tabular results with mean and standard deviations for all experiments. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We provide link to open source repository for the code, and all details for reproducing experiments are in Appendic B. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. ", "page_idx": 24}, {"type": "text", "text": "\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We include all details in Appendix B. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: ", "page_idx": 25}, {"type": "text", "text": "[Yes] We provide confidence intervals and standard deviations in all tables and figures we report. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: All details can be found in Appendix B. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 25}, {"type": "text", "text": "\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We conform with all guidlines. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: Our paper proposes methods that help optimizing seed designs. It is representation-agnostic and as such it can help in advancing many scientific domains. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: he paper poses no such risks. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer:[Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We use the NeuralFoil software and propery cite it in the manuscript. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We provide documented annonimized demo code for running part of the experiments in our paper. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer:[NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]