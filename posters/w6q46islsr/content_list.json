[{"type": "text", "text": "Training Dynamics of Transformers to Recognize Word Co-occurrence via Gradient Flow Analysis ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hongru Yang\\* The University of Texas at Austin & Princeton University hy6385@utexas.edu ", "page_idx": 0}, {"type": "text", "text": "Bhavya Kailkhura Lawrence Livermore National Laboratory kailkhura1@llnl.gov ", "page_idx": 0}, {"type": "text", "text": "Zhangyang Wang The University of Texas at Austin atlaswang@utexas.edu ", "page_idx": 0}, {"type": "text", "text": "Yingbin Liang The Ohio State University liang.889@osu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Understanding the training dynamics of transformers is important to explain the impressive capabilities behind large language models. In this work, we study the dynamics of training a shallow transformer on a task of recognizing co-occurrence of two designated words. In the literature of studying training dynamics of transformers, several simplifications are commonly adopted such as weight reparameterization, attention linearization, special initialization, and lazy regime. In contrast, we analyze the gradient flow dynamics of simultaneously training three attention matrices and a linear MLP layer from random initialization, and provide a framework of analyzing such dynamics via a coupled dynamical system. We establish near minimum loss and characterize the attention model after training. We discover that gradient fow serves as an inherent mechanism that naturally divide the training process into two phases. In Phase 1, the linear MLP quickly aligns with the two target signals for correct classification, whereas the softmax attention remains almost unchanged. In Phase 2, the attention matrices and the MLP evolve jointly to enlarge the classification margin and reduce the loss to a near minimum value. Technically, we prove a novel property of the gradient flow, termed automatic balancing of gradients, which enables the loss values of different samples to decrease almost at the same rate and further facilitates the proof of near minimum training loss. We also conduct experiments to verify our theoretical results. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ever since the invention of self-attention $[\\mathrm{VSP^{+}17}]$ , transformers have become a dominating backbone architecture in many machine learning applications such as computer vision $[\\bar{\\mathrm{DBK}}^{+}20$ $\\mathrm{LLC}^{+}21]$ and natural language processing [DCLT18]. Nowadays, ChatGPT and GPT-4 [Ope23] have demonstrated astonishing abilities in many areas such as language understanding, mathematics and coding, which have sparked artificial general intelligence $[\\bar{\\mathbf{B}}\\mathbf{C}\\mathbf{E}^{+}23]$ In the meantime, there has been a burgeoning development of large language models (LLMs) $[\\mathrm{TL}\\mathrm{I}^{+}23\\$ ,MH23, $\\mathrm{ADF}^{+}23]$ I as well as multi-modal models [Tea23]. ", "page_idx": 0}, {"type": "text", "text": "Despite the huge empirical success, theoretical understanding of why a pre-trained language model can possess such impressive performance has been significantly lagging behind. Some previous efforts have been made in understanding the capacity and representational power of transformers [EGKZ22, $\\mathrm{LAG}^{+}22$ ,ZPGA23,SHT23, $\\bar{\\mathrm{BCW}}^{+}\\bar{2}3]$ . However, most results of this type of works are existential and rely on manual construction of the weights. It is unclear whether the constructed weights are the actual solutions after training transformers. In order to understand the mechanism behind those pre-trained language models, a line of studies have aimed to open the black box of optimization via studying training dynamics of transformers and explaining why transformers can be trained to perform well [JSL22, LWLC22, TWCD23, $\\mathrm{TWZ}^{+}23$ ,LLR23,TLZO23,TLTO23,ZFB24, HCL23]. However, those previous works often relied on various simplifications in their analysis such as weight reparameterization, attention linearization, special initialization, lazy regime, etc. One goal of this paper is to take a further step to demystify the training dynamics of transformers and consider more practical training setup, thus better capturing the actual training process. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Our study of transformers\u2019 training dynamics will focus on a basic problem of recognizing cooccurrence of words under a binary classification setup, which is an important ability of LLMs to perform many tasks correctly in natural language processing (NLP). For example, the classical n-gram model [MS99, Dam18] predicts the next word based on co-occurrence of multiple words. Consider the following scenario: if the task for the language model is to read a paragraph describing a children and then answer some questions, say, \u201cIs Bob eating a banana?\". In order to answer the question correctly, the model must be able to detect the co-occurrence of the two words \u201cBob\" and \"banana\u201d in the paragraph. Motivated by this, we study the problem of detecting co-occurrence of two target words via the model of a one-layer transformer with a self-attention module followed by a linear multi-layer perceptrons (MLP) layer. Our goal is to characterize the dynamics of the training process via the gradient flow analysis, thus providing a theory to explain how transformers can be trained to perform well. ", "page_idx": 1}, {"type": "text", "text": "Our contribution is summarized below: ", "page_idx": 1}, {"type": "text", "text": "\u00b7 We study the gradient flow dynamics of detecting word co-occurrence. The training starts with random initialization and then simultaneously updates four weight matrices (including key, query, and value matrices and a linear MLP) in the transformer architecture via gradient flow. We show that gradient flow can achieve small loss although the loss function is highly nonconvex. We further characterize the explicit form of attention matrices after training, which captures the strong positive correlation between the two target signals and strong negative correlation between one target signal and the common token, both leading to large classification margin. \u25cf We characterize the training process into two phases. In Phase 1 (alignment of MLP for correct classification), we show that the linear MLP of the transformer quickly aligns with the two target word tokens whereas all other variables in the dynamical system stay almost unchanged from their initialization values. All training samples are correctly classified at the end of Phase 1, but the loss value is still large due to small classification margin. In Phase 2 (evolution of attention and MLP for large classification margin), along with the continual evolution of MLP, correct classification by MLP also encourages the gradients of attention matrices to learn. Specifically, the softmax probability increases if the key and query tokens correspond to the two target words, and the value transform of the two words becomes more positively correlated, both leading to enlarge the classification margin. Thus, the training and test loss values both are driven down to nearly zero. \u00b7 Technically, our proof techniques do not rely on several commonly used assumptions in the literature such as weight reparameterization, attention linearization, special initialization, lazy regime, etc. Our main idea is to treat the problem as a coupled dynamical system with six different types of dynamic variables, for which we provide an articulated analysis on the gradient flow dynamics. In particular, we prove a novel property of the gradient flow, termed automatic balancing of gradients, which shows that the ratio of several important gradients will evolve closely within the same range during training. This enables us to show that the losses of all training samples can decrease almost at the same rate, and is also a key component in proving the near minimum training loss as well as analyzing the changes of softmax. ", "page_idx": 1}, {"type": "text", "text": "1.1 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Transformer representational power. Several previous works have studied the expressiveness of transformers. One line of work was from a universal approximation perspective and thus provided the existential results $[\\mathrm{YBR^{+}19}$ WCM22,PBM21,ZPGA23, $\\mathrm{LAG}^{+}22$ $\\mathrm{BCW}^{+}23]$ .As aseparate view, [EGKZ22] showed that a single attention head can represent a sparse function over the input sequence with sample complexity much smaller than the context length. [ZZYW23] studied the approximation and generalization performance of transformers in in-context learning. [SHT23] proved that transformers can represent certain functions more efficiently than MLPs. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Training transformers. Various settings of training transformers have been studied recently. [WXM21] studied the impact of head and prompt tuning of transformer on the downstream learning tasks. [JSL22] proved that transformers can learn spatial structures.  [LWLC22] studied how a shallow transformer learns a dataset with both label-relevant and label-irrelevant tokens. [TWCD23] studied a next-token prediction problem and showed that self-attention behaves like a discriminating scanning algorithm. [LLR23] analyzed a layer-wise optimization scheme on how transformers learn topic structures. [TLZO23, TLTO23, VDT24] studied a setting where transformers can learn a SVM solution. $[\\mathrm{LWM}^{+}23]$ provided analysis of training graph transformers for node classification tasks. [Thr24] studied the implicit bias in the next-token prediction problem. For in-context linear regression, $[\\mathrm{VONR}^{+}23]$ constructed transformer weights to solve this task and showed empirically that this is similar to what the transformer learned by gradient descent, [ACDS24] proved that the critical points of the training objective of linear transformers implement a pre-conditioned gradient descent, [ZFB24] provided the training dynamics of linear attention models, [HCL23] characterized the training dynamics of softmax transformers, and [CSWY24] studied a multi-task linear regression problem with a multi-headed softmax transformer. Further, $[\\mathrm{LWL}^{+}24]$ focused on nonlinear selfattention and nonlinear MLP over classification tasks in in-context learning. [WLCC23] proved the convergence of transformers via neural tangent kernel. [NDL24] showed that two-layer transformers can learn causal structure via gradient descent. [CL24] developed algorithms for provably learning a multi-head attention layer. [HWCL24] studied how transformers learn feature-position correlation. ", "page_idx": 2}, {"type": "text", "text": "This paper studies a different problem of detecting co-occurrence of words via transformers. Such a setting has not been considered in the literature. More importantly, the previous studies of training dynamics of transformers have adopted various assumptions/simplifications such as weight reparameterization, special initialization, attention linearization, lazy regime, etc. In contrast, our analysis here based on gradient flow does not rely on those simplifications, which can be of independent interest for studying transformers in other settings. ", "page_idx": 2}, {"type": "text", "text": "2 Problem Setting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notations. For a vector $\\boldsymbol{v}\\in\\mathbb{R}^{d}$ , we use $\\mathrm{diag}(v)$ to denote a diagonal matrix with $v$ being the diagonal entries. When we subtract the vector $v$ by a scalar $a$ , we subtract each entry of $v$ by $a$ i.e., $v-a\\in\\mathbb{R}^{d}$ and $(v-a)_{i}=v_{i}-a.$ We use $\\widetilde\\Omega,\\widetilde\\Theta,\\widetilde{\\cal O}$ to hide polylogarithmic factors. ", "page_idx": 2}, {"type": "text", "text": "2.1 Data Model ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Definition 2.1 Data distribution). Given a set of orthonormal vectors $\\{\\mu_{i}\\}_{i=1}^{d}$ as word embedding,   \nlet $\\mu_{1},\\mu_{2}\\,\\in\\,\\mathbb{R}^{d}$ be two target signals whose co-occurrence needs to be detected by the model,   \nand let $\\mu_{3}\\,\\in\\,\\mathbb{R}^{d}$ be a common token vector. A data entry $(X,y)\\,\\in\\,\\mathbb{R}^{d\\times L}\\times\\{\\pm1\\}$ where $X\\,=$   \n$[x_{1},x_{2},\\dots,x_{L}]$ consists of $L$ tokens, is generated by the distribution $\\mathcal{D}$ as follows:   \n1. Uniformly randomly select an index $i_{3}\\in[L]$ and set $x_{i_{3}}=\\mu_{3}$   \n2. Then, one of the following cases occurs: \u00b7With probability $1/2$ set $y=1$ and uniformly randomly select two indices $i_{1}\\neq i_{2}\\in[L]\\setminus\\{i_{3}\\}$ and set $x_{i_{1}}=\\mu_{1}$ \uff0c $x_{i_{2}}=\\mu_{2}$ For $i\\in[L]\\setminus\\{i_{1},i_{2},i_{3}\\}$ ,set $x_{i}=\\tt U n i f o r m(\\{\\mu_{i}\\}_{i=4}^{d})$ \u00b7 With probability $1/6$ set $y=-1$ and uniformly randomly select one index $i_{1}\\in[L]\\setminus\\{i_{3}\\}$ and set $x_{i_{1}}=\\mu_{1}$ For $i\\in[L]\\setminus\\{i_{3},i_{1}\\}$ , we set $x_{i}=\\mathtt{U n i f o r m}(\\{\\mu_{i}\\}_{i=4}^{d})$ \u00b7 With probability $1/6_{\\mathrm{:}}$ set $y=-1$ and uniformly randomly select one index $i_{2}\\in[L]\\setminus\\{i_{3}\\}$ and set $x_{i_{2}}=\\mu_{2}$ For $i\\in[L]\\setminus\\{i_{3},i_{2}\\}$ , we set $x_{i}=\\mathtt{U n i f o r m}(\\{\\mu_{i}\\}_{i=4}^{d})$ \u00b7With probability $1/6$ set $y=-1$ For all $i\\in[L]\\setminus\\{i_{3}\\}$ ,we set $x_{i}=\\mathtt{U n i f o r m}(\\{\\mu_{i}\\}_{i=4}^{d}).$ ", "page_idx": 2}, {"type": "text", "text": "In summary, there are 4 types of data: (1) both $\\mu_{1},\\mu_{2}$ appear, (2) only $\\mu_{1}$ appears, (3) only $\\mu_{2}$ appears, and (4) neither $\\mu_{1}$ nor $\\mu_{2}$ appears. We denote the set of indices of the above 4 different types of data by $I_{1},I_{2},I_{3},I_{4}\\subseteq[n]$ We further define $\\mathcal{R}=\\{\\mu_{i}\\}_{i=4}^{d}$ .For simplicity, our data distribution assumes $\\mu_{1},\\mu_{2},\\mu_{3}$ appear only once in a data entry. The occurrence probability of each type of data is chosen in the above way to make the distribution label-balanced. We assume there is a fixed set of orthonormal vectors as word embedding, which is analogous to the one-hot embedding of a set of vocabularies. Furthermore, in our daily language, there are some words appearing in almost every sentence such as \u201ca\" and \u201cthe'\". Thus, to model those words, we include a common token in every data entry. Finally, notice that if we ignore the common token and random tokens, the data distribution simplifies to a logical AND problem. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Remark 2.2. Recognizing co-occurrence of words is an important ability for language models to perform many NLP tasks correctly. Consider the example of a language model first reading a paragraph describing a children and then answering the question \u201cIs Bob eating a banana?\" If the description is \u201cBob is watching a television while eating a banana\", then the model should answer \"Yes'\". If the description is \u201cBob is playing computer games\", then the model should answer \"No\". Thus, the model needs to recognize the co-occurrence of\u201cBob\"and \u201cbanana\". ", "page_idx": 3}, {"type": "text", "text": "For simplicity of our analysis, we make the following assumption on our training data set. ", "page_idx": 3}, {"type": "text", "text": "$\\begin{array}{r}{\\frac{|I_{1}|}{n}=\\frac{1}{2}}\\end{array}$ mh $\\begin{array}{r}{\\frac{|I_{2}|}{n}=\\frac{|I_{3}|}{n}=\\frac{|I_{4}|}{n}=\\frac{1}{6}.}\\end{array}$ $i_{1},i_{2}\\in[n],\\:l_{1},l_{2}\\in[L],\\:i f\\:X_{l_{1}}^{(i_{1})},X_{l_{2}}^{(i_{2})}\\notin\\{\\mu_{1},\\mu_{2},\\bar{\\mu_{3}}\\}.$ $X_{l_{1}}^{(i_{1})}\\neq X_{l_{2}}^{(i_{2})}$ words are different. ", "page_idx": 3}, {"type": "text", "text": "The first assumption can be approximately satisfied with high probability given the total number $n$ of samples is large enough. Such an assumption can be removed by applying the standard concentration theorems. The second assumption implicitly assumes $n L\\leq d$ . If the irrelevant words are uniformly sampled from a large entire vocabulary, then each irrelevant word appears only very few times in the training set. Thus, letting irrelevant words appear only once in the entire training set is a reasonable way to simplify our analysis. ", "page_idx": 3}, {"type": "text", "text": "2.2  Transformer Architecture and Training ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Consider a training set $\\{(X^{(i)},y_{i})\\}_{i=1}^{n}$ with $n$ training samples. Each data point $X^{(i)}\\ \\in\\ \\mathbb{R}^{d\\times L}$ contains $L$ tokens, i.e., $X^{(i)}\\,=\\,[x_{1}^{(i)},x_{2}^{(i)},...\\,,x_{L}^{(i)}]$ .We consider the transformer model with a self-attention module followed by a linear MLP: ", "page_idx": 3}, {"type": "equation", "text": "$$\nF(X;W,W_{V},W_{K},W_{Q})=\\sum_{l=1}^{L}\\sum_{j=1}^{m_{1}}a_{j}\\left(w_{j}^{\\top}W_{V}X\\cdot\\mathrm{Softmax}\\Big(\\frac{X^{\\top}W_{K}^{\\top}W_{Q}x_{l}}{\\sqrt{m}}\\Big)\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the query matrix $W_{Q}\\in\\mathbb{R}^{m\\times d}$ , the key matrix $W_{K}\\in\\mathbb{R}^{m\\times d}$ , the value matrix $W_{V}\\in\\mathbb{R}^{m\\times d}$ \uff0c the hidden-layer MLP weights $W\\in\\mathbb{R}^{m_{1}\\times m}$ (with $w_{j}^{\\top}$ being the $j$ -th row of $W$ ), and the output-layer weights of the MLP $a\\in\\mathbb{R}^{m_{1}}$ . We define the linear MLP function of the transformer to be $G(\\mu)=$ $\\textstyle\\sum_{j=1}^{m_{1}}a_{j}w_{j}^{\\top}W_{V}\\mu$ . We now introduce some shorthand notations $K=W_{K}X,\\;Q=W_{Q}X,\\;V=$ $W_{V}X$ and let $k_{l}=W_{K}x_{l}$ . Notice that $K=[k_{1},k_{2},...\\,,k_{L}]$ . We further extend this shorthand to $q_{l}$ and $v_{l}$ . We also define functions $k(\\mu)={\\cal W}_{K}\\mu$ \uff0c $q(\\mu)=W_{Q}\\mu$ \uff0c $v(\\mu)=W_{V}\\mu$ . We introduce the shorthand for the score vector XWWoel and th ttention vector p = Softmax(s) Forthe atention vetor if $\\mu,\\nu\\in X^{(i)}$ $l(i,\\mu),l(i,\\nu)$ be te indies such that $X_{l(i,\\mu)}^{(i)}=\\mu$ \uff0c $X_{l(i,\\nu)}^{(i)}=\\nu$ and we define $\\begin{array}{r}{p_{q\\leftarrow\\mu,k\\leftarrow\\nu}^{(i)}:=p_{q\\leftarrow l(i,\\mu),k\\leftarrow l(i,\\nu)}^{(i)}:=\\mathrm{Softmax}\\left(\\frac{X^{(i)\\top}W_{K}^{\\top}W_{Q\\mu}\\mu}{\\sqrt{m}}\\right)_{l(i,\\nu)}.}\\end{array}$ ", "page_idx": 3}, {"type": "text", "text": "Initialization. We initialize $a_{j}\\ \\stackrel{i.i.d.}{\\sim}$ Uniform $(\\pm1)$ and the value of $a$ is fixed during training. The trainable parameters are $[W,W_{V},W_{K},W_{Q}]$ . We initialize $[W,W_{V},W_{K},W_{Q}]$ by $W_{i,j}$ i.k.d. $\\mathcal{N}(0,\\sigma_{1}^{2})$ and $(W_{V})_{i,j},(W_{K})_{i,j},(W_{Q})_{i,j}\\stackrel{i.i.d.}{\\sim}\\mathcal{N}(0,\\sigma_{0}^{2}).$ ", "page_idx": 3}, {"type": "text", "text": "Training. We adopt the cross-entropy loss $l(x)=\\log(1+\\exp(-x))$ . The gradient of the crossentropy lossis givenby $\\begin{array}{r}{l^{\\prime}(x)=-\\frac{\\bar{1}}{1+\\exp(x)}}\\end{array}$ and wedefine $\\begin{array}{r}{g(x)=\\frac{1}{1+\\exp(x)}}\\end{array}$ The model is rained by gradient fow to minimize the following empirical loss: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\widehat{L}(W,W_{V},W_{K},W_{Q})=\\frac{1}{n}\\sum_{i=1}^{n}l(y_{i}F(X^{(i)};W,W_{V},W_{K},W_{Q})).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Similarly, we define the generalization loss $L:=\\mathbb{E}_{(X,y)\\sim{\\mathcal{D}}}\\,\\ell(y F(X))$ . We introduce the parameter condition that we take throughout the entire analysis and proofs. ", "page_idx": 4}, {"type": "text", "text": "Condition 1. We make the following parameter choices in our analysis: ", "page_idx": 4}, {"type": "text", "text": "\u00b7 The embedding dimension and network width satisfy $m\\geq\\widetilde\\Omega(\\operatorname*{max}\\{m_{1},L^{2}\\})$ and $m_{1}\\geq\\widetilde{\\Omega}(1)$ \u00b7 The network weight initialization variance satisfies 0o = $\\begin{array}{r}{\\sigma_{0}=\\frac{1}{\\widetilde{\\Theta}(\\sqrt{L m})}}\\end{array}$ and $\\begin{array}{r}{\\sigma_{1}=\\frac{1}{\\widetilde{\\Theta}(\\sqrt{m_{1}})}}\\end{array}$ \u00b7The number of training samples and tokens satisfy $n\\geq\\widetilde\\Omega(L^{2})$ and $L\\ge\\widetilde\\Omega(1)$ \u00b7The failure probability satisfies $1/\\delta\\le\\mathrm{poly}(m)$ ", "page_idx": 4}, {"type": "text", "text": "3 Main Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Challenges. The essential goal is to derive the gradient fow update for each weight matrix (see the gradient expressions for all weight matrices in Appendix B). However, directly analyzing the dynamics of those weight matrices is extremely challenging, because: (i) keeping track of how the column and row spaces of each weight matrix change during training is difficult; and (ii) all attention and MLP weight matrices are affecting each other, leading to highly coupled dynamics. ", "page_idx": 4}, {"type": "text", "text": "Our General Idea. To overcome the above challenges, we first note that rather than tracking $W,W_{V},W_{K},W_{Q}$ directly, it is sufficient to analyzing their impact on inputs, i.e., $X^{\\top}W_{Q}^{\\top}W_{K}X$ and $a^{\\top}W W_{V}X$ , which are sufficient to compute $F(X;W,W_{V},W_{K},W_{Q})$ . Based on this observation, wefoladiflqonk $w_{j}^{(t)\\top}W_{V}^{(t)}\\mu$ and $\\nu^{\\top}W_{K}^{(t)\\top}W_{Q}^{(t)}\\mu$ Q'\u03bc(with respect to $t$ ) for all $\\mu,\\nu\\in\\{\\mu_{i}\\}_{i=1}^{d}$ (See Equation (6) and Equation (7) in Appendix C). We further ineludeaditioaeguationsto kep rack of $\\left\\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\\right\\rangle,\\,\\nu^{\\top}W_{V}^{(t)\\top}W_{V}^{(t)}\\mu,\\,\\nu^{\\top}W_{K}^{(t)\\top}W_{K}^{(t)}\\mu$ WK'\u03bc, and $\\nu^{\\top}W_{Q}^{(t)\\top}W_{Q}^{(t)}\\mu$ to complete the system. Intuitively, the additional equations keep track of the shape of the neurons and the word embedding after $W_{V},W_{K},W_{Q}$ transform. Although the dynamical system does not directly track the softmax, the softmax probability can be calculated via the scores of $\\dot{\\nu^{\\top}}W_{K}^{(t)\\top}W_{Q}^{(t)}\\mu$ The fulldynamialsystem s resented inApendinC. Then the rainingdynamies can be characterized by analyzing these differential equations (see a proof outline in Section 4). ", "page_idx": 4}, {"type": "text", "text": "In the next two theorems, we present our characterization of the training proces into two phases. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1 (Phase 1). With probability at least $1-\\delta$ over the randomness of weight initialization, there exists a time $T_{1}=\\tilde{O}(1/{m})$ suchthat ", "page_idx": 4}, {"type": "text", "text": "\u00b7 The linear MLP functions satisfy: $G^{(T_{1})}(\\mu_{1})\\geq\\Omega(1),\\,G^{(T_{1})}(\\mu_{2})\\geq\\Omega(1),\\,G^{(T_{1})}(\\mu_{3})\\leq-\\Omega(1).$ \u00b7All training samples are correctly classifed: $y_{i}F_{i}^{(T_{1})}>0$ for all $i\\in[n]$ \u00b7For $t\\ \\ \\in\\ [0,T_{1}]$ \uff0call dynamical variables $\\Big\\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\\Big\\rangle,\\ \\nu^{\\top}W_{K}^{(t)\\top}W_{Q}^{(t)}\\mu,\\ \\nu^{\\top}W_{V}^{(t)\\top}W_{V}^{(t)}\\mu,$ $\\nu^{\\top}W_{K}^{(t)\\top}W_{K}^{(t)}\\mu,$ and $\\nu^{\\top}W_{Q}^{(t)\\top}W_{Q}^{(t)}\\mu$ V? \u03bc are close to their initialization values. \u25cf Training loss is still large: $\\widehat{L}^{(T_{1})}=\\Theta(1)$ ", "page_idx": 4}, {"type": "text", "text": "In Theorem 3.1, item 1 implies that in a short time,the linear MLP function $G^{(T_{1})}(\\cdot)$ positively aligns with the two target signals $\\mu_{1}$ and $\\mu_{2}$ , but negatively aligns with the common token $\\mu_{3}$ . This further guarantees item 2 of Theorem 3.1 that all training samples are classified correctly. Further, item 3 of Theorem 3.1 indicates that the attention matrices are still close to their initialization values, and hence have not started to learn any knowledge yet. This results in item 4 of Theorem 3.1, which shows that the training loss is still large. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.2 (Phase 2). With probability at least $1-\\delta$ over therandomness of weight initialization, thereexistsatimerange $(T_{1},T_{2})$ With $T_{2}={\\mathrm{poly}}(m)$ suchthatforall $t\\in(T_{1},T_{2})$ ", "page_idx": 4}, {"type": "text", "text": "$\\mu_{2}^{\\top}W_{K}^{(t)\\top}W_{Q}^{(t)}\\mu_{1}$ and TWTW2 increase, whereas $\\mu_{3}^{\\top}W_{K}^{(t)\\top}W_{Q}^{(t)}\\mu_{1}$ and $\\mu_{3}^{\\top}W_{K}^{(t)\\top}W_{Q}^{(t)}\\mu_{2}$ decrease. ", "page_idx": 4}, {"type": "text", "text": "$\\mu_{1}^{\\top}W_{V}^{(t)\\top}W_{V}^{(t)}\\mu_{2}$ increases, whereas $\\mu_{1}^{\\top}W_{V}^{(t)\\top}W_{V}^{(t)}\\mu_{3}$ and $\\mu_{2}^{\\top}W_{V}^{(t)\\top}W_{V}^{(t)}\\mu_{3}$ decrease. ", "page_idx": 5}, {"type": "text", "text": "\u00b7 Linear MLP functions satisfy: $G^{(t)}(\\mu_{1})\\geq\\Omega(1),\\ G^{(t)}(\\mu_{2})\\geq\\Omega(1),\\ -G^{(t)}(\\mu_{3})\\leq\\Omega(1).$ $G^{(t)}(\\mu_{1})\\,+\\,G^{(t)}(\\mu_{2})\\,+\\,G^{(t)}(\\mu_{3})\\;\\geq\\;\\Omega(1),\\ G^{(t)}(\\mu_{1})\\,+\\,G^{(t)}(\\mu_{3})\\;\\leq\\;-\\Omega(1)\\;a$ nd $G^{(t)}(\\mu_{2})\\,+$ $G^{(t)}(\\mu_{3})\\leq-\\Omega(1)$ ", "page_idx": 5}, {"type": "text", "text": "In Theorem 3.2, item 1 indicates that, during Phase 2, gradient flow drives the self-attention module to weigh more between the two target signals $\\mu_{1}$ and $\\mu_{2}$ , and to weigh less between one of these signals and the common token $\\mu_{3}$ . Item 2 indicates that gradient fow drives the value matrix $W_{V}$ to positively align the two target signals $\\mu_{1}$ and $\\mu_{2}$ , but negatively align one target signal ( $\\dot{\\boldsymbol{\\mu}}_{1}$ or $\\mu_{2}$ with the common token $\\mu_{3}$ . Further, the last two items indicate that the MLP continue to classify correctly and further enlarge the classification margin. Hence, all items in Theorem 3.2 collectively indicate that attention and MLP evolve jointly to enlarge the classification margin and hence drive the loss value to decrease in Phase 2. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.3 (Near Minimum Training Loss and Attention). With probability at least $1-\\delta$ there existsatime $T^{\\star}=\\Theta(\\mathrm{poly}(m))$ suchthat ", "page_idx": 5}, {"type": "text", "text": "\u00b7 The training and generalization losses satisfy $\\widehat{L}^{(T^{\\star})}\\le1/\\mathrm{poly}(m)$ and $L^{(T^{\\star})}\\leq1/\\mathrm{poly}(m).$ ", "page_idx": 5}, {"type": "text", "text": "\u00b7 The attention matrices satisfies: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{W_{K}^{(T^{\\star})\\top}W_{Q}^{(T^{\\star})}=W_{K}^{(0)^{\\top}}W_{Q}^{(0)}+\\sum_{i_{1},i_{2}\\in[d]}C_{i_{1},i_{2}}^{(T^{\\star})}\\mu_{i_{1}}\\mu_{i_{2}}^{\\top},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$\\begin{array}{r l r}{C_{1,2}^{(T^{\\star})},C_{2,1}^{(T^{\\star})},-C_{3,1}^{(T^{\\star})},-C_{3,2}^{(T^{\\star})}}&{{}=}&{\\Theta\\left(\\frac{\\sigma_{0}^{2}m}{\\sqrt{m}L\\sigma_{1}^{2}m m_{1}}+\\frac{\\sigma_{0}^{2}\\sqrt{m}}{\\sqrt{m}\\sigma_{1}^{2}m m_{1}}\\right)}\\end{array}$ ", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Theorem 3.3 indicates that both training and test losses converge nearly to zero as long as the embedding dimension $m$ is sufficiently large, because both the attention and MLP matrices are trained towards enlarging the classification margin in Phase 2. Theorem 3.3 also provides the explicit form of the attention matrix in Equation (3), in which the second term captures the learned information $\\bar{C}_{1,2}^{(T^{\\star})}$ $C_{2,1}^{(T^{\\star})}$ coupingftttarget sigals\u03bcand \u03bcand tare negativects andC3,2 encourages strong negative coupling of one target signal $\\mu_{1}$ Or $\\mu_{2}$ and the common token $\\mu_{3}$ .Allthese attention terms contribute to enlarge correct classification margin. Further, the coefficients between all other random tokens are order-level smaller and hence do not corrupt the correct classification. ", "page_idx": 5}, {"type": "text", "text": "Synthetic Experiment: We next verify our theory and the two-phase characterization of the training process via synthetic experiments (see the experiment setup in Appendix A). ", "page_idx": 5}, {"type": "image", "img_path": "w6q46IslSR/tmp/1fc595a3815d3fd69d9a75f4bf6c6742416705f4177e33c5f3ae4285e4ddf50d.jpg", "img_caption": ["(a) Attention score correlation "], "img_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "w6q46IslSR/tmp/868aefef692df44232c2b4d243b90103ddd452ac0b50a5c864361b7902bd7388.jpg", "img_caption": ["(b) Training loss "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 1: Synthetic experiments with illustration of two training phases. The detailed experiment setup can be found in Appendix A. ", "page_idx": 5}, {"type": "text", "text": "FEigure (al shows how theatentionscorecorlaton $\\mu_{i}^{\\top}W_{K}^{(t)\\top}W_{Q}^{(t)}\\mu_{j}$ evolve during thetraining It is clear that these scores do not change significantly in Phase 1, verifying Theorem 3.1. In Phase 2, the score correlation between two target signals $\\mu_{1}$ and $\\mu_{2}$ increases, and the score between one target signal and the common token decreases, verifying Theorem 3.2. ", "page_idx": 5}, {"type": "text", "text": "Figure 1 (b) plots how the training loss changes during the two phases of training. The blue curve (indexed by \u201closs') represents the overall training loss of all samples. The other curves correspond to the training loss of four types of samples as indicated in the legend. In Phase 1, the training loss for samples with both target signals (i.e., orange curve) decreases because the linear MLP layer aligns with the target signals (verifying Lemma 4.1 in Section 4.1). The training loss for samples with one target signal and the common token (i.e., green or red curves) first increases because the linear MLP layer initially has not aligned negatively enough with the common token yet (as captured by Lemma 4.2 in Section 4.1), and then decreases in the later stage of Phase 1 when the MLP layer aligns negatively with the common token (as captured by Lemma 4.3 in Section 4.1). All loss functions decrease in Phase 2 because all attention matrices and linear MLP jointly enlarge the classification margin, verifying Theorem 3.2. ", "page_idx": 6}, {"type": "text", "text": "4 Proof Outline: Two-phase Gradient Flow Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1  Phase 1: Alignment of Linear MLP for Correct Classification ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In Phase 1, the linear MLP quickly aligns with the two target word tokens while all attention matrices stay roughly unchanged from their initialization values. We show that the linear MLP functions $|{\\cal G}^{(t)}(\\mu_{1})|,|{\\cal G}^{(t)}(\\mu_{2})|,|{\\cal G}^{(t)}(\\mu_{3})|$ become sufficiently large (larger than some constant threshold) so that all training samples are correctly classified at the end of Phase 1. ", "page_idx": 6}, {"type": "text", "text": "We first analyze the dynamical system at the initialization. In particular, the following lemma shows that at the initialization, the linear MLP layer receives a sufficiently large gradient from the two target signals, and hence samples with the two target signals will be classified correctly as co-occurrence soon afte the training starts. ", "page_idx": 6}, {"type": "text", "text": "Lemma 4.1 (Same as Lemma E.4). With probability at least $1-\\delta$ over the weight initialization, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\forall\\mu\\in\\{\\mu_{1},\\mu_{2}\\}:}&{\\frac{\\partial a_{j}w_{j}^{(0)}W_{V}^{(0)}\\mu}{\\partial t}=\\Theta((\\sigma_{0}^{2}+\\sigma_{1}^{2})m).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Further, by the definition of Phase 1 (see Definition E.2 for a formal definition), the gradients of the attention matrices in the dynamical system are much smaller than that of linear MLP given in Lemma 4.1. This implies that during Phase 1, mainly the linear MLP is performing learning, whereas all the attention matrices are changing slowly from their initialization. Based on this, we have $\\begin{array}{r}{\\frac{\\partial}{\\partial t}G^{(t)}(\\mu_{1})=\\Theta((\\sigma_{0}^{2}+\\sigma_{1}^{2})m m_{1})}\\end{array}$ Whichimpies thati takes only $O(1/(\\sigma_{0}^{2}+\\sigma_{1}^{2})m m_{1})$ iterations for $G^{(t)}(\\mu_{1})$ to reach a certain constant magnitude. ", "page_idx": 6}, {"type": "text", "text": "Lemma 4.1 indicates that the samples with co-occurrence of the two target signals are classified correctly. The following lemma shows that the initial gradient from the common token, i.e., the gradient of $G^{(t)}(\\mu_{3})$ , is much smaller than the gradient from the two target signals, which implies that the samples with only one target signal may be classified incorrectly as co-occurrence (since the network in this case will output a positive value). This is verified empirically by our experiments in Figure 1 (b), where the loss function corresponding to only one target signal and the common token first increases in Phase 1. ", "page_idx": 6}, {"type": "text", "text": "Lemma 4.2 (Same as Lemma E.16). Let $F=\\operatorname*{max}_{i}|F_{i}^{(0)}|$ . With probability at least $1-\\delta$ over the weight initialization, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\frac{\\partial a_{j}w_{j}^{(0)}W_{V}^{(0)}\\mu_{3}}{\\partial t}\\right|=\\widetilde{O}\\left(\\sigma_{1}^{2}\\sqrt{m m_{1}}+\\sigma_{0}^{2}\\sqrt{m L}+\\sigma_{1}^{2}m F\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Notice that the model output $F$ depends on the weight initialization scale and can be made small. ", "page_idx": 6}, {"type": "text", "text": "We next show in the following lemma that the gradient aG<(\u03bca3) o the common token will quickly become negative soon after the training begins, which drives the transformer model to output a negative value when it sees those types of samples. This implies that negative samples (without co-occurrence of two target tokens) will be classified correctly towards the end of Phase 1. This is also verified empirically by our experiments in Figure 1 (b), where the loss function corresponding to only one target signal and the common token descreases towards the end of Phase 1. ", "page_idx": 6}, {"type": "text", "text": "Lemma 4.3 (Abbreviated from Theorem E.19). There exists a time $T_{0.5}\\leq T_{1}$ andaconstant $C$ such that for all $t\\in[T_{0.5},T_{1}]$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(1+C)\\operatorname*{max}\\left(\\frac{\\partial G^{(t)}(\\mu_{1})}{\\partial t},\\frac{\\partial G^{(t)}(\\mu_{2})}{\\partial t}\\right)\\leq-\\frac{\\partial G^{(t)}(\\mu_{3})}{\\partial t}\\leq(1-C)\\left(\\frac{\\partial G^{(t)}(\\mu_{1})}{\\partial t}+\\frac{\\partial G^{(t)}(\\mu_{2})}{\\partial t}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Proof Intuition of Lemma 4.3. We first note that the term ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{n}\\sum_{i=1}^{n}g_{i}^{(0)}y_{i}\\sum_{l_{2}=1}^{L}\\sum_{j_{2}=1}^{m_{1}}\\|w_{j_{2}}^{(0)}\\|_{2}^{2}(X^{(i)}p_{l_{2}}^{(0,i)})^{\\top}\\mu}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "makes the major contribution to the gradient t Gt)(\u03bc3  Such a term is small at the initialization due to the cancellation effect from positive and negative $y_{i}$ 's. However, since the linear MLP $G$ will positivl aign thetwo target signals athe begining,fo the samples with posive labels,' $g_{i}^{(t)}$ will decrease, whereas for samples with only one target signal, $g_{i}^{(t)}$ will increase. Hence, $\\frac{\\partial a_{j}w_{j}^{(t)}W_{V}^{(t)}\\mu_{3}}{\\partial t}$ will become negative. This trend will continue until the gradient from $\\mu_{3}$ starts to match the gradients from $\\mu_{1},\\mu_{2}$ , which is what we establish in Lemma 4.3. \u53e3 ", "page_idx": 7}, {"type": "text", "text": "Using Lemma 4.3, we can show that all training samples are correctly classified at end of Phase 1. ", "page_idx": 7}, {"type": "text", "text": "4.2 Phase 2: Evolution of Attention and MLP for Large Classification Margin ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In Phase 2, both atention and MLP matrices evolve towards enlarging the classification margin, thus driving the loss value small. ", "page_idx": 7}, {"type": "text", "text": "We now analyze what happens in Phase 2. Let $T_{2}$ denote the end of Phase 2. Recall that at the end of Phase 1, we have $G^{(t)}(\\mu_{1}),G^{(t)}(\\mu_{2}),-G^{(t)}(\\mu_{3})\\ge\\Omega(1)$ We will mainly need to show that such a condition continues to hold in Phase 2, so that attention matrices will evolve with MLP to learn better classifiers. To this end, we exam the following gradient fow in the dynamical system: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial G^{(t)}(\\mu)}{\\partial t}=\\!\\frac{1}{n}\\sum_{\\substack{i_{2}:\\;\\mu\\in X^{(i_{2})}}}g_{i_{2}}^{(t)}y_{i_{2}}\\sum_{l_{2}=1}^{L}p_{q\\leftarrow l_{2},k\\leftarrow\\mu}^{(t,i_{2})}\\cdot\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}a_{j_{1}}a_{j_{2}}\\left<w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\\right>}\\\\ {\\displaystyle\\ +\\,\\frac{m_{1}}{n}\\sum_{i_{1}=1}^{n}g_{i_{1}}^{(t)}y_{i_{1}}\\sum_{l_{1}=1}^{L}a_{j_{1}}p_{l_{1}}^{(t,i_{1})\\top}V^{(t,i_{1})\\top}W_{V}^{(t)}\\mu.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "It has been proved that at the end of Phase 1, for $\\mu\\in\\left\\{\\mu_{1},\\mu_{2},\\mu_{3}\\right\\}$ , we have $\\begin{array}{r}{\\left|\\sum_{j_{1}}\\frac{\\partial w_{j_{1}}^{(t)\\top}}{\\partial t}W_{V}^{(t)}\\mu\\right|\\ll}\\end{array}$ $\\begin{array}{r}{\\left|\\sum_{j_{1}}w_{j_{1}}^{(t)\\top}\\frac{\\partial W_{V}^{(t)}}{\\partial t}\\mu\\right|}\\end{array}$ since the magnitude of m=1m=1 a w aj w92) is large. Assume this can hold for long enough (which we can indeed prove later). Then, we only need to focus on the first term in the sum on the right-hand side in Equation (4). On the other hand, from the dynamical system, we can calculate ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial t}\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle=\\frac{2m_{1}}{n}\\sum_{i=1}^{n}g_{i}^{(t)}y_{i}F_{i}^{(t)}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Thus, if $y_{i}F_{i}^{(t)}\\,>\\,0$ for all l $i\\;\\in\\;[n]$ then $\\begin{array}{r}{\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle}\\end{array}$ is always increasing. Thus, $\\frac{\\partial G^{(t)}(\\mu)}{\\partial t}$ mainly depends on the behavior of iz; \u03bceX(te) g2 yia $\\begin{array}{r}{\\frac{!}{\\i}\\sum_{i_{2}:\\ \\mu\\in X^{(i_{2})}}g_{i_{2}}^{(t)}y_{i_{2}}\\sum_{l_{2}=1}^{L}p_{q\\leftarrow l_{2},k\\leftarrow\\mu}^{(t,i_{2})}}\\end{array}$ Further, this is also a key quantity we need to analyze $\\frac{\\partial\\nu^{\\top}W_{V}^{(t)\\top}W_{V}^{(t)}\\mu}{\\partial t}$ and $\\frac{\\partial\\nu^{\\top}W_{K}^{(t)\\top}W_{Q}^{(t)}\\mu}{\\partial t}$ Note $\\textstyle{\\frac{1}{n}}\\sum_{i_{2}}$ $\\begin{array}{r}{\\L_{\\mu\\in X^{(i_{2})}}\\,g_{i_{2}}^{(t)}\\L{y}_{i_{2}}\\sum_{l_{2}=1}^{L}p_{q\\leftarrow l_{2},k\\leftarrow\\mu}^{(t,i_{2})}\\approx\\frac{1}{n}\\sum_{i_{2}}}\\end{array}$ $\\mu{\\in}X^{(i_{2})}\\ g_{i_{2}}^{(t)}y_{i_{2}}$ $p_{q\\leftarrow l_{2},k\\leftarrow\\mu}^{(t,i_{2})}\\approx1/L$ holds at the beginning of Phase 2. Later, we are going to prove convergence of the training loss via the following: (1) the training loss can decrease if the softmax probability is uniform; (2) even though the softmax probability wili deviate from uniform distribution during training, we can bound such deviation and the loss value can still decrease. ", "page_idx": 7}, {"type": "text", "text": "Automatic balancing of gradients. As argued above, our main focus is on analyzing the behavior of $\\textstyle{\\frac{1}{n}}\\sum_{i_{2}:\\;\\mu\\in X^{(i_{2})}}g_{i_{2}}^{(t)}{\\bar{y_{i_{2}}}}$ This cnstsof two pats i) Lemma4 which shows that the two groups of samples with only the presence of one target signal have gradients $\\textstyle\\sum_{i\\in I_{2}}g_{i}^{(t)}$ and $\\textstyle\\sum_{i\\in I_{3}}g_{i}^{(t)}$ close to each other durin raining; and (i) Lemma 4.5, which shows that the gradient gaps $\\sum_{i\\in I_{1}}g_{i}^{(t)}-$ $\\textstyle\\sum_{i\\in I_{2}}g_{i}^{(t)}$ and $\\begin{array}{r}{\\sum_{i\\in I_{2}\\cup I_{3}\\cup I_{4}}g_{i}^{(t)}-\\sum_{i\\in I_{1}}g_{i}^{(t)}}\\end{array}$ are not to sml ompared ith $\\textstyle\\sum_{i\\in[n]}g_{i}^{(t)}$ Both ", "page_idx": 7}, {"type": "text", "text": "Lemmas 4.4 and 4.5 establish that the ratio of those important gradients are kept within certain ranges during training. We call such a key property as automatic balancing of gradients, which is further used for proving that the gradient flow can drive the training loss small. ", "page_idx": 8}, {"type": "text", "text": "Lemma 4.4 (Same as Lemma F.5). For $t\\in[T_{1},T_{2}].$ there exists a small constant $C\\ll1$ such that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\frac{\\left|\\sum_{i\\in I_{2}}g_{i}^{(t)}-\\sum_{i\\in I_{3}}g_{i}^{(t)}\\right|}{\\operatorname*{min}(\\sum_{i\\in I_{2}}g_{i}^{(t)},\\sum_{i\\in I_{3}}g_{i}^{(t)})}\\leq C.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Proof Intuion of Lemma4. The intuitionbehind the resultis as fllows. If $\\textstyle\\sum_{i\\in I_{2}}g_{i}^{(t)}$ becomes much bigger than ie1s 9 g(t) during the training, then \u2265ieI 9(t) $\\begin{array}{r}{\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{2}}g_{i}^{(t)}}\\end{array}$ is much smaller than $\\begin{array}{r}{\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{3}}g_{i}^{(t)}}\\end{array}$ Which makes $\\frac{\\partial G^{(t)}(\\mu_{1})}{\\partial t}\\,<\\,\\frac{\\partial G^{(t)}(\\mu_{2})}{\\partial t}$ aG(\u03bc2). I is not hard to show that random tokens make negligible contributions to the gradient. Thus, for $i\\in I_{2}$ , we have $\\begin{array}{r}{\\frac{\\partial F_{i}^{(t)}}{\\partial t}\\approx\\frac{\\partial G^{(t)}(\\mu_{1})}{\\partial t}+}\\end{array}$ dG(t)(\u03bca). By the chain rule, we have $\\begin{array}{r}{\\frac{\\partial g_{i}^{(t)}}{\\partial t}=g^{\\prime}(y_{i}F_{i}^{(t)})y_{i}\\frac{\\partial F_{i}^{(t)}}{\\partial t}}\\end{array}$ )y. Since $\\frac{\\partial G^{(t)}(\\mu_{3})}{\\partial t}<0$ $\\frac{\\partial G^{(t)}(\\mu_{1})}{\\partial t}<$ G(\u03bc2) , then ie1, 9i $\\textstyle\\sum_{i\\in I_{3}}g_{i}^{(t)}$ $\\begin{array}{r}{-\\frac{\\partial}{\\partial t}\\sum_{i\\in I_{2}}g_{i}^{(t)}>-\\frac{\\partial}{\\partial t}\\sum_{i\\in I_{3}}g_{i}^{(t)}}\\end{array}$ InAppediallrovLmabnalin $\\begin{array}{r}{\\sum_{i\\in I_{2}}g_{i}^{(t)}/\\sum_{i\\in I_{3}}g_{i}^{(t)}}\\end{array}$ , and show that this ratio hangs over around 1 during training. \u53e3 ", "page_idx": 8}, {"type": "text", "text": "Lemma 4.5 (Abbreviated from Lemma F.6). For $t\\in[T_{1},T_{2}]$ the gradient satisfies that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\frac{\\sum_{i\\in[n]}g_{i}^{(t)}}{\\sum_{i\\in I_{2}\\cup I_{3}\\cup I_{4}}g_{i}^{(t)}-\\sum_{i\\in I_{1}}g_{i}^{(t)}}=O(1),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "equation", "text": "$$\n\\frac{\\sum_{i\\in[n]}g_{i}^{(t)}}{\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{2}}g_{i}^{(t)}}=O(1).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Further, for some constant $C$ ,we have ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathrm{1}+C)\\operatorname*{max}\\left(\\frac{\\partial G^{(t)}(\\mu_{1})}{\\partial t},\\frac{\\partial G^{(t)}(\\mu_{2})}{\\partial t}\\right)\\leq-\\frac{\\partial G^{(t)}(\\mu_{3})}{\\partial t}\\leq(1-C)\\left(\\frac{\\partial G^{(t)}(\\mu_{1})}{\\partial t}+\\frac{\\partial G^{(t)}(\\mu_{2})}{\\partial t}\\right).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Proof Sketch of Lemma 4.5. The proof of Lemma 4.5 relies on analyzing how the ratio between a ad changWeshwthat this atowillhang oer aroud serangeReall the relationship that $\\begin{array}{r}{\\frac{\\bar{\\partial}G^{(t)}(\\mu)}{\\partial t}\\approx\\frac{1}{n}\\sum_{i_{2}:\\;\\mu\\in X^{(i_{2})}}g_{i_{2}}^{(t)}y_{i_{2}}\\cdot\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}a_{j_{1}}a_{j_{2}}\\left\\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\\right\\rangle}\\end{array}$ Itis not hard to show that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\frac{-\\frac{\\partial G^{(t)}(\\mu_{3})}{\\partial t}}{\\frac{\\partial G^{(t)}(\\mu_{1})}{\\partial t}}\\approx\\frac{-\\sum_{i\\in I_{1}}g_{i}^{(t)}+\\sum_{i\\in I_{2}\\cup I_{3}\\cup I_{4}}g_{i}^{(t)}}{\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{2}}g_{i}^{(t)}}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Define R(t) := Zten+eurag . Solving when $\\begin{array}{r}{\\frac{\\partial}{\\partial t}R(t)\\geq0}\\end{array}$ yields a quadratic inequality, and further analysis shows that the root is contractive and is within some specific range. \u53e3 ", "page_idx": 8}, {"type": "text", "text": "Utilizing the gradient automatic balancing properties, the following corollary characterizes how the attention matrices in the dynamical system change in Phase 2. In particular, we can show that after $W_{V}$ -transform, $\\mu_{1}$ and $\\mu_{2}$ become more positively correlated whereas $\\mu_{1}$ and $\\mu_{3}$ (also $\\mu_{2}$ and $\\mu_{3}$ become negatively correlated. This is a direct result following from updates of the dynamical system. ", "page_idx": 8}, {"type": "text", "text": "Corollary 4.6 (Abbreviated from Corollary F.13). For $t\\in[T_{1},T_{2}],$ ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial t}\\mu_{2}^{\\top}W_{V}^{(t)\\top}W_{V}^{(t)}\\mu_{1}>0,\\qquad\\qquad\\qquad\\frac{\\partial}{\\partial t}\\mu_{1}^{\\top}W_{V}^{(t)\\top}W_{V}^{(t)}\\mu_{3}<0.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Since we have analyzed how $G^{(t)}(\\cdot)$ will change in stage 2, we can utilize this information to analyze the change of softmax attention via the following relationship: by Appendix C, we can derive ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mu_{1}^{\\top}W_{K}^{(t)\\top}W_{Q}^{(t)}\\mu_{2}}{\\partial t}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\displaystyle\\frac{1}{n\\sqrt{m}}\\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\\sum_{l=1}^{L}\\mu_{1}^{\\top}W_{K}^{(t)\\top}K^{(t,i)}\\cdot\\operatorname{diag}\\left(G^{(t)}(X^{(i)})-(G^{(t)}(X^{(i)}))^{\\top}p_{l}^{(t,i)}\\right)p_{l}^{(t,i)}x_{l}^{(i)\\top}\\mu_{2}}\\\\ &{\\displaystyle+\\,\\frac{1}{n\\sqrt{m}}\\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\\sum_{l=1}^{L}\\mu_{2}^{\\top}W_{Q}^{(t)\\top}q_{l}^{(t,i)}p_{l}^{(t,i)\\top}\\cdot\\operatorname{diag}\\left(G^{(t)}(X^{(i)})-(G^{(t)}(X^{(i)}))^{\\top}p_{l}^{(t,i)}\\right)X^{(i)\\top}\\mu_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "The following lemma shows that the attention score between the two target signals $\\mu_{1}$ and $\\mu_{2}$ increases, whereas that between one target signal $\\mu_{1}$ or $\\mu_{2}$ and the common token $\\mu_{3}$ decreases. Lemma 4.7 (Abbreviated from Lemma F.16). For $\\mu,\\nu\\in\\{\\mu_{1},\\mu_{2}\\},\\,\\mu\\neq\\nu_{\\!\\scriptscriptstyle{i}}$ and for $t\\in[T_{1},T_{2}]$ $\\frac{\\partial}{\\partial t}\\nu^{\\top}W_{K}^{(t)\\top}W_{Q}^{(t)}\\mu=\\frac{1}{\\sqrt{m}}\\tilde{\\Theta}(\\widehat{L}^{(t)}\\sigma_{0}^{2}m)\\frac{1}{L},\\qquad\\frac{\\partial}{\\partial t}\\mu_{3}^{\\top}W_{K}^{(t)\\top}W_{Q}^{(t)}\\mu=-\\frac{1}{\\sqrt{m}}\\tilde{\\Theta}(\\widehat{L}^{(t)}\\sigma_{0}^{2}m)\\frac{1}{L}.$ ", "page_idx": 9}, {"type": "text", "text": "Lemma 4.7 is keeping track of the attention coefcients C,i2 in Theorem 3.3 via gradient flow, which proves the second item of Theorem 3.3. ", "page_idx": 9}, {"type": "text", "text": "5 Discussion and Future Directions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we developed a novel gradient flow based framework for analyzing the training dynamics of a one-layer transformer to recognize co-occurring tokens. We provided a two-phase characterization of the training process. In Phase 1, the linear MLP layer is trained to classify samples correctly, with attention weights almost unchanged. In Phase 2, both attention matrices and the linear MLP jointly evolve to enlarge the classification margin, thus reducing the loss to near minimum. ", "page_idx": 9}, {"type": "text", "text": "As future work, it will be interesting to analyze more general transformer architectures such as multi-headed attention, multi-layer transformer, etc. Further, it is of interest to study the dynamics of more advanced gradient descent algorithms such as gradient descent with adaptive learning rate, with momentum, etc., and explore how the hyperparameters will affect the training dynamics. Another direction is to study more practical language sequences where tokens are generated in a correlated fashion. Then the next token prediction becomes an intriguing problem. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "H. Yang would like to thank Jason D. Lee and Yunwei Ren for insightful discussion and suggestions. This work was performed under the auspices of the U.S. Department of Energy by the Lawrence Livermore National Laboratory under Contract No. DE-AC52-07NA27344 and supported by the LLNL-LDRD Program under Project No. 22-SI-004 and 24-ERD-010. The work of Y. Liang was supported in part by the U.S. National Science Foundation under the grants ECCS-2113860 and DMS-2134145. The work of Z. Wang was in part supported by an NSF Scale-MoDL grant (award number: 2133861) and the CAREER Award (award number: 2145346). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[ACDS24] Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement preconditioned gradient descent for in-context learning. Advances in Neural Information Processing Systems, 36, 2024. ", "page_idx": 9}, {"type": "text", "text": "$[\\mathrm{ADF}^{+}23]$ 1Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.   \n$[\\mathbf{BCE}^{+}23]$ S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712,2023.   \n$[\\mathbf{BCW}^{+}23]$ Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. arXiv preprint arXiv:2306.04637,2023. [CL24] Sitan Chen and Yuanzhi L1. Provably learning a muiti-nead attention layer. arXiv preprint arXiv:2402.04084, 2024.   \n[CSWY24] Siyu Chen, Heejune Sheen, Tianhao Wang, and Zhuoran Yang. Training dynamics of multi-head sofmax attention for in-context learning: mergence, convergence, and optimality. arXiv preprint arXiv:2402.19442, 2024. [Dam18] Friederick J Damerau. Markov models and linguistic theory, volume 95. Walter de Gruyter GmbH & Co KG, 2018.   \n$[\\mathrm{DBK}^{+}20]$ Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020.   \n[DCLT18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pretraining of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.   \n[EGKZ22] Benjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and variable creation in self-attention mechanisms. In International Conference on Machine Learning, pages 5793-5831. PMLR, 2022. [HCL23] Yu Huang, Yuan Cheng, and Yingbin Liang. In-context convergence of transformers. arXiv preprint arXiv:2310.05249, 2023.   \n[HWCL24] Yu Huang, Zixin Wen, Yuejie Chi, and Yingbin Liang. Transformers provably learn feature-position correlations in masked image modeling.  arXiv preprint arXiv:2403.02233, 2024. [JSL22] Samy Jelassi, Michael Sander, and Yuanzhi Li. Vision transformers provably learn spatial structure. Advances in Neural Information Processing Systems, 35:37822- 37836, 2022.   \n[LAG $^{+}22$ 1 Bingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Transformers learn shortcuts to automata. In The Eleventh International Conference on Learning Representations, 2022.   \n[LLC+21] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012-10022, 2021. [LLR23] Yuchen Li, Yuanzhi Li, and Andrej Risteski. How do transformers learn topic structure: Towards a mechanistic understanding. arXiv preprint arXiv:2303.04245, 2023.   \n[LWL $^{+}24\\$ 1 Hongkang Li, Meng Wang, Songtao Lu, Xiaodong Cui, and Pin-Yu Chen. Training nonlinear transformers for efficient in-context learning: A theoretical learning and generalization analysis. arXiv preprint arXiv:2402.15607, 2024.   \n[LWLC22] Hongkang Li, Meng Wang, Sijia Liu, and Pin- Yu Chen. A theoretical understanding of shallow vision transformers: Learning, generalization, and sample complexity. In The Eleventh International Conference on Learning Representations, 2022.   \n$[\\mathrm{LWM}^{+}23]$ Hongkang Li, Meng Wang, Tengfei Ma, Sijia Liu, Zaixi Zhang, and Pin-Yu Chen. What improves the generalization of graph transformer? a theoretical dive into self-attention and positional encoding. NeuriPS 2023 Workshop: New Frontiers in Graph Learning, 2023, 2023. [MH23] James Manyika and Sissie Hsiao. An overview of bard: an early experiment with generative ai. Al. Google Static Documents, 2, 2023. [MS99] Christopher Manning and Hinrich Schutze. Foundations of statistical natural language processing. MIT press, 1999. [NDL24] Eshaan Nichani, Alex Damian, and Jason D Lee. How transformers learn causal structure with gradient descent. arXiv preprint arXiv:2402.14735, 2024. [Ope23]  OpenA1. Gpt-4 technical report, 2023. [PBM21] Jorge Perez, Pablo Barcelo, and Javier Marinkovic. Attention is turing complete. The Journal of Machine Learning Research, 22(1):3463-3497, 2021. [SHT23] Clayton Sanford, Daniel Hsu, and Matus Telgarsky. Representational strengths and limitations of transformers. arXiv preprint arXiv:2306.02896, 2023. [Tea23] Gemini Team. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805,2023. [Thr24] Christos Thrampoulidis. Implicit bias of next-token prediction. arXiv:2402.18551, 2024. $[\\mathrm{TLI}^{+}23]$ Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and effcient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [TLTO23] Davoud Ataee Tarzanagh, Yingcong Li, Christos Thrampoulidis, and Samet Oymak. Transformers as support vector machines. In NeurIPS 2023 Workshop on Mathematics of Modern Machine Learning, 2023. [TLZO23] Davoud Ataee Tarzanagh, Yingcong Li, Xuechen Zhang, and Samet Oymak. Maxmargin token selection in attention mechanism. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[TWCD23] Yuandong Tian, Yiping Wang, Beidi Chen, and Simon Du. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. arXiv preprint arXiv:2305.16380, 2023. $[\\mathrm{TWZ}^{+}23]$ Yuandong Tian, Yiping Wang, Zhenyu Zhang, Beidi Chen, and Simon Shaolei Du. Joma: Demystifying multilayer transformers via joint dynamics of mlp and attention. In The Twelfth International Conference on Learning Representations, 2023. [VDT24] Bhavya Vasudeva, Puneesh Deora, and Christos Thrampoulidis. Implicit bias and fast convergence rates for self-attention. arXiv preprint arXiv:2402.05738, 2024.   \n$[\\mathrm{VONR}^{+}23]$ Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, pages 35151-35174. PMLR, 2023. $[\\mathrm{VSP^{+}17}]$ Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [WCM22] Colin Wei, Yining Chen, and Tengyu Ma. Statistically meaningful approximation: a case study on approximating turing machines with transformers. Advances in Neural Information Processing Systems, 35: 12071-12083, 2022.   \n[WLCC23] Yongtao Wu, Fanghui Liu, Grigorios Chrysos, and Volkan Cevher. On the convergence of encoder-only shallow transformers. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [WXM21] Colin Wei, Sang Michael Xie, and Tengyu Ma. Why do pretrained language models help in downstream tasks? an analysis of head and prompt tuning. Advances in Neural Information Processing Systems, 34:16158-16170, 2021. $[\\mathrm{YBR}^{+}19]$ Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Are transformers universal approximators of sequence-to-sequence functions? In International Conference on Learning Representations, 2019. [ZFB24] Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models in-context. Journal of Machine Learning Research, 25(49):1-55, 2024.   \n[ZPGA23] Haoyu Zhao, Abhishek Panigrahi, Rong Ge, and Sanjeev Arora. Do transformers parse while predicting the masked word? arXiv preprint arXiv:2303.08117, 2023.   \n[ZZYW23] Yufeng Zhang, Fengzhuo Zhang, Zhuoran Yang, and Zhaoran Wang. What and how does in-context learning learn? bayesian model averaging, parameterization, and generalization. arXiv preprint arXiv:2305.19420, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "1  Introduction 1.1  Related Work ", "page_idx": 13}, {"type": "text", "text": "2  Problem Setting 3 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "2.1Data Model 3   \n2.2  Transformer Architecture and Training . ", "page_idx": 13}, {"type": "text", "text": "3  Main Results 5 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "4  Proof Outline: Two-phase Gradient Flow Analysis ", "page_idx": 13}, {"type": "text", "text": "4.1  Phase 1: Alignment of Linear MLP for Correct Classification . . 7   \n4.2  Phase 2: Evolution of Attention and MLP for Large Classification Margin 8 ", "page_idx": 13}, {"type": "text", "text": "5   Discussion and Future Directions 10 ", "page_idx": 13}, {"type": "text", "text": "A Setup of Synthetic Experiment 16 ", "page_idx": 13}, {"type": "text", "text": "B  Gradient Flow Update for Weight Matrices 16 ", "page_idx": 13}, {"type": "text", "text": "C Gradient Flow Dynamical System 16 ", "page_idx": 13}, {"type": "text", "text": "C.1 Derivation of the Dynamical System 17 ", "page_idx": 13}, {"type": "text", "text": "D  Initialization 21 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "E Training Dynamics: Phase 1 23 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "E.1 Initial Gradients . 24   \nE.2 Maximum Perturbation of Neuron Outputs . 27   \nE.3 Perturbation Term Involving Correlation of Value-transformed Data 27   \nE.4 Perturbation Term Involving Correlation of Neurons . . 29   \nE.5 Neuron Weights Align with Signal Value . . . 30   \nE.6 Alignment of Common Token 32   \nE.7 Small Score Movement in Phase 1 37   \nE.8All Variables are within Range in Definition of Phase 1 43 ", "page_idx": 13}, {"type": "text", "text": "F Training Dynamics: Phase 2 44 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "F.1 Automatic Balancing of Gradients 48   \nF.2 How Fast the Loss Decreases . 54   \nF.3 Growth of Neuron Correlation 55   \nF.4 Growth of Correlation of Value-Transformed Data . . 56   \nF.5 Change of Random-Token Sub-Network . . 57   \nF.6 Change of Score and Softmax Probability . . 59   \nF.7 Change of Self-Correlation of Key/Query-Transformed data 61   \nF.8  Small Loss is Achieved 62   \nF.9 Proof of Theorem 3.2 . 63   \nF.10 Proof of Theorem 3.3 63 ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "G  Auxiliary Results 63 ", "page_idx": 14}, {"type": "text", "text": "H Probability 64 ", "page_idx": 14}, {"type": "text", "text": "A  Setup of Synthetic Experiment ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We conduct synthetic experiment to verify our theoretical results. We create a dataset following our data distribution in Definition 2.1 with 60 training samples: 30 samples have both $\\mu_{1}$ and $\\mu_{2}$ in it, 10 samples have only $\\mu_{1}$ ,10 samples have only $\\mu_{2}$ , and 10 samples have neither $\\mu_{1}$ nor $\\mu_{2}$ . Each data consists of 5 patches and each patch has dimension 64. The embedding dimension $m$ is set to be 128 and the number of neurons is set to be 256. We use Kaiming initialization to initialize the transformer weights. The transformer is trained by gradient descent with learning rate 0.01 for 30000 epochs. ", "page_idx": 15}, {"type": "text", "text": "B  Gradient Flow Update for Weight Matrices ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We provide the gradient flow update for each weight matrix as follows. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{\\partial n_{f}^{(1)}}{\\partial t}}&{=\\frac{1}{n_{f}}\\displaystyle\\sum_{i=1}^{n}\\hat{\\sigma}_{i}^{(1)}\\sum_{s=1}^{\\nu}\\sigma_{i}^{(1)}\\nu_{i}^{(2)}\\mu_{s}^{(2)}=\\frac{1}{n_{f}}\\displaystyle\\sum_{s=1}^{n}\\hat{\\sigma}_{i}^{(1)}\\nu_{f,s}^{(1)}\\sum_{s=1}^{\\nu}\\hat{\\sigma}_{i}^{(2)}\\nu_{f,s}^{(1)}\\mu_{s}^{(2)}}\\\\ &{~~\\frac{\\partial n_{f}^{(1)}}{\\partial t}=\\frac{1}{n_{f}}\\sum_{i=1}^{n}\\hat{\\sigma}_{i}^{(1)}\\sum_{s=1}^{\\nu}\\sum_{s=1}^{\\nu}\\sigma_{i}^{(2)}\\nu_{f,s}^{(1)}\\left(X^{((1)}\\right)^{\\prime}\\right)^{\\top}}\\\\ &{~~\\frac{\\partial n_{f}^{(1)}}{\\partial t}=\\frac{1}{n_{f}}\\sum_{i=1}^{n}\\hat{\\sigma}_{i}^{(1)}\\nu_{i}^{(2)}\\displaystyle\\sum_{s=1}^{\\nu}\\hat{\\sigma}_{i}^{(1)}\\nu_{f,s}^{(1)}\\left(X^{(1)}\\right)^{\\prime}}\\\\ &{~~\\frac{\\partial n_{f}^{(1)}}{\\partial t}=\\frac{1}{n_{f}}\\sum_{i=1}^{n}\\hat{\\sigma}_{i}^{(1)}\\nu_{i}^{(2)}\\displaystyle\\sum_{s=1}^{n}\\hat{\\sigma}_{i}^{(2)}\\nu_{f,s}^{(1)}\\mu_{s}^{(2)}\\displaystyle\\sum_{s=1}^{n}\\hat{\\sigma}_{i}^{(2)}\\nu_{f,s}^{(1)}\\mu_{s}^{(2)}}\\\\ &{~~-\\frac{1}{n_{f}}\\displaystyle\\sum_{s=1}^{n}\\hat{\\sigma}_{i}^{(1)}\\nu_{f,s}^{(1)}\\displaystyle\\sum_{s=1}^{n}\\sum_{s=1}^{n}\\hat{\\sigma}_{i}^{(1)}\\nu_{f,s}^{(1)}\\mu_{s}^{(2)}\\displaystyle\\sum_{s=1}\n$$$$\n\\begin{array}{r l}&{\\phantom{=}\\frac{1}{n\\sqrt{m_{c}}}\\sum_{i=1}^{n}\\mu_{c\\_i}\\sum_{\\nu=0}^{\\nu\\mu_{c}}\\alpha_{i\\mu_{c}}^{[i,0]}\\frac{1}{n\\sqrt{n^{\\mu}}}\\\\\\\\\\\\hdots\\frac{n(\\mu_{c})^{i}}{n(n\\sqrt{n^{\\mu}})^{\\alpha}}\\frac{\\mu_{c\\_i}(\\lambda_{\\mu}^{i}(\\lambda_{\\nu}^{i}))^{\\alpha}}{\\mu_{c\\_i}^{[i,1]}}+\\mathrm{c.c.}\\frac{1}{n^{\\alpha}}\\alpha_{i\\mu_{c}}^{[i,1]}\\frac{1}{n\\sqrt{n^{\\mu}}}\\mathrm{L.c.}}\\\\ &{\\phantom{=}\\frac{1}{n\\sqrt{m_{c}}}\\sum_{i=1}^{n}\\mu_{c\\_i}^{[i,1]}\\frac{1}{n\\sqrt{n^{\\mu}}}\\sum_{i=1}^{n}\\mu_{c\\_i}^{[i,2]}\\left(-\\nu_{c}^{[i]^{\\prime}}\\gamma^{[i,1]}\\mu_{c\\_{\\mu}}^{[i,2]}\\mu_{c\\_i}^{[i]}\\right)^{\\alpha}+\\mathrm{c.c.}\\frac{n^{[i]^{\\prime}}}{n^{\\alpha}}\\gamma^{[i,1]}\\delta\\alpha\\mathrm{ag}_{\\mu}^{[i]}\\frac{1}{n^{\\mu}}\\right)X^{[i]^{\\prime}}}\\\\ &{\\phantom{=}-\\frac{1}{n\\sqrt{m_{c}}}\\sum_{i=1}^{n}\\mu_{c\\_i}^{[i]}\\frac{1}{n\\sqrt{n^{\\mu}}}\\sum_{i=1}^{n}\\alpha_{i\\mu_{c}}^{[i,1]}\\mu_{c\\_i}^{[i]}\\left(\\alpha_{i\\mu_{c}}^{[i,2]}\\gamma^{[i,1]}\\left(\\alpha_{i\\mu_{c}}^{[i,2]}\\gamma^{[i,2]}\\mu_{c\\_i}^{[i,3]}\\right)X^{[i]^{\\prime}}\\right.}\\\\ &{\\left.\\phantom{=}\\frac{1}{n\\sqrt{m_{c}}}\\sum_{i=1}^{n}\\mu_{c\\_i}^{[i]}\\frac{1}{n\\sqrt{n^{\\mu}}}\\sum_{i=1}^{n}\\alpha_{i\\mu_{c}}^{[i,1]}\\frac{1}{n^{\\mu} \n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "C  Gradient Flow Dynamical System ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We first provide our complete dynamical system. The derivation of each equation is provided in Appendix C.1. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\partial w_{j_{1}}^{(t)\\top}W_{V}^{(t)}\\mu}{\\partial t}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n=\\frac{1}{n}\\sum_{i_{2}=1}^{n}g_{i_{2}}^{(t)}y_{i_{2}}\\sum_{l_{2}=1}^{L}\\sum_{j_{2}=1}^{m_{1}}a_{j_{2}}\\left\\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\\right\\rangle\\left(X^{(i_{2})}p_{l_{2}}^{(t,i_{2})}\\right)^{\\top}\\mu+\\frac{1}{n}\\sum_{i_{1}=1}^{n}g_{i_{1}}^{(t)}y_{i_{1}}\\sum_{l_{1}=1}^{L}a_{j_{1}}p_{l_{1}}^{(t,i_{1})\\top}V^{(t,i_{1})}^{\\top}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\displaystyle\\gamma\\nu^{\\top}W_{K}^{(t)\\top}W_{Q}^{(t)}\\mu}{\\displaystyle\\partial t}}\\\\ &{=\\frac{1}{n\\sqrt{m}}\\displaystyle\\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\\displaystyle\\sum_{l=1}^{L}\\sum_{j=1}^{m_{1}}a_{j}\\nu^{\\top}W_{K}^{(t)\\top}K^{(t,i)}\\cdot\\mathrm{diag}\\left(V^{(t,i)\\top}w_{j}^{(t)}-w_{j}^{(t)\\top}V^{(t,i)}p_{l}^{(t,i)}\\right)p_{l}^{(t,i)}x_{l}^{(i)\\top}\\mu}\\\\ &{+\\displaystyle\\frac{1}{n\\sqrt{m}}\\displaystyle\\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\\displaystyle\\sum_{l=1}^{L}\\sum_{j=1}^{m_{1}}a_{j}\\mu^{\\top}W_{Q}^{(t)\\top}q_{l}^{(t,i)}p_{l}^{(t,i)\\top}\\cdot\\mathrm{diag}\\left(w_{j}^{(t)\\top}V^{(t,i)}-w_{j}^{(t)\\top}V^{(t,i)}p_{l}^{(t,i)}\\right)X_{\\;\\;\\;\\nu}^{(i)\\top}\\nu_{i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-\\frac{\\Delta t}{n\\sqrt{n}}}\\\\ &{=\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}g_{i\\rightarrow0}^{i\\uparrow}\\displaystyle\\sum_{i=1}^{k}g_{i\\downarrow}^{i\\uparrow}W_{i\\uparrow}^{i\\uparrow}W_{i\\uparrow}^{i\\uparrow}W_{i\\downarrow,i\\downarrow\\downarrow\\uparrow}^{i\\uparrow}+\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}g_{i\\uparrow}^{i\\uparrow}W_{i\\downarrow}^{i\\downarrow}\\displaystyle\\sum_{i\\neq0}^{k}\\displaystyle\\sum_{j\\downarrow=1}^{n}g_{i\\uparrow}^{i\\uparrow}W_{i\\downarrow}^{i\\uparrow}W_{i\\uparrow}^{i\\uparrow}W_{i\\uparrow}^{i\\uparrow}}\\\\ &{\\quad-\\displaystyle\\frac{\\Delta t}{n\\sqrt{n}}\\displaystyle\\sum_{i\\neq0}^{n}g_{i\\downarrow}^{i\\downarrow}W_{i\\downarrow}^{i\\downarrow}}\\\\ &{=\\displaystyle\\frac{1}{n\\sqrt{n}}\\displaystyle\\sum_{i\\neq0}^{n}g_{i\\uparrow}^{i\\downarrow}W_{i\\downarrow}^{i\\downarrow}\\displaystyle\\sum_{j\\downarrow\\downarrow}^{n}g_{i\\downarrow}^{i\\uparrow}W_{i\\downarrow}^{i\\uparrow}W_{i\\downarrow}^{i\\downarrow}\\displaystyle\\prod_{i\\downarrow}^{n}\\displaystyle\\sum_{i\\downarrow\\downarrow}^{n}\\displaystyle\\sum_{i\\downarrow\\uparrow}^{n}\\displaystyle\\sum_{j\\downarrow}^{n}\\displaystyle\\sum_{i\\downarrow\\downarrow}^{n}\\displaystyle\\sum_{i\\downarrow\\downarrow}^{n}g_{i\\downarrow}^{i\\uparrow}W_{i\\downarrow}^{i\\downarrow}}\\\\ &{\\quad+\\displaystyle\\frac{1}{n\\sqrt{n}}\\displaystyle\\sum_{i\\neq0}^{n}g_{i\\uparrow}^{i\\downarrow}W_{i\\uparrow}^{i\\uparrow}W_{i\\downarrow}^{i\\uparrow}W_{i\\uparrow}^{i\\uparrow}W_{i\\downarrow}^{i\\downarrow}\\displaystyle\\cdot\\Delta\\mathbf{a}\\mu_{\\mathrm{g}}\\left(V^{i\\downarrow,\\uparrow}W_{i\\uparrow}^{i\\downarrow}\\displaystyle\\alpha_{i\\uparrow}^{-}-W_{j\\downarrow}^{i\\uparrow}V^{i\\downarrow}\\displaystyle\\alpha_{j\\downarrow}^{0}\\right)p_{\\mathrm{tot}}^{i\\downarrow}\\displaystyle\\sum_{i\\downarrow\\downarrow,i\\uparrow}^{n}}\\\\ &{\\quad\\displaystyle\\frac{\\Delta t^{2}\\Delta t^{2}\\Delta t^{2}}{n\\sqrt{n}}\\displaystyle\\sum_{i\\uparrow}^{n}g_{i\\downarrow}^{i\\downarrow}W_{i\\downarrow}^{i\\downarrow}W_{i\\uparrow}^{i\\uparrow}W_{i\\downarrow}^{i\\uparrow}W_{i\\downarrow}^{i\\downarrow}\\\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "C.1 Derivation of the Dynamical System ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lemma C.1. Let $\\mu\\in\\{\\mu_{i}\\}_{i=1}^{d}$ . For all $j\\in[m]$ we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial w_{j_{1}}^{(t)\\top}W_{V}^{(t)}\\mu}{\\partial t}=\\frac{1}{n}\\displaystyle\\sum_{i_{2}=1}^{n}g_{i_{2}}^{(t)}y_{i_{2}}\\displaystyle\\sum_{l_{2}=1}^{L}\\sum_{j_{2}=1}^{m_{1}}a_{j_{2}}\\left<w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\\right>\\left(X^{(i_{2})}p_{l_{2}}^{(t,i_{2})}\\right)^{\\top}\\mu}\\\\ &{\\phantom{\\frac{(t)^{2}}{\\partial t}}\\qquad\\qquad+\\frac{1}{n}\\displaystyle\\sum_{i_{1}=1}^{n}g_{i_{1}}^{(t)}y_{i_{1}}\\displaystyle\\sum_{l_{1}=1}^{L}a_{j_{1}}p_{l_{1}}^{(t,i_{1})\\top}V^{(t,i_{1})\\top}W_{V}^{(t)}\\mu}\\\\ &{=\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i_{1}\\neq\\in X^{(i)}}g_{i}^{(t)}y_{i}\\displaystyle\\sum_{l=1}^{L}a_{j}\\left(\\lVert w_{j}^{(t)}\\rVert_{2}^{2}+\\lVert v^{(t)}(\\mu)\\rVert_{2}^{2}\\right)p_{q\\leftarrow l,k\\leftarrow\\mu}^{(t,i)}+\\varepsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\varepsilon=\\frac{1}{n}\\sum_{i:\\mu\\in X^{(i)}}g_{i}^{(t)}y_{i}\\sum_{l=1}^{L}\\sum_{j_{2}\\neq j_{1}}a_{j_{2}}\\left<w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\\right>p_{q\\leftarrow l,k\\leftarrow\\mu}^{(t,i)}}\\\\ {\\displaystyle\\ \\ +\\,\\frac{1}{n}\\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\\sum_{l_{1}=1}^{L}a_{j_{1}}\\sum_{l_{2}=1}^{L}\\left<p_{l_{1},l_{2}}^{(t,i)}v_{l_{2}}^{(t,i)},v^{(t)}(\\mu)\\right>\\mathbb{I}(v_{l_{2}}^{(t,i)}\\neq v^{(t)}(\\mu)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. Let $l(i,\\mu)$ denote the index such that $X_{l(i,\\mu)}^{(i)}=\\mu$ .By the gradient fow update, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\frac{1}{n}\\displaystyle\\sum_{u,v\\neq\\infty}\\sum_{u^{\\prime},u^{\\prime},v\\neq\\nu_{x}}^{}\\beta_{u,v}\\sum_{u^{\\prime},u^{\\prime},v^{\\prime},u^{\\prime}}^{(1)}\\beta_{u^{\\prime},u^{\\prime},u^{\\prime}}^{(1)}\\mu_{v^{\\prime},u^{\\prime},u^{\\prime}}^{(1)}}\\\\ &{~~~+\\frac{1}{n}\\displaystyle\\sum_{u^{\\prime},u^{\\prime},v\\neq\\nu_{x}}^{}\\sum_{u^{\\prime},u^{\\prime},v^{\\prime},u^{\\prime}}^{}\\sum_{u^{\\prime},u^{\\prime},v^{\\prime}=1}^{n}\\beta_{u^{\\prime},u^{\\prime},u^{\\prime},u^{\\prime}}^{(1)}\\mu_{v^{\\prime},u^{\\prime},u^{\\prime}}^{(1)}}\\\\ &{~~~+\\frac{1}{n}\\displaystyle\\sum_{u^{\\prime},u^{\\prime},v^{\\prime}=1}^{n}\\sum_{u^{\\prime},u^{\\prime},v^{\\prime}=1}^{n}\\sum_{u^{\\prime},u^{\\prime},v^{\\prime}=1}^{n}\\sum_{u^{\\prime},u^{\\prime},v^{\\prime}=1}^{n}\\sum_{u^{\\prime},u^{\\prime},v^{\\prime}=1}^{n}\\beta_{u^{\\prime},u^{\\prime},u^{\\prime},u^{\\prime}}^{(1)}\\mu_{v^{\\prime},u^{\\prime},u^{\\prime}}^{(1)}}\\\\ &{~~~+\\frac{1}{n}\\displaystyle\\sum_{u^{\\prime},u^{\\prime},v^{\\prime}=1}^{n}\\sum_{u^{\\prime},u^{\\prime},v^{\\prime}=1}^{n}\\sum_{u^{\\prime},u^{\\prime},v^{\\prime}=1}^{n}\\beta_{u^{\\prime},u^{\\prime},u^{\\prime},u^{\\prime},u^{\\prime}}^{(1)}\\mu_{v^{\\prime},u^{\\prime},u^{\\prime}}^{(1)}}\\\\ &{~~~+\\frac{1}{n}\\displaystyle\\sum_{u^{\\prime},u^{\\prime},v^{\\prime}=1}^{n}\\sum_{u^{\\prime},u^{\\prime},v^{\\prime}=1}^{n}\\sum_{u^{\\prime},u\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma C.2. The following equation on the gradient fow holds: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{\\partial\\left\\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\\right\\rangle}{\\partial t}=\\frac{1}{n}\\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\\sum_{l=1}^{L}\\left(\\sum_{\\nu:X_{\\nu^{\\prime}}^{(i)}\\in\\mathcal{U}}a_{j_{1}}w_{j_{1}}^{(t)\\top}V_{l^{\\prime}}^{(t,i)}p_{l,\\nu^{\\prime}}^{(t,i)}+a_{j_{1}}w_{j_{2}}^{(t)\\top}V_{l^{\\prime}}^{(t,i)}p_{l,\\nu^{\\prime}}^{(t,i)}\\right)}}\\\\ &{}&{+\\,\\frac{1}{n}\\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\\sum_{l=1}^{L}\\left(\\sum_{\\nu:X_{\\nu^{\\prime}}^{(i)}\\notin\\mathcal{U}}a_{j_{2}}w_{j_{1}}^{(t)\\top}V_{l^{\\prime}}^{(t,i)}p_{l,\\nu^{\\prime}}^{(t,i)}+a_{j_{1}}w_{j_{2}}^{(t)\\top}V_{l^{\\prime}}^{(t,i)}p_{l,\\nu^{\\prime}}^{(t,i)}\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. By gradient flow update, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial\\left<w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\\right>}{\\partial t}=\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\\displaystyle\\sum_{l=1}^{L}a_{j_{2}}w_{j_{1}}^{(t)\\top}V^{(t,i)}p_{l}^{(t,i)}+\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\\displaystyle\\sum_{l=1}^{L}a_{j_{1}}w_{j_{2}}^{(t)\\top}V^{(t,i)}p_{l}^{(t,i)}}\\\\ &{\\phantom{\\frac{1}{\\partial t}\\displaystyle\\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\\displaystyle\\sum_{l=1}^{L}\\left(\\sum_{l:\\mathcal{X}_{i^{\\prime}}^{(t)}\\in\\mathcal{U}}a_{j_{2}}w_{j_{1}}^{(t)\\top}V_{l^{\\prime}}^{(t,i)}p_{l,l^{\\prime}}^{(t,i)}+a_{j_{1}}w_{j_{2}}^{(t)\\top}V_{l^{\\prime}}^{(t,i)}p_{l,l^{\\prime}}^{(t,i)}\\right)}\\\\ &{\\phantom{\\frac{1}{\\partial t}\\displaystyle\\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\\displaystyle\\sum_{l=1}^{L}\\left(\\sum_{l:\\mathcal{X}_{i^{\\prime}}^{(t)}\\in\\mathcal{U}}a_{j_{2}}w_{j_{1}}^{(t)\\top}V_{l^{\\prime}}^{(t,i)}p_{l,l^{\\prime}}^{(t,i)}+a_{j_{1}}w_{j_{2}}^{(t)\\top}V_{l^{\\prime}}^{(t,i)}p_{l,l^{\\prime}}^{(t,i)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Lemma C.3. Let $\\mu,\\nu\\in\\{\\mu_{i}\\}_{i=1}^{d}$ . Then the following equation on gradient flow holds: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\partial\\nu W_{V}^{(t)\\top}W_{V}^{(t)}\\mu}{\\partial t}=\\frac{1}{n}\\displaystyle\\sum_{i:\\mu\\in X^{(i)}}g_{i}^{(t)}y_{i}\\displaystyle\\sum_{l=1}^{L}\\sum_{j=1}^{m_{1}}a_{j}\\nu^{\\top}W_{V}^{(t)\\top}w_{j}^{(t)}p_{q\\leftarrow l,k\\leftarrow\\mu}^{(t,i)}}\\\\ {+\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i:\\nu\\in X^{(i)}}g_{i}^{(t)}y_{i}\\displaystyle\\sum_{l=1}^{L}\\sum_{j=1}^{m_{1}}a_{j}\\mu^{\\top}W_{V}^{(t)\\top}w_{j}^{(t)}p_{q\\leftarrow l,k\\leftarrow\\nu}^{(t,i)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. The gradient flow update can be derived as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{\\partial\\nu W_{V}^{(t)\\top}W_{V}^{(t)}\\mu}{\\partial t}=\\frac{1}{n}\\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\\displaystyle\\sum_{l=1}^{L}\\sum_{j=1}^{m_{1}}a_{j}\\nu^{\\top}W_{V}^{(t)\\top}w_{j}^{(t)}\\left(X^{(i)}p_{l}^{(t,i)}\\right)^{\\top}\\mu}\\\\ &{\\phantom{\\leq}\\displaystyle+\\frac{1}{n}\\sum_{i=1}^{n}g_{i}^{(i)}y_{i}\\displaystyle\\sum_{l=1}^{L}\\sum_{j=1}^{m_{1}}a_{j}\\mu^{\\top}W_{V}^{(t)\\top}w_{j}^{(t)}\\left(X^{(i)}p_{l}^{(t,i)}\\right)^{\\top}\\nu}\\\\ &{\\phantom{\\leq\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}}g_{i}^{(t)}y_{i}\\displaystyle\\sum_{l=1}^{L}\\sum_{j=1}^{m_{1}}a_{j}\\nu^{\\top}W_{V}^{(t)\\top}w_{j}^{(t)}p_{q\\leftarrow l,k\\leftarrow\\mu}^{(t)}}\\\\ &{\\phantom{\\leq\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{i\\neq k}S_{i}^{(t)}\\mu_{i}^{(t)}\\displaystyle\\sum_{l=1}^{L}\\sum_{j=1}^{m_{1}}a_{j}\\mu^{\\top}W_{V}^{(t)\\top}w_{j}^{(t)}p_{q\\leftarrow l,k\\leftarrow\\mu}^{(t,i)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Lemma C.4. Let $\\mu,\\nu\\in\\{\\mu_{i}\\}_{i=1}^{d}$ . Then the following equations on gradient flow hold. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial\\nu^{\\top}W_{Q}^{(t)^{\\top}}W_{Q}^{(t)}\\mu}{\\partial t}}\\\\ &{=\\cfrac{1}{n\\sqrt{m}}\\underset{i\\neq\\ell}{\\sum}\\underset{j=1}^{m}g_{i}^{(t)}y_{j}\\underset{j=1}{\\overset{m_{1}}{\\sum}}a_{j}\\nu^{\\top}W_{Q}^{(t)^{\\top}}K^{(t,i)}\\mathrm{diag}\\left(V^{(t,i)^{\\top}}w_{j}^{(t)}-w_{j}^{(t)^{\\top}}V^{(t,i)}p_{l(i,\\mu)}^{(t,i)}\\right)p_{l(i,\\mu)}^{(t,i)}}\\\\ &{\\quad+\\cfrac{1}{n\\sqrt{m}}\\underset{i\\neq\\ell}{\\sum}\\underset{k\\neq\\ell}{\\sum}g_{i}^{(t)}y_{k}\\underset{j=1}{\\overset{m_{1}}{\\sum}}a_{j}\\mu^{\\top}W_{Q}^{(t)^{\\top}}K^{(t,i)}\\mathrm{diag}\\left(V^{(t,i)^{\\top}}w_{j}^{(t)}-w_{j}^{(t)^{\\top}}V^{(t,i)}p_{l(i,\\nu)}^{(t,i)}\\right)p_{l(i,\\nu)}^{(t,i)},}\\\\ &{\\frac{\\partial\\nu^{\\top}W_{K}^{(t)^{\\top}}W_{K}^{(t)}\\mu}{\\partial t}}\\\\ &{=\\cfrac{1}{n\\sqrt{m}}\\underset{i\\neq\\ell}{\\sum}\\underset{j=1}{\\overset{m_{1}}{\\sum}}\\underset{j=1}{\\overset{m_{1}}{\\sum}}\\underset{j=1}{\\sum}\\underset{j=1}{\\sum}\\,q_{j}\\nu^{\\top}W_{K}^{(t)^{\\top}}q_{l}^{(t,i)}p_{q\\ell}^{(t,i)}\\underset{k\\neq\\ell}{\\sum}\\underset{j=1}{\\overset{m_{1}}{\\sum}}\\underset{k\\neq\\ell}{\\sum}\\left(w_{j}^{(t)^{\\top}}v^{(t,i)}(\\mu)-w_{j}^{(t)^{\\top}}V^{(t,i)}p_{l}^{(t,i)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+\\frac{1}{n\\sqrt{m}}\\displaystyle\\sum_{i\\neq\\partial\\nu\\leq K(0)}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. To prove the first result, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\boldsymbol{\\mu}_{p}^{\\mathrm{\\scriptscriptstyleT}}\\boldsymbol{W}_{Q}^{(t)\\top}\\boldsymbol{W}_{Q}^{(t)}\\mu}{\\boldsymbol{\\mu}_{q}}}\\\\ &{=\\frac{1}{n\\sqrt{m}}\\sum_{i=1}^{n}g_{i}^{(t)}\\boldsymbol{y}_{i}\\sum_{l=1}^{L\\mathrm{\\tiny~m}_{1}}a_{j}\\boldsymbol{\\nu}^{\\top}\\boldsymbol{W}_{Q}^{(t)\\top}\\boldsymbol{K}^{(t,i)}\\mathrm{diag}\\left(\\boldsymbol{V}^{(t,i)\\top}\\boldsymbol{w}_{j}^{(t)}-\\boldsymbol{w}_{j}^{(t)\\top}\\boldsymbol{V}^{(t,i)}\\boldsymbol{p}_{l}^{(t,i)}\\right)p_{l}^{(t,i)}\\boldsymbol{x}_{l}^{(i)\\top}\\boldsymbol{\\mu}}\\\\ &{\\quad+\\frac{1}{n\\sqrt{m}}\\sum_{i=1}^{n}g_{i}^{(t)}\\boldsymbol{y}_{i}\\sum_{l=1}^{L\\mathrm{\\tiny~m}_{1}}a_{j}\\boldsymbol{\\mu}^{\\top}\\boldsymbol{W}_{Q}^{(t)\\top}\\boldsymbol{K}^{(t,i)}\\mathrm{diag}\\left(\\boldsymbol{V}^{(t,i)\\top}\\boldsymbol{w}_{j}^{(t)}-\\boldsymbol{w}_{j}^{(t)\\top}\\boldsymbol{V}^{(t,i)}p_{l}^{(t,i)}\\right)p_{l}^{(t,i)}\\boldsymbol{x}_{l}^{(i)\\top}\\boldsymbol{\\nu}_{l}}\\\\ &{=\\frac{1}{n\\sqrt{m}}\\sum_{i\\neq j\\in\\mathcal{K}^{(t)}}g_{i}^{(t)}\\boldsymbol{y}_{i}\\sum_{j=1}^{m_{1}}a_{j}\\boldsymbol{\\mu}^{\\top}\\boldsymbol{W}_{Q}^{(t)\\top}\\boldsymbol{K}^{(t,i)}\\mathrm{diag}\\left(\\boldsymbol{V}^{(t,i)\\top}\\boldsymbol{w}_{j}^{(t)}-\\boldsymbol{w}_{j}^{(t)\\top}\\boldsymbol{V}^{(t,i)}p_{l(i,n)}^{(t,i)}\\right)p_{l(i,n)}^{(t,i)}}\\\\ &{\\quad+\\frac{1}{n\\sqrt{m}}\\sum_{i\\neq j\\in\\mathcal{K}^{(t)}}g_{i}^{(t)}\\boldsymbol{y}_{j}\\sum_{l=1}^{m_{1}}a_{j}\\boldsymbol{\\mu}^{\\top}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "To prove the second result, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\displaystyle{h_{j}^{\\top}W_{W}^{\\top}W_{W}^{\\top}\\mu_{j}}}{\\displaystyle{\\delta t}}}\\\\ &{=\\frac{1}{\\displaystyle n\\sqrt{m}}\\sum_{i=1}^{n}g_{i}^{(i)}y_{j_{k}}\\sum_{l=1}^{L}a_{j}b^{\\top}W_{K}^{(i)\\top}q_{l}^{(i,i)}p_{l}^{(t,i)\\top}\\mathrm{diag}\\left(w_{j}^{(t)\\top}V^{(t,i)}-w_{j}^{(i)\\top}V^{(t,i)}p_{l}^{(t,i)}\\right)X^{(i)\\top}\\mu}\\\\ &{\\phantom{\\sum_{\\mu}\\sum_{\\tau\\in\\mathcal{T}}}+\\frac{1}{\\displaystyle n\\sqrt{m}}\\sum_{i=1}^{n}g_{i}^{(i)}y_{\\mu}\\sum_{l=1}^{L}\\sum_{j=1}^{m_{1}}a_{j}\\mu^{\\top}W_{K}^{(t)\\top}q_{l}^{(t,i)}p_{l}^{(t,i)\\top}\\mathrm{diag}\\left(w_{j}^{(t)\\top}V^{(t,i)}-w_{j}^{(t)\\top}V^{(t,i)}p_{l}^{(t,i)}\\right)X^{(i)\\top}\\nu}\\\\ &{=\\frac{1}{\\displaystyle n\\sqrt{m}}\\sum_{i\\neq j\\in\\mathcal{S}(4)}g_{i}^{(i)}y_{j_{l}}\\sum_{l=1}^{L}\\sum_{j=1}^{m_{1}}a_{j}\\nu^{\\top}W_{K}^{(t)\\top}q_{i}^{(t,i)}p_{\\varphi\\leftarrow,l,k\\leftarrow\\mu}\\left(w_{j}^{(t)\\top}v^{(t,i)}(\\mu)-w_{j}^{(t)\\top}V^{(t,i)}p_{l}^{(t,i)}\\right)}\\\\ &{\\phantom{\\sum_{\\mu}\\sum_{\\tau\\in\\mathcal{T}}}+\\frac{1}{\\displaystyle n\\sqrt{m}}\\sum_{i=1}^{n}g_{i}^{(i)}y_{j_{l}}\\sum_{l=1}^{L}\\sum_{j=1}^{m_{1}}a_{j}\\mu^{\\top}W_{K}^{(t)\\top}q_{l}^{(t,i)}p_{\\varphi\\leftarrow,l,k\\leftarrow\\mu}^{(t,i)}\\left(w_{j}^{(t)\\top}v^{(t,i)}(\\nu)-w_{j}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "To prove the third result, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\partial\\nu^{\\top}W_{K}^{(t)\\top}W_{Q}^{(t)}\\mu}{\\partial t}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "$\\begin{array}{r l}&{=\\displaystyle\\frac{1}{n\\sqrt{m}}\\displaystyle\\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\\sum_{l=1}^{L}\\sum_{j=1}^{m_{1}}a_{j}\\nu^{\\top}W_{K}^{(t)\\top}K^{(t,i)}\\mathrm{diag}\\left(V^{(t,i)\\top}w_{j}^{(t)}-w_{j}^{(t)\\top}V^{(t,i)}p_{l}^{(t,i)}\\right)p_{l}^{(t,i)}x_{l}^{(i)\\top}\\mu}\\\\ &{\\quad+\\displaystyle\\frac{1}{n\\sqrt{m}}\\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\\displaystyle\\sum_{l=1}^{L}\\sum_{j=1}^{m_{1}}a_{j}\\mu^{\\top}W_{Q}^{(t)\\top}q_{l}^{(t,i)}p_{l}^{(t,i)\\top}\\mathrm{diag}\\left(w_{j}^{(t)\\top}V^{(t,i)}-w_{j}^{(t)\\top}V^{(t,i)}p_{l}^{(t,i)}\\right)X^{(i)\\top}\\nu}\\\\ &{=\\displaystyle\\frac{1}{n\\sqrt{m}}\\sum_{i:\\mu\\in X^{(i)}}g_{i}^{(t)}y_{i}\\displaystyle\\sum_{j=1}^{m_{1}}a_{j}\\nu^{\\top}W_{K}^{(t)\\top}K^{(t,i)}\\mathrm{diag}\\left(V^{(t,i)\\top}w_{j}^{(t)}-w_{j}^{(t)\\top}V^{(t,i)}p_{l(i,\\mu)}^{(t,i)}\\right)p_{l(i,\\mu)}^{(t,i)}}\\end{array}$ $+\\;\\frac{1}{n\\sqrt{m}}\\sum_{i:\\nu\\in X^{(i)}}g_{i}^{(t)}y_{i}\\sum_{l=1}^{L}\\sum_{j=1}^{m_{1}}a_{j}\\mu^{\\top}W_{Q}^{(t)\\top}q_{l}^{(t,i)}p_{q\\leftarrow l,k\\leftarrow\\nu}^{(t,i)}\\left(w_{j}^{(t)\\top}v^{(t,i)}(\\nu)-w_{j}^{(t)\\top}V^{(t,i)}p_{l}^{(t,i)}\\right)$ $=\\frac{1}{n\\sqrt{m}}\\sum_{\\substack{i:\\mu,\\nu\\in X^{(i)}}}g_{i}^{(t)}y_{i}\\sum_{j=1}^{m_{1}}a_{j}\\Vert k^{(t)}(\\nu)\\Vert_{2}^{2}\\left(v^{(t)\\top}(\\nu)w_{j}^{(t)}-w_{j}^{(t)\\top}V^{(t,i)}p_{l(i,\\mu)}^{(t,i)}\\right)p_{q\\leftarrow\\mu,k\\leftarrow\\nu}^{(t,i)}$ $-\\,\\frac{1}{n\\sqrt{m}}\\sum_{i;\\mu\\in X^{(i)}}g_{i}^{(t)}y_{i}\\sum_{j=1}^{m_{1}}a_{j}\\sum_{l=1}^{L}\\nu^{\\top}W_{K}^{(t)\\top}K_{l}^{(t,i)}\\left(V_{l}^{(t,i)\\top}w_{j}^{(t)}-w_{j}^{(t)\\top}V^{(t,i)}p_{l(i,\\mu)}^{(t,i)}\\right)p_{q\\leftarrow\\mu,k\\leftarrow l}^{(t,i)}\\mathbb{I}(K_{l}^{(t,i)\\top}w_{i}^{(t)})~.$ p)PI(K(t\u2260k(() $+\\left.\\frac{1}{n\\sqrt{m}}\\sum_{\\substack{i:\\nu,\\mu\\in X^{(i)}}}g_{i}^{(t)}y_{i}\\sum_{j=1}^{m_{1}}a_{j}\\Vert q^{(t)}(\\mu)\\Vert_{2}^{2}p_{q\\leftarrow\\mu,k\\leftarrow\\nu}^{(t,i)}\\left(w_{j}^{(t)\\top}v^{(t,i)}(\\nu)-w_{j}^{(t)\\top}V^{(t,i)}p_{l(i,\\mu)}^{(t,i)}\\right)$ $\\cdot\\,\\frac{1}{n\\sqrt{m}}\\sum_{\\substack{i:\\nu\\in X^{(i)}}}g_{i}^{(t)}y_{i}\\sum_{l=1}^{L}\\sum_{j=1}^{m_{1}}a_{j}\\mu^{\\top}W_{Q}^{(t)\\top}q_{l}^{(t,i)}p_{q\\leftarrow l,k\\leftarrow\\nu}^{(t,i)}\\left(w_{j}^{(t)\\top}v^{(t,i)}(\\nu)-w_{j}^{(t)\\top}V^{(t,i)}p_{l}^{(t,i)}\\right)\\mathbb{I}(q_{l}^{(t,i)}(\\nu)-v_{m}^{(t,i)})\\},$ ", "page_idx": 20}, {"type": "text", "text": "D Initialization ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Lemma D.1. With probability at least $1-\\delta$ over the randomness of the initialization of $W_{K}$ and $W_{Q}$ forany $l_{1},l_{2}\\in[d]$ wehave ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left\\langle W_{K}^{(0)}\\mu_{l_{1}},W_{Q}^{(0)}\\mu_{l_{2}}\\right\\rangle\\right|\\leq\\sigma_{0}^{2}m\\left(\\sqrt{\\frac{4}{m}\\log\\frac{2d}{\\delta}}+\\frac{4}{m}\\log\\frac{2d}{\\delta}\\right),}\\\\ &{\\left|\\left\\langle W_{K}^{(0)}\\mu_{l_{1}},W_{K}^{(0)}\\mu_{l_{2}}\\right\\rangle\\right|\\leq\\sigma_{0}^{2}m\\left(\\sqrt{\\frac{4}{m}\\log\\frac{2d}{\\delta}}+\\frac{4}{m}\\log\\frac{2d}{\\delta}\\right),\\quad l_{1}\\neq l_{2}}\\\\ &{\\left|\\left\\langle W_{Q}^{(0)}\\mu_{l_{1}},W_{Q}^{(0)}\\mu_{l_{2}}\\right\\rangle\\right|\\leq\\sigma_{0}^{2}m\\left(\\sqrt{\\frac{4}{m}\\log\\frac{2d}{\\delta}}+\\frac{4}{m}\\log\\frac{2d}{\\delta}\\right),\\quad l_{1}\\neq l_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and for any $l\\in[d]$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|W_{K}^{(0)}\\mu_{l}\\|_{2}^{2}=\\sigma_{0}^{2}m\\left(1\\pm\\left(\\sqrt{\\frac{4}{m}\\log\\frac{2d}{\\delta}}+\\frac{4}{m}\\log\\frac{2d}{\\delta}\\right)\\right),}\\\\ &{\\|W_{Q}^{(0)}\\mu_{l}\\|_{2}^{2}=\\sigma_{0}^{2}m\\left(1\\pm\\left(\\sqrt{\\frac{4}{m}\\log\\frac{2d}{\\delta}}+\\frac{4}{m}\\log\\frac{2d}{\\delta}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Note that $W_{K}^{(0)}\\mu_{l_{1}},W_{Q}^{(0)}\\mu_{l_{2}}\\sim\\mathcal{N}(0,\\sigma_{0}^{2}I)$ The rest of proof appies LemmaH2. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Corollary D.2. For all $i\\in[n],\\,l,k\\in[L],$ we have ", "page_idx": 20}, {"type": "equation", "text": "$$\np_{l,k}^{(0,i)}=\\frac{1}{L}\\pm\\widetilde{O}\\left(\\frac{1}{L m}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Following from Lemma G.2 and from the first-order Taylor approximation on the softmax function from O, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\np_{l,k}^{(0,i)}=\\frac{1}{L}\\pm O\\left(\\frac{\\sigma_{0}^{2}m}{L\\sqrt{m}}\\left(\\sqrt{\\frac{4}{m}\\log\\frac{2d}{\\delta}}+\\frac{4}{m}\\log\\frac{2d}{\\delta}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The corollary then follows from Condition 1. ", "page_idx": 21}, {"type": "text", "text": "Lemma D.3. With probability at least $1-\\delta$ over the randomness of the initialization of $W$ and $W_{V}$ \uff0c thenfor $l_{1}\\neq l_{2}\\in[d]$ we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left|\\left\\langle W_{V}^{(0)}\\mu_{l_{1}},W_{V}^{(0)}\\mu_{l_{2}}\\right\\rangle\\right|\\leq\\sigma_{0}^{2}m\\left(\\sqrt{\\frac{4}{m}\\log\\frac{2d}{\\delta}}+\\frac{4}{m}\\log\\frac{2d}{\\delta}\\right),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for $j_{1}\\neq j_{2}\\in[m_{1}]$ we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left|\\left\\langle w_{j_{1}}^{(0)},w_{j_{2}}^{(0)}\\right\\rangle\\right|\\leq\\sigma_{1}^{2}m\\left(\\sqrt{\\frac{4}{m}\\log\\frac{2m_{1}^{2}}{\\delta}}+\\frac{4}{m}\\log\\frac{2m_{1}^{2}}{\\delta}\\right),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and for all $j\\in[m_{1}],\\;l\\in[d]$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left\\langle w_{j}^{(0)},W_{V}^{(0)}\\mu_{l}\\right\\rangle\\right|\\leq\\sigma_{0}\\sigma_{1}m\\left(\\sqrt{\\frac{4}{m}\\log\\frac{2m_{1}d}{\\delta}}+\\frac{4}{m}\\log\\frac{2m_{1}d}{\\delta}\\right)}\\\\ &{\\qquad\\qquad\\|w_{j}\\|_{2}^{2}=\\sigma_{1}^{2}m\\left(1\\pm\\left(\\sqrt{\\frac{4}{m}\\log\\frac{2m_{1}}{\\delta}}+\\frac{4}{m}\\log\\frac{2m_{1}}{\\delta}\\right)\\right)}\\\\ &{\\qquad\\qquad\\|W_{V}^{(0)}\\mu_{l}\\|_{2}^{2}=\\sigma_{0}^{2}m\\left(1\\pm\\left(\\sqrt{\\frac{4}{m}\\log\\frac{2d}{\\delta}}+\\frac{4}{m}\\log\\frac{2d}{\\delta}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. The proof is similar to that for Lemma D.1 and is omitted. ", "page_idx": 21}, {"type": "text", "text": "Lemma D.4. Conditioned on the success of the event in Corollary $D.2$ forall $i\\in[n],\\;l\\in[L],$ with probabilityat least $1-\\delta$ over the randomness in the initialization of $W_{V}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|W_{V}^{(0)}X^{(i)}p_{l}^{(0,i)}\\|_{2}^{2}=\\frac{\\sigma_{0}^{2}m}{L}\\left(1\\pm\\sqrt{\\frac{4}{m}\\log\\frac{2n L}{\\delta}}\\pm\\frac{4}{m}\\log\\frac{2n L}{\\delta}\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. First of all, by Corollary D.2 and Assumption 2.3, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Vert W_{V}^{(0)}X^{(i)}p_{l}^{(0,i)}\\Vert_{2}^{2}\\right]=\\mathbb{E}\\left[\\sum_{j=1}^{m}\\left(\\left\\langle\\left(W_{V}^{(0)}\\right)_{j},X^{(i)}p_{l}^{(0,i)}\\right\\rangle\\right)^{2}\\right]=\\sigma_{0}^{2}m\\Vert X^{(i)}p_{l}^{(0,i)}\\Vert_{2}^{2}=\\frac{\\sigma_{0}^{2}m}{L}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Finally, applying Bernstein's inequality and taking a union bound over $[n]$ and $[L]$ we finish the proof. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "Corollary D.5. Conditioned on the success of Lemma $D.4$ with probability at least $1-\\delta$ over the randomnessof $W$ for all $j\\in[m_{1}],\\;i\\in[n],\\;l\\in[L]$ wehave ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left|w_{j}^{(0)\\top}V^{(0,i)}p_{l}^{(0,i)}\\right|\\leq\\sigma_{1}\\sigma_{0}\\sqrt{\\frac{m}{L}\\log\\frac{m_{1}n L}{\\delta}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Conditioned on V(0,i)p(0,i), we have $w_{j}^{(0)\\top}V^{(0,i)}p_{l}^{(0,i)}\\sim\\mathcal{N}(0,\\sigma_{1}^{2}\\|V^{(0,i)}p_{l}^{(0,i)}\\|_{2}^{2})$ Thus, by Gaussian tail bound and a union bound over $\\bar{j}\\,\\in\\,[m_{1}],\\,\\,i\\,\\in\\,[n],\\,\\,l\\,\\in\\,[L]$ , with probability at least $1-\\delta$ ,wehave ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left|w_{j}^{(0)\\top}V^{(0,i)}p_{l}^{(0,i)}\\right|\\leq\\sigma_{1}\\sigma_{0}\\sqrt{\\frac{m}{L}\\log\\frac{m_{1}n L}{\\delta}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Lemma D.6. With probability at least $1-\\delta$ over the randomness in the initialization of a, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle|S_{+1}|=m_{1}\\left(\\frac{1}{2}\\pm\\sqrt{\\frac{2\\log(4/\\delta)}{m_{1}}}\\right),}\\\\ {\\displaystyle|S_{-1}|=m_{1}\\left(\\frac{1}{2}\\pm\\sqrt{\\frac{2\\log(4/\\delta)}{m_{1}}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. The proof follows by applying Hoeffding's inequality. ", "page_idx": 22}, {"type": "text", "text": "Lemma D.7 (Initial sub-network output). Assume that the success of the events in Lemma $D.3$ holds. Forall $i\\in[d]$ withprobability at least $1-\\delta$ over the randomness in the weight initialization, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n|G^{(0)}(\\mu_{i})|\\leq\\widetilde{O}(\\sigma_{1}\\sigma_{0}\\sqrt{m m_{1}}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. Consider a fixed $i\\in[d]$ . By Lemma D.3, we have $\\lVert W_{V}^{(0)}\\mu_{i}\\rVert_{2}^{2}=\\Theta(\\sigma_{0}^{2}m)$ . Thus, conditioned $W_{V}^{(0)}\\mu_{i}$ we have $\\textstyle\\sum_{j=1}^{m_{1}}w_{j}^{(0)}W_{V}^{(0)}\\mu_{i}\\,\\sim\\,{\\mathcal N}(0,\\sigma_{1}^{2}\\sigma_{0}^{2}m m_{1})$ Thus byGaussinconcnraion bound, we have $|G^{(0)}(\\mu_{i})|\\leq\\widetilde O(\\sigma_{1}\\sigma_{0}\\sqrt{m m_{1}})$ \uff1a \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Lemma D.8 (Initial network output). Assume that the success of the events in Lemma D.3 and Lemma $D.4$ holds.For all $i\\in[n]$ withprobabilityat least $1-\\delta$ over therandomness in theweight initialization,wehave ", "page_idx": 22}, {"type": "equation", "text": "$$\n|F^{(0)}(X^{(i)})|\\leq2\\sigma_{0}\\sigma_{1}\\sqrt{2L m_{1}m\\log(2L n/\\delta)}\\leq0.01.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. For fixed $l\\in L,\\;j\\in[m],\\;i\\in[n]$ , by Corollary D.5, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left|w_{j}^{(0)\\top}V^{(0,i)}p_{l}^{(0,i)}\\right|\\leq\\sigma_{1}\\sigma_{0}\\sqrt{\\frac{m}{L}\\log\\frac{m_{1}n L}{\\delta}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus, this implies that a,w? )T() is asub-Gausian randm variable with variance proxy $\\begin{array}{r}{\\sigma_{0}^{2}\\sigma_{1}^{2}\\frac{m}{L}\\log\\frac{m_{1}n L}{\\delta}}\\end{array}$ Therefore,the following inequality holds. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|\\sum_{j=1}^{m_{1}}a_{j}w_{j}^{(0)\\top}V^{(0,i)}p_{l}^{(0,i)}\\right|\\geq2\\sigma_{0}\\sigma_{1}\\sqrt{\\frac{2m_{1}m(\\log\\frac{m_{1}n L}{\\delta})\\log(2/\\delta)}{L}}\\right]\\leq\\delta.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Taking a union bound over $i\\in[n],\\;l\\in[L]$ , with probability at least $1-\\delta$ , for all $i\\in[n]$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n|F^{(0)}(X^{(i)})|\\leq2\\sigma_{0}\\sigma_{1}\\sqrt{2L m_{1}m(\\log\\frac{m_{1}n L}{\\delta})\\log(2L n/\\delta)}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Finally, by Condition 1, we can make $|F^{(0)}(X^{(i)})|\\leq0.01$ ", "page_idx": 22}, {"type": "text", "text": "E Training Dynamics: Phase 1 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "During Phase 1 of training, the linear layer quickly aligns with the target signals and all the remaining quantities stay roughly the same. The analysis need to keep track of the evolution of the above quantities with respect to the two signals $\\mu_{1},\\mu_{2}$ ,thecommontoken $\\mu_{3}$ and the random tokens. ", "page_idx": 22}, {"type": "text", "text": "Definition E.1 (Radius of keys and queries). Define the radius of keys and queries $R_{K},R_{Q}$ respectively tobe ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{K}:=\\underset{i,j\\in[d]}{\\operatorname*{max}}\\left|\\mu_{i}^{\\top}W_{K}^{(t)\\top}W_{K}^{(t)}\\mu_{j}-\\mu_{i}^{\\top}W_{K}^{(0)\\top}W_{K}^{(0)}\\mu_{j}\\right|,}\\\\ &{R_{Q}:=\\underset{i,j\\in[d]}{\\operatorname*{max}}\\left|\\mu_{i}^{\\top}W_{Q}^{(t)\\top}W_{Q}^{(t)}\\mu_{j}-\\mu_{i}^{\\top}W_{Q}^{(0)\\top}W_{Q}^{(0)}\\mu_{j}\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Definition E.2 (Phase 1). Define the range of Phase $^{\\,l}$ to be $[0,T_{1}]$ where $\\begin{array}{r l}{T_{1}}&{{}=}\\end{array}$ $\\operatorname*{min}\\{t^{\\prime},C_{T_{1}}/(\\sigma_{1}^{2}m m_{1})\\}$ for some suffciently large constant ${\\cal C}_{T_{1}}$ and $t^{\\prime}$ is defined to be the maximum time such that for all $t\\le t^{\\prime}$ , all of the following hold: ", "page_idx": 23}, {"type": "equation", "text": "$\\begin{array}{r}{\\cdot\\ \\operatorname*{max}_{j\\in[m],\\mu\\in\\{\\mu_{i}\\}_{i=1}^{3}}\\left|w_{j}^{(t)}W_{V}^{(t)}\\mu-w_{j}^{(0)}W_{V}^{(0)}\\mu\\right|\\leq R\\,w h e r e\\ R<O(1/m_{1});}\\end{array}$ ", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{max}_{j\\in[m],\\mu\\not\\in\\{\\mu_{i}\\}_{i=1}^{3}}\\left|w_{j}^{(t)}W_{V}^{(t)}\\mu-w_{j}^{(0)}W_{V}^{(0)}\\mu\\right|\\leq O(R/n+R/\\sqrt{m});}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{3.\\ \\operatorname*{max}_{\\mu,\\nu\\in\\{\\mu\\}_{i=1}^{d}}\\left|\\mu^{\\top}W_{Q}^{(t)^{\\top}}W_{K}^{(t)}\\nu-\\mu^{\\top}W_{Q}^{(0)^{\\top}}W_{K}^{(0)}\\nu\\right|\\leq R_{S}\\ w h e r e\\ R_{S}\\leq O(1/(m\\sqrt{m}));}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Based on this definition, we can further obtain the maximum softmax probability change as follows. Proposition E.3. Define ", "page_idx": 23}, {"type": "equation", "text": "$$\nR_{P}:=\\operatorname*{max}_{i\\in[n],~\\mu,\\nu\\in X^{(i)}}\\left|p_{q\\leftarrow\\mu,k\\leftarrow\\nu}^{(t,i)}-p_{q\\leftarrow\\mu,k\\leftarrow\\nu}^{(0,i)}\\right|.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then ", "page_idx": 23}, {"type": "equation", "text": "$$\nR_{P}=O\\left({\\frac{1}{\\sqrt{m}L}}+{\\frac{L}{m}}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. By Lemma G.2, we have $\\begin{array}{r}{R_{P}\\leq O(R_{S}/L+R_{S}^{2}L)=O\\left(\\frac{1}{\\sqrt{m}L}+\\frac{L}{m}\\right)\\!.}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "Initially, the loss for the samples with one signal will increase. ", "page_idx": 23}, {"type": "text", "text": "E.1 Initial Gradients ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Lemma E.4 (Signal updates, same as Lemma 4.1). At $t=0$ for $\\mu\\in\\{\\mu_{1},\\mu_{2}\\}$ wehave ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{\\partial a_{j}w_{j}^{(0)}W_{V}^{(0)}\\mu}{\\partial t}=\\Theta((\\sigma_{0}^{2}+\\sigma_{1}^{2})m).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof.Take $\\mu=\\mu_{1}$ . First of all, by the gradient fow update in Lemma C.1, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Lambda_{\\gamma\\gamma}^{(k)}\\mu_{i}=\\frac{1}{n}\\displaystyle\\sum_{s=1}^{n}\\delta_{i,s}^{(k)}\\mu_{\\gamma\\delta_{i}}\\sum_{l=1}^{k}\\alpha_{s,l}\\left<\\sum_{s=1}^{n}\\alpha_{s}\\left<\\sum_{s=1}^{n}\\alpha_{s}^{(k)}\\right>_{l}^{(k)}\\Lambda_{l}^{(k)}\\mu_{\\gamma}^{(k)}\\right>_{l}^{\\gamma}\\mu_{\\gamma}}&{}\\\\ &{\\quad\\quad+\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{s=1}^{n}\\delta_{i,s}^{(k)}\\mu_{\\gamma}\\sum_{l=1}^{k}\\alpha_{s,l}\\beta_{i,s}^{(k)}|\\mathcal{H}_{l}^{(k)}|\\mathcal{H}_{l}^{(k)}|\\mathcal{H}_{l}^{(k)}}\\\\ &{\\quad\\quad=\\frac{1}{n}\\displaystyle\\sum_{s=1}^{n}\\delta_{i,s}^{(k)}\\mu_{\\gamma\\delta_{i}}\\sum_{l=1}^{k}\\sum_{s=1}^{n}\\alpha_{s}\\left<\\sum_{s=1}^{n}\\alpha_{s}^{(k)}\\right>_{l}^{(k)}\\mu_{\\gamma\\delta_{i},l,s-1}^{(k)}\\,,}\\\\ &{\\quad\\quad+\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{s=1}^{n}\\delta_{i,s}^{(k)}\\mu_{\\gamma\\delta_{i}}\\sum_{l=1}^{k}\\alpha_{s,l}\\left<\\sum_{s=1}^{n}\\left<\\mu_{\\gamma,s,l}^{(k)}\\right>_{l}^{(k)}\\Lambda_{l}^{(k)}\\mu_{\\gamma\\delta_{i},l,s-1}^{(k)}\\,,}\\\\ &{\\quad\\quad=\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{s=1}^{n}\\sum_{s=1}^{n}\\delta_{i,s}^{(k)}\\mu_{\\gamma\\delta_{i}}\\sum_{l=1}^{k}\\left(\\alpha_{s,l}\\left|\\mu_{\\gamma}^{(k)}\\right|\\right)_{l}^{(k)}+\\displaystyle\\sum_{s=1}^{n}\\alpha_{s}\\left<\\mu_{\\gamma,s,l}^{(k)}\\right>_{l}^{\\gamma}\\mu_{\\gamma,l,s-1,s-1}^{(k)}\\,}\\\\ &{\\quad\\quad+\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{s\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\displaystyle\\frac{1}{n}\\sum_{i:\\mu\\in X^{(i)}}g_{i}^{(t)}y_{i}\\sum_{l=1}^{L}a_{j_{1}}\\left(||w_{j_{1}}^{(t)}||_{2}^{2}+||v^{(t)}(\\mu)||_{2}^{2}\\right)p_{q\\leftarrow l,k\\leftarrow\\mu}^{(t,i)}}\\\\ &{\\quad+\\displaystyle\\frac{1}{n}\\sum_{i:\\mu\\in X^{(i)}}g_{i}^{(t)}y_{i}\\sum_{l=1}^{L}\\sum_{j_{2}\\neq j_{1}}a_{j_{2}}\\left<w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\\right>p_{q\\leftarrow l,k\\leftarrow\\mu}^{(t,i)}}\\\\ &{\\quad+\\displaystyle\\underbrace{\\frac{1}{n}\\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\\sum_{l=1}^{L}a_{j_{1}}\\sum_{l_{2}=1}^{L}\\left<p_{l_{1},l_{2}}^{(t,i)}v_{l_{2}}^{(t,i)},v^{(t)}(\\mu)\\right>\\mathbb{I}(v_{l_{2}}^{(t,i)}\\neq v^{(t)}(\\mu))}_{\\xi_{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now, by Lemma D.3, Corollary D.2 and Lemma D.8, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\iota_{j_{1}}\\displaystyle\\frac{1}{n}\\sum_{i:\\mu_{1}\\in X^{(i)}}g_{i}^{(0)}y_{i}\\sum_{l=1}^{L}a_{j_{1}}\\left(\\|w_{j_{1}}^{(0)}\\|_{2}^{2}+\\|v^{(0)}(\\mu)\\|_{2}^{2}\\right)p_{q\\leftarrow l,k\\leftarrow\\mu_{1}}^{(0,i)}}\\\\ &{=\\displaystyle\\frac{1}{n}\\sum_{i\\in I_{1}}g_{i}^{(0)}\\sum_{l=1}^{L}\\left(\\|w_{j}^{(0)}\\|_{2}^{2}+\\|v^{(0)}(\\mu_{1})\\|_{2}^{2}\\right)p_{q\\leftarrow l,k\\leftarrow\\mu_{1}}^{(0,i)}-\\displaystyle\\frac{1}{n}\\sum_{i\\in I_{2}}g_{i}^{(0)}\\sum_{l=1}^{L}\\left(\\|w_{j}^{(0)}\\|_{2}^{2}+\\|v^{(0)}(\\mu_{1})\\|_{2}^{2}\\right)p_{l}^{(0)}}\\\\ &{=\\left(\\frac{1}{3}\\pm0.01\\right)L\\cdot\\left(\\sigma_{1}^{2}m+\\sigma_{0}^{2}m\\right)\\left(1\\pm\\left(\\sqrt{\\frac{4}{m}\\log\\frac{2d}{\\delta}}+\\frac{4}{m}\\log\\frac{2d}{\\delta}\\right)\\right)\\displaystyle\\frac{1}{L}(1+o(1)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "On the other hand, by Proposition E.5, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\varepsilon_{1}|=\\left|\\frac{1}{n}\\sum_{i,\\mu_{1}\\in X^{(i)}\\atop i\\neq1}g_{i}^{(0)}y_{i}\\sum_{l=1}^{L}\\sum_{2\\neq j_{1}}a_{j_{l}}\\left\\langle w_{j_{1}}^{(0)},w_{j_{2}}^{(0)}\\right\\rangle p_{q\\in{I},k\\leftarrow\\mu_{1}}^{(0,i)}\\right|}\\\\ &{\\quad\\le\\sigma_{1}^{2}\\sqrt{m_{1}}m\\left(\\sqrt{\\frac{4}{m}}\\log\\frac{2m_{1}^{2}}{\\delta}+\\frac{4}{m}\\log\\frac{2m_{1}^{2}}{\\delta}\\right)\\sqrt{\\log\\frac{m_{1}}{\\delta}};}\\\\ &{|\\varepsilon_{2}|=\\left|\\frac{1}{n}\\sum_{i=1}^{n}g_{i}^{(0)}y_{i}\\sum_{i=1}^{L}a_{j_{1}}\\sum_{l=1}^{L}\\left\\langle p_{l_{1},l_{2}}^{(0,i)}v_{l_{2}}^{(0,i)},v^{(0)}(\\mu_{1})\\right\\rangle\\mathbb{I}(v_{l_{2}}^{(0,i)}\\ne v^{(0)}(\\mu_{1}))\\right|}\\\\ &{\\quad\\le\\sigma_{0}^{2}m\\sqrt{L}\\left(\\sqrt{\\frac{4}{m}}\\log\\frac{2n L}{\\delta}+\\frac{4}{m}\\log\\frac{2n L}{\\delta}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "$m\\geq C m_{1}\\log^{2}{\\frac{m_{1}}{\\delta}}$ inCondifrescil $C$ then $|\\varepsilon_{1}|\\,\\le\\,0.01\\sigma_{1}^{2}m$ andif $\\begin{array}{r}{m\\geq C^{\\prime}L\\log\\frac{2n L}{\\delta}}\\end{array}$ for some sufciently large $C^{\\prime}$ .then $\\left|\\varepsilon_{2}\\right|\\leq0.01\\sigma_{0}^{2}m$ \u53e3 ", "page_idx": 24}, {"type": "text", "text": "Proposition E.5. Assume the events in Lemma $D.3$ andCorollary $D.2$ succeed. With probability at least $1-\\delta$ over the randomness in the weight initialization, for all $j_{1}\\in[m_{1}],$ wehave ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left|\\sum_{j_{2}:j_{2}\\neq j_{1}}a_{j_{2}}\\left\\langle w_{j_{1}}^{(0)},w_{j_{2}}^{(0)}\\right\\rangle\\right|\\leq\\sigma_{1}^{2}\\sqrt{m_{1}}m\\left(\\sqrt{\\frac{4}{m}\\log\\frac{2m_{1}^{2}}{\\delta}}+\\frac{4}{m}\\log\\frac{2m_{1}^{2}}{\\delta}\\right)\\sqrt{\\log\\frac{4m_{1}}{\\delta}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Further, for all $\\mu\\in\\{\\mu_{i}\\}_{i=1}^{d}$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\cfrac{1}{n}\\,\\displaystyle\\sum_{i:\\mu\\in X^{(i)}}g_{i}^{(0)}y_{i}\\sum_{l=1}^{L}\\sum_{j_{2}\\neq j_{1}}a_{j_{2}}\\left<w_{j_{1}}^{(0)},w_{j_{2}}^{(0)}\\right>p_{q\\leftarrow l,k\\leftarrow\\mu}^{(0,i)}\\right|}\\\\ &{\\leq\\frac{|i:\\ \\mu\\in X^{(i)}|}{n}\\sigma_{1}^{2}\\sqrt{m_{1}}m\\left(\\sqrt{\\cfrac{4}{m}\\log\\frac{2m_{1}^{2}}{\\delta}}+\\frac{4}{m}\\log\\frac{2m_{1}^{2}}{\\delta}\\right)\\sqrt{\\log\\frac{m_{1}}{\\delta}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{n}\\sum_{i=1}^{n}g_{i}^{(0)}y_{i}\\sum_{l_{1}=1}^{L}a_{j_{1}}\\sum_{l_{2}=1}^{L}\\left<p_{l_{1},l_{2}}^{(0,i)}v_{l_{2}}^{(0,i)},v^{(0)}(\\mu)\\right>\\mathbb{I}(v_{l_{2}}^{(0,i)}\\neq v^{(0)}(\\mu))\\Bigg|\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\leq\\sigma_{0}^{2}m\\sqrt{L}\\left(\\sqrt{\\frac{4}{m}\\log\\frac{2n L d}{\\delta}}+\\frac{4}{m}\\log\\frac{2n L d}{\\delta}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. First, fix $i,l$ , and consider the randomness of $a$ . By Lemma D.3, $a_{j_{2}}\\left\\langle w_{j_{1}}^{(0)},w_{j_{2}}^{(0)}\\right\\rangle$ is a subGaussian random variable with variance proxy $\\begin{array}{r}{\\sigma_{1}^{4}m^{2}\\left(\\sqrt{\\frac{4}{m}\\log\\frac{2m_{1}^{2}}{\\delta}}+\\frac{4}{m}\\log\\frac{2m_{1}^{2}}{\\delta}\\right)^{2}}\\end{array}$ . This implies that with probability at least $1-\\delta/2$ , for all $j_{1}\\in[m_{1}]$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left|\\sum_{j_{2}:j_{2}\\neq j_{1}}a_{j_{2}}\\left\\langle w_{j_{1}}^{(0)},w_{j_{2}}^{(0)}\\right\\rangle\\right|\\leq\\sigma_{1}^{2}\\sqrt{m_{1}}m\\left(\\sqrt{\\frac{4}{m}\\log\\frac{2m_{1}^{2}}{\\delta}}+\\frac{4}{m}\\log\\frac{2m_{1}^{2}}{\\delta}\\right)\\sqrt{\\log\\frac{4m_{1}}{\\delta}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Thus, by Corollary D.2, for all $\\mu\\in\\{\\mu_{i}\\}_{i=1}^{d}$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\displaystyle\\frac{1}{n}\\sum_{i:\\mu_{1}\\in X^{(i)}}g_{i}^{(0)}y_{i}\\sum_{l=1}^{L}\\sum_{j_{2}:j_{2}\\neq j_{1}}a_{j_{2}}\\left\\langle w_{j_{1}}^{(0)},w_{j_{2}}^{(0)}\\right\\rangle p_{q\\in l,k\\leftarrow\\mu}^{(0,i)}\\right|}\\\\ &{\\leq\\frac{|i:\\ \\mu\\in X^{(i)}|}{n}\\sigma_{1}^{2}\\sqrt{m_{1}}m\\left(\\sqrt{\\displaystyle\\frac{4}{m}\\log\\displaystyle\\frac{2m_{1}^{2}}{\\delta}}+\\frac{4}{m}\\log\\displaystyle\\frac{2m_{1}^{2}}{\\delta}\\right)\\sqrt{\\log\\displaystyle\\frac{m_{1}}{\\delta}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We next derive the second inequality. Consider the randomness in $W_{V}^{(0)}$ . Note that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{l_{2}=1}^{L}p_{l_{1},l_{2}}^{(0,i)}v_{l_{2}}^{(0,i)}\\mathbb{I}(v_{l_{2}}^{(0,i)}\\neq v^{(0)}(\\mu))\\sim\\mathcal{N}\\left(0,\\sigma_{0}^{2}\\sum_{l_{2}=1}^{L}(p_{l_{1},l_{2}}^{(0,i)})^{2}\\mathbb{I}(v_{l_{2}}^{(0,i)}\\neq v^{(0)}(\\mu))I\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Thus, by Lemma H.2 and Corollary D.2 and taking a union bound over $i\\,\\in\\,[n],\\,\\,l_{1}\\,\\in\\,[L],\\,\\,\\mu\\,\\in$ $\\{\\mu_{i}\\}_{i=1}^{d}$ , we have with probability at least $1-\\delta/2$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left|\\left\\langle\\sum_{l=1}^{L}p_{l_{1},l_{2}}^{(0,i)}v_{l_{2}}^{(0,i)}\\mathbb{I}(v_{l_{2}}^{(0,i)}\\neq v^{(0)}(\\mu)),v^{(0)}(\\mu)\\right\\rangle\\right|\\leq\\sigma_{0}^{2}m\\frac{1}{\\sqrt{L}}\\left(\\sqrt{\\frac{4}{m}\\log\\frac{2n L d}{\\delta}}+\\frac{4}{m}\\log\\frac{2n L d}{\\delta}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Therefore, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\cfrac{1}{n}\\sum_{i=1}^{n}g_{i}^{(0)}y_{i}\\sum_{1=1}^{L}a_{j_{1}}\\sum_{l_{2}=1}^{L}\\left<p_{l_{1},l_{2}}^{(0,i)}v_{l_{2}}^{(0,i)},v^{(0)}(\\mu)\\right>\\mathbb{I}(v_{l_{2}}^{(0,i)}\\neq v^{(0)}(\\mu))\\right|}\\\\ &{\\leq\\sigma_{0}^{2}m\\sqrt{L}\\left(\\sqrt{\\cfrac{4}{m}\\log\\frac{2n L d}{\\delta}}+\\frac{4}{m}\\log\\frac{2n L d}{\\delta}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Lemma E.6 (Random token updates). For $\\mu\\in\\{\\mu_{i}\\}_{i=4}^{d}$ wehave ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{\\partial a_{j}w_{j}^{(0)}W_{V}^{(0)}\\mu}{\\partial t}=O\\left(\\frac{1}{n}(\\sigma_{0}^{2}+\\sigma_{1}^{2})m+\\sigma_{0}^{2}\\sqrt{m L}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. Following the proof of Lemma E.4, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\frac{\\partial{a_{j}}w_{j}^{(0)}W_{V}^{(0)}{\\mu}}{\\partial{t}}=\\frac{1}{n}\\sum_{i_{2}=1}^{n}g_{i_{2}}^{(0)}{y}_{i_{2}}\\sum_{l_{2}=1}^{L}\\sum_{j_{2}=1}^{m_{1}}a_{j_{2}}\\left<w_{j_{1}}^{(0)},w_{j_{2}}^{(0)}\\right>\\left(X^{(i_{2})}p_{l_{2}}^{(0,i_{2})}\\right)^{\\top}\\mu}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad+\\,\\frac{1}{n}\\sum_{i_{1}=1}^{n}g_{i_{1}}^{(0)}{y}_{i_{1}}\\sum_{l_{1}=1}^{L}a_{j_{1}}p_{l_{1}}^{(0,i_{1})\\top}{V}^{(0,i_{1})\\top}W_{V}^{(0)}\\mu}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad=\\frac{1}{n}\\sum_{i_{2}\\neq\\pm N^{(i)}}g_{i}^{(0)}{y}_{i_{2}}\\sum_{l=1}^{L}a_{j_{1}}\\left(\\|w_{j_{1}}^{(0)}\\|_{2}^{2}+\\|v^{(0)}(\\mu)\\|_{2}^{2}\\right)p_{q\\leftarrow l,k\\leftarrow\\mu}^{(0,i)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle+\\underbrace{\\frac{1}{n}\\sum_{i:\\mu\\in X^{(i)}}g_{i}^{(0)}y_{i}\\sum_{l=1}^{L}\\sum_{j_{2}:j_{2}\\neq j_{1}}a_{j_{2}}\\left<w_{j_{1}}^{(0)},w_{j_{2}}^{(0)}\\right>p_{q\\leftarrow l,k\\leftarrow\\mu}^{(0,i)}}_{{\\varepsilon_{1}}}}}\\\\ {{\\displaystyle+\\underbrace{\\frac{1}{n}\\sum_{i=1}^{n}g_{i}^{(0)}y_{i}\\sum_{l_{1}=1}^{L}a_{j_{1}}\\sum_{l_{2}=1}^{L}\\left<p_{l_{1},l_{2}}^{(0,i)}v_{l_{2}}^{(0,i)},v^{(0)}(\\mu)\\right>\\mathbb{I}(v_{l_{2}}^{(0,i)}\\neq v^{(0)}(\\mu))}_{{\\varepsilon_{1}}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By Proposition E.5 and the fact that only one $X^{(i)}$ satisfies $\\boldsymbol{\\mu}\\in X^{(i)}$ ,wehave ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{i:\\mu\\in X^{(i)}}g_{i}^{(0)}y_{i}\\sum_{l=1}^{L}a_{j_{1}}\\left(\\|w_{j_{1}}^{(0)}\\|_{2}^{2}+\\|v^{(0)}(\\mu)\\|_{2}^{2}\\right)p_{q\\leftarrow l,k\\leftarrow\\mu}^{(0,i)}=\\Theta\\left(\\frac{1}{n}(\\sigma_{0}^{2}+\\sigma_{1}^{2})m\\right),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\varepsilon_{1}|\\leq\\displaystyle\\frac{1}{n}\\sigma_{1}^{2}\\sqrt{m_{1}}m\\left(\\sqrt{\\displaystyle\\frac{4}{m}\\log\\displaystyle\\frac{2m_{1}^{2}}{\\delta}}+\\frac{4}{m}\\log\\displaystyle\\frac{2m_{1}^{2}}{\\delta}\\right)\\sqrt{\\log\\displaystyle\\frac{4m_{1}}{\\delta}},}\\\\ &{|\\varepsilon_{2}|\\leq\\sigma_{0}^{2}m\\sqrt{L}\\left(\\sqrt{\\displaystyle\\frac{4}{m}\\log\\displaystyle\\frac{2n L d}{\\delta}}+\\frac{4}{m}\\log\\displaystyle\\frac{2n L d}{\\delta}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "E.2  Maximum Perturbation of Neuron Outputs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Lemma E.7. For all $t\\leq T_{1},$ for all $i\\in[n],\\;l\\in[L].$ we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left|w_{j}^{(t)}W_{V}^{(t)}X^{(i)}p_{l}^{(t,i)}-w_{j}^{(0)}W_{V}^{(0)}X^{(i)}p_{l}^{(0,i)}\\right|\\leq\\widetilde{O}(L R_{P}\\sigma_{0}\\sigma_{1}\\sqrt{m})+4R/L.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. By Definition E.2, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w_{j}^{(t)}V^{(t,i)}p_{l}^{(t,i)}-w_{j}^{(0)}V^{(0,i)}p_{l}^{(0,i)}\\Big|}\\\\ &{\\leq\\displaystyle\\sum_{l^{\\prime}=1}^{L}\\left|w_{j}^{(t)}V_{l^{\\prime}}^{(t,i)}p_{l,l^{\\prime}}^{(t,i)}-w_{j}^{(0)}V_{l^{\\prime}}^{(0,i)}p_{l,l^{\\prime}}^{(0,i)}\\right|}\\\\ &{\\leq\\displaystyle\\sum_{l^{\\prime}:V_{l^{\\prime}}\\in v(\\mu_{1},\\mu_{2},\\mu_{3})}\\left(R_{P}\\widetilde{O}(\\sigma_{0}\\sigma_{1}\\sqrt{m})+R/L\\right)+\\displaystyle\\sum_{l^{\\prime}:V_{l^{\\prime}}\\notin v(\\mu_{1},\\mu_{2},\\mu_{3})}\\left(R_{P}\\widetilde{O}(\\sigma_{0}\\sigma_{1}\\sqrt{m})+R/(n L)\\right)}\\\\ &{\\leq\\widetilde{O}(R p\\sigma_{0}\\sigma_{1}\\sqrt{m}+R/L)+\\widetilde{O}(L R p\\sigma_{0}\\sigma_{1}\\sqrt{m}+R/n)}\\\\ &{=\\widetilde{O}(L R p\\sigma_{0}\\sigma_{1}\\sqrt{m})+4R/L.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By our choice of parameters in Condition 1 and Definition E.2, we have $\\widetilde{\\cal O}(L R_{P}\\sigma_{0}\\sigma_{1}\\sqrt{m})\\!+\\!4R/L<$ $\\sigma_{0}\\sigma_{1}\\sqrt{m/L}$ \u53e3 ", "page_idx": 26}, {"type": "text", "text": "E.3Perturbation Term Involving Correlation of Value-transformed Data ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Proposition E.8. With probability at least $1-\\delta,$ for all $\\mu\\in\\{\\mu_{i}\\}_{i=1}^{d}$ ,we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left|\\sum_{j=1}^{m_{1}}a_{j}\\mu^{\\top}W_{V}^{(0)\\top}w_{j}^{(0)}\\right|\\leq\\widetilde{O}(\\sigma_{0}\\sigma_{1}\\sqrt{m m_{1}}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. The proof is similar to Proposition E.22, and is omitted. ", "page_idx": 26}, {"type": "text", "text": "Lemma E.9 (Value correlation change). For all $\\mu,\\nu\\in\\{\\mu_{i}\\}_{i=1}^{d}$ we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left|{\\frac{\\partial\\left\\langle v^{(t)}(\\mu),v^{(t)}(\\nu)\\right\\rangle}{\\partial t}}\\right|\\leq{\\frac{\\left|\\left\\{i:\\mu\\in X^{(i)}\\right\\}\\right|+\\left|\\left\\{i:\\nu\\in X^{(i)}\\right\\}\\right|}{n}}O(1),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and thus, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left|\\left\\langle v^{(t)}(\\mu),v^{(t)}(\\nu)\\right\\rangle-\\left\\langle v^{(0)}(\\mu),v^{(0)}(\\nu)\\right\\rangle\\right|\\leq t\\frac{\\left|\\left\\{i:\\mu\\in X^{(i)}\\right\\}\\right|+\\left|\\left\\{i:\\nu\\in X^{(i)}\\right\\}\\right|}{n}O(1)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for all $t\\,\\leq\\,T_{1}$ .Thus, for $\\mu\\neq\\nu$ we have $\\left|\\left\\langle v^{(t)}(\\mu),v^{(t)}(\\nu)\\right\\rangle\\right|\\,\\leq\\,\\widetilde{O}(\\sigma_{0}^{2}\\sqrt{m})$ and $\\|v^{(t)}(\\mu)\\|_{2}^{2}\\,=$ $\\Theta(\\sigma_{0}^{2}m)$ for $t\\leq T_{1}$ ", "page_idx": 27}, {"type": "text", "text": "Proof. By Lemma C.3, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\partial\\nu W_{V}^{(t)\\top}W_{V}^{(t)}\\mu}{\\partial t}=\\frac{1}{n}\\displaystyle\\sum_{i:\\mu\\in X^{(i)}}g_{i}^{(t)}y_{i}\\displaystyle\\sum_{l=1}^{L}\\sum_{j=1}^{m_{1}}a_{j}\\nu^{\\top}W_{V}^{(t)\\top}w_{j}^{(t)}p_{q\\leftarrow l,k\\leftarrow\\mu}^{(t,i)}}\\\\ {+\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i:\\nu\\in X^{(i)}}g_{i}^{(t)}y_{i}\\displaystyle\\sum_{l=1}^{L}\\sum_{j=1}^{m_{1}}a_{j}\\mu^{\\top}W_{V}^{(t)\\top}w_{j}^{(t)}p_{q\\leftarrow l,k\\leftarrow\\nu}^{(t,i)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Further, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left|\\sum_{j=1}^{m_{1}}a_{j}\\nu^{\\top}W_{V}^{(t)\\top}w_{j}^{(t)}-\\sum_{j=1}^{m_{1}}a_{j}\\nu^{\\top}W_{V}^{(0)\\top}w_{j}^{(0)}\\right|\\leq m_{1}R.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, by Proposition E.8, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{j=1}^{m_{1}}a_{j}w_{j}^{(t)\\top}W_{V}^{(t)}\\nu\\Bigg|\\leq\\left|\\displaystyle\\sum_{j=1}^{m_{1}}a_{j}\\dot{\\sigma}_{i,l,j}^{(0)}w_{j}^{(0)\\top}W_{V}^{(0)}\\nu\\right|+\\left|\\displaystyle\\sum_{j=1}^{m_{1}}a_{j}\\nu^{\\top}W_{V}^{(t)\\top}w_{j}^{(t)}-\\sum_{j=1}^{m_{1}}a_{j}\\nu^{\\top}W_{V}^{(0)\\top}w_{j}^{(0)}\\right|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\widetilde{O}(\\sigma_{0}\\sigma_{1}\\sqrt{m m_{1}})+m_{1}R,}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which implies ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{n}\\displaystyle\\sum_{i:\\mu\\in X^{(i)}}g_{i}^{(t)}y_{i}\\displaystyle\\sum_{l=1}^{L}\\sum_{j=1}^{m_{1}}a_{j}\\nu^{\\top}W_{V}^{(t)\\top}w_{j}^{(t)}p_{q\\leftarrow l,k\\leftarrow\\mu}^{(t,i)}}\\\\ &{\\quad+\\frac{1}{n}\\displaystyle\\sum_{i:\\nu\\in X^{(i)}}g_{i}^{(t)}y_{i}\\displaystyle\\sum_{l=1}^{L}\\sum_{j=1}^{m_{1}}a_{j}\\mu^{\\top}W_{V}^{(t)\\top}w_{j}^{(t)}p_{q\\leftarrow l,k\\leftarrow\\nu}^{(t,i)}\\Bigg|}\\\\ &{\\leq\\frac{1}{n}\\left(\\left|\\left\\{i:\\mu\\in X^{(i)}\\right\\}\\right|+\\left|\\left\\{i:\\nu\\in X^{(i)}\\right\\}\\right|\\right)\\left(\\tilde{O}(\\sigma_{0}\\sigma_{1}\\sqrt{m m_{1}})+m_{1}R\\right)}\\\\ &{\\leq\\frac{\\left|\\left\\{i:\\mu\\in X^{(i)}\\right\\}\\right|+\\left|\\left\\{i:\\nu\\in X^{(i)}\\right\\}\\right|}{n}O(1)}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the last inequality applies Lemma E.7. ", "page_idx": 27}, {"type": "text", "text": "Corollary E.10. For all $t\\leq T_{1}$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{n}\\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\\sum_{l_{1}=1}^{L}a_{j_{1}}\\sum_{l_{2}=1}^{L}\\left<p_{l_{1},l_{2}}^{(t,i)}v_{l_{2}}^{(t,i)},v^{(t)}(\\mu_{1})\\right>\\mathbb{I}(v_{l_{2}}^{(t,i)}\\neq v^{(t)}(\\mu_{1}))\\right|\\leq L\\cdot\\widetilde{O}\\left(\\sigma_{0}^{2}\\sqrt{m}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. We derive the following bound: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac1{n}\\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\\sum_{l_{1}=1}^{L}a_{j_{1}}\\sum_{l_{2}=1}^{L}\\left<p_{l_{1},l_{2}}^{(t,i)}v_{l_{2}}^{(t,i)},v^{(t)}(\\mu)\\right>\\mathbb{I}(v_{l_{2}}^{(t,i)}\\ne v^{(t)}(\\mu))}\\\\ &{\\le\\displaystyle\\sum_{l_{1}=1}^{L}\\sum_{l_{2}=1}^{L}p_{l_{1},l_{2}}^{(t,i)}\\left|\\left<v_{l_{2}}^{(t,i)},v^{(t)}(\\mu)\\right>\\right|\\mathbb{I}(v_{l_{2}}^{(t,i)}\\ne v^{(t)}(\\mu))}\\\\ &{\\le L\\cdot\\widetilde{O}\\left(\\sigma_{0}^{2}\\sqrt{m}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the last inequality follows from Lemma E.9 and Lemma D.3. ", "page_idx": 27}, {"type": "text", "text": "E.4 Perturbation Term Involving Correlation of Neurons ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Lemma E.11. For all $t\\leq T_{1}$ ,we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{n}\\sum_{\\substack{i;\\mu_{1}\\in X^{(i)}}}g_{i}^{(t)}y_{i}\\sum_{l=1}^{L}\\sum_{j_{2}\\neq j_{1}}a_{j_{2}}\\left\\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\\right\\rangle p_{q\\leftarrow l,k\\leftarrow\\mu_{1}}^{(t,i)}\\right|\\leq\\frac{|\\{i:\\mu_{1}\\in X^{(i)}\\}|}{n}\\widetilde{O}(\\sigma_{1}^{2}\\sqrt{m m_{1}}).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. We first derive: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{n}\\sum_{\\substack{i:\\mu_{1}\\in X^{(i)}}}g_{i}^{(t)}y_{i}\\sum_{l=1}^{L}\\sum_{j_{2}\\neq j_{1}}a_{j_{2}}\\left<w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\\right>p_{q\\in-l,k\\leftarrow\\mu_{1}}^{(t,i)}\\Bigg|}\\\\ &{\\leq\\displaystyle\\left|\\frac{1}{n}\\sum_{\\substack{i:\\mu_{1}\\in X^{(i)}}}g_{i}^{(t)}y_{i}\\sum_{l=1}^{L}\\sum_{j_{2}\\neq j_{1}}a_{j_{2}}\\left<w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\\right>\\right|\\cdot\\operatorname*{max}_{i,l}p_{q\\in-l,k\\leftarrow\\mu_{1}}^{(t,i)}}\\\\ &{\\leq\\displaystyle\\left|\\frac{1}{n}\\sum_{\\substack{i:\\mu_{1}\\in X^{(i)}}}g_{i}^{(t)}y_{i}\\sum_{l=1}^{L}\\sum_{j_{2}\\neq j_{1}}a_{j_{2}}\\left<w_{j_{1}}^{(0)},w_{j_{2}}^{(0)}\\right>\\right|\\cdot\\operatorname*{max}_{i,l}p_{q\\in-l,k\\leftarrow\\mu_{1}}^{(t,i)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n+\\left|\\frac{1}{n}\\sum_{i;\\mu_{1}\\in X^{(i)}}g_{i}^{(t)}y_{i}\\sum_{l=1}^{L}\\sum_{j_{2}\\neq j_{1}}a_{j_{2}}\\left(\\left\\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\\right\\rangle-\\left\\langle w_{j_{1}}^{(0)},w_{j_{2}}^{(0)}\\right\\rangle\\right)\\right|\\cdot\\operatorname*{max}_{i,l}p_{q\\gets l,k\\gets\\mu_{1}}^{(t,i)}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "For term (1), by Proposition E.5, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{n}\\sum_{\\substack{i;\\mu_{1}\\in X^{(i)}}}g_{i}^{(t)}y_{i}\\sum_{l=1}^{L}\\sum_{j_{2}\\neq j_{1}}a_{j_{2}}\\left\\langle w_{j_{1}}^{(0)},w_{j_{2}}^{(0)}\\right\\rangle\\right|\\leq\\frac{|\\{i:\\mu_{1}\\in X^{(i)}\\}|}{n}\\widetilde{O}\\left(\\sigma_{1}^{2}\\sqrt{m m_{1}}L\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "For term (2), note that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\cfrac{1}{n}\\,\\displaystyle\\sum_{i:\\mu_{1}\\in X^{(i)}}g_{i}^{(t)}y_{i}\\sum_{l=1}^{L}\\sum_{j_{2}\\neq j_{1}}a_{j_{2}}\\left(\\left<w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\\right>-\\left<w_{j_{1}}^{(0)},w_{j_{2}}^{(0)}\\right>\\right)\\right|}\\\\ &{\\leq\\displaystyle\\frac{\\left|\\left\\{i:\\mu_{1}\\in X^{(i)}\\right\\}\\right|}{n}L m_{1}\\cdot O\\left(\\frac{1}{m}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the inequality is by Lemma D.3, Proposition E.12. ", "page_idx": 28}, {"type": "text", "text": "Finally, since $\\begin{array}{r}{p_{q\\leftarrow l,k\\leftarrow\\mu_{1}}^{(t,i)}\\leq\\frac{1}{L}+\\frac{1}{L^{2}}+R_{P}}\\end{array}$ combinng theuerbodfth ms 1\uff09a we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\left\\|\\frac{\\alpha}{n}\\sum_{i:\\mu_{1}\\in X^{(i)}}g_{i}^{(t)}y_{i}\\sum_{l=1}^{L}\\sum_{j_{2}\\neq j_{1}}a_{j_{2}}\\dot{\\sigma}_{i,l,j_{2}}^{(t)}\\left\\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\\right\\rangle p_{q\\leftarrow l,k\\leftarrow\\mu_{1}}^{(t,i)}\\right\\|}\\\\ &{\\quad\\quad\\leq\\alpha\\frac{\\left|\\left\\{i:\\mu_{1}\\in X^{(i)}\\right\\}\\right|}{n}\\cdot\\widetilde{O}\\left(\\sigma_{1}^{2}\\sqrt{m m_{1}}L\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proposition E.12 (Neuron correlation change, Phase 1). For $t\\leq T_{1},$ for all $j_{1},j_{2}\\in[m_{1}]$ we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left|\\frac{\\partial\\left\\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\\right\\rangle}{\\partial t}\\right|\\leq2L\\left(\\sigma_{1}\\sigma_{0}\\sqrt{\\frac{m}{L}\\log\\frac{m_{1}n L}{\\delta}}+\\widetilde{O}(L R_{P}\\sigma_{0}\\sigma_{1}\\sqrt{m})+4R/L\\right),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and thus, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left|\\left\\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\\right\\rangle-\\left\\langle w_{j_{1}}^{(0)},w_{j_{2}}^{(0)}\\right\\rangle\\right|\\leq t2L\\left(\\sigma_{1}\\sigma_{0}\\sqrt{\\frac{m}{L}\\log\\frac{m_{1}n L}{\\delta}}+\\widetilde{O}(L R_{P}\\sigma_{0}\\sigma_{1}\\sqrt{m})+4R/L\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. By the gradient flow update in Lemma C.2, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\displaystyle\\partial\\left\\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\\right\\rangle}{\\displaystyle\\partial t}=\\frac{1}{n}\\sum_{i^{\\prime}=1}^{n}g_{i^{\\prime}}^{(t)}y_{i^{\\prime}}\\sum_{l^{\\prime}=1}^{L}a_{j_{2}}w_{j_{1}}^{(t)^{\\top}}V^{(t,i^{\\prime})}p_{l^{\\prime}}^{(t,i^{\\prime})}+\\frac{1}{n}\\sum_{i^{\\prime}=1}^{n}g_{i^{\\prime}}^{(t)}y_{i^{\\prime}}\\sum_{l^{\\prime}=1}^{L}a_{j_{1}}w_{j_{2}}^{(t)^{\\top}}V^{(t,i^{\\prime})}p_{l^{\\prime}}^{(t,i^{\\prime})}}\\\\ &{\\qquad\\qquad=\\frac{1}{n}\\sum_{i^{\\prime}=1}^{n}g_{i^{\\prime}}^{(t)}y_{i^{\\prime}}\\sum_{l^{\\prime}=1}^{L}a_{j_{2}}\\left(w_{j_{1}}^{(0)^{\\top}}V^{(0,i^{\\prime})}p_{l^{\\prime}}^{(0,i^{\\prime})}+(w_{j_{1}}^{(t)^{\\top}}V^{(t,i^{\\prime})}p_{l^{\\prime}}^{(t,i^{\\prime})}-w_{j_{1}}^{(0)^{\\top}}V^{(0,i^{\\prime})}p_{l^{\\prime}}^{(0,i^{\\prime})}\\right.}\\\\ &{\\qquad\\qquad\\left.+\\frac{1}{n}\\sum_{i^{\\prime}=1}^{n}g_{i^{\\prime}}^{(t)}y_{i^{\\prime}}\\sum_{l^{\\prime}=1}^{L}a_{j_{1}}\\left(w_{j_{2}}^{(0)^{\\top}}V^{(0,i^{\\prime})}p_{l^{\\prime}}^{(0,i^{\\prime})}\\right)+(w_{j_{2}}^{(t)^{\\top}}V^{(t,i^{\\prime})}p_{l^{\\prime}}^{(t,i^{\\prime})}-w_{j_{2}}^{(0)^{\\top}}V^{(0,i^{\\prime})}p_{l^{\\prime}}^{(0,i^{\\prime})}\\right)}\\end\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Now, since by Lemma E.7 we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left|w_{j}^{(t)}W_{V}^{(t)}X^{(i)}p_{l}^{(t,i)}-w_{j}^{(0)}W_{V}^{(0)}X^{(i)}p_{l}^{(0,i)}\\right|\\leq\\tilde{O}(L R_{P}\\sigma_{0}\\sigma_{1}\\sqrt{m})+4R/L,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "by Definition E.2 and Corollary D.5, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\Big|\\frac{1}{n}\\sum_{i^{\\prime}=1}^{n}g_{i^{\\prime}}^{(t)}y_{i^{\\prime}}^{L}\\sum_{\\nu=1}^{L}a_{j_{2}}\\left(w_{j_{1}}^{(0)\\top}V^{(0,i^{\\prime})}p_{l^{\\prime}}^{(0,i^{\\prime})}+(w_{j_{1}}^{(t)\\top}V^{(t,i^{\\prime})}p_{l^{\\prime}}^{(t,i^{\\prime})}-w_{j_{1}}^{(0)\\top}V^{(0,i^{\\prime})}p_{l^{\\prime}}^{(0,i^{\\prime})})\\right)\\Big|}\\\\ &{\\le L\\left(\\sigma_{1}\\sigma_{0}\\sqrt{\\displaystyle\\frac{m}{L}\\log\\frac{m_{1}n L}{\\delta}}+\\widetilde{O}(L R_{P}\\sigma_{0}\\sigma_{1}\\sqrt{m})+4R/L\\right)}\\\\ &{\\displaystyle\\Big|\\frac{1}{n}\\sum_{i^{\\prime}=1}^{n}g_{i^{\\prime}}^{(t)}y_{i^{\\prime}}^{L}\\sum_{\\nu=1}^{L}a_{j_{1}}\\left(w_{j_{2}}^{(0)\\top}V^{(0,i^{\\prime})}p_{l^{\\prime}}^{(0,i^{\\prime})}+(w_{j_{2}}^{(t)\\top}V^{(t,i^{\\prime})}p_{l^{\\prime}}^{(t,i^{\\prime})}-w_{j_{2}}^{(0)\\top}V^{(0,i^{\\prime})}p_{l^{\\prime}}^{(0,i^{\\prime})})\\right)\\Big|}\\\\ &{\\le L\\left(\\sigma_{1}\\sigma_{0}\\sqrt{\\displaystyle\\frac{m}{L}\\log\\frac{m_{1}n L}{\\delta}}+\\widetilde{O}(L R_{P}\\sigma_{0}\\sigma_{1}\\sqrt{m})+4R/L\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Thus, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left\\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\\right\\rangle-\\left\\langle w_{j_{1}}^{(0)},w_{j_{2}}^{(0)}\\right\\rangle\\right|}\\\\ &{\\leq\\displaystyle\\int_{\\tau=0}^{t}\\left|\\frac{\\partial\\left\\langle w_{j_{1}}^{(\\tau)},w_{j_{2}}^{(\\tau)}\\right\\rangle}{\\partial\\tau}\\right|\\leq t2L\\left(\\sigma_{1}\\sigma_{0}\\sqrt{\\frac{m}{L}\\log\\frac{m_{1}n L}{\\delta}}+\\widetilde{O}(L R_{P}\\sigma_{0}\\sigma_{1}\\sqrt{m})+4R/L\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Corollary E.13 (Neuron Norm Change, Phase 1). For all $t\\leq T_{1}$ andall $j\\in[m]$ wehave ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left|\\|w_{j}^{(t)}\\|_{2}^{2}-\\|w_{j}^{(0)}\\|_{2}^{2}\\right|\\leq t2L(\\sigma_{1}\\sigma_{0}\\sqrt{\\frac{m}{L}\\log\\frac{m_{1}n L}{\\delta}}+\\widetilde{O}(L R_{P}\\sigma_{0}\\sigma_{1}\\sqrt{m})+4R/L).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "E.5  Neuron Weights Align with Signal Value ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Theorem E.14 (Signal correlation growth, phase 1). For $t\\leq T_{1}$ for $\\mu\\in\\{\\mu_{1},\\mu_{2}\\}$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{\\partial a_{j}w_{j}^{(t)}W_{V}^{(t)}\\mu}{\\partial t}=\\frac{1}{n}\\Bigg(\\sum_{\\substack{i;\\mu\\in X^{(i)},}}g_{i}^{(t)}\\sum_{l=1}^{L}p_{q\\leftarrow l,k\\leftarrow\\mu}^{(t,i)}-\\sum_{\\substack{i;\\mu\\in X^{(i)},}}g_{i}^{(t)}\\sum_{l=1}^{L}p_{q\\leftarrow l,k\\leftarrow\\mu}^{(t,i)}\\Bigg)\\Theta((\\sigma_{0}^{2}+\\sigma_{1}^{2})m)+\\varepsilon_{0}^{2}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where ", "page_idx": 29}, {"type": "equation", "text": "$$\n|\\varepsilon|\\leq\\widetilde O(L\\sigma_{0}^{2}\\sqrt{m}+\\sigma_{1}^{2}\\sqrt{m m_{1}}).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. We take $\\mu=\\mu_{1}$ and the proof is similar for $\\mu=\\mu_{2}$ . By Lemma C.1, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{\\partial a_{j}w_{j}^{(t)}W_{V}^{(t)}\\mu}{\\partial t}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\displaystyle\\frac{1}{n}\\sum_{\\substack{i;\\mu_{1}\\in X^{(i)},y_{i}=1}}g_{i}^{(t)}\\sum_{l=1}^{L}\\left(\\|w_{j}^{(t)}\\|_{2}^{2}+\\|v^{(t)}(\\mu_{1})\\|_{2}^{2}\\right)p_{q\\leftarrow l,k\\leftarrow\\mu_{1}}^{(t,i)}}\\\\ &{\\quad-\\displaystyle\\frac{1}{n}\\sum_{\\substack{i;\\mu_{1}\\in X^{(i)},y_{i}=-1}}g_{i}^{(t)}\\sum_{l=1}^{L}\\left(\\|w_{j}^{(t)}\\|_{2}^{2}+\\|v^{(t)}(\\mu_{1})\\|_{2}^{2}\\right)p_{q\\leftarrow l,k\\leftarrow\\mu_{1}}^{(t,i)}+\\varepsilon}\\\\ &{=\\left(\\|w_{j}^{(t)}\\|_{2}^{2}+\\|v^{(t)}(\\mu_{1})\\|_{2}^{2}\\right)\\displaystyle\\frac{1}{n}\\left(\\sum_{\\substack{i;\\mu_{1}\\in X^{(i)},\\,j=1}}g_{i}^{(t)}\\sum_{l=1}^{L}p_{q\\leftarrow l,k\\leftarrow\\mu_{1}}^{(t,i)}-\\sum_{\\substack{i;\\mu_{1}\\in X^{(i)},\\,l=1}}g_{i}^{(t)}\\sum_{l=1}^{L}p_{q\\leftarrow l,k\\leftarrow\\mu_{1}}^{(t,i)}\\right)+\\varepsilon}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\varepsilon=\\frac{1}{n}\\sum_{i:\\mu_{1}\\in X^{(i)}}g_{i}^{(t)}y_{i}\\sum_{l=1}^{L}\\sum_{j_{2}\\neq j_{1}}a_{j_{2}}\\left<w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\\right>p_{q\\leftarrow l,k\\leftarrow\\mu_{1}}^{(t,i)}}\\\\ {\\displaystyle\\ \\ +\\frac{1}{n}\\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\\sum_{l_{1}=1}^{L}a_{j_{1}}\\sum_{l_{2}=1}^{L}\\left<p_{l_{1},l_{2}}^{(t,i)}v_{l_{2}}^{(t,i)},v^{(t)}(\\mu_{1})\\right>\\mathbb{I}(v_{l_{2}}^{(t,i)}\\neq v^{(t)}(\\mu_{1})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We can now bound the magnitude of $\\varepsilon$ by Corollary E.10, Lemma E.11 and Proposition E.5 and obtain: ", "page_idx": 30}, {"type": "equation", "text": "$$\n|\\varepsilon|\\leq L\\cdot\\widetilde{O}\\left(\\sigma_{0}^{2}\\sqrt{m}\\right)+\\widetilde{O}(\\sigma_{1}^{2}\\sqrt{m m_{1}}).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Finally, by Lemma D.3 and Corollary E.13, we have $\\|w_{j}^{(t)}\\|_{2}^{2}=\\Theta(\\sigma_{1}^{2}m)$ and by Lemma E.9, we have $\\|v^{(t)}(\\mu_{1})\\|_{2}^{2}=\\Theta(\\sigma_{0}^{2}m)$ . The proof is completed. \u53e3 ", "page_idx": 30}, {"type": "text", "text": "Theorem E.15 (Random token growth, Phase 1). For $t\\leq T_{1}$ for $\\mu\\in\\mathcal R$ wehave ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{\\partial a_{j}w_{j}^{(t)}W_{V}^{(t)}\\mu}{\\partial t}=O\\left(\\frac{1}{n}(\\sigma_{0}^{2}+\\sigma_{1}^{2})m\\right)+\\widetilde{O}(\\sigma_{0}^{2}L\\sqrt{m}).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. Fix a $\\mu\\in\\mathcal R$ . By our Assumption 2.3, $\\mu$ appears at most once in the training data set. Now, assume $\\mu$ is in the training set and let $i^{\\star}$ be the index of the sample containing $\\mu$ . Applying Lemma C.1 on the random token $\\mu$ ,we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{\\partial a_{j}w_{j}^{(t)}W_{V}^{(t)}\\mu}{\\partial t}=\\frac{1}{n}g_{i^{\\star}}^{(t)}y_{i^{\\star}}\\sum_{l=1}^{L}a_{j}\\left(\\|w_{j}^{(t)}\\|_{2}^{2}+\\|v^{(t)}(\\mu)\\|_{2}^{2}\\right)p_{q\\leftarrow l,k\\leftarrow\\mu}^{(t,i^{\\star})}+\\varepsilon,\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\varepsilon=\\frac{1}{n}g_{i^{\\star}}^{(t)}y_{i^{\\star}}\\sum_{l=1}^{L}\\sum_{j_{2}\\neq j_{1}}a_{j_{2}}\\left<w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\\right>p_{q\\leftarrow l,k\\leftarrow\\mu}^{(t,i^{\\star})}}}\\\\ {{\\displaystyle\\qquad+\\,\\frac{1}{n}\\sum_{i=1}^{n}g^{(t)}y_{i}\\sum_{l_{1}=1}^{L}a_{j_{1}}\\sum_{l_{2}=1}^{L}\\left<p_{l_{1},l_{2}}^{(t,i)}v_{l_{2}}^{(t,i)},v^{(t)}(\\mu)\\right>\\mathbb{I}(v_{l_{2}}^{(t,i)}\\neq v^{(t)}(\\mu)).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We can now bound the magnitude of $\\varepsilon$ by Corollary E.10 and Lemma E.11 as follows: ", "page_idx": 30}, {"type": "equation", "text": "$$\n|\\varepsilon|\\leq L\\cdot\\widetilde{O}\\left(\\sigma_{0}^{2}\\sqrt{m}\\right)+\\frac{1}{n}\\widetilde{O}(\\sigma_{1}^{2}\\sqrt{m m_{1}}).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Finally, by Lemma D.3 and Corollary E.13, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{\\partial a_{j}w_{j}^{(t)}W_{V}^{(t)}\\mu}{\\partial t}=O\\left(\\frac{1}{n}(\\sigma_{0}^{2}+\\sigma_{1}^{2})m\\right)+\\widetilde{O}(\\sigma_{0}^{2}L\\sqrt{m}).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "E.6Alignment of Common Token ", "page_idx": 31}, {"type": "text", "text": "Lemma E.16 (Initial Per Neuron Gradient of Common Token, same as Lemma 4.2). Let ${\\cal F}=$ $\\operatorname*{max}_{i}F_{i}^{(0)}$ We have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\left|\\frac{\\partial a_{j}w_{j}^{(0)}W_{V}^{(0)}\\mu_{3}}{\\partial t}\\right|=\\widetilde{O}\\left(\\sigma_{1}^{2}\\sqrt{m m_{1}}+\\sigma_{0}^{2}\\sqrt{m L}+(\\sigma_{0}^{2}+\\sigma_{1}^{2})m F\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. Following from the proof of Lemma E.4, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\frac{\\partial a_{j_{1}}w_{j_{1}}^{(0)}W_{V}^{(0)}\\mu_{3}}{\\partial t}=\\frac{1}{n}\\sum_{i;\\mu_{3}\\in X^{(i)}}g_{i}^{(0)}y_{i}\\sum_{l=1}^{L}\\left(\\|w_{j_{1}}^{(0)}\\|_{2}^{2}+\\|v^{(0)}(\\mu_{1})\\|_{2}^{2}\\right)p_{q\\leftarrow l,k\\leftarrow\\mu_{3}}^{(0,i)}}\\\\ {\\displaystyle}&{\\displaystyle+\\,a_{j_{1}}\\,\\frac{1}{n}\\sum_{i:\\mu_{3}\\in X^{(i)}}g_{i}^{(0)}y_{i}\\sum_{l=1}^{L}\\sum_{j_{2}\\neq j_{1}}a_{j_{2}}\\left<w_{j_{1}}^{(0)},w_{j_{2}}^{(0)}\\right>p_{q\\leftarrow l,k\\leftarrow\\mu_{3}}^{(0,i)}}\\\\ &{\\displaystyle}&{\\displaystyle+\\,a_{j_{1}}\\,\\frac{1}{n}\\sum_{i=1}^{n}g_{i}^{(0)}y_{i}\\sum_{l=1}^{L}a_{j_{1}}\\sum_{l_{2}=1}^{L}\\left<p_{l_{1},l_{2}}^{(0,i)}v_{l_{2}}^{(0,i)},v^{(0)}(\\mu_{3})\\right>\\mathbb{I}(v_{l_{2}}^{(0,i)}\\neq v^{(0)}(\\mu_{3}))\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "For the first term, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{n}\\sum_{\\substack{i\\neq n\\leq K}}g_{i}^{(0)}y_{i}\\frac{L}{l-1}\\left(\\|w_{j\\parallel}^{(0)}\\|_{2}^{2}+\\|v^{(0)}(\\mu_{3})\\|_{2}^{2}\\right)p_{q\\leftarrow l,k\\leftarrow\\mu_{3}}^{(0,i)}\\right.}\\\\ &{=\\displaystyle\\left|\\frac{1}{n}\\sum_{y_{i}=1}^{\\infty}g_{i}^{(0)}\\sum_{l=1}^{L}\\left(\\|w_{j\\parallel}^{(0)}\\|_{2}^{2}+\\|v^{(0)}(\\mu_{3})\\|_{2}^{2}\\right)p_{q\\leftarrow l,k\\leftarrow\\mu_{3}}^{(0,i)}\\right.}\\\\ &{\\quad\\left.-\\displaystyle\\frac{1}{n}\\sum_{y_{i}=-1}^{\\infty}g_{i}^{(0)}\\sum_{l=1}^{L}\\left(\\|w_{j\\parallel}^{(0)}\\|_{2}^{2}+\\|v^{(0)}(\\mu_{3})\\|_{2}^{2}\\right)p_{q\\leftarrow l,k\\leftarrow\\mu_{3}}^{(0,i)}\\right|}\\\\ &{=\\displaystyle\\frac{1}{n}\\left(\\|w_{j\\parallel}^{(0)}\\|_{2}^{2}+\\|v^{(0)}(\\mu_{3})\\|_{2}^{2}\\right)\\left|\\underset{y=l}{\\sum_{l=1}^{\\infty}g_{i}^{(0)}\\sum_{l=1}^{L}p_{q\\leftarrow l,k\\leftarrow\\mu_{3}}^{(0,i)}}-\\sum_{y_{i}=-1}^{L}g_{i}^{(0)}\\sum_{l=1}^{L}p_{q\\leftarrow l,k\\leftarrow\\mu_{3}}^{(0,i)}\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "By Lemma D.8, and Corollary D.2, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac1n\\left|\\displaystyle\\sum_{y_{i}=1}\\boldsymbol g_{i}^{(0)}\\displaystyle\\sum_{l=1}^{L}p_{q\\leftarrow l,k\\leftarrow\\mu_{3}}^{(0,i)}-\\displaystyle\\sum_{y_{i}=-1}\\boldsymbol g_{i}^{(0)}\\displaystyle\\sum_{l=1}^{L}p_{q\\leftarrow l,k\\leftarrow\\mu_{3}}^{(0,i)}\\right|}\\\\ &{\\leq\\displaystyle\\frac1n\\left(\\displaystyle\\sum_{y_{i}=1}\\left(\\frac12+F\\right)\\displaystyle\\sum_{l=1}^{L}\\left(\\frac1L+\\frac1{L^{2}}\\right)-\\displaystyle\\sum_{y_{i}=-1}^{L}\\left(\\frac12-F\\right)\\displaystyle\\sum_{l=1}^{L}\\left(\\frac1L-\\frac1L\\right)\\right)}\\\\ &{\\leq\\left(\\frac12+F\\right)\\left(\\frac1L+\\frac1{L m}\\right)\\frac L2-\\left(\\frac12-F\\right)\\left(\\frac1L-\\frac1{L m}\\right)\\frac L2}\\\\ &{\\leq2F+O(1/m).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "By Lemma D.3, we have $\\lVert w_{j_{1}}^{(0)}\\rVert_{2}^{2}=\\Theta(\\sigma_{1}^{2}m)$ and $\\|v^{(0)}(\\mu_{3})\\|_{2}^{2}=\\Theta(\\sigma_{0}^{2}m)$ , and thus, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{n}\\sum_{i:\\mu_{3}\\in X^{(i)}}g_{i}^{(0)}y_{i}\\sum_{l=1}^{L}a_{j_{1}}\\left(\\|w_{j_{1}}^{(0)}\\|_{2}^{2}+\\|v^{(0)}(\\mu_{3})\\|_{2}^{2}\\right)p_{q\\leftarrow l,k\\leftarrow\\mu_{3}}^{(0,i)}\\right|\\leq\\widetilde{O}(\\alpha(\\sigma_{0}^{2}+\\sigma_{1}^{2})m F).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Further, by Proposition E.5, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\left|\\varepsilon_{1}\\right|=\\left|\\frac{1}{n}\\sum_{i:\\mu_{3}\\in X^{(i)}}g_{i}^{(0)}y_{i}\\sum_{l=1}^{L}\\sum_{j_{2}\\neq j_{1}}a_{j_{2}}\\left\\langle w_{j_{1}}^{(0)},w_{j_{2}}^{(0)}\\right\\rangle p_{q\\leftarrow l,k\\leftarrow\\mu_{3}}^{(0,i)}\\right|\n$$", "text_format": "latex", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\le\\sigma_{1}^{2}\\sqrt{m_{1}}m\\left(\\sqrt{\\displaystyle\\frac{4}{m}\\log\\displaystyle\\frac{2m_{1}^{2}}{\\delta}}+\\displaystyle\\frac{4}{m}\\log\\displaystyle\\frac{2m_{1}^{2}}{\\delta}\\right)\\sqrt{\\log\\displaystyle\\frac{m_{1}}{\\delta}}.}\\\\ &{\\left|\\varepsilon_{2}\\right|=\\left|\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}g_{i}^{(0)}y_{i}\\sum_{l_{1}=1}^{L}a_{j_{1}}\\sum_{l_{2}=1}^{L}\\left\\langle p_{l_{1},l_{2}}^{(0,i)}v_{l_{2}}^{(0,i)},v^{(0)}(\\mu_{3})\\right\\rangle\\mathbb{I}(v_{l_{2}}^{(0,i)}\\ne v^{(0)}(\\mu_{3}))\\right|}\\\\ &{\\quad\\le\\sigma_{0}^{2}m\\sqrt{L}\\left(\\sqrt{\\displaystyle\\frac{4}{m}\\log\\displaystyle\\frac{2n L}{\\delta}}+\\displaystyle\\frac{4}{m}\\log\\displaystyle\\frac{2n L}{\\delta}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Finally, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\left|\\frac{\\partial a_{j_{1}}w_{j_{1}}^{(0)}W_{V}^{(0)}\\mu_{3}}{\\partial t}\\right|\\leq\\widetilde{O}\\left(\\sigma_{1}^{2}\\sqrt{m m_{1}}+\\sigma_{0}^{2}\\sqrt{m L}+(\\sigma_{0}^{2}+\\sigma_{1}^{2})m F\\right).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Lemma E.17 (Per neuron gradient of common token). For $t\\leq T_{1}$ ,wehave ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\left|\\frac{\\partial a_{j}w_{j}^{(0)}W_{V}^{(0)}\\mu_{3}}{\\partial t}\\right|\\leq\\widetilde{O}(\\sigma_{1}^{2}m)\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. The proof is similar to that for Theorem E.14 and we omit it here. ", "page_idx": 32}, {"type": "text", "text": "Definition E.18 (sub-network). Define the sub-network structure as ", "page_idx": 32}, {"type": "equation", "text": "$$\nG(\\mu)=\\sum_{j=1}^{m_{1}}a_{j}w_{j}W_{V}\\mu.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We can compute the gradient of the sub-network as ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial G(\\mu)}{\\partial t}=\\sum_{j=1}^{m_{1}}\\frac{\\partial a_{j}w_{j}^{(t)}W_{V}^{(t)}\\mu}{\\partial t}=\\sum_{j=1}^{m_{1}}a_{j}w_{j}^{(t)}\\frac{\\partial W_{V}^{(t)}\\mu}{\\partial t}+a_{j}\\frac{\\partial w_{j}^{(t)}}{\\partial t}W_{V}^{(t)}\\mu}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad=\\frac{1}{n}\\sum_{i_{2}:\\;\\mu\\in X^{(i_{2})}}g_{i_{2}}^{(t)}y_{i_{2}}\\sum_{l_{2}=1}^{L}\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}a_{j_{1}}a_{j_{2}}\\left\\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\\right\\rangle p_{q\\leftarrow l_{2},k\\leftarrow\\mu}^{(t,i_{2})}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\frac{1}{n}\\sum_{i_{1}=1}^{n}g_{i_{1}}^{(t)}y_{i_{1}}m_{1}\\sum_{l_{1}=1}^{L}\\sum_{l_{2}=1}^{L}\\left\\langle p_{l_{1},l_{2}}^{(t,i_{1})}v_{l_{2}}^{(t,i_{1})},v^{(t)}(\\mu)\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Theorem E.19 (Complete version of Lemma 4.3). There exists a $T_{0.5}\\leq T_{1}$ andaconstant $0<C<1$ suchthatforall $T_{0.5}\\leq t\\leq T_{1}$ ", "page_idx": 32}, {"type": "equation", "text": "$$\n1+C)\\operatorname*{max}\\left(\\frac{\\partial G^{(t)}(\\mu_{1})}{\\partial t},\\frac{\\partial G^{(t)}(\\mu_{2})}{\\partial t}\\right)\\leq-\\frac{\\partial G^{(t)}(\\mu_{3})}{\\partial t}\\leq(1-C)\\left(\\frac{\\partial G^{(t)}(\\mu_{1})}{\\partial t}+\\frac{\\partial G^{(t)}(\\mu_{2})}{\\partial t}\\right).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Further, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{\\partial G^{(t)}(\\mu_{1})}{\\partial t},\\frac{\\partial G^{(t)}(\\mu_{2})}{\\partial t}\\geq\\Omega((\\sigma_{0}^{2}+\\sigma_{1}^{2})m m_{1}).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. First of all, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{\\partial F_{i}^{(t)}}{\\partial t}=\\sum_{l_{1}=1}^{L}\\sum_{j=1}^{m_{1}}\\sum_{l_{2}=1}^{L}\\left(\\frac{\\partial a_{j}w_{j}^{(t)}W_{V}^{(t)}X_{l_{2}}}{\\partial t}p_{l_{1},l_{2}}^{(t,i)}+a_{j}w_{j}^{(t)}W_{V}^{(t)}X_{l_{2}}\\frac{\\partial p_{l_{1},l_{2}}^{(t,i)}}{\\partial t}\\right).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "By Lemma E.4 and Lemma E.16, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{\\partial a_{j}w_{j}^{(0)}W_{V}^{(0)}\\mu_{1}}{\\partial t}-\\frac{\\partial a_{j}w_{j}^{(0)}W_{V}^{(0)}\\mu_{3}}{\\partial t}=\\Theta((\\sigma_{0}^{2}+\\sigma_{1}^{2})m),\n$$", "text_format": "latex", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{\\partial a_{j}w_{j}^{(0)}W_{V}^{(0)}\\mu_{2}}{\\partial t}-\\frac{\\partial a_{j}w_{j}^{(0)}W_{V}^{(0)}\\mu_{3}}{\\partial t}=\\Theta((\\sigma_{0}^{2}+\\sigma_{1}^{2})m).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "By Theorem E.15 and Corollary E.24, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{l_{1}=1}^{L}\\sum_{j=1}^{m_{1}}\\sum_{l_{2}=1}^{L}\\left(\\frac{\\partial a_{j}w_{j}^{(t)}W_{V}^{(t)}X_{l_{2}}}{\\partial t}\\mathbb{I}(X_{l_{2}}\\notin\\{\\mu_{i}\\}_{i=1}^{3})p_{l_{1},l_{2}}^{(t,i)}+a_{j}w_{j}^{(t)}W_{V}^{(t)}X_{l_{2}}\\frac{\\partial p_{l_{1},l_{2}}^{(t,i)}}{\\partial t}\\right)}\\\\ &{\\quad\\quad=O\\left(\\frac{L}{n}(\\sigma_{0}^{2}+\\sigma_{1}^{2})m m_{1}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Thus, for all $i\\in I_{1}\\cup I_{2}\\cup I_{3}$ $\\frac{\\partial F_{i}^{(0)}}{\\partial t}\\,>\\,0$ ,which implies $\\frac{\\partial g_{i}^{(0)}}{\\partial t}\\,<\\,0$ for $i\\in I_{1}$ and $\\frac{\\partial g_{i}^{(0)}}{\\partial t}\\,>\\,0$ for $i\\in I_{2}\\cup I_{3}$ ", "page_idx": 33}, {"type": "text", "text": "$\\begin{array}{r}{-\\frac{\\partial G^{(t)}(\\mu_{3})}{\\partial t}\\,=\\,\\operatorname*{max}(\\frac{\\partial G^{(t)}(\\mu_{1})}{\\partial t},\\frac{\\partial G^{(t)}(\\mu_{2}}{\\partial t}}\\end{array}$ $\\frac{\\partial G^{(t)}(\\mu_{1})}{\\partial t}\\,>\\,\\frac{\\partial G^{(t)}(\\mu_{2})}{\\partial t}$ magnitude of the update of the common token is less than the magnitude of the update of the signals. By Lemma C.1, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial}{\\partial t}\\displaystyle\\sum_{i=1}^{\\infty}\\partial_{i}\\overline{{u}}^{\\prime}\\overline{{b}}^{i}\\mu^{i}\\gamma_{i}\\geq-\\frac{\\gamma_{1}}{\\gamma_{2}}\\displaystyle\\sum_{i=1}^{\\infty}\\partial_{i}\\overline{{u}}^{\\prime}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i}\\overline{{b}}^{i \n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "$m$ $m_{1}$ $C$ $\\frac{\\partial G^{(t)}(\\mu_{1})}{\\partial t}\\geq$ $\\frac{\\partial G^{(t)}(\\mu_{3})}{\\partial t}$ $\\begin{array}{r}{\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{2}}g_{i}^{(t)}=\\Omega(1)}\\end{array}$ $\\begin{array}{r}{\\sum_{i\\in I_{3}\\cup I_{4}}g_{i}^{(t)}=\\Omega(1)}\\end{array}$ for $t\\leq T_{1}$ Prpsilal $\\begin{array}{r}{\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{3}}g_{i}^{(t)}\\geq\\Omega(1)}\\end{array}$ Thus, if $\\begin{array}{r}{\\frac{\\partial G^{(t)}(\\mu_{1})}{\\partial t}\\;\\geq\\;-\\frac{\\partial G^{(t)}(\\mu_{3})}{\\partial t}}\\end{array}$ aG((\u03bca), then ie $\\begin{array}{r}{\\frac{1}{n}\\sum_{i\\in I_{1}}\\frac{\\partial g_{i}^{(t)}}{\\partial t}\\,=\\,-\\Theta((\\sigma_{0}^{2}+\\bar{\\sigma_{1}^{2}})m m_{1})}\\end{array}$ . Therefore, there must exist a time t = @(1/m) such that aG()(\u03bc) $\\begin{array}{r}{\\frac{\\partial G^{(t^{\\prime})}(\\mu_{1})}{\\partial t}\\,=\\,-\\frac{\\partial G^{(t^{\\prime})}(\\mu_{3})}{\\partial t}}\\end{array}$ aG(t)(\u03bcua). Further, for t \u2265 (F/m), we have yi F(t) $y_{i}F_{i}^{(t)}\\,>\\,0$ > O for alli E I4 and Zie14 9 $\\begin{array}{r}{\\sum_{i\\in I_{4}}g_{i}^{(t)}\\;<\\;\\operatorname*{min}(\\sum_{i\\in I_{2}}g_{i}^{(t)},\\sum_{i\\in I_{3}}g_{i}^{(t)})}\\end{array}$ , where ${\\cal F}=$ $\\operatorname*{max}_{i\\in[n]}|F_{i}^{(0)}|$ ", "page_idx": 33}, {"type": "text", "text": "Now, consider a time point $t^{\\prime}$ when $\\begin{array}{r l r}{\\frac{\\partial G^{(t^{\\prime})}(\\mu_{1})}{\\partial t}}&{=}&{-\\frac{\\partial G^{(t^{\\prime})}(\\mu_{3})}{\\partial t}}\\end{array}$ At $t^{\\prime}$ \uff0c we must have $\\begin{array}{r}{\\operatorname*{min}(\\sum_{i\\in I_{2}}g_{i}^{(t^{\\prime})},\\sum_{i\\in I_{3}}g_{i}^{(t^{\\prime})})-\\sum_{i\\in I_{4}}g_{i}^{(t^{\\prime})}=\\stackrel{\\smile}{\\Omega}(1)}\\end{array}$ (t') = \u03a9(1). Thus, ", "page_idx": 34}, {"type": "equation", "text": "$$\n2\\sum_{i\\in I_{1}}g_{i}^{(t^{\\prime})}-2\\sum_{i\\in I_{4}}g_{i}^{(t^{\\prime})}=\\Omega(1),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "which implies ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial F}2\\sum_{i\\in I_{1}}g_{i}^{(t^{\\prime})}-\\frac{\\partial}{\\partial F}\\sum_{i\\in I_{4}}g_{i}^{(t^{\\prime})}=\\Omega(1).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "For $i\\in I_{1}$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial F_{i}^{(t^{\\prime})}}{\\partial t}=\\frac{\\partial G^{(t^{\\prime})}(\\mu_{1})}{\\partial t}+\\frac{\\partial G^{(t^{\\prime})}(\\mu_{2})}{\\partial t}+\\frac{\\partial G^{(t^{\\prime})}(\\mu_{3})}{\\partial t}+O\\left(\\frac{L}{n}\\sigma_{1}^{2}m m_{1}\\right)}\\\\ &{\\quad\\quad\\quad=\\frac{\\partial G^{(t^{\\prime})}(\\mu_{2})}{\\partial t}+O\\left(\\frac{L}{n}\\sigma_{1}^{2}m m_{1}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "For $i\\in I_{2}$ \uff0c", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{\\partial F_{i}^{(t)}}{\\partial t}=\\frac{\\partial G^{(t^{\\prime})}(\\mu_{1})}{\\partial t}+\\frac{\\partial G^{(t^{\\prime})}(\\mu_{3})}{\\partial t}+O\\left(\\frac{L}{n}\\sigma_{1}^{2}m m_{1}\\right)=O\\left(\\frac{L}{n}\\sigma_{1}^{2}m m_{1}\\right).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "For $i\\in I_{3}$ , by Proposition E.21, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{\\partial F_{i}^{(t)}}{\\partial t}=\\frac{\\partial G^{(t^{\\prime})}(\\mu_{2})}{\\partial t}+\\frac{\\partial G^{(t^{\\prime})}(\\mu_{3})}{\\partial t}+O\\left(\\frac{L}{n}\\sigma_{1}^{2}m m_{1}\\right)=O\\left(F\\sigma_{1}^{2}m m_{1}\\right),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and for $i\\in I_{4}$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{\\partial F_{i}^{(t)}}{\\partial t}=\\frac{\\partial G^{(t^{\\prime})}(\\mu_{3})}{\\partial t}+O\\left(\\frac{L}{n}\\sigma_{1}^{2}m m_{1}\\right).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Thus, by chain rule $\\begin{array}{r}{\\frac{\\partial g_{i}^{(t)}}{\\partial t}=\\frac{\\partial g_{i}}{\\partial F_{i}}\\frac{\\partial F_{i}^{(t)}}{\\partial t}}\\end{array}$ ot, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial t}\\left(2\\sum_{i\\in I_{1}}g_{i}^{(t^{\\prime})}-2\\sum_{i\\in I_{2}}g_{i}^{(t^{\\prime})}-\\sum_{i\\in I_{3}\\cup I_{4}}g_{i}^{(t^{\\prime})}\\right)=-\\Theta(\\sigma_{1}^{2}m m_{1}).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "This implies that there exists a constant $L$ such that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial t}\\left(\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{2}}g_{i}^{(t)}+(1+L)\\left(\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{2}\\cup I_{3}\\cup I_{4}}g_{i}^{(t)}\\right)\\right)=-\\Theta(\\sigma_{1}^{2}m m_{1}),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "which implies that there must exist a time $T_{0.5}\\leq O(1/m)$ such that for all $t\\geq T_{0.5}$ \uff0c ", "page_idx": 34}, {"type": "equation", "text": "$$\n-\\frac{\\partial G^{(t)}(\\mu_{3})}{\\partial t}\\geq(1+L)\\frac{\\partial G^{(t)}(\\mu_{1})}{\\partial t}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Moreover, by Theorem E.26, if ${\\cal C}_{T_{1}}$ is sufficiently large, we have $T_{0.5}<T_{1}$ ", "page_idx": 34}, {"type": "text", "text": "Finally, consider the time when $\\begin{array}{r}{\\frac{\\partial G^{(t)}(\\mu_{1})}{\\partial t}+\\frac{\\partial G^{(t)}(\\mu_{2})}{\\partial t}>-\\frac{\\partial G^{(t)}(\\mu_{3})}{\\partial t}}\\end{array}$ G(\u03bca) During this time, note that we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{\\partial G^{(t)}(\\mu_{1})}{\\partial t},\\frac{\\partial G^{(t)}(\\mu_{2})}{\\partial t}\\geq\\Theta(\\sigma_{1}^{2}m m_{1}).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "$\\begin{array}{r}{\\frac{\\partial G^{(t)}(\\mu_{1})}{\\partial t}+\\frac{\\partial G^{(t)}(\\mu_{2})}{\\partial t}=-\\frac{\\partial G^{(t)}(\\mu_{3})}{\\partial t}}\\end{array}$ $\\begin{array}{r}{\\frac{\\partial}{\\partial t}g_{i}^{(t)}=-\\Theta(\\sigma_{1}^{2}m m_{1})}\\end{array}$ for al $i\\in I_{2}\\cup I_{3}\\cup I_{4}$ Thus, there must exist a constant $U$ such that ", "page_idx": 34}, {"type": "equation", "text": "$$\n-\\frac{\\partial G^{(t)}(\\mu_{3})}{\\partial t}\\leq(1-U)\\left(\\frac{\\partial G^{(t)}(\\mu_{1})}{\\partial t}+\\frac{\\partial G^{(t)}(\\mu_{2})}{\\partial t}\\right)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "for all $t\\leq T_{1}$ ", "page_idx": 34}, {"type": "text", "text": "Corollary E.20. There exists a small positive constant $C$ such that if we define $T^{\\prime}:=\\,\\operatorname*{min}\\{t:$ $\\operatorname*{min}_{i\\in I_{2}\\cup I_{3}}F_{i}^{(t)}\\geq C\\}$ then $T^{\\prime}\\leq O(1/m)$ ", "page_idx": 35}, {"type": "text", "text": "Proposition E.21. Let $F=\\operatorname*{max}_{i\\in[n]}|F_{i}^{(0)}|$ For $t\\leq T_{1}$ we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{n}\\sum_{i\\in I_{2}}g_{i}^{(t)}-\\frac{1}{n}\\sum_{i\\in I_{3}}g_{i}^{(t)}\\right|\\leq O\\left(F\\right).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. First of all, by Lipschitzness of $g$ ,we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{i\\in I_{2}}g_{i}^{(t)}-\\frac{1}{n}\\sum_{i\\in I_{3}}g_{i}^{(t)}\\bigg|\\leq|G^{(t)}(\\mu_{1})-G^{(0)}(\\mu_{1})-(G^{(t)}(\\mu_{2})-G^{(0)}(\\mu_{2}))|+2\\operatorname*{max}_{i\\in[n]}\\left|F_{i}^{(0)}\\right|+o(1)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Without loss of generality, assume $G^{(t)}(\\mu_{1})>G^{(t)}(\\mu_{2})$ for all $t\\leq T_{1}$ ; otherwise, we can break the interval $[0,T_{1}]$ into sub-intervals by the time points when $G^{(t)}(\\mu_{1})-G^{(t)}(\\mu_{2})$ changes its sign and then apply the analysis below to each sub-interval. We first derive ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\frac{1}{n}\\sum_{i,j\\in\\mathcal{N}_{D}}\\omega_{i,j}^{2}\\sum_{m,n,m^{\\prime},n^{\\prime},n^{\\prime}}^{\\infty}\\omega_{i,n^{\\prime}}(v^{\\dagger},v^{\\dagger},w^{\\dagger})_{\\mathcal{N}_{D},n^{\\prime}}\\sum_{p,m^{\\prime},n^{\\prime},n^{\\prime}}^{\\infty}-\\frac{1}{n}\\sum_{i,j\\in\\mathcal{N}_{D}}\\omega_{i,n^{\\prime}}^{2}\\sum_{p,m^{\\prime},n^{\\prime},n^{\\prime}}^{\\infty}\\omega_{j,n^{\\prime}}^{2}}\\\\ &{-\\frac{1}{n}\\sum_{i,j\\in\\mathcal{N}_{D}}\\omega_{i,n^{\\prime}}^{2}\\sum_{w^{\\prime},n,m^{\\prime},n^{\\prime}}^{\\infty}\\left(\\frac{1}{n^{\\prime}\\omega_{i,n^{\\prime}}}\\sum_{w^{\\prime},n^{\\prime},m^{\\prime},n^{\\prime},n^{\\prime},n^{\\prime},n^{\\prime}}^{2}\\right.}\\\\ &{\\left.-\\frac{1}{n^{\\prime}\\omega_{i,n^{\\prime}}}\\sum_{w^{\\prime},n,m^{\\prime},n^{\\prime},n^{\\prime},n^{\\prime}}^{2}\\right)\\left(\\frac{1}{n^{\\prime}\\omega_{i,n^{\\prime}}}\\sum_{w^{\\prime},n^{\\prime},m^{\\prime},n^{\\prime},n^{\\prime},n^{\\prime},n^{\\prime}}^{2}-\\frac{1}{n^{\\prime}\\omega_{i,n^{\\prime}}}\\sum_{w^{\\prime},n^{\\prime},n^{\\prime},n^{\\prime},n^{\\prime}}^{2}\\right)}\\\\ &{+\\frac{1}{n^{\\prime}}\\sum_{i,j\\in\\mathcal{N}_{D}}\\omega_{i,n^{\\prime}}^{2}\\left(v^{\\dagger},v^{\\dagger},w^{\\dagger}\\right)_{\\mathcal{N}_{D},n^{\\prime}}\\sum_{w^{\\prime},n^{\\prime},n^{\\prime},n^{\\prime}}^{\\infty}\\sum_{w^{\\prime},n^{\\prime},n^{\\prime}}^{\\infty}\\eta_{\\ast,n^{\\prime}}^{2}\\sum_{w^{\\prime},n^{\\prime},n^{\\prime},n^{\\prime}}^{\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "By Theorem E.14, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{n_{1}\\displaystyle\\int_{0}^{T}\\frac{1}{n}\\sum_{i_{1}=1}^{n}g_{i_{1}}^{(t)}y_{i_{1}}\\sum_{l_{1}=1}^{L}a_{j_{1}}p_{l_{1}}^{(t,i_{1})\\top}V^{(t,i_{1})\\top}W_{V}^{(t)}\\mu_{1}-\\displaystyle\\frac{1}{n}\\sum_{i_{1}=1}^{n}g_{i_{1}}^{(t)}y_{i_{1}}\\sum_{l_{1}=1}^{L}a_{j_{1}}p_{l_{1}}^{(t,i_{1})\\top}V^{(t,i_{1})\\top}W_{V}^{(t)}\\mu_{1}}\\\\ &{\\leq T\\cdot\\tilde{O}(L\\sigma_{0}^{2}\\sqrt{m}m_{1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\int_{0}^{T}\\left(G^{(0)}(\\mu_{1})+G^{(0)}(\\mu_{2})+O\\left(\\operatorname*{max}_{i\\in[n]}|F_{i}^{(0)}|\\right)+o(1)\\right)\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}a_{j_{1}}a_{j_{2}}\\left\\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\\right\\rangle\\,d t}\\\\ {\\displaystyle\\leq O\\left(\\left(G^{(0)}(\\mu_{1})+G^{(0)}(\\mu_{2})+\\operatorname*{max}_{i\\in[n]}|F_{i}^{(0)}|\\right)T\\sigma_{1}^{2}m m_{1}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "and ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\exp\\left(\\int_{0}^{T}\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}a_{j_{1}}a_{j_{2}}\\left\\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\\right\\rangle\\,d t\\right)=O(1).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "By Gronwall's inequality, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{G^{(T)}(\\mu_{1})-G^{(0)}(\\mu_{1})-(G^{(T)}(\\mu_{2})-G^{(0)}(\\mu_{2}))}\\\\ &{\\le T\\cdot\\widetilde{O}(L\\sigma_{0}^{2}\\sqrt{m}m_{1})+O\\left(\\left(G^{(0)}(\\mu_{1})+G^{(0)}(\\mu_{2})+\\underset{i\\in[n]}{\\operatorname*{max}}|F_{i}^{(0)}|\\right)T\\sigma_{1}^{2}m m_{1}\\right)}\\\\ &{\\quad+\\displaystyle\\int_{0}^{T}\\left(t\\cdot\\widetilde{O}(L\\sigma_{0}^{2}\\sqrt{m})+O\\left(\\left(G^{(0)}(\\mu_{1})+G^{(0)}(\\mu_{2})+\\underset{i\\in[n]}{\\operatorname*{max}}|F_{i}^{(0)}|\\right)t\\sigma_{1}^{2}m\\right)\\right)\\cdot O(1)\\,d t}\\\\ &{\\le O\\left(G^{(0)}(\\mu_{1})+G^{(0)}(\\mu_{2})+\\underset{i\\in[n]}{\\operatorname*{max}}|F_{i}^{(0)}|\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "This implies that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{n}\\sum_{i\\in I_{2}}g_{i}^{(t)}-\\frac{1}{n}\\sum_{i\\in I_{3}}g_{i}^{(t)}\\right|\\leq O\\left(G^{(0)}(\\mu_{1})+G^{(0)}(\\mu_{2})+\\operatorname*{max}_{i\\in[n]}|F_{i}^{(0)}|\\right).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Thstasy ${\\textstyle\\frac{1}{n}}\\sum_{i\\in I_{2}}g_{i}^{(t)}$ and ${\\scriptstyle{\\frac{1}{n}}}\\sum_{i\\in I_{3}}g_{i}^{(t)}$ ar onlydiffered by a small constant. \u53e3 ", "page_idx": 36}, {"type": "text", "text": "E.7Small Score Movement in Phase 1 ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Proposition E.22. Conditioned on the success of Lemma $D.I$ and Lemma $D.3$ with probability at least $1-\\delta$ over the randomness of $a,$ for all $i\\in[n]$ and $\\mu,\\nu\\in X^{(i)}$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\left|\\sum_{j=1}^{m_{1}}a_{j}\\|k^{(0)}(\\mu)\\|_{2}^{2}w_{j}^{(0)\\top}v^{(0)}(\\nu)\\right|\\leq O\\left(\\sigma_{0}^{2}m\\sigma_{1}\\sigma_{0}\\sqrt{m m_{1}\\log\\frac{m_{1}d}{\\delta}}\\sqrt{\\log\\frac{n d}{\\delta}}\\right),}\\\\ &{\\displaystyle\\left|\\sum_{j=1}^{m_{1}}a_{j}\\|q^{(0)}(\\mu)\\|_{2}^{2}w_{j}^{(0)\\top}v^{(0)}(\\nu)\\right|\\leq O\\left(\\sigma_{0}^{2}m\\sigma_{1}\\sigma_{0}\\sqrt{m m_{1}\\log\\frac{m_{1}d}{\\delta}}\\sqrt{\\log\\frac{n d}{\\delta}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "and for all $l,l^{\\prime}\\in[L]$ with $K_{l}^{(0,i)}\\neq k^{(0)}(\\nu)$ and $q_{l}^{(0,i)}\\neq Q^{(0)}(\\nu),$ we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{j=1}^{m_{1}}a_{j}\\nu^{\\top}W_{K}^{(0)\\top}K_{l}^{(0,i)}V_{l^{\\prime}}^{(0,i)\\top}w_{j}^{(0)}\\Bigg|\\leq\\tilde{O}(\\sigma_{0}^{2}\\sqrt{m}\\sigma_{0}\\sigma_{1}\\sqrt{m m_{1}}),}\\\\ &{\\displaystyle\\sum_{j=1}^{m_{1}}a_{j}\\nu^{\\top}W_{Q}^{(0)\\top}q_{l}^{(0,i)}V_{l^{\\prime}}^{(0,i)\\top}w_{j}^{(0)}\\Bigg|\\leq\\tilde{O}(\\sigma_{0}^{2}\\sqrt{m}\\sigma_{0}\\sigma_{1}\\sqrt{m m_{1}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. Fix $i\\in[n]$ and $\\mu,\\nu\\in X^{(i)}$ . Consider the randomness of $a$ . By Lemma D.1, Corollary D.2 and Lemma D.3, $\\mathring{a}_{j}\\|k^{(0)}(\\mu)\\|_{2}^{2}w_{j}^{(0)\\top}v^{(0)}(\\nu)$ is asub-Gaussiananariable withvaiae proy $O((\\sigma_{0}^{2}m\\sigma_{1}\\sigma_{0}\\sqrt{m}\\sqrt{\\log(d m_{1}/\\delta)})^{2})$ . Then the following inequality holds. ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\left|\\sum_{j=1}^{m_{1}}a_{j}\\|k^{(0)}(\\mu)\\|_{2}^{2}w_{j}^{(0)\\top}v^{(0)}(\\nu)\\right|\\leq O\\left(\\sigma_{0}^{2}m\\sigma_{1}\\sigma_{0}\\sqrt{m m_{1}\\log\\frac{m_{1}d}{\\delta}}\\sqrt{\\log\\frac{2}{\\delta}}\\right).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Finally, take a union bound over $i\\in[n]$ \uff0c $\\mu,\\nu\\in\\{\\mu_{i}\\}_{i=1}^{d}$ . The analysis for the second term is similar. Noext noe thaaT(0.@0T( is a sub-Gaussian random variable with variance proxy $\\widetilde{\\cal O}((\\sigma_{0}^{2}\\sqrt{m}\\sigma_{0}\\sigma_{1}\\sqrt{m})^{2})$ . Thus, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\left|\\sum_{j=1}^{m_{1}}a_{j}\\nu^{\\top}W_{K}^{(0)\\top}K_{l}^{(0,i)}V_{l^{\\prime}}^{(0,i)\\top}w_{j}^{(0)}\\right|\\leq\\widetilde{O}(\\sigma_{0}^{2}\\sqrt{m}\\sigma_{0}\\sigma_{1}\\sqrt{m m_{1}}).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Lemma E.23 (Score change). For al $t\\leq T_{1}$ for all $\\nu,\\mu\\in\\{\\mu_{i}\\}_{i=1}^{d}$ we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\left|\\frac{\\partial\\nu^{\\top}W_{K}^{(t)\\top}W_{Q}^{(t)}\\mu}{\\partial t}\\right|\\leq\\frac{1}{\\sqrt{m}}\\frac{\\left|\\left\\{i:\\,\\mu\\in X^{(i)}\\right\\}\\right|+\\left|\\left\\{i:\\,\\nu\\in X^{(i)}\\right\\}\\right|}{n}\\widetilde{O}\\left(\\frac{1}{L}+\\frac{1}{\\sqrt{m}}\\right)\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and thus, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\nu^{\\top}W_{K}^{(t)\\top}W_{Q}^{(t)}\\mu-\\nu^{\\top}W_{K}^{(0)\\top}W_{Q}^{(0)}\\mu\\Big|\\leq t\\frac{1}{\\sqrt{m}}\\frac{\\left|\\left\\{i:\\mu\\in X^{(i)}\\right\\}\\right|+\\left|\\left\\{i:\\nu\\in X^{(i)}\\right\\}\\right|}{n}\\tilde{O}\\left(\\frac{1}{L}+\\frac{1}{\\sqrt{m}}\\right)\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. First of all, by Lemma C.4, we expand the per step gradient descent update as follows: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial\\nu^{\\top}W_{K}^{(t)^{\\top}}W_{Q}^{(t)}\\mu}{\\partial t}}\\\\ &{=\\frac{1}{n\\sqrt{m}}\\displaystyle\\sum_{i:\\mu,\\nu\\in X^{(i)}}g_{i}^{(t)}y_{i}\\displaystyle\\sum_{j=1}^{m_{1}}a_{j}\\|k^{(t)}(\\nu)\\|_{2}^{2}\\left(v^{(t)^{\\top}}(\\nu)w_{j}^{(t)}-w_{j}^{(t)^{\\top}}V^{(t,i)}p_{l(i,\\mu)}^{(t,i)}\\right)p_{q\\leftarrow\\mu,k\\leftarrow\\nu}^{(t,i)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "equation", "text": "$$\n-\\,\\frac{1}{n\\sqrt{m}}\\sum_{i;\\mu\\in X^{(i)}}g_{i}^{(t)}y_{i}\\sum_{j=1}^{m_{1}}a_{j}\\sum_{l=1}^{L}\\nu^{\\top}W_{K}^{(t)\\top}K_{l}^{(t,i)}\\left(V_{l}^{(t,i)\\top}w_{j}^{(t)}-w_{j}^{(t)\\top}V^{(t,i)}p_{l(i,\\mu)}^{(t,i)}\\right)p_{q\\leftarrow\\mu,k\\leftarrow l}^{(t,i)}\\mathbb{I}(K_{l}^{(t,i)\\top}w_{i}^{(t)})~.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "equation", "text": "$$\n+\\left.\\frac{1}{n\\sqrt{m}}\\sum_{\\substack{i:\\nu,\\mu\\in X^{(i)}}}g_{i}^{(t)}y_{i}\\sum_{j=1}^{m_{1}}a_{j}\\Vert q^{(t)}(\\mu)\\Vert_{2}^{2}p_{q\\leftarrow\\mu,k\\leftarrow\\nu}^{(t,i)}\\left(w_{j}^{(t)\\top}v^{(t,i)}(\\nu)-w_{j}^{(t)\\top}V^{(t,i)}p_{l(i,\\mu)}^{(t,i)}\\right)\n$$", "text_format": "latex", "page_idx": 37}, {"type": "equation", "text": "$$\n-\\,\\frac{1}{n\\sqrt{m}}\\sum_{\\substack{i:\\nu\\in X^{(i)}}}g_{i}^{(t)}y_{i}\\sum_{l=1}^{L}\\sum_{j=1}^{m_{1}}a_{j}\\mu^{\\top}W_{Q}^{(t)\\top}q_{l}^{(t,i)}p_{q\\leftarrow l,k\\leftarrow\\nu}^{(t,i)}\\left(w_{j}^{(t)\\top}v^{(t,i)}(\\nu)-w_{j}^{(t)\\top}V^{(t,i)}p_{l}^{(t,i)}\\right)\\mathbb{I}(q_{l}^{(t,i)}(\\nu)-v_{m}^{(t,i)}),\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Analysis of (1): By triangle inequality, we have ", "text_level": 1, "page_idx": 37}, {"type": "equation", "text": "$$\n|(1)|\\leq\\frac{1}{n\\sqrt{m}}\\left|\\sum_{i:\\mu,\\nu\\in X^{(i)}}g_{i}^{(t)}y_{i}\\sum_{j=1}^{m_{1}}a_{j}\\|k^{(t)}(\\nu)\\|_{2}^{2}v^{(t)\\top}(\\nu)w_{j}^{(t)}p_{q\\leftarrow\\mu,k\\leftarrow\\nu}^{(t,i)}\\right|\n$$", "text_format": "latex", "page_idx": 37}, {"type": "equation", "text": "$$\n+\\left.\\frac{1}{n\\sqrt{m}}\\right|\\sum_{i:\\mu,\\nu\\in X^{(i)}}g_{i}^{(t)}y_{i}\\sum_{j=1}^{m_{1}}a_{j}\\Vert k^{(t)}(\\nu)\\Vert_{2}^{2}w_{j}^{(t)\\top}V^{(t,i)}p_{l(i,\\mu)}^{(t,i)}p_{q\\leftarrow\\mu,k\\leftarrow\\nu}^{(t,i)}\\right|.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "To analyze $(a)$ , since $(a+\\varepsilon_{1})(b+\\varepsilon_{2})=a b+a\\varepsilon_{2}+b\\varepsilon_{1}+\\varepsilon_{1}\\varepsilon_{2}$ , we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\displaystyle\\sum_{j=1}^{m_{1}}a_{j}\\|k^{(t)}(\\nu)\\|_{2}^{2}w_{j}^{(t)\\top}v^{(t)}(\\nu)p_{q\\leftarrow\\mu,k\\leftarrow\\nu}^{(t,i)}-\\displaystyle\\sum_{j=1}^{m_{1}}a_{j}\\|k^{(0)}(\\nu)\\|_{2}^{2}w_{j}^{(0)\\top}v^{(0)}(\\nu)p_{q\\leftarrow\\mu,k\\leftarrow\\nu}^{(0,i)}\\bigg|\n$$", "text_format": "latex", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\leq\\left|\\sum_{j=1}^{m_{1}}a_{j}\\|k^{(t)}(\\nu)\\|_{2}^{2}w_{j}^{(t)\\top}v^{(t)}(\\nu)-\\sum_{j=1}^{m_{1}}a_{j}\\|k^{(0)}(\\nu)\\|_{2}^{2}w_{j}^{(0)\\top}v^{(0)}(\\nu)\\right|p_{q\\leftarrow\\mu,k\\leftarrow\\nu}^{(t,i)}}\\\\ {\\displaystyle\\ +\\left|\\sum_{j=1}^{m_{1}}a_{j}\\|k^{(0)}(\\nu)\\|_{2}^{2}w_{j}^{(0)\\top}v^{(0)}(\\nu)\\right|\\cdot\\left|p_{q\\leftarrow\\mu,k\\leftarrow\\nu}^{(t,i)}-p_{q\\leftarrow\\mu,k\\leftarrow\\nu}^{(0,i)}\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Further, by Lemma D.1, Lemma D.3, Definition E.1 and Definition E.2, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{j=1}^{m_{1}}a_{j}\\|k^{(t)}(\\nu)\\|_{2}^{2}w_{j}^{(t)\\top}v^{(t)}(\\nu)-\\displaystyle\\sum_{j=1}^{m_{1}}a_{j}\\|k^{(0)}(\\nu)\\|_{2}^{2}w_{j}^{(0)\\top}v^{(0)}(\\nu)\\|}\\\\ &{\\leq\\displaystyle\\operatorname*{max}_{\\nu,j}\\left|\\|k^{(t)}(\\nu)\\|_{2}^{2}w_{j}^{(t)\\top}v^{(t)}(\\nu)-\\|k^{(0)}(\\nu)\\|_{2}^{2}w_{j}^{(0)\\top}v^{(0)}(\\nu)\\right|}\\\\ &{\\leq m_{1}\\left(R_{K}\\widetilde O(\\sigma_{0}\\sigma_{1}\\sqrt{m})+R\\widetilde O(\\sigma_{0}^{2}m)+R_{K}R\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Combining with Proposition E.22, this implies ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{j=1}^{m_{1}}a_{j}\\|k^{(t)}(\\nu)\\|_{2}^{2}w_{j}^{(t)\\top}v^{(t)}(\\nu)p_{q+\\mu,k+\\nu}^{(t,i)}-\\sum_{j=1}^{m_{1}}a_{j}\\|k^{(0)}(\\nu)\\|_{2}^{2}w_{j}^{(0)\\top}v^{(0)}(\\nu)p_{q+\\mu,k+\\nu}^{(0,i)}\\Bigg|}}\\\\ &{\\leq m_{1}\\cdot\\underset{\\nu,j}{\\operatorname*{max}}\\left|\\|k^{(t)}(\\nu)\\|_{2}^{2}w_{j}^{(t)\\top}v^{(t)}(\\nu)-\\|k^{(0)}(\\nu)\\|_{2}^{2}w_{j}^{(0)\\top}v^{(0)}(\\nu)\\right|p_{q+\\mu,k+\\nu}^{(t,i)}}\\\\ &{\\quad+\\left|\\sum_{j=1}^{m_{1}}a_{j}\\|k^{(0)}(\\nu)\\|_{2}^{2}w_{j}^{(0)\\top}v^{(0)}(\\nu)\\right|\\cdot\\left|p_{q+\\mu,k+\\nu}^{(t,i)}-p_{q+\\mu,k+\\nu}^{(0,i)}\\right|}\\\\ &{\\leq m_{1}\\left(R_{K}\\widetilde{O}(\\sigma_{0}\\sigma_{1}\\sqrt{m})+R\\widetilde{O}(\\sigma_{0}^{2}m)+R_{K}R\\right)\\left(\\frac{2}{L}+R_{P}\\right)+\\widetilde{O}\\left(R_{P}\\sigma_{0}^{2}m\\sigma_{1}\\sigma_{0}\\sqrt{m m_{1}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Thus, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{|(a)|=\\left|\\displaystyle\\sum_{j=1}^{m_{1}}a_{j}\\|k^{(t)}(\\nu)\\|_{2}^{2}w_{j}^{(t)\\top}v^{(t)}(\\nu)p_{q-\\mu,k\\leftarrow\\nu}^{(t,i)}\\right|}}\\\\ &{\\leq\\left|\\displaystyle\\sum_{j=1}^{m_{1}}a_{j}\\|k^{(t)}(\\nu)\\|_{2}^{2}w_{j}^{(t)\\top}v^{(t)}(\\nu)p_{q-\\mu,k\\leftarrow\\nu}^{(t,i)}-\\displaystyle\\sum_{j=1}^{m_{1}}a_{j}\\|k^{(0)}(\\nu)\\|_{2}^{2}w_{j}^{(0)\\top}v^{(0)}(\\nu)p_{q+\\mu,k\\leftarrow\\nu}^{(0,i)}\\right|}\\\\ &{\\quad+\\left|\\displaystyle\\sum_{j=1}^{m_{1}}a_{j}\\|k^{(0)}(\\nu)\\|_{2}^{2}w_{j}^{(0)\\top}v^{(0)}(\\nu)p_{q+\\mu,k\\leftarrow\\nu}^{(0,i)}\\right|}\\\\ &{\\leq m_{1}\\left(R_{K}\\tilde{O}(\\sigma_{0}\\sigma_{1}\\sqrt{m})+R\\tilde{O}(\\sigma_{0}^{2}m)+R_{K}R\\right)\\left(\\displaystyle\\sum_{l}^{2}+R_{P}\\right)}\\\\ &{\\quad+\\left.\\tilde{O}\\left(R p_{0}\\sigma_{0}^{2}m\\sigma_{1}\\sigma_{0}\\sqrt{m m}\\right)+\\tilde{O}\\left(\\sigma_{0}^{2}m\\sigma_{1}\\sigma_{0}\\sqrt{m m}\\right)\\frac{1}{L}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "On the other hand, to analyze $(b)$ , we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{j=1}^{m_{1}}a_{j}\\|k^{(t)}(\\nu)\\|_{2}^{2}w_{j}^{(t)\\top}V^{(t,i)}p_{l(i,\\mu)}^{(t,i)}p_{q\\leftarrow\\mu,k\\leftarrow\\nu}^{(t,i)}-\\displaystyle\\sum_{j=1}^{m_{1}}a_{j}\\|k^{(0)}(\\nu)\\|_{2}^{2}w_{j}^{(0)\\top}V^{(0,i)}p_{l(i,\\mu)}^{(0,i)}p_{q\\leftarrow\\mu,k\\leftarrow\\nu}^{(0,i)}\\Bigg|}\\\\ {\\displaystyle}&{\\le\\displaystyle\\left|\\sum_{j=1}^{m_{1}}a_{j}\\|k^{(t)}(\\nu)\\|_{2}^{2}w_{j}^{(t)\\top}V^{(t,i)}p_{l(i,\\mu)}^{(t,i)}-\\displaystyle\\sum_{j=1}^{m_{1}}a_{j}\\|k^{(0)}(\\nu)\\|_{2}^{2}w_{j}^{(0)\\top}V^{(0,i)}p_{l(i,\\mu)}^{(0,i)}\\right|\\cdot p_{q\\leftarrow\\mu,k\\leftarrow\\nu}^{(t,i)}}\\\\ &{\\quad+\\displaystyle\\left|\\sum_{j=1}^{m_{1}}a_{j}\\|k^{(0)}(\\nu)\\|_{2}^{2}w_{j}^{(0)\\top}V^{(0,i)}p_{l(i,\\mu)}^{(0,i)}\\right|\\cdot\\left|p_{q\\leftarrow\\mu,k\\leftarrow\\nu}^{(t,i)}-p_{q\\leftarrow\\mu,k\\leftarrow\\nu}^{(0,i)}\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Note that ", "text_level": 1, "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{j=0}^{\\infty}a_{j}\\|A^{\\mathrm{tor}}(y)\\|_{\\infty}^{2}\\nu^{(j)}V^{\\mathrm{tor}}\\|\\nu(\\rho_{1,j}^{(k)})-\\frac{\\sqrt{\\lambda}}{\\nu}\\|a_{j}\\|\\nu^{(k)}\\|_{\\infty}^{2}\\nu^{(j)}V^{\\mathrm{tor}}\\|\\nu^{(k)}\\|_{\\infty}^{2}}\\\\ &{\\displaystyle=\\left|\\frac{\\dot{L}}{\\nu+\\sqrt{\\lambda}}\\sum_{s_{j}=1}^{\\infty}a_{j}\\|\\nu^{(k)}\\|_{\\infty}^{2}\\nu^{(j)}V_{1}^{\\mathrm{tor}}\\|\\nu^{(k)}\\|_{\\infty}^{2},\\frac{\\sqrt{\\lambda}}{\\nu+\\sqrt{\\lambda}}\\sum_{s_{j}=1}^{\\infty}a_{j}\\|\\nu^{(k)}\\|_{\\infty}^{2}\\nu^{(j)}V_{1}^{\\mathrm{tor}}\\|\\nu^{(k)}\\|_{\\infty}^{2}\\right|}\\\\ &{\\displaystyle=\\left|\\frac{\\dot{L}}{\\nu+\\sqrt{\\lambda}}\\sum_{s_{j}=1}^{\\infty}a_{j}\\|\\nu^{(k)}\\|_{\\infty}^{2}\\nu^{(j)}V_{1}^{\\mathrm{tor}}\\|\\nu^{(k)}\\|_{\\infty}^{2},\\frac{\\sqrt{\\lambda}}{\\nu+\\sqrt{\\lambda}}\\right|\\nu^{(k)}\\|_{\\infty}^{2}\\nu^{(j)}V_{1}^{\\mathrm{tor}}\\|\\nu^{(k)}\\|_{\\infty}^{2}}\\\\ &{\\displaystyle=\\sum_{j=1}^{\\infty}\\left|\\sum_{s_{j}=1}^{\\infty}a_{j}\\|\\nu^{(j)}\\|_{\\infty}^{2}\\nu^{(j)}V_{1}^{\\mathrm{tor}}\\|\\nu^{(k)}\\|_{\\infty}^{2},\\frac{\\sqrt{\\lambda}}{\\nu+\\sqrt{\\lambda}}\\right|\\nu^{(k)}\\|_{\\infty}^{2}\\nu^{(j)}V_{1}^{\\mathrm{tor}}\\|\\nu^{(k)}\\|_{\\infty}^{2}\\right|}\\\\ &{\\displaystyle\\leq\\sum_{j=1}^{\\infty}\\left|\\sum_{s_{j}=1}^{\\infty}a_{j}\\|\\nu^{(j)}\\|_{\\infty}^{2}\\nu^{(j)}V_{ \n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "which implies ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\begin{array}{r l}&{\\frac{1}{n(n-1)}\\displaystyle\\int_{\\mathbb{R}^{n}}^{\\infty}q_{i}\\mathrm{d}\\lambda^{i\\alpha}(t)\\mathrm{i}\\lambda^{i\\alpha}\\rho_{i}^{0}\\lambda^{\\lambda\\alpha}\\mathrm{i}\\lambda^{\\lambda\\alpha\\lambda}_{i,j\\alpha}}\\\\ &{+\\Biggl[\\displaystyle\\sum_{s=0}^{n}q_{i}\\mathrm{d}\\lambda^{i\\alpha}(t)\\mathrm{i}\\lambda^{i\\alpha}\\mathrm{i}\\lambda^{\\alpha\\lambda}(\\lambda^{\\alpha})\\mathrm{i}\\lambda^{\\alpha}\\mathrm{i}\\lambda^{\\alpha\\lambda}\\mathrm{i}-\\sum_{s=1}^{n}q_{i}\\mathrm{d}\\lambda^{\\alpha}(\\lambda^{\\alpha})\\mathrm{i}\\lambda^{\\alpha}\\mathrm{i}\\lambda^{\\alpha}\\mathrm{i}\\lambda^{\\alpha\\lambda}\\mathrm{i}}\\\\ &{+\\Biggl[\\displaystyle\\sum_{s=0}^{n}q_{i}\\mathrm{d}\\lambda^{\\alpha}(t)\\mathrm{i}\\lambda^{\\alpha}(t)\\mathrm{i}\\lambda^{\\alpha}\\mathrm{i}\\lambda^{\\alpha\\lambda}\\mathrm{i}\\lambda^{\\alpha\\lambda}\\mathrm{i}-\\sum_{s=1}^{n}q_{i}\\mathrm{d}\\lambda^{\\alpha}(\\lambda^{\\alpha})\\mathrm{i}\\lambda^{\\alpha}\\mathrm{i}\\lambda^{\\alpha\\lambda}\\mathrm{i}\\lambda^{\\alpha\\lambda}\\mathrm{i}\\lambda^{\\alpha\\lambda}\\mathrm{i}}\\\\ &{+\\Biggl[\\displaystyle\\sum_{s=0}^{n}q_{i}\\mathrm{d}\\lambda^{\\alpha}(\\lambda^{\\alpha})\\mathrm{i}\\lambda^{\\alpha}\\mathrm{i}\\lambda^{\\alpha\\lambda}\\mathrm{i}\\lambda^{\\alpha\\lambda}\\mathrm{i}\\lambda^{\\alpha\\lambda}\\mathrm{i}\\lambda^{\\alpha\\lambda}\\mathrm{i}-\\sum_{s=1}^{n}q_{i}\\mathrm{d}\\lambda^{\\alpha}(\\lambda^{\\alpha})\\mathrm{i}\\lambda^{\\alpha}\\mathrm{i}\\lambda^{\\alpha\\lambda}\\mathrm{i}\\lambda^{\\alpha\\lambda}\\mathrm{i}\\lambda^{\\alpha\\lambda}}\\\\ &{+\\Biggl[\\displaystyle\\sum_{s=0}^{n}q_{i}\\mathrm{d}\\lambda^{\\alpha}(\\lambda^{\\alpha})\\mathrm{i}\\lambda^{\\alpha}\\mathrm{i}\\lambda^{\\alpha\\lambda}\\mathrm{i}\\lambda^{\\alpha\\lambda}\\mathrm{i}\\lambda^{\\alpha\\lambda}\\mathrm{i}\\lambda^{\\alpha\\lambda}\\mathrm{i}-\\sum_{s=1}^{n}q_{i}\\mathrm{d}\\lambda^{\\alpha}\\mathrm{i}\\lambda^{\\alpha}\\mathrm{i}\\lambda^{\\alpha\\lambda}\\mathrm \n$$", "text_format": "latex", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+\\left|\\displaystyle\\sum_{j=1}^{m_{1}}a_{j}\\|k^{(0)}(\\nu)\\|_{2}^{2}w_{j}^{(0)\\top}V^{(0,i)}p_{l(i,\\mu)}^{(0,i)}\\right|\\cdot\\left|p_{q\\leftarrow\\mu,k\\leftarrow\\nu}^{(t,i)}-p_{q\\leftarrow\\mu,k\\leftarrow\\nu}^{(0,i)}\\right|}\\\\ &{\\quad+\\left|\\displaystyle\\sum_{j=1}^{m_{1}}a_{j}\\|k^{(0)}(\\nu)\\|_{2}^{2}w_{j}^{(0)\\top}V^{(0,i)}p_{l(i,\\mu)}^{(0,i)}p_{q\\leftarrow\\mu,k\\leftarrow\\nu}^{(0,i)}\\right|}\\\\ &{\\overset{(i i i)}{\\leq}\\left(m_{1}\\left(R_{K}\\widetilde{O}(\\sigma_{0}\\sigma_{1}\\sqrt{m})+R\\widetilde{O}(\\sigma_{0}^{2}m)+R K R\\right)}\\\\ &{\\quad+L R_{P}\\widetilde{O}(\\sigma_{0}^{2}m\\sigma_{0}\\sigma_{1}\\sqrt{m m_{1}})\\right)\\cdot\\left(\\frac{1}{L}+R_{P}\\right)+\\widetilde{O}\\left(\\sigma_{0}^{2}m\\sigma_{0}\\sigma_{1}\\sqrt{m m_{1}}\\left(\\frac{1}{L}+R_{P}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where $(i)$ follows from Equation (10), $(i i)$ follows from Equation (11) and $(i i i)$ follows from Equation (9) and Proposition E.22. Combining the upper bound for both $(a)$ and $(b)$ , we obtain ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|(1)|\\leq\\displaystyle\\frac{1}{\\sqrt{m}}\\displaystyle\\frac{\\big|\\{i:\\mu,\\nu\\in X^{(i)}\\}\\big|}{n}\\tilde{O}\\Biggl(\\Biggl(m_{1}\\left(R_{K}\\tilde{O}(\\sigma_{0}\\sigma_{1}\\sqrt{m})+R\\tilde{O}(\\sigma_{0}^{2}m)+R_{K}R\\right)}\\\\ &{\\qquad+L R_{P}\\tilde{O}(\\sigma_{0}^{2}m\\sigma_{0}\\sigma_{1}\\sqrt{m m_{1}})\\Biggr)\\cdot\\bigg(\\displaystyle\\frac{1}{L}+R_{P}\\bigg)+\\sigma_{0}^{2}m\\sigma_{0}\\sigma_{1}\\sqrt{m m_{1}}\\left(\\displaystyle\\frac{1}{L}+R_{P}\\right)\\Biggr)}\\\\ &{=\\displaystyle\\frac{1}{\\sqrt{m}}\\displaystyle\\frac{\\big|\\{i:\\mu,\\nu\\in X^{(i)}\\}\\big|}{n}\\tilde{O}(1/L).}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Analysisof(2):2canbeaalyzedsimilalyaswih $\\lVert k^{(0)}(\\nu)\\rVert_{2}^{2}$ replaced by $\\nu^{\\top}W_{K}^{(t)}K_{l}^{(t,i)}$ Thus, we only need to replace $\\sigma_{0}^{2}m$ with $\\sigma_{0}^{2}{\\sqrt{m}}$ and then take a sum over We have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|(2)|\\le\\displaystyle\\frac{1}{\\sqrt{m}}\\displaystyle\\frac{\\big|\\{i:\\mu\\in X^{(i)}\\}\\big|}{n}\\tilde{O}\\bigg(\\bigg(m_{1}\\,\\Big(R_{K}\\tilde{O}(\\sigma_{0}\\sigma_{1}\\sqrt{m})+R\\tilde{O}(\\sigma_{0}^{2}\\sqrt{m})+R_{K}R\\Big)}\\\\ &{\\qquad+\\,L R_{P}\\tilde{O}(\\sigma_{0}^{2}\\sqrt{m}\\sigma_{0}\\sigma_{1}\\sqrt{m}m_{1})\\bigg)\\cdot(1+L R_{P})+\\sigma_{0}^{2}\\sqrt{m}\\sigma_{0}\\sigma_{1}\\sqrt{m m_{1}}\\,(1+L R_{P})\\bigg)}\\\\ &{=\\displaystyle\\frac{1}{\\sqrt{m}}\\displaystyle\\frac{\\big|\\{i:\\mu\\in X^{(i)}\\}\\big|}{n}\\tilde{O}(1/\\sqrt{m}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Analysis of (3): (3) can be analyzed similarly to (1), and we obtain ", "page_idx": 40}, {"type": "equation", "text": "$$\n|(3)|\\leq\\frac{1}{\\sqrt{m}}\\frac{\\left|\\left\\{i:\\;\\mu,\\nu\\in X^{(i)}\\right\\}\\right|}{n}\\widetilde O(1/L).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Analysis of (4): (4) can be analyzed similarly to (2), and we obtain ", "page_idx": 40}, {"type": "equation", "text": "$$\n|(4)|\\leq\\frac{1}{\\sqrt{m}}\\frac{\\left|\\left\\{i:\\:\\nu\\in X^{(i)}\\right\\}\\right|}{n}\\widetilde{O}(1/\\sqrt{m}).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Finally, combining the bounds on $\\left(1\\right)-\\left(4\\right)$ , we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial\\nu^{\\top}W_{K}^{(t)^{\\top}}W_{Q}^{(t)}\\mu}{\\partial t}\\bigg|\\leq\\frac{1}{\\sqrt{m}}\\frac{\\big|\\big\\{i:\\mu,\\nu\\in X^{(i)}\\big\\}\\big|}{n}\\tilde{O}(1/L)+\\frac{1}{\\sqrt{m}}\\frac{\\big|\\big\\{i:\\mu\\in X^{(i)}\\big\\}\\big|+\\big|\\{i:\\nu\\in X^{(i)}\\big\\}}{n}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{1}{\\sqrt{m}}\\frac{\\big|\\big\\{i:\\mu\\in X^{(i)}\\big\\}\\big|+\\big|\\{i:\\nu\\in X^{(i)}\\big\\}\\big|}{n}\\tilde{O}(1/L+1/\\sqrt{m}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Corollary E.24 (Softmax change). For $t\\leq T_{1}$ , we have the following: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 if both $X_{l_{1}}^{(i)},X_{l_{2}}^{(i)}\\in\\mathcal{R},$ then ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\left|\\frac{\\partial p_{l_{1},l_{2}}^{(t,i)}}{\\partial t}\\right|\\leq\\widetilde{O}\\left(\\frac{1}{L(L^{2}+n)\\sqrt{m}}\\right),\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "\u00b7 otherwise, ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\left|\\frac{\\partial p_{l_{1},l_{2}}^{(t,i)}}{\\partial t}\\right|\\leq\\widetilde{O}\\left(\\frac{1}{L^{2}\\sqrt{m}}\\right).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Thus, ", "page_idx": 41}, {"type": "text", "text": "\u00b7 if borh $X_{l_{1}}^{(i)},X_{l_{2}}^{(i)}\\in\\mathcal{R},$ then ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\left|p_{l_{1},l_{2}}^{(t,i)}-p_{l_{1},l_{2}}^{(0,i)}\\right|\\le\\widetilde{O}\\left(\\left(\\frac{1}{L}+\\frac{1}{n}\\right)\\frac{1}{L m^{2}}\\left(\\frac{1}{L}+\\frac{1}{\\sqrt{m}}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "\u00b7 otherwise, ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\left|p_{l_{1},l_{2}}^{(t,i)}-p_{l_{1},l_{2}}^{(0,i)}\\right|\\leq\\widetilde{O}\\left(\\frac{1}{L m^{2}}\\left(\\frac{1}{L}+\\frac{1}{\\sqrt{m}}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Proof. Consider fixed $i,l_{1},l_{2}$ . By Lemma G.2, we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\left|\\frac{\\partial p_{l_{1},l_{2}}^{(t,i)}}{\\partial t}\\right|\\leq p_{l_{1},l_{2}}^{(t,i)}\\left|\\frac{\\partial s_{l_{1},l_{2}}^{(t,i)}}{\\partial t}\\right|+p_{l_{1},l_{2}}^{(t,i)}\\left|p_{l_{1}}^{(t,i)\\top}\\frac{\\partial s_{l_{1}}^{(t,i)}}{\\partial t}\\right|.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "By Lemma E.23, we have the following cases: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 if both $X_{l_{1}}^{(i)},X_{l_{2}}^{(i)}\\in\\mathcal{R}$ then ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\left|\\frac{\\partial s_{l_{1},l_{2}}^{(t,i)}}{\\partial t}\\right|\\leq\\widetilde{O}\\left(\\frac{1}{n m}\\left(\\frac{1}{L}+\\frac{1}{\\sqrt{m}}\\right)\\right);\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "\u00b7 otherwise, ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\left|\\frac{\\partial s_{l_{1},l_{2}}^{(t,i)}}{\\partial t}\\right|\\leq\\widetilde{O}\\left(\\frac{1}{m}\\left(\\frac{1}{L}+\\frac{1}{\\sqrt{m}}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Next, during Phase 1, we have $\\begin{array}{r}{p_{l_{1},l_{2}}^{(t,i)}\\leq\\frac{1}{L}+\\frac{1}{L m}+R_{P}}\\end{array}$ . Therefore, ", "page_idx": 41}, {"type": "text", "text": "\u00b7if $X_{l_{1}}^{(i)}\\in\\mathcal{R}$ , then ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\left|p_{l_{1}}^{(t,i)\\top}\\frac{\\partial s_{l_{1}}^{(t,i)}}{\\partial t}\\right|\\leq\\widetilde{O}\\left(\\left(\\frac{1}{L}+\\frac{1}{n}\\right)\\frac{1}{m}\\left(\\frac{1}{L}+\\frac{1}{\\sqrt{m}}\\right)\\right);\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "\u00b7 otherwise, ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\left|p_{l_{1}}^{(t,i)\\top}\\frac{\\partial s_{l_{1}}^{(t,i)}}{\\partial t}\\right|\\leq\\widetilde{O}\\left(\\frac{1}{m}\\left(\\frac{1}{L}+\\frac{1}{\\sqrt{m}}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Thus, ", "page_idx": 41}, {"type": "text", "text": "\u00b7 if both $X_{l_{1}}^{(i)},X_{l_{2}}^{(i)}\\in\\mathcal{R}$ , then ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\left|\\frac{\\partial p_{l_{1},l_{2}}^{(t,i)}}{\\partial t}\\right|\\leq\\widetilde{O}\\left(\\left(\\frac{1}{L}+\\frac{1}{n}\\right)\\frac{1}{L m}\\left(\\frac{1}{L}+\\frac{1}{\\sqrt{m}}\\right)\\right);\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "\u00b7 otherwise, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\left|\\frac{\\partial p_{l_{1},l_{2}}^{(t,i)}}{\\partial t}\\right|\\leq\\widetilde{O}\\left(\\frac{1}{L m}\\left(\\frac{1}{L}+\\frac{1}{\\sqrt{m}}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "This implies that ", "page_idx": 42}, {"type": "text", "text": "\u00b7 if both $X_{l_{1}}^{(i)},X_{l_{2}}^{(i)}\\in\\mathcal{R}$ thn ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\left|p_{l_{1},l_{2}}^{(t,i)}-p_{l_{1},l_{2}}^{(0,i)}\\right|\\le\\widetilde{O}\\left(\\left(\\frac{1}{L}+\\frac{1}{n}\\right)\\frac{1}{L m^{2}}\\left(\\frac{1}{L}+\\frac{1}{\\sqrt{m}}\\right)\\right);\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "\u00b7 otherwise, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\left|p_{l_{1},l_{2}}^{(t,i)}-p_{l_{1},l_{2}}^{(0,i)}\\right|\\leq\\widetilde{O}\\left(\\frac{1}{L m^{2}}\\left(\\frac{1}{L}+\\frac{1}{\\sqrt{m}}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Lemma E.25 $(K,Q$ self-correlation change). For $t\\leq T_{1}$ for $\\mu,\\nu\\in\\{\\mu_{i}\\}_{i=1}^{d}$ ,we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mu^{\\top}W_{K}^{(t)\\top}W_{K}^{(t)}\\nu-\\mu^{\\top}W_{K}^{(0)\\top}W_{K}^{(0)}\\nu\\right|\\leq t\\frac{1}{\\sqrt{m}}\\frac{\\left|\\left\\{i:\\;\\mu\\in X^{(i)}\\right\\}\\right|+\\left|\\left\\{i:\\;\\nu\\in X^{(i)}\\right\\}\\right|}{n}\\tilde{O}\\left(\\frac{1}{\\sqrt{m}}\\right),}\\\\ &{\\left|\\mu^{\\top}W_{Q}^{(t)\\top}W_{Q}^{(t)}\\nu-\\mu^{\\top}W_{Q}^{(0)\\top}W_{Q}^{(0)}\\nu\\right|\\leq t\\frac{1}{\\sqrt{m}}\\frac{\\left|\\left\\{i:\\;\\mu\\in X^{(i)}\\right\\}\\right|+\\left|\\left\\{i:\\;\\nu\\in X^{(i)}\\right\\}\\right|}{n}\\tilde{O}\\left(\\frac{1}{\\sqrt{m}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Proof. By Lemma C.4, we only need to replace $\\widetilde{O}(\\sigma_{0}^{2}m)$ by $\\widetilde{O}(\\sigma_{0}^{2}\\sqrt{m})$ in the proof of Lemma E.23. Thus, we omit the proof here. \u53e3 ", "page_idx": 42}, {"type": "text", "text": "E.8  All Variables are within Range in Definition of Phase 1 ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Finally, we prove that at the end of Phase 1, all the variables in Definition E.2 stay in the range. Theorem E.26. For $t\\,\\leq\\,C_{T_{1}}/(\\sigma_{1}^{2}m m_{1})$ (where the constant ${{C}_{{T}_{1}}}$ is from the definition of $T_{1}$ in Definition $E.2.$ , all of the following hold: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{max}_{j\\in[m],\\mu\\not\\in\\{\\mu_{i}\\}_{i=1}^{3}}\\left|w_{j}^{(t)}W_{V}^{(t)}\\mu-w_{j}^{(0)}W_{V}^{(0)}\\mu\\right|\\leq O(R/n+R/\\sqrt{m});}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\4.\\ \\operatorname*{max}_{\\mu,\\nu\\in\\{\\mu_{i}\\}_{i=1}^{d}}\\left|\\mu^{\\top}W_{Q}^{(t)\\top}W_{Q}^{(t)}\\nu-\\mu^{\\top}W_{Q}^{(0)\\top}W_{Q}^{(0)}\\nu\\right|\\leq R_{Q};}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{5.\\ \\operatorname*{max}_{\\mu,\\nu\\in\\{\\mu_{i}\\}_{i=1}^{d}}\\left|\\mu^{\\top}W_{K}^{(t)\\top}W_{K}^{(t)}\\nu-\\mu^{\\top}W_{K}^{(0)\\top}W_{K}^{(0)}\\nu\\right|\\leq R_{K}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Thus, $T_{1}=C_{T_{1}}/(\\sigma_{1}^{2}m m_{1})$ ", "page_idx": 42}, {"type": "text", "text": "Proof. The first two results are proved by Theorem E.14, Theorem E.15, Lemma E.17. The third result is proved by Lemma E.23, The fourth and fifth results are proved by Lemma E.25. ", "page_idx": 42}, {"type": "text", "text": "Theorem E.27 (End of Phase 1). Af $t=T_{1}$ we have $y_{i}F_{i}^{(T_{1})}=\\Theta(1)$ for all $i\\in[n]$ and ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{G^{(T_{1})}(\\mu_{1})=\\Theta(1),\\quad G^{(T_{1})}(\\mu_{2})=\\Theta(1),\\quad-G^{(T_{1})}(\\mu_{3})=\\Theta(1),}\\\\ &{\\forall\\mu\\in{\\mathcal{R}}:\\;G^{(T_{1})}(\\mu)=\\widetilde{O}(\\sigma_{0}\\sigma_{1}\\sqrt{m m_{1}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Further, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(T_{1})},a_{j_{2}}w_{j_{2}}^{(T_{1})}\\right\\rangle=\\Theta(\\sigma_{1}^{2}m m_{1}).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Proof By Theorem E.26, we have $y_{i}F_{i}^{(T_{1})}=O(1)$ and ", "page_idx": 43}, {"type": "equation", "text": "$$\nG^{(T_{1})}(\\mu_{1})=O(1),\\quad G^{(T_{1})}(\\mu_{2})=O(1),\\quad-G^{(T_{1})}(\\mu_{3})=O(1).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Further, by Theorem E.14 and Theorem E.19, we have $y_{i}F_{i}^{(T_{1})}\\geq\\Omega(1)$ for $i\\in I_{1}$ and ", "page_idx": 43}, {"type": "equation", "text": "$$\nG^{(T_{1})}(\\mu_{1})\\geq\\Omega(1),\\quad G^{(T_{1})}(\\mu_{2})\\geq\\Omega(1).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Finally, by Theorem E.26, we have that $T_{1}~=~C_{T_{1}}/m$ . And note that if the constant ${\\cal C}_{T_{1}}$ in Definition E.2 is sufficiently large, then by Corollary E.20, we have $T^{\\prime}\\,\\leq\\,T_{1}$ . Thus, we have $y_{i}F_{i}^{(T_{1})}\\geq\\Omega(1)$ for $i\\in I_{2}\\cup I_{3}\\cup I_{4}$ and $-G^{(T_{1})}(\\mu_{3})\\geq\\Omega(1)$ ", "page_idx": 43}, {"type": "text", "text": "By Lemma D.7 and Theorem E.15, we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\forall\\mu\\in\\mathcal{R}:\\ G^{(T_{1})}(\\mu)=\\widetilde O(\\sigma_{0}\\sigma_{1}\\sqrt{m m_{1}}).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Finally, by Lemma D.3, Proposition E.5 and Proposition E.12, we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(T_{1})},a_{j_{2}}w_{j_{2}}^{(T_{1})}\\right\\rangle=\\Theta(\\sigma_{1}^{2}m m_{1}).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Theorem E.28 (Phase 1, formal restatement of Theorem 3.1). With probability at least $1-\\delta$ Oover therandomness of weight initialization,there exists atime $T_{1}=\\tilde{O}(1/m)$ suchthat ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\cdot\\ G^{(T_{1})}(\\mu_{1})\\geq\\Omega(1),\\ G^{(T_{1})}(\\mu_{2})\\geq\\Omega(1),\\ G^{(T_{1})}(\\mu_{3})\\leq-\\Omega(1).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "\u00b7All the training samles are corectlycaifd: $y_{i}F_{i}^{(T_{1})}=\\Omega(1)$ for all $i\\in[n]$ ", "page_idx": 43}, {"type": "text", "text": "\u00b7 For $t\\in[0,T_{1}].$ ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle w_{j1}^{(t)},w_{j2}^{(t)}\\right\\rangle-\\left\\langle w_{j1}^{(0)},w_{j2}^{(0)}\\right\\rangle\\Big|\\leq\\widetilde{O}\\left(\\frac{1}{m}\\right)}\\\\ &{\\left\\langle v^{(t)}(\\mu),v^{(t)}(\\nu)\\right\\rangle-\\left\\langle v^{(0)}(\\mu),v^{(0)}(\\nu)\\right\\rangle\\Big|\\leq\\widetilde{O}\\left(\\frac{1}{m}\\right)}\\\\ &{\\left|\\nu^{\\top}W_{K}^{(t)\\top}W_{Q}^{(t)}\\mu-\\nu^{\\top}W_{K}^{(0)\\top}W_{Q}^{(0)}\\mu\\right|\\leq\\widetilde{O}\\left(\\frac{1}{m^{3/2}}\\left(\\frac{1}{L}+\\frac{1}{\\sqrt{m}}\\right)\\right)}\\\\ &{\\left|\\mu^{\\top}W_{K}^{(t)\\top}W_{K}^{(t)}\\nu-\\mu^{\\top}W_{K}^{(0)\\top}W_{K}^{(0)}\\nu\\right|\\leq\\widetilde{O}\\left(\\frac{1}{m^{2}}\\right)}\\\\ &{\\left|\\mu^{\\top}W_{Q}^{(t)\\top}W_{Q}^{(t)}\\nu-\\mu^{\\top}W_{Q}^{(0)\\top}W_{Q}^{(0)}\\nu\\right|\\leq\\widetilde{O}\\left(\\frac{1}{m^{2}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Proof. The first two results are proved in Theorem E.27. The third result is proved by Lemma E.9, Proposition E.12, Lemma E.23, and Lemma E.25. The result on the training loss is a direct consequence of Definition E.2. \u53e3 ", "page_idx": 43}, {"type": "text", "text": "F Training Dynamics: Phase 2 ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "The idea of the proof is to first define conditions for Phase 2, which guarantees that the small training loss can be achieved. Then we will show that those conditions can be satisfied starting from the end of Phase 1 and up to at least $\\Omega({\\mathrm{poly}}(m))$ time, which will serve as the end of Phase 2. ", "page_idx": 43}, {"type": "text", "text": "Definition F.1. We define Phase 2 of the training to be $t\\in[T_{1},T_{2}]$ suchthat ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The change of $K,Q$ self-correlation is small: ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mu,\\nu}{\\operatorname*{max}}\\left|\\mu^{\\top}W_{K}^{(t)\\top}W_{K}^{(t)}\\nu-\\mu^{\\top}W_{K}^{(0)\\top}W_{K}^{(0)}\\nu\\right|=\\widetilde O(\\sigma_{0}^{2}\\sqrt{m})}\\\\ &{\\underset{\\mu,\\nu}{\\operatorname*{max}}\\left|\\mu^{\\top}W_{Q}^{(t)\\top}W_{Q}^{(t)}\\nu-\\mu^{\\top}W_{Q}^{(0)\\top}W_{Q}^{(0)}\\nu\\right|=\\widetilde O(\\sigma_{0}^{2}\\sqrt{m})}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "\u00b7 The change of softmax probability satisfies: ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{l_{1},l_{2}\\in[L],~i\\in[n]}\\left|p_{l_{1},l_{2}}^{(t,i)}-p_{l_{1},l_{2}}^{(0,i)}\\right|<O(1/L^{2})\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "\u00b7 The sum of neuron correlation satisfies ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle\\leq O(\\sigma_{1}^{2}m m_{1}).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "\u00b7 The gradient of $W$ $W_{V}$ satisfies ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\operatorname*{max}_{\\mu\\in\\{\\mu_{i}\\}_{i=1}^{d}}\\left|\\sum_{j=1}^{m_{1}}a_{j}\\frac{\\partial w_{j}^{(t)}}{\\partial t}W_{V}^{(t)}\\mu\\right|}{\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle}\\leq o(1/L)\\frac{1}{n}\\sum_{i\\in[n]}g_{i}^{(t)}}\\\\ &{\\frac{\\operatorname*{max}_{i\\in[n]}\\left|\\sum_{l_{1}=1}^{L}\\sum_{l_{2}=1}^{L}G^{(t)}(X_{l_{2}}^{(i)})\\frac{\\partial p_{l_{1},l_{2}}^{(t,i)}}{\\partial t}\\right|}{\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle}\\leq o(1)\\frac{1}{n}\\sum_{i\\in[n]}g_{i}^{(t)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "\u00b7 The gradients $\\textstyle\\sum_{i\\in I}g_{i}^{(t)}$ for $I\\in\\{I_{1},I_{2},I_{3},I_{4}\\}$ satisfies ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\sum_{i\\in I_{4}}g_{i}\\leq\\operatorname*{min}\\left(\\sum_{i\\in I_{2}}g_{i}^{(t)},\\sum_{i\\in I_{3}}g_{i}^{(t)}\\right)\\leq\\operatorname*{max}\\left(\\sum_{i\\in I_{2}}g_{i}^{(t)},\\sum_{i\\in I_{3}}g_{i}^{(t)}\\right)\\leq\\sum_{i\\in I_{1}}g_{i}^{(t)}\\leq\\sum_{i\\in I_{2}\\cup I_{3}\\cup I_{4}}g_{i}^{(t)}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "and ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\leq\\frac{\\sum_{i\\in I_{2}}g_{i}^{(t)}}{\\sum_{i\\in I_{3}}g_{i}^{(t)}}\\leq2.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "$y_{i}F_{i}^{(t)}>C$ for all $i\\in[n]$ for some fixed constant $C$ ", "page_idx": 44}, {"type": "equation", "text": "$$\n|G^{(t)}(\\mu)|\\leq O(\\log m)\\,f o r\\,\\mu\\in\\{\\mu_{i}\\}_{i=1}^{3}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bullet\\;\\sum_{l_{1}=1}^{L}\\sum_{l_{2}:\\;X_{l_{2}}^{(i)}\\in\\{\\mu_{k}\\}_{k=4}^{d}}G^{(t)}(X_{l_{2}}^{(i)})p_{q\\leftarrow l_{1},k\\leftarrow l_{2}}^{(t,i)}\\leq O(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "\u00b7 $T_{2}\\leq O(\\mathrm{poly}(m)).$ ", "page_idx": 44}, {"type": "text", "text": "Corollary F.2. For $t\\in[T_{1},T_{2}].$ $i,j\\in I$ where $I\\in\\{I_{1},I_{2},I_{3},I_{4}\\}$ then ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i,j\\in I}\\frac{g_{i}^{(t)}}{g_{j}^{(t)}}\\le O(1).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Proof. Take $I=I_{1}$ and the proof is similar for the remaining cases. For fixed $i,j\\in I_{1}$ ,wehave ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\frac{g_{i}^{(t)}}{g_{j}^{(t)}}=\\frac{1+\\exp(y_{j}F_{j}^{(t)})}{1+\\exp(y_{i}F_{i}^{(t)})}\\leq2\\frac{\\exp(y_{j}F_{j}^{(t)})}{\\exp(y_{i}F_{i}^{(t)})},\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where the inequality is due to $y_{i}F_{i}^{(t)}\\geq C$ in Definition F.1. Now we consider ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\frac{\\exp(y_{j}F_{j}^{(t)})}{\\exp(y_{i}F_{i}^{(t)})}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\displaystyle\\frac{\\exp(y_{j}\\sum_{l_{1}=1}^{L}\\sum_{l_{2}=1}^{L}G^{(t)}(X_{l_{2}}^{(j)})p_{q\\leftarrow l_{1},k\\leftarrow l_{2}}^{(t,j)})}{\\exp(y_{i}\\sum_{l_{1}=1}^{L}\\sum_{l_{2}=1}^{L}G^{(t)}(X_{l_{2}}^{(i)})p_{q\\leftarrow l_{1},k\\leftarrow l_{2}}^{(t,i)})}}\\\\ &{=\\displaystyle\\exp\\left(\\left(\\sum_{l_{1}=1}^{L}\\sum_{\\mu\\in\\{\\mu_{k}\\}_{k=1}^{3}}G^{(t)}(\\mu)p_{q\\leftarrow l_{1},k\\leftarrow\\mu}^{(t,j)}\\right)-\\left(\\sum_{l_{1}=1}^{L}\\sum_{\\mu\\in\\{\\mu_{k}\\}_{k=1}^{3}}G^{(t)}(\\mu)p_{q\\leftarrow l_{1},k\\leftarrow\\mu}^{(t,i)}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "equation", "text": "$$\n\\cdot\\exp\\left(\\left(\\sum_{l_{1}=1}^{L}\\sum_{l_{2}:\\;X_{l_{2}}^{(j)}\\in\\{\\mu_{k}\\}_{k=4}^{d}}G^{(t)}(X_{l_{2}}^{(j)})p_{q\\in-l_{1},k\\leftarrow l_{2}}^{(t,j)}\\right)-\\left(\\sum_{l_{1}=1}^{L}\\sum_{l_{2}:\\;X_{l_{2}^{(i)}}^{(i)}\\in\\{\\mu_{k}\\}_{k=4}^{d}}G^{(t)}(X_{l_{2}}^{(i)})p_{q\\in-l_{1},k\\leftarrow l_{2}}^{(t,i)}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "By Definition F.1, it is easy to see that $|(2)|\\le{\\cal O}(1)$ . For (1), we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left(\\displaystyle\\sum_{l_{1}=1}^{L}\\sum_{\\mu\\in\\{\\mu_{k}\\}_{k=1}^{3}}G^{(t)}(\\mu)p_{q\\leftarrow l_{1},k\\leftarrow\\mu}^{(t,j)}\\right)-\\left(\\displaystyle\\sum_{l_{1}=1}^{L}\\sum_{\\mu\\in\\{\\mu_{k}\\}_{k=1}^{3}}G^{(t)}(\\mu)p_{q\\leftarrow l_{1},k\\leftarrow\\mu}^{(t,i)}\\right)\\right|}\\\\ &{\\leq\\displaystyle\\left|\\sum_{\\mu\\in\\{\\mu_{k}\\}_{k=1}^{3}}G^{(t)}(\\mu)(1\\pm o(1))-\\sum_{\\mu\\in\\{\\mu_{k}\\}_{k=1}^{3}}G^{(t)}(\\mu)(1\\pm o(1))\\right|\\leq O(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Thus, we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n{\\frac{\\exp(y_{j}F_{j}^{(t)})}{\\exp(y_{i}F_{i}^{(t)})}}\\le O(1)\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "for $t\\in[T_{1},T_{2}]$ ", "page_idx": 45}, {"type": "text", "text": "Lemma F.3. Phase 2 in Definition F.1 is well-defined. ", "page_idx": 45}, {"type": "text", "text": "Proof. We need to show that the definition is valid at $t=T_{1}$ with conditions satisfied with strict inequality. Then since everything changes continuously, there naturally exists a $T_{2}>T_{1}$ such that Definition F.1 is well-defined. ", "page_idx": 45}, {"type": "text", "text": "First of all, by Lemma E.25, for $\\mu,\\nu\\in\\{\\mu_{i}\\}_{i=1}^{d}$ , we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mu,\\nu}\\left|\\mu^{\\top}W_{K}^{(T_{1})\\top}W_{K}^{(T_{1})}\\nu-\\mu^{\\top}W_{K}^{(0)\\top}W_{K}^{(0)}\\nu\\right|=\\widetilde O(\\sigma_{0}^{2}\\sqrt{m}),\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Next, by Proposition E.12, Proposition E.5 and Lemma D.3, we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(T_{1})},a_{j_{2}}w_{j_{2}}^{(T_{1})}\\right\\rangle=\\Theta(\\sigma_{1}^{2}m m_{1}).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Recall that  \u2265'=1 91) 1 9(Ti) = (1). On the other hand, by Corllry E.10, Lemma D.3, we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\underset{\\mu\\in\\{\\mu_{i}\\}_{i=1}^{d}}{\\operatorname*{max}}\\left\\vert\\sum_{j=1}^{m_{1}}a_{j}\\frac{\\partial w_{j}^{(T_{1})}}{\\partial t}W_{V}^{(T_{1})}\\mu\\right\\vert=O(\\sigma_{0}^{2}m m_{1})}&\\\\ &{\\implies}&{\\frac{\\operatorname*{max}_{\\mu\\in\\{\\mu_{i}\\}_{i=1}^{d}}\\,\\left\\vert\\sum_{j=1}^{m_{1}}a_{j}\\frac{\\partial w_{j}^{(T_{1})}}{\\partial t}W_{V}^{(T_{1})}\\mu\\right\\vert}{\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(T_{1})},a_{j_{2}}w_{j_{2}}^{(T_{1})}\\right\\rangle}\\leq O\\left(\\frac{\\sigma_{0}^{2}}{\\sigma_{1}^{2}}\\right)=\\Tilde{O}\\left(\\frac{m_{1}}{L m}\\right).}&\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Also, by Corollary E.24, we have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\displaystyle\\operatorname*{max}_{i\\in[n]}\\left\\vert\\sum_{l_{1}=1}^{L}\\sum_{l_{2}=1}^{L}G^{(T_{1})}(X_{l_{2}}^{(i)})\\frac{\\partial p_{l_{1},l_{2}}^{(T_{1},i)}}{\\partial t}\\right\\vert=\\widetilde{O}\\left(\\frac{1}{\\sqrt{m}}\\right)}&\\\\ &{\\Longrightarrow}&{\\displaystyle\\frac{\\operatorname*{max}_{i\\in[n]}\\left\\vert\\sum_{l_{1}=1}^{L}\\sum_{l_{2}=1}^{L}G^{(T_{1})}(X_{l_{2}}^{(i)})\\frac{\\partial p_{l_{1},l_{2}}^{(T_{1},i)}}{\\partial t}\\right\\vert}{\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(T_{1})},a_{j_{2}}w_{j_{2}}^{(T_{1})}\\right\\rangle}\\leq\\widetilde{O}\\left(\\frac{1}{m^{3/2}}\\right)\\leq o(1)\\sum_{i\\in[n]}g_{i}^{(T_{1})}.}&\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Further, by Corollary E.24, we have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\substack{l_{1},l_{2}\\in[L],\\;i\\in[n]}}\\left|p_{l_{1},l_{2}}^{(T_{1},i)}-p_{l_{1},l_{2}}^{(0,i)}\\right|<O(1/L^{2}).\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Now, by Proposition E.21, if $F$ is small enough (which can be achieved by making the initialization scale small enough), then ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\frac{1}{2}<\\frac{\\sum_{i\\in I_{2}}g_{i}^{(T_{1})}}{\\sum_{i\\in I_{3}}g_{i}^{(T_{1})}}<2.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Lasty,by Thorem E.27 we hae $y_{i}F_{i}^{(T_{1})}\\geq\\Omega(1)$ Andiis sraghrwad to $|G^{(T_{1})}(\\mu)|\\leq$ ${\\cal O}(\\log m)$ for $\\mu\\in\\{\\mu_{i}\\}_{i=1}^{3}$ ", "page_idx": 46}, {"type": "text", "text": "Finaly we prove $\\begin{array}{r}{\\sum_{l_{1}=1}^{L}\\sum_{l_{2}:\\;X_{l_{2}}^{(i)}\\in\\{\\mu_{k}\\}_{k=4}^{d}}G^{(T_{1})}(X_{l_{2}}^{(i)})p_{q\\leftarrow l_{1},k\\leftarrow l_{2}}^{(T_{1},i)}\\leq O(1)}\\end{array}$ A simple corlary from Lemma D.7 and Lemma D.8 is that ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\left|\\sum_{l_{1}=1}^{L}\\sum_{l_{2}:\\;X_{l_{2}}^{(i)}\\in\\{\\mu_{k}\\}_{k=4}^{d}}G^{(0)}(X_{l_{2}}^{(i)})p_{q\\leftarrow l_{1},k\\leftarrow l_{2}}^{(0,i)}\\right|\\leq O(1).\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Next, we have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{l=1}^{L}\\displaystyle\\sum_{l_{2}:\\chi_{l_{2}^{(i)}}^{(i)}\\in\\{\\mu_{k}\\}_{k=4}^{d}}G^{(T_{1})}(X_{l_{2}}^{(i)})p_{\\notin-l_{1},k\\vdash l_{2}}^{(T_{1},i)}-\\displaystyle\\sum_{l_{1}=1}^{L}\\displaystyle\\sum_{l_{2}:\\chi_{l_{2}^{(i)}}^{(i)}\\in\\{\\mu_{k}\\}_{k=4}^{d}}G^{(0)}(X_{l_{2}}^{(i)})p_{\\notin-l_{1},k\\vdash l_{2}}^{(0,i)}}\\\\ &{\\le\\displaystyle\\sum_{l_{1}=1}^{L}\\displaystyle\\sum_{l_{2}:\\chi_{l_{2}^{(i)}}^{(i)}\\in\\{\\mu_{k}\\}_{k=4}^{d}}\\Big|\\big(G^{(T_{1})}(X_{l_{2}}^{(i)})-G^{(0)}(X_{l_{2}}^{(i)})\\big)\\Big|p_{\\notin-l_{1},k\\vdash l_{2}}^{(0,i)}}\\\\ &{\\quad+\\displaystyle\\sum_{l_{1}=1}^{L}\\displaystyle\\sum_{l_{2}:\\chi_{l_{2}^{(i)}}^{(i)}\\in\\{\\mu_{k}\\}_{k=4}^{d}}G^{(0)}(X_{l_{2}}^{(i)})\\left|p_{\\notin-l_{1},k\\vdash l_{2}}^{(T_{1},i)}-p_{\\notin-l_{1},k\\vdash l_{2}}^{(0,i)}\\right|}\\\\ &{\\quad+\\displaystyle\\sum_{l_{1}=1}^{L}\\displaystyle\\sum_{l_{2}:\\chi_{l_{2}^{(i)}}^{(i)}\\in\\{\\mu_{k}\\}_{k=4}^{d}}\\Big|G^{(T_{1})}(X_{l_{2}}^{(i)})-G^{(0)}(X_{l_{2}}^{(i)})\\Big|\\left|p_{\\notin-l_{1},k\\vdash l_{2}}^{(T_{1},i)}-p_{\\notin-l_{1},k\\vdash l_{2}}^{(0,i)}\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "which proves that ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\sum_{l_{1}=1}^{L}\\sum_{l_{2}:\\;X_{l_{2}}^{(i)}\\in\\{\\mu_{k}\\}_{k=4}^{d}}G^{(T_{1})}(X_{l_{2}}^{(i)})p_{q\\leftarrow l_{1},k\\leftarrow l_{2}}^{(T_{1},i)}\\leq O(1).\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "In Phase 2, we will analyze the dynamical system in a different way since all the variables now might change dramatically from their values at initialization. ", "page_idx": 46}, {"type": "text", "text": "Lemma F.4. For $t\\in[T_{1},T_{2}]$ we have ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial t}\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle=\\frac{2m_{1}}{n}\\sum_{i=1}^{n}g_{i}^{(t)}y_{i}F_{i}^{(t)}>0\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "and thus, ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle=\\Omega(\\sigma_{1}^{2}m m_{1}).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Proof. By Lemma C.2, we obtain ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial}{\\partial t}\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left<a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right>}\\\\ &{\\displaystyle=\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left(\\frac{1}{n}\\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\\sum_{l_{1}=1}^{L}a_{j_{1}}w_{j_{1}}^{(t)\\top}V^{(t,i)}p_{l_{1}}^{(t,i)}+\\frac{1}{n}\\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\\sum_{l_{2}=1}^{L}a_{j_{2}}w_{j_{2}}^{(t)\\top}V^{(t,i)}p_{l_{2}}^{(t,i)}\\right)}\\\\ &{\\displaystyle=\\frac{2m_{1}}{n}\\sum_{i=1}^{n}g_{i}^{(t)}y_{i}F_{i}^{(t)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Thn,ttha tisiw $y_{i}F_{i}^{(t)}>0$ for all $i\\in[n]$ Finally, by Theorem E.27, we have for all $t\\in[T_{1},T_{2}]$ \uff0c ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle\\geq\\Omega(\\sigma_{1}^{2}m m_{1}).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "F.1 Automatic Balancing of Gradients ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Lemma F.5 (Same as Lemma 4.4). For $t\\in[T_{1},T_{2}]$ , there exists a small constant $C\\ll1$ suchthat ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\frac{\\left|\\sum_{i\\in I_{2}}g_{i}^{(t)}-\\sum_{i\\in I_{3}}g_{i}^{(t)}\\right|}{\\operatorname*{min}(\\sum_{i\\in I_{2}}g_{i}^{(t)},\\sum_{i\\in I_{3}}g_{i}^{(t)})}\\leq C.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Proof. Without loss of enerality assume $\\begin{array}{r}{\\sum_{i\\in I_{2}}g_{i}^{(t)}>\\sum_{i\\in I_{3}}g_{i}^{(t)}}\\end{array}$ . Then, we have ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\frac{\\left|\\sum_{i\\in I_{2}}g_{i}^{(t)}-\\sum_{i\\in I_{3}}g_{i}^{(t)}\\right|}{\\operatorname*{min}(\\sum_{i\\in I_{2}}g_{i}^{(t)},\\sum_{i\\in I_{3}}g_{i}^{(t)})}=\\frac{\\sum_{i\\in I_{2}}g_{i}^{(t)}-\\sum_{i\\in I_{3}}g_{i}^{(t)}}{\\sum_{i\\in I_{3}}g_{i}^{(t)}}=\\frac{\\sum_{i\\in I_{2}}g_{i}^{(t)}}{\\sum_{i\\in I_{3}}g_{i}^{(t)}}-1.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Now by the quotient rule, we have $\\begin{array}{r}{\\frac{\\partial}{\\partial t}\\left(\\frac{\\sum_{i\\in I_{2}}g_{i}^{(t)}}{\\sum_{i\\in I_{3}}g_{i}^{(t)}}\\right)\\leq0}\\end{array}$ if and only if ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial}{\\partial t}\\left(\\sum_{i\\in I_{2}}g_{i}^{(t)}\\right)\\left(\\sum_{i\\in I_{3}}g_{i}^{(t)}\\right)-\\left(\\sum_{i\\in I_{2}}g_{i}^{(t)}\\right)\\frac{\\partial}{\\partial t}\\left(\\sum_{i\\in I_{3}}g_{i}^{(t)}\\right)\\leq0}\\\\ &{\\Leftrightarrow\\left(\\sum_{i\\in I_{2}}g^{\\prime}(y_{i}F_{i}^{(t)})\\frac{\\partial y_{i}F_{i}^{(t)}}{\\partial t}\\right)\\left(\\sum_{i\\in I_{3}}g_{i}^{(t)}\\right)-\\left(\\sum_{i\\in I_{2}}g_{i}^{(t)}\\right)\\left(\\sum_{i\\in I_{3}}g^{\\prime}(y_{i}F_{i}^{(t)})\\frac{\\partial y_{i}F_{i}^{(t)}}{\\partial t}\\right)\\leq0}\\\\ &{\\Leftrightarrow\\left(\\sum_{i\\in I_{2}}g_{i}^{(t)}\\right)\\left(\\sum_{i\\in I_{3}}g_{i}^{(t)}\\right)\\left(\\frac{1}{\\sum_{i\\in I_{2}}g_{i}^{(t)}}\\sum_{i\\in I_{2}}g^{\\prime}(y_{i}F_{i}^{(t)})\\frac{\\partial y_{i}F_{i}^{(t)}}{\\partial t}-\\frac{1}{\\sum_{i\\in I_{3}}g_{i}^{(t)}}\\sum_{i\\in I_{3}}g^{\\prime}(y_{i}F_{i}^{(t)})\\frac{\\partial y_{i}F_{i}^{(t)}}{\\partial t}\\right.}\\\\ &{\\Leftrightarrow\\left(\\frac{1}{\\sum_{i\\in I_{2}}g_{i}^{(t)}}\\sum_{i\\in I_{2}}g^{\\prime}(y_{i}F_{i}^{(t)})\\frac{\\partial y_{i}F_{i}^{(t)}}{\\partial t}-\\frac{1}{\\sum_{i\\in I_{3}}g_{i}^{(t)}}\\sum_{i\\in I_{3}}g^{\\prime}(y_{i}F_{i}^{(t)})\\frac{\\partial y_{i}F_{i}^{(t)}}{\\partial t}\\right)\\leq0}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Leftrightarrow\\left(\\frac{1}{\\sum_{i\\in I_{0}}g_{i}^{(t)}}\\sum_{i\\in I_{2}}\\frac{-1}{(1+\\exp(y_{i}F_{i}^{(t)}))(1+\\exp(-y_{i}F_{i}^{(t)}))}\\frac{\\partial y_{i}F_{i}^{(t)}}{\\partial t}\\right)}\\\\ &{\\quad-\\left(\\frac{1}{\\sum_{i\\in I_{3}}g_{i}^{(t)}}\\sum_{i\\in I_{3}}\\frac{-1}{(1+\\exp(y_{i}F_{i}^{(t)}))(1+\\exp(-y_{i}F_{i}^{(t)}))}\\frac{\\partial y_{i}F_{i}^{(t)}}{\\partial t}\\right)\\leq0}\\\\ &{\\Leftrightarrow\\left(\\frac{1}{\\sum_{i\\in I_{3}}g_{i}^{(t)}}\\sum_{i\\in I_{3}}\\frac{1}{(1+\\exp(y_{i}F_{i}^{(t)}))(1+\\exp(-y_{i}F_{i}^{(t)}))}\\frac{\\partial y_{i}F_{i}^{(t)}}{\\partial t}\\right)}\\\\ &{\\quad\\leq\\left(\\frac{1}{\\sum_{i\\in I_{2}}g_{i}^{(t)}}\\sum_{i\\in I_{2}}\\frac{1}{(1+\\exp(y_{i}F_{i}^{(t)}))(1+\\exp(-y_{i}F_{i}^{(t)}))}\\frac{\\partial y_{i}F_{i}^{(t)}}{\\partial t}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Note that ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\frac{\\sum_{i\\in I}g_{i}^{(t)}}{\\mid+\\operatorname*{min}_{i\\in I}\\exp(-y_{i}F_{i}^{(t)})}\\ge\\sum_{i\\in I}\\frac{1}{(1+\\exp(y_{i}F_{i}^{(t)}))(1+\\exp(-y_{i}F_{i}^{(t)}))}\\ge\\frac{\\sum_{i\\in I}g_{i}^{(t)}}{1+\\operatorname*{max}_{i\\in I}\\exp(-y_{i}F_{i}^{(t)})}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "By Definition F.1, we have that for $i^{\\prime}\\in I_{2}$ \uff0c ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle=y_{i^{\\prime}}\\sum_{l_{1}=1}^{L}\\sum_{j=1}^{m_{1}}\\sum_{l_{2}=1}^{L}\\left(\\frac{\\partial a_{j}w_{j}^{(t)}W_{V}^{(t)}X_{l_{2}}^{(t^{\\prime})}}{\\partial t}p_{l_{1},l_{2}}^{(t,i^{\\prime})}+a_{j}w_{j}^{(t)}W_{V}^{(t)}X_{l_{2}}^{(i^{\\prime})}\\frac{\\partial p_{l_{1},l_{2}}^{(t,i^{\\prime})}}{\\partial t}\\right)}\\\\ &{\\displaystyle=y_{i^{\\prime}}\\sum_{l_{2}=1}^{L}\\frac{\\partial G^{(t)}(X_{l_{2}}^{(t^{\\prime})})}{\\partial t}(1+o(1))+\\sum_{l_{1}=1}^{L}\\sum_{l_{2}=1}^{L}y_{i^{\\prime}}G^{(t)}(X_{l_{2}}^{(t^{\\prime})})\\frac{\\partial p_{l_{1},l_{2}}^{(t,i^{\\prime})}}{\\partial t}}\\\\ &{\\displaystyle=-\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle\\frac{1}{n}\\left(\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{2}\\cup I_{3}\\cup I_{4}}g_{i}^{(t)}+\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{2}}g_{i}^{(t)}+o(1)\\sum_{i\\in[n]}g_{i}^{(t)}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Similarly, for $i^{\\prime}\\in I_{3}$ , we have ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\partial t}}\\\\ {{\\displaystyle=-\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left<a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right>\\frac1n\\left(\\displaystyle\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{2}\\cup I_{3}\\cup I_{4}}g_{i}^{(t)}+\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{3}}g_{i}^{(t)}+o(1)\\sum_{i\\in[n]}g_{i}^{(t)}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Now, we analyze Equation (12). Note that for $t\\ \\in\\ [T_{1},T_{2}]$ if $\\textstyle\\sum_{i\\in I_{2}}g_{i}^{(t)}$ is sufciently larger than ie1s 9 (i.e, $\\begin{array}{r}{\\sum_{i\\in I_{2}}g_{i}^{(t)}/\\sum_{i\\in I_{3}}g_{i}^{(t)}}\\end{array}$ is bigger than some treshold, thn $\\frac{\\partial G^{(t)}(\\mu_{1})}{\\partial t}\\;<$ aG(t)(\u03bc2) and minie I2 $\\begin{array}{r}{\\operatorname*{min}_{i\\in I_{2}}\\,\\frac{\\partial y_{i}F_{i}^{(t)}}{\\partial t}\\ >\\ \\operatorname*{max}_{i\\in I_{3}}\\frac{\\partial y_{i}F_{i}^{(t)}}{\\partial t}}\\end{array}$ Thus, the ratio will decrease. Similarly, $\\begin{array}{r}{\\sum_{i\\in I_{2}}g_{i}^{(t)}/\\sum_{i\\in I_{3}}g_{i}^{(t)}}\\end{array}$ is too small, the ratio will increase. Next, we compute a bound on $\\begin{array}{r}{\\sum_{i\\in I_{2}}g_{i}^{(t)}/\\sum_{i\\in I_{3}}g_{i}^{(t)}}\\end{array}$ s that $\\begin{array}{r}{\\frac{\\partial}{\\partial t}\\left(\\frac{\\sum_{i\\in I_{2}}g_{i}^{(t)}}{\\sum_{i\\in I_{3}}g_{i}^{(t)}}\\right)\\;=\\;0}\\end{array}$ .Substituting dyir(t) in Equation (12), we have ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Bigg(\\frac{1}{\\sum_{i\\in I_{3}}g_{i}^{(t)}}\\sum_{i\\in I_{3}}\\frac{1}{(1+\\exp(y_{i}F_{i}^{(t)}))(1+\\exp(-y_{i}F_{i}^{(t)}))}\\Bigg)}\\\\ &{\\quad\\cdot\\left(-\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle\\left(\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{2}\\cup I_{3}\\cup I_{4}}g_{i}^{(t)}+\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{2}}g_{i}^{(t)}\\pm o(1)\\sum_{i\\in[n]}g_{i}^{(t)}\\right)\\right)}\\\\ &{=\\left(\\frac{1}{\\sum_{i\\in I_{2}}g_{i}^{(t)}}\\sum_{i\\in I_{2}}\\frac{1}{(1+\\exp(y_{i}F_{i}^{(t)}))(1+\\exp(-y_{i}F_{i}^{(t)}))}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\cdot\\left(-\\displaystyle\\sum_{j=1}^{m_{1}}\\sum_{j=1}^{m_{1}}\\left\\langle a_{j}\\mathfrak{w}_{j_{1}}^{(0)},a_{j}\\mathfrak{w}_{j_{2}}^{(0)}\\right\\rangle\\left(\\displaystyle\\sum_{i\\in\\mathcal{C}_{j_{1}}}\\mathfrak{z}_{i}^{(i)}-\\displaystyle\\sum_{i\\in\\mathcal{C}_{j_{2}}\\cap\\mathcal{V}_{i}}\\mathfrak{z}_{i}^{(i)}+\\displaystyle\\sum_{i\\in\\mathcal{C}_{j_{1}}}\\mathfrak{z}_{i}^{(i)}-\\sum_{i\\in\\mathcal{C}_{j_{1}}}\\mathfrak{z}_{i}^{(i)}\\pm\\alpha(1)\\sum_{i\\in\\mathcal{C}_{j_{2}}}\\mathfrak{z}_{i}^{(i)}\\right.\\right.}\\\\ &{\\left.\\left.\\Rightarrow\\displaystyle\\frac{1}{1+\\operatorname*{min}_{\\xi\\in\\mathcal{C}_{j_{2}}\\cap\\mathcal{V}_{i}}(-y_{i},F_{i}^{(i)})}\\left(-\\sum_{i\\in\\mathcal{C}_{i_{1}}}\\mathfrak{z}_{i}^{(i)}+\\displaystyle\\sum_{i\\in\\mathcal{C}_{j_{2}}\\cap\\mathcal{V}_{i}}\\mathfrak{z}_{i}^{(i)}-\\sum_{i\\in\\mathcal{C}_{i_{1}}}\\mathfrak{z}_{i}^{(i)}+\\sum_{i\\in\\mathcal{C}_{j_{2}}}\\mathfrak{z}_{i}^{(i)}\\pm\\alpha(1)\\sum_{i\\in\\mathcal{C}_{j_{1}}}\\mathfrak{z}_{i}^{(i)}\\right.\\right.}\\\\ &{\\left.\\left.\\quad=\\left(-\\displaystyle\\sum_{i\\in\\mathcal{C}_{j_{1}}}\\mathfrak{z}_{i}^{(i)}+\\displaystyle\\sum_{i\\in\\mathcal{C}_{j_{2}}\\cap\\mathcal{V}_{i}}\\mathfrak{z}_{i}^{(i)}-\\displaystyle\\sum_{i\\in\\mathcal{C}_{j_{1}}}\\mathfrak{z}_{i}^{(i)}+\\displaystyle\\sum_{i\\in\\mathcal{C}_{j_{2}}}\\mathfrak{z}_{i}^{(i)}\\pm\\alpha(1)\\sum_{i\\in\\mathcal{C}_{j_{1}}}\\mathfrak{z}_{i}^{(i)}\\right)\\right.}\\\\ &{\\left.\\Rightarrow\\displaystyle\\sum_{k\\in\\mathcal{C}_{j_{2}}}\\mathfrak{z}_{i}^{(i)}=1+ \n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Since ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{\\sum_{i\\in I}g_{i}^{(t)}}\\displaystyle\\sum_{i\\in I}\\frac{1}{(1+\\exp(y_{i}F_{i}^{(t)}))(1+\\exp(-y_{i}F_{i}^{(t)}))}}\\\\ &{\\qquad\\in\\left[\\frac{1}{1+\\operatorname*{max}_{i\\in I}\\exp(-y_{i}F_{i}^{(t)})},\\frac{1}{1+\\operatorname*{min}_{i\\in I}\\exp(-y_{i}F_{i}^{(t)})}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "we have ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{\\sum_{i\\in I_{3}}g_{i}^{(t)}}\\sum_{i\\in I_{3}}\\frac{1}{(1+\\exp(y_{i}F_{i}^{(t)}))(1+\\exp(-y_{i}F_{i}^{(t)}))}}\\\\ &{\\frac{1}{\\sum_{i\\in I_{2}}g_{i}^{(t)}}\\sum_{i\\in I_{2}}\\frac{1}{(1+\\exp(y_{i}F_{i}^{(t)}))(1+\\exp(-y_{i}F_{i}^{(t)}))}}\\\\ &{\\qquad\\in\\left[\\frac{1+\\operatorname*{min}_{i\\in I_{2}}\\exp\\left(-y_{i}F_{i}^{(t)}\\right)}{1+\\operatorname*{max}_{i\\in I_{3}}\\exp\\left(-y_{i}F_{i}^{(t)}\\right)},\\frac{1+\\operatorname*{max}_{i\\in I_{2}}\\exp\\left(-y_{i}F_{i}^{(t)}\\right)}{1+\\operatorname*{min}_{i\\in I_{3}}\\exp\\left(-y_{i}F_{i}^{(t)}\\right)}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "By Definition F.1, we have ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\left|\\frac{-\\sum_{i\\in I_{1}}g_{i}^{(t)}+\\sum_{i\\in I_{2}\\cup I_{3}\\cup I_{4}}g_{i}^{(t)}-\\sum_{i\\in I_{1}}g_{i}^{(t)}+\\sum_{i\\in I_{2}}g_{i}^{(t)}\\pm o(1)\\sum_{i\\in[n]}g_{i}^{(t)}}{\\sum_{i\\in I_{3}}g_{i}^{(t)}}\\right|\\leq6.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Thus, ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\frac{\\sum_{i\\in I_{2}}g_{i}^{(t)}}{\\sum_{i\\in I_{3}}g_{i}^{(t)}}\\in1\\pm6\\cdot\\left(\\left[\\frac{1+\\operatorname*{min}_{i\\in I_{2}}\\exp(-y_{i}F_{i}^{(t)})}{1+\\operatorname*{max}_{i\\in I_{3}}\\exp(-y_{i}F_{i}^{(t)})},\\frac{1+\\operatorname*{max}_{i\\in I_{2}}\\exp(-y_{i}F_{i}^{(t)})}{1+\\operatorname*{min}_{i\\in I_{3}}\\exp(-y_{i}F_{i}^{(t)})}\\right]-1\\right).\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Lemma F.6 (Complete version of Lemma 4.5). For $t\\in[T_{1},T_{2}]$ wehave ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{\\sum_{i\\in[n]}g_{i}^{(t)}}{\\sum_{i\\in I_{2}\\cup I_{3}\\cup I_{4}}g_{i}^{(t)}-\\sum_{i\\in I_{1}}g_{i}^{(t)}}=O(1),\\quad}&{\\frac{\\sum_{i\\in[n]}g_{i}^{(t)}}{\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{2}}g_{i}^{(t)}}=O(1)}\\\\ {\\frac{\\sum_{i\\in[n]}g_{i}^{(t)}}{\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{3}}g_{i}^{(t)}}=O(1),\\quad}&{\\frac{\\frac{\\partial G^{(t)}(\\mu_{1})}{\\partial t}}{\\frac{\\partial G^{(t)}(\\mu_{2})}{\\partial t}}=\\Theta(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Further,thereexists aconstant $C$ suchthat ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\mathrm{1}+C)\\operatorname*{max}\\left(\\frac{\\partial G^{(t)}(\\mu_{1})}{\\partial t},\\frac{\\partial G^{(t)}(\\mu_{2})}{\\partial t}\\right)\\leq-\\frac{\\partial G^{(t)}(\\mu_{3})}{\\partial t}\\leq(1-C)\\left(\\frac{\\partial G^{(t)}(\\mu_{1})}{\\partial t}+\\frac{\\partial G^{(t)}(\\mu_{2})}{\\partial t}\\right).\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Proof. Without loss of generality, assume $\\frac{\\partial G^{(t)}(\\mu_{1})}{\\partial t}\\,>\\,\\frac{\\partial G^{(t)}(\\mu_{2})}{\\partial t}$ aG?(\u03bc2). First of all, by Definition F.1, we have ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\frac{\\partial G^{(t)}(\\mu_{1})}{\\partial t}=\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle\\frac{1}{n}\\left(\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{2}}g_{i}^{(t)}\\pm o(1)\\sum_{i\\in[n]}g_{i}^{(t)}\\right),}\\\\ {\\frac{\\partial G^{(t)}(\\mu_{2})}{\\partial t}=\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle\\frac{1}{n}\\left(\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{3}}g_{i}^{(t)}\\pm o(1)\\sum_{i\\in[n]}g_{i}^{(t)}\\right),}\\\\ {\\frac{\\partial G^{(t)}(\\mu_{3})}{\\partial t}=\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle\\frac{1}{n}\\left(\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{2}\\cup I_{3}\\cup I_{4}}g_{i}^{(t)}\\pm o(1)\\sum_{i\\in[n]}g_{i}^{(t)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "This implies that ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r l}&{\\frac{\\partial G^{(t)}(\\mu_{1})}{\\partial t}=\\frac{\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{2}}g_{i}^{(t)}\\pm o\\left(1\\right)\\sum_{i\\in[n]}g_{i}^{(t)}}{\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{3}}g_{i}^{(t)}\\pm o\\left(1\\right)\\sum_{i\\in[n]}g_{i}^{(t)}},}\\\\ &{\\frac{-\\frac{\\partial G^{(t)}(\\mu_{3})}{\\partial t}}{\\partial t}=\\frac{-\\sum_{i\\in I_{1}}g_{i}^{(t)}+\\sum_{i\\in I_{2}\\cup I_{3}\\cup I_{4}}g_{i}^{(t)}\\pm o\\left(1\\right)\\sum_{i\\in[n]}g_{i}^{(t)}}{\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{2}}g_{i}^{(t)}\\pm o\\left(1\\right)\\sum_{i\\in[n]}g_{i}^{(t)}}.}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Wenext analyze $\\frac{\\partial}{\\partial t}\\,\\frac{-\\frac{\\partial G^{(t)}(\\mu_{3})}{\\partial t}}{\\frac{\\partial G^{(t)}(\\mu_{1})}{\\partial t}}$ We frst define the ratio $\\begin{array}{r}{R(t)=\\frac{\\sum_{i\\in I_{2}\\cup I_{3}\\cup I_{4}}g_{i}^{(t)}-\\sum_{i\\in I_{1}}g_{i}^{(t)}}{\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{2}}g_{i}^{(t)}}}\\end{array}$ .Since the dependence of $R$ on $t$ is clear and for the ease of notation, we omit this dependence below. Rearranging this definition, we obtain ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\left(1+R\\right)\\left(\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{2}}g_{i}^{(t)}\\right)=\\sum_{i\\in I_{3}\\cup I_{4}}g_{i}^{(t)}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "We consider the range of $R\\in[1,3]$ . By Definition F.1, we have ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\frac{\\sum_{i\\in[n]}g_{i}^{(t)}}{\\sum_{i\\in I_{2}\\cup I_{3}\\cup I_{4}}g_{i}^{(t)}-\\sum_{i\\in I_{1}}g_{i}^{(t)}}=O(1),\\qquad\\frac{\\sum_{i\\in[n]}g_{i}^{(t)}}{\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{2}}g_{i}^{(t)}}=O(1),\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where the second equation follows from Lemma F.5. This implies that ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\frac{\\frac{\\partial G^{(t)}(\\mu_{1})}{\\partial t}}{\\frac{\\partial G^{(t)}(\\mu_{2})}{\\partial t}}=\\Theta(1),\\qquad\\frac{\\sum_{i\\in[n]}g_{i}^{(t)}}{\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{3}}g_{i}^{(t)}}=O(1),\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "which proves the first result and ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\frac{-\\frac{\\partial G^{(t)}(\\mu_{3})}{\\partial t}}{\\frac{\\partial G^{(t)}(\\mu_{1})}{\\partial t}}=\\frac{-\\sum_{i\\in I_{1}}g_{i}^{(t)}+\\sum_{i\\in I_{2}\\cup I_{3}\\cup I_{4}}g_{i}^{(t)}}{\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{2}}g_{i}^{(t)}}+o(1).\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Thus, to analyze $\\frac{\\partial}{\\partial t}\\,\\frac{-\\frac{\\partial G^{(t)}(\\mu_{3})}{\\partial t}}{\\frac{\\partial G^{(t)}(\\mu_{1})}{\\partial t}}$ , we can instead analyze ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial}{\\partial t}\\frac{\\sum_{i\\in I_{2}\\cup I_{3}\\cup I_{4}}g_{i}^{(t)}-\\sum_{i\\in I_{1}}g_{i}^{(t)}}{\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{2}}g_{i}^{(t)}}\\ge0}\\\\ &{\\displaystyle\\Leftrightarrow\\frac{\\partial}{\\partial t}\\left(\\sum_{i\\in I_{2}\\cup I_{3}\\cup I_{4}}g_{i}^{(t)}-\\sum_{i\\in I_{1}}g_{i}^{(t)}\\right)\\left(\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{2}}g_{i}^{(t)}\\right)}\\\\ &{\\displaystyle\\ \\ -\\left(\\sum_{i\\in I_{2}\\cup I_{3}\\cup I_{4}}g_{i}^{(t)}-\\sum_{i\\in I_{1}}g_{i}^{(t)}\\right)\\frac{\\partial}{\\partial t}\\left(\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{2}}g_{i}^{(t)}\\right)\\ge0}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "equation", "text": "$$\n\\Leftrightarrow\\sum_{i\\in I_{2}\\cup I_{3}\\cup I_{4}}\\frac{\\partial g_{i}^{(t)}}{\\partial t}+R\\sum_{i\\in I_{2}}\\frac{\\partial g_{i}^{(t)}}{\\partial t}\\geq(1+R)\\sum_{i\\in I_{1}}\\frac{\\partial g_{i}^{(t)}}{\\partial t}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Recall that by Definition F.1, we have for $i_{1}\\in I_{1}$ ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\frac{\\partial y_{i_{1}}F_{i_{1}}^{(t)}}{\\partial t}=\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left<a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right>\\frac{1}{n}\\left(3\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{2}\\cup I_{3}\\cup I_{4}}g_{i}^{(t)}-\\sum_{i\\in I_{2}\\cup I_{3}}g_{i}^{(t)}+o(1)\\sum_{i\\in[n]}g_{i}^{(t)}\\right)\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "for $i_{2}\\in I_{2}$ \uff0c", "page_idx": 51}, {"type": "equation", "text": "$$\n\\frac{\\partial y_{i_{2}}F_{i_{2}}^{(t)}}{\\partial t}=-\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle\\frac{1}{n}\\left(\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{2}\\cup I_{3}\\cup I_{4}}g_{i}^{(t)}+\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{2}}g_{i}^{(t)}+o(\\mathbb{R}^{n})\\right),\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "for $i_{3}\\in I_{3}$ \uff0c", "page_idx": 51}, {"type": "equation", "text": "$$\n\\frac{\\partial y_{i_{3}}F_{i_{3}}^{(t)}}{\\partial t}=-\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle\\frac{1}{n}\\left(\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{2}\\cup I_{3}\\cup I_{4}}g_{i}^{(t)}+\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{3}}g_{i}^{(t)}+o(\\mathbf{I}_{2}\\cup I_{3}\\cup I_{4})\\right).\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "and for $i_{4}\\in I_{4}$ ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\frac{\\partial y_{i_{4}}F_{i_{4}}^{(t)}}{\\partial t}=-\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle\\frac{1}{n}\\left(\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{2}\\cup I_{3}\\cup I_{4}}g_{i}^{(t)}+o(1)\\sum_{i\\in[n]}g_{i}^{(t)}\\right).\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "The above implies that for $i_{1}\\in I_{1}$ ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\frac{\\frac{\\partial y_{i_{1}}F_{i_{1}}^{(t)}}{\\partial t}}{\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{2}}g_{i}^{(t)}}=\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle\\frac{1}{n}\\left(1+\\frac{\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{3}}g_{i}^{(t)}}{\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{2}}g_{i}^{(t)}}-R+o(\\theta)\\frac{\\sum_{i\\in I_{1}}g_{i}^{(t)}}{\\sum_{i\\in I_{2}}g_{i}^{(t)}}\\right)\\frac{1}{n}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "for $i_{2}\\in I_{2}$ \uff0c", "page_idx": 51}, {"type": "equation", "text": "$$\n\\frac{\\frac{\\partial y_{i_{2}}F_{i_{2}}^{\\left(t\\right)}}{\\partial t}}{\\sum_{i\\in I_{1}}g_{i}^{\\left(t\\right)}-\\sum_{i\\in I_{2}}g_{i}^{\\left(t\\right)}}=-\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{\\left(t\\right)},a_{j_{2}}w_{j_{2}}^{\\left(t\\right)}\\right\\rangle\\frac{1}{n}\\left(1-R+o(1)\\right);\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "for $i_{3}\\in I_{3}$ \uff0c", "page_idx": 51}, {"type": "equation", "text": "$$\n\\frac{\\frac{\\partial y_{i_{3}}F_{i_{3}}^{(t)}}{\\partial t}}{\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{2}}g_{i}^{(t)}}=-\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle\\frac{1}{n}\\left(\\frac{\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{3}}g_{i}^{(t)}}{\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{2}}g_{i}^{(t)}}-R+o(1)\\right)\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "and for $i_{4}\\in I_{4}$ \uff0c ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\frac{\\frac{\\partial y_{i_{1}}F_{i_{1}}^{\\left(t\\right)}}{\\partial t}}{\\sum_{i\\in I_{1}}g_{i}^{\\left(t\\right)}-\\sum_{i\\in I_{2}}g_{i}^{\\left(t\\right)}}=-\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{\\left(t\\right)},a_{j_{2}}w_{j_{2}}^{\\left(t\\right)}\\right\\rangle\\frac{1}{n}\\left(-R+o(1)\\right).\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Substituting the above into Equation (15) and divide both sides by $\\begin{array}{r}{\\frac{1}{n}\\sum_{j_{1}=1}^{m_{1}}\\bar{\\sum_{j_{2}=1}^{m_{1}}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle}\\end{array}$ wehave ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{(1+R)\\displaystyle\\sum_{i_{2}\\in I_{2}}g^{\\prime}(F_{i_{2}}^{(t)})(R-1+o(1))+\\displaystyle\\sum_{i_{3}\\in I_{3}}g^{\\prime}(F_{i_{3}}^{(t)})\\left(R-\\displaystyle\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{3}}g_{i}^{(t)}+o(1)\\right)}&\\\\ &{\\quad+\\displaystyle\\sum_{i_{4}\\in I_{4}}g^{\\prime}(F_{i_{4}}^{(t)})(R+o(1))}&\\\\ &{\\geq(1+R)\\displaystyle\\sum_{i_{1}\\in I_{1}}g^{\\prime}(F_{i_{1}}^{(t)})\\left(1+\\displaystyle\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{3}}g_{i}^{(t)}-R+o(1)\\right).}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "By Lemma F.5, we obtain ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\left|\\frac{\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{3}}g_{i}^{(t)}}{\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{2}}g_{i}^{(t)}}-1\\right|\\leq\\varepsilon,\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "whereweuse $\\varepsilon$ to denote the small deviation. Note that Equation (16) is a quadratic inequality in $R$ and can be rearranged as $a R^{2}+b R+c\\geq0$ where ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle a=\\sum_{i_{1}\\in I_{1}}g^{\\prime}(F_{i_{1}}^{(t)})+\\sum_{i_{2}\\in I_{2}}g^{\\prime}(F_{i_{2}}^{(t)}),}\\\\ {\\displaystyle b=\\sum_{i_{3}\\in I_{3}}g^{\\prime}(F_{i_{3}}^{(t)})+\\sum_{i_{4}\\in I_{4}}g^{\\prime}(F_{i_{4}}^{(t)})-\\sum_{i_{1}\\in I_{1}}g^{\\prime}(F_{i_{1}}^{(t)}),}\\\\ {\\displaystyle c=(1+o(1))\\left(-\\sum_{i_{2}\\in I_{2}}g^{\\prime}(F_{i_{2}}^{(t)})-(1\\pm\\varepsilon)\\sum_{i_{3}\\in I_{3}}g^{\\prime}(F_{i_{3}}^{(t)})-(2\\pm\\varepsilon)\\sum_{i_{1}\\in I_{1}}g^{\\prime}(F_{i_{1}}^{(t)})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Now we analyze the equality condition in Equation (16) where we calculate the root of the equation $a R^{2}+b R+c=0$ Wehave ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{b}{a}=\\frac{\\sum_{i_{3}\\in I_{3}}g^{\\prime}(F_{i_{3}}^{(t)})+\\sum_{i_{4}\\in I_{4}}g^{\\prime}(F_{i_{4}}^{(t)})-\\sum_{i_{1}\\in I_{1}}g^{\\prime}(F_{i_{1}}^{(t)})}{\\sum_{i_{1}\\in I_{1}}g^{\\prime}(F_{i_{1}}^{(t)})+\\sum_{i_{2}\\in I_{2}}g^{\\prime}(F_{i_{2}}^{(t)})},}\\\\ &{\\frac{c}{a}=\\frac{(1+o(1))\\left(-\\sum_{i_{2}\\in I_{2}}g^{\\prime}(F_{i_{2}}^{(t)})-(1\\pm\\varepsilon)\\sum_{i_{3}\\in I_{3}}g^{\\prime}(F_{i_{3}}^{(t)})-(2\\pm\\varepsilon)\\sum_{i_{1}\\in I_{1}}g^{\\prime}(F_{i_{1}}^{(t)})\\right)}{\\sum_{i_{1}\\in I_{1}}g^{\\prime}(F_{i_{1}}^{(t)})+\\sum_{i_{2}\\in I_{2}}g^{\\prime}(F_{i_{2}}^{(t)})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Note that ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\sum_{i\\in I}g_{i}^{(t)}\\operatorname*{min}_{i\\in I}\\frac{1}{1+\\exp(y_{i}F_{i}^{(t)})}\\leq-\\sum_{i\\in I}g^{\\prime}(F_{i}^{(t)})\\leq\\sum_{i\\in I}g_{i}^{(t)}\\operatorname*{max}_{i\\in I}\\frac{1}{1+\\exp(y_{i}F_{i}^{(t)})}.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "By Equation (14), we have $-{\\frac{b}{2a}}\\geq-1/4$ and ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\frac{b}{a}\\Big|\\leq(1+O(\\operatorname*{max}_{i\\in[n]}\\exp(-y_{i}F_{i}^{(t)})))\\operatorname*{max}\\left(\\left|\\frac{\\sum_{i_{1}\\in I_{1}}g(F_{i_{1}}^{(t)})-\\sum_{i_{3}\\in I_{3}}g(F_{i_{3}}^{(t)})}{\\sum_{i_{1}\\in I_{1}}g(F_{i_{1}}^{(t)})+\\sum_{i_{2}\\in I_{2}}g(F_{i_{2}}^{(t)})}\\right|,\\left|\\frac{\\sum_{i_{4}\\in I_{4}}g(F_{i_{3}}^{(t)})}{\\sum_{i_{1}\\in I_{1}}g(F_{i_{1}}^{(t)})+\\sum_{i_{2}\\in I_{3}}g(F_{i_{2}}^{(t)})}\\right|\\right)\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "and ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\left|{\\frac{c}{a}}\\right|=(1\\pm O(\\varepsilon)\\pm O(\\operatorname*{max}_{i\\in[n]}\\exp(-y_{i}F_{i}^{(t)})))\\cdot2.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Recall that we are considering the case of $R\\ \\in\\ [1,3]$ . Thus, we only need to consider the root that is positive and we can calculate the root $\\begin{array}{r}{R^{\\star}\\,=\\,-\\frac{b}{2a}\\,+\\,\\sqrt{\\frac{b^{2}}{4a^{2}}\\,-\\,\\frac{c}{a}}\\ \\in\\ (1,2)}\\end{array}$ $\\varepsilon$ and $\\mathrm{max}_{i\\in[n]}\\exp(-y_{i}F_{i}^{(t)})$ are both sufently small. ", "page_idx": 52}, {"type": "text", "text": "Next, since $g^{\\prime}(F_{i}^{(t)})<0$ the root $R^{\\star}$ is contractive (i. if $R(t)>R^{\\star}$ then $R(t)$ is decreasing and if $R(t)<R^{\\star}$ then $R(t)$ is increasing). ", "page_idx": 52}, {"type": "text", "text": "Finally, the result at the end of Phase 1 mlies that $R(T_{1})\\in[1,3]$ , which completes the proof. Corollary F.7. For $t\\in[T_{1},T_{2}]$ wehave ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\frac{G^{(t)}(\\mu_{1})}{G^{(t)}(\\mu_{2})},\\frac{G^{(t)}(\\mu_{1})}{-G^{(t)}(\\mu_{3})},\\frac{G^{(t)}(\\mu_{2})}{-G^{(t)}(\\mu_{3})}=\\Theta(1).\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Proof. We first prove $\\frac{G^{(t)}(\\mu_{1})}{G^{(t)}(\\mu_{2})}=\\Theta(1)$ . Following from Lemma F.6, we have $\\frac{\\frac{\\partial G^{(t)}(\\mu_{1})}{\\partial t}}{\\frac{\\partial G^{(t)}(\\mu_{2})}{\\partial t}}=\\Theta(1)$ .By Theorem E.27, we have $\\frac{G^{(T_{1})}(\\mu_{1})}{G^{(T_{1})}(\\mu_{2})}=\\Theta(1)$ Thus, for $t\\in[T_{1},T_{2}]$ , we obtain ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\frac{G^{(t)}(\\mu_{1})}{G^{(t)}(\\mu_{2})}=\\frac{G^{(T_{1})}(\\mu_{1})+\\int_{T_{1}}^{t}\\frac{\\partial G^{(\\tau)}(\\mu_{1})}{\\partial\\tau}\\;d\\tau}{G^{(T_{1})}(\\mu_{2})+\\int_{T_{1}}^{t}\\frac{\\partial G^{(\\tau)}(\\mu_{2})}{\\partial\\tau}\\;d\\tau}=\\Theta(1).\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Note that Lemma F.6 and Definition F.1 imply that $\\frac{\\frac{\\partial G^{(t)}(\\mu_{1})}{\\partial t}}{-\\frac{\\partial G^{(t)}(\\mu_{3})}{\\partial t}}=\\Theta(1)$ . Since $\\frac{G^{(T_{1})}(\\mu_{1})}{-G^{(T_{1})}(\\mu_{3})}=\\Theta(1)$ similarly, we have G(6(\u03bc3) $\\frac{G^{(t)}(\\mu_{1})}{-G^{(t)}(\\mu_{3})}\\,=\\,\\Theta(1)$ = O(1). Finally, Go(\u03bc2) $\\frac{G^{(t)}(\\mu_{1})}{G^{(t)}(\\mu_{2})}\\,=\\,\\Theta(1)$ and $\\frac{G^{(t)}(\\mu_{1})}{-G^{(t)}(\\mu_{3})}\\,=\\,\\Theta(1)$ imply that $\\frac{G^{(t)}(\\mu_{2})}{-G^{(t)}(\\mu_{3})}=\\Theta(1)$ \u53e3 ", "page_idx": 53}, {"type": "text", "text": "F.2How Fast the Loss Decreases ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Theorem F.8. For $t\\in[T_{1},T_{2}]$ , we have ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\widehat{L}(t)=\\frac{1}{\\Theta(\\sigma_{1}^{2}m m_{1})(t-T_{1})+(1/\\widehat{L}(T_{1}))}.\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "ProfFisfalgadwatefortil lsis $\\begin{array}{r l}{\\frac{\\partial\\widehat{L}}{\\partial t}}&{{}=}\\end{array}$ $\\textstyle\\sum_{i=1}^{n}\\ell^{\\prime}\\bigl(y_{i}F_{i}^{(t)}\\bigr)\\frac{\\partial y_{i}F_{i}^{(t)}}{\\partial t}$ . By Definition F.1, we have for $i_{1}\\in I_{1}$ ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\frac{F_{i_{1}}^{(t)}}{\\gamma_{t}}=\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle\\frac{1}{n}\\left(3\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{2}\\cup I_{3}\\cup I_{4}}g_{i}^{(t)}-\\sum_{i\\in I_{2}\\cup I_{3}}g_{i}^{(t)}+o(1)\\sum_{i\\in[n]}g_{i}^{(t)}\\right)\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "for $i_{2}\\in I_{2}$ \uff0c", "page_idx": 53}, {"type": "equation", "text": "$$\n\\frac{F_{i_{2}}^{(t)}}{j t}=-\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle\\frac{1}{n}\\left(\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{2}\\cup I_{3}\\cup I_{4}}g_{i}^{(t)}+\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{2}}g_{i}^{(t)}+o(1)\\right),\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "for $i_{3}\\in I_{3}$ ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\frac{\\partial y_{i_{3}}F_{i_{3}}^{(t)}}{\\partial t}=-\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle\\frac{1}{n}\\left(\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{2}\\cup I_{3}\\cup I_{4}}g_{i}^{(t)}+\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{3}}g_{i}^{(t)}+o(\\mathbf{I}_{2}\\cup I_{3}\\cup I_{4})\\right).\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "and for $i_{4}\\in I_{4}$ \uff0c ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\frac{\\partial y_{i_{4}}F_{i_{4}}^{(t)}}{\\partial t}=-\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle\\frac{1}{n}\\left(\\sum_{i\\in I_{1}}g_{i}^{(t)}-\\sum_{i\\in I_{2}\\cup I_{3}\\cup I_{4}}g_{i}^{(t)}+o(1)\\sum_{i\\in[n]}g_{i}^{(t)}\\right).\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "By Lemma F.6, we have ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\frac{\\partial y_{i}F_{i}^{(t)}}{\\partial t}=\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle\\Theta\\left(\\frac{1}{n}\\sum_{i\\in[n]}g_{i}^{(t)}\\right)\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "for all $i\\in[n]$ . Therefore, ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial\\hat{L}}{\\partial t}=\\frac{1}{n}\\sum_{i=1}^{n}\\ell^{\\prime}(y_{i}F_{i}^{(t)})\\frac{\\partial y_{i}F_{i}^{(t)}}{\\partial t}}\\\\ &{\\quad=\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}-g_{i}^{(t)}\\sum_{j_{1}=1}^{m}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle\\Theta\\left(\\frac{1}{n}\\sum_{i^{\\prime}\\in[n]}g_{i^{\\prime}}^{(t)}\\right)}\\\\ &{\\displaystyle\\quad=-\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle\\Theta\\left(\\left(\\frac{1}{n}\\sum_{i\\in[n]}g_{i}^{(t)}\\right)^{2}\\right)}\\\\ &{\\displaystyle\\quad=-\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle\\Theta\\left(\\hat{L}^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "where the last equality follows from the property of binary cross-entropy loss that $\\ell(x)=\\Theta(-\\ell^{\\prime}(x))$ for $x>0$ . By Definition F.1 and Lemma F.4, we have for all $t\\in[T_{1},T_{2}]$ ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle=\\Theta(\\sigma_{1}^{2}m m_{1}).\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Thus, we have ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\frac{\\partial\\widehat{L}}{\\partial t}=-\\Theta(\\sigma_{1}^{2}m m_{1})\\widehat{L}^{2}.\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Nowcidft q $\\begin{array}{r}{\\frac{d L}{d t}\\,=\\,-C_{1}L^{2}}\\end{array}$ .Note tha thsis aseparabe dia equation in and we can solve it by ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\frac{1}{L^{2}}\\frac{d L}{d t}+C_{1}=0\\quad\\Rightarrow\\quad\\frac{d}{d t}\\left(C_{1}t-L^{-1}+C_{2}\\right)=0\\quad\\Rightarrow\\quad L(t)=\\frac{1}{C_{1}t+C_{2}}.\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "This implies for $t\\in[T_{1},T_{2}]$ , we have ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\widehat{L}(t)=\\frac{1}{\\Theta(\\sigma_{1}^{2}m m_{1})(t-T_{1})+(1/\\widehat{L}(T_{1}))}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Corollary F.9. The following bound holds: ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\left|\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(T_{2})},a_{j_{2}}w_{j_{2}}^{(T_{2})}\\right\\rangle-\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(T_{1})},a_{j_{2}}w_{j_{2}}^{(T_{1})}\\right\\rangle\\right|\\leq\\tilde{O}\\left(\\frac{m_{1}}{m}\\right).\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Proof. This is a direct consequence of Lemma F.4 and Theorem F.8. ", "page_idx": 54}, {"type": "text", "text": "F.3Growth of Neuron Correlation ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Lemma F.10. For $t\\in[T_{1},T_{2}],$ we have ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle-\\displaystyle\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(T_{1})},a_{j_{2}}w_{j_{2}}^{(T_{1})}\\right\\rangle}\\\\ &{\\quad\\quad=O\\left(\\frac{m_{1}\\log m\\log t}{\\sigma_{1}^{2}m m_{1}}\\right)=O\\left(\\frac{m_{1}\\log^{2}m}{\\sigma_{1}^{2}m m_{1}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "and thus, ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle=\\Theta(\\sigma_{1}^{2}m m_{1}).\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Proof. By Lemma F.4, we have ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial t}\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle=\\frac{2m_{1}}{n}\\sum_{i=1}^{n}g_{i}^{(t)}y_{i}F_{i}^{(t)}.\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "By Theorem F.8, we have $\\begin{array}{r}{\\frac{1}{n}\\sum_{i=1}^{n}g_{i}^{(t)}\\,=\\,O(\\widehat{L}^{(t)})\\,=\\,O\\left(\\frac{1}{(\\sigma_{1}^{2}m m_{1})(t-T_{1})+1/\\widehat{L}(T_{1})}\\right)}\\end{array}$ ((mm1)(t-T)+1/L(T)). Further, by Definition F.1, we have $|F_{i}^{(t)}|\\le O(\\log m)$ Thus, for $t\\in[T_{1},T_{2}]$ , we obtain ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle-\\displaystyle\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(T_{1})},a_{j_{2}}w_{j_{2}}^{(T_{1})}\\right\\rangle}\\\\ &{=2m_{1}\\displaystyle\\int_{T_{1}}^{t}\\frac{1}{n}\\sum_{i=1}^{n}g_{i}^{(\\tau)}y_{i}F_{i}^{(\\tau)}\\;d\\tau}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\leq{\\cal O}(m_{1}\\log m)\\int_{T_{1}}^{t}{\\cal O}\\left(\\frac{1}{(\\sigma_{1}^{2}m m_{1})(\\tau-T_{1})+1/\\widehat{L}(T_{1})}\\right)\\,d\\tau}}\\\\ {{\\displaystyle={\\cal O}\\left(\\frac{m_{1}\\log m\\log t}{\\sigma_{1}^{2}m m_{1}}\\right)={\\cal O}\\left(\\frac{m_{1}\\log^{2}m}{\\sigma_{1}^{2}m m_{1}}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "where the last line follows because $T_{2}\\leq O(\\mathrm{poly}(m))$ in Definition F.1. Finally, by Theorem E.27 wehave ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(T_{1})},a_{j_{2}}w_{j_{2}}^{(T_{1})}\\right\\rangle=\\Theta(\\sigma_{1}^{2}m m_{1}).\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "F.4Growth of Correlation of Value-Transformed Data ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "We now analyze the correlation term with value-transformed data. ", "page_idx": 55}, {"type": "text", "text": "Lemma F.11 (Growth of correlation of value-transformed data). For $\\mu,\\nu\\in\\{\\mu_{i}\\}_{i=1}^{d}$ wehave ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial t}\\mu^{\\top}W_{V}^{(t)\\top}W_{V}^{(t)}\\nu=G^{(t)}(\\nu)\\frac{1}{n}\\sum_{i:\\mu\\in X^{(i)}}g_{i}^{(t)}y_{i}\\sum_{l=1}^{L}p_{q\\leftarrow l,k\\leftarrow\\mu}^{(t,i)}+G^{(t)}(\\mu)\\frac{1}{n}\\sum_{i:\\nu\\in X^{(i)}}g_{i}^{(t)}y_{i}\\sum_{l=1}^{L}p_{q\\leftarrow l,k\\leftarrow\\mu}^{(t,i)}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Thus, for $t\\in[T_{1},T_{2}]$ we have ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mu,\\nu}\\left|\\mu^{\\top}W_{V}^{(t)\\top}W_{V}^{(t)}\\nu-\\mu^{\\top}W_{V}^{(T_{1})\\top}W_{V}^{(T_{1})}\\nu\\right|\\leq O\\left(\\frac{\\log m\\log t}{\\sigma_{1}^{2}m m_{1}}\\right).\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Proof. By the gradient flow update, we have ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial\\mu W_{V}^{(t)\\top}W_{V}^{(t)}\\nu}{\\partial t}}\\\\ &{=\\displaystyle\\frac{1}{n}\\sum_{i:\\mu\\in X^{(i)}}g_{i}^{(t)}y_{i}\\sum_{l=1}^{L}\\sum_{j=1}^{m_{1}}a_{j}\\nu^{\\top}W_{V}^{(t)\\top}w_{j}^{(t)}p_{q\\leftarrow l,k\\leftarrow\\mu}^{(t,i)}+\\frac{1}{n}\\sum_{i:\\nu\\in X^{(i)}}g_{i}^{(t)}y_{i}\\sum_{l=1}^{L}\\sum_{j=1}^{m_{1}}a_{j}\\mu^{\\top}W_{V}^{(t)\\top}w_{j}^{(t)}p_{q\\leftarrow l,k\\leftarrow\\mu}^{(t)}}\\\\ &{=G^{(t)}(\\nu)\\frac{1}{n}\\sum_{i:\\mu\\in X^{(i)}}g_{i}^{(t)}y_{i}\\sum_{l=1}^{L}p_{q\\leftarrow l,k\\leftarrow\\mu}^{(t,i)}+G^{(t)}(\\mu)\\frac{1}{n}\\sum_{i:\\nu\\in X^{(i)}}g_{i}^{(t)}y_{i}\\sum_{l=1}^{L}p_{q\\leftarrow l,k\\leftarrow\\nu}^{(t,i)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Thus, we obtain ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mu W_{V}^{(\\tau)\\top}W_{V}^{(\\tau)}\\nu-\\mu W_{V}^{(T_{1})\\top}W_{V}^{(T_{1})}\\nu\\Big|}\\\\ {\\displaystyle\\leq\\int_{T_{1}}^{\\tau}|G^{(t)}(\\nu)|\\left|\\frac{1}{n}\\sum_{i:\\mu\\in X^{(i)}}g_{i}^{(t)}y_{i}\\right|\\left|\\sum_{l=1}^{L}p_{q\\leftarrow l,k\\leftarrow\\mu}^{(t,i)}\\right|+|G^{(t)}(\\mu)|\\left|\\frac{1}{n}\\sum_{i:\\nu\\in X^{(i)}}g_{i}^{(t)}y_{i}\\right|\\left|\\sum_{l=1}^{L}p_{q\\leftarrow l,k\\leftarrow\\nu}^{(t,i)}\\right|d t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "By Defintion EI,for $t\\,\\in\\,[T_{1},T_{2}]$ wehave $|G^{(t)}(\\mu)|\\,\\leq\\,O(\\log m)$ and $\\begin{array}{r}{\\sum_{l=1}^{L}p_{q\\leftarrow l,k\\leftarrow\\mu}^{(t,i)}\\leq O(1)}\\end{array}$ Further, by the property of the cross-entropy loss, we have ", "page_idx": 55}, {"type": "equation", "text": "$$\n{\\frac{1}{n}}\\sum_{i:\\mu\\in X^{(i)}}g_{i}^{(t)}y_{i}=O({\\widehat{L}}^{(t)}).\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Therefore, by Theorem F.8, we obtain ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\left|\\mu W_{V}^{(\\tau)\\top}W_{V}^{(\\tau)}\\nu-\\mu W_{V}^{(T_{1})\\top}W_{V}^{(T_{1})}\\nu\\right|\\leq O(\\log m)\\int_{T_{1}}^{\\tau}\\widehat{L}^{(t)}d t\\leq O(\\log m)\\cdot O\\left(\\frac{\\log\\tau}{\\sigma_{1}^{2}m m_{1}}\\right).\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Corollary F.12. For $t\\in[T_{1},T_{2}]$ we have ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\frac{\\operatorname*{max}_{\\mu\\in\\{\\mu_{i}\\}_{i=1}^{d}}\\left\\vert\\sum_{j=1}^{m_{1}}a_{j}\\frac{\\partial w_{j}^{(t)}}{\\partial t}W_{V}^{(t)}\\mu\\right\\vert}{\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle}\\leq o(1/L)\\frac{1}{n}\\sum_{i\\in[n]}g_{i}^{(t)}.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "ProofByLmmaE9and andLmmafrall $\\mu\\neq\\nu\\in\\{\\mu_{i}\\}_{i=1}^{d}$ we have $|\\mu^{\\top}W_{V}^{(t)\\top}W_{V}^{(t)}\\nu|\\leq$ $\\widetilde{O}(\\sigma_{0}^{2}\\sqrt{m}+1/m)$ and $\\|W_{V}^{(t)}\\nu\\|_{2}^{2}=\\widetilde{O}(\\sigma_{0}^{2}m+1/m)$ . Thus, by Lemma F.11, we have ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\operatorname*{max}_{\\mu,\\nu}\\left|\\mu^{\\top}W_{V}^{(t)\\top}W_{V}^{(t)}\\nu\\right|}{\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle}\\leq\\tilde{O}\\left(\\frac{\\sigma_{0}^{2}\\sqrt{m}+1/m}{\\sigma_{1}^{2}m m_{1}}\\right),}\\\\ &{\\frac{\\operatorname*{max}_{\\nu}\\|W_{V}^{(t)}\\nu\\|_{2}^{2}}{\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle}\\leq\\tilde{O}\\left(\\frac{\\sigma_{0}^{2}m+1/m}{\\sigma_{1}^{2}m m_{1}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Recall that ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{m_{1}}a_{j}\\frac{\\partial w_{j}^{(t)}}{\\partial t}W_{V}^{(t)}\\mu=\\frac{m_{1}}{n}\\sum_{i_{1}=1}^{n}g_{i_{1}}^{(t)}y_{i_{1}}\\sum_{l_{1}=1}^{L}p_{l_{1}}^{(t,i_{1})\\top}V^{(t,i_{1})\\top}W_{V}^{(t)}\\mu.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "This implies that ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\frac{\\operatorname*{max}_{\\mu\\in\\{\\mu_{i}\\}_{i=1}^{d}}\\left\\vert\\sum_{j=1}^{m_{1}}a_{j}\\frac{\\partial w_{j}^{(t)}}{\\partial t}W_{V}^{(t)}\\mu\\right\\vert}{\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle}\\leq\\widetilde{O}\\left(\\frac{\\sigma_{0}^{2}\\sqrt{m}L+L/m+\\sigma_{0}^{2}m+1/m}{\\sigma_{1}^{2}m}\\right)\\cdot\\frac{1}{n}\\sum_{i=1}^{n}g_{i}^{(t)}.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Corollary F.13 (Complete version of Corollary 4.6). For $t\\in[T_{1},T_{2}]_{\\cdot}$ wehave ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l r}&{\\displaystyle\\frac{\\partial}{\\partial t}\\mu_{2}^{\\top}W_{V}^{(t)\\top}W_{V}^{(t)}\\mu_{1}>0,}&&{\\displaystyle\\frac{\\partial}{\\partial t}\\mu_{1}^{\\top}W_{V}^{(t)\\top}W_{V}^{(t)}\\mu_{1}>0,}&&{\\displaystyle\\frac{\\partial}{\\partial t}\\mu_{2}^{\\top}W_{V}^{(t)\\top}W_{V}^{(t)}\\mu_{2}>0}\\\\ &{\\displaystyle\\frac{\\partial}{\\partial t}\\mu_{1}^{\\top}W_{V}^{(t)\\top}W_{V}^{(t)}\\mu_{3}<0,}&&{\\displaystyle\\frac{\\partial}{\\partial t}\\mu_{2}^{\\top}W_{V}^{(t)\\top}W_{V}^{(t)}\\mu_{3}<0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Proof. This is a direct consequence of Lemma F.11, Lemma F.6 and Definition F.1. ", "page_idx": 56}, {"type": "text", "text": "F.5  Change of Random-Token Sub-Network ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Lemma F.14. For $t\\in[T_{1},T_{2}]$ and $\\mu\\in\\{\\mu_{i}\\}_{i=4}^{d}$ we have ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\left|\\frac{\\partial G^{(t)}(\\mu)}{\\partial t}\\right|\\leq O\\left(\\frac{1}{n}\\widehat{L}^{(t)}\\sigma_{1}^{2}m m_{1}\\right)+\\widetilde{O}(\\widehat{L}^{(t)}m_{1}(L(\\sigma_{0}^{2}\\sqrt{m}+1/m)+\\sigma_{0}^{2}m)).\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Thus, ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\left|G^{(t)}(\\mu)-G^{(T_{1})}(\\mu)\\right|\\leq\\widetilde O\\left(\\frac{1}{n}+\\frac{m_{1}(L(\\sigma_{0}^{2}\\sqrt{m}+1/m)+\\sigma_{0}^{2}m)}{\\sigma_{1}^{2}m m_{1}}\\right).\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Proof. By Lemma C.1 and Definition F.1, we have ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left\\lvert\\frac{\\partial G^{(t)}(\\mu)}{\\partial t}\\right\\rvert=\\displaystyle\\left\\lvert\\sum_{j=1}^{m_{1}}a_{j}w_{j}^{(t)}\\frac{\\partial W_{V}^{(t)}\\mu}{\\partial t}+a_{j}\\frac{\\partial w_{j}^{(t)}}{\\partial t}W_{V}^{(t)}\\mu\\right\\rvert}\\\\ {\\displaystyle\\qquad\\leq\\left\\lvert\\frac{1}{n}\\sum_{\\substack{i_{2}:\\;\\mu\\in X^{(i_{2})}}}g_{i_{2}}^{(t)}y_{i_{2}}\\sum_{l_{2}=1}^{L}\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}a_{j_{1}}a_{j_{2}}\\left<w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\\right>p_{q\\leftarrow l_{2},k\\leftarrow\\mu}^{(t,i_{2})}\\right\\rvert}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "equation", "text": "$$\n+\\left|\\frac{1}{n}\\sum_{i_{1}=1}^{n}g_{i_{1}}^{(t)}y_{i_{1}}m_{1}\\sum_{l_{1}=1}^{L}\\sum_{l_{2}=1}^{L}\\left\\langle p_{l_{1},l_{2}}^{(t,i_{1})}v_{l_{2}}^{(t,i_{1})},v^{(t)}(\\mu)\\right\\rangle\\right|.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "By Definition F.1 and Corollary F.2, ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{n}\\sum_{\\substack{i_{2}:\\;\\mu\\in X^{(i_{2})}}}g_{i_{2}}^{(t)}y_{i_{2}}\\sum_{l_{2}=1}^{L}\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}a_{j_{1}}a_{j_{2}}\\left\\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\\right\\rangle p_{q\\leftarrow l_{2},k\\leftarrow\\mu}^{(t,i_{2})}\\right|\\leq O\\left(\\frac{1}{n}\\widehat{L}^{(t)}\\sigma_{1}^{2}m m_{1}\\right).\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "By Lemma E.9 and and Lemma F11, we have $|\\mu^{\\top}W_{V}^{(t)\\top}W_{V}^{(t)}\\nu|\\leq\\tilde{O}(\\sigma_{0}^{2}\\sqrt{m}+1/m)$ for \u03bc# V and $\\|W_{V}^{(t)}\\mu\\|_{2}^{2}=O(\\sigma_{0}^{2}m)$ .Thus, ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\left\\vert\\frac{1}{n}\\sum_{i_{1}=1}^{n}g_{i_{1}}^{(t)}y_{i_{1}}m_{1}\\sum_{l_{1}=1}^{L}\\sum_{l_{2}=1}^{L}\\left\\langle p_{l_{1},l_{2}}^{(t,i_{1})}v_{l_{2}}^{(t,i_{1})},v^{(t)}(\\mu)\\right\\rangle\\right\\vert\\leq\\widetilde{O}(\\widehat{L}^{(t)}m_{1}(L(\\sigma_{0}^{2}\\sqrt{m}+1/m)+\\sigma_{0}^{2}m)).\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Thus, by Theorem F.8, for $t\\in[T_{1},T_{2}]$ , we have ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{G^{(t)}(\\mu)-G^{(T_{1})}(\\mu)\\Big|=\\left|\\int_{T_{1}}^{t}\\frac{\\partial G^{(\\tau)}(\\mu)}{\\partial\\tau}\\,d\\tau\\right|\\le\\left(O\\left(\\frac{\\sigma_{1}^{2}m m_{1}}{n}\\right)+\\widetilde{O}(m_{1}L(\\sigma_{0}^{2}\\sqrt{m}+1/m))\\right)\\int_{T_{1}}^{t}\\widehat{L}^{(\\tau)}(\\mu)d\\tau}&{{}}&{}\\\\ {\\le\\widetilde{O}\\left(\\frac{1}{n}+\\frac{m_{1}\\left(L(\\sigma_{0}^{2}\\sqrt{m}+1/m)+\\sigma_{0}^{2}m\\right)}{\\sigma_{1}^{2}m m_{1}}\\right).\\ \\ \\ \\ \\ \\ \\ }&{}&{{}}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Corollary F.15. For $t\\in[T_{1},T_{2}]$ we have ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\left|\\sum_{l_{1}=1}^{L}\\sum_{l_{2}:\\;X_{l_{2}}^{(i)}\\in\\{\\mu_{k}\\}_{k=4}^{d}}G^{(t)}(X_{l_{2}}^{(i)})p_{q\\leftarrow l_{1},k\\leftarrow l_{2}}^{(t,i)}\\right|\\leq O(1).\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Proof. By Lemma F.3, we have ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\left|\\sum_{l_{1}=1}^{L}\\sum_{l_{2}:\\;X_{l_{2}}^{(i)}\\in\\{\\mu_{k}\\}_{k=4}^{d}}G^{(T_{1})}(X_{l_{2}}^{(i)})p_{q\\leftarrow l_{1},k\\leftarrow l_{2}}^{(T_{1},i)}\\right|\\leq O(1)\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "By Lemma F.14, for $t\\,\\in\\,[T_{1},T_{2}]$ \uff0c $G^{(t)}(\\mu)\\,=\\,O(1)$ for $\\mu\\,\\in\\,\\{\\mu_{i}\\}_{i=4}^{d}$ . On the other hand, by the triangle inequality, we have ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{l=1}^{L}\\displaystyle\\sum_{L_{l}\\geq\\lambda_{l}}(\\sum_{\\ell\\neq\\ell(\\mu)\\leq l=4}^{Q(T_{1})}\\ G_{\\ell-l,k-l}^{(T_{\\ell})})_{\\ell\\neq\\ell-l,k-l}-\\displaystyle\\sum_{l=1}^{L}\\sum_{l_{2}:\\lambda_{l}^{(\\ell)}\\in\\{\\mu\\}_{k-l,k-l}^{(L)}}G^{(t)}(X_{l_{2}}^{(i)})_{\\ell\\neq\\ell-l_{1},k-l_{2}}^{(i)}}\\\\ &{\\leq\\displaystyle\\sum_{l=1}^{L}\\sum_{l_{2}:\\lambda_{l}^{(i)}\\in\\{\\mu\\}_{k-l,k-l}^{(i)}}\\Big|(G^{(T_{1})}(X_{l_{2}}^{(i)})-G^{(t)}(X_{l_{2}}^{(i)}))\\Big|\\Big|p_{\\ell-l,k-l_{2}}^{(t)}}\\\\ &{\\quad+\\displaystyle\\sum_{l_{1}=1}^{L}\\sum_{l_{2}:\\lambda_{l}^{(i)}\\in\\{\\mu\\}_{k-l}^{(i)}}G^{(t)}(X_{l_{2}}^{(i)})\\Big|p_{\\ell-l_{1},k-l_{2}}^{(T_{1})}-p_{\\ell-l_{1},k-l_{2}}^{(t_{i})}\\Big|}\\\\ &{\\quad+\\displaystyle\\sum_{l_{2}=1}^{L}\\sum_{l_{2}:\\lambda_{l}^{(i)}\\in\\{\\mu\\}_{k-l_{2}}^{(i)}}\\Big|\\Big|G^{(T_{1})}(X_{l_{2}}^{(i)})-G^{(t)}(X_{l_{2}}^{(i)})\\Big|\\Big|p_{\\ell+l_{1},k-l_{2}}^{(T_{1})}-p_{\\ell+l_{1},k-l_{2}}^{(t_{i})}\\Big|}\\\\ &{\\quad+\\displaystyle\\sum_{l_{1}=1}^{L}\\sum_{l_{2}:\\lambda_{l}^{(i)}\\in\\{\\mu\\}_{k-l_{2}}^{(i)}}\\Big|\\Big|G^{(T_{1})}(X_{l_{2}}^{(i)})-G^{(t)}(X_{l_{2}}^{(i)} \n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "\u2264 0(1), ", "page_idx": 57}, {"type": "text", "text": "where the last inequality applies Definition F.1. This implies that, for $t\\in[T_{1},T_{2}]$ , by Lemma F.14, wehave ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\left|\\sum_{l_{1}=1}^{L}\\sum_{l_{2}:\\;X_{l_{2}}^{(i)}\\in\\{\\mu_{k}\\}_{k=4}^{d}}G^{(t)}(X_{l_{2}}^{(i)})p_{q\\leftarrow l_{1},k\\leftarrow l_{2}}^{(t,i)}\\right|\\leq O(1).\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "F.6 Change of Score and Softmax Probability ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Lemma F.16 (Change of score, complete version of Lemma 4.7). For $t\\in[T_{1},T_{2}]$ theattention scores are changing in the following way: ", "page_idx": 58}, {"type": "text", "text": "\u00b7For $\\mu$ $\\nu\\in\\{\\mu_{1},\\mu_{2}\\}$ $\\mu\\neq\\nu$ the query-key-correlation score between the two target signals increases, while the query-key-correlation score between one target signal and the common tokendecreases,i.e., ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial}{\\partial t}\\nu^{\\top}W_{K}^{(t)\\top}W_{Q}^{(t)}\\mu=\\frac{1}{\\sqrt{m}}\\widetilde{\\Theta}(\\widehat{L}^{(t)}\\sigma_{0}^{2}m)\\frac{1}{L},}\\\\ &{\\frac{\\partial}{\\partial t}\\mu_{3}^{\\top}W_{K}^{(t)\\top}W_{Q}^{(t)}\\mu=-\\frac{1}{\\sqrt{m}}\\widetilde{\\Theta}(\\widehat{L}^{(t)}\\sigma_{0}^{2}m)\\frac{1}{L}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "\u00b7 The change of score satisfies: ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mu,\\nu\\in\\{\\mu_{i}\\}_{i=1}^{3}}\\left|\\nu^{\\top}W_{K}^{(t)}W_{Q}^{(t)}\\mu-\\nu^{\\top}W_{K}^{(T_{1})}W_{Q}^{(T_{1})}\\mu\\right|=\\Theta\\left(\\frac{\\sigma_{0}^{2}m}{\\sqrt{m}L\\sigma_{1}^{2}m m_{1}}+\\frac{\\sigma_{0}^{2}\\sqrt{m}}{\\sqrt{m}\\sigma_{1}^{2}m m_{1}}\\right).\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "\u00b7For all $\\mu\\in\\left\\{\\mu_{1},\\mu_{2},\\mu_{3}\\right\\}$ $\\gamma\\in\\{\\mu_{i}\\}_{i=4}^{d}$ the query-key-correlation score changes as follws: ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\left|\\frac{\\partial}{\\partial t}\\mu^{\\top}W_{K}^{(t)\\top}W_{Q}^{(t)}\\gamma\\right|=\\frac{1}{n\\sqrt{m}}\\widetilde{\\Theta}(\\widehat{L}^{(t)}\\sigma_{0}^{2}m)\\frac{1}{L}+\\widetilde{O}\\left(\\frac{1}{\\sqrt{m}}\\widehat{L}^{(t)}\\sigma_{0}^{2}\\sqrt{m}\\right),\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "and ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mu^{\\top}W_{K}^{(t)^{\\top}}W_{Q}^{(t)}\\gamma-\\mu^{\\top}W_{K}^{(T_{1})\\top}W_{Q}^{(T_{1})}\\gamma\\right|\\leq\\tilde{O}\\left(\\frac{\\sigma_{0}^{2}m}{n\\sqrt{m}L\\sigma_{1}^{2}m m_{1}}+\\frac{\\sigma_{0}^{2}\\sqrt{m}}{\\sqrt{m}\\sigma_{1}^{2}m m_{1}}\\right),}\\\\ &{\\left|\\gamma^{\\top}W_{K}^{(t)^{\\top}}W_{Q}^{(t)}\\mu-\\gamma^{\\top}W_{K}^{(T_{1})\\top}W_{Q}^{(T_{1})}\\mu\\right|\\leq\\tilde{O}\\left(\\frac{\\sigma_{0}^{2}m}{n\\sqrt{m}L\\sigma_{1}^{2}m m_{1}}+\\frac{\\sigma_{0}^{2}\\sqrt{m}}{\\sqrt{m}\\sigma_{1}^{2}m m_{1}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Proof. By Lemma C.4, we have ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial_{t}\\gamma^{\\top}W_{Q}^{(t)^{\\top}}W_{Q}^{(t)}\\mu}{\\partial t}}\\\\ &{=\\frac{1}{n\\sqrt{m}}\\sum_{i\\neq\\mu,\\nu\\in\\{0\\}}g_{i}^{(t)}y_{j}\\displaystyle\\sum_{j=1}^{m_{1}}a_{j}\\lVert k^{(t)}(\\nu)\\rVert_{2}^{2}\\left(\\nu^{(t)^{\\top}}(\\nu)w_{j}^{(t)}-w_{j}^{(t)^{\\top}}V^{(t,i)}p_{i+\\mu,\\nu}^{(t,i)}\\right)p_{q+\\mu,k\\leftarrow\\nu}^{(t,i)}}\\\\ &{\\quad+\\frac{1}{n\\sqrt{m}}\\displaystyle\\sum_{i\\neq\\mu}\\underbrace{g_{i}^{(t)}y_{j}\\displaystyle\\sum_{\\bar{j}=1}^{m_{1}}a_{j}}_{j=1}\\sum_{l=1}^{L}\\nu^{\\top}W_{K}^{(t)^{\\top}}K_{l}^{(t,i)}\\left(V_{l}^{(t,i)^{\\top}}w_{j}^{(t)}-w_{j}^{(t)^{\\top}}V^{(t,i)}p_{l+\\mu,\\nu}^{(t,i)}\\right)p_{q+\\mu,k\\leftarrow l}^{(t,i)}\\mathbb{I}_{l}^{(t,i)}}\\\\ &{\\quad+\\frac{1}{n\\sqrt{m}}\\displaystyle\\sum_{i\\neq\\mu,\\nu\\in\\{0\\}}g_{i}^{(t)}y_{\\underline{{j}}-1}^{m_{1}}\\alpha_{j}\\lVert q^{(t)}(\\mu)\\rVert_{2}^{2}p_{q+\\mu,k\\leftarrow\\nu}^{(t,i)}\\left(w_{j}^{(t)^{\\top}}v^{(t,i)}(\\nu)-w_{j}^{(t)^{\\top}}V^{(t,i)}p_{l(i,\\nu)}^{(t,i)}\\right)}\\\\ &{\\quad+\\frac{1}{n\\sqrt{m}}\\displaystyle\\sum_{i\\neq\\nu\\in\\{0\\}}\\underbrace{g_{i}^{(t)}y_{j}\\displaystyle\\sum_{\\bar{j}=1}^{m_{1}}\\sum_{j=1}^{m_{1}}a_{j}\\mu^{\\top}W_{Q}^{(t)}\\eta_{q}^{(t,i)}p_{q-\\mu,k\\leftarrow\\nu}^{(t,i)}\\left(w_{j}^{(t)^{\\top}}v^{(t,i)}( \n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Now, we take $\\mu=\\mu_{1},\\ \\nu=\\mu_{2}$ . By Theorem E.27, we have $G^{(T_{1})}(\\mu_{2})\\geq\\Omega(1)$ . A consequence of Defnition F1 andLemma6is that $\\frac{\\partial G^{(t)}(\\mu_{2})}{\\partial t}>0$ 0 $t\\in[T_{1},T_{2}]$ Thus, we have $G^{(t)}(\\mu_{2})\\geq\\Omega(1)$ ", "page_idx": 58}, {"type": "text", "text": "Further by Theorem E.27, we have $G^{(T_{1})}(\\mu)=\\widetilde{O}(\\sigma_{0}\\sigma_{1}\\sqrt{m m_{1}})$ for $\\mu\\in\\mathcal R$ and then by Lemma F.14, we have $\\begin{array}{r}{G^{(t)}(\\mu)\\,=\\,\\widetilde{O}(\\sigma_{0}\\sigma_{1}\\sqrt{m m_{1}})+\\widetilde{O}\\left(\\frac{1}{n}+\\frac{m_{1}(L(\\sigma_{0}^{2}\\sqrt{m}+1/m)+\\sigma_{0}^{2}m)}{\\sigma_{1}^{2}m m_{1}}\\right)}\\end{array}$ for $t\\,\\in\\,[T_{1},T_{2}]$ Also, Definition F.1 implies that G(t)(\u03bc2) - \u2265=1 w) $\\begin{array}{r}{G^{(t)}(\\mu_{2})-\\sum_{j=1}^{m_{1}}w_{j}^{(t)}V^{(t,i)}p_{l(i,\\mu)}^{(t,i)}\\geq\\Omega(1)}\\end{array}$ Pi(t,a) \u2265 \u03a9(1). Now,this yields ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mu_{2}^{\\top}W_{K}^{(t)^{\\top}}W_{Q}^{(t)}\\mu_{1}}{\\partial t}=\\frac{1}{\\sqrt{m}}\\widetilde{\\Theta}(\\widehat{L}^{(t)}\\sigma_{0}^{2}m)\\frac{1}{L}+\\widetilde{O}\\left(\\frac{1}{\\sqrt{m}}\\widehat{L}^{(t)}\\sigma_{0}^{2}\\sqrt{m}\\right).\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "On the other hand, by the analysis similar to the above, we obtain ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mu_{3}^{\\top}W_{K}^{(t)\\top}W_{Q}^{(t)}\\mu_{1}}{\\partial t}=-\\frac{1}{\\sqrt{m}}\\widetilde{\\Theta}(\\widehat{L}^{(t)}\\sigma_{0}^{2}m)\\frac{1}{L}+\\widetilde{O}\\left(\\frac{1}{\\sqrt{m}}\\widehat{L}^{(t)}\\sigma_{0}^{2}\\sqrt{m}\\right).\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Next, to prove the maximum change of the score, we have ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mu,\\nu}\\left|\\frac{\\partial\\nu^{\\top}W_{K}^{(t)}W_{Q}^{(t)}\\mu}{\\partial t}\\right|\\leq\\frac{1}{\\sqrt{m}}\\widetilde{\\Theta}(\\widehat{L}^{(t)}\\sigma_{0}^{2}m)\\frac{1}{L}+\\widetilde{O}\\left(\\frac{1}{\\sqrt{m}}\\widehat{L}^{(t)}\\sigma_{0}^{2}\\sqrt{m}\\right).\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "By Theorem F.8, we have ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\nu^{\\top}W_{K}^{(t)\\top}W_{Q}^{(t)}\\mu-\\nu^{\\top}W_{K}^{(T_{1})\\top}W_{Q}^{(T_{1})}\\mu\\right|\\leq\\int_{T_{1}}^{t}\\frac{1}{\\sqrt{m}}\\widetilde{\\Theta}(\\widehat{L}^{(\\tau)}\\sigma_{0}^{2}m)\\frac{1}{L}+\\widetilde{O}\\left(\\frac{1}{\\sqrt{m}}\\widehat{L}^{(\\tau)}\\sigma_{0}^{2}\\sqrt{m}\\right)\\,d\\tau}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\widetilde{O}\\left(\\frac{\\sigma_{0}^{2}m}{\\sqrt{m}L\\sigma_{1}^{2}m m_{1}}+\\frac{\\sigma_{0}^{2}\\sqrt{m}}{\\sqrt{m}\\sigma_{1}^{2}m m_{1}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Finally, for $\\gamma\\in\\{\\mu_{i}\\}_{i=4}^{d},\\;\\mu\\in\\{\\mu_{i}\\}_{i=1}^{d}$ , we have ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\left|\\frac{\\partial}{\\partial t}\\mu^{\\top}W_{K}^{(t)\\top}W_{Q}^{(t)}\\gamma\\right|=\\frac{1}{n\\sqrt{m}}\\widetilde{\\Theta}(\\widehat{L}^{(t)}\\sigma_{0}^{2}m)\\frac{1}{L}+\\widetilde{O}\\left(\\frac{1}{\\sqrt{m}}\\widehat{L}^{(t)}\\sigma_{0}^{2}\\sqrt{m}\\right),\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "which implies that ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\left|\\mu^{\\top}W_{K}^{(t)^{\\top}}W_{Q}^{(t)}\\gamma-\\mu^{\\top}W_{K}^{(T_{1})^{\\top}}W_{Q}^{(T_{1})}\\gamma\\right|\\leq\\tilde{O}\\left(\\frac{\\sigma_{0}^{2}m}{n\\sqrt{m}L\\sigma_{1}^{2}m m_{1}}+\\frac{\\sigma_{0}^{2}\\sqrt{m}}{\\sqrt{m}\\sigma_{1}^{2}m m_{1}}\\right).\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Corollary F.17 (Change of softmax). For $t\\in[T_{1},T_{2}]$ , the softmax probability is changing in the following way: ", "page_idx": 59}, {"type": "text", "text": "\u00b7For $\\mu,\\nu~\\in~\\{\\mu_{1},\\mu_{2}\\}$ $\\mu\\mathrm{~\\ne~}\\nu$ .the softmax probability between the two target signals increases, whereas the softmax probability between one target signal and the common token decreases,i.e., ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial}{\\partial t}p_{q\\leftarrow\\mu,k\\leftarrow\\nu}^{(t,i)}=\\frac{1}{\\sqrt{m}}\\widetilde{\\Theta}(\\widehat{L}^{(t)}\\sigma_{0}^{2}m)\\frac{1}{L^{2}},}\\\\ &{\\displaystyle\\frac{\\partial}{\\partial t}p_{q\\leftarrow\\mu,k\\leftarrow\\mu_{3}}^{(t,i)}=-\\frac{1}{\\sqrt{m}}\\widetilde{\\Theta}(\\widehat{L}^{(t)}\\sigma_{0}^{2}m)\\frac{1}{L^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "\u00b7For all $\\mu\\in\\left\\{\\mu_{1},\\mu_{2}\\right\\}$ $\\gamma\\in\\{\\mu_{i}\\}_{i=4}^{d}$ the softmax probability beween one target signal and a random token changes as follows: ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\left|\\frac{\\partial}{\\partial t}p_{q\\leftarrow\\mu,k\\leftarrow\\gamma}^{(t,i)}\\right|\\leq\\frac{1}{n\\sqrt{m}}\\widetilde{\\Theta}(\\widehat{L}^{(t)}\\sigma_{0}^{2}m)\\frac{1}{L^{2}}+\\widetilde{O}\\left(\\frac{1}{L\\sqrt{m}}\\widehat{L}^{(t)}\\sigma_{0}^{2}\\sqrt{m}\\right).\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Furthermore, we have ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i\\in[n],l_{1},l_{2}\\in[L]}\\left|p_{l_{1},l_{2}}^{(t,i)}-p_{l_{1},l_{2}}^{(0,i)}\\right|\\leq O(1/L^{2}).\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Proof. Take $\\mu=\\mu_{1},\\ \\nu=\\mu_{2}$ . First of all, by Definition F.1 and Lemma F.16, we have ", "page_idx": 60}, {"type": "equation", "text": "$$\np_{q\\leftarrow\\mu_{1}}^{(t,i)\\top}\\frac{\\partial X^{(i)\\top}W_{K}^{(t)\\top}W_{Q}^{(t)}\\mu_{1}}{\\partial t}\\biggr|\\leq\\frac{1}{\\sqrt{m}}O(\\widehat{L}^{(t)}\\sigma_{0}^{2}m)\\frac{1}{L^{2}}+\\frac{1}{n\\sqrt{m}}\\widetilde{\\Theta}(\\widehat{L}^{(t)}\\sigma_{0}^{2}m)\\frac{1}{L}+\\widetilde{O}\\left(\\frac{1}{\\sqrt{m}}\\widehat{L}^{(t)}\\sigma_{0}^{2}\\sqrt{n}\\right)\\biggr|_{L^{2}},\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Thus, by Lemma G.1 and Lemma F.16, for $i\\in I_{1}$ ,we obtain ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial}{\\partial t}p_{q\\leftarrow\\mu_{1},k\\leftarrow\\mu_{2}}^{(t,i)}=\\frac{1}{m}\\widetilde{\\Theta}(\\widehat{L}^{(t)}\\sigma_{0}^{2}m)\\frac{1}{L^{2}},}\\\\ &{\\displaystyle\\frac{\\partial}{\\partial t}p_{q\\leftarrow\\mu_{1},k\\leftarrow\\mu_{3}}^{(t,i)}=-\\frac{1}{m}\\widetilde{\\Theta}(\\widehat{L}^{(t)}\\sigma_{0}^{2}m)\\frac{1}{L^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "On the other hand, for $\\gamma\\in\\{\\mu_{i}\\}_{i=4}^{d}$ , we have ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\left|\\frac{\\partial}{\\partial t}p_{q\\leftarrow\\mu,k\\leftarrow\\gamma}^{(t,i)}\\right|\\leq\\frac{1}{n m}\\widetilde{\\Theta}({\\widehat{L}}^{(t)}\\sigma_{0}^{2}m)\\frac{1}{L^{2}}+\\widetilde{O}\\left(\\frac{1}{L m}{\\widehat{L}}^{(t)}\\sigma_{0}^{2}\\sqrt{m}\\right).\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Finally, by Lemma F.16, we have ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mu,\\nu}\\left|\\nu^{\\top}W_{K}^{(t)}W_{Q}^{(t)}\\mu-\\nu^{\\top}W_{K}^{(T_{1})}W_{Q}^{(T_{1})}\\mu\\right|\\leq\\tilde{O}\\left(\\frac{\\sigma_{0}^{2}m}{\\sqrt{m}L\\sigma_{1}^{2}m m_{1}}+\\frac{\\sigma_{0}^{2}\\sqrt{m}}{\\sqrt{m}\\sigma_{1}^{2}m m_{1}}\\right).\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Thus, by Lemma G.2, we obtain ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\operatorname*{max}_{i,l_{1},l_{2}}\\left\\vert p_{l_{1},l_{2}}^{(t,i)}-p_{l_{1},l_{2}}^{(T_{1},i)}\\right\\vert}}\\\\ &{}&{\\leq\\tilde{O}\\left(\\frac{\\sigma_{0}^{2}m}{m L^{2}\\sigma_{1}^{2}m m_{1}}+\\frac{\\sigma_{0}^{2}\\sqrt{m}}{m\\sigma_{1}^{2}m m_{1}L}+L\\left(\\frac{\\sigma_{0}^{2}m}{m L\\sigma_{1}^{2}m m_{1}}+\\frac{\\sigma_{0}^{2}\\sqrt{m}}{m\\sigma_{1}^{2}m m_{1}}\\right)^{2}\\right)=O(1/L^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Corollary F.18. For $t\\in[T_{1},T_{2}]$ we have ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\frac{\\operatorname*{max}_{i\\in[n]}\\left\\vert\\sum_{l_{1}=1}^{L}\\sum_{l_{2}=1}^{L}G^{(t)}(X_{l_{2}}^{(i)})\\frac{\\partial p_{l_{1},l_{2}}^{(t,i)}}{\\partial t}\\right\\vert}{\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle}\\leq o(1)\\frac{1}{n}\\sum_{i\\in[n]}g_{i}^{(t)}.\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Proof. This is a direct consequence of Definition F.1, Lemma F.10 and Corollary F.17. ", "page_idx": 60}, {"type": "text", "text": "F.7 Change of Self-Correlation of Key/Query-Transformed data ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Lemma F.19 (Change of self-correlation). For $\\mu,\\nu\\in\\{\\mu_{i}\\}_{i=1}^{d}$ we have ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\nu^{\\top}W_{Q}^{(t)\\top}W_{Q}^{(t)}\\mu-\\nu^{\\top}W_{Q}^{(T_{1})\\top}W_{Q}^{(T_{1})}\\mu\\right|\\leq\\tilde{O}\\left(\\frac{\\sigma_{0}^{2}\\sqrt{m}}{\\sqrt{m}\\sigma_{1}^{2}m m_{1}}\\right),}\\\\ &{\\left|\\nu^{\\top}W_{K}^{(t)\\top}W_{K}^{(t)}\\mu-\\nu^{\\top}W_{K}^{(T_{1})\\top}W_{K}^{(T_{1})}\\mu\\right|\\leq\\tilde{O}\\left(\\frac{\\sigma_{0}^{2}\\sqrt{m}}{\\sqrt{m}\\sigma_{1}^{2}m m_{1}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Proof. We prove the result for vTW(t) T wt) Vu and the proof for vT w(t)Tw(t) is similar. By Lemma C.4, we have ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial\\nu^{\\top}W_{Q}^{(t)\\top}W_{Q}^{(t)}\\mu}{\\partial t}}\\\\ &{=\\frac{1}{n\\sqrt{m}}\\displaystyle\\sum_{i:\\mu\\in X^{(i)}}g_{i}^{(t)}y_{i}\\displaystyle\\sum_{j=1}^{m_{1}}a_{j}\\nu^{\\top}W_{Q}^{(t)\\top}K^{(t,i)}\\mathrm{diag}\\left(V^{(t,i)\\top}w_{j}^{(t)}-w_{j}^{(t)\\top}V^{(t,i)}p_{l(i,\\mu)}^{(t,i)}\\right)p_{l(i,\\mu)}^{(t,i)}}\\\\ &{\\quad+\\displaystyle\\frac{1}{n\\sqrt{m}}\\displaystyle\\sum_{i:\\nu\\in X^{(i)}}g_{i}^{(t)}y_{i}\\displaystyle\\sum_{j=1}^{m_{1}}a_{j}\\mu^{\\top}W_{Q}^{(t)\\top}K^{(t,i)}\\mathrm{diag}\\left(V^{(t,i)\\top}w_{j}^{(t)}-w_{j}^{(t)\\top}V^{(t,i)}p_{l(i,\\nu)}^{(t,i)}\\right)p_{l(i,\\nu)}^{(t,i)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "A simpleresul fom Lemma F16 is hat $|\\nu^{\\top}W_{K}^{(t)\\top}W_{Q}^{(t)}\\mu|\\,\\leq\\,\\widetilde{O}(\\sigma_{0}^{2}\\sqrt{m})$ $t\\ \\in\\ [T_{1},T_{2}]$ and $\\mu,\\nu\\in\\{\\mu_{i}\\}_{i=1}^{d}$ . Therefore, by Definition F.1, ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\left|\\frac{\\partial\\nu^{\\top}W_{Q}^{(t)\\top}W_{Q}^{(t)}\\mu}{\\partial t}\\right|\\leq\\widetilde{O}\\left(\\frac{1}{\\sqrt{m}}\\widehat{L}^{(t)}\\sigma_{0}^{2}\\sqrt{m}\\log m\\right),\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "which implies ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\nu^{\\top}W_{Q}^{(t)\\top}W_{Q}^{(t)}\\mu-\\nu^{\\top}W_{Q}^{(T_{1})\\top}W_{Q}^{(T_{1})}\\mu\\Big|\\leq\\int_{T_{1}}^{t}\\tilde{O}\\left(\\frac{1}{\\sqrt{m}}\\widehat{L}^{(t)}\\sigma_{0}^{2}\\sqrt{m}\\log m\\right)\\,d\\tau\\leq\\tilde{O}\\left(\\frac{\\sigma_{0}^{2}\\sqrt{m}}{\\sqrt{m}\\sigma_{1}^{2}m m_{1}}\\right)\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "where the inequality follows from Theorem F.8 and Definition F.1. ", "page_idx": 61}, {"type": "text", "text": "F.8Small Loss is Achieved ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Theorem F.20. Define $T^{\\star}=\\operatorname*{min}_{t}\\{t:\\ \\widehat{L}^{(t)}=\\Theta(1/\\mathrm{poly}(m))\\}$ Then, $T^{\\star}\\in[T_{1},T_{2}]$ ", "page_idx": 61}, {"type": "text", "text": "Proof. The following results altogether show that Phase 2 can last for at least $\\Theta(\\mathbf{poly}(m))$ time: ", "page_idx": 61}, {"type": "text", "text": "\u00b7 Lemma F.19 proves that the change of $K,Q$ self-correlation as follows: ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mu,\\nu}{\\operatorname*{max}}\\left|\\mu^{\\top}W_{K}^{(t)\\top}W_{K}^{(t)}\\nu-\\mu^{\\top}W_{K}^{(0)\\top}W_{K}^{(0)}\\nu\\right|=\\widetilde O(\\sigma_{0}^{2}\\sqrt{m}),}\\\\ &{\\underset{\\mu,\\nu}{\\operatorname*{max}}\\left|\\mu^{\\top}W_{Q}^{(t)\\top}W_{Q}^{(t)}\\nu-\\mu^{\\top}W_{Q}^{(0)\\top}W_{Q}^{(0)}\\nu\\right|=\\widetilde O(\\sigma_{0}^{2}\\sqrt{m}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "\u00b7 Corollary F.9 proves that ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle\\leq O(\\sigma_{1}^{2}m m_{1}).\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "\u00b7 Corollary F.12 proves that ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\frac{\\operatorname*{max}_{\\mu\\in\\{\\mu_{i}\\}_{i=1}^{d}}\\left\\vert\\sum_{j=1}^{m_{1}}a_{j}\\frac{\\partial w_{j}^{(t)}}{\\partial t}W_{V}^{(t)}\\mu\\right\\vert}{\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle}\\leq o(1/L)\\frac{1}{n}\\sum_{i\\in[n]}g_{i}^{(t)}.\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "\u00b7 Corollary F.15 proves that ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\left|\\sum_{l_{1}=1}^{L}\\sum_{l_{2}:\\;X_{l_{2}}^{(i)}\\in\\{\\mu_{k}\\}_{k=4}^{d}}G^{(t)}(X_{l_{2}}^{(i)})p_{q\\leftarrow l_{1},k\\leftarrow l_{2}}^{(t,i)}\\right|\\leq O(1).\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "\u00b7 Corollary F.17 proves that ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i\\in[n],l_{1},l_{2}\\in[L]}\\left|p_{l_{1},l_{2}}^{(t,i)}-p_{l_{1},l_{2}}^{(0,i)}\\right|\\leq O(1/L^{2}).\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "\u00b7 Corollary F.18 proves that ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\frac{\\operatorname*{max}_{i\\in[n]}\\left\\vert\\sum_{l_{1}=1}^{L}\\sum_{l_{2}=1}^{L}G^{(t)}(X_{l_{2}}^{(i)})\\frac{\\partial p_{l_{1},l_{2}}^{(t,i)}}{\\partial t}\\right\\vert}{\\sum_{j_{1}=1}^{m_{1}}\\sum_{j_{2}=1}^{m_{1}}\\left\\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\\right\\rangle}\\leq o(1)\\frac{1}{n}\\sum_{i\\in[n]}g_{i}^{(t)}.\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "\u00b7 Lemma F.5 implies that ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\leq\\frac{\\sum_{i\\in I_{2}}g_{i}^{(t)}}{\\sum_{i\\in I_{3}}g_{i}^{(t)}}\\leq2.\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "\u00b7 Lemma F6 implies that the gradients $\\textstyle\\sum_{i\\in I}g_{i}^{(t)}$ for $I\\in\\{I_{1},I_{2},I_{3},I_{4}\\}$ satisfies ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\sum_{i\\in I_{4}}g_{i}\\leq\\operatorname*{min}\\left(\\sum_{i\\in I_{2}}g_{i}^{(t)},\\sum_{i\\in I_{3}}g_{i}^{(t)}\\right)\\leq\\operatorname*{max}\\left(\\sum_{i\\in I_{2}}g_{i}^{(t)},\\sum_{i\\in I_{3}}g_{i}^{(t)}\\right)\\leq\\sum_{i\\in I_{1}}g_{i}^{(t)}\\leq\\sum_{i\\in I_{2}\\cup I_{3}\\cup I_{4}}g_{i}^{(t)}.\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "\u00b7 Lemma F.6 and Corollary F.17 proves that $y_{i}F_{i}^{(t)}\\geq y_{i}F_{i}^{(T_{1})}\\geq C$ ", "page_idx": 62}, {"type": "text", "text": "The above shows that all the requirements needed to satisfy the definition of Phase 2 (Definition F.1) can hold for at least $\\Omega({\\mathrm{poly}}(m))$ time. Thus, $T_{2}-T_{1}\\geq\\Omega(\\mathrm{poly}(m))$ . Further, by Theorem F.8, we have that $\\widehat{L}^{(t)}\\leq O(1/\\mathrm{poly}(m))$ implies $t=\\Theta(\\mathrm{poly}(m))$ \u53e3 ", "page_idx": 62}, {"type": "text", "text": "F.9Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Proof. This is proved in Corollary F.13 and Lemma F.16. ", "page_idx": 62}, {"type": "text", "text": "F.10 Proof of Theorem 3.3 ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Proof. This is proved as a direct consequence of Lemma F.6 and Theorem F.20. ", "page_idx": 62}, {"type": "text", "text": "For the generalization loss, since the training loss satisfies $\\widehat{L}^{(T^{\\star})}\\le1/\\mathrm{poly}(m)$ , for each class $I\\in$ $\\{I_{1},I_{2},\\bar{I}_{3},I_{4}\\}$ there exists a sample $X_{I}^{\\star}$ such that $\\ell(X_{I}^{\\star})\\leq1/\\mathrm{poly}(m)$ . Note that by Definition F.1 the random tokens only contributes to $O(1)$ in $F^{(T^{\\star})}(X)$ . Thus, given a fixed new sample $X\\sim{\\mathcal{D}}$ we have $|F^{(T^{\\star})}(X)-F^{(T^{\\star})}(X_{I}^{\\star})|\\leq O(1)$ which implies $\\ell(y F^{(T^{\\star})}(X))\\leq1/\\mathrm{poly}(m)$ Since this holds for all $X\\sim{\\mathcal{D}}$ , we have $L^{(T^{\\star})}\\leq1/\\mathrm{poly}(m)$ \u53e3 ", "page_idx": 62}, {"type": "text", "text": "G Auxiliary Results ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "The gradient of $p(x)_{i}=\\mathrm{Softmax}(x)_{i}$ for $x\\in\\mathbb{R}^{n}$ ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial p(x)_{i}}{\\partial x_{j}}=\\frac{\\partial}{\\partial x_{j}}\\frac{\\exp(x_{i})}{\\sum_{k=1}^{n}\\exp(x_{k})}=\\left\\{\\begin{array}{l l}{-\\frac{\\exp(x_{i})}{\\sum_{k}\\exp(x_{k})}\\frac{\\exp(x_{j})}{\\sum_{k}\\exp(x_{k})}=-p(x)_{i}p(x)_{j}}&{i\\neq j}\\\\ {\\frac{\\exp(x_{i})}{\\sum_{k}\\exp(x_{k})}-\\left(\\frac{\\exp(x_{i})}{\\sum_{k}\\exp(x_{k})}\\right)^{2}=p(x)_{i}(1-p(x)_{i})}&{i=j}\\end{array}\\right.}\\\\ &{\\qquad\\qquad=p(x)_{i}(\\mathbb{I}(i=j)-p(x)_{j})}\\\\ &{\\Rightarrow J(p(x))=\\mathrm{diag}(p(x))-p(x)p(x)^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "Lemma G.1 (Gradient of softmax). Let $s(t)\\in\\mathbb{R}^{l}$ be differentiable in $t$ and $p(s)=\\operatorname{Softmax}(s)$ Denote $p_{i}(s)=\\mathrm{Softmax}(s)_{i}$ Then ", "page_idx": 62}, {"type": "equation", "text": "$$\n{\\frac{\\partial p(s(t))}{\\partial t}}={\\frac{\\partial p(s)}{\\partial s}}{\\frac{\\partial s(t)}{\\partial t}}=(\\operatorname{diag}(p(s))-p(s)p(s)^{\\top}){\\frac{\\partial s(t)}{\\partial t}}.\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "Proof. By the chain rule and the gradient of softmax in Equation (17), we obtain ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\frac{\\partial p(s(t))}{\\partial t}=\\frac{\\partial p(s)}{\\partial s}\\frac{\\partial s(t)}{\\partial t}=J(p(s))\\frac{\\partial s(t)}{\\partial t}=(\\mathrm{diag}(p(s))-p(s)p(s)^{\\top})\\frac{\\partial s(t)}{\\partial t}.\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "Lemma G.2 (Perturbation of softmax). Let $s\\,\\in\\,\\mathbb{R}^{l}$ and $p(s)\\;=\\;\\mathrm{Softmax}(s)$ Denote $p_{i}(s)\\;=\\;$ Softmax $(s)_{i}$ . Consider a small perturbation $\\varepsilon\\in\\mathbb{R}^{l}$ to s. Then ", "page_idx": 62}, {"type": "equation", "text": "$$\np(s+\\varepsilon)-p(s)=(\\mathrm{diag}(p(s))-p(s)p(s)^{\\top})\\varepsilon+\\xi,\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "where $\\|\\xi\\|_{\\infty}=O(\\|\\varepsilon\\|_{2}^{2})$ ", "page_idx": 62}, {"type": "text", "text": "Proof. By Taylor's expansion theorem on softmax and the gradient of softmax in Equation (17), we have ", "page_idx": 62}, {"type": "equation", "text": "$$\np(s+\\varepsilon)-p(s)=J(p(s))\\varepsilon+\\xi=(\\mathrm{diag}(p(s))-p(s)p(s)^{\\top})\\varepsilon+\\xi,\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "where $\\|\\xi\\|_{\\infty}=O(\\|\\varepsilon\\|_{2}^{2})$ ", "page_idx": 62}, {"type": "text", "text": "H Probability ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Lemma H.1 (Bernstein's inequality for bounded random variables). Assume $Z_{1},\\ldots,Z_{n}$ are n i.i.d. randomvariableswith $\\mathbb{E}[Z_{i}]\\bar{\\mathbf{\\alpha}}=0$ and $|Z_{i}|\\leq M$ for all $i\\in[n]$ almost surely. Let $\\textstyle Z=\\sum_{i=1}^{n}Z_{i}$ Then, for all $t>0$ ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\mathbb{P}[Z>t]\\leq\\exp\\left(-\\frac{t^{2}/2}{\\sum_{j=1}^{n}\\mathbb{E}[Z_{j}^{2}]+M t/3}\\right)\\leq\\exp\\left(-\\operatorname*{min}\\left\\{\\frac{t^{2}}{2\\sum_{j=1}^{n}\\mathbb{E}[Z_{j}^{2}]},\\frac{t}{2M}\\right\\}\\right),\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "which implies with probability at least $1-\\delta$ ", "page_idx": 63}, {"type": "equation", "text": "$$\nZ\\leq\\sqrt{2\\sum_{j=1}^{n}\\mathbb{E}[Z_{j}^{2}]\\log\\frac{1}{\\delta}}+2M\\log\\frac{1}{\\delta}.\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Lemma H.2. For $w_{1},w_{2}\\in\\mathbb{R}^{m}$ with $w_{1},w_{2}\\stackrel{i.i.d.}{\\sim}\\mathcal{N}(0,I_{m}/m)$ we have ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{P}\\left[|\\left\\|w_{1}\\right\\|_{2}^{2}-1\\right|\\geq\\sqrt{\\frac{4}{m}\\log\\frac{2}{\\delta}}+\\frac{4}{m}\\log\\frac{2}{\\delta}\\right]\\leq\\delta,}\\\\ {\\mathbb{P}\\left[|\\langle w_{1},w_{2}\\rangle|\\geq\\sqrt{\\frac{4}{m}\\log\\frac{2}{\\delta}}+\\frac{4}{m}\\log\\frac{2}{\\delta}\\right]\\leq\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Proof. We first have ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Vert w_{1}\\Vert_{2}^{2}\\right]=\\mathbb{E}\\left[\\sum_{i=1}^{m}w_{1,i}^{2}\\right]=1.\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Note that $w_{1,i}^{2}$ is a sub-Gamma random variable with parameters $\\textstyle\\left({\\frac{4}{m^{2}}},{\\frac{4}{m}}\\right)$ . Thus, by Bernstein's inequality, ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|\\|w_{1}\\|_{2}^{2}-\\mathbb{E}\\left[\\|w_{1}\\|_{2}^{2}\\right]\\right|\\geq\\sqrt{\\frac{4}{m}\\log\\frac{2}{\\delta}}+\\frac{4}{m}\\log\\frac{2}{\\delta}\\right]\\leq\\delta.\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Next, ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\langle w_{1},w_{2}\\rangle]=\\mathbb{E}\\left[\\sum_{i=1}^{m}w_{1,i}w_{2,i}\\right]=0\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "By Bernstein's inequality, we obtain ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[|\\langle w_{1},w_{2}\\rangle|\\ge\\sqrt{\\frac{4}{m}\\log\\frac{2}{\\delta}}+\\frac{4}{m}\\log\\frac{2}{\\delta}\\right]\\le\\delta.\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 64}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 64}, {"type": "text", "text": "\u00b7 You should answer [Yes] , [No] , or [NA] .   \n\u00b7 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u00b7 Please provide a short (1-2 sentence) justification right after your answer (even for NA). ", "page_idx": 64}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 64}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 64}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 64}, {"type": "text", "text": "\u00b7 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u00b7 Keep the checklist subsection headings, questions/answers and guidelines below. . Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 64}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 64}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 64}, {"type": "text", "text": "Guidelines: ", "page_idx": 64}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 64}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 64}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 64}, {"type": "text", "text": "Guidelines: ", "page_idx": 65}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 65}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 65}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 65}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 65}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 65}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 65}, {"type": "text", "text": "Answer: [Yes]   \nJustification: [NA]   \nGuidelines: \u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b)  If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 65}, {"type": "text", "text": "", "page_idx": 66}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 66}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 66}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 66}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 66}, {"type": "text", "text": "Guidelines: ", "page_idx": 66}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so ^No\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 66}, {"type": "text", "text": "\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 67}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 67}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 67}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 67}, {"type": "text", "text": "Justification: [NA] Guidelines: ", "page_idx": 67}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 67}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 67}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 67}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 67}, {"type": "text", "text": "Justification: The experiments are done with synthetic data of small scale and the behavior of the experiment results are pretty consistent. ", "page_idx": 67}, {"type": "text", "text": "Guidelines: ", "page_idx": 67}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 67}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 67}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 67}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 67}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 67}, {"type": "text", "text": "Guidelines: ", "page_idx": 67}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 67}, {"type": "text", "text": "\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 68}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 68}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 68}, {"type": "text", "text": "Answer: [Yes] Justification: [NA] Guidelines: ", "page_idx": 68}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 68}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 68}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 68}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 68}, {"type": "text", "text": "Justification: [NA] Guidelines: ", "page_idx": 68}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 68}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 68}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 68}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 68}, {"type": "text", "text": "", "page_idx": 68}, {"type": "text", "text": "Guidelines: ", "page_idx": 68}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks. ", "page_idx": 68}, {"type": "text", "text": "\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 69}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 69}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 69}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 69}, {"type": "text", "text": "Guidelines: ", "page_idx": 69}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 69}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 69}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 69}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 69}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 69}, {"type": "text", "text": "Guidelines: ", "page_idx": 69}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 69}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 69}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 69}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 69}, {"type": "text", "text": "Guidelines: ", "page_idx": 70}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with humansubjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 70}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 70}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 70}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 70}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 70}]