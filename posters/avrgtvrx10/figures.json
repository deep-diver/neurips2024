[{"figure_path": "AVrGtVrx10/figures/figures_1_1.jpg", "caption": "Figure 1: In a two-modality scenario, when both modalities are present, the modality-complete representation is derived through fusion. When one modality is absent, the mapped representation inferred from the remaining modality is subject to a certain probability distribution in the modality-complete space.", "description": "This figure illustrates the difference between modality-complete and modality-incomplete data representations in a two-modality scenario.  When both modalities are available, a fused modality-complete representation is created. However, when one modality is missing, the representation derived from the remaining modality is not a single point but rather a probability distribution within the modality-complete space. This distribution is centered around the expected location of the complete representation, reflecting the uncertainty introduced by the missing data.  This visualization highlights the core concept that the paper addresses: instead of enforcing a deterministic mapping between incomplete and complete representations, it's more appropriate to model the relationship probabilistically due to inherent information asymmetry.", "section": "1 Introduction"}, {"figure_path": "AVrGtVrx10/figures/figures_3_1.jpg", "caption": "Figure 2: An overview of the proposed method. PCD is a self-KD architecture, where the teacher and student share the same framework. The teacher provides the modality-complete feature z* and the geometric structure g* to guide the student. In the student, modality-missing features are parameterized as different normal distributions to fit the corresponding PDF. To achieve this, PCD maximizes distributions at positive z and minimizes it at z, while aligning g with positive g*. ", "description": "This figure illustrates the PCD framework, a self-knowledge distillation (self-KD) architecture.  The teacher network, using complete modality data, provides the modality-complete feature vector (z*) and geometric structure (g*) as guidance. The student network, handling modality-missing data, models the missing features as probabilistic distributions (using Gaussian distributions). The objective is twofold: maximize the probability for points close to the complete data (z*) and minimize it for distant points (z*), while ensuring geometric consistency between the student's structure (g) and the teacher's structure (g*).", "section": "3.2 Probabilistic Conformal Distillation"}, {"figure_path": "AVrGtVrx10/figures/figures_8_1.jpg", "caption": "Figure 3: The prediction distributions of both the teacher and the distilled student of PCD under all multimodal combinations on CeFA. The X-axis represents the normalized logit output and the Y-axis is the number of samples after taking the square root.", "description": "This figure visualizes the prediction distributions of both the teacher and student models trained using PCD on the CeFA dataset.  The plots show the normalized logit outputs (x-axis) against the square root of the sample counts (y-axis), separated by class (Class 0 and Class 1). The comparison highlights how PCD improves the clarity of the classification boundary by increasing the separation between the two classes, as evidenced by the concentration of the student's logits closer to 0 or 1.", "section": "Classification Boundary of the Teacher and Student"}, {"figure_path": "AVrGtVrx10/figures/figures_8_2.jpg", "caption": "Figure 4: The average performance of PCD under different \u03bb and \u03c4 values on CeFA. The hyperparameter \u03bb is used to balance the loss terms, \u03c4 is the temperature.", "description": "This figure shows the performance of the PCD model on the CeFA dataset under different values of hyperparameters \u03bb and \u03c4.  The hyperparameter \u03bb balances the probability extremum loss and the geometric consistency loss, while \u03c4 is a temperature parameter in the contrastive learning component that affects the similarity measure. The figure indicates that PCD shows relatively stable performance across a range of \u03bb values between 1.4 and 2.2 and \u03c4 values between 0.1 and 0.6, although there is a slight peak in performance within those ranges.", "section": "4 Experiments"}, {"figure_path": "AVrGtVrx10/figures/figures_16_1.jpg", "caption": "Figure 5: The visualization of the distributions of the modality-complete, RGB, Depth, and IR representations from the unified model without distillation.", "description": "This figure visualizes the distributions of modality-complete features and individual modality features (RGB, Depth, and IR) obtained from a unified model *without* probabilistic conformal distillation.  It shows how the distributions of the individual modalities differ from, but remain similar to, the distribution of the complete modality features. This similarity supports the paper's argument that modality-missing features are probabilistically related to the complete feature representation, rather than deterministically aligned. The differences in distributions highlight the inherent indeterminacy in the mapping from incomplete to complete representations, justifying the use of probabilistic methods like the proposed PCD.", "section": "D Visualization of Feature distribution"}, {"figure_path": "AVrGtVrx10/figures/figures_16_2.jpg", "caption": "Figure 6: The average performance of PCD under different \u03bb and \u03c4 values on CASIA-SURF and CeFA.", "description": "This figure shows the impact of hyperparameters \u03bb and \u03c4 on the performance of the Probabilistic Conformal Distillation (PCD) method. The left panel shows the average performance of PCD across a range of \u03bb values on the CASIA-SURF and CeFA datasets.  The right panel shows the average performance of PCD across a range of \u03c4 values on the same datasets. The plots demonstrate the stability of PCD across a range of hyperparameter values, indicating robustness to hyperparameter tuning. ", "section": "4.3 Further Analysis"}]