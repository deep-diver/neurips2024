[{"type": "text", "text": "Multi-Agent Coordination via Multi-Level Communication ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ziluo Ding1,2,\u2217,\u2021 Zeyuan Liu1,\u2217 Zhirui Fang1,\u2217 Kefan Su2 Liwen Zhu3 ", "page_idx": 0}, {"type": "text", "text": "Zongqing Lu2,\u2020 ", "page_idx": 0}, {"type": "text", "text": "1Tsinghua Shenzhen International Graduate School, Tsinghua University, 2Peking University, 3Tencent AI Lab ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The partial observability and stochasticity in multi-agent settings can be mitigated by accessing more information about others via communication. However, the coordination problem still exists since agents cannot communicate actual actions with each other at the same time due to the circular dependencies. In this paper, we propose a novel multi-level communication scheme, Sequential Communication (SeqComm). SeqComm treats agents asynchronously (the upper-level agents make decisions before the lower-level ones) and has two communication phases. In the negotiation phase, agents determine the priority of decision-making by communicating hidden states of observations and comparing the value of intention, obtained by modeling the environment dynamics. In the launching phase, the upper-level agents take the lead in making decisions and then communicate their actions with the lower-level agents. Theoretically, we prove the policies learned by SeqComm are guaranteed to improve monotonically and converge. Empirically, we show that SeqComm outperforms existing methods in various cooperative multi-agent tasks. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Centralized training with decentralized execution (CTDE) [Lowe et al., 2017] is a popular learning paradigm in cooperative multi-agent reinforcement learning (MARL). Although the centralized value function can be learned to evaluate the joint policy of agents, the decentralized policies of agents are essentially independent. Therefore, a coordination problem arises. That is, agents may make sub-optimal actions by mistakenly assuming others\u2019 actions when there exist multiple optimal joint actions [Busoniu et al., 2008]. Communication allows agents to obtain information about others to avoid miscoordination [Jiang et al., 2024]. However, most existing work only focuses on communicating messages, e.g., the information of agents\u2019 current observation or historical trajectory [Jiang and Lu, 2018, Singh et al., 2019, Das et al., 2019, Ding et al., 2020]. It is impossible for an agent to acquire other\u2019s actions before making decisions since the game model is usually synchronous, i.e., agents make decisions and execute actions simultaneously. ", "page_idx": 0}, {"type": "text", "text": "A general approach to solving the coordination problem is to make sure that ties between equally good actions are broken by all agents. One simple mechanism for doing so is to know exactly what others will do and adjust the behavior accordingly under a unique ordering of agents and actions [Busoniu et al., 2008]. Inspired by this, we reconsider the cooperative game from an asynchronous perspective. In other words, each agent is assigned a priority (i.e. order) of decision-making at each step, thus the Stackelberg equilibrium (SE) [Von Stackelberg, 2010] is naturally set up as the learning objective. Specifically, the upper-level agents make decisions before the lower-level agents (Each agent represents a unique level, with upper and lower levels being relative.). Therefore, the lower-level agents can acquire the actual actions of the upper-level agents by communication and make their decisions conditioned on what the upper-level agents would do. Importantly, we never break the fundamental dynamic, p(st+1|st, a1:k\u22121), in the multi-agent system. The agents make decisions asynchronously but perform actions simultaneously as the default environment setting. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Under this setting, the SE is likely to be Pareto superior to the average Nash equilibrium (NE) in games that require a high cooperation level [Zhang et al., 2020]. However, is it necessary to decide a specific priority of decision-making for each agent? Ideally, the optimal joint policy can be decomposed by any orders [Wen et al., 2019], e.g., $\\pi^{\\ast}(a_{1},a_{2}|s)=\\pi^{\\ast}(a_{1}|s)\\pi^{\\ast}(a_{2}|s,a_{1})=\\pi^{\\ast}(a_{2}|s)\\pi^{\\ast}(a_{1}|s,a_{2}).$ . But during the learning process, agents are unlikely to use other agents\u2019 optimal actions for gradient calculation, making it still vulnerable to the relative overgeneralization problem [Wei et al., 2018]. This means there is no guarantee that different orders will converge to the same suboptimal. We also claim that the different priorities of decision-making may affect the optimality of the convergence of the learning algorithm in Section 3. Note that relative overgeneralization occurs when a suboptimal NE in the joint space of actions is preferred over an optimal NE because each agent\u2019s action in the suboptimal equilibrium is a better choice when matched with arbitrary actions from the cooperative agents. ", "page_idx": 1}, {"type": "text", "text": "This work proposes a novel multi-level communication scheme for cooperative MARL, Sequential Communication (SeqComm), to enable agents to coordinate with each other explicitly. Specifically, SeqComm has two-phase communication, negotiation phase and launching phase. In the negotiation phase, agents communicate their hidden states of observations with others simultaneously. Then, they can generate multiple predicted trajectories, called intention, by modeling the environmental dynamics and other agents\u2019 actions. In addition, the priority of decision-making is determined by communicating and comparing the agents\u2019 intentions, which are evaluated by their state-value functions. The value of each intention represents the predicted rewards obtained by treating that agent as the first mover of the order sequence. The sequence of others follows the same procedure as aforementioned with the upper-level agents fixed. In the launching phase, the upper-level agents take the lead in decision-making and communicate their actual actions with the lower-level agents. The actual actions will be executed simultaneously in the environment without changes. ", "page_idx": 1}, {"type": "text", "text": "SeqComm is currently built on MAPPO [Yu et al., 2021]. Theoretically, we prove the policies learned by SeqComm are guaranteed to improve monotonically and converge. Empirically, we evaluate SeqComm on StarCraft multi-agent challenge v2 (SMACv2) [Samvelyan et al., 2019]. We demonstrate that SeqComm outperforms existing communication-free and communicationbased methods in various maps in SMACv2. By ablation studies, we confirm that treating agents asynchronously is a more effective way to promote coordination, and SeqComm can provide the proper priority of decision-making for agents to develop better coordination. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Communication. Existing work [Jiang and Lu, 2018, Kim et al., 2019, Singh et al., 2019, Das et al., 2019, Zhang et al., 2019, Jiang et al., 2020, Ding et al., 2020, Konan et al., 2022] in this realm mainly focus on how to extract valuable messages. ATOC [Jiang and Lu, 2018] and IC3Net [Singh et al., 2019] utilize gate mechanisms to decide when to communicate with other agents. Several studies [Das et al., 2019, Konan et al., 2022] employ multi-round communication to fully reason the intentions of others and establish complex collaboration strategies. Social influence [Jaques et al., 2019] uses communication to influence the behaviors of others. I2C [Ding et al., 2020] only communicates with agents that are relevant and influential which are determined by causal inference. However, all these methods focus on how to exploit valuable information from current or past partial observations effectively and properly. More recently, some studies [Kim et al., 2021, Du et al., 2021, Pretorius et al., 2021] begin to answer the question: can we favor cooperation beyond sharing partial observation? They allow agents to imagine their future states with a world model and communicate those with others. IS [Pretorius et al., 2021], as the representation of this line of research, enables each agent to share its intention with other agents in the form of the encoded imagined trajectory and use the attention module to figure out the importance of the received intention. However, two concerns arise. On one hand, circular dependencies can lead to inaccurate predicted future trajectories as long as the multi-agent system treats agents synchronously. On the other hand, MARL struggles in extracting useful information from numerous messages, not to mention more complex and dubious messages, i.e. predicted future trajectories. Unlike these studies, we treat the agents from an asynchronous perspective, therefore, circular dependencies can be naturally resolved. Moreover, agents send actions to lower-level agents, making the messages compact and informative. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Coordination. The agents are essentially independent decision-makers in execution and may break ties between equally good actions randomly. Thus, in the absence of additional mechanisms, different agents may break ties in different ways, and the resulting joint actions may be suboptimal. Coordination graphs [Guestrin et al., 2002, B\u00a8ohmer et al., 2020, Wang et al., 2021] simplify the coordination when the global Q-function can be additively decomposed into local Q-functions that only depend on the actions of a subset of agents. Typically, a coordination graph expresses a higherorder value decomposition among agents. This improves the representational capacity to distinguish other agents\u2019 effects on local utility functions, which addresses the miscoordination problems caused by partial observability. ", "page_idx": 2}, {"type": "text", "text": "Another general approach to solving the coordination problem is to make sure that ties are broken by all agents in the same way, requiring that random action choices are somehow coordinated or negotiated. Social conventions [Boutilier, 1996] or role assignments [Prasad et al., 1998] encode prior preferences towards certain joint actions and help break ties during action selection. Communication [Fischer et al., 2004, Vlassis, 2007] can be used to negotiate action choices, either alone or in combination with the aforementioned techniques. Our method follows this line of research by utilizing the ordering of agents and actions to break the ties, other than the enhanced representational capacity of the local value function. ", "page_idx": 2}, {"type": "text", "text": "More discussions of related work are in Appendix C. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Cost-Free Communication. The decentralized partially observable Markov decision process (DecPOMDP) can be extended to multi-agent POMDP [Oliehoek et al., 2016] by sharing observations among agents via communication. The joint observations are not necessarily equivalent to the state. However, joint observations can be used to represent the state better than single observations. ", "page_idx": 2}, {"type": "text", "text": "Previous work [Pynadath and Tambe, 2002] shows that under cost-free communication, agents would share optimal messages for mutual interest. If the communication cost is high, there is a balance between delivering all the useful messages for greater benefits and keeping the amount of communication as low as possible. In addition, analyzing this extreme case gives us some understanding of the benefit of communication, even if the results do not apply across all domains. However, even under multi-agent POMDP where agents can get joint observations, coordination problems can still arise [Busoniu et al., 2008]. Suppose the centralized critic has learnt actions pairs $\\bar{[}a_{1},a_{2}]$ and $[b_{1},b_{2}]$ that are equally optimal. Without any prior information, the individual policies $\\pi_{1}$ and $\\pi_{2}$ learned from the centralized critic can break the ties randomly and may choose $a_{1}$ and $b_{2}$ , respectively. ", "page_idx": 2}, {"type": "text", "text": "Multi-Agent Sequential Decision-Making. We consider fully cooperative multi-agent tasks that are modeled as multi-agent POMDP, where $n$ agents interact with the environment according to the following procedure, which we refer to as multi-agent sequential decision-making. ", "page_idx": 2}, {"type": "text", "text": "At each timestep $t$ , assume the priority (i.e. order) of decision-making for all agents is given and each priority level has only one agent (i.e., agents make decisions one by one). Note that the smaller the level index, the higher priority of decision-making is. The agent at each level $k$ gets its own observation $o_{t}^{k}$ drawn from the state $s_{t}$ , and receives messages $m_{t}^{-k}$ kfrom all other agents, $\\pmb{m}_{t}^{-k}\\triangleq\\{\\{o_{t}^{1},\\bar{a}_{t}^{1}\\},\\dots,\\{o_{t}^{k-1},a_{t}^{k-1}\\},o_{t}^{k+1},\\dots,o_{t}^{n}\\}$ .  alEl qaugievnatlse netxlcy,e $m_{t}^{-k}$ inc apnr abcet icwer,i tatgeen natss $\\{o_{t}^{-k},\\pmb{a}_{t}^{1:k-1}\\}$ $o_{t}^{-k}$ $k$ communicate the hidden states/encodings of observations), and at1:k\u22121denotes the joint actions of agents 1 to $k-1$ . For the agent at the first level $(i.e.,k=1)$ , at1:k\u22121= \u2205. Then, the agent determines its action $a_{t}^{k}$ sampled from its policy $\\pi_{k}(\\cdot|o_{t}^{k},m_{t}^{-k})$ or equivalently $\\pi_{k}(\\cdot|o_{t},a_{t}^{1:k-1})$ at1:k\u22121) and sends it to the lower-level agents. After all, agents have determined their actions, they perform the joint actions ", "page_idx": 2}, {"type": "image", "img_path": "3l2HnZXNou/tmp/35660809d2d5689efce97a59435737940fd9971aa041dc033281d67ea3b2bc00.jpg", "img_caption": ["(a) payoff matrix of the game "], "img_footnote": [], "page_idx": 3}, {"type": "image", "img_path": "3l2HnZXNou/tmp/b9d458d74eacc5b24ae805c5cb499842fa274058b2062669cb924af3d9338523.jpg", "img_caption": ["(b) evaluations of different methods "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 1: (a) Payoff matrix for a one-step game. There are multiple local optima. (b) Evaluations of different methods for the game in terms of the mean reward and standard deviation of ten runs. $A\\rightarrow B$ , $B\\rightarrow A$ , Simultaneous, and Learned represent that agent $A$ makes decisions first, agent $B$ makes decisions first, two agents make decisions simultaneously, and there is another learned policy determining the priority of decision making, respectively. MAPPO $\\mathrm{\\Delta[Yu}$ et al., 2021] is used as the backbone. ", "page_idx": 3}, {"type": "text", "text": "$\\mathbf{\\deltaa}_{t}$ , which can be seen as sampled from the joint policy $\\pi(\\cdot|s_{t})$ factorized as $\\textstyle\\prod_{k=1}^{n}\\pi_{k}(\\cdot|o_{t},a_{t}^{1:k-1})$ , in the environment and get a shared reward $r(s_{t},\\pmb{a}_{t})$ and the state transitions t o next state $s^{\\prime}$ according to the transition probability $p(s^{\\prime}|s_{t},\\pmb{a}_{t})$ . All agents aim to maximize the expected return $\\textstyle\\sum_{t=0}^{\\infty}\\gamma^{t}{\\bar{r_{t}}}$ , where $\\gamma$ is the discount factor. The state-value function and action-value function of the le vel- $k$ agent are defined as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{\\pi_{k}}(s,a^{1:k-1})\\triangleq\\underset{a_{0}^{k:n}\\sim\\pi_{k:n}}{\\mathbb{E}}[\\sum_{=0}^{\\infty}\\gamma^{t}r_{t}|s_{0}=s,a_{0}^{1:k-1}=a^{1:k-1}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\underset{a_{1:\\infty}\\sim\\pi}{a_{0:n}^{k:n}\\sim\\pi_{k:n}}\\;t=0}\\\\ &{Q_{\\pi_{k}}(s,a^{1:k})\\triangleq\\underset{a_{0}^{k+1:n}\\sim\\pi_{k+1:n}}{\\mathbb{E}}[\\sum_{=0}^{\\infty}\\gamma^{t}r_{t}|s_{0}=s,a_{0}^{1:k}=a^{1:k}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For the setting of multi-agent sequential decision-making discussed above, we have the following proposition. ", "page_idx": 3}, {"type": "text", "text": "Proposition 1. If all the agents update their policy with individual TRPO [Schulman et al., 2015] sequentially in multi-agent sequential decision-making, then the joint policy of all agents are guaranteed to improve monotonically and converge. ", "page_idx": 3}, {"type": "text", "text": "Proposition 1 indicates that SeqComm has the performance guarantee regardless of the priority of decision-making in multi-agent sequential decision-making. However, the priority of decision-making indeed affects the optimality of the converged joint policy, and we have the following claim. ", "page_idx": 3}, {"type": "text", "text": "Claim 1. The different priorities of decision-making affect the optimality of the convergence of the learning algorithm due to the relative overgeneralization problem. ", "page_idx": 3}, {"type": "text", "text": "We use a one-step matrix game as an example, as illustrated in Figure 1(a), to demonstrate the influence of the priority of decision-making on the learning process. Due to relative overgeneralization [Wei et al., 2018], agent $B$ tends to choose $b_{2}$ or $b_{3}$ . Specifically, $b_{2}$ or $b_{3}$ in the suboptimal equilibrium is a better choice than $b_{1}$ in the optimal equilibrium when matched with arbitrary actions from agent $A$ . Therefore, as shown in Figure 1(b), $B\\rightarrow A$ (i.e., agent $B$ makes decisions before $A$ , and $A$ \u2019s policy conditions on the action of $B$ ) and Simultaneous (i.e., two agents make decisions simultaneously and independently) are easily trapped into local optima. However, if agent $A$ goes first, things can be different, as $A\\rightarrow B$ achieves the optimum. As long as agent $A$ does not suffer from relative overgeneralization, it can help agent $B$ get rid of local optima by narrowing down the search space of $B$ . Besides, a policy that determines the priority of decision-making can be learned under the guidance of the state-value function, denoted as Learned. It obtains better performance than $B\\rightarrow A$ and Simultaneous, which indicates that dynamically determining the order during policy learning can be beneficial as we do not know the optimal priority in advance. ", "page_idx": 3}, {"type": "image", "img_path": "3l2HnZXNou/tmp/b77c13dc8023177a0129d91667fd39add9b0b0a0b975391f0a9f7ef32ecf8af7.jpg", "img_caption": ["Figure 2: Overview of SeqComm. SeqComm has two communication phases, the negotiation phase (left) and the launching phase (right). In the negotiation phase, agents communicate hidden states of observations with others and obtain their own intention. The priority of decision-making is determined by sharing and comparing the value of all the intentions. In the launching phase, the agents who hold the upper-level positions will make decisions prior to the lower-level agents. Besides, their actions will be shared with anyone that has not yet made decisions. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Remark 1. The priority (i.e. order) of decision-making affects the optimality of the converged joint policy in multi-agent sequential decision-making, thus it is critical to determine the order. However, learning the order directly requires an additional centralized policy in execution, which is not generalizable in a scenario where the number of agents varies. Moreover, its learning complexity exponentially increases with the number of agents, making it infeasible in many cases. ", "page_idx": 4}, {"type": "text", "text": "4 Sequential Communication ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this paper, we cast our eyes in another direction and resort to the world model, which is the dynamic model of the environment. Ideally, we can randomly sample candidate order sequences, evaluate them under the world model (see Section 4.1), and choose the order sequence that is deemed the most promising under the true dynamic. SeqComm is designed based on this principle to determine the priority of decision-making via communication. ", "page_idx": 4}, {"type": "text", "text": "In SeqComm, communication is separated into phases serving different purposes and multi-round communication in one phase is possible. One is the negotiation phase for agents to determine the priority of decision-making. Another is the launching phase for agents to act conditioning on actual actions upper-level agents will take to implement explicit coordination via communication. The overview of SeqComm is illustrated in Figure 2. Each SeqComm agent consists of a policy, a critic, and a world model, as illustrated in Figure 3, and the parameters of all networks are shared across agents [Gupta et al., 2017]. ", "page_idx": 4}, {"type": "text", "text": "World Model. The world model is needed to predict and evaluate future trajectories. SeqComm, unlike previous works [Kim et al., 2021, Du et al., 2021, Pretorius et al., 2021], can utilize received hidden states of other agents in the first round of communication to model more precise environment dynamics for the explicit coordination in the next round of communication. Once an agent can access other agents\u2019 hidden states, it shall have adequate information to estimate their actions since all agents are homogeneous and parameter-sharing. Therefore, the world model $\\mathcal{M}(\\cdot)$ takes as input the joint hidden states $\\pmb{h}_{t}=\\{h_{t}^{\\updownarrow},\\ldots,h_{t}^{n}\\}$ and actions $\\mathbf{\\delta}_{a_{t}}$ , and predicts the next joint observations and reward. In practice, before the inputs pass into the world model, the attention module $\\mathrm{AM_{w}}$ is utilized to process the input. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\pmb{o}}_{t+1},\\hat{r}_{t+1}=\\mathcal{M}_{i}(\\mathrm{AM}_{\\mathrm{w}}(\\pmb{h}_{t},\\pmb{a}_{t})).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The reason that we adopt the attention module is to entitle the world model to be generalizable in the scenarios where additional agents are introduced or existing agents are removed. ", "page_idx": 4}, {"type": "text", "text": "4.1 Negotiation Phase ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the negotiation phase, the observation encoder first takes $o_{t}$ as input and outputs a hidden state $h_{t}$ to compress the information, which is used to communicate with others. Note that many studies [Ding et al., 2020, Jiang and Lu, 2018] found that redundant messages may impair the learning process empirically. In more detail, the model can converge slowly or sometimes lead to a worse sub-optimal. Agents then determine the priority of decision-making by intentions which is established and evaluated based on the world model. ", "page_idx": 5}, {"type": "text", "text": "Priority of Decision-Making. Intention is the key element in determining the priority of decisionmaking. The notion of intention is described as an agent\u2019s future behavior in previous works [Rabinowitz et al., 2018, Raileanu et al., 2018, Kim et al., 2021]. However, we define the intention as an agent\u2019s future behavior without considering others. ", "page_idx": 5}, {"type": "text", "text": "As mentioned before, an agent\u2019s intention considering others can lead to circular dependencies and cause miscoordination. By our definition, the intention of an agent should be depicted as all future trajectories considering that agent as the first mover and ignoring the others. However, there are many possible future trajectories as the priority of the rest of the agents is unfixed. In practice, we use the Monte Carlo method to estimate the intention value based on all future trajectories. Note that it is uniform across priorities for unfixed agents. Each order should be treated equally since we do not have any prior for the distribution. ", "page_idx": 5}, {"type": "text", "text": "Taking agent $i$ at timestep $t$ to illustrate, it firstly considers itself as the first-mover and produces its action only based on the joint hidden states, $\\hat{a}_{t}^{i}\\stackrel{\\cdot}{\\sim}\\pi_{i}(\\cdot|\\mathrm{AM_{a}}(h_{t},\\emptyset)$ , where we again use an attention module $\\mathrm{{AM_{a}}}$ to handle the input. For the order sequence of lower-level agents, we randomly sample a set of order sequences from unfixed agents. Assume agent $j$ is the second-mover, agent $i$ models $j$ \u2019s action by considering the upper-level action following its own policy $\\hat{a}_{t}^{j}\\sim\\pi_{i}(\\cdot|\\mathrm{AM_{a}}(h_{t},\\hat{a}_{t}^{i}))$ . The same procedure is applied to predict the actions of all other agents following the sampled order sequence. Based on the joint hidden states and predicted actions, the next joint observations $\\hat{\\pmb{o}}_{t+1}$ can be predicted by the world model $\\mathcal{M}$ . The length of the predicted future trajectory is $H$ and it can then be written as $\\tau^{t}=\\left\\{\\hat{\\pmb{o}}_{t+1},\\hat{\\pmb{a}}_{t+1},\\dots,\\hat{\\pmb{o}}_{t+H},\\hat{\\pmb{a}}_{t+H}\\right\\}$ by repeating the procedure aforementioned. ", "page_idx": 5}, {"type": "text", "text": "Then, the agent uses its critic (state-value function) to evaluate the future trajectory and output value $v_{\\tau^{t}}$ . The intention value is defined as the average value of $F$ future trajectories with different sampled order sequences. Through the critic, we have linked the order and agent performance together. ", "page_idx": 5}, {"type": "text", "text": "After all the agents have computed their intentions and the corresponding value, they again communicate their intention values to others. Then, agents would compare and choose the agent with the highest intention value to be the first mover. The priority of lower-level decision-making follows the same procedure with the upper-level agents fixed. Note that some agents may communicate intention values with others multiple times until the priority of decision-making is finally determined. ", "page_idx": 5}, {"type": "text", "text": "4.2 Negotiation Phase for Local Communication ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The full communication version of SeqComm is constructed based on theoretical derivation. It has a theoretical guarantee to some extent, but some assumptions, e.g., broadcast communication, can be unrealistic and incur lots of communication overhead. Therefore, we provide another version of SeqComm in scenarios where agents can only communicate with nearby agents (agents within a limited communication range). ", "page_idx": 5}, {"type": "text", "text": "In more detail, the agent first calculates its intention value based only on the hidden states of nearby agents. After comparing with the intention values of nearby agents (intention values are communicated with the nearby agents), the agent can determine the upper-level and lower-level nearby agents. Unlike the previous version of SeqComm, agents cannot distinguish the detailed order sequence of the upper-level nearby agents since their communication ranges may not overlap. Therefore, the intention values are calculated and communicated among agents for only one time. The local communication version greatly reduces communication overhead, making it more suitable for many real applications. ", "page_idx": 5}, {"type": "text", "text": "For more details of the algorithms, please refer to the Appendix D for the pseudo-code. ", "page_idx": 5}, {"type": "text", "text": "4.3 Launching Phase ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "As for the launching phase, agents communicate to obtain additional information to make decisions. Apart from the received hidden states from the last phase, we allow agents to get what actual actions the upper-level agents will take in execution, while other studies can only infer others\u2019 actions by opponent modeling [Rabinowitz et al., 2018, Raileanu et al., 2018] or communicating intentions [Kim et al., 2021]. Therefore, miscoordination can be naturally avoided, and a better cooperation strategy is possible since lower-level agents can adjust their behaviors accordingly. ", "page_idx": 6}, {"type": "text", "text": "A lower-level agent $i$ make a decision following the policy $\\bar{\\pi}_{i}(\\cdot|\\mathrm{AM_{a}}(h_{t},a_{t}^{u p p e r}))$ , where ${\\pmb a}_{t}^{\\bar{u}p p e r}$ means received actual actions from all upper-level agents. As long as the agent has decided on an action, it will send the action to all other lower-level agents through the communication channel. Note that the actions are executed simultaneously and distributedly in execution, though agents make decisions sequentially. ", "page_idx": 6}, {"type": "image", "img_path": "3l2HnZXNou/tmp/9556938926e84201b28c5110524ca448207a8dd6cc8dd2381eb38d7deb614412.jpg", "img_caption": ["Figure 3: Architecture of SeqComm. The critic and policy of each agent take input as its own observation and received messages. The world model takes as input the joint hidden states and predicted joint actions. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.4 Theoretical Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As intention values determine the priority of decision-making, SeqComm is likely to choose different orders at different timesteps during training. However, we have the following proposition that theoretically guarantees the performance of the learned joint policy under SeqComm. ", "page_idx": 6}, {"type": "text", "text": "Proposition 2. The monotonic improvement and convergence of the joint policy in SeqComm are independent of the priority of decision-making of agents at each timestep. ", "page_idx": 6}, {"type": "text", "text": "Proof. The proof is given in Appendix A. ", "page_idx": 6}, {"type": "text", "text": "The priority of decision-making is chosen under the world model, thus the compounding errors in the world model can result in discrepancies between the predicted returns of the same order under the world model and the true dynamics. We then analyze the monotonic improvement for the joint policy under the world model based on Janner et al. [2019]. ", "page_idx": 6}, {"type": "text", "text": "Theorem 1. Let the expected total variation between two transition distributions be bounded at each timestep as $\\mathrm{max}_{t}$ $\\mathbb{E}_{s\\sim\\pi_{\\beta,t}}[D_{T V}(p(s^{\\prime}|s,\\pmb{a})||\\hat{p}(s^{\\prime}|s,\\pmb{a}))]\\le\\epsilon_{m},$ , and the policy divergences at level $k$ be bounded as $\\begin{array}{r}{\\operatorname*{max}_{s,a^{1:k-1}}D_{T V}(\\pi_{\\beta,k}(a^{k}|s,a^{1:k-1})||\\pi_{k}(a^{k}|s,a^{1:k-1}))\\leq\\epsilon_{\\pi_{k}}}\\end{array}$ , where $\\pi_{\\beta}$ is the data collecting policy for the model and $\\hat{p}(s^{\\prime}|s,a)$ is the transition distribution under the model. Then the model return $\\hat{\\eta}$ and true return $\\eta$ of the policy $\\pi$ are bounded as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\eta}[\\pi]\\geq\\eta[\\pi]-}\\\\ &{\\underbrace{[\\frac{2\\gamma r_{\\operatorname*{max}}\\left(\\epsilon_{m}+2\\sum_{k=1}^{n}\\epsilon_{\\pi_{k}}\\right)}{(1-\\gamma)^{2}}+\\frac{4r_{\\operatorname*{max}}\\sum_{k=1}^{n}\\epsilon_{\\pi_{k}}}{(1-\\gamma)}]}_{C(\\epsilon_{m},\\epsilon_{\\pi_{1:n}})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Proof. The proof is given in Appendix B. ", "page_idx": 6}, {"type": "text", "text": "Remark 2. Theorem 1 provides a useful relationship between the compounding errors and the policy update. As long as we improve the return under the true dynamic by more than the gap, $C(\\epsilon_{m},\\bar{\\epsilon}_{\\pi_{1:n}})$ , we can guarantee the policy improvement under the world model. If no such policy exists to overcome the gap, it implies the model error is too high, that is, there is a large discrepancy between the world model and true dynamics. Thus the order sequence obtained under the world model is not reliable. Such an order sequence is almost the same as a random one. Though a random order sequence also has the theoretical guarantee of Proposition 2, we will show in Section 5.2 that a random order sequence leads to a poor local optimum empirically. ", "page_idx": 6}, {"type": "image", "img_path": "3l2HnZXNou/tmp/3170e3985cc90194ba9dfe3e5466d3c285fa4923db9f9b32b407a9176d5fd7f7.jpg", "img_caption": ["Figure 4: Learning curves of SeqComm and baselines in nine SMACv2 maps. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "SeqComm is currently instantiated based on MAPPO [Yu et al., 2021]. We evaluate SeqComm nine maps in StarCraft multi-agent challenge v2 (SMACv2) [Ellis et al., 2024]. ", "page_idx": 7}, {"type": "text", "text": "In the experiments, SeqComm and baselines are parameter-sharing for fast convergence [Gupta et al., 2017, Terry et al., 2020]. We have fine-tuned the baselines for a fair comparison. The world model in the SMACv2 environment is trained from scratch and kept fine-tuned in the learning process. Therefore, no extra prior knowledge is provided. Please refer to the Appendix for the hyperparameter settings. All results are presented in terms of the mean and standard deviation of five runs with different random seeds. ", "page_idx": 7}, {"type": "text", "text": "5.1 Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "SMACv2. We have evaluated our method on the most representative and challenging multi-agent environment currently available. Compared with SMAC [Samvelyan et al., 2019], SMACv2 has some better properties, i.e. stochasticity and partial observability. In other words, agents need to cooperate more in the new environment to complete tasks, whereas they could achieve a certain success rate without cooperation in the original environment. ", "page_idx": 7}, {"type": "text", "text": "We have chosen nine maps for extensive evaluation and made some minor changes to the observation part of agents to make it more difficult. Specifically, the sight range of agents is reduced from 9 to 3, and agents cannot perceive any information about their allies even if they are within the sight range. NDQ [Wang et al., 2020] adopts a similar change to increase the difficulty of action coordination. The rest of the settings remain the same as the default. In summary, we require the environment to be one where a high success rate cannot be achieved solely based on individual observations. ", "page_idx": 7}, {"type": "text", "text": "We also evaluate the local communication version of SeqComm. Agents can only communicate with nearby agents (agents within their communication range). Note that the map size and the total number of agents restrict the number of nearby agents. As the task progresses, the number of nearby agents is from 2 to 4. ", "page_idx": 7}, {"type": "image", "img_path": "3l2HnZXNou/tmp/fb00aa7555e50bcb682e555dbf280eac2227b1399fe8c2aa1a4f22542b4c9952.jpg", "img_caption": ["Figure 5: Ablation studies of the communication ranges. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Analysis. The learning curves of SeqComm and the baselines in terms of the win rate are illustrated in Figure 4. All communication-based methods perform better than communicaion-free method (MAPPO). In easy scenarios, communication may not be very useful, but experiments have shown that in cases with significant partial observability and stochasticity, communication can greatly enhance agent ability. ", "page_idx": 8}, {"type": "text", "text": "We compare our method with TarMAC [Das et al., 2019], which holds a similar position in communication settings to that of MAPPO in communication-free settings. SeqComm outperforms TarMAC in all maps, which verifies the gain of explicit action coordination. Moreover, the full-communication version performs better than the local-communication version because the former can access more information. However, it also costs more communication overhead. ", "page_idx": 8}, {"type": "text", "text": "5.2 Ablation Studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Priority of Decision-Making. We primarily want to contribute a practical version to the community. Moreover, the fewer communicative agents there are, the fewer possible orders there are, thus increasing the probability of randomly obtaining a good order. It would be more meaningful to demonstrate that devoting effort to finding a good order is still important in such a scenario. Therefore, we do the ablation study for the local version of the SeqComm. In more detail, we compare SeqComm with two ablation baselines: the priority of decision-making is determined randomly at each timestep, denoted as Random, and agents only access the observations of others during training and execution, denoted as No action. ", "page_idx": 8}, {"type": "text", "text": "As depicted in Figure 7, SeqComm achieves a higher win rate than Random and No action in all the maps. These results verify the importance of the priority of decision-making and the necessity to adjust it continuously during one episode. It is also demonstrated that SeqComm can provide a proper priority of decision-making. As discussed in Section 4.4, although Random also has the theoretical guarantee, they converge to poor local optima in practice. Surprisingly, in most tasks, Random performs worse than No action. It again verifies that a bad order may fail to improve coordination or even impair it. ", "page_idx": 8}, {"type": "text", "text": "Communication Range. We also conduct experiments to demonstrate the impact of different communication ranges. We set communication ranges to $\\{1,\\,3,\\,9\\}$ , in addition to the default range of 6. We notice a steady improvement in performance as the communication range increases. Therefore, the choice of communication range is a trade-off between communication overhead and agent performance. In our previous experiments, we choose a compromise value of 3 for the local version to validate the effectiveness of our method. Results refer to Figure 5. ", "page_idx": 8}, {"type": "image", "img_path": "3l2HnZXNou/tmp/aa844006e07b4e453ab8af52b9251e9739c89288569311a3006d7559e3d98960.jpg", "img_caption": ["Figure 6: Ablation studies on the network mechanisms. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "3l2HnZXNou/tmp/e4caeee5fa0f5959bc9f4c5459957c64fee2fe1b89d97245b2b8f1c36540c2fc.jpg", "img_caption": ["Figure 7: Ablation studies under local communication in SMACv2. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Network Mechanisum. We replaced the attention mechanism for local communication with an aggregation method. In more detail, messages are concatenated and passd into a five-layer linear neural networks. The curve is based on 3 random seeds and tested on the terran 10v10 map. The results refer to Figure 6. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have proposed SeqComm, which enables agents to coordinate well and explicitly with each other, and it, from an asynchronous perspective, allows agents to make decisions sequentially. A two-phase communication scheme has been adopted to determine the priority of decision-making and transfer messages accordingly. Empirically, it is demonstrated that SeqComm outperforms baselines in a variety of cooperative multi-agent scenarios. ", "page_idx": 9}, {"type": "text", "text": "Limitations. The assumption of accessing the local observation of any other agent could be strong since it is unsuitable for all applications. Thus, we provide a local communication version of SeqComm for assumption relaxation in the experiment. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the STI 2030-Major Projects under Grant 2021ZD0201404 and the NSFC under Grants 62450001 and 62476008. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Wendelin Bo\u00a8hmer, Vitaly Kurin, and Shimon Whiteson. Deep coordination graphs. In International Conference on Machine Learning (ICML), 2020.   \nCraig Boutilier. Planning, learning and coordination in multiagent decision processes. In Conference on Theoretical Aspects of Rationality and Knowledge, 1996.   \nLucian Busoniu, Robert Babuska, and Bart De Schutter. A comprehensive survey of multiagent reinforcement learning. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 38(2):156\u2013172, 2008.   \nAbhishek Das, The\u00b4ophile Gervet, Joshua Romoff, Dhruv Batra, Devi Parikh, Mike Rabbat, and Joelle Pineau. Tarmac: Targeted multi-agent communication. In International Conference on Machine Learning (ICML), 2019.   \nZiluo Ding, Tiejun Huang, and Zongqing Lu. Learning individually inferred communication for multi-agent cooperation. In Advances in Neural Information Processing Systems (NeurIPS), 2020.   \nYali Du, Yifan Zhao, Meng Fang, Jun Wang, Gangyan Xu, and Haifeng Zhang. Learning predictive communication by imagination in networked system control, 2021.   \nBenjamin Ellis, Jonathan Cook, Skander Moalla, Mikayel Samvelyan, Mingfei Sun, Anuj Mahajan, Jakob Foerster, and Shimon Whiteson. Smacv2: An improved benchmark for cooperative multiagent reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024.   \nFelix Fischer, Michael Rovatsos, and Gerhard Weiss. Hierarchical reinforcement learning in communication-mediated multiagent coordination. In International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2004.   \nAmy Greenwald, Keith Hall, and Roberto Serrano. Correlated q-learning. In A comprehensive survey of multiagent reinforcement learning, 2003.   \nCarlos Guestrin, Michail Lagoudakis, and Ronald Parr. Coordinated reinforcement learning. In International Conference on Machine Learning (ICML), 2002.   \nJayesh K Gupta, Maxim Egorov, and Mykel Kochenderfer. Cooperative multi-agent control using deep reinforcement learning. In International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2017.   \nMichael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based policy optimization. In Advances in Neural Information Processing Systems (NeurIPS), 2019.   \nNatasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro Ortega, Dj Strouse, Joel Z Leibo, and Nando De Freitas. Social influence as intrinsic motivation for multi-agent deep reinforcement learning. In International Conference on Machine Learning (ICML), 2019.   \nHaobin Jiang, Ziluo Ding, and Zongqing Lu. Settling decentralized multi-agent coordinated exploration by novelty sharing. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 17444\u201317452, 2024.   \nJiechuan Jiang and Zongqing Lu. Learning attentional communication for multi-agent cooperation. Advances in Neural Information Processing Systems (NeurIPS), 2018.   \nJiechuan Jiang, Chen Dun, Tiejun Huang, and Zongqing Lu. Graph convolutional reinforcement learning. In International Conference on Learning Representation (ICLR), 2020.   \nDaewoo Kim, Sangwoo Moon, David Hostallero, Wan Ju Kang, Taeyoung Lee, Kyunghwan Son, and Yung Yi. Learning to schedule communication in multi-agent reinforcement learning. In International Conference on Learning Representations (ICLR), 2019.   \nWoojun Kim, Jongeui Park, and Youngchul Sung. Communication in multi-agent reinforcement learning: Intention sharing. In International Conference on Learning Representations (ICLR), 2021.   \nSachin Konan, Esmaeil Seraj, and Matthew Gombolay. Iterated reasoning with mutual information in cooperative and byzantine decentralized teaming. In International Conference on Learning Representations (ICLR), 2022.   \nVille Ko\u00a8no\u00a8nen. Asymmetric multiagent reinforcement learning. Web Intelligence and Agent Systems: An international journal, 2(2):105\u2013121, 2004.   \nRyan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information Processing Systems (NeurIPS), 2017.   \nHang Ma, Daniel Harabor, Peter J Stuckey, Jiaoyang Li, and Sven Koenig. Searching with consistent prioritization for multi-agent path finding. In AAAI Conference on Artificial Intelligence (AAAI), 2019.   \nFrans A Oliehoek, Christopher Amato, et al. A concise introduction to decentralized POMDPs, volume 1. Springer, 2016.   \nMV Nagendra Prasad, Victor R Lesser, and Susan E Lander. Learning organizational roles for negotiated search in a multiagent system. International Journal of Human-Computer Studies, 48 (1):51\u201367, 1998.   \nArnu Pretorius, Scott Cameron, Andries Petrus Smit, Elan van Biljon, Lawrence Francis, Femi Azeez, Alexandre Laterre, and Karim Beguir. Learning to communicate through imagination with model-based deep multi-agent reinforcement learning, 2021.   \nDavid V. Pynadath and Milind Tambe. The communicative multiagent team decision problem: Analyzing teamwork theories and models. J. Artif. Intell. Res., 16:389\u2013423, 2002.   \nNeil Rabinowitz, Frank Perbet, Francis Song, Chiyuan Zhang, SM Ali Eslami, and Matthew Botvinick. Machine theory of mind. In International Conference on Machine Learning (ICML), 2018.   \nRoberta Raileanu, Emily Denton, Arthur Szlam, and Rob Fergus. Modeling others using oneself in multi-agent reinforcement learning. In International Conference on Machine Learning (ICML), 2018.   \nMikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli, Tim G. J. Rudner, Chia-Man Hung, Philiph H. S. Torr, Jakob Foerster, and Shimon Whiteson. The StarCraft Multi-Agent Challenge. arXiv preprint arXiv:1902.04043, 2019.   \nJohn Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning (ICML), 2015.   \nAmanpreet Singh, Tushar Jain, and Sainbayar Sukhbaatar. Individualized controlled continuous communication model for multiagent cooperative and competitive tasks. In International Conference on Learning Representations (ICLR), 2019.   \nEric Sodomka, Elizabeth Hilliard, Michael Littman, and Amy Greenwald. Coco-q: Learning in stochastic games with side payments. In International Conference on Machine Learning (ICML), 2013.   \nJustin K. Terry, Nathaniel Grammel, Ananth Hari, Luis Santos, Benjamin Black, and Dinesh Manocha. Parameter sharing is surprisingly useful for multi-agent deep reinforcement learning. arXiv preprint arXiv:2005.13625, 2020.   \nJur P Van Den Berg and Mark H Overmars. Prioritized motion planning for multiple robots. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2005.   \nNikos Vlassis. A concise introduction to multiagent systems and distributed artificial intelligence. Synthesis Lectures on Artificial Intelligence and Machine Learning, 1(1):1\u201371, 2007.   \nHeinrich Von Stackelberg. Market structure and equilibrium. Springer Science & Business Media, 2010.   \nTonghan Wang, Jianhao Wang, Chongyi Zheng, and Chongjie Zhang. Learning nearly decomposable value functions via communication minimization. In International Conference on Learning Representation (ICLR), 2020.   \nTonghan Wang, Liang Zeng, Weijun Dong, Qianlan Yang, Yang Yu, and Chongjie Zhang. Contextaware sparse deep coordination graphs. arXiv preprint arXiv:2106.02886, 2021.   \nErmo Wei, Drew Wicke, David Freelan, and Sean Luke. Multiagent soft q-learning. In AAAI Spring Symposium Series, 2018.   \nYing Wen, Yaodong Yang, Rui Luo, Jun Wang, and W Pan. Probabilistic recursive reasoning for multi-agent reinforcement learning. In International Conference on Learning Representations (ICLR), 2019.   \nChao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising effectiveness of mappo in cooperative, multi-agent games. arXiv preprint arXiv:2103.01955, 2021.   \nHaifeng Zhang, Weizhe Chen, Zeren Huang, Minne Li, Yaodong Yang, Weinan Zhang, and Jun Wang. Bi-level actor-critic for multi-agent coordination. In AAAI Conference on Artificial Intelligence (AAAI), 2020.   \nSai Qian Zhang, Qi Zhang, and Jieyu Lin. Efficient communication in multi-agent reinforcement learning via variance based control. In Advances in Neural Information Processing Systems (NeurIPS), 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Proofs of Proposition 1 and Proposition 2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Lemma 1 (Agent-by-Agent PPO). If we update the policy of each agent i with TRPO Schulman et al. [2015] (or approximately PPO) when fixing all the other agent\u2019s policies, then the joint policy will improve monotonically. ", "page_idx": 13}, {"type": "text", "text": "Proof. We consider the joint surrogate objective in TRPO $L_{\\pi_{\\mathrm{old}}}(\\pi_{\\mathrm{new}})$ where $\\pi_{\\mathrm{old}}$ is the joint policy before updating and $\\pi_{\\mathrm{new}}$ is the joint policy after updating. ", "page_idx": 13}, {"type": "text", "text": "Given that $\\pi_{\\mathrm{new}}^{-i}=\\pi_{\\mathrm{old}}^{-i}$ , we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{L_{\\pi_{\\mathrm{old}}}(\\pi_{\\mathrm{new}})=\\mathbb{E}_{\\alpha\\sim\\pi_{\\mathrm{new}}}[A_{\\pi_{\\mathrm{add}}}(s,a)]}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{\\alpha\\sim\\pi_{\\mathrm{add}}}\\Big[\\frac{\\pi_{\\mathrm{new}}(a|s)}{\\pi_{\\mathrm{old}}(a|s)}A_{\\pi_{\\mathrm{old}}}(s,a)]}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{\\alpha\\sim\\pi_{\\mathrm{old}}}\\Big[\\frac{\\pi_{\\mathrm{new}}^{2}(a^{\\dagger}|s)}{\\pi_{\\mathrm{old}}^{2}(a^{\\dagger}|s)}A_{\\pi_{\\mathrm{old}}}(s,a)]}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{a^{+}\\sim\\pi_{\\mathrm{old}}^{4}}\\Big[\\frac{\\pi_{\\mathrm{new}}^{2}(a^{\\dagger}|s)}{\\pi_{\\mathrm{old}}^{2}(a^{\\dagger}|s)}\\mathbb{E}_{a^{+}\\sim\\pi_{\\mathrm{old}}^{-4}}[A_{\\pi_{\\mathrm{old}}}(s,a^{\\dagger},a^{-i})]\\Big]}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{a^{+}\\sim\\pi_{\\mathrm{old}}^{4}}\\Big[\\frac{\\pi_{\\mathrm{new}}^{2}(a^{\\dagger}|s)}{\\pi_{\\mathrm{old}}^{2}(a^{\\dagger}|s)}A_{\\pi_{\\mathrm{old}}}(s,a^{i})\\Big]}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{\\pi_{\\mathrm{id}}^{+}}(\\pi_{\\mathrm{new}}^{4}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where Ai\u03c0old(s, ai) = Ea\u2212i\u223c\u03c0o\u2212lid[ $A_{\\pmb{\\pi}_{\\mathrm{old}}}^{i}(s,a^{i})=\\mathbb{E}_{a^{-i}\\sim\\pi_{\\mathrm{old}}^{-i}}[A_{\\pmb{\\pi}_{\\mathrm{old}}}(s,a^{i},a^{-i})]$ is the individual advantage of agent $i$ , and the third equation is from the condition \u03c0n\u2212eiw $\\pi_{\\mathrm{new}}^{-i}=\\pi_{\\mathrm{old}}^{-i}$ ", "page_idx": 13}, {"type": "text", "text": "With the result of TRPO, we have the following conclusion: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J(\\pi_{\\mathrm{new}})-J(\\pi_{\\mathrm{old}})\\geq L_{\\pi_{\\mathrm{old}}}(\\pi_{\\mathrm{new}})-\\mathrm{CD}_{\\mathrm{KL}}^{\\mathrm{max}}(\\pi_{\\mathrm{new}}||\\pi_{\\mathrm{old}})}\\\\ &{\\qquad\\qquad\\qquad=L_{\\pi_{\\mathrm{old}}^{i}}(\\pi_{\\mathrm{new}}^{i})-\\mathrm{CD}_{\\mathrm{KL}}^{\\mathrm{max}}(\\pi_{\\mathrm{new}}^{i}||\\pi_{\\mathrm{old}}^{i})\\quad(\\mathrm{from}\\,\\pi_{\\mathrm{new}}^{-i}=\\pi_{\\mathrm{old}}^{-i})}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This means the individual objective is the same as the joint objective so the monotonic improvement is guaranteed. \u53e3 ", "page_idx": 13}, {"type": "text", "text": "Then we can show the proof of Proposition 1. ", "page_idx": 13}, {"type": "text", "text": "Proof. We will build a new MDP $\\tilde{M}$ based on the original MDP. We keep the action space $\\tilde{A}=A=$ $\\times_{i=1}^{n}A^{i}$ , where $A^{i}$ is the original action space of agent $i$ . The new state space contains multiple layers. We define $\\tilde{S}^{k}=S\\times\\left(\\times_{i=1}^{k}A^{i}\\right)$ for $k=1,2,\\cdots\\,,n-1$ and $\\tilde{S}^{0}=S$ , where $S$ is the original state space. Then a new state $\\tilde{s}^{k}\\in\\tilde{S}^{k}$ means that $\\tilde{s}^{k}=(s,a^{1},a^{2},\\cdots,a^{k})$ . The total new state space is defined as $\\tilde{S}=\\cup_{i=0}^{n-1}\\tilde{S}^{i}$ . Next we define the transition probability $\\tilde{P}$ as following: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{P}(\\tilde{s}^{\\prime}|\\tilde{s}^{k},a^{k+1},a^{-(k+1)})=\\mathbb{1}\\left(\\tilde{s}^{\\prime}=(\\tilde{s}^{k},a^{k+1})\\right),\\;k<n-1}\\\\ &{\\tilde{P}(\\tilde{s}^{\\prime}|\\tilde{s}^{k},a^{k+1},a^{-(k+1)})=\\mathbb{1}\\left(\\tilde{s}^{\\prime}\\in\\tilde{S}^{0}\\right)P(\\tilde{s}^{\\prime}|\\tilde{s}^{k},a^{k+1}),\\;k=n-1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This means that the state in the layer $k$ can only transition to the state in the layer $k+1$ with the corresponding action, and the state in the layer $n-1$ will transition to the layer 0 with the probability $P$ in the original MDP. The reward function $\\tilde{r}$ is defined as following: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\tilde{r}(s,\\mathbf{a})=\\mathbb{1}\\left(\\tilde{s}\\in\\tilde{S}_{0}\\right)r(\\tilde{s},\\mathbf{a}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This means the reward is only obtained when the state in layer 0 and the value is the same as the original reward function. Now we obtain the total definition of the new MDP $\\tilde{M}=\\{\\tilde{S},\\tilde{A},\\tilde{P},\\tilde{r},\\gamma\\}$ . Then we claim that if all agents learn in multi-agent sequential decision-making by PPO, they are actually taking agent-by-agent PPO in the new MDP $\\tilde{M}$ . To be precise, one update of multi-agent sequential decision-making in the original MDP $M$ equals to a round of update from agent 1 to agent $n$ by agent-by-agent PPO in the new MDP $\\tilde{M}$ . Moreover, the total reward of a round in the new MDP $\\tilde{M}$ is the same as the reward in one timestep in the original MDP $M$ . With this conclusion and Lemma 1, we complete the proof. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "The proof of Proposition 2 can be seen as a corollary of the proof of Proposition 1. ", "page_idx": 14}, {"type": "text", "text": "Proof. From Lemma 1 we know that the monotonic improvement of the joint policy in the new MDP $\\tilde{M}$ is guaranteed for each update of one single agent\u2019s policy. So even if the different round of updates in the new MDP $\\tilde{M}$ is with different order of the decision-making, the monotonic improvement of the joint policy is still guaranteed. Finally, from the proof of Proposition 1, we know that the monotonic improvement in the new MDP $\\tilde{M}$ equals to the monotonic improvement in the original MDP $M$ . These complete the proof. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "B Proofs of Theorem 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Lemma 2 (TVD of the joint distributions). Suppose we have two distribution $p_{1}(x,y)\\;\\;=\\;\\;$ $p_{1}(x)p_{1}(x|y)$ and $p_{2}(x,y)\\;=\\;p_{2}(x)p_{2}(x|y)$ . We can bound the total variation distance of the joint as: ", "page_idx": 14}, {"type": "equation", "text": "$$\nD_{T V}(p_{1}(x,y)||p_{2}(x,y))\\leq D_{T V}(p_{1}(x)||p_{2}(x))+\\operatorname*{max}_{x}D_{T V}(p_{1}(y|x)||p_{2}(y|x))\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. See [Janner et al., 2019] (Lemma B.1). ", "page_idx": 14}, {"type": "text", "text": "Lemma 3 (Markov chain TVD bound, time-varing). Suppose the expected $K L$ -divergence between two transition is bounded as maxt $\\mathbb{E}_{s\\sim p_{1,t}(s)}D_{K L}\\bigl(p_{1}(s^{\\prime}|s)||p_{2}(s^{\\prime}|s)\\bigr)\\,\\leq\\,\\delta$ , and the initial state distributions are the same $p_{1,t=0}(s)=p_{2,t=0}(s)$ . Then the distance in the state marginal is bounded as: ", "page_idx": 14}, {"type": "equation", "text": "$$\nD_{T V}(p_{1,t}(s)||p_{2,t}(s))\\leq t\\delta\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. See [Janner et al., 2019] (Lemma B.2). ", "page_idx": 14}, {"type": "text", "text": "Lemma 4 (Branched Returns Bound). Suppose the expected KL-divergence between two dynamics distributions is bounded as maxt $\\mathbb{E}_{s\\sim p_{1,t}(s)}[D_{T V}(p_{1}(s^{\\prime}|s,\\pmb{a})||p_{2}(s^{\\prime}|s,\\pmb{a}))]$ , and the policy divergences at level $k$ are bounded as $\\begin{array}{r}{\\operatorname*{max}_{s,a^{1:k-1}}D_{T V}\\big(\\pi_{1}(a^{k}|s,a^{1:k-1})||\\pi_{2}(a^{k}|s,a^{1:k-1})\\big)\\ \\leq\\ \\epsilon_{\\pi_{k}}}\\end{array}$ . Then the returns are bounded as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n|\\eta_{1}-\\eta_{2}|\\leq\\frac{2r_{\\operatorname*{max}}\\gamma(\\epsilon_{m}+\\sum_{k=1}^{n}\\epsilon_{\\pi_{k}})}{(1-\\gamma)^{2}}+\\frac{2r_{\\operatorname*{max}}\\sum_{k=1}^{n}\\epsilon_{\\pi_{k}}}{1-\\gamma},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $r_{\\mathrm{max}}$ is the upper bound of the reward function. ", "page_idx": 14}, {"type": "text", "text": "Proof. Here, $\\eta_{1}$ denotes the returns of $\\pi_{1}$ under dynamics $p_{1}\\!\\left(s^{\\prime}|s,\\pmb{a}\\right)$ , and $\\eta_{2}$ denotes the returns of $\\pi_{2}$ under dynamics $p_{2}(s^{\\prime}|s,\\pmb{a})$ . Then we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\eta_{1}-\\eta_{2}|=|\\sum_{s,a}(p_{1}(s,a)-p_{2}(s,a))r(s,a)|}\\\\ &{\\qquad\\quad=|\\sum_{t}\\sum_{s,a}\\gamma^{t}(p_{1,t}(s,a)-p_{2,t}(s,a))r(s,a)|}\\\\ &{\\displaystyle\\qquad\\leq\\sum_{t}\\sum_{s,a}\\gamma^{t}|p_{1,t}(s,a)-p_{2,t}(s,a)|r(s,a)}\\\\ &{\\displaystyle\\qquad\\leq r\\operatorname*{max}_{t}\\sum_{s,a}\\gamma^{t}|p_{1,t}(s,a)-p_{2,t}(s,a)|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By Lemma 2, we get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{max}_{s}D_{T V}\\big(\\pi_{1}(a|s)\\big|\\big|\\pi_{2}(a|s)\\big)\\leq\\displaystyle\\operatorname*{max}_{s,a_{1}}D_{T V}\\big(\\pi_{1}(a^{-1}|s,a^{1})\\big|\\big|\\pi_{2}(a^{-1}|s,a^{1})\\big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\operatorname*{max}_{s}D_{T V}\\big(\\pi_{1}(a^{1}|s)\\big|\\big|\\pi_{2}(a^{1}|s)\\big)}\\\\ &{\\leq\\displaystyle\\cdots}\\\\ &{\\leq\\displaystyle\\sum_{k=1}^{n}\\operatorname*{max}_{s,a^{1:k-1}}D_{T V}\\big(\\pi_{1}(a^{k}|s,a^{1:k-1})\\big|\\big|\\pi_{2}(a^{k}|s,a^{1:k-1})\\big)}\\\\ &{\\leq\\displaystyle\\sum_{k=1}^{n}\\epsilon_{\\pi_{k}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We then apply Lemma 3, using $\\begin{array}{r}{\\delta=\\epsilon_{m}+\\sum_{k=1}^{n}\\epsilon_{\\pi_{k}}}\\end{array}$ (via Lemma 3 and 2) to get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{D_{T V}(p_{1,t}(s)||p_{2,t}(s))\\leq t\\operatorname*{max}_{t}E_{s\\sim p_{1,t}(s)}D_{T V}(p_{1,t}(s^{\\prime}|s)||p_{2,t}(s^{\\prime}|s))}&{}\\\\ {\\leq t\\operatorname*{max}_{t}E_{s\\sim p_{1,t}(s)}D_{T V}(p_{1,t}(s^{\\prime},a|s)||p_{2,t}(s^{\\prime},a|s))}&{}\\\\ {\\leq t(\\operatorname*{max}_{t}E_{s\\sim p_{1,t}(s)}D_{T V}(p_{1,t}(s^{\\prime}|s,a)||p_{2,t}(s^{\\prime}|s,a))}&{}\\\\ {+\\operatorname*{max}_{t}E_{s\\sim p_{1,t}(s)}\\operatorname*{max}_{s}D_{T V}(\\pi_{1,t}(a|s)||\\pi_{2,t}(a|s)))}&{}\\\\ {\\leq t(\\epsilon_{m}+\\displaystyle\\sum_{k=1}^{n}\\epsilon_{\\pi_{k}})}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "And we also get $\\begin{array}{r}{D_{T V}\\big(p_{1,t}(s,\\mathbf{a})||p_{2,t}(s,\\mathbf{a})\\big)\\leq t\\big(\\epsilon_{m}+\\sum_{k=1}^{n}\\epsilon_{\\pi_{k}}\\big)+\\sum_{k=1}^{n}\\epsilon_{\\pi_{k}}}\\end{array}$ by Lemma 2. Thus, by plugging this back, we get: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\left\\lvert\\eta_{1}-\\eta_{2}\\right\\rvert\\leq r_{\\operatorname*{max}}\\sum_{t}\\sum_{s,a}\\gamma^{t}\\big\\lvert p_{1,t}(s,\\mathbf{a})-p_{2,t}(s,\\mathbf{a})\\big\\rvert}\\\\ &{\\qquad\\qquad\\leq2r_{\\operatorname*{max}}\\displaystyle\\sum_{t}\\gamma^{t}\\big(t(\\epsilon_{m}+\\displaystyle\\sum_{k=1}^{n}\\epsilon_{\\pi_{k}})+\\displaystyle\\sum_{k=1}^{n}\\epsilon_{\\pi_{k}}\\big)}\\\\ &{\\qquad\\qquad\\leq2r_{\\operatorname*{max}}\\big(\\displaystyle\\frac{\\gamma\\big(\\epsilon_{m}+\\sum_{k=1}^{n}\\epsilon_{\\pi_{k}}\\big)}{(1-\\gamma)^{2}}+\\displaystyle\\frac{\\sum_{k=1}^{n}\\epsilon_{\\pi_{k}}}{1-\\gamma}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then we can show the proof of Theorem 1. ", "page_idx": 15}, {"type": "text", "text": "Proof. Let $\\pi_{\\beta}$ denote the data collecting policy. We use Lemma 4 to bound the returns, but it will require bounded model error under the new policy $\\pi$ . Thus, we need to introduce $\\pi_{\\beta}$ by adding and subtracting $\\eta[\\pmb{\\pi}_{\\beta}]$ , to get: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{\\eta}[\\pmb{\\pi}]-\\eta[\\pmb{\\pi}]=\\hat{\\eta}[\\pmb{\\pi}]-\\eta[\\pmb{\\pi}_{\\beta}]+\\eta[\\pmb{\\pi}_{\\beta}]-\\eta[\\pmb{\\pi}].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "we can bound $L_{1}$ and $L_{2}$ both using Lemma 4 by using $\\begin{array}{r}{\\delta=\\sum_{k=1}^{n}\\epsilon_{\\pi_{k}}}\\end{array}$ and $\\begin{array}{r}{\\delta=\\epsilon_{m}+\\sum_{k=1}^{n}\\epsilon_{\\pi_{k}}}\\end{array}$ respectively, and obtain: ", "page_idx": 15}, {"type": "equation", "text": "$$\nL_{1}\\ge-\\frac{2\\gamma r_{\\operatorname*{max}}\\sum_{k=1}^{n}\\epsilon_{\\pi_{k}}}{(1-\\gamma)^{2}}-\\frac{2r_{\\operatorname*{max}}\\sum_{k=1}^{n}\\epsilon_{\\pi_{k}}}{(1-\\gamma)}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\nL_{2}\\ge-\\frac{2\\gamma r_{\\operatorname*{max}}(\\epsilon_{\\pi_{m}}+\\sum_{k=1}^{n}\\epsilon_{\\pi_{k}})}{(1-\\gamma)^{2}}-\\frac{2r_{\\operatorname*{max}}\\sum_{k=1}^{n}\\epsilon_{\\pi_{k}}}{(1-\\gamma)}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Adding these two bounds together yields the conclusion. ", "page_idx": 15}, {"type": "text", "text": "C Additional Related Work ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Reinforcement Learning in Stackelberg Game Many previous studies [Ko\u00a8no\u00a8nen, 2004, Sodomka et al., 2013, Greenwald et al., 2003, Zhang et al., 2020] have investigated reinforcement learning in finding Stackelberg equilibrium. Bi-AC [Zhang et al., 2020] is a bi-level actor-critic method that allows agents to have different knowledge base so that Stackelberg equilibrium (SE) is possible to find. The actions can still be executed simultaneously and distributedly. It empirically studies the relationship between the cooperation level and the superiority of Stackelberg equilibrium to Nash equilibrium. AQL [Ko\u00a8no\u00a8nen, 2004] updates the Q-value by solving the SE in each iteration and can be regarded as the value-based version of Bi-AC. ", "page_idx": 16}, {"type": "text", "text": "Existing work mainly focuses on two-agent settings, and their order is fixed in advance. However, fixed order can hardly be an optimal solution, especially for large-scale homogeneous agent scenarios. To address this issue, we exploit agents\u2019 intentions to dynamically determine the priority of decisionmaking along the way of interacting with each other. ", "page_idx": 16}, {"type": "text", "text": "Multi-Agent Path Finding (MAPF) MAPF aims to plan collision-free paths for multiple agents on a given graph from their given start vertices to target vertices. In MAPF, prioritized planning is deeply coupled with collision avoidance [Van Den Berg and Overmars, 2005, Ma et al., 2019], where collision is used to design constraints or heuristics for planning. ", "page_idx": 16}, {"type": "text", "text": "We will distinguish MAPF from our work from three perspectives, i.e. problem definition, the motivation behind agent ordering, and the incompatibility of the two methods. ", "page_idx": 16}, {"type": "text", "text": "Problem definition: MAPF aims to plan collision-free paths for multiple agents on a given graph from their given start vertices to their given target vertices. However, we aim to find a communicationbased solution for any Markov decision process with interests aligned. MDP covers lots of possible coordination-needed scenarios, not just avoiding collisions. Besides, each agent has no specific given target. ", "page_idx": 16}, {"type": "text", "text": "Motivation: In MAPF, prioritized planning does not offer completeness or optimality guarantees. It is nevertheless popular because of its efficiency. In addition, the order is mainly used for avoiding collision. Unlike MAPF, our main contribution is to introduce prioritized decision-making to MARL and a method to determine the priority of decision-making. To the best of our knowledge, determining the priority of decision-making for learning algorithms has not been investigated. Moreover, combining with learning algorithms will make prioritized decision-making more general (solving MDPs), not just motion planning. ", "page_idx": 16}, {"type": "text", "text": "Methods: The different motivations and problems to solve will lead to the incompatibility of the algorithms in the two fields. For MAPF, the order is assigned arbitrarily or derived from the problem at hand. Collision is the keyword and prioritized planning is deeply coupled with this specific coordination problem so that better performance can be achieved. Taking the method Ma et al. [2019] as an example, their two algorithms are confilct-driven search frameworks. That is, collision is used to design some constraints which are guided for search. In MARL, we have lots of unseen coordination problems and we cannot enumerate them all to design constraints. ", "page_idx": 16}, {"type": "text", "text": "D Implementation Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "D.1 Algorithm ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this part, we provide the pseudo-code of SeqComm as below: ", "page_idx": 16}, {"type": "text", "text": "Algorithm 1 Negotiation Phase ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Require: Number of agents $N$ ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "$\\bar{\\mathcal{P}}=[\\,]$ : already determined priority   \n$\\mathcal{A}=\\dot{\\{1,2,...,N\\}}$ : remaining agents   \n$/*$ Agents communicate the hidden state $^h$ of their observations with each other\\*/ d ", "page_idx": 16}, {"type": "text", "text": "for $j$ in $\\boldsymbol{\\mathcal{A}}$ do Compute agent $j$ \u2019s intention value $v_{j}$ via Algorithm 2 $/*$ Agents in $\\boldsymbol{\\mathcal{A}}$ communicate the intention values with each other\\*/ Set $p_{i}$ to be the agent $j$ with the maximum $v_{j}$ Append $p_{i}$ to $\\mathcal{P}$ and remove it from $\\boldsymbol{\\mathcal{A}}$ ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "end for ", "page_idx": 17}, {"type": "text", "text": "Algorithm 2 Intention Value Calculation of Agent $a$ ", "page_idx": 17}, {"type": "text", "text": "Require: Already determined priority $\\mathcal{P}$ , remaining agents $\\boldsymbol{\\mathcal{A}}$ , number of sampling trajectories $F$ , length of predicted future trajectory $H$ , policy $\\pi$ and attention module $\\mathrm{{AM_{a}}}$ , world model $\\mathcal{M}$ and attention module $\\mathrm{AM}_{\\mathrm{w}}$ , discount factor $\\gamma$ for $i=1,2,...,F$ do Randomly shuffle $A\\setminus\\{a\\}$ to sample a decision-making priority ${\\mathcal{P}}_{{\\mathcal{A}}\\backslash\\{a\\}}$ of the remaining agents except agent $a$ for $j=0,1,...,H-1$ do $\\hat{\\pmb{a}}^{u p p e r}=\\{\\}$ : predicted actions from all upper-level agents for $k$ in $\\mathrm{Concat}(\\mathcal{P},[a],\\mathcal{P}_{A\\backslash\\{a\\}})$ do Sample $\\hat{a}^{k}$ following $\\pi(\\cdot|\\mathrm{AM_{a}}(h_{t+j},a^{u p p e r}))$ Append \u02c6ak to \u02c6aupper end for Rollout one step with the world model $\\hat{\\pmb{o}}_{t+j+1},\\hat{r}_{t+j+1}=\\mathcal{M}(\\mathrm{AM}_{\\mathrm{w}}(\\pmb{h}_{t+j},\\pmb{a}^{u p p e r}))$ end for Compute the return of the trajectory vi = tt\u2032+=Ht+1 \u03b3t via the critic end for Compute the average return $\\begin{array}{r}{v=\\frac{1}{F}\\sum_{i=1}^{F}v_{i}}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "Algorithm 3 Launching Phase ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Require: Decision-making priority $\\mathcal{P}$ , policy $\\pi$ and $\\mathrm{{AM_{a}}}$ $\\bar{\\pmb{a}_{t}^{u p p e r}}=\\{\\}$ : actions from all upper-level agents for $i$ in $\\mathcal{P}$ do Sample $a_{t}^{i}$ following $\\pi_{i}(\\cdot|\\mathrm{AM_{a}}(h_{t},a_{t}^{u p p e r}))$ Append $a_{t}^{i}$ to $\\pmb{a}_{t}^{u p p e\\bar{r}}$ /\\* Send atu $\\pmb{a}_{t}^{\\bar{u}p p e r}$ to the lower agent\\*/ end for Interact with the environment with $\\mathbf{\\deltaa}_{t}$ ", "page_idx": 17}, {"type": "text", "text": "We also provide the pseudo-code of the local communication version as below: ", "page_idx": 17}, {"type": "text", "text": "Algorithm 4 Local Negotiation Phase of Agent $a$ ", "page_idx": 17}, {"type": "text", "text": "Require: Neighbouring agents $\\mathcal{N}$ $/*$ Agents communicate the hidden state $^h$ of their observations with neighbouring agents\\*/ Compute local intention $v_{a}$ via Algorithm 5 $/*$ Send $v_{a}$ to neighbouring agents and receive $\\{v_{i}\\}_{i\\in\\mathcal{N}}$ from them \\*/ Set upper-level neighbouring agents ${\\mathcal{N}}^{u p p e r}=\\{i\\mid v_{i}>v_{a},i\\in{\\mathcal{N}}\\}$ Set lower-level neighbouring agents $\\mathcal{N}^{l o w e r}=\\{i\\mid v_{i}<v_{a},i\\in\\mathcal{N}\\}$ ", "page_idx": 17}, {"type": "text", "text": "Algorithm 5 Local Intention Value Calculation of Agent $a$ ", "page_idx": 17}, {"type": "text", "text": "Require: Neighbouring agents $\\mathcal{N}$ , number of sampling trajectories $F$ , length of predicted future trajectory $H$ , policy $\\pi$ and $\\mathrm{{AM_{a}}}$ , world model $\\mathcal{M}$ and $\\mathrm{AM_{w}}$ , discount factor $\\gamma$ for $i=1,2,...,F$ do Randomly shuffle $\\mathcal{N}$ to sample a local decision-making priority $\\mathcal{P}_{\\mathcal{N}}$ for $j=0,1,...,H-1$ do $\\hat{\\pmb{a}}^{u p p e r}=\\{\\}$ : predicted actions from all upper-level agents for $k$ in $\\mathrm{Concat}([a],\\mathcal{P}_{\\mathcal{N}})$ do Sample $\\hat{a}^{k}$ following $\\pi(\\cdot|\\mathrm{AM_{a}}(h_{t+j},a^{u p p e r}))$ Append \u02c6ak to \u02c6aupper end for Rollout one step with the world model $\\hat{\\pmb{o}}_{t+j+1},\\hat{r}_{t+j+1}=\\mathcal{M}(\\mathrm{AM}_{\\mathrm{w}}(\\pmb{h}_{t+j},\\pmb{a}^{u p p e r}))$ ", "page_idx": 17}, {"type": "text", "text": "end for Compute the return of the trajectory $\\begin{array}{r}{v_{i}=\\sum_{t^{\\prime}=t+1}^{t+H}\\gamma^{t^{\\prime}-t-1}\\hat{r}_{t^{\\prime}}}\\end{array}$ end for Compute the average return $\\begin{array}{r}{v=\\frac{1}{F}\\sum_{i=1}^{F}v_{i}}\\end{array}$ via the critic ", "page_idx": 18}, {"type": "text", "text": "Algorithm 6 Local Launching Phase of Agent $a$ ", "page_idx": 18}, {"type": "text", "text": "Require: Upper-level neighbouring agents $\\mathcal{N}^{u p p e r}$ , lower-level neighbouring agents $N^{l o w e r}$ , policy $\\pi$ and $\\mathrm{{AM_{a}}}$ $/*$ Receive upper-level actions $\\pmb{a}_{t}^{u p p e r}$ from all upper-level neighbouring agents N upper /\\* Sample $a_{t}^{i}$ following $\\pi_{i}(\\cdot|\\mathrm{AM_{a}}(h_{t},a_{t}^{u p p e r}))$ $/*$ Send $a_{t}^{i}$ to all lower-level neighbouring agents $\\mathcal{N}^{l o w e r\\ *}\\slash$ Interact with the environment with $\\mathbf{\\deltaa}_{t}$ ", "page_idx": 18}, {"type": "text", "text": "D.2 Architecture and Hyperparameters ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Our models, including SeqComm and its ablations, are implemented based on MAPPO. Two fully connected layers realize the critic and policy network. As for the attention module, key, query, and value have one fully connected layer each. The size of the hidden layers is 100. Tanh functions are used as nonlinearity. As there is no released code of TarMAC, we implement TarMAC by ourselves, following the instructions mentioned in the original papers [Das et al., 2019]. ", "page_idx": 18}, {"type": "text", "text": "For the world model, observations and actions are firstly encoded by a fully connected layer. The output size for the observation encoder is 48, and the output size for the action encoder is 16. Then, the outputs of the encoder will be passed into the attention module using the same structure aforementioned. Finally, we use a fully connected layer to decode. In these layers, Tanh is used as the nonlinearity. ", "page_idx": 18}, {"type": "text", "text": "SeqComm and its ablation baseline share the same hyperparameters. For Protoss, the learning rate is $\\mathrm{1e^{-5}}$ , while for Terran and Zerg, the learning rate is $\\bar{2.5\\mathrm{e}^{-5}}$ . $H$ and $F$ for calculating intention value is set to 20 and 2. For TarMAC, the learning rate is tuned as $5\\mathrm{e}^{-5}$ . TarMAC adopts MAPPO as the backbone and two-round communication mechanism. For MAPPO, we follow the default settings of the official code [Yu et al., 2021]. ", "page_idx": 18}, {"type": "text", "text": "D.3 Attention Module ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Attention module (AM) is applied to process messages in the world model, critic network, and policy network. AM consists of three components: query, key, and values. The output of AM is the weighted sum of values, where the weight of value is determined by the dot product of the query and the corresponding key. ", "page_idx": 18}, {"type": "text", "text": "For AM in the world model denoted as $\\mathrm{AM_{w}}$ , agent $i$ gets messages $m_{t}^{-i}=h_{t}^{-i}$ from all other agents at timestep $t$ in negotiation phase, and predicts a query vector $q_{t}^{i}$ following $\\mathrm{AM}_{\\mathrm{w},q}^{i}(h_{t}^{i})$ . The query is used to compute a dot product with keys $\\pmb{k}_{t}=[k_{t}^{1},\\cdot\\cdot\\cdot\\,,k_{t}^{n}]$ . Note that $k_{t}^{j}$ is obtained by the message from\u221a agent $j$ following $\\mathrm{AM}_{\\mathrm{a},k}^{i}(h_{t}^{j})$ for $j\\ne i$ , and $k_{t}^{i}$ is from $\\mathrm{AM}_{\\mathrm{neg},k}^{i}(h_{t}^{i})$ . Besides, it is scaled by $1/\\sqrt{d_{k}}$ followed by a softmax to obtain attention weights $\\alpha$ for each value vector: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\alpha_{i}=\\mathrm{softmax}\\left[\\frac{q_{t}^{i^{T}}k_{t}^{1}}{\\sqrt{d_{k}}}\\cdot\\cdot\\underbrace{\\frac{q_{t}^{i^{T}}k_{t}^{j}}{\\sqrt{d_{k}}}}_{\\alpha_{i j}}\\cdot\\cdot\\cdot\\frac{q_{t}^{i^{T}}k_{t}^{n}}{\\sqrt{d_{k}}}\\right]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The output of attention module is defined as: $\\begin{array}{r}{c_{t}^{i}=\\sum_{j=1}^{n}\\alpha_{i j}v_{t}^{j}}\\end{array}$ , where $v_{t}^{j}$ is obtained from messages or its own hidden state of observation following $\\mathrm{AM}_{\\mathrm{w},v}^{i}(\\cdot)$ . ", "page_idx": 18}, {"type": "text", "text": "As for AM in the policy and critic network denoted as $\\mathrm{{AM}_{a}}$ , agent $i$ gets additional messages from buep peexrp-laenvdeel da agse $m_{t}^{u p p e r}=[h_{t}^{u p p e\\bar{r}},\\mathbf{\\bar{\\alpha}}_{t}^{u p p e r}]$ atupper] mlto haen dm $m_{t}^{l o w e r}=[h_{t}^{l\\bar{o}\\bar{w}e r},0]$ ,v reel sapnedc tliovewleyr. -lIen vaedl daigtieonnt,  ctahne query depends on agent\u2019s own hidden state of observation $h_{t}^{i}$ , but keys and values are only from messages of other agents. ", "page_idx": 18}, {"type": "text", "text": "D.4 Training ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The training of SeqComm is an extension of MAPPO. The observation encoder $e$ , the critic $V$ , and the policy $\\pi$ are respectively parameterized by $\\theta_{e}$ , $\\theta_{v}$ , $\\theta_{\\pi}$ . Besides, the attention module $\\mathrm{{AM_{a}}}$ is parameterized by $\\theta_{a}$ and takes as input the agent\u2019s hidden state, the messages (hidden states of other agents) in the negotiation phase, and the messages (the actions of upper-level agents) in launching phase. Let D = {\u03c4k}kK=1 be a set of trajectories by running policy in the environment. Note that we drop time $t$ in the following notations for simplicity. ", "page_idx": 19}, {"type": "text", "text": "The value function is fitted by regression on mean-squared error: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta_{v},\\theta_{a},\\theta_{e})=\\frac{1}{K T}\\sum_{\\tau\\in\\mathcal{D}}\\sum_{t=0}^{T-1}\\left\\|V(\\mathrm{AM}_{\\mathrm{a}}(e(o),a^{u p p e r}))-\\hat{R}\\right\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\hat{R}$ is the discount rewards-to-go. ", "page_idx": 19}, {"type": "text", "text": "We update the policy by maximizing the PPO-Clip objective: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta_{\\pi},\\theta_{a},\\theta_{e})=\\frac{1}{K T}\\sum_{\\tau\\in\\mathcal{D}}\\sum_{t=0}^{T-1}\\operatorname*{min}(\\frac{\\pi(a|\\mathrm{AM_{a}}(e(o),a^{u p p e r}))}{\\pi_{o l d}(a|\\mathrm{AM_{a}}(e(o),a^{u p p e r}))}A_{\\pi_{o l d}},g(\\epsilon,A_{\\pi_{o l d}}))\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $g(\\epsilon,A)=\\left\\{\\stackrel{(1+\\epsilon)A}{(1-\\epsilon)A}\\;\\;\\;A\\geq0\\right.$ (1 \u2212\u03f5)A A \u22640, and A\u03c0old(o, aupper, a) is computed using the GAE method. ", "page_idx": 19}, {"type": "text", "text": "The world model $\\mathcal{M}$ is parameterized by $\\theta_{w}$ is trained as a regression model using the training data set $\\boldsymbol{S}$ . It is updated with the loss: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta_{w})=\\frac{1}{|S|}\\sum_{o,a,o^{\\prime},r\\in S}\\Big\\|(o^{\\prime},r)-\\mathcal{M}(\\mathrm{AM}_{\\mathrm{w}}(e(o),a))\\Big\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We trained our model on one GeForce GTX 1050 Ti and Intel(R) Core(TM) i9-9900K CPU $@$ 3.60GHz. ", "page_idx": 19}, {"type": "text", "text": "D.5 Addtional Ablation Studies ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We conduct a comparison of SeqComm against MAIC and CommFormer across six different maps: Protoss, Terran, and Zerg in 5v5 scenarios (first row) and Protoss, Terran, and Zerg in 10v10 scenarios (second row). The evaluation uses the official codebase for each method, with three random seeds per map under a full communication setting. The results refer to Figure 8. ", "page_idx": 19}, {"type": "text", "text": "D.6 Emergence of Behavioral Patterns ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We have visualized several key frames in Figure 9 to highlight the observed behavioral patterns. In the combat game, concentrating attacks on a single enemy is consistently more effective than dispersing them. In frames 1-3, the agents lack specific targets until one agent, located at the end of the orange arrow, approaches an enemy in the bottom right corner. By frame 4, following the negotiation phase, this agent is designated as the highest-level agent (level 5), given its advantageous position to choose an enemy to attack. Once lower-level agents receive the actions from higher-level agents (represented by the white dashed line), all the red units cease random roaming and instead coordinate a unified attack on the blue units. A similar pattern can be observed in frames 7-9. ", "page_idx": 19}, {"type": "image", "img_path": "3l2HnZXNou/tmp/cdf2cf28b45e493be7098d996879ce9d4a70e4caca04d084ac995510de934db7.jpg", "img_caption": ["Figure 8: Results of experiments with extra baseline algorithms. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "3l2HnZXNou/tmp/b56886a88d30deba6eb34ac3e5fae8b7583bbc26f807c0e32fe4ae4364a9016e.jpg", "img_caption": ["Figure 9: Illustration of the Emergence of Behavioral Patterns "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Claims ", "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: In the Abstract, we outlined our contributions and reiterated our scope in the introduction. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We discuss our limitations in the Conclusions. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The Method or Appendix includes all assumptions and proofs. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. ", "page_idx": 21}, {"type": "text", "text": "\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We fully disclose all the information needed to reproduce the main experimental results. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [No] ", "page_idx": 22}, {"type": "text", "text": "Justification: We plan to release data and code ASAP/upon acceptance. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. ", "page_idx": 22}, {"type": "text", "text": "\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We specify all the training and test details in the Experiments and Appendix. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We report error bars that are suitably and correctly defined, along with other appropriate information about the statistical significance of the experiments. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). ", "page_idx": 23}, {"type": "text", "text": "\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 24}, {"type": "text", "text": "Justification: We provide sufficient information on the computer resources in the Appendix. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: All the research conducted conforms with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: SeqComm is a MARL method that does not have potential societal impacts. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. ", "page_idx": 24}, {"type": "text", "text": "\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We properly credit all the assets. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not release new assets. ", "page_idx": 25}, {"type": "text", "text": "\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]