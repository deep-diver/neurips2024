[{"figure_path": "Tck41RANGK/figures/figures_3_1.jpg", "caption": "Figure 1: Optimization trajectories of Adam, TopK-Adam and TopK-Adam with EF applied on the Rosenbrock function f(x, y) = (1 - x)\u00b2 + 100(y \u2212 x\u00b2)\u00b2 starting from (x0, y0) = (\u22121, 1). Notice the extremely \"jagged\" profile of TopK-Adam without EF, and the recovered convergence when EF is added.", "description": "This figure compares the optimization trajectories of three different Adam optimizer variants on the Rosenbrock function.  The standard Adam optimizer shows smooth convergence. TopK-Adam, which uses a sparse gradient, exhibits a jagged and less effective convergence path.  Finally, TopK-Adam with error feedback (EF) demonstrates convergence that is nearly identical to the standard Adam, highlighting the effectiveness of the EF mechanism in mitigating the adverse effects of gradient sparsity.", "section": "3 The MICROADAM Algorithm"}, {"figure_path": "Tck41RANGK/figures/figures_3_2.jpg", "caption": "Figure 1: Optimization trajectories of Adam, TopK-Adam and TopK-Adam with EF applied on the Rosenbrock function f(x, y) = (1 - x)\u00b2 + 100(y \u2212 x\u00b2)\u00b2 starting from (x0, y0) = (\u22121, 1). Notice the extremely \"jagged\" profile of TopK-Adam without EF, and the recovered convergence when EF is added.", "description": "This figure compares the optimization trajectories of three different Adam optimizers on the Rosenbrock function: the standard Adam, Adam with TopK compression (TopK-Adam), and Adam with TopK compression and error feedback (TopK-Adam with EF).  The Rosenbrock function is a well-known test function in optimization known for its non-convexity and challenging landscape. The figure visually demonstrates that TopK compression significantly degrades the optimization trajectory, leading to a highly oscillatory and inefficient path toward the minimum. However, by incorporating error feedback, the TopK-Adam with EF optimizer recovers a smooth and efficient trajectory comparable to the standard Adam. This illustrates the effectiveness of error feedback in mitigating the detrimental effect of TopK gradient compression.", "section": "3 The MICROADAM Algorithm"}, {"figure_path": "Tck41RANGK/figures/figures_14_1.jpg", "caption": "Figure 2: Training curves for BERT-Base on GLUE/MNLI", "description": "This figure compares the training loss curves of six different optimizers (MicroAdam, AdamW-8bit, GaLore, AdamW, CAME) during the fine-tuning of a BERT-Base model on the GLUE/MNLI dataset. The x-axis represents the training steps, and the y-axis shows the training loss.  The plot visually demonstrates the convergence speed and stability of each optimizer.", "section": "C Training Graphs"}, {"figure_path": "Tck41RANGK/figures/figures_15_1.jpg", "caption": "Figure 3: Training curves for BERT-Large on GLUE/MNLI", "description": "This figure shows the training loss curves for different optimizers (MicroAdam, AdamW-8bit, GaLore, AdamW, and CAME) during the fine-tuning of the BERT-Large model on the GLUE/MNLI dataset.  It visually compares the convergence speed and stability of each optimizer. The x-axis represents the training step, and the y-axis represents the training loss.", "section": "C Training Graphs"}, {"figure_path": "Tck41RANGK/figures/figures_15_2.jpg", "caption": "Figure 1: Optimization trajectories of Adam, TopK-Adam and TopK-Adam with EF applied on the Rosenbrock function f(x, y) = (1 - x)\u00b2 + 100(y \u2212 x\u00b2)\u00b2 starting from (x\u2080, y\u2080) = (\u22121, 1). Notice the extremely \"jagged\" profile of TopK-Adam without EF, and the recovered convergence when EF is added.", "description": "This figure compares the optimization trajectories of three different Adam variants on the Rosenbrock function, a well-known non-convex function.  The variants are:\n\n1.  **Standard Adam:** The original Adam optimizer.\n2.  **TopK-Adam:** Adam with a TopK sparsification applied to the gradients (only the top k largest gradient components are used).\n3.  **TopK-Adam with EF:** TopK-Adam augmented with an error feedback (EF) mechanism to correct for the information lost during the TopK compression.\n\nThe plot visually demonstrates that TopK-Adam without EF produces a poor, jagged convergence trajectory.  However, adding EF to the TopK-Adam method allows for a smoother, near-perfect recovery of the original Adam's convergence behavior.", "section": "3 The MICROADAM Algorithm"}, {"figure_path": "Tck41RANGK/figures/figures_16_1.jpg", "caption": "Figure 5: Training curves for Llama-2 7B on GSM-8k", "description": "This figure shows the training loss curves for three different optimizers: MicroAdam, AdamW-8bit, and AdamW, when training the Llama-2 7B language model on the GSM-8k dataset.  The x-axis represents the training step, and the y-axis represents the training loss.  The plot allows for a visual comparison of the training performance and convergence speed of each optimizer. MicroAdam shows comparable performance with AdamW, and outperforms AdamW-8bit.", "section": "C Training Graphs"}, {"figure_path": "Tck41RANGK/figures/figures_16_2.jpg", "caption": "Figure 1: Optimization trajectories of Adam, TopK-Adam and TopK-Adam with EF applied on the Rosenbrock function f(x, y) = (1 - x)\u00b2 + 100(y \u2212 x\u00b2)\u00b2 starting from (x0, y0) = (\u22121, 1). Notice the extremely \"jagged\" profile of TopK-Adam without EF, and the recovered convergence when EF is added.", "description": "This figure shows the optimization trajectories for three different optimization methods applied to the Rosenbrock function.  The Rosenbrock function is a well-known test function for optimization algorithms known for its difficulty due to its non-convexity. The three methods are: (1) Adam, the original adaptive optimization algorithm; (2) TopK-Adam, a version of Adam that compresses the gradient information by considering only the top K largest elements; and (3) TopK-Adam with Error Feedback (EF), which adds a correction term to the TopK-Adam algorithm to account for the loss of information due to the compression. The figure demonstrates that while compressing the gradient information using TopK reduces the memory footprint, it can significantly hamper convergence.  However, the addition of EF successfully recovers the convergence of the original Adam optimizer, proving its ability to correct the errors introduced by the compression. ", "section": "3 The MICROADAM Algorithm"}, {"figure_path": "Tck41RANGK/figures/figures_16_3.jpg", "caption": "Figure 1: Optimization trajectories of Adam, TopK-Adam and TopK-Adam with EF applied on the Rosenbrock function f(x, y) = (1 - x)\u00b2 + 100(y \u2212 x\u00b2)\u00b2 starting from (x0, y0) = (\u22121, 1). Notice the extremely \"jagged\" profile of TopK-Adam without EF, and the recovered convergence when EF is added.", "description": "This figure compares the optimization trajectories of three different Adam optimizer variants on the Rosenbrock function. The first one is the original Adam optimizer, while the second one is an Adam optimizer with TopK sparsification that only considers the largest coordinate of the gradient. The last one adds error feedback to the TopK Adam optimizer. The figure shows that the TopK Adam alone fails to converge and has a jagged trajectory due to its sparse gradient.  However, adding error feedback results in a recovered convergence trajectory that is similar to the original Adam optimizer.", "section": "3 The MICROADAM Algorithm"}, {"figure_path": "Tck41RANGK/figures/figures_35_1.jpg", "caption": "Figure 8: Dynamics of the norm of the error compared to norm of the gradient (of output of the 3rd attention layer) during fine-tuning of ROBERTa-base model on GLUE/MNLI from surrogate GaLore with error feedback optimizer. We used hyperparameters from [Zhao et al., 2024], i.e. batch size 16, learning rate 0.00001, projection update gap 200, rank 4 and GaLore scale 4.", "description": "This figure shows the dynamics of the error norm compared to the gradient norm during fine-tuning of a ROBERTa-base model on the GLUE/MNLI dataset using a surrogate GaLore optimizer with error feedback.  It highlights the linear growth of error between subspace updates (indicated by grey shaded regions) and how the error norm significantly exceeds the gradient norm.  The hyperparameters used are detailed in the caption, aligning with those from the Zhao et al. (2024) paper.", "section": "F Error Feedback applied to GaLore"}, {"figure_path": "Tck41RANGK/figures/figures_35_2.jpg", "caption": "Figure 1: Optimization trajectories of Adam, TopK-Adam and TopK-Adam with EF applied on the Rosenbrock function f(x, y) = (1 - x)\u00b2 + 100(y - x\u00b2)\u00b2 starting from (x0, yo) = (-1, 1). Notice the extremely \"jagged\" profile of TopK-Adam without EF, and the recovered convergence when EF is added.", "description": "This figure shows the optimization trajectories of three different optimizers on the Rosenbrock function: Adam, TopK-Adam (Adam with Top-K compression), and TopK-Adam with EF (error feedback).  The Rosenbrock function is a well-known non-convex optimization problem. The figure demonstrates that TopK compression alone leads to a less efficient optimization trajectory (jagged), but adding error feedback significantly improves performance and recovers convergence similar to the original Adam optimizer. This illustrates the importance of the error feedback mechanism in maintaining optimization efficiency in MICROADAM while using compressed gradients.", "section": "3 The MICROADAM Algorithm"}]