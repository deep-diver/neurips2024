[{"type": "text", "text": "A Separation in Heavy-Tailed Sampling: Gaussian vs. Stable Oracles for Proximal Samplers ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ye He Georgia Institute of Technology yhe367@gatech.edu ", "page_idx": 0}, {"type": "text", "text": "Alireza Mousavi-Hosseini University of Toronto, and Vector Institute mousavi@cs.toronto.edu ", "page_idx": 0}, {"type": "text", "text": "Krishnakumar Balasubramanian University of California, Davis kbala@ucdavis.edu ", "page_idx": 0}, {"type": "text", "text": "Murat A. Erdogdu University of Toronto, and Vector Institute erdogdu@cs.toronto.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study the complexity of heavy-tailed sampling and present a separation result in terms of obtaining high-accuracy versus low-accuracy guarantees i.e., samplers that require only ${\\bar{O(}}\\mathrm{log}(1/\\varepsilon))$ versus $\\Omega({\\mathrm{poly}}(1/\\varepsilon))$ iterations to output a sample which is $\\varepsilon,$ -close to the target in $\\chi^{2}$ -divergence. Our results are presented for proximal samplers that are based on Gaussian versus stable oracles. We show that proximal samplers based on the Gaussian oracle have a fundamental barrier in that they necessarily achieve only low-accuracy guarantees when sampling from a class of heavy-tailed targets. In contrast, proximal samplers based on the stable oracle exhibit high-accuracy guarantees, thereby overcoming the aforementioned limitation. We also prove lower bounds for samplers under the stable oracle and show that our upper bounds cannot be fundamentally improved. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The task of sampling from heavy-tailed targets arises in various domains such as Bayesian statistics [GJPS08, GLM18], machine learning [CDV09, BZ17, NS\u00b8R19, S\u00b8ZTG20, DKTZ20], robust statistics [KN04, JR07, Kam18, Y\u0141R22], multiple comparison procedures [GBH04, GB09], and study of geophysical systems [SP15, QM16, PBEM23]. This problem is particularly challenging when using gradient-based Markov Chain Monte Carlo (MCMC) algorithms due to diminishing gradients, which occurs when the tails of the target density decay at a slow (e.g. polynomial) rate. Indeed, canonical algorithms like Langevin Monte Carlo (LMC) have been empirically observed to perform poorly [LWME19, HMW21, HFBE24] when sampling from such heavy-tailed targets. ", "page_idx": 0}, {"type": "text", "text": "Several approaches have been proposed in the literature to overcome these limitations of LMC and related algorithms. The predominant ones include (i) transformation-based approaches, where a diffeomorphic (invertible) transformation is used to first map the heavy-tailed density to a light-tailed one so that a light-tailed sampling algorithm can be used [JG12, Y\u0141R22, HBE24], (ii) discretizing general It\u00f4 diffusions with non-standard Brownian motion that have heavy-tailed densities as their equilibrium density [EMS18, LWME19, HFBE24], and (iii) discretizing stable-driven stochastic differential equations [ZZ23]. However, the few theoretical results available on the analysis of algorithms based on approaches (i) and (ii) provide only low-accuracy heavy-tailed samplers; such algorithms require poly $(1/\\varepsilon)$ iterations to obtain a sample that is $\\varepsilon$ -close to the target in a reasonable metric of choice. Furthermore, quantitative complexity guarantees for the sampling approach used in (iii) are not yet available; thus, existing comparisons are mainly based on empirical studies. ", "page_idx": 0}, {"type": "text", "text": "In stark contrast, when the target density is light-tailed it is well-known that algorithms like proximal samplers based on Gaussian oracles and the Metropolis Adjusted Langevin Algorithm (MALA) have high-accuracy guarantees; these algorithms require only polylog $(1/\\bar{\\varepsilon})$ iterations to obtain a sample which is $\\varepsilon$ -close to the target in some metric. See, for example, the works by [DCWY19, LST21b, ", "page_idx": 0}, {"type": "table", "img_path": "zuwLGhgxtQ/tmp/f611a95aa74da57c1f31371a88015cca27ea69fde1939c161a14cc3e5b4546bf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "Table 1: Separation for Proximal Samplers: Gaussian vs. practical Stable oracles $(\\alpha\\!=\\!1)$ ): Upper and lower iteration complexity bounds to generate an $\\varepsilon$ -accurate sample in $\\chi^{2}$ -divergence from the generalized Cauchy target densities with degrees of freedom $\\nu$ , i.e. $\\pi_{\\nu}\\propto(1+|x|^{2})^{-(d+\\nu)/2}$ . Here, $\\tilde{\\Omega}$ , $\\tilde{\\mathcal{O}}$ hide constants depending on $\\nu$ and polylog $(d,1/\\varepsilon)$ . For the proximal sampler with a general $\\alpha$ -Stable oracle (Algorithm 2), the upper bound for $\\nu\\in(0,1)$ is ${\\mathcal{O}}(\\log(1/\\varepsilon{\\hat{)}})$ when $\\alpha=\\nu$ . The lower bounds are from Corollary 2 via $2\\mathrm{T}\\dot{\\mathrm{V}}^{2}\\le\\chi^{2}$ . ", "page_idx": 1}, {"type": "text", "text": "WSC22a, CCSW22, CG23]. Specifically, [LST21b] analyzed the proximal sampling algorithm to sample from a class of strongly log-concave densities and obtained high-accuracy guarantees. [CCSW22] established similar high-accuracy guarantees for the proximal sampler to sample from target densities that satisfy a certain functional inequality, covering a range of light-tailed densities with exponentially fast tail decay (e.g. log-Sobolev and Poincar\u00e9 inequalities). However, it is not clear if the proximal sampler achieves the same desirable performance when the target is not light-tailed. ", "page_idx": 1}, {"type": "text", "text": "In light of existing results, in this work, we first consider the following question: ", "page_idx": 1}, {"type": "text", "text": "Q1. What are the fundamental limits of proximal samplers under the Gaussian oracle when sampling from heavy-tailed targets? ", "page_idx": 1}, {"type": "text", "text": "To answer this question, we construct lower bounds showing that Gaussian-based samplers necessarily require $\\mathrm{poly}(1/\\varepsilon)$ iterations to sample from a class of heavy-tailed targets. These results complement the lower bounds on the complexity of sampling from heavy-tailed densities using the LMC algorithm established in $[\\mathrm{MHFH^{+}}23]$ . With this lower bound in hand, we next consider the following question: ", "page_idx": 1}, {"type": "text", "text": "Q2. Is it possible to design high-accuracy samplers for heavy-tailed targets? ", "page_idx": 1}, {"type": "text", "text": "We answer this in the affirmative by constructing proximal samplers that are based on stable oracles (see Definition 1 and Algorithm 2) by leveraging the fractional heat-flow corresponding to a class of stable-driven SDEs. We analyze the complexity of this algorithm when sampling from heavy-tailed densities that satisfy a fractional Poincar\u00e9 inequality, and establish that they require only $\\log(1/\\varepsilon)$ iterations. Together, our answers to Q1 and Q2 provide a clear separation between samplers based on Gaussian and stable oracles. Our contributions can be summarized as follows. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Lower bounds for the Gaussian oracle: In Section 2, we focus on Q1 and establish in Theorems 1 and 2 respectively that the Langevin diffusion and the proximal sampler based on the Gaussian oracle necessarily have a fundamental barrier when sampling from heavy-tailed densities. Our proof technique builds on [Hai10], and provides a novel perspective for obtaining algorithmdependent lower bounds for sampling, which may be of independent interest. ", "page_idx": 1}, {"type": "text", "text": "\u2022 A proximal sampler based on the stable oracle: In Section 3, we introduce a proximal sampler based on the $\\alpha$ -stable oracle, which fundamentally relies on the exact implementations of the fractional heat flow that correspond to a stable-driven SDE. Here, the parameter $\\alpha$ determines the allowed class of heavy-tailed targets which could be sampled with high-accuracy. In Theorem 3 and Proposition 1, we provide upper bounds on the iteration complexity that are of smaller order than the corresponding lower bounds established for the Gaussian oracle. We provide a rejectionsampling based implementation of the $\\alpha$ -stable oracle for the case $\\alpha=1$ and prove complexity upper bounds in Corollary 3. Finally, in Theorem 4, considering a sub-class of Cauchy-type targets, we prove lower bounds showing that our upper bounds cannot be fundamentally improved. ", "page_idx": 1}, {"type": "text", "text": "An illustration of our results for Cauchy target densities, $\\pi_{\\nu}\\propto(1+|x|^{2})^{-(d+\\nu)/2}$ where $\\nu$ is the degrees of freedom, is provided in Table 1. We specifically consider the practical version of the stable proximal sampler with $\\alpha=1$ (i.e., Algorithm 2 with the stable oracle implemented by Algorithm 3), and show that it always outperforms the Gaussian proximal sampler (Algorithm 1). Indeed, when $\\nu\\geq1$ , the separation between these algorithms is obvious. In the case $\\nu\\in(0,1)$ , Algorithm 2 & 3 has a poly $(1/\\varepsilon)$ complexity, nevertheless, it still improves the complexity of the Gaussian proximal sampler by a factor of $\\varepsilon$ . We also show via lower bounds (in Section 3.4) that the poly $(1/\\varepsilon)$ complexity for Algorithm 2 & 3, when $\\nu\\in(0,1)$ , can only be improved up to certain factors. We remark that for the ideal proximal sampler (Algorithm 2), the upper bound when $\\nu\\in(0,1)$ is also ${\\mathcal{O}}(\\log(1/\\varepsilon))$ . These results demonstrate a clear separation between Gaussian and stable proximal samplers. ", "page_idx": 1}, {"type": "text", "text": "Related works. We first discuss works analyzing the complexity of heavy-tailed sampling as characterized by a functional inequality assumption. [CDV09] analyzed the connection between sampling algorithms for a class of $s$ -concave densities satisfying a certain isoperimetry condition related to weighted Poincar\u00e9 inequalities. [HFBE24] undertook a mean-square analysis of discretization of a specific It\u00f4 diffusion that characterizes a class of heavy-tailed densities satisfying a weighted Poincar\u00e9 inequality. [ALPW22] and [ALPW23] analyzed the complexity of pseudo-marginal MCMC algorithms and the random-walk Metropolis algorithm respectively, under weak Poincar\u00e9 inequalities. As mentioned before, $[\\mathrm{MHFH^{+}23}]$ showed lower bounds for the LMC algorithm when the target density satisfies a weak Poincar\u00e9 inequality. [HBE24] and [Y\u0141R22] analyzed a transformation based approach for heavy-tailed sampling under conditions closely related to the same functional inequality. This transformation methodology is also used to demonstrate asymptotic exponential ergodicity for other sampling algorithms like the bouncy particle sampler and the zig-zag sampler, in the heavytailed settings [DBCD19, DGM20, BRZ19]. These works provide only low-accuracy guarantees for heavy-tailed sampling and do not consider the use of weak Fractional Poincar\u00e9 inequalities. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Recent years have witnessed a significant focus on (strongly) log-concave sampling, leading to an extensive body of work that is challenging to encapsulate succinctly. In the context of (strongly) logconcave or light-tailed distributions, a plethora of non-asymptotic investigations have been conducted on LMC variations, including advanced integrators [SL19, LWME19, HBE20], underdamped LMC [CCBJ18, EGZ19, CLW23, DRD20], and MALA [DCWY19, LST20, $\\mathrm{CLA}^{+}21$ , WSC22b]. Outside the realm of log-concavity, the dissipativity assumption, which regulates the growth of the potential, has been used in numerous studies to derive convergence guarantees [DM17, RRT17, EMS18, EH21, MFWB22, EHZ22, $\\mathbf{BCE}^{+}22\\$ ]. ", "page_idx": 2}, {"type": "text", "text": "While research on upper bounds of sampling algorithms\u2019 complexity has advanced considerably, the exploration of lower bounds is still nascent. $[\\bar{\\mathrm{CGL}}^{+}22]$ explored the query complexity of sampling from strongly log-concave distributions in one-dimensional settings. [LZT22] established lower bounds for LMC in sampling from strongly log-concave distributions. [CBL22] presented lower bounds for sampling from strongly log-concave distributions with noisy gradients. [GLL20] focused on lower bounds for estimating normalizing constants of log-concave densities. Contributions by [LST21a] and [WSC22b] provide lower bounds in the metropolized algorithm category, including Langevin and Hamiltonian Monte Carlo, in strongly log-concave contexts. Finally, [CGLL22] contributed to lower bounds in Fisher information for non-log-concave sampling. ", "page_idx": 2}, {"type": "text", "text": "2 Lower Bounds for Sampling with the Gaussian Oracle ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we focus on Q1 for both the Langevin diffusion (in continuous time) and the proximal sampler (in discrete time), where both procedures have the target density as their invariant measures. Our results below illustrate the limitation of the Gaussian oracle1 for heavy-tailed sampling in both continuous and discrete time, showing that the phenomenon is not because of the discretization effect, but is inherently related to the use of Gaussian oracles. ", "page_idx": 2}, {"type": "text", "text": "Langevin diffusion. We first start with the overdamped Langevin diffusion (LD): ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{d}X_{t}=-\\nabla V(X_{t})\\mathrm{d}t+{\\sqrt{2}}\\mathrm{d}B_{t}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "LD achieves high-accuracy \u201csampling\u201d in continuous time, i.e. a $\\mathrm{polylog}(1/\\varepsilon)$ convergence rate in the light-tailed setting. We make the following dissipativity-type assumption. ", "page_idx": 2}, {"type": "text", "text": "Assumption 1. The target density is given by $\\pi^{X}(x)\\propto\\exp(-V(x))$ , where $V:\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ satisfies ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\forall x\\in\\mathbb{R}^{d},\\quad\\frac{(d+\\nu_{1})|x|^{2}}{1+|x|^{2}}\\leq\\langle x,\\nabla V(x)\\rangle\\leq\\frac{(d+\\nu_{2})|x|^{2}}{1+|x|^{2}}\\ \\ \\,f o r\\,s o m e\\,\\,\\nu_{2}\\geq\\nu_{1}\\geq0\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Remark 1. The upper bound on $\\langle x,\\nabla V(x)\\rangle$ ensures that $V$ grows at most logarithmically in $|x|$ . Consequently, $\\pi^{X}$ is heavy-tailed and in fact does not satisfy a Poincar\u00e9 inequality. The lower bound on $\\langle x,\\nabla V(x)\\rangle$ is only needed for deriving the dimension dependency in our guarantees. If one is only interested in the $\\varepsilon$ dependency, this condition can be replaced with $0\\leq\\langle\\bar{x},\\nabla V(x)\\rangle$ . ", "page_idx": 2}, {"type": "text", "text": "A classical example of a density satisfying the above assumption is the generalized Cauchy density with degrees of freedom $\\nu=\\nu_{1}=\\nu_{2}>0$ , where the potential is given by ", "page_idx": 2}, {"type": "equation", "text": "$$\nV_{\\nu}(x):=\\frac{d+\\nu}{2}\\ln(1+|x|^{2}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The following result, proved in Appendix A, provides a lower bound on the performance of LD. ", "page_idx": 2}, {"type": "table", "img_path": "zuwLGhgxtQ/tmp/3c6b722c0d561cb4a5ee71ea349c94de2a217cf4f97435e90c927c892a2ff2bb.jpg", "table_caption": ["Algorithm 1 Gaussian Proximal Sampler [LST21b] "], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "Theorem 1. Suppose $\\pi^{X}\\propto\\exp(-V)$ satisfies Assumption $^{\\,l}$ . Let $X_{t}$ be the solution of the Langevin diffusion, and $\\mu_{t}:=\\operatorname{Law}(X_{t})$ . Then, for any $\\delta>0$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{TV}(\\pi^{X},\\mu_{t})\\ge C_{\\nu_{1},\\nu_{2}}d^{\\frac{\\nu_{1}-\\nu_{2}}{2}(1+\\delta)}\\left(C_{\\delta}(\\mu_{0})+\\kappa_{\\delta}t\\right)^{-\\frac{\\nu_{2}(1+\\delta)}{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\begin{array}{r}{\\kappa_{\\delta}:=1\\vee\\frac{2}{d+\\nu_{2}}\\vee\\frac{\\nu_{2}(1+\\delta)}{(d+\\nu_{2})\\delta}}\\end{array}$ , $\\begin{array}{r}{C_{\\delta}(\\mu_{0}):=\\frac{1}{d+\\nu_{2}}\\mathbb{E}[(1+|X_{0}|^{2})^{\\gamma}]^{1/\\gamma}}\\end{array}$ with $\\gamma=\\kappa_{\\delta}(d+\\nu_{2})/2,$ , and $C_{\\nu_{1},\\nu_{2}}$ is a constant depending only on $\\nu_{1}$ and $\\nu_{2}$ . ", "page_idx": 3}, {"type": "text", "text": "If we assume $|X_{0}|\\leq\\mathcal{O}(\\sqrt{d})$ for simplicity, then by choosing $\\begin{array}{r}{\\delta=\\frac{2\\ln\\ln t}{\\nu_{2}\\ln t}\\wedge\\frac{2\\ln\\ln d}{(\\nu_{2}-\\nu_{1})\\ln d}}\\end{array}$ \u2227(\u03bd2\u2212\u03bd1) ln d, we obtain ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{TV}(\\pi^{X},\\mu_{t})\\geq\\tilde{\\Omega}_{\\nu_{1},\\nu_{2}}(d^{\\frac{\\nu_{1}-\\nu_{2}}{2}}t^{-\\frac{\\nu_{2}}{2}}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Thus, LD requires at least $T=\\tilde{\\Omega}_{\\nu_{1},\\nu_{2}}\\big(d^{\\frac{\\nu_{1}-\\nu_{2}}{\\nu_{2}}}(1/\\varepsilon)^{2/\\nu_{2}}\\big)$ to reach $\\varepsilon$ error in total variation. While this bound may be small in high dimensions when $\\nu_{2}>\\nu_{1}$ , for the canonical model of Cauchy-type potentials with $\\nu_{2}=\\nu_{1}=\\nu$ , it will be independent of dimension, as stated by the following result. Note that Assumption 1 can also cover a general scaling by replacing $|x|$ with $c|x|$ for some constant $c$ , which would introduce a multiplicative factor of $1/\\bar{c}^{2}$ for the lower bound on $T$ . This is expected as e.g., mixing to the Gibbs potential $c^{2}|x|^{2}$ can be faster than mixing to $|x|^{2}$ by a factor of $\\mathrm{1}\\dot{/c}^{2}$ . ", "page_idx": 3}, {"type": "text", "text": "Corollary 1. Consider the generalized Cauchy density $\\pi_{\\nu}^{X}\\,\\propto\\,\\exp(-V_{\\nu})$ where $V_{\\nu}$ is as in (1). Let $X_{t}$ be the solution of the Lan\u221agevin diffusion, and $\\mu_{t}:=\\operatorname{Law}(X_{t})$ . For simplicity, assume the initialization satisfies $|X_{0}|\\leq{\\mathcal{O}}({\\sqrt{d}})$ . Then, achieving $\\mathrm{TV}(\\pi_{\\nu}^{X},\\mu_{T})\\leq\\varepsilon$ requires $T\\geq\\tilde{\\Omega}_{\\nu}\\big(\\varepsilon^{-\\frac{2}{\\nu}}\\big)$ . ", "page_idx": 3}, {"type": "text", "text": "The above lower bound implies that LD is a low-accuracy \u201csampler\u201d for this target density in the sense that it depends polynomially on $1/\\varepsilon$ ; this dependence gets worse with smaller $\\nu$ as the tails get heavier. It is worth highlighting the gap between the upper bound of $[\\mathrm{MHFH}^{+}23$ , Corollary 8], which is $\\tilde{\\mathcal{O}}\\big(1/\\varepsilon^{4/\\nu}\\big)$ , and the lower bound in Corollary 1. ", "page_idx": 3}, {"type": "text", "text": "Gaussian proximal sampler. In the remainder of this section, we prove that the Gaussian proximal sampler, described in Algorithm 1, also suffers from a $\\mathrm{poly}(1/\\varepsilon)$ rate when the target density is heavy-tailed. In each iteration of Algorithm 1, the first step involves sampling a standard Gaussian random variable $y_{k}$ centered at the current iterate $x_{k}$ with variance $\\eta I$ ; this is a one-step isotropic Brownian random walk. Alternatively, since the Fokker-Planck equation of the standard Brownian motion is the classical heat equation, this step could also be interpreted as an exact simulation of the heat flow; see, for example, [CG03] and [Wib18]. Specifically, the density of $y_{k}$ is the solution to the heat flow at time $\\eta$ with the initial condition being the density of $x_{k}$ . The second step is called the restricted Gaussian oracle (RGO) as coined by [LST21b]; under which $(x_{k},y_{k})$ is a reversible Markov chain whose stationary density has $x$ -marginal $\\pi^{X}$ . ", "page_idx": 3}, {"type": "text", "text": "Assumption 2. For some $\\nu_{2}\\ge\\nu_{1}\\ge0$ , the target $\\pi^{X}(x)\\propto\\exp(-V(x))$ with $V:\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ satisfies ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\forall x\\in\\mathbb{R}^{d}\\ \\ \\frac{(d+\\nu_{1})|x|^{2}}{1+|x|^{2}}\\leq\\langle x,\\nabla V(x)\\rangle,\\quad|\\nabla V(x)|\\leq\\frac{(d+\\nu_{2})|x|}{1+|x|^{2}},\\quad\\Delta V(x)\\leq\\frac{(d+\\nu_{2})^{2}}{1+|x|^{2}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The first condition above also appears in Assumption 1 and the second condition implies the upper bound of Assumption 1; thus, the above assumption is stronger. Note that the generalized Cauchy measure (1) satisfies this assumption with $\\nu_{1}=\\nu_{2}=\\nu$ . Under Assumption 2, we state the following lower bound on the Gaussian proximal sampler and defer its proof to Appendix A. ", "page_idx": 3}, {"type": "text", "text": "Theorem 2. Suppose $\\pi^{X}\\propto\\exp(-V)$ satisfies Assumption 2. Let $x_{k}$ denote the $k^{t h}$ iterate of the Gaussian proximal sampler (Algorithm $^{\\,l}$ ) with step $\\eta$ and let $\\rho_{k}^{X}:=\\operatorname{Law}(x_{k})$ . Then, for any $\\delta>0$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{TV}(\\pi^{X},\\rho_{k}^{X})\\ge C_{\\nu_{1},\\nu_{2}}d^{\\frac{\\nu_{1}-\\nu_{2}}{2}(1+\\delta)}\\left(C_{\\delta}(\\mu_{0})+\\kappa_{\\delta}\\eta k\\right)^{-\\frac{\\nu_{2}\\left(1+\\delta\\right)}{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\kappa_{\\delta}$ , $C_{\\delta}(\\mu_{0})$ , and $C_{\\nu_{1},\\nu_{2}}$ are defined in Theorem $^{\\,l}$ . ", "page_idx": 3}, {"type": "text", "text": "Above, assuming $|X_{0}|\\leq\\mathcal{O}(\\sqrt{d})$ with the same choice of $\\delta$ as in Theorem 1 yields $\\mathrm{TV}(\\pi_{\\nu}^{X},\\rho_{k}^{X})\\geq$ $\\tilde{\\Omega}_{\\nu_{1},\\nu_{2}}\\big(d^{\\frac{\\nu_{1}-\\nu_{2}}{2}}(k\\eta)^{\\frac{-\\nu_{2}}{2}}\\big)$ . Note that in order for the RGO step to be efficiently implementable, we need to have a sufficiently small $\\eta$ . The state-of-the-art implementation of RGO requires a step size of order $\\eta=\\tilde{\\mathcal{O}}\\big(1/(L d^{1/2})\\big)$ when $V$ has $L$ -Lipschitz gradients [FYC23]. With this choice of step size, the above lower bound requires at least $N=\\tilde{\\Omega}_{\\nu_{1},\\nu_{2}}\\big(L d^{1/2+(\\nu_{1}-\\nu_{2})/\\nu_{2}}(1/\\varepsilon)^{2/\\nu_{2}}\\big)$ iterations. The assumptions in Theorem 2 once again cover the canonical examples of generalized Cauchy densities, where we have $L=d+\\nu$ , which simplifies the lower bound as follows. ", "page_idx": 4}, {"type": "text", "text": "Corollary 2. Consider the generalized Cauchy density $\\pi_{\\nu}^{X}\\,\\propto\\,\\exp(-V_{\\nu})$ where $V_{\\nu}$ is as in (1). Let $x_{k}$ denote the $k^{t h}$ iterate of the Gaussian proximal sampler, \u221aand define $\\rho_{k}^{X}:=\\operatorname{Law}(x_{k})$ , and choose the step size $\\eta=\\tilde{\\mathcal{O}}(1/(L d^{1/2}))$ . If we assume $|X_{0}|\\leq\\mathcal{O}(\\sqrt{d})$ for simplicity, then achieving $\\mathrm{TV}(\\pi_{\\nu}^{X},\\rho_{N}^{X})\\leq\\varepsilon$ requires $N\\geq\\tilde{\\Omega}_{\\nu}\\Big(d^{\\frac{3}{2}}\\varepsilon^{-\\frac{2}{\\nu}}\\Big)$ iterations. ", "page_idx": 4}, {"type": "text", "text": "We emphasize that the above lower bound is of order poly $(1/\\varepsilon)$ as advertised. Thus, the RGO-based proximal sampler can only yield a low-accuracy guarantee in this setting. ", "page_idx": 4}, {"type": "text", "text": "3 Stable Proximal Sampler and the Restricted $\\alpha$ -Stable Oracle ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Having characterized the limitations of Gaussian oracles for heavy-tailed sampling, thereby answering Q1, in what follows, we will focus on Q2 and construct proximal samplers based on the $\\alpha$ -stable oracle, and prove that they achieve high-accuracy guarantees when sampling from heavy-tailed targets. First, we provide a basic overview of $\\alpha$ -stable processes and fractional heat flows. ", "page_idx": 4}, {"type": "text", "text": "Isotropic $\\alpha$ -stable process. For $t\\geq0$ , let $X_{t}^{(\\alpha)}$ be the isotropic stable L\u00e9vy process in $\\mathbb{R}^{d}$ , starting from $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , with the index of stability $\\alpha\\in(0,2]$ , defined uniquely via its characteristic function $\\mathbb{E}_{x}e^{i\\langle\\xi,X_{t}^{(\\alpha)}-x\\rangle}=e^{-t|\\xi|^{\\alpha}}$ . When $\\alpha=2$ , $X_{t}^{(2)}$ is a scaled Brownian motion, and when $0<\\alpha<2$ , it becomes a pure L\u00e9vy jump process in $\\mathbb{R}^{d}$ . The transition density of $X_{t}^{(\\alpha)}$ is then given by ", "page_idx": 4}, {"type": "equation", "text": "$$\np^{(\\alpha)}(t;x,y)=p_{t}^{(\\alpha)}(y-x)\\quad\\mathrm{with}\\quad p_{t}^{(\\alpha)}(y)=(2\\pi)^{-d}\\int_{\\mathbb{R}^{d}}\\exp(-t|\\xi|^{\\alpha})e^{-i\\langle\\xi,y\\rangle}\\mathrm{d}\\xi,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the second equation above is the inverse Fourier transform of the characteristic function, thus returns the density. The transition kernel and the density in (2) have closed-form expressions for the special cases $\\alpha=1,2$ . In particular, when $\\alpha=1$ , $p_{t}^{(1)}$ reduces to a Cauchy density with degrees of freedom $\\nu=1$ , i.e. $p_{t}^{(1)}(y)\\propto(|y|^{2}+t^{2})^{-(d+1)/2}$ . We finally note that the isotropic stable L\u00e9vy hpraovce etshse $X_{t}^{(\\alpha)}$ ddiisstprliabyust isoenl.f -Tsihimsi lparroitpye rltiyk ei st chreu cBiraol iwnn tihaen  dmeovteiloonp; mtheen t porfo tchees ssetas $X_{a t}^{(\\alpha)}$ oaxnidm $a^{1/\\alpha}X_{t}^{(\\alpha)}$ ", "page_idx": 4}, {"type": "text", "text": "Fractional heat flow. The equation $\\partial_{t}u(t,x)\\,=\\,-(-\\Delta)^{\\alpha/2}u(t,x)$ with the condition $u(0,x)\\,=$ $u_{0}(x)$ is an extension of the classical heat flow, and is referred to as the fractional heat flow. Here, $-(-\\Delta)^{\\alpha/2}$ is the fractional Laplacian operator with $\\alpha\\in(0,2]$ , which is the infinitesimal generator of the isotropic $\\alpha$ -stable process. For $\\alpha=2$ , it reduces to the standard Laplacian operator $\\Delta$ . ", "page_idx": 4}, {"type": "text", "text": "Stable proximal sampler. Let $\\pi(x,y)$ be a joint density such that $\\pi(x,y)\\propto\\pi^{X}(x)p^{(\\alpha)}(\\eta;x,y)$ , where $\\pi^{X}$ is the target and $p^{(\\alpha)}(\\eta;x,y)$ is the transition density of the $\\alpha$ -stable process, introduced in (2). It is easy to verify that (i) the $X$ -marginal of $\\pi$ is $\\pi^{X}$ , (ii) the conditional density of $Y$ given $X$ is $\\pi_{\\dots}^{Y|X}(\\cdot|x)=p^{(\\alpha)}(\\eta;x,\\cdot)$ , (iii) the $Y$ -marginal is $\\pi^{Y}=\\pi^{X}*p_{\\eta}^{(\\alpha)}$ , i.e. $\\pi^{Y}$ is obtained by evolving $\\pi^{X}$ along the $\\alpha$ -fractional heat flow for time $\\eta$ , and (iv) the conditional density of $X$ given $Y$ is $\\pi^{X|Y}(\\cdot|y)\\propto\\pi^{X}(\\cdot)p^{(\\alpha)}(\\eta;\\cdot,y)$ . Based on these, we introduce the following stable oracle. ", "page_idx": 4}, {"type": "text", "text": "Definition 1 (Restricted $\\alpha$ -Stable Oracle). Given $\\boldsymbol{y}\\in\\mathbb{R}^{d}$ , an oracle that outputs a random vector distributed according to $\\pi^{X|Y}(\\cdot|y)$ , is called the Restricted $\\alpha$ -Stable Oracle $(R\\alpha S O)$ . ", "page_idx": 4}, {"type": "text", "text": "Note that when $\\alpha=2$ , the $\\mathbf{R}\\alpha\\mathbf{S}\\mathbf{O}$ reduces to the RGO of [LST21b]. The Stable Proximal Sampler (Algorithm 2) with parameter $\\alpha$ is initialized at a point $\\boldsymbol{x_{0}}\\in\\mathbb{R}^{d}$ and performs Gibbs sampling on the joint density $\\pi$ . In each iteration, the first step involves sampling an isotropic $\\alpha$ -stable random vector $y_{k}$ centered at the current iterate $x_{k}$ , which is a one-step isotropic $\\alpha$ -stable random walk. This could also be interpreted as an exact simulation of the fractional heat flow. Indeed, due to the relation between the fractional heat flow and the isotropic stable process, the density of $y_{k}$ is exactly the solution to the $\\alpha$ -fractional heat flow at time $\\eta$ with the initial condition being the density of $x_{k}$ . ", "page_idx": 4}, {"type": "table", "img_path": "zuwLGhgxtQ/tmp/fb0f3d861acf36e10cf27d1ba9a20aa8ac2ae13b63b589de83976fae5f2550dd.jpg", "table_caption": ["Algorithm 2 Stable Proximal Sampler with parameter $\\alpha$ "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "When $\\alpha=2$ , the first step reduces to an isotropic Brownian random walk and a simulation of the classical heat flow. The second step calls the $\\mathrm{R}\\alpha\\mathrm{S}\\mathrm{O}$ at the point $y_{k}$ . ", "page_idx": 5}, {"type": "text", "text": "3.1 Convergence guarantees ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We next provide convergence guarantees for the stable proximal sampler in $\\chi^{2}$ -divergence assuming access to the $\\mathbf{R}\\alpha\\mathbf{S}\\mathbf{O}$ . Similar results for a practical implementation are presented in Section 3.2. To proceed, we introduce the fractional Poincar\u00e9 inequality, first introduced in [WW15] to characterize a class of heavy-tailed densities including the canonical Cauchy class. ", "page_idx": 5}, {"type": "text", "text": "Definition 2 (Fractional Poincar\u00e9 Inequality). For $\\vartheta\\in\\mathsf{\\Gamma}(0,2)$ , a probability density $\\mu$ satisfies a $\\vartheta$ -fractional Poincar\u00e9 inequality $\\left(F P I\\right)$ if there exists a positive constant $C_{\\mathrm{FPI}(\\vartheta)}$ such that for any function $\\phi:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ in the domain of $\\mathcal{E}_{\\mu}^{(\\vartheta)}$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{V a r_{\\mu}(\\phi)\\leq C_{\\mathrm{FPI}(\\vartheta)}\\mathcal{E}_{\\mu}^{(\\vartheta)}(\\phi).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathcal{E}_{\\mu}^{(\\vartheta)}$ is a non-local Dirichlet form associated with $\\mu$ defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{E}_{\\mu}^{(\\vartheta)}(\\phi):=c_{d,\\vartheta}\\!\\int\\!\\!\\int_{\\{x\\neq y\\}}\\!\\frac{(\\phi(x)-\\phi(y))^{2}}{|x-y|^{(d+\\vartheta)}}\\mathrm{d}x\\mu(y)\\mathrm{d}y\\quad w i t h\\quad c_{d,\\vartheta}=\\frac{2^{\\vartheta}\\Gamma((d+\\vartheta)/2)}{\\pi^{d/2}|\\Gamma(-\\vartheta/2)|}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Remark 2. FPI is a weaker condition than Assumption 2. In fact, any density satisfying the first 2 conditions in Assumption 2 satisfies $\\vartheta$ -FPI for all $\\vartheta<\\nu_{1}$ [WW15, Theorem 1.1]. In Proposition 2, we show that as $\\vartheta\\rightarrow2^{-}$ , FPI becomes equivalent to the standard Poincar\u00e9 inequality. ", "page_idx": 5}, {"type": "text", "text": "In the sequel, $\\rho_{k}^{X}$ denotes the law of $x_{k}$ , $\\rho_{k}^{Y}$ denotes the law of $y_{k}$ , and $\\rho_{k}=\\rho_{k}^{X,Y}$ \u03c1kX,Y is the joint law of $(x_{k},y_{k})$ . We provide the following convergence guarantee under an FPI, proved in Appendix B.2. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3. Assume that $\\pi^{X}$ satisfies the $\\alpha$ -FPI with parameter $C_{\\mathrm{FPI}(\\alpha)}$ for $\\alpha\\in(0,2)$ . For any step size $\\eta>0$ and initial density $\\rho_{0}^{X}$ , the $k^{t h}$ iterate of Algorithm 2, with parameter $\\alpha$ , satisfies ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\chi^{2}(\\rho_{k}^{X}|\\pi^{X})\\leq\\exp\\left(-k\\eta\\left(C_{\\mathrm{FPI}(\\alpha)}+\\eta\\right)^{-1}\\right)\\chi^{2}(\\rho_{0}^{X}|\\pi^{X}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "As a consequence of Remark 2 and Proposition 2, we recover the result in [CCSW22, Theorem 4], by letting $\\alpha\\rightarrow2^{-}$ . While our results in Theorem 3 are based on Algorithm 2 which requires exact calls to $\\mathrm{R}\\alpha\\mathrm{SO}$ , the next result, proved in Appendix B.3, shows that even with an inexact implementation of $\\mathrm{R}\\alpha\\mathrm{S}\\mathrm{O}$ , the error accumulation is at most linear, and Algorithm 2 still converges quickly. ", "page_idx": 5}, {"type": "text", "text": "Proposition 1. Suppose the R\u03b1SO in Algorithm 2 is implemented inexactly, i.e. there exists a positive constant $\\varepsilon_{\\mathrm{TV}}$ such that $\\mathrm{TV}\\big(\\tilde{\\rho}_{k}^{X|Y}(\\cdot|y),\\rho_{k}^{X|Y}(\\cdot|y)\\big)\\leq\\varepsilon_{\\mathrm{TV}}$ for all $\\boldsymbol{y}\\in\\mathbb{R}^{d}$ and $k\\geq1$ , where $\\tilde{\\rho}_{k}^{\\bar{X}|Y}(\\cdot|y)$ is the density of the inexact R\u03b1SO sample conditioned on $y$ . Let $\\tilde{\\rho}_{k}^{X}$ be the density of the output of the $k^{t h}$ step of Algorithm 2 with the inexact $R\\alpha S O$ and $\\rho_{k}^{X}$ be the density of the output of $k^{t h}$ step Algorithm 2 with the exact R\u03b1SO. Then, for all $k\\geq0$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{TV}(\\tilde{\\rho}_{k}^{X},\\rho_{k}^{X})\\leq\\mathrm{TV}(\\tilde{\\rho}_{0}^{X},\\rho_{0}^{X})+k\\,\\varepsilon_{\\mathrm{TV}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Further, if $\\dot{\\pi}_{0}^{X}=\\rho_{0}^{X}$ , for any $K\\geq K_{0}$ , we get $\\operatorname{TV}(\\tilde{\\rho}_{X}^{K},\\pi^{X})\\leq\\varepsilon,$ , i $f_{\\ensuremath{\\varepsilon}_{\\mathrm{TV}}}\\le\\varepsilon/2K$ , where the constant $K_{0}=\\left(1+C_{\\mathrm{FPI}(\\alpha)}\\eta^{-1}\\right)\\log\\left(\\chi^{2}(\\tilde{\\rho}_{0}^{X}|\\pi^{X})/\\varepsilon^{2}\\right)$ with $C_{\\mathrm{FPI}(\\alpha)}$ being the $\\alpha$ -FPI parameter of $\\pi^{X}$ . ", "page_idx": 5}, {"type": "text", "text": "3.2 A practical implementation of $\\mathbf{R}\\alpha\\mathbf{S}\\mathbf{0}$ ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the sequel, we introduce a practical implementation of $\\mathrm{R}\\alpha\\mathrm{S}\\mathrm{O}$ when $\\alpha=1$ . For this, we consider the case when the target density $\\pi^{X}\\propto\\stackrel{\\cdot}{e}^{-V}$ satisfies the 1-FPI with parameter $C_{\\mathrm{FPI(1)}}$ . A more thorough implementation of $\\mathbf{R}\\alpha\\mathbf{S}\\mathbf{O}$ for other values of $\\alpha$ will be investigated in future work. ", "page_idx": 5}, {"type": "text", "text": "Assumption 3. There exist constants $\\beta,L>0$ such that for any minimizer $x^{*}\\in\\arg\\operatorname*{min}_{y\\in\\mathbb{R}^{d}}V(y)$ and for all $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , $V$ satisfies $V(x)-V(x^{*})\\leq L|x-x^{*}|^{\\beta}$ . ", "page_idx": 5}, {"type": "table", "img_path": "zuwLGhgxtQ/tmp/fb3d90c14e6418601003c17a7829b523ec198130e61c33cdf8c4d5035e036afc.jpg", "table_caption": ["Algorithm 3 R\u03b1SO Implementation for $\\alpha=1$ via Rejection Sampling "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Algorithm 3 provides an exact implementation of $\\mathrm{R}\\alpha\\mathrm{SO}$ for $\\alpha=1$ via rejection sampling. Inputs to this algorithm are the intermediate points $y_{k}$ in the stable proximal sampler (Algorithm 2). Note that Algorithm 3 requires a global minimizer of $V$ , which is always assumed to exist, which guarantees that the acceptance probability is non-trivial. It generates proposals with density $p^{(1)}(\\bar{\\eta};\\cdot,y)$ and utilizes that $\\ensuremath{p^{(1)}}$ is a Cauchy density and Cauchy random vectors can be generated via ratios between a Gaussian random vector and square-root of a $\\chi^{2}$ random variable. Finally, the accept-reject step ensures that the output $x$ has density $\\pi^{X|Y}(\\cdot|y)\\,\\propto\\,e^{-V}p^{(1)}(\\eta;\\cdot,y)$ . This makes Algorithm 3 a zeroth-order algorithm requiring only access to function evaluations of $V$ . Under Assumption 3, by choosing a small step-size, we can control the expected number of rejections in Algorithm 3. We now state the iteration complexity of our stable proximal sampler with this $\\mathbf{R}\\alpha\\mathbf{S}\\mathbf{O}$ implementation in the following result, whose proof is provided in Appendix B.3. ", "page_idx": 6}, {"type": "text", "text": "Corollary 3. Assume $V$ satisfies Assumption 3. If we choose the step-size $\\eta=\\Theta(d^{-\\frac{1}{2}}L^{-\\frac{1}{\\beta}})$ , then Algorithm $3$ implements the R\u03b1SO with $\\alpha=1$ , with the expected number of zeroth-order calls to $V$ of order $\\mathbb{E}[\\exp(\\bar{L}|y_{k}|^{\\beta})]$ . Further assume $\\pi^{X}$ satisfies 1-FPI with parameter $C_{\\mathrm{FPI(1)}}$ . Suppose we run Algorithm 2 with $R\\alpha S O$ implemented for with $\\alpha=1$ by Algorithm 3. Then, to return a sample which is $\\varepsilon$ -close in $\\chi^{2}$ -divergence to the target, the expected number of iterations required by Algorithm 2 is ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{O}\\big(C_{\\mathrm{FPI(1)}}d^{\\frac{1}{2}}L^{\\frac{1}{\\beta}}\\log(\\chi^{2}(\\rho_{0}^{X}|\\pi^{X})/\\varepsilon)\\big).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Note that the above result provides a high-accuracy guarantee for the implementable version of the stable proximal sampler (Algorithm 3) for a class of heavy-tailed targets, overcoming the fundamental barrier established in Theorem 2 for the Gaussian proximal sampler (i.e., Algorithm 1). A numerical illustration of this improvement is provided in Appendix D by sampling from student-t distributions. ", "page_idx": 6}, {"type": "text", "text": "Remark 3. $(I)$ Finding a global minimizer of the potential $V$ can be hard, which could be avoided if a lower bound on the potential $V$ is available; see Appendix B.3. (2) A trivial bound for $\\mathbb{E}[\\exp(L|y_{k}|^{\\beta})]$ is $\\exp(L M)$ for $M=\\mathbb{E}_{\\pi^{X}}[|X|^{\\beta}]+\\chi^{2}(\\rho_{0}^{X}|\\pi^{X})\\mathbb{E}_{\\pi^{X}}[|X|^{2\\beta}]^{\\frac{1}{2}}$ . Since our main focus is high vs low accuracy samplers, deriving a sharper bound is beyond the scope of the current paper. ", "page_idx": 6}, {"type": "text", "text": "3.3 Illustrative examples ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To illustrate our results, we now apply the proximal algorithms to sample from Cauchy densities and discuss the complexity of both the ideal sampler (Algorithm 2) in which we can choose any $\\alpha\\in(0,2)$ and the implementable version with $\\alpha\\,=\\,1$ (Algorithm 3). For the ideal sampler, we can choose $\\alpha\\leq\\nu$ for any degrees of freedom $\\nu>0$ , and apply Theorem 3 since $\\pi_{\\nu}$ satisfies a $\\alpha$ -FPI [WW15]. ", "page_idx": 6}, {"type": "text", "text": "Corollary 4. For any $\\nu>0$ , consider the generalized Cauchy target $\\pi_{\\nu}\\propto\\exp(-V_{\\nu})$ with $V_{\\nu}$ defined in (1). For the stable proximal sampler with parameter $\\alpha\\in(0,2)$ and $\\alpha\\leq\\nu$ (i.e., Algorithm 2), suppose we set the step-size $\\eta\\,\\in\\,(0,1)$ and draw the initial sample from the standard Gaussian density. Then, the number of iterations required by Algorithm 2 to produce an $\\varepsilon$ -accurate sample in $\\chi^{2}$ -divergence is ${\\mathcal O}(C_{\\mathrm{FPI}(\\alpha)}{\\dot{\\eta}}^{-1}\\log(d/\\varepsilon))$ , where $C_{\\mathrm{FPI}(\\alpha)}$ is the $\\alpha$ -FPI parameter of $\\pi_{\\nu}$ . ", "page_idx": 6}, {"type": "text", "text": "For the implementable sampler, since the parameter $\\alpha$ is fixed to be 1, whether a suitable FPI is satisfied or not depends on the degrees of freedom $\\nu$ . Specifically, when $\\nu\\geq1$ , 1-FPI is satisfied and Corollary 5 applies. When $\\nu\\in(0,1)$ , on the other hand, 1-FPI is not satisfied. To tackle this issue, we prove convergence guarantees for the proximal sampler under a weak fractional Poincar\u00e9 inequality; the next corollary, proved in Appendix B.4, summarizes these results. ", "page_idx": 6}, {"type": "text", "text": "Corollary 5. For the Cauchy target $\\pi_{\\nu}\\,\\propto\\,\\exp(-V_{\\nu})$ where $V_{\\nu}$ is defined in (1), we consider Algorithm 2 with $\\alpha=1$ , a standard Gaussian initialization, and R\u03b1SO implemented by Algorithm 3. ", "page_idx": 6}, {"type": "text", "text": "(1) When $\\nu\\geq1$ , if we set the step-size $\\eta=\\Theta\\big(d^{-\\frac{1}{2}}(d+\\nu)^{-4}\\big)$ , the expected number iterations required by Algorithm 2 to output a sample which is $\\varepsilon$ -close in $\\chi^{2}$ -divergence to the target is of order $\\mathcal{O}\\big(C_{\\mathrm{FPI(1)}}d^{\\frac{1}{2}}(d+\\nu)^{4}\\log(d/\\varepsilon)\\big)$ , where $C_{\\mathrm{FPI(1)}}$ is the 1-FPI parameter of $\\pi_{\\nu}$ . ", "page_idx": 6}, {"type": "text", "text": "(2) When $\\nu\\,\\in\\,(0,1)$ , if we set the step-size $\\eta\\,=\\,\\Theta\\bigl(d^{-\\frac{1}{2}}(d+\\nu)^{-\\frac{4}{\\nu}}\\bigr)$ , the expected number of iterations required by Algorithm 2, to output a sample which is $\\varepsilon$ -close in $\\chi^{2}$ -divergence to the target is of order $\\tilde{\\mathcal{O}}\\big(\\operatorname*{max}\\left\\{c^{\\frac{1}{\\nu}}d^{\\frac{1}{2\\nu}+\\frac{4}{\\nu^{2}}},c\\mathring{d}^{\\frac{1}{2}+\\frac{4}{\\nu}}\\varepsilon^{-\\frac{1}{\\nu}+1}\\right\\}\\big)$ , where $c$ is the positive constant given in (16). Here, $\\tilde{\\mathcal{O}}$ hides the polylog factors on d and $1/\\varepsilon$ . ", "page_idx": 7}, {"type": "text", "text": "The stable proximal sampler (Algorithm 2) is a high accuracy sampler for the class of generalized Cauchy targets, as long as $\\alpha\\,\\leq\\,\\nu$ , meaning that it achieves $\\log(1/\\varepsilon)$ iteration complexity. The improvement from poly $(1/\\varepsilon)$ to $\\log(1/\\varepsilon)$ separates the stable proximal sampler and the Gaussian proximal sampler in the task of heavy-tailed sampling. When we use the rejection-sampling implementation with parameter $\\alpha=1$ (Algorithm 3), iteration complexity goes through a phase transition as the tails get heavier. When the generalized Cauchy density has a finite mean $(\\nu>1)$ ), we achieve a high-accuracy sampler with $\\log(1/\\varepsilon)$ iteration complexity. However, without a finite mean (i.e., $\\nu\\in(0,1)$ ), the algorithm becomes a low-accuracy sampler with poly $(1/\\varepsilon)$ ) complexity. Even in this low-accuracy regime, the implementable stable proximal sampler outperforms the Gaussian one, as originally highlighted in Table 1. Last, we claim that the poly $(1/\\varepsilon)$ complexity of Algorithms 2 and 3 is not due to a loose analysis, as we show poly $(1/\\varepsilon)$ lower bounds in the following section. ", "page_idx": 7}, {"type": "text", "text": "3.4 Lower bounds for the stable proximal sampler ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now study lower bounds on the stable proximal sampler to sample from the class of target densities satisfying Assumption 2, which includes the generalized Cauchy target. Recall that Assumption 2 implies the FPI used in Theorem 3. The result below, proved in Appendix C, complements Theorem 3, showing the impossibility of achieving $\\log(1/\\varepsilon)$ rates for a sufficiently large $\\alpha$ . ", "page_idx": 7}, {"type": "text", "text": "Theorem 4. Suppose $\\pi^{X}\\propto\\exp(-V)$ with $V$ satisfying Assumption 2 and $\\begin{array}{r}{\\frac{\\nu_{2}(d+\\nu_{2})}{d+\\nu_{1}}<\\alpha\\leq2}\\end{array}$ . Let $x_{k}$ denote the $k^{t h}$ iterate of Algorithm 2 with parameter $\\alpha$ and step size $\\eta,$ , and let $\\rho_{k}^{X}:=\\operatorname{Law}(x_{k})$ . Then for any $\\begin{array}{r}{\\tau\\in\\big(\\frac{\\nu_{2}\\left(d+\\nu_{2}\\right)}{d+\\nu_{1}},\\alpha\\big)}\\end{array}$ , and $g(d,\\nu_{1},\\nu_{2},\\tau)=\\nu_{2}/\\{\\tau(d+\\nu_{1})-\\nu_{2}(d+\\nu_{2})\\},$ , we have $\\begin{array}{r}{\\mathrm{{IV}}(\\pi^{X},\\rho_{k}^{X})\\geq C_{\\nu_{1},\\nu_{2},\\alpha}d^{\\frac{\\tau(d+\\nu_{1})g(d,\\nu_{1},\\nu_{2},\\tau)}{2}}\\big(\\mathbb{E}[(1+|x_{0}|^{2})^{\\frac{\\tau}{2}}]+m_{\\tau}^{(\\alpha)}k^{\\frac{\\tau}{2}+1}\\eta^{\\frac{\\tau}{\\alpha}}\\big)^{-(d+\\nu_{2})g(d,\\nu_{1},\\nu_{2},\\tau)},}\\end{array}$ where $C_{\\nu_{1},\\nu_{2},\\alpha}$ is a constant depending only on $\\nu_{1},\\nu_{2},\\alpha$ , and $m_{\\tau}^{(\\alpha)}$ is the $\\tau^{t h}$ absolute moment of the $\\alpha$ -stable random variable with density $p_{1}^{(\\alpha)}$ defined in (2). ", "page_idx": 7}, {"type": "text", "text": "Remark 4. The parameter $\\tau$ in Theorem $^{4}$ can be chosen arbitrarily close to \u03b1. Specifically, if we assume $|X_{0}|\\leq{\\mathcal{O}}({\\sqrt{d}})$ , then with the choice of $\\begin{array}{r}{\\tau=\\alpha-\\left(\\frac{\\log(\\log d)}{\\log d}\\wedge\\frac{\\log\\log(\\eta^{-1})}{\\log(\\eta^{-1})}\\right)}\\end{array}$ , we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{TV}(\\pi^{X},\\rho_{k}^{X})\\geq\\tilde{\\Omega}_{\\nu_{1},\\nu_{2},\\alpha}\\big(d^{\\frac{\\tau(d+\\nu_{1})g(d,\\nu_{1},\\nu_{2},\\alpha)}{2}}\\big(d^{\\alpha}+m_{\\tau}^{(\\alpha)}k^{\\frac{\\alpha}{2}+1}\\eta\\big)^{-(d+\\nu_{2})g(d,\\nu_{1},\\nu_{2},\\alpha)}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\tilde{\\Omega}$ hides polylog $(d/\\eta)$ factors. ", "page_idx": 7}, {"type": "text", "text": "The $\\tau^{\\mathrm{th}}$ absolute moment of the $\\alpha$ -stable random variable depends on the choice of $\\alpha$ and the dimension $d$ . It is hard to find an explicit formula of $m_{\\tau}^{(\\alpha)}$ in general. An explicit formula is only available in some special cases, such as $\\alpha=1,2$ . Specializing Theorem 4 for the generalized Cauchy potential (i.e., $\\nu_{1}=\\nu_{2},$ ) we obtain the following explicit result. ", "page_idx": 7}, {"type": "text", "text": "Corollary 6. Let $\\alpha\\in(0,2]$ . Suppose $\\pi_{\\nu}\\propto\\exp(-V_{\\nu})$ where $V_{\\nu}(x)$ is as in (1) for some $\\nu\\in(0,\\alpha)$ . Let $(x_{k})_{k\\geq0}$ be the output of Algorithm 2 with parameter $\\alpha$ and step-size $\\eta>0$ , and $\\rho_{k}^{X}:=\\operatorname{Law}(x_{k})$ for all $k\\geq0$ . Then for any $\\tau\\in(\\nu,\\alpha)$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathrm{TV}(\\rho_{k}^{X},\\pi_{\\nu})\\ge C_{\\nu,\\alpha}d^{\\frac{\\nu\\tau}{2(\\tau-\\nu)}}\\bigl(\\mathbb{E}\\bigl[\\bigl(1+\\vert x_{0}\\vert^{2}\\bigr)^{\\frac{\\tau}{2}}\\bigr]+m_{\\tau}^{(\\alpha)}k^{\\frac{\\tau}{2}+1}\\eta^{\\frac{\\tau}{\\alpha}}\\bigr)^{-\\frac{\\nu}{\\tau-\\nu}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $m_{\\tau}^{(\\alpha)}$ is the $\\tau^{t h}$ absolute moment of the $\\alpha$ -stable random variable with density $p_{1}^{(\\alpha)}$ as in (2). ", "page_idx": 7}, {"type": "text", "text": "For the rejection sampling implementation in Algorithm 3, $\\alpha=1$ and $m_{\\tau}^{(1)}=\\Theta(d^{\\frac{\\tau}{2}})$ for all $\\tau<1$ (see Appendix B.1). Notice that to implement the $\\mathbf{R}\\alpha\\mathbf{S}\\mathbf{O}$ in the Stable proximal sampler efficiently, we need a sufficiently small step-size $\\eta$ . When the target potential satisfies Assumption 3, i.e. $V$ is $\\beta$ -H\u00f6lder continuous with parameter $L$ , we require $\\eta\\stackrel{.}{=}\\Theta(d^{-\\frac{1}{2}}L^{-\\frac{1}{\\beta}})$ to ensure $\\mathrm{R}\\alpha\\mathrm{S}\\mathrm{O}$ can be implemented with $\\mathcal{O}(1)$ queries. Therefore, if we choose $\\eta=\\Theta(d^{-\\frac{1}{2}}L^{-\\frac{1}{\\beta}})$ , the minimum number of iterations we need to get an $\\varepsilon$ -error in TV is ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\Omega_{\\nu,\\tau}\\Bigl(\\varepsilon^{-\\frac{2(\\tau-\\nu)}{(2+\\tau)\\nu}}d^{\\frac{\\tau}{2+\\tau}}L^{\\frac{2\\tau}{\\beta(2+\\tau)}}\\Bigr).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "For the generalized Cauchy potential with $\\nu\\in(0,1)$ , we have $\\beta=\\nu/4$ and $L=(d+\\nu)/\\nu$ , which leads to the following corollary. ", "page_idx": 8}, {"type": "text", "text": "Corollary 7. Suppose $\\pi_{\\nu}^{X}\\propto\\exp(-V_{\\nu})$ is the generalized Cauchy density with $\\nu\\in(0,1)$ . Let $x_{k}$ denote the $k$ -th iterate of the stable proximal sampler with $\\alpha=1$ (Algorithm 3), and $\\rho_{k}^{X}:=\\operatorname{Law}(x_{k})$ . If we choose the step size $\\eta=\\Theta(L^{-\\frac{4}{\\nu}}d^{-\\frac{1}{2}})$ where $\\begin{array}{r}{L=\\frac{d+\\nu}{\\nu}}\\end{array}$ is the $\\nu/4$ -H\u00f6lder constant of $V_{\\nu}$ , and assume, for simplicity, $|x_{0}|\\leq{\\mathcal{O}}({\\sqrt{d}})$ , then, $\\mathrm{TV}(\\pi_{\\nu}^{X},\\rho_{N}^{X})\\leq\\varepsilon$ requires $N\\ge\\Omega_{\\nu,\\tau}\\big(d^{\\frac{\\tau+8\\tau/\\nu}{2+\\tau}}\\varepsilon^{-\\frac{2(\\tau-\\nu)}{\\nu(2+\\tau)}}\\big)$ for any $\\tau\\in(\\nu,1)$ . Further, by choosing $\\begin{array}{r}{\\tau=\\operatorname*{max}(\\nu,1-\\frac{\\log(\\log(d/\\varepsilon))}{\\log(d/\\varepsilon)})}\\end{array}$ , we obtain ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{N\\geq\\tilde{\\Omega}_{\\nu}\\Big(d^{\\frac{\\nu+8}{3\\nu}}\\varepsilon^{-\\frac{2(1-\\nu)}{3\\nu}}\\Big),\\quad i n\\;o r d e r\\ f o r\\quad\\mathrm{TV}(\\pi_{\\nu}^{X},\\rho_{N}^{X})\\leq\\varepsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The above result shows that when implementing the $\\mathrm{R}\\alpha\\mathrm{SO}$ in Algorithm 2 with Algorithm 3, to sample from generalized Cauchy targets with $\\nu\\in(0,1)$ , we can at best have an iteration complexity of order poly $(1/\\varepsilon)$ , matching the upper bounds in Corollary 5 up to certain factors. ", "page_idx": 8}, {"type": "text", "text": "4 Overview of Proof Techniques ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Lower bounds. We build on the techniques developed in [Hai10]. Let $\\mu_{t}$ denotes the law of LD along its trajectory. To proceed, we need some $G:\\mathbb{R}^{d}\\xrightarrow{}\\mathbb{R}$ for which we can upper bound $\\mu_{t}(G):=$ $\\int G\\mathrm{d}\\mu_{t}$ , and some $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ that satisfies $\\pi^{X}(G\\geq y)\\geq f(y)$ for all $y\\in\\mathbb{R}_{+}$ . After finding the candidates $G$ and $f$ , Lemma 1 in Appendix A guarantees $\\begin{array}{r}{\\mathrm{TV}(\\pi^{X},\\mu_{t})\\geq\\operatorname*{sup}_{y\\in\\mathbb R_{+}}f(y)-\\mu_{t}(G)/y}\\end{array}$ . This technique relies on choosing $G$ such that it has heavy tails under $\\pi^{X}$ leading to a large $f(y)$ , while having light tails along the trajectory, thus small $\\mu_{t}(G)$ . By picking $G=\\exp(\\kappa V)$ with $\\kappa\\geq1$ , one can immediately observe that $\\bar{\\pi}^{X}(G)=\\infty$ , thus $G$ indeed has heavy tails under $\\pi^{X}$ . ", "page_idx": 8}, {"type": "text", "text": "To control $\\mu_{t}(G)$ along the trajectory, one can use the generator of LD to bound $\\partial_{t}\\mu_{t}(G)$ . Recall the generator of LD, $\\bar{\\mathcal{L}}_{\\mathrm{LD}}(\\cdot)\\,=\\,\\Delta\\dot{(}\\cdot)\\,-\\,\\langle\\nabla V,\\nabla\\cdot\\rangle$ . Therefore, with a choice of $G\\,=\\,\\exp(\\kappa V)$ , controlling $\\partial_{t}\\mu_{t}(G)$ requires bounding the first and second derivatives of $V$ . To avoid making extra assumptions for $V$ in the analysis of LD, we instead construct $G$ based on a surrogate potential $\\begin{array}{r}{\\tilde{V}(x)=\\frac{d+\\nu_{2}}{2}\\ln(1+|x|^{2})}\\end{array}$ , which is an upper bound to the potential $V$ . We then estimate $f$ based on this surrogate potential in Lemma 2, and control the growth of $\\mu_{t}(G)$ in Lemma 3. Combined with Lemma 1, this leads to the proof of Theorem 1, with the details provided in Appendix A. ", "page_idx": 8}, {"type": "text", "text": "For the Gaussian proximal sampler, bounding $\\rho_{k}^{X}(G)$ requires controlling the expectation of $G$ along the forward and backward heat flow. For the particular choice of $\\bar{G^{\\prime}}=\\exp(\\kappa V)$ , we show in Lemma 4 that the growth of $\\rho_{k}^{X}(G)$ can be controlled only by considering a forward heat flow with the corresponding generator $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{HF}}=\\frac{1}{2}\\Delta}\\end{array}$ . Therefore, given additional estimates on the second derivatives of $V$ , we bound the growth of $\\rho_{k}^{X}(G)$ in Lemma 5. Once this bound is achieved, we can invoke Lemma 1 to finish the proof of Theorem 2. ", "page_idx": 8}, {"type": "text", "text": "Upper bounds. Our upper bound analysis builds on that by [CCSW22] in the specific ways discussed next. We consider the change in $\\chi^{2}$ divergence when we apply the two operations to the law $\\rho_{k}^{X}$ to the iterates and the target $\\pi^{X}$ : $(i)$ evolving the two densities along the $\\alpha$ -fractional heat flow for time $\\eta$ and $(i i)$ applying the $\\mathrm{R}\\alpha\\mathrm{SO}$ to the resulting densities. For the step (i), it is required to show that the solution along the fractional heat flow of the stable proximal sampler at any time, satisfies FPI. To show this, $(a)$ the convolution property of the FPI is proved in Lemma 6, and $(b)$ the FPI parameter for the stable process follows from [Cha04, Theorem 23]. In Proposition 3, it is then shown that the $\\chi^{2}$ divergence decays exponentially fast along the fractional heat flow under the assumption of FPI. The aforementioned results enable us to prove the exponential decay of $\\chi^{2}$ divergence along the fractional heat flow under FPI in Proposition 3. To deal with the step (ii) above, we use the data processing inequality; see Proposition 3. These two steps together, enable us to derive the stated upper bounds for the stable proximal sampler. ", "page_idx": 8}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We showed the limitations of Gaussian proximal samplers for high-accuracy heavy-tailed sampling, and proposed and analyzed stable proximal samplers, establishing that they are indeed high-accuracy algorithms. We now list a few important limitations and problems for future research: (i) It is important to develop efficiently implementable versions of the stable proximal sampler for all values of $\\bar{\\alpha}\\,\\in\\,(0,2)$ , and characterize their complexity in terms of problem parameters, (ii) Gaussian proximal samplers can be interpreted as a proximal point method for approximating the entropic regularized Wasserstein gradient flow of the KL objective [CCSW22]. This leads to the question, can we provide a variational intepreration of the stable proximal sampler? A potential approach is to leverage the results by [Erb14] on gradient flow interpretation of jump processes corresponding to the fractional heat equation, (iii) It is possible to use a non-standard It\u00f4 process in the proximal sampler (in place of the $\\alpha$ -stable diffusion); see, for example, [EMS18, LWME19, HFBE24]. With this modification, it is interesting to examine the rates under weighted Poincar\u00e9 inequalities that also characterize heavy-tailed densities. There are two difficulties to overcome here: $(a)$ How to generate an exact non-standard It\u00f4 process? $(b)$ How to implement the corresponding Restricted non-standard Gaussian Oracle, which requires the zeroth order information of the transition density of the It\u00f4 process? In certain cases, non-standard It\u00f4 diffusion can be interpreted as a Brownian motion on an embedded sub-manifold; thus, the approach in $[\\mathrm{GLL}^{+}23]$ ] might be useful. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "KB was supported in part by NSF grants DMS-2053918 and DMS-2413426. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[ALPW22] Christophe Andrieu, Anthony Lee, Sam Power, and Andi Q Wang, Comparison of Markov chains via weak Poincar\u00e9 inequalities with application to pseudo-marginal MCMC, The Annals of Statistics 50 (2022), no. 6, 3592\u20133618.   \n[ALPW23] , Weak Poincar\u00e9 Inequalities for Markov chains: Theory and Applications, arXiv preprint arXiv:2312.11689 (2023).   \n[App09] David Applebaum, L\u00e9vy processes and stochastic calculus, Cambridge university press, 2009.   \n$[\\mathbf{BCE}^{+}22]$ Krishnakumar Balasubramanian, Sinho Chewi, Murat A Erdogdu, Adil Salim, and Shunshi Zhang, Towards a theory of non-log-concave sampling: First-order stationarity guarantees for Langevin Monte Carlo, Conference on Learning Theory, PMLR, 2022, pp. 2896\u20132923.   \n[BHJ08] Krzysztof Bogdan, Wolfhard Hansen, and Tomasz Jakubowski, Time-dependent Schr\u00f6dinger perturbations of transition densities, Studia Mathematica 189 (2008), no. 3, 235\u2013254.   \n[BRZ19] Joris Bierkens, Gareth O Roberts, and Pierre-Andr\u00e9 Zitt, Ergodicity of the zigzag process, The Annals of Applied Probability 29 (2019), no. 4, 2266\u20132301.   \n[BZ17] Maria-Florina F Balcan and Hongyang Zhang, Sample and computationally efficient learning algorithms under s-concave distributions, Advances in Neural Information Processing Systems 30 (2017).   \n[CBL22] Niladri S Chatterji, Peter L Bartlett, and Philip M Long, Oracle lower bounds for stochastic gradient sampling algorithms, Bernoulli 28 (2022), no. 2, 1074\u20131092.   \n[CCBJ18] Xiang Cheng, Niladri S Chatterji, Peter L Bartlett, and Michael I Jordan, Underdamped Langevin MCMC: A non-asymptotic analysis, Conference on learning theory, PMLR, 2018, pp. 300\u2013323.   \n[CCSW22] Yongxin Chen, Sinho Chewi, Adil Salim, and Andre Wibisono, Improved analysis for a proximal algorithm for sampling, Conference on Learning Theory, PMLR, 2022, pp. 2984\u20133014.   \n[CDV09] Karthekeyan Chandrasekaran, Amit Deshpande, and Santosh Vempala, Sampling s-concave functions: The limit of convexity based isoperimetry, International Workshop on Approximation Algorithms for Combinatorial Optimization, Springer, 2009, pp. 420\u2013433.   \n[CG03] Eric A Carlen and Wilfrid Gangbo, Constrained steepest descent in the 2-Wasserstein metric, Annals of mathematics (2003), 807\u2013846.   \n[CG23] Yuansi Chen and Khashayar Gatmiry, A Simple Proof of the Mixing of MetropolisAdjusted Langevin Algorithm under Smoothness and Isoperimetry, arXiv preprint arXiv:2304.04095 (2023).   \n$[\\mathbf{C}\\mathbf{G}\\mathbf{L}^{+}22]$ Sinho Chewi, Patrik R Gerber, Chen Lu, Thibaut Le Gouic, and Philippe Rigollet, The query complexity of sampling from strongly log-concave distributions in one dimension, Proceedings of Thirty Fifth Conference on Learning Theory, vol. 178, PMLR, 2022, pp. 2041\u20132059.   \n[CGLL22] Sinho Chewi, Patrik Gerber, Holden Lee, and Chen Lu, Fisher information lower bounds for sampling, arXiv preprint arXiv:2210.02482 (2022).   \n[Cha04] Djalil Chafa\u00ef, Entropies, convexity, and functional inequalities, on $\\Phi$ -entropies and $\\Phi$ -sobolev inequalities, Journal of Mathematics of Kyoto University 44 (2004), no. 2, 325\u2013363.   \n$[\\mathrm{CLA}^{+}21]$ Sinho Chewi, Chen Lu, Kwangjun Ahn, Xiang Cheng, Thibaut Le Gouic, and Philippe Rigollet, Optimal dimension dependence of the Metropolis-Adjusted Langevin Algorithm, Conference on Learning Theory, PMLR, 2021, pp. 1260\u20131300.   \n[CLW23] Yu Cao, Jianfeng Lu, and Lihan Wang, On explicit $L_{2}$ -convergence rate estimate for underdamped Langevin dynamics, Archive for Rational Mechanics and Analysis 247 (2023), no. 5, 90.   \n[DBCD19] George Deligiannidis, Alexandre Bouchard-C\u00f4t\u00e9, and Arnaud Doucet, Exponential ergodicity of the Bouncy Particle Sampler, Annals of Statistics 47 (2019), no. 3.   \n[DCWY19] Raaz Dwivedi, Yuansi Chen, Martin J Wainwright, and Bin Yu, Log-concave sampling: Metropolis-Hastings algorithms are fast, Journal of Machine Learning Research 20 (2019), no. 183, 1\u201342.   \n[DGM20] Alain Durmus, Arnaud Guillin, and Pierre Monmarch\u00e9, Geometric ergodicity of the Bouncy Particle Sampler, Annals of applied probability 30 (2020), no. 5, 2069\u20132098.   \n[DKTZ20] Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis, Learning halfspaces with Massart noise under structured distributions, Conference on Learning Theory, PMLR, 2020, pp. 1486\u20131513.   \n[DM17] Alain Durmus and \u00c9ric Moulines, Nonasymptotic convergence analysis for the unadjusted Langevin algorithm, The Annals of Applied Probability 27 (2017), no. 3, 1551\u20131587 (en).   \n[DRD20] Arnak S Dalalyan and Lionel Riou-Durand, On sampling from a log-concave density using kinetic Langevin diffusions, Bernoulli 26 (2020), no. 3, 1956\u20131988.   \n[EGZ19] Andreas Eberle, Arnaud Guillin, and Raphael Zimmer, Couplings and quantitative contraction rates for Langevin dynamics, The Annals of Probability 47 (2019), no. 4, 1982\u20132010.   \n[EH21] Murat A Erdogdu and Rasa Hosseinzadeh, On the convergence of Langevin Monte Carlo: The interplay between tail growth and smoothness, Conference on Learning Theory, PMLR, 2021, pp. 1776\u20131822.   \n[EHZ22] Murat A Erdogdu, Rasa Hosseinzadeh, and Shunshi Zhang, Convergence of Langevin Monte Carlo in chi-squared and R\u00e9nyi divergence, International Conference on Artificial Intelligence and Statistics, PMLR, 2022, pp. 8151\u20138175.   \n[EMS18] Murat A Erdogdu, Lester Mackey, and Ohad Shamir, Global non-convex optimization with discretized diffusions, Advances in Neural Information Processing Systems 31 (2018).   \n[Erb14] Matthias Erbar, Gradient flows of the entropy for jump processes, Annales de l\u2019IHP Probabilit\u00e9s et statistiques, vol. 50, 2014, pp. 920\u2013945.   \n[FYC23] Jiaojiao Fan, Bo Yuan, and Yongxin Chen, Improved dimension dependence of a proximal algorithm for sampling, arXiv preprint arXiv:2302.10081 (2023).   \n[GB09] Alan Genz and Frank Bretz, Computation of multivariate normal and t-probabilities, vol. 195, Springer Science & Business Media, 2009.   \n[GBH04] Alan Genz, Frank Bretz, and Yosef Hochberg, Approximations to multivariate t integrals with application to multiple comparison procedures, Recent Developments in Multiple Comparison Procedures, Institute of Mathematical Statistics, 2004, pp. 24\u201332.   \n[GJPS08] Andrew Gelman, Aleks Jakulin, Maria Grazia Pittau, and Yu-Sung Su, A weakly informative default prior distribution for logistic and other regression models, The annals of applied statistics 2 (2008), no. 4, 1360\u20131383.   \n[GLL20] Rong Ge, Holden Lee, and Jianfeng Lu, Estimating normalizing constants for logconcave distributions: Algorithms and lower bounds, Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, 2020, pp. 579\u2013586.   \n$[\\mathrm{GLL}^{+}23]$ Sivakanth Gopi, Yin Tat Lee, Daogao Liu, Ruoqi Shen, and Kevin Tian, Algorithmic aspects of the log-Laplace transform and a non-Euclidean proximal sampler, arXiv preprint arXiv:2302.06085 (2023).   \n[GLM18] Joyee Ghosh, Yingbo Li, and Robin Mitra, On the use of Cauchy prior distributions for Bayesian logistic regression, Bayesian Analysis 13 (2018), no. 2, 359\u2013383.   \n[Hai10] Martin Hairer, Convergence of Markov processes, Lecture notes (2010).   \n[HBE20] Ye He, Krishnakumar Balasubramanian, and Murat A Erdogdu, On the ergodicity, bias and asymptotic normality of randomized midpoint sampling method, Advances in Neural Information Processing Systems 33 (2020), 7366\u20137376.   \n[HBE24] , An analysis of Transformed Unadjusted Langevin Algorithm for Heavy-tailed Sampling, IEEE Transactions on Information Theory (2024).   \n[HFBE24] Ye He, Tyler Farghly, Krishnakumar Balasubramanian, and Murat A Erdogdu, Meansquare analysis of discretized It\u00f4 diffusions for heavy-tailed sampling, Journal of Machine Learning Research (to appear) (2024).   \n[HMW21] Lu-Jing Huang, Mateusz B Majka, and Jian Wang, Approximation of heavy-tailed distributions via stable-driven SDEs, Bernoulli 27 (2021), no. 3, 2040\u20132068.   \n[JG12] Leif T Johnson and Charles J Geyer, Variable transformation to obtain geometric ergodicity in the Random-Walk Metropolis algorithm, The Annals of Statistics 40 (2012), no. 6, 3050\u20133076.   \n[JR07] S\u00f8ren Jarner and Gareth Roberts, Convergence of heavy-tailed Monte Carlo Markov Chain algorithms, Scandinavian Journal of Statistics 34 (2007), no. 4, 781\u2013815.   \n[Kam18] Kengo Kamatani, Efficient strategy for the Markov chain Monte Carlo in highdimension with heavy-tailed target probability distribution, Bernoulli 24 (2018), no. 4B, 3711\u20133750.   \n[KN04] Samuel Kotz and Saralees Nadarajah, Multivariate t-distributions and their applications, Cambridge University Press, 2004.   \n[Kwa17] Mateusz Kwa\u00b4snicki, Ten equivalent definitions of the fractional Laplace operator, Fractional Calculus and Applied Analysis 20 (2017), no. 1, 7\u201351.   \n[LST20] Yin Tat Lee, Ruoqi Shen, and Kevin Tian, Logsmooth gradient concentration and tighter runtimes for Metropolized Hamiltonian Monte Carlo, Conference on learning theory, PMLR, 2020, pp. 2565\u20132597.   \n[LST21a] , Lower bounds on Metropolized sampling methods for well-conditioned distributions, Advances in Neural Information Processing Systems 34 (2021), 18812\u201318824.   \n[LST21b] , Structured logconcave sampling with a Restricted Gaussian Oracle, Conference on Learning Theory, PMLR, 2021, pp. 2993\u20133050.   \n[LWME19] Xuechen Li, Yi Wu, Lester Mackey, and Murat A Erdogdu, Stochastic Runge-Kutta accelerates Langevin Monte Carlo and beyond, Advances in neural information processing systems 32 (2019).   \n[LZT22] Ruilin Li, Hongyuan Zha, and Molei Tao, Sqrt(d) Dimension Dependence of Langevin Monte Carlo, The International Conference on Learning Representations, 2022.   \n[MFWB22] Wenlong Mou, Nicolas Flammarion, Martin J Wainwright, and Peter L Bartlett, Improved bounds for discretization of Langevin diffusions: Near-optimal rates without convexity, Bernoulli 28 (2022), no. 3, 1577\u20131601.   \n$\\mathrm{[MHFH^{+}23]}$ ] Alireza Mousavi-Hosseini, Tyler K. Farghly, Ye He, Krishna Balasubramanian, and Murat A. Erdogdu, Towards a Complete Analysis of Langevin Monte Carlo: Beyond Poincar\u00e9 Inequality, Proceedings of Thirty Sixth Conference on Learning Theory, vol. 195, 2023, pp. 1\u201335.   \n[Nol20] John P Nolan, Univariate stable distributions, Springer, 2020.   \n[NS\u00b8R19] Than Huy Nguyen, Umut S\u00b8ims\u00b8ekli, and Ga\u00ebl Richard, Non-asymptotic analysis of Fractional Langevin Monte Carlo for non-convex optimization, International Conference on Machine Learning, 2019, pp. 4810\u20134819.   \n[PBEM23] Mathieu Le Provost, Ricardo Baptista, Jeff D Eldredge, and Youssef Marzouk, An adaptive ensemble fliter for heavy-tailed distributions: Tuning-free inflation and localization, arXiv preprint arXiv:2310.08741 (2023).   \n[QM16] Di Qi and Andrew J Majda, Predicting fat-tailed intermittent probability distributions in passive scalar turbulence with imperfect models through empirical information theory, Communications in Mathematical Sciences 14 (2016), no. 6, 1687\u20131722.   \n[RRT17] Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky, Non-convex learning via stochastic gradient Langevin dynamics: A nonasymptotic analysis, Conference on Learning Theory, PMLR, 2017, pp. 1674\u20131703.   \n[SL19] Ruoqi Shen and Yin Tat Lee, The randomized midpoint method for log-concave sampling, Advances in Neural Information Processing Systems 32 (2019).   \n[SP15] Prashant D Sardeshmukh and C\u00e9cile Penland, Understanding the distinctively skewed and heavy tailed character of atmospheric and oceanic probability distributions, Chaos: An Interdisciplinary Journal of Nonlinear Science 25 (2015), no. 3.   \n[S\u00b8ZTG20] Umut S\u00b8im\u00b8sekli, Lingjiong Zhu, Yee Whye Teh, and Mert Gurbuzbalaban, Fractional underdamped Langevin dynamics: Retargeting SGD with momentum under heavytailed gradient noise, International Conference on Machine Learning, 2020, pp. 8970\u2013 8980.   \n[Wib18] Andre Wibisono, Sampling as optimization in the space of measures: The Langevin dynamics as a composite optimization problem, Conference on Learning Theory, PMLR, 2018, pp. 2093\u20133027.   \n[WSC22a] Keru Wu, Scott Schmidler, and Yuansi Chen, Minimax mixing time of the Metropolisadjusted Langevin algorithm for log-concave sampling, The Journal of Machine Learning Research 23 (2022), no. 1, 12348\u201312410.   \n[WSC22b] , Minimax Mixing Time of the Metropolis-Adjusted Langevin Algorithm for Log-Concave Sampling, Journal of Machine Learning Research 23 (2022), no. 270, 1\u201363.   \n[WW15] Feng-Yu Wang and Jian Wang, Functional inequalities for stable-like Dirichlet forms, Journal of Theoretical Probability 28 (2015), no. 2, 423\u2013448.   \n[Y\u0141R22] Jun Yang, Krzysztof \u0141atuszy\u00b4nski, and Gareth Roberts, Stereographic Markov Chain Monte Carlo, arXiv preprint arXiv:2205.12112 (2022).   \n[ZZ23] Xiaolong Zhang and Xicheng Zhang, Ergodicity of supercritical SDEs driven by $\\alpha$ -stable processes and heavy-tailed sampling, Bernoulli 29 (2023), no. 3, 1933\u20131958. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Lower Bound Proofs for the Langevin Diffusion and the Gaussian Proximal Sampler ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "While research on upper bounds of sampling algorithms\u2019 complexity has advanced considerably, the exploration of lower bounds is still nascent. $[\\bar{\\mathrm{CGL}}^{+}22]$ explored the query complexity of sampling from strongly log-concave distributions in one-dimensional settings. [LZT22] established lower bounds for LMC in sampling from strongly log-concave distributions. [CBL22] presented lower bounds for sampling from strongly log-concave distributions with noisy gradients. [GLL20] focused on lower bounds for estimating normalizing constants of log-concave densities. Contributions by [LST21a] and [WSC22b] provide lower bounds in the metropolized algorithm category, including Langevin and Hamiltonian Monte Carlo, in strongly log-concave contexts. Finally, [CGLL22] contributed to lower bounds in Fisher information for non-log-concave sampling. In what follows, we take a different approach and rely on the arguments developed in [Hai10]. ", "page_idx": 14}, {"type": "text", "text": "We begin by stating the following result which drives our lower bound strategy. ", "page_idx": 14}, {"type": "text", "text": "Lemma 1 ([Hai10, Theorem 5.1]). Suppose $\\mu$ and $\\nu$ are probability measures on $\\mathbb{R}^{d}$ . Consider some $G:\\mathbb{R}^{d}\\rightarrow\\bar{\\mathbb{R}}_{+}$ and $f:\\mathbb{R}_{+}\\to\\mathbb{R}_{+}$ satisfying $\\mu(G\\geq y)\\geq f(y)$ for all $y\\in\\mathbb{R}_{+}$ . Then, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname{TV}(\\mu,\\nu)\\geq\\operatorname*{sup}_{y\\in\\mathbb{R}_{+}}f(y)-\\frac{\\int G\\mathrm{d}\\nu}{y}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In particular, suppose $\\operatorname{Id}\\cdot f:\\mathbb{R}_{+}\\ni y\\mapsto y f(y)\\in\\mathbb{R}_{+}$ is a bijection, then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{TV}(\\mu,\\nu)\\geq\\frac{1}{2}f\\Bigl((\\mathrm{Id}\\cdot f)^{-1}\\bigl(2m\\bigr)\\Bigr),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for any $m\\geq\\int G\\mathrm{d}\\nu$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. By the definition of total variation and Markov\u2019s inequality, for any $y>0$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname{TV}(\\mu,\\nu)\\geq\\mu(G\\geq y)-\\nu(G\\geq y)\\geq f(y)-{\\frac{\\int G\\mathrm{d}\\nu}{y}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "When $\\operatorname{Id}\\cdot f$ is invertible, choosing $y=(\\operatorname{Id}\\cdot f)^{-1}(2m)$ implies $y f(y)=2m$ and yields the desired result. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "To apply Lemma 1 when the target density satisfies Assumption 1, we need to establish tail lower bounds for this density, which we do so via the following lemma. In the following, let $\\omega_{d}\\;:=\\;$ \u0393((d\u03c0+2)/2) denote the volume of the unit d-ball. ", "page_idx": 14}, {"type": "text", "text": "Lemma 2. Suppose $\\pi^{X}(x)\\propto\\exp(-V(x))$ satisfies Assumption 1. Then, for all $R>0$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\pi^{X}(|x|\\geq R)\\geq\\frac{2d e^{-\\nu_{1}/d}}{(d+\\nu_{1})\\Gamma(\\nu_{1}/2)}(d/2)^{\\nu_{1}/2}(1+R^{-2})^{-(d+\\nu_{2})/2}R^{-\\nu_{2}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "When focusing on dependence on $R$ and $d$ , we obtain, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\pi^{X}(|x|\\geq R)\\geq C_{\\nu_{1}}d^{\\nu_{1}/2}(1+R^{-2})^{-(d+\\nu_{2})/2}R^{-\\nu_{2}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where C\u03bd1 = $\\begin{array}{r}{C_{\\nu_{1}}=\\frac{2^{1-\\nu_{1}/2}e^{-\\nu_{1}}}{(1+\\nu_{1})\\Gamma\\left(\\nu_{1}/2\\right)}}\\end{array}$ ", "page_idx": 14}, {"type": "text", "text": "Proof. Without loss of generality assume $V(0)=0$ . Via Assumption 1, we have the estimates for $V$ ", "page_idx": 14}, {"type": "equation", "text": "$$\nV(x)=\\int_{t=0}^{1}\\langle x,\\nabla V(t x)\\rangle\\mathrm{d}t\\leq(d+\\nu)\\int_{t=0}^{1}{\\frac{t|x|^{2}\\mathrm{d}t}{1+|t x|^{2}}}={\\frac{d+\\nu_{2}}{2}}\\ln(1+|x|^{2}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and similarly ", "page_idx": 14}, {"type": "equation", "text": "$$\nV(x)\\ge\\frac{d+\\nu_{1}}{2}\\ln(1+|x|^{2}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Consequently, using the spherical coordinates, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pi^{X}(|x|\\geq R)\\geq\\displaystyle\\frac{1}{Z}\\int_{|x|\\geq R}(1+|x|^{2})^{-(d+\\nu_{2})/2}\\mathrm{d}x}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\frac{d\\omega_{d}}{Z}\\int_{r\\geq R}(1+r^{2})^{-(d+\\nu_{2})/2}r^{d-1}\\mathrm{d}r}\\\\ &{\\qquad\\qquad\\geq\\displaystyle\\frac{d\\omega_{d}(1+R^{-2})^{-(d+\\nu_{2})/2}}{Z}\\int_{r\\geq R}r^{-\\nu_{2}-1}\\mathrm{d}r}\\\\ &{\\qquad\\qquad=\\displaystyle\\frac{d\\omega_{d}(1+R^{-2})^{-(d+\\nu_{2})/2}}{Z_{d}}R^{-\\nu_{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Next, using the lower bound established on $V$ and spherical coordinates, we obtain, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Z\\le\\displaystyle\\int_{\\mathbb R^{d}}(1+|x|^{2})^{-(d+\\nu_{1})/2}\\ensuremath{\\mathrm{d}}x}\\\\ &{\\quad=d\\omega_{d}\\displaystyle\\int_{0}^{\\infty}(1+r^{2})^{-(d+\\nu_{1})/2}r^{d-1}\\ensuremath{\\mathrm{d}}r}\\\\ &{\\quad=\\displaystyle\\frac12d\\omega_{d}\\displaystyle\\int_{0}^{\\infty}u^{\\nu_{1}/2-1}(1-u)^{d/2-1}\\ensuremath{\\mathrm{d}}u}\\\\ &{\\quad=\\displaystyle\\frac12d\\omega_{d}\\ensuremath{\\mathrm{B}}(\\nu_{1}/2,d/2)}\\\\ &{\\quad=\\displaystyle\\frac{d\\omega_{d}\\ensuremath{\\mathrm{D}}(\\nu_{1}/2)\\ensuremath{\\Gamma{(d/2)}}}{2\\Gamma((d+\\nu_{1})/2)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where B denotes the beta function. Plugging back into our tail lower bound, we obtain, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\pi^{X}(|x|\\geq R)\\geq{\\frac{2\\Gamma((d+\\nu_{1})/2)}{\\Gamma(\\nu_{1}/2)\\Gamma(d/2)}}(1+R^{-2})^{-(d+\\nu_{2})/2}R^{-\\nu_{2}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Moreover, by $[\\mathrm{MHFH}^{+}23$ , Lemma 32] we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\Gamma((d+\\nu_{1})/2)}{\\Gamma(d/2)}=\\frac{d}{d+\\nu_{1}}\\frac{\\Gamma((d+\\nu_{1}+2)/2)}{\\Gamma((d+2)/2)}\\geq\\frac{2d e^{-\\nu_{1}/d}}{d+\\nu_{1}}(d/2)^{\\nu_{1}/2},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which completes the proof. ", "page_idx": 15}, {"type": "text", "text": "Another element of Lemma 1 is controlling the growth of $\\mathbb{E}[G(X_{t})]$ throughout the process. The following lemma achieves such control under the Langevin diffusion. ", "page_idx": 15}, {"type": "text", "text": "Lemma 3. Suppose $(X_{t})_{t\\geq0}$ is the solution to the Langevin diffusion starting at $X_{0}$ with the corresponding potential $\\textstyle{\\frac{d+\\nu_{2}}{2}}\\ln(1+|x|^{2})$ and $\\begin{array}{r}{\\kappa\\geq\\frac{2}{d+\\nu_{2}}\\vee1}\\end{array}$ $V(x)$ satisfying Assumption . Then, $^{\\,l}$ . Let $G(x)=\\exp(\\kappa\\tilde{V}(x))$ where ${\\tilde{V}}(x)=$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[G(X_{t})]\\leq\\Big(\\mathbb{E}[G(X_{0})]^{\\frac{2}{\\kappa(d+\\nu_{2})}}+4\\kappa(d+\\nu_{2})t\\Big)^{\\frac{\\kappa(d+\\nu_{2})}{2}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Recall the generator of the Langevin diffusion $\\mathcal{L}(\\cdot)=\\Delta\\cdot-\\langle\\nabla V,\\nabla\\cdot\\rangle$ . Then, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathrm{d}\\mathbb{E}[G(X_{t})]}{\\mathrm{d}t}=\\mathbb{E}[Z G(X_{t})]}\\\\ &{\\quad\\quad\\quad\\quad=\\kappa\\mathbb{E}\\left[\\left(\\kappa|\\nabla\\tilde{V}|^{2}+\\Delta\\tilde{V}-\\langle\\nabla\\tilde{V},\\nabla V\\rangle\\right)G\\right]}\\\\ &{\\quad\\quad\\quad\\leq\\kappa\\mathbb{E}\\left[\\left(\\kappa|\\nabla\\tilde{V}|^{2}+\\Delta\\tilde{V}\\right)G\\right]\\quad\\quad\\mathrm{(Assumption~1)}}\\\\ &{\\quad\\quad\\quad\\leq2\\kappa^{2}(d+\\nu_{2})^{2}\\mathbb{E}\\left[\\frac{G(X_{t})}{1+|X_{t}|^{2}}\\right]}\\\\ &{\\quad\\quad\\quad=2\\kappa^{2}(d+\\nu_{2})^{2}\\mathbb{E}\\left[G(X_{t})^{1-\\frac{2}{\\kappa(d+\\nu_{2})}}\\right]}\\\\ &{\\quad\\quad\\quad\\leq2\\kappa^{2}(d+\\nu_{2})^{2}\\mathbb{E}[G(X_{t})^{1-\\frac{2}{\\kappa(d+\\nu_{2})}}\\quad\\quad\\mathrm{(Jensen's~Inequality),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Integrating the above inequality completes the proof. ", "page_idx": 15}, {"type": "text", "text": "With the above lemmas in hand, we are ready to present the proof of Theorem 1. ", "page_idx": 16}, {"type": "text", "text": "f woift hT .a pBply y LLeemmmmaa  21  wwee  hcahvoeose $G(x)=\\exp(\\kappa\\tilde{V}(x))$ where $\\begin{array}{r}{\\tilde{V}(x)=\\frac{d+\\nu_{2}}{2}\\ln(1+}\\end{array}$ $|x|^{2})$ $\\begin{array}{r}{\\kappa\\geq1\\vee\\frac{2}{d+\\nu_{2}}}\\end{array}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\pi^{X}(G(x)\\geq y)\\geq\\pi^{X}\\left(|x|\\geq y^{\\frac{1}{\\kappa(d+\\nu_{2})}}\\right)\\geq C_{\\nu_{1}}d^{\\nu_{1}/2}\\left(1+y^{\\frac{-2}{\\kappa(d+\\nu_{2})}}\\right)^{-(d+\\nu_{2})/2}y^{\\frac{-\\nu_{2}}{\\kappa(d+\\nu_{2})}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Moreover, define ", "page_idx": 16}, {"type": "equation", "text": "$$\ng(t):=\\Big(g(0)^{\\frac{2}{\\kappa(d+\\nu_{2})}}+4\\kappa(d+\\nu_{2})t\\Big)^{\\frac{\\kappa(d+\\nu_{2})}{2}}\\,,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with $g(0):=\\mathbb{E}[G(X_{0})]$ . Then by Lemma 3 we have $\\mathbb{E}[G(X_{t})]\\leq g(t)$ and we can invoke Lemma 1 to obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{TV}(\\pi^{X},\\mu_{t})\\geq\\underset{y\\in\\mathbb R_{+}}{\\operatorname*{sup}}C_{\\nu_{1}}d^{\\nu_{1}/2}\\left(1+y^{\\frac{-2}{\\kappa(d+\\nu_{2})}}\\right)^{-(d+\\nu_{2})/2}y^{\\frac{-\\nu_{2}}{\\kappa(d+\\nu_{2})}}-\\frac{g(t)}{y}.}\\\\ &{\\qquad\\qquad\\geq\\underset{y\\in\\mathbb R_{+}}{\\operatorname*{sup}}C_{\\nu_{1}}d^{\\nu_{1}/2}\\exp\\left(-\\frac{(d+\\nu_{2})y^{\\frac{-2}{\\kappa(d+\\nu_{2})}}}{2}\\right)y^{\\frac{-\\nu_{2}}{\\kappa(d+\\nu_{2})}}-\\frac{g(t\\vee1)}{y},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we used the fact that $1+x\\leq e^{x}$ for all $x\\in\\mathbb R$ and $g(t)$ is non-decreasing in $t$ . Choose ", "page_idx": 16}, {"type": "equation", "text": "$$\ny^{*}:=C_{\\nu_{1},\\nu_{2}}^{\\prime}\\left(\\frac{g(t\\vee1)}{d^{\\nu_{1}/2}}\\right)^{\\frac{\\kappa(d+\\nu_{2})}{\\kappa(d+\\nu_{2})-\\nu_{2}}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for a sufficiently large constant $C_{\\nu_{1},\\nu_{2}}^{\\prime}\\geq1$ . For simplicity, let ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\tilde{g}(t):=\\frac{g(t\\vee1)^{\\frac{2}{\\kappa(d+\\nu_{2})}}}{4\\kappa(d+\\nu_{2})},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and notice that ", "page_idx": 16}, {"type": "equation", "text": "$$\ny^{*}=C_{\\nu_{1},\\nu_{2}}^{\\prime}d^{\\frac{\\kappa(d+\\nu_{2})}{2}\\cdot\\frac{\\kappa(d+\\nu_{2})-\\nu_{1}}{\\kappa(d+\\nu_{2})-\\nu_{2}}}\\left(4\\kappa(1+\\nu_{2}/d)\\tilde{g}(t)\\right)^{\\frac{\\kappa^{2}(d+\\nu_{2})^{2}}{2(\\kappa(d+\\nu_{2})-\\nu_{2})}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Using the fact that ", "page_idx": 16}, {"type": "equation", "text": "$$\ny^{*}\\ge(4\\kappa)^{\\frac{\\kappa^{2}(d+\\nu_{2})^{2}}{2(\\kappa(d+\\nu_{2})-\\nu_{2})}}d^{\\frac{\\kappa(d+\\nu_{2})}{2}\\cdot\\frac{\\kappa(d+\\nu_{2})-\\nu_{1}}{\\kappa(d+\\nu_{2})-\\nu_{2}}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{TV}(\\pi^{X},\\mu_{t})\\geq C_{\\nu_{1}}\\exp\\Big(-\\frac{1+\\nu_{2}/d}{8\\kappa}\\cdot d^{\\frac{\\nu_{1}-\\nu_{2}}{\\kappa(d+\\nu_{2})-\\nu_{2}}}\\Big)d^{\\nu_{1}/2}y^{\\ast\\frac{-\\nu_{2}}{\\kappa(d+\\nu_{2})}}-\\frac{g(t\\vee1)}{y^{\\ast}}}\\\\ &{\\qquad\\qquad\\geq\\tilde{C}_{\\nu_{1},\\nu_{2}}d^{\\nu_{1}/2}y^{\\ast\\frac{-\\nu_{2}}{\\kappa(d+\\nu_{2})}}-\\frac{g(t\\vee1)}{y^{\\ast}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\tilde{C}_{\\nu_{1},\\nu_{2}}=C_{\\nu_{1}}e^{-\\frac{1+\\nu_{2}/d}{8}}$ . By plugging in the value of $y^{\\ast}$ from (3), we obtain, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathrm{TV}(\\pi^{X},\\mu_{t})}\\\\ &{\\geq\\left\\{\\tilde{C}_{\\nu_{1},\\nu_{2}}C_{\\nu_{1},\\nu_{2}}^{\\prime}\\frac{-\\nu_{2}}{\\kappa(d+\\nu_{2})}-C_{\\nu_{1},\\nu_{2}}^{\\prime}-1\\right\\}\\left\\{d^{\\frac{\\nu_{1}-\\nu_{2}}{2}}\\left(2\\kappa(1+\\nu_{2}/d)\\tilde{g}(t)\\right)^{\\frac{-\\nu_{2}}{2}}\\right\\}^{1+\\frac{\\nu_{2}}{\\kappa(d+\\nu_{2})-\\nu_{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus for sufficiently large $C_{\\nu_{1},\\nu_{2}}^{\\prime}$ , there exists $C_{\\nu_{1},\\nu_{2}}^{\\prime\\prime}$ such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{TV}(\\pi^{X},\\mu_{t})\\ge C_{\\nu_{1},\\nu_{2}}^{\\prime\\prime}\\left\\{d^{\\frac{\\nu_{1}-\\nu_{2}}{2}}\\left(4\\kappa(1+\\nu/d)\\tilde{g}(t)\\right)\\right)^{\\frac{-\\nu_{2}}{2}}\\right\\}^{1+\\frac{\\nu_{2}}{\\kappa(d+\\nu_{2})-\\nu_{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Choosing $\\kappa$ according to the statement of the theorem completes the proof. ", "page_idx": 16}, {"type": "text", "text": "In order to prove a similar theorem for the Gaussian proximal sampler, we control the growth of $\\mathbb{E}[G(x_{k})]$ for the iterates of the proximal sampler via the following lemmas. ", "page_idx": 16}, {"type": "text", "text": "Lemma 4. Suppose $(x_{k},y_{k})_{k}$ are the iterates of the Gaussian proximal sampler with step size \u03b7 and target density $\\pi^{X}\\propto\\exp(-V)$ for some $V:\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ . Let $G(x)=\\exp(\\kappa V(x))$ with $\\kappa\\geq1$ . Then, for every $k\\geq0$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}[G(x_{k+1})]\\le\\mathbb{E}[G(x_{k}+\\sqrt{2\\eta}z)],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $z\\sim\\mathcal{N}(0,I_{d})$ is sampled independently from $x_{k}$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. Recall that $\\begin{array}{r}{\\pi^{X|Y}(x|y)\\propto\\exp\\big(-V(x)-\\frac{|x-y|^{2}}{2\\eta}\\big)}\\end{array}$ . Therefore, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[G(x_{k+1})\\mid y_{k}]=C_{y_{k}}\\int\\frac{\\exp\\big((\\kappa-1)V(x)-\\frac{|x-y_{k}|^{2}}{2\\eta}\\big)}{(2\\pi\\eta)^{d/2}}\\mathrm{d}x}\\\\ &{\\qquad\\qquad\\qquad=C_{y_{k}}\\mathbb{E}[G(y_{k}+\\sqrt{\\eta}z_{1})^{1-1/\\kappa}\\mid y_{k}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $z_{1}\\sim\\mathcal{N}(0,I_{d})$ . Furthermore, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle C_{y_{k}}=\\frac{1}{(2\\pi\\eta)^{d/2}}\\int\\exp\\Big(-V(x)-\\frac{|x-y_{k}|^{2}}{2\\eta}\\Big)\\mathrm{d}x}}\\\\ {{\\displaystyle\\qquad=\\mathbb{E}[G(y_{k}+\\sqrt{\\eta}z_{1})^{-1/\\kappa}\\mid y_{k}].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}[G(x_{k+1})\\mid y_{k}]}}\\\\ &{=\\!\\!\\frac{\\mathbb{E}[G(y_{k}+\\sqrt{\\eta}z_{1})^{1-1/\\kappa}\\mid y_{k}]}{\\mathbb{E}[G(y_{k}+\\sqrt{\\eta}z_{1})^{-1/\\kappa}\\mid y_{k}]}}\\\\ &{\\le\\!\\!\\mathbb{E}[G(y_{k}+\\sqrt{\\eta}z_{1})\\mid y_{k}]^{1-1/\\kappa}\\mathbb{E}[G(y_{k}+\\sqrt{\\eta}z_{1})\\mid y_{k}]^{1/\\kappa}}&{\\quad\\mathrm{(Jensen's~Ine~}}\\\\ &{=\\!\\!\\mathbb{E}[G(y_{k}+\\sqrt{\\eta}z_{1})\\mid y_{k}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Recall $y_{k}=x_{k}+\\sqrt{\\eta}z_{2}$ where $z_{2}\\sim\\mathcal{N}(0,I_{d})$ is independent from $x_{k}$ . By the towering property of conditional expectation, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[G(x_{k+1})]\\leq\\mathbb{E}[G(x_{k}+\\sqrt{\\eta}z_{1}+\\sqrt{\\eta}z_{2})]}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\mathbb{E}[G(x_{k}+\\sqrt{2\\eta}z)],}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $z\\sim\\mathcal{N}(0,I_{d})$ is independent from $x_{k}$ , which completes the proof. ", "page_idx": 17}, {"type": "text", "text": "In order to provide a more refined control over $\\mathbb{E}[G(x_{k})]$ , we need additional assumptions on $V$ . In particular, when considering the generalized Cauchy density, we arrive at the following lemma. ", "page_idx": 17}, {"type": "text", "text": "Lemma 5. Suppose $(x_{k},y_{k})_{k}$ are the iterates of the Gaussian proximal sampler with step size \u03b7 and target density $\\pi^{X}\\propto\\exp(-V)$ satisfies ", "page_idx": 17}, {"type": "equation", "text": "$$\n|\\nabla V(x)|\\leq\\frac{(d+\\nu_{2})|x|}{1+|x|^{2}}\\quad a n d\\quad\\Delta V(x)\\leq\\frac{(d+\\nu_{2})^{2}}{1+|x|^{2}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for all $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ . Let $\\begin{array}{r}{G(x)=\\exp(\\kappa V(x))\\,w i t h\\,\\kappa\\geq1\\vee\\frac{2}{d+\\nu_{2}}}\\end{array}$ . Then, for every $k\\geq0$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[G(x_{k+1})]^{\\frac{2}{\\kappa(d+\\nu_{2})}}\\leq\\mathbb{E}[G(x_{k})]^{\\frac{2}{\\kappa(d+\\nu_{2})}}+4\\kappa\\eta k(d+\\nu_{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. From Lemma 4, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}[G(x_{k+1})]\\le\\mathbb{E}[G(x_{k}+\\sqrt{2\\eta}z)],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $z\\sim\\mathcal{N}(0,I_{d})$ is independent from $x_{k}$ . Consider the Brownian motion starting at $x_{k}$ , denoted by $Z_{t}=B_{t}+x_{k}$ where $\\left(B_{t}\\right)$ is a standard Brownian motion in $\\mathbb{R}^{d}$ . Notice that the generator for the ", "page_idx": 17}, {"type": "text", "text": "process $\\mathrm{d}Z_{t}=\\mathrm{d}B_{t}$ is $\\begin{array}{r}{\\mathcal{L}=\\frac{1}{2}\\Delta}\\end{array}$ . Therefore, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathrm{d}\\mathbb{E}[G(Z_{t})]}{\\mathrm{d}t}=\\mathbb{E}[\\mathcal{L}G(Z_{t})]}\\\\ &{\\qquad\\qquad=\\frac{\\kappa}{2}\\mathbb{E}\\big[G(Z_{t})\\big(\\kappa|\\nabla V|^{2}+\\Delta V\\big)\\big]}\\\\ &{\\qquad\\qquad\\leq\\frac{\\kappa(\\kappa+1)}{2}\\mathbb{E}\\Big[G(Z_{t})\\frac{(d+\\nu_{2})^{2}}{1+|Z_{t}|^{2}}\\Big]}\\\\ &{\\qquad\\qquad\\leq2\\kappa^{2}(d+\\nu_{2})^{2}\\mathbb{E}\\big[G(Z_{t})^{1-\\frac{2}{\\kappa(d+\\nu_{2})}}\\big]}\\\\ &{\\qquad\\qquad\\leq2\\kappa^{2}(d+\\nu_{2})^{2}\\mathbb{E}[G(Z_{t})]^{1-\\frac{2}{\\kappa(d+\\nu_{2})}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Integrating the above inequality yields ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}[G(Z_{t})]^{\\frac{2}{\\kappa(d+\\nu_{2})}}\\leq\\mathbb{E}[G(Z_{0})]^{\\frac{2}{\\kappa(d+\\nu_{2})}}+2\\kappa(d+\\nu_{2})t.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The proof is complete by noticing that $Z_{0}=x_{k}$ and $Z_{t}=x_{k}+{\\sqrt{2\\eta}}z$ for $t=2\\eta$ . ", "page_idx": 18}, {"type": "text", "text": "Proof of Theorem 2. Notice that the statements of Lemmas 3 and 5 are virtually the same by changing $t$ to $2k\\eta$ . Using this fact, the rest of the proof follows exactly the same as the proof of Theorem 1. ", "page_idx": 18}, {"type": "text", "text": "B Proofs for the Stable Proximal Sampler ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "B.1 Preliminaries ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we introduce additional preliminaries on the isotropic $\\alpha$ -stable process, the fractional Poincar\u00e9-type inequalities, the fractional Laplacian and the fractional heat flow. ", "page_idx": 18}, {"type": "text", "text": "The L\u00e9vy process is a stochastic process that is stochastically continuous with independent and stationary increments. Due to the stochastic continuity, the L\u00e9vy processes have c\u00e0dl\u00e0g trajectories, which allows jumps in the paths. A L\u00e9vy process $Y_{t}$ is uniquely determined by a triple $(b,A,\\nu)$ through the following L\u00e9vy-Khinchine formula: for all $t\\geq0$ and $\\xi\\in\\mathbb{R}^{d}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[e^{i\\langle\\xi,Y_{t}\\rangle}\\big]=\\exp\\bigg(t\\big(i\\langle b,\\xi\\rangle-\\xi^{\\top}A\\xi+\\int_{\\mathbb{R}^{d}\\setminus\\{0\\}}\\big(e^{i\\langle\\xi,y\\rangle}-1-i\\langle\\xi,y\\rangle1_{\\{|y|\\leq1\\}}(y)\\big)\\nu(\\mathrm{d}y)\\big)\\bigg),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $b\\in\\mathbf{R}^{d}$ is a drift vector. $A\\in\\mathbb{R}^{d\\times d}$ is the covariance matrix of the Brownian motion in the L\u00e9vy-It\u00f4 decomposition[App09, Thereom 2.4.16] and $\\nu$ is the L\u00e9vy measure related to the jump parts in the L\u00e9vy-It\u00f4 decomposition. ", "page_idx": 18}, {"type": "text", "text": "The rotationally invariant(isotropic) stable process is a special case for the Lev\u00b4y process when $b=0$ , $A=0$ and $\\nu$ is the measure given by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\nu(\\ensuremath{\\mathrm{d}}y)=c_{d,\\alpha}|y|^{-(d+\\alpha)},\\quad c_{d,\\alpha}=2^{\\alpha}\\Gamma((d+\\alpha)/2)/(\\pi^{d/2}|\\Gamma(-\\alpha/2)|).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Based on the L\u00e9vy-Khinchine formula (4), if we initialize the process at $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , its characteristic function is given by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x}e^{i\\langle\\xi,X_{t}^{(\\alpha)}-x\\rangle}=e^{-t|\\xi|^{\\alpha}},\\qquad x,\\xi\\in\\mathbb{R}^{d},\\;t\\geq0.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The index of stability $\\alpha\\in(0,2]$ determines the tail-heaviness of the densities: the smaller is $\\alpha$ , the heavier is the tail. The parameter $t$ in (6) measures the spread of $X_{t}$ around the center. When $\\alpha=2$ , the stable process pertains to the Brownian motion running with a time clock twice as fast as the standard one and hence it has continuous paths. When $\\alpha\\in(0,2)$ , the stable process paths contain discontinuities, which are often referred as jumps. At each fixed time, unlike the Brownian motion, the $\\alpha$ -stable process density only has a finite $p^{\\mathrm{th}}$ -moment for $p<\\alpha$ , i.e. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}[|X_{1}^{(\\alpha)}|^{p}]=\\left\\{{+\\infty\\qquad\\qquad p\\in[\\alpha,+\\infty),\\alpha\\in(0,2),}\\right.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "When $d=1$ , the fractional absolute moment formula for $m_{p}^{(\\alpha)}$ can be derived explicitly, see [Nol20, Chapter 3.7]. When $d>1$ , the explicit formula for $m_{p}^{(\\alpha)}$ is only known in some special cases. For example, when $\\alpha=1$ , m(p1)= \u0393((d+\u0393(pd)//22))\u0393\u0393(((11/2\u2212)p)/2)for all p < 1. Another good property of \u03b1-stable process is the self-similarity. By examining the characteristic functions, it is easy to verify that the isotropic $\\alpha$ -stable process is self-similar with the Hurst index $1/\\alpha$ , i.e. $X_{a t}^{(\\alpha)}$ and $\\bar{a}^{1/\\alpha}X_{t}^{(\\alpha)}$ have the same distribution. Or equivalently, $p_{t}^{(\\alpha)}(x)=t^{-\\frac{d}{\\alpha}}p_{1}^{(\\alpha)}(t^{-\\frac{1}{\\alpha}}x)$ for all $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ and $t>0$ . ", "page_idx": 19}, {"type": "text", "text": "The fractional Laplacian operator in $\\mathbb{R}^{d}$ of order $\\alpha$ is denoted by $-(-\\Delta)^{\\alpha/2}$ for $\\alpha\\,\\in\\,(0,2]$ . It was introduced as a non-local generalization of the Laplacian operator to model various physical phenomenons. In [Kwa17], ten equivalent definitions of the fractional Laplacian operator are introduced. Here we recall two of them: ", "page_idx": 19}, {"type": "text", "text": "(a) Distributional definition: For all Schwartz functions $\\phi$ defined on $\\mathbb{R}^{d}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\int_{\\mathbb{R}^{d}}-(-\\Delta)^{\\alpha/2}f(y)\\phi(y)\\mathrm{d}y=\\int_{\\mathbb{R}^{d}}f(x)\\left(-(-\\Delta)^{\\alpha/2}\\phi(x)\\right)d x.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "(b) Singular integral definition: For a limit in the space $L^{p}(\\mathbb{R}^{d}),p\\in[1,\\infty)$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n-(-\\Delta)^{\\alpha/2}f(x)=\\operatorname*{lim}_{r\\to0^{+}}\\frac{2^{\\alpha}\\Gamma(\\frac{d+\\alpha}{2})}{\\pi^{d/2}|\\Gamma(-\\frac{\\alpha}{2})|}\\int_{\\mathbb{R}^{d}\\backslash B_{r}}\\frac{f(x+z)-f(x)}{|z|^{d+\\alpha}}\\mathrm{d}z.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $B_{r}$ is the unit ball with radius $r$ centered at the origin. ", "page_idx": 19}, {"type": "text", "text": "The fractional Laplacian can be understood as the infinitesimal generator of the stable Lev\u00b4y process. More explicitly, the semigroup defined by the transition probability $p_{t}^{(\\alpha)}$ in (2) has the infinitesimal generator $-(-\\Delta)^{\\alpha/2}$ , i.e. the density function $p_{t}^{(\\alpha)}$ satisfies the following equation in the sense of distribution, [BHJ08]: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\partial_{t}p_{t}^{(\\alpha)}(x)=-(-\\Delta)^{\\alpha/2}p_{t}^{(\\alpha)}(x).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "(7) is usually referred as the $\\alpha$ -fractional heat flow. When $\\alpha\\,=\\,2$ , $-(-\\Delta)^{\\alpha/2}$ is the Laplacian operator and (7) becomes the heat flow. ", "page_idx": 19}, {"type": "text", "text": "Proposition 2 (From FPI to PI). When $\\vartheta\\,\\rightarrow\\,2^{-}$ , the $\\vartheta$ -FPI reduces to the classical Poincar\u00e9 inequality with Dirichlet form $\\begin{array}{r}{\\mathcal{E}_{\\mu}(\\phi)=\\int|\\nabla\\phi(x)|^{2}{\\mathrm{d}}x}\\end{array}$ for any smooth bounded $\\phi:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d}$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. It suffices to prove that $\\mathcal{E}_{\\mu}^{(\\vartheta)}(\\phi)$ converges to $\\mathcal{E}_{\\mu}(\\phi)$ as $\\vartheta\\rightarrow2^{-}$ for any smooth function $\\phi$ Recall the definition of $\\mathcal{E}_{\\mu}^{(\\vartheta)}(\\phi)$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{E}_{\\mu}^{(\\vartheta)}(\\phi):=c_{d,\\vartheta}\\!\\int\\!\\!\\int_{\\{x\\neq y\\}}\\!\\frac{(\\phi(x)-\\phi(y))^{2}}{|x-y|^{(d+\\vartheta)}}\\mathrm{d}x\\mu(y)\\mathrm{d}y\\quad\\mathrm{~with~}\\quad c_{d,\\vartheta}=\\frac{2^{\\vartheta}\\Gamma((d+\\vartheta)/2)}{\\pi^{d/2}|\\Gamma(-\\vartheta/2)|},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $c_{d,\\vartheta}\\,=\\,\\mathcal{O}(2-\\vartheta)$ as $\\vartheta\\rightarrow2^{-}$ . Now we rewrite the inside integral in $\\mathcal{E}_{\\mu}^{(\\vartheta)}(\\phi)$ and split the integral region into a centered unit ball, denoted as $B_{1}$ , and its complement: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int_{x\\neq y}\\frac{(\\phi(x)-\\phi(y))^{2}}{|x-y|^{(d+\\vartheta)}}\\mathrm{d}x=\\int_{z\\neq0}\\frac{(\\phi(y+z)-\\phi(y))^{2}}{|z|^{(d+\\vartheta)}}\\mathrm{d}z}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\underbrace{\\int_{B_{1}}\\frac{\\left(\\phi(y+z)-\\phi(y)\\right)^{2}}{|z|^{(d+\\vartheta)}}\\mathrm{d}z}_{I_{1}}+\\underbrace{\\int_{\\mathbb{R}^{d}\\setminus B_{1}}\\frac{(\\phi(y+z)-\\phi(y))^{2}}{|z|^{(d+\\vartheta)}}\\mathrm{d}z}_{I_{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For $I_{2}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\nI_{2}\\leq4\\left\\|\\phi\\right\\|_{\\infty}^{2}\\int_{\\mathbb{R}^{d}\\setminus B_{1}}\\frac{1}{|z|^{d+\\vartheta}}\\mathrm{d}z=\\frac{4\\left\\|\\phi\\right\\|_{\\infty}^{2}d\\pi^{\\frac{d}{2}}}{\\Gamma(\\frac{d}{2}+1)}\\int_{1}^{\\infty}r^{-\\vartheta+1}\\mathrm{d}r=\\frac{4\\left\\|\\phi\\right\\|_{\\infty}^{2}d\\pi^{\\frac{d}{2}}}{\\vartheta\\Gamma(\\frac{d}{2}+1)}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "As a result, the term in $\\mathcal{E}_{\\mu}^{(\\vartheta)}(\\phi)$ that is induced by $I_{2}$ satisfies ", "page_idx": 19}, {"type": "equation", "text": "$$\nc_{d,\\vartheta}\\int_{\\mathbb{R}^{d}}I_{2}\\mu(y)\\mathrm{d}y\\leq c_{d,\\vartheta}\\frac{4\\left\\|\\phi\\right\\|_{\\infty}^{2}d\\pi^{\\frac{d}{2}}}{\\vartheta\\Gamma(\\frac{d}{2}+1)}\\rightarrow0\\quad\\mathrm{as}\\ \\vartheta\\rightarrow2^{-}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For $I_{1}$ , we have when $\\vartheta>1$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad I_{1}-\\int_{B_{1}}\\frac{\\lvert\\langle\\nabla\\phi(y),z\\rangle\\rvert^{2}}{|z|^{d+\\phi}}\\mathrm{d}z}\\\\ &{=\\int_{B_{1}}\\frac{(\\phi(y+z)-\\phi(y)-\\langle\\nabla\\phi(y),z\\rangle)\\left(\\phi(y+z)-\\phi(y)+\\langle\\nabla\\phi(y),z\\rangle\\right)}{|z|^{d+\\theta}}\\mathrm{d}z}\\\\ &{\\leq\\|\\phi\\|_{C^{2}(\\mathbb{R}^{d})}\\,\\|\\phi\\|_{C^{1}({\\mathbb{R}^{d}})}\\int_{B_{1}}|z|^{-(d+\\vartheta-3)}\\mathrm{d}z}\\\\ &{=\\|\\phi\\|_{C^{2}(\\mathbb{R}^{d})}\\,\\|\\phi\\|_{C^{1}({\\mathbb{R}^{d}})}\\,\\frac{d\\pi^{\\frac{d}{2}}}{\\Gamma(\\frac{d}{2}+1)}\\int_{0}^{1}r^{\\theta-2}\\mathrm{d}r}\\\\ &{=\\|\\phi\\|_{C^{2}(\\mathbb{R}^{d})}\\,\\|\\phi\\|_{C^{1}(\\mathbb{R}^{d})}\\,\\frac{d\\pi^{\\frac{d}{2}}}{(\\vartheta-1)\\Gamma(\\frac{d}{2}+1)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\begin{array}{r}{\\|\\phi\\|_{C^{i}(\\mathbb{R}^{d})}:=\\operatorname*{sup}_{x\\in\\mathbb{R}^{d}}|\\phi^{(i)}(x)|}\\end{array}$ for $i=1,2$ . As a result, the term in $\\mathcal{E}_{\\mu}^{(\\vartheta)}(\\phi)$ that is induced by $I_{1}$ satisfies ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\because\\ d,\\vartheta\\int_{\\mathbb{R}^{d}}\\big(I_{2}-\\int_{B_{1}}\\frac{|\\langle\\nabla\\phi(y),z\\rangle|^{2}}{|z|^{d+\\vartheta}}\\mathrm{d}z\\big)\\mu(y)\\mathrm{d}y\\leq c_{d,\\vartheta}\\frac{\\|\\phi\\|_{C^{2}(\\mathbb{R}^{d})}\\,\\|\\phi\\|_{C^{1}(\\mathbb{R}^{d})}\\,d\\pi^{\\frac{d}{2}}}{(\\vartheta-1)\\Gamma\\big(\\frac{d}{2}+1\\big)}\\to0\\quad\\mathrm{as}\\,\\vartheta\\to2^{-}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore we have $\\begin{array}{r}{\\mathcal{E}_{\\mu}^{(\\vartheta)}(\\phi)\\rightarrow c_{d,\\vartheta}\\int_{\\mathbb{R}^{d}}\\int_{B_{1}}\\frac{|\\langle\\nabla\\phi(y),z\\rangle|^{2}}{|z|^{d+\\vartheta}}\\mu(y)\\mathrm{d}z\\mathrm{d}y}\\end{array}$ as $\\vartheta\\rightarrow2^{-}$ . Last, we prove the limit is equivalent to $2\\mathcal{E}_{\\mu}(\\phi)$ . For $i\\neq j$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\int_{B_{1}}\\partial_{i}\\phi(y)\\partial_{j}\\phi(y)z_{i}z_{j}\\mathrm{d}z=-\\int_{B_{1}}\\partial_{i}\\phi(y)\\partial_{j}\\phi(y)\\tilde{z}_{i}\\tilde{z}_{j}\\mathrm{d}\\tilde{z},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\tilde{z}_{k}=z_{k}$ for all $k\\neq j$ and $\\tilde{z}_{j}=-z_{j}$ . Therefore, $\\begin{array}{r}{\\int_{B_{1}}\\partial_{i}\\phi(y)\\partial_{j}\\phi(y)z_{i}z_{j}\\mathrm{d}z=0}\\end{array}$ . As a result, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\int_{B_{1}}\\frac{|\\langle\\nabla\\phi(y),z\\rangle|^{2}}{|z|^{d+\\vartheta}}\\mathrm d z=\\int_{B_{1}}\\frac{\\sum_{i=1}^{d}(\\partial_{i}\\phi(y))^{2}z_{i}^{2}}{|z|^{d+\\vartheta}}\\mathrm d z}}\\\\ &{}&{=\\sum_{i=1}^{d}(\\partial_{i}\\phi(y))^{2}\\frac{1}{d}\\int_{B_{1}}\\frac{|z|^{2}}{|z|^{d+\\vartheta}}\\mathrm d z}\\\\ &{}&{=|\\nabla\\phi(y)|^{2}\\frac{\\pi^{\\frac{d}{2}}}{(2-\\vartheta)\\Gamma\\big(\\frac{d}{2}+1\\big)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and the proof follows from $\\begin{array}{r}{c_{d,\\vartheta}\\frac{\\pi^{\\frac{d}{2}}}{(2-\\vartheta)\\Gamma(\\frac{d}{2}+1)}\\rightarrow2}\\end{array}$ as $\\vartheta\\rightarrow2^{-}$ ", "page_idx": 20}, {"type": "text", "text": "B.2 $\\chi^{2}$ convergence under FPI ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we study the decaying property of $\\chi^{2}$ -divergence from $\\rho_{k}^{X}$ to $\\pi^{X}$ , where $\\rho_{k}^{X}$ is the law of $x_{k}$ . In the following analysis, we denote $\\rho_{k}=\\rho_{k}^{X,Y}$ as the law of $(x_{k},y_{k})$ , $\\rho_{k}^{Y}$ the law of $y_{k}$ . We will analyze the two steps in the stable proximal sampler separately. ", "page_idx": 20}, {"type": "text", "text": "Step 1. In the following proposition, we study the decay of $\\chi^{2}$ -divergence in step 1. ", "page_idx": 20}, {"type": "text", "text": "Proposition 3. Assume that $\\pi^{X}$ satisfies the $\\alpha$ -FPI with parameter $C_{\\mathrm{FPI}(\\alpha)}$ , then for each $k\\geq0$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\chi^{2}(\\rho_{k}^{Y}|\\pi^{Y})\\leq\\exp\\left(-\\eta\\left(C_{\\sf F P I(\\alpha)}+\\eta\\right)^{-1}\\right)\\chi^{2}(\\rho_{k}^{X}|\\pi^{X}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof of Proposition 3. For the simplicity of notations, we will write $p^{(\\alpha)}$ and $p_{t}^{(\\alpha)}$ as $p$ and $p_{t}$ respectively in this proof. Since $x_{k}\\sim\\rho_{k}^{X}$ and $y_{k}|x_{k}\\sim p(\\eta;x,\\cdot)$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\rho_{k}^{Y}(y)=\\int_{{\\ensuremath{\\mathbb R}}^{d}}p(\\eta;x,y)\\rho_{k}^{X}(x)\\mathrm{d}x=\\int_{{\\ensuremath{\\mathbb R}}^{d}}\\rho_{k}^{X}(x)p_{\\eta}(y-x)\\mathrm{d}x=\\rho_{k}^{X}*p_{\\eta}(y).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, we can view $\\rho_{k}^{Y}$ as $\\rho_{k}^{X}$ evolving along the following factional heat flow ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\partial_{t}\\tilde{\\rho}_{t}=-(-\\Delta)^{\\frac{\\alpha}{2}}\\tilde{\\rho}_{t}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "That is if $\\tilde{\\rho}_{0}=\\rho_{k}^{X}$ , then $\\tilde{\\rho}_{\\eta}=\\rho_{k}^{Y}$ . Similarly, since $\\pi^{Y}=\\pi^{X}*p_{\\eta}$ , if $\\tilde{\\rho}_{0}=\\pi^{X}$ , then $\\tilde{\\rho}_{\\eta}=\\pi^{Y}$ . For any $t\\in[0,\\eta]$ , define $\\pi_{t}^{X}=\\pi^{X}*p_{t}$ and $\\rho_{t}^{X}=\\rho_{k}^{X}*p_{t}$ . The derivative of $\\phi$ -divergence from $\\rho_{t}^{X}$ to $\\pi_{t}^{X}$ can be calculated as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac{d}{d t}\\displaystyle\\int_{\\mathbb R^{d}}\\phi(\\frac{\\rho_{t}^{X}}{\\pi_{t}^{X}})\\pi_{t}^{X}\\,\\mathrm{d}x}\\\\ &{=\\displaystyle\\int_{\\mathbb R^{d}}\\partial_{t}\\pi_{t}^{X}\\phi(\\frac{\\rho_{t}^{X}}{\\pi_{t}^{X}})+\\phi^{\\prime}(\\frac{\\rho_{t}^{X}}{\\pi_{t}^{X}})\\left(\\partial_{t}\\rho_{t}^{X}-\\partial_{t}\\pi_{t}^{X}\\frac{\\rho_{t}^{X}}{\\pi_{t}^{X}}\\right)\\,\\mathrm{d}x}\\\\ &{=-\\displaystyle\\int_{\\mathbb R^{d}}\\phi(\\frac{\\rho_{t}^{X}}{\\pi_{t}^{X}})(-\\Delta)^{\\frac{\\alpha}{2}}\\pi_{t}^{X}\\,\\mathrm{d}x+\\displaystyle\\int_{\\mathbb R^{d}}\\phi^{\\prime}(\\frac{\\rho_{t}^{X}}{\\pi_{t}^{X}})\\left(\\frac{\\rho_{t}^{X}}{\\pi_{t}^{X}}(-\\Delta)^{\\frac{\\alpha}{2}}\\pi_{t}^{X}-(-\\Delta)^{\\frac{\\alpha}{2}}\\rho_{t}^{X}\\right)\\,\\mathrm{d}x}\\\\ &{=\\displaystyle\\int_{\\mathbb R^{d}}\\left[-\\frac{\\rho_{t}^{X}}{\\pi_{t}^{X}}(-\\Delta)^{\\frac{\\alpha}{2}}\\phi^{\\prime}(\\frac{\\rho_{t}^{X}}{\\pi_{t}^{X}})+(-\\Delta)^{\\frac{\\alpha}{2}}\\left(\\frac{\\rho_{t}^{X}}{\\pi_{t}^{X}}\\phi^{\\prime}(\\frac{\\rho_{t}^{X}}{\\pi_{t}^{X}})\\right)-(-\\Delta)^{\\frac{\\alpha}{2}}\\phi(\\frac{\\rho_{t}^{X}}{\\pi_{t}^{X}})\\right]\\pi_{t}^{X}\\,\\mathrm{d}x,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where in the second identity we used the distributional definition of the fractional Laplacian. Next according to the singular integral definition of fractional Laplacian, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n-(-\\Delta)^{\\frac{\\alpha}{2}}f(x):=c_{d,\\alpha}\\operatorname*{lim}_{r\\to0^{+}}\\int_{\\mathbb{R}^{d}\\backslash B_{r}}\\frac{f(x+z)-f(x)}{|z|^{d+\\alpha}}\\mathrm{d}z,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $B_{r}=\\{x\\in\\mathbb{R}^{d}:|x|\\leq r\\}$ and $c_{d,\\alpha}$ is given in (5). With (8), we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\cfrac{d}{d t}\\int_{\\mathbb{R}^{d}}\\phi(\\frac{\\rho_{t}^{X}}{\\pi_{t}^{X}})\\pi_{t}^{X}\\,\\mathrm{d}x}\\\\ &{=c_{d,\\alpha}\\displaystyle\\operatorname*{lim}_{r\\to0^{+}}\\int_{\\mathbb{R}^{d}}\\int_{\\mathbb{R}^{d}\\setminus B_{r}}\\frac{\\phi\\big(\\frac{\\rho_{t}^{X}(x+z)}{\\pi_{t}^{X}(x+z)}\\big)-\\phi\\big(\\frac{\\rho_{t}^{X}(x)}{\\pi_{t}^{X}(x)}\\big)-\\frac{\\rho_{t}^{X}(x+z)}{\\pi_{t}^{X}(x+z)}\\phi^{\\prime}\\big(\\frac{\\rho_{t}^{X}(x+z)}{\\pi_{t}^{X}(x+z)}\\big)+\\frac{\\rho_{t}^{X}(x)}{\\pi_{t}^{X}(x)}\\phi^{\\prime}\\big(\\frac{\\rho_{t}^{X}(x+z)}{\\pi_{t}^{X}(x+z)}\\big)}{\\big|z\\big|^{d+\\alpha}}\\mathrm{d}z\\pi_{t}^{X}(x)\\mathrm{d}z}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "When $\\begin{array}{r}{\\phi(r)=(r-1)^{2},\\int_{\\mathbb{R}^{d}}\\phi(\\frac{\\rho_{t}^{X}}{\\pi_{t}^{X}})\\pi_{t}^{X}\\mathrm{d}x=\\chi^{2}(\\rho_{t}^{X}|\\pi_{t}^{X})}\\end{array}$ and we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\chi^{2}(\\rho_{t}^{X}|\\pi_{t}^{X})=-c_{d,\\alpha}\\operatorname*{lim}_{r\\rightarrow0^{+}}\\int_{\\mathbb{R}^{d}}\\int_{\\mathbb{R}^{d}\\backslash B_{r}}\\frac{\\left(\\frac{\\rho_{t}^{X}(x+z)}{\\pi_{t}^{X}(x+z)}-\\frac{\\rho_{t}^{X}(x)}{\\pi_{t}^{X}(x)}\\right)^{2}}{|z|^{d+\\alpha}}\\mathrm{d}z\\pi_{t}^{X}\\mathrm{d}x:=-\\mathcal{E}_{\\pi_{t}^{X}}(\\frac{\\rho_{t}^{X}}{\\pi_{t}^{X}}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "According to $\\mathrm{[Cha04}$ , Theorem 23], $p_{t}$ satisfies $\\alpha$ -FPI with parameter $t$ for all $t\\in(0,\\eta]$ . Since $\\pi^{X}$ also satisfies the $\\alpha$ -FPI with parameter $C_{\\mathrm{FPI}(\\alpha)}$ , Lemma 6 implies that $\\overline{{\\pi}}_{t}^{X}=\\pi^{X}*p_{t}$ satisfies the $\\alpha$ -FPI with parameter $C_{\\mathrm{FPI}(\\alpha)}+\\eta$ for all $t\\in(0,\\eta]$ . Therefore we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\chi^{2}(\\rho_{t}^{X}|\\pi_{t}^{X})=-\\mathcal{E}_{\\pi_{t}^{X}}(\\frac{\\rho_{t}^{X}}{\\pi_{t}^{X}})\\leq-\\left(C_{\\mathtt{F P I}(\\alpha)}+\\eta\\right)^{-1}\\chi^{2}(\\rho_{t}^{X}|\\pi_{t}^{X}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Last, according to Gronwall\u2019s inequality we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\chi^{2}(\\rho_{k}^{Y}|\\pi^{Y})=\\chi^{2}(\\rho_{\\eta}^{X}|\\pi_{\\eta}^{X})\\leq\\exp\\left(-\\eta\\left(C_{\\mathrm{FPI}(\\alpha)}+\\eta\\right)^{-1}\\right)\\chi^{2}(\\rho_{k}^{X}|\\pi^{X}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Step 2. In this step, we study the decay of $\\chi^{2}$ -divergence in step 2. building on the work by [CCSW22]. According to the $\\mathrm{R}\\alpha\\mathrm{S}\\mathrm{O}$ , we have $\\begin{array}{r}{\\rho_{k+1}^{X}(x)=\\int_{\\mathbb{R}^{d}}\\pi^{X|Y}(x|y)\\rho_{k}^{Y}(y)\\mathrm{d}y}\\end{array}$ . Also notice that $\\begin{array}{r}{\\pi^{X}(x)=\\int_{\\mathbb{R}^{d}}\\pi^{X|Y}(x|y)\\pi^{Y}(y)\\mathrm{d}y}\\end{array}$ . According to the data processing inequalities, $\\chi^{2}$ divergence won\u2019t increase after step 2, i.e. $\\chi^{2}(\\rho_{k+1}^{X}|\\pi^{X})\\leq\\chi^{2}(\\rho_{k}^{Y}|\\pi^{Y})$ . ", "page_idx": 21}, {"type": "text", "text": "Combining our results in Step 1 and Step 2, we prove Theorem 3. ", "page_idx": 21}, {"type": "text", "text": "Lemma 6. Let $\\mu_{1},\\mu_{2}$ be two probability densities satisfying the $\\vartheta$ -FPI with parameters $C_{1},C_{2}$ respectively. Then $\\mu_{1}*\\mu_{2}$ satisfies the $\\vartheta$ -FPI with parameter $C_{1}+C_{2}$ . ", "page_idx": 21}, {"type": "text", "text": "Proof of Lemma $\\theta.$ . Let $X,Y$ be two independent random variables such that $X\\sim\\mu_{1}$ and $Y\\sim\\mu_{2}$ . Then $X+Y\\sim\\mu_{1}*\\mu_{2}$ . According to variance decomposition, we have for any function $\\phi$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname{Var}_{\\mu_{1}*\\mu_{2}}{\\left(\\phi\\right)}=\\operatorname{Var}\\left(\\phi(X+Y)\\right)=\\operatorname{\\mathbb{E}}\\left[\\operatorname{Var}\\left(\\phi(X+Y)|Y\\right)\\right]+\\operatorname{Var}\\left(\\mathbb{E}\\left[\\phi(X+Y)|Y\\right]\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since $X\\sim\\mu_{1}$ and $\\mu_{1}$ satisfies the $\\vartheta$ -FPI with parameter $C_{1}$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname{Var}\\left(\\phi(X+Y)|Y\\right)\\leq C_{1}c_{d,\\alpha}\\iint_{\\{z\\neq0\\}}{\\frac{\\left(\\phi(x+Y+z)-\\phi(x+Y)\\right)^{2}}{|z|^{(d+\\vartheta)}}}\\mathrm{d}z\\mu_{1}(x)\\mathrm{d}x,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "therefore we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\mathrm{Var}\\left(\\phi(X+Y)|Y\\right)\\right]}\\\\ &{\\leq\\!C_{1}c_{d,\\alpha}\\displaystyle\\int\\!\\!\\!\\int_{\\{z\\neq0\\}}\\frac{\\left(\\phi(x+y+z)-\\phi(x+y)\\right)^{2}}{|z|^{(d+\\vartheta)}}\\mathrm{d}z\\mu_{1}(x)\\mathrm{d}x\\mu_{2}(y)\\mathrm{d}y.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since $Y\\sim\\mu_{2}$ and $\\mu_{2}$ satisfies the $\\vartheta$ -FPI with parameter $C_{2}$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathrm{Var}\\left(\\mathbb E\\left[\\phi(X+Y)\\vert Y\\right]\\right)}\\\\ &{\\le C_{2}c_{d,\\alpha}\\displaystyle\\int\\!\\!\\int_{\\{z\\neq0\\}}\\frac{\\left(\\int\\phi(x+y+z)\\mu_{1}(x)\\mathrm{d}x-\\int\\phi(x+y)\\mu_{1}(x)\\mathrm{d}x\\right)^{2}}{\\vert z\\vert^{(d+\\vartheta)}}\\mathrm{d}z\\mu_{2}(y)\\mathrm{d}y}\\\\ &{\\le C_{2}c_{d,\\alpha}\\displaystyle\\int\\!\\!\\int_{\\{z\\neq0\\}}\\int\\frac{\\left(\\phi(x+y+z)-\\phi(x+y)\\right)^{2}}{\\vert z\\vert^{(d+\\vartheta)}}\\mu_{1}(x)\\mathrm{d}x\\mathrm{d}z\\mu_{2}(y)\\mathrm{d}y}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the last inequality follows from Jensen\u2019s inequality. Combining (9) and (10), we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Var}_{\\mu_{1}+\\mu_{2}}(\\phi)\\leq C_{1}c_{d,\\alpha}\\iint_{\\{z\\neq0\\}}\\frac{\\left(\\phi(x+y+z)-\\phi(x+y)\\right)^{2}}{\\left\\vert z\\right\\vert^{(d+\\theta)}}\\mathrm{d}z\\mu_{1}(x)\\mathrm{d}x\\mu_{2}(y)\\mathrm{d}y}\\\\ &{\\qquad\\qquad\\qquad+C_{2}c_{d,\\alpha}\\iint_{\\{z\\neq0\\}}\\int\\frac{\\left(\\phi(x+y+z)-\\phi(x+y)\\right)^{2}}{\\left\\vert z\\right\\vert^{(d+\\theta)}}\\mu_{1}(x)\\mathrm{d}x\\mathrm{d}z\\mu_{2}(y)\\mathrm{d}y}\\\\ &{\\qquad\\qquad\\leq(C_{1}+C_{2})\\,c_{d,\\alpha}\\iiint_{\\{z\\neq0\\}}\\frac{\\left(\\phi(x+y+z)-\\phi(x+y)\\right)^{2}}{\\left\\vert z\\right\\vert^{(d+\\theta)}}\\mathrm{d}z\\mu_{1}(x)\\mathrm{d}x\\mu_{2}(y)\\mathrm{d}y}\\\\ &{\\qquad=(C_{1}+C_{2})\\,c_{d,\\alpha}\\iint_{\\{z\\neq0\\}}\\frac{\\left(\\phi(u+z)-\\phi(u)\\right)^{2}}{\\left\\vert z\\right\\vert^{(d+\\theta)}}\\mathrm{d}z\\mu_{1}*\\mu_{2}(u)\\mathrm{d}u}\\\\ &{\\qquad=(C_{1}+C_{2})\\,\\xi\\mu_{1}*\\mu_{2}(\\phi),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the second inequality follows from Fatou\u2019s lemma. ", "page_idx": 22}, {"type": "text", "text": "B.3 Implementation of the Stable Proximal Sampler ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section we discuss the implementation of the $\\mathrm{R}\\alpha\\mathrm{SO}$ step in our stable proximal sampler. We introduce an exact implementation of the $\\mathrm{R}\\alpha\\mathrm{SO}$ step without optimizing the target potential and the proofs for Corollary 3 and Proposition 1. ", "page_idx": 22}, {"type": "text", "text": "Rejection sampling without optimization. Suppose a uniform lower bound of the target potential is known, i.e. there is a constant $\\displaystyle\\mathit{C_{\\mathrm{Low}}}$ such that $\\operatorname*{inf}_{x\\in\\mathbb{R}^{d}}V(x)\\geq C_{\\mathrm{Low}}>-\\infty$ , $\\mathrm{R}\\alpha\\mathrm{S}\\mathrm{O}$ at each step can be implemented exactly via a rejection sampler with proposals $\\tilde{x}_{k+1}$ following $p_{\\eta}^{(\\alpha)}(\\cdot-y_{k})$ and the acceptance probability $\\exp(-\\bar{V}(\\tilde{x}_{k+1})+\\bar{C}_{\\mathrm{Low}})$ . Then the expected number of rejections, $N$ , satisfies ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{V}=\\big(\\int_{\\mathbb{R}^{d}}e^{-V(x)+C_{\\mathrm{low}}}p(\\eta;x,y_{k})\\mathrm{d}x\\big)^{-1}\\quad\\mathrm{and}\\quad\\log N=-C_{\\mathrm{Low}}-\\log\\big(\\int_{\\mathbb{R}^{d}}e^{-V(x)}p^{(\\alpha)}(\\eta;x,y_{k})\\mathrm{d}x\\big)^{-1}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Without loss of generality, we assume $x^{*}=0$ , which always hold if we translate the potential $V$ by $V(0)$ . Then we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log N\\leq-C_{\\mathrm{Low}}+\\displaystyle\\int_{\\mathbb R^{d}}\\big(V(x)-V(0)\\big)p^{(\\alpha)}(\\eta;x,y_{k})\\mathrm{d}x}\\\\ &{\\qquad\\leq-C_{\\mathrm{Low}}+L\\displaystyle\\int_{\\mathbb R^{d}}\\big|x+y_{k}\\big|^{\\beta}p_{\\eta}^{(\\alpha)}(x)\\mathrm{d}x}\\\\ &{\\qquad\\leq-C_{\\mathrm{Low}}+L\\mathbb{E}_{X\\sim\\pi^{X}}\\big[|X|^{\\beta}\\big]+L\\eta^{\\beta}d^{\\frac{\\beta}{2}}+L\\mathbb{E}_{X\\sim\\pi^{X}}\\big[|X|^{2\\beta}\\big]^{\\frac{1}{2}}\\chi^{2}\\big(\\rho_{0}^{X}|\\pi^{X}\\big)^{\\frac{1}{2}}+\\frac{\\Gamma\\big(\\frac{d+1}{2}\\big)\\Gamma\\big(\\frac{1-\\beta}{2}\\big)L}{\\Gamma\\big(\\frac{d+1-\\beta}{2}\\big)\\pi^{\\frac{1}{2}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the second inequality follows from Assumption 3 and the last inequality follows from the proof of Corollary 3. With the above estimation, we can pick $\\bar{\\eta}~=~\\bar{\\Theta(C_{\\mathrm{Low}}^{\\frac{1}{\\beta}}d^{-\\frac{1}{2}}L^{-\\frac{1}{\\beta}})}$ and the expected number of rejections satisfies $\\begin{array}{r l r}{\\log N}&{{}=}&{\\mathcal{O}(C_{\\mathrm{Low}}\\;\\;+\\;\\;\\stackrel{..}{L}M)}\\end{array}$ with $M=\\mathbb{E}_{\\pi^{X}}[|X|^{\\beta}]+\\chi^{2}(\\rho_{0}^{X}|\\pi^{X})\\mathbb{E}_{\\pi^{X}}[|X|^{2\\beta}]^{\\frac{1}{2}}$ . ", "page_idx": 23}, {"type": "text", "text": "Proof of Corollary 3. The expected number of iterations conditioned on $y_{k}$ in the rejection sampling is ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{N=\\left(\\int_{\\mathbb{R}^{d}}e^{-V(x)+V(x^{*})}p^{(\\alpha)}(\\eta;x,y_{k})\\mathrm{d}x\\right)^{-1}}}\\\\ {\\mathrm{and}}&{\\log N=-V(x^{*})-\\log\\big(\\int_{\\mathbb{R}^{d}}e^{-V(x)}p^{(\\alpha)}(\\eta;x,y_{k})\\mathrm{d}x\\big)}\\\\ &{\\qquad\\le\\int_{\\mathbb{R}^{d}}\\big(V(x)-V(x^{*})\\big)p^{(\\alpha)}(\\eta;x,y_{k})\\mathrm{d}x}\\\\ &{=\\int_{\\mathbb{R}^{d}}\\big(V(x+y_{k})-V(x^{*})\\big)p_{\\eta}^{(\\alpha)}(x)\\mathrm{d}x.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "WLOG, assume $x^{*}=0$ . Since $V$ satisfies Assumption 3, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\log N\\leq L\\int_{\\mathbb{R}^{d}}\\left|x+y_{k}\\right|^{\\beta}p_{\\eta}^{(\\alpha)}(x)\\mathrm{d}x=\\frac{L\\Gamma(\\frac{d+1}{2})}{\\pi^{\\frac{d+1}{2}}}\\eta\\int_{\\mathbb{R}^{d}}\\left|x+y_{k}\\right|^{\\beta}(\\left|x\\right|^{2}+\\eta^{2})^{-\\frac{d+1}{2}}\\mathrm{d}x}\\\\ {\\leq L|y_{k}|^{\\beta}+\\frac{L\\Gamma(\\frac{d+1}{2})}{\\pi^{\\frac{d+1}{2}}}\\eta\\int_{\\mathbb{R}^{d}}\\left|x\\right|^{\\beta}(\\left|x\\right|^{2}+\\eta^{2})^{-\\frac{d+1}{2}}\\mathrm{d}x}\\\\ {\\leq L|y_{k}|^{\\beta}+\\frac{L\\Gamma(\\frac{d+1}{2})}{\\pi^{\\frac{d+1}{2}}}\\eta\\int_{\\mathbb{R}^{d}}(\\left|x\\right|^{2}+\\eta^{2})^{-\\frac{d+1-\\beta}{2}}\\mathrm{d}x}\\\\ {=L|y_{k}|^{\\beta}+\\frac{\\Gamma(\\frac{d+1}{2})\\Gamma(\\frac{1-\\beta}{2})L}{\\Gamma(\\frac{d+1-\\beta}{2})\\pi^{\\frac{1}{2}}}\\eta^{\\beta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, when $\\eta=\\Theta(d^{-\\frac{1}{2}}L^{-\\frac{1}{\\beta}})$ , the expected number of rejections $_\\mathrm{N}$ is of order $\\mathbb{E}[\\exp(L|y_{k}|^{\\beta}]$ . Since $\\pi^{X}$ satisfies a 1-FPI with parameter $C_{\\mathrm{FPI(1)}}$ , according to [Cha04], $p_{t}$ satisfies the 1-FPI with parameter $\\eta$ for any $t\\,\\in\\,(0,\\eta)$ . Last it follows from Theorem 9 that for any $\\eta>0$ , to achieve a $\\varepsilon$ -accuracy in $\\chi^{2}$ divergence, we need to perform the stable proximal sampler $K$ steps with ", "page_idx": 23}, {"type": "equation", "text": "$$\nK\\geq\\big(C_{\\mathrm{FPI}(1)}\\eta^{-1}+1\\big)\\log\\bigg(\\frac{\\chi^{2}\\big(\\rho_{0}^{X}|\\pi^{X}\\big)}{\\varepsilon}\\bigg)=\\mathcal O\\big(C_{\\mathrm{FPI}(1)}d^{\\frac12}L^{\\frac1\\beta}\\log\\big(\\frac{\\chi^{2}\\big(\\rho_{0}^{X}|\\pi^{X}\\big)}{\\varepsilon}\\big)\\big).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof of Proposition $^{\\,l}$ . For all $k\\geq0$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{TV}(\\tilde{\\rho}_{k+1}^{X},\\rho_{k+1}^{X})=\\mathrm{TV}\\big(\\displaystyle\\int\\tilde{\\rho}_{k+1}^{X|Y}(\\cdot|y)\\tilde{\\rho}_{k}^{Y}(y)\\mathrm{d}y,\\displaystyle\\int\\rho_{k+1}^{X|Y}(\\cdot|y)\\rho_{k}^{Y}(y)\\mathrm{d}y\\big)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathrm{TV}\\big(\\displaystyle\\int\\tilde{\\rho}_{k+1}^{X|Y}(\\cdot|y)\\tilde{\\rho}_{k}^{Y}(y)\\mathrm{d}y,\\displaystyle\\int\\rho_{k+1}^{X|Y}(\\cdot|y)\\tilde{\\rho}_{k}^{Y}(y)\\mathrm{d}y\\big)}\\\\ &{\\qquad\\qquad\\qquad+\\mathrm{TV}\\big(\\displaystyle\\int\\rho_{k+1}^{X|Y}(\\cdot|y)\\tilde{\\rho}_{k}^{Y}(y)\\mathrm{d}y,\\displaystyle\\int\\rho_{k+1}^{X|Y}(\\cdot|y)\\rho_{k}^{Y}(y)\\mathrm{d}y\\big)}\\\\ &{\\qquad\\qquad\\leq\\mathbb{E}_{\\tilde{\\rho}_{k}^{Y}}[\\mathrm{TV}(\\tilde{\\rho}_{k+1}^{X|Y}(\\cdot,y)],\\rho_{k+1}^{X|Y}(\\cdot|y))+\\mathrm{TV}(\\tilde{\\rho}_{k}^{Y},\\rho_{k}^{Y})}\\\\ &{\\qquad\\qquad\\leq\\varepsilon_{\\mathrm{TV}}+\\mathrm{TV}(\\tilde{\\rho}_{k}^{X},\\rho_{k}^{X}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the last two inequalities follow from the data processing inequality. Therefore, $\\mathrm{TV}(\\tilde{\\rho}_{k}^{X},\\rho_{k}^{X})\\leq$ $k\\varepsilon_{\\mathrm{TV}}+\\mathrm{TV}(\\tilde{\\rho}_{0}^{X},\\rho_{0}^{X})$ for all $k\\geq1$ . ", "page_idx": 23}, {"type": "text", "text": "Next, the iteration complexity of Algorithm 2 with an inexact $\\mathrm{R}\\alpha\\mathrm{S}\\mathrm{O}$ can be obtained from Proposition 1. Since $\\tilde{\\rho}_{0}^{X}=\\rho_{0}^{X}$ , according to Pinsker\u2019s inequality, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{TV}(\\tilde{\\rho}_{k}^{X},\\pi^{X})\\leq\\mathrm{TV}(\\tilde{\\rho}_{k}^{X},\\rho_{k}^{X})+\\mathrm{TV}(\\rho_{k}^{X},\\pi^{X})\\leq\\mathrm{TV}(\\tilde{\\rho}_{k}^{X},\\rho_{k}^{X})+\\sqrt{\\chi^{2}(\\rho_{k}^{X}\\vert\\pi^{X})/2}}\\\\ &{\\qquad\\qquad\\leq k\\varepsilon_{\\mathrm{TV}}+\\sqrt{\\exp(-k\\eta(C_{\\mathrm{FPI}(\\alpha)}+\\eta)^{-1})\\chi^{2}(\\tilde{\\rho}_{0}^{X}\\vert\\pi^{X})/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For any $\\varepsilon>0$ and any $K$ satisfies ", "page_idx": 24}, {"type": "equation", "text": "$$\nK\\geq\\big(C_{\\mathrm{FPI}(\\alpha)}\\eta^{-1}+1\\big)\\ln\\big(2\\chi^{2}(\\tilde{\\rho}_{0}^{X}|\\pi^{X})/\\varepsilon^{2}\\big),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "if the $\\mathrm{R}\\alpha\\mathrm{S}\\mathrm{O}$ can be implemented inexactly with $\\begin{array}{r}{\\varepsilon_{\\mathrm{TV}}~\\leq~\\frac{\\varepsilon}{2K}}\\end{array}$ , the density of the $\\boldsymbol{K}^{\\mathrm{th}}$ iterate of Algorithm 2 is $\\varepsilon$ -close to the target in the total variation distance, i.e. $\\mathrm{TV}(\\tilde{\\rho}_{X}^{K},\\pi^{X})\\leq\\varepsilon$ . \u53e3 ", "page_idx": 24}, {"type": "text", "text": "B.4 Convergence under Weak Fractional Poincar\u00e9 Inequality ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Our main result for Algorithm 2 in Theorem 3 is proved under the assumption the target satisfying $\\alpha$ -FPI. Furthermore, for the rejection-sampling based implementation of the $\\mathrm{R}\\alpha\\mathrm{SO}$ in Algorithm 3, the parameter $\\alpha$ is set to be 1. In order to use Theorem 3 for the case of generalized Cauchy targets, one has to check if the $\\alpha$ -FPI is satisfied or not, which depends on the degrees of freedom parameter $\\nu$ of the generalized Cauchy desity. Specifically, when $\\nu\\geq1$ , 1-FPI is satisfied and we hence have Corollary 5, part (i) based on Theorem 3. When $\\nu\\in(0,1)$ , 1-FPI is not satisfied and hence Theorem 3 no longer applies. ", "page_idx": 24}, {"type": "text", "text": "To tackle this issue, we now introduce a generalization of Theorem 3 to the case when the target satisfies a weak version of Fractioanl Poincar\u00e9 inequality (wFPI) and provide convergence guarantees for the stable proximal sampler in $\\chi^{2}$ -divergence. ", "page_idx": 24}, {"type": "text", "text": "Definition 3 (weak Fractional Poincar\u00e9 Inequality). For $\\vartheta\\in(0,2)$ , a probability density $\\mu$ satisfies a $\\vartheta$ -weak fractional Poincar\u00e9 inequality if there exists a decreasing function $\\beta_{\\mathrm{WFPI}(\\vartheta)}:\\mathbb{R}_{+}\\rightarrow\\mathbb{R}_{+}$ such that for any $\\phi:\\mathbb{R}^{d}\\!\\rightarrow\\mathbb{R}$ in the domain of $\\mathcal{E}_{\\mu}^{(\\vartheta)}$ with $\\mu(\\phi)=0,$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mu(\\phi^{2})\\leq\\beta_{\\mathrm{WFPI}(\\vartheta)}(r)\\mathcal{E}_{\\mu}^{(\\vartheta)}(\\phi)+r\\left\\|\\phi\\right\\|_{\\infty}^{2},\\qquad\\forall r>0,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\mathcal{E}_{\\mu}^{(\\vartheta)}$ is a non-local Dirichlet form associated with $\\mu$ defined as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{E}_{\\mu}^{(\\vartheta)}(\\phi):=c_{d,\\vartheta}\\!\\int\\!\\!\\int_{\\{x\\neq y\\}}\\!\\frac{(\\phi(x)-\\phi(y))^{2}}{|x-y|^{(d+\\vartheta)}}\\mathrm{d}x\\mu(y)\\mathrm{d}y\\quad w i t h\\quad c_{d,\\vartheta}=\\frac{2^{\\vartheta}\\Gamma((d+\\vartheta)/2)}{\\pi^{d/2}|\\Gamma(-\\vartheta/2)|}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The wFPI is satisfied by any probability density that is locally bounded, and is hence extremely general. Setting the parameter $r=0$ , wFPI reduces to FPI with $C_{\\mathrm{FPI}(\\vartheta)}=\\beta_{\\mathrm{WFPI}(\\vartheta)}(0)$ . ", "page_idx": 24}, {"type": "text", "text": "Theorem 5. Assume that $\\pi^{X}$ satisfies the $\\alpha$ -wFPI with parameter $\\beta_{\\mathrm{WFPI}(\\alpha)}(r)$ for some $\\alpha\\in(0,2)$ . Then for any step size $\\eta>0$ and initial condition $\\rho_{0}^{X}$ such that $R_{\\infty}(\\rho_{0}^{X}|\\pi^{X})<\\infty$ , the $k^{t h}$ iterate of the stable proximal sampler with parameter $\\alpha$ (Algorithm 2) satisfies ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\chi^{2}(\\rho_{k}^{X}|\\pi^{X})\\leq\\exp\\big(-(\\beta_{\\sf N F P I}(\\alpha)(r)+\\eta)^{-1}k\\eta\\big)\\chi^{2}(\\rho_{0}^{X}|\\pi^{X})}\\\\ &{\\qquad\\qquad\\qquad+\\,4r\\big(1-\\exp\\big(-(\\beta_{\\sf N F P I}(\\alpha)(r)+\\eta)^{-1}(k+1)\\eta\\big)\\big)\\exp\\big(2R_{\\infty}(\\rho_{0}^{X}|\\pi^{X})\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The proof of Theorem 5 follows the same two-step analysis as it is introduced in the beginning of Section B.2. The convergence property corresponding to Step 1 is stated in the following Proposition. ", "page_idx": 24}, {"type": "text", "text": "Proposition 4. Assume that $\\pi^{X}$ satisfies the $\\alpha$ -wFPI with parameter $\\beta_{\\mathrm{WFPI}(\\alpha)}$ for some $\\alpha\\in(0,2)$ , then for each $k\\geq0,r>0$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\chi^{2}(\\rho_{k}^{Y}|\\pi^{Y})\\leq\\exp\\big(-(\\beta_{\\sf W F P I(\\alpha)}(r)+\\eta)^{-1}\\eta\\big)\\chi^{2}(\\rho_{k}^{X}|\\pi^{X})}\\\\ &{\\qquad\\qquad\\qquad+\\ 4r\\big(1-\\exp\\big(-(\\beta_{\\sf W F P I(\\alpha)}(r)+\\eta)^{-1}\\eta\\big)\\exp\\big(2R_{\\infty}(\\rho_{k}^{X}|\\pi^{X})\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof of Proposition 4. In the stable proximal sampler with parameter $\\alpha$ , we have $\\rho_{k}^{Y}=\\rho_{k}^{X}*p_{\\eta}^{(\\alpha)}$ and $\\pi^{Y}=\\pi^{X}*p_{\\eta}^{(\\alpha)}$ . Therefore we can view $\\rho_{k}^{Y}$ and $\\pi^{Y}$ as $\\rho_{k}^{X}$ and $\\pi^{X}$ evolving along the fractional ", "page_idx": 24}, {"type": "text", "text": "heat flow by time $\\eta$ respectively. For any $t\\in[0,\\eta]$ , define $\\pi_{t}^{X}=\\pi^{X}*p_{t}^{(\\alpha)}$ and $\\rho_{t}^{X}=\\rho_{k}^{X}*p_{t}^{(\\alpha)}$ . We have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\chi^{2}(\\rho_{t}^{X}|\\pi_{t}^{X})=-\\mathcal{E}_{\\pi_{t}^{X}}(\\frac{\\rho_{t}^{X}}{\\pi_{t}^{X}})=-\\mathcal{E}_{\\pi_{t}^{X}}(\\frac{\\rho_{t}^{X}}{\\pi_{t}^{X}}-1).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "According to [Cha04, Theorem 23], pt(\u03b1)satisfies \u03b1-FPI with parameter \u03b7 for all t \u2208(0, \u03b7]. According to Lemma 7, $\\pi_{t}^{X}$ satisfies the $\\alpha$ -wFPI with $\\beta_{\\mathrm{WFPI}(\\alpha)}(r)+\\eta$ . Therefore we get ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\mathrm{d}}{\\mathrm{i}t}\\chi^{2}(\\rho_{t}^{X}|\\pi_{t}^{X})\\le\\big(\\beta_{\\mathsf{W F I}(\\alpha)}(r)+\\eta\\big)^{-1}\\chi^{2}(\\rho_{t}^{X}|\\pi_{t}^{X})+r\\big(\\beta_{\\mathsf{W F I}(\\alpha)}(r)+\\eta\\big)^{-1}\\left\\|\\rho_{t}^{X}/\\pi_{t}^{X}-1\\right\\|_{\\infty}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\big(\\beta_{\\mathsf{W F I}(\\alpha)}(r)+\\eta\\big)^{-1}\\chi^{2}(\\rho_{t}^{X}|\\pi_{t}^{X})+4r\\big(\\beta_{\\mathsf{W F P I}(\\alpha)}(r)+\\eta\\big)^{-1}\\exp\\big(2R_{\\infty}(\\rho_{k}^{X}|\\pi^{X})\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the last inequality follows from the definition of Renyi-divergence and the data processing inequality. Last, (11) follows from Gronwall\u2019s inequality. ", "page_idx": 25}, {"type": "text", "text": "Proof of Theorem 5. According to Proposition 4, the $\\chi^{2}$ decaying property in step 1 of the algorithm is as follows, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\chi^{2}(\\rho_{k}^{Y}|\\pi^{Y})\\leq\\exp\\big(-(\\beta_{\\sf W F P I(\\alpha)}(r)+\\eta)^{-1}\\eta\\big)\\chi^{2}(\\rho_{k}^{X}|\\pi^{X})}\\\\ &{\\qquad\\qquad\\qquad+~4r\\big(1-\\exp\\big(-(\\beta_{\\sf W F P I(\\alpha)}(r)+\\eta)^{-1}\\eta\\big)\\exp\\big(2R_{\\infty}(\\rho_{k}^{X}|\\pi^{X})\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "In step 2, we have $\\rho_{k+1}^{X}\\,=\\,\\rho_{k}^{Y}\\,*\\,\\pi^{X\\vert Y}$ and $\\pi^{X}\\,=\\,\\pi^{Y}\\,*\\,\\pi^{X\\vert Y}$ . Therefore according to the data processing inequality, we get ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\chi^{2}(\\rho_{k+1}^{X}|\\pi^{X})\\leq\\chi^{2}(\\rho_{k}^{Y}|\\pi^{Y})}\\\\ &{\\qquad\\qquad\\leq\\exp\\big(-(\\beta_{\\mathsf{W F P I}(\\vartheta)}(r)+\\eta)^{-1}\\eta\\big)\\chi^{2}(\\rho_{k}^{X}|\\pi^{X})}\\\\ &{\\qquad\\qquad\\qquad+4r\\big(1-\\exp\\big(-(\\beta_{\\mathsf{W F P I}(\\alpha)}(r)+\\eta)^{-1}\\eta\\big)\\exp\\big(2R_{\\infty}(\\rho_{k}^{X}|\\pi^{X})\\big)}\\\\ &{\\qquad\\qquad\\leq\\exp\\big(-k(\\beta_{\\mathsf{W F P I}(\\alpha)}(r)+\\eta)^{-1}\\eta\\big)\\chi^{2}(\\rho_{0}^{X}|\\pi^{X})}\\\\ &{\\qquad\\qquad\\qquad++4r\\big(1-\\exp\\big(-(\\beta_{\\mathsf{W F P I}(\\alpha)}(r)+\\eta)^{-1}(k+1)\\eta\\big)\\big)\\exp\\big(2R_{\\infty}(\\rho_{0}^{X}|\\pi^{X})\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the last inequality follows from the data processing inequality. Last, apply the above iterative relation $k$ times and we prove (11). \u53e3 ", "page_idx": 25}, {"type": "text", "text": "Lemma 7. Let $\\mu_{1}$ be a probability density on $\\mathbb{R}^{d}$ satisfying the $\\vartheta$ -wFPI with parameter $\\beta_{\\mathrm{WFPI}(\\vartheta)}(r)$ . Let $\\mu_{2}$ be a probability density on $\\mathbb{R}^{d}$ satisfying the $\\vartheta$ -FPI with parameter $C_{\\mathrm{FPI}(\\vartheta)}$ . Then $\\mu_{1}\\ast\\mu_{2}$ satisfies $\\vartheta$ -wFPI with parameter $\\beta_{\\mathrm{WFPI}(\\vartheta)}(r)+C_{\\mathrm{FPI}(\\vartheta)}$ . ", "page_idx": 25}, {"type": "text", "text": "Proof of Lemma 7. Let $X,Y$ be two independent random variables such that $X\\sim\\mu_{2}$ and $Y\\sim\\mu_{1}$ . According to variance decomposition, we have for any function $\\phi$ such that $\\mu_{1}*\\mu_{2}(\\phi)=0$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname{Var}_{\\mu_{1}*\\mu_{2}}{\\left(\\phi\\right)}=\\operatorname{Var}\\left(\\phi(X+Y)\\right)=\\operatorname{\\mathbb{E}}\\left[\\operatorname{Var}\\left(\\phi(X+Y)|Y\\right)\\right]+\\operatorname{Var}\\left(\\mathbb{E}\\left[\\phi(X+Y)|Y\\right]\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since $X\\sim\\mu_{2}$ and $\\mu_{2}$ satisfies the $\\vartheta$ -FPI with parameter $C_{\\mathrm{FPI}(\\vartheta)}$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}[\\mathrm{Var}\\left(\\phi(X+Y)\\vert Y\\right)]}\\\\ &{\\leq C_{\\mathrm{FPI}(\\vartheta)}c_{d,\\alpha}\\displaystyle\\int\\!\\!\\!\\int_{\\{z\\neq0\\}}\\frac{\\left(\\phi(x+y+z)-\\phi(x+y)\\right)^{2}}{\\vert z\\vert^{(d+\\vartheta)}}\\mathrm{d}z\\mu_{2}(x)\\mathrm{d}x\\mu_{1}(y)\\mathrm{d}y.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since $Y\\sim\\mu_{1}$ and $\\mu_{1}$ satisfies the $\\vartheta$ -wFPI with parameter $\\beta_{\\mathrm{WFPI}(\\vartheta)}$ , following the proof of Lemma 6, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\quad~\\mathop{Var}\\left(\\mathbb{E}\\left[\\phi(X+Y)\\vert Y\\right]\\right)}}\\\\ &{\\le\\!\\beta_{\\mathrm{WFP}(\\vartheta)}c_{d,\\alpha}\\displaystyle\\iint_{\\{z\\neq0\\}}\\int\\frac{\\left(\\phi(x+y+z)-\\phi(x+y)\\right)^{2}}{\\vert z\\vert^{(d+\\vartheta)}}\\mu_{2}(x)\\mathrm{d}x\\mathrm{d}z\\mu_{1}(y)\\mathrm{d}y}\\\\ &{\\qquad+\\left.r\\left\\Vert\\int\\phi(x+\\cdot)\\mu_{2}(x)\\mathrm{d}x-\\iint\\phi(x+y)\\mu_{2}(x)\\mathrm{d}x\\mu_{1}(y)\\mathrm{d}y\\right\\Vert_{\\infty}^{2}}\\\\ &{\\le\\!\\beta_{\\mathrm{WFP}(\\vartheta)}c_{d,\\alpha}\\displaystyle\\iint_{\\{z\\neq0\\}}\\int\\frac{\\left(\\phi(x+y+z)-\\phi(x+y)\\right)^{2}}{\\vert z\\vert^{(d+\\vartheta)}}\\mu_{2}(x)\\mathrm{d}x\\mathrm{d}z\\mu_{1}(y)\\mathrm{d}y+r\\left\\Vert\\phi\\right\\Vert_{\\infty}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the last inequality follows from the fact that $\\mu_{1}\\ast\\mu_{2}(\\phi)=0$ and the convexity $\\|\\cdot\\|_{\\infty}$ . Combining (12) and (14), we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathrm{Var}_{\\mu_{1}\\ast\\mu_{2}}(\\phi)}\\\\ &{\\leq C_{\\mathrm{FPI}(\\vartheta)}c_{d,\\alpha}\\displaystyle\\iint_{\\{\\varepsilon\\neq0\\}}\\frac{\\left(\\phi(x+y+z)-\\phi(x+y)\\right)^{2}}{|z|^{(d+\\vartheta)}}\\mathrm{d}z\\mu_{2}(x)\\mathrm{d}x\\mu_{1}(y)\\mathrm{d}y}\\\\ &{\\qquad+\\left.\\beta_{\\mathrm{WFI}(\\vartheta)}(r)c_{d,\\alpha}\\displaystyle\\iint_{\\{z\\neq0\\}}\\int\\left(\\frac{\\phi(x+y+z)-\\phi(x+y))^{2}}{|z|^{(d+\\vartheta)}}\\mu_{2}(x)\\mathrm{d}x\\mathrm{d}z\\mu_{1}(y)\\mathrm{d}y+r\\left\\|\\phi\\right\\|_{\\infty}^{2}}\\\\ &{=\\left(\\beta_{\\mathrm{WFI}(\\vartheta)}(r)+C_{\\mathrm{FPI}(\\vartheta)}\\right)c_{d,\\alpha}\\displaystyle\\iint_{\\{z\\neq0\\}}\\frac{\\left(\\phi(u+z)-\\phi(u)\\right)^{2}}{|z|^{(d+\\vartheta)}}\\mathrm{d}z\\mu_{1}\\ast\\mu_{2}(u)\\mathrm{d}u+r\\left\\|\\phi\\right\\|_{\\infty}^{2}}\\\\ &{=\\left(\\beta_{\\mathrm{WFI}(\\vartheta)}(r)+C_{\\mathrm{FPI}(\\vartheta)}\\right)\\xi_{\\mu_{1}\\ast\\mu_{2}}(\\phi)+r\\left\\|\\phi\\right\\|_{\\infty}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Lemma 7 is hence proved. ", "page_idx": 26}, {"type": "text", "text": "B.5 Proofs for the Generalized Cauchy Examples ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we provide proofs for the two corollaries in Section 3.2. ", "page_idx": 26}, {"type": "text", "text": "Proof of Corollary 4. According to [WW15, Corollary 1.2], $\\pi_{\\nu}$ satisfies $\\alpha$ -FPI with parameter $C_{\\mathrm{FPI}(\\vartheta)}$ for any $\\alpha\\le\\operatorname*{min}(2,\\nu)$ . Therefore it follows from Theorem 3 that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\chi^{2}(\\rho_{k}^{X}|\\pi_{\\nu})\\leq\\exp\\left(-k\\eta\\left(C_{\\mathrm{FPI}(\\alpha)}+\\eta\\right)^{-1}\\right)\\chi^{2}(\\rho_{0}^{X}|\\pi_{\\nu}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "According to $[\\mathrm{MHFH}^{+}23$ , Corollary 22], when $\\rho_{0}^{X}\\ =\\ {\\mathcal{N}}(0,I_{d})$ and $d\\ \\geq\\ 2$ , $R_{\\infty}(\\rho_{0}^{X}|\\pi_{\\nu})\\ \\leq$ $\\begin{array}{r}{\\ln(2^{\\nu/2}\\Gamma(\\nu/2))+\\ln(\\frac{d+\\nu}{2e})}\\end{array}$ which implies $\\chi^{2}(\\rho_{0}^{X}|\\pi_{\\nu})\\:=\\:\\Theta(d)$ . Therefore Corollary 4 follows from (15) and $\\eta\\in(0,1)$ . ", "page_idx": 26}, {"type": "text", "text": "Proof of Corollary 5. We prove the two part in the Corollary separately: ", "page_idx": 26}, {"type": "text", "text": "(i) When $\\nu\\geq1$ , according to [WW15, Corollary 1.2] $\\pi_{\\nu}$ satisfies the 1-FPI with parameter $C_{\\mathrm{FPI(1)}}$ Corollary 3 applies with $L=4(d+\\nu)$ and $\\beta=1/4$ and the iteration complexity of Algorithm 2 is of order $\\mathcal{O}\\big(C_{\\mathrm{FPI(1)}}d^{\\frac{1}{2}}(d+\\nu)^{4}\\ln(\\chi^{2}(\\rho_{0}^{X}|\\pi_{\\nu})/\\varepsilon)\\big)$ . ", "page_idx": 26}, {"type": "text", "text": "(ii) When $\\nu\\in(0,1)$ , according to [WW15, Corollary 1.2], there exists a positive constant $c$ such that $\\pi_{\\nu}$ satisfies the 1-wFPI with parameter ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\beta_{\\mathrm{WFPI}(1)}(r)=c(1+r^{-(1-\\nu)/\\nu}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Theorem 5 implies that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\chi^{2}(\\rho_{k}^{X}|\\pi_{\\nu})\\leq\\exp\\big(-\\frac{k\\eta}{\\eta+c(1+r^{-(1-\\nu)/\\nu})}\\big)\\chi^{2}(\\rho_{0}^{X}|\\pi_{\\nu})}\\\\ &{\\qquad\\qquad\\qquad+\\,r\\big(1-\\exp\\big(-\\frac{(k+1)\\eta}{\\eta+c(1+r^{-(1-\\nu)/\\nu})}\\big)\\big)\\exp\\big(2R_{\\infty}(\\rho_{0}^{X}|\\pi^{X})\\big)}\\\\ &{\\qquad\\qquad\\leq\\exp\\big(-\\frac{k\\eta}{\\eta+c(1+r^{-(1-\\nu)/\\nu})}\\big)\\chi^{2}(\\rho_{0}^{X}|\\pi_{\\nu})}\\\\ &{\\qquad\\qquad\\qquad+\\,\\frac{(k+1)\\eta r}{\\eta+c(1+r^{-(1-\\nu)/\\nu})}\\exp\\big(2R_{\\infty}(\\rho_{0}^{X}|\\pi^{X})\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For any $\\varepsilon>0$ and $k\\geq1$ , pick $\\begin{array}{r}{r=\\frac{\\exp\\big(-2\\nu R_{\\infty}(\\rho_{0}^{X}|\\pi_{\\nu})\\big)c^{\\nu}\\varepsilon^{\\nu}}{(k+1)^{\\nu}\\eta^{\\nu}}}\\end{array}$ , we have $\\chi^{2}(\\rho_{k}^{X}|\\pi_{\\nu})\\leq\\varepsilon$ if ", "page_idx": 26}, {"type": "equation", "text": "$$\nk\\geq\\big[1+c^{\\frac{1}{\\nu}}\\eta^{-\\frac{1}{\\nu}}+2^{1/\\nu}c\\eta^{-1}\\varepsilon^{-(1-\\nu)/\\nu}\\exp\\big(\\frac{2(1-\\nu)R_{\\infty}(\\rho_{0}^{X}|\\pi_{\\nu})}{\\nu}\\big)\\big]\\ln^{1/\\nu}(\\frac{2\\chi^{2}(\\rho_{0}^{X}|\\pi_{\\nu})}{\\varepsilon}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Corollary 3 applies with $L=(d+\\nu)/\\nu$ and $\\beta=\\nu/4$ . Therefore, by choosing $\\eta=\\Theta(d^{-\\frac{1}{2}}(d+\\nu)^{-\\frac{4}{\\nu}})$ , the iteration complexity in Algorithm 2 is of order ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{O}\\bigg(\\operatorname*{max}\\big\\{c^{\\frac{1}{\\nu}}d^{\\frac{1}{2\\nu}+\\frac{4}{\\nu^{2}}},c d^{\\frac{1}{2}+\\frac{4}{\\nu}}\\varepsilon^{-\\frac{1-\\nu}{\\nu}}\\exp\\big(\\frac{2(1-\\nu)R_{\\infty}(\\rho_{0}^{X}|\\pi_{\\nu})}{\\nu}\\big)\\big\\}\\ln^{\\frac{1}{\\nu}}\\big(\\frac{2\\chi^{2}(\\rho_{0}^{X}|\\pi_{\\nu})}{\\varepsilon}\\big)\\bigg).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "C Proofs for the Lower Bounds on the Stable Proximal Sampler ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section we introduce the proofs for the lower bounds for the stable proximal sampler with parameter $\\alpha$ when the target is the generalized Cauchy density with degrees of freedom strictly smaller than $\\alpha$ . The lower bound is proved following the idea introduced in Section 2. ", "page_idx": 27}, {"type": "text", "text": "Lemma 8. Suppose $(x_{k},y_{k})_{k}$ are the iterates of the stable proximal sampler with parameter $\\alpha$ , step size $\\eta$ and target density $\\pi^{X}\\propto\\exp(-V)$ for some $V:\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}.$ . Let $G(x)=\\exp(\\kappa V(x))$ with $\\kappa\\in(0,1)$ . Then, for every $k\\geq0$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}[G(x_{k+1})]\\le\\mathbb{E}[G(x_{k}+2^{\\frac{1}{\\alpha}}\\eta^{\\frac{1}{\\alpha}}z_{k})],\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $z_{k}$ , with density $p_{1}^{(\\alpha)}$ , is sampled independently from $x_{k}$ . ", "page_idx": 27}, {"type": "text", "text": "Proof of Lemma 8. Recall that $\\pi^{X|Y}(x|y)\\propto\\pi^{X}(x)p^{(\\alpha)}(\\eta;x,y)$ . We have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[G(x_{k+1})]=\\mathbb{E}\\big[\\mathbb{E}[G(x_{k+1})|y_{k}]\\big]=\\mathbb{E}\\big[Z_{y_{k}}^{-1}\\int G(x)\\pi^{X}(x)p_{\\eta}^{(\\alpha)}(x-y_{k})\\mathrm{d}x\\big]}\\\\ &{\\qquad\\qquad=\\mathbb{E}\\big[Z_{y_{k}}^{-1}\\mathbb{E}[G(y_{k}+\\eta^{\\frac{1}{\\alpha}}z_{k})\\pi^{X}(y_{k}+\\eta^{\\frac{1}{\\alpha}}z_{k})|y_{k}]\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\begin{array}{r}{Z_{y_{k}}\\,=\\,\\int\\pi^{X}(x)p_{\\eta}^{(\\alpha)}(x-y_{k})\\mathrm{d}x\\,=\\,\\mathbb{E}[\\pi^{X}(y_{k}+\\eta^{\\frac{1}{\\alpha}}z_{k})|y_{k}]}\\end{array}$ and $z_{k}$ is the $\\alpha$ -stable random vector with density $p_{1}^{(\\alpha)}$ , which is independent to $y_{k},x_{k}$ . Let $T:\\mathbb{R}_{+}\\to\\mathbb{R}$ be $T(r)=r^{-\\kappa}$ . Since $\\kappa\\in(0,1)$ , $T$ is convex and $r\\mapsto r T(r)$ is concave. According to the fact that $G(x)=T(\\pi^{X})(x)$ and Jensen\u2019s inequality, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[G(x_{k+1})]=\\mathbb{E}\\Bigg[\\frac{\\mathbb{E}\\big[(\\pi^{X}T(\\pi^{X}))(y_{k}+\\eta^{\\frac{1}{\\alpha}}z_{k})|y_{k}\\big]}{\\mathbb{E}\\big[\\pi^{X}(y_{k}+\\eta^{\\frac{1}{\\alpha}}z_{k})|y_{k}\\big]}\\Bigg]}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\mathbb{E}\\big[T\\big(\\mathbb{E}[\\pi^{X}(y_{k}+\\eta^{\\frac{1}{\\alpha}}z_{k})|y_{k}]\\big)\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since $T$ is convex, apply Jensen\u2019s inequality again and we get ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[G(x_{k+1})]\\leq\\mathbb{E}[G(y_{k}+\\eta^{\\frac{1}{\\alpha}}z_{k})]=\\mathbb{E}\\big[\\mathbb{E}[G(x_{k}+\\eta^{\\frac{1}{\\alpha}}z_{k}^{\\prime}+\\eta^{\\frac{1}{\\alpha}}z_{k})|x_{k}]\\big]}\\\\ &{\\qquad\\qquad=\\mathbb{E}[G(x_{k}+2^{\\frac{1}{\\alpha}}\\eta^{\\frac{1}{\\alpha}}\\bar{z}_{k})|x_{k}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $z_{k}^{\\prime}$ is the $\\alpha$ -stable random vector with density $p_{1}^{(\\alpha)}$ , which is independent to $x_{k},z_{k}$ and the last identity follows from the self-similarity of $\\alpha$ -stable process with $\\bar{z}_{k}\\sim p_{1}^{(\\alpha)}$ which is independent to $x_{k}$ . \u53e3 ", "page_idx": 27}, {"type": "text", "text": "Lemma 9. Suppose $(x_{k},y_{k})_{k}$ are the iterates of the stable proximal sampler with parameter $\\alpha$ , step size $\\eta$ and target density $\\pi^{X}\\propto\\exp(-V)$ satisfies ", "page_idx": 27}, {"type": "equation", "text": "$$\n|\\nabla V(x)|\\leq\\frac{(d+\\nu_{2})|x|}{1+|x|^{2}}\\quad a n d\\quad\\Delta V(x)\\leq\\frac{(d+\\nu_{2})^{2}}{1+|x|^{2}},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for some $\\nu_{2}\\in(0,\\alpha)$ and for all $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ . Let $G(x)=\\exp(\\kappa V(x))$ with ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\kappa\\in(\\nu_{2}(d+\\nu_{2})^{-1},\\alpha(d+\\nu_{2})^{-1}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then, for every $k\\geq0$ and for all $r>0$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[G(x_{k+1})]\\leq(1+r)^{\\frac{\\kappa(d+\\nu_{2})}{2}}\\mathbb{E}[G(x_{k})]+2^{\\frac{\\kappa(d+\\nu_{2})}{\\alpha}}\\eta^{\\frac{\\kappa(d+\\nu_{2})}{\\alpha}}(1+r^{-1})^{\\frac{\\kappa(d+\\nu_{2})}{2}}m_{\\kappa(d+\\nu_{2})}^{(\\alpha)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $m_{\\kappa(d+\\nu_{2})}^{(\\alpha)}\\;=\\;\\mathbb{E}[|z_{k}|^{\\kappa(d+\\nu_{2})}]$ with $z_{k}$ being an $\\alpha$ -stable random vector with density $p_{1}^{(\\alpha)}$ Moreover, for every $N\\geq0$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[G(x_{N})]\\lesssim\\mathbb{E}[G(x_{0})]+m_{\\kappa(d+\\nu_{2})}^{(\\alpha)}N^{\\frac{\\kappa(d+\\nu_{2})}{2}+1}\\eta^{\\frac{\\kappa(d+\\nu_{2})}{\\alpha}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\lesssim$ is hiding a uniform positive constant factor. ", "page_idx": 27}, {"type": "text", "text": "Proof of Lemma 9. Without loss of generality assume $V(0)=0$ . Then, we have that, ", "page_idx": 28}, {"type": "equation", "text": "$$\nV(x)=\\int_{0}^{1}\\langle x,\\nabla V(t x)\\rangle\\mathrm{d}t\\le(d+\\nu_{2})\\int_{0}^{1}\\frac{t|x|}{1+|t x|^{2}}\\mathrm{d}t=\\frac{d+\\nu_{2}}{2}\\ln(1+|x|^{2}).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Therefore $G(x)\\,=\\,\\exp(\\kappa V(x))\\,\\leq\\,(1+|x|^{2})^{\\kappa(d+\\nu_{2})/2}$ , Since $\\kappa\\in(\\nu_{2}(d+\\nu_{2})^{-1},\\alpha(d+\\nu_{2})^{-1})$ , $G(x)={\\mathcal{O}}(|x|^{\\kappa(d+\\nu_{2})})$ when $|x|\\gg1$ and $\\mathbb{E}[G(x_{k}+2^{\\frac{1}{\\alpha}}\\eta^{\\frac{1}{\\alpha}}z_{k})]$ in Lemma 8 is finite. We have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}[G(x_{k}+2^{\\frac{1}{\\alpha}}\\eta^{\\frac{1}{\\alpha}}z_{k})]}\\\\ &{\\le\\!\\mathbb{E}[(1+|x_{k}+2^{\\frac{1}{\\alpha}}\\eta^{\\frac{1}{\\alpha}}z_{k}|^{2})^{\\frac{\\kappa(d+\\nu_{2})}{2}}]}\\\\ &{\\le\\!\\mathbb{E}[(1+(1+r)|x_{k}|^{2}+4^{\\frac{1}{\\alpha}}\\eta^{\\frac{2}{\\alpha}}(1+r^{-1})|z_{k}|^{2})^{\\frac{\\kappa(d+\\nu_{2})}{2}}]}\\\\ &{\\le\\!(1+r)^{\\frac{\\kappa(d+\\nu_{2})}{2}}\\mathbb{E}[G(x_{k})]+2^{\\frac{\\kappa(d+\\nu_{2})}{\\alpha}}\\eta^{\\frac{\\kappa(d+\\nu_{2})}{\\alpha}}(1+r^{-1})^{\\frac{\\kappa(d+\\nu_{2})}{2}}\\mathbb{E}[|z_{k}|^{\\kappa(d+\\nu_{2})}]}\\\\ &{\\le\\!(1+r)^{\\frac{\\kappa(d+\\nu_{2})}{2}}\\mathbb{E}[G(x_{k})]+2^{\\frac{\\kappa(d+\\nu_{2})}{\\alpha}}\\eta^{\\frac{\\kappa(d+\\nu_{2})}{\\alpha}}(1+r^{-1})^{\\frac{\\kappa(d+\\nu_{2})}{2}}m_{\\kappa(d+\\nu_{2})}^{(\\alpha)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the first inequality follows from the Young\u2019s inequality and $m_{\\kappa(d+\\nu_{2})}^{(\\alpha)}=\\mathbb{E}[|z_{k}|^{\\kappa(d+\\nu_{2})}]$ with $z_{k}$ being an $\\alpha$ -stable random vector with density $p_{1}^{(\\alpha)}$ . (17) follows from Lemma 8. Furthermore, by induction we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[G(x_{N})]\\leq(1+r)^{\\kappa(d+\\nu_{2})N/2}\\mathbb{E}[G(x_{0})]}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\,\\frac{(1+r)^{\\kappa(d+\\nu_{2})N/2}-1}{(1+r)^{\\kappa(d+\\nu_{2})/2}-1}2^{\\frac{\\kappa(d+\\nu_{2})}{\\alpha}}\\eta^{\\frac{\\kappa(d+\\nu_{2})}{\\alpha}}(1+r^{-1})^{\\frac{\\kappa(d+\\nu_{2})}{2}}m_{\\kappa(d+\\nu_{2})}^{(\\alpha)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Pick \u03ba(d+2\u03bd2)N and (18) is proved. ", "page_idx": 28}, {"type": "text", "text": "Proof of Theorem 4. To apply Lemma 1, we choose $G(x)~=~\\exp(\\kappa V(x))$ with $\\kappa~\\in~(\\nu_{2}(d+$ $\\nu_{2})^{-1},\\stackrel{.}{\\alpha}(d+\\nu_{2})^{-1})\\subset(\\bar{0,1})$ . Without loss of generality assume $V(0)=0$ . Via Assumption 1, we have the estimates for $V$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\nV(x)=\\int_{0}^{1}\\langle x,\\nabla V(t x)\\rangle\\mathrm{d}t\\ge(d+\\nu_{1})\\int_{0}^{1}\\frac{t|x|}{1+|t x|^{2}}\\mathrm{d}t=\\frac{d+\\nu_{1}}{2}\\ln(1+|x|^{2}).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By Lemma 2 we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pi^{X}(G(x)\\geq y)\\geq\\pi^{X}\\left(|x|\\geq y^{\\frac{1}{\\kappa(d+\\nu_{1})}}\\right)\\geq C_{\\nu_{1}}d^{\\frac{\\nu_{1}}{2}}\\left(1+y^{\\frac{-2}{\\kappa(d+\\nu_{1})}}\\right)^{-\\frac{d+\\nu_{2}}{2}}y^{\\frac{-\\nu_{2}}{\\kappa(d+\\nu_{1})}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We then invoke Lemma 1 and Lemma 9 to obtain ", "page_idx": 28}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathrm{TV}(\\rho_{N}^{X},\\pi^{X})}\\\\ &{\\gtrsim\\underset{y\\geq1}{\\operatorname*{sup}}C_{\\nu_{1}}d^{\\frac{\\nu_{1}}{2}}\\left(1+y^{\\frac{-2}{\\kappa(d+\\nu_{1})}}\\right)^{-\\frac{d+\\nu_{2}}{2}}y^{\\frac{-\\nu_{2}}{\\kappa(d+\\nu_{1})}}-\\frac{\\mathbb{E}[G(x_{0})]+m_{\\kappa(d+\\nu_{2})}^{(\\alpha)}N^{\\frac{\\kappa(d+\\nu_{2})}{2}+1}\\eta^{\\frac{\\kappa(d+\\nu_{2})}{\\alpha}}}{y}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The fact that $\\kappa\\in(\\nu_{2}(d+\\nu_{1})^{-1},\\alpha(d+\\nu_{2})^{-1})$ ensures that the supremum on the right side is always positive. In particular, picking $y$ such that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{y^{1-\\frac{\\nu_{2}}{\\kappa(d+\\nu_{1})}}=\\Theta\\Big(C_{\\nu_{1}}^{-1}d^{-\\frac{\\nu_{2}}{2}}\\big(\\mathbb{E}[G(x_{0})]+m_{\\kappa(d+\\nu_{2})}^{(\\alpha)}N^{\\frac{\\kappa(d+\\nu_{2})}{2}+1}\\eta^{\\frac{\\kappa(d+\\nu_{2})}{\\alpha}}\\big)\\Big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "we obtain that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathrm{TV}(\\rho_{N}^{X},\\pi^{X})}\\\\ &{\\gtrsim C_{\\nu_{1}}^{\\frac{\\kappa(d+\\nu_{1})}{\\kappa(d+\\nu_{1})-\\nu_{2}}}d^{\\frac{\\kappa(d+\\nu_{1})\\nu_{2}}{2\\kappa(d+\\nu_{1})-2\\nu_{2}}}\\bigl(\\mathbb{E}[G(x_{0})]+m_{\\kappa(d+\\nu_{2})}^{(\\alpha)}N^{\\frac{\\kappa(d+\\nu_{2})}{2}+1}\\eta^{\\frac{\\kappa(d+\\nu_{2})}{\\alpha}}\\bigr)^{-\\frac{\\nu_{2}}{\\kappa(d+\\nu_{1})-\\nu_{2}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "$\\gtrsim$ ,u nwife ocramn  pchosoiotisve raen, df ogre ta tnhya $\\begin{array}{r}{\\alpha\\,\\in\\,\\big(\\frac{\\nu_{2}\\left(d+\\nu_{2}\\right)}{d+\\nu_{1}},2\\big]}\\end{array}$ and $\\begin{array}{r}{\\delta\\in(0,\\alpha-\\frac{\\nu_{2}(d+\\nu_{2})}{d+\\nu_{1}})}\\end{array}$ $\\begin{array}{r}{\\kappa=\\frac{\\alpha-\\delta}{d+\\nu_{2}}\\in\\big(\\frac{\\nu_{2}}{d+\\nu_{1}},\\frac{\\alpha}{d+\\nu_{2}}\\big)}\\end{array}$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathrm{TV}(\\rho_{N}^{X},\\pi^{X})}\\\\ &{\\geq C_{\\nu_{1},\\nu_{2},\\delta}d^{\\frac{\\nu_{2}(\\alpha-\\delta)(d+\\nu_{1})}{(\\alpha-\\delta)(d+\\nu_{1})-2\\nu_{2}(d+\\nu_{2})}}\\left(\\mathbb{E}[G(x_{0})]+m_{\\alpha-\\delta}^{(\\alpha)}N^{\\frac{\\alpha-\\delta}{2}+1}\\eta^{\\frac{\\alpha-\\delta}{\\alpha}})^{-\\frac{\\nu_{2}(d+\\nu_{2})}{(\\alpha-\\delta)(d+\\nu_{1})-\\nu_{2}(d+\\nu_{2})}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Theorem 4 then follows by taking $\\tau=\\alpha-\\delta$ . ", "page_idx": 28}, {"type": "image", "img_path": "zuwLGhgxtQ/tmp/14c50b91692e46d4cee7893deb2581fd13f8409ef197c224224204c2fe8b356f.jpg", "img_caption": ["Figure 1: Comparison between Gaussian and Stable Proximal Sampler: target is chosen to be onedimensional student-t with center 0 and 4 degrees of freedom; initialization is chosen $x_{0}=20$ . "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "C.1 Further Discussions on Lower bounds of the stable proximal sampler ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "To derive a lower bound for the stable proximal sampler with parameter $\\alpha$ , it is worth mentioning that there is an extra difficulty applying our method when $\\nu\\geq\\alpha$ . Recall that when $\\nu\\in(0,\\alpha)$ , $\\pi_{\\nu}$ has heavier tail than $\\rho_{k}^{X}$ does. Therefore, when we apply ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathrm{TV}(\\rho_{k}^{X},\\pi_{\\nu})\\geq|\\pi_{\\nu}(G\\geq y)-\\rho_{k}^{X}(G\\geq y)|,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "to study the lower bound, it suffices to derive a lower bound on $\\pi_{\\nu}(G\\geq y)$ , and an upper bound on $\\rho_{k}^{X}(G\\geq y)$ which is smaller than the lower bound on $\\pi_{\\nu}(G\\geq y)$ . Deriving these bounds is not too hard: the lower bound can be obtained by looking at an explicit integral against $\\pi_{\\nu}$ directly and the upper bound is derived based on the fractional absolute moment accumulation of the isotropic $\\alpha$ -stable random variables along the stable proximal sampler. ", "page_idx": 29}, {"type": "text", "text": "However, when $\\nu\\geq\\alpha$ , we expect that $\\rho_{k}^{X}$ has heavier tail than $\\pi_{\\nu}$ . Therefore, to apply (19), we need to find an upper bound on $\\pi_{\\nu}(G\\geq y)$ , and a lower bound on $\\rho_{k}^{X}(G\\geq y)$ which is smaller than the upper bound on $\\pi_{\\nu}(G\\geq y)$ . Notice that $\\rho_{k}^{X}(G\\geq y)$ is a quantity varying along the trajectory of the stable proximal sampler. Deriving a lower bound along the trajectory is essentially more challenging than deriving an upper bound. ", "page_idx": 29}, {"type": "text", "text": "In order to derive a satisfying lower bound in this case, it hence remains to characterize the stable proximal sampler as an approximation of an appropriate gradient flow, just as that the Browniandriven proximal sampler can be interpret as the entropy-regularized JKO scheme in [CCSW22]; see also Section 5. To understand this kind of gradient flow approximations itself is an interesting future work as it may help us to understand and characterize the class of MCMC samplers that utilize heavy-tail samples to approximate lighter-tail target densities, which is non-standard compared to commonly used MCMC samplers such as ULA, MALA, etc. ", "page_idx": 29}, {"type": "text", "text": "D Numerical Illustrations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this section, we present numerical results that illustrate the improved performance of the proximal sampler with stable oracles $\\left[\\alpha=1\\right.$ ) compared to that with Gaussian oracles. We first sample from the one-dimensional student-t distribution with center zero and 4 degrees of freedom by running the proximal samplers with different oracles in parallel for 100 times. Each individual chain is run for 100 iterations with step-size $\\eta=0.1$ . Figures 1,2,3 present the convergence results for different initializations $x_{0}\\,=\\,20,5,-5$ respectively. In each figure, the first column shows the means and variances of the iterates along the trajectories; the center column shows the histograms of the last iterates and the target density (red curve); the last column shows the convergence of Wasserstein-2 distance along the trajectories. We also sample from the two-dimensional student-t distribution with center at the origin and 4 degrees of freedom by running the proximal samplers with different oracles in parallel for 30 times. Each individual chain is run for 20 iterations with step-size $\\eta=0.1$ with the initialization at $x_{0}=[5,1]$ . In Figure 4, we present the convergence results, the first column showing the means and variances of the first-coordinates along the trajectories, the center column showing the histograms of first-coordinate in the last iterates and the first-coordinate marginal density of the target distribution (red curve), and the last column showing the convergence of Wasserstein-2 distance along the trajectories. ", "page_idx": 29}, {"type": "image", "img_path": "zuwLGhgxtQ/tmp/f123381ef76250dd87b518662aad8fb5e01ca9e07e06e52e325fb3b9f7ead8b9.jpg", "img_caption": ["Figure 2: Comparison between Gaussian and Stable Proximal Sampler: target is chosen to be onedimensional student-t with center 0 and 4 degrees of freedom; initialization is chosen $x_{0}=5$ . "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "zuwLGhgxtQ/tmp/bc8514f205c3298648ee0ce850c925b01a0321f0869f1177f2c50bc2c7be0085.jpg", "img_caption": ["Figure 3: Comparison between Gaussian and Stable Proximal Sampler: target is chosen to be onedimensional student-t with center 0 and 4 degrees of freedom; initialization is chosen $x_{0}=-5$ . "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "zuwLGhgxtQ/tmp/7aa510d9bfe1921b9126379bc9b5df227715593138d92160ad4687c16670f2d3.jpg", "img_caption": ["Figure 4: Comparison between Gaussian and Stable Proximal Sampler: target is chosen to be twodimensional student-t with center $(0,0)$ and 4 degrees of freedom; initialization is chosen $x_{0}=[5,1]$ . "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The main claim in the abstract and introduction is the separation result between Gaussian and Proximal Sampler. The rest of the sections are exactly stating (and proving) the aforementioned separation result. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: Please see Section 5. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The assumptions are listed in the respective theorem. The (correct) proofs are provided in the appendix. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our work is primarily theoretical. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our work is primarily theoretical. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our work is primarily theoretical. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our work is primarily theoretical. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: Our work is primarily theoretical. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The authors have read the Ethics Guideline and followed it in the paper preperation. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: Our work is primarily theoretical. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: Our work is primarily theoretical. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: Our work is primarily theoretical. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 35}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: Our work is primarily theoretical. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: Our work is primarily theoretical. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: Our work is primarily theoretical. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}]