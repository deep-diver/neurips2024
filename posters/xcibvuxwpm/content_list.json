[{"type": "text", "text": "Trading off Consistency and Dimensionality of Convex Surrogates for Multiclass Classification ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Enrique Nueve Department of Computer Science University of Colorado Boulder enrique.nueveiv@colorado.edu ", "page_idx": 0}, {"type": "text", "text": "Bo Waggoner   \nDepartment of Computer Science   \nUniversity of Colorado Boulder bwag@colorado.edu ", "page_idx": 0}, {"type": "text", "text": "Dhamma Kimpara Department of Computer Science University of Colorado Boulder dhamma.kimpara@colorado.edu ", "page_idx": 0}, {"type": "text", "text": "Jessie Finocchiaro\u2217 Department of Computer Science Boston College finocch@bc.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In multiclass classification over $n$ outcomes, we typically optimize some surrogate loss $L:\\mathbb R^{d}\\times\\mathcal{Y}\\to\\mathbb R$ assigning real-valued error to predictions in $\\mathbb{R}^{d}$ . In this paradigm, outcomes must be embedded into the reals with dimension $d\\approx n$ in order to design a consistent surrogate loss. Consistent losses are well-motivated theoretically, yet for large $n$ , such as in information retrieval and structured prediction tasks, their optimization may be computationally infeasible. In practice, outcomes are typically embedded into some $\\mathbb{R}^{\\hat{d}}$ for $d\\ll n$ , with little known about their suitability for multiclass classification. We investigate two approaches for trading off consistency and dimensionality in multiclass classification while using a convex surrogate loss. We first formalize partial consistency when the optimized surrogate has dimension $d\\ll n$ . We then check if partial consistency holds under a given embedding and low-noise assumption, providing insight into when to use a particular embedding into $\\mathbb{R}^{d}$ . Finally, we present a new method to construct (fully) consistent losses with $d\\ll n$ out of multiple problem instances. Our practical approach leverages parallelism to sidestep lower bounds on $d$ . ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Multiclass classification, due to its combinatorial and discontinuous nature, is intractable to optimize directly, which drives machine learners to optimize some nicer surrogate loss. To ensure these surrogates properly \u201ccorrespond\u201d to the discrete classification task, we seek to design consistent surrogates. If one uses a consistent surrogate loss, in the limit of infinite data and model expressivity, one ends up with the same classifications as if one had solved the original intractable problem directly with probability 1. ", "page_idx": 0}, {"type": "text", "text": "Surrogate losses form the backbone of gradient-based optimization for classification tasks. Optimizing a surrogate is easier than direct optimization, but a large dimension $d$ of the surrogate loss $L$ : $\\mathbb R^{d}\\times\\bar{\\mathcal{V}}\\rightarrow\\mathbb R$ can make gradient-based optimization intractable. Therefore, previous literature has operated under the premise that the prediction dimension $d$ should be as low as possible, subject to consistency for the classification task [Ramaswamy and Agarwal, 2016, Finocchiaro et al., 2024, 2020]. For multi-class classification, the lower bound on $d$ is $n-1$ [Ramaswamy and Agarwal, 2016]. ", "page_idx": 0}, {"type": "text", "text": "These previous works implicitly focus on a binary approach to consistency: a surrogate is either consistent for every possible label distribution, or it is not consistent. But there is a way out: lower bounds on the surrogate dimension $d$ rely on edge-cases that rarely show up in reality [Ramaswamy and Agarwal, 2016]. As a result, practitioners are often willing to trade-off the guarantee of consistency in order to improve the computational tractability of optimization. However, we currently lack rigorous analysis tools to analyze many of the partially-consistent surrogates commonly used in practice. Thus, unlike previous works, our work focuses on this more realistic paradigm of partial consistency. We apply our unique approach to rigorously analyze a popular surrogate construction that encompasses methods such as one-hot and binary encoding. Our approach allows for fine-grained control of the trade-off between consistency and dimension. ", "page_idx": 1}, {"type": "text", "text": "Prior works have informally brushed upon the proposed partial-consistency paradigm, without rigorous study. For example, Agarwal and Agarwal [2015] impose a low-noise assumption to construct a surrogate for classification with $d=\\log(n)$ . However, their work does not provide any way to control the consistency-dimension trade-off. Similarly, Struminsky et al. [2018] characterize the excess risk bounds of inconsistent surrogates, which teaches us about the learning rates for inconsistent surrogates, but not under which distributional assumptions we can recover consistency guarantees. ", "page_idx": 1}, {"type": "text", "text": "Using different techniques than both of these approaches, we seek to understand the tradeoffs of consistency, surrogate prediction dimension, and number of problem instances through the use of polytope embeddings which are common in the literature [Wainwright et al., 2008, Blondel et al., 2020]. When embedding outcomes into $d\\ll n$ dimensions, we first show there always exists a set of distributions where hallucinations occur: where the report minimizing the surrogate leads to a prediction $\\hat{y}$ such that the underlying true distribution has no weight on the prediction; that is, $P r[\\bar{Y}=\\hat{y}]=0$ (Theorem 3). Following this, we show that every polytope embedding is partially consistent under strong enough low-noise assumptions (Theorem 5). Finally, we demonstrate through leveraging the embedding structure and multiple problem instances that the mode (in particular, a full rank ordering) over $n$ outcomes embedded into a $\\begin{array}{l}{{\\frac{n}{2}}}\\end{array}$ dimensional surrogate space is elicitable over all distributions via $O(n^{2})$ problem instances (Theorem 10). This alternative approach to recovering consistency is parallelizable, detangling the complexity of gradient computation of one high-dimensional surrogate. ", "page_idx": 1}, {"type": "text", "text": "2 Background and Notation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Let $\\boldsymbol{\\wp}$ be a finite label space, and throughout let $n=|\\mathcal{V}|$ . Define $\\mathbb{R}_{+}^{y}$ to be the nonnegative orthant. Let $\\Delta_{\\mathcal{Y}}=\\{p\\in\\mathbb{R}_{+}^{\\mathcal{Y}}\\mid\\lvert\\lvert p\\rvert\\rvert_{1}=1\\}$ be the set of probability distributions on $\\boldsymbol{\\wp}$ , represented as vectors. We denote the point mass distribution of an outcome $y\\in\\mathcal{V}$ by $\\delta_{y}\\,\\in\\,\\Delta_{y}$ . Let $[d]:=\\{1,\\dots,d\\}$ . In general, we denote a discrete loss by $\\ell:\\mathcal{Y}\\times\\mathcal{Y}\\to\\mathbb{R}_{+}$ with outcomes denoted by $y\\in\\mathcal{V}$ and a surrogate loss by $L:\\mathbb R^{d}\\times\\mathcal{V}\\rightarrow\\mathbb R$ with surrogate reports $u\\in\\mathbb{R}^{d}$ and outcomes $y\\in\\mathcal{V}$ . The surrogate must be accompanied by a link $\\psi:\\mathbb{R}^{d}\\rightarrow\\mathcal{V}$ mapping the convex surrogate model\u2019s predictions back into the discrete target space, and we discuss consistency of a pair $(L,\\psi)$ with respect to the target $\\ell$ . ", "page_idx": 1}, {"type": "text", "text": "For $\\epsilon>0$ , we define an epsilon ball via $B_{\\epsilon}(u)\\,=\\,\\{{x}\\,\\in\\,\\mathbb{R}^{d}\\,\\mid\\,\\|u-x\\|_{2}\\,<\\,\\epsilon\\}$ and $B_{\\epsilon}:=B_{\\epsilon}(\\vec{0})$ . Given a closed convex set $\\mathcal{C}\\subset\\mathbb{R}^{d}$ , we define a projection operation onto $\\mathcal{C}$ via ${\\mathrm{Proj}}_{\\mathcal{C}}(u)~:=$ ar $\\operatorname{g\\,min}_{x\\in c}\\|u-x\\|_{2}$ . Given a closed convex set $\\mathcal{C}\\subset\\mathbb{R}^{d}$ and $u\\,\\in\\,\\mathbb R^{d}$ , we let the set-pointwise distance to be defined as $\\|u-\\mathcal{C}\\|_{2}\\;:=\\;\\|u-\\operatorname{Proj}_{\\mathcal{C}}(u)\\|_{2}$ . Full tables of notation are found in Appendix A. ", "page_idx": 1}, {"type": "text", "text": "2.1 Property Elicitation, Consistency, and Prediction Dimension ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Discrete label prediction requires optimization of a target loss function, $\\ell$ , e.g. multi-class classification and 0-1 loss. When designing surrogate losses, consistency is the key notion of correspondence between surrogate and target loss. Intuitively, consistency implies that minimizing surrogate risk corresponds to solving the target problem. Finocchiaro et al. [2021] show that surrogate loss consistency is a necessary precursor to excess risk bounds and convergence rates. ", "page_idx": 1}, {"type": "text", "text": "Consistency is generally a difficult condition to work with directly. Hence, we will use the notion of calibration, which is equivalent to consistency in our setting with finite outcomes. Our approach follows from the property elicitation literature, which allows us to abstract away from the feature space $\\mathcal{X}$ and focus on the conditional distributions over the labels, $p=\\mathrm{Pr}[Y\\mid X=x]\\in\\Delta_{y}$ [Bartlett et al., 2006, Zhang, 2004, Ramaswamy and Agarwal, 2016, Steinwart, 2007]. In this approach, the central object of study is a property which maps label distributions to reports that minimize the loss. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Definition 1 (Property, Elicits, Level Set). Let $\\mathcal{R}$ be an arbitrary report set. For $\\mathcal{P}\\subseteq\\Delta_{\\mathcal{X}}$ , a property is a set-valued function $\\Gamma:{\\vec{\\mathcal{P}}}\\rightarrow2^{\\mathcal{R}}\\setminus\\{\\mathcal{D}\\}$ , which we denote $\\Gamma:\\mathcal{P}\\overset{}{\\underset{}{\\rightarrow}}\\mathcal{D}$ . A loss $L:\\mathcal{R}\\times\\mathcal{y}\\rightarrow\\mathbb{R}$ elicits the property $\\Gamma$ on $\\mathcal{P}$ if ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\forall\\;p\\in\\mathcal{P},\\;\\Gamma(p)=\\underset{u\\in\\mathcal{R}}{\\arg\\operatorname*{min}}\\,\\mathbb{E}_{Y\\sim p}[L(u,Y)]\\;.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "If $L$ elicits a property, it is unique and we denote it prop $[L]$ . The level set of $\\Gamma$ for report $r$ is the set $\\Gamma_{r}:=\\{p\\in\\mathcal{P}\\mid r=\\Gamma(p)\\}$ . $I f\\,\\mathrm{prop}[L]=\\Gamma$ and $|\\Gamma(p)|=1$ for all $p\\in\\mathcal{P}$ , we say that $L$ is strictly proper for $\\Gamma$ . ", "page_idx": 2}, {"type": "text", "text": "In this work, $\\mathcal{R}=\\mathcal{V}$ for target losses and $\\mathcal{R}=\\mathbb{R}^{d}$ for surrogate losses. ", "page_idx": 2}, {"type": "text", "text": "Once a model is optimized wrt. a surrogate $L$ , it predicts reports in the surrogate space, $\\mathbb{R}^{d}$ . Then, to map surrogate reports to discrete labels, the surrogate loss must be paired with a link, $\\psi:\\mathbb{R}^{d}\\rightarrow\\mathcal{V}$ . Intuitively, a surrogate and link pair $(L,\\psi)$ are calibrated with respect to a target loss $\\ell$ , if the optimal expected surrogate loss when making the incorrect classification (by $\\psi$ ) is strictly greater than the optimal surrogate loss. ", "page_idx": 2}, {"type": "text", "text": "Definition 2 $\\boldsymbol{\\ell}$ -Calibrated Loss). Given discrete loss $\\ell:\\mathcal{Y}\\times\\mathcal{Y}\\to\\mathbb{R}_{+}$ , surrogate loss $L:\\mathbb R^{d}\\times\\mathcal{Y}\\rightarrow$ $\\mathbb{R}$ , and link function $\\psi:\\mathbb{R}^{d}\\rightarrow\\mathcal{V}$ . We say that $(L,\\psi)$ is $\\ell$ -calibrated over $\\mathcal{P}\\subseteq\\Delta_{\\mathcal{X}}$ if, for all $p\\in\\mathcal{P}$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{u\\in\\mathbb{R}^{d}:\\psi(u)\\notin\\mathrm{{prop}}[\\ell](p)}\\mathbb{E}_{Y\\sim p}[L(u,Y)]>\\operatorname*{inf}_{u\\in\\mathbb{R}^{d}}\\mathbb{E}_{Y\\sim p}[L(u,Y)]\\;.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "If $\\mathcal{P}$ is not specified, then we are discussing calibration over $\\Delta{y}$ . In general, when $(L,\\psi)$ is $\\ell$ -calibrated over $\\mathcal{P}$ such that $\\mathcal{P}\\subset\\Delta_{\\mathcal{X}}$ , we say partial calibration holds with respect to $\\mathcal{P}$ . ", "page_idx": 2}, {"type": "text", "text": "Our analysis crucially relies on the ability to specify $\\mathcal{P}$ when invoking the definition of calibration. This is because the surrogates we analyze break the $d=n-1$ lower bound on the dimension of any consistent surrogate loss. So the surrogates will not be calibrated over the whole simplex $\\Delta{y}$ . To aid in our analysis, we use a condition that shows that converging to a property value implies calibration for the target loss itself [Agarwal and Agarwal, 2015]. ", "page_idx": 2}, {"type": "text", "text": "Definition 3 (\u2113-Calibrated Property). Let $\\mathcal{P}\\subseteq\\Delta_{\\mathcal{X}}$ , $\\Gamma:\\mathcal{P}\\rightrightarrows\\mathbb{R}^{d}$ , discrete loss $\\ell:\\mathcal{Y}\\times\\mathcal{Y}\\to\\mathbb{R}_{+}$ , and $\\psi:\\mathbb{R}^{d}\\rightarrow\\mathcal{V}$ . We will say $(\\Gamma,\\psi)$ is $\\ell$ -calibrated for all $p\\in\\mathcal{P}$ and all sequences in $\\{u_{m}\\}$ in $\\mathbb{R}^{d}\\:i f,$ ", "page_idx": 2}, {"type": "equation", "text": "$$\nu_{m}\\rightarrow\\Gamma(p)\\Rightarrow\\mathbb{E}_{Y\\sim p}[\\ell(\\psi(u_{m}),Y)]\\rightarrow\\operatorname*{min}_{r\\in\\mathcal{Y}}\\mathbb{E}_{Y\\sim p}[\\ell(r,Y)]\\;.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Theorem 1 ([Agarwal and Agarwal, 2015, Theorem 3]). Let $\\ell:\\mathcal{Y}\\times\\mathcal{Y}\\to\\mathbb{R}_{+}$ and $\\mathcal{P}\\subseteq\\Delta_{\\mathcal{X}}$ . Let $\\Gamma:\\mathcal{P}\\rightrightarrows\\mathbb{R}^{d}$ and $\\psi:\\mathbb{R}^{d}\\rightarrow\\dot{\\mathcal{X}}$ be such that $\\Gamma$ is elicitable and $(\\Gamma,\\psi)$ is an $\\ell$ -calibrated property over $\\mathcal{P}$ . Let $L:\\mathbb R^{d}\\times\\mathcal{Y}\\rightarrow\\mathbb R$ be a convex function for all $y\\in\\mathcal{V}$ and strictly proper for $\\Gamma$ i.e. $\\mathrm{prop}[L]=\\Gamma$ and $|\\Gamma(p)|=1$ for all $p\\in\\mathcal{P}$ . Then, $(L,\\psi)$ is $\\ell$ -calibrated over $\\mathcal{P}$ . ", "page_idx": 2}, {"type": "text", "text": "Finally, we present the 0-1 loss that we analyze, which is the target loss for multiclass classification. Definition 4 (0-1 Loss). We denote the 0-1 loss by $\\ell_{0-1}:{\\mathcal{V}}\\times{\\mathcal{V}}\\to\\{0,1\\}$ such that $\\ell_{0-1}(y,\\hat{y}):=$ $\\mathbb{1}_{y\\neq\\hat{y}}$ . Observe $\\gamma^{\\mathrm{mode}}(p):=\\mathrm{prop}[\\ell_{0-1}](p)=\\{y\\in\\mathcal{y}|y\\in\\arg\\operatorname*{max}_{y}p_{y}\\}$ . ", "page_idx": 2}, {"type": "text", "text": "3 Polytope Embedding and Existence of Calibrated Regions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Often, discrete outcomes are embedded in continuous space onto the vertices of the simplex via one-hot encoding, or the vertices of the unit cube via binary encoding [Seger, 2018]. Generalizing, we introduce an approach to surrogate construction inspired by Wainwright et al. [2008] and Blondel et al. [2020] that encompasses the aforementioned embedding methods. This construction utilizes embeddings onto the vertices of arbitrary low-dimensional polytopes $\\varphi:\\mathcal{V}\\,\\rightarrow\\,\\mathbb{R}^{d}$ . Then, an embedding scheme naturally induces a large class of loss functions $\\bar{L}_{\\varphi}^{G}$ defined by the embedding, any $G$ -Bregman Divergence, and a link function $\\psi^{\\varphi}$ . ", "page_idx": 2}, {"type": "text", "text": "Our analysis begins by defining a condition stronger than inconsistency that arises when embedding into $d<n-1$ dimensions for multiclass classification. To this end, we introduce the notion of hallucination as a means to characterize the \u201cworst case\u201d behavior of a surrogate pair $(\\S\\ 3.2)$ . In a positive manner, we characterize the calibration regions of various embeddings $\\left(\\S\\ 3.3\\right)$ , which are sets $\\mathcal{P}\\subseteq\\Delta_{\\mathcal{X}}$ such that our surrogate and link pair $(L_{\\varphi}^{G},\\psi^{\\varphi})$ are $\\ell_{}$ -calibrated over $\\mathcal{P}$ . We refer the reader to the Appendix B for omitted full proofs. ", "page_idx": 2}, {"type": "text", "text": "3.1 Polytope Embedding Construction ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A Convex Polytope $P\\subset\\mathbb{R}^{d}$ , or simply a polytope, is the convex hull of a finite number of points $u_{1},\\ldots,u_{n}\\in\\dot{\\mathbb{R}}^{d}$ . An extreme point of a convex set $A$ , is a point $u\\in A$ such that if $u=\\lambda y+(1-\\lambda)z$ with $y,z\\in A$ and $\\lambda\\in[0,1]$ , then $y=u$ and/or $z=u$ . We shall denote by $\\mathrm{vert}(P)$ a polytope\u2019s set of extreme points. A polytope can be expressed by the convex hull of its extreme points, i.e. $P=\\mathrm{conv}\\left(\\mathrm{vert}(P)\\right)$ [Brondsted, 2012, Theorem 7.2]. Additional definitions pertaining to polytopes are used for proofs that are omitted to the appendix, we refer the reader to $(\\S\\ B.1)$ for said definitions. ", "page_idx": 3}, {"type": "text", "text": "We propose the following embedding procedure that allows one to construct surrogate losses with almost any polytope, and any Bregman divergence. ", "page_idx": 3}, {"type": "text", "text": "Construction 1 (Polytope Embedding). Given $\\boldsymbol{\\wp}$ outcomes, $|\\mathcal{V}|=n$ , choose a polytope $P\\subset\\mathbb{R}^{d}$ such that $\\left|\\operatorname{vert}(P)\\right|=n$ . Choose a bijection between $\\boldsymbol{\\wp}$ and $\\mathrm{vert}(P)$ . According to this bijection, assign each vertex a unique outcome so that $\\{v_{y}\\,\\in\\,\\mathbb{R}^{d}|y\\,\\in\\,\\mathcal{V}\\}\\,=\\,\\mathrm{vert}(P)$ . Then the polytope embedding $\\varphi:\\Delta y\\rightarrow P$ is $\\begin{array}{r}{\\varphi(p):=\\sum_{y\\in\\mathcal{y}}p_{y}v_{y}}\\end{array}$ , which is the sum of $p$ -scaled vectors ", "page_idx": 3}, {"type": "text", "text": "Following the work of Blondel [2019] and their proposed Projection-based losses, we use the extremely general class of Bregman divergences (Definition 5) and a polytope embedding $\\varphi$ to define an induced loss $L_{\\varphi}^{G}$ (Definition 6). ", "page_idx": 3}, {"type": "text", "text": "Definition 5 (Bregman Divergence). Given a strictly convex function $G:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}\\quad$ , $D_{G}(u,v):=$ $G(v)-[G(u)+\\langle d G_{v},u-v\\rangle]$ is a Bregman divergence where $d G_{v}$ denotes a subgradient of $G$ at $v$ . For this work, we shall always assume that $\\mathrm{dom}(G)=\\mathbb{R}^{d}$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 6 $((D_{G},\\varphi)$ Induced Loss). Given a Bregman divergence $D_{G}$ and a polytope embedding $\\varphi$ , we say $(D_{G},\\varphi)$ induces a loss $L_{\\varphi}^{G}:\\mathbb{R}^{d}\\times\\mathcal{V}\\rightarrow\\mathbb{R}$ defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\nL_{\\varphi}^{G}(u,y):=D_{G}(u,v_{y})=G(v_{y})-\\left[G(u)+\\langle d G_{v_{y}},u-v_{y}\\rangle\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We show that for any $p\\in\\Delta{}_{\\mathcal{V}}$ , the report that uniquely minimizes the expectation of the loss $L_{\\varphi}^{G}$ is $\\varphi(p)$ , the embedding point of $p$ . Furthermore, the polytope $P$ contains all of, and only the minimizing reports in expectation under L\u03c6G. ", "page_idx": 3}, {"type": "text", "text": "Proposition 2. For a given induced loss $L_{\\varphi}^{G}$ , the unique report which minimizes the expected loss is $u^{\\ast}:=\\arg\\operatorname*{min}_{u\\in\\mathbb{R}^{d}}$ $\\mathbb{E}_{Y\\sim p}[L_{\\varphi}^{G}(u,Y)]=\\dot{\\varphi}(p)$ such that $u^{*}\\in P$ . Furthermore, every $\\hat{u}\\in P$ is $a$ minimizer of $\\mathbb{E}_{Y\\sim\\hat{p}}[L_{\\varphi}^{G}(u,Y)]$ for some $\\hat{p}\\in\\Delta y$ . ", "page_idx": 3}, {"type": "text", "text": "We now define the maximum a posteriori (MAP) link, which will be used in conjunction with an induced loss $L_{\\varphi}^{G}$ to form a surrogate pair for the 0-1 loss. The MAP link projects surrogate predictions onto the polytope $P$ , then links to the nearest vertex of $P$ , and is commonly used in the literature [Tsochantaridis et al., 2005, Blondel, 2019, Xue et al., 2016]. Since the MAP link performs a projection, one may ask if this is computationally challenging; fortunately this operation is computationally feasible because the polytope is convex [Blondel, 2019]. ", "page_idx": 3}, {"type": "text", "text": "Definition 7 (MAP Link). Let $\\varphi$ be a polytope embedding from $\\Delta{y}$ to $P$ . The MAP link $\\psi^{\\varphi}$ : $\\mathbb{R}^{d}\\rightarrow\\mathcal{Y}$ is defined as $\\begin{array}{r}{\\psi^{\\varphi}(u)=\\arg\\operatorname*{min}_{y\\in\\mathcal{Y}}||P r o j_{P}(u)-v_{y}||_{2}}\\end{array}$ . The level set of the link for $y$ is $\\psi_{y}^{\\varphi}=\\{u\\in\\mathbb{R}^{d}|y=\\psi^{\\varphi}(u)\\}$ . We break ties arbitrarily but deterministically. ", "page_idx": 3}, {"type": "text", "text": "3.2 Hallucination Regions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Since our polytope embedding violates surrogate dimension bounds, calibration for 0-1 loss will not hold for all distributions. In particular, we show there always exists some distribution $p$ such that $p_{y}\\,=\\,0$ yet $\\mathbb{E}_{Y\\sim p}[L_{\\varphi}^{G}(u,Y)]$ is minimized at some $u$ such that $\\psi^{\\varphi}(u)\\,=\\,y$ . This implies a \u201cworst case\u201d inconsistency where the reported outcome could never actually occur with respect to our embedding of $n$ events via $\\varphi$ into $\\mathrm{vert}(P)$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 8 (Hallucination). Given $(L,\\psi)$ such that $L:\\mathbb R^{d}\\times\\mathcal{Y}\\to\\mathbb R_{+}$ , $|\\mathcal{V}|\\,=\\,n,$ , $d<n$ , and $\\psi:\\mathbb{R}^{d}\\rightarrow\\mathcal{V}$ , we say that a hallucination occurs at a surrogate report $u\\in\\mathbb{R}^{d}$ if, for some $p\\in\\Delta_{\\mathcal{V}}$ , $u\\in\\arg\\operatorname*{min}_{\\hat{u}\\in\\mathbb{R}^{d}}\\mathbb{E}_{\\mathcal{Y}\\sim p}[L(\\hat{u},Y)]$ and $\\psi(u):=y$ but $p_{y}\\,=\\,0$ . We denote by $\\mathcal{H}\\subseteq P\\subset\\mathbb{R}^{d}$ as the hallucination region as the elements of $P$ at which hallucinations can occur. ", "page_idx": 3}, {"type": "image", "img_path": "xCIbVuXwPM/tmp/cd3ef77f5efe4ce7f501e0d94e1ceb87c889a239d7e79ebd889aadb951143134.jpg", "img_caption": ["Figure 1: (Left) Mode level sets of $\\Delta{y}$ where $\\mathcal{V}=\\{a,b,c,d\\}$ embedded into a two dimensional unit cube. The center red point denotes the origin $(0,0)$ which is the hallucination region. (Right) An embedding of $\\Delta{y}$ where $\\mathcal{V}=\\{a,b,c,d,e,f\\}$ into a three-dimensional permutahedron: the beige region expresses strict calibration regions, the light pink regions expresses regions with inconsistency, and the auburn region expresses regions with hallucinations. For example, consider the report $u={\\vec{0}}$ . Since losses are convex, if $\\boldsymbol{p}=(0,\\overline{{\\frac{1}{2}}},0,0,\\frac{1}{2},0)$ , then conv $(\\{b,e\\})$ (dashed grey) is optimal, which includes $u$ . However, $\\vec{0}$ is also contained in conv $(\\{a,d\\})$ which is optimal for the distribution $\\begin{array}{r}{p^{\\prime}=(\\frac{1}{2},0,0,\\frac{1}{2},0,0)}\\end{array}$ . Therefore, we cannot distinguish the optimal reports for a hallucination at $\\vec{0}$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "We express the subspace of the surrogate space where hallucinations can occur as the hallucination region denoted by $\\mathcal{H}$ . In Theorem 3, we characterize the hallucination region for any polytope embedding while using the surrogate pair $(L_{\\varphi}^{G},\\psi^{\\varphi})$ and show that $\\mathcal{H}$ is never empty. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3. For any given pair $(L_{\\varphi}^{G},\\psi^{\\varphi})$ and $\\ell_{0-1}$ with embedding dimension $d<n-1$ ; it holds that $\\mathcal{H}=\\cup_{y\\in\\mathcal{Y}}\\mathrm{conv}\\left(\\mathrm{vert}(P)\\setminus\\{v_{y}\\}\\right)\\cap\\psi_{y}^{\\varphi}$ and furthermore $\\mathcal{H}\\ne\\mathcal{O}$ . ", "page_idx": 4}, {"type": "text", "text": "Sketch. Fix $y\\ \\in\\ \\mathcal{V}$ . We abuse notation and write $\\mathrm{vert}(P_{-y})\\;:=\\;\\mathrm{vert}(P)\\,\\setminus\\,\\{v_{y}\\}$ . Observe conv $\\begin{array}{r}{(\\mathrm{vert}(P_{-y}))\\cap\\psi_{y}^{\\varphi}\\subseteq\\mathcal{H}}\\end{array}$ since any point in this set can be expressed as a convex combination without needing vertex $v_{y}$ implying there is a distribution embedded by $\\varphi$ to said point which has no weight on $y$ . We seek to show that $\\mathcal{H}\\subseteq\\cup_{y\\in\\mathcal{y}}\\mathrm{conv}\\left(\\mathrm{vert}(P_{-y})\\right)\\cap\\psi_{y}^{\\varphi}$ . Assume there exists a point $u\\not\\in{\\mathrm{conv}}\\left({\\mathrm{vert}}(P)\\setminus v_{y}\\right)\\cap\\psi_{y}^{\\varphi}$ such that there exists some $p\\in\\Delta_{\\mathcal{Y}}$ where $\\varphi(p)=u$ , $p_{y}=0$ , and $\\psi^{\\varphi}(u)\\,=\\,y$ . Since $\\psi^{\\varphi}(u)\\,=\\,y$ and $u\\,\\not\\in\\,\\mathrm{conv}\\,(\\mathrm{vert}(P_{-y}))\\cap\\psi_{y}^{\\varphi}$ , it must be the case that $u\\not\\in\\mathrm{conv}\\left(\\mathrm{vert}(P_{-y})\\right)$ . However, that implies that $u$ is strictly in the vertex figure and thus must have weight on the coefficient for $y$ . Thus, forming a contradiction that $p_{y}=0$ which implies that $\\mathcal{H}\\subseteq\\cup_{y\\in\\mathcal{y}}\\mathrm{conv}\\left(\\mathrm{vert}(P_{-y})\\right)\\cap\\psi_{y}^{\\varphi}$ . Finally, using Helly\u2019s Theorem [Rockafellar, 1997, Corollary 21.3.2] we show that $\\cap_{y\\in\\mathcal{y}}\\mathrm{conv}\\left(\\mathrm{vert}(P)\\setminus v_{y}\\right)\\neq\\emptyset$ , which implies the non-emptiness of $\\mathcal{H}$ as well. \u53e3 ", "page_idx": 4}, {"type": "text", "text": "Theorem 3 suggests that using machine learning in high-risk settings such as medical and legal applications while violating the known $n-1$ dimensional bound for surrogate losses in multiclass classification is inherently ill-advised without human intervention given the possibility for hallucinations. Furthermore, hallucinations may be forced by the target loss, as in the case of Hamming loss (see Appendix C). In these cases practitioners should carefully consider the choice of target loss. We conjecture that hallucinations are common for many structured prediction losses. However this is not a concern in our primary loss of study of multi-class classification. ", "page_idx": 4}, {"type": "text", "text": "3.3 Calibration Regions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Ideally, we would like calibration to hold over the entire simplex since that would imply minimizing surrogate risk would always correspond to solving the target problem regardless of the true underlying distribution. We observe that the mode\u2019s embedded level sets in the polytope overlap (see Figure 1L), which is unsurprising given that we are violating the lower bounds on surrogate prediction for the mode and hence calibration does not hold over the entire simplex. Since $|2^{y}\\setminus\\overline{{\\{\\mathcal{D}\\}}}\\}$ is a finite set, we know that the number of unique mode level sets is finite. Although every point in the polytope is a minimizing report for some distribution, if multiple distributions with non-intersecting mode sets are embedded to the same point, there is no way to define a link function that is correct in all cases. However, if the union of mode sets for the $p$ \u2019s mapped to any $u\\in P$ is a singleton, regardless of the underlying distribution\u2020, a link $\\psi$ would be calibrated over the union if it mapped $u$ to the mentioned singleton. Given $(L,\\psi)$ , $\\varphi$ , and a target loss $\\ell$ , we define strict calibrated regions as the points for which calibration holds regardless of the actual distribution realized, which are possible at said points. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Definition 9 (Strict Calibrated Region). Suppose we are given $(L,\\psi)$ , $\\varphi$ , and a target loss $\\ell$ . We say $R\\subseteq P$ is $a$ strict calibrated region via $(L,\\psi)$ with respect to \u2113i $f(L,\\psi)$ is $\\ell$ -calibrated for all $p\\stackrel{.}{\\in}\\varphi^{-1}(R):=\\{p\\in\\Delta_{\\mathcal{Y}}:\\varphi(p)\\in\\bar{R}\\}$ . ", "page_idx": 5}, {"type": "text", "text": "For any $y\\in\\mathcal{V}$ , we define $R_{y}:=R\\cap\\psi_{y}$ . We let $R_{3}:=\\cup_{y\\in\\mathcal{y}}R_{y}$ . ", "page_idx": 5}, {"type": "text", "text": "By violating lower bounds, we are in a partially consistent paradigm where surrogate reports do not necessarily correspond to a unique distribution $p$ . However, strict calibration regions allow us to check whether or not the loss is calibrated for the distribution $p$ generating the data \u2014 even without explicit access to $p$ . One simply has to check whether the report $u$ is in $R_{\\mathcal{Y}}$ . ", "page_idx": 5}, {"type": "text", "text": "In Theorem 4, regardless of one\u2019s chosen $P$ , we show that there always exists a non-zero Lebesgue measurable strict calibration region and that $(L_{\\varphi}^{G},\\psi_{-}^{\\varphi})$ is calibrated for the 0-1 loss overall distributions embedded into the strict calibration region. This result shows that our surrogate and link construction for any $d$ , always yields discernible calibration regions \u2014 lending support to the practical use and study of these surrogates. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4. Let $D_{G}$ be a Bregman divergence, $\\varphi$ be any polytope embedding, $\\psi^{\\varphi}$ be the MAP link, and $L_{\\varphi}^{G}$ be the loss induced by $(D_{G},\\varphi)$ . There exists a $\\mathcal{P}\\subseteq\\Delta_{\\mathcal{X}}$ with non-zero Lebesgue measure and $\\varphi(\\mathcal{P})\\subseteq R_{\\mathcal{Y}}$ via $(L_{\\varphi}^{G},\\psi^{\\varphi})$ with respect to $\\ell_{0-1}$ . ", "page_idx": 5}, {"type": "text", "text": "Although strict calibration regions $R_{y}$ exist for each outcome $y\\in\\mathcal{V}$ via the polytope embedding, tightly characterizing strict calibration regions is non-trivial. Since the level sets of elicitable properties are convex within the underlying simplex, characterizing the strict calibration regions becomes a collision detection problem, which is often computationally hard. ", "page_idx": 5}, {"type": "text", "text": "4 Restoring Inconsistent Surrogates via Low-Noise Assumptions ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Looking towards application, we refine our results on the existence of strict calibration regions by examining a low-noise assumption, which provides an interpretable calibration region $(\\S\\,4.1)$ . We show which low-noise assumptions imply calibration when embedding $2^{d}$ outcomes into $d$ dimensions and $d!$ outcomes into $d$ dimensions $(\\S\\,4.2)$ . We refer the reader to Appendix B for omitted proofs. ", "page_idx": 5}, {"type": "text", "text": "4.1 Calibration via Low Noise Assumptions ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We demonstrate that every polytope embedding leads to calibration under some low-noise assumption. Our results enable practictioners to choose the dimension $d$ , unlike in previous works. Following previous work [Agarwal and Agarwal, 2015], we define a low noise assumption to be a subset of the probability simplex on the label distribution parameterized by $\\hat{\\alpha}$ : $\\Theta_{\\hat{\\alpha}}=\\bar{\\{}p\\in\\Delta_{\\mathcal{Y}}\\vert\\,\\operatorname*{max}_{y\\in\\mathcal{y}}p_{y}\\geq$ $1-\\hat{\\alpha}\\}$ where $\\hat{\\alpha}\\in[0,1]$ . This noise assumption can be understood as Massart noise [Massart and N\u00e9d\u00e9lec, 2006] in the multiclass setting. ", "page_idx": 5}, {"type": "text", "text": "Given $\\alpha\\in[0,1]$ and $y\\in\\mathcal{V}$ , we define the set $\\Psi_{\\alpha}^{y}=\\{(1-\\alpha)\\delta_{y}+\\alpha\\delta_{\\hat{y}}\\mid\\hat{y}\\in\\mathcal{y}\\}$ . With an embedding $\\varphi$ onto $P$ , we define the set $P_{\\alpha}^{y}:=\\varphi(\\mathrm{conv}\\left(\\Psi_{\\alpha}^{y}\\right))$ , a scaled version of $P$ anchored at $v_{y}$ , that moves vertices $(1-\\alpha)$ proportionally towards $y$ , (Figure 2R). ", "page_idx": 5}, {"type": "text", "text": "Theorem 5. Let $D_{G}$ be a Bregman divergence, $\\varphi$ be any polytope embedding, and $L_{\\varphi}^{G}$ be the loss induced by $(D_{G},\\varphi)$ . There exists an $\\alpha\\in[0,.5)$ such that for the link $\\psi^{\\varphi,\\alpha}(u)=\\arg\\operatorname*{inin}_{y\\in\\mathcal{y}}\\|u-$ $P_{\\alpha}^{y}\\|_{2}$ , $(L_{\\varphi}^{G},\\psi^{\\varphi,\\alpha})$ is $\\ell_{0-1}$ -calibrated over the distributions $\\Theta_{\\alpha}:=\\{p\\in\\Delta y\\mid\\operatorname*{max}_{y\\in\\mathcal{y}}p_{y}\\geq1-\\alpha\\}$ . ", "page_idx": 5}, {"type": "text", "text": "Proof. Part 1 (Choosing $\\alpha\\in[0,.5)]$ ): By Theorem 4, there exists an $\\epsilon>0$ such that $B_{\\epsilon}(v_{y})\\cap P\\subseteq$ $R_{y}$ for all $y\\in\\mathcal{V}$ . Given that $\\mathrm{vert}(P)$ are unique points, there exists a sufficiently small $\\epsilon^{\\prime}>0$ such that $B_{\\epsilon^{\\prime}}(v)\\cap B_{\\epsilon^{\\prime}}(\\hat{v})=\\varnothing$ for all $v,{\\hat{v}}\\in\\operatorname{vert}(P)$ where $v\\neq\\hat{v}$ . Let $\\epsilon^{\\prime\\prime}=\\operatorname*{min}\\left(\\epsilon,\\dot{\\epsilon}^{\\prime}\\right)$ . For any $y\\in\\mathcal{V}$ , observe the set conv $\\left(\\Psi_{\\alpha}^{y}\\right)$ , defined using any $\\alpha\\in[0,.5)$ , is a scaled-down translated unit simplex and that for all $p\\in\\mathrm{conv}\\left(\\Psi_{\\alpha}^{y}\\right)\\subset\\Delta_{y}$ it holds that $\\bar{y}=\\bar{\\mathrm{mode}}(p)$ . ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "We shall show that for some sufficiently small $\\alpha\\;\\in\\;[0,.5)$ , $P_{\\alpha}^{y}$ is a scaled down version of $P$ positioned at the respective vertex $v_{y}$ . Furthermore, we shall show that $P_{\\alpha}^{y}\\subset B_{\\epsilon^{\\prime\\prime}}(v_{y})\\cap P\\subseteq R_{y}$ for all $y\\in\\mathcal{V}$ . Observe that by linearity of $\\varphi$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\textstyle\\operatorname*{\\mathcal{P}}_{\\alpha}:=\\varphi(\\operatorname{conv}\\left(\\Psi_{\\alpha}^{y}\\right))=\\operatorname{conv}\\left(\\varphi(\\{(1-\\alpha)\\delta_{y}+\\alpha\\delta_{\\bar{y}}|\\hat{y}\\in\\mathcal{Y}\\})\\right)=\\operatorname{conv}\\left(\\{(1-\\alpha)v_{y}+\\alpha v_{\\bar{y}}|\\hat{y}\\in\\mathcal{Y}\\}\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and hence, $P_{\\alpha}^{y}$ is a scaled version of $P$ positioned at $v_{y}$ . Hence for some sufficiently small $\\alpha$ , $(1-\\alpha)v_{y}+\\bar{\\alpha}v_{\\hat{y}}\\in B_{\\epsilon^{\\prime\\prime}}(v_{y})$ for all $\\hat{y}$ and hence $P_{\\alpha}^{y}\\subseteq B_{\\epsilon^{\\prime\\prime}}(v_{y})\\subseteq R_{y}$ . With said sufficiently small $\\alpha$ , define $\\psi^{\\varphi,\\alpha}$ and the respective sets conv $\\left(\\Psi_{\\alpha}^{y}\\right)$ for each $y\\in\\mathcal{V}$ . Using the previous $\\alpha$ , define the set $\\Theta_{\\alpha}$ as well. ", "page_idx": 6}, {"type": "text", "text": "Part 2 (Showing Calibration): Recall, by Proposition 2, for any $p\\in\\Delta y$ , $u=\\varphi(p)$ minimizes the expected surrogate loss $\\mathbb{E}_{\\mathcal{Y}\\sim p}[L_{\\varphi}^{G}(u,Y)]$ . For any fixed $y\\in\\mathcal{V}$ , observe that conv $\\{(1-\\alpha)\\delta_{y}+\\alpha\\delta_{\\hat{y}}\\mid$ $\\hat{y}\\,\\in\\,\\mathcal{Y}\\}=\\,\\{p\\,\\in\\,\\Delta y\\,:\\,p_{y}\\,\\geq\\,\\mathrm{i}-\\alpha\\}\\,\\subset\\,\\Delta y$ and hence, by Proposition 2, $\\cup_{y\\in{\\ y}}P_{\\alpha}^{y}$ contains all of the minimizing surrogate reports with respect to $\\Theta_{\\alpha}$ . By our choice of $\\alpha$ and the construction of $\\psi_{\\alpha}^{P}$ , every $u\\,\\in\\,\\cup_{y\\in\\mathcal{y}}P_{\\alpha}^{y}$ is linked to the proper unique mode outcome since $\\cup_{y\\in{\\mathcal{Y}}}P_{\\alpha}^{y}\\subseteq R_{\\mathcal{Y}}$ Assuming a low-noise condition where $p\\in\\Theta_{\\alpha}$ , any $u\\notin\\cup_{y\\in\\mathfrak{y}}P_{\\alpha}^{y}$ is never optimal for any low-noise distribution. In such cases, we project the point to the nearest $P_{\\alpha}^{y}$ as a matter of convention. Given that calibration is a result pertaining to minimizing reports, this design choice is non-influential. Finally, since every $\\cup_{y\\in{\\mathcal{Y}}}P_{\\alpha}^{y}\\subseteq R{\\mathrm{y}}$ , by the definition of strict calibration region, it holds that $(L_{\\varphi}^{G},\\psi^{\\varphi,\\alpha})$ is $\\ell_{0-1}$ -calibrated for $\\Theta_{\\alpha}$ . \u53e3 ", "page_idx": 6}, {"type": "text", "text": "4.2 Embedding into the Unit Cube and Permutahedron under Low-Noise ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we demonstrate embedding onto the unit cube and the permutahedron [Blondel et al., 2020, Seger, 2018]. We show that by embedding $2^{d}$ outcomes into a $d$ dimensional unit cube $P^{\\square}$ , $(L_{\\varphi}^{G},\\psi^{\\bar{D}^{\\bar{\\Omega}},\\alpha})$ is calibrated over $\\Theta_{\\alpha}$ for all $\\alpha\\in[0,\\frac{1}{2})$ . Furthermore, we found that by embedding $d!$ outcomes into a $d$ dimensional permutahedron $P^{w}$ , $(L_{\\varphi}^{G},\\psi^{P^{w},\\alpha})$ is calibrated for $\\Theta_{\\alpha}$ for $\\alpha\\in[0,\\frac{1}{d})$ . Theorem 6 enables us to simultaneously study the aforementioned embeddings. ", "page_idx": 6}, {"type": "text", "text": "Theorem 6. Let $D_{G}$ be a Bregman divergence, $\\varphi$ be any polytope embedding, and $L_{\\varphi}^{G}$ be the loss induced by $(D_{G},\\varphi)$ . Fix $\\alpha\\,\\in\\,[0,.5)$ and with it define $\\Theta_{\\alpha}$ . If for all $y,\\hat{y}\\,\\in\\,\\mathcal{Y}$ such that $y\\ne\\hat{y}$ it holds that $P_{\\alpha}^{y}\\cap P_{\\alpha}^{\\hat{y}}=\\emptyset$ , then $(L_{\\varphi}^{G},\\psi^{\\varphi,\\alpha})$ is $\\ell_{0-1}$ -calibrated for $\\Theta_{\\alpha}$ where $\\psi^{\\varphi,\\alpha}(u)=$ arg $\\mathrm{min}_{y\\in\\mathcal{Y}}\\|u-P_{\\alpha}^{y}\\|_{2}$ . ", "page_idx": 6}, {"type": "text", "text": "Proof. Pick an $\\alpha$ such that for all $y,\\hat{y}\\in\\mathcal{Y}$ , $P_{\\alpha}^{y}\\cap P_{\\alpha}^{\\hat{y}}=\\emptyset$ . Define $\\Theta_{\\alpha}$ and $\\psi^{\\varphi,\\alpha}$ accordingly. For $p\\in\\Theta_{\\alpha}$ and some $y\\in\\mathcal{V}$ , say a sequence $\\{u_{m}\\}$ converges to $\\mathrm{prop}[L_{\\varphi}^{G}](p)=\\varphi(p)\\in P_{\\alpha}^{y}$ , where the equality follows from Proposition 2. Given that each $P_{\\alpha}^{y}$ is closed and pairwise disjoint, there exists some $\\hat{\\epsilon}>0$ such that for all $y,\\hat{y}\\in\\mathcal{Y}$ where $y\\ne\\hat{y}$ , it also holds that $(P_{\\alpha}^{y}+B_{\\hat{\\epsilon}})\\cap(P_{\\alpha}^{\\hat{y}}+B_{\\hat{\\epsilon}})=\\varnothing$ where $^+$ denotes the Minkowski sum. Since $\\{u_{m}\\}$ converges to $\\varphi(p)$ , there exists some $N\\in\\mathbb N$ such that for all $n\\geq N$ , $\\|u_{n}-\\varphi(p)\\|_{2}<\\hat{\\epsilon}$ . By the definition of $\\psi^{\\varphi,\\alpha}$ , any $u_{n}$ where $n\\geq N$ will be mapped to $y$ , the correct unique report given that prop $[L_{\\varphi}^{G}](p)\\in P_{\\alpha}^{y}$ . Hence, $(\\mathrm{prop}[L_{\\varphi}^{G}],\\psi^{\\varphi,\\alpha})$ is $\\ell_{0-1}$ -calibrated property with respect to $\\Theta_{\\alpha}$ . Finally, since $L_{\\varphi}^{G}$ is strictly proper for $\\mathrm{prop}[L_{\\varphi}^{G}]$ , by Theorem 1, we have that $(L_{\\varphi}^{G},\\psi^{\\varphi,\\alpha})$ is $\\ell_{0-1}$ -calibrated for $\\Theta_{\\alpha}$ . \u53e3 ", "page_idx": 6}, {"type": "text", "text": "Unit Cube Define a unit cube in $d$ -dimensions by $P^{\\perp}:=\\,\\mathrm{conv}\\left(\\{-1,1\\}^{d}\\right)$ . Binary encoding outcomes into the elements of $\\{-1,1\\}^{d}$ (the vertices of a unit cube) is a commonly used method in practice (e.g., [Seger, 2018, Yu and Blaschko, 2018]). We show that calibration holds under a low noise assumption of $\\Theta_{\\alpha}$ when $\\alpha<.5$ . ", "page_idx": 6}, {"type": "text", "text": "Corollary 7. Let $\\varphi$ be an embedding from $2^{d}$ outcomes into the vertices of $P^{\\square}$ in $d$ -dimensions and define an induced loss $L_{\\varphi}^{G}$ . Fix $:\\alpha\\in[0,.5)$ and define $\\Theta_{\\alpha}$ . $(L_{\\varphi}^{G},\\psi^{P^{\\square},\\alpha})$ is $\\ell_{0-1}$ -calibrated for $\\Theta_{\\alpha}$ . ", "page_idx": 6}, {"type": "text", "text": "Corollary 7 suggests that binary encoding is an appropriate methodology when one has a prior over the data that the mode of the label distribution $\\bar{\\mathrm{Pr}}[\\bar{Y_{\\mathbf{\\Lambda}}}\\mid X=x]$ is greater than half for all $x\\in\\mathscr{X}$ . ", "page_idx": 6}, {"type": "image", "img_path": "xCIbVuXwPM/tmp/9b10321335208d883693035e655edea8c0b3bba6d918f8097fb99a8932e33832.jpg", "img_caption": ["Figure 2: (Left) Corners represent the strict calibration regions for $\\Theta_{\\alpha}$ where $\\mathcal{V}\\,=\\,\\{a,b,c,d\\}$ is embedded into a two dimensional unit cube such that $\\alpha=.25$ . (Right) Auburn regions show that strict calibration holds for $\\Theta_{\\alpha}$ where $\\mathcal{V}=\\{a,b,c,d,e,f\\}$ is embedded into a three-dimensional permutahedron such that $\\begin{array}{r}{\\alpha=\\frac{1}{3}-\\epsilon}\\end{array}$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Interestingly, the bound of $\\alpha$ is not dependent on the dimension of $d$ . We now present a result for embedding outcomes into a factorially lower dimension via the permutahedron. Intuitively, ranking can be recast as a multiclass classification problem, in which case the outcomes are orderings of the $d$ possible labels. ", "page_idx": 7}, {"type": "text", "text": "Permutahedron Let $\\textstyle S_{d}$ express the set of permutations on $[d]$ . The permutahedron associated with a vector $w\\in\\mathbb{R}^{d}$ is defined to be the convex hull of the permutations of the indices of $w$ , i.e., $P^{w}:=\\operatorname{conv}\\big(\\{\\pi(w)\\mid\\pi\\in S_{d}\\}\\big)\\subset\\mathbb{R}^{d}$ . The permutahedron may serve as an embedding from $d!$ outcomes into $d$ -dimensions; it is a natural choice for embedding full rankings over $d$ items. ", "page_idx": 7}, {"type": "text", "text": "sCuocrho tllhaart $\\varphi$ mw $d!$ roeu n.t oF itxh o. f $P^{w}$ ni niss $\\begin{array}{r}{\\stackrel{}{w}=(0,\\frac{'1}{\\beta d},\\frac{2}{\\beta d},\\dots,\\frac{d-1}{\\beta d})\\stackrel{\\smile}{\\in}\\mathbb{R}^{d}}\\end{array}$ $\\beta={\\frac{d-1}{2}}$ $\\alpha\\in[0,\\frac{1}{d})$ $(L_{\\varphi}^{G},\\psi^{P^{w},\\alpha})$ $\\ell_{0-1}$ -calibrated over $\\Theta_{\\alpha}$ . ", "page_idx": 7}, {"type": "text", "text": "The calibration region in Corollary 8 show that consistency in $\\Theta_{\\alpha}$ shrinks exponentially in $d$ . Unless one has a prior that the data follows some form of a power distribution, Corollary 8 suggests not to factorially embed outcomes. ", "page_idx": 7}, {"type": "text", "text": "5 Elicitation in Low Dimensions with Multiple Problem Instances ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The tools developed in previous sections now enable us to address the setting in which we require full consistency, $\\mathcal{P}=\\Delta_{\\mathcal{X}}$ , but also desire surrogate prediction dimension $d\\ll n-1$ . We side-step the $n-1$ lower bound by utilizing multiple problem instances and aggregation of the outputs. Although cumulatively we have a larger surrogate prediction dimension than $n-1$ , each individual problem instance has a less than $n-1$ surrogate prediction dimension. This approach is well-motivated since it allows for distributed computing of separate, smaller models which leads to faster convergence overall since in general optimization is at least $p o l y(d)$ . Previous work such as Ramaswamy et al. [2014] has explored the consistency of multiclass problem reductions; however, we take a different, geometrically motivated, approach. ", "page_idx": 7}, {"type": "text", "text": "Definition 10. Extending Definition $^{\\,l}$ , we say a loss and link pair $(L,\\psi)$ , where $L:\\mathbb R^{d}\\times\\mathcal{Y}\\rightarrow\\mathbb R$ and $\\psi\\ :\\ \\mathbb{R}^{d}\\ \\rightarrow\\ \\mathcal{X},$ , elicits a property $\\Gamma\\;:\\;{\\mathcal{P}}\\;\\rightrightarrows\\;\\mathcal{D}$ on $\\mathcal{P}\\ \\subseteq\\ \\Delta_{\\mathcal{Y}}\\ i f\\ \\forall\\ \\,p\\ \\in\\ \\mathcal{P}$ , $\\Gamma(p)\\;=\\;$ $\\begin{array}{r}{\\psi(\\arg\\operatorname*{min}_{u\\in\\mathbb{R}^{d}}\\mathbb{E}_{Y\\sim p}[L(u,Y)])}\\end{array}$ . ", "page_idx": 7}, {"type": "text", "text": "Definition 11 $((n,d,m)$ -Polytope Elicitable). Suppose we are given a property $\\gamma:\\mathcal{P}\\overset\\rightarrow{\\mathcal{D}}$ such that $\\mathcal{P}\\subseteq\\Delta_{\\mathcal{X}}$ and $|\\mathcal{V}|=n$ finite outcomes. Say we have m unique polytope embeddings $\\{\\varphi_{j}:\\Delta_{\\mathcal{V}}\\rightarrow$ $\\mathbb{R}^{d}\\}_{j=1}^{m}$ where $d<n-1$ , and a set of induced losses $\\{L_{\\varphi_{j}}^{G}\\}_{j=1}^{m}$ and links $\\psi_{j}:\\mathbb{R}^{d}\\rightarrow B_{j}$ defined wrt. $\\varphi_{j}$ , where $B_{j}$ is an arbitrary report set. For each $j\\in[m]$ , assume the pair $(L_{\\varphi_{j}}^{G},\\psi_{j})$ elicits the property $\\Gamma_{j}:\\mathscr{P}\\overset{}{\\rightarrow}\\mathscr{B}_{j}$ . If there exists $a$ function $\\Upsilon:B_{1}\\times\\cdot\\cdot\\times B_{m}\\rightrightarrow y$ such that for any $p\\in\\Delta{}_{\\mathcal{V}}$ it holds that $\\Upsilon(\\Gamma_{1}(p),\\stackrel{*}{,}\\cdot\\cdot\\cdot,\\Gamma_{m}(p))=\\gamma(p)$ , we say that $\\gamma$ is $(n,d,m)$ -Polytope Elicitable over $\\mathcal{P}$ . ", "page_idx": 7}, {"type": "text", "text": "Equivalently, we will also say that the pair $(\\{(L_{\\varphi_{j}}^{G},\\psi_{j})\\}_{j=1}^{m},\\Upsilon)\\left(n,d,m\\right)$ -Polytope elicits the property $\\gamma$ with respect to $\\mathcal{P}$ . ", "page_idx": 7}, {"type": "image", "img_path": "xCIbVuXwPM/tmp/0436bdbea81c362af0316ec6a11da0ec461caa311f8c364118f8d770fa401d32.jpg", "img_caption": ["Figure 3: Four outcomes embedded in $\\mathbb{R}^{2}$ in two different ways, with the minimizing reports $\\bullet$ for a distribution $p$ .\" (Left) Configuration $\\varphi_{1}$ with $\\bullet$ at $(-.5,.3)$ implying $p_{a}>p_{d}$ and $p_{b}>p_{c}$ . (Right) Configuration $\\varphi_{2}$ with $\\bullet$ at $(0,0)$ implying $p_{a}=p_{b}$ and $p_{c}=p_{d}$ . This implies the true distribution is $p=(0.4,0.4,0.1,0.1)$ .\" "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "We shall express a $d$ -cross polytope by $P^{\\oplus}\\;:=\\;\\mathrm{conv}\\left(\\{\\pi((\\pm1,0,\\ldots,0))\\;\\;|\\;\\;\\pi\\;\\in\\;S_{d}\\}\\right)$ where $(\\pm1,0,\\dots,0)\\in\\mathbb{R}^{d}$ . Observe that a $d$ -cross polytope has $2d$ vertices. For any vertex of a ${\\mathrm{d}}\\cdot$ -cross polytope $v\\in{\\mathrm{vert}}(P^{\\oplus})$ , we shall say that $(v,-v)$ forms a diagonal vertex pair. ", "page_idx": 8}, {"type": "text", "text": "Lemma 9. Say we are given a cross-polytope embedding $\\varphi:\\Delta_{2d}\\to P^{\\oplus}$ and induced loss $L_{\\varphi}^{G}$ . Let $(v_{a_{i}},v_{b_{i}})$ , be the $i^{t h}$ diagonal pair (i.e. $\\varphi(\\delta_{a_{i}})\\,=\\,v_{a_{i}},$ ). Define the property $\\Gamma^{\\varphi}:\\Delta_{2d}\\rightarrow B$ element-wise by ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\Gamma^{\\varphi}(p)_{i}:=\\left\\{\\begin{array}{l l}{(<,a_{i},b_{i})}&{i f p_{a_{i}}<p_{b_{i}}}\\\\ {(>,a_{i},b_{i})}&{i f p_{a_{i}}>p_{b_{i}}}\\\\ {(=,a_{i},b_{i})}&{i f p_{a_{i}}=p_{b_{i}}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Furthermore define the link $\\psi^{P^{\\oplus}}:\\mathbb{R}^{d}\\to B$ with respect to each diagonal pair as ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\psi(u;v_{a_{i}},v_{b_{i}})_{i}^{P^{\\oplus}}:=\\left\\{\\begin{array}{r l r}{(<,a_{i},b_{i})}&{i f||u-v_{a_{i}}||_{2}>||u-v_{b_{i}}||_{2}}\\\\ {(>,a_{i},b_{i})}&{i f||u-v_{a_{i}}||_{2}<||u-v_{b_{i}}||_{2}}\\\\ {(=,a_{i},b_{i})}&{o.w.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Then $(L_{\\varphi}^{G},\\psi^{P^{\\oplus}})$ elicits $\\Gamma^{\\varphi}$ . ", "page_idx": 8}, {"type": "text", "text": "The following theorem states that by using multiple problem instances, based on Lemma 9, we can Polytope-elicit the mode. Algorithm 1 outlines how to aggregate the individual solutions to infer the mode. We defer the proof to Appendix B. ", "page_idx": 8}, {"type": "text", "text": "Theorem 10. Let $d\\geq2$ . The mode is $(2d,d,m)$ -Polytope Elicitable for some $m\\in[2d\\!-\\!1,d(2d\\!-\\!1)]$ . ", "page_idx": 8}, {"type": "table", "img_path": "xCIbVuXwPM/tmp/7d0d45b0045b99cb9460d78f675f7a2080e4968b9e24f456cb01bfbfb2d09e1e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Although Theorem 10 states that the mode is $(2d,d,m)$ -Polytope Elicitable for some $m\\in[2d-$ $1,d(2d-1)]$ , it does not state how we select said $\\{(L_{\\varphi_{j}}^{G},\\psi_{j}^{P^{\\oplus}})\\}_{j=1}^{m}$ problem instances in an optimal manner. Unfortunately, selecting the min number of problem instances reduces to a a minimum set cover problem which is computationally hard. Even so, through a greedy approach, one can choose problem instances that are log approximate optimal relative to the true best configuration. In practice using real data, given that these are asymptotic results, we may have confilcting logic for the provided individual reports. In Appendix D, we discuss an approach of how to address this in practice. ", "page_idx": 8}, {"type": "text", "text": "6 Discussion and Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work examines various tradeoffs between surrogate loss dimension, restricting the region of consistency in the simplex when using the 0-1 loss, and number of problem instances. Since our analysis is based on an embedding approach commonly used in practice, our work provides theoretical guidance for practitioners choosing an embedding. We see several possible future directions. The first is a deeper investigation into hallucinations. Future work could investigate the size of the hallucination region in theory, and the frequency of reports in the hallucination region in practice. Another direction would be to construct a method that efficiently identifies the strict calibration regions and the distributions embedded into them. This would provide better guidance on whether or not a particular polytope embedding aligns with one\u2019s prior over the data. Finally, another direction is to identify other properties that can be elicited via multiple problem instances while also reducing the dimension of any one instance. ", "page_idx": 9}, {"type": "text", "text": "Broader Impacts: Our work broadly informs the selection of loss functions for machine learning. Thus our work may influence practitioners\u2019 choice of loss function. Of course, such loss functions can be used for ethical or unethical purposes. We do not know of particular risks of negative impacts of this work beyond risks of machine learning in general. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank Rafael Frongillo for discussions about hallucinations, which led to the exploration of many of the ideas in this work and Amzi Jeffs for discussions regarding convex geometry. This material is based upon work supported by the National Science Foundation under Award No. 2202898 (JF). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Arpit Agarwal and Shivani Agarwal. On consistent surrogate risk minimization and property elicitation. In Conference on Learning Theory, pages 4\u201322. PMLR, 2015.   \nArindam Banerjee, Xin Guo, and Hui Wang. On the optimality of conditional expectation as a bregman predictor. IEEE Transactions on Information Theory, 51(7):2664\u20132669, 2005.   \nPeter L Bartlett, Michael I Jordan, and Jon D McAuliffe. Convexity, classification, and risk bounds. Journal of the American Statistical Association, 101(473):138\u2013156, 2006.   \nMathieu Blondel. Structured prediction with projection oracles. Advances in neural information processing systems, 32, 2019.   \nMathieu Blondel, Andr\u00e9 FT Martins, and Vlad Niculae. Learning with fenchel-young losses. The Journal of Machine Learning Research, 21(1):1314\u20131382, 2020.   \nArne Brondsted. An introduction to convex polytopes, volume 90. Springer Science & Business Media, 2012.   \nJessie Finocchiaro, Rafael Frongillo, and Bo Waggoner. Embedding dimension of polyhedral losses. In Conference on Learning Theory, pages 1558\u20131585. PMLR, 2020.   \nJessie Finocchiaro, Rafael Frongillo, and Bo Waggoner. Unifying lower bounds on prediction dimension of consistent convex surrogates. arXiv preprint arXiv:2102.08218, 2021.   \nJessie Finocchiaro, Rafael M Frongillo, and Bo Waggoner. An embedding framework for the design and analysis of consistent polyhedral surrogates. Journal of Machine Learning Research, 25(63): 1\u201360, 2024.   \nPeter M Gruber. Convex and discrete geometry, volume 336. Springer, 2007.   \nJean-Baptiste Hiriart-Urruty and Claude Lemar\u00e9chal. Fundamentals of convex analysis. Springer Science & Business Media, 2004.   \nPascal Massart and \u00c9lodie N\u00e9d\u00e9lec. Risk bounds for statistical learning. 2006.   \nHarish G Ramaswamy and Shivani Agarwal. Convex calibration dimension for multiclass loss matrices. The Journal of Machine Learning Research, 17(1):397\u2013441, 2016.   \nHarish G Ramaswamy, Balaji Srinivasan Babu, Shivani Agarwal, and Robert C Williamson. On the consistency of output code based learning algorithms for multiclass learning problems. In Conference on Learning Theory, pages 885\u2013902. PMLR, 2014.   \nR Tyrrell Rockafellar. Convex analysis, volume 11. Princeton university press, 1997.   \nCedric Seger. An investigation of categorical variable encoding techniques in machine learning: binary versus one-hot and feature hashing, 2018.   \nIngo Steinwart. How to compare different loss functions and their risks. Constructive Approximation, 26(2):225\u2013287, 2007.   \nKirill Struminsky, Simon Lacoste-Julien, and Anton Osokin. Quantifying learning guarantees for convex but inconsistent surrogates. Advances in Neural Information Processing Systems, 31, 2018.   \nIoannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, Yasemin Altun, and Yoram Singer. Large margin methods for structured and interdependent output variables. Journal of machine learning research, 6(9), 2005.   \nMartin J Wainwright, Michael I Jordan, et al. Graphical models, exponential families, and variational inference. Foundations and Trends\u00ae in Machine Learning, 1(1\u20132):1\u2013305, 2008.   \nYexiang Xue, Zhiyuan Li, Stefano Ermon, Carla P Gomes, and Bart Selman. Solving marginal map problems with np oracles and parity constraints. Advances in Neural Information Processing Systems, 29, 2016.   \nJiaqian Yu and Matthew B Blaschko. The lov\u00e1sz hinge: A novel convex surrogate for submodular losses. IEEE transactions on pattern analysis and machine intelligence, 42(3):735\u2013748, 2018.   \nTong Zhang. Statistical analysis of some multi-category large margin classification methods. Journal of Machine Learning Research, 5(Oct):1225\u20131251, 2004. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "table", "img_path": "xCIbVuXwPM/tmp/c7fc732b59aac5421c30dab9a75bafb290ed549fd0cc38d0c2eb18d17428e9bd.jpg", "table_caption": [], "table_footnote": ["Table 1: Table of general notation "], "page_idx": 12}, {"type": "table", "img_path": "xCIbVuXwPM/tmp/089b99064e5c0f231529be45900b9e649116b8866971bef42226a4f7e87b1c1d.jpg", "table_caption": [], "table_footnote": ["Table 2: Table of polytope and embedding notation "], "page_idx": 12}, {"type": "table", "img_path": "xCIbVuXwPM/tmp/4559f058b3b8e0a7abf197a3e464c9f84d0a0df1ec4a3d28f9ed8637cb1d682a.jpg", "table_caption": [], "table_footnote": ["Table 3: Table of calibration region notation "], "page_idx": 12}, {"type": "text", "text": "B Polytopes, Omitted Proofs, and Results ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Polytopes ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Convex Polytope $P\\subset\\mathbb{R}^{d}$ , or simply a polytope, is the convex hull of a finite number of points $u_{1},\\ldots,u_{n}\\in\\dot{\\mathbb{R}}^{d}$ . An extreme point of a convex set $A$ , is a point $u\\in A$ such that if $u=\\lambda y+(1-\\lambda)z$ with $y,z\\in A$ and $\\lambda\\in[0,1]$ , then $y=u$ and/or $z=u$ . We shall denote by $\\mathrm{vert}(P)$ a polytope\u2019s set of extreme points. A polytope can be expressed by the convex hull of its extreme points, i.e. $P=\\mathrm{conv}\\left(\\mathrm{vert}(P)\\right)$ [Brondsted, 2012, Theorem 7.2]. ", "page_idx": 13}, {"type": "text", "text": "We define the dimension of $P$ via $\\dim(P):=\\dim(\\operatorname{affhull}(P))$ where $\\mathrm{afluull}(P)$ denotes the smallest affine set containing $P$ . A set $F\\subseteq P$ is a face of $P$ is there exists a hyperplane $H(y,\\alpha):=\\{u\\in$ $\\mathbb{R}^{d}\\mid\\langle u,y\\rangle=\\alpha\\}$ such that $F=P\\cap H$ and $P\\subseteq H^{+}$ such that $H^{+}(y,\\alpha):=\\{u\\in\\mathbb{R}^{d}\\mid\\langle u,y\\rangle\\leq\\alpha\\}$ Let $F_{i}(P)$ where $i\\in[d-1]$ denote set of faces of dim $i$ of a polytope $P$ . A face of dimension zero is called a vertex and a face of dimension one is called an edge. We define the edge set of a polytope $P$ by $\\begin{array}{r}{E(P):=\\{\\operatorname{conv}\\big((v_{i},v_{j})\\big)~|~(v_{i},v_{j})\\;\\subseteq~\\binom{\\operatorname{vert}(P)}{2},\\mathrm{c}}\\end{array}$ onv $\\left((v_{i},v_{j})\\right)\\in F_{1}(P)\\}$ . We define the neighbors of a ver $\\operatorname{tex}\\,v\\,\\operatorname{by}\\,\\operatorname{ne}(v;P):=\\{\\hat{v}\\in\\operatorname{vert}(P)\\mid\\,\\operatorname{conv}\\left((v,\\hat{v})\\right)\\in E(P)\\}.$ We will denote conv $((v,\\hat{v}))\\in E(P)$ by as $e_{v,\\hat{v}}$ and $\\operatorname{ne}(v;P)$ by $\\mathrm{ne}(v)$ when clear from context. ", "page_idx": 13}, {"type": "text", "text": "B.2 Omitted Proofs from $\\S\\,3$ ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proposition 2. For a given induced loss $L_{\\varphi}^{G}$ , the unique report which minimizes the expected loss is $\\begin{array}{r}{u^{*}:=\\arg\\operatorname*{min}_{u\\in\\mathbb{R}^{d}}\\mathbb{E}_{Y\\sim p}[L_{\\varphi}^{G}(u,Y)]=\\varphi(p)}\\end{array}$ such that $u^{*}\\in P$ . Furthermore, every $\\hat{u}\\in P$ is $a$ minimizer of $\\mathbb{E}_{Y\\sim\\hat{p}}[L_{\\varphi}^{G}(u,Y)]$ for some $\\hat{p}\\in\\Delta y$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. By [Banerjee et al., 2005, Theorem 1], the minimizer of $\\mathbb{E}_{Y\\sim p}[L_{\\varphi}^{G}(u,Y)]$ is $\\begin{array}{r}{\\sum_{y\\in{\\mathcal{y}}}p_{y}v_{y}=}\\end{array}$ $\\varphi(p)$ . Thus, by the construction of the polytope embedding, it holds that $u^{*}\\;=\\;\\varphi(p)$ . Since Bregman divergences are defined with respect to strictly convex functions, $\\boldsymbol{u}^{*}$ uniquely minimizes $\\mathbb{E}_{Y\\sim p}[L_{\\varphi}^{G}(u,\\bar{Y})]$ . ", "page_idx": 13}, {"type": "text", "text": "Conversely, every $\\hat{u}\\in P$ is expressible as a convex combination of vertices; hence, by the definition of $\\varphi$ , for some distribution, say $\\hat{p}\\in\\Delta y$ , it holds $\\hat{u}=\\varphi(\\hat{p})$ . Therefore, it holds that $\\hat{u}$ minimizes $\\mathbb{E}_{Y\\sim\\hat{p}}[L_{\\varphi}^{G}(u,Y)]$ . \u53e3 ", "page_idx": 13}, {"type": "text", "text": "Theorem 3. For any given pair $(L_{\\varphi}^{G},\\psi^{\\varphi})$ and $\\ell_{0-1}$ with embedding dimension $d<n-1$ ; it holds that $\\mathcal{H}=\\cup_{y\\in\\mathcal{Y}}\\mathrm{conv}\\left(\\mathrm{vert}(P)\\setminus\\{v_{y}\\}\\right)\\cap\\psi_{y}^{\\varphi}$ and furthermore $\\mathcal{H}\\ne\\mathcal{O}$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. Choose a $y\\in\\mathcal{V}$ . We abuse notation and write $\\mathrm{vert}(P)\\setminus v_{y}:=\\mathrm{vert}(P)\\setminus\\{v_{y}\\}$ . Observe all $u\\in\\mathrm{conv}\\,(\\mathrm{vert}(P)\\setminus v_{y})\\cap\\psi_{y}^{\\varphi}$ can be expressed as a convex combination of vertices without needing vertex $v_{y}$ . The coefficients of said convex combination express a $p\\in\\Delta y$ that is embedded to the point $u\\in P$ where $p_{y}=0$ . Yet, by Proposition 2, said $u$ is an expected minimizer of $L_{\\varphi}^{G}$ with respect to $p$ Given the intersection with $\\psi_{y}^{\\varphi}$ and by Definition 8, it holds that $\\cup_{y\\in\\mathcal{y}}\\mathrm{conv}\\left(\\mathrm{vert}(\\dot{P})\\setminus v_{y}\\right)\\cap\\psi_{y}^{\\varphi}\\subseteq\\mathcal{H}$ ", "page_idx": 13}, {"type": "text", "text": "We now shall show that $\\mathcal{H}\\subseteq\\cup_{y\\in\\mathcal{y}}\\mathrm{conv}\\left(\\mathrm{vert}(P)\\setminus v_{y}\\right)\\cap\\psi_{y}^{\\varphi}$ . Fix $y\\in\\mathcal{V}$ . Assume there exists a point $u\\not\\in{\\mathrm{conv}}\\left({\\mathrm{vert}}(P)\\setminus v_{y}\\right)\\cap\\psi_{y}^{\\varphi}$ such that there exists some $p\\in\\Delta y$ where $\\varphi(p)=u$ , $p_{y}=0$ , and $\\psi^{\\varphi}(u)=y$ . Since $\\psi^{\\varphi}(u)\\,=\\,y$ and $u\\not\\in\\mathrm{conv}\\,(\\mathrm{vert}(P)\\setminus v_{y}))\\cap\\psi_{y}^{\\varphi}$ , it must be the case that $u\\not\\in\\mathrm{conv}\\left(\\mathrm{vert}(P)\\setminus v_{y}\\right)$ . However, that implies that $u$ is strictly in the vertex figure and thus must have weight on the coefficient for $y$ . Thus, forming a contradiction that $p_{y}=0$ which implies that $\\mathcal{\\bar{H}}=\\cup_{y\\in\\mathcal{Y}}\\mathrm{conv}\\left(\\mathrm{vert}(P)\\setminus v_{y}\\right)\\cap\\mathcal{\\bar{\\psi}}_{y}^{\\varphi}.$ . ", "page_idx": 13}, {"type": "text", "text": "To show non-emptiness of $\\mathcal{H}$ , we shall use Helly\u2019s Theorem (Rockafellar [1997], Corollary 21.3.2). W.l.o.g, assign an index such that $\\mathcal{Y}=\\{y_{1},\\dots,y_{d},y_{d+1},\\dots,y_{n}\\}$ . Observe the elements of the set $\\{y\\setminus y_{i}\\}_{i=1}^{n}$ each differ by one element. W.l.o.g, pick the first $d+1$ elements of the previous set. Observe $|\\cap_{i=1}^{d+1}\\mathcal{Y}\\setminus y_{i}|=|\\mathcal{Y}\\setminus\\{y_{1},\\dots,y_{d},y_{d+1}\\}|=n-(d+1)>0$ . Hence, by Helly\u2019s theorem and uniqueness of $y_{i}$ \u2019s, $\\cap_{y\\in\\mathcal{y}}\\mathrm{conv}\\left(\\mathrm{vert}(P)\\setminus v_{y}\\right)\\neq\\emptyset$ . ", "page_idx": 13}, {"type": "text", "text": "Pick a point $u^{\\prime}\\in\\cap_{y\\in\\mathcal{Y}}\\mathrm{conv}\\left(\\mathrm{vert}(P)\\setminus v_{y}\\right)$ . Since $\\psi^{\\varphi}$ is well-defined, $u^{\\prime}$ will be linked to some outcome $y^{\\prime}\\in\\mathcal{V}$ and thus $u^{\\prime}\\,\\in\\,\\mathrm{conv}\\,\\bigl(\\mathrm{vert}(P)\\setminus v_{y^{\\prime}}\\bigr)\\cap\\psi_{y^{\\prime}}^{\\varphi}\\,\\subset\\,\\mathcal{H}$ . Yet, $u^{\\prime}$ can be expressed as a convex combination which does not use $v_{y^{\\prime}}$ since it lies in $\\cap_{y\\in\\mathcal{y}}\\mathrm{conv}\\left(\\mathrm{vert}(P)\\setminus v_{y}\\right)$ . Thus, by using Proposition 2 and by the definition of Hallucination (Def. 8), we have that $\\mathcal{H}\\ne\\mathcal{O}$ . \u53e3 ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Lemma 1 (Proposition 1.2.4). [Hiriart-Urruty and Lemar\u00e9chal, 2004] If $\\varphi$ is an affine transformation of $\\mathbb{R}^{n}$ and $A\\subset\\mathbb{R}^{n}$ is convex, then then the image $\\varphi(A)$ is also convex. In particular, if the set $A$ is $a$ convex polytope, the image is also a convex polytope. ", "page_idx": 14}, {"type": "text", "text": "Lemma 2. Let $D_{G}$ be a Bregman divergence, $\\varphi$ be any polytope embedding, $\\psi$ be the MAP link, and $L_{\\varphi}^{G}$ be the loss induced by $(D_{G},\\varphi)$ . Assume the target loss is $\\ell_{0-1}$ . If a point is in a strict calibrated region such that $u\\in R_{y}$ for some $y\\in\\mathcal{V}$ , it is necessary that $u\\in$ conv $(\\{v_{y}\\}\\cup\\mathrm{ne}(v_{y}))\\mid$ conv $(\\mathrm{ne}(v_{y})),$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. If $u\\in R_{y}$ and $u\\in P\\setminus\\left(\\operatorname{conv}\\left(\\{v_{y}\\}\\cup\\operatorname{ne}(v_{y})\\right)\\setminus\\operatorname{conv}\\left(\\operatorname{ne}(v_{y})\\right)\\right)$ , then $u$ can be expressed as a convex combination which has no weight on the coefficient for $v_{y}$ . Hence, there exists a distribution embedded into $u$ where $y$ would not be the mode, thus violating the initial claim that $u\\in R_{y}$ \uff0e\u53e3 ", "page_idx": 14}, {"type": "text", "text": "Lemma 3. Let $D_{G}$ be a Bregman divergence, $\\varphi$ be any polytope embedding, $\\psi$ be the MAP link, and $L_{\\varphi}^{G}$ be the loss induced by $(D_{G},\\varphi)$ . For any $u\\in e_{(v_{i},v_{j})}\\in E(P),$ , it holds that $|\\varphi^{-1}(u)|=1$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. Observe, the two vertices of an edge define the convex hull making up the edge and hence, by (Gruber [2007] ,Theorem 2.3) the two vertices are affinely independent. Therefore, all elements of the edge have a unique convex combination which are expressed by the convex combinations of the edge\u2019s vertices. Given the relation of the embedding $\\varphi$ and convex combinations of vertices expressing distributions, it holds that $|\\varphi^{-1}(u)|=1$ . \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Lemma 4. Let $D_{G}$ be a bregman divergence, $\\varphi$ be a polytope embedding, and $L_{\\varphi}^{G}$ be the induced loss by $(D_{G},\\varphi)$ . For all $y\\in\\mathcal{V}$ , it holds that $\\mathrm{dim}(\\varphi(m o d e_{y}))=\\mathrm{dim}(P)\\geq2.$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. By the construction of $\\varphi$ , we know that $\\dim(P)\\geq2$ . Fix $y\\in\\mathcal{V}$ . By Lemma 3, we know that any edge connected from $v_{y}$ and $\\hat{v}\\in\\mathrm{ne}(v_{y})$ , the distributions embedded into the half of the line segment closer to $v_{y},y$ is in the mode. By Lemma 1, we know that $\\varphi(\\gamma_{y}^{\\mathrm{mode}})$ is a convex set. Thus, the convex hull of the half line segments is part of $\\varphi(\\gamma_{y}^{\\mathrm{mode}})$ . Since each vertex has at least $\\dim(P)$ neighbors, it holds that $\\dim(\\varphi(\\gamma_{y}^{\\mathrm{mode}}))=\\dim(P)$ . \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Theorem 4. Let $D_{G}$ be a Bregman divergence, $\\varphi$ be any polytope embedding, $\\psi^{\\varphi}$ be the MAP link, and $L_{\\varphi}^{G}$ be the loss induced by $(D_{G},\\varphi)$ . There exists a $\\mathcal{P}\\subseteq\\Delta_{\\mathcal{X}}$ with non-zero Lebesgue measure and $\\varphi(\\mathcal{P})\\subseteq R_{\\mathcal{Y}}$ via $(L_{\\varphi}^{G},\\psi^{\\varphi})$ with respect to $\\ell_{0-1}$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. Recall that $\\gamma^{\\mathrm{mode}}(p):=\\mathrm{prop}[\\ell_{0-1}](p)=\\mathrm{mode}(p)$ . Fix $y\\in\\mathcal{V}$ . For contradiction, assume for any $\\hat{y}\\in\\mathcal{V}$ where $y\\ne\\hat{y}$ , it holds that $B_{\\epsilon}(v_{y})\\cap\\varphi(\\gamma_{\\hat{y}}^{\\mathrm{mode}})\\neq\\emptyset$ for all $\\epsilon>0$ . By Lemma 3, it holds that conv $(\\{v_{y}\\}\\cup m_{v_{y},\\alpha})\\,\\subseteq\\,\\varphi(\\gamma_{y}^{\\mathrm{mode}})$ where $m_{v_{y},\\alpha}:=\\{(1-\\alpha)v_{y}+\\alpha\\overline{{v}}\\mid\\overline{{v}}\\in\\mathrm{ne}(v_{y})\\}$ defined by any $\\alpha\\in(0,.5)$ . Furthermore, the elements of $\\cup_{m\\in m_{v_{y},\\alpha}}\\mathrm{conv}\\left(\\left\\{v_{y}\\right\\}\\cup\\left\\{m\\right\\}\\right)$ have one distribution embedded onto it where $y$ is the only valid mode thus, we know that $\\varphi(\\mathrm{mode}_{\\hat{y}})\\cap$ $\\cup_{m\\in m_{v_{y},\\alpha}}\\mathrm{conv}\\left(\\left\\{v_{y}\\right\\}\\cup\\left\\{m\\right\\}\\right)=\\emptyset$ . Since $\\varphi(\\gamma_{\\hat{y}}^{\\mathrm{mode}})\\subset P$ is closed and convex, there must exist some non-negative min distance between $\\varphi(\\gamma_{\\hat{y}}^{\\mathrm{mode}})$ and $v_{y}$ which we shall denote by $d_{v}$ . For any $\\epsilon\\in(0,d_{v_{y}})$ , we can define $B_{\\epsilon}(v_{y})$ such that $B_{\\epsilon}(v_{y})\\cap\\varphi(\\gamma_{\\hat{y}}^{\\mathrm{mode}})=\\varnothing$ , forming a contradiction. ", "page_idx": 14}, {"type": "text", "text": "For each $v_{y}\\in\\mathrm{vert}(P)$ define a $d_{v_{y}}$ and let $\\epsilon^{\\prime}\\in\\cap_{v_{y}\\in\\mathrm{vert}(P)}(0,d_{v_{y}})$ . By the construction of $P$ and the definition of $\\psi^{\\varphi}$ , there exists a $\\epsilon^{\\prime\\prime}>0$ such that for all $u\\in B_{\\epsilon^{\\prime\\prime}}(v_{y})$ it holds that $\\psi(u)=y$ and $B_{\\epsilon^{\\prime\\prime}}(v_{y})\\subset\\psi_{y}^{\\varphi}$ . For any $y\\in\\mathcal{V}$ , we know that $B_{\\operatorname*{min}\\{\\epsilon^{\\prime},\\epsilon^{\\prime\\prime}\\}}(v_{y}))\\cap P\\subseteq R_{y}$ by the construction of our epsilon ball. We claim $\\varphi^{-1}(B_{\\mathrm{min}\\{\\epsilon^{\\prime},\\epsilon^{\\prime\\prime}\\}}(v_{y})\\cap P)$ is a set of distributions for which calibration holds. ", "page_idx": 14}, {"type": "text", "text": "For $\\textit{p}\\in\\mathsf{\\Delta}\\Delta_{\\mathcal{X}}$ such that $\\varphi(p)\\;\\in\\;{\\cal B}_{\\mathrm{min}\\{\\epsilon^{\\prime},\\epsilon^{\\prime\\prime}\\}}(v_{y})\\cap{\\cal P}$ for some $v_{y}\\ \\in\\ \\mathrm{vert}(P)$ , suppose a sequence $\\{u_{m}\\}$ converges to $\\mathrm{prop}[L_{\\varphi}^{G}](p)\\,=\\,\\varphi(p)$ (equality by Proposition 2). By construction of $B_{\\mathrm{min}\\{\\epsilon^{\\prime},\\epsilon^{\\prime\\prime}\\}}(v_{y})\\cap P,\\,\\psi^{\\varphi}(\\varphi(p))\\stackrel{}{=}y\\in\\mathrm{mode}(p)$ and hence, a minimizing report for $\\ell_{0-1}(y;p)$ . Furthermore, since $B_{\\operatorname*{min}\\{\\epsilon^{\\prime},\\epsilon^{\\prime\\prime}\\}}(v_{y})\\subset\\psi_{\\varphi^{-1}(v_{y})}^{\\varphi}$ , all elements within $B_{\\mathrm{min}\\{\\epsilon^{\\prime},\\epsilon^{\\prime\\prime}\\}}(v_{y})$ link to $y$ . Since $\\{u_{m}\\}$ converges to prop $[L_{\\varphi}^{G}](p)$ , there exists some $N\\in\\mathbb{N}$ and $n\\geq N$ , such that $\\|u_{n}-\\varphi(p)\\|_{2}<$ $\\operatorname*{min}\\{\\epsilon^{\\prime},\\epsilon^{\\prime\\prime}\\}$ , meaning that $\\begin{array}{r}{\\mathbb{E}_{\\mathcal{Y}\\sim p}[\\ell_{0-1}(\\psi^{\\varphi}(u_{m}),Y)]\\rightarrow\\operatorname*{min}_{y\\in\\mathcal{Y}}\\mathbb{E}_{\\mathcal{Y}\\sim p}[\\ell_{0-1}(y,Y)]}\\end{array}$ . Hence, for any $v_{y}\\in\\mathrm{vert}(P)$ , $(\\mathrm{prop}[L_{\\varphi}^{G}],\\psi^{\\varphi})$ is $\\ell_{0-1}$ -calibrated property with respect to $\\varphi^{-1}(B_{\\mathrm{min}\\{\\epsilon^{\\prime},\\epsilon^{\\prime\\prime}\\}}(v_{y})\\cap P)$ . Furthermore, by the construction of $B_{\\operatorname*{min}\\epsilon^{\\prime},\\epsilon^{\\prime\\prime}}(v_{y})$ for each $v_{y}\\;\\in\\;\\mathrm{vert}(P)$ , we have that $L_{\\varphi}^{G}$ is strictly for prop $[L_{\\varphi}^{G}]$ . Thus, by Theorem 1, $(L_{\\varphi}^{G},\\psi^{\\varphi})$ is $\\ell_{0-1}$ -calibrated for at least the distributions $\\mathcal{P}=\\cup_{v_{y}\\in\\mathrm{vert}(P)}\\dot{\\varphi^{-1}}(B_{\\mathrm{min}\\{\\epsilon^{\\prime},\\epsilon^{\\prime\\prime}\\}}(v_{y})\\cap P)$ as well as $\\varphi(\\mathcal{P})\\subseteq R_{\\mathcal{Y}}$ . Furthermore, since $B_{\\mathrm{min}\\{\\epsilon^{\\prime},\\epsilon^{\\prime\\prime}\\}}$ for each $v_{y}\\in\\mathrm{vert}(P)$ is non-empty, we have that $\\mathcal{P}\\neq\\mathcal{O}$ . \u53e3 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "B.3 Omitted Proofs from $\\S\\,4$ ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Corollary 7. Let $\\varphi$ be an embedding from $2^{d}$ outcomes into the vertices of $P^{\\square}$ in $d$ -dimensions and define an induced loss $L_{\\varphi}^{G}$ . Fix $\\alpha\\in[0,.5)$ and define $\\Theta_{\\alpha}$ . $(L_{\\varphi}^{G},\\psi^{P^{\\square},\\alpha})$ is $\\ell_{0-1}$ -calibrated for $\\Theta_{\\alpha}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. W.l.o.g, say the outcome $y_{1}\\in\\mathcal{V}$ is embedded into $\\mathbb{1}_{[d]}\\in\\mathrm{vert}(P^{\\perp})$ . Say $\\alpha=.5$ . Observe that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Psi_{\\alpha}^{y_{1}}=\\left\\{\\left(\\begin{array}{c}{{1}}\\\\ {{0}}\\\\ {{\\vdots}}\\\\ {{0}}\\\\ {{0}}\\end{array}\\right),\\left(\\begin{array}{c}{{1-\\alpha}}\\\\ {{\\alpha}}\\\\ {{\\vdots}}\\\\ {{0}}\\\\ {{0}}\\end{array}\\right),\\left(\\begin{array}{c}{{1-\\alpha}}\\\\ {{0}}\\\\ {{\\alpha}}\\\\ {{\\vdots}}\\\\ {{0}}\\end{array}\\right),\\ldots,\\left(\\begin{array}{c}{{1-\\alpha}}\\\\ {{0}}\\\\ {{\\vdots}}\\\\ {{\\alpha}}\\\\ {{0}}\\end{array}\\right),\\left(\\begin{array}{c}{{1-\\alpha}}\\\\ {{0}}\\\\ {{\\vdots}}\\\\ {{0}}\\\\ {{\\alpha}}\\end{array}\\right)\\right\\}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and that $1\\,\\geq\\,(1\\,-\\,\\alpha)\\pm\\alpha\\,\\geq\\,0$ for any $\\alpha\\,\\in\\,(0,.5)$ . Hence, for any $\\alpha\\,\\in\\,(0,.5)$ it holds that $P_{0.5}^{y_{1}}=\\mathrm{conv}\\left(\\{0,1\\}^{d}\\right)$ ad nfdu ,o rwe $P_{\\alpha}^{y_{1}}\\subset P_{0.5}^{y_{1}}\\subset\\mathbb{R}_{>0}^{d}$ .t riBcyt  ssuybmsemt eotrf yt hoef $P^{\\square}$ aanntd t thhaet  lcionnetaariintys $\\varphi$ $\\alpha\\in(0,.5)$ $y\\in\\mathcal{V}$ $P_{\\alpha}^{y}$ $v_{y}$ . Hence, for all $y,\\hat{y}\\in\\mathcal{Y}$ such that $y\\ne\\hat{y}$ , it holds that $P_{\\alpha}^{y}\\cap P_{\\alpha}^{\\hat{y}}\\,=\\,\\emptyset$ . Thus by Theorem 6, $(\\bar{L}_{\\varphi}^{G},\\psi^{P^{\\perp},\\alpha})$ is $\\ell_{0-1}$ -calibrated for $\\Theta_{\\alpha}$ where $\\alpha\\in(0,.5)$ . \u53e3 sCuocrho tllhaart $\\varphi$ mw $d!$ e roeu n.t oF itxh o. f $P^{w}$ ni $d$ niss $\\begin{array}{r}{w=(0,\\frac{^{\\cdot}1}{\\beta d},\\frac{2}{\\beta d},\\dots,\\frac{d-1}{\\beta d})\\in\\mathbb{R}^{d}}\\end{array}$ $\\beta=\\frac{d\\!-\\!1}{2}$ $\\alpha\\in[0,\\frac{1}{d})$ $(L_{\\varphi}^{G},\\psi^{P^{w},\\alpha})$ -calibrated over $\\Theta_{\\alpha}$ . ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Proof. Let $\\Delta_{d}:=\\mathrm{conv}\\left(\\{\\mathbb{1}_{i}\\in\\mathbb{R}^{d}\\ |\\ i\\in[d]\\}\\right)$ and observe $P^{w}\\subset\\Delta_{d}$ since for all $\\pi$ , $\\|\\pi\\cdot w\\|_{1}=$ $\\|w\\|_{1}=1$ . Observe that $P^{w}$ can be symmetrically partitioned into $d!$ regions with disjoint interiors, one for each permutation $\\pi\\~\\in~S_{d}$ via $\\Delta_{d}^{\\pi}\\;:=\\;\\bar{\\{u\\;\\in\\;\\Delta_{d}\\;\\mid\\;u_{1}\\;\\leq\\;\\cdots\\;\\leq\\;u_{d}\\}}$ . Fix $\\pi\\,\\in\\,S_{d}$ and w.l.o.g assume $\\pi$ is associated with the constraints $\\Delta_{w}^{\\pi}:=\\{u\\in\\Delta_{w}\\mid u_{1}\\leq\\cdot\\cdot\\leq u_{d}\\}$ implying that $\\begin{array}{r}{\\pi(w)=(\\frac{0}{\\beta d},\\frac{1}{\\beta d},\\dots,\\frac{d-1}{\\beta d})}\\end{array}$ . Let $\\textstyle\\alpha={\\frac{1}{d}}$ and define $\\Theta_{\\alpha}$ . With respect to $\\Theta_{\\alpha}$ , let $y:=\\varphi^{-1}(\\pi(w))\\in\\mathcal{V}$ and $\\hat{y}:=\\varphi^{-1}(\\hat{\\pi}(w))\\in\\mathcal{V}$ such that $\\hat{\\pi}\\in S_{d}$ . Thus the set $\\Psi_{\\alpha}^{y}:=\\{(1-\\textstyle\\frac{1}{d})\\delta_{y}+(\\textstyle\\frac{1}{d})\\delta_{\\hat{y}}\\mid\\hat{y}\\in\\mathcal{y}\\}$ is mapped via $\\varphi$ to the following points ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\varphi(\\Psi_{\\alpha}^{y})=\\big\\{(1-{\\frac{1}{d}})(\\pi(w))+({\\frac{1}{d}})(\\hat{\\pi}(w))\\mid\\hat{\\pi}\\in S_{d}\\big\\}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "within the permutahedron. ", "page_idx": 15}, {"type": "text", "text": "We shall show that $P_{\\alpha}^{y}\\subseteq\\Delta_{d}^{\\pi}$ . If this were not true, there would exists an element of $w^{\\pi,\\hat{\\pi}}\\in\\varphi(\\Psi_{\\alpha}^{y})$ such such that for some pair of adjacent indices, say $i,i+1\\in[d-1]$ , $w_{i}^{\\pi,\\hat{\\pi}}>w_{i+1}^{\\pi,\\hat{\\pi}}$ wi\u03c0+,\u03c0\u02c61. For sake of contradiction, fix $i\\in[d-1]$ and assume there exists a $\\hat{\\pi}\\in S_{d}$ such that $w_{i}^{\\pi,\\hat{\\pi}}>w_{i+1}^{\\pi,\\hat{\\pi}}$ Observe that ", "page_idx": 15}, {"type": "text", "text": "any element of $\\hat{\\pi}(w)$ can be expressed by $\\frac{j}{\\beta d}$ using some $j\\in\\{0,1,\\ldots,d-1\\}$ . Thus, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w_{i}^{\\pi,\\hat{\\pi}}>w_{i+1}^{\\pi,\\hat{\\pi}}}\\\\ &{\\Leftrightarrow(1-\\frac{1}{d})(\\frac{i-1}{\\beta d})+(\\frac{1}{d})(\\hat{\\pi}(w))_{j}>(1-\\frac{1}{d})(\\frac{i}{\\beta d})+(\\frac{1}{d})(\\hat{\\pi}(w))_{\\hat{\\mathcal{I}}}}\\\\ &{\\Rightarrow\\quad(1-\\frac{1}{d})(\\frac{i-1}{\\beta d})+(\\frac{1}{d})(\\frac{j}{\\beta d})>(1-\\frac{1}{d})(\\frac{i}{\\beta d})+(\\frac{1}{d})(\\frac{\\hat{j}}{\\beta d})}\\\\ &{\\Rightarrow\\quad(i-1)(1-\\frac{1}{d})+j(\\frac{1}{d})>i(1-\\frac{1}{d})+\\hat{j}(\\frac{1}{d})}\\\\ &{\\Rightarrow\\quad1-d>\\hat{j}-j}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Multiply by $\\beta d$ ", "page_idx": 16}, {"type": "text", "text": "for some $j,{\\hat{j}}\\in\\{0,1,\\cdot\\cdot\\cdot,d-1\\}$ where $j\\neq\\hat{j}$ . ", "page_idx": 16}, {"type": "text", "text": "Case 1: $(j<\\hat{j})$ : The smallest value possible for ${\\widehat{j}}\\gets j$ is $0-(d-1)$ however, $1-d\\geqslant1-d$ . ", "page_idx": 16}, {"type": "text", "text": "Case $2{\\div}(j>\\hat{j})$ : The smallest value possible for ${\\widehat{j}}\\rightharpoonup j$ is 1 however, $1-d\\not>1$ . ", "page_idx": 16}, {"type": "text", "text": "Hence, $P_{\\alpha}^{y}\\subseteq\\Delta_{d}^{\\pi}$ and specifically, there can exists an extreme point of $P_{\\alpha}^{y}$ that lies on the boundary of $\\Delta_{d}^{\\pi}$ as shown in Case 1. However, if $\\alpha\\,\\in\\,(0,{\\frac{1}{d}})$ , every extreme point of $P_{\\alpha}^{y}$ moves closer to $\\pi(w)$ (besides the extreme point itself already on $\\pi(w),$ ) and therefore $P_{\\alpha}^{y}$ lies strictly within $\\Delta_{d}^{\\pi}$ . By symmetry of $P^{w}$ and the linearity of $\\varphi$ , this would imply that for all $\\bar{y^{\\prime}},y^{\\prime\\prime}\\in\\mathcal{Y}$ such that $y^{\\prime}\\ne y^{\\prime\\prime}$ it holds that $P_{\\alpha}^{y^{\\prime}}\\cap P_{\\alpha}^{y^{\\prime\\prime}}=\\emptyset$ . Thus by Corollary 6, $(L_{\\varphi}^{G},\\psi^{P^{w},\\alpha})$ is $\\ell_{0-1}$ -calibrated for $\\Theta_{\\alpha}$ where $\\alpha\\in(0,\\frac{1}{d})$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "B.4 Omitted Proofs from $\\S\\,5$ ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lemma 9. Say we are given a cross-polytope embedding $\\varphi:\\Delta_{2d}\\to P^{\\oplus}$ and induced loss $L_{\\varphi}^{G}$ . Let $(v_{a_{i}},v_{b_{i}})$ , be the $i^{t h}$ diagonal pair (i.e. $\\varphi(\\delta_{a_{i}})\\,=\\,v_{a_{i}}.$ ). Define the property $\\Gamma^{\\varphi}:\\Delta_{2d}\\rightarrow B$ element-wise by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Gamma^{\\varphi}(p)_{i}:=\\left\\{\\begin{array}{l l}{(<,a_{i},b_{i})}&{i f p_{a_{i}}<p_{b_{i}}}\\\\ {(>,a_{i},b_{i})}&{i f p_{a_{i}}>p_{b_{i}}}\\\\ {(=,a_{i},b_{i})}&{i f p_{a_{i}}=p_{b_{i}}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Furthermore define the link $\\psi^{P^{\\oplus}}:\\mathbb{R}^{d}\\to B$ with respect to each diagonal pair as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\psi(u;v_{a_{i}},v_{b_{i}})_{i}^{P^{\\oplus}}:=\\left\\{\\begin{array}{r l r}{(<,a_{i},b_{i})}&{i f||u-v_{a_{i}}||_{2}>||u-v_{b_{i}}||_{2}}\\\\ {(>,a_{i},b_{i})}&{i f||u-v_{a_{i}}||_{2}<||u-v_{b_{i}}||_{2}}\\\\ {(=,a_{i},b_{i})}&{o.w.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then $(L_{\\varphi}^{G},\\psi^{P^{\\oplus}})$ elicits $\\Gamma^{\\varphi}$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. W.l.o.g, fix a diagonal pair $(v_{a},v_{b})$ and let $v_{a}:=\\mathbb{1}_{1}$ and $v_{b}:=-\\mathbb{1}_{1}$ . Define the embedding $\\varphi$ accordingly. We will show that the following is true for all distributions mapped via $\\varphi$ to $u\\in P^{\\bar{\\oplus}}$ . ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{||u-v_{a}||_{2}>||u-v_{b}||_{2}\\iff p_{a}<p_{b}}\\\\ &{}&{\\mathrm{OR}\\ ||u-v_{a}||_{2}<||u-v_{b}||_{2}\\iff p_{a}>p_{b}}\\\\ &{}&{\\mathrm{OR}\\ ||u-v_{a}||_{2}=||u-v_{b}||_{2}\\iff p_{a}=p_{b}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "First, fix $p~\\in~\\Delta_{2d}$ . Recall, by Proposition 2, the minimizing report for $L_{\\varphi}^{G}$ in expectation is $u=\\varphi(p)\\in P\\subset\\mathbb{R}^{d}$ . We will prove the forward direction of the first and second lines. Then the reverse directions follow from the contrapositives. ", "page_idx": 16}, {"type": "text", "text": "Case $1,\\implies:$ Assume for contradiction that $p_{a}<p_{b}$ and $||\\varphi(p)-v_{a}||_{2}<||\\varphi(p)-v_{b}||_{2}$ . Then ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\varphi(p)-\\mathbb{1}_{1},\\varphi(p)-\\mathbb{1}_{1}\\rangle<\\langle\\varphi(p)+\\mathbb{1}_{1},\\varphi(p)+\\mathbb{1}_{1}\\rangle}\\\\ &{\\qquad(u_{1}-1)^{2}+\\displaystyle\\sum_{i=1}u_{i}^{2}<(u_{1}+1)^{2}+\\displaystyle\\sum_{i=1}u_{i}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad-u_{1}<u.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By the definition of a $d$ -cross polytope $P^{\\oplus}\\;:=\\;\\mathrm{conv}\\left(\\{\\pi((\\pm1,0,\\ldots,0))\\;\\;|\\;\\;\\pi\\;\\in\\;S_{d}\\}\\right)$ and the orthogonal relation between vertices, to express a $u\\in P^{\\oplus}$ as a convex combination of vertices, each diagonal pair of vertices coefficients solely influence the position along a single unit basis vector. Hence, due to the definition of $\\varphi$ , we have $u_{1}=\\mathbb{1}_{1}\\cdot p_{a}-\\mathbb{1}_{1}\\cdot p_{b}<0$ since we have assumed that $p_{a}<p_{b}$ . Hence $-u_{1}<u_{1}<0$ , a contradiction. ", "page_idx": 17}, {"type": "text", "text": "Case 2, $\\Longrightarrow$ : Assume $p_{a}>p_{b}$ and $||\\varphi(p)-v_{a}||_{2}<||\\varphi(p)-v_{b}||_{2}$ . By symmetry with case 1, all the inequalities are reversed, leading to the contradiction that $-u_{1}>u_{1}>0$ . ", "page_idx": 17}, {"type": "text", "text": "Case 3: $\\ y_{a}=p_{b})$ ): Follows from the if and only ifs of cases 1 and 2.   \nHence $(L_{\\varphi}^{G},\\psi_{\\varphi})$ elicits $\\Gamma^{\\varphi}$ . ", "page_idx": 17}, {"type": "text", "text": "Theorem 10. Let $d\\geq2$ . The mode is $(2d,d,m)$ -Polytope Elicitable for some $m\\in[2d\\!-\\!1,d(2d\\!-\\!1)]$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. We will elicit the mode via the intermediate properties, $\\Gamma^{\\varphi_{j}}$ , defined in Lemma 9. First we construct a set of embeddings so that we guarantee that all the $\\varphi_{j}$ \u2019s allow comparison between any pair of outcome probabilities. For example, for each unique pair $\\bar{(}a,b)_{j}\\in\\bar{(}_{2}^{\\mathcal{V}})$ define an embedding: $\\varphi_{j}(\\delta_{a})=\\mathbb{1}_{1}$ and $\\varphi_{j}(\\delta_{b})=-\\mathbb{1}_{1}$ , and embed every other remaining report $\\bar{r}\\in\\mathcal{V}\\setminus\\{a,b\\}$ arbitrarily. Since $(L_{\\varphi}^{G},\\psi^{P^{\\oplus}})$ elicits $\\Gamma^{\\varphi}$ , minimizing each $L_{\\varphi_{j}}^{G}$ with a separate model yields us comparisons via the link $\\psi^{P^{\\oplus}}$ . To find the set $r\\,\\in\\,\\mathcal{V}$ such that $p_{r}$ is maximum, we use a sorting algorithm that uses pairwise comparisons, such as bubble sort. Hence with $\\Upsilon$ as Algorithm 1, we have that $\\Upsilon(\\{L_{\\varphi_{j}}^{G},\\psi^{P^{\\oplus}}\\})=\\mathrm{mode}(p).$ . ", "page_idx": 17}, {"type": "text", "text": "Assuming there exist $\\varphi_{j}\\mathbf{s}$ such that there is no redundancy in comparison pairs between each $\\Gamma^{\\varphi_{j}}$ , we would need only $\\begin{array}{r}{\\frac{d(2d-1)}{d}=2d-1}\\end{array}$ problem instances. Hence, we establish our lower bound on the needed number of problem instances. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "C Hamming Loss Hallucination Example ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Hamming loss $\\ell:\\mathcal{V}\\times\\mathcal{V}\\to\\mathbb{R}_{+}$ is defined by $\\begin{array}{r}{\\ell(y,\\hat{y})\\,=\\,\\sum_{i=1}^{d}\\mathbb{1}_{y_{i}\\neq\\hat{y_{i}}}}\\end{array}$ 1 1yi\u0338= y\u02c6i where Y = {\u22121, 1, }d. Suppose $d=3$ and we have the following indexing over outc omes ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{V}:=\\left\\{y_{1}\\equiv(1,1,1),y_{2}\\equiv(1,1,-1),y_{3}\\equiv(1,-1,1),y_{4}\\equiv(-1,1,1),\\right.\\qquad\\qquad}\\\\ {\\left.y_{5}\\equiv(-1,-1,1),y_{6}\\equiv(1,-1,-1),y_{7}\\equiv(-1,1,-1),y_{8}\\equiv(-1,-1,-1)\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let us define the following distribution ", "page_idx": 17}, {"type": "equation", "text": "$$\np_{\\epsilon}=(0,\\frac{1}{3}-\\epsilon,\\frac{1}{3}-\\epsilon,\\frac{1}{3}-\\epsilon,0,0,0,3\\epsilon)\\in\\Delta_{y}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "such that $\\epsilon>0$ . ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\ \\mathbb{E}_{Y\\sim p_{\\epsilon}}[\\ell(y_{1},Y)]=1+6\\epsilon}\\\\ &{\\bullet\\ \\mathbb{E}_{Y\\sim p_{\\epsilon}}[\\ell(y_{2},Y)]=\\mathbb{E}_{Y\\sim p_{\\epsilon}}[\\ell(y_{3},Y)]=\\mathbb{E}_{Y\\sim p_{\\epsilon}}[\\ell(y_{4},Y)]=\\frac{4}{3}+2\\epsilon}\\\\ &{\\bullet\\ \\mathbb{E}_{Y\\sim p_{\\epsilon}}[\\ell(y_{5},Y)]=\\mathbb{E}_{Y\\sim p_{\\epsilon}}[\\ell(y_{6},Y)]=\\mathbb{E}_{Y\\sim p_{\\epsilon}}[\\ell(y_{7},Y)]=\\frac{7}{3}-4\\epsilon}\\\\ &{\\bullet\\ \\mathbb{E}_{Y\\sim p_{\\epsilon}}[\\ell(y_{8},Y)]=2-6\\epsilon}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For all $\\epsilon\\in[0,\\frac{1}{12})$ , the minimizing report in expectation is $y_{1}=(1,1,1)$ . However, $p_{\\epsilon,1}=0$ and thus, a hallucination would occur under a calibrated surrogate and link pair. ", "page_idx": 17}, {"type": "text", "text": "D Linking under Multiple Problem Instances ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "As stated in $\\S\\,5$ , when using real data, given that these are asymptotic results, we may have confilcting logic for the provided individual reports. In this section, we provide an approach such that the algorithm still reports information in the aforementioned scenario and will reduce to Algorithm 1 asymptotically. We build a binary relation table $M\\in\\{0,1\\}^{n\\times n}$ with the provided reports. Based on $M$ , we select a largest subset of $S\\subseteq\\mathcal{V}$ such that when $M$ is restricted to rows and columns corresponding to the elements of $S$ , denoted by $M_{S}$ , we have that $M_{S}$ is reflexive, antisymmetric, transitive, and strongly connected implying $M_{S}$ has a total-order relation defined over its elements. Having a total-order relation infers the mode can be found via comparisons. The algorithm returns $(R,S)$ , where $R$ is the mode set with respect to the elements of $S$ . ", "page_idx": 18}, {"type": "text", "text": "Algorithm 2 Elicit mode via comparisons and the d-Cross Polytopes over well-defined partial orderings ", "page_idx": 18}, {"type": "text", "text": "Require: M = {(L\u03c6j, \u03c8j )}j=1 ", "page_idx": 18}, {"type": "text", "text": "Learn a model $h_{j}:\\mathcal{X}\\rightarrow\\mathbb{R}^{d}$ for each instance $(L_{\\varphi_{j}}^{G},\\psi_{j}^{P^{\\oplus}})\\in M$   \nFor some fixed $x\\in\\mathscr{X}$ , collect all $B_{j}\\leftarrow\\psi_{j}^{P^{\\oplus}}(h_{j}(\\bar{x_{\\slash}}))$ where $B_{j}\\in B_{j}$   \nBuild $M\\in\\{0,1\\}^{n\\times n}$ binary relation table with provided $\\{B_{j}\\}_{j=1}^{m}$ as such \u2022 Label rows top to bottom by $y_{1},\\ldots,y_{n}$ and columns left to right by $y_{1},\\ldots,y_{n}$ . \u2022 For all $(\\cdot,p_{y_{i}},p_{y_{k}})\\in B_{j}$ , if $p_{y_{i}}\\le p_{y_{k}}$ set $M[i,k]=1$ and 0 otherwise.   \nSelect largest subset $S\\subseteq\\mathcal{V}$ such that $M_{S}$ is reflexive, antisymmetric, transitive, and strongly   \nconnected.   \nReport $(R,S)\\leftarrow$ FindMaxElements-of- $S(M;S)$ ", "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 19}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 19}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 19}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 19}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 19}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 19}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: Any claimed result in the abstract is proved within this work. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Yes, our paper discuss how these results are asymptotic. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: Yes, we thoroughly introduce every necessary definition and past result necessary to understand the assumptions that hold under our results. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The results of this work have rigorous proofs presented next to the results or referenced clearly in the appendix. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 20}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification:This paper does not include experiments requiring code. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 21}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not include experiments Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 22}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: None of our conducted work for this paper violates the code of ethics presented. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: In the body of our paper, we provide a broader impace section. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The answer NA means that the paper poses no such risks. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 23}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]