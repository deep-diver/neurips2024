[{"figure_path": "28bFUt6rUY/tables/tables_6_1.jpg", "caption": "Table 1: Alignment of VLMs with Human Preferences. The best value is highlighted in blue, and the second-best value is highlighted in green", "description": "This table presents the results of evaluating various Vision-Language Models (VLMs) based on their alignment with human preferences.  Two key aspects were assessed: Discrimination (ability to correctly identify which image best aligns with a given text prompt) and Expansion (ability to generate diverse variations of a text prompt).  The table shows the accuracy and diversity scores for each VLM's expansion capabilities, as well as the accuracy scores for discrimination.  LLaVA-Next and GPT-4V show the best overall performance, achieving high accuracy in discrimination and high scores for both accuracy and diversity in expansion.", "section": "4.2 Selection of Vision-Language Models"}, {"figure_path": "28bFUt6rUY/tables/tables_6_2.jpg", "caption": "Table 2: Ablation Studies. The best value is highlighted in blue, and the second-best value is highlighted in green.", "description": "This table presents the results of ablation studies conducted to evaluate the impact of different components of the EvolveDirector framework on the performance of the base model. The studies examine the effects of discrimination, expansion, and mutation operations on the model's FID score and human evaluation results across various data scales. It compares the performance of models trained with different combinations of these operations, highlighting the effectiveness of the proposed framework in improving data efficiency and model performance.", "section": "4.3 Ablation Studies"}, {"figure_path": "28bFUt6rUY/tables/tables_7_1.jpg", "caption": "Table 3: Select Ratios of Advanced Models. The highest value is highlighted in blue, and the lowest value is highlighted in gray.", "description": "This table presents the percentages of images generated by four different advanced text-to-image models (DeepFloyd IF, Playground 2.5, Ideogram, and Stable Diffusion 3) that were selected by the Vision-Language Model (VLM) in the training process of EvolveDirector.  The selection indicates that the VLM considered these images to be superior in quality compared to those generated by the base model. The table breaks down these selection rates into overall percentages and also provides a more granular view of the selection percentages for specific image domains: images containing humans, text, and multiple objects. Highlighting shows the best-performing model in each category.", "section": "4.4 Approaching Advanced Models"}]