[{"figure_path": "oLoqHRbXYE/tables/tables_6_1.jpg", "caption": "Table 1: Main WER (%) results of the proposed STAR adaptation and baselines in various ASR domains. \"Whisper (frozen)\" denotes the zero-shot performance without adaptation. \"Whisper (self-train.)\" is the vanilla self-training scheme consisting of pseudo-labeling and finetuning. Based on that, \"UTTfilter\" adds utterance-level filtering explained in \u00a73.3, and \"TOKreweight\" performs two token-level re-weighting explained in \u00a73.2. \"Whisper (real label)\" is supervised learning with real (ground truth) labels and can be viewed as the upper-bound performance of source-free UDA.", "description": "This table presents the Word Error Rate (WER) results for different ASR models and methods across various testing scenarios (noise, accents, specific tasks).  It compares the performance of the proposed STAR method against baselines like zero-shot Whisper, self-training with and without utterance-level filtering and token re-weighting, and the upper-bound performance achieved by supervised learning with ground truth labels.", "section": "5.1 Effectiveness of STAR"}, {"figure_path": "oLoqHRbXYE/tables/tables_6_2.jpg", "caption": "Table 2: WER (%) results regarding catastrophic forgetting. \u201cFrozen\u201d denotes Whisper zero-shot without adaptation. \u201cSTAR\u201d model is adapted to CHIME-4 using STAR; then evaluated on other domains. More results are in Table 11.", "description": "This table presents the Word Error Rate (WER) results of the STAR model and baselines. The \"Frozen\" column shows the performance of the Whisper model without any adaptation. The \"Self-train.\" column represents the performance of the vanilla self-training approach. The \"STAR\" column shows the performance of the proposed STAR model after adapting to the CHIME-4 dataset. The table demonstrates that STAR effectively prevents catastrophic forgetting, maintaining good performance on unseen domains after finetuning on CHIME-4.  The results are shown for multiple datasets representing various noise and accent conditions.", "section": "5.1 Effectiveness of STAR"}, {"figure_path": "oLoqHRbXYE/tables/tables_7_1.jpg", "caption": "Table 3: Case study of an accented speech in CV-in (ID: \u201cen_19795319\u201d). The wrong tokens are highlighted in red. Variance indicates the stability of different scores. \u201cNCE\u201d denotes normalized cross-entropy, where a higher value indicates better measure quality (more results are in Fig. 5).", "description": "This table shows a case study of an accented speech using the Common Voice dataset. It compares the ground truth transcription with the pseudo labels generated by the model, also providing confidence scores, attentive scores, and STAR scores. The variance and normalized cross-entropy (NCE) scores are also included to assess the quality of these indicators.  The wrong tokens are highlighted in red.", "section": "5.1 Effectiveness of STAR"}, {"figure_path": "oLoqHRbXYE/tables/tables_7_2.jpg", "caption": "Table 5: BLEU results of STAR on speech translation task with FLEURS [14] test sets.", "description": "This table presents the BLEU scores achieved by the STAR method on speech translation tasks, using the FLEURS benchmark. It compares the performance of STAR against baseline methods, self-training, and the performance achieved using real labels (ground truth).  The results show improvements across multiple language pairs after using STAR for adaptation, demonstrating its applicability to speech translation in addition to speech recognition.", "section": "5 Results and Analysis"}, {"figure_path": "oLoqHRbXYE/tables/tables_15_1.jpg", "caption": "Table 6: WER (%) results of STAR with latest speech foundation model, SeamlessM4T-Large-V2 [4], on CHIME-4 test sets.", "description": "This table presents the Word Error Rate (WER) results of the STAR adaptation method using the SeamlessM4T-Large-V2 speech foundation model on the CHiME-4 test sets.  It compares the performance of the STAR method against a baseline, a self-training approach, and supervised learning using real labels.  The results are broken down by different test set scenarios (test-real, test-simu, dev-real, dev-simu), showing the relative WER reduction achieved by STAR in each condition.", "section": "5. Results and Analysis"}, {"figure_path": "oLoqHRbXYE/tables/tables_18_1.jpg", "caption": "Table 7: Ablation study on employing different pseudo tokens to calculate the attentive score in Eq. 5 using CHiME-4 test-real data.", "description": "This table presents the results of an ablation study conducted to evaluate the impact of different sets of pseudo tokens on the calculation of the attentive score (Eq. 5).  The study uses the CHiME-4 test-real dataset. Three different methods for selecting tokens are compared: using only history tokens, using only future tokens, and using both history and future tokens. The table shows the impact of this variation on two metrics: Normalized Cross-Entropy (NCE) and Word Error Rate (WER). The results indicate which combination of pseudo tokens leads to the most effective attentive score.", "section": "5.3 Ablation Study"}, {"figure_path": "oLoqHRbXYE/tables/tables_18_2.jpg", "caption": "Table 1: Main WER (%) results of the proposed STAR adaptation and baselines in various ASR domains. \u201cWhisper (frozen)\u201d denotes the zero-shot performance without adaptation. \u201cWhisper (self-train.)\u201d is the vanilla self-training scheme consisting of pseudo-labeling and finetuning. Based on that, \u201cUTTfilter\u201d adds utterance-level filtering explained in \u00a73.3, and \u201cTOKreweight\u201d performs two token-level re-weighting explained in \u00a73.2. \u201cWhisper (real label)\u201d is supervised learning with real (ground truth) labels and can be viewed as the upper-bound performance of source-free UDA.", "description": "This table presents the Word Error Rate (WER) comparison of different methods for adapting the Whisper ASR model to various domains (noise, accents, scenarios). It compares the zero-shot performance of the model, a baseline self-training approach, the proposed STAR method with and without utterance-level filtering and token re-weighting, and finally the supervised learning performance (upper bound). The WER reduction achieved by STAR is also shown.", "section": "5.1 Effectiveness of STAR"}, {"figure_path": "oLoqHRbXYE/tables/tables_19_1.jpg", "caption": "Table 1: Main WER (%) results of the proposed STAR adaptation and baselines in various ASR domains. \"Whisper (frozen)\" denotes the zero-shot performance without adaptation. \"Whisper (self-train.)\" is the vanilla self-training scheme consisting of pseudo-labeling and finetuning. Based on that, \u201cUTTfilter\u201d adds utterance-level filtering explained in \u00a73.3, and \u201cTOKreweight\u201d performs two token-level re-weighting explained in \u00a73.2. \"Whisper (real label)\" is supervised learning with real (ground truth) labels and can be viewed as the upper-bound performance of source-free UDA.", "description": "This table presents the Word Error Rate (WER) results for different ASR adaptation methods on various datasets.  It compares the performance of the proposed STAR method against baselines including a frozen Whisper model (zero-shot), self-training with and without utterance and token-level filtering, and finally supervised training (using real labels). The table shows the WER for different noise conditions, accents, and specific scenarios, allowing a comprehensive comparison of STAR's effectiveness across diverse domains.", "section": "5 Results and Analysis"}, {"figure_path": "oLoqHRbXYE/tables/tables_19_2.jpg", "caption": "Table 10: WER (%) results of different finetuning methods on CHiME-4 test-real. * is the number of trainable parameters. \u201cFull\u201d is full finetuning, \"Enc/Dec-only\" is encoder/decoder-only finetune.", "description": "This table compares the performance of different finetuning approaches (full, encoder-only, decoder-only, LoRA, and reprogramming) on the CHiME-4 test-real dataset. It shows the Word Error Rate (WER), the number of trainable parameters for each approach, and the relative WER reduction achieved by the STAR method compared to the baseline.", "section": "5.3 Ablation Study"}, {"figure_path": "oLoqHRbXYE/tables/tables_20_1.jpg", "caption": "Table 1: Main WER (%) results of the proposed STAR adaptation and baselines in various ASR domains. \"Whisper (frozen)\" denotes the zero-shot performance without adaptation. \"Whisper (self-train.)\" is the vanilla self-training scheme consisting of pseudo-labeling and finetuning. Based on that, \u201cUTTfilter\u201d adds utterance-level filtering explained in \u00a73.3, and \u201cTOKreweight\u201d performs two token-level re-weighting explained in \u00a73.2. \"Whisper (real label)\" is supervised learning with real (ground truth) labels and can be viewed as the upper-bound performance of source-free UDA.", "description": "This table presents the Word Error Rate (WER) results for different ASR models (Whisper with different configurations and other models) across various scenarios (background noise, speaker accents, specific scenarios). It compares the performance of the proposed STAR method with baselines like zero-shot, self-training, and supervised learning, highlighting STAR's effectiveness in unsupervised domain adaptation.", "section": "5.1 Effectiveness of STAR"}, {"figure_path": "oLoqHRbXYE/tables/tables_20_2.jpg", "caption": "Table 1: Main WER (%) results of the proposed STAR adaptation and baselines in various ASR domains. \"Whisper (frozen)\" denotes the zero-shot performance without adaptation. \"Whisper (self-train.)\" is the vanilla self-training scheme consisting of pseudo-labeling and finetuning. Based on that, \u201cUTTfilter\u201d adds utterance-level filtering explained in \u00a73.3, and \u201cTOKreweight\u201d performs two token-level re-weighting explained in \u00a73.2. \"Whisper (real label)\" is supervised learning with real (ground truth) labels and can be viewed as the upper-bound performance of source-free UDA.", "description": "This table presents the Word Error Rate (WER) for different ASR models (Whisper with various modifications and a real label baseline) across a range of testing scenarios (background noise, speaker accents, and specific situations). It compares the performance of the proposed STAR method with other baselines, highlighting the relative WER reduction achieved by STAR and illustrating its effectiveness in various conditions.", "section": "5.1 Effectiveness of STAR"}]