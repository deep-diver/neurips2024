[{"type": "text", "text": "Decoupling Semantic Similarity from Spatial Alignment for Neural Networks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Tassilo Wald\u2217,1,2,3 Constantin Ulrich1,4,7 , Gregor K\u00f6hler 1,3, David Zimmerer 1,2, Stefan Denner 1,3, Michael Baumgartner 1,3 Fabian Isensee 1,2, Priyank Jaini\u2020,5, Klaus H. Maier-Hein\u2020,1,2,3,4,6 ", "page_idx": 0}, {"type": "text", "text": "1 Division of Medical Image Computing,   \nGerman Cancer Research Center (DKFZ), Heidelberg, Germany 2 Helmholtz Imaging, DKFZ, Heidelberg, Germany 3 Faculty of Mathematics and Computer Science, University of Heidelberg, Germany   \n4 Medical Faculty Heidelberg, University of Heidelberg, Germany 5 Google Deepmind ", "page_idx": 0}, {"type": "text", "text": "6 Pattern Analysis and Learning Group, Department of Radiation Oncology   \n7 National Center for Tumor Diseases (NCT) Heidelberg, Germany ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "What representation do deep neural networks learn? How similar are images to each other for neural networks? Despite the overwhelming success of deep learning methods key questions about their internal workings still remain largely unanswered, due to their internal high dimensionality and complexity. To address this, one approach is to measure the similarity of activation responses to various inputs. Representational Similarity Matrices (RSMs) distill this similarity into scalar values for each input pair. These matrices encapsulate the entire similarity structure of a system, indicating which input leads to similar responses. While the similarity between images is ambiguous, we argue that the spatial location of semantic objects does neither influence human perception nor deep learning classifiers. Thus this should be reflected in the definition of similarity between image responses for computer vision systems. Revisiting the established similarity calculations for RSMs we expose their sensitivity to spatial alignment. In this paper, we propose to solve this through semantic RSMs, which are invariant to spatial permutation. We measure semantic similarity between input responses by formulating it as a set-matching problem. Further, we quantify the superiority of semantic RSMs over spatio-semantic RSMs through image retrieval and by comparing the similarity between representations to the similarity between predicted class probabilities. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep neural networks are trained to extract powerful feature representations for a wide range of downstream tasks. Despite this, their inner workings are highly-complex, making understanding how networks solve tasks and what they learn challenging. To obtain a better understanding of these fundamental questions, researchers in the fields of neuroscience, cognitive science, and machine learning independently developed various methods to interpret and relate representations [29]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In the realm of machine learning, a lot of prior-methods have been proposed to meaningfully measure the similarity between intermediate representations of ANNs [15, 31, 25, 19, 12]. Klabunde et al. [10] provides a comprehensive summary and categorizes them into measures based on a) Canonical Correlation Analysis (CCA) [25, 19] b) Alignment measures c) Representational Similarity Matrices (RSMs) d) Nearest Neighbors e) Topologies and f) Descriptive Statistics. ", "page_idx": 1}, {"type": "text", "text": "Of all these methods, Representational Similarity Matrix (RSM) [13] based measures have enjoyed the most attention over the last years. RSMs consist of sample-to-sample comparison, measuring the similarity between the (intermediate) responses of the same network to two different samples. Based on many such comparisons, the RSM represents the similarity structure of what a model considers similar. This representation of a system\u2019s behavior reduces the highly-dimensional, complex internal structure, that the model of interest may possess, to a $\\dot{N}\\times N$ Matrix for $N$ samples. It enables the comparison of the similarity structure of any system, as long as one can input the same samples and measure the similarity between the responses. ", "page_idx": 1}, {"type": "text", "text": "In the machine learning domain, this concept was introduced by Kornblith et al. [12] in conjunction with Centered Kernel Alignment (CKA) to compare the similarity between RSMs of different layers within a model or across models. CKA superseded previously popular Canonical Correlation Analysis (CCA) metrics [25, 19], since they need a vast amount of samples to measure similarity. Consequently, CKA was used in various applications, to measure the similarity between Transformers and CNNs [26] or wide and deep networks [21] and to understand catastrophic forgetting [27] or transfer learning [20] to name a few. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we revisit the key component of the most used similarity measure in the field: Representational Similarity Matrices and how they are constructed in the vision domain. The key contributions of our work are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We highlight that current RSMs are constructed in a way that couples localization and semantic information, which constraints one only to measure similarity if, spatial and semantic information aligns between two samples.   \n\u2022 To address this issue we propose semantic RSMs, which are invariant to spatial permutation and exclusively measure semantic similarity, by formulating it as a set-matching problem.   \n\u2022 We show that the inter-sample similarity of semantic RSMs leads to improved retrieval performance and better reflects the similarity between representations of classifiers and their predictive behavior.   \n\u2022 Moreover, due to the computational complexity of the proposed algorithm, we introduce approximations that significantly reduce computation time. ", "page_idx": 1}, {"type": "text", "text": "The Code is available here. ", "page_idx": 1}, {"type": "text", "text": "2 Representational Similarity ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Formalization To establish the concept of representational similarity in the context of computer vision, we provide a brief formalization of the problem. Let $x\\in\\{x_{0},\\ldots,x_{N}\\}$ denote the input samples for which we collect input responses $Z_{1}\\,\\in\\,\\{z_{1,0},\\dots,z_{1,N}\\}$ and $Z_{2}$ , which we call representations. The representations can take different shapes, with $Z_{\\mathrm{CNN}}\\in\\mathbb{R}^{N\\times C\\times W\\times H}$ denoting the responses of a CNN with $C$ channels and a spatial extent of $W,H$ or $Z_{\\mathrm{ViT}}\\,\\in\\,\\mathbb{R}^{N\\times D\\times T}$ denoting the responses of a ViT with depth $D$ and tokens $T$ . For the purpose of simplification and without loss of generality, we unify the spatial dimensions $W,H$ for CNNs and $T$ for ViTs into a joint spatial dimension $S_{.}$ , resulting in $Z\\in\\mathbb{R}^{N\\times C\\times S}$ . Given representations $Z$ , we can construct RSMs $K,L\\in\\mathbb{R}^{N\\times N},$ , with values $K_{i j}\\,=\\,k(z_{1,i},z_{1,j})$ and $\\mathbf{\\tilde{{L}}}_{i j}\\,=\\,l(z_{2,i},z_{2,j})$ measuring how similar the representation $z_{i}$ is to $z_{j}$ given the kernels $k$ or $l$ . The kernels define the measure of similarity between the representation vectors and hence play an important role. We introduce an exemplary kernel in Section 3 and all kernels used in this paper with some properties in Appendix A. With the two RSMs $K$ and $L$ at hand, it is possible to compare the similarity structure between the two models. As introduced in Kornblith et al. [12] one can use the HilbertSchmidt independence criterion (HSIC) [5, 28] to calculate the level of independence through Centered Kernel Alignment, providing a measure of similarity of the two representations $\\mathrm{\\bar{Z}_{1}}$ and $Z_{2}$ with $\\mathcal{H}$ denoting the centering matrix. ", "page_idx": 1}, {"type": "table", "img_path": "ypFgcT147Z/tmp/d56a0854f91dbeec6f2026b4385dc31408136dcf6457cd291cb9acd5d59d43d3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{HSIC}(K,L)=\\frac{1}{(n-1)^{2}}\\mathrm{tr}\\left(K\\mathcal{H}L\\mathcal{H}\\right)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{CKA}(K,L)=\\frac{\\mathrm{HSIC}(K,L)}{\\sqrt{\\mathrm{HSIC}(K,K)\\mathrm{HSIC}(L,L)}}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Alternatively, a variety of different measures based on RSMs are possible for which we refer to Section 3.3 of Klabunde et al. [10]. As highlighted above, the similarity calculation based on RSMs is a two-step process with the first being the calculation of the RSMs and the latter being the comparison of the RSMs. In this paper, we focus on the first step, by quantifying the importance of disentangling semantic similarity from spatial alignment. While not the focus of this paper, we provide qualitative examples of the downstream effect on CKA measures in Appendix $_\\mathrm{G}$ . ", "page_idx": 2}, {"type": "text", "text": "3 The Semantic Representational Similarity Matrix ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Representational Similarity Matrices (RSMs) are designed to reflect the system behavior of interest. The RSM $K$ , originally introduced by Kriegeskorte et al. [13], represents the similarity structure of a system given a set of inputs $x\\in\\{x_{0},\\ldots x_{N}\\}$ . Each value $K_{i j}$ in $K$ quantifies how similar the responses of two inputs $z_{i}$ and $z_{j}$ are to each other. The definition of what symmetries between representations similarity measures should be invariant to is a central point of debate. Previous work proposed permutation invariance [15], invariance to orthogonal transformations [12], or invariances to invertible linear transformations [25, 19]. While arguments for any of these invariances are valid, we believe that an important aspect has been neglected in the calculation of RSMs: The spatial alignment between the representations! ", "page_idx": 2}, {"type": "text", "text": "The dependency on spatial alignment Revisiting the structure of representations of a CNN, channels $C$ correspond to semantic concepts while the spatial position corresponds to where the semantic concept is localized in the input image [34]. Consequently, one can reformulate the representation $z_{i}$ of a sample $x_{i}$ to be fully defined by a set of semantic concept vectors $\\mathbf{v,}$ one for each spatial location $S$ : $z_{i}=\\{\\mathbf{v}_{0},\\dots,\\mathbf{v}_{S}\\}$ with $\\mathbf{v}\\in\\mathbb{R}^{C}$ . In the case of linear CKA [12], the RSMs are then calculated, between semantic concept vectors at the same spatial location. For instance, when employing the linear kernel ${\\bar{K}}_{i j}$ can be expressed as: ", "page_idx": 2}, {"type": "equation", "text": "$$\nK_{i j}=\\sum_{s}\\langle\\mathbf{v}_{z_{1},s},\\mathbf{v}_{z_{2},s}\\rangle\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This formulation emphasizes the coupling of semantic similarity and localization during similarity calculation, which we term as spatio-semantic RSMs. This coupling can lead to issues, e.g. when comparing an image to a translated version of itself. Due to the quasi translation-equivariant nature of $\\mathbf{\\boldsymbol{C}}\\mathbf{\\boldsymbol{N}}\\bar{\\mathbf{N}}\\mathbf{\\bar{s}}^{3}$ semantic vectors are translated similarly, changing the alignment of pairs of $\\mathbf{v}_{,}$ , leading to a low perceived similarity despite highly similar semantic vectors. This issue is visualized in a small toy example in Fig. 1 ", "page_idx": 3}, {"type": "text", "text": "3.1 Decoupling Localization and Semantic Content ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As shown above, current RSMs compare different input samples without accounting for the lack of spatial alignment. Previous work of Williams et al. [32] recognized this and introduced translation invariance to RSMs by finding the optimal translation $a,b$ of the representations $z_{j}^{\\prime}=\\left\\{\\mathbf{v}_{0+a,0+b},\\ldots,\\mathbf{v}_{w+a,h+b}\\right\\}$ to maximize similarity $K_{i j}$ $\\operatorname*{max}_{a,b}=\\langle z_{i},z_{j}^{\\prime}\\rangle$ through circular shifts. ", "page_idx": 3}, {"type": "text", "text": "While this is an improvement to no spatial alignment and emulates a CNN\u2019s inherent translation equivariance, we argue that the measure of representational similarity should not be constrained to what the underlying model is invariant to, but the similarity measure should be invariant to the possible spatial configurations of semantic features in the input image. ", "page_idx": 3}, {"type": "text", "text": "To motivate this, we propose a thought experiment: ", "page_idx": 3}, {"type": "text", "text": "Imagine we have trained a classifier with an augmentation pipeline including rotations. Given an image and a rotated version of the image, we extract representations $z$ at layer $i$ once for the normal $z_{i}$ and once for the rotated image $z_{i,r o t}$ . Due to the initial rotation, these representations may differ in earlier layers $i,$ due to the network extracting different edges and corners. However, if the network successfully learned to become invariant to the augmentation, it may have learned to map it to the same semantic vector $\\mathbf{v}$ at a later layer but at a different spatial location. For such cases, we argue that the similarity between the two representations should be high. Should the model be sensitive to the rotation, no semantically similar representations may be expressed at a later layer, which should lead to a low similarity. ", "page_idx": 3}, {"type": "text", "text": "This reasoning can be extended to all kinds of shifts, be they artificial augmentations like shearing or mirroring or natural variations of the input manifold. Subsequently, we argue that the similarity measure should be invariant to as many spatial shifts as possible. This alone allows one to measure the similarity of representations a model is invariant to, be these learned or designed invariances. Such variable shifts cannot be captured with simple translation operations. ", "page_idx": 3}, {"type": "text", "text": "3.1.1 Introducing permutation invariance ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To impose as minimal constraints on spatial structure as possible, we propose to make $K_{i j}$ invariant to all spatial permutations of the semantic concept vectors $\\mathbf{v}$ . Formalizing this we demand that the similarity $K_{i j}=k(z_{i},z_{j})=k(z_{i},{\\bf P}_{i j}z_{j})$ with $\\mathbf{P}_{i j}\\in\\mathbb{R}^{S\\times S}$ being a unique permutation matrix for the pair of $z_{i}$ and $z_{j}$ . To accomplish this, we propose to find the optimal permutation matrix $\\bar{\\mathcal{P}}_{i j}$ that maximizes the similarity $K_{i j}$ . ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{P}_{i j}=\\operatorname{argmax}_{P}\\quad k(z_{i},\\mathbf{P}_{i j}z_{j})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "To find the optimal permutation matrix $\\mathbf{P}_{i j},$ we decide to use the linear kernel $\\langle\\cdot,\\cdot\\rangle$ to maximize both the magnitude of activation and the direction of vectors, as both magnitude of activation and direction of the vectors matter[12]. This allows us to calculate an affinity matrix Aij \u2208RS\u00d7S measuring the similarity between all concept vectors: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{A}_{i j}=[\\mathbf{v}_{i,0},\\ldots\\mathbf{v}_{i,S}]^{\\mathsf{T}}\\big[\\mathbf{v}_{j,0},\\ldots\\mathbf{v}_{j,S}\\big].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "With this affinity matrix, bipartite set-matching algorithms, such as Hungarian matching, can be employed to find the optimal permutation matrix $\\mathcal{P}_{i j}$ that maximizes the inner product between $z_{i}$ and $z_{j}^{\\prime}$ . Finding all $\\bar{\\mathcal{P}}_{i j}$ for all pairs $i,j$ and applying the chosen kernel $k$ yields the semantic RSM. This semantic RSM is invariant to any arbitrary, unique spatial permutation $\\mathbf{P}_{i j}$ for each pair of representations, and, depending on the choice of kernel $k,$ invariant to orthogonal transformations $U\\in\\mathbb{R}^{C\\times C}$ along the channel dimension. These semantic RSMs can be used as a drop-in replacement for any other RSM, e.g. for applications such as calculating $\\mathsf{C K A}(K,L)$ to measure the similarity between systems. ", "page_idx": 3}, {"type": "image", "img_path": "ypFgcT147Z/tmp/4ef54b4638d247bd09351da561b964d777a0b7c4454062b1cbfb90da6cc54447.jpg", "img_caption": ["Figure 2: Semantic RSMs capture similarity independent of spatial localization, in contrast to current spatio-semantic RSMs. We utilize Tiny-ImageNet to generate partially overlapping crops of the same sample (left) and calculate RSMs for a trained ResNet18 model. The plot displays the original spatio-semantic RSMs (middle top) and our proposed semantic RSMs (middle bottom) across various layers for a single batch. Additionally, the distribution of similarity values over multiple batches is shown (right). The results indicate that spatio-semantic RSMs struggle to detect largely identical but translated images, while semantic RSMs exhibit an enhanced off-diagonal in the RSMs and a significant gap between distributions. This demonstrates the capability of our method to detect the same semantics even when translated. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Computational Complexity Finding the optimal permutation matrix $\\mathcal{P}_{i j}$ is NP-hard and needs to be repeated for each pair of representations $z_{i},z_{j}$ . With $N$ samples, this results in $\\frac{N\\!\\cdot\\!(N\\!+\\!1)}{2}$ unique permutations that need to be computed for a semantic RSM. The overall complexity of bipartite matching algorithms grows with the spatial dimensions cubed, resulting in $\\mathcal{O}(N^{2})\\times\\mathcal{O}(S^{3})$ . The outer $\\mathcal{O}(N^{2})$ complexity can be parallelized, or reduced by decreasing the batch size. However, the inner permutation can become timeconsuming, especially with large spatial dimensionality. To address this, we provide various approximations to reduce the complexity, which are detailed in Section 4.4. For all later experiments, except the translational toy example, we use the Batch-Optimal approximation with windows size $b$ 512, with the batch referring to batches of semantic concept vectors v and not samples. The pseudo-code for calculating semantic RSMs is visualized in the Appendix under Algorithm 1. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments: Semantic vs Spatio-Semantic RSMs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Given the novel permutation-invariant similarity definition, we evaluate the utility of our semantic RSMs relative to spatio-semantic RSMs for various similarity kernels, architectures, and tasks. Across all experiments we compare the linear kernel, the radial basis function (RBF) kernel, and the cosine similarity kernel, see Appendix A for details. ", "page_idx": 4}, {"type": "image", "img_path": "ypFgcT147Z/tmp/8b1e214bcc47d0c63b5da109ce3d6e906445bfd279696801a595e89fde7293c2.jpg", "img_caption": ["Figure 3: Relaxing the constraint of spatial alignment leads to better retrieval. We leverage general feature extractors to embed images of the EgoObjects dataset. We then compare these embeddings either with or without permutation invariance. PI: Permutation Invariant "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "4.1 Translation sensitivity ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To illustrate the problems of coupling semantic content and localization a toy dataset is created using $84\\,\\mathrm{\\overline{{\\times}}\\,84}$ pixels large, downsampled images of ImageNet [2]. For each image two $64\\times64$ crops are extracted, one from the upper-left and one from the lower-left corner, resulting in two images that share $44\\times64$ identical pixels (Fig. 2 left). Ten upper-left and ten lower-left crops are then used to extract representations of a ResNet18 [7], which are subsequently used to calculate spatio-semantic and semantic RSMs at different layers of the architecture (Fig. 2 middle). As kernel, we use the radial basis function, as it provides bounded similarity values allowing a better visualization. ", "page_idx": 5}, {"type": "text", "text": "As expected, the spatio-semantic RSM measures low similarity between pairs of overlapping crops, due to the semantic concept vectors not aligning. Only in the last layer, after many pooling operations, the off-diagonal is slightly expressed. Conversely, our semantic RSM is capable of detecting the high semantic similarity of the partially overlapping crops throughout the entire depth of the architecture, as evident by the highly similar off-diagonal. ", "page_idx": 5}, {"type": "text", "text": "Aggregating the similarity values between partially-overlapping and between different images across multiple batches, allows us to measure the distribution of similarity values between overlapping crops, and non-related image comparisons. Throughout the entire depth of the architecture, the similarity distributions show that our measure better separates overlapping images from different images. Notably, the similarity distribution in spatiosemantic RSMs shows a significant overlap of the distributions of partially overlapping images and non-related images, making differentiation between them difficult (Fig. 2 right). A similar toy experiment for a ViT-B/16 [4], is provided in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "4.2 Similarity-based retrieval ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To test the impact of the semantic RSMs in real-world applications, we now investigate the common task of image retrieval. Each entry in an RSM quantifies a sample-to-sample similarity value, which can be directly used for retrieval. While not specifically designed for it, we argue that better retrieval performance reflects a better inter-sample similarity. This allows us to quantify improvements in the RSM structure. To measure retrieval performance the EgoObjects dataset [35] is used. It contains frames of video that capture the same scene from different viewing perspectives and lighting conditions. This results in object centers being distributed across the extent of the image. ", "page_idx": 5}, {"type": "text", "text": "By randomly sampling 2000 query images and 5000 database images from the test set and using general feature extractors to extract embeddings from them we construct RSMs that allow us to do retrieval. As feature extractors we use CLIP (ViT/B32) [24], CLIPSeg (Rd64) [16], DinoV2-Giant [22], SAM (ViT/B32) [9] and BIT-50 [11] and as kernels for similarity calculation we use the cosine similarity, RBF and the inner product. ", "page_idx": 5}, {"type": "text", "text": "For all RSMs, we retrieve the most similar image that is not part of the same video \u2013 the same scene but different conditions are allowed. As multiple objects can be present in each scene, we quantify retrieval performance by the F1-Score, measuring the overlap of annotated objects between images. Due to the rather complex dataset, we elaborate this in more detail in Appendix D.1. ", "page_idx": 5}, {"type": "image", "img_path": "ypFgcT147Z/tmp/faa5e2dc439cb48e813112bd721784422275a2915575f5b05d8021ccbff92f89.jpg", "img_caption": ["Figure 4: Retrieving by permutation invariant similarity returns similar scenes of different spatial geometry. We visualize the top 3 most similar images according to two exemplary query images for SAM ViT/B32. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Across all architectures and metrics, the inclusion of permutation invariance (PI) for the similarity calculation improves retrieval performance relative to the non-invariant similarity, in some cases with a dramatic difference in performance, see Fig. 3. For models designed for dense downstream tasks like SAM or CLIPSeg, the retrieval performance changes particularly much, while models with more global reasoning, like BiT improve less, relatively. ", "page_idx": 6}, {"type": "text", "text": "Qualitative Similarity Aside from a quantitative comparison, we visualize the most similar retrieved images for two exemplary queries of SAM in Fig. 4 as case examples. ", "page_idx": 6}, {"type": "text", "text": "Left Query: The image displays various utensils scattered on a desk. When retrieving with the permutation-invariant similarity metric two images of the same scene but a very different perspective are successfully retrieved as most similar. Retrieving with the non-invariant similarity metric fails to retrieve similar images, due to lack of spatial alignment of the semantic concepts. Instead, it retrieves images of a whiteboard, possibly due to its spatial alignment with the paper on the desk. ", "page_idx": 6}, {"type": "text", "text": "Right Query: The image features a blender on a counter. The retrieval based on nonpermutation-invariant similarity fails to retrieve any of the semantically similar scenes and returns images with a light switch, likely due to the spatial alignment of the light-switchlooking object to the right of the blender. Contrary, the retrieval based on permutationinvariant similarity correctly returns the blender in all cases from different perspectives. Additional qualitative examples are provided in Appendix D.3. ", "page_idx": 6}, {"type": "text", "text": "These experiments display clearly, that demanding spatial alignment can be a significant shortcoming when semantically similar concepts are misaligned. In Fig. 4, the network learned to represent the objects very similarly, despite a shift in perspective, but due to the same objects not aligning anymore, spatio-semantic similarity fails to recognize this. This effect should generalize to other datasets where objects are not heavily centered. For datasets with heavy object-centric behavior, like ImageNet, this should be less pronounced. ", "page_idx": 6}, {"type": "table", "img_path": "ypFgcT147Z/tmp/3c86875f12d2a7115fd6bbf887b5f6de8ea3635c7477e537855da4cd4f0308bc.jpg", "table_caption": ["Table 1: Similarity invariant to spatial permutations is better at predicting if the class probabilities will be similar. PI: Permutation Invariant "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.3 Output similarity vs Representational Similarity ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "While the retrieval experiments relate to a rather human notion of similarity, one can raise the question if semantic RSMs are also better at measuring the similarity for classifiers. For each pair of samples, we can compare how similar the predicted class probabilities of a model are and compare this to the representational similarity. A commonly used metric for this is the Jensen-Shannon Divergence (JSD), which quantifies how dissimilar the two probability distributions are from one another. More details are provided in Appendix E. ", "page_idx": 7}, {"type": "text", "text": "Consequently, we use various classifiers trained to predict ImageNet1k from Huggingface and compare the Pearson correlation $\\rho$ between their JSD and the representational similarity of their last hidden layer. We chose to use the Pearson correlation, as it allows observing a direct linear behavior between representational similarity and predictive similarity. Again we measure semantic similarity and spatio-semantic similarity with different kernels. Due to JSD measuring dissimilarity, we want the correlation to be as negative as possible. As models we use multiple ResNets [7], ViTs [4] , a fine-tuned DinoV2 [22] classifier from and a convnextv2[33] classifier. ", "page_idx": 7}, {"type": "text", "text": "The results, displayed in Table 1, show that for almost all architectures and kernels tested, the permutation invariant similarities are better at capturing the notion of what a classifier deems similar. While better than the spatio-semantic similarity, overall correlations are generally low, indicating that either, the similarity metric is confounded by irrelevant representations, or that the kernels should be improved. Moreover, the RBF kernel sometimes provides a positive correlation indicating it is unsuitable to predict the similarity of output probabilities, whereas the Cosine Similarity and the Inner Product both are consistently negative for all architectures tested. ", "page_idx": 7}, {"type": "text", "text": "4.4 Optimizing runtime ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Since we find the best possible permutation matrix through linear sum assignment algorithms that maximize the inner product of two samples, we can guarantee that the $K_{i j,s e m a n t i c}\\geq$ $K_{i j}\\forall i,j$ . This provides us with an upper bound of similarity that can be leveraged to measure how much of the maximally achievable semantic similarity was measured by the spatio-semantic similarity. Additionally, it can be used as a baseline to estimate the quality of permutation matrices $\\mathbf{\\dot{P}}_{i j}$ provided by faster, approximative assignment algorithms. ", "page_idx": 7}, {"type": "text", "text": "Decreasing computational complexity Determining the optimal permutation between samples poses a substantial computational challenge with a complexity of $\\mathcal{O}(S^{3})$ for each of the $\\bar{\\mathcal{O}}\\bar{(N^{2})}$ pairs in the same mini-batch, particularly for early layers with large spatial resolution $S$ . Although, in theory, the calculation of the $K$ matrix needs to be conducted only once for the desired representations, applying the method to representations with larger spatial extents becomes impractical with the demands of optimal matching. ", "page_idx": 7}, {"type": "text", "text": "To mitigate runtime, two options are available: reducing the batch size $N$ to lessen the number of permutation calculations or decreasing the time spent on finding the permutation. Given that scenarios like image retrieval often desire larger batches, our focus is on minimizing the time required to obtain suitable assignments. ", "page_idx": 7}, {"type": "image", "img_path": "ypFgcT147Z/tmp/57e444df5299cff3adf32b5df80a42352e4cbd7eb81ad83d60fb1fc4a69e4f31.jpg", "img_caption": ["Figure 5: Approximative algorithms yield Figure 6: Relative similarity is not isotropic. comparable matching quality to optimal When aligning semantic concepts we observe algorithms. The ratio of similarity from that similarity changes heterogeneously, invarious approximations relative to maximal dicating that some pairs of samples have semantic similarity is visualized across mul- more spatially misaligned semantic concepts tiple layers of a ResNet18. than others. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "ypFgcT147Z/tmp/52335068072e595c0c48f5057b9d003c2d1816960dd7a5fa95808809ebfd05f5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Solving the optimal bipartite matching between semantic concept vectors is equivalent to the well-known assignment problem [18, 1]. We attempted to find existing approximate algorithms for this purpose. Unfortunately, most established algorithms primarily focus on optimal solutions, and existing approximate algorithm implementations, such as those based on the auction algorithm [6], are not runtime-optimized, often taking longer than optimal algorithms in our experiments. To enhance computational efficiency nonetheless, we explored three tailored approximation algorithms: ", "page_idx": 8}, {"type": "text", "text": "A) A Greedy breadth-first matching (Greedy) ", "page_idx": 8}, {"type": "text", "text": "B) An optimal matching of the TopK values based on their Norm, followed by the Greedy algorithm for the remaining samples (TopK-Greedy) ", "page_idx": 8}, {"type": "text", "text": "C) Optimal matching of smaller batches, with samples batched by their Norm (BatchOptimal) ", "page_idx": 8}, {"type": "text", "text": "For explicit details on the approximation algorithms, we refer to Appendix F. We conducted a comprehensive comparison between the approximate algorithms and the optimal algorithm. We compare their runtime per sample and the quality of matches, quantified by the average relative similaritykoptkimal . The evaluation utilized representations from a ResNet18 on TinyImageNet, as illustrated in Fig. 5. ", "page_idx": 8}, {"type": "text", "text": "It can be seen that the measured spatio-semantic similarity for TinyImageNet samples are, on average, $30\\%$ lower with layers of higher spatial resolution exceeding $40\\%$ . This suggests a notable misalignment of semantic concept vectors. Notably, the Batch-Optimal approximation stands out as a reliable approximation for optimal matching. The fastest of the Batch-Optimal approximation methods shows $<8\\%$ error while improving run-time $\\times36$ relative to the fastest optimal algorithm for spatial extent 4096, while no spatial alignment shows $42\\%$ deviation from the optimal matching. Moreover, we highlight the time vs accuracy trade-off of the different optimal and approximate algorithms in Appendix F.1. Furthermore, it can be seen that the changes between spatio-semantic and semantic RSMs are anisotropic, as highlighted in Fig. 6, indicating scale invariant downstream applications may be influenced. ", "page_idx": 8}, {"type": "text", "text": "5 Discussion, Limitations, and Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The concept of Representational Similarity Matrices (RSMs) is a powerful tool to represent the similarity structure of complex systems. In this paper we revisit the construction of such RSMs for neural networks of the vision domain, question the current state, and propose semantic RSMs, warranting discussion. ", "page_idx": 8}, {"type": "text", "text": "Spatio-semantic coupling Being aware that current, spatio-semantic RSMs demand semantic concepts to be aligned is highly relevant to understand what RSMs are sensitive to. Previous work [32] identified this shortcoming and proposed translation invariance, partially addressing this issue. We argue translation invariance is insufficient, since models may learn invariances during training, which the translation invariant metric would not be sensitive to. Subsequently, we propose a new \u2013 spatially permutation invariant \u2013 similarity measure between samples that allows the detection of similarity whenever a model expresses similar semantic vectors in its representations, irrespective of spatial geometry. To highlight the benefits of our similarity, we propose that better similarity measures should allow more accurate retrieval when comparing last-layer representations and should allow better predictions about the similarity of class probabilities of a classifier. However, we acknowledge certain limitations in our current evaluation. Specifically, we have not yet compared our method to more established retrieval techniques. Traditional retrieval methods are often not applied to representations directly but utilize a lower-dimensional non-spatial, global vector representing the entire sample. In contrast, we chose to limit ourselves to methods that are directly applied to the representations. ", "page_idx": 9}, {"type": "text", "text": "Computational Complexity Aside from quantitative or qualitative benefits, the construction of semantic RSMs is time-consuming, limiting its applicability. This complexity mostly affects layers of large spatial extent, which mostly corresponds to early CNN layers while later layers and ViTs are unproblematic. Our proposed Batch-optimal approximation alleviates this partially, yet application to large-scale representations at higher resolution, like at the output of a segmentation architecture with spatial extents of $s{=}\\bar{6}5.536$ would be too costly. We leave optimizing the compute efficiency or finding better approximations for future work. ", "page_idx": 9}, {"type": "text", "text": "Conclusion In conclusion, our investigation into semantic RSMs has shed light on the limitations of spatio-semantic RSMs and introduced a novel approach to disentangle spatial alignment from semantic similarity. The proposed method provides a more accurate measure of how representations capture underlying semantic content, showcasing its potential in various applications, particularly in scenarios where spatial alignment cannot be assumed. While challenges such as computational complexity and scalability need to be addressed, the findings open avenues for further research and improvement in the analysis of neural network representations. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] R. E. Burkard. Quadratic assignment problems. European Journal of Operational Research, 15(3):283\u2013289, 1984.   \n[2] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \n[3] F. Ding, J.-S. Denain, and J. Steinhardt. Grounding representation similarity through statistical testing. Advances in Neural Information Processing Systems, 34:1556\u20131568, 2021.   \n[4] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[5] A. Gretton, R. Herbrich, A. Smola, O. Bousquet, B. Sch\u00f6lkopf, and A. Hyv\u00e4rinen. Kernel methods for measuring independence. Journal of Machine Learning Research, 6(12), 2005.   \n[6] S. Guthe and D. Thuerck. Algorithm: A fast scalable solver for the dense linear (sum) assignment problem. ACM Trans. Math. Softw., 47(2), apr 2021. ISSN 0098-3500. doi: 10.1145/3442348. URL https://doi.org/10.1145/3442348.   \n[7] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.   \n[8] R. Jonker and T. Volgenant. A shortest augmenting path algorithm for dense and sparse linear assignment problems. In DGOR/NSOR: Papers of the 16th Annual Meeting of DGOR in Cooperation with NSOR/Vortr\u00e4ge der 16. Jahrestagung der DGOR zusammen mit der NSOR, pages 622\u2013622. Springer, 1988.   \n[9] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4015\u20134026, 2023.   \n[10] M. Klabunde, T. Schumacher, M. Strohmaier, and F. Lemmerich. Similarity of neural network models: A survey of functional and representational measures. arXiv preprint arXiv:2305.06329, 2023.   \n[11] A. Kolesnikov, L. Beyer, X. Zhai, J. Puigcerver, J. Yung, S. Gelly, and N. Houlsby. Big transfer (bit): General visual representation learning. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part V 16, pages 491\u2013507. Springer, 2020.   \n[12] S. Kornblith, M. Norouzi, H. Lee, and G. Hinton. Similarity of neural network representations revisited. In 36th International Conference on Machine Learning, ICML 2019, volume 2019-June, pages 6156\u20136175, 2019. ISBN 9781510886988. URL https: //arxiv.org/abs/1905.00414.   \n[13] N. Kriegeskorte, M. Mur, and P. A. Bandettini. Representational similarity analysisconnecting the branches of systems neuroscience. Frontiers in systems neuroscience, page 4, 2008.   \n[14] A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.   \n[15] Y. Li, J. Yosinski, J. Clune, H. Lipson, and J. Hopcroft. Convergent learning: Do different neural networks learn the same representations? Technical report, 2015.   \n[16] T. L\u00fcddecke and A. Ecker. Image segmentation using text and image prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 7086\u20137096, June 2022.   \n[17] S. Marcel and Y. Rodriguez. Torchvision the machine-vision package of torch. In Proceedings of the 18th ACM international conference on Multimedia, pages 1485\u20131488, 2010.   \n[18] S. Martello and P. Toth. Linear assignment problems. In North-Holland Mathematics Studies, volume 132, pages 259\u2013282. Elsevier, 1987.   \n[19] A. S. Morcos, M. Raghu, and S. Bengio. Insights on representational similarity in neural networks with canonical correlation. Technical report, 2018. URL https: //arxiv.org/abs/1806.05759.   \n[20] B. Neyshabur, H. Sedghi, and C. Zhang. What is being transferred in transfer learning? Advances in neural information processing systems, 33:512\u2013523, 2020.   \n[21] T. Nguyen, M. Raghu, and S. Kornblith. Do wide and deep networks learn the same things? uncovering how neural network representations vary with width and depth. arXiv preprint arXiv:2010.15327, 2020.   \n[22] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023.   \n[23] L. Perron and V. Furnon. Or-tools. URL https://developers.google.com/ optimization/.   \n[24] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[25] M. Raghu, J. Gilmer, J. Yosinski, and J. Sohl-Dickstein. SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability. Advances in Neural Information Processing Systems, 2017-Decem:6077\u20136086, jun 2017. URL http://arxiv.org/abs/1706.05806.   \n[26] M. Raghu, T. Unterthiner, S. Kornblith, C. Zhang, and A. Dosovitskiy. Do vision transformers see like convolutional neural networks? Advances in neural information processing systems, 34:12116\u201312128, 2021.   \n[27] V. V. Ramasesh, E. Dyer, and M. Raghu. Anatomy of catastrophic forgetting: Hidden representations and task semantics. arXiv preprint arXiv:2007.07400, 2020.   \n[28] L. Song, A. Smola, A. Gretton, J. Bedo, and K. Borgwardt. Feature selection via dependence maximization. Journal of Machine Learning Research, 13:1393\u20131434, 2012. ISSN 15324435.   \n[29] I. Sucholutsky, L. Muttenthaler, A. Weller, A. Peng, A. Bobu, B. Kim, B. C. Love, E. Grant, I. Groen, J. Achterberg, et al. Getting aligned on representational alignment. arXiv preprint arXiv:2310.13018, 2023.   \n[30] P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau, E. Burovski, P. Peterson, W. Weckesser, J. Bright, S. J. van der Walt, M. Brett, J. Wilson, K. J. Millman, N. Mayorov, A. R. J. Nelson, E. Jones, R. Kern, E. Larson, C. J. Carey, \u0130. Polat, Y. Feng, E. W. Moore, J. VanderPlas, D. Laxalde, J. Perktold, R. Cimrman, I. Henriksen, E. A. Quintero, C. R. Harris, A. M. Archibald, A. H. Ribeiro, F. Pedregosa, P. van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17:261\u2013272, 2020. doi: 10.1038/s41592-019-0686-2.   \n[31] L. Wang, L. Hu, J. Gu, Y. Wu, Z. Hu, K. He, and J. Hopcroft. Towards Understanding Learning Representations: To What Extent Do Different Neural Networks Learn the Same Representation. Technical report, 2018. URL https://arxiv.org/abs/1810. 11750.   \n[32] A. H. Williams, E. Kunz, S. Kornblith, and S. Linderman. Generalized shape metrics on neural representations. Advances in Neural Information Processing Systems, 34:4738\u20134750, 2021.   \n[33] S. Woo, S. Debnath, R. Hu, X. Chen, Z. Liu, I. S. Kweon, and S. Xie. Convnext v2: Co-designing and scaling convnets with masked autoencoders. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16133\u201316142, 2023.   \n[34] J. Yosinski, J. Clune, A. Nguyen, T. Fuchs, and H. Lipson. Understanding neural networks through deep visualization. arXiv preprint arXiv:1506.06579, 2015.   \n[35] C. Zhu, F. Xiao, A. Alvarado, Y. Babaei, J. Hu, H. El-Mohri, S. Culatana, R. Sumbaly, and Z. Yan. Egoobjects: A large-scale egocentric dataset for fine-grained object understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 20110\u201320120, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Kernel function definitions ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Defining the notion of similarity between two vectors is a matter of preference and wanted properties. In this work, we include the Radial Basis Function kernel Eq. (6) and the linear kernel Eq. (7), as well as the cosine similarity kernel Eq. (8). We include the former two as they were proposed for RSM construction by Kornblith et al. [12] and the cosine similarity due to its popularity in the retrieval domain, despite generally being applied to class probabilities. ", "page_idx": 12}, {"type": "text", "text": "We denote that in our manuscript we refer to the linear kernel as the inner product and dot product interchangeably as they correspond to the same operation. ", "page_idx": 12}, {"type": "equation", "text": "$$\nk_{\\mathrm{RBF}}(\\mathbf{x},\\mathbf{y})=\\exp\\left(-\\frac{||\\mathbf{x}-\\mathbf{y}||^{2}}{2\\sigma^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 12}, {"type": "equation", "text": "$$\nk_{\\mathrm{linear}}(\\mathbf x,\\mathbf y)=\\langle\\mathbf x,\\mathbf y\\rangle\n$$", "text_format": "latex", "page_idx": 12}, {"type": "equation", "text": "$$\nk_{\\mathrm{cosine}}(\\mathbf{x},\\mathbf{y})=\\frac{\\langle\\mathbf{x},\\mathbf{y}\\rangle}{\\|\\mathbf{x}\\|\\|\\mathbf{y}\\|}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Different properties The three selected kernels all have different properties: The RBF kernel and the Cosine similarity are bounded between $k_{R B F}(x,y),k_{c o s i n e}(\\dot{x_{,}}y)\\in[0,1]$ $\\forall x,y\\in\\mathcal{R},$ while the $k_{l i n e a r}(x,y)$ is not bounded. ", "page_idx": 12}, {"type": "text", "text": "Moreover, the RBF kernel is parametrized by $\\sigma_{.}$ , which influences at which rate the distance between the representations results in a decrease in similarity. For all our experiments we choose $\\sigma$ as the square root of the median Euclidean distance of all distances within a mini-batch. ", "page_idx": 12}, {"type": "text", "text": "B Translation Sensitivity of a ViT-B/16 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Similarly to the translated Tiny-ImageNet experiment, we prepared a very similar experiment for a ViT-B/16 vision transformer that was pre-trained on ImageNet1k. We utilize the implementation and weights provided by torchvision [17]. ", "page_idx": 12}, {"type": "text", "text": "Given the larger ImageNet images, we resample them to $324\\times324$ pixels and crop two partially overlapping images of size $224\\times224$ from it. Given these image pairs, we calculate semantic and spatio-semantic RSMs again, see Fig. 7. ", "page_idx": 12}, {"type": "text", "text": "It is important to note that our approach intentionally avoids achieving a perfect translation that would lead to the same patchified tokens, as we shift the image by a factor that is not divisible by 16, the patching window size. We believe this realistic imperfection is preferable to a perfect overlap, where identical tokens would be formed from the exact same set of pixels. ", "page_idx": 12}, {"type": "text", "text": "Similarly to the previous partially overlapping crop experiment, we can observe that spatio-semantic RSMs are incapable of identifying the largely identical content of the two partially overlapping crops due to their different localization. The spatio-semantic RSMs can capture this notion of similarity as evidenced by the off-diagonal and the larger gap in the distribution of similarity between partially overlapping samples and independent samples, Fig. 7 (bottom). ", "page_idx": 12}, {"type": "text", "text": "Contrary to the CNN example in the main, overall similarity between samples is much lower overall and separation between translated image pairs and random image pairs follows a vastly different trajectory. While there is a profound difference, the origin of this difference cannot be clearly made out. We hypothesize that this may be due to the different ways that Transformers process information and learn different representations, as highlighted in Raghu et al. [26]. ", "page_idx": 12}, {"type": "text", "text": "Moreover, we emphasize, that calculating the optimal permutation is significantly faster for ViTs than the CNNs, as the early tokenization reduces the spatial dimension $S$ substantially at an early stage, whereas the iterative downsampling of CNNs makes comparing representations of early layers very costly. ", "page_idx": 12}, {"type": "image", "img_path": "ypFgcT147Z/tmp/16c4e890b8b0fca2c218d54a61291d97ac5fd0848327a17284cc3a4b17661097.jpg", "img_caption": ["Figure 7: Semantic RSMs do also capture spatial translations for token sequences of ViT\u2019s. We calculate spatio-semantic and semantic RSMs with the Radial Basis Function (RBF) kernel for a ViT-B/16 with representations extracted from ImageNet. Similarly, we introduce partially-overlapping crops that get tokenized and processed by the ViT. Due to the initial shift, the token sequence does not align anymore between the crops. Similar to the CNNs, spatio-semantic RSMs exhibit low similarity values in the off-diagonals, providing a limited indication of overlapping content between crops. In contrast, Semantic RSMs prove notably more effective in discerning substantial overlap, offering higher similarity values in the off-diagonals and thereby indicating a greater degree of similarity between large portions of the image. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "C Pseudo Code ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In addition to our provided explanation of the algorithm in the main manuscript, we provide the pseudo-code used to compute semantic RSMs in Algorithm 1. The only difference between semantic and spatio-semantic RSMs algorithmically is where the optimal permutation is calculated that maximally aligns the two representations. The current definition of spatio-semantic RSMs assumes that spatial locations are corresponding, while semantic RSMs calculate correspondence through similarity matching. ", "page_idx": 13}, {"type": "text", "text": "D Additions: Retrieval Experiment ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In addition to the provided retrieval examples in the main manuscript, we provide more details on the retrieval experiments in Appendix D.1, an additional table holding the quantitative data of Fig. 3 with varying database sizes in Appendix D.2 and lastly, additional qualitative retrieval examples for each model including direct comparisons of all models for the same query image in Appendix D.3. ", "page_idx": 13}, {"type": "text", "text": "D.1 Details of Retrieval Experiment ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "For the retrieval experiment, we utilize the EgoObjects dataset [35]. It contains multiple frames from multiple videos, with multiple videos capturing the same scene under different shifts like lighting conditions, distances, viewing angles, and different motion trajectories. For each frame, multiple objects of different categories can be present and are annotated ", "page_idx": 13}, {"type": "text", "text": "Algorithm 1: Semantic RSM calculation. We calculate the optimal permutation matrix, resulting in maximal similarity between the representations of two samples. ", "page_idx": 14}, {"type": "image", "img_path": "ypFgcT147Z/tmp/e82f7264dacbccd640e9784fe9db0f8c3609eb914b392320fd1684ca9d4247c1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "through a bounding box. Moreover, frames can vary in spatial resolution, yet a large fraction was captured in 16:9 format of $1920\\times1028$ pixels. ", "page_idx": 14}, {"type": "text", "text": "Image preprocessing For our experiments, we utilize the EgoObjects test set, which is comprised of $29.5\\mathrm{K}$ images. Of these $29.5\\mathbf{k}$ we remove all images not in 16:9 format and resize the remaining to the $1920\\times1028$ format. This discards roughly 10k images. ", "page_idx": 14}, {"type": "text", "text": "Of all remaining images, we then draw $2\\mathbf{k}$ query images and 5k database images used for extracting embeddings for similarity calculation and later retrieval. Naturally, we sample in a way to keep the 2k query and 5k database image sets non-overlapping. When passing the images to the models for feature extraction each image is preprocessed according to the corresponding Huggingface ImagePreprocessor. This mostly represents resizing the image by the shortest edge to the expected image input dimensions and normalizing the image. The only exception is SAM, of which we use the official implementation, which handles feature extraction and embedding of the image itself. ", "page_idx": 14}, {"type": "text", "text": "Feature Extraction and preparation As mentioned in the main manuscript we use ", "page_idx": 14}, {"type": "text", "text": "1. CLIP (ViT/B32) [24]   \n2. ClipSeg (Rd64) [16]   \n3. DinoV2-Giant [22]   \n4. SAM (ViT/B32) [9] and   \n5. BIT-50 [11] ", "page_idx": 14}, {"type": "text", "text": "as general feature extractors as they were trained on a vast amount of data.4. ", "page_idx": 14}, {"type": "text", "text": "Of all models, we use the last hidden layer as image embeddings should they not per-default provide image embeddings as output. After extracting representations we calculate the mean from the database embeddings to zero-center all representations by, query and database representations alike. ", "page_idx": 14}, {"type": "text", "text": "RSM construction Given all 2000 query embeddings and 5000 database embeddings, we calculate the RSMs. To parallelize this process we mini-batch the representations into $100\\times100$ pairs and populate the $2000\\times5\\bar{0}00$ matrix in this fashion. We denote that this proved to be necessary for models with large spatial embedding dimensions like SAM, starring $64\\times64$ spatial extent. Moreover, we denote that, due to the RBF choosing its parameter based on the median of the measured values within one batch this patch-wise calculation is not optimal for this kernel. The inner product and the cosine similarity kernels are not affected by this. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Retrieval measurement Given the RSMs containing a sample-to-sample similarity measure, we can retrieve the most similar sample of the database for each query from it. ", "page_idx": 15}, {"type": "text", "text": "As each image can contain multiple objects measuring retrieval performance is not trivial. For both images we quantify how many objects of each class are present in the image, resulting in a count of class instances for each image. With the query image representing the ground truth (GT) and the database representing the prediction, we match class instance counts. Each correctly matched GT instance represents a TP, each missed an FN and all unmatched database instances represent an FP. ", "page_idx": 15}, {"type": "text", "text": "To formalize: Let $Q_{c}$ be the number of instances of class $c$ in the query image and $D_{c}$ be the number of instances of class $c$ in the database image. With this, the used $\\breve{\\mathrm{F}}1$ metric can be expressed as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle T P=\\sum_{c\\in C}\\operatorname*{min}(Q_{c},D_{c})}}\\\\ {{\\displaystyle F N=\\sum_{c\\in C}\\operatorname*{max}(0,Q_{c}-D_{c})}}\\\\ {{\\displaystyle F P=\\sum_{c\\in C}F P_{c}=\\sum_{c\\in C}\\operatorname*{max}(0,D_{c}-Q_{c})}}\\\\ {{\\displaystyle F1=\\frac{2\\cdot T P}{2\\cdot T P+F P+F N}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "D.2 Additional Quantitative Retrieval data ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In addition to the results highlighted in Fig. 3 we provide retrieval results for varying database sizes to retrieve from EgoObjects. Specifically, results for database sizes of 2.5k, $5\\bar{\\mathrm{k}},$ and $10\\mathbf{k}$ samples are given in Table 2. ", "page_idx": 15}, {"type": "table", "img_path": "ypFgcT147Z/tmp/731ecbb7b39d4a6683faffea64583b8a8ff51856f712356bfb0887175fd77edc.jpg", "table_caption": ["Table 2: Quantitative results of the EgoObject retrieval experiment for multiple models and multiple database sizes. Database Size 5.000 represents Fig. 3 of the main manuscript. It can be observed that models with a greater spatial extent (CLIPSeg and SAM) show greater improvement in retrieval performance over models with a lower spatial extent (DinoV2-Giant, BiT-50, CLIP). Moreover, it can be observed that retrieval improvements decrease with growing database size. This is likely due to the increasing odds of finding one image where spatial position and semantics align w.r.t. the query images. PI: Permutation Invariant, Diff: Difference (PI - None) "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "D.3 Additional Qualitative Examples ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In addition to the two qualitative examples provided for SAM in the main, additional examples are provided. Qualitative examples are picked from the first 50 query images when retrieving from a database size of 5.000 images. Additional qualitative retrieval examples are provided for DinoV2 in Fig. 8, CLIP in Fig. 9, BiT-50 in Fig. 10, CLIPSeg in Fig. 11 and SAM in Fig. 12. Moreover, we highlight a direct comparison between all models for the same images in Fig. 13 and Fig. 14. ", "page_idx": 16}, {"type": "text", "text": "Across all models, it can be observed that models retrieve images where semantic content and spatial positions are aligned when using standard cosine similarity. This leads to larger-scale objects like desks, tiled floors, or countertops dominating retrieval when imaged from a similar perspective. When decoupling spatial-alignment from semantic content, images of the same scene but a different perspective get retrieved more often, leading to the same scene appearing more regularly in the 5 nearest neighbors. This effect is currently not quantified in the F1 metric, due to only comparing object presence between the query and the 1st neighbor and not the average F1 between the query and the top 5 neighbors. We opted against using this, as retrieval metrics most commonly consider the maximum match in the top 5 neighbors. ", "page_idx": 16}, {"type": "text", "text": "Moreover, it can be observed that models with lower spatial extent can retrieve quite different neighbors as opposed to models with higher spatial extent, see Fig. 13. While DinoV2 and CLIP retrieve very similar 4th and 5th neighbors of the same book object without the smaller mouse and headphone object, CLIPSeg and SAM retrieve scenes with these two objects still present instead. ", "page_idx": 16}, {"type": "text", "text": "D.4 Cityscapes Quantitative Retrieval ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Since previous results were limited to the EgoObjects dataset we provide an additional quantitative experiment on CityScapes. Analog to before, we use $\\scriptstyle\\mathrm{N}=500$ validation images as the query dataset and the remaining $\\scriptstyle\\mathrm{N}=2975$ training images as the database for retrieval. We utilize the IoU metric to compare the presence of semantic classes between the images. ", "page_idx": 16}, {"type": "image", "img_path": "ypFgcT147Z/tmp/9e71b6f34feb8091d6f192b7870a32d7f9f90533b49e35f47b85aa1eeed1ba70.jpg", "img_caption": ["Figure 8: Additional qualitative retrieval samples for DinoV2. We visualize the top 5 most similar neighbors for four query images. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "ypFgcT147Z/tmp/b897288b0daa9759db8fe104061bbfe657e6c9fa429a40ab2ef460236c7a21bc.jpg", "img_caption": ["Figure 9: Additional qualitative retrieval samples for CLIP. We visualize the top 5 most similar neighbors for four query images. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "ypFgcT147Z/tmp/2773b2a10dbc6f570e533a0e97cf7ba856d1e2d43c93f3125516f2498b6c4518.jpg", "img_caption": ["Figure 10: Additional qualitative retrieval samples for BiT-50. We visualize the top 5 most similar neighbors for four query images. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "ypFgcT147Z/tmp/df74f8337426afa7a694acdde70d304f664d2c9930a28727617f9c7a1ad29881.jpg", "img_caption": ["Figure 11: Additional qualitative retrieval samples for CLIPSeg. We visualize the top 5 most similar neighbors for four query images. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "ypFgcT147Z/tmp/030122376cd09be973ec14c063f3b8d7a1f08e1b2ab2fa48b02d5c1f389a4921.jpg", "img_caption": ["Figure 12: Additional qualitative retrieval samples for SAM. We visualize the top 5 most similar neighbors for four query images. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "ypFgcT147Z/tmp/c99a279d7ee7ff2f6c1e90751652e81f9450bbf1a8604a620df3980882286af9.jpg", "img_caption": ["Figure 13: Direct comparison of models. We visualize the top 5 most similar images of all models retrieved through cosine similarity or permutation invariant cosine similarity. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "ypFgcT147Z/tmp/310b3d7a5b7cdc7315c16fff291f2c353ccd35a7f3105f19b43e71c2cf557f52.jpg", "img_caption": ["Figure 14: Direct comparison of models. We visualize the top 5 most similar images of all models retrieved through cosine similarity or permutation invariant cosine similarity. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Due to the lack of instance labels in the CityScapes dataset, we can\u2019t take the quantity of objects into account as before, but can only consider if a semantic class is present or absent. As apparent from Table 3 permutation invariance allows for consistently improved retrieval when utilizing cosine similarity or RBF kernels. ", "page_idx": 24}, {"type": "text", "text": "Table 3: Retrieval results for the Cityscapes Dataset. We retrieve the most similar image according to RSMs and calculate the IoU of query semantic classes and retrieved semantic classes. Overall query images used are validation images of size $\\scriptstyle\\mathrm{N}=500$ and the database are the training images of size $\\scriptstyle\\mathrm{N}=2975$ . Differences between metrics are low, due to many images containing a large number of classes and the lack of instance label information. Despite this, permutation invariance improves Cosine Sim and RBF retrieval performance consistently, with the Inner Product showing mixed results. ", "page_idx": 24}, {"type": "table", "img_path": "ypFgcT147Z/tmp/30889769e1a035f6817ce881c6b1d03bab118b0a354ed8be84838d79bc7341d6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "E Details: Output similarity vs Representational Similarity ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "To measure the correlation between the inter-sample representational similarity and the prediction probability inter-sample similarity we utilize pre-trained classifiers and the ImageNet1k [2] dataset. Unlike during the retrieval results this constraints the possible model selections to models trained for classification. ", "page_idx": 24}, {"type": "text", "text": "Image preprocessing Test set images of ImageNet1k are randomly sampled without applying any filtering to them. In total, we utilize a subset of 2k ImageNet test set samples in this experiment. This may appear small, yet provides a sufficient basis as the combinatoric growth increases the absolute number of measurements substantially. ", "page_idx": 24}, {"type": "text", "text": "Feature and logit extraction and preparation As mentioned in the main manuscript we use ", "page_idx": 24}, {"type": "text", "text": "1. ResNet18 [7]   \n2. ResNet50 [7]   \n3. ResNet101 [7]   \n4. a DinoV2-Giant based classifier [22]   \n5. ConvNeXt V2 [33]   \n6. ViT-B/16 [4] and   \n7. ViT-L/32 [4] and ", "page_idx": 24}, {"type": "text", "text": "as pre-trained classifiers for predicting the ImageNet1k classes.5. For each sample, we extract the last hidden layer\u2019s representations and center them analog to before. For the same sample, we extract the logits and obtain the probability distribution through the softmax, saving the pair for later comparisons. ", "page_idx": 24}, {"type": "text", "text": "Correlation measurement For each pair of representations and probabilities, we calculate the similarities between their representations for all three kernel functions, once permutation invariant and once not. Additionally, we calculate the Jensen-Shannon Divergence (JSD) ", "page_idx": 24}, {"type": "text", "text": "between the predicted class probability distributions for $P$ and $Q$ . A formal definition of the JSD is provided in Eq. (13). ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{JSD}(P\\parallel Q)=\\frac{1}{2}D_{\\mathrm{KL}}(P\\parallel M)+\\frac{1}{2}D_{\\mathrm{KL}}(Q\\parallel M)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $M$ is the pointwise mean of $P$ and $Q$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\nM=\\frac{1}{2}(P+Q)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and $D_{\\mathrm{KL}}$ is the Kullback-Leibler divergence defined as: ", "page_idx": 25}, {"type": "equation", "text": "$$\nD_{\\mathrm{KL}}(P\\parallel Q)=\\sum_{i}P(i)\\log\\frac{P(i)}{Q(i)}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Given the paired JSD and Similarity $K$ between all samples $i,j$ we utilize the Pearson correlation $\\rho$ to calculate the correlation between the two. Due to the JSD being 0 for identical probabilities and increasing for more dissimilar values and the Similarity being 1 for perfectly similar representations and 0 for dissimilar representations, the desired correlation between the two should be negative. ", "page_idx": 25}, {"type": "text", "text": "E.1 Additional correlation results ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In addition to the Pearson correlation between the Jensen-Shannon-Divergence (JSD) and inter-image similarity, we also present the results of their relationship measured by the Spearman correlation, as shown in Table 4. ", "page_idx": 25}, {"type": "text", "text": "While the Pearson correlation demonstrated consistently stronger correlations with cosine similarity, the inner product, and the RBF kernel, the Spearman rank correlations are less stable across these methods. ", "page_idx": 25}, {"type": "text", "text": "For ResNets, we observe a significant decline in correlation consistency and strength with the exception of ResNet18. Opposed to this, ViTs display notably higher negative correlation values when using cosine similarity and radial basis function kernels, in contrast to the Pearson correlation results. ", "page_idx": 25}, {"type": "table", "img_path": "ypFgcT147Z/tmp/d281f180662e0f1253e25be759f08c12a5f2c640a3437a316cadcb2ad2cca39d.jpg", "table_caption": ["Table 4: Correlation between the representational similarity and the output probabilities of multiple images. In total $20\\mathbf{k}$ samples of the IN1k test set are used (as opposed to 2k in the table in the main) "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "F Details of the Approximation Algorithms ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The computational complexity of determining optimal matchings using the Jonker-Volgenant algorithm [8] scales significantly with $O(s^{3})$ , resulting in substantial computation time for input-patch sizes with spatial dimensions $s=64^{2}$ . To address this challenge, we propose alternative approximate algorithms with reduced computational complexity. In all our approximations, we take advantage of additional information, specifically the L2-norm $\\lVert\\bar{v_{i}}\\rVert_{2}$ of each semantic concept vector. We assume that achieving a high degree of matching involves pairing vectors with the highest norms and high cosine similarity. This assumption guides our design of more efficient matching algorithms. ", "page_idx": 25}, {"type": "text", "text": "1. Greedy: The simplest approach we employ is breadth-first matching. We determine the order in which to match $v_{i}$ by considering the L2-norm $\\|{v_{i}}\\|_{2}^{\\smile}$ in descending order. We then match the current $v_{i}$ with the best, non-assigned $v_{j}$ based on $\\bar{A_{i j}}^{\\bar{}}$ . The sorting complexity is ${\\mathcal{O}}(s\\log(s))$ , making this the fastest approximate algorithm among those tested. ", "page_idx": 26}, {"type": "text", "text": "2. TopK-Greedy: Recognizing that the TopK norm concept vectors $\\lVert v_{i}\\rVert_{2}$ might have a significantly higher impact on the final similarity, we attempt to find the optimal matching for only the highest TopK norm concept vectors $v_{i}$ and $v_{j}$ . The remaining lower norm concept vectors are assigned using the Greedy algorithm as described above. The process involves an initial sorting based on the semantic concept vectors\u2019 L2-norm, followed by optimal matching with $\\mathcal{O}(k^{3})$ complexity for the $k$ TopK values and the greedy matching for the remaining values. ", "page_idx": 26}, {"type": "text", "text": "3. Batch-Optimal: If the TopK norm concept vectors do not sufficiently approximate an optimal matching, we apply optimal matching for the remaining concept vectors in batches. To achieve this, we create $s//b$ smaller batches, with semantic concept vectors assigned to batches according to their L2-norm. All values within a batch are then optimally matched, leading to a matching complexity of $\\left(\\frac{s}{b}\\right)\\cdot O(b^{3})$ . ", "page_idx": 26}, {"type": "text", "text": "Evaluating the various approximations, we observe that the Greedy matching yields suboptimal approximation quality and offers marginal to no improvement over the current same-position assignment. Although we do not present the details of the greedy matching, it is important to highlight that it is guaranteed to be worse or equal to the TopK-Greedy matching with a $k$ value of 128, as shown in Fig. 5. We include the Greedy algorithm for completeness as a simple baseline. ", "page_idx": 26}, {"type": "text", "text": "Furthermore, it is noteworthy that the TopK-Greedy matching demonstrates that exclusively matching the largest norm concept vectors is insufficient for a good approximation of the optimal matching. This insight suggests that a substantial portion of the overall similarity is contributed by semantic concept vectors not included in the set of highest norms. ", "page_idx": 26}, {"type": "text", "text": "Lastly, we observe that the Batch-Optimal approximation, using a small batch size of 128 samples, provides an approximation with less than $10\\%$ error compared to the optimal matching. This result underscores the effectiveness of our batching approach based on the L2-norm of the concept vectors. It offers a reliable estimate for overall similarity, simplifying the matching process significantly. ", "page_idx": 26}, {"type": "text", "text": "F.1 Runtime Evaluation ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In order to assess algorithm performance across different spatial resolutions, we conducted a benchmarking study. For each resolution, we randomly selected 10,000 pairs of samples from a ResNet101 trained on Tiny-ImageNet. Affinity matrices $(\\mathbf{A}i j)$ were pre-computed to facilitate permutation $(\\mathbf{P}i j)$ calculations. The average time taken per matching was then reported for the same single CPU core, as outlined in Table 5. We observe that the OR-Tools implementation outperforms other alternatives, being four times faster than the lapjv implementation 7. However, even this optimal approach requires 1.52 seconds per pair on a $64\\times64$ resolution. Despite the potential for parallelizing sample-wise matching, optimal algorithms face scalability challenges with larger spatial dimensions $S$ . In contrast, the Batch-Optimal approximation offers a compelling balance between computation time and approximation quality. Importantly, its complexity scales linearly with $S$ due to the fixed batch size. ", "page_idx": 26}, {"type": "text", "text": "G Semantic RSMs and CKA \u2013 Qualitative changes ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Building upon the success of Linear/RBF (spatio-semantic) CKA to compare systems, we provide some preliminary qualitative comparisons gauging how CKA comparisons are affected by our differently proposed RSM. Unfortunately, hardly any quantitative benchmark exists to quantify if a representational similarity metric is better than another. Previously Kornblith et al. [12] evaluated CKA by showing it was better at finding layers of the same architecture than SVCCA and PWCCA. This was extended by Ding et al. [3] through the inclusion of statistical testing but remains a rather shallow benchmark. Subsequently, we constrain ourselves to qualitative experiments, leaving quantitative testing to potential future benchmarks. ", "page_idx": 26}, {"type": "table", "img_path": "ypFgcT147Z/tmp/b3dd71e68c2ab9d0d3d61bebca4e6f40b43f32f28cbef5bb8b3cc5544a7589d2.jpg", "table_caption": ["Table 5: We compare different implementations of the optimal Jonker-Volgenant algorithm [8] against our linearly scaling Batch-Optimal approximation and no matching. The table presents average run-times per pair $(T)$ and average similarity relative to the optimal value $\\bar{(\\frac{k}{k_{\\mathrm{opt}}})}$ for 1,000 randomly chosen image pairs. Utilizing representations of varying sizes from a ResNet101 trained on Tiny-ImageNet, the optimal solutions are reported relative to the maximum similarity achieved by the optimal algorithms. The Batch-Optimal approximation demonstrates a substantial fraction of optimal matching performance with significantly improved scaling. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "In all following experiments, all representations are extracted globally and zero-centered along the sample dimension. Given the zero-centered representations spatio-semantic and semantic RSMs are computed in mini-batches of 250 samples. To calculate the semantic RSMs on CIFAR an optimal bipartite matching algorithm is used, while for Tiny-ImageNet and ImageNet we utilize the Batch-Optimal approximation with window size $b\\,\\bar{5}12$ . ", "page_idx": 27}, {"type": "text", "text": "G.1 CKA between semantic and spatio-semantic RSMs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "As initial inspection, we evaluate how different the similarity structures of a model measured through spatio-semantic RSMs are to a model measured through semantic RSMs. We do this by constructing both RSMs from the same representations of a model. Subsequently, we compare the two alternative RSMs of the same representations to each other through CKA Eq. (2). The diagonal of this matrix represents a direct comparison of identical representations, just with another definition of what is considered \"similar\". This is evaluated for three ResNet18s and three ResNet34s on Tiny-ImageNet with the linear and RBF kernels respectively. ", "page_idx": 27}, {"type": "text", "text": "We display the average CKA matrices across architecture seeds for both architectures, as well as the diagonal values of the CKA matrix. Results are shown in Fig. 15. ", "page_idx": 27}, {"type": "text", "text": "Examining these CKA matrices multiple observations can be made: ", "page_idx": 27}, {"type": "text", "text": "A) Despite the diagonal representing a comparison between identical representations, the CKA values are not 1. This indicates that the different way of constructing RSMs changes the perceived similarity structure of the system, as measured by CKA. ", "page_idx": 27}, {"type": "text", "text": "B) Inspecting the diagonal shows, that earlier layers with greater spatial extent express higher differences in similarity, whereas layers at a later layer and lower spatial extent are less dissimilar. This is consistent with the expectation that, with shrinking spatial extent, alignment of semantic concepts gets more likely. ", "page_idx": 27}, {"type": "text", "text": "Given these large changes in CKA similarity, we conclude that the definition of what a model perceives as similar can highly influence inter-system similarity. This is especially relevant when comparing systems across domains, where RSM construction may be domain-specific, disallowing to be consistent with RSM construction. Exemplary when comparing ML vision systems to human vision models, in particular when comparing representations of high spatial extent. ", "page_idx": 27}, {"type": "text", "text": "Figure 15: The dissimilarity between semantic and spatio-semantic RSMs decreases with shrinking spatial extent. Comparison of semantic and spatio-semantic Representational Similarity Matrices (RSMs) for the same model. The dissimilarity in early layers decreases with decreasing spatial extent, as illustrated by Centered Kernel Alignment (CKA) values. The left and middle panel shows the CKA comparison between all layers, while the right panels visualize the heatmap\u2019s diagonal, emphasizing the evolving similarity trend from early to late layers. ", "page_idx": 28}, {"type": "text", "text": "G.2 Differences in CKA self-similarity ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In the previous paragraph, semantic RSMs were directly compared to spatio-semantic RSMs. While this can influence measurements when models are compared across domains (e.g. CNN Vision to Biological Vision), it does not need to imply that the CKA similarity of vision models changes substantially. ", "page_idx": 28}, {"type": "text", "text": "Given that within the same domain, the RSM construction will likely be chosen consistently, one can opt to either: Calculate only spatio-semantic RSMs or only semantic RSMs, due to personal opinions or preferences. Subsequently, the question is not \"Are spatio-semantic RSMs similar to semantic RSMs\", but \"Does the CKA similarity between spatio-semantic RSMs change when calculating semantic RSMs\". ", "page_idx": 28}, {"type": "text", "text": "To address this question we: A) Compare the CKA matrix difference of the CKA matrix based on semantic RSMs to the CKA matrix of spatio-semantic RSMs when comparing a model with itself (Intra Model) and B) when comparing between different models (Cross-Model). ", "page_idx": 28}, {"type": "text", "text": "Intra-Model CKA Similarly to before, we extract representations and form semantic and spatio-semantic RSMs. We extract representations on CIFAR100 [14] $(32\\times32)$ , Tiny-ImageNet $({\\bar{6}}4\\times64)$ and ImageNet-1k [2] $(160\\times\\dot{1}60)$ datasets, from 3 differently seeded and trained from scratch ResNet18, ResNet34 and ResNet101 architectures. The semantic RSMs are calculated utilizing the Batch \u2212Optimal matching with $b\\,512$ for matching on Tiny-ImageNet and ImageNet. We calculate semantic and spatio-semantic RSMs with a mini-batch size of 250, subsequently using them for Canonical Correlation Analysis (CKA) calculations. The corresponding cka matrices and their differences are displayed in Fig. 16. If not further specified the experiment uses the linear kernel. ", "page_idx": 28}, {"type": "text", "text": "Introspecting the results it can be seen that across all Architectures ResNet18, ResNet34, and ResNet101 largely the same change in similarity structure can be observed. For CIFAR100, the very first layers are perceived as less similar to semantic RSMs than with spatio-semantic RSMs, while the CKA between the middle to later layers is more similar. This structure, though does not remain consistent across datasets: When moving from CIFAR100 to TinyImageNet earlier layers appear to become more similar while intermediate layers become less so. On ImageNet1k CKA on semantic RSMs seem to indicate models are more similar. This trend indicates that the influence of semantic RSMs on spatio-semantic RSMs seems to be largely dataset-dependent. Moreover, the overall maximum change in CKA similarity in these matrices is between $-0.2$ and $+0.2$ for Tiny-ImageNet, indicating a modest change in overall CKA. ", "page_idx": 28}, {"type": "text", "text": "Cross-Model CKA Aside from evaluating only CKA similarity of RSMs of the same model we extend to comparing RSMs between models, as commonly done when comparing models through CKA. ResNet18/101 models trained on CIFAR100 are used with RSMs constructed identically to previously specified. Results are displayed in Fig. 17. ", "page_idx": 28}, {"type": "image", "img_path": "ypFgcT147Z/tmp/671c89764b693e45800724854b4e343e3e3e27f3f352159a829bb609dc10ad77.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Figure 16: CKA similarity between different Layers of the same ResNet18, ResNet34, ResNet101, on CIFAR100, Tiny-ImageNet and ImageNet-1k. For each row the left-most CKA matrix displays the spatio-semantic RSMs, the middle represents the semantic RSMs while the right represents the difference in CKA similarity between the two. Blue regions indicate where the We observe that similarity within the block structure is largely unchanged, whereas the similarity across the later blocks seems to be more similar and the similarity of the very first blocks is less similar. ", "page_idx": 29}, {"type": "image", "img_path": "ypFgcT147Z/tmp/70b032704167ccc3e04d54b77e377cbd7d7928618b87d4b59fc1e41094413658.jpg", "img_caption": ["Figure 17: CKA similarity between different Layers of different ResNet18 and ResNet101 models on CIFAR100. We observe a decrease in similarity at high-resolution layers, whereas similarity between deeper layers is largely unchanged. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "It can be seen that for both, ResNet18 and ResNet101, the cross-model CKA similarity is mostly negative for the majority of the layers, indicating that CKA on spatio-semantic RSMs estimates models to be more similar than when applying CKA on semantic RSMs. Similarly to before CKA changes range from $-0.175$ to $+0.{\\bar{0}}2{\\bar{5}}$ providing modest changes. ", "page_idx": 29}, {"type": "text", "text": "Concluding the Intra and Cross-Model CKA experiments it can be seen that the choice of RSMs results in qualitatively different CKA matrices. Unfortunately, due to the lack of quantitative benchmarks, no direct recommendation of which RSM to use for inter-model similarity calculation through CKA can be given. ", "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer:[Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: The experiments in Section 4 demonstrate the advantage of introducing permutation invariance for RSMs and directly correspond to the contributions listed in Section 1. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: In Section 5, a comprehensive discussion of the results and their limitations, such as the runtime, is provided. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speechto-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: While the paper provides a comprehensive mathematical formulation, no new theorems were introduced and thus no theoretical proofs were needed. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: All experiments were performed using public datasets and models. The information provided in Appendix F, Appendix D.1 and Algorithm 1 provide all necessary information to reproduce the results. Additionally, we will publish the code. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Code will be provided by acceptance. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips. cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so $\\mathbf{\\Sigma}^{\\prime\\prime}\\mathbf{No}^{\\prime\\prime}$ is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The information provided in Section 4 provide all necessary information to understand the results. Appendix F, Appendix D.1 and Algorithm 1 add additional details that are needed to reproduce the exact experimental setting but not necessarily to understand the results. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [No] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper introduces a new concept that has no direct related methods to compare with. Therefore, no statistical analysis or ranking methods are needed. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The presented work does not rely on specific compute requirements. However, in Appendix F.1 we compare the runtime of multiple approximation algorithms to solve the assignment proble. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. \u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. \u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The authors have strictly adhered to the ethical guidelines. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: We do not see any direct societal impact. The work only provides a new introspection into the concept of semantic similarity of networks. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: See broader impact ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. \u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. ", "page_idx": 34}, {"type": "text", "text": "\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: All relevant and previous work is cited and only open-source assets have been used. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/ datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The codebase will be published under a CC BY license. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: No crowdsourcing experiments or research with human subjects were performed. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 35}, {"type": "text", "text": "\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: No crowdsourcing experiments or research with human subjects were performed. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}]