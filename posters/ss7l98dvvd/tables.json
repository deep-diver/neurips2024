[{"figure_path": "Ss7l98DVvD/tables/tables_6_1.jpg", "caption": "Table 1: Quantitative experimental results of existing methods (NeRF-W (Martin-Brualla et al., 2021), Ha-NeRF (Chen et al., 2022b), and CR-NeRF (Yang et al., 2023)) and Wild-GS on Phototourism dataset. Wild-GS\u2020 indicates that the model is trained by 15k iterations. The efficiencies of different methods are quantified by their training times (hours) and inference speeds (frame per second). Crop, Global, Mask, and Depth are abbreviations for triplane cropping, global appearance encoding, transient mask prediction, and depth regularization, respectively. {c'} refers to {Cxy, Cyz, Czx}.", "description": "This table presents a quantitative comparison of Wild-GS against existing novel view synthesis methods (NeRF-W, Ha-NeRF, and CR-NeRF) on the Phototourism dataset.  It shows metrics like PSNR, SSIM, and LPIPS, along with training time and inference speed for each method.  Ablation studies are also included to demonstrate the impact of different components within the Wild-GS model.", "section": "5 Experimental Results"}, {"figure_path": "Ss7l98DVvD/tables/tables_14_1.jpg", "caption": "Table 2: Quantitative experimental results on three extra datasets. 3DGS-AE replaces the appearance encoding of Wild-GS (ours) with a learnable embedding as NeRF-W and optimizes it in an autoencoder way. Local and fin refer to the triplane local appearance embedding and the intrinsic feature.", "description": "This table presents a quantitative comparison of different methods on three additional datasets (Palace of Westminster, Pantheon Exterior, and Buckingham Palace). It shows the PSNR, SSIM, and LPIPS scores for each method.  The table also includes ablation studies, removing the local appearance embedding and intrinsic feature to show their impact on the results.  The results demonstrate the superior performance of Wild-GS compared to other approaches.", "section": "5.1 Comparison Experiments"}]