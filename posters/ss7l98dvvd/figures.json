[{"figure_path": "Ss7l98DVvD/figures/figures_1_1.jpg", "caption": "Figure 1: Visual comparison between Wild-GS and other existing approaches (Chen et al., 2022b; Yang et al., 2023). Wild-GS presents superior computational efficiency (tested on single RTX3090), as well as better appearance and geometry reconstruction. Additionally, by modifying the appearance features defined by Wild-GS, one can freely adjust the visual appearance of the entire scene.", "description": "This figure compares the novel view synthesis results of Wild-GS against other state-of-the-art methods (Ha-NeRF, CR-NeRF) on the Brandenburg Gate scene.  It highlights Wild-GS's superior computational efficiency (training time and inference speed) while achieving higher PSNR and SSIM scores, indicating better visual quality. The bottom row demonstrates the capability of Wild-GS to modify the appearance of the entire scene by simply adjusting appearance features.", "section": "1 Introduction"}, {"figure_path": "Ss7l98DVvD/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of the architecture of our proposed Wild-GS. The reference view is first processed by a 2D Parsing module to extract the visibility mask and global appearance embedding. Given the mask and rendered depth from 3DGS, we back-project the 2D reference image without transient objects to the space and construct the static 3D point cloud. Then, these 3D points are re-projected to three predefined orthogonal planes using their normalized coordinates for generation of triplane features. Each 3D Gaussian queries its local appearance embedding by providing the spatial coordinate to the 3D Wrapping module. With the global and local embeddings and the stored intrinsic feature, we can predict the SH coefficients sh of every 3D Gaussian for RGB rasterization.", "description": "This figure illustrates the architecture of Wild-GS, which consists of three main components: 2D Parsing Module, 3D Wrapping Module, and a fusion network. The 2D Parsing Module extracts the visibility mask and global appearance embedding from the reference view. The 3D Wrapping Module uses the depth information from 3DGS to back-project the reference image and constructs the 3D point cloud. This point cloud is then used to generate triplane features, which are used by each 3D Gaussian to query its local appearance embedding. Finally, the fusion network combines the global and local embeddings with the intrinsic feature to predict the SH coefficients for RGB rasterization.", "section": "4 Wild-GS"}, {"figure_path": "Ss7l98DVvD/figures/figures_5_1.jpg", "caption": "Figure 3: (a) The point cloud from the reference image is projected along three axes and their reverses to generate the triplane color; (b) Illustration of the distribution of the 3D Gaussians on the original triplane and cropped one. Axis-aligned bounding box (AABB) is utilized to accomplish 3D cropping.", "description": "This figure shows the process of generating triplane features from the reference image's point cloud in Wild-GS.  (a) illustrates the back-projection of the point cloud onto three orthogonal planes and their reverses to create six triplane color maps (Cxy, Cyz, Czx, Cry, C\u2019yz, C\u2019zx). (b) demonstrates an efficiency improvement by cropping the original triplane to a smaller Axis-Aligned Bounding Box (AABB) which contains most of the 3D Gaussian points, thereby reducing computation.", "section": "4 Wild-GS"}, {"figure_path": "Ss7l98DVvD/figures/figures_7_1.jpg", "caption": "Figure 4: Visual comparison of rendering quality between different approaches. Red and blue crops mainly emphasize appearance and geometry differences, respectively.", "description": "This figure shows a visual comparison of the rendering quality produced by different novel view synthesis methods on three example scenes from the Phototourism dataset.  The methods compared are NeRF-W, Ha-NeRF, CR-NeRF, and the proposed Wild-GS (trained for 15k and 30k iterations).  Each row represents a different scene. For each scene, the ground truth image is shown alongside the results from each method.  Red boxes highlight areas where appearance differences are particularly noticeable, while blue boxes highlight areas with geometric discrepancies.", "section": "5.1 Comparison Experiments"}, {"figure_path": "Ss7l98DVvD/figures/figures_7_2.jpg", "caption": "Figure 5: Rendering results of ablation study on Wild-GS when removing depth regularization, transient mask (left), and global appearance encoding (right). Red rectangles indicate the areas where geometry is missing or color inconsistency happens. Notations follow Table 1", "description": "This figure shows the results of an ablation study on the Wild-GS model.  It compares the full Wild-GS model to versions where depth regularization, the transient mask, and global appearance encoding have been removed, individually.  The red boxes highlight areas where the removal of these components leads to missing geometry or color inconsistencies. The figure demonstrates the importance of each component for the quality of novel view synthesis.", "section": "5.2 Ablation Study"}, {"figure_path": "Ss7l98DVvD/figures/figures_8_1.jpg", "caption": "Figure 6: Appearance and style transfer to novel views using reference images inside and outside the training dataset. Two arbitrary style images are borrowed from Ha-NeRF (Chen et al., 2022b).", "description": "This figure demonstrates the appearance and style transfer capabilities of Wild-GS and CR-NeRF.  It shows novel view synthesis results where the appearance of the scene has been altered using different reference images, both from within and outside the training datasets used to train the models. The results highlight the ability of Wild-GS to more accurately capture and transfer the appearance of the reference image compared to CR-NeRF, showcasing the superior performance of Wild-GS in appearance and style transfer tasks.", "section": "5.3 Appearance Transfer"}, {"figure_path": "Ss7l98DVvD/figures/figures_14_1.jpg", "caption": "Figure 7: Visual comparison of Wild-GS (ours) and GS-W (concurrent work) and ablation study.", "description": "This figure compares the visual results of Wild-GS with a concurrent work, GS-W, and also shows the effects of removing the local appearance embedding and intrinsic feature from the Wild-GS model.  The top row shows three images: Ground Truth, Wild-GS, and GS-W. The bottom row shows Wild-GS, Wild-GS without local appearance, and Wild-GS without intrinsic features. The yellow boxes highlight details, and it is evident that Wild-GS shows improved details and visual quality compared to GS-W.  The ablation study demonstrates that both the local appearance and intrinsic features are essential for high-quality results.", "section": "5.1 Comparison Experiments"}, {"figure_path": "Ss7l98DVvD/figures/figures_15_1.jpg", "caption": "Figure 8: Visualization of the transient masks (learned in an unsupervised way) predicted by Wild-GS.", "description": "This figure visualizes the transient masks generated by the Wild-GS model.  The masks identify areas of the image containing transient objects, which are those that change between different views of the scene, like moving people or vehicles. The unsupervised learning process means the model learns to identify these areas without explicit training data specifying where transient objects are. The masks allow the model to focus on static elements when rendering novel views, which improves the quality and consistency of the synthetic images.", "section": "More Implementation Details"}, {"figure_path": "Ss7l98DVvD/figures/figures_15_2.jpg", "caption": "Figure 1: Visual comparison between Wild-GS and other existing approaches (Chen et al., 2022b; Yang et al., 2023). Wild-GS presents superior computational efficiency (tested on single RTX3090), as well as better appearance and geometry reconstruction. Additionally, by modifying the appearance features defined by Wild-GS, one can freely adjust the visual appearance of the entire scene.", "description": "This figure compares the novel view synthesis results of Wild-GS against other state-of-the-art methods (Ha-NeRF, CR-NeRF) on the Brandenburg Gate scene. Wild-GS demonstrates superior performance in terms of both visual quality (PSNR and SSIM) and computational efficiency (training time and inference speed).  The figure highlights Wild-GS's ability to reconstruct fine details and handle complex scenes with dynamic appearance changes more effectively than previous methods.", "section": "1 Introduction"}]