[{"figure_path": "hkBhX5ABjk/tables/tables_7_1.jpg", "caption": "Table 1: Normalized average returns on D4RL benchmark. The results are evaluated in the target domain and we bold the highest mean.", "description": "This table presents the normalized average returns achieved by different reinforcement learning algorithms on the D4RL benchmark.  The results are obtained in the target domain, indicating the performance of each algorithm when transferring a policy trained in a source domain to the target domain.  The highest average return for each task is bolded, enabling a quick comparison of algorithm performance across multiple tasks and dataset variations.", "section": "5.2 Performance Comparison on the Benchmarks"}, {"figure_path": "hkBhX5ABjk/tables/tables_7_2.jpg", "caption": "Table 2: Normalized average returns on NeoRL benchmark. The results are evaluated in the target domain and we bold the highest mean.", "description": "This table presents the results of the proposed Madoc algorithm and several baseline algorithms on the NeoRL benchmark.  The \"Normalized average returns\" represent a performance metric, adjusted to allow comparison across different tasks. The results are evaluated in the target domain (the real-world environment the algorithm is ultimately intended for).  The highest average return for each task is bolded to easily highlight the best performing algorithm for each task.", "section": "5.2 Performance Comparison on the Benchmarks"}, {"figure_path": "hkBhX5ABjk/tables/tables_17_1.jpg", "caption": "Table 3: The comparisons of method complexities.", "description": "This table compares the GPU memory cost and modules of three different methods: Madoc, CQL, and MOREC.  Madoc has the highest memory cost because it includes reward models, a Variational Autoencoder (VAE) for parameter grouping, multiple calibration agents, and running agents. CQL uses only running agents, resulting in the lowest memory cost.  MOREC falls between the two, incorporating a dynamics reward function, dynamics models, and running agents.", "section": "5.2 Performance Comparison on the Benchmarks"}, {"figure_path": "hkBhX5ABjk/tables/tables_17_2.jpg", "caption": "Table 1: Normalized average returns on D4RL benchmark. The results are evaluated in the target domain and we bold the highest mean.", "description": "This table presents the normalized average returns achieved by different reinforcement learning algorithms on the D4RL benchmark.  The results are specifically for the target domain, showing how well policies trained in a source domain generalize. The highest mean return for each task is bolded to highlight the top-performing algorithms.  The D4RL benchmark includes variations in dataset quality, providing a comprehensive evaluation.", "section": "5.2 Performance Comparison on the Benchmarks"}, {"figure_path": "hkBhX5ABjk/tables/tables_17_3.jpg", "caption": "Table 1: Normalized average returns on D4RL benchmark. The results are evaluated in the target domain and we bold the highest mean.", "description": "This table presents the results of the normalized average returns achieved by different algorithms on the D4RL benchmark.  The experiments were conducted on the target domain, and the highest mean for each task is highlighted in bold. The table allows for a direct comparison of the performance of Madoc against various baseline methods across multiple locomotion tasks and dataset variations.", "section": "5.2 Performance Comparison on the Benchmarks"}, {"figure_path": "hkBhX5ABjk/tables/tables_19_1.jpg", "caption": "Table 1: Normalized average returns on D4RL benchmark. The results are evaluated in the target domain and we bold the highest mean.", "description": "This table presents the normalized average returns achieved by different reinforcement learning algorithms on the D4RL benchmark.  The results are broken down by task (HalfCheetah, Hopper, Walker, Ant) and dataset type (medium, medium-replay, medium-expert). Higher values indicate better performance.  The table highlights the superior performance of Madoc compared to other methods.", "section": "5.2 Performance Comparison on the Benchmarks"}, {"figure_path": "hkBhX5ABjk/tables/tables_19_2.jpg", "caption": "Table 7: The setting of Hopper physics parameter at easy/normal/hard level.", "description": "This table shows the initial range and ground truth values for various physics parameters of the Hopper robot in a simulation environment.  The parameters are categorized into three levels of difficulty: easy, normal, and hard.  The initial ranges represent the bounds within which the physics parameter values can vary during the simulation, while the ground truth values are the actual values used in the real-world environment. This table is crucial for understanding how the difficulty levels influence the calibration process in the Madoc framework.", "section": "5.1 Experiment Setup"}, {"figure_path": "hkBhX5ABjk/tables/tables_19_3.jpg", "caption": "Table 1: Normalized average returns on D4RL benchmark. The results are evaluated in the target domain and we bold the highest mean.", "description": "This table presents the normalized average returns achieved by different reinforcement learning algorithms on the D4RL benchmark.  The results are specifically for the target domain, and the highest average return for each task is highlighted in bold.  The table compares Madoc's performance against several other state-of-the-art methods across various locomotion tasks and dataset variations (medium, medium-replay, and medium-expert).", "section": "5.2 Performance Comparison on the Benchmarks"}, {"figure_path": "hkBhX5ABjk/tables/tables_20_1.jpg", "caption": "Table 9: The setting of Ant physics parameter at normal level.", "description": "This table shows the initial range and ground truth values for various physics parameters of the Ant robot in the simulation environment.  The parameters include gravity, and the mass of different body parts.  These values are used to calibrate the source domain model to match the target domain.", "section": "D Experiment Details"}, {"figure_path": "hkBhX5ABjk/tables/tables_21_1.jpg", "caption": "Table 10: The mean absolute calibration error on D4RL benchmark. We bold the lowest mean.", "description": "This table shows the mean absolute calibration error for different algorithms on the D4RL benchmark.  The error is a measure of how far the calibrated source domain parameters are from the true target domain parameters. Lower values indicate better calibration and thus better transfer performance to the target domain.  The results are broken down by task and dataset (medium, medium-replay, medium-expert) within each task.  The lowest mean error for each task and dataset is bolded.", "section": "5.2 Performance Comparison on the Benchmarks"}, {"figure_path": "hkBhX5ABjk/tables/tables_21_2.jpg", "caption": "Table 1: Normalized average returns on D4RL benchmark. The results are evaluated in the target domain and we bold the highest mean.", "description": "This table presents the normalized average returns achieved by different algorithms on the D4RL benchmark.  The results are specifically measured in the target domain to evaluate the performance of each method after domain transfer. The highest mean for each task is highlighted in bold.", "section": "5.2 Performance Comparison on the Benchmarks"}, {"figure_path": "hkBhX5ABjk/tables/tables_22_1.jpg", "caption": "Table 1: Normalized average returns on D4RL benchmark. The results are evaluated in the target domain and we bold the highest mean.", "description": "This table presents the normalized average returns achieved by various algorithms on the D4RL benchmark.  The results represent the performance of each algorithm in the target domain (real-world environment).  The 'highest mean' return for each task is bolded, highlighting the best-performing algorithm for that particular task.", "section": "5.2 Performance Comparison on the Benchmarks"}]