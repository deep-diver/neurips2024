[{"type": "text", "text": "Multi-Agent Domain Calibration with a Handful of Offline Data ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Tao Jiang1,2,3 ,\u2217 Lei Yuan1,2,3,\u2217 Lihe Li1,2, Cong Guan1,2, Zongzhang Zhang1,2,\u2020 Yang Yu1,2,3 ", "page_idx": 0}, {"type": "text", "text": "1National Key Laboratory of Novel Software Technology, Nanjing University, Nanjing, China 2School of Artificial Intelligence, Nanjing University, Nanjing, China 3Polixir Technologies, Nanjing, China {jiangt,yuanl,lilh,guanc}@lamda.nju.edu.cn, {zzzhang, yuy}@nju.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The shift in dynamics results in significant performance degradation of policies trained in the source domain when deployed in a different target domain, posing a challenge for the practical application of reinforcement learning (RL) in real-world scenarios. Domain transfer methods aim to bridge this dynamics gap through techniques such as domain adaptation or domain calibration. While domain adaptation involves refining the policy through extensive interactions in the target domain, it may not be feasible for sensitive fields like healthcare and autonomous driving. On the other hand, offline domain calibration utilizes only static data from the target domain to adjust the physics parameters of the source domain (e.g., a simulator) to align with the target dynamics, enabling the direct deployment of the trained policy without sacrificing performance, which emerges as the most promising for policy deployment. However, existing techniques primarily rely on evolution algorithms for calibration, resulting in low sample efficiency. To tackle this issue, we propose a novel framework Madoc (Multi-agent domain calibration). Firstly, we formulate a bandit RL objective to match the target trajectory distribution by learning a couple of classifiers. We then address the challenge of a large domain parameter space by modeling domain calibration as a cooperative multi-agent reinforcement learning (MARL) problem. Specifically, we utilize a Variational Autoencoder (VAE) to automatically cluster physics parameters with similar effects on the dynamics, grouping them into distinct agents. These grouped agents train calibration policies coordinately to adjust multiple parameters using MARL. Our empirical evaluation on 21 offilne locomotion tasks in D4RL and NeoRL benchmarks showcases the superior performance of our method compared to strong existing offilne model-based RL, offline domain calibration, and hybrid offline-and-online RL baselines. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reinforcement learning (RL) has gained significant traction in various fields [1], such as sequential recommendation systems [2] and robotic control [3], demonstrating tremendous potential in realworld applications. However, the inherent trial-and-error nature of RL limits its application, especially in safety-critical areas such as healthcare [4] and autonomous driving [5], as extensive interactions with the target environment can entail prohibitive costs and pose substantial safety risks. To address this problem, a range of studies have proposed collecting training samples from a surrogate source domain (e.g., simulation environment) to learn policies, which are then deployed to the downstream target domain [6, 7]. Nonetheless, due to complex system dynamics and the characteristics of open environments, a high-fidelity simulator may not always be available [8], leading to severe dynamics shifts between the source and target domains [9]. Consequently, policies trained optimally in the source domain may fail catastrophically in the target domain. To bridge the dynamics gap, various kinds of solutions have been developed recently. Domain randomization [10] methods, for instance, randomly sample the physics parameters of the source domain and train policies across multiple simulated environments to approximate the target domain. However, as the target domain is often unknown and set in an open environment [8], these methods can also suffer from unpredictable policy degradation, which hinders further development. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Integrating the source domain with some data from the target domain offers a promising solution to the mentioned problem [11]. A class of methods, known as domain calibration, attempts to use data from the target domain as feedback to calibrate the easily obtained source domain and then transfer the policy directly to the target domain. This approach shows enormous potential when the parameters are adjusted accurately enough. Some typical methods automatically tune the physics parameters by minimizing the transition discrepancy between the source and target domains [12, 13] or by maximizing the expected return in the target domain [14]. While these methods can successfully transfer learned policies in robotics [15], they still require interaction feedback from the target domain during training. Instead, DROID [16] and DROPO [17] introduce an offline setting for domain calibration, where the physics parameters are adjusted using offline demonstrations pre-collected in the target domain, showing potential for real-world applications. ", "page_idx": 1}, {"type": "text", "text": "Nevertheless, in complex real-world scenarios, numerous physics parameters may require calibration. The above-mentioned methods primarily employ evolutionary algorithms [16, 18] or samplingbased methods [19, 20] for black-box optimization, often results in low sample efficiency [21, 22]. Recently, some algorithms have attempted to mitigate this issue by learning sampling strategies [22] or leveraging causal discovery [23] to eliminate parameters that have little impact on the environment. Despite the effectiveness of these methods, a significant challenge remains in handling complex scenarios where all physics parameters critically influence the dynamics, and different parameters may have varying, or even opposite, impacts on these dynamics [24]. A method for efficiently addressing the interrelations among different parameters is urgently needed. ", "page_idx": 1}, {"type": "text", "text": "From the perspective of whole-domain calibration, each physics parameter contributes to different aspects of the calibration process. This can be modeled as a typical multi-agent system (MAS) problem [25], where each agent adjusts a group of domain parameters, and all agents cooperate to reduce the domain gap. This problem can be addressed using cooperative multi-agent reinforcement learning (MARL) [26], leading to the development of the Madoc (Multi-agent domain calibration) framework. Specifically, we first formulate domain calibration as a target trajectory distribution matching problem and derive a bandit RL optimization objective by introducing a couple of classifiers to act as the reward model. We then formulate the problem into the MAS where multiple agents calibrate different parameters to reduce the dynamics gap between the source and target domains. Concretely, we propose an automatic grouping technique to cluster physics parameters based on their impacts on the dynamics. We then employ popular value decomposition methods in MARL to train cooperative calibration policies to adjust domain parameters. We conduct experiments on popular locomotion tasks to showcase Madoc\u2019s superior performance against baselines and highlight the contributions of its core design components. The source code is available at https: //github.com/LAMDA-RL/Madoc. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Domain transfer in RL. Transferring RL policies learned from imperfect source domains to the target domain is a crucial step in the practical use of RL algorithms [11]. However, the trained policy often suffers from severe performance deterioration when directly deployed into the target domain due to the distribution shift between different domains with varying transition dynamics [27]. Previous works have addressed this problem with three common strategies: domain randomization (DR), domain adaptation (DA), and system identification (SI). DR attempts to train a generalizable policy that works well across a variety of randomized simulated dynamics [28, 9]. While the motivation is simple and often effective, these methods require manually determining which parameters to randomize and may result in underfitting or failing policies due to hand-tuning parameter ranges. DA involves using a huge amount of data from source domains to improve policy performance on a different target domain [29, 30, 31]. However, these efforts are constrained by the quality and quantity of target domain data and often still require interaction with the target domain. Another line of work, SI, uses measured data to build mathematical models of dynamical systems [32]. These methods rely on numerous interactions with the target domain to study how to learn a model of the system dynamics [33, 34], which results in learning a biased policy with fewer interactions. Most recent works calibrate the parameters of the biased source domain to bridge the domain gap [19, 13, 15], also known as domain calibration, and try to improve efficiency by learning a parameter sampling strategy [22] or leveraging causal discovery [23]. However, these methods still require interacting with the target domain, posing potential safety hazards during the training process. To mitigate this problem, DROID [16] and DROPO [17] use offilne datasets to adjust the source domain parameters via evolutionary algorithms [18] with different optimization objectives, perform poorly when the number of domain parameters is large [35]. Unlike the above methods, we propose to adjust the source domain with a handful of offline data, enabling the domain parameters to match the target trajectory distribution with high sample efficiency. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Cooperative multi-agent RL. Many real-world problems are inherently large-scale and complex, making it inefficient and impractical to model them as single-agent systems. Instead, they are more suitably addressed as multi-agent systems (MASs) [25]. Multi-agent reinforcement learning (MARL) provides frameworks for modeling and solving such challenges [26]. In scenarios where agents within MAS share common objectives, these problems are categorized under cooperative MARL, which has demonstrated significant advancements in domains like power management [36], path planning [37], and dynamic algorithm configuration [38]. One of the primary challenges in cooperative MARL is the scalability issue [39, 40, 41], exacerbated by the exponential growth of the search space with the number of agents, complicating policy exploration and learning. Various approaches have been proposed to enhance agent coordination recently. These include policy-based methods such as MADDPG [42] and MAPPO [43], value-based techniques like VDN [44] and QMIX [45], and innovations like the transformer architecture [46]. Among these methods, value-based approaches have demonstrated promising results in diverse and complex settings [47, 48]. VDN leverages additivity to factorize global value functions, QMIX further enforces monotonicity in global value functions, and DOP [49] introduces value function decomposition within multi-agent actor-critic frameworks. These methods exhibit remarkable coordination capabilities across various tasks such as SMAC, Hanabi, and GRF [26]. In this paper, our method formulates domain calibration as a cooperative MARL problem, improving efficiency and fidelity. ", "page_idx": 2}, {"type": "text", "text": "3 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Reinforcement Learning can be generally modeled as a Markov decision process (MDP) [50], formulated as a tuple $\\mathcal{M}:=(\\mathcal{S},\\mathcal{A},T,r,\\gamma,\\rho_{0})$ , where $\\boldsymbol{S}$ and $\\boldsymbol{\\mathcal{A}}$ denote the state and action spaces, $T(s^{\\prime}|s,a)\\in[0,1]$ and $r(s,a)$ represent the transition and reward functions, $\\gamma\\in[0,1)$ implies the discount factor, and $\\rho_{0}(s)$ is the initial state distribution. The agent running in the environment perceives the state $s_{t}\\in\\mathcal S$ at time step $t$ , performs an action $a_{t}\\in\\mathcal A$ based on a learnable policy $\\pi(a|s\\rangle\\in[0,1]$ , then the environment receives the action, transits to a new state $s_{t+1}$ , and rewards the agent according to the transition function $T(s_{t+1}|s_{t},a_{t})$ and reward function $r(s_{t},a_{t})$ at next time step. The above process is continuously iterated until termination, we can record the whole trajectory of length $H+1$ as $\\tau=(s_{0},a_{0},r_{0},s_{1},a_{1},r_{1},\\cdot\\cdot\\cdot\\:,s_{H},a_{H},r_{H})$ and the trajectory distribution over agent\u2019s policy and the environment can be defined as $\\begin{array}{r}{d_{\\pi,\\mathcal{M}}(\\tau)=\\rho_{0}(s_{0})\\prod_{t=0}^{H}T(s_{t+1}\\vert s_{t},a_{t})\\pi(a_{t}\\vert s_{t})}\\end{array}$ . The objective of RL algorithms is to learn a policy $\\pi(a|s)$ which max imizes the expected discounted return across the distribution of trajectories, i.e., $\\mathcal{J}(\\mathcal{M},\\pi)=\\mathbb{E}_{\\tau\\sim d_{\\pi,\\mathcal{M}}(\\cdot)}R(\\tau)$ with $\\begin{array}{r}{\\boldsymbol{R}(\\tau)=\\sum_{t=0}^{H}\\gamma^{t}\\boldsymbol{r}(s_{t},a_{t})}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "Domain Calibration aims to adjust a manipulable source domain to close the domain gap between it and the target domain. Both the target and source domains can be modeled as MDPs, and the only difference between them is the transition functions which are determined by the physics dynamics parameter vector $\\boldsymbol{\\xi}\\in\\Xi\\subset\\mathbb{R}^{N}$ (e.g., friction, mass, damping). Here, $\\Xi$ denotes the physics parameter space, and $N$ represents the dimension of the physics parameters. Each parameter $\\xi^{i}$ is bounded on a closed interval, which can only be inferred roughly based on experience and expert knowledge, i.e., $\\xi^{i}\\in[\\xi_{\\mathrm{low}}^{i},\\xi_{\\mathrm{high}}^{i}]$ . We assume the unknown physics parameters of the target domain $\\xi^{*}$ are included in the parameter space $\\Xi$ if the physical modeling is reasonable, i.e., $\\xi^{*}\\in\\Xi$ , as we can set sufficient wide parameter ranges. We now denote the transition function conditioned on domain parameters as $T_{\\xi}\\bar{\\bf\\Xi}=\\,T(s^{\\prime}|s,a,\\bar{\\xi})$ and the corresponding MDP as $M_{\\xi}\\,:=\\,(S,{\\mathcal A},T_{\\xi},r,\\gamma,\\rho_{0})$ . However, the manipulable source domain is typically non-differentiable, we can only calibrate the distribution of the source domain parameters $\\xi\\sim q_{\\phi}(\\cdot)$ , and the optimal policy learned under this distribution is marked as $\\pi^{*}(q_{\\phi})=\\arg\\operatorname*{max}_{\\pi}\\mathbb{E}_{\\xi\\sim q_{\\phi}(\\cdot)}\\mathcal{J}(M_{\\xi},\\pi)$ . The objective of domain calibration is to learn the source domain parameters that maximize the expected discounted return under the target domain: $\\operatorname*{max}_{q_{\\phi}}\\mathcal{I}(M_{\\xi^{*}},\\pi^{*}\\bar{(}q_{\\phi}))$ . ", "page_idx": 2}, {"type": "image", "img_path": "hkBhX5ABjk/tmp/113e749beb1164aeb69562c146198276c10bfa44f60736f86c9ce461d2cbb754.jpg", "img_caption": ["Figure 1: The conceptual workflow of the multi-agent domain calibration framework. The orange arrow represents the simulated data flow in the source domain, with the transition function $T(\\bar{s}^{\\prime}|s,a,\\xi)$ , while the blue represents the offline data in the target domain, with the transition function $T(s^{\\prime}|s,a,\\xi^{*})$ . The subscripts \u201csrc\u201d and \u201ctar\u201d are used to distinguish between the source and target domains, respectively. After learning the grouping scheme, we use the red arrow to represent the process of domain calibration by MARL value decomposition methods. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "4 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we propose the Madoc (Multi-agent domain calibration) framework for leveraging a modest amount of offline data from the target domain to calibrate the biased source domain, thus facilitating optimal policy transfer. The overall workflow of the Madoc framework is shown in Fig. 1. We first deduce a bandit RL objective to adjust the domain parameters in Sec. 4.1, improving the synthetic data sampled from the source domain to align with the target trajectory distribution. We further model it as a cooperative multi-agent reinforcement learning problem in Sec. 4.2 and use an automatic grouping technique to improve the efficiency and fidelity of domain calibration. Finally, a practical algorithm under the Madoc framework is presented in Sec. 4.3. ", "page_idx": 3}, {"type": "text", "text": "4.1 Domain Calibration via Reinforcement Learning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Domain calibration is the process of tuning the parameter distribution of a mismatched source domain to better align with the target domain, which can be realized by comparing the divergence between target domain interactions and simulated synthetic rollouts based on the same policy [12, 13]. However, since interacting with the target domain may not be feasible for sensitive fields, we propose an alternative approach to minimize the trajectory discrepancy between the two domains by employing a handful of offline target domain data. ", "page_idx": 3}, {"type": "text", "text": "Formally speaking, the static offline dataset $\\boldsymbol{\\mathcal{D}}\\,=\\,\\{\\tau_{1},\\tau_{2},\\cdot\\cdot\\cdot\\,,\\tau_{k}\\}$ contains $k$ trajectories where $\\tau_{i}=(s_{0}^{i},a_{0}^{\\bar{i}},r_{0}^{i},s_{1}^{\\bar{i}},a_{1}^{i},r_{1}^{i},\\cdot\\cdot\\cdot\\,,s_{H}^{i},a_{H}^{i},r_{H}^{i})$ , which are collected previously by an unknown behavior policy $\\mu$ from the target domain, i.e., $\\tau_{i}^{-\\bar{\\mathbf{\\alpha}}}\\stackrel{\\cdots}{\\sim}d_{\\mu,\\mathcal{M}_{\\xi^{*}}}(\\cdot)$ . By introducing a prior normal parameter distribution $p(\\xi)$ to foster better generalization [51], we intend to learn a sample policy $\\pi$ and calibrate the domain parameters to match the target trajectory distribution: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pi,q_{\\phi}}D_{\\mathrm{KL}}\\left(q_{\\phi}(\\xi)d_{\\pi,\\mathcal{M}_{\\xi}}(\\tau)||p(\\xi)d_{\\mu,\\mathcal{M}_{\\xi^{*}}}(\\tau)\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the Kullback-Leibler (KL) divergence can be further derived as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\textit{\\xi}\\sim d_{\\psi}(\\cdot)}\\left[\\log\\frac{\\prod_{t=0}^{H}\\pi\\left(a_{t}\\vert s_{t}\\right)T\\left(s_{t+1}\\vert s_{t},a_{t},\\xi\\right)}{\\prod_{t=0}^{H}\\mu\\left(a_{t}\\vert s_{t}\\right)T\\left(s_{t+1}\\vert s_{t},a_{t},\\xi^{*}\\right)}+\\log\\frac{q_{\\phi}(\\xi)}{p(\\xi)}\\right],}\\\\ &{=\\!\\!\\mathbb{E}_{\\textit{\\xi}\\sim d_{\\phi}(\\cdot)}\\left[\\displaystyle\\sum_{t=0}^{H}\\left(\\log\\frac{\\pi\\left(a_{t}\\vert s_{t}\\right)}{\\mu\\left(a_{t}\\vert s_{t}\\right)}+\\log\\frac{T\\left(s_{t+1}\\vert s_{t},a_{t},\\xi\\right)}{T\\left(s_{t+1}\\vert s_{t},a_{t},\\xi^{*}\\right)}\\right)\\right]+\\mathbb{E}_{\\xi\\sim q_{\\phi}(\\cdot)}\\left[\\log\\frac{q_{\\phi}(\\xi)}{p(\\xi)}\\right],}\\\\ &{\\approx\\!\\!\\mathbb{E}_{(s,a)\\sim B}\\left[\\log\\frac{\\pi\\left(a\\vert s\\right)}{\\mu\\left(a\\vert s\\right)}\\right]-\\mathbb{E}_{(s,a,s^{\\prime},\\xi)\\sim B}\\left[\\log\\frac{T\\left(s^{\\prime}\\vert s,a,\\xi^{*}\\right)}{T\\left(s^{\\prime}\\vert s,a,\\xi\\right)}\\right]+D_{\\mathrm{KL}}(q_{\\phi}(\\xi)\\vert\\vert p(\\xi)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where Eq. 4 is an approximation of Eq. 3 as the parameter distribution and trajectory distribution used to calculate the expectation are difficult to compute. Consequently, we use Monte Carlo sampling on the source domain to approximate the expected results. To enhance sampling efficiency, we sample a domain parameter $\\xi\\sim q_{\\phi}(\\cdot)$ , generate the trajectories $\\tau\\sim d_{\\pi,\\mathcal{M}_{\\xi}}(\\cdot)$ , and store the rollouts $(s,a,s^{\\prime},\\xi)$ in the replay buffer $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ . By doing so, we are able to convert the trajectory-based objective into a transition-based one, following the classic off-policy RL paradigm. ", "page_idx": 4}, {"type": "text", "text": "It is delighted to discover that the objective in Eq. 4 can be clearly divided into three terms: the $\\begin{array}{r l}&{\\operatorname*{min}_{\\pi}\\mathbb{E}_{(s,a)\\sim{\\cal B}}\\left[\\log\\frac{\\pi(\\bar{a}|s)}{\\mu(a|s)}\\right]\\approx\\operatorname*{min}_{\\pi}\\bar{D}_{\\mathrm{KL}}(\\pi(a|s)||\\mu(\\bar{a}|s))}\\end{array}$ , attempts to minimize the KL divergence between $\\pi(a|s)$ and $\\mu(a|s)$ , we can consider it as a variant of behavior cloning; the second term is formulated as maxq\u03d5 E(s,a,s\u2032,\u03be)\u223cB lo g TT  ((ss\u2032\u2032||ss,,aa,,\u03be\u03be\u2217)) , which can be seen as a bandit RL objective for policy $q_{\\phi}(\\xi)$ to maximize reward $\\begin{array}{r}{\\mathcal{R}_{q}(\\xi)=\\log\\frac{T(s^{\\prime}|s,a,\\xi^{*})}{T(s^{\\prime}|s,a,\\xi)}}\\end{array}$ g T (s\u2032\u2032|s,a,\u03be\u2217) ; and the last term is regarded as a policy regularizer added on $q_{\\phi}(\\xi)$ to prevent it from collapsing. It is worth noting that the policy $q_{\\phi}(\\xi)$ here is not the one running (sampling) on the source domain, but the one outputting physics parameter vector $\\xi$ as an action to adjust the source domain. To prevent confusion, in the following paper, the policy running on the source domain, i.e., $\\pi(a|s)$ , is referred to as the \u201crunning policy\u201d, while the one adjusting the domain parameters, i.e., $q_{\\phi}(\\xi)$ , is referred to as the \u201ccalibration policy\u201d. Additionally, we similarly define the \u201ccalibration critic\u201d, which is responsible for evaluating the accuracy of the parameters output by the calibration actor. The calibration critic and the calibration policy (actor) together constitute a calibration agent. ", "page_idx": 4}, {"type": "text", "text": "The key challenge lies in how to estimate the stochastic reward log TT  ((ss\u2032||ss,,aa,,\u03be\u03be)) given the offilne data and simulated rollouts. According to Bayes\u2019 rule, we can transform the transition probability [29] as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nT(s^{\\prime}|s,a,\\xi)=\\frac{P(\\xi|s,a,s^{\\prime})P(s,a,s^{\\prime})}{P(\\xi)P(s,a|\\xi)}=\\frac{P(\\xi|s,a,s^{\\prime})P(s,a,s^{\\prime})}{P(\\xi|s,a)P(s,a)},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and the reward can be derived as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{R}_{q}(\\xi)=\\log P(\\xi^{*}|s,a,s^{\\prime})-\\log P(\\xi^{*}|s,a)-\\log P(\\xi|s,a,s^{\\prime})+\\log P(\\xi|s,a),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\xi^{*}$ and $\\xi$ stand for the target and source domains respectively. Hence, we can train a couple of binary classifiers $D_{\\psi_{\\mathrm{sas}}}(\\cdot|s,a,s^{\\prime})$ and $D_{\\psi_{\\mathrm{sa}}}(\\cdot|s,a)$ to discriminate whether state-action-state and state-action pairs come from the offilne dataset (referred to as the binary variable target) or synthetic samples (referred to as the binary variable source). These two discriminators form a reward model with certain generalization ability [52], and the corresponding cross-entropy losses are written as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\psi_{\\mathrm{sas}}}=-\\mathbb{E}_{(s,a,s^{\\prime})\\sim\\mathcal{D}}[\\log D_{\\psi_{\\mathrm{sas}}}(\\mathrm{target}|s,a,s^{\\prime})]-\\mathbb{E}_{(s,a,s^{\\prime})\\sim\\mathcal{B}}[\\log D_{\\psi_{\\mathrm{sas}}}(\\mathrm{source}|s,a,s^{\\prime})],}\\\\ &{\\mathcal{L}_{\\psi_{\\mathrm{sa}}}=-\\mathbb{E}_{(s,a)\\sim\\mathcal{D}}[\\log D_{\\psi_{\\mathrm{sa}}}(\\mathrm{target}|s,a)]-\\mathbb{E}_{(s,a)\\sim\\mathcal{B}}[\\log D_{\\psi_{\\mathrm{sa}}}(\\mathrm{source}|s,a)].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "4.2 Multi-Agent Domain Calibration ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As the complexity of the source domain grows, characterized by an expanding number of physics parameters, the calibration policy learned by single-agent reinforcement learning often struggles to consistently reduce the domain gap. To address this challenge, we employ multi-agent reinforcement learning (MARL) algorithms to effectively reduce the search space for a single calibration agent, thereby enhancing the efficiency and fidelity of domain calibration. ", "page_idx": 4}, {"type": "text", "text": "We conduct experiments on the HalfCheetah [53] environment and train to calibrate the gravity coefficient along with other physics parameters. The preliminary results are shown in Fig. 2, we capture snapshots for both approaches at the same training step to investigate Pearson correlation [54] between the critic value of the gravity coefficient and the absolute calibration error. Here, the absolute calibration error represents the absolute difference between the parameters output by the calibration actor and the target parameters. The singleagent method, represented by the blue dots, utilizes just a shared calibration critic for parameter adjusting, facing challenges in assessing a specific action within a huge action space. In contrast, the multi-agent method, depicted by the red dots, employs value decomposition algorithms [44] to narrow the action space of each individual calibration policy. This decomposition leads to more accurate evaluations of domain parameters, thereby reducing calibration errors and improving policy transfer. ", "page_idx": 5}, {"type": "image", "img_path": "hkBhX5ABjk/tmp/1ed820c1bc0a2196475f4283e2243325b94967de70f1ebbbab84c0247548c7ac.jpg", "img_caption": ["Figure 2: The Pearson correlation between the critic value of the gravity coefficient and the absolute calibration error. Each dot represents a sampled action, which is then fed into the corresponding critic to compute the critic value. When the parameters output by the calibration actor are closer to the target parameters (indicating a smaller absolute calibration error), the evaluation value output by a \u201cgood\u201d critic should be higher. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "To utilize the cooperative MARL for domain calibration, we now formally define $N$ agents (since the source domain has a total of $N$ physics parameters) to perform domain calibration coordinately where each of them $q_{\\phi}^{i}(\\xi^{i})$ attempts to adjust the single parameter $\\xi^{i}$ , therefore the joint calibration policy can be decomposed as $\\begin{array}{r}{q_{\\phi}(\\xi)=\\prod_{i=1}^{N}q_{\\phi}^{i}(\\xi^{i})}\\end{array}$ . Utilizing any off-the-shelf value decomposition algorithm VD, we employ the single global rewards $[\\mathcal{R}_{q}^{1}(\\xi^{i}),\\cdot\\cdot\\cdot\\;,\\mathcal{R}_{q}^{N}(\\xi^{N})]=\\mathrm{VD}(\\mathcal{R}_{q}(\\xi))$ to guide individual calibration policy updates: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{q_{\\phi}^{i}}\\mathbb{E}_{\\xi^{i}\\sim q_{\\phi}^{i}(\\cdot)}\\mathcal{R}_{q}^{i}(\\xi^{i})-D_{\\mathrm{KL}}\\left(q_{\\phi}^{i}(\\xi^{i})||p^{i}(\\xi^{i})\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Nonetheless, when the number of the domain parameters $N$ is large, employing $N$ agents for domain calibration leads to low exploration and optimization efficiency [55]. To mitigate this problem, clustering physics parameters with similar effects on the transition dynamics, e.g., the mass of a symmetrical robot\u2019s left and right feet, into one calibration agent is an advisable choice. Hence we introduce an automatic grouping technique by adopting a Variational Autoencoder (VAE) [56]. By adjusting a specific parameter $\\xi^{i}$ while keeping all other parameters $\\xi^{-i}$ fixed [57], we can assume the identity $i$ of each parameter (i.e., the one-hot encoding) to be representative of the transition $\\mathrm{tr}=(s,a,\\dot{s}^{\\prime},\\xi^{i})$ , the Evidence Lower BOund (ELBO) of the transition is then derived as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\log P(\\mathrm{tr})\\geq\\mathbb{E}_{z\\sim f_{\\mathrm{e}}(z|i)}[\\log f_{\\mathrm{d}}(\\mathrm{tr}|z)]-D_{\\mathrm{KL}}\\left(f_{\\mathrm{e}}(z|i)||p(z)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $f_{\\mathrm{e}},f_{\\mathrm{d}}$ stand for the encoder and decoder, $z$ is the latent variable, and $p(z)$ represents the corresponding prior distribution. We can deduce the reconstruction term as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log f_{\\mathrm{d}}(\\mathrm{tr}|z)=\\log\\left[f_{\\mathrm{d}}(s^{\\prime}|s,a,\\xi^{i},z)f_{\\mathrm{d}}(s,a,\\xi^{i}|z)\\right],}\\\\ &{\\qquad\\qquad\\qquad=\\log f_{\\mathrm{d}}(s^{\\prime}|s,a,\\xi^{i},z)+c,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $c$ is a constant as $s,\\,a$ , and $\\xi^{i}$ do not depend on the latent variable $z$ . We parameterize the encoder $f_{\\mathrm{e}}$ and the decoder $f_{\\mathrm{d}}$ with $\\Psi$ , making them $f_{\\Psi_{\\mathrm{e}}}$ and $f_{\\Psi_{\\mathrm{d}}}$ , and optimize the VAE model with the following loss: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\Psi}=-\\mathbb{E}_{(s,a,s^{\\prime},\\xi^{i},\\xi^{-i})\\sim{B},z\\sim f_{\\Psi_{\\mathrm{e}}}(\\cdot|i)}\\left[\\log f_{\\Psi_{\\mathrm{d}}}(s^{\\prime}|s,a,\\xi^{i},z)\\right]+D_{\\mathrm{KL}}\\left(f_{\\Psi_{\\mathrm{e}}}(z|i)||p(z)\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Before domain calibration, we first minimize the VAE loss (Eq. 10). Then we apply the $\\mathbf{k}$ -means clustering method [58] to the means generated by the encoder $f_{\\Psi_{\\mathrm{e}}}(z|i)$ for all $i\\in N$ , in order to group the domain parameters [39]. The resulting $n$ $[1\\,\\leq\\,n\\,\\leq\\,N]$ ) grouping scheme is recorded as $[\\xi^{g1},\\cdots\\,,\\xi^{g n}]$ , each group forms one agent equipped with a calibration actor $q_{\\phi}^{g i}(\\xi^{g i})$ and a calibration critic $v_{\\Phi}^{g i}(\\xi^{g i})$ , following the multi-agent actor-critic framework. ", "page_idx": 5}, {"type": "text", "text": "4.3 Practical Algorithm ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Based on the above analysis, we now present a practical algorithm under the Madoc framework, the pseudo-code is shown in App. A. We apply DOP [49], a popular multi-agent policy gradient algorithm as the value decomposition method. Concretely, there are $n$ agents for calibrating the source domain after automatic grouping, we factor the joint critic as a weighted summation of individual critics: ", "page_idx": 6}, {"type": "equation", "text": "$$\nV_{\\Phi}^{\\mathrm{tot}}=\\sum_{i=1}^{n}k_{i}v_{\\Phi}^{g i}(\\xi^{g i})+b,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $k_{i}\\geq0$ and $b$ are denoted as learnable weights and biases. The individual critics are learned by back-propagating gradients from global Temporal Difference updates: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\Phi}=\\mathbb{E}_{(s,a,s^{\\prime},\\xi)\\sim\\mathcal{B}}\\left[\\frac{1}{2}\\left(V_{\\Phi}^{\\mathrm{tot}}-\\mathcal{R}_{q}(\\xi)\\right)^{2}\\right].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Given individual critics, we use SAC [59] to update the stochastic actors in an off-policy manner: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\phi}=\\mathbb{E}_{\\xi^{g i}\\sim q_{\\phi}^{g i}(\\cdot)}\\left[\\alpha\\log q_{\\phi}^{g i}(\\xi^{g i})-v_{\\Phi}^{g i}(\\xi^{g i})+\\lambda D_{\\mathrm{KL}}\\left(q_{\\phi}^{g i}(\\xi^{g i})||p^{g i}(\\xi^{g i})\\right)\\right],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\alpha$ and $\\lambda$ control the relative importance of the entropy and regularization term respectively. Besides, the prior domain parameter distribution $p^{g i}({\\xi^{g i}})^{\\mathbf{\\alpha}}$ is set to be an exponentially moving average of the current calibration policy $q_{\\phi}^{g i}(\\xi^{g i})$ , which has been shown to stabilize training like target network. Finally, we parameterize the policy running in the source domain $\\pi$ with $\\theta$ and enable it to clone the behavior policy on offilne data during domain calibration. Once domain calibration is complete, we train the policy $\\pi_{\\theta}$ from scratch on the source domain using SAC, and directly deploy it to the target domain. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we present the empirical evaluations of our proposed Madoc framework. We first describe the experiment environments and related baselines in Sec. 5.1, and then conduct a series of experiments to answer the following questions: (1) How is the comprehensive performance of Madoc against multiple baselines (Sec. 5.2)? (2) How do core components of Madoc contribute to the overall performance (Sec. 5.3)? (3) How is the generalization capability of Madoc across datasets of varying sizes and source domains with different initial ranges (Sec. 5.4)? ", "page_idx": 6}, {"type": "text", "text": "5.1 Experiment Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In our experiments, we evaluate Madoc on classic continuous control tasks from the MuJoCo [53] engine and choose two offilne benchmarks to serve as offilne datasets collected in the target domain. On the popular D4RL benchmark [60], we choose four locomotion tasks (HalfCheetah, Hopper, Walker2d, Ant), each with three types of datasets (medium, medium-replay, medium-expert), to evaluate different algorithms\u2019 performance when faced with datasets of varying quality. Considering more challenging scenarios, three environments (HalfCheetah, Hopper, Walker2d) along with three levels of datasets (low, medium, high) from NeoRL benchmark [61] are also selected. The main difference between the two benchmarks lies in that the static datasets in the NeoRL benchmark occupy more narrow distributions. During the training process, we are given imperfect source domains, and only aware of the initial range of specific physics parameters (gravity, body_mass, dof_damping). Each environment has different parameter dimensions, initial ranges, and ground truth values, refer to App. D for detailed information. ", "page_idx": 6}, {"type": "text", "text": "Madoc utilizes static offline datasets to calibrate biased source domains, and we choose several baseline algorithms with identical or similar settings for comparison. DROPO [17] and DROID [16] use fixed offline datasets from the target domain to optimize the distribution bounds with different objectives via evolution algorithms like CMA-ES [18]. OTED [51] models the parameter optimization process as a bandit RL problem, which is similar to our method, but the objective is different and cannot cope with large parameter space. H2O [62] and DR+BC are two hybrid offline-and-online algorithms where the former penalizes the Q-function learning on simulated state-action pairs with large dynamics gaps, and the latter directly combines uniform domain randomization with behavior cloning. CQL [63] is the classic pure offilne RL algorithm using value regularization, MOREC [52] is the state-of-the-art offilne model-based RL algorithm by learning a generalizable dynamics reward function, representing the upper-bound performance by solely utilizing offline data. Madoc-S is an ablation algorithm of our proposed Madoc, which only uses a single calibration agent for parameter tuning, without applying the multi-agent decomposition and automatic grouping technique. Madoc and all the above baselines use SAC [59] as the online algorithm after domain calibration for its practicality and convenience on MuJoCo tasks. ", "page_idx": 6}, {"type": "table", "img_path": "hkBhX5ABjk/tmp/66abaa4a47efbb86c65190b8ff0085345a0b01f4e4681cd06c0ec5bfc53f6465.jpg", "table_caption": ["Table 1: Normalized average returns on D4RL benchmark. The results are evaluated in the target domain and we bold the highest mean. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "hkBhX5ABjk/tmp/a774ce3c66d2665557520d4bb64f70a910448b28308c64de58a0feaee4b74148.jpg", "table_caption": ["Table 2: Normalized average returns on NeoRL benchmark. The results are evaluated in the target domain and we bold the highest mean. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "All the numerical results in our experiments are reported in terms of mean and $95\\%$ confidence interval computed over 6 random seeds, and we give more details about the model implementation of our method in App. C. ", "page_idx": 7}, {"type": "text", "text": "5.2 Performance Comparison on the Benchmarks ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We first evaluate the overall performance of our method and baselines on the two benchmarks and use the normalized average scores introduced by D4RL [60] for intuitive comparison. In realistic scenarios, both the size of offilne datasets and the usage of the source domain can be restricted, thus we constrain each algorithm to access a static dataset with up to $2\\times10^{5}$ transitions and execute only $1\\times10^{6}$ simulated samples for domain calibration (when used). Due to these constraints, the performance of the baseline algorithms has experienced varying degrees of degradation. As shown in Tab. 1 and Tab. 2, we observe that our methods outperform prior methods on both benchmarks: two pure offline algorithms, CQL and MOREC, suffer from severe performance degradation in some scenarios, especially when encountering narrow NeoRL datasets; two hybrid offilne-and-online algorithms are limited by the dataset quality and domain fidelity, and cannot achieve the expected performance; DROPO and DROID employ evolutionary algorithms to optimize domain parameters, encountering challenges in transferring learned policies when the parameter space is large; the comparison with OTED demonstrates the rationality of our optimization objective described in Sec. 4.1, while the comparison with Madoc-S further illustrates the effectiveness of modeling domain calibration as a multi-agent system formulated in Sec. 4.2. To verify that Madoc indeed reduces the dynamics gap between the source and target domains, we report the mean absolute calibration error for different algorithms in App. E.1. Furthermore, given the significant variance in domain ", "page_idx": 7}, {"type": "image", "img_path": "hkBhX5ABjk/tmp/cd4c93c74877b2f36eb627405d73a4b7f8aa37fdc75f77499438a5bb0572923d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 3: (a) The visualization results of the automatic grouping technique. The left part is a schematic of the Hopper robot, each point on it representing a physics parameter to be calibrated, and different colors indicate the final grouping results. For example, the four parameters encircled by the yellow rectangle are clustered into one group in the embedding space on the right part. (b) The normalized average return of Madoc and Madoc-S on the Ant environment. We can observe that as the parameter dimension of the source domain increases, the performance gap between them (indicated by the black shadow) becomes more pronounced. ", "page_idx": 8}, {"type": "text", "text": "calibration algorithms, we provide a more detailed discussion on the stability of the experimental results in App. E.3. ", "page_idx": 8}, {"type": "text", "text": "5.3 Effectiveness of Different Components ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To investigate the impact of the design components of Madoc, we first design experiments on the Hopper environment to visualize the automatic grouping technique, as shown in Fig. 3(a). The Hopper is a one-legged robot simulation with four distinct body sections: the torso, thigh, leg, and foot. These components are connected by three joints, which serve as the articulation points between each pair of bodies. We need to calibrate the gravity coefficient, the mass of each body, and the damping coefficients at each joint. After projecting the embedding space onto a three-dimensional space, we can discover that the robot\u2019s physics parameters are divided in an orderly manner from top to bottom, while the gravity coefficient forms a separate group on its own. The result aligns perfectly with our expectations, as parameters that have similar impacts on dynamics can be adjusted using one calibration policy. Additionally, in Fig. 3(b), we conduct ablation studies on the Ant environment of the D4RL benchmark, to verify the effectiveness of modeling domain calibration as an MARL problem. When the dimension of the domain parameters is 1 or 2, there is no significant performance gap between Madoc and Madoc-S; however, as the source domain becomes more complex, the performance of both declines, but the drop is faster when only one agent is employed for parameter calibration. This experimental result favorably supports that modeling as MARL can enhance the efficiency of domain calibration in large parameter spaces. ", "page_idx": 8}, {"type": "text", "text": "5.4 Generalization across Various Conditions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Madoc leverages static offilne datasets to adjust the domain parameters, driving our curiosity toward its generalization capacity under varying dataset sizes and initial parameter ranges of the source domain. Here we choose all the above-mentioned tasks on the NeoRL benchmark and calculate the averaged normalized returns for comparison. As illustrated in Fig. 4(a), the algorithms access datasets of different magnitudes, $5\\times10^{4}$ (small), $2\\times10^{5}$ (medium), and $1\\times10^{6}$ (large), to reflect a spectrum of data availability. The results reveal that the effectiveness of both the hybrid offline-and-online H2O algorithm and the purely offilne MOREC algorithm rises with the expansion of the dataset size. This discovery suggests that their dependency on the size of offline data for improved performance. On the contrary, our method maintains stable and excellent performance, unfazed by the dataset size. Besides, for source domains with various initial parameter ranges, categorized as easy, medium, and hard (see App. D for more details), Madoc exhibits remarkable performances across all levels, particularly excelling in \u201chard\u201d cases with the largest parameter search space, as shown in Fig. 4(b). ", "page_idx": 8}, {"type": "image", "img_path": "hkBhX5ABjk/tmp/20565355c3a12ee7a631cfe1c1fad3cfba5876a45f78d49f0472a332d423d22e.jpg", "img_caption": ["Figure 4: The generalization ability of Madoc compared to the baselines under different conditions. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "This underscores our algorithm\u2019s effectiveness in coping with challenging scenarios, affirming its robustness and adaptability in diverse conditions. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion and Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduce Madoc, a framework for closing the dynamics gap by calibrating the source domain with a handful of offline data via multi-agent reinforcement learning. Concretely, the target domain data serve as a guide for target transition dynamics, which is leveraged to train classifiers generating rewards and derive a bandit RL objective for domain calibration, To improve calibration efficiency with a large number of parameters, we further model it as a cooperative MARL problem and propose to group parameters with similar effects on dynamics. Experiments on popular control tasks demonstrate that our method can calibrate the source domain with sufficient accuracy, allowing the optimal trained policy to be transferred to the target domain without severe performance deterioration. One possible constraint of our method is that, when dealing with high-dimensional vision tasks [64], using a handful of offilne data may not guarantee the accuracy and generalizability of the reward model. This challenge could be mitigated by deploying more expressively powerful tools like diffusion models [65], which is left for future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements This work is supported by the National Science Foundation of China (62276126, 62250069), the Natural Science Foundation of Jiangsu (BK20221442, BK20243039, and BK2024119), and the Fundamental Research Funds for the Central Universities (0221/14380022). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Xu Wang, Sen Wang, Xingxing Liang, Dawei Zhao, Jincai Huang, Xin Xu, Bin Dai, and Qiguang Miao. Deep reinforcement learning: A survey. IEEE Transactions on Neural Networks and Learning Systems, 35(4):5064\u20135078, 2024.   \n[2] M Mehdi Afsar, Trafford Crump, and Behrouz Far. Reinforcement learning based recommender systems: A survey. ACM Computing Surveys, 55(7):1\u201338, 2022.   \n[3] Bharat Singh, Rajesh Kumar, and Vinay Pratap Singh. Reinforcement learning in robotic applications: A comprehensive survey. Artificial Intelligence Review, 55(2):945\u2013990, 2022.   \n[4] Chao Yu, Jiming Liu, Shamim Nemati, and Guosheng Yin. Reinforcement learning in healthcare: A survey. ACM Computing Surveys, 55(1):1\u201336, 2021.   \n[5] B Ravi Kiran, Ibrahim Sobh, Victor Talpaert, Patrick Mannion, Ahmad A Al Sallab, Senthil Yogamani, and Patrick P\u00e9rez. Deep reinforcement learning for autonomous driving: A survey. IEEE Transactions on Intelligent Transportation Systems, 23(6):4909\u20134926, 2021.   \n[6] Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter. Learning quadrupedal locomotion over challenging terrain. Science Robotics, 5(47):5986, 2020.   \n[7] Jie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen, Yunfei Bai, Danijar Hafner, Steven Bohez, and Vincent Vanhoucke. Sim-to-real: Learning agile locomotion for quadruped robots. In Robotics: Science and Systems, 2018.   \n[8] Zhi-Hua Zhou. Open-environment machine learning. National Science Review, 9(8):nwac123, 2022.   \n[9] OpenAI, Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, Jonas Schneider, Nikolas Tezak, Jerry Tworek, Peter Welinder, Lilian Weng, Qiming Yuan, Wojciech Zaremba, and Lei Zhang. Solving rubik\u2019s cube with a robot hand. arXiv preprint arXiv:1910.07113, 2019.   \n[10] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In International Conference on Intelligent Robots and Systems, pages 23\u201330, 2017.   \n[11] Wenshuai Zhao, Jorge Pe\u00f1a Queralta, and Tomi Westerlund. Sim-to-real transfer in deep reinforcement learning for robotics: A survey. In IEEE Symposium Series on Computational Intelligence, pages 737\u2013744, 2020.   \n[12] Yuqing Du, Olivia Watkins, Trevor Darrell, Pieter Abbeel, and Deepak Pathak. Auto-tuned sim-to-real transfer. In IEEE International Conference on Robotics and Automation, pages 1290\u20131296, 2021.   \n[13] Fabio Ramos, Rafael Possas, and Dieter Fox. Bayessim: Adaptive domain randomization via probabilistic inference for robotics simulators. In Robotics: Science and Systems, 2019.   \n[14] Fabio Muratore, Christian Eilers, Michael Gienger, and Jan Peters. Data-efficient domain randomization with bayesian optimization. IEEE Robotics and Automation Letters, 6(2):911\u2013 918, 2021.   \n[15] Marius Memmel, Andrew Wagenmaker, Chuning Zhu, Dieter Fox, and Abhishek Gupta. ASID: Active exploration for system identification and reconstruction in robotic manipulation. In International Conference on Learning Representations, 2023.   \n[16] Ya-Yen Tsai, Hui Xu, Zihan Ding, Chong Zhang, Edward Johns, and Bidan Huang. DROID: Minimizing the reality gap using single-shot human demonstration. IEEE Robotics and Automation Letters, 6(2):3168\u20133175, 2021.   \n[17] Gabriele Tiboni, Karol Arndt, and Ville Kyrki. DROPO: Sim-to-real transfer with offline domain randomization. Robotics and Autonomous Systems, 166:104432, 2023.   \n[18] Nikolaus Hansen and Andreas Ostermeier. Completely derandomized self-adaptation in evolution strategies. Evolutionary Computation, 9(2):159\u2013195, 2001.   \n[19] Yevgen Chebotar, Ankur Handa, Viktor Makoviychuk, Miles Macklin, Jan Issac, Nathan Ratliff, and Dieter Fox. Closing the sim-to-real loop: Adapting simulation randomization with real world experience. In International Conference on Robotics and Automation, pages 8973\u20138979, 2019.   \n[20] Jan Peters, Katharina Mulling, and Yasemin Altun. Relative entropy policy search. In AAAI Conference on Artificial Intelligence, pages 1607\u20131612, 2010.   \n[21] Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal J\u00f3zefowicz, Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, Jonas Schneider, Szymon Sidor, Josh Tobin, Peter Welinder, Lilian Weng, and Wojciech Zaremba. Learning dexterous in-hand manipulation. The International Journal of Robotics Research, 39(1):3\u201320, 2020.   \n[22] Bhairav Mehta, Manfred Diaz, Florian Golemo, Christopher J Pal, and Liam Paull. Active domain randomization. In Conference on Robot Learning, pages 1162\u20131176, 2020.   \n[23] Peide Huang, Xilun Zhang, Ziang Cao, Shiqi Liu, Mengdi Xu, Wenhao Ding, Jonathan Francis, Bingqing Chen, and Ding Zhao. What went wrong? closing the sim-to-real gap via differentiable causal discovery. In Conference on Robot Learning, pages 734\u2013760, 2023.   \n[24] Nima Fazeli, Russ Tedrake, and Alberto Rodriguez. Identifiability analysis of planar rigid-body frictional contact. Robotics Research: Volume 2, pages 665\u2013682, 2018.   \n[25] Ali Dorri, Salil S Kanhere, and Raja Jurdak. Multi-agent systems: A survey. IEEE Access, 6:28573\u201328593, 2018.   \n[26] Lei Yuan, Ziqian Zhang, Lihe Li, Cong Guan, and Yang Yu. A survey of progress on cooperative multi-agent reinforcement learning in open environment. arXiv preprint arXiv:2312.01058, 2023.   \n[27] Manuel Kaspar, Juan D Mu\u00f1oz Osorio, and J\u00fcrgen Bock. Sim2real transfer for reinforcement learning without dynamics randomization. In International Conference on Intelligent Robots and Systems, pages 4383\u20134388, 2020.   \n[28] Fereshteh Sadeghi and Sergey Levine. CAD2RL: real single-image filght without a single real image. In Robotics: Science and Systems, 2017.   \n[29] Benjamin Eysenbach, Shreyas Chaudhari, Swapnil Asawa, Sergey Levine, and Ruslan Salakhutdinov. Off-dynamics reinforcement learning: Training for transfer with domain classifiers. In International Conference on Learning Representations, 2020.   \n[30] Kang Xu, Chenjia Bai, Xiaoteng Ma, Dong Wang, Bin Zhao, Zhen Wang, Xuelong Li, and Wei Li. Cross-domain policy adaptation via value-guided data flitering. In Advances in Neural Information Processing Systems, pages 73395\u201373421, 2023.   \n[31] Xiong-Hui Chen, Shengyi Jiang, Feng Xu, Zongzhang Zhang, and Yang Yu. Cross-modal domain adaptation for cost-efficient visual reinforcement learning. In Advances in Neural Information Processing Systems, pages 12520\u201312532, 2021.   \n[32] Lennart Ljung. System identification. In Signal Analysis and Prediction, pages 163\u2013173. Springer, 1998.   \n[33] Horia Mania, Michael I Jordan, and Benjamin Recht. Active learning for nonlinear system identification with guarantees. Journal of Machine Learning Research, 23(32):1\u201330, 2022.   \n[34] Wenhao Yu, Jie Tan, C. Karen Liu, and Greg Turk. Preparing for the unknown: Learning a universal policy with online system identification. In Robotics: Science and Systems, 2017.   \n[35] Zhi-Hua Zhou, Yang Yu, and Chao Qian. Evolutionary learning: Advances in theories and algorithms. Springer, 2019.   \n[36] Jianhong Wang, Wangkun Xu, Yunjie Gu, Wenbin Song, and Tim C Green. Multi-agent reinforcement learning for active voltage control on power distribution networks. In Advances in Neural Information Processing Systems, pages 3271\u20133284, 2021.   \n[37] Guillaume Sartoretti, Justin Kerr, Yunfei Shi, Glenn Wagner, TK Satish Kumar, Sven Koenig, and Howie Choset. PRIMAL: Pathfinding via reinforcement and imitation multi-agent learning. IEEE Robotics and Automation Letters, 4(3):2378\u20132385, 2019.   \n[38] Ke Xue, Jiacheng Xu, Lei Yuan, Miqing Li, Chao Qian, Zongzhang Zhang, and Yang Yu. Multi-agent dynamic algorithm configuration. In Advances in Neural Information Processing Systems, pages 20147\u201320161, 2022.   \n[39] Filippos Christianos, Georgios Papoudakis, Muhammad A Rahman, and Stefano V Albrecht. Scaling multi-agent reinforcement learning with selective parameter sharing. In International Conference on Machine Learning, pages 1989\u20131998, 2021.   \n[40] Lei Yuan, Tao Jiang, Lihe Li, Feng Chen, Zongzhang Zhang, and Yang Yu. Robust cooperative multi-agent reinforcement learning via multi-view message certification. Science China Information Sciences, 67(4):142102, 2024.   \n[41] Ming Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In International Conference on Machine Learning, pages 330\u2013337, 1993.   \n[42] Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information Processing Systems, pages 6379\u20136390, 2017.   \n[43] Chao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising effectiveness of PPO in cooperative multi-agent games. In Advances in Neural Information Processing Systems, pages 24611\u201324624, 2022.   \n[44] Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vin\u00edcius Flores Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z. Leibo, Karl Tuyls, and Thore Graepel. Value-decomposition networks for cooperative multi-agent learning based on team reward. In International Conference on Autonomous Agents and MultiAgent Systems, pages 2085\u20132087, 2018.   \n[45] Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Monotonic value function factorisation for deep multi-agent reinforcement learning. Journal of Machine Learning Research, 21(178):1\u201351, 2020.   \n[46] Muning Wen, Jakub Grudzien Kuba, Runji Lin, Weinan Zhang, Ying Wen, Jun Wang, and Yaodong Yang. Multi-agent reinforcement learning is a sequence modeling problem. In Advances in Neural Information Processing Systems, pages 16509\u201316521, 2022.   \n[47] Rihab Gorsane, Omayma Mahjoub, Ruan John de Kock, Roland Dubb, Siddarth Singh, and Arnu Pretorius. Towards a standardised performance evaluation protocol for cooperative marl. In Advances in Neural Information Processing Systems, pages 5510\u20135521, 2022.   \n[48] Lei Yuan, Lihe Li, Ziqian Zhang, Fuxiang Zhang, Cong Guan, and Yang Yu. Multiagent continual coordination via progressive task contextualization. IEEE Transactions on Neural Networks and Learning Systems, 2024.   \n[49] Yihan Wang, Beining Han, Tonghan Wang, Heng Dong, and Chongjie Zhang. DOP: Offpolicy multi-agent decomposed policy gradients. In International Conference on Learning Representations, 2020.   \n[50] Richard S. Sutton and Andrew G. Barto. Reinforcement learning: An introduction. Adaptive computation and machine learning. MIT Press, 1998.   \n[51] Izzeddin Gur, Ofir Nachum, and Aleksandra Faust. Targeted environment design from offilne data. In Deep RL Workshop NeurIPS, 2021.   \n[52] Fan-Ming Luo, Tian Xu, Xingchen Cao, and Yang Yu. Reward-consistent dynamics models are strongly generalizable for offline reinforcement learning. In International Conference on Learning Representations, 2023.   \n[53] Emanuel Todorov, Tom Erez, and Yuval Tassa. MuJoCo: A physics engine for model-based control. In International Conference on Intelligent Robots and Systems, pages 5026\u20135033, 2012.   \n[54] Philip Sedgwick. Pearson\u2019s correlation coefficient. Bmj, 345, 2012.   \n[55] Jayesh K Gupta, Maxim Egorov, and Mykel Kochenderfer. Cooperative multi-agent control using deep reinforcement learning. In Autonomous Agents and Multiagent Systems: AAMAS 2017 Workshops, Best Papers, pages 66\u201383, 2017.   \n[56] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In International Conference on Learning Representations, 2014.   \n[57] Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Counterfactual multi-agent policy gradients. In AAAI conference on Artificial Intelligence, 2018.   \n[58] Tapas Kanungo, David M Mount, Nathan S Netanyahu, Christine D Piatko, Ruth Silverman, and Angela Y Wu. An efficient k-means clustering algorithm: Analysis and implementation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 24(7):881\u2013892, 2002.   \n[59] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. In International Conference on Machine Learning, pages 1861\u20131870, 2018.   \n[60] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4RL: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.   \n[61] Rong-Jun Qin, Xingyuan Zhang, Songyi Gao, Xiong-Hui Chen, Zewen Li, Weinan Zhang, and Yang Yu. NeoRL: A near real-world benchmark for offilne reinforcement learning. In Advances in Neural Information Processing Systems, pages 24753\u201324765, 2022.   \n[62] Haoyi Niu, Shubham Sharma, Yiwen Qiu, Ming Li, Guyue Zhou, Jianming Hu, and Xianyuan Zhan. When to trust your simulator: Dynamics-aware hybrid offilne-and-online reinforcement learning. In Advances in Neural Information Processing Systems, pages 36599\u201336612, 2022.   \n[63] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. In Advances in Neural Information Processing Systems, pages 1179\u20131191, 2020.   \n[64] Md Tanzil Shahria, Md Samiul Haque Sunny, Md Ishrak Islam Zarif, Jawhar Ghommam, Sheikh Iqbal Ahamed, and Mohammad H Rahman. A comprehensive review of vision-based robotic applications: Current state, components, approaches, barriers, and potential solutions. Robotics, 11(6):139, 2022.   \n[65] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, pages 6840\u20136851, 2020.   \n[66] Yang Yu. Towards sample efficient reinforcement learning. In International Joint Conference on Artificial Intelligence, pages 5739\u20135743, 2018.   \n[67] Petar Kormushev, Sylvain Calinon, and Darwin G Caldwell. Reinforcement learning in robotics: Applications and real-world challenges. Robotics, 2(3):122\u2013148, 2013.   \n[68] Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Micha\u00ebl Mathieu, Andrew Dudzik, Junyoung Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John P. Agapiou, Max Jaderberg, Alexander Sasha Vezhnevets, R\u00e9mi Leblond, Tobias Pohlen, Valentin Dalibard, David Budden, Yury Sulsky, James Molloy, Tom Le Paine, \u00c7aglar G\u00fcl\u00e7ehre, Ziyu Wang, Tobias Pfaff, Yuhuai Wu, Roman Ring, Dani Yogatama, Dario W\u00fcnsch, Katrina McKinney, Oliver Smith, Tom Schaul, Timothy P. Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps, and David Silver. Grandmaster level in StarCraft II using multi-agent reinforcement learning. Nature, 575(7782):350\u2013354, 2019.   \n[69] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning, pages 1889\u20131897, 2015.   \n[70] Hans-Georg Beyer and Hans-Paul Schwefel. Evolution strategies \u2013 A comprehensive introduction. Natural Computing, 1:3\u201352, 2002.   \n[71] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine learning algorithms. In Advances in Neural Information Processing Systems, pages 2960\u20132968, 2012.   \n[72] Shimon Whiteson. Evolutionary computation for reinforcement learning. Reinforcement Learning: State-of-the-art, pages 325\u2013355, 2012.   \n[73] Thomas M. Moerland, Joost Broekens, Aske Plaat, and Catholijn M. Jonker. Model-based reinforcement learning: A survey. Foundations and Trends\u00ae in Machine Learning, 16(1):1\u2013118, 2023.   \n[74] Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The International Journal of Robotics Research, 32(11):1238\u20131274, 2013.   \n[75] Ching-An Cheng, Andrey Kolobov, and Alekh Agarwal. Policy improvement via imitation of multiple oracles. In Advances in Neural Information Processing Systems, pages 5587\u20135598, 2020.   \n[76] Eliseo Ferrante, Alessandro Lazaric, and Marcello Restelli. Transfer of task representation in reinforcement learning using policy-based proto-value functions. In International Conference on Autonomous Agents and MultiAgent Systems, pages 1329\u20131332, 2008.   \n[77] Richard S Sutton, Doina Precup, and Satinder Singh. Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. Artificial Intelligence, 112(1- 2):181\u2013211, 1999.   \n[78] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.   \n[79] Yuhang Ran, Yi-Chen Li, Fuxiang Zhang, Zongzhang Zhang, and Yang Yu. Policy regularization with dataset constraint for offline reinforcement learning. In International Conference on Machine Learning, pages 28701\u201328717, 2023.   \n[80] Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. MOPO: Model-based offilne policy optimization. In Advances in Neural Information Processing Systems, pages 14129\u201314142, 2020.   \n[81] Michael E Tipping and Christopher M Bishop. Probabilistic principal component analysis. Journal of the Royal Statistical Society Series B: Statistical Methodology, 61(3):611\u2013622, 1999.   \n[82] Christian Schroeder De Witt, Tarun Gupta, Denys Makoviichuk, Viktor Makoviychuk, Philip HS Torr, Mingfei Sun, and Shimon Whiteson. Is independent learning all you need in the StarCraft multi-agent challenge? arXiv preprint arXiv:2011.09533, 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A Algorithm Description ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The pseudo-code of Madoc is presented in Alg. 1, we utilize SAC [59] and DOP [49] as our backbone algorithms for domain calibration. From lines 1 to 9, we group the domain parameters by learning a VAE and employing $\\mathbf{k}\\cdot$ -means; lines 13 to 16 aim at training discriminators as the reward model and regularizing the running policy; the calibration actors and critics are updated from lines 17 to 19; finally, we train the running policy from scratch and deploy it to the target domain. ", "page_idx": 15}, {"type": "text", "text": "Algorithm 1 Madoc ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Input: A pre-collected dataset $\\mathcal{D}$ , an imperfect manipulable source domain $\\mathcal{M}$   \nInitialize: Policy $\\pi_{\\theta}$ , joint calibration actor $q_{\\phi}$ and critic $v_{\\Phi}$ , a couple of binary discriminators $D_{\\psi}$ ,   \nVAE $f_{\\Psi}$ , prior distributions $p(\\xi)$ and $p(z)$ , replay buffer $B={\\bar{\\emptyset}}$ for simulated rollouts, step for   \nparameter grouping $T_{\\mathrm{group}}$ , step for domain calibration $T_{\\mathrm{calibration}}$ , learning rate $\\eta$   \n1: for step $t=1,\\cdot\\cdot\\cdot\\;,T_{\\mathrm{group}}^{-}\\:\\mathbf{do}$   \n2: Behavior cloning:   \n3: $\\pi_{\\theta}\\leftarrow\\mathrm{BC}(\\pi_{\\theta},\\overline{{D}})$   \n4: Collect simulated data:   \n5: $\\mathcal{B}\\gets\\mathcal{B}\\bigcup\\mathrm{ROLLOUT}(\\pi_{\\theta},q_{\\phi},\\mathcal{M})$   \n6: Update V AE:   \n7: $f_{\\Psi}\\gets f_{\\Psi}-\\eta\\nabla_{\\Psi}\\mathcal{L}_{\\Psi}$ according to Eq. 10   \n8: end for   \n9: Get $n$ groups by $\\boldsymbol{\\mathrm{k}}$ -means, clear replay buffer $B=\\emptyset$ , and reinitialize policy $\\pi_{\\theta}$   \n10: for step $t=T_{\\mathrm{group}}+1,\\cdot\\cdot\\cdot,T_{\\mathrm{group}}+T_{\\mathrm{calibration}}\\;{\\bf d o}$   \n11: Collect simulated data:   \n12: $\\mathcal{B}\\gets\\mathcal{B}\\bigcup\\mathrm{ROLLOUT}(\\pi_{\\theta},q_{\\phi},\\mathcal{M})$   \n13: Update di scriminators:   \n14: $\\boldsymbol{D}_{\\psi}\\leftarrow\\boldsymbol{D}_{\\psi}-\\eta\\nabla_{\\psi}\\mathcal{L}_{\\psi}$ according to Eq. 6   \n15: Behavior cloning:   \n16: $\\pi_{\\theta}\\leftarrow\\mathrm{BC}(\\pi_{\\theta},\\overline{{D}})$   \n17: Update the joint calibration actor and critic:   \n18: $v_{\\Phi}\\leftarrow v_{\\Phi}-\\eta\\nabla_{\\Phi}\\mathcal{L}_{\\Phi}$ according to Eq. 12   \n19: $q_{\\phi}\\leftarrow q_{\\phi}-\\eta\\nabla_{\\phi}\\mathcal{L}_{\\phi}$ according to Eq. 13   \n20: end for   \n21: Train policy from scratch:   \n22: $\\pi_{\\theta}\\bar{\\leftarrow}\\operatorname{SAC}(\\pi_{\\theta},q_{\\phi},\\mathcal{M})$ ", "page_idx": 15}, {"type": "text", "text": "B Extended Related Work ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Sample efficient RL. A significant drawback of current reinforcement learning methods is their poor sample efficiency, leading to extensive environmental interactions [66]. This results in prohibitive costs in real-world applications [67] and hinders policy learning even in complex digital environments, like the full StarCraft game [68]. Multiple factors may restrict the sample efficiency, while several aspects can alleviate these limitations. Different optimization methods possess varying capabilities for exploring and exploiting data. Mainstream algorithms in reinforcement learning commonly rely on the gradient of the objective or surrogate objectives [69]. Still, there are also sample-based methods, a.k.a. derivative-free optimization, that offer their unique advantages. Typical algorithms like evolutionary algorithms [70] and Bayesian optimization [71] have been applied for reinforcement learning [72], showcasing better performance on some tasks. Nonetheless, these methods also encounter certain drawbacks, including slow convergence, difficulties in scaling, sensitivity to noise, and a lack of theoretical guarantee. From another perspective, model-based algorithms can be much more efficient as planning in the model is free of real-world samples in ideals [73]. Nevertheless, in high-dimensional environments, learning an accurate transition model using supervised learning is challenging. Employing manually constructed environments, i.e., simulators, is more efficient and practical, especially in the field of robotics [74]. However, it also encounters new problems due to the sim2real dynamics gap, and simulators are generally non-differentiable, unlike neural models. Besides the aspects discussed above, the ability to transfer knowledge is also crucial for improving the sampling efficiency. Rather than starting from scratch for each task, humans continuously learn and build upon experiences from a variety of tasks. Many methods have proposed various types of knowledge transfer, such as policy transfer [75], representation transfer [76], and skill transfer [77]. However, these methods work effectively only in specific cases. A general approach to transfer reinforcement learning is yet to be developed. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Offilne RL. In the offilne RL setting, the agent no longer has the ability to interact with the environment [78]. Instead, the learning algorithm is access to a static offline dataset, collected previously by an unknown behavior policy from the target domain. The main obstacle to this data-driven learning paradigm is the distribution shift problem due to the discrepancy between the learned and behavior policies, leading to severe extrapolation error. Previous works tackle this problem by constraining the learned policies against the behavior policy [79] or penalizing the value function on out-of-distribution (OOD) actions [63], both of which require large enough datasets. Besides, offilne model-based RL algorithms learn a dynamics model from offline data to enhance the efficiency of offline RL [80]. Benefiting from generating the synthetic data by the learned dynamics, model-based algorithms improve the coverage of the dataset. However, this depends on accurate dynamics models and still requires sufficient offilne data. It is worth noting that in this work we use offilne datasets to calibrate domain parameters, thereby avoiding the selection of OOD actions, and alleviating the requirements for dataset quality and quantity. ", "page_idx": 16}, {"type": "text", "text": "C Implementation Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Discriminators. We train two discriminator networks $D_{\\psi_{\\mathrm{sas}}}(\\cdot|s,a,s^{\\prime})$ and $D_{\\psi_{\\mathrm{sa}}}(\\cdot|s,a)$ to classify whether the rollouts comes from the offilne dataset or the source domain in our method. We follow the implementation in DARC [29], propagating gradients back through both discriminators as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{D_{\\psi_{\\mathrm{sa}}}(\\cdot|s,a)=\\mathrm{SoftMax}(f_{\\mathrm{sa}}(s,a)),\\quad\\quad\\quad}\\\\ {D_{\\psi_{\\mathrm{sas}}}(\\cdot|s,a,s^{\\prime})=\\mathrm{SoftMax}(f_{\\mathrm{sas}}(s,a,s^{\\prime})+f_{\\mathrm{sa}}(s,a)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $f_{\\mathrm{sa}}(s,a)$ and $f_{\\mathrm{sas}}(s,a,s^{\\prime})$ represent the outputs of the two classifier networks, Sof $\\operatorname{Alax}(x_{i})=$ xe\u2208xXp  xeixp xj , X = {target, source}. Both networks contain two hidden layers, while dropout layers and ReLU activations are used between layers. Before the final SoftMax layer, we employ the Tanh activation function to map the outputs into probabilities. ", "page_idx": 16}, {"type": "text", "text": "Grouping VAE. We train both the encoder and decoder with two hidden layers, both dropout layers and ReLU activation functions are used between layers. We employ MSE loss for state reconstruction and eliminate KL divergence, as our goal is to encode the agent identity rather than generating states. For each domain parameter, we encode its identity into a 16-dimensional latent space. Then the latent variable is concatenated with the current observation and action, and the combined data is fed into the decoder to predict the observation for the next time step. The latent variables of all parameters are projected to three dimensions using Principal Component Analysis (PCA) [81], after which they are grouped using the $\\mathbf{k}$ -means algorithm [58]. To reduce randomness in the group results, we repeatedly extract schemes from the last phase of group training and determine the final grouping scheme by choosing the one that occurs most frequently. ", "page_idx": 16}, {"type": "text", "text": "Calibration policy. We use DOP [49] as the multi-agent decomposition method because it is a popular off-policy multi-agent policy gradient method. As claimed in the paper, individual critics $v_{\\Phi}^{g i}$ are learned by backpropagating gradients from global TD updates, guiding the updates of individual calibration actors by a similar objective as in SAC [59]: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\phi}=\\mathbb{E}_{\\xi^{g i}\\sim q_{\\phi}^{g i}(\\cdot)}\\left[\\alpha\\log q_{\\phi}^{g i}(\\xi^{g i})-v_{\\Phi}^{g i}(\\xi^{g i})+\\lambda D_{\\mathrm{KL}}\\left(q_{\\phi}^{g i}(\\xi^{g i})||p^{g i}(\\xi^{g i})\\right)\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Here, we do not employ the double-Q function and the automatic tuning scheme of $\\alpha$ . Instead we update $\\begin{array}{r}{\\lambda_{t}=\\operatorname*{min}(10,\\exp(\\frac{t}{2\\times10^{5}}))}\\end{array}$ and prior $p^{g i}(\\xi^{g i})$ as a target network of $q^{g i}(\\xi^{g i})$ with smoothing coefficient $1\\times10^{-3}$ to stabilize training. The prior parameter $p^{g i}(\\xi^{g i})$ is distribution is initialized as a normal distribution ${\\mathcal{N}}(0,1)$ to enhance exploration at the early stages of training. ", "page_idx": 16}, {"type": "text", "text": "We scale the output of the calibration actors by using a Gaussian with Tanh squashing, e.g., we rescale the output of gravity to be within $[-30,0]$ in our preliminary experiments. To further enhance the ", "page_idx": 16}, {"type": "text", "text": "Table 3: The comparisons of method complexities. ", "page_idx": 17}, {"type": "table", "img_path": "hkBhX5ABjk/tmp/cb7c1a9c89270147300ee1881cdea5868a35ff26e447099480e4a26dc9440d49.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "hkBhX5ABjk/tmp/8cbcf6f5ec4134dbbb9e03185d0af1b2edb4d665946293b91d4b0855d0b2367b.jpg", "table_caption": ["Table 4: The comparisons of computational costs. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "hkBhX5ABjk/tmp/2981f215f4aabbe6c3a62ab3f0e8e54fad2d48c1e1453dd220d19af470a53ab8.jpg", "table_caption": ["Table 5: The common hyper-parameters in Madoc. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "exploration of domain parameters, we sample a series of domain parameters every 100 rollouts in the source domain. After calibration, we set the physics parameters with the means outputted by actors, and train SAC from scratch with default hyper-parameters. ", "page_idx": 17}, {"type": "text", "text": "Most experiments were conducted on a server outftited with a 13th Gen Intel(R) Core(TM) i9-13900K CPU, 2 NVIDIA RTX A5000 GPUs, and 125GB of RAM, running Ubuntu 22.04. We also conduct a comparison between Madoc and various offilne RL algorithms with regard to method complexities and computational costs on the hfctah-med-rep task of the D4RL benchmark, the results are presented in Tab. 3 and Tab. 4. Compared to the traditional pure offilne RL algorithm CQL, Madoc incorporates additional modules that lead to increased GPU memory cost. Furthermore, it encompasses three training stages, which consequently require more computational cost. Nevertheless, given the significant performance improvement, these extra expenditures are deemed justifiable. We list the default hyper-parameter settings for Madoc in Tab. 5. ", "page_idx": 17}, {"type": "image", "img_path": "hkBhX5ABjk/tmp/2eb73e1d713349f9c5b8ca83c29c1e32b218396ff9b0e7a2f94bb71713c40075.jpg", "img_caption": ["Figure 5: An illustration of environments used in our experiments. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "D Experiment Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "D.1 Extended Environment Descriptions ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We adopt the popular Gym-MuJoCo tasks [53] as our benchmarks used in the experiments. To investigate the performance of our method and baselines, we conduct experiments on four locomotion tasks shown in Fig. 5, including HalfCheetah, Hopper, Walker2d, Ant. ", "page_idx": 18}, {"type": "text", "text": "HalfCheetah is a 2D robotic simulation featuring 9 links and 8 joints, including two paws, designed to move forward rapidly by applying torque on 6 specific joints. The main objective of this robot centers on maximizing forward progression to earn positive rewards while minimizing backward movement to avoid penalties. ", "page_idx": 18}, {"type": "text", "text": "Hopper is designed to enhance the complexity with more state and control variables than traditional control settings, using a two-dimensional, one-legged hopper composed of a torso, thigh, leg, and foot. The overall aim is to propel the hopper forward by exerting torque on the three joints that link its four body sections. ", "page_idx": 18}, {"type": "text", "text": "Walker2d is a two-dimensional figure with two legs, including a torso, two thighs, two legs, and two feet, focusing on coordinated movement to advance. The objective is to utilize torque on six joints to synchronize the movement of all six body parts toward the desired direction. ", "page_idx": 18}, {"type": "text", "text": "Ant is a 3D model with a freely rotating torso and four two-linked legs, designed for coordinated forward movement. The aim is to maneuver the ant by applying torques on eight hinges to effectively control the movement of its nine parts toward the desired direction. ", "page_idx": 18}, {"type": "text", "text": "The number of groups $n$ for each task is 6, 4, 6, and 6, respectively. We verify in extended experiments (shown in Fig. 7(a)) that the performance results do not differ significantly as long as it is within a reasonable range. We report the initial ranges and ground truth value of each physics parameter in Tab. 6, Tab. 7, Tab. 8, and Tab. 9. ", "page_idx": 18}, {"type": "table", "img_path": "hkBhX5ABjk/tmp/b80e61e63fec9cddee514f2add83b5c6b94039365ecdf0b3dc8cd8cad1fcfe83.jpg", "table_caption": ["Table 6: The setting of HalfCheetah physics parameter at easy/normal/hard level. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "hkBhX5ABjk/tmp/3b3970c501b107bb592fc1161be90fccca75a945e988fb3fec03fd44f8929462.jpg", "table_caption": ["Table 7: The setting of Hopper physics parameter at easy/normal/hard level. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "hkBhX5ABjk/tmp/f8d3878a54857b0c67bdb2902dc2732ecf880f72f3a73f06cdee5f0003401b41.jpg", "table_caption": ["Table 8: The setting of Walker2d physics parameter at easy/normal/hard level."], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "hkBhX5ABjk/tmp/14c6ada48482a10ba0408d6847ebabe436c5d1c9e77a94bb385295b2d1c36f6c.jpg", "table_caption": ["Table 9: The setting of $A n t$ physics parameter at normal level. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "D.2 Baselines ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Here we introduce the baselines used in our experiments, including offilne domain calibration, hybrid offline-and-online RL, and pure offline RL algorithms. ", "page_idx": 20}, {"type": "text", "text": "DROPO [17] adapts a distribution of dynamics parameters to match an offilne dataset by employing a probabilistic distance measure, aimed at directly maximizing the likelihood of replicating real-world data within a simulation. Consequently, the simulator can be envisioned as a stochastic forward model, where the inherent randomness is attributed to variations in the scene\u2019s physical parameters. ", "page_idx": 20}, {"type": "text", "text": "DROID [16] harnesses human demonstrations to synchronize the simulator\u2019s trajectories with those observed in the real world, rather than relying on guesswork or exhaustive adjustments to establish the domain randomization (DR) range. This process helps in finding the most suitable range of parameters for the simulator, which can be formulated as a statistical model. Subsequently, this model can be sampled to inform the training process of RL agents. ", "page_idx": 20}, {"type": "text", "text": "OTED [51] is designed to autonomously learn a set of simulator parameters that align with a given offline dataset. Using the calibrated simulator, it proceeds to train a Reinforcement Learning (RL) agent using conventional online methods. An objective is formulated for the tuning of simulator parameters, aiming to minimize a divergence metric between the state-action distribution generated by the simulator and the provided target offline dataset. ", "page_idx": 20}, {"type": "text", "text": "H2O [62] presents a novel policy evaluation framework that is aware of dynamics, adaptively imposing penalties on Q-function training for simulated state-action pairs that exhibit significant dynamics discrepancies. At the same time, it permits learning from a predetermined dataset originating from the real world without direct interactions with it. ", "page_idx": 20}, {"type": "text", "text": "CQL [63] enhances the conventional Bellman error objective by adding an uncomplicated Q-value regularization term, which is easy to apply to most current deep Q-learning and actor-critic models. The objective of CQL is to overcome existing limitations by training a conservative Q-function, ensuring that the policy\u2019s expected value, as estimated by this Q-function, is a conservative estimate of its actual value, thus avoiding selecting OOD actions. ", "page_idx": 20}, {"type": "text", "text": "MOREC [52] acquires a dynamics reward function that can be generalized from offilne data. This reward function is then utilized as a transition filter within any offline Model-Based Reinforcement Learning (MBRL) approach. During the transition generation process, the dynamics model produces a set of possible transitions, from which the one with the highest dynamics reward value is chosen for selection and used for policy update. ", "page_idx": 20}, {"type": "text", "text": "E Additional Experiment Results ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "E.1 The Absolute Calibration Error ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We report the corresponding mean absolute calibration error of experiments in Sec. 5.2. As shown in Tab. 10 and Tab. 11, there are five methods performing domain transfer by tuning simulator parameters. It is apparent that the evolutionary algorithm-based methods, DROPO and DROID, result in huge calibration errors, which suggests their limited effectiveness within the realm of high-dimensional parameter spaces. Additionally, Madoc-S outperforms OTED, highlighting the more rational design ", "page_idx": 20}, {"type": "table", "img_path": "hkBhX5ABjk/tmp/df29face66cbd93a5ec9699053d2de58210738180604c2b114ffcc8e4d73d141.jpg", "table_caption": ["Table 10: The mean absolute calibration error on D4RL benchmark. We bold the lowest mean. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "hkBhX5ABjk/tmp/d4e1c0e0dcf70a0cb5d1fa3ff1f74bfd06f7ac27b6e24169a6f7e000be7ef8e0.jpg", "table_caption": ["Table 11: The mean absolute calibration error on NeoRL benchmark. We bold the lowest mean. "], "table_footnote": ["of our reward model. Madoc minimizes the mean absolute calibration errors in almost all tasks, which also serves as a basis for its efficient domain transfer. "], "page_idx": 21}, {"type": "text", "text": "E.2 Ablation Studies ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In order to further validate the effectiveness of the automatic grouping technique, as in Fig. 6, we implement a variant of Madoc, which is not equipped with the automatic grouping technique, i.e., there are $N$ agents each responsible for calibrating a single parameter. We set the same hyperparameters, and train both policies on the HalfCheetah tasks of the NeoRL benchmark. We can find the policy performance of Madoc significantly decreases without the use of the automatic grouping technique, underscoring the importance of this component. ", "page_idx": 21}, {"type": "text", "text": "Besides, we additionally consider independent learning methods and different value decomposition methods. Independent learning [41] treats each agent as an independent individual, optimizing each policy with shared rewards without considering the joint policy, has gained traction again due to their surprising performance in some domains [82]. Therefore, we have implemented the Madoc-ISAC algorithm for domain calibration. VDN [44] is a classic value decomposition method representing the global value function as a simple sum of individual value functions, and we denote the corresponding version as Madoc-VDN. We train these policies with the same hyper-parameters on the HalfCheetah tasks of the D4RL benchmark. The results, shown in Tab. 12, demonstrate that Madoc-ISAC and Madoc-VDN perform worse than Madoc on all three datasets. All physics parameters in the source domain are interrelated and cooperative. Consequently, the independent learning method overlooks the policy changes of other calibration agents, which leads to non-stationary problems and a decline in performance. The gap between Madoc-VDN and Madoc is small, we speculate the reason is domain calibration is essentially a bandit RL problem, where there is no state space (therefore, we do not compare with more complex value decomposition algorithms [45] either, as their implementation would be the same without the global state). The main purpose of value decomposition methods is to perform credit assignment and both methods can achieve this. This also reflects that our algorithm can be integrated with any existing MARL value decomposition methods. ", "page_idx": 21}, {"type": "image", "img_path": "hkBhX5ABjk/tmp/6d70b760696c55475e553bba40106dcf50ae29a1a48feebf4c8482bb144545d2.jpg", "img_caption": ["Figure 6: The average return of rollout steps in the source domain with or without the automatic grouping technique. "], "img_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "hkBhX5ABjk/tmp/8ce08e1016dcc40d78da5e37a3c9568a64cc47b6f0f79d76f4d25c99eabb4ea2.jpg", "table_caption": ["Table 12: Normalized average return of Madoc and its variants. The results are evaluated in the target domain and we bold the highest mean. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "E.3 Stability of the Experimental Results ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The performance results presented in Tab. 1 and Tab. 2 reveal that Madoc exhibits greater variance in certain task scenarios compared to offline RL algorithms. In this subsection, we explain the large variance and have designed corresponding adjustments to improve upon this instability. ", "page_idx": 22}, {"type": "text", "text": "Madoc has achieved a trade-off between high mean and low variance in return performance. On the one hand, Madoc requires online interaction with the source domain to search for the domain parameters that best match the offilne dataset. Consequently, the random seed significantly influences exploration and exploitation, leading to a larger variance for Madoc. In contrast, pure offline algorithms like CQL and MOREC learn on a fixed dataset in a conservative manner and do not need to explore. Therefore, they are less influenced by the random seed and have smaller variance. On the other hand, algorithms with lower variance, namely H2O, $\\mathrm{DR+BC}$ , CQL, and MOREC, obtain conservative policies by penalizing the value functions on OOD actions or directly constraining the policies against the behavior policies. As a result, their mean performances are also limited by the dataset. Our method has achieved a trade-off between high mean and low variance, attaining optimal performance compared to baselines in most scenarios. ", "page_idx": 22}, {"type": "text", "text": "Regarding the large variance problem of Madoc, we have made some improvements. Once domain calibration is completed, we no longer use pure SAC to train the policy on the source domain from scratch. Instead, we combine SAC with BC to impose appropriate constraints on the learned policy, referred to as Madoc+BC. The results are shown in the Tab. 13. We can observe that the mean performance of Madoc $\\mathrel{\\+}\\mathrm{BC}$ decreased slightly but became more stable, confirming our approach. ", "page_idx": 22}, {"type": "text", "text": "E.4 Sensitivity of Hyper-parameters ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "By utilizing the automatic grouping technique, Madoc clusters physical parameters into several groups; thus, we investigate the impact of the number of groups $n$ on algorithm performance in the hfctah-med-rep task. As illustrated in Fig. 7(a), the best performance is achieved when the number of groups is 6; however, the impact is not significant as long as the value is within an appropriate range. ", "page_idx": 22}, {"type": "text", "text": "Table 13: Normalized average return of Madoc and its variants. The results are evaluated in the target domain and we bold the highest mean. ", "page_idx": 23}, {"type": "image", "img_path": "hkBhX5ABjk/tmp/ee44f4744eee8eaed5fa966825564d1c61627d4fa3ba116b8dbc7eafdbd700ca.jpg", "img_caption": ["Figure 7: Sensitivity of hyper-parameters. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Therefore, we default to setting 6 as the number of groups for the three tasks HalfCheetah, Walker2d, $A n t$ , and for the Hopper task, which has fewer domain parameters, we choose 4 as the number of groups. Additionally, for the $\\alpha$ hyper-parameter that affects the entropy of the calibration policy during the domain calibration process, we also design experiments on this task to verify parameter sensitivity. Fig. 7(b) demonstrates that $\\alpha=0.05$ is the best choice, values that are too large or too small will both lead to reduced search efficiency, affecting the final policies\u2019 performance. We also set this hyper-parameter to the same value for all experiments, and it showcases stable results. ", "page_idx": 23}, {"type": "image", "img_path": "hkBhX5ABjk/tmp/4c41fd0b9998eaf8499c935c188b3253d502b2bc19ddd02ff877ab0ef3994f37.jpg", "img_caption": ["Figure 8: More results about the generalization ability of Madoc compared to the baselines under different conditions. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "E.5 More Generalization Results ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In Sec. 5.4, we have already verified the generalization capability of Madoc across different datasets and under different search spaces. Here, we list the comparative results with more baseline algorithms. As shown in Fig. 8, the pure offline RL algorithm CQL is inferior to Madoc across various datasets, highlighting the limitations of conservative algorithms; DROID employs an evolutionary algorithm for parameter optimization and can achieve performance comparable to Madoc in simple cases, but collapses in complex ones. Madoc, on the other hand, can stably handle different scenarios and achieve excellent performance, supporting its generalization stability. ", "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We clearly state the main claims in the abstract and introduction, which accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We discuss the possible limitations in Sec. 6. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: All assumptions and proofs are clearly stated in Sec. 3 and Sec. 4. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We present the implementation details in Sec. 5 and App. C. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We provide the code at https://github.com/LAMDA-RL/Madoc. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We present the experimental setting in App. C and App. D. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: The results and accompanied by error bars and confidence intervals. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provide sufficient information in App. C. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: I have read and followed the ethics guidelines. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 27}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 28}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: I cite the origin codebase in the last paragraph in App. C. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provide the code in the supplementary material, and document our method in a README. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]