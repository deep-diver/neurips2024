[{"figure_path": "H6C4p8Dir7/figures/figures_2_1.jpg", "caption": "Figure 1: Architecture of OmniTokenizer, which consists of patch embedding layers, and separate spatial-temporal attention blocks. To obtain the latent representations, OmniTokenizer-VQVAE looks up a codebook to quantize the encoder embeddings, while OmniTokenizer-VAE samples from a Gaussian distribution. We omit the decoder and only show the tokenization process.", "description": "This figure illustrates the architecture of OmniTokenizer, a transformer-based tokenizer for joint image and video data.  It shows how image and video patches are processed separately through embedding layers and then fed into spatial and temporal transformer layers.  The output is then processed by two different tokenizers: an LM tokenizer using a codebook and a diffusion tokenizer sampling from a Gaussian distribution. The decoder is omitted for simplicity, focusing solely on the tokenization aspect.", "section": "3 Methodology"}, {"figure_path": "H6C4p8Dir7/figures/figures_3_1.jpg", "caption": "Figure 2: Illustration of the proposed progressive training paradigm. With this, OmniTokenizer could tokenize both image and video inputs with the same architecture and weight.", "description": "The figure illustrates the progressive training strategy used in OmniTokenizer.  It shows that the model is first trained on image data alone (using existing image tokenizers like VQGAN or VIT-VQGAN as a starting point) to develop its spatial understanding.  Then, it is jointly trained on both image and video data (potentially using methods like TATS or MAGVITv2 as video training initializations) to learn temporal dynamics. The result is a unified model (OmniTokenizer) capable of processing both image and video data using the same architecture and weights, demonstrating the synergy between the two modalities.", "section": "3.1.2 Progressive Training"}, {"figure_path": "H6C4p8Dir7/figures/figures_7_1.jpg", "caption": "Figure 3: Image and video reconstruction results of VQGAN [12], TATS [13], and our method.", "description": "This figure compares the image and video reconstruction capabilities of three different methods: VQGAN, TATS, and the proposed OmniTokenizer method.  The top row shows ground truth (GT) images and videos. The second row shows the results from VQGAN, the third row shows the results from TATS, and the bottom row shows the results from OmniTokenizer.  The red boxes highlight specific regions of the images and videos to better illustrate the differences in reconstruction quality between the methods.  The figure visually demonstrates the superior reconstruction performance achieved by the OmniTokenizer method compared to the existing state-of-the-art techniques.", "section": "4.5 Visualizations"}, {"figure_path": "H6C4p8Dir7/figures/figures_8_1.jpg", "caption": "Figure 3: Image and video reconstruction results of VQGAN [12], TATS [13], and our method.", "description": "This figure presents a comparison of image and video reconstruction results between three different methods: VQGAN, TATS, and the proposed OmniTokenizer method.  The images showcase the superior performance of OmniTokenizer, particularly in reconstructing images with intricate details, such as faces and text. This demonstrates the effectiveness of the OmniTokenizer method in capturing complex visual information.", "section": "4.5 Visualizations"}, {"figure_path": "H6C4p8Dir7/figures/figures_8_2.jpg", "caption": "Figure 1: Architecture of OmniTokenizer, which consists of patch embedding layers, and separate spatial-temporal attention blocks. To obtain the latent representations, OmniTokenizer-VQVAE looks up a codebook to quantize the encoder embeddings, while OmniTokenizer-VAE samples from a Gaussian distribution. We omit the decoder and only show the tokenization process.", "description": "This figure presents the architecture of OmniTokenizer, a transformer-based tokenizer for joint image and video processing.  It shows two branches: one for OmniTokenizer-VQVAE (using a codebook for quantization) and one for OmniTokenizer-VAE (sampling from a Gaussian distribution). Both branches utilize patch embedding layers followed by spatial and temporal transformer layers to process image and video data, respectively. The decoder is omitted for simplicity, focusing only on the tokenization aspect.", "section": "3.1 Joint Image and Video Tokenization"}, {"figure_path": "H6C4p8Dir7/figures/figures_8_3.jpg", "caption": "Figure 8: Visualization of the frame prediction results by OmniTokenizer. The frames marked in red are given during inference, while the following frames are generated.", "description": "This figure visualizes the frame prediction capabilities of OmniTokenizer.  The model is given a short sequence of frames (marked in red), and it predicts the subsequent frames.  This demonstrates the model's ability to extrapolate motion and temporal coherence, hinting at its potential for generating longer video sequences.", "section": "Visualizations"}, {"figure_path": "H6C4p8Dir7/figures/figures_9_1.jpg", "caption": "Figure 7: Unconditional UCF-101 generation using diffusion models (and OmniTokenizer-VAE).", "description": "This figure shows several video sequences generated by diffusion models using OmniTokenizer-VAE for unconditional generation on the UCF-101 dataset.  Each row represents a different generated video, demonstrating the model's ability to produce various actions without specific prompts. The videos display a range of actions, showcasing the diversity achieved by the model.", "section": "4.3 Visual Generation with Diffusion Models"}]