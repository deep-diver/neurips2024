[{"figure_path": "A34sBX4R5N/tables/tables_6_1.jpg", "caption": "Table 1: Comparison with state-of-the-art methods on Market-Sketch-1K dataset. Both training and testing set uses all sketches. 'S' and 'M' represent single-query and multi-query, respectively. 'Backbone' refers to network structure used by each method, mainly including: ResNet50 [50] and CLIP [40]. Bold values represent the optimal results.", "description": "This table compares the performance of the proposed OLTM model against several state-of-the-art methods on the Market-Sketch-1K dataset for sketch re-identification.  It shows the mean average precision (mAP) and top-k ranking accuracy (Rank@k) for both single-query and multi-query retrieval settings.  The 'Backbone' column indicates the underlying architecture (ResNet50 or CLIP) used by each method.  The best performance for each metric is highlighted in bold.", "section": "5.2 Performance on Market-Sketch-1K"}, {"figure_path": "A34sBX4R5N/tables/tables_7_1.jpg", "caption": "Table 2: Comparison with state-of-the-art methods on PKU-Sketch dataset. \u2018Backbone\u2019 includes GoogleNet [62], VGG-16 [63], ResNet50, ViT [64], and CLIP. \u2018-\u2019 denotes the unavailable results. \u2018\u2020\u2019 indicates that we reproduce UNIReID results following our training configuration.", "description": "This table compares the performance of the proposed OLTM model against other state-of-the-art methods on the PKU-Sketch dataset.  It shows the mean average precision (mAP) and rank@k (k=1, 5, 10) metrics. The different methods use various backbones (e.g., GoogleNet, VGG-16, ViT, CLIP), and the table highlights the superior performance of the proposed method.", "section": "5.2 Performance on PKU-Sketch"}, {"figure_path": "A34sBX4R5N/tables/tables_8_1.jpg", "caption": "Table 3: Ablation studies on Market-Sketch-1K dataset. Training and testing are under the multi-query setting. \"Handcrafted\" and \"VQA\" denote manually annotated and VQA generated text attributes, respectively. \"Template\" represents the sentence template defined by experts. \"Prompt\" denotes the learnable text prompts. The 'Baseline' uses an image encoder to process both modalities and employs simple cross-attention to integrate the global features. 'Lhtl' [67] represents the hard triplet loss. Bold values represent the optimal results.", "description": "This ablation study on the Market-Sketch-1K dataset evaluates the impact of different components of the proposed OLTM model on its performance under a multi-query setting.  It compares various text prompt generation strategies (handcrafted, VQA, template, learnable prompts) and the effects of including the Text-injected Coarse-grained Alignment Module (TCA) and Consensus-guided Fine-grained Interaction Module (CFI).  Different loss functions (identity loss, hard triplet loss, and the proposed triplet assignment loss) are also compared.  The results highlight the contribution of each module and the effectiveness of the proposed text prompt modeling and loss function.", "section": "5.3 Ablation Study"}, {"figure_path": "A34sBX4R5N/tables/tables_9_1.jpg", "caption": "Table 4: Performance of TAL Ltal with various baselines. \u2018+\u2019 represents WRT; \u2018*\u2019 represents HTL Chtl.", "description": "This table compares the performance of the proposed Triplet Assignment Loss (TAL) against existing methods, namely weighted regularization triplet loss (WRT) and hard triplet loss (HTL).  It shows the mAP and Rank@1 metrics for different baseline methods (BDG, baseline) with and without the TAL. The results highlight the improvement achieved by incorporating TAL.", "section": "5.3 Ablation Study"}, {"figure_path": "A34sBX4R5N/tables/tables_17_1.jpg", "caption": "Table 5: The number of parameters, FLOPs, and FPS of different methods, where bold indicates the best performance in this field and underline indicates the second-best performance. VI denotes visible-infrared person re-identification.", "description": "This table compares several methods for person re-identification, showing the number of parameters, floating point operations (FLOPs), and frames per second (FPS).  It highlights the best and second-best performers in each category. VI refers to visible-infrared person re-identification, and the methods include those using ResNet50 and ViT backbones, along with the proposed OLTM method.", "section": "5.2 Comparison with State-of-the-Art Methods"}, {"figure_path": "A34sBX4R5N/tables/tables_18_1.jpg", "caption": "Table 1: Comparison with state-of-the-art methods on Market-Sketch-1K dataset. Both training and testing set uses all sketches. 'S' and 'M' represent single-query and multi-query, respectively. 'Backbone' refers to network structure used by each method, mainly including: ResNet50 [50] and CLIP [40]. Bold values represent the optimal results.", "description": "This table compares the performance of the proposed OLTM model against several state-of-the-art methods on the Market-Sketch-1K dataset for sketch re-identification.  It shows the mean average precision (mAP) and ranking at different levels (Rank@1, Rank@5, Rank@10) for both single-query and multi-query settings. The backbone network architecture used by each method is also specified (ResNet50 or CLIP), highlighting the impact of the network architecture on the results. The best-performing method for each metric is indicated in bold.", "section": "5.2 Performance on Market-Sketch-1K"}, {"figure_path": "A34sBX4R5N/tables/tables_18_2.jpg", "caption": "Table 7: Performance comparison of different multi-query experimental methods.", "description": "This table compares the performance of three different multi-query fusion methods: Simple Fusion, Average Pooling, and Non-local Attention.  The results show that the Simple Fusion method achieves the best performance, as measured by mAP, Rank@1, Rank@5, and Rank@10.  Multi-query settings involve combining multiple sketches of the same person during both training and inference.", "section": "5.2 Comparison with State-of-the-Art Methods"}, {"figure_path": "A34sBX4R5N/tables/tables_19_1.jpg", "caption": "Table 8: Comparison results of our method on visible-infrared datasets, namely SYSU-MM01 and RegDB. Market-Sketch-1K is a sketch dataset used for reference. \"VI\" and \"Sketch\" represent their respective task domains.", "description": "This table compares the performance of the proposed OLTM model against other state-of-the-art methods on three different datasets: SYSU-MM01, RegDB (both visible-infrared person re-identification datasets), and Market-Sketch-1K (sketch-based person re-identification).  The results are presented in terms of Rank-1 accuracy and mean Average Precision (mAP) for both \"All Search\" and \"Indoor Search\" scenarios on SYSU-MM01, and for \"VIS to IR\" and \"IR to VIS\" on RegDB.  Finally, it presents the same metrics for the \"Sketch to VIS\" task on Market-Sketch-1K.", "section": "5.2 Comparison with State-of-the-Art Methods"}, {"figure_path": "A34sBX4R5N/tables/tables_20_1.jpg", "caption": "Table 9: Performance comparison of different VQA models.", "description": "This table compares the performance of three different Visual Question Answering (VQA) models used in the paper to generate textual attributes for images.  The models are compared based on mean Average Precision (mAP) and ranking metrics (R@1, R@5, R@10).  The results show that the VILT model achieves slightly better results than BLIP and GIT models.", "section": "E Analysis of VQA Models"}, {"figure_path": "A34sBX4R5N/tables/tables_21_1.jpg", "caption": "Table 3: Ablation studies on Market-Sketch-1K dataset. Training and testing are under the multi-query setting. \"Handcrafted\" and \"VQA\" denote manually annotated and VQA generated text attributes, respectively. \"Template\" represents the sentence template defined by experts. \"Prompt\" denotes the learnable text prompts. The 'Baseline' uses an image encoder to process both modalities and employs simple cross-attention to integrate the global features. 'Lhtl' [67] represents the hard triplet loss. Bold values represent the optimal results.", "description": "This ablation study on the Market-Sketch-1K dataset investigates the impact of different components of the proposed OLTM model on its performance under the multi-query setting.  It examines the effects of using handcrafted versus VQA-generated text attributes, the use of learnable prompts versus fixed templates, and the contributions of the text-injected coarse-grained alignment module (TCA) and the consensus-guided fine-grained interaction module (CFI).  The results show the improvements achieved by each component and highlight the effectiveness of the overall model architecture.", "section": "5.3 Ablation Study"}]