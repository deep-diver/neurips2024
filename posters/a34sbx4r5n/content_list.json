[{"type": "text", "text": "Optimal Transport-based Labor-free Text Prompt Modeling for Sketch Re-identification ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Rui Li,\u2217 Tingting Ren,\u2217 Jie Wen,\u2020 Jinxing Li\u2020 ", "page_idx": 0}, {"type": "text", "text": "School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen lrhit@stu.hit.edu.cn, {rentt0410,lijinxing158}@gmail.com, jiewen_pr@126.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sketch Re-identification (Sketch Re-ID), which aims to retrieve target person from an image gallery based on a sketch query, is crucial for criminal investigation, law enforcement, and missing person searches. Existing methods aim to alleviate the modality gap by employing semantic metrics constraints or auxiliary modal guidance. However, they incur expensive labor costs and inevitably omit fine-grained modality-consistent information due to the abstraction of sketches. To address this issue, this paper proposes a novel Optimal Transport-based Labor-free Text Prompt Modeling (OLTM) network, which hierarchically extracts coarse- and fine-grained similarity representations guided by textual semantic information without any additional annotations. Specifically, multiple target attributes are flexibly obtained by a pre-trained visual question answering (VQA) model. Subsequently, a text prompt reasoning module employs learnable prompt strategy and optimal transport algorithm to extract discriminative global and local text representations, which serve as a bridge for hierarchical and multi-granularity modal alignment between sketch and image modalities. Additionally, instead of measuring the similarity of two samples by only computing their distance, a novel triplet assignment loss is further proposed, in which the whole data distribution also contributes to optimizing the inter/intra-class distances. Extensive experiments conducted on two public benchmarks consistently demonstrate the robustness and superiority of our OLTM over state-of-the-art methods. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "With the growing need for urban public safety, traditional person re-identification (Re-ID) methods [1, 2, 3] are gradually becoming inadequate for criminal investigations and missing person tracking, as the individuals of interest may not have been captured by surveillance cameras. To bolster social security management and combat criminal activities, sketch person re-identification (Sketch Re-ID), which utilizes eyewitness clues to draw professional sketches as queries and match target images in a photo gallery database, has received widespread attention from researchers and scholars [4, 5, 6], as shown in Fig. 1. Nonetheless, due to the considerable disparity in modal heterogeneity resulting from the varied sketch styles of different artists and the diverse postures of real pedestrians in monitoring, sketch Re-ID remains highly challenging and necessitates further exploration and investigation. ", "page_idx": 0}, {"type": "image", "img_path": "A34sBX4R5N/tmp/ae2e31a9ab4fe9d577eefceb19f81ddacfe91a0a5331244ff6b426fcd0f74a63.jpg", "img_caption": ["Figure 1: The illustration of sketch Re-ID. Different artists create sketches based on clues provided by witness to assist the police in identifying targets. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Giving the high generalization and inherent abstraction of person characteristics in sketches, the pedestrian features depicted in a single sketch (such as clothing and gender) may match multiple similar real images, as illustrated in Fig. 1. A viable solution is to leverage the inter-modality interaction within the feature space to achieve hard alignment. Such methods typically employ loss constraints to directly map different modalities into a generic latent space [4, 7, 8, 9]. However, this hard alignment manner may not fully capture the complex dependencies and correlations that exist within and across modalities. To compensate for the lack of details in the above manner, another branch is to introduce an intermediate modality to bridge two source modalities. For instance, [10, 11] generate simulated sketches through adversarial learning, but the generated sketches are inevitably corrupted with noise due to limited generation performance. Additionally, [7, 12] construct benchmarks that contain textual information to alleviate modal gap; [6] improves inference efficiency by introducing text only during the training process. Despite the fact that these additional texts do contribute to mitigating the modal gap, they are all manually labeled, requiring significant human labor in real-world applications. Moreover, existing text-guided methods [6, 7] only focus on global text embeddings as masks, neglecting finer and richer local features. Therefore, this paper aims to address two key challenges: i) developing sufficient textual information as a transition mechanism without incurring additional costs, and ii) further exploring fine-grained discriminative information for multi-granularity interaction. ", "page_idx": 1}, {"type": "text", "text": "To address the above issues, we propose the Optimal Transport-based Labor-free Text Prompt Modeling (OLTM) framework, which implicitly incorporates text semantic information during training, facilitating hierarchical and multi-granularity modal alignment. In particular, OLTM is composed of three main components: i) text prompt reasoning (TPR); ii) text-injected coarse-grained alignment module (TCA); iii) consensus-guided fine-grained interaction module (CFI). On the one hand, to introduce text sequences without additional manual annotation, we dynamically transfer pre-trained language-visual knowledge into the downstream task. Specifically, TPR first generates multi-dimensional person attributes based on real images with a pre-trained visual question answering (VQA) model. Then, these attributes are inserted as fixed parts into learnable prompts to obtain the textual embedding representations. TCA integrates global parts of the embeddings to achieve the coarse-grained alignment across modalities. On the other hand, to explore fine-grained information, we employ optimal transport theory to enhance deep-level interaction. Concretely, TPR formulates the mapping from local parts of the textual embeddings to more discriminative feature representations, i.e., consensus, as an optimal transport problem. Subsequently, guided by consensus, CFI selectively focuses on key details, extracting fine-grained conceptual representations for sketch-ID. In addition, due to the significant heterogeneity gap between modalities, using Euclidean distance as a sole metric to measure feature similarity is inadequate. Thus, we propose a triplet assignment loss to optimize feature distance measurement and improve model performance. Extensive experiments are conducted on two challenging benchmarks, which demonstrate the favorable comparison of OLTM with other state-of-the-art methods. ", "page_idx": 1}, {"type": "text", "text": "The core contributions of this work are summarized as follows: (1) This paper proposes a novel optimal transport-based labor-free text prompt modeling framework for sketch Re-ID. To our best knowledge, this is the first attempt to apply VQA-generated text responses as a means to achieve modal alignment in sketch Re-ID without any additional annotations. (2) A novel text prompt reasoning module is deployed to dynamically extract global textual embeddings and discriminative fine-grained consensus, which guide the hierarchical multi-granularity alignment module in injecting semantic knowledge into the modeling process. (3) A new triplet assignment loss is proposed, which optimizes inter-/intra-class distance by considering overall data distribution information. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Sketch Re-identification As an important part of public safety guarantee, sketch Re-ID is a novel and challenging task that aims to match a person image with given professional sketches. Existing sketch Re-ID methods could be roughly classified into two groups according to their interaction modes, i.e., hard alignment methods [4, 8, 9] and soft alignment methods [12, 10, 6]. The former try to learn modality embeddings in a common latent space by employing some modality interaction operations or semantic metrics. Pang et al. [4] pioneered a sketch-photo benchmark and introduced cross-domain adversarial learning to narrow the feature gap. Zhang et al. [9] proposed an advanced cross-modal learning mechanism for handling non-corresponding information between modalities. However, due to significant differences between modalities, this direct alignment paradigm inevitably loses fine-grained modality-specific cues [13]. Hence, some of the latter methods investigate gentler alignment techniques through transitional modality. For example, Chen et al. [10] designed a dynamic updatable auxiliary sketch modality to increases the diversity of training samples; Zhai et al. [12] introduced a multi-modal Re-ID task by combining text and sketch as query for retrieval, exploiting their complementary advantages. Obviously, auxiliary modalities lacking detailed information may introduce noise, while data annotation is a labor-intensive task. In this paper, we first attempt to use text attributes generated by a reasonable VQA model as guidance for achieving multi-granularity alignment across modalities in sketch Re-ID. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Optimal Transport For optimizing the moving cost between distributions, Optimal Transport (OT) was first proposed by Kantorovich [14], which has shown significant potential in machine learning and computer vision, e.g., domain adaptation [15, 16, 17], learning with noisy labels [18, 19], and feature matching [20, 21]. Zhang et al. [22] incorporated OT into the re-ranking phase of image retrieval, significantly improving accuracy and efficiency. Similarly, Sergio et al. [23] first applied OT in visual place recognition and introduced a novel local feature aggregation method. In semi-supervised person Re-ID, OT often achieves the mapping between pseudo labels and classes as a classifier [24, 25]. In addition, Ling et al. [26] designed a assignment strategy for alleviating the intra-identity variations; Wasserstein distance was used to rectify the original global distance between samples and provides aligned distance estimation for local features [27]. Considering the enormous potential of OT in feature aggregation and distribution mapping, our study adopts OT to assist fine-grained alignment between modalities and guide the model in extracting the overall sample distribution pattern. ", "page_idx": 2}, {"type": "text", "text": "Prompt Learning Prompt learning initially garnered widespread attention and extensive research in natural language processing [28, 29, 30], which has gradually demonstrated significant potential in vision-language (V-L) models [31, 32, 33] and pure vision models [34, 35, 36]. Prompt learning provides a flexible way to adapt pre-trained models to downstream tasks by training only additional parameters. This enables prompts to capture task-specific information while guiding the fixed model\u2019s performance [37, 38]. In sketch-based image retrieval, [39] innovatively learns a unified prompt for different branches in CLIP\u2019s [40] visual encoding layer, fully exploiting CLIP\u2019s zero-shot learning potential. In text-to-image person Re-ID, [41] introduces a multi-prompt strategy to integrate text prompts from various sources for fine-grained interaction. Furthermore, Li et al. [42] first attempt to conduct in-depth research on zero-shot multi-modal ReID through a large foundational model. In this paper, we delve into the significant application of prompt learning in sketch Re-ID, innovatively generating global text representations by integrating fixed and learnable prompts, and utilizing OT to reason consensus for effectively guiding detailed interactions across modalities. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Problem Statement ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To ensure clarity, we represent the gallery containing $m$ images $I$ as $\\mathcal{G}=\\{I_{i},y_{i}\\}_{i=1}^{m}$ and the query set containing $n$ sketches $S$ as $\\bar{S}=\\{S_{j},{y_{j}}\\}_{j=1}^{n}$ , where $y\\in\\{1,\\ldots,C\\}$ are the identity labels for $C$ distinct pedestrian entities. Notably, each entity may include multiple images and sketches. The goal of Sketch Re-ID is to retrieve pedestrian images from the gallery $\\mathcal{G}$ that match one or multiple given sketch. Like [6], there exist two types of query methods: single sketch query and multiple sketches query. This section will use single sketch query as an example, and the same applies to multiple sketches query. Formally, we define a matching function $\\mathcal{M}:\\mathcal{S}\\times\\mathcal{G}\\rightarrow\\mathbb{R}^{n\\times m}$ that assigns a similarity score to each pair $(I_{i},S_{j})$ . The objective is to learn a function $\\mathcal{M}$ such that for any sketch $S_{j}$ and image $I_{i}$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\boldsymbol{\\mathcal{M}}(I_{i},S_{j})>\\boldsymbol{\\mathcal{M}}(I_{k},S_{j})\\quad i\\boldsymbol{f}\\quad y_{i}=y_{j}~a n d~y_{k}\\neq y_{j},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $I_{i}$ and $I_{k}$ are images from the gallery set, and $y_{i}$ and $y_{k}$ are their respective identity labels. ", "page_idx": 2}, {"type": "text", "text": "3.2 Optimal Transport ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Optimal Transport is a mathematical theory that focuses on finding an efficient solution between two probability distributions, minimizing the cost of transporting one distribution into another. We briefly review the theoretical derivation of optimal transport. Let $\\Gamma_{r}:=\\{\\pmb{x}\\in\\mathbb{R}_{+}^{r}|\\pmb{x}^{\\top}\\mathbf{1}_{r}=1\\}$ represents ", "page_idx": 2}, {"type": "image", "img_path": "A34sBX4R5N/tmp/ac0947561447fb206f6576eb6a9af50b59914eb063358efe1bcc6c3928fd7d89.jpg", "img_caption": ["Figure 2: Overview of our proposed OLTM network. Our model includes four main parts, i.e., text prompt reasoning (TPR), text-injected coarse-grained alignment Module (TCA), consensus-guided fine-grained interaction module (CFI) and triplet assignment loss (TAL). Specifically, TPR flexibly generates target characteristics through VQA, and combines prompt learning and optimal transport to reason text global embedding and local consensus. TCA and CFI extract modality-specific representations from image and sketch modalities to achieve hierarchical and multi-granularity alignment. Finally, TAL is designed to optimize distance measurement between samples and improve the model\u2019s capacity to capture local relationships. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "the probability simplex, where ${\\mathbf{1}}_{r}$ is the $r$ -dimensional vector of ones. Given two probability simplex vectors $\\alpha\\in\\Gamma_{m}$ and $\\beta\\in\\Gamma_{n}$ and a cost matrix $C\\in\\mathbb{R}^{m\\times n}$ , the objective of $\\mathrm{OT}$ is to seek the optimal transport plan $P^{*}$ mapping $_{\\alpha}$ to $\\beta$ at the minimum cost: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle{d_{C}(\\alpha,\\beta)=\\operatorname*{min}_{P\\in U(\\alpha,\\beta)}\\langle C,P\\rangle,}}\\\\ {\\displaystyle{U(\\alpha,\\beta)=\\left\\{P\\in\\mathbb{R}_{+}^{m\\times n}\\mid P\\mathbf{1}_{n}=\\alpha,P^{\\top}\\mathbf{1}_{m}=\\beta\\right\\},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $U(\\pmb{\\alpha},\\beta)$ denotes the transport polytope of $_{\\alpha}$ and $\\beta$ , i.e., the solution space of $_{P}$ . The above problem is to find optimal solution $P^{*}$ in a set of all possible joint probabilities of $(X,Y)$ , where $X$ and $Y$ represent random variables with marginal distribution $_{\\alpha}$ and $\\beta$ . ", "page_idx": 3}, {"type": "text", "text": "Eq. 8 indicates that OT is a linear programming problem which is theoretically solvable in polynomial time, but its complexity becomes prohibitively high as the feature dimension increases [43]. To this end, Sinkhorn algorithm [44] adopts an iterative strategy to obtain the optimal solution $P^{*}=$ $D i a g(\\pmb{u})\\pmb{K}D i a g(\\pmb{v})$ with near-square complexity [45]. $\\textbf{\\em u}$ and $\\pmb{v}$ can be solved through alternately iterating the following two equations: $\\pmb{u}^{(z)}\\,=\\,\\pmb{\\alpha}/(K\\pmb{v}^{(z-1)})$ and $\\pmb{v}^{(z)}\\,=\\,\\beta/(K^{\\top}\\pmb{u}^{(z)})$ , where $K=e x p(C/\\epsilon)$ , $\\epsilon$ is the regularization coefficient and $z$ is the iterations (cf. Appendix). Since this method integrates the importance of all features when solving the optimal solution, it can analyze the overall data distribution. ", "page_idx": 3}, {"type": "text", "text": "4 The Proposed Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "4.1 Overall Architecture ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Fig. 2 provides an overview of the OLTM architecture. The image and text encoders discussed in this paper are based on CLIP [40], and any language-visual model utilizing a Transformer architecture may also be employed. Notably, the image encoders utilized for both images and sketches employ shared weights to ensure the mapping of features into a unified semantic space. For an input RGB image, we obtain embeddings $\\overset{\\cdot}{R^{\\prime}}=\\ \\left\\{R_{\\mathrm{cls}},r_{1},\\ldots,r_{p}\\right\\}\\;\\in\\;\\mathbb{R}^{(p+1)\\times d}$ through the image encoder, where $p$ is the number of non-overlapping patches, $R_{\\mathrm{cls}}$ and $R_{\\mathrm{local}}\\,=\\,\\{r_{i}\\}_{i=1}^{p}$ represent $d$ -dimensional global and local features, respectively. Similarly, the embeddings of a sketch can be represented as $S=\\{S_{c l s},S_{l o c a l}\\}=\\{S_{\\mathrm{cls}},s_{1},\\ldots,s_{p}\\}\\in\\mathbb{R}^{(p+1)\\times d}$ . Firstly, to provide reasonable text semantic guidance, Text Prompt Reasoning (TPR) generates attribute descriptions about pedestrians based on the RGB image, and obtains the textual embeddings $\\mathbf{\\bar{\\calT}}=\\{T_{e o s},\\mathbf{\\bar{\\calT}}_{l o c a l}\\}$ through prompt learning. Then, TPR extracts the fine-grained consensus $\\mathbf{\\deltaX}$ from $T_{l o c a l}$ through clustering. Subsequently, Textinjected Coarse-grained Alignment (TCA) module embeds global contextual information ${\\mathbf{}}T_{e o s}$ into visual features $R_{\\mathrm{cls}}$ and $S_{\\mathrm{cls}}$ . Meanwhile, Consensus-guided Fine-grained Interaction (CFI) module utilizes $\\mathbf{\\deltaX}$ to address fine-grained semantic misalignment between $\\mathbf{\\delta}R_{l o c a l}$ and $S_{l o c a l}$ . Additionally, due to the significant differences between sketches and RGB images, Euclidean distance between independent samples may ignore the influence of overall sample distribution. Thus, we introduce a more comprehensive distance measurement method and propose triplet assignment loss $\\mathcal{L}_{t a l}$ . During training, all these modules will be jointly optimized through identity loss [46] $\\mathcal{L}_{i d}$ and $\\mathcal{L}_{t a l}$ : $\\mathcal{L}_{O L T M}=\\mathcal{L}_{i d}+\\eta\\mathcal{L}_{t a l}$ , where $\\eta$ is a scaling factor. During inference, only $R_{\\mathrm{cls}}$ and $S_{\\mathrm{cls}}$ are used to match queries for practical application requirements. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "4.2 Text Prompt Reasoning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Significant image differences and inherent abstract nature, cause semantic misalignment during knowledge acquisition, severely impacting model\u2019s reasoning and generalization capabilities. To address this issue, TPR introduces intermediate modality to guide alignment between modalities without additional costs. Moreover, TPR employs a dynamic consensus acquisition strategy to enhance the discriminative power of local text features. ", "page_idx": 4}, {"type": "text", "text": "Text Attribute Generation Sketches, unlike conventional Re-ID tasks, are vulnerable to subjective emotions and drawing skills of artists, leading to a lack of detailed information crucial for model learning. The text\u2019s objectivity and flexibility prompt the model to focus more on semantic contextual information during knowledge acquisition. However, directly generating a comprehensive textual description of pedestrian images inevitably introduces irrelevant noise, thereby reducing model performance. Therefore, we retain the advanced modeling capabilities of large-scale language-visual models for images as possible. Specifically, for a given RGB image, TPR utilizes a pre-trained visual question answering model to address $k$ specific details (cf. Appendix) and acquire corresponding descriptions for the target: $a t t=\\{a t t_{1},a t t_{2},\\ldots,a t t_{k}\\}$ . Importantly, this process introduces textual detail guidance during model training but excludes text-related components during inference. ", "page_idx": 4}, {"type": "text", "text": "Learnable Prompt Strategy Inspired by [47], we combine the learnable prompt with original text attributes, without incurring additional expert knowledge compared to the handcrafted prompt. Concretely, TPR initially transform these attributes into tokens through CLIP tokenizer, i.e., $\\textbf{\\em a}=$ Tokenizer $(a t t)$ . Then, $l$ learnable prompts $\\{p_{1},p_{2},\\ldots,p_{l}\\}$ are embedded into these fixed attributes tokens, forming the textual description: $\\mathbf{\\widehat{\\lambda}}\\mathbf{q}=\\left\\{p_{1},a_{1},p_{2},a_{2},\\ldots,p_{l},a_{k}\\right\\}$ . This integration introduces a dynamic knowledge learning mechanism that reduces noise introduction compared to handcrafted prompts, while enhancing the flexibility of modal interaction and transferability of text embeddings. Subsequently, the whole token $\\pmb q$ is fed into a frozen text encoder to generate text embeddings $\\pmb{T}=\\{\\pmb{T}_{\\mathrm{sos}},t_{1},t_{2},\\ldots,t_{n},\\pmb{T}_{\\mathrm{eos}}\\}\\,\\in\\,\\mathbb{R}^{(n+2)\\times d}$ , where $\\mathbf{\\calT}_{\\mathrm{sos}}$ and $T_{\\mathrm{eos}}$ denote the [SOS] and [EOS] token, $n$ is the number of $d$ -dimensional word tokens. Based on widely-used token selection, $T_{\\mathrm{eos}}$ serves as the global feature, while $T_{\\mathrm{local}}=\\{t_{j}\\}_{j=1}^{n}$ represents a sequence of basic local tokens. ", "page_idx": 4}, {"type": "text", "text": "Dynamic Consensus Acquisition To more effectively address fine-grained semantic variations (e.g., hats, shoes) across modalities, we explore methods to filter out non-informative features for enhancing the representational capacity of text embedding for detailed information. Therefore, based on local textual feature $\\mathbf{\\delta}T_{l o c a l}$ , we employ metric learning to draft a dynamic consensus acquisition strategy for capturing the discriminative prototypical representations $\\mathbf{\\deltaX}$ . ", "page_idx": 4}, {"type": "text", "text": "To begin, in order to adaptively learn related-task knowledge, local text representations $\\pmb{T}_{l o c a l}\\in\\mathbb{R}^{n\\times d}$ are fed into consensus multi-layer perceptron (ConMLP) blocks to achieve feature enhancement. Then, a prototypical descriptor is formed by assigning all enhanced features to a set of atoms. The cost matrix $\\pmb{C}\\tilde{\\in}\\ensuremath{\\mathbb{R}}_{+}^{n\\times m}$ can be calculated for assignment, where the $(i,j)$ -th element $C_{i,j}$ represents the cost of assigning a feature to an atom. In other words, $_{C}$ evaluates the affinity between the enhanced features and the prototypical atoms. Concretely, to introduce certain priors as guidance [23], the enhanced features are used to learn the cost matrix through two fully connected layers initialized randomly. In addition, some irrelevant information in textual features, such as those representing association, may disrupt the model\u2019s ability to learn the target details. Inspired by the solutions used in graph matching and key-point matching [20, 23], we set a learnable \"bin\" in $_{C}$ . Due to the differences between prior distributions, non-informative features can be assigned to it. Specifically, we extend the cost matrix from $_{C}$ to $\\bar{C}=[C,\\bar{c}]\\in\\mathbb{R}_{+}^{n\\times(m+1)}$ , and $\\bar{\\pmb{c}}=w\\mathbf{1}_{n}$ , where $w$ is a learnable parameter and $\\mathbf{1}_{n}=[1,\\ldots,1]^{\\top}\\in\\mathbb{R}^{n}$ represents $n$ -dimensional vector of ones. Following Sec. 3.2, we consider that assignment where the enhanced features\u2019 mass, $\\alpha=1_{n}$ , should be assigned to the atoms or the \"bin\", $\\bar{\\beta^{}}=[\\mathbf{1}_{m}^{\\top},n-m]^{\\top}$ , is an optimal transport problem: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{c}{d_{\\bar{C}}(\\alpha,\\beta)=\\underset{P\\in U(\\alpha,\\beta)}{\\operatorname*{min}}\\langle\\bar{C},P\\rangle,\\;}\\\\ {U(\\alpha,\\beta)=\\{P\\in\\mathbb{R}_{+}^{n\\times(m+1)}|P\\mathbf{1}_{m+1}=\\alpha,P^{\\top}\\mathbf{1}_{n}=\\beta\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The optimal assignment plan can be solved by Sinkhorn algorithm (cf. Appendix) through iterative strategy. For better descriptors quality, the extra \"bin\" is discarded to obtain the optimal assignment $P\\in\\bar{\\mathbb{R}}_{+}^{n\\times m}$ . Finally, the augmented comprehensive representation, i.e., consensus $\\pmb{X}\\in\\mathbb{R}^{\\breve{m}\\times d}$ , is obtained by aggregating enhanced textual features and optimal transport plan: $\\pmb{X}=\\pmb{P}^{\\top}\\pmb{T}_{l o c a l}$ . ", "page_idx": 5}, {"type": "text", "text": "4.3 Hierarchical and Multi-granularity Modal Alignment ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "TPR develops a robust transitional means to facilitate reasonable and efficient modal alignment. In order to leverage effectively the multi-granularity textual information, TCA utilizes global embedding to achieve coarse-grained modal interaction, while CFI optimizes fine-grained alignment based on enhanced comprehensive representation. ", "page_idx": 5}, {"type": "text", "text": "Text-injected Coarse-grained Alignment Module TCA utilizes transformer architecture, known for its efficiency in modeling long-distance dependencies, to capture global information, as shown in Fig. 2. Furthermore, we implement a cross-attention mechanism using global text embedding to emphasize contextual information injection at the beginning. For an input pair $(R,S)$ , cross-attention mechanism computes the query using text global representation $\\mathbf{\\Delta}T_{e o s}$ and derives the key and value from modality-specific global features ${\\cal R}_{c l s}$ and $S_{c l s}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\pmb{Q}_{R/S}=\\pmb{T}_{e o s}\\cdot\\pmb{w}^{Q},\\pmb{K}_{R/S}=(\\pmb{R}/S)_{c l s}\\cdot\\pmb{w}^{K},V_{R/S}=(\\pmb{R}/S)_{c l s}\\cdot\\pmb{w}^{V},}\\\\ &{}&{\\pmb{C A}(\\pmb{R}/S,\\pmb{T}_{e o s})=\\pmb{A t t e n t i o n}(\\pmb{Q}_{R/S},\\pmb{K}_{R/S},\\pmb{V}_{R/S}),\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $R/S$ signifies identical operations across both modalities; $\\pmb{w}^{Q}$ , $w^{K}$ and $w^{V}$ denote shared learnable parameters, while $Q_{R/S},K_{R/S}$ and $V_{R/S}$ represent query, key and value for either the RGB or sketch modality, respectively. After obtaining the fusing features that contain textual knowledge, TCA introduces the standard transformer blocks to refine modality-specific features. Finally, we can acquire the final global concept representations ${\\cal R}_{c l s}^{'}$ and $S_{c l s}^{'}$ for sketch Re-ID. ", "page_idx": 5}, {"type": "text", "text": "Consensus-guided Fine-grained Interaction Module Due to the inherent complexity of sketches and RGB images and the potential for semantic misalignment during learning, capturing detail variations in modalities is crucial. Fortunately, the fine-grained consensus $\\mathbf{\\deltaX}$ provided by TPR, which contains rich detail information, offers a key solution to this problem. CFI adopts a transformer structure based on multi-head cross attention, and it converts the original local feature $\\mathbf{\\delta}R_{l o c a l}$ and $S_{l o c a l}$ to more discriminative representations through for robust Re-ID: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\hat{Q}_{R/S}=X\\cdot w^{\\hat{Q}},\\hat{K}_{R/S}=(R/S)_{l o c a l}\\cdot w^{\\hat{K}},\\hat{V}_{R/S}=(R/S)_{l o c a l}\\cdot w^{\\hat{V}},}}\\\\ {{{\\cal H}e a d_{h}^{R/S}=A t t e n t i o n(\\hat{Q}_{R/S},\\hat{K}_{R/S},\\hat{V}_{R/S}),}}\\\\ {{{\\cal M}{\\cal H}(R/S,X)=C o n c a t({\\cal H}e a d_{1}^{R/S},\\dots,{\\cal H}e a d_{{\\cal H}}^{R/S}),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $w^{\\hat{Q}},w^{\\hat{K}}$ and $\\pmb{w}^{\\hat{V}}$ denote the parameters of project layers of $h$ -th head for both modalities. As a result, the text detail information about pedestrian characteristics can assist our model to address the problem caused by the diversity and uncertainty of sketches and RGB images. Eventually, the local concept representations $\\bar{R_{l o c a l}^{'}}$ and $\\boldsymbol{S}_{l o c a l}^{'}$ can be obtained. ", "page_idx": 5}, {"type": "text", "text": "4.4 Triplet Assignment Loss ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The triple loss, a commonly-used matching loss in cross-modal learning, performs well in performance through adjusting the distance of hardest negatives in scenarios like image-text matching [12, 48] and video-text retrieval [49]. However, this strategy independently trains each semantic part with equal contribution, disregarding the overall data distribution impact when multiple samples from different modalities exhibit slight differences. This oversight may lead to inaccurate estimation of sample distances and potentially result in sub-optimal local minima [27]. To this end, we propose a new triplet assignment loss (TAL) to establish a more rational measure for evaluating the proximity of local features. ", "page_idx": 5}, {"type": "table", "img_path": "A34sBX4R5N/tmp/0facc42e22a391c84707f006ab73f0fa5a4b657d891711cea33d0431408e61b4.jpg", "table_caption": ["Table 1: Comparison with state-of-the-art methods on Market-Sketch-1K dataset. Both training and testing set uses all sketches. \u2018S\u2019 and \u2018M\u2019 represent single-query and multi-query, respectively. \u2018Backbone\u2019 refers to network structure used by each method, mainly including: ResNet50 [50] and CLIP [40]. Bold values represent the optimal results. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "For an input positive pair $(R_{i},S_{i})$ in a mini-batch $x$ , the feature representations obtained through model inference are $\\pmb{R}_{i}^{'}$ and $\\boldsymbol{S}_{i}^{'}$ . If we treat the feature sets of all samples for each of two modalities in $x$ as two discrete distributions, their alignment can be considered an optimal transport problem. The cost matrix $\\hat{C}$ is derived from pairwise feature similarities: $\\hat{C}_{i,j}=[(\\bar{\\pmb{R}}_{i}^{'})^{\\top}{\\pmb{S}}_{j}^{'}]_{+}$ . We aim to acquire the optimal transport matrix $P^{*}$ with the least amount of cost, where $\\boldsymbol{P}_{i,j}^{*}$ represents the assignment weight of $(R_{i},S_{j})$ obtained after balancing the overall distribution. TAL can be represented based on triplet loss as the weighted sum of the original distance and the optimal assignment distance, dynamically updated at a certain rate $\\gamma$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{t a l}(R_{i},S_{i})=[m-D(R_{i},S_{i})+D(R_{i},\\hat{S}_{h})]_{+}+[m-D(R_{i},S_{i})+D(\\hat{R}_{h},S_{i})]_{+},}\\\\ &{\\qquad\\qquad\\qquad D(R_{i},S_{i})=\\gamma E(R_{i},S_{i})+(1-\\gamma)(1-P_{i,i}^{*})E(R_{i},S_{i})}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $[x]_{+}=\\operatorname*{max}(x,0)$ , $\\hat{\\pmb{R}}_{h}=a r g m a x_{R_{j}\\neq R_{i}}D({\\pmb R}_{j},S_{i})$ and $\\hat{\\pmb{S}}_{h}=a r g m a x_{S_{j}\\neq S_{i}}D(\\pmb{R}_{i},S_{j})$ are the most similar negatives in $x$ for $(R_{i},S_{i})$ , and $E(\\pmb{R}_{i},\\pmb{S}_{i})=\\|\\pmb{R}_{i}^{'}-\\pmb{S}_{i}^{'}\\|_{2}$ denotes the Euclidean distance between feature representations. ", "page_idx": 6}, {"type": "text", "text": "5 Experiment ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Experiment Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets Two publicly available benchmark datasets, namely PKU-Sketch [4] and Market-Sketch-1K [6], are utilized for performance evaluation. Both of them are sketched and annotated by professional artists. PKU-Sketch is the first publicly Sketch Re-ID dataset, containing data for 200 pedestrians, with each individual being represented through one sketch and two photos. In accordance with the setting of [4], we randomly select 150 identities for training and 50 for testing, and final results are derived from the average of 10 experimental runs. Market-Sketch-1K is a large-scale dataset derived from the Market-1501 [1], which is created by six artists based on descriptions, featuring multiple perspectives and artistic styles. The training set consists of 2,332 sketches and 12,936 photos, while the testing set comprises 2,375 sketches and 19,732 photos. Following the experimental setup in [6], our method will be evaluated in three settings: single-query, multi-query, and cross-style retrieval. ", "page_idx": 6}, {"type": "text", "text": "Table 2: Comparison with state-of-the-art methods on PKU-Sketch dataset. \u2018Backbone\u2019 includes GoogleNet [62], VGG-16 [63], ResNet50, ViT [64], and CLIP. \u2018-\u2019 denotes the unavailable results. \u2018\u2020\u2019 indicates that we reproduce UNIReID results following our training configuration. ", "page_idx": 7}, {"type": "table", "img_path": "A34sBX4R5N/tmp/78ee02bfca84fd2acb5e1c52faa369693e5071f8f5d865826546b65ce433139d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "A34sBX4R5N/tmp/52e67fab5292fb8fc494f642dcf0a71b9b1a1d2abda06e4aea4c102028affae6.jpg", "img_caption": ["Figure 3: The Rank-5 retrieval results on two datasets. For the Market-Sketch-1K dataset, both single-query and multi-query scenarios are presented. Green border indicates correctly retrieved target pedestrians, while yellow border indicates incorrectly matched pedestrians. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Evaluation Metrics In line with [4, 9, 6, 59], we use Rank- $k$ metrics $(k=1,5,10)$ and mean Average Precision (mAP) as evaluation metrics. The higher values of the above three metrics, the better performance. ", "page_idx": 7}, {"type": "text", "text": "Implementation Details OLTM uses a pre-trained CLIP-ViT-B/16 [40] as image encoder, and extracts text features with a pre-trained CLIP Text Transformer. Fine-grained text attributes are derived from a ViLT-based [60] VQA model. Importantly, our VQA model is replaceable. To avoid the cost overhead from multiple calls to VQA model during training and inference, text attribute generation is performed during the data processing stage. For multi-query scenarios, we input a weighted sum of multiple sketches. Input images are resized to $288\\times144$ , and augmented with random horizontal filpping and style augmentation [61]. More experimental configuration details are available in the supplementary materials. ", "page_idx": 7}, {"type": "text", "text": "5.2 Comparison with State-of-the-Art Methods ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Performance on Market-Sketch-1K. The experimental results for Market-Sketch-1K are shown in Tab. 1. OLTM significantly outperforms all state-of-the-art methods in both single-query and multi-query settings. In the single-query scenario, OLTM achieves an mAP of $38.35\\%$ and a Rank-1 of $36.75\\%$ , surpassing state-of-the-art method by $3.38\\%$ and $5.23\\%$ , respectively. In the multi-query scenario, OLTM achieves an mAP of $62.55\\%$ and a Rank-1 of $69.48\\%$ , exceeding state-of-the-art methods by $7.37\\%$ and $12.85\\%$ , respectively. This result is primarily due to the introduction of textual semantic information in the training process of OLTM, which implicitly guides images and sketches to focus on modality-invariant features. Through hierarchical and multi-granularity alignment, the model is able to uncover discriminative fine-grained information, leading to more ", "page_idx": 7}, {"type": "text", "text": "Table 3: Ablation studies on Market-Sketch-1K dataset. Training and testing are under the multi-query setting. \"Handcrafted\" and \"VQA\" denote manually annotated and VQA generated text attributes, respectively. \"Template\" represents the sentence template defined by experts. \"Prompt\" denotes the learnable text prompts. The \u2018Baseline\u2019 uses an image encoder to process both modalities and employs simple cross-attention to integrate the global features. $\\mathcal{L}_{\\mathrm{htl}}\\,:$ \u2019 [67] represents the hard triplet loss. Bold values represent the optimal results. ", "page_idx": 8}, {"type": "table", "img_path": "A34sBX4R5N/tmp/542de4b8b860557b21df7e0832e33e2e8193f59c534a6ce36dbe9c4eee0c3fec.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "accurate queries. The visualization results on Market-Sketch-1K are shown in Fig. 3. When sketch possesses clearly distinguishable features, query results are satisfactory. In contrast, when sketch closely resembles multiple images, making its features challenging to distinguish with the naked eye, the model encounters errors that are justifiable. In addition, to test the generalization capability of OLTM on unknown styles, cross-style retrieval evaluation is performed on Market-Sketch-1K, and detailed results are in supplementary material. ", "page_idx": 8}, {"type": "text", "text": "Performance on PKU-Sketch. Tab. 2 presents the model\u2019s performance on PKU-SKetch dataset. The results indicate that our OLTM outperforms all competitors by a significant margin. For example, mAP and Rank-1 of OLTM are remarkably high at 91.4 and 94.0, surpassing state-of-the-art method by $5.2\\%$ and $4.0\\%$ , respectively. Because the sketches on PKU-Sketch contain more detailed information, they can assist multi-granularity interaction in acquiring more fine-grained knowledge. Fig. 3 illustrates the top-5 visualization results of OLTM on PKU-Sketch. Our method can accurately identify the target pedestrians despite challenges such as variations in posture, viewpoint, occlusion, sketch abstraction, and different painting styles. ", "page_idx": 8}, {"type": "text", "text": "5.3 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, ablation experiments are conducted on Market-Sketch-1K to evaluate the effectiveness of each component within the OLTM framework. ", "page_idx": 8}, {"type": "text", "text": "The Effectiveness of Text Prompt Reasoning. To evaluate the contribution of different prompt setting, we train the model with different combination of each setting. As shown in Tab. 3, the combination of VQA and Prompt brings significant contribution. Compared to the strategy of directly aligning different modalities, leveraging TPR to implicitly guide the alignment results in improvements of $7.08\\%$ in mAP and $9.44\\%$ in rank-1. This significant improvement is primarily attributed to introducing textual information during the training phase, which enables the model to effectively capture semantic correlations between images and sketches during inference. Furthermore, compared to manually annotated fixed attributes, those generated by VQA enhance the model\u2019s performance by adaptively adjusting the level of detail on which it focuses. Moreover, the introduction of learnable prompts improved the mAP and rank-1 by $0.79\\%$ and $4.02\\%$ , respectively, compared to fixed templates. Prompt learning can enhance the network\u2019s learning and reasoning capabilities, allowing it to more flexibly adapt to diverse modalities and enhance its sensitivity to subtle distinctions. ", "page_idx": 8}, {"type": "text", "text": "The Effectiveness of Our Designed Modules. To validate the effectiveness of the TCA and CFI module, we progressively integrate them into the baseline and evaluate performance. The results in Tab. 3 indicate that both modules significantly enhance the alignment and interaction capabilities of model across modalities. Specifically, TCA improves mAP and rank-1 by $3.36\\%$ and $4.82\\%$ compared to baseline, respectively. The introduction of textual information in TCA effectively provides semantic guidance for coarse-grained alignment between modalities, enabling the model to focus more on similar semantic relationships. Furthermore, the integration of CFI has increased mAP and rank-1 by $1.45\\%$ and $3.82\\%$ , respectively. CFI selectively focuses on key regions in visual concept representations through semantic consensus. This process optimizes feature interaction and ensures capturing more detailed information at a fine-grained level. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "The Effectiveness of Triplet Assignment Loss. The experimental results in Tab. 3 demonstrate that the combination of proposed TAL $\\mathcal{L}_{\\mathrm{tal}}$ and identity loss $\\mathcal{L}_{\\mathrm{ID}}$ achieves optimal performance. The identity loss ensures that the model correctly identifies different individual identities, while TAL further optimizes feature space by pulling positive samples closer and pushing negative samples apart. Additionally, to verify the generalization of TAL, as shown in Tab. 4, we achieve superior performance across various network frameworks by substituting TAL for the weighted regularization triplet loss (WRT) [59] and hard triplet loss (HTL). Balancing between Euclidean distance and optimal transport distance can significantly enhance model performance. Please refer to the supplementary materials for more verification experiments on the key role of overall data distribution in enhancing sample feature distance. ", "page_idx": 9}, {"type": "table", "img_path": "A34sBX4R5N/tmp/975583b1a497a1f5dee30a96b6fc18caa46f40870ed53843f903c01298fb75a6.jpg", "table_caption": ["Table 4: Performance of TAL $\\mathcal{L}_{\\mathrm{tal}}$ with various baselines. $\"+\"$ represents WRT; \u2018\\*\u2019 represents HTL $\\mathcal{L}_{\\mathrm{htl}}$ . "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we present a optimal transport-based labor-free text prompt modeling (OLTM) framework for sketch re-identification. OLTM embeds text prompt reasoning module and distance measurement into transformer for achieving hierarchical multi-granularity alignment through the guidance of text semantics, leveraging the advantages of prompt learning and optimal transport. In addition, to address the limitations of Euclidean distance in measuring sample similarity, we propose a triplet assignment loss that guarantees a more effective standard based on the overall data distribution. Extensive experiments conducted on two public datasets indicate outstanding performance compared to other state-of-the-art methods for sketch Re-ID. ", "page_idx": 9}, {"type": "text", "text": "Discussion: In this work, we employ text injection and sample distance optimization to direct the model\u2019s attention toward key details, thereby minimizing performance losses due to modal gap and sample abstraction. However, our experiments revealed that when confronted with extremely vague sketch samples (i.e., those that human cannot discern features or match), the model\u2019s identification process deteriorates into a random selection among multiple potential outcomes. Therefore, sketch Re-ID heavily depend on the quality of sketches. Enhancing the discriminability of sketches without incurring additional labor costs is a topic worthy of further exploration in future research. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported in part by the National Natural Science Foundation of China (62272133), in part by the Shenzhen Science and Technology Program (KJZD20230923114600002), in part by the Shenzhen Colleges and Universities Stable Support Program (GXWD20220811170100001), in part by the Key Laboratory of Industrial Equipment Quality Big Data (2024-IEQBD-01), in part by Guangdong Basic and Applied Basic Research Foundation (2024A1515030213). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jingdong Wang, and Qi Tian. Scalable person re-identification: A benchmark. In Proceedings of the IEEE international conference on computer vision, pages 1116\u20131124, 2015.   \n[2] Hao Luo, Youzhi Gu, Xingyu Liao, Shenqi Lai, and Wei Jiang. Bag of tricks and a strong baseline for deep person re-identification. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 0\u20130, 2019.   \n[3] Jing Xu, Rui Zhao, Feng Zhu, Huaming Wang, and Wanli Ouyang. Attention-aware compositional network for person re-identification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2119\u20132128, 2018.   \n[4] Lu Pang, Yaowei Wang, Yi-Zhe Song, Tiejun Huang, and Yonghong Tian. Cross-domain adversarial feature learning for sketch re-identification. In Proceedings of the 26th ACM international conference on Multimedia, pages 609\u2013617, 2018.   \n[5] Cuiqun Chen, Mang Ye, Meibin Qi, and Bo Du. Sketchtrans: Disentangled prototype learning with transformer for sketch-photo recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.   \n[6] Kejun Lin, Zhixiang Wang, Zheng Wang, Yinqiang Zheng, and Shin\u2019ichi Satoh. Beyond domain gap: Exploiting subjectivity in sketch-based person retrieval. In Proceedings of the 31st ACM International Conference on Multimedia, pages 2078\u20132089, 2023.   \n[7] Cuiqun Chen, Mang Ye, and Ding Jiang. Towards modality-agnostic person re-identification with descriptive query. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15128\u201315137, 2023.   \n[8] Shaojun Gui, Yu Zhu, Xiangxiang Qin, and Xiaofeng Ling. Learning multi-level domain invariant features for sketch re-identification. Neurocomputing, 403:294\u2013303, 2020.   \n[9] Yafei Zhang, Yongzeng Wang, Huafeng Li, and Shuang Li. Cross-compatible embedding and semantic consistent feature construction for sketch re-identification. In Proceedings of the 30th ACM International Conference on Multimedia, pages 3347\u20133355, 2022.   \n[10] Cuiqun Chen, Mang Ye, Meibin Qi, and Bo Du. Sketch transformer: Asymmetrical disentanglement learning from dynamic synthesis. In Proceedings of the 30th ACM International Conference on Multimedia, pages 4012\u20134020, 2022.   \n[11] Xingyu Liu, Xu Cheng, Haoyu Chen, Hao Yu, and Guoying Zhao. Differentiable auxiliary learning for sketch re-identification. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 3747\u20133755, 2024.   \n[12] Yajing Zhai, Yawen Zeng, Da Cao, and Shaofei Lu. Trireid: Towards multi-modal person reidentification via descriptive fusion model. In Proceedings of the 2022 International Conference on Multimedia Retrieval, pages 63\u201371, 2022.   \n[13] Enhao Ning, Changshuo Wang, Huang Zhang, Xin Ning, and Prayag Tiwari. Occluded person re-identification with deep learning: a survey and perspectives. Expert Systems with Applications, page 122419, 2023.   \n[14] Leonid V Kantorovich. On the translocation of masses. Journal of mathematical sciences, 133(4):1381\u20131382, 2006.   \n[15] Wanxing Chang, Ye Shi, Hoang Tuan, and Jingya Wang. Unified optimal transport framework for universal domain adaptation. Advances in Neural Information Processing Systems, 35:29512\u2013 29524, 2022.   \n[16] Bharath Bhushan Damodaran, Benjamin Kellenberger, R\u00e9mi Flamary, Devis Tuia, and Nicolas Courty. Deepjdot: Deep joint distribution optimal transport for unsupervised domain adaptation. In Proceedings of the European conference on computer vision (ECCV), pages 447\u2013463, 2018.   \n[17] Kilian Fatras, Thibault S\u00e9journ\u00e9, R\u00e9mi Flamary, and Nicolas Courty. Unbalanced minibatch optimal transport; applications to domain adaptation. In International Conference on Machine Learning, pages 3186\u20133197. PMLR, 2021.   \n[18] Wanxing Chang, Ye Shi, and Jingya Wang. Csot: Curriculum and structure-aware optimal transport for learning with noisy labels. Advances in Neural Information Processing Systems, 36:8528\u20138541, 2023.   \n[19] Chuanwen Feng, Yilong Ren, and Xike Xie. Ot-filter: An optimal transport filter for learning with noisy labels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16164\u201316174, 2023.   \n[20] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superglue: Learning feature matching with graph neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4938\u20134947, 2020.   \n[21] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8922\u20138931, 2021.   \n[22] Chao Zhang, Stephan Liwicki, and Roberto Cipolla. Beyond the cls token: Image reranking using pretrained vision transformers. In BMVC, page 80, 2022.   \n[23] Sergio Izquierdo and Javier Civera. Optimal transport aggregation for visual place recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16164\u201316174, 2023.   \n[24] Jiangming Shi, Yachao Zhang, Xiangbo Yin, Yuan Xie, Zhizhong Zhang, Jianping Fan, Zhongchao Shi, and Yanyun Qu. Dual pseudo-labels interactive self-training for semi-supervised visible-infrared person re-identification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11218\u201311228, 2023.   \n[25] Jiangming Wang, Zhizhong Zhang, Mingang Chen, Yi Zhang, Cong Wang, Bin Sheng, Yanyun Qu, and Yuan Xie. Optimal transport for label-efficient visible-infrared person re-identification. In European Conference on Computer Vision, pages 93\u2013109. Springer, 2022.   \n[26] Yongguo Ling, Zhun Zhong, Zhiming Luo, Fengxiang Yang, Donglin Cao, Yaojin Lin, Shaozi Li, and Nicu Sebe. Cross-modality earth mover\u2019s distance for visible thermal person reidentification. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 1631\u20131639, 2023.   \n[27] Zhizhong Zhang, Yuan Xie, Ding Li, Wensheng Zhang, and Qi Tian. Learning to align via wasserstein for person re-identification. IEEE Transactions on Image Processing, 29:7104\u20137116, 2020.   \n[28] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021.   \n[29] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021.   \n[30] Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. arXiv preprint arXiv:2110.07602, 2021.   \n[31] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16816\u201316825, 2022.   \n[32] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. International Journal of Computer Vision, 130(9):2337\u20132348, 2022.   \n[33] Beier Zhu, Yulei Niu, Yucheng Han, Yue Wu, and Hanwang Zhang. Prompt-aligned gradient for prompt tuning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15659\u201315669, 2023.   \n[34] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision, pages 709\u2013727. Springer, 2022.   \n[35] Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, et al. Dualprompt: Complementary prompting for rehearsal-free continual learning. In European Conference on Computer Vision, pages 631\u2013648. Springer, 2022.   \n[36] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, and Tomas Pfister. Learning to prompt for continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 139\u2013149, 2022.   \n[37] Bingzhi Chen, Zhongqi Wu, Yishu Liu, Biqing Zeng, Guangming Lu, and Zheng Zhang. Enhancing cross-modal retrieval via visual-textual prompt hashing. In Proceedings of international joint conference on artificial intelligence (IJCAI), pages 1\u201310, 2024.   \n[38] Rajshekhar Das, Yonatan Dukler, Avinash Ravichandran, and Ashwin Swaminathan. Learning expressive prompting with residuals for vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3366\u20133377, 2023.   \n[39] Aneeshan Sain, Ayan Kumar Bhunia, Pinaki Nath Chowdhury, Subhadeep Koley, Tao Xiang, and Yi-Zhe Song. Clip for all things zero-shot sketch-based image retrieval, fine-grained or not. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2765\u20132775, 2023.   \n[40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[41] Yajing Zhai, Yawen Zeng, Zhiyong Huang, Zheng Qin, Xin Jin, and Da Cao. Multi-prompts learning with cross-modal alignment for attribute-based person re-identification. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 6979\u20136987, 2024.   \n[42] He Li, Mang Ye, Ming Zhang, and Bo Du. All in one framework for multimodal re-identification in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17459\u201317469, 2024.   \n[43] Daeha Kim and Byung Cheol Song. Optimal transport-based identity matching for identityinvariant facial expression recognition. Advances in Neural Information Processing Systems, 35:18749\u201318762, 2022.   \n[44] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural information processing systems, 26, 2013.   \n[45] Jason Altschuler, Jonathan Niles-Weed, and Philippe Rigollet. Near-linear time approximation algorithms for optimal transport via sinkhorn iteration. Advances in neural information processing systems, 30, 2017.   \n[46] Zhedong Zheng, Liang Zheng, Michael Garrett, Yi Yang, Mingliang Xu, and Yi-Dong Shen. Dual-path convolutional image-text embeddings with instance loss. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), 16(2):1\u201323, 2020.   \n[47] Peng Wu, Xuerong Zhou, Guansong Pang, Lingru Zhou, Qingsen Yan, Peng Wang, and Yanning Zhang. Vadclip: Adapting vision-language models for weakly supervised video anomaly detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 6074\u20136082, 2024.   \n[48] Haiwen Diao, Ying Zhang, Lin Ma, and Huchuan Lu. Similarity reasoning and filtration for image-text matching. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 1218\u20131226, 2021.   \n[49] Jianfeng Dong, Xirong Li, Chaoxi Xu, Xun Yang, Gang Yang, Xun Wang, and Meng Wang. Dual encoding for video retrieval by text. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(8):4065\u20134080, 2021.   \n[50] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[51] Mang Ye, Jianbing Shen, David J. Crandall, Ling Shao, and Jiebo Luo. Dynamic dual-attentive aggregation learning for visible-infrared person re-identification. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XVII 16, pages 229\u2013247. Springer, 2020.   \n[52] Chaoyou Fu, Yibo Hu, Xiang Wu, Hailin Shi, Tao Mei, and Ran He. Cm-nas: Cross-modality neural architecture search for visible-infrared person re-identification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11823\u201311832, 2021.   \n[53] Mang Ye, Weijian Ruan, Bo Du, and Mike Zheng Shou. Channel augmented joint learning for visible-infrared recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13567\u201313576, 2021.   \n[54] Yukang Zhang, Yan Yan, Yang Lu, and Hanzi Wang. Towards a unified middle modality learning for visible-infrared person re-identification. In Proceedings of the 29th ACM international conference on multimedia, pages 788\u2013796, 2021.   \n[55] Mouxing Yang, Zhenyu Huang, Peng Hu, Taihao Li, Jiancheng Lv, and Xi Peng. Learning with twin noisy labels for visible-infrared person re-identification. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 14308\u201314317, 2022.   \n[56] Hanzhe Sun, Jun Liu, Zhizhong Zhang, Chengjie Wang, Yanyun Qu, Yuan Xie, and Lizhuang Ma. Not all pixels are matched: Dense contrastive learning for cross-modality person reidentification. In Proceedings of the 30th ACM international conference on multimedia, pages 5333\u20135341, 2022.   \n[57] Yiyuan Zhang, Yuhao Kang, Sanyuan Zhao, and Jianbing Shen. Dual-semantic consistency learning for visible-infrared person re-identification. IEEE Transactions on Information Forensics and Security, 18:1554\u20131565, 2022.   \n[58] Yukang Zhang and Hanzi Wang. Diverse embedding expansion network and low-light crossmodality benchmark for visible-infrared person re-identification. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2153\u20132162, 2023.   \n[59] Mang Ye, Jianbing Shen, Gaojie Lin, Tao Xiang, Ling Shao, and Steven CH Hoi. Deep learning for person re-identification: A survey and outlook. IEEE transactions on pattern analysis and machine intelligence, 44(6):2872\u20132893, 2021.   \n[60] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or region supervision. In International conference on machine learning, pages 5583\u20135594. PMLR, 2021.   \n[61] Chuhao Zhou, Jinxing Li, Huafeng Li, Guangming Lu, Yong Xu, and Min Zhang. Video-based visible-infrared person re-identification via style disturbance defense and dual interaction. In Proceedings of the 31st ACM International Conference on Multimedia, pages 46\u201355, 2023.   \n[62] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1\u20139, 2015.   \n[63] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.   \n[64] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[65] Qian Yu, Feng Liu, Yi-Zhe Song, Tao Xiang, Timothy M Hospedales, and Chen-Change Loy. Sketch me that shoe. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 799\u2013807, 2016.   \n[66] Patsorn Sangkloy, Nathan Burnell, Cusuh Ham, and James Hays. The sketchy database: learning to retrieve badly drawn bunnies. ACM Transactions on Graphics (TOG), 35(4):1\u201312, 2016.   \n[67] Alexander Hermans, Lucas Beyer, and Bastian Leibe. In defense of the triplet loss for person re-identification. arXiv preprint arXiv:1703.07737, 2017.   \n[68] Brandon Amos and J Zico Kolter. Optnet: Differentiable optimization as a layer in neural networks. In International Conference on Machine Learning, pages 136\u2013145. PMLR, 2017.   \n[69] Yang Qin, Yingke Chen, Dezhong Peng, Xi Peng, Joey Tianyi Zhou, and Peng Hu. Noisycorrespondence learning for text-to-image person re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1\u201318, 2024.   \n[70] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping languageimage pre-training for unified vision-language understanding and generation. In International conference on machine learning, pages 12888\u201312900. PMLR, 2022.   \n[71] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. Git: A generative image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Details about Optimal Transport ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "This section mainly introduces optimal transport and its corresponding algorithm, Sinkhorn-Knopp [44]. Let $\\Gamma_{r}\\;:=\\;\\overline{{\\{x\\;\\in\\;\\mathbb{R}_{+}^{r}\\vert x^{\\top}\\}}}\\;=\\;1\\}$ represents the probability simplex, where $1_{r}$ is the $r$ - dimensional vector of ones. Given two probability simplex vectors $\\alpha\\in\\Gamma_{m}$ and $\\beta\\in\\Gamma_{n}$ and a cost matrix $C\\in\\mathbb{R}^{m\\times n}$ , the objective of $\\mathrm{OT}$ is to seek the optimal transport plan $P^{*}$ mapping $\\alpha$ to $\\beta$ at the minimum cost: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{c}{d_{C}(\\alpha,\\beta)=\\underset{P\\in U(\\alpha,\\beta)}{\\operatorname*{min}}\\langle C,P\\rangle,\\;}\\\\ {U(\\alpha,\\beta)=\\left\\{P\\in\\mathbb{R}_{+}^{m\\times n}\\mid P1_{n}=\\alpha,P^{\\top}1_{m}=\\beta\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $U(\\alpha,\\beta)$ denotes the transport polytope of $\\alpha$ and $\\beta$ , i.e., the solution space of $P$ . The above problem is to find optimal solution $P^{*}$ in a set of all possible joint probabilities of $(X,Y)$ , where $X$ and $Y$ represent random variables with marginal distribution $\\alpha$ and $\\beta$ . ", "page_idx": 15}, {"type": "text", "text": "Generally, optimal transport (OT) problem is a linear programming problem that can theoretically be solved in polynomial time. However, in the actual solving process, it involves the square of anchor feature dimensions at all scales, requiring near-cubic complexity [68]. Therefore, consider optimizing the problem through iteration. This method optimizes the solving process by adding an entropy constraint to the OT problem, transforming Eq. 7 into a non-linear convex form with a regularization term: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\quad d_{C}(\\alpha,\\beta)=\\underset{P\\in U(\\alpha,\\beta)}{\\operatorname*{min}}\\langle C,P\\rangle+\\delta E(P),}\\\\ &{U(\\alpha,\\beta)=\\left\\{P\\in\\mathbb{R}_{+}^{m\\times n}\\mid P\\boldsymbol{1}_{n}=\\alpha,P^{\\top}\\boldsymbol{1}_{m}=\\beta\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\delta$ is a hyper-parameter and $E(P)\\;=\\;P(\\log P\\mathrm{~-~}1)$ is the entropy of $P$ . Introducing a regularization term is equivalent to introducing a prior knowledge: without considering the cost matrix $C$ , the distribution of assignment matrix $P$ is expected to be as uniform as possible. Eq. 8 is an entropy-regularized OT (EOT) problem and can be solved by Sinkhorn-Knopp algorithm [44] based on iterative updates of vectors. According to the Lagrange Multiplier Method, the conditional extremum problem (i.e., Eq. 8) can be transformed into an unconditional extremum problem: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{d}_{C}(\\alpha,\\beta)=\\operatorname*{min}_{P}\\langle C,P\\rangle+\\delta E(P)+\\mu(P1_{n}-\\alpha)+\\rho(P^{\\top}1_{m}-\\beta)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where, $\\mu$ and $\\rho$ are the Lagrange multipliers. If we take its derivative and set it to 0, we can find $P^{*}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\nP^{*}=e x p(-\\frac{\\mu}{\\delta})e x p(-\\frac{C}{\\delta})e x p(-\\frac{\\rho}{\\delta})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Let $\\begin{array}{r}{u=e x p(-\\frac{\\mu}{\\delta})}\\end{array}$ , $\\begin{array}{r}{v\\,=\\,e x p(-\\frac{\\rho}{\\delta})}\\end{array}$ and $\\begin{array}{r}{K\\,=\\,e x p(-\\frac{C}{\\delta})}\\end{array}$ , two constraint of Eq. 8 needs to be met simultaneously. Thus, one possible solution is to iterate enough times according to the following iteration formula: ", "page_idx": 15}, {"type": "equation", "text": "$$\nu^{z}=\\frac{\\alpha}{K v^{z-1}},v^{z}=\\frac{\\beta}{K^{\\top}u^{z}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Eq. 11 is called as Sinkhorn-Knopp iteration. After $z$ iterations, $P^{*}$ can be obtained by the following equation: ", "page_idx": 15}, {"type": "equation", "text": "$$\nP^{*}=D i a g(u)K D i a g(v)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "It is worth noting that the process of updating $u$ and $v$ (i.e., Eq. 11) alternately can be simplified to a single step: $u^{z}\\,\\,\\bar{\\leftarrow}\\,\\alpha/K(\\bar{\\beta}/K^{\\top}u^{z-1})$ . Furthermore, when all elements of $\\alpha$ are positive, this single step can be further simplified as: $u^{z}\\gets1./\\hat{K}(\\beta/K^{\\top}u^{z-1})$ , where $\\hat{K}=D i a g(1./\\alpha)K$ . The overall flow of Sinkhorn-Knopp is described by Algorithm 1. ", "page_idx": 15}, {"type": "text", "text": "B Details about Triplet Assignment Loss ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Derivation for Gradient ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "This appendix provides some details on gradient derivation. To simplify representation and analysis, we focus on a single direction, following the approach in [69], considering that sketch-to-image retrieval and image-to-sketch retrieval are symmetrical. Moreover, we assume that there is only one ", "page_idx": 15}, {"type": "text", "text": "Algorithm 1 The OT problem via Sinkhorn-Knopp ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Require: Cost matrix $C$ , weights $\\alpha$ , $\\beta$   \nRequire: Hyper-parameter $\\delta$ , iterations $i=1$ , max iteration $z$   \n1: Initialize $K=e^{-C/\\delta}$ , $\\hat{K}=D i a g(1./\\alpha)K$ , $u_{0}=1_{n}/n$   \n2: while $u$ changes or $i$ is less than or equal $z$ do   \n3: $u^{i}\\gets1./\\dot{K}(\\beta/K^{\\top}u^{i-1})$   \n4: i \u2190i + 1   \n5: end while   \n6: Get optimal value $u^{*}=u$   \n7: $v^{*}=\\dot{\\beta}/K^{\\top}u^{*}$   \n8: return optimal flow matrix $P^{*}={D i a g(u^{*})K D i a g(v^{*})}$ ", "page_idx": 16}, {"type": "text", "text": "paired image for each sketch in the mini-batch. Consequently, we can simplify TRL and TAL as shown below: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{t r l}(R_{i},S_{i})=[m-r_{i}^{\\top}s_{i}+\\hat{r}_{i}^{\\top}s_{i}]_{+},~~~~~~~~~~~~~~~~~~~~~~~}\\\\ {\\mathcal{L}_{t a l}(R_{i},S_{i})=[m-d(r_{i},s_{i})+d(\\hat{r}_{i},s_{i}))]_{+},~d(r,s)=\\gamma r^{\\top}s+(1-\\gamma)(1-e x p(-\\frac{r^{\\top}s}{\\delta}))r^{\\top}s_{i}^{\\top}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\delta$ is the hyper-parameter of OT problem, $\\hat{r}_{i}$ and $r_{i}$ are the hardest negative sample and positive sample of the anchor sample $s_{i}$ , respectively. These $l_{2}$ -normalized features are embedded by the modality-specific models, i.e., $f_{\\theta_{r}}(\\cdot)$ and $f_{\\theta_{s}}(\\cdot)$ . Due to the truncation operation $[x]_{+}$ , we only discuss the case of $\\mathcal{L}>0$ that could generate gradients. For TRL, the gradients to the parameters $\\theta_{r}$ and $\\theta_{s}$ are: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathcal{L}_{t r l}}{\\partial\\theta_{s}}=\\frac{\\partial\\mathcal{L}_{t r l}}{\\partial s_{i}}\\frac{\\partial s_{i}}{\\partial\\theta_{s}},\\quad\\frac{\\partial\\mathcal{L}_{t r l}}{\\partial\\theta_{r}}=\\frac{\\partial\\mathcal{L}_{t r l}}{\\partial r_{i}}\\frac{\\partial r_{i}}{\\partial\\theta_{r}}+\\frac{\\partial\\mathcal{L}_{t r l}}{\\partial\\hat{r}_{i}}\\frac{\\partial\\hat{r}_{i}}{\\partial\\theta_{r}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since the learning of normalized features can be viewed as the movement process of points on a unit hyperplane, we only consider the loss gradients with respect to $r_{i},\\,\\hat{r}_{i}$ and $s_{i}$ are: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathcal{L}_{t r l}}{\\partial r_{i}}=-s_{i},\\quad\\frac{\\partial\\mathcal{L}_{t r l}}{\\partial s_{i}}=\\hat{r}_{i}-r_{i},\\quad\\frac{\\partial\\mathcal{L}_{t r l}}{\\partial\\hat{r}_{i}}=s_{i}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For our TAL, the gradients to the parameters $\\theta_{r}$ and $\\theta_{s}$ are: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathcal{L}_{t a l}}{\\partial\\theta_{s}}=\\frac{\\partial\\mathcal{L}_{t a l}}{\\partial s_{i}}\\frac{\\partial s_{i}}{\\partial\\theta_{s}},\\quad\\frac{\\partial\\mathcal{L}_{t a l}}{\\partial\\theta_{r}}=\\frac{\\partial\\mathcal{L}_{t a l}}{\\partial r_{i}}\\frac{\\partial r_{i}}{\\partial\\theta_{r}}+\\frac{\\partial\\mathcal{L}_{t a l}}{\\partial\\hat{r}_{i}}\\frac{\\partial\\hat{r}_{i}}{\\partial\\theta_{r}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since the result of $e x p(-\\frac{r^{\\top}s}{\\delta})$ is obtained through several iterations of Sinkhorn-Knopp algorithm, this part does not conduct gradients and can be simplified as a coefficient $\\begin{array}{r}{\\hat{\\delta}=1-e x p(-\\frac{r^{\\top}s}{\\delta})\\in[0,1]}\\end{array}$ . Thus, the gradients for $r_{i},\\,\\hat{r}_{i}$ and $s_{i}$ are: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\partial\\mathcal{L}_{t a l}}{\\partial r_{i}}=-[\\gamma+(1-\\gamma)\\hat{\\delta}]s_{i},\\quad\\frac{\\partial\\mathcal{L}_{t a l}}{\\partial\\hat{r}_{i}}=[\\gamma+(1-\\gamma)\\hat{\\delta}]s_{i},}\\\\ {\\frac{\\partial\\mathcal{L}_{t a l}}{\\partial s_{i}}=\\gamma(\\hat{r}_{i}-r_{i})+(1-\\gamma)\\hat{\\delta}(\\hat{r}_{i}-r_{i}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B.2 The Effectiveness of Triplet Assignment Loss ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We have included visual analysis in Fig. 4, which illustrates the convergence curve and sample distances during training. Figure 1(a) shows that conventional triplet loss converges prematurely. In contrast, our proposed triplet assignment loss exhibits higher volatility, reducing the risk of suboptimal local minima. Additionally, Figure 1(b) shows that a specific sketch sample (red box in the top left image) may have similar Euclidean distances to multiple RGB samples. The triplet assignment loss comprehensively considers the distribution of all samples (red box in the lower right image), offering broader possibilities for selecting the most relevant ones. ", "page_idx": 16}, {"type": "image", "img_path": "A34sBX4R5N/tmp/89fdd47ae42acd30e69289fcae1f571cb9a3a8edcd85d6058426a40b16a3e907.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "A34sBX4R5N/tmp/bd8d2fb3dc34d0bbcbaa36781ebee48e5d441cdbb0f36f052e002f7309df6119.jpg", "img_caption": ["Figure 4: The effectiveness analysis of Triplet Assignment Loss. In Figure (b), the vertical axis represents RGB images, and the horizontal axis represents sketches. For each ID, 4 training examples are sampled, so the $4\\mathrm{x4}$ cells on the diagonal represent positive sample pairs. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "C Computational Complexity ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Our OLTM achieves the trade-off between performance enhancement and computational complexity. To this end, we select several methods for comparing parameters, floating-point operations (FLOPs), and frames per second (FPS), as shown in the Tab. 5 below. The results show that OLTM gets remarkable performance while maintaining reasonable computational costs. The reason is that: 1) only TCA module is required for inference; 2) the Visual Question Answering(VQA) model is used during data processing. ", "page_idx": 17}, {"type": "table", "img_path": "A34sBX4R5N/tmp/e0b9fd0028e65eefb4104767e9fbfeb495ee7d13d59c53ce75892ddcde6fc345.jpg", "table_caption": ["Table 5: The number of parameters, FLOPs, and FPS of different methods, where bold indicates the best performance in this field and underline indicates the second-best performance. VI denotes visible-infrared person re-identification. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "D Additional Experimental Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "D.1 Experimental Setup ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Implementation Details The dimensions of image and text features are set to 512. Within a batch, we randomly select 8 identities, each comprising 4 images and 4 sketches. Each image is associated with 9 fine-grained textual attributes. To ensure more reliable comparisons, the random seeds are all set to 0. In the Text Prompt Reasoning module, ConMLP consists of a stack of $N=2$ identical MLPs, where $\\theta_{m l p}$ represents the trainable parameters. The iteration number of the Optimal Transport algorithm is 3. In the Triplet Assignment Loss, the iteration number of the Optimal Transport algorithm is 50. The model is trained with the Adam optimizer, starting with a learning rate of 1e-5, decaying with a cosine scheduler. The model is implemented in PyTorch on the RTX 4090 24GB GPU. ", "page_idx": 17}, {"type": "text", "text": "D.2 Experiment Result ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Cross-style Retrieval Given the extensive Market-Sketch-1K dataset, where each person is sketched by six different artists, notable variations exist across these sketches. Consequently, we devise this experiment to assess our model\u2019s resilience to diverse artistic styles. Experimental setups involve sketches labeled as S1 to S6, each originating from different artists. Models are trained on sketches by specific artists and tested on sketches by others. And, \"single query\" denotes separate queries for sketches by different artists of the same individual, while \"multi query\" indicates queries combining multiple sketches of the same person. The specific details of these experiments are presented in Tab. 6. ", "page_idx": 18}, {"type": "table", "img_path": "A34sBX4R5N/tmp/43037502b06fdb4e01a387d060b2dca86d3549617286470abb2f4c0acb5fee9c.jpg", "table_caption": ["Table 6: Testing on unseen styles. We report the mAP score. ) single-query train and single-query test b) multi-query train and multi-query test "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Multi-query Setting Similar to [6], \"multi-query\" involves combining multiple sketches of the same ID during both training and inference. Our paper employs a straightforward fusion method by averaging the image features from multiple sketches. Tab. 7 below provides a comparative analysis of various fusion strategies. The results demonstrate that the basic and simple fusion method achieves the best experimental performance. ", "page_idx": 18}, {"type": "table", "img_path": "A34sBX4R5N/tmp/2c3eda317e381082ad087fb425e1ec858051653e102f816fb57f729bf6e51dce.jpg", "table_caption": ["Table 7: Performance comparison of different multi-query experimental methods. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Overfitting Analysis To mitigate overfitting, we apply various data augmentation techniques, including random cropping, rotation, and style augmentation. Furthermore, to validate the model\u2019s robustness and generalization, we conduct supplementary evaluation experiments on two large-scale datasets (SYSU-MM01 and RegDB) for visible-infrared person re-ID, as shown in Tab. 8. The results demonstrate that our OLTM achieves comparable performance in the visible-infrared domain. ", "page_idx": 18}, {"type": "text", "text": "Parameter Analysis For the Triplet Assignment Loss proposed in our work, we compute the sample distance using Eq. 18. Fig. 5 illustrates an analysis of the hyper-parameter $\\gamma$ . It can be observed that setting $\\gamma=0.3$ yields the best performance in Rank-1 and mAP. ", "page_idx": 18}, {"type": "equation", "text": "$$\nD(R_{i},S_{i})=\\gamma E(R_{i},S_{i})+(1-\\gamma)(1-P_{i,i}^{*})E(R_{i},S_{i})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $E(R_{i},S_{i})=\\|R_{i}^{'}-S_{i}^{'}\\|_{2}$ denotes the Euclidean distance between feature representations. ", "page_idx": 18}, {"type": "text", "text": "In Dynamic Consensus Acquisition, the cost matrix $C\\in\\mathbb{R}_{+}^{n\\times m}$ can be calculated for assignment, where the $(i,j)$ -th element $C_{i,j}$ represents the cost of assigning a feature to an atom. Concretely, to introduce certain priors as guidance [23], the enhanced features are used to learn the cost matrix through two fully connected layers initialized randomly. $m$ is a hyper-parameter that needs to be set. Fig. 6 analyzes the values of $m$ , showing that the best performance in terms of Rank-1 and mAP is achieved when $m=32$ . ", "page_idx": 18}, {"type": "table", "img_path": "A34sBX4R5N/tmp/048a7bb8667af7f1ca47c8f6f6fbe3a1cf8890238a414536468d4fb11b425b07.jpg", "table_caption": ["Table 8: Comparison results of our method on visible-infrared datasets, namely SYSU-MM01 and RegDB. Market-Sketch-1K is a sketch dataset used for reference. \"VI\" and \"Sketch\" represent their respective task domains. "], "table_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "A34sBX4R5N/tmp/44f317a1c586a0e78d34f8f07b9f623c385db5070a339d49eaa29c6edfdc9dee.jpg", "img_caption": ["Figure 5: Analysis of the hyperparameter $\\gamma$ "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "A34sBX4R5N/tmp/ad6eb4fbcb451ef00f5017382138b72998fbf2ea6d952096a90d3a3acf39fd01.jpg", "img_caption": ["Figure 6: Analysis of the hyperparameter $m$ "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Qualitative Analysis Fig. 7 shows the Rank-5 results of OLTM and the baseline on the MarketSketch-1K dataset. The left and right parts show the retrieval results of the baseline and OLTM, respectively. We can observe that OLTM can focus on more fine-grained discriminative information, such as bag and hat. In contrast, the baseline only considers global information matching, which leads to performance degradation. ", "page_idx": 19}, {"type": "image", "img_path": "A34sBX4R5N/tmp/a4cc9409ec628c3b7231307ac613e0b438c43b498ac24cb3b3d35a19d2dc8a93.jpg", "img_caption": ["Figure 7: The Rank-5 retrieval results under the multi-query setting on the Market-Sketch-1K dataset are presented. On the left side are the retrieval results of the baseline, and on the right side are the retrieval results of OLTM. Green borders indicate successful retrieval of the target pedestrian, while red borders indicate incorrect results. Yellow boxes represent fine-grained information. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "E Analysis of VQA Models ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "E.1 The Setup of the VQA Problems ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Through extensive statistical analysis, we have formulated 9 specific questions to obtain the corresponding fine-grained textual attributes for a given image. These nine questions are as follows: ", "page_idx": 20}, {"type": "text", "text": "1. What is the gender of this person?   \n2. Is this person with long or short hair?   \n3. What is the color of this person\u2019s shirt?   \n4. Is this person wearing long sleeves or short sleeves?   \n5. Is this person wearing pants or a dress underneath?   \n6. What is the color of this person\u2019s lower garment?   \n7. Is this person carrying a backpack?   \n8. Is this person wearing a hat?   \n9. Is this person wearing glasses? ", "page_idx": 20}, {"type": "text", "text": "E.2 Replaceability of VQA Model ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The VQA model is inherently substitutable. Essentially, any visual-language model which is capable of generating target attribute information from images can serve as an alternative. We also use other VQA models to demonstrate their substitutability, as shown in the Tab. 9 below. ", "page_idx": 20}, {"type": "table", "img_path": "A34sBX4R5N/tmp/323e8c420c5ed3a15acef7f9a75e03f6e261c58844b694d9b56e2daaed4b3486.jpg", "table_caption": ["Table 9: Performance comparison of different VQA models. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "E.3 The Fine-grained Recognition Ability of VQA Model ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The VQA model has the capability to describe fine-grained recognition information for the following reasons: 1) The VQA model generates detailed attributes about various aspects of the pedestrian target (e.g., hair, backpack, hat), rather than relying on complete descriptive sentences. 2) We provide a visualization comparison, as shown in Fig. 8. This comparison demonstrates that using text attributes can guide effectively the attention of model. ", "page_idx": 20}, {"type": "image", "img_path": "A34sBX4R5N/tmp/22d2b5c852d6e2e73f9af6c77c8aecf8e0d9908d9aae7e2150069aa30c2f257b.jpg", "img_caption": ["Figure 8: Visualization of attention maps. The Green and Red box indicate presence and absence of text attribute guidance, respectively. The Orange box represents partial text attribute guidance (excluding head-related information). "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "E.4 Background Information ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We conduct additional background evaluation on Market-Sketch-1k dataset based on our initial studies. Specifically, we formulate the question \u2019What is the background of this image?\u2019 to extract textual attributes about background. The extracted background details are illustrated in Fig. 9. However, the introduction of background information result in a decrease of 2.81 in Rank-1 and 1.62 in mAP. This decline can be attributed to the absence of corresponding background information in the sketches, which potentially interferes with the model\u2019s learning process. ", "page_idx": 21}, {"type": "image", "img_path": "A34sBX4R5N/tmp/fcdae42039a2fd09f8a837c8c6595a6a9ce8f25bdb04e42db2223b3cb391e99b.jpg", "img_caption": ["Figure 9: The text attributes generated by VQA model on RGB images. Red indicates background information obtained from the question: \"What is the background of this image?\". "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "E.5 The Availability of Text Attributes ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "To verify the effectiveness of different text attributes, we have provided additional ablation experiments in Tab. 10 below. The results show a significant decrease in model performance after discarding several hard-distinguished attributes (e.g., color and gender) in sketches. As shown in Fig. 10, sketches convey gender-related information through factors like body shape, and the contrast between light and dark areas effectively highlights specific color details. The TPR module injects detailed information into modal interactions during training. This enables the model to focus on these nuances autonomously, even without TPR during inference. ", "page_idx": 21}, {"type": "table", "img_path": "A34sBX4R5N/tmp/c2f27f17ecb0bba76cb75118739e946294dd41de7d1464261292d38ade145971.jpg", "table_caption": ["Table 10: Building on the original experimental setup, this comparison evaluates performance by removing the fine-grained textual attributes of gender, up color, and down color. "], "table_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "A34sBX4R5N/tmp/948fce2a994f0ee6670fb86445245aa1238458c56c48a3bf400ec3a2e386b741.jpg", "img_caption": ["Figure 10: Comparative analysis of RGB images and corresponding sketches. Gender-related factors in sketches include body shape and hair, and the contrast highlights color details. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. The abstract and introduction provide a concise overview of the research area, the novel approaches introduced, and the key contributions, ensuring that readers have a clear understanding of what the paper aims to achieve and the significance of its findings. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: In the \"Discussion\" at the end of the paper, we discuss in detail the limitations of this work. However, our experiments revealed that when confronted with extremely vague sketch samples (i.e., those that human cannot discern features or match), the model\u2019s identification process deteriorates into a random selection among multiple potential outcomes. Therefore, sketch Re-ID heavily depend on the quality of sketches. Enhancing the discriminability of sketches without incurring additional labor costs is a topic worthy of further exploration in future research. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. ", "page_idx": 22}, {"type": "text", "text": "\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: All formulas and proofs in the paper are numbered and cross-referenced. The paper provides complete proofs of the formulas, and for those appearing in the supplementary material, a brief proof sketch is provided in the main text. The proofs in the paper are rigorously reasoned, adhere to accepted mathematical principles, and do not omit any critical steps. The theorems and lemmas relied upon in the proofs are appropriately referenced and cross-referenced. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: The paper fully discloses all the information needed to reproduce the main experimental results. The experimental section of the paper provides detailed descriptions of experimental setups, configurations, and other relevant details. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example   \n(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [No] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not currently provide open-access code, but it is planned to be made public in the future. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper specifies all the training and test details necessary to understand the results. It includes information such as data splits, hyperparameters, how they were chosen, the type of optimizer used, and any other relevant details regarding the training and testing procedures. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [No] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not report error bars suitably and correctly defined, nor does it provide other appropriate information about the statistical significance of the experiments. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: For each experiment, the paper provides sufficient information on the computer resources needed to reproduce the experiments (type of compute workers, memory). ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper discusses the potential positive societal impacts of the work. See introduction 1 ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and the license and terms of use explicitly mentioned and properly respected. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]