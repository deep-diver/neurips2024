[{"figure_path": "A34sBX4R5N/figures/figures_0_1.jpg", "caption": "Figure 1: The illustration of sketch Re-ID. Different artists create sketches based on clues provided by witness to assist the police in identifying targets.", "description": "This figure illustrates the process of sketch-based person re-identification (re-ID).  It starts with a witness who describes a person to a police sketch artist. The artist creates a sketch of the suspect based on the witness's description. The police then use the sketch as a query to search a gallery of images to find a match.", "section": "1 Introduction"}, {"figure_path": "A34sBX4R5N/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of our proposed OLTM network. Our model includes four main parts, i.e., text prompt reasoning (TPR), text-injected coarse-grained alignment Module (TCA), consensus-guided fine-grained interaction module (CFI) and triplet assignment loss (TAL). Specifically, TPR flexibly generates target characteristics through VQA, and combines prompt learning and optimal transport to reason text global embedding and local consensus. TCA and CFI extract modality-specific representations from image and sketch modalities to achieve hierarchical and multi-granularity alignment. Finally, TAL is designed to optimize distance measurement between samples and improve the model's capacity to capture local relationships.", "description": "The figure provides a detailed overview of the Optimal Transport-based Labor-free Text Prompt Modeling (OLTM) network architecture.  It illustrates the four main components of the model: Text Prompt Reasoning (TPR), Text-Injected Coarse-grained Alignment (TCA), Consensus-Guided Fine-grained Interaction (CFI), and Triplet Assignment Loss (TAL). TPR leverages a Visual Question Answering (VQA) model and prompt learning to generate and reason with textual embeddings for multi-granularity modal alignment.  TCA and CFI then utilize these embeddings to achieve both coarse-grained and fine-grained alignment between image and sketch modalities. Finally, TAL is used to optimize inter- and intra-class distances. The diagram shows the flow of information between the different components and the interactions between image, sketch, and text modalities.", "section": "4 The Proposed Method"}, {"figure_path": "A34sBX4R5N/figures/figures_7_1.jpg", "caption": "Figure 3: The Rank-5 retrieval results on two datasets. For the Market-Sketch-1K dataset, both single-query and multi-query scenarios are presented. Green border indicates correctly retrieved target pedestrians, while yellow border indicates incorrectly matched pedestrians.", "description": "This figure shows the top 5 retrieval results for both Market-Sketch-1K and PKU-Sketch datasets.  The left panel displays single-query results for Market-Sketch-1K, the middle panel shows the multi-query results for Market-Sketch-1K, and the right panel shows results for PKU-Sketch.  Green boxes indicate correctly identified pedestrians, while yellow boxes highlight incorrect matches. This visualization helps illustrate the model's performance in different query settings and datasets.", "section": "5.2 Comparison with State-of-the-Art Methods"}, {"figure_path": "A34sBX4R5N/figures/figures_17_1.jpg", "caption": "Figure 4: The effectiveness analysis of Triplet Assignment Loss. In Figure (b), the vertical axis represents RGB images, and the horizontal axis represents sketches. For each ID, 4 training examples are sampled, so the 4x4 cells on the diagonal represent positive sample pairs.", "description": "This figure shows a comparison of different loss functions and distance metrics used in the proposed method. Figure 4(a) illustrates the convergence curves of different losses, highlighting the superior performance of the Triplet Assignment Loss. Figure 4(b) visualizes the distance distribution across modalities for various distance metrics (Euclidean distance, optimal transport matrix, optimal assignment distance, and adjustment distance). The red boxes highlight the relationships between positive sample pairs for a specific ID.", "section": "4.4 Triplet Assignment Loss"}, {"figure_path": "A34sBX4R5N/figures/figures_17_2.jpg", "caption": "Figure 4: The effectiveness analysis of Triplet Assignment Loss. In Figure (b), the vertical axis represents RGB images, and the horizontal axis represents sketches. For each ID, 4 training examples are sampled, so the 4x4 cells on the diagonal represent positive sample pairs.", "description": "This figure shows the convergence curves of the triplet assignment loss and the hard triplet loss. It also shows the distance distribution across modalities, illustrating how the proposed triplet assignment loss considers the overall distribution of samples when measuring distances, unlike the hard triplet loss that focuses on individual sample pairs.  The heatmaps visualize the Euclidean distance and the optimal transport-based distance, highlighting the difference in how they capture similarity.", "section": "4.4 Triplet Assignment Loss"}, {"figure_path": "A34sBX4R5N/figures/figures_19_1.jpg", "caption": "Figure 5: Analysis of the hyperparameter \u03b3", "description": "The figure shows the Rank-5 results of OLTM and the baseline on the Market-Sketch-1K dataset. The left and right parts show the retrieval results of the baseline and OLTM, respectively. We can observe that OLTM can focus on more fine-grained discriminative information, such as bag and hat. In contrast, the baseline only considers global information matching, which leads to performance degradation.", "section": "5.2 Comparison with State-of-the-Art Methods"}, {"figure_path": "A34sBX4R5N/figures/figures_19_2.jpg", "caption": "Figure 4: The effectiveness analysis of Triplet Assignment Loss. In Figure (b), the vertical axis represents RGB images, and the horizontal axis represents sketches. For each ID, 4 training examples are sampled, so the 4x4 cells on the diagonal represent positive sample pairs.", "description": "This figure analyzes the effectiveness of the Triplet Assignment Loss compared to the Hard Triplet Loss.  Subfigure (a) shows the convergence curves of both loss functions, highlighting the more stable convergence of the proposed Triplet Assignment Loss. Subfigure (b) visualizes the distance distribution between RGB images and sketches in feature space, illustrating how the proposed loss considers the overall distribution when determining distances, unlike the Hard Triplet Loss which focuses solely on the hardest negative samples.", "section": "4.4 Triplet Assignment Loss"}, {"figure_path": "A34sBX4R5N/figures/figures_19_3.jpg", "caption": "Figure 3: The Rank-5 retrieval results on two datasets. For the Market-Sketch-1K dataset, both single-query and multi-query scenarios are presented. Green border indicates correctly retrieved target pedestrians, while yellow border indicates incorrectly matched pedestrians.", "description": "This figure shows the top 5 retrieval results for both the Market-Sketch-1K and PKU-Sketch datasets.  For Market-Sketch-1K, it demonstrates results for both single-query and multi-query scenarios.  Correctly identified pedestrians are highlighted with a green border, while incorrect matches have a yellow border. This visually demonstrates the model's performance in retrieving relevant images from a gallery based on a sketch query.", "section": "5.2 Comparison with State-of-the-Art Methods"}, {"figure_path": "A34sBX4R5N/figures/figures_20_1.jpg", "caption": "Figure 3: The Rank-5 retrieval results on two datasets. For the Market-Sketch-1K dataset, both single-query and multi-query scenarios are presented. Green border indicates correctly retrieved target pedestrians, while yellow border indicates incorrectly matched pedestrians.", "description": "This figure shows the top 5 retrieval results for both the PKU-Sketch and Market-Sketch-1K datasets.  The Market-Sketch-1K results are shown for both single-query and multi-query scenarios.  Green borders indicate that the retrieved image correctly matches the sketch, while yellow borders indicate an incorrect match.  The figure visually demonstrates the model's performance in retrieving relevant images from a gallery based on a sketch query.", "section": "5.2 Comparison with State-of-the-Art Methods"}, {"figure_path": "A34sBX4R5N/figures/figures_21_1.jpg", "caption": "Figure 9: The text attributes generated by VQA model on RGB images. Red indicates background information obtained from the question: \"What is the background of this image?\"", "description": "This figure shows examples of text attributes generated by a Visual Question Answering (VQA) model for RGB images.  The attributes describe details about the person in the image (gender, hair, clothing, accessories) as well as the background. The red text specifically highlights the attributes extracted in response to the question, \"What is the background of this image?\"", "section": "E.4 Background Information"}, {"figure_path": "A34sBX4R5N/figures/figures_21_2.jpg", "caption": "Figure 3: The Rank-5 retrieval results on two datasets. For the Market-Sketch-1K dataset, both single-query and multi-query scenarios are presented. Green border indicates correctly retrieved target pedestrians, while yellow border indicates incorrectly matched pedestrians.", "description": "This figure shows the top 5 retrieval results for both the Market-Sketch-1K and PKU-Sketch datasets.  The Market-Sketch-1K results are split into single-query and multi-query scenarios, visualizing the model's performance under different query types.  Correctly identified pedestrians have green borders, while incorrect matches have yellow borders.  This provides a visual comparison of the model's accuracy on two different benchmark datasets.", "section": "5.2 Comparison with State-of-the-Art Methods"}]