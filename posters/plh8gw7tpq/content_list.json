[{"type": "text", "text": "Algorithmic Capabilities of Random Transformers ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ziqian Zhong, Jacob Andreas Massachusetts Institute of Technology {ziqianz, jda}@mit.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Trained transformer models have been found to implement interpretable procedures for tasks like arithmetic and associative recall, but little is understood about how the circuits that implement these procedures originate during training. To what extent do they depend on the supervisory signal provided to models, and to what extent are they attributable to behavior already present in models at the beginning of training? To investigate these questions, we investigate what functions can be learned by randomly initialized transformers in which only the embedding layers are optimized, so that the only input\u2013output mappings learnable from data are those already implemented (up to a choice of encoding scheme) by the randomly initialized model. We find that these random transformers can perform a wide range of meaningful algorithmic tasks, including modular arithmetic, in-weights and in-context associative recall, decimal addition, parenthesis balancing, and even some aspects of natural language text generation. Our results indicate that some algorithmic capabilities are present in transformers (and accessible via appropriately structured inputs) even before these models are trained.1 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A large body of recent work has demonstrated the effectiveness of transformer language models (LMs) [46] on general sequence-modeling tasks. Transformers seem to be especially well-suited (relative to other flexible neural models) at problems involving numerical reasoning [41, 24, 30], string manipulation [28], and various forms of in-context learning [7, 17, 1, 25]. Why is this the case? ", "page_idx": 0}, {"type": "text", "text": "One possibility is that some aspect of the transformer architecture makes these behaviors easy to learn. Under this hypothesis, transformer models do not implement any useful functionality when initialized; however, their loss landscape is structured such that they can be (computation- and sample-) efficiently optimized for behaviors of interest. But another possibility is that\u2014because of intrinsic properties of the transformer architecture and parameter initialization schemes\u2014these capabilities are already implemented in some fashion even in randomly initialized models. ", "page_idx": 0}, {"type": "text", "text": "To disentangle these possibilities, we investigate the behavior of randomly initialized transformer models in which only the embedding layers are optimized, leaving all other model-internal parameters fixed. If such embedding-only training is successful, it implies that the randomly initialized model\u2019s behavior on some subspace already corresponds to the input\u2013output mapping of interest, up to a choice of encoding scheme\u2014in other words, that the randomly initialized model can already perform the target task, and it suffices to find an encoding of inputs and outputs that induces the target behavior. ", "page_idx": 0}, {"type": "text", "text": "In experiments on seven tasks, we find that embedding-only training yields accurate models for a diverse set of problems spanning arithmetic, associative recall, and sequence generation\u2014in some cases substantially outperforming similarly trained recurrent models. Remarkably, transformer language models trained in this fashion can even produce grammatical (though largely nonsensical) ", "page_idx": 0}, {"type": "image", "img_path": "plH8gW7tPQ/tmp/44de21504e7333b67eaedfb838dcddbcfbad0267944b554bb1f4786329759dc0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Overview of problem setup. A: Modeling approach. We initialize transformers randomly, then optimize only their input and output embedding layers on a dataset of interest. We find that these random transformers can be successfully trained to perform a diverse set of human-meaningful tasks. B: Task set. We evaluate the effectiveness of random transformers on a set of model problems involving arithmetic and memorization, as well as modeling of natural language text. ", "page_idx": 1}, {"type": "text", "text": "natural language text. We explain these results by showing that embedding-only training steers both inputs and model-internal representations into low-dimensional subspaces on which the model implements the target computation, a phenomenon we call \u201csubspace selection\u201d. Embedding-only training is most successful when the target computation can be performed in a subspace that is low-dimensional relative to the ambient dimension of the model\u2019s hidden representations. ", "page_idx": 1}, {"type": "text", "text": "These findings build on a long line of research aimed at understanding the effectiveness of deep networks in terms of their behavior at initialization\u2014e.g. showing that random convolutional networks are high-quality feature extractors [3, 8], or that overparameterized networks can be pruned down to sparse \u201clottery ticket\u201d subnetworks that implement the correct behavior [16, 53, 39, 11]. But in contrast to past work, the solutions found by embedding-only training involve algorithmic computation rather than feature extraction, performed in low-dimensional subspaces but not by sparse sub-networks. Even more generally, our results show that pruning and optimization are not always necessary to surface useful capabilities\u2014at least in transformers, some capabilities are available as soon as models are initialized, requiring only a learned encoding of inputs. This in turn suggests that it may be possible to partially understand the effectiveness of transformers simply by understanding their behavior at initialization. ", "page_idx": 1}, {"type": "text", "text": "Our work also has implications for research on circuit-level interpretability of transformers and other neural models: if even random models can perform structured, algorithmic tasks, then attempts to understand models by directly inspecting parameter matrices\u2014and not their behavior on natural data distributions\u2014may be fundamentally limited in their ability to characterize learned behaviors. ", "page_idx": 1}, {"type": "text", "text": "2 Background and Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Random feature extractors Random deep convolutional networks are highly effective visual feature extractors even without training. Jarrett et al. [23] first discovered that linearly combining features from a randomly initialized one-layer convolutional network achieved comparable performance to fully trained networks for downstream vision tasks. Saxe et al. [40] showed that performance improvements from training is relatively minor comparing to architectural changes. In this work, we expanded the discussion to language models and demonstrated that training embeddings alone is sufficient to succeed in many tasks, highlighting the strong inductive bias of transformer architectures. ", "page_idx": 1}, {"type": "text", "text": "Neural reprogramming Neural reprogramming aims to repurpose existing neural networks for novel tasks via simple transformation layers. This technique was first purposed by Elsayed et al. [15] as a way to exploit trained neural network served by existing providers, and it was later used as a resource-efficient domain-transfer technique [45, 49]. In our work, we showed that in addition to neural networks trained for other tasks, even randomly initialized neural networks can be reprogrammed to achieve non-trivial performance. ", "page_idx": 1}, {"type": "text", "text": "Sparse sub-networks and lottery tickets The lottery ticket hypothesis was first proposed by Frankle and Carbin [16]: a randomly-initialized dense neural network contains subnetworks, the winning tickets, that when trained in isolation can match the performance of the original network. Zhou et al. [53] and Ramanujan et al. [39] strengthened the hypothesis by discovering pruned subnetworks that achieve comparable accuracy of the trained full network within untrained, randomly initialized networks - winning tickets do not even need to be trained. The hypothesis also holds in transformers [11, 44]. Similar to the hypothesis, our work also focuses on the models\u2019 capabilities at initialization, but we showed that these capabilities can be surfaced without any pruning. ", "page_idx": 2}, {"type": "text", "text": "Interpreting neural circuits Many efforts have been dedicated to the interpretation of neural networks. On the arithmetic task we study in this paper, Nanda et al. [31] and Zhong et al. [52] described two different mechanisms in transformers on modular addition; Quirke and Barez [37] performed detailed mechanistic analysis for one-layer transformers trained on decimal addition. These studies provide insights into possible mechanisms transformers might employ to solve these tasks. With theoretical analysis, Wen et al. [47] proved that on the bounded Dyck task, the attention patterns and weights of neural circuits could be quite arbitrary, which we confirm in this work. ", "page_idx": 2}, {"type": "text", "text": "Reservoir computing Reservoir computing is a framework for computation. In the diagram, a blackbox reservoir receives data and updates its inner states there upon. A simple readout mechanism is then trained to map its inner states to the desired output. The reservior is generally kept untrained and only the readout part is trained. Under our notations, we can interpret the paradigm as training only the unembedding part of random neural networks. See Benjamin et al. [42] for a general overview for reservoir computing and Mantas et al. [29] for a survey on its applications on recurrent neural networks. ", "page_idx": 2}, {"type": "text", "text": "3 Setup", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Models We study the behavior of decoder-only transformer language models. In these models, inputs $x$ (represented as sequence of token IDs) are first assigned vector embeddings $h^{(0)}=E(x)$ via an embedding layer $E$ . These embeddings are then passed through a series of $m$ intermediate layers $F^{(1)},F^{(2)},\\cdots,F^{(m)}$ so that $h^{(i)}=\\dot{F}^{(i)}(h^{(i-1)})$ , with each $\\overline{{F^{(i)}(x)}}$ computing a hidden representation $h^{(i)}$ via a transformation: ", "page_idx": 2}, {"type": "equation", "text": "$$\nh^{(i)}=F^{(i)}(h^{(i-1)})=\\mathrm{FFN}^{(i)}(\\mathrm{SelfAtt}^{(i)}(h^{(i-1)}))\\;,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where FFN, SelfAtt are feed-forward and causal self-attention modules as in Radford et al. [38] (layer norms are omitted for simplicity). The final activation $h^{(m)}$ is mapped by a unembedding layer $U$ to a distribution over next tokens. ", "page_idx": 2}, {"type": "text", "text": "To encode information about the ordering of input tokens, we implement the embedding layer $E$ using two matrices: a token embedding matrix $E_{\\mathrm{token}}$ and a positional embedding matrix $E_{\\mathrm{pos}}$ . For an input $x=\\left[x_{1},x_{2},\\cdot\\cdot\\cdot\\,,x_{n}\\right]$ , we first calculate the initial activations ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\iota^{(0)}=E([x_{1},x_{2},\\cdot\\cdot\\cdot,x_{n}])=[E_{\\mathrm{token}}[x_{1}]+E_{\\mathrm{pos}}[1],E_{\\mathrm{token}}[x_{2}]+E_{\\mathrm{pos}}[2],\\cdots,E_{\\mathrm{token}}[x_{n}]+E_{\\mathrm{pos}}[n]].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Similarly, the unembedding layer is parameterized by a single matrix, and model predictions have the form: ", "page_idx": 2}, {"type": "equation", "text": "$$\np(x_{n+1}\\mid x_{1}...n;E,F,U)\\triangleq\\operatorname{softmax}\\Big(U h_{n}^{(m)}\\Big)\\,[x_{n+1}],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where (in a slight abuse of notation) $E,F$ and $U$ denote embedding, intermediate, and unembedding parameters respectively. ", "page_idx": 2}, {"type": "text", "text": "In terms of parameters, let the hidden dimension be $d$ , the number of layers be $m$ , the vocabulary size be $v$ and the maximum context length be $n$ , the embedding layers have $\\Omega((n+v)d)$ parameters as matrix $E_{\\mathrm{token}}$ and $U$ have shape $v\\times d$ and matrix $E_{\\mathrm{pos}}$ has shape $n\\times d$ , while the full network has an extra $\\Omega(m d^{2})$ parameters. ", "page_idx": 2}, {"type": "text", "text": "Initialization Models are trained via gradient descent from some random initial parameterization. Following Radford et al. [38], parameters of feed-forward la\u221ayers are initialized by sampling from isotropic Gaussians with mean 0 and standard deviation $0.02/\\dot{\\sqrt{2n}}$ . All the other weight matrices are initialized with 0-mean Gaussians with standard deviation 0.02. The affine transformations in layer normalizations are initialized as identity. ", "page_idx": 2}, {"type": "text", "text": "Training In this work, we examine language models with frozen intermediate layers, which we call random transformers. In these models, we fix the randomly chosen parameters intermediate layers, and train only the embedding layer $E$ and unembedding layer $U$ . Our experiments thus compare: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{Full\\,Training}\\!:\\,}&{\\underset{E,F,U}{\\mathrm{arg\\,min}}\\,\\sum_{x,n\\geq0}-\\log p(x_{n+1}\\mid x_{1}...n;E,F,U)}\\\\ {\\mathrm{Embedding\\-Only\\Iraining}\\!:\\,}&{\\underset{E,U}{\\mathrm{arg\\,min}}\\,\\sum_{x,n\\geq0}-\\log p(x_{n+1}\\mid x_{1}...n;E,F,U)}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the arg min is computed approximately via mini-batch stochastic gradient descent. ", "page_idx": 3}, {"type": "text", "text": "4 Random Transformers Can Perform Simple Algorithmic Tasks ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Can random transformers be steered to perform meaningful tasks by optimizing only input and output tokens\u2019 embeddings? We begin by evaluating four widely-studied tasks that serve as toy models of important behaviors in large-scale LMs. ", "page_idx": 3}, {"type": "text", "text": "4.1 Tasks ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Modular Addition This task evaluates models\u2019 ability to perform integer addition under a fixed prime modulus $p=199$ . Models receive a sequence of input tokens $[a,\\bar{b}]$ for $a,b\\in[0,p-1]$ and must compute $(a+b)$ mod $p$ . When over-parameterized models are trained to perform this task, grokking (a long period of memorization followed by an abrupt transition to generalization [36]) is typically observed [26, 18]. Neural sequence models of different kinds have been found to implement two interpretable algorithms, sometimes referred to as the \u201cClock\u201d [31] and \u201cPizza\u201d [52], when trained to perform this task. ", "page_idx": 3}, {"type": "text", "text": "Needle-in-a-Haystack This task evaluates models\u2019 abilities to process long input sequences [4]. In the variant we study, models receive as input a sequence of form $[m_{1},c_{1},m_{2},c_{2},\\cdot\\cdot\\cdot\\,,m_{k},c_{k},\\underline{{{m_{u}}}}]$ . Here, $m_{1},m_{2},\\cdot\\cdot\\cdot,m_{k}$ are distinct markers $[k\\leq30)$ and $c_{i}$ \u2019s are corresponding values. The input ends with a marker $\\underline{m}_{u}$ $(u\\in[1,k])$ , and models must search for the previous occurrence of that marker in the input sequence and output the corresponding $c_{u}$ . Specific circuits like induction heads are often observed in models that perform this task [34]. ", "page_idx": 3}, {"type": "text", "text": "Decimal Addition This task evaluates models\u2019 ability to perform arithmetic operations distributed over sequences of multiple input tokens\u2014in this case, addition of two equal-length numbers represented as digit sequences in base 10. The order of digits of both numbers and the results are reversed to simplify the task. For example, the task $39+71=116$ is encoded as a pair with input $9\\ 3\\ 1\\ \\ 7$ and output 0 1 1. We use 10-digit numbers in our setup. Past work has found that fully trained models can reliably learn some versions of this task [32, 51, 43]. ", "page_idx": 3}, {"type": "text", "text": "Parenthesis Balancing (Dyck Recognition) In this task, models are presented with a sequence of parentheses, and must predict whether they are balanced\u2014i.e., whether the sequence contain an equal number of opening and closing parentheses, and within every prefix, the number of closing parentheses is no greater than the opening parentheses. Such sequences are also called Dyck sequences, and have been widely studied in language models because of their connection to context-free models of natural language syntax [50, 47]. Note that this task has a vocabulary of size 4 (two parentheses and two labels), so only a very small number of parameters are optimized by embedding-only training. In our setup, the input parenthesis sequences have lengths at most 60. ", "page_idx": 3}, {"type": "text", "text": "For the modular addition task, we partition the full set of well-formed input\u2013output pairs into a fixed train/test split; for the other problems, we pre-generate a fixed test set but randomly generate new pairs for each training batch. Additional details may be found in Appendix D.1. ", "page_idx": 3}, {"type": "text", "text": "4.2 Results ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Results are shown in Table 1. Here we compare random transformers with a hidden size of 1024 to fully trained models with hidden sizes of 16 and 1024. For reference, we also compare to a (fully trained) LSTM, a recurrent neural sequence model [22]. All models have two hidden layers, and all results are aggregated across ten random initializations. ", "page_idx": 3}, {"type": "table", "img_path": "plH8gW7tPQ/tmp/c74cdc4768153cabadb9256d0fdf1bfd76cc05aa1f6e793f695a041929759a75.jpg", "table_caption": [], "table_footnote": ["Table 1: Test accuracy of fully trained and random transformers, as well as fully trained LSTMs, on algorithmic tasks. Denoted numbers (1024 and 16) are hidden sizes; results are median over 10 random restarts. Random models with only trained embeddings reliably perform all four tasks, and even outperform fully trained LSTMs. See Appendix E for the accuracy curve on multiple hidden sizes. "], "page_idx": 4}, {"type": "table", "img_path": "plH8gW7tPQ/tmp/e112dc964fc56c837d26e510383a5856e6eb89cabeab577a5da558cd352b5255.jpg", "table_caption": ["Table 2: Accuracy of embedding-only training with additional parameters fixed: optimizing only the unembedding layer, only the embedding layer, or only non-positional embeddings. Hidden sizes are all 1024; results are median over 10 random restarts. All three embedding matrices must be optimized for models to reliably complete all tasks. "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Random transformers learn to perform all four tasks Random transformers with trained embeddings and unembeddings obtain perfect accuracy on all four tasks consistently across restarts\u2014 sometimes outperforming fully trained recurrent networks (Table 1). These results thus point toward the role of a transformer-specific inductive bias in the effectiveness of embedding-only training. ", "page_idx": 4}, {"type": "text", "text": "In general, embedding and unembedding parameters must both be trained To further identify which model components must be optimized to obtain these results, we consider several variants of embedding-only training: (a) leaving both token and positional embeddings fixed (so only the unembedding is trained); (b) leaving the unembedding layer fixed (so only the embedding is trained); and (c) leaving positional embeddings fixed (so token embeddings and the unembedding is trained). Results are shown in Table 2. All variants fail to reach perfect accuracy on at least one task. Notably, the variant that only trains the unembedding is unable to reach near-perfect accuracy on any task. ", "page_idx": 4}, {"type": "text", "text": "Random transformers exhibit interpretable attention patterns A closer examination of the trained random models reveals similar mechanistic behaviors to their fully trained counterparts. For example, we observe attention patterns similar to \u201cinduction heads\u201d [34] previously described in fully trained models for associative recall tasks (Figure 2). ", "page_idx": 4}, {"type": "text", "text": "Random transformers use structured embeddings In the modular addition task, learned embeddings form circles in low-dimensional subspaces, another phenomenon observed in fully trained models for these tasks [26, 31] (Fig. 3). To better understand similarities between these models and their fully trained counterparts, we also computed the distance irrelevance and gradient symmetricity metrics described by Zhong et al. [52] for distinguishing between networks that perform modular arithmetic via the \u201cClock\u201d or \u201cPizza\u201d algorithm. We find a gradient symmetricity of 0.88 and a distance irrelevance of 0.88, consistent with a Clock-like solution. ", "page_idx": 4}, {"type": "image", "img_path": "plH8gW7tPQ/tmp/9afe7e4ecf03beefd02c30a18766730caf7beb179abf020ba2f5001e4e3701f2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 2: Attention patterns observed in a 2-layer 1024-width random transformer trained on the needle-in-a-haystack task. The input sequence is a 1 b 2 c 3 d 4 b. The layer-1 head is used by values to attend to their markers, and the layer-2 head is used by the query to attend to its associated value. ", "page_idx": 5}, {"type": "text", "text": "Figure 3: Circular embedding observed in random transformer on modular addition. We plot the projection of the embedding matrix onto its first and third principal components, with tokens colored according to their numeric value. ", "page_idx": 5}, {"type": "table", "img_path": "plH8gW7tPQ/tmp/27ba502faebe287ce292ae7d1accd5e5d994c1a7f601ca370ef6f2e196cc4ab8.jpg", "table_caption": ["Table 3: Comparison of normal and random transformer in the memorization task. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "5 Random Transformers Can Memorize and Generate Structured Sequences ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The preceding experiments evaluated the ability of random transformers to implement single, highly structured input\u2013output mappings. Can these models scale to more challenging tasks, involving memorization of arbitrary associations or even free-form text generation? ", "page_idx": 5}, {"type": "text", "text": "5.1 Memorization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Past work by Allen-Zhu and Li [2] has found that fully trained transformers can store roughly two bits of input per parameter. We investigate whether a similar scaling trend holds for random transformers. We study a simple memorization task in which we generate a random mapping from a two-integer key to a one-integer value, with all integers ranging from 1 to 512. Such a function requires 9 bits per input\u2013output mapping to represent $(\\log_{2}512=9)$ ), and may be defined for up to 262144 $\\left(=51\\Bar{2}^{2}\\right)$ ) values. Unlike the algorithmic tasks above, here the learned function must be fully specified by embeddings rather than the pre-trained model, and these experiments mainly evaluate how efficiently information can be stored in these embeddings. ", "page_idx": 5}, {"type": "text", "text": "We evaluate fully trained and random transformers of width 128 and 2 layers (Table 3). We measure success using an exact-match metric\u2014an input\u2013output pair is considered to be successfully memorized if the output token assigned highest probability by the model matches the training data. Fully trained transformers memorized $80\\%$ of training examples, stored 2.9 bits per parameter, while random transformers memorized only $5\\%$ of examples, corresponding to 0.4 bits per trainable parameter. ", "page_idx": 5}, {"type": "text", "text": "5.2 Language Modeling ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Modeling natural language requires both memorization of arbitrary associations (e.g. between words and their parts of speech, as in Section 5.1), and structured sequence generation procedures (e.g. to enforce subject\u2013verb agreement and close quotation marks, as in Section 4). Can random transformers make any headway on this task? ", "page_idx": 5}, {"type": "text", "text": "We train models on the TinyStories dataset, a collection of easy-to-understand stories generated by GPT-3.5 and GPT-4 [14] which have been shown to induce fluent text generation when used to train ", "page_idx": 5}, {"type": "image", "img_path": "plH8gW7tPQ/tmp/c1af652bd23de64fec5c57c1c4c601c4a2134d0c37e7894ff0c6d30b1dab86c8.jpg", "img_caption": ["Figure 4: Language modeling performances (measured in cross-entropy loss or equivalently log perplexity, lower is better) for fully trained and random transformers. Comparatively large hidden sizes are needed for random models to match the performance of fully trained models. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Once upon a time, there was a little boy named Max. Max loved to play with his toy blocks. He had blocks of all colors and shapes. Max liked to build tall towers and big castles. One day, Max was playing with his blocks in his room. He built the tallest tower he had ever made. It was so tall that it almost touched the ceiling! Max was very proud of his tower. Suddenly, ", "page_idx": 6}, {"type": "text", "text": "Reference (GPT-4o): ... Suddenly, Max\u2019s cat, Whiskers, ran into the room. Whiskers saw the tall tower and thought it was a fun thing to play with. He jumped up and knocked the tower over. The blocks fell all over the floor. Max was very sad. He had worked so hard to build the tower, and now it was gone. ", "page_idx": 6}, {"type": "text", "text": "Fully Trained Transformer: ... Suddenly, Max\u2019s little sister came into the room. She wanted to play with Max\u2019s blocks too. But Max didn\u2019t want to share. He wanted to keep playing with his blocks. His sister started to cry and Max didn\u2019t understand why his sister was so upset. He thought he was just playing with them. So, he went to his room and looked for the blocks. After a while, he found them under the bed. Max was so happy that he hugged his sister and his sister. They played together and had a lot of fun. The end. ", "page_idx": 6}, {"type": "text", "text": "Random Transformer (Completion 1): ... Suddenly, Max started to blow. Max saw a loud noise coming from the tower. Max\u2019s tower was very high and strong. Max wanted to play with his blocks. Max was very proud of himself for being so helpful. Max decided to take care of his tower and his tower. Max was so happy that he had done. Max wanted to keep himself. He had to be so much fun and proud of himself. Max was happy to be able to help. Max was able to save the tower and help his tower. Max was so proud of himself. Max was happy to have such a good tower and could do it. ", "page_idx": 6}, {"type": "text", "text": "Random Transformer (Completion 2): ... Suddenly, Max\u2019s tower fell down and landed on his floor. Max fell down and hurt his knee. Max cried and came to him. Max was very sad and didn\u2019t want to get hurt. Max\u2019s feelings. Max didn\u2019t want to get away. Max just wanted to make it so badly! Max tried to take it to break. Max put him back on the floor. Max tried to make up but Max didn\u2019t know what to do. Max was wrong. Max said, \"Don\u2019t worry, Max. Max are very bad and Max. Max didn\u2019t want to share it with Max. Max to play with Max and Max. Max was very sad. Max loved to play with his friends and Max played together and had lots of fun. Max played together all day. Max were happy that Max learned that sharing is caring. Max learned that being selfish and always had learned that sometimes things can share and others can make others happy again. ", "page_idx": 6}, {"type": "text", "text": "Figure 5: Sample completion generated by fully trained and random 512-width 2-layer transformers. While random models produce less coherent than fully trained models, they nonetheless generate text that is largely grammatical topically appropriate. ", "page_idx": 6}, {"type": "text", "text": "smaller models. As in previous sections, we evaluate both embedding-only and full training with 2- and 4-layer transformers with various hidden sizes. Scaling curves are shown in Fig. 4. Fully trained models obtain a cross-entropy loss of 1.35, on par with the results reported by Eldan and Li [14]. Our trained random transformer with 512 width and $\\mathord{\\sim}10\\ensuremath{\\mathrm{M}}$ trainable parameters achieved a cross-entropy loss of 2.64, roughly on par with the fully trained ones with 32 width and only ${\\sim}0.7\\mathbf{M}$ trainable parameters. Moreover, adding additional hidden layers does not appear to improve performance performance\u20142-layer random transformers in fact achieve better losses than 4-layer models. ", "page_idx": 6}, {"type": "text", "text": "Example outputs sampled from these models (on a newly generated prompt) are shown in Fig. 5. Even though random models obtain significantly worse perplexity than fully trained models, they could still perform several key sub-tasks needed for language generation: generated sentences are generally grammatical, topically coherent with the prompt, correctly resolve some long-distance dependencies (e.g. references to the name Max) and perhaps even high-level narrative structure. ", "page_idx": 7}, {"type": "text", "text": "6 Random Transformers Operate in Low-Dimensional Subspaces ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Can we explain the success of random transformers in the tasks studied above? In this section, we present evidence that embedding-only training steers the hidden computation in transformers into low-dimensional subspaces in which target functions are already implemented. We term this phenomenon subspace selection, and show that it is distinct from sparsification, as these subspaces are distributed across neurons. ", "page_idx": 7}, {"type": "text", "text": "In Section 6.1, we measured fraction of activation variance explained by top principal components in various tasks. For algorithmic tasks, we show that both normal and random transformers work in low-dimensional subspaces, which are sufficient for solving these tasks (Appendix F). However, for language modeling and memorization, the random transformers displayed more subspace selection compared to the fully trained ones, and as a result, they attained lower performances. In Section 6.2 we constructed a task that explicitly requires operating on high-dimensional spaces, circuit imitation, and indeed, the random transformers exhibit significant performance gap compared to normal transformers. ", "page_idx": 7}, {"type": "text", "text": "6.1 Low-Dimensional Hidden Representations in Algorithmic and Memorization Tasks ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To characterize the geometry of random transformers\u2019 internal representations, we present models (trained for all previously described tasks) with a set of randomly chosen inputs and collect their embeddings and hidden representations of these inputs at different layers. Using these representations, we perform two analyses: (1) the fraction of variance explained by the top principal components of these hidden representations (which will be large if representations lie in a low-dimensional subspace), and (2) the fraction of variance explained by the most variable entries in hidden state vectors, or neurons (which will be large if computation is confined to a sparse sub-network). ", "page_idx": 7}, {"type": "text", "text": "Both fully trained and random transformers exhibit subspace selection but not sparsification (Table 4 top) in the four algorithmic tasks. In Appendix F, we show that this behavior is expected, insofar as all four algorithmic tasks can be solved by shallow transformer-like circuits that operate in lowdimensional subspaces. On memorization and language modeling tasks, random transformers become much more concentrated on a small subspace than fully trained transformers, thus using a lower effective dimension (Table 4 bottom and Table 5). In the language modeling task, more than $30\\%$ of variance in hidden representations is explained by 10 components. ", "page_idx": 7}, {"type": "text", "text": "6.2 Subspace Selection in Circuit Imitation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To provide another window into these results, we characterize how large the hidden representations of a random model must be for it to simulate a random circuit that operates in a low-dimensional subspace. ", "page_idx": 7}, {"type": "text", "text": "To do so, we first generate a small, random target transformer associated with a distribution over strings $\\tilde{p}$ , then perform embedding-only training in a different, randomly initialized transformer to simulate its behavior on some domain of interest by minimizing: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\underset{E,U}{\\arg\\operatorname*{min}}\\ \\ \\mathbb{E}_{x}[\\mathrm{KL}(\\tilde{p}(\\cdot\\mid x)\\parallel p(\\cdot\\mid x;E,F,U)]\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "To construct target models, we begin with the same initialization scheme described in Section 3, then we scale the query and key parameters in the attention mechanism by a factor of 10, and and t\u221ahe feed forward weights and biases by a factor of 20. We also scale the final projection layer by $100/\\sqrt{\\mathrm{width}}$ . (This initialization scheme increases variability of attention patterns and target model predictions across random restarts; see Appendix D.2.2 for additional discussion.) ", "page_idx": 7}, {"type": "table", "img_path": "plH8gW7tPQ/tmp/9f656b35cb3cafba0b37c3c585ae6199eed7f8d3b2018aff612def4e928d5414.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 4: Median explained variance from top 10 directions under principal and neuron basis. Activations after embedding $\\left(E m b\\right)$ , layer 1 $(L I)$ and layer 2 $(L2)$ are collected from multiple trained 2-layer models of width 1024 and 128 (for memorization). Normal transformers are fully trained while Random transformers have only embedding and unembedding layers trained, as in previous experiments. Across tasks, a large fraction variance in models\u2019 hidden representations is explained by a small number of principal components, but these components do not appear to be aligned to individual neurons or sparse sub-networks. ", "page_idx": 8}, {"type": "table", "img_path": "plH8gW7tPQ/tmp/8db0c0c5b05739970c39f2bf358cb13e220fee17e4b63cb2326843a299956820.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 5: Median explained variance from top 10 directions under principal and neuron basis for language modeling task. Activations after embedding $\\left(E m b\\right)$ and after every layer $(L I,L2,L3,L4)$ are collected from trained 4-layer models of width 512. As above, a substantial fraction of variance is explained by a small number of principal components, especially in random transformers. ", "page_idx": 8}, {"type": "text", "text": "We evaluate fully trained and random transformers\u2019 ability to fit the target distribution for target models with a 512-token vocabulary, three layers, two attention heads, and varying hidden dimensions. Training inputs $x$ consist of 40-token sequences generated uniformly at random. ", "page_idx": 8}, {"type": "text", "text": "Results are shown in Fig. 6. In general, random transformers can only match the behavior of shallower (3 vs 1) and or significantly narrower (512 vs 128) models, with a sharp increase in error moving from $12\\rightarrow16\\rightarrow32\\cdot$ -dimensional models, suggesting that random transformers may only be able to learn computations that can be performed in lower-dimensional subspaces. ", "page_idx": 8}, {"type": "text", "text": "7 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We have shown that transformer sequence models can accomplish a variety of meaningful tasks when only their embedding layers are optimized. For tasks involving memorization, these results show that much of models\u2019 \u201cknowledge\u201d can be encapsulated by input embeddings rather than models\u2019 internal parameters. For more algorithmic tasks like arithmetic and parenthesis balancing, which require relatively sophisticated circuits to perform, our experiments show that versions of these circuits can be accessed in random transformers (but not LSTM sequence models) simply by constructing appropriate input and output embeddings that confine models\u2019 internal states to low-dimensional subspaces in which these tasks are performed. ", "page_idx": 8}, {"type": "image", "img_path": "plH8gW7tPQ/tmp/ea75c9bb402e714fcb4ff0f1de736e4795e2a1068d234431f15588c1c255ee7f.jpg", "img_caption": ["Figure 6: Kullback\u2013Leibler divergence of circuit imitation with fully trained and random transformers (the lower the better). Both plots show the same set of results with different scales (linear and log) on the vertical axis. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "However, our experiments have also highlighted several important differences between embeddingonly and fully trained models, especially with regard to their parameter efficiency and information capacity. This paper leaves open the question of how computation in embedding-only models relates to fully trained ones\u2014e.g. whether, during full training, the mechanisms we have discovered here evolve gradually into their fully trained forms, or whether fully trained models use entirely different pathways [10]. ", "page_idx": 9}, {"type": "text", "text": "We anticipate that these random transformers will also provide an interesting new test-bed for interpretability research, and future work might investigate how learned feature codebooks [12, 6] and automated neuron labeling procedures [21, 5, 33] behave when applied to these models. Even more generally, these results motivate a closer study of the behavior of untrained models as a source of insight into differences between, and improvements upon, existing neural model architectures. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We would like to thank MIT SuperCloud for computational resources and Mingyang Deng for valuable discussions. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Ekin Aky\u00fcrek, Bailin Wang, Yoon Kim, and Jacob Andreas. In-context language learning: Arhitectures and algorithms. arXiv preprint arXiv:2401.12973, 2024.   \n[2] Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.3, knowledge capacity scaling laws. arXiv preprint arXiv:2404.05405, 2024.   \n[3] Ehsan Amid, Rohan Anil, Wojciech Kot\u0142owski, and Manfred K Warmuth. Learning from randomly initialized neural network features. arXiv preprint arXiv:2202.06438, 2022.   \n[4] Anthropic. Long context prompting for Claude 2.1, 2023. URL https://www.anthropic. com/news/claude-2-1-prompting.   \n[5] Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan Leike, Jeff Wu, and William Saunders. Language models can explain neurons in language models. OpenAI, 2023. URL https://openai.com/index/ language-models-can-explain-neurons-in-language-models/.   \n[6] Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Zac Hatfield-Dodds, Alex Tamkin, Karina Nguyen, Brayden McLean, Josiah E Burke, Tristan Hume, Shan Carter, Tom Henighan, and Christopher Olah. Towards monosemanticity: Decomposing language ", "page_idx": 9}, {"type": "text", "text": "models with dictionary learning. Transformer Circuits Thread, 2023. https://transformer", "page_idx": 10}, {"type": "text", "text": "circuits.pub/2023/monosemantic-features/index.html.   \n[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n[8] Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. In Seventh International Conference on Learning Representations, pages 1\u201317, 2019.   \n[9] Devendra Singh Chaplot, Kanthashree Mysore Sathyendra, Rama Kumar Pasumarthi, Dheeraj Rajagopal, and Ruslan Salakhutdinov. Gated-attention architectures for task-oriented language grounding. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.   \n[10] Angelica Chen, Ravid Schwartz-Ziv, Kyunghyun Cho, Matthew L Leavitt, and Naomi Saphra. Sudden drops in the loss: Syntax acquisition, phase transitions, and simplicity bias in mlms. arXiv preprint arXiv:2309.07311, 2023.   \n[11] Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, and Michael Carbin. The lottery ticket hypothesis for pre-trained BERT networks. Advances in neural information processing systems, 33:15834\u201315846, 2020.   \n[12] Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. Sparse autoencoders find highly interpretable features in language models. arXiv preprint arXiv:2309.08600, 2023.   \n[13] Gr\u00e9goire Del\u00e9tang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Li Kevin Wenliang, Elliot Catt, Chris Cundy, Marcus Hutter, Shane Legg, Joel Veness, et al. Neural networks and the chomsky hierarchy. arXiv preprint arXiv:2207.02098, 2022.   \n[14] Ronen Eldan and Yuanzhi Li. Tinystories: How small can language models be and still speak coherent english? arXiv preprint arXiv:2305.07759, 2023.   \n[15] Gamaleldin F Elsayed, Ian Goodfellow, and Jascha Sohl-Dickstein. Adversarial reprogramming of neural networks. In Seventh International Conference on Learning Representations, 2019.   \n[16] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In International Conference on Learning Representations, 2018.   \n[17] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583\u201330598, 2022.   \n[18] Andrey Gromov. Grokking modular arithmetic. arXiv preprint arXiv:2301.02679, 2023.   \n[19] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023.   \n[20] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs). arXiv preprint arXiv:1606.08415, 2016.   \n[21] Evan Hernandez, Sarah Schwettmann, David Bau, Teona Bagashvili, Antonio Torralba, and Jacob Andreas. Natural language descriptions of deep visual features. In International Conference on Learning Representations, 2021.   \n[22] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735\u20131780, 1997.   \n[23] Kevin Jarrett, Koray Kavukcuoglu, Marc\u2019Aurelio Ranzato, and Yann LeCun. What is the best multi-stage architecture for object recognition? In 2009 IEEE 12th international conference on computer vision, pages 2146\u20132153. IEEE, 2009.   \n[24] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:3843\u20133857, 2022.   \n[25] Yingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms: Generalization and stability in in-context learning. In International Conference on Machine Learning, pages 19565\u201319594. PMLR, 2023.   \n[26] Ziming Liu, Ouail Kitouni, Niklas S Nolte, Eric Michaud, Max Tegmark, and Mike Williams. Towards understanding grokking: An effective theory of representation learning. Advances in Neural Information Processing Systems, 35:34651\u201334663, 2022.   \n[27] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.   \n[28] Cedric Lothritz, Kevin Allix, Lisa Veiber, Jacques Klein, and Tegawend\u00e9 Fran\u00e7ois D Assise Bissyande. Evaluating pretrained transformer-based models on the task of fine-grained named entity recognition. In 28th International Conference on Computational Linguistics, 2020.   \n[29] Mantas Luko\u0161evic\u02c7ius and Herbert Jaeger. Reservoir computing approaches to recurrent neural network training. Computer science review, 3(3):127\u2013149, 2009.   \n[30] Arindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah. Orca-Math: Unlocking the potential of SLMs in grade school math. arXiv preprint arXiv:2402.14830, 2024.   \n[31] Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress measures for grokking via mechanistic interpretability. In International Conference on Learning Representations, 2023.   \n[32] Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. Investigating the limitations of transformers with simple arithmetic tasks. arXiv preprint arXiv:2102.13019, 2021.   \n[33] Tuomas Oikarinen and Tsui-Wei Weng. Clip-dissect: Automatic description of neuron representations in deep vision networks. In International Conference on Learning Representations, 2023.   \n[34] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022.   \n[35] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023.   \n[36] Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overftiting on small algorithmic datasets. arXiv preprint arXiv:2201.02177, 2022.   \n[37] Philip Quirke and Fazl Barez. Understanding addition in transformers. arXiv preprint arXiv:2310.13121, 2023.   \n[38] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \n[39] Vivek Ramanujan, Mitchell Wortsman, Aniruddha Kembhavi, Ali Farhadi, and Mohammad Rastegari. What\u2019s hidden in a randomly weighted neural network? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11893\u201311902, 2020.   \n[40] Andrew M Saxe, Pang Wei Koh, Zhenghao Chen, Maneesh Bhand, Bipin Suresh, and Andrew Y Ng. On random weights and unsupervised feature learning. In ICML, volume 2, page 6, 2011.   \n[41] David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models. In International Conference on Learning Representations, 2018.   \n[42] Benjamin Schrauwen, David Verstraeten, and Jan Van Campenhout. An overview of reservoir computing: theory, applications and implementations. In Proceedings of the 15th european symposium on artificial neural networks. p. 471-482 2007, pages 471\u2013482, 2007.   \n[43] Ruoqi Shen, S\u00e9bastien Bubeck, Ronen Eldan, Yin Tat Lee, Yuanzhi Li, and Yi Zhang. Positional description matters for transformers arithmetic. arXiv preprint arXiv:2311.14737, 2023.   \n[44] Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple and effective pruning approach for large language models. arXiv preprint arXiv:2306.11695, 2023.   \n[45] Yun-Yun Tsai, Pin-Yu Chen, and Tsung-Yi Ho. Transfer learning without knowing: Reprogramming black-box machine learning models with scarce data and limited resources. In International Conference on Machine Learning, pages 9614\u20139624. PMLR, 2020.   \n[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[47] Kaiyue Wen, Yuchen Li, Bingbin Liu, and Andrej Risteski. Transformers are uninterpretable with myopic methods: a case study with bounded dyck grammars. Advances in Neural Information Processing Systems, 36, 2024.   \n[48] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, et al. Huggingface\u2019s transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.   \n[49] Chao-Han Huck Yang, Yun-Yun Tsai, and Pin-Yu Chen. Voice2series: Reprogramming acoustic models for time series classification. In International conference on machine learning, pages 11808\u201311819. PMLR, 2021.   \n[50] Shunyu Yao, Binghui Peng, Christos Papadimitriou, and Karthik Narasimhan. Self-attention networks can process bounded hierarchical languages. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3770\u20133785, 2021.   \n[51] Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, and Songfang Huang. How well do large language models perform in arithmetic tasks? arXiv preprint arXiv:2304.02015, 2023.   \n[52] Ziqian Zhong, Ziming Liu, Max Tegmark, and Jacob Andreas. The clock and the pizza: Two stories in mechanistic explanation of neural networks. Advances in Neural Information Processing Systems, 36, 2024.   \n[53] Hattie Zhou, Janice Lan, Rosanne Liu, and Jason Yosinski. Deconstructing lottery tickets: Zeros, signs, and the supermask. Advances in neural information processing systems, 32, 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Supplementary material ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Even within the class of transformers, the space of architectural decisions (both around model size and implementation of attention mechanisms, normalization procedures, tokenization, etc.) is very large; our experiments in this paper generally characterize a small part of this phase space. It is thus possible that some of the described trends will change as models grow or differ in parameterization details. Outside Section 4, our experiments have focused on standard transformer models, and do not answer whether these trends hold in other related linear attention [9, 35] or state-space model [19] families. Our discussion in Section 6 focused only on linear subspaces and used principal component analysis as the primary tool, but it is also possible that subspaces or latent semantics appear non-linearly which is not addressed by our current analysis. ", "page_idx": 13}, {"type": "text", "text": "B Impact statement ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We do not anticipate any ethical concerns associated with these results and we believe understanding AI systems is a crucial step in harnessing their power for good. ", "page_idx": 13}, {"type": "text", "text": "C Computational Resources ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Roughly 154 GPU days of NVidia V100 were spent on this project. ", "page_idx": 13}, {"type": "text", "text": "D Setup details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "D.1 Data Curation and Tokenization Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "D.1.1 Modular Addition ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Fix modulus $p=199$ . We randomly shuffle all possible inputs $\\textstyle p^{2}$ of them) perform a $95\\%$ : $5\\%$ for training and test set. The split is the same (generated with the same seed and procedure) for all runs across all architectures. ", "page_idx": 13}, {"type": "text", "text": "D.1.2 Dynamically Generated Tasks ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "For these tasks, we used a stream of training data and a heldout test set. The test set is fixed for all runs across all architectures. The training data is generated on the fly within the same distribution to avoid overfitting. ", "page_idx": 13}, {"type": "text", "text": "Needle-in-a-Haystack The number of entities (marker-value pairs) is first uniformly generated in [1, 30]. The values are generated as integers in [1, 127]. The markers are generated as distinct integers in $[127+1,128+30]$ . The final query token is the asked marker (uniformly chosen) plus 30. For example, a $1\\texttt{b2c3d4b}$ in token ids would be [128, 1, 129, 2, 130, 3, 131, 4, 159] and the expected output will be token 2. ", "page_idx": 13}, {"type": "text", "text": "Decimal Addition Fix the number of digits $l=10$ . The two numbers to be added are independently uniformly generated within $[10^{9},10^{10}\\ -1]$ . The added numbers and the results are reversed. The addition sign has token id 10 and the equal sign has id 11. For example, $\\scriptstyle111111112+222222223=$ in token ids would be $[2,1,\\cdot\\cdot\\cdot,1,10,3,2,\\cdot\\cdot\\cdot,2,11]$ . The result uses [20, 29] to represent digits and 30 to signal the end of output. For the example, the expected output will be 3333333335 encoded as $[25,23,\\cdot\\cdot\\cdot\\,,23,30]$ . ", "page_idx": 13}, {"type": "text", "text": "Parentheses Balancing ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The opening parenthesis has token id 1, the closing parenthesis has token id 2, and the question mark has token id 3. For the result, 2 signals balanced and 1 signals unbalanced. For example, (())()? in token ids will be $[1,1,2,2,1,2,3]$ and the expected output is balanced, token 2. ", "page_idx": 13}, {"type": "text", "text": "As sequences of uniformly random parentheses are (a) likely unbalanced (b) easier to validate, we performed a generate-then-mutate strategy to generate strong data for the task. ", "page_idx": 14}, {"type": "text", "text": "Generate With $1/3$ probability, we generate a random sequence of at most 60 parentheses (length uniformly chosen, then independently all the parentheses). With $2/3$ probability, we generate a balanced parentheses sequence. We first uniformly choose the number of pairs of parentheses $t$ in [1, 30]. We then generate recursively: when $t\\geq2$ , with $1/2$ probability we generate a $(t-1)$ - parentheses sequence recursively and add a pair of parentheses on the outside, with $1/2$ probability we uniformly sample $u\\in[1,t-1]$ and output the concatenation of a $u$ -parentheses and a $(t-u)$ - parentheses sequence generated recursively. ", "page_idx": 14}, {"type": "text", "text": "Mutate With $1/2$ probability, we choose some random index pairs and swap the corresponding parentheses. Independently with $1/2$ probability, we (then) choose some random indices and filp the corresponding parentheses. The number of pairs and indices are sampled according to a geometric distribution. ", "page_idx": 14}, {"type": "text", "text": "Circuit Imitation The inputs are uniformly random 40-token sequences of integer tokens [0, 511].   \nThe target output is the distribution from a target transformer generated as in Appendix D.2.2 below.   \nThe width of the target transformer is chosen in [4, 128] (Figure 6). ", "page_idx": 14}, {"type": "text", "text": "D.1.3 Memorization ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For every integer pair $x\\in[0,511]$ , $y\\,\\in\\,[512,512+511]$ , we generate one data point with input $[x,y]$ and output $z$ uniform in $[0,511]$ . There is no \u201ctest split\u201d as the goal is to memorize all the associations. ", "page_idx": 14}, {"type": "text", "text": "D.1.4 Language Modeling ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We used the original train-test split of the TinyStories dataset [14]. We trained a 10000-token BPE tokenizer from the training split alone. ", "page_idx": 14}, {"type": "text", "text": "D.2 Model Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "D.2.1 Transformers ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We use the GPT-2 [38] implementation of Huggingface [48]. Dropout and weight tying are disabled.   \nThe activation function is kept as the default GeLU [20]. ", "page_idx": 14}, {"type": "text", "text": "Specifically, all the weights of feed-forward laye\u221ars are initialized by sampling from isotropic Gaussians with mean 0 and standard deviation $0.02/\\sqrt{2n}$ where $n$ in the number of layers. All the bias matrices are initialized with zeroes. All the other weight matrices (including key, value, query, embedding, unembedding matrices) are initialized with 0-mean Gaussians with standard deviation 0.02. The affine transformations in layer normalizations are initialized as identity. ", "page_idx": 14}, {"type": "text", "text": "We used two layer transformers except for the language modeling and circuit imitation task. The number of parameters for the algorithmic tasks can be found in Table 6. ", "page_idx": 14}, {"type": "table", "img_path": "plH8gW7tPQ/tmp/8df7b90f0cbd3338ae331ad6b01867552cc68d6ab787d5c6d6ad04deb2191ecd.jpg", "table_caption": [], "table_footnote": ["Table 6: Number of parameters of each type for the 2-layer 1024-width transformers in Section 4. "], "page_idx": 14}, {"type": "text", "text": "D.2.2 Target Transformer in Circuit Imitation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Based on the initialization in Appendix D.2.1, we make the following modifications. ", "page_idx": 14}, {"type": "text", "text": "Feed-forward Layers: Standard deviation scaled up by $20\\mathrm{x}$ : changed to $0.4/\\sqrt{2n}$ . ", "page_idx": 14}, {"type": "table", "img_path": "plH8gW7tPQ/tmp/ef8ac1f4faabf228401ea55d28f30a8e6fdb139dd95a4758dffa20929d1033e4.jpg", "table_caption": [], "table_footnote": ["Table 7: Median explained variance from top 10 directions under principal and neuron basis collected from width 512 transformers. Rounded to one decimal piece. See Table 4 for more details. "], "page_idx": 15}, {"type": "text", "text": "Attention Matrices (key, value, query): Standard deviation scaled up by $10\\mathrm{x}$ : changed to 0.4. ", "page_idx": 15}, {"type": "text", "text": "Unembedding Matrix: Standard deviation changed to $2/\\sqrt{n}$ . ", "page_idx": 15}, {"type": "text", "text": "After such modifications, we measured the entropy of output distribution on two random inputs. Across all the widths, the mean entropy stays within [2.89, 3.02]. We also measured similarity of output distributions on different inputs to prevent mode-collapse-like results, and the mean KL divergence of output distributions on random inputs is [0.8, 3.3] across all widths. ", "page_idx": 15}, {"type": "text", "text": "D.2.3 LSTM ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We used textbook long short-term memory [22] with an encoding layer and an unembedding (projection) layer added. ", "page_idx": 15}, {"type": "text", "text": "D.3 Training Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For synthetic experiments, we used AdamW optimizer [27] with a learning rate $10^{-3}$ and weight decay $10^{-3}$ . For LSTM a learning rate $5\\times10^{\\,\\pm\\,3}$ is used for faster convergence. For the language modeling task, we used AdamW optimizer with a learning rate $6\\times10^{-4}$ and weight decay 0.1. We clip all gradient norms at 1. ", "page_idx": 15}, {"type": "text", "text": "For random transformer experiments, the intermediate (query, embedding, unembedding, feedforward) layers are kept as randomly initialized. We use a fixed number of training steps and no early stopping. ", "page_idx": 15}, {"type": "text", "text": "Modular Addition: 5000 epoches. Batch size 4000. ", "page_idx": 15}, {"type": "text", "text": "Needle-in-a-Haystack, Decimal Addition, Parentheses Balancing, Circuit Imitation: $10^{4}$ steps of batch size 1000. Again, the training data is generated dynamically. ", "page_idx": 15}, {"type": "text", "text": "Memorization: 21000 epoches. Batch size $2^{15}$ ", "page_idx": 15}, {"type": "text", "text": "Language Modeling: 5 epoches. Batch size 20 and context window 512. ", "page_idx": 15}, {"type": "text", "text": "E Performance on synthetic tasks across model scales ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In Fig. 7 we provide the test accuracy of needle-in-a-haystack, decimal addition and parenthese balancing for fully trained and random transformers across different model widths. Note that the 1024-width fully trained transformer had trouble reaching perfect accuracy in parentheses balancing, likely due to imperfect hyperparameter choices. ", "page_idx": 15}, {"type": "text", "text": "We also include the explained variance measurements on 512-width models in these three tasks for completeness (Table 7). Generally more variances are explained from the top directions as the models are narrower. ", "page_idx": 15}, {"type": "image", "img_path": "plH8gW7tPQ/tmp/3330c2bd6892aa96c727463f53a2a9ce8cae2c187cbece49ad8da6f33a9bfb36.jpg", "img_caption": ["Figure 7: Box plot of test accuracy of synthetic tasks across model scales. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "F Constant-dimensional subspaces are sufficient for many synthetic tasks ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Definition F.1 (Intermediate complexity of tasks (informal)). For a family of neural networks with varying intermediate width, we define the intermediate complexity of a task as the minimal width of intermediate layers required to succeed in the task. ", "page_idx": 16}, {"type": "text", "text": "For example, a 2-layer transformer with intermediate width 2 starts by embed the tokens into the 2-dimensional hidden space, passes them through two transformer blocks of width 2, then linearly project the activation for the final output (unembedding). If one task could be solved with one such transformer, its intermediate complexity for 2-layer transformers would be at most 2. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "If we consider the activations as the working memory of the LLM, intuitively a task has low intermediate complexity if it requires a low working memory per token. In the following, we show that all the synthetic tasks we proposed indeed have constant intermediate complexity for 2-layer transformers, thus they only require a constant-dimensional intermediate subspaces to work on. ", "page_idx": 17}, {"type": "text", "text": "Modular Addition We may use a transformer to implement three instances of the Pizza algorithm described in Zhong et al. [52] (multiple copies are needed to avoid the \u201cantipodal pairs\u201d problem described there). Each implementation of this algorithm requires intermediate representations of size 2, so the intermediate complexity for 2-layer transformers is $\\leq6$ . ", "page_idx": 17}, {"type": "text", "text": "Needle-in-a-Haystack We may use a transformer to implement induction heads [34] as follows: We first store each token\u2019s preceding token in its activations by attending to the previous position. We then retrieve the answer by attending to the position storing the matching token in the previous step. Both steps could be done by using attention to compute squares of differences and take the minimum. The number of hidden states needed to distinguish each symbol scales only with the size of the vocabulary, and is at most equal to the vocabulary size, so the intermediate complexity is at most twice the vobaulary size. ", "page_idx": 17}, {"type": "text", "text": "Parentheses Balancing Let the given sequence of parentheses be $p_{1},p_{2},\\cdot\\cdot\\cdot\\,,p_{n}$ . Let $\\textstyle x_{i}=\\sum_{j=1}^{i}c_{i}$ where $c_{i}=1$ if $p_{i}$ is an open parenthesis and $c_{i}=-1$ if $p_{i}$ is a closing parenthesis. The given sequence is a valid parentheses sequence if and only if $x_{n}=0$ and $\\mathrm{min}_{i=1}^{n}\\,x_{i}=0$ . In a transformer, we can first compute $x_{i}/i$ with a uniform attention. Then, we attend to the position with minimum $x_{i}/i$ , breaking ties by position (let the attention on $i$ pre-softmax be $T(-x_{i}/i+\\epsilon\\cdot i)$ for large enough $T$ and small enough $\\epsilon>0$ ). For valid parentheses sequences, the last position should only attend to itself. We then check if the attended position is indeed the last one and has $x_{i}/i=0$ by computing squares of differences. The intermediate complexity for 2-layer transformers is thereby again constant. (note that a different construction for bounded-depth Dyck was given in [50]) ", "page_idx": 17}, {"type": "text", "text": "Decimal Addition ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Let the reversed list of digits of two numbers and their sum be $a_{1},a_{2},\\cdot\\cdot\\cdot,a_{n},b_{1},b_{2},\\cdot\\cdot\\cdot,b_{n}$ , and $c_{1},c_{2},\\cdot\\cdot\\cdot.$ Let $\\begin{array}{r}{x_{i}=\\sum_{j=1}^{i}a_{j}10^{j-i},y_{i}=\\sum_{j=1}^{i}b_{j}10^{j-i},z_{i}=\\sum_{j=1}^{i-1}c_{j}10^{j-i}}\\end{array}$ , we have $c_{i}=(x_{i}+$ $y_{i}-z_{i})$ mod 10: $\\begin{array}{r}{a+b\\equiv\\sum_{j=1}^{i}10^{j-1}(a_{j}+b_{j})}\\end{array}$ $(\\mathrm{mod}~10^{i})$ , $(a+b)$ mod $\\begin{array}{r}{10^{i-1}=\\sum_{j=1}^{i-1}c_{j}10^{j-1}}\\end{array}$ , and $c_{i}=((a+b)\\bmod{10^{i}}-(a+b)\\bmod{10^{i-1}})/10^{i-1}.$ . ", "page_idx": 17}, {"type": "text", "text": "Prepare: Take small $\\epsilon>0$ . For positions $1,2,\\cdots\\,,n$ , let the positional embedding of position $i$ be $[1,\\bar{10}^{i},10^{-i}\\epsilon]$ . In the first head of the first layer, take the first dimension in the positional embedding for both $\\mathrm{\\DeltaQ}$ and $\\mathbf{K}$ , so we get a uniform attention on the prefix from which $\\begin{array}{r}{\\alpha_{i}=\\frac{1}{i}\\sum_{j=1}^{i}a_{j}}\\end{array}$ can be calculated. In the second head of the first layer, take the third dimension in the positional embedding for $\\mathrm{\\DeltaQ}$ and the second dimension in the positional embedding for K, so the contribution from the j-th position to the i-th position is Z1i e $\\begin{array}{r}{\\frac{1}{Z_{i}}e^{\\epsilon10^{j-i}}\\,\\approx\\,\\epsilon\\frac{1}{Z_{i}}(10^{j-i}+1)}\\end{array}$ $\\left(e^{c}\\approx1+c$ for $0\\,<\\,c\\,\\ll\\,1)$ ) for normalizing constant $Z_{i}$ , we can then calculate $\\begin{array}{r}{\\beta_{i}=\\epsilon\\frac{1}{Z_{i}}\\sum_{j=1}^{i}a_{j}(10^{j-i}+1)}\\end{array}$ . We then have $\\begin{array}{r}{x_{i}=\\frac{Z_{i}}{\\epsilon}\\beta_{i}-i\\alpha_{i}}\\end{array}$ which we will utilize in the next step. Similarly, we can have $y_{i}$ and $z_{i}$ ready in the corresponding position (position of $b_{i}$ and $c_{i-1,}$ ). ", "page_idx": 17}, {"type": "text", "text": "Generate $x_{i}+y_{i}-z_{i}$ : In the second layer, attend from the position of $a_{i}$ and $b_{i}$ at the position of $c_{i-1}$ . Specifically, set the attention from the $a_{k}$ \u2019s position to the $c_{i-1}$ \u2019s position be $\\epsilon i-\\bar{\\Lambda}\\cos((k-$ $i)/n)\\,\\bar{=}\\,\\epsilon i-\\Lambda(\\cos(k/n)\\cos(i/n)+\\sin(k/n)\\sin(i/n))$ pre-softmax. This could be done using three dimensions in the positional embeddings. We also set the attention from the plus sign to the $c_{i-1}$ \u2019s position be 1 pre-softmax. The attention from $a_{k}$ to $c_{i-1}$ will be negligible if $k\\neq i$ and will be proportional to $\\epsilon i$ post-softmax for $k=i$ . We can then calculate $-i\\alpha_{i}$ from it and similarly $\\scriptstyle{\\frac{Z_{i}}{\\epsilon}}\\beta_{i}$ , and thus $x_{i},y_{i}$ and $z_{i}$ . ", "page_idx": 17}, {"type": "text", "text": "Approximate the Answer: As $x_{i}+y_{i}-z_{i}$ is an integer from [0, 19] and we can also compute an affine transform of it, we may now simply proceed with the universal approximation theorem. As an example, in ReLU networks we may check if $x_{i}+y_{i}-z_{i}=v$ by noticing ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "F $\\operatorname{sLU}(x_{i}+y_{i}-z_{i}-(v+1))+\\operatorname{ReLU}(x_{i}+y_{i}-z_{i}-(v-1))-2\\operatorname{ReLU}(x_{i}+y_{i}-z_{i}-v)=[x_{i}+y_{i}-z_{i}=v],$ So to create an indicator variable $[x_{i}+y_{i}-z_{i}\\,=\\,1]$ we simply need to generate $x_{i}+y_{i}-z_{i}$ , $x_{i}+y_{i}-z_{i}-1,x_{i}+y_{i}-z_{i}-2$ , pre-ReLU. ", "page_idx": 18}, {"type": "text", "text": "The intermediate complexity for 2-layer transformers is thereby again constant. This approach is quite crude so it is likely that the constant can be greatly improved. ", "page_idx": 18}, {"type": "text", "text": "G Difficulty of the synthetic tasks ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Chomsky hierarchy has been found to be predictive of the neural architectures\u2019 performances, and especially length-generalization performances, in algorithmic setups [13]. We list the Chomsky hierarchy of the synthetic tasks we picked in Table 8. ", "page_idx": 18}, {"type": "table", "img_path": "plH8gW7tPQ/tmp/65289f8abdffb690ffb41a45432fe24b2625779561fca7ec7d24a17f5a1981ca.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Table 8: Classification of tasks according to the Chomsky hierarchy. For the decimal addition task, we consider the most flexible setting where all the strings representing decimal additions (does not have to be equal-lengthed) are considered within the language. ", "page_idx": 18}, {"type": "text", "text": "We also examined a baseline with linearized transformer. In this variant of transformer, the attention matrix (softmax $(Q K^{T}))$ is replaced with a lower diagonal matrix of 1s. In other words, the attention is replaced by a simple token prefix sum mechanism. We tested such transformers with width 512 and 2 layers on the synthetic tasks as a baseline performance. The result is shown in Table 9. We can see that such transformers have large performance gaps with normal transformers, confirming the difficulty of our chosen tasks. ", "page_idx": 18}, {"type": "table", "img_path": "plH8gW7tPQ/tmp/2b07831dbdf6777f12fe8374aac02c0f042efda21e362ff360f5cf8e91220757.jpg", "table_caption": ["Table 9: Linearized transformer performance in terms of accuracy. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "H Accuracy curve of the memorization task ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The accuracy curve in the memorization task during training is shown in Fig. 8. Training of both transformers has converged. ", "page_idx": 18}, {"type": "image", "img_path": "plH8gW7tPQ/tmp/2a3082a3dddb442a270cb83db293fe74fd1b51a77994594fd2fe7cc46e3e18ce.jpg", "img_caption": ["Accuracy for random transformer "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "plH8gW7tPQ/tmp/91fb28eb764fa28eb59178aa495353006dc0fd996fef138614ad3d61f0fabf70.jpg", "img_caption": ["Accuracy for fully trained transformer "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 8: During training, the accuracy curve from fully trained and random transformers in the memorization task (Section 5.1). Note that the evaluation set is exactly the training set as the goal is to memorize. ", "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: All our claims are based on empirical evidences. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: See Appendix A. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The only theoretical result is included in Appendix F where the full proof is given. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We tried our best to convey all the experiment details and we will also be releasing code and data after some final cleanup. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We plan to release code and data after finalizing. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Details are included in Appendix D. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We include the $80\\%$ CI bars in Appendix E. Some experiments are not repeated due to computational constraints. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: See Appendix C. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We reviewed the Code of Ethics and we believe our work will not cause potential harms and negative societal impacts. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: See Appendix B. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 23}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our results do not pose high risk. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We cited the original papers for each asset. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We will release the code and trained models as well as documentation on how to apply them. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: No crowdsourcing or human-subject research is involved. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: No crowdsourcing or human-subject research is involved. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]