{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Language models are unsupervised multitask learners", "publication_date": "2019-01-01", "reason": "This paper introduces the GPT-2 model, a foundational model for many language models used in this study."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-01-01", "reason": "This paper introduces the Transformer architecture, which is the basis for the models analyzed in this study."}, {"fullname_first_author": "Jonathan Frankle", "paper_title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks", "publication_date": "2018-01-01", "reason": "This paper introduces the lottery ticket hypothesis, which is directly relevant to the investigation of random transformers."}, {"fullname_first_author": "Zeyuan Allen-Zhu", "paper_title": "Physics of language models: Part 3.3, knowledge capacity scaling laws", "publication_date": "2024-01-01", "reason": "This paper provides relevant theoretical background on scaling laws of language models, which helps interpret some of the experimental results."}, {"fullname_first_author": "Neel Nanda", "paper_title": "Progress measures for grokking via mechanistic interpretability", "publication_date": "2023-01-01", "reason": "This paper explores the phenomenon of \"grokking,\" which is directly relevant to the investigation of the capabilities of randomly initialized transformers."}]}