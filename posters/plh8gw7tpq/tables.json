[{"figure_path": "plH8gW7tPQ/tables/tables_4_1.jpg", "caption": "Table 1: Test accuracy of fully trained and random transformers, as well as fully trained LSTMs, on algorithmic tasks. Denoted numbers (1024 and 16) are hidden sizes; results are median over 10 random restarts. Random models with only trained embeddings reliably perform all four tasks, and even outperform fully trained LSTMs. See Appendix E for the accuracy curve on multiple hidden sizes.", "description": "This table compares the performance of fully trained and randomly initialized transformers (with only embedding layers trained) on four algorithmic tasks: modular addition, needle-in-a-haystack, decimal addition, and parenthesis balancing.  It shows that even randomly initialized transformers can achieve high accuracy, sometimes exceeding that of fully trained LSTMs, when only the embedding layers are trained.", "section": "4 Random Transformers Can Perform Simple Algorithmic Tasks"}, {"figure_path": "plH8gW7tPQ/tables/tables_4_2.jpg", "caption": "Table 2: Accuracy of embedding-only training with additional parameters fixed: optimizing only the unembedding layer, only the embedding layer, or only non-positional embeddings. Hidden sizes are all 1024; results are median over 10 random restarts. All three embedding matrices must be optimized for models to reliably complete all tasks.", "description": "This table shows the results of an experiment where different parts of a randomly initialized transformer model were trained while keeping other parts frozen.  The goal was to determine which parts of the model were essential for successful completion of several algorithmic tasks.  The table compares the performance (accuracy) when only the unembedding layer was trained, only the embedding layer was trained, and when only the token and unembedding layers were trained.  The results indicate that all three embedding layers must be trained for the model to reliably perform all the tasks.", "section": "4 Random Transformers Can Perform Simple Algorithmic Tasks"}, {"figure_path": "plH8gW7tPQ/tables/tables_5_1.jpg", "caption": "Table 3: Comparison of normal and random transformer in the memorization task.", "description": "This table compares the performance of normal (fully trained) and random transformers on a memorization task.  It shows the accuracy achieved, the number of bits memorized, the number of trainable parameters, and the bits per parameter for each type of transformer.  The results highlight the difference in efficiency between the two approaches.", "section": "5 Random Transformers Can Memorize and Generate Structured Sequences"}, {"figure_path": "plH8gW7tPQ/tables/tables_8_1.jpg", "caption": "Table 4: Median explained variance from top 10 directions under principal and neuron basis. Activations after embedding (Emb), layer 1 (L1) and layer 2 (L2) are collected from multiple trained 2-layer models of width 1024 and 128 (for memorization). Normal transformers are fully trained while Random transformers have only embedding and unembedding layers trained, as in previous experiments. Across tasks, a large fraction variance in models' hidden representations is explained by a small number of principal components, but these components do not appear to be aligned to individual neurons or sparse sub-networks.", "description": "This table shows the median explained variance from the top 10 principal components and neuron basis for various tasks.  It compares the explained variance in fully trained vs. randomly initialized transformers, looking at activations at different layers (embedding, layer 1, and layer 2). The results demonstrate that, across tasks, a substantial portion of the variance is explained by a small number of principal components, regardless of training method. However, these components are not aligned with individual neurons or sparse subnetworks.", "section": "Low-Dimensional Hidden Representations in Algorithmic and Memorization Tasks"}, {"figure_path": "plH8gW7tPQ/tables/tables_8_2.jpg", "caption": "Table 5: Median explained variance from top 10 directions under principal and neuron basis for language modeling task. Activations after embedding (Emb) and after every layer (L1, L2, L3, L4) are collected from trained 4-layer models of width 512. As above, a substantial fraction of variance is explained by a small number of principal components, especially in random transformers.", "description": "This table shows the median explained variance from the top 10 principal components and neuron basis for the language modeling task.  It compares the results from fully trained and random transformers (with only embedding layers trained) at different layers (embedding, layer 1-4).  It demonstrates the extent to which the hidden representations of these models are concentrated in low-dimensional subspaces.", "section": "Low-Dimensional Hidden Representations in Algorithmic and Memorization Tasks"}, {"figure_path": "plH8gW7tPQ/tables/tables_14_1.jpg", "caption": "Table 6: Number of parameters of each type for the 2-layer 1024-width transformers in Section 4.", "description": "This table shows the number of parameters for each component of the 2-layer, 1024-width transformer models used in section 4 of the paper.  The components are: token embeddings (Etoken), positional embeddings (Epos), unembedding layer (U), and intermediate layers (F). The table provides a breakdown of the parameter counts for each of these components across four different algorithmic tasks: modular addition, needle-in-a-haystack, decimal addition, and parenthesis balancing.", "section": "4 Random Transformers Can Perform Simple Algorithmic Tasks"}, {"figure_path": "plH8gW7tPQ/tables/tables_15_1.jpg", "caption": "Table 7: Median explained variance from top 10 directions under principal and neuron basis collected from width 512 transformers. Rounded to one decimal piece. See Table 4 for more details.", "description": "This table shows the median explained variance from the top 10 principal components and neuron basis for three tasks: Decimal Addition, Needle-in-a-Haystack, and Balanced Parentheses.  It compares the explained variance in the embedding layer (Emb), layer 1 (L1), and layer 2 (L2) for both fully trained ('Normal') and randomly initialized transformer models ('Random').  The data illustrates the extent to which the models' hidden representations are concentrated in low-dimensional subspaces for each task and model type.  Values are rounded to one decimal place, and more detailed results can be found in Table 4.", "section": "6.1 Low-Dimensional Hidden Representations in Algorithmic and Memorization Tasks"}, {"figure_path": "plH8gW7tPQ/tables/tables_18_1.jpg", "caption": "Table 1: Test accuracy of fully trained and random transformers, as well as fully trained LSTMs, on algorithmic tasks. Denoted numbers (1024 and 16) are hidden sizes; results are median over 10 random restarts. Random models with only trained embeddings reliably perform all four tasks, and even outperform fully trained LSTMs. See Appendix E for the accuracy curve on multiple hidden sizes.", "description": "This table shows the accuracy of three different model types (fully trained transformer, random transformer, and fully trained LSTM) on four algorithmic tasks.  The table compares models with different hidden sizes (16 and 1024), and results are the median accuracy across 10 runs with different random initializations.  It highlights that randomly initialized transformers, with only embedding layers trained, can achieve high accuracy on these tasks, sometimes even surpassing fully trained models.", "section": "4 Random Transformers Can Perform Simple Algorithmic Tasks"}, {"figure_path": "plH8gW7tPQ/tables/tables_18_2.jpg", "caption": "Table 9: Linearized transformer performance in terms of accuracy.", "description": "This table shows the accuracy achieved by linearized transformers (transformers with attention replaced by a simple token prefix sum mechanism) on three different algorithmic tasks: Needle-in-a-haystack, Decimal Addition, and Parenthesis Balancing.  The results highlight the significant performance gap between linearized transformers and standard transformers on these tasks, which demonstrates the difficulty of the tasks.", "section": "G Difficulty of the synthetic tasks"}]