[{"figure_path": "vA4s3kN4QE/tables/tables_5_1.jpg", "caption": "Table 1: Results of image reconstruction on TextCaps, CelebA-HQ, CUB-200, and MS-COCO. \u201cVQ-VAE+LG\u201d denotes considering our method LG-VQ based on VQ-VAE.", "description": "This table presents the results of image reconstruction experiments performed using four different models: VQ-VAE, VQ-VAE+LG, VQ-GAN, and VQ-GAN+LG.  The evaluation metrics used are Fr\u00e9chet Inception Distance (FID) and Peak Signal-to-noise Ratio (PSNR). Lower FID scores indicate better perceptual similarity to the original images, while higher PSNR values indicate better pixel-level similarity. The experiments were conducted on four datasets: TextCaps, CelebA-HQ, CUB-200, and MS-COCO, allowing for a comprehensive evaluation of the models' performance across different image types and complexities.", "section": "4 Experiments"}, {"figure_path": "vA4s3kN4QE/tables/tables_5_2.jpg", "caption": "Table 2: Ablation study of our three loss functions on TextCaps and CUB-200.", "description": "This table presents the results of an ablation study conducted to evaluate the individual contributions of the three loss functions (Lgsa, Lmtp, and Lras) used in the LG-VQ model.  The study compares the model's performance on two datasets, TextCaps and CUB-200, measured by FID (Fr\u00e9chet Inception Distance), a metric that assesses the perceptual similarity between generated images and real images. The lower the FID score, the better the performance. By systematically adding each loss function, the table shows the impact of each component on the overall image reconstruction quality.", "section": "4.3 Ablation Study"}, {"figure_path": "vA4s3kN4QE/tables/tables_5_3.jpg", "caption": "Table 3: Results (Recall@1) of masked word prediction on CelebA-HQ and CUB-200. \"Mask-1\" denotes that text is randomly masked one word.", "description": "This table presents the recall@1 scores achieved by the LG-VQ model in a masked word prediction task.  Two datasets are used, CelebA-HQ and CUB-200, and two masking scenarios are tested: masking one word randomly, and masking three words randomly. The recall@1 metric indicates the percentage of times the model correctly predicts the masked word as the top prediction.  Higher scores signify better performance in recovering masked words.", "section": "4.2 Discussion of Results"}, {"figure_path": "vA4s3kN4QE/tables/tables_6_1.jpg", "caption": "Table 4: Results of similarity evaluation between codes and words on CUB-200 all test data.", "description": "This table presents a quantitative comparison of the similarity between word semantics and corresponding code representations.  The Mean Squared Error (MSE) is used as the metric to measure the difference in similarity between word pairs and their corresponding code pairs. Lower MSE values indicate a higher degree of alignment between word semantics and code representations, suggesting the effectiveness of the proposed method in capturing semantic relationships within the codebook.", "section": "4.3 Ablation Study"}, {"figure_path": "vA4s3kN4QE/tables/tables_8_1.jpg", "caption": "Table 5: Results of text-to-image on CelebA-HQ.", "description": "This table presents the results of the text-to-image task on the CelebA-HQ dataset.  It compares the Fr\u00e9chet Inception Distance (FID) scores, a metric for evaluating the quality of generated images, for several models. Lower FID scores indicate better image quality. The models compared include Unite and Conqu, Corgi, LAFITE, VQ-GAN, CVQ, VQ-GAN+LG (the proposed method with language guidance added to VQ-GAN), and CVQ+LG (the proposed method with language guidance added to CVQ). The table shows that the models incorporating the language-guided codebook learning (LG-VQ) achieve significantly lower FID scores, demonstrating improved image generation quality.", "section": "4.4 Application"}, {"figure_path": "vA4s3kN4QE/tables/tables_8_2.jpg", "caption": "Table 6: Result (FID\u2193) of semantic synthesis on CelebA-HQ.", "description": "This table presents the Fr\u00e9chet Inception Distance (FID) scores, a metric evaluating the quality of generated images, for semantic synthesis on the CelebA-HQ dataset.  Lower FID scores indicate better image quality. The table compares the performance of several models, including baseline models (Reg-VQ, VQCT, VQ-GAN, CVQ) and the proposed LG-VQ method integrated with VQ-GAN and CVQ.  The results show the FID scores achieved by each model, demonstrating the improvement in image quality obtained by incorporating the language-guided codebook learning approach.", "section": "4.4 Application"}, {"figure_path": "vA4s3kN4QE/tables/tables_9_1.jpg", "caption": "Table 1: Results of image reconstruction on TextCaps, CelebA-HQ, CUB-200, and MS-COCO. \u201cVQ-VAE+LG\u201d denotes considering our method LG-VQ based on VQ-VAE.", "description": "This table presents the results of image reconstruction experiments using different models on four datasets: TextCaps, CelebA-HQ, CUB-200, and MS-COCO.  The table compares the performance of several vector quantization (VQ) models, specifically VQ-VAE, VQ-GAN, and CVQ, both with and without the proposed LG-VQ method.  The performance is measured using two metrics: FID (Fr\u00e9chet Inception Distance), which assesses perceptual similarity, and PSNR (Peak Signal-to-Noise Ratio), which evaluates pixel-level similarity. Lower FID scores indicate better performance.", "section": "4 Experiments"}, {"figure_path": "vA4s3kN4QE/tables/tables_9_2.jpg", "caption": "Table 1: Results of image reconstruction on TextCaps, CelebA-HQ, CUB-200, and MS-COCO. \u201cVQ-VAE+LG\u201d denotes considering our method LG-VQ based on VQ-VAE.", "description": "This table presents the quantitative results of image reconstruction experiments performed using four different models on four datasets.  The models are VQ-VAE, VQ-VAE with the proposed LG-VQ method, VQ-GAN, and VQ-GAN with LG-VQ.  The datasets are TextCaps, CelebA-HQ, CUB-200, and MS-COCO.  The results are evaluated using two metrics: FID (Fr\u00e9chet Inception Distance) and PSNR (Peak Signal-to-Noise Ratio). Lower FID scores indicate better perceptual similarity, while higher PSNR scores indicate better pixel-level similarity between the reconstructed images and the original images.", "section": "4 Experiments"}, {"figure_path": "vA4s3kN4QE/tables/tables_9_3.jpg", "caption": "Table 10: Result (FID\u2193) of image completion on CelebA-HQ.", "description": "This table presents the Fr\u00e9chet Inception Distance (FID) scores, a lower score indicating better performance, for image completion on the CelebA-HQ dataset.  It compares the performance of the VQ-GAN model (baseline) with the LG-VQ model (the proposed method).  The FID score for LG-VQ is significantly lower than for VQ-GAN, demonstrating the improved performance of LG-VQ in image completion tasks.", "section": "4.4 Application"}, {"figure_path": "vA4s3kN4QE/tables/tables_9_4.jpg", "caption": "Table 11: Results of image captioning on CUB-200.", "description": "This table presents the results of image captioning experiments conducted on the CUB-200 dataset.  Four different models were compared: VQ-GAN, V2L Tokenizer, VQCT, and the proposed LG-VQ model. The performance of each model is evaluated using four metrics: BLEU4, ROUGE-L, METEOR, and CIDEr-D.  The table shows LG-VQ achieves competitive results compared to state-of-the-art methods.", "section": "4.4.2 Visual Text Reasoning"}, {"figure_path": "vA4s3kN4QE/tables/tables_14_1.jpg", "caption": "Table 1: Results of image reconstruction on TextCaps, CelebA-HQ, CUB-200, and MS-COCO. \u201cVQ-VAE+LG\u201d denotes considering our method LG-VQ based on VQ-VAE.", "description": "This table presents the results of image reconstruction experiments performed using different models on four datasets: TextCaps, CelebA-HQ, CUB-200, and MS-COCO.  It compares the performance of standard VQ-VAE and VQ-GAN models against their counterparts that incorporate the proposed LG-VQ method. The metrics used to evaluate reconstruction quality are Fr\u00e9chet Inception Distance (FID) and Peak Signal-to-Noise Ratio (PSNR). Lower FID scores and higher PSNR values indicate better reconstruction performance.  The results show that the inclusion of LG-VQ leads to improvements in image reconstruction quality across all datasets.", "section": "4 Experiments"}, {"figure_path": "vA4s3kN4QE/tables/tables_14_2.jpg", "caption": "Table 13: Comparison of reconstruction and VQA on VQCT and VQCT+LG on the MS-COCO dataset.", "description": "This table presents a comparison of the performance of two models, VQCT and VQCT+LG, on image reconstruction and Visual Question Answering (VQA) tasks using the MS-COCO dataset.  The FID (Fr\u00e9chet Inception Distance) score, a lower value indicating better image reconstruction quality, is reported for the image reconstruction task. The accuracy, a higher value indicating better performance, is shown for the VQA task.  The results demonstrate the impact of integrating the Language-Guided VQ (LG-VQ) method into the VQCT model.", "section": "4.2 Discussion of Results"}]