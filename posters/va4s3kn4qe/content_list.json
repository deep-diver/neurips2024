[{"type": "text", "text": "LG-VQ: Language-Guided Codebook Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Guotao Liang1,2, Baoquan Zhang\u22171, Yaowei Wang2, Xutao $\\mathrm{Li^{1}}$ , Yunming $\\mathbf{Y}\\mathbf{e}^{1}$ , Huaibin Wang1 Chuyao Luo1, Kola $\\mathbf{Y}\\mathbf{e}^{3}$ , Linfeng Luo3 ", "page_idx": 0}, {"type": "text", "text": "Harbin Institute of Technology, Shenzhen, 2Peng Cheng Laboratory, 3SiFar Company {lianggt, wangyw}@pcl.ac.cn {23B951062, 22S051022}@stu.hit.edu.cn {baoquanzhang, lixutao, yeyunming}@hit.edu.cn {luochuyao.dalian, kolaygm, llf10811020205}@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Vector quantization (VQ) is a key technique in high-resolution and high-fidelity image synthesis, which aims to learn a codebook to encode an image with a sequence of discrete codes and then generate an image in an auto-regression manner. Although existing methods have shown superior performance, most methods prefer to learn a single-modal codebook (e.g., image), resulting in suboptimal performance when the codebook is applied to multi-modal downstream tasks (e.g., text-toimage, image captioning) due to the existence of modal gaps. In this paper, we propose a novel language-guided codebook learning framework, called LG-VQ, which aims to learn a codebook that can be aligned with the text to improve the performance of multi-modal downstream tasks. Specifically, we first introduce pre-trained text semantics as prior knowledge, then design two novel alignment modules (i.e., Semantic Alignment Module, and Relationship Alignment Module) to transfer such prior knowledge into codes for achieving codebook text alignment. In particular, our LG-VQ method is model-agnostic, which can be easily integrated into existing VQ models. Experimental results show that our method achieves superior performance on reconstruction and various multi-modal downstream tasks. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In recent years, with the growing development of various multi-modal task scenarios [37, 36, 38], unified modeling of visuals and language has sparked considerable interest. Vector Quantization (VQ)-based image modeling technique, exemplified by VQ-VAE [43] and VQ-GAN [9], has emerged as a pivotal approach in the realm of unified modeling. The VQ methodology [43] typically follows a two-stage generation paradigm. In the initial stage, a trainable discrete codebook is employed to quantize continuous image features into a discrete token sequence to finish the reconstruction task. Subsequently, the codebook is utilized for various downstream tasks by generative models [42, 37]. ", "page_idx": 0}, {"type": "text", "text": "Learning a robust codebook during the initial stage is crucial for optimizing performance in downstream tasks. At present, lots of VQ methods have been proposed to achieve robust code representation [15, 14, 7, 12]. For instance, VQ-GAN [9] introduces an adversarial training loss to learn a perceptually rich codebook. Some other works consider improving the codebook representation from the perspective of addressing the problem of codebook collapse [53, 52]. ", "page_idx": 0}, {"type": "text", "text": "Although existing methods have shown superior performance, most methods only focus on learning a single-modal codebook contains more low-level information (e.g., image\u2019s pixel, edge, and texture), resulting in suboptimal performance when the codebook is applied to multi-modal downstream tasks (e.g., text-to-image [37], image captioning [36], VQA [24]). That is because the codebook lacks high-level semantics and the existence of modal gaps. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To address the above issue, we propose a novel codebook learning method (i.e., multi-modal codebook learning), called Language-Guided VQ (LG-VQ). The novelty lies in utilizing pre-trained text semantics as supervised information to guide the codebook to learn abundant multi-modal knowledge. ", "page_idx": 1}, {"type": "text", "text": "Specifically, we first employ a cross-modal pre-trained model (i.e., CLIP [32]) to encode text semantics. Then, we propose two novel semantic supervision modules to transfer the text semantics into codebook, i.e., Semantic Alignment Module, and Relationship Alignment Module. Within the semantic alignment module, we enhance the consistency between the semantic representations of the codebook and text through global semantic alignment and masked text prediction. On the other hand, simply aligning the text and codebook in the holistic semantic space cannot satisfy more complex reasoning tasks like image captioning and VQA. Inspired by some VQA techniques [26, 40, 24], the semantic relationships between words play a very important role in various tasks of natural language processing (See Fig. 1). Based on this fact, we further propose to transfer the semantic relationships between words into ", "page_idx": 1}, {"type": "image", "img_path": "vA4s3kN4QE/tmp/98987b377167ea54a1722f611decebe92022d147c2a13c2d332f379b5910abf7.jpg", "img_caption": ["Q: What is this woman holding? "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: To answer the question, one not only needs to identify \u201cwomen\u201d and \u201cracket\u201d but also understand the semantic relationship between them (\u201cholding\u201d). ", "page_idx": 1}, {"type": "text", "text": "codes to achieve better alignment between the codes and words. Such a text-aligned codebook helps alleviate modal gaps and improve codebook performance on cross-modal tasks. ", "page_idx": 1}, {"type": "text", "text": "The contributions of this work are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We point out the limitations of existing methods in learning an expressive codebook since they learn a single-modal codebook. We propose a novel multi-modal codebook learning method, named LG-VQ, which can enable the codebook to effectively retain fine-grained reconstruction information while aligning with the text.   \n\u2022 Resorting to pre-trained text semantics, we propose two novel semantic supervision modules, i.e., Semantic Alignment Module and Relationship Alignment Module, effectively learn text-aligned codebook. The advantage of such alignment modules is the abundant context and relationship semantics contained in pre-trained text can be sufficiently leveraged for enhancing multi-modal codebook learning.   \n\u2022 We conduct comprehensive experiments on four public datasets, which shows that our LG-VQ method outperforms various state-of-the-art models on reconstruction and various cross-modal tasks (e.g., text-to-image, image captioning, VQA). ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Vector Quantization for Image Generation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Vector quantization (VQ) is designed to learn a codebook, which aims to encode continuous image features into a discrete sequence. Then, the learned codebook can be utilized for various downstream tasks. Oord et al. [43] first propose a novel VQ method called VQ-VAE. This method innovatively replaces the prior distribution of Variational Autoencoder (VAE) with a discrete deterministic distribution (i.e., a codebook). To further improve the performance of VQ, various models are proposed to learn a more expressive codebook [9, 48, 2, 17, 7, 15, 14, 21]. For example, VQ-GAN [9] addresses the issue of image blur generated by VQ-VAE through the introduction of an adversarial training loss. However, the above methods do not tackle the codebook collapse issue. To address the issue, many novel methods are proposed from the perspective of regularization [33], codebook update [53], codebook transfer [51]. Recently, inspired by the large language models (LLMs), instead of mapping images to the visual code tokens, some works attempt to map the images to the word tokens of LLMs by viewing images as \u201cforeign languages\u201d [22, 50, 56]. However, because of the inherent differences between vision and language, these works have difficulty assigning correct semantic words to images. ", "page_idx": 1}, {"type": "image", "img_path": "vA4s3kN4QE/tmp/2e76254b4e2ed62de1c696d3a5d948cf9873680adbe72fce5275ad4b06fd3f5b.jpg", "img_caption": ["Figure 2: The overall architecture of the proposed LG-VQ method. The right part of the figure is the basic VQ-VAE module, left is our language-guided module, which consists of three losses: global semantic alignment $(\\mathcal{L}_{g s a})$ , masked text prediction $(\\mathcal{L}_{m t p})$ , and relationship alignment supervision $(\\mathcal{L}_{r a s})$ . Here, pre-trained text information guides discrete code tokens of an image to learning rich semantic knowledge based on three losses. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Compared with the aforementioned methods, our approach focuses more on multi-modal alignment in feature space (i.e., learning a text-aligned codebook). We use pre-trained text semantics to supervise the codebook learning. The advantage is that the rich semantic information from the text can be fully exploited for more robust codebook learning so that the codebook can not only retain more reconstruction information but also be able to understand and match text. More importantly, our method is model-agnostic, which can be easily integrated into existing VQ models. ", "page_idx": 2}, {"type": "text", "text": "2.2 Vision-Language Representation Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Vision-language Pre-training (VLP) aims to learn multi-modal representations from large-scale imagetext pairs that can improve vision-language downstream tasks, for example, VQA[1]. Early methods such as LXMERT [41], UNITER [4] employ pre-trained object detectors to extract image region features, and fuse image features with text by a cross-modal encoder to achieve the vision-language representation learning. Although these methods achieve superior performance on downstream tasks, they require high-resolution input images and pre-trained object detectors. To remove the object detectors, a large number of researchers focus on learning two separate representations for image and text [32, 16, 18]. For instance, CLIP [32] learns a robust representation for each image and text using contrastive learning based on large-scale image-text pair data. ", "page_idx": 2}, {"type": "text", "text": "In this paper, we propose to employ pre-trained text semantics as supervised information to guide codebook learning. Its advantage is that abundant multi-modal knowledge contained in text can be fully leveraged for robust codebook learning. Additionally, we design a novel relationship alignment module to inject semantic relationships between words into codes. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Preliminaries: VQ-VAE ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "VQ-VAE [43], as a pioneering work on the VQ research domain, aims to learn a discrete codebook to encode images into discrete token sequences through an Encoder-Decoder framework. As illustrated in Fig. 2 right, the VQ-VAE consists of a visual encoder $E_{\\theta_{e}}(\\cdot)$ with parameter $\\theta_{e}$ , a token decoder $D_{\\theta_{d}}(\\cdot)$ with parameter $\\theta_{d}$ , a quantizer $Q(\\cdot)$ , and a codebook is defined as $\\mathcal{Z}=\\{e_{k}\\}_{k=1}^{K}$ k}kK=1 that consists of learnable $K$ entries $\\boldsymbol{e}_{k}\\in\\mathbb{R}^{d_{z}}$ with dimension $d_{z}$ . Given an input image $x\\in\\mathbb{R}^{H\\times W\\times C}$ , where $H$ , $W$ , and $C$ represent the height, width, and channel of the image respectively. The visual encoder $E_{\\theta_{e}}(\\cdot)$ learns to convert the original image into grid features $\\bar{Z}=E_{\\theta_{e}}\\bar{(x)}\\in\\mathbb{R}^{\\overbar{\\frac{H}{f}}\\times\\frac{W}{f}\\times d_{z}}$ and $f$ is the down-sampling factor. The quantizer $Q(\\cdot)$ looks up the nearest neighbor in the codebook for each grid representation $\\hat{z}_{i}\\in\\mathbb{R}^{d_{z}}$ in ${\\hat{Z}}_{i}$ using the following equation: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nz_{i}=Q(\\hat{z}_{i})=a r g m i n\\:\\|\\hat{z}_{i}-e_{k}\\|.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The token decoder $D_{\\theta_{d}}(\\cdot)$ is used to reconstruct the original image by $\\widetilde{x}\\,=\\,D_{\\theta_{d}}(Z)$ , where $Z$ is discrete code tokens of whole image obtained by Eq. 1. During training,  t he visual encoder $E_{\\theta_{e}}(\\cdot)$ , codebook $\\mathcal{Z}$ , and token decoder $D_{\\theta_{d}}(\\cdot)$ are jointly optimized by minimizing the following objective: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{v q}=||x-\\widetilde{x}||_{2}^{2}+||s g[E_{\\theta_{e}}(x)]-Z||_{2}^{2}+\\omega||E_{\\theta_{e}}(x)-s g[Z]||_{2}^{2},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where, the first term is reconstruction loss, which measures the difference between the original image $x$ and the reconstructed image $\\widetilde{x}.\\ s g[\\cdot]$ represents the stop-gradient operator, and the second term is codebook loss, which encourages the codebook to be close grid features. The third term is the \u201ccommitment loss\u201d [43], where $\\omega$ serves as a hyper-parameter. However, existing VQ-based methods mainly focus on the learning of single-modal codebook, thereby limiting their applicability to multi-modal downstream tasks. ", "page_idx": 3}, {"type": "text", "text": "3.2 Proposed Method: LG-VQ ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Existing works attempt to improve codebook reconstruction capabilities to obtain better performance on downstream tasks. However, ignoring modal differences results in suboptimal performance when the codebook is applied to cross-modal tasks. To address this issue, we propose to utilize the pretrained text semantics as supervised information to learn a text-aligned codebook. Its advantage is abundant semantic information from text can be fully exploited for more robust codebook learning to improve the performance of reconstruction and cross-modal tasks. The comprehensive architecture of the proposed LG-VQ method is illustrated in Fig. 2 left. It consists of two supervision modules: Semantic Alignment Module (i.e., $\\mathcal{L}_{g s a}$ and $\\mathcal{L}_{m t p}$ ), and Relationship Alignment Module (i.e., $\\mathcal{L}_{r a s})$ . The first module encourages global semantic consistency between the codebook and text. The second module aims to transfer the rich semantic relationship between words into codes. Next, we introduce these two modules in detail. ", "page_idx": 3}, {"type": "text", "text": "3.2.1 Semantic Alignment Module ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Considering that paired image and text data have consistent semantic information and the missing information of masked data can be completed from the other modality, we propose global semantic alignment, which aims to enhance the consistency of global semantics between text and visual codes, and masked text prediction, which uses visual codes to restore the masked words. Next, we discuss how to align text and codebook in the semantic space. ", "page_idx": 3}, {"type": "text", "text": "Text Information Encoder: Instead of jointly training text and codebook from scratch, we employ a pre-trained cross-modal model CLIP [32] to encode text information. Its advantage is that such text information already has good cross-modal semantic knowledge and is beneficial for codebook learning. Specifically, for a given text description of an image $t=\\{w_{S O T},w_{1},w_{2},\\cdot\\cdot\\cdot,w_{n-2},w_{E O T}\\}$ , where $w_{i}$ denotes the $i$ -th word, $w_{S O T}$ and $w_{E O T}$ represent the $[s t a r t]$ token and $[e n d]$ token, respectively, and $n$ is text sequence length. We use the text encoder of a pre-trained CLIP model to obtain whole sequence embedding T \u2208Rn\u00d7dt: ", "page_idx": 3}, {"type": "equation", "text": "$$\nT=\\{e_{S O T},e_{w_{1}},e_{w_{2}},\\cdot\\cdot\\cdot\\,,e_{w_{n}},e_{E O T}\\}=\\mathrm{CLIP}(t).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Similar to CLIP, we use the $e_{E O T}$ to represent the global context feature of the sequence. ", "page_idx": 3}, {"type": "text", "text": "Global Semantic Alignment aims to align text and image visual codes in the global semantic space. For getting the global representation of visual codes, we employ a vision transformer (ViT) $f_{\\theta_{v t}}$ [8] to encode the discrete codes of image. Specifically, given an image, we firstly obtain the discrete codes of image $Z$ by Eq. 1. Then, we introduce a learnable global token $[C L S]$ at the beginning to form a token sequence $Z_{c}$ , where global token $[C L S]$ is employed to capture the image\u2019s global context information. We feed the sequence into $f_{\\theta_{v\\ t}}$ to get a new visual code representation, that is: ", "page_idx": 3}, {"type": "equation", "text": "$$\nZ_{v t}=\\{e_{C L S},e_{1},e_{2},\\cdot\\cdot\\cdot\\cdot,e_{\\frac{H}{f}\\times\\frac{W}{f}}\\}=f_{\\theta_{v t}}(Z_{c}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Finally, we employ InfoNCE [29], which maximizes the similarity between visual and text in the global representation, as our learning objective, where $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ is the batch size, $s(\\cdot,\\cdot)$ is cosine similarity: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{g s a}=-\\sum_{i\\in\\mathcal{B}}\\log\\frac{\\exp(s(e_{C L S}^{i},e_{E O T}^{i}))}{\\sum_{j\\in\\mathcal{B}}\\exp(s(e_{C L S}^{i},e_{E O T}^{j}))}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Masked Text Prediction: To further enhance the semantic alignment, we propose to use discrete visual codes to reconstruct the masked words from a more fine-grained perspective, refer to Fig. 2 left. Formally, for a given fixed-length text sequence of $n-2$ , we first randomly sample the masking ratio $r$ from a truncated Gaussian distribution [19]. Subsequently, we randomly mask out $r\\cdot(n-2)$ words and replace them with learnable $[m a s k_{i}]$ tokens based on their positions $i$ . Next, a self-attention module [44] is employed to learn adaptive masked word embeddings based on unmasked words. The resulting adaptive masked sequence is denoted as $T^{m s k}=\\{e_{S O T}^{\\ \u3001\\ \u3001}m_{1},e_{w_{2}},m_{3},\\cdot\\cdot\\cdot,e_{E O T}\\}$ , where $m_{i}$ is the mask token embedding at the $i$ -th position in the sequence. Following this, a cross attention decoder $f_{\\theta_{m}}(\\cdot,\\cdot)$ is employed to predict the masked word tokens given the discrete visual codes $Z_{v t}$ obtained by Eq. 4. Finally, we add a cross-entropy loss $H(\\cdot,\\cdot)$ between the ground-truth word tokens and the output of the decoder. Let $y_{m s k}$ denote a one-hot vocabulary distribution where the ground-truth word token has a probability of 1, $f_{\\theta_{m}}(Z_{v t},T^{m s k})$ denote the predicted probability of model for masked word tokens. That is: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{m t p}=-\\mathbb{E}_{(Z_{v t},T^{m s k})\\sim\\mathcal{B}}H(y_{m s k},f_{\\theta_{m}}(Z_{v t},T^{m s k})).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3.2.2 Relationship Alignment Module ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "While the two aforementioned loss functions for achieving good alignment at holistic semantic space have demonstrated initial promise, they cannot satisfy more complex reasoning tasks like image captioning and VQA. Inspired by some VQA techniques [26, 40, 24, 1], the semantic relationships between pre-trained words play a very important role in complex text reasoning tasks. For instance, as shown in Fig 1, to answer question (\u201cWhat is this woman holding?\u201d), one needs to fully understand the visual objects \u201cwomen\u201d, \u201cracket\u201d, and semantic relationship between them (\u201cholding\u201d). Based on the above fact, we propose to transfer the semantic relationship between words into codes. Such semantic relationships enable the model to better understand the image for addressing complex reasoning tasks. ", "page_idx": 4}, {"type": "text", "text": "But unfortunately, there is an issue there is no alignment between words and codes. Thanks for the above two losses that have provided semantic alignment of text and visual codes. To achieve the above idea, as shown in Fig. 3, we first use $Z_{v t}$ to align with words. Then, we inject semantic relationships between words into the initial codebook $Z$ , instead of the $Z_{v t}$ . Its advantage is it can prevent codes from collapsing into a single point for learning more diverse representations by relationship limiting. Then, $Z_{v t}$ primarily serves the purpose of aligning words and codes, but it is a crucial step for subsequent processes. Specifically, given any two words of a sentence, we use pre-trained word embedding [32] to encode words, $e_{w_{i}}$ and $e_{w_{j}}$ . We employ cosine similarity to find the index of the code from $Z_{v t}$ that is most similar to the word. Then, one can get code embedding from $Z$ based on the index: ", "page_idx": 4}, {"type": "image", "img_path": "vA4s3kN4QE/tmp/ef20f8e90b055529ca0c0d596a7186b8dcc0f3a2537fc36fd1f6dda00e5edd7b.jpg", "img_caption": ["Figure 3: Illustration of relationship alignment module, we use $Z_{v t}$ to align with two words, then inject the semantic relationship of two words into $Z$ codes. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{e_{z_{i}}=Z[\\underset{e_{z}\\in Z_{v t[1:]}}{a r g m a x}\\,s(e_{w_{i}},e_{z}),:],}&{}&{e_{z_{j}}=Z[\\underset{e_{z}\\in Z_{v t[1:]}}{a r g m a x}\\,s(e_{w_{j}},e_{z}),:].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Next, we consider cosine similarity as a measure of semantic relationships between words and leverage it to establish corresponding relationships between codes achieving semantic relationship transfer. Finally, we utilize the following loss function as learning objective: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{r a s}=\\sum_{(w_{i},w_{j})\\in t}(s(e_{w_{i}},e_{w_{j}})-s(e_{z_{i}},e_{z_{j}}))^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3.2.3 Training Objective ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We use three hyperparameters (i.e., $\\alpha,\\,\\beta.$ , and $\\gamma$ ) to control three losses, respectively. Finally, the overall objective function is: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\mathcal{L}_{v q}+\\alpha\\mathcal{L}_{g s a}+\\beta\\mathcal{L}_{m t p}+\\gamma\\mathcal{L}_{r a s}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Table 1: Results of image reconstruction on TextCaps, CelebA-HQ, CUB-200, and MS-COCO.   \n\u201cVQ-VAE+LG\u201d denotes considering our method LG-VQ based on VQ-VAE. ", "page_idx": 5}, {"type": "table", "img_path": "vA4s3kN4QE/tmp/0bb2014bed6d1186ed3ad801568e7882a30063799079872caf4109545887a5d6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Experimental Settings ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Evaluation Metrics. As our method is model-agnostic, we choose recent models, including VQ-VAE [43], VQ-GAN [9], and CVQ [53] as our backbone network. Following existing works [52, 53], we evaluate the reconstruction image quality on two evaluation metrics, i.e., Fr\u00e9chet Inception Distance (FID) [13] which evaluates the perceptual similarity of reconstructed images and original images, and Peak Signal-to-noise Ratio (PSNR) [10] is employed to measure the pixel-level similarity between the reconstructed and original images. ", "page_idx": 5}, {"type": "text", "text": "Dataset. We evaluate our method on four public datasets, including TextCaps [39], CelebA-HQ [23], CUB-200 [45], and MS-COCO [20]. For CelebA-HQ, CUB-200, and MS-COCO datasets, we use publicly available image captions, CelebA-HQ from [47], CUB-200 from [34], MS-COCO from [3] Implementation Details. Following VQ-GAN [9], all images are reshaped $256\\times256$ for reconstruction and generation. Down-sampling factor $f$ is set to 16. The codebook size $K$ is 1024. The batch size is 8. In our experiments, we maintain consistent parameter settings between our method LG-VQ and the chosen backbone networks (i.e., VQ-VAE [43], VQ-GAN [9], and CVQ [53]) for a fair comparison. For each image, we randomly select a text from multi-text for training. Since our method introduces additional text and pre-trained CLIP model, for a fair comparison, we select VQCT [51] as the baseline for various downstream tasks. VQCT extracts many visual-related words from a large amount of text and designs a novel codebook transfer network based on the pre-trained CLIP model to learn the visual codebook. ", "page_idx": 5}, {"type": "table", "img_path": "vA4s3kN4QE/tmp/4982367f84ce838ea93c532efbfc37e818d67810d84402fa840d0073cffd3f96.jpg", "table_caption": ["Table 2: Ablation study of our three loss functions on TextCaps and CUB-200. "], "table_footnote": [], "page_idx": 5}, {"type": "table", "img_path": "vA4s3kN4QE/tmp/4b0e01e857321aaa76cc046458e07529ceef4a4b89c2929934c196a17e5bf214.jpg", "table_caption": ["Table 3: Results (Recall $@1$ ) of masked word prediction on CelebA-HQ and CUB-200. \u201cMask-1\u201d denotes that text is randomly masked one word. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "4.2 Discussion of Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Table 1 illustrates the image reconstruction performance of our model compared to the backbone model on multiple datasets. It can be observed that our method LG-VQ outperforms all compared methods on most evaluations, which suggests that our method is extremely effective and has strong generality. Compared with FID, our PSNR improvement is marginal, this is reasonable in the VQ research domain, which widely exists in previous VQ methods [21, 52, 53]. The main reason is that PSNR only measures the pixel-level similarity of the images, while FID can effectively measure the diversity and semantic similarity of image generation. Compared with backbone models, the key difference lies in that our method introduces well pre-trained text semantics, which is beneficial to learning a more expressive codebook. This shows the effectiveness of our method. We also provide a qualitative comparison of the image reconstruction performance of different methods, please refer to ", "page_idx": 5}, {"type": "text", "text": "4.3 Ablation Study ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Are our three loss functions both effective? In Table 2, we conduct an ablation study to show the effectiveness of the proposed three loss functions. Specifically, the VQ-GAN serves as the baseline model (i.e., without introducing any loss). We do not conduct a separate experiment on $\\mathcal{L}_{r a s}$ because this module requires code and words to be well aligned. Based on the results from $\\mathrm{{\\dot{1}})\\sim(v i)}$ , we draw several key conclusions: Firstly, each loss function plays a crucial role in improving the performance of image reconstruction. Secondly, the performance of (iii) outperforms (ii) by a large margin on TextCaps. This is reasonable because TextCaps\u2019s texts are richer and more diverse than CUB-200, it can provide more knowledge for more fine-grained alignment between codes and text, which is useful for the learning of a more robust codebook. Thirdly, analyzing the results of (iii) and (iv), injecting word-level semantic relationships into codes is beneficial, which confirms our motivation. Furthermore, the performance of (v) outperforms (i), which is reasonable because the abundant semantic knowledge from pre-trained text can be fully exploited for learning more robust codebook representation. This supports the motivation of learning a multi-modal codebook (i.e., aligned with text). Finally, comparing the results of (vi) with $(\\mathrm{i}){\\sim}(\\mathrm{v})$ , fully considering all losses achieves the best performance, indicating the effectiveness of our method. ", "page_idx": 6}, {"type": "text", "text": "Can our global semantic supervision align vision and language? In Fig. 4, we provide several image-to-text retrieval cases on CelebA-HQ and CUB-200 datasets based on VQ-GAN+LG. From the figure, it can be observed that our method can accurately retrieve text very similar to the image content, achieving the alignment of vision and language. For example, row 2 examples show that our method can precisely understand some key attributes of images (e.g., \u201cgray hair\u201d, \u201cnecktie\u201d, \u201cbig nose\u201d and \u201cchubby\u201d) and retrieve similar text. This suggests that the codes learned through our method have obtained good alignment with the text, which verifies the effectiveness of our method. Moreover, such alignment is beneficial for learning robust code representations and enhancing performance in multi-modal downstream tasks. ", "page_idx": 6}, {"type": "image", "img_path": "vA4s3kN4QE/tmp/581b36ee0043364f5f7aa27874ad04757cb600f1e16fd3384a818caf883d8449.jpg", "img_caption": ["Figure 4: Examples of the top-1 most similar text selected on image-to-text retrieval task. "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "vA4s3kN4QE/tmp/f9123cf6cc93170b10df2be10de7858ea7089fc8f95f9513f70f33866c33718d.jpg", "img_caption": ["Figure 5: Examples of the top-1 word predicted on masked word prediction task. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Can our codebook accurately predict masked words? To answer this question, we conduct a word prediction task on test data based on VQ-GAN+LG by randomly masking one word or three words of text, as shown in Table 3. We use Recall $@1$ as the evaluation metric [18]. From the table, our method demonstrates accurate predictions of masked words, confirming the effectiveness of our approach. Fine-grained word prediction can help the codebook better understand the text semantics, which is crucial for improving the performance of the downstream task. Additionally, several examples in Fig. 5 demonstrate our method\u2019s ability that accurately predict subject words (e.g., wings, eyes) and verbs (e.g., has, is, and smiling), further affirming its strong multi-modal understanding capabilities. ", "page_idx": 6}, {"type": "text", "text": "Can our codebook learn the word semantic relationships? ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Table 4: Results of similarity evaluation between codes and words on CUB-200 all test data. ", "page_idx": 6}, {"type": "text", "text": "In Fig. 6, we visualize the cosine similarity between words and the cosine similarity between codes aligned with the words for a certain sample based on VQ-GAN+LG. From the figure, we can see our codes can learn consistent relationships with word semantics compared with VQ-GAN. For example, the similarity \u201ccode $33^{\\circ}$ vs \u201ccode $232^{\\circ}$ (0.46) resembles \u201cwings\u201d vs \u201cchest\u201d (0.49). In addition, we provide a quantitative similarity evaluation between codes and words in Table 4. From the results, we can find that our codes indeed achieve consistent se ", "page_idx": 6}, {"type": "table", "img_path": "vA4s3kN4QE/tmp/2ff121ee6bba7d9dc30a85d8a010d27294c181c001230470508fe3f81a738a01.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Is our method effectively learning more diverse code representation? Following [52], we directly feed each codebook embedding $e_{k}$ (size: $1\\times1\\times256)$ ) into the decoder $D_{\\theta_{d}}(\\cdot)$ to generate codebook image (size: $16\\times16\\times3)$ . Then, we concatenate all codebook images to form a big image with $32\\times32$ patches. Finally, we visualize the result of VQ-GAN and our LG-VQ on TextCaps and MS-COCO as shown in Fig. 7. This visualization suggests that our method enables the model to learn more diverse code representations and improve codebook usage. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "vA4s3kN4QE/tmp/8cb761cb790d250eb8524dec3008c749afea9262972856c198561c75b198ef87.jpg", "img_caption": ["Figure 6: Visualization of words similarity and image codes similarity aligned with the word. We extract some representative words from the text as a demonstration. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "vA4s3kN4QE/tmp/2dc1ed7896476e4aabf18935baf7731530429d536c8c75deaed1b38a5a03d44c.jpg", "img_caption": ["Figure 7: Visualization of the codebook of VQ-GAN and LG-VQ on TextCaps and MS-COCO. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.4 Application ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "4.4.1 Image Generation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Following [9, 37, 11], we conduct image generation downstream tasks (i.e., text-to-image, semantic synthesis, unconditional generation, and image completion) to fully validate the effectiveness of the learned codebook on CelebA-HQ. ", "page_idx": 7}, {"type": "text", "text": "Text-to-Image. In Table 5, we compare our LG-VQ with the state-of-the-art models on CelebA-HQ dataset for text-to-image. From the results, our LG-VQ method outperforms baseline methods by a large margin. This is reasonable due to the incorporation of pre-trained text knowledge enabling a comprehensive understanding of the text, which suggests our method\u2019s effectiveness. Moreover, we provide some synthesis examples comparing the results of our LG-VQ with baseline methods in Figure 8, showing the performance in the text-to-image task. From the figure, we can see our method not only comprehensively understands the given text conditions but also excels in generating realistic images compared with baseline methods. For instance, our method can capture the \u201cglasses\u201d, \u201cman\u201d, \u201clong black hair\u201d, and \u201cno beard\u201d key attributions. ", "page_idx": 7}, {"type": "text", "text": "Semantic Synthesis. Following [9], we compare with existing semantic synthesis models in Table 6. Our method achieves the best performance, which suggests our method\u2019s effectiveness. We provide some examples in Appendix Figure 13. ", "page_idx": 7}, {"type": "text", "text": "Unconditional Generation and Image Completion. Following [9], we conduct unconditional image generation and Image Completion on CelebA-HQ dataset, as shown in Table 7 and Table 10. From the results, we can see that our method can significantly improve the performance of VQ-GAN, which is reasonable because pre-trained text can provide rich semantic knowledge for learning more robust codebook representation. This suggests the effectiveness of our method. We provide some examples in Appendix Figure 18 and Figure 17. ", "page_idx": 7}, {"type": "image", "img_path": "vA4s3kN4QE/tmp/171a7f11a39433e91be76996e722b36062710d748ab83867ff9338ab3c5afd6d.jpg", "img_caption": ["Figure 8: Text-to-image synthesis and semantic image synthesis on CelebA-HQ. Text with background color emphasizes generated details "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.4.2 Visual Text Reasoning ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Follow [27, 6], we use the learned codebook to conduct two visual text reasoning tasks: 1) image captioning on CUB-200; and 2) visual question answering (VQA) on COCO-QA [35]. For the experimental setting, please refer to Appendix A.1. ", "page_idx": 8}, {"type": "text", "text": "Image captioning. Following [27], we conduct the image captioning task on the CUB-200 dataset. We compare two recent work V2L Tokenizer [56] and VQCT [51]. We select VQ-GAN as our backbone network. The results are shown in Table 11. For the results, we can see that our LG-VQ method outperforms the performance of VQ-GAN. This is reasonable because the pre-trained text provides rich context and relationship semantics for codebook learning, which verifies our motivation for learning a text-aligned codebook to improve the performance of the codebook on cross-modal tasks. On the other hand, the V2L Tokenizer and VQCT cannot achieve very good performance because it is difficult to assign correct semantic language tokens to images. Compared with the V2L Tokenizer, our method utilizes pre-trained text semantics as supervised information. Its advantage is can make the codebook learn semantic information consistent with the text (i.e., learning a textaligned codebook). And, our method is model-agnostic, which can be easily integrated into existing VQ models. ", "page_idx": 8}, {"type": "text", "text": "Visual Question Answering. We select VQ-GAN and VQCT [51] as the baseline. We conduct the VQA task on the COCO-QA [35] using the codebook trained on the MS-COCO dataset. The results are shown in Table 9. From the results, we can see that our LG-VQ method significantly improves the performance of VQ-GAN on VQA task (approximately $8.32\\%\\uparrow$ on Accuracy). That is reasonable due to we introduce pre-trained text semantics to enable us to obtain a codebook aligned with the text, which is helpful for comprehensively understanding the given text question. This confirms our motivation and the effectiveness of our method. ", "page_idx": 8}, {"type": "table", "img_path": "vA4s3kN4QE/tmp/875c723f6f5c8a6a9f3fb429490f0fab74d952ec02b80d83831550af8b03a017.jpg", "table_caption": ["Table 5: Results of text-to-image on CelebA-HQ. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "vA4s3kN4QE/tmp/17abd1421d4d70fe4190996de1365c550eed284636ee226e7c6a4ed1c4643394.jpg", "table_caption": ["Table 6: Result $\\left(\\mathrm{FID}\\downarrow\\right)$ ) of semantic synthesis on CelebA-HQ. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.4.3 Visual Grounding ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conduct a visual grounding task on refcoco dataset [49] to validate the effectiveness of the learned MS-COCO\u2019s codebook. Following the same metric used in [5], a prediction is right if the IoU between the grounding-truth box and the predicted bounding box is larger than 0.5. We select VQ-GAN and VQCT [51] as the baseline. The results are shown in Table 8. From the results, we can see that the performance of our method consistently outperforms VQ-GAN and VQCT, which suggests its effectiveness. We also provide a qualitative comparison in Appendix Figure 19. For the experimental setting, please refer to Appendix A.1. ", "page_idx": 8}, {"type": "table", "img_path": "vA4s3kN4QE/tmp/4ef784f1dc418f8156b1ea11fd13e37a1a6cc58d1ce230dc333dc739e3ccadf6.jpg", "table_caption": ["Table 7: Result $\\mathrm{(FID\\downarrow)}$ ) of unconditional image generation on CelebA-HQ. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Table 8: Result $\\left(\\mathrm{FID}\\downarrow\\right)$ of Table 9: Results of (Accuvisual grounding on refcoco racy and WUPS [46]) VQA on dataset using MS-COCO\u2019s COCO-QA [35] dataset using codebook. MS-COCO\u2019s codebook. ", "page_idx": 9}, {"type": "table", "img_path": "vA4s3kN4QE/tmp/18bd72a56bad23f226090d25d2fcd738e2f4c8cc2fee1a66789ec375f088a54a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "vA4s3kN4QE/tmp/dc824bf35394a911847d4394ef3dbcaef58865b682039701b16f3c89f79ab56b.jpg", "table_caption": ["Table 10: Result $(\\mathrm{FID}\\downarrow)$ of image completion on CelebA-HQ. "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "vA4s3kN4QE/tmp/1f501a17e588aab00e0851092a861306a46d4789ee43853f3db2ce78873016cd.jpg", "table_caption": ["Table 11: Results of image captioning on CUB-200. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose a novel codebook learning method, named LG-VQ. LG-VQ is a modelagnostic method and can easily be integrated into existing VQ models. In particular, we propose to incorporate pre-trained text semantics into the codebook by two novel supervision modules, i.e., semantic and relationship. Quantitative and qualitative experiments demonstrate the strong generality of our method, showing its ability to improve the performance of the codebook in cross-modal tasks. ", "page_idx": 9}, {"type": "text", "text": "Limitations. In our current paper, we suppose each word aligns with a code, but it fails to capture some more complex relationships between words and codes (e.g., one code aligns with multiple words). In the future, we plan to investigate the relationships between codes and words. Moreover, although our results show that the performance of VQ in visual text reasoning tasks can be significantly improved, its results are still far lower than the performance of image captioning or VQA models. ", "page_idx": 9}, {"type": "text", "text": "Broader impact Our paper shows that learning a multi-modal codebook (i.e., a text-aligned codebook) can not only significantly improve the performance of reconstruction but also the performance of the codebook on cross-modal tasks. The potential impact of our research lies in its influence on future studies, specifically in the area of unified modeling of multi-modal understanding and generation. For instance, our work can be extended to interact with LLMs to improve multi-modal understanding and generation capabilities. In particular, our model can be used to generate images or text. It may be exploited to produce some erroneous and unethical information, which needs to be handled carefully before employing our model in practical applications. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the Shenzhen Peacock Program under Grant No. ZX20230597, NSFC under Grant No. 62272130 and Grant No. 62376072, and the Shenzhen Science and Technology Program under Grant No. KCXFZ20211020163403005. It was also supported by the Major Key Project of PCL (PCL2023A08) and the National Science Foundation of China: Multi-source Crossplatform Video Analysis and Understanding for Intelligent Perception in Smart City: U20B2052. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Stanislaw Antol and Aishwarya Agrawal. Vqa: Visual question answering. In ICCV, pages 2425\u20132433, 2015. ", "page_idx": 9}, {"type": "text", "text": "[2] Huiwen Chang and Han Zhang. Maskgit: Masked generative image transformer. In CVPR, pages 11315\u201311325, 2022. ", "page_idx": 9}, {"type": "text", "text": "[3] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015.   \n[4] Yen-Chun Chen and Linjie Li. Uniter: Universal image-text representation learning. In ECCV, pages 104\u2013120. Springer, 2020.   \n[5] Jiajun Deng, Zhengyuan Yang, Tianlang Chen, Wengang Zhou, and Houqiang Li. Transvg: End-to-end visual grounding with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1769\u20131779, 2021.   \n[6] Ming Ding and Zhuoyi Yang. Cogview: Mastering text-to-image generation via transformers. NeurIPS, 34:19822\u201319835, 2021.   \n[7] Xiaoyi Dong and Jianmin Bao. Peco: Perceptual codebook for bert pre-training of vision transformers. In AAAI, volume 37, pages 552\u2013560, 2023.   \n[8] Alexey Dosovitskiy and Lucas Beyer. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021.   \n[9] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, pages 12873\u201312883, 2021.   \n[10] Fernando A Fardo and Victor H Conforto. A formal evaluation of psnr as quality measurement parameter for image segmentation algorithms. arXiv preprint arXiv:1605.07116, 2016.   \n[11] Shuyang Gu and Dong Chen. Vector quantized diffusion model for text-to-image synthesis. In CVPR, pages 10696\u201310706, 2022.   \n[12] Yuchao Gu and Xintao Wang. Rethinking the objectives of vector-quantized tokenizers for image synthesis. arXiv preprint arXiv:2212.03185, 2022.   \n[13] Martin Heusel and Hubert Ramsauer. Gans trained by a two time-scale update rule converge to a local nash equilibrium. NeurIPS, 30, 2017.   \n[14] Mengqi Huang and Zhendong Mao. Not all image regions matter: Masked vector quantization for autoregressive image generation. In CVPR, pages 2002\u20132011, 2023.   \n[15] Mengqi Huang and Zhendong Mao. Towards accurate image coding: Improved autoregressive image generation with dynamic vector quantization. In CVPR, pages 22596\u201322605, 2023.   \n[16] Chao Jia and Yinfei Yang. Scaling up visual and vision-language representation learning with noisy text supervision. In ICML, pages 4904\u20134916. PMLR, 2021.   \n[17] Doyup Lee and Chiheon Kim. Autoregressive image generation using residual quantization. In CVPR, pages 11523\u201311532, 2022.   \n[18] Junnan Li and Ramprasaath Selvaraju. Align before fuse: Vision and language representation learning with momentum distillation. volume 34, pages 9694\u20139705, 2021.   \n[19] Tianhong Li and Huiwen Chang. Mage: Masked generative encoder to unify representation learning and image synthesis. In CVPR, pages 2142\u20132152, 2023.   \n[20] Tsung-Yi Lin and Michael Maire. Microsoft coco: Common objects in context. In ECCV, pages 740\u2013755. Springer, 2014.   \n[21] Xinmiao Lin and Yikang Li. Catch missing details: Image reconstruction with frequency augmented variational autoencoder. In CVPR, pages 1736\u20131745, 2023.   \n[22] Hao Liu and Wilson and Yan. Language quantized autoencoders: Towards unsupervised text-image alignment. arXiv preprint arXiv:2302.00902, 2023.   \n[23] Ziwei Liu and Ping Luo. Deep learning face attributes in the wild. In ICCV, pages 3730\u20133738, 2015.   \n[24] Jiasen Lu and Jianwei Yang. Hierarchical question-image co-attention for visual question answering. NeurIPS, 29, 2016.   \n[25] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 11\u201320, 2016.   \n[26] Tomas Mikolov and Kai Chen. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.   \n[27] Ron Mokady and Amir Hertz. Clipcap: Clip prefix for image captioning. arXiv preprint arXiv:2111.09734, 2021.   \n[28] Varun K Nagaraja, Vlad I Morariu, and Larry S Davis. Modeling context between objects for referring expression understanding. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11\u201314, 2016, Proceedings, Part IV 14, pages 792\u2013807. Springer, 2016.   \n[29] Aaron van den Oord and Yazhe Li. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.   \n[30] Gaurav Parmar, Dacheng Li, Kwonjoon Lee, and Zhuowen Tu. Dual contradistinctive generative autoencoder. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 823\u2013832, 2021.   \n[31] Stanislav Pidhorskyi, Donald A Adjeroh, and Gianfranco Doretto. Adversarial latent autoencoders. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14104\u201314113, 2020.   \n[32] Alec Radford and Jong Wook Kim. Learning transferable visual models from natural language supervision. In ICML, pages 8748\u20138763. PMLR, 2021.   \n[33] Ali Razavi and Aaron Van den Oord. Generating diverse high-fidelity images with vq-vae-2. NeurIPS, 32, 2019.   \n[34] Scott Reed, Zeynep Akata, Honglak Lee, and Bernt Schiele. Learning deep representations of fine-grained visual descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 49\u201358, 2016.   \n[35] Mengye Ren and Ryan Kiros. Exploring models and data for image question answering. NeurIPS, 28, 2015.   \n[36] Yuchen Ren and Zhendong Mao. Crossing the gap: Domain generalization for image captioning. In CVPR, pages 2871\u20132880, 2023.   \n[37] Robin Rombach and Andreas Blattmann. High-resolution image synthesis with latent diffusion models. In CVPR, pages 10684\u201310695, 2022.   \n[38] Robin Rombach and Andreas Blattmann. High-resolution image synthesis with latent diffusion models. In CVPR, pages 10684\u201310695, 2022.   \n[39] Oleksii Sidorov and Ronghang Hu. Textcaps: a dataset for image captioningwith reading comprehension. 2020.   \n[40] Mohammed Suhail and Abhay Mittal. Energy-based learning for scene graph generation. In CVPR, pages 13936\u201313945, 2021.   \n[41] Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transformers. arXiv preprint arXiv:1908.07490, 2019.   \n[42] Aaron Van den Oord and Nal Kalchbrenner. Conditional image generation with pixelcnn decoders. NeurIPS, 29, 2016.   \n[43] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. NeurIPS, 30, 2017. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[44] Ashish Vaswani and Noam Shazeer. Attention is all you need. NeurIPS, 30, 2017. ", "page_idx": 12}, {"type": "text", "text": "[45] Catherine Wah and Steve Branson. The caltech-ucsd birds-200-2011 dataset. 2011. ", "page_idx": 12}, {"type": "text", "text": "[46] Zhibiao Wu and Martha Palmer. Verb semantics and lexical selection. arXiv preprint cmplg/9406033, 1994.   \n[47] Weihao Xia, Yujiu Yang, Jing-Hao Xue, and Baoyuan Wu. Tedigan: Text-guided diverse face image generation and manipulation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2256\u20132265, 2021.   \n[48] Jiahui Yu and Xin Li. Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627, 2021.   \n[49] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in referring expressions. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 69\u201385. Springer, 2016.   \n[50] Lijun Yu and Yong Cheng. Spae: Semantic pyramid autoencoder for multimodal generation with frozen llms. arXiv preprint arXiv:2306.17842, 2023.   \n[51] Baoquan Zhang, Huaibin Wang, Chuyao Luo, Xutao Li, Guotao Liang, Yunming Ye, Xiaochen Qi, and Yao He. Codebook transfer with part-of-speech for vector-quantized image modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7757\u20137766, 2024.   \n[52] Jiahui Zhang and Fangneng Zhan. Regularized vector quantization for tokenized image synthesis. In CVPR, pages 18467\u201318476, 2023.   \n[53] Chuanxia Zheng and Andrea Vedaldi. Online clustered codebook. In ICCV, pages 22798\u201322807, 2023.   \n[54] Yufan Zhou, Bingchen Liu, Yizhe Zhu, Xiao Yang, Changyou Chen, and Jinhui Xu. Shifted diffusion for text-to-image generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10157\u201310166, 2023.   \n[55] Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and Tong Sun. Towards language-free training for text-to-image generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 17907\u201317917, 2022.   \n[56] Lei Zhu and Fangyun Wei. Beyond text: Frozen large language models in visual signal comprehension. arXiv preprint arXiv:2403.07874, 2024. ", "page_idx": 12}, {"type": "text", "text": "A Appendix / supplemental material ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Experiment Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Semantic image synthesis, unconditional generation and image completion: All models follow the default setting of VQ-GAN-Transformer2. Specifically, the vocabulary size, embedding number, and input sequence length are 1024, 1024, and 512, respectively. The layers and heads of the transformer are both 16. The semantic image synthesis experiments are conducted on 1 4090 GPU with a batch size of 15 and one day of training time. The unconditional generation and image completion experiments are conducted on 2 4090 GPUs with a batch size of 36 and one day of training time. ", "page_idx": 13}, {"type": "text", "text": "Text-to-image generation: All models follow the default setting of VQ-Diffusion3. Specifically, the layers of the transformer are 19 with dimension of 1024. The diffusion step is 100. The training epoch is 90 for all models. The experiments are conducted on 1 4090 GPU with a batch size of 24 and two days of training time. ", "page_idx": 13}, {"type": "text", "text": "Image captioning: Inspired by ClipCap [27], we use the trained codes to replace the ClipCap\u2019s prefix embeddings. The model framework is shown in Fig. 9 (a). The training epoch is 100 for all models. The experiments are conducted on 2 4090 GPUs with a batch size of 60 and one day of training time. ", "page_idx": 13}, {"type": "text", "text": "Visual question answering: The COCO-QA [35] dataset is automatically generated from captions in the Microsoft COCO dataset [20]. There are 78,736 train questions and 38,948 test questions in the dataset, These questions are based on 8,000 and 4,000 images respectively. There are four types of questions including object, number, color, and location. Each type takes $70\\%$ , $7\\%$ , $17\\%$ , and $6\\%$ of the whole dataset, respectively. All answers in this data set are single word. Following the image captioning task, we use the last hidden embedding to do VQA, as shown in Fig. 9 (b). Following the [24], we report classification accuracy and Wu-Palmer similarity (WUPS). The training epoch is 50 for all models. The experiments are conducted on 2 4090 GPUs with a batch size of 60 and one day of training time. ", "page_idx": 13}, {"type": "text", "text": "Visual Grounding: The refcoco dataset [49] includes 19,994 images with 50,000 referred objects. Each object has more than one referring expression, and there are 142,210 referring expressions in this dataset. There are two commonly used split protocols for this dataset. One is RefCOCOg-google [25], and the other is RefCOCOgumd [28]. We follow RefCOCOgumd [28] to split the dataset. The train set has 42,404 expressions, the validation set has 3,811 expressions, and the test set has 3,785 expressions. Following [5], we concatenate the image codes and text tokens and feed them into a learnable transformer with coordinate regression layers (i.e., FNN) to predict the object box. The training epoch is 100 for all models. The experiments are conducted on 2 4090 GPUs with a batch size of 30 and several hours of training time. ", "page_idx": 13}, {"type": "image", "img_path": "vA4s3kN4QE/tmp/1981d866bfe5137772332651afa40fe2e4b3543b24154d1c50d2d354c2d26a07.jpg", "img_caption": ["Figure 9: The architecture of the visual text reasoning based on GPT. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "A.2 Experimental comparison with VQCT ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We provide comparisons with VQCT [51] for image reconstruction in Table 12. Moreover, we also provide experimental results on further integrating our method into VQCT to verify its effectiveness and versatility. The results are shown in Table 13. ", "page_idx": 14}, {"type": "table", "img_path": "vA4s3kN4QE/tmp/f0653937fd129b6b7b2d8081c76c07fb9bb2c34078d6cfde4bc1ca63aed91731.jpg", "table_caption": ["Table 12: Comparison of VQCT [51] and our method on reconstruction "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "vA4s3kN4QE/tmp/05fa0b883ace21fe24932670a62f44cd8ec9f45c1b0b632ff536d28dcdda6eef.jpg", "table_caption": ["Table 13: Comparison of reconstruction and VQA on VQCT and $\\mathrm{VQCT+LG}$ on the MS-COCO dataset. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.3 More Examples and Qualitative Results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We provide more examples of image reconstruction in Fig. 10, image-to-text retrieval in Fig. 11 and Fig. 12. We also provide more image synthesis results in Fig. 13 for semantic image synthesis, and text-to-image synthesis in Fig. 14. We provide some examples of image captioning in Fig. 15 and VQA in Fig. 16. We provide some examples of unconditional generation in Fig. 18, and image completion in Fig. 17. We also provide a qualitative comparison of visual grounding in Fig. 19. ", "page_idx": 14}, {"type": "image", "img_path": "vA4s3kN4QE/tmp/8afaa451ae877ca12f32ddafbe758c46a010b6675e3bd8171644970626d071fe.jpg", "img_caption": ["Figure 10: Reconstruction from different models on four datasets. The red-color boxes highlight reconstruction details. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "vA4s3kN4QE/tmp/2cdce100731cdd7a13b52189981bc39685751c3d0b854ace4eb56fccd66c5d3f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 11: Examples of the top-5 most similar text selected on Textcaps based on VQ-GAN+LG.   \nThe bold text means the same as the ground truth result. Figure 12: Examples of the top-5 most similar text selected on CUB-200 based on VQ-GAN+LG.   \nThe bold text means the same as the ground truth result. ", "page_idx": 15}, {"type": "image", "img_path": "vA4s3kN4QE/tmp/e528fa48d204e47acd365e851a35366ba866e688fa3fe9b5cabc1357f1904579.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "image", "img_path": "vA4s3kN4QE/tmp/86294105a15e2c35bd141fea1376b16dabcaa0b66ea4d7af8010656993051eca.jpg", "img_caption": ["Figure 13: Semantic image synthesis on CelebA-HQ. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "vA4s3kN4QE/tmp/d6d88b39947e48dcaf8f3d1d403ce5fbc22a246905746a8ded6b332e45a80160.jpg", "img_caption": ["Figure 14: More text-to-image generation on CelebA-HQ. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "vA4s3kN4QE/tmp/3f040f2f2e8013b3d5e8408a6a1fa354ade5e871b2fbd743a6e95687fdacb4ce.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 15: Image Captioning on CUB-200 based on VQ-GAN and VQ-GAN+LG. ", "page_idx": 16}, {"type": "image", "img_path": "vA4s3kN4QE/tmp/4a5f0c5225b2186b2db8ec91567da913c912586d88cb947bac96fbe486a97ee9.jpg", "img_caption": ["Figure 16: VQA on COCO-QA based on VQ-GAN and VQ-GAN+LG. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "vA4s3kN4QE/tmp/973458528324f848e0a8fb6e9199b1e5697c5519cab2a42417df3d921846bca6.jpg", "img_caption": ["Figure 17: Image completion on CelebA-HQ. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "vA4s3kN4QE/tmp/88e725ff2817f012c38a2ef9f332373661dac43f4f1e662f98c88ac2abc648b7.jpg", "img_caption": ["Figure 18: Examples of unconditional image generation on CelebA-HQ based on VQ-GAN+LG. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "vA4s3kN4QE/tmp/65db145f1a39940821f857239f1be86f58b34d36bfb2a9e0204aaa37f0e1b9d8.jpg", "img_caption": ["Figure 19: Examples of visual grounding on refcoco. Blue boxes are the ground-truth, red boxes are the model predictions. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: See the last paragraph in Section 1. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: See the second paragraph in Section 5. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: Our work mainly involves empirical contributions. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We provide our code in the supplementary materials. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We provide our code and dataset information in the supplementary material. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We provide detailed experimental settings in Section 4.1 and Appendix A.1. In the supplementary materials, we provide our code, which contains more details of the model and parameters. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: See the third paragraph \u201cVisual Question Answering\u201d in Section 4.4.2. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We discuss our model\u2019s computational resources in Appendix A.1. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: See the last paragraph in Section 5. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: We are using publicly available datasets for all experiments ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We cite all used public datasets in Section 4 and Appendix A.1. All datasets are publicly available. They are under a non-commercial license. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 23}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our code and pre-trained model will be released later for the assets. We are using publicly available datasets for all experiments. No personally identifiable information is involved. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: No human subjects were involved in our experiments. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: No human subjects were involved in our experiments. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]