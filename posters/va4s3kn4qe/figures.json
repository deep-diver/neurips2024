[{"figure_path": "vA4s3kN4QE/figures/figures_1_1.jpg", "caption": "Figure 1: To answer the question, one not only needs to identify \u201cwomen\u201d and \u201cracket", "description": "This figure shows an example of a visual question answering (VQA) task. The image shows a woman holding a tennis racket on a grass court.  To answer the question \u201cWhat is this woman holding?\u201d, a model needs not only identify the objects \"woman\" and \"racket\", but also understand the semantic relationship between them, i.e., the woman is \"holding\" the racket.  This illustrates the importance of considering semantic relationships in multi-modal tasks.", "section": "1 Introduction"}, {"figure_path": "vA4s3kN4QE/figures/figures_2_1.jpg", "caption": "Figure 2: The overall architecture of the proposed LG-VQ method. The right part of the figure is the basic VQ-VAE module, left is our language-guided module, which consists of three losses: global semantic alignment (Lgsa), masked text prediction (Lmtp), and relationship alignment supervision (Lras). Here, pre-trained text information guides discrete code tokens of an image to learning rich semantic knowledge based on three losses.", "description": "The figure shows the architecture of the Language-Guided Vector Quantization (LG-VQ) model.  The right side depicts the standard VQ-VAE (Vector Quantized Variational Autoencoder) architecture, which encodes an image into a sequence of discrete codes using an encoder and a codebook, and then reconstructs the image using a decoder. The left side shows the LG-VQ's additions: a language-guided module that leverages pre-trained text semantics. This module consists of three loss functions: global semantic alignment (Lgsa), masked text prediction (Lmtp), and relationship alignment (Lras).  These losses aim to align the image codebook with the text semantics, improving the model's ability to handle multi-modal tasks. The pre-trained text is processed to obtain text semantics, which are then integrated into the codebook learning process via these three loss functions.", "section": "3 Methodology"}, {"figure_path": "vA4s3kN4QE/figures/figures_4_1.jpg", "caption": "Figure 3: Illustration of relationship alignment module, we use Zvt to align with two words, then inject the semantic relationship of two words into Z codes.", "description": "This figure illustrates the process of the Relationship Alignment Module in LG-VQ.  It shows how semantic relationships between words are transferred into the codebook (Z).  First, the visual tokens (Zvt) are aligned with words using a similarity measure. Then, the semantic relationship between these words is calculated. Finally, this relationship is used to adjust the codebook embeddings (Z) to better capture the relationships between words, enhancing the alignment between the visual tokens and text.", "section": "3.2.2 Relationship Alignment Module"}, {"figure_path": "vA4s3kN4QE/figures/figures_6_1.jpg", "caption": "Figure 2: The overall architecture of the proposed LG-VQ method. The right part of the figure is the basic VQ-VAE module, left is our language-guided module, which consists of three losses: global semantic alignment (Lgsa), masked text prediction (Lmtp), and relationship alignment supervision (Lras). Here, pre-trained text information guides discrete code tokens of an image to learning rich semantic knowledge based on three losses.", "description": "This figure illustrates the architecture of the Language-Guided Vector Quantization (LG-VQ) method. The architecture is divided into two main parts: a basic VQ-VAE module (right) and a language-guided module (left). The VQ-VAE module is a standard vector quantization model for image reconstruction, while the language-guided module incorporates pre-trained text information to guide the codebook learning process.  The language-guided module consists of three loss functions: global semantic alignment (Lgsa), masked text prediction (Lmtp), and relationship alignment supervision (Lras). These losses work together to align the learned codebook with the input text, resulting in a multi-modal codebook that can be used for various downstream tasks. In essence, the pre-trained text helps the model learn richer semantic information for improved performance in downstream, multi-modal tasks. ", "section": "3 Methodology"}, {"figure_path": "vA4s3kN4QE/figures/figures_6_2.jpg", "caption": "Figure 3: Illustration of relationship alignment module, we use Zvt to align with two words, then inject the semantic relationship of two words into Z codes.", "description": "This figure illustrates the process of the Relationship Alignment Module.  First, the visual tokens (Zvt) are aligned with words using a pre-trained word embedding. Then, the semantic relationship between those two words is calculated. Finally, this semantic relationship is injected into the original codebook (Z) to further refine the alignment between visual and textual information. This step helps improve the model's performance on complex reasoning tasks by incorporating relational context.", "section": "3.2.2 Relationship Alignment Module"}, {"figure_path": "vA4s3kN4QE/figures/figures_7_1.jpg", "caption": "Figure 2: The overall architecture of the proposed LG-VQ method. The right part of the figure is the basic VQ-VAE module, left is our language-guided module, which consists of three losses: global semantic alignment (Lgsa), masked text prediction (Lmtp), and relationship alignment supervision (Lras). Here, pre-trained text information guides discrete code tokens of an image to learning rich semantic knowledge based on three losses.", "description": "This figure illustrates the architecture of the Language-Guided Vector Quantization (LG-VQ) model. The model consists of two main parts: a basic VQ-VAE module (right) and a language-guided module (left). The VQ-VAE module is responsible for encoding images into discrete code tokens. The language-guided module takes pre-trained text semantics as input and uses three losses (global semantic alignment, masked text prediction, and relationship alignment) to guide the codebook learning process. This alignment ensures the codebook effectively captures rich semantic information from the text, leading to improved performance in multi-modal downstream tasks.", "section": "3 Methodology"}, {"figure_path": "vA4s3kN4QE/figures/figures_7_2.jpg", "caption": "Figure 7: Visualization of the codebook of VQ-GAN and LG-VQ on TextCaps and MS-COCO.", "description": "This figure visualizes the codebooks learned by VQ-GAN and LG-VQ on two datasets: TextCaps and MS-COCO.  The visualization shows that LG-VQ leads to a more diverse and effectively utilized codebook compared to VQ-GAN.  For TextCaps, VQ-GAN uses only 18.62% of its codebook, while LG-VQ uses 43.58%.  The difference is even more pronounced on MS-COCO, where VQ-GAN uses 40.09% of its codebook, compared to 97.89% for LG-VQ.  This indicates that LG-VQ learns a more comprehensive and representative codebook.", "section": "4.2 Discussion of Results"}, {"figure_path": "vA4s3kN4QE/figures/figures_8_1.jpg", "caption": "Figure 8: Text-to-image synthesis and semantic image synthesis on CelebA-HQ. Text with background color emphasizes generated details", "description": "This figure shows the results of text-to-image and semantic image synthesis experiments performed on the CelebA-HQ dataset using four different models: VQ-GAN, VQ-GAN+LG, CVQ, and CVQ+LG.  The text conditions are shown above each set of generated images. The images generated by VQ-GAN+LG and CVQ+LG, which incorporate the proposed LG-VQ method, appear to better capture the details specified in the text conditions, such as hair color, facial features, and accessories.  The background color highlights these details.", "section": "4.4 Application"}, {"figure_path": "vA4s3kN4QE/figures/figures_13_1.jpg", "caption": "Figure 2: The overall architecture of the proposed LG-VQ method. The right part of the figure is the basic VQ-VAE module, left is our language-guided module, which consists of three losses: global semantic alignment (Lgsa), masked text prediction (Lmtp), and relationship alignment supervision (Lras). Here, pre-trained text information guides discrete code tokens of an image to learning rich semantic knowledge based on three losses.", "description": "This figure shows the overall architecture of the proposed LG-VQ model.  The model combines a basic VQ-VAE (vector quantization variational autoencoder) module with a language-guided module. The language-guided module uses three losses to incorporate pre-trained text semantics into the codebook: global semantic alignment, masked text prediction, and relationship alignment.  This alignment improves the learning of multi-modal knowledge and enhances performance on downstream tasks.", "section": "3 Methodology"}, {"figure_path": "vA4s3kN4QE/figures/figures_14_1.jpg", "caption": "Figure 10: Reconstruction from different models on four datasets. The red-color boxes highlight reconstruction details.", "description": "This figure shows the image reconstruction results from four different models (VQ-GAN, VQ-GAN+LG, CVQ, and CVQ+LG) on four datasets (CelebA-HQ, CUB-200, MS-COCO, and TextCaps). The red boxes highlight the reconstruction details, showing where the models struggle or succeed in reconstructing the images. This helps in visually comparing the performance of the different models on different types of images.", "section": "4.2 Discussion of Results"}, {"figure_path": "vA4s3kN4QE/figures/figures_15_1.jpg", "caption": "Figure 2: The overall architecture of the proposed LG-VQ method. The right part of the figure is the basic VQ-VAE module, left is our language-guided module, which consists of three losses: global semantic alignment (Lgsa), masked text prediction (Lmtp), and relationship alignment supervision (Lras). Here, pre-trained text information guides discrete code tokens of an image to learning rich semantic knowledge based on three losses.", "description": "This figure shows the architecture of the proposed LG-VQ model. The model is composed of two main modules: a basic VQ-VAE module and a language-guided module. The VQ-VAE module is responsible for encoding and decoding images using a codebook. The language-guided module uses pre-trained text semantics to guide the learning of the codebook, aiming to improve the performance of multi-modal downstream tasks. The language-guided module uses three losses: global semantic alignment loss, masked text prediction loss, and relationship alignment supervision loss. These losses help to align the codebook with text semantics, resulting in a more expressive codebook.", "section": "3 Methodology"}, {"figure_path": "vA4s3kN4QE/figures/figures_15_2.jpg", "caption": "Figure 2: The overall architecture of the proposed LG-VQ method. The right part of the figure is the basic VQ-VAE module, left is our language-guided module, which consists of three losses: global semantic alignment (Lgsa), masked text prediction (Lmtp), and relationship alignment supervision (Lras). Here, pre-trained text information guides discrete code tokens of an image to learning rich semantic knowledge based on three losses.", "description": "This figure illustrates the architecture of the Language-Guided Vector Quantization (LG-VQ) model.  The architecture is divided into two main parts. The right side shows the standard VQ-VAE (Vector Quantized Variational Autoencoder) module responsible for encoding and decoding images. The left side shows the novel language-guided module which incorporates pre-trained text semantics to improve the quality of the codebook. This module uses three loss functions: global semantic alignment (Lgsa), masked text prediction (Lmtp), and relationship alignment supervision (Lras) to align the codebook with the text, thereby leveraging rich semantic information from the text to enhance the quality and multi-modal capabilities of the codebook.", "section": "3 Methodology"}, {"figure_path": "vA4s3kN4QE/figures/figures_16_1.jpg", "caption": "Figure 13: Semantic image synthesis on CelebA-HQ.", "description": "This figure shows the results of semantic image synthesis on the CelebA-HQ dataset using different methods. The results demonstrate that the proposed LG-VQ method can generate high-quality images with specific semantic attributes. The figure is divided into several rows, each row showing the results for a different image. Each row consists of three columns. The first column shows the input image, the second column shows the output generated by VQ-GAN+LG, and the third column shows the output generated by CVQ+LG. The images are accompanied by masks representing the semantic segmentation of the input image. Each mask represents different semantic attributes. In the figure, the masks and generated images can be seen to be well aligned with the semantic attributes of the input images.", "section": "4.4 Application"}, {"figure_path": "vA4s3kN4QE/figures/figures_16_2.jpg", "caption": "Figure 8: Text-to-image synthesis and semantic image synthesis on CelebA-HQ. Text with background color emphasizes generated details", "description": "This figure shows the results of text-to-image synthesis and semantic image synthesis experiments conducted using the CelebA-HQ dataset.  The top row demonstrates text-to-image generation where the model generates images based on given text descriptions. The text descriptions are shown in light-blue boxes above each image, highlighting specific features or attributes.  The bottom row showcases semantic image synthesis, where the model generates images corresponding to the given semantic descriptions (also shown in light-blue boxes). The color of the text background highlights the features mentioned in the descriptions. The purpose is to illustrate that the model can accurately generate images that closely match both the text and semantic attributes.", "section": "4.4 Application"}, {"figure_path": "vA4s3kN4QE/figures/figures_16_3.jpg", "caption": "Figure 15: Image Captioning on CUB-200 based on VQ-GAN and VQ-GAN+LG.", "description": "This figure shows a qualitative comparison of image captioning results on the CUB-200 dataset using VQ-GAN and the proposed LG-VQ model.  For each image, the automatically generated captions from both models are displayed, highlighting the differences in the descriptions produced by each model. The LG-VQ model incorporates language-guided codebook learning, aiming to improve the quality and accuracy of the generated captions.", "section": "4.4 Application"}, {"figure_path": "vA4s3kN4QE/figures/figures_17_1.jpg", "caption": "Figure 6: Visualization of words similarity and image codes similarity aligned with the word. We extract some representative words from the text as a demonstration.", "description": "This figure visualizes the similarity between words and their corresponding image codes in the LG-VQ model.  The top row shows a similarity matrix for words, highlighting the semantic relationships between them. The bottom row displays the similarity between the codes and words after alignment through the model, demonstrating how the model learns to capture semantic information from text and integrate it into its representation of images.  The figure provides evidence of successful text-image alignment and the capability of the LG-VQ model to learn meaningful relationships between semantic representations of text and visual features.", "section": "4.2 Discussion of Results"}, {"figure_path": "vA4s3kN4QE/figures/figures_17_2.jpg", "caption": "Figure 10: Reconstruction from different models on four datasets. The red-color boxes highlight reconstruction details.", "description": "This figure shows the image reconstruction results from four different models (VQ-GAN, VQ-GAN+LG, CVQ, CVQ+LG) on four datasets (TextCaps, CelebA-HQ, CUB-200, MS-COCO).  The red boxes highlight specific details to illustrate the differences in reconstruction quality between the models. This helps in visually comparing the performance of the proposed LG-VQ method against existing methods on various datasets and image types.", "section": "4.2 Discussion of Results"}, {"figure_path": "vA4s3kN4QE/figures/figures_17_3.jpg", "caption": "Figure 10: Reconstruction from different models on four datasets. The red-color boxes highlight reconstruction details.", "description": "This figure compares the image reconstruction results of four different models (VQ-GAN, VQ-GAN+LG, CVQ, CVQ+LG) on four different datasets (TextCaps, CUB-200, CelebA-HQ, MS-COCO).  Red boxes highlight details to emphasize the differences in reconstruction quality between the models and datasets.  It visually demonstrates the improved reconstruction capability of the proposed LG-VQ method by comparing it to the baseline models.", "section": "4.2 Discussion of Results"}, {"figure_path": "vA4s3kN4QE/figures/figures_18_1.jpg", "caption": "Figure 19: Examples of visual grounding on refcoco. Blue boxes are the ground-truth, red boxes are the model predictions.", "description": "This figure shows the results of visual grounding experiments on the refcoco dataset.  The task involves locating specific objects within an image based on a textual description.  For each image, the ground truth bounding box (blue) for the described object is shown next to the model's prediction (red). By visually comparing the two, one can assess the model's accuracy in locating the correct object based on the textual cue. This demonstrates the performance of the LG-VQ model in handling cross-modal tasks.", "section": "4.4.3 Visual Grounding"}]