[{"figure_path": "JNl6h3U3oW/tables/tables_1_1.jpg", "caption": "Table 1: Hardware cost under 45nm CMOS [27, 69, 23, 50, 5].", "description": "This table presents the hardware costs, in terms of energy consumption (in picojoules) and area (in square micrometers), for different arithmetic operations (multiplication, addition, and shift) under a 45nm CMOS technology.  The costs are broken down further by data type (FP32, FP16, INT32, INT8) where applicable.  Additionally, the cost of a lookup table (LUT) operation for an 8-bit query is provided.", "section": "1 Introduction"}, {"figure_path": "JNl6h3U3oW/tables/tables_2_1.jpg", "caption": "Table 2: Perplexity comparisons of the OPT models on WikiText-2. Note that we set the group size of all methods as the length of rows following the setting of OPTQ [18] for a fair comparison.", "description": "This table compares the perplexity scores achieved by different methods (FP16, OPTQ, LUT-GEMM, AWQ, and ShiftAddLLM) on the WikiText-2 dataset using various OPT models (125M, 350M, 1.3B, 2.7B, 6.7B, 13B, 30B, 66B parameters).  The comparison is made for different bit precisions (3-bit and 2-bit).  The table shows the superior performance of the proposed ShiftAddLLM method, especially at lower bit precisions where other methods struggle.", "section": "5.2 ShiftAddLLM over SOTA LLM Quantization Baselines"}, {"figure_path": "JNl6h3U3oW/tables/tables_5_1.jpg", "caption": "Table 2: Perplexity comparisons of the OPT models on WikiText-2. Note that we set the group size of all methods as the length of rows following the setting of OPTQ [18] for a fair comparison.", "description": "This table presents a comparison of perplexity scores achieved by different methods (FP16, OPTQ, LUT-GEMM, AWQ, and ShiftAddLLM) on the WikiText-2 dataset using OPT models of varying sizes (125M, 350M, 1.3B, 2.7B, 6.7B, 13B, 30B, 66B parameters).  The comparison is done across different bit precisions (16-bit, 3-bit, and 2-bit) for each method. The group size for all methods is consistent with the setting used in the OPTQ paper for a fair evaluation.", "section": "5.2 ShiftAddLLM over SOTA LLM Quantization Baselines"}, {"figure_path": "JNl6h3U3oW/tables/tables_5_2.jpg", "caption": "Table 3: Perplexity comparisons of the LLaMA models on WikiText-2. The group size is set to 128 following [48, 38].", "description": "This table compares the perplexity scores achieved by different methods (OPTQ, LUT-GEMM, AWQ, and ShiftAddLLM) on the WikiText-2 benchmark using three different sizes of LLaMA language models (7B, 13B, and 70B parameters).  The group size, a parameter in the quantization process, was set to 128, matching the settings of prior work.  The table highlights the perplexity results for each method and model size, allowing for a comparison of the effectiveness of different quantization techniques.", "section": "5.2 ShiftAddLLM over SOTA LLM Quantization Baselines"}, {"figure_path": "JNl6h3U3oW/tables/tables_6_1.jpg", "caption": "Table 2: Perplexity comparisons of the OPT models on WikiText-2. Note that we set the group size of all methods as the length of rows following the setting of OPTQ [18] for a fair comparison.", "description": "This table presents the perplexity scores achieved by different models on the WikiText-2 dataset.  It compares the performance of the original FP16 model with several quantization methods (OPTQ, LUT-GEMM, AWQ, and ShiftAddLLM) at both 3-bit and 2-bit precisions. Lower perplexity indicates better performance.", "section": "5.2 ShiftAddLLM over SOTA LLM Quantization Baselines"}, {"figure_path": "JNl6h3U3oW/tables/tables_7_1.jpg", "caption": "Table 3: Perplexity comparisons of the LLaMA models on WikiText-2. The group size is set to 128 following [48, 38].", "description": "This table compares the perplexity scores achieved by different methods (OPTQ, LUT-GEMM, AWQ, and ShiftAddLLM) on the WikiText-2 benchmark using LLaMA language models of various sizes (7B, 13B, 70B).  The group size is kept constant at 128 to ensure a fair comparison. The table shows perplexity results for 3-bit and 2-bit quantization levels.", "section": "5.2 ShiftAddLLM over SOTA LLM Quantization Baselines"}, {"figure_path": "JNl6h3U3oW/tables/tables_7_2.jpg", "caption": "Table 4: Results on Gemma/Mistral/Bloom models.", "description": "This table presents the perplexity results for three different open-source LLMs (Gemma, Mistral, and Bloom) using different quantization methods (FP16, OPTQ, LUT-GEMM, and ShiftAddLLM).  It shows the perplexity scores at 3-bit precision, comparing the proposed ShiftAddLLM against existing state-of-the-art quantization techniques. The goal is to demonstrate the effectiveness of ShiftAddLLM across various LLMs.", "section": "5.2 ShiftAddLLM over SOTA LLM Quantization Baselines"}, {"figure_path": "JNl6h3U3oW/tables/tables_8_1.jpg", "caption": "Table 5: Accuracy comparisons on seven downstream tasks for OPT-66B and LLaMA-2-70B.", "description": "This table presents a comparison of the accuracy achieved by different methods (Floating Point, OPTQ [18], LUT-GEMM [48], and Ours (Acc.)) on seven downstream tasks for two large language models: OPT-66B and LLaMA-2-70B.  The accuracy is measured using several metrics (ARC_C, ARC_E, Copa, BoolQ, PIQA, Storycloze, RTE, MMLU), and the mean accuracy across all tasks is also reported.  The table shows the results for 3-bit quantized models and compares them against the floating-point baseline, allowing for evaluation of the accuracy trade-off resulting from the quantization techniques.", "section": "5.2 ShiftAddLLM over SOTA LLM Quantization Baselines"}, {"figure_path": "JNl6h3U3oW/tables/tables_8_2.jpg", "caption": "Table 9: A100 GPU latency comparisons on the OPT model family.", "description": "This table presents the latency (in milliseconds) measured on an A100 GPU for different sizes of OPT models (125M, 350M, 1.3B, 2.7B, 6.7B, 13B, 30B, 66B parameters) using various methods: FP16 (full precision), OPTQ, LUT-GEMM, AWQ, Ours (Lat.), and Ours (Mixed) at different bit-widths (2 and 3 bits).  The results show the latency trade-offs for different model sizes and quantization approaches.  Ours (Lat.) refers to the ShiftAddLLM method optimized for low latency, while Ours (Mixed) denotes the optimized mixed bit allocation strategy.", "section": "5.2 ShiftAddLLM over SOTA LLM Quantization Baselines"}, {"figure_path": "JNl6h3U3oW/tables/tables_8_3.jpg", "caption": "Table 7: Performance breakdown analysis.", "description": "This table presents the results of ablation studies on OPT models (6.7B and 13B parameters) to analyze the impact of each component in ShiftAddLLM. It shows the perplexity and latency for three scenarios: only using the post-training shift-and-add reparameterization (Sec 4.1), incorporating the multi-objective optimization (Sec 4.2), and finally adding the automated bit allocation (Sec 4.3). This allows for a quantitative assessment of how each technique contributes to the overall performance and efficiency of ShiftAddLLM.", "section": "5.3 Ablation Studies of ShiftAddLLM"}, {"figure_path": "JNl6h3U3oW/tables/tables_14_1.jpg", "caption": "Table 8: Perplexity comparisons of the OPT models on WikiText-2. Note that we set the group size of all methods as the number of columns following the setting of OPTQ [18] for a fair comparison.", "description": "This table presents the perplexity scores achieved by different quantization methods (OPTQ, LUT-GEMM, AWQ, and ShiftAddLLM) on the WikiText-2 dataset using various bit-widths (3-bit, 2-bit).  It compares the perplexity of these methods against the full-precision (FP16) results for different sizes of the OPT language model.  Lower perplexity indicates better performance.", "section": "5.2 ShiftAddLLM over SOTA LLM Quantization Baselines"}, {"figure_path": "JNl6h3U3oW/tables/tables_14_2.jpg", "caption": "Table 9: A100 GPU latency comparisons on the OPT model family.", "description": "This table presents the latency (in milliseconds) measured on an NVIDIA A100 GPU for different OPT models (125M to 66B parameters) using various quantization methods (FP16, OPTQ, LUT-GEMM, AWQ, Ours (Lat.), and Ours (Mixed)).  The latency is shown for different bit precisions (16-bit, 3-bit, 2-bit, and 2.2-bit) and highlights the performance tradeoffs between accuracy and latency.  It's used to demonstrate the speed improvements of ShiftAddLLM over baseline quantization techniques.", "section": "5.2 ShiftAddLLM over SOTA LLM Quantization Baselines"}, {"figure_path": "JNl6h3U3oW/tables/tables_14_3.jpg", "caption": "Table 10: Energy comparisons on the OPT model family.", "description": "This table presents a comparison of energy consumption (in Joules) for different model sizes of the OPT family (125M, 350M, 1.3B, 2.7B, 6.7B, 13B, 30B, and 66B parameters) across various quantization methods: FP16 (full precision), OPTQ [18], LUT-GEMM [48], Ours (Lat.) at 3 bits, OPTQ [18], LUT-GEMM [48], Ours (Lat.) at 2 bits, and Ours (Mixed) at 2.2 bits.  It highlights the energy savings achieved by the proposed ShiftAddLLM method compared to existing state-of-the-art quantization techniques.  The energy is estimated using an Eyeriss-like hardware accelerator.", "section": "5.2 ShiftAddLLM over SOTA LLM Quantization Baselines"}, {"figure_path": "JNl6h3U3oW/tables/tables_15_1.jpg", "caption": "Table 11: Perplexity comparisons of the LLaMA models on WikiText-2.", "description": "This table compares the perplexity scores achieved by different quantization methods (OPTQ, LUT-GEMM, AWQ, and ShiftAddLLM) on various LLaMA models (7B, 13B, and 70B parameters) using the WikiText-2 dataset. The results show the perplexity at 3-bit and 2-bit quantization levels, providing a comparison of accuracy across different methods.  The group size is set to 128 following the settings used in prior works for a fair comparison.", "section": "5.2 ShiftAddLLM over SOTA LLM Quantization Baselines"}, {"figure_path": "JNl6h3U3oW/tables/tables_15_2.jpg", "caption": "Table 12: A100 GPU latency comparisons of the LLaMA models.", "description": "This table presents the latency results measured on an A100 GPU for various LLaMA models at different bit precisions.  It compares the performance of the proposed ShiftAddLLM against several state-of-the-art quantization methods (OPTQ, LUT-GEMM, AWQ). The latency is presented in milliseconds (ms), and the bit precision is specified for each configuration.  The table shows latency across several model sizes (7B, 13B, 70B parameters).  The results illustrate the speed improvements of ShiftAddLLM compared to the baseline methods.", "section": "5.2 ShiftAddLLM over SOTA LLM Quantization Baselines"}, {"figure_path": "JNl6h3U3oW/tables/tables_15_3.jpg", "caption": "Table 13: Energy comparisons of the LLaMA models.", "description": "This table presents a comparison of energy consumption (in Joules) for different LLaMA models (7B, 13B, 70B parameters) and configurations using various quantization techniques (FP16, OPTQ, LUT-GEMM, ShiftAddLLM).  It shows the energy efficiency improvements achieved by ShiftAddLLM at 3-bit and 2-bit precision, and its mixed-bit allocation strategy.", "section": "5.2 ShiftAddLLM over SOTA LLM Quantization Baselines"}, {"figure_path": "JNl6h3U3oW/tables/tables_16_1.jpg", "caption": "Table 14: Ablation studies on various optimization objectives.", "description": "This table presents the results of ablation studies conducted to evaluate the effectiveness of different optimization objectives used in the ShiftAddLLM model. The objectives compared include using only the weight objective, only the activation objective, a combination of both, and the proposed multi-objective optimization approach used in ShiftAddLLM. The evaluation metric is perplexity on the OPT model for three different sizes (13B, 30B, and 66B parameters). The results show the superiority of the multi-objective optimization approach in terms of achieving lower perplexity.", "section": "4.2 ShiftAddLLM: Multi-objective Optimization"}, {"figure_path": "JNl6h3U3oW/tables/tables_17_1.jpg", "caption": "Table 15: Perplexity comparisons between ShiftAddLLM and OmniQuant using OPT models and LLaMA models on WikiText-2. The group size is set as the length of rows for OPT models and 128 for LLAMA models following baselines.", "description": "This table compares the perplexity results of ShiftAddLLM and OmniQuant on both OPT and LLaMA models for different bit-widths (4, 3, and 2 bits).  It showcases the perplexity achieved on the WikiText-2 dataset for different model sizes within each family. The group size is adjusted to 128 for LLaMA models to maintain consistency with existing baselines.", "section": "E Benchmark with More Recent Baselines"}, {"figure_path": "JNl6h3U3oW/tables/tables_17_2.jpg", "caption": "Table 15: Perplexity comparisons between ShiftAddLLM and OmniQuant using OPT models and LLaMA models on WikiText-2. The group size is set as the length of rows for OPT models and 128 for LLAMA models following baselines.", "description": "This table compares the perplexity achieved by ShiftAddLLM and OmniQuant on the WikiText-2 dataset using OPT and LLaMA models with different bit configurations.  It shows the perplexity scores for various model sizes (125M, 350M, 1.3B, 2.7B, 6.7B, 13B, 30B, 66B for OPT; 7B, 13B, 70B for LLaMA) and bit depths (4-bit and 3-bit). The group size used for comparison is consistent with the baselines.", "section": "E Benchmark with More Recent Baselines"}, {"figure_path": "JNl6h3U3oW/tables/tables_17_3.jpg", "caption": "Table 17: Perplexity and correlation results of our mixed bit allocation.", "description": "This table presents the perplexity results for different LLM models (OPT, LLaMA, and Gemma) using two bit allocation strategies: Ours (Lat.) and Ours (Mixed).  The correlation (\u03c4) values show the high correlation between the proxy criteria and the actual reparameterization error. Ours (Mixed) generally shows lower perplexity than Ours (Lat.) across various model sizes, suggesting the efficacy of the mixed bit allocation strategy.", "section": "F More Results for Mixed Bit Allocation"}, {"figure_path": "JNl6h3U3oW/tables/tables_18_1.jpg", "caption": "Table 18: Perplexity comparisons of the OPT models and LLaMA models with 4-bit quantization on WikiText-2. We set the group size as the length of rows for OPT models and 128 for LLaMA models following baselines for fair comparisons.", "description": "This table compares the perplexity scores achieved by different methods (OPTQ, LUT-GEMM, AWQ, and ShiftAddLLM) on the WikiText-2 dataset using 4-bit quantization.  It shows the perplexity for various sizes of OPT and LLaMA language models. The group size is set to the number of rows for OPT models and 128 for LLaMA models.", "section": "G 4-Bit Results and Explanation for Using Lower Bit Widths"}, {"figure_path": "JNl6h3U3oW/tables/tables_19_1.jpg", "caption": "Table 19: Comparison between MSFP and ShiftAddLLM with varying bits on KL Divergence and Quantization Error.", "description": "This table compares the performance of MSFP and ShiftAddLLM in terms of KL divergence and quantization error at different bit-widths (4, 3, and 2 bits).  The results show that ShiftAddLLM consistently outperforms MSFP across all bit-widths, demonstrating lower KL divergence and quantization error.", "section": "I Additional Clarifications on Eyeriss"}]