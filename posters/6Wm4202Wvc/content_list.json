[{"type": "text", "text": "Label Privacy in Split Learning for Large Models with Parameter-Efficient Training ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 As deep learning models become larger and more expensive, many practitioners   \n2 turn to fine-tuning APIs. These web services allow fine-tuning a model between   \n3 two parties: the client that provides the data, and the server that hosts the model.   \n4 While convenient, these APIs raise a new concern: the data of the client is at   \n5 risk of privacy breach during the training procedure. This challenge presents   \n6 an important practical case of vertical federated learning, where the two parties   \n7 perform parameter-efficient fine-tuning (PEFT) of a large model. In this study, we   \n8 systematically search for a way to fine-tune models over an API while keeping the   \n9 labels private. We analyze the privacy of LoRA, a popular approach for parameter  \n10 efficient fine-tuning when training over an API. Using this analysis, we propose   \n11 $\\mathrm{P^{3}E F T}$ , a multi-party split learning algorithm that takes advantage of existing PEFT   \n12 properties to maintain privacy at a lower performance overhead. To validate our   \n13 algorithm, we fine-tune DeBERTa-v2-XXLarge, Flan-T5 Large and LLaMA-2 7B   \n14 using LoRA adapters on a range of NLP tasks. We find that $\\bar{\\mathrm{P^{3}E F T}}$ is competitive   \n15 with existing privacy-preserving methods in multi-party and two-party setups while   \n16 having higher accuracy. ", "page_idx": 0}, {"type": "text", "text": "17 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "18 One of the main reasons behind deep learning success is its ability to transfer knowledge between   \n19 tasks [34]. When training a model for any particular problem, it is common to reuse previously   \n20 trained models from other, related problems. In the past, this was typically done by downloading   \n21 pre-trained model weights from public hubs, then fine-tuning the said models on the downstream   \n22 task. However, as models grow larger and more compute-intensive, fine-tuning them locally becomes   \n23 an increasingly difficult task. Furthermore, many recent models are not released, but instead made   \n24 available as proprietary services.   \n25 When a model cannot be fine-tuned locally, many practitioners opt instead for the so-called fine-tuning   \n26 APIs [27, 16, 6, 26]. These APIs are web services that host one or several pre-trained models and   \n27 allow clients to perform limited fine-tuning. More specifically, APIs usually allow their clients to run   \n28 parameter-efficient fine-tuning (PEFT), such as LoRA [15] or Prefix-tuning [21]. These techniques   \n29 allow adapting a model to a dataset while training a relatively small number of additional weights,   \n30 which is particularly important for large language or image generation models that have billions of   \n31 parameters.   \n32 Although the fine-tuning APIs can be convenient, they also introduce new risk in terms of data privacy.   \n33 When a client uses such API to train on sensitive data, they need to ensure that their data will stay   \n34 private [7]. This is particularly important when dealing with patient\u2019s medical records, personal   \n35 user data or trade secrets [24, 19]. The two main threats to data privacy are that the API provider   \n36 obtains the private data and that a third party intercepts data in transit. Therefore, data privacy is   \n37 not guaranteed even if the API provider is trusted. Several recent works propose LLM fine-tuning   \n38 protocols that establish a certain level of privacy for multi-party fine-tuning [42, 7, 22]. Unfortunately,   \n39 these algorithms work for a narrow class of fine-tuning algorithms or assume that a client can run   \n40 LLM training locally using an obfuscated version of the model, provided by a remote server [42].   \n41 As a result, these algorithms are impractical for our use case of fine-tuning over an API. The few   \n42 algorithms that are suitable for API fine-tuning guarantee the privacy of input tokens [22], meaning   \n43 that the attacker can infer private training labels.   \n44 In this work, we seek to alleviate this problem by designing a two-party fine-tuning protocol that   \n45 performs standard parameter-efficient fine-tuning with privacy guarantees. We formulate our protocol   \n46 as a special case of split learning (or vertical federated learning), where one side (server) holds the   \n47 pre-trained model and the other (client) has private training data. More specifically, we focus on the   \n48 privacy of client\u2019s training labels. While input privacy is often crucial, there are scenarios where   \n49 input data is publicly available, such as social media user pages. In these cases, labels could include   \n50 ad clicks (known to the social network) or financial information (known to a bank that matches social   \n51 proflies to its internal records). This example further justifies the use of LLMs, as social media pages   \n52 often contain substantial amounts of text, and LLMs excel at processing long-context data.   \n53 Instead of developing a specific privacy-preserving architecture, we seek algorithms that can work   \n54 with popular existing models and PEFT algorithms. Furthermore, our approach relies on the properties   \n55 of parameter-efficient fine-tuning. Notably, since the adapters are compact, both parties can maintain   \n56 multiple sets of adapters and swap between them with relative ease. This allows us to design a   \n57 PEFT-specific algorithm that can solve its task more effectively than general split learning strategies   \n58 [18]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "59 We summarize our main contributions as follows: ", "page_idx": 1}, {"type": "text", "text": "60 \u2022 We analyze Low-Rank Adaptation, a common parameter-efficient fine-tuning algorithm,   \n61 from the perspective of label privacy in the split learning setup. We observe that, despite   \n62 fine-tuning less than $0.1\\%$ of model parameters, PEFT algorithms leak client\u2019s training   \n63 labels against simple attacks that work for modern pretrained transformers.   \n64 \u2022 Based on our analysis, we formulate a framework for privacy-preserving parameter-efficient   \n65 fine-tuning $(\\mathrm{P^{3}E F T)}$ . This framework leverages the properties of PEFT to obfuscate the   \n66 gradients and parameters communicated during fine-tuning with little impact on the fine  \n67 tuned model quality.   \n68 \u2022 To verify the practical viability of $\\mathrm{P^{3}E F T}$ , we conduct experiments on popular real-world   \n69 PEFT workloads1. Specifically, we fine-tune DeBERTa-v2-XXL [13], Flan-T5-Large [4]   \n70 and LLaMA-2 7B [35] on a set of standard language understanding problems. We find that,   \n71 compared to prior split learning algorithms, $\\mathrm{P^{\\tilde{3}}}$ EFT can maintain label privacy throughout   \n72 training with a significantly smaller accuracy drop. ", "page_idx": 1}, {"type": "text", "text": "73 2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "74 2.1 Federated learning and split learning ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "75 Privacy preservation in machine learning has been a subject of active study within several frameworks.   \n76 An important branch of privacy-preserving learning methods is federated learning, or FL [24], which   \n77 can be broadly described as an approach allowing several parties to train a model jointly without   \n78 sharing their private data. In particular, vertical federated learning [12, 43] targets the scenario where   \n79 different features (including the label) of each training instance are kept by different parties.   \n80 One of the most popular approaches to vertical FL for neural networks is split learning [10, 37],   \n81 where each party stores its part of the overall model. To train the model in such an approach, it is   \n82 only necessary to transfer the intermediate activations and the gradients between layers, while the   \n83 data itself is stored at the premises of the participant hosting each layer. In this work, we focus on   \n84 the two-party formulation of split learning, where one side stores the features for each example and   \n85 another one stores the labels.   \n86 Recent works have investigated the setting of two-party split learning from the label leakage per  \n87 spective [38, 28]: because the label party needs to pass the gradients of the loss function to the   \n88 non-label party, it is possible for the latter party to deduce the labels by inspecting the gradients or   \n89 activations or by hijacking the training procedure. Li et al. [18] provide a set of attack methods that   \n90 allow recovering private labels and propose a defense mechanism that injects noise into the gradients;   \n91 however, they test the approach on pretraining smaller models, and we study finetuning large models   \n92 on private downstream data. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "93 2.2 Parameter-efficient finetuning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "94 The majority of large neural networks today are not trained with a specific task in mind: instead, they   \n95 are pretrained on a general objective and then adapted for the downstream problem. Importantly, the   \n96 growth in the size of foundation models has led to the increased popularity of parameter-efficient   \n97 finetuning (PEFT) methods that adapt the model to a given task by training a small number of   \n98 task-specific parameters. There are several prominent approaches to parameter-efficient finetuning,   \n99 ranging from trainable prompts [21, 11], to residual adapters [14, 29]. We focus on Low-Rank   \n100 Adaptation (or LoRA, 15), one of the most popular PEFT methods that adds extra parameters to each   \n101 weight matrix in the form of a low-rank factorization (see Appendix C for a more detailed description).   \n102 Such formulation allows LoRA adapters to be merged into the original weights after finetuning; this   \n103 ability, combined with the simplicity of the method, has made LoRA a broadly popular approach in   \n104 multiple domains. Still, the approach we propose can be applied to any PEFT method.   \n105 Several recent lines of work explore the problem of fine-tuning LLMs with privacy guarantees [44, 31].   \n106 Zhao et al. [46] analyze the viability of prompt tuning for federated learning, and Zhang et al. [45], Liu   \n107 et al. [23] study PEFT algorithms in the setting of horizontal federated learning, that is, where multiple   \n108 users train a shared model on their local private data. Another, more relevant research direction   \n109 considers private fine-tuning in a vertical federated learning scenario, where participants hold different   \n110 model layers [22, 40]. Most of these studies leverage the idea of differential privacy to prove an   \n111 upper bound on how much information is leaked [8]. Unfortunately, these upper bounds are typically   \n112 loose and do not match practical observations for real models. Furthermore, the majority of these   \n113 studies only guarantees privacy of specific parts of the training procedure: for instance, Li et al.   \n114 [22] only protects the input features, and not labels or model parameters. Finally, Xiao et al. [42]   \n115 presents an alternative algorithm that protects client data by running the entire fine-tuning on client   \n116 side by emulating the server-side model layers. While this approach is more holistic, it assumes that   \n117 clients can run fine-tuning locally, which makes it impractical for many real-world users of LLM   \n118 fine-tuning APIs. The primary distinction between our work and these studies is that we investigate   \n119 parameter-efficient adaptation in the setting of split learning: we aim to finetune a model without   \n120 disclosing the labels of examples to the model provider. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "121 3 Privacy-preserving parameter-efficient fine-tuning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "122 In this section, we analyze the privacy of parameter-efficient fine-tuning and propose a protocol for   \n123 two-party parameter-efficient fine-tuning with the desired privacy guarantees. We begin by analyzing   \n124 the privacy of API fine-tuning with popular PEFT algorithms in Sections 3.1 and 3.2. Then, in   \n125 Section 3.3, we formulate a protocol for privately computing gradients over fine-tuning APIs. Finally,   \n126 we formulate the full $\\mathrm{P^{3}E F\\dot{T}}$ protocol in Section 3.4. ", "page_idx": 2}, {"type": "text", "text": "127 3.1 Setup", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "128 To analyze the privacy of API fine-tuning, we first need to formulate a common framework for   \n129 this type of APIs and develop private learning protocols. This step is important, because existing   \n130 fine-tuning APIs greatly vary in what they offer to the client: from closed APIs that require users to   \n131 submit their full training data [27] to more flexible APIs where clients can run individual training   \n132 steps [20, 2, 30]. Similarly to most existing works on split learning, we focus on the latter type of   \n133 APIs that allows clients to run individual forward and backward passes over a remote model. Thus,   \n134 a client can use these APIs to obtain the training gradients for their PEFT adapters, then update   \n135 adapters locally with any optimization method. In our work, we adopt this archetype of fine-tuning   \n136 API as it offers sufficient flexibility to develop privacy-preserving algorithms.   \n137 We formulate fine-tuning over an API for two or more parties: a client, and one or several servers.   \n138 The client owns a training dataset with inputs $X$ and labels $Y$ . In turn, each server has the same   \n139 pre-trained model $h(x_{i},\\breve{\\theta})\\,\\in\\,\\mathcal{R}^{d}$ . Note that the parameters $\\theta$ denote not the pre-trained model   \n140 weights, but the trainable adapter weights for a certain PEFT algorithm. A model can encode an input   \n141 $x_{i}\\in X$ and produce a $d\\!.$ -dimensional vector of activations that depend on the learned adapter weights   \n142 $\\theta$ .   \n149 We further assume that both forward $(\\cdot)$ and backprop $(\\cdot)$ APIs are stateless and deterministic, i.e.   \n150 calling the same API method multiple times (or on multiple servers) with the same inputs produces   \n151 identical results. Thus, if the model uses dropout or any other form of non-determinism, we assume   \n152 that clients provide the random seed as a part of $x$ .   \n153 To fine-tune a model with this API, a client can initialize adapters locally, alongside with a small   \n154 task-specific head2, then train both adapters and the head. For each training batch $\\bar{(x,y)}\\in D$ , a client   \n155 calls forward $(x,\\theta)$ to compute feature representations, then predicts with local \u201chead\u201d and computes   \n156 task-specific loss function $L$ . After that, a client performs backward pass: first, it computes gradients   \n157 w.r.t. local head inputs gh = \u2202\u2202Lh , t hen passes those gradients to a remote server via backprop $(x,\\theta,g_{h})$   \n158 API call to compute gradients w.r.t. $\\frac{\\partial L}{\\partial\\theta}$ . Finally, a client updates both $\\theta$ and local \u201chead\u201d parameters   \n159 using the optimizer of choice.   \n160 Before building more advanced algorithms, let us analyze the privacy of client\u2019s labels under standard   \n161 fine-tuning. We consider an \u201chonest, but curious\u201d attacker model. This means that the server will   \n162 faithfully run the forward and backprop computations as requested by the client without changing   \n163 the results. Furthermore, we assume that servers are independent and do not communicate client\u2019s   \n164 data between each other. However, a server can recover client\u2019s labels by performing arbitrary   \n165 computations using any information it receives from the client. When training in this way, a client   \n166 does not directly communicate training labels to the server. However, it communicates inputs, adapter   \n167 parameters, and gradients. Furthermore, the server communicates input representations that can be   \n168 intercepted by a third party. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "image", "img_path": "6Wm4202Wvc/tmp/3284f89e3d03d5532165e4e655289bfe965a4d504b8c410140e5a255952bcb67.jpg", "img_caption": ["Figure 1: A visualization of top-2 principal components of gradients (top) and activations (bottom) from different fine-tuning steps (left to right). Color indicates the training labels (binary). "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "table", "img_path": "6Wm4202Wvc/tmp/7ba0bf99f9139b8c9e2091ccee990077b3dc3d8210580d476159e76748e8c4ef.jpg", "table_caption": ["143 To allow fine-tuning, a server offers two API methods: "], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "169 3.2 Label Leakage of Standard Split Learning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "170 In Figure 1, we train a DeBERTa-v2-XXL model on the SST-2 [32] sentiment classification dataset.   \n171 The top row depicts the gradients $g_{h}$ communicated by the client when calling backprop $(\\cdot)$ at different   \n172 training stages. In the bottom row, we similarly track activations $h(x,\\theta)$ that server may compute   \n173 based on the specified $x,\\theta$ . We defer further additional figures and details to Section 4.1.   \n174 As we can see, both gradients and activations are arranged in such a way that simple $\\mathbf{k}$ -means   \n175 clustering would reveal which objects have the same label. The training activations (bottom row) do   \n176 not reveal labels right away (at least not against this attack). However, they gradually \u201cleak\u201d private   \n177 label information during training. Informally, it appears that the training gradients gradually pull   \n178 apart the feature representations for each label, until eventually they turn into separate clusters. From   \n179 an information-theoretic perspective, knowing just one vector of gradients or trained activations   \n180 allows the attacker to learn all but one bit3 of information about client\u2019s private labels.   \n181 To summarize, leaving any one data source unprotected (gradients, activations or parameters) would   \n182 already compromise label privacy. However, we found that gradients and activations require different   \n183 means of protection. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "image", "img_path": "6Wm4202Wvc/tmp/b2a376db309c8ff81fb60b696c173b1b6d1eaac1e646eb1cbe01abe7dd35ec17.jpg", "img_caption": ["Figure 2: An intuitive illustration of the proposed fine-tuning protocol. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "184 3.3 Privacy-preserving backpropagation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "185 In this section, we formulate an algorithm for \u201canonymizing\u201d the gradients communicated over a   \n186 single training step with arbitrary PEFT type. Several prior works approach this by modifying the   \n187 training objective or model architecture. However, when dealing with a real-world PEFT workload   \n188 with optimized hyperparameters, changing the model or loss function often results in reduced model   \n189 accuracy4. Thus, we seek an algorithm that preserves both model and training objective.   \n190 We design our algorithm based on an observation that backpropagation is conditionally lin  \n191 ear in output gradients, even when the model itself is nonlinear. Formally, if we take a model   \n192 $h(\\cdot,\\cdot)$ , a fixed set of trainable parameters $\\theta$ and input samples $x$ , the backprop function5 computes   \n193 backprop $\\begin{array}{r}{(x,\\theta,\\frac{\\partial L}{\\partial h(x,\\theta)})\\,=\\,\\frac{\\partial L}{\\partial\\theta}}\\end{array}$ . For convenience, we shorten it to backprop $(x,\\theta,g_{h})=g_{\\theta}$ , where   \n194 $\\begin{array}{r}{g_{h}=\\frac{\\partial L}{\\partial h(x,\\theta)}}\\end{array}$ represents the gradients of some objective function with respect to model activations   \n195 (outputs), and $\\begin{array}{r}{g_{\\theta}=\\frac{\\partial L}{\\partial\\theta}}\\end{array}$ are gradients of the same objective function w.r.t. trainable parameters. In   \n196 this notation, backprop is linear in terms of $g_{h}$ for any fixed $x,\\theta$ .   \n197 This becomes self-evident if we view backprop as multiplying $g_{h}$ by the Jacobian of model outputs   \n198 w.r.t. trainable parameters , \u2202h(\u2202x\u03b8,\u03b8). If x, \u03b8 are constant, the Jacobian is also constant, and backprop   \n199 is a linear operator: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname{backprop}(x,\\theta,{\\frac{\\partial L}{\\partial h(x,\\theta)}})={\\frac{\\partial L}{\\partial\\theta}}={\\frac{\\partial L}{\\partial h(x,\\theta)}}\\times{\\frac{\\partial h(x,\\theta)}{\\partial\\theta}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "200 This observation allows us to design a private backpropagation protocol. To illustrate   \n201 this protocol, let us first consider a distributed API with two identical independent servers   \n202 that offer backprop API. Then, for arbitrary vector $z$ , we can rewrite backprop $(x,\\theta,g_{h})$ as   \n203 backprop $(x,\\theta,\\bar{g_{h}}\\bar{+}z)\\mathrm{+backprop}(x,\\theta,g_{h}\\!-\\!z)$ .   \n204 During API fine-tuning, we obtain backprop $(x,\\theta,g_{h}+z)$ using an API call to server 1, whereas the   \n205 second term backprop $\\iota(x,\\theta,g_{h}-z)$ translates to an API call to server 2. Note that neither of two   \n206 servers has access to the true gradient $g_{h}$ : they only receive the sum $[g_{h}+z]$ . If we sample a large   \n207 noise vector $z$ $(\\mathrm{Var}(z)\\gg\\|g_{h}\\|_{2}^{2})$ , this sum also becomes dominated by noise. However, when both   \n208 API calls finish, a client can sum the results to recover the true gradient of the loss with respect to   \n209 parameters.   \n210 If both requests are processed by the same server, it can obviously recover $g_{h}$ by adding up gradients   \n211 from both calls, which leads us to the final step. Instead of generating a single noise vector, a client   \n212 needs to generate (privately) a set of $m>1$ random vectors $\\hat{g}_{h}^{1},\\ldots,\\hat{g}_{h}^{m}$ and scalars $\\alpha_{1},\\ldots,\\alpha_{m}$ such   \n213 that ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\ng_{h}=\\sum_{i=1}^{m}\\alpha_{i}\\cdot\\hat{g}_{h}^{i}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "214 Then, for each $\\hat{g}_{h}^{i}$ , client computes backprop $(x,\\theta,\\hat{g}_{h}^{i})$ as $m$ parallel API calls. Once this is done,   \n215 client recovers ", "page_idx": 5}, {"type": "equation", "text": "$$\ng_{\\theta}=\\sum_{i=1}^{m}\\alpha_{i}\\cdot\\mathrm{backprop}(x,\\theta,\\hat{g}_{h}^{i}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "216 Note that the client does not reveal $\\alpha_{1},\\ldots,\\alpha_{m}$ to anyone. ", "page_idx": 5}, {"type": "text", "text": "217 The resulting procedure is formulated in Algorithm 1. This algorithm is conceptually similar to   \n218 the secure aggregation protocol for conventional (horizontal) federated learning [1]. This protocol   \n219 allows clients to average their local vector with peers while keeping each individual vector provably   \n220 private. Similarly to our scheme, clients perturb the vector in such a way that the average of perturbed   \n221 vectors remains the same. Unlike Bonawitz et al. [1], our protocol privately backpropagates through   \n222 a server-hosted model by leveraging the conditional linearity of the backpropagation operator.   \nAlgorithm 1 private_backprop \u2014 Privacy-Preserving Backpropagation (from the client\u2019s perspective)   \n1: Input: $x$ inputs, $\\theta$ adapter weights, $g_{h}$ gradients w.r.t. activations, $m>1$ - number of passes   \n2: $\\hat{g}_{h}^{1},\\ldots,\\hat{g}_{h}^{m}$ , \u03b11, . . . , \u03b1m = obfuscate $(g_{h},m)$ \u25b72   \n3: for $j=1,\\dots,m$ do   \n4: $\\hat{g}_{\\theta}^{j}=\\ensuremath{\\mathrm{backprop}}(x,\\theta,\\hat{g}_{h}^{j})$ \u25b7computed by server   \n5: end for   \n6: $\\begin{array}{r}{g_{\\theta}=\\sum_{j=1}^{m}\\alpha_{j}\\cdot\\hat{g}_{\\theta}^{j}}\\end{array}$   \n7: Return: g\u03b8   \n223 The private backpropagation algorithm can allow client to safely compute gradients once, but, in   \n224 practice, client usually needs to run many consecutive steps. This creates an additional vector of   \n225 attack: if the same server receives two sets of parameters $\\theta_{t},\\theta_{t+1}$ , they could potentially recover $g_{\\theta}$   \n226 by inverting the optimizer.   \n227 In the simplest case, if the server somehow knows that the client computes $\\theta_{t+1}=\\theta_{t}-\\eta\\cdot g_{\\theta}$ , then   \n228 they can compute $g_{\\theta}=(\\theta_{t}-\\theta_{t+1})/\\eta$ . While $g_{\\theta}$ does not necessarily leak private labels, a server   \n229 could, in some cases, use $g_{\\boldsymbol{\\theta}}$ to recover $g_{h}$ , either fully (e.g. if Jacobian is invertible), or partially.   \n230 The client has two ways to prevent this attack. The first one is to ensure that no single server runs   \n231 backprop on two consecutive steps. This is easy to do in decentralized systems where there are   \n232 many potential servers. However, even when there is a single server, they could be required to set up   \n233 multiple trusted execution environments [25]. A more risky alternative is to ensure that the gradients   \n234 cannot be reversed from consecutive parameters: randomize initial optimizer statistics or add noise to   \n235 parameters. This solution is easier, but it can slow down training in some cases.   \n236 To summarize, we formulated a procedure that allows a client to compute gradients privately for any   \n237 given model and PEFT type. Furthermore, since Equation 3 recovers true gradients, this obfuscation   \n238 method does not affect the training dynamics. However, as we have shown in Section 3.1, gradients   \n239 are not the only source of privacy leakage. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "240 3.4 Full fine-tuning ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "241 The other major attack vector are training activations. As the model fits to training data, it\u2019s   \n242 intermediate activations $h(x,\\theta)$ allow attackers to recover labels, e.g. by clustering (see Figure 1).   \n243 To combat this issue, we take advantage of the fact that PEFT has few trainable parameters. Instead   \n244 of learning just one set of trainable parameters, a client creates $n$ independent adapter sets $\\theta_{1},...,\\theta_{n}$ .   \n245 Note that this does not require $n$ unique servers: a single server can run multiple sets of adapters.   \n246 Furthermore, a client can alternate between using different servers for the same adapters. During   \n247 forward pass, the outputs of different adapters are mixed together using randomized mixing weights   \n248 W \u2208Rn,d: ", "page_idx": 5}, {"type": "equation", "text": "$$\nh^{\\prime}(x,\\theta_{1},\\ldots,\\theta_{n})=\\sum_{i=1}^{n}W_{i}\\odot h(x,\\theta_{i})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "224590 Overall, we design this model in such a way the combined model $h^{\\prime}$ can predict the labels, but the   \n251 adapters $h(x,\\theta_{i})$ do not allow predicting these labels without knowing the mixing weights W. The   \n252 mixing weights are generated such that initial activations $h^{\\prime}(x,\\ldots)$ are equal to mean $h(x,\\cdot)$ for all   \n253 $x$ . To achieve this, we generate $\\mathrm{W}$ as follows: first, we generate $n\\cdot(n-1)/2$ d-dimensional random   \n254 vectors $\\xi_{i,j}\\in\\mathcal{R}^{d}\\!\\!\\forall i\\in[1,n],j\\in[i+1,n]$ . Then, we add them up in the following way: ", "page_idx": 6}, {"type": "equation", "text": "$$\nW=\\left(\\begin{array}{c}{\\frac{1}{n}e+\\xi_{1,2}+\\xi_{1,3}+\\cdot\\cdot+\\xi_{1,n}}\\\\ {-\\xi_{1,2}+\\frac{1}{n}e+\\xi_{2,3}+\\cdot\\cdot+\\xi_{2,n}}\\\\ {\\cdot\\cdot}\\\\ {-\\xi_{1,n}-\\xi_{2,n}-\\xi_{3,n}-\\cdot\\cdot+\\frac{1}{n}e}\\end{array}\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "255 Here, $e$ stands for a vector of all ones. The purpose of these mixing weights is to ensure that the   \n256 gradients w.r.t. individual $h(x,\\theta_{i})$ are obfuscated, but the averaged model behaves the same as   \n257 regular PEFT adapter. To illustrate this, consider $n{=}2$ identical LoRA adapters $\\theta_{1},\\theta_{2}$ . During the   \n258 first training step $h(x,\\theta_{1})=h(x,\\theta_{2})$ . Therefore, ", "page_idx": 6}, {"type": "equation", "text": "$$\nh^{\\prime}(x,\\theta_{1},\\ldots,\\theta_{n})=(1/2e+\\xi_{1,2})\\odot h(x,\\theta_{1})+(1/2e-\\xi_{1,2})\\odot h(x,\\theta_{2})=h(x,\\theta_{1})\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "259 However, the two adapters will learn different functions as they receive different gradients. From the   \n260 first update on, $h^{\\prime}$ will be equal to an average of adapter predictions.   \n261 Finally, to ensure that individual adapters $h(x,\\theta)$ do not accidentally \u201clearn to leak\u201d labels, we   \n262 maintain this over the course of training with a privacy regularizer inspired by [9]. This ensures that   \n263 it is impossible to predict labels from individual adapters $\\bar{h}(x,\\theta_{i})$ . Intuitively, on each training step,   \n264 client fits $n$ linear \u201cheads\u201d that learn to predict labels $y$ from $h(x,\\theta_{i})$ , then performs an adversarial   \n265 update of $\\theta_{i}$ to prevent the \u201chead\u201d from predicting $y$ . Formally, each of $n$ \u201cheads\u201d minimize the same   \n266 objective function as the full model. For instance, if the full model solves multi-class classification,   \n267 each head is trained to minimize cross-entropy: ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\eta_{i}^{*}=\\underset{\\eta_{i}}{\\arg\\operatorname*{min}}\\sum_{x,y\\in D}-y\\cdot\\log\\frac{e^{\\langle\\eta_{i j},h(x,\\theta_{i})\\rangle}}{\\sum_{k}e^{\\langle\\eta_{i k},h(x,\\theta_{i})\\rangle}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "268 ", "page_idx": 6}, {"type": "text", "text": "269 where y is one-hot encoding of the correct class. ", "page_idx": 6}, {"type": "text", "text": "270 The whole adversarial update takes place locally on client\u2019s side, using the same $h(x,\\theta)$ it uses for the   \n271 main training objective. The resulting procedure appears complicated but it typically takes negligible   \n272 time compared to running the large pre-trainied model $h(x,\\theta)$ . Furthermore, since adversarial \u201cheads\u201d   \n273 are linear, minimizing the objective above is done with standard logistic regression solver.   \n274 To summarize, our approach combines the two proposed ideas: we use the private backpropagation   \n275 algorithm from Section 3.3 to protect the gradients, then trains a mixture of adapters in such a way   \n276 that obfuscates learned activatons leaking labels. The resulting procedure is described in Algorithm 2.   \n277 In the next section, we will evaluate the efficacy of $\\mathrm{P^{3}E F T}$ on popular NLP benchmarks. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "278 4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "279 The main goal of our study is to find a practical method of private fine-tuning that would scale to   \n280 large models. Because our approach leverages parameter-efficient fine-tuning techniques, we evaluate   \n281 $\\mathrm{{P^{3}}}$ EFT with fine-tuning Transformer models on popular NLP benchmarks that these techniques were   \n282 designed for.   \n283 To that end, we chose three pre-trained models: DeBERTa-XXLarge [13], Flan-T5-Large [4] and   \n284 LLaMA-2 7B [35]. We train these models on several datasets from the GLUE benchmark [39]:   \n285 SST-2 [32], MNLI [41] and QNLI. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "286 4.1 Privacy of gradients and activations ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "287 For this experiment, we train DeBERTa-XXLarge on SST-2 dataset using LoRA adapters with   \n288 hyperparameters from [15]. First, we train the model locally and track model activations $h$ and   \n289 gradients w.r.t. those activations. We apply principal component analysis to them and plot the first   \n290 2 dimensions in Figure 1. Similarly, we visualize gradients of individual per-sample loss functions   \n291 w.r.t. LoRA parameters $\\theta$ in Figure 3 (top row). The results suggest that a hypothetical attacker could   \n292 easily recover private labels by performing K-Means clustering over any data source: activations,   \n293 gradients with respect to activations, or individual gradients with respect to parameters.   \n294 Next, we run the same experiment using privacy-preserving backpropagation as defined in Section 3.3.   \n295 We use $n=2$ with the noise variance set to 1000. As expected, we observed the same learning curve   \n296 as with normal training. However, instead of sending gradients w.r.t. activations to the server, a   \n297 client uses specially crafted random noise vectors that are not informative. In Figure 3 (bottom) we   \n298 plot the same kind of individual gradients as in the top row, except that we visualize the gradients   \n299 computed by the first of the two servers. Finally, we train XGBoost [3] with default hyperparameters   \n300 to predict labels given the noisy gradients (pre-PCA): the resulting classifier is able to fti the training   \n301 data perfectly, but has at most $50.4\\%$ accuracy on a balanced test set. ", "page_idx": 6}, {"type": "image", "img_path": "6Wm4202Wvc/tmp/6c00960f30551de8f2d3116d163f69eb07a0afa71b12c42ef499ef2efb518fc7.jpg", "img_caption": ["Figure 3: Gradients of cross-entropy w.r.t. LoRA parameters for DeBERTa-v2-XXLarge. The top row corresponds to normal backpropagation and the bottom row uses privacy-preserving backprop. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "302 4.2 Main fine-tuning experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "303 Next, we evaluated the entire P3EFT algorithm. To control tasks and model type, we examined   \n304 DeBERTa and Flan-T5 across all four datasets mentioned above, in addition to evaluating LLaMA on   \n305 SST2 and QNLI datasets. For each setup, we compare against three baselines:   \n306 \u2022 Without LoRAs. In this baseline, the client gathers $h$ activations at the beginning (with no   \n307 adapters), then proceeds to train local \u201chead\u201d layers using these activations. This method cannot   \n308 leak information about training labels except for what is stored in X.   \n309 \u2022 Regular fine-tuning (Regular FT) refers to training a single LoRA adapter normally. This baseline   \n310 represents an upper bound on model accuracy, but lacks privacy.   \n311 \u2022 Distance Correlation (DC). Our re-implementation of the distance correlation defense formulated   \n312 in [33] for Transformer models.   \n313 For each algorithm, we evaluated a task-specific metric (accuracy or F1), as well as the privacy   \n314 leakage value for the 3 following measures:   \n315 \u2022 Spectral attack AUC \u2014 a measure of vulnerability to an attack proposed in [33], measured as   \n316 classifier ROC AUC: lower value corresponds to better privacy.   \n317 \u2022 Norm attack AUC \u2014 vulnerability to a variant of attack proposed in [18], measured as classifier   \n318 ROC AUC (lower is better). Despite the initial proposal of this approach for attacking gradients,   \n319 we observed that it is also well-suited for attacking activations.   \n320 \u2022 K-means accuracy \u2014 vulnerability to clusterization attack, measured in the percentage of correctly   \n321 clustered activations, lower is better.   \n322 For all setups, we report the worst (least private) value among these metrics throughout the entire   \n323 training period as a measure of privacy leakage, because it is the worst possible scenario that matters   \n324 from the client\u2019s perspective. For DC and $\\mathrm{P^{3}\\overline{{E}}F T}$ , we specify the values for the best configuration in   \n325 terms of the utility-privacy trade-off. See details in Appendix A. We also report adjusted standard   \n326 deviations for the two privacy aware algorithms: $\\mathrm{{P^{3}}}$ EFT and DC. To do so, we run the full training   \n327 procedure from scratch with 3 random seeds.   \n328 The results for DeBERTa are presented in Table 1. To improve reproducibility, we reuse the hyperpa  \n329 rameters from original paper, with the exception of the LoRA dropout value. We disable dropout   \n330 because it interferes with the mixing weights (5). In preliminary experiments, we observed that with   \n331 dropout enabled, both our algorithm and DC begin to perform significantly worse.   \n332 We use $n\\,=\\,2$ adapter sets for $\\mathrm{P^{3}E F T}$ for all datasets and adhered to the same approach for the   \n333 other models as well. Overall, $\\mathrm{P^{3}F T}$ achieves nearly the same accuracy as traditional (non-private)   \n334 fine-tuning, outperforming the DC-based algorithm in terms of accuracy given the same privacy level.   \n335 On the MNLI dataset, we could not find the hyperparameters for DC that ensure stable training while   \n336 maintaining privacy. Meanwhile, $\\mathrm{{P^{3}}}$ EFT maintains consistent performance on this task with a slight   \n337 drop in quality.   \n338 Table 2 a reports evaluation for the Flan-T5 base model[4]. For this model, we adapt the exact same   \n339 hyperparameters as in the previous evaluation with DeBERTa-XXLarge. Compared to DeBERTa,   \n340 these results are more closely matched. Both both our algorothm and DC consistently solve all three   \n341 tasks, but $\\mathrm{P^{3}E F T}$ slightly outperforms DC in terms of privacy.   \n342 To evaluate how our algorithm scales to larger models, we also fine-tune Llama-2 7B [35] on   \n343 SST2 [32] and QNLI [39] datasets. For these evaluations, we use LoRA hyperparameters that Hu   \n344 et al. [15] used when fine-tuning GPT-3, with several changes inspired by Dettmers et al. [5]. Namely,   \n345 we use the NF4 weight format, apply LoRA to both attention and MLP layers with rank 16. We   \n346 fine-tune both tasks with maximum context length of 512 and weight decay 0.01. Table 3 summarizes   \n347 our results: for QNLI, $\\mathrm{P^{3}E F T}$ achieves somewhat better privacy-accuracy trade-off. On SST2, $\\mathrm{{P^{3}}}$ EFT   \n348 shows similarly favorable trade-offs while DC struggles to preserve privacy. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "6Wm4202Wvc/tmp/89a98df2fdcfed0557427d7a7c5f9b89ae8bb01e60b76b7c39e16b74e014fb30.jpg", "table_caption": ["Table 1: Accuracy and privacy metrics. DeBERTa XXLarge. ", "Table 2: Accuracy and privacy metrics. Flan-T5-Large. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "6Wm4202Wvc/tmp/87ff32f0442622552633859d4f8197bb355bafd9cb8debdfcd775ec896bd0b7c.jpg", "table_caption": ["Table 3: Accuracy and privacy metrics for LLaMA-2 7B. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "349 5 Conclusion and Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "350 In this work, we analyze privacy-preserving fine-tuning of large neural networks in the context of   \n351 parameter-efficient fine-tuning and the two-party split learning setting. We show that while standard   \n352 fine-tuning suffers from label leakage even in the parameter-efficient case, it is possible to leverage   \n353 the efficiency of PEFT to alter the procedure without any significant performance drawbacks. We test   \n354 the resulting method, named $\\mathrm{P^{3}E F T}$ , on a range of pretrained language models and multiple datasets,   \n355 showing that it is competitive with a strong baseline in terms of label privacy while having higher   \n356 task performance.   \n357 In future work, it is natural to explore how this approach can be extended to establish holistic privacy   \n358 in both labels and inputs. This problem can be approached from two directions: either adapt the   \n359 ideas of $\\mathrm{P^{3}E F T}$ for input privacy, or combine it with an existing work like [22]. Another important   \n360 direction for future research is exploring the privacy of the long-term client-provider interaction. In a   \n361 typical real-world use case of API fine-tuning, a client performs multiple training runs on overlapping   \n362 data and hyperparameters. This could open additional attacks vectors that combine information from   \n363 multiple training runs. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "364 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "365 [1] Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan McMahan,   \n366 Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for   \n367 privacy-preserving machine learning. In proceedings of the 2017 ACM SIGSAC Conference on   \n368 Computer and Communications Security, pages 1175\u20131191, 2017.   \n369 [2] Alexander Borzunov, Dmitry Baranchuk, Tim Dettmers, Max Ryabinin, Younes Belkada, Artem   \n370 Chumachenko, Pavel Samygin, and Colin Raffel. Petals: Collaborative inference and fine-tuning   \n371 of large models. arXiv preprint arXiv:2209.01188, 2022. URL https://arxiv.org/abs/   \n372 2209.01188.   \n373 [3] Tianqi Chen and Carlos Guestrin. XGBoost: A scalable tree boosting system. In Proceedings of   \n374 the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,   \n375 KDD \u201916, pages 785\u2013794, New York, NY, USA, 2016. ACM. ISBN 978-1-4503-4232-2. doi:   \n376 10.1145/2939672.2939785. URL http://doi.acm.org/10.1145/2939672.2939785.   \n377 [4] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li,   \n378 Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu,   \n379 Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav   \n380 Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H.   \n381 Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling   \n382 instruction-finetuned language models, 2022. URL https://arxiv.org/abs/2210.11416.   \n383 [5] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient   \n384 finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023.   \n385 [6] Dreambooth API. Dreambooth API \u2013 Easily finetune Stable Diffusion and generate customised   \n386 AI images \u2014 dreamboothapi.ai. https://dreamboothapi.ai/, 2023. [Accessed 28-09-   \n387 2023].   \n388 [7] Haonan Duan, Adam Dziedzic, Nicolas Papernot, and Franziska Boenisch. Flocks of stochastic   \n389 parrots: Differentially private prompt learning for large language models. arXiv preprint   \n390 arXiv:2305.15594, 2023.   \n391 [8] Cynthia Dwork. Differential privacy. In International colloquium on automata, languages, and   \n392 programming, pages 1\u201312. Springer, 2006.   \n393 [9] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation.   \n394 In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference   \n395 on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 1180\u2013   \n396 1189, Lille, France, 07\u201309 Jul 2015. PMLR. URL https://proceedings.mlr.press/v37/   \n397 ganin15.html.   \n398 [10] Otkrist Gupta and Ramesh Raskar. Distributed learning of deep neural network over multiple   \n399 agents. Journal of Network and Computer Applications, 116:1\u20138, 2018. ISSN 1084-8045.   \n400 doi: https://doi.org/10.1016/j.jnca.2018.05.003. URL https://www.sciencedirect.com/   \n401 science/article/pii/S1084804518301590.   \n402 [11] Karen Hambardzumyan, Hrant Khachatrian, and Jonathan May. WARP: Word-level Ad  \n403 versarial ReProgramming. In Proceedings of the 59th Annual Meeting of the Associa  \n404 tion for Computational Linguistics and the 11th International Joint Conference on Natural   \n405 Language Processing (Volume 1: Long Papers), pages 4921\u20134933, Online, August 2021.   \n406 Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.381. URL   \n407 https://aclanthology.org/2021.acl-long.381.   \n408 [12] Stephen Hardy, Wilko Henecka, Hamish Ivey-Law, Richard Nock, Giorgio Patrini, Guillaume   \n409 Smith, and Brian Thorne. Private federated learning on vertically partitioned data via entity   \n410 resolution and additively homomorphic encryption, 2017.   \n411 [13] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced   \n412 bert with disentangled attention. In International Conference on Learning Representations,   \n413 2021. URL https://openreview.net/forum?id=XPZIaotutsD.   \n414 [14] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,   \n415 Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning   \n416 for NLP. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th   \n417 International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning   \n418 Research, pages 2790\u20132799. PMLR, 09\u201315 Jun 2019. URL https://proceedings.mlr.   \n419 press/v97/houlsby19a.html.   \n420 [15] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,   \n421 Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In   \n422 International Conference on Learning Representations, 2022. URL https://openreview.   \n423 net/forum?id $\\equiv$ nZeVKeeFYf9.   \n424 [16] Hugging Face. AutoTrain \u2014 huggingface.co. https://huggingface.co/autotrain, 2023.   \n425 [Accessed 28-09-2023].   \n426 [17] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint   \n427 arXiv:1412.6980, 2014.   \n428 [18] Oscar Li, Jiankai Sun, Xin Yang, Weihao Gao, Hongyi Zhang, Junyuan Xie, Virginia Smith,   \n429 and Chong Wang. Label leakage and protection in two-party split learning. In International   \n430 Conference on Learning Representations, 2022. URL https://openreview.net/forum?   \n431 id=cOtBRgsf2fO.   \n432 [19] Qinbin Li, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, Yuan Li, Xu Liu, and Bingsheng He.   \n433 A survey on federated learning systems: Vision, hype and reality for data privacy and protection.   \n434 IEEE Transactions on Knowledge and Data Engineering, 2021.   \n435 [20] Shen Li, Pritam Damania, Luca Wehrstedt, and Rohan Varma. PyTorch RPC: Distributed Deep   \n436 Learning Built on Tensor-Optimized Remote Procedure Calls. In Proceedings of Machine   \n437 Learning and Systems 5 (MLSys), 2023.   \n438 [21] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In   \n439 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the   \n440 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),   \n441 pages 4582\u20134597, Online, August 2021. Association for Computational Linguistics. doi:   \n442 10.18653/v1/2021.acl-long.353. URL https://aclanthology.org/2021.acl-long.353.   \n443 [22] Yansong Li, Zhixing Tan, and Yang Liu. Privacy-preserving prompt tuning for large language   \n444 model services. ArXiv, abs/2305.06212, 2023. URL https://api.semanticscholar.org/   \n445 CorpusID:258588141.   \n446 [23] Xiao-Yang Liu, Rongyi Zhu, Daochen Zha, Jiechao Gao, Shan Zhong, and Meikang Qiu.   \n447 Differentially private low-rank adaptation of large language model using federated learning.   \n448 arXiv preprint arXiv:2312.17493, 2023.   \n449 [24] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Ar  \n450 cas. Communication-Efficient Learning of Deep Networks from Decentralized Data. In Aarti   \n451 Singh and Jerry Zhu, editors, Proceedings of the 20th International Conference on Artifi  \n452 cial Intelligence and Statistics, volume 54 of Proceedings of Machine Learning Research,   \n453 pages 1273\u20131282. PMLR, 20\u201322 Apr 2017. URL https://proceedings.mlr.press/v54/   \n454 mcmahan17a.html.   \n455 [25] Nvidia. Nvidia confidential computing. https://www.nvidia.com/en-us/data-center/   \n456 solutions/confidential-computing, 2023. [Accessed 28-09-2023].   \n457 [26] OctoAI. Fine-tuning Stable Diffusion \u2014 docs.octoai.cloud. https://docs.octoai.cloud/   \n458 docs/fine-tuning-stable-diffusion, 2023. [Accessed 28-09-2023].   \n459 [27] OpenAI. OpenAI Platform \u2014 platform.openai.com. https://platform.openai.com/   \n460 docs/guides/fine-tuning, 2023. [Accessed 28-09-2023].   \n461 [28] Dario Pasquini, Giuseppe Ateniese, and Massimo Bernaschi. Unleashing the tiger: Inference   \n462 attacks on split learning. In Proceedings of the 2021 ACM SIGSAC Conference on Computer and   \n463 Communications Security, CCS \u201921, page 2113\u20132129, New York, NY, USA, 2021. Association   \n464 for Computing Machinery. ISBN 9781450384544. doi: 10.1145/3460120.3485259. URL   \n465 https://doi.org/10.1145/3460120.3485259.   \n466 [29] Jonas Pfeiffer, Aishwarya Kamath, Andreas R\u00fcckl\u00e9, Kyunghyun Cho, and Iryna Gurevych.   \n467 Adapterfusion: Non-destructive task composition for transfer learning, 2021.   \n468 [30] Yuma Rao, Jacob Steeves, Ala Shaabana, Daniel Attevelt, and Matthew McAteer. Bittensor: A   \n469 peer-to-peer intelligence market, 2021.   \n470 [31] Weiyan Shi, Ryan Shea, Si Chen, Chiyuan Zhang, Ruoxi Jia, and Zhou Yu. Just fine-tune twice:   \n471 Selective differential privacy for large language models. arXiv preprint arXiv:2204.07667,   \n472 2022.   \n473 [32] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew   \n474 Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a   \n475 sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural   \n476 Language Processing, pages 1631\u20131642, Seattle, Washington, USA, October 2013. Association   \n477 for Computational Linguistics. URL https://www.aclweb.org/anthology/D13-1170.   \n478 [33] Jiankai Sun, Xin Yang, Yuanshun Yao, and Chong Wang. Label leakage and protection from   \n479 forward embedding in vertical federated learning. arXiv preprint arXiv:2203.01451, 2022.   \n480 [34] Chuanqi Tan, Fuchun Sun, Tao Kong, Wenchang Zhang, Chao Yang, and Chunfang Liu. A   \n481 survey on deep transfer learning. In V\u02c7era K\u02daurkov\u00e1, Yannis Manolopoulos, Barbara Hammer,   \n482 Lazaros Iliadis, and Ilias Maglogiannis, editors, Artificial Neural Networks and Machine   \n483 Learning \u2013 ICANN 2018, pages 270\u2013279, Cham, 2018. Springer International Publishing. ISBN   \n484 978-3-030-01424-7.   \n485 [35] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,   \n486 Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open   \nfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n488 [36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N   \n489 Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon,   \n490 U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, ed  \n491 itors, Advances in Neural Information Processing Systems, volume 30. Curran Associates,   \n492 Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/   \n493 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.   \n494 [37] Praneeth Vepakomma, Otkrist Gupta, Tristan Swedish, and Ramesh Raskar. Split learning for   \n495 health: Distributed deep learning without sharing raw patient data, 2018.   \n496 [38] Praneeth Vepakomma, Otkrist Gupta, Abhimanyu Dubey, and Ramesh Raskar. Reducing   \n497 leakage in distributed deep learning for sensitive health data. 05 2019.   \n498 [39] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.   \n499 Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv   \n500 preprint arXiv:1804.07461, 2018.   \n501 [40] Yiming Wang, Yu Lin, Xiaodong Zeng, and Guannan Zhang. Privatelora for efficient privacy   \n502 preserving llm. arXiv preprint arXiv:2311.14030, 2023.   \n503 [41] Adina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge corpus   \n504 for sentence understanding through inference. arXiv preprint arXiv:1704.05426, 2017.   \n505 [42] Guangxuan Xiao, Ji Lin, and Song Han. Offsite-tuning: Transfer learning without full model.   \n506 arXiv preprint arXiv:2302.04870, 2023.   \n507 [43] Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated machine learning: Concept   \n508 and applications. ACM Trans. Intell. Syst. Technol., 10(2), jan 2019. ISSN 2157-6904. doi:   \n509 10.1145/3298981. URL https://doi.org/10.1145/3298981.   \n510 [44] Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A Inan, Gautam Kamath,   \n511 Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz, Sergey Yekhanin, and   \n512 Huishuai Zhang. Differentially private fine-tuning of language models. In International   \n513 Conference on Learning Representations, 2022. URL https://openreview.net/forum?   \n514 id=Q42f0dfjECO.   \n515 [45] Zhuo Zhang, Yuanhang Yang, Yong Dai, Qifan Wang, Yue Yu, Lizhen Qu, and Zenglin   \n516 Xu. FedPETuning: When federated learning meets the parameter-efficient tuning methods of   \n517 pre-trained language models. In Findings of the Association for Computational Linguistics:   \n518 ACL 2023, pages 9963\u20139977, Toronto, Canada, July 2023. Association for Computational   \n519 Linguistics. doi: 10.18653/v1/2023.findings-acl.632. URL https://aclanthology.org/   \n520 2023.findings-acl.632.   \n[46] Haodong Zhao, Wei Du, Fangqi Li, Peixuan Li, and Gongshen Liu. Fedprompt: Communication  \n522 efficient and privacy preserving prompt tuning in federated learning, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "523 A Hyperparameters search ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "524 In $\\mathrm{{P^{3}}}$ EFT and Distance Correlation methods resulting loss $L$ function can be viewed in the form ", "page_idx": 12}, {"type": "equation", "text": "$$\nL=L_{m}+\\alpha\\cdot L_{r},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "525 where $L_{m}$ - main task loss, $L_{r}$ - regularizer and $\\alpha$ is a coefficient that controls the tradeoff between   \n526 these two losses. The selection of this coefficient affects the final performance of the model. Therefore,   \n527 to find the best configurations for both methods, we iterated through this hyperparameter using a grid   \n528 search.   \n529 We started with $\\alpha=1$ and then altered it with a multiplicative step of $10^{\\frac{1}{2}}$ . Values were discarded if   \n530 the quality did not exceed that achieved by solely training the classifier without LoRA. This criterion   \n531 was adopted because such outcomes would suggest the method\u2019s inability to outperform training   \n532 scenarios in which the server does not engage with the labels whatsoever. Additionally, we excluded   \n533 values that led to unstable training. By this, we mean instances where, although the model initially   \n534 trained on the primary task, at some point, the regularizer began contributing significantly more,   \n535 and the utility value dropped to the starting value. We observed this issue for the DC method with   \n536 DeBERTa on the MNLI. From the remaining values, we aimed to choose the one that offered the   \n537 lowest privacy leakage. The final hyperparameter values for $\\mathrm{{P^{3}}}$ EFT can be found in the Table 4 and   \n538 for DC in the Table 5. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Table 4: Regularization parameter $\\alpha$ for the $\\mathrm{P^{3}E F T}$ method. The values in the table represent powers of the $10^{\\frac{1}{2}}$ . ", "page_idx": 12}, {"type": "table", "img_path": "6Wm4202Wvc/tmp/22d26cd1fc66d35db1c0fc1f45e8cd681bb56e0f232fecf82fdb54365b5f0cd8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "Table 5: Regularization parameter $\\alpha$ for the DC method. The values in the table represent powers of the $10^{\\frac{1}{2}}$ . ", "page_idx": 12}, {"type": "table", "img_path": "6Wm4202Wvc/tmp/09edc4b7edb7ed8f751b58aceb7e6fec5bd1e7c9bb4327e003f801e8b8f983e9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "539 B Formal algorithm definition ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "540 Below, we define the full $\\mathrm{P^{3}E F T}$ algorithm. In Algorithm 2, main_loss is the task-specific objective   \n541 e.g. cross-entropy; reg_loss is the adversarial regularizer described in Section 3.4. We denote   \n542 client-side model \"head\" as $f(h,\\psi^{t})$ , where $\\psi$ represent trainable head parameters. Finally, opt_step   \n543 function performs a single gradient descent step with a task-specific optimizer, typically Adam [17].   \nAlgorithm 2 P3EFT - full training algorithm   \n1: Input: dataset $D=\\{X,Y\\}$ , $n>1$ number of adapters, $\\alpha\\geq0$ - regularizing weight, $m>1$   \nnumber of obfuscated gradients   \n2: Initialize head $\\psi^{0}$ , mixing weights $W_{i}$ and adapters $\\theta_{i}^{0},i=\\overline{{1,n}}$   \n3: for $t=0,1,\\ldots,T-1$ do   \n4: Sample batch $\\{x^{t},y^{t}\\}$   \n5: for $i=1,\\hdots,n$ do   \n6: $\\begin{array}{l}{{h_{i}^{t}=h(x^{t},\\theta_{i}^{t})}}\\\\ {{l_{i}=\\mathrm{reg}\\mathrm{\\mathrm{{\\mathrm{Joss}}}}(h_{i}^{t},y^{t})}}\\end{array}$ \u25b7by server   \n7: $\\triangleright$ by client   \n8: end for   \n9: $\\begin{array}{r l}&{h^{\\prime}=\\sum_{i=1}^{n}W_{i}\\odot h_{i}^{t}}\\\\ &{l=\\mathrm{main\\_loss}(f(h^{\\prime},\\psi^{t}),y^{t})}\\\\ &{L=l+\\alpha\\cdot\\sum_{i=1}^{n}l_{i}}\\\\ &{\\mathbf{for}\\,i=1,\\dots,n\\,\\mathbf{do}}\\\\ &{\\quad g_{h}=\\partial L/\\partial h_{i}^{t}}\\\\ &{\\quad g_{i}^{t}=\\mathrm{private\\_backprop}(x,\\theta_{i}^{t},g_{h},m)}\\\\ &{\\theta_{i}^{t+1}=\\mathrm{opt\\_step}(\\theta_{i}^{t},g_{i}^{t},t)}\\end{array}$   \n10:   \n11:   \n12:   \n13: \u25b7Client performs partial backprop   \n14:   \n15:   \n16: end for   \n17: $\\psi^{t+1}=\\mathrm{opt\\_step}(\\psi^{t},\\partial l/\\partial\\psi^{t},t)$   \n18: end for   \n19: Return: $\\psi^{T},\\theta_{1}^{T},\\dots,\\theta_{M}^{T}$ ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "544 C Informal description of LoRA fine-tuning ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "545 For convenience, we provide a brief summary of fine-tuning with LoRA [15]. This PEFT method   \n546 was originally designed for fine-tuning large pre-trained language models on downstream NLP tasks.   \n547 These language models are typically based on the Transformer architecture [36], where most trainable   \n548 parameters are allocated to linear layers in multi-head self-attention and feedforward blocks.   \n549 In the first stage of LoRA fine-tuning, user augments the model with adapters. To do so, a user goes   \n550 over linear layers in transformer blocks and adds two trainable matrices, $A$ and $B$ that affect this   \n551 layer\u2019s forward pass. Let $W_{i}\\times x+b_{i}$ be the original layer with $n$ inputs and $m$ hidden units. Here,   \n552 $W_{i}\\in\\mathcal{R}^{m\\times n}$ is a pre-trained weight matrix, $b_{i}\\in\\mathcal{R}^{m}$ is a pre-trained intercept vector and $x\\in\\mathcal{R}^{n}$   \n553 represents a vector of inputs to this particular layer. During the forward pass, a layer with LoRA   \n554 adapter computes $W_{i}\\times x+b_{i}+B_{i}\\times A_{i}\\times x$ , or equivalently, $(W_{i}+B\\times A)\\times x+b_{i}$ . Here, $A_{i}$   \n555 and $B_{i}$ are two newly added matrices that constitute a LoRA adapter.   \n556 The adapter matrices $A\\in\\mathcal{R}^{r\\times n}$ and $B\\in\\mathcal{R}^{m\\times r}$ have a very small intermediate dimension $r$ . For   \n557 instance, when training GPT-3 with LoRA adapters, authors use $1\\leq r\\leq64$ , whereas the main   \n558 weight dimensions are $m=n=12288$ . The first matrix $A$ is initialized with small random normal   \n559 values, and the second matrix $B$ is initialized at zeros. That way, initial $A$ and $B$ do not affect the   \n560 model predictions.   \n561 Once all adapters are initilized, the user trains all $A_{i}$ and $B_{i}$ matrices of the model, while keeping   \n562 the rest of the weights frozen. This way, only a small faction (less than $1\\%$ ) of model weights are   \n563 updated. Once the training is over, the learned adapters $A_{i}$ and $B_{i}$ can be merged into the main   \n564 weights $(W_{i}:=W_{i}+A_{i}\\times B_{i})$ or used separately.   \n565 LoRA adapters are designed with two objectives in mind: i) to allow fine-tuning models in limited   \n566 GPU memory and ii) to allow inferencing many fine-tuned models using one inference server. When   \n567 fine-tuning, LoRA achieves small memory footprint due to the fact that user does not need to compute   \n568 gradients (or optimizer statistics) for billions of main model parameters. During inference, a server   \n569 can keep a library of several adapters for different tasks and swap between them on demand. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "570 D Informal description of LoRA fine-tuning ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "571 We used NVIDIA A100 GPUs for all the experiments. Experiments with DeBERTA [13] and Flan-T5   \n572 [4] on SST2 [32] were conducted on the single GPU, while experiments on MNLI [41] and QNLI   \n573 require 4 A100. LLaMA-2 [35] expetiments were carried out on the node of 8 A100.   \n574 All the experiments last 12-24 hours. However, it is possible to speed up some of them using more   \n575 GPUs, as well as conduct them on a smaller number of GPUs using technics to save GPU memory   \n576 (see parameters in our code). ", "page_idx": 14}, {"type": "text", "text": "577 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "578 1. Claims   \n579 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n580 paper\u2019s contributions and scope?   \n4 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n85 made in the paper.   \n86 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n87 contributions made in the paper and important assumptions and limitations. A No or   \n88 NA answer to this question will not be perceived well by the reviewers.   \n89 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n90 much the results can be expected to generalize to other settings.   \n91 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n2 are not attained by the paper. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "593 2. Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: We discuss potential limitations of the method in Section 1, Section 3 and Section 5. ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 15}, {"type": "text", "text": "25 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "626 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n627 a complete (and correct) proof? ", "page_idx": 15}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 15}, {"type": "text", "text": "629 Justification: We include no proofs.   \n630 Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 16}, {"type": "text", "text": "641 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "42 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n43 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n4 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We describe all the technical details in the Section 4 and in the README of our attached code. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 16}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 16}, {"type": "text", "text": "684 Answer: [Yes]   \n685 Justification: We provide attached code.   \n686 Guidelines:   \n687 \u2022 The answer NA means that paper does not include experiments requiring code.   \n688 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n689 public/guides/CodeSubmissionPolicy) for more details.   \n690 \u2022 While we encourage the release of code and data, we understand that this might not be   \n691 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n692 including code, unless this is central to the contribution (e.g., for a new open-source   \n693 benchmark).   \n694 \u2022 The instructions should contain the exact command and environment needed to run to   \n695 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n696 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n697 \u2022 The authors should provide instructions on data access and preparation, including how   \n698 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n699 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n700 proposed method and baselines. If only a subset of experiments are reproducible, they   \n701 should state which ones are omitted from the script and why.   \n702 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n703 versions (if applicable).   \n704 \u2022 Providing as much information as possible in supplemental material (appended to the   \n705 paper) is recommended, but including URLs to data and code is permitted.   \n706 6. Experimental Setting/Details   \n707 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n708 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n709 results?   \n710 Answer: [Yes]   \n711 Justification: See Section 4, Appendix A.   \n712 Guidelines:   \n713 \u2022 The answer NA means that the paper does not include experiments.   \n714 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n715 that is necessary to appreciate the results and make sense of them.   \n716 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n717 material.   \n718 7. Experiment Statistical Significance   \n719 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n720 information about the statistical significance of the experiments?   \n721 Answer: [Yes]   \n722 Justification: We report error bars in main results.   \n723 Guidelines:   \n724 \u2022 The answer NA means that the paper does not include experiments.   \n725 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n726 dence intervals, or statistical significance tests, at least for the experiments that support   \n727 the main claims of the paper.   \n728 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n729 example, train/test split, initialization, random drawing of some parameter, or overall   \n730 run with given experimental conditions).   \n731 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n732 call to a library function, bootstrap, etc.)   \n733 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n734 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n735 of the mean.   \n36 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n37 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n38 of Normality of errors is not verified.   \n39 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n40 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n41 error rates).   \n42 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n43 they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "744 8. Experiments Compute Resources ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "745 Question: For each experiment, does the paper provide sufficient information on the com  \n746 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n747 the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "749 Justification: See Appendix D.   \n750 Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 18}, {"type": "text", "text": "59 9. Code Of Ethics ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 18}, {"type": "text", "text": "763 Justification: We confirm the NeurIPS Code of Ethics   \n764 Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 18}, {"type": "text", "text": "770 10. Broader Impacts ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "771 Question: Does the paper discuss both potential positive societal impacts and negative   \n772 societal impacts of the work performed? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "773   \n774 Justification: We describe a potential social impact in Introduction.   \n775 Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 18}, {"type": "text", "text": "787 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n788 that a generic algorithm for optimizing neural networks could enable people to train   \n789 models that generate Deepfakes faster.   \n790 \u2022 The authors should consider possible harms that could arise when the technology is   \n791 being used as intended and functioning correctly, harms that could arise when the   \n792 technology is being used as intended but gives incorrect results, and harms following   \n793 from (intentional or unintentional) misuse of the technology.   \n794 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n795 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n796 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n797 feedback over time, improving the efficiency and accessibility of ML).   \n798 11. Safeguards   \n799 Question: Does the paper describe safeguards that have been put in place for responsible   \n800 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n801 image generators, or scraped datasets)?   \n802 Answer: [No]   \n803 Justification: We do not describe the safeguards   \n804 Guidelines:   \n805 \u2022 The answer NA means that the paper poses no such risks.   \n806 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n807 necessary safeguards to allow for controlled use of the model, for example by requiring   \n808 that users adhere to usage guidelines or restrictions to access the model or implementing   \n809 safety filters.   \n810 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n811 should describe how they avoided releasing unsafe images.   \n812 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n813 not require this, but we encourage authors to take this into account and make a best   \n814 faith effort.   \n815 12. Licenses for existing assets   \n816 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n817 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n818 properly respected?   \n819 Answer: [Yes]   \n820 Justification: We use open datasets from GLUE benchmark and open-sourced models and   \n821 cite them in our work.   \n822 Guidelines:   \n823 \u2022 The answer NA means that the paper does not use existing assets.   \n824 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n825 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n826 URL.   \n827 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n828 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n829 service of that source should be provided.   \n830 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n831 package should be provided. For popular datasets, paperswithcode.com/datasets   \n832 has curated licenses for some datasets. Their licensing guide can help determine the   \n833 license of a dataset.   \n834 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n835 the derived asset (if it has changed) should be provided.   \n836 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n837 the asset\u2019s creators. ", "page_idx": 19}, {"type": "text", "text": "838 13. New Assets ", "page_idx": 19}, {"type": "text", "text": "39 Question: Are new assets introduced in the paper well documented and is the documentation   \n40 provided alongside the assets?   \n41 Answer: [NA]   \n42 Justification: We do not release any of the assets   \n43 Guidelines:   \n44 \u2022 The answer NA means that the paper does not release new assets.   \n45 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n46 submissions via structured templates. This includes details about training, license,   \n47 limitations, etc.   \n48 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n49 asset is used.   \n50 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n51 create an anonymized URL or include an anonymized zip file.   \n52 14. Crowdsourcing and Research with Human Subjects   \n53 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n54 include the full text of instructions given to participants and screenshots, if applicable, as   \n55 well as details about compensation (if any)?   \n56 Answer: [NA]   \n57 Justification: We do not involve crowdsourcing.   \n58 Guidelines:   \n59 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n60 human subjects.   \n61 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n62 tion of the paper involves human subjects, then as much detail as possible should be   \n63 included in the main paper.   \n64 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n65 or other labor should be paid at least the minimum wage in the country of the data   \n66 collector.   \n67 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n68 Subjects ", "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 20}, {"type": "text", "text": "Justification: Our paper does not involve research with human subjects Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 20}]