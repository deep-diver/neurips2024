[{"figure_path": "7EQx56YSB2/tables/tables_8_1.jpg", "caption": "Table 1: Results on GLUE benchmark with RoBERTa-base model. We report Pearson correlation for STS-B, Matthew's correlation for CoLA and accuracy for other tasks. # Params denotes number of trainable parameters", "description": "This table presents the results of the experiment conducted on the GLUE benchmark using the RoBERTa-base model.  The experiment evaluated several different parameter-efficient fine-tuning methods, including the proposed GSOFT method.  The table shows the performance of each method on various GLUE tasks, reported as Pearson correlation (STS-B), Matthews correlation (CoLA), and accuracy (other tasks). The '# Params' column indicates the number of trainable parameters for each method.", "section": "7.1 Natural language understanding"}, {"figure_path": "7EQx56YSB2/tables/tables_8_2.jpg", "caption": "Table 2: Results on subject-driven generation. # Params denotes the number of training parameters in each parametrization. Training time is computed for 3000 iterations on a single GPU V100 in hours.", "description": "This table presents the results of subject-driven generation experiments using different parameter-efficient fine-tuning methods.  It shows the number of trainable parameters (# Params), training time in hours on a single V100 GPU, CLIP image similarity (CLIP-I\u2191), and CLIP text similarity (CLIP-T\u2191) for various models: Full model (all parameters trained), LoRA, BOFT, GSOFT, and Double GSOFT.  The table helps in comparing the efficiency and effectiveness of different methods for this task.", "section": "7.2 Subject-driven generation"}, {"figure_path": "7EQx56YSB2/tables/tables_9_1.jpg", "caption": "Table 3: Results of training LipConvnet-15 architecture on CIFAR-100. (a, b) in \u201cGroups\u201d column denotes number of groups in two grouped exponential convolutions (with kernel sizes 3 and 1). (a, -) corresponds to only one GS orthogonal convolutional layer. Before each grouped layer with k groups use a ChShuffle operator.", "description": "This table presents the results of training a LipConvnet-15 architecture on the CIFAR-100 dataset.  It compares the performance of standard skew orthogonal convolutions (SOC) with the proposed GS orthogonal convolutions (GS-SOC). The table shows the number of parameters, the number of groups used in the grouped convolutional layers, speedup factor compared to SOC, the activation function used, the accuracy, and robust accuracy achieved by each model. Different configurations of GS-SOC are evaluated by varying the number of groups in the two grouped convolutional layers and using different activation functions and permutation types within the ChShuffle operator.", "section": "7.3 GS Orthogonal Convolutions"}, {"figure_path": "7EQx56YSB2/tables/tables_20_1.jpg", "caption": "Table 3: Results of training LipConvnet-15 architecture on CIFAR-100. (a, b) in \u201cGroups\u201d column denotes number of groups in two grouped exponential convolutions (with kernel sizes 3 and 1). (a, -) corresponds to only one GS orthogonal convolutional layer. Before each grouped layer with k groups use a ChShuffle operator.", "description": "This table presents the results of training a LipConvnet-15 architecture on the CIFAR-100 dataset. It compares different configurations of the GS-orthogonal convolution layer, varying the number of groups, the speedup achieved, the activation function used, the permutation method applied, and the resulting accuracy and robust accuracy.  The table helps to analyze the impact of various design choices on model performance and efficiency.", "section": "7.3 GS Orthogonal Convolutions"}]