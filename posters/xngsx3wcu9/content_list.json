[{"type": "text", "text": "Visual Data Diagnosis and Debiasing with Concept Graphs ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Rwiddhi Chakraborty1,2\u21e4 Yinong (Oliver) Wang1 Jialu Gao1 Runkai Zheng1 Cheng Zhang1,3 Fernando De la Torre1 ", "page_idx": 0}, {"type": "text", "text": "1Carnegie Mellon University 2UiT The Arctic University of Norway 3Texas A&M University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The widespread success of deep learning models today is owed to the curation of extensive datasets significant in size and complexity. However, such models frequently pick up inherent biases in the data during the training process, leading to unreliable predictions. Diagnosing and debiasing datasets is thus a necessity to ensure reliable model performance. In this paper, we present CONBIAS, a novel framework for diagnosing and mitigating Concept co-occurrence Biases in visual datasets. CONBIAS represents visual datasets as knowledge graphs of concepts, enabling meticulous analysis of spurious concept co-occurrences to uncover concept imbalances across the whole dataset. Moreover, we show that by employing a novel clique-based concept balancing strategy, we can mitigate these imbalances, leading to enhanced performance on downstream tasks. Extensive experiments show that data augmentation based on a balanced concept distribution augmented by CONBIAS improves generalization performance across multiple datasets compared to state-of-the-art methods.2 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Over the last decade we have witnessed an unparalleled growth in the capabilities of deep learning models across a wide range of tasks, such as image classification [17, 47, 7], object detection [41, 55], semantic segmentation [20, 26, 43], and so on. More recently, with the introduction of large multimodal models, these capabilities have improved further [25, 15]. However, such models, while demonstrating impressive performance on a wide range of tasks, have been shown to be biased in their predictions [30, 13]. These biases come in various forms, based in texture [14], shape [39, 32], object co-occurrence [51, 52, 48], and so on. In addition to exploring model biases, dataset diagnosis, or evaluating biases directly within the dataset, is particularly crucial as large datasets available today are beyond the scope of human evaluation, owing to their size and complexity. For example, ImageNet [6], a widely used dataset in deep learning literature, is known to have thousands of erroneous labels and a lack of diversity in its class hierarchy [33, 58]. Other popular datasets such as MS-COCO [23] and CelebA [27], have problematic social biases with respect to gendered captions and prejudicial attributes of people from different races. As a result, frameworks that effectively diagnose and debias these datasets are sought. ", "page_idx": 0}, {"type": "text", "text": "While multiple works exist in the categorization and exploration of biases in visual data [9, 30], an end-to-end pipeline incorporating both diagnosis and debiasing has received relatively scant attention. ALIA [8] is the closest and most recent work exploring such a data-augmentation-based approach to debiasing, but it has two shortcomings - first, it does not diagnose the dataset which it aims to debias. Without such a diagnosis, it is challenging to identify the biases to be mitigated in the first place. ", "page_idx": 0}, {"type": "image", "img_path": "XNGsx3WCU9/tmp/98278294bd169160d75a8eb795226b186fb9a1d1f9f176d50a128485cc8a3395.jpg", "img_caption": ["Figure 1: The conventional data diagnosis and augmentation pipeline begins with an original (biased) dataset. Existing methods address these biases via object frequency calibration [52], metadata analysis [8], or traditional augmentation techniques [60, 5]. In contrast, our framework models visual data as a knowledge graph of concepts, with orange nodes representing classes and blue nodes representing concepts, facilitating a systematic diagnosis of class-concept imbalances for debiasing object co-occurrences in vision datasets. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Second, the method relies on a large language model (ChatGPT-4 [4]) to generate diverse, unbiased, in-domain descriptions. This approach is potentially confounding since there is no reliable way to ensure that the biases of the large language model itself do not affect the quality of such domain descriptions. In this work, we address both these shortcomings. ", "page_idx": 1}, {"type": "text", "text": "We present CONBIAS, our framework for diagnosis and debiasing of visual data. Our key contribution is in representing a visual dataset as a knowledge graph of concepts. Analyzing this graph for imbalanced class-concept combinations leads to a principled diagnosis of biases present in the dataset. Once identified, we generate images to address under-represented class-concept combinations, promoting a more uniform concept distribution across classes. By using a concept graph, we circumvent the reliance on a large language model to generate debiased data. Figure 1 illustrates the core idea of our approach in contrast with existing methods. We target object co-occurrence bias, a human-interpretable issue known to confound downstream tasks [34, 10]. Object co-occurrence bias refers to any spurious correlation between a label and an object causally unrelated to the label. Representing the dataset as a knowledge graph of object co-occurrences provides a structured and controllable method to diagnose and mitigate these spurious correlations. ", "page_idx": 1}, {"type": "text", "text": "Our framework proceeds in three steps: (1) Concept Graph Construction: We construct a knowledge graph of concepts from the dataset. These concepts are assumed to come from dataset ground truth such as captions or segmentation masks. (2) Concept Diagnosis: This stage then analyzes the knowledge graph for concept imbalances, revealing potential biases in the original dataset. (3) Concept Debiasing: We sample imbalanced concept combinations from the knowledge graph using graph cliques, each representing a class-concept combination identified as imbalanced. Finally, we generate images containing under-represented concept combinations to supplement the dataset. The image generation protocol is generic and uses an off-the-shelf inpainting process with a textto-image generative model. This principled approach ensures that the concept distribution in our augmented data is uniform and less biased. Our experiments validate this approach, showing that data augmentation based on a balanced concept distribution improves generalization performance across multiple datasets compared to existing baselines. In summary, our contributions include: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose a new concept graph-based framework to diagnose biases in visual datasets, which represents a principled approach to diagnosing datasets for biases, and to mitigating them.   \n\u2022 Based on our graph construction and diagnosis, we propose a novel clique-based concept balancing strategy to address detected biases.   \n\u2022 We demonstrate that balanced concept generation in data augmentation enhances classifier generalization across multiple datasets, over baselines. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Bias discovery in deep learning models. The identification of biases in trained deep learning models has a rich history, with early works exploring the texture and shape-bias tradeoff in ImageNetpretrained ResNets [14, 21, 32, 39]. More recently, the field of worst group robustness has emerged, aiming to generalize classifier performance across multiple groups in the data that correspond to known spurious correlations [49, 45, 24, 42]. Debiasing and concept discovery in the feature space of the learned classifier is also common [1, 54, 59]. Testing model performance sensitivity to the presence of particular attributes has also been explored [53, 36]. With the recent rise in popularity of large language models, efforts have been made to identify learned biases using off-the-shelf captioning models [56], adaptive testing [11], and language guidance [19, 37]. Traditional data augmentation approaches such as CutMix [60], and RandAug [5], are used as baselines as well. Our work intervenes on the dataset directly, instead of operating in the model feature space or testing model sensitivity. This allows for a more intuitive and principled approach to bias discovery. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Data diagnosis. Our work is placed in the context of data diagnosis, i.e. identifying biases directly from the data without using the model as a proxy. One of the early influential works expounding the importance of datasets in deep learning research was a systematic review of the popular datasets in computer vision [51]. A modern appraisal categorizing more diverse types of biases in visual datasets exists in [9]. Additionally, works investigating possible issues with dataset labels have also received interest [33, 58]. Data diagnosis tools such as REVISE [52] compute object statistics (including co-occurrence) to generate high-level insights of the data. However, REVISE is not an end-to-end framework that at once diagnoses and debiases data. It is rather an exploratory tool for an overview of common concepts in the dataset. A more recent method, ALIA, uses a language model to populate diverse descriptions of the given dataset, consequently generating images from such descriptions. A more critical look on dataset bias lies in the field of fairness, particularly with regards to societal bias [12, 16]. Finally, benchmark datasets for data diagnosis have also been proposed [29, 28]. ", "page_idx": 2}, {"type": "text", "text": "Object co-occurrence bias in visual recognition. Objects are biased in the company they keep. This adage is well known in the computer vision literature, as outlined in [34, 10]. Modern efforts to mitigate object co-occurrence bias involve feature decorrelation [48], object aware contrastive representations [31], causal interventions [38], and fusing object and contextual information via attention [2]. The common theme in tackling contextual and co-occurrence bias lies entirely in using better models (feature representations) rather than intervening in the dataset directly. We place our debiasing method along the data augmentation direction, allowing for better controllability and interpretability of the debiasing stage, rather than relying on semantic features learned by a classifier, which may be difficult for humans to interpret. ", "page_idx": 2}, {"type": "text", "text": "3 Approach ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Figure 2 illustrates the overall pipeline of our method. In this section, we begin with the problem statement in Section 3.1, and move to the three major stages in our method definition. Section 3.2 describes the procedure of concept graph construction. Section 3.3 illustrates the details of concept diagnosis. Finally, Section 3.4 presents our method for concept debiasing. ", "page_idx": 2}, {"type": "text", "text": "3.1 Problem Statement ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We are given a dataset $D=\\{(x_{i},y_{i})\\}_{i=1}^{N}$ , a set of images and their corresponding labels. We also assume access to a concept set $C=\\{c_{1},c_{2},\\ldots,c_{k}\\}$ that describes unique objects present in the data. An example concept set looks like the following: {alley, crosswalk, downtown, ..., gas station}, i.e. a list of unique objects present in each image in addition to the class label. Finally, we are given a classifier $f_{\\theta}(X)$ parameterized by network parameters $\\theta$ . The central hypothesis of this work is that the class labels exhibit co-occurring bias with the concept set $C$ , affecting downstream task performance. In this light, we wish to generate an augmented dataset $D_{\\mathrm{aug}}$ that is debiased with respect to the concepts and their corresponding class labels. Thus, given the new dataset $D^{\\prime}=D\\,\\bar{\\cup}\\,D_{\\mathrm{aug}}$ , we wish to retrain $f_{\\theta}(X)$ in the standard classification setup: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{f}^{*}=\\arg\\operatorname*{min}_{f}\\mathbb{E}_{(x,y)\\in D^{\\prime}}[\\mathcal{L}(y,f_{\\theta}(x))],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where ${\\mathcal{L}}(y,f_{\\theta}(x))$ is the cross entropy loss between the class label and classifier prediction. Our framework consists of three stages: Concept Graph Construction, Concept Diagnosis, and Concept Debiasing. Next, we provide details on each step. ", "page_idx": 2}, {"type": "image", "img_path": "XNGsx3WCU9/tmp/8b9fffc0812cec06f23eb3ee56128b9c891ce344767e0fb3d87445d620038bb8.jpg", "img_caption": ["Figure 2: Overview of our framework CONBIAS. (a) Given a dataset and its concept metadata which contains the objects present in each image, (b) we build the concept graph using object co-occurrences. The line thickness indicates the co-occurrence frequencies of particular concepts with their respective classes. (c) Next, the clique-based sampling strategy generates under-represented class-concept combinations, which yield (d) the dataset diagnosis result. (e) Finally, with biases discovered, we generate images of classes containing under-represented concept combinations in the dataset with a standard text-to-image generative model. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.2 Concept Graph Construction ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We construct a concept graph $G=(V,E,W)$ from the data, where $|V|$ is the node set of the graph, $|E|$ is the edge set, and $|W|$ is the set of weights for each edge in the graph. We first construct the node set $V$ as a union of the label set $Y$ and concept set $C$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nV=Y\\cup C.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Next, we construct the edge set $E$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nE=\\{(i,j)\\mid\\exists{\\bmod{\\alpha}}D_{k}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Finally, we construct the weight set $W$ by computing the weights $w_{i j}$ for each edge $(i,j)$ in $G$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nw_{i j}=\\sum_{n=1}^{N}\\mathbb{I}(i\\in D_{n}\\;{\\mathrm{and}}\\;j\\in D_{n}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbb{I}$ is the indicator function that returns 1 if both $i$ and $j$ are present in the $n$ -th image in $D$ , and 0 otherwise, and $N$ is the total number of images in the dataset. ", "page_idx": 3}, {"type": "text", "text": "The concept graph $G$ encapsulates co-occurrence counts between nodes, thus providing an alternative representation of the (originally visual) data. As we show in the next section, this representation helps uncover novel imbalances (bias) contained in the dataset. ", "page_idx": 3}, {"type": "text", "text": "3.3 Concept Graph Diagnosis ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the previous section, we define how to build the concept graph. Here, we present how to leverage the concept graph for discovering co-occurrence biases. We present a principled approach to discovering concept-combinations across classes that co-occur in an imbalanced fashion. ", "page_idx": 3}, {"type": "text", "text": "Definition (Class Clique Sets) For each class $Y_{i}\\in Y$ , we construct a set of $k$ -cliques using the concept graph $G$ . The set of all possible $k$ -cliques for class $Y_{i}$ is denoted as ${\\boldsymbol{\\kappa}}_{i}^{k}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{K}_{i}^{k}=\\left\\{\\left\\{c_{j_{1}},c_{j_{2}},\\ldots,c_{j_{k}}\\right\\}\\mid c_{j_{1}},c_{j_{2}},\\ldots,c_{j_{k}}\\in C\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $j_{1},j_{2},\\ldots$ are the indices of concepts in $C$ . Then, $\\kappa_{i}$ for class $Y_{i}$ can be successfully constructed for $k\\,=\\,1,2,\\dots,K$ , where $K$ is the size of the largest clique in $G$ containing $Y_{i}$ . We construct class clique sets for every class in the dataset. An illustration of concept cliques in the Waterbirds dataset that help in bias diagnosis is provided in Figure 3. ", "page_idx": 3}, {"type": "image", "img_path": "XNGsx3WCU9/tmp/4e38392a68c53bedcc7d915feae905070fa367d737ea85ceac9cc8723735f7fb.jpg", "img_caption": ["Figure 3: Examples of concept clique sets for Landbird class in Waterbirds dataset uncovered by our diagnosis. Concepts such as Tree, Forest, Man, Woman, Bamboo are overwhelmingly associated with this class, indicating strong co-occurrence bias. All these concepts are causally unrelated to the bird type. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Definition (Common Class Clique Sets) Given $\\kappa_{i}$ for each class, we then compute the cliques common to all classes. These are the cliques of interest, whose imbalances we want to investigate: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\kappa=\\bigcap_{i}{\\cal K}_{i},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\kappa$ encapsulates all common cliques enumerated across the dataset for all classes. Refer to Figure 2 for a broad illustration of the $k$ -clique set construction from the concept graph $G$ . ", "page_idx": 4}, {"type": "text", "text": "Definition (Imbalanced Common Cliques) Given the set of common cliques across all classes $\\kappa$ , we compute the imbalanced class-concept combinations, i.e. the imbalanced clique set $I$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nI_{[K]_{m=1}^{M}}=\\{(|F_{K_{y_{i}}^{m}}-F_{K_{y_{j}}^{m}}|,\\arg\\operatorname*{min}(F_{K_{y_{i}}^{m}},F_{K_{y_{j}}^{m}}))\\},\\forall i,j,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $F_{{K_{y}^{m}}}$ and $F_{{K_{y}^{m}}}$ indicates the co-occurrence frequency of concepts in clique $m$ with respect to class $y_{i}$ and $y_{j}$ respectively, and the arg min operator identifies the underrepresented class for the particular concept clique. Thus, each element in $I$ is a number representing the imbalance of each common clique across all classes. For the special case where the size of clique $m$ is 1, this equates to simply looking up the value $w_{i j}$ in $G$ . For the case where the size of $m>2$ , it is straightforward to compute the co-occurrence of class $y_{i}$ with respect to concepts in $m$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nw_{i j\\ldots k}=\\sum_{n=1}^{N}\\mathbb{I}(i\\in D_{n}{\\mathrm{~and~}}j\\in D_{n}\\ldots{\\mathrm{~and~}}k\\in D_{n}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for each image $D_{n}$ in the data. The set $I$ holds rich information about the data. In addition to holding the imbalanced counts of concept combinations across all classes, the set $I$ also holds which is the underrepresented class with respect to a particular concept clique. ", "page_idx": 4}, {"type": "text", "text": "Intuitively, concept combinations that are common across all the classes, but do not co-occur uniformly across the classes are likely biased concept combinations. We provide an example from the Waterbirds dataset in Figure 4. The training set in Waterbirds is intentionally biased to the background: $95\\%$ of landbirds appear with land backgrounds, and $95\\%$ of waterbirds appear with water backgrounds. First, we find common cliques of varying sizes across the classes (Landbird, Waterbird). One example of a common clique of size 3 is (Landbird, Beach, Ocean) and ", "page_idx": 4}, {"type": "image", "img_path": "XNGsx3WCU9/tmp/8764f93c111ec3566a34744abf5a9084e489e27d8c1bef5ccf39995a8860c16b.jpg", "img_caption": ["Figure 4: Examples of concept imbalances in the Waterbirds dataset. We show the frequencies of concepts cliques as discovered in the dataset. We see imbalances across not only single concepts (e.g., Ocean, Grass) but also concept combinations (e.g., (Beach, Ocean), (Tree, Forest)). These are the biases we aim to mitigate for the downstream task. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "(Waterbird, Beach, Ocean). We compute the co-occurrence of (Landbird, Beach, Ocean) and (Waterbird, Beach, Ocean) from the extracted metadata, and the imbalance is clear. Since waterbirds are far more prone to appear on water, there are significantly more images of waterbirds containing concepts Beach and Ocean than landbirds, which are more prone to be in land-based environments. If we look at the co-occurence of Landbird with a land-based concept such as Grass, we see the opposite imbalance. There are significantly more images of landbirds containing trees over waterbirds. Similarly, for the water-based concept of Ocean, we see a strong imbalance towards the Waterbird class. In our debiasing stage, we should therefore generate more images of waterbirds with tree-based backgrounds, and landbirds with beach/water-based backgrounds. Using the clique-based approach, we have successfully uncovered the known background bias in the Waterbirds dataset. This approach is generalizable to multiple classes. All we need are common cliques, and the computation of concept co-occurrences across the dataset. In this way, our concept graph approach uncovers interesting concept combinations across the whole dataset that appear in an imbalanced and spurious fashion. More examples of such imbalances are provided in the supplementary material. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "3.4 Concept Graph Debiasing ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We have, to this point, constructed a knowledge graph of the visual data, and diagnosed it for conceptbased co-occurrence biases. Once the imbalanced clique set $I$ is identified in $G$ , we debias the data by generating images containing under-represented concepts across classes. ", "page_idx": 5}, {"type": "text", "text": "Recall that $I=\\left\\{f_{i},Y_{i}\\right\\}$ inherently holds the underrepresented class $Y_{i}$ and the frequency $f_{i}$ by which the original dataset needs to be adjusted with new images of class $Y_{i}$ with respect to concept clique $i$ . Following the example in the previous section, we notice that the concepts (Beach, Ocean) are significantly over-represented in the Waterbird class than Landbird. Similarly, the concept Tree is significantly over-represented with the Landbird class than the Waterbird class. As a result, we sample $f_{i}$ instances of these under-represented cliques with respect to their classes, and prompt a text-to-image generative model for more images of the Waterbird class with the concept Tree. Similarly, we would prompt the model to generate images of Landbird with the concept Beach, Ocean. We generate images for all class based imbalances following this upsampling protocol.Typical prompts for our image-inpainting model would look like: An image of a ocean and a beach, An image of a tree, An image of a forest, etc. We use an inpainting-based method to make sure that the original object is not modified in the image, and that the new concepts are only injected into the non-object space in the image. See the supplementary material for the generated images and the prompts. ", "page_idx": 5}, {"type": "text", "text": "Using this upsampling protocol, we generate a set of images that leads to our augmented, debiased dataset $D_{\\mathrm{aug}}$ . The original training data $D$ can now be augmented using this data, and the classifier $f_{\\theta}(X)$ can be retrained on the dataset $D\\cup D_{\\mathrm{aug}}$ . In the next section, we conduct experiments on three datasets to demonstrate our method\u2019s significant improvements of baselines. ", "page_idx": 5}, {"type": "text", "text": "Note on concept set annotations. We assume the availability of reliable ground truth concept sets. Such annotations already exist for the datasets we investigate - Waterbirds, UrbanCars, and COCO-GB. We agree that unreliable ground truth concept sets would hinder generalization abilities, but this assumption is not dissimilar to the assumption of reliable ground truth labels in classification tasks. Moreover, the reliance on ground truth concept sets, sometimes referred to as concept banks, have also been considered in prior work [57]. Ground truth concept sets serve as auxiliary knowledge bases and provide human level interpretability to the task at hand. ", "page_idx": 5}, {"type": "text", "text": "Note on computational complexity. In general, given $K$ classes and $C$ concepts, the graph clique enumeration is expected to grow in $O(e x\\bar{p}(K+\\bar{C}))$ . However, in practice, we find that constraining clique sizes $\\leq4$ leads to interpretable bias combinations, with no significant effect of the exponential runtime. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We validate our method on vision datatset diagnosis and debiasing across various scenarios. We begin by introducing the experimental setup including the datasets, baselines, tasks, and implementation details in Section 4.1. Section 4.2 presents the main results of our proposed framework, CONBIAS, compared with state-of-the-art methods. Finally, Section 4.3 details ablation studies and analyses. ", "page_idx": 5}, {"type": "table", "img_path": "XNGsx3WCU9/tmp/ffbba54dbfa279d29bdb8acfddaf6b65c7cebdec8c4e6ba150d65d6e2fcdf3d5.jpg", "table_caption": ["Table 1: State-of-the-art comparison on different datasets. Results are averaged over three training runs. CB: class balanced split. OOD: out-of-distribution split. Binary class classification accuracy is used as the metric. Our method outperforms previous approaches across multiple datasets. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.1 Setup", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets. We use three datasets in our work: Waterbirds [45], UrbanCars [22], and COCO-GB [50], that are commonly used in the bias mitigation domain. We tackle background bias in the Waterbirds dataset, background and co-occuring object bias in the UrbanCars dataset, and finally gender bias in COCO-GB. All the tasks are binary classification tasks. More details on the training splits and class labels are provided in the supplementary material. For Waterbirds, there are 66 nodes and 865 edges in the concept graph. For UrbanCars, the graph contains 19 nodes and 106 edges. For COCO-GB, there are 81 nodes and 2326 edges in the graph. ", "page_idx": 6}, {"type": "text", "text": "Baseline methods. Our baselines are include a vanilla Resnet-50 model pre-trained on ImageNet, two typical data augmentation based debiasing methods: (1) CutMix, a technique where we cut and paste patches between different training images to generate diverse discriminative features, and (2) RandAug, which creates random transformations on the training data during the learning phase. Finally, we compare against the recently proposed and state-of-the-art ALIA[8], which uses a large language model to generate diverse, in-distribution prompts for a text-to-image generative model. ", "page_idx": 6}, {"type": "text", "text": "Evaluation protocols. We compute the mean test accuracy over the class-balanced test data and the out-of-distribution (OOD) test data, similar to [8]. The class-balanced data contains an even distribution of classes and their respective spurious correlations, while the OOD data contains counterfactual concepts. For example, in Waterbirds dataset, for the class-balanced test data $50\\%$ images of Landbirds have Land backgrounds, while $50\\%$ images of Waterbirds have Water backgrounds. The OOD test set contains Landbirds on Water, and Waterbirds on Land. More details on the test sets are presented in the supplementary material. ", "page_idx": 6}, {"type": "text", "text": "Implementation details. We use existing implementations to train our models. Our Base model is a Resnet-50 pretrained on ImageNet [17]. We generate the same number of images per dataaugmentation protocol to ensure a fair comparison. For comparison with ALIA on Waterbirds, we directly use their generated dataset available here. For the other datasets, we used the existing ALIA implementation to generate the augmented data. Following previous work, we use validation loss based checkpointing to choose the best model, the Adam optimizer with a learning rate of $10^{-3}$ , a weight decay of $10^{-5}$ , and a cosine learning schedule over 100 epochs. To generate images, we use Stable Diffusion [44] with a CLIP [40]-based filtering mechanism to ensure reliable image generation. Finally, we inpaint the object onto the generated image using ground truth masks (available for all datasets). All code was written in PyTorch [35]. ", "page_idx": 6}, {"type": "text", "text": "4.2 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In Table 1 we present the main results, averaged over three training runs. First, we note that for Waterbirds and UrbanCars, we observe significant improvements in both the Class-Balanced and OOD test sets over the typical augmentation methods such as CutMix and RandAug. Second, we note the significant improvement in performance over the most recent state-of-the-art augmentation method, ALIA. Third, for COCO-GB, while we notice slightly smaller difference in the CB and OOD accuracies between our method and the baselines, our hypothesis is that this happens because of limited number of samples used for augmentation. ALIA uses a confidence based filtering mechanism to remove generated samples. This leads to a small final number of 260 samples to be added for the retraining part. In the ablation section, we show this hypothesis to be true, and further demonstrate that on adding more images for the retraining step, we progressively increase the performance gap between our method and the baselines. These three observations taken together validate the usefulness of our approach. The next section provides additional insights on the usefulness of our method and the effect of ablating its components. ", "page_idx": 6}, {"type": "table", "img_path": "XNGsx3WCU9/tmp/4d36c682c061ca13605f55581ac9ef1236566920e713a8303733adf8c7e1c721.jpg", "table_caption": ["Table 2: Benefit of the graph structure in CONBIAS. Leveraging the graph structure is beneficial as opposed to simply computing single concept-class frequency counts on UrbanCars. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "XNGsx3WCU9/tmp/bb2e204e16d6db4be9d533bcd107fba871363dcc534d3aa999f71b0d95ed3cf5.jpg", "table_caption": ["Table 3: Performance for the IP2P variant of CONBIAS with respect to base, ALIA, and our original model on Waterbirds. Our method significantly improves over ALIA even when using IP2P, although the best results are still achieved when using the stable diffusion based inpainting protocol. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "XNGsx3WCU9/tmp/3de4cf1cdc58c3953308e36722910c4611c831bdf335b14f8d54bb6c6ea739ed.jpg", "table_caption": ["Table 4: Robustness of our method to evaluation metrics In addition to CB and OOD performance, we also report metrics evaluating multiple shortcut mitigation. Results on UrbanCars (Average of three training runs). "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.3 Ablations and Analyses ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We further analyze our method along five axes: (1) The usefulness of the graph structure, (2) Robustness of our method to other evaluation metrics, (3) The impact on CB and OOD performance by increasing the number of added samples for the retraining step, (4) The usefulness of discovered concepts by our method on the trained classifier, and (5) The impact of the generative component in our work compared to ALIA, since the latter uses InstructPix2Pix [3] while we use a Stable Diffusion based inpainting protocol. ", "page_idx": 7}, {"type": "text", "text": "Effect of the graph structure. Recalling the definition of Class Clique Sets, in principle one could only use cliques sizes of 1, i.e., the direct neighbors of each class node. This would be equivalent to computing the frequency of co-occurrence over a single hop neighborhood of the class node in the graph. In this ablation we show that one should use larger cliques, i.e. leverage the graph structure, instead of a simple direct neighborhood based frequency calculation. We trained three separate models on three different types of $D_{\\mathrm{aug}}$ : Ours (BG), trained on images containing only background shortcuts, Ours (CoObj), images containing only the co-occurrence shortcuts, and Ours (Both), images containing both shortcuts, but not simultaneously. ", "page_idx": 7}, {"type": "text", "text": "Table 2 shows the results. First, our approach of leveraging the graph structure provides improvement over simply using the frequency of a 1-hop neighborhood. Second, we note that all the methods outperform the baseline and ALIA, which shows that incorporating frequency based co-occurrences is in a broader sense much more useful than relying on diverse prompts generated by ChatGPT-4, which is the approach taken by ALIA. ", "page_idx": 7}, {"type": "text", "text": "Robustness to evaluation metrics. The CB and OOD test accuracies test for generalization capabilities, but more direct evaluators of shortcut learning exist in the literature. In [22], for instance, the authors propose (i) The ID Accuracy - which is the accuracy when the test set contains common background and co-occurring objects, (ii) The BG-GAP - which is the drop in $I D$ accuracy when the test set contains common co-occurring objects, but uncommon background objects, (iii) The ", "page_idx": 7}, {"type": "image", "img_path": "XNGsx3WCU9/tmp/3d897775f2eaba4f1d7cc0f1ff8bcad38837a44edfa2b2096b1db7def6ca1ea7.jpg", "img_caption": ["Figure 5: Performance on COCO-GB. We show the accuracies on (a) Class-Balanced (CB) and (b) Out-of-Distribution (OOD) splits. We observe that increasing number of images in $D_{\\mathrm{aug}}$ improves performance up to a certain point (1000 images). "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "XNGsx3WCU9/tmp/d8c2af1b9aa829f2086ed289616c5da5e84165bd0353a88305d13d5d66ae2228.jpg", "img_caption": ["Figure 6: Visualization of the heatmaps for different methods. Top row: Waterbirds. Middle row: UrbanCars. Bottom row: COCO-GB. Our method enforces the base model to focus on only the relevant features in the data, and removing reliance on shortcut features, i.e. the background for Waterbirds, the background and co-occurring object for UrbanCars, and the gender for COCO-GB. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "CoObj-GAP, which is the drop in $I D$ accuracy when the test set contains uncommon co-occurring objects, but common background objects, and finally (iv) The $B G+C o O b j\\,G A_{i}$ P, which is the case when both background and co-occurring objects are uncommon in the test set. A multiple shortcut mitigation method should minimize the $B G+C o O B j$ $G A P$ metric, and also make sure it does not exacerbate any shortcut that the base model relies on. In Table 4, we present results of Base, Base (BG), Base (CoObj), Base(Both), and Base (CONBIAS) on these metrics for UrbanCars. We are able to post the lowest drops among all baselines on the CoObj- $\\ G A P$ and $B G+C o O B j$ $G A P$ metrics, suggesting mitigation of multiple shortcut reliance. This places our method in a more realistic context, as it is infeasible to assume that real world data will only have a single type of bias in them. ", "page_idx": 8}, {"type": "text", "text": "Scaling the number of images in $D_{\\mathbf{aug}}$ . In Table 1, we commented on the fact that our method provides marginal improvement over the baselines in the COCO-GB dataset. Our hypothesis was that this was due to the low number of images in the augmented dataset. In Figure 5, we demonstrate the impact of adding more images to $D_{\\mathrm{aug}}$ for retraining. Clearly, our method benefits from this protocol, leading to significant differences over ALIA as we keep increasing the number of images. Note that, infinite enrichment is not recommended and has been found to be detrimental to classifier performance, as progressive addition of synthetic images will likely lead to addition of out-of-distribution examples in the training data. This explains why, after an inflexion point, the accuracy suffers from adding more images. Similar observations have been made in [8] and [18]. ", "page_idx": 8}, {"type": "text", "text": "Discovered concept imbalances and feature attributions. To verify that the model indeed debiases the imbalanced concepts that our method discovers, we present GradCAM [46] attributions of the model predictions after retraining. In Figure 6, we show results on all datasets. While other methods frequently focus on the spurious feature , CONBIAS helps the model focus only on the relevant, object level features of the data. ", "page_idx": 8}, {"type": "text", "text": "The impact of the generative model. ALIA uses an InstructPix2Pix (IP2P) based generation procedure, while we use stable diffusion with a mask-inpainting procedure to make sure the objects remain consistent in the image. To ablate the effect of the generation, we present results of our method with IP2P as the generative model instead, on Waterbirds dataset, in Table 3. First, we note that even with IP2P as the generative component, we are able to outperform ALIA, which suggests that it is actually the superior quality of our concept discovery method that leads to the improved results. Second, our inpainting based method outperforms our IP2P based method, which we argue is due to the objects being preserved in the generated image, as opposed to traditional image editing methods, where the object may transform arbitrarily, hurting the quality of augmented data. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion, Limitations, and Future Works ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "While CONBIAS is the first end-to-end pipeline to both diagnose and debias visual datasets, there are some limitations: First, that the enumeration of cliques grows exponentially with the size of the graph. For larger real world graphs, there could be more efficient strategies to find the concept combinations. Second, in this work we focus on biases emanating out of object co-occurrences. A variety of other biases exist in vision datasets, and future work would look to address the same. We add an extended section on broader impact of our work in the supplementary material. In summary, datasets in the real world are biased, and the exponential increase in dataset sizes over the past decade amplifies the challenge of investigating model and dataset biases. While both dataset and model diagnosis are exciting areas of research, an end-to-end diagnosis and debiasing pipeline such as CONBIAS offers a principled approach to diagnosing and debiasing visual datasets, in turn improving downstream classification performance. Our state-of-the art results open up numerous interesting possibilities for future work - incorporating more novel graph structures, and diagnosis under the regime where concept sets may be wholly or partially unavailable, remain interesting directions to pursue. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research is partially supported by a grant from Apple Inc. We thank Nicholas Apostoloff, Oncel Tuzel, and Jerremy Holland for their valuable feedback on the draft of this paper. Any views, opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and should not be interpreted as reflecting the views, policies or position, either expressed or implied, of Apple Inc. The authors would also like to thank the Norwegian Research Council for funding a doctoral research visit to the Human Sensing Lab, Carnegie Mellon University. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Md Rifat Arefin, Yan Zhang, Aristide Baratin, Francesco Locatello, Irina Rish, Dianbo Liu, and Kenji Kawaguchi. Unsupervised concept discovery mitigates spurious correlations. arXiv preprint arXiv:2402.13368, 2024.   \n[2] Philipp Bomatter, Mengmi Zhang, Dimitar Karev, Spandan Madan, Claire Tseng, and Gabriel Kreiman. When pigs fly: Contextual reasoning in synthetic and natural scenes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 255\u2013264, 2021.   \n[3] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18392\u201318402, 2023.   \n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In NeurIPS, 2020.   \n[5] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 702\u2013703, 2020.   \n[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248\u2013255, 2009. [7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. [8] Lisa Dunlap, Alyssa Umino, Han Zhang, Jiezhi Yang, Joseph E Gonzalez, and Trevor Darrell. Diversify your vision datasets with automatic diffusion-based augmentation. Advances in Neural Information Processing Systems, 36, 2024. [9] Simone Fabbrizzi, Symeon Papadopoulos, Eirini Ntoutsi, and Ioannis Kompatsiaris. A survey on bias in visual datasets. Computer Vision and Image Understanding, 223:103552, 2022.   \n[10] Carolina Galleguillos, Andrew Rabinovich, and Serge Belongie. Object categorization using co-occurrence, location and appearance. In 2008 IEEE Conference on Computer Vision and Pattern Recognition, pages 1\u20138. IEEE, 2008.   \n[11] Irena Gao, Gabriel Ilharco, Scott Lundberg, and Marco Tulio Ribeiro. Adaptive testing of computer vision models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4003\u20134014, 2023.   \n[12] Noa Garcia, Yusuke Hirota, Yankun Wu, and Yuta Nakashima. Uncurated image-text datasets: Shedding light on demographic bias. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6957\u20136966, 2023.   \n[13] Robert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665\u2013673, 2020.   \n[14] Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and Wieland Brendel. Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. In International Conference on Learning Representations, 2019.   \n[15] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14953\u201314962, 2023.   \n[16] Laura Gustafson, Chloe Rolland, Nikhila Ravi, Quentin Duval, Aaron Adcock, Cheng-Yang Fu, Melissa Hall, and Candace Ross. Facet: Fairness in computer vision evaluation benchmark. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 20370\u201320382, 2023.   \n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.   \n[18] Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip Torr, Song Bai, and XIAOJUAN QI. Is synthetic data from generative models ready for image recognition? In The Eleventh International Conference on Learning Representations, 2022.   \n[19] Younghyun Kim, Sangwoo Mo, Minkyu Kim, Kyungmin Lee, Jaeho Lee, and Jinwoo Shin. Bias-to-text: Debiasing unknown visual biases through language interpretation. arXiv preprint arXiv:2301.11104, 2023.   \n[20] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00e1r, and Ross Girshick. Segment anything. arXiv:2304.02643, 2023.   \n[21] Sangjun Lee, Inwoo Hwang, Gi-Cheon Kang, and Byoung-Tak Zhang. Improving robustness to texture bias via shape-focused augmentation. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 4322\u20134330, 2022.   \n[22] Zhiheng Li, Ivan Evtimov, Albert Gordo, Caner Hazirbas, Tal Hassner, Cristian Canton Ferrer, Chenliang Xu, and Mark Ibrahim. A whac-a-mole dilemma: Shortcuts come in multiples where mitigating one amplifies others. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20071\u201320082, 2023.   \n[23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740\u2013755. Springer, 2014.   \n[24] Evan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa, Percy Liang, and Chelsea Finn. Just train twice: Improving group robustness without training group information. In International Conference on Machine Learning, pages 6781\u20136792. PMLR, 2021.   \n[25] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024.   \n[26] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023.   \n[27] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), December 2015.   \n[28] Aengus Lynch, Gb\u00e8tondji JS Dovonon, Jean Kaddour, and Ricardo Silva. Spawrious: A benchmark for fine control of spurious correlation biases. arXiv preprint arXiv:2303.05470, 2023.   \n[29] Mark Mazumder, Colby Banbury, Xiaozhe Yao, Bojan Karla\u0161, William Gaviria Rojas, Sudnya Diamos, Greg Diamos, Lynn He, Alicia Parrish, Hannah Rose Kirk, et al. Dataperf: Benchmarks for data-centric ai development. Advances in Neural Information Processing Systems, 36, 2024.   \n[30] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A survey on bias and fairness in machine learning. ACM computing surveys (CSUR), 54(6):1\u201335, 2021.   \n[31] Sangwoo Mo, Hyunwoo Kang, Kihyuk Sohn, Chun-Liang Li, and Jinwoo Shin. Object-aware contrastive learning for debiased scene representation. Advances in Neural Information Processing Systems, 34:12251\u2013 12264, 2021.   \n[32] Chaithanya Kumar Mummadi, Ranjitha Subramaniam, Robin Hutmacher, Julien Vitay, Volker Fischer, and Jan Hendrik Metzen. Does enhanced shape bias improve neural network robustness to common corruptions? In International Conference on Learning Representations, 2020.   \n[33] Curtis Northcutt, Lu Jiang, and Isaac Chuang. Confident learning: Estimating uncertainty in dataset labels. Journal of Artificial Intelligence Research, 70:1373\u20131411, 2021.   \n[34] Aude Oliva and Antonio Torralba. The role of context in object recognition. Trends in cognitive sciences, 11(12):520\u2013527, 2007.   \n[35] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.   \n[36] Gregory Plumb, Marco Tulio Ribeiro, and Ameet Talwalkar. Finding and fixing spurious patterns with explanations. Transactions on Machine Learning Research, 2022.   \n[37] Viraj Prabhu, Sriram Yenamandra, Prithvijit Chattopadhyay, and Judy Hoffman. Lance: Stress-testing visual models by generating language-guided counterfactual images. Advances in Neural Information Processing Systems, 36, 2024.   \n[38] Wei Qin, Hanwang Zhang, Richang Hong, Ee-Peng Lim, and Qianru Sun. Causal interventional training for image recognition. IEEE Transactions on Multimedia, 25:1033\u20131044, 2023.   \n[39] Xinkuan Qiu, Meina Kan, Yongbin Zhou, Yanchao Bi, and Shiguang Shan. Shape-biased cnns are not always superior in out-of-distribution robustness. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2326\u20132335, 2024.   \n[40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[41] Joseph Redmon and Ali Farhadi. Yolo9000: better, faster, stronger. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7263\u20137271, 2017.   \n[42] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples for robust deep learning. In International conference on machine learning, pages 4334\u20134343. PMLR, 2018.   \n[43] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang. Grounded sam: Assembling open-world models for diverse visual tasks, 2024.   \n[44] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models, 2021.   \n[45] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks. In International Conference on Learning Representations, 2019.   \n[46] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision, pages 618\u2013626, 2017.   \n[47] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.   \n[48] Krishna Kumar Singh, Dhruv Mahajan, Kristen Grauman, Yong Jae Lee, Matt Feiszli, and Deepti Ghadiyaram. Don\u2019t judge an object by its context: Learning to overcome contextual bias. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11070\u201311078, 2020.   \n[49] Nimit Sohoni, Jared Dunnmon, Geoffrey Angus, Albert Gu, and Christopher R\u00e9. No subclass left behind: Fine-grained robustness in coarse-grained classification problems. Advances in Neural Information Processing Systems, 33:19339\u201319352, 2020.   \n[50] Ruixiang Tang, Mengnan Du, Yuening Li, Zirui Liu, Na Zou, and Xia Hu. Mitigating gender bias in captioning systems. In Proceedings of the Web Conference 2021, pages 633\u2013645, 2021.   \n[51] A Torralba and AA Efros. Unbiased look at dataset bias. In Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition, pages 1521\u20131528, 2011.   \n[52] Angelina Wang, Alexander Liu, Ryan Zhang, Anat Kleiman, Leslie Kim, Dora Zhao, Iroha Shirai, Arvind Narayanan, and Olga Russakovsky. Revise: A tool for measuring and mitigating bias in visual datasets. International Journal of Computer Vision, 130(7):1790\u20131810, 2022.   \n[53] Angelina Wang and Olga Russakovsky. Overwriting pretrained bias with finetuning data. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3957\u20133968, 2023.   \n[54] Bowen Wang, Liangzhi Li, Yuta Nakashima, and Hajime Nagahara. Learning bottleneck concepts in image classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10962\u201310971, 2023.   \n[55] Zhenyu Wang, Yali Li, Xi Chen, Ser-Nam Lim, Antonio Torralba, Hengshuang Zhao, and Shengjin Wang. Detecting everything in the open world: Towards universal object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11433\u201311443, 2023.   \n[56] Olivia Wiles, Isabela Albuquerque, and Sven Gowal. Discovering bugs in vision models using off-the-shelf image generation and captioning. In NeurIPS ML Safety Workshop, 2022.   \n[57] Shirley Wu, Mert Yuksekgonul, Linjun Zhang, and James Zou. Discover and cure: Concept-aware mitigation of spurious correlation. In International Conference on Machine Learning, pages 37765\u201337786. PMLR, 2023.   \n[58] Kaiyu Yang, Klint Qinami, Li Fei-Fei, Jia Deng, and Olga Russakovsky. Towards fairer datasets: Filtering and balancing the distribution of the people subtree in the imagenet hierarchy. In Proceedings of the 2020 conference on fairness, accountability, and transparency, pages 547\u2013558, 2020.   \n[59] Sriram Yenamandra, Pratik Ramesh, Viraj Prabhu, and Judy Hoffman. Facts: First amplify correlations and then slice to discover bias. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4794\u20134804, 2023.   \n[60] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6023\u20136032, 2019. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Supplementary Materials ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this supplementary materials, we provide details and additional results omitted in the main text. ", "page_idx": 13}, {"type": "text", "text": "\u2022 Section 6: Broader Impact.   \n\u2022 Section 7: Dataset details - Splits and image examples.   \n\u2022 Section 8: Full Concept Set for each dataset.   \n\u2022 Section 9: Dataset Imbalances.   \n\u2022 Section 10: The Generative Model.   \n\u2022 Section 11: Examples of Generated Images using CONBIAS.   \n\u2022 Section 12: Results with standard deviations.   \n\u2022 Section 13: Compute details and runtime.   \n\u2022 Section 14: The sampling algorithm for attribute rebalancing.   \n\u2022 Section 15: An illustration of our method diagnosing spurious correlations in ImageNet-1k. ", "page_idx": 13}, {"type": "text", "text": "6 Broader Impact ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Fairness in AI is rapidly gaining priority in current research as models and datasets grow exponentially larger, thus making it more and more complicated to diagnose them for biases. It is imperative to focus on understanding and mitigating biases learned by models, and inherent biases in the data, to ensure reliable and transparent predictions in the real world. The advent of generative models in particular, including large language models, and image generative models, invites new questions into how to reliably regulate such technologies. These models are trained on datasets in the order of hundreds of billions of data points. How do we ensure that problematic aspects of the data do not pass onto the models learning from them? How do we ensure that models do not generate synthetic data that is potentially harmful, misleading, and misinformative in nature? How do we evaluate the quality of generated data by such models? These are the pressing questions that our research direction is interested in. ", "page_idx": 13}, {"type": "text", "text": "7 Dataset Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We use three datasets in our work - Waterbirds [45], UrbanCars [22], and COCO-GB [50]. ", "page_idx": 13}, {"type": "text", "text": "For Waterbirds, the class labels are Landbird, Waterbird. The Waterbirds dataset has the background bias, i.e. $95\\%$ images of landbirds have land-based backgrounds, and $95\\%$ images of waterbirds have water-based backgrounds. For the concept set annotations, we use the captions extracted by authors of [8] captions available here. ", "page_idx": 13}, {"type": "text", "text": "For UrbanCars, the class labels are Urban, Country, defining the type of car. There are multiple biases in UrbanCars - (1) Background Bias, i.e. Urban cars appear with $95\\%$ correlation with urban backgrounds, and Country cars appear with $95\\%$ correlation with country backgrounds. (2) Co-Occurring object, i.e. Urban cars appear with $95\\%$ correlation with urban objects, and Country cars appear with $95\\%$ correlation with country objects. ", "page_idx": 13}, {"type": "text", "text": "For COCO-GB, the class labels are Man, Woman. The bias for the dataset are the set of objects in the MS-COCO dataset [23]. The authors of [50] find a strong bias of most objects in the data with respect to the \"Man\" class, and design a secret, gender-balanced test set to evaluate gender bias in classifiers. ", "page_idx": 13}, {"type": "text", "text": "7.1 Waterbirds ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In Fig 7 we present examples from the Waterbirds training data. The classes are heavily biased to the backgrounds, i.e. Landbirds on Land, Waterbirds on Water. ", "page_idx": 13}, {"type": "text", "text": "7.2 UrbanCars ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In Fig 8 we present examples from the UrbanCars training data. The classes are heavily biased to multiple shortcuts - Background and Co-Occurring objects. ", "page_idx": 13}, {"type": "image", "img_path": "XNGsx3WCU9/tmp/ec1d34f88186dd32caa629d3fc1d0cdb3e8b8bd0e9d8e86c58529f4effe0dee9.jpg", "img_caption": ["Figure 7: Examples of training data in Waterbirds dataset. Waterbirds (Top) are $95\\%$ biased towards water backgrounds, while Landbirds (Bottom) are $95\\%$ biased towards land backgrounds. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "XNGsx3WCU9/tmp/291b6f425816ecd19ccb7797f73d042ea90c77b21c5aefea800034635a8b223c.jpg", "img_caption": ["Figure 8: Examples of training data in UrbanCars dataset. Urban cars (Top) are $95\\%$ biased towards urban backgrounds and urban co-occurring objects. Country cars (Bottom) are $95\\%$ biased towards country backgrounds and country co-occurring objects. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "7.3 COCO-GB", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In Fig 9 we present examples from the COCO-GB training data. The \"Man\" class is known to be heavily biased in MS-COCO to everyday objects. ", "page_idx": 14}, {"type": "image", "img_path": "XNGsx3WCU9/tmp/656a4eae66a34add289079ef57eb98d74bcfcea315c3deda5d96e4b823c731a3.jpg", "img_caption": ["Figure 9: Examples of training data in MS-COCO dataset. Images of men are heavily biased towards common, everyday objects, as opposed to women. Authors of [50] find over a $90\\%$ in all object correlations towards men. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "7.4 Splits ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In Table 5 we present the train, validation, and test splits for our three datasets. ", "page_idx": 14}, {"type": "table", "img_path": "XNGsx3WCU9/tmp/f5b8878a683e8b1178a50f78c3ad8cf1acb961e933a7498c730380a6520b0846.jpg", "table_caption": ["Table 5: Dataset sizes for Train, Test, and Validation sets "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "8 Concept Sets ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In Table 6 we present the full concept sets for each dataset. The Waterbirds dataset has 64 unique concepts, the UrbanCars dataset has 17 unique concepts, and COCO-GB has 81 unique concepts, all ", "page_idx": 14}, {"type": "text", "text": "from the MS-COCO dataset. Note that both MS-COCO and UrbanCars have ground truth concepts, while for Waterbirds, we use the extracted captions here. ", "page_idx": 15}, {"type": "table", "img_path": "XNGsx3WCU9/tmp/8ab56db4db0c21d0d6d2f79716ed131d6e2110d755f95fe15b9a5ea76ab68e0d.jpg", "table_caption": ["Table 6: Concepts for Waterbirds, UrbanCars, and COCO-GB datasets "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "9 Dataset Imbalances ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section we shed more insight into what sort of concept imbalances ConBias discovers. These object level insights are also, to the best of our knowledge, the first of its kind, shedding more light on the secret co-ocurrence biases hidden in data. ", "page_idx": 15}, {"type": "text", "text": "9.1 Waterbirds ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In addition to the main paper, we list some other category imbalances in Waterbirds in Figure 10. Some of these extreme imbalances appear in diverse 2-clique/3-clique combinations. For example, we see that concepts like forest, man, woman are significantly biased towards the Landbird class, while concepts like beach, man, sun, lake, mountain are biased towards the Waterbird class. This is the background bias that is known in the Waterbirds dataset, that ConBias successfully uncovers. ", "page_idx": 15}, {"type": "text", "text": "9.2 UrbanCars ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In UrbanCars, the class labels (country car, urban car) are intentionally biased towards background and co-occurring objects. In Figure 11, we see that there exists an extreme imbalance betwee urban concepts such as driveway, traffic light towards urban cars, and country concepts such as forest road, field road, cow, horse towards country cars. These are exactly the background and co-occurring biases in the construction of the data, that ConBias successfully uncovers. ", "page_idx": 15}, {"type": "text", "text": "9.3 COCO-GB", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The gender bias in COCO-GB has been extensively studied in [50]. In Figure 12, we show the extreme imbalance towards specific concepts in the MS-COCO dataset. Concepts such as baseball bat, sports ball, motorcycle, truck overwhelmingly correlate with images of men, which may be problematic for the classifier to learn. ", "page_idx": 15}, {"type": "image", "img_path": "XNGsx3WCU9/tmp/6637a77264c2d34831aa9b7d6f605b0b0c167e504ff0a1117923162b5e76d8d7.jpg", "img_caption": ["Figure 10: Extreme imbalance of particular concepts in Waterbirds dataset, as discovered by ConBias. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "XNGsx3WCU9/tmp/7850116f90a1ea8ff4b671c7b95f806e3c7fee589c94628982621f6564d06507.jpg", "img_caption": ["Figure 11: Extreme imbalance of particular concepts in UrbanCars dataset, as discovered by ConBias. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "10 Generative Model ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Here we present more details of our generative model. We use Stable Diffusion based inpainting, as illustrated in Figure 13. Given the prompt, we first generate an image using Stable Diffusion [44]. Next, using ground truth masks of the object, we paste the object at the foreground of the generated image. In this way, we preserve the original object in the image, which is a challenge for traditional image editing methods such as InstructPix2Pix. We believe the inpainting method is a more principled approach to synthetic image generation, particularly if the downstream task is classification in nature. ", "page_idx": 16}, {"type": "text", "text": "The generation process of a single image takes the followings as input: The sampled concept combination and the class for which this concept combination needs to be generated. The output of the generative model is the final image with the specified concepts in the background and an instance of the specified category in the foreground. ", "page_idx": 16}, {"type": "text", "text": "The process will first transform the concept list [concept 1, concept 2, . . . , concept N] into a prompt: \u201ca photo of concept 1, concept 2, . . . , and concept N.\u201d The prompt is then passed into the text-to-image generation model (stable diffusion) to get the generated image as background. We apply a clip-score filtering after the generation process to only keep the images with a CLIP-score over 0.6 to make sure ", "page_idx": 16}, {"type": "image", "img_path": "XNGsx3WCU9/tmp/138f7d2b1537e23fe91f67c6050f1f383ac883d0f4613f7e948bc752df4298ed.jpg", "img_caption": ["Figure 12: Extreme imbalance of particular concepts in MS-COCO dataset, as discovered by ConBias. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "XNGsx3WCU9/tmp/075f81e258dcc0f1882bb7572f7bc6238c55f5405c87addfe2cc89030dd331e2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 13: Image generation Pipeline: Given concepts to be upsampled as discovered ConBias, we sample the concept combinations and images from the class to be upsampled. We prompt Stable Diffusion for an image containing such concepts. We extract the object of interest using ground truth masks, and inpaint the object over the generated image. This ensures that the object features are not harmed during generation. We use a CLIP-based scoring filter to make sure the generated image contains the concepts requested in the prompt. We have found a score of 0.6 to be satisfactory as a threshold. ", "page_idx": 17}, {"type": "text", "text": "that the generated images can accurately represent the concept list. Next, the process will sample an image of the specified category from the original dataset, and use the mask to segment out the desired object. Finally, inpainting is performed to clip the desired object as foreground onto the generated image to obtain the final image. ", "page_idx": 17}, {"type": "text", "text": "11 Generated Images by ConBias ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section we present examples of synthetic data generated by ConBias for Waterbirds, UrbanCars, and COCO-GB. ", "page_idx": 17}, {"type": "text", "text": "11.1 Waterbirds ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Figure 14 we present diverse images generated by ConBias for the two classes of Landbird and Waterbird. Due to the bias diagnosis stage where we found the overwhelming correlation between landbirds with land based backgrounds such as tree, forest, field, grass, etc, and waterbirds with water based backgrounds such as beach, ocean, boat, etc, ConBias was automatically able to decide which concept combinations to use to generate new, debiased images. ", "page_idx": 18}, {"type": "image", "img_path": "XNGsx3WCU9/tmp/f9c0a6c493423f73600f8ef795682054f7b48ca4ca9de17f97c3d580b22ea98d.jpg", "img_caption": ["Figure 14: (Top) Generated images of waterbirds with land-based backgrounds. (Bottom) Generated images of landbirds with water-based backgrounds, as discovered by ConBias. Note the consistency in object preservation. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "11.2 UrbanCars ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Figure 15 we present diverse images generated by ConBias for the two classes of Urban and Country cars. Due to the bias diagnosis stage, we were able to discover the overwhelming correlation between urban cars with urban based backgrounds such as gas station, driveway, alley, etc and urban co-occurring objects such as fireplug, stop sign, etc. Similarly, for country cars, we discovered bias towards country backgrounds such as desert road, field road, forest road, and, and country co-occurring objects such as cow, sheep, horse. As a result, ConBias helps generate urban cars with country based backgrounds and co-occurring objects, and vice versa. ", "page_idx": 18}, {"type": "image", "img_path": "XNGsx3WCU9/tmp/182ed41f01d7ac61b2583747fdb0857bf57211f8e2e703727c7798d1dc404eef.jpg", "img_caption": ["Figure 15: (Top) Generated images of country cars with urban-based backgrounds and co-ocurring objects. (Bottom) Generated images of urban cars with urban-based backgrounds and co-occurring objects, as discovered by ConBias. Note the consistency in object preservation. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "11.3 COCO-GB", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Figure 16 we present diverse images generated by ConBias for the two classes of Man and Woman in COCO-GB. In this dataset, we were able to discover significant under-representation of women with respect to common, everyday objects in the MS-COCO dataset. Some examples include skateboard, motorcycle, car, truck, etc. These objects could have gendered assumptions and it is imperative for debiased datasets to have uniform representation across classes for such concepts. ", "page_idx": 18}, {"type": "text", "text": "We would also like to bring to the attention of our readers the successful nature of the inpainting procedure. We are able to consistently preserve the class label of interest in the synthetic images. This is imperative to ensure that the generative pipeline does not create unreasonable objects that make it infeasible for the classifier to learn. ", "page_idx": 18}, {"type": "image", "img_path": "XNGsx3WCU9/tmp/cc489b3d846c30f362903bf240e8e1a27c78bf8ea708263dbd98cc02c41d27d3.jpg", "img_caption": ["Figure 16: Generated images of COCO-GB using everyday, common objects that are discovered to be biased towards men by ConBias. Example concepts include skateboard, motorcycle, truck, sports ball, etc. Note the consistency in object preservation. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "12 Confidence Intervals ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In Table 7 we present the averaged results with standard deviations over three training runs. For both Waterbirds and UrbanCars, our improvements are large and significant. For COCO-GB, while originally did not observe statistically significant results, in the main paper we showed that increasing the number of images in $D_{a u g}$ leads to significant improvements over the baselines. ", "page_idx": 19}, {"type": "table", "img_path": "XNGsx3WCU9/tmp/df57f9bdd25a609779ca7e4dbbe6b48d5d4dc9ad7c1dd25bc07f129c29b9543a.jpg", "table_caption": ["Table 7: State-of-the-art comparison on different datasets. Results are averaged over three training runs. CB: class balanced split. OOD: out-of-distribution split. Binary class classification accuracy is used as the metric. CONBIAS outperforms previous approaches across multiple datasets. Standard deviations included. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "13 Compute Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We trained all models on a single NVIDIA RTX A4000 and used PyTorch [35] for all experiments. With the early stopping cosine learning scheduled described in the main paper, we observed fast training times, with 90 minutes for three runs on Waterbirds and UrbanCars, and 180 minutes for three runs on COCO-GB. ", "page_idx": 19}, {"type": "text", "text": "14 Sampling Algorithm ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The rebalance sampling algorithm 1 receives the concept graph as a concept co-occurrence matrix. The algorithm iterates through all cliques with an order of decreasing clique sizes to make sure we would not double compensate for the imbalance, e.g., 3-cliques would impact the already-balanced 2-cliques if we operate in a bottom-up fashion. For each iteration, it retrieves all cliques of concepts of size k along with their corresponding frequencies with each class. The algorithm identifies the maximum co-occurrence count among all classes for each combination and checks if any class is under-represented by comparing its count with the maximum. If a class is under-represented, the algorithm computes the number of synthetic samples needed to balance the representation and adds this information to the results list. This process continues for all combinations and classes until all clique sizes have been processed. The output of the algorithm is a list of queries specifying the class, concept combination, and the number of samples needed to balance the dataset. ", "page_idx": 19}, {"type": "text", "text": "15 Diagnosing ImageNet-1k ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section we demonstrate the usefulness of ConBias in diagnosing spurious concepts in a more complex dataset, i.e. ImageNet-1k. Specifically, we investigate the classes ambulance, beach wagon, sports car, limousine, minivan, jeep, convertible, cab. These are associated with the superclass of Car. This approach can be used for any set of classes in the dataset. We use the open-source concept annotations available here. ", "page_idx": 19}, {"type": "text", "text": "Input: $M_{o c c}$ - Concept occurrence matrix as a dictionary   \n1: result $s\\gets[]$ $\\triangleright$ Initialize result list for complement samples   \n2: $k\\leftarrow$ the maximum size of concept cliques   \n3: while $k>1$ do   \n4: $c o m b o s\\gets M_{o c c}[k]$ $\\triangleright$ Get the concept combinations and counts of all $k$ -cliques   \n5: for each concept combination $C\\in$ combos do   \n6: $n_{i}\\leftarrow$ number of co-occurrence between combo $C$ and class $i$   \n7: $m\\leftarrow\\operatorname*{max}(\\{n_{i}\\})$ $\\triangleright$ Determine the maximum frequency among the classes   \n8: for each class $i$ do   \n9: if $n_{i}<m$ then $\\triangleright$ If the class $i$ is under-represented w.r.t. combo $C$   \n10: $\\begin{array}{l}{\\hat{n}_{i}\\leftarrow m-n_{i}}\\\\ {r e s u l t s\\leftarrow r e s u l t s\\cup(i,C,\\hat{n}_{i})}\\end{array}$ $\\triangleright$ Compute the number of samples to generate   \n11: $\\triangleright$ Save generation query   \n12: end if   \n13: end for   \n14: end for   \n15: Update $M_{o c c}[k^{\\prime}]_{k^{\\prime}<k}$ with the generated samples   \n16: $k\\gets k-1$ $\\triangleright$ Move to cliques with size smaller by 1 ", "page_idx": 20}, {"type": "image", "img_path": "XNGsx3WCU9/tmp/491227b7c0c2e8c9e33e92a93d351b7f6a1cdc145098f7f4f5aefbd3d664b7d3.jpg", "img_caption": ["Figure 17: Concept imbalances for cars in ImageNet-1k "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "In Figure 17 we notice some interesting imbalances discovered by ConBias. For 1-clique concept combinations such as Tram and Prison, the dataset disproportionately contains images of cabs and ambulances respectively. For 2-clique concept combinations, the dataset disproportionately represents the jeep, convertible, and beach wagon classes. It is evident that such concepts are spurious when it comes to classifying a car type, but a strong imbalanced distribution would bias the classifier to pick up on spurious features. In this way, ConBias helps us uncover such biases, allowing for intervention in downstream tasks. ", "page_idx": 20}, {"type": "text", "text": "491 The checklist is designed to encourage best practices for responsible machine learning research,   \n492 addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove   \n493 the checklist: The papers not including the checklist will be desk rejected. The checklist should   \n494 follow the references and follow the (optional) supplemental material. The checklist does NOT count   \n495 towards the page limit.   \n496 Please read the checklist guidelines carefully for information on how to answer these questions. For   \n497 each question in the checklist:   \n498 \u2022 You should answer [Yes] , [No] , or [NA] .   \n499 \u2022 [NA] means either that the question is Not Applicable for that particular paper or the   \n500 relevant information is Not Available.   \n501 \u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA).   \n502 The checklist answers are an integral part of your paper submission. They are visible to the   \n503 reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it   \n504 (after eventual revisions) with the final version of your paper, and its final version will be published   \n505 with the paper.   \n506 The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.   \n507 While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a   \n508 proper justification is given (e.g., \"error bars are not reported because it would be too computationally   \n509 expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering   \n510 \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we   \n511 acknowledge that the true answer is often more nuanced, so please just use your best judgment and   \n512 write a justification to elaborate. All supporting evidence can appear either in the main paper or the   \n513 supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification   \n514 please point to the section(s) where related material for the question can be found.   \n516 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n517 paper\u2019s contributions and scope?   \n518 Answer: [Yes]   \n519 Justification: Table 1 in the main paper and our extended analysis section demonstrate the   \n520 usefulness of our approach.   \n521 Guidelines:   \n522 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n523 made in the paper.   \n524 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n525 contributions made in the paper and important assumptions and limitations. A No or   \n526 NA answer to this question will not be perceived well by the reviewers.   \n527 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n528 much the results can be expected to generalize to other settings.   \n529 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n530 are not attained by the paper.   \n531 2. Limitations   \n532 Question: Does the paper discuss the limitations of the work performed by the authors?   \n533 Answer: [Yes]   \n534 Justification: We discuss limitations of our work in the final section of the paper.   \n535 Guidelines:   \n536 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n537 the paper has limitations, but those are not discussed in the paper.   \n538 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n539 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n540 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n541 model well-specification, asymptotic approximations only holding locally). The authors   \n542 should reflect on how these assumptions might be violated in practice and what the   \n543 implications would be.   \n544 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n545 only tested on a few datasets or with a few runs. In general, empirical results often   \n546 depend on implicit assumptions, which should be articulated.   \n547 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n548 For example, a facial recognition algorithm may perform poorly when image resolution   \n549 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n550 used reliably to provide closed captions for online lectures because it fails to handle   \n551 technical jargon.   \n552 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n553 and how they scale with dataset size.   \n554 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n555 address problems of privacy and fairness.   \n556 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n557 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n558 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n559 judgment and recognize that individual actions in favor of transparency play an impor  \n560 tant role in developing norms that preserve the integrity of the community. Reviewers   \n561 will be specifically instructed to not penalize honesty concerning limitations.   \n562 3. Theory Assumptions and Proofs   \n563 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n564 a complete (and correct) proof?   \n565 Answer: [NA]   \n566 Justification: There are no theoretical results or proofs in the paper.   \n567 Guidelines:   \n568 \u2022 The answer NA means that the paper does not include theoretical results.   \n569 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n570 referenced.   \n571 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n572 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n573 they appear in the supplemental material, the authors are encouraged to provide a short   \n574 proof sketch to provide intuition.   \n575 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n576 by formal proofs provided in appendix or supplemental material.   \n577 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n578 4. Experimental Result Reproducibility   \n579 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n580 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n581 of the paper (regardless of whether the code and data are provided or not)?   \n582 Answer: [Yes]   \n583 Justification: We include details of the datasets in both the main paper and supplementary.   \n584 The implementation of baselines rely on open source codes that are well documented.   \n585 Further, we will release our own code for better reproducibility.   \n586 Guidelines:   \n587 \u2022 The answer NA means that the paper does not include experiments.   \n588 \u2022 If the paper includes experiments, a No answer to this question will not be perceived   \n589 well by the reviewers: Making the paper reproducible is important, regardless of   \n590 whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken   \n92 to make their results reproducible or verifiable.   \n93 \u2022 Depending on the contribution, reproducibility can be accomplished in various ways.   \n94 For example, if the contribution is a novel architecture, describing the architecture fully   \n95 might suffice, or if the contribution is a specific model and empirical evaluation, it may   \n96 be necessary to either make it possible for others to replicate the model with the same   \n97 dataset, or provide access to the model. In general. releasing code and data is often   \n98 one good way to accomplish this, but reproducibility can also be provided via detailed   \n99 instructions for how to replicate the results, access to a hosted model (e.g., in the case   \n00 of a large language model), releasing of a model checkpoint, or other means that are   \n01 appropriate to the research performed.   \n02 \u2022 While NeurIPS does not require releasing code, the conference does require all submis  \n03 sions to provide some reasonable avenue for reproducibility, which may depend on the   \n04 nature of the contribution. For example   \n05 (a) If the contribution is primarily a new algorithm, the paper should make it clear how   \n06 to reproduce that algorithm.   \n07 (b) If the contribution is primarily a new model architecture, the paper should describe   \n08 the architecture clearly and fully.   \n09 (c) If the contribution is a new model (e.g., a large language model), then there should   \n10 either be a way to access this model for reproducing the results or a way to reproduce   \n11 the model (e.g., with an open-source dataset or instructions for how to construct   \n12 the dataset).   \n13 (d) We recognize that reproducibility may be tricky in some cases, in which case   \n4 authors are welcome to describe the particular way they provide for reproducibility.   \n15 In the case of closed-source models, it may be that access to the model is limited in   \n16 some way (e.g., to registered users), but it should be possible for other researchers   \n17 to have some path to reproducing or verifying the results.   \n18 5. Open access to data and code   \n19 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n20 tions to faithfully reproduce the main experimental results, as described in supplemental   \n21 material?   \n22 Answer: [Yes]   \n23 Justification: We attach the code in addition to the supplementary material. All datasets   \n24 except UrbanCars are open source. The code to generate UrbanCars, however, is open   \n25 source.   \n26 Guidelines:   \n27 \u2022 The answer NA means that paper does not include experiments requiring code.   \n28 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n29 public/guides/CodeSubmissionPolicy) for more details.   \n30 \u2022 While we encourage the release of code and data, we understand that this might not be   \n31 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n32 including code, unless this is central to the contribution (e.g., for a new open-source   \n33 benchmark).   \n34 \u2022 The instructions should contain the exact command and environment needed to run to   \n35 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n36 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n37 \u2022 The authors should provide instructions on data access and preparation, including how   \n38 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n39 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n40 proposed method and baselines. If only a subset of experiments are reproducible, they   \n41 should state which ones are omitted from the script and why.   \n42 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n43 versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the   \n45 paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "646 6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "647 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n648 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n649 results?   \n650 Answer: [Yes]   \n651 Justification: All training details and hyperparameters are included in the main paper as well   \n652 as the supplementary.   \n653 Guidelines:   \n654 \u2022 The answer NA means that the paper does not include experiments.   \n655 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n656 that is necessary to appreciate the results and make sense of them.   \n657 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n658 material.   \n659 7. Experiment Statistical Significance   \n660 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n661 information about the statistical significance of the experiments?   \n662 Answer: [Yes]   \n663 Justification: All results reported are the mean of three separate training runs. We include   \n664 error bars in the supplementary material, and omit them from Table 1 in the main paper for   \n665 better readability.   \n666 Guidelines:   \n667 \u2022 The answer NA means that the paper does not include experiments.   \n668 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n669 dence intervals, or statistical significance tests, at least for the experiments that support   \n670 the main claims of the paper.   \n671 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n672 example, train/test split, initialization, random drawing of some parameter, or overall   \n673 run with given experimental conditions).   \n674 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n675 call to a library function, bootstrap, etc.)   \n676 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n677 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n678 of the mean.   \n679 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n680 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n681 of Normality of errors is not verified.   \n682 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n683 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n684 error rates).   \n685 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n686 they were calculated and reference the corresponding figures or tables in the text.   \n687 8. Experiments Compute Resources   \n688 Question: For each experiment, does the paper provide sufficient information on the com  \n689 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n690 the experiments?   \n691 Answer: [Yes]   \n692 Justification: We provide details on the architecture used and the time of execution in the   \n693 supplementary.   \n694 Guidelines:   \n695 \u2022 The answer NA means that the paper does not include experiments.   \n696 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n697 or cloud provider, including relevant memory and storage.   \n698 \u2022 The paper should provide the amount of compute required for each of the individual   \n699 experimental runs as well as estimate the total compute.   \n700 \u2022 The paper should disclose whether the full research project required more compute   \n701 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n702 didn\u2019t make it into the paper).   \n703 9. Code Of Ethics   \n704 Question: Does the research conducted in the paper conform, in every respect, with the   \n705 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n706 Answer: [Yes]   \n707 Justification: We have reviewed and agree to the NeurIPS code of ethics.   \n708 Guidelines:   \n709 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n710 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n711 deviation from the Code of Ethics.   \n712 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n713 eration due to laws or regulations in their jurisdiction).   \n714 10. Broader Impacts   \n715 Question: Does the paper discuss both potential positive societal impacts and negative   \n716 societal impacts of the work performed?   \n717 Answer: [Yes]   \n718 Justification: We discuss the broader impact of our work in the supplementary.   \n719 Guidelines:   \n720 \u2022 The answer NA means that there is no societal impact of the work performed.   \n721 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n722 impact or why the paper does not address societal impact.   \n723 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n724 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n725 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n726 groups), privacy considerations, and security considerations.   \n727 \u2022 The conference expects that many papers will be foundational research and not tied   \n728 to particular applications, let alone deployments. However, if there is a direct path to   \n729 any negative applications, the authors should point it out. For example, it is legitimate   \n730 to point out that an improvement in the quality of generative models could be used to   \n731 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n732 that a generic algorithm for optimizing neural networks could enable people to train   \n733 models that generate Deepfakes faster.   \n734 \u2022 The authors should consider possible harms that could arise when the technology is   \n735 being used as intended and functioning correctly, harms that could arise when the   \n736 technology is being used as intended but gives incorrect results, and harms following   \n737 from (intentional or unintentional) misuse of the technology.   \n738 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n739 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n740 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n741 feedback over time, improving the efficiency and accessibility of ML).   \n742 11. Safeguards   \n743 Question: Does the paper describe safeguards that have been put in place for responsible   \n744 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n745 image generators, or scraped datasets)?   \n746 Answer: [Yes]   \n747 Justification: We perform the necessary manual checks to ensure the safety of our data   \n748 generation process.   \n749 Guidelines:   \n750 \u2022 The answer NA means that the paper poses no such risks.   \n751 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n752 necessary safeguards to allow for controlled use of the model, for example by requiring   \n753 that users adhere to usage guidelines or restrictions to access the model or implementing   \n754 safety filters.   \n755 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n756 should describe how they avoided releasing unsafe images.   \n757 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n758 not require this, but we encourage authors to take this into account and make a best   \n759 faith effort. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "760 12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "761 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n762 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n763 properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] 65 Justification: All original creators/owners of assets are cited in the work. We use CC-BY 66 4.0 license. ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "783 13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "784 Question: Are new assets introduced in the paper well documented and is the documentation   \n785 provided alongside the assets?   \n787 Justification: We share the code to reproduce experiments in our work, and will make them   \n788 open source. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "798 14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "799 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n800 include the full text of instructions given to participants and screenshots, if applicable, as   \n801 well as details about compensation (if any)?   \n802   \n803   \n804   \n805   \n806   \n807   \n808   \n809   \n810   \n811   \n812   \n813   \n814   \n815   \n816   \n817   \n818   \n819   \n820   \n821   \n822   \n823   \n824   \n825   \n826   \n827   \n828   \n829   \n830   \n831 ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] Justification: This work does not involve research with human subjects. Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This work does not involve research with human subjects. Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. \u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. \u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]