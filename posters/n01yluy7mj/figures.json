[{"figure_path": "n01yLUy7Mj/figures/figures_1_1.jpg", "caption": "Figure 1: CLIP maps visual and textual inputs into a joint embedding space, with an information channel expressed in terms of the Mutual Information (MI) between them (a). We interpret the visual features from the vision encoder with multimodal concepts (b) which represent object parts and their corresponding textual description. From the language encoder, we identify points (shown in grey) around the zero-shot prediction (shown in green) as textual descriptions of the predicted class (c). By considering the textual descriptors corresponding to the visual concepts, and the textual descriptors of the language encoder for the predicted class, the two encoders establish a common space of textual concepts allowing us to identify mutual concepts and analyze their shared knowledge (d).", "description": "This figure illustrates the process of interpreting CLIP's zero-shot image classification.  It shows how visual and textual inputs are mapped into a shared embedding space.  (a) depicts the overall process with a mutual information channel. (b) shows visual features from the vision encoder being represented with multimodal concepts (object parts with textual descriptions). (c) highlights points from the language encoder around the predicted class. Finally, (d) demonstrates how common textual concepts are identified in both encoders, revealing shared knowledge.", "section": "1 Introduction"}, {"figure_path": "n01yLUy7Mj/figures/figures_3_1.jpg", "caption": "Figure 2: A high-level overview of our method for deriving visual concepts at the vision encoder (a), querying each visual concept individually from a textual bank to describe the visual concept in natural text (b), and then deriving textual concepts at the language encoder (c). The outputs of (b) and (c) share a common space of fine-grained textual concepts such that mutual information can be better calculated.", "description": "This figure illustrates the process of obtaining multimodal concepts for interpreting CLIP's zero-shot image classification decisions. It shows three main steps: (a) Deriving visual concepts from the vision encoder by applying eigendecomposition or K-means clustering on the image patches, (b) Querying each visual concept from a textual bank to obtain corresponding textual descriptions, and (c) Deriving textual concepts from the language encoder by identifying points around the zero-shot prediction in the embedding space. The final output is a common space of fine-grained textual concepts from both encoders, enabling the calculation of mutual information.", "section": "3 Method"}, {"figure_path": "n01yLUy7Mj/figures/figures_8_1.jpg", "caption": "Figure 3: MI Dynamics curve comparing model families (left) and pretraining datasets (middle). Correlation of AUC with zero-shot classification accuracy is shown right for ViTs and ResNets.", "description": "This figure presents three plots that illustrate the relationship between mutual information (MI), area under the curve (AUC), and zero-shot classification accuracy in CLIP models. The left plot shows MI dynamics curves for different ViT model architectures with the same pretraining data.  The middle plot shows MI dynamics curves for the ViT-B/16 model trained on different datasets.  Finally, the right plot shows the correlation between AUC and zero-shot accuracy for both ViT and ResNet models, demonstrating a positive correlation.", "section": "Experiments and Analysis"}, {"figure_path": "n01yLUy7Mj/figures/figures_8_2.jpg", "caption": "Figure 4: Qualitative examples of multimodal concepts in the vision encoder. The second-top textual descriptor may be omitted to avoid clutter.", "description": "This figure shows four examples of multimodal concepts extracted from the vision encoder. Each example shows an image with different regions highlighted in different colors, each region representing a visual concept. Below each image, there is a list of textual descriptions corresponding to each of the visual concepts. These descriptions are short and concise, making them human-friendly and easily interpretable. The figure aims to showcase how the model extracts and represents fine-grained visual concepts and how these are linked to textual concepts in the vision encoder.", "section": "3.1 Multi-modal Concepts in the Visual Encoder"}, {"figure_path": "n01yLUy7Mj/figures/figures_8_3.jpg", "caption": "Figure 3: MI Dynamics curve comparing model families (left) and pretraining datasets (middle). Correlation of AUC with zero-shot classification accuracy is shown right for ViTs and ResNets.", "description": "This figure presents three graphs illustrating the relationship between mutual information (MI), area under the curve (AUC), and zero-shot classification accuracy across different CLIP models.  The left graph shows MI dynamics across several ViT models with the same pretraining data but varying architecture and patch sizes.  The middle graph shows MI dynamics across different ViT models with varying pretraining data but a fixed architecture. The right graph shows a positive correlation between AUC and zero-shot accuracy for both ViT and ResNet models, indicating stronger shared knowledge leads to higher accuracy.  This demonstrates how different architectural choices and data size affect model performance.", "section": "4 Experiments and Analysis"}, {"figure_path": "n01yLUy7Mj/figures/figures_9_1.jpg", "caption": "Figure 6: Visualizing Vision-Language-Mutual Concepts", "description": "This figure visualizes the mutual concepts learned by both vision and language encoders of CLIP for two examples.  In the first example, mutual concepts are shown to be distinctive for the prediction of cello (e.g., handheld musical instrument, strings stretched across the head, a sound hole), indicating effective representation of the image and class in the joint space. The second example shows that language encoder is stronger than visual encoder at encoding the concept of rattle snake, since it provides related concepts, while the mutual concepts are weaker.  These visualizations highlight how the two encoders learn in common and influence each other.", "section": "5 Conclusion"}, {"figure_path": "n01yLUy7Mj/figures/figures_14_1.jpg", "caption": "Figure 1: CLIP maps visual and textual inputs into a joint embedding space, with an information channel expressed in terms of the Mutual Information (MI) between them (a). We interpret the visual features from the vision encoder with multimodal concepts (b) which represent object parts and their corresponding textual description. From the language encoder, we identify points (shown in grey) around the zero-shot prediction (shown in green) as textual descriptions of the predicted class (c). By considering the textual descriptors corresponding to the visual concepts, and the textual descriptors of the language encoder for the predicted class, the two encoders establish a common space of textual concepts allowing us to identify mutual concepts and analyze their shared knowledge (d).", "description": "This figure illustrates the proposed method for interpreting CLIP's zero-shot image classification. It shows how visual and textual inputs are mapped into a shared embedding space, and how mutual information between the two modalities is used to understand the model's decisions.  The figure is broken down into four parts:\n(a) CLIP's joint embedding space and its information channel.\n(b) Multimodal concepts extracted from the vision encoder.\n(c) Textual descriptions identified from the language encoder around the zero-shot prediction.\n(d) The common space of textual concepts shared by the vision and language encoders, revealing the mutual knowledge learned.", "section": "1 Introduction"}, {"figure_path": "n01yLUy7Mj/figures/figures_15_1.jpg", "caption": "Figure 2: A high-level overview of our method for deriving visual concepts at the vision encoder (a), querying each visual concept individually from a textual bank to describe the visual concept in natural text (b), and then deriving textual concepts at the language encoder (c). The outputs of (b) and (c) share a common space of fine-grained textual concepts such that mutual information can be better calculated.", "description": "This figure illustrates the methodology used to derive visual and textual concepts to calculate mutual information.  Panel (a) shows the process of deriving visual concepts through PCA or k-means clustering on image patches. Panel (b) shows how these visual concepts are described using textual concepts from a pre-defined bank of concepts.  Finally, Panel (c) shows how textual concepts are derived from the language encoder to establish a common ground for MI calculation.", "section": "3 Method"}, {"figure_path": "n01yLUy7Mj/figures/figures_18_1.jpg", "caption": "Figure 4: Qualitative examples of multimodal concepts in the vision encoder. The second-top textual descriptor may be omitted to avoid clutter.", "description": "This figure shows four examples of how the model identifies and describes visual concepts using multimodal concepts. Each image is divided into regions, each with a distinct color representing a different visual concept.  Beneath each image are textual descriptions corresponding to the color-coded regions, providing fine-grained visual and textual descriptions of the object's parts.  These details demonstrate the model's ability to break down object recognition into fine-grained details, going beyond simple high-level interpretations of an object as a whole.", "section": "3.1 Multi-modal Concepts in the Visual Encoder"}, {"figure_path": "n01yLUy7Mj/figures/figures_19_1.jpg", "caption": "Figure 4: The MM-CBM baseline we formulate", "description": "This figure illustrates the architecture of the Multimodal Concept Bottleneck Model (MM-CBM) baseline.  The process starts with encoding the set of textual descriptors (D) using CLIP's language encoder, followed by a linear projection to obtain concept features (Q). These features serve as queries for an attention mechanism.  The image (I) is encoded using CLIP's vision encoder, producing key and value features (K, V).  Cross-attention is then applied, followed by a linear layer (W) to create bottleneck output (U).  Finally, U is fed to a classifier to predict the class.  The visual concepts are obtained by decomposing the prediction into its elements before summation and visual attention is performed on its tokens.", "section": "D Baselines for Concept-based Multimodal Explanations"}, {"figure_path": "n01yLUy7Mj/figures/figures_20_1.jpg", "caption": "Figure 5: Training and Concept Labeling processes of the concept labeler module used in our baseline to label a feature activation map with a textual description", "description": "This figure shows the training and concept labeling process for the concept labeler module used as a baseline in the paper.  The process involves using the CLIP vision encoder to extract feature activation maps. A DropBlock technique is applied to simulate a feature map labeling scenario. Then, a concept labeling process is performed where these features are used to train a classifier to predict textual concepts. The output is a set of concept labels corresponding to these features.", "section": "D Baselines for Concept-based Multimodal Explanations"}, {"figure_path": "n01yLUy7Mj/figures/figures_21_1.jpg", "caption": "Figure 6: Feature activation maps of different neurons of the ViT-B/16. The left-most activation map represents the highest spatial norm. Neuron 689 with the highest norm always encodes the main object, while other neurons encode different concepts", "description": "This figure visualizes feature activation maps from different neurons of a Vision Transformer (ViT-B/16) model.  Each image shows a different neuron's activation map overlaid on the input image. The leftmost image in each row shows the neuron with the highest activation, generally corresponding to the main object in the image. The other neurons within the same row highlight different features or parts of the object, demonstrating that various neurons are specialized for encoding different aspects of the visual input.", "section": "Analyzing Concepts in the Vision Encoder"}, {"figure_path": "n01yLUy7Mj/figures/figures_23_1.jpg", "caption": "Figure 7: Deletion and Insertion steps generated by removing/adding the visual concepts (left) and inspecting how the zero-shot predicted class score changes (right). Last insertion step is removed to avoid clutter. Similarity score is multiplied by 2.5, following [20].", "description": "This figure shows the deletion and insertion curves generated by successively removing and adding the identified visual concepts in the order of importance. The deletion curve shows the class score decreasing as more concepts are removed, while the insertion curve shows the class score increasing as more concepts are added. The similarity score is multiplied by 2.5.", "section": "E Evaluation of Concept-based Multimodal Explanations"}, {"figure_path": "n01yLUy7Mj/figures/figures_25_1.jpg", "caption": "Figure 1: CLIP maps visual and textual inputs into a joint embedding space, with an information channel expressed in terms of the Mutual Information (MI) between them (a). We interpret the visual features from the vision encoder with multimodal concepts (b) which represent object parts and their corresponding textual description. From the language encoder, we identify points (shown in grey) around the zero-shot prediction (shown in green) as textual descriptions of the predicted class (c). By considering the textual descriptors corresponding to the visual concepts, and the textual descriptors of the language encoder for the predicted class, the two encoders establish a common space of textual concepts allowing us to identify mutual concepts and analyze their shared knowledge (d).", "description": "This figure illustrates the process of interpreting CLIP's zero-shot image classification from the perspective of mutual knowledge between vision and language modalities. It shows how visual and textual inputs are mapped into a shared embedding space, and how this space is used to interpret the classification decisions. The figure also illustrates how the authors' approach uses textual concept-based explanations to analyze the shared knowledge and zero-shot predictions.", "section": "1 Introduction"}, {"figure_path": "n01yLUy7Mj/figures/figures_26_1.jpg", "caption": "Figure 4: Qualitative examples of multimodal concepts in the vision encoder. The second-top textual descriptor may be omitted to avoid clutter.", "description": "This figure shows four examples of how the model identifies and extracts multimodal concepts from visual input. Each example visualizes different parts of an object and their corresponding textual concepts.  The textual concepts are fine-grained and descriptive, going beyond general labels to highlight specific visual features. This method helps to break down complex visual features into smaller, more manageable units that contribute to the model's prediction.", "section": "3.1 Multi-modal Concepts in the Visual Encoder"}, {"figure_path": "n01yLUy7Mj/figures/figures_26_2.jpg", "caption": "Figure 6: Visualizing Vision-Language-Mutual Concepts", "description": "This figure visualizes the mutual concepts identified by both vision and language encoders of CLIP for two example zero-shot predictions.  The first example shows mutual concepts that are highly relevant to the prediction of a flute (e.g., musical instrument, mouthpiece).  The second example depicts mutual concepts that are less specific and relevant to the prediction of a moped (e.g., two-wheeled vehicle, wheels). This illustrates how the strength of mutual knowledge between the two encoders can vary depending on the specific class and the prediction.", "section": "5 Conclusion"}]