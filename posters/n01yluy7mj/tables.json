[{"figure_path": "n01yLUy7Mj/tables/tables_6_1.jpg", "caption": "Table 1: Evaluation scores of our multimodal explanations compared to the baselines established. All use the same features, model and textual concept bank for fair comparison.", "description": "This table presents a comparison of the performance of the proposed multimodal explanation method against three established baselines.  The evaluation is based on four metrics: Deletion, Insertion, Accuracy Drop, and Accuracy Increase.  These metrics assess the impact of adding or removing concepts on classification accuracy.  The table also indicates whether each method requires training. The results show that the proposed method, even without training, achieves comparable or better performance in certain aspects compared to the other methods that require training.", "section": "4 Experiments and Analysis"}, {"figure_path": "n01yLUy7Mj/tables/tables_6_2.jpg", "caption": "Table 2: Effectiveness and Relevancy of our multimodal concepts in boosting zero-shot accuracy of both ResNet and ViT CLIP models on the ImageNet validation set compared to baselines [36, 43].", "description": "This table presents the zero-shot classification accuracy results on the ImageNet validation set for various CLIP models.  It compares the performance of CLIP models using the proposed multimodal concepts against existing baselines. The table shows the baseline accuracy, the accuracy achieved using the authors' multimodal concepts, and the improvement in accuracy resulting from using the proposed method.  Both ResNet and ViT architectures are included in the comparison.", "section": "4 Experiments and Analysis"}, {"figure_path": "n01yLUy7Mj/tables/tables_7_1.jpg", "caption": "Table 3: MI and AUC scores for different model families using PCA and K-means evaluated on the full ImageNet validation split, along with the pretraining data and Top-1 accuracy.", "description": "This table presents the Mutual Information (MI) and Area Under the Curve (AUC) scores for thirteen different CLIP models.  The models vary in architecture (ViTs, ResNets, ConvNeXTs), size, and pretraining datasets (400M, 1B, 2B).  Both PCA and K-means clustering methods were used for feature extraction. The table also includes the Top-1 accuracy for each model on the ImageNet validation set.  This data allows for analysis of the relationship between model architecture, size, pretraining data, feature extraction method, and the strength of shared knowledge between the vision and language encoders of CLIP as measured by MI and AUC.", "section": "Experiments and Analysis"}, {"figure_path": "n01yLUy7Mj/tables/tables_14_1.jpg", "caption": "Table 1: Ablation studies on CorLoc for different feature facets and decomposition methods. GDC: Graph Decomposition.", "description": "This table presents the results of ablation studies conducted to determine the best approach for extracting prominent image patches.  Three types of visual features were compared: tokens and keys from the last attention layer of the Vision Transformer, and an ensemble of both. Two decomposition methods were used: Graph Decomposition (GDC) and Principal Component Analysis (PCA). The table shows the CorLoc (Correct Localization) metric for each combination of feature type and decomposition method, indicating the percentage of samples where the identified patch accurately represents the ground truth object.  The results highlight the superior performance of GDC, especially when using token features.", "section": "A Ablation Studies"}, {"figure_path": "n01yLUy7Mj/tables/tables_14_2.jpg", "caption": "Table 2: Effectiveness and Relevancy of our multimodal concepts in boosting zero-shot accuracy of both ResNet and ViT CLIP models on the ImageNet validation set compared to baselines [36, 43].", "description": "This table presents the results of an experiment designed to evaluate the effectiveness of the proposed multimodal concepts in improving the zero-shot classification accuracy of CLIP models.  It compares the zero-shot accuracy achieved by using the authors' multimodal concepts against baseline methods from previous work ([36, 43]). The comparison is done separately for ResNet and ViT architectures, showing the improvement in accuracy provided by the multimodal concepts. The 'A' column indicates the increase in accuracy relative to the baseline.", "section": "Experiments and Analysis"}, {"figure_path": "n01yLUy7Mj/tables/tables_15_1.jpg", "caption": "Table 3: MI and AUC scores for different model families using PCA and K-means evaluated on the full ImageNet validation split, along with the pretraining data and Top-1 accuracy.", "description": "This table presents the Mutual Information (MI) and Area Under the Curve (AUC) scores obtained for various CLIP models using both PCA and K-means methods.  The models are categorized into families (ViTs, ResNets, and ConvNeXts), each with variations in architecture, size, and pretraining datasets.  The table also shows the Top-1 accuracy achieved by each model on the ImageNet validation set.  The data helps in understanding the relationship between model characteristics (size, architecture, training data), the MI and AUC values which represent shared knowledge between vision and text encoders, and the resulting zero-shot classification accuracy.", "section": "4 Experiments and Analysis"}, {"figure_path": "n01yLUy7Mj/tables/tables_16_1.jpg", "caption": "Table 4: Results of different LLMs and prompts using ViT-B/16.", "description": "This table presents the ablation study on different prompts and LLMs for generating textual descriptors.  Four prompts (P1-P4) and four LLMs (GPT-3.5, GPT-40-mini, GPT-40, Llama3.1-8B-Instruct) along with an ensemble of GPT-3.5 and GPT-40-mini were tested.  The table shows the zero-shot top-1 and top-5 accuracy, inter-class diversity (InterDiv), and intra-class diversity (IntraDiv) for each combination.  InterDiv measures the diversity of descriptors across classes, while IntraDiv measures the similarity between descriptors within a class (lower is better). The results highlight the impact of prompt engineering and LLM choice on descriptor quality, which affects the downstream image classification performance.", "section": "A Ablation Studies"}, {"figure_path": "n01yLUy7Mj/tables/tables_20_1.jpg", "caption": "Table 1: Evaluation scores of our multimodal explanations compared to the baselines established. All use the same features, model and textual concept bank for fair comparison.", "description": "This table presents the performance comparison between the proposed multimodal explanation method and three established baselines (MM-CBM, MM-ProtoSim, Feature Maps) using four evaluation metrics (Deletion, Insertion, AccDrop, AccInc).  The baselines require training while the proposed method is training-free. The metrics measure the impact on model accuracy of adding or removing concepts. Higher Insertion and AccInc scores, and lower Deletion and AccDrop scores indicate better explanation performance.", "section": "4 Experiments and Analysis"}, {"figure_path": "n01yLUy7Mj/tables/tables_24_1.jpg", "caption": "Table 6: MI and its dynamics (AUC) on the Places365 dataset", "description": "This table presents the mutual information (MI) and Area Under the Curve (AUC) for various ViT models on the Places365 dataset.  The models are grouped by size (Model Size section) and pretraining data (Pretrain Data section). For each model, the top-1 accuracy on the dataset is also provided. This allows for a comparison of the MI and AUC scores in relation to model architecture and the size and quality of pretraining data used.", "section": "Models and Datasets"}, {"figure_path": "n01yLUy7Mj/tables/tables_24_2.jpg", "caption": "Table 7: MI and its dynamics (AUC) on the Food-101 dataset.", "description": "This table presents the Mutual Information (MI) and Area Under the Curve (AUC) for different CLIP models evaluated on the Food-101 dataset.  The models vary in size and pretraining dataset size (1B or 2B images).  The MI quantifies the shared information between the vision and language encoders.  The AUC describes the MI dynamics as concepts are sequentially removed, indicating the robustness of the shared knowledge.  Higher AUC values suggest stronger shared knowledge.", "section": "Models and Datasets"}, {"figure_path": "n01yLUy7Mj/tables/tables_27_1.jpg", "caption": "Table 1: Evaluation scores of our multimodal explanations compared to the baselines established. All use the same features, model and textual concept bank for fair comparison.", "description": "This table presents a comparison of the performance of the proposed multimodal explanations against three baselines on four evaluation metrics: Deletion, Insertion, Accuracy Drop, and Accuracy Increase.  The baselines represent existing single-modality methods adapted to the multimodal setting.  The proposed method does not require training, while the baselines do. The metrics assess how well the explanations identify important features for the model's predictions. Higher Insertion and Accuracy Increase scores are better, while lower Deletion and Accuracy Drop scores are better.", "section": "4 Experiments and Analysis"}]