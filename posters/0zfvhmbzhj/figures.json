[{"figure_path": "0zFVhMBZHJ/figures/figures_1_1.jpg", "caption": "Figure 1: Mixture of Tokens: Each expert receives a unique mixture of tokens in the group. Mixing weights are determined by the controller, which is a fully connected layer (omitted for clarity). For a given token, its update is a linear combination of expert outputs, with the coefficients equal to the token's original mixing weights for each expert.", "description": "The figure illustrates the Mixture of Tokens architecture. Tokens from different examples are grouped, and each group is processed by a set of experts. Each expert receives a weighted mixture of tokens, determined by a controller (not shown for clarity). The output of each expert is a weighted combination that contributes to the final token updates.  This design allows for scaling parameters without proportionally increasing computation.", "section": "3 Mixture of Tokens"}, {"figure_path": "0zFVhMBZHJ/figures/figures_3_1.jpg", "caption": "Figure 2: (Left). Diagram of a standard Feed-Forward layer featured in the Transformer architecture: each token is processed with the same MLP independently of other tokens. (Right). Diagram of a Token Choice layer, where each token decides which expert to choose. In this way, different experts process a different number of tokens. If one expert is chosen by too many tokens, a portion of the tokens is dropped - they receive no update.", "description": "The figure illustrates the difference between a standard Feed-Forward layer in a Transformer and a Token Choice layer in a Mixture of Experts model.  In the standard Feed-Forward layer, each token is processed independently by the same Multi-Layer Perceptron (MLP). In contrast, the Token Choice layer allows each token to choose an expert to process it. This creates sparsity as not all experts are utilized for every token, and if an expert is overloaded, some tokens might be dropped, resulting in unequal treatment of tokens and potential training instability. ", "section": "3 Mixture of Tokens"}, {"figure_path": "0zFVhMBZHJ/figures/figures_5_1.jpg", "caption": "Figure 3: Each group consists of tokens with the same position in a sequence. In this example, the group size is 2. Note that the maximum possible group size equals the size of the batch.", "description": "This figure shows how tokens are grouped in Mixture of Tokens (MoT). Tokens from different sequences are grouped together, and each group contains tokens that occupy the same position within their respective sequences.  The size of these groups influences the number of experts used, maintaining a balance between model size and computational efficiency. The maximum group size is limited by the batch size.", "section": "3 Mixture of Tokens"}, {"figure_path": "0zFVhMBZHJ/figures/figures_7_1.jpg", "caption": "Figure 4: Scaling with respect to the number of parameters. Also featured are the Transformer baseline and an MoT model with a non-learnable, uniform routing strategy.", "description": "The figure shows the evaluation loss for different models during training, plotted against the number of training steps.  The models compared include a standard Transformer-Medium model and several Mixture of Tokens (MoT) models with varying numbers of experts (8, 32, 128).  An additional MoT model with a uniform (non-learnable) routing strategy is also included for comparison.  The plot demonstrates the scaling properties of MoT models, showing that increasing the number of parameters (by increasing the number of experts) consistently improves performance, exceeding that of the standard Transformer model.", "section": "4.2 Scaling Results"}, {"figure_path": "0zFVhMBZHJ/figures/figures_7_2.jpg", "caption": "Figure 5: Scaling with respect to the number of token mixtures.", "description": "The figure shows how the model's evaluation loss changes as the number of token mixtures per expert increases during training.  Different lines represent models with varying numbers of mixtures (1, 8, 16, and 32). The results demonstrate that increasing the number of mixtures consistently improves model performance, as indicated by a lower evaluation loss. This suggests that allowing each expert to process multiple mixtures of tokens enhances the model's expressiveness and learning capabilities.", "section": "4.2 Scaling Results"}, {"figure_path": "0zFVhMBZHJ/figures/figures_8_1.jpg", "caption": "Figure 6: Comparison of MoT and sMoE architectures. An increased number of smaller experts allows MoT to match the performance of the best sMoE model. Due to computational constraints, the models were trained for 100K steps.", "description": "This figure compares the performance of Mixture of Tokens (MoT) against two state-of-the-art sparse Mixture of Experts (sMoE) models: Expert Choice and Token Choice.  The results show that MoT, using a larger number of smaller experts, achieves comparable performance to the best-performing sMoE model.  Due to limitations in computational resources, all models were trained for a reduced number of steps (100K).", "section": "4.3 Comparison with the Transformer and Sparse MoEs"}, {"figure_path": "0zFVhMBZHJ/figures/figures_8_2.jpg", "caption": "Figure 7: Our best MoT model reaches the final loss of the baseline in just 33% of the compute budget.", "description": "This figure shows a comparison of training curves between a standard Transformer-Base model and the best performing Mixture of Tokens (MoT) model from the paper.  The y-axis represents the evaluation loss, and the x-axis represents the number of training steps. The MoT model achieves a similar final loss to the Transformer model but in significantly fewer training steps \u2013 approximately one-third the number of steps. This demonstrates the computational efficiency of the MoT architecture.", "section": "4.2 Scaling Results"}, {"figure_path": "0zFVhMBZHJ/figures/figures_9_1.jpg", "caption": "Figure 8: Transition tuning: The first 150K steps of the model are completed using the Mixture of Tokens architecture. Then, a new Token Choice model is initialized with weights from the MoT model, and the model trains for an additional 15K steps to recover performance. The spike in loss results from the sudden change of architecture.", "description": "This figure shows the training loss curves for transition tuning.  Two models, one medium-sized and one base-sized, are initially trained using the Mixture of Tokens (MoT) architecture for 150,000 steps. Then, these models are converted to Token Choice models (a type of sparse Mixture of Experts architecture). The weights from the MoT models are used to initialize the Token Choice models, and training continues for an additional 15,000 steps.  The spike in loss at around 150,000 steps is attributed to the architecture switch.", "section": "4.4 Transition Tuning"}]