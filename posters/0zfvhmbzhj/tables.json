[{"figure_path": "0zFVhMBZHJ/tables/tables_3_1.jpg", "caption": "Table 1: Comparison of training result loss comparison. MoT performs better in the bfloat16-only setting. Learning rates were separately tuned in lower precision for both EC and MoT. Results are averaged over 3 random seeds.", "description": "This table compares the training loss of MoT-Medium/32E and Expert Choice-Medium/32E models using mixed precision and bfloat16 precision.  It shows that MoT achieves lower loss, especially in the bfloat16-only setting, indicating greater robustness to lower precision training. Learning rates were optimized separately for each model and precision level.", "section": "4.2 Scaling Results"}, {"figure_path": "0zFVhMBZHJ/tables/tables_7_1.jpg", "caption": "Table 1: Comparison of training result loss comparison. MoT performs better in the bfloat16-only setting. Learning rates were separately tuned in lower precision for both EC and MoT. Results are averaged over 3 random seeds.", "description": "This table compares the training loss of MoT-Medium/32E and Expert Choice-Medium/32E models under two different precision settings: mixed precision and bfloat16 only.  The results show that MoT exhibits lower training loss in both settings, and particularly demonstrates superior stability compared to Expert Choice when using bfloat16 precision only.  The comparison highlights MoT's robustness to lower precision training.", "section": "4.3 Comparison with the Transformer and Sparse MoEs"}, {"figure_path": "0zFVhMBZHJ/tables/tables_15_1.jpg", "caption": "Table 2: Training hyperparameters. The table provides example models featured in our experiments. All remaining models can be derived from this table.", "description": "This table lists the hyperparameters used for training different models in the paper's experiments.  It shows the number of experts, expert group size, total parameters, number of blocks, d_model dimension, d_ff dimension (feed-forward network hidden units), and number of attention heads for each model configuration (Transformer-Medium, MoT-Medium/32E, MoT-Medium/32E/8, Transformer-Base, MoT-Base/32E, and MoT-Base/64E/16).  This allows readers to reproduce the experiments.", "section": "A Training Hyperparameters"}, {"figure_path": "0zFVhMBZHJ/tables/tables_15_2.jpg", "caption": "Table 3: Performance of a medium-sized model on downstream benchmarks.", "description": "This table presents the performance results of three different models\u2014Transformer-Medium, MoT-Medium/32E/1, and MoT-Medium/32E/16\u2014on three downstream benchmarks: PIQA, HellaSwag, and ARC-e.  The results demonstrate the improved performance of the MoT models (especially MoT-Medium/32E/16) over the Transformer-Medium baseline in a zero-shot setting.", "section": "4.3 Comparison with the Transformer and Sparse MoEs"}, {"figure_path": "0zFVhMBZHJ/tables/tables_16_1.jpg", "caption": "Table 4: Compute resources used for each experiment. All models were trained on NVIDIA A100 GPUs, with either 40 or 80 GB of RAM.", "description": "This table details the computational resources used for training different models mentioned in the paper.  It shows the GPU RAM, training time, and number of GPUs used for each model.  The models include various sizes of Transformers, MoT (Mixture of Tokens), and sparse MoE (Mixture of Experts) models.", "section": "E Compute Resources"}]