[{"figure_path": "6FTlHaxCpR/figures/figures_1_1.jpg", "caption": "Figure 1: (a) Gaussian flow under different supervision. We model Gaussian flow under the supervision of optical flow and motion flow respectively. The latter can produce a more direct description of object motion, thereby effectively guiding the deformation of 3D Gaussians. (b) The decoupling of optical flow. We decouple the optical flow into motion flow which is only related to object motion and camera flow which is only related to camera motion.", "description": "This figure demonstrates the concept of Gaussian flow and how it's used to guide the deformation of 3D Gaussians in MotionGS.  (a) shows that using motion flow directly as supervision (right) gives better deformation results than using overall optical flow (left) because the motion flow isolates object movement from camera movement. (b) illustrates how MotionGS decouples the optical flow into camera flow (due to camera movement) and motion flow (due to object motion). This separation is crucial for isolating and accurately representing object motion, which is then used to guide the deformation of Gaussians, leading to more accurate motion representation in dynamic scene reconstruction.", "section": "1 Introduction"}, {"figure_path": "6FTlHaxCpR/figures/figures_4_1.jpg", "caption": "Figure 2: The overall architecture of MotionGS. It can be viewed as two data streams: (1) The 2D data stream utilizes the optical flow decoupling module to obtain the motion flow as the 2D motion prior; (2) The 3D data stream involves the deformation and transformation of Gaussians to render the image for the next frame. During training, we alternately optimize 3DGS and camera poses through the camera pose refinement module.", "description": "This figure illustrates the MotionGS framework's architecture.  It shows two main data streams: a 2D stream processing optical flow to extract motion information for guiding Gaussian deformation and a 3D stream handling the deformation and rendering of 3D Gaussians. The figure highlights the optical flow decoupling module, which separates camera and object motion, and the camera pose refinement module, which iteratively optimizes camera poses and 3D Gaussians for improved accuracy.", "section": "4.1 Overall Architecture"}, {"figure_path": "6FTlHaxCpR/figures/figures_5_1.jpg", "caption": "Figure 3: Flow calculation. Figure 4: Pose refinement on iterative training.", "description": "This figure shows two subfigures. The left subfigure (Figure 3) illustrates the calculation of camera flow and motion flow from optical flow. It shows how camera poses and depth are used to calculate camera flow, and how object motion is extracted to obtain motion flow. The right subfigure (Figure 4) illustrates the camera pose refinement module, showing how camera poses are optimized iteratively with 3D Gaussians fixed, and then 3D Gaussians are optimized with camera poses fixed, enhancing the rendering quality and robustness.  This iterative process refines camera poses by alternating between optimizing 3D Gaussians while keeping camera poses fixed and optimizing camera poses while keeping 3D Gaussians fixed.", "section": "4.2 Optical Flow Decoupling Module"}, {"figure_path": "6FTlHaxCpR/figures/figures_8_1.jpg", "caption": "Figure 5: Qualitative comparison on NeRF-DS dataset. Refer to Figure 12 for more scenes.", "description": "This figure shows a qualitative comparison of the results from different methods on the NeRF-DS dataset, focusing on the \"basin\" and \"plate\" scenes.  It visually demonstrates the performance differences between the proposed MotionGS method, the Deformable 3DGS baseline, NeRF-DS, and the original 3DGS. The ground truth is also included for each scene to provide a direct comparison.", "section": "5.2 Results"}, {"figure_path": "6FTlHaxCpR/figures/figures_8_2.jpg", "caption": "Figure 6: Qualitative comparison on HyperNeRF dataset. Refer to Figure 13 for more scenes.", "description": "This figure shows a qualitative comparison of the results from different methods on the HyperNeRF dataset.  The top row shows the ground truth frames of two scenes (Chicken and Broom).  Subsequent rows compare the reconstructions produced by the proposed MotionGS method against the baseline deformable 3DGS method and the original 3DGS method. This visual comparison aims to demonstrate the superior quality of the MotionGS reconstructions in terms of detail, accuracy, and artifact reduction, especially in dynamic scenes with complex movements and irregular motion patterns.", "section": "5.2 Results"}, {"figure_path": "6FTlHaxCpR/figures/figures_8_3.jpg", "caption": "Figure 7: Visualization of all data flows. Each example corresponds to two rows.", "description": "This figure visualizes the data flow in MotionGS.  It shows two examples, each occupying two rows.  The top row of each example displays (from left to right): the current frame's image; the next frame's image; the rendered image (output of MotionGS); and the rendered depth map. The bottom row of each example shows (from left to right): the calculated optical flow; the camera flow (optical flow caused by camera movement only); the motion flow (optical flow caused by object movement only); and the Gaussian flow (the 2D projection of Gaussian deformation). This helps to illustrate how MotionGS decouples optical flow into camera and object motion, using the latter to guide the deformation of 3D Gaussians.", "section": "4.2 Optical Flow Decoupling Module"}, {"figure_path": "6FTlHaxCpR/figures/figures_9_1.jpg", "caption": "Figure 8: Visualization of the camera trajectories optimized by our method and COLMAP.", "description": "This figure compares the camera trajectories estimated by the proposed MotionGS method and the COLMAP method.  The red lines represent the camera trajectories optimized by MotionGS, demonstrating its ability to refine camera poses. The blue dotted lines show the camera trajectories estimated by COLMAP. The comparison visually highlights the improvement in accuracy and robustness achieved by MotionGS, particularly in complex dynamic scenes where COLMAP might struggle due to rapid movements and inaccurate initialization.", "section": "4.3 Camera Pose Refinement Module"}, {"figure_path": "6FTlHaxCpR/figures/figures_14_1.jpg", "caption": "Figure 9: The formulation of Gaussian flow. We first project the point  corresponding to the i-th Gaussian at time t into the canonical Gaussian space, and then reproject this point from the canonical Gaussian space to the i-th Gaussian at time t + 1.", "description": "This figure illustrates the process of calculating Gaussian flow.  First, a point  representing the i-th Gaussian at time *t* is projected into a canonical Gaussian space (centered at (0,0)). Then, this point is reprojected from the canonical space to the location of the i-th Gaussian at time *t+1*. The difference between these two projected points ( ) represents the Gaussian flow for the i-th Gaussian, indicating its movement between the two time steps.", "section": "A.1 Formulation of Gaussian Flow"}, {"figure_path": "6FTlHaxCpR/figures/figures_16_1.jpg", "caption": "Figure 10: Rendered depth from 3D Gaussian splatting (ours) and off-the-shelf monocular depth estimator (MiDas). Our rendered depth has richer details and is scale-aligned with the scene. MiDas rendered depth is usually more smooth and suffers from scale ambiguity.", "description": "This figure compares the rendered depth maps generated by the proposed method and an off-the-shelf monocular depth estimator (MiDaS).  The top row shows the depth maps produced by the proposed method, highlighting richer details and better scale alignment with the actual scene. The bottom row displays the depth maps from MiDaS, which appear smoother but suffer from scale ambiguity, indicating less accurate depth estimation.", "section": "A.3 Ablation Study"}, {"figure_path": "6FTlHaxCpR/figures/figures_17_1.jpg", "caption": "Figure 11: Failure case in DyNeRF dataset. Since the viewpoints are fixed and sparse, neither motion flow nor optical flow can help our method avoid floating artifacts.", "description": "This figure shows a failure case of the MotionGS method on the DyNeRF dataset.  The DyNeRF dataset uses fixed and sparsely sampled viewpoints, which means there is less information available for accurate depth and motion estimation.  Because of this, neither using motion flow nor optical flow as supervision is able to prevent floating artifacts (visual distortions where objects appear to be slightly detached from the background). This highlights a limitation of MotionGS in scenes with limited viewpoints.", "section": "A.5 Limitation"}, {"figure_path": "6FTlHaxCpR/figures/figures_18_1.jpg", "caption": "Figure 12: Qualitative comparison on NeRF-DS dataset per-scene. Compared with the state-of-the-art methods, our method can render more reasonable details, especially on dynamic objects.", "description": "This figure compares the visual results of the proposed MotionGS method with several other state-of-the-art methods on the NeRF-DS dataset.  It shows a qualitative comparison, focusing on several different scenes involving dynamic objects.  Each row represents a different scene from the dataset. The columns showcase the Ground Truth, the results from the proposed MotionGS method, the results from Deformable 3DGS, the results from NeRF-DS and finally, the results from 3DGS.  The comparison highlights the superior visual quality and detail preservation of the MotionGS method, particularly when rendering dynamic elements in the scene.", "section": "5.2 Results"}, {"figure_path": "6FTlHaxCpR/figures/figures_19_1.jpg", "caption": "Figure 6: Qualitative comparison on HyperNeRF dataset. Refer to Figure 13 for more scenes.", "description": "This figure displays a qualitative comparison of the results obtained using different methods on the HyperNeRF dataset. The ground truth frames are compared with the results generated by the proposed MotionGS method, the Deformable 3DGS method, and the 3DGS method.  It visually demonstrates the improved accuracy and details captured by MotionGS, especially in handling complex and dynamic movements. More detailed comparisons for additional scenes are available in Figure 13.", "section": "5.2 Results"}, {"figure_path": "6FTlHaxCpR/figures/figures_20_1.jpg", "caption": "Figure 14: Visualization of all data flows. In order: ground truth of It, ground truth of It+1, rendered image of It, rendered depth of frame It, optical flow, camera flow, motion flow, Gaussian flow.", "description": "This figure shows a visualization of the data flows used in MotionGS. The first two columns show the ground truth images at time t and t+1. The next two columns are the rendered image and depth map at time t, which are inputs to the model. The remaining four columns show the optical flow, camera flow, motion flow, and Gaussian flow.  The figure visually demonstrates the steps involved in decoupling the optical flow into camera and object motion, and how the motion flow is used to guide the deformation of 3D Gaussians.", "section": "4.2 Optical Flow Decoupling Module"}]