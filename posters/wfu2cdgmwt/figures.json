[{"figure_path": "wfU2CdgmWt/figures/figures_8_1.jpg", "caption": "Figure 1: This plot shows the control objective values for different algorithms (Adjoint, SOCM, and Cross-entropy) across multiple dimensions, with error bars indicating the standard deviations. The y-axis is restricted to [0, 0.1] for better visibility of the lower range values; cross-entropy takes value 2.915 \u00b1 0.008 at d = 64.", "description": "This figure compares the performance of three stochastic optimal control algorithms (Adjoint, SOCM, and Cross-entropy) across different dimensions in estimating the control objective, which ideally should be zero. The y-axis shows the control objective estimate, and the x-axis represents the dimensionality of the problem. Error bars indicate standard deviations.  Noticeably, the Cross-entropy method performs significantly worse than the others in higher dimensions.", "section": "Experiments"}, {"figure_path": "wfU2CdgmWt/figures/figures_9_1.jpg", "caption": "Figure 2: Plots of the L2 error incurred by the learned control (left), and the norm squared of the gradient with respect to the parameters \u03b8 of the control (right), for the QUADRATIC ORNSTEIN UHLENBECK (EASY) (top) and (HARD) (bottom) settings and for each IDO loss. Both plots show exponential moving averages.", "description": "This figure compares the performance of different iterative diffusion optimization (IDO) algorithms for stochastic optimal control. The left plots show the L2 error between the learned control and the ground truth control.  The right plots show the squared norm of the gradient of the loss function with respect to the control parameters. The results are shown for two settings: QUADRATIC ORNSTEIN UHLENBECK (EASY) and QUADRATIC ORNSTEIN UHLENBECK (HARD). Exponential moving averages are used to smooth the data.", "section": "4 Experiments"}, {"figure_path": "wfU2CdgmWt/figures/figures_9_2.jpg", "caption": "Figure 3: Plots of the L\u00b2 error of the learned control for the LINEAR ORNSTEIN UHLENBECK and DOUBLE WELL settings.", "description": "This figure presents plots showing the L2 error of the learned control for two different control problems: LINEAR ORNSTEIN UHLENBECK and DOUBLE WELL.  The plots display the L2 error (a measure of how close the learned control is to the optimal control) over the number of training iterations. Each line on the plot represents a different algorithm used to learn the control. The plot allows for a comparison of the performance of different algorithms in high dimensional stochastic optimal control problems.  The DOUBLE WELL problem is particularly interesting as it involves a multimodal solution.", "section": "Experiments"}, {"figure_path": "wfU2CdgmWt/figures/figures_36_1.jpg", "caption": "Figure 4: Plots of the control objective for the four settings.", "description": "This figure presents the control objective for four different experimental settings: Quadratic Ornstein Uhlenbeck (easy), Quadratic Ornstein Uhlenbeck (hard), Linear Ornstein Uhlenbeck, and Double Well.  Each setting's plot shows the control objective over the course of training for several different algorithms, including the authors' proposed SOCM method and various comparative algorithms.  The plots illustrate the performance of each algorithm in terms of minimizing the control objective over time. Error bars indicating standard deviation are shown for some of the plots, giving an estimate of confidence intervals for the control objective estimations.", "section": "Experiments"}, {"figure_path": "wfU2CdgmWt/figures/figures_36_2.jpg", "caption": "Figure 5: Plots of the normalized standard deviation of the importance weights: \u221aVar[\u03b1(u, Xu, B)]/E[\u03b1(u, Xu, B)].", "description": "This figure presents plots illustrating the normalized standard deviation of the importance weights (\u221aVar[\u03b1(u, Xu, B)]/E[\u03b1(u, Xu, B)]) across four different experimental settings: Quadratic Ornstein Uhlenbeck (easy), Quadratic Ornstein Uhlenbeck (hard), Linear Ornstein Uhlenbeck, and Double Well.  The plots show this metric as a function of the number of iterations for several different stochastic optimal control algorithms (SOCM, SOCM with constant Mt = I, SOCM-Adjoint, Adjoint, Cross-Entropy, Log-Variance, Moment, and Variance).  The normalized standard deviation of \u03b1 serves as an indicator of the learned control's proximity to the optimal control; a value of zero indicates optimality. The plots reveal the performance of each algorithm in terms of minimizing the variance of the importance weight, offering insights into their efficiency and stability during training.", "section": "Experiments"}, {"figure_path": "wfU2CdgmWt/figures/figures_37_1.jpg", "caption": "Figure 6: Plots of the norm squared of the gradient for the LINEAR ORNSTEIN UHLENBECK and DOUBLE WELL settings.", "description": "This figure compares the norm squared of the gradient for different stochastic optimal control algorithms across two distinct problem settings: Linear Ornstein Uhlenbeck and Double Well.  The plots showcase the gradient magnitude over the course of the training iterations for each algorithm.  The aim is to illustrate the stability and efficiency of the gradient calculations for each method.  The lower the gradient norm, the smoother the optimization landscape and potentially the more efficient the training process.", "section": "Experiments"}, {"figure_path": "wfU2CdgmWt/figures/figures_37_2.jpg", "caption": "Figure 7: Plots of the control L\u00b2 error, the norm squared of the gradient, and the control objective for the QUADRATIC ORNSTEIN-UHLENBECK (HARD) setting, without using warm-start.", "description": "This figure presents the results of the QUADRATIC ORNSTEIN-UHLENBECK (HARD) experiment, showing the performance of SOCM and other IDO techniques without warm-starting.  The plots display the exponential moving averages (EMA) of the L2 error of the learned control, the norm squared of the gradients, and the control objective over the course of training iterations.  The plots illustrate the comparative performance across different methods and highlight the effectiveness of SOCM in achieving lower error and variance.", "section": "Experiments"}, {"figure_path": "wfU2CdgmWt/figures/figures_38_1.jpg", "caption": "Figure 8: Plots of the training loss for SOCM and its two ablations: SOCM with constant Mt = I, and SOCM-Adjoint.", "description": "This figure presents plots of the training loss for three different stochastic optimal control algorithms: SOCM, SOCM with constant reparameterization matrices (Mt = I), and SOCM-Adjoint.  The training loss is shown for four different control problems: Quadratic Ornstein Uhlenbeck (easy), Quadratic Ornstein Uhlenbeck (hard), Linear Ornstein Uhlenbeck, and Double Well. The plots show how the training loss decreases over the number of iterations for each algorithm and control problem. The plots illustrate the effect of using different loss functions and reparameterization methods on the overall performance of each algorithm.", "section": "Experiments"}, {"figure_path": "wfU2CdgmWt/figures/figures_38_2.jpg", "caption": "Figure 9: Plots of the control L2 error and the norm squared of the gradient for the adjoint method on DOUBLE WELL, for two different values of the Adam learning rate. The instabilities of the adjoint method persist for small learning rates, signaling an inherent issue with the loss.", "description": "This figure shows the control L2 error and the gradient norm squared for the adjoint method applied to the DOUBLE WELL problem, using two different Adam learning rates (3e-5 and 1e-4).  The plots demonstrate that the instabilities observed in the adjoint method are not resolved even when using smaller learning rates, suggesting a fundamental problem within the loss function itself.", "section": "Experiments"}]