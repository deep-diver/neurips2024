[{"figure_path": "EjKNSErSMJ/figures/figures_20_1.jpg", "caption": "Figure 1: Convergence Rate Comparison for the Rock-Paper-Scissors Game", "description": "The figure compares the convergence rate of the generalized Frank-Wolfe algorithm and the extragradient method for solving the Rock-Paper-Scissors game, a classic example of a zero-sum game. The x-axis represents the number of iterations, and the y-axis represents the Nash gap, which measures how far the players are from their best response. Both algorithms achieve a fast last-iterate convergence rate of O(T-1/2), as theoretically proven in the paper. The plot shows that the extragradient method tends to be more stable than the generalized Frank-Wolfe algorithm, especially in the beginning iterations, but their performances are close asymptotically.", "section": "D.1 Generalized Frank-Wolfe"}, {"figure_path": "EjKNSErSMJ/figures/figures_20_2.jpg", "caption": "Figure 1: Convergence Rate Comparison for the Rock-Paper-Scissors Game", "description": "This figure compares the convergence rates of the generalized Frank-Wolfe algorithm and the extragradient method on the Rock-Paper-Scissors game.  The x-axis represents the number of iterations, and the y-axis represents the Nash Gap, a measure of how far the current strategy is from a Nash Equilibrium.  The graph shows that both methods exhibit convergence, but the generalized Frank-Wolfe algorithm initially converges faster before the extragradient method eventually outperforms it asymptotically.", "section": "D.1 Generalized Frank-Wolfe"}, {"figure_path": "EjKNSErSMJ/figures/figures_21_1.jpg", "caption": "Figure 1: Convergence Rate Comparison for the Rock-Paper-Scissors Game", "description": "This figure compares the convergence rates of the Generalized Frank-Wolfe algorithm and the Extragradient method on the Rock-Paper-Scissors game.  The x-axis represents the number of iterations, and the y-axis represents the Nash Gap, a measure of how far the current strategies are from a Nash equilibrium. The plot shows the Nash Gap decreasing over iterations for both algorithms, indicating convergence.  The Generalized Frank-Wolfe algorithm shows faster initial convergence but the Extragradient method seems to be more stable in the long run, converging to a lower Nash Gap. ", "section": "D.1 Generalized Frank-Wolfe"}, {"figure_path": "EjKNSErSMJ/figures/figures_21_2.jpg", "caption": "Figure 4: Convergence of Algorithm 3", "description": "This figure shows the convergence of the stochastic generalized Frank-Wolfe algorithm (Algorithm 3 in the paper) in a stochastic setting. The black line represents the average gap, which is the measure of how far the current iterate is from being a solution to the monotone variational inequality problem.  The grey area represents the standard deviation across multiple runs, illustrating the variability introduced by the stochasticity. The algorithm shows convergence, but not to zero because of the presence of noise in the stochastic setting. The result aligns with Theorem 5.1 which discusses convergence in such scenarios.", "section": "D.2 Stochastic Generalized Frank-Wolfe"}]