[{"figure_path": "up0qqbdeQu/tables/tables_6_1.jpg", "caption": "Table 1: Multi-label recognition with zero-shot methods on MS-COCO [26], VOC2007 [16] and NUS-WIDE [12]. Without training, our method significantly enhances the performance of existing zero-shot methods. The evaluation is based on mAP.", "description": "This table presents the results of multi-label image recognition experiments using zero-shot methods on three benchmark datasets: MS-COCO, VOC2007, and NUS-WIDE.  It compares the performance of two existing zero-shot methods (ZSCLIP and TaI-DPT) with and without the proposed method.  The 'Training-free' column indicates whether the method requires any training. The performance metric used is mean Average Precision (mAP). The numbers in parentheses show the improvement in mAP achieved by adding the proposed method.", "section": "4.2 Evaluation on Zero-Shot Setting"}, {"figure_path": "up0qqbdeQu/tables/tables_6_2.jpg", "caption": "Table 2: Multi-label recognition with 17 unseen classes on MS-COCO [26]. In zero-shot learning (ZSL, recognizing only unseen classes) and generalized ZSL (GZSL, recognizing both seen and unseen classes), our method effectively supplements the complementary information of unseen classes to the supervised DualCoOp[37] on 48 seen classes. The evaluation is based on mAP.", "description": "This table presents the results of multi-label image recognition experiments using the proposed method in a zero-shot learning setting.  The experiments were performed on the MS-COCO dataset, focusing on 17 unseen classes while leveraging 48 seen classes from DualCoOp. The table compares the performance of the proposed method against the DualCoOp method.  The performance is measured using mean average precision (mAP) and broken down for both zero-shot learning (ZSL, only unseen classes) and generalized zero-shot learning (GZSL, both seen and unseen classes). The numbers in parentheses show the improvement achieved by incorporating the proposed method.", "section": "4.2 Evaluation on Limited Data Setting"}, {"figure_path": "up0qqbdeQu/tables/tables_7_1.jpg", "caption": "Table 3: Comparison with few-shot methods on MS-COCO [26]. The evaluation is based on mAP with 16 novel classes. For each shot, we highlighted the best performance in bold.", "description": "This table compares the performance of the proposed training-free method against several few-shot learning methods on the MS-COCO dataset.  The comparison is done using mean average precision (mAP) as the evaluation metric across three experimental settings: 0-shot, 1-shot, and 5-shot.  The results highlight the effectiveness of the proposed method, particularly in the low-data regimes (1-shot and 5-shot), where it significantly outperforms existing few-shot learning approaches. Note that the table also highlights the performance improvements when combining the proposed method with ZSCLIP and TaI-DPT.", "section": "4.2 Evaluation on Limited Data Setting"}, {"figure_path": "up0qqbdeQu/tables/tables_7_2.jpg", "caption": "Table 4: Performance of multi-label recognition based on the partially labeled dataset [26, 16, 12]. Without training and labeled samples, our method consistently enhanced the performance of supervised DualCoOp [37] over all partial label ratio. DualCoOp [37] is reproduced and the evaluation is based on mAP.", "description": "This table presents the results of multi-label image recognition experiments conducted on three benchmark datasets (MS-COCO, VOC2007, and NUS-WIDE) using partially labeled data.  The performance of the DualCoOp method (a supervised method) is compared to the performance of DualCoOp enhanced with the proposed method (DualCoOp+Ours). The results demonstrate that the proposed method consistently improves performance across different levels of partial labeling, even without additional training or labeled samples.", "section": "4.2 Evaluation on Limited Data Setting"}, {"figure_path": "up0qqbdeQu/tables/tables_8_1.jpg", "caption": "Table 5: Effectiveness of our method on MS-COCO [26] and VOC2007 [16]. Each component of our method consistently improves performance, with significant enhancements achieved particularly in context-guided visual feature through narrowing the gap between visual and text features. The evaluation is based on mAP.", "description": "This table shows the ablation study of the proposed method.  It demonstrates that each component (Class concept representation and Context-guided visual feature) contributes to improved performance on the MS-COCO and VOC2007 datasets, as measured by mean Average Precision (mAP). The improvements highlight the effectiveness of aligning visual and textual features for multi-label image recognition.", "section": "4.3 Ablation Study and Analysis"}, {"figure_path": "up0qqbdeQu/tables/tables_8_2.jpg", "caption": "Table 6: Ablation studies in terms of the number of the text descriptions. As increasing the number of texts, we measured the performance of ZSCLIP[32] with our method in mAP on MS-COCO [26] and VOC2007 [16]. Note that ZSCLIP[32] achieves 57.4 mAP and 82.8 mAP for MS-COCO [26] and VOC2007 [16], respectively.", "description": "This table shows the ablation study results on the impact of the number of text descriptions used in the proposed method.  It demonstrates how the performance (measured by mean Average Precision or mAP) of the ZSCLIP model improves as more text descriptions are used, both on the MS-COCO and VOC2007 datasets.  The baseline mAP scores for ZSCLIP on MS-COCO and VOC2007 are provided for context.", "section": "4.3.2 The Number of Text Descriptions"}, {"figure_path": "up0qqbdeQu/tables/tables_12_1.jpg", "caption": "Table 7: Ablation study of hyperparameter searching on validation set. We varied the modulation parameters \u03b1f,t and \u03b1F,t and searched the proper values for context-guided visual feature.", "description": "This table presents the results of an ablation study on the hyperparameters \u03b1f,t and \u03b1F,t used in the context-guided visual feature generation.  Five different combinations of these parameters were tested, and the resulting mAP scores for MS-COCO, VOC2007, and NUS-WIDE datasets are reported.  The best performing combination for each dataset is highlighted in bold.  This table helps in understanding the impact of these parameters on the overall model performance.", "section": "4.3.1 Effectiveness of our method"}]