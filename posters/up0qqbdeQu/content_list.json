[{"type": "text", "text": "Class Concept Representation from Contextual Texts for Training-Free Multi-Label Recognition ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 The power of large vision-language models (VLMs) has been demonstrated for   \n2 diverse vision tasks including multi-label recognition with training-free approach or   \n3 prompt tuning by measuring the cosine similarity between the text features related   \n4 to class names and the visual features of images. Prior works usually formed the   \n5 class-related text features by averaging simple hand-crafted text prompts with class   \n6 names (e.g., \u201ca photo of {class name}\u201d). However, they may not fully exploit the   \n7 capability of VLMs considering how humans form the concepts on words using rich   \n8 contexts with the patterns of co-occurrence with other words. Inspired by that, we   \n9 propose class concept representation for zero-shot multi-label recognition to better   \n10 exploit rich contexts in the massive descriptions on images (e.g., captions from MS  \n11 COCO) using large VLMs. Then, for better aligning visual features of VLMs to our   \n12 class concept representation, we propose context-guided visual representation that   \n13 is in the same linear space as class concept representation. Experimental results   \n14 on diverse benchmarks show that our proposed methods substantially improved   \n15 the performance of zero-shot methods like Zero-Shot CLIP and yielded better   \n16 performance than zero-shot prompt tunings that require additional training like   \n17 TaI-DPT. In addition, our proposed methods can synergetically work with existing   \n18 prompt tuning methods, consistently improving the performance of DualCoOp and   \n19 TaI-DPT in a training-free manner with negligible increase in inference time. ", "page_idx": 0}, {"type": "text", "text": "20 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "21 The goal of multi-label image recognition is to assign all semantic labels (or class names) within an   \n22 image [10, 44, 48, 11, 27, 33, 31]. Differing from single-label recognition, multi-label recognition   \n23 addresses a broader range of practical applications such as image retrieval [36, 39], recommendation   \n24 systems [52, 8], medical diagnosis recognition [43] and retail checkout recognition [17, 45]. However,   \n25 one of the challenges in multi-label recognition is the difficulty of collecting full label annotations,   \n26 which is laborious and prone to missing. To alleviate it, recent works have investigated training with   \n27 incomplete labels such as partial labels [37, 6, 31, 15, 9] or a single positive label [13, 46].   \n28 Recent advances of large vision-language models (VLMs) [32, 2, 22, 25, 47, 49] has demon  \n29 strated their strong transferability on various downstream tasks with great performance. Contrastive   \n30 Language-Image Pretraining (CLIP) achieved impressive performance in zero-shot classification by   \n31 measuring the cosine similarity between images and class-related hand-crafted text prompts [32].   \n32 Fine-tuning VLMs for adapting desired downstream datasets [32] can further improve performance   \n33 for targeted tasks, but tuning millions of parameters is usually undesirable due to computation burden   \n34 and possible forgetting. Prompt tuning has been investigated as an efficient and low-cost training   \n35 paradigm [54, 53], learning only a few context tokens of VLMs for a given task. In multi-label   \n36 recognition, prompt tuning with CLIP has been investigated for distinguishing multiple objects in an   \n37 image [37, 18, 41], mitigating the difficulty of acquiring annotated samples. However, prompt tuning   \n38 inherently requires labeled data with additional training and may be susceptible to overfitting for   \n39 context tokens, hindering generalization. The capability of VLMs for label-free and/or training-free   \n40 classification has been exploited using prompt engineering [32, 34, 50, 4]. However, prompt ensem  \n41 bles by averaging text features from simple hand-crafted prompts (e.g.,\u201ca sketch of {class name}\u201d)   \n42 yielded marginal improvements and struggled with multi-label recognition. Thus, the approach of   \n43 prior works on zero-shot or prompt-tuning based multi-label recognition using class names to obtain   \n44 class-related text features from VLMs may not use the full capacity of VLMs properly.   \n45 Humans form concepts on words from past experience, especially using their patterns of co-occurrence   \n46 with other words [5, 29, 20]. Inspired by this perspective in cognitive neuroscience, we propose a   \n47 novel approach of exploiting VLMs for multi-label recognition by replacing single class name-related   \n48 hand-crafted prompts with our proposed class concept representation using text descriptions such   \n49 as \u201cA person holding a large pair of scissors,\u201d capturing rich contextual information with target   \n50 class names (e.g., person) as well as related words (e.g., holding, scissors). Our class concept will   \n51 be constructed from rich contextual descriptions on classes that may contain diverse and realistic   \n52 patterns of co-occurrence with target class name and other related class names. Then, this novel text   \n53 features with class concept representation requires aligned visual features with them for multi-label   \n54 recognition to properly match them with our class concepts. Thus, we propose context-guided visual   \n55 features to bring VLM\u2019s visual features to the same representation domain as our class concept   \n56 representation by using our sequential attention. See Fig. 1 for the differences of performing multi  \n57 label recognition using (a) prior zero-shot approach (ZS-CLIP), (b) our proposed class concepts from   \n58 text descriptions and (c) our proposed context-guided visual features on the same space as the class   \n59 concepts. We demonstrated that our proposed methods achieved improved performance on multiple   \n60 benchmark datasets without additional training (tuning), without additional labels (text-image pairs)   \n61 and with negligible increase in inference time. Here is the summary of the contributions:   \n62 \u2022 Proposing a novel class concept representation for training-free multi-label recognition tasks   \n63 using VLMs from massive text descriptions inspired by how human forms concept on words.   \n64 \u2022 Proposing a context-guided visual feature, transformed onto the same text feature space as   \n65 class concepts using sequential attention for better aligning multi-modal features.   \n66 \u2022 Demonstrating that our methods synergetically improve the performance of ZSCLIP and   \n67 other state-of-the-art prompt tuning methods with a negligible increase in inference time. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "up0qqbdeQu/tmp/8fcaa8a7ac7d4516fdfbbe2e900d4b56c7fe2a4fdc85d578f7c57f834f29fc8f.jpg", "img_caption": ["Figure 1: Illustration of our methods applied to zero-shot CLIP (ZSCLIP) [32]. $(\\mathbf{a}{\\rightarrow}\\mathbf{b})$ ) Class concept is formed from the text descriptions that contain rich contextual information with relevant class names and other related words, yielding substantially improved performance without aligning with visual features yet. $(\\mathfrak{b}{\\to}\\mathfrak{c})$ Context-guided visual feature is transformed from visual feature so that it is in the same linear space as class concept representation, yielding significantly improved performance. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "68 2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "69 Multi-label image recognition with CLIP. Multi-Label Recognition (MLR) aims to identify all   \n70 semantic labels within an image. However, it is difficult to collect the annotation of multi-label images   \n71 which involve complex scenes and diverse objects. Recently, prompt tuning with the pre-trained vision  \n72 language model CLIP has been developed to address the high labeling costs of multi-label images in   \n73 incomplete label setting. Among them, DualCoOp [37] proposed a novel prompt tuning approach   \n74 that trains positive and negative learnable contexts with class names in the partially labeled setting.   \n75 For mitigating data-limited or label-limited issues, TaI-DPT [18] proposed effective dual-grained   \n76 prompt tuning method using easily accessible text descriptions. It is worth noting that TaI-DPT   \n77 used the same text descriptions as ours not for performing training-free multi-label recognition   \n78 itself, but for label-free prompt tuning by replacing the image features with the contextual text   \n79 features (text as image) under the conventional framework of multi-label recognition with class   \n80 name. SCPNet [14] is designed to leverage the structured semantic prior from CLIP to complement   \n81 deficiency of label supervision for MLR with incomplete labels. CDUL [1] proposed unsupervised   \n82 multi-label recognition through pseudo-labeling using CLIP, alleviating the annotation burden. Even   \n83 though recent works has demonstrated outstanding performance of multi-label recognition task, they   \n84 still require tuning costs or labeled dataset to adapt pre-trained CLIP to various downstream tasks. In   \n85 this work, our method enables training-free and label-free adaptation of CLIP into downstream tasks,   \n86 utilizing the text descriptions.   \n87 Training-free enhancement with CLIP. For single-label recognition, recent works has developed   \n88 the training-free enhancement of CLIP. ZPE [4] weighted-averaged many prompts by automatically   \n89 scoring the importance of each prompt in zero-shot manner for improving prompt ensemble technique.   \n90 CALIP [19] designed a simple parameter-free attention module for zero-shot enhancement over CLIP   \n91 without any tuning of model parameter. With few-shot samples, Tip-Adapter [51] proposed training  \n92 free approach for fast adaptation to target task, obtaining the weights of adapter using few-shot   \n93 samples during inference. Since these methods were originally developed for single-label recognition,   \n94 it is difficult to be directly applied to multi-label recognition. In multi-label recognition, our method   \n95 enables training-free enhancement and demonstrated its effectiveness on the benchmark dataset. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "96 3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "97 First of all, we propose class concept representation as a training-free approach for multi-label   \n98 recognition instead of class name by exploiting pre-trained VLM and rich contextual text descriptions.   \n99 Secondly, we also propose context-guided visual feature that can enhance the alignment of the   \n100 visual feature of VLM with our novel class concept. Our proposed methods are label-free as well as   \n101 training-free so that they can be applicable synergetically for most existing VLM-based multi-label   \n102 recognition methods. The overall pipeline of our method is illustrated in Figure 2. ", "page_idx": 2}, {"type": "text", "text": "103 3.1 Class Concept Representation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "104 Humans form concepts on words from past experience, often using their patterns of co-occurrence   \n105 with other words [5, 29, 20]. For example, the word \u201capple\u201d does not exist alone, but often comes   \n106 with the verb \u201ceat\u201d or the noun \u201cbasket.\u201d However, it may not well associate with other words such   \n107 as \u201cfly\u201d or \u201cspace.\u201d Fortunately, we can easily obtain rich contextual text descriptions from various   \n108 public sources, including captions from benchmark datasets [26, 23, 24, 30], web crawling and large   \n109 language models [38, 7, 40, 28]. These text descriptions do not only contain class names, but also   \n110 include other words like class-related verbs and nouns in real-world contexts.   \n111 Assume that rich contextual text descriptions were gathered from the public sources that include one   \n112 or multiple class names. We denote the set of text descriptions as $Z^{\\hat{a}l l}=\\{z_{1},z_{2},...,z_{M}\\}$ where $z_{i}$   \n113 refers to an individual text description. $M$ denotes the total number of text descriptions across all   \n114 classes. Note that $M$ can be dynamically changed at inference since our proposed method does not   \n115 require additional training, thus can be seen as test-time adaptation. Assuming that the target task   \n116 uses the class names of person, scissors, clock, building and cake, the examples of the contextual text   \n117 descriptions from $Z^{a l l}$ are as follows:   \n118 \u201cA person holding a large pair of scissors.\u201d   \n119 \u201cA clock mounted on top of a building in the city.\u201d   \n120 \u201cHalf of a white cake with coconuts on top.\u201d   \n121 TaI-DPT [18] used these descriptions with rich contextual information as a surrogate for images   \n122 to propose a label-free prompt tuning. In this work, we propose to use these descriptions to form   \n123 concepts on class names to compare with images, so that ways of using them are completely different.   \n124 We define the class concept as a vector in the space constructed by the text descriptions as fol  \n125 lows. Firstly, the linear space $\\mathcal{Z}$ can be constructed by spanning the VLM\u2019s text features from   \n126 all text descriptions $z_{i}$ in $Z^{a l l}$ using the VLM\u2019s text encoder $\\mathcal{E}_{\\mathrm{txt}}(z_{i})\\ \\in\\ R^{1\\times D}$ , leading to   \n127 $\\mathcal{Z}=\\mathrm{span}\\{\\mathcal{E}_{\\mathrm{txt}}(z_{1}),\\mathcal{E}_{\\mathrm{txt}}(z_{2}),\\ldots,\\mathcal{E}_{\\mathrm{txt}}(z_{M})\\}$ . Secondly, we propose the class concept for a target   \n128 class name $c$ as a vector $t_{c}^{c o n c e p t}$ in the space $\\mathcal{Z}$ by defining it as follows: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "image", "img_path": "up0qqbdeQu/tmp/7e330ab4cefaf9ddee5f50392784e67027d3ac4bc6d5f4d4a26840b1970300ea.jpg", "img_caption": ["Figure 2: (a) Overall pipeline of our method. 1) Class concept representation: VLM\u2019s text features from the rich contextual descriptions associated with each class name are used to construct the class concept. 2) Context-guided visual features: VLM\u2019s visual features are sequentially transformed onto the class concept representation space using (b) sequential attention mechanism. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nt_{c}^{c o n c e p t}=\\sum_{i=1}^{M}w_{c,i}\\mathbb{1}_{c}(z_{i})\\mathcal{E}_{\\mathrm{txt}}(z_{i})\\in R^{1\\times D}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "129 where $\\mathbb{1}_{c}(z_{i})$ an indicator function such that $\\mathbb{1}_{c}(z_{i})=1$ if the text description $z_{i}$ contains the class   \n130 name $c$ and $\\mathbb{1}_{c}(z_{i})\\,=\\,0$ otherwise. The weight $w_{c,i}$ is assigned to the text feature of each text   \n131 description within a class $c$ and it is assumed to be normalized within the class. In this work, we set   \n132 $\\begin{array}{r}{w_{c,i}=1/\\sum_{j}^{M}\\mathbb{1}_{c}(z_{j})}\\end{array}$ for $\\forall i$ , thus will be the same for all $i$ for each class, which was guided by the   \n133 prior work on prompt ensembling [4], demonstrated that the prompt ensembling with equal weights   \n134 achieved significant performance gains that were comparable to weighted ensembling for single-label   \n135 recognition. Each class concept can be stored individually or together as a matrix.   \n136 Our class concept representation thus consists of various text features including diverse contextual   \n137 information related to the target class name. For instance, the descriptions for the class name \u201cdog\u201d   \n138 should contain the target class name as the following examples of the text descriptions:   \n139   \n140   \n141   \n142 Note that the descriptions include the target class name (bold) as well as other related words in class  \n143 related contexts (underline) as intended. We expect that our novel class concepts will be beneficial for   \n144 multi-label recognition due to other nouns (other class names) as well as other verbs to better explain   \n145 the context where the target class name is used. In this work, we obtain the texts from two sources to   \n146 collect the sufficient contextual text descriptions. The first source is the MS-COCO dataset [26] that is   \n147 publicly available and the second source is large language model(i.e., GPT-3.5[28]) that can generate   \n148 the several sentences quickly if the set of class names related to the target task were provided. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "149 3.2 Context-Guided Visual Feature ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "150 Our novel class concept representation forms new vectors for diverse class names in the linear space   \n151 $\\mathcal{Z}$ instead of the embedding space of the VLM where the text and image encoders were relatively   \n152 well-aligned. Thus, it is expected that the class concept representation and the VLM\u2019s visual feature   \n153 may not be aligned well. Here, we propose context-guided visual feature by transforming the visual   \n154 features of the VLM onto the same space as the class concept representation $\\mathcal{Z}$ by using our sequential   \n155 attention with the text descriptions $\\dot{Z}^{a l l}$ that were used for class concept construction.   \n156 For the target image $q$ and the VLM\u2019s visual encoder $\\mathcal{E}_{\\mathrm{img}}(q)$ , the L2-normalized global visual feature   \n157 $f$ is obtained by using $\\mathcal{E}_{\\mathrm{img}}(q)\\in R^{1\\times D}$ and the flatten local visual feature $F\\in R^{H W\\times D}$ is also   \n158 constructed by using $\\mathcal{E}_{\\mathrm{img}}(\\bar{P_{i,j}}(q))$ where $P_{i,j}(\\cdot)$ is an extractor of the $(i,j)$ th patch of the input image.   \n159 Then, we aim to transform both the global visual feature vector $f$ and the local visual feature matrix $F$   \n160 onto the same linear space $\\mathcal{Z}$ as our class concept representation. One easy way is to \u201cproject\u201d these   \n161 visual features $f$ and $F$ onto the space $\\mathcal{Z}$ by computing the cosine similarity between visual features   \n162 $f$ and the column vectors of $F$ ) and all the text features $t_{i}\\,=\\,\\mathcal{E}_{\\mathrm{txt}}(z_{i})\\,\\,\\stackrel{\\cdot}{\\in}\\,R^{1\\times D},i\\,=\\,1,\\dots,M$   \n163 that constructed $\\mathcal{Z}$ . Unfortunately, when the softmax function is applied to the cosine similarity   \n164 values, they tend to become similar, thus weigh both relevant and irrelevant texts almost equally   \n165 as illustrated in Figure 3 (a). To address this challenge, we propose sequential attention, applying   \n166 the softmax function to part of the cosine similarity values by dividing them into $G$ groups. For the   \n167 text feature matrix $T=\\mathbf{\\dot{[}}t_{1}\\;\\;t_{2}\\;\\;\\dots\\;\\;t_{M}]\\in R^{M\\times\\bar{D}}$ , let us determine $M_{i}$ for $i=1,\\dots,G$ such that   \n168 $M=\\Pi_{i=1}^{G}M_{i}$ and reshape the text feature matrix to be $T\\,\\in\\,R^{M_{1}\\times\\dots\\times M_{G}\\times D}$ . Then, propose to   \n169 sequentially apply the following attention process for $G$ iterations for estimating both global and   \n170 local context-guided visual features $\\boldsymbol{v}^{(k)}$ and $V^{(k)}$ , respectively, at the $k$ th iteration: ", "page_idx": 3}, {"type": "image", "img_path": "up0qqbdeQu/tmp/98126f8edbf6af73399d934343e8ee8b1299429bcdd27950736231bede7a4bc1.jpg", "img_caption": ["Figure 3: Softmax values can be used to weigh the relevance with the given image. However, (a) naive attention mechanisms yielded almost equal softmax values, thus may include texts with low relevance. The proposed sequential attention method focuses on a subset of texts most relevant to the test image, thus can transforms visual features to context-guided visual features for multi-label recognition by assigning very high softmax value to the relevant text at index 0 while very low softmax value to the irrelevant text at index 5000. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{v^{(k)}=\\left\\{\\begin{array}{l l}{T}&{\\mathrm{if}\\;k=0,}\\\\ {\\mathrm{Softmax}_{\\mathrm{dim}_{k}}\\left(\\frac{f\\left(v^{(k-1)}\\right)^{t}}{\\alpha_{f}}\\right)v^{(k-1)}}&{\\mathrm{if}\\;k>0,}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "171 ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{V^{(k)}=\\left\\{\\!\\!\\begin{array}{l l}{T}&{\\mathrm{if}\\ k=0,}\\\\ {\\mathrm{Softmax}_{\\mathrm{dim}_{k}}(\\frac{F(V^{(k-1)})^{t}}{\\alpha_{F}})V^{(k-1)}}&{\\mathrm{if}\\ k>0,}\\end{array}\\!\\!\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "172 where $\\alpha_{f}$ and $\\alpha_{F}$ denote the modulation parameters, $\\mathrm{Softmax}_{M_{k}}$ refers to the softmax operation   \n173 applied along the dimension corresponding to $M_{k}$ . In this work, we utilize $\\boldsymbol{v}^{(3)}$ and $V_{(3)}$ to compute   \n174 classification score. The sequential attention process is illustrated in Figure 2 (b). Figure 3 further   \n175 demonstrates that our sequential attention is particularly effective in handling massive text descriptions.   \n176 Without sequential attention, weighted averaging essentially becomes equal averaging. ", "page_idx": 4}, {"type": "text", "text": "177 3.3 Multi-Label Recognition with Class Concepts ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "178 Architecture of model. Two encoders of CLIP are denoted as $\\mathcal{E}_{\\mathrm{img}}$ and $\\mathcal{E}_{\\mathrm{txt}}$ for the visual encoder   \n179 and text encoder, respectively. Following TaI-DPT [18], we adopt the structure of double-grained   \n180 prompts (DPT), which has been shown effective for enhancing zero-shot multi-label recognition   \n181 performance. To obtain visual representations at both coarse-grained and fine-grained levels, we   \n182 extract the local visual feature map $F=\\mathcal{E}_{\\mathrm{img}}(x)\\in R^{H W\\times D}$ is extracted before attention pooling   \n183 layer, where $H$ and $W$ are spatial dimension of visual feature. After attention pooling layer, we   \n184 obtain the global visual feature $f\\in R^{1\\times D}$ . Similarly, text features $t=\\mathcal{E}_{\\mathrm{txt}}(z)\\in\\dot{R}^{1\\times D}$ are obtained   \n185 by projecting the End-of-Sentence (EOS) token of the text prompt. Thus, we leverage both global   \n186 and local visual features for multi-label recognition.   \n187 Inference. Through our sequential attention, we obtain the context-guided visual features $\\boldsymbol{v}^{(G)}$ and   \n188 $V^{(G)}$ at both global and local levels, respectively. The similarity score $S^{g l o}$ and $S^{l o c}$ are calculated   \n189 between the transformed context-guided visual features $\\boldsymbol{v}^{(G)}$ , $\\dot{V}^{(G)}$ and the class concepts $t_{c}^{c o n c e p t}$   \n190 using the cosine similarity $\\Psi(\\cdot,\\cdot)$ as follows: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{S_{c}^{t o t}=S_{c}^{g l o}+S_{c}^{l o c}=\\Psi(v^{(G)},t_{c}^{c o n c e p t})+\\sum_{j=1}^{H W}\\mathrm{Softmax}(s_{c,j}^{l o c})\\cdot s_{c,j}^{l o c}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "191 where $S_{c}^{t o t}$ is the classification score for the class $c$ and $s_{c,j}^{l o c}=\\Psi([V^{(G)}]_{j},t_{c}^{c o n c e p t})$ for the class $c$   \n192 For obtaining $S_{c}^{l o c}$ , we employ the spatial aggregation over $H W$ [37].   \n193 Finally, we combined ZSCLIP[32] and other prompt tuning methods with our training-free approach   \n194 through simple logit ensemble. In our experiments, we demonstrate the effectiveness of integrating of   \n195 our method with existing methods, thereby boosting the performance of multi-label recognition. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "196 4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "197 4.1 Implementation Details ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "198 Architecture. We empoly CLIP ResNet-50 in the Table. 2 and Table. 3 and ResNet-101 in other   \n199 experiments as the visual encoders and the CLIP transformer as the text encoder for ZSCLIP[32],   \n200 TaI-DPT [18], DualCoOP [37] and our method in the paper. In addition, ZSCLIP[32], TaI-DPT [18]   \n201 and our method are based on the double-grained prompt [18] for both global and local features1.   \n202 Datasets. For evaluation, we performed multi-label recognition experiments on 3 benchmark datasets.   \n203 MS-COCO [26] consists of 80 classes with 82,081 images for training and 40,504 images for test.   \n204 VOC2007[16] consists of 20 object classes with 5,011 image for training and 4,952 images for test.   \n205 NUS-WIDE[12] consists of 81 concepts with 161,789 image for training and 107,859 image for   \n206 test. For MS-COCO [26] and VOC2007 [26], text description source is from MS-COCO [26]. For   \n207 NUS-WIDE[12], we gathered the text descriptions from GPT-3.5. Note that there is example of text   \n208 template for extracting sentence from GPT-3.5 in supplementary.   \n209 Inference Details. In the paper, we set the total number of text descriptions, denoted as $M$ , for   \n210 the MSCOCO[26], VOC2007[16], and NUS-WIDE[12] at 40,000, 64,000, and 57,600, respectively.   \n211 Note that we prepared the text embeddings of every text descriptions from CLIP text encoder in   \n212 advance. We set values of modulation parameter $\\alpha$ via validation. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "213 4.2 Evaluation on Limited Data Setting ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "214 To evaluate our method, we conducted the experiments in limited data scenarios, including zero-shot   \n215 and few-shot settings for data-limited cases and partially labeled setting for label-limited cases. Note   \n216 that only our method provides training-free enhancement of CLIP without tuining cost for multi-label   \n217 recognition. Therefore, our method can be easily combined with existing methods to improve their   \n218 performance.   \n219 Evaluation on Zero-Shot Setting. We performed comparison studies for different zero-shot and fully   \n220 supervised methods in multi-label image recognition. To evaluate the effectiveness of our method   \n221 which, we combined our method with existing zero-shot methods, ZSCLIP[32] and TaI-DPT [18],   \n222 for zero-shot setting, as shown in Table 1. Additionally, we utilized the fully supervised method,   \n223 DualCoOp[37] with our method, for zero-shot learning setting (ZSL) as presented in Table 2.   \n224 Table 1 summarizes the results of the zero-shot experiment on benchmark datasets. In MS-COCO [26]   \n225 and VOC2007 [16], TaI-DPT [18] and our method utilized the public language data from MS  \n226 COCO [26]. By applying our method to ZSCLIP[32] and TaI-DPT [18] during inference, we yield   \n227 performance improvements without tuning costs. Especially, the performance of ZSCLIP[32] with ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "up0qqbdeQu/tmp/e5819e8149aeabff0b957047b5b43c2d86b6aa55fa55d1faa979b2418939c911.jpg", "table_caption": ["Table 1: Multi-label recognition with zero-shot methods on MS-COCO [26], VOC2007 [16] and NUS-WIDE [12]. Without training, our method significantly enhances the performance of existing zero-shot methods. The evaluation is based on mAP. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Table 2: Multi-label recognition with 17 unseen classes on MS-COCO [26]. In zero-shot learning (ZSL , recognizing only unseen classes) and generalized ZSL (GZSL, recognizing both seen and unseen classes), our method effectively supplements the complementary information of unseen classes to the supervised DualCoOp[37] on 48 seen classes. The evaluation is based on mAP. ", "page_idx": 6}, {"type": "table", "img_path": "up0qqbdeQu/tmp/910129e17d30f534f3901562b1c228d6045d64216d6c8cc8ea585b4ae9d4d344.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "228 our method is notably enhanced, achieving better and comparable performance to TaI-DPT [18],   \n229 which requires mild tuning. In NUSWIDE [12], we incorporate contextual text descriptions from   \n230 a large language model (GPT-3.5) to validate the potential of utilizing generated texts instead of   \n231 well-curated caption data. With provided class name of NUSWIDE [12], we readily gathered the   \n232 massive set of text descriptions within a short amount of time. TaI-DPT [18] is trained with the   \n233 public caption data from OpenImages[23]. Our method exceeds the performance of ZSCLIP[32] and   \n234 TaI-DPT [18] by a large margin, with improvements of $9.3\\;\\mathrm{mAP}$ and $2.6\\;\\mathrm{mAP};$ respectively.   \n235 Table 2 shows the results of the zero-shot learning setting for unseen classes. In MS-COCO [26],   \n236 we follow the DualCoOp[37] and split the dataset into 48 seen classes and 17 unseen classes.   \n237 The evaluation is conducted in both zero-shot setting (ZSL, recognizing only unseen classes) and   \n238 generalized zero-shot setting (GZSL, recognizing both seen and unseen classes). Based on prompt   \n239 tuning, DualCoOp[37] trains learnable context tokens on 48 seen classes and achieves the state-of-the  \n240 art performance on both ZSL and GZSL. Our method was originally designed to handle novel classes   \n241 (unseen classes) by leveraging text descriptions. As a result, our method significantly improved   \n242 the ZSL and GZSL performance of the supervised DualCoOp[37] by providing complementary   \n243 information. Table 1 and Table 2 demonstrate the effectiveness of our method performing training  \n244 free enhancement of CLIP with only text descriptions that are easily obtained.   \n245 Evaluation on Few-Shot Setting. We performed comparison study with few-shot methods in multi  \n246 label recognition. In TaI-DPT [18], they have investigate to confirm the effectiveness of their zero-shot   \n247 method. Here, we further validate our method, which is zero-shot test-time task adaption without   \n248 tuning costs.   \n249 Table 3 summarizes the results of the few-shot methods on MS-COCO dataset [26], especially using   \n250 1 and 5 shot samples for all classes. While existing few-shot methods [3, 35, 54, 51] demonstrated   \n251 the performance enhancements with an increase of labeled samples, TaI-DPT [18] and our method   \n252 are performed within the zero-shot setting. By applying our method with existing zero-shot methods   \n253 (ZSCLIP[32] and TaI-DPT [18]), we consistently enhance performance, as already demonstrated in a   \n254 zero-shot setting. In the absence of labeled samples and tuning, we achieved comparable performance   \n255 with ML-FSL[35] and better results than other few-shot methods utilizing 5-shot samples.   \n256 Evaluation on Partially Labeled Setting. Due to high costs of annotation in multi-label image   \n257 recognition, training with partially labeled samples [37, 21, 31, 6] has been studied. Following   \n258 DualCoOp [37], we performed the evaluation of partially labeled setting. As shown in Table 4,   \n259 our method supplements the decreased performance of DualCoOp [37] caused by partially labeled   \n260 samples by providing complementary information during inference. Through zero-shot test time task   \n261 adaptation without tuning costs, we consistently enhance the the performance of DualCoOp [37] on   \n262 all benchmark dataset. Furthermore, we achieved the performance of DualCoOp [37] trained with   \n263 $90\\%$ labels by applying our method with DualCoOp trained with $60\\%$ labels from MS-COCO [26],   \n264 $50\\%$ labels from VOC2007 [16], and $70\\%$ labels from NUSWIDE [12]. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "table", "img_path": "up0qqbdeQu/tmp/adcbe45dd812f1d997fde319e2797a40e543b29ef0f7f9226af9011c4a10b526.jpg", "table_caption": ["Table 3: Comparison with few-shot methods on MS-COCO [26]. The evaluation is based on mAP with 16 novel classes. For each shot, we highlighted the best performance in bold. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "up0qqbdeQu/tmp/ce349643100d3fca8c87ab866f40bfa02aa57fae15098b48f06aa2fa31aba8e9.jpg", "table_caption": ["Table 4: Performance of multi-label recognition based on the partially labeled dataset [26, 16, 12]. Without training and labeled samples, our method consistently enhanced the performance of supervised DualCoOp [37] over all partial label ratio. DualCoOp [37] is reproduced and the evaluation is based on mAP. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "265 4.3 Ablation Study and Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "266 4.3.1 Effectiveness of our method ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "267 To verify the effectiveness of components of our method, we conducted an ablation study for analyzing   \n268 our method. As shown in Table 5, we first proposed a novel class concept representation with text   \n269 descriptions by class to ZSCLIP[32]. Since the text descriptions contain the semantic meaning among   \n270 multiple class names and contextual information for multi-label recognition, the alignment between   \n271 visual features of test image and text features are improved compared to the hand-crafted prompts as   \n272 shown in the Fig.1. Thus, the performance is increased by $4.1\\;\\mathrm{mAP}$ and $1.1\\;\\mathrm{mAP}$ on MS-COCO [26]   \n273 and VOC2007 [16], respectively. Then, we performed the context-guided visual feature using a large   \n274 set of text descriptions, $\\dot{Z}^{a l l}$ . Transforming the visual features into same text feature space as our class   \n275 concept representation is essential to minimize the gap between visual feature from task-agnostic   \n276 visual encoder and text features for each class. Constructing context-guided visual feature, our method   \n277 yield remarkable performance gain by $8.5\\;\\mathrm{mAP}$ and $5.3\\;\\mathrm{mAP}$ on MS-COCO [26] and VOC2007 [16],   \n278 respectively. Thus, we effectively designed our method that improves the alignment between visual   \n279 and text features. ", "page_idx": 7}, {"type": "text", "text": "280 4.3.2 The Number of Text Descriptions ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "281 We investigate the effect of the number of text descriptions for our method. As shown in Table 6,   \n282 we evaluated performance by increasing the number of randomly selected text descriptions from 1K   \n283 to 32K texts. With only 1K text descriptions, our method enhances performance by approximately ", "page_idx": 7}, {"type": "text", "text": "Table 5: Effectiveness of our method on MS-COCO [26] and VOC2007 [16]. Each component of our method consistently improves performance, with significant enhancements achieved particularly in context-guided visual feature through narrowing the gap between visual and text features. The evaluation is based on mAP. ", "page_idx": 8}, {"type": "table", "img_path": "up0qqbdeQu/tmp/352b1455c9e45d5d80a10b75aa7e19bfa7a404764a89a6bbb7a5002660b5eea2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "up0qqbdeQu/tmp/40edc9e22a98444cb5b0fde43beeba26b3b8b51fe13556819d2e2041f16962b4.jpg", "table_caption": ["Table 6: Ablation studies in terms of the number of the text descriptions. As increasing the number of texts, we measured the performance of ZSCLIP[32] with our method in mAP on MS-COCO [26] and VOC2007 [16]. Note that ZSCLIP[32] achieves $57.4\\;\\mathrm{mAP}$ and $82.8\\;\\mathrm{mAP}$ for MS-COCO [26] and VOC2007 [16], respectively. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "284 8 mAP on MS-COCO [26] and $5\\ \\mathrm{mAP}$ on VOC2007 [16], respectively. As the number of text   \n285 descriptions ranges from 1K to 32K, the text embeddings of $\\zeta^{a l l^{\\prime}}$ can cover the wider range of test   \n286 dataset, resulting in increased performance gains. For adapting to novel classes during inference, our   \n287 method not only achieves a significant performance improvement with only 1K texts but also further   \n288 enhances performance as the quantity of texts increases. ", "page_idx": 8}, {"type": "text", "text": "289 4.3.3 Analysis of Inference Time ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "290 We analyzed the inference time of our method depending on the number of text descriptions. When   \n291 extracting text embeddings from the text descriptions in advance, we measure the inference time   \n292 as the number of text descriptions increases. ZSCLIP[32], as the baseline model, processes each   \n293 sample for classification in $7.2\\mathrm{ms}$ . When the number of texts increases from 1K to 32K, integrating   \n294 ZSCLIP[32] with our method only increases the inference time by $0.4\u20130.5\\mathrm{ms}$ , with tests conducted on   \n295 the RTX3090. In addition, Our method (6.8GB) requires slightly more memory than ZSCLIP (6.5GB)   \n296 on VOC2007 [16]. Therefore, our method presents a simple and efficient approach for training-free   \n297 enhancement approach at inference. ", "page_idx": 8}, {"type": "text", "text": "298 5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "299 In this paper, we propose a novel class concept representation from massive text descriptions for   \n300 training-free multi-label recognition tasks. Inspired by how humans form concepts based on words,   \n301 as studied in cognitive neuroscience, we replace single class name prompts with the class concept   \n302 representation that capture various patterns of co-occurrence with other words. To further enhance   \n303 alignment between multi-modal features of VLMs, we propose a context-guided visual representation   \n304 that is transformed onto the same linear space as the class concept representation. Remarkably,   \n305 our proposed method outperforms zero-shot prompt tuning methods such as TaI-DPT and achieves   \n306 significant enhancements over ZSCLIP and other state-of-the-art prompt tuning methods without   \n307 requiring parameter tuning or labeled samples, and with minimal inference time overhead.   \n308 Limitations. While our method achieved impressive results with training-free enhancement of CLIP,   \n309 it exhibits limitations. First, a significant performance gap exists compared to prompt tuning methods   \n310 with full samples, like DualCoOp [37]. Second, the computational memory demands of our method   \n311 grow at a faster rate than ZSCLIP[32] as the batch size increases. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "312 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "313 [1] Rabab Abdelfattah, Qing Guo, Xiaoguang Li, Xiaofeng Wang, and Song Wang. Cdul: Clip-driven   \n314 unsupervised learning for multi-label image classification. In ICCV, 2023.   \n315 [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,   \n316 Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for   \n317 few-shot learning. NeurIPS, 2022.   \n318 [3] Amit Alfassy, Leonid Karlinsky, Amit Aides, Joseph Shtok, Sivan Harary, Rogerio Feris, Raja Giryes, and   \n319 Alex M Bronstein. Laso: Label-set operations networks for multi-label few-shot learning. In CVPR, 2019.   \n320 [4] James Urquhart Allingham, Jie Ren, Michael W Dusenberry, Xiuye Gu, Yin Cui, Dustin Tran, Jeremiah Zhe   \n321 Liu, and Balaji Lakshminarayanan. A simple zero-shot prompt weighting technique to improve prompt   \n322 ensembling in text-image models. In ICML, 2023.   \n323 [5] Lawrence W Barsalou. Perceptual symbol systems. Behavioral and brain sciences, 22(4):577\u2013660, 1999.   \n324 [6] Emanuel Ben-Baruch, Tal Ridnik, Itamar Friedman, Avi Ben-Cohen, Nadav Zamir, Asaf Noy, and Lihi   \n325 Zelnik-Manor. Multi-label classification with partial annotations using class-aware selective loss. In CVPR,   \n326 2022.   \n327 [7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind   \n328 Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.   \n329 In NeurIPS, 2020.   \n330 [8] Dolly Carrillo, Vivian F L\u00f3pez, and Mar\u00eda N Moreno. Multi-label classification for recommender systems.   \n331 In Trends in Practical Applications of Agents and Multiagent Systems: 11th International Conference on   \n332 Practical Applications of Agents and Multi-Agent Systems, pages 181\u2013188. Springer, 2013.   \n333 [9] Tianshui Chen, Tao Pu, Hefeng Wu, Yuan Xie, and Liang Lin. Structured semantic transfer for multi-label   \n334 recognition with partial labels. In AAAI, 2022.   \n335 [10] Tianshui Chen, Muxin Xu, Xiaolu Hui, Hefeng Wu, and Liang Lin. Learning semantic-specific graph   \n336 representation for multi-label image recognition. In CVPR, pages 522\u2013531, 2019.   \n337 [11] Zhao-Min Chen, Xiu-Shen Wei, Peng Wang, and Yanwen Guo. Multi-label image recognition with graph   \n338 convolutional networks. In CVPR, 2019.   \n339 [12] Tat-Seng Chua, Jinhui Tang, Richang Hong, Haojie Li, Zhiping Luo, and Yantao Zheng. Nus-wide: a   \n340 real-world web image database from national university of singapore. In CIVR, pages 1\u20139, 2009.   \n341 [13] Elijah Cole, Oisin Mac Aodha, Titouan Lorieul, Pietro Perona, Dan Morris, and Nebojsa Jojic. Multi-label   \n342 learning from single positive labels. In CVPR, 2021.   \n343 [14] Zixuan Ding, Ao Wang, Hui Chen, Qiang Zhang, Pengzhang Liu, Yongjun Bao, Weipeng Yan, and Jungong   \n344 Han. Exploring structured semantic prior for multi label recognition with incomplete labels. In CVPR,   \n345 2023.   \n346 [15] Thibaut Durand, Nazanin Mehrasa, and Greg Mori. Learning a deep convnet for multi-label classification   \n347 with partial labels. In CVPR, 2019.   \n348 [16] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The   \n349 pascal visual object classes (voc) challenge. IJCV, 2010.   \n350 [17] Marian George and Christian Floerkemeier. Recognizing products: A per-exemplar multi-label image   \n351 classification approach. In ECCV, 2014.   \n352 [18] Zixian Guo, Bowen Dong, Zhilong Ji, Jinfeng Bai, Yiwen Guo, and Wangmeng Zuo. Texts as images in   \n353 prompt tuning for multi-label image recognition. In CVPR, 2023.   \n354 [19] Ziyu Guo, Renrui Zhang, Longtian Qiu, Xianzheng Ma, Xupeng Miao, Xuming He, and Bin Cui. Calip:   \n355 Zero-shot enhancement of clip with parameter-free attention. In AAAI, 2023.   \n356 [20] Paul Hoffman, James L. McClelland, and Matthew A. Lambon Ralph. Concepts, control, and context: A   \n357 connectionist account of normal and disordered semantic cognition. Psychological Review, 125(3):293\u2013328,   \n358 Apr.   \n359 [21] Ping Hu, Ximeng Sun, Stan Sclaroff, and Kate Saenko. Dualcoop $^{++}$ : Fast and effective adaptation to   \n360 multi-label recognition with limited annotations. arXiv preprint arXiv:2308.01890, 2023.   \n361 [22] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung,   \n362 Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text   \n363 supervision. In ICML. PMLR, 2021.   \n364 [23] Ivan Krasin, Tom Duerig, Neil Alldrin, Vittorio Ferrari, Sami Abu-El-Haija, Alina Kuznetsova, Hassan   \n365 Rom, Jasper Uijlings, Stefan Popov, Andreas Veit, et al. Openimages: A public dataset for large-scale   \n366 multi-label and multi-class image classification. Dataset available from https://github. com/openimages,   \n367 2017.   \n368 [24] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen,   \n369 Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision   \n370 using crowdsourced dense image annotations. IJCV, 2017.   \n371 [25] Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu, and Junjie   \n372 Yan. Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm.   \n373 arXiv preprint arXiv:2110.05208, 2021.   \n374 [26] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r,   \n375 and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014.   \n376 [27] Yongcheng Liu, Lu Sheng, Jing Shao, Junjie Yan, Shiming Xiang, and Chunhong Pan. Multi-label image   \n377 classification via knowledge distillation from weakly-supervised detection. In ACMMM, 2018.   \n378 [28] OpenAI. Gpt-4 technical report, 2023.   \n379 [29] Karalyn Patterson, Peter J Nestor, and Timothy T Rogers. Where do you know what you know? the   \n380 representation of semantic knowledge in the human brain. Nature reviews neuroscience, 8(12):976\u2013987,   \n381 2007.   \n382 [30] Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana   \n383 Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence   \n384 models. In ICCV, 2015.   \n385 [31] Tao Pu, Tianshui Chen, Hefeng Wu, and Liang Lin. Semantic-aware representation blending for multi-label   \n386 image recognition with partial labels. In AAAI, 2022.   \n387 [32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish   \n388 Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from   \n389 natural language supervision. In ICML, 2021.   \n390 [33] Nikolaos Sarafianos, Xiang Xu, and Ioannis A Kakadiaris. Deep imbalanced attribute classification using   \n391 visual attention aggregation. In ECCV, 2018.   \n392 [34] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei   \n393 Xiao. Test-time prompt tuning for zero-shot generalization in vision-language models. Advances in Neural   \n394 Information Processing Systems, 35:14274\u201314289, 2022.   \n395 [35] Christian Simon, Piotr Koniusz, and Mehrtash Harandi. Meta-learning for multi-label few-shot classifica  \n396 tion. In WACV, 2022.   \n397 [36] Josef Sivic and Andrew Zisserman. Video google: Efficient visual search of videos. Toward category-level   \n398 object recognition, pages 127\u2013144, 2006.   \n399 [37] Ximeng Sun, Ping Hu, and Kate Saenko. Dualcoop: Fast adaptation to multi-label recognition with limited   \n400 annotations. NeurIPS, 2022.   \n401 [38] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,   \n402 and Tatsunori B Hashimoto. Alpaca: A strong, replicable instruction-following model. Stanford Center for   \n403 Research on Foundation Models. https://crfm.stanford.edu/2023/03/13/alpaca.html, 2023.   \n404 [39] Ivona Tautkute, Tomasz Trzcin\u00b4ski, Aleksander P Skorupa, \u0141ukasz Brocki, and Krzysztof Marasek. Deep  \n405 style: Multimodal search engine for fashion and interior design. IEEE Access, 2019.   \n406 [40] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay   \n407 Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and   \n408 fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n409 [41] Ao Wang, Hui Chen, Zijia Lin, Zixuan Ding, Pengzhang Liu, Yongjun Bao, Weipeng Yan, and Guiguang   \n410 Ding. Hierarchical prompt learning using clip for multi-label classification with single positive labels. In   \n411 Proceedings of the 31st ACM International Conference on Multimedia, pages 5594\u20135604, 2023.   \n412 [42] Shuai Wang, Daoan Zhang, Zipei Yan, Jianguo Zhang, and Rui Li. Feature alignment and uniformity for   \n413 test time adaptation. In CVPR, 2023.   \n414 [43] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, and Ronald M Summers. Tienet: Text-image embedding   \n415 network for common thorax disease classification and reporting in chest x-rays. In CVPR, 2018.   \n416 [44] Ya Wang, Dongliang He, Fu Li, Xiang Long, Zhichao Zhou, Jinwen Ma, and Shilei Wen. Multi-label   \n417 classification with label graph superimposing. In AAAI, 2020.   \n418 [45] Xiu-Shen Wei, Quan Cui, Lei Yang, Peng Wang, and Lingqiao Liu. Rpc: A large-scale retail product   \n419 checkout dataset. arXiv preprint arXiv:1901.07249, 2019.   \n420 [46] Ning Xu, Congyu Qiao, Jiaqi Lv, Xin Geng, and Min-Ling Zhang. One positive label is sufficient:   \n421 Single-positive multi-label learning with label enhancement. NeurIPS, 2022.   \n422 [47] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li,   \n423 Xin Jiang, and Chunjing Xu. Filip: Fine-grained interactive language-image pre-training. arXiv preprint   \n424 arXiv:2111.07783, 2021.   \n425 [48] Vacit Oguz Yazici, Abel Gonzalez-Garcia, Arnau Ramisa, Bartlomiej Twardowski, and Joost van de Weijer.   \n426 Orderless recurrent models for multi-label classification. In CVPR, 2020.   \n427 [49] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong   \n428 Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. arXiv   \n429 preprint arXiv:2111.11432, 2021.   \n430 [50] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and   \n431 Lucas Beyer. Lit: Zero-shot transfer with locked-image text tuning. In Proceedings of the IEEE/CVF   \n432 Conference on Computer Vision and Pattern Recognition, pages 18123\u201318133, 2022.   \n433 [51] Renrui Zhang, Rongyao Fang, Wei Zhang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, and Hong  \n434 sheng Li. Tip-adapter: Training-free clip-adapter for better vision-language modeling. arXiv preprint   \n435 arXiv:2111.03930, 2021.   \n436 [52] Yong Zheng, Bamshad Mobasher, and Robin Burke. Context recommendation using multi-label classi  \n437 fication. In IEEE/WIC/ACM International Joint Conferences on WI and IAT, volume 2, pages 288\u2013295,   \n438 2014.   \n439 [53] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for   \n440 vision-language models. In CVPR, 2022.   \n441 [54] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language   \n442 models. IJCV, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "443 A Generation of Text Descriptions using LLMs ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "444 Our proposed method leverages the text descriptions for enhancing the alignment between the   \n445 visual and text features. In practice, gathering the proper text descriptions is an essential process for   \n446 replacing the hand-crafted prompts. As mentioned in the main paper, the text descriptions can be   \n447 readily gathered from benchmark dataset, web crawling, or large language models. Recent advances   \n448 in large language models (LLMs) enable to rapidly generate text descriptions that are similar to   \n449 image captions in MS-COCO [26]. Therefore, we utilized the generated text descriptions from large   \n450 language model. With provided class name of NUSWIDE [12], Fig. 6 illustrates the example of input   \n451 prompt template and corresponding generated text descriptions using GPT3.5. We carefully designed   \n452 the instruction of input prompt including main description, constraints, examples of bad and good   \n453 cases, class names of target task and output format. ", "page_idx": 11}, {"type": "text", "text": "454 B Implementing Other Zero-Shot Training-Free Method ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "455 In single-label recognition, CALIP [19] proposed zero-shot alignment enhancement of CLIP for adapt  \n456 ing target task without few-shot samples or additional training. The parameter-free attention module of   \n457 cross-modal interaction effectively enhances the alignment of visual and text features. CALIP utilized   \n458 the visual feature $F=\\mathrm{Enc}_{v}(x_{k})\\dot{\\in}R^{H W\\times D}$ via reshaping and the text feature $T=\\mathrm{Enc}_{t}(P^{h})\\!\\in\\!R^{C\\times D}$   \n459 where $P^{h}$ is a hand-crafted description and $C$ denotes the number of classes. The parameter-free   \n460 attention module is formulated as follows: ", "page_idx": 11}, {"type": "table", "img_path": "up0qqbdeQu/tmp/6fa8436dc0d9a69d42a8fe72ff9dfb99e2c3752520f1e905f1f517e6a3a4951c.jpg", "table_caption": ["Table 7: Ablation study of hyperparameter searching on validation set. We varied the modulation parameters $\\alpha_{f,t}$ and $\\alpha_{F,t}$ and searched the proper values for context-guided visual feature. "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{F^{a}}}&{{=}}&{{\\mathrm{Softmax}(A/\\alpha_{t})T,}}\\\\ {{}}&{{}}&{{}}\\\\ {{T^{a}}}&{{=}}&{{\\mathrm{Softmax}(A^{T}/\\alpha_{v})F}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "461 where the attention matrix is $A{=}F T^{T}{\\in}R^{H W\\times C}$ , $\\alpha_{t}$ and $\\alpha_{v}$ are the modulation parameters of textual   \n462 and visual features, respectively, and $T^{a}$ and $F^{a}$ are bidirectionally updated textual and visual features.   \n463 After pooling the updated visual feature $F_{v}^{a}{\\in}R^{1{\\times}D}$ and the global visual feature $F_{v}{\\in}R^{1\\times D}$ , the   \n464 classification logit $S$ is obtained as below: ", "page_idx": 12}, {"type": "equation", "text": "$$\nS=\\beta_{1}\\cdot F_{v}T^{T}+\\beta_{2}\\cdot F_{v}T^{a T}+\\beta_{3}\\cdot F_{v}^{a}T^{T},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "465 where $\\beta_{1},\\beta_{2},\\beta_{3}$ are the weights for the three logits. ", "page_idx": 12}, {"type": "text", "text": "466 CALIP [19] tuned the hyperparameters $\\beta_{2},\\beta_{3}$ for each dataset while fixed $\\beta_{1}$ to be 1 for simplicity.   \n467 As shown in Fig. 4, we have explored the value of $\\beta_{2},\\beta_{3}$ for multi-label recognition setting on   \n468 MS-COCO [26] and have observed that the parameter-free attention module consistently decreases   \n469 the mAP performance since multi-label recognition covers the identification of multiple objects   \n470 within an image, involving complex scene and diverse objects. ", "page_idx": 12}, {"type": "text", "text": "471 C Exploring Modulation Parameters ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "472 For hyperparameter searching, following existing methods for classification tasks, such as zero  \n473 shot [18, 19], training-free [51, 19], and test-time adaptation [42], we explore the modulation pa  \n474 rameters $\\alpha_{t}$ by conducting ablation studies on validation set. For simplicity, we set the value of   \n475 $\\alpha_{f,t}$ to be half of $\\alpha_{F,t}$ . As shown in Table 7, the value of $(1/\\alpha_{f,t},1/\\bar{\\alpha_{F,t}})$ is suitable in the range   \n476 of $(40{\\sim}80{,}20{\\sim}40)$ ). In the experiments of main paper, we set the $(1/\\alpha_{f,t},1/\\alpha_{F,t})$ as (80,40) for   \n477 MS-COCO [26], (60,30) for VOC2007 [16] and (40,20) for NUSWIDE [12]. ", "page_idx": 12}, {"type": "text", "text": "478 D Examples of Local Alignment Enhancement ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "479 In Fig. 5, we visualized the examples of local alignment enhancement by applying our method.   \n480 Enhancing local alignment is important to recognize multiple objects in a test image [37]. Our   \n481 proposed method enhances the local alignment between the visual features of test image and the   \n482 text features of each class name, thereby suppressing the false-positive prediction. Therefore, Fig. 5   \n483 demonstrates the effectiveness of our method. ", "page_idx": 12}, {"type": "text", "text": "484 E Positive and Negative Societal Impacts ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "485 As a positive societal impact, our method can allow people with limited computing resources to   \n486 achieve better performance in multi-label classification using existing vision-language models. This   \n487 is because it does not require extensive training or labeled data. However, as a negative societal   \n488 impact, the failure of classification could produce the negative side effects. For example, in security   \n489 applications, incorrect classification of objects could lead to false alarms or missed detections,   \n490 potentially compromising safety and security. ", "page_idx": 12}, {"type": "image", "img_path": "up0qqbdeQu/tmp/a3b515ffbf6ccaa1487ebd52ccf5060f17836db6ec7991defda765782e58a187.jpg", "img_caption": ["Figure 4: Results of hyperparameter searching of CALIP [19] on MS-COCO [26] on $\\beta_{2}$ and $\\beta_{3}$ . Applying the parametric-free attention module of CALIP consistently decreases performance as compared to the zero-shot CLIP (ZSCLIP) [32]. "], "img_footnote": [], "page_idx": 13}, {"type": "image", "img_path": "up0qqbdeQu/tmp/4c277fc099b1bacf44c06abfb6feae1fd855c76352b89bd7c685070c58a6b2d5.jpg", "img_caption": ["Figure 5: Additional examples of local alignment enhancement via our method. We visualized the test image in the left column and its corresponding spatial similarity map of each class name in the right column. The yellow and red boxes refer to the bounding boxes for different labels in a multi-label setting. By applying our method, the local alignment is enhanced across multiple objects in a test image, thereby suppressing false-positive predictions. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Such as MSCOCO caption, generates the diverse 100 sentences including following class names for multi-label classification caption. The sentence should include proper context and multiple classes in a sentence. ", "page_idx": 15}, {"type": "text", "text": "Constraints: ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "1.Must be in JSON file format.   \n2.No code is required.   \n3.Sentence length should be between 40-50 characters.   \n4.Sentences must start with a lowercase letter.   \n5.Each sentence must contain at least one class name or more.   \n6.Must describe a scene that is likely to occur in reality.   \n7.Do not start the subject with 'I'.   \n8. Sentences must be visually descriptive.   \n9.Avoid abstract descriptions and ensure sentences depict concrete scenes.   \n10.A total of 100 sentences are required.   \n11. must include text of\"{class name}\" for all sentences but it's not need to start   \nwith this text and should be natural sentence. ", "page_idx": 15}, {"type": "text", "text": "Examples: ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "bad example $:$ boats dot the harbor near the sleepy town good example : a cat lounges in the sunny apartment window ", "page_idx": 15}, {"type": "text", "text": "Class names: ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "airport animal beach ... whales window zebra ", "page_idx": 15}, {"type": "text", "text": "Output format : ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "[\"sentence\": \"an airport buzzes with planes and travelers at dawn\"}, {\"sentence\": \"surf crashes onto the beach as swimmers look on\"\u2032, {\"sentence\": \"a person reads a book in the quiet public library\", {\"sentence\": \"tigers roam the rocky terrain under the hot sun\"',..] ", "page_idx": 15}, {"type": "text", "text": "Here are 100 sentences that meet your requirements, each containing one or more of the provided class names. These sentences describe various scenes likely to occur in reality: ", "page_idx": 15}, {"type": "text", "text": "[ {\"sentence\": \"a dog runs through the park chasing a frisbee\"}, {\"sentence\": \"birds soar high in the clear blue sky\"}, {\"sentence\": \"surfers catch waves at the crowded beach\",...] ", "page_idx": 15}, {"type": "image", "img_path": "up0qqbdeQu/tmp/f9a9673a88d70082ce9c868c68e2422c13d93fe2cab534e568a09fdc45966ed9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 6: Example of text description generation using GPT3.5 for contextual text descriptions of NUSWIDE [12]. We carefully designed the input prompt to ensure that the generated sentences include the class name of the target task. The elements considered in designing the input prompt include the main description, constraints, examples, class names, and the desired output format. ", "page_idx": 15}, {"type": "text", "text": "491 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "493 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n494 paper\u2019s contributions and scope?   \n495 Answer: [Yes]   \n496 Justification: We clearly state our contributions in both the abstract and introduction. Espe  \n497 cially, we summarize our contributions in the last part of the introduction.   \n498 Guidelines:   \n499 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n500 made in the paper.   \n501 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n502 contributions made in the paper and important assumptions and limitations. A No or   \n503 NA answer to this question will not be perceived well by the reviewers.   \n504 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n505 much the results can be expected to generalize to other settings.   \n506 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n507 are not attained by the paper. ", "page_idx": 16}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "12 Guidelines:   \n13 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n14 the paper has limitations, but those are not discussed in the paper.   \n15 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n16 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n17 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n18 model well-specification, asymptotic approximations only holding locally). The authors   \n19 should reflect on how these assumptions might be violated in practice and what the   \n20 implications would be.   \n21 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n22 only tested on a few datasets or with a few runs. In general, empirical results often   \n23 depend on implicit assumptions, which should be articulated.   \n24 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n25 For example, a facial recognition algorithm may perform poorly when image resolution   \n26 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n27 used reliably to provide closed captions for online lectures because it fails to handle   \n28 technical jargon.   \n29 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n30 and how they scale with dataset size.   \n31 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n32 address problems of privacy and fairness.   \n33 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n34 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n35 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n36 judgment and recognize that individual actions in favor of transparency play an impor  \n37 tant role in developing norms that preserve the integrity of the community. Reviewers   \n38 will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 16}, {"type": "text", "text": "539 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "540 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n541 a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: Our paper does not include theoretical results, assumptions and proof. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 17}, {"type": "text", "text": "555 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We provide the details of the used model, hyperparameters, source of datasets and proposed algorithm for reproducing main experimental results. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 17}, {"type": "text", "text": "598 Answer: [Yes]   \n599 Justification: We provide open access to the code for our proposed method. In our experi  \n600 ments, we utilize a publicly accessible benchmark dataset described in Section ??.   \n601 Guidelines:   \n602 \u2022 The answer NA means that paper does not include experiments requiring code.   \n603 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n604 public/guides/CodeSubmissionPolicy) for more details.   \n605 \u2022 While we encourage the release of code and data, we understand that this might not be   \n606 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n607 including code, unless this is central to the contribution (e.g., for a new open-source   \n608 benchmark).   \n609 \u2022 The instructions should contain the exact command and environment needed to run to   \n610 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n611 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n612 \u2022 The authors should provide instructions on data access and preparation, including how   \n613 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n614 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n615 proposed method and baselines. If only a subset of experiments are reproducible, they   \n616 should state which ones are omitted from the script and why.   \n617 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n618 versions (if applicable).   \n619 \u2022 Providing as much information as possible in supplemental material (appended to the   \n620 paper) is recommended, but including URLs to data and code is permitted.   \n621 6. Experimental Setting/Details   \n622 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n623 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n624 results?   \n625 Answer: [Yes]   \n626 Justification: We provide the details of the experimental setting, including hyperparameters,   \n627 in Sections 4.1 and C. The generation details of the texts used in our method are also   \n628 provided in Section A.   \n629 Guidelines:   \n630 \u2022 The answer NA means that the paper does not include experiments.   \n631 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n632 that is necessary to appreciate the results and make sense of them.   \n633 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n634 material.   \n635 7. Experiment Statistical Significance   \n636 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n637 information about the statistical significance of the experiments?   \n638 Answer: [Yes]   \n639 Justification: We do not report the statistical significance of the experimental results, as our   \n640 method does not rely on statistical variables for inference.   \n641 Guidelines:   \n642 \u2022 The answer NA means that the paper does not include experiments.   \n643 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n644 dence intervals, or statistical significance tests, at least for the experiments that support   \n645 the main claims of the paper.   \n646 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n647 example, train/test split, initialization, random drawing of some parameter, or overall   \n648 run with given experimental conditions).   \n649 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n650 call to a library function, bootstrap, etc.)   \n651 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n652 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n653 of the mean.   \n654 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n655 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n656 of Normality of errors is not verified.   \n657 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n658 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n659 error rates).   \n660 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n661 they were calculated and reference the corresponding figures or tables in the text.   \n662 8. Experiments Compute Resources   \n663 Question: For each experiment, does the paper provide sufficient information on the computer   \n664 resources (type of compute workers, memory, time of execution) needed to reproduce the   \n665 experiments?   \n666 Answer: [Yes]   \n667 Justification: We provide the information of types of compute worker (GPU model), memory   \n668 usage and inference time in Section. 4.3.3.   \n669 Guidelines:   \n670 \u2022 The answer NA means that the paper does not include experiments.   \n671 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n672 or cloud provider, including relevant memory and storage.   \n673 \u2022 The paper should provide the amount of compute required for each of the individual   \n674 experimental runs as well as estimate the total compute.   \n675 \u2022 The paper should disclose whether the full research project required more compute   \n676 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n677 didn\u2019t make it into the paper).   \n678 9. Code Of Ethics   \n679 Question: Does the research conducted in the paper conform, in every respect, with the   \n680 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n681 Answer: [Yes]   \n682 Justification: We abide by the NeurIPS Code of Ethics.   \n683 Guidelines:   \n684 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n685 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n686 deviation from the Code of Ethics.   \n687 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n688 eration due to laws or regulations in their jurisdiction).   \n689 10. Broader Impacts   \n690 Question: Does the paper discuss both potential positive societal impacts and negative   \n691 societal impacts of the work performed?   \n692 Answer: [Yes]   \n693 Justification: We discuss the positive and negative societal impacts of our paper in the   \n694 supplementary material.   \n695 Guidelines:   \n696 \u2022 The answer NA means that there is no societal impact of the work performed.   \n697 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n698 impact or why the paper does not address societal impact.   \n699 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n700 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n701 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n702 groups), privacy considerations, and security considerations.   \n703 \u2022 The conference expects that many papers will be foundational research and not tied   \n704 to particular applications, let alone deployments. However, if there is a direct path to   \n705 any negative applications, the authors should point it out. For example, it is legitimate   \n706 to point out that an improvement in the quality of generative models could be used to   \n707 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n708 that a generic algorithm for optimizing neural networks could enable people to train   \n709 models that generate Deepfakes faster.   \n710 \u2022 The authors should consider possible harms that could arise when the technology is   \n711 being used as intended and functioning correctly, harms that could arise when the   \n712 technology is being used as intended but gives incorrect results, and harms following   \n713 from (intentional or unintentional) misuse of the technology.   \n714 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n715 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n716 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n717 feedback over time, improving the efficiency and accessibility of ML).   \n718 11. Safeguards   \n719 Question: Does the paper describe safeguards that have been put in place for responsible   \n720 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n721 image generators, or scraped datasets)?   \n722 Answer: [NA]   \n723 Justification: Our paper does not pose a high risk for misuse in terms of model and dataset.   \n724 Guidelines:   \n725 \u2022 The answer NA means that the paper poses no such risks.   \n726 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n727 necessary safeguards to allow for controlled use of the model, for example by requiring   \n728 that users adhere to usage guidelines or restrictions to access the model or implementing   \n729 safety filters.   \n730 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n731 should describe how they avoided releasing unsafe images.   \n732 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n733 not require this, but we encourage authors to take this into account and make a best   \n734 faith effort.   \n735 12. Licenses for existing assets   \n736 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n737 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n738 properly respected?   \n739 Answer: [Yes]   \n740 Justification: We cite the papers that provide datasets, code and models in the Section. 4.   \n741 Guidelines:   \n742 \u2022 The answer NA means that the paper does not use existing assets.   \n743 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n744 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n745 URL.   \n746 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n747 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n748 service of that source should be provided.   \n749 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n750 package should be provided. For popular datasets, paperswithcode.com/datasets   \n751 has curated licenses for some datasets. Their licensing guide can help determine the   \n752 license of a dataset. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "\u2022 For existing datasets that are re-packaged, both the original license and the license of ", "page_idx": 21}, {"type": "text", "text": "754 the derived asset (if it has changed) should be provided.   \n755 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n756 the asset\u2019s creators.   \n757 13. New Assets   \n758 Question: Are new assets introduced in the paper well documented and is the documentation   \n759 provided alongside the assets?   \n760 Answer: [Yes]   \n761 Justification: We provide the detail documentation for the code in our submission.   \n762 Guidelines:   \n763 \u2022 The answer NA means that the paper does not release new assets.   \n764 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n765 submissions via structured templates. This includes details about training, license,   \n766 limitations, etc.   \n767 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n768 asset is used.   \n769 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n770 create an anonymized URL or include an anonymized zip file.   \n771 14. Crowdsourcing and Research with Human Subjects   \n772 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n773 include the full text of instructions given to participants and screenshots, if applicable, as   \n774 well as details about compensation (if any)?   \n775 Answer: [NA]   \n776 Justification: Our paper does not involve neither crowdsourcing nor research with human   \n777 subjects.   \n778 Guidelines:   \n779 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n780 human subjects.   \n781 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n782 tion of the paper involves human subjects, then as much detail as possible should be   \n783 included in the main paper.   \n784 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n785 or other labor should be paid at least the minimum wage in the country of the data   \n786 collector.   \n787 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n788 Subjects   \n789 Question: Does the paper describe potential risks incurred by study participants, whether   \n790 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n791 approvals (or an equivalent approval/review based on the requirements of your country or   \n792 institution) were obtained?   \n793 Answer: [NA]   \n794 Justification: Our paper does not involve neither crowdsourcing nor research with human   \n795 subjects.   \n796 Guidelines:   \n797 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n798 human subjects.   \n799 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n800 may be required for any human subjects research. If you obtained IRB approval, you   \n801 should clearly state this in the paper.   \n802 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n803 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n804 guidelines for their institution. ", "page_idx": 21}, {"type": "text", "text": "\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 22}]