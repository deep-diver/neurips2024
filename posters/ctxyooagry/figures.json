[{"figure_path": "ctXYOoAgRy/figures/figures_1_1.jpg", "caption": "Figure 1: Ratio of English and non-English tokens among layers given non-English queries.", "description": "This figure shows the ratio of English and non-English tokens across different layers of two large language models (Vicuna-13b-v1.5 and BLOOMZ-7b1) when processing non-English queries.  The heatmaps illustrate how the proportion of English tokens changes across layers.  In both models, a shift towards English-centric representations is observed in the middle layers before reverting to predominantly non-English tokens in the output layers.  This pattern suggests a multilingual workflow where the model initially processes the query in the original language, switches to English for reasoning in intermediate layers, and finally generates a response in the original language.", "section": "1 Introduction"}, {"figure_path": "ctXYOoAgRy/figures/figures_1_2.jpg", "caption": "Figure 2: Our hypothesized multilingual workflow, MWork, converts multilingual queries to English for reasoning in English and generates responses in the original language, demonstrating a layered processing approach.", "description": "This figure illustrates the proposed multilingual workflow (MWork) of large language models (LLMs). It consists of three stages: understanding, task-solving, and generating.  The understanding stage involves converting multilingual inputs into a unified representation (likely English). The task-solving stage is further broken down into two sub-processes: reasoning (using English primarily in the self-attention layers) and knowledge extraction (incorporating multilingual knowledge with feed-forward structures). Finally, the generating stage outputs the response in the original language of the query.", "section": "1 Multilingual Workflow (MWork) of LLMs"}, {"figure_path": "ctXYOoAgRy/figures/figures_5_1.jpg", "caption": "Figure 3: Number of language-specific neurons when processing multilingual queries.", "description": "This figure shows the average number of activated language-specific neurons in Mistral's attention and feed-forward structures when processing various multilingual queries.  The x-axis represents the layer index, ranging from 0 to 31. The y-axis shows the number of language-specific neurons. Two lines are plotted: one for the attention structure and one for the feed-forward structure. The plot reveals that the number of language-specific neurons decreases in the attention structure during the task-solving phase but remains relatively consistent in the feed-forward structure across all layers. This suggests that the model relies more on English for reasoning in the attention structure while using multilingual knowledge from the feed-forward structure for additional context.", "section": "3.2 Verification Experiment Setup"}, {"figure_path": "ctXYOoAgRy/figures/figures_8_1.jpg", "caption": "Figure 4: Enhancement results on high-resource languages, while the number is average among languages.", "description": "This figure displays the performance improvement achieved by fine-tuning language-specific neurons in four high-resource languages (De, Fr, Zh, Es, Ru) across four different multilingual tasks.  The x-axis represents the datasets (MGSM, XQuAD, X-CSQA, XLSum), and the y-axis shows the scores. Different bars in each group represent the results using various training corpus sizes (100, 200, 400, 800 documents). The original performance is also shown as a baseline for comparison.", "section": "3.2 Verification Experiment Setup"}, {"figure_path": "ctXYOoAgRy/figures/figures_13_1.jpg", "caption": "Figure 1: Ratio of English and non-English tokens among layers given non-English queries.", "description": "This figure shows the ratio of English and non-English tokens across different layers of two LLMs (Vicuna-13b-v1.5 and BLOOMZ-7b1) when processing non-English queries.  It illustrates a trend where non-English queries initially have predominantly non-English tokens, but the representation becomes surprisingly English-centric in the middle layers, before reverting to mostly non-English tokens in the final layers. This observation motivated the authors' hypothesis of a three-stage multilingual workflow.", "section": "1 Introduction"}, {"figure_path": "ctXYOoAgRy/figures/figures_14_1.jpg", "caption": "Figure 6: Overlapping ratio of language-specific neurons in BLOOMZ", "description": "This figure visualizes the degree of overlap among language-specific neurons in the BLOOMZ large language model.  It presents two heatmaps: one for the self-attention layers and one for the feed-forward layers. Each cell in the heatmap represents the degree of overlap between the language-specific neurons of two different languages. Darker colors indicate a higher degree of overlap, suggesting that certain neurons may be shared across multiple languages.  The figure demonstrates that in BLOOMZ, in contrast to other models studied in the paper, there is significantly less overlap between the language-specific neurons of different languages. This suggests that BLOOMZ has a more modular multilingual representation compared to other LLMs, with each language relying on a relatively more distinct set of neurons.", "section": "Analysis of Language-Specific Neurons"}, {"figure_path": "ctXYOoAgRy/figures/figures_14_2.jpg", "caption": "Figure 7: Ratio of languages among layers in Chinese Llama given non-English instructions.", "description": "This figure shows the layer-wise distribution of languages in the Chinese Llama model when processing non-English instructions.  It illustrates the proportion of each language's tokens at each layer, revealing how the model handles multilingual inputs and shifts language representation across layers.", "section": "Analysis of Language-Specific Neurons"}]