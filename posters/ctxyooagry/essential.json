{"importance": "This paper is crucial for researchers working with multilingual LLMs because it unveils the inner workings of how these models handle multiple languages.  The findings, especially the **MWork framework and PLND method**, provide valuable tools and insights for improving multilingual capabilities.  This research also opens doors for **further investigation on language-specific neurons** and their role in various tasks, potentially leading to more efficient and effective multilingual models.", "summary": "LLMs surprisingly process multilingual queries via an English-centric intermediate stage before generating responses in the original language, a phenomenon explained by the proposed MWork framework and verified by Parallel Language-specific Neuron Detection (PLND).", "takeaways": ["Large language models (LLMs) process multilingual queries through a three-stage workflow: understanding (multilingual to English), task-solving (English), and generating (English to original language).", "Parallel Language-specific Neuron Detection (PLND) effectively identifies activated neurons for various languages without labeled data.", "Fine-tuning language-specific neurons with a small dataset significantly improves multilingual performance without harming other languages."], "tldr": "Multilingual large language models (LLMs) pose a research challenge due to their complex internal mechanisms. Existing studies primarily focus on English, neglecting crucial insights into multilingual processing. This paper tackles this issue by investigating how LLMs handle multilingualism.  The study found a **surprising three-stage multilingual workflow (MWork): understanding, task-solving, and generating.** The authors propose this framework to explain how LLMs handle multiple languages.  \nThe research introduces a novel method called **Parallel Language-Specific Neuron Detection (PLND)** to identify activated neurons for inputs in different languages. Using PLND, the authors conducted experiments by deactivating language-specific neurons in various layers. They **validate the MWork framework** by observing the impact on LLMs' performance when specific neurons were deactivated, demonstrating that language-specific neurons are indeed crucial for multilingual capabilities.  The authors show how fine-tuning these neurons can improve LLMs' multilingual abilities in a particular language without compromising performance in other languages.", "affiliation": "DAMO Academy, Alibaba Group, Singapore", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "ctXYOoAgRy/podcast.wav"}