[{"figure_path": "PaqJ71zf1M/figures/figures_7_1.jpg", "caption": "Figure 3: Illustration of the proposed framework.", "description": "This figure illustrates the overall framework of the proposed CCL method. It consists of two main parts: the classification part and the contrastive learning part. The classification part utilizes logit rectification of the classifier by class prior estimated with a dual-branch. For the contrastive learning part, the energy score is used to select reliable unlabeled data, which are merged with labeled data for continuous contrastive loss to ensure calibration. Besides, information of labeled data and unlabeled data are used in a decoupled manner while maintaining the constraints of aligning features in the contrastive learning space, thereby forming a smoothed contrastive loss.", "section": "D Illustration of The Proposed Algorithm"}, {"figure_path": "PaqJ71zf1M/figures/figures_8_1.jpg", "caption": "Figure 5: Sensitive analysis of hyperparameters under consistent setting of CIFAR100-LT.", "description": "This figure presents a sensitivity analysis of three key hyperparameters in the CCL model: beta (\u03b2), lambda1 (\u03bb\u2081), and lambda2 (\u03bb\u2082).  Each subplot shows how the top-1 accuracy changes as the value of a specific hyperparameter varies while holding others constant. The x-axes represent the range of values tested for each hyperparameter, and the y-axis displays the corresponding top-1 accuracy. The consistent setting of the CIFAR100-LT dataset was used for this analysis.  The results illustrate the robustness of CCL to changes in these hyperparameters within a reasonable range.", "section": "G In-depth Analysis"}, {"figure_path": "PaqJ71zf1M/figures/figures_16_1.jpg", "caption": "Figure 3: Illustration of the proposed framework.", "description": "The figure illustrates the overall framework of the proposed CCL method.  It shows the dual-branch classifier, logit fusion, and the components for continuous reliable and smoothed pseudo-labels, including data augmentation, energy mask, and label propagation.  The different branches, labeled and unlabeled data processing, and the merging of different loss functions (classification loss, continuous reliable pseudo-label loss, and continuous smoothed pseudo-label loss) are clearly shown.  The figure provides a visual representation of how the different parts of the CCL algorithm interact to learn representations from both labeled and unlabeled data, especially in the long-tailed setting.", "section": "3 CCL: Continuous Contrastive Learning"}, {"figure_path": "PaqJ71zf1M/figures/figures_17_1.jpg", "caption": "Figure 3: Illustration of the proposed framework.", "description": "This figure illustrates the overall framework of the proposed Continuous Contrastive Learning (CCL) method.  It shows the data flow, highlighting the key components: dual-branch classifiers (fs and fb), feature extractors, projection head (g), data augmentation techniques (Aw and As), energy score-based filtering for reliable pseudo-labels, and label propagation for smoothed pseudo-labels.  The figure visualizes how labeled and unlabeled data are processed, and how the balanced classification loss, reliable continuous contrastive loss, and smoothed continuous contrastive loss are integrated to optimize the model's performance.", "section": "3 CCL: Continuous Contrastive Learning"}, {"figure_path": "PaqJ71zf1M/figures/figures_19_1.jpg", "caption": "Figure 5: Sensitive analysis of hyperparameters under consistent setting of CIFAR100-LT.", "description": "This figure shows the sensitivity analysis of three hyperparameters in the CCL model on the CIFAR100-LT dataset under a consistent setting.  Three sub-figures display the effect of varying beta (\u03b2) in the smoothed pseudo-labels loss, lambda1 (\u03bb\u2081) in the total loss, and lambda2 (\u03bb\u2082) in the total loss, respectively, on the top-1 accuracy.  The plots illustrate the robustness of the model's performance to changes in these hyperparameters within a certain range, demonstrating the stability and effectiveness of the proposed approach.", "section": "G In-depth Analysis"}, {"figure_path": "PaqJ71zf1M/figures/figures_20_1.jpg", "caption": "Figure 6: Confusion matrices of the predictions on the test set of CIFAR10-LT.", "description": "This figure displays confusion matrices for both ACR and CCL methods, applied to the CIFAR10-LT dataset under different settings of imbalance ratio (\u03b3l = \u03b3u = 100 and \u03b3l = \u03b3u = 150).  The matrices visually represent the model's performance in correctly classifying images, showing the counts of true positive and false positive classifications for each class. By comparing the matrices, the improvement of CCL over ACR in accurately classifying images (especially those in minority classes) is evident.", "section": "G.3 Confusion matrix"}, {"figure_path": "PaqJ71zf1M/figures/figures_21_1.jpg", "caption": "Figure 7: The precision and recall of pseudo-labels for ACR and CCL on CIFAR100-LT dataset in consistent, uniform, reversed settings.", "description": "This figure shows the precision and recall of pseudo-labels generated by ACR and CCL on the CIFAR100-LT dataset under different settings of labeled and unlabeled data distributions.  The consistent setting implies that both labeled and unlabeled data follow the same long-tailed distribution. The uniform setting means the unlabeled data distribution is uniform, whereas the reversed setting means the unlabeled data has an inverted long-tailed distribution. The figure visually compares the performance of ACR and CCL in terms of precision and recall across different class indexes under various data distribution settings.", "section": "G.4 Precision and recall"}, {"figure_path": "PaqJ71zf1M/figures/figures_21_2.jpg", "caption": "Figure 7: The precision and recall of pseudo-labels for ACR and CCL on CIFAR100-LT dataset in consistent, uniform, reversed settings.", "description": "This figure compares the precision and recall of pseudo-labels generated by ACR and CCL on the CIFAR100-LT dataset under different label distribution scenarios.  It presents six subplots, two for each scenario (consistent, uniform, reversed). Each subplot shows the precision and recall for each of the ten classes (grouped from the original 100), allowing for a direct comparison of the two methods across various conditions of label distribution. This visualization aids in understanding the performance differences between the two methods, and how their ability to generate reliable pseudo-labels varies under different levels of class imbalance and label distribution shifts.", "section": "G.4 Precision and recall"}, {"figure_path": "PaqJ71zf1M/figures/figures_22_1.jpg", "caption": "Figure 9: The t-SNE visualization of the test set for ACR and CCL on CIFAR-10-LT dataset in consistent settings.", "description": "This figure uses t-SNE to visualize the learned representations from ACR and CCL on the CIFAR-10-LT dataset.  The visualizations are shown for two different imbalance ratios (\u03b3\u03b9 = \u03b3\u03c5 = 100 and \u03b3\u03b9 = \u03b3\u03c5 = 150). Each point represents a data point from the test set, and the color indicates its true class label. The red circles highlight areas where the classification boundaries are less well-defined in ACR, indicating that CCL produces more distinct clusters and better separation of classes, particularly for those with imbalanced data representation.", "section": "G.5 Visualization"}]