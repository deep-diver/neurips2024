[{"figure_path": "8XoWofmZkI/figures/figures_4_1.jpg", "caption": "Figure 1: Proof of Theorem 4. GF cannot converge for all points of G: such a convergence would require (zt) to be simultaneously close to both g(yt) and g(-yt), which are far from each other.", "description": "This figure is a visual aid to understanding Theorem 4, which demonstrates that underparameterization (W<d) in gradient flow (GF) learning severely restricts the set of learnable targets.  The figure illustrates that for a set G (homeomorphic to a W-sphere), GF cannot converge for all points in G simultaneously.  This is because convergence would require the GF trajectory to be simultaneously close to two antipodal points, g(yt) and g(-yt), which are necessarily far apart.", "section": "General impossibility results"}, {"figure_path": "8XoWofmZkI/figures/figures_5_1.jpg", "caption": "Figure 2: In Theorem 6, we ensure that the learnable set of targets F contains a multidimensional \"fat\" Cantor set Fo having almost full measure \u03bc. The set Fo has the form Fo = \u2229\u221en=1 \u222aa B(n), where {B(n)}n,a is a nested hierarchy of rectangular boxes in Rd. Here, n is the level of the hierarchy and a is the index of the box within the level.", "description": "This figure is a schematic representation of the construction of a multidimensional \"fat\" Cantor set, denoted as Fo, which is a subset of the learnable targets in Theorem 6 of the paper. This Cantor set is built through a hierarchical process where, at each level n, a set of rectangular boxes B(n) is created.  Each box at level n contains several smaller non-intersecting boxes at level n+1. This process is repeated infinitely, resulting in a Cantor set of almost full measure. The boxes are arranged in a grid-like structure, with the indexing 'a' representing the individual boxes within a level.", "section": "Almost guaranteed learning with two parameters"}, {"figure_path": "8XoWofmZkI/figures/figures_6_1.jpg", "caption": "Figure 3: (a) Stage-wise decomposition of the map \u03a6. The map \u03a6 is defined by its stages \u03a6(n) = \u03a6|un\u2264u\u2264un+1 separated by the level lines l(n) = lun = {(u, v) : u = un} and respective level curves \u03a6(l(n)). Each stage \u03a6(n) deforms the level curve \u03a6(l(n)) in the splitting direction xkn+1 to form the new level curve \u03a6(l(n+1)). A non-exceptional GF trajectory w(t) = (u(t), v(t)) passes through all level lines. The splitting indices kn cycle over the values 1,..., d to ensure convergence w.r.t. each coordinate. (b) Box splitting and curve-box alignment at the stage \u03a6(n). The stage curve \u03a6(l(n)) includes segments l(n) (thick red and blue segments) aligned with respective boxes B(n) of the box hierarchy. On the right, a box B(n) (the big square) is split into 2sn = 6 smaller boxes B(n+1) along the splitting direction xkn. Accordingly, the aligned segment l(n) is transformed into 6 new aligned segments l(n+1) (thick blue). The splitting only affects the coordinates xkn and xkn+1. During the splitting, gaps are left in the direction xkn between the child boxes, and in the direction xkn+1 between the level curve \u03a6(l(n)) and the child boxes, to accommodate convergent GF trajectories. Each non-exceptional GF trajectory w(t) passes through some aligned segments l(n), l(n+1). (c) Transition from \u03a6(l(n)) to \u03a6(l(n+1)) through intermediate level curves \u03a6(lu'), \u03a6(lu*), \u03a6(lu'') (violet). These curves ensure that during the n'th stage, for all targets f = (f1, ..., fd) in the respective box B(n+1), a point \u03a6(w(tn)) having a coordinate kn \u03a6(w(tn)) \u2248 fkn is moved by GF to a point \u03a6(w(tn+1)) with a coordinate kn+1 \u03a6(w(tn+1)) \u2248 fkn+1. The points \u03a6(w(t)) are approximately those closest to f on the respective level curves. To avoid local minima, the level curves \u03a6(lu) at each u must be deformed at each u so as to bring such points closer to f. The desired propagation from \u03a6(w(tn)) to \u03a6(w(tn+1)) can be achieved by first deforming \u03a6(lu) so as to bring \u03a6(w(t)) to the tip of the line \u03a6(lu*) (\u201cgathering sub-stage\u201d), and then extending this tip so as to let \u03a6(w(t)) slip off it at the appropriate position xkn+1 (\u201cspreading sub-stage\u201d).", "description": "This figure shows a detailed illustration of the map \u03a6 used in the proof of Theorem 6. It is broken down into three subfigures:\n(a) Explains the stage-wise decomposition of the map \u03a6, showing how it's built from stages \u03a6(n), level lines l(n), and level curves \u03a6(l(n)). A GF trajectory passes through these levels.\n(b) Illustrates box splitting and alignment with curve segments at stage \u03a6(n). Shows how a parent box B(n) is split into smaller boxes B(n+1), and how aligned segments l(n) are transformed into segments l(n+1).\n(c) Details the transition from \u03a6(l(n)) to \u03a6(l(n+1)) using intermediate curves. It explains how the map ensures that during each stage, a GF trajectory moves towards the target by adjusting coordinates.", "section": "4 Almost guaranteed learning with two parameters"}, {"figure_path": "8XoWofmZkI/figures/figures_6_2.jpg", "caption": "Figure 3: Box splitting and curve-box alignment at the stage \u03a6(n). The stage curve \u03a6(l(n)) includes segments l(n) (thick red and blue segments) aligned with respective boxes B(n) of the box hierarchy. On the right, a box B(n) (the big square) is split into 2sn = 6 smaller boxes B(n+1) along the splitting direction xkn. Accordingly, the aligned segment l(n) is transformed into 6 new aligned segments l(n+1) (thick blue). The splitting only affects the coordinates xkn and xkn+1. During the splitting, gaps are left in the direction xkn between the child boxes, and in the direction xkn+1 between the level curve \u03a6(l(n)) and the child boxes, to accommodate convergent GF trajectories. Each non-exceptional GF trajectory w(t) passes through some aligned segments l(n), l(n+1).", "description": "This figure illustrates the aligned hierarchical decomposition of both parameter and target spaces. Each element of the target hierarchy is served by a corresponding element in the parameter hierarchy. The aligned segments in the parameter space are transformed into new segments aligned with the next level of the target space hierarchy. The gaps between the segments and boxes accommodate convergent GF trajectories.", "section": "Almost guaranteed learning with two parameters"}, {"figure_path": "8XoWofmZkI/figures/figures_6_3.jpg", "caption": "Figure 3: Transition from \u03a6(l(n)) to \u03a6(l(n+1)) through intermediate level curves \u03a6(lu\u2032), \u03a6(lu*), \u03a6(lu\u2032\u2032) (violet). These curves ensure that during the n\u2019th stage, for all targets f = (f1, ..., fd) in the respective box B(n+1), a point \u03a6(w(tn)) having a coordinate kn \u03a6(w(tn)) \u2248 fkn is moved by GF to a point \u03a6(w(tn+1)) with a coordinate kn+1 \u03a6(w(tn+1)) \u2248 fkn+1. The points \u03a6(w(t)) are approximately those closest to f on the respective level curves. To avoid local minima, the level curves \u03a6(lu) at each u must be deformed at each u so as to bring such points closer to f. The desired propagation from \u03a6(w(tn)) to \u03a6(w(tn+1)) can be achieved by first deforming \u03a6(lu) so as to bring \u03a6(w(t)) to the tip of the line \u03a6(lu*) (\u201cgathering sub-stage\u201d), and then extending this tip so as to let \u03a6(w(t)) slip off it at the appropriate position xkn+1 (\u201cspreading sub-stage\u201d).", "description": "This figure illustrates the transition from one stage of the hierarchical map construction to the next. It shows how intermediate level curves are used to guide the gradient flow trajectory towards the target, while avoiding local minima.  The process involves two sub-stages: a 'gathering' stage that moves the trajectory closer to the target along one axis, followed by a 'spreading' stage that refines the approximation along a second axis. The combined effect allows for accurate approximation of the target by the two-parameter model.", "section": "Almost guaranteed learning with two parameters"}, {"figure_path": "8XoWofmZkI/figures/figures_8_1.jpg", "caption": "Figure 4: The curve \u03a6(w) = (sin(w), sin(\u221a2w)) densely fills the square [-1,1]2, but for all targets f except for a set of Lebesgue measure 0 the respective GF trajectory w(t) is trapped at a spurious local minimum so that \u03a6(w(t)) \u2192 f. Corollary 10 shows that this is true for all models (6) with any number of parameters W < d.", "description": "This figure shows a curve in a 2D space defined by the equations x = sin(w) and y = sin(\u221a2w).  The curve densely fills the [-1, 1] x [-1, 1] square. However, for almost all target points 'f' in the square, the gradient flow (GF) trajectory does not converge to 'f'. Instead, the trajectory gets stuck at a local minimum, preventing convergence to the desired target. This illustrates that even when the model's output densely covers the target space, underparameterization can severely limit the ability to learn the targets via gradient flow. Corollary 10 in the paper generalizes this behavior to higher dimensions.", "section": "General impossibility results"}]