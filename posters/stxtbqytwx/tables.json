[{"figure_path": "stXtBqyTWX/tables/tables_13_1.jpg", "caption": "Table 1: Model detail for Language Modeling-Small (LM-Small), Language Modeling (LM) and Machine Translation (MT). Hyperparameters include number of experts (E), MoE layer interval (M), and expert capacity fraction (C). Model parameters include token dimension (TD), hidden dimension (HD), and vocabulary size.", "description": "This table presents the architecture details and hyperparameters for three different language modeling tasks: LM-Small, LM, and MT.  For each task, it shows the model type (Dense or MoE), the total number of parameters (Size), the number of experts (E) and the interval between MoE layers (M) for the MoE models.  Capacity fraction (C) specifies the portion of total capacity used by each expert.  Finally, it also lists architecture parameters such as token dimension (TD), hidden dimension (HD) and vocabulary size.", "section": "3 Mixture-of-Experts Characterization"}, {"figure_path": "stXtBqyTWX/tables/tables_13_2.jpg", "caption": "Table 1: Model detail for Language Modeling-Small (LM-Small), Language Modeling (LM) and Machine Translation (MT). Hyperparameters include number of experts (E), MoE layer interval (M), and expert capacity fraction (C). Model parameters include token dimension (TD), hidden dimension (HD), and vocabulary size.", "description": "This table provides detailed information about the models used in the experiments. It includes the model type (dense or MoE), the model size, the number of experts (E), the MoE layer interval (M), the expert capacity fraction (C), and various model parameters such as token dimension (TD), hidden dimension (HD), and vocabulary size.  These specifications are crucial for understanding and replicating the experimental setup and results.", "section": "3 Mixture-of-Experts Characterization"}, {"figure_path": "stXtBqyTWX/tables/tables_14_1.jpg", "caption": "Table 1: Model detail for Language Modeling-Small (LM-Small), Language Modeling (LM) and Machine Translation (MT). Hyperparameters include number of experts (E), MoE layer interval (M), and expert capacity fraction (C). Model parameters include token dimension (TD), hidden dimension (HD), and vocabulary size.", "description": "This table provides detailed information about the model configurations used in the experiments for Language Modeling-Small (LM-Small), Language Modeling (LM), and Machine Translation (MT).  It lists key hyperparameters such as the number of experts (E), how often a feed-forward network (FFN) block is replaced by a mixture-of-experts (MoE) block (M), and the capacity fraction of each expert (C).  It also shows the model parameters including the token dimension (TD), hidden dimension (HD), and vocabulary size. These parameters are crucial for understanding the experimental setup and replicating the results.", "section": "3 Mixture-of-Experts Characterization"}]