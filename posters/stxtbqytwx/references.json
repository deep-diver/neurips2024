{"references": [{"fullname_first_author": "Lepikhin, D.", "paper_title": "Gshard: Scaling giant models with conditional computation and automatic sharding", "publication_date": "2020-06-16", "reason": "This paper introduces GShard, a crucial technique for training large Mixture-of-Experts models, directly addressing the challenges of scaling MoE models that the current paper builds upon."}, {"fullname_first_author": "Shen, L.", "paper_title": "Se-moe: A scalable and efficient mixture-of-experts distributed training and inference system", "publication_date": "2022-05-10", "reason": "This paper presents Se-MoE, a system focusing on scalability and efficiency in MoE training and inference, which provides a relevant comparison for the current paper's proposed optimizations."}, {"fullname_first_author": "Fedus, W.", "paper_title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity", "publication_date": "2022-00-00", "reason": "This paper introduces Switch Transformers, a significant advancement in sparsely activated models, offering a comparison to the MoE approach explored in the current research."}, {"fullname_first_author": "Du, N.", "paper_title": "GLaM: Efficient scaling of language models with mixture-of-experts", "publication_date": "2022-00-00", "reason": "This paper details GLaM, a model that efficiently scales language models using MoEs, providing a strong baseline and comparison for the current paper's work."}, {"fullname_first_author": "Artetxe, M.", "paper_title": "Efficient large scale language modeling with mixtures of experts", "publication_date": "2022-00-00", "reason": "This paper focuses on efficient large-scale language modeling with MoEs, offering a direct comparison and context for the current paper's contributions in improving inference efficiency."}]}