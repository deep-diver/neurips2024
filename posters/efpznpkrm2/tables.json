[{"figure_path": "EfpZNpkrm2/tables/tables_2_1.jpg", "caption": "Table 1: Performance of base and LoRA fine-tuned LLaMA2-7B on RTE [49] and DROP [50] datasets. We use accuracy and F\u2081-score as the metrics for them respectively.", "description": "This table presents the performance comparison of the base LLaMA2-7B model and the same model fine-tuned using LoRA with rank 64 and 128 on two different datasets: RTE and DROP.  RTE (Recognizing Textual Entailment) is a simpler classification task, while DROP (Difficult Reasoning Over Paragraphs) is a more complex generation task. The table shows that LoRA improves performance on both datasets, but the improvement is more significant on the RTE dataset. This suggests that the low-rank assumption of LoRA might not hold as well for complex tasks such as DROP.", "section": "3 Motivation: Low Rank is not Always Sufficient"}, {"figure_path": "EfpZNpkrm2/tables/tables_6_1.jpg", "caption": "Table 2: Benchmark of various fine-tuning methods on the DROP dataset using LLaMA2 7-70 billion parameter models as the base model. In each case, we report the average of F\u2081 score over 2-4 experiments with different random seeds.", "description": "This table compares the performance of several parameter-efficient fine-tuning (PEFT) methods on the DROP dataset, using different sizes of the LLaMA2 language model.  The methods compared include full fine-tuning (FT), series adapters, parallel adapters, LoRA with different ranks, and QuanTA with different parameter configurations. The table shows the number of trainable parameters used by each method (as a percentage of the total parameters) and the resulting F1 score achieved.  The results highlight QuanTA's superior performance compared to other PEFT methods, especially when using a small fraction of trainable parameters.", "section": "7 Experiments"}, {"figure_path": "EfpZNpkrm2/tables/tables_7_1.jpg", "caption": "Table 3: Benchmark on various commonsense reasoning tasks. All results of models and PEFT methods labeled with \u201c*\u201d are from [54], and results with \u201c\u2020\u201d are from [20].", "description": "This table compares the performance of various parameter-efficient fine-tuning (PEFT) methods on several commonsense reasoning tasks using different sized language models (LLaMAs).  It shows the accuracy achieved by different methods (Full Fine-tuning, Prefix Tuning, Adapter methods, LoRA, DORA, and QuanTA) with varying numbers of trainable parameters.  The results demonstrate QuanTA's superior performance and efficiency compared to other PEFT methods.", "section": "7 Experiments"}, {"figure_path": "EfpZNpkrm2/tables/tables_8_1.jpg", "caption": "Table 4: Benchmark on various arithmetic reasoning tasks. GPT-3.5 (labeled with *) results are taken from [54].", "description": "This table presents the results of several models on four arithmetic reasoning tasks.  The models tested include LLaMA2-7B and LLaMA2-13B, fine-tuned with different parameter-efficient fine-tuning (PEFT) methods such as full fine-tuning (FT), LoRA, and QuanTA.  The table compares the accuracy of each model and method across the four tasks.  The results show that QuanTA consistently outperforms LoRA and often surpasses full fine-tuning, particularly on the MAWPS and SVAMP datasets, while using a significantly smaller number of parameters.", "section": "7 Experiments"}, {"figure_path": "EfpZNpkrm2/tables/tables_17_1.jpg", "caption": "Table D.1: List of datasets used in this work.", "description": "This table lists all the datasets used in the QuanTA paper, specifying the dataset name, the task it was used for (reading comprehension, commonsense reasoning, or arithmetic reasoning), and the number of training, validation, and test samples for each.  The table also indicates the evaluation metric (F\u2081-score or accuracy) and the type of answer expected (phrase, yes/no, option, or number).", "section": "D Datasets"}, {"figure_path": "EfpZNpkrm2/tables/tables_20_1.jpg", "caption": "Table 2: Benchmark of various fine-tuning methods on the DROP dataset using LLaMA2 7-70 billion parameter models as the base model. In each case, we report the average of F\u2081 score over 2-4 experiments with different random seeds.", "description": "This table compares the performance of several parameter-efficient fine-tuning (PEFT) methods on the DROP dataset, using different sizes of the LLaMA2 model as a base.  It shows the number of trainable parameters used by each method (as a percentage of the total parameters), and the resulting F1 score. The methods compared include full fine-tuning, series adapters, parallel adapters, LoRA with different ranks, and QuanTA with different configurations. The table highlights QuanTA's ability to achieve high F1 scores with significantly fewer trainable parameters compared to other methods.", "section": "7 Experiments"}, {"figure_path": "EfpZNpkrm2/tables/tables_21_1.jpg", "caption": "Table 2: Benchmark of various fine-tuning methods on the DROP dataset using LLaMA2 7-70 billion parameter models as the base model. In each case, we report the average of F\u2081 score over 2-4 experiments with different random seeds.", "description": "This table compares different parameter-efficient fine-tuning (PEFT) methods on the DROP dataset using LLaMA2 models with varying parameter counts (7B and 70B).  The methods include full fine-tuning (FT), series and parallel adapters, LoRA with different ranks, and QuanTA with different configurations.  The performance metric is the F1 score, averaged across multiple experiments with different random seeds to account for variability.", "section": "7 Experiments"}, {"figure_path": "EfpZNpkrm2/tables/tables_22_1.jpg", "caption": "Table E.2: Hyperparameters used for DROP dataset for various fine-tuning methods. Curly brackets include the hyperparameter values tested during hyperparameter optimization, with the actual hyperparameter(s) underscored. Square brackets include hyperparameter values for different experiments conducted in the main paper.", "description": "This table lists the hyperparameters used in the experiments for the DROP dataset.  It shows the values used for each of the different fine-tuning methods (Full Fine-tuning (FT), Series Adapters, Parallel Adapters, LoRA, and QuanTA).  Curly brackets indicate the range of hyperparameters tested during optimization, while the underscored values are the ones finally selected. Square brackets show hyperparameter variations used in different experiments reported in the main paper. The table provides a detailed breakdown of the settings used for each method, aiding in reproducibility and understanding of the experimental setup.", "section": "E Experiments"}, {"figure_path": "EfpZNpkrm2/tables/tables_23_1.jpg", "caption": "Table 2: Benchmark of various fine-tuning methods on the DROP dataset using LLaMA2 7-70 billion parameter models as the base model. In each case, we report the average of F\u2081 score over 2-4 experiments with different random seeds.", "description": "This table compares the performance of different parameter-efficient fine-tuning (PEFT) methods on the DROP dataset, using LLaMA2 models with varying parameter counts.  It shows the F1 score achieved by each method, along with the percentage of parameters used relative to full fine-tuning.  The goal is to demonstrate QuanTA's efficiency and effectiveness compared to other techniques like LoRA and adapter-based methods.", "section": "7 Experiments"}, {"figure_path": "EfpZNpkrm2/tables/tables_23_2.jpg", "caption": "Table 3: Benchmark on various commonsense reasoning tasks. All results of models and PEFT methods labeled with \u201c*\u201d are from [54], and results with \u201c\u2020\u201d are from [20].", "description": "This table compares the performance of different parameter-efficient fine-tuning (PEFT) methods on various commonsense reasoning tasks.  The results are shown as accuracy scores for several benchmark datasets (BoolQ, PIQA, SIQA, Hellaswag, Winograd Schema Challenge, ARC-e, ARC-c, and OBQA). The table highlights the performance of QuanTA against other PEFT methods such as full fine-tuning (FT), LoRA, DORA, and adapter-based methods (Series and Parallel).  The \"#Params (%) column shows the percentage of parameters trained for each method relative to the full fine-tuning approach.  Some results are sourced from external studies [54, 20].", "section": "7 Experiments"}, {"figure_path": "EfpZNpkrm2/tables/tables_23_3.jpg", "caption": "Table F.7: Benchmark on five natural language understanding tasks using RoBERTa model as the base model.", "description": "This table presents the results of benchmarking various parameter-efficient fine-tuning (PEFT) methods on five natural language understanding tasks from the GLUE benchmark, using the RoBERTa model as the base.  It compares the performance of LoRA and QuanTA (the proposed method) in terms of accuracy on SST-2, MRPC, CoLA, RTE, and STS-B tasks.  The table highlights the number of trainable parameters used by each method as a percentage of the total model parameters.", "section": "F Additional Benchmarking Results"}]