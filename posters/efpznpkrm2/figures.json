[{"figure_path": "EfpZNpkrm2/figures/figures_1_1.jpg", "caption": "Figure 1: Conceptual comparison of QuanTA and LoRA methods. LoRA parameterizes the weight matrix update as a outer product of two low-rank matrices, limiting its capacity. QuanTA, inspired by quantum circuits, uses tensors that operate on specific axes of the (reshaped) input, enabling high-rank parameterization. Supported by the universality theorem and rank representation theorem, QuanTA can represent arbitrary matrices effectively, allowing it to achieve performance comparable to or sometimes even better than full fine-tuning, with only a fraction of the parameters. Note: the performance graph is a conceptual illustration.", "description": "This figure compares QuanTA and LoRA, two methods for fine-tuning large language models. LoRA uses low-rank matrices to update model weights, limiting its ability to capture complex relationships. QuanTA, inspired by quantum circuits, uses tensors to update weights along specific axes.  This allows for high-rank updates, improving performance while using fewer parameters. The graph illustrates that QuanTA achieves performance closer to full fine-tuning than LoRA.", "section": "1 Introduction"}, {"figure_path": "EfpZNpkrm2/figures/figures_2_1.jpg", "caption": "Figure 1: Conceptual comparison of QuanTA and LoRA methods. LoRA parameterizes the weight matrix update as a outer product of two low-rank matrices, limiting its capacity. QuanTA, inspired by quantum circuits, uses tensors that operate on specific axes of the (reshaped) input, enabling high-rank parameterization. Supported by the universality theorem and rank representation theorem, QuanTA can represent arbitrary matrices effectively, allowing it to achieve performance comparable to or sometimes even better than full fine-tuning, with only a fraction of the parameters. Note: the performance graph is a conceptual illustration.", "description": "This figure conceptually compares QuanTA and LoRA methods for parameter-efficient fine-tuning of LLMs.  LoRA uses low-rank matrix updates, limiting its representational capacity.  QuanTA, drawing inspiration from quantum circuits, employs tensors operating on specific input axes, enabling higher-rank parameterizations.  Theoretically, QuanTA's ability to represent arbitrary matrices effectively is supported by the universality theorem and the rank representation theorem, promising performance comparable to or exceeding full fine-tuning with significantly fewer parameters.", "section": "1 Introduction"}, {"figure_path": "EfpZNpkrm2/figures/figures_3_1.jpg", "caption": "Figure 3: Any unitary matrix can be decomposed into a quantum circuit using one- and two-qubit gates.", "description": "This figure illustrates the universality of quantum circuits.  It shows that any unitary matrix (a type of mathematical transformation representing a quantum operation) can be broken down into a sequence of simpler operations, namely single-qubit and two-qubit gates.  This decomposition is crucial because it demonstrates that complex quantum computations can be constructed from a limited set of basic building blocks. This concept is foundational to the development of QuanTA, as it demonstrates that high-rank operations can be achieved using a composition of smaller operations.", "section": "Preliminary: Quantum Circuit"}, {"figure_path": "EfpZNpkrm2/figures/figures_14_1.jpg", "caption": "Figure 1: Conceptual comparison of QuanTA and LoRA methods. LoRA parameterizes the weight matrix update as a outer product of two low-rank matrices, limiting its capacity. QuanTA, inspired by quantum circuits, uses tensors that operate on specific axes of the (reshaped) input, enabling high-rank parameterization. Supported by the universality theorem and rank representation theorem, QuanTA can represent arbitrary matrices effectively, allowing it to achieve performance comparable to or sometimes even better than full fine-tuning, with only a fraction of the parameters. Note: the performance graph is a conceptual illustration.", "description": "This figure compares QuanTA and LoRA, two parameter-efficient fine-tuning methods for LLMs.  LoRA uses low-rank matrix updates, limiting its ability to capture complex relationships in data.  QuanTA, inspired by quantum circuits, uses tensor operations on specific axes of the input, allowing for high-rank updates. This enables QuanTA to represent a wider range of matrices than LoRA, leading to performance closer to or even surpassing full fine-tuning, but with far fewer parameters.", "section": "1 Introduction"}, {"figure_path": "EfpZNpkrm2/figures/figures_15_1.jpg", "caption": "Figure 1: Conceptual comparison of QuanTA and LoRA methods. LoRA parameterizes the weight matrix update as a outer product of two low-rank matrices, limiting its capacity. QuanTA, inspired by quantum circuits, uses tensors that operate on specific axes of the (reshaped) input, enabling high-rank parameterization. Supported by the universality theorem and rank representation theorem, QuanTA can represent arbitrary matrices effectively, allowing it to achieve performance comparable to or sometimes even better than full fine-tuning, with only a fraction of the parameters. Note: the performance graph is a conceptual illustration.", "description": "This figure compares QuanTA and LoRA methods. LoRA uses low-rank matrices to update weight matrices, limiting its capacity to handle complex tasks.  QuanTA, inspired by quantum circuits, employs tensors operating on specific input axes, allowing for high-rank parameterization.  Theoretically, QuanTA can represent arbitrary matrices efficiently, potentially outperforming full fine-tuning with fewer parameters.", "section": "1 Introduction"}, {"figure_path": "EfpZNpkrm2/figures/figures_15_2.jpg", "caption": "Figure 1: Conceptual comparison of QuanTA and LoRA methods. LoRA parameterizes the weight matrix update as a outer product of two low-rank matrices, limiting its capacity. QuanTA, inspired by quantum circuits, uses tensors that operate on specific axes of the (reshaped) input, enabling high-rank parameterization. Supported by the universality theorem and rank representation theorem, QuanTA can represent arbitrary matrices effectively, allowing it to achieve performance comparable to or sometimes even better than full fine-tuning, with only a fraction of the parameters. Note: the performance graph is a conceptual illustration.", "description": "This figure conceptually compares QuanTA and LoRA, highlighting their differences in parameterizing weight matrix updates.  LoRA uses low-rank matrices, limiting its representational capacity, while QuanTA leverages tensors inspired by quantum circuits for high-rank parameterization, enabling more expressive updates and better performance with fewer parameters. The universality and rank representation theorems underpin QuanTA's ability to efficiently represent arbitrary matrices.", "section": "1 Introduction"}, {"figure_path": "EfpZNpkrm2/figures/figures_18_1.jpg", "caption": "Figure 1: Conceptual comparison of QuanTA and LoRA methods. LoRA parameterizes the weight matrix update as a outer product of two low-rank matrices, limiting its capacity. QuanTA, inspired by quantum circuits, uses tensors that operate on specific axes of the (reshaped) input, enabling high-rank parameterization. Supported by the universality theorem and rank representation theorem, QuanTA can represent arbitrary matrices effectively, allowing it to achieve performance comparable to or sometimes even better than full fine-tuning, with only a fraction of the parameters. Note: the performance graph is a conceptual illustration.", "description": "The figure conceptually compares QuanTA and LoRA methods for fine-tuning LLMs.  LoRA uses low-rank matrix updates, limiting its ability to capture complex relationships. QuanTA, inspired by quantum circuits, uses tensors for high-rank updates, enabling more flexible adaptation and potentially better performance with fewer parameters.", "section": "1 Introduction"}, {"figure_path": "EfpZNpkrm2/figures/figures_27_1.jpg", "caption": "Figure 1: Conceptual comparison of QuanTA and LoRA methods. LoRA parameterizes the weight matrix update as a outer product of two low-rank matrices, limiting its capacity. QuanTA, inspired by quantum circuits, uses tensors that operate on specific axes of the (reshaped) input, enabling high-rank parameterization. Supported by the universality theorem and rank representation theorem, QuanTA can represent arbitrary matrices effectively, allowing it to achieve performance comparable to or sometimes even better than full fine-tuning, with only a fraction of the parameters. Note: the performance graph is a conceptual illustration.", "description": "The figure conceptually compares QuanTA and LoRA, highlighting their differences in how they parameterize weight matrix updates. LoRA uses low-rank matrices, limiting its capacity to capture complex relationships.  In contrast, QuanTA uses tensors inspired by quantum circuits to enable high-rank parameterization, allowing it to effectively represent arbitrary matrices and potentially achieve superior performance while using fewer parameters.", "section": "1 Introduction"}]