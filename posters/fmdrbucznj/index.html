<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>Expected Probabilistic Hierarchies &#183; NeurIPS 2024</title>
<meta name=title content="Expected Probabilistic Hierarchies &#183; NeurIPS 2024"><meta name=description content="Expected Probabilistic Hierarchies (EPH) offers a novel, scalable approach to hierarchical clustering by optimizing expected scores under a probabilistic model, outperforming existing methods on vario..."><meta name=keywords content="Machine Learning,Unsupervised Learning,üè¢ Munich Data Science Institute,"><link rel=canonical href=https://deep-diver.github.io/neurips2024/posters/fmdrbucznj/><link type=text/css rel=stylesheet href=/neurips2024/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/neurips2024/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/neurips2024/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/neurips2024/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/neurips2024/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/neurips2024/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/neurips2024/favicon-16x16.png><link rel=manifest href=/neurips2024/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/neurips2024/posters/fmdrbucznj/"><meta property="og:site_name" content="NeurIPS 2024"><meta property="og:title" content="Expected Probabilistic Hierarchies"><meta property="og:description" content="Expected Probabilistic Hierarchies (EPH) offers a novel, scalable approach to hierarchical clustering by optimizing expected scores under a probabilistic model, outperforming existing methods on vario‚Ä¶"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posters"><meta property="article:published_time" content="2024-09-26T00:00:00+00:00"><meta property="article:modified_time" content="2024-09-26T00:00:00+00:00"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Unsupervised Learning"><meta property="article:tag" content="üè¢ Munich Data Science Institute"><meta property="og:image" content="https://deep-diver.github.io/neurips2024/posters/fmdrbucznj/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/neurips2024/posters/fmdrbucznj/cover.png"><meta name=twitter:title content="Expected Probabilistic Hierarchies"><meta name=twitter:description content="Expected Probabilistic Hierarchies (EPH) offers a novel, scalable approach to hierarchical clustering by optimizing expected scores under a probabilistic model, outperforming existing methods on vario‚Ä¶"><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Posters","name":"Expected Probabilistic Hierarchies","headline":"Expected Probabilistic Hierarchies","abstract":"Expected Probabilistic Hierarchies (EPH) offers a novel, scalable approach to hierarchical clustering by optimizing expected scores under a probabilistic model, outperforming existing methods on vario\u0026hellip;","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/neurips2024\/posters\/fmdrbucznj\/","author":{"@type":"Person","name":"AI Paper Reviewer"},"copyrightYear":"2024","dateCreated":"2024-09-26T00:00:00\u002b00:00","datePublished":"2024-09-26T00:00:00\u002b00:00","dateModified":"2024-09-26T00:00:00\u002b00:00","keywords":["Machine Learning","Unsupervised Learning","üè¢ Munich Data Science Institute"],"mainEntityOfPage":"true","wordCount":"4277"}]</script><meta name=author content="AI Paper Reviewer"><link href=https://neurips.cc/ rel=me><link href=https://x.com/NeurIPSConf rel=me><link href rel=me><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://x.com/algo_diver/ rel=me><script src=/neurips2024/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/neurips2024/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/neurips2024/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/neurips2024/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/neurips2024/ class="text-base font-medium text-gray-500 hover:text-gray-900">NeurIPS 2024</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/neurips2024/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><div><div class="cursor-pointer flex items-center nested-menu"><a class="text-base font-medium text-gray-500 hover:text-primary-600 dark:hover:text-primary-400" title>Oral
</a><span><span class="relative block icon"><svg viewBox="0 0 20 20" fill="currentcolor" aria-hidden="true"><path fill-rule="evenodd" d="M5.23 7.21a.75.75.0 011.06.02L10 11.168l3.71-3.938a.75.75.0 111.08 1.04l-4.25 4.5a.75.75.0 01-1.08.0l-4.25-4.5a.75.75.0 01.02-1.06z" clip-rule="evenodd"/></svg></span></span></div><div class="absolute menuhide"><div class="pt-2 p-5 mt-2 rounded-xl backdrop-blur shadow-2xl"><div class="flex flex-col space-y-3"><a href=/neurips2024/oral-ai-applications/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(o) AI Applications</p></a><a href=/neurips2024/oral-ai-theory/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(o) AI Theory</p></a><a href=/neurips2024/oral-image-generation/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(o) Image Generation</p></a><a href=/neurips2024/oral-large-language-models/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(o) Large Language Models</p></a><a href=/neurips2024/oral-others/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(o) Others</p></a><a href=/neurips2024/oral-reinforcement-learning/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(o) Reinforcement Learning</p></a></div></div></div></div><div><div class="cursor-pointer flex items-center nested-menu"><a class="text-base font-medium text-gray-500 hover:text-primary-600 dark:hover:text-primary-400" title>Spotlight
</a><span><span class="relative block icon"><svg viewBox="0 0 20 20" fill="currentcolor" aria-hidden="true"><path fill-rule="evenodd" d="M5.23 7.21a.75.75.0 011.06.02L10 11.168l3.71-3.938a.75.75.0 111.08 1.04l-4.25 4.5a.75.75.0 01-1.08.0l-4.25-4.5a.75.75.0 01.02-1.06z" clip-rule="evenodd"/></svg></span></span></div><div class="absolute menuhide"><div class="pt-2 p-5 mt-2 rounded-xl backdrop-blur shadow-2xl"><div class="flex flex-col space-y-3"><a href=/neurips2024/spotlight-ai-theory/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(s) AI Theory</p></a><a href=/neurips2024/spotlight-large-language-models/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(s) Large Language Models</p></a><a href=/neurips2024/spotlight-optimization/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(s) Optimization</p></a><a href=/neurips2024/spotlight-others/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(s) Others</p></a><a href=/neurips2024/spotlight-reinforcement-learning/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-sm" title>(s) Reinforcement Learning</p></a></div></div></div></div><a href=/neurips2024/posters/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Posters</p></a><a href=/neurips2024/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/neurips2024/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Oral</p><span><span class="relative block icon"><svg viewBox="0 0 20 20" fill="currentcolor" aria-hidden="true"><path fill-rule="evenodd" d="M5.23 7.21a.75.75.0 011.06.02L10 11.168l3.71-3.938a.75.75.0 111.08 1.04l-4.25 4.5a.75.75.0 01-1.08.0l-4.25-4.5a.75.75.0 01.02-1.06z" clip-rule="evenodd"/></svg></span></span></a></li><li class=mt-1><a href=/neurips2024/oral-ai-applications/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(o) AI Applications</p></a></li><li class=mt-1><a href=/neurips2024/oral-ai-theory/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(o) AI Theory</p></a></li><li class=mt-1><a href=/neurips2024/oral-image-generation/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(o) Image Generation</p></a></li><li class=mt-1><a href=/neurips2024/oral-large-language-models/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(o) Large Language Models</p></a></li><li class=mt-1><a href=/neurips2024/oral-others/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(o) Others</p></a></li><li class=mt-1><a href=/neurips2024/oral-reinforcement-learning/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(o) Reinforcement Learning</p></a></li><li class=mb-2></li><li class=mt-1><a class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Spotlight</p><span><span class="relative block icon"><svg viewBox="0 0 20 20" fill="currentcolor" aria-hidden="true"><path fill-rule="evenodd" d="M5.23 7.21a.75.75.0 011.06.02L10 11.168l3.71-3.938a.75.75.0 111.08 1.04l-4.25 4.5a.75.75.0 01-1.08.0l-4.25-4.5a.75.75.0 01.02-1.06z" clip-rule="evenodd"/></svg></span></span></a></li><li class=mt-1><a href=/neurips2024/spotlight-ai-theory/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(s) AI Theory</p></a></li><li class=mt-1><a href=/neurips2024/spotlight-large-language-models/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(s) Large Language Models</p></a></li><li class=mt-1><a href=/neurips2024/spotlight-optimization/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(s) Optimization</p></a></li><li class=mt-1><a href=/neurips2024/spotlight-others/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(s) Others</p></a></li><li class=mt-1><a href=/neurips2024/spotlight-reinforcement-learning/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-sm font-small" title>(s) Reinforcement Learning</p></a></li><li class=mb-2></li><li class=mt-1><a href=/neurips2024/posters/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Posters</p></a></li><li class=mt-1><a href=/neurips2024/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/neurips2024/posters/fmdrbucznj/cover_hu4928415906685409342.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/neurips2024/>NeurIPS 2024</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/neurips2024/posters/>Posters</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/neurips2024/posters/fmdrbucznj/>Expected Probabilistic Hierarchies</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Expected Probabilistic Hierarchies</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2024-09-26T00:00:00+00:00>26 September 2024</time><span class="px-2 text-primary-500">&#183;</span><span>4277 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">21 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_posters/fMdrBucZnj/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_posters/fMdrBucZnj/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/neurips2024/tags/machine-learning/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Machine Learning
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/neurips2024/tags/unsupervised-learning/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Unsupervised Learning
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/neurips2024/tags/-munich-data-science-institute/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ Munich Data Science Institute</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="AI Paper Reviewer" src=/neurips2024/img/avatar_hu1344562329374673026.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">AI Paper Reviewer</div><div class="text-sm text-neutral-700 dark:text-neutral-400">As an AI, I specialize in crafting insightful blog content about cutting-edge research in the field of artificial intelligence</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://neurips.cc/ target=_blank aria-label=Homepage rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg fill="currentcolor" height="800" width="800" id="Capa_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 491.398 491.398"><g><g id="Icons_19_"><path d="M481.765 220.422 276.474 15.123c-16.967-16.918-44.557-16.942-61.559.023L9.626 220.422c-12.835 12.833-12.835 33.65.0 46.483 12.843 12.842 33.646 12.842 46.487.0l27.828-27.832v214.872c0 19.343 15.682 35.024 35.027 35.024h74.826v-97.62c0-7.584 6.146-13.741 13.743-13.741h76.352c7.59.0 13.739 6.157 13.739 13.741v97.621h74.813c19.346.0 35.027-15.681 35.027-35.024V239.091l27.812 27.815c6.425 6.421 14.833 9.63 23.243 9.63 8.408.0 16.819-3.209 23.242-9.63 12.844-12.834 12.844-33.65.0-46.484z"/></g></g></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://x.com/NeurIPSConf target=_blank aria-label=X-Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href target=_blank aria-label=Line rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg id="Capa_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 14.707 14.707"><g><rect x="6.275" y="0" style="fill:currentColor" width="2.158" height="14.707"/></g></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://x.com/algo_diver/ target=_blank aria-label=X-Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#eph-a-novel-method>EPH: A Novel Method</a></li><li><a href=#expected-score-optim>Expected Score Optim.</a></li><li><a href=#differentiable-sampling>Differentiable Sampling</a></li><li><a href=#scalable-subgraph>Scalable Subgraph</a></li><li><a href=#future-research>Future Research</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#eph-a-novel-method>EPH: A Novel Method</a></li><li><a href=#expected-score-optim>Expected Score Optim.</a></li><li><a href=#differentiable-sampling>Differentiable Sampling</a></li><li><a href=#scalable-subgraph>Scalable Subgraph</a></li><li><a href=#future-research>Future Research</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>fMdrBucZnj</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Marcel Kollovieh et el.</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href="https://openreview.net/forum?id=fMdrBucZnj" target=_blank role=button>‚Üó OpenReview
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://neurips.cc/virtual/2024/poster/94213 target=_blank role=button>‚Üó NeurIPS Homepage
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href="https://huggingface.co/spaces/huggingface/paper-central?tab=tab-chat-with-paper&amp;paper_id=fMdrBucZnj&amp;paper_from=neurips" target=_blank role=button>‚Üó Chat</a></p><audio controls><source src=https://ai-paper-reviewer.com/fMdrBucZnj/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Hierarchical clustering, a fundamental task in unsupervised learning, traditionally faces challenges with discrete optimization methods being computationally expensive and continuous methods failing to guarantee alignment with discrete optima. Existing approaches also struggle to scale effectively to large datasets. This limits the accuracy and applicability of hierarchical clustering across various domains.</p><p>The paper introduces Expected Probabilistic Hierarchies (EPH), addressing these issues by learning hierarchies through probabilistic modeling and optimizing expected scores. <strong>EPH uses differentiable hierarchy sampling</strong>, allowing for end-to-end gradient descent optimization, and <strong>an unbiased subgraph sampling technique</strong> to handle large datasets efficiently. <strong>Experimental results demonstrate EPH&rsquo;s superiority</strong> over existing methods on various benchmarks. The work provides a significant advancement in developing more effective and scalable hierarchical clustering algorithms.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-e1511d350ae85ebf037ca19dd9194e69></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-e1511d350ae85ebf037ca19dd9194e69",{strings:[" EPH optimizes expected scores, unlike existing continuous methods that optimize relaxed scores, resulting in better alignment with discrete optimal values. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-61afdb8aeec9fae462024b2bb4436719></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-61afdb8aeec9fae462024b2bb4436719",{strings:[" EPH uses differentiable hierarchy sampling and an unbiased subgraph sampling approach to achieve end-to-end optimization and scalability to large datasets. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-da9299b95bcd999e4c5ac256dac14653></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-da9299b95bcd999e4c5ac256dac14653",{strings:[" EPH outperforms state-of-the-art methods on various synthetic and real-world datasets, providing both quantitative and qualitative improvements in hierarchical clustering. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This paper is crucial for researchers in unsupervised learning and hierarchical clustering because it <strong>introduces a novel and scalable probabilistic model</strong> (EPH) that overcomes limitations of existing methods. EPH&rsquo;s end-to-end differentiability enables gradient-based optimization, and its unbiased sampling strategy allows for efficient scaling to large datasets. This opens <strong>new avenues for developing more accurate and efficient hierarchical clustering algorithms</strong> with applications in various fields.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/figures_3_1.jpg alt></figure></p><blockquote><p>This figure provides a visual overview of the Expected Probabilistic Hierarchies (EPH) model. The process begins with a probabilistic hierarchy, which is used to sample multiple discrete hierarchies. Concurrently, a subgraph is sampled from the input graph data. The sampled discrete hierarchies and subgraph are used to compute expected scores (e.g., Exp-Das, Exp-TSD). These scores are then averaged to compute the loss, which is used to update the parameters of the probabilistic hierarchy via backpropagation. This iterative process refines the probabilistic hierarchy until convergence.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/tables_4_1.jpg alt></figure></p><blockquote><p>This table compares four different cost functions used for hierarchical clustering: Soft-Das, Exp-Das, Soft-TSD, and Exp-TSD. It shows whether each function is a minimization or maximization problem, whether it&rsquo;s convex or concave with respect to the parameters A and B that describe the probabilistic hierarchy, whether it always results in an integral solution, whether its optimum is equal to the optimal discrete solution, and whether the optimal value of the soft-score aligns with the optimal value of the corresponding discrete score. The table highlights the key theoretical properties of each cost function, indicating their suitability for optimization.</p></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">EPH: A Novel Method<div id=eph-a-novel-method class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#eph-a-novel-method aria-label=Anchor>#</a></span></h4><p>The heading &ldquo;EPH: A Novel Method&rdquo; suggests a research paper introducing a new technique called EPH. <strong>EPH likely represents an algorithm or framework</strong> designed to solve a specific problem within a field like machine learning, data mining, or graph analysis. The &ldquo;novel&rdquo; aspect implies that this method offers significant improvements over existing approaches, perhaps by addressing limitations or improving efficiency. The paper likely details the method&rsquo;s theoretical foundation, including its mathematical formulation and algorithms. <strong>Experimental results are likely presented to demonstrate the efficacy of EPH</strong>, comparing its performance to other state-of-the-art techniques. The evaluation probably focuses on metrics relevant to the task EPH aims to solve, such as accuracy, efficiency, or scalability. <strong>A significant contribution of EPH would be its innovative approach</strong> to the problem and its potential for broader applications across diverse domains.</p><h4 class="relative group">Expected Score Optim.<div id=expected-score-optim class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#expected-score-optim aria-label=Anchor>#</a></span></h4><p>The heading &lsquo;Expected Score Optim.&rsquo; suggests a methodology focusing on optimizing a probabilistic score, rather than directly optimizing a deterministic objective function. This is particularly relevant in the context of unsupervised learning problems, like hierarchical clustering, where the search space of discrete hierarchies is vast and complex. <strong>By working with expected scores, one can utilize techniques from continuous optimization, such as gradient descent</strong>, opening up efficient methods unavailable to traditional discrete optimization approaches. This probabilistic framework <strong>allows for learning hierarchies through differentiable hierarchy sampling</strong>, a technique that leverages continuous relaxations to facilitate gradient-based optimization. <strong>A key advantage is that the expected scores may directly translate to the optimal values of their discrete counterparts</strong>, providing a theoretical grounding for using continuous methods to approximate solutions to discrete problems. The approach likely involves sampling from a probability distribution over hierarchies, estimating the score for each sample, and then using this empirical estimate to guide the optimization process, making it computationally more tractable than exhaustively searching the discrete space.</p><h4 class="relative group">Differentiable Sampling<div id=differentiable-sampling class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#differentiable-sampling aria-label=Anchor>#</a></span></h4><p>Differentiable sampling techniques are crucial for training models with discrete latent variables, a common challenge in areas like hierarchical clustering. The core idea is to <strong>approximate discrete sampling processes with continuous, differentiable counterparts</strong>, allowing for the application of gradient-based optimization methods. This is essential because standard backpropagation cannot directly handle discrete variables. <strong>The Gumbel-softmax trick and related methods are prominent examples</strong>, providing a way to sample from a categorical distribution while maintaining differentiability. However, directly applying these techniques to complex structures like hierarchical trees can be challenging. <strong>The success of differentiable sampling hinges on finding effective approximations that balance accuracy and computational efficiency.</strong> The trade-off between these two factors is crucial, impacting the model&rsquo;s ability to learn meaningful representations. For complex structures, unbiased sampling methods are particularly important to guarantee that the resulting gradients accurately reflect the underlying discrete optimization problem. <strong>Therefore, methods like unbiased subgraph sampling become significant for handling large datasets where exhaustive enumeration of all possibilities would be computationally prohibitive.</strong> Careful consideration must be given to the selection of appropriate approximations, and the effects of biased gradients on the final model&rsquo;s performance needs to be studied.</p><h4 class="relative group">Scalable Subgraph<div id=scalable-subgraph class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#scalable-subgraph aria-label=Anchor>#</a></span></h4><p>The concept of &ldquo;Scalable Subgraph&rdquo; in the context of hierarchical clustering suggests a method to efficiently handle large datasets. The core idea likely revolves around <strong>breaking down the computational complexity</strong> of evaluating similarity scores or objective functions. Instead of processing the entire graph, a smaller, representative subgraph is sampled and used for computation. This approach trades off perfect accuracy for significant gains in speed, allowing the algorithm to scale to datasets that would otherwise be intractable. The scalability is achieved by strategically choosing the subgraph to <strong>capture the essential structure</strong> of the larger graph. Different sampling techniques, like random edge sampling or more sophisticated methods that consider node centrality or community structure, may be employed to ensure the representative nature of the subgraph. <strong>Unbiased sampling</strong> is crucial to avoid introducing systematic errors or biases that could impact the accuracy of the hierarchical clustering results. Furthermore, the choice of subgraph size needs careful consideration, balancing the trade-off between computational cost and the information loss due to the subsampling.</p><h4 class="relative group">Future Research<div id=future-research class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#future-research aria-label=Anchor>#</a></span></h4><p>Future research directions stemming from this work on Expected Probabilistic Hierarchies (EPH) could explore several promising avenues. <strong>Improving the scalability</strong> of EPH for extremely large datasets remains a key challenge; investigating more sophisticated sampling techniques or approximation methods is crucial. <strong>Extending EPH to handle various data modalities</strong> beyond graphs and vectors, such as text or time-series data, would broaden its applicability. A deeper theoretical analysis of the model&rsquo;s convergence properties and the impact of different sampling strategies would strengthen the EPH foundation. Finally, applying EPH to real-world problems and <strong>developing novel evaluation metrics</strong> specifically tailored for probabilistic hierarchies would further validate its effectiveness and impact.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/figures_4_1.jpg alt></figure></p><blockquote><p>This figure demonstrates a scenario where the Flexible Probabilistic Hierarchy (FPH) method fails to find the optimal hierarchy that minimizes the Dasgupta cost, unlike the Expected Probabilistic Hierarchies (EPH) method. It uses an unweighted K4 graph (complete graph with 4 nodes) as an example. FPH, using a continuous relaxation, achieves a Dasgupta cost of 4.0 after discretization, while the optimal discrete hierarchy has a cost of 3.0. However, EPH successfully identifies this optimal discrete hierarchy with a cost of 3.0, showcasing its advantage over FPH in finding the optimal discrete hierarchy.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/figures_7_1.jpg alt></figure></p><blockquote><p>This figure shows the impact of two hyperparameters on the performance of the Expected Probabilistic Hierarchies (EPH) model for hierarchical clustering. The left panel illustrates how the normalized Dasgupta cost changes with varying numbers of sampled hierarchies used during training. Different colors represent different datasets. The right panel examines the effect of the number of sampled edges on the normalized Dasgupta cost, comparing EPH against the average linkage algorithm (AL) and a full graph training (FG). The normalization ensures that each dataset&rsquo;s mean is zero and standard deviation is one, allowing for comparison across different datasets.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/figures_8_1.jpg alt></figure></p><blockquote><p>This figure visualizes the ground truth and the hierarchical clustering results obtained by EPH for both small and large Hierarchical Stochastic Block Models (HSBMs). The top row shows the results for a small HSBM, while the bottom row shows the results for a larger HSBM. Within each row, the leftmost panel displays the ground truth community structure (GT), the middle panel shows the dendrogram generated by EPH using the expected Dasgupta cost (Exp-Das), and the rightmost panel shows the dendrogram produced by EPH using the expected Tree-Sampling Divergence (Exp-TSD). The color-coding in the dendrograms represents the clusters identified by the algorithm. The figure provides a visual comparison of how well EPH&rsquo;s clustering results align with the ground truth community structure for different network sizes and using different evaluation metrics.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/figures_9_1.jpg alt></figure></p><blockquote><p>The figure shows the largest cluster inferred on the Cifar-100 dataset using EPH. The left subplot shows the 16 images with the highest probability, and the right subplot shows the 16 images with the lowest probability. The images with high probabilities are all similar and related to insects, demonstrating EPH&rsquo;s ability to group similar images together. The images with low probabilities do not fit into the group, illustrating EPH&rsquo;s capacity to measure uncertainty in cluster assignments.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/figures_12_1.jpg alt></figure></p><blockquote><p>This figure illustrates the three possible scenarios when calculating the joint probability of the lowest common ancestor (LCA) and ancestor probabilities. It shows how the paths from three leaves (v1, v2, and v) to an internal node (zk) can intersect at different points (zk&rsquo;, zk, or zk) depending on the tree structure. This is important for calculating the expected Dasgupta cost.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/figures_15_1.jpg alt></figure></p><blockquote><p>This figure demonstrates that the expected Dasgupta cost (Exp-Das), a function used in the Expected Probabilistic Hierarchies (EPH) method, is neither convex nor concave. It presents three different hierarchies (a, b, c) and two graphs (d, e). Hierarchy (c) is a linear interpolation between (a) and (b). Graphs (d) and (e) illustrate scenarios where the function behaves convexly and concavely, respectively. This non-convexity and non-concavity property complicates optimization but is explained and addressed within the EPH method.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/figures_15_2.jpg alt></figure></p><blockquote><p>This figure shows an example where the Flexible Probabilistic Hierarchy (FPH) method fails to find the optimal hierarchy that minimizes the Dasgupta cost. FPH&rsquo;s continuous relaxation results in a Soft-Das score lower than the optimal discrete Dasgupta cost. In contrast, the Expected Probabilistic Hierarchies (EPH) method successfully finds the minimizing hierarchy. The figure uses an unweighted K4 graph for simplicity, but a similar issue is demonstrated with a weighted graph in Figure 8.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/figures_18_1.jpg alt></figure></p><blockquote><p>The figure visualizes the largest cluster inferred on the Cifar-100 dataset using the EPH method. It shows the 16 images with the highest probability (left) and the 16 images with the lowest probability (right) for the largest cluster. The images with high probabilities are visually similar (insects), demonstrating EPH&rsquo;s ability to group similar images. The images with low probabilities do not visually fit in this group, illustrating EPH&rsquo;s capability to measure uncertainty in cluster assignments.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/figures_18_2.jpg alt></figure></p><blockquote><p>The figure shows the largest cluster obtained by applying the EPH model on the Cifar-100 dataset. It displays two sets of 16 images each. (a) shows the 16 images with the highest probability of belonging to the cluster, and (b) shows the 16 images with the lowest probability. The images in (a) are visually similar and consistent with the theme of the cluster, whereas the images in (b) are more diverse and less representative of the cluster&rsquo;s characteristics. This visualization demonstrates the capacity of EPH to both identify coherent clusters and quantify the uncertainty associated with cluster assignments.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/figures_19_1.jpg alt></figure></p><blockquote><p>This figure visualizes the largest cluster identified by the EPH model on the Cifar-100 dataset. It showcases two sub-figures. (a) Highest Probability displays the 16 images with the highest probability within that cluster, showing a clear visual similarity related to insects. (b) Lowest Probability shows the 16 images with the lowest probability in that same cluster, demonstrating that they visually differ and do not strongly fit the identified theme. The images illustrate the model&rsquo;s ability to discern clear visual patterns and quantify uncertainty within clusters by examining the probability assignments to each image.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/figures_19_2.jpg alt></figure></p><blockquote><p>This figure visualizes the results of applying the Expected Probabilistic Hierarchies (EPH) method to the OpenFlight dataset, a graph representing flight connections between various locations worldwide. The left side shows the geographical distribution of the 64 clusters identified by EPH using two different objective functions: Exp-Das (expected Dasgupta cost) and Exp-TSD (expected Tree-Sampling Divergence). The right side displays the corresponding dendrograms for both objective functions, visually representing the hierarchical structure of the clusters. This allows for a direct comparison of the clustering results based on these two different metrics, showcasing how the hierarchical structure changes when optimizing for different objective functions. Each color represents a cluster, and the dendrogram&rsquo;s branch lengths reflect the distance between clusters.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/figures_20_1.jpg alt></figure></p><blockquote><p>This figure visualizes the results of the EPH algorithm on five vector datasets (Zoo, Iris, Digits, Segmentation, and Spambase). It uses t-SNE to reduce the dimensionality of the data for better visualization. For each dataset, it shows three plots side-by-side: 1. <strong>Ground Truth Clusters:</strong> Shows the actual cluster assignments for each data point, providing a baseline for comparison. 2. <strong>Inferred Flattened Clusters:</strong> Shows the cluster assignments generated by the EPH algorithm after flattening the hierarchical structure. 3. <strong>Dendrograms:</strong> Provides a visual representation of the hierarchical clustering produced by EPH. The dendrogram illustrates the relationships between clusters and how they merge at different levels of granularity. The purpose is to allow a visual comparison of the ground truth and the EPH&rsquo;s performance in clustering these datasets.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/figures_21_1.jpg alt></figure></p><blockquote><p>This figure displays the results of a linear interpolation experiment conducted on seven different graph datasets (Brain, Citeseer, Cora-ML, Genes, OpenFlight, Polblogs, and WikiPhysics). The experiment interpolates between the average linkage hierarchy and the hierarchy inferred by the Exp-Das algorithm, evaluating the Soft-Das and Exp-Das scores at different interpolation points (denoted by &lsquo;Factor a&rsquo; on the x-axis). The y-axis shows the normalized Soft-Das and Exp-Das scores, respectively. This visualization provides insight into the relationship between the average linkage approach and the Exp-Das optimized hierarchy, highlighting potential differences in scoring metrics and the impact of the optimization procedure.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/figures_22_1.jpg alt></figure></p><blockquote><p>This figure shows the results of a hyperparameter study for the EPH model, investigating the impact of the number of sampled hierarchies and the number of sampled edges on the normalized Dasgupta cost. The left panel displays the effect of varying the number of sampled hierarchies, comparing EPH&rsquo;s performance with the average linkage algorithm (AL) and training on the full graph. The right panel shows a similar comparison using different numbers of sampled edges. In both panels, the results are normalized across different datasets to allow for a clear comparison.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/figures_22_2.jpg alt></figure></p><blockquote><p>This figure shows the results of a hyperparameter study conducted to determine the optimal number of sampled hierarchies and edges for the EPH model. The left panel shows how the normalized Dasgupta cost varies with the number of sampled hierarchies for different datasets (Brain, OpenFlight, Genes, Citeseer, Cora-ML, Polblogs, WikiPhysics). The right panel shows how the normalized Dasgupta cost varies with the number of sampled edges for different datasets (Zoo, Iris, Glass, Digits, Segmentation, Spambase, Letter, Cifar-100). The average linkage algorithm (AL) and a training on the full graph (FG) are also included as baselines for comparison. The scores are normalized to have a mean of zero and a standard deviation of one for each dataset.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/figures_23_1.jpg alt></figure></p><blockquote><p>This figure provides a visual overview of the Expected Probabilistic Hierarchies (EPH) model. It shows the process of sampling discrete hierarchies and subgraphs, computing the expected scores, and updating the probabilistic hierarchy through backpropagation. The figure highlights the key steps involved in EPH and shows how differentiable hierarchy and subgraph sampling are used to optimize expected scores.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/figures_23_2.jpg alt></figure></p><blockquote><p>This figure illustrates the workflow of the Expected Probabilistic Hierarchies (EPH) model. EPH begins by sampling discrete hierarchies and subgraphs from the input data using differentiable sampling techniques. These samples are then used to compute and average the expected scores. Finally, backpropagation is used to update the probabilistic hierarchy, improving its ability to represent the underlying data structure.</p></blockquote></details><details><summary>More on tables</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/tables_7_1.jpg alt></figure></p><blockquote><p>This table presents the results of different hierarchical clustering methods on various graph datasets. The methods are compared using two metrics: Dasgupta cost and Tree-sampling divergence. Lower Dasgupta cost and higher Tree-sampling divergence indicate better clustering performance. The table highlights the best performing method for each dataset and metric.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/tables_7_2.jpg alt></figure></p><blockquote><p>This table presents the Dasgupta costs achieved by different hierarchical clustering methods on eight vector datasets. The Dasgupta cost is a metric used to evaluate the quality of hierarchical clustering results. Lower scores indicate better clustering performance. The table compares several methods, including traditional linkage-based algorithms (WL, AL, SL, CL), and more recent methods like Louvain, RSC, UF, gHHC, HypHC, FPH, and the proposed method EPH. The best and second-best results for each dataset are highlighted in bold and underlined, respectively, to show the relative performance of each method.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/tables_8_1.jpg alt></figure></p><blockquote><p>This table presents the results of the Expected Probabilistic Hierarchies (EPH) method on two Hierarchical Stochastic Block Models (HSBMs) with varying sizes. It compares the Dasgupta cost and Tree-sampling divergence of the hierarchies generated by EPH against the ground truth (GT) hierarchies. It also shows the normalized mutual information (NMI) at different levels of the hierarchy, indicating the alignment between the EPH-generated hierarchies and the ground truth.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/tables_8_2.jpg alt></figure></p><blockquote><p>This table presents the results of the Expected Probabilistic Hierarchies (EPH) method on two Hierarchical Stochastic Block Models (HSBMs), one small and one large. It compares the performance of EPH against the ground truth (GT) in terms of Dasgupta cost and Tree-Sampling Divergence. The normalized mutual information (NMI) at different levels of the hierarchy is also included, showing how well EPH recovers the structure of the ground truth HSBMs.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/tables_14_1.jpg alt></figure></p><blockquote><p>This table shows the Dasgupta costs for different combinations of hierarchies (T1, T2, and T_I) and graphs (convex and concave examples) from Figure 7 in the paper. The values demonstrate that the Exp-Das function is neither convex nor concave, supporting the paper&rsquo;s claim about the function&rsquo;s properties.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/tables_16_1.jpg alt></figure></p><blockquote><p>This table provides a summary of the graph datasets used in the paper&rsquo;s experiments. For each dataset, it lists the number of nodes (vertices), the number of edges, and the license under which the dataset is available.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/tables_16_2.jpg alt></figure></p><blockquote><p>This table presents an overview of the eight vector datasets used in the paper&rsquo;s experiments. For each dataset, it lists the number of data points, the number of attributes (features) for each data point, the number of classes, and the license under which the dataset is available. The datasets include Zoo, Iris, Glass, Digits, Segmentation, Spambase, Letter, and Cifar-100, representing a variety of data types and sizes.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/tables_16_3.jpg alt></figure></p><blockquote><p>This table presents a summary of the characteristics of the two hierarchical stochastic block models (HSBMs) used in the experiments. It shows the number of nodes, edges, and clusters for both the small and large HSBMs.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/tables_17_1.jpg alt></figure></p><blockquote><p>This table presents the hyperparameters used in the experiments for the different methods: EPH, FPH, HypHC, UF, and DeepWalk. It specifies the learning rate (LR), initialization methods, number of samples used for approximating expectations, temperature parameters for softmax functions, number of epochs for training, number of triplets, loss functions, and embedding dimensions. Specific hyperparameter settings are noted for different datasets (DBLP, Spambase, Letter, and Cifar-100) reflecting adaptations to variations in dataset properties.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/tables_17_2.jpg alt></figure></p><blockquote><p>This table presents the results of different hierarchical clustering methods on various graph datasets. The methods are compared based on two metrics: Dasgupta cost and Tree-sampling divergence. Lower Dasgupta cost and higher Tree-sampling divergence indicate better clustering performance. The table shows that the proposed EPH method achieves the best results in most cases, outperforming various baselines including other state-of-the-art methods.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/tables_18_1.jpg alt></figure></p><blockquote><p>This table presents the results obtained using the Flexible Probabilistic Hierarchy (FPH) method on two synthetic datasets generated using the Hierarchical Stochastic Block Model (HSBM): a small HSBM and a large HSBM. The results are evaluated based on two metrics: the Dasgupta cost and the Tree-sampling divergence. The table also includes the normalized mutual information (NMI) at different levels (Level 1, 2, and 3) of the hierarchy. The ground truth (GT) values are provided for comparison, allowing for the assessment of FPH&rsquo;s ability to recover the ground truth hierarchy.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/tables_21_1.jpg alt></figure></p><blockquote><p>This table presents the Dasgupta costs achieved by different hierarchical clustering methods on eight vector datasets. The Dasgupta cost is a lower score is better metric used to evaluate the quality of a hierarchical clustering. The table compares the performance of various methods including traditional linkage algorithms (WL, AL, SL, CL), Louvain modularity maximization, recursive sparsest cut, and more recent continuous methods such as Ultrametric Fitting, Hyperbolic Hierarchical Clustering, gradient-based Hyperbolic Hierarchical Clustering, Flexible Probabilistic Hierarchy, and the proposed Expected Probabilistic Hierarchies (EPH) method. The best and second-best scores for each dataset are highlighted.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/tables_21_2.jpg alt></figure></p><blockquote><p>This table compares the Dasgupta costs achieved by using constrained and unconstrained optimization methods on various graph datasets. The number of internal nodes (n&rsquo;) is fixed at 512. The results highlight the impact of enforcing the row-stochasticity constraint on the optimization process, revealing how this constraint affects the quality of the resulting hierarchical clustering as measured by the Dasgupta cost.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/tables_21_3.jpg alt></figure></p><blockquote><p>This table compares the Dasgupta costs obtained using different initialization methods for the EPH model on several graph datasets. The first three rows show the initial Dasgupta costs before training using random, Average Linkage (AL), and Flexible Probabilistic Hierarchy (FPH) methods. The last three rows present the Dasgupta costs after training with the EPH model using each of these initialization methods. The best and second-best results are highlighted in bold and underlined, respectively.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/tables_22_1.jpg alt></figure></p><blockquote><p>This table compares the Dasgupta costs achieved by using direct parametrization versus embedding parametrization for the matrices A and B in the EPH model. It shows that the direct parametrization consistently outperforms the embedding approach except for PolBlogs. The results highlight the trade-off between the simplicity and performance of the direct method compared to the added complexity of embedding, especially when considering that the embedding approach required significantly more training epochs (20000 vs 1000).</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/tables_23_1.jpg alt></figure></p><blockquote><p>This table presents the results of different hierarchical clustering methods on various graph datasets. The methods compared include several linkage algorithms (WL, AL), Louvain, recursive sparsest cut (RSC), Ultrametric Fitting (UF), gradient-based Hyperbolic Hierarchical Clustering (gHHC), Hyperbolic Hierarchical Clustering (HypHC), Flexible Probabilistic Hierarchy (FPH), and the proposed Expected Probabilistic Hierarchies (EPH). The table shows the Dasgupta cost and Tree-sampling divergence for each method on each dataset. Lower Dasgupta cost and higher Tree-sampling divergence indicate better performance.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/tables_24_1.jpg alt></figure></p><blockquote><p>This table presents the standard deviations of the Dasgupta cost and tree-sampling divergence for different graph datasets. The values represent the variability or uncertainty in the results obtained for the different algorithms on each dataset. A higher standard deviation indicates more variability in the results.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/tables_24_2.jpg alt></figure></p><blockquote><p>This table presents a comparison of the Dasgupta cost achieved by different hierarchical clustering methods on eight vector datasets. The Dasgupta cost is a lower-is-better metric that evaluates the quality of a hierarchical clustering. The table shows that EPH consistently achieves the lowest Dasgupta cost across most datasets, indicating superior performance compared to other methods.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/tables_24_3.jpg alt></figure></p><blockquote><p>This table presents a comparison of different hierarchical clustering methods on several graph datasets. The methods compared include various linkage algorithms (WL, AL, Louv, RSC), continuous optimization approaches (UF, gHHC, HypHC, FPH), and the proposed EPH method. For each method and each dataset, the Dasgupta cost and tree-sampling divergence are reported. The best-performing method for each metric and dataset is shown in bold, with the second-best underlined. This allows for a quantitative comparison of the methods across different datasets and metrics.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/tables_24_4.jpg alt></figure></p><blockquote><p>This table presents a comparison of different hierarchical clustering methods on several graph datasets. The methods are evaluated based on two metrics: Dasgupta cost (lower is better) and Tree-sampling divergence (higher is better). The table shows the performance of various methods (WL, AL, Louv, RSC, UF, gHHC, HypHC, FPH, and EPH) across multiple datasets (PolBlogs, Brain, Citeseer, Genes, Cora-ML, OpenFlight, WikiPhysics, and DBLP). The best and second-best performing methods for each dataset and metric are highlighted.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/tables_24_5.jpg alt></figure></p><blockquote><p>This table presents the results of different hierarchical clustering methods on several graph datasets. It shows the Dasgupta cost and Tree-sampling divergence scores for each method. The best-performing method for each dataset and metric is highlighted in bold, while the second-best is underlined. This allows for a comparison of the performance of various algorithms across different datasets and metrics.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/tables_25_1.jpg alt></figure></p><blockquote><p>This table shows the runtime in seconds for various hierarchical clustering algorithms on eight different graph datasets. The number of internal nodes (n&rsquo;) is fixed at 512. The algorithms include standard linkage methods (WL, AL, Louv.), a recursive sparsest cut method (RSC), gradient-based continuous methods (UF, gHHC, HypHC), the Flexible Probabilistic Hierarchy method (FPH), and the proposed Expected Probabilistic Hierarchies method (EPH) and its minimized version. The table helps illustrate the computational efficiency of the different methods, particularly highlighting the runtime of EPH compared to others.</p></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://ai-paper-reviewer.com/fMdrBucZnj/tables_25_2.jpg alt></figure></p><blockquote><p>This table shows the runtime in seconds for different hierarchical clustering algorithms on eight vector datasets. The number of internal nodes (n&rsquo;) used in the algorithms is the minimum between n-1 (where n is the number of data points in the dataset) and 512. The algorithms compared include various linkage methods (WL, AL, SL, CL), Louvain, RSC, UF, gHHC, HypHC, FPH, and EPH. The table provides a detailed comparison of the computational efficiency of each algorithm on various datasets of different sizes.</p></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-75ec204e0a6853617430f481753a98ed class=gallery><img src=https://ai-paper-reviewer.com/fMdrBucZnj/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/fMdrBucZnj/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/fMdrBucZnj/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/fMdrBucZnj/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/fMdrBucZnj/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/fMdrBucZnj/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/fMdrBucZnj/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/fMdrBucZnj/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/fMdrBucZnj/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/fMdrBucZnj/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/fMdrBucZnj/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/fMdrBucZnj/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/fMdrBucZnj/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/fMdrBucZnj/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/fMdrBucZnj/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/fMdrBucZnj/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/fMdrBucZnj/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/fMdrBucZnj/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/fMdrBucZnj/19.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/fMdrBucZnj/20.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/neurips2024/posters/fmdrbucznj/&amp;title=Expected%20Probabilistic%20Hierarchies" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/neurips2024/posters/fmdrbucznj/&amp;text=Expected%20Probabilistic%20Hierarchies" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/neurips2024/posters/fmdrbucznj/&amp;subject=Expected%20Probabilistic%20Hierarchies" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_posters/fMdrBucZnj/index.md",oid_likes="likes_posters/fMdrBucZnj/index.md"</script><script type=text/javascript src=/neurips2024/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/neurips2024/posters/qxidsicaja/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Expert-level protocol translation for self-driving labs</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-09-26T00:00:00+00:00>26 September 2024</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/neurips2024/posters/abzyngwfpn/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Expanding Sparse Tuning for Low Memory Usage</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2024-09-26T00:00:00+00:00>26 September 2024</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2024
AI Paper Reviewer</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/neurips2024/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/neurips2024/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>