[{"figure_path": "fMdrBucZnj/tables/tables_4_1.jpg", "caption": "Table 1: Properties of Soft-Das, Exp-Das, Soft-TSD, and Exp-TSD.", "description": "This table compares four different cost functions used for hierarchical clustering: Soft-Das, Exp-Das, Soft-TSD, and Exp-TSD.  It shows whether each function is a minimization or maximization problem, whether it's convex or concave with respect to the parameters A and B that describe the probabilistic hierarchy, whether it always results in an integral solution, whether its optimum is equal to the optimal discrete solution, and whether the optimal value of the soft-score aligns with the optimal value of the corresponding discrete score.  The table highlights the key theoretical properties of each cost function, indicating their suitability for optimization.", "section": "4.2 Theoretical Analysis of EPH and FPH"}, {"figure_path": "fMdrBucZnj/tables/tables_7_1.jpg", "caption": "Table 2: Results for the graph datasets. Best scores in bold, second best underlined.", "description": "This table presents the results of different hierarchical clustering methods on various graph datasets.  The methods are compared using two metrics: Dasgupta cost and Tree-sampling divergence. Lower Dasgupta cost and higher Tree-sampling divergence indicate better clustering performance.  The table highlights the best performing method for each dataset and metric.", "section": "5.2 Results"}, {"figure_path": "fMdrBucZnj/tables/tables_7_2.jpg", "caption": "Table 3: Dasgupta costs (\u2193) for the vector datasets. Best scores in bold, second best underlined.", "description": "This table presents the Dasgupta costs achieved by different hierarchical clustering methods on eight vector datasets. The Dasgupta cost is a metric used to evaluate the quality of hierarchical clustering results. Lower scores indicate better clustering performance.  The table compares several methods, including traditional linkage-based algorithms (WL, AL, SL, CL),  and more recent methods like Louvain, RSC, UF, gHHC, HypHC, FPH, and the proposed method EPH. The best and second-best results for each dataset are highlighted in bold and underlined, respectively, to show the relative performance of each method.", "section": "5.2 Results"}, {"figure_path": "fMdrBucZnj/tables/tables_8_1.jpg", "caption": "Table 4: Results of EPH for the HSBMs with n\u2032 = #Cluster.", "description": "This table presents the results of the Expected Probabilistic Hierarchies (EPH) method on two Hierarchical Stochastic Block Models (HSBMs) with varying sizes.  It compares the Dasgupta cost and Tree-sampling divergence of the hierarchies generated by EPH against the ground truth (GT) hierarchies.  It also shows the normalized mutual information (NMI) at different levels of the hierarchy, indicating the alignment between the EPH-generated hierarchies and the ground truth.", "section": "5.2 Results"}, {"figure_path": "fMdrBucZnj/tables/tables_8_2.jpg", "caption": "Table 4: Results of EPH for the HSBMs with n\u2032 = #Cluster.", "description": "This table presents the results of the Expected Probabilistic Hierarchies (EPH) method on two Hierarchical Stochastic Block Models (HSBMs), one small and one large.  It compares the performance of EPH against the ground truth (GT) in terms of Dasgupta cost and Tree-Sampling Divergence.  The normalized mutual information (NMI) at different levels of the hierarchy is also included, showing how well EPH recovers the structure of the ground truth HSBMs.", "section": "5.2 Results"}, {"figure_path": "fMdrBucZnj/tables/tables_14_1.jpg", "caption": "Table 6: Dasgupta costs for all combinations of hierarchies and graphs from Fig. 7.", "description": "This table shows the Dasgupta costs for different combinations of hierarchies (T1, T2, and T_I) and graphs (convex and concave examples) from Figure 7 in the paper. The values demonstrate that the Exp-Das function is neither convex nor concave, supporting the paper's claim about the function's properties.", "section": "4.2 Theoretical Analysis of EPH and FPH"}, {"figure_path": "fMdrBucZnj/tables/tables_16_1.jpg", "caption": "Table 7: Overview of the graph datasets.", "description": "This table provides a summary of the graph datasets used in the paper's experiments. For each dataset, it lists the number of nodes (vertices), the number of edges, and the license under which the dataset is available.", "section": "5.1 Experimental Setup"}, {"figure_path": "fMdrBucZnj/tables/tables_16_2.jpg", "caption": "Table 8: Overview of the vector datasets.", "description": "This table presents an overview of the eight vector datasets used in the paper's experiments.  For each dataset, it lists the number of data points, the number of attributes (features) for each data point, the number of classes, and the license under which the dataset is available. The datasets include Zoo, Iris, Glass, Digits, Segmentation, Spambase, Letter, and Cifar-100, representing a variety of data types and sizes.", "section": "5.1 Experimental Setup"}, {"figure_path": "fMdrBucZnj/tables/tables_16_3.jpg", "caption": "Table 9: Overview of the HSBMs.", "description": "This table presents a summary of the characteristics of the two hierarchical stochastic block models (HSBMs) used in the experiments. It shows the number of nodes, edges, and clusters for both the small and large HSBMs.", "section": "5.1 Experimental Setup"}, {"figure_path": "fMdrBucZnj/tables/tables_17_1.jpg", "caption": "Table 10: Overview of the Hyperparameters.", "description": "This table presents the hyperparameters used in the experiments for the different methods: EPH, FPH, HypHC, UF, and DeepWalk. It specifies the learning rate (LR), initialization methods, number of samples used for approximating expectations, temperature parameters for softmax functions, number of epochs for training, number of triplets, loss functions, and embedding dimensions.  Specific hyperparameter settings are noted for different datasets (DBLP, Spambase, Letter, and Cifar-100) reflecting adaptations to variations in dataset properties.", "section": "5.1 Experimental Setup"}, {"figure_path": "fMdrBucZnj/tables/tables_17_2.jpg", "caption": "Table 2: Results for the graph datasets. Best scores in bold, second best underlined.", "description": "This table presents the results of different hierarchical clustering methods on various graph datasets.  The methods are compared based on two metrics: Dasgupta cost and Tree-sampling divergence.  Lower Dasgupta cost and higher Tree-sampling divergence indicate better clustering performance. The table shows that the proposed EPH method achieves the best results in most cases, outperforming various baselines including other state-of-the-art methods.", "section": "5.2 Results"}, {"figure_path": "fMdrBucZnj/tables/tables_18_1.jpg", "caption": "Table 12: Results of FPH for the HSBMs with n'=# Cluster.", "description": "This table presents the results obtained using the Flexible Probabilistic Hierarchy (FPH) method on two synthetic datasets generated using the Hierarchical Stochastic Block Model (HSBM): a small HSBM and a large HSBM. The results are evaluated based on two metrics: the Dasgupta cost and the Tree-sampling divergence. The table also includes the normalized mutual information (NMI) at different levels (Level 1, 2, and 3) of the hierarchy.  The ground truth (GT) values are provided for comparison, allowing for the assessment of FPH's ability to recover the ground truth hierarchy.", "section": "C.1 HSBM Results for FPH"}, {"figure_path": "fMdrBucZnj/tables/tables_21_1.jpg", "caption": "Table 3: Dasgupta costs (\u2193) for the vector datasets. Best scores in bold, second best underlined.", "description": "This table presents the Dasgupta costs achieved by different hierarchical clustering methods on eight vector datasets.  The Dasgupta cost is a lower score is better metric used to evaluate the quality of a hierarchical clustering.  The table compares the performance of various methods including traditional linkage algorithms (WL, AL, SL, CL),  Louvain modularity maximization, recursive sparsest cut, and more recent continuous methods such as Ultrametric Fitting, Hyperbolic Hierarchical Clustering, gradient-based Hyperbolic Hierarchical Clustering, Flexible Probabilistic Hierarchy, and the proposed Expected Probabilistic Hierarchies (EPH) method. The best and second-best scores for each dataset are highlighted.", "section": "5.2 Results"}, {"figure_path": "fMdrBucZnj/tables/tables_21_2.jpg", "caption": "Table 14: Dasgupta costs for constrained and unconstrained optimization on several graph datasets with n' = 512 internal nodes.", "description": "This table compares the Dasgupta costs achieved by using constrained and unconstrained optimization methods on various graph datasets.  The number of internal nodes (n') is fixed at 512.  The results highlight the impact of enforcing the row-stochasticity constraint on the optimization process, revealing how this constraint affects the quality of the resulting hierarchical clustering as measured by the Dasgupta cost.", "section": "5.2 Results"}, {"figure_path": "fMdrBucZnj/tables/tables_21_3.jpg", "caption": "Table 15: Dasgupta costs for different initializations on several graph datasets with n' = 512 internal nodes. In the first three rows the initial Dasgupta costs and in the last three rows the Dasgupta costs after the training. Best scores in bold, second best underlined.", "description": "This table compares the Dasgupta costs obtained using different initialization methods for the EPH model on several graph datasets. The first three rows show the initial Dasgupta costs before training using random, Average Linkage (AL), and Flexible Probabilistic Hierarchy (FPH) methods. The last three rows present the Dasgupta costs after training with the EPH model using each of these initialization methods. The best and second-best results are highlighted in bold and underlined, respectively.", "section": "5.2 Results"}, {"figure_path": "fMdrBucZnj/tables/tables_22_1.jpg", "caption": "Table 16: Dasgupta costs for the direct and embedding parametrization on several graph datasets with n' = 512 internal nodes. Best scores in bold, second best underlined.", "description": "This table compares the Dasgupta costs achieved by using direct parametrization versus embedding parametrization for the matrices A and B in the EPH model.  It shows that the direct parametrization consistently outperforms the embedding approach except for PolBlogs. The results highlight the trade-off between the simplicity and performance of the direct method compared to the added complexity of embedding, especially when considering that the embedding approach required significantly more training epochs (20000 vs 1000).", "section": "5.2 Results"}, {"figure_path": "fMdrBucZnj/tables/tables_23_1.jpg", "caption": "Table 2: Results for the graph datasets. Best scores in bold, second best underlined.", "description": "This table presents the results of different hierarchical clustering methods on various graph datasets.  The methods compared include several linkage algorithms (WL, AL), Louvain, recursive sparsest cut (RSC), Ultrametric Fitting (UF), gradient-based Hyperbolic Hierarchical Clustering (gHHC), Hyperbolic Hierarchical Clustering (HypHC), Flexible Probabilistic Hierarchy (FPH), and the proposed Expected Probabilistic Hierarchies (EPH).  The table shows the Dasgupta cost and Tree-sampling divergence for each method on each dataset.  Lower Dasgupta cost and higher Tree-sampling divergence indicate better performance.", "section": "5.2 Results"}, {"figure_path": "fMdrBucZnj/tables/tables_24_1.jpg", "caption": "Table 18: Standard Deviations for the graph datasets.", "description": "This table presents the standard deviations of the Dasgupta cost and tree-sampling divergence for different graph datasets.  The values represent the variability or uncertainty in the results obtained for the different algorithms on each dataset.  A higher standard deviation indicates more variability in the results.", "section": "5.2 Results"}, {"figure_path": "fMdrBucZnj/tables/tables_24_2.jpg", "caption": "Table 3: Dasgupta costs (\u2193) for the vector datasets. Best scores in bold, second best underlined.", "description": "This table presents a comparison of the Dasgupta cost achieved by different hierarchical clustering methods on eight vector datasets.  The Dasgupta cost is a lower-is-better metric that evaluates the quality of a hierarchical clustering.  The table shows that EPH consistently achieves the lowest Dasgupta cost across most datasets, indicating superior performance compared to other methods.", "section": "5.2 Results"}, {"figure_path": "fMdrBucZnj/tables/tables_24_3.jpg", "caption": "Table 2: Results for the graph datasets. Best scores in bold, second best underlined.", "description": "This table presents a comparison of different hierarchical clustering methods on several graph datasets.  The methods compared include various linkage algorithms (WL, AL, Louv, RSC), continuous optimization approaches (UF, gHHC, HypHC, FPH), and the proposed EPH method.  For each method and each dataset, the Dasgupta cost and tree-sampling divergence are reported.  The best-performing method for each metric and dataset is shown in bold, with the second-best underlined. This allows for a quantitative comparison of the methods across different datasets and metrics.", "section": "5.2 Results"}, {"figure_path": "fMdrBucZnj/tables/tables_24_4.jpg", "caption": "Table 2: Results for the graph datasets. Best scores in bold, second best underlined.", "description": "This table presents a comparison of different hierarchical clustering methods on several graph datasets.  The methods are evaluated based on two metrics: Dasgupta cost (lower is better) and Tree-sampling divergence (higher is better).  The table shows the performance of various methods (WL, AL, Louv, RSC, UF, gHHC, HypHC, FPH, and EPH) across multiple datasets (PolBlogs, Brain, Citeseer, Genes, Cora-ML, OpenFlight, WikiPhysics, and DBLP). The best and second-best performing methods for each dataset and metric are highlighted.", "section": "5.2 Results"}, {"figure_path": "fMdrBucZnj/tables/tables_24_5.jpg", "caption": "Table 2: Results for the graph datasets. Best scores in bold, second best underlined.", "description": "This table presents the results of different hierarchical clustering methods on several graph datasets.  It shows the Dasgupta cost and Tree-sampling divergence scores for each method. The best-performing method for each dataset and metric is highlighted in bold, while the second-best is underlined.  This allows for a comparison of the performance of various algorithms across different datasets and metrics.", "section": "5.2 Results"}, {"figure_path": "fMdrBucZnj/tables/tables_25_1.jpg", "caption": "Table 20: Runtime in seconds for the graph datasets with n' = 512 internal nodes.", "description": "This table shows the runtime in seconds for various hierarchical clustering algorithms on eight different graph datasets.  The number of internal nodes (n') is fixed at 512. The algorithms include standard linkage methods (WL, AL, Louv.), a recursive sparsest cut method (RSC), gradient-based continuous methods (UF, gHHC, HypHC), the Flexible Probabilistic Hierarchy method (FPH), and the proposed Expected Probabilistic Hierarchies method (EPH) and its minimized version.  The table helps illustrate the computational efficiency of the different methods, particularly highlighting the runtime of EPH compared to others.", "section": "5.2 Results"}, {"figure_path": "fMdrBucZnj/tables/tables_25_2.jpg", "caption": "Table 21: Runtime in seconds for the vector datasets with n' = min{n \u2013 1,512} internal nodes.", "description": "This table shows the runtime in seconds for different hierarchical clustering algorithms on eight vector datasets.  The number of internal nodes (n') used in the algorithms is the minimum between n-1 (where n is the number of data points in the dataset) and 512. The algorithms compared include various linkage methods (WL, AL, SL, CL), Louvain, RSC, UF, gHHC, HypHC, FPH, and EPH. The table provides a detailed comparison of the computational efficiency of each algorithm on various datasets of different sizes.", "section": "5.2 Results"}]