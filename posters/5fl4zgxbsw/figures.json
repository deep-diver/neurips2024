[{"figure_path": "5Fl4zgXbsW/figures/figures_1_1.jpg", "caption": "Figure 1: (a) The score report provided by GRE and an example to show that a low MSE cannot guarantee the correct ranking of students' testing results. (b) This line chart shows the performance of previous CAT methods in ranking, and it can be seen that the method that performs state-of-the-art (BECAT) in accuracy may only achieve the effect of random selection in ranking.", "description": "The figure demonstrates that minimizing the mean squared error (MSE) in ability scores does not guarantee accurate student ranking.  Subfigure (a) shows a GRE score report and an example illustrating this point. Subfigure (b) compares several state-of-the-art Computerized Adaptive Testing (CAT) methods and shows that even the best-performing methods in terms of MSE may perform no better than random guessing when it comes to student ranking.", "section": "1 Introduction"}, {"figure_path": "5Fl4zgXbsW/figures/figures_3_1.jpg", "caption": "Figure 2: The structure of CCAT framework. CCAT consists of two parts: question selection and ability estimation. The question selection part utilizes the performance of collaborative students in answering various questions to select appropriate questions for the tested student, and the ability estimation part ranks the tested student with collaborative students and uses the ranking as the test result.", "description": "The figure illustrates the CCAT framework, which has two main parts: question selection and ability estimation. In the question selection part, the performance of collaborative students on various questions is used to select suitable questions for the tested student.  The ability estimation part then ranks the tested student against the collaborative students, using this ranking as the final test score.  This collaborative approach aims to improve the accuracy and consistency of student ranking in computerized adaptive testing.", "section": "4 The CCAT Framework"}, {"figure_path": "5Fl4zgXbsW/figures/figures_8_1.jpg", "caption": "Figure 3: The performance on ACC and AUC of different question selection algorithms on the dataset NIPS-EDU for the IRT model estimated by MCMC and GD methods.", "description": "This figure shows the accuracy (ACC) and area under the ROC curve (AUC) of different question selection algorithms (Random, FSI, KLI, MAAT, NCAT, BECAT, CCAT, BOBCAT) on the NIPS-EDU dataset.  The performance is evaluated using the Item Response Theory (IRT) model, with parameters estimated by both Markov Chain Monte Carlo (MCMC) and Gradient Descent (GD) methods. The x-axis represents the test step, and the y-axis represents the ACC and AUC values.  This visualization helps to compare the effectiveness of various question selection algorithms in terms of both accuracy and ranking performance of students.", "section": "5 Experiments"}, {"figure_path": "5Fl4zgXbsW/figures/figures_9_1.jpg", "caption": "Figure 4: Visualization of differences in abilities estimation by IRT method and CCAT method is more appropriate for IRT parameter estimation than the GD method, particularly when considering the ranking consistency of CAT.", "description": "This figure visualizes the differences in ability estimation between the IRT method and the CCAT method for student pairs from the NIPS-EDU and JUNYI datasets. Each cell represents the difference in estimated ability between two students in a pair.  Redder colors indicate larger differences, and the visualization helps demonstrate the improved ranking consistency achieved with CCAT, especially when comparing the results of IRT estimation done with GD and MCMC methods. The results suggest that CCAT is superior to IRT with GD when considering ranking consistency.", "section": "5 Experiments"}, {"figure_path": "5Fl4zgXbsW/figures/figures_18_1.jpg", "caption": "Figure 3: The performance on ACC and AUC of different question selection algorithms on the dataset NIPS-EDU for the IRT model estimated by MCMC and GD methods.", "description": "This figure shows the performance of different question selection algorithms (Random, FSI, KLI, MAAT, NCAT, BECAT, and CCAT) on the NIPS-EDU dataset.  The performance is measured using two metrics: Accuracy (ACC) and Area Under the Curve (AUC).  The results are shown separately for two methods of estimating Item Response Theory (IRT) parameters: Markov Chain Monte Carlo (MCMC) and Gradient Descent (GD).  This allows for a comparison of the algorithms' performance under different IRT estimation methods. The x-axis shows the test step (number of questions asked) and the y-axis represents the ACC or AUC values.  The figure helps demonstrate the effectiveness of different question selection techniques and how their performance is affected by the method used for IRT parameter estimation.", "section": "5 Experiments"}, {"figure_path": "5Fl4zgXbsW/figures/figures_18_2.jpg", "caption": "Figure 4: Visualization of differences in abilities estimation by IRT method and CCAT method is more appropriate for IRT parameter estimation than the GD method, particularly when considering the ranking consistency of CAT.", "description": "This figure visualizes the differences in ability estimation between the IRT method and the CCAT method for pairs of students.  It demonstrates that CCAT offers superior discrimination, especially when the goal is ranking consistency in CAT. The visualization uses a heatmap to show the difference in estimated ability between pairs of students, highlighting instances where the ranking is inconsistent (indicated by red).", "section": "5 Experiments"}, {"figure_path": "5Fl4zgXbsW/figures/figures_19_1.jpg", "caption": "Figure 4: Visualization of differences in abilities estimation by IRT method and CCAT method is more appropriate for IRT parameter estimation than the GD method, particularly when considering the ranking consistency of CAT.", "description": "This figure visualizes the differences in ability estimation between the IRT method and the CCAT method for student pairs. It shows that the CCAT method provides better discrimination in ability estimation, particularly when the ranking consistency of CAT is taken into account. This improvement is achieved by using collaborative students to assist in ranking test-takers.", "section": "5 Experiments"}]