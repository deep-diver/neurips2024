{"references": [{"fullname_first_author": "J. Devlin", "paper_title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2019-06-01", "reason": "This paper introduces BERT, a foundational model for many NLP tasks, which is later converted to an SNN in this work."}, {"fullname_first_author": "A. Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduces the transformer architecture, the basis of many modern large language models, and whose SNN conversion is a central focus of this work."}, {"fullname_first_author": "Z. Liu", "paper_title": "Swin Transformer: Hierarchical vision transformer using shifted windows", "publication_date": "2021-10-01", "reason": "This paper introduces Swin Transformer, a vision transformer that is directly converted to an SNN for image classification."}, {"fullname_first_author": "S. Kim", "paper_title": "I-BERT: Integer-only BERT quantization", "publication_date": "2021-07-01", "reason": "This paper presents a quantization method for BERT that is relevant to the SNN conversion process."}, {"fullname_first_author": "B. Han", "paper_title": "RMP-SNN: Residual membrane potential neuron for enabling deeper high-accuracy and low-latency spiking neural network", "publication_date": "2020-06-01", "reason": "This paper is a significant prior work on SNNs that is directly compared with in this work."}]}