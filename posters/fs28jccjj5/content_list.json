[{"type": "text", "text": "SpikedAttention: Training-Free and Fully Spike-Driven Transformer-to-SNN Conversion with Winner-Oriented Spike Shift for Softmax Operation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sangwoo Hwang Electrical Engineering and Computer Science Daegu Gyeongbuk Institute of Science and Technology (DGIST) nemesis0523@dgist.ac.kr ", "page_idx": 0}, {"type": "text", "text": "Seunghyun Lee School of Electrical Engineering Korea University coder@korea.ac.kr ", "page_idx": 0}, {"type": "text", "text": "Dahoon Park   \nSchool of Electrical Engineering Korea University   \nmanyteacher93@korea.ac.kr ", "page_idx": 0}, {"type": "text", "text": "Donghun Lee School of Electrical Engineering Korea University dhleeids@korea.ac.kr ", "page_idx": 0}, {"type": "text", "text": "Jaeha Kung\u2217 School of Electrical Engineering Korea University jhkung@korea.ac.kr ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Event-driven spiking neural networks (SNNs) are promising neural networks that reduce the energy consumption of continuously growing AI models. Recently, keeping pace with the development of transformers, transformer-based SNNs were presented. Due to the incompatibility of self-attention with spikes, however, existing transformer-based SNNs limit themselves by either restructuring self-attention architecture or conforming to non-spike computations. In this work, we propose a novel transformer-to-SNN conversion method that outputs an end-to-end spikebased transformer, named SpikedAttention. Our method directly converts the well-trained transformer without modifying its attention architecture. For the vision task, the proposed method converts Swin Transformer into an SNN without post-training or conversion-aware training, achieving state-of-the-art SNN accuracy on ImageNet dataset, i.e., $80.0\\%$ with $28.7\\mathbf{M}$ parameters. Considering weight accumulation, neuron potential update, and on-chip data movement, SpikedAttention reduces energy consumption by $42\\%$ compared to the baseline ANN, i.e., Swin-T. Furthermore, for the first time, we demonstrate that SpikedAttention successfully converts a BERT model to an SNN with only $0.3\\%$ accuracy loss on average consuming $58\\%$ less energy on GLUE benchmark. Our code is available at Github ( https://github.com/sangwoohwang/SpikedAttention ). ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recently, many practical AI applications such as conversational question answering (1), intelligent code completion (2), weather forecasting (3), and text-to-image generation (4) have rapidly evolved with the advances in artificial neural networks (ANNs) based on transformer architecture. Transformers are powered by the self-attention mechanism, which extracts long-range dependencies between input tokens, e.g., words (5), image patches (6), or both (7). However, deploying state-of-the-art transformers requires a large amount of computations and a huge memory footprint. For instance, generating a single token on a GPT3-175B model requires at least 350 GFLOPs (8). ", "page_idx": 0}, {"type": "image", "img_path": "fs28jccJj5/tmp/d95c730381646701b76e72152b41d9ba13f14d15514882c931a775887b380b56.jpg", "img_caption": ["Figure 1: (a) Accuracy, energy consumption (refer to Appendix A.1), and parameter size of the propsoed SpikedAttention and other spike-based SNNs on ImageNet classification (15). (b) The structure of the proposed fully spike-based attention module (more details are in Section 4). "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In order to reduce the ever-growing computational overhead of transformers, there were some efforts to improve their energy efficiency by utilizing spiking neural networks (SNNs). The major strength of an SNN, which mimics a biological neuron, is that only weight accumulations (ACs) are required when input spikes are encountered (event-driven), instead of power-hungry multiplyaccumulate (MAC) operations in ANNs. The energy consumption of a 32-bit MAC is $5\\times$ higher than that of a 32-bit accumulation (9). To exploit the high energy efficiency of SNNs, many CNN models based on SNNs (10; 11) have been proposed, and recently, SNN-based transformers have emerged. Implementing an SNN-based transformer is done either by direct training via a surrogate gradient (12; 13) or converting a well-trained transformer to an SNN (14). ", "page_idx": 1}, {"type": "text", "text": "When implementing the attention module in a spike-based transformer, there are two types of matrix multiplication. One is associated with generating $Q,K$ , and $V$ where static weight matrices $(W_{Q}$ , $W_{K}$ , and $W_{V}$ ) are used. In this case, when an input neuron fires, its associated weight value gets accumulated to the output neuron\u2019s potential. Another type is the multiplication between dynamically generated matrices, e.g., $Q\\cdot K^{T}$ . In previous works (12; 14), logical AND operations are performed between two \u201crate-coded\" spike trains, e.g., $S_{Q(i,k)}$ and $S_{K(j,k)}$ , for each dot product. However, due to the probabilistic nature of the rate coding, it requires a long timestep $T>128)$ to maintain high ANN-to-SNN conversion accuracy (14). Unfortunately, this translates to high energy consumption in running SNNs (refer to MST in Fig. 1). ", "page_idx": 1}, {"type": "text", "text": "Another challenge with realizing spike-based transformers is the softmax operation. In the field of ANN quantization, there have been many efforts to approximate softmax operations (16; 17). I-BERT (16) approximates the exponential function with a second-order polynomial, successfully converting it to an integer operation (i-exp), and FQ-ViT (17) combines $i^{\\th}$ -exp with logarithmic quantization. In SNN-based transformers, however, softmax has not been converted to a spike-based operation due to the presence of exponential functions. Since SNNs have binary inputs and outputs, previous approaches in ANN quantization cannot be directly applied. ", "page_idx": 1}, {"type": "text", "text": "Our Contribution: In this work, we present an end-to-end spike-based transformer, named SpikedAttention, solving the abovementioned challenges to achieve state-of-the-art accuracy ${\\left.\\right.}0.6\\%$ accuracy loss compared to ANN) among spike-based transformers (Fig. 1(a)). In addition, it consumes the minimum energy, which is $42\\%$ lower than the original Swin Transformer (6), for image classification task. Furthermore, for the first time, we demonstrate that a BERT model can be successfully converted to an SNN, i.e., SpikedAttention, with negligible accuracy loss ( $0.3\\%$ on average) while consuming $58\\%$ less energy on GLUE benchmark. Note that the SpikedAttention does not require any additional training since it is created by the direct transformer-to-SNN conversion method thanks to the proposed fully spike-based attention module (Fig. 1(b)). The main contributions are summarized below: ", "page_idx": 1}, {"type": "image", "img_path": "fs28jccJj5/tmp/1104bd64428931f4e6a008f137a0b1c1c3dd5ad18b0e2c71534b5f24f991deae.jpg", "img_caption": ["Figure 2: The computation of attention in previous SNN-based transformers (a) with the softmax (14), and (b) without the softmax operation (12). Both involve non-spike computations. Total timestep $T$ of each spike tensor is set to 1 for simplicity. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u2022 Fully spike-based transformer: SpikedAttention encodes external inputs and all intermediate features in spikes. Especially, each element in intermediate features is encoded as a single spike for a given timestep $T$ to minimize the energy consumption. \u2022 Trace-driven matrix multiplication: To reduce the required timestep $T$ in performing multiplication between two dynamically generated matrices, we propose a trace-driven matrix multiplication. Owing to the reduced timestep, we can reduce energy consumption. \u2022 Exponent-free spike-based softmax: We present a winner-oriented spike shift that controls the output spike timing to realize the spike-based softmax. This allows us to keep the vanilla self-attention architecture of the original transformer to maintain high accuracy. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Most SNN studies have focused on replacing CNNs to improve energy efficiency in vision tasks (10; 18). With the recent advent of transformers, there have been increasing efforts to convert transformers into SNNs (14) or directly train spike-based transformers (12; 13; 19; 20). When realizing a spikebased transformer, self-attention is the major hurdle in converting the transformer to a fully spikedriven network. Here, \u201cfully spike-driven\" means that no real-valued multiplications or nonlinear functions are involved in any operations. Fig. 2(a) illustrates the required operations in vanilla self-attention (VSA) (5) but with $Q,K$ , and $V$ in the form of spikes (14). ", "page_idx": 2}, {"type": "text", "text": "In the original VSA module, real-valued multiplications $(Q\\cdot K^{T}$ and $A t t n\\cdot V)$ and a softmax operation $(A t t n=s o f t m a x(Q\\cdot K^{T}/\\sqrt{d}))$ are required. In MST (14), the authors encode $Q,K$ , and $V$ as spike trains (denoted by $S_{Q}$ , $S_{K}$ , and $S_{V}$ in Fig. 2(a)). By doing so, the matrix multiplication $(Q\\cdot K^{T})$ simply becomes AND operations followed by floating-point accumulations. To convey meaningful information (in the form of spikes) to the next layer, however, a long timestep $T\\geq128$ is required in (14). This is due to the fact that the simultaneous firing of spikes at both $S_{Q}$ and $S_{K^{T}}$ is rare. This problem gets worse for temporal coding or phase coding, which produces much fewer spikes than the rate coding. Moreover, in MST, the complex softmax operations are done to compute attention maps, which requires non-spike computations (i.e., expensive exponential functions). ", "page_idx": 2}, {"type": "text", "text": "To remove the burden of computing the softmax, some previous studies on direct training of SNNs for transformers have restructured the attention module (12; 13). As shown in Fig. 2(b), Spikformer (12) removes the softmax operation since $'S_{Q}\\cdot S_{K^{T}}$ \u2019 or $^{\\bullet}S_{K^{T}}\\cdot S_{V}$ \u2019 only produces non-negative values. There was another attempt to modify the attention module, such as replacing ${}^{\\prime}S_{Q}\\cdot S_{K^{T}}\\,\\,\\,$ \u2019 with a Hadamard product followed by the column summation to obtain a mask vector that masks out less important channels in $S_{V}$ (13). Since these prior works restructure the network topology, they need to train the model from scratch with surrogate gradients to improve the performance. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Neuron Model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "There has been a long-standing effort in neuroscience to mathematically model biological neurons (21; 22). Neuron models with high biological plausibility are computationally expensive, while models with low biological similarity are energy efficient. Due to a high volume of parameters and significant computations involved in ANNs, energy efficiency has become more of a priority than biological plausibility when replacing them with SNN counterparts. Therefore, the most energy efficient neuron model, i.e., the leaky integrated-and-fire (LIF) model (23), is widely used. The LIF neuron model is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v_{j}(t)=\\lambda v_{j}(t-1)+\\displaystyle\\sum_{i=1}^{N_{p r e}}w_{i j}S_{i}(t),}\\\\ &{S_{j}(t)=1,\\quad v_{j}(t)=v_{j}(t)-V_{\\theta}\\,\\,\\,\\mathrm{when}\\quad v_{j}(t)\\geq V_{\\theta},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $i$ or $j$ is the index of a pre- or post-synaptic neuron, $v_{j}$ is the potential of the neuron $j$ , and $\\lambda$ is the leak factor of potential that causes the $v_{j}$ to gradually decay as the time $t$ proceeds. The $w_{i j}$ is the synaptic weight between the neuron $i$ and $j$ converted from a pre-trained ANN or directly trained. The $S_{i}(t)$ represents the binary spike (0 or 1) from the neuron $i$ at time $t$ . The $N_{p r e}$ is the number of pre-synaptic neurons. The pre-synaptic neurons that generate spikes activate the weight accumulation on $v_{j}(t)$ . When the $v_{j}(t)$ exceeds the threshold $V_{\\theta}$ , the neuron $j$ fires a spike $(\\bar{S_{j}}(t)=1)$ and its potential resets to $v_{j}(t)-V_{\\theta}$ . Note that decreasing the potential by a threshold instead of resetting it to zero is a common technique in ANN-to-SNN conversion methods (10; 24). ", "page_idx": 3}, {"type": "text", "text": "3.2 Spike Coding Schemes ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To achieve higher SNN performance, it is essential to determine how real-valued inputs are encoded into a sequence of spikes, known as \u2018spike coding\u2019. The spike coding is a scheme that determines whether to generate a spike at time $t$ when converting continuous values into a set of spikes. The most widely used coding scheme is rate coding, which generates multiple spikes at each neuron via Poisson sampling over pre-defined timestep $T$ (10; 25). The rate coding generates more spikes as it converts larger values into spikes. However, increasing the number of spikes and the timestep for higher resolution leads to higher energy consumption due to more weight accumulations in Eq. (1). ", "page_idx": 3}, {"type": "text", "text": "Unlike the rate coding, which generates multiple spikes at each neuron, temporal coding generates a single spike per neuron. In the temporal coding, each spike represents a different value at a different spike time $t$ for a given total timestep $T$ . The spike at an earlier time represents a larger value. For instance, an earlier spike can represent a linearly larger value as $\\textstyle{\\frac{T-t}{T}}$ (11; 26) or an exponentially larger value as e T (27). The temporal coding is energy efficient but requires a long timestep $T$ to increase the precision/resolution. ", "page_idx": 3}, {"type": "text", "text": "The alternative coding scheme, known as phase coding (28; 29), combines the characteristics of the temporal coding and the rate coding. Like the temporal coding, the phase coding assigns each timestep $t$ a different value, i.e., $B^{-t}$ , where $B$ is the base of the phase. As proven in Appendix B, the phase coding can be realized by replacing the leak factor $(\\lambda)$ of the widely used LIF neuron model in Eq. (1) with $B$ , making the spike at one timestep earlier has the $B\\times$ larger impact on the potential increase. When $B=2$ , we call it a binary coding. Unlike the temporal coding, the phase coding allows multiple spikes when encoding a value. For a fixed (total) timestep $T$ , the phase coding provides a higher resolution than other coding schemes. Recently proposed one-spike phase coding maximizes the energy efficiency, where the sequence of spikes is approximated by the first spike (30). Note that the neuron ceases to update its potential after the first spike being generated because it is unnecessary to fire the following spikes at the cost of approximation error. In the one-spike phase coding, $V_{\\theta}$ is tuned to the midpoint between two consecutive phase-based weights, i.e., $\\dot{\\left(B^{-\\dot{t}}+B^{-t-1}\\right)}\\check{/}2^{\\flat}$ , to reduce the approximation error. The authors in (30) have proposed a technique to reduce the approximation error by decreasing the base $B$ of phase coding. However, decreasing the base is only guaranteed to reduce the error when $T$ is large enough. In SpikedAttention, therefore, we use the one-spike phase coding by carefully selecting the proper base $B$ and total timestep $T$ to balance well between the accuracy and the energy efficiency. The details on selecting $B$ and $T$ are presented in Appendix E. ", "page_idx": 3}, {"type": "image", "img_path": "fs28jccJj5/tmp/90fc2670819ffbe9a01b32a1560e4acda47e5ce784aedb55fc513614a9dcd06e.jpg", "img_caption": ["Figure 3: The proposed trace-driven dot product in SpikedAttention. A global trace decays by its base $(B)$ at each timestep and the trace is transferred to each neuron\u2019s local memory when its first spike, from either $S_{Q}$ or $S_{K}$ , is observed. The $\\mathcal{V}(Q K^{T})(1,1)$ is the potential of the neuron $(Q K^{T})_{1,1}$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "4 Fully Spike-based Attention: SpikedAttention ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The main goal of designing the SpikedAttention was to encode external inputs and all intermediate features by binary spikes without altering the VSA structure at all. Prior work on spike-based vision transformers (12; 13; 14) feed in floating-point inputs, i.e., 2D images, to the patch partition module instead of binary spikes. Also, the nonlinear nature of softmax and the high sparsity in $S_{Q}$ , $S_{K}$ , and $S_{V}$ make things more challenging in making fully spike-based transformers. In the proposed SpikedAttention, all intermediate neurons fire a single spike using the one-spike phase coding $(B<2)$ . Only the external input allows multiple spikes with the binary coding, i.e., phase coding with $B=2$ . Until now, previous transformer-to-SNN conversion methods (14) could not achieve fully spike-based computations due to softmax operations, and they require a long timestep due to extremely high sparsity after AND operations between two spike-based vectors. The direct SNN training methods (12; 13) simplified the VSA structure by removing softmax, which degrades the performance. Therefore, we present two novel techniques to (i) minimize the spike timestep by performing trace-driven matrix multiplication and (ii) exponent-free spike-based softmax to realize the transformer-to-SNN conversion method without any training. ", "page_idx": 4}, {"type": "text", "text": "4.1 Trace-driven Matrix Multiplication ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We focus on developing an efficient spike-based computing scheme for the multiplication between two dynamically generated spike-based matrices. We utilize a global trace that tracks the phase at time $t$ to consider the values associated with every spike. With the rate coding, probabilistic multiplications are performed with logical AND between two spike trains. However, since we use one-spike phase coding, each spike must carry its information to the next layer. Consider an example of performing $S_{Q}\\cdot S_{K^{T}}$ . The output value at $(i,j)$ , denoted as $(Q K^{T})_{i,j}$ , is computed by the dot product which is $\\begin{array}{r}{\\sum_{k}S_{Q(i,k)}S_{K(j,k)}}\\end{array}$ . Each spike train in $S_{Q}$ (or $S_{K}$ ) consists of only one spike which represents  the value $B^{-t_{Q(i,k)}}$ , where $t_{Q(i,k)}$ is the spike time of $S_{Q(i,k)}$ and $B$ is the base. Thus, the dot product output should become $\\begin{array}{r}{\\sum_{k}\\stackrel{}{B}{\\stackrel{}{-t}}\\!\\!_{Q(i,k)}\\,\\cdot\\,B^{-t_{K(j,k)}}}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "As mentioned earlier, our work exploits the \u2018spike trace model\u2019 to record the value associated with the spike that fires first among the neuron pair, e.g., $S_{Q(i,k)}$ and $S_{K(j,k)}$ . In neuroscience research, spike trace is a popular method to determine the correlation between connected neurons by relative spike timing (31). Typically, the trace of a neuron $\\mathbf{\\nabla}n\\,\\mathbf{\\dot{\\omega}}$ , i.e., $x_{n}(t)$ , reflects the history of spikes during timestep $T$ . We simplify the trace model to globally track the value $B^{-t}$ at spike time $t$ , which is ", "page_idx": 4}, {"type": "equation", "text": "$$\nx^{g}(t)=x^{g}(t-1)/B,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "image", "img_path": "fs28jccJj5/tmp/fe706e5eec9c4ca420de2009be30977c2a547034520c421193f9fea638c1d477.jpg", "img_caption": ["Figure 4: The proposed winner-oriented spike shift (WOSS) for approximating softmax. (a) Each WOSS neuron increases its potential $(V_{W O S S_{i}})$ by the incoming input spike $S_{\\mathbf{z}(i)}(t)$ . The first (winner) spike among $S_{\\mathbf{z}}(t)$ activates a (global) inhibitory neuron which depresses all neurons. (b) Potential at neuron $i$ (winner) and its corresponding output spike at $t=T+0$ . We are shifting the first spike time from $t=1$ to $t=0$ . The following spikes are time-shifted by the same amount. (c) Potential at neuron $j$ (non-winner) and its corresponding output spike at $t=T+2$ (effectively, $t=2$ ) due to the global inhibition and the threshold shift. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "where $x^{g}(t)$ is the global trace which is initialized to 1 at $t=0$ . With the one-spike neuron model, when a neuron encounters the first spike at $t$ , we store $B^{-t}$ in its local memory as its own trace $x_{n}(t)$ , which will be used later to compute the dot product. We intentionally leveraged the trace-driven method since neuromorphic chips such as Loihi (32) already have the hardware module, e.g., trace memory, for updating neuron traces for widely used STDP learning. ", "page_idx": 5}, {"type": "text", "text": "Fig. 3 illustrates the process of a dot product for computing $(Q K^{T})_{1,1}$ with spike traces. If $S_{Q(1,1)}$ generates a spike earlier $\\mathit{\\Theta}^{\\mathit{\\Theta}}(t=3)$ ) than $S_{K(1,1)}$ , the trace value $B^{-3}$ is stored in the local memory of the neuron $S_{Q(1,1)}$ . When the neuron $S_{K(1,1)}$ exceeds the threshold and generates the spike at $t=4$ the trace of $S_{Q(1,1)}\\,(=B^{-3})$ propagates to update the potential of $(Q K^{T})_{1,1}$ . Since the current phase is $B^{-4}$ at $t=4$ , propagation of $B^{-3}$ at $t=4$ (colored in red) is equivalent to $\\phantom{+}B^{-4}\\times B^{-3}$ \u2019. There might be a situation in which multiple neuron pairs observe the second spike simultaneously. For instance, $S_{Q(1,2)}\u2013S_{K(1,2)}$ and $S_{Q(1,3)}\u2013S_{K(1,3)}$ pairs have the second spike at $t=3$ (it may coincide with the first spike like the $S_{Q(1,3)}\u2013S_{K(1,3)}$ pair). Thus, at $t=3$ , both pairs already have their stored trace for one of two neurons ( $\\boldsymbol{S_{Q}}$ or $S_{K}$ ) that fired the earlier spike in the pair. Then, pre-stored traces from those pairs that share the second spike time will be accumulated together, i.e., $(B^{-2}+B^{-3})$ in this case (colored in blue). The accumulated traces are weighted by the phase of the current timestep, i.e., $B^{-3}$ . By accumulating traces first, we can bound the number of multiplications to the timestep $T$ of the neuron model irrespective of the number of input neurons. Usually, an input spike tensor $S_{Q}$ or $S_{K}$ lies in $\\mathbb{R}^{N\\times D\\times T}$ where $N$ is the number of image patches (or token length), $D$ is the embedding dimension, and $T$ is the total timestep. The accumulations happen across the dimension $D$ , making the proposed approach energy efficient when $T\\ll D$ , which is the general case for transformers. ", "page_idx": 5}, {"type": "text", "text": "4.2 Winner-Oriented Spike Shift (WOSS) for Softmax ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The main issue with the typical softmax operation is the presence of exponential functions. Our work uses the normalized softmax similar to (33), which is expressed as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\sigma(z_{i})=\\frac{\\exp(z_{i})}{\\sum_{j=0}^{N-1}\\exp(z_{j}))}=\\frac{\\frac{\\exp(z_{i})}{\\exp(\\operatorname*{max}(\\mathbf{z}))}}{\\sum_{j=0}^{N-1}\\frac{\\exp(z_{j})}{\\exp(\\operatorname*{max}(\\mathbf{z}))}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{z}\\in\\mathbb{R}^{N}$ and $\\sigma(z_{i})$ is the softmax output with respect to the $i^{\\mathrm{th}}$ neuron. Our SNN approximates xpe(xmp(azxi()z)) by using the proposed WOSS neuron model and scales the threshold (V\u03b8) of the following layer by their sum ( j=0 exp(maxj(z))) , leading to the final softmax result. Note that multiplying the threshold by the sum term has the effect of dividing inputs by the same amount. ", "page_idx": 5}, {"type": "text", "text": "Since SpikedAttention generates a single spike per neuron, the objective is to approximateexpe(xmp(azxi()z)) with $B^{-t_{i}}\\in[0,1]$ . Before converting it to a spike-based representation, we extract the maximum value of $\\mathbf{z}$ , i.e., $M_{\\mathbf{z}}$ , by running a proxy training dataset. Then, by setting $z_{i}=M_{\\mathbf{z}}z_{i}^{\\prime}$ and by applying $\\log_{B}$ on the normalized exponential term, it can be expressed as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\log_{B}(\\frac{\\exp(M_{\\mathbf{z}}z_{i}^{\\prime})}{\\exp(\\operatorname*{max}(M_{\\mathbf{z}}\\mathbf{z}^{\\prime}))})=\\frac{(z_{i}^{\\prime}-\\operatorname*{max}(\\mathbf{z}^{\\prime}))M_{\\mathbf{z}}}{\\log_{e}B}\\approx-t_{i},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $t_{i}$ is the required spike time at neuron $i$ to approximate the left-hand side of Eq. (4). The $\\mathbf{z}^{\\prime}$ arrives at WOSS neurons by spikes $S_{\\mathbf{z}}$ from the previous layer, i.e., each row of $S_{(Q K^{T})}$ , with the form of the one-spike phase coding. ", "page_idx": 6}, {"type": "text", "text": "First, we need to compute $\\mathbf{\\nabla}^{*}z_{i}^{\\prime}-\\operatorname*{max}(\\mathbf{z}^{\\prime})^{*}$ from the input spikes, i.e., $S_{\\mathbf{z}}$ . To do so, we first find $\\operatorname*{max}(\\mathbf{z}^{\\prime})$ by detecting the first incoming spike, called a winner spike. In Fig. 4(a), the winner spike is observed at neuron $i$ . For the neuron $i$ , its potential increases by 1 at $t=1$ (Fig. 4(b)). Inspired by inhibitory neurons in (25), the winner spike activates the global inhibition so that potentials of all neurons (including $i$ ) within the same softmax group decrease by 1. The global inhibition performs the subtraction by $\\operatorname*{max}(\\mathbf{z}^{\\prime})$ in Eq. (4). After the global inhibition at $t\\,=\\,1$ , non-winner neurons receive an input spike at $t>1$ . Fig. 4(c) shows how the potential of non-winner neuron $j$ changes. At $t=3$ , the neuron $j$ receives the input spike, increasing the potential by 1. At each timestep $t$ , the potential decays by the base $B$ (at $t=2$ and $t=3$ in Fig. 4(c)), which is identical to the leak factor $\\lambda$ in Eq. (1). This decay factor allows us to assign a dedicated phase $B^{-t}$ to each timestep. Thus, after the first \u201cpotential update\u201d stage $\\left.t\\geq T\\right.$ ), each neuron $i$ has the fixed potential equal to ", "page_idx": 6}, {"type": "equation", "text": "$$\nV_{W O S S(i)}(t\\geq T)=B^{(T-1)}(z_{i}^{\\prime}-\\operatorname*{max}(\\mathbf{z}^{\\prime})).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Now, we need to generate a single output spike at each neuron by comparing its potential to the threshold (i.e., \u201cspike generation\u201d stage in Fig. 4(b-c)). The threshold is initially set to 0 so that the winner-spike is time-shifted and fires at $t=0$ for a more precise approximation of softmax, i.e., utilizing full timestep $T$ (Fig. 4(b)). The desired time $t_{i}$ that each neuron should fire approximately equals to $\\frac{(\\operatorname*{max}(\\mathbf{z}^{\\prime})\\!-\\!z_{i}^{\\prime})M_{\\mathbf{z}}}{\\log_{e}B}$ . To find the $t_{i}$ , we shift the threshold by following ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{d V_{\\theta}(t)}{d t}=-B^{T-1}\\frac{\\log_{e}B}{M_{\\mathbf{z}}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and generate output spike comparing the threshold with the neuron\u2019s potential. Note that the righthand side of Eq. (6) is a constant and thus can be pre-computed. The appearance of $B^{T-1}$ is to match the scale of the potential after the \u201cpotential update\u201d stage. The $-\\frac{\\log_{e}B}{M_{\\mathbf{z}}}$ term is to consider the $M_{\\mathbf{z}}/\\log_{e}B$ in Eq. (4). Then, by comparing the Eq. (5) with the shifted threshold ${V_{\\theta}}(t_{i})$ , we can generate a spike if the threshold becomes more negative (Fig. 4(c)) that satisfies ", "page_idx": 6}, {"type": "equation", "text": "$$\nt_{i}>(\\mathrm{max}(\\mathbf{z}^{\\prime})-z_{i}^{\\prime})\\frac{M_{\\mathbf{z}}}{\\log_{e}B}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The output spike from the WOSS neuron, i.e., $S_{W O S S}$ , approximates $\\frac{\\exp(z_{i})}{\\exp(\\operatorname*{max}(\\mathbf{z}))}$ . Thus, it needs to be divided by $\\sum{\\frac{\\exp(z_{i})}{\\exp(\\operatorname*{max}(\\mathbf{z}))}}$ to complete the softmax ca his is done by amplifying the threshold at the following trace-driven dot products (i.e., $S_{A t t n}\\cdot S_{V})$ summation of spikes from the WOSS neurons in the same group, i.e., each row of $S_{W O S S}$ . Similar troe aEcqh.e s( $\\sum{\\frac{\\exp(z_{i})}{\\exp(\\operatorname*{max}(\\mathbf{z}))}}$ ,d  wish iicnhc ries meexnptreeds sbeyd $S_{W O S S}(t)$ and multiplied by $B$ at each timestep until it ", "page_idx": 6}, {"type": "equation", "text": "$$\nV_{\\theta(i,:)}(t+1)=B\\times V_{\\theta(i,:)}(t)+\\sum_{j\\in{\\bf z}_{i}}S_{W O S S(i,j)}(t),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $V_{\\theta}(t=0)$ is initially set to 1. Note that all elements at each row $i$ share the same threshold $V_{\\theta(i,:)}$ . Finally, the output neuron at \u221athe final trace-driven dot product in Fig. 1(b) generates spikes that approximate \u2018softmax $(S_{Q}\\cdot S_{K^{T}}/\\sqrt{d})\\cdot S_{V}\\mathrm{\"}$ . Note that implementing WOSS neurons for softmax in the neuromorphic hardware incurs $9.88\\%$ area and $12.35\\%$ power overheads compared to the hardware only with LIF neurons with no support of softmax (details are discussed in Appendix C). ", "page_idx": 6}, {"type": "image", "img_path": "fs28jccJj5/tmp/510fc91c3bc83415ef6d1761e7dd0c7ad85bdfee4c8ce882997467fbac5f6373.jpg", "img_caption": ["Figure 5: Scatter plots showing the correlation between the actual activation values in Swin Transformer $\\mathbf{X}$ -axis) and the decoded spike values in converted SpikedAttention (y-axis). The parameters are set to $T=40,B=1.15$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "5 Experimental Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "5.1 Conversion of Swin Transformer to SpikedAttention ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The proposed SpikedAttention is implemented by SpikingJelly (34) and PyTorch computing with four NVIDIA GeForce RTX 4090 GPUs. For evaluation, Swin Transformer (6) is selected as a baseline and converted to an SNN, i.e., SpikedAttention, to perform image classification. Conventional transformers require GeLU and layer normalization, which are non-spike computations. Thus, as a pre-trained model, we replaced GeLU with ReLU and layer normalization with batch normalization according to (35). Since some activations are not followed by the ReLU function in Swin-T, those cannot be converted to unsigned/positive spikes. To cover both positive and negative values, we utilized a signed neuron (36) that generates a positive spike $(+1)$ if the potential is greater than ${\\mathit{V}}_{\\theta}$ and a negative spike $(-1)$ if the potential is less than $V_{\\theta}$ . Therefore, we trained two baselines, i.e., one with ReLUs at every layer and another with no change, to convert them to SNNs. Fig. 5 illustrates the correlation between the decoded spike values and the actual activations at trace-driven matrix multiplication and softmax layers of SpikedAttention. The earlier layers (e.g., First Block) have little conversion error, but the deeper layers (e.g., Last Block) have higher conversion error. Nevertheless, the classifier exhibits a high correlation between the actual activations and the decoded spikes. ", "page_idx": 7}, {"type": "table", "img_path": "fs28jccJj5/tmp/09d6ea96b3a6affdf0524f799230319114817039e1b10500ff0112e6958fe55e.jpg", "table_caption": ["Table 1: Comparison between SpikedAttention and the prior work in terms of the parameter size, the energy consumption, the required timestep, and the accuracy on ImageNet classification task "], "table_footnote": ["\u2020For the fair comparison with the prior work, only the energy consumption for weight accumulations is included. "], "page_idx": 7}, {"type": "text", "text": "Table 1 compares performance between the prior work and the proposed SpikedAttention (denoted as \u2018Ours\u2019 in the table). For the fair comparison, we estimate the energy consumption of only weight accumulations (discussed in Appendix A.1) for the entire timestep similar to the prior work (12; 13). Previous SNNs based on vision transformers demonstrated better performance than CNN-based SNNs (10; 11; 24). However, existing SOTA transformer-based SNNs consume $2\\sim53\\times$ higher energy than SpikedAttention (w/o ReLU) while presenting lower accuracy $(\\le80\\%$ ). Other SNNs feed external inputs as floating point numbers, resulting in higher energy consumption. It is noteworthy that SpikedAttention minimizes the energy consumption even with $T=40$ by allowing only one spike per neuron. To perform a hardware-realistic energy comparison between ANNs and SNNs, we estimated the total energy consumption by using the energy metric presented in (38) which considers the data movement and the membrane potential update as well (details can be found in Appendix A.2). As a result, SpikedAttention reduces the energy consumption by $42\\%$ with only $0.6\\%$ accuracy loss when converting an ANN (w/o ReLU) which consumes $45.4\\mathrm{mJ}$ . Similarly, the energy consumption of SpikedAttention is $47\\%$ lower than the ANN (w/ ReLU) which consumes ", "page_idx": 7}, {"type": "table", "img_path": "fs28jccJj5/tmp/4198f296fde542bfa862e16550c97d88190209dd376250a53093eae89c775e58.jpg", "table_caption": ["Table 2: Conversion results of pre-trained models from MST (14) to SpikedAttention "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "$27.5\\;\\mathrm{mJ}$ . To directly compare with the previous ANN-to-SNN conversion method, i.e., MST (14), the pre-trained ANNs from MST were converted to SpikedAttention as well (Table 2). In Table 2, we computed the energy consumption of ANNs and SNNs using the hardware-realistic energy metric (Appendix A.2). Since the pre-trained models of MST embed ReLU in each layer, they were converted to SpikedAttention using unsigned neurons. The MST consumes significantly higher energy $\\left(\\geq48\\times\\right)$ than SpikedAttention because multiple spikes are generated for a longer timestep $T=64$ or 128. In addition, the first convolution layer is based on floating point-based MACs instead of spike-based accumulations. Compared to the MST, SpikedAttention achieves higher accuracy with shorter $T$ thanks to the trace-driven matrix multiplication. As a result, we achieve SOTA SNN accuracy on ImageNet without any training or modifying the architecture while minimizing energy consumption. ", "page_idx": 8}, {"type": "text", "text": "5.2 Conversion of BERT to SpikedAttention ", "text_level": 1, "page_idx": 8}, {"type": "table", "img_path": "fs28jccJj5/tmp/4a6b07a1be1d89a32143fd0358bf9248e285a956a3889fcfdb4b8de15d425933.jpg", "table_caption": ["Table 3: Comparison between SpikedAttention and other BERT models on GLUE Benchmark "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Our proposed SpikedAttention is also applicable to NLP since it converts attention modules into spike-based computations. However, there are some functions, such as GeLU and LayerNorm, that are difficult to compute with an SNN for now. Since MA-BERT (39) replaces GELU with ReLU, converts LayerNorm to BatchNorm, and fuses normalization layers into adjacent linear layers, MA-BERT can be easily converted to SpikedAttention without any modification. Therefore, we converted MA-BERT (base-uncased) with 110M parameters to SpikedAttention. Note that accuracy loss of MA-BERT compared to traditional BERT is only $0.1\\%$ . Even though MA-BERT approximates the softmax with a two-layer neural network, we converted MA-BERT with the original softmax to SpikedAttention (thanks to WOSS). The output of the token embedding is binary coded and fed to our SNN model. By converting the pre-trained MA-BERT for text classification on GLUE benchmark (41), we observed only $0.3\\%$ accuracy loss on average without any additional training (Table 3). Note that Matthews correlation coefficient is reported for CoLA, while accuracy is reported for the other tasks. By estimating the energy consumption as presented in Appendix A.2, SpikedAttention reduces the energy by $58\\%$ on average compared to MA-BERT. The SpikedAttention is the first work that demonstrates the transformer-to-SNN conversion for NLP tasks. SpikingBERT (40) directly trains SNN as a student using knowledge distillation from a pre-trained BERT as a teacher. Compared to ", "page_idx": 8}, {"type": "text", "text": "SpikingBERT ( $T=125)$ ), SpikedAttention was able to achieve $3.6\\%$ higher accuracy on average as it directly converts a well-trained BERT into an SNN without training. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper presented trace-driven matrix multiplication and winner-oriented spike shift to convert the attention module with spike-based computations. Our proposed methods accurately approximate all activations in the self-attention module as spikes without performing expensive exponent computations. Thus, our work outperforms previous transformer-based SNNs in accuracy and consumes much less energy ${25.60}\\mathrm{J}$ for vision tasks and $80.2\\mathrm{mJ}$ for language tasks on average) without structural modifications or direct training. By converting ANNs with a high number of multiplications into addition-only SNNs without any training iterations, our work makes AI more accessible on energy-constrained devices. Since the presented conversion method does not require any additional training, we can obtain SNNs without increasing the amount of carbon emissions. However, there are some limitations: first, the timestep required for SpikedAttention to maintain high accuracy is longer than directly trained SNNs. Thus, our next goal would be reducing the timestep by learning the per-layer base for better one-spike phase coding. The second limitation is that SpikedAttention do not support GeLU and LayerNorm making it difficult to be generalized to any language models. Nevertheless, our work is an essential step towards converting large language models to SNNs, and converting LayerNorm and GeLU to spike-based computations remains as our future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was partially supported by the Institute of Information & Communications Technology Planning & Evaluation (IITP) funded by the Ministry of Science and ICT under Grant 2022-0-01170 and Grant RS-2023-00229849; in part by the National Research Foundation of Korea (NRF) funded by the Ministry of Science and ICT under Grant RS-2023-00258227. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBERT: Pre-training of deep bidirectional transformers for language understanding,\u201d in Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (NAACL-HLT), pp. 4171\u20134186, 2019.   \n[2] S. Gunasekar, Y. Zhang, J. Aneja, C. C. T. Mendes, A. Del Giorno, S. Gopi, M. Javaheripi, P. Kauffmann, G. de Rosa, O. Saarikivi, et al., \u201cTextbooks are all you need,\u201d arXiv:2306.11644, 2023.   \n[3] H. Wu, H. Zhou, M. Long, and J. Wang, \u201cInterpretable weather forecasting for worldwide stations with a unified deep model,\u201d Nature Machine Intelligence, pp. 1\u201310, 2023.   \n[4] W. Peebles and S. Xie, \u201cScalable diffusion models with transformers,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 4195\u20134205, 2023.   \n[5] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d in Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), 2017.   \n[6] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, \u201cSwin Transformer: Hierarchical vision transformer using shifted windows,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021.   \n[7] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al., \u201cLearning transferable visual models from natural language supervision,\u201d in Proceedings of International Conference on Machine Learning (ICML), pp. 8748\u20138763, PMLR, 2021.   \n[8] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., \u201cLanguage models are few-shot learners,\u201d in Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), pp. 1877\u20131901, 2020.   \n[9] M. Horowitz, \u201c1.1 Computing\u2019s energy problem (and what we can do about it),\u201d in Proceedings of IEEE International Solid-State Circuits Conference (ISSCC), 2014.   \n[10] B. Han, G. Srinivasan, and K. Roy, \u201cRMP-SNN: Residual membrane potential neuron for enabling deeper high-accuracy and low-latency spiking neural network,\u201d in Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 13555\u201313564, 2020.   \n[11] B. Han and K. Roy, \u201cDeep spiking neural network: Energy efficiency through time based coding,\u201d in Proceedings of European Conference on Computer Vision (ECCV), pp. 388\u2013404, Springer, 2020.   \n[12] Z. Zhou, Y. Zhu, C. He, Y. Wang, S. YAN, Y. Tian, and L. Yuan, \u201cSpikformer: When spiking neural network meets transformer,\u201d in Proceedings of International Conference on Learning Representations (ICLR), 2023.   \n[13] M. Yao, J. Hu, Z. Zhou, L. Yuan, Y. Tian, X. Bo, and G. Li, \u201cSpike-driven transformer,\u201d in Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), 2023.   \n[14] Z. Wang, Y. Fang, J. Cao, Q. Zhang, Z. Wang, and R. Xu, \u201cMasked spiking transformer,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023.   \n[15] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \u201cImagenet: A large-scale hierarchical image database,\u201d in Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 248\u2013255, Ieee, 2009.   \n[16] S. Kim, A. Gholami, Z. Yao, M. W. Mahoney, and K. Keutzer, \u201cI-BERT: Integer-only BERT quantization,\u201d in Proceedings of International Conference on Machine Learning (ICML), pp. 5506\u20135518, PMLR, 2021.   \n[17] Y. Lin, T. Zhang, P. Sun, Z. Li, and S. Zhou, \u201cFQ-ViT: Post-training quantization for fully quantized vision transformer,\u201d in Proceedings of International Joint Conference on Artificial Intelligence (IJCAI), pp. 1173\u20131179, 2022.   \n[18] H. Zheng, Y. Wu, L. Deng, Y. Hu, and G. Li, \u201cGoing deeper with directly-trained larger spiking neural networks,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), pp. 11062\u201311070, 2021.   \n[19] C. Lv, T. Li, J. Xu, C. Gu, Z. Ling, C. Zhang, X. Zheng, and X. Huang, \u201cSpikebert: A language spikformer trained with two-stage knowledge distillation from bert,\u201d arXiv preprint arXiv:2308.15122, 2023.   \n[20] R.-J. Zhu, Q. Zhao, and J. K. Eshraghian, \u201cSpikegpt: Generative pre-trained language model with spiking neural networks,\u201d arXiv preprint arXiv:2302.13939, 2023.   \n[21] A. L. Hodgkin and A. F. Huxley, \u201cCurrents carried by sodium and potassium ions through the membrane of the giant axon of loligo,\u201d The Journal of Physiology, vol. 116, no. 4, pp. 449\u2013472, 1952.   \n[22] E. M. Izhikevich, \u201cWhich model to use for cortical spiking neurons?,\u201d IEEE Transactions on Neural Networks (TNN), vol. 15, pp. 1063\u20131070, Sept. 2004.   \n[23] N. Fourcaud-Trocm\u00e9, D. Hansel, C. van Vreeswijk, and N. Brunel, \u201cHow spike generation mechanisms determine the neuronal response to fluctuating inputs,\u201d The Journal of Neuroscience, vol. 23, no. 37, pp. 11628\u201311640, 2003.   \n[24] T. Bu, W. Fang, J. Ding, P. Dai, Z. Yu, and T. Huang, \u201cOptimal ANN-SNN conversion for high-accuracy and ultra-low-latency spiking neural networks,\u201d in Proceedings of International Conference on Learning Representations (ICLR), 2021.   \n[25] P. U. Diehl and M. Cook, \u201cUnsupervised learning of digit recognition using spike-timing-dependent plasticity,\u201d Frontiers in Computational Neuroscience, vol. 9, p. 99, Aug. 2015.   \n[26] B. Rueckauer and S.-C. Liu, \u201cConversion of analog to spiking neural networks using sparse temporal coding,\u201d in Proceedings of IEEE International Symposium on Circuits and Systems (ISCAS), pp. 1\u20135, 2018.   \n[27] S. Park, S. Kim, B. Na, and S. Yoon, \u201cT2FSNN: deep spiking neural networks with time-to-first-spike coding,\u201d in Proceedings of ACM/IEEE Design Automation Conference (DAC), pp. 1\u20136, 2020.   \n[28] J. Kim, H. Kim, S. Huh, J. Lee, and K. Choi, \u201cDeep neural networks with weighted spikes,\u201d Neurocomputing, vol. 311, pp. 373\u2013386, 2018.   \n[29] M. Zhang, Z. Gu, N. Zheng, D. Ma, and G. Pan, \u201cEfficient spiking neural networks with logarithmic temporal coding,\u201d IEEE Access, vol. 8, pp. 98156\u201398167, 2020.   \n[30] S. Hwang and J. Kung, \u201cOne-spike snn: Single-spike phase coding with base manipulation for ann-to-snn conversion loss minimization,\u201d IEEE Transactions on Emerging Topics in Computing, 2024.   \n[31] J.-P. Pfister and W. Gerstner, \u201cTriplets of spikes in a model of spike timing-dependent plasticity,\u201d Journal of Neuroscience, vol. 26, Sept. 2006.   \n[32] M. Davies, N. Srinivasa, T.-H. Lin, G. Chinya, Y. Cao, S. H. Choday, G. Dimou, P. Joshi, N. Imam, S. Jain, et al., \u201cLoihi: A neuromorphic manycore processor with on-chip learning,\u201d IEEE Micro, vol. 38, no. 1, pp. 82\u201399, 2018.   \n[33] R. Hu, B. Tian, S. Yin, and S. Wei, \u201cEfficient hardware architecture of softmax layer in deep neural network,\u201d in Proceedings of International Conference on Digital Signal Processing (DSP), pp. 1\u20135, IEEE, 2018.   \n[34] W. Fang, Y. Chen, J. Ding, Z. Yu, T. Masquelier, D. Chen, L. Huang, H. Zhou, G. Li, and Y. Tian, \u201cSpikingjelly: An open-source machine learning infrastructure platform for spike-based intelligence,\u201d Science Advances, vol. 9, no. 40, p. eadi1480, 2023.   \n[35] Z. Yao, Y. Cao, Y. Lin, Z. Liu, Z. Zhang, and H. Hu, \u201cLeveraging batch normalization for vision transformers,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops (ICCVW), pp. 413\u2013422, 2021.   \n[36] B. Rueckauer, I.-A. Lungu, Y. Hu, M. Pfeiffer, and S.-C. Liu, \u201cConversion of continuous-valued deep networks to efficient event-driven networks for image classification,\u201d Frontiers in Neuroscience, vol. 11, p. 682, 2017.   \n[37] M. Yao, J. Hu, T. Hu, Y. Xu, Z. Zhou, Y. Tian, B. XU, and G. Li, \u201cSpike-driven Transformer V2: Meta spiking neural network architecture inspiring the design of next-generation neuromorphic chips,\u201d in Proceedings of International Conference on Learning Representations (ICLR), 2024.   \n[38] F. Ottati, C. Gao, Q. Chen, G. Brignone, M. R. Casu, J. K. Eshraghian, and L. Lavagno, \u201cTo spike or not to spike: A digital hardware perspective on deep learning acceleration,\u201d IEEE Journal on Emerging and Selected Topics in Circuits and Systems (JETCAS), 2023.   \n[39] N. W. Ming, Z. Wang, C. Liu, R. S. M. Goh, and T. Luo, \u201cMa-bert: Towards matrix arithmetic-only bert inference by eliminating complex non-linear functions,\u201d in Proceedings of International Conference on Learning Representations (ICLR), 2022.   \n[40] M. Bal and A. Sengupta, \u201cSpikingbert: Distilling bert to train spiking language models using implicit differentiation,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), vol. 38, pp. 10998\u2013 11006, 2024.   \n[41] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman, \u201cGlue: A multi-task benchmark and analysis platform for natural language understanding,\u201d in Proceedings of International Conference on Learning Representations (ICLR), 2018.   \n[42] R. Yin, A. Moitra, A. Bhattacharjee, Y. Kim, and P. Panda, \u201cSata: Sparsity-aware training accelerator for spiking neural networks,\u201d IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (TCAD), 2022.   \n[43] T. Ajayi and D. Blaauw, \u201cOpenroad: Toward a self-driving, open-source digital layout implementation tool chain,\u201d in Proceedings of Government Microcircuit Applications and Critical Technology Conference (GOMACTech), 2019.   \n[44] H. Wang, Z. Wang, M. Du, F. Yang, Z. Zhang, S. Ding, P. Mardziel, and X. Hu, \u201cScore-CAM: Scoreweighted visual explanations for convolutional neural networks,\u201d in Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 111\u2013119, 2020.   \n[45] P. Diehl, D. Neil, J. Binas, M. Cook, S. Liu, and M. Pfeiffer, \u201cFast-classifying, high-accuracy spiking deep networks through weight and threshold balancing,\u201d in Proceedings of International Joint Conference on Neural Networks (IJCNN), 2015.   \n[46] S. Kim, S. Park, B. Na, and S. Yoon, \u201cSpiking-yolo: spiking neural network for energy-efficient object detection,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2020.   \n[47] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, \u201cSQuAD: $^{100,000+}$ questions for machine comprehension of text,\u201d in Proceedings of the the Conference on Empirical Methods in Natural Language Processing (EMNLP) (J. Su, K. Duh, and X. Carreras, eds.), (Austin, Texas), pp. 2383\u20132392, Association for Computational Linguistics, Nov. 2016. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Energy Metrics ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Simple Energy Metric Considering Only Synaptic Operations ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "For the fair comparison with the prior work (12; 13), energy cost of synaptic operations is estimated based on the power consumption of 32-bit floating-point arithmetic units synthesized in $45\\mathrm{{nm}\\,\\mathrm{{CMOS}}}$ technology (9). The synaptic operations of the ANN architecture are MACs, i.e., input activations are multiplied by the corresponding weights and partial sums are accumulated. We estimate the energy consumption of an ANN (denoted as $E_{A N N})$ ) by using the number of synaptic operations associated with the layer $l$ of the ANN $(S y n O P^{l})$ , which is defined as: ", "page_idx": 12}, {"type": "equation", "text": "$$\nE_{A N N}=(E_{a d d}+E_{m u l t})\\cdot\\sum_{l=1}^{N_{L}}{S y n O P^{l}}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $E_{a d d}+E_{m u l t}$ is $3.7p J$ for a single 32-bit floating point MAC operation. ", "page_idx": 12}, {"type": "text", "text": "Unlike ANNs that require both multipliers and adders, SNNs only require additions. Only weights that are connected to pre-synaptic neurons that fire spikes are accumulated together. Therefore, the energy consumption of an SNN can be estimated by: ", "page_idx": 12}, {"type": "equation", "text": "$$\nE_{S N N}=E_{a d d}\\cdot\\sum_{t=1}^{T}\\sum_{l=1}^{N_{L}}\\left(\\gamma_{t}^{l}\\cdot S y n O P^{l}\\right)\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\gamma_{t}^{l}$ is the ratio of spikes at timestep $t$ and $E_{a d d}$ is $0.9p J$ for a single 32-bit floating point addition. Note that the energy numbers reported in the Table 1 accounts for the entire timesteps in SNN models. ", "page_idx": 12}, {"type": "text", "text": "A.2 More Realistic Energy Metric Considering Hardware ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The energy metric described in Appendix A.1 only considers the computing energy. However, a large portion of energy is being consumed by repeatedly reading/writing activations and weights, as well as updating the neuron potential over multiple timesteps. The data movement highly depends on the underlying hardware architecture, e.g., on-chip memory capacity and interconnect topology. For instance, membrane potentials can be fetched from DRAM, whereas the neuron model is computed on the core (42). In that case, the energy cost of fetching membrane potentials becomes significant. On the contrary, Intel\u2019s Loihi (32) places a large on-chip memory to store membrane potentials or traces inside the chip. Therefore, to perform a hardware-agnostic energy comparison between ANNs and SNNs, we estimated the total energy consumption by using the energy metric presented in (38), which considers the energy of reading weights and spikes, the energy of computing the neuron model, and the energy of writing spikes back to the memory. ", "page_idx": 12}, {"type": "text", "text": "First, the energy consumption of an ANN is computed by: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{E_{A N N}=E_{r d_{t o t}}+E_{M A C}+E_{o f f m a p},}\\\\ &{}&{E_{r d_{t o t}}=2E_{r d}\\cdot\\displaystyle\\sum_{l=1}^{N_{L}}\\gamma_{l}S y n O P^{l},}\\\\ &{}&{E_{M A C}=(E_{a d d}+E_{m u t})\\cdot\\displaystyle\\sum_{l=1}^{N_{L}}\\gamma^{l}S y n O P^{l},}\\\\ &{}&{E_{o f f m a p}=E_{w r}\\cdot\\displaystyle\\sum_{l=1}^{N_{L}}A c t^{l},}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $E_{r d_{t o t}}$ is the energy for reading weights and activations, $E_{M A C}$ is the energy for arithmetic operations, and $E_{o f f m a p}$ is the energy for writing MAC results to the scratchpad memory. The $A c t^{l}$ is number of output activations at layer $l$ and $E_{r d}/E_{w r}$ is set to $5p J$ for the 32-bit read/write operation (9). Also, the activation sparsity is considered by the term $\\gamma_{l}$ for the fair comparison with the SNN. ", "page_idx": 12}, {"type": "text", "text": "Next, the energy consumption of an SNN is computed as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{}}&{{}}&{{E_{S N N}=E_{r d e t}+E_{A C}+E_{N e u r o n}+E_{o f f m a p},}}\\\\ {{}}&{{}}&{{E_{r d e t}=(1+\\displaystyle\\frac{1}{32})E_{r d}.\\sum_{t=1}^{T}\\sum_{l=1}^{N_{L}}\\gamma_{l}^{l}.\\ \\mathcal{S}y n O P^{l},}}\\\\ {{}}&{{}}&{{E_{M A C}=E_{a d d}.\\sum_{t=1}^{T}\\sum_{l=1}^{N_{L}}\\gamma_{l}^{l}.\\ \\mathcal{S}y n O P^{l},}}\\\\ {{}}&{{}}&{{E_{N e u r o n}=(E_{r d}+E_{m u l t}+E_{a d d}+E_{c o m p}+E_{s u b}+E_{w r})\\cdot\\sum_{t=1}^{T}\\sum_{l=1}^{N_{L}}A c^{l},}}\\\\ {{}}&{{}}&{{E_{a f f m a p}=\\displaystyle\\frac{E_{w r}}{32}.\\sum_{t=1}^{T}\\sum_{l=1}^{N_{L}}A c^{l},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $E_{r d_{t o t}}$ is the energy for reading weights and input spikes, $E_{A C}$ is the energy for synaptic operations (i.e., additions), $E_{N e u r o n}$ is the energy for computing neuron models and $E_{o f f m a p}$ is the energy for writing output spikes to the scratchpad memory. In $E_{r d_{t o t}}$ and $E_{o f f m a p}$ , the energy of reading/writing 1-bit spike is $E_{r d}/32$ (or $E_{w r}/32)$ ) since $E_{r d}/E_{w r}$ is the read/write energy for 32-bit data. The $E_{N e u r o n}$ consists of retrieving the neuron\u2019s potential $(E_{r d})$ , multiplying leak factor $(E_{m u l t i})$ , adding input to potential $(E_{a d d})$ , comparing with the threshold $(E_{c o m p})$ , subtracting the potential by the threshold $(E_{s u b})$ , and finally writing the potential back to the memory $(E_{w r})$ . As in (38), we defined $E_{s u b}$ and $E_{c o m p}$ to have the same energy value as $E_{a d d}\\,=\\,0.9p J$ . Note that SpikedAttention stops generating spikes at neurons that fired already implying that there is no energy consumed by those neurons afterwards. The energy overhead of computing the neuron model was $30{\\sim}40\\%$ of the total energy, even though the total timestep of SpikedAttention converted from Swin-T was $T=40$ . This is because the number of synaptic operations $(\\sum_{l=1}^{N_{L}}S y n O P^{l})$ is larger than the number of output activations ( lN=L1 A ctl). Therefore, even with the overhead of computing the neuron model, SpikedAttention, which generates a single spike for the entire timestep (i.e., $\\begin{array}{r}{\\sum_{t=1}^{T}\\gamma_{t}^{l}\\leq1)}\\end{array}$ , is energy efficient since the sparsity of synaptic operations is the most critical factor in improving energy efficiency. ", "page_idx": 13}, {"type": "text", "text": "B LIF Model with Phase Coding ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this work, our phase coding with the base $\"B'$ follows the phase coding scheme presented in (28), which fixes the base to 2. The dynamics of a neuron in the phase coding can be computed by Eq. 1 by changing the leak factor of the LIF to the selected base $B$ . ", "page_idx": 13}, {"type": "text", "text": "To express the equivalence clearly, we formulate it as the following theorem: ", "page_idx": 13}, {"type": "text", "text": "Theorem B.1. Given the following neuron dynamics of the phase coding with the base $B$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{u_{j}(t)=u_{j}(t-1)+\\displaystyle\\sum_{i=1}^{N_{p r e}}w_{i j}B^{-t}S_{i}(t),}\\\\ {S_{j}(t)=\\displaystyle\\left\\{1,i f\\,u_{j}(t)\\geq V_{\\theta}\\times B^{-t}\\right.\\qquad}\\\\ {0,\\displaystyle o t h e r w i s e\\qquad}\\end{array}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "It can be formulated to the following LIF-based neural dynamics: ", "page_idx": 13}, {"type": "equation", "text": "$$\nv_{j}(t)=B\\cdot v_{j}(t-1)+\\sum_{i=1}^{N_{p r e}}w_{i j}S_{i}(t),S_{j}(t)=\\left\\{\\overset{\\displaystyle1,i f\\,v_{j}(t)\\geq V_{\\theta}}{\\displaystyle0,o t h e r w i s e}\\right.\\quad.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. The equivalence of the two dynamics lies in the equivalence of the conditions, $u_{j}(t)\\geq B^{-t}{\\cdot}V_{\\theta}$ and $v_{j}(t)\\geq V_{\\theta}$ , i.e., $u_{j}(t)=v_{j}(t)\\cdot B^{-t},t=1,\\ldots,T$ . We prove this using mathematical induction. For $t=1$ , we have $u_{j}(1)=v_{j}(1)\\cdot B^{-1}=0$ . Assume that $u_{j}(t)=v_{j}(t)\\cdot B^{-t}$ holds for $t=1,...,t$ ", "page_idx": 13}, {"type": "text", "text": "next we prove that uj(t + 1) = vj(t + 1) \u00b7 B\u2212(t+1): ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{u_{j}(t+1)=u_{j}(t)+\\displaystyle\\sum_{i=1}^{N_{p r e}}w_{i j}B^{-(t+1)}S_{i}(t+1)}\\\\ &{\\qquad\\qquad=v_{j}(t)B^{-t}+\\displaystyle\\sum_{i=1}^{N_{p r e}}w_{i j}B^{-(t+1)}S_{i}(t+1)}\\\\ &{\\qquad\\qquad=B^{-(t+1)}\\left(B\\cdot v_{j}(t)+\\displaystyle\\sum_{i=1}^{N_{p r e}}w_{i j}S_{i}(t+1)\\right)}\\\\ &{\\qquad\\qquad=B^{-(t+1)}\\cdot v_{j}(t+1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "C Hardware Overhead of Implementing WOSS Neurons ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To implement WOSS in the neuromorphic hardware, we need some modifications to the hardware structure of the existing LIF neuron. The recent Loihi2 (32) allows users to select a neuron model among many neuron models implemented in hardware. Thus, we designed WOSS neuron in Verilog and synthesized the hardware module in $45\\mathrm{{nm}\\,\\mathrm{{CMOS}}}$ technology using OpenROAD (43) to estimate the area/power overhead when realizing it on top of the commercial neuromorphic hardware. Please note that WOSS neurons are only required for the softmax layer accounting for $19\\%$ of all neurons at each attention block of Swin-T. The remaining $81\\%$ are implemented as typical LIF neurons. As summarized in Table. 4, compared to a general LIF model, the proposed WOSS neuron increases the area by $52\\%$ and the power consumption by $65\\%$ . This translates to $9.88\\%$ larger area and $12.35\\%$ higher power consumption to support the proposed WOSS method for softmax operations. ", "page_idx": 14}, {"type": "table", "img_path": "fs28jccJj5/tmp/b36ef6b495e644974cbfa8729095666e09053af89d06e18f076fa491b10a8461.jpg", "table_caption": ["Table 4: Area and power overheads of a hardware module for supporting the WOSS neuron "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "D Visualization of Attention Map ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Fig. 6 presents the attention maps, highlighting the areas of activation\u2019s interest in the image at the outputs of several self-attention layers. The attention maps from the pre-trained Swin Transformer and the SpikedAttention were extracted using Score-CAM (44). The attention maps of SpikedAttention from the 9th to 12th attention layer are similar to those of Swin-T, even though the conversion error increases at deeper layers. This visualization of attention maps implies that our ANN-to-SNN conversion method successfully transforms attention modules into spike-based operations. ", "page_idx": 14}, {"type": "text", "text": "E Selecting the Optimal Base $B$ for a Given Timestep $T$ ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To achieve the optimal performance, SpikedAttention needs to set an optimal base. For selecting the optimal base, we define an error function to find the optimal base $B$ , which presents the smallest difference between approximated spike values and true activations. The most common error evaluation is an absolute or relative error. However, we need something more suitable for our work. It is not effective to use absolute error like MSE , i.e., $\\textstyle{\\frac{1}{n}}\\sum\\left(x-{\\hat{x}}\\right)^{2}$ where $x$ is the actual value and $\\hat{x}$ is the approximated value, since larger error gets accumulated for large $x$ as our spike approximation is log-based. The relative error like ARE, i.e., $\\begin{array}{r}{\\frac{1}{n}\\sum|\\big(\\frac{x-\\hat{x}}{x}\\big)|}\\end{array}$ , is also not a good candidate, since the same relative difference may imply different timestep scale which depends on the base $B$ . For instance, for a given $x$ , $1.1\\times x$ is 1.95 timesteps away from $x$ when the base is 1.05. When the base increases to 1.07, the timestep difference between the two values decreases to 1.41. ", "page_idx": 14}, {"type": "image", "img_path": "fs28jccJj5/tmp/a81feb61a85df271b25fcfdb07d071653023c11abe50224665f3c84bddaba008.jpg", "img_caption": ["Figure 6: Comparison of attention maps based on Score-CAM (44) between Swin-T (baseline ANN) and SpikedAttention (Ours; $B=1.16$ and $T=40$ ) on ImageNet-1K at the output of various attention blocks. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "We can accurately estimate the approximation error between the real-valued activation and the decoded spike value using the logarithmic error $E_{q}(x)$ as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\nE_{q}(x)=\\left\\{B\\times|L+1|,\\begin{array}{c l}{{}}&{{\\mathrm{if}\\ \\ L>-1}}\\\\ {{}}&{{\\mathrm{if}\\ \\ -1\\geq L\\geq-T}}\\\\ {{\\ \\ B\\times0.5,}}&{{\\mathrm{otherwise.}}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $x$ is the sampled activation from $\\mathbf{X}$ of ANN and $:L\\;=\\;\\log_{B}x\\,-\\,\\log_{B}{\\left(\\operatorname*{max}(\\mathbf{X})\\right)}^{\\ast}$ is the logarithm of normalized activation. Note that as the weights are normalized (45; 46) by $\\operatorname*{max}(\\mathbf{X})$ , i.e., the maximum value of $\\mathbf{X}$ in a subset of the training dataset, each activation is approximated by a spike as $\\mathrm{\\dot{\\max}}(\\mathbf{X})\\times B^{-t}$ . In other words, spike time $t$ represents $-\\lfloor L\\rceil$ . Error with values less than $\\dot{\\mathrm{\\max}}(\\mathbf{X})\\times B^{\\dot{-}T},$ is fixed to 0.5, since $\\log_{B}0$ is not defined. ", "page_idx": 15}, {"type": "image", "img_path": "fs28jccJj5/tmp/5bf27b04be209db63e45417a0e3a912073af6ce1bbf5f4b33a795aa5585e3139.jpg", "img_caption": ["Figure 7: (a) Normalized logarithmic error ( EqB(x) with respect to the normalized activation $(\\frac{x}{\\operatorname*{max}(\\mathbf{X})})$ at a given timestep $T\\,=\\,4$ . (b) Logarithmic error $(E_{q}(x))$ and the accuracy on the ImageNet classification task at various base $B$ values at a given timestep $T=40$ . "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Fig. 7(a) shows the error function for a given timestep $T\\,=\\,4$ . The Eq. (14) first computes the $\\log_{B}$ -based absolute error, i.e., $|L-\\lfloor L\\rceil|$ , where $L$ is $\\mathbf{\\dot{\\log}}_{B}\\,x-\\log_{B}{\\left(\\operatorname*{max}(\\mathbf{X})\\right)},$ . By doing this, we can accurately measure the error when approximating an activation with a spike having $\\bar{B^{-t}}$ value. Two exception ranges are (i) $L>-1$ and (ii) $L<-T$ and they are correctly handled to estimate the error conservatively. Then, this $\\log_{B}$ -based error term is multiplied by $B$ to set the timestep scale properly. ", "page_idx": 15}, {"type": "text", "text": "To find the optimal base, our method sets the initial base $B_{0}$ in the midpoint of the search space (1.0, 1.5]. Then, we compute the $\\sum E_{q}(x)$ for both $B\\,=\\,B_{0}\\,-\\,\\epsilon$ and $B\\,=\\,B_{0}\\,+\\,\\epsilon.$ , and update the base to one of them that provides a smaller error. The optimal base with the smallest error can be found within a given search space by iterating the process several times. Fig. 7(b) presents the logarithmic error $E_{q}(x)$ and the accuracy according to the base $B$ . The accuracy loss follows the same trend to our logarithmic error function, i.e., as $B$ gets smaller, the accuracy increases while our error term decreases. If $B$ gets too small, however, the accuracy decreases (equivalently, our error term increases) due to the increased number of underflows. As a result, we utilized the $\\log_{B}$ -based absolute error multiplied by the base when determining the optimal base for a given benchmark. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "F Performance on Question Answering ", "text_level": 1, "page_idx": 16}, {"type": "table", "img_path": "fs28jccJj5/tmp/6c4e24b515b1689f7eba60aee3c3ab71835014f2081f389598fe7bce7a6fd398.jpg", "table_caption": ["Table 5: Performance of SpikedAttention on Question Answering "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "To demonstrate that SpikedAttention can reduce energy consumption for more complex tasks, we converted MA-BERT for question answering to SpikedAttention. We trained the existing MA-BERT on the SQuAD dataset (47) for question answering and converted it to spike-based computations. As a result, SpikedAttention achieves an energy reduction of $59.5\\%$ with only $1.1\\%$ accuracy loss, as presented on Table 5. ", "page_idx": 16}, {"type": "text", "text": "G Impact of Total Timestep on Accuracy and Energy Consumption ", "text_level": 1, "page_idx": 16}, {"type": "image", "img_path": "fs28jccJj5/tmp/f0e5e75d4cbd32d7acf9530b994bd91e5af5cc48437722eba50ad2b9c7e4a85c.jpg", "img_caption": ["Figure 8: Accuracy loss and the energy consumption at various timesteps when converting Swin-T to an SNN for ImageNet classification. (a) SpikedAttention from Swin-T without any modification, and (b) SpikedAttention from Swin-T with inserted ReLUs. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Regarding the accuracy, as discussed in Appendix E, the larger the timestep T, the smaller the base value of an one-spike SNN can be. A smaller base reduces the conversion loss, leading to higher accuracy. Regarding the energy consumption, as discussed in Appendix A.2, a longer timestep incurs higher energy consumption due to more neuron model computations and data movements. To summarize, a longer timestep increases the energy consumption while reducing the accuracy loss. Fig. 8 shows the accuracy loss and the energy consumption at various timesteps when converting Swin-T to an SNN for ImageNet classification. It clearly shows the trade-off between the total timestep and the accuracy/energy consumption. Note that even at $T=24$ the accuracy loss is less than $0.5\\%$ . ", "page_idx": 16}, {"type": "image", "img_path": "fs28jccJj5/tmp/8501de933022c55d5c744ae4e10a5f35407e5baa80d589ccd73727799e03cf10.jpg", "img_caption": ["Figure 9: Energy consumption of MA-BERT and SpikedAttention, and accuracy loss of BERT-toSNN conversion on the GLUE SST2 dataset according to the maximum input length (i.e., 64, 128, 192 and 256). "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "H Energy Consumption at Various Input Token Lengths ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Energy consumption of ANN/SNN varies with the input token length for NLP tasks. To evalute this, we varied the input token length of MA-BERT (i.e., target ANN) from 64 to 256 on SST-2 dataset. Also, the same lengths of input tokens are fed into the converted SpikedAttention model for the energy evaluation. Fig. 9 shows energy consumptions at various input lengths for both ANN and SpikedAttention. In addition, the accuracy losses due to the ANN-to-SNN conversion are presented which are negligible $(<1\\%)$ . ", "page_idx": 17}, {"type": "text", "text": "For both ANN and SpikedAttention, the energy consumption increases as the maximum input length increases from 64 to 256. This is because the number of computations increases in the attention module with the increase in the input length. For instance, MA-BERT with the input length of 128 consumes $189.7\\mathrm{mJ}$ of energy, while MA-BERT with the input length of 256 consumes $458.9m J$ $(2.4\\times)$ . Since SpikedAttention benefits from fully spiked-based computations, the energy consumption for input lengths of 128 and 256 is $79.9m J$ and $188.0m J$ , respectively. These energy numbers imply that SpikedAttention is $2.4\\times$ more energy-efficient compared to the ANN, regardless of the input token length. As we optimized the spiked-based computation in the attention module with WOSS and trace-based matrix multiplication, the energy reduction ratio of SpikedAttention compared to ANN slightly increases as the input length increases. ", "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: See line 51-68 in Section 1. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: See line 307-313 in Section 6. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: See Appnedix B. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: See Section 5 and Appendix E. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: See supplemental material. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: See Section 5 and Appendix E. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [No] ", "page_idx": 20}, {"type": "text", "text": "Justification: We do not observe large enought variance in our inference of conversion from pre-trained model. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: See line 241 in Section 5. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Our research conform with the NeurIPS Code of Ethics. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: See line 304-307 in Section 6. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our paper poses no such risks. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We properly cited the pre-trained models (Swin-Transformer,Masked Spiking Transformer and MA-BERT) on which our work is based. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 22}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 23}, {"type": "text", "text": "Justification: Our work does not release new assets ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 23}, {"type": "text", "text": "Justification: Our work does not involve crowdsourcing nor research with human subjects Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: Our work does not involve crowdsourcing nor research with human subjects Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]