[{"figure_path": "fs28jccJj5/tables/tables_7_1.jpg", "caption": "Table 1: Comparison between SpikedAttention and the prior work in terms of the parameter size, the energy consumption, the required timestep, and the accuracy on ImageNet classification task", "description": "This table compares the performance of SpikedAttention with other state-of-the-art spike-based transformer models on the ImageNet dataset. The comparison is based on four key metrics: the number of parameters (Param (M)), energy consumption (Energy (mJ)), required timestep (Timestep), and accuracy (Acc (%)).  The table shows that SpikedAttention achieves state-of-the-art accuracy with significantly lower energy consumption compared to existing methods.  Two versions of SpikedAttention are shown: one without ReLU activation function and another with it.", "section": "5 Experimental Results"}, {"figure_path": "fs28jccJj5/tables/tables_8_1.jpg", "caption": "Table 1: Comparison between SpikedAttention and the prior work in terms of the parameter size, the energy consumption, the required timestep, and the accuracy on ImageNet classification task", "description": "This table compares the performance of SpikedAttention with other state-of-the-art spike-based transformer models on the ImageNet dataset for image classification.  It shows the parameter size (in millions), energy consumption (in mJ), required timestep, and accuracy achieved by each model.  The energy consumption is specifically for weight accumulations to ensure a fair comparison with other studies.  Note that the energy values of SpikedAttention are significantly lower than those of other models and that the accuracy of SpikedAttention is competitive or superior.", "section": "5 Experimental Results"}, {"figure_path": "fs28jccJj5/tables/tables_8_2.jpg", "caption": "Table 3: Comparison between SpikedAttention and other BERT models on GLUE Benchmark", "description": "This table compares the performance of SpikedAttention against other BERT models on the GLUE benchmark.  It shows the accuracy and energy consumption for each model across various GLUE tasks (CoLA, MNLI, MRPC, QNLI, QQP, RTE, SST-2, WNLI, STS-B).  The table highlights SpikedAttention's ability to achieve comparable accuracy with significantly lower energy consumption compared to traditional ANN-based models.", "section": "5.2 Conversion of BERT to SpikedAttention"}, {"figure_path": "fs28jccJj5/tables/tables_14_1.jpg", "caption": "Table 1: Comparison between SpikedAttention and the prior work in terms of the parameter size, the energy consumption, the required timestep, and the accuracy on ImageNet classification task", "description": "This table compares the performance of SpikedAttention with other state-of-the-art spike-based transformer models on the ImageNet dataset.  It evaluates each model based on its parameter size (in millions), energy consumption (in mJ), required timestep for spike processing, and classification accuracy (in percentage). The comparison highlights SpikedAttention's superior efficiency and accuracy compared to existing methods.", "section": "5 Experimental Results"}, {"figure_path": "fs28jccJj5/tables/tables_16_1.jpg", "caption": "Table 1: Comparison between SpikedAttention and the prior work in terms of the parameter size, the energy consumption, the required timestep, and the accuracy on ImageNet classification task", "description": "This table compares the performance of SpikedAttention against several prior state-of-the-art spike-based transformer models on the ImageNet dataset.  It shows that SpikedAttention achieves state-of-the-art accuracy while significantly reducing both energy consumption and the required timestep.  The comparison highlights the improvements in efficiency and accuracy SpikedAttention offers over existing methods.", "section": "5 Experimental Results"}]