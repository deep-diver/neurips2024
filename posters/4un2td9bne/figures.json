[{"figure_path": "4Un2TD9bNe/figures/figures_4_1.jpg", "caption": "Figure 1: The schematic of PaRO-DP is illustrated within a cluster comprising four GPUs. Each group in the cluster consists of two GPU devices. In particular, we illustrate the computation and communication phases of a global step that incorporates gradient accumulation. We detail the intra- and inter-group communication by using specific prefixes for each collective communication primitive.", "description": "This figure shows two schematics illustrating the PaRO-DP (Partial Redundancy Optimizer-Data Parallelism) training strategy using four GPUs divided into two groups.  Schematic (a) depicts the PIIG strategy (no parameter partitioning, intra-group gradient partitioning, global optimizer state partitioning). Schematic (b) shows the PNIG strategy (no parameter partitioning, intra-group gradient partitioning, global optimizer state partitioning). Both diagrams detail the forward pass, backward pass, gradient accumulation and update steps, highlighting intra-group and inter-group communication operations with specific prefixes. The figure demonstrates the proposed PaRO-DP's ability to optimize memory and communication costs by strategically partitioning model states.", "section": "3.1 PaRO-DP"}, {"figure_path": "4Un2TD9bNe/figures/figures_6_1.jpg", "caption": "Figure 1: The schematic of PaRO-DP is illustrated within a cluster comprising four GPUs. Each group in the cluster consists of two GPU devices. In particular, we illustrate the computation and communication phases of a global step that incorporates gradient accumulation. We detail the intra- and inter-group communication by using specific prefixes for each collective communication primitive.", "description": "This figure illustrates the PaRO-DP strategy using a cluster with four GPUs divided into two groups. It shows the computation and communication flow for a global step (incorporating gradient accumulation).  Panel (a) shows the PIIG strategy and panel (b) shows the PNIG strategy.  The figure highlights the intra- and inter-group communication aspects of each strategy using specific prefixes for each collective communication primitive. This helps in visualizing how PaRO-DP optimizes the communication operations by performing intra-group operations before inter-group operations, thus reducing the communication overhead in heterogeneous network environments.", "section": "3.1 PaRO-DP"}, {"figure_path": "4Un2TD9bNe/figures/figures_7_1.jpg", "caption": "Figure 3: The throughput and memory usage during LLaMA training with varying trainable parameters (\u03a8'). The blue dashed line represents the trend of the throughput indicator, represented by the TPS Indicator using log(1/T), calculated based on the guideline. The cross indicates OOM.", "description": "This figure displays the throughput and memory usage during the training of LLaMA models with different numbers of trainable parameters.  Four different scenarios are shown, varying both model size (7B and 65B parameters) and the ratio of trainable parameters to total parameters (100%, 1/16).  The x-axis represents the different training strategies used, including PaRO strategies and existing methods (ZeRO, MiCS, FSDP-hz, ZeRO++).  The y-axis shows throughput and single-GPU memory usage.  The blue dashed line indicates the trend of the throughput indicator (TPS, calculated with log(1/T)) predicted by the guideline presented in the paper, while the crosses denote out-of-memory (OOM) errors. The figure illustrates how the choice of strategy affects both training speed and memory consumption under different parameter scaling scenarios.", "section": "4.3 Efficiency of PaRO-DP"}, {"figure_path": "4Un2TD9bNe/figures/figures_7_2.jpg", "caption": "Figure 4: Throughput and Memory Usage of training LLaMA-65B in the PEFT(\u03a8' = 3/1000) scenario.", "description": "This figure shows the throughput, TPS indicator (log(1/T)), and peak memory usage for different model parallel training strategies in a PEFT scenario with LLaMA-65B model. The results highlight the superior performance of PaRO-DP strategies over existing approaches, indicating improvements in training speed and memory efficiency.", "section": "4.3 Efficiency of PaRO-DP"}, {"figure_path": "4Un2TD9bNe/figures/figures_7_3.jpg", "caption": "Figure 5: Collective Communication Time (millisecond/ms) with the increasing number of GPUs.", "description": "This figure compares the collective communication time of the traditional Ring topology and the proposed PaRO-CC method, as the number of GPUs increases from 16 to 128. It shows that PaRO-CC significantly reduces the communication time compared to the Ring topology, especially with a larger number of GPUs. This improvement is due to the efficient utilization of intra- and inter-group communication within PaRO-CC.", "section": "4.4 Efficiency of PaRO-CC"}, {"figure_path": "4Un2TD9bNe/figures/figures_8_1.jpg", "caption": "Figure 6: The throughput (samples/sec) when increasing the scale of GPUs.", "description": "The figure shows the throughput of different training strategies (ZeRO-2, ZeRO-3, NIG, IGG, and IIG) as the number of GPUs increases from 16 to 128.  It illustrates the scalability and performance of each strategy in a data parallel setting.  The throughput is measured in samples per second.", "section": "4.4 Efficiency of PaRO-CC"}, {"figure_path": "4Un2TD9bNe/figures/figures_8_2.jpg", "caption": "Figure 7: Training convergence of PaRO against ZeRO using LLaMA-7B.", "description": "This figure compares the training convergence curves of PaRO and ZeRO-3 using the LLaMA-7B language model.  It demonstrates that the different PaRO strategies (NIG, IGG, IIG) achieve comparable convergence to the baseline ZeRO-3 method, indicating that the proposed PaRO optimization techniques do not negatively impact model training performance.  The y-axis shows the loss, while the x-axis represents the training step.", "section": "4.4 Efficiency of PaRO-CC"}, {"figure_path": "4Un2TD9bNe/figures/figures_14_1.jpg", "caption": "Figure 1: The schematic of PaRO-DP is illustrated within a cluster comprising four GPUs. Each group in the cluster consists of two GPU devices. In particular, we illustrate the computation and communication phases of a global step that incorporates gradient accumulation. We detail the intra- and inter-group communication by using specific prefixes for each collective communication primitive.", "description": "This figure schematically shows how the Partial Redundancy Optimizer (PaRO) Data Parallelism (PaRO-DP) works in a cluster of four GPUs divided into two groups.  It illustrates the computation and communication steps during a global training step using gradient accumulation.  The diagrams highlight the intra-group and inter-group communications using specific prefixes (e.g., \"Intra-AllGather\", \"Inter-ReduceScatter\") for each collective communication primitive,  illustrating the differences between the PIIG and PNIG strategies of PaRO-DP.", "section": "3.1 PaRO-DP"}, {"figure_path": "4Un2TD9bNe/figures/figures_16_1.jpg", "caption": "Figure 1: The schematic of PaRO-DP is illustrated within a cluster comprising four GPUs. Each group in the cluster consists of two GPU devices. In particular, we illustrate the computation and communication phases of a global step that incorporates gradient accumulation. We detail the intra- and inter-group communication by using specific prefixes for each collective communication primitive.", "description": "This figure illustrates the PaRO-DP strategy using two different model partitioning schemes (PIIG and PNIG) within a cluster of 4 GPUs divided into 2 groups.  It shows the data flow and communication operations (intra- and inter-group) during a single global training step, including the forward pass, backward pass, gradient accumulation and gradient application phases. The visualization highlights the differences between PIIG and PNIG in terms of parameter and gradient partitioning, showcasing how PaRO-DP optimizes communication costs by leveraging intra-group communication whenever possible.", "section": "3.1 PaRO-DP"}]