[{"figure_path": "WI2VpcBdnd/tables/tables_1_1.jpg", "caption": "Table 1: Comparison with existing theoretical analysis of dataset distillation. The number of distilled data needed to recover original model's performance and models analyzed. \u201c-\u201d means not applicable. For linear ridge regression (LRR) and kernel ridge regression (KRR) with subjective feature mapping, our results only need one distilled data per class (k \u2264 d in our setting), which is far less than the existing work [9, 21] that require n or p points. As an example, k = 10,d = 3072, n = 50000 for CIFAR-10. The k, d, n of standard datasets are listed in Table 2. p is the dimension of feature mapping : Rd \u2192 RP.", "description": "This table compares the number of distilled data points required by different theoretical analyses to recover the original model's performance for various models.  It highlights that the proposed method requires significantly fewer data points (k, where k is the number of classes) compared to existing work (n, the number of data points in the original dataset, or p, the dimensionality of the feature mapping).", "section": "Related works"}, {"figure_path": "WI2VpcBdnd/tables/tables_2_1.jpg", "caption": "Table 2: k (number of class), d (dimension of data), and n (number of training data) of standard datasets.", "description": "This table presents the number of classes (k), the dimensionality of the data (d), and the number of training samples (n) for four benchmark datasets commonly used in machine learning: MNIST, CIFAR-10, CIFAR-100, and ImageNet-1k.  These values provide context for the scale of the datasets used in the experiments and the analysis presented in the paper.", "section": "3 Preliminaries"}, {"figure_path": "WI2VpcBdnd/tables/tables_8_1.jpg", "caption": "Table 3: Verification of our theory. Test accuracy of original models and models trained on the distilled dataset. IPC: images per class.", "description": "This table presents the results of an experiment designed to validate the theoretical findings of the paper.  The experiment compares the performance of original models (trained on the full dataset) against models trained on distilled datasets created using the proposed algorithm.  The table shows test accuracy for three different model types (Linear, FCNN, RFF) on three different datasets (MNIST, CIFAR-10, CIFAR-100).  The 'IPC' column represents the number of images per class in the distilled dataset. The results demonstrate that the analytically-computed distilled datasets effectively recover the original models' performance.", "section": "7 Experiments"}, {"figure_path": "WI2VpcBdnd/tables/tables_9_1.jpg", "caption": "Table 4: Comparison between our algorithm and KIP.", "description": "This table compares the performance and computational cost (in GPU seconds) of the proposed dataset distillation algorithm and KIP [25] across various datasets (MNIST, CIFAR-10, and CIFAR-100) and different numbers of distilled images per class (IPC).  The results demonstrate that the proposed algorithm achieves comparable accuracy while being significantly faster than KIP.", "section": "7 Experiments"}]