[{"figure_path": "vJLTcCBZVT/figures/figures_1_1.jpg", "caption": "Figure 1: Our method (D3M) improves worst group accuracy by identifying and removing the training samples which most negatively impact worst-group accuracy. Specifically, we use TRAK [33] to identify examples that exacerbate the discrepancy in group performance. We then remove and re-train a model on the remaining data.", "description": "This figure illustrates the core idea of the Data Debiasing with Datamodels (D3M) method.  It contrasts a naive data balancing approach with the D3M approach.  Data balancing attempts to address subgroup performance disparities by removing data from overrepresented groups, potentially sacrificing a significant portion of the dataset. In contrast, D3M identifies and removes specific data points that disproportionately harm the worst-performing group, achieving similar improvements in worst-group accuracy with substantially less data removal. The visualization uses examples of a dataset with spurious correlation between class labels (cat vs. dog) and an extra feature (fur color), showing how D3M can isolate harmful data points without requiring full group annotations.", "section": "1 Introduction"}, {"figure_path": "vJLTcCBZVT/figures/figures_6_1.jpg", "caption": "Figure 2: Worst group accuracy on CelebA-Age as a function of the number of examples k removed from the training set, using various removal methods. In green, D3M removes the k training examples with the most negative alignment scores Ai. The green star marks the value of k selected by our heuristic (Ai < 0). In blue is the performance of a random baseline that removes k examples at random from the training set, and in orange is dataset balancing, which removes examples randomly from the majority group. Compared to baselines, D3M efficiently improves worst group accuracy. Moreover, D3M performs on par with methods that have full access to both training and validation group labels.", "description": "The figure shows the worst-group accuracy on the CelebA-Age dataset as a function of the number of examples removed from the training dataset using four different methods: D3M, random removal, random removal from the majority group, and dataset balancing.  The green line represents D3M, which consistently outperforms other methods. The green star indicates the number of examples removed using D3M's heuristic. The figure highlights D3M's efficiency in improving worst-group accuracy while maintaining dataset size.", "section": "5.1 Quantitative results"}, {"figure_path": "vJLTcCBZVT/figures/figures_7_1.jpg", "caption": "Figure 3: Randomly sampled examples from the subpopulations with the most negative group alignment scores. We find that many of these examples have labeling errors (e.g., platinum blond instead of gray hair.)", "description": "This figure shows examples from subpopulations in the CelebA-Age dataset which the model has identified as having the most negative group alignment scores.  The negative scores indicate that these examples disproportionately contribute to the model's poor performance on the worst-performing group (worst-group accuracy).  Many of the images shown have labeling errors which illustrates that some of the model's inaccuracies are attributable to inconsistencies or errors in the dataset's labels rather than inherent limitations of the model.", "section": "5.2 Qualitative results"}, {"figure_path": "vJLTcCBZVT/figures/figures_7_2.jpg", "caption": "Figure 4: The average group alignment score of the training examples in each subpopulation of CelebA-Age. Subpopulations such as \u201cold\u201d with \u201cbushy eyebrows\u201d or \u201cyoung\u201d with \u201cgray hair", "description": "This figure shows the average group alignment scores for different subpopulations within the CelebA-Age dataset.  Each subpopulation is defined by a combination of age (old/young) and other attributes (e.g., bushy eyebrows, gray hair). The heatmap visually represents how strongly each subpopulation influences the model's worst-group accuracy. Subpopulations with highly negative scores are considered to disproportionately contribute to model bias. For example, the subpopulation of \"young\" individuals with \"gray hair\" has a strongly negative score, suggesting that these examples may be causing the model to perform poorly on a specific group.", "section": "5.2 Qualitative results"}, {"figure_path": "vJLTcCBZVT/figures/figures_8_1.jpg", "caption": "Figure 5: For four ImageNet classes, the most extreme (positive or negative) examples according to the top PCA direction of the TRAK matrix. Our method identifies color and co-occurrence biases.", "description": "This figure shows example images from four ImageNet classes (Tench, Strawberries, Red Wolf, and Cauliflower) that are most extreme according to the top principal component of the TRAK matrix. The top row displays examples with positive scores, while the bottom row shows examples with negative scores. The figure illustrates how the method identifies biases related to color and co-occurrence within the dataset.  For example, Tench images with humans are considered positive, while Tench images without humans are considered negative.  This indicates the algorithm detects co-occurrence bias, as the presence or absence of humans affects the classification. Similarly, the other classes show biases related to color variations, or other contextual factors.", "section": "6 Case Study: Finding and Mitigating Model Failures on ImageNet"}, {"figure_path": "vJLTcCBZVT/figures/figures_8_2.jpg", "caption": "Figure 6: Worst-group accuracy for the ImageNet classes studied in Section 6 after intervening with either D3M or AUTO-D3M (error bars over 1 std).", "description": "This figure shows the worst-group accuracy for eight ImageNet classes after applying three different methods: ERM (standard Empirical Risk Minimization), D3M (Data Debiasing with Datamodels), and Auto-D3M (automatic version of D3M).  Each bar represents a class, and the height of the bar indicates the worst-group accuracy achieved by each method. Error bars illustrate the standard deviation across multiple runs. The results demonstrate that both D3M and Auto-D3M improve worst-group accuracy compared to the baseline ERM, indicating their effectiveness in mitigating biases within the ImageNet dataset.  Auto-D3M, notably, achieves this without requiring validation group labels, highlighting its practicality.", "section": "6 Case Study: Finding and Mitigating Model Failures on ImageNet"}]