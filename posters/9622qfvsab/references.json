{"references": [{"fullname_first_author": "Jordan Hoffmann", "paper_title": "Training compute-optimal large language models", "publication_date": "2022-03-15", "reason": "This paper is foundational for understanding the training of large language models (LLMs), which are a core component of the multimodal models discussed in the paper."}, {"fullname_first_author": "Susan Zhang", "paper_title": "OPT: Open pre-trained transformer language models", "publication_date": "2022-05-01", "reason": "This paper introduces OPT, an open-source LLM, which is directly relevant to the study as it allows for the reproducibility and open analysis of the model's internal representations."}, {"fullname_first_author": "Aakanksha Chowdhery", "paper_title": "PaLM: Scaling language modeling with pathways", "publication_date": "2022-04-02", "reason": "This paper describes PaLM, a very large LLM that helps establish the context for scaling up language models, impacting the multimodal models."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-09", "reason": "This paper introduces Llama 2, another prominent open-source LLM that is directly used and analyzed in the presented research."}, {"fullname_first_author": "Xi Chen", "paper_title": "PaLI-X: On scaling up a multilingual vision and language model", "publication_date": "2023-05-18", "reason": "This paper expands the understanding to multilingual vision and language models, providing a broader context for the capabilities of large multimodal models."}]}