[{"figure_path": "merJ77Jipt/figures/figures_9_1.jpg", "caption": "Figure 2: Empirical distributions of the conditional potential outcomes. Left: p(Y(0) | x). Right: p(Y(1) | x).", "description": "This figure displays the empirical distributions of the conditional potential outcomes learned by DiffPO for a real-world dataset from medicine. The left panel shows the distribution of the potential outcome under no treatment (Y(0) given covariates x), while the right panel shows the distribution of the potential outcome under treatment (Y(1) given covariates x).  The distributions illustrate the differences in potential outcomes given the covariates, and it highlights how DiffPO learns complex distributions beyond simple point estimates, enabling more reliable decision-making.", "section": "6.5 Visualizing the learned distributions of POs"}, {"figure_path": "merJ77Jipt/figures/figures_23_1.jpg", "caption": "Figure 3: We manually perturb the propensity score during training on the synthetic data. We replace the estimated propensity score \u03c0(x) with a randomly sampled value \u03c0\u0303(x) from the interval (0, 1). The weight W\u0303(x, a) for each sample in the orthogonal diffusion loss L(\u03b8, \u03c0\u0303) is thus replaced by weight w\u0303(x, a). The CATE estimation error gradually converges as the sample size increases. This aligns with our expectation, as the loss remains robust even with varying errors in the estimation of nuisance functions.", "description": "This figure shows the results of simulation experiments to demonstrate the robustness of the orthogonal diffusion loss to errors in estimating the propensity score.  The x-axis represents the number of samples used for training, while the y-axis shows the CATE estimation error.  The plot demonstrates that even with manually perturbed propensity scores, the error decreases and converges to zero as the sample size increases, indicating that the orthogonal loss remains robust to misspecification of nuisance functions.", "section": "5.2 Orthogonal diffusion loss for addressing selection bias"}]