{"references": [{"fullname_first_author": "P. Abbeel", "paper_title": "Apprenticeship learning via inverse reinforcement learning", "publication_date": "2004-01-01", "reason": "This paper introduces apprenticeship learning, a foundational approach in inverse reinforcement learning (IRL) that significantly influenced the development of IRL algorithms and applications."}, {"fullname_first_author": "A. Y. Ng", "paper_title": "Algorithms for inverse reinforcement learning", "publication_date": "2000-01-01", "reason": "This paper is a seminal work in IRL, providing a comprehensive overview of fundamental algorithms and theoretical analyses that remain relevant and highly cited in the field."}, {"fullname_first_author": "B. D. Ziebart", "paper_title": "Modeling purposeful adaptive behavior with the principle of maximum causal entropy", "publication_date": "2010-01-01", "reason": "Ziebart's work introduced the maximum causal entropy framework for IRL, which addressed the issue of reward function non-uniqueness by incorporating an entropy regularization term, leading to more robust and well-defined solutions."}, {"fullname_first_author": "M. Geist", "paper_title": "A theory of regularized Markov decision processes", "publication_date": "2019-01-01", "reason": "This paper offers a rigorous theoretical foundation for regularized Markov decision processes (MDPs), providing essential background and supporting analyses for the development of regularized IRL methods."}, {"fullname_first_author": "J. Fu", "paper_title": "Learning robust rewards with adversarial inverse reinforcement learning", "publication_date": "2017-01-01", "reason": "This work significantly improves robustness to adversarial scenarios, addressing a critical limitation in the practical applicability of traditional IRL methods."}]}