[{"figure_path": "jfHkAEgKwH/tables/tables_5_1.jpg", "caption": "Table 1: Result comparison with previous SOTA methods on RefCOCO benchmarks.", "description": "This table compares the performance of the proposed LocCa model with state-of-the-art (SOTA) methods on three RefCOCO benchmarks: RefCOCO, RefCOCO+, and RefCOCOg.  The benchmarks evaluate performance on referring expression comprehension tasks. The table is divided into sections based on whether the models were trained on images that were also present in the validation/test sets.  LocCa achieves state-of-the-art results on all three benchmarks.", "section": "4.2 Quantitative results"}, {"figure_path": "jfHkAEgKwH/tables/tables_6_1.jpg", "caption": "Table 2: Comparison with baselines on RefCOCOs. A randomly initialized decoder is trained for REC and RES, with frozen image encoders. We report Acc@0.5 for REC and mIoU for RES. Here CLIP uses model checkpoints released by [7]; all other baselines use the same data as LocCa.", "description": "This table compares the performance of LocCa against several baseline models on the RefCOCO, RefCOCO+, and RefCOCOg datasets for both Referring Expression Comprehension (REC) and Referring Expression Segmentation (RES).  The baseline models include different versions of CLIP and Captioners.  The table highlights LocCa's superior performance, particularly when considering that its decoder was randomly initialized and the image encoders were frozen.  Acc@0.5 represents accuracy at a threshold of 0.5, and mIoU is the mean Intersection over Union.", "section": "4.2 Quantitative results"}, {"figure_path": "jfHkAEgKwH/tables/tables_6_2.jpg", "caption": "Table 3: Results on holistic image understanding tasks. Here CLIP uses model checkpoints released by [7]; all other baselines are trained on the same data as LocCa.", "description": "This table presents the quantitative results on holistic image understanding tasks for various models, including CLIP, Cap, CapPa, and LocCa.  The tasks evaluated are image classification on six datasets (ILK, SUN, Food, RES, PET, COCO), image captioning on COCO and Flickr datasets, OCR-VQA, VQA, and VQAv2, and GQA.  The table allows for comparison of LocCa's performance against several baseline models in a holistic image understanding context.", "section": "4.2 Quantitative results"}, {"figure_path": "jfHkAEgKwH/tables/tables_7_1.jpg", "caption": "Table 4: Results on PaLI-3 [31] transfers to diverse captioning and question answering tasks. LocCa consistently outperforms other image encoders, especially on tasks requiring understanding of objects, including both natural and text (OCR) objects.", "description": "This table presents the results of transferring the LocCa model to various downstream tasks using the PaLI-3 framework.  It compares LocCa's performance to several baselines (SigLIPL, Cap, CapPaz) across six different tasks: COCO captioning, VQAv2, OKVQA, TextVQA, ST-VQA, and TallyQA. The results demonstrate that LocCa consistently outperforms the baseline models, particularly on tasks that necessitate understanding objects (both natural images and OCR text).", "section": "4 Experiments"}, {"figure_path": "jfHkAEgKwH/tables/tables_8_1.jpg", "caption": "Table 5: Fine-tuning vision backbones with a linear head [94] for semantic seg. on ADE20k.", "description": "This table presents the results of fine-tuning different vision backbones (Seg ViT-L, Cap, CapPa, and LocCa) with a linear head for semantic segmentation on the ADE20k dataset.  The main metric reported is the mean Intersection over Union (mIoU), which measures the accuracy of the segmentation.  The table shows that LocCa outperforms other methods, suggesting its effectiveness in learning detailed visual representations.", "section": "4 Experiments"}, {"figure_path": "jfHkAEgKwH/tables/tables_9_1.jpg", "caption": "Table 6: Ablation study on applying loss on AREF and GCAP tasks during training.", "description": "This table presents the ablation study results focusing on the impact of applying loss on AREF and GCAP tasks during the training process.  It shows the performance of the LocCa model across various tasks (ImageNet classification, COCO captioning, VQAv2, and GQA) and metrics (RefCOCO and RefCOCO+) under different combinations of applying or not applying loss on AREF and GCAP. This allows analyzing the individual and combined effects of location-aware tasks on the model's overall performance.", "section": "4.4 Ablations"}, {"figure_path": "jfHkAEgKwH/tables/tables_18_1.jpg", "caption": "Table 1: Result comparison with previous SOTA methods on RefCOCO benchmarks.", "description": "This table compares the performance of the proposed method, LocCa, against other state-of-the-art (SOTA) methods on three RefCOCO benchmarks: RefCOCO, RefCOCO+, and RefCOCOg.  It shows the validation and test results (Acc@0.5) for each method on each benchmark, highlighting LocCa's superior performance. The table also categorizes methods based on whether they've seen validation/test images during pretraining. This provides context for the comparison by indicating factors influencing results.", "section": "4.2 Quantitative results"}, {"figure_path": "jfHkAEgKwH/tables/tables_18_2.jpg", "caption": "Table 8: Details of PaLI-3 [31] transfer evaluations, where the model is fine-tuned for each task separately.", "description": "This table details the hyperparameters used for fine-tuning the PaLI-3 model on various downstream tasks.  It shows the number of training steps, batch size, learning rate, and weight decay used for each task (COCO, VQAv2, OKVQA, TextVQA, ST-VQA, TallyQA). The image encoder was frozen during fine-tuning, and the resolution remained at 224x224.", "section": "A.2 LiT-Decoder setup for Referring Expression Comprehension and holistic image understanding tasks"}, {"figure_path": "jfHkAEgKwH/tables/tables_19_1.jpg", "caption": "Table 9: Video evaluation results with LocCa-pretrained vision encoder. We use CIDEr [107] (C) and BLEU-4 (B) for captioning, matching accuracy (A) for QA and YTBB, and the Jackard index (J) for VLOGS. LocCa improves upon CapPa, particularly when using the larger ViT-L/16 encoder.", "description": "This table presents the results of video evaluation tasks using the LocCa-pretrained vision encoder.  It compares the performance of LocCa and CapPa across several video-related tasks, including captioning (using CIDEr and BLEU-4 scores), question answering (using accuracy), and classification (using accuracy and Jaccard index).  The results are presented separately for two different encoder sizes (ViT-B/16 and ViT-L/16), demonstrating the impact of encoder size on performance.", "section": "4.2 Quantitative results"}, {"figure_path": "jfHkAEgKwH/tables/tables_19_2.jpg", "caption": "Table 2: Comparison with baselines on RefCOCOs. A randomly initialized decoder is trained for REC and RES, with frozen image encoders. We report Acc@0.5 for REC and mIoU for RES. Here CLIP uses model checkpoints released by [7]; all other baselines use the same data as LocCa.", "description": "This table compares the performance of LocCa with several baseline models on the RefCOCO, RefCOCO+, and RefCOCOg datasets for referring expression comprehension (REC) and referring expression segmentation (RES).  It highlights LocCa's superior performance, especially when using a randomly initialized decoder and frozen image encoders.  The results demonstrate the effectiveness of LocCa's approach, even when compared against models that utilize pretrained components or different training methods.", "section": "4.2 Quantitative results"}, {"figure_path": "jfHkAEgKwH/tables/tables_22_1.jpg", "caption": "Table 1: Result comparison with previous SOTA methods on RefCOCO benchmarks.", "description": "This table compares the performance of LocCa with state-of-the-art (SOTA) methods on three RefCOCO benchmark datasets: RefCOCO, RefCOCO+, and RefCOCOg.  The results are shown for both validation and test sets (testA and testB).  It highlights LocCa's performance relative to other models that have (or have not) seen the validation/test data during pre-training, distinguishing between different training regimes and model architectures.", "section": "4.2 Quantitative results"}]