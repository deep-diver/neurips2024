{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces CLIP, a vision-language model used extensively in the target paper for its cross-modal capabilities."}, {"fullname_first_author": "Chao Jia", "paper_title": "Scaling up visual and vision-language representation learning with noisy text supervision", "publication_date": "2021-07-01", "reason": "This paper introduces ALIGN, another crucial vision-language model utilized in the target paper for zero-shot downstream tasks."}, {"fullname_first_author": "Chen Ju", "paper_title": "Prompting visual-language models for efficient video understanding", "publication_date": "2022-10-01", "reason": "This paper directly tackles zero-shot temporal action detection (ZSTAD), providing a baseline for comparison and context for the target paper's contributions."}, {"fullname_first_author": "Sauradip Nag", "paper_title": "Zero-shot temporal action detection via vision-language prompting", "publication_date": "2022-08-01", "reason": "This paper proposes an influential method for ZSTAD based on vision-language prompting, which serves as a critical prior technique compared against in the target paper."}, {"fullname_first_author": "Thinh Phan", "paper_title": "Zeetad: Adapting pretrained vision-language model for zero-shot end-to-end temporal action detection", "publication_date": "2024-01-01", "reason": "This paper presents a strong contemporary ZSTAD method, offering a direct comparison and highlighting the advancements made by the target paper."}]}