[{"figure_path": "3ZAfFoAcUI/figures/figures_1_1.jpg", "caption": "Figure 1: (a) Pictorial depiction of gradual stacking and MIDAS. (b) Accuracy improvements (in %) for model trained with MIDAS over baseline for various task groups, despite having the same perplexity. For both 1B, 2B and 8B models, we see that improvements are mostly positive, and are much larger for tasks that require a lot of reasoning.", "description": "This figure shows a comparison between gradual stacking and MIDAS (a novel variant of gradual stacking proposed in the paper).  Panel (a) provides a visual illustration of the two methods. Panel (b) presents a bar chart displaying the accuracy improvements achieved by MIDAS compared to a baseline method across different task groups (closed-book QA, open-book QA, math word problems, and reasoning primitives) for models with 1B, 2B, and 8B parameters.  Importantly, MIDAS achieves these accuracy improvements while maintaining the same perplexity as the baseline.", "section": "1 Introduction"}, {"figure_path": "3ZAfFoAcUI/figures/figures_3_1.jpg", "caption": "Figure 2: (a) For an ALBert model trained with weight sharing across all layers, we measure the functional similarity between layers by looking at the top 1% activated neurons in each MLP layer and measure the intersection-over-union (IoU) metric for each pair of layers. Despite all layers having the same parameters, a natural functional similarity structure emerges around the middle. (b) For a UL2 model trained with GRADSTACK, we measure the cosine similarity between every pair of layer blocks for the first feedforward layer weights. (c) The same similarity measured for MIDAS. The cosine similarities for stacking based models suggests a strong connection to looped models, and MIDAS has a closer similarity structure to ALBert style looped models than GRADSTACK.", "description": "This figure shows the functional similarity between layers for different models.  (a) shows the similarity structure in an ALBert model with weight sharing. (b) shows the block similarity in a UL2 model trained with GRADSTACK. (c) shows the block similarity in a UL2 model trained with MIDAS. The results suggest that stacking-based models have a strong connection to looped models, and MIDAS is more similar to ALBert-style looped models than GRADSTACK.", "section": "Algorithm: MIDAS"}, {"figure_path": "3ZAfFoAcUI/figures/figures_4_1.jpg", "caption": "Figure 3: Histogram of accuracy improvements for models trained with MIDAS over baseline. The data points are MIDAS 1B models listed in Table 1. The figure shows that MIDAS-based models have much higher improvement in the contextual version of TyDiQA compared to the non-contextual version.", "description": "This histogram visualizes the accuracy improvements achieved by MIDAS over the baseline model for different tasks.  The data used comes from the 1B parameter MIDAS models detailed in Table 1. Notably, the improvement is significantly greater for the contextual TyDiQA task than for the non-contextual version, highlighting MIDAS's advantage in tasks that require contextual understanding.", "section": "Experiments: UL2 Pretraining"}, {"figure_path": "3ZAfFoAcUI/figures/figures_7_1.jpg", "caption": "Figure 4: Downstream evalulation vs validation log perplexity isoplots as training proceeds for baseline and MIDAS 1B models trained on the same data (stacking is 24% faster here). On the y-axis we track the performance on various task groups \u2013 closed book QA, open book QA, math word problems and our reasoning primitives from Section 5. On the x-axis the log perplexity is presented in the reverse order, thus downstream performance for both methods improves as log perplexity gets lower. For closed book QA (memorization) tasks MIDAS has very similar trends to baseline. For open book QA tasks and math word problems, MIDAS has much better downstream performance at an equivalent log perplexity. This showcases the inductive bias of MIDAS towards better overall quality and better reasoning abilities.", "description": "This figure shows the downstream evaluation performance (y-axis) against the validation log perplexity (x-axis) for both baseline and MIDAS models.  It compares four task groups: closed book QA, open book QA, math word problems, and reasoning primitives.  The plot demonstrates that MIDAS achieves similar or better performance on open-book QA and math problems while having the same or even better perplexity than the baseline model, highlighting the inductive bias of MIDAS towards reasoning abilities.", "section": "4 Inductive bias of stacking"}, {"figure_path": "3ZAfFoAcUI/figures/figures_8_1.jpg", "caption": "Figure 5: Accuracy improvements for model trained with MIDAS over baseline for representative reasoning primitives, despite having the same perplexity. We see clear improvements for MIDAS on almost all the primitives, both with 5-shot evaluation and after fine-tuning (FT) for the depth 1 and 2 primitive.", "description": "This figure shows the accuracy improvements achieved by MIDAS compared to the baseline model across various reasoning primitives.  The improvements are consistent across both 5-shot evaluation and fine-tuning, particularly for tasks of greater complexity (Depth 1 and 2).", "section": "Deep dive into reasoning improvements"}, {"figure_path": "3ZAfFoAcUI/figures/figures_15_1.jpg", "caption": "Figure 6: Measure of linearity for different layers in pretrained BERT-Base and BERT-Large models. For each layer i, we fit a linear map Ai between inputs Yi and the output of the Transformer block (without the residual connection), Yi+1 - Yi. We then measure the r2 score and cosine similarity for the learned linear fit. The first and last few layers demonstrate a much higher level of linearity compared to the rest of the layers.", "description": "This figure shows the linearity of different layers in pre-trained BERT models (Base and Large).  For each layer, a linear map is fit between inputs and the output of the transformer block (excluding residual connections). The R-squared and cosine similarity are then measured for the fit. The results show that the first and last few layers exhibit higher linearity than the rest, suggesting a difference in the role these layers play in the model.", "section": "3.2 MIDAS algorithm"}]