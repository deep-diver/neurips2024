[{"figure_path": "3ZAfFoAcUI/tables/tables_6_1.jpg", "caption": "Table 1: Downstream evaluations for UL2 pretrained models with 1B, 2B and 8B parameters. Comparisons include standard training (Baseline), gradual stacking (GRADSTACK) from [Reddi et al., 2023] and our proposed method MIDAS. The downstream evaluations are averaged over tasks within 3 task groups. See Appendix A for precise tasks included in each task group. For each cateory and model size, we highlight the top model is bolded and the second best model is underlined. Firstly, MIDAS is much better than GRADSTACK, thus justifying stacking in the middle. Secondly, MIDAS can match the log perplexity of baseline training while being roughly 24% faster. Furthermore, even the schedule with 40% speedup has much better downstream evaluations compared to baseline, even though it has worse log perplexity. The improvements are particularly large for task groups that require reasoning (open book QA, math word problems).", "description": "This table presents the downstream evaluation results for three different language model sizes (1B, 2B, and 8B parameters) trained using three different methods: standard training (Baseline), gradual stacking (GRADSTACK), and the proposed MIDAS method.  It compares their performance across three task groups: closed-book QA, open-book QA, and math word problems, showing the average accuracy improvements and validation loss. The table highlights the superior performance of MIDAS in terms of both speed and accuracy, particularly on tasks requiring reasoning, while maintaining comparable or even better perplexity compared to the Baseline.", "section": "Experiments: UL2 Pretraining"}, {"figure_path": "3ZAfFoAcUI/tables/tables_8_1.jpg", "caption": "Table 1: Downstream evaluations for UL2 pretrained models with 1B, 2B and 8B parameters. Comparisons include standard training (Baseline), gradual stacking (GRADSTACK) from [Reddi et al., 2023] and our proposed method MIDAS. The downstream evaluations are averaged over tasks within 3 task groups. See Appendix A for precise tasks included in each task group. For each cateory and model size, we highlight the top model is bolded and the second best model is underlined. Firstly, MIDAS is much better than GRADSTACK, thus justifying stacking in the middle. Secondly, MIDAS can match the log perplexity of baseline training while being roughly 24% faster. Furthermore, even the schedule with 40% speedup has much better downstream evaluations compared to baseline, even though it has worse log perplexity. The improvements are particularly large for task groups that require reasoning (open book QA, math word problems).", "description": "This table presents the downstream evaluation results for three different language models (1B, 2B, and 8B parameters) trained using three different methods: standard training (Baseline), gradual stacking (GRADSTACK), and the proposed MIDAS method.  The table compares the models' performance across three task categories (Closed Book QA, Open Book QA, and Math Word Problems) and overall, showing MIDAS's superior performance and efficiency.  The results highlight MIDAS's inductive bias towards improving reasoning tasks.", "section": "4 Inductive bias of stacking"}, {"figure_path": "3ZAfFoAcUI/tables/tables_13_1.jpg", "caption": "Table 1: Downstream evaluations for UL2 pretrained models with 1B, 2B and 8B parameters. Comparisons include standard training (Baseline), gradual stacking (GRADSTACK) from [Reddi et al., 2023] and our proposed method MIDAS. The downstream evaluations are averaged over tasks within 3 task groups. See Appendix A for precise tasks included in each task group. For each cateory and model size, we highlight the top model is bolded and the second best model is underlined. Firstly, MIDAS is much better than GRADSTACK, thus justifying stacking in the middle. Secondly, MIDAS can match the log perplexity of baseline training while being roughly 24% faster. Furthermore, even the schedule with 40% speedup has much better downstream evaluations compared to baseline, even though it has worse log perplexity. The improvements are particularly large for task groups that require reasoning (open book QA, math word problems).", "description": "This table presents the downstream evaluation results for UL2 pretrained language models with 1B, 2B, and 8B parameters, comparing standard training, gradual stacking, and the proposed MIDAS method.  It shows MIDAS outperforms other methods, especially on reasoning tasks, while maintaining comparable or even better training efficiency.", "section": "4 Inductive bias of stacking"}, {"figure_path": "3ZAfFoAcUI/tables/tables_14_1.jpg", "caption": "Table 1: Downstream evaluations for UL2 pretrained models with 1B, 2B and 8B parameters. Comparisons include standard training (Baseline), gradual stacking (GRADSTACK) from [Reddi et al., 2023] and our proposed method MIDAS. The downstream evaluations are averaged over tasks within 3 task groups. See Appendix A for precise tasks included in each task group. For each cateory and model size, we highlight the top model is bolded and the second best model is underlined. Firstly, MIDAS is much better than GRADSTACK, thus justifying stacking in the middle. Secondly, MIDAS can match the log perplexity of baseline training while being roughly 24% faster. Furthermore, even the schedule with 40% speedup has much better downstream evaluations compared to baseline, even though it has worse log perplexity. The improvements are particularly large for task groups that require reasoning (open book QA, math word problems).", "description": "This table presents downstream evaluation results for UL2 pretrained language models with 1B, 2B, and 8B parameters, comparing standard training (Baseline), gradual stacking (GRADSTACK), and the proposed MIDAS method.  It shows accuracy improvements across different task categories (closed book QA, open book QA, math word problems) and overall, highlighting the efficiency and reasoning improvements of MIDAS, especially for tasks requiring reasoning abilities.", "section": "4 Inductive bias of stacking"}, {"figure_path": "3ZAfFoAcUI/tables/tables_14_2.jpg", "caption": "Table 1: Downstream evaluations for UL2 pretrained models with 1B, 2B and 8B parameters. Comparisons include standard training (Baseline), gradual stacking (GRADSTACK) from [Reddi et al., 2023] and our proposed method MIDAS. The downstream evaluations are averaged over tasks within 3 task groups. See Appendix A for precise tasks included in each task group. For each cateory and model size, we highlight the top model is bolded and the second best model is underlined. Firstly, MIDAS is much better than GRADSTACK, thus justifying stacking in the middle. Secondly, MIDAS can match the log perplexity of baseline training while being roughly 24% faster. Furthermore, even the schedule with 40% speedup has much better downstream evaluations compared to baseline, even though it has worse log perplexity. The improvements are particularly large for task groups that require reasoning (open book QA, math word problems).", "description": "This table presents the downstream evaluation results for UL2 pretrained language models with 1B, 2B, and 8B parameters, comparing three training methods: standard training (Baseline), gradual stacking (GRADSTACK), and the proposed MIDAS method.  It shows that MIDAS achieves better performance on downstream tasks, particularly those requiring reasoning, while maintaining or even improving training efficiency and log perplexity compared to the baseline.", "section": "4 Inductive bias of stacking"}, {"figure_path": "3ZAfFoAcUI/tables/tables_20_1.jpg", "caption": "Table 1: Downstream evaluations for UL2 pretrained models with 1B, 2B and 8B parameters. Comparisons include standard training (Baseline), gradual stacking (GRADSTACK) from [Reddi et al., 2023] and our proposed method MIDAS. The downstream evaluations are averaged over tasks within 3 task groups. See Appendix A for precise tasks included in each task group. For each cateory and model size, we highlight the top model is bolded and the second best model is underlined. Firstly, MIDAS is much better than GRADSTACK, thus justifying stacking in the middle. Secondly, MIDAS can match the log perplexity of baseline training while being roughly 24% faster. Furthermore, even the schedule with 40% speedup has much better downstream evaluations compared to baseline, even though it has worse log perplexity. The improvements are particularly large for task groups that require reasoning (open book QA, math word problems).", "description": "This table presents downstream evaluation results for UL2 pretrained language models with 1B, 2B, and 8B parameters, comparing three training methods: standard training (Baseline), gradual stacking (GRADSTACK), and the proposed MIDAS method.  It shows accuracy across three task groups (Closed Book QA, Open Book QA, Math Word Problems) and overall average performance.  Key findings highlight MIDAS's superior performance and efficiency compared to GRADSTACK and, surprisingly, comparable or even better downstream performance than Baseline despite similar or worse perplexity scores, especially on reasoning-intensive tasks.  Appendix A provides details on specific tasks within each group.", "section": "4 Inductive bias of stacking"}, {"figure_path": "3ZAfFoAcUI/tables/tables_20_2.jpg", "caption": "Table 7: Fine-tuning results corresponding to Figure 5's 2 fine-tuning tasks. Additionally, this table reports the standard deviation across the 3 runs with \u00b1 std dev.", "description": "This table presents the fine-tuning results for the variable assignment tasks (depth 1 and 2) from Figure 5.  The results show the accuracy of MIDAS and Baseline models after fine-tuning on a small dataset (64 examples total). The accuracy is averaged across three runs, with standard deviations included to show variability.", "section": "B.2 Fine-tuning details"}]