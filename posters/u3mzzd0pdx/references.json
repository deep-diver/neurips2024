{"references": [{"fullname_first_author": "Yoshua Bengio", "paper_title": "Gradient-based optimization of hyperparameters", "publication_date": "2000-00-00", "reason": "This paper is foundational for gradient-based hyperparameter optimization, which is the main topic of the current paper."}, {"fullname_first_author": "Luca Franceschi", "paper_title": "Bilevel programming for hyperparameter optimization and meta-learning", "publication_date": "2018-00-00", "reason": "This paper is highly influential in establishing the theoretical framework for bilevel programming applied to hyperparameter optimization."}, {"fullname_first_author": "Moritz Hardt", "paper_title": "Train faster, generalize better: Stability of stochastic gradient descent", "publication_date": "2016-00-00", "reason": "This paper is crucial for understanding the concept of algorithmic stability and its connection to generalization, which is a key theoretical basis for the current paper."}, {"fullname_first_author": "Fan Bao", "paper_title": "Stability and generalization of bilevel programming in hyperparameter optimization", "publication_date": "2021-00-00", "reason": "This paper provides the first generalization upper bounds for UD-based bilevel optimization algorithm, which is directly relevant to the current paper's focus on stability analysis."}, {"fullname_first_author": "Olivier Bousquet", "paper_title": "Stability and generalization", "publication_date": "2002-00-00", "reason": "This paper provides the fundamental theoretical foundation for the notion of algorithmic stability, which is central to the current paper's analysis."}]}