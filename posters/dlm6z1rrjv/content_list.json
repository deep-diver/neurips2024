[{"type": "text", "text": "Is Knowledge Power? On the (Im)possibility of Learning from Strategic Interactions ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Nivasini Ananthakrishnan1, Nika Haghtalab1, Chara Podimata2, and Kunhe Yang ", "page_idx": 0}, {"type": "text", "text": "1UC Berkeley, {nivasini,nika,kunheyang}@berkeley.edu 2MIT & Archimedes AI, podimata@mit.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "When learning in strategic environments, a key question is whether agents can overcome uncertainty about their preferences to achieve outcomes they could have achieved absent any uncertainty. Can they do this solely through interactions with each other? We focus this question on the ability of agents to attain the value of their Stackelberg optimal strategy and study the impact of information asymmetry. We study repeated interactions in fully strategic environments where players\u2019 actions are decided based on learning algorithms that take into account their observed histories and knowledge of the game. We study the pure Nash equilibria (PNE) of a meta-game where players choose these algorithms as their actions. We demonstrate that if one player has perfect knowledge about the game, then any initial informational gap persists. That is, while there is always a PNE in which the informed agent achieves her Stackelberg value, there is a game where no PNE of the meta-game allows the partially informed player to achieve her Stackelberg value. On the other hand, if both players start with some uncertainty about the game, the quality of information alone does not determine which agent can achieve her Stackelberg value. In this case, the concept of information asymmetry becomes nuanced and depends on the game\u2019s structure. Overall, our findings suggest that repeated strategic interactions alone cannot facilitate learning effectively enough to earn an uninformed player her Stackelberg value. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Learning to act in strategic environments is fundamental to the study of decision making under uncertainty in a wide range of applications, such as security, economic policy, and market design (e.g., [31, 8, 19]). In these environments, acting and learning are intimately connected: agents\u2019 actions and the reactions they elicit generate payoffs, and help clarify the latent preferences of other agents. A central question is whether, through repeated interactions alone, agents (aka players) can overcome uncertainty about each other\u2019s preferences in order to achieve outcomes they could have achieved in the absence of uncertainty. An extensive line of work on learning in Stackelberg Games [5, 8, 31, 35] has focused on answering this question for achieving the Stackelberg value, which is the optimal payoff a player guarantees herself when assuming other players will best respond to her actions. While a player who hopes to attain her Stackelberg value in a one-shot game must know the game (i.e., know the utilities of all players), this line of work asks whether a player who is a-priori uninformed can overcome her lack of knowledge and attain her Stackelberg value through repeated interactions with other players. ", "page_idx": 0}, {"type": "text", "text": "By and large, existing works have studied this question by constructing learning algorithms for uninformed players that attain their Stackelberg value, through repeated interactions with other players who myopically best respond. While these results are encouraging for learning about the preferences of well-behaved best responding agents1, they do not provide clear evidence of the ability of uninformed players to learn from strategic interactions alone. Indeed, the two players\u2019 different attitudes towards their outcomes \u2014 namely, one player planning a long-term strategy to maximize her long-term payoff while the other player responding without considering the impact of her actions on her long-term payoff \u2014 confounds the overall impact uncertainty may have on how well players learn from strategic interactions; leaving one to wonder whether it was the lack of rational long-term planning on the part of one agent or some genius of the learning algorithm employed by the other agent that enabled her to learn from strategic interactions. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this paper, we revisit the problem of learning in strategic environments with a renewed focus on the impact of information asymmetry between two equally rational players who aim to maximize their total payoff. We ask again: can an uninformed player learn to attain the value of her Stackelberg outcome, through repeated interactions alone? In contrast to the aforementioned results, our findings largely imply that strategic interactions alone cannot facilitate learning effectively enough to earn an uninformed player the value of her optimal strategy. ", "page_idx": 1}, {"type": "text", "text": "Our Model and Contributions. To study the impact of informativeness on the ability of players to gain the payoff of their Stackelberg outcome, we study repeated interactions between two rational agents playing repeatedly a one-shot game $G\\sim\\mathcal{D}$ . While the players know $\\mathcal{D}$ , they may not know the realized game $G$ except perhaps through signals of differing precision about $G$ . For example, one player may know $G$ and another may have access to a signal that reveals $G$ with probability 0.5 and is uninformative (i.e., independently drawn from $\\mathcal{D}$ ) otherwise. Each player deploys an algorithm, which specifies her actions at every round, given all that the player has observed so far (e.g., history of actions and/or utilities experienced) and the information she possesses about $G$ . We consider a meta-game where players\u2019 actions are algorithms that specify agent\u2019s strategies in $T$ rounds of interactions and study pairs of algorithms that form pure Nash Equilibria in the meta-game. We use the overall utility attained by pairs of algorithms that form pure Nash Equilibria to draw a clear separation between the informed and uninformed players\u2019 ability to attain the value of their Stackelberg optimal strategies. ", "page_idx": 1}, {"type": "text", "text": "In the following, we use $\\operatorname{StackVal}_{i}(G)$ to refer to player $i$ \u2019s value of her optimal Stackelberg strategy in game $G$ . We summarize our results as follows. ", "page_idx": 1}, {"type": "text", "text": "\u2022 In Section 3, we study the full information asymmetry when player 1 $\\left(\\mathrm{P_{1}}\\right)$ knows the realized game $G$ and player 2 $\\left(\\mathrm{P_{2}}\\right)$ only has partial information based on an imperfect signal. We show a full separation in the achievable utilities. In particular, we show in Theorem 3.1 that for every distribution $\\mathcal{D}$ and realized game $G$ , there is a pure Nash Equilibrium (PNE) in the meta game induced between the algorithms\u2019 of the two players for which $\\mathrm{P_{1}}$ achieves her Stackelberg value, i.e., $\\operatorname{StackVal}_{1}(G)$ . On the other hand, Theorem 3.2 gives a distribution $\\mathcal{D}$ such that no PNE of the meta game allows $\\mathrm{P_{2}}$ to achieve $\\mathbb{E}_{G\\sim\\mathcal{D}}[\\mathrm{Stack}\\mathrm{Val}_{2}(G)]$ . In other words, for some realized game $G$ and all PNE of the meta-game, $\\mathrm{P_{2}}$ cannot achieve the value of her optimal Stackelberg strategy for $G$ . Taken together, Theorem 3.1 and Theorem 3.2 establish that learning through interactions alone is not sufficient to allow an uninformed player (in this case, $\\mathrm{P_{2}}$ ) to attain the value of her optimal Stackelberg strategy. Does this mean that $\\mathrm{P_{2}}$ in unable to learn the game matrix through interactions that form a PNE in the meta-game? This is not necessarily the case (see Observation A.4) as indeed $\\mathrm{P_{2}}$ may be able to learn the underlying game $G$ eventually. What our results imply is that in every PNE, either $\\mathrm{P_{2}}$ never learns the game $G$ sufficiently well, or she has enough information to identify $G$ but the stability condition for her algorithm to be in a PNE does not allow her to extract the value of her optimal Stackelberg strategy. This points to the limitations on what agents can achieve if their only source of learning is through repeated interactions. \u2022 In Section 4, we study a setting where neither player fully knows the realized game $G$ . Interestingly, a separation need not hold in this case. In particular, there are distributions $\\mathcal{D}$ where the player with a less informative signal about $G$ is able to extract her benchmark $\\mathbb{E}_{G\\sim\\mathcal{D}}[\\mathrm{StackVal}(G)]$ while the player with the more informative signal cannot achieve her corresponding benchmark. This occurs when the less-informed player is able to learn the identity of $G$ more efficiently, possibly due to the structure of $\\mathcal{D}$ . This is perhaps not surprising, given that a less-informed player can become more informed or even perfectly aware of the realized game faster, while the player who started the meta-game with a more informative signal continues to remain only partially informed. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "On the possibility and impossibility of learning from strategic interactions. We view our work as providing a different lens on studying learnability in the presence of strategic interactions that also elucidates the context and subtleties of a vast line of prior work in this space. By and large, prior work in this space [31, 8, 5, 35, 11, 25, 22, 15, 38, 28, 10] has attempted to establish the following message: \u201cAn uninformed player can always learn to achieve (even surpass) its Stackelberg value through repeated strategic interactions alone\u201d. At a high level, our work demonstrates the opposite, that \u201cIn some cases, an uninformed player cannot learn, through repeated interactions alone, to achieve its Stackelberg value\u201d. Of course, these messages, while both technically correct, are contrary to each other. So, what accounts for this difference? ", "page_idx": 2}, {"type": "text", "text": "One of our takeaways is that prior work\u2019s findings (that an uninformed can always overcome her informational disadvantage through repeated strategic interactions) heavily hinges on the lack of rationality of at least one of the agents in those strategic interactions. That is, the dynamics studied in prior work involve pairs of agent algorithms that are not best-responses to each other. On the other hand, our work shows that the inherent uncertainty about the game \u2014 or more precisely, the information asymmetry between two equally rational agents \u2014 can persists throughout repeated interactions and makes it impossible for an uninformed agent to overcome her informational disadvantage. ", "page_idx": 2}, {"type": "text", "text": "The processes of learning and acting based on the learned knowledge are naturally intertwined when dealing with uncertainty in strategic environments. Our work implies that it is precisely because of their intertwined nature that an uninformed agent cannot overcome her informational disadvantage from strategic interactions alone. That is, information disadvantage between a pair of rational agents persists for one of two reasons: Either actions taken by the agents\u2019 algorithms do not reveal enough information to identify the game at play, or if they do, the less-informed agents use of the elicited information would have lead the informed agent to deviate to an algorithm that barred her from learning in the first place. ", "page_idx": 2}, {"type": "text", "text": "1.1 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Algorithms and benchmarks for repeated principal-agent interactions. There is a vast literature investigating online algorithms and benchmarks in repeated games with agents under various behavioral models such as: 1) myopically best-responding [31, 5, 8, 38]; 2) optimizing time-discounted utilities [27, 29, 2, 1]; 3) employing no-regret [9, 17, 22, 25, 10, 26, 18], no-swap-regret [17, 32, 10], no counterfactual-internal-regret algorithms [11, 15], or online calibrated forecasting algorithms [28]. ", "page_idx": 2}, {"type": "text", "text": "Given a particular model of the agent, what is the optimal algorithm to employ? This has been studied in both the complete and incomplete information setting. In the complete information setting, the static algorithm of playing the optimal Stackelberg strategy is shown to be optimal against no-swap-regret agents [17, 28]. But it is not necessarily optimal against general no-regret algorithms, including common algorithms such as EXP3 [9, 17, 26, 36]. Additionally, it is not optimal against no-swap-regret agents in Bayesian games where agents have hidden information but is optimal if agents satisfy a stronger notion called no-polytope-swap-regret [32]. ", "page_idx": 2}, {"type": "text", "text": "Long-term rationality of agents in the meta-game. Instead of modeling agents as no-regret learners, another line of research treats the repeated game as a meta-game in which players\u2019 actions are their choice of algorithms. Towards understanding the PNE of this meta-game, Brown et al. [10] show that no pair of no-swap-regret algorithms can form a PNE unless the stage game has a PNE. Previous work discussed above on optimally responding to no-regret agents also has implications on the meta-game\u2019s PNE such as (1) no-swap-regret algorithms are supported in a meta-game PNE for all games $G$ [17], and (2) there are games where no meta-game PNE contains certain common regret-minimizing algorithms such as EXP3 [10]. We discuss these implications in Appendix C. ", "page_idx": 2}, {"type": "text", "text": "Kolumbus and Nisan [30] study a meta-game where players are restricted to choose no-regret algorithms but have the option to manipulate their private information. They show that non-truthful PNE exists in multiple classes of games. Besides PNE, some previous works also study Stackelberg strategies of meta-games [14, 40]. Recently, Arunachaleswaran et al. [3] study the Pareto optimality relative to all possible games instead of exact optimality in a particular game. ", "page_idx": 2}, {"type": "text", "text": "Information asymmetry in repeated games. The final line of related work is the substantial literature on information asymmetry and repeated interactions including classical work by Aumann et al. [4]. They also study repeated games between a player knowing the game and one who does not. For zero-sum games, they show that all PNE of the meta-game yields the same utility to the informed player and this utility can be higher than the informed player\u2019s one-shot utility. The higher utility is due to the informed player\u2019s ability to shape the learned beliefs of the uninformed player and this power of information is also shown in a recent line of work on follower deception in Stackelberg games [24, 33, 23, 6, 13, 12]. ", "page_idx": 3}, {"type": "text", "text": "2 Model and Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We study games between two players, referred to as $\\mathrm{{P}_{1}}$ and $\\mathrm{P_{2}}$ . Wlog, we assume that $\\mathrm{P_{1}}$ is generally more informed than $\\mathrm{P_{2}}$ (to be defined formally below). Although we focus on repeated games, we first provide the setting for one-shot games and then build upon it for repeated games. ", "page_idx": 3}, {"type": "text", "text": "Bayesian Games. A game $G$ is a tuple $(\\mathcal{A}_{1},\\mathcal{A}_{2},U_{1},U_{2})$ , where $\\boldsymbol{A}_{i}$ is $\\mathrm{P}_{i}$ \u2019s discrete action space, and $U_{i}:A_{1}\\times A_{2}\\rightarrow\\mathbb{R}$ is $\\mathrm{P}_{i}$ \u2019s utility function $(i\\in\\{1,2\\})$ . A Bayesian game is described by a family of games $\\mathcal{G}$ and a distribution $\\mathscr{D}\\in\\Delta(\\mathscr{G})$ over games in this family, where $\\mathcal{G}$ consists of games sharing the same action space. When $\\mathrm{{P}_{1}}$ plays action $x\\in A_{1}$ and $\\mathrm{P_{2}}$ plays action $y\\in A_{2}$ , then they receive utilities $U_{1}(x,y)$ and $U_{2}(x,y)$ respectively. We sometimes overload notation and write $U_{1}(x,y;G),U_{2}(x,y;G)$ to denote that the utilities of the two players come from a particular game instance $G$ . Instead of pure strategies (i.e., playing discrete actions), the players can also choose to play mixed strategies $\\textbf{x}\\in\\,\\Delta(A_{1})$ and $\\textbf{y}\\in\\,\\Delta({\\mathcal A}_{2})$ for players 1 and 2 respectively. To simplify notation, we sometimes write $U_{i}(\\mathbf{x},\\mathbf{y})$ in place of $\\mathbb{E}_{x\\sim\\mathbf{x},y\\sim\\mathbf{y}}[U_{i}(x,y)]$ for $\\mathrm{P}_{i}$ \u2019s utility. Unless specified otherwise, we assume that the players are moving simultaneously and they both know the prior distribution $\\mathcal{D}$ . We also assume that every game $G$ in the support of $\\mathcal{D}$ has no weakly dominated action for either player. An action $x_{0}\\in A_{1}$ is weakly dominated for $\\mathrm{P_{1}}$ in $G$ if there exists $\\mathbf{x}\\in\\Delta(A_{1}\\setminus\\{x_{0}\\})$ s.t., $U_{1}(\\mathbf{x},y;G)\\geq U_{1}(x_{0},y;G)$ for every $y\\in A_{2}$ . The weakly dominance property of actions $y_{0}\\in A_{2}$ is defined symmetrically for $\\mathrm{P_{2}}$ . ", "page_idx": 3}, {"type": "text", "text": "Optimistic Stackelberg Value. The optimistic Stackelberg value of a game $G$ for $\\mathrm{P_{1}}$ , denoted with $\\operatorname{StackVal}_{1}(G)$ , is the optimal value of the following optimization problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname{StackVal}_{1}(G)\\triangleq\\operatorname*{max}_{\\substack{\\mathbf{x}^{\\star}\\in\\Delta(A_{1})\\,y\\in\\mathrm{BR}_{2}(\\mathbf{x}^{\\star};G)}}U_{1}(\\mathbf{x}^{\\star},y),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\begin{array}{r}{{\\mathsf{B R}}_{2}(\\mathbf{x}^{\\star};G)\\,\\triangleq\\,\\mathrm{argmax}_{y\\in A_{2}}\\,U_{2}(\\mathbf{x}^{\\star},y)}\\end{array}$ indicates $\\mathrm{P_{2}}$ \u2019s set of best responses to $\\mathbf{x}^{\\star}$ . When there are multiple actions in $\\mathsf{B R}_{2}(\\mathbf{x}^{\\star};G)$ , ties are broken optimistically in favor of $\\mathrm{{P}_{1}}$ . We use $(\\mathbf{x}^{\\star}(G),y(\\mathbf{x}^{\\star};G))$ to denote the pair of strategies that achieves the value $\\operatorname{StackVal}_{1}(G)$ . For $\\mathrm{P_{2}}$ , the optimistic Stackelberg value $\\operatorname{StackVal}_{2}(G)$ and $(x(\\mathbf{y}^{\\star};G),\\mathbf{y}^{\\star}(G))$ are defined symmetrically. Finally, we define $\\operatorname{StackVal}_{i}({\\mathcal{D}})\\triangleq\\mathbb{E}_{G\\sim{\\mathcal{D}}}[\\operatorname{StackVal}_{i}(G)]$ to be the expected optimistic Stackelberg value for $\\mathrm{P}_{i}$ under the prior distribution $\\mathcal{D}$ $)\\;(i\\in\\{1,2\\})$ ). ", "page_idx": 3}, {"type": "text", "text": "Game Information. We assume that both players know the prior $\\mathcal{D}$ . After the game $G\\sim\\mathcal{D}$ is realized, each player $\\mathrm{P}_{i}$ also receives additional information about the realization of $G$ , which is characterized by a signal $s_{i}\\in\\mathcal{G}$ . We assume that nature generates both signals $s_{1},s_{2}$ independently and with potentially different precision levels $p_{1},p_{2}\\in[0,1]$ , and each player can only observe their own signal. Fixing a precision $p_{i},\\,s_{i}$ perfectly reveals the true game $G$ with probability $p_{i}$ , and with probability $1-p_{i}$ , it provides an independent draw from the prior distribution $\\mathcal{D}$ . Formally, the conditional distribution of $s_{i}$ given $G$ is defined as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\forall G,s_{i}\\in\\mathcal{G},\\quad\\varphi_{p_{i}}(s_{i}\\mid G)=p_{i}\\cdot\\mathbb{1}\\left\\{s_{i}=G\\right\\}+(1-p_{i})\\cdot\\mathcal{D}(s_{i}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "While each player can only observe their own signal $s_{i}$ , we assume that the distributions generating both signals are common knowledge, i.e., both players know $p_{1}$ and $p_{2}$ . Note that when $p_{i}=1$ , the signal $s_{i}$ perfectly correlates with the realization $G$ , in which case we say that $\\mathrm{P}_{i}$ is fully-informed or have perfect knowledge about which game is being played. On the other extreme, if $p_{i}=0$ , then the signal $s_{i}$ reveals no additional information compared to the prior distribution $\\mathcal{D}$ . In this case, we call $\\mathrm{P}_{i}$ uninformed. Throughout this paper, we focus on settings with information asymmetry where we always assume $\\mathrm{P_{1}}$ is more informed than $\\mathrm{P_{2}}$ , i.e., $p_{1}>p_{2}$ . ", "page_idx": 3}, {"type": "text", "text": "Repeated Games. In this paper, we focus on repeated (Bayesian) games. Initially, nature draws a game $G\\in{\\mathcal{G}}$ from prior $\\mathcal{D}$ . The game is then fixed and repeated for $T$ rounds. At each round $t\\in[T]$ , $\\mathrm{P_{1}}$ and $\\mathrm{P_{2}}$ play strategies $\\mathbf{x}^{t},\\mathbf{y}^{t}$ and obtain utilities $U_{1}(\\mathbf{\\bar{x}}^{t},\\mathbf{y}^{t};G),U_{2}(\\mathbf{x}^{t},\\mathbf{y}^{t};G)$ respectively. We call $G$ the stage game of the repeated interaction. ", "page_idx": 4}, {"type": "text", "text": "Without loss of generality, we use algorithms to describe both players\u2019 adaptive strategies in the repeated game. For $i\\in\\{1,2\\}$ , we use $\\pi_{i}$ to denote the algorithm used by $\\mathrm{P}_{i}$ , which is a sequence of mappings $(\\pi_{i}^{t})_{t\\in[T]}$ that at each round maps from player $i$ \u2019s information about the game and historical observations to the distribution of mixed strategies from which the next strategy is drawn. Specifically, for each round $t$ , the mapping is defined as $\\bar{\\pi}_{i}^{t}:(s_{i};H_{i}^{1:t-1})\\mapsto\\Delta(\\mathbf{x}_{t})$ , where $s_{i}$ is the signal received by $\\mathrm{P}_{i}$ about the realization of $G$ , and $H_{i}^{r}$ is the feedback that $\\mathrm{P}_{i}$ observed at round $r$ $(r\\in[t-1])$ . When both players observe each other\u2019s realized strategies as well as their own (but not the other\u2019s) realized utilities, we have $H_{i}^{r}=\\left(\\mathbf{x}^{r},\\mathbf{y}^{r},U_{i}(\\mathbf{x}^{r},\\mathbf{y}^{r};G)\\right)$ . We call this the full-information feedback setting. We also consider the bandit feedback setting, where the players do not observe the strategies of their opponent, i.e., $H_{1}^{r}=(\\mathbf{x}^{r},U_{1}(\\mathbf{x}^{r},\\mathbf{y}^{r};G))$ and $H_{2}^{r}=(\\mathbf{y}^{r},U_{2}(\\mathbf{x}^{r},\\mathbf{y}^{r};G))$ . ", "page_idx": 4}, {"type": "text", "text": "Trajectories and expected utilities. Consider a fixed pair of algorithms $(\\pi_{1},\\pi_{2})$ . Under every realization of $\\left(G,s_{1},s_{2}\\right)$ , algorithms $(\\pi_{1},\\pi_{2})$ induce a distribution over trajectories of mixed strategy pairs of length $T$ , which we denote with $(\\mathbf{x}^{t},\\mathbf{y}^{t})_{t\\in[T]}\\,\\sim\\,\\mathcal{T}^{T}(\\pi_{1},\\pi_{2};\\check{G_{,}}\\,s_{1},s_{2})\\,\\in\\,\\Delta(\\Delta(\\mathcal{A}_{1})^{T}\\,\\check{\\times}\\,\\mathcal{T}(\\mathcal{A}_{1})^{T}).$ $\\Delta(A_{2})^{T})$ . In particular, the signals $s_{i}$ $(i\\in\\{1,2\\})$ are inputs of $\\pi_{i}$ that specify $\\mathrm{P}_{i}$ \u2019s behavior upon receiving certain feedbacks, whereas $G$ influences $\\mathrm{P}_{i}$ \u2019s observed utilities, which is part of the feedback and indirectly influences $\\mathrm{P}_{i}$ \u2019s strategies of the next round.2 We also use $\\mathcal{T}^{T}(\\pi_{1},\\dot{\\pi}_{2};G)$ to denote the mixture of $\\dot{T}^{T}(\\pi_{1},\\pi_{2};G,s_{1},s_{2})$ as $s_{i}\\sim\\varphi_{p_{i}}(\\cdot\\mid G)$ for $i\\in\\{1,2\\}$ . ", "page_idx": 4}, {"type": "text", "text": "When the realized game is $G$ and players use algorithms $(\\pi_{1},\\pi_{2})$ with time horizon $T$ , the expected average utility of $\\mathrm{P}_{i}$ under $G$ , denoted as $\\bar{U}_{i}(\\pi_{1},\\bar{\\pi}_{2};G)$ , can be expressed as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\bar{U}_{i}^{T}(\\pi_{1},\\pi_{2};G)\\triangleq\\underset{\\tau\\sim\\mathcal{T}^{T}(\\pi_{1},\\pi_{2};G)}{\\mathbb{E}}\\left[\\frac{1}{T}\\sum_{t\\in[T]}U_{i}(\\mathbf{x}^{t},\\mathbf{y}^{t};G)\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We further define $\\bar{U}_{i}^{T}(\\pi_{1},\\pi_{2};T)\\triangleq\\mathbb{E}_{G\\sim{\\cal D}}\\,\\bar{U}_{i}^{T}(\\pi_{1},\\pi_{2};G)$ as the expected average utility under $\\mathcal{D}$ . ", "page_idx": 4}, {"type": "text", "text": "Equilibrium in the Meta-Game. We model the rationality of long-term players by treating the repeated Bayesian game $\\mathcal{D}$ as a meta-game, where each player $\\mathrm{P}_{i}$ \u2019s action is an algorithm $\\pi_{i}$ , and the utilities of each pair of action $(\\pi_{1},\\pi_{2})$ are given by $\\bar{U}_{i}(\\dot{\\pi}_{1},\\dot{\\pi}_{2};D)$ . Our analysis focuses on the pure Nash equilibria (PNE) of this meta game applied to the asymptotic regime $T\\to\\infty$ . ", "page_idx": 4}, {"type": "text", "text": "Definition 2.1 (PNE of the Meta-Game). We say that a pair of algorithms $(\\pi_{1},\\pi_{2})$ form a pure Nash equilibrium (PNE) in the meta-game if for all $i\\in\\{1,2\\}$ and all other algorithms $\\pi_{i}^{\\prime}$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{T\\to\\infty}\\left(\\bar{U}_{i}^{T}(\\pi_{i}^{\\prime},\\pi_{-i};D)-\\bar{U}_{i}^{T}(\\pi_{i},\\pi_{-i};D)\\right)\\leq0,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\pi_{-i}$ denotes the algorithm of $\\mathrm{P}_{i}$ \u2019s opponent. ", "page_idx": 4}, {"type": "text", "text": "Finally, we define no-regret and no-swap regret algorithms below. ", "page_idx": 4}, {"type": "text", "text": "Definition 2.2 (No-(Swap) Regret Algorithms). An algorithm $\\pi_{1}$ of $\\mathrm{P_{1}}$ is called no-regret if for all adversarial sequences $\\mathbf{y}^{\\mathbf{1}:T}\\in\\overline{{\\Delta}}(A_{2})^{\\top}$ , the strategies $\\mathbf{x}^{1:T}$ output by $\\pi_{1}$ satisfies ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}[r e g r e t_{1}^{T}]\\triangleq\\mathbb{E}\\left[\\operatorname*{max}_{x^{\\star}\\in\\mathcal{A}_{1}}\\sum_{t\\in[T]}U_{1}(x^{\\star},\\mathbf{y}^{t})-U_{1}(\\mathbf{x}^{t},\\mathbf{y}^{t})\\right]\\in o(T).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Furthermore, $\\pi_{1}$ is called no swap-regret $i f$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}[s w a p\\!-\\!r e g r e t_{1}^{T}]\\triangleq\\mathbb{E}\\left[\\operatorname*{max}_{f:A_{1}\\to A_{1}}\\sum_{t\\in[T]}U_{1}\\big(f(\\mathbf{x}^{t}),\\mathbf{y}^{t}\\big)-U_{1}(\\mathbf{x}^{t},\\mathbf{y}^{t})\\right]\\in o(T),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $f(\\mathbf{x})\\in\\Delta(A_{1})$ denotes the mixed strategy induced by $f(x)$ as $x\\sim\\mathbf{x}$ . We define no-(swap) regret algorithms for $\\mathrm{P_{2}}$ symmetrically. ", "page_idx": 4}, {"type": "text", "text": "We remark that there exist no-regret and no-swap regret algorithms under both the full-information feedback and bandit feedback setting. ", "page_idx": 4}, {"type": "text", "text": "3 Interactions between fully-informed $\\mathrm{{P}_{1}}$ and partially-informed $\\mathrm{P_{2}}$ ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we analyze the setting with a fully-informed $\\mathrm{P_{1}}$ \u2014 i.e., $\\mathrm{P_{1}}$ knows the game $G$ being played \u2014 and a partially informed $\\mathrm{P_{2}}$ . In other words, algorithms $\\pi_{1}$ and $\\pi_{2}$ can each take an observable signal as input, where the signals received by $\\mathrm{P_{1}}$ and $\\mathrm{P_{2}}$ are independently drawn from signal distributions $\\bar{\\varphi_{p_{1}}}\\bar{(}\\cdot\\vert G)$ and $\\varphi_{p_{2}}(\\cdot|G)$ with precision $p_{1}=1$ and $p_{2}<1$ , respectively. Recall that each $\\mathrm{P}_{i}$ sees her realized signal $s_{i}$ and knows the precision levels $p_{1},p_{2}$ of both players\u2019 signals. ", "page_idx": 5}, {"type": "text", "text": "Our main takeaway is that there is a separation in the benchmarks for achievable cumulative utilities between $\\mathrm{P_{1}}$ and $\\mathrm{P_{2}}$ in this setting, when $\\mathrm{P_{1}}$ and $\\mathrm{P_{2}}$ employ algorithms that form a PNE of the metagame. This is not surprising in the one-shot setting. But in the repeated setting, even with infinite rounds for $\\mathrm{P_{2}}$ to learn the game from feedback gained throughout the interaction, we show that there is still a separation in achievable benchmarks. ", "page_idx": 5}, {"type": "text", "text": "This separation could be due to two factors: 1) $\\mathrm{P_{2}}$ \u2019s inability to learn the game based on repeated interactions, and 2) $\\mathrm{P_{2}}$ \u2019s failure to achieve the benchmark utility despite successful learning. We discuss this in more detail in Section 3.2 and Appendix A. We show that if $\\mathrm{P_{2}}$ was able to learn the game based on external signals, then $\\mathrm{P_{2}}$ would be able to achieve the benchmark. This highlights a fundamental difference between learning based on interactions with the other player and learning independently without relying on the other player. In the latter scenario, a utility benchmark is always achievable, whereas in the former, it is sometimes unattainable. ", "page_idx": 5}, {"type": "text", "text": "The benchmark that we will show separates $\\mathrm{P_{1}}$ from $\\mathrm{P_{2}}$ is the average Stackelberg value with the player of interest as leader. Recall that $\\operatorname{StackVal}_{i}({\\mathcal{D}})=\\mathbb{E}_{G\\sim{\\mathcal{D}}}[\\operatorname{StackVal}_{i}(G)]$ for each $\\mathrm{P}_{i}$ . We will demonstrate the separation by showing that $\\mathrm{P_{1}}$ is always able to achieve this benchmark through a PNE of the meta-game, for all $\\mathcal{D}$ , but there exists some distribution $\\mathcal{D}$ in which no PNE of the meta-game yields $\\mathrm{P_{2}}$ her counterpart benchmark. ", "page_idx": 5}, {"type": "text", "text": "We will first state the theorems and provide proof sketches later. Our first theorem (Theorem 3.1) asserts that $\\mathrm{P_{1}}$ can achieve the benchmark $\\mathrm{StackVal}_{1}(\\mathcal{D})$ by explicitly constructing a PNE pair of algorithms $(\\pi_{1},\\pi_{2})$ that grants $\\mathrm{P_{1}}$ this utility in the asymptotic regime. In the proof of this theorem, we provide the rate of convergence to this utility (Remark B.1). ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.1 (Benchmark achievable by $\\mathrm{P_{1}}$ for all $\\mathcal{D}$ ). For every game family $\\mathcal{G}$ and every distribution $\\mathcal{D}\\in\\Delta(\\mathcal{G})$ supported on it, there exists an algorithm pair $(\\pi_{1},\\pi_{2})$ such that $(\\pi_{1},\\pi_{2})$ is a PNE of the meta-game, and $\\forall G\\in\\mathcal{G},\\bar{U}_{1}^{T}(\\pi_{1},\\pi_{2};G)\\geq\\bar{S t a c k V a}\\dot{l_{1}}(G)-o_{T}(\\dot{1}).$ ", "page_idx": 5}, {"type": "text", "text": "That is, for every realized game $G$ , the expected average utility of $\\mathrm{P_{1}}$ over $T$ rounds tends to $S t a c k V a l_{1}(G)$ as $T\\rightarrow\\infty$ . The expectation is over the trajectories \u2014 sequence of player strategies, and resulting utilities induced by the algorithms $\\pi_{1},\\pi_{2}$ and $G$ . ", "page_idx": 5}, {"type": "text", "text": "The next theorem completes the separation argument by constructing a specific game distribution where no PNE of the meta-game allows $\\mathrm{P_{2}}$ to asymptotically achieve the benchmark $\\operatorname{stackVal}_{2}(\\mathcal{D})$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.2 (Benchmark unachievable by $\\mathrm{P_{2}}$ for some $\\mathcal{D}$ ). For all thresholds $p^{\\star}\\in[0,1)$ , there exists a game family $\\mathcal{G}$ and a distribution $\\mathcal{D}\\in\\Delta(\\mathcal{G})$ , s.t., $\\forall p_{2}\\leq p^{\\star}$ , all PNE $(\\pi_{1},\\pi_{2})$ of the meta-game where $\\mathrm{P_{2}}\\,^{,}s$ signal is of precision $p_{2}$ must suffer $\\mathbb{E}_{G\\sim\\mathcal{D}}\\,\\bar{U}_{2}^{T}(\\pi_{1},\\pi_{2};G)\\le S t a c k V a l_{2}(\\mathcal{D})-\\Omega_{T}(1)$ . ", "page_idx": 5}, {"type": "text", "text": "This implies that there is a game $G\\in{\\mathcal{G}}$ such that when $G$ is realized, $\\mathrm{P_{2}}$ \u2019s expected average utility over $T$ rounds remains strictly bounded below Stack $V a l_{2}(G)$ even as $T\\rightarrow\\infty$ . ", "page_idx": 5}, {"type": "text", "text": "Theorems 3.1 and 3.2 show that there is a separation in achievable benchmark whenever the lessinformed player is at any informational disadvantage, however small, compared to the fully-informed player. $\\mathrm{P_{2}}$ \u2019s signal could be arbitrarily close to being fully informative (i.e., $p_{2}$ is arbitrarily close to 1), but there is still a barrier between what $\\mathrm{P_{2}}$ can achieve compared to $\\mathrm{P_{1}}$ , when $\\mathrm{P_{1}}$ has full knowledge. ", "page_idx": 5}, {"type": "text", "text": "3.1 Proof sketches of main theorems ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Now we present proof sketches for the two theorems, defering the full proofs to the appendices. ", "page_idx": 5}, {"type": "text", "text": "Proof sketch of Theorem 3.1. Our proof puts together results from previous work [17, 28]. We present the proofs of these results for completion. In this proof sketch, we will prove the theorem when every game $G$ in the support of $\\mathcal{D}$ is such that $\\mathrm{P_{2}}$ has a unique best-response $y(\\mathbf{x}^{\\star};G)$ to $\\mathrm{P_{1}}$ \u2019s optimal Stackelberg strategy $\\mathbf{x}^{\\star}(G)$ . The full proof is in Appendix B.1. ", "page_idx": 5}, {"type": "text", "text": "Let $\\pi_{1}$ be the algorithm that plays $\\mathrm{P_{1}}$ \u2019s optimal Stackelberg strategy of the realized game $G$ (i.e., $\\mathbf{x}^{\\star}(G))$ at every round. Since $\\mathrm{P_{1}}$ has access to a signal that fully reveals the realized game $G$ , $\\mathrm{P_{1}}$ can compute $\\mathbf{x}^{\\star}(G)$ and employ this strategy. Let $\\pi_{2}$ be a no-swap-regret algorithm in the banditfeedback setting (the algorithm is only based on the utilities received in each round). Such algorithms exist [7, 16, 34] and are deployable by $\\mathrm{P_{2}}$ without any knowledge of the game played or $\\mathrm{P_{1}}$ \u2019s strategies. Note that $\\pi_{2}$ does not use $\\mathrm{P_{2}}$ \u2019s signal $s_{2}$ . Therefore our analysis holds for all levels of precision of $s_{2}$ . ", "page_idx": 6}, {"type": "text", "text": "First, let us analyze the expected utility of $\\mathrm{P_{2}}$ due to $(\\pi_{1},\\pi_{2})$ . The generated trajectories when the game $G$ is realized are of the form $(\\mathbf{x}^{\\star}(G),\\mathbf{y}^{t})_{t=1}^{\\infty}$ . Since $\\pi_{2}$ is a no-swap-regret algorithm, the regret of this trajectory up to round $T$ is sub-linear in $T\\,(o(T))$ . ", "page_idx": 6}, {"type": "text", "text": "Since we assumed that $\\mathsf{B R}_{2}(\\mathbf{x}^{\\star};G)$ is unique, any round where $\\mathrm{P_{2}}$ is not employing this unique best-response $(y(\\mathbf{x}^{\\star};G))$ causes $\\mathrm{P_{2}}$ to incur regret. The no-swap-regret property for $\\mathrm{P_{2}}$ essentially means that $\\mathrm{P_{2}}$ \u2019s strategies in the trajectory $(\\mathbf{y}^{t})_{t=1}^{\\infty}$ become close to $y(\\mathbf{x}^{\\star};G)$ . And as a result, $\\mathrm{P_{1}}$ \u2019s utility per round gets close to $U_{1}(\\mathbf{x}^{\\star}(G),y(\\mathbf{x}^{\\star};G);G)$ which is $\\operatorname{StackVal}_{1}(G)$ . ", "page_idx": 6}, {"type": "text", "text": "More formally, $\\mathrm{P_{1}}$ \u2019s cumulative utility over $T$ rounds satisfies $\\begin{array}{r l}{\\sum_{t\\in[T]}U_{1}(\\mathbf{x}^{\\star}(G),\\mathbf{y}^{t})}&{{}\\geq}\\end{array}$ $\\begin{array}{r}{\\operatorname{StackVal}_{1}(G)\\cdot T-c_{1}\\sum_{t\\in[T]}\\|\\mathbf{y}^{t}-\\mathbf{y}(\\mathbf{x}^{\\star};G)\\|_{1}}\\end{array}$ 1, where $\\mathbf{y}(\\mathbf{x}^{\\star};G)$ is the one-hot vector encoding of $y(\\mathbf{x}^{\\star};G)$ and $\\begin{array}{r}{c_{1}=\\operatorname*{max}_{y\\in\\mathcal{A}_{2}\\setminus\\{y(\\mathbf{x}\\star;G)\\}}U_{1}(\\mathbf{x}^{\\star}(G),y;G)}\\end{array}$ . We bound term $\\begin{array}{r}{\\sum_{t\\in[T]}\\|\\mathbf{y}^{t}-\\mathbf{y}(\\mathbf{x}^{\\star};G)\\|_{1}}\\end{array}$ using the no-swap-regret property. $\\mathrm{P_{2}}$ \u2019s swap regret is at least $\\begin{array}{r}{\\sum_{t\\in[T]}\\dot{c}_{2}\\|\\mathbf{y}^{t}\\,-\\,\\mathbf{y}(\\mathbf{x}^{\\star};G)\\|_{1}}\\end{array}$ , where $\\begin{array}{r}{\\begin{array}{r c l}{c_{2}\\!}&{=}&{\\!U_{2}(\\mathbf{x}^{\\star}(G),y(\\mathbf{x}^{\\star};G))\\,-\\,\\operatorname*{max}_{y\\in\\mathcal{A}_{2}\\backslash\\{\\mathbf{y}(\\mathbf{x}^{\\star};G)\\}}U_{2}(\\mathbf{x}^{\\star}(G),y)}\\end{array}}\\end{array}$ is the minimum difference of $\\mathrm{P_{2}}$ \u2019s utility between playing the best response action $\\mathbf{y}(\\mathbf{x}^{\\star};G)$ and any other action in $\\boldsymbol{A}_{2}$ . Sub-linear swap regret therefore implies that $\\begin{array}{r}{\\mathbb{E}\\left[\\sum_{t=1}^{T}\\|\\mathbf{y}^{t}-\\mathbf{y}(x^{\\star};G)\\|_{1}\\right]\\,\\in\\,o(T)}\\end{array}$ and thus $\\begin{array}{r}{\\mathbb{E}\\left[\\sum_{t=1}^{T}U_{1}(\\mathbf{x}^{\\star}(G),\\mathbf{y}^{t})\\right]\\ge\\mathrm{StackVal}_{1}(G)\\cdot T-o(T)}\\end{array}$ , i.e., $\\mathrm{{P}_{1}}$ \u2019s expected average utility in $T$ rounds is at least Stack $\\mathrm{Val}_{1}(G)\\bar{-o}_{T}(1)$ . ", "page_idx": 6}, {"type": "text", "text": "We have shown that the pair $(\\pi_{1},\\pi_{2})$ achieves $\\mathrm{{P}_{1}}$ \u2019s benchmark utility. We now show that it is a PNE of the meta-game. Fixing $\\pi_{1}$ , the maximum utility $\\mathrm{P_{2}}$ can get is the utility achieved by playing $y(\\mathbf{x}^{\\star};G)$ , $\\forall t$ . $\\mathrm{P_{2}}$ does not necessarily know $G$ to play $y(\\mathbf{x}^{\\star};G)$ for all $t\\in[T]$ , but we have shown that due to $\\pi_{2}$ being a no-swap-regret algorithm, $\\mathrm{P_{2}}$ ends up playing strategies close to $y(\\mathbf{x}^{\\star};G)$ asymptotically. The difference between $\\mathrm{P_{2}}$ \u2019s cumulative utility between playing $\\pi_{2}$ against $\\pi_{1}$ , versus playing per-round best response against $\\pi_{1}$ is at most $\\begin{array}{r}{O(\\sum_{t\\in[T]}\\mathbb{E}\\,\\|\\mathbf{y}^{t}-\\mathbf{\\bar{y}}(\\mathbf{x}^{\\bar{\\star}};G)\\|_{1}^{\\bar{\\star}})}\\end{array}$ which is $o(T)$ by the no-swap-regret property. So $\\mathrm{P_{2}}$ has vanishing incentive to deviate from $\\pi_{2}$ in the meta-game. ", "page_idx": 6}, {"type": "text", "text": "Next fixing $\\pi_{2}$ to be a no-swap-regret algorithm, previous work [17, 28] caps $\\mathrm{{P}_{1}}$ \u2019s achievable utility through any algorithm $\\pi_{1}^{\\prime}$ (Deng et al. [17, Theorem 6]). These results show that for every $\\pi_{1}^{\\prime}$ , $\\mathrm{P_{1}}$ \u2019s expected average utility induced by $(\\pi_{1}^{\\prime},\\pi_{2})$ in $T$ rounds is at most $\\operatorname{StackVal}_{1}(G)+o_{T}(1)$ . Since we have shown that $(\\pi_{1},\\pi_{2})$ yields at least Stack $\\mathrm{Val}_{1}(G)-o_{T}(1)$ for $\\mathrm{P_{1}}$ , there is vanishing incentive for $\\mathrm{{P}_{1}}$ to deviate. ", "page_idx": 6}, {"type": "text", "text": "In Appendix B.1, we extend this proof to the scenario with potential ties in $\\mathrm{P_{2}}$ \u2019s best response, but under the assumption that $\\mathrm{P_{2}}$ has no weakly dominated action. Using regret rates of standard swap-regret algorithms, we also provide the rate of convergence to the Stackelberg benchmark. ", "page_idx": 6}, {"type": "text", "text": "Proof sketch of Theorem 3.2. To prove this theorem, we construct a family of two games $G_{1}$ and $G_{2}$ (shown in Figure 1) and let the prior distribution $\\mathcal{D}$ to be uniform over $G_{1}$ and $G_{2}$ . Note that the maximum value of game parameters depends inversely on $\\begin{array}{r}{\\gamma\\triangleq\\frac{1-p^{\\star}}{1+p^{\\star}}}\\end{array}$ , where $p^{\\star}$ is the maximum precision of the signal received by $\\mathrm{P_{2}}$ . In this construction, the utility functions in both games are identical for $\\mathrm{P_{2}}$ but different for $\\mathrm{P_{1}}$ . This implies that $\\mathrm{P_{2}}$ cannot gain any additional knowledge about which game is realized from looking at her own utility function. ", "page_idx": 6}, {"type": "text", "text": "We first illustrate the high-level idea by considering a hypothetical situation where the trajectory always converges to the Stackelberg equilibrium led by $\\mathrm{P_{2}}$ for all $G$ . In other words, the trajectory converges to $(\\bar{x^{}}(\\mathbf{y}^{\\star};G_{1}),\\mathbf{y}^{\\star}(G_{1}))$ when $G_{1}$ is realized and $(x(\\mathbf{y}^{\\star};G_{2}),\\mathbf{y}^{\\star}(G_{2}))$ when $G_{2}$ is realized. It is not hard to check that the Stackelberg equilibria turns out to be supported on different purestrategy pairs: $(A,C)$ in $G_{1}$ and $(B,D)$ in $G_{2}$ (shaded cells in Figure 1). Because the Stackelberg strategies differ for $G_{1}$ and $G_{2}$ , to converge to the correct equilibrium, $\\mathrm{P_{2}}$ must have gained full information about which game $G$ is being played through repeated interactions with $\\mathrm{P_{1}}$ . However, from $\\mathrm{P_{1}}$ \u2019s perspective, the strategy pair $(A,C)$ \u2014the Stackelberg equilibrium led by $\\mathrm{P_{2}}$ in $G_{1}$ \u2014is more favorable than the other equilibrium $(B,D)$ in both $G_{1}$ and $G_{2}$ . Therefore, instead of disclosing information about which $G$ is realized, it would be more beneficial for $\\mathrm{P_{1}}$ to conceal this information and always behave as if $G$ were $G_{1}$ . Therefore, any pair of algorithms that give rise to this hypothetical situation cannot be an equilibrium in the space of algorithms. ", "page_idx": 6}, {"type": "image", "img_path": "Dlm6Z1RrjV/tmp/239b999baa31aef60046d9b01553217dbfca98d5c1c70d446d07917c4a5f7673.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "", "img_caption": ["Figure 1: game matrices $G_{1}$ and $G_{2}$ . $\\mathrm{P_{1}}$ is the row player and $\\mathrm{P_{2}}$ is the column player. The values in each cell are ( $\\mathrm{{P}_{1}}$ \u2019s utility, $\\mathrm{P_{2}}$ \u2019s utility). Shaded cells represent the action profiles supported in the Stackelberg equilibria led by the column player. The parameter $\\gamma$ is defined as $\\begin{array}{r}{\\frac{1-p^{\\star}}{1+p^{\\star}}\\in(0,1]}\\end{array}$ , where $p^{\\star}$ is the precision threshold of $p_{2}$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Our actual proof applies similar ideas to establish a stronger claim: not only is it impossible for $\\mathrm{P_{2}}$ to have the trajectory always converge to their Stackelberg equilibrium, but they cannot recover an average utility of $\\mathrm{StackVal}_{2}(\\mathcal{D})$ through any repeated interactions with $\\mathrm{{P}_{1}}$ that are specified by PNE algorithm pairs. To argue this, we will use the notion of correlated strategy profiles (CSP) [3] as a succinct way of analyzing the expected utility of each player. For a distribution $\\mathcal{T}^{T}$ over trajectories of length $T$ , the CSP induced by $\\mathcal{T}^{T}$ , denoted as $\\mathsf{C S P}_{\\mathcal{T}^{T}}$ , is a correlated distribution in $\\Delta(A_{1}\\times A_{2})$ which is taken as the empirical average of the mixed-strategy proflies in each time step, i.e., $\\begin{array}{r}{\\mathsf{C S P}_{\\mathcal T^{T}}\\triangleq\\mathbb E_{(\\mathbf x_{t},\\mathbf y_{t})_{t\\in[T]}\\sim\\mathcal T^{T}}\\big[(1/T)\\sum_{t\\in[T]}\\mathbf x_{t}\\otimes\\mathbf y_{t}\\big]}\\end{array}$ . Since CSPs serve as a sufficient statistics of both players\u2019 expected utility (which is a direct consequence of the linearity of utilities), working with them significantly reduces the dimension of the problem. ", "page_idx": 7}, {"type": "text", "text": "Special case: full information asymmetry. We start with the full information asymmetry setting, i.e., $p_{1}=1$ and $p_{2}=0$ . For the sake of contradiction, assume that a pair of equilibrium algorithms $(\\pi_{1},\\pi_{2})$ can let $\\mathrm{P_{2}}$ achieve the benchmark $\\mathrm{StackVal_{2}}({\\mathcal{D}})=3/2$ . With the CSPs introduced above, we can rewrite $\\mathrm{P_{2}}$ \u2019s average expected utility as $\\begin{array}{r}{\\frac{1}{2}\\operatorname{\\mathbb{E}}_{(x,y)\\sim\\!\\!\\sum\\!\\!\\operatorname{\\mathsf{CSP}}_{1}}U_{2}\\big(x,y;G_{1}\\big)\\!+\\!\\frac{1}{2}\\operatorname{\\mathbb{E}}_{(\\underline{{x}},y)\\sim\\mathsf{C S P}_{2}}U_{2}\\big(x,y;G_{2}\\big)}\\end{array}$ , where we have used $\\mathsf{C S P}_{1}$ and $\\mathsf{C S P_{2}}$ to denote the CSPs induced by the distribution over trajectories generated by $\\mathcal{T}^{T}(\\pi_{1},\\pi_{2};G_{1})$ and $\\bar{\\mathcal{T}}^{T}(\\pi_{1},\\pi_{2};G_{2})$ , respectively. ", "page_idx": 7}, {"type": "text", "text": "Similar to the hypothetical situation sketched above, we want to argue that there is incentive for $\\mathrm{P_{1}}$ to deviate to an algorithm $\\pi_{1}^{\\prime}$ that always behaves according to $\\pi_{1}(G_{1})$ even when the actual game is $G_{2}$ . In other words, we aim to show that $\\mathrm{{P}_{1}}$ \u2019s expected utility in $G_{2}$ strictly increases after replacing the induced CSP from $\\mathsf{C S P}_{2}$ to $\\mathsf{C S P}_{1}$ , i.e., $\\mathbb{E}_{\\tau\\sim\\tilde{\\mathsf{C S P}}_{1}}\\,U_{1}(\\tau;\\dot{G}_{2})>\\mathbb{E}_{\\tau\\sim\\tilde{\\mathsf{C S P}}_{2}}\\,U_{1}(\\tau;G_{2})$ . Note that for $\\mathrm{P_{1}}$ \u2019s utility in $G_{2}$ , cells involving action $C$ all have utility close to 1, whereas those involving action $D$ all have utility close to 0. Therefore, it suffices to show that cells involving action $D$ take up a significant probability mass in $\\mathsf{C S P_{2}}$ but very little in $\\mathsf{C S P}_{1}$ . We break these into the following three claims and use the equilibrium condition to establish them in Appendix B.3. ", "page_idx": 7}, {"type": "text", "text": "\u2022 Claim 1. $\\mathsf{C S P}_{1}(B,D)$ is very small, otherwise $\\mathrm{P_{1}}$ would deviate to always playing action $A$ .   \n\u2022 Claim 2. $\\mathsf{C S P}_{1}(A,D)$ is very small, otherwise $\\mathrm{P_{2}}$ would deviate to always playing action $C$ .   \n\u2022 Claim 3. $\\mathsf{C S P}_{2}(B,D)$ is very large, otherwise $\\mathrm{P_{2}}$ cannot achieve benchmark $\\mathrm{StackVal}_{2}(\\mathcal{D})$ . ", "page_idx": 7}, {"type": "text", "text": "Towards partial information asymmetry. In the remainder of this sketch, we discuss the extension of the above approach to the partial asymmetry setting where $p_{1}=1$ and $0\\leq p_{2}\\leq p^{\\star}<1$ . The fact that $\\mathrm{P_{2}}$ \u2019s signal is partially informative introduces extra challenge to our analysis, since $\\mathrm{P_{2}}$ \u2019s belief about the true game depends not only on $\\mathrm{{P}_{1}}$ \u2019s behavior during the interaction, but also on the information carried by the external signal $s_{2}$ . As a result, if $\\mathrm{P_{1}}$ deviates to acting according to $G_{1}$ when the actual game is $G_{2}$ , it does not trigger the expected CSP when the realized game is $G_{1}$ , but instead causes a \u201cdistorted\u201d posterior since the distribution of $s_{2}\\sim\\varphi_{p_{2}}(\\cdot|G_{2})$ does not change. ", "page_idx": 7}, {"type": "text", "text": "To illustrate this, consider the four different CSPs introduced by all combinations of the realized signals received by both players. For $(i,j)\\,\\in\\,\\{1,2\\}^{2}$ , let ${\\mathsf{C S P}}_{i j}$ to denote the CSP induced by $\\bar{\\cal T}^{\\bar{T}}(\\pi_{1},\\pi_{2};s_{1},s_{2},\\bar{G}=s_{1}\\bar{)}$ when $s_{1}=G_{i}$ and $s_{2}=G_{j}$ (we have set $G=s_{1}$ because $s_{1}$ perfectly reveals $G$ ). When the realized game is $G_{2}$ , $\\mathrm{P_{1}}$ \u2019s expected utility before deviation is given by ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\bar{U}_{1}(\\pi_{1},\\pi_{2};G_{2})=\\frac{1-p_{2}}{2}\\operatorname*{\\mathbb{E}}_{\\tau\\sim\\mathsf{C S P}_{21}}U_{2}(\\tau;G_{2})+\\frac{1+p_{2}}{2}\\operatorname*{\\mathbb{E}}_{\\tau\\sim\\mathsf{C S P}_{22}}U_{2}(\\tau;G_{2}),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "because the probability of $\\mathrm{P_{2}}$ seeing signals $s_{2}=G_{1}$ and $s_{2}=G_{2}$ are $\\scriptstyle{\\frac{1-p_{2}}{2}}$ and $\\scriptstyle{\\frac{1+p_{2}}{2}}$ , respectively. So, as for the expected utility after deviation, the coefficients 1\u2212p2and 1 $\\scriptstyle{\\frac{1+p_{2}}{2}}$ remain the same, but the first distribution $\\mathsf{C S P_{21}}$ becomes ${\\mathsf{C S P}}_{22}$ and the second distribution changes from ${\\mathsf{C S P}}_{22}$ to ${\\mathsf{C S P}}_{12}$ : ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\bar{U}_{1}(\\pi_{1}^{\\prime},\\pi_{2};G_{2})=\\frac{1-p_{2}}{2}\\underbrace{\\mathbb{E}}_{\\tau\\sim\\mathsf{C S P}_{11}}U_{2}(\\tau;G_{2})+\\frac{1+p_{2}}{2}\\underbrace{\\mathbb{E}}_{\\tau\\sim\\mathsf{C S P}_{12}}U_{2}(\\tau;G_{2}),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "However, if the true game were $G_{1}$ , then ${\\mathsf{C S P}}_{11}$ and $\\mathsf{C S P}_{12}$ would be realized with swapped probability 1+2p2 and $\\scriptstyle{\\frac{1-p_{2}}{2}}$ , not the ones appeared in $\\bar{U}(\\pi_{1}^{\\prime},\\pi_{2};G_{2})!$ ! Hence, even if we can guarantee that action pairs $(A,D)^{\\circ}$ and $(B,D)$ occur very infrequently when the true game is $G=G_{1}$ , they may only occur under CSP12, whose frequency gets amplified by 11\u2212+pp22 times when factoring into the utility after deviation $\\bar{U}(\\pi_{1}^{\\prime},\\pi_{2};G_{2})$ . Therefore, establishing the benefti of deviation requires a much smaller probability of $(A,D)$ and $(B,D)$ under the CSPs induced by $G=G_{1}$ . This is why we need the game parameters to inversely depend on $\\begin{array}{r}{\\gamma=\\frac{1-p^{\\star}}{1+p^{\\star}}}\\end{array}$ , where $p^{\\star}$ is an upper bound on $p_{2}$ . \u53e3 ", "page_idx": 8}, {"type": "text", "text": "3.2 Difference in learning through repeated interactions and learning independently ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we provide an informal discussion on the reason behind $\\mathrm{P_{2}}$ \u2019s failure to recover their Stackelberg value benchmark through repeated interactions, with a more formal treatment deferred to Appendix A. We argue that the failure is not due to the PNE of meta-game always preventing $\\mathrm{P_{2}}$ from \u201clearning\u201d the game, but rather because $\\mathrm{P_{2}}$ cannot apply her learned knowledge to recover her Stackelberg value in any equilibrium. ", "page_idx": 8}, {"type": "text", "text": "At each round, $\\mathrm{P_{2}}$ can form a posterior belief about the realized game $G$ based on her observed feedback from the historical interactions and the initial signal $s_{2}$ received. We say that $\\mathrm{P_{2}}$ successfully learns $G$ if her posterior belief converges to the point distribution on $G$ (formally in Definition A.2). Interestingly, using the same pair of PNE algorithms designed to show that $\\mathrm{P_{1}}$ can achieve their Stackelberg value, we can show that $\\mathrm{P_{2}}$ is indeed able to successfully learn $G$ through strategic interactions. This is because $\\mathrm{P_{2}}$ \u2019s strategy converges to the best response $y(\\mathbf{x}^{\\star};G)$ , which perfectly reveals $G$ for some game families $\\mathcal{G}$ . Thus, successful learning of $G$ through repeated interactions can happen in a PNE of a meta-game where Sta $\\operatorname{ckVal}_{2}(\\mathcal{D})$ cannot be achieved (Observation A.4). ", "page_idx": 8}, {"type": "text", "text": "The problem preventing $\\mathrm{P_{2}}$ from achieving $\\mathrm{StackVal}_{2}(\\mathcal{D})$ is not an insufficient rate or accuracy of learning, but rather the fact that learning and acting on this learned knowledge are intertwined. In fact, if $\\mathrm{P_{2}}$ \u2019s learning was independent of the repeated interaction, i.e., when $\\mathrm{P_{2}}$ has access to external signals that become more accurate over time, she can achieve $\\mathrm{StackVal}_{2}(\\mathcal{D})$ (Proposition A.5). ", "page_idx": 8}, {"type": "text", "text": "4 Interactions between two partially-informed players ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we consider the setting where neither player is fully informed. That is, the precision of both player\u2019s signals $(p_{1},p_{2})$ are less than one. Even though there may be information asymmetry in the form of different precision levels of player signals, we show that there is no longer a clear separation between players through the average Stackelberg value benchmark. ", "page_idx": 8}, {"type": "text", "text": "At a high level, what distinguishes this setting from the previous setting (hence resulting in the lack of separation), is that the identity of the more-informed player can shift throughout the course of the repeated interaction. Due to the structure of $\\mathcal{D}$ , more information about the realized game may be released to one player compared to the other. In contrast, when the more informed player starts with perfectly knowing the realized game, there is no possibility of her becoming less informed since there is no information beyond what she already knows. ", "page_idx": 8}, {"type": "text", "text": "Example 4.1. Consider $\\mathcal{D}$ to be the uniform distribution over the two game matrices defined in the figure below. ", "page_idx": 8}, {"type": "table", "img_path": "Dlm6Z1RrjV/tmp/a2d6f0433595843a11c30bab0f76937116862af19dabc533849f9fced0f48bf2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "Dlm6Z1RrjV/tmp/598e7ae58cf99cb0fbf5333ad6547268c63243c0938095cf92744c372392c2dc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 2: Example game matrices $\\mathcal{G}=\\{G_{1},G_{2}\\}$ revealing more information to $\\mathrm{P_{2}}$ compared to $\\mathrm{P_{1}}$ . Here, $\\mathrm{P_{1}}$ is the row player and $\\mathrm{P_{2}}$ is the column player. ", "page_idx": 8}, {"type": "text", "text": "Note that $i f\\mathrm{P_{2}}$ chooses the pure strategy $C$ , then for any strategy of $\\mathrm{P_{1}}$ , $\\mathrm{P_{2}}$ \u2019s utility lies in the range [1, 2] if $G_{1}$ is realized and in the range [3, 7] $i f G_{2}$ is realized. Since both ranges are non-intersecting, $\\mathrm{P_{2}}$ can deduce $G$ exactly after a single round by choosing the pure strategy $B$ in the first round. ", "page_idx": 9}, {"type": "text", "text": "However, since the utilities of $\\mathrm{{P}_{1}}$ for all action profiles are the same in both $G_{1},G_{2}$ , $\\mathrm{P_{1}}$ gains no additional information about the realized game. So even $i f\\mathrm{P_{1}}$ started off with a more informative signal, after a single round, $\\mathrm{P_{2}}$ becomes more informed and in fact perfectly informed. ", "page_idx": 9}, {"type": "text", "text": "To show that the average Stackelberg value benchmark does not separate the more- from the lessinformed player, we will show that neither player can achieve the average Stackelberg value in all instances. Put another way, for every possible pair of player signals\u2019 precision, there is an instance such that this player cannot achieve the benchmark value at equilibrium. ", "page_idx": 9}, {"type": "text", "text": "Proposition 4.2. For every player signal precision values $p_{1},p_{2}\\in[0,1)$ , for each $i\\in\\{1,2\\}$ , there exists $\\mathcal{D}$ such that for every PNE $(\\pi_{1},\\pi_{2})$ of the meta-game, $\\bar{U}_{i}^{T}(\\pi_{1},\\pi_{2};D)\\leq S t a c k V a l_{i}(\\mathcal{D})-\\Omega_{T}(1)$ . ", "page_idx": 9}, {"type": "text", "text": "Proof. The proof of this proposition reduces to the proof of Theorem 3.2. This is due to the following game-revealing property (similar to Example 4.1) of the construction $\\mathcal{D}$ used in the proof of Theorem 3.2 described by Figure 1. Since the range of the set of attainable utilities in $G_{1},G_{2}$ when $\\mathrm{P_{1}}$ chooses action $A$ , has no intersection for $\\mathrm{{P}_{1}}$ , regardless of $\\mathrm{P_{2}}$ \u2019s strategy, $\\mathrm{P_{1}}$ can deduce the game exactly after a single round while $\\mathrm{P_{2}}$ gains no additional information after a single round. ", "page_idx": 9}, {"type": "text", "text": "After the first round, we are in the regime of a fully informed $\\mathrm{P_{1}}$ and a partially informed $\\mathrm{P_{2}}$ since $p_{2}<1$ . Theorem 3.2 already shows that in this regime, no equilibrium provides $\\mathrm{P_{2}}$ her average Stackelberg value benchmark. Using a distribution $\\mathcal{D}^{\\prime}$ that is the same as $\\mathcal{D}$ but with player utilities flipped proves the proposition for $\\mathrm{P_{2}}$ . \u53e3 ", "page_idx": 9}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we study the effects of information asymmetry (codified in terms of signals about the game played) on the achievable benchmarks of two non-myopic players interacting repeatedly over $T$ rounds. First, we showed that when $\\mathrm{P_{1}}$ is fully informed (i.e., knows $G$ ) while $\\mathrm{P_{2}}$ is not, then there is a separation between the more and the less informed player by way of each player\u2019s achievable benchmarks. Next, we showed that when neither player is fully informed (i.e., both $p_{1},p_{2}<1)$ ) then, there is no longer a clear separation between players in terms of benchmarks. ", "page_idx": 9}, {"type": "text", "text": "There are several avenues for future research stemming from our work. ", "page_idx": 9}, {"type": "text", "text": "Characterizations of algorithms that can be supported in an equilibrium. We should gain a better understanding of what algorithms from natural classes can be in equilibrium. A useful step would be to characterize the necessary and/or sufficient conditions for an algorithm to be supported in the PNE of the meta-game. Our work and previous work provide sufficient conditions such as no-swap-regret algorithms and best-responding per round: an algorithm satisfying either condition can be supported in a PNE of every meta-game if at least one player is fully informed. Previous work also implies that no-regret is not sufficient for an algorithm to be part of a meta-game PNE (see Appendix C for more details). Finally, in the case where neither player is fully informed, it would be very useful to characterize the structure of the meta-game that causes a shift wrt the information advantage. ", "page_idx": 9}, {"type": "text", "text": "Other models of how signals are generated. Alleviating some of our modeling assumptions, one could ask how the results would change if nature was not assumed to be truthful with respect to the signal reporting but it may strategically modify the signals to achieve its own goals, such as maximizing social welfare. Taking this aspect into account, we can consider an information design setting where the nature designs a signaling scheme that shapes both agents\u2019 beliefs about the state and therefore their algorithms of choice. In addition, we have assumed that nature provides signals cost-free. This is a required and natural first step, but an interesting direction would be to understand what happens when the signals are costly and their accuracy is positively correlated with their cost. ", "page_idx": 9}, {"type": "text", "text": "Computational aspects of meta-game equilibrium. Moreover, it would be very interesting to see how the results about the effects of information asymmetry generalize in the case where the players are computationally bounded; note that our current setup provides information-theoretic results, but it could be computationally hard for players to communicate their algorithms to each other, or even verify that two algorithms are at equilibrium. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Jacob D Abernethy, Rachel Cummings, Bhuvesh Kumar, Sam Taggart, and Jamie H Morgenstern. Learning auctions with robust incentive guarantees. Advances in Neural Information Processing Systems, 32, 2019.   \n[2] Kareem Amin, Afshin Rostamizadeh, and Umar Syed. Learning prices for repeated auctions with strategic buyers. In Advances in Neural Information Processing Systems (NeurIPS), volume 26, 2013.   \n[3] Eshwar Ram Arunachaleswaran, Natalie Collina, and Jon Schneider. Pareto-optimal algorithms for learning in games. arXiv preprint arXiv:2402.09549, 2024.   \n[4] Robert J Aumann, Michael Maschler, and Richard E Stearns. Repeated games with incomplete information. MIT press, 1995.   \n[5] Maria-Florina Balcan, Avrim Blum, Nika Haghtalab, and Ariel D Procaccia. Commitment without regrets: Online learning in Stackelberg security games. In Proceedings of the 16th ACM Conference on Economics and Computation (EC), pages 61\u201378, 2015.   \n[6] Georgios Birmpas, Jiarui Gan, Alexandros Hollender, Francisco Marmolejo, Ninad Rajgopal, and Alexandros Voudouris. Optimally deceiving a learning leader in stackelberg games. In Advances in Neural Information Processing Systems (NeurIPS), volume 33, pages 20624\u201320635, 2020.   \n[7] Avrim Blum and Yishay Mansour. From external to internal regret. Journal of Machine Learning Research, 8(6), 2007.   \n[8] Avrim Blum, Nika Haghtalab, and Ariel Procaccia. Learning optimal commitment to overcome insecurity. In Advances in Neural Information Processing Systems (NeurIPS), volume 27, pages 1826\u20131834, 2014.   \n[9] Mark Braverman, Jieming Mao, Jon Schneider, and Matt Weinberg. Selling to a no-regret buyer. In Proceedings of the 19th ACM Conference on Economics and Computation (EC), pages 523\u2013538, 2018.   \n[10] William Brown, Jon Schneider, and Kiran Vodrahalli. Is learning in games good for the learners? Advances in Neural Information Processing Systems (NeurIPS), 36, 2024.   \n[11] Modibo K Camara, Jason D Hartline, and Aleck Johnsen. Mechanisms for a no-regret agent: Beyond the common prior. In Proceedings of the 61st Annual Symposium on Foundations of Computer Science (FOCS), pages 259\u2013270. IEEE, 2020.   \n[12] Yurong Chen, Xiaotie Deng, and Yuhao Li. Optimal private payoff manipulation against commitment in extensive-form games. arXiv preprint arXiv:2206.13119, 2022.   \n[13] Yurong Chen, Xiaotie Deng, Jiarui Gan, and Yuhao Li. Learning to manipulate a commitment optimizer. arXiv preprint arXiv:2302.11829, 2023.   \n[14] Natalie Collina, Eshwar Ram Arunachaleswaran, and Michael Kearns. Efficient stackelberg strategies for finitely repeated games. In Proceedings of the 2023 International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), AAMAS \u201923, page 643\u2013651, 2023.   \n[15] Natalie Collina, Aaron Roth, and Han Shao. Efficient prior-free mechanisms for no-regret agents. arXiv preprint arXiv:2311.07754, 2023.   \n[16] Yuval Dagan, Constantinos Daskalakis, Maxwell Fishelson, and Noah Golowich. From external to swap regret 2.0: An efficient reduction for large action spaces. In Proceedings of the 56th Annual ACM Symposium on Theory of Computing (STOC), pages 1216\u20131222, 2024.   \n[17] Yuan Deng, Jon Schneider, and Balasubramanian Sivan. Strategizing against no-regret learners. In Advances in Neural Information Processing Systems (NeurIPS), volume 32, pages 1579\u20131587, 2019.   \n[18] Kate Donahue, Nicole Immorlica, Meena Jagadeesan, Brendan Lucier, and Aleksandrs Slivkins. Impact of decentralized learning on player utilities in stackelberg games. arXiv preprint arXiv:2403.00188, 2024.   \n[19] Fei Fang, Thanh Nguyen, Benjamin Ford, Nicole Sintov, and Milind Tambe. Introduction to green security games. In International Joint Conference on Artificial Intelligence (IJCAI), 2015.   \n[20] Julius Farkas. Theorie der einfachen ungleichungen. Journal f\u00fcr die reine und angewandte Mathematik (Crelles Journal), 1902(124):1\u201327, 1902.   \n[21] Tanner Fiez, Benjamin Chasnov, and Lillian J Ratliff. Convergence of learning dynamics in stackelberg games. arXiv preprint arXiv:1906.01217, 2019.   \n[22] Tanner Fiez, Benjamin Chasnov, and Lillian Ratliff. Implicit learning dynamics in Stackelberg games: Equilibria characterization, convergence analysis, and empirical study. In International Conference on Machine Learning (ICML), pages 3133\u20133144. PMLR, 2020.   \n[23] Jiarui Gan, Qingyu Guo, Long Tran-Thanh, Bo An, and Michael Wooldridge. Manipulating a learning defender and ways to counteract. In Advances in Neural Information Processing Systems (NeurIPS), volume 32, 2019.   \n[24] Jiarui Gan, Haifeng Xu, Qingyu Guo, Long Tran-Thanh, Zinovi Rabinovich, and Michael Wooldridge. Imitative follower deception in stackelberg games. In Proceedings of the 20th ACM Conference on Economics and Computation (EC), pages 639\u2013657, 2019.   \n[25] Denizalp Goktas, Jiayi Zhao, and Amy Greenwald. Robust no-regret learning in min-max stackelberg games. In Proceedings of the 2022 International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), pages 543\u2013552, 2022.   \n[26] Guru Guruganesh, Yoav Kolumbus, Jon Schneider, Inbal Talgam-Cohen, Emmanouil-Vasileios Vlatakis-Gkaragkounis, Joshua R Wang, and S Matthew Weinberg. Contracting with a learning agent. arXiv preprint arXiv:2401.16198, 2024.   \n[27] Nika Haghtalab, Thodoris Lykouris, Sloan Nietert, and Alexander Wei. Learning in stackelberg games with non-myopic agents. In Proceedings of the 23rd ACM Conference on Economics and Computation, pages 917\u2013918, 2022.   \n[28] Nika Haghtalab, Chara Podimata, and Kunhe Yang. Calibrated stackelberg games: Learning optimal commitments against calibrated agents. In Advances in Neural Information Processing Systems (NeurIPS), volume 36, 2024.   \n[29] Mohammad Hajiaghayi, Mohammad Mahdavi, Keivan Rezaei, and Suho Shin. Regret analysis of repeated delegated choice. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), volume 38, pages 9757\u20139764, 2024.   \n[30] Yoav Kolumbus and Noam Nisan. How and why to manipulate your own agent: On the incentives of users of learning agents. Advances in Neural Information Processing Systems, 35: 28080\u201328094, 2022.   \n[31] Joshua Letchford, Vincent Conitzer, and Kamesh Munagala. Learning and approximating the optimal strategy to commit to. In Algorithmic Game Theory, pages 250\u2013262. Springer, 2009.   \n[32] Yishay Mansour, Mehryar Mohri, Jon Schneider, and Balasubramanian Sivan. Strategizing against learners in Bayesian games. In Conference on Learning Theory (COLT), pages 5221\u2013 5252. PMLR, 2022.   \n[33] Thanh Nguyen and Haifeng Xu. Imitative attacker deception in stackelberg security games. In International Joint Conference on Artificial Intelligence (IJCAI), pages 528\u2013534, 2019.   \n[34] Binghui Peng and Aviad Rubinstein. Fast swap regret minimization and applications to approximate correlated equilibria. In Proceedings of the 56th Annual ACM Symposium on Theory of Computing (STOC), pages 1223\u20131234, 2024.   \n[35] Aaron Roth, Jonathan Ullman, and Zhiwei Steven Wu. Watch and learn: Optimizing from revealed preferences feedback. In Proceedings of the 48th Annual ACM Symposium on Theory of Computing (STOC), pages 949\u2013962, 2016.   \n[36] Aviad Rubinstein and Junyao Zhao. Strategizing against no-regret learners in first-price auctions. arXiv preprint arXiv:2402.08637, 2024.   \n[37] Gilles Stoltz and G\u00e1bor Lugosi. Internal regret in on-line portfolio selection. Machine Learning, 59:125\u2013159, 2005.   \n[38] Geng Zhao, Banghua Zhu, Jiantao Jiao, and Michael Jordan. Online learning in stackelberg games with an omniscient follower. In International Conference on Machine Learning, pages 42304\u201342316. PMLR, 2023.   \n[39] Tijana Zrnic, Eric Mazumdar, Shankar Sastry, and Michael Jordan. Who leads and who follows in strategic classification? In Advances in Neural Information Processing Systems (NeurIPS), volume 34, pages 15257\u201315269, 2021.   \n[40] Song Zuo and Pingzhong Tang. Optimal machine strategies to commit to in two-person repeated games. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), volume 29, 2015. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Difference in learning through repeated interactions and learning independently ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this part, we will use Theorem 3.2 to show that there is a difference between learning through repeated interactions and learning independently based on external signals. We will argue that learning independently is more powerful as it allows achieving the average Stackelberg benchmark, whereas learning based on repeated interactions does not always allow this. ", "page_idx": 13}, {"type": "text", "text": "Our approach to doing this is to introduce two models of learning: one based on histories generated by the repeated interaction and the other based on external signals. Fixing the same success criterion for both models (to be defined soon), we will show that ", "page_idx": 13}, {"type": "text", "text": "1. Successful learning by $\\mathrm{P_{2}}$ based on repeated interactions is possible at a PNE of the metagame, but still does not yield $\\mathrm{P_{2}}$ her average Stackelberg value (Observation A.4). 2. Successful learning by $\\mathrm{P_{2}}$ based on external signals makes $\\mathrm{P_{2}}$ \u2019s average Stackelberg value achievable at a PNE of the meta-game, for all $\\mathcal{D}$ (Proposition A.5). ", "page_idx": 13}, {"type": "text", "text": "We first define both learning models and the success criterion of learning. Later in this section, we state results demonstrating the separation between the two learning models. ", "page_idx": 13}, {"type": "text", "text": "Successfully learning a realized game $G$ entails forming beliefs over the realized game such that the beliefs asymptotically concentrate on the true realized game $G$ . ", "page_idx": 13}, {"type": "text", "text": "Definition A.1 (Successful learning criterion). Given a realized game $G$ from a game family $\\mathcal{G}$ , a belief sequence $(\\hat{\\beta}^{t})_{t=1}^{\\infty}$ , where $\\bar{\\hat{\\beta}}^{t}\\,\\in\\,\\Delta(\\mathcal{G})$ is a belief (distribution) over $\\mathcal{G}$ for each $t\\,\\in\\,\\mathbb{N}$ successfully learns $G$ if $\\operatorname*{Pr}_{\\tilde{G}^{T}\\sim\\hat{\\beta}^{T}}[\\tilde{G}^{T}\\,\\neq\\,G]\\,\\in\\,o_{T}(1)$ . That is, the beliefs asymptotically fully concentrate on the realized game $G$ . ", "page_idx": 13}, {"type": "text", "text": "Different models of learning involve different constraints on or power afforded to how beliefs of the realized game are generated. The first model of interest is learning based on repeated interactions. Here, the belief at a round $t$ is constrained to be formed based on the initial signal and the history at round $t$ generated during the repeated interaction. ", "page_idx": 13}, {"type": "text", "text": "Definition A.2 (Successful learning based on repeated interactions). Given a family of games $\\mathcal{G}$ and a player $\\mathrm{P}_{i}$ $(i\\in\\{1,2\\})$ ), $a$ history-based belief function for $\\mathrm{P}_{i}$ is a mapping from the player\u2019s signal value and a history of repeated interactions to a belief distribution supported on $\\mathcal{G}$ . Formally, we denote the belief function with $h:(s_{i};H_{i}^{1:t-1})\\mapsto\\Delta(\\mathring{\\mathcal{G}})$ . ", "page_idx": 13}, {"type": "text", "text": "Given a distribution $\\mathcal{D}\\in\\Delta(\\mathcal{G})$ , we say a history-based belief function $h$ , as defined above, successfully learns based on interactions through the algorithm pair $(\\pi_{1},\\pi_{2})$ if for every realized game $G\\in{\\mathcal{G}}$ and every realized signal $s_{1},s_{2}$ , the induced beliefs $\\left(\\hat{\\beta}_{i}^{t}\\right)_{t=1}^{\\infty}$ where each belief $\\hat{\\beta}_{i}^{t}=h(s_{i},H_{i}^{1:t-1})$ is induced by histories $H_{i}^{r}$ generated from the distribution $\\bar{T}^{\\bar{T}}(\\pi_{1},\\pi_{2};G,s_{1},s_{2})$ , successfully learns the realized game $G$ (according to Definition $A.l$ ) with probability 1. ", "page_idx": 13}, {"type": "text", "text": "The second form of learning occurs without dependence on the other player\u2019s actions and instead based on externally provided signals. ", "page_idx": 13}, {"type": "text", "text": "Definition A.3 (Successful independent learning). Given a distribution of games $\\mathcal{D}$ , we say that $\\mathrm{P}_{i}$ can independently learn successfully if for every realized game $G\\sim\\mathcal{D}$ , there is a sequence of signals $(q^{t})_{t=1}^{\\infty}$ with $q^{t}\\in\\mathcal G$ , that are generated by a sequence of signaling distributions $(Q^{t})_{t=1}^{\\infty}$ with $Q^{t}\\in\\Delta(\\mathcal{G})$ , where the sequence $(\\bar{Q}^{t})_{t=1}^{\\infty}$ successfully learns the realized game $G$ (according to Definition A.1). ", "page_idx": 13}, {"type": "text", "text": "The signal $q^{t}$ is provided to $\\mathrm{P}_{i}$ at round $t$ . So $\\mathrm{P}_{i}$ \u2019s algorithm $\\pi_{i}$ maps the initial signal $s_{i}$ , history at each round $t$ $(\\mathring{H_{i}^{1:t-1}})$ , and $q^{t}$ to a strategy taken at round $t$ . ", "page_idx": 13}, {"type": "text", "text": "Revisiting Theorem 3.2, which states that the average Stackelberg value is not achievable by $\\mathrm{P_{2}}$ for some $\\mathcal{D}$ , we can question whether some property of $\\mathcal{D}$ and the equilibria of the meta-game prevents $\\mathrm{P_{2}}$ from successfully learning or if $\\mathrm{P_{2}}$ can successfully learn but cannot use this learning to achieve her Stackelberg value. We assert that it is the latter. ", "page_idx": 13}, {"type": "text", "text": "Observation A.4. There is a game distribution $\\mathcal{D}$ , such that there exists a PNE $(\\pi_{1},\\pi_{2})$ of the meta-game that allows $\\mathrm{P_{2}}$ to successfully learn based on repeated interactions (in the sense of ", "page_idx": 13}, {"type": "text", "text": "Definition A.2), but no equilibrium allows $\\mathrm{P_{2}}$ to achieve her average Stackelberg value benchmark $S t a c k V a l_{2}(\\mathcal{D})$ . ", "page_idx": 14}, {"type": "image", "img_path": "Dlm6Z1RrjV/tmp/daeba0522063bc900ad398f260548e111e5efa6cd774d3288297993f59020cd3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "", "table_caption": ["Game Matrix $G_{2}^{\\prime}$ "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 3: game matrices $G_{1}^{\\prime}$ and $G_{2}^{\\prime}$ . $\\mathrm{P_{1}}$ is the row player and $\\mathrm{P_{2}}$ is the column player. The values in each cell are ( $\\mathrm{{P}_{1}}$ \u2019s utility, $\\mathrm{P_{2}}$ \u2019s utility). The parameter $\\gamma$ is defined as $\\textstyle{\\frac{1-p^{\\star}}{1+p^{\\star}}}\\in(0,1]$ , where $p^{\\star}$ is the precision threshold of p2. ", "page_idx": 14}, {"type": "text", "text": "Proof. Consider the construction of $\\mathcal{D}$ used in Theorem 3.2 (Figure 1). $\\mathcal{D}$ is a distribution with equal probabilities over two game matrices $G_{1},G_{2}$ . Theorem 3.2 proves that the average Stackelberg benchmark is unattainable by $\\mathrm{P_{2}}$ in any equilibrium. ", "page_idx": 14}, {"type": "text", "text": "For this proof, we will consider another construction $\\mathcal{D}^{\\prime}$ that is equal probability over game matrices $G_{1}^{\\prime},G_{2}^{\\prime}$ defined in Figure 3. $G_{1}^{\\prime}$ is essentially $G_{1}$ with an additional row and column, where the additional row is essentially a duplicate of the first row and the additional column is essentially a duplicate of the first column. Rather than being an exact duplicate, the new row/column is a small perturbation (given by parameter $\\epsilon$ ) of the original row/column it duplicates. $G_{2}^{\\prime}$ is obtained from $G_{2}$ similarly. ", "page_idx": 14}, {"type": "text", "text": "Since the new construction is essentially duplicating rows/columns of the old construction, by the same argument as is Theorem 3.2, the average Stackelberg benchmark is also unattainable by $\\mathrm{P_{2}}$ in any equilibrium of the meta-game of the Bayesian game $\\mathcal{D}^{\\prime}$ . ", "page_idx": 14}, {"type": "text", "text": "Despite this, we can show there is an equilibrium enabling $\\mathrm{P_{2}}$ to successfully learn through repeated interactions. We construct the perturbations to ensure that the $\\mathrm{{P}_{1}}$ -led equilibrium response has a different response for $\\mathrm{P_{2}}$ in $G_{1}^{\\prime}$ compared to $G_{2}^{\\prime}$ . Our argument is that the meta-game PNE pair results in both players playing the $\\mathrm{P_{1}}$ -led Stackelberg equilibrium of the realized game. Therefore, based on $\\mathrm{P_{2}}$ \u2019s responses generated by the trajectories of the meta-game PNE, $\\mathrm{P_{2}}$ can determine the realized game. ", "page_idx": 14}, {"type": "text", "text": "Consider the algorithm pair where $\\mathrm{P_{1}}$ plays her Stackelberg strategy of the realized game and $\\mathrm{P_{2}}$ plays a no-swap-regret algorithm. The proof of Theorem 3.1 showed that this pair forms an equilibrium and that $\\mathrm{P_{2}}$ \u2019s strategy eventually becomes the best-response of the realized game due to $\\mathrm{P_{2}}$ \u2019s no-swap-regret property. Note that the two games $G_{1}^{\\prime},G_{2}^{\\prime}$ in the support of $\\mathcal{D}^{\\prime}$ have different $\\mathrm{P_{2}}$ responses in $\\mathrm{P_{1}}$ \u2019s Stackelberg equilibrium. So based on the strategies $\\mathrm{P_{2}}$ plays, she will be able to deduce the realized game. Consider the belief function $h$ that at round $t$ computes the average strategy employed thus far: $\\begin{array}{r}{\\bar{\\mathbf{y}}_{t}\\,=\\,1/(t-1)\\sum_{r=1}^{t-1}\\mathbf{y}_{r}}\\end{array}$ and forms a belief concentrated on $G_{1}^{\\prime}$ if $\\|\\bar{\\mathbf{y}}_{t}-\\mathbf{\\bar{y}}(\\mathbf{x}^{\\star};G_{1}^{\\bar{\\prime}})\\|<\\|\\bar{\\mathbf{y}}_{t}-\\mathbf{\\bar{y}}(\\mathbf{x}^{\\star};G_{2}^{\\bar{\\prime}})\\|$ and forms a belief concentrated on $G_{2}^{\\prime}$ otherwise, where ${\\bf y}({\\bf x}^{\\star};G_{i}^{\\prime})$ is the one-hot encoding vector of the best response $y(\\mathbf{x}^{\\star};G)$ in game $G$ . The induced sequence of beliefs concentrates on the true realized game and hence satisfies the accuracy criterion for learning the realized game. Therefore, there is an equilibrium of algorithms that allows $\\mathrm{P_{2}}$ to successfully learn. ", "page_idx": 14}, {"type": "text", "text": "The above observation highlighted a limitation of successful learning based on repeated interactions. There could be two reasons driving this limitation. One possibility is that the accuracy criterion for successful learning is not strong enough to always enable achieving the average Stackelberg value benchmark. The other possibility is that the learning process relies on the actions of the other player in the repeated interaction. ", "page_idx": 14}, {"type": "text", "text": "We will now argue that the limitation arises from the second reason and not the first. If the accuracy criterion for successful learning is met through independent learning based on external signals instead of histories of the game dynamics, we will show that the average Stackelberg value benchmark becomes achievable. ", "page_idx": 15}, {"type": "text", "text": "The following proposition shows that if $\\mathrm{P_{2}}$ can successfully learn independently for every $\\mathcal{D}$ , then $\\mathrm{P_{2}}$ can achieve her average Stackelberg value $\\mathrm{StackVal}_{2}(\\mathcal{D})$ for every $\\mathcal{D}$ at a PNE of the meta-game. This extends Theorem 3.1, which stated that this is possible if $\\mathrm{P_{2}}$ was fully informed. Essentially, successful independent learning guarantees the same power to a player as being fully informed from the beginning. ", "page_idx": 15}, {"type": "text", "text": "Proposition A.5. For every $\\mathcal{D}$ for which $\\mathrm{P_{2}}$ can successfully learn independently (Definition A.3), there is an algorithm pair $(\\pi_{1},\\pi_{2})$ such that $(\\pi_{1},\\pi_{2})$ is a PNE of the meta-game and $\\forall G\\ \\in$ $\\mathcal{G}$ , $\\bar{U}_{2}(\\pi_{1},\\pi_{2};\\stackrel{\\cdot}{G})\\geq S t a c k V a l_{2}(G)-o_{T}(1).$ . ", "page_idx": 15}, {"type": "text", "text": "Proof sketch. The equilibrium algorithm pair we will show guarantees this utility to $\\mathrm{P_{2}}$ is the one where $\\mathrm{P_{2}}$ employs the Stackelberg response of the game denoted by the external signal $q^{t}$ at round $t$ and $\\mathrm{P_{1}}$ employs a no-swap-regret algorithm. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "We defined successful independent learning as when the less informed player learns the realized game to arbitrarily high precision, based on external signals. If the external signals were not quite as powerful, the average Stackelberg benchmark can no longer be always achieved. In particular, if $\\mathrm{P_{2}}$ could only learn the realized game to a precision bounded away from 1, Theorem 3.2 shows that the average Stackelberg benchmark is not always achievable when $\\mathrm{P_{1}}$ is fully informed about the game. ", "page_idx": 15}, {"type": "text", "text": "B Missing Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Theorem 3.1 is implied by Proposition A.5. Please see the proof of Proposition A.5 in Appendix B.2. ", "page_idx": 15}, {"type": "text", "text": "B.2 Proof of Proposition A.5 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We will construct an algorithm pair $(\\pi_{1},\\pi_{2})$ such that when $\\mathrm{P_{1}}$ is able to successfully learn through an independent sequence of signals $(q^{t})_{t=1}^{\\infty}$ , $(\\pi_{1},\\pi_{2})$ is an equilibrium pair and guarantees $\\mathrm{P_{1}}$ an expected average utility of at least $\\mathrm{StackVal}_{1}(G)\\!-\\!o_{T}(1)$ in $T$ rounds, for every realized game $G$ . The same proof also holds if $\\mathrm{P_{2}}$ is the player able to learn through external signals. Let $(\\mathbf{x}^{\\star}(\\bar{G}),y(\\mathbf{x}^{\\ast};G))$ denote $\\mathrm{{P}_{1}}$ \u2019s Stackelberg strategy and $\\mathrm{P_{2}}$ \u2019s best response in the game $G$ . Let $\\mathbf{y}(\\mathbf{x}^{*};G)$ be the one-hot encoding vector of $y(\\mathbf{x}^{*};G)$ . ", "page_idx": 15}, {"type": "text", "text": "We choose $\\pi_{2}$ to be a no-swap-regret algorithm with swap-regret rate ${\\cal O}(T^{a})$ for $a<1$ . We will first construct $\\pi_{1}$ in the setting with a known, finite time horizon $T$ so that $\\mathrm{P_{1}}$ achieves average regret $\\operatorname{StackVal}_{1}(G)-o_{T}(1)$ in $T$ rounds when interacting with $\\mathrm{P_{2}}$ employing a no-swap-regret algorithm. This is builds on the proof of Theorem 4 by Deng et al. [17]. We will present the argument here for completeness. We will later show how to apply the doubling trick to extend this to the setting with infinite time horizon. ", "page_idx": 15}, {"type": "text", "text": "Under the assumption that the realized game has no weakly dominated actions, we will show how $\\mathrm{P_{1}}$ can choose a strategy $\\mathbf{x}^{\\prime}$ so that $y(\\mathbf{x}^{\\star};G)$ is $\\mathrm{P_{2}}$ \u2019s unique best response and $\\mathbf{x}^{\\prime}$ is close to $\\mathbf{x}^{\\star}(G)$ . Since $y(\\mathbf{x}^{\\star};G)$ is not weakly dominated, there is no $\\bar{\\mathbf{y}}^{\\prime}\\in\\Delta(A_{2}\\top\\{y(\\mathbf{x}^{\\star};G)\\})$ such that $U_{2}(\\mathbf{x},\\mathbf{y}^{\\prime})\\ge U_{2}(\\mathbf{x},y(\\mathbf{x}^{\\star};G))$ for all $x\\in A_{1}$ . By Farkas\u2019 lemma [20], there must exist a $\\dot{\\mathbf{x}}\\in\\Delta(A_{1})$ such that $U_{2}(\\bar{\\mathbf{x}},y(\\mathbf{x}^{\\star};G))\\geq U_{2}(\\bar{\\mathbf{x}},y(\\mathbf{x}^{\\star};G))+c$ for a constant $c>0$ . This implies that if $\\mathrm{P_{1}}$ plays the strategy $\\mathbf{x}_{\\delta}^{\\prime}=(1-\\delta)\\mathbf{x}^{\\star}+\\delta\\bar{\\mathbf{x}}$ , then for all values of $\\delta>0$ , $\\mathrm{P_{2}}$ \u2019s unique best response to $\\mathbf{x}_{\\delta}^{\\prime}$ is $y(\\mathbf{x}^{\\star};G)$ . To indicate dependence of this strategy on the game $G$ , we will also denote it by $\\mathbf{x}_{\\delta}^{\\prime}(G)$ . Choosing a small $\\delta$ enables $\\mathrm{P_{1}}$ to play a strategy close to the Stackelberg strategy while ensuring that $\\mathrm{P_{2}}$ \u2019s unique best response is $y(\\mathbf{x}^{\\star};G)$ . ", "page_idx": 15}, {"type": "text", "text": "In the each round $t\\in[T],\\mathrm{P}_{1}$ employs $\\mathbf{x}_{\\delta^{T}}^{\\prime}(q^{t})$ , where $q^{t}$ is the signal at round $t$ . We will later describe how to choose $\\delta^{T}$ . Let $(\\mathbf{x}^{t},\\mathbf{y}^{t})_{t=1}^{\\infty}$ be the sequence of strategies generated through the interaction of the above algorithm of $\\mathrm{P_{1}}$ and $\\mathrm{P_{2}}$ \u2019s no-swap-regret algorithm. Given a $\\delta^{T}$ , let us compute $\\mathrm{P_{1}}$ \u2019s expected cumulative utility. Let $Z^{t}$ be a random variable indicating if the external signal at round $t$ is the realized game i.e., $Z^{t}\\,=\\,\\Im\\{q^{t}\\,=\\,G\\}$ . In rounds with $Z^{t}\\,=\\,1$ , the immediate regret of $\\mathrm{P_{2}}$ at round $t$ is $\\bar{c}\\|\\mathbf{y}^{t}-\\mathbf{y}(\\mathbf{x}^{\\star};G)\\|$ . A lower bound on $\\mathrm{P_{2}}$ \u2019s regret based on regret accumulated only in rounds with $Z^{t}\\,=\\,1$ is $\\begin{array}{r}{\\sum_{t=1}^{T}\\delta^{T}c\\cdot Z^{t}\\|\\mathbf{y}^{t}-\\mathbf{y}(\\mathbf{x}^{\\star};G)\\|}\\end{array}$ , where $c\\,=\\,U_{2}({\\bf x}^{\\prime},y({\\bf x}^{\\star};G))\\,-\\,$ $\\begin{array}{r}{\\operatorname*{max}_{y\\in\\mathcal{A}_{2}\\setminus\\{y(\\mathbf{x}^{\\star};G)\\}}U_{2}(\\mathbf{x}^{\\prime},y)}\\end{array}$ . ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{swap-regret}\\geq\\displaystyle\\sum_{t=1}^{T}\\delta^{T}c\\cdot Z^{t}\\|\\mathbf{y}^{t}-\\mathbf{y}(\\mathbf{x}^{\\star};G)\\|}\\\\ &{\\mathbb{E}[\\mathrm{swap-regret}]\\geq\\displaystyle\\sum_{t=1}^{T}\\delta^{T}c\\cdot\\mathbb{E}[Z^{t}]\\cdot\\mathbb{E}[\\|\\mathbf{y}^{t}-\\mathbf{y}(\\mathbf{x}^{\\star};G)\\|]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "(Independence between $q^{t}$ and $\\mathrm{P_{2}}$ \u2019s action) ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\geq\\delta^{T}c\\cdot(1-o_{T}(1))\\cdot\\mathbb{E}\\left[\\sum_{t=1}^{T}\\|\\mathbf{y}^{t}-\\mathbf{y}(\\mathbf{x}^{\\star};G)\\|\\right]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{E}[\\mathrm{swap-regret}]\\in O(T^{a})}\\\\ &{\\Longrightarrow\\left[\\displaystyle\\sum_{t=1}^{T}\\|\\mathbf{y}^{t}-\\mathbf{y}(\\mathbf{x}^{\\star};G)\\|\\right]\\in O(T^{a}/\\delta^{T}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "(Successful learning criteria) $\\pi_{2}$ \u2019s swap regret bound) ", "page_idx": 16}, {"type": "text", "text": "$\\mathrm{P_{1}}$ \u2019s cumulative utility satisfies: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\mathbb{\\Sigma}\\left[\\displaystyle\\sum_{t=1}^{T}U_{1}(\\mathbf{x}^{t},\\mathbf{y}^{t};G)\\right]\\geq\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}[Z^{t}]\\left(U_{1}(\\mathbf{x}_{\\delta}^{\\prime},y(\\mathbf{x}^{\\star};G);G)-c_{1}\\,\\mathbb{E}[\\|\\mathbf{y}^{t}-\\mathbf{y}(\\mathbf{x}^{\\star};G)\\|_{1}]\\right)+c_{2}\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}[1-\\delta\\mathbf{x}^{t}]^{2}}\\\\ {\\displaystyle\\geq\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}[Z^{t}]\\left(\\mathrm{StackVal}_{1}(G)-\\delta^{T}c_{3}-c_{1}\\,\\mathbb{E}[\\|\\mathbf{y}^{t}-\\mathbf{y}(\\mathbf{x}^{\\star};G)\\|_{1}]\\right)+c_{2}\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}[1-\\delta\\mathbf{y}^{t}]^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{c_{1}=U_{1}(\\mathbf{x}_{\\delta^{T}}^{\\prime},\\mathbf{y}(\\mathbf{x}^{\\star};G);G)-\\underset{y\\in\\mathcal{A}_{2}\\backslash\\{y(\\mathbf{x}^{\\star};G)\\}}{\\operatorname*{min}}U_{1}(\\mathbf{x}_{\\delta^{T}}^{\\prime},y;G),}\\\\ &{c_{2}=\\underset{x\\in\\mathcal{A}_{1},y\\in\\mathcal{A}_{2}}{\\operatorname*{min}}U_{1}(x,y),}\\\\ &{c_{3}=\\mathtt{S t a c k V a l}_{1}(G)-U_{1}(\\bar{\\mathbf{x}},y(\\mathbf{x}^{\\star};G);G).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Finally, we use Equation (1) and $\\mathbb{E}[Z^{t}]=1-o_{t}(1)$ to conclude that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t=1}^{T}U_{1}(\\mathbf{x}^{t},y^{t})\\right]\\geq\\operatorname{StackVal}_{1}(G)\\cdot T-c_{3}\\delta^{T}T-c_{1}O(T^{a})/\\delta^{T}-o(T).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In eq. (2), the second term $c_{3}\\delta^{T}T$ comes from playing action $\\bar{\\bf x}$ instead of $\\mathbf{x}$ to induce a unique best response; the third term $c_{1}O(T^{a})/\\delta^{T}$ comes from the swap regret of $\\mathrm{P_{2}}$ , and the last $o(T)$ term comes from errors in the external signals $Z_{t}$ . By choosing $\\delta^{T}=T^{-b}$ for some $0<b<1-a,$ $\\pi_{1}$ yields $\\operatorname{StackVal}_{1}(G)\\cdot T-o(T)$ utility to $\\mathrm{P_{1}}$ . In the special case where the last $o(T)$ term is zero (e.g., when $\\mathrm{P_{1}}$ has perfect knowledge about the game $G$ as in Theorem 3.1), we can achieve the optimal tradeoff by setting $b$ to be $\\textstyle{\\frac{1-a}{2}}$ , which yields a convergence rate of $O(T^{\\frac{1+a}{2}})$ to $\\operatorname{StackVal}_{1}(G)$ . ", "page_idx": 16}, {"type": "text", "text": "The doubling trick to construct $\\pi_{1}$ that does not rely on the time horizon being known: Initialize a maximum horizon $T_{m}$ . Until the round index $t$ hits $T_{m}$ , employ $\\mathbf{x}_{\\delta^{T_{m}}}^{\\prime}$ . Once $t\\,=\\,T_{m}$ , update $T_{m}\\gets2T_{m}$ and employ $\\mathbf{x}_{\\delta^{T_{m}}}^{\\prime}$ until the round index exceeds the new max horizon. ", "page_idx": 16}, {"type": "text", "text": "Remark B.1 (Rate of convergence to Stackelberg benchmark.). Algorithms that provide swapregret rates of ${\\cal O}(T^{1/2})$ are known [7, 37]. By setting $\\textstyle a={\\frac{1}{2}}$ and $\\textstyle b={\\frac{1-a}{2}}$ , our proof shows that these algorithms lie in a PNE of the meta-game that results in a $O(T^{3/4})$ convergence rate to $\\mathrm{P}_{1}\\,\\mathit{\\Omega}_{s}$ Stackelberg benchmark. ", "page_idx": 16}, {"type": "text", "text": "B.3 Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We prove this theorem for every fixed signal precision $p_{2}=p\\leq p^{\\star}$ . Before diving into the proof, we will first introduce some notations. For $(i,\\bar{j})\\in\\{1,2\\}\\times\\{\\bar{1},2\\}$ , we use ${\\mathsf{C S P}}_{i j}$ to denote the CSP ", "page_idx": 16}, {"type": "text", "text": "Figure 4: game matrices $G_{1}$ and $G_{2}$ . $\\mathrm{P_{1}}$ is the row player and $\\mathrm{P_{2}}$ is the column player. The values in each cell are ( $\\mathrm{{P}_{1}}$ \u2019s utility, $\\mathrm{P_{2}}$ \u2019s utility). Shaded cells represent the action profiles supported in the Stackelberg equilibria led by the column player. The parameter $\\gamma$ is defined as $\\begin{array}{r}{\\frac{1-p^{\\star}}{1+p^{\\star}}\\in(0,1]}\\end{array}$ , where $p^{\\star}$ is the precision threshold of $p_{2}$ . ", "page_idx": 17}, {"type": "text", "text": "induced by $\\mathcal{T}^{T}(\\pi_{1},\\pi_{2};s_{1}=G_{i},s_{2}=G_{j},G=G_{i})$ . Note that we have taken $s_{1}$ and $G$ to both be $G_{i}$ because $s_{1}\\equiv G$ when $p_{1}=1$ . Recall that $\\mathcal{T}^{T}(\\pi_{1},\\pi_{2};G_{i},G_{j},G_{i})$ is the distribution over trajectories of length $T$ (denoted as $\\boldsymbol{\\tau}=(\\mathbf{x}^{t},\\mathbf{y}^{t})_{t\\in[T]})$ generated by the pair of algorithms $(\\pi_{1},\\pi_{2})$ when $\\pi_{1}$ takes input signal $s_{1}=G_{i}$ and $\\pi_{2}$ takes input signal $s_{2}=G_{j}$ . Formally, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathsf{C S P}_{i j}\\triangleq\\mathsf{C S P}_{\\mathcal T^{T}(\\pi_{1},\\pi_{2};G_{i},G_{j},G_{i})}=\\frac{\\mathbb{E}}{\\tau\\sim T^{T}(\\pi_{1},\\pi_{2};G_{i},G_{j},G_{i})}\\left[\\frac{1}{T}\\sum_{t=1}^{T}\\mathbf{x}_{t}\\otimes\\mathbf{y}_{t}\\right].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In addition, for $i\\in\\{1,2\\}$ , we use ${\\mathsf{C S P}}_{i}$ to denote the CSP generated by the trajectory distribution $\\mathcal{T}^{T}(\\pi_{1},\\pi_{2};G_{i})$ . Since $s_{1}$ perfectly correlates with $G$ , and $s_{2}$ is drawn from the signal distribution $\\begin{array}{r}{\\varphi_{p_{2}}(s|G)=\\frac{1+p_{2}}{2}\\cdot\\mathbb{1}\\left\\{s=G\\right\\}+\\frac{1-p_{2}}{2}\\cdot\\mathbb{1}\\left\\{s\\neq G\\right\\}\\!.}\\end{array}$ , we can equivalently write $\\mathsf{C S P_{1}}$ and $\\mathsf{C S P_{2}}$ as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathsf{C S P}_{1}=\\frac{1+p}{2}\\mathsf{C S P}_{11}+\\frac{1-p}{2}\\mathsf{C S P}_{12};}\\\\ {\\displaystyle\\mathsf{C S P}_{2}=\\frac{1-p}{2}\\mathsf{C S P}_{21}+\\frac{1+p}{2}\\mathsf{C S P}_{22}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "As argued in the proof sketch, we will establish the following three claims about $\\mathsf{C S P}_{1}$ and $\\mathsf{C S P}_{2}$ . ", "page_idx": 17}, {"type": "text", "text": "Claim B.2. If $\\cdot(\\pi_{1},\\pi_{2})$ forms an equilibrium in the algorithm space, then $\\begin{array}{r}{\\mathsf{C S P}_{1}(B,D)\\leq\\frac{\\gamma}{8}+o_{T}(1)}\\end{array}$ . ", "page_idx": 17}, {"type": "text", "text": "Claim B.3. $I f(\\pi_{1},\\pi_{2})$ forms an equilibrium in the algorithm space, then $\\begin{array}{r}{\\mathsf{C S P}_{1}(A,D)\\leq\\frac{\\gamma}{8}+o_{T}(1)}\\end{array}$ . ", "page_idx": 17}, {"type": "text", "text": "Claim B.4. If Claim B.3 holds and $(\\pi_{1},\\pi_{2})$ grants $\\mathrm{P_{2}}$ the expected Stackelberg value, i.e., ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{T\\to\\infty}\\bar{U}_{2}(\\pi_{1},\\pi_{2};D)\\geq\\frac{3}{2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "then $\\mathsf{C S P}_{2}(B,D)\\geq1/2+o(1)$ . ", "page_idx": 17}, {"type": "text", "text": "We will first build the proof of Theorem 3.2 on these claims, and then formally establish these claims in appendix B.4. ", "page_idx": 17}, {"type": "text", "text": "Assume that $(\\pi_{1},\\pi_{2})$ is a PNE in the meta-game that lets $\\mathrm{P_{2}}$ achieve the benchmark $\\begin{array}{r}{\\mathrm{StackVal}({\\mathcal{D}})=\\frac{3}{2}}\\end{array}$ . We show that if $(\\pi_{1},\\pi_{2})$ satisfies Claims B.2 through B.4, then $\\mathrm{P_{1}}$ gains utility by deviating to an algorithm $\\pi_{1}^{\\prime}$ that always plays according to $G_{1}$ . ", "page_idx": 17}, {"type": "text", "text": "Formally, consider $\\pi_{1}^{\\prime}$ defined as follows. For any history time step $t>0$ and any history $H_{1}^{r}\\left(r>0\\right)$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\pi_{1}^{\\prime}(G,H_{1}^{1:t-1})\\triangleq\\pi_{1}(G_{1},H_{1}^{1:t-1}),\\quad\\forall G\\in\\{G_{1},G_{2}\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "After the deviation, $\\mathrm{{P}_{1}}$ \u2019s average utility can be expressed as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\bar{U}_{1}^{T}({\\pi}_{1}^{\\prime},{\\pi}_{2})=\\frac{1}{2}\\bar{U}_{1}({\\pi}_{1},{\\pi}_{2};G_{1})+\\frac{1-p}{4}\\,{\\displaystyle{\\frac{\\mathbb{E}}{{\\cal\\tau}\\sim{\\cal T}^{T}({\\pi}_{1}^{\\prime},{\\pi}_{2};G_{2},G_{1},G_{2})}}\\left[\\frac{1}{T}\\sum_{t=1}^{T}U_{1}({\\bf x}_{t},{\\bf y}_{t};G_{2})\\right]}}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad+\\,\\frac{1+p}{4}\\,{\\displaystyle{\\frac{\\mathbb{E}}{\\mathrm{\\pi}\\sim{\\cal T}^{T}({\\pi}_{1}^{\\prime},{\\pi}_{2};G_{2},G_{2},G_{2})}}\\left[\\frac{1}{T}\\sum_{t=1}^{T}U_{1}({\\bf x}_{t},{\\bf y}_{t};G_{2})\\right]}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $\\pi_{1}^{\\prime}$ is defined to behave according to $\\pi_{1}$ on observing $G_{1}$ , the above equation can be rewritten as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{U}_{1}(\\pi_{1}^{\\prime},\\pi_{2})=\\displaystyle\\frac{1}{2}\\bar{U}_{1}(\\pi_{1},\\pi_{2};G_{1})+\\frac{1-p}{4}\\underbrace{\\mathbb{E}}_{(x,y)\\sim\\mathsf{C S P}_{11}}[U_{1}(x,y;G_{2})]}\\\\ &{\\qquad\\qquad\\qquad+\\displaystyle\\frac{1+p}{4}\\underbrace{\\mathbb{E}}_{(x,y)\\sim\\mathsf{C S P}_{12}}[U_{1}(x,y;G_{2})]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now we define a new CSP to be the mixture of ${\\mathsf{C S P}}_{11}$ and $\\mathsf{C S P}_{12}$ but ${\\mathsf{C S P}}_{12}$ is taking up more probability mass than $\\mathsf{C S P}_{11}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathsf{C S P}_{1}^{\\prime}\\triangleq\\frac{1-p}{2}\\mathsf{C S P}_{11}+\\frac{1+p}{2}\\mathsf{C S P}_{12}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that compared with the ${\\mathsf{C S P}}_{1}$ defined before (repeated below) ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathsf{C S P}_{1}=\\frac{1+p}{2}\\mathsf{C S P}_{11}+\\frac{1-p}{2}\\mathsf{C S P}_{12},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "we can conclude that for any action pair $(x,y)\\in\\{A,B\\}\\times\\{C,D\\}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\mathsf{C S P}}_{1}^{\\prime}(x,y)\\leq{\\frac{1+p}{1-p}}{\\mathsf{C S P}}_{1}(x,y)\\leq{\\frac{1+p^{\\star}}{1-p^{\\star}}}{\\mathsf{C S P}}_{1}(x,y)={\\frac{1}{\\gamma}}{\\mathsf{C S P}}_{1}(x,y),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the second step used $p\\leq p^{\\star}$ and the last step uses the definition of $\\begin{array}{r}{\\gamma=\\frac{1-p^{\\star}}{1+p^{\\star}}}\\end{array}$ . Combined with the upper bounds on $\\mathsf{C S P}_{1}(B,D)$ and $\\mathsf{C S P}_{1}(A,D)$ from Claim B.2 and Claim B.3, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathsf{C S P}_{1}^{\\prime}(B,D)+\\mathsf{C S P}_{1}^{\\prime}(A,D)\\leq\\frac{1}{\\gamma}\\left(\\mathsf{C S P}_{1}(B,D)+\\mathsf{C S P}_{1}(A,D)\\right)\\leq\\frac{1}{4}+o_{T}(1).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Finally, we lower bound the increase in $\\mathrm{P_{1}}$ \u2019s utility after deviating from $\\pi_{1}$ to $\\pi_{1}^{\\prime}$ as follows. Since ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\bar{U}_{1}^{T}(\\pi_{1}^{\\prime},\\pi_{2})-\\bar{U}_{1}^{T}(\\pi_{1}^{\\prime},\\pi_{2})=\\frac{1}{2}\\left(\\underset{(x,y)\\sim\\mathsf{C S P}_{1}^{\\prime}}{\\mathbb{E}}[U_{1}(x,y;G_{2})]-\\underset{(x,y)\\sim\\mathsf{C S P}_{1}}{\\mathbb{E}}[U_{1}(x,y;G_{2})]\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "it suffices to lower bound the difference in utility when $G_{2}$ is realized. ", "page_idx": 18}, {"type": "text", "text": "On the one hand, note that $U_{1}(x,y;G_{2})\\ge0.9$ as long as $(x,y)\\neq(A,D)$ or $(B,D)$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{(x,y)\\sim C5\\mathsf{P}_{1}^{\\prime}}{\\mathbb{E}}\\left[U_{1}(x,y;G_{2})\\right]\\geq(1-C5\\mathsf{P}_{1}^{\\prime}(A,D)-C5\\mathsf{P}_{1}^{\\prime}(B,D))\\cdot0.9}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq\\bigg(\\frac{3}{4}-o_{T}(1)\\bigg)\\cdot0.9>0.6-o_{T}(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "On the other hand, since $U_{1}(x,y;G_{2})\\le0.1$ when $(x,y)=(B,D)$ and $U_{1}(x,y;G_{2})\\le1$ otherwise, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{(x,y)\\sim\\mathsf{C S P}_{1}}{\\mathbb{E}}[U_{1}(x,y;G_{2})]\\leq C\\mathsf{S P}_{1}(B,D)\\cdot0.1+(1-\\mathsf{C S P}_{1}(B,D))\\cdot1}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\frac{1}{2}\\cdot1.1+o_{T}(1)\\qquad(\\mathsf{C S P}_{1}(B,D)\\geq\\frac{1}{2}+o_{T}(1)\\mathrm{~from~Claim~B.4})}\\\\ &{\\qquad\\qquad\\qquad\\qquad<0.6+o_{T}(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "As a result, after taking their difference, we have that in the asymptotic regime ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{T\\to\\infty}\\left(\\bar{U}_{1}^{T}(\\pi_{1}^{\\prime},\\pi_{2};D)-\\bar{U}_{1}^{T}(\\pi_{1}^{\\prime},\\pi_{2};D)\\right)>0,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which contradicts with the assumption that $(\\pi_{1},\\pi_{2})$ forms a PNE (cf. Definition 2.1) in the metagame! Therefore, it cannot be possible for any PNE $(\\pi_{1},\\pi_{2})$ of the meta game to achieve the benchmark Sta $\\Im\\mathrm{Val}_{2}(\\mathcal{D})$ for $\\mathrm{P_{2}}$ . The proof is thus complete. ", "page_idx": 18}, {"type": "text", "text": "B.4 Proof of Technical Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof of Claim B.2. We prove this claim by contradiction. Assume that the above claim does not hold, i.e., ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{T\\to\\infty}\\mathsf{C S P}_{1}((B,D))>\\frac{\\gamma}{8},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "we will show that $\\mathrm{P_{1}}$ gains utility by deviating to another algorithm $\\pi_{1}^{\\prime}$ that always plays action $A$ regardless of the signals and the feedbacks observed. We have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{T\\rightarrow\\infty}{\\operatorname*{liminf}}\\ \\hat{U}_{1}^{T}(\\pi_{1},\\pi_{2};O)}\\\\ &{=\\underset{T\\rightarrow\\infty}{\\operatorname*{liminf}}\\left(\\frac{1}{2}\\tilde{U}_{1}^{T}(\\pi_{1},\\pi_{2};G_{1})+\\frac{1}{2}\\tilde{U}_{1}^{T}(\\pi_{1},\\pi_{2};G_{2})\\right)}\\\\ &{=\\underset{T\\rightarrow\\infty}{\\operatorname*{liminf}}\\left(\\frac{1}{2}\\underset{(x,y)\\rightarrow\\infty}{\\operatorname*{lim}}{\\operatorname*{lim}}\\int_{\\mathbb{I}}(x,y;G_{1})+\\frac{1}{2}\\underset{(x,y)\\rightarrow\\infty}{\\operatorname*{lim}}{\\operatorname*{lim}}{\\operatorname*{lim}}_{(x,y;G_{2})}\\right)}\\\\ &{\\leq\\underset{T\\rightarrow\\infty}{\\operatorname*{liminf}}\\left(\\frac{1}{2}\\left(\\mathbf{C}\\mathbf{P}_{1}(B,D)\\cdot\\boldsymbol{0}+(1-\\mathbf{C}\\mathbf{P}_{1}(B,D))\\cdot\\frac{16}{\\gamma}\\right)+\\frac{1}{2}\\cdot1\\right)}\\\\ &{\\in\\underset{(U_{1}(x,y),G_{1})}{\\operatorname*{lim}}\\leq\\frac{16}{\\gamma}\\,\\mathrm{for~all}\\ (x,y)\\neq(B,D);U_{2}(x,y;G_{2})\\leq1\\mathrm{~for~all}\\ (x,y))}\\\\ &{<\\frac{1}{2}\\left(1-\\frac{\\gamma}{\\gamma}\\right)\\frac{16}{\\gamma}+\\frac{1}{2}\\qquad\\qquad\\mathrm{~(assumption~that~limsup}_{T\\rightarrow\\infty}\\subset\\mathbf{S}\\mathbf{P}_{1}((B,D))>\\frac{\\gamma}{8})}\\\\ &{=\\frac{8}{\\gamma}-\\frac{1}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "On the other hand, since $\\mathrm{{P}_{1}}$ always plays action $A$ under $\\pi_{1}^{\\prime}$ , which has utility $\\frac{16}{\\gamma}$ in $G_{1}$ regardless of the strategy of $\\mathrm{P_{2}}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{T\\to\\infty}\\operatorname{sup}_{}\\bar{U}_{1}^{T}(\\pi_{1}^{\\prime},\\pi_{2};\\mathcal{D})\\ge\\operatorname*{lim}_{T\\to\\infty}\\operatorname*{sup}_{}\\frac{1}{2}\\bar{U}_{1}^{T}(\\pi_{1}^{\\prime},\\pi_{2};\\mathcal{D})\\ge\\frac{1}{2}\\cdot\\frac{16}{\\gamma}=\\frac{8}{\\gamma}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Combining the above two inequalities give us ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{T\\to\\infty}\\left(\\bar{U}_{1}^{T}(\\pi_{1}^{\\prime},\\pi_{2};D)-\\bar{U}_{1}^{T}(\\pi_{1},\\pi_{2};D)\\right)>0,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which violates the requirement of PNE in Definition 2.1. Therefore, we have established the claim that $\\begin{array}{r}{\\mathsf{C S P}_{1}(B,D)\\leq\\frac{\\bar{\\gamma}}{8}+o_{T}(1)}\\end{array}$ . \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Proof of Claim B.3. Again, assume for the sake of contradiction that $\\begin{array}{r}{\\mathsf{C S P}_{1}(A,D)\\leq\\frac{\\gamma}{8}+o_{T}(1)}\\end{array}$ does not hold, which implies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{T\\to\\infty}\\mathsf{C S P}_{1}(A,D)>\\frac{\\gamma}{8}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We upper bound $\\mathrm{P_{2}}$ \u2019s utility under equilibrium as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{T\\rightarrow\\infty}{\\operatorname*{liminf}}\\,\\bar{U}_{2}^{T}(\\pi_{1},\\pi_{2};\\mathcal{D})}\\\\ &{=\\operatorname*{liminf}\\left(\\frac{1}{2}\\underset{(x,y)\\sim\\mathsf{C S P}_{1}}{\\mathbb{E}}\\bar{U}_{2}(x,y;G_{1})+\\frac{1}{2}\\underset{(x,y)\\sim\\mathsf{C S P}_{2}}{\\mathbb{E}}\\bar{U}_{2}(x,y;G_{2})\\right)}\\\\ &{\\leq\\underset{T\\rightarrow\\infty}{\\operatorname*{liminf}}\\left(\\frac{1}{2}\\cdot\\mathsf{C S P}_{1}(A,D)\\cdot\\left(-\\frac{32}{\\gamma}\\right)+\\left(1-\\frac{1}{2}\\cdot\\mathsf{C S P}_{1}(A,D)\\right)\\cdot2\\right)}\\\\ &{\\quad-\\frac{\\gamma}{16}\\cdot\\bigg(-\\frac{32}{\\gamma}\\bigg)+\\left(1-\\frac{\\gamma}{16}\\right)\\cdot1\\qquad\\quad\\mathrm{(P2)^{\\top}{\\mathrm{~sutility~is-32/\\gamma}}\\,f o r~}(A,D)\\mathrm{~and\\,\\,\\leq~2\\,for~all~oti~}}\\\\ &{<\\frac{\\gamma}{16}\\cdot\\left(-\\frac{32}{\\gamma}\\right)+\\left(1-\\frac{\\gamma}{16}\\right)\\cdot1\\qquad\\quad\\mathrm{(assumption~that~limsup}_{T\\rightarrow\\infty}\\,\\mathsf{C S P}_{1}(A,D)}\\\\ &{<0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "On the other hand, if $\\mathrm{P_{2}}$ deviates to an algorithm $\\pi_{2}^{\\prime}$ that always plays action $C$ regardless of the signal $s_{2}$ and the observed feedbacks, then the average utility $\\vec{U}_{2}(\\dot{\\pi}_{1},\\dot{\\pi}_{2}^{\\prime};D)$ is always nonnegative. As a result, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{T\\to\\infty}\\left(\\bar{U}_{2}^{T}(\\pi_{1},\\pi_{2}^{\\prime};\\mathcal D)-\\bar{U}_{2}^{T}(\\pi_{1},\\pi_{2},;\\mathcal D)\\right)>0,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which again violates the condition of $(\\pi_{1},\\pi_{2})$ being in a PNE in the meta-game. Therefore, we must have $\\begin{array}{r}{\\mathsf{C S P}_{1}(A,D)\\leq\\frac{\\gamma}{8}+O_{T}(1)}\\end{array}$ . ", "page_idx": 19}, {"type": "text", "text": "Proof of Claim B.4. Note that $U_{2}$ is the same under $G_{1}$ and $G_{2}$ , for which the top-2 highest utility values are achieved by $(B,D)$ and $(A,C)$ respectively. Therefore, we can upper bound $\\mathrm{P_{2}}$ \u2019s expected average utility as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{U}_{2}^{T}(\\pi_{1},\\pi_{2};\\mathcal{D})=\\frac{1}{2}\\underset{(x,y)\\sim<5\\mathsf{P}_{1}}{\\mathbb{E}}U_{2}(x,y;G_{1})+\\frac{1}{2}\\underset{(x,y)\\sim<5\\mathsf{P}_{2}}{\\mathbb{E}}U_{2}(x,y;G_{2})}\\\\ &{\\phantom{\\bar{U}_{2}^{T}(\\pi_{1},\\pi_{2};\\mathcal{D})=}\\leq\\frac{1}{2}\\Big((1-C5\\mathsf{P}_{1}(B,D))\\cdot1+C5\\mathsf{P}_{1}(B,D)\\cdot2\\Big)}\\\\ &{\\phantom{\\bar{U}_{2}^{T}(\\pi_{1},\\pi_{2};\\mathcal{D})=}+\\frac{1}{2}\\Big((1-C5\\mathsf{P}_{2}(B,D))\\cdot1+C5\\mathsf{P}_{2}(B,D)\\cdot2\\Big)}\\\\ &{\\phantom{\\bar{U}_{2}^{T}(\\pi_{1},\\pi_{2};\\mathcal{D})=}=1+\\frac{1}{2}C5\\mathsf{P}_{1}(B,D)+\\frac{1}{2}C5\\mathsf{P}_{2}(B,D)}\\\\ &{\\phantom{\\bar{U}_{2}^{T}(\\pi_{1},\\pi_{2};\\mathcal{D})=}\\leq1+\\frac{\\gamma}{16}+\\frac{1}{2}C5\\mathsf{P}_{2}(B,D)+o(1),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last step uses Claim B.3. On the other hand, from the assumption, $\\bar{U}_{2}(\\pi_{1},\\pi_{2};D)$ is lower bounded by $3/2$ as $T\\to\\infty$ . Therefore, we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{T\\to\\infty}{\\mathsf{C S P}}_{2}(B,D)\\geq1-\\frac{\\gamma}{8}\\geq\\frac{1}{2},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which proves $\\begin{array}{r}{\\mathsf{C S P}_{2}(B,D)\\geq\\frac{1}{2}+o(1)}\\end{array}$ . ", "page_idx": 20}, {"type": "text", "text": "C Implications from prior works ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we provide arguments for the following claims about PNE of the meta-game that are implied either directly or indirectly by previous works. We say that an algorithm $\\pi_{i}$ of $\\mathrm{P}_{i}$ is supported in a meta-game PNE if there exists an algorithm $\\pi_{-i}$ such that the pair $(\\pi_{i},\\pi_{-i})$ forms a PNE of the meta-game. The claims in this section provide answers to whether common classes of algorithms such as (swap)regret-minimizing, myopically best-responding, playing Stackelberg response are always/sometimes/never supported in a meta-game PNE. ", "page_idx": 20}, {"type": "text", "text": "Claim C.1. All no-swap-regret algorithms are supported in a meta-game PNE for all games $G$ . Put another way, being no-swap-regret is a sufficient condition for an algorithm to be supported in some meta-game PNE. ", "page_idx": 20}, {"type": "text", "text": "Proof. This is a direct corollary of [17, Theorem 6] which can also be justified by our proof of Theorem 3.1. Consider the pair of algorithms $(\\pi_{1},\\pi_{2})$ in which $\\pi_{1}$ plays strategies close to the Stackelberg optimal strategy (via a doubling trick, see Appendix B.2 for the full construction), and $\\pi_{2}$ is a no-swap-regret algorithm. We have proved in Appendix B.2 that $(\\pi_{1},\\pi_{2})$ is a PNE in the metagame because no-swap-regret algorithms are able to cap their opponent\u2019s utility at the Stackelberg value. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "The second claim is stated as Theorem 2 by Brown et al. [10]. Since most game matrices do not have PNE, this claim effectively states that a pair of no-swap-regret algorithms cannot be PNE in the meta-game for most games. ", "page_idx": 20}, {"type": "text", "text": "Claim C.2 (Theorem 2, [10]). Unless the stage game $G$ has a PNE, any pair of two no-swap regret algorithms cannot form a PNE of the meta-game. ", "page_idx": 20}, {"type": "text", "text": "Claim C.3. No-regret is not a sufficient condition for an algorithm to be supported in a meta-game PNE. For common no-regret algorithms such as EXP3, there is a game where no meta-game PNE contains this algorithm. ", "page_idx": 20}, {"type": "text", "text": "Proof sketch. This claim can be established by combining two claims in [9]. In their setting, a single seller repeatedly sells a single item to a single buyer for $T$ rounds. We will use two of their results: ", "page_idx": 20}, {"type": "text", "text": "1. (Theorem 3.1 of [9]) If the buyer uses EXP3 or other mean-based algorithms, then there exists an algorithm for the seller which extracts (almost) full welfare.   \n2. (Theorem 3.3 of [9]) There exists an algorithm for the buyer (no-regret without overbidding), which caps the seller\u2019s revenue at the Mayerson value. Wlog, assume $\\mathrm{P_{1}}$ \u2019s algorithm $\\pi_{1}$ is a mean-based no-regret algorithm such as EXP3. We will show that for any algorithm $\\pi_{2}$ of $\\mathrm{P_{2}}$ , the pair $(\\pi_{1},\\pi_{2})$ cannot be a PNE of the meta-game.   \nLet $\\pi_{2}^{\\prime}$ be the algorithm given by the above result 1 that lets $\\mathrm{P_{2}}$ extract full welfare against $\\pi_{1}$ . Let $\\pi_{1}^{\\prime}$ be the algorithm that achieves the property in the above result 2.   \nOn the one hand, if $\\bar{U}_{1}^{T}(\\pi_{1},\\pi_{2})\\leq\\mathsf{W e l f a r e}({\\cal D})-\\Omega(T)$ , then $\\mathrm{P_{1}}$ will increase utility by deviating to algorithm $\\pi_{1}^{\\prime}$ , thus $(\\pi_{1},\\pi_{2})$ cannot be a PNE in the meta-game.   \nOn the other hand, if $\\pi_{1}$ is already extracting full welfare against $\\pi_{2}$ (meaning that $\\mathrm{{P}_{1}}$ is getting asymptotically zero utility), then $\\mathrm{P_{1}}$ has the incentive to deviate to algorithm $\\pi_{2}^{\\prime}$ , under which she can cap $\\mathrm{P_{2}}$ \u2019s utility at the Myerson value and therefore guarantee herself nonzero utility. For this reason, $(\\pi_{1},\\pi_{2})$ cannot be a PNE in the meta-game.   \nCombining the above two cases, we conclude that $\\pi_{1}$ cannot be supported in any PNE of the meta-game. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The abstract and the introduction clearly state the claims and contributions of this paper. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We discuss the limitations in Section 5. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We provide a proof for each theoretical result in either the main body or the appendix. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: While our work provides commentary and critiques of how to interpret previous results on learning in strategic interactions to overcome lack of information, we do not prescribe any new methods. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 26}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This paper does not use existing assets. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]