[{"figure_path": "TEwQSWWn7S/tables/tables_14_1.jpg", "caption": "Table 1: Performance comparison of different models on various datasets for the sampling using PTT versus Gibbs sampling for 104 mcmc steps. The acceleration factor is defined as the ratio of the average number of jumps obtained until 104 steps between PTT and Gibbs sampling. For pre-train+PCD, the RCM machine has not to be counted among the list of models (hence the +1) because it is very fast to sample from.", "description": "This table compares the performance of different sampling methods (PTT vs Gibbs sampling) for different models (pre-train+PCD, JarJar, PCD) trained on two datasets (MNIST01, Human Genome).  The acceleration factor shows how much faster PTT is compared to Gibbs sampling in terms of the number of jumps between clusters within 10<sup>4</sup> MCMC steps.  The table highlights that pre-training significantly improves sampling efficiency.", "section": "5 Standard Gibbs sampling vs. Parallel Trajectory Tempering (PTT)"}, {"figure_path": "TEwQSWWn7S/tables/tables_15_1.jpg", "caption": "Table 2: Details of the datasets used during training.", "description": "This table presents details about three different datasets used in the paper's experiments: Human Genome Dataset (HGD), MNIST-01, and Mickey. For each dataset, the table provides the number of samples, the number of dimensions, and the split between training and testing sets (both 60% and 40%).  This information is crucial for understanding the scale and characteristics of the data used in the experiments, allowing readers to assess the generalizability and reproducibility of the results.", "section": "C Training details"}, {"figure_path": "TEwQSWWn7S/tables/tables_17_1.jpg", "caption": "Table 3: Hyperparameters used for the training of RBMs.", "description": "This table lists the hyperparameters used for training Restricted Boltzmann Machines (RBMs) on three different datasets: HGD, MNIST-01, and Mickey. For each dataset and training method (PCD, Jar-RBM, and Pre-train+PCD), the table shows the batch size, number of chains, number of epochs, learning rate, number of MCMC steps, and number of hidden nodes.  The hyperparameters were chosen based on previous studies and experimentation to ensure convergence and good performance.", "section": "Training details"}]