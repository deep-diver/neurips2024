[{"type": "text", "text": "Qualitative Mechanism Independence ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Oliver E. Richardson Spencer Peters Joseph Y. Halpern   \nDept of Computer Science Dept of Computer Science Dept of Computer Science Cornell University Cornell University Cornell University Ithaca NY 14853 Ithaca NY 14853 Ithaca NY 14853   \noli@cs.cornell.edu speters@cs.cornell.edu halpern@cs.cornell.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We define what it means for a joint probability distribution to be (QIM-)compatible with a set of independent causal mechanisms, at a qualitative level\u2014or, more precisely, with a directed hypergraph $\\boldsymbol{\\mathcal{A}}$ , which is the qualitative structure of a probabilistic dependency graph (PDG). When $\\boldsymbol{\\mathcal{A}}$ represents a qualitative Bayesian network, QIM-compatibility with $\\boldsymbol{\\mathcal{A}}$ reduces to satisfying the appropriate conditional independencies. But giving semantics to hypergraphs using QIM-compatibility lets us do much more. For one thing, we can capture functional dependencies. For another, QIM-compatibility captures important aspects of causality: we can use compatibility to understand cyclic causal graphs, and to demonstrate compatibility is essentially to produce a causal model. Finally, compatibility has deep connections to information theory. Applying compatibility to cyclic structures helps to clarify a longstanding conceptual issue in information theory. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The structure of a (standard) probabilistic graphical model (like a Bayesian Network or Markov Random Field) encodes a set of conditional independencies among variables. This is useful because it enables a compact description of probability distributions that have those independencies; it also lets us use graphs as a visual language for describing important qualitative properties of a probabilistic world. Yet these kinds of independencies are not the only important qualitative aspects of a probability measure. In this paper, we study a natural generalization of standard graphical model structures that can describe far more than conditional independence. ", "page_idx": 0}, {"type": "text", "text": "For example, another qualitative aspect of a probability distribution is that of functional dependence, which is also exploited across computer science to enable compact representations and simplify probabilistic analysis. Acyclic causal models, for instance, specify a distribution via a probability over contexts (the values of variables whose causes are viewed as outside the model), and a collection of equations (i.e., functional dependencies) [18]. And in deep learning, a popular class of models called normalizing flows [25, 12] specify a distribution by composing a fixed distribution over some latent space, say a standard normal distribution, with a function (i.e., a functional dependence) fit to observational data. Functional dependence and independence are deeply related and interacting notions. For instance, if $B$ is a function of $A$ (written $A\\to B)$ and $A$ is independent of $C$ (written $A\\bot C)$ , then $B$ and $C$ are also independent $(B\\bot C)$ .1 Moreover, dependence can be written in terms of independence: $Y$ is a function of $X$ if and only if $Y$ is conditionally independent of itself given $X$ (i.e., $X\\rightarrow Y$ iff $Y\\perp Y\\mid X)$ . Traditional graph-based languages such as Bayesian Networks (BNs) and Markov Random Fields (MRFs) cannot capture these relationships. Indeed, the graphoid axioms (which describe BNs and MRFs) [21] and axioms for conditional independence [17], do not even consider statements like $A$ \u22a5\u22a5 $A$ to be syntactically valid. Yet such statements are perfectly meaningful, and reflect a deep relationship between independence, dependence, and generalizations of both notions (grounded in information theory, a point we will soon revisit). ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "This paper provides a simple yet expressive graphical language for describing qualitative structure such as dependence and independence in probability distributions. The idea is to specify the inputs and outputs of a set of independent mechanisms: processes by which some target variables $T$ are determined as a (possibly randomized) function of some source variables $S$ . This idea generalizes intuition going back to Pearl [18] by allowing, for example, two mechanisms to share a target variable. So at a qualitative level, the modeler specifies not a (directed) graph, but a (directed) hypergraph. ", "page_idx": 1}, {"type": "text", "text": "If we were interested in a concrete probabilistic model, we would also need to annotate this hypergraph with quantitative information describing the mechanisms. For directed acyclic graphs, there are two standard approaches: supply conditional probability distributions (cpds) to get a BN, or supply equations to get a causal model. Correspondingly, there are two approaches to probabilistic modeling based on hypergraphs. The analogue of the first approach\u2014supplying a probability $P(T|S)$ for each mechanism\u2014leads to the notion of a probabilistic dependency graph $(P D G)$ [23, 22, 24]. The analogue of the second approach\u2014supplying an equation describing $T$ as a function of $S$ and independent random noise\u2014leads to a novel generalization of a causal model (Definition 4). Models of either kind are of interest to us only insofar as they explain how hypergraphs encode qualitative aspects of probability. Qualitative information in a PDG was characterized by Richardson and Halpern [23] using a scoring function that, despite having some attractive properties, lacks justification and has not been fully understood. In particular, the PDG formalism does not appear to answer a basic question: what does it mean for a distribution to be compatible with a directed hypergraph structure? ", "page_idx": 1}, {"type": "text", "text": "We develop precisely such a notion (Definition 2) of compatibility between a distribution $\\mu$ and a directed hypergraph qualitatively representing a collection of independent mechanisms\u2014or, for short, simply (QIM-)compatibility. This definition allows us to use directed hypergraphs as a language for specifying structure in probability distributions, of which the semantics of qualitative BNs are a special case (Theorem 1). Yet QIM-compatibility can do far more than represent conditional independencies in acyclic networks. For one thing, it can encode arbitrary functional dependencies (Theorem 2); for another, it gives meaningful semantics to cyclic models. Indeed, compatibility lets us go well beyond capturing dependence and independence. The fact that Pearl [18] views causal models as representing independent mechanisms suggests that there might be a connection to causality. In fact, there is. A witness that a distribution $\\mu$ is compatible with a hypergraph $\\boldsymbol{\\mathcal{A}}$ is an extended distribution $\\bar{\\mu}$ that is nearly equivalent to (and guarantees the existence of) a causal model that explains $\\mu$ with dependency structure $\\boldsymbol{\\mathcal{A}}$ (Propositions 3 to 5). As we shall see, thinking in terms of witnesses and compatibility allows us to tie together causality, dependence, and independence. ", "page_idx": 1}, {"type": "text", "text": "Perhaps surprisingly, compatibility also has deep connections with information theory (Section 4). The conditional independencies of a BN can be viewed as a certain kind of information-theoretic constraint. Our notion of compatibility with a hypergraph $\\boldsymbol{\\mathcal{A}}$ turns out to imply a generalization of this constraint (closely related to the qualitative PDG scoring function) that is meaningful for all hypergraphs (Theorem 7). Applied to cyclic models, it yields a causally inspired notion of pairwise interaction that clarifies some important misunderstandings in information theory (Examples 5 and 6). ", "page_idx": 1}, {"type": "text", "text": "Saying that one approach to qualitative graphical modeling has connections to so many different notions is a rather bold claim. We spend the rest of the paper justifying it. ", "page_idx": 1}, {"type": "text", "text": "2 Qualitative Independent-Mechanism (QIM) Compatibility ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we present the central definition of our paper: a way of making precise Pearl\u2019s notion of \u201cindependent mechanisms\u201d, used to motivate Bayesian Networks from a causal perspective. Pearl [19, p.22] states that \u201ceach parent-child relationship in a causal Bayesian network represents a stable and autonomous physical mechanism.\u201d But, technically speaking, a parent-child relationship only partially describes the mechanism. Instead, the autonomous mechanism that determines the child is really represented by that child\u2019s joint relationship with all its parents. So, the qualitative aspect of a mechanism is best represented as a directed hyperarc [5], that can have multiple sources. ", "page_idx": 1}, {"type": "text", "text": "Definition 1. A directed hypergraph (or simply a hypergraph, since all our hypergraphs will be directed) consists of a set $\\mathcal{N}$ of nodes and a set $\\boldsymbol{\\mathcal{A}}$ of directed hyperedges, or hyperarcs; each hyperarc $a\\in A$ is associated with a set $S_{a}\\subseteq N$ of source nodes and a set $T_{a}\\subseteq{\\mathcal{N}}$ of target nodes. We write $S{\\stackrel{a_{\\vee}}{\\to}}T\\in{\\mathcal{A}}$ to specify a hyperarc $a\\in A$ together with its sources $S=S_{a}$ and targets $T=T_{a}$ . Nodes that are neither a source nor a target of any hyperarc will seldom have any effect on our constructions; the other nodes can be recovered from the hyperarcs (by selecting $\\textstyle N:=\\bigcup_{a\\in{\\cal A}}S_{a}\\cup T_{a})$ . Thus, we often leave $\\mathcal{N}$ implicit, referring to the hypergraph simply as $\\boldsymbol{\\mathcal{A}}$ . \u53e3 ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Following the graphical models literature, we are interested in hypergraphs whose nodes represent variables, so that each $X\\in{\\mathcal{N}}$ will ultimately be associated with a (for simplicity, finite) set $\\operatorname{V}(X)$ of possible values. However, one should not think of $\\mathrm{V}$ as part of the information carried by the hypergraph. It makes perfect sense to say that $X$ and $Y$ are independent without specifying the possible values of $X$ and $Y$ . Of course, when we talk concretely about a distribution $\\mu$ on a set of variables $\\boldsymbol{\\mathcal{X}}\\cong(\\mathcal{N},\\mathrm{V})$ , those variables must have possible values\u2014but the qualitative properties of $\\mu$ , such as independence, can be expressed purely in terms of $\\mathcal{N}$ , without reference to $\\mathrm{V}$ . ", "page_idx": 2}, {"type": "text", "text": "Intuitively, we expect a joint distribution $\\mu(\\mathcal{X})$ to be qualitatively compatible with a set of independent mechanisms (whose structure is given by a hypergraph $\\mathcal{A}$ ) if there is a mechanistic explanation of how each target arises as a function of the variable(s) on which it depends and independent random noise. This is made precise by the following definition. ", "page_idx": 2}, {"type": "text", "text": "Definition 2 (QIM-compatibility). Let $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ be (possibly identical) sets of variables, and $A=$ $\\{S_{a}{\\stackrel{a}{\\to}}T_{a}\\}_{a\\in\\mathcal{A}}$ be a hypergraph with nodes $\\mathcal{X}$ . We say a distribution $\\mu(\\mathcal{V})$ is qualitatively independentmechanism compatible, or (QIM-)compatible, with $\\boldsymbol{\\mathcal{A}}$ (symbolically: $\\mu\\vDash\\diamond\\mathcal{A})$ iff there exists an extended distribution $\\bar{\\mu}(\\mathcal{V}\\cup\\mathcal{X}\\cup\\mathcal{U}_{A})$ of $\\mu(\\mathcal{V})$ to $\\mathcal{X}$ and to $\\mathcal{U}_{A}=\\{U_{a}\\}_{a\\in\\mathcal{A}}$ , an additional set of \u201cnoise\u201d variables (one variable per hyperarc) according to which: ", "page_idx": 2}, {"type": "text", "text": "(a) the variables $\\boldsymbol{\\wp}$ are distributed according to $\\mu$ (i.e., $\\bar{\\mu}(\\mathcal{D})=\\mu(\\mathcal{D}))$ , (b) the variables $\\mathcal{U}_{A}$ are mutually independent (i.e., $\\begin{array}{r}{\\bar{\\mu}(\\mathcal{U}_{A})=\\prod_{a\\in\\mathcal{A}}\\bar{\\mu}(U_{a})\\;)}\\end{array}$ , and (c) the target variable(s) $T_{a}$ of each hyperarc $a\\in A$ are determined by $U_{a}$ and the source variable(s) $S_{a}$ $({\\mathrm{i.e.,}}\\,\\forall a\\in{\\mathcal{A}}.\\;{\\bar{\\mu}}\\,\\vert=(S_{a},U_{a})\\to T_{a})$ ", "page_idx": 2}, {"type": "text", "text": "We call such a distribution $\\bar{\\mu}(\\mathcal{X}\\cup\\mathcal{Y}\\cup\\mathcal{U}_{A})$ a witness that $\\mu$ is QIM-compatible with $\\boldsymbol{\\mathcal{A}}$ . ", "page_idx": 2}, {"type": "text", "text": "While Definition 2 requires the noise variables $\\{U_{a}\\}_{a\\in A}$ to be independent of one another, note that they need not be independent of any variables in $\\mathcal{X}$ . In particular, $U_{a}$ may not be independent of $S_{a}$ , and so the situation can diverge from what one would expect from a randomized algorithm, whose randomness $U$ is assumed to be independent of its input $S$ . Furthermore, the variables in $\\boldsymbol{\\mathcal{U}}$ may not be independent of one another conditional on the value of some $X\\in\\mathcal{X}$ . ", "page_idx": 2}, {"type": "text", "text": "Example 1. $\\mu(X,Y)$ is compatible with ${\\mathcal{A}}=\\{\\emptyset{\\overset{1}{\\to}}\\{X\\},\\emptyset{\\overset{2}{\\to}}\\{Y\\}\\}$ (depicted in PDG notation as $\\neg\\alpha\\int Y\\dot{\\leftarrow}$ ) iff $X$ and $Y$ are independent, i.e., $\\mu(X,Y)=\\mu(X)\\mu(Y)$ . For if $U_{1}$ and $U_{2}$ are independent and respectively determine $X$ and $Y$ , then $X$ and $Y$ must also be independent. \u25b3 ", "page_idx": 2}, {"type": "text", "text": "This is a simple illustration of a more general phenomenon: when $\\boldsymbol{\\mathcal{A}}$ describes the structure of a Bayesian Network (BN), then QIM-compatibility with $\\boldsymbol{\\mathcal{A}}$ coincides with satisfying the independencies of that BN (which are given, equivalently, by the ordered Markov properties [14], factoring as a product of probability tables, or $d$ -separation [6]). To state the general result (Theorem 1), we must first clarify how the graphs of standard graphical and causal models give rise to directed hypergraphs. ", "page_idx": 2}, {"type": "text", "text": "Suppose that $G=(V,E)$ is a graph, whose edges may be directed or undirected. Given a vertex $u\\in V$ , write $\\mathbf{Pa}_{G}(u):=\\{v:(v,u)\\in E\\}$ for the set of vertices that can \u201cinfluence\u201d $u$ . There is a natural way to interpret the graph $G$ as giving rise to a set of mechanisms: one for each variable $u$ , which determines the value of $u$ based the values of the variables on which $u$ can depend. Formally, let $A_{G}:=\\left\\{\\mathbf{\\partial}\\mathbf{P}\\mathbf{a}_{G}(u)\\stackrel{u}{\\to}\\{u\\}\\right\\}_{u\\in V}$ be the hypergraph corresponding to the graph $G$ . ", "page_idx": 2}, {"type": "text", "text": "Theorem 1. If $G$ is a directed acyclic graph and ${\\mathcal{T}}(G)$ consists of the independencies of its corresponding Bayesian network, then $\\mu=\\diamond A_{G}$ if and only $i f\\mu$ satisfies ${\\mathcal{T}}(G)$ . ", "page_idx": 2}, {"type": "text", "text": "Theorem 1 shows, for hypergraphs that correspond to directed acyclic graphs (dags), our definition of compatibility reduces exactly to the well-understood independencies of BNs. This means that QIMcompatibility, a notion based on the independence of causal mechanisms, gives us a very different way of characterizing these independencies\u2014one that can be generalized to a much larger class of graphical models that includes, for example, cyclic variants [1]. Moreover, QIM-compatibility can capture properties other than independence. As the next example shows, it can capture determinism. ", "page_idx": 2}, {"type": "text", "text": "Example 2. If $\\begin{array}{r}{\\mathcal{A}=\\frac{1}{\\mathcal{\\lambda}\\left(\\underline{{X}}\\right)}\\mathcal{\\ell}^{2}}\\end{array}$ consists of just two hyperarcs pointing to a single variable $X$ , then a distribution $\\mu(X)$ is QIM-compatible with $\\boldsymbol{\\mathcal{A}}$ iff $\\mu$ places all mass on a single value $x\\in\\mathrm{V}(X)$ . $\\triangle$ ", "page_idx": 2}, {"type": "text", "text": "Intuitively, if two independent coins always give the same answer (the value of $X$ ), then neither coin can be random. This simple example shows that we can capture determinism with multiple hyperarcs pointing to the same variable. Such hypergraphs do not correspond to graphs; recall that in a BN, two arrows pointing to $X$ (e.g., $Y\\rightarrow X$ and $Z\\to X$ ) represent a single mechanism by which $X$ is jointly determined (by $Y$ and $Z$ ), rather than two distinct mechanisms. ", "page_idx": 3}, {"type": "text", "text": "Given a hypergraph $\\mathcal{A}=(\\mathcal{N},\\mathcal{A}),\\,\\boldsymbol{X},\\boldsymbol{Y}\\subseteq\\mathcal{N}$ , and a natural number $n\\geq0$ , let ${\\mathcal{A}}\\sqcup_{X\\to Y}^{(+n)}$ denote the hypergraph that results from augmenting $\\boldsymbol{\\mathcal{A}}$ with $n$ additional (distinct) hyperarcs from $X$ to $Y$ . ", "page_idx": 3}, {"type": "text", "text": "Theorem 2. (a) \u00b5 |= X\u2192\u2192Y \u2227\u2662A if and only if \u2200n \u22650. \u00b5 |= \u2662A \u2294X(+\u2192nY) (b) if A = AG for a dag G, then \u00b5 |= X\u2192\u2192Y \u2227\u2662A if and only if \u00b5 |= \u2662A \u2294X(+\u21921)Y . (c) if \u2203a \u2208A such that Sa = \u2205and X \u2208Ta, then \u00b5 |= X\u2192\u2192Y \u2227\u2662A iff \u00b5 |= \u2662A \u2294X(+\u21922)Y . ", "page_idx": 3}, {"type": "text", "text": "Based on the intuition given after Example 2, it may seem unnecessary to ever add more than two parallel hyperarcs to ensure functional dependence in part (a). However, this intuition implicitly assumes that the randomness $U_{1}$ and $U_{2}$ of the two mechanisms is independent conditional on $X$ , which may not be the case. See Appendix D for counterexamples. ", "page_idx": 3}, {"type": "text", "text": "Finally, as mentioned above, QIM-compatibility gives meaning to cyclic structures, a topic that we will revisit often in Sections 3 and 4. We start with a simple example. ", "page_idx": 3}, {"type": "text", "text": "Example 3. Every $\\mu(X,Y)$ is compatible with $(X)\\not\\geq\\left(Y\\right)$ , because every distribution is compatible with $\\to\\!{\\underline{{x}}})\\!\\to\\!{\\underline{{Y}}}).$ , and a mechanism with no inputs is a special case of one that can depend on $Y$ . $\\triangle$ ", "page_idx": 3}, {"type": "text", "text": "The logic above is an instance of an important reasoning principle, which we develop in Appendix B. Although the 2-cycle in Example 3 is straightforward, generalizing it even slightly to a 3-cycle raises a not-so-straightforward question, whose answer will turn out to have surprisingly broad implications. ", "page_idx": 3}, {"type": "text", "text": "Example 4. What $\\mu(X,Y,Z)$ are compatible with the 3-cycle shown, on the right? By the reasoning above, among them must be all distributions consistent with a linear chain ${\\to}X{\\to}Y{\\to}Z$ . Thus, any distribution in which two variables are conditionally independent given the third is compatible with the 3-cycle. Are there distributions that are not compatible with this hypergraph? It is not obvious. We return to this in Section 4. $\\triangle$ ", "page_idx": 3}, {"type": "image", "img_path": "RE5LSV8QYH/tmp/4eadfc0bc20d8846ee12de5ced7d2e3533c7217c9bab7bad08b7cf0a25b0ab84.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Because QIM-compatibility applies to cyclic structures, one might wonder if it also captures the independencies of undirected models. Our definition of $A_{G}$ , as is common, implicitly identifies a undirected edge $A\\!-\\!B$ with the pair $\\{A\\!\\to\\!B,B\\!\\to\\!A\\}$ of directed edges; in this way, it naturally converts even an undirected graph $G$ to a (directed) hypergraph. Compatibility with $A_{G}$ , however, does not coincide with any of the standard Markov properties corresponding to $G$ [13]. This may appear to be a flaw in Definition 2, but it is unavoidable (see Appendix B) if we wish to also capture causality, as we do in the next section. ", "page_idx": 3}, {"type": "text", "text": "3 QIM-Compatibility and Causality ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Recall that in the definition of QIM-compatibility, each hyperarc represents an independent mechanism. Equations in a causal model are also viewed as representing independent mechanisms. This suggests a possible connection between the two formalisms, which we now explore. We will show that QIM-compatibility with $\\boldsymbol{\\mathcal{A}}$ means exactly that a distribution can be generated by a causal model with the corresponding dependency structure (Section 3.1). Moreover, such causal models and QIMcompatibility witnesses are themselves closely related (Section 3.2). In this section, we establish a causal grounding for QIM-compatibility. To do so, we must first review some standard definitions. ", "page_idx": 3}, {"type": "text", "text": "Definition 3 (Pearl [19]). A structural equations model (SEM) is a tuple $M=(\\mathcal{U},\\mathcal{V},\\mathcal{F})$ , where ", "page_idx": 3}, {"type": "text", "text": "\u2022 $\\boldsymbol{\\mathcal{U}}$ is a set of exogenous variables;   \n\u2022 $\\mathcal{V}$ is a set of endogenous variables (disjoint from $\\boldsymbol{\\mathcal{U}}$ );   \n\u2022 $\\mathcal{F}=\\{f_{Y}\\}_{Y\\in\\mathcal{V}}$ associates to each endogenous variable $Y$ an equation $f_{Y}:\\operatorname{V}({\\mathcal{U}}\\cup\\mathcal{V}\\!-\\!Y)\\to\\operatorname{V}(Y)$ that determines its value as a function of the other variables. \u53e3 ", "page_idx": 3}, {"type": "text", "text": "In a SEM $M$ , a variable $X\\in\\mathcal{V}$ does not depend on $Y\\in\\mathcal{V}\\cup\\mathcal{U}$ if $f_{X}(\\dots,y,\\dots)=f_{X}(\\dots,y^{\\prime},\\dots)$ for all $y,y^{\\prime}\\in\\mathrm{V}(Y)$ . Let the parents $\\mathbf{Pa}_{M}(X)$ of $X$ be the set of variables on which $X$ depends. $M$ is acyclic iff $\\mathbf{P}\\mathbf{a}_{M}(X)\\cap\\mathcal{V}=\\mathbf{P}\\mathbf{a}_{G}(X)$ for some dag $G$ with vertices $\\mathcal{V}$ . In an acyclic SEM, it is easy to see that a setting of the exogneous variables determines the values of the endogenous variables (symbolically: $M\\vDash\\mathcal{U}\\nrightarrow\\mathcal{V}$ ). A probabilistic SEM (PSEM) $\\mathcal{M}=(M,P)$ is a SEM, together with a probability $P$ over the exogenous variables. When $\\mathcal{M}\\vDash\\mathcal{U}\\vDash\\mathcal{V}$ , the distribution $\\boldsymbol{P}(\\boldsymbol{U})$ extends uniquely to a distribution over $\\mathrm{V}(\\upnu\\cup\\mathcal{U})$ . A cylic PSEM, however, may induce more than one such distribution, or none at all. In general, a PSEM $\\mathcal{M}$ induces a (possiby empty) convex set of distributions over $\\mathrm{V}(\\mathcal{U}\\cup\\mathcal{V})$ . This set is defined by two (linear) constraints: the equations $\\mathcal{F}$ must hold with probability 1, and the marginal probability over $\\boldsymbol{\\mathcal{U}}$ must equal $P$ . Given a PSEM $\\mathcal{M}$ , let $\\mathbb{\\{}\\mathcal{M}\\nparallel$ consist of all joint distributions $\\nu(\\mathcal{U},\\mathcal{V})$ that satisfy the two constraints above; this set captures the behavior of $\\mathcal{M}$ in the absence of interventions. A joint distribution $\\mu({\\mathbf X})$ over $\\mathbf{X}\\subseteq\\mathcal{V}\\cup\\mathcal{U}$ can arise from a (P)SEM $\\mathcal{M}$ iff there is some $\\nu\\in\\{\\mathbb{M}\\}$ whose marginal on $\\mathbf{X}$ is $\\mu$ . ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "We now review the syntax of a language for describing causality. A basic causal formula is one of the form $[\\mathbf{Y}\\{-\\mathbf{y}\\}\\varphi$ , where $\\varphi$ is a Boolean expression over the endogenous variables $\\nu$ , $\\mathbf{Y}\\subseteq\\mathcal{V}$ is a subset of them, and $\\mathbf{y}\\in\\mathrm{V}(\\mathbf{Y})$ . The language then consists of all Boolean combinations of basic formulas. In a causal model $M$ and context $\\mathbf{u}\\in\\mathrm{V}(\\mathcal{U})$ , a Boolean expression $\\varphi$ over $\\nu$ is true iff it holds for all $(\\mathbf{u},\\mathbf{x})\\in\\mathrm{V}(\\mathcal{U},\\mathcal{V})$ consistent with the equations of $M$ . Basic causal formulas are then given semantics by $(M,\\mathbf{u})\\vDash[\\mathbf{Y}\\{-\\mathbf{y}\\}\\varphi$ iff $(M_{\\mathbf{Y}\\leftarrow\\mathbf{y}},\\mathbf{u})\\models\\varphi$ , where $M_{\\mathbf{Y}\\leftarrow\\mathbf{y}}$ is the result of changing each $f_{Y}$ , for $Y\\in\\mathbf{Y}$ , to the constant function $\\mathbf s\\mapsto\\mathbf y[Y]$ , which returns (on all inputs s) the value of $Y$ in the joint setting y. The dual formula $\\langle\\mathbf{Y}{\\leftarrow}\\mathbf{y}\\rangle\\varphi:={\\mathsf{\\neg}}[\\mathbf{Y}{\\leftarrow}\\mathbf{y}]{\\backprime}\\varphi$ is equivalent to $[\\mathbf{Y}\\{-\\mathbf{y}\\}\\varphi$ in SEMs where each context u induces a unique setting of the endogenous variables [7]. A PSEM $\\mathcal{M}=(M,P)$ assigns probabilities to causal formulas according to ${\\operatorname*{Pr}}_{M}(\\varphi):=P(\\{{\\mathbf{u}}\\in{\\operatorname{V}}({\\mathcal{U}}):(M,{\\mathbf{u}})\\mid=\\varphi\\})$ ). ", "page_idx": 4}, {"type": "text", "text": "Some authors assume that for each variable $X$ , there is a special \u201cindependent noise\u201d exogenous variable $U_{X}$ on which only the equation $f_{X}$ can depend; we call a PSEM $(M,P)$ randomized if it contains such exogenous variables that are mutually independent according to $P$ , and fully randomized if all its exogenous variables are of this form. Randomized PSEMs are clearly a special class of PSEMs, but note also that every PSEM can be converted to an equivalent randomized PSEM by extending it with additional dummy variables $\\{U_{X}\\}_{X\\in\\mathcal{V}}$ that can take only a single value. Thus, we do not lose expressive power by using randomized PSEMs. In fact, qualitatively, randomized PSEMs are more expressive: they can encode independence. ", "page_idx": 4}, {"type": "text", "text": "3.1 The Equivalence Between QIM-Compatibility and Randomized PSEMs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We are now equipped to formally describe the connection between QIM-compatibility and causality. At a high level, this connection should be unsurprising: witnesses and causal models both relate dependency structures to distributions, but in \u201copposite directions\u201d. QIM-compatibility starts with distributions and asks what dependency structures they are compatible with. Causal models, on the other hand, are explicit (quantitative) representations of dependency structures that give rise to sets of distributions. We now show that the existence of a causal model coincides with the existence of a witness. We start by showing this for the hypergraphs generated by graphs (like Bayesian networks, except possibly cyclic), which we show correspond to fully randomized causal models (Proposition 3). We then give a natural generalization of a causal model that exactly captures QIM-compatibility with an arbitrary hypergraph (Proposition 4). In both cases, the high-level result is the same: $\\mu\\vDash A$ iff there is a causal model that \u201chas dependency structure $\\mathcal{A}^{\\ast}$ that gives rise to $\\mu$ . ", "page_idx": 4}, {"type": "text", "text": "More precisely, a randomized causal model $\\mathcal{M}$ has dependency structure $\\boldsymbol{\\mathcal{A}}$ iff there is a 1-1 correspondence between $a\\in{\\mathcal{A}}$ and the equations of $\\mathcal{M}$ , such that the equation $f_{a}$ produces a value of $T_{a}$ and depends only on $S_{a}$ and $U_{a}$ . The definition above emphasizes the hypergraph; for readers interested in causality, here is an equivalent one that emphasizes the causal model: $\\mathcal{M}$ is of dependency structure $\\boldsymbol{\\mathcal{A}}$ iff the targets of $\\boldsymbol{\\mathcal{A}}$ are disjoint singletons corresponding to the elements of $\\nu$ (so $\\mathcal{A}=\\{\\dot{S}_{Y}\\rightarrow\\{Y\\}\\}_{Y\\in\\mathcal{V}})$ , and $\\mathbf{Pa}_{\\mathcal{M}}(Y)\\subseteq S_{Y}\\cup\\{U_{Y}\\}$ for all $Y\\in\\mathcal{V}$ . We start by presenting the result in the case where $\\boldsymbol{\\mathcal{A}}$ corresponds to a directed graph. ", "page_idx": 4}, {"type": "text", "text": "Proposition 3. Given a graph $G$ and a distribution \u00b5 $,\\mu\\vDash\\diamond\\mathcal{A}_{G}$ iff there exists a fully randomized PSEM of dependency structure $A_{G}$ from which $\\mu$ can arise. ", "page_idx": 4}, {"type": "text", "text": "In other words, compatibility with a hypergraph corresponding to a graph means arising from a fully randomized PSEM of the appropriate dependency structure. In light of this, Theorem 1 can be viewed as formalizing a phenomenon that seems to be almost universally implicitly understood: every acyclic fully randomized SEM induces a distribution with the independencies of the corresponding Bayesian Network. Conversely, every distribution with those independencies arises from such a causal model. Both halves have been recognized before. Druzdzel and Simon [4, Theorem 1] arguably establish one direction of the correspondence (turning a BN into a causal model), but their statement of the result obscures the possibility of a converse.2 Pearl\u2019s causal Markov condition [20, Theorem 1], on the other hand, is closely related to that converse (as will be made explicit by our Proposition 3). Yet, to the best of our knowledge, the two results have not before been combined and recognized as an equivalent characterization of a BN\u2019s conditional independencies. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Like before, QIM-compatibility allows us to go much futher. It is easy to extend Proposition 3 to the dependency structures of all randomized PSEMs. But what happens if $\\boldsymbol{\\mathcal{A}}$ contains hyperarcs with overlapping targets? Here the correspondence starts to break down for a simple reason: by definition, there is at most one equation per variable in a (P)SEM; thus, no PSEM can have dependency structure $\\boldsymbol{\\mathcal{A}}$ . Nevertheless, the correspondence between witnesses and causal models persists if we simply drop the (traditional) requirement that $\\mathcal{F}$ is indexed by $\\mathcal{V}$ . This leads us to consider a natural generalization of a (randomized) PSEM that has an arbitrary set of equations\u2014not just one per variable. ", "page_idx": 5}, {"type": "text", "text": "Definition 4. Let $({\\mathcal{N}},{\\mathcal{A}})$ be a hypergraph. A generalized randomized PSEM $\\mathcal{M}=(\\mathcal{X},\\mathcal{U},\\mathcal{F},P)$ with structure $\\boldsymbol{\\mathcal{A}}$ consists of sets of variables $\\mathcal{X}$ and $\\mathcal{U}=\\{U_{a}\\}_{a\\in\\mathcal{A}}$ , together with a set of functions $\\mathcal{F}\\!=\\!\\{f_{a}:\\mathrm{V}(S_{a})\\times\\mathrm{V}(U_{a})\\to\\mathrm{V}(T_{a})\\}_{a\\in A}$ , and a probability $P_{a}$ over each independent noise variable $U_{a}$ . The meanings of $\\mathbb{\\{}\\mathcal{M}\\nparallel$ and can arise are the same as for a PSEM. \u25a1 ", "page_idx": 5}, {"type": "text", "text": "Proposition 4. $\\mu\\vDash\\diamond\\mathcal{A}$ iff there exists a generalized randomized PSEM with structure $\\boldsymbol{\\mathcal{A}}$ from which $\\mu$ can arise. ", "page_idx": 5}, {"type": "text", "text": "Generalized randomized PSEMs can capture functional dependencies, and constraints. For instance, an equality (say $X=Y.$ ) can be encoded in a generalized randomized PSEM with a second equation for $X$ . Indeed, we believe that generalized randomized PSEMs can capture a wide class of constraints, and are closely related to causal models with constraints [2], a discussion we defer to future work. ", "page_idx": 5}, {"type": "text", "text": "3.2 Interventions and the Correspondence Between Witnesses and Causal Models ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We have seen that QIM-compatibility with $\\boldsymbol{\\mathcal{A}}$ (i.e., the existence of a witness $\\bar{\\mu}$ ) coincides exactly with the existence of a causal model $\\mathcal{M}$ from which a distribution can arise. But which witnesses correspond to which causal models? The answer to this question will be critical to extend the correspondence we have given so that it can deal with interventions. Different causal models may give rise to the same distribution, yet handle interventions differently. ", "page_idx": 5}, {"type": "text", "text": "There are two directions of the correspondence. Given a randomized PSEM $\\mathcal{M}$ , distributions arising from it are compatible with its dependency structure, and the corresponding witnesses are exactly the distributions in $\\mathbb{\\{}\\mathcal{M}\\nparallel$ (see Appendix E). In particular, if $\\mathcal{M}$ is acyclic, there is a unique witness. The converse is more interesting: how can we turn a witness into a causal model? ", "page_idx": 5}, {"type": "text", "text": "Construction 5. Given a witness $\\bar{\\mu}(\\mathcal{X})$ to compatibility with a hypergraph $\\boldsymbol{\\mathcal{A}}$ with disjoint targets, construct a PSEM according to the following (non-deterministic) procedure. Take $\\mathcal{V}:=\\cup_{a\\in\\mathcal{A}}T_{a}$ , $\\mathcal{U}:=\\mathcal{U}_{A}\\cup(\\mathcal{X}\\!-\\!\\mathcal{V})$ , and $P(\\bar{\\mathcal{U}}):=\\bar{\\mu}(\\mathcal{U})$ . For each $X\\in\\mathcal{V}$ , there is a unique $a_{X}\\in A$ whose targets $T_{a\\!x}$ contain $X$ . Since $\\bar{\\mu}\\,\\left|=(U_{a_{X}},S_{a_{X}}\\right)\\rightarrow T_{a_{X}}$ (this is just property (c) in Definition 2), $X\\in T_{a x}$ must also be a function of $S_{a_{X}}$ and $U_{a_{X}}$ ; take $f_{X}$ to be such a function. More precisely, for each $u\\in\\mathrm{V}(U_{a_{X}})$ and $\\mathbf{s}\\in\\mathrm{V}(S_{a_{X}})$ for which $\\bar{\\mu}(U_{a_{X}}\\!=\\!u,S_{a_{X}}\\!=\\!\\mathbf{s})>0$ , there is a unique $t\\in\\mathrm{V}(T_{a_{X}})$ such that $\\bar{\\mu}(u,{\\bf s},t)>0$ . In this case, set $f_{X}(u,\\mathbf{s},\\dots):=t[X]$ . If $\\bar{\\mu}(U_{a_{X}}\\!=\\!u,S_{a_{X}}\\!=\\!\\mathbf{s})=0$ , $f_{X}(u,{\\bf s},\\ldots)$ can be an arbitrary function of $u$ and s. Let $\\mathrm{PSEMs}_{A}(\\bar{\\mu})$ denote the set of PSEMs that can result. ", "page_idx": 5}, {"type": "text", "text": "It\u2019s clear from Construction 5 that $\\mathrm{PSEMs}_{A}(\\bar{\\mu})$ is always nonempty, and is a singleton iff $\\bar{\\mu}(u,s)>0$ for all $(a,u,s)\\,\\in\\,\\sqcup_{a\\in A}\\!\\operatorname{V}(U_{a},S_{a})$ . A witness with this property exists when $\\mu$ is positive (i.e., $\\mu({\\mathcal{X}}{=}{\\mathbf{\\hat{x}}})\\;>\\;0$ for all $\\textbf{x}\\in\\textrm{V}(\\boldsymbol{\\mathscr{X}}))$ , in which case the construction gives a unique causal model. Conversely, we have seen that an acylic model $\\mathcal{M}$ gives rise to a unique witness. So, in the simplest cases, models $\\mathcal{M}$ with structure $\\boldsymbol{\\mathcal{A}}$ and witnesses $\\bar{\\mu}$ to compatibility with $\\boldsymbol{\\mathcal{A}}$ are equivalent. But there are two important caveats. ", "page_idx": 5}, {"type": "text", "text": "1. A causal model $\\mathcal{M}$ can contain more information than a witness $\\bar{\\mu}$ if some events have probability zero. For instance, $\\bar{\\mu}$ could be a point mass on a single joint outcome $\\omega$ of all variables that satisfies the equations of $\\mathcal{M}$ . But $\\mathcal{M}$ cannot be reconstructed uniquely from $\\bar{\\mu}$ because there may be many causal models for which $\\omega$ is a solution. ", "page_idx": 5}, {"type": "text", "text": "2. A witness $\\bar{\\mu}$ can contain more information than a causal model $\\mathcal{M}$ if $\\mathcal{M}$ is cyclic. For example, suppose that $\\mathcal{M}$ consists of two variables, $X$ and $X^{\\prime}$ , and equations $f_{X}(X^{\\prime})=X^{\\prime}$ and $f_{X^{\\prime}}(X)=X$ . In this case, $\\bar{\\mu}$ cannot be reconstructed from $\\mathcal{M}$ , because $\\mathcal{M}$ does not contain information about the distribution of $X$ . ", "page_idx": 6}, {"type": "text", "text": "These two caveats appear to be very different, but they fit together in a surprisingly elegant way. ", "page_idx": 6}, {"type": "text", "text": "Proposition 5. If $\\bar{\\mu}(\\mathcal{X},\\mathcal{U}_{A})$ is a witness for QIM-compatibility with $\\boldsymbol{\\mathcal{A}}$ and $\\mathcal{M}$ is a PSEM with dependency structure $\\boldsymbol{\\mathcal{A}}$ , then $\\bar{\\mu}\\in\\{\\mathcal{M}\\}$ if and only if $\\mathcal{M}\\in\\mathrm{PSEMs}_{A}(\\bar{\\mu})$ . ", "page_idx": 6}, {"type": "text", "text": "Equivalently, this means that $\\mathrm{PSEMs}_{A}(\\bar{\\mu})$ , the possible outputs of Construction 5, are precisely the randomized PSEMs of dependency structure $\\boldsymbol{\\mathcal{A}}$ that can give rise to $\\bar{\\mu}$ . This is already substantial evidence that causal models $\\mathcal{M}\\in\\mathrm{PSEMs}_{A}(\\bar{\\mu})$ are closely related to the QIM-compatibility witness $\\bar{\\mu}$ . But everything we have seen so far describes only the correspondence in the absence of intervention, a setting in which many causal models are indistinguishable. Yet the correspondence goes deeper; it extends to interventions. This claim may seem dubious, as the obvous distinction between observing and doing is a fundemental principle of causality. What could a witness, which is purely probabilistic, have to say about intervention? In a randomized PSEM $\\mathcal{M}$ , we can define an event ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{do}_{\\mathcal{M}}(\\mathbf{X}{=}\\mathbf{x}):=\\bigcap_{X\\in\\mathbf{X}}\\bigcap_{\\mathbf{s}\\in\\mathrm{V}(\\mathbf{Pa}(X))}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "This is intuitively the event in which the randomness is such that ${\\bf X}={\\bf x}$ regardless of the values of the parent variables. As we now show, conditioning $\\bar{\\mu}$ on $\\mathrm{do}_{\\mathcal{M}}(\\mathbf{X}{=}\\mathbf{x})$ has the effect of intervention\u2014at least as long as the noise variables $\\mathcal{U}_{A}=\\{\\mathcal{U}_{a}\\}_{a\\in\\mathcal{A}}$ are independent of the other exogenous variables $\\mathcal{U}\\setminus\\mathcal{U}_{A}$ in addition to one another (e.g., when $\\mathcal{M}$ is fully randomized). ", "page_idx": 6}, {"type": "text", "text": "Theorem 6. Suppose that $\\bar{\\mu}$ is a QIM witness for $\\boldsymbol{\\mathcal{A}}$ , that $\\mathcal{M}=(\\mathcal{U},\\mathcal{V},\\mathcal{F},P)\\in\\mathrm{PSEMs}_{A}(\\bar{\\mu})$ is $a$ corresponding PSEM, and that the noise variables $\\mathcal{U}_{A}=\\{U_{X}\\}_{X\\in\\mathcal{V}}$ are independent of the other exogenous variables $\\mathcal{U}\\setminus\\mathcal{U}_{A}$ . For all $\\mathbf{X}\\subseteq\\mathcal{V}$ and $\\mathbf{x}\\in\\mathrm{V}(\\mathbf{X})$ , if $\\dot{\\bar{\\mu}}(\\mathrm{do}_{\\mathcal{M}}(\\mathbf{X}{=}\\mathbf{x}))>0,$ , then ", "page_idx": 6}, {"type": "text", "text": "Theorem 6 shows that the relationship between witnesses and causal models extends to interventions. Even when $\\mathrm{do}_{\\mathcal{M}}(\\mathbf{X}{=}\\mathbf{x})$ has probability zero, it is always possible to find a nearly equivalent setting where the bounds of the theorem apply.3Intervention and conditioning are conceptually very different, so it may seem surprising that conditioning can have the effect of intervention (and also that the Pearl\u2019s $\\operatorname{io}(\\,\\cdot\\,)$ notation actually corresponds to an event [9]). We emphasize that the conditioning (on $\\mathrm{do}_{\\mathcal{M}}(\\mathbf{X}{=}\\mathbf{x}))$ is on the randomness $\\{U_{X}\\}_{X\\in\\mathbf{X}}$ and not $\\mathbf{X}$ itself; intervening on ${\\bf X}\\mathrm{=}{\\bf x}$ is indeed fundamentally different from conditioning on ${\\bf X}\\mathrm{=}{\\bf x}$ . ", "page_idx": 6}, {"type": "text", "text": "4 QIM-Compatibility and Information Theory ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The fact that the dependency structure of a (causal) Bayesian network describes the independencies of the distribution it induces is fundamental to both causality and probability. It makes explicit the distributional consequences of BN structure. Yet, despite substantial interest [1], generalizing the BN case to more complex (e.g., cyclic) dependency structures remains largely an open problem. In Section 4.1, we generalize the BN case by providing an information-theoretic constraint, capable of capturing conditional independence, functional dependence, and more, on the distributions that can arise from an arbitrary dependency structure. This connection between causality and information theory has implications for both fields. It grounds the cyclic dependency structures found in causality in concrete constraints on the distributions they represent. At the same time, it allows us to resolve longstanding confusion about structure in information theory, clarifying the meaning of the so-called \u201cinteraction information\u201d, and recasting a standard counterexample to substantiate the claim it was intended to oppose. In Section 4.2, we strengthen this connection. Using entropy to measure distance to (in)dependence, we develop a scoring function to measure how far a distribution is from being QIMcompatible with a given dependency structure. This function turns out to have an intimate relationship with the qualitative PDG scoring fucntion $I D e f$ , which we use to show that our information-theoretic constraints degrade gracefully on \u201cnear-compatible\u201d distributions. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "We now review the critical information theoretic concepts and their relationships to (in)dependence (see Appendix C.1 for a full primer). Conditional entropy $\\bar{\\mathrm{H}_{\\mu}}(Y|X)$ measures how far $\\mu$ is from satisfying the functional dependency $X\\rightarrow Y$ . Conditional mutual information $\\operatorname{I}_{\\mu}(Y;Z|X)$ measures how far $\\mu$ is from satisfying the conditional independence $Y\\perp\\!\\!\\!\\perp Z\\mid X$ . Linear combinations of these quantities (for $X,Y,Z\\subseteq\\mathcal{X})$ can be viewed as the inner product between a coefficient vector $\\mathbf{v}$ and a $2^{|\\mathcal{X}|}-1$ dimensional vector ${\\mathbf{I}}_{\\mu}$ that we will call the information profile of $\\mu$ . For three variables, the components of this vector are illustrated in Figure 1 (right). It is not hard to see that an arbitrary conjunction of (conditional) (in)dependencies can be expressed as a constraint $\\mathbf{I}_{\\mu}\\cdot\\mathbf{v}\\geq0$ , for some appropriate choice of vector $\\mathbf{v}$ . ", "page_idx": 7}, {"type": "image", "img_path": "RE5LSV8QYH/tmp/1b17f94c47185870bd0634ec8064458896a5ac3d0921da86a617d4bc119faabf.jpg", "img_caption": ["Figure 1: I\u00b5. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "We now formally introduce the qualitative PDG scoring function $I D e f$ , which interprets a hypergraph structure $\\boldsymbol{\\mathcal{A}}$ as a function of the form $\\mathbf{I}_{\\mu}\\cdot\\mathbf{v}_{A}$ . This information deficiency, given by ", "page_idx": 7}, {"type": "equation", "text": "$$\nI D e f_{A}(\\mu)=\\mathbf{I}_{\\mu}\\cdot\\mathbf{v}_{A}:=-\\;\\mathrm{H}_{\\mu}(\\mathcal{X})+\\sum_{a\\in\\mathcal{A}}\\mathrm{H}_{\\mu}(T_{a}\\mid S_{a}),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "is the difference between the number of bits needed to (independently) specify the randomness in $\\mu$ along the hyperarcs of $\\boldsymbol{\\mathcal{A}}$ , and the number of bits needed to specify a sample of $\\mu$ according to its own structure $\\!\\leftmoon\\rightleftharpoons\\chi)$ ). While $I D e f$ has some nice properties4, it can also behave unintuitively in some cases; for instance, it can be negative. Clearly, it does not measure how close $\\mu$ is to being structurally compatible with $\\boldsymbol{\\mathcal{A}}$ , in general. Nevertheless, there is still a fundamental relationship between $I D e f$ and QIM-compatibility, as we now show. ", "page_idx": 7}, {"type": "text", "text": "4.1 A Necessary Condition for QIM-Compatibility ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "What constraints does QIM-compatibility with $\\boldsymbol{\\mathcal{A}}$ place on a distribution $\\mu?$ When $G$ is a dag, we have seen that if $\\mu=\\diamond A_{G}$ , then $\\mu$ must satisfy the independencies of the corresponding Bayesian network (Theorem 1); we have also seen that additional hyperarcs impose functional dependencies (Theorem 2). But these results apply only when $\\boldsymbol{\\mathcal{A}}$ is of a very special form. More generally, $\\mu\\vDash\\diamond\\mathcal{A}$ implies that $\\mu$ can arise from some randomized causal model whose equations have dependency structure $\\boldsymbol{\\mathcal{A}}$ (Propositions 3 and 4). Still, unless $\\boldsymbol{\\mathcal{A}}$ has a particularly special form, it is not obvious whether or not this says something about $\\mu$ . The primary result of this section is an informationtheoretic bound (Theorem 7) that generalizes most of the concrete consequences of QIM-compatibility we have seen so far (Theorems 1 and 2). The result is a connection between information theory and causality; it yields an information-theoretic test for complex causal dependency structures, and enables causal notions of structure to dispel misconceptions in information theory. ", "page_idx": 7}, {"type": "text", "text": "Theorem 7. If $\\mu\\vDash\\diamond\\triangleleft,$ , then $I D e f_{A}(\\mu)\\leq0$ . ", "page_idx": 7}, {"type": "text", "text": "Theorem 7 applies to all hypergraphs, and subsumes every general-purpose technique we know of for proving that $\\mu\\models\\langle\\ A$ . Indeed, the negative directions of Theorems 1 and 2 are immediate consequences of it. To illustrate some of its subtler implications, we return to the 3-cycle in Example 4. ", "page_idx": 7}, {"type": "text", "text": "Example 5. It is easy to see (e.g., by inspecting Figure 1) that $I D e f_{3-\\mathrm{cycle}}(\\mu)\\,=$ $\\mathrm{H}_{\\mu}(Y|X)+\\mathrm{H}_{\\mu}(Z|Y)+\\mathrm{H}_{\\mu}(X|Z)\\,-\\,\\mathrm{H}_{\\mu}(X Y Z)\\,=\\,-\\,\\mathrm{I}_{\\mu}(X;Y;Z)$ . Theorem 7 therefore tells us that a distribution $\\mu$ that is QIM-compatible with the 3-cycle cannot have negative interaction information $\\operatorname{I}_{\\mu}(X;Y;Z)$ . What does this mean? When $\\operatorname{I}(X;Y;{\\bar{Z}})\\,<\\,0$ , conditioning on one variable causes the other two to share more information than they did before. The most extreme instance is $\\mu_{x o r}$ , the distribution in which two variables are independent and the third is their parity (illustrated on the right). It seems intuitively clear that $\\mu_{x o r}$ cannot arise from the 3-cycle, a causal model with only pairwise dependencies. This is difficult to prove directly, but is an immediate consequence of Theorem 7. $\\triangle$ ", "page_idx": 7}, {"type": "image", "img_path": "RE5LSV8QYH/tmp/50229f23cada9fd855461d827162895bb8628ec1334a48964602b1d08e9cdd9b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "For many, there is an intuition that $\\operatorname{I}(X;Y;Z)<0$ should require a fundementally \u201c3-way\u201d interaction between the variables, and should not arise through pairwise interactions alone [10]. This has been a source of conflict [26, 16, 15, 3], because traditional ways of making precise \u201cpairwise interactions\u201d (e.g., maximum entropy subject to pairwise marginal constraints and pairwise factorization) do not ensure that $\\operatorname{I}(X;Y;Z)\\geq0$ . But QIM-compatibility does. One can verify by enumeration that the 3-cycle is the most expressive causal structure with no joint dependencies, and we have already proven that QIM-compatibility with that hypergraph implies non-negative interaction information. QIM-compatibility has another even more noteworthy clarifying effect on information theory. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "There is a school of thought that contends that all structural information in $\\mu(\\mathcal{X})$ is captured by its information profile ${\\mathbf{I}}_{\\mu}$ . This position has fallen out of favor in some communities due to standard counterexamples: distributions that have intuitively different structures yet share an information proflie [11]. However, with \u201cstructure\u201d explicated by compatibility, the prototypical counterexample of this kind suddenly supports the very notion it was meant to challenge, suggesting in an unexpected way that the information profile may yet capture the essence of probabilistic structure. ", "page_idx": 8}, {"type": "text", "text": "Example 6. Let $A,B$ , and $C$ be variables with $\\mathrm{V}(A),\\mathrm{V}(B),\\mathrm{V}(C)=\\{0,1\\}^{2}$ . Using independent fair coin flips $X_{1}$ , $X_{2}$ , and $X_{3}$ , define two joint distributions, $P$ and $Q$ , over $A,B,C$ as follows. Define $P$ by selecting $A:=(X_{1},X_{2})$ , $B:=(X_{2},X_{3})$ , and $C:=(X_{3},X_{1})$ . Define $Q$ by selecting $A:=(X_{1},X_{2})$ , $B:=(X_{1},X_{3})$ , and $C:=(X_{1},X_{2}\\oplus X_{3})$ . Structurally, $P$ and $Q$ appear to be very different. According to $P$ , the first components of the three variables $(A,B,C)$ are independent, yet they are identical according to $Q$ . Moreover, $P$ has only simple pairwise interactions between the variables, while $Q$ has $\\mu_{x o r}$ (a clear 3-way interaction) embedded within it. Yet $P$ and $Q$ have identical information profiles (see right): in both cases, each of $\\{A,B,C\\}$ is determined by the values of the other two, each pair share one bit of information given the third, and $\\operatorname{I}(A;B;C)=0$ . ", "page_idx": 8}, {"type": "image", "img_path": "RE5LSV8QYH/tmp/d897bfb5bbdb2ede07a1f28ed1f1f7991da83bfc7bc2516a8aa506793ff901e0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "This example has been used to argue that multivariate Shannon information does not take into account important structural differences between distributions [11]. We are now in a position to give a novel and particularly persuasive response, by appealing to QIM-compatibility. Unsurprisingly, $P$ is compatible with the 3-cycle; it is clearly consists of \u201c2-way\u201d interactions, as each pair of variables shares a bit. But, counterintuitively, the distribution $Q$ is also compatible with the 3-cycle! (The reader is encouraged to verify that $U_{1}=X_{3}\\oplus X_{1}$ , $U_{2}=X_{2}$ , and $U_{3}=X_{3}$ serves as a witness.) To emphasize: this is despite the fact that $Q$ is just $\\mu_{x o r}$ (which is certainly not compatible with the 3-cycle) together with a seemingly irrelevant random bit $X_{1}$ . By the results of Section 3, this means there is a causal model without joint dependence giving rise to $Q_{\\mathrm{~\\,~}}$ \u2014so, despite appearances, $Q$ does not require a 3-way interaction. Indeed, $P$ and $Q$ are QIM-compatible with precisely the same hypergraphs over $\\{A,B,C\\}$ , suggesting that they don\u2019t have a structural difference after all. $\\triangle$ ", "page_idx": 8}, {"type": "text", "text": "In light of Example 6, one might reasonably conjecture that the converse of Theorem 7 holds. Unfortunately, it does not (see Appendix C.3); the quantity $I D e f_{A}(\\mu)$ does not completely determine whether or not $\\mu=\\diamond{A}$ . We now pursue a new (entropy-based) scoring function that does. This will allow us to generalize Theorem 7 to distributions that are only \u201cnear-compatible\u201d with $\\boldsymbol{\\mathcal{A}}$ . ", "page_idx": 8}, {"type": "text", "text": "4.2 A Scoring Function for QIM-Compatibility ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Here is a function that measures how far a distribution $\\mu$ is from being QIM-compatible with $\\boldsymbol{\\mathcal{A}}$ ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathrm{QIM}I n c_{A}(\\mu):=\\operatorname*{inf}_{\\nu({\\mathbb{Z}})=\\mu(X)}-\\mathrm{H}_{\\nu}({\\mathbb{Z}})+\\sum_{a\\in A}\\mathrm{H}_{\\nu}(U_{a})+\\sum_{a\\in A}\\mathrm{H}_{\\nu}(T_{a}|S_{a},U_{a}).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "QIMInc is a direct translation of Definition 2 (a-c); it measures the (optimal) quality of an extended distribution $\\nu$ as a witness. The infimum restricts the search to $\\nu$ satisfying (a), the first two terms measure $\\nu$ \u2019s discrepancy of with (b), and the last term measures $\\nu$ \u2019s discrepancy with (c). Therefore: ", "page_idx": 8}, {"type": "text", "text": "Proposition 8. $\\operatorname{QIM}\\!I n c_{A}(\\mu)\\geq0,$ , with equality iff $\\mu=\\diamond A.$ ", "page_idx": 8}, {"type": "text", "text": "Although they seem to be very different, QIMInc and IDef turn out to be closely related. In fact, modulo the infimum, $\\mathrm{QIM}I n c_{\\mathcal{A}}$ is a special case of $I D e f.$ \u2014not for the hypergraph $\\boldsymbol{\\mathcal{A}}$ , but rather for a transformed one $\\mathcal{A}^{\\dagger}$ that models the noise variables explcitly. To construct ${\\bar{A}}^{\\dagger}$ from $\\boldsymbol{\\mathcal{A}}$ , add new nodes $\\mathcal{U}=\\{U_{a}\\}_{a\\in\\mathcal{A}}$ , and replace each hyperarc ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\underbrace{\\left(S_{a}\\right)}_{\\mathrm{(D_{a})}}\\xrightarrow{a}\\underbrace{\\left(T_{a}\\right)}_{\\mathrm{(T_{a})}}\\,\\,\\,\\mathrm{with\\the\\pair\\of\\hyperarcs}}&{\\underbrace{\\left(a_{0}\\right)}_{\\mathrm{(S_{a})}\\xrightarrow[a_{1}]{\\longrightarrow}\\left(T_{a}\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Finally, add one additional hyperarc $\\mathcal{U}\\rightarrow\\mathcal{X}$ . (Intuitively, this hyperarc creates functional dependencies in the spirit of Theorem 2.) With these definitions in place, we can state a theorem that bounds QIMInc above and below with information deficiencies (Theorem 9). The lower bound generalizes Theorem 7 by giving an upper limit on $I D e f_{A}(\\mu)$ even for distributions $\\mu$ that are not QIM-compatible with $\\boldsymbol{\\mathcal{A}}$ . The upper bound is tight in general, and shows that $\\mathrm{QIM}I n c_{A}$ can be equivalently defined as a minimization over $I D e f_{A^{\\dagger}}$ . ", "page_idx": 9}, {"type": "text", "text": "Theorem 9. (a) $I f({\\mathcal{X}},A)$ is a hypergraph, $\\mu(\\mathcal{X})$ is a distribution, and $\\nu(\\mathcal{X},\\mathcal{U})$ is an extension of $\\nu$ to additional variables $\\mathcal{U}=\\{U_{a}\\}_{a\\in\\mathcal{A}}$ indexed by $\\mathcal{A}$ , then: ", "page_idx": 9}, {"type": "equation", "text": "$$\nI D e f_{A}(\\mu)\\leq\\mathrm{QIM}I n c_{A}(\\mu)\\leq I D e f_{A^{\\dagger}}(\\nu).\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "(b) For all $\\mu$ and $\\mathcal{A}$ , there is a choice of $\\nu$ that achieves the upper bound. That is, ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\mathrm{QIM}I n c_{\\mathcal{A}}(\\mu)=\\operatorname*{min}\\Big\\{\\,I D e f_{\\mathcal{A}^{\\dagger}}(\\nu):\\begin{array}{l}{\\nu\\in\\Delta\\mathrm{V}(\\mathcal{X},\\mathcal{U})}\\\\ {\\nu(\\mathcal{X})=\\mu(\\mathcal{X})}\\end{array}\\Big\\}.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "The semantics of PDGs are based on the idea of measuring (and resolving) inconsistency, which is defined as a minimization over $I D e f$ (plus a term that captures relevant concrete probabilistic information). Thus, Theorem 9 (b) tells us that QIM-compatibility (with $\\mathcal{A}$ ) can be captured with a qualitative PDG (namely, $\\mathcal{A}^{\\dagger}$ ). It follows that our notion of QIM-compatibility can be viewed as a special case of the semantics of PDGs\u2014one that, as we have shown, has a causal interpretation. ", "page_idx": 9}, {"type": "text", "text": "5 Discussion and Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have shown how directed hypergraphs can be used to represent structural aspects of distributions. Moreover, they can do so in a way that generalizes conditional independencies and functional dependencies and has deep connections to causality and information theory. This notion of QIMcompatibility can be captured with PDGs, and also partially explains the qualitative foundations of these models. Still, many questions remain open. ", "page_idx": 9}, {"type": "text", "text": "Perhaps the most important open problem is that of computing whether or not a given distribution $\\mu$ is QIM compatible with a directed hypergraph $\\boldsymbol{\\mathcal{A}}$ . We have implemented a rudimentary approach (based on solving problem (3) to calculate QIMInc) that works in practice for small examples, but that approach scales poorly, and its correctness has not yet been proved. Even representing a distribution $\\mu$ over $n$ variables requires $\\Omega(2^{n})$ space in general, and a candidate witness $\\bar{\\mu}$ is even bigger: if all variables are binary, $|{\\mathcal{A}}|={\\dot{m}}$ , and $|S_{a}|,|T_{a}|\\,\\le\\,k$ for all $a\\in{\\mathcal{A}}$ , then a direct implementation of (3) is a non-convex optimization problem with at most $2^{n+m k(2^{k})}$ variables. Even accepting the (substantial) cost of representing extended distributions, we do not have a bound on the time needed to solve the optimization problem. There are more compact ways of representing the joint distributions $\\mu$ used in practice (by assuming (in)dependencies), but we do not know if such independence assumptions make it easier to determine whether $\\mu\\vDash\\diamond\\mathcal{A}$ for arbitrary $\\boldsymbol{\\mathcal{A}}$ . But computing $I D e f_{A}(\\mu)$ can be much easier.5 We suspect that Theorem 7, a nontrivial condition for QIM-compatibility that requires only computing $I D e f_{A}(\\mu)$ , could play a critical role in designing such an inference procedure. ", "page_idx": 9}, {"type": "text", "text": "Another major open problem is that of more precisely understanding the implications of QIMcompatibility in cyclic models. We do not yet know, for example, whether the same set of distributions are QIM-compatible with the clockwise and counter-clockwise 3-cycles. ", "page_idx": 9}, {"type": "text", "text": "As mentioned in Section 3, our notion of QIM-compatibility has led us to a generalization of a standard causal model (Definition 4). A proper investigation of this novel modeling tool (which we have not attempted in this paper) would include concrete motivating examples, a careful account of interventions and counterfactuals in this general setting, and results situating these causal models among other generalizations of causal models in the literature. ", "page_idx": 9}, {"type": "text", "text": "We hope to address these questions in future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We would like to thank the reviewers for useful discussion and helpful feedback, such as the pointer to Druzdzel and Simon [4], and for asking us to expand on the complexity of inference. Thank you to Matt MacDermott for identifying a bug in a prior version of Theorem 6, and to Matthias Georg Mayer for catching several low-level issues with the presentation. The work of Halpern and Richardson was supported in part by AFOSR grant FA23862114029, MURI grant W911NF-19-1-0217, ARO grant W911NF-22-1-0061, and NSF grant FMitF-2319186. S.P. is supported in part by the NSF under Grants Nos. CCF-2122230 and CCF-2312296, a Packard Foundation Fellowship, and a generous gift from Google. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] C. Baier, C. Dubslaff, H. Hermanns, and N. K\u00e4fer. On the foundations of cycles in bayesian networks. In Lecture Notes in Computer Science, pages 343\u2013363. Springer Nature Switzerland, 2022. doi: 10.1007/978-3-031-22337-2_17. URL https://doi.org/10.1007% 2F978-3-031-22337-2_17. [2] S. Beckers, J. Y. Halpern, and C. Hitchcock. Causal models with constraints, 2023. [3] T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley, New York, 1991. [4] M. J. Druzdzel and H. A. Simon. Causality in bayesian belief networks. In Conference on Uncertainty in Artificial Intelligence, 1993. URL https://api.semanticscholar.org/ CorpusID:14801431. [5] G. Gallo, G. Longo, S. Pallottino, and S. Nguyen. Directed hypergraphs and applications. Discrete Applied Mathematics, 42(2):177\u2013201, 1993. ISSN 0166-218X. doi: https://doi. org/10.1016/0166-218X(93)90045-P. URL https://www.sciencedirect.com/science/ article/pii/0166218X9390045P. [6] D. Geiger, T. Verma, and J. Pearl. Identifying independence in bayesian networks. Networks, 20(5):507\u2013534, 1990. doi: https://doi.org/10.1002/net.3230200504. URL https: //onlinelibrary.wiley.com/doi/abs/10.1002/net.3230200504. [7] J. Y. Halpern. Axiomatizing causal reasoning. Journal of Artificial Intelligence Research, 12: 317\u2013337, 2000. [8] D. Heckerman, D. M. Chickering, C. Meek, R. Rounthwaite, and C. Kadie. Dependency networks for inference, collaborative filtering, and data visualization. Journal of Machine Learning Research, 1(Oct):49\u201375, 2000.   \n[9] C. Hitchcock. Causal Models. In E. N. Zalta and U. Nodelman, editors, The Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, Summer 2024 edition, 2024.   \n[10] R. James. (stumbling blocks) on the road to understanding multivariate information theory. Discrete Information Theory package documentation, 2018. URL https://dit.readthedocs. io/en/latest/stumbling.html.   \n[11] R. G. James and J. P. Crutchfield. Multivariate dependence beyond shannon information. Entropy, 19(10), 2017. ISSN 1099-4300. doi: 10.3390/e19100531. URL https://www.mdpi. com/1099-4300/19/10/531.   \n[12] I. Kobyzev, S. J. Prince, and M. A. Brubaker. Normalizing flows: An introduction and review of current methods. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43 (11):3964\u20133979, nov 2021. doi: 10.1109/tpami.2020.2992934. URL https://doi.org/10. 1109%2Ftpami.2020.2992934.   \n[13] D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques. MIT press, 2009.   \n[14] S. L. Lauritzen, A. P. Dawid, B. N. Larsen, and H.-G. Leimer. Independence properties of directed markov fields. Networks, 20(5):491\u2013505, 1990. doi: https://doi.org/10. 1002/net.3230200503. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/ net.3230200503.   \n[15] leonbloy (https://math.stackexchange.com/users/312/leonbloy). conditioning reduces mutual information. Mathematics Stack Exchange, 2015. URL https://math.stackexchange. com/q/1219753. URL:https://math.stackexchange.com/q/1219753 (version: 2015-04-04).   \n[16] D. J. C. MacKay. Information Theory, Inference and Learning Algorithms. Cambridge University Press, 2003.   \n[17] P. Naumov and B. Nicholls. R.e. axiomatization of conditional independence, 2013.   \n[18] J. Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, New York, 2000.   \n[19] J. Pearl. Causality. Cambridge university press, 2009.   \n[20] J. Pearl. Causal inference in statistics: An overview. Statistics Surveys, 3(none):96 \u2013 146, 2009. doi: 10.1214/09-SS057. URL https://doi.org/10.1214/09-SS057.   \n[21] J. Pearl and A. Paz. Graphoids: A graphbased logic for reasoning about relevance relations. advances in artificial intelligence, vol. ii, 1987.   \n[22] O. E. Richardson. Loss as the inconsistency of a probabilistic dependency graph: Choose your model, not your loss function. AISTATS \u201922, 151, 2022.   \n[23] O. E. Richardson and J. Y. Halpern. Probabilistic dependency graphs. In Proc. Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21), pages 12174\u201312181, 2021.   \n[24] O. E. Richardson, J. Y. Halpern, and C. De Sa. Inference for probabilistic dependency graphs. In Uncertainty in Artificial Intelligence, pages 1741\u20131751. PMLR, 2023.   \n[25] E. G. Tabak and E. Vanden-Eijnden. Density estimation by dual ascent of the log-likelihood. Communications in Mathematical Sciences, 8(1):217\u2013233, 2010.   \n[26] P. L. Williams and R. D. Beer. Nonnegative decomposition of multivariate information, 2010. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 12}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 12}, {"type": "text", "text": "Justification: All claims are substantiated with precise theorem statements in Sections 2-4, and their implications are carefully discussed. ", "page_idx": 12}, {"type": "text", "text": "Guidelines: ", "page_idx": 12}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 12}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 12}, {"type": "text", "text": "Justification: For example, we discuss the inherent limitations of our central notion with respect to undirected graphical models, and we pose several open problems regarding the distributional implications of cyclic dependency structures that we were not able to solve in this work. We also explain that we only partially characterize the distributions compatible with general dependency structures. ", "page_idx": 12}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 12}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Justification: This is a theoretical work and as such we take mathematical precision very seriously. ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 13}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 13}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 13}, {"type": "text", "text": "Justification: This paper is theoretical in nature and does not include experimental results. Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 13}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 14}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 14}, {"type": "text", "text": "Justification: This paper is theoretical in nature and there is no associated data or code. Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 14}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 14}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 14}, {"type": "text", "text": "Justification: This paper is theoretical and does not involve any training or testing of models. Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 14}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 14}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 14}, {"type": "text", "text": "Justification: This paper is theoretical and does not contain experiments. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 15}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 15}, {"type": "text", "text": "Justification: This paper does not have experiments, computational or otherwise. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 15}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: To the best of our understanding, since this paper focuses on purely theoretical questions regarding graphical languages and the structure of probability distributions, there are no substantive ethical concerns to address. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 15}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 15}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 15}, {"type": "text", "text": "Justification: This paper is best viewed as basic theoretical research and is therefore far from direct societal impacts. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 15}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 16}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: We do not have and have not released any data or models ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 16}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: We use no external assets. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: We do not release new assets. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 17}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 17}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 17}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We begin with a de-randomization construction, that will be useful for the proofs. ", "page_idx": 18}, {"type": "text", "text": "A.1 From CPDs to Distributions over Functions ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Compare two objects: ", "page_idx": 18}, {"type": "text", "text": "\u2022 a cpd $p(Y|X)$ , and \u2022 a distribution $q(Y^{X})$ over functions $g:\\mathrm{V}X\\to\\mathrm{V}Y$ . ", "page_idx": 18}, {"type": "text", "text": "The latter is significantly larger \u2014 if both $|\\mathrm{V}X|=|\\mathrm{V}Y|=\\underline{{N}},$ , then $q$ is a $N^{N}$ dimensional object, while $p$ is only dimension $N^{\\overline{{2}}}$ . A choice of distribution $q(Y^{X})$ corresponds to a unique choice cpd $p(Y|X)$ , according to ", "page_idx": 18}, {"type": "equation", "text": "$$\np(Y{=}y\\mid X{=}x):=q(Y^{X}(x)=y).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Claim 1. $^{\\,I}$ . The definition above in fact yields a cpd, i.e., $\\scriptstyle\\sum_{y}p(Y=y|X=x)\\;=\\;1$ for all $x\\in\\operatorname{V}X$ . 2. This definition of $p(Y|X)$ is the conditional marginal of any joint distribution $\\mu(X,Y,Y^{X})$ satisfying $\\mu(Y^{X})=q$ and $\\mu(Y=Y^{X}(X))=\\breve{1}$ . ", "page_idx": 18}, {"type": "text", "text": "Both $p$ and $q$ give probabilistic information about $Y$ conditioned on $X$ . But $q(Y^{X})$ contains strictly more information. Not only does it specify the distribution over $Y$ given $X{=}x$ , but it also contains counter-factual information about the distribution of $Y$ if $X$ were equal to $x^{\\prime}$ , conditioned on the fact that, in reality, $X{=}x$ . ", "page_idx": 18}, {"type": "text", "text": "Is there a natural construction that goes in the opposite direction, intuitively making as many independence assumptions as possible? It turns out there is: ", "page_idx": 18}, {"type": "equation", "text": "$$\nq(Y^{X}{=}g)=\\prod_{x\\in\\operatorname{V}X}p(Y{=}g(x)\\mid X{=}x).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Think of $Y^{X}$ as a collection of variables $\\{Y^{x}:x\\in\\operatorname{V}X\\}$ describing the value of the function for each input, so that $q$ is a joint distribution over them. This construction simply asks that these variables be independent. Specifying a distribution with these independences amounts to a choice of \u201cmarginal\u201d distribution $q(Y^{x})$ for each $x\\in V X$ , and hence is essentially a funciton of type $\\mathrm{V}X\\to\\Delta\\mathrm{V}Y$ , the same as $p$ . In addition, if we apply the previous construction, we recover $p$ , since: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q(Y^{X}(x)=y)=\\displaystyle\\sum_{g:\\mathrm{V}\\rightarrow\\mathrm{V}Y}\\mathbb{1}[g(x)=y]\\displaystyle\\prod_{x^{\\prime}\\in\\mathrm{V}X}p(Y\\!=\\!g(x^{\\prime})\\mid X\\!=\\!x^{\\prime})}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\sum_{g:\\mathrm{V}X\\rightarrow\\mathrm{V}Y}\\mathbb{1}[g(x)=y]p(Y\\!=\\!g(x)\\mid X\\!=\\!x)\\displaystyle\\prod_{x^{\\prime}\\neq x}p(Y\\!=\\!g(x^{\\prime})\\mid X\\!=\\!x^{\\prime})}\\\\ &{\\qquad\\qquad=p(Y\\!=\\!y\\mid X\\!=\\!x)\\displaystyle\\sum_{g:\\mathrm{V}X\\rightarrow\\mathrm{V}Y}\\mathbb{1}[g(x)=y]\\displaystyle\\prod_{x^{\\prime}\\neq x}p(Y\\!=\\!g(x^{\\prime})\\mid X\\!=\\!x^{\\prime})}\\\\ &{\\qquad\\qquad=p(Y\\!=\\!y\\mid X\\!=\\!x)\\displaystyle\\sum_{g:\\mathrm{V}X\\backslash\\{x\\}\\rightarrow\\mathrm{V}Y}\\prod_{\\substack{x^{\\prime}\\in\\mathrm{V}X\\backslash\\{x\\}}}p(Y\\!=\\!g(x^{\\prime})\\mid X\\!=\\!x^{\\prime})}\\\\ &{\\qquad\\qquad=p(Y\\!=\\!y\\mid X\\!=\\!x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The final equality holds because the remainder of the terms can be viewed as the probability of selecting any function from $X\\setminus\\{x\\}$ to $Y$ , under an analogous measure; thus, it equals 1. This will be a useful construction for us in general. ", "page_idx": 18}, {"type": "text", "text": "A.2 Results on (In)dependence ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Lemma 10. Suppose $X_{1},\\ldots,X_{n}$ are variables, $Y_{1},\\ldots,Y_{n}$ are sets, and for each $i\\in\\{1,\\ldots n\\}$ , we have a function $f_{i}:\\mathrm{V}(X_{i})\\rightarrow Y_{i}$ . Then if $X_{1},\\ldots,X_{n}$ are mutually independent (according to $a$ joint distribution $\\mu$ ), then so are $f_{1}(X_{1}),\\ldots,f_{n}(X_{n})$ . ", "page_idx": 18}, {"type": "text", "text": "Proof. This is an intuitive fact, but we provide a proof for completeness. Explicitly, mutual independence of $X_{1},\\ldots,X_{n}$ means that, for all joint settings $\\mathbf{\\bar{x}}\\;=\\;(x_{1},\\ldots{\\bar{x_{n}}})$ , we have $\\mu(X_{1}{=}\\Bar{x_{1}},\\ldots,X_{n}{=}x_{n})\\ =\\ \\prod_{i=1}^{n}\\mu(X_{i}{=}x_{i})$ . So, for any joint setting $\\textbf{y}=~(y_{1},\\dots,y_{n})~\\in$ $Y_{1}\\times\\cdot\\cdot\\cdot\\times Y_{n}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mu\\left(f_{1}(X_{1})\\!\\!=\\!\\!p_{1},\\dots,f_{n}(X_{n})\\!\\!=\\!p_{n}\\right)=\\mu\\{\\mathrm{(x:f_{1}(\\cdot)~}\\!=\\!\\!y\\})}\\\\ {=}&{\\phantom{+}\\!\\!\\!\\!\\!\\sum_{j\\in[-1,\\frac{n}{2}]}\\!\\!\\!\\!\\!\\!\\mu\\left(X_{1}\\!\\!=\\!\\!\\!x_{1},\\dots,X_{n}\\!\\!=\\!\\!x_{n}\\right)}\\\\ {\\overset{(x_{1},\\dots,x_{n})\\in\\Omega}{\\mathrm{~)~}}}\\\\ {=}&{\\!\\!\\!\\!\\!\\frac{\\mu\\left(x_{1}\\right)\\cdot\\mu\\left(y_{1}(X_{1})-X_{n}\\right)}{j\\left(x_{1}\\right)\\cdot\\mu\\left(y_{1}(X_{1})-y_{1}(\\cdot)\\right)\\cdot\\mu\\left(x_{1}\\right)}}\\\\ {=}&{\\!\\!\\!\\!\\!\\!\\sum_{j\\in[-1,\\frac{n}{2}]}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Lemma 11 (properties of determination). ", "page_idx": 19}, {"type": "text", "text": "Proof. $\\nu\\in X\\twoheadrightarrow Y$ , means there exists a function $f:V(A)\\to V(B)$ such that $\\nu(f(Y)=X)=1$ , i.e., the event $f(A)=B$ occurs with probability 1. ", "page_idx": 19}, {"type": "text", "text": "1. Let $f\\,:\\,\\mathrm{V}(A)\\;\\to\\;\\mathrm{V}(B)$ and $g\\;:\\;\\mathrm{V}(A)\\;\\rightarrow\\;\\mathrm{V}(C)$ be such that $\\nu(f(A)\\,=\\,B)\\,=\\,1\\,=$ $\\nu(g(A)=C)$ . Since both events happen with probability 1, so must the event $f(A)=$ $B\\cap g(A)=C$ . Thus the event $(f(A),{\\bar{g}}(A))=({\\bar{B}},C)$ occurs with probability 1. Therefore, $\\nu\\left|=A\\rightarrow(B,C)\\right.$ .   \n2. The same ideas, but faster: we have $f:\\mathrm{V}(A)\\to\\mathrm{V}(B)$ as before, and $g:\\mathrm{V}(B)\\to\\mathrm{V}(C)$ , such that the events $f(A)=B$ and $g(B)=C$ occur with proability 1. By the same logic, it follows that their conjunction holds with probability 1, and hence $\\dot{C}=f\\bar{(}g(A))$ occurs with probability 1. So $\\nu\\models\\!A\\uplus C$ . ", "page_idx": 19}, {"type": "text", "text": "Theorem 1. If $G$ is a directed acyclic graph and ${\\mathcal{T}}(G)$ consists of the independencies of its corresponding Bayesian network, then $\\mu=\\diamond A_{G}$ if and only if \u00b5 satisfies ${\\mathcal{T}}(G)$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. Label the vertics of $G=(\\mathcal{N},E)$ by natural numbers so that they are a topological sort of $G$ \u2014that is, without loss of generality, suppose ${\\mathcal{N}}=[n]:=\\{1,2,\\dots,n\\}$ , and $i\\,<\\,j$ whenever $i\\rightarrow j\\in E$ . By the definition of $A_{G}$ , the arcs ${\\cal A}_{G}\\,=\\,\\{S_{i}\\stackrel{i}{\\to}\\,i\\}_{i=1}^{n}$ are also indexed by integers. Finally, write $\\mathcal{X}=(X_{1},\\ldots,X_{n})$ for the variables $\\mathcal{X}$ corresponding to $\\mathcal{N}$ over which $\\mu$ is defined. ", "page_idx": 19}, {"type": "text", "text": "$(\\implies)$ . Suppose $\\mu\\vDash\\diamond\\mathcal{A}_{G}$ . This means there is an extension of $\\bar{\\mu}(\\mathcal{X},\\mathcal{U})$ of $\\mu(\\mathcal{X})$ to additional independent variables $\\mathcal{U}=(U_{1},\\ldots,U_{n})$ , such that $\\bar{\\mu}\\vDash(S_{i},U_{i})\\to i$ for all $i\\in[n]$ . ", "page_idx": 19}, {"type": "text", "text": "First, we claim that if $\\bar{\\mu}$ is such a witness, then ${\\bar{\\mu}}\\vDash(U_{1},\\ldots,U_{k})\\to(X_{1},\\ldots,X_{k})$ for all $k\\in[n]$ , and so in particular, ${\\bar{\\mu}}\\vDash\\mathcal{U}\\vDash\\mathcal{X}$ . This follows from QIM-compatibility\u2019s condition (c) and the fact that $G$ is acyclic, by induction. In more detail: The base case of $k=0$ holds vacuously. Suppose that ${\\bar{\\mu}}\\left|=(X_{1},\\ldots,X_{k})\\right.$ for some $k<n$ . Now, conditon (c) of Definition 2 says $\\bar{\\mu}\\,\\left|=(S_{k+1},U_{k+1})\\rightarrow$ $X_{k+1}$ . Because the varaibles are sorted in topological order, the parent variables $S_{k+1}$ are a subset of $\\{X_{1},\\ldots,X_{n}\\}$ , which are determined by $\\boldsymbol{\\mathcal{U}}$ by the induction hypothesis; at the same time clearly $\\bar{\\mu}\\vDash$ $(U_{1},\\ldots,U_{k+1})\\twoheadrightarrow U_{k+1}$ as well. So, by two instances of Lemma 11, $\\bar{\\mu}\\,\\big|{=}\\,(U_{1},\\dots(U_{k+1})\\to X_{k+1}$ . Combining with our inductive hypothesis, we find that ${\\bar{\\mu}}\\left|=(U_{1},\\dots U_{k+1})\\to(X_{1},\\dots,X_{k+1})\\right.$ . So, by induction, ${\\bar{\\mu}}\\left|=(U_{1},\\dots,U_{k})\\right\\rangle\\to(X_{1},\\dots,X_{k})$ for $k\\in[n]$ , and in particular, ${\\bar{\\mu}}\\vDash\\mathcal{U}\\vDash\\mathcal{X}$ . ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "With this in mind, we now return to proving that $\\mu$ has the required independencies. It suffices to show that $\\textstyle\\mu({\\mathcal{X}})=\\prod_{i=1}^{n}\\mu(X_{i}\\mid S_{i})$ . We do so by showing that, for all $k\\in[n]$ , $\\mu(X_{1},\\dots,X_{k})=$ $\\mu(X_{1},\\dotsc,X_{k-1})\\mu(\\Bar{X_{k}}\\stackrel{\\mathcal{-}}{|}\\ S_{k})$ . By QIM-compatibility witness condition (c), we know that $\\bar{\\mu}\\vDash$ $(S_{k},U_{k})\\rightarrow X_{k}$ , and so there exists a function $f_{k}:\\operatorname{V}(S_{k})\\times\\operatorname{V}(U_{k})\\to\\operatorname{V}(X_{k})$ for which the event $f_{k}(S_{k},U_{k})=X_{k}$ occurs with probability 1. Since ${\\bar{\\mu}}\\,\\left|=(U_{1},\\dots,U_{k-1})\\to(X_{1},\\dots,X_{k-1})\\right.$ , and $U_{k}$ is independent of $(U_{1},\\dots,U_{k-1})$ , it follows from Lemma 10 that ${\\bar{\\mu}}\\,\\left|=(X_{1},\\dots,X_{k-1})\\perp0_{k}\\right.$ . Thus ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mu(X_{1},\\ldots,X_{k-1},X_{k})=\\sum_{u\\in\\operatorname{V}(U_{k})}\\mu(X_{1},\\ldots,X_{k-1})\\bar{\\mu}(U_{k}=u)\\cdot\\mathbb{1}[X_{k}=f_{k}(S_{k},u)]}}\\\\ &{}&{=\\mu(X_{1},\\ldots,X_{k-1})\\sum_{u\\in\\operatorname{V}(U_{k})}\\bar{\\mu}(U_{k}=u)\\cdot\\mathbb{1}[X_{k}=f_{k}(S_{k},u)].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Observe that the quantity on the right, including the sum, is a function of $X_{k}$ and $S_{k}$ , but no other variables; let $\\varphi(X_{k},S_{k})$ denote this quantity. Because $\\mu$ is a probability distribution, know that $\\varphi(X_{k},S_{k})$ must be the conditional probability of $X_{k}$ given $X_{1},\\ldots,X_{k-1}$ , and it depends only on the variables $S_{k}$ . Thus $\\mu(X_{1},\\dots,{\\bar{X_{k}}})=\\mu({\\bar{X_{1}}},\\dots,{\\bar{X_{k-1}}})\\mu(X_{k}\\mid S_{k})$ . ", "page_idx": 20}, {"type": "text", "text": "Therefore $\\nu(\\mathcal{X})=\\mu(\\mathcal{X})$ factors as required by the $\\mathbf{BN}\\,G$ , meaning that $\\mu$ has the independencies specified by $G$ . (See Koller & Friedman $\\operatorname{Thm}3.2$ , for instance.) ", "page_idx": 20}, {"type": "text", "text": "$(\\Longleftarrow)$ . Suppose $\\mu$ satiesfies the independencies of $G$ , meaning that each node is conditionally independent of its non-descendents given its parents. We now repeatedly apply the construction Appendix A.1 to construct a QIM-compatibility witness. Specifically, for $k\\in\\{1,\\ldots,n\\}$ , let $U_{k}$ be a variable whose values $\\mathrm{V}(U_{k}):=\\mathrm{V}(X_{k})^{\\mathrm{V}(S_{k})}$ are functions from values of $X_{k}$ \u2019s parents, to values of $X_{k}$ . Let $\\boldsymbol{\\mathcal{U}}$ denote the joint variable $(U_{1},\\bot...,U_{n})$ , and observe that a setting $\\mathbf{{\\bar{g}}}=(g_{1},\\dots,g_{n})$ of $\\boldsymbol{\\mathcal{U}}$ uniquely picks out a value of $\\mathcal{X}$ , by evaluating each function in order. Let\u2019s call this function $f:\\mathrm{V}(\\mathcal{U})\\bar{\\to}\\,\\mathrm{\\bar{V}}(\\mathcal{X})$ . ", "page_idx": 20}, {"type": "text", "text": "To be more precise, we now construct $f(\\mathbf{g})$ inductively. The first component we must produce is $X_{1}$ , but since $X_{1}$ has no parents, $g_{1}$ effectively describes a single value of $X_{1}$ , so we define the first component $f(\\mathbf{g})[X_{1}]$ to be that value. More generally, assuming that we have already defined the components $X_{1},\\ldots,X_{i-1}$ , among which are the variables $S_{k}$ on which $X_{i}$ depends, we can determine the value of $X_{i}$ ; formally, this means defining ", "page_idx": 20}, {"type": "equation", "text": "$$\nf(\\mathbf{g})[X_{i}]:=g_{i}(f(\\mathbf{g})[S_{i}]),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which, by our inductive assumption, is well-defined. Note that, for all $\\mathbf{g}\\in\\mathrm{V}({\\mathcal{U}})$ and $\\mathbf{x}\\in\\mathrm{V}(\\mathcal{X})$ , the function $f$ is characterized by the property ", "page_idx": 20}, {"type": "equation", "text": "$$\nf(\\mathbf{g})=\\mathbf{x}\\quad\\Longleftrightarrow\\quad\\bigwedge_{i=1}^{n}g_{i}(\\mathbf{x}[S_{i}])=\\mathbf{x}[X_{i}].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "To quickly verify this: if $f(\\mathbf{g})=\\mathbf{x}$ , then in particular, for $i\\in[n]$ , then $\\mathbf{x}[X_{i}]=f(\\mathbf{g})[X_{i}]=g_{i}(\\mathbf{x}[S_{i}])$ by the definition above. Conversely, if the right hand side of (4) holds, then we can prove $f(\\mathbf{g})=\\mathbf{x}$ by induction over our construction of $f$ : if $f(\\mathbf{g})[X_{j}]\\;=\\;\\mathbf{x}[X_{j}]$ for all $j~<i$ , then $f(\\mathbf{g})[X_{i}]\\;=\\;$ $\\bar{g_{i}}(f(\\mathbf{g})[S_{i}])=g_{i}(\\mathbf{x}[S_{i}])=\\mathbf{x}[X_{i}]$ . ", "page_idx": 20}, {"type": "text", "text": "Next, we define an unconditional probability over each $U_{k}$ according to ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\bar{\\mu}_{i}(U_{i}=g):=\\prod_{{\\bf s}\\in\\mathrm{V}(S_{k})}\\mu(X_{i}=g(s)\\mid S_{i}\\,{=}\\,{\\bf s}),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which, as verified in Appendix A.1, is indeed a conditional probability, and has the property that $\\bar{\\mu}_{i}(U_{i}(\\mathbf{s})=x)=\\mu(X_{i}\\overset{\\cdot\\cdot}{=}x\\mid S_{i}\\,\\,{=}\\,\\,\\mathbf{s})$ for all $x\\in\\mathrm{V}(X_{i})$ and $\\mathbf{s}\\in\\mathrm{V}(S_{i})$ . By taking an independent combination (tensor product) of each of these unconditional distributions, we obtain a joint distribution $\\begin{array}{r}{\\bar{\\mu}(\\mathcal{U})=\\prod_{i=1}^{n}\\bar{\\mu}_{i}(U_{i}^{\\bullet})}\\end{array}$ . Finally, we extend this distribution to a full joint distribution $\\bar{\\mu}(\\mathcal{U},\\mathcal{X})$ via the pushfor ward of $\\bar{\\mu}(\\mathcal{U})$ through the function $f$ defined by induction above. In this distribution, each $X_{i}$ is determined by $U_{i}$ and $S_{i}$ . ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "By construction, the variables $\\boldsymbol{\\mathcal{U}}$ are mutually independent (for Definition 2(b)), and satisfy $(\\dot{S}_{k},U_{k})\\rightarrow X_{k}$ for all $k\\in[n]$ (Definition 2(c)). It remains only to verify that the marginal of $\\bar{\\mu}$ on the variables $\\mathcal{X}$ is the original distribution $\\mu$ (Definition 2(a)). Here is where we rely on the fact that $\\mu$ satisfies the independencies of $G$ , which means that we can factor $\\mu(\\mathcal{X})$ as $\\textstyle\\mu({\\boldsymbol{\\chi}})={\\dot{\\prod}}_{i=1}^{n}\\mu(X_{i}\\mid S_{i})$ . ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\partial_{t}(X=x)=\\displaystyle\\sum_{k=1}^{r}\\beta_{1}(l\\,\\beta\\,\\sigma_{1}\\otimes\\cdots\\otimes l(x))}\\\\ {=\\displaystyle\\sum_{i=1}^{r}\\mathrm{e}_{\\hat{X}}(l\\,\\beta\\,\\sigma_{1})}\\\\ {=\\displaystyle\\sum_{i=1}^{r}\\mathrm{e}_{\\hat{X}}(l\\,\\beta\\,\\sigma_{1}\\otimes\\cdots\\otimes l)+\\displaystyle\\prod_{i=1}^{r}\\beta_{i}(l\\,\\sigma_{1}\\otimes\\cdots\\otimes l)}\\\\ {=\\displaystyle\\sum_{i=1}^{r}\\sum_{i=1}^{r}\\mathrm{e}_{\\hat{X}}(\\hat{X}_{i}\\otimes(\\hat{X}_{i}))-\\Delta\\hat{X}_{i}[l\\,\\beta\\,\\hat{H}_{i}-\\phi_{i}]}\\\\ {=\\displaystyle\\sum_{i=1}^{r}\\mathrm{e}_{\\hat{X}}(l\\,\\beta\\,\\sigma_{1}\\otimes\\cdots\\otimes l)+\\Delta\\hat{X}_{i}[l\\,\\beta\\,\\hat{H}_{i}-\\phi_{i}]}\\\\ {=\\displaystyle\\prod_{i=1}^{r}\\mathrm{e}_{\\hat{X}}(l\\,\\beta\\,\\sigma_{1}\\otimes\\cdots\\otimes l)}\\\\ {=\\displaystyle\\prod_{i=1}^{r}\\left(\\hat{Z}\\left(\\phi\\in\\mathbb{V}\\left(l\\,\\beta\\right)\\right)\\,\\mu(s_{1})-\\Delta\\hat{X}_{i}\\right)}\\\\ {=\\displaystyle\\prod_{i=1}^{r}\\hat{H}(l\\,\\beta\\,\\sigma_{1}\\otimes\\cdots\\otimes l)}\\\\ {=\\displaystyle\\prod_{i=1}^{r}\\hat{H}(l\\,\\beta\\,\\sigma_{1}\\otimes\\cdots\\otimes l)}\\\\ {=\\displaystyle\\hat{H}_{i}(X_{i}=\\hat{X}_{i}\\,|\\,\\delta\\,\\sigma_{1}\\otimes\\cdots\\otimes l)}\\\\ {=\\displaystyle\\hat{H}_{i}(X_{s}=\\hat{X})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, when $\\mu$ satisfies the independencies of a BN $G$ , it is QIM-compatible with $A_{G}$ . ", "page_idx": 21}, {"type": "text", "text": "Before we move on to proving the other results in the paper, we first illustrate how this relatively substantial first half of the proof of Theorem 1 can be dramatically simplified by relying on two information theoretic arguments. ", "page_idx": 21}, {"type": "text", "text": "Alternate, information-based proof. $(\\implies)$ . Let $G$ be a dag. If $\\mu\\,\\vDash\\,\\diamond A_{G}$ , then by Theorem 7, $I D e f_{A_{G}}(\\mu)\\leq0$ . In the appendix of [23], it is shown that $I D e f_{A_{G}}(\\mu)\\geq0$ with equality iff $\\mu$ satisfies the BN\u2019s independencies. Thus $\\mu$ must satisfy the appropriate independencies. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "Theorem 2. ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "(a) $\\mu\\vDash X\\vDash Y\\land\\diamond\\vartriangle$ if and only if $\\forall n\\geq0.\\;\\mu\\models\\diamond\\mathcal{A}\\sqcup_{X\\to Y}^{(+n)}$ .   \n(b) if A = AG for a dag G, then \u00b5 |= X\u2192\u2192Y \u2227\u2662A if and only if \u00b5 |= \u2662A \u2294X(+\u21921)Y .   \n(c) if \u2203a \u2208A such that Sa = \u2205and X \u2208Ta, then \u00b5 |= X\u2192\u2192Y \u2227\u2662A iff \u00b5 |= \u2662A \u2294X(+\u21922)Y . ", "page_idx": 21}, {"type": "text", "text": "Proof. (a). The forward direction is straightforward. Suppose that $\\mu\\vDash\\diamond\\mathcal{A}$ and $\\mu\\,\\left|=\\,X\\,\\xrightarrow{}\\,Y$ . The former condition gives us a witness $\\nu(\\mathcal{X},\\mathcal{U})$ in which $\\mathcal{U}=\\{U_{a}\\}_{a\\in\\mathcal{A}}$ are mutually independent variables indexed by $\\boldsymbol{\\mathcal{A}}$ , that determine their respective edges. \u201cExtend\u201d $\\nu$ in the unique way to $n$ additional constant variables $U_{1},\\ldots,U_{n}$ , each of which can only take on one value. We claim that this \u201cextended\u201d distribution $\\nu^{\\prime}$ , which we conflate with $\\nu$ because it is not meaningfully different, is a witness to $\\mu\\,\\left|=\\,\\diamond\\mathcal{A}\\sqcup_{X\\to Y}^{\\,(+n)}\\right.$ . Since $\\mu\\vDash X\\twoheadrightarrow Y$ it must also be that $\\nu\\in X\\nrightarrow Y$ , and it follows that $\\nu\\vDash(X,U_{i})\\,\\Dot{\\twoheadrightarrow}\\,\\Dot{Y}$ for all $i\\in\\{1,\\ldots,n\\}$ , demonstrating that the new requirements of $\\nu^{\\prime}$ imposed by Definition 2(c) hold. (The remainder of the requirements for condition (c), namely that $\\bar{\\nu^{\\prime}}\\,\\vDash\\,({\\dot{S_{a}}},U_{a})\\,\\vDash\\,T_{a}$ for $a\\in{\\mathcal{A}}$ , still hold because $\\nu^{\\prime}$ is an extension of $\\nu$ , which we know has this property.) Finally, since $\\boldsymbol{\\mathcal{U}}$ are mutually independent and each $U_{i}$ is a constant (and hence independent of everything), the variables $\\mathcal{U}^{\\prime}:=\\mathcal{U}\\sqcup\\{U_{i}\\}_{i=1}^{n}$ are also mutually independent. Thus $\\nu$ (or, more precisely, an isomorphic \u201cextension\u201d of it to additional trivial variables) is a witness of \u00b5 |= \u2662A \u2294X(+\u2192nY) . ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "The reverse direction is difficult to prove directly, yet it is a straightforward application of Theorem 7. Suppose that $\\mu\\models\\odot A\\sqcup_{X\\to Y}^{(+n)}$ for all $n\\geq0$ . By Theorem 7, we know that ", "page_idx": 22}, {"type": "equation", "text": "$$\n0\\geq I D e f_{{A\\sqcup\\binom{(+n)}{X\\to Y}}}(\\mu)=I D e f_{A}(\\mu)+n\\operatorname{H}_{\\mu}(Y|X).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Because $I D e f_{A}(\\mu)$ is bounded below $(\\log|\\mathrm{V}(\\mathcal{X})|)$ , it cannot be the case that $\\mathrm{H}_{\\mu}(Y|X)>0$ ; otherwise, the inequality above would not hold for large $n$ (specifically, for $n>\\log|\\mathrm{V}({\\mathcal{X}})|/\\,\\mathrm{H}_{\\mu}(Y|X))$ . By Gibbs inequailty, $\\mathrm{H}_{\\mu}(Y|X)$ is non-negative, and thus it must be the case that $\\mathrm{H}_{\\mu}(Y|X)=0$ . Thus $\\mu\\vDash X\\to Y$ . It is also true that $\\mu\\vDash\\diamond\\mathcal{A}$ by monotonicity (Proposition 13), which is itself a direct application of Theorem 7 ", "page_idx": 22}, {"type": "text", "text": "(b). Now $A=A_{G}$ for some graph $G$ . The forward direction of the equivalence is strictly weaker than the one we already proved in part (a); we have shown $\\mu\\models\\diamond\\square\\sqcup_{X\\rightarrow Y}$ for all $n\\geq0$ , and needed only to show it for $n\\,=\\,1$ . The reverse direction is what\u2019s interesting. As before, we will take a significant shortcut by using Theorem 7. Suppose $\\mu\\bigm\\vert=\\diamond\\mathcal{A}\\sqcup_{X\\to Y}$ . In this case where $A=A_{G}$ , it was shown by Richardson and Halpern [23] that $I D e f_{A}(\\mu)\\geq\\dot{0}$ . It follows that ", "page_idx": 22}, {"type": "equation", "text": "$$\n0\\stackrel{(\\mathrm{Theorem}\\,7)}{\\geq}I D e f_{A\\,\\cup\\,{\\frac{(+n)}{X\\rightarrow Y}}}\\left(\\mu\\right)=I D e f_{A}(\\mu)+\\mathrm{H}_{\\mu}(Y|X)\\;\\geq\\;0,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and thus $\\mathrm{H}_{\\mu}(Y|X)=0$ , meaning that $\\mu\\vDash X\\twoheadrightarrow Y$ as promised. As before, we also have $\\mu\\vDash\\diamond\\mathcal{A}$ by monotonicity. ", "page_idx": 22}, {"type": "text", "text": "(c). As in part (b), the forward direction is a special case of the forward direction of part (a), and it remains only to prove the reverse direction. Equipped with the additional information that $A\\sim\\{\\to\\{X\\}\\}$ , suppose that $\\mu\\models\\odot A\\sqcup_{X\\to Y}$ . By monotonicity, this means $\\mu\\vDash\\diamond\\mathcal{A}$ and also that $\\mu\\vDash\\to\\operatorname{\\rho}(X)$ $\\underline{{\\boldsymbol{Y}}}$ . Let $\\mathcal{A}^{\\prime}$ denote this hypergraph. Once again by appeal to Theorem 7, we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n0\\ge I D e f_{\\mathcal{A}^{\\prime}}=-\\operatorname{H}_{\\mu}(X,Y)+\\operatorname{H}(X)+2\\operatorname{H}_{\\mu}(Y|X)=\\operatorname{H}_{\\mu}(Y|X)\\ge0.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "It follows that $\\mathrm{H}_{\\mu}(Y|X)=0$ , and thus $\\mu\\vDash X\\vDash Y$ . As mentioned above, we also know that $\\mu\\vDash\\diamond\\mathcal{A}$ , and thus $\\mu\\models\\odot A\\,\\land\\,X{\\to}Y$ as promised. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "A.3 Causality Results of Section 3 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Proposition 3. Given a graph $G$ and a distribution $\\iota,\\,\\mu\\in\\langle\\rangle A_{G}$ iff there exists a fully randomized PSEM of dependency structure $A_{G}$ from which $\\mu$ can arise. ", "page_idx": 23}, {"type": "text", "text": "Proof. $(\\implies$ ). Suppose $\\mu=\\diamond A_{G}$ . Thus there exists some witness $\\bar{\\mu}(\\mathcal{X},\\mathcal{U})$ to this fact, satisfying conditions (a-c) of Definition 2. Because $A_{G}$ is partitional, the elements of $\\mathrm{PSEMs}_{A_{G}}(\\bar{\\mu})$ are ordinary (i.e., not generalized) randomized PSEMs. We claim that every $\\mathcal{M}=(M,P)\\in\\bar{\\mathrm{PSEMs}}_{A_{G}}(\\bar{\\mu})$ that is a randomized PSEM from which $\\mu$ can arise, and also has the property that ${\\bf P a}_{M}(Y)\\subseteq$ $\\mathbf{Pa}_{G}(Y)\\cup\\{U_{Y}\\}$ for all $Y\\in\\mathcal{X}$ . ", "page_idx": 23}, {"type": "text", "text": "\u2022 The hyperarcs of $A_{G}$ correspond to the vertices of $G$ , which in turn correspond to the variables in $\\mathcal{X}$ ; thus $\\mathcal{U}=\\{U_{X}\\}_{X\\in\\mathcal{X}}$ . By property (b) of QIM-compatibility witnesses (Definition 2), these variables $\\{U_{X}\\}_{X\\in{\\mathcal{X}}}$ are mutually independent according to $\\bar{\\mu}$ . Furthermore, because $\\mathcal{M}=(M,P)\\in\\mathrm{{\\dot{PS}}}\\dot{\\mathrm{{EMs}}}_{{\\mathcal{A}}_{G}}(\\bar{\\mu})$ , we know that $\\bar{\\mu}(\\mathcal{U})=P$ , and thus the variables in $\\boldsymbol{\\mathcal{U}}$ must be mutually independent according to $P$ . By construction, in causal models $\\mathcal{M}\\in\\mathrm{PSEMs}_{A_{G}}(\\bar{\\mu})$ the equation $f_{Y}$ can depend only on $S_{Y}={\\mathbf{P}}{\\mathbf{a}}_{G}(Y)\\subseteq{\\mathcal{X}}$ and $U_{Y}$ . So, in particular, $f_{Y}$ does not depend on $U_{X}$ for $X\\neq Y$ . Altogether, we have shown that $\\mathcal{M}$ contains exogenous variables $\\{U_{X}\\}_{X\\in{\\mathcal{X}}}$ that are mutually independent according to $P$ , and that $f_{Y}$ does not depend on $U_{X}$ when $X\\neq Y$ . Thus, $\\mathcal{M}$ is a randomized PSEM.   \n\u2022 By condition (a) on QIM-compatibility witnesses (Definition 2), we know that $\\bar{\\mu}(\\mathcal{X})=\\mu$ . By Proposition 5(a), we know that $\\mu\\in\\{\\mathbb{M}\\}$ . Together, the previous two sentences mean that $\\mu$ can arise from $\\mathcal{M}$ .   \n\u2022 Finally, as mentioned in the first bullet item, the equation $f_{Y}$ in $M$ can depend only on $S_{Y}=$ $\\mathbf{Pa}_{G}(\\bar{Y})$ and on $U_{Y}$ . Thus $\\mathbf{Pa}_{M}(Y)\\subseteq\\mathbf{Pa}_{G}(Y)\\cup\\{U_{Y}\\}$ for all $Y\\in\\mathcal{X}$ . ", "page_idx": 23}, {"type": "text", "text": "Under the assumption that $\\mu\\vDash\\diamond\\mathcal{A}_{G}$ , we have now shown that there exists a randomized causal model $\\mathcal{M}$ from which $\\mu$ can arise, with the property that $\\mathbf{Pa}_{\\mathcal{M}}(Y)\\subseteq\\mathbf{Pa}_{G}(Y)\\cup\\{U_{Y}\\}$ for all $Y\\in\\mathcal{X}$ . ", "page_idx": 23}, {"type": "text", "text": "$(\\Longleftarrow)$ . Conversely, suppose there is a randomized PSEM $\\mathcal{M}=(M=(\\mathcal{V},\\mathcal{U},\\mathcal{F}),P)$ with the property that $\\mathbf{Pa}_{M}(\\dot{Y})\\subseteq\\mathbf{\\dot{Pa}}_{G}(Y)\\cup\\{U_{Y}\\}$ for all $Y$ , from which $\\mu$ can arise. The last clause means there exists some $\\nu\\in\\{\\mathbb{M}\\}$ such that $\\nu(\\mathcal{X})=\\mu$ . We claim that this $\\nu$ is a witness to $\\mu=\\diamond A_{G}$ . We already know that condition (a) of being a QIM-compatibility witness is satisfied, since $\\nu(\\mathcal{X})=\\mu$ Condition (b) holds because of the assumption that $\\{U_{X}\\}_{X\\in{\\mathcal{X}}}$ are mutually independent in the distribution $P$ for a randomized PSEM (and the fact that $\\nu(\\mathcal{U})=P$ , since $\\boldsymbol{\\nu}\\in\\{\\mathbb{M}\\bar{\\mathbb{y}})$ . Finally, we must show that (c) for each $Y\\in\\mathcal{X}$ , $\\nu\\left|=\\mathbf{Pa}_{G}(Y)\\cup\\{U_{Y}\\}\\rightarrow Y$ . Since $\\nu\\in\\{\\mathbb{M}\\}$ , we know that $M$ \u2019s equation holds with probability 1 in $\\nu$ , and so it must be the case that $\\nu\\big\\vert={\\bf P a}_{M}(Y)\\rightarrow Y$ . Note that, in general, if $\\mathbf{A}\\subseteq\\mathbf{B}$ and $\\mathbf{A}\\twoheadrightarrow\\mathbf{C}$ , then $\\mathbf{B}\\twoheadrightarrow\\mathbf{C}$ . By assumption, $\\mathbf{Pa}_{M}(Y)\\subseteq\\mathbf{Pa}_{G}(Y)\\cup\\{U_{Y}\\}$ , and thus $\\nu\\models\\mathbf{Pa}_{G}(Y)\\cup\\{U_{Y}\\}\\rightarrow Y$ . ", "page_idx": 23}, {"type": "text", "text": "Thus $\\nu$ satisfies all conditions (a-c) for a QIM-compatibility witness, and hence $\\mu=\\diamond A_{G}$ . \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Proposition 4. $\\mu\\vDash\\diamond\\mathcal{A}$ iff there exists a generalized randomized PSEM with structure $\\boldsymbol{\\mathcal{A}}$ from which $\\mu$ can arise. ", "page_idx": 23}, {"type": "text", "text": "Proof. $(\\implies)$ . Suppose $\\mu\\,\\vDash\\,\\diamond\\!\\!\\backslash\\,A$ , meaning there exists a witness $\\nu(\\mathcal{X},\\mathcal{U})$ with property Definition 2(c), meaning that, for all $a\\in A$ , there is a functional dependence $(S_{a},U_{a})\\rightarrow T_{a}$ . Thus, there is some set of functions $\\mathcal{F}$ with these types that holds with probability 1 according to $\\nu$ Meanwhile, by Definition 2(b), $\\nu(\\mathcal{U})$ are mutually independent, so defining $\\bar{P_{a}}(U_{a}):=\\nu(\\bar{U_{a}})$ , we have $\\begin{array}{r}{\\nu(\\mathcal{U})=\\prod_{a\\in\\mathcal{A}}P_{a}(U_{a})}\\end{array}$ . Together, the previous two conditions (non-deterministically) define a generalized randomized PSEM $\\mathcal{M}$ of shape $\\boldsymbol{\\mathcal{A}}$ for which $\\nu\\in\\{\\mathbb{M}\\}$ . Finally, by Definition 2(a), we know that $\\mu$ can arise from $\\mathcal{M}$ . ", "page_idx": 23}, {"type": "text", "text": "$(\\Longleftarrow)$ . Conversely, suppose there is a generalized randomized SEM $\\mathcal{M}$ of shape $\\boldsymbol{\\mathcal{A}}$ from which $\\mu(\\mathcal{X})$ can arise. Thus, there is some $\\nu\\in\\{\\!\\!\\{M\\}\\!\\!\\}$ whose marginal on $\\mathcal{X}$ is $\\mu$ . We claim that this $\\nu$ is also a witness that $\\mu\\vDash\\diamond\\mathcal{A}$ . The marginal constraint from Definition 2(a) is clearly satisfied. Condition (b) is immediate as well, because $\\begin{array}{r}{\\dot{\\nu}(\\mathcal{U})=\\prod_{a}P_{a}(U_{a})}\\end{array}$ . Finally, condition (c) is satisfied, because the equations of $\\mathcal{M}$ hold with probability 1, e nsuring the appropriate functional dependencies. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "Proposition 5. If $\\bar{\\mu}(\\mathcal{X},\\mathcal{U}_{A})$ is a witness for QIM-compatibility with $\\boldsymbol{\\mathcal{A}}$ and $\\mathcal{M}$ is a PSEM with dependency structure $\\boldsymbol{\\mathcal{A}}$ , then $\\bar{\\mu}\\in\\{\\mathcal{M}\\}$ if and only if $\\mathcal{M}\\in\\mathrm{PSEMs}_{A}(\\bar{\\mu})$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. (a) is straightforward. Suppose $\\mathcal{M}\\in\\mathrm{PSEMs}(\\nu)$ . By construction, the equations of $\\mathcal{M}$ reflect functional dependencies in $\\nu$ , and hence hold with probability 1.6 Furthermore, the distribution $P(\\mathcal{U})$ in all $\\mathcal{M}\\in\\bar{\\mathrm{PSEMs}}(\\nu)$ is equal to $\\nu(\\mathcal{U})$ . These two facts, demonstrate that $\\nu$ satisfies the two constraints required for membership in $\\mathbb{\\{}\\mathcal{M}\\nparallel$ . ", "page_idx": 24}, {"type": "text", "text": "(b). We do the two directions separately. First, suppose $\\mathcal{M}\\in\\mathrm{PSEMs}(\\nu)$ . We have already shown (in part (a)) that $\\nu\\in\\{\\mathbb{M}\\}$ . The construction of $\\mathrm{PSEMs}(\\nu)$ depends on the hypergraph $\\boldsymbol{\\mathcal{A}}$ (even if the dependence is not explicitly clear from our notation) in such a way that $f_{X}$ does not depend on any variables beyond $U_{a}$ and $S_{a_{X}}$ . Thus, ${\\bf P a}_{\\mathcal{M}}(X)\\subseteq S_{a_{X}}\\cup\\{U_{a_{X}}\\}$ . ", "page_idx": 24}, {"type": "text", "text": "Conversely, suppose $\\mathcal{M}=(\\mathcal{X},\\mathcal{U},\\mathcal{F})$ is a PSEM satisfying $\\nu\\in\\{\\mathbb{M}\\}$ and ${\\mathbf{Pa}}_{\\mathcal{M}}(X)\\subseteq S_{a_{X}}\\cup\\{U_{a_{X}}\\}$ We would like to show that $\\mathcal{M}\\in\\mathrm{PSEMs}(\\nu)$ . Because $\\nu\\in\\{\\mathbb{M}\\}$ , we know that the distribution $P(\\mathcal{U})$ over the exogenous variables in the PSEM $\\mathcal{M}$ is equal to $\\nu(\\mathcal{U})$ , matching the first part of our construction. What remains is to show that the equations $\\mathcal{F}$ are consistent with our transformation. Choose any $X\\in\\mathcal{X}$ . Because $\\boldsymbol{\\mathcal{A}}$ is subpartitional, there is a unique $a_{X}\\in A$ such that $X\\in T_{a_{X}}$ . Now choose any values $\\mathbf{s}\\in\\mathrm{V}(S_{a_{X}})$ and $\\bar{u^{\\ast}}\\in\\mathrm{V}(U_{a_{X}})$ . If $\\nu(\\mathbf{s},u)>0$ , then we know there is a unique value of $x\\in\\mathrm{V}(X)$ such that $\\nu(\\mathbf{s},u,x)\\,>\\,0$ . Since $\\mathcal{M}$ \u2019s equation for $X$ , $f_{X}$ , depends only on s and $u$ , and holds with probability 1, we know that $f_{X}(\\mathbf{s},u)=t$ , as required. On the other hand, if $\\nu(\\mathbf{s},u)=0$ , then any choice of $f_{X}({\\bf s},u)$ is consistent with our procedure. Since this is true for all $X$ , and all possible inputs to the equation $f_{X}$ , we conclude that the equations $\\mathcal{F}$ can arise from the procedure described in the main text, and therefore $\\mathcal{M}\\in\\mathrm{PSEMs}(\\nu)$ . \u53e3 ", "page_idx": 24}, {"type": "text", "text": "Theorem 6. Suppose that $\\bar{\\mu}$ is a QIM witness for ${\\mathcal{A}},$ , that $\\mathcal{M}=(\\mathcal{U},\\mathcal{V},\\mathcal{F},P)\\in\\mathrm{PSEMs}_{A}(\\bar{\\mu})$ is a corresponding PSEM, and that the noise variables $\\mathcal{U}_{A}=\\{U_{X}\\}_{X\\in\\mathcal{V}}$ are independent of the other exogenous variables $\\mathcal{U}\\setminus\\mathcal{U}_{A}$ . For all $\\mathbf{X}\\subseteq\\mathcal{V}$ and $\\mathbf{x}\\in\\mathrm{V}(\\mathbf{X})$ , if $\\bar{\\bar{\\mu}}(\\mathrm{do}_{\\mathcal{M}}(\\mathbf{X}\\!\\!=\\!\\mathbf{x}))>0,$ , then ", "page_idx": 24}, {"type": "text", "text": "(a) $\\bar{\\mu}(\\mathcal{V}\\,|\\,\\mathrm{do}_{\\mathcal{M}}(\\mathbf{X}{=}\\mathbf{x}))$ can arise from $\\mathcal{M}_{\\mathbf{X}\\leftarrow\\mathbf{x}}$ ; $(b)$ for all events $\\approx\\mathrm{V}({\\boldsymbol{\\mathcal{V}}}),\\,\\mathrm{Pr}_{\\mathcal{M}}\\left(\\left[\\mathbf{X}{\\gets}\\mathbf{x}\\right]\\varphi\\right)\\leq\\bar{\\mu}\\big(\\varphi\\,\\big|\\,\\mathrm{do}_{\\mathcal{M}}(\\mathbf{X}{=}\\mathbf{x})\\big)\\leq\\mathrm{Pr}_{\\mathcal{M}}\\left(\\langle\\mathbf{X}{\\gets}\\mathbf{x}\\rangle\\varphi\\right)$ and all three are equal when $\\mathcal{M}\\vDash\\mathcal{U}\\xrightarrow{}\\mathcal{V}$ (such as when $\\mathcal{M}$ is acyclic). ", "page_idx": 24}, {"type": "text", "text": "Proof. (part a). Because we have assumed $\\bar{\\mu}(\\mathrm{do}_{\\mathcal{M}}(\\mathbf{X}{=}\\mathbf{x}))>0$ , the conditional distribution ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\bar{\\mu}\\mid\\mathrm{do}_{\\mathcal{M}}(\\mathbf{X}\\!=\\!\\mathbf{x})=\\bar{\\mu}(\\mathcal{M},\\mathcal{X})\\cdot\\prod_{X\\in\\mathbf{X}}\\mathbb{1}\\!\\left[\\big\\forall\\mathbf{s}.\\,f_{X}(U_{X},\\mathbf{s})=\\mathbf{x}[X]\\right]\\,\\Big/\\,\\bar{\\mu}(\\mathrm{do}_{\\mathcal{M}}(\\mathbf{X}\\!=\\!\\mathbf{x}))\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "is defined. By assumption, $\\mathcal{M}\\in\\mathrm{PSEMs}(\\bar{\\mu})$ and $\\bar{\\mu}$ is a witness to the fact that $\\mu\\vDash\\diamond\\mathcal{A}$ . Thus, by Proposition 5, we know that $\\bar{\\mu}\\in\\{\\mathcal{M}\\}$ . So in particular, all equations of $\\mathcal{M}$ hold for all joint settings $(\\mathbf{u},\\mathbf{v})\\,\\in\\,\\mathrm{V}(\\mathcal{U}\\cup\\mathcal{V})$ in the support of $\\bar{\\mu}$ . But the support of the conditional distribution $\\bar{\\mu}\\mid\\mathrm{do}_{\\mathcal{M}}(\\mathbf{X}{=}\\mathbf{\\dot{x}})$ is a subset of the support of $\\bar{\\mu}$ , so all equations of $\\mathcal{M}$ also hold in the conditioned distribution. Furthermore, the event $\\mathrm{do}_{\\mathcal{M}}(\\mathbf{X}{=}\\mathbf{x})$ is the event in which, for all $X\\in\\mathbf{X}$ , the variable $U_{X}$ takes on a value such that $f_{X}(\\boldsymbol{\\cdot}\\boldsymbol{\\cdot}\\boldsymbol{\\cdot},U_{X},\\dots)=\\mathbf{x}[X]$ . Thus the equations corresponding to ${\\bf X}={\\bf x}$ also hold with probability 1 in $\\bar{\\boldsymbol{\\lambda}}\\mid\\operatorname{do}_{\\mathcal{M}}(\\mathbf{X}{=}\\mathbf{x})$ . ", "page_idx": 24}, {"type": "text", "text": "This shows that all equations of $\\mathcal{M}_{\\mathbf{X}\\leftarrow\\mathbf{x}}$ hold with probability 1 in $\\bar{\\mu}\\mid\\operatorname{do}_{\\mathcal{M}}(\\mathbf{X}{=}\\mathbf{x})$ . However, the marginal distribution $\\bar{\\mu}(\\mathcal{U}\\mid\\mathrm{do}_{\\mathcal{M}}(\\mathbf{X}{=}\\mathbf{x}))$ over $\\boldsymbol{\\mathcal{U}}$ is typically not equal to $P(\\mathcal{U})$ \u2014after all, we have collapsed distribution of the variables $\\mathcal{U}_{\\mathbf{X}}:=\\{U_{X}:X\\in\\mathbf{X}\\}$ . Clearly, $\\bar{\\boldsymbol{u}}\\mid\\operatorname{do}_{\\mathcal{M}}(\\mathbf{X}{=}\\mathbf{x})\\notin$ $\\{\\mathcal{M}_{\\mathbf{X}\\leftarrow\\mathbf{x}}\\}$ . However, as we now show, there exists a different distribution $\\bar{\\mu}^{\\prime}\\in\\{\\mathcal{M}_{\\mathbf{X}\\leftarrow\\mathbf{x}}\\}$ such that $\\bar{\\bar{\\mu}}^{\\prime}(\\mathcal{V})=\\bar{\\nu}(\\mathcal{V}\\mid\\mathrm{do}_{\\mathcal{M}}(\\mathbf{X}{=}\\mathbf{x}))$ . ", "page_idx": 24}, {"type": "text", "text": "Let $\\mathcal{U}_{0}:=\\mathcal{U}\\setminus\\mathcal{U}_{\\mathbf{X}}$ . We can define $\\bar{\\mu}^{\\prime}$ according to ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\bar{\\mu}^{\\prime}(\\mathcal{V},\\mathcal{U}_{\\mathbf{X}},\\mathcal{U}_{0}):=\\bar{\\mu}(\\mathcal{V},\\mathcal{U}_{0}\\mid\\mathrm{do}_{\\mathcal{M}}(\\mathbf{X}\\!\\!=\\!\\!\\mathbf{x}))P(\\mathcal{U}_{\\mathbf{X}}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The distribution $\\bar{\\mu}^{\\prime}$ satisfies three critical properties: ", "page_idx": 25}, {"type": "text", "text": "2. At the same time, the marginal of $\\bar{\\mu}^{\\prime}$ on exogenous variables is ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\tilde{\\mu}^{\\prime}(\\mathcal{U})=\\tilde{\\mu}^{\\prime}(\\mathcal{U}_{\\mathbf{X}},\\mathcal{U}_{0})}&{}\\\\ {=\\int_{\\mathbf{V}(\\mathcal{V})}\\tilde{\\mu}^{\\prime}(\\mathbf{v},\\mathcal{U}_{\\mathbf{X}},\\mathcal{U}_{0})\\;\\mathrm{d}\\mathbf{v}}\\\\ {=\\int_{\\mathbf{V}(\\mathcal{V})}\\tilde{\\mu}(\\mathbf{v},\\mathcal{U}_{0}\\mid\\mathrm{d}\\mathbf{o}_{M}(\\mathbf{X}\\!-\\!\\mathbf{x}))P(\\mathcal{U}_{\\mathbf{X}})\\;\\mathrm{d}\\mathbf{v}}\\\\ {=P(\\mathcal{U}_{\\mathbf{X}})\\tilde{\\mu}(\\mathcal{U}_{0}\\mid\\mathrm{d}\\mathbf{o}_{M}(\\mathbf{X}\\!-\\!\\mathbf{x}))}&{}\\\\ {=P(\\mathcal{U}_{\\mathbf{X}})P(\\mathcal{U}_{0}\\mid\\mathrm{d}\\mathbf{o}_{M}(\\mathbf{X}\\!-\\!\\mathbf{x}))}&{\\qquad[\\mathrm{bc~}]}\\\\ {=P(\\mathcal{U}_{\\mathbf{X}})P(\\mathcal{U}_{0})}&{}\\\\ {=P(\\mathcal{U}_{\\mathbf{X}},\\mathcal{U}_{0})}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "3. Finally, $\\bar{\\mu}^{\\prime}$ satisfies all equations of $\\mathcal{M}_{\\mathbf{X}\\leftarrow\\mathbf{x}}$ . It satisfies the equations for the variables $\\mathbf{X}$ because ${\\bf X}={\\bf x}$ holds with probability 1. At the same time, the equations of $\\mathcal{M}_{\\mathbf{X}\\leftarrow\\mathbf{x}}$ corresponding to other variables, say $f_{Z}$ for $Z\\in\\mathcal{V}\\setminus\\mathbf{X}$ , also hold with probability one. This is because the marginal $\\bar{\\mu}^{\\prime}(\\mathcal{U}_{\\mathbf{Z}},\\mathcal{V})$ is shared with the distribution $\\bar{\\mu}\\mid\\mathrm{do}_{\\mathcal{M}}(\\mathbf{X}{=}\\mathbf{x})$ , and that distribution satisfies these equations. (It suffices to show that they share this particular marginal because the equations for $\\mathbf{Z}$ do not depend on $\\mathcal{U}_{\\mathbf{X}}$ .) ", "page_idx": 25}, {"type": "text", "text": "Together, properties 2 and 3 show that $\\bar{\\mu}^{\\prime}\\;\\in\\;\\{\\mathcal{M}_{\\mathbf{X}\\leftarrow\\mathbf{x}}\\}$ , while property 1 shows that $\\bar{\\mu}(\\nu\\,\\,|\\,\\,$ $\\bar{\\mathrm{do}_{\\mathcal{M}}}(\\mathbf{X}{=}\\bar{\\mathbf{x}})\\rangle$ ) can arise from $\\mathcal{M}_{\\mathbf{X}\\leftarrow\\mathbf{x}}$ . This establishes part (a). ", "page_idx": 25}, {"type": "text", "text": "(part b). We will again make use of the distribution $\\bar{\\mu}^{\\prime}$ defined in part (a), and its three critical properties listed above. Given a setting $\\mathbf{u}\\in\\mathrm{V}({\\mathcal{U}})$ of the exogenous variables, let ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathcal{F}_{\\mathbf{X}\\leftarrow\\mathbf{x}}(\\mathbf{u}):=\\left\\{\\omega\\in\\mathrm{V}(\\mathcal{V})\\,\\left|\\,\\begin{array}{c c}{\\forall X\\in\\mathbf{X}.}&{\\omega[X]=\\mathbf{x}[X]}\\\\ {\\forall Y\\in\\mathcal{V}\\setminus\\mathbf{X}.}&{\\omega[Y]=f_{X}(\\omega[\\mathcal{X}\\setminus Y],\\mathbf{u})}\\end{array}\\right.\\right\\}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "denote the set of joint settings of endogenous variables that are consistent with the equations of MX\u2190x. ", "page_idx": 25}, {"type": "text", "text": "If $\\mathbf{u}\\in\\mathrm{V}(\\mathcal{U})$ is such that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{(M,\\mathbf{u})\\models[\\mathbf{X}{\\leftarrow}\\mathbf{x}]\\varphi\\quad\\Longleftrightarrow\\quad(M_{\\mathbf{X}{\\leftarrow}\\mathbf{x}},\\mathbf{u})\\models\\varphi}\\\\ &{\\iff\\forall\\omega\\in\\mathcal{F}_{\\mathbf{X}{\\leftarrow}\\mathbf{x}}(\\mathbf{u}).~\\omega\\in\\varphi}\\\\ &{\\iff\\mathcal{F}_{\\mathbf{X}{\\leftarrow}\\mathbf{x}}(\\mathbf{u})~\\subseteq~\\varphi,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "then $\\phi$ holds at all points that satisfy the equations of $M_{\\mathbf{X}\\leftarrow\\mathbf{x}}$ . So, since $\\bar{\\mu}^{\\prime}$ is supported only on such points (property 3), it must be that $\\bar{\\mu}^{\\prime}(\\varphi)\\,\\bar{=}\\,1$ . By property 1 $,\\bar{\\mu}^{\\prime}(\\varphi)=\\bar{\\mu}(\\varphi\\mid\\mathrm{do}_{\\mathcal{M}}(\\mathbf{X}{=}\\mathbf{x}))$ . ", "page_idx": 25}, {"type": "text", "text": "Furthermore, if $\\bar{\\mu}^{\\prime}(\\varphi)\\;>\\;0$ , then there must exist some $\\omega\\;\\in\\;{\\mathcal F}_{\\mathbf{X}\\leftarrow\\mathbf{x}}(\\mathbf{u})$ satisfying $\\varphi$ , and thus $(M,\\mathbf{u})\\vDash\\langle\\mathbf{X}\\{\\leftarrow\\dot{\\mathbf{x}}\\}\\varphi$ . Putting both of these observations together, and with a bit more care to the ", "page_idx": 25}, {"type": "text", "text": "symbolic manipulation, we find that: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{P r}(|\\mathbf{X}\\gets\\mathbf{x}|\\varphi)=P(\\{\\mathbf{u\\inV}(U):(M,\\mathbf{u})\\in[\\mathbf{X}\\gets\\mathbf{x}]\\varphi\\})}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{\\mathbf{u\\inV}(U)}P(\\mathbf{u})\\,\\mathbb{I}\\left[\\mathcal{F}_{\\mathbf{X}\\gets\\mathbf{x}}(\\mathbf{u})\\subseteq\\varphi\\right]}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\sum_{\\mathbf{u\\inV}(U)}P(\\mathbf{u})\\bar{\\mu}^{\\prime}(\\varphi\\mid\\mathbf{u})\\qquad=\\bar{\\mu}^{\\prime}(\\varphi)=\\bar{\\mu}(\\varphi\\mid\\mathbf{d}\\mathsf{o}_{M}(\\mathbf{X}\\!\\!-\\!\\mathbf{x}))}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\sum_{\\mathbf{u\\inV}(U)}P(\\mathbf{u})\\,\\mathbb{I}\\left[\\mathcal{F}_{\\mathbf{X}\\gets\\mathbf{x}}(\\mathbf{u})\\cap\\varphi\\neq\\bar{\\theta}\\right]}\\\\ &{\\qquad\\qquad=P(\\{\\mathbf{u\\inV}(U):(M,\\mathbf{u})\\in\\langle\\mathbf{X}\\!\\!-\\!\\mathbf{x}\\rangle\\varphi\\})}\\\\ &{\\qquad=\\displaystyle\\mathbf{Rr}(\\langle\\mathbf{X}\\!\\!+\\!\\mathbf{x}\\rangle\\varphi),\\qquad\\qquad\\mathrm{as~desiced}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Finally, if $\\bar{\\mu}\\,\\vDash\\,\\mathcal{U}\\,\\twoheadrightarrow\\,\\mathcal{V}$ , then $\\mathcal{F}_{\\mathbf{X}\\leftarrow\\mathbf{x}}(\\mathbf{u})$ is a singleton for all $\\mathbf{u}$ , and hence $\\varphi$ holding for all $\\omega\\in{\\mathcal{F}}_{\\mathbf{X}\\leftarrow\\mathbf{x}}$ and for some $\\omega\\in{\\mathcal{F}}_{\\mathbf{X}\\leftarrow\\mathbf{x}}$ are equivalent. So, in this case, ", "page_idx": 26}, {"type": "equation", "text": "$$\n(M,\\mathbf{u})\\left|=[\\mathbf{X}{\\leftarrow}\\mathbf{x}]\\varphi\\qquad\\Longleftrightarrow\\qquad(M,\\mathbf{u})\\left|=\\langle\\mathbf{X}{\\leftarrow}\\mathbf{x}\\rangle\\varphi,\\right.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and thus the probability of both formulas are the same\u2014and it must also equal $\\bar{\\mu}(\\varphi\\mid\\mathrm{do}_{\\mathcal{M}}(\\mathbf{X}{=}\\mathbf{x}))$ which we have shown lies between them. ", "page_idx": 26}, {"type": "text", "text": "A.4 Information Theoretic Results of Section 4 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "To prove Theorem 7 and Theorem 9(a), we will need the following Lemma. ", "page_idx": 26}, {"type": "text", "text": "Lemma 12. Consider a set of variables $\\mathbf{Y}\\,=\\,\\{Y_{1},\\ldots,Y_{n}\\}$ , and another (set of) variable(s) $X$ . Every joint distribution $\\mu(X,\\mathbf{Y})$ over the values of $X$ and $\\mathbf{Y}$ satisfies ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\operatorname{I}_{\\mu}(X\\,;\\,Y_{i})\\ \\leq\\ \\operatorname{I}_{\\mu}(X\\,;\\,\\mathbf{Y})+\\sum_{i=1}^{n}\\operatorname{H}_{\\mu}(Y_{i})-\\operatorname{H}_{\\mu}(\\mathbf{Y}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. Since there is only one joint distribution in scope, we omit the subscript $\\mu$ , writing $\\operatorname{I}(-)$ instead of $\\mathrm{I}_{\\mu}(-)$ and $\\mathrm{H}(-)$ instead of $\\mathrm{H}_{\\mu}(-)$ , in the body of this proof. The following fact will also be very useful: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathrm{I}(A;B,C)=\\mathrm{I}(A;C)+\\mathrm{I}(A;B\\mid C)\\qquad\\quad\\mathrm{(the~chain~rule~for~mutual~information).}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We prove this by induction on $n$ . In the base case $[n=1]$ ), we must show that $\\operatorname{I}(X;Y)\\leq\\operatorname{I}(X;Y)+$ $\\mathrm{H}(\\bar{Y})-\\mathrm{H}(Y)$ , which is an obvious tautology. Now, suppose inductively that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{k}\\operatorname{I}(X\\,;\\,Y_{i})\\ \\leq\\ \\operatorname{I}(X\\,;\\,\\mathbf{Y}_{1:k})+\\sum_{i=1}^{k}\\operatorname{H}(Y_{i})-\\operatorname{H}(\\mathbf{Y}_{1:k})\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for some $k<n$ , where $\\mathbf{Y}_{1:k}=(Y_{1},\\ldots,Y_{k})$ . We now prove that the analogue for $k+1$ also holds. Some calculation reveals: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathrm{I(\\boldsymbol{X};Y_{k+1})}}}\\\\ &{}&{=\\mathrm{I(\\boldsymbol{X};\\mathbf{Y}_{1:k+1})}-\\mathrm{I(\\boldsymbol{X};\\mathbf{Y}_{1:k}~|~\\boldsymbol{Y}_{k+1})}}\\\\ &{}&{\\leq\\mathrm{I(\\boldsymbol{X};\\mathbf{Y}_{1:k+1})}}\\\\ &{}&{=\\mathrm{I(\\boldsymbol{X};Y_{k+1}~|~\\mathbf{Y}_{1:k})}+\\mathrm{I(\\mathbf{Y}_{1:k};Y_{k+1})}}\\\\ &{}&{=\\mathrm{I(\\boldsymbol{X};Y_{k+1}~|~\\mathbf{Y}_{1:k})}+\\mathrm{I(\\mathbf{Y}_{1:k};Y_{k+1})}}\\\\ &{}&{=\\left(\\begin{array}{l}{\\mathrm{I(\\boldsymbol{X};\\mathbf{Y}_{1:k+1})}+\\mathrm{H(\\boldsymbol{Y}_{k+1})}-\\mathrm{H(\\mathbf{Y}_{1:k+1})}}\\\\ {-\\mathrm{I(\\boldsymbol{X};\\mathbf{Y}_{1:k})}}\\end{array}\\right)}\\end{array}\\,\\left[\\begin{array}{l}{\\mathrm{left:~one~more~MI~chain~rule~(5);}}\\\\ {\\mathrm{~right:~defn~of~mutual~infomation~}}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Observe: adding this inequality to our inductive hypothesis $\\left(\\mathrm{IH}_{k}\\right)$ ) yields $(\\mathrm{IH}_{k+1}$ )! So, by induction, the lemma holds for all $k$ . \u53e3 ", "page_idx": 26}, {"type": "text", "text": "Theorem 7. If $\\mu\\vDash\\diamond\\triangleleft$ , then $I D e f_{A}(\\mu)\\leq0$ . ", "page_idx": 27}, {"type": "text", "text": "Proof. Suppose that $\\mu\\,\\vDash\\,\\diamond\\!A$ , meaning that there is a witness $\\nu(\\mathcal{X},\\mathcal{U})$ that extends $\\mu$ , and has properties (a-c) of Definition 2. For each hyperarc a, since $\\nu\\,\\left|=(S_{a},U_{a}\\right)\\to T_{a}$ , we have $\\mathrm{H}_{\\nu}(T_{a}\\mid$ $\\bar{S}_{a},\\bar{U}_{a})=0$ , and so ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathrm{H}_{\\mu}(T_{a}\\mid S_{a})=\\mathrm{H}_{\\nu}(T_{a}\\mid S_{a},U_{a})+\\mathrm{I}_{\\nu}(T_{a};U_{a}\\mid S_{a})=\\mathrm{I}_{\\nu}(T_{a};U_{a}\\mid S_{a}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, we compute ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{u\\in A}\\mathrm{H}_{\\mu}\\left(T_{u}\\ |\\ S_{u}\\right)=\\displaystyle\\sum_{u\\in A}\\mathrm{L}_{\\nu}(U_{u};T_{a}\\ |\\ S_{u})}&{}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\sum_{u\\in A}\\mathrm{L}_{\\nu}(U_{u};T_{a},S_{u})-\\mathrm{I}_{\\nu}(U_{u};S_{u})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\times\\displaystyle\\sum_{u\\in A}\\mathrm{L}_{\\nu}(U_{u};T_{a},S_{u})}\\\\ &{\\leq\\displaystyle\\sum_{u\\in A}\\mathrm{L}_{\\nu}(U_{u};T_{a},S_{u})}\\\\ &{\\leq\\displaystyle\\sum_{u\\in A}\\mathrm{L}_{\\nu}(U_{u};\\chi)}\\\\ &{\\leq\\displaystyle\\mathrm{L}_{\\nu}(X;U_{u})+\\sum_{u\\in A}\\mathrm{H}_{\\nu}(U_{u})-\\mathrm{H}_{\\nu}(U_{u}^{\\prime})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad=\\displaystyle\\mathrm{L}_{\\nu}(X;U)}\\\\ &{=\\displaystyle\\mathrm{I}_{\\nu}(X;U)}\\\\ &{\\leq\\mathrm{H}_{\\nu}(X)=\\mathrm{H}_{\\nu}(X).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, $I D e f_{A}(\\mu)\\leq0$ . ", "page_idx": 27}, {"type": "text", "text": "Proposition 8. $\\operatorname{QIM}\\!I n c_{A}(\\mu)\\geq0,$ , with equality iff $\\mu=\\diamond A.$ ", "page_idx": 27}, {"type": "text", "text": "Proof. The first term in the definition of QIMInc be written as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\Big(-\\mathrm{H}_{\\nu}(\\mathcal{U})+\\sum_{a\\in\\mathcal{A}}\\mathrm{H}_{\\nu}(U_{a})\\Big)=\\mathbb{E}\\left[\\log\\frac{\\nu(\\mathcal{U})}{\\prod_{a}\\nu(U_{a})}\\right]\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and is therefore the relative entropy between $\\nu(\\mathcal{U})$ and the independent product distribution $\\textstyle\\prod_{a\\in{\\cal A}}\\nu(U_{a})$ . Thus, it is non-negative. The remaining terms of $\\mathrm{QIM}I n c_{A}(\\mu)$ , are all conditional entropies, and hence non-negative as well. Thus $\\operatorname{QIM}\\!I n c_{\\mathcal{A}}(\\mu)\\geq0$ . ", "page_idx": 27}, {"type": "text", "text": "Now, suppose $\\mu$ is $\\mathrm{s}2$ -comaptible with $\\boldsymbol{\\mathcal{A}}$ , i.e., there exists some $\\nu(\\mathcal{U},\\mathcal{X})$ such that (a) $\\nu(\\mathcal{X})=\\mu(\\mathcal{X})$ , (b) $\\mathrm{H}_{\\nu}(T_{a}|S_{a},U_{a})=0$ , and (d) $\\{U_{a}\\}_{a\\in A}$ are mutually independent. Then clearly $\\nu$ satisfies the condition under the infemum, every $\\mathrm{H}_{\\nu}(T_{a}|S_{a},U_{a})$ is zero. It is also immediate that the final term is zero as well, because it equals $\\begin{array}{r}{D(\\nu(\\mathcal{U})\\parallel\\prod_{a}\\nu(U_{a}))}\\end{array}$ , and $\\begin{array}{r}{\\nu(\\mathcal{U})=\\prod_{a}\\nu(U_{a})}\\end{array}$ , per the definition of mutual independence. Thus, $\\nu$ witnesses that $\\operatorname{QIM}\\!I\\!n c_{(\\mathcal{A},\\lambda)}=0$ . ", "page_idx": 27}, {"type": "text", "text": "Conversely, suppose $\\operatorname{QIM}\\!I\\!n c_{({\\cal A},\\lambda)}=0$ . Because the feasible set is closed and bounded, as is the function, the infemum is achieved by some joint distribution $\\nu(\\mathcal{X},\\mathcal{A})$ with marginal $\\mu(\\mathcal{X})$ . In this distribution $\\nu$ , we know that every $\\mathrm{H}_{\\nu}(T_{a}|S_{a},U_{a})=0$ and $\\begin{array}{r}{D(\\nu(\\mathcal{U})\\parallel\\prod_{a}\\nu(U_{a}))=0.}\\end{array}$ \u2014 because if any of these terms were positive, then the result would be positive as well. So $\\nu$ satisfies (a) and (b) by definition. And, because relative entropy is zero iff its arguments are identical we have $\\begin{array}{r}{\\nu(\\mathcal{U})=\\prod_{a}\\nu(U_{a})}\\end{array}$ , so the $U_{a}$ \u2019s are mutually independent, and $\\nu$ satisfies (d) as well. \u53e3 ", "page_idx": 27}, {"type": "text", "text": "Theorem 9. ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "(a) $I f({\\mathcal{X}},A)$ is a hypergraph, $\\mu(\\mathcal{X})$ is a distribution, and $\\nu(\\mathcal{X},\\mathcal{U})$ is an extension of $\\nu$ to additional variables $\\mathcal{U}=\\{U_{a}\\}_{a\\in\\mathcal{A}}$ indexed by $\\boldsymbol{\\mathcal{A}}$ , then: ", "page_idx": 27}, {"type": "equation", "text": "$$\nI D e f_{A}(\\mu)\\leq\\mathrm{QIM}I n c_{A}(\\mu)\\leq I D e f_{A^{\\dagger}}(\\nu).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "(b) For all $\\mu$ and $\\mathcal{A}$ , there is a choice of $\\nu$ that achieves the upper bound. That is, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathrm{QIM}I n c_{\\mathcal{A}}(\\mu)=\\operatorname*{min}\\Big\\{\\,I D e f_{\\mathcal{A}^{\\dagger}}(\\nu):\\begin{array}{c}{\\nu\\in\\Delta\\mathrm{V}(\\mathcal{X},\\mathcal{U})}\\\\ {\\nu(\\mathcal{X})=\\mu(\\mathcal{X})}\\end{array}\\Big\\}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. Part (a). The left hand side of the theorem $(I D e f_{A}(\\nu)\\leq\\mathrm{QIM}I n c_{A}(\\mu))$ is a strengthening of the argument used to prove Theorem 7. Specifically, let $\\nu^{*}$ be a minimizer of the optimization problem defining QIMInc We calculate ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{~\\displaystyle=~\\left(\\sum_{a\\inA}H_{\\nu^{*}}({\\cal T}_{a}\\mid{\\cal S}_{a},{\\cal U}_{a})-{\\mathrm{H}}_{\\nu^{*}}({\\cal U})+\\sum_{a\\in A}\\mathrm{H}_{\\nu^{*}}({\\cal U}_{a})\\right)-\\left(\\sum_{a\\in A}\\mathrm{H}_{\\mu}({\\cal T}_{a}\\mid{\\cal S}_{a})-\\mathrm{H}_{\\mu}({\\cal X})\\right)}}\\\\ &{\\mathrm{~\\=~\\displaystyle\\sum_{a\\inA}\\left(\\mathrm{H}_{\\nu^{*}}({\\cal T}_{a}\\mid{\\cal S}_{a},{\\cal U}_{a})-\\mathrm{H}_{\\nu^{*}}({\\cal T}_{a}\\mid{\\cal S}_{a})\\right)+\\mathrm{H}_{\\mu}({\\cal X})-\\mathrm{H}_{\\nu^{*}}({\\cal U})+\\sum_{a\\in A}\\mathrm{H}_{\\nu^{*}}({\\cal U}_{a})}}\\\\ &{\\mathrm{~\\=~\\displaystyle-\\sum_{a\\inA}\\mathrm{I}_{\\nu^{*}}({\\cal T}_{a};{\\cal U}_{a}\\mid{\\cal S}_{a})~~\\quad~~+\\mathrm{H}_{\\mu}({\\cal X})-\\mathrm{H}_{\\nu^{*}}({\\cal U})+\\sum_{a\\in A}\\mathrm{H}_{\\nu^{*}}({\\cal U}_{a}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The argument given in the first five lines of the proof of Theorem 7, gives us a particularly convenient bound for the first group of terms on the left: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sum_{a\\in\\mathcal{A}}\\operatorname{I}_{\\nu^{*}}\\left(U_{a};T_{a}\\mid S_{a}\\right)\\leq\\operatorname{I}_{\\nu^{*}}\\left(\\mathcal{X};\\mathcal{U}\\right)+\\sum_{a\\in\\mathcal{A}}\\operatorname{H}_{\\nu^{*}}\\left(U_{a}\\right)-\\operatorname{H}_{\\nu^{*}}\\left(\\mathcal{U}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Substituting this into our previous expression, we have: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{QIM}I n c_{A}(\\mu)-I D e f_{A}(\\mu)}\\\\ &{\\~~\\geq-\\Big(\\operatorname{I}_{\\nu^{*}}(X;\\mathcal{U})+\\displaystyle\\sum_{a\\in\\mathcal{A}}\\operatorname{H}_{\\nu^{*}}(U_{a})-\\operatorname{H}_{\\nu^{*}}(\\mathcal{U})\\Big)+\\operatorname{H}_{\\mu}(\\mathcal{X})-\\operatorname{H}_{\\nu^{*}}(\\mathcal{U})+\\displaystyle\\sum_{a\\in\\mathcal{A}}\\operatorname{H}_{\\nu^{*}}(U_{a})}\\\\ &{~~=\\operatorname{H}_{\\mu}(\\mathcal{X})-\\operatorname{I}_{\\nu^{*}}(\\mathcal{X};\\mathcal{U})}\\\\ &{~~\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The final inequality holds because of our assumption that the marginal $\\nu^{*}(\\mathcal{X})$ equals $\\mu(\\mathcal{X})$ . Thus, $\\mathrm{QIM}I n c_{A}(\\mu)\\ge I D e f_{A}(\\mu)$ , as proimised. ", "page_idx": 28}, {"type": "text", "text": "We now turn to the right hand inequality, and part (b) of the theorem. Recall that $\\nu^{*}$ is defined to be a minimizer of the optimization problem defining QIMInc. For the right inequality $(\\mathrm{QIM}I n c_{A}(\\mu)\\leq$ $I D e f_{A^{\\dagger}}(\\nu))$ of part (a), observe that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I D e f_{A^{\\dagger}}(\\nu)=-\\operatorname{H}_{\\nu}(\\mathcal{X},\\mathcal{U})+\\displaystyle\\sum_{a\\in\\mathcal{A}}\\operatorname{H}_{\\nu}(U_{a})+\\displaystyle\\sum_{a\\in\\mathcal{A}}\\operatorname{H}_{\\nu}(T_{a}|S_{a},U_{a})+\\operatorname{H}_{\\nu}(\\mathcal{X}\\mid\\mathcal{U})}\\\\ &{\\qquad\\qquad=\\Big(-\\operatorname{H}_{\\nu}(\\mathcal{U})+\\displaystyle\\sum_{a\\in\\mathcal{A}}\\operatorname{H}_{\\nu}(U_{a})\\Big)+\\displaystyle\\sum_{a\\in\\mathcal{A}}\\operatorname{H}_{\\nu}\\bigl(T_{a}|S_{a},U_{a}\\bigr)}\\\\ &{\\qquad\\quad\\geq\\Big(-\\operatorname{H}_{\\nu^{*}}(\\mathcal{U})+\\displaystyle\\sum_{a\\in\\mathcal{A}}\\operatorname{H}_{\\nu^{*}}(U_{a})\\Big)+\\displaystyle\\sum_{a\\in\\mathcal{A}}\\operatorname{H}_{\\nu^{*}}\\bigl(T_{a}|S_{a},U_{a}\\bigr)}\\\\ &{\\qquad=\\operatorname{QIM}I n c(\\mu).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "This proves the right hand side of the inequality of part (a). Moreover, because the one inequality holds with equality when $\\nu=\\nu^{*}$ is a minimizer of this quantity (subject to having marginal $\\mu(\\mathcal{X}))$ we have shown part (b) as well. ", "page_idx": 28}, {"type": "text", "text": "B Monotonicity and Undirected Graphical Models ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The fact that (quantitative) PDG inconsistency is monotonic is a powerful reasoning principle that can be used to prove many important inequalities [22]. In this section, we develop a related principle for QIM-compatibility. Here is a direct but not very useful analague: if ${\\mathcal{A}}\\subseteq A^{\\prime}$ and $\\mu\\,\\vDash\\,\\diamond\\!\\!\\diamond A^{\\prime}$ , conclude $\\mu\\vDash\\diamond\\mathcal{A}$ . After all, if $\\mu$ is consistent with a set of independent causal mechanisms, then surely it is consistent with a causal picture in which only a subset of those mechanisms are present and independent. There is a sense in which BNs and MRFs are also monotonic, but in the opposite direction: adding edges to a graph results in a weaker independence statement. We will soon see why. ", "page_idx": 28}, {"type": "text", "text": "Since we use directed hypergraphs, there is actually a finer notion of monotonicity at play. Inputs and ouputs play opposite roles, and they are naturally monotonic in opposite directions. If there is an obvious way to regard an element of $B$ as an element of $B^{\\prime}$ (abbreviated $B\\!\\leftrightarrow\\!B^{\\prime}$ ), and $A^{\\prime}\\hookrightarrow A$ , then a function $f:A\\rightarrow B$ can be regarded as one of type $A^{\\prime}\\rightarrow B^{\\prime}$ . This is depicted to the right. The same principile applies in ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{A\\stackrel{f}{\\longrightarrow}B}}\\\\ {{\\stackrel{\\uparrow}{\\downarrow}\\,\\stackrel{\\downarrow}{\\longrightarrow}\\,\\stackrel{\\uparrow}{\\downarrow}}}\\\\ {{A^{\\prime}\\stackrel{\\ldots}{\\longrightarrow}B^{\\prime}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "our setting. If $\\mathbf{X}$ and $\\mathbf{Z}$ are sets of variables and $\\mathbf{X}\\subseteq\\mathbf{Z}$ , then $\\mathrm{V}(\\mathbf{Z}){\\overset{=}{\\hookrightarrow}}\\mathrm{V}(\\mathbf{\\bar{X}})$ , by restriction. It follows, for example, that any mechanism by which $X$ determines $(Y,Y^{\\prime})$ can be viewed as a mechanism by which $(X,X^{\\prime})$ determines $Y$ . The general phenomenon is captured by the following. ", "page_idx": 29}, {"type": "text", "text": "Definition 6. If $\\mathcal{A}=\\{S^{\\underline{{{a}}}}\\rangle T\\}_{a}$ , $\\mathcal{A}^{\\prime}=\\{S^{\\prime}{\\overset{a^{\\prime}}{\\to}}T^{\\prime}\\}_{a^{\\prime}}$ , and there is an injective map $\\iota:{\\mathcal{A}}^{\\prime}\\!\\to\\!{\\mathcal{A}}$ such that $T_{a}^{\\,\\prime}\\subseteq T_{\\iota(a)}$ and $S_{a}^{\\prime}\\supseteq S_{\\iota(a)}$ for all $a\\in A^{\\prime}$ , then $\\mathcal{A^{\\prime}}$ is a weakening of $\\boldsymbol{\\mathcal{A}}$ (written $A{\\sim}A^{\\prime}$ ). \u25a1 ", "page_idx": 29}, {"type": "text", "text": "Proposition 13. If $A\\sim A^{\\prime}$ and $\\mu\\vDash\\diamond\\triangleleft,$ , then $\\mu\\vDash\\diamond\\triangleleft^{\\prime}$ . ", "page_idx": 29}, {"type": "text", "text": "Proposition 13 is strictly stronger than the simple monotonicity mentioned at the beginning of the section, because a hyperarc with no targets is vacuous, so removing all targets of a hyperarc is equivalent to deleting it. It also explains why BNs and MRFs are arguably anti-monotonic: adding $X\\rightarrow Y$ to a graph $G$ means adding $X$ to the sources the hyperarc whose target is $Y$ , in $A_{G}$ . ", "page_idx": 29}, {"type": "text", "text": "As mentioned in the main body of the paper, the far more important consequence of this result is that it helps us begin to understand what QIM-compatibility means for cyclic hypergraphs. For the reader\u2019s convenience, we now restate the examples in the main text, which are really about monotonicity.. ", "page_idx": 29}, {"type": "text", "text": "Example 3. Every $\\mu(X,Y)$ is compatible with ${\\underset{(X)\\in\\Sigma}{X}}{\\underset{\\Sigma}{\\rightleftharpoons}}{\\underset{(Y)}{\\rightleftharpoons}}$ . This is because this cycle is weaker than a hypergraph that can already represent any distribution, i.e., $\\to\\!(X)\\!\\to\\!(Y)\\ \\,\\,\\to\\,\\,(X)\\!{\\underset{\\textstyle\\rightleftarrows}{\\rightleftarrows}}\\!(Y)$ $\\triangle$ . ", "page_idx": 29}, {"type": "text", "text": "Example 4. What $\\mu(X,Y,Z)$ are compatible with the 3-cycle shown, on the right? By monotonicity, among them must be all distributions consistent with a linear chain ${\\to}X{\\to}Y{\\to}Z$ . Thus, any distribution in which two variables are conditionally independent given the third is compatible with the 3-cycle. Are there any distributions that are not compatible with this hypergraph? It is not obvious. We return to this in Section 4. $\\triangle$ ", "page_idx": 29}, {"type": "image", "img_path": "RE5LSV8QYH/tmp/e5006d2597d934ae6d7a302a5e0b6228664b2f492835ce4c212025341600df84.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "$\\underline{{\\boldsymbol{B}}}$ Because QIM-compatibility applies to cyclic structures, one might wonder if it also captures the independencies of undirected models. Undirected edges $A\\!-\\!B$ are commonly identified with a (cylic) pair of directed edges $\\{A{\\to}B,B{\\to}A\\}$ , as we have $\\circleddash$ $\\underline{{\\boldsymbol{C}}})$ implicitly done in defining $A_{G}$ . In this way, undirected graphs, too, naturally correspond to directed hypergraphs. For example, $G=A{-}B{-}C$ ", "page_idx": 29}, {"type": "text", "text": "C corresponds to the hypergraph $A_{G}$ shown on the left. Compatibility with $A_{G}$ , however, does not coincide with any of the standard Markov properties corresponding to $G$ [13]. This may appear to be a flaw in Definition 2 (QIM-compatibility), but it is unavoidable. While both BNs and MRFs are monotonic, it is impossible to capture both classes with a monotonic definition. ", "page_idx": 29}, {"type": "text", "text": "Theorem 14. It is possible to define a relation |=\u2022 between distributions $\\mu$ and directed hypergraphs $\\boldsymbol{\\mathcal{A}}$ satisfying any two, but not all three, of the following. ", "page_idx": 29}, {"type": "text", "text": "(monotonicity) $I f\\,\\mu$ |=\u2022 $\\boldsymbol{\\mathcal{A}}$ and $A\\sim A^{\\prime}$ , then $\\mu\\left\\vert\\right\\vert^{\\bullet}A^{\\prime}$ . ", "page_idx": 29}, {"type": "text", "text": "(positive BN capture) $I f\\mu$ satisfies the independencies ${\\mathcal{T}}(G)$ of a dag $G_{i}$ , then $\\mu\\models\\mathcal{A}_{G}$ . ", "page_idx": 29}, {"type": "text", "text": "(negative MRF capture) If $\\mu\\!\\left|\\right|\\geq\\!A_{G}$ for an undirected directed graph $G$ , then $\\mu$ has one of the Markov properties with respect to $G$ . ", "page_idx": 29}, {"type": "text", "text": "The proof is a direct and easy-to-visualize application of monotonicity (Proposition 13). Assume montonicity and positive BN capture. Let $\\mu_{x o r}(A,B,C)$ be the joint distribution in which $A$ and $C$ are independent fair coins, and $B=A\\oplus C$ is their parity. We then have: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mu_{x o r}=\\underbrace{\\downarrow\\downarrow}_{(A)}\\underbrace{\\frac{(B)}{\\uparrow}}_{(C)}\\downarrow\\ \\rightarrow\\ \\underbrace{\\downarrow\\downarrow}_{(A)}\\underbrace{\\downarrow\\downarrow}_{(C)}=\\ A_{A-B-C}.\\quad\\mathrm{But}\\,\\mu_{x o r}\\,\\forall\\,A\\perp C\\mid B.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We emphasize that Theorem 14 has implications for the qualitative semantics of any graphical model (even if one were to reject the definition QIM-compatibility). We now look into the implications for some lesser-known graphical models, which may appear not to comply with Theorem 14. ", "page_idx": 29}, {"type": "text", "text": "Dependecny Networks To readers familiar with dependency networks (DNs) [8], Theorem 14 may raise some conceptual issues. When $G$ is an undirected graph, $A_{G}$ is the structure of a consistent DN. ", "page_idx": 29}, {"type": "text", "text": "The semantics of such a DN, which intuitively describe an independent mechanism on each hyperarc, coincide with the MRFs for $G$ (at least for positive distributions). In more detail, DN semantics are given by the fixed point of a markov chain that repeatedly generates independent samples along the hyperarcs of $A_{G}$ for some (typically cyclic) directed graph $G$ . The precise definition requires an order in which to do sampling. Although this choice doesn\u2019t matter for the \u201cconsistent DNs\u201d that represent MRFs, it does in general. With a fixed sampling order, the DN is monotonic and captures MRFs, but can represent only BNs for which that order is a topological sort. ", "page_idx": 30}, {"type": "text", "text": "C Information Theory, PDGs, and QIM-Compatibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "C.1 More Detailed Primer on Information Theory ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We now expand on the fundemental information quantities introduced at the beginning of Section 4. Let $\\mu$ be a probability distribution, and be $X,Y,Z$ be (sets of) discrete random variables. The entropy of $X$ is the uncertainty in $X$ , when it is distributed according to $\\mu$ , as measured by the number of bits of information needed (in expectation) needed to determine it, if the distribution $\\mu$ is known. It is given by ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathrm{H}_{\\mu}(X):=\\sum_{x\\in\\mathrm{V}(X)}\\mu(X\\!=\\!x)\\log{\\frac{1}{\\mu(X\\!=\\!x)}}\\qquad=-\\operatorname{\\mathbb{E}}[\\log\\mu(X)],\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and a few very important properties; chief among them, $\\operatorname{H}_{\\mu}(X)$ is non-negative, and equal to zero iff $X$ is a constant according to $\\mu$ . The \u201cjoint entropy\u201d $\\operatorname{H}({\\dot{X}},Y)$ is just the entropy of the combined variable $(X,Y)$ whose values are pairs $(x,y)$ for $x\\,\\in\\,\\mathrm{V}(X),y\\,\\in\\,\\mathrm{V}(Y)$ ; this is the same as the entropy of the variable $X\\cup Y$ when $X$ and $Y$ are themselves sets of variables. ", "page_idx": 30}, {"type": "text", "text": "The conditional entropy of $Y$ given $X$ measures the uncertainty present in $Y$ if one knows the value of $X$ (think: the information in $Y$ but not $X$ ), and is equivalently defined as any of the following three quantities: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}{\\mathrm{H}_{\\mu}(Y|X):=}&{{}\\mathbb{E}[\\mathrm{~log}\\,^{1}/\\mu(Y|X)\\,]}&{{}=\\mathrm{H}_{\\mu}(X,Y)-\\mathrm{H}_{\\mu}(X)}&{{}=\\underset{x\\sim\\mu(X)}{\\mathbb{E}}[\\mathrm{~H}_{\\mu|X=x}(Y)\\,].}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "The mutual information $\\operatorname{I}(X;Y)$ , and its conditional variant $\\operatorname{I}(X;Y|Z)$ , are given, respectively, by ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathrm{I}_{\\mu}(X;Y):=\\operatorname{\\mathbb{E}}\\Big[\\log\\frac{\\mu(X,Y)}{\\mu(X)\\mu(Y)}\\Big],\\quad\\mathrm{and}\\quad\\mathrm{I}(X;Y|Z):=\\operatorname{\\mathbb{E}}\\Big[\\log\\frac{\\mu(X,Y,Z)\\mu(Z)}{\\mu(X,Z)\\mu(Y,Z)}\\Big].\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "The former is non-negative and equal to zero iff $\\mu\\vDash X\\perp\\Sigma\\,Y$ , and the latter is non-negative and equal to zero iff $\\mu\\vDash\\mathbf{\\bar{{X}}}\\perp\\v{|}Y\\mid Z$ . All of these quantities are purely \u201cstructural\u201d or \u201cqualitative\u201d in the sense that they are invariant to relabelings of values, and ", "page_idx": 30}, {"type": "text", "text": "Just as conditional entropy can be written as a linear combination of unconditional entropies, so too can conditional mutual information be written as a linear combination of unconditional mutual informations: $\\operatorname{I}(X;Y|Z)=\\operatorname{I}(X;(Y,Z))-\\operatorname{I}(X;Z)$ . Thus conditional quantities are easily derived from the unconditional ones. But at the same time, the unconditional versions are clearly special cases of the conditional ones; for example, $\\operatorname{H}_{\\mu}(X)$ is clearly the special case of $\\operatorname{H}(X|Z)$ when $Z$ is a constant (e.g., $Z=\\emptyset$ ). Furthermore, entropy and mutual information are also interdefinable and generated by linear combinations of one another. It is easy to verify that $\\operatorname{I}_{\\mu}(X;Y)=\\operatorname{H}_{\\mu}(X)+$ $\\mathrm{H}_{\\mu}(\\bar{Y})\\,-\\,\\mathrm{H}(X,Y)$ and $\\mathrm{I}_{\\mu}(X;Y|Z)\\;=\\;\\mathrm{H}_{\\mu}(X|Z)\\,+\\,\\mathrm{H}_{\\mu}({\\dot{Y}}|Z)\\,-\\,\\mathrm{H}(X,Y|\\dot{Z})$ , and thus mutual information is derived from entropy. Yet on the other hand, $\\operatorname{I}_{\\mu}(Y;Y)=\\operatorname{H}_{\\mu}(Y)$ and $\\operatorname{I}_{\\mu}(Y;Y|X)=$ $\\mathrm{H}_{\\mu}(Y|X)$ \u2014thus entropy is a special case of mutual information. ", "page_idx": 30}, {"type": "text", "text": "C.2 Structural Deficiency: More Motivation, and Examples ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "To build intuition for $I D e f$ , which characterizes our bounds in Section 4, we now visualize the vector $\\mathbf{v}_{A}$ for various example hypergraphs. ", "page_idx": 30}, {"type": "text", "text": "\u2022 Subfigures 2a, 2b, and 2c show how adding hyperarcs makes distriutions more deterministic. When $\\boldsymbol{\\mathcal{A}}$ is the empty hypergraph, $I D e f$ reduces to negative entropy, and so prefers distributions that are \u201cmaximally uncertain\u201d (e.g., Subfigures 2a and 2d). For this empty but all distributions $\\mu$ have negative IDef ${\\bf\\nabla}_{\\bf A}(\\mu)\\leq0$ . In the definition of $I D e f$ , each hyperarc $X\\rightarrow Y$ is compiled to a \u201ccost\u201d $H(Y|X)$ for uncertainty in $Y$ given $X$ . One can see this visually in Figure 2 as a red crescent that\u2019s added to the information profile as we move from 2d to 2e to 2f. ", "page_idx": 30}, {"type": "image", "img_path": "RE5LSV8QYH/tmp/8e670421ab877d65a4408283bfda3e511b5e9844f0713a9fd693a270c26f418f.jpg", "img_caption": ["Figure 2: Illustrations of the structural deficiency $I D e f_{A}$ underneath drawn underneath their associated hypergraphs $\\{G_{i}\\}$ . Each circle represents a variable; an area in the intersection of circles $\\{C_{j}\\}$ but outside of circles $\\{D_{k}\\}$ corresponds to information that is shared between all $C_{j}$ \u2019s, but not in any $D_{k}$ . Variation of a candidate distribution $\\mu$ in a green area makes its qualitative fti better (according to $I D e f$ ), while variation in a red area makes its qualitative fti worse; grey is neutral. Only the boxed structures in blue, whose $I D e f$ can be seen as measuring distance to a particular set of (conditional) independencies, are expressible as BNs. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "\u2022 Some hypergraphs (see Figures 2b and 2h) are indiscriminate, in the sense that every distribution gets the same score (of zero, because a point mass $\\delta$ always has $S D e f_{A}(\\delta)\\,\\dot{=}$ 0). Such a graph has a structure such that any distribution can be precisely encoded by the process in (b). As shown here and also in Richardson and Halpern [23], IDef can also indicate independencies and conditional independencies, illustrated respectively in Subfigures $2\\mathrm{g}$ and 2i.   \n\u2022 For more complex structures, structural information deficiency IDef can represent more than independence and dependence. The cyclic structures in Examples 3 and 4, correspond to the structural deficiencies pictured in Subfigures 2f and 2j, respectively, which are functions that encourage shared information between the three variables. ", "page_idx": 31}, {"type": "text", "text": "C.3 Counter-Examples to the Converse of Theorem 7 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In light of Example 6 and its connections to $I D e f$ through Theorem 7, one might hope this criterion is not just a bound, but a precise characterization of the distributions that are QIM-compatible with the 3-cycle. Unfortunately, it does not, and the converse of Theorem 7 is false. ", "page_idx": 31}, {"type": "text", "text": "Example 7. Suppose $\\mu(X,Y,Z)\\,=\\,\\mathrm{Unif}(X,Z)\\delta\\mathrm{id}(Y|X)$ and ${\\mathcal{A}}\\,=\\,\\{\\rightarrow\\,X,\\rightarrow\\,Y\\}$ , where all variables are binary. Then $I D e f_{A}(\\mu)\\!=\\!0$ , but $X$ and $Y$ are not independent. $\\triangle$ ", "page_idx": 31}, {"type": "text", "text": "Here is another counter-example, of a very different kind. ", "page_idx": 31}, {"type": "text", "text": "Example 8. Suppose $A,B,C$ are binary variables. It can be shown by enumeration (see appendix) that no distribution supported on seven of the eight possible joint settings of of $\\mathrm{V}(A,B,C)$ can be QIM-compatible with the 3-cycle $\\mathcal{A}_{3\\circ}$ . Yet it is easy to find examples of such distributions $\\mu$ that have positive interaction information $\\operatorname{I}(A;B;C)$ , and thus $I D e f_{\\mu}(A_{3\\circ})\\leq0$ for such distributions. $\\triangle$ ", "page_idx": 31}, {"type": "text", "text": "D QIM-Compatibility Constructions and Counterexamples ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "We now give a counterexample to a simpler previously conjectured strengthening of Theorem 2, in which part (a) is an if-and-only-if. This may be surprising. In the unconditional case, it is true that, two arcs $\\{\\cdot^{1}\\cdot X,{\\xrightarrow{2}}X\\}$ precisely encode that $X$ is a constant, as illustrated by Example 2. The following, slightly more general result, is an immediate correlary of Theorem 2(c). ", "page_idx": 32}, {"type": "text", "text": "Proposition 15. $\\mu\\vDash\\langle\\upharpoonright\\Pi\\sqcup\\{\\xrightarrow{1}X,\\xrightarrow{2}X\\}$ if and only if $\\mu\\vDash\\diamond\\mathcal{A}$ and $\\mu\\vDash\\varnothing\\to X$ . ", "page_idx": 32}, {"type": "text", "text": "One can be forgiven for imagining that the conditional case would be analogous\u2014that QIMcompatibility with a hypergraph that has two parallel arcs from $X$ to $Y$ would imply that $Y$ is a function of $X$ . But this is not the case. Furthermore, our counterexample also shows that neither of the two properties we consider in the main text (requiring that $\\boldsymbol{\\mathcal{A}}$ is partitional, or that the QIMcompatibility with $\\mu$ is even) are enough to ensure this. That is, there are partitional graphs $\\boldsymbol{\\mathcal{A}}$ such that $\\dot{\\mu}\\overset{\\vartriangle}{\\=}A$ but $\\mu\\not\\in\\{\\neg\\neg\\}\\sqcup\\{X^{\\bot}\\neg\\},X^{\\corner}Y\\}$ . ", "page_idx": 32}, {"type": "text", "text": "Example 9. We will construct a witness of SIM-compatibility for the hypergraph ", "page_idx": 32}, {"type": "equation", "text": "$$\nA:=\\underset{\\kappa}{\\overset{\\underset{\\mathrm{()}}{\\sum}}{\\sum}}\\underset{\\mathrm{0}}{\\overset{\\underset{\\mathrm{()}}{\\sum}}{\\sum}}\\frac{\\underset{\\mathrm{()}}{\\mathrm{)}}}{\\underset{\\mathrm{()}}{\\sum}},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "in which $Y$ is not a function of $X$ , which for $n=3$ will disprove the analogue of Theorem 2 for the partitional context $\\mathcal{A^{\\prime}}$ equal to the 2-cycle. ", "page_idx": 32}, {"type": "text", "text": "Let $\\mathcal{U}=(U_{0},U_{1},\\ldots,U_{n})$ be a vector of $n$ mutually independent random coins, and $A$ is one more independent random coin. For notational convenience, define the random vector $\\mathbf{U}:=(U_{0},\\ldots,U_{n})$ consisting of all variables $U_{i}$ except for $U_{0}$ . Then, define variables $X$ and $Y$ according to: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{X:=(A\\oplus U_{1},\\,\\dots,A\\oplus U_{n},\\,\\,U_{0}\\oplus U_{1},U_{0}\\oplus U_{2},\\,\\dots,U_{0}\\oplus U_{n})}\\\\ &{\\quad=(A\\oplus\\mathbf{U},\\,\\,U_{0}\\oplus\\mathbf{U})}\\\\ &{Y:=(A,U_{0}\\oplus\\mathbf{U})=(A,\\,U_{0}\\oplus U_{1},U_{0}\\oplus U_{2},\\,\\dots,U_{0}\\oplus U_{n}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where and the operation $Z\\oplus\\mathbf{V}$ is element-wise xor (or addition in $\\mathbb{F}_{2}^{n}$ ), after implicitly converting the scalar $Z$ to a vector by taking $n$ copies of it. Call the resulting distribution $\\nu(X,Y,\\mathcal{U})$ . ", "page_idx": 32}, {"type": "text", "text": "It we now show that $\\nu$ witnesses that its marginal on $X,Y$ is QIM-compatible with $\\boldsymbol{\\mathcal{A}}$ , which is straightforward. ", "page_idx": 32}, {"type": "text", "text": "(b) $\\boldsymbol{\\mathcal{U}}$ are mutually independent by assumption; ", "page_idx": 32}, {"type": "text", "text": "(c.0) $Y=(A,\\mathbf{B})$ and $U_{0}$ determine $X$ according to: ", "page_idx": 32}, {"type": "equation", "text": "$$\n{\\begin{array}{r l r l}&{g(A,\\mathbf{B},U_{0})=(A\\oplus U_{0}\\oplus\\mathbf{B},\\ \\mathbf{B})}\\\\ &{\\qquad\\qquad=(A\\oplus U_{0}\\oplus U_{0}\\oplus\\mathbf{U},\\ U_{0}\\oplus\\mathbf{U})\\qquad}&&{{\\mathrm{since~}}\\mathbf{B}=U_{0}\\oplus\\mathbf{U}}\\\\ &{\\qquad\\qquad=(A\\oplus\\mathbf{U},U_{0}\\oplus\\mathbf{U})=X}\\end{array}}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "$_{(\\mathsf{c}.1-n)}$ for $i\\in\\{1,\\ldots,n\\}$ , $U_{i}$ and $X=({\\bf V},{\\bf B})$ together determine $Y$ according to ", "page_idx": 32}, {"type": "equation", "text": "$$\nf_{i}(\\mathbf{V},\\mathbf{B},U_{i}):=(V_{i}\\oplus U_{i},\\ \\mathbf{B})=(A\\oplus U_{i}\\oplus U_{i},\\ U_{0}\\oplus\\mathbf{U})=Y.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "In addition, this distribution $\\nu(\\mathcal{U},X,Y)$ satisfies condition ", "page_idx": 32}, {"type": "text", "text": "(d) $\\begin{array}{r}{\\nu(X,Y\\mid\\mathcal{U})=\\frac{1}{2}\\mathbb{1}[g(Y,U_{0})=X]\\prod_{i=1}^{n}\\mathbb{1}[f_{i}(X,U_{i})=Y],}\\end{array}$ , since, for all joint settings of $\\boldsymbol{\\mathcal{U}}$ , there are two possible values of $(X,Y)$ , corresponding to the two values of $A$ , and both happen with probability $\\frac{1}{2}$ . ", "page_idx": 32}, {"type": "text", "text": "Thus, we have constructed a distribution that witnessing the fact that $\\mu(X,Y)\\vDash\\v{A}$ . ", "page_idx": 32}, {"type": "text", "text": "Yet, observe that $X$ alone does not determine $Y$ in this distribution, because $X$ alone is not enough to determine $A$ (without also knowing some $U_{i}$ ). ", "page_idx": 32}, {"type": "text", "text": "For those who are interested, observe that the bound of Theorem 7 tells that we must satisfy ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{0\\geq I D e f_{A}(\\mu)=-\\operatorname{H}_{\\mu}(X,Y)+n\\operatorname{H}_{\\mu}(Y\\mid X)+\\operatorname{H}_{\\mu}(X\\mid Y)}}\\\\ {{=-\\operatorname{I}_{\\mu}(X;Y)+(n-1)\\operatorname{H}_{\\mu}(Y\\mid X)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Indeed, this distribution has information profile ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathrm{H}(X\\mid Y)=1\\,\\mathrm{bit},\\qquad\\operatorname{I}(X;Y)=n\\,\\mathrm{bits},\\qquad\\mathrm{H}(Y\\mid X)=1\\,\\mathrm{bit},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and so $I D e f_{A}(\\mu)=-1$ bit. Intuitively, this one missing bit corresponds to the value of $A$ that is not determined by the structure of $\\boldsymbol{\\mathcal{A}}$ . $\\triangle$ ", "page_idx": 33}, {"type": "text", "text": "E From Causal Models to Witnesses ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "We now return to the \u201ceasy\u201d direction of the correspondence between QIM-compatibility witnesses and causal models, mentioned at the beginning of Section 3.2. Given a (generalized) randomized PSEM $\\mathcal{M}$ , we now show that distributions $\\nu\\in\\{\\mathbb{M}\\}$ , are QIM-compatibility witness showing that the marginals of $\\nu$ are QIM-compatible with the hypergraph $\\mathcal{A}_{\\mathcal{M}}$ . More formally: ", "page_idx": 33}, {"type": "text", "text": "Proposition 16. If $\\mathcal{M}=(M{=}(\\mathcal{U},\\mathcal{V},\\mathcal{F}),P)$ is a randomized PSEM, then every $\\nu\\in\\{\\mathbb{M}\\}$ witnesses the QIM-compatibility of its marginal on its exogenous variables, with the dependency structure of $\\mathcal{M}$ . That is, for all $\\nu\\in\\{\\mathbb{M}\\}$ and $\\mathcal{D}\\subseteq\\mathcal{U}\\cup\\mathrm{V}$ , $\\bar{\\nu}(\\mathcal{Y})\\vDash\\odot\\mathcal{A}_{\\mathcal{M}}$ . ", "page_idx": 33}, {"type": "text", "text": "The proof is straightforward: by definition, if $\\nu\\in\\{\\mathbb{M}\\}$ , then it must satisfy the equations, and so automatically fulfills condition (c). Condition (a) is also satisfied trivially, by assumption: the distribution we\u2019re considering is defined to be a marginal of $\\nu$ . Finally, (b) is also satisfied by construction: we assumed that $\\mathcal{U}_{A}=\\{U_{a}\\}_{a\\in\\mathcal{A}}$ are independent. ", "page_idx": 33}]