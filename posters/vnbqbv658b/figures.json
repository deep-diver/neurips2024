[{"figure_path": "VNbQbv658b/figures/figures_0_1.jpg", "caption": "Figure 1: The overview of CoVoMix framework, which consists of a multi-stream text-to-semantic model, a conditional flow-matching based acoustic model for mixed mel-spectrogram generation, and a HiFi-GAN based vocoder for waveform production.", "description": "The figure provides a high-level overview of the CoVoMix framework, illustrating its three main components: 1) a multi-stream text-to-semantic model that processes dialogue text and generates separate semantic representations for each speaker; 2) a conditional flow-matching based acoustic model that takes the semantic representations and speaker prompts as input to generate a mixed mel-spectrogram representing the combined speech of all speakers; and 3) a HiFi-GAN vocoder that converts the mel-spectrogram into a final waveform representing the generated speech.  The example text shows a simple multi-turn dialogue between two speakers, A and B.  The figure clearly depicts the flow of information from text input to final speech output, highlighting the key steps and components of the CoVoMix architecture.", "section": "3 CoVoMix"}, {"figure_path": "VNbQbv658b/figures/figures_4_1.jpg", "caption": "Figure 2: Dialogue transcription preparation. To better demonstrate our method, we use | and emoji to represent [spkchange] and [laughter] tokens.", "description": "This figure shows an example of how dialogue transcriptions are prepared for the CoVoMix model.  The example illustrates how the model handles multiple speakers and various spontaneous behaviors such as laughter.  Each utterance is represented by a numbered segment, separated by a special token '|', representing the transition between speakers, and laughter is represented using an emoji. This highlights the process of preparing data that incorporates natural, conversational speech features.", "section": "4 Experimental Setup"}, {"figure_path": "VNbQbv658b/figures/figures_7_1.jpg", "caption": "Figure 3: Distribution of durations of turn-taking events across models. The blue line and the green line represent the median and mean of each event. The more similar to groundtruth, the better.", "description": "This figure visualizes the distribution of four turn-taking activities in dialogues: intra-speaker pause, inter-speaker silence, overlapped segments, and active speech.  It compares the distributions generated by the CoVoMix and CoVoSingle models against the ground truth. The goal is to show how well each model replicates the natural timing and pauses found in human conversations. The blue and green lines represent the median and mean duration of each activity, respectively.  A closer match to the ground truth indicates more natural and human-like dialogue generation. ", "section": "5.2 Dialogue Metrics"}, {"figure_path": "VNbQbv658b/figures/figures_8_1.jpg", "caption": "Figure 4: Comparison of number and duration of laughter among models", "description": "This figure compares the number and duration of laughter instances generated by three different models: CoVoSingle, CoVoMix, and the ground truth.  It visually represents the performance of each model in generating human-like laughter, a key aspect of natural and spontaneous speech. The bar charts show that CoVoMix achieves a balance between the number and duration of laughter instances similar to the ground truth, while CoVoSingle generates significantly fewer instances of laughter.", "section": "5.2.2 Para-linguistic Behaviors"}, {"figure_path": "VNbQbv658b/figures/figures_8_2.jpg", "caption": "Figure 5: Speech consistency of CoVoSingle and CoVoMix for dialogue generation", "description": "This figure presents a heatmap comparing the speaker consistency between different utterances in dialogues generated by two different methods: CoVoSingle (utterance-level concatenation) and CoVoMix (no concatenation). The heatmaps show the cosine similarity between pairs of utterances.  Lighter shades indicate lower speaker similarity, highlighting that utterance-level concatenation in CoVoSingle leads to inconsistent speaker characteristics, whereas CoVoMix, generating the entire dialogue without concatenation, demonstrates significantly improved consistency in speaker characteristics across various utterances.", "section": "5.2 Dialogue Metrics"}, {"figure_path": "VNbQbv658b/figures/figures_14_1.jpg", "caption": "Figure 6: Comparison of generation pipeline among conventional method and our proposed CoVoSingle and CoVoMix methods. (a) shows the conventional monologue generation process with phoneme representation. (b) shows our proposed CoVoSingle approach for monologue generation. (c) demonstrates the concatenation method for conventional and CoVoSingle models to generate dialogue. (d) shows the architecture of our proposed CoVoMix model for monologue and dialogue generation.", "description": "This figure compares four different methods for speech generation: conventional monologue generation using phoneme representation, CoVoSingle monologue generation, conventional and CoVoSingle dialogue generation by concatenating single utterances, and CoVoMix monologue and dialogue generation.  The conventional method uses a G2P (grapheme-to-phoneme) conversion and a duration predictor before an acoustic model and vocoder generate speech. CoVoSingle utilizes a text-to-semantic model to generate semantic token sequences, followed by an acoustic model and vocoder. The conventional dialogue method concatenates individual speech segments generated using either the conventional or CoVoSingle methods. CoVoMix, in contrast, uses a multi-stream text-to-semantic model to generate simultaneous semantic tokens for multiple speakers, feeding them to an acoustic model which generates a mixed mel-spectrogram before a vocoder produces the final speech. This figure illustrates the fundamental differences in architecture and functionality between the approaches, highlighting CoVoMix's ability to handle multi-speaker dialogues in a more sophisticated and natural-sounding manner.", "section": "3 CoVoMix"}, {"figure_path": "VNbQbv658b/figures/figures_14_2.jpg", "caption": "Figure 7: Text-to-semantic model", "description": "This figure shows the architecture of the text-to-semantic model.  It presents two variations: CoSingle and CoMix.  CoSingle uses a single-stream decoder to produce a single semantic token sequence, while CoMix uses a multi-stream decoder to generate multiple semantic token sequences, each corresponding to a different speaker in a conversation.  This is a crucial element of the CoVoMix system, allowing the model to handle multi-speaker dialogues.", "section": "3.1 Multi-stream Text-to-Semantic Model"}, {"figure_path": "VNbQbv658b/figures/figures_15_1.jpg", "caption": "Figure 8: The acoustic model", "description": "This figure shows the architecture of the acoustic model used in the CoVoMix system.  It illustrates the flow-matching process (a) where an ordinary differential equation updates a sample mel-spectrogram towards the target. The model architecture (b) shows how the vector field estimator is used with transformer encoders to generate mixed mel-spectrograms for different configurations: VoSingle (single-speaker), VoMix (mixed), and VoMix-Stereo (stereo).", "section": "3 CoVoMix"}, {"figure_path": "VNbQbv658b/figures/figures_16_1.jpg", "caption": "Figure 9: Speaker similarity across acoustic model of different size", "description": "This figure shows the results of ablation study on discrete semantic representation and model size. The x-axis represents the number of transformer layers in the acoustic model, and the y-axis represents the speaker similarity.  Four lines are plotted: predicted phoneme, oracle phoneme, predicted semantic token, and oracle semantic token.  The results show that larger acoustic models generally improve speaker similarity and that semantic token representations achieve higher similarity than phoneme representations.", "section": "B.2 Ablation Study on Discrete Semantic Representation and Model Size"}, {"figure_path": "VNbQbv658b/figures/figures_18_1.jpg", "caption": "Figure 1: The overview of CoVoMix framework, which consists of a multi-stream text-to-semantic model, a conditional flow-matching based acoustic model for mixed mel-spectrogram generation, and a HiFi-GAN based vocoder for waveform production.", "description": "This figure provides a high-level overview of the CoVoMix framework. It illustrates the three main components: a multi-stream text-to-semantic model that converts text into multiple streams of semantic tokens, a conditional flow-matching based acoustic model that generates a mixed mel-spectrogram from these token streams, and a HiFi-GAN-based vocoder that produces the final speech waveforms. The figure shows how these components work together to generate human-like, multi-speaker conversations.", "section": "Abstract"}]