{"references": [{"fullname_first_author": "Anthony Bagnall", "paper_title": "The UEA multivariate time series classification archive", "publication_date": "2018-11-01", "reason": "This paper introduces a widely used benchmark dataset for evaluating time series classification models, providing a crucial resource for comparison and validating new approaches."}, {"fullname_first_author": "Aaron van den Oord", "paper_title": "Neural discrete representation learning", "publication_date": "2017-12-01", "reason": "This paper introduces vector-quantized variational autoencoders (VQ-VAEs), a fundamental technique used in VQShape for learning a codebook of discrete representations of time series."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, a key component of VQShape's encoder, which has revolutionized various deep learning domains due to its efficiency in handling sequential data."}, {"fullname_first_author": "Mononito Goswami", "paper_title": "Moment: A family of open time-series foundation models", "publication_date": "2024-01-01", "reason": "This paper introduces MOMENT, a significant foundation model for time series, providing a strong baseline and context for evaluating the performance and generalizability of VQShape."}, {"fullname_first_author": "Yuqi Nie", "paper_title": "A time series is worth 64 words: Long-term forecasting with transformers", "publication_date": "2023-01-01", "reason": "This paper introduces PatchTST, a transformer-based model that processes time series data in patches, a methodology that directly influenced VQShape's design for encoding time series data."}]}