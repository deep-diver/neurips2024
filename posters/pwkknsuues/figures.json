[{"figure_path": "pwKkNSuuEs/figures/figures_3_1.jpg", "caption": "Figure 1: Overview of VQShape", "description": "The figure shows an overview of the VQShape architecture. It is composed of a TS encoder, a TS decoder, a latent-space codebook, a shape decoder, and attribute encoders and decoders. The TS encoder takes a univariate time series as input and transforms it into patch embeddings using a transformer model. These embeddings are then passed through an attribute decoder that outputs a set of attribute tuples for each patch.  These tuples are composed of the code for the abstracted shape of the patch, the offset, scale, start time, and duration. The attribute tuples are then quantized using vector quantization to select the nearest code in the codebook. The quantized codes are passed through a shape decoder to reconstruct the abstracted shapes of the patches. The abstracted shapes and attributes are then used to reconstruct the input time series. The figure also shows the pre-training objectives that are used to train the VQShape model.", "section": "3 Proposed Method"}, {"figure_path": "pwKkNSuuEs/figures/figures_7_1.jpg", "caption": "Figure 6: Visualizations of the abstracted shapes decoded from the codebook of VQShape", "description": "This figure visualizes the 512 abstracted shapes contained within the codebook of the VQShape model. Each shape is represented as a line graph, showing its unique form. The visualization helps demonstrate the diversity and variety of shapes learned by the model, and provides a visual representation of the model's capacity to represent diverse time series data.", "section": "C Additional Visualizations"}, {"figure_path": "pwKkNSuuEs/figures/figures_8_1.jpg", "caption": "Figure 1: Overview of VQShape", "description": "This figure provides a detailed overview of the VQShape architecture.  It illustrates the flow of data through the model, starting with the time-series input (x). The input undergoes a TS encoding process using a patch-based transformer encoder to produce latent embeddings. These embeddings are then passed through an attribute decoder, which extracts attributes such as abstracted shapes, offsets, scales, starting positions and durations. Vector quantization selects the closest code for each abstracted shape from a learned codebook. The resulting quantized attribute tuple is used to produce the final shape representations (sk) via shape decoding, and the reconstructed TS (x) via attribute encoding and reconstruction.  The figure shows the various components and their interactions in the VQShape architecture.", "section": "3 Proposed Method"}, {"figure_path": "pwKkNSuuEs/figures/figures_8_2.jpg", "caption": "Figure 4: Example of how the code histogram representations provide discriminative features for classification. Histogram representations are obtained from VQShape-64. The histograms are averaged over samples of the two classes from the test split of the UWaveGestureLibrary dataset. The top and bottom rows represent samples labeled as \u201cCW circle\u201d and \u201cCCW circle\u201d, respectively. Each column represent a variate (channel).", "description": "This figure demonstrates how the code histogram representations learned by VQShape-64 can distinguish between different classes in the UWaveGestureLibrary dataset.  It shows the average code histograms for two classes (\u201cCW circle\u201d and \u201cCCW circle\u201d) across three different variates (channels).  The significant differences in code frequencies between the two classes highlight the discriminative power of these representations for classification.  Each histogram bar visually represents the frequency of a specific shape code, allowing for direct interpretation of the feature patterns that distinguish the two classes.", "section": "5.2 Interpretability"}, {"figure_path": "pwKkNSuuEs/figures/figures_8_3.jpg", "caption": "Figure 5: Mean accuracy of classifiers trained with token and histogram representations across different codebook sizes. Performance of the token classifiers improve with larger codebook, while performance of the histogram classifiers peak at codebook size 64 and decline with larger codebook.", "description": "This figure shows the performance of classifiers trained using two different types of representations extracted from the VQShape model: token and histogram representations.  The x-axis represents the codebook size used during training, while the y-axis shows the mean accuracy achieved on a classification task. The results indicate that token-based classifiers consistently improve in accuracy as the codebook size increases, suggesting that more detailed representations are beneficial. In contrast, histogram-based classifiers exhibit an optimal performance around a codebook size of 64, with accuracy declining as the codebook size increases beyond this point.", "section": "5.3 Ablation Study"}, {"figure_path": "pwKkNSuuEs/figures/figures_17_1.jpg", "caption": "Figure 1: Overview of VQShape", "description": "This figure shows the architecture of the VQShape model, which is composed of a TS encoder, a TS decoder, a latent-space codebook, a shape decoder, and an attribute encoder/decoder. The TS encoder transforms a univariate TS into patch embeddings that are fed into a transformer model to produce latent embeddings. The attribute decoder extracts attribute tuples (code for abstracted shape, offset, scale, start time, and duration) from the latent embeddings.  Vector quantization selects discrete codes from the codebook based on Euclidean distance. The shape decoder takes the code and outputs a normalized subsequence with offset and scale removed. Finally, the attribute encoder and decoder reconstruct the attribute tuple and the whole TS. ", "section": "3 Proposed Method"}, {"figure_path": "pwKkNSuuEs/figures/figures_18_1.jpg", "caption": "Figure 1: Overview of VQShape", "description": "This figure provides a detailed overview of the VQShape architecture.  It shows the different components, including the TS encoder, the TS decoder, the latent-space codebook, the shape decoder, and the attribute encoder/decoder. The data flow is illustrated, showing how the model processes time-series data to extract interpretable representations.", "section": "3 Proposed Method"}, {"figure_path": "pwKkNSuuEs/figures/figures_19_1.jpg", "caption": "Figure 7: t-SNE plot of the codes.", "description": "This figure visualizes the distribution of 512 codes in the codebook of the VQShape model.  The codes were reduced to two dimensions using t-SNE, a dimensionality reduction technique. The plot shows that the codes cluster into approximately 60 groups, suggesting that only around 60 distinct abstracted shapes are learned by the model, despite the larger codebook size. This indicates a degree of redundancy or similarity among the learned shapes.", "section": "C.2 Code Distribution"}, {"figure_path": "pwKkNSuuEs/figures/figures_19_2.jpg", "caption": "Figure 1: Overview of VQShape", "description": "This figure shows the architecture of the VQShape model, illustrating the flow of data through the different components.  The time series (TS) is first encoded into patches using a transformer encoder, generating patch embeddings. These embeddings are then processed by the transformer model, resulting in latent embeddings. The attribute decoder extracts attributes like abstracted shape, offset, scale, start time, and duration from the latent embeddings. Vector quantization then maps these attributes to a discrete codebook of abstracted shapes.  Finally, a shape decoder reconstructs the original TS subsequence using this codebook. The encoder and decoder, along with the codebook, form the core of VQShape\u2019s learning process.  The attribute encoder and decoder are critical for creating an interpretable representation that links the latent space to shape-level features.", "section": "3 Proposed Method"}]