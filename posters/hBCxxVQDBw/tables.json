[{"figure_path": "hBCxxVQDBw/tables/tables_1_1.jpg", "caption": "Table 1: Description of the relative strengths and weaknesses of the five evaluation methods we consider. We include a discussion of this in Section 7.", "description": "This table compares five different methods for evaluating recurrent neural networks (RNNs): sequential evaluation, DEER, quasi-DEER, ELK, and quasi-ELK.  It summarizes each method's performance across four key criteria: whether it allows for parallel computation, its computational complexity (Work), its memory requirements (Memory), and its numerical stability (Stability). The table highlights the trade-offs between parallelization, efficiency, and robustness for each approach.", "section": "1 Introduction"}, {"figure_path": "hBCxxVQDBw/tables/tables_21_1.jpg", "caption": "Table 2: Time to evaluate a length T = 10,000 trained AR GRU using sequential vs parallelized methods. We note the dynamax package [12] we used for the parallel Kalman filter implementation in ELK is not optimized for speed, and hence these run times could be further improved.", "description": "This table compares the time taken to evaluate a trained autoregressive GRU (with sequence length T=10000) using different methods. It shows the time per Newton step, the number of steps to convergence, and the total convergence time for each method. The methods compared are sequential evaluation, DEER, quasi-DEER, ELK, and quasi-ELK. The table highlights the trade-offs between the speed of individual steps and the total number of steps required for convergence.", "section": "6.3 ELK and Quasi-ELK for Evaluating Autoregressive RNNS"}]