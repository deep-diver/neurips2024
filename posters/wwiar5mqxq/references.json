{"references": [{"fullname_first_author": "Aman Madaan", "paper_title": "Self-refine: Iterative refinement with self-feedback", "publication_date": "2023-12-10", "reason": "This paper introduces self-reflection mechanism, a key concept for improving LLM-based agents' ability to learn from environment and improve themselves autonomously, which is directly relevant to the proposed method in this paper."}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-11-28", "reason": "This paper is foundational for the use of reinforcement learning from human feedback (RLHF) for fine-tuning LLMs, which is a crucial component of the proposed method in this paper."}, {"fullname_first_author": "Noah Shinn", "paper_title": "Reflexion: language agents with verbal reinforcement learning", "publication_date": "2023-12-10", "reason": "This paper introduces the Reflexion framework, which uses prior trajectories and environmental rewards to generate reflections, directly inspiring the self-reflection mechanism proposed in this paper."}, {"fullname_first_author": "Weiran Yao", "paper_title": "Retroformer: Retrospective large language agents with policy gradient optimization", "publication_date": "2023-08-02", "reason": "This paper proposes a method for approximating reflection rewards and training a plug-in reflector through policy optimization, which is extended and adapted in the proposed COPPER framework for multi-agent systems."}, {"fullname_first_author": "Yilun Du", "paper_title": "Improving factuality and reasoning in language models through multiagent debate", "publication_date": "2023-05-14", "reason": "This paper explores the use of multi-agent systems for improving the factuality and reasoning abilities of LLMs, which is the foundation for the multi-agent collaboration approach taken in this paper."}]}