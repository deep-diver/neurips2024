[{"type": "text", "text": "A Unifying Post-Processing Framework for Multi-Objective Learn-to-Defer Problems ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Mohammad-Amin Charusaie ", "page_idx": 0}, {"type": "text", "text": "Samira Samadi ", "page_idx": 0}, {"type": "text", "text": "Max Planck Institute for Intelligent Systems Max Planck Institute for Intelligent Systems Tuebingen, Germany Tuebingen, Germany mcharusaie@tuebingen.mpg.de samira.samadi@tuebingen.mpg.de ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Learn-to-Defer is a paradigm that enables learning algorithms to work not in isolation but as a team with human experts. In this paradigm, we permit the system to defer a subset of its tasks to the expert. Although there are currently systems that follow this paradigm and are designed to optimize the accuracy of the final human-AI team, the general methodology for developing such systems under a set of constraints (e.g., algorithmic fairness, expert intervention budget, defer of anomaly, etc.) remains largely unexplored. In this paper, using a $d$ -dimensional generalization to the fundamental lemma of Neyman and Pearson ( $\\grave{d}$ GNP),we obtain the Bayes optimal solution for learn-to-defer systems under various constraints. Furthermore, we design a generalizable algorithm to estimate that solution and apply this algorithm to the COMPAS, Hatespeech, and ACSIncome datasets. Our algorithm shows improvements in terms of constraint violation over a set of learn-to-defer baselines and can control multiple constraint violations at once. The useof $d$ -GNP is beyond learn-to-defer applications and can potentially obtain a solution to decision-making problems with a set of controlled expected performance measures. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Machine learning algorithms are increasingly used in diverse fields, including critical applications, such as medical diagnostics [72] and predicting optimal prognostics [63]. To address the sensitivity of such tasks, existing approaches suggest keeping the human expert in the loop and using the machine learning prediction as advice [35], or playing a supportive role by taking over the tasks on which machine learning is uncertain [39, 60, 4]. The abstention of the classifier in making decisions, and letting the human expert do so, is where the paradigm of learn-to-defer (L2D) started to exist. ", "page_idx": 0}, {"type": "text", "text": "The development of L2D algorithms has mainly revolved around optimizing the accuracy of the final system under such paradigm [60, 50]. Although they achieve better accuracy than either the machine learning algorithm or the human expert in isolation, these works provide inherently single-objective solutions to the L2D problem. In the critical tasks that are mentioned earlier, more often than not, we face a challenging multi-objective problem of ensuring the safety, algorithmic fairness, and practicality of the final solution. In such settings, we seek to limit the cost of incorrect decisions [46], algorithmic biases [13], or human expert intervention [57], while optimizing the accuracy of the system. Although the seminal paper that introduced the first L2D algorithm targeted an instance of such multi-objective problem [44], a general solution to such class of problems, besides specific examples [26, 57, 51, 52], has remained unknown to date. Multi-objective machine learning extends beyond the realm of L2D problems. A prime example that is extensively studied in various settings is ensuring algorithmic fairness [18] while optimizing accuracy. Recent advances in the algorithmic fairness literature have suggested the superiority of post-processing methodology for tackling this multi-objective problem [73, 14, 20, 76]. Post-processing algorithms operate in two steps: first, they find a calibrated estimation of a set of probability scores for each input via learning algorithms, and then they obtain the optimal predictor as a function of these scores. Similarly, in a recent set of works, optimal algorithms to reject the decision-making under a variety of secondary objectives are determined via post-processing algorithms [51, 52], which is in line with classical results such as Chow's rule [16] that is the simplest form of a post-processing method, thresholding the likelihood. ", "page_idx": 0}, {"type": "image", "img_path": "Mtsi1eDdbH/tmp/3bb2f883be947d77a96e0ce8442a3649b2514d4e6c68cac64db55de499e4b19e.jpg", "img_caption": ["Figure 1: Diagram of applying $d$ GNP to solve multi-objective L2D problem. The role of randomness is neglected due to simplicity of presentation. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Inspired by the above works, in this paper, we fully characterize the solution to multi-objective L2D problems using a post-processing framework. In particular, we consider a deferral system together with a set of conditional performance measures $\\{\\Psi_{0},\\dots,\\Psi_{m}\\}$ that are functions of the system outcome $\\hat{Y}$ , the target label $Y$ , and the input $X$ . The goal is to optimize the average value of $\\Psi_{0}$ over data distribution while keeping the average value of the rest of performance measures $\\Psi_{1},\\ldots,\\Psi_{m}$ for all inputs under control. As an example, in binary classification, $\\Psi_{0}$ can be the $0-1$ deferral loss function, while $\\Psi_{1}$ can be the difference between positive prediction rates of $\\hat{Y}$ for all instances of $X$ that belong to demographic group $A=0$ or $A=1$ . The solution for which we aim optimizes the accuracy while assuring that the demographic parity measure between the two groups is bounded by a tolerance value $\\delta_{1}\\in[0,1]$ ", "page_idx": 1}, {"type": "text", "text": "To provide the optimal solution, we move beyond staged learning [12] methodology, in which the classifier $h(x)$ is trained in the absence of human decision-makers, and then the optimal rejection function $r(x)$ is obtained for that classifier to decide when the human expert should intervene $(r(x)=1)$ . Instead, we jointly obtain the classifier and rejection function. The reason that we avoid this methodology is that firstly, objectives such as algorithmic fairness are not compositional, i.e., even if the classifier and the human are fair, due to the emergence of Yule's effect [62] the obtained deferral system is not necessarily fair (see Appendix A), and in fact abstention systems can deter the algorithmic biases [36]. Secondly, the feasibility of constraints is not guaranteed under staged learning methodology [74], e.g., there can be cases in which achieving a purely fair solution is impossible, while this occurs neither in vanilla classification [20] nor in our solution. ", "page_idx": 1}, {"type": "text", "text": "This paper shows that the joint learning of classifier and rejection function for finding the optimal multi-objective L2D solution boils down to a generalization of the fundamental Neyman-Pearson lemma [55]. This lemma is initially introduced in studying hypothesis testing problems and characterizes the most powerful test (i.e., the test with the highest true positive rate) while keeping the significance level (true negative rate) under control. As a natural extension to this paradigm, we consider a multi-hypothesis setting where for each true positive prediction and false negative prediction, we receive a reward and loss, respectively. Then, we show that the extension of Neyman-Pearson lemma to this setting provides us with a solution for our multi-objective L2D problem. ", "page_idx": 1}, {"type": "text", "text": "In summary, the contribution of this paper is as below: ", "page_idx": 1}, {"type": "text", "text": "\u00b7 In Section 3, we show that obtaining the optimal deterministic classifier and rejection function under a constraint is, in general, an NP-Hard problem, then ", "page_idx": 1}, {"type": "text", "text": "\u00b7 by introducing randomness, we rephrase the multi-objective L2D problem into a functional linear programming.   \n\u00b7 In Section 4, we show that such linear programming problem is an instance of $d$ -dimensional generalized Neyman-Pearson ( $d$ GNP) problem, then   \n\u00b7 we characterize the solution to $d$ -GNP problem, and we particularly derive the corresponding parameters of the solution when the optimization is restricted by a single constraint.   \n\u00b7 In Section 5, we show that a post-processing algorithm that is based on $d$ GNP solution generalizes in constraints and objective with the rate $O(\\sqrt{\\log n/n},\\sqrt{\\log(1/\\epsilon)/n},\\epsilon^{\\prime})$ and $O((\\log n/n)^{1/2\\gamma}$ \uff0c $(\\log(1/\\epsilon)/n)^{1/2\\gamma},\\epsilon^{\\prime})$ , respectively, with probability at least $1\\,-\\,\\epsilon$ where $n$ is the size of the set using which we fine-tune the algorithm, $\\epsilon^{\\prime}$ measures the accuracy of learned post-processing scores, and $\\gamma$ is a parameter that measures the sensitivity of the constraint to the change of the predictor. Then,   \n\u00b7 we show that the use of in-processing methods in L2D problem does not necessarily generalize to the unobserved data, and finally   \n\u00b7 we experiment our post-processing algorithm on two tabular datasets and a text dataset, and observe its performance compared to the baselines for ensuring demographic parity and equality of opportunity on final predictions. ", "page_idx": 2}, {"type": "text", "text": "Lastly, the $d$ GNP theorem has potential use cases beyond the L2D problem, particularly in vanilla classification problems under constraints. However, such applications are beyond the scope of this paper, and except for a brief explanation of the use of $d$ GNP in algorithmic fairness for multiclass classification, we leave them to future works. ", "page_idx": 2}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Human and ML's collaboration in decision-making has been demonstrated to enhance the accuracy of final decisions compared to predictions that are made solely by humans or ML [37, 68]. This overperformance is due to the ability to estimate the accuracy and confidence of each agent on different regions of data and subsequently allocate instances between human and ML to optimize the overall accuracy [2]. Since the introduction of the L2D problem, the implementation of its optimal rule has been the focus of interest in this field [8, 50, 12, 51, 9, 43, 48, 45]. The multiobjective classification with abstention problems is studied for specific objectives in [44, 57, 48] via in-processing methods. The application of Neyman-Pearson lemma for learning problems with fairness criteria is recently introduced in [75]. ", "page_idx": 2}, {"type": "text", "text": "We refer the reader to Appendix $\\mathbf{B}$ for further discussion on related works. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Setting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Assume that we are given input features $x_{i}\\in\\mathcal{X}$ , corresponding labels $y_{i}\\in\\mathcal{Y}=\\{1,\\ldots,L\\}$ , and the human expert decision $m_{i}$ for such input, and assume that these are i.i.d. realizations of random variables $X,Y,M\\sim\\mu=\\mu_{X Y M}$ . Since there exists randomness in the human decision-making process, for the sake of generality, we treat $M$ as a random variable similar to $Y$ and do not assume that $m_{i}=m(x_{i})$ for some function $m$ . Further, assume that for the true label $y$ and a certain feature vector $x$ , the cost of incorrect predictions is measured by a loss function $\\ell_{A I}(y,h(x))$ for the classifier prediction $h(x)$ , and a loss function $\\ell_{H}(y,m)$ for human's prediction $m$ . The question that we tackle in this paper is the following: What is an optimal classifier and otherwise an optimal way of deferring the decision to the human when there are constraints that limit the decision-making? The constraints above can be algorithmic fairness constraints (e.g., demographic parity, equality of opportunity, equalized odds), expert intervention constraints (e.g., when the human expert can classify up to $b$ proportion of the data), or spatial constraints to enforce deferral on certain inputs, or any combination thereof. ", "page_idx": 2}, {"type": "text", "text": "Let us put the above question in a formal optimization form. To that end, let $r(x)\\in\\{0,1\\}$ be the rejection function', i.e., when $r(x)=0$ the classifier makes the decision for input $x$ and otherwise $x$ is deferred to the expert. We obtain the deferral loss on $x$ and given a label $y$ and the expert decision $m$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\ell_{\\mathrm{def}}(y,m,h(x),r(x))=r(x)\\ell_{H}(y,m)+(1-r(x))\\ell_{A I}(y,h(x)).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Table 1: A list of embedding functions corresponding to the constraints that are discussed in Section 3. This list is a version of the results in Appendix $\\mathrm{D}$ when we assume that the input feature contains demographic group identifier $A$ . To simplify the notations, we define $t(A,y):=$ ", "page_idx": 3}, {"type": "table", "img_path": "Mtsi1eDdbH/tmp/ecdaf4d8487fae4dd89685b79281c58bdae3fe58eea17a5d2f9178b8026f24ab.jpg", "table_caption": ["$\\begin{array}{r}{\\frac{\\mathbb{I}_{A=1}}{P r(Y=y,A=1)}-\\frac{\\mathbb{I}_{A=0}}{P r(Y=y,A=0)}.}\\end{array}$ "], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "Therefore, we can find the average deferral loss on distribution $\\mu$ as ", "page_idx": 3}, {"type": "equation", "text": "$$\nL_{\\operatorname*{def}}^{\\mu}(h,r):=\\mathbb{E}_{X,Y,M\\sim\\mu}\\bigl[\\ell_{\\operatorname*{def}}(Y,M,h(X),r(X))\\bigr].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We aim to find a randomized algorithm $\\boldsymbol{\\mathcal{A}}$ that defines a probability distribution $\\mu_{\\mathcal{A}}$ on $\\mathcal{H}\\times\\mathcal{R}$ that solves the optimization problem ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{A}\\in\\underset{\\mu_{A}}{\\mathrm{argmin}}\\,\\mathbb{E}_{(h,r)\\sim A}\\big[L_{\\operatorname*{def}}^{\\mu}(h,r)\\big],}\\\\ {s.t.}&{\\mathbb{E}_{X,Y,M\\sim\\mu}\\mathbb{E}_{(h,r)\\sim\\mu_{A}}\\big[\\Psi_{i}\\big(X,Y,M,h(X),r(X)\\big)\\big]\\leq\\delta_{i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\Psi_{i}$ is a performance measure that induces the desired constraint in our optimization problem. We assume that $\\Psi_{i}$ , similar to $\\ell_{\\mathrm{def}}$ , is an outcome-dependent function, i.e., if the deferral occurs, the outcome of the classifier does not change $\\Psi_{i}$ , and otherwise, if deferral does not occur, the human decision does not change $\\Psi_{i}$ . In other words, the value of the constraints can only be a function of input feature $x$ and of the deferral system prediction $\\hat{Y}=r(x)M+\\big(1-r(x)\\big)h(x)$ . Here, $\\hat{Y}$ is the expert decision when deferral occurs, and is the classifier decision otherwise. ", "page_idx": 3}, {"type": "text", "text": "Types of constraints. Before we discuss our methodology to solve (2), it is beneficial to review the types of constraints with which we are concerned: (1) expert intervention budget that can be written in form of $\\operatorname*{Pr}\\left(r(X)=1\\right)\\leq\\delta$ , limits the rejection function to defer up to $\\delta$ proportion of the instance, (2) demographic parity that is formulated as $\\left|P(\\hat{Y}=1|A=0)-P(\\hat{Y}=1|A=0)\\right|\\leq\\delta$ ensures that the proportion of positive predictions for the first demographic group $\\langle A=0$ ) is comparable to that for the second demographic group ( $\\left.A=1\\right.$ 0. (3) equality of opportunity that is defined as $|P r(\\hat{Y}=1|A=1,Y=1)-P r(\\hat{Y}|A=0,Y=1)|\\leq\\delta$ limits the differences between correct positive predictions among two demographic groups, (4) equalized odds that is similar to equality of opportunity but targets the differences of correct positive and negative predictions among two groups, i.e., $\\begin{array}{r}{\\operatorname*{max}_{y=0,1}\\bar{|}P r(\\hat{Y}=1|A=1,Y=y)-P r(\\hat{Y}=1|A=0,\\bar{Y^{}}=y)\\bar{|}\\leq\\delta,}\\end{array}$ (5) out-ofdistribution (OoD) detection that is written as $\\mathrm{Pr}_{\\mathrm{out}}(r(X)=0)\\leq\\delta$ limits the prediction of the classifier on points that are outside its training distribution and incentivizes deferral in such cases, (6) long-tail classification deals with high class imbalances. This method aims to minimize a balanced error of classifier prediction on instances where deferral does not occur. Achieving this objective as ", "page_idx": 3}, {"type": "text", "text": "$\\begin{array}{r}{\\sum_{i=1}^{K}\\frac{1}{\\alpha_{i}}\\operatorname*{Pr}(Y\\ne h(X),r(X)=0|Y\\in G_{i})}\\end{array}$ Wwhen the feasible set is $\\begin{array}{r}{\\operatorname*{Pr}(r(X)=0,Y\\in G_{i})=\\frac{\\alpha_{i}}{K}}\\end{array}$ , and where $\\{G_{i}\\}_{i=1}^{K}$ is a partition of classes, and finally (7) type $k$ error bounds that is a generalization of Type-I and Type-II errors, limits errors of a specific class $k$ using $\\operatorname*{Pr}(\\hat{Y}\\neq k|Y=k)\\leq\\delta$ ", "page_idx": 4}, {"type": "text", "text": "All above constraints are expected values of outcome-dependent functions (see Appendix D for proof). To put it informally, if we change the classifier outcome after the rejection, such constraints donot vary. ", "page_idx": 4}, {"type": "text", "text": "Linear Programming Equivalent to (2). The outcome-dependence property helps us to show that (see Appendix C) obtaining the optimal classifier and rejection function is equivalent to obtaining the solutionof ", "page_idx": 4}, {"type": "equation", "text": "$$\nf^{*}=\\left[f_{1}^{*},\\ldots,f_{d}^{*}\\right]\\in\\operatorname{argmax}_{f\\in\\Delta_{d}^{x}}\\mathbb{E}\\left[\\langle f(X),\\psi_{0}(X)\\rangle\\right],\\quad\\mathrm{s.t.~}\\mathbb{E}\\left[\\langle f(x),\\psi_{i}(x)\\rangle\\right]\\leq\\delta_{i},i\\in[1:m]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\Delta_{d}$ is a simplex of $d$ dimensions, $d=L+1$ , and $\\psi_{i}:\\mathcal{X}\\rightarrow\\mathbb{R}^{d}$ is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\psi_{i}(x):=\\mathbb{E}_{Y,M|X=x}\\bigg[\\Big[\\Psi_{i}(x,Y,M,1,0),\\ldots,\\Psi_{i}(x,Y,M,l,0),\\Psi_{i}(x,Y,M,0,1)\\big]\\Big]\\bigg]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "that we name the embedding function\u00b2 corresponding to the performance measure $\\Psi_{i}$ for $i\\in[0:m]$ where for simplifying the notation we define $\\Psi_{0}\\,\\equiv\\,-\\ell_{\\mathrm{def}}$ . Furthermore, the optimal algorithm is obainedby predcting $h(x)\\;=\\;i$ with normalized probabity of $f_{i}^{*}(x)/\\Sigma_{j=1}^{d-1}\\,f_{j}^{*}(x)$ where $\\textstyle\\sum_{j=1}^{d-1}f_{j}^{*}(x)\\neq0$ and rejecting $r(x)=1$ with probability $f_{d}^{*}(x)$ . In case of $\\textstyle\\sum_{j=1}^{d-1}f_{j}^{*}(x)=0$ the classifier is defined arbitrarily. A list of embedding functions for the mentioned constraints and objectives is provided in Table 1 (See Appendix D for derivations). ", "page_idx": 4}, {"type": "text", "text": "Hardness. We first derive the following negative result for the optimal deterministic predictor in (3). We use the similarity between (3) and $0{-}1$ Knapsack problem (see [58, pp. 374]) to show that there are cases in which solving the former is equivalent to solving an NP-Hard problem. More particularly, if we assume that the distribution of $X$ contains finite atoms $x_{1},\\ldots,x_{n}$ each of which have probability of $\\operatorname*{Pr}(X=x_{i})=p_{i}$ , and if we set $\\begin{array}{r}{\\psi_{1}(x_{i})=[0,\\frac{w_{i}}{p_{i}}]}\\end{array}$ and $\\begin{array}{r}{\\psi_{0}(x_{i})=[0,\\frac{v_{i}}{p_{i}}]}\\end{array}$ for $v_{i},w_{i}\\in\\mathbb{R}^{+}$ , then (3) reduces in argmax $\\textstyle\\sum_{i}f^{1}(x_{i})v_{i}$ subjected to $f^{1}:{\\bar{\\boldsymbol{\\chi}}}\\rightarrow\\{0,1\\}$ and $\\begin{array}{r}{\\sum_{i}\\^{}f^{1}(x_{i})w_{i}\\leq\\delta_{1}}\\end{array}$ , which is the main form of the Knapsack problem. In the following theorem, we show that a similar result can be obtained if we choose $\\psi_{0}$ and $\\psi_{1}$ to be embedding functions corresponding to accuracy and expert intervention budget. All proofs of theorems can be found in the appendix. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1 (NP-Hardness of (2)). Let the human expert and the classifier induce $0-1$ losses andassume $\\mathcal{X}$ to be finite.Finding an optimal deterministic classifier and rejection function for $a$ bounded expert intervention budget is an NP-Hard problem. ", "page_idx": 4}, {"type": "text", "text": "Note that the above finding is different from the complexity results for deferral problems in [49, Theorem 1] and [23, Theorem 1]. NP-hardness results in these settings are consequences of restricting the search to a specific space of models, i.e., the intersection of half-spaces and linear models on a subset of the data. However, in our theorem, the hardness arises due to a possibly complex data distribution and not because of the complex model space. ", "page_idx": 4}, {"type": "text", "text": "The above hardness theorem for deterministic predictors justifies our choice of using randomized algorithms to solve multi-objective L2D. In the next section, by finding a closed-form solution for the randomized algorithm, we show that such relaxation indeed simplifies the problem. ", "page_idx": 4}, {"type": "text", "text": "4 $d$ -dimensional Generalization of Neyman-Pearson Lemma ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The idea behind minimizing an expected error while keeping another expected error bounded is naturally related to the problem that is designed by Neyman and Pearson [55]. They consider two hypotheses $H_{0},H_{1}$ as two distributions with density functions $g_{0}(x)$ and $g_{1}(x)$ for which a given point $x$ can be drawn. Then, they maximize the probability of correctly rejecting $H_{0}$ , while bounding the probability of incorrectly rejecting $H_{0}$ , i.e., for a test $\\dot{T}(x)\\in[0,1]$ that rejects the null hypothesis when $T(x)=1$ , they solved the problem ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{T\\in[0,1]^{\\mathcal{X}}}\\mathbb{E}_{X\\sim g_{1}}\\big[T(X)\\big],\\quad s.t.\\ \\mathbb{E}_{X\\sim g_{0}}\\big[T(X)\\big]\\leq\\alpha.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "They concluded that thresholding the likelihood ratio is a solution to the above problem. Formally, they show that all optimal hypothesis tests take the value $T(x)=1$ when $g_{1}(x)/\\bar{g}_{0}(x)>k$ and take the value $T(x)=0$ when $\\bar{g_{1}}\\bar{(}x)/g_{0}(x)<k$ , where $k$ is a scalar and dependent on $\\alpha$ ", "page_idx": 5}, {"type": "text", "text": "Multi-hypothesis testing with rewards. In this section, we aim to solve (3) as a generalization of Neyman-Pearson lemma for binary testing to the case of multi-hypothesis testing, in which correctly and incorrectly rejecting each hypothesis has a certain reward and loss. To clarify how the extension of this setting and the problem (3) are equivalent, assume the general case of $d$ hypotheses $H_{0},\\dots,H_{d-1}$ , each of which corresponding to $X$ being drawn from the density function $g_{i}(x)$ for $i\\in\\{0,\\ldots,d-1\\}$ . Further, assume that for each hypothesis $H_{i}$ , in case of true positive, we receive the reward $r_{i}(x)$ , and in case of false negative, we receive the loss $\\ell_{i}(x)$ . Assume that we aim to find a test $f:\\mathcal{X}\\to\\Delta_{d}$ that for each input $x\\in\\mathscr{X}$ rejects $d-1$ hypotheses, each hypothesis $H_{i}$ with probability $1-f^{i}(x)$ and maximizes a sum of true positive rewards, and that keeps the sum of false negative losses under control. Then, this is equivalent to $\\begin{array}{r}{\\underset{f\\in\\Delta_{d}^{\\mathcal{X}}}{\\mathrm{argmax}}\\sum_{i=0}^{d-1}\\mathbb{E}_{X\\sim g_{i}}\\Big[f^{i}(x)r_{i}(x)\\Big]}\\end{array}$ subjected to $\\begin{array}{r}{\\sum_{i=0}^{d-1}\\mathbb{E}_{X\\sim g_{i}}\\Big[(1-f^{i}(x))\\ell_{i}(x)\\Big]\\le\\delta_{1}}\\end{array}$ which in turns is equivalent to ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\underset{f\\in\\Delta_{d}^{\\chi}}{\\operatorname{argmax}}\\mathbb{E}_{X\\sim g_{0}}\\Big[\\sum_{i=0}^{d-1}f^{i}(x)r_{i}(x)\\frac{g_{i}(x)}{g_{0}(x)}\\Big]\\quad\\mathrm{~s.t.~}\\mathbb{E}_{X\\sim g_{0}}\\Big[\\sum_{i=0}^{d-1}f^{i}(x)\\sum_{j\\neq i}\\ell_{j}(x)\\frac{g_{j}(x)}{g_{0}(x)}\\Big]\\leq\\delta_{1}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This problem can be seen as instance of (3), when we set $\\begin{array}{r}{\\psi_{0}(x)\\,=\\,\\bigl[r_{0}(x),\\ldots,r_{d-1}(x)\\frac{g_{d-1}(x)}{g_{0}(x)}\\bigr]}\\end{array}$ $\\begin{array}{r}{\\psi_{1}(x)\\;=\\;\\big[\\sum_{j\\neq0}\\ell_{j}(x){\\textstyle\\frac{g_{j}(x)}{g_{0}(x)}},\\cdot\\cdot\\cdot,\\sum_{j\\neq d-1}\\ell_{j}(x){\\textstyle\\frac{g_{j}(x)}{g_{0}(x)}}\\big]}\\end{array}$   \n$\\psi_{0}(x),\\psi_{1}(x)$ in (3) there exists a set of densities $g_{1}(x),\\ldots,g_{d-1}(x)$ and rewards and losses such   \nthat (6) and (3) are equivalent. This can be done by setting $g_{i}\\equiv g_{0}$ and noting that the mapping from   \n$\\ell_{i}\\mathbf{s}$ and $r_{i}\\mathbf{s}$ into $\\psi_{0}$ and $\\psi_{2}$ is invertible. ", "page_idx": 5}, {"type": "text", "text": "The formulation of (3) can be seen as an extension of the setting in [69] when we move beyond type- $k$ error bounds to a general set of constraints. That work achieves the optimal test by applying strong duality on the Lagrangian form of the constrained optimization problem. However, we avoided using this approach in proving our solution, since finding $f^{*}$ , and not the optimal objective, is possible via strong duality only when we know apriori that the Lagrangian has a single saddle point (for more details and fallacy of such approach, see Section E). As another improvement to the duality method, we not only find a solution to (3), but also show that there is no other solution that works as well as ours. ", "page_idx": 5}, {"type": "text", "text": "Before we express our solution in the following theorem, we define an import notation as an extension of the argmax function that helps us articulate the optimal predictor. In fact, we define ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{T}_{d}=\\left\\{\\tau:\\mathbb{R}^{d}\\times\\mathbb{R}^{d}\\rightarrow\\Delta_{d}\\,\\vert\\,\\sum_{\\substack{i:x_{i}=\\mathrm{max}\\{x_{1},\\dots,x_{d}\\}}}\\left(\\tau(\\mathbf{x}_{1}^{d},\\cdot)\\right)(i)=1\\right\\}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "that is a set of functions that result in one-hot encoded argmax when there is a clear maximum, and otherwise, based on its second argument, results in a probability distribution on all components that achieved the maximum value. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.1 ( $\\grave{d}$ -GNP). For a set of functions $\\psi_{i}$ where $i\\in[0,m]$ assume that $(\\delta_{1},\\hdots,\\delta_{m})$ is an interior point of the set $\\mathcal{F}=\\left\\{\\big(\\mathbb{E}[\\langle r(x),\\psi_{1}(x)\\rangle],\\ldots,\\mathbb{E}[\\langle r(x),\\psi_{m}(x)\\rangle]\\big):f\\in\\Delta_{d}^{\\mathcal{X}}\\right\\}$ Then, there is a set of fixed values $k_{1},\\hdots,\\dot{k}_{m}$ and $\\tau\\in{\\mathcal{T}}_{d}$ such that the predictor ", "page_idx": 5}, {"type": "equation", "text": "$$\nf^{*}(x)=\\tau{\\big(}\\psi_{0}(x)-\\sum_{i=1}^{m}k_{i}\\psi_{i}(x),x{\\big)},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "obtains the optimal solution of $\\operatorname*{sup}_{f\\in\\Delta_{d}^{x}}\\mathbb{E}\\big[\\langle f(x),\\psi_{0}(x)\\rangle\\big]$ , subjected to the constraints being achieved tightly, i.e., when for $i\\,\\in\\,[1\\,:\\,m]$ we have $\\mathbb{E}\\big[\\langle f(x),\\psi_{i}(x)\\rangle\\big]\\;=\\;\\delta_{i}$ .If $k_{1},\\hdots,k_{m}$ are further non-negative, then $f^{*}(x)$ is the optimal solution to (3). Moreover, all optimal solutions of (3) that tightly achieve the constraints are in form of (8) almost everywhere on $\\mathcal{X}$ ", "page_idx": 5}, {"type": "text", "text": "Example 1 (L2D with Demographic Parity). In the setting that we have a deferral system and we aim for controlling demographic disparity under the tolerance $\\delta$ wecanset $\\psi_{0}(x)\\big{}^{\\prime}=\\,\\left[\\,\\operatorname*{Pr}(Y=$ ", "page_idx": 5}, {"type": "text", "text": "$0|x),\\operatorname*{Pr}(Y=1|x),\\operatorname*{Pr}(Y=M|x)]$ and $\\psi_{1}(x)=s(A)\\bigl[0,1,\\operatorname*{Pr}(M=1|x)\\bigr]$ , using Table 1, where $\\begin{array}{r}{s(A):=\\left(\\frac{\\mathbb{I}_{A=1}}{P r(A=1)}-\\frac{\\mathbb{I}_{A=0}}{P r(A=0)}\\right)}\\end{array}$ Pr(). Therefore, dGNP, together with the discussion after(4) shows that the optimal classifier and rejection function are obtained as ", "page_idx": 6}, {"type": "equation", "text": "$$\nh(x)=\\left\\{\\begin{array}{l l}{1}&{\\operatorname*{Pr}(Y=1|x)>\\frac{1+k s(A)}{2}}\\\\ {0}&{\\operatorname*{Pr}(Y=1|x)<\\frac{1+k s(A)}{2}}\\end{array}\\right.,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and ", "page_idx": 6}, {"type": "equation", "text": "$$\nr(x)=\\left\\{\\begin{array}{l l}{1}&{\\operatorname*{Pr}(Y=M|x)-k s(A)\\operatorname*{Pr}(M=1|x)>\\lambda(A,x)}\\\\ {0}&{\\operatorname*{Pr}(Y=M|x)-k s(A)\\operatorname*{Pr}(M=1|x)<\\lambda(A,x)}\\end{array}\\right.,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for a fixed value $k\\in\\mathbb{R}$ , and where $\\lambda(A,x):=\\operatorname*{max}\\{\\operatorname*{Pr}(Y=0|x),\\operatorname*{Pr}(Y=1|x)-k s(A)\\}$ .The above identities imply that the optimal fair classifier for the deferral system thresholds the scores for different demographic groups using two thresholds $k s(0)$ and $k s(1)$ . This is similar in form to the optimal fair classifier in vanilla classification problem [14, 20]. However, the rejection function does not merely threshold the scores for different groups, but adds an input-dependent threshold $k s(A)\\operatorname*{Pr}(M=1|x)$ to the unconstrained deferral system scores. ", "page_idx": 6}, {"type": "text", "text": "It is important to note that although we have a thresholding rule for the classifier, the thresholds are not necessarily the same as of isolated classifier under fairness criteria. Furthermore, the deferral rule is dependent on the thresholds that we use for the classifier. Therefore, we cannot train the classifier for a certain demographic parity and a rejection function in two independent stages. This further affirms the lack of compositionality of algorithmic fairness that we discussed earlier in the introduction of this paper. ", "page_idx": 6}, {"type": "text", "text": "Example 2 (L2D with Equality of Opportunity). Here, similar to the previous example, we can obtain the embedding function for accuracy and equality of opportunity constraint as $\\psi_{0}(\\dot{x})=\\big[p_{x}^{0},p_{x}^{1},p_{x}^{M}\\big]$ and $\\psi_{1}(x)\\,=\\,t(A,1)\\left[0,p_{x}^{1}\\right.$ \uff0c $\\operatorname*{Pr}(M=1,Y=1|x){\\rceil}$ , respectively, where $p_{x}^{i}\\,:=\\,\\mathrm{Pr}(Y=i|x)$ for $i\\in\\{1,2\\}$ and similarly $p_{x}^{M}=\\operatorname*{Pr}(Y=M|x)$ . Therefore, the characterization of optimal classifier and rejection function using $d$ -GNP results in ", "page_idx": 6}, {"type": "equation", "text": "$$\nh(x)=\\left\\{\\begin{array}{l l}{1}&{\\big(2-k t(A,1)\\big)p_{x}^{1}>1}\\\\ {0}&{\\big(2-k t(A,1)\\big)p_{x}^{1}<1}\\end{array}\\right.,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and ", "page_idx": 6}, {"type": "equation", "text": "$$\nr(x)=\\left\\{\\begin{array}{l l}{1}&{p_{x}^{M}\\big(1-k t(A,1)\\operatorname*{Pr}(M=1|Y=M,x)\\big)>\\nu(A,x)}\\\\ {0}&{p_{x}^{M}\\big(1-k t(A,1)\\operatorname*{Pr}(M=1|Y=M,x)\\big)<\\nu(A,x)}\\end{array}\\right.,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for $k\\in\\mathbb{R}$ and where $\\nu(A,x):=\\operatorname*{max}\\{p_{x}^{0}$ \uff0c $\\big(1-k t(A,1)\\big)p_{x}^{1}\\big\\}$ .Assuming $2-k t(A,1)$ takes positive values for all choices of $A$ , we conclude that the optimal classifier is to threshold positive scores differently for different demographic groups. However, the optimal deferral is a function of probability of positive prediction by human expert. ", "page_idx": 6}, {"type": "text", "text": "Example 3 (Algorithmic Fairness for Multiclass Classification). In addition to addressing the L2D problem, the formulation of $d_{\\cdot}$ GNP in Theorem 4.1 allows for finding the optimal solution in vanilla classification. In fact, for an $L$ -class classifier, if we aim to set constraints on demographic parity $\\big|\\operatorname*{Pr}(\\hat{Y}=0|A=0)-\\operatorname*{Pr}(\\hat{Y}=0|A=1)\\big|\\le\\delta$ or equality of opportunity $|\\operatorname*{Pr}({\\hat{Y}}={\\bar{0}}|{\\bar{Y^{}}}={\\bar{0_{,}}}A=$ $0)-\\operatorname*{Pr}({\\hat{Y}}=0|Y=0,A=1){\\big|}\\leq\\delta$ on Class O, then we can follow similar steps as in Appendix $\\mathrm{D}$ to find the embedding functions as $\\psi_{\\mathrm{DP}}=s(A)\\big[1,0,\\cdot\\cdot\\cdot,0\\big]$ and $\\psi_{\\mathrm{EO}}=t(A,0)\\left[p_{x}^{0},0,\\dots,0\\right]$ \uff0c where $p_{x}^{i}:=\\operatorname*{Pr}(Y=i|x)$ for $i\\in[L]$ ", "page_idx": 6}, {"type": "text", "text": "As a result, since the accuracy embedding function is $\\psi_{0}(x)=\\left[p_{x}^{0},\\dots,p_{x}^{L}\\right]$ , then, by neglecting the effect of randomness, the optimal classifier under such constraints are as ", "page_idx": 6}, {"type": "equation", "text": "$$\nh_{\\mathrm{DP}}(x)=\\operatorname{argmax}\\big\\{p_{x}^{0}-k s(A),p_{x}^{1},\\dots,p_{x}^{L}\\big\\},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and ", "page_idx": 6}, {"type": "equation", "text": "$$\nh_{\\mathrm{EO}}(x)=\\mathrm{argmax}\\left\\{p_{x}^{0}{\\left(1-k t(A,0)\\right)},p_{x}^{1},\\dots,p_{x}^{L}\\right\\}\\!.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Equivalently, for demographic parity, the optimal classifier includes a shift on the score of Class 0 as a function of demographic group, and for equality of opportunity, the optimal classifier includes a multiplication of the score of Class O with a value that is a function of demographic group. It is easy to show that under condition of positivity of the multiplied value, these classifiers both reduce to thresholding rules in binary setting. ", "page_idx": 6}, {"type": "text", "text": "Note that although Theorem 4.1 characterizes the optimal solution of (3), it leaves us uninformed regardingparameters $k_{1},\\ldots,k_{m}$ , and further does not give us the form of the optimal solution when $\\begin{array}{r}{\\psi_{0}\\overline{{(x)}}-\\overline{{\\sum}}_{i=1}^{m}\\,k_{i}\\psi_{i}(x)}\\end{array}$ has more than one maximizer. In the followingtheorem, we adressthese issues for the case that we have a single constraint. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.2 ( $d$ GNP with a single constraint). The optimal solution (8) of the optimization problem (3) with one constraint is equal to $f_{k,p}^{*}(x)=\\tau{\\big(}\\psi_{0}(x)-k\\psi_{1}(x),x{\\big)}$ where $\\tau$ is a member of $\\tau_{d}$ such that if there is a non-singleton set $\\mathcal{T}$ of maximizers of a vector $\\mathbf{y}\\in\\mathbb{R}^{d}$ , then we have $(\\tau(\\mathbf{y},x))(i)=p$ and $(\\tau(\\mathbf{y},x))(j)=1\\!-\\!p,$ where i and $j$ are the first indices in $\\mathcal{T}$ that minimizes $\\psi_{1}(x)$ and maximizes $\\psi_{0}(x)$ , respectively. In this case, $k$ is a member of the set $K=\\left\\{t:\\delta\\,\\in\\,\\big[\\operatorname*{lim}_{\\tau\\uparrow t}C(\\tau),C(t)\\big]\\right\\}$ where $C(t)=\\mathbb{E}\\big[\\langle f_{t,0}^{*}(x),\\psi_{0}(x)\\rangle\\big]$ is the expected constraint of the predictor $f_{t,0}^{*}$ .Moreover, $p=$ C(k)-in)- C(r) if C(r) is lower-discontinuwous a , and otherwise = O. ", "page_idx": 7}, {"type": "text", "text": "This theorem reduces the complexity of finding $k_{i}\\mathbf{s}$ from the complexity of an exhaustive search to the complexity of finding the root of the monotone function $C(t)-\\delta$ (see Lemma J.2 for the proof of monotonicity), and further finds the randomized response for the cases that Theorem 4.1 leaves undetermined. ", "page_idx": 7}, {"type": "text", "text": "Before we proceed to the designed algorithm based on $d$ GNP, we should address two issues. Firstly, during the course of optimization, it can occur that the solution of Theorem 4.1 does not compute non-negative values $k_{i}$ for an $i\\in[1:m]$ . This means that the constraints are not achieved tightly in the final solution of (3). Therefore, we are able to achieve the optimal solution with the constraint $\\delta_{i}^{\\prime}<\\delta_{i}$ . Now, if we can assure that the constraint tuples are still inner points of $\\mathcal{F}$ when we substitute $\\delta_{i}$ by $\\delta_{i}^{\\prime}$ , then Theorem 4.1 shows that (8) is still an optimal solution to (3). ", "page_idx": 7}, {"type": "text", "text": "Secondly, for tackling various objectives that are defined in Section 3, we usually need to upper- and lower-bound a performance measure by $\\delta$ and $-\\delta$ . However, since both bounds cannot hold tightly and simultaneously unless the tolerance is $\\delta=0$ , then we can use only one of the constraints in turn and apply the result of Theorem 4.2 and check whether the constraint is active in the final solution. In the next section, we design an algorithm based on these results and show its generalization to the unseen data. ", "page_idx": 7}, {"type": "text", "text": "5 Empirical $d$ GNP and its Statistical Generalization ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In previous sections, we obtained the optimal solution to the constrained optimization problem (3) using $d$ GNP. Based on this optimal solution, we can design a plug-in method (see Algorithm 1 in Appendix F) to solve the constrained learning problem using empirical data. This algorithm varies from many Lagrangian-based algorithms for solving constrained learning problem (e.g., Primal-Dual method [1o]) in which the optimal predictor parameter and constraint penalties are dependent to each other, and therefore we should learn them iterativaly. However, as we saw in Theorem 5.1 (respectively in Algorithm 1), the solution of $d$ GNP is a mere thresholding on the corresponding embedding functions, where the threshold is obtained in a post-hoc manner and from validation dataset. Therefore, although Lagrangian-based algorithms can lead to oscillations or converge with a large computational cost, the $d$ GNP can potentially reduce such complexity costs and improve convergence conditions. To show such convergence, we bound the generalization error of the objective and constraints based on this solution. These results are extensions of the generalization results for Neyman-Pearson [1, 71] and further hold when multiple constraints should be controlled at once. The first result is the following theorem that shows if the solution to our plug-in method meets constraints of the optimization problem on training data, this generalizes to the unseen data. ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.1 (Generalization of the Constraints). For the approximation of the Neyman-Pearson solution $\\hat{f}_{\\hat{k},\\hat{p}}(x)$ inAlgorithm1 suchthat $\\mathbb{E}_{S^{n}}\\left[\\langle\\hat{f}_{\\hat{k},\\hat{p}}(x),\\hat{\\psi}_{i}(\\bar{x})\\rangle\\right]\\leq\\delta_{i}$ for $i\\in[1:m]$ , if we assume that embeddin finction ae bounded,thnfor $\\begin{array}{r}{d_{n}(\\epsilon)\\simeq O(\\frac{\\sqrt{\\log n}+\\sqrt{\\log1/\\epsilon}}{\\sqrt{n}})}\\end{array}$ and $S^{n}\\sim\\mu$ we have $\\begin{array}{r}{\\mathbb{E}_{\\mu}\\big[\\langle\\hat{f}_{\\hat{k},\\hat{p}}(x),\\psi_{i}(x)\\rangle\\big]\\leq\\delta_{i}+d_{n}(\\frac{\\epsilon}{m})}\\end{array}$ for all $i\\in[1:m]$ and with probability at least $1-\\epsilon.$ ", "page_idx": 7}, {"type": "text", "text": "In the above theorem, we show that the optimal empirical solution for the constraint, probably and approximately satisfies the constraint on true distribution. Therefore, if we assume that we have an approximation $\\hat{\\psi}_{i}(x)$ in hand where $\\|\\hat{\\psi}_{i}(x)-\\psi_{i}(x)\\|_{\\infty}\\,\\le\\,\\epsilon^{\\prime}$ With high probability, this theorem together with Holder's inequality shows that we need to assure $\\mathbb{E}_{S^{n}}\\left[\\langle\\hat{f}_{\\hat{k},\\hat{p}}(x),\\hat{\\psi}_{i}(x)\\rangle\\right]\\leq$ $\\delta-d_{n}(\\frac{\\epsilon}{m})-\\epsilon^{\\prime}$ to achieve the corresponding generalization with high probability. ", "page_idx": 7}, {"type": "image", "img_path": "Mtsi1eDdbH/tmp/83f76ed5abb0f3627f14b57960820479ce320334e9dc0af0a137e207c4a7b8a2.jpg", "img_caption": ["Figure 2: Performance of $d_{\\cdot}$ GNP on COMPAS dataset (left), and ACSIncome (center and right) "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Next, we ask whether the objectives of the empirical optimal solution and the true optimal solution are close. We answer to this question positively in the following theorem. First, however, let us define the notions of $(\\gamma,\\Delta)$ -sensitivity condition as the following. This is an extension to detection condition in [71] and assumes that changing the parameter in predictor leads to a detectable change in constraints. ", "page_idx": 8}, {"type": "text", "text": "Definition 5.2. For an embedding function $\\psi_{1}$ , and a distribution $\\mu_{X}$ on $\\mathcal{X}$ ,we refer to a function $r_{k}(x)$ as a prediction with $(\\gamma,\\,\\Delta)$ -sensitivity around $k$ , if there exists $C\\in\\mathbb{R}^{+}$ such that for all $\\delta\\in(0,\\Delta]$ we have ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\mathbb{E}_{\\mu_{X}}\\left[\\langle r_{k}(x)-r_{k+\\delta}(x),\\psi_{1}(x)\\rangle\\right]\\right|\\geq C\\delta^{\\gamma}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Now, we express the following generalization theorem for predictors that address the above conditions: Theorem 5.3 (Generalization of Objective). Assume that $(\\delta\\mathrm{~-~}\\epsilon_{l},\\delta\\mathrm{~+~}\\epsilon_{u})$ is a subset of of all achievable constraints $\\mathbb{E}\\big[\\langle f(x),\\psi_{1}(x)\\rangle\\big]$ , and that $\\|\\psi_{i}(x)\\|_{\\infty}\\leq1$ for $i=1,2$ .Further, let the size $n$ of validation databe large enough such that $\\begin{array}{r}{d_{n}(\\delta/3)\\leq\\frac{\\epsilon_{l}}{2}}\\end{array}$ . Now, if the optimal predictor $f_{k,0}^{*}(x)$ is $(\\gamma,\\,\\Delta)$ -sensitive around optimal $k^{*}$ for $\\Delta\\,\\simeq\\,\\Omega\\bigl(d_{n}^{1/\\gamma}(\\delta/3),\\delta_{0}^{1/2\\gamma},\\delta_{1}^{1/2\\gamma}\\bigr)$ and $\\gamma\\le1$ , then for $\\begin{array}{r}{n\\geq\\frac{16}{\\epsilon_{l}^{2}}\\log\\frac{3}{\\delta}}\\end{array}$ and withprobbiliy at least $1-\\delta$ tpf 1 has an objective that is at most $O\\big(d_{n}^{1/\\gamma}(\\delta/3),\\delta_{0}^{1/\\gamma},\\delta_{0}^{1/2},\\delta_{1}^{1/2},C^{-1/\\gamma},C^{-1/2}\\big)$ far from the true optimal objective. ", "page_idx": 8}, {"type": "text", "text": "Now that we have proven generalization of our post-processing method, we should briefly compare this to other possible algorithms to learn an approximation of the optimal classifier and rejection function pair. A possible method is to find the appropriate 'defer\u2019 or 'no defer value for each instance in the training dataset, and for a given set of constraints. Although these types of in-processing algorithms can perform computationally efficient (e.g., $O(n\\log{n})$ complexityfor $\\textstyle{\\frac{1}{n}}$ -suboptimal solution for human intervention budget as shown in Theorem G.1), they do not necessarily generalize to unseen data. In particular, we can show that for all algorithms that estimate deferral labels from empirical data, there exist two underlying distributions on the data on which the algorithm results in similar deferral labels, while the optimal rejection functions for these two distributions are not interchangeable. This argument is further formalized in the following proposition: ", "page_idx": 8}, {"type": "text", "text": "Proposition 5.4 (Impossibility of generalization of deferral labels). For every deterministic deferral rule $\\hat{r}$ for empirical distributions and based on the two losses $\\mathbb{1}_{m\\not=y}$ and $\\mathbb{1}_{h(x)\\neq y},$ there exist two probabilitymeasures $\\mu_{1}$ and $\\mu_{2}$ on $\\mathcal{X}\\times\\mathcal{Y}\\times\\mathcal{M}$ such that the corresponding $(\\hat{r},X)$ for both measures is distributed equally. However,the optimal defeal $r_{\\mu_{1}}^{\\ast}$ and r2 for these measures are not interchangeable, that is $L_{\\mathrm{def}}^{\\mu_{i}}(h,r_{\\mu_{i}}^{*})\\leq\\frac{1}{3}$ while $\\begin{array}{r}{L_{\\mathrm{def}}^{\\mu_{i}}(h,r_{\\mu_{j}}^{*})=\\frac{2}{3}}\\end{array}$ for $i=1,2$ and $j\\neq i$ ", "page_idx": 8}, {"type": "text", "text": "In a nutshell, this proposition implies that, every algorithm that reduces the two-bit data of human accuracy and AI accuracy for an input into a single-bit data of \u2018defer\u2019 or \u2018no defer' looses the information that is important for obtaining the optimal rejection function that generalizes to the unseen data. This is a drawback of in-processing algorithms that are used in multi-objective L2D problems. We refer the reader to Appendix M for more details and proof of aforementioned proposition. ", "page_idx": 8}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "COMPAS dataset. We implemented 4 Algorithm 1, first for COMPAS dataset [27] in which the recidivism rate of 7214 criminal defendants is predicted. The human assessment is done in this dataset on 10o0 cases by giving humans a description of the case and asking them whether the defendant would recidivate within two years of their most recent crime.5 The demographic parity is assessed for two racial groups of white and non-white defendants. Figure 2 shows the average performanceof $d$ -GNP over 10 random seeds compared to two baselines: (1) Madras et al. [44] in which a demographic parity regularizer is added to the surrogate loss, and over a variation of 100 regularizer coefficient, and (2) Mozannar et al. [50] in which after training the classifier and rejector pair, we shift the corresponding scores to find a new thresholding rule. All scores, classifiers, and rejection functions are trained on a 1-layer feed-forward neural network. The figure shows that achieving better fairness criteria is possible using $d$ -GNP, while this might not lead to better accuracy when the constraint violation is not of interest. ", "page_idx": 8}, {"type": "image", "img_path": "Mtsi1eDdbH/tmp/5b30a84259876f087293837be38387e6977b824191d9847e47fef926f28eab28.jpg", "img_caption": ["Figure 3: Prediction of $d$ GNP on Hatespeech dataset [22] and for tweets with predicted AfricanAmerican (left), and Non-African-American (center) dialect and the disparity between groups (right) "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Hatespeech dataset. The next experiment is on flagging offensive tweets in Hatespeech dataset [22]. This dataset contains 24,802 tweets that are labeled by at least three crowd workers as hate speech, offensive but not hate speech, or neither hate speech nor offensive. We used a pre-trained model [5] to detect whether the tweet contains an African-American dialect. Next, we used $d$ GNP method to control the demographic disparity of predicting a tweet hate speech or offensive bounded by $\\delta_{H S}=0.1$ and $\\delta_{O}=0.01$ . In the result of this experiment that is displayed in Figure 3 we can observe the following points: (i) in test-time the resulting demographic disparity for both classes are bounded as expected, (ii) the accuracy of $d_{\\cdot}$ GNP method is bounded by the vanilla deferral method, while stricter constraint control (in here offensive prediction parity) keeps the accuracy lower, and (ii) interestingly, the performance of $d$ GNP for controlled offensive prediction parity copies that of human. Therefore, a good strategy for obtaining such constrained learn-to-defer system seems to be to defer the offensive tweet prediction to human, when the tweet contains African-American dialect, and otherwise either bias the classifier scores or use a mixture of human and classifier involvement to achieve the final controlled disparity. ", "page_idx": 9}, {"type": "text", "text": "ACS dataset. We further tested our method on folktables dataset [25] that contains an income prediction task based on 1.6M rows of American Community Survey data. Since we had no access to human expert data, we simulated a human expert that has different accuracy on two racial groups of white and non-white individuals ( $85\\%$ and $60\\%$ , respectively). We considered the L2D problem with bounded equalized odds violation. Figure 2 shows our method's accuracy and constraint violation, coupled with a confidence bound that is obtained using ten iterations of bootstrapping. This figure shows that violation bounds are accurately met for the test data, and the performance increases when these bounds are loosened. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The $d_{\\cdot}$ GNP is a general framework that obtains the optimal solution to various constrained learning problems, including but not limited to multi-objective L2D problems. Using this post-processing framework, we can first estimate the scores related to our problem and then find a linear rule of these scores by fine-tuning for specific violation tolerances. This method reduces the computational complexity of in-processing methods while guaranteeing achieving a near-optimal solution in a large dataregime. ", "page_idx": 9}, {"type": "text", "text": "8Acknowledgment ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "M.A. Charusaie thanks the International Max Planck Research School for Intelligent Systems (IMPRSIS) and Tubingen AI Center for the support and funding of this project. He is further grateful to Matthaus Kleindessner for his significant intellectual contributions to the first draft of this paper. The idea of obtaining an extension to the Neyman-Pearson lemma emerged from discussions with Andr\u00e9 Cruz and Florian Dorner. The very initial draft of this paper was written during an hours-long train delay in Germany, and thus, M.A. Charusaie is thankful to Deutsche Bahn in that regard. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1]  Jean-Yves Audibert and Alexandre B Tsybakov. Fast learning rates for plug-in classifiers. 2007. ", "page_idx": 10}, {"type": "text", "text": "[2] Gagan Bansal, Besmira Nushi, Ece Kamar, Eric Horvitz, and Daniel S Weld. Is the most accurate ai the best teammate? optimizing ai for teamwork. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 11405-11414, 2021. [3]  Peter L Bartlett and Marten H Wegkamp. Classification with a reject option using a hinge loss. Journal of Machine Learning Research, 9(8), 2008. [4] Emma Beede, Elizabeth Baylor, Fred Hersch, Anna Iurchenko, Lauren Wilcox, Paisan Ruamviboonsuk, and Laura M Vardoulakis. A human-centered evaluation of a deep learning system deployed in clinics for the detection of diabetic retinopathy. In Proceedings of the 2020 CHI conference on human factors in computing systems, pages 1-12, 2020. [5]  Su Lin Blodgett, Lisa Green, and Brendan O'Connor. Demographic dialectal variation in social media: A case study of african-american english. arXiv preprint arXiv:1608.08868, 2016. [6]  Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K Warmuth. Learnability and the vapnik-chervonenkis dimension. Journal of the ACM (JACM), 36(4):929-965, 1989. [7] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, et al. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends@ in Machine learning, 3(1):1-122, 2011.   \n[8] Yuzhou Cao, Tianchi Cai, Lei Feng, Lihong Gu, GU Jinjie, Bo An, Gang Niu, and Masashi Sugiyama. Generalizing consistent multi-class classification with rejection to be compatible with arbitrary losses. In Advances in Neural Information Processing Systems. [9] Yuzhou Cao, Hussein Mozannar, Lei Feng, Hongxin Wei, and Bo An. In defense of softmax parametrization for calibrated and consistent learning to defer. Advances in Neural Information Processing Systems, 36, 2024.   \n[10] Luiz FO Chamon, Santiago Paternain, Miguel Calvo-Fullana, and Alejandro Ribeiro. Constrained learning with non-convex losses. IEEE Transactions on Information Theory, 69(3):1739- 1760,2022.   \n[11] Nontawat Charoenphakdee, Zhenghang Cui, Yivan Zhang, and Masashi Sugiyama. Classification with rejection based on cost-sensitive classification. In International Conference on Machine Learning, pages 1507-1517. PMLR, 2021.   \n[12] Mohammad-Amin Charusaie, Hussein Mozannar, David Sontag, and Samira Samadi. Sample efficient learning of predictors that complement humans. In Proceedings of the 39th International Conference on Machine Learning,volume 162 of Proceedings of Machine Learning Research, pages 2972-3005. PMLR, 2022.   \n[13] Richard J Chen, Judy J Wang, Drew FK Williamson, Tiffany Y Chen, Jana Lipkova, Ming Y Lu, Sharifa Sahai, and Faisal Mahmood. Algorithmic fairness in artificial intelligence for medicine and healthcare. Nature biomedical engineering, 7(6):719-742, 2023.   \n[14] Wenlong Chen, Yegor Klochkov, and Yang Liu. Post-hoc bias scoring is optimal for fair classification. arXiv preprint arXiv:2310.05725, 2023.   \n[15] Xin Cheng, Yuzhou Cao, Haobo Wang, Hongxin Wei, Bo An, and Lei Feng. Regression with cost-based rejection. Advances in Neural Information Processing Systems, 36, 2024.   \n[16]  C Chow. On optimum recognition error and reject tradeoff. IEEE Transactions on information theory, 16(1):41-46, 1970.   \n[17] Evgeni Chzhen, Christophe Denis, Mohamed Hebiri, Luca Oneto, and Massimiliano Pontil. Leveraging labeled and unlabeled data for consistent fair binary classification. Advances in Neural Information Processing Systems, 32, 2019.   \n[18] Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. Algorithmic decision making and the cost of fairness. In Proceedings of the 23rd acm sigkdd international conference on knowledge discovery and data mining, pages 797-806, 2017.   \n[19] Corinna Cortes, Giulia DeSalvo, and Mehryar Mohri. Learning with rejection. In Algorithmic Learning Theory: 27th International Conference, pages 67-82. Springer, 2016.   \n[20]  Andre F Cruz and Moritz Hardt. Unprocessing seven years of algorithmic fairness. arXiv preprint arXiv:2306.07261, 2023.   \n[21]  George B Dantzig and Abraham Wald. On the fundamental lemma of neyman and pearson. The Annals of Mathematical Statistics, 22(1):87-93, 1951.   \n[22]  Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. Automated hate speech detection and the problem of offensive language. In Proceedings of the international AAAl conference on web and social media, volume 11, pages 512-515, 2017.   \n[23]  Abir De, Paramita Koley, Niloy Ganguly, and Manuel Gomez-Rodriguez. Regression under human assistance. In Proceedings of the AAAl Conference on Artificial Intelligence, volume 34, pages 2611-2620, 2020.   \n[24]  Abir De, Nastaran Okati, Ali Zarezade, and Manuel Gomez Rodriguez. Classification under human assistance. InProceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 5905-5913, 2021.   \n[25]  Frances Ding, Moritz Hardt, John Miller, and Ludwig Schmidt. Retiring adult: New datasets for fair machine learning. Advances in neural information processing systems, 34:6478-6490, 2021.   \n[26] Kate Donahue, Alexandra Chouldechova, and Krishnaram Kenthapadi. Human-algorithm collaboration: Achieving complementarity and avoiding unfairness. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, pages 1639-1656, 2022.   \n[27] Julia Dressel and Hany Farid. The accuracy, fairness, and limits of predicting recidivism. Science advances, 4(1):eaa05580, 2018.   \n[28] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In Proceedings of the 3rd innovations in theoretical computer science conference, pages 214-226, 2012.   \n[29] Cynthia Dwork and Christina Ivento. Fairness under composition. arXiv preprint arXiv:1806.06122, 2018.   \n[30]  Ran El-Yaniv et al. On the foundations of noise-free selective classification. Journal of Machine Learning Research, 11(5), 2010.   \n[31] David Heaver Fremlin. Measure theory, volume 4. Torres Fremlin, 2000.   \n[32]  Aditya Gangrade, Anil Kag, and Venkatesh Saligrama. Selective classification via one-sided prediction. In International Conference on Artificial Intelligence and Statistics, pages 2179- 2187. PMLR, 2021.   \n[33]  Yonatan Geifman and Ran El- Yaniv. Selective classification for deep neural networks. Advances in neural information processing systems, 30, 2017.   \n[34]  Morit Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. Advances in neural information processing systems, 29, 2016.   \n[35]  Xiaoqian Jiang, Melanie Osl, Jihoon Kim, and Lucila Ohno-Machado. Calibrating predictive model estimates to support personalized medicine. Journal of the American Medical Informatics Association, 19(2):263-274, 2012.   \n[36] Erik Jones, Shiori Sagawa, Pang Wei Koh, Ananya Kumar, and Percy Liang. Selective classification can magnify disparities across groups. arXiv preprint arXiv:2010.14134, 2020.   \n[37] Ece Kamar, Severin Hacker, and Eric Horvitz. Combining human and machine intelligence in large-scale crowdsourcing. In AAMAS, volume 12, pages 467-474, 2012.   \n[38]  Achim Klenke. Probability theory: a comprehensive course. Springer Science & Business Media, 2013.   \n[39] Benjamin Kompa, Jasper Snoek, and Andrew L Beam. Second opinion needed: communicating uncertainty in medical machine learning. NPJ Digital Medicine, 4(1):1-6, 2021.   \n[40]  Thomas Landgrebe and R Duin. On neyman-pearson optimisation for multiclass classifiers. In Proceedings 16th Annual Symposium of the Pattern Recognition Association of South Africa. PRASA, pages 165-170, 2005.   \n[41] Joshua K Lee, Yuheng Bu, Deepta Rajan, Prasanna Sattigeri, Rameswar Panda, Subhro Das, and Gregory W Wornell. Fair selective classification via sufficiency. In International conference on machine learning, pages 6076-6086. PMLR, 2021.   \n[42] Erich Leo Lehmann, Joseph P Romano, and George Casella. Testing statistical hypotheses, volume 3. Springer, 1986.   \n[43] Shuqi Liu, Yuzhou Cao, Qiaozhen Zhang, Lei Feng, and Bo An. Mitigating underfitting in learning to defer with consistent losses. In International Conference on Artificial Intelligence and Statistics, pages 4816-4824. PMLR, 2024.   \n[44] David Madras, Toni Pitasi, and Richard Zemel. Predict responsibly: improving fairness and accuracy by learning to defer. Advances in Neural Information Processing Systems, 31, 2018.   \n[45]  Anqi Mao, Mehryar Mohri, and Yutao Zhong. Predictor-rejector multi-class abstention: Theoretical analysis and algorithms. In International Conference on Algorithmic Learning Theory, pages 822-867. PMLR, 2024.   \n[46]  Charles E Metz. Basic principles of roc analysis. In Seminars in nuclear medicine, volume 8, pages 283-298. Elsevier, 1978.   \n[47]  Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT press, 2018.   \n[48] Hussein Mozannar, Hunter Lang, Dennis Wei, Prasanna Sattigeri, Subhro Das, and David Sontag. Who should predict? exact algorithms for learning to defer to humans. In International conference on artificial intelligence and statistics, pages 10520-10545. PMLR, 2023.   \n[49] Hussein Mozannar, Hunter Lang, Dennis Wei, Prasanna Sattigeri, Subhro Das, and David Sontag. Who should predict? exact algorithms for learning to defer to humans. arXiv preprint arXiv:2301.06197, 2023.   \n[50] Hussein Mozannar and David Sontag. Consistent estimators forlearning to defer to an expert. In International Conference on Machine Learning, pages 7076-7087. PMLR, 2020.   \n[51] Harikrishna Narasimhan, Wittawat Jitkrittum, Aditya K Menon, Ankit Rawat, and Sanjiv Kumar. Post-hoc estimators for learning to defer to an expert. Advances in Neural Information Processing Systems, 35:29292-29304, 2022.   \n[52] Harikrishna Narasimhan, Aditya Krishna Menon, Wittawat Jitkrittum, Neha Gupta, and Sanjiv Kumar. Learning to reject meets long-tail learning. In The Twelth International Conference on Learning Representations.   \n[53] Harikrishna Narasimhan, Aditya Krishna Menon, Wittawat Jitkrittum, and Sanjiv Kumar. Plugin estimators for selective classification with out-of-distribution detection. arXiv preprint arXiv:2301.12386, 2023.   \n[54] Harikrishna Narasimhan, Harish G Ramaswamy, Shiv Kumar Tavker, Drona Khurana, Praneeth Netrapalli, and Shivani Agarwal. Consistent multiclass algorithms for complex metrics and constraints. arXiv preprint arXiv:2210.09695, 2022.   \n[55]  Jerzy Neyman and Egon Sharpe Pearson. Ix. on the problem of the most efficient tests of statistical hypotheses. Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character, 231(694-706):289-337, 1933.   \n[56]  Jerzy Neyman and Egon Sharpe Pearson. Contributions to the theory of testing statistical hypotheses. Statistical research memoirs, 1936.   \n[57]  Nastaran Okati, Abir De, and Manuel Rodriguez. Differentiable learning under triage. Advances in Neural Information Processing Systems, 34:9140-9151, 2021.   \n[58]  Christos H Papadimitriou and Kenneth Steiglitz. Combinatorial optimization: algorithms and complexity. Courier Corporation, 1998.   \n[59] Charles Chapman Pugh and CC Pugh. Real mathematical analysis, volume 2011. Springer, 2002.   \n[60] Maithra Raghu, Katy Blumer, Greg Corrado, Jon Kleinberg, Ziad Obermeyer, and Sendhil Mullainathan. The algorithmic automation problem: Prediction,triage, and human effort. arXiv preprint arXiv: 1903.12220, 2019.   \n[61]  Philippe Rigollet and Xin Tong. Neyman-pearson classification, convexity and stochastic constraints. Journal of machine learning research, 2011.   \n[62] Salvatore Ruggieri, Jose M Alvarez, Andrea Pugnana, Franco Turini, et al. Can we trust fair-ai? In Proceedings of the AAAl Conference on Artificial Intelligence, volume 37, pages 15421-15430, 2023.   \n[63] Stephen-John Sammut, Mireia Crispin-Ortuzar, Suet-Feung Chin, Elena Provenzano, Helen A Bardwell, Wenxin Ma, Wei Cope, Ali Dariush, Sarah-Jane Dawson, Jean E Abraham, et al. Multi-omicmachine leaning predictor of breast cancer therapy response. Natre, 601(7894):623-629, 2022.   \n[64]  Clayton Scott. Performance measures for neyman-pearson classification. IEEE Transactions on Information Theory, 53(8):2852-2863, 2007.   \n[65]  Clayton Scott and Robert Nowak. A neyman-pearson approach to statistical learning. IEEE Transactions on Information Theory, 51(11):3806-3819, 2005.   \n[66]  Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge University Press, 2014.   \n[67] Dharmesh Tailor, Aditya Patra, Rajeev Verma, Putra Manggala, and Eric Nalisnick. Learning to defer to a population: A meta-learning approach. In International Conference on Artificial Intelligence and Statistics, pages 3475-3483. PMLR, 2024.   \n[68] Sarah Tan, Julius Adebayo, Kori Inkpen, and Ece Kamar. Investigating human $^+$ machine complementarity for recidivism predictions. arXiv preprint arXiv:1808.09123, 2018.   \n[69] Ye Tian and Yang Feng. Neyman-pearson multi-class classification via cost-sensitive learning. arXiv preprint arXiv:2111.04597, 2021.   \n[70] Alexandru Tifrea, Preethi Lahoti, Ben Packer, Yoni Halpern, Ahmad Beirami, and Flavien Prost. Frappe: A group fairness framework for post-processing everything. In Forty-first International Conference on Machine Learning.   \n[71] Xin Tong. A plug-in approach to neyman-pearson classification. The Journal of Machine Learning Research, 14(1):3011-3040, 2013.   \n[72] C Vermeulen, M Pages-Gallego, L Kester, MEG Kranendonk, P Wesseling, N Verburg, P de Witt Hamer, EJ Kooi, L Dankmeijer, J van der Lugt, et al. Ultra-fast deep-learned cns tumour classification during surgery. Nature, 622(7984):842-849, 2023.   \n[73]  Ruicheng Xian, Lang Yin, and Han Zhao. Fair and optimal classification via post-processing. In International Conference on Machine Learning, pages 37977-38012. PMLR, 2023.   \n[74] Tongxin Yin, Jean-Francois Ton, Ruocheng Guo, Yuanshun Yao, Mingyan Liu, and Yang Liu. Fair classifiers that abstain without harm. arXiv preprint arXiv:2310.06205, 2023.   \n[75] Xianli Zeng, Guang Cheng, and Edgar Dobriban. Bayes-optimal fair classification with linear disparity constraints via pre-, in-, and post-processing. arXiv preprint arXiv:2402.02817, 2024.   \n[76] Xianli Zeng, Edgar Dobriban, and Guang Cheng. Bayes-optimal classifiers under group fairness. arXiv preprint arXiv:2202.09724, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Content of Appendices ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A   Lack of Compositionality of Fairness Criteria 17   \nB Extended Related Works 17   \nC  Rephrasing (2) into Linear Functional Programming 18   \nD Derivation of Embedding Functions 19   \nE Limitations of Cost-Sentitive Methods 22   \nF $d$ GNP Learning Algorithm 24   \nG  On Failure of In-Processing Methods 24   \nH Proof of Theorem 3.1 27   \nProof of Theorem 4.1 28   \nJ Proof of Theorem 4.2 33   \nK Proof of Theorem 5.1 40   \nL Proof of Theorem 5.3 43   \nM Proof of Theorem G.1 48 ", "page_idx": 15}, {"type": "text", "text": "A Lack of Compositionality of Fairness Criteria ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Here, we show an example of lack of compositionality of fairness criteria for learn-to-defer problems. This falls in line with [29], where the authors studied the effect of the operators such as ^OR' or \"AND'. Here, we show that a similar non-compositionality holds for the operator DEFER'. The following example is found based on the insight that a fair predictor is fair over all the space $\\mathcal{X}$ ,and if it could take a decision over only a subset of $\\mathcal{X}$ it will not necessarily be a fair predictor. This can be seen as a particular application of Yule's effect [62] which explains that vanishing correlation in a mixture of distributions does not necessarily concludes vanishing correlation on each of such distributions. ", "page_idx": 16}, {"type": "text", "text": "Let us assume that the space $\\mathcal{X}$ contains only four points $x_{1},x_{2},x_{3}$ , and $x_{4}$ , and that the input takes these values with probability $\\begin{array}{r}{\\operatorname*{Pr}(X=x_{1})=\\operatorname*{Pr}(X=x_{2})=\\operatorname*{Pr}(X=x_{3})=\\operatorname*{Pr}(X=x_{4})=\\frac{1}{4}}\\end{array}$ The first two points $x_{1},x_{2}$ are corresponded to the demographic group $A=0$ and the last two points are corresponded to the demographic group $A\\,=\\,1$ . Further, assume that the conditional target probability is $\\operatorname*{Pr}(Y=1|x_{1})=\\operatorname*{Pr}^{\\star}(Y=1|{\\dot{x}}_{2})=\\operatorname*{Pr}(Y=1|x_{3})=\\operatorname*{Pr}(Y=1|x_{4})=1$ . Moreover, we consider the equality of opportunity as the measure of fairness. Now, assume that the classifier $h(\\cdot):\\mathcal{X}\\rightarrow\\{0,1\\}$ is taking values $\\dot{h(x_{1})}\\,=\\,1,\\,h(x_{2})\\,=\\,0,\\,h(x_{3})\\,=\\,1$ , and $h(x_{4})\\,=\\,0$ and the human decision maker predicts $M=0$ conditioned on $x_{1}$ \uff0c $M=1$ conditioned on $x_{2}$ , and $M=1$ conditioned on $x_{3}$ , and $M=0$ conditioned on $x_{4}$ . Therefore, both classifier and human expert have accuracy of $\\frac{1}{2}$ on the data. ", "page_idx": 16}, {"type": "text", "text": "Following the above assumptions, we can find the fairness measure for classifier as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}(h(X)=1|Y=1,A=0)-\\operatorname*{Pr}(h(X)=1|Y=1,A=1)}\\\\ &{\\quad=\\operatorname*{Pr}(h(X)=1|Y=1,A=0,X=x_{1})\\operatorname*{Pr}(X=x_{1}|Y=1,A=0)}\\\\ &{\\quad\\,+\\operatorname*{Pr}(h(X)=1|Y=1,A=0,X=x_{2})\\operatorname*{Pr}(X=x_{2}|Y=1,A=0)}\\\\ &{\\quad\\,-\\operatorname*{Pr}(h(X)=1|Y=1,A=1,X=x_{3})\\operatorname*{Pr}(X=x_{3}|Y=1,A=1)}\\\\ &{\\quad\\,-\\operatorname*{Pr}(h(X)=1|Y=1,A=1,X=x_{4})\\operatorname*{Pr}(X=x_{4}|Y=1,A=1)=\\frac{1}{2}+0-\\frac{1}{2}-0=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which means that the classifier is fully fair. We can derive a similar result for the human expert, i.e. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}(M=1|Y=1,A=0)-\\operatorname*{Pr}(M=1|Y=1,A=1)=0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now that we established a fair classifier and a fair expert, we take the step to find an optimal deferral solution, i.e., a deferral system that minimizes the overall loss. We can observe that for $x_{1}$ the classifier is accurate, while for $x_{2}$ the human expert is accurate. Furthermore, for $x_{3}$ and $x_{4}$ they both are equally inaccurate. Therefore, an optimal solution is not to defer for $x_{1}$ , and defer for $x_{2}$ , and take an arbitrary decision for $x_{3}$ and $x_{4}$ . Now, if we find the fairness measure of the resulting deferral predictor, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}(\\hat{Y}=1|Y=1,A=0)-\\operatorname*{Pr}(\\hat{Y}=1|Y=1,A=1)}\\\\ &{=\\operatorname*{Pr}(h(X)=1|Y=1,A=0,X=x_{1})\\operatorname*{Pr}(X=x_{1}|Y=1,A=0)}\\\\ &{\\quad+\\operatorname*{Pr}(M=1|Y=1,A=0,X=x_{2})\\operatorname*{Pr}(X=x_{2}|Y=1,A=0)}\\\\ &{\\quad-\\operatorname*{Pr}(h(X)=1|Y=1,A=1,X=x_{3})\\operatorname*{Pr}(X=x_{3}|Y=1,A=1)}\\\\ &{\\quad-\\operatorname*{Pr}(h(X)=1|Y=1,A=1,X=x_{4})\\operatorname*{Pr}(X=x_{4}|Y=1,A=1)=\\frac{1}{2}+\\frac{1}{2}-\\frac{1}{2}-0=\\frac{1}{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "or equivalently the resulting predictor is unfair for the demographic group $A=1$ .This means the \u2018DEFER\u2019 composition of the predictors does not preserve fairness. One can further easily show that no deferral system from the above classifier and human expert that has the accuracy better than $\\frac{1}{2}$ is fair. ", "page_idx": 16}, {"type": "text", "text": "B Extended Related Works ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The deferral problem has been studied under a variety of conditions. Rejection learning [19, 3, 11, 15] or selective classification [30, 33, 32], assumes that a fixed cost is incurred to the overall loss, when ML decides not to make a prediction on an input. The first Bayes optimal rule for rejection learning was derived in [16]. Assuming that the accuracy of human, and consequently the cost of deferring to the human, can vary for different inputs, [50] obtained the Bayes optimal deferral rule. The deferral problem is further studied assuming that the number of available instances for deferral are bounded and a near-optimal classifier and deferral rule is required as a solution of empirical risk minimization [23, 24]. Most recently, the implementation of deferral rules using neural networks and surrogate losses is studied for binary and multi-class classification [8, 50, 12, 51, 9, 43, 48, 45]. A possible shift in human expert for L2D methods recently studied in [67]. The problem multi-objective L2D and rejection learning is mainly studied in an in-processing approach. A few instances of tackling such problems can be found in [57, 52, 53] and [74, 41] for L2D and rejection learning, respectively. Neyman-Pearson's fundamental lemma is introduced in [55] originally for binary hypothesis testing and later was generalized in [56] to give a close-form formulation for a variety of binary constrained optimization problems. Later, [21] found conditions for which Neyman and Pearson solution exists and is unique. The generalization error of the empirical solution to Neyman-Pearson problem is studied in two lines of works: (i) the generalization of direct (in-processing) solutions to the optimization problem [65, 64, 61], and (ii) the generalization of plug-in methods [71] that first approximate the score functions and then use Neyman-Pearson lemma to approximate the predictor. The generalization of Neyman-Pearson lemma to multiclass setting is first empirically studied in [40] and under strong duality assumption is proved in [69].  Our lemma $d$ GNPextendsthese works in order to (i) be able to control a general set of constraints instead of Type- $K$ errors, and (ii) be valid in absence of strong duality assumption. Further, the idea of using Neyman-Pearson lemma for controlling fairness criteria originally dates back to [76] (later as [75]). More recently, a similar post-processing method is introduced in [14] using cost-sensitive learning and strong duality technique. Although these works cover binary classification problem, in this paper we focus on solving multi-class classification problem, and particularly in a deferral system. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Moreover, his work differs from multi-class classification with complex performance metrics [54] in the sense that they consider constraints that are non-linear functions of confusion matrix, while ignoring the dependence on input $x$ . In our setting, the constraints are linear in terms of confusion matrix when conditioned on the input, but the linear coefficients vary with the input. ", "page_idx": 17}, {"type": "text", "text": "Finally, the work [70] has recently studied an extension of post-processing method to other constrained learning problems. The difference of that work with our method is threefold: (i) while we prove that the optimal post-processing method is a linear combination of scores, they have no such claim, (ii) we have no assumption on the format of the loss function, while they assume a particular set of strictly convex loss functions, (ii) we have no bound on our hypothesis class while they assume the representation of the predictor with a multidimensional vector and a fixed dimension. ", "page_idx": 17}, {"type": "text", "text": "C  Rephrasing (2) into Linear Functional Programming ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Here, we frst characterize functions that are outcome-dependent. To that end, we define $\\iota(x)$ as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\iota=\\left[\\mathbb{I}_{r(x)=0}\\mathbb{I}_{h(x)=1},\\ldots,\\mathbb{I}_{r(x)=0}\\mathbb{I}_{h(x)=L},\\mathbb{I}_{r(x)=1}\\right]\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This function can retrieve the value of $r(x)$ and can retrieve the value of $h(x)$ only if $r(x)=0$ In fact, we can obtain $r(x)={\\bigl(}\\imath(x){\\bigr)}(L+1)$ and $h(x)=i$ $r(x)=0$ and $\\bigl(\\i(x)\\bigr)(i)=1$ . Therefore, for a function $\\begin{array}{r}{\\overline{{\\Psi}}(x,h(x),r(x))\\,=\\,\\mathbb{E}_{Y,M|X=x}\\bigl[\\Psi(x,Y,M,h(x),r(x))\\bigr]}\\end{array}$ and $\\bar{\\ell}_{\\mathrm{def}}(x,h(x),r(x))\\,=$ $\\mathbb{E}_{Y,M|X=x}\\big[\\ell_{\\operatorname*{def}}(x,Y,M,h(x),r(x))\\big]$ to be outcome dependent, it must only be a function of $x$ and $\\iota(x)$ . In fact, we must have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\overline{{\\Psi}}_{i}(x,h(x),r(x))=\\Psi_{i}^{\\prime}(x,\\imath(x)),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{\\ell}}_{\\mathrm{def}}\\bigl(x,h(x),r(x)\\bigr)=\\ell_{\\mathrm{def}}^{\\prime}(x,\\iota(x)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for a choice of $\\Psi^{\\prime}$ and $\\ell_{\\mathrm{def}}^{\\prime}$ , where $\\overline{{\\ell}}_{\\mathrm{def}}(\\boldsymbol{x},h(\\boldsymbol{x}),\\boldsymbol{r}(\\boldsymbol{x}))=\\mathbb{E}_{Y,M|X=x}\\big[\\ell_{\\mathrm{def}}(\\boldsymbol{x},Y,M,h(\\boldsymbol{x}),\\boldsymbol{r}(\\boldsymbol{x}))\\big]$ Now, we can check that $\\iota(x)$ can take $L+1$ different values, in each of which one of its components takes the value 1 and others take the value 0. Therefore, by conditioning on each of these $L+1$ values we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Psi_{i}^{\\prime}(x,\\iota(x))=\\sum_{i=1}^{L+1}\\Psi^{\\prime}(x,[0,\\ldots,\\underbrace{1}_{i},\\ldots,0])\\Big((\\iota(x))(i)\\Big)=\\langle\\iota(x),\\psi_{i}(x)\\rangle,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\psi_{i}(x)$ is defined as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\psi_{i}(x)=\\Big[\\Psi_{i}^{\\prime}(x,[1,0,\\dots,0]),\\dots,\\Psi^{\\prime}(x,[0,0,\\dots,1])\\Big]}\\\\ &{\\qquad\\quad=\\big[\\overline{{\\Psi}}_{i}(x,1,0),\\dots,\\overline{{\\Psi}}_{i}(x,L,0),\\overline{{\\Psi}}_{i}(x,0,1)\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Similarly, we can show that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\ell_{\\mathrm{def}}^{\\prime}(x,\\iota(x))=\\langle\\iota(x),\\vec{\\ell}_{\\mathrm{def}}(x)\\rangle,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\vec{\\ell_{\\mathrm{def}}}(x)$ is defined as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\vec{\\ell}_{\\mathrm{def}}(x)=\\left[\\vec{\\ell}_{\\mathrm{def}}(x,1,0),\\ldots,\\vec{\\ell}_{\\mathrm{def}}(x,L,0),\\vec{\\ell}_{\\mathrm{def}}(x,0,1)\\right]\\!.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Next, we know that due to the randomization of $\\boldsymbol{\\mathcal{A}}$ , the vector $\\iota(x)$ can take various values on each instance $x$ . This, however, is not the case for $\\psi_{i}(x)$ and $\\vec{\\ell_{\\mathrm{def}}}(x)$ , since they are defined independent of $r(x)$ and $h(x)$ . Therefore, the average of constraints and loss can be rewritten as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{(r,h)\\sim A}\\big[\\overline{{\\Psi}}_{i}(x,h(x),r(x))\\big]=\\mathbb{E}_{(r,h)\\sim A}\\big[\\langle\\psi_{i}(x),\\iota(x)\\rangle\\big]=\\langle f(x),\\psi_{i}(x)\\rangle,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{(r,h)\\sim A}\\big[\\ell_{\\mathrm{def}}(x,h(x),r(x))\\big]=\\mathbb{E}_{(r,h)\\sim A}\\big[\\langle\\vec{\\ell}_{\\mathrm{def}}(x),\\iota(x)\\rangle\\big]=\\langle f(x),\\vec{\\ell}_{\\mathrm{def}}(x)\\rangle,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $f(x)$ is defined as ", "page_idx": 18}, {"type": "equation", "text": "$$\nf(x)=\\mathbb{E}[\\imath(x)]=\\left[\\operatorname*{Pr}(r(x)=0,h(x)=1),\\ldots,\\operatorname*{Pr}(r(x)=0,h(x)=L),\\operatorname*{Pr}(r(x)=1)\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, the optimization problem in (2) is effectively reduced to the linear programming problem in (3). Moreover, if $f^{*}(x)$ is the solution to that linear program, then the corresponding $r(x)$ should be distributed as $\\operatorname*{Pr}(r(x)\\;=\\;1)\\;=\\;\\bigl(f^{*}(x)\\bigr)(L+1)$ ,where $h(x)$ should be distributed as $\\begin{array}{r}{\\operatorname*{Pr}(h(x)\\;=\\;i)\\;=\\;\\operatorname*{Pr}(h(x)\\;=\\;i|r(x)\\;=\\;0)\\;=\\;\\frac{\\big(\\,f(x)\\,\\big)\\,(i)}{\\sum_{i=1}^{L}\\,\\big(f(x)\\big)\\,(j)}}\\end{array}$ . Note that the assumption of independence of $h(x)$ and $r(x)$ comes with no loss of generality, since the value of $h(x)$ does not variate the loss or constraints in the system when we have $r(x)=1$ ", "page_idx": 18}, {"type": "text", "text": "D   Derivation of Embedding Functions ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this appendix we derive the embedding functions in Table 1 that are corresponded to the constraints of choice, as named in Section 3. The trick that we use for all these constraints is that we first rewrite the constraint in terms of the expected value of a function over the randomness of the algorithm $\\boldsymbol{\\mathcal{A}}$ and the inputvariable $X$ , and then we use (17) to transform that function into the embedding function. ", "page_idx": 18}, {"type": "text", "text": "\u00b7 Overall Loss: To find the embedding function that is corresponded to the overall loss of the system, we should first note that by loss we mean the probability of incorrectness of $\\hat{Y}$ Therefore, the corresponding $\\ell_{d e f}(x,\\dot{h}(x),r(x))$ in this case, as defined in (1) is obtained as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{\\ell}}_{d e f}(x,h(x),r(x))=\\!\\mathbb{E}_{Y,M|X=x}\\!\\left[\\mathbb{I}_{r(x)=1}\\mathbb{I}_{M\\neq Y}+\\mathbb{I}_{r(x)=0}\\mathbb{I}_{h(x)\\neq Y}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathbb{I}_{r(x)=1}\\operatorname*{Pr}(M=Y|X=x)+\\mathbb{I}_{r(x)=0}\\operatorname*{Pr}(Y\\neq h(x)|X=x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, using (19) we find $\\vec{\\ell}_{\\mathrm{def}}$ as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\vec{\\ell}_{\\mathrm{def}}=\\big[\\operatorname*{Pr}(Y\\neq1|X=x),\\ldots,\\operatorname*{Pr}(Y\\neq n|X=x),\\operatorname*{Pr}(Y\\neq M|X=x)\\big].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "\u00b7 Expert intervention budget: In this case, similar to the case before, we first derive $\\overline{{\\Psi}}(\\overline{{x}},h(x),r(x))$ . To that end, we first note that the expert intervention constraint in Section 3 is equivalent with ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}(r(X)=1)=\\mathbb{E}_{x\\sim\\mu_{X},(r,h)\\sim A}\\big[\\mathbb{I}_{r(x)=1}\\big]\\leq\\delta,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which in turn suggests that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\overline{{\\Psi}}(x,h(x),r(x))=\\mathbb{I}_{r(x)=1}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Next, we find $\\psi(x)$ using (17), as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\psi(x)=[0,\\ldots,0,1].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "\u00b7 OOD Detection: To obtain the corresponding embedding function to the OOD detection constraint in Section 3, we can rewrite $\\mathrm{Pr}_{\\mathrm{out}}$ $\\tilde{\\left[{r(X)=1}\\right]}$ as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{Pr}_{\\mathrm{out}}\\left(r(X)=1\\right)=\\mathbb{E}_{X\\sim f_{X}^{\\mathrm{out}},(r,h)\\sim A}\\big[\\mathbb{I}_{r(X)=1}\\big]}&{=\\mathbb{E}_{X\\sim\\mu_{X^{\\mathrm{in}}},(r,h)\\sim A}\\big[\\frac{\\mathbb{I}_{r}(X)=1\\,f_{X}^{\\mathrm{out}}(X)}{f_{X}^{\\mathrm{in}}(X)}\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the last equation holds when $X$ and $X_{\\mathrm{out}}$ are absolutely continuous distributions, and therefore have probability density functions. A similar assumption is made by [53]. This results in $\\overline{{\\Psi}}(x,h(x),r(x))$ being obtained as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{\\Psi}}(x,h(x),r(x))=\\frac{\\mathbb{I}_{r(x)=1}f_{X}^{\\mathrm{out}}(X)}{f_{X}^{\\mathrm{in}}(X)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, we conclude that the embedding function can be calculated using (17) as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\psi(x)=\\big[0,\\dots,0,\\frac{f_{X}^{\\mathrm{out}}(X)}{f_{X}^{\\mathrm{in}}(X)}\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In the simplecasetat $\\begin{array}{r}{f_{X}^{\\mathrm{out}}(x)=\\frac{f_{X}^{\\mathrm{in}}(x)\\mathbb{I}_{f_{X}^{\\mathrm{in}}(x)\\leq\\epsilon}}{\\int f_{X}^{\\mathrm{in}}(x)\\mathbb{I}_{f_{X}^{\\mathrm{in}}(x)\\leq\\epsilon}\\mathrm{d}x}}\\end{array}$ thebedin fuetonis quato ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\psi(x)=\\big[0,\\dots,0,\\frac{\\mathbb{I}_{f_{X}^{\\mathrm{in}}(x)\\le\\epsilon}}{\\operatorname*{Pr}_{\\mathrm{in}}(f_{X}^{\\mathrm{in}}(X)\\le\\epsilon)}\\big].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "\u00b7 Long-Tail Classification: This methodology aims to minimize the balanced loss ", "text_level": 1, "page_idx": 19}, {"type": "equation", "text": "$$\n{\\frac{1}{K}}\\sum_{i=1}^{K}\\operatorname*{Pr}(Y\\neq h(X)|r(X)=0,Y\\in G_{i}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "However, as mentioned in [52], this optimization problem can be rewritten as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{K}\\frac{\\operatorname*{Pr}(Y\\ne h(X),r(X)=0|Y\\in G_{i})}{\\alpha_{i}},~~~~\\mathrm{s.t.}~~~\\operatorname*{Pr}(r(X)=0|Y\\in G_{i})=\\frac{\\alpha_{i}}{K}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, the objective can rewritten as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{K}\\frac{\\mathbb{E}_{(r,h)\\sim\\mathcal{A},X^{\\prime}\\sim\\mu_{X}}\\left[\\operatorname*{Pr}(Y\\neq h(X),r(X)=0,Y\\in G_{i}|X=X^{\\prime})\\right]}{\\alpha_{i}\\operatorname*{Pr}(Y\\in G_{i})},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which together with (17) shows that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\psi_{0}(x)=-\\Big[\\sum_{i=1}^{K}\\frac{\\operatorname*{Pr}(Y\\neq1,Y\\in G_{i}|X=x)}{\\alpha_{i}\\operatorname*{Pr}(Y\\in G_{i})},\\dots,\\sum_{i=1}^{K}\\frac{\\operatorname*{Pr}(Y\\neq L,Y\\in G_{i}|X=x)}{\\alpha_{i}\\operatorname*{Pr}(Y\\in G_{i})},0\\Big].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The reason that we use negative sign is because in the definition of (3) we aim to maximize the objective. ", "page_idx": 19}, {"type": "text", "text": "Similarly, we can rewrite the objectives as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\mathbb{E}_{(r,h)\\sim A,X^{\\prime}\\sim\\mu_{X}}\\left[\\operatorname*{Pr}(r(X)=0,Y\\in G_{i}|X=X^{\\prime})-\\frac{\\alpha_{i}}{K}\\operatorname*{Pr}(Y\\in G_{i})\\right]}{\\operatorname*{Pr}(Y\\in G_{i})}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, using (17) we can obtain $\\psi_{i}(x)$ as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\psi_{i}(x)={\\frac{\\mathrm{Pr}(Y\\in G_{i}|X=x)}{\\mathrm{Pr}(Y\\in G_{i})}}\\Big[1,\\dots,1,0\\Big]-{\\frac{\\alpha_{i}}{K}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "\u00b7Type- $k$ Error Bound: We first rewrite Type- $k$ constraint in 3 as ", "text_level": 1, "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}(\\hat{Y}\\neq k|Y=k)=\\frac{\\operatorname*{Pr}(\\hat{Y}\\neq k,Y=k)}{\\operatorname*{Pr}(Y=k)}}\\\\ &{\\qquad\\qquad\\qquad\\overset{(a)}{=}\\frac{\\mathbb{E}_{X\\sim\\mu_{X}}\\left[\\operatorname*{Pr}(\\hat{Y}\\neq k,Y=k|X=x)\\right]}{\\operatorname*{Pr}(Y=k)}}\\\\ &{\\qquad\\qquad=\\frac{\\mathbb{E}_{X\\sim\\mu_{X}}\\left[\\operatorname*{Pr}(\\hat{Y}\\neq k|Y=k,X=x)\\operatorname*{Pr}(Y=k|X=x)\\right]}{\\operatorname*{Pr}(Y=k)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $(a)$ is followed by chain rule. ", "page_idx": 20}, {"type": "text", "text": "Next, we condition $\\operatorname*{Pr}({\\hat{Y}}\\neq k|Y=k,X=x)$ on $r(X)$ being 1 and 0, which concludes that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}(\\hat{Y}\\neq k|Y=k,X=x)=\\operatorname*{Pr}(\\hat{Y}\\neq k,r(x)=1|Y=k,X=x)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\operatorname*{Pr}(\\hat{Y}\\neq k,r(x)=0|Y=k,X=x)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\operatorname*{Pr}(M\\neq k,r(x)=1|Y=k,X=x)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\operatorname*{Pr}(h(x)\\neq k,r(x)=0|Y=k,X=x)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\operatorname*{Pr}(\\jmath_{\\hat{X}}{A}_{\\hat{Y}}{\\times}{A}_{\\hat{Y}}{(x)}_{\\hat{Y}}{(x)}_{\\hat{Y}}{(x)}_{\\hat{Y}}{(x)}_{\\hat{Y}}{(x)}_{\\hat{Y}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, using (24) we conclude that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}(\\hat{Y}\\neq k|Y=k)=\\frac{\\mathbb{E}_{X^{\\prime}\\sim\\mu_{X},(r,h)\\sim A}\\big[\\mathbb{I}_{r(X)=1}\\operatorname*{Pr}(M\\neq k,Y=k|X=X^{\\prime})\\big]}{\\operatorname*{Pr}(Y=k)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,\\frac{\\mathbb{E}_{X^{\\prime}\\sim\\mu_{X},(r,h)\\sim A}\\big[\\mathbb{I}_{h(X^{\\prime})\\neq k}\\mathbb{I}_{r(X^{\\prime})=0}\\operatorname*{Pr}(Y=k|X=X^{\\prime})\\big]}{\\operatorname*{Pr}(Y=k)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which together with (17) shows that the embedding function is obtained as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\psi(x)={\\frac{\\operatorname*{Pr}(Y=k|X=x)}{\\operatorname*{Pr}(Y=k)}}{\\Big[}1,\\ldots,1,\\underbrace{0}_{k},1,\\ldots,1,\\operatorname*{Pr}(M\\neq k|X=x,Y=k){\\Big]}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that here we used the assumption that $(Y,M)$ and $\\boldsymbol{\\mathcal{A}}$ are independent for each choice of $X$ , i.e., the value noise that is introduced in $\\boldsymbol{\\mathcal{A}}$ for each $X=x$ is generated independent of the value of $Y$ and $M$ , which is the true assumption, since the algorithm only has access to $X$ and not true label or the human label. ", "page_idx": 20}, {"type": "text", "text": "\u00b7 Demographic Parity: We know that the demographic parity constraint in Section 3 can be writtenas ", "page_idx": 20}, {"type": "equation", "text": "$$\n-\\delta\\leq\\operatorname*{Pr}({\\hat{Y}}=1|A=0)-\\operatorname*{Pr}({\\hat{Y}}=1|A=1)\\leq\\delta.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Here, we find the corresponding embedding function $\\psi(x)$ for the upper-bound in the above inequality. For the lower-bound, we can use $-\\psi(x)$ and follow the steps that are proposed in the main text of the manuscript. ", "page_idx": 20}, {"type": "text", "text": "To find the embedding function that corresponds to the upper-bound of (25), we first rewrite $\\operatorname*{Pr}({\\hat{Y}}=1|A=0)-\\operatorname*{Pr}({\\hat{Y}}=1|A=1)$ as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}(\\hat{Y}=1|A=0)-\\operatorname*{Pr}(\\hat{Y}=1|A=1)=\\frac{\\operatorname*{Pr}(\\hat{Y}=1,A=0)}{\\operatorname*{Pr}(A=0)}-\\frac{\\operatorname*{Pr}(\\hat{Y}=1,A=1)}{\\operatorname*{Pr}(A=1)}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now, similar to what we did in previous section, we condition $\\operatorname*{Pr}(\\hat{Y}\\,=\\,1,A\\,=\\,a)$ for $a\\in\\{0,1\\}$ on the value of $h(x)$ and $r(x)$ , and we conclude ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\operatorname*{Pr}(\\hat{Y}=1,A=a)=\\operatorname*{Pr}(\\hat{Y}=1,A=a,r(X)=1)+\\operatorname*{Pr}(\\hat{Y}=1,A=a,r(X)=0)}}\\\\ &{}&{=\\operatorname*{Pr}(M=1,A=a,r(X)=1)+\\operatorname*{Pr}(h(X)=1,A=a,r(X)=0)}\\\\ &{}&{=\\mathbb{E}_{X,A,M,A}\\big[\\mathbb{I}_{M=1}\\mathbb{I}_{A=a}\\mathbb{I}_{r(X)=1}+\\mathbb{I}_{h(X)=1}\\mathbb{I}_{A=a}\\mathbb{I}_{r(X)=0}\\big]}\\\\ &{}&{=\\mathbb{E}_{X,A}\\big[\\operatorname*{Pr}(M=1,A=a|X=x)\\mathbb{I}_{r(X)=1}}\\\\ &{}&{\\quad+\\operatorname*{Pr}(A=a|X=x)\\mathbb{I}_{h(X)=1}\\mathbb{I}_{r(X)=0}\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Here, we used the assumption of independence of $X$ and $(M,Y)$ given a choice of $X$ As a result of (26), (27), and (17) we can find the embedding function as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\psi(x)=\\Big[0,\\frac{\\mathrm{Pr}(A=1|X=x)}{\\mathrm{Pr}(A=1)}-\\frac{\\mathrm{Pr}(A=0|X=x)}{\\mathrm{Pr}(A=0)},}\\\\ &{\\quad\\quad\\quad\\frac{\\mathrm{Pr}(M=1,A=1|X=x)}{\\mathrm{Pr}(A=1)}-\\frac{\\mathrm{Pr}(M=1,A=0|X=x)}{\\mathrm{Pr}(A=0)}\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "$\\bf(I n-)$ Equality of Opportunity: Similar to the previous items, we rewrite equality of opportunity constraint in Section 3 as ", "page_idx": 21}, {"type": "equation", "text": "$$\n-\\delta\\leq\\operatorname*{Pr}({\\hat{Y}}=1|Y=1,A=1)-\\operatorname*{Pr}({\\hat{Y}}=1|Y=1,A=0)\\leq\\delta.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Again, we only consider the upper-bound and rewrite $\\operatorname*{Pr}(\\hat{Y}=1|Y=1,A=1)-\\operatorname*{Pr}(\\hat{Y}=$ $1|Y=1,A=0)$ as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\operatorname*{Pr}(\\hat{Y}=1|Y=1,A=1)-\\operatorname*{Pr}(\\hat{Y}=1|Y=1,A=0)}\\\\ {\\quad\\quad\\quad\\quad=\\frac{\\operatorname*{Pr}(\\hat{Y}=1,Y=1,A=1)}{\\operatorname*{Pr}(Y=1,A=1)}-\\frac{\\operatorname*{Pr}(\\hat{Y}=1,Y=1,A=0)}{\\operatorname*{Pr}(Y=1,A=0)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Next, by conditioning on $r(X)=1$ and $r(X)=0$ , we rewrite $\\mathrm{Pr}(\\hat{Y}=1,Y=1,A=a)$ for $a\\in\\{0,1\\}$ as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\operatorname*{Pr}(\\hat{Y}=1,Y=1,A=a)=\\operatorname*{Pr}(\\hat{Y}=1,Y=1,A=a,r(X)=1)}\\\\ {\\quad}&{+\\operatorname*{Pr}(\\hat{Y}=1,Y=1,A=a,r(X)=0)}\\\\ {\\quad}&{=\\operatorname*{Pr}(M=1,Y=1,A=a,r(X)=1)}\\\\ {\\quad}&{\\quad+\\operatorname*{Pr}(h(X)=1,Y=1,A=a,r(X)=0)}\\\\ {\\quad}&{=\\mathbb{E}_{X,Y,M,A,A}\\Big[\\mathbb{I}_{M=1}\\mathbb{I}_{Y=1}\\mathbb{I}_{A=a}\\mathbb{I}_{r(X)=1}}\\\\ {\\quad}&{\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\mathbb{I}_{h(X)=1}\\mathbb{I}_{Y=1}\\mathbb{I}_{A=a}\\mathbb{I}_{r(X)=0}\\Big]}\\\\ {\\quad}&{=\\mathbb{E}_{X,A}\\Big[\\mathbb{I}_{r(X)=1}\\operatorname*{Pr}(M=1,Y=1,A=a|X=x)}\\\\ {\\quad}&{\\quad\\quad\\quad\\quad+\\mathbb{I}_{h(X)=1}\\mathbb{I}_{r(X)=0}\\operatorname*{Pr}(Y=1,A=a|X=x)\\Big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the last identity is followed by the assumption of independence of $\\boldsymbol{\\mathcal{A}}$ and $(Y,M,A)$ given an instance $X=x$ ", "page_idx": 21}, {"type": "text", "text": "As a result of (28), (29), and (17) we can obtain the embedding function as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\psi(x)=\\Big[0,\\displaystyle\\frac{\\mathrm{Pr}(Y=1,A=1|X=x)}{\\mathrm{Pr}(Y=1,A=1)}-\\frac{\\mathrm{Pr}(Y=1,A=0|X=x)}{\\mathrm{Pr}(Y=1,A=0)}}\\\\ &{\\quad\\quad\\quad\\frac{\\mathrm{Pr}(M=1,Y=1,A=1|X=x)}{\\mathrm{Pr}(Y=1,A=1)}-\\frac{\\mathrm{Pr}(M=1,Y=1,A=0|X=x)}{\\mathrm{Pr}(Y=1,A=0)}\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "$\\mathbf{(In-)}$ Equality of Odds: This induces the same constraint as that of equality of opportunity, and further induces an extra constraint that is in nature similar to equality of opportunity with the difference that it uses $Y=0$ instead of $Y=1$ . Therefore, we have two embedding functions, one is similar to that of equality of opportunity as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\psi_{1}(\\boldsymbol{x})=\\Big[0,\\frac{\\operatorname*{Pr}(Y=1,A=1|X=\\boldsymbol{x})}{\\operatorname*{Pr}(Y=1,A=1)}-\\frac{\\operatorname*{Pr}(Y=1,A=0|X=\\boldsymbol{x})}{\\operatorname*{Pr}(Y=1,A=0)}}\\\\ {\\displaystyle\\frac{\\operatorname*{Pr}(M=1,Y=1,A=1|X=\\boldsymbol{x})}{\\operatorname*{Pr}(Y=1,A=1)}-\\frac{\\operatorname*{Pr}(M=1,Y=1,A=0|X=\\boldsymbol{x})}{\\operatorname*{Pr}(Y=1,A=0)}\\Big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and another similar to that with changing $Y=1$ into $Y=0$ , and therefore as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\psi_{2}(\\boldsymbol{x})=\\Big[\\frac{\\mathrm{Pr}(Y=0,A=1|X=x)}{\\mathrm{Pr}(Y=0,A=1)}-\\frac{\\mathrm{Pr}(Y=0,A=0|X=x)}{\\mathrm{Pr}(Y=0,A=0)},0}\\\\ {\\displaystyle\\frac{\\mathrm{Pr}(M=1,Y=0,A=1|X=x)}{\\mathrm{Pr}(Y=0,A=1)}-\\frac{\\mathrm{Pr}(M=1,Y=0,A=0|X=x)}{\\mathrm{Pr}(Y=0,A=0)}\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "E  Limitations of Cost-Sentitive Methods ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "A variety of works have tackled constrained classification problems using cost-sensitive modeling [42, 17, 57]. In other words, they use the expected loss that is penalized with the constraints and solve that for certain coefficients for those constraints (a.k.a., they form Lagrangian from that problem). In the next step, they optimize the coefficients and obtain the optimal predictor. The issue that we discuss further in the following we concern is that during this process, the optimal predictor is achieved only when the corresponding cost-sensitive Lagrangian has a single saddle point in terms of coefficients and predictors. Such assumption, unless by analyzing the Lagrangian closely, is hard to be validated. However, our results in this paper have no such assumption, and instead use statistical hypothesis testing methods to show their optimality. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "To further clarify the issue with such methodology, we bring an example of L2D problem when human intervention budget is controlled. Suppose that the features in $\\mathcal{X}$ are distributed with an atomless probability measure $\\mu_{X}$ (e.g., normal or uniform distribution).6 Further, assume that the human has perfect information of the label, i.e. $Y=M$ , while the input features have no information of the label, i.e., $\\operatorname*{Pr}(Y=1|X=x)=1/2$ for all $x\\in\\mathscr{X}$ . Moreover, let the classifier and the human induce the $0-1$ loss function. In this case, we can see that the optimal classifier is the maximizer of the scores (see the early discussion of Section $\\mathrm{H}$ ), which in this case, since there is no clear maximizer, without loss of generality can be set to $h(x)\\equiv1$ ", "page_idx": 22}, {"type": "text", "text": "For such assumptions, if we write the Lagrangian in form of ", "page_idx": 22}, {"type": "equation", "text": "$$\nL(\\lambda,r)=L_{\\mathrm{def}}^{\\mu}(h,r)+\\lambda(\\mathbb{E}[r(X)]-b)=\\frac{1}{2}-\\frac{1}{2}\\mathbb{E}\\big[r(X)\\big]+\\lambda(\\mathbb{E}[r(X)]-b),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "then strong duality shows that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{r\\in[0,1]}\\operatorname*{max}_{\\lambda\\geq0}L(\\lambda,r)=\\operatorname*{max}_{\\lambda\\geq0}\\operatorname*{min}_{r\\in[0,1]^{\\chi}}L(\\lambda,r),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "or to put it informally, the objective is invariant under the interchange of minimum and maximum over Lagrange multipliers and the variable of interest. However, this does not prove the interchangeability ofth sadd pnts  tenwcat uaa $\\begin{array}{r}{\\mathrm{argmin}_{r\\in[0,1]}\\,L(\\lambda^{*},r)=f^{*}}\\end{array}$ where $\\lambda^{*}\\,\\in\\,\\mathrm{argmax}_{\\lambda}\\,\\mathrm{min}_{r\\in[0,1]}\\,L(\\lambda,r)$ , and $f^{*}\\,\\in\\,\\mathrm{argmin}_{r\\in[0,1]}\\,\\mathrm{max}_{\\lambda}\\,L(\\lambda,r)$ . In fact, this guarantee holds only in particular examples, e.g., when $L(\\lambda_{r}^{*},r)$ is strictly convex [7, page 8]. ", "page_idx": 22}, {"type": "text", "text": "In fact, if we optimize $r$ for all $\\lambda$ as in RHS of (30), we can show that $r_{\\lambda}(x)={\\left\\{\\begin{array}{l l}{1}&{\\lambda<{\\frac{1}{2}}}\\\\ {0}&{{\\mathrm{otherwise}}}\\end{array}\\right.}$ Therefore, $\\lambda^{*}$ can be obtained as $\\lambda^{*}=\\operatorname*{argmax}_{\\mathbf{\\mu}_{1}\\ldots\\mathbf{\\mu}_{n}}(\\lambda-1/2)^{-}-\\lambda b$ where $(x)^{-}:=\\operatorname*{min}\\{x,0\\}$ . This can be rewritten as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\lambda^{*}=\\operatorname*{argmax}_{\\lambda\\geq0}\\left\\{\\begin{array}{c c}{{-\\frac{1}{2}-\\lambda(b-1)}}&{{0\\leq\\lambda\\leq\\frac{1}{2}}}\\\\ {{-\\lambda b}}&{{\\lambda>\\frac{1}{2}}}\\end{array}\\right.=\\frac{1}{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence, the condition $\\lambda<1/2$ is never satisfied and we have $r_{\\lambda^{*}}(x)=0$ , i.e., we should never defer. For the deferral rule $r_{\\lambda^{*}}$ , the deferral loss (1) is ", "page_idx": 22}, {"type": "equation", "text": "$$\nL_{\\operatorname*{def}}^{\\mu}(h,\\hat{f})=\\mathbb{E}_{X,Y,M}\\big[\\ell_{A I}(Y,h(X),X)\\big]=\\frac{1}{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "To showthat $r_{\\lambda^{*}}$ is not optimal, we provide random and deterministic deferral rules $f^{*}$ and $r^{**}$ that satisfy the constraint in (2), while having a smaller deferral loss: ", "page_idx": 22}, {"type": "text", "text": "$\\diamond$ Let $f^{*}(x)=b$ , that is a random deferral rule that defers with probability $b$ everywhere on $\\mathcal{X}$ .Therefore, on average $b$ proportion of inquiries are deferred and thus it satisfies the constraint in (2). The deferral loss for $f^{*}(x)$ is equal to ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{\\operatorname*{def}}^{\\mu}(h,f^{*})=\\underbrace{\\mathbb{E}[r(X)]}_{b}\\cdot\\underbrace{\\mathbb{E}[\\ell_{H}(Y,M)]}_{0}}\\\\ &{\\qquad\\qquad\\qquad+\\underbrace{\\mathbb{E}[1-r(X)]}_{1-b}\\cdot\\underbrace{\\mathbb{E}[\\ell_{A I}(Y,h(X))]}_{\\frac{1}{2}}}\\\\ &{\\qquad\\qquad=\\frac{1-b}{2}<\\frac{1}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "$\\diamond$ The second example is a deterministic deferral rule. Since the probability measure on $\\mathcal{X}$ is atomless, for all $b\\in[0,1]$ there exists a set $\\boldsymbol{\\mathcal{A}}$ such that $\\operatorname*{Pr}(X\\in{\\mathcal{A}})=b$ [31, Proposition 215D]. Hence, defining $\\bar{r}^{**}(x)\\,=\\,\\mathbb{1}_{x\\in\\mathcal{A}}$ the constraint in (2) is met. Similar to the last example $\\begin{array}{r}{L_{\\mathrm{def}}^{\\mu}(h,r^{**})=\\frac{1-b}{2}<\\frac{1}{2}}\\end{array}$ ", "page_idx": 22}, {"type": "text", "text": "The above two examples show that the deferral rule $r_{\\lambda^{*}}$ is sub-optimal. The reason is that, for optimality of $r_{\\lambda^{*}}$ we should make sure that $L(\\lambda_{r}^{*},r)$ has a single minimizer of $r$ . However, in our example we had $L(\\textstyle{\\frac{1}{2}},r)=-\\lambda b$ has infinite number of minimizers in terms of $r(x)$ . Therefore, the equality of the solutions to minimax problem and maximin problem is not guaranteed. ", "page_idx": 22}, {"type": "text", "text": "Algorithm 1 Finding Optimal Classifier and Rejection Function   \nRequire: The formulation of $\\ell_{\\mathrm{def}}\\left(\\cdot,\\cdot,\\cdot\\right)$ and $\\{\\Psi_{i}(\\cdot,\\cdot,\\cdot)\\}_{i=1}^{m}$ , and the datasets $\\begin{array}{r l}{\\mathcal{D}_{\\mathrm{train}}}&{{}=}\\end{array}$   \n$\\{(x^{i},a^{i},m^{i},y^{i})\\}_{i=1}^{n_{\\mathrm{train}}}$ $\\mathcal{D}_{\\mathrm{val}}=\\{(x^{i},a^{i},m^{i},y^{i})\\}_{i=n_{\\mathrm{train}}+1}^{n_{\\mathrm{train}}+n_{\\mathrm{val}}}$ $\\{\\delta_{i}\\}_{i=1}^{m}$   \n$r^{*}(x)$ $h^{*}(x)$   \n1: Parameters: $\\epsilon=1e-8$   \n2: procedure CONSTRAINEDDEFER( $\\boldsymbol{\\ell}_{\\mathrm{def}}$ \uff0c $\\{\\Psi_{i}\\}_{i=1}^{m}$ \uff0c $\\mathcal{D}_{\\mathrm{train}}$ \uff0c $\\mathcal{D}_{\\mathrm{val}})$   \n3: Obtain closed-form of $\\{\\psi_{i}(\\boldsymbol{x})\\}_{i=0}^{m}$ using $\\ell_{\\mathrm{def}}$ and $\\Psi_{i}\\mathbf{s}$ via (4) and in terms of the scores as in   \nTable 1   \n4: Estimate the scores in Table 1 using classification/regression methods on $\\mathcal{D}_{\\mathrm{train}}$   \n5: Find estimate $\\{\\hat{\\psi}_{i}\\}_{i=0}^{m}$ using estimated scores in previous step and closed-form of Step 3   \n6: if $m=2$ then   \n7: Define routine $\\hat{f}_{k,p}(x):=\\tau\\big(\\hat{\\psi}_{0}(x)-k\\hat{\\psi}_{1}(x),x\\big)$ for $\\tau$ in Theorem 4.2   \n8: Define routine $\\hat{C}(t):=\\widehat{\\mathbb{E}}_{{\\mathcal{D}}_{\\mathrm{val}}}\\Big[\\langle\\hat{f}_{k,0}(x_{i}),\\hat{\\psi}_{1}(x_{i})\\rangle\\Big]$   \n9: Find $\\hat{k}=\\operatorname*{min}k$ over the feasibility set $\\hat{C}(t)\\leq\\delta_{1}$   \n10: $\\hat{k}=\\emptyset$ then   \n11: Return \u2018Not Feasible'   \n12: else   \n13: if $\\hat{C}(\\hat{k}-\\epsilon)-\\hat{C}(k^{*})\\leq1e-3$ then   \n14: $\\hat{p}\\gets0$   \n15: else   \n16: $\\begin{array}{r}{\\hat{p}\\leftarrow\\frac{\\delta-\\hat{C}(\\hat{k})}{\\hat{C}(\\hat{k}-\\epsilon)-\\hat{C}(\\hat{k})}}\\end{array}$   \n17: end if   \n18: end if   \n19: $s(x):=\\hat{f}_{\\hat{k},\\hat{p}}(x)$   \n20: else   \n21: Optimize (3) for $\\mathcal{D}_{\\mathrm{val}}$ and for $\\begin{array}{r}{f(x)=\\tau(\\hat{\\psi}_{0}(x)-\\sum_{i=1}^{m}\\hat{\\psi}_{i}(x),x)}\\end{array}$ for $\\tau$ as in Theorem   \n4.1 and via exhaustive search over $\\{k_{1},\\dots,k_{m}\\}$ and randomizations of $\\tau$ and find $s(x):={\\hat{f}}(x)$   \n22: end if   \n23: $\\begin{array}{r l}&{h^{*}(x):=\\underset{i\\in[0:L-1]}{\\mathrm{argmax}}\\;s_{i}(x)}\\\\ &{r^{*}(x):=\\underset{i\\in\\{0,1\\}}{\\mathrm{argmax}}\\left[s_{h^{*}(x)}(x),s_{L}(x)\\right]}\\\\ &{\\mathbf{Return}\\;h^{*}(x),r^{*}(x)}\\end{array}$   \n24:   \n25:   \n26: end procedure ", "page_idx": 23}, {"type": "text", "text": "G  On Failure of In-Processing Methods ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "One might think that the need of using post-processing methods does not necessarily appear in some examples of multi-objective L2D problem. As an instance, for the expert intervention budget we can rank samples based on the difference between machine and human loss and defer the top $b^{\\phantom{\\dagger}}$ proportion of samples for which the machine loss is higher than the human one. This method is illustrated in Algorithm 2. Indeed, in the following we show that the sub-optimality of such deterministic deferral rule, compared to the optimal deferral rule diminishes as the size of training set increases. ", "page_idx": 23}, {"type": "text", "text": "Theorem G.1 (Optimal Deferral for Empirical Distribution). For a classifier $h(x)$ and dataset ${\\cal D}\\;=\\;\\{(x_{i},y_{i},m_{i})\\}_{i=1}^{n}$ , where we assume $x_{i}\\ \\neq\\ x_{j}.$ $i\\ne j$ thedeterministic deferral ruleas in Algorithm 2 is $(i)$ the optimal deterministic deferralrulefor the empirical distribution on $\\mathcal{D}$ and bounded expert intervention budget, and(i) at most $\\frac{1}{n}$ -suboptimal(intermsof deferral loss) compared to the optimal random deferral rule for theempirical distribution on $\\mathcal{D}$ ", "page_idx": 23}, {"type": "text", "text": "Next in the following, we show that such policy does not provide sufficient information to determine the optimal deferral rule for the true distribution. To that end, we first recall that in classification tasks, the optimal classifier typically thresholds an estimation of conditional probability of the label $Y$ given $X$ that is obtained using the available dataset. As a result, if we observe enough pairs of $\\left({x_{i},y_{i}}\\right)$ ,then ", "page_idx": 23}, {"type": "text", "text": "Algorithm 2 Deterministic Algorithm for Deferring Tasks to Human or AI for the Empirical Distribution and Expert Intervention Budget ", "page_idx": 24}, {"type": "text", "text": "Input: The dataset $\\mathcal{D}$ , he human and classifier loss $\\ell_{H}$ and $\\ell_{A I}$ and available proportion $b$ of instances to defer Output: Labels of \"defer\" or \"'no defer\" for each instance in $\\mathcal{D}$ 1: procedure DEFERTASKS $(\\mathcal{D},\\ell_{H},\\ell_{A I},b)$ 2: Make the set $A=\\{(x,y,m)\\in{\\mathcal{D}}:\\ell_{H}(y,m)-\\ell_{A I}(y,h(x))\\leq0\\}$ 3: if $|A|\\geq b|{\\mathcal{D}}|$ then 4: Defer all tasks in $A$ to human 5: else 6: Defer the $b|\\mathcal{D}|$ tasks with the lowest $\\ell_{H}(x,y,m)-\\ell_{A I}(x,y)$ to human 7:end if 8: end procedure ", "page_idx": 24}, {"type": "text", "text": "we improve upon such estimation of conditional probability and increase the accuracy of the obtained classifier. However, we argue that this paradigm is inapplicable in the case of deferral rule as follows. Although the output $\\hat{r}$ of Algorithm 2 for each feature $x$ is a deterministic 0 or 1 label, it varies with the choice of the dataset $\\mathcal{D}$ . Hence, if we draw datasets from a distribution $\\mu$ , the outcome of $\\hat{r}$ becomes probabilistic. In the following, we introduce two probability distributions $\\mu_{1}$ and $\\mu_{2}$ over $(X,Y,M)$ such that for random draws of the dataset from $\\mu_{i}$ , the conditional probability of such optimal deferral label $\\hat{r}$ given $X$ is equal, yet the optimal deferral rule for the true distribution is different. ", "page_idx": 24}, {"type": "text", "text": "Although the following discussion bears some resemblance with the No-Free-Lunch theorem [e.g. 66], there exists the following difference between the two. The No-Free-Lunch theorem states that for each learning algorithm, there exists a data distribution on which the algorithm does not generalize well. However, in the following discussion, we assume that we can observe infinite number of datasets and indeed, we can find the underlying probability of the deferral labels. In fact, we show that the limiting factor for finding the optimal deferral for the true distribution is that we only use deferral labels and if we use values of both losses, we can accordingly find the optimal deferral rule as suggested in Theorem 4.1. ", "page_idx": 24}, {"type": "text", "text": "Assume that we have a dataset $\\textstyle D=\\{(x_{i},y_{i},m_{i})\\}_{i=1}^{n}$ that contains i.i.d. samples from the distribution $\\mu_{X Y M}$ . Further, assume that we have no budget constraint, that is $b=1$ in Algorithm 2. Therefore, the optimal randomized deferral rule over the empirical distribution is the solution of the problem ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\hat{r}_{i}\\in[0,1]}\\sum_{i=1}^{n}\\mathbb{1}_{m_{i}\\neq y_{i}}\\hat{r}_{i}+\\mathbb{1}_{h(x_{i})\\neq y_{i}}\\big(1-\\hat{r}_{i}\\big).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Itis easyto see that thesoltiontothisproblmisgivenby $\\hat{r}_{i}=0$ $\\mathbb{1}_{h(x_{i})\\neq y_{i}}<\\mathbb{1}_{m_{i}\\neq y_{i}}$ and $\\hat{r}_{i}=1$ $\\mathbb{1}_{h(x_{i})\\neq y_{i}}>\\mathbb{1}_{m_{i}\\neq y_{i}}$ . As a result, the optimal deferral is obtained as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\hat{r}_{i}=\\left\\{\\begin{array}{c c}{1}&{m_{i}=y_{i}\\,,\\,h(x_{i})\\neq y_{i}}\\\\ {0}&{m_{i}\\neq y_{i},\\,h(x_{i})=y_{i}}\\\\ {\\mathrm{~any~value~in~}[0,1]}&{o.w.}\\end{array}\\right.\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Among all the possible policies, we can choose ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\hat{r}_{i}=\\left\\{\\begin{array}{c c}{1}&{m_{i}=y_{i}\\,\\&\\,h(x_{i})\\neq y_{i}}\\\\ {0}&{o.w.}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Next, we assume that we have a classifier $h$ and two probability distributions $\\mu_{1}$ and $\\mu_{2}$ over $(X,Y,M)$ : For both distributions $X$ is   uniformly   distributed  over $[0,1]$ , and we have $\\begin{array}{r l r}{\\mu_{1}(Y=M,h(X)=Y)=\\frac{2}{3},\\mu_{1}(Y}&{{}={}}&{M,h(X){}^{\\prime}\\quad\\neq\\quad Y)\\!=\\!\\frac{1}{3}}\\end{array}$ and $\\mu_{2}(Y\\ne M,h(X)=Y)=\\textstyle{\\frac{2}{3}},\\mu_{2}(Y\\,=\\,M,h(X)\\,\\ne\\,Y)\\!=\\frac{1}{3}$ We can see that although the observed $\\scriptstyle{\\hat{r}}\\mathbf{s}$ are fixed for a given choice of $\\mathcal{D}$ , since $\\mathcal{D}$ is randomly drawn, $\\hat{r}$ values are randomly distributed. Furthermore, the distribution of $\\mathrm{Pr}({\\hat{r}}|X)$ is according to $B e r n({\\textstyle{\\frac{1}{3}}})$ , since in both cases we have $\\begin{array}{r}{\\mu_{i}(Y=M,h(X)\\neq Y)=\\frac{1}{3}}\\end{array}$ . However, the optimal deferral rule for the first distribution .s $r_{1}^{*}(x)=1$ for all $x\\in\\mathscr{X}$ , since we have $L_{\\mathrm{def}}^{\\mu_{1}}(h,r_{1}^{*})=0$ while for the second case the optimal deferral rule is $r_{2}^{*}(x)=0$ for all $x\\in\\mathscr{X}$ because we have $\\begin{array}{r}{L_{\\mathrm{def}}^{\\mu_{2}}(h,r_{2}^{*})=\\frac{1}{3}}\\end{array}$ . Furthermore, such deferral rules are not interchangeable, because we have $L_{\\mathrm{def}}^{\\mu_{1}}(h,r_{2}^{*})=L_{\\mathrm{def}}^{\\mu_{2}}(h,r_{1}^{*})=\\frac23$ Asaresut, $\\mathrm{Pr}({\\hat{r}}|X)$ does not provide sufficient information for obtaining optimal deferral rule on true distribution. For an arbitrary choice of deterministic deferral rule for empirical distribution, we state the following proposition as a proof of insuffciency of deferral labels for obtaining optimal deferral rule over the true distribution. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "Proposition G.2 (Impossibility of generalization of deferral labels). For every deterministic deferral rule $\\hat{r}$ for empirical distributions and based on the two losses $\\mathbb{1}_{m\\not=y}$ and $\\mathbb{1}_{h(x)\\neq y},$ there exist two probabilitymeasures $\\mu_{1}$ and $\\mu_{2}$ on $\\mathcal{X}\\times\\mathcal{Y}\\times\\mathcal{M}$ such that the corresponding $(\\hat{r},X)$ for both measuresis distriutd equalyowever te optal deferal $r_{\\mu_{1}}^{\\ast}$ and for these measures are not interchangeable, that is $L_{\\mathrm{def}}^{\\mu_{i}}(h,r_{\\mu_{i}}^{*})\\leq\\frac{1}{3}$ while $\\begin{array}{r}{L_{\\mathrm{def}}^{\\mu_{i}}(h,r_{\\mu_{j}}^{*})=\\frac{2}{3}}\\end{array}$ for $i=1,2$ and $j\\neq i$ ", "page_idx": 25}, {"type": "text", "text": "Proof. As mentioned in (31), there are four possibilities of a deterministic deferral rule for empirical distribution based on the events $h(X)\\neq Y$ and $M\\ne Y$ .The reason is that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\hat{r}=\\left\\{\\begin{array}{l l}{1}&{h(x)\\neq y\\,,\\,m=y}\\\\ {0}&{h(x)=y\\,,\\,m\\neq y}\\\\ {a}&{h(x)\\neq y\\,,\\,m\\neq y}\\\\ {b}&{h(x)=y\\,,\\,m=y}\\end{array}\\right.\\,,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "the parameters $a$ and $b$ can take binary values. One of the cases in which $a=b=0$ is analyzed previously in this section. We study the other cases as follows: ", "page_idx": 25}, {"type": "text", "text": "1. $\\mathbf{a}=\\mathbf{1},\\mathbf{b}=\\mathbf{0}$ : In this case we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\hat{r}=\\left\\{\\begin{array}{c c}{1}&{h(x)\\neq y}\\\\ {0}&{o.w.}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "If we define a measure $\\mu_{1}$ such that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mu_{1}{\\big(}h(X)\\neq Y,M=Y{\\big)}={\\frac{1}{3}},\\,\\,\\,\\mu_{1}{\\big(}h(X)=Y,M\\neq Y{\\big)}={\\frac{2}{3}},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and a measure $\\mu_{2}$ such that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mu_{2}{\\big(}h(X)\\neq Y,M=Y{\\big)}={\\frac{1}{3}},\\,\\,\\,\\mu_{2}{\\big(}h(X)=Y,M=Y{\\big)}={\\frac{2}{3}},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "then on one hand one can see that $\\hat{r}$ is according to $B e r n({\\textstyle{\\frac{1}{3}}})$ in both cases. On the other hand, because the probability of classifier accuracy is larger than human accuracy in $\\mu_{1}$ and is smaller than human accuracy in $\\mu_{2}$ , we have $r_{\\mu_{1}}^{*}(x)=0$ while $r_{\\mu_{2}}^{*}(x)=1$ . Therefore, we conclude that ", "page_idx": 25}, {"type": "equation", "text": "$$\nL_{\\mathrm{def}}^{\\mu_{1}}(r_{\\mu_{1}}^{*},h)=\\frac{1}{3},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and ", "page_idx": 25}, {"type": "equation", "text": "$$\nL_{\\mathrm{def}}^{\\mu_{2}}(r_{\\mu_{2}}^{*},h)=0,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "while the losses with interchanging deferral policies are equal to ", "page_idx": 25}, {"type": "equation", "text": "$$\nL_{\\mathrm{def}}^{\\mu_{1}}(r_{\\mu_{2}}^{*},h)=L_{\\mathrm{def}}^{\\mu_{2}}(r_{\\mu_{1}}^{*},h)=\\frac{2}{3}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "2. $\\mathbf{a}=\\mathbf{0},\\mathbf{b}=\\mathbf{1}$ : In this case, the deferral rule is as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\hat{r}=\\left\\{\\begin{array}{c c}{0}&{m\\neq y}\\\\ {1}&{o.w.}\\end{array}\\right.\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Next, if we set two probability measures $\\mu_{1}$ and $\\mu_{2}$ such that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mu_{1}{\\big(}M\\neq Y,h(X)=Y{\\big)}={\\frac{1}{3}},\\,\\,\\,\\mu_{1}{\\big(}M=Y,h(X)\\neq Y{\\big)}={\\frac{2}{3}},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mu_{2}{\\big(}M\\neq Y,h(X)=Y{\\big)}={\\frac{1}{3}},\\;\\;\\mu_{2}{\\big(}M=Y,h(X)=Y{\\big)}={\\frac{2}{3}},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "then $\\hat{r}$ is according to $B e r n({\\frac{2}{3}})$ in both cases. However, $r_{\\mu_{1}}^{*}=1$ While $r_{\\mu_{2}}^{*}=0$ Furthermore, the expected deferral losses are equal to ", "page_idx": 26}, {"type": "equation", "text": "$$\nL_{\\mathrm{def}}^{\\mu_{1}}(r_{\\mu_{1}}^{*},h)=\\frac{1}{3},\\,\\,\\,L_{\\mathrm{def}}^{\\mu_{2}}(r_{\\mu_{2}}^{*},h)=0,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "while after interchanging the deferral policies we have ", "page_idx": 26}, {"type": "equation", "text": "$$\nL_{\\mathrm{def}}^{\\mu_{1}}(r_{\\mu_{2}}^{*},h)=L_{\\mathrm{def}}^{\\mu_{2}}(r_{\\mu_{1}}^{*},h)=\\frac{2}{3}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "3. $\\mathbf{a}=\\mathbf{1},\\mathbf{b}=\\mathbf{1}$ : In this case, the deferral rule is as ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\hat{r}=\\left\\{\\begin{array}{c c}{0}&{h(x)=y,m\\neq y}\\\\ {1}&{o.w.}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Next, if we set two probability measures $\\mu_{1}$ and $\\mu_{2}$ such that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mu_{1}{\\big(}M\\neq Y,h(X)=Y{\\big)}={\\frac{1}{3}},\\,\\,\\,\\mu_{1}{\\big(}M=Y,h(X)\\neq Y{\\big)}={\\frac{2}{3}},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mu_{2}{\\big(}M\\neq Y,h(X)=Y{\\big)}=\\mu_{2}{\\big(}M\\neq Y,h(X)\\neq Y{\\big)}=\\mu_{2}{\\big(}M=Y,h(X)=Y{\\big)}={\\frac{1}{3}},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "then we can see that $\\hat{r}$ has the distribution $B e r n({\\frac{2}{3}})$ . However, one can find the optimal deferal plicies forth rue distribtions are $r_{\\mu_{1}}^{*}=1$ and $r_{\\mu_{2}}^{*}=0$ Furthermore, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\nL_{\\mathrm{def}}^{\\mu_{1}}(r_{\\mu_{1}}^{*},h)=\\frac{1}{3},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and ", "page_idx": 26}, {"type": "equation", "text": "$$\nL_{\\mathrm{def}}^{\\mu_{2}}(r_{\\mu_{2}}^{*},h)=\\frac{2}{3},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "while ", "page_idx": 26}, {"type": "equation", "text": "$$\nL_{\\mathrm{def}}^{\\mu_{1}}(r_{\\mu_{1}}^{*},h)=\\frac{1}{3},\\,\\ L_{\\mathrm{def}}^{\\mu_{2}}(r_{\\mu_{2}}^{*},h)=\\frac{1}{3}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "HProof of Theorem 3.1 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Let ${\\mathcal{X}}=\\{x_{1},\\ldots,x_{n}\\}$ and $\\ y=\\{1,\\ldots,n\\}$ .We first show that obtaining the optimal classifier isof $O(n)$ complexity, since in this case is equivalent to obtaining the Bayes optimal classifier in isolation. The reason is that, the unconstrained Bayes optimal classifier is a deterministic classifier that minimizes ", "page_idx": 26}, {"type": "equation", "text": "$$\nh^{*}(x)\\in\\underset{\\hat{\\mathcal{Y}}}{\\mathrm{argmin}}\\,\\mathbb{E}_{\\mu_{Y|X}}\\big[\\ell_{A I}(Y,\\hat{y},X)|X=x\\big],\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for all $x\\in\\mathscr{X}$ . This is regardless of whether the deferral occurs or not. Therefore, this solution is further the solution to ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h^{*}(x)\\in\\underset{\\hat{y}}{\\mathrm{argmin}}\\,\\mathbb{E}_{\\mu_{Y\\mid x}}\\big[(1-r(X))\\ell_{A I}(Y,\\hat{y},X)\\lvert X=x\\big]}\\\\ &{\\qquad=\\underset{\\hat{y}}{\\mathrm{argmin}}\\,\\mathbb{E}_{\\mu_{Y,M\\mid X}}\\big[(1-r(X))\\ell_{A I}(Y,\\hat{y},X)+r(X)\\ell_{H}(Y,M,X)\\lvert X=x\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for every rejection function $r$ , including the optimal rejection function of the constrained optimization problem. In the particular case of expert intervention budget, the constraint is further independent of $h$ and is only a function of $r$ . Therefore, the unconstrained Bayes classifier is an optimal classifier for the constrained L2D problem with human intervention budget. ", "page_idx": 26}, {"type": "text", "text": "Next, we consider a specific case in which $\\begin{array}{r l r l r}{\\breve{\\mathbb{E}}_{\\mu_{Y|X}}\\left[\\ell_{A I}(Y,1,X)|X\\right.}&{{}=}&{{}\\left.x\\right]}&{{}>}&{}\\end{array}$ $\\mathbb{E}_{\\mu_{Y|X}}\\left[\\ell_{A I}(Y,0,X)|X~=~x\\right]$ for all $x\\ \\in\\ \\mathcal{X}$ , and therefore $h(x)~=~1$ over all input space. ", "page_idx": 26}, {"type": "text", "text": "Further, we assume the data distribution has the property $\\mu_{X Y M}=\\mu_{X Y}\\delta(M=Y)$ ,i.e. $M=Y$ on all the data. In this case, we know that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[\\ell_{H}(Y,M,X)|X=x_{i}\\big]=\\mathbb{E}\\big[\\mathbb{1}_{M\\neq Y}|X=x_{i}\\big]=0,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and we define ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[\\ell_{A I}(Y,h(X),X)|X=x_{i}\\big]=\\mathbb{E}\\big[\\mathbb{1}_{Y\\neq1}|X=x_{i}\\big]=\\ell_{i}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Now, if we set $\\operatorname*{Pr}(X=x_{i})=p_{i}$ , and $r(x_{i})=r_{i}$ , then the optimization problem ", "page_idx": 27}, {"type": "equation", "text": "$$\nf^{*}=\\operatorname*{argmin}_{r(\\cdot)\\in\\{0,1\\}^{x}}L_{\\mathrm{def}}^{\\mu}(h,r),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "is equivalent to ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{argmin}_{r_{i}\\in\\{0,1\\}}\\sum_{i=1}^{n}p_{i}\\times0\\times r_{i}+p_{i}\\times(1-r_{i})\\times\\ell_{i},\\quad s.t.\\ \\sum_{i=1}^{n}p_{i}r_{i}\\leq b,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "that is equivalent to ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname{argmax}\\sum_{i\\in\\{0,1\\}}^{n}p_{i}r_{i}\\ell_{i},\\quad s.t.\\ \\sum_{i=1}^{n}p_{i}r_{i}\\leq b.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Next, we show that the above problem spans all instances of the $0-1$ knapsack problem, which is known to be NP-hard (Theorem 15.8 of [58]). Let ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname{argmax}\\sum_{i\\in\\{0,1\\}}^{n}r_{i}c_{i},\\ \\ \\ s.t.\\ \\ \\sum_{i=1}^{n}w_{i}r_{i}\\leq K,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "be an instance of the $0\\mathrm{~-~}1$ knapsack problem 7 with $w_{i},c_{i}\\;>\\;0$ \uff0c $i~\\in~[n]$ , and $K\\,>\\,0$ .With $\\begin{array}{r}{\\ell_{i}=\\frac{c_{i}/w_{i}}{\\sum_{i=1}^{n}c_{i}/w_{i}}}\\end{array}$ $\\begin{array}{r}{p_{i}=\\frac{w_{i}}{\\sum_{i=1}^{n}w_{i}}}\\end{array}$ wg,problem (3) can be writen in the form of (32). $\\begin{array}{r}{\\sum_{i=1}^{n}l_{i}=\\sum_{i=1}^{n}p_{i}=1}\\end{array}$ ", "page_idx": 27}, {"type": "text", "text": "1 Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We start this proof by introducing a few useful lemmas: ", "page_idx": 27}, {"type": "text", "text": "Lemma I.1. The set ${\\mathcal{F}}={\\Delta}_{n}^{\\mathcal{X}}$ of all functions that map $\\mathcal{X}$ to an n-dimensional probability is weakly compact, i.e., for each sequence $\\{f_{n}\\}_{n=1}^{\\infty}$ , there is a sub-sequence $\\{f_{n_{i}}\\}$ and a function $f^{*}\\in\\mathcal{F}$ such that for all measurable embedding functions $\\psi$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{k\\to\\infty}\\mathbb{E}\\bigl[\\langle f_{n_{k}},\\psi\\rangle\\bigr]=\\mathbb{E}\\bigl[\\langle f^{*},\\psi\\rangle\\bigr].\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. We know that all components of each element of the function sequence is bounded by 1. We define $\\{f_{m}^{i}\\}_{m=1}^{\\infty}$ as the sequence of the $i$ th component of the function sequence. Therefore, using [42, Theorem A.5.1 we know that there is asub-squence $\\{f_{m_{k}^{1}}^{1}\\}_{k=1}^{\\infty}$ and ann-negative 1-bouned function $f_{1}^{*}$ , such that for each $\\mu$ -integrable function $\\psi_{1}(x)$ we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{k\\to\\infty}\\mathbb{E}_{\\mu}\\bigl[f_{m_{k}}^{1}(x)\\psi_{1}(x)\\bigr]=\\mathbb{E}_{\\mu}\\bigl[f_{1}^{*}(x)\\psi_{1}(x)\\bigr].\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Nextwecarete p $\\{f_{m_{k}^{1}}^{i}\\}_{k=1}^{\\infty}$ where $i\\in[2:n]$ and we can fn a sub-sequence $m_{k}^{i+1}$ f $m_{k}^{i}$ and a no-ngative $1-$ bounded function $f_{i+1}^{*}$ for which ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{k\\to\\infty}\\mathbb{E}_{\\mu}\\bigl[f_{m_{k}^{i+1}}^{i+1}(x)\\psi_{i+1}(x)\\bigr]=\\mathbb{E}_{\\mu}\\bigl[f_{i+1}^{*}(x)\\psi_{i+1}(x)\\bigr].\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Now, since all sub-sequences of a converging sequence converges to the same limit, we can use $m_{k}^{n}$ that is the intersection of all sequences and show that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{k\\to\\infty}\\mathbb{E}_{\\mu}\\bigl[f_{m_{k}^{n}}^{i}(x)\\psi_{i}(x)\\bigr]=\\mathbb{E}_{\\mu}\\bigl[f_{i}^{*}(x)\\psi_{i}(x)\\bigr],\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for all $i\\in[1:n]$ and integrable functions $\\psi_{i}$ : As a result, due to interchangeability of limit and summation, when the sum is over a finite set of elements, it is easy to show that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\underset{k\\rightarrow\\infty}{\\operatorname*{lim}}\\mathbb{E}\\big[\\langle f_{m_{k}^{n}},\\psi\\rangle\\big]=\\displaystyle\\operatorname*{lim}_{k\\rightarrow\\infty}\\mathbb{E}\\big[\\sum_{i=1}^{n}f_{m_{k}^{n}}^{n}(x)\\psi_{i}(x)\\big]=\\displaystyle\\sum_{i=1}^{n}\\underset{k\\rightarrow\\infty}{\\operatorname*{lim}}\\mathbb{E}\\big[f_{m_{k}^{n}}^{n}(x)\\psi_{i}(x)\\big]}\\\\ &{\\displaystyle=\\sum_{i=1}^{n}\\mathbb{E}_{\\mu}\\big[f_{i}^{*}(x)\\psi_{i}(x)\\big]=\\mathbb{E}_{\\mu}\\big[\\langle f^{*}(x),\\psi(x)\\rangle\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Next, we need to show that $f^{*}\\in\\mathcal{F}$ . We already know that all components of $f^{*}$ is 1-bounded and non-negative. Therefore, we only need to prove that all elements of $f^{*}$ sum up to 1 almost everywhere. If not, then assume that there is a non-zero set $A$ where $\\mu(A)>0$ and there exists $l>0$ such that $|\\sum_{i}f_{i}^{*}-1|\\geq l$ for all $x\\in A$ . We know that there is either a subset $B\\subseteq A$ with $\\mu(B)>0$ such that for all $x\\in B$ we have $\\textstyle\\sum_{i}f_{i}^{*}(x)\\geq1\\!+\\!l,$ or similarly a subset for which $\\begin{array}{r}{\\sum_{i}f_{i}^{*}(x)\\le1-l}\\end{array}$ The reason is that otherwise a non-zero measure set $A$ is a union of two zero-measure set, which is a contradiction. Without loss of generality we assume the first, which means $\\textstyle\\sum_{i}f_{i}^{*}(x)\\geq1+l$ for $x\\in B$ . Now, if we define $\\hat{\\psi}(x)=[1,\\ldots,1]$ for $x\\in B$ and otherwise $\\hat{\\psi}(x)=[0,\\ldots,0]$ , then we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mu}\\big[\\langle f^{*}(x),\\hat{\\psi}(x)\\rangle\\big]\\ge(1+l)\\mu(B),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "while ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mu}\\left[\\langle f_{m_{k}^{n}}(x),\\psi\\rangle\\right]=1,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "for all $k\\in\\mathbb{N}$ . This is a contradction, because the limit of a constant sequence is not different from that constant value. Hence, $f^{*}$ sums up to 1 almost everywhere, and that completes the proof. ", "page_idx": 28}, {"type": "text", "text": "Proof of Theorem 4.1: We prove the theorem using the following steps: (i) for the class $\\mathcal{C}$ of prediction functions for which $\\mathbb{E}\\big[\\langle f(x),\\psi_{i}(x)\\rangle\\big]=\\delta_{i}$ for $i\\in[1:m]$ , we show that the supremum of the objective function $\\mathbb{E}\\big[\\langle f(x),\\bar{\\psi_{0}}(x)\\rangle\\big]$ is a maximum, (ii) we show that it is suffcient for a predictor $f\\in\\mathcal{C}$ to be in form of (8) to achieve the maximum objective $\\mathbb{E}\\big[\\langle f(x),\\psi_{0}(x)\\rangle\\big]$ in $\\mathcal{C}$ and also for all predictors with $\\mathbb{E}\\big[\\langle f(x),\\psi_{i}(x)\\rangle\\big]\\leq\\delta_{i}$ , (i) we show that the space of all possible constraints for any prediction function in $\\Delta_{d}^{\\mathcal{X}}$ is convex and compact, and (iv) we show that if the tuple of constraints is an interior point of all possible tuples of constraints, then a point in $\\mathcal{C}$ achieves its maximum if and only if it follows the thresholding rule (8) almost everywhere. ", "page_idx": 28}, {"type": "text", "text": "\u00b7 Step (i): Due to the definition of supremum, we know that for each $\\epsilon>0$ , there exists a function $f_{\\epsilon}$ in $\\mathcal{C}$ such that $\\begin{array}{r}{\\mathbb{E}\\big[\\langle f_{\\epsilon},\\psi_{0}(\\dot{x})\\rangle\\big]\\ge\\operatorname*{sup}_{f\\in\\mathcal{C}}\\mathbb{E}\\big[\\langle f,\\psi_{0}(x)\\rangle\\big]-\\epsilon}\\end{array}$ Equivalently,there is a sequence of functions $f_{n}$ for which $\\begin{array}{r}{\\operatorname*{lim}_{n\\rightarrow\\infty}\\mathbb{E}\\big[\\langle f_{n},\\psi_{0}(x)\\rangle\\big]=\\operatorname*{sup}_{f\\in{\\mathscr C}}\\mathbb{E}\\big[\\langle f,\\psi_{0}(x)\\rangle\\big]}\\end{array}$ Using weakly-compactnessof the function clas $\\Delta_{n+1}^{\\mathcal{X}}$ as inLemmaL, weknow that for the sequence $f_{n}$ , there exists a subsequence $f_{n_{k}}$ and a function $f^{*}\\in\\Delta_{n+1}^{\\chi}$ such that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{k\\to\\infty}\\mathbb{E}\\bigl[\\langle f_{n_{k}},\\psi_{m+1}(x)\\rangle\\bigr]=\\mathbb{E}\\bigl[\\langle f^{*}(x),\\psi_{m+1}(x)\\rangle\\bigr].\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Furthermore, we know that each subsequence $a_{n_{k}}$ of a converging sequence $a_{n}$ has the same limit as the limit of the mother sequence $a_{n}$ [59, Chapter 2, Theorem 1]. Therefore, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[\\langle f^{*}(x),\\psi_{m+1}(x)\\rangle\\big]=\\operatorname*{sup}_{f\\in\\mathcal{C}}\\mathbb{E}\\big[\\langle f,\\psi_{m+1}(x)\\rangle\\big],\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which means that the supremum of the objective is achievable by $f^{*}$ ", "page_idx": 28}, {"type": "text", "text": "Moreover, for $\\psi_{i}(x)$ where $i\\,\\in\\,[1\\,:\\,m]$ , we have $\\mathbb{E}\\big[\\langle f_{n},\\psi_{i}(\\dot{x})\\rangle\\big]\\,=\\,\\delta_{i}$ for all $n$ , which concludes ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\delta_{i}=\\operatorname*{lim}_{k\\to\\infty}\\mathbb{E}\\bigl[\\langle f_{n_{k}},\\psi_{i}(x)\\rangle\\bigr]=\\mathbb{E}\\bigl[\\langle f^{*}(x),\\psi_{i}(x)\\rangle\\bigr].\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "This means that the equality constraints holds for $f^{*}$ ,i.e., $f^{*}\\in\\mathcal{C}$ , if it holds for each predictor $f_{n}$ ", "page_idx": 28}, {"type": "text", "text": "\u00b7 Step (i): Assume that there is a predictor $\\hat{f}$ such that $\\mathbb{E}\\big[\\langle\\hat{f},\\psi_{i}\\rangle\\big]\\leq\\delta_{i}$ . In this step, we show that if exists a predictor $f$ in form of (8) and in $\\mathcal{C}$ , then $\\hat{f}$ always has smaller objective than $\\hat{f}$ . To that end, consider the following expression: ", "page_idx": 28}, {"type": "equation", "text": "$$\nA=\\mathbb{E}\\big[\\langle f(x)-\\hat{f}(x),\\psi_{0}(x)-\\sum_{i=1}^{m}k_{i}\\psi_{i}(x)\\rangle\\big].\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Now, we know that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\big[\\langle f(x)-\\hat{f}(x),\\displaystyle\\sum_{i=1}^{m}k_{i}\\psi_{i}(x)\\rangle\\big]=\\displaystyle\\sum_{i=1}^{m}k_{i}\\Big(\\mathbb{E}\\big[\\langle f(x),\\psi_{i}(x)\\rangle\\big]-\\mathbb{E}\\big[\\langle\\hat{f}(x),\\psi_{i}(x)\\rangle\\big]\\Big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\overset{(a)}{=}\\displaystyle\\sum_{i=1}^{m}k_{i}\\Big(\\delta_{i}-\\mathbb{E}\\big[\\langle\\hat{f}(x),\\psi_{i}(x)\\rangle\\big]\\Big)\\geq0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $(a)$ holds because $f\\in\\mathcal{C}$ . As a result, if $A\\geq0$ , then we could show that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\big[\\langle f(x)-\\hat{f}(x),\\psi_{0}(x)\\rangle\\big]\\ge0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and complete the proof. ", "page_idx": 29}, {"type": "text", "text": "To that end, first note that both $f$ and $\\hat{f}$ are in $\\Delta_{d}^{\\mathcal{X}}$ , and therefore ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\langle f(x),[1,\\dots,1]\\rangle=\\langle\\hat{f}(x),[1,\\dots,1]\\rangle=1.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "As a result, for any fixed scalar $c$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\langle f(x)-\\hat{f}(x),\\psi_{0}(x)-\\sum_{i=1}^{m}k_{i}\\psi_{i}(x)\\rangle=\\langle f(x)-\\hat{f}(x),\\psi_{0}(x)-\\sum_{i=1}^{m}k_{i}\\psi_{i}(x)-c\\rangle.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Next, we fix $c$ to be the maximum component of the vector $\\begin{array}{r}{\\psi_{0}(x)-\\sum_{i=1}^{m}k_{i}\\psi_{i}(x)}\\end{array}$ i.e., ", "page_idx": 29}, {"type": "equation", "text": "$$\nc:=\\operatorname*{max}_{i\\in[1:d]}\\{\\psi_{0}^{i}(x)-\\sum_{j=1}^{m}k_{j}\\psi_{j}^{i}(x)\\}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Now, we rewrite $A$ using (35) as ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{A=\\mathbb{E}\\Big[\\langle f(x)-\\hat{f}(x),\\psi_{0}(x)-\\sum_{i=1}^{m}k_{i}\\psi_{i}(x)-c\\rangle\\Big]}}}\\\\ {{\\displaystyle{=\\sum_{i=1}^{d}\\mathbb{E}\\Big[(f_{i}(x)-\\hat{f}_{i}(x))(\\psi_{0}^{i}(x)-\\sum_{j=1}^{m}k_{j}\\psi_{j}^{i}(x)-c))\\Big]}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Now, we consider two cases for which $E_{1}^{i}(x):f_{i}(x)>\\hat{f}_{i}(x)$ , and $E_{2}^{i}(x):f_{i}(x)\\leq\\hat{f}_{i}(x)$ If $f_{i}(x)\\,>\\,{\\hat{f}}_{i}(x)$ , then we have $f_{i}(x)\\,>\\,0$ , because $1\\,\\geq\\,{\\hat{f}}_{i}(x)\\,\\geq\\,0$ for all $i\\in[1:d]$ Therefore, using the definition of $\\textstyle{\\mathcal{S}}_{d}$ and because $f\\in S_{d}$ we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\psi_{0}^{i}(x)-\\sum_{j=1}^{m}k_{j}\\psi_{j}^{i}(x)=\\operatorname*{max}_{i\\in[1:d]}\\left\\{\\psi_{0}^{i}(x)-\\sum_{j=1}^{m}k_{j}\\psi_{j}^{i}(x)\\right\\}=c.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Consequently, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{A=\\displaystyle\\sum_{i=1}^{d}\\mathbb{E}\\Big[(f_{i}(x)-\\hat{f}_{i}(x))(\\psi_{0}^{i}(x)-\\displaystyle\\sum_{j=1}^{m}k_{j}\\psi_{j}^{i}(x)-c))\\Big]}\\qquad}&{}\\\\ &{=\\displaystyle\\sum_{i=1}^{d}\\mathbb{E}\\Big[(f_{i}(x)-\\hat{f}_{i}(x))(\\psi_{0}^{i}(x)-\\displaystyle\\sum_{j=1}^{m}k_{j}\\psi_{j}^{i}(x)-c))\\big|E_{1}^{i}(x)\\Big]\\operatorname*{Pr}\\big(E_{1}^{i}(x)\\big)}\\\\ &{+\\displaystyle\\sum_{i=1}^{d}\\mathbb{E}\\Big[(f_{i}(x)-\\hat{f}_{i}(x))(\\psi_{0}^{i}(x)-\\displaystyle\\sum_{j=1}^{m}k_{j}\\psi_{j}^{i}(x)-c))\\big|E_{2}^{i}(x)\\Big]\\operatorname*{Pr}\\big(E_{2}^{i}(x)\\big)}\\\\ &{\\overset{(a)}{=}\\displaystyle\\sum_{i=1}^{d}\\mathbb{E}\\Big[(f_{i}(x)-\\hat{f}_{i}(x))(\\psi_{0}^{i}(x)-\\displaystyle\\sum_{j=1}^{m}k_{j}\\psi_{j}^{i}(x)-c))\\big|E_{2}^{i}(x)\\Big]\\operatorname*{Pr}\\big(E_{2}^{i}(x)\\big)}\\\\ &{\\overset{(b)}{\\geq}0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $(a)$ holds due to (36) and $(b)$ holds because $f_{i}(x)\\ \\leq\\ \\hat{f}_{i}(x)$ and $\\psi_{m+1}^{i}(x)\\textrm{--}$ $\\begin{array}{r}{\\sum_{j=1}^{m}k_{j}\\psi_{j}^{i}(x)\\,\\le\\,c=\\operatorname*{max}_{i\\in[1:n+1]}\\{\\psi_{m+1}^{i}(x)-\\sum_{j=1}^{m}k_{j}\\psi_{j}^{i}(x)\\}}\\end{array}$ As a result, we have $A\\geq0$ that concludes (34) and completes the proof of this step. ", "page_idx": 29}, {"type": "image", "img_path": "Mtsi1eDdbH/tmp/3f79b497a70ba5623bae308d41837122e141491030eb1bef9d30bb43789aabfb.jpg", "img_caption": ["Figure 4: If an interior point of $\\mathcal{N}$ has one corresponding point at $M$ , then so are all interior points of $N$ "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "\u00b7 Step (ii): In this step, we show that the space of joint set of expected inner-products ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathcal{G}=\\Big\\{\\big(\\mathbb{E}[\\langle f(x),\\psi_{1}(x)\\rangle],\\dots,\\mathbb{E}[f(x),\\psi_{m}(x)]\\big):f\\in\\Delta_{d}^{\\chi}\\big\\},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "is compact under Euclidean metric, and convex. ", "page_idx": 30}, {"type": "text", "text": "To show the compactness of that space, assume that there is a sequence $\\{g_{n}\\}_{n=1}^{\\infty}$ such that $\\textstyle\\operatorname*{lim}_{n\\to\\infty}g_{n}~=~g$ . or accordingly there is a sequence of $f_{n}~\\in~\\Delta_{d}^{X}$ for which $\\operatorname*{lim}_{n\\to\\infty}$ $\\begin{array}{r}{\\mathbb{1}_{n\\to\\infty}\\left(\\mathbb{E}[\\langle f_{n}(x),\\psi_{1}(x)\\rangle],\\dots,\\mathbb{E}[f_{n}(x),\\psi_{m}(x)]\\right)\\,=\\,(g_{1},\\dots,g_{m})}\\end{array}$ . Since the metric is Euclidean, this is equivalent to $\\begin{array}{r}{\\operatorname*{lim}_{n\\rightarrow\\infty}\\mathbb{E}[\\langle f_{n}(x),\\psi_{i}(x)\\rangle]=g_{i}}\\end{array}$ for all $i\\in[1:m]$ . The weak compactness of $\\Delta_{d}^{\\mathcal{X}}$ , as proved in Lemma I.1, shows that there exists $f^{*}$ and a subsequence $f_{n_{k}}$ such that $\\begin{array}{r}{\\operatorname*{lim}_{k\\to\\infty}\\mathbb{E}[\\langle f_{n_{k}}(x),\\psi_{i}(x)\\rangle]=\\mathbb{E}[\\langle f^{*},\\psi_{i}(x)\\rangle]}\\end{array}$ for all $i\\in[1:d]$ Therefore, because of the choice of Euclidean metric, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{k\\rightarrow\\infty}{\\operatorname*{lim}}\\Big(\\mathbb{E}[\\langle f_{n_{k}}(x),\\psi_{1}(x)\\rangle],\\dots,\\mathbb{E}[\\langle f_{n_{k}}(x),\\psi_{m}(x)\\rangle]\\Big)}&{}\\\\ {=\\Big(\\mathbb{E}[\\langle f^{*},\\psi_{1}(x)\\rangle],\\dots,\\mathbb{E}[\\langle f^{*},\\psi_{m}(x)\\rangle]\\Big),}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which is equivalent to compactness of $\\mathcal{G}$ ", "page_idx": 30}, {"type": "text", "text": "To show the convexity of $\\mathcal{G}$ , it is enough to prove the convexity of $\\Delta_{d}^{\\mathcal{X}}$ . The reason is that $g(f)\\,=\\,\\bigl(\\mathbb{E}[\\langle f(x),\\psi_{1}(x)\\rangle],\\ldots,\\mathbb{E}[f(x),\\psi_{m}(x)]\\bigr)$ is a linear functional of $f$ , and a linear functional images a convex set to another convex set. ", "page_idx": 30}, {"type": "text", "text": "To prove the convexity of $\\Delta_{d}^{\\mathcal{X}}$ , let $f,f^{\\prime}\\in\\Delta_{d}^{\\mathcal{X}}$ . This means that $f_{i}(x),f_{i}^{\\prime}(x)\\in[0,1]$ for all $i\\in[1:d]$ and $\\begin{array}{r}{\\sum_{i=1}^{d}f_{i}(x)=\\sum_{i=1}^{d}f_{i}^{\\prime}(x)=1.}\\end{array}$ Consequently, $a f_{i}(x)+(1-a)f_{i}^{\\prime}(x)\\geq0$ since $a,1\\mathrm{~-~}a\\mathrm{~}\\geq\\mathrm{~}0$ .Moreover, $\\begin{array}{r}{\\sum_{i=1}^{d}a f_{i}(x)+(1-a)f_{i}^{\\prime}(x)\\,=\\,a\\sum_{i=1}^{d}f_{i}(x)\\,+\\,(1-a)f_{i}^{\\prime}(x)\\;\\;}\\end{array}$ $\\begin{array}{r}{a)\\sum_{i=1}^{d}f_{i}^{\\prime}(x)=a+1-a=1}\\end{array}$ . As a result of these two facts, $a f+(1-a)f^{\\prime}\\in\\Delta_{d}^{\\mathcal{X}}$ , and the proof of this step is completed. ", "page_idx": 30}, {"type": "text", "text": "\u00b7 Step (iv): In this step we show that if the tuple of constraints is an interior points of all possible achievable tuples of constraints using different prediction functions, then a point in $\\mathcal{C}$ achieves its supremum in terms of objective $\\mathbb{E}\\big[\\langle f(x),\\dot{\\psi}_{0}(x)\\big]$ if and only if it is in form of (8) almost everywhere. This is an extension to [21, Theorem 3.1] and its proof resembles to the proof that is provided there. The sufficiency is already shown in Step (i). Therefore, we only need to show that if a prediction function in $\\mathcal{C}$ maximizes the objective, then it is in form of (8). ", "page_idx": 30}, {"type": "text", "text": "Firstly, using..Step (i),  we know that the space $\\mathcal{N}$ ofall\u3000points $\\big(\\mathbb{E}[\\langle{\\dot{f}}(x),\\psi_{1}({\\bar{x}})\\rangle],\\dots,\\mathbb{E}[f(x),\\psi_{m}(x)]\\big)$ and\u3000 the\u3000space $\\mathcal{M}$ ofallpoints $\\big(\\mathbb{E}[\\langle f(x),\\psi_{1}(x)\\rangle],\\dots,\\mathbb{E}[f(x),\\psi_{0}(x)]\\big)$ are compact and convex. Now, assume that $v~=~(\\delta_{1},\\ldots,\\delta_{m})$ is an interior point of $\\mathcal{N}$ . Then, the corresponding set in $\\mathcal{M}$ , i.e., $B_{v}=\\{(\\delta_{0},\\allowbreak\\cdot\\cdot,\\delta_{m})\\in\\mathcal{M}:\\delta_{0}\\in\\mathbb{R}\\}$ has a supremum and an infimum of the first component that we name $\\delta^{**}$ and $\\delta^{*}$ . Now, since $\\mathcal{M}$ is compact, then $v^{**}=(\\delta^{**},\\delta_{1},\\dots,\\delta_{m})$ and $v^{*}=(\\delta^{*},\\delta_{1},\\ldots,\\delta_{m})$ are contained in $\\mathcal{M}$ . Next, assume the following two cases: ", "page_idx": 30}, {"type": "text", "text": "$\\delta^{**}=\\delta^{*}$ : In this case for all other points $\\overline{{v}}=(\\overline{{\\delta}}_{1},\\hdots,\\overline{{\\delta}}_{m})$ of $\\mathcal{N}$ , the corresponding set $B_{v^{\\prime}}$ in $\\mathcal{M}$ is a single point. The reason is that, if it is not so, then we have two points $\\overline{{v}}^{**}=(\\overline{{\\delta}}^{**},\\overline{{\\delta}}_{1},\\dots,\\overline{{\\delta}}_{m})$ and $\\overline{{v}}^{*}=(\\overline{{\\delta}}^{*},\\overline{{\\delta}}_{1},\\dots,\\overline{{\\delta}}_{m})$ where $\\overline{{{\\delta}}}^{**}>\\overline{{{\\delta}}}^{*}$ . Now, since $v$ is an interior point of $\\mathcal{N}$ , then on any direction in a small neighborhood around $v$ there exists a point $v^{\\prime}$ within $\\mathcal{N}$ . Let that direction be opposite the connecting line of $v$ and $\\overline{{v}}$ i.e., let $v$ be on a connecting line of $v^{\\prime}$ and $\\overline{{v}}^{*}$ . Now, make a convex hull using the three points $v^{\\prime},\\,\\overline{{v}}^{**}$ , and $\\overline{{v}}^{*}$ , which are all in $\\mathcal{M}$ . Because of the convexity of $\\mathcal{M}$ , the convex hull is also a subset of $\\mathcal{M}$ . Since $v$ is an interior point of the convex hull, this means that a neighborhood of $v$ along any direction is inside $\\mathcal{M}$ . Now, if we set $(m+1)\\mathrm{th}$ axis to be that direction, we contradict with the fact that $\\delta^{*}=\\delta^{**}$ . (See Figure 4) Now, we know that in such case all points within $\\mathcal{N}$ have one corresponding point in $\\mathcal{M}$ . Because of the convexity of $\\mathcal{M}$ this is equivalent to $\\mathcal{M}$ being a subset of a hyperplane with the generating formula $\\textstyle x_{0}\\,=\\,\\sum_{i=1}^{m}k_{i}x_{i}\\,+\\,k_{0}$ .Therefore, we have $\\begin{array}{r}{\\mathbb{E}\\big[\\langle f,\\psi_{0}\\rangle\\big]\\,=\\,\\mathbb{E}\\big[\\langle f,\\sum_{i=1}^{m}k_{i}\\psi_{i}\\rangle\\big]+k_{0}\\,}\\end{array}$ for all $f\\in\\Delta_{d}^{\\mathcal{X}}$ . Therefore, for $d\\geq2$ if we set $\\begin{array}{r}{f_{1}\\;=\\;\\big(\\frac{p(x)}{d-2},\\cdot\\cdot\\cdot,\\frac{p(x)}{d-2},\\underline{{1}}-p(x),\\frac{p(x)}{d-2},\\cdot\\cdot\\cdot,\\underbrace{0}_{\\longrightarrow},\\frac{p(x)}{d-2},\\cdot\\cdot\\cdot,\\frac{p(x)}{d-2}\\big)}\\end{array}$ a..,) and f = 2 $\\big(\\frac{p(x)}{d-2},\\cdot\\cdot\\cdot,\\frac{p(x)}{d-2},\\underbrace{0}_{i},\\frac{p(x)}{d-2},\\cdot\\cdot\\cdot,\\underbrace{1-p(x)}_{j},\\frac{p(x)}{d-2},\\cdot\\cdot\\cdot,\\frac{p(x)}{d-2}\\big)$ for $p(x)\\in[0,1]^{\\chi}$ , then we ", "page_idx": 31}, {"type": "text", "text": "have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[\\langle f_{1},\\psi_{0}\\rangle\\big]-\\mathbb{E}\\big[\\langle f_{1},\\sum_{i=1}^{m}k_{i}\\psi_{i}\\rangle\\big]=\\mathbb{E}\\big[\\langle f_{2},\\psi_{0}\\rangle\\big]-\\mathbb{E}\\big[\\langle f_{2},\\sum_{i=1}^{m}k_{i}\\psi_{i}\\rangle\\big],\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "or equivalently ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[(1-p(x))(\\psi_{0}^{i}(x)-\\sum_{t=1}^{m}k_{t}\\psi_{t}^{i}(x)-\\psi_{m+1}^{j}(x)+\\sum_{t=1}^{m}k_{t}\\psi_{t}^{j}(x))\\big]=0,\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "for all function $p(x)\\in\\Delta_{d}^{\\mathcal{X}}$ . A similar result can be achieved for $d=2$ and by setting $f_{1}=(p(x),1-\\dot{p}(\\dot{x}))$ and $f_{2}=(1-p(x),p(x))$ . As a result, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\psi_{0}^{i}(x)-\\sum_{t=1}^{m}k_{t}\\psi_{t}^{i}(x)=\\psi_{0}^{j}(x)-\\sum_{t=1}^{m}k_{t}\\psi_{t}^{j}(x),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "for all $i\\neq j\\in[1:d]$ , and consequently ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\psi_{0}^{i}(x)-\\sum_{t=1}^{m}k_{t}\\psi_{t}^{i}(x)=\\operatorname*{max}_{j\\in[1:d]}\\{\\psi_{0}^{j}(x)-\\sum_{t=1}^{m}k_{t}\\psi_{t}^{j}(x)\\},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "for all $i\\in[1:n+1]$ . As a result, there is a set of $k_{1},\\hdots,k_{m}$ such that $\\psi_{0}(x)~-~$ $\\textstyle\\sum_{i=1}^{m}k_{i}\\psi_{i}({\\bar{x}})$ has equal components almost everywhere. As a result, due to the freedom of choice for $\\begin{array}{r}{\\tau(\\psi_{0}(x)-\\sum_{i=1}^{m}k_{i}\\psi_{i}(x),x)}\\end{array}$ where $\\tau\\ \\in\\ S_{d}$ and when we have more than one maximizer component, then, without loss of generality we can assume that every prediction function $f$ almost everywhere is in form of $\\tau(\\psi_{m+1}(x)-$ $\\textstyle\\sum_{i=1}^{m}k_{i}\\psi_{i}(x),{\\dot{x}})$ ", "page_idx": 31}, {"type": "text", "text": "2. $\\delta^{**}>\\delta^{*}$ : In such case, for all $\\delta_{0}\\,\\in\\,[\\delta^{*},\\delta^{**}]$ , we can show that $v\\,=\\,(\\delta_{0},\\ldots,\\delta_{m})$ is an interior point of $\\mathcal{M}$ To show that, we first find $m$ points $v_{1}^{\\prime},\\ldots,v_{m}^{\\prime}\\in\\mathcal{N}$ that are linearly independent and such that their convex hull include $(\\delta_{1},\\hdots,\\delta_{m})$ . Using the definition of $\\mathcal{M}$ , for each of these points $v_{i}^{\\prime}=(\\delta_{1}^{\\prime\\,i},\\cdot\\cdot\\cdot,\\delta_{m}^{\\prime\\,\\ i})$ , there exists $h_{i}^{\\prime}\\in\\mathbb{R}$ such that $v_{i}^{\\prime\\prime}=(h_{i}^{\\prime},\\delta_{1}^{\\prime\\,i},\\dots,\\delta_{m}^{\\prime\\,\\ i})$ is within $\\mathcal{M}$ Nowwadttns $v^{**}$ and $v^{*}$ to these sets of points. It is easy to see that $v_{i}^{\\prime\\prime}\\mathrm{s}$ are linearly independent. Furthermore, we know that $(\\delta_{1},\\hdots,\\delta_{m})$ is a convex combination of $v_{i}^{\\prime}\\mathbf{s}$ , i.e., $\\begin{array}{r}{\\sum_{i}a_{i}v_{i}^{\\prime}=\\left(\\delta_{1},\\ldots,\\delta_{m}\\right)}\\end{array}$ As a result, if $\\begin{array}{r}{\\sum_{i}b_{i}v_{i}^{\\prime\\prime}-v^{**}=(0,\\ldots,0)}\\end{array}$ , then we have $b_{i}\\,=\\,a_{i}$ for $i\\,\\in\\,[1\\,:\\,m]$ Furthermore, we have $\\sum a_{i}h_{i}^{\\prime}\\;=\\;\\sum b_{i}h_{i}^{\\prime}\\;=\\;\\delta^{**}$ Similarly, if $\\begin{array}{r l r}{\\sum_{i}c_{i}v_{i}^{\\prime\\prime}\\,-\\,v^{*}\\,=}&{{}}&{}\\end{array}$ $(0,\\ldots,0)$ we have $c_{i}=a_{i}$ and $\\begin{array}{r}{\\sum a_{i}h_{i}^{\\prime}=\\sum c_{i}h_{i}^{\\prime}=\\delta^{*}}\\end{array}$ . As a result, since $\\delta^{*}\\neq\\delta^{**}$ at least one of these cases would not occur, or equivalently, the dimension of the convex hull of $v_{1}^{\\prime\\prime},\\ldots v_{m}^{\\prime\\prime},v^{**},v^{*}$ is of dimension $m+1$ .As a result, $v$ is an interior point of this convex hull, and because the convex hull is $(m+1)$ -dimensional, it is an interior point of $\\mathcal{M}$ ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "Now, since $v^{\\ast\\ast}$ is a border point in $\\mathcal{M}$ and due to the convexity of $\\mathcal{M}$ there is a hyperplane $\\mathcal{P}$ such that it passes $v^{**}$ and it lays above all points of $\\mathcal{M}$ . Since $v$ is an interior point of $\\mathcal{M}$ , a neighborhood of $v$ is laid under the hyperplane $\\mathcal{P}$ , hence $v$ cannot be laid on the hyperplane. Therefore, if the generating formula of such hyperplane is $\\textstyle\\sum_{i=0}^{m}k_{i}x_{i}={\\overset{\\cdot}{\\sum}}_{i=1}^{\\hat{m}}\\,k_{i}\\delta_{i}+k_{0}\\delta^{**}$ since $v$ isnot ladonthe hyperplane we assure that $\\begin{array}{r}{\\sum_{i=1}^{m}k_{i}\\delta_{i}+k_{0}\\delta_{0}\\neq\\sum_{i=1}^{m}k_{i}\\delta_{i}+k_{0}\\delta^{**},}\\end{array}$ or equivalently $k_{0}\\neq0$ Hence, without loss of generality assume that $k_{0}\\,=\\,-1$ . This shows that for all points in $\\left(u_{0},\\ldots,u_{m}\\right)\\in\\mathcal{M}$ we have ", "page_idx": 32}, {"type": "equation", "text": "$$\nu_{0}-\\sum_{i=1}^{m}k_{i}u_{i}\\leq\\delta^{**}-\\sum_{i=1}^{m}k_{i}\\delta_{i},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "or equivalently, by the definition of $\\mathcal{M}$ , for all prediction function $f$ ,we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[\\langle f(x),\\psi_{0}(x)-\\sum_{i=1}^{m}k_{i}\\psi_{i}(x)\\rangle\\big]\\leq\\delta^{**}-\\sum_{i=1}^{m}k_{i}\\delta_{i}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Assuming that $\\hat{f}\\in\\mathcal{C}$ maximizes the objective, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[\\langle f(x),\\psi_{0}(x)-\\sum_{i=1}^{m}k_{i}\\psi_{i}(x)\\rangle\\big]\\leq\\mathbb{E}\\big[\\langle\\hat{f}(x),\\psi_{0}(x)-\\sum_{i=1}^{m}k_{i}\\psi_{i}(x)\\rangle\\big].\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "This shows that almost everywhere when there is a unique maximizing component $j$ in $\\begin{array}{r}{\\psi_{0}(x)-\\sum_{i=1}^{m}k_{i}\\psi_{i}(x)}\\end{array}$ then ${\\hat{f}}_{j}(x)=1$ The reason is that otherwise and i there is a set $A$ such that $\\mu(A)>0$ and for $x\\in A$ and a choice of $l\\in[0,1)$ \uff0c $\\epsilon\\in\\mathbb{R}$ , and all $t\\not=j$ we have $\\begin{array}{r}{\\psi_{m+1}^{j}(x)-\\sum_{i=1}^{m}k_{i}\\psi_{i}^{j}(x)\\geq\\epsilon+\\psi_{m+1}^{t}(x)-\\sum_{i=1}^{m}k_{i}\\psi_{i}^{t}(x)}\\end{array}$ while $f_{j}\\leq1-l$ , then we can make a function $f(x)$ that is $f(x)={\\hat{f}}(x)$ for $x\\in{\\mathcal{X}}\\setminus A$ and $f(x)=[0,\\ldots,\\underbrace{1}_{j},\\ldots,0]$ for $x\\in A$ . Such function leads to ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[\\langle f(x),\\psi_{0}(x)-\\sum_{i=1}^{m}k_{i}\\psi_{i}(x)\\rangle\\big]\\ge\\epsilon l\\mu(A)+\\mathbb{E}\\big[\\langle\\hat{f}(x),\\psi_{0}(x)-\\sum_{i=1}^{m}k_{i}\\psi_{i}(x)\\rangle\\big],\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "that is in contradiction with (37). This completes the proof of this step. ", "page_idx": 32}, {"type": "text", "text": "JProof of Theorem 4.2 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In the following, we introduce a few lemmas that are useful in our proofs. ", "page_idx": 32}, {"type": "text", "text": "Lemma J.1. For every random variable $X$ on $\\mathbb{R}$ wehave ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\tau\\to t^{-}}\\operatorname*{Pr}(\\tau\\leq X<t)=\\operatorname*{lim}_{\\tau\\to t^{+}}\\operatorname*{Pr}(t<X<\\tau)=0\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. For each increasing sequence $\\{\\tau_{i}\\}_{i=1}^{\\infty}$ we show that the first limit is zero, which proves the claim that the function of $\\tau$ has a zero limit. ", "page_idx": 32}, {"type": "text", "text": "We define ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{S_{i}=[\\tau_{i},t),}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and notice that ", "page_idx": 32}, {"type": "equation", "text": "$$\n{\\cal S}_{1}\\supseteq{\\cal S}_{2}\\supseteq...\\,.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Further, we note that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\bigcap_{i=1}^{\\infty}S_{i}=\\varnothing.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "As a result ", "page_idx": 32}, {"type": "equation", "text": "$$\nS_{1}^{c}\\subseteq S_{2}^{c}\\subseteq\\ldots,\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\bigcup_{i=1}^{\\infty}S_{i}^{c}=\\mathbb{R}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Next, because probability measure is $\\sigma$ -additive, we conclude its lower-semicontinuity [38, Theorem 13.6], and therefore we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{i\\to\\infty}\\operatorname*{Pr}(X\\in\\mathcal{S}_{i}^{c})=\\operatorname*{Pr}(X\\in\\cup_{i=1}^{\\infty}S_{i}^{c})=1,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "which proves $\\mathrm{lim}_{i\\to\\infty}\\operatorname*{Pr}(X\\in{\\mathcal{S}}_{i})=0$ ", "page_idx": 33}, {"type": "text", "text": "We could take similar steps to show that since $\\cap_{i=1}^{\\infty}(t,\\tau_{i}^{\\prime})=\\emptyset$ for decreasing $\\tau_{i}^{\\prime}$ we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{i\\to\\infty}\\operatorname*{Pr}(X\\in(t,\\tau_{i}^{\\prime}))=0.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Lemma J.2. Let $\\psi_{1}:\\mathcal{X}\\to\\mathbb{R}^{d}$ be a bounded function. Further, we define two functions $C(k)=$ $\\mathbb{E}\\big[\\langle f_{k,0}^{*}(x),\\psi_{1}(x)\\rangle\\big]$ $D(k)=\\mathbb{E}\\big[\\langle f_{k,1}^{*}(x),\\psi_{1}(x)\\rangle\\big]$ and $F(k)=\\mathbb{E}\\big[\\langle f_{k,1}^{*}(x),\\psi_{0}(x)\\rangle\\big]$ , where $f_{k,p}^{*}$ . defined in Theorem 4.2. Then, ", "page_idx": 33}, {"type": "text", "text": "1. $C(k)$ is monotonically non-increasing,   \n2. $C(k)$ is upper semi-continuous,   \n3. $F(k)$ is monotonically non-decreasing,   \n4. $D(k)$ is lower semi-continuous, and we have   \n5. $\\mathrm{lim}_{k^{\\prime}\\uparrow k}\\,C(k)=\\mathrm{lim}_{k^{\\prime}\\uparrow k}\\,D(k)$ ", "page_idx": 33}, {"type": "text", "text": "Proof. 1. Firstly, let us define $\\ell_{k}(x)\\,=\\,\\psi_{0}(x)\\,-\\,k\\psi_{1}(x)$ . For the setting where $p=0$ , the prediction function $f_{k,p}^{*}(x)$ is defined as ", "page_idx": 33}, {"type": "equation", "text": "$$\nf_{k,0}^{*}(x,p)=\\left\\lbrace\\begin{array}{c c}{1}&{i=\\operatorname*{min}\\lbrace\\operatorname*{argmin}_{j\\in\\mathrm{argmax}\\,\\ell_{k}(x)}\\left(\\psi_{1}(x)\\right)(j)\\rbrace}\\\\ {0}&{\\mathrm{otherwise}}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Further, for $k_{1},k_{2}$ such that $k_{1}\\leq k_{2}$ , let us define $j_{1}$ and $j_{2}$ as the only non-zero index of $f_{k_{1},0}^{*}(x,p)$ and $f_{k_{2},0}^{*}(x,p)$ , respectively. To show that $C(k)$ is monotonically non-increasing we only need to show that $\\left(\\psi_{1}(x)\\right)(j_{1})\\ =\\ \\langle f_{k_{1},0}^{*}(x),\\psi_{1}(x)\\rangle\\ \\geq\\ \\langle f_{k_{2},0}^{*}(x),\\psi_{1}(x)\\rangle\\ =$ $\\left(\\psi_{1}(x)\\right)(j_{2})$ . Assume that this does not occur, or equivalently $\\left(\\psi_{1}(x)\\right)\\!\\left(j_{1}\\right)<\\left(\\psi_{1}(x)\\right)\\!\\left(j_{2}\\right)$ In such case we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{max}\\ell_{k_{2}}(x)\\overset{(a)}{=}\\big(\\ell_{k_{2}}(x)\\big)(j_{2})}\\\\ &{\\hphantom{(b)}=\\big(\\ell_{k_{1}}(x)-(k_{2}-k_{1})\\psi_{1}(x)\\big)(j_{2})}\\\\ &{\\hphantom{(b)}\\leq(k_{1}-k_{2})\\big(\\psi_{1}(x)\\big)(j_{2})+\\operatorname*{max}\\big(\\ell_{k_{1}}(x)\\big)(j)}\\\\ &{\\hphantom{(b)}\\overset{(b)}{=}(k_{1}-k_{2})\\big(\\psi_{1}(x)\\big)(j_{2})+\\big(\\ell_{k_{1}}(x)\\big)(j_{1})}\\\\ &{\\hphantom{(b)}\\overset{(c)}{<}(k_{1}-k_{2})\\big(\\psi_{1}(x)\\big)(j_{1})+\\big(\\ell_{k_{1}}(x)\\big)(j_{1})}\\\\ &{\\hphantom{(b)}=\\big(\\ell_{k_{2}}(x)\\big)(j_{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $(a)$ and $(b)$ holds due to the definition of $j_{1}$ and $j_{2}$ , and $(c)$ holds due to the assumption $\\left(\\psi_{1}(x)\\right)\\!\\left(j_{1}\\right)<\\left(\\psi_{1}(x)\\right)\\!\\left(j_{2}\\right)$ . The last inequality is clearly a contradiction, and shows that $\\langle f_{k_{1},0}^{*}(x),\\psi_{1}(x)\\rangle\\geq\\langle f_{k_{2},0}^{*}(x),\\psi_{1}(x)\\rangle$ , and therefore $C(k_{1})\\geq C(k_{2})$ 2. Let us divide the space $\\mathcal{X}$ into two subsets ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 33}, {"type": "equation", "text": "$$\nA_{k}=\\Big\\{x\\in\\mathcal{X}:\\,|\\,\\mathrm{argmax}(\\ell_{k}(x))(i)|=d\\Big\\},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "equation", "text": "$$\nB_{k}=\\Big\\{x\\in\\mathcal{X}:\\,|\\,\\mathrm{argmax}_{i}(\\ell_{k}(x))(i)|\\in[1:d-1]\\Big\\}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "For each $x\\in A_{k}$ we know ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(f_{k,0}^{*}(x)\\right)(i)=\\left\\{\\begin{array}{c c}{1}&{i=\\operatorname*{min}\\{\\underset{j}{\\operatorname{argmin}}\\left(\\psi_{1}(x)\\right)(j)\\}}\\\\ {0}&{\\mathrm{otherwise}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Using previous part, we know that by increasing $k$ we have no increase in $\\langle f_{k,0}^{*}(x),\\psi_{1}(x)\\rangle$ and in this case since $\\langle f_{k,0}^{*}(x),\\psi_{1}(x)\\rangle=\\mathrm{min}_{j}\\left(\\psi_{1}(x)\\right)(j)$ , then this value cannot reduce with the change of $k$ . Therefore, $\\langle f_{k,0}^{*}(x),\\psi_{1}(x)\\rangle$ is a constant function for all $k^{\\prime}\\geq k$ , and consequently $\\mathbb{E}\\big[\\langle f_{k^{\\prime},0}^{*}(x),\\psi_{1}(x)\\rangle|x\\in A_{k}\\big]\\operatorname*{Pr}(x\\in A_{k})$ is a constant function for $k^{\\prime}\\geq k$ If $x~\\in~B_{k}$ , then for $j\\;\\;\\notin\\;\\underset{i}{\\mathrm{argmax}}\\left(\\ell_{k}(x)\\right)(i)$ and $l\\;\\in\\;\\underset{i}{\\mathrm{argmax}}\\left(\\ell_{k}(x)\\right)(i)$ , we have $\\big(\\ell_{k}(x)\\big)(j)<\\big(\\ell_{k}(x)\\big)(l)$ . Define the set $C_{\\delta}$ for $\\delta\\geq0$ as ", "page_idx": 34}, {"type": "equation", "text": "$$\nC_{\\delta}=\\{x\\in B_{k}:\\,\\left(\\ell_{k}(x)\\right)(j)\\leq\\left(\\ell_{k}(x)\\right)(l)-\\delta\\}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Using Lemma J.1 we know that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\delta\\to0}\\operatorname*{Pr}(B_{k}\\setminus C_{\\delta})=0,\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "or equivalently for all $\\epsilon\\geq0$ , there exists $\\delta$ such that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathrm{Pr}(B_{k}\\setminus C_{\\delta})\\leq\\epsilon^{\\prime}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Therefore, if without loss of generality, we assume that $\\psi_{1}(x)$ is bounded by 1, then there exists $\\delta\\geq0$ such that we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\big[\\langle f_{k^{\\prime},0}^{*}(x),\\psi_{1}(x)\\rangle|x\\in B_{k}\\setminus C_{\\delta}\\big]\\operatorname*{Pr}(x\\in B_{k}\\setminus C_{\\delta})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad\\overset{(a)}{\\leq}\\|\\psi_{1}(x)\\|_{\\infty}\\operatorname*{Pr}(x\\in B_{k}\\setminus C_{\\delta})\\leq\\epsilon/2,}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $(a)$ holds due to Holder's inequality. ", "page_idx": 34}, {"type": "text", "text": "$x\\in C_{\\delta}$ , and because we assumed $\\|\\psi_{1}(x)\\|_{\\infty}\\leq1$ , then we know that by increasing $k$ to $k^{\\prime}\\in[k-\\delta/2,k+\\delta/2)$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathcal{Z}=\\mathrm{argmax}\\,\\ell_{k^{\\prime}}(x)\\subseteq\\mathrm{argmax}\\,\\ell_{k}(x)=\\mathcal{I}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "This means that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\langle f_{k,0}^{*}(x),\\psi_{1}(x)\\rangle=\\operatorname*{min}_{j\\in\\mathcal{I}}\\big(\\psi_{1}(x)\\big)(j)\\leq\\operatorname*{min}_{j\\in\\mathcal{I}}\\big(\\psi_{1}(x)\\big)(j)=\\langle f_{k^{\\prime},0}^{*}(x),\\psi_{1}(x)\\rangle.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "This,  together with the previous part in  which  we showed $\\begin{array}{r l}{\\langle f_{k,0}^{*}(x),\\psi_{0}(x)\\rangle}&{{}\\geq}\\end{array}$ $\\langle f_{k^{\\prime},0}^{*}(x),\\psi_{0}(x)\\rangle$ , concludes that $\\langle f_{k,0}^{*}(x),\\psi_{0}(x)\\rangle\\,=\\,\\langle f_{k^{\\prime},0}^{*}(x),\\psi_{0}(x)\\rangle$ . This means that $\\mathbb{E}\\big[\\langle f_{k^{\\prime},0}^{*}(x),\\psi_{0}(x)\\rangle|x\\in C_{\\delta}\\big]\\operatorname*{Pr}(x\\in C_{\\delta})$ is a constant function for all $k^{\\prime}\\geq k$ Finally, since we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C(k^{\\prime})=\\mathbb{E}\\big[\\langle f_{k^{\\prime},0}^{*}(x),\\psi_{1}(x)\\rangle\\big]=\\!\\mathbb{E}\\big[\\langle f_{k^{\\prime},0}^{*}(x),\\psi_{1}(x)\\rangle|x\\in A_{k}\\big]\\operatorname*{Pr}(x\\in A_{k})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad+\\,\\mathbb{E}\\big[\\langle f_{k^{\\prime},0}^{*}(x),\\psi_{1}(x)\\rangle|x\\in B_{k}\\setminus C_{\\delta}\\big]\\operatorname*{Pr}(x\\in B_{k}\\setminus C_{\\delta})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad+\\,\\mathbb{E}\\big[\\langle f_{k^{\\prime},0}^{*}(x),\\psi_{1}(x)\\rangle|x\\in C_{\\delta}\\big]\\operatorname*{Pr}(x\\in C_{\\delta}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and because the first term and the third term in RHS are constant in terms of $k^{\\prime}$ andfor $k^{\\prime}\\geq k$ , and the second term is diminishing, then we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|C(k^{\\prime})-C(k)\\right|=\\left|\\mathbb{E}\\big[\\langle f_{k^{\\prime},0}^{*}(x),\\psi_{1}(x)\\rangle|x\\in B_{k}\\setminus C_{\\delta}\\big]\\operatorname*{Pr}(x\\in B_{k}\\setminus C_{\\delta})\\right.}\\\\ &{\\qquad\\qquad\\qquad\\left.-\\,\\mathbb{E}\\big[\\langle f_{k,0}^{*}(x),\\psi_{1}(x)\\rangle|x\\in B_{k}\\setminus C_{\\delta}\\big]\\operatorname*{Pr}(x\\in B_{k}\\setminus C_{\\delta})\\right|\\le\\epsilon/2+\\epsilon/2,}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "which is equivalent to say that $\\mathrm{lim}_{k^{\\prime}\\uparrow k}\\,C(k^{\\prime})=C(k)$ ", "page_idx": 34}, {"type": "text", "text": "3. For $p=1$ , we know that the prediction function $f_{k,p}^{*}(\\boldsymbol{x})$ is obtained as ", "page_idx": 34}, {"type": "equation", "text": "$$\nf_{k,1}^{*}(x)=\\left\\{\\begin{array}{c c}{{1}}&{{i=\\operatorname*{min}\\{\\underset{j\\in\\mathrm{argmax}\\,\\ell_{k}(x)}{\\mathrm{argmax}\\,\\ell_{k}(x)}\\left(\\psi_{0}(x)\\right)(j)\\}}}\\\\ {{0}}&{{\\mathrm{otherwise}}}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "If we define $\\psi_{1}^{\\prime}(x):=-\\psi_{0}(x)$ , then we have ", "page_idx": 35}, {"type": "equation", "text": "$$\nf_{k,1}^{*}(x)=\\left\\{\\begin{array}{c c}{{1}}&{{i=\\operatorname*{min}\\{\\underset{j\\in\\mathrm{argmax}\\,\\ell_{k}(x)}{\\mathrm{argmin}}\\left(\\psi_{1}^{\\prime}(x)\\right)(j)\\}}}\\\\ {{0}}&{{\\mathrm{otherwise}}}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Since the above is equal to (38), then using the first part of this lemma, we know that $\\mathbb{E}\\big[\\langle f_{k,1}^{*}(x),\\psi_{1}^{\\prime}(x)\\rangle\\big]\\,\\stackrel{\\,.}{=}\\,-\\mathbb{E}\\big[\\langle f_{k,1}^{*}(x),\\psi_{0}(x)\\rangle\\big]$ is monotonically non-increasing, which is equivalento $F(k)=\\mathbb{E}\\big[\\langle f_{k,1}^{*}(x),\\psi_{0}(x)\\rangle\\big]$ being monotonically non-decreasing. ", "page_idx": 35}, {"type": "text", "text": "4. This part is similar to the second part of the proof. In fact, if $x\\in A_{k}$ ,thenwehave ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left(f_{k,1}^{\\ast}(x)\\right)(i)=\\left\\{\\begin{array}{c c}{{1}}&{{i=\\mathrm{min}\\{\\mathrm{argmax}\\left(\\psi_{0}(x)\\right)(j)\\}}}\\\\ {{0}}&{{\\mathrm{otherwise}}}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "For $k^{\\prime}\\leq k$ and because of the third part of this lemma, we know that $\\langle f_{k^{\\prime},1}^{*}(x),\\psi_{0}(x)\\rangle\\geq$ $\\langle f_{k,1}^{*}(x),\\psi_{0}(x)\\rangle$ . Furthermore, because of (41) we know that $\\left\\langle f_{k,1}^{*}(x),\\psi_{0}(x)\\right\\rangle\\;\\,=$ max $\\psi_{0}(x)$ , and therefore by reducing $k^{\\prime}$ , the prediction function $f_{k^{\\prime},1}^{*}(x)$ stays constant. As a result, $\\mathbb{E}\\big[\\langle f_{k^{\\prime},1}^{*}(x),\\psi_{1}(x)\\rangle|x\\in A_{k}\\big]\\operatorname*{Pr}(x\\in A_{k})$ is a constant function for $k^{\\prime}\\leq k$ Furthermore, similar to the second part of this lemma, we can show that for each $\\epsilon>0$ there exists $\\delta^{\\prime}\\geq0$ such that for all $\\bar{0^{\\prime}}\\le\\delta\\le\\delta^{\\prime}$ we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\big[\\langle f_{k^{\\prime},1}^{*}(x),\\psi_{1}(x)\\rangle|x\\in B_{k}\\ \\backslash\\ C_{\\delta}\\big]\\operatorname*{Pr}(x\\in B_{k}\\ \\backslash\\ C_{\\delta})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\overset{(a)}{\\leq}\\|\\psi_{1}(x)\\|_{\\infty}\\operatorname*{Pr}(x\\in B_{k}\\ \\backslash\\ C_{\\delta})\\leq\\epsilon/4,}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Moreover, for the case of $x\\in C_{\\delta}$ , since in this case $\\mathcal{I}\\subseteq\\mathcal{Z}$ , then we know that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\langle f_{k,1}^{*}(x),\\psi_{0}(x)\\rangle=\\operatorname*{max}_{j\\in\\mathcal{I}}\\big(\\psi_{0}(x)\\big)(j)\\leq\\operatorname*{max}_{j\\in\\mathcal{I}}\\big(\\psi_{0}(x)\\big)(j)=\\langle f_{k^{\\prime},1}^{*}(x),\\psi_{0}(x)\\rangle.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Next, using the third part of this lemma, we know that for $k^{\\prime}\\ \\ \\leq\\ \\ k$ we have $\\begin{array}{r l r}{\\langle f_{k^{\\prime},1}^{*}(x),\\bar{\\psi_{0}}(x)\\rangle}&{{}\\leq}&{\\bar{\\langle}f_{k,1}^{*}(x),\\psi_{0}(x)\\rangle}\\end{array}$ \uff0cwhich together with (43\uff09 concludes that $\\langle f_{k,1}^{*}(x),\\psi_{0}(x)\\rangle\\,=\\,\\langle f_{k^{\\prime},1}^{*}(x),\\psi_{0}(x)\\rangle$ . Next, because $\\left(\\psi_{0}(x)-k\\psi_{1}(x)\\right)(i)\\,=\\,\\bigl(\\psi_{0}(x)\\,-$ $k\\psi_{1}(x))(j)$ for $i,j\\ \\in\\ {\\mathcal{I}}$ , then we know that $\\Big|\\big(\\ell_{k^{\\prime}}(x)\\big)(i)\\;-\\;\\big(\\ell_{k^{\\prime}}(x)\\big)(j)\\Big|\\;=\\;\\big|(k\\;-\\;$ $k^{\\prime})\\Big(\\big(\\psi_{1}(x))(i)\\,-\\,\\big(\\psi_{1}(x))(j)\\Big)\\,\\le\\,2|k\\,-\\,k^{\\prime}|$ .Therefore, if for $i,j\\in\\mathcal{I}$ we know that $\\left(\\psi_{0}(x)\\right)(i)=\\left(\\psi_{0}(x)\\right)(j)$ , then the difference between $\\psi_{1}$ for those indices is bounded as ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Big|\\big(\\psi_{1}(x)\\big)(i)-\\big(\\psi_{1}(x)\\big)(j)\\Big|\\le\\displaystyle\\frac{1}{k}\\Big|\\big(\\psi_{0}(x)\\big)(i)-\\big(\\psi_{0}(x)\\big)(j)\\Big|}&{}\\\\ {+\\left|\\big(\\ell_{k}(x)\\big)(i)-\\big(\\ell_{k}(x)\\big)(j)\\right|}&{}\\\\ {\\le2|k-k^{\\prime}|.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Now, we know that because $x~\\in~C_{\\delta}$ , then $\\langle f_{k,1}^{*}(x),\\psi_{1}(x)\\rangle~=~\\big(\\psi_{1}(x)\\big)(i)$ for $i\\ \\in$ $\\underset{j\\in\\mathcal{J}}{\\mathrm{argmax}}\\left(\\psi_{0}(x)\\right)(j)$ , and $\\langle f_{k^{\\prime},1}^{*}(x),\\psi_{1}(x)\\rangle\\;=\\;\\big(\\psi_{1}(x)\\big)(j)$ for $j\\;\\in\\;\\underset{k\\in\\mathcal{Z}}{\\mathrm{argmax}}\\left(\\psi_{0}(x)\\right)(j)$ ", "page_idx": 35}, {"type": "text", "text": "Hence, we can see that $i\\ \\ \\in\\ {\\mathcal{I}}\\ \\ \\subsetneq\\ {\\mathcal{Z}}$ and $\\textit{j}\\in\\textit{\\mathcal{T}}$ \uff0c and because $\\begin{array}{r l}{\\bigl(\\psi_{0}(x)\\bigr)(i)}&{{}=}\\end{array}$ $\\langle f_{k,1}^{*}(x),\\psi_{0}(x)\\rangle=\\langle f_{k^{\\prime},1}^{*}(x),\\psi_{0}(x)\\rangle=\\bigl(\\psi_{0}(x)\\bigr)(j).$ . and due to (44) we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left|\\langle f_{k,1}^{*}(x),\\psi_{1}(x)\\rangle-\\langle f_{k^{\\prime},1}^{*}(x),\\psi_{1}(x)\\rangle\\right|\\leq2|k-k^{\\prime}|,\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "as long as $k^{\\prime}\\in[k-\\delta/2,k)$ . Therefore, if we set $\\delta=\\operatorname*{max}\\{\\delta^{\\prime},\\epsilon/2\\}$ we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left|\\langle f_{k,1}^{*}(x),\\psi_{1}(x)\\rangle-\\langle f_{k^{\\prime},1}^{*}(x),\\psi_{1}(x)\\rangle\\right|\\leq\\epsilon/2,\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and therefore ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathbb{E}\\big[\\langle f_{k^{\\prime},1}^{*}(x),\\psi_{1}(x)\\rangle|x\\in C_{\\delta}\\big]-\\mathbb{E}\\big[\\langle f_{k,1}^{*}(x),\\psi_{1}(x)\\rangle|x\\in C_{\\delta}\\big]\\right|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\mathbb{E}\\Big[\\|\\langle f_{k,1}^{*}(x),\\psi_{0}(x)\\rangle-\\langle f_{k^{\\prime},1}^{*}(x),\\psi_{0}(x)\\rangle\\|\\Big]\\leq\\epsilon/2}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Finally, we can rewrite $D(k^{\\prime})$ as ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D(k^{\\prime})=\\mathbb{E}\\big[\\langle f_{k^{\\prime},1}^{*}(x),\\psi_{0}(x)\\rangle\\big]=\\!\\mathbb{E}\\big[\\langle f_{k^{\\prime},1}^{*}(x),\\psi_{0}(x)\\rangle|x\\in A_{k}\\big]\\operatorname*{Pr}(x\\in A_{k})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\mathbb{E}\\big[\\langle f_{k^{\\prime},1}^{*}(x),\\psi_{0}(x)\\rangle|x\\in B_{k}\\setminus C_{\\delta}\\big]\\operatorname*{Pr}(x\\in B_{k}\\setminus C_{\\delta})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\mathbb{E}\\big[\\langle f_{k^{\\prime},1}^{*}(x),\\psi_{0}(x)\\rangle|x\\in C_{\\delta}\\big]\\operatorname*{Pr}(x\\in C_{\\delta}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "and because of (42) and (45), and since the first term is a constant function in terms of $k^{\\prime}$ and for all $k^{\\prime}\\in[k-\\delta/2,k]$ ,thenwehave ", "page_idx": 36}, {"type": "equation", "text": "$$\n|D(k^{\\prime})-D(k)|\\leq\\epsilon/4+\\epsilon/4+\\epsilon/2=\\epsilon.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "This shows that $D(k^{\\prime})$ is lower semi-continuous around $k^{\\prime}=k$ ", "page_idx": 36}, {"type": "text", "text": "5. To prove this part, we first divide $\\mathcal{X}$ into two subsets ", "page_idx": 36}, {"type": "equation", "text": "$$\nG_{k^{\\prime}}=\\Big\\{x\\in\\mathcal{X}:\\,\\vert\\operatorname{argmax}_{i}(\\ell_{k^{\\prime}}(x))(i)\\vert=1\\Big\\},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "and $H_{k^{\\prime}}=\\boldsymbol{\\mathcal{X}}\\setminus G_{k^{\\prime}}$ . We know that for $x\\in G_{k^{\\prime}}$ we have ", "page_idx": 36}, {"type": "equation", "text": "$$\nf_{k^{\\prime},0}^{*}(x)=f_{k^{\\prime},1}^{*}(x)={\\left\\{\\begin{array}{l l}{1}&{i=\\operatorname*{min}\\{j\\in\\arg\\operatorname*{max}\\ell_{k^{\\prime}}(x)\\}}\\\\ {0}&{{\\mathrm{otherwise}}}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "This concludes that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[\\langle f_{k^{\\prime},0}^{*}(x),\\psi_{1}(x)\\rangle|\\,x\\in G_{k}\\big]=\\mathbb{E}\\big[\\langle f_{k^{\\prime},1}^{*}(x),\\psi_{1}(x)\\rangle|\\,x\\in G_{k}\\big].\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Moreover, let us  define  the  set $\\begin{array}{r c c c c c c c c l}{\\Psi_{1}^{k^{\\prime}}}&{=}&{\\{x}&{\\in}&{\\mathcal{X}}&{:}&{\\exists c}&{\\in}&{\\mathbb{R},\\forall j}&{\\in}\\end{array}$ argmax $\\ell_{k^{\\prime}}(x)$ \uff0c $\\bigl(\\psi_{1}(x)\\bigr)(j)=c\\bigr\\}$ . We show that sum of the probabilities of $H_{k^{\\prime}}\\setminus\\Psi_{1}^{k^{\\prime}}$ is always bounded by $2^{d}$ for a set of choices for $k^{\\prime}$ , or equivalently ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sum_{k^{\\prime}\\in K}\\operatorname*{Pr}(x\\in H_{k^{\\prime}}\\setminus\\Psi_{1}^{k^{\\prime}})\\leq2^{d},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "for all finite or countably infinite choice of $\\mathcal{K}\\subseteq\\mathbb{R}^{+}$ . In fact, we know that for each instance $x$ ,argmax $(\\ell_{k}(x))(j)$ can take up to $2^{d}$ cases of all subsets of $\\{1,\\ldots,d\\}$ . Therefore, we $j\\!\\in\\![1;\\!d]$ ", "page_idx": 36}, {"type": "text", "text": "need to show that there cannot exist two values of $k,k^{\\prime}$ such that for $x\\,\\in\\,\\left(H_{k}\\,\\setminus\\Psi_{1}^{k}\\right)\\cap$ $\\left(H_{k^{\\prime}}\\setminus\\Psi_{1}^{k^{\\prime}}\\right)$ wehave ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\underset{j}{\\mathop{\\mathrm{argmax}}}\\left(\\ell_{k}(x)\\right)(j)=\\underset{j}{\\mathop{\\mathrm{argmax}}}\\left(\\ell_{k^{\\prime}}(x)\\right)(j).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "If we prove such identity, then due to pigeonhole principle, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sum_{k^{\\prime}\\in K}\\mathbb{1}_{x\\in H_{k^{\\prime}}\\backslash\\Psi_{1}^{k^{\\prime}}}\\leq2^{d},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "which by integration over all values of $x$ concludes in (50). We prove this claim by contradiction. If we assume $k,k^{\\prime}\\in\\mathcal{K}$ such that for $x\\,\\in\\,\\big(H_{k}\\,\\backslash\\,\\Psi_{1}^{k}\\big)\\cap\\big(H_{k^{\\prime}}\\,\\backslash\\,\\Psi_{1}^{k^{\\prime}}\\big)$ the identity (51) holds, then because $x\\in H_{k}\\cap H_{k^{\\prime}}$ , then the size of argmax $(\\ell_{k}(x))(j)$ and ", "page_idx": 36}, {"type": "text", "text": "argmax $\\big(\\ell_{k^{\\prime}}(x)\\big)(j)$ is at least 2. This concludes that ", "page_idx": 36}, {"type": "text", "text": ") ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\bigl(\\psi_{0}(x)-k\\psi_{1}(x)\\bigr)(i)=\\bigl(\\psi_{0}(x)-k\\psi_{1}(x)\\bigr)(j)\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "as well as ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\left(\\psi_{0}(x)-k^{\\prime}\\psi_{1}(x)\\right)(i)=\\left(\\psi_{0}(x)-k^{\\prime}\\psi_{1}(x)\\right)(j)\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "for all choices of $i,j\\in\\mathrm{argmax}\\,\\ell_{k}(x)$ . As a result, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n(k-k^{\\prime}){\\Big(}{\\big(}\\psi_{1}(x){\\big)}(i)-\\psi_{1}(x){\\big)}(j){\\Big)}=0,\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "and because $k^{\\prime}\\neq k$ , we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\bigl(\\psi_{1}(x)\\bigr)(i)=\\psi_{1}(x)\\bigr)(j),\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "for all $i,j\\in\\mathrm{argmax}\\,\\ell_{k}(x)$ . Therefore, $x\\in\\Psi_{1}^{k^{\\prime}}$ and that is a contradiction. Now that we know that the sum of the probabilities of $\\mathrm{Pr}(x\\in H_{k^{\\prime}}\\setminus\\Psi_{1}^{k^{\\prime}})$ is bounded, we can renormalize that and make a probability measure as ", "page_idx": 37}, {"type": "equation", "text": "$$\ng(A)=\\frac{\\sum_{k\\in A,\\operatorname*{Pr}(x\\in H_{k}\\backslash\\Psi_{1}^{k})>0}\\operatorname*{Pr}(x\\in H_{k}\\setminus\\Psi_{1}^{k})}{\\sum_{k:\\operatorname*{Pr}(x\\in H_{k}\\backslash\\Psi_{1}^{k})>0}\\operatorname*{Pr}(x\\in H_{k}\\backslash\\Psi_{1}^{k})}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Due to Lemma J.1, for all $\\epsilon\\geq0$ we can find a small enough $\\delta\\geq0$ such that $g([k-\\delta,k))\\leq$ $\\epsilon/2^{d+1}$ , and therefore for all $k^{\\prime}\\in[k-\\delta,k)$ wehave ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}(x\\in H_{k^{\\prime}}\\setminus\\Psi_{1}^{k^{\\prime}})\\leq\\underset{t\\in[k-\\delta,k),\\operatorname*{Pr}(x\\in H_{t}\\setminus\\Psi_{1}^{t})>0]}{\\sum}\\operatorname*{Pr}(x\\in H_{t}\\setminus\\Psi_{1}^{t})}\\\\ &{\\qquad\\qquad\\qquad\\qquad=g\\big([k-\\delta,k)\\big)\\underset{k:\\operatorname*{Pr}(x\\in H_{k}\\setminus\\Psi_{1}^{k})>0}{\\sum}\\operatorname*{Pr}(x\\in H_{k}\\setminus\\Psi_{1}^{k})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\frac{\\epsilon}{2^{d+1}}2^{d}=\\epsilon/2,}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where the last inequality holds because of (50)) ", "page_idx": 37}, {"type": "text", "text": "Now, using this and due to (49), and by defning $g_{i}(x)=\\langle f_{k,i}^{*}(x),\\psi_{0}(x)\\rangle$ for $i=1,2$ we can bound the difference of $D(k)$ and $C(k)$ as ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|D(k)-C(k)\\right|=\\left|\\mathbb{E}\\big[g_{1}(x)-g_{0}(x)\\big|\\,x\\in H_{k^{\\prime}}\\big]\\operatorname*{Pr}(x\\in H_{k^{\\prime}})\\right|}\\\\ &{\\qquad\\qquad\\leq\\operatorname*{Pr}(x\\in H_{k^{\\prime}}\\setminus\\Psi_{1}^{k^{\\prime}}|x\\in H_{k^{\\prime}})\\bigg|\\mathbb{E}\\big[g_{1}(x)-g_{0}(x)\\big|\\,x\\in H_{k^{\\prime}}\\setminus\\Psi_{1}^{k^{\\prime}}\\big]\\bigg|}\\\\ &{\\qquad\\qquad\\qquad+\\operatorname*{Pr}(x\\in H_{k^{\\prime}}\\cap\\Psi_{1}^{k^{\\prime}}|x\\in H_{k^{\\prime}})\\bigg|\\mathbb{E}\\big[g_{1}(x)-g_{0}(x)|\\,x\\in H_{k^{\\prime}}\\cap\\Psi_{1}^{k^{\\prime}}\\big]\\bigg|}\\\\ &{\\qquad\\qquad\\stackrel{(a)}{\\leq}2(\\epsilon/2)+\\left|\\mathbb{E}\\big[g_{1}(x)-g_{0}(x)\\big|\\,x\\in H_{k^{\\prime}}\\cap\\Psi_{1}^{k^{\\prime}}\\big]\\right|}\\\\ &{\\qquad\\qquad\\overset{(b)}{=}\\epsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $(a)$ holds because $\\|f_{k,0}^{*}-f_{k,1}^{*}\\|_{1}\\leq\\|f_{k,0}^{*}\\|_{1}+\\|f_{k,1}^{*}\\|_{1}=2$ and because of Holder inequality we have $\\left|\\langle f_{k,0}^{*}(x)-f_{k,1}^{*},\\psi_{1}(x)\\rangle\\right|\\leq\\|f_{k,0}^{*}-f_{k,1}^{*}\\|_{1}\\|\\psi_{1}(x)\\|_{\\infty}\\leq2$ . Moreover, to show that $(b)$ holds we know that for $x\\,\\in\\,\\Psi_{1}^{k^{\\prime}}$ we have $\\bigl(\\psi_{1}(x)\\bigr)(i)\\;=\\;\\bigl(\\psi_{1}(x)\\bigr)(j)$ for all $i,j\\ \\in$ argmax $\\ell_{k^{\\prime}}(x)$ . Therefore, because we know $g_{0}(x)\\,=\\,\\bigl(\\psi_{1}(x)\\bigr)(i)$ for $i\\in$ argmin $\\bigl(\\psi_{1}(x)\\bigr)(j)\\,\\subseteq\\,\\underset{l}{\\mathrm{argmax}}\\,\\bigl(\\ell_{k^{\\prime}}(x)\\bigr)(l)$ and $g_{1}(x)\\,=\\,\\bigl(\\psi_{1}(x)\\bigr)(j)$ for $j~\\in$ jEargmax (ek(x))(l) ", "page_idx": 37}, {"type": "text", "text": "argmax (o(x)(i) \u2264 argmax (lk(x))(l), we have $g_{0}(x)=g_{1}(x)$ . The above jEargmax (ek(x))(l) ", "page_idx": 37}, {"type": "text", "text": "inequality proves that the limit of $C(k^{\\prime})$ and $D(k^{\\prime})$ for $k^{\\prime}\\uparrow k$ are equal and that completes the proof. ", "page_idx": 37}, {"type": "text", "text": "To prove this theorem, we take the following steps: (i) We show that the set $\\kappa$ has a non-negative member, (i) we show that the prediction function $f_{k,p}^{*}(x)$ achieves the inequality constraint tightly, and by Theorem 4.1 we can conclude that $f_{k,p}^{*}(x)$ is the optimal solution. ", "page_idx": 37}, {"type": "text", "text": "\u00b7 step (i): It is easy to see that the Bayes optimal solution of the prediction function in (3) without any constraint is ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\left(f^{*}(x)\\right)(i)=\\left\\{\\begin{array}{c c}{{1}}&{{\\left(\\psi_{0}(x)\\right)(i)>\\left(\\psi_{0}(x)\\right)(j)\\;\\mathrm{for}\\;\\mathrm{all}\\;j\\neq i}}\\\\ {{0}}&{{\\left(\\psi_{0}(x)\\right)(i)<\\operatorname*{max}_{j}\\,\\left(\\psi_{0}(x)\\right)(j)}}\\\\ {{p_{i}(x)}}&{{\\mathrm{otherwise}}}\\end{array}\\right.,\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $p_{i}(x)\\in\\Delta_{d}$ is an arbitrary vector. We can see that by setting ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\bigl(p_{i}(x)\\bigr)(j)=\\left\\{\\begin{array}{c l}{{1}}&{{j=\\operatorname*{min}\\bigl\\{\\begin{array}{c}{{\\mathrm{argmin}}}\\\\ {{t\\mathrm{{eargmax}}\\,\\ell_{0}(x)}}\\end{array}\\bigr(\\psi_{1}(x)\\bigr)(t)\\bigr\\}}}\\\\ {{0}}&{{\\mathrm{otherwise}}}\\end{array}\\right.,\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "then the two prediction functions $f^{*}(x)$ and $f_{0,0}^{*}(x)$ are equal (See statement of Theorem 4.2). ", "page_idx": 38}, {"type": "text", "text": "Now, in the first and second part of Lemma J.2 we have shown that $\\mathbb{E}\\big[\\langle f_{k,0}^{*}(x),\\psi_{1}(x)\\rangle\\big]$ is upper semi-continuous and monotonically non-increasing. Therefore, for all $k\\in\\mathbb{R}^{+}$ we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\big[\\langle f_{k,0}^{*}(x),\\psi_{1}(x)\\rangle\\big]\\leq\\mathbb{E}\\big[\\langle f_{0,0}^{*}(x),\\psi_{1}(x)\\rangle\\big]=\\mathbb{E}\\big[\\langle f^{*}(x),\\psi_{1}(x)\\rangle\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Similarly, we can show that for $k\\rightarrow\\infty$ , the solution is equivalent to the Bayes minimizer of ", "page_idx": 38}, {"type": "equation", "text": "$$\nf^{**}(x)=\\underset{f\\in\\Delta_{d}^{x}}{\\operatorname{argmin}}\\,\\mathbb{E}\\big[\\langle f(x),\\psi_{1}(x)\\rangle\\big].\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Therefore, since $\\delta$ is an interior point of all possible values, it lays on the interval $\\left(\\mathbb{E}\\big[\\langle f^{**}(x),\\psi_{1}(x)\\rangle\\big],\\mathbb{E}\\big[\\langle f^{*}(x),\\mathbf{\\dot{\\psi}}\\rangle_{1}(x)\\rangle\\big]\\right)$ , due to the montonicity and upper semicontinuity of $\\mathbb{E}\\big[\\langle f_{k,0}^{*},\\psi_{1}(x)\\rangle\\big]$ , we can find $t$ such that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\langle f_{t,0}^{*}(x),\\psi_{1}(x)\\rangle\\right]\\leq\\delta\\leq\\underset{\\tau\\uparrow t}{\\operatorname*{lim}}\\mathbb{E}\\left[\\langle f_{k,0}^{*}(x),\\psi_{1}(x)\\rangle\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Moreover, this $t$ should be a positive scalar, since otherwise we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[\\langle f_{t,0}^{*}(x),\\psi_{1}(x)\\rangle\\big]\\ge\\mathbb{E}\\big[\\langle f_{0,0}^{*}(x),\\psi_{1}(x)\\rangle\\big]=\\mathbb{E}\\big[\\langle f^{*}(x),\\psi_{1}(x)\\rangle\\big]>\\delta,\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "which is a contradiction to (54). ", "page_idx": 38}, {"type": "text", "text": "step (ll): in tnis step, we consiaer tne 1ollowing two cases: \u4e00 $C(t)$ is continuous at $\\pmb{t}$ :In this case,(54) is equivalent to $\\delta\\;\\;=\\;\\;C(t)\\;\\;=\\;\\;$ $\\mathbb{E}\\big[\\langle f_{t,0}^{*}(x),\\psi_{0}(x)\\rangle\\big]$ , which means that the prediction function $f_{k,0}^{*}(x)$ achieves the constraint tightly, and therefore using Theorem 4.1 $f_{k,0}^{*}(x)$ is the optimal solution. ", "page_idx": 38}, {"type": "text", "text": "$\\mathbf{\\mu}-\\mathbf{\\nabla}C(t)$ is discontinuous at $\\pmb{t}$ : To show that we can achieve the highest constraint in this case, we first condition the constraint into two events $x\\in G_{k}$ and $x\\in\\mathcal{X}\\setminus G_{k}$ where $G_{k}$ is defined in (47). We know that in the latter case $x\\in\\mathcal{X}\\setminus G_{k}$ , the prediction function $f_{k,p}^{*}$ can be decomposed into two components ", "page_idx": 38}, {"type": "equation", "text": "$$\nf_{k,p}^{*}(x)=p f_{k,1}^{*}(x)+(1-p)f_{k,0}^{*}(x),\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "while for $x\\in G_{k}$ the prediction function $f_{k,p}^{*}(x)=f_{k,0}^{*}(x)=f_{k,1}^{*}(x)$ for all $p\\in[0,1]$ Therefore, in both cases (55) holds, and we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\big[\\langle f_{k,p}^{*}(x),\\psi_{1}(x)\\rangle\\big]=\\mathbb{E}\\big[\\langle p f_{k,1}^{*}(x)+(1-p)f_{k,0}^{*}(x),\\psi_{1}(x)\\rangle\\big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=p\\mathbb{E}\\big[\\langle f_{k,1}^{*}(x),\\psi_{1}(x)\\rangle\\big]+(1-p)\\mathbb{E}\\big[\\langle f_{k,0}^{*}(x),\\psi_{1}(x)\\rangle\\big]}\\\\ &{\\qquad\\qquad\\qquad\\quad=p D(k)+(1-p)C(k),}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $C(\\cdot)$ and $D(\\cdot)$ are defined in Lemma J.2. Using this lemma, we know that $D(\\cdot)$ is lower semi-continuous, and $\\begin{array}{r}{\\operatorname*{lim}_{k^{\\prime}\\uparrow k}C(k)=\\operatorname*{lim}_{k^{\\prime}\\uparrow k}D(k)}\\end{array}$ . Therefore, together with (56) and the definition of $p$ in the statement of theorem, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}\\big[\\langle f_{k,p}^{*}(x),\\psi_{0}(x)\\rangle\\big]=\\!p\\operatorname*{lim}_{k^{\\prime}\\uparrow k}C(k^{\\prime})+(1-p)C(k)\\,}}\\\\ &{}&{\\quad=\\!\\!\\frac{C(k)-c}{C(k)-\\operatorname*{lim}_{k^{\\prime}\\uparrow k}C(k^{\\prime})}\\operatorname*{lim}_{k^{\\prime}\\uparrow k}C(k^{\\prime})\\,}\\\\ &{}&{\\quad\\quad+\\,\\frac{c-\\operatorname*{lim}_{k^{\\prime}\\uparrow k}C(k^{\\prime})}{C(k)-\\operatorname*{lim}_{k^{\\prime}\\uparrow k}C(k^{\\prime})}C(k)=c.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Equivalently, the prediction function achieves the constraint inequality tightly, and therefore by Theorem 4.1 this is sufficient to be the optimal solution to the constrained optimization problem. ", "page_idx": 38}, {"type": "text", "text": "K  Proof of Theorem 5.1 ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Through the proof of this theorem, we use [6, Lemma 3.2.3] that implies that the class of multiplicationsof $k$ binaryfunctions $f_{i}(x)$ for $i\\in[1:k]$ within a hypothesis class with VC dimension $\\bar{V}C(f_{i})=d$ itself has a VC dimension that is bounded as ", "page_idx": 39}, {"type": "equation", "text": "$$\nV C(\\big\\{\\prod_{i=1}^{k}f_{i}:f_{i}\\in\\mathcal{H}_{i},V C(\\mathcal{H}_{i})=d\\big\\})\\leq2d k\\log3k.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "In fact, we use a simple extension to this lemma for which the VC dimension of the functions is not $d$ itself but is bounded above by $d$ . In such case we claim that (58) still holds. The starting point for the proof to this lemma is bounding the size of the restriction $\\Pi_{\\mathcal{H}}(S)=|\\{h\\cap S:h\\in\\mathcal{H}\\}|$ forthe hypothesis class $\\mathcal{H}$ by ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\Pi_{\\mathcal{H}}(S)\\leq\\big(\\frac{e m}{d}\\big)^{d},\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where $V C(\\mathcal{H})=d$ and $m=|S|$ . However, this inequality holds for the hypothesis classes that have VC dimensions that are bounded by $d$ . The reason is increasingly monotonicity of RHS of (59). In fact, by obtaining the gradient of $\\left(\\frac{e m}{d}\\right)^{d}$ in terms of $d$ we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\frac{\\partial\\big(\\frac{e m}{d}\\big)^{d}}{\\partial d}=\\frac{\\partial\\big(e^{d\\log e m/d}\\big)}{\\partial d}=\\big(\\log e m/d-1\\big)\\big(\\frac{e m}{d}\\big)^{d},\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "which is nonnegative as long as $m\\geq d$ . If we particularly set $m^{*}=2d k\\log3k$ , then $m^{*}\\geq d$ and therefore (59) holds. Next, similar to the proof of [6, Lemma 3.2.3], we can show that for the set $S$ with size $m^{*}$ we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\Pi_{\\mathcal{H}^{\\prime}}(S)\\leq\\Pi_{\\mathcal{H}_{1}}^{k}(S)\\leq(\\frac{e m^{*}}{d})^{d k}\\leq2^{m^{*}},\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "which means that $S$ cannot be shattered by $\\mathcal{H}^{\\prime}$ , and therefore the VC dimension of this hypothesis class must be bounded by. $m^{*}$ ", "page_idx": 39}, {"type": "text", "text": "We further use the following lemma: ", "page_idx": 39}, {"type": "text", "text": "Lemma K.1. For arbitrary sets of functions $\\{\\phi_{1}^{i}(x)\\}_{i=1}^{n}$ and $\\{\\phi_{2}^{i}(x)\\}_{i=1}^{n}$ on $\\mathbb{R}$ and for a given $d\\in\\mathbb R$ thehypothesis class ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathcal{H}=\\big\\{\\prod_{i=1}^{n}\\operatorname{sgn}\\bigl(\\phi_{1}^{i}(x)-k\\phi_{2}^{i}(x)-d\\bigr)\\,:\\,k\\in\\mathbb{R}\\big\\},\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "has theVC dimension of at most 4. ", "page_idx": 39}, {"type": "text", "text": "Proof. To prove this lemma, we show that the form of the product in the definition of $\\mathcal{H}$ reduces to the form of an interval on $\\mathbb{R}$ , which is known to have VC dimension of 2. In fact, each term $\\mathrm{sgn}(\\phi_{1}^{i}(x)-k\\phi_{2}^{i}(x)-d)$ can be rewritten as ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{sgn}(\\phi_{1}^{i}(x)-k\\phi_{2}^{i}(x)-d)=\\!\\mathrm{sgn}(\\frac{\\phi_{1}^{i}(x)-d}{\\phi_{2}^{i}(x)}-k)\\mathrm{sgn}(\\phi_{2}^{i}(x))+\\mathrm{sgn}(k-\\frac{\\phi_{1}^{i}(x)-d}{\\phi_{2}^{i}(x)})\\mathrm{sgn}(-\\phi_{2}^{i}(x))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\mathrm{\\sgn}(\\phi_{1}^{i}(x)-d)\\mathbb{I}_{\\phi_{2}^{i}(x)=0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "As a result, by multiplying all terms we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\prod_{i=1}^{n}\\mathrm{sgn}\\big(\\phi_{1}^{i}(x)-k\\phi_{2}^{i}(x)-d\\big)=\\mathrm{sgn}(\\operatorname*{min}_{i\\in A_{x}}\\frac{\\phi_{1}^{i}(x)-d}{\\phi_{2}(x)}-k)\\mathrm{sgn}(k-\\operatorname*{max}_{i\\in B_{x}}\\frac{\\phi_{1}^{i}(x)-d}{\\phi_{2}(x)})\\prod_{i\\in C_{x}}\\mathrm{sgn}(\\phi_{1}^{i}(x)-\\phi_{2}^{i}(x))\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where $A_{x},B_{x}$ , and $\\mathcal{C}_{x}$ are defined as $\\mathcal{A}_{x}=\\{i\\in[1:n]\\,:\\,\\phi_{2}^{i}(x)>0\\}$ $\\mathcal{B}_{x}=\\{i\\in[1:n]\\,:\\,\\phi_{2}^{i}(x)<\\,$ $0\\}$ , and $\\mathcal{C}_{x}=\\{i\\in[1:n]\\,:\\,\\phi_{2}^{i}(x)=0\\}$ . Now, we see that the first two terms define an interval for $k\\;\\in\\;\\big(f_{1}(x),f_{2}(x)\\big)$ where $\\begin{array}{r}{f_{1}(x)\\,=\\,\\operatorname*{max}_{i\\in\\mathcal{B}_{x}}\\frac{\\phi_{1}^{i}(x)-d}{\\phi_{2}^{i}(x)}}\\end{array}$ $\\begin{array}{r}{f_{2}(x)\\,=\\,\\operatorname*{min}_{i\\in\\mathcal A_{x}}\\,\\frac{\\phi_{1}^{i}(x)-d}{\\phi_{2}^{i}(x)}}\\end{array}$ . Next, we prove that the VC dimension of the hypothesis class of all such functions is less than the VC dimension of $\\mathcal{G}=\\big\\{f:\\mathbb{R}\\times\\mathbb{R}\\to\\{0,1\\}\\,:\\,f(x,y)=\\mathrm{sgn}(x-k_{1})\\mathrm{sgn}(k_{2}-y),\\,k_{1},k_{2}\\in\\mathbb{R}\\big\\}$ . The reason is that if the aforementioned interval can shatter a set $\\boldsymbol{S}$ , then we can find the corresponding values of $f_{1}(x)$ and $f_{2}(x)$ for each $x\\in S$ , and then form the pair $(x_{i},y_{i})$ where $x_{i}=f_{1}(x)$ and $y_{i}\\,=\\,f_{2}(x)$ , and by setting $k_{1}\\,=\\,k_{2}\\,=\\,k$ , we can shatter the set $\\{(x_{i},y_{i})\\}_{i=1}^{|S|}$ with $\\mathcal{G}$ . Note that here all pairs are identical. The reason is that if not, i.e., if $f_{1}(x)\\,=\\,f_{1}(x^{\\prime})$ and $f_{2}(x)\\,=\\,f_{2}(x^{\\prime})$ for $x,x^{\\prime}\\,\\in\\,{\\mathcal{S}}$ and $x\\ne x^{\\prime}$ , then, for all possible $k$ ,we have $\\mathrm{sgn}(k\\mathrm{~-~}f_{1}(x))\\mathrm{sgn}(f_{2}(x)\\mathrm{~-~}k)\\mathrm{~=~}$ $\\operatorname{sgn}(k-f_{1}(x^{\\prime}))\\mathrm{sgn}(f_{2}(x^{\\prime})-k)$ , and therefore we cannot shatter $\\boldsymbol{S}$ by $\\operatorname{sgn}(k-f_{1}(x))\\operatorname{sgn}(f_{2}(x)\\,-\\,k)$ Therefore,the set $\\{(x_{i},y_{i})\\}_{i=1}^{|S|}$ has the same cardinality of $\\boldsymbol{S}$ wWhichi consequence proves tha the VC dimension of all $\\mathrm{sgn}(\\stackrel{...}{k}-\\stackrel{..}{f}_{1}(x))\\mathrm{sgn}(f_{2}(x)-k)$ is bounded by $V C(\\mathcal{G})$ . Moreover, $V C(\\mathcal{G})\\leq4$ since for each 5 points in two-dimensional space, one is in the convex hull of the others, and in case that all others are labeled as 1, the one in the convex hull also must be labeled as 1. As a result, $\\mathcal{G}$ cannot shatter 5points, and therefore $V C(\\mathcal{G})\\leq4$ ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "Up to now, we have shown that the class of functions equal to the first two terms of (60) has a VC dimension that is bounded by 4. Next, we show that multiplying a hypothesis class $\\mathcal{H}$ with abinary function $\\phi(x)$ does not increase the VC dimension of that class. More formally, if we define ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathcal{H}=\\{\\phi(\\boldsymbol{x})f(\\boldsymbol{x})\\,:\\,f\\in\\mathcal{H}^{\\prime}\\},\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "then $V C(\\mathcal{H})\\leq V C(\\mathcal{H}^{\\prime})$ . The reason is that if we can shatter a set $\\boldsymbol{S}$ using $\\mathcal{H}$ , then for each member $x\\in S$ there exists two members $f_{1},f_{2}$ of $\\mathcal{H}^{\\prime}$ such that $f_{1}(x)=1$ and $f_{2}(x)=0$ . This means that $\\phi({\\boldsymbol{x}})\\neq0$ , because otherwise $f_{1}(x)=1$ would not be achievable. Therefore, $\\phi(x)=1$ for all $x\\in S$ and as a result similarly $\\mathcal{H}^{\\prime}$ can shatter $\\boldsymbol{S}$ , which proves that $V C(\\mathcal{H})\\leq V C(\\dot{\\mathcal{H}}^{\\prime})$ ", "page_idx": 40}, {"type": "text", "text": "Finally, since we know that the class of all functions in $\\mathcal{H}$ is in form of $\\operatorname{sgn}(k-f_{1}(x))\\mathrm{sgn}(f_{2}(x)-k)$ multiplied with a binary function, then we conclude that $V C(\\mathcal{H})\\leq4$ \u53e3 ", "page_idx": 40}, {"type": "text", "text": "To prove the rest of the theorem, we need to show that for all choices of $\\hat{k}$ and $\\hat{p}$ the difference of the empirical and the true loss is bounded. In fact, we should find a bound in form of ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\Big(\\operatorname*{sup}_{k,p}\\big|\\mathbb{E}_{S^{n}}\\big[\\langle f_{k,p}^{*}(x),\\psi_{0}(x)\\rangle\\big]-\\mathbb{E}_{\\mu}\\big[\\langle f_{k,p}^{*}(x),\\psi_{0}(x)\\rangle\\big]\\big|\\leq d_{n}\\Big)\\geq1-\\epsilon.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Here, we divide the class $\\mathcal{X}$ into two subsets $G_{k}$ and $H_{k}=\\mathcal{X}\\setminus G_{k}$ , where $G_{k}$ is defined in (47). Now, using the definition of $f_{k,p}^{*}(x)$ , we know that within $G_{k}$ , the inner-product $\\langle f_{k,p}^{*}(x),\\psi_{1}(x)\\rangle$ can be rewritten as ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\langle f_{k,p}^{*}(x),\\psi_{1}(x)\\rangle=\\Big(\\psi_{1}(x)\\Big)(\\mathrm{argmax}\\left(\\ell_{k}(x)\\right)(i))}\\\\ &{\\displaystyle=\\sum_{j=1}^{d}\\Big(\\psi_{1}(x)\\big)(j)\\displaystyle\\prod_{i\\neq j}\\mathrm{sgn}\\Big(\\big(\\ell_{k}(x)\\big)(j)-\\big(\\ell_{k}(x)\\big)(i)\\Big)}\\\\ &{\\displaystyle=\\sum_{j=1}^{d}\\Big(\\psi_{1}(x)\\big)(j)\\displaystyle\\prod_{i\\neq j}\\mathrm{sgn}\\Big(\\big(\\psi_{0}(x)\\big)(j)-\\big(\\psi_{0}(x)\\big)(0)-k\\big[\\big(\\psi_{1}(x)\\big)(j)-\\big(\\psi_{1}(x)\\big)(i)\\big]\\Big)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Now, we can condition $x$ on being a member of $G_{k}$ , and therefore the maximum difference between the two empirical and true expectation is as ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\operatorname*{sup}_{k,p}\\Big|\\mathbb{E}_{S^{n}}\\left[\\langle f_{k,p}^{*}(x),\\psi_{1}(x)\\rangle\\,|\\,x\\in G_{k}\\right]-\\mathbb{E}_{\\mu}\\left[\\langle f_{k,p}^{*}(x),\\psi_{1}(x)\\rangle\\,|\\,x\\in G_{k}\\right]\\Big|}\\\\ {\\displaystyle\\leq\\sum_{j=1}^{d}\\operatorname*{sup}_{k,p}\\Big|\\mathbb{E}_{S^{n}}\\left[\\big(\\psi_{1}(x)\\big)(j)\\cdot\\Phi_{j}^{k}(x)\\,|\\,x\\in G_{k}\\right]-\\mathbb{E}_{\\mu}\\left[\\big(\\psi_{1}(x)\\big)(j)\\cdot\\Phi_{j}^{k}(x)\\,|\\,x\\in G_{k}\\right]\\Big|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Now, we bound the inner term of (61) in a high probability setting. To that end, we use Rademacher's inequality in [66, Theorem 26.5], which shows that maximum difference between the expected value of a function $h\\in\\mathcal H$ over empirical distribution and the true distribution is $2R(\\mathcal{H})+4c\\sqrt{\\frac{\\ln{4/\\epsilon}}{n}}$ /Im 4/e where $R(\\mathcal{H})$ is the Rademacher's complexity of the class of function $\\mathcal{H}$ and $c$ is maximum value that $h$ can take. By defining ", "page_idx": 40}, {"type": "equation", "text": "$$\nh(x):=\\big(\\psi_{1}(x)\\big)(j)\\cdot\\Phi_{j}^{k}(x),\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "we have $c=\\|\\big(\\psi_{1}(x)\\big)(j)\\|_{\\infty}\\leq1$ . Therefore, we have for all $h$ ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{h\\in\\mathcal{H}}\\mathbb{E}_{S^{n}}\\left[h(x)\\right]-\\mathbb{E}_{\\mu}\\left[h(x)\\right]\\leq2R(\\mathcal{H})+4\\sqrt{\\frac{\\ln4d/\\epsilon}{n}},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "with probability at least $1\\textrm{--}\\frac{\\epsilon}{d}$ . Now, we can use contraction Lemma [66, Lemma 26.9] to show that since $\\|\\bigl(\\psi_{1}(x)\\bigr)(j)\\|_{\\infty}\\leq\\hat{1}$ ,then $R({\\mathcal{H}})\\leq R({\\mathcal{F}})$ where $\\mathcal{F}=\\{\\Phi_{j}^{k}(x),k\\in\\mathbb{R}\\}$ . Moreover, $\\mathcal{F}$ contains functions that are all multiplication of $d-1$ binary functions all in form of ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathrm{sgn}\\Big(\\big(\\psi_{1}(x)\\big)(j)-\\big(\\psi_{1}(x)\\big)(0)-k\\big[\\big(\\psi_{0}(x)\\big)(j)-\\big(\\psi_{0}(x)\\big)(i)\\big]\\Big).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Lemma K.1 shows that the hypothesis class that contains products of all such function has a VCdimension that is bounded by 4. As a result, the Rademacher's complexity of $\\mathcal{F}$ is bounded using [47, Corollary 3.8, Corollary3.18] as ", "page_idx": 41}, {"type": "equation", "text": "$$\nR(\\mathcal{F})\\leq\\sqrt{\\frac{4\\log e n/4}{n}},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "and therefore together with (62) for all $h\\in\\mathcal H$ wehave ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbb{E}_{S^{n}}\\big[h(x)\\big]-\\mathbb{E}_{\\mu}\\big[h(x)\\big]\\leq2\\sqrt{\\frac{4\\log e n/4}{n}}+4\\sqrt{\\frac{\\ln4d/\\epsilon}{n}},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "with probability at least $\\textstyle1-{\\frac{\\epsilon}{d}}$ . Hence, using (61) we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\underset{k,p}{\\operatorname*{sup}}\\left|\\mathbb{E}_{S^{n}}\\left[\\langle f_{k,p}^{*}(x),\\psi_{1}(x)\\rangle\\:|\\:x\\in G_{k}\\right]-\\mathbb{E}_{\\mu}\\left[\\langle f_{k,p}^{*}(x),\\psi_{1}(x)\\rangle\\:|\\:x\\in G_{k}\\right]\\right|}\\\\ &{}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\le2d\\sqrt{\\frac{4\\log e l/4}{l}}+4d\\sqrt{\\frac{\\ln4d/\\epsilon}{l}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "with probability at least $1-\\epsilon$ . In the last inequality, we used Bonferroni's inequality on $\\epsilon/d$ bad events that each summand of (61) is not within the concentration bound. ", "page_idx": 41}, {"type": "text", "text": "Next, we consider the region $H_{k}$ in which there are at least two maximizer components of $\\ell_{k}(x)$ . In this case, by definition of $\\hat{f}_{k,p}(x)$ , among these maximizers, we choose the first maximizer of $\\psi_{0}(x)$ with probability $p$ and the first minimizer of $\\psi_{1}(x)$ with probability $1-p$ . Therefore, by condition on these cases, and if we define ", "page_idx": 41}, {"type": "equation", "text": "$$\nE(k,p):=\\Big|\\mathbb{E}_{S^{n}}\\big[\\langle\\hat{f}_{k,p}(x),\\psi_{1}(x)\\rangle\\,|\\,x\\in H_{k}\\big]-\\mathbb{E}_{\\mu}\\big[\\langle\\hat{f}_{k,p}(x),\\psi_{1}(x)\\rangle\\,|\\,x\\in H_{k}\\big]\\Big|,\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "then we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{k,p}E(k,p)\\leq\\operatorname*{sup}_{k,p}p E(k,1)+(1-p)E(k,0)\\leq\\operatorname*{sup}_{k,p}E(k,1)+\\operatorname*{sup}_{k,p}E(k,0).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Now, to bound $E(k,1)$ , we first rewrite the closed-form solution of $\\hat{f}_{k,1}(x)$ as ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\left(\\hat{f}_{k,1}(x)\\right)(i)=\\operatorname{sgn}\\!\\Big(\\big(\\ell_{k}(x)\\big)(i)\\geq\\operatorname*{max}_{j}\\big(\\ell_{k}(x)\\big)(j)-d\\Big)\\prod_{j<i}l_{i j}(x)\\prod_{j>i}u_{i j}(x),\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $l_{i j}(x)$ and $u_{i j}(x)$ are defined as ", "page_idx": 41}, {"type": "equation", "text": "$$\nl_{i j}(x):=1-\\mathbb{I}_{\\left(\\psi_{0}(x)\\right)(i)\\leq\\left(\\psi_{0}(x)\\right)(j)}\\mathbb{I}_{\\left(\\ell_{k}(x)\\right)(j)\\geq\\operatorname*{max}_{t}\\big(\\ell_{k}(x)\\big)(t)},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "and ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{u_{i j}(x):=1-\\mathbb{I}_{\\left(\\psi_{0}(x)\\right)(i)<\\left(\\psi_{0}(x)\\right)(j)}\\mathbb{I}_{\\left(\\ell_{k}(x)\\right)(j)\\geq\\operatorname*{max}_{t}\\big(\\ell_{k}(x)\\big)(t)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "respectively. Note that the only difference between the definition of $u_{i j}(x)$ and $l_{i j}(x)$ is that $u_{i j}(x)$ permits the equality of $\\left(\\psi_{0}(x)\\right)(i)$ with other components, while that is not the case for $l_{i j}(x)$ . This difference lets us find the first component with the largest value of $\\psi_{0}(x)$ ", "page_idx": 41}, {"type": "text", "text": "Now, we can rewrite $\\operatorname{sgn}\\Bigl(\\bigl(\\ell_{k}(x)\\bigr)(j)\\geq\\operatorname*{max}_{t}\\big(\\ell_{k}(x)\\big)(t)\\Bigr)$ as the product ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\operatorname{sgn}\\Bigl(\\bigl(\\ell_{k}(x)\\bigr)(j)\\geq\\operatorname*{max}_{t}\\big(\\ell_{k}(x)\\big)(t)-d\\Bigr):=\\prod_{l\\in[1:d]}\\operatorname{sgn}\\Bigl(\\bigl(\\ell_{k}(x)\\bigr)(j)\\geq\\bigl(\\ell_{k}(x)\\bigr)(l)\\Bigr).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "As shown in Lemma K.1, the class of such function has VC dimension of at most 4. Furthermore, multiplying a hypothesis class with a function such as $\\operatorname{sgn}\\Bigl(\\bigl(\\psi_{0}(x)\\bigr)(i)\\geq\\bigl(\\psi_{0}(x)\\bigr)(j)\\Bigr)$ and $\\operatorname{sgn}\\biggl(\\bigl(\\psi_{0}(x)\\bigr)(i)>\\bigl(\\psi_{0}(x)\\bigr)(j)\\biggr)$ does not increase the VC dimension (See proof of Lemma K.1, and neither does negation. Therefore, in RHS of (66) we can count $d$ number of functions, each with a hypothesis class with the VC dimension of at most 4, and therefore using the early discussions in this proof (58), $(\\hat{f}_{k,1}(x))(i)$ is within a function class with the VC dimension of at most $8d\\log(3d)$ Therefore, similar to (63) in previous part, we can bound $\\operatorname*{sup}_{k,p}E(k,1)$ as ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{k,p}{\\operatorname*{sup}}\\,E(k,1)\\leq2d\\sqrt{\\frac{8d\\log(3d)\\log(e n/\\big(8d\\log(3d)\\big)}{n}}}\\\\ &{\\qquad\\qquad\\qquad+\\,4d\\sqrt{\\frac{\\ln4d/\\epsilon}{n}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "for $l\\geq8d\\log(3d)$ with probability at least $1-\\epsilon$ We can similarly, show that $\\operatorname*{sup}_{k,p}E(k,0)$ is bounded as ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{k,p}{\\operatorname*{sup}}E(k,0)\\leq2d\\sqrt{\\frac{8d\\log(3d)\\log\\left(e n/\\left((8n+8)\\log(3d)\\right)\\right.}{n}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\left.4d\\sqrt{\\frac{\\ln4d/\\epsilon}{n}},\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Therefore, using (63), (64), (65), (67),(68), and the application Bonferonni's inequality we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{k,p}{\\operatorname*{sup}}\\left|\\mathbb{E}_{S^{n}}\\left[\\left\\langle f_{k,p}^{*}(x),\\psi_{0}(x)\\right\\rangle\\right]-\\mathbb{E}_{\\mu}\\left[\\left\\langle f_{k,p}^{*}(x),\\psi_{0}(x)\\right\\rangle\\right]\\right|}\\\\ &{\\qquad\\qquad\\leq6d\\sqrt{\\frac{8d\\log(3d)\\log\\frac{e l}{(8n+8)\\log(3d)}}{l}}+12d\\sqrt{\\frac{\\ln\\frac{12d}{\\epsilon}}{l}}}\\\\ &{\\qquad\\qquad:=d_{n}(\\epsilon),}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "with probability at least $1\\mathrm{~-~}\\epsilon$ Therefore, by assuming $\\mathbb{E}_{S^{n}}\\left[\\langle f_{k,p}^{*}(x),\\psi_{1}(x)\\rangle\\right]\\,\\le\\,\\alpha-d_{n}(\\epsilon)$ we assure that $\\mathbb{E}_{\\mu}\\left[\\langle f_{k,p}^{*}(x),\\psi_{1}(x)\\rangle\\right]\\leq\\alpha$ with probability at least $1-\\epsilon$ and this completes the proof ", "page_idx": 42}, {"type": "text", "text": "LProof of Theorem 5.3 ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "We first introduce three lemmas that are useful in proving this theorem. ", "page_idx": 42}, {"type": "text", "text": "Lemma L.1. If $\\delta$ is an $\\epsilon$ -interior point of the set ${\\mathcal{C}}=\\left\\{\\mathbb{E}_{\\mu}\\left[\\langle f(x),\\psi_{1}(x)\\rangle\\right]\\ :\\ f\\in\\Delta_{d}^{\\mathcal{X}}\\right\\}$ then $\\delta$ is $(\\epsilon/2)$ -interior point of $\\mathcal{D}=\\left\\{\\mathbb{E}_{S^{n}}\\left[\\langle f(x),\\psi_{1}(x)\\rangle\\right]\\,:\\,f\\in\\Delta_{d}^{\\mathcal{X}}\\right\\}$ with probability $1-2e^{-\\frac{l\\epsilon^{2}}{4}}$ ", "page_idx": 42}, {"type": "text", "text": "Proof. The proof of this lmmais adirect application of Hoeffding's inequality In fact, for $\\|\\psi_{1}\\|_{\\infty}\\leq$ $C$ that inequality together with Holder's inequality imply that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{Pr}\\Big(\\big|\\mathbb{E}_{\\mu}\\big[\\langle f(x),\\psi_{1}(x)\\rangle\\big]-\\mathbb{E}_{S^{n}}\\big[\\langle f(x),\\psi_{1}(x)\\rangle\\big]\\big|\\geq\\epsilon/2\\Big)\\leq e^{-\\frac{n\\epsilon^{2}}{4C^{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Therefore i there xis f such tha E, [(fr(2),3ga(z) = , then wit robabilty a leat - e- G2 we have $\\mathbb{E}_{S^{n}}\\left[\\langle f_{1}(x),\\psi_{1}(x)\\rangle\\right]\\in[\\epsilon/2,3\\bar{\\epsilon}/2]$ . Similarly, if $f_{2}$ exists such that $\\mathbb{E}_{\\mu}\\big[\\langle f_{1}(x),\\psi_{1}(x)\\rangle\\big]=$ $-\\epsilon$ , then with probability 1 - e- #c we have $\\mathbb{E}_{S^{n}}\\left[\\langle f_{2}(x),\\psi_{1}(x)\\rangle\\right]\\,\\in\\,[-3\\epsilon/2,-\\epsilon/2]$ . As a result of Bonferroni's inequality, with probability at least $1-2e^{-\\frac{n\\epsilon^{2}}{4C^{2}}}$ both these events happen, and because of the convexity of the set $\\mathcal{D}$ we can say that with such probability all values between $a_{0}\\in\\left[-3\\epsilon/2,-\\epsilon/2\\right]$ and $a_{1}\\in[\\epsilon/2,3\\epsilon/2]$ are in $\\mathcal{D}$ too. This, of course at least contains the interval $[-\\epsilon/2,\\epsilon/2]$ \u53e3 ", "page_idx": 42}, {"type": "text", "text": "Lemma L.2. Assume that we have an approximation $\\hat{\\psi}_{1}(x)$ of $\\psi_{1}(x)$ withtheerrorbounded as $\\lVert\\hat{\\psi}_{1}(x)-\\psi_{1}(x)\\rVert_{\\infty}\\leq\\epsilon.$ Further let $\\epsilon^{\\prime}\\in\\mathbb{R}^{+}$ such that $\\epsilon^{\\prime}\\geq\\epsilon.$ Now, if for $\\sigma\\in\\{-\\epsilon^{\\prime},\\epsilon^{\\prime}\\}$ there exists a rule $f\\in\\Delta_{d}^{\\mathcal{X}}$ such that $\\mathbb{E}_{\\mu}\\big[\\langle f(x),\\psi_{1}(x)\\rangle\\big]=\\delta+\\sigma$ then there exists $k\\in\\mathbb{R}$ as well as $p\\in[0,1]$ such that $\\begin{array}{r}{\\mathbb{E}_{\\mu}\\left[\\langle\\hat{f}_{k,p}(x),\\hat{\\psi}_{1}(x)\\rangle\\right]=\\delta+\\frac{\\epsilon^{\\prime}-\\epsilon}{2}}\\end{array}$ ", "page_idx": 42}, {"type": "text", "text": "Proof. Firstly, because of Holder's inequality we know that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\mathbb{E}_{\\mu}\\left[\\langle f(x),\\psi_{1}(x)\\rangle\\right]-\\mathbb{E}_{\\mu}\\left[\\langle f(x),\\hat{\\psi}_{1}(x)\\rangle\\right]\\right|\\leq\\epsilon\\|f_{k,p}^{*}(x)\\|_{1}=\\epsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "for all $f\\in\\Delta_{d}^{\\mathcal{X}}$ . Therefore, by setting $\\sigma=\\epsilon^{\\prime}$ and $\\sigma=-\\epsilon^{\\prime}$ , we can show that for $f_{1}\\in\\Delta_{d}^{\\mathcal{X}}$ such that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mu}\\left[\\langle f_{1}(x),\\psi_{1}(x)\\rangle\\rangle\\right]=\\delta+\\epsilon^{\\prime},\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "then ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mu}\\left[\\langle f_{1}(x),\\hat{\\psi}_{1}(x)\\rangle\\right]\\ge\\delta+\\epsilon^{\\prime}-\\epsilon,\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "and where for $f_{2}\\in\\Delta_{d}^{\\mathcal{X}}$ ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mu}\\left[\\langle f_{2}(x),\\psi_{1}(x)\\rangle\\rangle\\right]=\\delta-\\epsilon^{\\prime},\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "then ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mu}\\left[\\langle f_{2}(x),\\hat{\\psi}_{1}(x)\\rangle\\right]\\le\\delta-\\epsilon^{\\prime}+\\epsilon.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Now, because of step (ii) of the proof of Theorem 4.1, we know that the set of constraints for all rules within $\\Delta_{d}^{\\mathcal{X}}$ is convex. Therefore, since we can achieve two points $f_{1},f_{2}$ such that the constraint $\\mathbb{E}_{\\mu}\\left[\\langle f_{i}(x),\\hat{\\psi}_{1}(x)\\rangle\\right]$ can achieve two points above $\\delta+\\epsilon^{\\prime}-\\epsilon$ and below $\\delta-\\epsilon^{\\prime}+\\epsilon.$ then for each $c\\in[\\delta-\\epsilon^{\\prime}+\\epsilon,\\delta+\\epsilon^{\\prime}-\\epsilon]$ there exists $f\\in\\Delta_{d}^{\\mathcal{X}}$ such that $\\mathbb{E}_{\\mu}\\left[\\langle f(x),\\hat{\\psi}_{1}(x)\\rangle\\right]=c$ . Now, let $c=\\delta+\\frac{\\epsilon^{\\prime}\\!-\\!\\epsilon}{2}$ In the following, we show that therexists $k\\in\\mathbb{R}$ and $p\\in[0,1]$ such that further $\\mathbb{E}_{\\mu}\\left[\\langle\\hat{f}_{k,p}(x),\\hat{\\psi}_{1}(x)\\rangle\\right]=c.$ ", "page_idx": 43}, {"type": "text", "text": "To that end, we first remind that Lemma J.2 shows that $\\mathbb{E}_{\\mu}\\big[\\langle\\hat{f}_{k,0}(x),\\hat{\\psi}_{1}(x)\\rangle\\big]$ is monotonically nonincreasing in terms of $k$ We show that for $k\\in\\mathbb{R}^{-}$ we have max $\\begin{array}{r}{\\hat{\\psi}_{1}(x)-\\bar{\\langle f_{k,0}(x),\\hat{\\psi}_{1}(x)\\rangle}\\leq-\\frac{2}{k}}\\end{array}$ The reason is that if $j\\in\\underset{l}{\\mathrm{argmax}}\\left(\\hat{\\psi}_{0}(x)-k\\hat{\\psi}_{1}(x)\\right)(l)$ and $j^{\\prime}\\in\\underset{l}{\\mathrm{argmax}}\\left(\\hat{\\psi}_{1}(x)\\right)(l)$ , then we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bigl(\\hat{\\psi}_{0}(x)-k\\hat{\\psi}_{1}(x)\\bigr)(j)\\geq\\bigl(\\hat{\\psi}_{0}(x)-k\\hat{\\psi}_{1}(x)\\bigr)(j^{\\prime}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "which concludes that ", "page_idx": 43}, {"type": "equation", "text": "$$\n-k\\big[\\big(\\hat{\\psi}_{1}(x)\\big)(j)-\\big(\\hat{\\psi}_{1}(x)\\big)(j^{\\prime})\\big]\\geq\\big(\\hat{\\psi}_{0}(x)\\big)(j^{\\prime})-\\big(\\hat{\\psi}_{0}(x)\\big)(j)\\geq-2.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Therefore, since ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mu}\\big[\\langle\\operatorname{argmax}\\hat{\\psi}_{1}(x),\\hat{\\psi}_{1}(x)\\rangle\\big]=\\operatorname*{max}_{f\\in\\Delta_{d}^{\\mathcal{X}}}\\mathbb{E}_{\\mu}\\big[\\langle f(x),\\hat{\\psi}_{1}(x)\\rangle\\big]\\ge\\delta+\\epsilon^{\\prime}-\\epsilon,\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where the last inequality holds due to the existence of $f_{1}$ , then for $k\\leq-8/(\\epsilon^{\\prime}-\\epsilon)$ wehave ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mu}\\big[\\langle\\hat{f}_{k,0}(x),\\hat{\\psi}_{1}(x)\\rangle\\big]\\geq\\delta+\\epsilon^{\\prime}-\\epsilon-\\frac{2}{-8/(\\epsilon^{\\prime}-\\epsilon)}\\geq\\delta+3\\frac{\\epsilon^{\\prime}-\\epsilon}{4}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Similarly, if we let $k\\geq8/(\\epsilon^{\\prime}-\\epsilon)$ we can prove that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mu}\\big[\\langle\\hat{f}_{k,0}(x),\\hat{\\psi}_{1}(x)\\rangle\\big]\\leq\\delta-\\epsilon^{\\prime}+\\epsilon+2l\\leq\\delta-3\\frac{\\epsilon^{\\prime}-\\epsilon}{4}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "As result the set $\\mathcal{C}=\\left\\{k:\\mathbb{E}_{\\mu}\\left[\\langle\\hat{f}_{k,0}(x),\\hat{\\psi}_{1}(x)\\rangle\\right]\\ge c\\right\\}$ is nonempty and boud belowby $-\\,\\frac{8}{\\epsilon^{\\prime}-\\epsilon}$ $-\\,\\frac{8}{\\epsilon^{\\prime}-\\epsilon}$ Let us nametat infmum $\\hat{k}$ Now. $\\mathbb{E}_{\\mu}\\big[\\langle\\hat{f}_{k,0,0}(x),\\hat{\\psi}_{1}(x)\\rangle\\big]$ is continuous at $k=\\hat{k}$ , then we can show that $\\mathbb{E}_{\\mu}\\big[\\langle\\hat{f}_{\\hat{k},0}(x),\\hat{\\psi}_{1}(x)\\rangle\\big]=c$ If not, then as shown in step (i) of the proof of Theorem 4.1, and in particular in (57), there exists $p$ such that $\\mathbb{E}_{\\mu}\\left[\\langle\\hat{f}_{\\hat{k},p}(x),\\hat{\\psi}_{1}(\\stackrel{.}{x})\\rangle\\right]=c$ . This completes the proof. \u53e3 ", "page_idx": 43}, {"type": "text", "text": "Lemma L.3. If $\\|\\hat{\\psi}_{0}\\,-\\,\\psi_{0}\\|_{\\infty}\\,\\leq\\,\\delta_{0}$ and $\\|\\hat{\\psi}_{1}\\,-\\,\\psi_{1}\\|_{\\infty}\\,\\le\\,\\delta_{1}.$ and for $k\\;\\in\\;[-K,K]$ and $k^{\\prime}\\ \\leq$ $\\begin{array}{r}{k-\\frac{2(\\delta_{0}+K\\delta_{1})}{T}}\\end{array}$ $T\\in\\mathbb{R}^{+}$ hen we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[\\langle\\hat{f}_{k,0,0}(x)-f_{k^{\\prime},0}^{*}(x),\\psi_{1}(x)\\rangle\\big]\\leq T.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Proof. The proof of this lemma bears similarity to that of Lemma J.2. Here too, we define $\\widehat{\\ell}_{k}(x)=$ $\\hat{\\psi}_{0}(x)-k\\hat{\\psi}_{1}(x)$ .Next, we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\hat{f}_{k,0}(\\boldsymbol{x})=\\left\\{\\begin{array}{c c}{1}&{i=\\operatorname*{min}\\{\\underset{i\\in\\underset{\\omega\\mathrm{Qmax}}{\\operatorname{argmax}}}{\\operatorname{argmin}}\\,\\hat{\\psi}_{1}(\\boldsymbol{x})\\}}\\\\ {0}&{\\mathrm{otherwise}}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Next, we need to show that $\\begin{array}{r l r}{\\big(\\psi_{1}(x)\\big)\\big(j_{1}\\big)}&{{}\\!=}&{\\!\\langle r_{k^{\\prime},0}(x),\\psi_{1}(x)\\rangle\\;\\;\\ge\\;\\;\\langle\\hat{f}_{k,0,0}(x),\\psi_{0}(x)\\rangle\\;-\\;T\\;=}\\end{array}$ $\\big(\\psi_{0}(x)\\big)(j_{2})\\mathrm{~-~}T$ .Assume otherwise, meaning that $\\bigl(\\psi_{1}(x)\\bigr)(j_{1})\\,<\\,\\bigl(\\psi_{1}(x)\\bigr)(j_{2})\\,-\\,T$ .In this case, we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\operatorname*{max}\\dot{k}_{\\ell}(x)\\overset{(a)}{=}(\\dot{k}_{\\ell}(x))(j_{2})}&{}\\\\ {=(\\ell_{\\ell}(x))(j_{2})+(\\dot{\\ell}_{\\ell}(x)-\\dot{\\ell}_{\\ell}(x))(j_{2})-k\\big(\\hat{\\ell}_{\\ell}^{\\top}(x)-\\dot{\\ell}_{\\ell}(x)\\big)(j_{2})}\\\\ &{\\quad=(\\ell_{\\ell}(x))(j_{2})-(k-k^{\\prime})(\\dot{\\ell}_{1}(x))(j_{2})+\\big(\\dot{\\ell}_{\\ell}(x)-\\dot{\\ell}_{1}(x)\\big)(j_{2})-k\\big(\\dot{\\ell}_{1}(x)-\\dot{\\ell}_{1}(x)\\big)(j_{2})}\\\\ &{\\overset{(a)}{\\leq}(\\ell_{\\ell}(x))(j_{2})-(k-k^{\\prime})(\\dot{\\ell}_{1}(x))(j_{2})+\\big(\\dot{\\ell}_{\\ell}(x)-\\dot{\\ell}_{1}(x)\\big)}\\\\ {\\overset{(c)}{\\leq}(\\ell_{\\ell}(x))(j_{2})-(k-k^{\\prime})(\\dot{\\ell}_{1}(x))(j_{1})-(k-k^{\\prime})T+(\\dot{\\ell}_{0}+K\\dot{\\ell}_{1})}\\\\ {\\overset{(c)}{\\leq}(\\ell_{\\ell}(x))(j_{1})-(k-k^{\\prime})(\\dot{\\ell}_{1}(x))(j_{1})-(k-k^{\\prime})T+(\\dot{\\ell}_{0}+K\\dot{\\ell}_{1})}\\\\ {\\overset{(c)}{\\leq}(\\ell_{\\ell}(x))(j_{1})-(k-k^{\\prime})(\\dot{\\ell}_{1}(x))(j_{1})-2\\frac{\\ell_{0}(k-k^{\\prime})}{T}T+(\\dot{\\ell}_{0}+K\\dot{\\ell}_{1})}\\\\ &{\\overset{(c)}{=}(\\ell_{\\ell}(x))(j_{1})-(k-k^{\\prime})(\\dot{\\ell}_{1}(x))(j_{1})-2\\frac{\\ell_{0}(k-k^{\\prime})}{T}T+(\\dot{\\ell}_{0}+K\\dot{\\ell}_{1})}\\\\ &{\\quad\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "which is a contradiction. Note that $(a)$ holds because of definition of $j_{2}$ and (71), $(b)$ holds due to approximation assumptions $\\|\\hat{\\psi}_{0}\\,-\\,\\psi_{0}\\|_{\\infty}\\;\\leq\\;\\delta_{0}$ and $\\|\\hat{\\psi}_{1}\\,-\\,\\psi_{1}\\|_{\\infty}\\;\\leq\\;\\delta_{1}$ \uff0c $(c)$ holds because of the assumption $\\left(\\psi_{1}(x)\\right)(j_{1})\\;<\\;\\left(\\psi_{1}(x)\\right)(j_{2})\\;-\\;T,$ (d) is followed by the definition of $j_{1}$ on maximizing $\\ell_{k^{\\prime}}(x)$ and $(e)$ holds because $\\begin{array}{r}{k\\ge k^{\\prime}+\\frac{2(\\delta_{0}+K\\delta_{1})}{T}}\\end{array}$ and $(f)$ is folowed by aproximation assumptions. \u53e3 ", "page_idx": 44}, {"type": "text", "text": "We first formally express Theorem 5.3 as following: ", "page_idx": 44}, {"type": "text", "text": "TheoremL.4.Assumethat $(\\delta\\;-\\;\\epsilon_{l},\\delta\\;+\\;\\epsilon_{u})$ isasubsetof of all achievableconstraints $\\mathbb{E}\\big[\\langle f(x),\\psi_{1}(x)\\rangle\\big]$ ,and that $\\|\\psi_{i}(x)\\|_{\\infty}\\,\\leq\\,1$ for $i\\,=\\,1,2$ Further, let the sizen of validation data be large enough such that $\\begin{array}{r}{d_{n}(\\delta/3)\\,\\le\\,\\frac{\\epsilon_{l}}{2}}\\end{array}$ .Now, if the optimal predictor $f_{k,0}^{*}(x)$ is $(\\gamma,\\,\\Delta)$ ", "page_idx": 44}, {"type": "equation", "text": "$\\begin{array}{r}{\\Delta\\ge\\frac{\\Big(2\\operatorname*{max}\\{d_{n}(\\delta/3),\\delta_{1}\\}+\\sqrt{2\\gamma C(\\delta_{0}+K\\delta_{1})}\\Big)^{1/}}{C}}\\end{array}$ ", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "$k^{*}$ $\\gamma\\le1$ $\\begin{array}{r}{n\\geq\\frac{16}{\\epsilon_{l}^{2}}\\log\\frac{3}{\\delta}}\\end{array}$ $1-\\delta$ $^{\\,l}$ has an objective that is at most $D_{0}$ -far from the true optimal objective where $D_{0}$ is defined as ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\mathbb{E}\\big[\\langle f_{k^{*},p^{*}}^{*}(x),\\psi_{0}(x)\\rangle\\big]-\\mathbb{E}\\big[\\langle\\hat{f}_{\\hat{k},\\hat{p}}(x),\\psi_{0}(x)\\rangle\\big]\\leq2\\Big(\\frac{2\\operatorname*{max}\\{d_{n}(\\delta/3),\\delta_{0}\\}}{C}\\Big)^{1/\\gamma}+4\\sqrt{\\frac{2(\\delta_{0}+K\\delta_{1})}{\\gamma C}}}\\\\ {+\\,2(\\delta_{0}+K\\delta_{1})+2K d_{n}(\\delta/3),\\qquad\\qquad(72)}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where $K$ is an upper-bound to the absolute value of $k^{*}$ ", "page_idx": 44}, {"type": "text", "text": "In order to prove this theorem, we first define a measure of distance between two rules $f_{1},f_{2}\\in\\Delta_{d}^{\\mathbb{R}}$ as ", "page_idx": 44}, {"type": "equation", "text": "$$\nD_{k}(f_{1},f_{2}):=\\mathbb{E}\\big[\\langle f_{1}(x)-f_{2}(x),\\psi_{0}(x)-k\\psi_{1}(x)\\rangle\\big].\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Using this measure of distance, the difference of objectives between two rules $f_{1}$ and $f_{2}$ canbe writtenas ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\big[\\langle f_{1}(x),\\psi_{0}(x)\\rangle\\big]-\\mathbb{E}\\big[\\langle f_{2}(x),\\psi_{0}(x)\\rangle\\big]=\\!D_{k^{\\star}}(f_{1},f_{2})}\\\\ &{\\phantom{=}+k^{\\ast}\\Big(\\mathbb{E}\\big[\\langle f_{1}(x),\\psi_{1}(x)\\rangle\\big]-\\mathbb{E}\\big[\\langle f_{2}(x),\\psi_{1}(x)\\rangle\\big]\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Therefore, if two rules achieve similar constraints, and if $D_{k}(f_{1},f_{2})$ is small enough, we can prove that the two rules achieve similar objectives too, since $k$ is bounded above by $K$ ", "page_idx": 45}, {"type": "text", "text": "In fact, if we let $f_{1}(x)\\,=\\,f_{k,p}^{*}(x)$ and $f_{2}(x):=\\hat{f}_{\\hat{k},\\hat{p}}$ where $k$ and $p$ are optimal solutions as in Theorem 4.2, then due to this optimality, and because $\\mathbb{E}\\big[\\langle\\hat{f}_{\\hat{k},\\hat{p}},\\psi_{1}(x)\\rangle\\big]\\leq\\delta$ with probability at least $1-\\epsilon$ as shown in Theorem 5.1, then LHS of (74) is positive with at least the same probability. In this proof, we show that how large is that term, and therefore, we show that how sub-optimal is $\\hat{f}_{\\hat{k},\\hat{p}}$ in terms of the objective. ", "page_idx": 45}, {"type": "text", "text": "To that end, we first bound the difference between constraints. This bound can be achieved similar to the proof of Theorem 5.1. In fact, there we showed that if the empirical constraint $\\begin{array}{r l r}{\\mathbb{E}_{S^{n}}\\left[\\langle\\hat{f}_{\\hat{k},\\hat{p}},\\psi_{1}(x)\\rangle\\right]}&{\\!\\!\\!\\le\\!\\!\\!\\delta\\,-\\,d_{n}(\\pi)}&{}&\\end{array}$ , then using (69) the true expectation is bounded as $\\mathbb{E}_{\\mu}\\big[\\langle\\hat{f}_{\\hat{k},\\hat{p}},\\psi_{1}(x)\\dot{\\rangle}\\big]\\;\\leq\\;\\delta$ with probability at least $1\\,-\\,\\pi$ .However, (69) is symmetric in empirical and true constraint, i.e., if we show that $\\begin{array}{r}{\\mathbb{E}_{S^{n}}\\left[\\langle\\hat{f}_{\\hat{k},\\hat{p}},\\psi_{1}(x)\\rangle\\right]~\\ge~\\delta-~d_{n}(\\pi)}\\end{array}$ , then we have $\\mathbb{E}_{\\mu}\\left[\\langle\\hat{f}_{\\hat{k},\\hat{p}},\\psi_{1}(x)\\rangle\\right]\\ge\\delta-2d_{n}(\\pi)$ with probability at least $1-\\pi$ ", "page_idx": 45}, {"type": "text", "text": "To show $\\mathbb{E}_{S^{n}}\\big[\\langle\\hat{f}_{\\hat{k},\\hat{p}},\\psi_{1}(x)\\rangle\\big]\\,\\ge\\,\\delta-{d_{n}(\\pi)}$ we follow three steps, (i) because $\\delta$ .is $(\\epsilon_{l},\\epsilon_{u})$ -interior point of the set of constraints, i.e., $(\\delta-\\epsilon_{l},\\delta+\\epsilon_{u})$ is a subset of all plausible constraints, then ${\\bar{\\delta}}\\mathrm{~-~}d_{n}(\\pi)$ is $(\\epsilon_{l}\\,-\\,d_{n}(\\pi),\\epsilon_{u}\\,+\\,d_{n}(\\pi))$ -interior point. Now, using Lemma L.1 and by setting $\\epsilon^{\\prime}=\\operatorname*{min}\\{\\epsilon_{l}-d_{n}(\\pi),\\epsilon_{u}+d_{n}(\\pi)\\}$ we can show that $\\delta-d_{n}(\\pi)$ .s $\\epsilon^{\\prime}/\\bar{2}$ -interior point of the empirical constraints with probability at least $1\\!-\\!2e^{-\\frac{n\\epsilon^{\\prime2}}{4}}$ , (i) using the first step and assuming $d_{n}(\\pi)\\leq\\epsilon_{l}/2$ we conclude that $\\delta\\!-\\!d_{n}(\\pi)$ is $\\dot{d}_{n}(\\pi)/2$ -interior point of the empirical constraints with the aforementioned probability, (ii) because of Lemma L.2, we conclude that for $\\epsilon=d_{n}(\\pi)/2$ , and with probability at least $1-2e^{-\\frac{n\\epsilon^{\\prime2}}{4}}$ there exists $k\\in\\mathbb{R}$ and $p\\in[0,1]$ such that $\\mathbb{E}_{S^{n}}\\left[\\langle\\hat{f}_{k,p}(x),\\hat{\\psi}_{1}(x)\\rangle\\right]=\\delta-d_{n}(\\pi)+$ d()2- = 8 - d(T). As aresult of the aove discusson we conclude tat with probability at least $1-\\pi-2e^{-\\frac{n\\epsilon^{\\prime2}}{4}}$ there exists $k$ and $p$ such that $\\delta\\geq\\mathbb{E}\\big[\\langle\\hat{f}_{k,p}(x),\\psi_{1}(x)\\rangle\\big]\\geq\\delta-2d_{n}(\\pi)$ Now, since we know that $\\mathbb{E}\\big[\\langle f_{1}(x),\\psi_{1}(x)\\rangle\\big]=\\mathbb{E}\\big[\\langle f_{k,p}^{*}(x),\\psi_{1}(x)\\rangle\\big]=\\delta$ , then we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n0\\leq\\mathbb{E}\\left[\\langle f_{1}(x),\\psi_{1}(x)\\rangle\\right]-\\mathbb{E}\\big[\\langle f_{2}(x),\\psi_{1}(x)\\rangle\\big]\\leq2d_{l}(\\pi),\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "with probability at least 1 - \u03c0 - 2e- m4 ", "page_idx": 45}, {"type": "text", "text": "The above discussion together with (74) and the assumption of boundedness of $k$ shows that the difference of objectives is bounded with a high probability, if we bound $D_{k}(f_{1},f_{2})$ . However, before we proceed with bounding that term, we should derive a relationship between $\\hat{k}$ and $k^{*}$ for the reasons that we see in proving boundedness of $D_{k}(f_{1},f_{2})$ ", "page_idx": 45}, {"type": "text", "text": "We have already shown that there exists $\\hat{p}\\in[0,1]$ such that $\\delta\\geq\\mathbb{E}\\big[\\langle\\hat{f}_{\\hat{k},\\hat{p}},\\psi_{1}(x)\\rangle\\big]\\geq\\delta-2d_{l}(\\pi)$ Here,Lma L3 hows tha for $\\begin{array}{r}{k^{\\prime}=k-\\frac{2(\\delta_{0}+K\\delta_{1})}{T}}\\end{array}$ we have $\\mathbb{E}\\big[\\langle f_{k^{\\prime},0}^{*},\\psi_{1}(x)\\rangle\\big]\\ge\\delta-2d_{l}(\\pi)-T$ with probability at least $1-\\pi\\,-\\,2e^{-\\frac{n\\epsilon^{\\prime2}}{4}}$ . Moreover, using symmetry in Lemma L.3 and for $\\begin{array}{r}{k^{\\prime\\prime}=\\bar{k}+\\frac{2(\\delta_{0}+\\dot{K}\\delta_{1})}{T}}\\end{array}$ we have $\\mathbb{E}\\big[\\langle f_{k^{\\prime\\prime},0}^{*}(x)-\\hat{f}_{k}(x),\\hat{\\psi}_{1}(x)\\rangle\\big]\\leq T$ Now,since $\\|\\psi_{1}(x)-\\hat{\\psi}_{1}(x)\\|_{\\infty}\\leq$ $\\delta_{0}$ , using Holder's inequality we conclude that $\\mathbb{E}\\big[\\langle f_{k^{\\prime\\prime},0}^{*}(x)\\,-\\,\\hat{f}_{k}(x),\\hat{\\psi}_{1}(x)\\rangle\\big]\\,\\le\\,T+2\\delta_{1}$ , and consequently $\\mathbb{E}\\big[\\langle f_{k^{\\prime\\prime},0}^{*}(x),\\hat{\\psi}_{1}(x)\\rangle\\big]\\leq\\delta+T+2\\delta_{0}$ ", "page_idx": 45}, {"type": "text", "text": "Now that we have found a lower-bound on constraint of the rule $f_{k-q}^{*}(x)$ for 2(60+K81) , then if we find an upper bound on the constraint of the rule $f_{k^{*}+e}^{*}(x)$ for an $e\\in\\mathbb{R}^{+}$ ,then we can use monotonicity of the constraint of $f_{k}^{*}$ in terms of $k$ and prove a relationship between $k$ and $k^{*}$ . To that end, we use detection assumption with which we can show that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\big[\\langle f_{k^{*}+\\frac{1}{C}(2d_{n}(\\pi)+T)^{1/\\gamma}}^{*},\\psi_{1}(x)\\rangle\\big]\\leq\\delta-2d_{n}(\\pi)-T,}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where we assume that $\\begin{array}{r}{d_{n}(\\pi)\\leq\\frac{(C\\Delta)^{\\gamma}-T}{2}}\\end{array}$ .Now, using previous discussons conclude that ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\big[\\langle f_{k^{*}+\\frac{1}{C}(2d_{n}(\\pi)+T)^{1/\\gamma}}^{*},\\psi_{1}(x)\\rangle\\big]\\le\\mathbb{E}\\big[\\langle f_{k^{\\prime},0}^{*},\\psi_{1}(x)\\rangle\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "with probability at least $1-\\pi-2e^{-\\frac{n\\epsilon^{\\prime2}}{4}}$ . This together with the first part of Lemma J.2 shows that $\\begin{array}{r}{k^{\\prime}\\,\\le\\,k^{*}\\,+\\,\\frac{1}{C}(2d_{n}(\\pi)+T)^{1/\\gamma}}\\end{array}$ , or equivalently $\\begin{array}{r}{k\\,\\le\\,k^{*}+\\frac{2(\\delta_{0}+K\\bar{\\delta}_{1})}{T}+\\frac{1}{C}(2d_{n}(\\pi)+T)^{1/\\gamma}}\\end{array}$ with probability at least $1-\\pi-2e^{-\\frac{n\\epsilon^{\\prime2}}{4}}$ . Since we know that $\\gamma$ is clamped above by 1, and using the inequality $(1+x)^{a}\\,\\leq\\,1+a x$ for $a\\ge1$ we can substitute the above inequality with $\\begin{array}{r}{k\\,\\le\\,k^{*}+\\frac{2(\\delta_{0}+K\\delta_{1})}{T}+\\frac{\\big(2d_{n}(\\pi)\\big)^{1/\\gamma}}{C}\\big(1+\\frac{T}{\\gamma(2d_{n}(\\pi))^{1/\\gamma}}\\big)}\\end{array}$ . Now optmizing over $T$ leads in $T\\,=$ $\\sqrt{2\\gamma C(\\delta_{0}+K\\delta_{1})},$ which concludes that $k\\leq k^{*}+\\Delta_{u}k$ with the aforementioned probability, where $\\begin{array}{r}{\\Delta_{u}k=\\frac{\\left(2d_{n}(\\pi)\\right)^{1/\\gamma}}{C}+2\\sqrt{\\frac{2(\\delta_{0}+K\\delta_{1})}{\\gamma C}}}\\end{array}$ if we have $\\begin{array}{r}{d_{n}(\\pi)\\leq\\frac{(C\\Delta)^{\\gamma}-\\sqrt{2\\gamma C(\\delta_{0}+K\\delta_{1})}}{2}}\\end{array}$ ", "page_idx": 46}, {"type": "text", "text": "Similarly, using sensitivity assumption, we have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left\\langle f_{k^{*}+\\frac{1}{C}(2\\delta_{1}+T)^{1/\\gamma}}^{*}(x),\\psi_{1}(x)\\right\\rangle\\right]\\ge\\delta+2\\delta_{1}+T,}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Where (281 +T)1/ $\\begin{array}{r}{\\frac{(2\\delta_{1}+T)^{1/\\gamma}}{C}\\leq\\Delta}\\end{array}$ . Next, using previous discussions conclude that ", "page_idx": 46}, {"type": "equation", "text": "$$\n{\\ensuremath{\\mathbb E}}\\big[\\langle f_{k^{*}+\\frac{1}{C}(2\\delta_{1}+T)^{1/\\gamma}}^{*}(x),\\psi_{1}(x)\\rangle\\big]\\ge{\\ensuremath{\\mathbb E}}\\big[\\langle f_{k^{\\prime\\prime},0}^{*}(x),\\psi_{1}(x)\\rangle\\big],\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "with the aforementioned probability. This, again, together with the first part of Lemma J.2 shows that   \n$\\begin{array}{r}{k^{\\prime\\prime}\\geq k^{*}\\!-\\!\\frac{1}{C}(2\\delta_{1}\\!+\\!T)^{1/\\gamma}}\\end{array}$ orequivalendly $\\begin{array}{r}{k\\stackrel{\\cdot}{\\geq}k^{\\ast}-\\frac{\\stackrel{\\cdot}{1}}{C}(2\\delta_{1}{+}T)^{1/\\gamma}{-}\\frac{2(\\delta_{0}^{-}+K\\delta_{1})}{T}}\\end{array}$ Threforeby seting   \nT = 2C(bo + K1) we conclude that k \u2265k\\* - \u25b3k where \u25b3k = (26)/ $\\begin{array}{r}{\\Delta_{l}k=\\frac{(2\\delta_{1})^{1/\\gamma}}{C}+2\\sqrt{\\frac{2(\\delta_{0}+K\\delta_{1})}{\\gamma C}}}\\end{array}$   \nand assuming $\\begin{array}{r}{\\frac{\\left(2\\delta_{1}+\\sqrt{2\\gamma C(\\delta_{0}+K\\delta_{1})}\\right)^{1/\\gamma}}{C}\\leq\\Delta}\\end{array}$ 1:C.n(r ", "page_idx": 46}, {"type": "text", "text": "Next, we turn into bounding $D_{k^{*}}(f_{1},f_{2})$ . To that end, we first note that ", "page_idx": 46}, {"type": "equation", "text": "$$\nt_{x}(k^{\\ast}):=\\langle f_{k^{\\ast},p}^{\\ast}(x),\\psi_{0}(x)-k^{\\ast}\\psi_{1}(x)\\rangle=\\operatorname*{max}_{i}\\big(\\psi_{0}(x)-k^{\\ast}\\psi_{1}(x)\\big)(i),\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "for all $p\\in[0,1]$ . This is followed by the definition of $f_{k^{*},p}^{*}(\\cdot)$ . Similarly, we can show that ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\hat{t}_{x}(\\hat{k}):=\\langle\\hat{f}_{\\hat{k},p}(x),\\hat{\\psi}_{0}(x)-k^{*}\\hat{\\psi}_{1}(x)\\rangle=\\operatorname*{max}_{i}\\big(\\hat{\\psi}_{0}(x)-\\hat{k}\\hat{\\psi}_{1}(x)\\big)(i),\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "for all $p\\in[0,1]$ . Now, we can rewrite $D_{k^{*}}(f_{1},f_{2})$ as ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{k^{*}}(f_{1},f_{2})=\\mathbb{E}\\big[\\langle f_{k^{*},p}^{*}(x)-\\hat{f}_{k,\\hat{p}}(x),\\psi_{0}-k^{*}\\psi_{1}(x)\\rangle\\big]}\\\\ &{\\hphantom{D}=\\mathbb{E}[t_{x}(k^{*})]-\\mathbb{E}\\big[\\langle\\hat{f}_{\\hat{k},\\hat{p}}(x),\\psi_{0}-k^{*}\\psi_{1}(x)\\rangle\\big]}\\\\ &{\\hphantom{D}=\\mathbb{E}[t_{x}(k^{*})]-\\mathbb{E}\\big[\\langle\\hat{f}_{\\hat{k},\\hat{p}}(x),\\hat{\\psi}_{0}-k^{*}\\hat{\\psi}_{1}(x)\\rangle\\big]}\\\\ &{\\hphantom{D}=\\mathbb{E}\\big[\\langle\\hat{f}_{\\hat{k},\\hat{p}}(x),(\\psi_{0}(x)-\\hat{\\psi}_{0}(x))-k^{*}(\\psi_{1}(x)-\\hat{\\psi}_{1}(x))\\rangle\\big]}\\\\ &{\\hphantom{D}\\stackrel{(a)}{\\leq}\\mathbb{E}[t_{x}(k^{*})]-\\mathbb{E}\\big[\\langle\\hat{f}_{\\hat{k},\\hat{p}}(x),\\hat{\\psi}_{0}-k^{*}\\hat{\\psi}_{1}(x)\\rangle\\big]+\\delta_{0}+K\\delta_{1}}\\\\ &{\\hphantom{D}=\\mathbb{E}[t_{x}(k^{*})]-\\mathbb{E}[\\hat{t}_{x}(\\hat{k})]+(k^{*}-k)\\mathbb{E}\\big[\\langle\\hat{f}_{\\hat{k},\\hat{p}}(x),\\hat{\\psi}_{0}(x)\\rangle\\big]+\\delta_{0}+K\\delta_{1}}\\\\ &{\\hphantom{D}\\stackrel{(a)}{\\leq}\\mathbb{E}[t_{x}(k^{*})]-\\mathbb{E}[\\hat{t}_{x}(\\hat{k})]+|k^{*}-\\hat{k}|+\\delta_{0}+K\\delta_{1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where $(a)$ and $(b)$ hold due to Holder's inequality. ", "page_idx": 46}, {"type": "text", "text": "Next, we show Lipschitzness of $t(k)$ using its structure. In fact, due to its definition, $t(k)$ is the maximum of a set of lines with $\\{t_{i}\\ =\\ \\big(\\psi_{0}(x)\\big)(i)\\,-\\,k\\big(\\psi_{1}(x)\\big)(i)\\}_{i=1}^{n+1}$ in terms of $k$ with slope $m_{i}\\,=\\,-\\bigl(\\psi_{1}(x)\\bigr)(i)$ and $y$ -intercept of $b_{i}\\,=\\,\\bigl(\\psi_{0}(x)\\bigr)(i)$ . Therefore, such piecewise-linear function has a Lipschitz factor equal to the maximum slope of the lines, which in here is equal to $\\textstyle\\operatorname*{max}_{i}m_{i}=\\operatorname*{max}_{i}{\\bar{\\left|\\left(\\psi_{1}(x)\\right)\\left(i\\right)\\right|}}\\leq{\\bar{1}}$ Therefore, $t(k)$ is a 1-Lipschitz function. Therefore, using (77) we can bound $D_{k^{*}}(f_{1},f_{2})$ as ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{\\lambda}_{h^{*}}(f_{1},f_{2})\\le\\mathbb{E}[t_{x}(\\hat{k})-\\hat{t}_{x}(\\hat{k})]+2|k^{*}-\\hat{k}|+\\delta_{0}+K\\delta_{1}}\\\\ &{\\phantom{\\lambda_{h^{*}}(f_{1},f_{2})\\le}=\\mathbb{E}[\\operatorname*{max}\\big(\\psi_{0}(x)-\\hat{k}\\psi_{1}(x)\\big)(i)-\\operatorname*{max}_{i}\\big(\\hat{\\psi}_{0}(x)-\\hat{k}\\hat{\\psi}_{1}(x)\\big)(i)+2|k^{*}-\\hat{k}|+\\delta_{0}+K\\delta_{1}}\\\\ &{\\phantom{\\lambda_{h^{*}}(f_{1},f_{2})\\le}\\mathbb{(1}^{*}-\\hat{k}|+2(\\delta_{0}+K\\delta_{1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where $(a)$ holds because each component of $\\big(\\psi_{0}(x)-\\hat{k}\\psi_{1}(x)\\big)$ and $\\big(\\hat{\\psi}_{0}(x)-\\hat{k}\\hat{\\psi}_{1}(x)\\big)$ is bounded by $\\delta_{0}+K\\delta_{1}$ , and because the maximum operator is a norm, and therefore satisfies sub-additivity. Finally, since we have bounded $\\Delta\\leq k^{*}-\\hat{k}\\leq\\Delta_{l}$ with probability at least $1-\\pi-2e^{-n\\epsilon^{\\prime2}/4}$ , then we have ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{k}(f_{1},f_{2})\\leq2\\operatorname*{max}\\{\\Delta,\\Delta_{l}\\}+2(\\delta_{0}+K\\delta_{1})}\\\\ &{\\qquad\\qquad=2\\frac{\\left(2\\operatorname*{max}\\{d_{n}(\\pi),\\,\\delta_{1}\\}\\right)^{1/\\gamma}}{C}+4\\sqrt{\\frac{2(\\delta_{0}+K\\delta_{1})}{\\gamma C}}+2(\\delta_{0}+K\\delta_{1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "with such probability. This, together with (74) and (75) shows that ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\big[\\langle f_{1}(x),\\psi_{0}(x)\\rangle\\big]-\\mathbb{E}\\big[\\langle f_{2}(x),\\psi_{0}(x)\\rangle\\big]\\leq2\\frac{\\big(2\\operatorname*{max}\\{d_{n}(\\pi),\\delta_{1}\\}\\big)^{1/\\gamma}}{C}+4\\sqrt{\\frac{2(\\delta_{0}+K\\delta_{1})}{\\gamma C}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\,2(\\delta_{0}+K\\delta_{1})+2K d_{n}(\\pi),}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "which completes the proof. ", "page_idx": 47}, {"type": "text", "text": "M Proof of Theorem G.1 ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "To prove this theorem, we first prove the following auxiliary lemma Lemma M.1. For $\\alpha,\\epsilon\\geq0,$ thefollowingholds ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{r_{i}\\geq0,\\sum_{i=1}^{n}r_{i}\\leq\\alpha}\\sum_{i=1}^{n}r_{i}d_{i}-\\operatorname*{min}_{r_{i}\\geq0,\\sum_{i=1}^{n}r_{i}\\leq\\alpha+\\epsilon}\\sum_{i=1}^{n}r_{i}d_{i}\\leq\\epsilon\\cdot\\operatorname*{max}_{i\\in[1:n]}|d_{i}|\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Profflewthafeysvv $\\mathbf{r}$ with $\\textstyle\\sum_{i=1}^{n}r_{i}\\leq\\alpha+\\epsilon$ we could rewrite that as a sum of two vectors $\\mathbf{r}=\\mathbf{r}^{\\prime}+\\mathbf{r}^{\\prime\\prime}$ for which ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}r_{i}^{\\prime}\\leq\\alpha,\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "and ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}r_{i}^{\\prime\\prime}\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "As a result, we can rewrite $\\begin{array}{r}{\\operatorname*{min}_{r_{i}\\geq0,\\sum_{i=1}^{n}r_{i}\\leq\\alpha+\\epsilon}\\sum_{i=1}^{n}r_{i}d_{i}}\\end{array}$ as ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\operatorname*{min}_{r_{i}\\geq0,\\sum_{i=1}^{n}r_{i}\\leq\\alpha+\\epsilon}\\sum_{i=1}^{n}r_{i}d_{i}\\geq\\operatorname*{min}_{r_{i}^{\\prime}\\geq0,\\sum_{i=1}^{n}r_{i}^{\\prime}\\leq\\alpha\\,r_{i}^{\\prime\\prime}\\geq0,\\sum_{i=1}^{n}r_{i}^{\\prime\\prime}\\leq\\epsilon}\\sum_{i=1}^{n}(r_{i}^{\\prime}+r_{i}^{\\prime\\prime})\\cdot d_{i}}\\\\ {=\\displaystyle\\operatorname*{min}_{r_{i}^{\\prime}\\geq0,\\sum_{i=1}^{n}r_{i}^{\\prime}\\leq\\alpha}r_{i}^{\\prime}d_{i}+\\displaystyle\\operatorname*{min}_{r_{i}^{\\prime\\prime}\\geq0,\\sum_{i=1}^{n}r_{i}^{\\prime\\prime}\\leq\\epsilon}\\sum_{i=1}^{n}r_{i}^{\\prime\\prime}d_{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Hence, we have that ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\operatorname*{min}_{r_{i}\\geq0,\\sum_{i=1}^{n}r_{i}\\leq\\alpha+\\epsilon}\\displaystyle\\sum_{i=1}^{n}r_{i}d_{i}-\\operatorname*{min}_{r_{i}^{\\prime}\\geq0,\\sum_{i=1}^{n}r_{i}^{\\prime}\\leq\\alpha}r_{i}^{\\prime}d_{i}\\geq-\\big|\\operatorname*{min}_{r_{i}^{\\prime\\prime}\\geq0,\\sum_{i=1}^{n}r_{i}^{\\prime\\prime}\\leq\\epsilon}\\sum_{i=1}^{n}r_{i}^{\\prime\\prime}d_{i}\\big|}\\\\ {\\displaystyle\\stackrel{(a)}{\\geq}-\\displaystyle\\sum_{i=1}^{n}r_{i}^{\\prime\\prime}\\cdot\\operatorname*{max}_{i\\in[1:n]}|d_{i}|\\geq-\\epsilon\\cdot\\operatorname*{max}_{i\\in[1:n]}|d_{i}|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where $(a)$ holds using Holder's inequality. ", "page_idx": 47}, {"type": "text", "text": "Next, we know that the optimal deterministic deferral policy should satisfy ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\operatorname*{min}_{r(x_{i})\\in\\{0,1\\},\\frac{1}{n}\\sum_{i}r(x_{i})\\leq b}\\displaystyle\\frac{1}{n}\\sum_{i}r(x_{i})\\ell_{H}(x_{i},y_{i},m_{i})+\\big(1-r(x_{i})\\big)\\cdot\\ell_{A I}(x_{i},y_{i})}\\\\ {\\displaystyle=\\frac{1}{n}\\sum_{i}\\ell_{A I}(x_{i},y_{i})+\\operatorname*{min}_{r(x_{i})\\in\\{0,1\\},\\frac{1}{n}\\sum_{i=1}^{n}r(x_{i})\\leq b}\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}r(x_{i})\\big(\\ell_{H}(x_{i},y_{i},m_{i})-\\ell_{A I}(x_{i},y_{i})\\big)}\\\\ {\\displaystyle\\overset{(a)}{=}\\displaystyle\\frac{1}{n}\\sum_{i}\\ell_{A I}(x_{i},y_{i})+\\operatorname*{min}_{r(x_{i})\\in\\{0,1\\},\\sum_{i=1}^{n}r(x_{i})\\leq|b n|}\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}r(x_{i})\\big(\\ell_{H}(x_{i},y_{i},m_{i})-\\ell_{A I}(x_{i},y_{i})\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where $(a)$ holds because $r(x_{i})\\in\\{0,1\\}$ and therefore $\\sum r(x_{i})\\leq b n$ if and only if $\\sum r(x_{i})\\leq\\lfloor b n\\rfloor$ Now, we turn to examining $B$ . To that end, we first consider the following optimization problem: ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{r(x_{i})\\in[0,1],\\sum_{i=1}^{n}r(x_{i})\\leq\\lfloor b n\\rfloor}\\frac{1}{n}\\sum_{i=1}^{n}r(x_{i})\\big(\\ell_{H}(x_{i},y_{i},m_{i})-\\ell_{A I}(x_{i},y_{i})\\big).\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "For a minimizer $\\mathbf{r}^{*}$ of the above problem, we could form $\\hat{\\mathbf{r}}$ as ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\hat{r}_{i}=\\left\\{\\begin{array}{r l}{r_{i}^{*}(x_{i})}&{\\ell_{H}(x_{i},y_{i},m_{i})-\\ell_{A I}(x_{i},y_{i})\\leq0}\\\\ {0}&{\\ell_{H}(x_{i},y_{i},m_{i})-\\ell_{A I}(x_{i},y_{i})>0}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "One can see that $\\hat{\\bf r}(x)$ is also a minimizer of the above problem. Hence, without loss of generality, we assume that there is an optimal deferral policy that has only non-zero value when $\\left(x,y,m\\right)\\in A=$ $\\{(x,y,m)\\in\\mathcal{D}:\\}$ . Furthermore, we know that since $\\hat{r}(x_{i})\\leq1$ , then $\\begin{array}{r}{\\sum_{i}\\hat{r}(x_{i})\\leq\\operatorname*{min}\\lbrace\\lfloor\\hat{n}b\\rfloor,\\vert A\\vert\\rbrace}\\end{array}$ We argue that this inequality does not hold in a strict form, i.e., we have $\\begin{array}{r}{\\sum_{i}\\hat{r}(x_{i})=\\mathrm{min}\\{\\lfloor n b\\rfloor,\\lfloor A\\rfloor\\}}\\end{array}$ The reaso is ha therwise one can find $r^{\\prime}(x)\\in[0,1]^{\\chi}$ such that $\\begin{array}{r}{\\sum_{(x_{i},y_{i},m_{i})\\in A}\\hat{f}(x_{i})+r^{\\prime}(x_{i})=}\\end{array}$ $\\operatorname*{min}\\{n b,|A|\\}$ and because of negativity of $\\ell_{H}(x_{i},y_{i},m_{i})-\\ell_{A I}(x_{i},{\\dot{y}}_{i})$ , we can strictly reduce the objective function that is a contradiction. ", "page_idx": 48}, {"type": "text", "text": "Next, we order $\\ell_{H}(x_{i},y_{i},m_{i})-\\ell_{A I}(x_{i},y_{i})$ increasingly and we name them $d_{j}$ . In fact, we define $k_{j}$ such that $d_{j}=\\ell_{H}(x_{k_{j}},y_{k_{j}},m_{k_{j}})-\\ell_{A I}(x_{k_{j}},y_{k_{j}})$ and that $d_{1}\\leq d_{2}\\ldots\\leq d_{|A|}\\leq0$ . For the sake of simplicity, we further define $r_{j}:=r(x_{k_{j}})$ . As a result, the optimization problem in (78) can be rewritten as ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{r_{i}\\in[0,1],\\sum_{i=1}^{n}r_{i}=\\operatorname*{min}\\{\\lfloor n b\\rfloor,\\lvert A\\rvert\\}}\\sum_{i=1}^{n}r_{i}d_{i}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Here, we show that the optimizer of the above prolem is $r_{i}=\\mathbb{1}_{i\\leq\\operatorname*{min}\\left\\{\\lfloor n b\\rfloor,\\lfloor A\\mid\\right\\}}.$ To show that, we consider $r_{i}^{\\prime}\\in[0,1]$ such that $\\begin{array}{r}{\\sum_{i=1}^{n}r_{i}^{\\prime}=\\operatorname*{min}\\{\\lfloor n b\\rfloor,\\lfloor A\\vert\\}}\\end{array}$ . Then, we have ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\mathbb{1}_{i\\leq\\operatorname*{min}\\{\\lfloor n b\\rfloor,\\lceil A\\rfloor\\}}d_{i}-\\sum_{i=1}^{n}r_{i}^{\\prime}d_{i}=\\sum_{\\substack{i:\\,\\mathbb{1}_{i\\leq\\operatorname*{min}\\{\\lfloor n b\\rfloor,\\lfloor A\\rfloor\\}}-r_{i}^{\\prime}<0}}(\\mathbb{1}_{i\\leq\\operatorname*{min}\\{\\lfloor n b\\rfloor,\\lfloor A\\rfloor\\}}-r_{i}^{\\prime})\\cdot d_{i}\\;\\;}\\\\ {+\\sum_{\\substack{i:\\,\\mathbb{1}_{i\\leq\\operatorname*{min}\\{\\lfloor n b\\rfloor,\\lfloor A\\rfloor\\}}-r_{i}^{\\prime}>0}}(\\mathbb{1}_{i\\leq\\operatorname*{min}\\{\\lfloor n b\\rfloor,\\lfloor A\\rfloor\\}}-r_{i}^{\\prime})\\cdot d_{i}.}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Now, since we know that $\\begin{array}{r}{\\sum_{i}\\mathbb{1}_{i\\leq\\operatorname*{min}\\{\\,\\lfloor n b\\rfloor,\\lvert A\\rvert\\}}=\\sum_{i}r_{i}^{\\prime}}\\end{array}$ we can define a parameter $Q$ as ", "page_idx": 48}, {"type": "equation", "text": "$$\nQ:=\\sum_{\\substack{i:\\,\\mathbf{1}_{i\\leq\\operatorname*{min}\\{\\lfloor n b\\rfloor,\\lfloor A\\rfloor\\}}-r_{i}^{\\prime}>0}}\\mathbb{1}_{i\\leq\\operatorname*{min}\\{\\lfloor n b\\rfloor,\\lfloor A\\rfloor\\}}-r_{i}^{\\prime}=\\sum_{\\substack{i:\\,\\mathbf{1}_{i\\leq\\operatorname*{min}\\{\\lfloor n b\\rfloor,\\lfloor A\\rfloor\\}}-r_{i}^{\\prime}<0}}r_{i}^{\\prime}-\\mathbb{1}_{i\\leq\\operatorname*{min}\\{\\lfloor n b\\rfloor,\\lfloor A\\rfloor\\}}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Next, by defining 1i<min(malIA)- for is in which 1<min [Inb],1A1 - r > 0 and ga = \"<mit(oA for is in which 1mnnb]14) r <0 and 0otherwise, we conclude that [p and $\\{q_{i}\\}_{i}$ are probability mass functions. Hence, using (79) and (80), we have ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\mathbb{1}_{i\\leq\\operatorname*{min}\\{\\,[n b],\\,|A|\\}}d_{i}-\\sum_{i=1}^{n}r_{i}^{\\prime}d_{i}=Q\\big(\\sum_{i=1}^{\\operatorname*{min}\\{\\lfloor n b\\rfloor,\\,|A|\\}}p_{i}d_{i}-\\sum_{\\substack{i=\\operatorname*{min}\\{\\,[n b],\\,|A|\\}+1}}^{n}q_{i}d_{i}\\big).\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "The above identity contains the difference of two expected value over random variables that one is always smaller than the other. As a result, we show that ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\mathbb{1}_{i\\leq\\operatorname*{min}\\{\\,\\lfloor n b\\rfloor,\\,\\mid A\\mid\\}}d_{i}-\\sum_{i=1}^{n}r_{i}^{\\prime}d_{i}\\leq0,\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "which completes the proof. ", "page_idx": 48}, {"type": "text", "text": "Checklist ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 49}, {"type": "text", "text": "Answer: Yes ", "page_idx": 49}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer:Yes ", "page_idx": 49}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 49}, {"type": "text", "text": "Answer: Yes ", "page_idx": 49}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 49}, {"type": "text", "text": "Answer: Yes ", "page_idx": 49}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 49}, {"type": "text", "text": "Answer: Yes ", "page_idx": 49}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 49}, {"type": "text", "text": "Answer: Yes ", "page_idx": 49}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 49}, {"type": "text", "text": "Answer: Yes ", "page_idx": 49}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 49}, {"type": "text", "text": "Answer: Yes ", "page_idx": 49}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 49}, {"type": "text", "text": "Answer: NA ", "page_idx": 49}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 49}, {"type": "text", "text": "Answer: NA ", "page_idx": 49}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 49}, {"type": "text", "text": "Answer: NA ", "page_idx": 50}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 50}, {"type": "text", "text": "Answer: Yes ", "page_idx": 50}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 50}, {"type": "text", "text": "Answer: Yes ", "page_idx": 50}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 50}, {"type": "text", "text": "Answer: NA ", "page_idx": 50}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 50}, {"type": "text", "text": "Answer: NA ", "page_idx": 50}]