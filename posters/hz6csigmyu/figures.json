[{"figure_path": "Hz6cSigMyU/figures/figures_1_1.jpg", "caption": "Figure 1: A Case to demonstrate: (a) the necessity of aligning language agents with environments to exclude the wrong option, since the agent does not initially know that \"coffee table is empty\". (b) Action-level optimization is uncertain to what extent the key tokens, i.e. P(\\\"kitchen\\\"|p, \\\"Walk to\\\"), will be enhanced when optimizing the joint probability P(\\\"Walk to kitchen\\\"|p).", "description": "This figure demonstrates two key aspects. First, it showcases the importance of aligning language agents with their environments.  The example shows that a language agent, lacking world knowledge, might suggest an incorrect action (walking to an empty coffee table). Second, it highlights a limitation of action-level optimization in reinforcement learning. This method struggles to precisely enhance the probability of crucial tokens within an action, leading to inefficient learning and exploration.", "section": "1 Introduction"}, {"figure_path": "Hz6cSigMyU/figures/figures_7_1.jpg", "caption": "Figure 3: Performance comparisons on Overcooked (first two) and VirtualHome (last two).", "description": "This figure compares the performance of three reinforcement learning algorithms (TWOSOME, NTPO, and POAD) on four different tasks from the Overcooked and VirtualHome environments.  The x-axis represents the number of environment steps, and the y-axis represents the episodic return.  The shaded area around each line indicates the standard deviation across multiple runs.  The results show that POAD generally outperforms TWOSOME and NTPO in terms of both final performance and learning speed, indicating its effectiveness in aligning language agents with interactive environments.", "section": "6.3.1 Classical Sequential Decision-Making Tasks with Restricted Action Space"}, {"figure_path": "Hz6cSigMyU/figures/figures_8_1.jpg", "caption": "Figure 3: Performance comparisons on Overcooked (first two) and VirtualHome (last two).", "description": "This figure compares the performance of POAD, NTPO, and TWOSOME on four tasks from the Overcooked and VirtualHome environments.  The x-axis represents the number of environment steps, and the y-axis represents the performance (likely average reward or cumulative reward).  Each subplot shows the learning curves for the three methods on a specific task, allowing for a comparison of their learning speed and final performance. The shaded areas likely represent standard deviation or confidence intervals, indicating the variability in performance across different runs. The results demonstrate that POAD generally outperforms NTPO and often achieves comparable or even better results than TWOSOME.", "section": "6.3 Main Results"}, {"figure_path": "Hz6cSigMyU/figures/figures_8_2.jpg", "caption": "Figure 3: Performance comparisons on Overcooked (first two) and VirtualHome (last two).", "description": "This figure shows the training curves of POAD, NTPO, TWOSOME, and ARCHER on four different tasks from Overcooked and VirtualHome.  The x-axis represents the number of environment steps, and the y-axis represents the episodic return.  The shaded area around each line represents the standard deviation across multiple runs. The figure demonstrates that POAD generally outperforms other methods in terms of both convergence speed and final performance. The comparison highlights POAD's superior stability and efficiency compared to other methods.", "section": "6.3.1 Classical Sequential Decision-Making Tasks with Restricted Action Space"}, {"figure_path": "Hz6cSigMyU/figures/figures_16_1.jpg", "caption": "Figure 6: Visual comparison of the differences between four different backup approaches with the optimal policy after convergence. We show that optimizing the Q-function for each token with BAD (POAD) is equivalent to backup for the full action (TWOSOME), while others are not.", "description": "This figure compares four different reinforcement learning approaches, visualizing their credit assignment processes.  It shows how each method assigns credit to actions and tokens over multiple steps to achieve a goal.  The key takeaway is that POAD, using Bellman backup with action decomposition (BAD), achieves the same outcome as TWOSOME (optimizing at the action level) but with finer-grained token-level supervision. In contrast, NTPO and ARCHER show discrepancies between token-level and action-level optimization.", "section": "C Comparison of Optimality between Three Backup Approaches"}, {"figure_path": "Hz6cSigMyU/figures/figures_18_1.jpg", "caption": "Figure 7: Visual demonstrations of Overcooked and VirtualHome tasks [5].", "description": "This figure shows four different scenarios from two different environments used in the paper: Overcooked and VirtualHome.  The top two images (a and b) depict the task scenarios from the Overcooked environment, showing the preparation steps for Tomato Salad and Tomato-Lettuce Salad. The bottom two images (c and d) illustrate scenes from the VirtualHome environment, showcasing the Food Preparation and Entertainment tasks.  These environments are used to test the performance of the proposed language agent.", "section": "6.1 Environmental Setup"}, {"figure_path": "Hz6cSigMyU/figures/figures_19_1.jpg", "caption": "Figure 8: Case Study: Demonstration of the token-level credit assignment learned by the BAD at two states (left two). And a comparison of the volume of credit assignment for key tokens between different methods (right one), where TWOSOME indicates the credit assigned to entire actions instead of specific tokens.", "description": "This figure demonstrates the token-level credit assignment learned by the Bellman backup with Action Decomposition (BAD). The left two subfigures show the advantages of each token at the first and last states. The right subfigure compares the volume of credit assignment for key tokens between BAD, TWOSOME, and NTPO.  It highlights how BAD precisely assigns credit to key tokens while minimizing credit assignment to irrelevant tokens, unlike the other methods. ", "section": "F Case Study in Token-Level Credit Assignment"}, {"figure_path": "Hz6cSigMyU/figures/figures_24_1.jpg", "caption": "Figure 2: Visual comparison of the differences between action-level Bellman backup (left) and our BAD (right), given the goal turn on the TV, where q is the action or token value estimations, \u03b4t = qt - qt\u22121 or di = qi \u2013 qi\u22121 represent the credit assigned to corresponding actions and tokens respectively for policy update, e.g. the advantage value[43]. To facilitate understanding, a step-by-step breakdown of the right figure is provided in Appendix L.", "description": "This figure compares the action-level Bellman backup with the proposed Bellman backup with Action Decomposition (BAD). The left side shows the traditional action-level backup, while the right side illustrates the BAD method. The key difference is that BAD assigns credit to individual tokens within an action, providing finer-grained supervision for policy optimization. The figure uses the example of turning on a TV to illustrate how the credit is assigned step by step. Appendix L provides a detailed breakdown of the BAD process shown on the right side.", "section": "5 Action-Decomposition Reinforcement Learning"}, {"figure_path": "Hz6cSigMyU/figures/figures_24_2.jpg", "caption": "Figure 2: Visual comparison of the differences between action-level Bellman backup (left) and our BAD (right), given the goal turn on the TV, where q is the action or token value estimations, \u03b4t = qt - qt\u22121 or di = qi \u2013 qi\u22121 represent the credit assigned to corresponding actions and tokens respectively for policy update, e.g. the advantage value[43]. To facilitate understanding, a step-by-step breakdown of the right figure is provided in Appendix L.", "description": "This figure compares the action-level Bellman backup with the proposed Bellman backup with Action Decomposition (BAD). The left side shows the traditional action-level approach, while the right side illustrates BAD.  BAD provides finer-grained credit assignment by considering individual tokens within an action, leading to more precise policy updates. The Appendix L provides a detailed step-by-step explanation of the BAD process shown on the right.", "section": "5 Action-Decomposition Reinforcement Learning"}, {"figure_path": "Hz6cSigMyU/figures/figures_24_3.jpg", "caption": "Figure 2: Visual comparison of the differences between action-level Bellman backup (left) and our BAD (right), given the goal turn on the TV, where q is the action or token value estimations, \u03b4t = qt - qt\u22121 or di = qi \u2013 qi\u22121 represent the credit assigned to corresponding actions and tokens respectively for policy update, e.g. the advantage value[43]. To facilitate understanding, a step-by-step breakdown of the right figure is provided in Appendix L.", "description": "This figure compares the action-level Bellman backup with the proposed Bellman backup with Action Decomposition (BAD). The left side shows the traditional action-level backup, while the right side shows the proposed BAD method. The key difference is that BAD provides finer-grained credit assignment to individual tokens within an action, rather than assigning credit to the entire action as a whole.  Appendix L provides a detailed, step-by-step explanation of BAD.", "section": "5 Action-Decomposition Reinforcement Learning"}, {"figure_path": "Hz6cSigMyU/figures/figures_25_1.jpg", "caption": "Figure 2: Visual comparison of the differences between action-level Bellman backup (left) and our BAD (right), given the goal turn on the TV, where q is the action or token value estimations, \u03b4t = qt - qt\u22121 or di = qi \u2013 qi\u22121 represent the credit assigned to corresponding actions and tokens respectively for policy update, e.g. the advantage value[43]. To facilitate understanding, a step-by-step breakdown of the right figure is provided in Appendix L.", "description": "This figure compares the action-level Bellman backup with the proposed Bellman backup with Action Decomposition (BAD). The left side shows the traditional action-level backup, where credit is assigned to the whole action. The right side shows BAD, where credit is assigned to individual tokens within the action. The step-by-step breakdown of BAD is provided in Appendix L for better understanding.", "section": "5 Action-Decomposition Reinforcement Learning"}, {"figure_path": "Hz6cSigMyU/figures/figures_25_2.jpg", "caption": "Figure 12: Step 7: Modifying q+ and q\u2212 in both trajectories with the same Q(\\\"Turn\\\"|ot). Step 8: Starting to back-propagate again for credits assigned to each token for optimization with \u03b4j = qj \u2212 qj\u22121 for j > 0; this process is similar to the calculation for advantage values.", "description": "This figure shows a step-by-step breakdown of how the Bellman backup with Action Decomposition (BAD) precisely assigns credit to each token.  It details steps 7 and 8 of the BAD process, showing how credit is modified and back-propagated for tokens in both positive and negative trajectories, ultimately leading to the calculation of advantage values for optimization.", "section": "Step-by-Step Breakdown of BAD"}]