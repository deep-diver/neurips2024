[{"figure_path": "dE1bTyyC9A/figures/figures_0_1.jpg", "caption": "Figure 1: Comparisons between the proposed method and current SOTA approaches specialized for specific tasks. (a) Representative specialized approaches on six tasks. (b) OneFormer3D, a recent unified framework, achieves SOTA performance on three generic segmentation tasks in one inference. (c) The proposed unified framework achieves six tasks in one inference. (d) Our method outperforms current SOTA approaches across six tasks involving two modalities using a single model.", "description": "This figure compares the proposed UniSeg3D method with existing state-of-the-art (SOTA) approaches for 3D scene understanding.  Subfigure (a) shows several specialized SOTA models, each designed for a single task (panoptic, semantic, instance, interactive, referring, and open-vocabulary segmentation). Subfigure (b) illustrates OneFormer3D, a recent unified method that handles three tasks (panoptic, semantic, and instance segmentation) in a single inference. In contrast, subfigure (c) presents UniSeg3D, the proposed unified framework which performs all six tasks in one inference. Finally, subfigure (d) provides a quantitative comparison showing UniSeg3D's superior performance over other SOTA methods across all six tasks.  The improvement highlights the effectiveness of the unified framework.", "section": "Introduction"}, {"figure_path": "dE1bTyyC9A/figures/figures_3_1.jpg", "caption": "Figure 2: The framework of UniSeg3D. This is a simple framework handling six tasks in parallel without any modules specialized for specific tasks. We take advantage of multi-task unification and enhance the performance through building associations between the supported tasks. Specifically, knowledge distillation transfers insights from interactive segmentation to the other tasks, while contrastive learning establishes connections between interactive segmentation and referring segmentation.", "description": "This figure illustrates the architecture of the UniSeg3D model, a unified framework designed to handle six 3D scene understanding tasks simultaneously.  The framework consists of three main modules: a point cloud backbone for processing point cloud data, prompt encoders for incorporating visual and textual information as prompts, and a mask decoder for generating predictions for all six tasks. The model leverages multi-task learning strategies, including knowledge distillation (transferring knowledge from interactive segmentation to other tasks) and contrastive learning (establishing connections between interactive and referring segmentation), to improve overall performance.  The unified query mechanism allows for parallel processing of the various task inputs.", "section": "3 Methodology"}, {"figure_path": "dE1bTyyC9A/figures/figures_4_1.jpg", "caption": "Figure 3: Illustration of the inter-task association. (a) A challenging case requiring the distinction of textual positional information within the expressions. (b) A contrastive learning matrix for vision-text pairs, where a ranking rule is employed to suppress incorrect pairings. (c) Knowledge distillation across multi-task predictions.", "description": "This figure illustrates how the UniSeg3D model handles the inter-task associations to improve its performance.  Panel (a) shows a challenging scenario where distinguishing objects based on textual descriptions requires precise understanding of spatial relationships. Panel (b) details the ranking-based contrastive learning, where vision and text features of objects are compared to strengthen associations, ranking similar pairs higher.  Panel (c) depicts knowledge distillation, transferring knowledge from interactive segmentation (a task that already has strong visual guidance) to other tasks to enhance their overall performance.  The overall aim is to leverage the interdependencies between different segmentation tasks for improved performance.", "section": "3.3 Explicit Inter-task Association"}, {"figure_path": "dE1bTyyC9A/figures/figures_16_1.jpg", "caption": "Figure I: Visualization of segmentation results obtained by UniSeg3D on ScanNet20 validation split.", "description": "This figure shows a visualization of the segmentation results obtained by the UniSeg3D model on the ScanNet20 validation split.  It provides a qualitative comparison of the model's performance across six different 3D segmentation tasks: Instance Segmentation, Semantic Segmentation, Panoptic Segmentation, Interactive Segmentation, Referring Segmentation, and Open-Vocabulary Segmentation. For each task, the figure displays the input point cloud data, the ground truth segmentation masks, and the model's predicted segmentation masks. This visual representation allows for a qualitative assessment of the model's accuracy and effectiveness in performing the various segmentation tasks.", "section": "C Qualitative visualizations illustrating model effectiveness"}, {"figure_path": "dE1bTyyC9A/figures/figures_16_2.jpg", "caption": "Figure II: Visualization of segmentation results obtained by UniSeg3D and current SOTA methods on ScanNet20 validation split.", "description": "This figure compares the segmentation results of UniSeg3D with several state-of-the-art (SOTA) methods on the ScanNet20 validation split.  It shows a visual comparison across six different 3D segmentation tasks: instance segmentation, semantic segmentation, panoptic segmentation, interactive segmentation, referring segmentation, and open-vocabulary segmentation.  For each task and method, it displays the input point cloud data, the ground truth segmentation, and the model's prediction. This allows for a direct visual assessment of UniSeg3D's performance relative to the current SOTA methods in 3D scene understanding.", "section": "C Qualitative visualizations illustrating model effectiveness"}, {"figure_path": "dE1bTyyC9A/figures/figures_16_3.jpg", "caption": "Figure 1: Comparisons between the proposed method and current SOTA approaches specialized for specific tasks. (a) Representative specialized approaches on six tasks. (b) OneFormer3D, a recent unified framework, achieves SOTA performance on three generic segmentation tasks in one inference. (c) The proposed unified framework achieves six tasks in one inference. (d) Our method outperforms current SOTA approaches across six tasks involving two modalities using a single model.", "description": "This figure compares the proposed UniSeg3D method with state-of-the-art (SOTA) approaches for 3D scene understanding.  It shows that existing methods typically focus on individual tasks (a), while a recent unified model, OneFormer3D, handles only three tasks (b).  In contrast, UniSeg3D offers a unified solution performing six different 3D segmentation tasks simultaneously (c) and outperforms all previous methods on these tasks (d). The six tasks include panoptic, semantic, instance, interactive, referring, and open-vocabulary segmentation. The figure highlights the efficiency and comprehensive nature of UniSeg3D.", "section": "Introduction"}]