{"references": [{"fullname_first_author": "Jacob Devlin", "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2018-10-04", "reason": "This paper introduces BERT, a masked language model architecture that is fundamental to the PRISM algorithm used in the main paper for bias detection."}, {"fullname_first_author": "Yinhan Liu", "paper_title": "Roberta: A robustly optimized bert pretraining approach", "publication_date": "2019-07-11", "reason": "This paper enhances the original BERT model, improving its robustness and performance, which is crucial for reliable bias detection using the PRISM algorithm."}, {"fullname_first_author": "Percy Liang", "paper_title": "Holistic evaluation of language models", "publication_date": "2022-11-09", "reason": "This paper provides a comprehensive framework for evaluating language models, which is relevant to the main paper's focus on assessing bias in language generation."}, {"fullname_first_author": "Danielle Gaucher", "paper_title": "Evidence that gendered wording in job advertisements exists and sustains gender inequality", "publication_date": "2011-00-00", "reason": "This paper establishes the link between gendered language in job advertisements and gender inequality, which is a key context for understanding the biases investigated in the main paper."}, {"fullname_first_author": "Alla Konnikov", "paper_title": "Bias word inventory for work and employment diversity, (in) equality and inclusivity (version 2.0)", "publication_date": "2022-00-00", "reason": "This paper provides a validated inventory of social cues, forming the basis for the bias evaluation framework in the main paper, ensuring contextually-salient and empirically-grounded assessments."}]}